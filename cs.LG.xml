<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#21333;&#20010;&#27531;&#24046;&#36830;&#25509;&#30340;&#26435;&#37325;&#32422;&#26463;&#26550;&#26500;&#65292;&#21487;&#20197;&#23454;&#29616;&#20219;&#24847;&#23376;&#38598;&#30340;&#36755;&#20837;&#30340;&#31934;&#30830;&#21333;&#35843;&#20381;&#36182;&#12290;&#19982;&#29616;&#26377;&#25216;&#26415;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#23454;&#26045;&#31616;&#21333;&#19988;&#29702;&#35770;&#22522;&#30784;&#26356;&#31616;&#21270;&#65292;&#35745;&#31639;&#24320;&#38144;&#24456;&#23567;&#65292;&#24182;&#19988;&#20445;&#35777;&#20135;&#29983;&#21333;&#35843;&#36755;&#20986;&#12290;</title><link>http://arxiv.org/abs/2307.07512</link><description>&lt;p&gt;
&#34920;&#36798;&#24615;&#21333;&#35843;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Expressive Monotonic Neural Networks. (arXiv:2307.07512v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07512
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#21333;&#20010;&#27531;&#24046;&#36830;&#25509;&#30340;&#26435;&#37325;&#32422;&#26463;&#26550;&#26500;&#65292;&#21487;&#20197;&#23454;&#29616;&#20219;&#24847;&#23376;&#38598;&#30340;&#36755;&#20837;&#30340;&#31934;&#30830;&#21333;&#35843;&#20381;&#36182;&#12290;&#19982;&#29616;&#26377;&#25216;&#26415;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#23454;&#26045;&#31616;&#21333;&#19988;&#29702;&#35770;&#22522;&#30784;&#26356;&#31616;&#21270;&#65292;&#35745;&#31639;&#24320;&#38144;&#24456;&#23567;&#65292;&#24182;&#19988;&#20445;&#35777;&#20135;&#29983;&#21333;&#35843;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#36755;&#20986;&#23545;&#26576;&#20123;&#36755;&#20837;&#30340;&#21333;&#35843;&#20381;&#36182;&#26159;&#35768;&#22810;&#22330;&#26223;&#20013;&#30340;&#37325;&#35201;&#24402;&#32435;&#20559;&#35265;&#65292;&#20854;&#20013;&#39046;&#22495;&#30693;&#35782;&#35201;&#27714;&#36825;&#31181;&#34892;&#20026;&#12290;&#36825;&#23545;&#35299;&#37322;&#24615;&#21644;&#20844;&#24179;&#24615;&#32771;&#34385;&#23588;&#20026;&#37325;&#35201;&#12290;&#22312;&#26356;&#24191;&#27867;&#30340;&#32972;&#26223;&#19979;&#65292;&#21333;&#35843;&#24615;&#37325;&#35201;&#30340;&#24773;&#20917;&#21487;&#22312;&#37329;&#34701;&#12289;&#21307;&#23398;&#12289;&#29289;&#29702;&#31561;&#23398;&#31185;&#20013;&#25214;&#21040;&#12290;&#22240;&#27492;&#65292;&#26500;&#24314;&#33021;&#22815;&#21487;&#38752;&#23454;&#29616;&#36825;&#31181;&#24402;&#32435;&#20559;&#35265;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26159;&#20540;&#24471;&#26399;&#26395;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#21333;&#20010;&#27531;&#24046;&#36830;&#25509;&#30340;&#26435;&#37325;&#32422;&#26463;&#26550;&#26500;&#65292;&#20197;&#23454;&#29616;&#20219;&#24847;&#23376;&#38598;&#30340;&#36755;&#20837;&#30340;&#31934;&#30830;&#21333;&#35843;&#20381;&#36182;&#12290;&#26435;&#37325;&#32422;&#26463;&#26041;&#26696;&#30452;&#25509;&#25511;&#21046;&#31070;&#32463;&#32593;&#32476;&#30340;Lipschitz&#24120;&#25968;&#65292;&#20174;&#32780;&#25552;&#20379;&#39069;&#22806;&#30340;&#40065;&#26834;&#24615;&#12290;&#19982;&#30446;&#21069;&#29992;&#20110;&#21333;&#35843;&#24615;&#30340;&#29616;&#26377;&#25216;&#26415;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23454;&#29616;&#21644;&#29702;&#35770;&#22522;&#30784;&#19978;&#26356;&#31616;&#21333;&#65292;&#35745;&#31639;&#24320;&#38144;&#20960;&#20046;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#65292;&#24182;&#19988;&#20445;&#35777;&#20135;&#29983;&#21333;&#35843;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
The monotonic dependence of the outputs of a neural network on some of its inputs is a crucial inductive bias in many scenarios where domain knowledge dictates such behavior. This is especially important for interpretability and fairness considerations. In a broader context, scenarios in which monotonicity is important can be found in finance, medicine, physics, and other disciplines. It is thus desirable to build neural network architectures that implement this inductive bias provably. In this work, we propose a weight-constrained architecture with a single residual connection to achieve exact monotonic dependence in any subset of the inputs. The weight constraint scheme directly controls the Lipschitz constant of the neural network and thus provides the additional benefit of robustness. Compared to currently existing techniques used for monotonicity, our method is simpler in implementation and in theory foundations, has negligible computational overhead, is guaranteed to produce mono
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#23558;&#21160;&#24577;&#36710;&#36742;&#35843;&#24230;&#38382;&#39064;&#24314;&#27169;&#20026;&#21322;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#22522;&#20110;&#20107;&#20214;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#38469;&#22330;&#26223;&#30340;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.07508</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#21160;&#24577;&#36710;&#36742;&#35843;&#24230;&#38382;&#39064;&#30340;&#22522;&#20110;&#20107;&#20214;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning for the dynamic vehicle dispatching problem: An event-based approach. (arXiv:2307.07508v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07508
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#23558;&#21160;&#24577;&#36710;&#36742;&#35843;&#24230;&#38382;&#39064;&#24314;&#27169;&#20026;&#21322;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#22522;&#20110;&#20107;&#20214;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#38469;&#22330;&#26223;&#30340;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#36710;&#36742;&#35843;&#24230;&#38382;&#39064;&#28041;&#21450;&#20915;&#23450;&#23558;&#21738;&#20123;&#36710;&#36742;&#20998;&#37197;&#32473;&#38543;&#26426;&#20135;&#29983;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#35831;&#27714;&#12290;&#35813;&#38382;&#39064;&#20986;&#29616;&#22312;&#21508;&#20010;&#39046;&#22495;&#65292;&#22914;&#23558;&#21345;&#36710;&#20998;&#37197;&#32473;&#35201;&#36816;&#36755;&#30340;&#36135;&#29289;&#12289;&#24212;&#24613;&#31995;&#32479;&#21644;&#39034;&#39118;&#36710;&#26381;&#21153;&#20013;&#12290;&#26412;&#25991;&#23558;&#35813;&#38382;&#39064;&#24314;&#27169;&#20026;&#21322;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#26102;&#38388;&#35270;&#20026;&#36830;&#32493;&#21464;&#37327;&#12290;&#22312;&#27492;&#35774;&#32622;&#20013;&#65292;&#20915;&#31574;&#26102;&#21051;&#19982;&#20107;&#20214;&#19968;&#33268;&#65292;&#20854;&#26102;&#38388;&#38388;&#38548;&#26159;&#38543;&#26426;&#30340;&#12290;&#25105;&#20204;&#35748;&#20026;&#22522;&#20110;&#20107;&#20214;&#30340;&#26041;&#27861;&#22823;&#22823;&#38477;&#20302;&#20102;&#20915;&#31574;&#31354;&#38388;&#30340;&#32452;&#21512;&#22797;&#26434;&#24615;&#65292;&#24182;&#20811;&#26381;&#20102;&#25991;&#29486;&#20013;&#24120;&#25552;&#20986;&#30340;&#31163;&#25955;&#26102;&#38388;&#27169;&#22411;&#30340;&#20854;&#20182;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#27979;&#35797;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#31163;&#25955;&#20107;&#20214;&#27169;&#25311;&#22120;&#65292;&#24182;&#20351;&#29992;&#21452;&#28145;&#24230;Q&#23398;&#20064;&#35757;&#32451;&#25105;&#20204;&#30340;&#20915;&#31574;&#20195;&#29702;&#12290;&#22312;&#20351;&#29992;&#32445;&#32422;&#24066;&#30340;&#25968;&#25454;&#36827;&#34892;&#23454;&#38469;&#22330;&#26223;&#30340;&#25968;&#20540;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23558;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;&#33719;&#24471;&#30340;&#31574;&#30053;&#19982;&#21551;&#21457;&#24335;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
The dynamic vehicle dispatching problem corresponds to deciding which vehicles to assign to requests that arise stochastically over time and space. It emerges in diverse areas, such as in the assignment of trucks to loads to be transported; in emergency systems; and in ride-hailing services. In this paper, we model the problem as a semi-Markov decision process, which allows us to treat time as continuous. In this setting, decision epochs coincide with discrete events whose time intervals are random. We argue that an event-based approach substantially reduces the combinatorial complexity of the decision space and overcomes other limitations of discrete-time models often proposed in the literature. In order to test our approach, we develop a new discrete-event simulator and use double deep q-learning to train our decision agents. Numerical experiments are carried out in realistic scenarios using data from New York City. We compare the policies obtained through our approach with heuristic
&lt;/p&gt;</description></item><item><title>MGit&#26159;&#19968;&#20010;&#27169;&#22411;&#29256;&#26412;&#25511;&#21046;&#21644;&#31649;&#29702;&#31995;&#32479;&#65292;&#35299;&#20915;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#27169;&#22411;&#27966;&#29983;&#29289;&#30340;&#22256;&#38590;&#31649;&#29702;&#38382;&#39064;&#65292;&#36890;&#36807;&#24341;&#20837;&#34880;&#32479;&#22270;&#21644;&#20248;&#21270;&#23384;&#20648;&#26426;&#21046;&#65292;&#20351;&#24471;&#23384;&#20648;&#12289;&#27979;&#35797;&#12289;&#26356;&#26032;&#21644;&#21327;&#20316;&#27169;&#22411;&#27966;&#29983;&#29289;&#26356;&#21152;&#23481;&#26131;&#12290;</title><link>http://arxiv.org/abs/2307.07507</link><description>&lt;p&gt;
MGit:&#19968;&#20010;&#27169;&#22411;&#29256;&#26412;&#25511;&#21046;&#21644;&#31649;&#29702;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
MGit: A Model Versioning and Management System. (arXiv:2307.07507v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07507
&lt;/p&gt;
&lt;p&gt;
MGit&#26159;&#19968;&#20010;&#27169;&#22411;&#29256;&#26412;&#25511;&#21046;&#21644;&#31649;&#29702;&#31995;&#32479;&#65292;&#35299;&#20915;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#27169;&#22411;&#27966;&#29983;&#29289;&#30340;&#22256;&#38590;&#31649;&#29702;&#38382;&#39064;&#65292;&#36890;&#36807;&#24341;&#20837;&#34880;&#32479;&#22270;&#21644;&#20248;&#21270;&#23384;&#20648;&#26426;&#21046;&#65292;&#20351;&#24471;&#23384;&#20648;&#12289;&#27979;&#35797;&#12289;&#26356;&#26032;&#21644;&#21327;&#20316;&#27169;&#22411;&#27966;&#29983;&#29289;&#26356;&#21152;&#23481;&#26131;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20854;&#20182;&#27169;&#22411;&#27966;&#29983;&#20986;&#30340;&#27169;&#22411;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#38750;&#24120;&#24120;&#35265;&#12290;&#20363;&#22914;&#65292;&#36890;&#36807;&#24494;&#35843;&#65292;&#36801;&#31227;&#23398;&#20064;&#29992;&#20110;&#20174;&#8220;&#39044;&#35757;&#32451;&#8221;&#27169;&#22411;&#21019;&#24314;&#29305;&#23450;&#20219;&#21153;&#30340;&#27169;&#22411;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#20010;&#27169;&#22411;&#20043;&#38388;&#30456;&#20851;&#19988;&#20849;&#20139;&#32467;&#26500;&#29978;&#33267;&#21442;&#25968;&#20540;&#30340;&#29983;&#24577;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#31649;&#29702;&#36825;&#20123;&#27169;&#22411;&#27966;&#29983;&#29289;&#26159;&#22256;&#38590;&#30340;&#65306;&#23384;&#20648;&#25152;&#26377;&#27966;&#29983;&#27169;&#22411;&#30340;&#24320;&#38144;&#24456;&#24555;&#21464;&#24471;&#32321;&#37325;&#65292;&#23548;&#33268;&#29992;&#25143;&#20002;&#24323;&#21487;&#33021;&#23545;&#36827;&#19968;&#27493;&#20998;&#26512;&#26377;&#29992;&#30340;&#20013;&#38388;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#38590;&#20197;&#36861;&#36394;&#27169;&#22411;&#20013;&#30340;&#19981;&#33391;&#34892;&#20026;&#65288;&#20363;&#22914;&#65292;&#26159;&#21542;&#20174;&#19978;&#28216;&#27169;&#22411;&#32487;&#25215;&#20102;&#19968;&#20010;&#38169;&#35823;&#65311;&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MGit&#30340;&#27169;&#22411;&#29256;&#26412;&#25511;&#21046;&#21644;&#31649;&#29702;&#31995;&#32479;&#65292;&#23427;&#20351;&#24471;&#23384;&#20648;&#12289;&#27979;&#35797;&#12289;&#26356;&#26032;&#21644;&#21327;&#20316;&#27169;&#22411;&#27966;&#29983;&#29289;&#26356;&#21152;&#23481;&#26131;&#12290;MGit&#24341;&#20837;&#20102;&#19968;&#20010;&#35760;&#24405;&#27169;&#22411;&#20043;&#38388;&#26469;&#28304;&#21644;&#29256;&#26412;&#20449;&#24687;&#30340;&#34880;&#32479;&#22270;&#65292;&#20197;&#21450;&#20248;&#21270;&#23384;&#20648;&#27169;&#22411;&#21442;&#25968;&#30340;&#25277;&#35937;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Models derived from other models are extremely common in machine learning (ML) today. For example, transfer learning is used to create task-specific models from "pre-trained" models through finetuning. This has led to an ecosystem where models are related to each other, sharing structure and often even parameter values. However, it is hard to manage these model derivatives: the storage overhead of storing all derived models quickly becomes onerous, prompting users to get rid of intermediate models that might be useful for further analysis. Additionally, undesired behaviors in models are hard to track down (e.g., is a bug inherited from an upstream model?). In this paper, we propose a model versioning and management system called MGit that makes it easier to store, test, update, and collaborate on model derivatives. MGit introduces a lineage graph that records provenance and versioning information between models, optimizations to efficiently store model parameters, as well as abstractio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19981;&#21516;&#26550;&#26500;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#20998;&#26512;&#21644;&#20998;&#31867;&#33041;&#32959;&#30244;&#65292;&#36890;&#36807;&#20248;&#21270;&#32593;&#32476;&#24182;&#28155;&#21152;&#36339;&#36291;&#36830;&#25509;&#65292;&#25552;&#39640;&#20102;&#32593;&#32476;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.07503</link><description>&lt;p&gt;
&#20351;&#29992;&#20855;&#26377;&#36339;&#36291;&#36830;&#25509;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#33041;&#32959;&#30244;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Brain Tumor Detection using Convolutional Neural Networks with Skip Connections. (arXiv:2307.07503v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07503
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19981;&#21516;&#26550;&#26500;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#20998;&#26512;&#21644;&#20998;&#31867;&#33041;&#32959;&#30244;&#65292;&#36890;&#36807;&#20248;&#21270;&#32593;&#32476;&#24182;&#28155;&#21152;&#36339;&#36291;&#36830;&#25509;&#65292;&#25552;&#39640;&#20102;&#32593;&#32476;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#22312;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;MRI&#65289;&#25216;&#26415;&#19979;&#20998;&#26512;&#21644;&#20998;&#31867;&#33041;&#32959;&#30244;&#20026;&#33391;&#24615;&#21644;&#24694;&#24615;&#30340;&#19981;&#21516;&#26550;&#26500;&#12290;&#36890;&#36807;&#23545;&#32593;&#32476;&#36827;&#34892;&#25299;&#23485;&#12289;&#28145;&#21270;&#21644;&#28155;&#21152;&#36339;&#36291;&#36830;&#25509;&#31561;&#20248;&#21270;&#25216;&#26415;&#65292;&#25552;&#39640;&#20102;&#32593;&#32476;&#30340;&#20934;&#30830;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#25216;&#26415;&#30340;&#23376;&#38598;&#21487;&#20197;&#24039;&#22937;&#22320;&#29992;&#20110;&#36229;&#36234;&#29992;&#20110;&#30456;&#21516;&#30446;&#30340;&#30340;&#22522;&#20934;CNN&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present different architectures of Convolutional Neural Networks (CNN) to analyze and classify the brain tumors into benign and malignant types using the Magnetic Resonance Imaging (MRI) technique. Different CNN architecture optimization techniques such as widening and deepening of the network and adding skip connections are applied to improve the accuracy of the network. Results show that a subset of these techniques can judiciously be used to outperform a baseline CNN model used for the same purpose.
&lt;/p&gt;</description></item><item><title>PseudoCal&#26159;&#19968;&#31181;&#26080;&#28304;&#26080;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#20013;&#29992;&#20110;&#26657;&#20934;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;PseudoCal&#19981;&#38656;&#35201;&#28304;&#25968;&#25454;&#19988;&#36866;&#29992;&#20110;&#20005;&#37325;&#22495;&#20559;&#31227;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2307.07489</link><description>&lt;p&gt;
PseudoCal&#65306;&#19968;&#31181;&#26080;&#28304;&#33258;&#36866;&#24212;&#39046;&#22495;&#20013;&#26080;&#30417;&#30563;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
PseudoCal: A Source-Free Approach to Unsupervised Uncertainty Calibration in Domain Adaptation. (arXiv:2307.07489v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07489
&lt;/p&gt;
&lt;p&gt;
PseudoCal&#26159;&#19968;&#31181;&#26080;&#28304;&#26080;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#20013;&#29992;&#20110;&#26657;&#20934;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;PseudoCal&#19981;&#38656;&#35201;&#28304;&#25968;&#25454;&#19988;&#36866;&#29992;&#20110;&#20005;&#37325;&#22495;&#20559;&#31227;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#65288;UDA&#65289;&#22312;&#25552;&#39640;&#26080;&#26631;&#31614;&#30446;&#26631;&#39046;&#22495;&#27169;&#22411;&#20934;&#30830;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#30446;&#26631;&#39046;&#22495;&#20013;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#26657;&#20934;&#65292;&#36825;&#26159;&#23433;&#20840;&#37096;&#32626;UDA&#27169;&#22411;&#30340;&#20851;&#38190;&#65292;&#21364;&#21463;&#21040;&#20102;&#38480;&#21046;&#12290;&#20256;&#32479;&#30340;&#39046;&#22495;&#20869;&#26657;&#20934;&#26041;&#27861;&#8220;&#28201;&#24230;&#32553;&#25918;&#8221;&#65288;TempScal&#65289;&#22312;&#38754;&#23545;&#22495;&#20998;&#24067;&#20559;&#31227;&#21644;&#26080;&#26631;&#35760;&#30446;&#26631;&#22495;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23384;&#22312;&#25361;&#25112;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#37319;&#29992;&#37325;&#35201;&#24615;&#21152;&#26435;&#25216;&#26415;&#26469;&#20272;&#35745;&#22522;&#20110;&#37325;&#26032;&#21152;&#26435;&#26631;&#35760;&#28304;&#25968;&#25454;&#30340;&#30446;&#26631;&#26368;&#20248;&#28201;&#24230;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#28304;&#25968;&#25454;&#65292;&#22312;&#20005;&#37325;&#22495;&#20559;&#31227;&#24773;&#20917;&#19979;&#23494;&#24230;&#20272;&#35745;&#19981;&#21487;&#38752;&#65292;&#19981;&#36866;&#29992;&#20110;&#26080;&#28304;UDA&#35774;&#32622;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PseudoCal&#65292;&#19968;&#31181;&#20165;&#20381;&#36182;&#26080;&#26631;&#31614;&#30446;&#26631;&#25968;&#25454;&#30340;&#26080;&#28304;&#26657;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised domain adaptation (UDA) has witnessed remarkable advancements in improving the accuracy of models for unlabeled target domains. However, the calibration of predictive uncertainty in the target domain, a crucial aspect of the safe deployment of UDA models, has received limited attention. The conventional in-domain calibration method, \textit{temperature scaling} (TempScal), encounters challenges due to domain distribution shifts and the absence of labeled target domain data. Recent approaches have employed importance-weighting techniques to estimate the target-optimal temperature based on re-weighted labeled source data. Nonetheless, these methods require source data and suffer from unreliable density estimates under severe domain shifts, rendering them unsuitable for source-free UDA settings. To overcome these limitations, we propose PseudoCal, a source-free calibration method that exclusively relies on unlabeled target data. Unlike previous approaches that treat UDA calib
&lt;/p&gt;</description></item><item><title>DreamTeacher&#26159;&#19968;&#20010;&#33258;&#30417;&#30563;&#29305;&#24449;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#32593;&#32476;&#39044;&#35757;&#32451;&#22270;&#20687;&#39592;&#24178;&#65292;&#28982;&#21518;&#23558;&#29983;&#25104;&#27169;&#22411;&#30340;&#30693;&#35782;&#33976;&#39311;&#21040;&#30446;&#26631;&#39592;&#24178;&#20013;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2307.07487</link><description>&lt;p&gt;
DreamTeacher: &#20351;&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#23545;&#22270;&#20687;&#39592;&#24178;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#33258;&#30417;&#30563;&#29305;&#24449;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
DreamTeacher: Pretraining Image Backbones with Deep Generative Models. (arXiv:2307.07487v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07487
&lt;/p&gt;
&lt;p&gt;
DreamTeacher&#26159;&#19968;&#20010;&#33258;&#30417;&#30563;&#29305;&#24449;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#32593;&#32476;&#39044;&#35757;&#32451;&#22270;&#20687;&#39592;&#24178;&#65292;&#28982;&#21518;&#23558;&#29983;&#25104;&#27169;&#22411;&#30340;&#30693;&#35782;&#33976;&#39311;&#21040;&#30446;&#26631;&#39592;&#24178;&#20013;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#33258;&#30417;&#30563;&#29305;&#24449;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;DreamTeacher&#65292;&#23427;&#21033;&#29992;&#29983;&#25104;&#32593;&#32476;&#26469;&#39044;&#35757;&#32451;&#19979;&#28216;&#22270;&#20687;&#39592;&#24178;&#12290;&#25105;&#20204;&#25552;&#20986;&#20174;&#35757;&#32451;&#22909;&#30340;&#29983;&#25104;&#27169;&#22411;&#20013;&#25552;&#21462;&#30693;&#35782;&#65292;&#24182;&#23558;&#20854;&#33976;&#39311;&#21040;&#24050;&#32463;&#20026;&#29305;&#23450;&#24863;&#30693;&#20219;&#21153;&#36827;&#34892;&#20102;&#20248;&#21270;&#30340;&#26631;&#20934;&#22270;&#20687;&#39592;&#24178;&#20013;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65306;1&#65289;&#23558;&#23398;&#20064;&#21040;&#30340;&#29983;&#25104;&#29305;&#24449;&#33976;&#39311;&#21040;&#30446;&#26631;&#22270;&#20687;&#39592;&#24178;&#19978;&#65292;&#20316;&#20026;&#39044;&#35757;&#32451;&#36825;&#20123;&#39592;&#24178;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#32780;&#19981;&#26159;&#20351;&#29992;&#22823;&#35268;&#27169;&#26631;&#27880;&#25968;&#25454;&#38598;&#65288;&#22914;ImageNet&#65289;&#36827;&#34892;&#39044;&#35757;&#32451;&#65307;2&#65289;&#23558;&#29983;&#25104;&#32593;&#32476;&#24471;&#21040;&#30340;&#26631;&#31614;&#36890;&#36807;&#20219;&#21153;&#22836;&#33976;&#39311;&#21040;&#30446;&#26631;&#39592;&#24178;&#30340;&#36923;&#36753;&#23618;&#19978;&#12290;&#25105;&#20204;&#23545;&#22810;&#20010;&#29983;&#25104;&#27169;&#22411;&#12289;&#23494;&#38598;&#39044;&#27979;&#22522;&#20934;&#21644;&#20960;&#31181;&#39044;&#35757;&#32451;&#26041;&#27861;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#12290;&#25105;&#20204;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;DreamTeacher&#22312;&#21508;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#33258;&#30417;&#30563;&#29305;&#24449;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#12290;&#20351;&#29992;DreamTeacher&#36827;&#34892;&#26080;&#30417;&#30563;&#30340;ImageNet&#39044;&#35757;&#32451;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we introduce a self-supervised feature representation learning framework DreamTeacher that utilizes generative networks for pre-training downstream image backbones. We propose to distill knowledge from a trained generative model into standard image backbones that have been well engineered for specific perception tasks. We investigate two types of knowledge distillation: 1) distilling learned generative features onto target image backbones as an alternative to pretraining these backbones on large labeled datasets such as ImageNet, and 2) distilling labels obtained from generative networks with task heads onto logits of target backbones. We perform extensive analyses on multiple generative models, dense prediction benchmarks, and several pre-training regimes. We empirically find that our DreamTeacher significantly outperforms existing self-supervised representation learning approaches across the board. Unsupervised ImageNet pre-training with DreamTeacher leads to significan
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39046;&#22495;&#36866;&#24212;&#25216;&#26415;&#25193;&#23637;&#20154;&#21475;&#30340;&#26041;&#27861;&#65292;&#20197;&#21152;&#36895;&#35757;&#32451;&#24182;&#25552;&#39640;&#20351;&#29992;&#36739;&#23569;&#35774;&#22791;&#36827;&#34892;&#35757;&#32451;&#26102;&#30340;&#27169;&#22411;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2307.07477</link><description>&lt;p&gt;
&#20351;&#29992;&#31169;&#20154;&#32852;&#37030;&#23398;&#20064;&#36827;&#34892;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#30340;&#20154;&#21475;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Population Expansion for Training Language Models with Private Federated Learning. (arXiv:2307.07477v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07477
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39046;&#22495;&#36866;&#24212;&#25216;&#26415;&#25193;&#23637;&#20154;&#21475;&#30340;&#26041;&#27861;&#65292;&#20197;&#21152;&#36895;&#35757;&#32451;&#24182;&#25552;&#39640;&#20351;&#29992;&#36739;&#23569;&#35774;&#22791;&#36827;&#34892;&#35757;&#32451;&#26102;&#30340;&#27169;&#22411;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#21512;&#24046;&#20998;&#38544;&#31169;&#30340;&#32852;&#37030;&#23398;&#20064;&#20026;&#20998;&#24067;&#24335;&#35774;&#22791;&#25552;&#20379;&#24102;&#26377;&#27491;&#24335;&#38544;&#31169;&#20445;&#35777;&#30340;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#12290;&#24403;&#35774;&#22791;&#25968;&#37327;&#24222;&#22823;&#26102;&#65292;&#32852;&#37030;&#23398;&#20064;&#19982;&#24046;&#20998;&#38544;&#31169;&#30340;&#32467;&#21512;&#33021;&#22815;&#21450;&#26102;&#29983;&#25104;&#24615;&#33021;&#33391;&#22909;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#35774;&#22791;&#25968;&#37327;&#36739;&#23569;&#30340;&#24212;&#29992;&#65292;&#30001;&#20110;&#24046;&#20998;&#38544;&#31169;&#22122;&#22768;&#19982;&#35774;&#22791;&#25968;&#37327;&#25104;&#21453;&#27604;&#65292;&#27169;&#22411;&#25928;&#29992;&#19979;&#38477;&#65292;&#21516;&#26102;&#30001;&#20110;&#38656;&#35201;&#31561;&#24453;&#26469;&#33258;&#36739;&#23567;&#35774;&#22791;&#27744;&#30340;&#36275;&#22815;&#23458;&#25143;&#31471;&#21487;&#29992;&#65292;&#35757;&#32451;&#24310;&#36831;&#20063;&#22686;&#21152;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#22522;&#20110;&#39046;&#22495;&#36866;&#24212;&#25216;&#26415;&#25193;&#23637;&#20154;&#21475;&#65292;&#20197;&#21152;&#24555;&#35757;&#32451;&#24182;&#25913;&#21892;&#20351;&#29992;&#36739;&#23569;&#35774;&#22791;&#36827;&#34892;&#35757;&#32451;&#26102;&#30340;&#26368;&#32456;&#27169;&#22411;&#36136;&#37327;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#25216;&#26415;&#21487;&#20197;&#20351;&#23454;&#38469;&#35821;&#35328;&#24314;&#27169;&#25968;&#25454;&#38598;&#30340;&#25928;&#29992;&#25552;&#39640;13%&#33267;30%&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) combined with differential privacy (DP) offers machine learning (ML) training with distributed devices and with a formal privacy guarantee. With a large population of devices, FL with DP produces a performant model in a timely manner. However, for applications with a smaller population, not only does the model utility degrade as the DP noise is inversely proportional to population, but also the training latency increases since waiting for enough clients to become available from a smaller pool is slower. In this work, we thus propose expanding the population based on domain adaptation techniques to speed up the training and improves the final model quality when training with small populations. We empirically demonstrate that our techniques can improve the utility by 13% to 30% on real-world language modeling datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#21270;&#35009;&#21098;&#24212;&#29992;&#20110;&#32422;&#26463;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#36807;&#22810;&#23548;&#33268;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.07457</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;&#32467;&#26500;&#21270;&#35009;&#21098;&#29992;&#20110;&#32422;&#26463;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Structured Pruning of Neural Networks for Constraints Learning. (arXiv:2307.07457v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07457
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#21270;&#35009;&#21098;&#24212;&#29992;&#20110;&#32422;&#26463;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#36807;&#22810;&#23548;&#33268;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#19982;&#36816;&#31609;&#23398;&#65288;OR&#65289;&#24037;&#20855;&#30340;&#25972;&#21512;&#26041;&#38754;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#37117;&#21463;&#21040;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#65292;&#21253;&#25324;&#30284;&#30151;&#27835;&#30103;&#12289;&#31639;&#27861;&#37197;&#32622;&#21644;&#21270;&#23398;&#36807;&#31243;&#20248;&#21270;&#12290;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#65292;ML&#21644;OR&#30340;&#32452;&#21512;&#36890;&#24120;&#20381;&#36182;&#20110;&#20351;&#29992;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#65288;MIP&#65289;&#24418;&#24335;&#34920;&#31034;ML&#27169;&#22411;&#36755;&#20986;&#12290;&#25991;&#29486;&#20013;&#30340;&#35768;&#22810;&#30740;&#31350;&#24050;&#32463;&#24320;&#21457;&#20102;&#36825;&#26679;&#30340;&#24418;&#24335;&#21270;&#26041;&#27861;&#26469;&#22788;&#29702;&#35768;&#22810;ML&#39044;&#27979;&#22120;&#65292;&#29305;&#21035;&#26159;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANNs&#65289;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#20855;&#26377;&#37325;&#35201;&#30340;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#32463;&#24120;&#21253;&#21547;&#22823;&#37327;&#21442;&#25968;&#65292;&#23548;&#33268; MIP &#24418;&#24335;&#21270;&#26041;&#27861;&#38590;&#20197;&#35299;&#20915;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#21487;&#25193;&#23637;&#24615;&#12290;&#20107;&#23454;&#19978;&#65292;&#26426;&#22120;&#23398;&#20064;&#30028;&#24050;&#32463;&#24341;&#20837;&#20102;&#20960;&#31181;&#25216;&#26415;&#26469;&#20943;&#23569;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#21442;&#25968;&#25968;&#37327;&#65292;&#32780;&#19981;&#24433;&#21709;&#24615;&#33021;&#65292;&#22240;&#20026;&#29616;&#20195;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#24222;&#22823;&#35268;&#27169;&#23545;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#25552;&#20986;&#20102;&#25361;&#25112;&#65292;&#23427;&#26126;&#26174;&#38477;&#20302;&#20102;&#31995;&#32479;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the integration of Machine Learning (ML) models with Operation Research (OR) tools has gained popularity across diverse applications, including cancer treatment, algorithmic configuration, and chemical process optimization. In this domain, the combination of ML and OR often relies on representing the ML model output using Mixed Integer Programming (MIP) formulations. Numerous studies in the literature have developed such formulations for many ML predictors, with a particular emphasis on Artificial Neural Networks (ANNs) due to their significant interest in many applications. However, ANNs frequently contain a large number of parameters, resulting in MIP formulations that are impractical to solve, thereby impeding scalability. In fact, the ML community has already introduced several techniques to reduce the parameter count of ANNs without compromising their performance, since the substantial size of modern ANNs presents challenges for ML applications as it significantly
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#25968;&#25454;&#31232;&#32570;&#20809;&#35889;&#24212;&#29992;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#26041;&#27861;&#12290;&#36890;&#36807;&#24341;&#20837;Wasserstein GANs&#65288;WGANs&#65289;&#24182;&#23545;&#20854;&#36827;&#34892;&#26465;&#20214;&#21270;&#65292;&#25104;&#21151;&#29983;&#25104;&#20102;&#20934;&#30830;&#30340;&#20809;&#35889;&#25968;&#25454;&#12290;&#36825;&#31181;&#26041;&#27861;&#20026;&#22312;&#31185;&#23398;&#30740;&#31350;&#20013;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2307.07454</link><description>&lt;p&gt;
&#38024;&#23545;&#25968;&#25454;&#31232;&#32570;&#20809;&#35889;&#24212;&#29992;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Generative adversarial networks for data-scarce spectral applications. (arXiv:2307.07454v1 [physics.optics])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07454
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#25968;&#25454;&#31232;&#32570;&#20809;&#35889;&#24212;&#29992;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#26041;&#27861;&#12290;&#36890;&#36807;&#24341;&#20837;Wasserstein GANs&#65288;WGANs&#65289;&#24182;&#23545;&#20854;&#36827;&#34892;&#26465;&#20214;&#21270;&#65292;&#25104;&#21151;&#29983;&#25104;&#20102;&#20934;&#30830;&#30340;&#20809;&#35889;&#25968;&#25454;&#12290;&#36825;&#31181;&#26041;&#27861;&#20026;&#22312;&#31185;&#23398;&#30740;&#31350;&#20013;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#26159;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#26368;&#24378;&#22823;&#19988;&#22810;&#21151;&#33021;&#30340;&#25216;&#26415;&#20043;&#19968;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25253;&#21578;&#20102;GANs&#22312;&#21512;&#25104;&#20809;&#35889;&#25968;&#25454;&#29983;&#25104;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#20026;&#35299;&#20915;&#22312;&#21508;&#31181;&#31185;&#23398;&#32972;&#26223;&#19979;&#25968;&#25454;&#31232;&#32570;&#30340;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#20854;&#24212;&#29992;&#20110;&#36817;&#22330;&#36752;&#23556;&#20256;&#28909;&#39046;&#22495;&#20013;&#30340;&#19968;&#20010;&#22810;&#23618;&#27425;&#21452;&#26354;&#20171;&#36136;&#38382;&#39064;&#26469;&#23637;&#31034;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#25104;&#21151;&#29983;&#25104;&#20809;&#35889;&#25968;&#25454;&#38656;&#35201;&#23545;&#20256;&#32479;GANs&#36827;&#34892;&#20004;&#20010;&#20462;&#25913;&#65306;&#65288;i&#65289;&#24341;&#20837;Wasserstein GANs&#65288;WGANs&#65289;&#20197;&#36991;&#20813;&#27169;&#24335;&#23849;&#28291;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#23545;WGANs&#36827;&#34892;&#26465;&#20214;&#21270;&#20197;&#33719;&#24471;&#29983;&#25104;&#25968;&#25454;&#30340;&#20934;&#30830;&#26631;&#31614;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#65288;FFNN&#65289;&#65292;&#22312;&#26377;&#38480;&#25968;&#25454;&#26465;&#20214;&#19979;&#65292;&#36890;&#36807;&#20351;&#29992;CWGAN&#29983;&#25104;&#30340;&#25968;&#25454;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#20854;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#20854;&#20869;&#22312;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative adversarial networks (GANs) are one of the most robust and versatile techniques in the field of generative artificial intelligence. In this work, we report on an application of GANs in the domain of synthetic spectral data generation, offering a solution to the scarcity of data found in various scientific contexts. We demonstrate the proposed approach by applying it to an illustrative problem within the realm of near-field radiative heat transfer involving a multilayered hyperbolic metamaterial. We find that a successful generation of spectral data requires two modifications to conventional GANs: (i) the introduction of Wasserstein GANs (WGANs) to avoid mode collapse, and, (ii) the conditioning of WGANs to obtain accurate labels for the generated data. We show that a simple feed-forward neural network (FFNN), when augmented with data generated by a CWGAN, enhances significantly its performance under conditions of limited data availability, demonstrating the intrinsic value o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#39318;&#20010;&#38024;&#23545;$k$-means&#21644;$k$-median&#32858;&#31867;&#30340;&#24046;&#20998;&#38544;&#31169;&#27969;&#31639;&#27861;&#65292;&#22312;&#27969;&#27169;&#22411;&#20013;&#23454;&#29616;&#23545;&#25968;&#25454;&#38544;&#31169;&#30340;&#20445;&#25252;&#65292;&#24182;&#20351;&#29992;&#23613;&#21487;&#33021;&#23569;&#30340;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2307.07449</link><description>&lt;p&gt;
&#25968;&#25454;&#27969;&#20013;&#30340;&#24046;&#20998;&#38544;&#31169;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Clustering in Data Streams. (arXiv:2307.07449v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07449
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#39318;&#20010;&#38024;&#23545;$k$-means&#21644;$k$-median&#32858;&#31867;&#30340;&#24046;&#20998;&#38544;&#31169;&#27969;&#31639;&#27861;&#65292;&#22312;&#27969;&#27169;&#22411;&#20013;&#23454;&#29616;&#23545;&#25968;&#25454;&#38544;&#31169;&#30340;&#20445;&#25252;&#65292;&#24182;&#20351;&#29992;&#23613;&#21487;&#33021;&#23569;&#30340;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#27169;&#22411;&#26159;&#22788;&#29702;&#22823;&#35268;&#27169;&#29616;&#20195;&#25968;&#25454;&#20998;&#26512;&#30340;&#19968;&#31181;&#24120;&#35265;&#26041;&#27861;&#12290;&#22312;&#27969;&#27169;&#22411;&#20013;&#65292;&#25968;&#25454;&#28857;&#20381;&#27425;&#27969;&#20837;&#65292;&#31639;&#27861;&#21482;&#33021;&#23545;&#25968;&#25454;&#27969;&#36827;&#34892;&#19968;&#27425;&#36941;&#21382;&#65292;&#30446;&#26631;&#26159;&#22312;&#20351;&#29992;&#23613;&#21487;&#33021;&#23569;&#30340;&#31354;&#38388;&#30340;&#21516;&#26102;&#65292;&#22312;&#27969;&#20013;&#36827;&#34892;&#19968;&#20123;&#20998;&#26512;&#12290;&#32858;&#31867;&#38382;&#39064;&#26159;&#22522;&#26412;&#30340;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#21407;&#35821;&#65292;&#36807;&#21435;&#24050;&#32463;&#23545;&#27969;&#32858;&#31867;&#31639;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#25968;&#25454;&#38544;&#31169;&#24050;&#25104;&#20026;&#19968;&#20010;&#26680;&#24515;&#20851;&#27880;&#28857;&#65292;&#38750;&#31169;&#26377;&#32858;&#31867;&#31639;&#27861;&#22312;&#35768;&#22810;&#22330;&#26223;&#19979;&#19981;&#36866;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#38024;&#23545;$k$-means&#21644;$k$-median&#32858;&#31867;&#30340;&#24046;&#20998;&#31169;&#26377;&#27969;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#38271;&#24230;&#26368;&#22810;&#20026;$T$&#30340;&#27969;&#19978;&#20351;&#29992;$poly(k,d,\log(T))$&#30340;&#31354;&#38388;&#26469;&#23454;&#29616;&#19968;&#20010;&#8220;&#24120;&#25968;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
The streaming model is an abstraction of computing over massive data streams, which is a popular way of dealing with large-scale modern data analysis. In this model, there is a stream of data points, one after the other. A streaming algorithm is only allowed one pass over the data stream, and the goal is to perform some analysis during the stream while using as small space as possible.  Clustering problems (such as $k$-means and $k$-median) are fundamental unsupervised machine learning primitives, and streaming clustering algorithms have been extensively studied in the past. However, since data privacy becomes a central concern in many real-world applications, non-private clustering algorithms are not applicable in many scenarios.  In this work, we provide the first differentially private streaming algorithms for $k$-means and $k$-median clustering of $d$-dimensional Euclidean data points over a stream with length at most $T$ using $poly(k,d,\log(T))$ space to achieve a {\it constant} 
&lt;/p&gt;</description></item><item><title>TSNet-SAC&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#21019;&#26032;&#32593;&#32476;&#65292;&#21033;&#29992;&#21551;&#21457;&#24335;&#31639;&#27861;&#25351;&#23548;&#35757;&#32451;&#65292;&#36890;&#36807;&#24341;&#20837;&#28369;&#21160;&#22686;&#24378;&#32452;&#20214;&#21644;&#25193;&#23637;&#32452;&#20214;&#26469;&#25552;&#39640;&#40065;&#26834;&#24615;&#21644;&#35299;&#20915;&#31639;&#27861;&#32570;&#38519;&#65292;&#36866;&#24212;&#19981;&#21516;&#30340;&#25509;&#20837;&#22330;&#26223;&#12290;&#27169;&#25311;&#23454;&#39564;&#34920;&#26126;&#65292;TSNet-SAC&#22312;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#32593;&#32476;&#65292;&#24182;&#20855;&#26377;&#26356;&#20248;&#30340;&#35843;&#24230;&#20915;&#31574;&#24310;&#36831;&#12290;</title><link>http://arxiv.org/abs/2307.07445</link><description>&lt;p&gt;
TSNet-SAC: &#21033;&#29992;Transformer&#36827;&#34892;&#39640;&#25928;&#20219;&#21153;&#35843;&#24230;
&lt;/p&gt;
&lt;p&gt;
TSNet-SAC: Leveraging Transformers for Efficient Task Scheduling. (arXiv:2307.07445v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07445
&lt;/p&gt;
&lt;p&gt;
TSNet-SAC&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#21019;&#26032;&#32593;&#32476;&#65292;&#21033;&#29992;&#21551;&#21457;&#24335;&#31639;&#27861;&#25351;&#23548;&#35757;&#32451;&#65292;&#36890;&#36807;&#24341;&#20837;&#28369;&#21160;&#22686;&#24378;&#32452;&#20214;&#21644;&#25193;&#23637;&#32452;&#20214;&#26469;&#25552;&#39640;&#40065;&#26834;&#24615;&#21644;&#35299;&#20915;&#31639;&#27861;&#32570;&#38519;&#65292;&#36866;&#24212;&#19981;&#21516;&#30340;&#25509;&#20837;&#22330;&#26223;&#12290;&#27169;&#25311;&#23454;&#39564;&#34920;&#26126;&#65292;TSNet-SAC&#22312;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#32593;&#32476;&#65292;&#24182;&#20855;&#26377;&#26356;&#20248;&#30340;&#35843;&#24230;&#20915;&#31574;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26410;&#26469;&#30340;6G&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#65288;MEC&#65289;&#20013;&#65292;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#38656;&#35201;&#20855;&#22791;&#22788;&#29702;&#22810;&#27169;&#24577;&#25968;&#25454;&#21644;&#24378;&#30456;&#20114;&#20381;&#36182;&#24615;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#21551;&#21457;&#24335;&#31639;&#27861;&#30001;&#20110;&#38656;&#35201;&#22810;&#27425;&#36845;&#20195;&#26469;&#24471;&#21040;&#26368;&#20248;&#26041;&#26696;&#65292;&#22312;&#23454;&#26102;&#35843;&#24230;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#21019;&#26032;&#32593;&#32476;TSNet-SAC&#65292;&#20165;&#21033;&#29992;&#21551;&#21457;&#24335;&#31639;&#27861;&#25351;&#23548;TSNet&#30340;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#28369;&#21160;&#22686;&#24378;&#32452;&#20214;&#65288;SAC&#65289;&#26469;&#22686;&#24378;&#40065;&#26834;&#24615;&#21644;&#35299;&#20915;&#31639;&#27861;&#32570;&#38519;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#25193;&#23637;&#32452;&#20214;&#26469;&#22788;&#29702;&#22810;&#23610;&#24230;&#35757;&#32451;&#25968;&#25454;&#24182;&#25552;&#20379;&#32593;&#32476;&#21487;&#25193;&#23637;&#24615;&#65292;&#20351;&#24471;TSNet&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#30340;&#25509;&#20837;&#22330;&#26223;&#12290;&#27169;&#25311;&#23454;&#39564;&#34920;&#26126;&#65292;TSNet-SAC&#22312;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#32593;&#32476;&#65292;&#19982;&#21551;&#21457;&#24335;&#31639;&#27861;&#30456;&#27604;&#20855;&#26377;&#26356;&#20248;&#30340;&#35843;&#24230;&#20915;&#31574;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;
In future 6G Mobile Edge Computing (MEC), autopilot systems require the capability of processing multimodal data with strong interdependencies. However, traditional heuristic algorithms are inadequate for real-time scheduling due to their requirement for multiple iterations to derive the optimal scheme. We propose a novel TSNet-SAC based on Transformer, that utilizes heuristic algorithms solely to guide the training of TSNet. Additionally, a Sliding Augment Component (SAC) is introduced to enhance the robustness and resolve algorithm defects. Furthermore, the Extender component is designed to handle multi-scale training data and provide network scalability, enabling TSNet to adapt to different access scenarios. Simulation demonstrates that TSNet-SAC outperforms existing networks in accuracy and robustness, achieving superior scheduling-making latency compared to heuristic algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#36827;&#34892;&#22686;&#24378;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38646;/&#23569;&#26679;&#26412;&#20998;&#23376;&#20998;&#31867;&#21644;&#29983;&#25104;&#26032;&#30340;&#35299;&#37322;&#26469;&#25512;&#36827;&#36825;&#19968;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2307.07443</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22686;&#24378;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models Empower Molecular Property Prediction?. (arXiv:2307.07443v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07443
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#36827;&#34892;&#22686;&#24378;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38646;/&#23569;&#26679;&#26412;&#20998;&#23376;&#20998;&#31867;&#21644;&#29983;&#25104;&#26032;&#30340;&#35299;&#37322;&#26469;&#25512;&#36827;&#36825;&#19968;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#22240;&#20854;&#22312;&#22810;&#20010;&#31185;&#23398;&#39046;&#22495;&#20013;&#30340;&#25913;&#21464;&#28508;&#21147;&#32780;&#21463;&#21040;&#37325;&#35270;&#12290;&#20256;&#32479;&#19978;&#65292;&#20998;&#23376;&#22270;&#21487;&#20197;&#34920;&#31034;&#20026;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#25110;SMILES&#25991;&#26412;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#12290;&#23613;&#31649;&#21033;&#29992;LLMs&#26469;&#29702;&#35299;&#29992;SMILES&#34920;&#31034;&#30340;&#20998;&#23376;&#26159;&#33258;&#28982;&#30340;&#65292;&#20294;LLMs&#22914;&#20309;&#24433;&#21709;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#30340;&#25506;&#32034;&#20173;&#22788;&#20110;&#26089;&#26399;&#38454;&#27573;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20174;&#38646;/&#23569;&#26679;&#26412;&#20998;&#23376;&#20998;&#31867;&#21644;&#20351;&#29992;LLMs&#29983;&#25104;&#30340;&#26032;&#35299;&#37322;&#20316;&#20026;&#20998;&#23376;&#34920;&#31034;&#20004;&#20010;&#35282;&#24230;&#25512;&#36827;&#20102;&#36825;&#19968;&#30446;&#26631;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#31034;LLMs&#36827;&#34892;&#19978;&#19979;&#25991;&#20998;&#23376;&#20998;&#31867;&#24182;&#35780;&#20272;&#20854;&#24615;&#33021;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;LLMs&#20026;&#21407;&#22987;SMILES&#29983;&#25104;&#35821;&#20041;&#20016;&#23500;&#30340;&#35299;&#37322;&#65292;&#24182;&#21033;&#29992;&#27492;&#26469;&#24494;&#35843;&#23567;&#35268;&#27169;&#30340;LM&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Molecular property prediction has gained significant attention due to its transformative potential in multiple scientific disciplines. Conventionally, a molecule graph can be represented either as a graph-structured data or a SMILES text. Recently, the rapid development of Large Language Models (LLMs) has revolutionized the field of NLP. Although it is natural to utilize LLMs to assist in understanding molecules represented by SMILES, the exploration of how LLMs will impact molecular property prediction is still in its early stage. In this work, we advance towards this objective through two perspectives: zero/few-shot molecular classification, and using the new explanations generated by LLMs as representations of molecules. To be specific, we first prompt LLMs to do in-context molecular classification and evaluate their performance. After that, we employ LLMs to generate semantically enriched explanations for the original SMILES and then leverage that to fine-tune a small-scale LM mode
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#35889;&#30340;&#21487;&#35299;&#37322;&#24180;&#40836;&#39044;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#20840;&#36523;&#22270;&#20687;&#30740;&#31350;&#20102;&#21508;&#20010;&#36523;&#20307;&#37096;&#20301;&#30340;&#24180;&#40836;&#30456;&#20851;&#21464;&#21270;&#12290;&#36890;&#36807;&#20351;&#29992;&#35299;&#37322;&#24615;&#26041;&#27861;&#21644;&#37197;&#20934;&#25216;&#26415;&#65292;&#30830;&#23450;&#20102;&#26368;&#33021;&#39044;&#27979;&#24180;&#40836;&#30340;&#36523;&#20307;&#21306;&#22495;&#65292;&#24182;&#21019;&#19979;&#20102;&#25972;&#20010;&#36523;&#20307;&#24180;&#40836;&#39044;&#27979;&#30340;&#26368;&#26032;&#27700;&#24179;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#33034;&#26609;&#12289;&#26412;&#21407;&#24615;&#32972;&#37096;&#32908;&#32905;&#21644;&#24515;&#33039;&#21306;&#22495;&#26159;&#26368;&#37325;&#35201;&#30340;&#20851;&#27880;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2307.07439</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#35889;&#30340;&#21487;&#35299;&#37322;&#24180;&#40836;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Atlas-Based Interpretable Age Prediction. (arXiv:2307.07439v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07439
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#35889;&#30340;&#21487;&#35299;&#37322;&#24180;&#40836;&#39044;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#20840;&#36523;&#22270;&#20687;&#30740;&#31350;&#20102;&#21508;&#20010;&#36523;&#20307;&#37096;&#20301;&#30340;&#24180;&#40836;&#30456;&#20851;&#21464;&#21270;&#12290;&#36890;&#36807;&#20351;&#29992;&#35299;&#37322;&#24615;&#26041;&#27861;&#21644;&#37197;&#20934;&#25216;&#26415;&#65292;&#30830;&#23450;&#20102;&#26368;&#33021;&#39044;&#27979;&#24180;&#40836;&#30340;&#36523;&#20307;&#21306;&#22495;&#65292;&#24182;&#21019;&#19979;&#20102;&#25972;&#20010;&#36523;&#20307;&#24180;&#40836;&#39044;&#27979;&#30340;&#26368;&#26032;&#27700;&#24179;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#33034;&#26609;&#12289;&#26412;&#21407;&#24615;&#32972;&#37096;&#32908;&#32905;&#21644;&#24515;&#33039;&#21306;&#22495;&#26159;&#26368;&#37325;&#35201;&#30340;&#20851;&#27880;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24180;&#40836;&#39044;&#27979;&#26159;&#21307;&#23398;&#35780;&#20272;&#21644;&#30740;&#31350;&#30340;&#37325;&#35201;&#37096;&#20998;&#65292;&#21487;&#20197;&#36890;&#36807;&#31361;&#20986;&#23454;&#38469;&#24180;&#40836;&#21644;&#29983;&#29289;&#24180;&#40836;&#20043;&#38388;&#30340;&#24046;&#24322;&#26469;&#24110;&#21161;&#26816;&#27979;&#30142;&#30149;&#21644;&#24322;&#24120;&#34928;&#32769;&#12290;&#20026;&#20102;&#20840;&#38754;&#20102;&#35299;&#21508;&#20010;&#36523;&#20307;&#37096;&#20301;&#30340;&#24180;&#40836;&#30456;&#20851;&#21464;&#21270;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#20840;&#36523;&#22270;&#20687;&#36827;&#34892;&#30740;&#31350;&#12290;&#25105;&#20204;&#21033;&#29992;Grad-CAM&#35299;&#37322;&#24615;&#26041;&#27861;&#30830;&#23450;&#26368;&#33021;&#39044;&#27979;&#19968;&#20010;&#20154;&#24180;&#40836;&#30340;&#36523;&#20307;&#21306;&#22495;&#12290;&#36890;&#36807;&#20351;&#29992;&#37197;&#20934;&#25216;&#26415;&#29983;&#25104;&#25972;&#20010;&#20154;&#32676;&#30340;&#35299;&#37322;&#24615;&#22270;&#65292;&#25105;&#20204;&#23558;&#20998;&#26512;&#25193;&#23637;&#21040;&#20010;&#20307;&#20043;&#22806;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20197;&#19968;&#20010;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#20026;2.76&#24180;&#30340;&#27169;&#22411;&#65292;&#21019;&#19979;&#20102;&#25972;&#20010;&#36523;&#20307;&#24180;&#40836;&#39044;&#27979;&#30340;&#26368;&#26032;&#27700;&#24179;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#19977;&#20010;&#20027;&#35201;&#30340;&#20851;&#27880;&#39046;&#22495;&#65306;&#33034;&#26609;&#12289;&#26412;&#21407;&#24615;&#32972;&#37096;&#32908;&#32905;&#21644;&#24515;&#33039;&#21306;&#22495;&#65292;&#20854;&#20013;&#24515;&#33039;&#21306;&#22495;&#20855;&#26377;&#26368;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Age prediction is an important part of medical assessments and research. It can aid in detecting diseases as well as abnormal ageing by highlighting the discrepancy between chronological and biological age. To gain a comprehensive understanding of age-related changes observed in various body parts, we investigate them on a larger scale by using whole-body images. We utilise the Grad-CAM interpretability method to determine the body areas most predictive of a person's age. We expand our analysis beyond individual subjects by employing registration techniques to generate population-wide interpretability maps. Furthermore, we set state-of-the-art whole-body age prediction with a model that achieves a mean absolute error of 2.76 years. Our findings reveal three primary areas of interest: the spine, the autochthonous back muscles, and the cardiac region, which exhibits the highest importance.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#38024;&#23545;&#22686;&#24378;&#20987;&#24358;&#25351;&#24377;&#30340;&#23454;&#26102;&#38899;&#20048;&#20449;&#24687;&#26816;&#32034;&#25216;&#26415;&#65292;&#36890;&#36807;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#23454;&#29616;&#20102;&#21513;&#20182;&#20307;&#20987;&#25171;&#30340;&#35782;&#21035;&#21644;&#23884;&#20837;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2307.07426</link><description>&lt;p&gt;
&#22768;&#23398;&#21513;&#20182;&#30340;&#23454;&#26102;&#25970;&#20987;&#25216;&#26415;&#35782;&#21035;&#21644;&#23884;&#20837;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Real-time Percussive Technique Recognition and Embedding Learning for the Acoustic Guitar. (arXiv:2307.07426v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07426
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#38024;&#23545;&#22686;&#24378;&#20987;&#24358;&#25351;&#24377;&#30340;&#23454;&#26102;&#38899;&#20048;&#20449;&#24687;&#26816;&#32034;&#25216;&#26415;&#65292;&#36890;&#36807;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#23454;&#29616;&#20102;&#21513;&#20182;&#20307;&#20987;&#25171;&#30340;&#35782;&#21035;&#21644;&#23884;&#20837;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#26102;&#38899;&#20048;&#20449;&#24687;&#26816;&#32034;&#65288;RT-MIR&#65289;&#22312;&#22686;&#24378;&#20256;&#32479;&#22768;&#23398;&#20048;&#22120;&#30340;&#33021;&#21147;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#38024;&#23545;&#22686;&#24378;&#20987;&#24358;&#25351;&#24377;&#30340;&#23454;&#26102;&#38899;&#20048;&#20449;&#24687;&#26816;&#32034;&#25216;&#26415;&#65292;&#23558;&#22768;&#23398;&#21513;&#20182;&#28436;&#22863;&#19982;&#21513;&#20182;&#20307;&#20987;&#25171;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#20026;&#22686;&#24378;&#20048;&#22120;&#34920;&#28436;&#30340;&#23454;&#26102;&#38899;&#20048;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#21046;&#23450;&#20102;&#20960;&#20010;&#35774;&#35745;&#30446;&#26631;&#65306;&#65288;i&#65289;&#22240;&#26524;&#32422;&#26463;&#65292;&#65288;ii&#65289;&#24863;&#30693;&#19978;&#21487;&#24573;&#30053;&#30340;&#38899;&#21160;&#24310;&#36831;&#65292;&#65288;iii&#65289;&#25511;&#21046;&#20146;&#23494;&#24615;&#25903;&#25345;&#65292;&#65288;iv&#65289;&#21512;&#25104;&#25511;&#21046;&#25903;&#25345;&#12290;&#25105;&#20204;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#21644;&#19982;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#20849;&#21516;&#35757;&#32451;&#30340;CNNs&#65292;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#23454;&#26102;&#21513;&#20182;&#20307;&#20987;&#25171;&#35782;&#21035;&#21644;&#23884;&#20837;&#23398;&#20064;&#25216;&#26415;&#12290;&#25105;&#20204;&#22522;&#20110;&#25163;&#37096;&#37096;&#20301;&#21644;&#20301;&#32622;&#65292;&#24341;&#20837;&#20102;&#21513;&#20182;&#20307;&#20987;&#25171;&#30340;&#20998;&#31867;&#31995;&#32479;&#12290;&#36890;&#36807;&#25910;&#38598;&#24182;&#26681;&#25454;&#20998;&#31867;&#31995;&#32479;&#26631;&#35760;&#30340;&#19977;&#20010;&#25968;&#25454;&#38598;&#65292;&#37319;&#29992;&#36328;&#25968;&#25454;&#38598;&#35780;&#20272;&#26041;&#27861;&#35780;&#20272;&#20102;&#27169;&#22411;&#30340;&#23884;&#20837;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-time music information retrieval (RT-MIR) has much potential to augment the capabilities of traditional acoustic instruments. We develop RT-MIR techniques aimed at augmenting percussive fingerstyle, which blends acoustic guitar playing with guitar body percussion. We formulate several design objectives for RT-MIR systems for augmented instrument performance: (i) causal constraint, (ii) perceptually negligible action-to-sound latency, (iii) control intimacy support, (iv) synthesis control support. We present and evaluate real-time guitar body percussion recognition and embedding learning techniques based on convolutional neural networks (CNNs) and CNNs jointly trained with variational autoencoders (VAEs). We introduce a taxonomy of guitar body percussion based on hand part and location. We follow a cross-dataset evaluation approach by collecting three datasets labelled according to the taxonomy. The embedding quality of the models is assessed using KL-Divergence across distribution
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#27969;&#27700;&#32447;&#65292;&#29992;&#20110;&#22810;&#26631;&#31614;&#20998;&#31867;&#30340;&#26893;&#20837;&#24335;&#24515;&#33039;&#30417;&#27979;&#22120;&#25968;&#25454;&#30340;&#24515;&#30005;&#22270;&#20998;&#26512;&#65292;&#20197;&#20943;&#36731;&#21307;&#25252;&#20154;&#21592;&#22240;&#19981;&#26029;&#22686;&#38271;&#30340;&#25968;&#25454;&#36127;&#33655;&#32780;&#38754;&#20020;&#30340;&#21387;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.07423</link><description>&lt;p&gt;
&#25552;&#21319;&#26893;&#20837;&#24335;&#24515;&#33039;&#30417;&#27979;&#22120;&#25968;&#25454;&#30340;&#24515;&#30005;&#22270;&#20998;&#26512;&#65306;&#22810;&#26631;&#31614;&#20998;&#31867;&#30340;&#39640;&#25928;&#27969;&#27700;&#32447;
&lt;/p&gt;
&lt;p&gt;
Enhancing ECG Analysis of Implantable Cardiac Monitor Data: An Efficient Pipeline for Multi-Label Classification. (arXiv:2307.07423v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07423
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#27969;&#27700;&#32447;&#65292;&#29992;&#20110;&#22810;&#26631;&#31614;&#20998;&#31867;&#30340;&#26893;&#20837;&#24335;&#24515;&#33039;&#30417;&#27979;&#22120;&#25968;&#25454;&#30340;&#24515;&#30005;&#22270;&#20998;&#26512;&#65292;&#20197;&#20943;&#36731;&#21307;&#25252;&#20154;&#21592;&#22240;&#19981;&#26029;&#22686;&#38271;&#30340;&#25968;&#25454;&#36127;&#33655;&#32780;&#38754;&#20020;&#30340;&#21387;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#24403;&#20170;&#22686;&#38271;&#26368;&#24555;&#30340;&#21487;&#26893;&#20837;&#24515;&#33039;&#35774;&#22791;&#24066;&#22330;&#65292;&#26893;&#20837;&#24335;&#24515;&#33039;&#30417;&#27979;&#22120;&#65288;ICM&#65289;&#35774;&#22791;&#22312;&#30149;&#20154;&#20013;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#29992;&#20110;&#27979;&#37327;&#24515;&#33039;&#30005;&#27963;&#21160;&#12290;ICM&#25345;&#32493;&#30417;&#27979;&#21644;&#35760;&#24405;&#30149;&#20154;&#30340;&#24515;&#24459;&#65292;&#24182;&#22312;&#35302;&#21457;&#26102;&#23558;&#25968;&#25454;&#21457;&#36865;&#21040;&#23433;&#20840;&#26381;&#21153;&#22120;&#65292;&#21307;&#30103;&#19987;&#19994;&#20154;&#21592;&#21487;&#20197;&#36827;&#34892;&#23457;&#26597;&#12290;&#20026;&#20102;&#19981;&#28431;&#35786;&#65292;&#36825;&#20123;&#35774;&#22791;&#37319;&#29992;&#20102;&#30456;&#23545;&#31616;&#21333;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#31639;&#27861;&#65288;&#30001;&#20110;&#33021;&#32791;&#32422;&#26463;&#65289;&#12290;&#30001;&#20110;&#20854;&#19981;&#26029;&#30417;&#27979;&#24515;&#33039;&#33410;&#24459;&#21644;&#26085;&#30410;&#26222;&#21450;&#30340;&#29305;&#28857;&#65292;&#35813;&#31639;&#27861;&#36890;&#24120;&#34987;&#21442;&#25968;&#21270;&#20026;&#36807;&#20110;&#25935;&#24863;&#30340;&#27169;&#24335;&#65292;&#23548;&#33268;&#30456;&#23545;&#36739;&#39640;&#30340;&#35823;&#25253;&#29575;&#65292;&#36825;&#20351;&#24471;&#21307;&#25252;&#20154;&#21592;&#38656;&#35201;&#20998;&#26512;&#21644;&#35786;&#26029;&#36234;&#26469;&#36234;&#22810;&#30340;&#25968;&#25454;&#12290;&#20026;&#20102;&#20943;&#36731;&#21307;&#25252;&#20154;&#21592;&#30340;&#36127;&#25285;&#65292;&#29616;&#22312;&#24050;&#32463;&#20986;&#29616;&#20102;&#33258;&#21160;&#21270;&#30340;&#24515;&#30005;&#22270;&#20998;&#26512;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Implantable Cardiac Monitor (ICM) devices are demonstrating as of today, the fastest-growing market for implantable cardiac devices. As such, they are becoming increasingly common in patients for measuring heart electrical activity. ICMs constantly monitor and record a patient's heart rhythm and when triggered - send it to a secure server where health care professionals (denote HCPs from here on) can review it. These devices employ a relatively simplistic rule-based algorithm (due to energy consumption constraints) to alert for abnormal heart rhythms. This algorithm is usually parameterized to an over-sensitive mode in order to not miss a case (resulting in relatively high false-positive rate) and this, combined with the device's nature of constantly monitoring the heart rhythm and its growing popularity, results in HCPs having to analyze and diagnose an increasingly growing amount of data. In order to reduce the load on the latter, automated methods for ECG analysis are nowadays becom
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#26032;&#30340;&#38382;&#39064;&#65292;&#21363;&#24102;&#26377;&#37096;&#20998;&#26631;&#31614;&#30340;&#20027;&#21160;&#23398;&#20064;&#65288;ALPL&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#36807;&#24230;&#25311;&#21512;&#38382;&#39064;&#21644;&#22686;&#24378;&#36873;&#25321;&#20195;&#34920;&#24615;&#26679;&#26412;&#30340;&#36807;&#31243;&#65292;&#25105;&#20204;&#21033;&#29992;&#21453;&#20363;&#26500;&#36896;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340; WorseNet &#26469;&#36827;&#34892;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2307.07413</link><description>&lt;p&gt;
&#21033;&#29992;&#21453;&#20363;&#23545;&#24102;&#26377;&#37096;&#20998;&#26631;&#31614;&#30340;&#20027;&#21160;&#23398;&#20064;&#36827;&#34892;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Exploiting Counter-Examples for Active Learning with Partial labels. (arXiv:2307.07413v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07413
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#26032;&#30340;&#38382;&#39064;&#65292;&#21363;&#24102;&#26377;&#37096;&#20998;&#26631;&#31614;&#30340;&#20027;&#21160;&#23398;&#20064;&#65288;ALPL&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#36807;&#24230;&#25311;&#21512;&#38382;&#39064;&#21644;&#22686;&#24378;&#36873;&#25321;&#20195;&#34920;&#24615;&#26679;&#26412;&#30340;&#36807;&#31243;&#65292;&#25105;&#20204;&#21033;&#29992;&#21453;&#20363;&#26500;&#36896;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340; WorseNet &#26469;&#36827;&#34892;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#26032;&#30340;&#38382;&#39064;&#65292;&#21363;&#24102;&#26377;&#37096;&#20998;&#26631;&#31614;&#30340;&#20027;&#21160;&#23398;&#20064;&#65288;ALPL&#65289;&#12290;&#22312;&#36825;&#31181;&#35774;&#32622;&#20013;&#65292;&#19968;&#20010; oracle &#29992;&#37096;&#20998;&#26631;&#31614;&#23545;&#26597;&#35810;&#26679;&#26412;&#36827;&#34892;&#27880;&#37322;&#65292;&#25918;&#23485;&#20102;&#23545;&#20934;&#30830;&#26631;&#27880;&#36807;&#31243;&#30340;&#35201;&#27714;&#12290;&#20026;&#20102;&#35299;&#20915; ALPL&#65292;&#25105;&#20204;&#39318;&#20808;&#24314;&#31435;&#20102;&#19968;&#20010;&#30452;&#35266;&#30340;&#22522;&#32447;&#65292;&#21487;&#20197;&#26080;&#32541;&#22320;&#34701;&#20837;&#21040;&#29616;&#26377;&#30340; AL &#26694;&#26550;&#20013;&#12290;&#34429;&#28982;&#26377;&#25928;&#65292;&#20294;&#36825;&#20010;&#22522;&#32447;&#20173;&#28982;&#23481;&#26131;&#36807;&#24230;&#25311;&#21512;&#65292;&#24182;&#19988;&#22312;&#26597;&#35810;&#36807;&#31243;&#20013;&#32570;&#20047;&#20195;&#34920;&#24615;&#30340;&#22522;&#20110;&#37096;&#20998;&#26631;&#31614;&#30340;&#26679;&#26412;&#12290;&#21463;&#35748;&#30693;&#31185;&#23398;&#20013;&#20154;&#31867;&#25512;&#29702;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#21033;&#29992;&#36825;&#31181;&#31867;&#20284;&#20154;&#31867;&#30340;&#23398;&#20064;&#27169;&#24335;&#26469;&#35299;&#20915;&#36807;&#24230;&#25311;&#21512;&#38382;&#39064;&#65292;&#21516;&#26102;&#22686;&#24378; ALPL &#20013;&#36873;&#25321;&#20195;&#34920;&#24615;&#26679;&#26412;&#30340;&#36807;&#31243;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#21453;&#36716;&#27599;&#20010;&#23454;&#20363;&#30340;&#37096;&#20998;&#26631;&#31614;&#26500;&#36896;&#21453;&#20363;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340; WorseNet &#26469;&#30452;&#25509;&#20174;&#36825;&#20123;&#21453;&#20363;&#20013;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies a new problem, \emph{active learning with partial labels} (ALPL). In this setting, an oracle annotates the query samples with partial labels, relaxing the oracle from the demanding accurate labeling process. To address ALPL, we first build an intuitive baseline that can be seamlessly incorporated into existing AL frameworks. Though effective, this baseline is still susceptible to the \emph{overfitting}, and falls short of the representative partial-label-based samples during the query process. Drawing inspiration from human inference in cognitive science, where accurate inferences can be explicitly derived from \emph{counter-examples} (CEs), our objective is to leverage this human-like learning pattern to tackle the \emph{overfitting} while enhancing the process of selecting representative samples in ALPL. Specifically, we construct CEs by reversing the partial labels for each instance, and then we propose a simple but effective WorseNet to directly learn from this c
&lt;/p&gt;</description></item><item><title>&#35838;&#31243;&#23398;&#20064;&#26694;&#26550;HuCurl&#33021;&#22815;&#26681;&#25454;&#20808;&#21069;&#23545;&#26679;&#26412;&#38590;&#24230;&#30340;&#20102;&#35299;&#65292;&#21457;&#29616;&#38750;&#21333;&#35843;&#30340;&#26377;&#25928;&#35838;&#31243;&#65292;&#24182;&#22312;&#22810;&#20010;NLP&#20219;&#21153;&#20013;&#32988;&#36807;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.07412</link><description>&lt;p&gt;
HuCurl: &#20154;&#31867;&#24341;&#23548;&#35838;&#31243;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
HuCurl: Human-induced Curriculum Discovery. (arXiv:2307.07412v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07412
&lt;/p&gt;
&lt;p&gt;
&#35838;&#31243;&#23398;&#20064;&#26694;&#26550;HuCurl&#33021;&#22815;&#26681;&#25454;&#20808;&#21069;&#23545;&#26679;&#26412;&#38590;&#24230;&#30340;&#20102;&#35299;&#65292;&#21457;&#29616;&#38750;&#21333;&#35843;&#30340;&#26377;&#25928;&#35838;&#31243;&#65292;&#24182;&#22312;&#22810;&#20010;NLP&#20219;&#21153;&#20013;&#32988;&#36807;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#35838;&#31243;&#21457;&#29616;&#38382;&#39064;&#65292;&#24182;&#25551;&#36848;&#20102;&#19968;&#20010;&#33021;&#22815;&#22312;&#35838;&#31243;&#31354;&#38388;&#20013;&#22522;&#20110;&#20808;&#21069;&#26377;&#20851;&#26679;&#26412;&#38590;&#24230;&#30340;&#30693;&#35782;&#21457;&#29616;&#26377;&#25928;&#35838;&#31243;&#30340;&#35838;&#31243;&#23398;&#20064;&#26694;&#26550;&#12290;&#36890;&#36807;&#20351;&#29992;&#27880;&#37322;&#29109;&#21644;&#25439;&#22833;&#20316;&#20026;&#38590;&#24230;&#30340;&#24230;&#37327;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#65306;&#65288;i&#65289;&#32473;&#23450;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#30340;&#26368;&#20339;&#21457;&#29616;&#35838;&#31243;&#24448;&#24448;&#19982;&#29616;&#26377;&#25991;&#29486;&#20013;&#30340;&#21333;&#35843;&#35838;&#31243;&#30456;&#21453;&#65307;&#65288;ii&#65289;&#20256;&#32479;&#30340;&#30001;&#26131;&#21040;&#38590;&#25110;&#30001;&#38590;&#21040;&#26131;&#36807;&#28193;&#30340;&#35838;&#31243;&#24448;&#24448;&#23384;&#22312;&#34920;&#29616;&#19981;&#20339;&#30340;&#39118;&#38505;&#65307;&#65288;iii&#65289;&#23545;&#36739;&#23567;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#21457;&#29616;&#30340;&#35838;&#31243;&#22312;&#36739;&#22823;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#28085;&#30422;&#20102;&#19968;&#20123;&#29616;&#26377;&#30340;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#33021;&#22815;&#21457;&#29616;&#22312;&#20960;&#20010;NLP&#20219;&#21153;&#19978;&#32988;&#36807;&#23427;&#20204;&#30340;&#35838;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the problem of curriculum discovery and describe a curriculum learning framework capable of discovering effective curricula in a curriculum space based on prior knowledge about sample difficulty. Using annotation entropy and loss as measures of difficulty, we show that (i): the top-performing discovered curricula for a given model and dataset are often non-monotonic as opposed to monotonic curricula in existing literature, (ii): the prevailing easy-to-hard or hard-to-easy transition curricula are often at the risk of underperforming, and (iii): the curricula discovered for smaller datasets and models perform well on larger datasets and models respectively. The proposed framework encompasses some of the existing curriculum learning approaches and can discover curricula that outperform them across several NLP tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;&#23545;&#35282;&#32447;&#24615;&#32593;&#32476;&#65288;DLNs&#65289;&#30340;&#26799;&#24230;&#27969;&#25152;&#26045;&#21152;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#65292;&#24847;&#22806;&#22320;&#19982;&#24191;&#20041;&#36817;&#20284;&#38590;&#24230;&#65288;GHA&#65289;&#20013;&#30340;&#30456;&#21464;&#29616;&#35937;&#32852;&#31995;&#36215;&#26469;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#27492;&#30340;&#38160;&#21033;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.07410</link><description>&lt;p&gt;
AI&#20013;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#19982;&#20248;&#21270;&#20013;&#30340;&#24191;&#20041;&#36817;&#20284;&#38590;&#24230;&#30456;&#36935;--&#23545;&#23545;&#35282;&#32447;&#24615;&#32593;&#32476;&#30340;&#38160;&#21033;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Implicit regularization in AI meets generalized hardness of approximation in optimization -- Sharp results for diagonal linear networks. (arXiv:2307.07410v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07410
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;&#23545;&#35282;&#32447;&#24615;&#32593;&#32476;&#65288;DLNs&#65289;&#30340;&#26799;&#24230;&#27969;&#25152;&#26045;&#21152;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#65292;&#24847;&#22806;&#22320;&#19982;&#24191;&#20041;&#36817;&#20284;&#38590;&#24230;&#65288;GHA&#65289;&#20013;&#30340;&#30456;&#21464;&#29616;&#35937;&#32852;&#31995;&#36215;&#26469;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#27492;&#30340;&#38160;&#21033;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#21644;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#26041;&#27861;&#25152;&#26045;&#21152;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#26159;&#28145;&#24230;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#38024;&#23545;&#36229;&#21442;&#25968;&#22238;&#24402;&#35774;&#32622;&#25552;&#20379;&#20102;&#23545;&#20110;&#23545;&#35282;&#32447;&#24615;&#32593;&#32476;&#65288;DLNs&#65289;&#30340;&#26799;&#24230;&#27969;&#25152;&#26045;&#21152;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#30340;&#38160;&#21033;&#32467;&#26524;&#65292;&#24182;&#24847;&#22806;&#22320;&#23558;&#20854;&#19982;&#24191;&#20041;&#36817;&#20284;&#38590;&#24230;&#65288;GHA&#65289;&#20013;&#30340;&#30456;&#21464;&#29616;&#35937;&#32852;&#31995;&#36215;&#26469;&#12290;GHA&#23558;&#36817;&#20284;&#38590;&#24230;&#30340;&#29616;&#35937;&#20174;&#35745;&#31639;&#26426;&#31185;&#23398;&#25512;&#24191;&#21040;&#36830;&#32493;&#21644;&#40065;&#26834;&#20248;&#21270;&#31561;&#39046;&#22495;&#12290;&#20247;&#25152;&#21608;&#30693;&#65292;&#20855;&#26377;&#24494;&#23567;&#21021;&#22987;&#21270;&#30340;DLNs&#30340;&#26799;&#24230;&#27969;&#30340;$\ell^1$-&#33539;&#25968;&#25910;&#25947;&#21040;&#22522;&#30784;&#36861;&#36394;&#30340;&#30446;&#26631;&#20989;&#25968;&#12290;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#20855;&#26377;&#24494;&#23567;&#21021;&#22987;&#21270;&#30340;DLNs&#30340;&#26799;&#24230;&#27969;&#36817;&#20284;&#20110;&#22522;&#30784;&#36861;&#36394;&#20248;&#21270;&#38382;&#39064;&#30340;&#26368;&#23567;&#21270;&#22120;&#65288;&#32780;&#19981;&#20165;&#20165;&#26159;&#30446;&#26631;&#20989;&#25968;&#65289;&#65292;&#36827;&#19968;&#27493;&#25913;&#36827;&#20102;&#36825;&#20123;&#32467;&#26524;&#65292;&#24182;&#33719;&#24471;&#20102;&#26032;&#30340;&#38160;&#21033;&#25910;&#25947;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the implicit regularization imposed by neural network architectures and gradient based optimization methods is a key challenge in deep learning and AI. In this work we provide sharp results for the implicit regularization imposed by the gradient flow of Diagonal Linear Networks (DLNs) in the over-parameterized regression setting and, potentially surprisingly, link this to the phenomenon of phase transitions in generalized hardness of approximation (GHA). GHA generalizes the phenomenon of hardness of approximation from computer science to, among others, continuous and robust optimization. It is well-known that the $\ell^1$-norm of the gradient flow of DLNs with tiny initialization converges to the objective function of basis pursuit. We improve upon these results by showing that the gradient flow of DLNs with tiny initialization approximates minimizers of the basis pursuit optimization problem (as opposed to just the objective function), and we obtain new and sharp converg
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#32852;&#37030;&#23398;&#20064;&#25910;&#25947;&#20998;&#26512;&#25216;&#26415;&#65292;&#24182;&#38024;&#23545;&#23384;&#22312;&#22122;&#22768;&#30340;&#36890;&#20449;&#22330;&#26223;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;&#20449;&#22122;&#27604;&#25511;&#21046;&#31574;&#30053;&#65292;&#20197;&#25552;&#39640;&#25910;&#25947;&#36895;&#29575;&#24182;&#33410;&#30465;&#21151;&#29575;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2307.07406</link><description>&lt;p&gt;
&#22312;&#22122;&#22768;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#25913;&#36827;&#20102;&#32852;&#37030;&#23398;&#20064;&#30340;&#25910;&#25947;&#20998;&#26512;&#21644;&#20449;&#22122;&#27604;&#25511;&#21046;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Improved Convergence Analysis and SNR Control Strategies for Federated Learning in the Presence of Noise. (arXiv:2307.07406v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07406
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#32852;&#37030;&#23398;&#20064;&#25910;&#25947;&#20998;&#26512;&#25216;&#26415;&#65292;&#24182;&#38024;&#23545;&#23384;&#22312;&#22122;&#22768;&#30340;&#36890;&#20449;&#22330;&#26223;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;&#20449;&#22122;&#27604;&#25511;&#21046;&#31574;&#30053;&#65292;&#20197;&#25552;&#39640;&#25910;&#25947;&#36895;&#29575;&#24182;&#33410;&#30465;&#21151;&#29575;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#25910;&#25947;&#20998;&#26512;&#25216;&#26415;&#65292;&#25551;&#36848;&#20102;&#20855;&#26377;&#19981;&#23436;&#32654;/&#22122;&#22768;&#19978;&#34892;&#21644;&#19979;&#34892;&#36890;&#20449;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#33539;&#24335;&#12290;&#36825;&#31181;&#19981;&#23436;&#32654;&#30340;&#36890;&#20449;&#22330;&#26223;&#22312;&#26032;&#20852;&#36890;&#20449;&#31995;&#32479;&#21644;&#21327;&#35758;&#20013;&#23454;&#38469;&#37096;&#32626;FL&#26102;&#20986;&#29616;&#12290;&#26412;&#25991;&#20013;&#24320;&#21457;&#30340;&#20998;&#26512;&#39318;&#27425;&#35777;&#26126;&#20102;FL&#20013;&#19978;&#34892;&#21644;&#19979;&#34892;&#36890;&#20449;&#30340;&#19981;&#21033;&#24433;&#21709;&#23384;&#22312;&#19981;&#23545;&#31216;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#19979;&#34892;&#22122;&#22768;&#30340;&#19981;&#21033;&#24433;&#21709;&#23545;FL&#31639;&#27861;&#30340;&#25910;&#25947;&#26356;&#20026;&#20005;&#37325;&#12290;&#22522;&#20110;&#36825;&#20010;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;&#20449;&#22122;&#27604;&#65288;SNR&#65289;&#25511;&#21046;&#31574;&#30053;&#65292;&#36890;&#36807;&#20002;&#24323;&#21487;&#24573;&#30053;&#30340;&#39640;&#38454;&#39033;&#65292;&#20351;FL&#30340;&#25910;&#25947;&#36895;&#29575;&#19982;&#23436;&#32654;&#30340;&#12289;&#26080;&#22122;&#22768;&#30340;&#36890;&#20449;&#20449;&#36947;&#24773;&#20917;&#19979;&#30456;&#20284;&#65292;&#21516;&#26102;&#27604;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#33410;&#30465;&#20102;&#22823;&#37327;&#30340;&#21151;&#29575;&#36164;&#28304;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30830;&#31435;&#20102;&#32500;&#25345;$O(\frac{1}{\sqrt{K}})$&#25910;&#25947;&#36895;&#29575;&#25152;&#38656;&#30340;&#20449;&#22122;&#27604;&#25511;&#21046;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an improved convergence analysis technique that characterizes the distributed learning paradigm of federated learning (FL) with imperfect/noisy uplink and downlink communications. Such imperfect communication scenarios arise in the practical deployment of FL in emerging communication systems and protocols. The analysis developed in this paper demonstrates, for the first time, that there is an asymmetry in the detrimental effects of uplink and downlink communications in FL. In particular, the adverse effect of the downlink noise is more severe on the convergence of FL algorithms. Using this insight, we propose improved Signal-to-Noise (SNR) control strategies that, discarding the negligible higher-order terms, lead to a similar convergence rate for FL as in the case of a perfect, noise-free communication channel while incurring significantly less power resources compared to existing solutions. In particular, we establish that to maintain the $O(\frac{1}{\sqrt{K}})$ rate of co
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#31232;&#30095;&#20984;&#20248;&#21270;&#20013;$\ell_1$&#27491;&#21017;&#21270;&#30340;&#24615;&#33021;&#65292;&#32473;&#20986;&#20102;Group LASSO&#30340;&#24674;&#22797;&#20445;&#35777;&#65292;&#24182;&#19988;&#21457;&#29616;&#20102;Group LASSO&#36873;&#25321;&#30456;&#21516;&#29305;&#24449;&#38598;&#30340;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2307.07405</link><description>&lt;p&gt;
$\ell_1$&#27491;&#21017;&#21270;&#22312;&#31232;&#30095;&#20984;&#20248;&#21270;&#20013;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Performance of $\ell_1$ Regularization for Sparse Convex Optimization. (arXiv:2307.07405v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07405
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#31232;&#30095;&#20984;&#20248;&#21270;&#20013;$\ell_1$&#27491;&#21017;&#21270;&#30340;&#24615;&#33021;&#65292;&#32473;&#20986;&#20102;Group LASSO&#30340;&#24674;&#22797;&#20445;&#35777;&#65292;&#24182;&#19988;&#21457;&#29616;&#20102;Group LASSO&#36873;&#25321;&#30456;&#21516;&#29305;&#24449;&#38598;&#30340;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;LASSO&#21644;Group LASSO&#22312;&#23454;&#36341;&#20013;&#34987;&#24191;&#27867;&#37319;&#29992;, &#20294;&#26159;&#23545;&#20110;&#38500;&#20102;&#32479;&#35745;&#38382;&#39064;&#20197;&#22806;&#30340;&#20854;&#20182;&#24773;&#20917;, &#36825;&#20123;&#31639;&#27861;&#30340;&#20445;&#35777;&#20196;&#20154;&#38663;&#24778;&#22320;&#32570;&#20047;, &#24182;&#19988;&#22312;&#30830;&#23450;&#24615;&#36755;&#20837;&#30340;&#31232;&#30095;&#20984;&#20248;&#21270;&#32972;&#26223;&#19979;&#36890;&#24120;&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#21551;&#21457;&#24335;&#31639;&#27861;&#12290;&#25105;&#20204;&#20026;&#20855;&#26377;&#21521;&#37327;&#20540;&#29305;&#24449;&#30340;&#31232;&#30095;&#20984;&#20248;&#21270;&#30340;Group LASSO&#32473;&#20986;&#20102;&#31532;&#19968;&#20010;&#24674;&#22797;&#20445;&#35777;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#65292;&#22914;&#26524;&#22312;&#26368;&#23567;&#21270;&#20005;&#26684;&#20984;&#20989;&#25968;$l$&#26102;&#24212;&#29992;&#36275;&#22815;&#22823;&#30340;Group LASSO&#27491;&#21017;&#21270;&#65292;&#37027;&#20040;&#26497;&#23567;&#21270;&#22120;&#26159;&#22312;&#20855;&#26377;&#26368;&#22823;&#26799;&#24230;&#30340;$\ell_2$&#33539;&#25968;&#30340;&#21521;&#37327;&#20540;&#29305;&#24449;&#19978;&#25903;&#25345;&#30340;&#31232;&#30095;&#21521;&#37327;&#12290;&#22240;&#27492;&#65292;&#37325;&#22797;&#27492;&#36807;&#31243;&#36873;&#25321;&#19982;&#27491;&#20132;&#21305;&#37197;&#36861;&#36394;&#31639;&#27861;&#30456;&#21516;&#30340;&#29305;&#24449;&#38598;&#65292;&#36890;&#36807;&#24369;&#27425;&#27169;&#24615;&#35777;&#26126;&#20102;&#23545;&#20110;&#20855;&#26377;&#21463;&#38480;&#24378;&#20984;&#24615;&#21644;&#20809;&#28369;&#24615;&#30340;&#20219;&#20309;&#20989;&#25968;$l$&#37117;&#20855;&#26377;&#24674;&#22797;&#20445;&#35777;&#12290;&#36825;&#22238;&#31572;&#20102;Tibshirani&#31561;&#20154;&#21644;Yasuda&#31561;&#20154;&#30340;&#24320;&#25918;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#39318;&#27425;&#22312;&#29702;&#35770;&#19978;&#35299;&#37322;&#20102;
&lt;/p&gt;
&lt;p&gt;
Despite widespread adoption in practice, guarantees for the LASSO and Group LASSO are strikingly lacking in settings beyond statistical problems, and these algorithms are usually considered to be a heuristic in the context of sparse convex optimization on deterministic inputs. We give the first recovery guarantees for the Group LASSO for sparse convex optimization with vector-valued features. We show that if a sufficiently large Group LASSO regularization is applied when minimizing a strictly convex function $l$, then the minimizer is a sparse vector supported on vector-valued features with the largest $\ell_2$ norm of the gradient. Thus, repeating this procedure selects the same set of features as the Orthogonal Matching Pursuit algorithm, which admits recovery guarantees for any function $l$ with restricted strong convexity and smoothness via weak submodularity arguments. This answers open questions of Tibshirani et al. and Yasuda et al. Our result is the first to theoretically expla
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SHIP&#30340;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21512;&#25104;&#30340;&#25552;&#31034;&#21644;&#31867;&#21035;&#21517;&#31216;&#26469;&#25913;&#36827;CLIP&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#38271;&#23614;&#20998;&#24067;&#21644;&#32570;&#20047;&#26631;&#35760;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.07397</link><description>&lt;p&gt;
&#36890;&#36807;&#29983;&#25104;&#30340;&#25552;&#31034;&#25913;&#36827;CLIP&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improving Zero-Shot Generalization for CLIP with Synthesized Prompts. (arXiv:2307.07397v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07397
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SHIP&#30340;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21512;&#25104;&#30340;&#25552;&#31034;&#21644;&#31867;&#21035;&#21517;&#31216;&#26469;&#25913;&#36827;CLIP&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#38271;&#23614;&#20998;&#24067;&#21644;&#32570;&#20047;&#26631;&#35760;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#20204;&#23545;CLIP&#31561;&#39044;&#35757;&#32451;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#20852;&#36259;&#26085;&#30410;&#22686;&#38271;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#37325;&#28857;&#22312;&#20110;&#23558;&#36825;&#20123;&#27169;&#22411;&#36866;&#24212;&#21040;&#19979;&#28216;&#20219;&#21153;&#19978;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#37117;&#38656;&#35201;&#25152;&#26377;&#31867;&#21035;&#30340;&#26631;&#35760;&#25968;&#25454;&#65292;&#32780;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#21487;&#33021;&#19981;&#20855;&#22791;&#36825;&#31181;&#26465;&#20214;&#65292;&#22240;&#20026;&#23384;&#22312;&#38271;&#23614;&#20998;&#24067;&#21644;Zipf&#23450;&#24459;&#12290;&#20363;&#22914;&#65292;&#26576;&#20123;&#31867;&#21035;&#21487;&#33021;&#23436;&#20840;&#32570;&#20047;&#26631;&#35760;&#25968;&#25454;&#65292;&#22914;&#26032;&#20852;&#27010;&#24565;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SYNTHESIZED PROMPTS (SHIP) &#30340;&#21363;&#25554;&#21363;&#29992;&#30340;&#29983;&#25104;&#26041;&#27861;&#65292;&#20197;&#25913;&#36827;&#29616;&#26377;&#30340;&#24494;&#35843;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#24341;&#20837;&#19968;&#20010;&#29983;&#25104;&#22120;&#65292;&#36890;&#36807;&#23558;&#21512;&#25104;&#30340;&#25552;&#31034;&#21644;&#30456;&#24212;&#30340;&#31867;&#21035;&#21517;&#31216;&#36755;&#20837;&#21040;CLIP&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#20013;&#26469;&#37325;&#26500;&#35270;&#35273;&#29305;&#24449;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#21487;&#20197;&#36731;&#26494;&#33719;&#21462;&#21097;&#20313;&#20165;&#26377;&#26631;&#31614;&#30340;&#31867;&#21035;&#30340;&#29983;&#25104;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#37319;&#29992;&#29616;&#25104;&#30340;&#26041;&#27861;&#23545;CLIP&#36827;&#34892;&#24494;&#35843;&#65292;&#23558;&#26631;&#35760;&#21644;&#21512;&#25104;&#30340;&#29305;&#24449;&#36827;&#34892;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the growing interest in pretrained vision-language models like CLIP, recent research has focused on adapting these models to downstream tasks. Despite achieving promising results, most existing methods require labeled data for all classes, which may not hold in real-world applications due to the long tail and Zipf's law. For example, some classes may lack labeled data entirely, such as emerging concepts. To address this problem, we propose a plug-and-play generative approach called \textbf{S}ynt\textbf{H}es\textbf{I}zed \textbf{P}rompts~(\textbf{SHIP}) to improve existing fine-tuning methods. Specifically, we follow variational autoencoders to introduce a generator that reconstructs the visual features by inputting the synthesized prompts and the corresponding class names to the textual encoder of CLIP. In this manner, we easily obtain the synthesized features for the remaining label-only classes. Thereafter, we fine-tune CLIP with off-the-shelf methods by combining labeled and sy
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#21452;&#20998;&#22270;&#20013;&#21487;&#35270;&#21270;&#37325;&#21472;&#32858;&#31867;&#21644;&#30456;&#20851;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#33021;&#28385;&#36275;&#22810;&#20010;&#30446;&#26631;&#30340;&#31639;&#27861;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#20013;&#26368;&#20339;&#24179;&#34913;&#26159;&#36890;&#36807;&#19968;&#31181;&#27809;&#26377;&#37325;&#21472;&#30340;&#32858;&#31867;&#35299;&#20915;&#26041;&#27861;&#24471;&#21040;&#30340;&#12290;</title><link>http://arxiv.org/abs/2307.07396</link><description>&lt;p&gt;
&#21487;&#35270;&#21270;&#37325;&#21472;&#21452;&#32858;&#31867;&#21644;&#24067;&#23572;&#30697;&#38453;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
Visualizing Overlapping Biclusterings and Boolean Matrix Factorizations. (arXiv:2307.07396v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07396
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#21452;&#20998;&#22270;&#20013;&#21487;&#35270;&#21270;&#37325;&#21472;&#32858;&#31867;&#21644;&#30456;&#20851;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#33021;&#28385;&#36275;&#22810;&#20010;&#30446;&#26631;&#30340;&#31639;&#27861;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#20013;&#26368;&#20339;&#24179;&#34913;&#26159;&#36890;&#36807;&#19968;&#31181;&#27809;&#26377;&#37325;&#21472;&#30340;&#32858;&#31867;&#35299;&#20915;&#26041;&#27861;&#24471;&#21040;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21452;&#20998;&#22270;&#20013;&#23547;&#25214;&#65288;&#21452;&#65289;&#32858;&#31867;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#25968;&#25454;&#20998;&#26512;&#26041;&#27861;&#12290;&#20998;&#26512;&#20154;&#21592;&#36890;&#24120;&#24076;&#26395;&#21487;&#35270;&#21270;&#36825;&#20123;&#32858;&#31867;&#65292;&#21482;&#35201;&#32858;&#31867;&#19981;&#37325;&#21472;&#65292;&#36825;&#26159;&#31616;&#21333;&#30340;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#20195;&#31639;&#27861;&#21457;&#29616;&#37325;&#21472;&#30340;&#32858;&#31867;&#65292;&#20351;&#21487;&#35270;&#21270;&#21464;&#24471;&#26356;&#21152;&#22797;&#26434;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#21452;&#20998;&#22270;&#20013;&#21487;&#35270;&#21270;&#37325;&#21472;&#32858;&#31867;&#21644;&#30456;&#20851;&#38382;&#39064;&#30340;&#38382;&#39064;&#65292;&#20197;&#21450;&#21487;&#35270;&#21270;&#24067;&#23572;&#30697;&#38453;&#20998;&#35299;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#27010;&#24565;&#21270;&#20102;&#20219;&#20309;&#22909;&#30340;&#21487;&#35270;&#21270;&#24212;&#35813;&#28385;&#36275;&#30340;&#19977;&#20010;&#19981;&#21516;&#30446;&#26631;&#65306;&#65288;1&#65289;&#32858;&#31867;&#20803;&#32032;&#30340;&#25509;&#36817;&#24615;&#65292;&#65288;2&#65289;&#30456;&#21516;&#32858;&#31867;&#30340;&#20803;&#32032;&#30340;&#22823;&#36830;&#32493;&#21306;&#22495;&#65292;&#20197;&#21450;&#65288;3&#65289;&#21487;&#35270;&#21270;&#20013;&#30340;&#22823;&#36830;&#32493;&#21306;&#22495;&#65292;&#26080;&#35770;&#32858;&#31867;&#25104;&#21592;&#22914;&#20309;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25429;&#25417;&#36825;&#20123;&#30446;&#26631;&#30340;&#30446;&#26631;&#20989;&#25968;&#21644;&#20248;&#21270;&#36825;&#20123;&#30446;&#26631;&#20989;&#25968;&#30340;&#31639;&#27861;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#22312;&#23545;&#30495;&#23454;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#36825;&#20123;&#31454;&#20105;&#30446;&#26631;&#20043;&#38388;&#30340;&#26368;&#20339;&#24179;&#34913;&#26159;&#36890;&#36807;&#19968;&#31181;&#27809;&#26377;&#37325;&#21472;&#30340;&#32858;&#31867;&#35299;&#20915;&#26041;&#27861;&#24471;&#21040;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finding (bi-)clusters in bipartite graphs is a popular data analysis approach. Analysts typically want to visualize the clusters, which is simple as long as the clusters are disjoint. However, many modern algorithms find overlapping clusters, making visualization more complicated. In this paper, we study the problem of visualizing \emph{a given clustering} of overlapping clusters in bipartite graphs and the related problem of visualizing Boolean Matrix Factorizations. We conceptualize three different objectives that any good visualization should satisfy: (1) proximity of cluster elements, (2) large consecutive areas of elements from the same cluster, and (3) large uninterrupted areas in the visualization, regardless of the cluster membership. We provide objective functions that capture these goals and algorithms that optimize these objective functions. Interestingly, in experiments on real-world datasets, we find that the best trade-off between these competing goals is achieved by a no
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20013;&#24515;&#26680;&#23545;&#40784;&#30340;&#31232;&#30095;&#24615;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20943;&#23569;&#19981;&#21516;&#23618;&#20043;&#38388;&#30340;&#29305;&#24449;&#30456;&#20284;&#24615;&#65292;&#21487;&#20197;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#30340;&#31232;&#30095;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.07389</link><description>&lt;p&gt;
&#23398;&#20064;&#24102;&#26377;&#24658;&#31561;&#23618;&#30340;&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Learning Sparse Neural Networks with Identity Layers. (arXiv:2307.07389v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07389
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20013;&#24515;&#26680;&#23545;&#40784;&#30340;&#31232;&#30095;&#24615;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20943;&#23569;&#19981;&#21516;&#23618;&#20043;&#38388;&#30340;&#29305;&#24449;&#30456;&#20284;&#24615;&#65292;&#21487;&#20197;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#30340;&#31232;&#30095;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#31232;&#30095;&#24615;&#24050;&#32463;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#65292;&#30446;&#30340;&#26159;&#26368;&#22823;&#21270;&#24615;&#33021;&#24182;&#23613;&#21487;&#33021;&#20943;&#23569;&#36807;&#21442;&#25968;&#21270;&#32593;&#32476;&#30340;&#22823;&#23567;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#20351;&#29992;&#38408;&#20540;&#21644;&#25351;&#26631;&#36890;&#36807;&#21098;&#26525;&#21442;&#25968;&#26469;&#36827;&#34892;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#31232;&#30095;&#21270;&#12290;&#32780;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35282;&#24230;&#65292;&#21363;&#19981;&#21516;&#23618;&#20043;&#38388;&#30340;&#29305;&#24449;&#30456;&#20284;&#24615;&#65292;&#35813;&#30456;&#20284;&#24615;&#22312;&#35777;&#26126;&#20043;&#21518;&#34987;&#35777;&#26126;&#19982;&#32593;&#32476;&#31232;&#30095;&#24615;&#39640;&#24230;&#30456;&#20851;&#12290;&#21463;&#21040;&#36229;&#21442;&#25968;&#21270;&#27169;&#22411;&#20013;&#23618;&#38388;&#29305;&#24449;&#30456;&#20284;&#24615;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#32593;&#32476;&#31232;&#30095;&#24615;&#19982;&#23618;&#38388;&#29305;&#24449;&#30456;&#20284;&#24615;&#20043;&#38388;&#30340;&#20869;&#22312;&#32852;&#31995;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35777;&#26126;&#22522;&#20110;&#20013;&#24515;&#26680;&#23545;&#40784;&#65288;CKA&#65289;&#26469;&#20943;&#23569;&#23618;&#38388;&#29305;&#24449;&#30456;&#20284;&#24615;&#21487;&#20197;&#25913;&#21892;&#32593;&#32476;&#30340;&#31232;&#30095;&#24615;&#65292;&#21033;&#29992;&#20449;&#24687;&#29942;&#39048;&#29702;&#35770;&#26469;&#25903;&#25345;&#36825;&#19968;&#35266;&#28857;&#12290;&#22522;&#20110;&#36825;&#20010;&#29702;&#35770;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CKA&#30340;&#31232;&#30095;&#24615;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#31216;&#20026;CKA-SR&#65292;&#23427;&#21033;&#29992;CKA&#26469;&#20943;&#23569;&#23618;&#38388;&#30340;&#29305;&#24449;&#30456;&#20284;&#24615;&#65292;&#20174;&#32780;&#22686;&#21152;&#32593;&#32476;&#30340;&#31232;&#30095;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The sparsity of Deep Neural Networks is well investigated to maximize the performance and reduce the size of overparameterized networks as possible. Existing methods focus on pruning parameters in the training process by using thresholds and metrics. Meanwhile, feature similarity between different layers has not been discussed sufficiently before, which could be rigorously proved to be highly correlated to the network sparsity in this paper. Inspired by interlayer feature similarity in overparameterized models, we investigate the intrinsic link between network sparsity and interlayer feature similarity. Specifically, we prove that reducing interlayer feature similarity based on Centered Kernel Alignment (CKA) improves the sparsity of the network by using information bottleneck theory. Applying such theory, we propose a plug-and-play CKA-based Sparsity Regularization for sparse network training, dubbed CKA-SR, which utilizes CKA to reduce feature similarity between layers and increase n
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#37327;&#23376;&#35745;&#31639;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23450;&#20041;&#25299;&#25169;&#26680;&#30340;&#26032;&#26041;&#27861;&#65292;&#22522;&#20110;&#26500;&#24314;&#38543;&#30528;&#38454;&#25968;&#22686;&#21152;&#30340;&#25299;&#25169;&#25351;&#32441;&#65292;&#21487;&#20197;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#36817;&#20284;&#35745;&#31639;&#39640;&#32500;Betti&#25968;</title><link>http://arxiv.org/abs/2307.07383</link><description>&lt;p&gt;
&#36890;&#36807;&#37327;&#23376;&#35745;&#31639;&#23454;&#29616;&#39640;&#38454;&#25299;&#25169;&#26680;
&lt;/p&gt;
&lt;p&gt;
Higher-order topological kernels via quantum computation. (arXiv:2307.07383v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07383
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#37327;&#23376;&#35745;&#31639;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23450;&#20041;&#25299;&#25169;&#26680;&#30340;&#26032;&#26041;&#27861;&#65292;&#22522;&#20110;&#26500;&#24314;&#38543;&#30528;&#38454;&#25968;&#22686;&#21152;&#30340;&#25299;&#25169;&#25351;&#32441;&#65292;&#21487;&#20197;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#36817;&#20284;&#35745;&#31639;&#39640;&#32500;Betti&#25968;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#65288;TDA&#65289;&#24050;&#25104;&#20026;&#20174;&#22797;&#26434;&#25968;&#25454;&#20013;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#35265;&#35299;&#30340;&#26377;&#21147;&#24037;&#20855;&#12290;TDA&#36890;&#36807;&#23558;&#23545;&#35937;&#23884;&#20837;&#21040;&#21333;&#32431;&#22797;&#21512;&#20013;&#65292;&#24182;&#25552;&#21462;&#26377;&#29992;&#30340;&#20840;&#23616;&#23646;&#24615;&#65292;&#20363;&#22914;Betti&#25968;&#65288;&#21363;&#22810;&#32500;&#31354;&#27934;&#30340;&#25968;&#37327;&#65289;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#23545;&#23545;&#35937;&#30340;&#20998;&#26512;&#65292;&#36825;&#21487;&#20197;&#29992;&#20110;&#23450;&#20041;&#19982;&#29616;&#26377;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36731;&#26494;&#38598;&#25104;&#30340;&#20869;&#26680;&#26041;&#27861;&#12290;&#36825;&#20123;&#20869;&#26680;&#26041;&#27861;&#24050;&#32463;&#25214;&#21040;&#20102;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#20381;&#36182;&#20110;&#25552;&#20379;&#20854;&#24615;&#33021;&#30340;&#29702;&#35770;&#20445;&#35777;&#30340;&#24378;&#22823;&#25968;&#23398;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#32463;&#20856;&#30828;&#20214;&#19978;&#35745;&#31639;&#39640;&#32500;Betti&#25968;&#21487;&#33021;&#20250;&#21464;&#24471;&#38750;&#24120;&#26114;&#36149;&#65292;&#32780;&#37327;&#23376;&#31639;&#27861;&#21487;&#20197;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#36817;&#20284;&#35745;&#31639;&#23427;&#20204;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26500;&#24314;Betti&#26354;&#32447;&#30340;&#37327;&#23376;&#26041;&#27861;&#26469;&#23450;&#20041;&#25299;&#25169;&#26680;&#65292;&#21363;&#38543;&#30528;&#38454;&#25968;&#22686;&#21152;&#30340;&#25299;&#25169;&#25351;&#32441;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#24037;&#20316;&#21407;&#22411;
&lt;/p&gt;
&lt;p&gt;
Topological data analysis (TDA) has emerged as a powerful tool for extracting meaningful insights from complex data. TDA enhances the analysis of objects by embedding them into a simplicial complex and extracting useful global properties such as the Betti numbers, i.e. the number of multidimensional holes, which can be used to define kernel methods that are easily integrated with existing machine-learning algorithms. These kernel methods have found broad applications, as they rely on powerful mathematical frameworks which provide theoretical guarantees on their performance. However, the computation of higher-dimensional Betti numbers can be prohibitively expensive on classical hardware, while quantum algorithms can approximate them in polynomial time in the instance size. In this work, we propose a quantum approach to defining topological kernels, which is based on constructing Betti curves, i.e. topological fingerprint of filtrations with increasing order. We exhibit a working prototy
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21477;&#23376;&#23884;&#20837;&#30340;&#26500;&#25104;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#25991;&#26412;&#21644;&#20854;&#35789;&#32452;&#25104;&#20998;&#30340;&#23545;&#40784;&#65292;&#23454;&#29616;&#20102;&#20174;&#26080;&#26631;&#31614;&#25968;&#25454;&#20013;&#23398;&#20064;&#25991;&#26412;&#34920;&#31034;&#30340;&#30446;&#26631;&#65292;&#24182;&#22312;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#21487;&#27604;&#36739;&#30340;&#25913;&#36827;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#30446;&#26631;&#25110;&#32593;&#32476;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2307.07380</link><description>&lt;p&gt;
&#21477;&#23376;&#23884;&#20837;&#30340;&#26500;&#25104;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Composition-contrastive Learning for Sentence Embeddings. (arXiv:2307.07380v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07380
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21477;&#23376;&#23884;&#20837;&#30340;&#26500;&#25104;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#25991;&#26412;&#21644;&#20854;&#35789;&#32452;&#25104;&#20998;&#30340;&#23545;&#40784;&#65292;&#23454;&#29616;&#20102;&#20174;&#26080;&#26631;&#31614;&#25968;&#25454;&#20013;&#23398;&#20064;&#25991;&#26412;&#34920;&#31034;&#30340;&#30446;&#26631;&#65292;&#24182;&#22312;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#21487;&#27604;&#36739;&#30340;&#25913;&#36827;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#30446;&#26631;&#25110;&#32593;&#32476;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#30340;&#21521;&#37327;&#34920;&#31034;&#22312;&#25628;&#32034;&#24212;&#29992;&#20013;&#38750;&#24120;&#26222;&#36941;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#21508;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#26080;&#26631;&#31614;&#25968;&#25454;&#20013;&#23398;&#20064;&#25991;&#26412;&#34920;&#31034;&#65307;&#36890;&#36807;&#26368;&#22823;&#21270;&#30456;&#21516;&#25991;&#26412;&#30340;&#24494;&#23567;&#25200;&#21160;&#23884;&#20837;&#20043;&#38388;&#30340;&#23545;&#40784;&#65292;&#24182;&#40723;&#21169;&#23884;&#20837;&#22312;&#26356;&#24191;&#27867;&#30340;&#35821;&#26009;&#24211;&#20013;&#30340;&#22343;&#21248;&#20998;&#24067;&#12290;&#19982;&#27492;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26368;&#22823;&#21270;&#25991;&#26412;&#21644;&#20854;&#35789;&#32452;&#25104;&#20998;&#30340;&#23545;&#40784;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#36825;&#19968;&#30446;&#26631;&#30340;&#20960;&#20010;&#23454;&#29616;&#26041;&#24335;&#65292;&#24182;&#35814;&#32454;&#35828;&#26126;&#20102;&#27599;&#31181;&#24773;&#20917;&#19979;&#23545;&#34920;&#31034;&#30340;&#24433;&#21709;&#12290;&#22312;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25552;&#39640;&#22522;&#32447;&#27700;&#24179;&#26041;&#38754;&#26377;&#25152;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#36825;&#39033;&#24037;&#20316;&#26159;&#31532;&#19968;&#20010;&#22312;&#19981;&#20135;&#29983;&#36741;&#21161;&#35757;&#32451;&#30446;&#26631;&#25110;&#39069;&#22806;&#32593;&#32476;&#21442;&#25968;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#36825;&#26679;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vector representations of natural language are ubiquitous in search applications. Recently, various methods based on contrastive learning have been proposed to learn textual representations from unlabelled data; by maximizing alignment between minimally-perturbed embeddings of the same text, and encouraging a uniform distribution of embeddings across a broader corpus. Differently, we propose maximizing alignment between texts and a composition of their phrasal constituents. We consider several realizations of this objective and elaborate the impact on representations in each case. Experimental results on semantic textual similarity tasks show improvements over baselines that are comparable with state-of-the-art approaches. Moreover, this work is the first to do so without incurring costs in auxiliary training objectives or additional network parameters.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#35270;&#35273;&#22788;&#29702;&#26469;&#20934;&#30830;&#20998;&#31867;&#22686;&#26448;&#21046;&#36896;&#22270;&#20687;&#25968;&#25454;&#38598;&#20013;&#30340;&#32570;&#38519;&#65292;&#24182;&#24212;&#29992;&#20027;&#21160;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#20248;&#21270;&#65292;&#20197;&#26500;&#24314;&#19968;&#20010;&#20154;&#22312;&#24490;&#29615;&#26426;&#21046;&#65292;&#20197;&#20943;&#23569;&#22521;&#35757;&#21644;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#25152;&#38656;&#30340;&#25968;&#25454;&#37327;&#12290;</title><link>http://arxiv.org/abs/2307.07378</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#35270;&#35273;&#22788;&#29702;&#22312;&#22686;&#26448;&#21046;&#36896;&#20013;&#36827;&#34892;&#32570;&#38519;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Defect Classification in Additive Manufacturing Using CNN-Based Vision Processing. (arXiv:2307.07378v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07378
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#35270;&#35273;&#22788;&#29702;&#26469;&#20934;&#30830;&#20998;&#31867;&#22686;&#26448;&#21046;&#36896;&#22270;&#20687;&#25968;&#25454;&#38598;&#20013;&#30340;&#32570;&#38519;&#65292;&#24182;&#24212;&#29992;&#20027;&#21160;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#20248;&#21270;&#65292;&#20197;&#26500;&#24314;&#19968;&#20010;&#20154;&#22312;&#24490;&#29615;&#26426;&#21046;&#65292;&#20197;&#20943;&#23569;&#22521;&#35757;&#21644;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#25152;&#38656;&#30340;&#25968;&#25454;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#21407;&#20301;&#30417;&#25511;&#21033;&#29992;&#35270;&#35273;&#20256;&#24863;&#22120;&#30340;&#21457;&#23637;&#20801;&#35768;&#20174;&#22686;&#26448;&#21046;&#36896;&#36807;&#31243;&#20013;&#25910;&#38598;&#22823;&#37327;&#25968;&#25454;&#38598;&#12290;&#36825;&#26679;&#30340;&#25968;&#25454;&#38598;&#21487;&#20197;&#19982;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#32467;&#21512;&#20351;&#29992;&#65292;&#20197;&#25913;&#21892;&#22686;&#26448;&#21046;&#36896;&#30340;&#36136;&#37327;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#31181;&#24773;&#26223;&#65306;&#39318;&#20808;&#65292;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#20934;&#30830;&#20998;&#31867;&#22686;&#26448;&#21046;&#36896;&#22270;&#20687;&#25968;&#25454;&#38598;&#20013;&#30340;&#32570;&#38519;&#65307;&#20854;&#27425;&#65292;&#24212;&#29992;&#20027;&#21160;&#23398;&#20064;&#25216;&#26415;&#23545;&#25152;&#24320;&#21457;&#30340;&#20998;&#31867;&#27169;&#22411;&#36827;&#34892;&#20248;&#21270;&#12290;&#36825;&#23558;&#26500;&#24314;&#19968;&#20010;&#20154;&#22312;&#24490;&#29615;&#26426;&#21046;&#65292;&#20197;&#20943;&#23569;&#22521;&#35757;&#21644;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#25152;&#38656;&#30340;&#25968;&#25454;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of computer vision and in-situ monitoring using visual sensors allows the collection of large datasets from the additive manufacturing (AM) process. Such datasets could be used with machine learning techniques to improve the quality of AM. This paper examines two scenarios: first, using convolutional neural networks (CNNs) to accurately classify defects in an image dataset from AM and second, applying active learning techniques to the developed classification model. This allows the construction of a human-in-the-loop mechanism to reduce the size of the data required to train and generate training data.
&lt;/p&gt;</description></item><item><title>&#19968;&#31181;&#26032;&#22411;&#30340;&#22270;&#20687;&#23383;&#24149;&#31070;&#32463;&#32593;&#32476;AIC-AB NET&#23558;&#31354;&#38388;&#27880;&#24847;&#21147;&#21644;&#25991;&#26412;&#23646;&#24615;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#31354;&#38388;&#27880;&#24847;&#21147;&#21644;&#36755;&#20837;&#25991;&#26412;&#23646;&#24615;&#20449;&#24687;&#65292;&#25552;&#21319;&#22270;&#20687;&#23383;&#24149;&#30340;&#24615;&#33021;&#21644;&#20943;&#23569;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.07370</link><description>&lt;p&gt;
AIC-AB NET&#65306;&#19968;&#31181;&#20855;&#26377;&#31354;&#38388;&#27880;&#24847;&#21147;&#21644;&#25991;&#26412;&#23646;&#24615;&#30340;&#22270;&#20687;&#23383;&#24149;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
AIC-AB NET: A Neural Network for Image Captioning with Spatial Attention and Text Attributes. (arXiv:2307.07370v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07370
&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#26032;&#22411;&#30340;&#22270;&#20687;&#23383;&#24149;&#31070;&#32463;&#32593;&#32476;AIC-AB NET&#23558;&#31354;&#38388;&#27880;&#24847;&#21147;&#21644;&#25991;&#26412;&#23646;&#24615;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#31354;&#38388;&#27880;&#24847;&#21147;&#21644;&#36755;&#20837;&#25991;&#26412;&#23646;&#24615;&#20449;&#24687;&#65292;&#25552;&#21319;&#22270;&#20687;&#23383;&#24149;&#30340;&#24615;&#33021;&#21644;&#20943;&#23569;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#23383;&#24149;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#23637;&#31034;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#23646;&#24615;-&#20449;&#24687;-&#32452;&#21512;&#27880;&#24847;&#21147;&#32593;&#32476;AIC-AB NET&#65292;&#35813;&#32593;&#32476;&#23558;&#31354;&#38388;&#27880;&#24847;&#21147;&#26550;&#26500;&#21644;&#25991;&#26412;&#23646;&#24615;&#32467;&#21512;&#22312;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#20013;&#12290;&#23545;&#20110;&#23383;&#24149;&#29983;&#25104;&#65292;&#33258;&#36866;&#24212;&#31354;&#38388;&#27880;&#24847;&#21147;&#30830;&#23450;&#26368;&#33021;&#20195;&#34920;&#22270;&#20687;&#30340;&#22270;&#20687;&#21306;&#22495;&#65292;&#24182;&#20915;&#23450;&#26159;&#20391;&#37325;&#20110;&#35270;&#35273;&#29305;&#24449;&#36824;&#26159;&#35270;&#35273;&#26631;&#35760;&#12290;&#25991;&#26412;&#23646;&#24615;&#20449;&#24687;&#21516;&#27493;&#36755;&#20837;&#35299;&#30721;&#22120;&#65292;&#20197;&#24110;&#21161;&#22270;&#20687;&#35782;&#21035;&#24182;&#20943;&#23569;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#22312;MS COCO&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#26032;&#25552;&#20986;&#30340;&#26102;&#23578;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23545;AICAB NET&#30340;&#27979;&#35797;&#21644;&#35780;&#20272;&#12290;&#35813;&#26102;&#23578;&#25968;&#25454;&#38598;&#34987;&#29992;&#20316;&#21333;&#29289;&#20307;&#22270;&#20687;&#30340;&#22522;&#20934;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;&#22522;&#32447;&#27169;&#22411;&#21644;&#21435;&#38500;&#20102;&#37096;&#20998;&#32467;&#26500;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;MSCOCO&#22270;&#20687;&#21644;&#25105;&#20204;&#30340;&#21333;&#29289;&#20307;&#22270;&#20687;&#19978;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;AIC-AB NET&#20248;&#20110;&#22522;&#32447;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image captioning is a significant field across computer vision and natural language processing. We propose and present AIC-AB NET, a novel Attribute-Information-Combined Attention-Based Network that combines spatial attention architecture and text attributes in an encoder-decoder. For caption generation, adaptive spatial attention determines which image region best represents the image and whether to attend to the visual features or the visual sentinel. Text attribute information is synchronously fed into the decoder to help image recognition and reduce uncertainty. We have tested and evaluated our AICAB NET on the MS COCO dataset and a new proposed Fashion dataset. The Fashion dataset is employed as a benchmark of single-object images. The results show the superior performance of the proposed model compared to the state-of-the-art baseline and ablated models on both the images from MSCOCO and our single-object images. Our AIC-AB NET outperforms the baseline adaptive attention network 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27979;&#35797;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#20998;&#31867;&#25968;&#25454;&#30340;&#32858;&#31867;&#24615;&#33021;&#12290;&#36890;&#36807;&#35745;&#31639;&#23646;&#24615;&#23545;&#30340;&#21345;&#26041;&#32479;&#35745;&#37327;&#30340;&#21644;&#24471;&#21040;&#19968;&#20010;&#35299;&#26512;&#30340;p&#20540;&#20316;&#20026;&#35780;&#20272;&#25351;&#26631;&#12290;&#32467;&#26524;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#22312;&#22522;&#20934;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.07346</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#27979;&#35797;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#20998;&#31867;&#25968;&#25454;&#30340;&#32858;&#31867;&#24615;
&lt;/p&gt;
&lt;p&gt;
A testing-based approach to assess the clusterability of categorical data. (arXiv:2307.07346v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07346
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27979;&#35797;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#20998;&#31867;&#25968;&#25454;&#30340;&#32858;&#31867;&#24615;&#33021;&#12290;&#36890;&#36807;&#35745;&#31639;&#23646;&#24615;&#23545;&#30340;&#21345;&#26041;&#32479;&#35745;&#37327;&#30340;&#21644;&#24471;&#21040;&#19968;&#20010;&#35299;&#26512;&#30340;p&#20540;&#20316;&#20026;&#35780;&#20272;&#25351;&#26631;&#12290;&#32467;&#26524;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#22312;&#22522;&#20934;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32858;&#31867;&#24615;&#35780;&#20272;&#30340;&#30446;&#26631;&#26159;&#26816;&#26597;&#25968;&#25454;&#38598;&#20013;&#26159;&#21542;&#23384;&#22312;&#32858;&#31867;&#32467;&#26500;&#12290;&#20316;&#20026;&#32858;&#31867;&#20998;&#26512;&#20013;&#19968;&#20010;&#33267;&#20851;&#37325;&#35201;&#20294;&#24120;&#24120;&#34987;&#24573;&#35270;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#24212;&#29992;&#20219;&#20309;&#32858;&#31867;&#31639;&#27861;&#20043;&#21069;&#37117;&#38656;&#35201;&#36827;&#34892;&#36825;&#26679;&#30340;&#26816;&#39564;&#12290;&#22914;&#26524;&#19968;&#20010;&#25968;&#25454;&#38598;&#19981;&#21487;&#32858;&#31867;&#65292;&#21017;&#20219;&#20309;&#21518;&#32493;&#30340;&#32858;&#31867;&#20998;&#26512;&#37117;&#19981;&#20250;&#20135;&#29983;&#26377;&#25928;&#32467;&#26524;&#12290;&#23613;&#31649;&#23427;&#30340;&#37325;&#35201;&#24615;&#65292;&#20294;&#29616;&#26377;&#30740;&#31350;&#20013;&#30340;&#22823;&#37096;&#20998;&#20851;&#27880;&#25968;&#20540;&#25968;&#25454;&#65292;&#23558;&#23545;&#20998;&#31867;&#25968;&#25454;&#30340;&#32858;&#31867;&#24615;&#35780;&#20272;&#38382;&#39064;&#30041;&#32473;&#25104;&#20026;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;TestCat&#65292;&#19968;&#31181;&#22522;&#20110;&#27979;&#35797;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#20998;&#31867;&#25968;&#25454;&#30340;&#32858;&#31867;&#24615;&#65292;&#20351;&#29992;&#35299;&#26512;&#30340;p&#20540;&#20316;&#20026;&#35780;&#20272;&#25351;&#26631;&#12290;TestCat&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#65292;&#21487;&#32858;&#31867;&#30340;&#20998;&#31867;&#25968;&#25454;&#20855;&#26377;&#35768;&#22810;&#24378;&#30456;&#20851;&#30340;&#23646;&#24615;&#23545;&#65292;&#22240;&#27492;&#23558;&#25152;&#26377;&#23646;&#24615;&#23545;&#30340;&#21345;&#26041;&#32479;&#35745;&#37327;&#30340;&#21644;&#20316;&#20026;p&#20540;&#35745;&#31639;&#30340;&#27979;&#35797;&#32479;&#35745;&#37327;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#19968;&#32452;&#22522;&#20934;&#20998;&#31867;&#25968;&#25454;&#38598;&#65292;&#32467;&#26524;&#34920;&#26126;TestCat&#30340;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The objective of clusterability evaluation is to check whether a clustering structure exists within the data set. As a crucial yet often-overlooked issue in cluster analysis, it is essential to conduct such a test before applying any clustering algorithm. If a data set is unclusterable, any subsequent clustering analysis would not yield valid results. Despite its importance, the majority of existing studies focus on numerical data, leaving the clusterability evaluation issue for categorical data as an open problem. Here we present TestCat, a testing-based approach to assess the clusterability of categorical data in terms of an analytical $p$-value. The key idea underlying TestCat is that clusterable categorical data possess many strongly correlated attribute pairs and hence the sum of chi-squared statistics of all attribute pairs is employed as the test statistic for $p$-value calculation. We apply our method to a set of benchmark categorical data sets, showing that TestCat outperforms
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#23558;&#22522;&#20110;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#30340;&#36827;&#21270;&#27169;&#22411;&#19982;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#22312;&#19968;&#36215;&#12290;&#36825;&#20123;&#23618;&#21487;&#20197;&#23454;&#29616;&#29305;&#23450;&#30340;&#27491;&#21017;&#21270;&#30446;&#26631;&#65292;&#24182;&#36171;&#20104;&#31070;&#32463;&#32593;&#32476;&#36755;&#20986;&#19982;&#36827;&#21270;&#27169;&#22411;&#23545;&#24212;&#30340;&#29305;&#24615;&#12290;&#27492;&#22806;&#65292;&#36870;&#36827;&#21270;&#23618;&#30340;&#26500;&#24314;&#21644;&#23454;&#29616;&#30456;&#23545;&#31616;&#21333;&#65292;&#21487;&#20197;&#36731;&#26494;&#22320;&#20026;&#19981;&#21516;&#30340;&#29289;&#29702;&#36827;&#21270;&#21644;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#12290;</title><link>http://arxiv.org/abs/2307.07344</link><description>&lt;p&gt;
&#36870;&#36827;&#21270;&#23618;:&#29289;&#29702;&#20449;&#24687;&#21270;&#27491;&#21017;&#21270;&#22120;&#29992;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Inverse Evolution Layers: Physics-informed Regularizers for Deep Neural Networks. (arXiv:2307.07344v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07344
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#23558;&#22522;&#20110;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#30340;&#36827;&#21270;&#27169;&#22411;&#19982;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#22312;&#19968;&#36215;&#12290;&#36825;&#20123;&#23618;&#21487;&#20197;&#23454;&#29616;&#29305;&#23450;&#30340;&#27491;&#21017;&#21270;&#30446;&#26631;&#65292;&#24182;&#36171;&#20104;&#31070;&#32463;&#32593;&#32476;&#36755;&#20986;&#19982;&#36827;&#21270;&#27169;&#22411;&#23545;&#24212;&#30340;&#29305;&#24615;&#12290;&#27492;&#22806;&#65292;&#36870;&#36827;&#21270;&#23618;&#30340;&#26500;&#24314;&#21644;&#23454;&#29616;&#30456;&#23545;&#31616;&#21333;&#65292;&#21487;&#20197;&#36731;&#26494;&#22320;&#20026;&#19981;&#21516;&#30340;&#29289;&#29702;&#36827;&#21270;&#21644;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#23558;&#22522;&#20110;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#30340;&#36827;&#21270;&#27169;&#22411;&#19982;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#22312;&#19968;&#36215;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#36827;&#21270;&#26041;&#31243;&#30340;&#36870;&#36827;&#21270;&#23618;&#65288;IELs&#65289;&#12290;&#36825;&#20123;&#23618;&#21487;&#20197;&#23454;&#29616;&#29305;&#23450;&#30340;&#27491;&#21017;&#21270;&#30446;&#26631;&#65292;&#24182;&#36171;&#20104;&#31070;&#32463;&#32593;&#32476;&#36755;&#20986;&#19982;&#36827;&#21270;&#27169;&#22411;&#23545;&#24212;&#30340;&#29305;&#24615;&#12290;&#27492;&#22806;&#65292;&#36870;&#36827;&#21270;&#23618;&#30340;&#26500;&#24314;&#21644;&#23454;&#29616;&#30456;&#23545;&#31616;&#21333;&#65292;&#21487;&#20197;&#36731;&#26494;&#22320;&#20026;&#19981;&#21516;&#30340;&#29289;&#29702;&#36827;&#21270;&#21644;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#23618;&#30340;&#35774;&#35745;&#36807;&#31243;&#21487;&#20197;&#20026;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#30452;&#35266;&#21644;&#25968;&#23398;&#21487;&#35299;&#37322;&#24615;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#26041;&#27861;&#30340;&#36879;&#26126;&#24230;&#21644;&#35299;&#37322;&#24615;&#12290;&#20026;&#20102;&#35777;&#26126;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12289;&#25928;&#29575;&#21644;&#31616;&#21333;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23558;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#36171;&#20104;&#28909;&#25193;&#25955;&#27169;&#22411;&#24179;&#28369;&#24615;&#23646;&#24615;&#30340;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel approach to integrating partial differential equation (PDE)-based evolution models into neural networks through a new type of regularization. Specifically, we propose inverse evolution layers (IELs) based on evolution equations. These layers can achieve specific regularization objectives and endow neural networks' outputs with corresponding properties of the evolution models. Moreover, IELs are straightforward to construct and implement, and can be easily designed for various physical evolutions and neural networks. Additionally, the design process for these layers can provide neural networks with intuitive and mathematical interpretability, thus enhancing the transparency and explainability of the approach. To demonstrate the effectiveness, efficiency, and simplicity of our approach, we present an example of endowing semantic segmentation models with the smoothness property based on the heat diffusion model. To achieve this goal, we design heat-diffusion IE
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#25903;&#25345;&#21521;&#37327;&#20998;&#31867;&#22120;&#24182;&#36873;&#25321;&#27169;&#22411;&#21442;&#25968;&#12290;&#36890;&#36807;&#24314;&#27169;&#20026;&#26497;&#23567;&#21270;&#26497;&#22823;&#20248;&#21270;&#38382;&#39064;&#65292;&#21033;&#29992;&#25237;&#24433;&#26799;&#24230;&#31639;&#27861;&#27714;&#35299;&#65292;&#23454;&#29616;&#20102;&#26356;&#20302;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2307.07343</link><description>&lt;p&gt;
MaxMin-L2-SVC-NCH:&#19968;&#31181;&#29992;&#20110;&#35757;&#32451;&#25903;&#25345;&#21521;&#37327;&#20998;&#31867;&#22120;&#24182;&#36873;&#25321;&#27169;&#22411;&#21442;&#25968;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MaxMin-L2-SVC-NCH: A New Method to Train Support Vector Classifier with the Selection of Model's Parameters. (arXiv:2307.07343v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07343
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#25903;&#25345;&#21521;&#37327;&#20998;&#31867;&#22120;&#24182;&#36873;&#25321;&#27169;&#22411;&#21442;&#25968;&#12290;&#36890;&#36807;&#24314;&#27169;&#20026;&#26497;&#23567;&#21270;&#26497;&#22823;&#20248;&#21270;&#38382;&#39064;&#65292;&#21033;&#29992;&#25237;&#24433;&#26799;&#24230;&#31639;&#27861;&#27714;&#35299;&#65292;&#23454;&#29616;&#20102;&#26356;&#20302;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#21442;&#25968;&#30340;&#36873;&#25321;&#22312;&#25903;&#25345;&#21521;&#37327;&#20998;&#31867;&#65288;SVC&#65289;&#30340;&#24212;&#29992;&#20013;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#24120;&#29992;&#30340;&#36873;&#25321;&#27169;&#22411;&#21442;&#25968;&#30340;&#26041;&#27861;&#26159;k&#25240;&#20132;&#21449;&#39564;&#35777;&#19982;&#26684;&#28857;&#25628;&#32034;&#65288;CV&#65289;&#12290;&#30001;&#20110;&#38656;&#35201;&#35757;&#32451;&#22823;&#37327;&#30340;SVC&#27169;&#22411;&#65292;&#36825;&#20010;&#26041;&#27861;&#38750;&#24120;&#32791;&#26102;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;SVC&#24182;&#36873;&#25321;&#27169;&#22411;&#21442;&#25968;&#12290;&#39318;&#20808;&#65292;&#23558;&#20855;&#26377;&#27169;&#22411;&#21442;&#25968;&#36873;&#25321;&#30340;SVC&#35757;&#32451;&#24314;&#27169;&#20026;&#26497;&#23567;&#21270;&#26497;&#22823;&#20248;&#21270;&#38382;&#39064;&#65288;MaxMin-L2-SVC-NCH&#65289;&#65292;&#20854;&#20013;&#26497;&#23567;&#21270;&#38382;&#39064;&#26159;&#23547;&#25214;&#20004;&#20010;&#27491;&#24120;&#20984;&#22771;&#20043;&#38388;&#26368;&#25509;&#36817;&#28857;&#30340;&#20248;&#21270;&#38382;&#39064;&#65288;L2-SVC-NCH&#65289;&#65292;&#32780;&#26497;&#22823;&#21270;&#38382;&#39064;&#26159;&#23547;&#25214;&#26368;&#20248;&#27169;&#22411;&#21442;&#25968;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;MaxMin-L2-SVC-NCH&#20855;&#26377;&#36739;&#20302;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#65292;&#22240;&#20026;&#25918;&#24323;&#20102;CV&#12290;&#28982;&#21518;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#31639;&#27861;&#26469;&#27714;&#35299;MaxMin-L2-SVC-NCH&#65292;&#20854;&#20013;L2-SVC-NCH&#36890;&#36807;&#25237;&#24433;&#26799;&#24230;&#31639;&#27861;&#27714;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
The selection of model's parameters plays an important role in the application of support vector classification (SVC). The commonly used method of selecting model's parameters is the k-fold cross validation with grid search (CV). It is extremely time-consuming because it needs to train a large number of SVC models. In this paper, a new method is proposed to train SVC with the selection of model's parameters. Firstly, training SVC with the selection of model's parameters is modeled as a minimax optimization problem (MaxMin-L2-SVC-NCH), in which the minimization problem is an optimization problem of finding the closest points between two normal convex hulls (L2-SVC-NCH) while the maximization problem is an optimization problem of finding the optimal model's parameters. A lower time complexity can be expected in MaxMin-L2-SVC-NCH because CV is abandoned. A gradient-based algorithm is then proposed to solve MaxMin-L2-SVC-NCH, in which L2-SVC-NCH is solved by a projected gradient algorithm 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25299;&#23637;&#20102;&#35780;&#20272;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#21051;&#26495;&#20559;&#35265;&#30340;&#30740;&#31350;&#65292;&#36890;&#36807;&#36328;&#35821;&#35328;&#20998;&#26512;&#21457;&#29616;mGPT-2&#22312;&#19981;&#21516;&#35821;&#35328;&#20013;&#26174;&#31034;&#20986;&#20196;&#20154;&#24778;&#35766;&#30340;&#21453;&#21051;&#26495;&#34892;&#20026;&#65292;&#24182;&#19988;&#33521;&#35821;&#27169;&#22411;&#34920;&#29616;&#20986;&#26368;&#24378;&#30340;&#20559;&#35265;&#65292;&#32780;&#22303;&#32819;&#20854;&#35821;&#21017;&#26368;&#19981;&#26126;&#26174;&#12290;</title><link>http://arxiv.org/abs/2307.07331</link><description>&lt;p&gt;
&#36328;&#35821;&#35328;&#30340;&#21051;&#26495;&#20559;&#35265;&#26377;&#20309;&#19981;&#21516;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Different Is Stereotypical Bias Across Languages?. (arXiv:2307.07331v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07331
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25299;&#23637;&#20102;&#35780;&#20272;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#21051;&#26495;&#20559;&#35265;&#30340;&#30740;&#31350;&#65292;&#36890;&#36807;&#36328;&#35821;&#35328;&#20998;&#26512;&#21457;&#29616;mGPT-2&#22312;&#19981;&#21516;&#35821;&#35328;&#20013;&#26174;&#31034;&#20986;&#20196;&#20154;&#24778;&#35766;&#30340;&#21453;&#21051;&#26495;&#34892;&#20026;&#65292;&#24182;&#19988;&#33521;&#35821;&#27169;&#22411;&#34920;&#29616;&#20986;&#26368;&#24378;&#30340;&#20559;&#35265;&#65292;&#32780;&#22303;&#32819;&#20854;&#35821;&#21017;&#26368;&#19981;&#26126;&#26174;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#23637;&#31034;&#20102;&#22914;&#20309;&#35780;&#20272;&#39044;&#35757;&#32451;&#30340;&#33521;&#35821;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#21051;&#26495;&#20559;&#35265;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#31995;&#32479;&#22320;&#35843;&#26597;(a)&#22810;&#35821;&#35328;&#27169;&#22411;&#21644;&#21333;&#35821;&#27169;&#22411;&#12289;(b)&#19981;&#21516;&#22522;&#30784;&#26550;&#26500;&#19979;&#30340;&#21051;&#26495;&#20559;&#35265;&#12289;(c)&#22810;&#31181;&#35821;&#35328;&#20013;&#30340;&#20559;&#35265;&#65292;&#25193;&#23637;&#20102;&#35813;&#30740;&#31350;&#39046;&#22495;&#30340;&#22810;&#20010;&#26041;&#38754;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#21033;&#29992;&#33521;&#35821;&#30340;StereoSet&#25968;&#25454;&#38598;&#23558;&#20854;&#21322;&#33258;&#21160;&#32763;&#35793;&#25104;&#24503;&#35821;&#12289;&#27861;&#35821;&#12289;&#35199;&#29677;&#29273;&#35821;&#21644;&#22303;&#32819;&#20854;&#35821;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#19979;&#36827;&#34892;&#36825;&#31181;&#31867;&#22411;&#30340;&#20998;&#26512;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#25105;&#20204;&#30340;&#23454;&#39564;&#23637;&#31034;&#20102;&#19968;&#20010;&#26356;&#20026;&#32454;&#33268;&#30340;&#30011;&#38754;&#65292;&#20197;&#21450;&#19982;&#20165;&#33521;&#35821;&#20998;&#26512;&#26377;&#26174;&#33879;&#24046;&#24322;&#30340;&#21457;&#29616;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#20027;&#35201;&#24471;&#20986;&#20197;&#19979;&#32467;&#35770;&#65306;mGPT-2&#65288;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#65289;&#22312;&#19981;&#21516;&#35821;&#35328;&#20013;&#26174;&#31034;&#20986;&#20196;&#20154;&#24778;&#35766;&#30340;&#21453;&#21051;&#26495;&#34892;&#20026;&#65292;&#33521;&#35821;&#65288;&#21333;&#35821;&#65289;&#27169;&#22411;&#34920;&#29616;&#20986;&#26368;&#24378;&#30340;&#20559;&#35265;&#65292;&#24182;&#19988;&#25968;&#25454;&#38598;&#20013;&#21453;&#26144;&#30340;&#21051;&#26495;&#21360;&#35937;&#22312;&#22303;&#32819;&#20854;&#35821;&#20013;&#26368;&#19981;&#26126;&#26174;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have demonstrated how to assess the stereotypical bias in pre-trained English language models. In this work, we extend this branch of research in multiple different dimensions by systematically investigating (a) mono- and multilingual models of (b) different underlying architectures with respect to their bias in (c) multiple different languages. To that end, we make use of the English StereoSet data set (Nadeem et al., 2021), which we semi-automatically translate into German, French, Spanish, and Turkish. We find that it is of major importance to conduct this type of analysis in a multilingual setting, as our experiments show a much more nuanced picture as well as notable differences from the English-only analysis. The main takeaways from our analysis are that mGPT-2 (partly) shows surprising anti-stereotypical behavior across languages, English (monolingual) models exhibit the strongest bias, and the stereotypes reflected in the data set are least present in Turkish mod
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22686;&#24378;&#21518;&#38376;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#21487;&#23398;&#20064;&#30340;&#27745;&#26579;&#26679;&#26412;&#36873;&#25321;&#31574;&#30053;&#26469;&#26377;&#25928;&#22320;&#36873;&#25321;&#27745;&#26579;&#26679;&#26412;&#12290;&#36825;&#35299;&#20915;&#20102;&#21518;&#38376;&#25915;&#20987;&#20013;&#35201;&#20174;&#25972;&#20010;&#25968;&#25454;&#38598;&#20013;&#36873;&#25321;&#27745;&#26579;&#26679;&#26412;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.07328</link><description>&lt;p&gt;
&#29992;&#21487;&#23398;&#20064;&#30340;&#27745;&#26579;&#26679;&#26412;&#36873;&#25321;&#31574;&#30053;&#22686;&#24378;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Boosting Backdoor Attack with A Learnable Poisoning Sample Selection Strategy. (arXiv:2307.07328v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07328
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22686;&#24378;&#21518;&#38376;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#21487;&#23398;&#20064;&#30340;&#27745;&#26579;&#26679;&#26412;&#36873;&#25321;&#31574;&#30053;&#26469;&#26377;&#25928;&#22320;&#36873;&#25321;&#27745;&#26579;&#26679;&#26412;&#12290;&#36825;&#35299;&#20915;&#20102;&#21518;&#38376;&#25915;&#20987;&#20013;&#35201;&#20174;&#25972;&#20010;&#25968;&#25454;&#38598;&#20013;&#36873;&#25321;&#27745;&#26579;&#26679;&#26412;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#27745;&#26579;&#30340;&#21518;&#38376;&#25915;&#20987;&#26088;&#22312;&#36890;&#36807;&#25805;&#32437;&#35757;&#32451;&#25968;&#25454;&#38598;&#26469;&#23558;&#21518;&#38376;&#25554;&#20837;&#27169;&#22411;&#20013;&#65292;&#32780;&#19981;&#25511;&#21046;&#30446;&#26631;&#27169;&#22411;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;&#29616;&#26377;&#30340;&#25915;&#20987;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#20110;&#35774;&#35745;&#35302;&#21457;&#22120;&#25110;&#35302;&#21457;&#22120;&#19982;&#33391;&#24615;&#26679;&#26412;&#20043;&#38388;&#30340;&#34701;&#21512;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24448;&#24448;&#38543;&#26426;&#36873;&#25321;&#35201;&#27745;&#26579;&#30340;&#26679;&#26412;&#65292;&#24573;&#35270;&#20102;&#27599;&#20010;&#27745;&#26579;&#26679;&#26412;&#22312;&#21518;&#38376;&#27880;&#20837;&#26041;&#38754;&#30340;&#19981;&#21516;&#37325;&#35201;&#24615;&#12290;&#26368;&#36817;&#30340;&#36873;&#25321;&#31574;&#30053;&#36890;&#36807;&#35760;&#24405;&#36951;&#24536;&#20107;&#20214;&#26469;&#36807;&#28388;&#22823;&#23567;&#22266;&#23450;&#30340;&#27745;&#26579;&#26679;&#26412;&#27744;&#65292;&#20294;&#23427;&#26410;&#33021;&#20174;&#20840;&#23616;&#35282;&#24230;&#32771;&#34385;&#27744;&#22806;&#30340;&#21097;&#20313;&#26679;&#26412;&#12290;&#27492;&#22806;&#65292;&#35745;&#31639;&#36951;&#24536;&#20107;&#20214;&#38656;&#35201;&#39069;&#22806;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#22240;&#27492;&#65292;&#22914;&#20309;&#20174;&#25972;&#20010;&#25968;&#25454;&#38598;&#20013;&#39640;&#25928;&#26377;&#25928;&#22320;&#36873;&#25321;&#27745;&#26579;&#26679;&#26412;&#26159;&#21518;&#38376;&#25915;&#20987;&#20013;&#30340;&#19968;&#20010;&#36843;&#20999;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#39318;&#20808;&#25105;&#20204;&#22312;&#24120;&#35268;&#30340;&#21518;&#38376;&#35757;&#32451;&#25439;&#22833;&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#27745;&#26579;&#25513;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-poisoning based backdoor attacks aim to insert backdoor into models by manipulating training datasets without controlling the training process of the target model. Existing attack methods mainly focus on designing triggers or fusion strategies between triggers and benign samples. However, they often randomly select samples to be poisoned, disregarding the varying importance of each poisoning sample in terms of backdoor injection. A recent selection strategy filters a fixed-size poisoning sample pool by recording forgetting events, but it fails to consider the remaining samples outside the pool from a global perspective. Moreover, computing forgetting events requires significant additional computing resources. Therefore, how to efficiently and effectively select poisoning samples from the entire dataset is an urgent problem in backdoor attacks.To address it, firstly, we introduce a poisoning mask into the regular backdoor training loss. We suppose that a backdoored model training w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38544;&#34255;&#21333;&#20803;&#32858;&#31867;&#26694;&#26550;&#36827;&#34892;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26080;&#25991;&#26412;&#36164;&#28304;&#19979;&#30340;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#12290;&#27169;&#22411;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#23618;&#23545;&#38899;&#39057;&#26679;&#26412;&#36827;&#34892;&#22788;&#29702;&#24182;&#29983;&#25104;&#19978;&#19979;&#25991;&#21521;&#37327;&#34920;&#31034;&#65292;&#36890;&#36807;&#38544;&#34255;&#21333;&#20803;&#32858;&#31867;&#26694;&#26550;&#23558;&#36825;&#20123;&#34920;&#31034;&#24402;&#31867;&#20026;&#23569;&#37327;&#31867;&#20284;&#38899;&#32032;&#30340;&#21333;&#20803;&#65292;&#29992;&#20110;&#23398;&#20064;&#35821;&#20041;&#20016;&#23500;&#30340;&#35821;&#38899;&#34920;&#31034;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.07325</link><description>&lt;p&gt;
&#26080;&#36164;&#28304;&#35821;&#38899;&#24212;&#29992;&#30340;&#38544;&#34255;&#21333;&#20803;&#32858;&#31867;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Representation Learning With Hidden Unit Clustering For Low Resource Speech Applications. (arXiv:2307.07325v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07325
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38544;&#34255;&#21333;&#20803;&#32858;&#31867;&#26694;&#26550;&#36827;&#34892;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26080;&#25991;&#26412;&#36164;&#28304;&#19979;&#30340;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#12290;&#27169;&#22411;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#23618;&#23545;&#38899;&#39057;&#26679;&#26412;&#36827;&#34892;&#22788;&#29702;&#24182;&#29983;&#25104;&#19978;&#19979;&#25991;&#21521;&#37327;&#34920;&#31034;&#65292;&#36890;&#36807;&#38544;&#34255;&#21333;&#20803;&#32858;&#31867;&#26694;&#26550;&#23558;&#36825;&#20123;&#34920;&#31034;&#24402;&#31867;&#20026;&#23569;&#37327;&#31867;&#20284;&#38899;&#32032;&#30340;&#21333;&#20803;&#65292;&#29992;&#20110;&#23398;&#20064;&#35821;&#20041;&#20016;&#23500;&#30340;&#35821;&#38899;&#34920;&#31034;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#25991;&#26412;&#36164;&#28304;&#19979;&#30340;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#26159;&#35768;&#22810;&#20302;&#36164;&#28304;&#35821;&#38899;&#24212;&#29992;&#20013;&#19968;&#20010;&#38750;&#24120;&#24863;&#20852;&#36259;&#30340;&#39046;&#22495;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#38544;&#34255;&#21333;&#20803;&#32858;&#31867;&#65288;HUC&#65289;&#26694;&#26550;&#20174;&#21407;&#22987;&#38899;&#39057;&#20013;&#36827;&#34892;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#27169;&#22411;&#30340;&#36755;&#20837;&#21253;&#25324;&#34987;&#31383;&#21475;&#21270;&#24182;&#32463;&#36807;1-D&#21367;&#31215;&#23618;&#22788;&#29702;&#30340;&#38899;&#39057;&#26679;&#26412;&#12290;&#20174;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#27169;&#22359;&#23398;&#20064;&#21040;&#30340;"&#26102;&#39057;"&#34920;&#31034;&#32463;&#36807;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#23618;&#36827;&#19968;&#27493;&#22788;&#29702;&#65292;&#20026;&#27599;&#20010;&#31383;&#21475;&#21270;&#29255;&#27573;&#29983;&#25104;&#19978;&#19979;&#25991;&#21521;&#37327;&#34920;&#31034;&#12290;&#20351;&#29992;&#38544;&#34255;&#21333;&#20803;&#32858;&#31867;&#26694;&#26550;&#23545;&#36825;&#20123;&#34920;&#31034;&#36827;&#34892;&#20998;&#31867;&#65292;&#23558;&#20854;&#24402;&#31867;&#20026;&#23569;&#37327;&#31867;&#20284;&#38899;&#32032;&#30340;&#21333;&#20803;&#65292;&#29992;&#20110;&#35757;&#32451;&#27169;&#22411;&#23398;&#20064;&#35821;&#20041;&#20016;&#23500;&#30340;&#35821;&#38899;&#34920;&#31034;&#12290;&#30446;&#26631;&#21253;&#25324;&#27599;&#20010;&#38899;&#39057;&#29255;&#27573;&#30340;&#31867;&#20284;&#38899;&#32032;&#20266;&#26631;&#31614;&#65292;&#36825;&#20123;&#26631;&#31614;&#26159;&#20351;&#29992;&#36845;&#20195;k-means&#31639;&#27861;&#29983;&#25104;&#30340;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#20123;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#30340;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
The representation learning of speech, without textual resources, is an area of significant interest for many low resource speech applications. In this paper, we describe an approach to self-supervised representation learning from raw audio using a hidden unit clustering (HUC) framework. The input to the model consists of audio samples that are windowed and processed with 1-D convolutional layers. The learned "time-frequency" representations from the convolutional neural network (CNN) module are further processed with long short term memory (LSTM) layers which generate a contextual vector representation for every windowed segment. The HUC framework, allowing the categorization of the representations into a small number of phoneme-like units, is used to train the model for learning semantically rich speech representations. The targets consist of phoneme-like pseudo labels for each audio segment and these are generated with an iterative k-means algorithm. We explore techniques that impro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#20999;&#21106;&#24179;&#38754;&#36873;&#25321;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#35780;&#20998;&#25351;&#26631;&#12289;&#36807;&#28388;&#25216;&#26415;&#21644;&#20572;&#27490;&#20934;&#21017;&#65292;&#20351;&#24471;SCIP&#22312;MIPLIB 2017&#22522;&#20934;&#38598;&#19978;&#30340;&#24615;&#33021;&#25552;&#21319;&#20102;4%&#12290;</title><link>http://arxiv.org/abs/2307.07322</link><description>&lt;p&gt;
&#19968;&#20010;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#20999;&#21106;&#24179;&#38754;&#36873;&#25321;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Context-Aware Cutting Plane Selection Algorithm for Mixed-Integer Programming. (arXiv:2307.07322v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07322
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#20999;&#21106;&#24179;&#38754;&#36873;&#25321;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#35780;&#20998;&#25351;&#26631;&#12289;&#36807;&#28388;&#25216;&#26415;&#21644;&#20572;&#27490;&#20934;&#21017;&#65292;&#20351;&#24471;SCIP&#22312;MIPLIB 2017&#22522;&#20934;&#38598;&#19978;&#30340;&#24615;&#33021;&#25552;&#21319;&#20102;4%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#22312;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#27714;&#35299;&#22120;&#20013;&#20351;&#29992;&#30340;&#20999;&#21106;&#24179;&#38754;&#36873;&#25321;&#31639;&#27861;&#33258;&#20854;&#21019;&#24314;&#20197;&#26469;&#22522;&#26412;&#20445;&#25345;&#19981;&#21464;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#32452;&#26032;&#30340;&#20999;&#21106;&#35780;&#20998;&#25351;&#26631;&#12289;&#20999;&#21106;&#36807;&#28388;&#25216;&#26415;&#21644;&#20572;&#27490;&#20934;&#21017;&#65292;&#25193;&#23637;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#65292;&#24182;&#22312;MIPLIB 2017&#22522;&#20934;&#38598;&#19978;&#20351;SCIP&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;4%&#12290;
&lt;/p&gt;
&lt;p&gt;
The current cut selection algorithm used in mixed-integer programming solvers has remained largely unchanged since its creation. In this paper, we propose a set of new cut scoring measures, cut filtering techniques, and stopping criteria, extending the current state-of-the-art algorithm and obtaining a 4\% performance improvement for SCIP over the MIPLIB 2017 benchmark set.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#33258;&#36866;&#24212;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#20013;&#38750;&#27491;&#24577;&#28176;&#36817;&#34892;&#20026;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#33258;&#36866;&#24212;&#32447;&#24615;&#20272;&#35745;&#26041;&#31243;&#26500;&#24314;&#21435;&#20559;&#20272;&#35745;&#37327;&#65292;&#24182;&#22312;&#22810;&#33218;&#32769;&#34382;&#26426;&#30340;&#32972;&#26223;&#19979;&#20445;&#25345;&#20102;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#37327;&#30340;&#38750;&#28176;&#36817;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.07320</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#32447;&#24615;&#20272;&#35745;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Adaptive Linear Estimating Equations. (arXiv:2307.07320v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07320
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#33258;&#36866;&#24212;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#20013;&#38750;&#27491;&#24577;&#28176;&#36817;&#34892;&#20026;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#33258;&#36866;&#24212;&#32447;&#24615;&#20272;&#35745;&#26041;&#31243;&#26500;&#24314;&#21435;&#20559;&#20272;&#35745;&#37327;&#65292;&#24182;&#22312;&#22810;&#33218;&#32769;&#34382;&#26426;&#30340;&#32972;&#26223;&#19979;&#20445;&#25345;&#20102;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#37327;&#30340;&#38750;&#28176;&#36817;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39034;&#24207;&#25968;&#25454;&#25910;&#38598;&#24050;&#25104;&#20026;&#22686;&#24378;&#25968;&#25454;&#25910;&#38598;&#36807;&#31243;&#25928;&#29575;&#30340;&#24191;&#27867;&#37319;&#29992;&#30340;&#25216;&#26415;&#12290;&#23613;&#31649;&#20855;&#26377;&#20248;&#21183;&#65292;&#20294;&#36825;&#31181;&#25968;&#25454;&#25910;&#38598;&#26426;&#21046;&#24120;&#24120;&#32473;&#32479;&#35745;&#25512;&#26029;&#36807;&#31243;&#24341;&#20837;&#22797;&#26434;&#24615;&#12290;&#20363;&#22914;&#65292;&#22312;&#33258;&#36866;&#24212;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#20013;&#65292;&#26222;&#36890;&#26368;&#23567;&#20108;&#20056;&#65288;OLS&#65289;&#20272;&#35745;&#37327;&#21487;&#33021;&#34920;&#29616;&#20986;&#38750;&#27491;&#24577;&#30340;&#28176;&#36817;&#34892;&#20026;&#65292;&#20174;&#32780;&#23545;&#20934;&#30830;&#30340;&#25512;&#26029;&#21644;&#35299;&#37322;&#25552;&#20986;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26500;&#24314;&#21435;&#20559;&#20272;&#35745;&#37327;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#33258;&#36866;&#24212;&#32447;&#24615;&#20272;&#35745;&#26041;&#31243;&#30340;&#24605;&#24819;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#20445;&#35777;&#20102;&#28176;&#36817;&#27491;&#24577;&#24615;&#65292;&#24182;&#35752;&#35770;&#20102;&#23454;&#29616;&#36817;&#20284;&#26368;&#20248;&#28176;&#36817;&#26041;&#24046;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#20272;&#35745;&#37327;&#30340;&#19968;&#20010;&#26174;&#33879;&#29305;&#28857;&#26159;&#65292;&#22312;&#22810;&#33218;&#32769;&#34382;&#26426;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#30340;&#20272;&#35745;&#37327;&#20445;&#30041;&#20102;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#37327;&#30340;&#38750;&#28176;&#36817;&#24615;&#33021;&#65292;&#21516;&#26102;&#33719;&#24471;&#20102;&#28176;&#36817;&#27491;&#24577;&#24615;&#12290;&#22240;&#27492;&#65292;&#26412;&#24037;&#20316;&#35299;&#20915;&#20102;&#33258;&#36866;&#24212;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#20013;&#38750;&#27491;&#24577;&#28176;&#36817;&#34892;&#20026;&#30340;&#38382;&#39064;&#65292;&#24182;&#20026;&#32479;&#35745;&#25512;&#26029;&#25552;&#20379;&#20102;&#21487;&#38752;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequential data collection has emerged as a widely adopted technique for enhancing the efficiency of data gathering processes. Despite its advantages, such data collection mechanism often introduces complexities to the statistical inference procedure. For instance, the ordinary least squares (OLS) estimator in an adaptive linear regression model can exhibit non-normal asymptotic behavior, posing challenges for accurate inference and interpretation. In this paper, we propose a general method for constructing debiased estimator which remedies this issue. It makes use of the idea of adaptive linear estimating equations, and we establish theoretical guarantees of asymptotic normality, supplemented by discussions on achieving near-optimal asymptotic variance. A salient feature of our estimator is that in the context of multi-armed bandits, our estimator retains the non-asymptotic performance of the least square estimator while obtaining asymptotic normality property. Consequently, this work
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26032;&#38395;&#32534;&#36753;&#23460;&#20013;&#20351;&#29992;&#30340;&#28151;&#21512;&#23457;&#26680;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21521;&#20869;&#23481;&#31649;&#29702;&#21592;&#25512;&#33616;&#29305;&#33394;&#24086;&#23376;&#26469;&#25903;&#25345;&#20182;&#20204;&#22312;&#36873;&#25321;&#29305;&#33394;&#20869;&#23481;&#26041;&#38754;&#20570;&#20986;&#20915;&#31574;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#27010;&#29575;&#25490;&#24207;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#32467;&#21512;&#20102;&#29992;&#25143;&#21644;&#25991;&#26412;&#20869;&#23481;&#29305;&#24449;&#65292;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20998;&#31867;&#21644;&#25490;&#24207;&#24615;&#33021;&#12290;&#20869;&#23481;&#31649;&#29702;&#21592;&#22312;&#35780;&#20272;&#20013;&#21457;&#29616;&#20102;&#21512;&#36866;&#30340;&#35780;&#35770;&#65292;&#24182;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#25509;&#21463;&#20102;&#25512;&#33616;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.07317</link><description>&lt;p&gt;
&#26032;&#38395;&#32534;&#36753;&#23460;&#20013;&#30340;&#28151;&#21512;&#23457;&#26680;&#65306;&#21521;&#20869;&#23481;&#31649;&#29702;&#21592;&#25512;&#33616;&#29305;&#33394;&#24086;&#23376;
&lt;/p&gt;
&lt;p&gt;
Hybrid moderation in the newsroom: Recommending featured posts to content moderators. (arXiv:2307.07317v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07317
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26032;&#38395;&#32534;&#36753;&#23460;&#20013;&#20351;&#29992;&#30340;&#28151;&#21512;&#23457;&#26680;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21521;&#20869;&#23481;&#31649;&#29702;&#21592;&#25512;&#33616;&#29305;&#33394;&#24086;&#23376;&#26469;&#25903;&#25345;&#20182;&#20204;&#22312;&#36873;&#25321;&#29305;&#33394;&#20869;&#23481;&#26041;&#38754;&#20570;&#20986;&#20915;&#31574;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#27010;&#29575;&#25490;&#24207;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#32467;&#21512;&#20102;&#29992;&#25143;&#21644;&#25991;&#26412;&#20869;&#23481;&#29305;&#24449;&#65292;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20998;&#31867;&#21644;&#25490;&#24207;&#24615;&#33021;&#12290;&#20869;&#23481;&#31649;&#29702;&#21592;&#22312;&#35780;&#20272;&#20013;&#21457;&#29616;&#20102;&#21512;&#36866;&#30340;&#35780;&#35770;&#65292;&#24182;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#25509;&#21463;&#20102;&#25512;&#33616;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#26032;&#38395;&#23186;&#20307;&#27491;&#21162;&#21147;&#22788;&#29702;&#35780;&#35770;&#21306;&#29992;&#25143;&#29983;&#25104;&#20869;&#23481;&#30340;&#23457;&#26680;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#25490;&#24207;&#30340;&#25512;&#33616;&#31995;&#32479;&#26469;&#25903;&#25345;&#21644;&#25480;&#26435;&#23457;&#26680;&#21592;&#36873;&#25321;&#29305;&#33394;&#24086;&#23376;&#65292;&#36825;&#26159;&#19968;&#39033;&#32791;&#26102;&#30340;&#20219;&#21153;&#12290;&#36890;&#36807;&#32467;&#21512;&#29992;&#25143;&#21644;&#25991;&#26412;&#20869;&#23481;&#30340;&#29305;&#24449;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#27979;&#35797;&#38598;&#19978;&#30340;&#26368;&#20339;&#20998;&#31867;F1&#20998;&#25968;&#20026;0.44&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#22823;&#37327;&#39564;&#35777;&#25991;&#31456;&#19978;&#35266;&#23519;&#21040;&#20102;&#22343;&#20540;NDCG@5&#30340;&#26368;&#20339;&#20540;&#20026;0.87&#12290;&#22312;&#19987;&#23478;&#35780;&#20272;&#20013;&#65292;&#20869;&#23481;&#31649;&#29702;&#21592;&#26681;&#25454;&#25512;&#33616;&#32467;&#26524;&#36873;&#25321;&#35201;&#25512;&#33616;&#30340;&#35780;&#35770;&#65292;&#24471;&#21040;&#20102;0.83&#30340;NDCG&#20998;&#25968;&#12290;&#25105;&#20204;&#24471;&#20986;&#30340;&#32467;&#35770;&#26159;&#65292;&#39318;&#20808;&#65292;&#28155;&#21152;&#25991;&#26412;&#29305;&#24449;&#21487;&#20197;&#33719;&#24471;&#26368;&#20339;&#24471;&#20998;&#65307;&#20854;&#27425;&#65292;&#34429;&#28982;&#36873;&#25321;&#29305;&#33394;&#20869;&#23481;&#20173;&#28982;&#26377;&#19968;&#23450;&#30340;&#20027;&#35266;&#24615;&#65292;&#20294;&#20869;&#23481;&#31649;&#29702;&#21592;&#22312;&#25152;&#26377;&#34987;&#35780;&#20272;&#30340;&#25512;&#33616;&#20013;&#37117;&#25214;&#21040;&#20102;&#21512;&#36866;&#30340;&#35780;&#35770;&#65292;&#38500;&#20102;&#19968;&#20010;&#20363;&#22806;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#34920;&#29616;&#26368;&#20339;&#30340;&#27169;&#22411;&#65292;&#36808;&#21521;&#36879;&#26126;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online news outlets are grappling with the moderation of user-generated content within their comment section. We present a recommender system based on ranking class probabilities to support and empower the moderator in choosing featured posts, a time-consuming task. By combining user and textual content features we obtain an optimal classification F1-score of 0.44 on the test set. Furthermore, we observe an optimum mean NDCG@5 of 0.87 on a large set of validation articles. As an expert evaluation, content moderators assessed the output of a random selection of articles by choosing comments to feature based on the recommendations, which resulted in a NDCG score of 0.83. We conclude that first, adding text features yields the best score and second, while choosing featured content remains somewhat subjective, content moderators found suitable comments in all but one evaluated recommendations. We end the paper by analyzing our best-performing model, a step towards transparency and explaina
&lt;/p&gt;</description></item><item><title>HEAL-SWIN&#26159;&#19968;&#20010;&#22522;&#20110;HEALPix&#32593;&#26684;&#21644;SWIN&#21464;&#21387;&#22120;&#32467;&#21512;&#30340;&#29699;&#38754;&#35270;&#35273;&#21464;&#21387;&#22120;&#65292;&#22312;&#22788;&#29702;&#39640;&#20998;&#36776;&#29575;&#24191;&#35282;&#40060;&#30524;&#22270;&#20687;&#26102;&#33021;&#22815;&#23454;&#29616;&#26080;&#22833;&#30495;&#19988;&#39640;&#25928;&#30340;&#35757;&#32451;&#65292;&#24182;&#22312;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.07313</link><description>&lt;p&gt;
HEAL-SWIN: &#29699;&#38754;&#19978;&#30340;&#35270;&#35273;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
HEAL-SWIN: A Vision Transformer On The Sphere. (arXiv:2307.07313v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07313
&lt;/p&gt;
&lt;p&gt;
HEAL-SWIN&#26159;&#19968;&#20010;&#22522;&#20110;HEALPix&#32593;&#26684;&#21644;SWIN&#21464;&#21387;&#22120;&#32467;&#21512;&#30340;&#29699;&#38754;&#35270;&#35273;&#21464;&#21387;&#22120;&#65292;&#22312;&#22788;&#29702;&#39640;&#20998;&#36776;&#29575;&#24191;&#35282;&#40060;&#30524;&#22270;&#20687;&#26102;&#33021;&#22815;&#23454;&#29616;&#26080;&#22833;&#30495;&#19988;&#39640;&#25928;&#30340;&#35757;&#32451;&#65292;&#24182;&#22312;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#20998;&#36776;&#29575;&#24191;&#35282;&#40060;&#30524;&#22270;&#20687;&#22312;&#33258;&#21160;&#39550;&#39542;&#31561;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#26222;&#36890;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#25110;&#35270;&#35273;&#21464;&#21387;&#22120;&#22788;&#29702;&#27492;&#31867;&#25968;&#25454;&#23384;&#22312;&#38382;&#39064;&#65292;&#22240;&#20026;&#23558;&#20854;&#25237;&#24433;&#21040;&#24179;&#38754;&#19978;&#30340;&#30697;&#24418;&#32593;&#26684;&#26102;&#20250;&#24341;&#20837;&#25237;&#24433;&#21644;&#22833;&#30495;&#25439;&#22833;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;HEAL-SWIN&#21464;&#21387;&#22120;&#65292;&#23427;&#23558;&#22312;&#22825;&#20307;&#29289;&#29702;&#23398;&#21644;&#23431;&#23449;&#23398;&#20013;&#20351;&#29992;&#30340;&#39640;&#24230;&#22343;&#21248;&#30340;&#23618;&#27425;&#31561;&#38754;&#31215;&#31561;&#32428;&#24230;&#20687;&#32032;&#21270;&#65288;HEALPix&#65289;&#32593;&#26684;&#19982;&#23618;&#27425;&#31227;&#20301;&#31383;&#21475;&#65288;SWIN&#65289;&#21464;&#21387;&#22120;&#32467;&#21512;&#36215;&#26469;&#65292;&#20174;&#32780;&#24471;&#21040;&#33021;&#22815;&#35757;&#32451;&#39640;&#20998;&#36776;&#29575;&#26080;&#22833;&#30495;&#29699;&#38754;&#25968;&#25454;&#30340;&#39640;&#25928;&#28789;&#27963;&#27169;&#22411;&#12290;&#22312;HEAL-SWIN&#20013;&#65292;HEALPix&#32593;&#26684;&#30340;&#23884;&#22871;&#32467;&#26500;&#29992;&#20110;&#25191;&#34892;SWIN&#21464;&#21387;&#22120;&#30340;&#25340;&#25509;&#21644;&#31383;&#21475;&#25805;&#20316;&#65292;&#20174;&#32780;&#24471;&#21040;&#20855;&#26377;&#26368;&#23567;&#35745;&#31639;&#24320;&#38144;&#30340;&#29699;&#38754;&#25968;&#25454;&#19968;&#32500;&#34920;&#31034;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#27169;&#22411;&#22312;&#35821;&#20041;&#20998;&#21106;&#19978;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-resolution wide-angle fisheye images are becoming more and more important for robotics applications such as autonomous driving. However, using ordinary convolutional neural networks or vision transformers on this data is problematic due to projection and distortion losses introduced when projecting to a rectangular grid on the plane. We introduce the HEAL-SWIN transformer, which combines the highly uniform Hierarchical Equal Area iso-Latitude Pixelation (HEALPix) grid used in astrophysics and cosmology with the Hierarchical Shifted-Window (SWIN) transformer to yield an efficient and flexible model capable of training on high-resolution, distortion-free spherical data. In HEAL-SWIN, the nested structure of the HEALPix grid is used to perform the patching and windowing operations of the SWIN transformer, resulting in a one-dimensional representation of the spherical data with minimal computational overhead. We demonstrate the superior performance of our model for semantic segmentati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#29289;&#29702;&#32422;&#26463;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#25104;&#21151;&#35299;&#20915;&#20102;&#39640;&#38454;Lane-Emden-Fowler&#31867;&#22411;&#26041;&#31243;&#65292;&#36890;&#36807;&#27604;&#36739;&#36719;&#32422;&#26463;&#21644;&#30828;&#32422;&#26463;&#30340;&#21464;&#20307;&#65292;&#24471;&#20986;&#20102;&#26377;&#25928;&#30340;&#25968;&#20540;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.07302</link><description>&lt;p&gt;
&#20351;&#29992;&#29289;&#29702;&#32422;&#26463;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#39640;&#38454;Lane-Emden-Fowler&#31867;&#22411;&#26041;&#31243;&#65306;&#22522;&#20934;&#27979;&#35797;&#27604;&#36739;&#36719;&#32422;&#26463;&#21644;&#30828;&#32422;&#26463;
&lt;/p&gt;
&lt;p&gt;
Solving higher-order Lane-Emden-Fowler type equations using physics-informed neural networks: benchmark tests comparing soft and hard constraints. (arXiv:2307.07302v1 [physics.comp-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07302
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#29289;&#29702;&#32422;&#26463;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#25104;&#21151;&#35299;&#20915;&#20102;&#39640;&#38454;Lane-Emden-Fowler&#31867;&#22411;&#26041;&#31243;&#65292;&#36890;&#36807;&#27604;&#36739;&#36719;&#32422;&#26463;&#21644;&#30828;&#32422;&#26463;&#30340;&#21464;&#20307;&#65292;&#24471;&#20986;&#20102;&#26377;&#25928;&#30340;&#25968;&#20540;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#29289;&#29702;&#32422;&#26463;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#35299;&#20915;&#39640;&#38454;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODEs&#65289;&#30340;&#25968;&#20540;&#26041;&#27861;&#12290;&#20107;&#23454;&#19978;&#65292;&#36825;&#31181;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#35299;&#20915;&#19981;&#21516;&#31867;&#21035;&#30340;&#22855;&#24322;ODEs&#65292;&#21363;&#30528;&#21517;&#30340;&#20108;&#38454;Lane-Emden&#26041;&#31243;&#12289;&#19977;&#38454;Emden-Fowler&#26041;&#31243;&#21644;&#22235;&#38454;Lane-Emden-Fowler&#26041;&#31243;&#12290;&#23545;PINNs&#25216;&#26415;&#30340;&#20004;&#20010;&#21464;&#20307;&#36827;&#34892;&#20102;&#32771;&#34385;&#21644;&#27604;&#36739;&#12290;&#39318;&#20808;&#65292;&#20351;&#29992;&#26368;&#23567;&#21270;&#36807;&#31243;&#26469;&#32422;&#26463;&#31070;&#32463;&#32593;&#32476;&#30340;&#24635;&#25439;&#22833;&#20989;&#25968;&#65292;&#22312;&#20854;&#20013;&#23558;&#26041;&#31243;&#27531;&#24046;&#32771;&#34385;&#20026;&#20855;&#26377;&#19968;&#23450;&#26435;&#37325;&#30340;&#22522;&#20110;&#29289;&#29702;&#30340;&#25439;&#22833;&#65292;&#24182;&#28155;&#21152;&#21040;&#21253;&#21547;&#21021;&#22987;/&#36793;&#30028;&#26465;&#20214;&#30340;&#35757;&#32451;&#25968;&#25454;&#25439;&#22833;&#20013;&#12290;&#20854;&#27425;&#65292;&#20026;&#20102;&#28385;&#36275;&#24494;&#20998;&#26041;&#31243;&#65292;&#37319;&#29992;&#20102;&#19968;&#31181;&#29305;&#23450;&#30340;&#35797;&#39564;&#35299;&#36873;&#25321;&#65292;&#30830;&#20445;&#36825;&#20123;&#26465;&#20214;&#25104;&#20026;&#30828;&#32422;&#26463;&#65292;&#19982;&#31532;&#19968;&#20010;&#21464;&#20307;&#22522;&#20110;&#35757;&#32451;&#25968;&#25454;&#30340;&#32422;&#26463;&#19981;&#21516;&#65292;&#32422;&#26463;&#20197;&#36719;&#32422;&#26463;&#24418;&#24335;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, numerical methods using Physics-Informed Neural Networks (PINNs) are presented with the aim to solve higher-order ordinary differential equations (ODEs). Indeed, this deep-learning technique is successfully applied for solving different classes of singular ODEs, namely the well known second-order Lane-Emden equations, third order-order Emden-Fowler equations, and fourth-order Lane-Emden-Fowler equations. Two variants of PINNs technique are considered and compared. First, a minimization procedure is used to constrain the total loss function of the neural network, in which the equation residual is considered with some weight to form a physics-based loss and added to the training data loss that contains the initial/boundary conditions. Second, a specific choice of trial solutions ensuring these conditions as hard constraints is done in order to satisfy the differential equation, contrary to the first variant based on training data where the constraints appear as soft ones. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#23436;&#25972;&#30340;3D&#24515;&#33039;&#24418;&#29366;&#65288;&#20197;&#28857;&#20113;&#24418;&#24335;&#65289;&#22312;&#25913;&#21892;&#24515;&#32908;&#26775;&#27515;&#65288;MI&#65289;&#20107;&#20214;&#26816;&#27979;&#26041;&#38754;&#30340;&#23454;&#29992;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;&#28857;&#20113;&#20998;&#31867;&#32593;&#32476;&#36827;&#34892;&#22810;&#27493;&#39588;&#30340;&#33258;&#21160;&#21270;&#27969;&#31243;&#65292;&#32467;&#21512;&#28857;&#20113;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#25105;&#20204;&#22312;&#33521;&#22269;&#29983;&#29289;&#38134;&#34892;&#21463;&#35797;&#32773;&#19978;&#24471;&#21040;&#20102;&#32422;13%&#21644;&#32422;5%&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.07298</link><description>&lt;p&gt;
&#22522;&#20110;&#28857;&#20113;&#20998;&#31867;&#32593;&#32476;&#30340;&#22522;&#20110;3D&#24418;&#29366;&#30340;&#24515;&#32908;&#26775;&#27515;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
3D Shape-Based Myocardial Infarction Prediction Using Point Cloud Classification Networks. (arXiv:2307.07298v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07298
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#23436;&#25972;&#30340;3D&#24515;&#33039;&#24418;&#29366;&#65288;&#20197;&#28857;&#20113;&#24418;&#24335;&#65289;&#22312;&#25913;&#21892;&#24515;&#32908;&#26775;&#27515;&#65288;MI&#65289;&#20107;&#20214;&#26816;&#27979;&#26041;&#38754;&#30340;&#23454;&#29992;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;&#28857;&#20113;&#20998;&#31867;&#32593;&#32476;&#36827;&#34892;&#22810;&#27493;&#39588;&#30340;&#33258;&#21160;&#21270;&#27969;&#31243;&#65292;&#32467;&#21512;&#28857;&#20113;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#25105;&#20204;&#22312;&#33521;&#22269;&#29983;&#29289;&#38134;&#34892;&#21463;&#35797;&#32773;&#19978;&#24471;&#21040;&#20102;&#32422;13%&#21644;&#32422;5%&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#32908;&#26775;&#27515;&#65288;MI&#65289;&#26159;&#26368;&#24120;&#35265;&#30340;&#24515;&#34880;&#31649;&#30142;&#30149;&#20043;&#19968;&#65292;&#30456;&#20851;&#30340;&#20020;&#24202;&#20915;&#31574;&#36890;&#24120;&#22522;&#20110;&#21333;&#20540;&#25104;&#20687;&#29983;&#29289;&#26631;&#24535;&#29289;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#25351;&#26631;&#21482;&#26159;&#36817;&#20284;&#20102;&#24515;&#33039;&#30340;&#22797;&#26434;3D&#32467;&#26500;&#21644;&#29983;&#29702;&#29305;&#24615;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#23545;MI&#32467;&#26524;&#30340;&#26356;&#22909;&#29702;&#35299;&#21644;&#39044;&#27979;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23436;&#25972;&#30340;3D&#24515;&#33039;&#24418;&#29366;&#65288;&#20197;&#28857;&#20113;&#24418;&#24335;&#65289;&#22312;&#25913;&#21892;MI&#20107;&#20214;&#26816;&#27979;&#26041;&#38754;&#30340;&#23454;&#29992;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#20840;&#33258;&#21160;&#21270;&#30340;&#22810;&#27493;&#39588;&#27969;&#31243;&#65292;&#21253;&#25324;3D&#24515;&#33039;&#34920;&#38754;&#37325;&#24314;&#27493;&#39588;&#21644;&#28857;&#20113;&#20998;&#31867;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#28857;&#20113;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#22312;&#39640;&#20998;&#36776;&#29575;&#24515;&#33039;&#35299;&#21078;&#27169;&#22411;&#19978;&#30340;&#30452;&#25509;&#21644;&#39640;&#25928;&#22810;&#23610;&#24230;&#23398;&#20064;&#12290;&#25105;&#20204;&#22312;1068&#20010;&#33521;&#22269;&#29983;&#29289;&#38134;&#34892;&#21463;&#35797;&#32773;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#24120;&#35265;MI&#26816;&#27979;&#21644;&#26032;&#21457;MI&#39044;&#27979;&#30340;&#20219;&#21153;&#65292;&#24182;&#21457;&#29616;&#20998;&#21035;&#25913;&#36827;&#20102;&#32422;13%&#21644;&#32422;5%&#12290;
&lt;/p&gt;
&lt;p&gt;
Myocardial infarction (MI) is one of the most prevalent cardiovascular diseases with associated clinical decision-making typically based on single-valued imaging biomarkers. However, such metrics only approximate the complex 3D structure and physiology of the heart and hence hinder a better understanding and prediction of MI outcomes. In this work, we investigate the utility of complete 3D cardiac shapes in the form of point clouds for an improved detection of MI events. To this end, we propose a fully automatic multi-step pipeline consisting of a 3D cardiac surface reconstruction step followed by a point cloud classification network. Our method utilizes recent advances in geometric deep learning on point clouds to enable direct and efficient multi-scale learning on high-resolution surface models of the cardiac anatomy. We evaluate our approach on 1068 UK Biobank subjects for the tasks of prevalent MI detection and incident MI prediction and find improvements of ~13% and ~5% respective
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#24050;&#26377;&#30340;Visual-Graph SLAM&#31639;&#27861;ExploreORB&#19982;&#24378;&#21270;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#20027;&#29615;&#22659;&#20013;&#22522;&#20110;&#21069;&#27839;&#25506;&#32034;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#22522;&#20110;&#22870;&#21169;&#30340;&#31995;&#32479;&#23398;&#20064;&#21644;&#20248;&#21270;&#25506;&#32034;&#36335;&#32447;&#65292;&#35299;&#20915;&#20102;&#22312;&#23384;&#22312;&#22810;&#20010;&#36317;&#31163;&#30456;&#20284;&#30340;&#21069;&#27839;&#30340;&#22330;&#26223;&#20013;&#21487;&#33021;&#23548;&#33268;&#38750;&#26368;&#20248;&#36335;&#24452;&#30340;&#38382;&#39064;&#12290;&#33521;&#25991;&#24635;&#32467;: This paper proposes a reinforcement learning algorithm with frontier-based exploration in autonomous environments by combining an existing Visual-Graph SLAM algorithm, ExploreORB. The algorithm allows the robot to learn and optimize exploration routes through a reward-based system to select frontiers accurately, addressing the issue of non-optimal paths in scenarios with similar-distance frontiers.</title><link>http://arxiv.org/abs/2307.07296</link><description>&lt;p&gt;
&#33258;&#20027;&#29615;&#22659;&#20013;&#22522;&#20110;&#21069;&#27839;&#25506;&#32034;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning with Frontier-Based Exploration via Autonomous Environment. (arXiv:2307.07296v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07296
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#24050;&#26377;&#30340;Visual-Graph SLAM&#31639;&#27861;ExploreORB&#19982;&#24378;&#21270;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#20027;&#29615;&#22659;&#20013;&#22522;&#20110;&#21069;&#27839;&#25506;&#32034;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#22522;&#20110;&#22870;&#21169;&#30340;&#31995;&#32479;&#23398;&#20064;&#21644;&#20248;&#21270;&#25506;&#32034;&#36335;&#32447;&#65292;&#35299;&#20915;&#20102;&#22312;&#23384;&#22312;&#22810;&#20010;&#36317;&#31163;&#30456;&#20284;&#30340;&#21069;&#27839;&#30340;&#22330;&#26223;&#20013;&#21487;&#33021;&#23548;&#33268;&#38750;&#26368;&#20248;&#36335;&#24452;&#30340;&#38382;&#39064;&#12290;&#33521;&#25991;&#24635;&#32467;: This paper proposes a reinforcement learning algorithm with frontier-based exploration in autonomous environments by combining an existing Visual-Graph SLAM algorithm, ExploreORB. The algorithm allows the robot to learn and optimize exploration routes through a reward-based system to select frontiers accurately, addressing the issue of non-optimal paths in scenarios with similar-distance frontiers.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#21516;&#26102;&#23450;&#20301;&#21450;&#24314;&#22270;&#65288;SLAM&#65289;&#26159;&#33258;&#20027;&#26426;&#22120;&#20154;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#24314;&#31435;&#20934;&#30830;&#30340;&#29615;&#22659;&#27169;&#22411;&#30340;&#21516;&#26102;&#23548;&#33322;&#21040;&#26032;&#30340;&#21306;&#22495;&#12290;&#35270;&#35273;SLAM&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#25216;&#26415;&#65292;&#21033;&#29992;&#34394;&#25311;&#20803;&#32032;&#26469;&#22686;&#24378;&#20307;&#39564;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#21069;&#27839;&#25506;&#32034;&#31574;&#30053;&#22312;&#23384;&#22312;&#22810;&#20010;&#36317;&#31163;&#30456;&#20284;&#30340;&#21069;&#27839;&#30340;&#22330;&#26223;&#20013;&#21487;&#33021;&#23548;&#33268;&#38750;&#26368;&#20248;&#30340;&#36335;&#24452;&#12290;&#36825;&#20010;&#38382;&#39064;&#21487;&#33021;&#20250;&#24433;&#21709;&#21040;&#35270;&#35273;SLAM&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#65292;&#32780;&#36825;&#23545;&#20110;&#19968;&#31995;&#21015;&#26426;&#22120;&#20154;&#24212;&#29992;&#38750;&#24120;&#20851;&#38190;&#65292;&#22914;&#25628;&#25937;&#12289;&#25506;&#32034;&#21644;&#24314;&#22270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#23558;&#24050;&#26377;&#30340;ExploreORB&#35270;&#35273;&#22270;SLAM&#19982;&#24378;&#21270;&#23398;&#20064;&#30456;&#32467;&#21512;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#20801;&#35768;&#26426;&#22120;&#20154;&#36890;&#36807;&#22522;&#20110;&#22870;&#21169;&#30340;&#31995;&#32479;&#23398;&#20064;&#21644;&#20248;&#21270;&#25506;&#32034;&#36335;&#32447;&#65292;&#20197;&#27491;&#30830;&#36873;&#25321;&#21069;&#27839;&#65292;&#24182;&#21019;&#24314;&#19968;&#20010;&#31934;&#30830;&#30340;&#29615;&#22659;&#22320;&#22270;&#12290;&#22522;&#20110;&#21069;&#27839;&#25506;&#32034;&#34987;&#29992;&#26469;&#26816;&#27979;&#26410;&#25506;&#32034;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Active Simultaneous Localisation and Mapping (SLAM) is a critical problem in autonomous robotics, enabling robots to navigate to new regions while building an accurate model of their surroundings. Visual SLAM is a popular technique that uses virtual elements to enhance the experience. However, existing frontier-based exploration strategies can lead to a non-optimal path in scenarios where there are multiple frontiers with similar distance. This issue can impact the efficiency and accuracy of Visual SLAM, which is crucial for a wide range of robotic applications, such as search and rescue, exploration, and mapping. To address this issue, this research combines both an existing Visual-Graph SLAM known as ExploreORB with reinforcement learning. The proposed algorithm allows the robot to learn and optimize exploration routes through a reward-based system to create an accurate map of the environment with proper frontier selection. Frontier-based exploration is used to detect unexplored area
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31283;&#20581;&#30340;&#20307;&#31215;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#30340;&#39057;&#22495;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#20854;&#30456;&#23545;&#20110;&#20256;&#32479;&#25915;&#20987;&#26041;&#27861;&#20855;&#26377;&#20248;&#21183;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#39057;&#22495;&#23545;&#25239;&#25915;&#20987;&#65292;&#36890;&#36807;&#24341;&#20837;&#39057;&#29575;&#19968;&#33268;&#24615;&#25439;&#22833;&#26469;&#20248;&#21270;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#23454;&#29616;&#23545;&#20307;&#32032;&#21644;&#39057;&#22495;&#25915;&#20987;&#30340;&#38450;&#24481;&#12290;</title><link>http://arxiv.org/abs/2307.07269</link><description>&lt;p&gt;
&#39057;&#22495;&#23545;&#25239;&#35757;&#32451;&#29992;&#20110;&#31283;&#20581;&#30340;&#20307;&#31215;&#21307;&#23398;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Frequency Domain Adversarial Training for Robust Volumetric Medical Segmentation. (arXiv:2307.07269v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07269
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31283;&#20581;&#30340;&#20307;&#31215;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#30340;&#39057;&#22495;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#20854;&#30456;&#23545;&#20110;&#20256;&#32479;&#25915;&#20987;&#26041;&#27861;&#20855;&#26377;&#20248;&#21183;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#39057;&#22495;&#23545;&#25239;&#25915;&#20987;&#65292;&#36890;&#36807;&#24341;&#20837;&#39057;&#29575;&#19968;&#33268;&#24615;&#25439;&#22833;&#26469;&#20248;&#21270;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#23454;&#29616;&#23545;&#20307;&#32032;&#21644;&#39057;&#22495;&#25915;&#20987;&#30340;&#38450;&#24481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20851;&#38190;&#24212;&#29992;&#65288;&#22914;&#21307;&#30103;&#20445;&#20581;&#65289;&#20013;&#65292;&#30830;&#20445;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#26368;&#36817;&#22312;&#20307;&#31215;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#20854;&#26131;&#21463;&#23545;&#25239;&#25915;&#20987;&#30340;&#24369;&#28857;&#65292;&#36825;&#20123;&#27169;&#22411;&#26080;&#27861;&#31435;&#21363;&#29992;&#20110;&#23454;&#38469;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20307;&#31215;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#30340;&#19977;&#32500;&#39057;&#22495;&#23545;&#25239;&#25915;&#20987;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#30456;&#23545;&#20110;&#20256;&#32479;&#36755;&#20837;&#25110;&#20307;&#32032;&#22495;&#25915;&#20987;&#30340;&#20248;&#21183;&#12290;&#21033;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#25915;&#20987;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39057;&#22495;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#65292;&#20197;&#20248;&#21270;&#38024;&#23545;&#20307;&#32032;&#21644;&#39057;&#22495;&#25915;&#20987;&#30340;&#31283;&#20581;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#39057;&#29575;&#19968;&#33268;&#24615;&#25439;&#22833;&#26469;&#35843;&#33410;&#25105;&#20204;&#30340;&#39057;&#22495;&#23545;&#25239;&#35757;&#32451;&#65292;&#20197;&#22312;&#28165;&#27905;&#26679;&#21697;&#21644;&#23545;&#25239;&#26679;&#26412;&#20043;&#38388;&#21462;&#24471;&#26356;&#22909;&#30340;&#24179;&#34913;&#12290;&#20195;&#30721;&#21487;&#20844;&#24320;&#35775;&#38382;https://github.com/asif-hanif/vafa&#12290;
&lt;/p&gt;
&lt;p&gt;
It is imperative to ensure the robustness of deep learning models in critical applications such as, healthcare. While recent advances in deep learning have improved the performance of volumetric medical image segmentation models, these models cannot be deployed for real-world applications immediately due to their vulnerability to adversarial attacks. We present a 3D frequency domain adversarial attack for volumetric medical image segmentation models and demonstrate its advantages over conventional input or voxel domain attacks. Using our proposed attack, we introduce a novel frequency domain adversarial training approach for optimizing a robust model against voxel and frequency domain attacks. Moreover, we propose frequency consistency loss to regulate our frequency domain adversarial training that achieves a better tradeoff between model's performance on clean and adversarial samples. Code is publicly available at https://github.com/asif-hanif/vafa.
&lt;/p&gt;</description></item><item><title>&#23398;&#20064;&#19987;&#23478;&#24314;&#35758;&#21644;&#22810;&#33218;&#36172;&#21338;&#26159;&#20004;&#20010;&#32463;&#20856;&#30340;&#22312;&#32447;&#20915;&#31574;&#38382;&#39064;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#32773;&#20043;&#38388;&#30340;&#25554;&#20540;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;$\mathbf{m}$-MAB&#30340;&#26497;&#23567;&#21518;&#24724;&#30028;&#24182;&#35774;&#35745;&#20102;$\mathbf{m}$-BAI&#30340;&#26368;&#20248;PAC&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#26088;&#22312;&#20197;&#23613;&#21487;&#33021;&#23569;&#30340;&#36718;&#25968;&#30830;&#23450;&#25439;&#22833;&#26368;&#23567;&#30340;&#33218;&#12290;</title><link>http://arxiv.org/abs/2307.07264</link><description>&lt;p&gt;
&#20851;&#20110;&#25554;&#20540;&#19987;&#23478;&#21644;&#22810;&#33218;&#36172;&#21338;&#26426;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Interpolating Experts and Multi-Armed Bandits. (arXiv:2307.07264v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07264
&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#19987;&#23478;&#24314;&#35758;&#21644;&#22810;&#33218;&#36172;&#21338;&#26159;&#20004;&#20010;&#32463;&#20856;&#30340;&#22312;&#32447;&#20915;&#31574;&#38382;&#39064;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#32773;&#20043;&#38388;&#30340;&#25554;&#20540;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;$\mathbf{m}$-MAB&#30340;&#26497;&#23567;&#21518;&#24724;&#30028;&#24182;&#35774;&#35745;&#20102;$\mathbf{m}$-BAI&#30340;&#26368;&#20248;PAC&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#26088;&#22312;&#20197;&#23613;&#21487;&#33021;&#23569;&#30340;&#36718;&#25968;&#30830;&#23450;&#25439;&#22833;&#26368;&#23567;&#30340;&#33218;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#19987;&#23478;&#24314;&#35758;&#21644;&#22810;&#33218;&#36172;&#21338;&#26159;&#20004;&#20010;&#32463;&#20856;&#30340;&#22312;&#32447;&#20915;&#31574;&#38382;&#39064;&#65292;&#23427;&#20204;&#22312;&#27599;&#19968;&#36718;&#35266;&#23519;&#20449;&#24687;&#30340;&#26041;&#24335;&#19978;&#26377;&#25152;&#19981;&#21516;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20004;&#32773;&#20043;&#38388;&#30340;&#25554;&#20540;&#38382;&#39064;&#12290;&#23545;&#20110;&#21521;&#37327;$\mathbf{m}=(m_1,\dots,m_K)\in \mathbb{N}^K$&#65292;$\mathbf{m}$-MAB&#30340;&#19968;&#20010;&#23454;&#20363;&#34920;&#31034;&#23558;&#33218;&#20998;&#25104;$K$&#32452;&#65292;&#31532;$i$&#32452;&#21253;&#21547;$m_i$&#20010;&#33218;&#12290;&#19968;&#26086;&#25289;&#21160;&#19968;&#20010;&#33218;&#65292;&#21516;&#19968;&#32452;&#20013;&#25152;&#26377;&#33218;&#30340;&#25439;&#22833;&#37117;&#34987;&#35266;&#23519;&#21040;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;$\mathbf{m}$-MAB&#30340;&#32039;&#33268;&#26497;&#23567;&#21518;&#24724;&#30028;&#65292;&#24182;&#20026;&#20854;&#32431;&#25506;&#32034;&#29256;&#26412;$\mathbf{m}$-BAI&#35774;&#35745;&#20102;&#19968;&#20010;&#26368;&#20248;&#30340;PAC&#31639;&#27861;&#65292;&#20854;&#20013;&#30446;&#26631;&#26159;&#29992;&#23613;&#21487;&#33021;&#23569;&#30340;&#36718;&#25968;&#26469;&#35782;&#21035;&#25439;&#22833;&#26368;&#23567;&#30340;&#33218;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;$\mathbf{m}$-MAB&#30340;&#26497;&#23567;&#21518;&#24724;&#26159;$\Theta\left(\sqrt{T\sum_{k=1}^K\log (m_k+1)}\right)$&#65292;&#23545;&#20110;&#19968;&#20010;$(\epsilon,0.05)$-PAC&#31639;&#27861;&#30340;$\mathbf{m}$-BAI&#65292;&#25289;&#21160;&#33218;&#30340;&#26368;&#23567;&#27425;&#25968;&#26159;$\Theta\left(\frac{1}{\epsilon^2}\cdot \sum_{k=1}^K\log (m_k+1)\right)$&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning with expert advice and multi-armed bandit are two classic online decision problems which differ on how the information is observed in each round of the game. We study a family of problems interpolating the two. For a vector $\mathbf{m}=(m_1,\dots,m_K)\in \mathbb{N}^K$, an instance of $\mathbf{m}$-MAB indicates that the arms are partitioned into $K$ groups and the $i$-th group contains $m_i$ arms. Once an arm is pulled, the losses of all arms in the same group are observed. We prove tight minimax regret bounds for $\mathbf{m}$-MAB and design an optimal PAC algorithm for its pure exploration version, $\mathbf{m}$-BAI, where the goal is to identify the arm with minimum loss with as few rounds as possible. We show that the minimax regret of $\mathbf{m}$-MAB is $\Theta\left(\sqrt{T\sum_{k=1}^K\log (m_k+1)}\right)$ and the minimum number of pulls for an $(\epsilon,0.05)$-PAC algorithm of $\mathbf{m}$-BAI is $\Theta\left(\frac{1}{\epsilon^2}\cdot \sum_{k=1}^K\log (m_k+1)\right)$. Bot
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25932;&#23545;&#21452;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#37327;&#21270;&#21644;&#32531;&#35299;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#38754;&#23545;&#25932;&#23545;&#36755;&#20837;&#26102;&#30340;&#33030;&#24369;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.07250</link><description>&lt;p&gt;
&#36890;&#36807;&#25932;&#23545;&#21452;&#26426;&#22120;&#23398;&#20064;&#30340;&#22240;&#26524;&#21442;&#25968;&#20272;&#35745;&#26469;&#32531;&#35299;&#25932;&#23545;&#33030;&#24369;&#24615;
&lt;/p&gt;
&lt;p&gt;
Mitigating Adversarial Vulnerability through Causal Parameter Estimation by Adversarial Double Machine Learning. (arXiv:2307.07250v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07250
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25932;&#23545;&#21452;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#37327;&#21270;&#21644;&#32531;&#35299;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#38754;&#23545;&#25932;&#23545;&#36755;&#20837;&#26102;&#30340;&#33030;&#24369;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#35270;&#35273;&#36755;&#20837;&#20013;&#34893;&#29983;&#20986;&#30340;&#25932;&#23545;&#20363;&#23376;&#21487;&#20197;&#36731;&#26494;&#22320;&#25439;&#23475;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#20026;&#20102;&#38450;&#27490;&#28508;&#22312;&#30340;&#23041;&#32961;&#65292;&#21508;&#31181;&#22522;&#20110;&#25932;&#23545;&#35757;&#32451;&#30340;&#38450;&#24481;&#26041;&#27861;&#36805;&#36895;&#22686;&#38271;&#65292;&#24182;&#25104;&#20026;&#31283;&#20581;&#24615;&#30340;&#20107;&#23454;&#19978;&#26631;&#20934;&#26041;&#27861;&#12290;&#23613;&#31649;&#26368;&#36817;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#25104;&#23601;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#25932;&#23545;&#33030;&#24369;&#24615;&#22312;&#19981;&#21516;&#30446;&#26631;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#65292;&#24182;&#19988;&#26576;&#20123;&#33030;&#24369;&#24615;&#20173;&#28982;&#26222;&#36941;&#23384;&#22312;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#21363;&#20351;&#20351;&#29992;&#26356;&#28145;&#23618;&#27425;&#30340;&#26550;&#26500;&#21644;&#20808;&#36827;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#36825;&#31181;&#22855;&#29305;&#30340;&#29616;&#35937;&#20173;&#28982;&#26080;&#27861;&#32531;&#35299;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#25932;&#23545;&#21452;&#26426;&#22120;&#23398;&#20064;&#65288;ADML&#65289;&#30340;&#22240;&#26524;&#26041;&#27861;&#65292;&#23427;&#20801;&#35768;&#25105;&#20204;&#37327;&#21270;&#32593;&#32476;&#39044;&#27979;&#30340;&#25932;&#23545;&#33030;&#24369;&#24615;&#31243;&#24230;&#65292;&#24182;&#25429;&#25417;&#23545;&#32467;&#26524;&#30340;&#22788;&#29702;&#25928;&#26524;&#12290;ADML&#21487;&#20197;&#30452;&#25509;&#20272;&#35745;&#25932;&#23545;&#25200;&#21160;&#26412;&#36523;&#30340;&#22240;&#26524;&#21442;&#25968;&#65292;&#24182;&#20943;&#36731;&#21487;&#33021;&#25439;&#23475;&#31283;&#20581;&#24615;&#30340;&#36127;&#38754;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial examples derived from deliberately crafted perturbations on visual inputs can easily harm decision process of deep neural networks. To prevent potential threats, various adversarial training-based defense methods have grown rapidly and become a de facto standard approach for robustness. Despite recent competitive achievements, we observe that adversarial vulnerability varies across targets and certain vulnerabilities remain prevalent. Intriguingly, such peculiar phenomenon cannot be relieved even with deeper architectures and advanced defense methods. To address this issue, in this paper, we introduce a causal approach called Adversarial Double Machine Learning (ADML), which allows us to quantify the degree of adversarial vulnerability for network predictions and capture the effect of treatments on outcome of interests. ADML can directly estimate causal parameter of adversarial perturbations per se and mitigate negative effects that can potentially damage robustness, bridgi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;KoBo&#30340;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#23558;&#20020;&#24202;&#30693;&#35782;&#25972;&#21512;&#21040;&#35270;&#35273;&#35821;&#35328;&#35821;&#20041;&#19968;&#33268;&#24615;&#23398;&#20064;&#20013;&#65292;&#35299;&#20915;&#20102;&#21307;&#23398;&#39046;&#22495;&#20013;&#30340;&#22823;&#35268;&#27169;&#35821;&#20041;&#37325;&#21472;&#21644;&#36716;&#31227;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.07246</link><description>&lt;p&gt;
&#30693;&#35782;&#22686;&#24378;&#65306;&#37325;&#26032;&#24605;&#32771;&#21307;&#23398;&#23545;&#27604;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Knowledge Boosting: Rethinking Medical Contrastive Vision-Language Pre-Training. (arXiv:2307.07246v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07246
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;KoBo&#30340;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#23558;&#20020;&#24202;&#30693;&#35782;&#25972;&#21512;&#21040;&#35270;&#35273;&#35821;&#35328;&#35821;&#20041;&#19968;&#33268;&#24615;&#23398;&#20064;&#20013;&#65292;&#35299;&#20915;&#20102;&#21307;&#23398;&#39046;&#22495;&#20013;&#30340;&#22823;&#35268;&#27169;&#35821;&#20041;&#37325;&#21472;&#21644;&#36716;&#31227;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#39044;&#35757;&#32451;&#25216;&#26415;&#30340;&#22522;&#30784;&#27169;&#22411;&#20174;&#29702;&#35770;&#19978;&#21040;&#23454;&#36341;&#24212;&#29992;&#26174;&#33879;&#25512;&#36827;&#20102;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#12290;&#36825;&#20123;&#27169;&#22411;&#25512;&#21160;&#20102;&#35745;&#31639;&#26426;&#36741;&#21161;&#35786;&#26029;&#30340;&#21487;&#34892;&#24615;&#65292;&#20351;&#20854;&#24191;&#27867;&#24212;&#29992;&#12290;&#21307;&#23398;&#23545;&#27604;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#35786;&#26029;&#25253;&#21578;&#20013;&#30340;&#25551;&#36848;&#20449;&#24687;&#26469;&#25351;&#23548;&#34920;&#24449;&#23398;&#20064;&#65292;&#26080;&#38656;&#20154;&#24037;&#27880;&#37322;&#12290;&#28982;&#32780;&#65292;&#39044;&#35757;&#32451;&#30340;&#25928;&#26524;&#21463;&#21046;&#20110;&#21307;&#23398;&#39046;&#22495;&#30340;&#22823;&#35268;&#27169;&#35821;&#20041;&#37325;&#21472;&#21644;&#36716;&#31227;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#30693;&#35782;&#22686;&#24378;&#23545;&#27604;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#26694;&#26550;&#65288;KoBo&#65289;&#65292;&#35813;&#26694;&#26550;&#23558;&#20020;&#24202;&#30693;&#35782;&#25972;&#21512;&#21040;&#35270;&#35273;&#35821;&#35328;&#35821;&#20041;&#19968;&#33268;&#24615;&#23398;&#20064;&#20013;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#26080;&#20559;&#30340;&#12289;&#24320;&#38598;&#26679;&#26412;&#32423;&#30693;&#35782;&#34920;&#31034;&#26469;&#34913;&#37327;&#36127;&#26679;&#26412;&#22122;&#22768;&#65292;&#24182;&#34917;&#20805;&#35270;&#35273;&#35821;&#35328;&#20114;&#20449;&#24687;&#19982;&#20020;&#24202;&#30693;&#35782;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
The foundation models based on pre-training technology have significantly advanced artificial intelligence from theoretical to practical applications. These models have facilitated the feasibility of computer-aided diagnosis for widespread use. Medical contrastive vision-language pre-training, which does not require human annotations, is an effective approach for guiding representation learning using description information in diagnostic reports. However, the effectiveness of pre-training is limited by the large-scale semantic overlap and shifting problems in medical field. To address these issues, we propose the Knowledge-Boosting Contrastive Vision-Language Pre-training framework (KoBo), which integrates clinical knowledge into the learning of vision-language semantic consistency. The framework uses an unbiased, open-set sample-wise knowledge representation to measure negative sample noise and supplement the correspondence between vision-language mutual information and clinical knowl
&lt;/p&gt;</description></item><item><title>Ed-Fed&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#36793;&#32536;&#35774;&#22791;&#36164;&#28304;&#24863;&#30693;&#23458;&#25143;&#31471;&#36873;&#25321;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#20248;&#21270;&#31561;&#24453;&#26102;&#38388;&#65292;&#22788;&#29702;&#24930;&#36895;&#35774;&#22791;&#65292;&#24182;&#19988;&#22312;FL&#20013;&#26174;&#33879;&#20248;&#21270;&#20102;&#31561;&#24453;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2307.07199</link><description>&lt;p&gt;
Ed-Fed&#65306;&#19968;&#31181;&#36890;&#29992;&#30340;&#36793;&#32536;&#35774;&#22791;&#36164;&#28304;&#24863;&#30693;&#23458;&#25143;&#31471;&#36873;&#25321;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Ed-Fed: A generic federated learning framework with resource-aware client selection for edge devices. (arXiv:2307.07199v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07199
&lt;/p&gt;
&lt;p&gt;
Ed-Fed&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#36793;&#32536;&#35774;&#22791;&#36164;&#28304;&#24863;&#30693;&#23458;&#25143;&#31471;&#36873;&#25321;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#20248;&#21270;&#31561;&#24453;&#26102;&#38388;&#65292;&#22788;&#29702;&#24930;&#36895;&#35774;&#22791;&#65292;&#24182;&#19988;&#22312;FL&#20013;&#26174;&#33879;&#20248;&#21270;&#20102;&#31561;&#24453;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#36793;&#32536;&#35774;&#22791;&#21512;&#20316;&#21019;&#24314;&#32479;&#19968;&#39044;&#27979;&#27169;&#22411;&#24182;&#20445;&#25252;&#20854;&#25935;&#24863;&#35757;&#32451;&#25968;&#25454;&#30340;&#37325;&#35201;&#26041;&#27861;&#12290;&#23613;&#31649;&#23384;&#22312;&#35768;&#22810;&#29992;&#20110;&#27169;&#25311;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#30340;&#30740;&#31350;&#26694;&#26550;&#65292;&#20294;&#23427;&#20204;&#19981;&#33021;&#22312;&#24322;&#26500;&#36793;&#32536;&#35774;&#22791;&#19978;&#20840;&#38754;&#37096;&#32626;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;Ed-Fed&#20316;&#20026;&#19968;&#20010;&#32508;&#21512;&#19988;&#36890;&#29992;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#25104;&#20026;&#26410;&#26469;&#23454;&#38469;FL&#31995;&#32479;&#30740;&#31350;&#30340;&#22522;&#30784;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#36164;&#28304;&#24863;&#30693;&#23458;&#25143;&#31471;&#36873;&#25321;&#31639;&#27861;&#65292;&#22312;FL&#29615;&#22659;&#20013;&#20248;&#21270;&#31561;&#24453;&#26102;&#38388;&#12290;&#25105;&#20204;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22788;&#29702;&#24930;&#36895;&#35774;&#22791;&#65292;&#24182;&#21160;&#24577;&#35774;&#32622;&#36873;&#23450;&#35774;&#22791;&#30340;&#35757;&#32451;&#26102;&#38388;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#19982;&#20256;&#32479;&#30340;&#38543;&#26426;&#23458;&#25143;&#31471;&#36873;&#25321;&#26041;&#27861;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26174;&#33879;&#20248;&#21270;&#20102;FL&#20013;&#30340;&#31561;&#24453;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) has evolved as a prominent method for edge devices to cooperatively create a unified prediction model while securing their sensitive training data local to the device. Despite the existence of numerous research frameworks for simulating FL algorithms, they do not facilitate comprehensive deployment for automatic speech recognition tasks on heterogeneous edge devices. This is where Ed-Fed, a comprehensive and generic FL framework, comes in as a foundation for future practical FL system research. We also propose a novel resource-aware client selection algorithm to optimise the waiting time in the FL settings. We show that our approach can handle the straggler devices and dynamically set the training time for the selected devices in a round. Our evaluation has shown that the proposed approach significantly optimises waiting time in FL compared to conventional random client selection methods.
&lt;/p&gt;</description></item><item><title>&#29992;&#26426;&#22120;&#23398;&#20064;&#25511;&#21046;&#21160;&#21147;&#31995;&#32479;&#21040;&#22797;&#26434;&#30340;&#30446;&#26631;&#29366;&#24577;&#65292;&#27604;&#36739;&#20102;&#19979;&#19968;&#20195;&#20648;&#22791;&#35745;&#31639;&#21644;&#20256;&#32479;&#20648;&#22791;&#35745;&#31639;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#25968;&#25454;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#19979;&#19968;&#20195;&#20648;&#22791;&#35745;&#31639;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#20026;&#23454;&#38469;&#25511;&#21046;&#24212;&#29992;&#25552;&#20379;&#20102;&#26356;&#22810;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.07195</link><description>&lt;p&gt;
&#29992;&#26426;&#22120;&#23398;&#20064;&#25511;&#21046;&#21160;&#21147;&#31995;&#32479;&#21040;&#22797;&#26434;&#30340;&#30446;&#26631;&#29366;&#24577;&#65306;&#19979;&#19968;&#20195;&#19982;&#20256;&#32479;&#20648;&#22791;&#35745;&#31639;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Controlling dynamical systems to complex target states using machine learning: next-generation vs. classical reservoir computing. (arXiv:2307.07195v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07195
&lt;/p&gt;
&lt;p&gt;
&#29992;&#26426;&#22120;&#23398;&#20064;&#25511;&#21046;&#21160;&#21147;&#31995;&#32479;&#21040;&#22797;&#26434;&#30340;&#30446;&#26631;&#29366;&#24577;&#65292;&#27604;&#36739;&#20102;&#19979;&#19968;&#20195;&#20648;&#22791;&#35745;&#31639;&#21644;&#20256;&#32479;&#20648;&#22791;&#35745;&#31639;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#25968;&#25454;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#19979;&#19968;&#20195;&#20648;&#22791;&#35745;&#31639;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#20026;&#23454;&#38469;&#25511;&#21046;&#24212;&#29992;&#25552;&#20379;&#20102;&#26356;&#22810;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#25511;&#21046;&#38750;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#19981;&#20165;&#21487;&#20197;&#20351;&#31995;&#32479;&#36827;&#20837;&#31616;&#21333;&#30340;&#21608;&#26399;&#34892;&#20026;&#65292;&#36824;&#21487;&#20197;&#23454;&#29616;&#26356;&#22797;&#26434;&#30340;&#20219;&#24847;&#21160;&#24577;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#20851;&#38190;&#22312;&#20110;&#35757;&#32451;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#33021;&#22815;&#24456;&#22909;&#22320;&#37325;&#29616;&#30446;&#26631;&#21160;&#21147;&#23398;&#12290;&#20197;&#23558;&#27931;&#20262;&#20857;&#31995;&#32479;&#30340;&#28151;&#27788;&#21442;&#25968;&#21270;&#39537;&#21160;&#20026;&#20363;&#65292;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#20256;&#32479;&#20648;&#22791;&#35745;&#31639;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#30340;&#20986;&#33394;&#34920;&#29616;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20351;&#29992;&#19981;&#21516;&#25968;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#22522;&#20110;&#19979;&#19968;&#20195;&#20648;&#22791;&#35745;&#31639;&#30340;&#32467;&#26524;&#19982;&#21478;&#19968;&#31181;&#35774;&#32622;&#30340;&#32467;&#26524;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#22312;&#36890;&#24120;&#30340;&#35757;&#32451;&#25968;&#25454;&#37327;&#19978;&#34920;&#29616;&#30456;&#24403;&#65292;&#20294;&#22312;&#21482;&#26377;&#38750;&#24120;&#26377;&#38480;&#30340;&#25968;&#25454;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#65292;&#19979;&#19968;&#20195;&#20648;&#22791;&#35745;&#31639;&#34920;&#29616;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;&#36825;&#20026;&#22312;&#25968;&#25454;&#21463;&#38480;&#30340;&#23454;&#38469;&#25511;&#21046;&#24212;&#29992;&#20013;&#36827;&#19968;&#27493;&#25552;&#20379;&#20102;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Controlling nonlinear dynamical systems using machine learning allows to not only drive systems into simple behavior like periodicity but also to more complex arbitrary dynamics. For this, it is crucial that a machine learning system can be trained to reproduce the target dynamics sufficiently well. On the example of forcing a chaotic parametrization of the Lorenz system into intermittent dynamics, we show first that classical reservoir computing excels at this task. In a next step, we compare those results based on different amounts of training data to an alternative setup, where next-generation reservoir computing is used instead. It turns out that while delivering comparable performance for usual amounts of training data, next-generation RC significantly outperforms in situations where only very limited data is available. This opens even further practical control applications in real world problems where data is restricted.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#30005;&#21147;&#36127;&#33655;&#39044;&#27979;&#23384;&#26723;&#65292;&#21253;&#25324;&#36127;&#33655;&#39046;&#22495;&#29305;&#23450;&#30340;&#29305;&#24449;&#24037;&#31243;&#65292;&#24110;&#21161;&#27169;&#22411;&#26356;&#22909;&#22320;&#27169;&#25311;&#36127;&#33655;&#25968;&#25454;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#26368;&#23567;&#21270;&#21518;&#32493;&#20219;&#21153;&#30340;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2307.07191</link><description>&lt;p&gt;
&#29992;&#20110;&#30005;&#21147;&#36127;&#33655;&#39044;&#27979;&#30340;&#22522;&#20934;&#21644;&#33258;&#23450;&#20041;&#21253;
&lt;/p&gt;
&lt;p&gt;
Benchmarks and Custom Package for Electrical Load Forecasting. (arXiv:2307.07191v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07191
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#30005;&#21147;&#36127;&#33655;&#39044;&#27979;&#23384;&#26723;&#65292;&#21253;&#25324;&#36127;&#33655;&#39046;&#22495;&#29305;&#23450;&#30340;&#29305;&#24449;&#24037;&#31243;&#65292;&#24110;&#21161;&#27169;&#22411;&#26356;&#22909;&#22320;&#27169;&#25311;&#36127;&#33655;&#25968;&#25454;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#26368;&#23567;&#21270;&#21518;&#32493;&#20219;&#21153;&#30340;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36127;&#33655;&#39044;&#27979;&#22312;&#30005;&#21147;&#34892;&#19994;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#21487;&#20197;&#20026;&#21518;&#32493;&#20219;&#21153;&#22914;&#30005;&#32593;&#35843;&#24230;&#25552;&#20379;&#21442;&#32771;&#65292;&#20174;&#32780;&#24102;&#26469;&#24040;&#22823;&#30340;&#32463;&#27982;&#25928;&#30410;&#12290;&#28982;&#32780;&#65292;&#36127;&#33655;&#39044;&#27979;&#19982;&#20256;&#32479;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20043;&#38388;&#23384;&#22312;&#35768;&#22810;&#24046;&#24322;&#12290;&#19968;&#26041;&#38754;&#65292;&#36127;&#33655;&#39044;&#27979;&#30340;&#30446;&#26631;&#26159;&#26368;&#23567;&#21270;&#21518;&#32493;&#20219;&#21153;&#65288;&#22914;&#30005;&#32593;&#35843;&#24230;&#65289;&#30340;&#25104;&#26412;&#65292;&#32780;&#19981;&#20165;&#20165;&#36861;&#27714;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#36127;&#33655;&#21463;&#21040;&#35768;&#22810;&#22806;&#37096;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#22914;&#28201;&#24230;&#25110;&#26085;&#21382;&#21464;&#37327;&#12290;&#27492;&#22806;&#65292;&#39044;&#27979;&#30340;&#35268;&#27169;&#65288;&#22914;&#24314;&#31569;&#32423;&#36127;&#33655;&#21644;&#32858;&#21512;&#32423;&#36127;&#33655;&#65289;&#20063;&#20250;&#23545;&#39044;&#27979;&#32467;&#26524;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#36127;&#33655;&#39044;&#27979;&#23384;&#26723;&#65292;&#20854;&#20013;&#21253;&#25324;&#36127;&#33655;&#39046;&#22495;&#29305;&#23450;&#30340;&#29305;&#24449;&#24037;&#31243;&#65292;&#20197;&#24110;&#21161;&#39044;&#27979;&#27169;&#22411;&#26356;&#22909;&#22320;&#27169;&#25311;&#36127;&#33655;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#19982;&#20256;&#32479;&#30340;&#25439;&#22833;&#20989;&#25968;&#20165;&#36861;&#27714;&#20934;&#30830;&#24615;&#19981;&#21516;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;...
&lt;/p&gt;
&lt;p&gt;
Load forecasting is of great significance in the power industry as it can provide a reference for subsequent tasks such as power grid dispatch, thus bringing huge economic benefits. However, there are many differences between load forecasting and traditional time series forecasting. On the one hand, load forecasting aims to minimize the cost of subsequent tasks such as power grid dispatch, rather than simply pursuing prediction accuracy. On the other hand, the load is largely influenced by many external factors, such as temperature or calendar variables. In addition, the scale of predictions (such as building-level loads and aggregated-level loads) can also significantly impact the predicted results. In this paper, we provide a comprehensive load forecasting archive, which includes load domain-specific feature engineering to help forecasting models better model load data. In addition, different from the traditional loss function which only aims for accuracy, we also provide a method to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#20056;&#27861;&#26356;&#26032;&#35268;&#21017;&#65292;&#21152;&#36895;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#24182;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.07189</link><description>&lt;p&gt;
&#29992;&#20110;&#21152;&#36895;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#21644;&#25552;&#39640;&#40065;&#26834;&#24615;&#30340;&#20056;&#27861;&#26356;&#26032;&#35268;&#21017;
&lt;/p&gt;
&lt;p&gt;
Multiplicative update rules for accelerating deep learning training and increasing robustness. (arXiv:2307.07189v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07189
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#20056;&#27861;&#26356;&#26032;&#35268;&#21017;&#65292;&#21152;&#36895;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#24182;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#22312;&#35768;&#22810;&#30740;&#31350;&#39046;&#22495;&#24050;&#32463;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#20173;&#28982;&#21152;&#36895;&#35757;&#32451;&#21644;&#26500;&#24314;&#40065;&#26834;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#20248;&#21270;&#31639;&#27861;&#65292;&#24182;&#20351;&#24471;&#21487;&#20197;&#24212;&#29992;&#26367;&#20195;&#30340;&#21442;&#25968;&#26356;&#26032;&#26041;&#27861;&#65292;&#20174;&#32780;&#21152;&#24555;&#35757;&#32451;&#21644;&#22686;&#24378;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Even nowadays, where Deep Learning (DL) has achieved state-of-the-art performance in a wide range of research domains, accelerating training and building robust DL models remains a challenging task. To this end, generations of researchers have pursued to develop robust methods for training DL architectures that can be less sensitive to weight distributions, model architectures and loss landscapes. However, such methods are limited to adaptive learning rate optimizers, initialization schemes, and clipping gradients without investigating the fundamental rule of parameters update. Although multiplicative updates have contributed significantly to the early development of machine learning and hold strong theoretical claims, to best of our knowledge, this is the first work that investigate them in context of DL training acceleration and robustness. In this work, we propose an optimization framework that fits to a wide range of optimization algorithms and enables one to apply alternative upda
&lt;/p&gt;</description></item><item><title>DISPEL&#26159;&#19968;&#31181;&#36890;&#36807;&#21518;&#22788;&#29702;&#32454;&#31890;&#24230;&#25513;&#34109;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#36807;&#28388;&#25481;&#26410;&#23450;&#20041;&#21644;&#26080;&#27861;&#21306;&#20998;&#30340;&#39046;&#22495;&#29305;&#23450;&#29305;&#24449;&#30340;&#39046;&#22495;&#27867;&#21270;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.07181</link><description>&lt;p&gt;
DISPEL&#65306;&#36890;&#36807;&#39046;&#22495;&#29305;&#23450;&#35299;&#25918;&#36827;&#34892;&#22495;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
DISPEL: Domain Generalization via Domain-Specific Liberating. (arXiv:2307.07181v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07181
&lt;/p&gt;
&lt;p&gt;
DISPEL&#26159;&#19968;&#31181;&#36890;&#36807;&#21518;&#22788;&#29702;&#32454;&#31890;&#24230;&#25513;&#34109;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#36807;&#28388;&#25481;&#26410;&#23450;&#20041;&#21644;&#26080;&#27861;&#21306;&#20998;&#30340;&#39046;&#22495;&#29305;&#23450;&#29305;&#24449;&#30340;&#39046;&#22495;&#27867;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#27867;&#21270;&#26088;&#22312;&#36890;&#36807;&#20165;&#22312;&#26377;&#38480;&#30340;&#28304;&#39046;&#22495;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#23398;&#20064;&#19968;&#20010;&#33021;&#22815;&#22312;&#26410;&#30693;&#27979;&#35797;&#39046;&#22495;&#19978;&#34920;&#29616;&#33391;&#22909;&#30340;&#27867;&#21270;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#39046;&#22495;&#27867;&#21270;&#26041;&#27861;&#24448;&#24448;&#24341;&#20837;&#19982;&#39044;&#27979;&#26080;&#20851;&#30340;&#22122;&#22768;&#25110;&#38656;&#35201;&#25910;&#38598;&#39046;&#22495;&#26631;&#31614;&#26469;&#35299;&#20915;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#20174;&#19981;&#21516;&#30340;&#35282;&#24230;&#32771;&#34385;&#20102;&#39046;&#22495;&#27867;&#21270;&#38382;&#39064;&#65292;&#23558;&#24213;&#23618;&#29305;&#24449;&#32452;&#21010;&#20998;&#20026;&#39046;&#22495;&#20849;&#20139;&#21644;&#39046;&#22495;&#29305;&#23450;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#39046;&#22495;&#29305;&#23450;&#29305;&#24449;&#24456;&#38590;&#20174;&#36755;&#20837;&#25968;&#25454;&#20013;&#36827;&#34892;&#35782;&#21035;&#21644;&#21306;&#20998;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DISPEL&#65288;DomaIn-SPEcific Liberating&#65289;&#30340;&#21518;&#22788;&#29702;&#32454;&#31890;&#24230;&#25513;&#34109;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#36807;&#28388;&#25481;&#26410;&#23450;&#20041;&#21644;&#26080;&#27861;&#21306;&#20998;&#30340;&#39046;&#22495;&#29305;&#23450;&#29305;&#24449;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;DISPEL&#21033;&#29992;&#19968;&#20010;&#25513;&#34109;&#29983;&#25104;&#22120;&#20026;&#27599;&#20010;&#36755;&#20837;&#25968;&#25454;&#29983;&#25104;&#19968;&#20010;&#21807;&#19968;&#30340;&#25513;&#34109;&#26469;&#36807;&#28388;&#39046;&#22495;&#29305;&#23450;&#29305;&#24449;&#12290;DISPEL&#26694;&#26550;&#38750;&#24120;&#28789;&#27963;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#32454;&#31890;&#24230;&#30340;&#25513;&#34109;&#20219;&#21153;&#21644;&#39046;&#22495;&#27867;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain generalization aims to learn a generalization model that can perform well on unseen test domains by only training on limited source domains. However, existing domain generalization approaches often bring in prediction-irrelevant noise or require the collection of domain labels. To address these challenges, we consider the domain generalization problem from a different perspective by categorizing underlying feature groups into domain-shared and domain-specific features. Nevertheless, the domain-specific features are difficult to be identified and distinguished from the input data. In this work, we propose DomaIn-SPEcific Liberating (DISPEL), a post-processing fine-grained masking approach that can filter out undefined and indistinguishable domain-specific features in the embedding space. Specifically, DISPEL utilizes a mask generator that produces a unique mask for each input data to filter domain-specific features. The DISPEL framework is highly flexible to be applied to any fin
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#23398;&#20064;&#30340;&#20195;&#29702;&#25968;&#25454;&#21516;&#21270;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#26377;&#38480;&#21306;&#22495;&#23454;&#29616;&#39640;&#25928;&#30340;&#29366;&#24577;&#20272;&#35745;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22312;&#32447;&#35745;&#31639;&#65292;&#36991;&#20813;&#20102;&#25972;&#21512;&#39640;&#32500;&#26377;&#38480;&#21306;&#22495;&#27169;&#22411;&#30340;&#38656;&#27714;&#65292;&#24182;&#19988;&#30456;&#27604;&#20256;&#32479;&#31639;&#27861;&#20855;&#26377;&#36739;&#22823;&#30340;&#35745;&#31639;&#20248;&#21183;&#12290;&#21516;&#26102;&#65292;&#35813;&#27169;&#22411;&#22522;&#20110;&#21487;&#35266;&#27979;&#24615;&#21644;&#26377;&#25928;&#21306;&#22495;&#30340;&#27010;&#24565;&#35774;&#35745;&#65292;&#33021;&#22815;&#30830;&#23450;&#25152;&#38656;&#30340;&#26368;&#20339;&#35266;&#27979;&#25968;&#25454;&#37327;&#65292;&#24182;&#20943;&#36731;&#19982;&#35745;&#31639;&#21487;&#35266;&#27979;&#24615;&#21644;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#30456;&#20851;&#30340;&#35745;&#31639;&#36127;&#25285;&#12290;</title><link>http://arxiv.org/abs/2307.07178</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#26377;&#38480;&#21306;&#22495;&#21160;&#21147;&#31995;&#32479;&#20272;&#35745;&#30340;&#20195;&#29702;&#25968;&#25454;&#21516;&#21270;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Surrogate Data Assimilation Model for the Estimation of Dynamical System in a Limited Area. (arXiv:2307.07178v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07178
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#23398;&#20064;&#30340;&#20195;&#29702;&#25968;&#25454;&#21516;&#21270;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#26377;&#38480;&#21306;&#22495;&#23454;&#29616;&#39640;&#25928;&#30340;&#29366;&#24577;&#20272;&#35745;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22312;&#32447;&#35745;&#31639;&#65292;&#36991;&#20813;&#20102;&#25972;&#21512;&#39640;&#32500;&#26377;&#38480;&#21306;&#22495;&#27169;&#22411;&#30340;&#38656;&#27714;&#65292;&#24182;&#19988;&#30456;&#27604;&#20256;&#32479;&#31639;&#27861;&#20855;&#26377;&#36739;&#22823;&#30340;&#35745;&#31639;&#20248;&#21183;&#12290;&#21516;&#26102;&#65292;&#35813;&#27169;&#22411;&#22522;&#20110;&#21487;&#35266;&#27979;&#24615;&#21644;&#26377;&#25928;&#21306;&#22495;&#30340;&#27010;&#24565;&#35774;&#35745;&#65292;&#33021;&#22815;&#30830;&#23450;&#25152;&#38656;&#30340;&#26368;&#20339;&#35266;&#27979;&#25968;&#25454;&#37327;&#65292;&#24182;&#20943;&#36731;&#19982;&#35745;&#31639;&#21487;&#35266;&#27979;&#24615;&#21644;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#30456;&#20851;&#30340;&#35745;&#31639;&#36127;&#25285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#20195;&#29702;&#25968;&#25454;&#21516;&#21270;&#65288;DA&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#26377;&#38480;&#21306;&#22495;&#36827;&#34892;&#39640;&#25928;&#29366;&#24577;&#20272;&#35745;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#37319;&#29992;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22312;&#32447;&#35745;&#31639;&#65292;&#28040;&#38500;&#20102;&#25972;&#21512;&#39640;&#32500;&#26377;&#38480;&#21306;&#22495;&#27169;&#22411;&#30340;&#38656;&#27714;&#12290;&#36825;&#31181;&#26041;&#27861;&#30456;&#23545;&#20256;&#32479;&#30340;DA&#31639;&#27861;&#20855;&#26377;&#26174;&#33879;&#30340;&#35745;&#31639;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22312;&#32447;&#21644;&#31163;&#32447;&#35745;&#31639;&#20013;&#36991;&#20813;&#20102;&#23545;&#26377;&#38480;&#21306;&#22495;&#27169;&#22411;&#30340;&#27178;&#21521;&#36793;&#30028;&#26465;&#20214;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#30340;&#20195;&#29702;DA&#27169;&#22411;&#30340;&#35774;&#35745;&#24314;&#31435;&#22312;&#19968;&#20010;&#22362;&#23454;&#30340;&#29702;&#35770;&#26694;&#26550;&#20043;&#19978;&#65292;&#21033;&#29992;&#20102;&#20004;&#20010;&#22522;&#26412;&#27010;&#24565;&#65306;&#21487;&#35266;&#27979;&#24615;&#21644;&#26377;&#25928;&#21306;&#22495;&#12290;&#21487;&#35266;&#27979;&#24615;&#30340;&#27010;&#24565;&#20351;&#25105;&#20204;&#33021;&#22815;&#23450;&#37327;&#30830;&#23450;&#20934;&#30830;&#30340;DA&#25152;&#38656;&#30340;&#26368;&#20339;&#35266;&#27979;&#25968;&#25454;&#37327;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#26377;&#25928;&#21306;&#22495;&#30340;&#27010;&#24565;&#22823;&#22823;&#20943;&#36731;&#20102;&#19982;&#35745;&#31639;&#21487;&#35266;&#27979;&#24615;&#21644;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#30456;&#20851;&#30340;&#35745;&#31639;&#36127;&#25285;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel learning-based surrogate data assimilation (DA) model for efficient state estimation in a limited area. Our model employs a feedforward neural network for online computation, eliminating the need for integrating high-dimensional limited-area models. This approach offers significant computational advantages over traditional DA algorithms. Furthermore, our method avoids the requirement of lateral boundary conditions for the limited-area model in both online and offline computations. The design of our surrogate DA model is built upon a robust theoretical framework that leverages two fundamental concepts: observability and effective region. The concept of observability enables us to quantitatively determine the optimal amount of observation data necessary for accurate DA. Meanwhile, the concept of effective region substantially reduces the computational burden associated with computing observability and generating training data.
&lt;/p&gt;</description></item><item><title>Safe DreamerV3&#26159;&#19968;&#31181;&#36890;&#36807;&#38598;&#25104;&#22522;&#20110;&#25289;&#26684;&#26391;&#26085;&#21644;&#35745;&#21010;&#30340;&#26041;&#27861;&#21040;&#19990;&#30028;&#27169;&#22411;&#20013;&#30340;&#26032;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#20302;&#32500;&#24230;&#21644;&#20165;&#37319;&#29992;&#35270;&#35273;&#30340;&#20219;&#21153;&#20013;&#20960;&#20046;&#38646;&#25104;&#26412;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2307.07176</link><description>&lt;p&gt;
Safe DreamerV3&#65306;&#24102;&#26377;&#19990;&#30028;&#27169;&#22411;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Safe DreamerV3: Safe Reinforcement Learning with World Models. (arXiv:2307.07176v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07176
&lt;/p&gt;
&lt;p&gt;
Safe DreamerV3&#26159;&#19968;&#31181;&#36890;&#36807;&#38598;&#25104;&#22522;&#20110;&#25289;&#26684;&#26391;&#26085;&#21644;&#35745;&#21010;&#30340;&#26041;&#27861;&#21040;&#19990;&#30028;&#27169;&#22411;&#20013;&#30340;&#26032;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#20302;&#32500;&#24230;&#21644;&#20165;&#37319;&#29992;&#35270;&#35273;&#30340;&#20219;&#21153;&#20013;&#20960;&#20046;&#38646;&#25104;&#26412;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#36824;&#27809;&#26377;&#23454;&#29616;, &#36825;&#20027;&#35201;&#26159;&#22240;&#20026;&#20854;&#26410;&#33021;&#28385;&#36275;&#36825;&#20123;&#31995;&#32479;&#30340;&#22522;&#26412;&#23433;&#20840;&#38656;&#27714;&#12290;&#29616;&#26377;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#20351;&#29992;&#25104;&#26412;&#20989;&#25968;&#26469;&#22686;&#24378;&#23433;&#20840;&#24615;&#65292;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#65292;&#21253;&#25324;&#20165;&#37319;&#29992;&#35270;&#35273;&#30340;&#20219;&#21153;&#20013;&#65292;&#21363;&#20351;&#36827;&#34892;&#20840;&#38754;&#30340;&#25968;&#25454;&#37319;&#26679;&#21644;&#35757;&#32451;&#65292;&#20063;&#26080;&#27861;&#23454;&#29616;&#38646;&#25104;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Safe DreamerV3&#65292;&#36825;&#26159;&#19968;&#31181;&#23558;&#22522;&#20110;&#25289;&#26684;&#26391;&#26085;&#21644;&#35745;&#21010;&#30340;&#26041;&#27861;&#38598;&#25104;&#21040;&#19990;&#30028;&#27169;&#22411;&#20013;&#30340;&#26032;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#35770;&#22312;SafeRL&#20013;&#20195;&#34920;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#36827;&#27493;&#65292;&#26159;&#31532;&#19968;&#20010;&#22312;Safety-Gymnasium&#22522;&#20934;&#20013;&#23454;&#29616;&#36817;&#20046;&#38646;&#25104;&#26412;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#39033;&#30446;&#32593;&#31449;&#21487;&#20197;&#22312;&#20197;&#19979;&#38142;&#25509;&#25214;&#21040;&#65306;https://sites.google.com/view/safedreamerv3&#12290;
&lt;/p&gt;
&lt;p&gt;
The widespread application of Reinforcement Learning (RL) in real-world situations is yet to come to fruition, largely as a result of its failure to satisfy the essential safety demands of such systems. Existing safe reinforcement learning (SafeRL) methods, employing cost functions to enhance safety, fail to achieve zero-cost in complex scenarios, including vision-only tasks, even with comprehensive data sampling and training. To address this, we introduce Safe DreamerV3, a novel algorithm that integrates both Lagrangian-based and planning-based methods within a world model. Our methodology represents a significant advancement in SafeRL as the first algorithm to achieve nearly zero-cost in both low-dimensional and vision-only tasks within the Safety-Gymnasium benchmark. Our project website can be found in: https://sites.google.com/view/safedreamerv3.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#25512;&#26029;&#21644;&#33258;&#36866;&#24212;&#20002;&#24323;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65288;FedBIAD&#65289;&#65292;&#36890;&#36807;&#23558;&#23616;&#37096;&#27169;&#22411;&#30340;&#26435;&#37325;&#34892;&#35270;&#20026;&#27010;&#29575;&#20998;&#24067;&#65292;&#24182;&#26681;&#25454;&#19982;&#23616;&#37096;&#35757;&#32451;&#25439;&#22833;&#36235;&#21183;&#30456;&#20851;&#30340;&#37325;&#35201;&#24230;&#25351;&#26631;&#33258;&#36866;&#24212;&#22320;&#20002;&#24323;&#37096;&#20998;&#26435;&#37325;&#34892;&#65292;&#35299;&#20915;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#36890;&#20449;&#25928;&#29575;&#21644;&#24615;&#33021;&#20445;&#35777;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.07172</link><description>&lt;p&gt;
FedBIAD: &#20855;&#26377;&#36125;&#21494;&#26031;&#25512;&#26029;&#33258;&#36866;&#24212;&#20002;&#24323;&#30340;&#36890;&#20449;&#39640;&#25928;&#21644;&#20934;&#30830;&#24615;&#20445;&#35777;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FedBIAD: Communication-Efficient and Accuracy-Guaranteed Federated Learning with Bayesian Inference-Based Adaptive Dropout. (arXiv:2307.07172v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07172
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#25512;&#26029;&#21644;&#33258;&#36866;&#24212;&#20002;&#24323;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65288;FedBIAD&#65289;&#65292;&#36890;&#36807;&#23558;&#23616;&#37096;&#27169;&#22411;&#30340;&#26435;&#37325;&#34892;&#35270;&#20026;&#27010;&#29575;&#20998;&#24067;&#65292;&#24182;&#26681;&#25454;&#19982;&#23616;&#37096;&#35757;&#32451;&#25439;&#22833;&#36235;&#21183;&#30456;&#20851;&#30340;&#37325;&#35201;&#24230;&#25351;&#26631;&#33258;&#36866;&#24212;&#22320;&#20002;&#24323;&#37096;&#20998;&#26435;&#37325;&#34892;&#65292;&#35299;&#20915;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#36890;&#20449;&#25928;&#29575;&#21644;&#24615;&#33021;&#20445;&#35777;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#36991;&#20813;&#20102;&#38544;&#31169;&#27844;&#38706;&#38382;&#39064;&#65292;&#26080;&#38656;&#23558;&#26368;&#32456;&#29992;&#25143;&#25968;&#25454;&#20256;&#36755;&#12290;&#21442;&#19982;FL&#30340;&#35774;&#22791;&#36890;&#24120;&#24102;&#23485;&#21463;&#38480;&#65292;&#19988;&#26080;&#32447;&#32593;&#32476;&#20013;&#19978;&#34892;&#36895;&#24230;&#36828;&#24930;&#20110;&#19979;&#34892;&#36895;&#24230;&#65292;&#23548;&#33268;&#19978;&#34892;&#36890;&#20449;&#29942;&#39048;&#20005;&#37325;&#12290;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#30340;&#19968;&#20010;&#37325;&#35201;&#26041;&#21521;&#26159;&#32852;&#37030;&#20002;&#24323;&#65292;&#21363;&#20002;&#24323;&#23616;&#37096;&#27169;&#22411;&#30340;&#37096;&#20998;&#26435;&#37325;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#32852;&#37030;&#20002;&#24323;&#30740;&#31350;&#38598;&#20013;&#22312;&#38543;&#26426;&#25110;&#26377;&#24207;&#20002;&#24323;&#19978;&#65292;&#24182;&#32570;&#20047;&#29702;&#35770;&#25903;&#25345;&#65292;&#23548;&#33268;&#24615;&#33021;&#26080;&#27861;&#20445;&#35777;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#25512;&#26029;&#21644;&#33258;&#36866;&#24212;&#20002;&#24323;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65288;FedBIAD&#65289;&#65292;&#35813;&#26041;&#27861;&#23558;&#23616;&#37096;&#27169;&#22411;&#30340;&#26435;&#37325;&#34892;&#35270;&#20026;&#27010;&#29575;&#20998;&#24067;&#65292;&#24182;&#26681;&#25454;&#19982;&#23616;&#37096;&#35757;&#32451;&#25439;&#22833;&#36235;&#21183;&#30456;&#20851;&#30340;&#37325;&#35201;&#24230;&#25351;&#26631;&#33258;&#36866;&#24212;&#22320;&#20002;&#24323;&#37096;&#20998;&#26435;&#37325;&#34892;&#12290;&#36890;&#36807;&#24212;&#29992;FedBIAD&#65292;&#27599;&#20010;&#23458;&#25143;&#31471;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#36873;&#25321;&#39640;&#36136;&#37327;&#30340;&#20002;&#24323;&#27169;&#24335;&#65292;&#20174;&#32780;&#33719;&#24471;&#20934;&#30830;&#30340;&#35770;&#25991;&#25688;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) emerges as a distributed machine learning paradigm without end-user data transmission, effectively avoiding privacy leakage. Participating devices in FL are usually bandwidth-constrained, and the uplink is much slower than the downlink in wireless networks, which causes a severe uplink communication bottleneck. A prominent direction to alleviate this problem is federated dropout, which drops fractional weights of local models. However, existing federated dropout studies focus on random or ordered dropout and lack theoretical support, resulting in unguaranteed performance. In this paper, we propose Federated learning with Bayesian Inference-based Adaptive Dropout (FedBIAD), which regards weight rows of local models as probability distributions and adaptively drops partial weight rows based on importance indicators correlated with the trend of local training loss. By applying FedBIAD, each client adaptively selects a high-quality dropping pattern with accurate app
&lt;/p&gt;</description></item><item><title>&#20256;&#32479;&#30340;&#38543;&#26426;&#24179;&#28369;&#26041;&#27861;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30452;&#25509;&#24212;&#29992;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#20026;&#20102;&#35299;&#20915;&#35748;&#35777;&#21322;&#24452;&#24456;&#23567;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#33258;&#25105;&#20928;&#21270;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.07171</link><description>&lt;p&gt;
&#24102;&#26377;&#33258;&#25105;&#20928;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35748;&#35777;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Certified Robustness for Large Language Models with Self-Denoising. (arXiv:2307.07171v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07171
&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#38543;&#26426;&#24179;&#28369;&#26041;&#27861;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30452;&#25509;&#24212;&#29992;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#20026;&#20102;&#35299;&#20915;&#35748;&#35777;&#21322;&#24452;&#24456;&#23567;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#33258;&#25105;&#20928;&#21270;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#24191;&#27867;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#23545;&#20110;&#22122;&#22768;&#36755;&#20837;&#30340;&#33030;&#24369;&#24615;&#26174;&#33879;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#24212;&#29992;&#65292;&#23588;&#20854;&#26159;&#22312;&#39640;&#39118;&#38505;&#29615;&#22659;&#20013;&#12290;&#22312;&#36825;&#20123;&#29615;&#22659;&#19979;&#65292;&#30830;&#20445;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27599;&#20010;&#39044;&#27979;&#37117;&#26159;&#31283;&#23450;&#30340;&#38750;&#24120;&#37325;&#35201;&#65292;&#21363;&#22312;&#36755;&#20837;&#30340;&#24494;&#23567;&#24046;&#24322;&#24773;&#20917;&#19979;&#65292;LLM&#30340;&#39044;&#27979;&#24212;&#35813;&#26159;&#19968;&#33268;&#30340;&#12290;&#36825;&#20027;&#35201;&#28041;&#21450;&#21040;&#35748;&#35777;&#40065;&#26834;LLM&#30340;&#30740;&#31350;&#65292;&#21363;&#22312;&#36755;&#20837;&#21608;&#22260;&#30340;&#23616;&#37096;&#21306;&#22495;&#20013;&#65292;&#25152;&#26377;LLM&#30340;&#39044;&#27979;&#37117;&#24471;&#21040;&#35748;&#35777;&#26159;&#27491;&#30830;&#30340;&#12290;&#38543;&#26426;&#24179;&#28369;&#24050;&#32463;&#23637;&#31034;&#20102;&#22312;&#35748;&#35777;LLM&#30340;&#40065;&#26834;&#24615;&#21644;&#39044;&#27979;&#31283;&#23450;&#24615;&#26041;&#38754;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#38543;&#26426;&#24179;&#28369;&#22312;&#36827;&#34892;&#27169;&#22411;&#39044;&#27979;&#20043;&#21069;&#38656;&#35201;&#23545;&#36755;&#20837;&#28155;&#21152;&#22122;&#22768;&#65292;&#20854;&#35748;&#35777;&#24615;&#33021;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#27169;&#22411;&#22312;&#21463;&#25439;&#25968;&#25454;&#19978;&#30340;&#34920;&#29616;&#12290;&#22240;&#27492;&#65292;&#23427;&#30452;&#25509;&#24212;&#29992;&#20110;LLM&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#24182;&#19988;&#36890;&#24120;&#20250;&#23548;&#33268;&#24456;&#23567;&#30340;&#35748;&#35777;&#21322;&#24452;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;
&lt;/p&gt;
&lt;p&gt;
Although large language models (LLMs) have achieved great success in vast real-world applications, their vulnerabilities towards noisy inputs have significantly limited their uses, especially in high-stake environments. In these contexts, it is crucial to ensure that every prediction made by large language models is stable, i.e., LLM predictions should be consistent given minor differences in the input. This largely falls into the study of certified robust LLMs, i.e., all predictions of LLM are certified to be correct in a local region around the input. Randomized smoothing has demonstrated great potential in certifying the robustness and prediction stability of LLMs. However, randomized smoothing requires adding noise to the input before model prediction, and its certification performance depends largely on the model's performance on corrupted data. As a result, its direct application to LLMs remains challenging and often results in a small certification radius. To address this issue,
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23454;&#20363;&#32423;&#21035;&#37325;&#26032;&#21152;&#26435;&#26041;&#26696;&#65292;&#36890;&#36807;&#32771;&#34385;&#27599;&#20010;&#33258;&#28982;&#31034;&#20363;&#30340;&#26131;&#21463;&#25915;&#20987;&#24615;&#21644;&#23545;&#25239;&#25915;&#20987;&#23548;&#33268;&#30340;&#20449;&#24687;&#25439;&#22833;&#65292;&#26469;&#25913;&#36827;&#23545;&#25239;&#35757;&#32451;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.07167</link><description>&lt;p&gt;
&#23545;&#25239;&#35757;&#32451;&#20013;&#30340;&#26131;&#21463;&#25915;&#20987;&#24615;&#23454;&#20363;&#37325;&#26032;&#21152;&#26435;
&lt;/p&gt;
&lt;p&gt;
Vulnerability-Aware Instance Reweighting For Adversarial Training. (arXiv:2307.07167v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07167
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23454;&#20363;&#32423;&#21035;&#37325;&#26032;&#21152;&#26435;&#26041;&#26696;&#65292;&#36890;&#36807;&#32771;&#34385;&#27599;&#20010;&#33258;&#28982;&#31034;&#20363;&#30340;&#26131;&#21463;&#25915;&#20987;&#24615;&#21644;&#23545;&#25239;&#25915;&#20987;&#23548;&#33268;&#30340;&#20449;&#24687;&#25439;&#22833;&#65292;&#26469;&#25913;&#36827;&#23545;&#25239;&#35757;&#32451;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#35757;&#32451;&#19968;&#30452;&#34987;&#21457;&#29616;&#33021;&#22823;&#22823;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#22120;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;&#23545;&#25239;&#35757;&#32451;&#36890;&#36807;&#22312;&#35757;&#32451;&#20998;&#31867;&#22120;&#26102;&#21152;&#20837;&#23545;&#25239;&#24615;&#26679;&#26412;&#26469;&#33719;&#24471;&#40065;&#26834;&#24615;&#12290;&#22823;&#22810;&#25968;&#23545;&#25239;&#35757;&#32451;&#31639;&#27861;&#30340;&#21464;&#20307;&#23558;&#27599;&#20010;&#35757;&#32451;&#26679;&#26412;&#24179;&#31561;&#23545;&#24453;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#36890;&#36807;&#23558;&#23427;&#20204;&#19981;&#24179;&#31561;&#22320;&#23545;&#24453;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#23545;&#25239;&#35757;&#32451;&#23545;&#35757;&#32451;&#38598;&#20013;&#30340;&#19981;&#21516;&#31867;&#21035;&#20135;&#29983;&#20102;&#19981;&#22343;&#34913;&#30340;&#24433;&#21709;&#65292;&#24182;&#19981;&#20844;&#24179;&#22320;&#25439;&#23475;&#20102;&#19982;&#26412;&#36136;&#19978;&#26356;&#38590;&#20998;&#31867;&#30340;&#31867;&#21035;&#30456;&#23545;&#24212;&#30340;&#31034;&#20363;&#12290;&#22240;&#27492;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#37325;&#26032;&#21152;&#26435;&#26041;&#26696;&#65292;&#20026;&#35757;&#32451;&#38598;&#20013;&#21508;&#20010;&#31034;&#20363;&#30340;&#40065;&#26834;&#25439;&#22833;&#20998;&#37197;&#19981;&#24179;&#31561;&#30340;&#26435;&#37325;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23454;&#20363;&#32423;&#21035;&#37325;&#26032;&#21152;&#26435;&#26041;&#26696;&#12290;&#23427;&#32771;&#34385;&#20102;&#27599;&#20010;&#33258;&#28982;&#31034;&#20363;&#30340;&#26131;&#21463;&#25915;&#20987;&#24615;&#21644;&#30001;&#20110;&#23545;&#25239;&#25915;&#20987;&#32780;&#23548;&#33268;&#30340;&#20854;&#23545;&#25163;&#31034;&#20363;&#19978;&#30340;&#20449;&#24687;&#25439;&#22833;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Adversarial Training (AT) has been found to substantially improve the robustness of deep learning classifiers against adversarial attacks. AT involves obtaining robustness by including adversarial examples in training a classifier. Most variants of AT algorithms treat every training example equally. However, recent works have shown that better performance is achievable by treating them unequally. In addition, it has been observed that AT exerts an uneven influence on different classes in a training set and unfairly hurts examples corresponding to classes that are inherently harder to classify. Consequently, various reweighting schemes have been proposed that assign unequal weights to robust losses of individual examples in a training set. In this work, we propose a novel instance-wise reweighting scheme. It considers the vulnerability of each natural example and the resulting information loss on its adversarial counterpart occasioned by adversarial attacks. Through extensive experiment
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36974;&#30422;&#39046;&#22495;&#20869;&#20851;&#38190;&#35789;&#36827;&#34892;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#20351;&#29992;&#38543;&#26426;&#36974;&#30422;&#30340;&#39046;&#22495;&#20869;&#39044;&#35757;&#32451;&#21644;&#24120;&#35265;&#30340;&#39044;&#35757;&#32451;&#28982;&#21518;&#24494;&#35843;&#33539;&#24335;&#12290;</title><link>http://arxiv.org/abs/2307.07160</link><description>&lt;p&gt;
&#19981;&#35201;&#38543;&#26426;&#36974;&#30422;&#65306;&#36890;&#36807;&#36974;&#30422;&#39046;&#22495;&#20869;&#20851;&#38190;&#35789;&#36827;&#34892;&#26377;&#25928;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Do not Mask Randomly: Effective Domain-adaptive Pre-training by Masking In-domain Keywords. (arXiv:2307.07160v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07160
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36974;&#30422;&#39046;&#22495;&#20869;&#20851;&#38190;&#35789;&#36827;&#34892;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#20351;&#29992;&#38543;&#26426;&#36974;&#30422;&#30340;&#39046;&#22495;&#20869;&#39044;&#35757;&#32451;&#21644;&#24120;&#35265;&#30340;&#39044;&#35757;&#32451;&#28982;&#21518;&#24494;&#35843;&#33539;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39046;&#22495;&#26080;&#20851;&#30340;&#39046;&#22495;&#20869;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#20171;&#20110;&#36890;&#29992;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#20043;&#38388;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36873;&#25321;&#24615;&#22320;&#36974;&#30422;&#39046;&#22495;&#20869;&#30340;&#20851;&#38190;&#35789;&#65292;&#21363;&#25552;&#20379;&#30446;&#26631;&#39046;&#22495;&#30340;&#32039;&#20945;&#34920;&#31034;&#30340;&#21333;&#35789;&#12290;&#25105;&#20204;&#20351;&#29992;KeyBERT (Grootendorst, 2020)&#26469;&#35782;&#21035;&#36825;&#20123;&#20851;&#38190;&#35789;&#12290;&#25105;&#20204;&#20351;&#29992;&#20845;&#31181;&#19981;&#21516;&#30340;&#35774;&#32622;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#35780;&#20272;&#65306;&#19977;&#20010;&#25968;&#25454;&#38598;&#19982;&#20004;&#20010;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#25105;&#20204;&#30340;&#39046;&#22495;&#20869;&#39044;&#35757;&#32451;&#31574;&#30053;&#24494;&#35843;&#30340;PLMs&#20248;&#20110;&#20351;&#29992;&#38543;&#26426;&#36974;&#30422;&#30340;&#39046;&#22495;&#20869;&#39044;&#35757;&#32451;&#30340;PLMs&#65292;&#24182;&#19988;&#20248;&#20110;&#36981;&#24490;&#24120;&#35265;&#30340;&#39044;&#35757;&#32451;&#28982;&#21518;&#24494;&#35843;&#33539;&#24335;&#30340;PLMs&#12290;&#27492;&#22806;&#65292;&#35782;&#21035;&#39046;&#22495;&#20869;&#20851;&#38190;&#35789;&#30340;&#24320;&#38144;&#26159;&#21512;&#29702;&#30340;&#65292;&#20363;&#22914;&#65292;&#23545;&#20110;BERT Large (Devlin et al., 2019)&#26469;&#35828;&#65292;&#26159;&#39044;&#35757;&#32451;&#26102;&#38388;&#30340;7-15%&#65288;&#20004;&#20010;epoch&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel task-agnostic in-domain pre-training method that sits between generic pre-training and fine-tuning. Our approach selectively masks in-domain keywords, i.e., words that provide a compact representation of the target domain. We identify such keywords using KeyBERT (Grootendorst, 2020). We evaluate our approach using six different settings: three datasets combined with two distinct pre-trained language models (PLMs). Our results reveal that the fine-tuned PLMs adapted using our in-domain pre-training strategy outperform PLMs that used in-domain pre-training with random masking as well as those that followed the common pre-train-then-fine-tune paradigm. Further, the overhead of identifying in-domain keywords is reasonable, e.g., 7-15% of the pre-training time (for two epochs) for BERT Large (Devlin et al., 2019).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20219;&#21153;&#26080;&#20851;&#30340;&#35780;&#20272;&#26694;&#26550;Camilla&#65292;&#36890;&#36807;&#23450;&#20041;&#22810;&#32500;&#24230;&#30340;&#35786;&#26029;&#24230;&#37327;Ability&#26469;&#21327;&#21516;&#27979;&#37327;&#27599;&#20010;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#22810;&#38754;&#20307;&#24378;&#24230;&#65292;&#35299;&#20915;&#20102;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#23454;&#38469;&#24615;&#33021;&#21644;&#26631;&#20934;&#21270;&#35780;&#20272;&#20013;&#23384;&#22312;&#30340;&#24046;&#36317;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.07134</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#22810;&#32500;&#33021;&#21147;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Multi-Dimensional Ability Diagnosis for Machine Learning Algorithms. (arXiv:2307.07134v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07134
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20219;&#21153;&#26080;&#20851;&#30340;&#35780;&#20272;&#26694;&#26550;Camilla&#65292;&#36890;&#36807;&#23450;&#20041;&#22810;&#32500;&#24230;&#30340;&#35786;&#26029;&#24230;&#37327;Ability&#26469;&#21327;&#21516;&#27979;&#37327;&#27599;&#20010;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#22810;&#38754;&#20307;&#24378;&#24230;&#65292;&#35299;&#20915;&#20102;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#23454;&#38469;&#24615;&#33021;&#21644;&#26631;&#20934;&#21270;&#35780;&#20272;&#20013;&#23384;&#22312;&#30340;&#24046;&#36317;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#24050;&#32463;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#21464;&#24471;&#26080;&#22788;&#19981;&#22312;&#65288;&#20363;&#22914;&#22270;&#20687;&#20998;&#31867;&#65289;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20256;&#32479;&#25351;&#26631;&#65288;&#20363;&#22914;&#27599;&#20010;&#20998;&#31867;&#22120;&#30340;&#31895;&#31890;&#24230;&#20934;&#30830;&#24230;&#65289;&#30340;&#27979;&#37327;&#19981;&#36275;&#65292;&#36890;&#24120;&#22312;&#36825;&#20123;&#31639;&#27861;&#30340;&#23454;&#38469;&#24615;&#33021;&#21644;&#26631;&#20934;&#21270;&#35780;&#20272;&#20013;&#23384;&#22312;&#23454;&#36136;&#24615;&#24046;&#36317;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#21463;&#21040;&#20154;&#31867;&#27979;&#37327;&#20013;&#30340;&#24515;&#29702;&#27979;&#37327;&#29702;&#35770;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20219;&#21153;&#26080;&#20851;&#30340;&#35780;&#20272;&#26694;&#26550;Camilla&#65292;&#20854;&#20013;&#23450;&#20041;&#20102;&#19968;&#20010;&#22810;&#32500;&#35786;&#26029;&#24230;&#37327;Ability&#26469;&#21327;&#21516;&#27979;&#37327;&#27599;&#20010;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#22810;&#38754;&#20307;&#24378;&#24230;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#32473;&#23450;&#19981;&#21516;&#31639;&#27861;&#23545;&#25968;&#25454;&#26679;&#26412;&#30340;&#21709;&#24212;&#26085;&#24535;&#65292;&#25105;&#20204;&#21033;&#29992;&#35748;&#30693;&#35786;&#26029;&#20551;&#35774;&#21644;&#31070;&#32463;&#32593;&#32476;&#26469;&#23398;&#20064;&#31639;&#27861;&#12289;&#26679;&#26412;&#20197;&#21450;&#27599;&#20010;&#26679;&#26412;&#30340;&#25216;&#33021;&#65288;&#26174;&#24335;&#25110;&#38544;&#24335;&#39044;&#23450;&#20041;&#65289;&#20043;&#38388;&#30340;&#22797;&#26434;&#20132;&#20114;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#21487;&#20197;&#21516;&#26102;&#35780;&#20272;&#27599;&#20010;&#31639;&#27861;&#22312;&#22810;&#20010;&#25216;&#33021;&#19978;&#30340;&#34920;&#29616;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning algorithms have become ubiquitous in a number of applications (e.g. image classification). However, due to the insufficient measurement of traditional metrics (e.g. the coarse-grained Accuracy of each classifier), substantial gaps are usually observed between the real-world performance of these algorithms and their scores in standardized evaluations. In this paper, inspired by the psychometric theories from human measurement, we propose a task-agnostic evaluation framework Camilla, where a multi-dimensional diagnostic metric Ability is defined for collaboratively measuring the multifaceted strength of each machine learning algorithm. Specifically, given the response logs from different algorithms to data samples, we leverage cognitive diagnosis assumptions and neural networks to learn the complex interactions among algorithms, samples and the skills (explicitly or implicitly pre-defined) of each sample. In this way, both the abilities of each algorithm on multiple skil
&lt;/p&gt;</description></item><item><title>DataAssist&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#25968;&#25454;&#38598;&#36136;&#37327;&#21644;&#33410;&#30465;&#25968;&#25454;&#28165;&#27927;&#21644;&#20934;&#22791;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2307.07119</link><description>&lt;p&gt;
DataAssist:&#19968;&#31181;&#29992;&#20110;&#25968;&#25454;&#28165;&#27927;&#21644;&#20934;&#22791;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DataAssist: A Machine Learning Approach to Data Cleaning and Preparation. (arXiv:2307.07119v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07119
&lt;/p&gt;
&lt;p&gt;
DataAssist&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#25968;&#25454;&#38598;&#36136;&#37327;&#21644;&#33410;&#30465;&#25968;&#25454;&#28165;&#27927;&#21644;&#20934;&#22791;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#20027;&#35201;&#20851;&#27880;&#20110;&#27169;&#22411;&#36873;&#25321;&#21644;&#21442;&#25968;&#20248;&#21270;&#65292;&#24573;&#30053;&#20102;&#25968;&#25454;&#28165;&#27927;&#21644;&#25972;&#29702;&#25152;&#21344;&#25454;&#30340;&#22823;&#37096;&#20998;&#26102;&#38388;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DataAssist&#30340;&#33258;&#21160;&#21270;&#25968;&#25454;&#20934;&#22791;&#21644;&#28165;&#27927;&#24179;&#21488;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#25552;&#39640;&#25968;&#25454;&#38598;&#36136;&#37327;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;DataAssist&#25552;&#20379;&#20102;&#19968;&#20010;&#29992;&#20110;&#25506;&#32034;&#24615;&#25968;&#25454;&#20998;&#26512;&#21644;&#25968;&#25454;&#28165;&#27927;&#30340;&#31649;&#36947;&#65292;&#21253;&#25324;&#20026;&#29992;&#25143;&#36873;&#25321;&#30340;&#21464;&#37327;&#29983;&#25104;&#21487;&#35270;&#21270;&#65292;&#32479;&#19968;&#25968;&#25454;&#27880;&#37322;&#65292;&#25552;&#20379;&#24322;&#24120;&#20540;&#21024;&#38500;&#24314;&#35758;&#20197;&#21450;&#23545;&#25968;&#25454;&#36827;&#34892;&#39044;&#22788;&#29702;&#12290;&#23548;&#20986;&#30340;&#25968;&#25454;&#38598;&#21487;&#20197;&#36731;&#26494;&#19982;&#20854;&#20182;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#25110;&#29992;&#25143;&#25351;&#23450;&#30340;&#27169;&#22411;&#36827;&#34892;&#25972;&#21512;&#65292;&#20197;&#36827;&#34892;&#21518;&#32493;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#20013;&#24515;&#21270;&#24037;&#20855;&#36866;&#29992;&#20110;&#22810;&#20010;&#39046;&#22495;&#65292;&#21253;&#25324;&#32463;&#27982;&#23398;&#12289;&#21830;&#19994;&#21644;&#39044;&#27979;&#24212;&#29992;&#65292;&#21487;&#33410;&#30465;&#36229;&#36807;50\%&#30340;&#25968;&#25454;&#28165;&#29702;&#21644;&#20934;&#22791;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current automated machine learning (ML) tools are model-centric, focusing on model selection and parameter optimization. However, the majority of the time in data analysis is devoted to data cleaning and wrangling, for which limited tools are available. Here we present DataAssist, an automated data preparation and cleaning platform that enhances dataset quality using ML-informed methods. We show that DataAssist provides a pipeline for exploratory data analysis and data cleaning, including generating visualization for user-selected variables, unifying data annotation, suggesting anomaly removal, and preprocessing data. The exported dataset can be readily integrated with other autoML tools or user-specified model for downstream analysis. Our data-centric tool is applicable to a variety of fields, including economics, business, and forecasting applications saving over 50\% time of the time spent on data cleansing and preparation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24212;&#29992;&#20110;&#20998;&#25955;&#38543;&#26426;&#21452;&#27491;&#21017;&#21270;&#38750;&#20984;&#24378;&#20984;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#30340;&#26041;&#24046;&#20943;&#23569;&#21152;&#36895;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#25289;&#26684;&#26391;&#26085;&#20056;&#23376;&#21644;&#37319;&#29992;&#21333;&#20010;&#37051;&#23621;&#36890;&#20449;&#24182;&#32467;&#21512;&#26041;&#24046;&#20943;&#23569;&#25216;&#26415;&#65292;&#35813;&#26041;&#27861;&#22312;&#38543;&#26426;&#35774;&#32622;&#19979;&#26679;&#26412;&#22797;&#26434;&#24230;&#36798;&#21040;$\mathcal{O}(\kappa^3\varepsilon^{-3})$&#12290;</title><link>http://arxiv.org/abs/2307.07113</link><description>&lt;p&gt;
&#20998;&#25955;&#38543;&#26426;&#21452;&#27491;&#21017;&#21270;&#38750;&#20984;&#24378;&#20984;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#30340;&#26041;&#24046;&#20943;&#23569;&#21152;&#36895;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Variance-reduced accelerated methods for decentralized stochastic double-regularized nonconvex strongly-concave minimax problems. (arXiv:2307.07113v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07113
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24212;&#29992;&#20110;&#20998;&#25955;&#38543;&#26426;&#21452;&#27491;&#21017;&#21270;&#38750;&#20984;&#24378;&#20984;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#30340;&#26041;&#24046;&#20943;&#23569;&#21152;&#36895;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#25289;&#26684;&#26391;&#26085;&#20056;&#23376;&#21644;&#37319;&#29992;&#21333;&#20010;&#37051;&#23621;&#36890;&#20449;&#24182;&#32467;&#21512;&#26041;&#24046;&#20943;&#23569;&#25216;&#26415;&#65292;&#35813;&#26041;&#27861;&#22312;&#38543;&#26426;&#35774;&#32622;&#19979;&#26679;&#26412;&#22797;&#26434;&#24230;&#36798;&#21040;$\mathcal{O}(\kappa^3\varepsilon^{-3})$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#22312;&#21407;&#22987;&#21464;&#37327;&#21644;&#23545;&#20598;&#21464;&#37327;&#19978;&#20855;&#26377;&#38750;&#20809;&#28369;&#27491;&#21017;&#21270;&#39033;&#30340;&#20998;&#25955;&#12289;&#38543;&#26426;&#12289;&#38750;&#20984;&#24378;&#20984;&#65288;NCSC&#65289;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#65292;&#22312;&#35813;&#38382;&#39064;&#20013;&#65292;m&#20010;&#35745;&#31639;&#20195;&#29702;&#36890;&#36807;&#28857;&#23545;&#28857;&#36890;&#20449;&#36827;&#34892;&#21327;&#20316;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#32806;&#21512;&#20989;&#25968;&#20026;&#26399;&#26395;&#25110;&#26377;&#38480;&#21644;&#24418;&#24335;&#30340;&#24773;&#20917;&#65292;&#24182;&#19988;&#21452;&#27491;&#21017;&#21270;&#20989;&#25968;&#20998;&#21035;&#24212;&#29992;&#20110;&#21407;&#22987;&#21464;&#37327;&#21644;&#23545;&#20598;&#21464;&#37327;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#26694;&#26550;&#24341;&#20837;&#20102;&#19968;&#20010;&#25289;&#26684;&#26391;&#26085;&#20056;&#23376;&#26469;&#28040;&#38500;&#23545;&#20598;&#21464;&#37327;&#19978;&#30340;&#20849;&#35782;&#32422;&#26463;&#12290;&#23558;&#27492;&#19982;&#26041;&#24046;&#20943;&#23569;&#65288;VR&#65289;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;VRLM&#65292;&#36890;&#36807;&#27599;&#27425;&#36845;&#20195;&#36827;&#34892;&#19968;&#27425;&#37051;&#23621;&#36890;&#20449;&#65292;&#33021;&#22815;&#22312;&#19968;&#33324;&#30340;&#38543;&#26426;&#35774;&#32622;&#19979;&#23454;&#29616;$\mathcal{O}(\kappa^3\varepsilon^{-3})$ &#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#20854;&#20013;$\kappa$&#26159;&#38382;&#39064;&#30340;&#26465;&#20214;&#25968;&#65292;$\varepsilon$&#26159;&#24076;&#26395;&#30340;&#35299;&#31934;&#24230;&#12290;&#36890;&#36807;&#20351;&#29992;&#22823;&#25209;&#37327;VR&#65292;
&lt;/p&gt;
&lt;p&gt;
In this paper, we consider the decentralized, stochastic nonconvex strongly-concave (NCSC) minimax problem with nonsmooth regularization terms on both primal and dual variables, wherein a network of $m$ computing agents collaborate via peer-to-peer communications. We consider when the coupling function is in expectation or finite-sum form and the double regularizers are convex functions, applied separately to the primal and dual variables. Our algorithmic framework introduces a Lagrangian multiplier to eliminate the consensus constraint on the dual variable. Coupling this with variance-reduction (VR) techniques, our proposed method, entitled VRLM, by a single neighbor communication per iteration, is able to achieve an $\mathcal{O}(\kappa^3\varepsilon^{-3})$ sample complexity under the general stochastic setting, with either a big-batch or small-batch VR option, where $\kappa$ is the condition number of the problem and $\varepsilon$ is the desired solution accuracy. With a big-batch VR,
&lt;/p&gt;</description></item><item><title>&#36825;&#26159;&#19968;&#20010;&#20851;&#20110;&#22270;&#20301;&#32622;&#21644;&#32467;&#26500;&#32534;&#30721;&#22120;&#30340;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#22270;&#20301;&#32622;&#21644;&#32467;&#26500;&#32534;&#30721;&#22120;&#65288;GPSE&#65289;&#65292;&#23427;&#33021;&#26377;&#25928;&#22320;&#25429;&#25417;&#22810;&#20010;PSE&#30340;&#20849;&#21516;&#28508;&#22312;&#34920;&#31034;&#65292;&#24182;&#22312;&#21508;&#31181;&#22270;&#39044;&#27979;&#20219;&#21153;&#20013;&#21462;&#24471;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2307.07107</link><description>&lt;p&gt;
&#22270;&#20301;&#32622;&#21644;&#32467;&#26500;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Graph Positional and Structural Encoder. (arXiv:2307.07107v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07107
&lt;/p&gt;
&lt;p&gt;
&#36825;&#26159;&#19968;&#20010;&#20851;&#20110;&#22270;&#20301;&#32622;&#21644;&#32467;&#26500;&#32534;&#30721;&#22120;&#30340;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#22270;&#20301;&#32622;&#21644;&#32467;&#26500;&#32534;&#30721;&#22120;&#65288;GPSE&#65289;&#65292;&#23427;&#33021;&#26377;&#25928;&#22320;&#25429;&#25417;&#22810;&#20010;PSE&#30340;&#20849;&#21516;&#28508;&#22312;&#34920;&#31034;&#65292;&#24182;&#22312;&#21508;&#31181;&#22270;&#39044;&#27979;&#20219;&#21153;&#20013;&#21462;&#24471;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20301;&#32622;&#21644;&#32467;&#26500;&#32534;&#30721;&#65288;PSE&#65289;&#21487;&#20197;&#26356;&#22909;&#22320;&#22312;&#22270;&#20013;&#35782;&#21035;&#33410;&#28857;&#65292;&#22240;&#20026;&#19968;&#33324;&#22270;&#32570;&#20047;&#35268;&#33539;&#30340;&#33410;&#28857;&#39034;&#24207;&#12290;&#36825;&#20351;&#24471;PSE&#25104;&#20026;&#36171;&#20104;&#29616;&#20195;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#21644;&#29305;&#21035;&#26159;&#22270;&#21464;&#25442;&#22120;&#37325;&#35201;&#21151;&#33021;&#30340;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#35774;&#35745;&#36866;&#29992;&#20110;&#21508;&#31181;&#22270;&#39044;&#27979;&#20219;&#21153;&#30340;PSE&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#19988;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22270;&#20301;&#32622;&#21644;&#32467;&#26500;&#32534;&#30721;&#22120;&#65288;GPSE&#65289;&#65292;&#36825;&#26159;&#39318;&#27425;&#23581;&#35797;&#35757;&#32451;&#19968;&#20010;&#33021;&#22815;&#25429;&#25417;&#20016;&#23500;&#30340;PSE&#34920;&#31034;&#20197;&#22686;&#24378;&#20219;&#20309;GNN&#30340;&#22270;&#32534;&#30721;&#22120;&#12290;GPSE&#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#22810;&#20010;PSE&#30340;&#20849;&#21516;&#28508;&#22312;&#34920;&#31034;&#65292;&#24182;&#19988;&#20855;&#26377;&#39640;&#24230;&#21487;&#20256;&#36755;&#24615;&#12290;&#22312;&#29305;&#23450;&#22270;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;&#21487;&#20197;&#22312;&#20174;&#26174;&#33879;&#19981;&#21516;&#20998;&#24067;&#29978;&#33267;&#27169;&#24577;&#30340;&#25968;&#25454;&#38598;&#19978;&#26377;&#25928;&#22320;&#20351;&#29992;&#12290;&#25105;&#20204;&#26174;&#31034;&#65292;&#22312;&#24191;&#27867;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#32463;&#36807;GPSE&#22686;&#24378;&#30340;&#27169;&#22411;&#22312;&#26576;&#20123;&#20219;&#21153;&#20013;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#65292;&#21516;&#26102;&#19982;&#26126;&#30830;&#20351;&#29992;PSE&#30340;&#27169;&#22411;&#24615;&#33021;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Positional and structural encodings (PSE) enable better identifiability of nodes within a graph, as in general graphs lack a canonical node ordering. This renders PSEs essential tools for empowering modern GNNs, and in particular graph Transformers. However, designing PSEs that work optimally for a variety of graph prediction tasks is a challenging and unsolved problem. Here, we present the graph positional and structural encoder (GPSE), a first-ever attempt to train a graph encoder that captures rich PSE representations for augmenting any GNN. GPSE can effectively learn a common latent representation for multiple PSEs, and is highly transferable. The encoder trained on a particular graph dataset can be used effectively on datasets drawn from significantly different distributions and even modalities. We show that across a wide range of benchmarks, GPSE-enhanced models can significantly improve the performance in certain tasks, while performing on par with those that employ explicitly c
&lt;/p&gt;</description></item><item><title>MaxCorrMGNN&#26159;&#19968;&#31181;&#29992;&#20110;&#24191;&#20041;&#22810;&#27169;&#24577;&#21307;&#30103;&#25968;&#25454;&#34701;&#21512;&#30340;&#21019;&#26032;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#36890;&#36807;MaxCorr embeddings&#24314;&#27169;&#24739;&#32773;&#20869;&#37096;&#21644;&#24739;&#32773;&#20043;&#38388;&#30340;&#38750;&#32447;&#24615;&#27169;&#24577;&#30456;&#20851;&#24615;&#65292;&#24182;&#36890;&#36807;&#22810;&#23618;&#22270;&#32593;&#32476;&#22312;&#20219;&#21153;&#20013;&#36827;&#34892;&#25512;&#29702;&#65292;&#26377;&#25928;&#39044;&#27979;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.07093</link><description>&lt;p&gt;
MaxCorrMGNN: &#19968;&#31181;&#29992;&#20110;&#24191;&#20041;&#22810;&#27169;&#24577;&#21307;&#30103;&#25968;&#25454;&#34701;&#21512;&#30340;&#22810;&#22270;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#29992;&#20110;&#39044;&#27979;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
MaxCorrMGNN: A Multi-Graph Neural Network Framework for Generalized Multimodal Fusion of Medical Data for Outcome Prediction. (arXiv:2307.07093v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07093
&lt;/p&gt;
&lt;p&gt;
MaxCorrMGNN&#26159;&#19968;&#31181;&#29992;&#20110;&#24191;&#20041;&#22810;&#27169;&#24577;&#21307;&#30103;&#25968;&#25454;&#34701;&#21512;&#30340;&#21019;&#26032;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#36890;&#36807;MaxCorr embeddings&#24314;&#27169;&#24739;&#32773;&#20869;&#37096;&#21644;&#24739;&#32773;&#20043;&#38388;&#30340;&#38750;&#32447;&#24615;&#27169;&#24577;&#30456;&#20851;&#24615;&#65292;&#24182;&#36890;&#36807;&#22810;&#23618;&#22270;&#32593;&#32476;&#22312;&#20219;&#21153;&#20013;&#36827;&#34892;&#25512;&#29702;&#65292;&#26377;&#25928;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22810;&#27169;&#24577;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#20986;&#29616;&#65292;&#32467;&#26524;&#30340;&#35777;&#25454;&#21487;&#33021;&#28085;&#30422;&#20174;&#20020;&#24202;&#21040;&#24433;&#20687;&#21644;&#22522;&#22240;&#32452;&#25968;&#25454;&#30340;&#22810;&#20010;&#27169;&#24577;&#12290;&#26377;&#25928;&#39044;&#27979;&#32467;&#26524;&#38656;&#35201;&#33021;&#22815;&#23545;&#24739;&#32773;&#20869;&#37096;&#21644;&#24739;&#32773;&#20043;&#38388;&#30340;&#27169;&#24577;&#29305;&#24449;&#36827;&#34892;&#32454;&#31890;&#24230;&#21644;&#22810;&#26041;&#38754;&#30340;&#22797;&#26434;&#20132;&#20114;&#24314;&#27169;&#30340;&#34701;&#21512;&#26694;&#26550;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#34701;&#21512;&#26041;&#27861;&#65292;&#31216;&#20026;MaxCorr MGNN&#65292;&#36890;&#36807;Hirschfeld-Gebelein-Renyi&#26368;&#22823;&#30456;&#20851;&#24615;(MaxCorr)&#23884;&#20837;&#27169;&#22411;&#38750;&#32447;&#24615;&#27169;&#24577;&#30456;&#20851;&#24615;&#65292;&#22312;&#24739;&#32773;&#20869;&#37096;&#21644;&#24739;&#32773;&#20043;&#38388;&#24314;&#31435;&#19968;&#20010;&#20445;&#23384;&#27169;&#24577;&#21644;&#24739;&#32773;&#36523;&#20221;&#30340;&#22810;&#23618;&#22270;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#39318;&#27425;&#35774;&#35745;&#20102;&#19968;&#31181;&#29992;&#20110;&#20219;&#21153;&#24863;&#30693;&#25512;&#29702;&#30340;&#24191;&#20041;&#22810;&#23618;&#22270;&#31070;&#32463;&#32593;&#32476;(MGNN)&#65292;&#20197;&#31471;&#21040;&#31471;&#26041;&#24335;&#23398;&#20064;&#23450;&#20041;&#24739;&#32773; - &#27169;&#24577;&#22270;&#36830;&#25509;&#24615;&#21644;&#20449;&#24687;&#20256;&#36882;&#30340;&#21442;&#25968;&#12290;&#25105;&#20204;&#22312;&#32954;&#32467;&#26680;&#32467;&#26524;&#39044;&#27979;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the emergence of multimodal electronic health records, the evidence for an outcome may be captured across multiple modalities ranging from clinical to imaging and genomic data. Predicting outcomes effectively requires fusion frameworks capable of modeling fine-grained and multi-faceted complex interactions between modality features within and across patients. We develop an innovative fusion approach called MaxCorr MGNN that models non-linear modality correlations within and across patients through Hirschfeld-Gebelein-Renyi maximal correlation (MaxCorr) embeddings, resulting in a multi-layered graph that preserves the identities of the modalities and patients. We then design, for the first time, a generalized multi-layered graph neural network (MGNN) for task-informed reasoning in multi-layered graphs, that learns the parameters defining patient-modality graph connectivity and message passing in an end-to-end fashion. We evaluate our model an outcome prediction task on a Tuberculos
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36873;&#25321;&#27169;&#22411;&#21644;&#32622;&#25442;&#19981;&#21464;&#24615;&#30340;&#22522;&#26412;&#29305;&#24449;&#21270;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#38750;&#21442;&#25968;&#20272;&#35745;&#22120;&#36924;&#36817;&#36873;&#25321;&#20989;&#25968;&#65292;&#20197;&#21450;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#28789;&#27963;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.07090</link><description>&lt;p&gt;
&#36873;&#25321;&#27169;&#22411;&#21644;&#32622;&#25442;&#19981;&#21464;&#24615;
&lt;/p&gt;
&lt;p&gt;
Choice Models and Permutation Invariance. (arXiv:2307.07090v1 [econ.EM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07090
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36873;&#25321;&#27169;&#22411;&#21644;&#32622;&#25442;&#19981;&#21464;&#24615;&#30340;&#22522;&#26412;&#29305;&#24449;&#21270;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#38750;&#21442;&#25968;&#20272;&#35745;&#22120;&#36924;&#36817;&#36873;&#25321;&#20989;&#25968;&#65292;&#20197;&#21450;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#28789;&#27963;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36873;&#25321;&#24314;&#27169;&#26159;&#35768;&#22810;&#32463;&#27982;&#23398;&#12289;&#36816;&#33829;&#21644;&#33829;&#38144;&#38382;&#39064;&#30340;&#26680;&#24515;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#36873;&#25321;&#20989;&#25968;&#36827;&#34892;&#22522;&#26412;&#29305;&#24449;&#21270;&#30340;&#26041;&#27861;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#29616;&#26377;&#30340;&#36873;&#25321;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#38750;&#21442;&#25968;&#20272;&#35745;&#22120;&#22914;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#33021;&#22815;&#36731;&#26494;&#36924;&#36817;&#27492;&#31867;&#20989;&#25968;&#65292;&#24182;&#20811;&#26381;&#22312;&#38750;&#21442;&#25968;&#20272;&#35745;&#36873;&#25321;&#20989;&#25968;&#20013;&#22266;&#26377;&#30340;&#32500;&#25968;&#28798;&#38590;&#12290;&#36890;&#36807;&#22823;&#37327;&#27169;&#25311;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#20989;&#25968;&#21487;&#20197;&#20197;&#23436;&#20840;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#28789;&#27963;&#25429;&#25417;&#28508;&#22312;&#30340;&#28040;&#36153;&#32773;&#34892;&#20026;&#65292;&#24182;&#32988;&#36807;&#20256;&#32479;&#30340;&#21442;&#25968;&#27169;&#22411;&#12290;&#22240;&#20026;&#38656;&#27714;&#35774;&#32622;&#24120;&#24120;&#20855;&#26377;&#20869;&#29983;&#29305;&#24449;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26694;&#26550;&#25193;&#23637;&#21040;&#21253;&#25324;&#22312;&#20869;&#29983;&#29305;&#24449;&#19979;&#30340;&#20272;&#35745;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25551;&#36848;&#20102;&#19968;&#20010;&#24418;&#24335;&#25512;&#29702;&#31243;&#24207;&#65292;&#20197;&#26500;&#24314;&#20851;&#20110;&#20215;&#26684;&#24377;&#24615;&#31561;&#24863;&#20852;&#36259;&#23545;&#35937;&#30340;&#26377;&#25928;&#32622;&#20449;&#21306;&#38388;&#12290;&#26368;&#21518;&#65292;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#20272;&#35745;&#22120;&#30340;&#23454;&#38469;&#36866;&#29992;&#24615;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#23454;&#35777;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Choice Modeling is at the core of many economics, operations, and marketing problems. In this paper, we propose a fundamental characterization of choice functions that encompasses a wide variety of extant choice models. We demonstrate how nonparametric estimators like neural nets can easily approximate such functionals and overcome the curse of dimensionality that is inherent in the non-parametric estimation of choice functions. We demonstrate through extensive simulations that our proposed functionals can flexibly capture underlying consumer behavior in a completely data-driven fashion and outperform traditional parametric models. As demand settings often exhibit endogenous features, we extend our framework to incorporate estimation under endogenous features. Further, we also describe a formal inference procedure to construct valid confidence intervals on objects of interest like price elasticity. Finally, to assess the practical applicability of our estimator, we utilize a real-world
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;Wasserstein&#21464;&#20998;&#20248;&#21270;&#65288;AWaVO&#65289;&#26041;&#27861;&#65292;&#21033;&#29992;&#24418;&#24335;&#26041;&#27861;&#35299;&#20915;&#20102;&#39034;&#24207;&#20915;&#31574;&#20013;&#30340;&#35299;&#37322;&#21644;&#36879;&#26126;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#22870;&#21169;&#35774;&#35745;&#21644;&#31574;&#30053;&#25910;&#25947;&#30340;&#27010;&#29575;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2307.07084</link><description>&lt;p&gt;
&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#20316;&#20026;Wasserstein&#21464;&#20998;&#25512;&#29702;&#65306;&#21487;&#35299;&#37322;&#24615;&#30340;&#24418;&#24335;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Safe Reinforcement Learning as Wasserstein Variational Inference: Formal Methods for Interpretability. (arXiv:2307.07084v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07084
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;Wasserstein&#21464;&#20998;&#20248;&#21270;&#65288;AWaVO&#65289;&#26041;&#27861;&#65292;&#21033;&#29992;&#24418;&#24335;&#26041;&#27861;&#35299;&#20915;&#20102;&#39034;&#24207;&#20915;&#31574;&#20013;&#30340;&#35299;&#37322;&#21644;&#36879;&#26126;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#22870;&#21169;&#35774;&#35745;&#21644;&#31574;&#30053;&#25910;&#25947;&#30340;&#27010;&#29575;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#25110;&#26368;&#20248;&#25511;&#21046;&#21487;&#20197;&#20026;&#20855;&#26377;&#21487;&#21464;&#21160;&#24577;&#30340;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#25552;&#20379;&#26377;&#25928;&#30340;&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#23454;&#26045;&#20013;&#65292;&#35299;&#37322;&#22870;&#21169;&#20989;&#25968;&#21644;&#30456;&#24212;&#30340;&#26368;&#20248;&#31574;&#30053;&#19968;&#30452;&#26159;&#19968;&#20010;&#25345;&#20037;&#30340;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#23558;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#25512;&#29702;&#20855;&#26377;&#37325;&#35201;&#20215;&#20540;&#65292;&#22240;&#20026;&#27010;&#29575;&#25512;&#29702;&#21407;&#21017;&#19978;&#25552;&#20379;&#20102;&#22810;&#26679;&#19988;&#24378;&#22823;&#30340;&#25968;&#23398;&#24037;&#20855;&#26469;&#25512;&#26029;&#38543;&#26426;&#21160;&#24577;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#22870;&#21169;&#35774;&#35745;&#21644;&#31574;&#30053;&#25910;&#25947;&#30340;&#27010;&#29575;&#35299;&#37322;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;Wasserstein&#21464;&#20998;&#20248;&#21270;&#65288;AWaVO&#65289;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#39034;&#24207;&#20915;&#31574;&#20013;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#24418;&#24335;&#26041;&#27861;&#26469;&#35299;&#37322;&#22870;&#21169;&#35774;&#35745;&#65292;&#36879;&#26126;&#22320;&#35757;&#32451;&#25910;&#25947;&#65292;&#20197;&#21450;&#23545;&#39034;&#24207;&#20915;&#31574;&#30340;&#27010;&#29575;&#35299;&#37322;&#12290;&#20026;&#20102;&#35777;&#26126;&#23454;&#29992;&#24615;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25910;&#25947;&#35757;&#32451;&#24182;&#20445;&#35777;&#20102;&#25910;&#25947;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning or optimal control can provide effective reasoning for sequential decision-making problems with variable dynamics. Such reasoning in practical implementation, however, poses a persistent challenge in interpreting the reward function and corresponding optimal policy. Consequently, formalizing the sequential decision-making problems as inference has a considerable value, as probabilistic inference in principle offers diverse and powerful mathematical tools to infer the stochastic dynamics whilst suggesting a probabilistic interpretation of the reward design and policy convergence. In this study, we propose a novel Adaptive Wasserstein Variational Optimization (AWaVO) to tackle these challenges in sequential decision-making. Our approach utilizes formal methods to provide interpretations of reward design, transparency of training convergence, and probabilistic interpretation of sequential decisions. To demonstrate practicality, we show convergent training with guara
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22330;&#26223;&#30340;&#21151;&#33021;&#27979;&#35797;&#26041;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#27979;&#35797;ML&#27169;&#22411;&#22312;&#19981;&#21516;&#22330;&#26223;&#19979;&#34920;&#29616;&#65292;&#24182;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#27835;&#30103;&#65292;&#20197;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.07083</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#22330;&#26223;&#30340;&#21151;&#33021;&#27979;&#35797;&#26041;&#27861;&#25552;&#39640;DNN&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
A Scenario-Based Functional Testing Approach to Improving DNN Performance. (arXiv:2307.07083v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07083
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22330;&#26223;&#30340;&#21151;&#33021;&#27979;&#35797;&#26041;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#27979;&#35797;ML&#27169;&#22411;&#22312;&#19981;&#21516;&#22330;&#26223;&#19979;&#34920;&#29616;&#65292;&#24182;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#27835;&#30103;&#65292;&#20197;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22330;&#26223;&#30340;&#21151;&#33021;&#27979;&#35797;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#24212;&#29992;&#30340;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#26159;&#19968;&#20010;&#36845;&#20195;&#36807;&#31243;&#65292;&#20174;&#27979;&#35797;ML&#27169;&#22411;&#22312;&#21508;&#31181;&#24773;&#22659;&#20013;&#30340;&#34920;&#29616;&#24320;&#22987;&#65292;&#20197;&#35782;&#21035;&#34180;&#24369;&#29615;&#33410;&#12290;&#38543;&#21518;&#65292;&#22312;&#30097;&#20284;&#34180;&#24369;&#24773;&#26223;&#19978;&#36827;&#34892;&#36827;&#19968;&#27493;&#27979;&#35797;&#65292;&#24182;&#23545;&#27169;&#22411;&#22312;&#36825;&#20123;&#24773;&#26223;&#19979;&#30340;&#24615;&#33021;&#36827;&#34892;&#32479;&#35745;&#35780;&#20272;&#20197;&#30830;&#35748;&#35786;&#26029;&#12290;&#19968;&#26086;&#27979;&#35797;&#32467;&#26524;&#30830;&#35748;&#20102;&#34180;&#24369;&#24773;&#26223;&#30340;&#35786;&#26029;&#65292;&#23601;&#20250;&#36890;&#36807;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#23545;&#27169;&#22411;&#36827;&#34892;&#27835;&#30103;&#65292;&#20351;&#29992;&#21407;&#22987;&#27169;&#22411;&#20316;&#20026;&#22522;&#30784;&#65292;&#24182;&#24212;&#29992;&#19968;&#32452;&#19987;&#38376;&#38024;&#23545;&#27835;&#30103;&#24773;&#20917;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20197;&#21450;&#20174;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#38543;&#26426;&#36873;&#25321;&#30340;&#19968;&#37096;&#20998;&#35757;&#32451;&#25968;&#25454;&#65292;&#20197;&#38450;&#27490;&#25152;&#35859;&#30340;&#8220;&#28798;&#38590;&#24615;&#36951;&#24536;&#8221;&#25928;&#24212;&#12290;&#26368;&#21518;&#65292;&#22312;&#27835;&#30103;&#21518;&#65292;&#36890;&#36807;&#22312;&#27835;&#30103;&#24773;&#26223;&#20197;&#21450;&#20854;&#20182;&#24773;&#22659;&#19978;&#36827;&#34892;&#27979;&#35797;&#65292;&#35780;&#20272;&#27169;&#22411;&#30340;&#34920;&#29616;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a scenario-based functional testing approach for enhancing the performance of machine learning (ML) applications. The proposed method is an iterative process that starts with testing the ML model on various scenarios to identify areas of weakness. It follows by a further testing on the suspected weak scenarios and statistically evaluate the model's performance on the scenarios to confirm the diagnosis. Once the diagnosis of weak scenarios is confirmed by test results, the treatment of the model is performed by retraining the model using a transfer learning technique with the original model as the base and applying a set of training data specifically targeting the treated scenarios plus a subset of training data selected at random from the original train dataset to prevent the so-call catastrophic forgetting effect. Finally, after the treatment, the model is assessed and evaluated again by testing on the treated scenarios as well as other scenarios to check if the tr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26680;&#21270;&#29256;&#26412;&#30340;t-SNE&#31639;&#27861;&#65292;&#21487;&#20197;&#23558;&#39640;&#32500;&#25968;&#25454;&#26144;&#23556;&#21040;&#20302;&#32500;&#31354;&#38388;&#24182;&#20445;&#25345;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#38750;&#27431;&#20960;&#37324;&#24503;&#24230;&#37327;&#19979;&#30340;&#36317;&#31163;&#12290;&#35813;&#31639;&#27861;&#20855;&#26377;&#25913;&#21892;&#24615;&#33021;&#21644;&#20934;&#30830;&#24615;&#30340;&#28508;&#21147;&#65292;&#22312;&#28041;&#21450;&#26680;&#26041;&#27861;&#30340;&#20998;&#31867;&#38382;&#39064;&#20013;&#23637;&#29616;&#20986;&#26356;&#28165;&#26224;&#30340;&#32858;&#31867;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.07081</link><description>&lt;p&gt;
&#26680;t&#20998;&#24067;&#38543;&#26426;&#37051;&#22495;&#23884;&#20837;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Kernel t-distributed stochastic neighbor embedding. (arXiv:2307.07081v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07081
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26680;&#21270;&#29256;&#26412;&#30340;t-SNE&#31639;&#27861;&#65292;&#21487;&#20197;&#23558;&#39640;&#32500;&#25968;&#25454;&#26144;&#23556;&#21040;&#20302;&#32500;&#31354;&#38388;&#24182;&#20445;&#25345;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#38750;&#27431;&#20960;&#37324;&#24503;&#24230;&#37327;&#19979;&#30340;&#36317;&#31163;&#12290;&#35813;&#31639;&#27861;&#20855;&#26377;&#25913;&#21892;&#24615;&#33021;&#21644;&#20934;&#30830;&#24615;&#30340;&#28508;&#21147;&#65292;&#22312;&#28041;&#21450;&#26680;&#26041;&#27861;&#30340;&#20998;&#31867;&#38382;&#39064;&#20013;&#23637;&#29616;&#20986;&#26356;&#28165;&#26224;&#30340;&#32858;&#31867;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;t-SNE&#31639;&#27861;&#30340;&#26680;&#21270;&#29256;&#26412;&#65292;&#33021;&#22815;&#23558;&#39640;&#32500;&#25968;&#25454;&#26144;&#23556;&#21040;&#20302;&#32500;&#31354;&#38388;&#65292;&#21516;&#26102;&#20445;&#30041;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#38750;&#27431;&#20960;&#37324;&#24503;&#24230;&#37327;&#19979;&#30340;&#25104;&#23545;&#36317;&#31163;&#12290;&#36825;&#21487;&#20197;&#36890;&#36807;&#22312;&#39640;&#32500;&#31354;&#38388;&#25110;&#20004;&#20010;&#31354;&#38388;&#20013;&#20351;&#29992;&#26680;&#25216;&#24039;&#26469;&#23454;&#29616;&#65292;&#20174;&#32780;&#24471;&#21040;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#26680;&#21270;&#29256;&#26412;&#12290;t-SNE&#31639;&#27861;&#30340;&#26680;&#21270;&#29256;&#26412;&#21487;&#20197;&#25552;&#20379;&#20851;&#20110;&#25968;&#25454;&#28857;&#20043;&#38388;&#20851;&#31995;&#30340;&#26032;&#35270;&#35282;&#65292;&#21487;&#20197;&#22312;&#29305;&#23450;&#24212;&#29992;&#20013;&#25913;&#21892;&#24615;&#33021;&#21644;&#20934;&#30830;&#24615;&#65292;&#22914;&#28041;&#21450;&#26680;&#26041;&#27861;&#30340;&#20998;&#31867;&#38382;&#39064;&#12290;&#36890;&#36807;&#23545;&#22810;&#20010;&#25968;&#25454;&#38598;&#30340;&#27604;&#36739;&#65292;&#23637;&#31034;&#20102;t-SNE&#21644;&#20854;&#26680;&#21270;&#29256;&#26412;&#30340;&#24046;&#24322;&#65292;&#26174;&#31034;&#20986;&#19981;&#21516;&#31867;&#21035;&#30340;&#25968;&#25454;&#28857;&#20043;&#38388;&#26356;&#28165;&#26224;&#30340;&#32858;&#31867;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a kernelized version of the t-SNE algorithm, capable of mapping high-dimensional data to a low-dimensional space while preserving the pairwise distances between the data points in a non-Euclidean metric. This can be achieved using a kernel trick only in the high dimensional space or in both spaces, leading to an end-to-end kernelized version. The proposed kernelized version of the t-SNE algorithm can offer new views on the relationships between data points, which can improve performance and accuracy in particular applications, such as classification problems involving kernel methods. The differences between t-SNE and its kernelized version are illustrated for several datasets, showing a neater clustering of points belonging to different classes.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#23450;&#37327;MRI&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#36127;&#23545;&#25968;Rician&#20284;&#28982;&#65288;NLR&#65289;&#25439;&#22833;&#20989;&#25968;&#65292;&#35299;&#20915;&#20102;&#20302;&#20449;&#22122;&#27604;&#26465;&#20214;&#19979;&#21442;&#25968;&#20272;&#35745;&#20559;&#24046;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.07072</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#30340;Rician&#20284;&#28982;&#25439;&#22833;&#36827;&#34892;&#23450;&#37327;MRI
&lt;/p&gt;
&lt;p&gt;
Rician likelihood loss for quantitative MRI using self-supervised deep learning. (arXiv:2307.07072v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07072
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#23450;&#37327;MRI&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#36127;&#23545;&#25968;Rician&#20284;&#28982;&#65288;NLR&#65289;&#25439;&#22833;&#20989;&#25968;&#65292;&#35299;&#20915;&#20102;&#20302;&#20449;&#22122;&#27604;&#26465;&#20214;&#19979;&#21442;&#25968;&#20272;&#35745;&#20559;&#24046;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;&#20197;&#21069;&#20351;&#29992;&#33258;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#30340;&#23450;&#37327;MRI&#30740;&#31350;&#25253;&#21578;&#22312;&#20302;&#20449;&#22122;&#27604;&#26465;&#20214;&#19979;&#23384;&#22312;&#21442;&#25968;&#20272;&#35745;&#20559;&#24046;&#30340;&#38382;&#39064;&#12290;&#36825;&#20123;&#31995;&#32479;&#35823;&#24046;&#26469;&#33258;&#20110;&#32593;&#32476;&#35757;&#32451;&#20013;&#36873;&#25321;&#30340;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#25439;&#22833;&#20989;&#25968;&#65292;&#35813;&#20989;&#25968;&#19982;Rician&#20998;&#24067;&#30340;MR&#24133;&#24230;&#20449;&#21495;&#19981;&#20860;&#23481;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36127;&#23545;&#25968;Rician&#20284;&#28982;&#65288;NLR&#65289;&#25439;&#22833;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#25968;&#20540;&#31283;&#23450;&#19988;&#20934;&#30830;&#30340;NLR&#25439;&#22833;&#30340;&#23454;&#29616;&#65292;&#29992;&#20110;&#20272;&#35745;&#34920;&#35266;&#25193;&#25955;&#31995;&#25968;&#65288;ADC&#65289;&#27169;&#22411;&#21644;&#20307;&#32032;&#20869;&#19981;&#30456;&#24178;&#36816;&#21160;&#65288;IVIM&#65289;&#27169;&#22411;&#30340;&#23450;&#37327;&#21442;&#25968;&#12290;&#36890;&#36807;&#22312;&#19968;&#31995;&#21015;&#20449;&#22122;&#27604;&#65288;5-30&#65289;&#19979;&#27604;&#36739;&#20559;&#24046;&#12289;&#26041;&#24046;&#21644;&#22343;&#26041;&#26681;&#35823;&#24046;&#26469;&#35780;&#20272;&#21442;&#25968;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12289;&#31934;&#24230;&#21644;&#24635;&#20307;&#35823;&#24046;&#65292;&#24182;&#19982;MSE&#25439;&#22833;&#36827;&#34892;&#27604;&#36739;&#12290;&#32467;&#26524;&#65306;&#20351;&#29992;NLR&#25439;&#22833;&#35757;&#32451;&#30340;&#32593;&#32476;&#22312;SNR&#38477;&#20302;&#26102;&#26174;&#31034;&#20986;&#27604;MSE&#26356;&#39640;&#30340;ADC&#21644;IVIM&#25193;&#25955;&#31995;&#25968;&#20272;&#35745;&#20934;&#30830;&#24615;&#65292;&#32780;&#31934;&#24230;&#25110;&#24635;&#35823;&#24046;&#20960;&#20046;&#27809;&#26377;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
Purpose: Previous quantitative MR imaging studies using self-supervised deep learning have reported biased parameter estimates at low SNR. Such systematic errors arise from the choice of Mean Squared Error (MSE) loss function for network training, which is incompatible with Rician-distributed MR magnitude signals. To address this issue, we introduce the negative log Rician likelihood (NLR) loss. Methods: A numerically stable and accurate implementation of the NLR loss was developed to estimate quantitative parameters of the apparent diffusion coefficient (ADC) model and intra-voxel incoherent motion (IVIM) model. Parameter estimation accuracy, precision and overall error were evaluated in terms of bias, variance and root mean squared error and compared against the MSE loss over a range of SNRs (5 - 30). Results: Networks trained with NLR loss show higher estimation accuracy than MSE for the ADC and IVIM diffusion coefficients as SNR decreases, with minimal loss of precision or total er
&lt;/p&gt;</description></item><item><title>Proof of Training (PoT)&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;AI&#21644;&#21306;&#22359;&#38142;&#25216;&#26415;&#30340;&#21327;&#35758;&#65292;&#21033;&#29992;&#23454;&#29992;&#24615;&#25308;&#21344;&#24237;&#38169;&#35823;&#23481;&#24525;&#65288;PBFT&#65289;&#20849;&#35782;&#26426;&#21046;&#21516;&#27493;&#20840;&#23616;&#29366;&#24577;&#12290;&#35813;&#21327;&#35758;&#22312;&#20219;&#21153;&#21534;&#21520;&#37327;&#12289;&#31995;&#32479;&#40065;&#26834;&#24615;&#21644;&#32593;&#32476;&#23433;&#20840;&#26041;&#38754;&#34920;&#29616;&#20986;&#30456;&#24403;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.07066</link><description>&lt;p&gt;
Proof of Training (PoT): &#21033;&#29992;&#21152;&#23494;&#25366;&#25496;&#30340;&#31639;&#21147;&#36827;&#34892;&#20998;&#24067;&#24335;AI&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Proof of Training (PoT): Harnessing Crypto Mining Power for Distributed AI Training. (arXiv:2307.07066v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07066
&lt;/p&gt;
&lt;p&gt;
Proof of Training (PoT)&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;AI&#21644;&#21306;&#22359;&#38142;&#25216;&#26415;&#30340;&#21327;&#35758;&#65292;&#21033;&#29992;&#23454;&#29992;&#24615;&#25308;&#21344;&#24237;&#38169;&#35823;&#23481;&#24525;&#65288;PBFT&#65289;&#20849;&#35782;&#26426;&#21046;&#21516;&#27493;&#20840;&#23616;&#29366;&#24577;&#12290;&#35813;&#21327;&#35758;&#22312;&#20219;&#21153;&#21534;&#21520;&#37327;&#12289;&#31995;&#32479;&#40065;&#26834;&#24615;&#21644;&#32593;&#32476;&#23433;&#20840;&#26041;&#38754;&#34920;&#29616;&#20986;&#30456;&#24403;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#19982;&#21152;&#23494;&#25366;&#25496;&#30456;&#34701;&#21512;&#30340;&#26032;&#36235;&#21183;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20004;&#20010;&#39046;&#22495;&#23384;&#22312;&#19977;&#20010;&#20027;&#35201;&#25361;&#25112;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;Proof of Training (PoT)&#30340;&#21327;&#35758;&#65292;&#23427;&#32467;&#21512;&#20102;AI&#21644;&#21306;&#22359;&#38142;&#25216;&#26415;&#30340;&#20248;&#21183;&#12290;PoT&#21327;&#35758;&#21033;&#29992;&#23454;&#29992;&#24615;&#25308;&#21344;&#24237;&#38169;&#35823;&#23481;&#24525;&#65288;PBFT&#65289;&#20849;&#35782;&#26426;&#21046;&#26469;&#21516;&#27493;&#20840;&#23616;&#29366;&#24577;&#12290;&#20026;&#20102;&#35780;&#20272;&#21327;&#35758;&#35774;&#35745;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;PoT&#21327;&#35758;&#30340;&#20998;&#25955;&#35757;&#32451;&#32593;&#32476;&#65288;DTN&#65289;&#30340;&#23454;&#29616;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#21327;&#35758;&#22312;&#20219;&#21153;&#21534;&#21520;&#37327;&#12289;&#31995;&#32479;&#40065;&#26834;&#24615;&#21644;&#32593;&#32476;&#23433;&#20840;&#26041;&#38754;&#20855;&#26377;&#30456;&#24403;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the midst of the emerging trend of integrating artificial intelligence (AI) with crypto mining, we identify three major challenges that create a gap between these two fields. To bridge this gap, we introduce the proof-of-training (PoT) protocol, an approach that combines the strengths of both AI and blockchain technology. The PoT protocol utilizes the practical Byzantine fault tolerance (PBFT) consensus mechanism to synchronize global states. To evaluate the performance of the protocol design, we present an implementation of a decentralized training network (DTN) that adopts the PoT protocol. Our results indicate that the protocol exhibits considerable potential in terms of task throughput, system robustness, and network security.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#32806;&#35821;&#35328;&#39044;&#35757;&#32451;&#65292;&#38598;&#20013;&#22312;&#35821;&#35328;&#32452;&#20214;&#19978;&#65292;&#25552;&#20379;&#20102;&#20248;&#21270;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#26377;&#25928;&#25913;&#21892;&#20102;&#35270;&#35273;&#35821;&#35328;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.07063</link><description>&lt;p&gt;
&#20351;&#29992;&#35299;&#32806;&#30340;&#35821;&#35328;&#39044;&#35757;&#32451;&#20026;&#35270;&#35273;&#35821;&#35328;&#23398;&#20064;&#24341;&#20837;&#24341;&#23548;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Bootstrapping Vision-Language Learning with Decoupled Language Pre-training. (arXiv:2307.07063v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07063
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#32806;&#35821;&#35328;&#39044;&#35757;&#32451;&#65292;&#38598;&#20013;&#22312;&#35821;&#35328;&#32452;&#20214;&#19978;&#65292;&#25552;&#20379;&#20102;&#20248;&#21270;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#26377;&#25928;&#25913;&#21892;&#20102;&#35270;&#35273;&#35821;&#35328;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#20248;&#21270;&#20923;&#32467;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#36164;&#28304;&#23494;&#38598;&#22411;&#35270;&#35273;&#35821;&#35328;&#65288;VL&#65289;&#39044;&#35757;&#32451;&#20013;&#30340;&#24212;&#29992;&#12290;&#24403;&#21069;&#30340;&#33539;&#24335;&#20351;&#29992;&#35270;&#35273;&#29305;&#24449;&#20316;&#20026;&#25552;&#31034;&#26469;&#24341;&#23548;&#35821;&#35328;&#27169;&#22411;&#65292;&#37325;&#28857;&#26159;&#30830;&#23450;&#19982;&#30456;&#24212;&#25991;&#26412;&#26368;&#30456;&#20851;&#30340;&#35270;&#35273;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#38598;&#20013;&#22312;&#35821;&#35328;&#32452;&#20214;&#19978;&#65292;&#20855;&#20307;&#26159;&#30830;&#23450;&#19982;&#35270;&#35273;&#29305;&#24449;&#23545;&#40784;&#30340;&#26368;&#20339;&#25552;&#31034;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;Prompt-Transformer&#65288;P-Former&#65289;&#65292;&#19968;&#31181;&#21487;&#20197;&#39044;&#27979;&#36825;&#20123;&#29702;&#24819;&#25552;&#31034;&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20165;&#22312;&#35821;&#35328;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#36991;&#20813;&#20102;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#30340;&#38656;&#35201;&#12290;&#36825;&#31181;&#31574;&#30053;&#23558;&#31471;&#21040;&#31471;&#30340;VL&#35757;&#32451;&#36807;&#31243;&#24039;&#22937;&#22320;&#20998;&#20026;&#20102;&#39069;&#22806;&#30340;&#29420;&#31435;&#38454;&#27573;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#26174;&#33879;&#25552;&#39640;&#20102;&#31283;&#20581;&#30340;&#22270;&#20687;&#21040;&#25991;&#26412;&#22522;&#32447;&#65288;BLIP-2&#65289;&#30340;&#24615;&#33021;&#65292;&#24182;&#26377;&#25928;&#22320;&#32553;&#23567;&#20102;&#20351;&#29992;4M&#25110;129M&#22270;&#20687;-&#25991;&#26412;&#23545;&#36827;&#34892;&#35757;&#32451;&#30340;&#27169;&#22411;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel methodology aimed at optimizing the application of frozen large language models (LLMs) for resource-intensive vision-language (VL) pre-training. The current paradigm uses visual features as prompts to guide language models, with a focus on determining the most relevant visual features for corresponding text. Our approach diverges by concentrating on the language component, specifically identifying the optimal prompts to align with visual features. We introduce the Prompt-Transformer (P-Former), a model that predicts these ideal prompts, which is trained exclusively on linguistic data, bypassing the need for image-text pairings. This strategy subtly bifurcates the end-to-end VL training process into an additional, separate stage. Our experiments reveal that our framework significantly enhances the performance of a robust image-to-text baseline (BLIP-2), and effectively narrows the performance gap between models trained with either 4M or 129M image-text pairs. Importan
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#25968;&#25454;&#30340;&#21487;&#25193;&#23637;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#25991;&#26412;&#36716;&#35821;&#38899;&#20013;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#37325;&#38899;&#65292;&#19981;&#38656;&#35201;&#29992;&#21040;&#24405;&#38899;&#25110;&#26631;&#27880;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#36890;&#36807;&#22686;&#21152;&#37325;&#38899;&#35789;&#30340;&#39044;&#27979;&#25345;&#32493;&#26102;&#38388;&#26469;&#23454;&#29616;&#37325;&#38899;&#35821;&#38899;&#65292;&#30456;&#27604;&#20110;&#22768;&#35889;&#22270;&#20462;&#25913;&#25216;&#26415;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#33258;&#28982;&#24230;&#21644;&#37325;&#38899;&#35789;&#35782;&#21035;&#26041;&#38754;&#37117;&#26377;&#26174;&#33879;&#30340;&#25552;&#39640;&#12290;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#35821;&#35328;&#12289;&#19981;&#21516;&#35821;&#38899;&#21644;&#22810;&#31181;&#35821;&#35328;&#39118;&#26684;&#19979;&#37117;&#34987;&#35777;&#26126;&#26159;&#21487;&#25193;&#23637;&#19988;&#39318;&#36873;&#30340;&#12290;</title><link>http://arxiv.org/abs/2307.07062</link><description>&lt;p&gt;
&#38646;&#25968;&#25454;&#19979;&#30340;&#21487;&#25511;&#37325;&#38899;&#25991;&#26412;&#36716;&#35821;&#38899;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Controllable Emphasis with zero data for text-to-speech. (arXiv:2307.07062v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07062
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#25968;&#25454;&#30340;&#21487;&#25193;&#23637;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#25991;&#26412;&#36716;&#35821;&#38899;&#20013;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#37325;&#38899;&#65292;&#19981;&#38656;&#35201;&#29992;&#21040;&#24405;&#38899;&#25110;&#26631;&#27880;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#36890;&#36807;&#22686;&#21152;&#37325;&#38899;&#35789;&#30340;&#39044;&#27979;&#25345;&#32493;&#26102;&#38388;&#26469;&#23454;&#29616;&#37325;&#38899;&#35821;&#38899;&#65292;&#30456;&#27604;&#20110;&#22768;&#35889;&#22270;&#20462;&#25913;&#25216;&#26415;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#33258;&#28982;&#24230;&#21644;&#37325;&#38899;&#35789;&#35782;&#21035;&#26041;&#38754;&#37117;&#26377;&#26174;&#33879;&#30340;&#25552;&#39640;&#12290;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#35821;&#35328;&#12289;&#19981;&#21516;&#35821;&#38899;&#21644;&#22810;&#31181;&#35821;&#35328;&#39118;&#26684;&#19979;&#37117;&#34987;&#35777;&#26126;&#26159;&#21487;&#25193;&#23637;&#19988;&#39318;&#36873;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#39640;&#36136;&#37327;&#25991;&#26412;&#36716;&#35821;&#38899;&#65288;TTS&#65289;&#30340;&#37325;&#38899;&#29983;&#25104;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#24405;&#38899;&#25110;&#27880;&#37322;&#12290;&#35768;&#22810;TTS&#27169;&#22411;&#21253;&#21547;&#19968;&#20010;&#38899;&#32032;&#25345;&#32493;&#26102;&#38388;&#27169;&#22411;&#12290;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26041;&#27861;&#26159;&#22686;&#21152;&#37325;&#38899;&#35789;&#30340;&#39044;&#27979;&#25345;&#32493;&#26102;&#38388;&#26469;&#23454;&#29616;&#37325;&#38899;&#35821;&#38899;&#12290;&#25105;&#20204;&#35777;&#26126;&#36825;&#27604;&#22768;&#35889;&#22270;&#20462;&#25913;&#25216;&#26415;&#26174;&#33879;&#22909;&#65292;&#33258;&#28982;&#24615;&#25552;&#39640;&#20102;7.3&#65285;&#65292;&#22312;&#19968;&#20010;&#21442;&#32771;&#32654;&#22269;&#33521;&#35821;&#22899;&#22768;&#20013;&#65292;&#21463;&#35797;&#32773;&#23545;&#21477;&#23376;&#20013;&#37325;&#38899;&#35789;&#30340;&#35782;&#21035;&#27491;&#30830;&#29575;&#25552;&#39640;&#20102;40&#65285;&#12290;&#25105;&#20204;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#26174;&#33879;&#32553;&#23567;&#20102;&#19982;&#38656;&#35201;&#26174;&#24335;&#24405;&#38899;&#30340;&#26041;&#27861;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#35813;&#26041;&#27861;&#22312;&#22235;&#31181;&#19981;&#21516;&#35821;&#35328;&#65288;&#33521;&#35821;&#12289;&#35199;&#29677;&#29273;&#35821;&#12289;&#24847;&#22823;&#21033;&#35821;&#12289;&#24503;&#35821;&#65289;&#12289;&#19981;&#21516;&#35821;&#38899;&#21644;&#22810;&#31181;&#35821;&#35328;&#39118;&#26684;&#19979;&#37117;&#34987;&#35777;&#26126;&#26159;&#21487;&#25193;&#23637;&#19988;&#39318;&#36873;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a scalable method to produce high quality emphasis for text-to-speech (TTS) that does not require recordings or annotations. Many TTS models include a phoneme duration model. A simple but effective method to achieve emphasized speech consists in increasing the predicted duration of the emphasised word. We show that this is significantly better than spectrogram modification techniques improving naturalness by $7.3\%$ and correct testers' identification of the emphasized word in a sentence by $40\%$ on a reference female en-US voice. We show that this technique significantly closes the gap to methods that require explicit recordings. The method proved to be scalable and preferred in all four languages tested (English, Spanish, Italian, German), for different voices and multiple speaking styles.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#26377;&#22870;&#23548;&#21521;&#29983;&#25104;&#65292;&#33021;&#22815;&#26377;&#25928;&#23398;&#20064;&#21644;&#20174;&#22870;&#21169;&#26465;&#20214;&#19979;&#30340;&#25968;&#25454;&#20998;&#24067;&#20013;&#36827;&#34892;&#37319;&#26679;&#65292;&#21516;&#26102;&#24674;&#22797;&#25968;&#25454;&#30340;&#28508;&#22312;&#23376;&#31354;&#38388;&#34920;&#31034;&#65292;&#24182;&#19988;&#29983;&#25104;&#26032;&#30340;&#32676;&#20307;&#38752;&#36817;&#29992;&#25143;&#25351;&#23450;&#30340;&#30446;&#26631;&#22870;&#21169;&#20540;&#12290;</title><link>http://arxiv.org/abs/2307.07055</link><description>&lt;p&gt;
&#26377;&#22870;&#23548;&#21521;&#30340;&#26465;&#20214;&#25193;&#25955;&#65306;&#21487;&#35777;&#26126;&#30340;&#20998;&#24067;&#20272;&#35745;&#21644;&#22870;&#21169;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Reward-Directed Conditional Diffusion: Provable Distribution Estimation and Reward Improvement. (arXiv:2307.07055v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07055
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#26377;&#22870;&#23548;&#21521;&#29983;&#25104;&#65292;&#33021;&#22815;&#26377;&#25928;&#23398;&#20064;&#21644;&#20174;&#22870;&#21169;&#26465;&#20214;&#19979;&#30340;&#25968;&#25454;&#20998;&#24067;&#20013;&#36827;&#34892;&#37319;&#26679;&#65292;&#21516;&#26102;&#24674;&#22797;&#25968;&#25454;&#30340;&#28508;&#22312;&#23376;&#31354;&#38388;&#34920;&#31034;&#65292;&#24182;&#19988;&#29983;&#25104;&#26032;&#30340;&#32676;&#20307;&#38752;&#36817;&#29992;&#25143;&#25351;&#23450;&#30340;&#30446;&#26631;&#22870;&#21169;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#36890;&#36807;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#26377;&#22870;&#23548;&#21521;&#29983;&#25104;&#30340;&#26041;&#27861;&#21644;&#29702;&#35770;&#12290;&#26377;&#22870;&#23548;&#21521;&#29983;&#25104;&#26088;&#22312;&#29983;&#25104;&#20855;&#26377;&#30001;&#22870;&#21169;&#20989;&#25968;&#34913;&#37327;&#30340;&#25152;&#38656;&#29305;&#24615;&#30340;&#26679;&#26412;&#65292;&#24191;&#27867;&#24212;&#29992;&#20110;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#12289;&#24378;&#21270;&#23398;&#20064;&#21644;&#35745;&#31639;&#29983;&#29289;&#23398;&#39046;&#22495;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#25968;&#25454;&#38598;&#21253;&#21547;&#26410;&#26631;&#35760;&#25968;&#25454;&#21644;&#19968;&#23567;&#37096;&#20998;&#20855;&#26377;&#24102;&#22122;&#22768;&#22870;&#21169;&#26631;&#31614;&#30340;&#25968;&#25454;&#30340;&#24120;&#35265;&#23398;&#20064;&#22330;&#26223;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#22312;&#36739;&#23567;&#25968;&#25454;&#38598;&#19978;&#23398;&#20064;&#21040;&#30340;&#22870;&#21169;&#20989;&#25968;&#20316;&#20026;&#20266;&#26631;&#31614;&#29983;&#25104;&#22120;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20010;&#26377;&#23548;&#21521;&#30340;&#29983;&#25104;&#22120;&#33021;&#22815;&#26377;&#25928;&#22320;&#23398;&#20064;&#21644;&#20174;&#22870;&#21169;&#26465;&#20214;&#19979;&#30340;&#25968;&#25454;&#20998;&#24067;&#20013;&#36827;&#34892;&#37319;&#26679;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#24674;&#22797;&#25968;&#25454;&#30340;&#28508;&#22312;&#23376;&#31354;&#38388;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24314;&#31435;&#20102;&#27169;&#22411;&#29983;&#25104;&#19968;&#20010;&#26032;&#30340;&#32676;&#20307;&#65292;&#35813;&#32676;&#20307;&#38752;&#36817;&#29992;&#25143;&#25351;&#23450;&#30340;&#30446;&#26631;&#22870;&#21169;&#20540;&#65292;&#20854;&#20013;&#26368;&#20248;&#24615;&#24046;&#36317;&#19982;&#29305;&#24449;&#23376;&#31354;&#38388;&#20013;&#30340;&#31163;&#32447;&#31574;&#30053;&#24378;&#30423;&#36951;&#25022;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore the methodology and theory of reward-directed generation via conditional diffusion models. Directed generation aims to generate samples with desired properties as measured by a reward function, which has broad applications in generative AI, reinforcement learning, and computational biology. We consider the common learning scenario where the data set consists of unlabeled data along with a smaller set of data with noisy reward labels. Our approach leverages a learned reward function on the smaller data set as a pseudolabeler. From a theoretical standpoint, we show that this directed generator can effectively learn and sample from the reward-conditioned data distribution. Additionally, our model is capable of recovering the latent subspace representation of data. Moreover, we establish that the model generates a new population that moves closer to a user-specified target reward value, where the optimality gap aligns with the off-policy bandit regret in the feature subspace. Th
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#26512;&#20020;&#24202;&#35760;&#24405;&#30340;&#37096;&#20998;&#65292;&#25105;&#20204;&#21457;&#29616;&#39044;&#27979;&#33021;&#21147;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#35760;&#24405;&#38388;&#23384;&#22312;&#24046;&#24322;&#65292;&#24182;&#19988;&#24403;&#19978;&#19979;&#25991;&#38271;&#24230;&#36739;&#22823;&#26102;&#65292;&#32452;&#21512;&#19981;&#21516;&#31867;&#22411;&#30340;&#35760;&#24405;&#21487;&#20197;&#25913;&#21892;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#31934;&#24515;&#36873;&#25321;&#30340;&#37319;&#26679;&#20989;&#25968;&#21487;&#20197;&#20351;&#20174;&#20020;&#24202;&#35760;&#24405;&#20013;&#25552;&#21462;&#20449;&#24687;&#26356;&#21152;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2307.07051</link><description>&lt;p&gt;
&#20805;&#20998;&#21033;&#29992;&#26377;&#38480;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#65306;&#39044;&#27979;&#33021;&#21147;&#38543;&#20020;&#24202;&#35760;&#24405;&#31867;&#22411;&#21644;&#35760;&#24405;&#37096;&#20998;&#30340;&#19981;&#21516;&#32780;&#21464;&#21270;
&lt;/p&gt;
&lt;p&gt;
Making the Most Out of the Limited Context Length: Predictive Power Varies with Clinical Note Type and Note Section. (arXiv:2307.07051v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07051
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#26512;&#20020;&#24202;&#35760;&#24405;&#30340;&#37096;&#20998;&#65292;&#25105;&#20204;&#21457;&#29616;&#39044;&#27979;&#33021;&#21147;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#35760;&#24405;&#38388;&#23384;&#22312;&#24046;&#24322;&#65292;&#24182;&#19988;&#24403;&#19978;&#19979;&#25991;&#38271;&#24230;&#36739;&#22823;&#26102;&#65292;&#32452;&#21512;&#19981;&#21516;&#31867;&#22411;&#30340;&#35760;&#24405;&#21487;&#20197;&#25913;&#21892;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#31934;&#24515;&#36873;&#25321;&#30340;&#37319;&#26679;&#20989;&#25968;&#21487;&#20197;&#20351;&#20174;&#20020;&#24202;&#35760;&#24405;&#20013;&#25552;&#21462;&#20449;&#24687;&#26356;&#21152;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#23637;&#20351;&#24471;&#20351;&#29992;&#20020;&#24202;&#35760;&#24405;&#30340;&#33258;&#30001;&#25991;&#26412;&#36827;&#34892;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#20852;&#36259;&#37325;&#26032;&#29123;&#36215;&#12290;&#20020;&#24202;&#35760;&#24405;&#30340;&#19968;&#20010;&#21306;&#21035;&#29305;&#28857;&#26159;&#23427;&#20204;&#36328;&#36234;&#22810;&#20010;&#38271;&#25991;&#26723;&#30340;&#38271;&#26102;&#38388;&#36328;&#24230;&#12290;&#20020;&#24202;&#35760;&#24405;&#30340;&#29420;&#29305;&#32467;&#26500;&#24102;&#26469;&#20102;&#19968;&#20010;&#26032;&#30340;&#35774;&#35745;&#36873;&#25321;&#65306;&#24403;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#22120;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#26377;&#38480;&#26102;&#65292;&#24212;&#36873;&#25321;&#20020;&#24202;&#35760;&#24405;&#30340;&#21738;&#20010;&#37096;&#20998;&#20316;&#20026;&#36755;&#20837;&#65311;&#29616;&#26377;&#30740;&#31350;&#35201;&#20040;&#36873;&#25321;&#20855;&#26377;&#39046;&#22495;&#30693;&#35782;&#30340;&#36755;&#20837;&#65292;&#35201;&#20040;&#31616;&#21333;&#22320;&#25130;&#26029;&#23427;&#20204;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#26512;&#39640;&#39044;&#27979;&#33021;&#21147;&#37096;&#20998;&#30340;&#26694;&#26550;&#12290;&#20351;&#29992;MIMIC-III&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20197;&#19979;&#21457;&#29616;&#65306;1&#65289;&#39044;&#27979;&#33021;&#21147;&#20998;&#24067;&#22312;&#25252;&#29702;&#35760;&#24405;&#21644;&#20986;&#38498;&#35760;&#24405;&#20043;&#38388;&#26159;&#19981;&#21516;&#30340;&#65307;2&#65289;&#24403;&#19978;&#19979;&#25991;&#38271;&#24230;&#36739;&#22823;&#26102;&#65292;&#32452;&#21512;&#19981;&#21516;&#31867;&#22411;&#30340;&#35760;&#24405;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#31934;&#24515;&#36873;&#25321;&#30340;&#37319;&#26679;&#20989;&#25968;&#21487;&#20197;&#20351;&#20174;&#20020;&#24202;&#35760;&#24405;&#20013;&#25552;&#21462;&#20449;&#24687;&#26356;&#21152;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in large language models have led to renewed interest in natural language processing in healthcare using the free text of clinical notes. One distinguishing characteristic of clinical notes is their long time span over multiple long documents. The unique structure of clinical notes creates a new design choice: when the context length for a language model predictor is limited, which part of clinical notes should we choose as the input? Existing studies either choose the inputs with domain knowledge or simply truncate them. We propose a framework to analyze the sections with high predictive power. Using MIMIC-III, we show that: 1) predictive power distribution is different between nursing notes and discharge notes and 2) combining different types of notes could improve performance when the context length is large. Our findings suggest that a carefully selected sampling function could enable more efficient information extraction from clinical notes.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;&#22522;&#20110;Wasserstein&#37327;&#23376;&#33945;&#29305;&#21345;&#27931;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#37327;&#23376;&#22810;&#20307;Schr\"odinger&#26041;&#31243;&#12290;&#35813;&#26041;&#27861;&#37325;&#26032;&#21046;&#23450;&#20102;&#33021;&#37327;&#27867;&#20989;&#30340;&#26368;&#23567;&#21270;&#38382;&#39064;&#65292;&#24182;&#23558;&#20854;&#36716;&#21270;&#20026;Born&#20998;&#24067;&#31354;&#38388;&#20013;&#30340;&#31890;&#23376;&#25490;&#21015;&#65288;&#21453;&#65289;&#23545;&#31216;&#27874;&#20989;&#25968;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#34920;&#31034;&#20016;&#23500;&#30340;&#27874;&#20989;&#25968;&#26063;&#12290;</title><link>http://arxiv.org/abs/2307.07050</link><description>&lt;p&gt;
Wasserstein&#37327;&#23376;&#33945;&#29305;&#21345;&#27931;&#65306;&#35299;&#20915;&#37327;&#23376;&#22810;&#20307;Schr\"odinger&#26041;&#31243;&#30340;&#19968;&#31181;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Wasserstein Quantum Monte Carlo: A Novel Approach for Solving the Quantum Many-Body Schr\"odinger Equation. (arXiv:2307.07050v1 [physics.comp-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07050
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;&#22522;&#20110;Wasserstein&#37327;&#23376;&#33945;&#29305;&#21345;&#27931;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#37327;&#23376;&#22810;&#20307;Schr\"odinger&#26041;&#31243;&#12290;&#35813;&#26041;&#27861;&#37325;&#26032;&#21046;&#23450;&#20102;&#33021;&#37327;&#27867;&#20989;&#30340;&#26368;&#23567;&#21270;&#38382;&#39064;&#65292;&#24182;&#23558;&#20854;&#36716;&#21270;&#20026;Born&#20998;&#24067;&#31354;&#38388;&#20013;&#30340;&#31890;&#23376;&#25490;&#21015;&#65288;&#21453;&#65289;&#23545;&#31216;&#27874;&#20989;&#25968;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#34920;&#31034;&#20016;&#23500;&#30340;&#27874;&#20989;&#25968;&#26063;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#37327;&#23376;&#22810;&#20307;Schr\"odinger&#26041;&#31243;&#26159;&#37327;&#23376;&#29289;&#29702;&#12289;&#37327;&#23376;&#21270;&#23398;&#21644;&#26448;&#26009;&#31185;&#23398;&#39046;&#22495;&#20013;&#19968;&#20010;&#22522;&#26412;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#19968;&#31181;&#24120;&#35265;&#30340;&#35745;&#31639;&#26041;&#27861;&#26159;&#37327;&#23376;&#21464;&#20998;&#33945;&#29305;&#21345;&#27931;&#65288;QVMC&#65289;&#65292;&#20854;&#20013;&#36890;&#36807;&#22312;&#19968;&#20010;&#21442;&#25968;&#21270;&#27874;&#20989;&#25968;&#26063;&#20013;&#26368;&#23567;&#21270;&#31995;&#32479;&#30340;&#33021;&#37327;&#26469;&#33719;&#24471;&#22522;&#24577;&#35299;&#12290;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#35299;&#20915;&#20102;&#20256;&#32479;QVMC&#30340;&#23616;&#38480;&#24615;&#65292;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#20016;&#23500;&#30340;&#27874;&#20989;&#25968;&#26063;&#12290;&#28982;&#32780;&#65292;&#22312;QVMC&#20013;&#20248;&#21270;&#30446;&#26631;&#20173;&#28982;&#38590;&#20197;&#26368;&#23567;&#21270;&#65292;&#38656;&#35201;&#20351;&#29992;&#33258;&#28982;&#26799;&#24230;&#31561;&#20108;&#38454;&#20248;&#21270;&#26041;&#27861;&#12290;&#26412;&#25991;&#39318;&#20808;&#37325;&#26032;&#21046;&#23450;&#20102;&#33021;&#37327;&#27867;&#20989;&#30340;&#26368;&#23567;&#21270;&#38382;&#39064;&#65292;&#23558;&#20854;&#36716;&#21270;&#20026;Born&#20998;&#24067;&#31354;&#38388;&#20013;&#30340;&#31890;&#23376;&#25490;&#21015;&#65288;&#21453;&#65289;&#23545;&#31216;&#27874;&#20989;&#25968;&#30340;&#38382;&#39064;&#65292;&#32780;&#19981;&#26159;&#27874;&#20989;&#25968;&#30340;&#31354;&#38388;&#12290;&#28982;&#21518;&#25105;&#20204;&#23558;QVMC&#35299;&#37322;&#20026;&#22312;&#36825;&#20010;&#31354;&#38388;&#20013;&#30340;Fisher-Rao&#26799;&#24230;&#27969;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solving the quantum many-body Schr\"odinger equation is a fundamental and challenging problem in the fields of quantum physics, quantum chemistry, and material sciences. One of the common computational approaches to this problem is Quantum Variational Monte Carlo (QVMC), in which ground-state solutions are obtained by minimizing the energy of the system within a restricted family of parameterized wave functions. Deep learning methods partially address the limitations of traditional QVMC by representing a rich family of wave functions in terms of neural networks. However, the optimization objective in QVMC remains notoriously hard to minimize and requires second-order optimization methods such as natural gradient. In this paper, we first reformulate energy functional minimization in the space of Born distributions corresponding to particle-permutation (anti-)symmetric wave functions, rather than the space of wave functions. We then interpret QVMC as the Fisher--Rao gradient flow in this
&lt;/p&gt;</description></item><item><title>AnyStar&#26159;&#19968;&#31181;&#22522;&#20110;&#22495;&#38543;&#26426;&#21270;&#30340;&#36890;&#29992;&#26143;&#20984;3D&#23454;&#20363;&#20998;&#21106;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#19981;&#38656;&#35201;&#26469;&#33258;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#38598;&#30340;&#27880;&#37322;&#22270;&#20687;&#21363;&#21487;&#20934;&#30830;&#20998;&#21106;&#26143;&#20984;&#24418;&#29366;&#12290;</title><link>http://arxiv.org/abs/2307.07044</link><description>&lt;p&gt;
AnyStar: &#22522;&#20110;&#22495;&#38543;&#26426;&#21270;&#30340;&#36890;&#29992;&#26143;&#20984;3D&#23454;&#20363;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
AnyStar: Domain randomized universal star-convex 3D instance segmentation. (arXiv:2307.07044v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07044
&lt;/p&gt;
&lt;p&gt;
AnyStar&#26159;&#19968;&#31181;&#22522;&#20110;&#22495;&#38543;&#26426;&#21270;&#30340;&#36890;&#29992;&#26143;&#20984;3D&#23454;&#20363;&#20998;&#21106;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#19981;&#38656;&#35201;&#26469;&#33258;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#38598;&#30340;&#27880;&#37322;&#22270;&#20687;&#21363;&#21487;&#20934;&#30830;&#20998;&#21106;&#26143;&#20984;&#24418;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26143;&#20984;&#24418;&#29366;&#22312;&#29983;&#29289;&#26174;&#24494;&#38236;&#21644;&#25918;&#23556;&#23398;&#20013;&#24191;&#27867;&#23384;&#22312;&#65292;&#22914;&#32454;&#32990;&#26680;&#12289;&#32467;&#33410;&#12289;&#36716;&#31227;&#31561;&#12290;&#38024;&#23545;&#36825;&#20123;&#32467;&#26500;&#30340;&#29616;&#26377;&#23454;&#20363;&#20998;&#21106;&#32593;&#32476;&#38656;&#35201;&#23545;&#27599;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#23494;&#38598;&#26631;&#27880;&#65292;&#36825;&#38656;&#35201;&#22823;&#37327;&#19988;&#24120;&#24120;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#25163;&#24037;&#27880;&#37322;&#24037;&#20316;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#23545;&#27604;&#24230;&#12289;&#24418;&#29366;&#12289;&#26041;&#21521;&#12289;&#20998;&#36776;&#29575;&#21644;&#23494;&#24230;&#30340;&#21464;&#21270;&#65292;&#24403;&#38754;&#23545;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#25104;&#20687;&#27169;&#24335;&#26102;&#65292;&#20173;&#38656;&#35201;&#36827;&#34892;&#22823;&#37327;&#30340;&#37325;&#24037;&#31243;&#25110;&#24494;&#35843;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;AnyStar&#65292;&#36825;&#26159;&#19968;&#20010;&#22495;&#38543;&#26426;&#21270;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#27169;&#25311;&#20102;&#20855;&#26377;&#38543;&#26426;&#22806;&#35266;&#12289;&#29615;&#22659;&#21644;&#25104;&#20687;&#29289;&#29702;&#29305;&#24615;&#30340;&#20223;&#30495;&#35757;&#32451;&#25968;&#25454;&#65292;&#29992;&#20110;&#35757;&#32451;&#36890;&#29992;&#30340;&#26143;&#20984;&#24418;&#29366;&#23454;&#20363;&#20998;&#21106;&#32593;&#32476;&#12290;&#22240;&#27492;&#65292;&#20351;&#29992;&#25105;&#20204;&#30340;&#29983;&#25104;&#27169;&#22411;&#35757;&#32451;&#30340;&#32593;&#32476;&#19981;&#38656;&#35201;&#26469;&#33258;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#38598;&#30340;&#27880;&#37322;&#22270;&#20687;&#12290;&#22312;&#25105;&#20204;&#21512;&#25104;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#21333;&#20010;&#32593;&#32476;&#20934;&#30830;&#22320;&#23545;C. elegans&#21644;P. dumerilii&#26680;&#36827;&#34892;3D&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;
Star-convex shapes arise across bio-microscopy and radiology in the form of nuclei, nodules, metastases, and other units. Existing instance segmentation networks for such structures train on densely labeled instances for each dataset, which requires substantial and often impractical manual annotation effort. Further, significant reengineering or finetuning is needed when presented with new datasets and imaging modalities due to changes in contrast, shape, orientation, resolution, and density. We present AnyStar, a domain-randomized generative model that simulates synthetic training data of blob-like objects with randomized appearance, environments, and imaging physics to train general-purpose star-convex instance segmentation networks. As a result, networks trained using our generative model do not require annotated images from unseen datasets. A single network trained on our synthesized data accurately 3D segments C. elegans and P. dumerilii nuclei in fluorescence microscopy, mouse co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#21152;&#36895;&#26799;&#24230;&#26041;&#27861;&#22312;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#19978;&#30340;&#34892;&#20026;&#65292;&#21253;&#25324;&#36867;&#31163;&#38797;&#28857;&#21644;&#25910;&#25947;&#21040;&#23616;&#37096;&#26497;&#23567;&#20540;&#28857;&#30340;&#20998;&#26512;&#12290;&#30740;&#31350;&#22312;&#28176;&#36827;&#21644;&#38750;&#28176;&#36827;&#24773;&#20917;&#19979;&#65292;&#25552;&#20986;&#20102;&#19968;&#31867;&#26032;&#30340;Nesterov&#31867;&#22411;&#30340;&#21152;&#36895;&#26041;&#27861;&#65292;&#24182;&#22238;&#31572;&#20102;Nesterov&#21152;&#36895;&#26799;&#24230;&#26041;&#27861;&#26159;&#21542;&#36991;&#20813;&#20102;&#20005;&#26684;&#38797;&#28857;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.07030</link><description>&lt;p&gt;
&#21152;&#36895;&#26799;&#24230;&#26041;&#27861;&#29992;&#20110;&#38750;&#20984;&#20248;&#21270;&#65306;&#36867;&#36920;&#36712;&#36857;&#21644;&#25910;&#25947;&#21040;&#23616;&#37096;&#26497;&#23567;&#20540;&#28857;
&lt;/p&gt;
&lt;p&gt;
Accelerated gradient methods for nonconvex optimization: Escape trajectories from strict saddle points and convergence to local minima. (arXiv:2307.07030v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07030
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#21152;&#36895;&#26799;&#24230;&#26041;&#27861;&#22312;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#19978;&#30340;&#34892;&#20026;&#65292;&#21253;&#25324;&#36867;&#31163;&#38797;&#28857;&#21644;&#25910;&#25947;&#21040;&#23616;&#37096;&#26497;&#23567;&#20540;&#28857;&#30340;&#20998;&#26512;&#12290;&#30740;&#31350;&#22312;&#28176;&#36827;&#21644;&#38750;&#28176;&#36827;&#24773;&#20917;&#19979;&#65292;&#25552;&#20986;&#20102;&#19968;&#31867;&#26032;&#30340;Nesterov&#31867;&#22411;&#30340;&#21152;&#36895;&#26041;&#27861;&#65292;&#24182;&#22238;&#31572;&#20102;Nesterov&#21152;&#36895;&#26799;&#24230;&#26041;&#27861;&#26159;&#21542;&#36991;&#20813;&#20102;&#20005;&#26684;&#38797;&#28857;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#24191;&#20041;&#30340;&#21152;&#36895;&#26799;&#24230;&#26041;&#27861;&#22312;&#20809;&#28369;&#38750;&#20984;&#20989;&#25968;&#19978;&#30340;&#34892;&#20026;&#12290;&#36890;&#36807;&#23545;Polyak&#30340;&#37325;&#29699;&#26041;&#27861;&#21644;Nesterov&#21152;&#36895;&#26799;&#24230;&#26041;&#27861;&#36827;&#34892;&#25913;&#36827;&#65292;&#20197;&#23454;&#29616;&#23545;&#38750;&#20984;&#20989;&#25968;&#23616;&#37096;&#26497;&#23567;&#20540;&#30340;&#25910;&#25947;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31867;Nesterov&#31867;&#22411;&#30340;&#21152;&#36895;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#28176;&#36827;&#20998;&#26512;&#21644;&#38750;&#28176;&#36827;&#20998;&#26512;&#23545;&#36825;&#20123;&#26041;&#27861;&#36827;&#34892;&#20102;&#20005;&#26684;&#30740;&#31350;&#65292;&#21253;&#25324;&#36867;&#31163;&#38797;&#28857;&#21644;&#25910;&#25947;&#21040;&#23616;&#37096;&#26497;&#23567;&#20540;&#28857;&#12290;&#22312;&#28176;&#36827;&#24773;&#20917;&#19979;&#65292;&#26412;&#25991;&#22238;&#31572;&#20102;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#65292;&#21363;&#24102;&#26377;&#21487;&#21464;&#21160;&#37327;&#21442;&#25968;&#30340;Nesterov&#21152;&#36895;&#26799;&#24230;&#26041;&#27861;&#65288;NAG&#65289;&#26159;&#21542;&#20960;&#20046;&#24517;&#23450;&#36991;&#20813;&#20102;&#20005;&#26684;&#38797;&#28857;&#12290;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#20004;&#31181;&#28176;&#36827;&#25910;&#25947;&#21644;&#21457;&#25955;&#30340;&#24230;&#37327;&#26041;&#24335;&#65292;&#24182;&#23545;&#20960;&#31181;&#24120;&#29992;&#30340;&#26631;&#20934;&#21152;&#36895;&#26041;&#27861;&#65288;&#22914;NAG&#21644;Ne&#65289;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper considers the problem of understanding the behavior of a general class of accelerated gradient methods on smooth nonconvex functions. Motivated by some recent works that have proposed effective algorithms, based on Polyak's heavy ball method and the Nesterov accelerated gradient method, to achieve convergence to a local minimum of nonconvex functions, this work proposes a broad class of Nesterov-type accelerated methods and puts forth a rigorous study of these methods encompassing the escape from saddle-points and convergence to local minima through a both asymptotic and a non-asymptotic analysis. In the asymptotic regime, this paper answers an open question of whether Nesterov's accelerated gradient method (NAG) with variable momentum parameter avoids strict saddle points almost surely. This work also develops two metrics of asymptotic rate of convergence and divergence, and evaluates these two metrics for several popular standard accelerated methods such as the NAG, and Ne
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#20998;&#35299;&#30340;&#21160;&#20316;&#31354;&#38388;&#26469;&#20943;&#36731;&#28041;&#21450;&#22823;&#22411;&#32452;&#21512;&#21160;&#20316;&#31354;&#38388;&#38382;&#39064;&#30340;&#38750;&#31574;&#30053;&#35780;&#20272;&#65288;OPE&#65289;&#30340;&#39640;&#20559;&#24046;&#21644;&#39640;&#26041;&#24046;&#38382;&#39064;&#12290;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20998;&#35299;&#21160;&#20316;&#31354;&#38388;&#30340;&#26032;&#22411;&#8220;&#20998;&#35299;&#8221;&#37325;&#35201;&#24615;&#25277;&#26679;&#65288;IS&#65289;&#20272;&#35745;&#22120;&#31995;&#21015;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20998;&#35299;IS&#20272;&#35745;&#22120;&#20855;&#26377;&#27604;&#38750;&#20998;&#35299;&#29256;&#26412;&#26356;&#23567;&#30340;&#26041;&#24046;&#65292;&#21516;&#26102;&#20855;&#26377;&#38646;&#20559;&#24046;&#30340;&#24615;&#36136;&#65292;&#24182;&#36890;&#36807;&#27169;&#25311;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#29702;&#35770;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.07014</link><description>&lt;p&gt;
&#21033;&#29992;&#20998;&#35299;&#30340;&#21160;&#20316;&#31354;&#38388;&#36827;&#34892;&#38750;&#31574;&#30053;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Leveraging Factored Action Spaces for Off-Policy Evaluation. (arXiv:2307.07014v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07014
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#20998;&#35299;&#30340;&#21160;&#20316;&#31354;&#38388;&#26469;&#20943;&#36731;&#28041;&#21450;&#22823;&#22411;&#32452;&#21512;&#21160;&#20316;&#31354;&#38388;&#38382;&#39064;&#30340;&#38750;&#31574;&#30053;&#35780;&#20272;&#65288;OPE&#65289;&#30340;&#39640;&#20559;&#24046;&#21644;&#39640;&#26041;&#24046;&#38382;&#39064;&#12290;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20998;&#35299;&#21160;&#20316;&#31354;&#38388;&#30340;&#26032;&#22411;&#8220;&#20998;&#35299;&#8221;&#37325;&#35201;&#24615;&#25277;&#26679;&#65288;IS&#65289;&#20272;&#35745;&#22120;&#31995;&#21015;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20998;&#35299;IS&#20272;&#35745;&#22120;&#20855;&#26377;&#27604;&#38750;&#20998;&#35299;&#29256;&#26412;&#26356;&#23567;&#30340;&#26041;&#24046;&#65292;&#21516;&#26102;&#20855;&#26377;&#38646;&#20559;&#24046;&#30340;&#24615;&#36136;&#65292;&#24182;&#36890;&#36807;&#27169;&#25311;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#29702;&#35770;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#31574;&#30053;&#35780;&#20272;&#65288;OPE&#65289;&#26088;&#22312;&#20272;&#35745;&#26681;&#25454;&#25191;&#34892;&#24207;&#21015;&#25910;&#38598;&#30340;&#25968;&#25454;&#65292;&#36981;&#24490;&#21453;&#20107;&#23454;&#30340;&#19968;&#31995;&#21015;&#21160;&#20316;&#30340;&#25928;&#30410;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;OPE&#20272;&#35745;&#22120;&#22312;&#28041;&#21450;&#22823;&#22411;&#32452;&#21512;&#21160;&#20316;&#31354;&#38388;&#30340;&#38382;&#39064;&#20013;&#32463;&#24120;&#34920;&#29616;&#20986;&#39640;&#20559;&#24046;&#21644;&#39640;&#26041;&#24046;&#12290;&#25105;&#20204;&#30740;&#31350;&#22914;&#20309;&#21033;&#29992;&#20998;&#35299;&#30340;&#21160;&#20316;&#31354;&#38388;&#26469;&#20943;&#36731;&#36825;&#20010;&#38382;&#39064;&#65292;&#21363;&#23558;&#27599;&#20010;&#21160;&#20316;&#34920;&#31034;&#20026;&#26469;&#33258;&#36739;&#23567;&#21160;&#20316;&#31354;&#38388;&#30340;&#29420;&#31435;&#23376;&#21160;&#20316;&#30340;&#32452;&#21512;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#21161;&#20110;&#23545;&#21160;&#20316;&#22312;&#20854;&#25928;&#26524;&#19978;&#30340;&#24046;&#24322;&#36827;&#34892;&#26356;&#31934;&#32454;&#30340;&#20998;&#26512;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#35299;&#21160;&#20316;&#31354;&#38388;&#30340;&#26032;&#22411;&#8220;&#20998;&#35299;&#8221;&#37325;&#35201;&#24615;&#25277;&#26679;&#65288;IS&#65289;&#20272;&#35745;&#22120;&#31995;&#21015;&#12290;&#22312;&#23545;&#24213;&#23618;&#38382;&#39064;&#32467;&#26500;&#36827;&#34892;&#19968;&#23450;&#30340;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20998;&#35299;IS&#20272;&#35745;&#22120;&#30340;&#26041;&#24046;&#27604;&#20854;&#21407;&#22987;&#38750;&#20998;&#35299;&#29256;&#26412;&#23567;&#65292;&#21516;&#26102;&#20445;&#25345;&#38646;&#20559;&#24046;&#30340;&#24615;&#36136;&#12290;&#36890;&#36807;&#27169;&#25311;&#65292;&#25105;&#20204;&#32463;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#65292;&#26816;&#39564;&#20102;&#21508;&#31181;&#20551;&#35774;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Off-policy evaluation (OPE) aims to estimate the benefit of following a counterfactual sequence of actions, given data collected from executed sequences. However, existing OPE estimators often exhibit high bias and high variance in problems involving large, combinatorial action spaces. We investigate how to mitigate this issue using factored action spaces i.e. expressing each action as a combination of independent sub-actions from smaller action spaces. This approach facilitates a finer-grained analysis of how actions differ in their effects. In this work, we propose a new family of "decomposed" importance sampling (IS) estimators based on factored action spaces. Given certain assumptions on the underlying problem structure, we prove that the decomposed IS estimators have less variance than their original non-decomposed versions, while preserving the property of zero bias. Through simulations, we empirically verify our theoretical results, probing the validity of various assumptions. P
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#30789;&#24494;&#29615;&#20849;&#25391;&#22120;&#36827;&#34892;&#30340;&#22522;&#20110;&#33258;&#30001;&#36733;&#27969;&#23376;&#38750;&#32447;&#24615;&#30340;&#33988;&#27700;&#23481;&#22120;&#35745;&#31639;&#65292;&#36890;&#36807;&#37327;&#21270;&#28909;&#20809;&#25928;&#24212;&#21644;&#33258;&#30001;&#36733;&#27969;&#23376;&#25928;&#24212;&#30340;&#24433;&#21709;&#65292;&#24182;&#30830;&#23450;&#20102;&#20351;NARMA-10&#20219;&#21153;&#20013;NMSE&#23567;&#20110;0.05&#30340;&#27893;&#28006;&#21151;&#29575;&#21644;&#39057;&#29575;&#22833;&#35856;&#33539;&#22260;&#12290;</title><link>http://arxiv.org/abs/2307.07011</link><description>&lt;p&gt;
&#30789;&#24494;&#29615;&#20849;&#25391;&#22120;&#23545;&#22522;&#20110;&#33258;&#30001;&#36733;&#27969;&#23376;&#38750;&#32447;&#24615;&#30340;&#33988;&#27700;&#23481;&#22120;&#35745;&#31639;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Impact of Free-carrier Nonlinearities on Silicon Microring-based Reservoir Computing. (arXiv:2307.07011v1 [cs.ET])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07011
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#30789;&#24494;&#29615;&#20849;&#25391;&#22120;&#36827;&#34892;&#30340;&#22522;&#20110;&#33258;&#30001;&#36733;&#27969;&#23376;&#38750;&#32447;&#24615;&#30340;&#33988;&#27700;&#23481;&#22120;&#35745;&#31639;&#65292;&#36890;&#36807;&#37327;&#21270;&#28909;&#20809;&#25928;&#24212;&#21644;&#33258;&#30001;&#36733;&#27969;&#23376;&#25928;&#24212;&#30340;&#24433;&#21709;&#65292;&#24182;&#30830;&#23450;&#20102;&#20351;NARMA-10&#20219;&#21153;&#20013;NMSE&#23567;&#20110;0.05&#30340;&#27893;&#28006;&#21151;&#29575;&#21644;&#39057;&#29575;&#22833;&#35856;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#37327;&#21270;&#20102;&#28909;&#20809;&#25928;&#24212;&#21644;&#33258;&#30001;&#36733;&#27969;&#23376;&#25928;&#24212;&#23545;&#20351;&#29992;&#30789;&#24494;&#29615;&#35856;&#25391;&#22120;&#30340;&#26102;&#24310;&#33988;&#27700;&#23481;&#22120;&#35745;&#31639;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#27893;&#28006;&#21151;&#29575;&#21644;&#39057;&#29575;&#22833;&#35856;&#33539;&#22260;&#65292;&#20351;&#24471;NARMA-10&#20219;&#21153;&#20013;NMSE&#23567;&#20110;0.05&#65292;&#36825;&#21462;&#20915;&#20110;&#20004;&#31181;&#32771;&#34385;&#25928;&#24212;&#30340;&#26102;&#38388;&#24120;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
We quantify the impact of thermo-optic and free-carrier effects on time-delay reservoir computing using a silicon microring resonator. We identify pump power and frequency detuning ranges with NMSE less than 0.05 for the NARMA-10 task depending on the time constants of the two considered effects.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35752;&#35770;&#24182;&#35780;&#20272;&#20102;&#22312;&#25968;&#23398;&#23545;&#35937;&#20013;&#30340;&#25968;&#25454;&#24179;&#34913;&#21644;&#25968;&#25454;&#22686;&#24378;&#30340;&#24605;&#36335;&#65292;&#36890;&#36807;&#20132;&#25442;&#21464;&#37327;&#21517;&#31216;&#29983;&#25104;&#26032;&#30340;&#38382;&#39064;&#23454;&#20363;&#65292;&#25552;&#39640;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#35748;&#20026;&#25968;&#25454;&#38598;&#24179;&#34913;&#21644;&#22686;&#21152;&#25968;&#25454;&#38598;&#22823;&#23567;&#23545;&#20110;&#25928;&#26524;&#30340;&#25552;&#21319;&#37117;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2307.06984</link><description>&lt;p&gt;
&#25968;&#23398;&#23545;&#35937;&#30340;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Data Augmentation for Mathematical Objects. (arXiv:2307.06984v1 [cs.SC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06984
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35752;&#35770;&#24182;&#35780;&#20272;&#20102;&#22312;&#25968;&#23398;&#23545;&#35937;&#20013;&#30340;&#25968;&#25454;&#24179;&#34913;&#21644;&#25968;&#25454;&#22686;&#24378;&#30340;&#24605;&#36335;&#65292;&#36890;&#36807;&#20132;&#25442;&#21464;&#37327;&#21517;&#31216;&#29983;&#25104;&#26032;&#30340;&#38382;&#39064;&#23454;&#20363;&#65292;&#25552;&#39640;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#35748;&#20026;&#25968;&#25454;&#38598;&#24179;&#34913;&#21644;&#22686;&#21152;&#25968;&#25454;&#38598;&#22823;&#23567;&#23545;&#20110;&#25928;&#26524;&#30340;&#25552;&#21319;&#37117;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#21644;&#35780;&#20272;&#20102;&#22312;&#25968;&#23398;&#23545;&#35937;&#30340;&#32972;&#26223;&#19979;&#25968;&#25454;&#24179;&#34913;&#21644;&#25968;&#25454;&#22686;&#24378;&#30340;&#24605;&#24819;&#65306;&#36825;&#23545;&#20110;&#31526;&#21495;&#35745;&#31639;&#21644;&#21487;&#28385;&#36275;&#24615;&#26816;&#26597;&#31038;&#21306;&#26469;&#35828;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#20027;&#39064;&#65292;&#24403;&#20182;&#20204;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#26469;&#20248;&#21270;&#20182;&#20204;&#30340;&#24037;&#20855;&#26102;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#38750;&#32447;&#24615;&#22810;&#39033;&#24335;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#36890;&#36807;&#20132;&#25442;&#24050;&#26631;&#35760;&#38382;&#39064;&#20013;&#30340;&#21464;&#37327;&#21517;&#31216;&#26469;&#35299;&#20915;&#22278;&#26609;&#20195;&#25968;&#20998;&#35299;&#30340;&#21464;&#37327;&#25490;&#24207;&#38382;&#39064;&#12290;&#36890;&#36807;&#29983;&#25104;&#19981;&#38656;&#35201;&#36827;&#19968;&#27493;&#26631;&#35760;&#30340;&#26032;&#38382;&#39064;&#23454;&#20363;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#22686;&#24378;&#24179;&#22343;&#25552;&#39640;&#20102;63&#65285;&#30340;ML&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#25968;&#25454;&#38598;&#24179;&#34913;&#21644;&#36827;&#19968;&#27493;&#22686;&#21152;&#25968;&#25454;&#38598;&#22823;&#23567;&#23545;&#20110;&#27492;&#25913;&#36827;&#30340;&#24433;&#21709;&#65292;&#24182;&#24471;&#20986;&#32467;&#35770;&#65292;&#20004;&#32773;&#37117;&#26377;&#38750;&#24120;&#26174;&#33879;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#26368;&#21518;&#24605;&#32771;&#20102;&#36825;&#20010;&#24819;&#27861;&#22914;&#20309;&#21487;&#20197;&#25193;&#23637;&#21040;&#20854;&#20182;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper discusses and evaluates ideas of data balancing and data augmentation in the context of mathematical objects: an important topic for both the symbolic computation and satisfiability checking communities, when they are making use of machine learning techniques to optimise their tools. We consider a dataset of non-linear polynomial problems and the problem of selecting a variable ordering for cylindrical algebraic decomposition to tackle these with. By swapping the variable names in already labelled problems, we generate new problem instances that do not require any further labelling when viewing the selection as a classification problem. We find this augmentation increases the accuracy of ML models by 63% on average. We study what part of this improvement is due to the balancing of the dataset and what is achieved thanks to further increasing the size of the dataset, concluding that both have a very significant effect. We finish the paper by reflecting on how this idea could 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20915;&#31574;&#26694;&#26550;&#65292;&#29992;&#20110;&#36873;&#25321;&#20154;&#32676;&#32467;&#26500;&#20581;&#24247;&#30417;&#27979;&#20013;&#30340;&#20449;&#24687;&#20256;&#36755;&#31574;&#30053;&#65292;&#36890;&#36807;&#36991;&#20813;&#36127;&#38754;&#20256;&#36755;&#21644;&#20248;&#21270;&#20449;&#24687;&#20256;&#36755;&#31574;&#30053;&#65292;&#21487;&#20197;&#38477;&#20302;&#25104;&#26412;&#21644;&#25552;&#39640;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.06978</link><description>&lt;p&gt;
&#29992;&#20110;&#36873;&#25321;&#20154;&#32676;&#32467;&#26500;&#20581;&#24247;&#30417;&#27979;&#20013;&#20449;&#24687;&#20256;&#36755;&#31574;&#30053;&#30340;&#20915;&#31574;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A decision framework for selecting information-transfer strategies in population-based SHM. (arXiv:2307.06978v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06978
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20915;&#31574;&#26694;&#26550;&#65292;&#29992;&#20110;&#36873;&#25321;&#20154;&#32676;&#32467;&#26500;&#20581;&#24247;&#30417;&#27979;&#20013;&#30340;&#20449;&#24687;&#20256;&#36755;&#31574;&#30053;&#65292;&#36890;&#36807;&#36991;&#20813;&#36127;&#38754;&#20256;&#36755;&#21644;&#20248;&#21270;&#20449;&#24687;&#20256;&#36755;&#31574;&#30053;&#65292;&#21487;&#20197;&#38477;&#20302;&#25104;&#26412;&#21644;&#25552;&#39640;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#20581;&#24247;&#30417;&#27979;&#31995;&#32479;&#30340;&#24320;&#21457;&#21644;&#23454;&#26045;&#20026;&#32467;&#26500;&#30340;&#36816;&#32500;&#25552;&#20379;&#20102;&#37325;&#35201;&#25903;&#25345;&#65292;&#28982;&#32780;&#65292;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#30340;&#26377;&#38480;&#24615;&#21046;&#32422;&#20102;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#25152;&#20381;&#36182;&#30340;&#32479;&#35745;&#27169;&#22411;&#30340;&#21457;&#23637;&#12290;&#20154;&#32676;&#32467;&#26500;&#20581;&#24247;&#30417;&#27979;&#36890;&#36807;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#65292;&#22312;&#20010;&#20307;&#32467;&#26500;&#20043;&#38388;&#20849;&#20139;&#20449;&#24687;&#20197;&#20943;&#36731;&#25968;&#25454;&#31232;&#32570;&#24615;&#24102;&#26469;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#8220;&#20449;&#24687;&#20256;&#36755;&#26399;&#26395;&#20215;&#20540;&#8221;&#30340;&#20915;&#31574;&#26694;&#26550;&#65292;&#29992;&#20110;&#36873;&#25321;&#20256;&#36755;&#31574;&#30053;&#65292;&#24182;&#36991;&#20813;&#36127;&#38754;&#20256;&#36755;&#12290;&#36890;&#36807;&#36991;&#20813;&#36127;&#38754;&#20256;&#36755;&#65292;&#24182;&#20351;&#29992;&#20256;&#36755;&#20915;&#31574;&#26694;&#26550;&#20248;&#21270;&#20449;&#24687;&#20256;&#36755;&#31574;&#30053;&#65292;&#21487;&#20197;&#20943;&#23569;&#36816;&#33829;&#21644;&#32500;&#25252;&#32467;&#26500;&#25152;&#38656;&#30340;&#25104;&#26412;&#65292;&#24182;&#25552;&#39640;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision-support for the operation and maintenance of structures provides significant motivation for the development and implementation of structural health monitoring (SHM) systems. Unfortunately, the limited availability of labelled training data hinders the development of the statistical models on which these decision-support systems rely. Population-based SHM seeks to mitigate the impact of data scarcity by using transfer learning techniques to share information between individual structures within a population. The current paper proposes a decision framework for selecting transfer strategies based upon a novel concept -- the expected value of information transfer -such that negative transfer is avoided. By avoiding negative transfer, and by optimising information transfer strategies using the transfer-decision framework, one can reduce the costs associated with operating and maintaining structures, and improve safety.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#31526;&#21495;&#30340;&#24378;&#21270;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#24037;&#19994;4.0&#20013;&#23454;&#26102;&#39044;&#27979;&#24322;&#24120;&#12290;&#35813;&#26041;&#27861;&#38598;&#25104;&#20102;&#24037;&#19994;&#26412;&#20307;&#35770;&#65292;&#20026;&#26234;&#33021;&#21046;&#36896;&#25552;&#20379;&#20102;&#24418;&#24335;&#21270;&#30693;&#35782;&#65292;&#24182;&#19988;&#36890;&#36807;&#38543;&#26426;&#20613;&#37324;&#21494;&#29305;&#24449;&#25552;&#21462;&#25193;&#25955;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#23884;&#20837;&#24335;&#31995;&#32479;&#20013;&#30452;&#25509;&#38598;&#25104;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#20197;&#21069;&#20174;&#26410;&#34987;&#25506;&#32034;&#36807;&#12290;</title><link>http://arxiv.org/abs/2307.06975</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#31526;&#21495;&#30340;&#24378;&#21270;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#22312;&#24037;&#19994;4.0&#20013;&#30340;&#23454;&#26102;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Neuro-symbolic Empowered Denoising Diffusion Probabilistic Models for Real-time Anomaly Detection in Industry 4.0. (arXiv:2307.06975v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06975
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#31526;&#21495;&#30340;&#24378;&#21270;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#24037;&#19994;4.0&#20013;&#23454;&#26102;&#39044;&#27979;&#24322;&#24120;&#12290;&#35813;&#26041;&#27861;&#38598;&#25104;&#20102;&#24037;&#19994;&#26412;&#20307;&#35770;&#65292;&#20026;&#26234;&#33021;&#21046;&#36896;&#25552;&#20379;&#20102;&#24418;&#24335;&#21270;&#30693;&#35782;&#65292;&#24182;&#19988;&#36890;&#36807;&#38543;&#26426;&#20613;&#37324;&#21494;&#29305;&#24449;&#25552;&#21462;&#25193;&#25955;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#23884;&#20837;&#24335;&#31995;&#32479;&#20013;&#30452;&#25509;&#38598;&#25104;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#20197;&#21069;&#20174;&#26410;&#34987;&#25506;&#32034;&#36807;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#19994;4.0&#23558;&#29289;&#32852;&#32593;&#12289;&#22823;&#25968;&#25454;&#21644;&#20154;&#24037;&#26234;&#33021;&#31561;&#25968;&#23383;&#25216;&#26415;&#25972;&#21512;&#21040;&#21046;&#36896;&#21644;&#24037;&#19994;&#27969;&#31243;&#20013;&#65292;&#20197;&#25552;&#39640;&#25928;&#29575;&#21644;&#29983;&#20135;&#21147;&#12290;&#38543;&#30528;&#36825;&#20123;&#25216;&#26415;&#30340;&#20114;&#32852;&#20114;&#36890;&#21644;&#30456;&#20114;&#20381;&#36182;&#31243;&#24230;&#36234;&#26469;&#36234;&#39640;&#65292;&#24037;&#19994;4.0&#31995;&#32479;&#21464;&#24471;&#26356;&#21152;&#22797;&#26434;&#65292;&#36825;&#23601;&#24102;&#26469;&#20102;&#35782;&#21035;&#21644;&#20572;&#27490;&#21487;&#33021;&#22312;&#21046;&#36896;&#36807;&#31243;&#20013;&#24341;&#36215;&#24178;&#25200;&#30340;&#24322;&#24120;&#30340;&#22256;&#38590;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#24037;&#19994;4.0&#27969;&#31243;&#20013;&#30340;&#23454;&#26102;&#24322;&#24120;&#39044;&#27979;&#12290;&#36890;&#36807;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#65292;&#25105;&#20204;&#23558;&#24037;&#19994;&#26412;&#20307;&#35770;&#38598;&#25104;&#21040;&#27169;&#22411;&#20013;&#65292;&#20174;&#32780;&#22312;&#26234;&#33021;&#21046;&#36896;&#20013;&#28155;&#21152;&#20102;&#24418;&#24335;&#21270;&#30693;&#35782;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#36890;&#36807;&#38543;&#26426;&#20613;&#37324;&#21494;&#29305;&#24449;&#25552;&#21462;&#25193;&#25955;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20197;&#20415;&#23558;&#20854;&#37096;&#32626;&#21040;&#23884;&#20837;&#24335;&#31995;&#32479;&#20013;&#65292;&#30452;&#25509;&#38598;&#25104;&#21040;&#21046;&#36896;&#36807;&#31243;&#20013;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#20197;&#21069;&#20174;&#26410;&#34987;&#25506;&#32034;&#36807;&#12290;
&lt;/p&gt;
&lt;p&gt;
Industry 4.0 involves the integration of digital technologies, such as IoT, Big Data, and AI, into manufacturing and industrial processes to increase efficiency and productivity. As these technologies become more interconnected and interdependent, Industry 4.0 systems become more complex, which brings the difficulty of identifying and stopping anomalies that may cause disturbances in the manufacturing process. This paper aims to propose a diffusion-based model for real-time anomaly prediction in Industry 4.0 processes. Using a neuro-symbolic approach, we integrate industrial ontologies in the model, thereby adding formal knowledge on smart manufacturing. Finally, we propose a simple yet effective way of distilling diffusion models through Random Fourier Features for deployment on an embedded system for direct integration into the manufacturing process. To the best of our knowledge, this approach has never been explored before.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#19968;&#20803;&#20851;&#31995;&#30340;&#25968;&#25454;&#27169;&#22411;&#30340;&#30701;&#24067;&#23572;&#20844;&#24335;&#35299;&#37322;&#30340;&#21487;&#34892;&#24615;&#65292;&#25552;&#20986;&#20102;&#23545;&#26399;&#26395;&#38169;&#35823;&#30340;&#23450;&#37327;&#30028;&#38480;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#19977;&#20010;&#20855;&#20307;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#36890;&#36807;&#38480;&#21046;&#20844;&#24335;&#38271;&#24230;&#65292;&#21487;&#20197;&#33719;&#24471;&#36991;&#20813;&#36807;&#25311;&#21512;&#19988;&#20934;&#30830;&#19988;&#26131;&#20110;&#29702;&#35299;&#30340;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2307.06971</link><description>&lt;p&gt;
&#23454;&#36341;&#20013;&#30340;&#30701;&#24067;&#23572;&#20844;&#24335;&#20316;&#20026;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Short Boolean Formulas as Explanations in Practice. (arXiv:2307.06971v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06971
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#19968;&#20803;&#20851;&#31995;&#30340;&#25968;&#25454;&#27169;&#22411;&#30340;&#30701;&#24067;&#23572;&#20844;&#24335;&#35299;&#37322;&#30340;&#21487;&#34892;&#24615;&#65292;&#25552;&#20986;&#20102;&#23545;&#26399;&#26395;&#38169;&#35823;&#30340;&#23450;&#37327;&#30028;&#38480;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#19977;&#20010;&#20855;&#20307;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#36890;&#36807;&#38480;&#21046;&#20844;&#24335;&#38271;&#24230;&#65292;&#21487;&#20197;&#33719;&#24471;&#36991;&#20813;&#36807;&#25311;&#21512;&#19988;&#20934;&#30830;&#19988;&#26131;&#20110;&#29702;&#35299;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#19968;&#20803;&#20851;&#31995;&#30340;&#25968;&#25454;&#27169;&#22411;&#20013;&#36890;&#36807;&#30701;&#24067;&#23572;&#20844;&#24335;&#36827;&#34892;&#35299;&#37322;&#30340;&#21487;&#34892;&#24615;&#12290;&#20316;&#20026;&#38271;&#24230;&#20026;k&#30340;&#35299;&#37322;&#65292;&#25105;&#20204;&#37319;&#29992;&#19968;&#20010;&#38271;&#24230;&#20026;k&#30340;&#24067;&#23572;&#20844;&#24335;&#65292;&#35813;&#20844;&#24335;&#22312;&#35299;&#37322;&#30446;&#26631;&#23646;&#24615;&#26041;&#38754;&#30340;&#38169;&#35823;&#26368;&#23567;&#21270;&#12290;&#25105;&#20204;&#39318;&#20808;&#20026;&#36825;&#31181;&#24773;&#20917;&#25552;&#20379;&#20102;&#26032;&#39062;&#30340;&#26399;&#26395;&#38169;&#35823;&#30340;&#23450;&#37327;&#30028;&#38480;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#19977;&#20010;&#20855;&#20307;&#30340;&#25968;&#25454;&#38598;&#26469;&#28436;&#31034;&#35813;&#35774;&#32622;&#22312;&#23454;&#36341;&#20013;&#30340;&#36816;&#20316;&#26041;&#24335;&#12290;&#22312;&#27599;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#20351;&#29992;Answer Set Programming&#20013;&#30340;&#32534;&#30721;&#35745;&#31639;&#19981;&#21516;&#38271;&#24230;&#30340;&#35299;&#37322;&#20844;&#24335;&#12290;&#25105;&#20204;&#24471;&#21040;&#30340;&#26368;&#20934;&#30830;&#30340;&#20844;&#24335;&#22312;&#30456;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#19982;&#20854;&#20182;&#26041;&#27861;&#31867;&#20284;&#30340;&#38169;&#35823;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36807;&#25311;&#21512;&#30340;&#21407;&#22240;&#65292;&#36825;&#20123;&#20844;&#24335;&#19981;&#19968;&#23450;&#26159;&#29702;&#24819;&#30340;&#35299;&#37322;&#65292;&#22240;&#27492;&#25105;&#20204;&#20351;&#29992;&#20132;&#21449;&#39564;&#35777;&#26469;&#30830;&#23450;&#21512;&#36866;&#30340;&#35299;&#37322;&#38271;&#24230;&#12290;&#36890;&#36807;&#38480;&#21046;&#20026;&#26356;&#30701;&#30340;&#20844;&#24335;&#65292;&#25105;&#20204;&#24471;&#21040;&#30340;&#35299;&#37322;&#19981;&#20165;&#36991;&#20813;&#20102;&#36807;&#25311;&#21512;&#65292;&#32780;&#19988;&#20381;&#28982;&#30456;&#24403;&#20934;&#30830;&#65292;&#24182;&#19988;&#37325;&#35201;&#30340;&#26159;&#65292;&#26131;&#20110;&#20154;&#31867;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate explainability via short Boolean formulas in the data model based on unary relations. As an explanation of length k, we take a Boolean formula of length k that minimizes the error with respect to the target attribute to be explained. We first provide novel quantitative bounds for the expected error in this scenario. We then also demonstrate how the setting works in practice by studying three concrete data sets. In each case, we calculate explanation formulas of different lengths using an encoding in Answer Set Programming. The most accurate formulas we obtain achieve errors similar to other methods on the same data sets. However, due to overfitting, these formulas are not necessarily ideal explanations, so we use cross validation to identify a suitable length for explanations. By limiting to shorter formulas, we obtain explanations that avoid overfitting but are still reasonably accurate and also, importantly, human interpretable.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20272;&#35745;&#20102;&#37319;&#29992;FDM&#24037;&#33402;&#21046;&#36896;&#30340;PLA&#35797;&#26679;&#30340;&#26497;&#38480;&#25239;&#25289;&#24378;&#24230;&#65288;UTS&#65289;&#65292;&#32467;&#26524;&#34920;&#26126;KNN&#31639;&#27861;&#20248;&#20110;&#20854;&#20182;&#31639;&#27861;&#65292;&#22312;&#21306;&#20998;&#19981;&#21516;UTS&#31867;&#21035;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.06970</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#27169;&#24335;&#35782;&#21035;&#31639;&#27861;&#29992;&#20110;&#20272;&#35745;&#29076;&#34701;&#27785;&#31215;&#27169;&#24335;&#32858;&#20083;&#37240;&#37240;&#65288;PLA&#65289;&#35797;&#26679;&#30340;&#26497;&#38480;&#25239;&#25289;&#24378;&#24230;
&lt;/p&gt;
&lt;p&gt;
Machine Learning-Assisted Pattern Recognition Algorithms for Estimating Ultimate Tensile Strength in Fused Deposition Modeled Polylactic Acid Specimens. (arXiv:2307.06970v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06970
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20272;&#35745;&#20102;&#37319;&#29992;FDM&#24037;&#33402;&#21046;&#36896;&#30340;PLA&#35797;&#26679;&#30340;&#26497;&#38480;&#25239;&#25289;&#24378;&#24230;&#65288;UTS&#65289;&#65292;&#32467;&#26524;&#34920;&#26126;KNN&#31639;&#27861;&#20248;&#20110;&#20854;&#20182;&#31639;&#27861;&#65292;&#22312;&#21306;&#20998;&#19981;&#21516;UTS&#31867;&#21035;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26469;&#20272;&#35745;&#37319;&#29992;&#29076;&#34701;&#27785;&#31215;&#27169;&#22411;&#65288;FDM&#65289;&#24037;&#33402;&#21046;&#36896;&#30340;&#32858;&#20083;&#37240;&#37240;&#65288;PLA&#65289;&#35797;&#26679;&#30340;&#26497;&#38480;&#25239;&#25289;&#24378;&#24230;&#65288;UTS&#65289;&#12290;&#24635;&#20849;&#21046;&#22791;&#20102;31&#20010;PLA&#35797;&#26679;&#65292;&#20854;&#20013;&#22635;&#20805;&#30334;&#20998;&#27604;&#12289;&#23618;&#39640;&#12289;&#25171;&#21360;&#36895;&#24230;&#21644;&#25380;&#20986;&#28201;&#24230;&#20316;&#20026;&#36755;&#20837;&#21442;&#25968;&#12290;&#20027;&#35201;&#30446;&#26631;&#26159;&#35780;&#20272;&#22235;&#31181;&#19981;&#21516;&#30417;&#30563;&#20998;&#31867;&#31639;&#27861;&#65292;&#20998;&#21035;&#26159;&#36923;&#36753;&#22238;&#24402;&#20998;&#31867;&#12289;&#26799;&#24230;&#25552;&#21319;&#20998;&#31867;&#12289;&#20915;&#31574;&#26641;&#21644;K&#36817;&#37051;&#65292;&#22312;&#39044;&#27979;&#35797;&#26679;UTS&#26041;&#38754;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#26524;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#20915;&#31574;&#26641;&#21644;K&#36817;&#37051;&#31639;&#27861;&#30340;F1&#20998;&#25968;&#22343;&#20026;0.71&#65292;&#20294;KNN&#31639;&#27861;&#30340;&#26354;&#32447;&#19979;&#38754;&#31215;&#65288;AUC&#65289;&#20998;&#25968;&#26356;&#39640;&#65292;&#36798;&#21040;0.79&#65292;&#20248;&#20110;&#20854;&#20182;&#31639;&#27861;&#12290;&#36825;&#34920;&#26126;KNN&#31639;&#27861;&#22312;&#21306;&#20998;&#20004;&#31867;&#26497;&#38480;&#25239;&#25289;&#24378;&#24230;&#26041;&#38754;&#20855;&#26377;&#36739;&#24378;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we investigate the application of supervised machine learning algorithms for estimating the Ultimate Tensile Strength (UTS) of Polylactic Acid (PLA) specimens fabricated using the Fused Deposition Modeling (FDM) process. A total of 31 PLA specimens were prepared, with Infill Percentage, Layer Height, Print Speed, and Extrusion Temperature serving as input parameters. The primary objective was to assess the accuracy and effectiveness of four distinct supervised classification algorithms, namely Logistic Classification, Gradient Boosting Classification, Decision Tree, and K-Nearest Neighbor, in predicting the UTS of the specimens. The results revealed that while the Decision Tree and K-Nearest Neighbor algorithms both achieved an F1 score of 0.71, the KNN algorithm exhibited a higher Area Under the Curve (AUC) score of 0.79, outperforming the other algorithms. This demonstrates the superior ability of the KNN algorithm in differentiating between the two classes of ultimate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#32447;&#24615;&#27169;&#24577;&#36830;&#25509;&#26041;&#27861;&#29992;&#20110;&#32852;&#37030;&#28145;&#24230;&#23398;&#20064;&#65292;&#36890;&#36807;&#35299;&#20915;&#27169;&#22411;&#28418;&#31227;&#21644;&#39640;&#25439;&#22833;&#38556;&#22721;&#30340;&#38382;&#39064;&#65292;&#33021;&#22815;&#26377;&#25928;&#25552;&#21319;&#20840;&#23616;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.06966</link><description>&lt;p&gt;
&#20998;&#23618;&#32447;&#24615;&#27169;&#24577;&#36830;&#25509;
&lt;/p&gt;
&lt;p&gt;
Layerwise Linear Mode Connectivity. (arXiv:2307.06966v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06966
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#32447;&#24615;&#27169;&#24577;&#36830;&#25509;&#26041;&#27861;&#29992;&#20110;&#32852;&#37030;&#28145;&#24230;&#23398;&#20064;&#65292;&#36890;&#36807;&#35299;&#20915;&#27169;&#22411;&#28418;&#31227;&#21644;&#39640;&#25439;&#22833;&#38556;&#22721;&#30340;&#38382;&#39064;&#65292;&#33021;&#22815;&#26377;&#25928;&#25552;&#21319;&#20840;&#23616;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32852;&#37030;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#22810;&#27425;&#23545;&#20998;&#31163;&#30340;&#26412;&#22320;&#27169;&#22411;&#36827;&#34892;&#32858;&#21512;&#65292;&#20197;&#33719;&#24471;&#26356;&#24378;&#22823;&#30340;&#20840;&#23616;&#27169;&#22411;&#65307;&#26368;&#24120;&#35265;&#30340;&#32858;&#21512;&#26041;&#27861;&#26159;&#21442;&#25968;&#30340;&#31616;&#21333;&#24179;&#22343;&#12290;&#29702;&#35299;&#22312;&#38750;&#20984;&#35774;&#32622;&#65288;&#22914;&#32852;&#37030;&#28145;&#24230;&#23398;&#20064;&#65289;&#20013;&#32858;&#21512;&#20309;&#26102;&#20197;&#21450;&#20026;&#20309;&#26377;&#25928;&#26159;&#19968;&#20010;&#23578;&#26410;&#35299;&#20915;&#30340;&#25361;&#25112;&#65292;&#36825;&#38459;&#30861;&#20102;&#33719;&#24471;&#39640;&#24615;&#33021;&#30340;&#20840;&#23616;&#27169;&#22411;&#12290;&#22312;i.i.d.&#25968;&#25454;&#38598;&#19978;&#65292;&#39057;&#32321;&#24179;&#22343;&#30340;&#32852;&#37030;&#28145;&#24230;&#23398;&#20064;&#26159;&#25104;&#21151;&#30340;&#12290;&#28982;&#32780;&#65292;&#24120;&#35265;&#30340;&#35266;&#28857;&#26159;&#22312;&#29420;&#31435;&#35757;&#32451;&#26399;&#38388;&#65292;&#27169;&#22411;&#20250;&#30456;&#20114;&#28418;&#31227;&#65292;&#22240;&#27492;&#22312;&#35768;&#22810;&#26412;&#22320;&#21442;&#25968;&#26356;&#26032;&#21518;&#24179;&#22343;&#21487;&#33021;&#19981;&#20877;&#36215;&#20316;&#29992;&#12290;&#36825;&#20010;&#38382;&#39064;&#21487;&#20197;&#20174;&#25439;&#22833;&#26354;&#38754;&#30340;&#35282;&#24230;&#26469;&#30475;&#65306;&#23545;&#20110;&#38750;&#20984;&#26354;&#38754;&#19978;&#30340;&#28857;&#65292;&#24179;&#22343;&#20540;&#21487;&#33021;&#21464;&#24471;&#20219;&#24847;&#31967;&#31957;&#12290;&#36890;&#24120;&#29992;&#20110;&#35299;&#37322;&#32852;&#37030;&#24179;&#22343;&#25104;&#21151;&#30340;&#23616;&#37096;&#20984;&#24615;&#20551;&#35774;&#19982;&#32463;&#39564;&#35777;&#25454;&#30456;&#30683;&#30462;&#65292;&#26174;&#31034;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#23384;&#22312;&#39640;&#25439;&#22833;&#38556;&#22721;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the federated setup one performs an aggregation of separate local models multiple times during training in order to obtain a stronger global model; most often aggregation is a simple averaging of the parameters. Understanding when and why averaging works in a non-convex setup, such as federated deep learning, is an open challenge that hinders obtaining highly performant global models. On i.i.d.~datasets federated deep learning with frequent averaging is successful. The common understanding, however, is that during the independent training models are drifting away from each other and thus averaging may not work anymore after many local parameter updates. The problem can be seen from the perspective of the loss surface: for points on a non-convex surface the average can become arbitrarily bad. The assumption of local convexity, often used to explain the success of federated averaging, contradicts to the empirical evidence showing that high loss barriers exist between models from the v
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#32479;&#19968;&#24403;&#20195;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#30340;&#25361;&#25112;&#65292;&#25351;&#20986;&#34429;&#28982;XAI&#26041;&#27861;&#20026;&#27169;&#22411;&#25552;&#20379;&#20102;&#26377;&#29992;&#30340;&#36755;&#20986;&#65292;&#20294;&#30740;&#31350;&#20154;&#21592;&#21644;&#20915;&#31574;&#32773;&#24212;&#27880;&#24847;&#23427;&#20204;&#30340;&#38480;&#21046;&#65292;&#38656;&#35201;&#22312;&#35813;&#39046;&#22495;&#26377;&#19968;&#20010;&#27010;&#24565;&#31361;&#30772;&#20197;&#35299;&#20915;XAI&#26041;&#27861;&#21644;&#24212;&#29992;&#20219;&#21153;&#20043;&#38388;&#30340;&#20860;&#23481;&#24615;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.06963</link><description>&lt;p&gt;
&#20219;&#21153;&#26080;&#20851;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26159;&#20010;&#31070;&#35805;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is Task-Agnostic Explainable AI a Myth?. (arXiv:2307.06963v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06963
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#32479;&#19968;&#24403;&#20195;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#30340;&#25361;&#25112;&#65292;&#25351;&#20986;&#34429;&#28982;XAI&#26041;&#27861;&#20026;&#27169;&#22411;&#25552;&#20379;&#20102;&#26377;&#29992;&#30340;&#36755;&#20986;&#65292;&#20294;&#30740;&#31350;&#20154;&#21592;&#21644;&#20915;&#31574;&#32773;&#24212;&#27880;&#24847;&#23427;&#20204;&#30340;&#38480;&#21046;&#65292;&#38656;&#35201;&#22312;&#35813;&#39046;&#22495;&#26377;&#19968;&#20010;&#27010;&#24565;&#31361;&#30772;&#20197;&#35299;&#20915;XAI&#26041;&#27861;&#21644;&#24212;&#29992;&#20219;&#21153;&#20043;&#38388;&#30340;&#20860;&#23481;&#24615;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#32479;&#19968;&#24403;&#20195;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;(XAI)&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#34429;&#28982;XAI&#26041;&#27861;&#20026;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#20102;&#34917;&#20805;&#21644;&#28508;&#22312;&#26377;&#29992;&#30340;&#36755;&#20986;&#65292;&#30740;&#31350;&#20154;&#21592;&#21644;&#20915;&#31574;&#32773;&#24212;&#35813;&#27880;&#24847;&#23427;&#20204;&#30340;&#27010;&#24565;&#21644;&#25216;&#26415;&#38480;&#21046;&#65292;&#36825;&#32463;&#24120;&#23548;&#33268;&#36825;&#20123;&#26041;&#27861;&#26412;&#36523;&#25104;&#20026;&#40657;&#30418;&#23376;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19977;&#20010;XAI&#30740;&#31350;&#26041;&#21521;&#65292;&#28085;&#30422;&#22270;&#20687;&#12289;&#25991;&#26412;&#21644;&#22270;&#24418;&#25968;&#25454;&#65292;&#21253;&#25324;&#26174;&#33879;&#24615;&#12289;&#27880;&#24847;&#21147;&#21644;&#22270;&#24418;&#35299;&#37322;&#22120;&#12290;&#23613;&#31649;&#25152;&#25552;&#21450;&#30340;&#26696;&#20363;&#30340;&#19978;&#19979;&#25991;&#21644;&#26102;&#38388;&#36328;&#24230;&#21508;&#19981;&#30456;&#21516;&#65292;&#20294;&#21516;&#26679;&#30340;&#25345;&#32493;&#38459;&#30861;&#20986;&#29616;&#20102;&#65292;&#31361;&#26174;&#20986;&#22312;&#35813;&#39046;&#22495;&#20013;&#35299;&#20915;XAI&#26041;&#27861;&#21644;&#24212;&#29992;&#20219;&#21153;&#20043;&#38388;&#20860;&#23481;&#24615;&#25361;&#25112;&#30340;&#27010;&#24565;&#31361;&#30772;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Our work serves as a framework for unifying the challenges of contemporary explainable AI (XAI). We demonstrate that while XAI methods provide supplementary and potentially useful output for machine learning models, researchers and decision-makers should be mindful of their conceptual and technical limitations, which frequently result in these methods themselves becoming black boxes. We examine three XAI research avenues spanning image, textual, and graph data, covering saliency, attention, and graph-type explainers. Despite the varying contexts and timeframes of the mentioned cases, the same persistent roadblocks emerge, highlighting the need for a conceptual breakthrough in the field to address the challenge of compatibility between XAI methods and application tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25968;&#20540;&#19981;&#31283;&#23450;&#24615;&#23545;&#21464;&#20998;&#27969;&#20013;&#37319;&#26679;&#12289;&#23494;&#24230;&#35780;&#20272;&#21644;ELBO&#20272;&#35745;&#30340;&#21487;&#38752;&#24615;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#29702;&#35770;&#20445;&#35777;&#21644;&#23454;&#39564;&#39564;&#35777;&#65292;&#25105;&#20204;&#21457;&#29616;&#23613;&#31649;&#23384;&#22312;&#20005;&#37325;&#30340;&#25968;&#20540;&#19981;&#31283;&#23450;&#24615;&#65292;&#21464;&#20998;&#27969;&#20135;&#29983;&#30340;&#32467;&#26524;&#22312;&#24212;&#29992;&#20013;&#24120;&#24120;&#36275;&#22815;&#20934;&#30830;&#12290;</title><link>http://arxiv.org/abs/2307.06957</link><description>&lt;p&gt;
&#25317;&#25265;&#28151;&#20081;&#65306;&#25968;&#20540;&#19981;&#31283;&#23450;&#24615;&#22312;&#21464;&#20998;&#27969;&#20013;&#30340;&#20998;&#26512;&#21644;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Embracing the chaos: analysis and diagnosis of numerical instability in variational flows. (arXiv:2307.06957v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06957
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25968;&#20540;&#19981;&#31283;&#23450;&#24615;&#23545;&#21464;&#20998;&#27969;&#20013;&#37319;&#26679;&#12289;&#23494;&#24230;&#35780;&#20272;&#21644;ELBO&#20272;&#35745;&#30340;&#21487;&#38752;&#24615;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#29702;&#35770;&#20445;&#35777;&#21644;&#23454;&#39564;&#39564;&#35777;&#65292;&#25105;&#20204;&#21457;&#29616;&#23613;&#31649;&#23384;&#22312;&#20005;&#37325;&#30340;&#25968;&#20540;&#19981;&#31283;&#23450;&#24615;&#65292;&#21464;&#20998;&#27969;&#20135;&#29983;&#30340;&#32467;&#26524;&#22312;&#24212;&#29992;&#20013;&#24120;&#24120;&#36275;&#22815;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25968;&#20540;&#19981;&#31283;&#23450;&#24615;&#23545;&#21464;&#20998;&#27969;&#20013;&#37319;&#26679;&#12289;&#23494;&#24230;&#35780;&#20272;&#21644;&#35777;&#25454;&#19979;&#30028;&#65288;ELBO&#65289;&#20272;&#35745;&#30340;&#21487;&#38752;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#23454;&#35777;&#39564;&#35777;&#20102;&#24120;&#35265;&#27969;&#21487;&#33021;&#20986;&#29616;&#20005;&#37325;&#30340;&#38169;&#35823;&#32047;&#31215;&#65306;&#25968;&#20540;&#27969;&#26144;&#23556;&#19982;&#31934;&#30830;&#26144;&#23556;&#30340;&#20559;&#24046;&#26174;&#33879;&#65292;&#24433;&#21709;&#37319;&#26679;&#65307;&#25968;&#20540;&#36870;&#27969;&#26144;&#23556;&#26080;&#27861;&#20934;&#30830;&#24674;&#22797;&#21021;&#22987;&#36755;&#20837;&#65292;&#24433;&#21709;&#23494;&#24230;&#21644;ELBO&#35745;&#31639;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#24778;&#35766;&#22320;&#21457;&#29616;&#65292;&#23613;&#31649;&#23384;&#22312;&#20005;&#37325;&#30340;&#25968;&#20540;&#19981;&#31283;&#23450;&#24615;&#65292;&#27969;&#20135;&#29983;&#30340;&#32467;&#26524;&#24120;&#24120;&#36275;&#22815;&#20934;&#30830;&#24212;&#23545;&#24212;&#29992;&#38656;&#27714;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#21464;&#20998;&#27969;&#35270;&#20026;&#21160;&#21147;&#31995;&#32479;&#65292;&#24182;&#21033;&#29992;&#38452;&#24433;&#29702;&#35770;&#36890;&#36807;&#29702;&#35770;&#20445;&#35777;&#23545;&#37319;&#26679;&#12289;&#23494;&#24230;&#35780;&#20272;&#21644;ELBO&#20272;&#35745;&#30340;&#38169;&#35823;&#26469;&#38416;&#26126;&#36825;&#31181;&#34892;&#20026;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#24182;&#32463;&#39564;&#24615;&#22320;&#27979;&#35797;&#20102;&#19968;&#31181;&#21487;&#20197;&#29992;&#20110;&#39564;&#35777;&#25968;&#20540;&#32467;&#26524;&#30340;&#35786;&#26029;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate the impact of numerical instability on the reliability of sampling, density evaluation, and evidence lower bound (ELBO) estimation in variational flows. We first empirically demonstrate that common flows can exhibit a catastrophic accumulation of error: the numerical flow map deviates significantly from the exact map -- which affects sampling -- and the numerical inverse flow map does not accurately recover the initial input -which affects density and ELBO computations. Surprisingly though, we find that results produced by flows are often accurate enough for applications despite the presence of serious numerical instability. In this work, we treat variational flows as dynamical systems, and leverage shadowing theory to elucidate this behavior via theoretical guarantees on the error of sampling, density evaluation, and ELBO estimation. Finally, we develop and empirically test a diagnostic procedure that can be used to validate results produced by numerica
&lt;/p&gt;</description></item><item><title>&#22269;&#38469;&#31038;&#20250;&#24517;&#39035;&#21512;&#20316;&#24212;&#23545;&#27668;&#20505;&#21464;&#21270;&#65292;&#36890;&#36807;&#32467;&#21512;&#20154;&#24037;&#26234;&#33021;&#21644;&#27668;&#20505;&#32463;&#27982;&#27169;&#25311;&#65292;&#35774;&#35745;&#20419;&#36827;&#21644;&#28608;&#21169;&#21512;&#20316;&#30340;&#22269;&#38469;&#26694;&#26550;&#65292;&#35299;&#20915;&#21327;&#35758;&#36981;&#23432;&#12289;&#25919;&#31574;&#30446;&#26631;&#23454;&#29616;&#21644;&#25345;&#32493;&#25215;&#35834;&#31561;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.06951</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#22312;&#20840;&#29699;&#27668;&#20505;&#21512;&#20316;2023&#31454;&#36187;&#35770;&#25991;&#38598;
&lt;/p&gt;
&lt;p&gt;
AI For Global Climate Cooperation 2023 Competition Proceedings. (arXiv:2307.06951v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06951
&lt;/p&gt;
&lt;p&gt;
&#22269;&#38469;&#31038;&#20250;&#24517;&#39035;&#21512;&#20316;&#24212;&#23545;&#27668;&#20505;&#21464;&#21270;&#65292;&#36890;&#36807;&#32467;&#21512;&#20154;&#24037;&#26234;&#33021;&#21644;&#27668;&#20505;&#32463;&#27982;&#27169;&#25311;&#65292;&#35774;&#35745;&#20419;&#36827;&#21644;&#28608;&#21169;&#21512;&#20316;&#30340;&#22269;&#38469;&#26694;&#26550;&#65292;&#35299;&#20915;&#21327;&#35758;&#36981;&#23432;&#12289;&#25919;&#31574;&#30446;&#26631;&#23454;&#29616;&#21644;&#25345;&#32493;&#25215;&#35834;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22269;&#38469;&#31038;&#20250;&#24517;&#39035;&#21512;&#20316;&#24212;&#23545;&#27668;&#20505;&#21464;&#21270;&#24182;&#20445;&#25345;&#32463;&#27982;&#22686;&#38271;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27809;&#26377;&#20840;&#29699;&#26435;&#23041;&#26426;&#26500;&#33021;&#30830;&#20445;&#22269;&#38469;&#27668;&#20505;&#21327;&#35758;&#30340;&#36981;&#23432;&#65292;&#21512;&#20316;&#24456;&#38590;&#23454;&#29616;&#12290;&#23558;&#20154;&#24037;&#26234;&#33021;&#19982;&#27668;&#20505;&#32463;&#27982;&#27169;&#25311;&#30456;&#32467;&#21512;&#65292;&#25552;&#20379;&#20102;&#35774;&#35745;&#20419;&#36827;&#21644;&#28608;&#21169;&#21512;&#20316;&#30340;&#22269;&#38469;&#26694;&#26550;&#30340;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#35848;&#21028;&#21327;&#35758;&#21644;&#27668;&#20505;&#21327;&#35758;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26694;&#26550;&#36824;&#24212;&#32771;&#34385;&#21040;&#27668;&#20505;&#32463;&#27982;&#21160;&#24577;&#21644;&#25112;&#30053;&#34892;&#20026;&#65292;&#20197;&#23454;&#29616;&#25919;&#31574;&#30446;&#26631;&#30340;&#23454;&#29616;&#21644;&#25345;&#32493;&#25215;&#35834;&#12290;&#36825;&#20123;&#25361;&#25112;&#38656;&#35201;&#36328;&#26426;&#22120;&#23398;&#20064;&#12289;&#32463;&#27982;&#23398;&#12289;&#27668;&#20505;&#31185;&#23398;&#12289;&#27861;&#24459;&#12289;&#25919;&#31574;&#12289;&#20262;&#29702;&#23398;&#31561;&#39046;&#22495;&#30340;&#36328;&#23398;&#31185;&#26041;&#27861;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#32452;&#32455;&#20102;AI for Global Climate Cooperation&#65288;&#20840;&#29699;&#27668;&#20505;&#21512;&#20316;&#20154;&#24037;&#26234;&#33021;&#65289;&#31454;&#36187;&#65292;&#21442;&#36187;&#22242;&#38431;&#25552;&#20132;&#20102;&#22522;&#20110;&#65288;&#20462;&#25913;&#30340;&#65289;RICE-N&#30340;&#22269;&#38469;&#26694;&#26550;&#30340;&#25552;&#26696;&#21644;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The international community must collaborate to mitigate climate change and sustain economic growth. However, collaboration is hard to achieve, partly because no global authority can ensure compliance with international climate agreements. Combining AI with climate-economic simulations offers a promising solution to design international frameworks, including negotiation protocols and climate agreements, that promote and incentivize collaboration. In addition, these frameworks should also have policy goals fulfillment, and sustained commitment, taking into account climate-economic dynamics and strategic behaviors. These challenges require an interdisciplinary approach across machine learning, economics, climate science, law, policy, ethics, and other fields.  Towards this objective, we organized AI for Global Climate Cooperation, a Mila competition in which teams submitted proposals and analyses of international frameworks, based on (modifications of) RICE-N, an AI-driven integrated ass
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25972;&#21512;&#20808;&#39564;&#30693;&#35782;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#23545;&#24037;&#31243;&#39046;&#22495;&#20013;&#30340;&#20449;&#24687;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#20102;&#26816;&#26597;&#65292;&#24182;&#25506;&#32034;&#20102;&#20197;&#19977;&#23618;&#30693;&#35782;&#25972;&#21512;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#36827;&#34892;&#30693;&#35782;&#20998;&#35299;&#12290;&#36825;&#31181;&#26041;&#27861;&#24179;&#34913;&#20102;&#25972;&#20307;&#35770;&#21644;&#36824;&#21407;&#35770;&#35266;&#28857;&#12290;</title><link>http://arxiv.org/abs/2307.06950</link><description>&lt;p&gt;
&#36808;&#21521;&#24037;&#31243;&#20013;&#20808;&#39564;&#30693;&#35782;&#25972;&#21512;&#30340;&#26426;&#22120;&#23398;&#20064;&#36335;&#24452;
&lt;/p&gt;
&lt;p&gt;
Pathway toward prior knowledge-integrated machine learning in engineering. (arXiv:2307.06950v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06950
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25972;&#21512;&#20808;&#39564;&#30693;&#35782;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#23545;&#24037;&#31243;&#39046;&#22495;&#20013;&#30340;&#20449;&#24687;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#20102;&#26816;&#26597;&#65292;&#24182;&#25506;&#32034;&#20102;&#20197;&#19977;&#23618;&#30693;&#35782;&#25972;&#21512;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#36827;&#34892;&#30693;&#35782;&#20998;&#35299;&#12290;&#36825;&#31181;&#26041;&#27861;&#24179;&#34913;&#20102;&#25972;&#20307;&#35770;&#21644;&#36824;&#21407;&#35770;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25968;&#23383;&#21270;&#36235;&#21183;&#21644;&#25968;&#25454;&#37327;&#28608;&#22686;&#65292;&#20294;&#22522;&#20110;&#31532;&#19968;&#21407;&#29702;&#30340;&#27169;&#22411;&#65288;&#20063;&#31216;&#20026;&#36923;&#36753;&#39537;&#21160;&#12289;&#22522;&#20110;&#29289;&#29702;&#12289;&#22522;&#20110;&#35268;&#21017;&#25110;&#22522;&#20110;&#30693;&#35782;&#30340;&#27169;&#22411;&#65289;&#19982;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#23384;&#22312;&#24182;&#34892;&#23384;&#22312;&#65292;&#21453;&#26144;&#20102;&#20851;&#20110;&#31526;&#21495;&#20027;&#20041;&#19982;&#36830;&#25509;&#20027;&#20041;&#30340;&#25345;&#32493;AI&#36777;&#35770;&#12290;&#22312;&#25968;&#25454;&#39537;&#21160;&#36807;&#31243;&#20013;&#20256;&#36755;&#21644;&#21033;&#29992;&#39046;&#22495;&#30693;&#35782;&#30340;&#36807;&#31243;&#24320;&#21457;&#30740;&#31350;&#24456;&#23569;&#35265;&#12290;&#26412;&#30740;&#31350;&#24378;&#35843;&#23558;&#22810;&#23398;&#31185;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#25972;&#21512;&#21040;&#26426;&#22120;&#30693;&#35782;&#21270;&#30340;&#25968;&#25454;&#39537;&#21160;&#36807;&#31243;&#20013;&#30340;&#21162;&#21147;&#21644;&#20027;&#27969;&#36235;&#21183;&#65292;&#20197;&#20004;&#31181;&#26041;&#24335;&#32452;&#32455;&#65306;&#26816;&#26597;&#30693;&#35782;&#34920;&#31034;&#20013;&#30340;&#20449;&#24687;&#19981;&#30830;&#23450;&#24615;&#26469;&#28304;&#65292;&#24182;&#25506;&#32034;&#20197;&#19977;&#23618;&#30693;&#35782;&#25972;&#21512;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#36827;&#34892;&#30693;&#35782;&#20998;&#35299;&#12290;&#36825;&#31181;&#26041;&#27861;&#24179;&#34913;&#20102;&#24037;&#31243;&#39046;&#22495;&#30340;&#25972;&#20307;&#35770;&#21644;&#36824;&#21407;&#35770;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the digitalization trend and data volume surge, first-principles models (also known as logic-driven, physics-based, rule-based, or knowledge-based models) and data-driven approaches have existed in parallel, mirroring the ongoing AI debate on symbolism versus connectionism. Research for process development to integrate both sides to transfer and utilize domain knowledge in the data-driven process is rare. This study emphasizes efforts and prevailing trends to integrate multidisciplinary domain professions into machine acknowledgeable, data-driven processes in a two-fold organization: examining information uncertainty sources in knowledge representation and exploring knowledge decomposition with a three-tier knowledge-integrated machine learning paradigm. This approach balances holist and reductionist perspectives in the engineering domain.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28508;&#22312;&#31354;&#38388;&#20998;&#35299;&#21644;&#26080;&#30417;&#30563;&#32858;&#31867;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#25581;&#31034;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23398;&#20064;&#21040;&#30340;&#27010;&#24565;&#21521;&#37327;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#27010;&#24565;&#21521;&#37327;&#19982;&#27169;&#22411;&#39044;&#27979;&#30456;&#20851;&#19988;&#20855;&#26377;&#35821;&#20041;&#30340;&#29420;&#29305;&#27010;&#24565;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#34920;&#26126;&#36825;&#20123;&#27010;&#24565;&#23545;&#20154;&#31867;&#26469;&#35828;&#26131;&#20110;&#29702;&#35299;&#21644;&#19982;&#20219;&#21153;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2307.06913</link><description>&lt;p&gt;
&#36890;&#36807;&#28508;&#22312;&#31354;&#38388;&#20998;&#35299;&#25581;&#31034;&#29420;&#29305;&#30340;&#27010;&#24565;&#21521;&#37327;
&lt;/p&gt;
&lt;p&gt;
Uncovering Unique Concept Vectors through Latent Space Decomposition. (arXiv:2307.06913v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06913
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28508;&#22312;&#31354;&#38388;&#20998;&#35299;&#21644;&#26080;&#30417;&#30563;&#32858;&#31867;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#25581;&#31034;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23398;&#20064;&#21040;&#30340;&#27010;&#24565;&#21521;&#37327;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#27010;&#24565;&#21521;&#37327;&#19982;&#27169;&#22411;&#39044;&#27979;&#30456;&#20851;&#19988;&#20855;&#26377;&#35821;&#20041;&#30340;&#29420;&#29305;&#27010;&#24565;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#34920;&#26126;&#36825;&#20123;&#27010;&#24565;&#23545;&#20154;&#31867;&#26469;&#35828;&#26131;&#20110;&#29702;&#35299;&#21644;&#19982;&#20219;&#21153;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#20869;&#37096;&#24037;&#20316;&#23545;&#20110;&#24314;&#31435;&#20449;&#20219;&#21644;&#30830;&#20445;&#27169;&#22411;&#23433;&#20840;&#33267;&#20851;&#37325;&#35201;&#12290;&#22522;&#20110;&#27010;&#24565;&#30340;&#35299;&#37322;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26356;&#26131;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#27604;&#22914;&#20687;&#32032;&#26174;&#33879;&#24615;&#31561;&#29305;&#24449;&#24402;&#22240;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#23450;&#20041;&#35299;&#37322;&#20998;&#26512;&#30340;&#27010;&#24565;&#20250;&#21463;&#21040;&#29992;&#25143;&#23545;&#27010;&#24565;&#26399;&#26395;&#30340;&#20559;&#24046;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20107;&#21518;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#25581;&#31034;&#28145;&#24230;&#27169;&#22411;&#22312;&#35757;&#32451;&#26399;&#38388;&#23398;&#20064;&#21040;&#30340;&#27010;&#24565;&#12290;&#36890;&#36807;&#20998;&#35299;&#19968;&#20010;&#23618;&#30340;&#28508;&#22312;&#31354;&#38388;&#25104;&#22855;&#24322;&#21521;&#37327;&#65292;&#24182;&#36890;&#36807;&#26080;&#30417;&#30563;&#32858;&#31867;&#23545;&#20854;&#36827;&#34892;&#31934;&#28860;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#19982;&#27169;&#22411;&#39044;&#27979;&#30456;&#20851;&#30340;&#39640;&#26041;&#24046;&#26041;&#21521;&#19978;&#30340;&#27010;&#24565;&#21521;&#37327;&#65292;&#24182;&#25351;&#21521;&#35821;&#20041;&#19978;&#29420;&#29305;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#22823;&#37096;&#20998;&#27010;&#24565;&#23545;&#20154;&#31867;&#26469;&#35828;&#26159;&#26131;&#20110;&#29702;&#35299;&#30340;&#65292;&#20855;&#26377;&#19968;&#33268;&#24615;&#65292;&#24182;&#19982;&#25152;&#38656;&#20219;&#21153;&#30456;&#20851;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Interpreting the inner workings of deep learning models is crucial for establishing trust and ensuring model safety. Concept-based explanations have emerged as a superior approach that is more interpretable than feature attribution estimates such as pixel saliency. However, defining the concepts for the interpretability analysis biases the explanations by the user's expectations on the concepts. To address this, we propose a novel post-hoc unsupervised method that automatically uncovers the concepts learned by deep models during training. By decomposing the latent space of a layer in singular vectors and refining them by unsupervised clustering, we uncover concept vectors aligned with directions of high variance that are relevant to the model prediction, and that point to semantically distinct concepts. Our extensive experiments reveal that the majority of our concepts are readily understandable to humans, exhibit coherency, and bear relevance to the task at hand. Moreover, we showcase
&lt;/p&gt;</description></item><item><title>PC-Droid&#26159;&#19968;&#20010;&#26032;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#26032;&#30340;&#25193;&#25955;&#20844;&#24335;&#21644;&#30740;&#31350;&#26356;&#36817;&#26399;&#30340;&#31215;&#20998;&#27714;&#35299;&#22120;&#65292;&#21516;&#26102;&#23545;&#25152;&#26377;&#31867;&#22411;&#30340;&#21943;&#27880;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#23427;&#19981;&#20165;&#33021;&#25552;&#20379;&#26356;&#24555;&#30340;&#29983;&#25104;&#36895;&#24230;&#65292;&#32780;&#19988;&#22312;&#25152;&#26377;&#35780;&#20272;&#25351;&#26631;&#19978;&#37117;&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.06836</link><description>&lt;p&gt;
PC-Droid: &#26356;&#24555;&#30340;&#25193;&#25955;&#36895;&#24230;&#21644;&#25913;&#36827;&#30340;&#31890;&#23376;&#20113;&#29983;&#25104;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
PC-Droid: Faster diffusion and improved quality for particle cloud generation. (arXiv:2307.06836v1 [hep-ex])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06836
&lt;/p&gt;
&lt;p&gt;
PC-Droid&#26159;&#19968;&#20010;&#26032;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#26032;&#30340;&#25193;&#25955;&#20844;&#24335;&#21644;&#30740;&#31350;&#26356;&#36817;&#26399;&#30340;&#31215;&#20998;&#27714;&#35299;&#22120;&#65292;&#21516;&#26102;&#23545;&#25152;&#26377;&#31867;&#22411;&#30340;&#21943;&#27880;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#23427;&#19981;&#20165;&#33021;&#25552;&#20379;&#26356;&#24555;&#30340;&#29983;&#25104;&#36895;&#24230;&#65292;&#32780;&#19988;&#22312;&#25152;&#26377;&#35780;&#20272;&#25351;&#26631;&#19978;&#37117;&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;PC-JeDi&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;PC-Droid&#65292;&#36825;&#26159;&#19968;&#20010;&#22823;&#24133;&#25913;&#36827;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#29983;&#25104;&#21943;&#27880;&#31890;&#23376;&#20113;&#12290;&#36890;&#36807;&#21033;&#29992;&#26032;&#30340;&#25193;&#25955;&#20844;&#24335;&#12289;&#30740;&#31350;&#26356;&#36817;&#26399;&#30340;&#31215;&#20998;&#27714;&#35299;&#22120;&#65292;&#24182;&#21516;&#26102;&#23545;&#25152;&#26377;&#31867;&#22411;&#30340;&#21943;&#27880;&#36827;&#34892;&#35757;&#32451;&#65292;&#25105;&#20204;&#33021;&#22815;&#22312;&#25152;&#26377;&#35780;&#20272;&#25351;&#26631;&#19978;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#27604;&#36739;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#20004;&#31181;&#20307;&#31995;&#32467;&#26500;&#21644;&#19968;&#33268;&#24615;&#33976;&#39311;&#30340;&#28508;&#21147;&#26469;&#30740;&#31350;&#29983;&#25104;&#36895;&#24230;&#21644;&#36136;&#37327;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#26356;&#24555;&#30340;&#20307;&#31995;&#32467;&#26500;&#21644;&#19968;&#33268;&#24615;&#27169;&#22411;&#37117;&#34920;&#29616;&#20986;&#36229;&#36234;&#35768;&#22810;&#31454;&#20105;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#29983;&#25104;&#26102;&#38388;&#27604;PC-JeDi&#24555;&#19978;&#20004;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building on the success of PC-JeDi we introduce PC-Droid, a substantially improved diffusion model for the generation of jet particle clouds. By leveraging a new diffusion formulation, studying more recent integration solvers, and training on all jet types simultaneously, we are able to achieve state-of-the-art performance for all types of jets across all evaluation metrics. We study the trade-off between generation speed and quality by comparing two attention based architectures, as well as the potential of consistency distillation to reduce the number of diffusion steps. Both the faster architecture and consistency models demonstrate performance surpassing many competing models, with generation time up to two orders of magnitude faster than PC-JeDi.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HyCPAP&#30340;&#28151;&#21512;&#25511;&#21046;&#31574;&#30053;&#65292;&#36890;&#36807;&#32467;&#21512;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#21644;&#38598;&#25104;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65292;&#24182;&#20805;&#20998;&#21033;&#29992;&#23427;&#20204;&#21508;&#33258;&#30340;&#20248;&#21183;&#65292;&#20197;&#35299;&#20915;&#20154;&#24037;&#33008;&#33146;&#30340;&#22797;&#26434;&#29983;&#29702;&#36807;&#31243;&#12289;&#24310;&#36831;&#33008;&#23707;&#32032;&#21453;&#24212;&#21644;&#19981;&#20934;&#30830;&#34880;&#31958;&#27979;&#37327;&#31561;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.06501</link><description>&lt;p&gt;
&#36890;&#36807;&#38598;&#25104;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#28151;&#21512;&#25511;&#21046;&#31574;&#30053;&#23454;&#29616;&#20154;&#24037;&#33008;&#33146;
&lt;/p&gt;
&lt;p&gt;
Hybrid Control Policy for Artificial Pancreas via Ensemble Deep Reinforcement Learning. (arXiv:2307.06501v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06501
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HyCPAP&#30340;&#28151;&#21512;&#25511;&#21046;&#31574;&#30053;&#65292;&#36890;&#36807;&#32467;&#21512;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#21644;&#38598;&#25104;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65292;&#24182;&#20805;&#20998;&#21033;&#29992;&#23427;&#20204;&#21508;&#33258;&#30340;&#20248;&#21183;&#65292;&#20197;&#35299;&#20915;&#20154;&#24037;&#33008;&#33146;&#30340;&#22797;&#26434;&#29983;&#29702;&#36807;&#31243;&#12289;&#24310;&#36831;&#33008;&#23707;&#32032;&#21453;&#24212;&#21644;&#19981;&#20934;&#30830;&#34880;&#31958;&#27979;&#37327;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#65306;&#20154;&#24037;&#33008;&#33146;(AP)&#22312;&#23454;&#29616;1&#22411;&#31958;&#23615;&#30149;&#24739;&#32773;&#38381;&#29615;&#34880;&#31958;&#25511;&#21046;&#26041;&#38754;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22797;&#26434;&#30340;&#29983;&#29702;&#36807;&#31243;&#12289;&#24310;&#36831;&#30340;&#33008;&#23707;&#32032;&#21453;&#24212;&#21644;&#19981;&#20934;&#30830;&#30340;&#34880;&#31958;&#27979;&#37327;&#65292;&#35774;&#35745;&#19968;&#31181;&#26377;&#25928;&#30340;AP&#25511;&#21046;&#31574;&#30053;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#34429;&#28982;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;(MPC)&#36890;&#36807;&#21160;&#24577;&#27169;&#22411;&#21644;&#23433;&#20840;&#32422;&#26463;&#25552;&#20379;&#20102;&#23433;&#20840;&#24615;&#21644;&#31283;&#23450;&#24615;&#65292;&#20294;&#20854;&#32570;&#20047;&#20010;&#24615;&#21270;&#65292;&#24182;&#19988;&#21463;&#21040;&#26410;&#23459;&#24067;&#30340;&#39278;&#39135;&#24433;&#21709;&#12290;&#30456;&#21453;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#25552;&#20379;&#20102;&#20010;&#24615;&#21270;&#21644;&#33258;&#36866;&#24212;&#31574;&#30053;&#65292;&#20294;&#38754;&#20020;&#20998;&#24067;&#20559;&#31227;&#21644;&#22823;&#37327;&#25968;&#25454;&#38656;&#27714;&#30340;&#25361;&#25112;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#25511;&#21046;&#31574;&#30053;&#65292;&#21363;HyCPAP&#65292;&#26469;&#24212;&#23545;&#19978;&#36848;&#25361;&#25112;&#12290;HyCPAP&#23558;MPC&#31574;&#30053;&#19982;&#38598;&#25104;DRL&#31574;&#30053;&#30456;&#32467;&#21512;&#65292;&#20805;&#20998;&#21033;&#29992;&#20004;&#31181;&#31574;&#30053;&#30340;&#20248;&#21183;&#65292;&#21516;&#26102;&#24357;&#34917;&#21508;&#33258;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Objective: The artificial pancreas (AP) has shown promising potential in achieving closed-loop glucose control for individuals with type 1 diabetes mellitus (T1DM). However, designing an effective control policy for the AP remains challenging due to the complex physiological processes, delayed insulin response, and inaccurate glucose measurements. While model predictive control (MPC) offers safety and stability through the dynamic model and safety constraints, it lacks individualization and is adversely affected by unannounced meals. Conversely, deep reinforcement learning (DRL) provides personalized and adaptive strategies but faces challenges with distribution shifts and substantial data requirements. Methods: We propose a hybrid control policy for the artificial pancreas (HyCPAP) to address the above challenges. HyCPAP combines an MPC policy with an ensemble DRL policy, leveraging the strengths of both policies while compensating for their respective limitations. To facilitate faste
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#32452;&#21512;&#20998;&#24067;&#20559;&#31227;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#30697;&#38453;&#34917;&#20840;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#29305;&#27530;&#24773;&#20917;&#19979;&#30340;&#21452;&#32447;&#24615;&#23884;&#20837;&#65292;&#23454;&#29616;&#23545;&#35757;&#32451;&#20013;&#26410;&#28085;&#30422;&#30340;&#27979;&#35797;&#20998;&#24067;&#36827;&#34892;&#22806;&#25512;&#12290;&#36825;&#20010;&#35774;&#32622;&#23558;&#32570;&#22833;&#38750;&#38543;&#26426;&#25968;&#25454;&#30340;&#30697;&#38453;&#34917;&#20840;&#38382;&#39064;&#24191;&#20041;&#21270;&#12290;</title><link>http://arxiv.org/abs/2307.06457</link><description>&lt;p&gt;
&#35299;&#20915;&#32452;&#21512;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#65306;&#22522;&#20110;&#30697;&#38453;&#34917;&#20840;&#30340;&#35266;&#28857;
&lt;/p&gt;
&lt;p&gt;
Tackling Combinatorial Distribution Shift: A Matrix Completion Perspective. (arXiv:2307.06457v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06457
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#32452;&#21512;&#20998;&#24067;&#20559;&#31227;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#30697;&#38453;&#34917;&#20840;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#29305;&#27530;&#24773;&#20917;&#19979;&#30340;&#21452;&#32447;&#24615;&#23884;&#20837;&#65292;&#23454;&#29616;&#23545;&#35757;&#32451;&#20013;&#26410;&#28085;&#30422;&#30340;&#27979;&#35797;&#20998;&#24067;&#36827;&#34892;&#22806;&#25512;&#12290;&#36825;&#20010;&#35774;&#32622;&#23558;&#32570;&#22833;&#38750;&#38543;&#26426;&#25968;&#25454;&#30340;&#30697;&#38453;&#34917;&#20840;&#38382;&#39064;&#24191;&#20041;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20998;&#24067;&#20559;&#31227;&#19979;&#33719;&#24471;&#20005;&#26684;&#30340;&#32479;&#35745;&#20445;&#35777;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#19988;&#27963;&#36291;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#31216;&#20026;&#32452;&#21512;&#20998;&#24067;&#20559;&#31227;&#30340;&#35774;&#32622;&#65292;&#20854;&#20013;(a)&#22312;&#27979;&#35797;&#21644;&#35757;&#32451;&#20998;&#24067;&#19979;&#65292;&#26631;&#31614;$z$&#30001;&#29305;&#24449;$(x,y)$&#30340;&#23545;&#20915;&#23450;&#65292;(b)&#35757;&#32451;&#20998;&#24067;&#28085;&#30422;&#20102;$x$&#21644;$y$&#20998;&#21035;&#30340;&#19968;&#23450;&#36793;&#32536;&#20998;&#24067;&#65292;&#20294;&#26159;(c)&#27979;&#35797;&#20998;&#24067;&#28041;&#21450;&#20102;&#19968;&#20010;&#22312;&#35757;&#32451;&#20998;&#24067;&#20013;&#26410;&#28085;&#30422;&#30340;$(x,y)$&#30340;&#20135;&#21697;&#20998;&#24067;&#30340;&#31034;&#20363;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#26631;&#31614;&#30001;&#21452;&#32447;&#24615;&#23884;&#20837;&#21040;Hilbert&#31354;&#38388;$H$&#20013;&#32473;&#20986;&#30340;&#29305;&#27530;&#24773;&#20917;&#65306;$\mathbb{E}[z \mid x,y ]=\langle f_{\star}(x),g_{\star}(y)\rangle_{{H}}$&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#23545;&#22312;&#35757;&#32451;&#20013;&#26410;&#28085;&#30422;&#30340;&#27979;&#35797;&#20998;&#24067;&#22495;&#36827;&#34892;&#22806;&#25512;&#65292;&#21363;&#23454;&#29616;&#21452;&#32447;&#24615;&#32452;&#21512;&#22806;&#25512;&#12290;&#25105;&#20204;&#30340;&#35774;&#32622;&#23558;&#32570;&#22833;&#38750;&#38543;&#26426;&#25968;&#25454;&#30340;&#30697;&#38453;&#34917;&#20840;&#30340;&#19968;&#20010;&#29305;&#27530;&#24773;&#20917;&#24191;&#20041;&#21270;&#65292;&#23545;&#20110;&#35813;&#24773;&#20917;&#65292;&#25152;&#26377;&#29616;&#26377;&#32467;&#26524;&#37117;&#35201;&#27714;....
&lt;/p&gt;
&lt;p&gt;
Obtaining rigorous statistical guarantees for generalization under distribution shift remains an open and active research area. We study a setting we call combinatorial distribution shift, where (a) under the test- and training-distributions, the labels $z$ are determined by pairs of features $(x,y)$, (b) the training distribution has coverage of certain marginal distributions over $x$ and $y$ separately, but (c) the test distribution involves examples from a product distribution over $(x,y)$ that is {not} covered by the training distribution. Focusing on the special case where the labels are given by bilinear embeddings into a Hilbert space $H$: $\mathbb{E}[z \mid x,y ]=\langle f_{\star}(x),g_{\star}(y)\rangle_{{H}}$, we aim to extrapolate to a test distribution domain that is $not$ covered in training, i.e., achieving bilinear combinatorial extrapolation.  Our setting generalizes a special case of matrix completion from missing-not-at-random data, for which all existing results requi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#36719;&#24178;&#39044;&#20013;&#30830;&#20445;&#22240;&#26524;&#20998;&#35299;&#30340;&#21487;&#35782;&#21035;&#24615;&#12290;&#36890;&#36807;&#24320;&#21457;&#19968;&#31181;&#33258;&#32534;&#30721;&#21464;&#20998;&#36125;&#21494;&#26031;&#31639;&#27861;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#32473;&#23450;&#19968;&#33324;&#21270;&#30340;&#24544;&#35802;&#24615;&#27010;&#24565;&#30340;&#24773;&#20917;&#19979;&#65292;&#21363;&#20351;&#23384;&#22312;&#26410;&#35266;&#27979;&#21040;&#30340;&#22240;&#26524;&#21464;&#37327;&#65292;&#20173;&#28982;&#21487;&#20197;&#24674;&#22797;&#28508;&#22312;&#30340;&#22240;&#26524;&#27169;&#22411;&#65292;&#24182;&#22312;&#26080;&#38480;&#25968;&#25454;&#30340;&#26497;&#38480;&#24773;&#20917;&#19979;&#39044;&#27979;&#26410;&#35265;&#32452;&#21512;&#30340;&#24178;&#39044;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.06250</link><description>&lt;p&gt;
&#20174;&#36719;&#24178;&#39044;&#20013;&#30830;&#20445;&#22240;&#26524;&#20998;&#35299;&#30340;&#21487;&#35782;&#21035;&#24615;
&lt;/p&gt;
&lt;p&gt;
Identifiability Guarantees for Causal Disentanglement from Soft Interventions. (arXiv:2307.06250v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06250
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#36719;&#24178;&#39044;&#20013;&#30830;&#20445;&#22240;&#26524;&#20998;&#35299;&#30340;&#21487;&#35782;&#21035;&#24615;&#12290;&#36890;&#36807;&#24320;&#21457;&#19968;&#31181;&#33258;&#32534;&#30721;&#21464;&#20998;&#36125;&#21494;&#26031;&#31639;&#27861;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#32473;&#23450;&#19968;&#33324;&#21270;&#30340;&#24544;&#35802;&#24615;&#27010;&#24565;&#30340;&#24773;&#20917;&#19979;&#65292;&#21363;&#20351;&#23384;&#22312;&#26410;&#35266;&#27979;&#21040;&#30340;&#22240;&#26524;&#21464;&#37327;&#65292;&#20173;&#28982;&#21487;&#20197;&#24674;&#22797;&#28508;&#22312;&#30340;&#22240;&#26524;&#27169;&#22411;&#65292;&#24182;&#22312;&#26080;&#38480;&#25968;&#25454;&#30340;&#26497;&#38480;&#24773;&#20917;&#19979;&#39044;&#27979;&#26410;&#35265;&#32452;&#21512;&#30340;&#24178;&#39044;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#20998;&#35299;&#26088;&#22312;&#36890;&#36807;&#28508;&#22312;&#21464;&#37327;&#30340;&#30456;&#20851;&#24615;&#25581;&#31034;&#25968;&#25454;&#30340;&#34920;&#24449;&#65292;&#20854;&#36890;&#36807;&#22240;&#26524;&#27169;&#22411;&#30456;&#20114;&#20851;&#32852;&#12290;&#22914;&#26524;&#35299;&#37322;&#25968;&#25454;&#30340;&#28508;&#22312;&#27169;&#22411;&#26159;&#21807;&#19968;&#30340;&#65292;&#37027;&#20040;&#36825;&#31181;&#34920;&#31034;&#26159;&#21487;&#35782;&#21035;&#30340;&#12290;&#26412;&#25991;&#20851;&#27880;&#30340;&#26159;&#24403;&#23384;&#22312;&#19981;&#37197;&#23545;&#30340;&#35266;&#27979;&#21644;&#24178;&#39044;&#25968;&#25454;&#26102;&#30340;&#24773;&#20917;&#65292;&#27599;&#20010;&#24178;&#39044;&#37117;&#20250;&#25913;&#21464;&#19968;&#20010;&#28508;&#22312;&#21464;&#37327;&#30340;&#26426;&#21046;&#12290;&#24403;&#22240;&#26524;&#21464;&#37327;&#23436;&#20840;&#35266;&#27979;&#21040;&#26102;&#65292;&#22312;&#35802;&#23454;&#24615;&#20551;&#35774;&#19979;&#65292;&#24050;&#32463;&#24320;&#21457;&#20986;&#20102;&#32479;&#35745;&#19968;&#33268;&#30340;&#31639;&#27861;&#26469;&#35782;&#21035;&#22240;&#26524;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#23637;&#31034;&#65292;&#21363;&#20351;&#23384;&#22312;&#26410;&#35266;&#27979;&#21040;&#30340;&#22240;&#26524;&#21464;&#37327;&#65292;&#22312;&#32473;&#23450;&#19968;&#33324;&#21270;&#30340;&#24544;&#35802;&#24615;&#27010;&#24565;&#30340;&#24773;&#20917;&#19979;&#20173;&#28982;&#21487;&#20197;&#23454;&#29616;&#21487;&#35782;&#21035;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20445;&#35777;&#20102;&#25105;&#20204;&#21487;&#20197;&#24674;&#22797;&#28508;&#22312;&#30340;&#22240;&#26524;&#27169;&#22411;&#65292;&#39044;&#27979;&#26410;&#35265;&#32452;&#21512;&#30340;&#24178;&#39044;&#25928;&#26524;&#65292;&#22312;&#26080;&#38480;&#25968;&#25454;&#30340;&#26497;&#38480;&#24773;&#20917;&#19979;&#12290;&#25105;&#20204;&#36890;&#36807;&#24320;&#21457;&#19968;&#31181;&#33258;&#32534;&#30721;&#21464;&#20998;&#36125;&#21494;&#26031;&#31639;&#27861;&#21644;ap&#26469;&#23454;&#29616;&#25105;&#20204;&#30340;&#22240;&#26524;&#20998;&#35299;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal disentanglement aims to uncover a representation of data using latent variables that are interrelated through a causal model. Such a representation is identifiable if the latent model that explains the data is unique. In this paper, we focus on the scenario where unpaired observational and interventional data are available, with each intervention changing the mechanism of a latent variable. When the causal variables are fully observed, statistically consistent algorithms have been developed to identify the causal model under faithfulness assumptions. We here show that identifiability can still be achieved with unobserved causal variables, given a generalized notion of faithfulness. Our results guarantee that we can recover the latent causal model up to an equivalence class and predict the effect of unseen combinations of interventions, in the limit of infinite data. We implement our causal disentanglement framework by developing an autoencoding variational Bayes algorithm and ap
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20197;&#20302;&#31209;&#35757;&#32451;&#25216;&#26415;&#20316;&#20026;&#26367;&#20195;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ReLoRA&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#20302;&#31209;&#26356;&#26032;&#26469;&#35757;&#32451;&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#12290;&#22312;&#39044;&#35757;&#32451;&#30340;Transformer&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;ReLoRA&#22312;&#19982;&#24120;&#35268;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30456;&#27604;&#30340;&#24615;&#33021;&#34920;&#29616;&#19978;&#30456;&#24403;&#65292;&#24182;&#21457;&#29616;&#20854;&#22312;&#27169;&#22411;&#36234;&#22823;&#30340;&#24773;&#20917;&#19979;&#25928;&#29575;&#36234;&#39640;&#65292;&#20026;&#39640;&#25928;&#35757;&#32451;&#21315;&#20159;&#32423;&#21442;&#25968;&#32593;&#32476;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.05695</link><description>&lt;p&gt;
&#20197;&#19981;&#21516;&#26041;&#24335;&#22534;&#21472;&#26356;&#22810;&#23618;&#65306;&#36890;&#36807;&#20302;&#31209;&#26356;&#26032;&#36827;&#34892;&#39640;&#31209;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Stack More Layers Differently: High-Rank Training Through Low-Rank Updates. (arXiv:2307.05695v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05695
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20197;&#20302;&#31209;&#35757;&#32451;&#25216;&#26415;&#20316;&#20026;&#26367;&#20195;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ReLoRA&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#20302;&#31209;&#26356;&#26032;&#26469;&#35757;&#32451;&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#12290;&#22312;&#39044;&#35757;&#32451;&#30340;Transformer&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;ReLoRA&#22312;&#19982;&#24120;&#35268;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30456;&#27604;&#30340;&#24615;&#33021;&#34920;&#29616;&#19978;&#30456;&#24403;&#65292;&#24182;&#21457;&#29616;&#20854;&#22312;&#27169;&#22411;&#36234;&#22823;&#30340;&#24773;&#20917;&#19979;&#25928;&#29575;&#36234;&#39640;&#65292;&#20026;&#39640;&#25928;&#35757;&#32451;&#21315;&#20159;&#32423;&#21442;&#25968;&#32593;&#32476;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#35268;&#27169;&#32593;&#32476;&#25317;&#26377;&#25968;&#30334;&#20159;&#20010;&#21442;&#25968;&#30340;&#35268;&#27169;&#24050;&#32463;&#21344;&#20027;&#23548;&#22320;&#20301;&#24182;&#19988;&#25928;&#26524;&#26174;&#33879;&#65292;&#20294;&#23545;&#20110;&#36807;&#24230;&#21442;&#25968;&#21270;&#27169;&#22411;&#30340;&#35757;&#32451;&#24517;&#35201;&#24615;&#20173;&#28982;&#32570;&#20047;&#28165;&#26224;&#30340;&#29702;&#35299;&#65292;&#32780;&#26367;&#20195;&#26041;&#27861;&#19981;&#19968;&#23450;&#33021;&#22815;&#38477;&#20302;&#35757;&#32451;&#39640;&#24615;&#33021;&#27169;&#22411;&#30340;&#25104;&#26412;&#12290;&#26412;&#25991;&#25506;&#32034;&#20102;&#20302;&#31209;&#35757;&#32451;&#25216;&#26415;&#20316;&#20026;&#35757;&#32451;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;ReLoRA&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#20302;&#31209;&#26356;&#26032;&#26469;&#35757;&#32451;&#39640;&#31209;&#32593;&#32476;&#12290;&#25105;&#20204;&#23558;ReLoRA&#24212;&#29992;&#20110;&#39044;&#35757;&#32451;&#30340;Transformer&#35821;&#35328;&#27169;&#22411;&#65292;&#21442;&#25968;&#37327;&#39640;&#36798;350M&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#19982;&#24120;&#35268;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;ReLoRA&#30340;&#25928;&#29575;&#38543;&#30528;&#27169;&#22411;&#22823;&#23567;&#30340;&#22686;&#21152;&#32780;&#25552;&#39640;&#65292;&#36825;&#20351;&#24471;&#23427;&#25104;&#20026;&#39640;&#25928;&#35757;&#32451;&#21315;&#20159;&#32423;&#21442;&#25968;&#32593;&#32476;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#20302;&#31209;&#35757;&#32451;&#25216;&#26415;&#30340;&#28508;&#21147;&#21450;&#20854;&#23545;&#20110;&#32553;&#25918;&#23450;&#24459;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the dominance and effectiveness of scaling, resulting in large networks with hundreds of billions of parameters, the necessity to train overparametrized models remains poorly understood, and alternative approaches do not necessarily make it cheaper to train high-performance models. In this paper, we explore low-rank training techniques as an alternative approach to training large neural networks. We introduce a novel method called ReLoRA, which utilizes low-rank updates to train high-rank networks. We apply ReLoRA to pre-training transformer language models with up to 350M parameters and demonstrate comparable performance to regular neural network training. Furthermore, we observe that the efficiency of ReLoRA increases with model size, making it a promising approach for training multi-billion-parameter networks efficiently. Our findings shed light on the potential of low-rank training techniques and their implications for scaling laws.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32447;&#24615;&#22238;&#24402;&#26469;&#36845;&#20195;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#36755;&#20986;&#21521;&#21518;&#35745;&#31639;&#31070;&#32463;&#20803;&#30340;&#29702;&#24819;&#24635;&#36755;&#20837;&#20540;&#65292;&#20197;&#32447;&#24615;&#26368;&#23567;&#20108;&#20056;&#38382;&#39064;&#36845;&#20195;&#26356;&#26032;&#21442;&#25968;&#21644;&#28608;&#27963;&#20540;&#12290;</title><link>http://arxiv.org/abs/2307.05189</link><description>&lt;p&gt;
&#20351;&#29992;&#32447;&#24615;&#22238;&#24402;&#36845;&#20195;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Using Linear Regression for Iteratively Training Neural Networks. (arXiv:2307.05189v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05189
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32447;&#24615;&#22238;&#24402;&#26469;&#36845;&#20195;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#36755;&#20986;&#21521;&#21518;&#35745;&#31639;&#31070;&#32463;&#20803;&#30340;&#29702;&#24819;&#24635;&#36755;&#20837;&#20540;&#65292;&#20197;&#32447;&#24615;&#26368;&#23567;&#20108;&#20056;&#38382;&#39064;&#36845;&#20195;&#26356;&#26032;&#21442;&#25968;&#21644;&#28608;&#27963;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31616;&#21333;&#32447;&#24615;&#22238;&#24402;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#21644;&#20559;&#32622;&#65292;&#20316;&#20026;&#26631;&#20934;&#26799;&#24230;&#21453;&#21521;&#20256;&#25773;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#30446;&#21069;&#30340;&#24037;&#20316;&#26159;&#25506;&#32034;&#24615;&#30340;&#65292;&#24182;&#19988;&#25105;&#20204;&#23558;&#25551;&#36848;&#21644;&#23454;&#39564;&#38480;&#21046;&#22312;&#65288;i&#65289;&#31616;&#21333;&#30340;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#65292;&#65288;ii&#65289;&#26631;&#37327;&#65288;&#21333;&#36755;&#20986;&#65289;&#22238;&#24402;&#38382;&#39064;&#65292;&#20197;&#21450;&#65288;iii&#65289;&#21487;&#36870;&#28608;&#27963;&#20989;&#25968;&#19978;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25193;&#23637;&#21040;&#26356;&#22823;&#12289;&#26356;&#22797;&#26434;&#30340;&#26550;&#26500;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#35266;&#23519;&#21040;&#31070;&#32463;&#32593;&#32476;&#20013;&#27599;&#20010;&#31070;&#32463;&#20803;&#30340;&#36755;&#20837;&#26159;&#21069;&#19968;&#23618;&#31070;&#32463;&#20803;&#30340;&#28608;&#27963;&#20197;&#21450;&#35813;&#23618;&#30340;&#21442;&#25968;&#65288;&#26435;&#37325;&#21644;&#20559;&#32622;&#65289;&#30340;&#32447;&#24615;&#32452;&#21512;&#12290;&#22914;&#26524;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#20174;&#36755;&#20986;&#21521;&#21518;&#35745;&#31639;&#27599;&#20010;&#31070;&#32463;&#20803;&#30340;&#29702;&#24819;&#24635;&#36755;&#20837;&#20540;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#23398;&#20064;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#32447;&#24615;&#26368;&#23567;&#20108;&#20056;&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#22312;&#26356;&#26032;&#21442;&#25968;&#21644;&#28608;&#27963;&#20540;&#20043;&#38388;&#36845;&#20195;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26126;&#30830;&#30340;&#31639;&#27861;&#26469;&#23454;&#29616;&#36825;&#20010;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a simple linear regression based approach for learning the weights and biases of a neural network, as an alternative to standard gradient based backpropagation. The present work is exploratory in nature, and we restrict the description and experiments to (i) simple feedforward neural networks, (ii) scalar (single output) regression problems, and (iii) invertible activation functions. However, the approach is intended to be extensible to larger, more complex architectures. The key idea is the observation that the input to every neuron in a neural network is a linear combination of the activations of neurons in the previous layer, as well as the parameters (weights and biases) of the layer. If we are able to compute the ideal total input values to every neuron by working backwards from the output, we can formulate the learning problem as a linear least squares problem which iterates between updating the parameters and the activation values. We present an explicit algorithm tha
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#31181;&#21033;&#29992;&#20984;&#31070;&#32463;&#26368;&#20248;&#36755;&#36816;&#27714;&#35299;&#22120;&#36827;&#34892;&#21435;&#30456;&#20851;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#20248;&#36755;&#36816;&#23558;&#36830;&#32493;&#29305;&#24449;&#31354;&#38388;&#19982;&#21463;&#20445;&#25252;&#23646;&#24615;&#21435;&#30456;&#20851;&#12290;&#22312;&#39640;&#33021;&#29289;&#29702;&#20013;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#21943;&#27880;&#20998;&#31867;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#33021;&#26356;&#22909;&#22320;&#21435;&#30456;&#20851;&#22810;&#31867;&#36755;&#20986;&#29305;&#24449;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2307.05187</link><description>&lt;p&gt;
&#20351;&#29992;&#26368;&#20248;&#36755;&#36816;&#36827;&#34892;&#21435;&#30456;&#20851;
&lt;/p&gt;
&lt;p&gt;
Decorrelation using Optimal Transport. (arXiv:2307.05187v1 [hep-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05187
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#31181;&#21033;&#29992;&#20984;&#31070;&#32463;&#26368;&#20248;&#36755;&#36816;&#27714;&#35299;&#22120;&#36827;&#34892;&#21435;&#30456;&#20851;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#20248;&#36755;&#36816;&#23558;&#36830;&#32493;&#29305;&#24449;&#31354;&#38388;&#19982;&#21463;&#20445;&#25252;&#23646;&#24615;&#21435;&#30456;&#20851;&#12290;&#22312;&#39640;&#33021;&#29289;&#29702;&#20013;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#21943;&#27880;&#20998;&#31867;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#33021;&#26356;&#22909;&#22320;&#21435;&#30456;&#20851;&#22810;&#31867;&#36755;&#20986;&#29305;&#24449;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#22815;&#20174;&#21463;&#20445;&#25252;&#23646;&#24615;&#20013;&#23558;&#29305;&#24449;&#31354;&#38388;&#21435;&#30456;&#20851;&#26159;&#20262;&#29702;&#23398;&#12289;&#20844;&#24179;&#24615;&#20197;&#21450;&#33258;&#28982;&#31185;&#23398;&#39046;&#22495;&#27963;&#36291;&#30740;&#31350;&#21644;&#23398;&#20064;&#30340;&#19968;&#20010;&#39046;&#22495;&#12290;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#21033;&#29992;&#20984;&#31070;&#32463;&#26368;&#20248;&#36755;&#36816;&#27714;&#35299;&#22120;&#65288;Cnots&#65289;&#36827;&#34892;&#21435;&#30456;&#20851;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#36890;&#36807;&#26368;&#20248;&#36755;&#36816;&#20351;&#36830;&#32493;&#29305;&#24449;&#31354;&#38388;&#19982;&#21463;&#20445;&#25252;&#23646;&#24615;&#21435;&#30456;&#20851;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#22312;&#39640;&#33021;&#29289;&#29702;&#20013;&#36827;&#34892;&#21943;&#27880;&#20998;&#31867;&#26102;&#65292;&#23427;&#30340;&#34920;&#29616;&#22914;&#20309;&#65292;&#20854;&#20013;&#20998;&#31867;&#22120;&#24471;&#20998;&#24076;&#26395;&#19982;&#21943;&#27880;&#30340;&#36136;&#37327;&#21435;&#30456;&#20851;&#12290;&#22312;&#20108;&#36827;&#21046;&#20998;&#31867;&#20013;&#23454;&#29616;&#30340;&#21435;&#30456;&#20851;&#31243;&#24230;&#25509;&#36817;&#20110;&#20351;&#29992;&#26465;&#20214;&#24402;&#19968;&#21270;&#27969;&#30340;&#26368;&#26032;&#27700;&#24179;&#12290;&#24403;&#36716;&#21521;&#22810;&#31867;&#36755;&#20986;&#26102;&#65292;&#26368;&#20248;&#36755;&#36816;&#26041;&#27861;&#30340;&#24615;&#33021;&#26174;&#33879;&#20248;&#20110;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#34920;&#26126;&#22312;&#21435;&#30456;&#20851;&#22810;&#32500;&#29305;&#24449;&#31354;&#38388;&#26041;&#38754;&#26377;&#23454;&#36136;&#24615;&#30340;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Being able to decorrelate a feature space from protected attributes is an area of active research and study in ethics, fairness, and also natural sciences. We introduce a novel decorrelation method using Convex Neural Optimal Transport Solvers (Cnots), that is able to decorrelate continuous feature space against protected attributes with optimal transport. We demonstrate how well it performs in the context of jet classification in high energy physics, where classifier scores are desired to be decorrelated from the mass of a jet. The decorrelation achieved in binary classification approaches the levels achieved by the state-of-the-art using conditional normalising flows. When moving to multiclass outputs the optimal transport approach performs significantly better than the state-of-the-art, suggesting substantial gains at decorrelating multidimensional feature spaces.
&lt;/p&gt;</description></item><item><title>&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24212;&#29992;&#20154;&#31867;&#22909;&#22855;&#24515;&#30340;&#20004;&#20010;&#29702;&#35770;&#65292;&#21457;&#23637;&#20102;&#19968;&#31181;&#20869;&#22312;&#39537;&#21160;&#30340;&#22270;&#25506;&#32034;&#26041;&#27861;&#12290;&#25105;&#20204;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24378;&#21270;&#23398;&#20064;&#23558;&#25299;&#25169;&#29305;&#24449;&#20316;&#20026;&#22870;&#21169;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#25506;&#32034;&#12290;&#22312;&#22810;&#31867;&#21512;&#25104;&#29983;&#25104;&#22270;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#21487;&#20197;&#25512;&#24191;&#21040;&#26356;&#22823;&#30340;&#29615;&#22659;&#65292;&#36824;&#21487;&#20197;&#36827;&#34892;&#26356;&#38271;&#30340;&#25506;&#32034;&#27493;&#34892;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#20256;&#32479;&#30340;&#36138;&#23146;&#35780;&#20272;&#26041;&#27861;&#26356;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2307.04962</link><description>&lt;p&gt;
&#21033;&#29992;&#20154;&#31867;&#22909;&#22855;&#24515;&#30340;&#32593;&#32476;&#29702;&#35770;&#36827;&#34892;&#20869;&#22312;&#39537;&#21160;&#30340;&#22270;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Intrinsically motivated graph exploration using network theories of human curiosity. (arXiv:2307.04962v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04962
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24212;&#29992;&#20154;&#31867;&#22909;&#22855;&#24515;&#30340;&#20004;&#20010;&#29702;&#35770;&#65292;&#21457;&#23637;&#20102;&#19968;&#31181;&#20869;&#22312;&#39537;&#21160;&#30340;&#22270;&#25506;&#32034;&#26041;&#27861;&#12290;&#25105;&#20204;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24378;&#21270;&#23398;&#20064;&#23558;&#25299;&#25169;&#29305;&#24449;&#20316;&#20026;&#22870;&#21169;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#25506;&#32034;&#12290;&#22312;&#22810;&#31867;&#21512;&#25104;&#29983;&#25104;&#22270;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#21487;&#20197;&#25512;&#24191;&#21040;&#26356;&#22823;&#30340;&#29615;&#22659;&#65292;&#36824;&#21487;&#20197;&#36827;&#34892;&#26356;&#38271;&#30340;&#25506;&#32034;&#27493;&#34892;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#20256;&#32479;&#30340;&#36138;&#23146;&#35780;&#20272;&#26041;&#27861;&#26356;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20869;&#22312;&#39537;&#21160;&#30340;&#25506;&#32034;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#24050;&#34987;&#35777;&#26126;&#20855;&#26377;&#29992;&#36884;&#65292;&#21363;&#20351;&#27809;&#26377;&#39069;&#22806;&#30340;&#22806;&#22312;&#22870;&#21169;&#12290;&#24403;&#29615;&#22659;&#33258;&#28982;&#34920;&#31034;&#20026;&#22270;&#26102;&#65292;&#22914;&#20309;&#26368;&#22909;&#22320;&#24341;&#23548;&#25506;&#32034;&#20173;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20154;&#31867;&#22909;&#22855;&#24515;&#30340;&#20004;&#20010;&#29702;&#35770;&#65306;&#20449;&#24687;&#24046;&#29702;&#35770;&#21644;&#21387;&#32553;&#36827;&#23637;&#29702;&#35770;&#65292;&#26469;&#28608;&#21169;&#23545;&#22270;&#32467;&#26500;&#25968;&#25454;&#36827;&#34892;&#25506;&#32034;&#12290;&#36825;&#20123;&#29702;&#35770;&#23558;&#22909;&#22855;&#24515;&#35270;&#20026;&#23545;&#29615;&#22659;&#20013;&#35775;&#38382;&#33410;&#28857;&#25152;&#24341;&#21457;&#30340;&#23376;&#22270;&#30340;&#25299;&#25169;&#29305;&#24449;&#36827;&#34892;&#20248;&#21270;&#30340;&#20869;&#22312;&#21160;&#26426;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#25552;&#20986;&#30340;&#29305;&#24449;&#20316;&#20026;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#22870;&#21169;&#12290;&#22312;&#22810;&#20010;&#31867;&#21035;&#30340;&#21512;&#25104;&#29983;&#25104;&#22270;&#19978;&#65292;&#25105;&#20204;&#21457;&#29616;&#35757;&#32451;&#20195;&#29702;&#21487;&#20197;&#25512;&#24191;&#21040;&#26356;&#22823;&#30340;&#29615;&#22659;&#21644;&#27604;&#35757;&#32451;&#36807;&#31243;&#20013;&#26356;&#38271;&#30340;&#25506;&#32034;&#24615;&#27493;&#34892;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#35745;&#31639;&#25928;&#29575;&#39640;&#20110;&#30456;&#20851;&#25299;&#25169;&#23646;&#24615;&#30340;&#36138;&#23146;&#35780;&#20272;&#12290;&#25152;&#25552;&#20986;&#30340;&#20869;&#22312;&#21160;&#26426;&#20135;&#29983;&#30340;&#22870;&#21169;&#22312;&#22810;&#31867;&#21512;&#25104;&#29983;&#25104;&#22270;&#29983;&#25104;&#19978;&#25512;&#24191;&#33391;&#22909;&#65292;&#24182;&#19988;&#22312;&#35757;&#32451;&#26399;&#38388;&#33021;&#22815;&#22312;&#26356;&#22823;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#26356;&#38271;&#30340;&#25506;&#32034;&#27493;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intrinsically motivated exploration has proven useful for reinforcement learning, even without additional extrinsic rewards. When the environment is naturally represented as a graph, how to guide exploration best remains an open question. In this work, we propose a novel approach for exploring graph-structured data motivated by two theories of human curiosity: the information gap theory and the compression progress theory. The theories view curiosity as an intrinsic motivation to optimize for topological features of subgraphs induced by the visited nodes in the environment. We use these proposed features as rewards for graph neural-network-based reinforcement learning. On multiple classes of synthetically generated graphs, we find that trained agents generalize to larger environments and to longer exploratory walks than are seen during training. Our method computes more efficiently than the greedy evaluation of the relevant topological properties. The proposed intrinsic motivations bea
&lt;/p&gt;</description></item><item><title>LINFA&#26159;&#19968;&#20010;&#29992;&#20110;&#21464;&#20998;&#25512;&#29702;&#30340;Python&#24211;&#65292;&#25903;&#25345;&#22788;&#29702;&#35745;&#31639;&#22797;&#26434;&#30340;&#27169;&#22411;&#21644;&#38590;&#20197;&#37319;&#26679;&#30340;&#20381;&#36182;&#21442;&#25968;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2307.04675</link><description>&lt;p&gt;
LINFA:&#19968;&#31181;&#29992;&#20110;&#21464;&#20998;&#25512;&#29702;&#30340;Python&#24211;&#65292;&#21253;&#21547;&#24402;&#19968;&#21270;&#27969;&#21160;&#21644;&#36864;&#28779;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
LINFA: a Python library for variational inference with normalizing flow and annealing. (arXiv:2307.04675v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04675
&lt;/p&gt;
&lt;p&gt;
LINFA&#26159;&#19968;&#20010;&#29992;&#20110;&#21464;&#20998;&#25512;&#29702;&#30340;Python&#24211;&#65292;&#25903;&#25345;&#22788;&#29702;&#35745;&#31639;&#22797;&#26434;&#30340;&#27169;&#22411;&#21644;&#38590;&#20197;&#37319;&#26679;&#30340;&#20381;&#36182;&#21442;&#25968;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#25512;&#29702;&#26159;&#32479;&#35745;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#36234;&#26469;&#36234;&#27969;&#34892;&#30340;&#19968;&#31181;&#36817;&#20284;&#27010;&#29575;&#20998;&#24067;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;LINFA&#65288;&#29992;&#20110;&#24402;&#19968;&#21270;&#27969;&#21160;&#21644;&#36864;&#28779;&#31639;&#27861;&#30340;&#25512;&#29702;&#24211;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#21464;&#20998;&#25512;&#29702;&#30340;Python&#24211;&#65292;&#36866;&#29992;&#20110;&#35745;&#31639;&#22797;&#26434;&#30340;&#27169;&#22411;&#21644;&#38590;&#20197;&#37319;&#26679;&#30340;&#20381;&#36182;&#21442;&#25968;&#20998;&#24067;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;LINFA&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#29702;&#35770;&#32972;&#26223;&#12289;&#21151;&#33021;&#21644;&#24615;&#33021;&#12290;LINFA&#21487;&#22312;GitHub&#19978;&#20844;&#24320;&#33719;&#21462;&#65292;&#32593;&#22336;&#20026;https://github.com/desResLab/LINFA&#12290;
&lt;/p&gt;
&lt;p&gt;
Variational inference is an increasingly popular method in statistics and machine learning for approximating probability distributions. We developed LINFA (Library for Inference with Normalizing Flow and Annealing), a Python library for variational inference to accommodate computationally expensive models and difficult-to-sample distributions with dependent parameters. We discuss the theoretical background, capabilities, and performance of LINFA in various benchmarks. LINFA is publicly available on GitHub at https://github.com/desResLab/LINFA.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#29983;&#29289;&#36827;&#21270;&#20013;&#21457;&#23637;&#20986;&#30340;&#32908;&#32905;&#21327;&#21516;&#25511;&#21046;&#31574;&#30053;&#24212;&#29992;&#20110;&#20154;&#25163;&#21644;&#33151;&#27169;&#22411;&#65292;&#21457;&#29616;&#36890;&#36807;&#21327;&#21516;&#34892;&#21160;&#34920;&#31034;&#65288;SAR&#65289;&#35757;&#32451;&#30340;&#31574;&#30053;&#22312;&#23398;&#20064;&#22797;&#26434;&#20219;&#21153;&#26102;&#26126;&#26174;&#20248;&#20110;&#31471;&#21040;&#31471;&#24378;&#21270;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2307.03716</link><description>&lt;p&gt;
SAR: &#36890;&#36807;&#21327;&#21516;&#34892;&#21160;&#34920;&#31034;&#23454;&#29616;&#29983;&#29702;&#25935;&#25463;&#24615;&#21644;&#28789;&#24039;&#24615;&#30340;&#26222;&#36866;&#24615;
&lt;/p&gt;
&lt;p&gt;
SAR: Generalization of Physiological Agility and Dexterity via Synergistic Action Representation. (arXiv:2307.03716v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03716
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#29983;&#29289;&#36827;&#21270;&#20013;&#21457;&#23637;&#20986;&#30340;&#32908;&#32905;&#21327;&#21516;&#25511;&#21046;&#31574;&#30053;&#24212;&#29992;&#20110;&#20154;&#25163;&#21644;&#33151;&#27169;&#22411;&#65292;&#21457;&#29616;&#36890;&#36807;&#21327;&#21516;&#34892;&#21160;&#34920;&#31034;&#65288;SAR&#65289;&#35757;&#32451;&#30340;&#31574;&#30053;&#22312;&#23398;&#20064;&#22797;&#26434;&#20219;&#21153;&#26102;&#26126;&#26174;&#20248;&#20110;&#31471;&#21040;&#31471;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#32500;&#31995;&#32479;&#20013;&#23398;&#20064;&#39640;&#25928;&#36830;&#32493;&#25511;&#21046;&#31574;&#30053;&#65292;&#21253;&#25324;&#32908;&#32905;&#39592;&#39612;&#20195;&#29702;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#36890;&#36807;&#29983;&#29289;&#36827;&#21270;&#30340;&#36807;&#31243;&#20013;&#65292;&#29983;&#29289;&#20307;&#21457;&#23637;&#20986;&#20102;&#20811;&#26381;&#36825;&#31181;&#22797;&#26434;&#24615;&#30340;&#40065;&#26834;&#26426;&#21046;&#65292;&#23398;&#20064;&#20102;&#39640;&#24230;&#22797;&#26434;&#30340;&#36816;&#21160;&#25511;&#21046;&#31574;&#30053;&#12290;&#26159;&#20160;&#20040;&#23548;&#33268;&#20102;&#36825;&#31181;&#40065;&#26834;&#30340;&#34892;&#20026;&#28789;&#27963;&#24615;&#21602;&#65311;&#36890;&#36807;&#32908;&#32905;&#21327;&#21516;&#30340;&#27169;&#22359;&#21270;&#25511;&#21046;&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#28508;&#22312;&#30340;&#26426;&#21046;&#65292;&#20351;&#29983;&#29289;&#33021;&#22815;&#22312;&#31616;&#21270;&#21644;&#21487;&#25512;&#24191;&#30340;&#34892;&#21160;&#31354;&#38388;&#20013;&#23398;&#20064;&#32908;&#32905;&#25511;&#21046;&#12290;&#21463;&#21040;&#36825;&#31181;&#36827;&#21270;&#30340;&#36816;&#21160;&#25511;&#21046;&#31574;&#30053;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20351;&#29992;&#29983;&#29702;&#20934;&#30830;&#30340;&#20154;&#25163;&#21644;&#33151;&#27169;&#22411;&#20316;&#20026;&#27979;&#35797;&#24179;&#21488;&#65292;&#30830;&#23450;&#20174;&#31616;&#21333;&#20219;&#21153;&#20013;&#33719;&#24471;&#30340;&#21327;&#21516;&#34892;&#21160;&#34920;&#31034;&#65288;SAR&#65289;&#22312;&#23398;&#20064;&#26356;&#22797;&#26434;&#20219;&#21153;&#20013;&#30340;&#20316;&#29992;&#31243;&#24230;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#36825;&#20004;&#31181;&#24773;&#20917;&#19979;&#65292;&#21033;&#29992;SAR&#30340;&#31574;&#30053;&#26126;&#26174;&#20248;&#20110;&#31471;&#21040;&#31471;&#24378;&#21270;&#23398;&#20064;&#12290;&#36890;&#36807;SAR&#36827;&#34892;&#35757;&#32451;&#30340;&#31574;&#30053;&#20248;&#20110;&#31471;&#21040;&#31471;&#24378;&#21270;&#23398;&#20064;.
&lt;/p&gt;
&lt;p&gt;
Learning effective continuous control policies in high-dimensional systems, including musculoskeletal agents, remains a significant challenge. Over the course of biological evolution, organisms have developed robust mechanisms for overcoming this complexity to learn highly sophisticated strategies for motor control. What accounts for this robust behavioral flexibility? Modular control via muscle synergies, i.e. coordinated muscle co-contractions, is considered to be one putative mechanism that enables organisms to learn muscle control in a simplified and generalizable action space. Drawing inspiration from this evolved motor control strategy, we use physiologically accurate human hand and leg models as a testbed for determining the extent to which a Synergistic Action Representation (SAR) acquired from simpler tasks facilitates learning more complex tasks. We find in both cases that SAR-exploiting policies significantly outperform end-to-end reinforcement learning. Policies trained wit
&lt;/p&gt;</description></item><item><title>DEFT&#26159;&#19968;&#31181;&#21033;&#29992;&#27169;&#22411;&#23618;&#20043;&#38388;&#30340;&#26799;&#24230;&#33539;&#25968;&#24046;&#24322;&#26469;&#23454;&#29616;&#21487;&#25193;&#23637;&#30340;&#26799;&#24230;&#31232;&#30095;&#21270;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20943;&#23569;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#36890;&#20449;&#27969;&#37327;&#65292;&#24182;&#22312;&#35745;&#31639;&#25104;&#26412;&#21644;&#26799;&#24230;&#32047;&#31215;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2307.03500</link><description>&lt;p&gt;
DEFT:&#21033;&#29992;&#27169;&#22411;&#23618;&#20043;&#38388;&#30340;&#26799;&#24230;&#33539;&#25968;&#24046;&#24322;&#26469;&#23454;&#29616;&#21487;&#25193;&#23637;&#30340;&#26799;&#24230;&#31232;&#30095;&#21270;
&lt;/p&gt;
&lt;p&gt;
DEFT: Exploiting Gradient Norm Difference between Model Layers for Scalable Gradient Sparsification. (arXiv:2307.03500v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03500
&lt;/p&gt;
&lt;p&gt;
DEFT&#26159;&#19968;&#31181;&#21033;&#29992;&#27169;&#22411;&#23618;&#20043;&#38388;&#30340;&#26799;&#24230;&#33539;&#25968;&#24046;&#24322;&#26469;&#23454;&#29616;&#21487;&#25193;&#23637;&#30340;&#26799;&#24230;&#31232;&#30095;&#21270;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20943;&#23569;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#36890;&#20449;&#27969;&#37327;&#65292;&#24182;&#22312;&#35745;&#31639;&#25104;&#26412;&#21644;&#26799;&#24230;&#32047;&#31215;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26799;&#24230;&#31232;&#30095;&#21270;&#26159;&#20943;&#23569;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#20013;&#36807;&#22810;&#36890;&#20449;&#27969;&#37327;&#30340;&#24191;&#27867;&#24212;&#29992;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26799;&#24230;&#31232;&#30095;&#21270;&#26041;&#27861;&#30001;&#20110;&#26799;&#24230;&#36873;&#25321;&#30340;&#35745;&#31639;&#25104;&#26412;&#30456;&#24403;&#22823;&#21644;&#26799;&#24230;&#32047;&#31215;&#22686;&#21152;&#30340;&#36890;&#20449;&#27969;&#37327;&#65292;&#20854;&#21487;&#25193;&#23637;&#24615;&#30456;&#23545;&#36739;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26799;&#24230;&#31232;&#30095;&#21270;&#26041;&#26696;DEFT&#65292;&#23558;&#26799;&#24230;&#36873;&#25321;&#20219;&#21153;&#20998;&#35299;&#20026;&#23376;&#20219;&#21153;&#24182;&#20998;&#37197;&#32473;&#24037;&#20316;&#33410;&#28857;&#12290; DEFT&#19982;&#29616;&#26377;&#30340;&#31232;&#30095;&#21270;&#26041;&#27861;&#19981;&#21516;&#65292;&#27599;&#20010;&#24037;&#20316;&#33410;&#28857;&#20165;&#20174;&#25152;&#26377;&#26799;&#24230;&#20013;&#36873;&#25321;&#26799;&#24230;&#12290;&#22240;&#27492;&#65292;&#38543;&#30528;&#24037;&#20316;&#33410;&#28857;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#35745;&#31639;&#25104;&#26412;&#21487;&#20197;&#38477;&#20302;&#12290;&#27492;&#22806;&#65292;DEFT&#20801;&#35768;&#24037;&#20316;&#33410;&#28857;&#22312;&#38750;&#20132;&#21449;&#30340;&#20998;&#21306;&#20013;&#36873;&#25321;&#26799;&#24230;&#65292;&#22240;&#27492;&#21363;&#20351;&#24037;&#20316;&#33410;&#28857;&#25968;&#37327;&#22686;&#21152;&#65292;&#36890;&#20449;&#27969;&#37327;&#20063;&#21487;&#20197;&#26681;&#25454;&#29992;&#25143;&#35201;&#27714;&#36827;&#34892;&#32500;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gradient sparsification is a widely adopted solution for reducing the excessive communication traffic in distributed deep learning. However, most existing gradient sparsifiers have relatively poor scalability because of considerable computational cost of gradient selection and/or increased communication traffic owing to gradient build-up. To address these challenges, we propose a novel gradient sparsification scheme, DEFT, that partitions the gradient selection task into sub tasks and distributes them to workers. DEFT differs from existing sparsifiers, wherein every worker selects gradients among all gradients. Consequently, the computational cost can be reduced as the number of workers increases. Moreover, gradient build-up can be eliminated because DEFT allows workers to select gradients in partitions that are non-intersecting (between workers). Therefore, even if the number of workers increases, the communication traffic can be maintained as per user requirement.  To avoid the loss 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#20013;&#30340;&#24418;&#24335;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#21450;&#20854;&#36817;&#20284;&#26041;&#27861;&#12290;&#29616;&#26377;&#30340;&#29305;&#24449;&#36873;&#25321;&#21644;&#24402;&#22240;&#26041;&#27861;&#23384;&#22312;&#19968;&#20123;&#38382;&#39064;&#65292;&#32780;&#24418;&#24335;&#21270;&#30340;XAI&#26041;&#27861;&#34429;&#28982;&#26159;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#20173;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2307.03380</link><description>&lt;p&gt;
&#20851;&#20110;&#24418;&#24335;&#29305;&#24449;&#24402;&#22240;&#21450;&#20854;&#36817;&#20284;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
On Formal Feature Attribution and Its Approximation. (arXiv:2307.03380v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03380
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#20013;&#30340;&#24418;&#24335;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#21450;&#20854;&#36817;&#20284;&#26041;&#27861;&#12290;&#29616;&#26377;&#30340;&#29305;&#24449;&#36873;&#25321;&#21644;&#24402;&#22240;&#26041;&#27861;&#23384;&#22312;&#19968;&#20123;&#38382;&#39064;&#65292;&#32780;&#24418;&#24335;&#21270;&#30340;XAI&#26041;&#27861;&#34429;&#28982;&#26159;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#20173;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#31639;&#27861;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;ML&#27169;&#22411;&#33030;&#24369;&#24615;&#65292;&#20844;&#24179;&#24615;&#20197;&#21450;&#35299;&#37322;&#24615;&#30340;&#32570;&#20047;&#31561;&#37325;&#35201;&#38382;&#39064;&#38656;&#35201;&#31215;&#26497;&#21457;&#23637;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#21644;&#24418;&#24335;&#21270;&#30340;ML&#27169;&#22411;&#39564;&#35777;&#12290;XAI&#30340;&#20004;&#20010;&#20027;&#35201;&#30740;&#31350;&#26041;&#21521;&#21253;&#25324;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65288;&#20363;&#22914;&#65292;Anchors&#65289;&#21644;&#29305;&#24449;&#24402;&#22240;&#25216;&#26415;&#65288;&#20363;&#22914;&#65292;LIME&#21644;SHAP&#65289;&#12290;&#23613;&#31649;&#26377;&#24076;&#26395;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#29305;&#24449;&#36873;&#25321;&#21644;&#24402;&#22240;&#26041;&#27861;&#37117;&#23481;&#26131;&#20986;&#29616;&#19968;&#31995;&#21015;&#20851;&#38190;&#38382;&#39064;&#65292;&#21253;&#25324;&#35299;&#37322;&#19981;&#27491;&#30830;&#21644;&#36229;&#20986;&#20998;&#24067;&#37319;&#26679;&#12290;&#36817;&#26399;&#19968;&#31181;&#24418;&#24335;&#21270;&#30340;XAI&#26041;&#27861;&#65288;FXAI&#65289;&#34429;&#28982;&#20316;&#20026;&#20197;&#19978;&#26041;&#27861;&#30340;&#26367;&#20195;&#21697;&#24182;&#36991;&#20813;&#20102;&#36825;&#20123;&#38382;&#39064;&#65292;&#20294;&#20173;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#12290;&#20363;&#22914;&#65292;&#38500;&#20102;&#21487;&#25193;&#23637;&#24615;&#38480;&#21046;&#22806;&#65292;&#36825;&#31181;&#24418;&#24335;&#21270;&#26041;&#27861;&#26080;&#27861;&#35299;&#20915;&#29305;&#24449;&#24402;&#22240;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed the widespread use of artificial intelligence (AI) algorithms and machine learning (ML) models. Despite their tremendous success, a number of vital problems like ML model brittleness, their fairness, and the lack of interpretability warrant the need for the active developments in explainable artificial intelligence (XAI) and formal ML model verification. The two major lines of work in XAI include feature selection methods, e.g. Anchors, and feature attribution techniques, e.g. LIME and SHAP. Despite their promise, most of the existing feature selection and attribution approaches are susceptible to a range of critical issues, including explanation unsoundness and out-of-distribution sampling. A recent formal approach to XAI (FXAI) although serving as an alternative to the above and free of these issues suffers from a few other limitations. For instance and besides the scalability limitation, the formal approach is unable to tackle the feature attribution prob
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#21512;&#25104;ECG&#22270;&#20687;&#30340;&#24037;&#20855;&#31665;&#65292;&#26088;&#22312;&#20419;&#36827;&#25195;&#25551;ECG&#25968;&#23383;&#21270;&#12290;&#36890;&#36807;&#24341;&#20837;&#30495;&#23454;&#20266;&#24433;&#65292;&#22914;&#25163;&#20889;&#25991;&#26412;&#20266;&#24433;&#12289;&#30385;&#32441;&#12289;&#25240;&#30165;&#21644;&#35270;&#35282;&#21464;&#25442;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#26631;&#20934;&#32440;&#36136;ECG&#32972;&#26223;&#19978;&#29983;&#25104;&#20855;&#26377;&#30495;&#23454;&#24615;&#30340;ECG&#22270;&#20687;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#21161;&#20110;&#35299;&#20915;&#21512;&#25104;ECG&#22270;&#20687;&#20013;&#32570;&#20047;&#21442;&#32771;&#26102;&#38388;&#24207;&#21015;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.01946</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21512;&#25104;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#22270;&#20687;&#29983;&#25104;&#24037;&#20855;&#31665;&#65292;&#20197;&#20419;&#36827;&#25195;&#25551;ECG&#25968;&#23383;&#21270;
&lt;/p&gt;
&lt;p&gt;
A Synthetic Electrocardiogram (ECG) Image Generation Toolbox to Facilitate Deep Learning-Based Scanned ECG Digitization. (arXiv:2307.01946v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01946
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#21512;&#25104;ECG&#22270;&#20687;&#30340;&#24037;&#20855;&#31665;&#65292;&#26088;&#22312;&#20419;&#36827;&#25195;&#25551;ECG&#25968;&#23383;&#21270;&#12290;&#36890;&#36807;&#24341;&#20837;&#30495;&#23454;&#20266;&#24433;&#65292;&#22914;&#25163;&#20889;&#25991;&#26412;&#20266;&#24433;&#12289;&#30385;&#32441;&#12289;&#25240;&#30165;&#21644;&#35270;&#35282;&#21464;&#25442;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#26631;&#20934;&#32440;&#36136;ECG&#32972;&#26223;&#19978;&#29983;&#25104;&#20855;&#26377;&#30495;&#23454;&#24615;&#30340;ECG&#22270;&#20687;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#21161;&#20110;&#35299;&#20915;&#21512;&#25104;ECG&#22270;&#20687;&#20013;&#32570;&#20047;&#21442;&#32771;&#26102;&#38388;&#24207;&#21015;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#26159;&#19968;&#31181;&#20934;&#30830;&#19988;&#24191;&#27867;&#24212;&#29992;&#20110;&#35786;&#26029;&#24515;&#34880;&#31649;&#30142;&#30149;&#30340;&#24037;&#20855;&#12290;&#20960;&#21313;&#24180;&#26469;&#65292;ECG&#20197;&#21360;&#21047;&#26684;&#24335;&#35760;&#24405;&#65292;&#24182;&#19988;&#23558;&#23427;&#20204;&#30340;&#25968;&#23383;&#21270;&#22312;&#31639;&#27861;&#24615;&#24515;&#30005;&#22270;&#35786;&#26029;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#20013;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#29289;&#29702;&#24615;ECG&#23384;&#26723;&#38754;&#20020;&#36864;&#21270;&#39118;&#38505;&#65292;&#20165;&#25195;&#25551;&#21360;&#21047;ECG&#26159;&#19981;&#22815;&#30340;&#65292;&#22240;&#20026;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;ECG&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;&#23558;&#32440;&#36136;ECG&#23384;&#26723;&#25968;&#23383;&#21270;&#21644;&#36716;&#25442;&#20026;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#33267;&#20851;&#37325;&#35201;&#12290;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#22270;&#20687;&#22788;&#29702;&#26041;&#38754;&#26174;&#31034;&#20986;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#20855;&#26377;&#21442;&#32771;&#26102;&#38388;&#24207;&#21015;&#30340;ECG&#23384;&#26723;&#31232;&#32570;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#21033;&#29992;&#8220;&#25968;&#23383;&#23402;&#29983;&#8221;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#21487;&#33021;&#26159;&#19968;&#20010;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20197;&#29983;&#25104;&#20855;&#26377;&#30495;&#23454;&#20266;&#24433;&#30340;&#26631;&#20934;&#32440;&#36136;ECG&#32972;&#26223;&#19979;&#30340;&#21512;&#25104;ECG&#22270;&#20687;&#12290;&#21253;&#25324;&#25163;&#20889;&#25991;&#26412;&#20266;&#24433;&#12289;&#30385;&#32441;&#12289;&#25240;&#30165;&#21644;&#35270;&#35282;&#36716;&#25442;&#31561;&#30072;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;
The electrocardiogram (ECG) is an accurate and widely available tool for diagnosing cardiovascular diseases. ECGs have been recorded in printed formats for decades and their digitization holds great potential for training machine learning (ML) models in algorithmic ECG diagnosis. Physical ECG archives are at risk of deterioration and scanning printed ECGs alone is insufficient, as ML models require ECG time-series data. Therefore, the digitization and conversion of paper ECG archives into time-series data is of utmost importance. Deep learning models for image processing show promise in this regard. However, the scarcity of ECG archives with reference time-series is a challenge. Data augmentation techniques utilizing \textit{digital twins} present a potential solution.  We introduce a novel method for generating synthetic ECG images on standard paper-like ECG backgrounds with realistic artifacts. Distortions including handwritten text artifacts, wrinkles, creases and perspective transf
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#36741;&#21161;&#30340;&#27010;&#29575;&#23433;&#20840;&#33258;&#36866;&#24212;&#25511;&#21046;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#20803;&#23398;&#20064;&#12289;&#36125;&#21494;&#26031;&#27169;&#22411;&#21644;&#25511;&#21046;&#38480;&#21046;&#20989;&#25968;&#26041;&#27861;&#65292;&#21033;&#29992;&#20808;&#21069;&#20219;&#21153;&#25910;&#38598;&#30340;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#24341;&#20837;&#24754;&#35266;&#32622;&#20449;&#21306;&#38388;&#26469;&#30830;&#20445;&#23433;&#20840;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2307.00828</link><description>&lt;p&gt;
&#27169;&#22411;&#36741;&#21161;&#30340;&#27010;&#29575;&#23433;&#20840;&#33258;&#36866;&#24212;&#25511;&#21046;&#19982;&#20803;&#36125;&#21494;&#26031;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Model-Assisted Probabilistic Safe Adaptive Control With Meta-Bayesian Learning. (arXiv:2307.00828v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00828
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#36741;&#21161;&#30340;&#27010;&#29575;&#23433;&#20840;&#33258;&#36866;&#24212;&#25511;&#21046;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#20803;&#23398;&#20064;&#12289;&#36125;&#21494;&#26031;&#27169;&#22411;&#21644;&#25511;&#21046;&#38480;&#21046;&#20989;&#25968;&#26041;&#27861;&#65292;&#21033;&#29992;&#20808;&#21069;&#20219;&#21153;&#25910;&#38598;&#30340;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#24341;&#20837;&#24754;&#35266;&#32622;&#20449;&#21306;&#38388;&#26469;&#30830;&#20445;&#23433;&#20840;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25511;&#21046;&#31995;&#32479;&#20013;&#25171;&#30772;&#23433;&#20840;&#32422;&#26463;&#21487;&#33021;&#23548;&#33268;&#28508;&#22312;&#39118;&#38505;&#65292;&#36896;&#25104;&#24847;&#22806;&#30340;&#25104;&#26412;&#25110;&#28798;&#38590;&#24615;&#25439;&#23475;&#12290;&#28982;&#32780;&#65292;&#19981;&#30830;&#23450;&#24615;&#26222;&#36941;&#23384;&#22312;&#65292;&#21363;&#20351;&#26159;&#30456;&#20284;&#30340;&#20219;&#21153;&#20063;&#26159;&#22914;&#27492;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#23433;&#20840;&#25511;&#21046;&#26694;&#26550;&#65292;&#23427;&#38598;&#25104;&#20102;&#20803;&#23398;&#20064;&#12289;&#36125;&#21494;&#26031;&#27169;&#22411;&#21644;&#25511;&#21046;&#38480;&#21046;&#20989;&#25968;&#65288;CBF&#65289;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20511;&#21161;CBF&#26041;&#27861;&#65292;&#25105;&#20204;&#36890;&#36807;&#32479;&#19968;&#30340;&#33258;&#36866;&#24212;&#36125;&#21494;&#26031;&#32447;&#24615;&#22238;&#24402;&#65288;ABLR&#65289;&#27169;&#22411;&#23398;&#20064;&#20869;&#22312;&#21644;&#22806;&#37096;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#35813;&#27169;&#22411;&#30001;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#21644;&#36125;&#21494;&#26031;&#36755;&#20986;&#23618;&#32452;&#25104;&#12290;&#21033;&#29992;&#20803;&#23398;&#20064;&#25216;&#26415;&#65292;&#25105;&#20204;&#20351;&#29992;&#20174;&#21382;&#21490;&#30456;&#20284;&#20219;&#21153;&#25910;&#38598;&#30340;&#25968;&#25454;&#23545;NN&#26435;&#37325;&#21644;ABLR&#27169;&#22411;&#30340;&#20808;&#39564;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#23545;&#20110;&#19968;&#20010;&#26032;&#30340;&#25511;&#21046;&#20219;&#21153;&#65292;&#25105;&#20204;&#36890;&#36807;&#23569;&#37327;&#26679;&#26412;&#26469;&#25913;&#36827;&#20803;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#23558;&#24754;&#35266;&#32622;&#20449;&#21306;&#38388;&#24341;&#20837;CBF&#32422;&#26463;&#65292;&#20197;&#30830;&#20445;&#23433;&#20840;&#25511;&#21046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#20934;&#21017;&#65292;&#20197;&#30830;&#20445;&#22312;&#25511;&#21046;&#36807;&#31243;&#20013;&#30340;&#27010;&#29575;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Breaking safety constraints in control systems can lead to potential risks, resulting in unexpected costs or catastrophic damage. Nevertheless, uncertainty is ubiquitous, even among similar tasks. In this paper, we develop a novel adaptive safe control framework that integrates meta learning, Bayesian models, and control barrier function (CBF) method. Specifically, with the help of CBF method, we learn the inherent and external uncertainties by a unified adaptive Bayesian linear regression (ABLR) model, which consists of a forward neural network (NN) and a Bayesian output layer. Meta learning techniques are leveraged to pre-train the NN weights and priors of the ABLR model using data collected from historical similar tasks. For a new control task, we refine the meta-learned models using a few samples, and introduce pessimistic confidence bounds into CBF constraints to ensure safe control. Moreover, we provide theoretical criteria to guarantee probabilistic safety during the control pro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23569;&#26679;&#26412;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#21517;&#20026;FLat-tO-WidE AppRoach (FLOWER)&#65292;&#36890;&#36807;&#23547;&#25214;&#25153;&#24179;&#21270;&#23485;&#21270;&#26497;&#23567;&#20540;&#30340;&#36807;&#31243;&#26469;&#35299;&#20915;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#21033;&#29992;&#29699;&#20307;&#29983;&#25104;&#22120;&#27010;&#24565;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#26469;&#20811;&#26381;&#25968;&#25454;&#31232;&#32570;&#24615;&#38382;&#39064;&#12290;&#22312;&#23567;&#22411;&#22522;&#30784;&#20219;&#21153;&#20013;&#65292;FLOWER&#34920;&#29616;&#20986;&#26174;&#33879;&#25913;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.14369</link><description>&lt;p&gt;
&#21033;&#29992;&#25153;&#24179;&#21270;&#21040;&#23485;&#21270;&#26041;&#27861;&#36827;&#34892;&#23569;&#26679;&#26412;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Few-Shot Continual Learning via Flat-to-Wide Approaches. (arXiv:2306.14369v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14369
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23569;&#26679;&#26412;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#21517;&#20026;FLat-tO-WidE AppRoach (FLOWER)&#65292;&#36890;&#36807;&#23547;&#25214;&#25153;&#24179;&#21270;&#23485;&#21270;&#26497;&#23567;&#20540;&#30340;&#36807;&#31243;&#26469;&#35299;&#20915;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#21033;&#29992;&#29699;&#20307;&#29983;&#25104;&#22120;&#27010;&#24565;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#26469;&#20811;&#26381;&#25968;&#25454;&#31232;&#32570;&#24615;&#38382;&#39064;&#12290;&#22312;&#23567;&#22411;&#22522;&#30784;&#20219;&#21153;&#20013;&#65292;FLOWER&#34920;&#29616;&#20986;&#26174;&#33879;&#25913;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#38656;&#35201;&#22823;&#37327;&#30340;&#26679;&#26412;&#12290;&#30001;&#20110;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#25317;&#26377;&#26377;&#38480;&#26679;&#26412;&#30340;&#35768;&#22810;&#23454;&#38469;&#38382;&#39064;&#20013;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23569;&#26679;&#26412;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;FLat-tO-WidE AppRoach (FLOWER)&#65292;&#20854;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#23547;&#25214;&#25153;&#24179;&#21270;&#23485;&#21270;&#26497;&#23567;&#20540;&#30340;&#25153;&#24179;&#21270;&#21040;&#23485;&#21270;&#23398;&#20064;&#36807;&#31243;&#65292;&#20197;&#35299;&#20915;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#29699;&#20307;&#29983;&#25104;&#22120;&#27010;&#24565;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#26469;&#20811;&#26381;&#25968;&#25454;&#31232;&#32570;&#24615;&#38382;&#39064;&#65292;&#23558;&#37319;&#26679;&#31354;&#38388;&#38480;&#21046;&#22312;&#26368;&#23567;&#22806;&#25509;&#29699;&#20869;&#12290;&#25105;&#20204;&#30340;&#25968;&#20540;&#30740;&#31350;&#34920;&#26126;&#65292;FLOWER&#30456;&#27604;&#20808;&#21069;&#30340;&#26041;&#27861;&#22312;&#23567;&#22411;&#22522;&#30784;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26174;&#33879;&#25913;&#36827;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#65292;FLOWER&#30340;&#28304;&#20195;&#30721;&#65292;&#31454;&#20105;&#31639;&#27861;&#21644;&#23454;&#39564;&#26085;&#24535;&#20844;&#24320;&#20849;&#20139;&#22312;\url{https://github.com/anwarmaxsum/FLOWER}&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing approaches on continual learning call for a lot of samples in their training processes. Such approaches are impractical for many real-world problems having limited samples because of the overfitting problem. This paper proposes a few-shot continual learning approach, termed FLat-tO-WidE AppRoach (FLOWER), where a flat-to-wide learning process finding the flat-wide minima is proposed to address the catastrophic forgetting problem. The issue of data scarcity is overcome with a data augmentation approach making use of a ball generator concept to restrict the sampling space into the smallest enclosing ball. Our numerical studies demonstrate the advantage of FLOWER achieving significantly improved performances over prior arts notably in the small base tasks. For further study, source codes of FLOWER, competitor algorithms and experimental logs are shared publicly in \url{https://github.com/anwarmaxsum/FLOWER}.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35760;&#24405;&#20102;&#19968;&#27425;&#40657;&#23458;&#26494;&#27963;&#21160;&#65292;&#21442;&#19982;&#32773;&#20351;&#29992;LLMs&#36827;&#34892;&#20102;&#21508;&#31181;&#24212;&#29992;&#65292;&#21253;&#25324;&#39044;&#27979;&#20998;&#23376;&#21644;&#26448;&#26009;&#29305;&#24615;&#12289;&#20174;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#25552;&#21462;&#30693;&#35782;&#12289;&#20026;&#24037;&#20855;&#35774;&#35745;&#26032;&#30028;&#38754;&#20197;&#21450;&#24320;&#21457;&#26032;&#30340;&#25945;&#32946;&#24212;&#29992;&#12290;&#36825;&#20123;&#22810;&#26679;&#21270;&#30340;&#39033;&#30446;&#21453;&#26144;&#20102;LLMs&#22312;&#26448;&#26009;&#31185;&#23398;&#21644;&#21270;&#23398;&#39046;&#22495;&#30340;&#22810;&#21151;&#33021;&#24615;&#21644;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.06283</link><description>&lt;p&gt;
LLMs&#22914;&#20309;&#25913;&#21464;&#26448;&#26009;&#31185;&#23398;&#21644;&#21270;&#23398;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#40657;&#23458;&#39532;&#25289;&#26494;&#30340;&#21453;&#24605;
&lt;/p&gt;
&lt;p&gt;
14 Examples of How LLMs Can Transform Materials Science and Chemistry: A Reflection on a Large Language Model Hackathon. (arXiv:2306.06283v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06283
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35760;&#24405;&#20102;&#19968;&#27425;&#40657;&#23458;&#26494;&#27963;&#21160;&#65292;&#21442;&#19982;&#32773;&#20351;&#29992;LLMs&#36827;&#34892;&#20102;&#21508;&#31181;&#24212;&#29992;&#65292;&#21253;&#25324;&#39044;&#27979;&#20998;&#23376;&#21644;&#26448;&#26009;&#29305;&#24615;&#12289;&#20174;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#25552;&#21462;&#30693;&#35782;&#12289;&#20026;&#24037;&#20855;&#35774;&#35745;&#26032;&#30028;&#38754;&#20197;&#21450;&#24320;&#21457;&#26032;&#30340;&#25945;&#32946;&#24212;&#29992;&#12290;&#36825;&#20123;&#22810;&#26679;&#21270;&#30340;&#39033;&#30446;&#21453;&#26144;&#20102;LLMs&#22312;&#26448;&#26009;&#31185;&#23398;&#21644;&#21270;&#23398;&#39046;&#22495;&#30340;&#22810;&#21151;&#33021;&#24615;&#21644;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21270;&#23398;&#21644;&#26448;&#26009;&#31185;&#23398;&#38750;&#24120;&#22797;&#26434;&#12290;&#26368;&#36817;&#65292;&#20351;&#29992;&#25968;&#25454;&#39537;&#21160;&#25110;&#35745;&#31639;&#25216;&#26415;&#35299;&#20915;&#20102;&#36825;&#31181;&#22797;&#26434;&#24615;&#30340;&#20013;&#26377;&#24456;&#22823;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#36755;&#20837;&#38656;&#35201;&#38750;&#24120;&#29305;&#23450;&#24418;&#24335;&#30340;&#32467;&#26500;&#20197;&#21450;&#24037;&#20855;&#25968;&#37327;&#19981;&#26029;&#22686;&#38271;&#25152;&#24102;&#26469;&#21487;&#29992;&#24615;&#21644;&#21487;&#35775;&#38382;&#24615;&#30340;&#25361;&#25112;&#12290;&#21152;&#19978;&#36825;&#20123;&#23398;&#31185;&#20013;&#30340;&#22823;&#22810;&#25968;&#25968;&#25454;&#37117;&#26159;&#38750;&#32467;&#26500;&#21270;&#30340;&#20107;&#23454;&#65292;&#20351;&#24471;&#36825;&#20123;&#24037;&#20855;&#30340;&#25928;&#29575;&#21463;&#21040;&#38480;&#21046;&#12290;&#26412;&#25991;&#35760;&#24405;&#20102;&#20851;&#20110;LLMs&#30340;&#40657;&#23458;&#26494;&#27963;&#21160;&#20013;&#26500;&#24314;&#30340;&#39033;&#30446;&#12290;&#21442;&#19982;&#32773;&#20351;&#29992;LLMs&#36827;&#34892;&#20102;&#21508;&#31181;&#24212;&#29992;&#65292;&#21253;&#25324;&#39044;&#27979;&#20998;&#23376;&#21644;&#26448;&#26009;&#30340;&#29305;&#24615;&#12289;&#20026;&#24037;&#20855;&#35774;&#35745;&#26032;&#30028;&#38754;&#12289;&#20174;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#25552;&#21462;&#30693;&#35782;&#20197;&#21450;&#24320;&#21457;&#26032;&#30340;&#25945;&#32946;&#24212;&#29992;&#12290;&#21508;&#31181;&#21508;&#26679;&#30340;&#39033;&#30446;&#21453;&#26144;&#20102;LLMs&#22312;&#36825;&#20123;&#39046;&#22495;&#30340;&#22810;&#21151;&#33021;&#24615;&#21644;&#36825;&#20123;&#27169;&#22411;&#25913;&#21464;&#26448;&#26009;&#31185;&#23398;&#21644;&#21270;&#23398;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chemistry and materials science are complex. Recently, there have been great successes in addressing this complexity using data-driven or computational techniques. Yet, the necessity of input structured in very specific forms and the fact that there is an ever-growing number of tools creates usability and accessibility challenges. Coupled with the reality that much data in these disciplines is unstructured, the effectiveness of these tools is limited.  Motivated by recent works that indicated that large language models (LLMs) might help address some of these issues, we organized a hackathon event on the applications of LLMs in chemistry, materials science, and beyond. This article chronicles the projects built as part of this hackathon. Participants employed LLMs for various applications, including predicting properties of molecules and materials, designing novel interfaces for tools, extracting knowledge from unstructured data, and developing new educational applications.  The diverse
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#20195;&#29702;&#20998;&#31867;&#25439;&#22833;&#30340;&#20551;&#35774;&#36801;&#31227;&#23398;&#20064;&#30340;&#23398;&#20064;&#29702;&#35770;&#65292;&#36890;&#36807;&#31639;&#27861;&#31283;&#23450;&#24615;&#25552;&#20379;&#20102;&#22312;&#28201;&#21644;&#20551;&#35774;&#19979;&#30340;&#23398;&#20064;&#20445;&#35777;&#65292;&#36866;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.19694</link><description>&lt;p&gt;
&#21033;&#29992;&#20195;&#29702;&#20998;&#31867;&#25439;&#22833;&#30340;&#20551;&#35774;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Hypothesis Transfer Learning with Surrogate Classification Losses. (arXiv:2305.19694v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19694
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#20195;&#29702;&#20998;&#31867;&#25439;&#22833;&#30340;&#20551;&#35774;&#36801;&#31227;&#23398;&#20064;&#30340;&#23398;&#20064;&#29702;&#35770;&#65292;&#36890;&#36807;&#31639;&#27861;&#31283;&#23450;&#24615;&#25552;&#20379;&#20102;&#22312;&#28201;&#21644;&#20551;&#35774;&#19979;&#30340;&#23398;&#20064;&#20445;&#35777;&#65292;&#36866;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20551;&#35774;&#36801;&#31227;&#23398;&#20064;&#65288;HTL&#65289;&#36890;&#36807;&#20801;&#35768;&#20808;&#21069;&#20219;&#21153;&#65288;&#21363;&#28304;&#20219;&#21153;&#65289;&#21521;&#19968;&#20010;&#26032;&#20219;&#21153;&#65288;&#30446;&#26631;&#20219;&#21153;&#65289;&#36716;&#31227;&#23398;&#20064;&#65292;&#32780;&#26080;&#38656;&#35775;&#38382;&#28304;&#25968;&#25454;&#65292;&#19982;&#39046;&#22495;&#33258;&#36866;&#24212;&#30456;&#23545;&#24212;&#12290;&#20107;&#23454;&#19978;&#65292;HTL&#20165;&#20381;&#36182;&#20110;&#20174;&#28304;&#25968;&#25454;&#23398;&#20064;&#21040;&#30340;&#20551;&#35774;&#65292;&#20813;&#38500;&#20102;&#22823;&#37327;&#25968;&#25454;&#23384;&#20648;&#30340;&#38556;&#30861;&#65292;&#24182;&#25552;&#20379;&#20102;&#24040;&#22823;&#30340;&#23454;&#38469;&#21033;&#30410;&#12290;&#22240;&#27492;&#65292;HTL&#23545;&#20110;&#20381;&#36182;&#20110;&#22823;&#25968;&#25454;&#30340;&#23454;&#38469;&#24212;&#29992;&#38750;&#24120;&#26377;&#21033;&#12290;&#26412;&#25991;&#36890;&#36807;&#31639;&#27861;&#31283;&#23450;&#24615;&#30740;&#31350;HTL&#30340;&#23398;&#20064;&#29702;&#35770;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#20998;&#26512;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#26377;&#21560;&#24341;&#21147;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#29305;&#21035;&#26159;&#22312;&#20108;&#20998;&#31867;&#24773;&#20917;&#19979;&#24863;&#20852;&#36259;&#12290;&#25105;&#20204;&#30340;&#31283;&#23450;&#24615;&#20998;&#26512;&#25552;&#20379;&#20102;&#22312;&#28201;&#21644;&#20551;&#35774;&#19979;&#30340;&#23398;&#20064;&#20445;&#35777;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#20960;&#20010;&#27604;&#20197;&#21069;&#26356;&#32039;&#23494;&#30340;&#29702;&#35770;&#30028;&#38480;&#65292;&#36825;&#20123;&#30028;&#38480;&#21487;&#20197;&#23454;&#38469;&#24212;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hypothesis transfer learning (HTL) contrasts domain adaptation by allowing for a previous task leverage, named the source, into a new one, the target, without requiring access to the source data. Indeed, HTL relies only on a hypothesis learnt from such source data, relieving the hurdle of expansive data storage and providing great practical benefits. Hence, HTL is highly beneficial for real-world applications relying on big data. The analysis of such a method from a theoretical perspective faces multiple challenges, particularly in classification tasks. This paper deals with this problem by studying the learning theory of HTL through algorithmic stability, an attractive theoretical framework for machine learning algorithms analysis. In particular, we are interested in the statistical behaviour of the regularized empirical risk minimizers in the case of binary classification. Our stability analysis provides learning guarantees under mild assumptions. Consequently, we derive several comp
&lt;/p&gt;</description></item><item><title>Dink-Net&#26159;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#22823;&#35268;&#27169;&#22270;&#24418;&#31070;&#32463;&#32858;&#31867;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#33192;&#32960;&#21644;&#25910;&#32553;&#30340;&#24605;&#24819;&#26469;&#22788;&#29702;&#30334;&#19975;&#33410;&#28857;&#30340;&#22823;&#22270;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.18405</link><description>&lt;p&gt;
Dink-Net: &#22823;&#35268;&#27169;&#22270;&#24418;&#31070;&#32463;&#32858;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Dink-Net: Neural Clustering on Large Graphs. (arXiv:2305.18405v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18405
&lt;/p&gt;
&lt;p&gt;
Dink-Net&#26159;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#22823;&#35268;&#27169;&#22270;&#24418;&#31070;&#32463;&#32858;&#31867;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#33192;&#32960;&#21644;&#25910;&#32553;&#30340;&#24605;&#24819;&#26469;&#22788;&#29702;&#30334;&#19975;&#33410;&#28857;&#30340;&#22823;&#22270;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#22270;&#32858;&#31867;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23558;&#22270;&#24418;&#30340;&#33410;&#28857;&#36827;&#34892;&#20998;&#32452;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#30334;&#19975;&#33410;&#28857;&#30340;&#22823;&#22270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;Dink-Net&#28145;&#24230;&#22270;&#32858;&#31867;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#33192;&#32960;&#21644;&#25910;&#32553;&#30340;&#24605;&#24819;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#21306;&#20998;&#24102;&#22686;&#24378;&#30340;&#36319;&#19981;&#24102;&#22686;&#24378;&#30340;&#33410;&#28857;&#65292;&#33258;&#25105;&#30417;&#30563;&#26041;&#24335;&#23398;&#20064;&#34920;&#31034;&#24418;&#24335;&#12290;&#21516;&#26102;&#65292;&#23558;&#32858;&#31867;&#20013;&#24515;&#21021;&#22987;&#21270;&#20026;&#21487;&#23398;&#20064;&#30340;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#12290;&#38543;&#21518;&#65292;&#36890;&#36807;&#23545;&#25239;&#24615;&#26041;&#24335;&#26368;&#23567;&#21270;&#25552;&#20986;&#30340;&#38598;&#32676;&#33192;&#32960;&#25439;&#22833;&#21644;&#38598;&#32676;&#25910;&#32553;&#25439;&#22833;&#65292;&#20248;&#21270;&#32858;&#31867;&#20998;&#24067;&#12290;&#36890;&#36807;&#36825;&#20123;&#35774;&#32622;&#65292;&#25105;&#20204;&#23558;&#34920;&#31034;&#23398;&#20064;&#21644;&#32858;&#31867;&#20248;&#21270;&#20004;&#20010;&#27493;&#39588;&#32479;&#19968;&#20026;&#19968;&#20010;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#24341;&#23548;&#32593;&#32476;&#23398;&#20064;&#32858;&#31867;&#21451;&#22909;&#30340;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;Dink-Net&#33021;&#24456;&#22909;&#22320;&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#30340;&#22270;&#24418;&#19978;&#65292;&#22240;&#20026;&#35774;&#35745;&#30340;&#33192;&#32960;&#25910;&#32553;&#25805;&#20316;&#21487;&#20197;&#26377;&#25928;&#22320;&#20943;&#23569;&#35745;&#31639;&#21644;&#20869;&#23384;&#28040;&#32791;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Dink-Net&#22312;&#22788;&#29702;&#30334;&#19975;&#33410;&#28857;&#22270;&#24418;&#30340;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#22823;&#22270;&#32858;&#31867;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep graph clustering, which aims to group the nodes of a graph into disjoint clusters with deep neural networks, has achieved promising progress in recent years. However, the existing methods fail to scale to the large graph with million nodes. To solve this problem, a scalable deep graph clustering method (Dink-Net) is proposed with the idea of dilation and shrink. Firstly, by discriminating nodes, whether being corrupted by augmentations, representations are learned in a self-supervised manner. Meanwhile, the cluster centres are initialized as learnable neural parameters. Subsequently, the clustering distribution is optimized by minimizing the proposed cluster dilation loss and cluster shrink loss in an adversarial manner. By these settings, we unify the two-step clustering, i.e., representation learning and clustering optimization, into an end-to-end framework, guiding the network to learn clustering-friendly features. Besides, Dink-Net scales well to large graphs since the designe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#26234;&#33021;&#20013;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#27880;&#24847;&#21147;&#27169;&#24335;&#29702;&#35770;&#65288;AST&#65289;&#12290;&#20316;&#32773;&#21457;&#29616;&#23558;AS&#23454;&#29616;&#20026;&#19968;&#31181;&#24490;&#29615;&#20869;&#37096;&#25511;&#21046;&#30340;&#26234;&#33021;&#20307;&#25928;&#26524;&#26368;&#20339;&#65292;&#36825;&#19968;&#29702;&#35770;&#20026;&#24212;&#29992;&#19982;&#25913;&#36827;&#31070;&#32463;&#26234;&#33021;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;</title><link>http://arxiv.org/abs/2305.17375</link><description>&lt;p&gt;
&#31070;&#32463;&#26234;&#33021;&#20013;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Attention Schema in Neural Agents. (arXiv:2305.17375v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17375
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#26234;&#33021;&#20013;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#27880;&#24847;&#21147;&#27169;&#24335;&#29702;&#35770;&#65288;AST&#65289;&#12290;&#20316;&#32773;&#21457;&#29616;&#23558;AS&#23454;&#29616;&#20026;&#19968;&#31181;&#24490;&#29615;&#20869;&#37096;&#25511;&#21046;&#30340;&#26234;&#33021;&#20307;&#25928;&#26524;&#26368;&#20339;&#65292;&#36825;&#19968;&#29702;&#35770;&#20026;&#24212;&#29992;&#19982;&#25913;&#36827;&#31070;&#32463;&#26234;&#33021;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27880;&#24847;&#21147;&#24050;&#32463;&#25104;&#20026;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#20013;&#30340;&#24120;&#35265;&#35201;&#32032;&#12290;&#23427;&#36890;&#36807;&#21152;&#20837;&#21160;&#24577;&#20449;&#24687;&#36873;&#25321;&#65292;&#25903;&#25345;&#38745;&#24577;&#30340;&#26435;&#37325;&#36873;&#25321;&#12290;&#21516;&#26679;&#22320;&#65292;&#25105;&#20204;&#21487;&#20197;&#24819;&#35937;&#22312;&#27880;&#24847;&#21147;&#20043;&#19978;&#26500;&#24314;&#19968;&#20010;&#26356;&#39640;&#38454;&#30340;&#20449;&#24687;&#36807;&#28388;&#22120;&#65306;&#27880;&#24847;&#21147;&#27169;&#24335;&#65288;AS&#65289;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#19968;&#20010;&#25551;&#36848;&#24615;&#21644;&#39044;&#27979;&#24615;&#30340;&#27880;&#24847;&#21147;&#27169;&#22411;&#12290;&#22312;&#35748;&#30693;&#31070;&#32463;&#31185;&#23398;&#20013;&#65292;&#27880;&#24847;&#21147;&#27169;&#24335;&#29702;&#35770;&#65288;AST&#65289;&#25903;&#25345;&#36825;&#31181;&#21306;&#20998;&#27880;&#24847;&#21147;&#21644;AS&#30340;&#24819;&#27861;&#12290;&#35813;&#29702;&#35770;&#30340;&#19968;&#20010;&#37325;&#35201;&#39044;&#27979;&#26159;&#65292;&#19968;&#20010;&#26234;&#33021;&#20307;&#21487;&#20197;&#20351;&#29992;&#33258;&#24049;&#30340;AS&#26469;&#25512;&#26029;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#27880;&#24847;&#29366;&#24577;&#65292;&#20174;&#32780;&#22686;&#24378;&#19982;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#21327;&#35843;&#12290;&#22240;&#27492;&#65292;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#20010;&#23454;&#39564;&#27979;&#35797;AST&#26377;&#25928;&#24615;&#30340;&#29702;&#24819;&#22330;&#26223;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#27880;&#24847;&#21147;&#21644;AS&#30456;&#20114;&#20316;&#29992;&#30340;&#19981;&#21516;&#26041;&#24335;&#65292;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;AS&#23454;&#29616;&#20026;&#19968;&#31181;&#24490;&#29615;&#20869;&#37096;&#25511;&#21046;&#30340;&#26234;&#33021;&#20307;&#25928;&#26524;&#26368;&#20339;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#36825;&#20123;&#23454;&#39564;&#20026;&#29702;&#35299;&#20197;&#21450;&#25913;&#36827;&#31070;&#32463;&#26234;&#33021;&#20013;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention has become a common ingredient in deep learning architectures. It adds a dynamical selection of information on top of the static selection of information supported by weights. In the same way, we can imagine a higher-order informational filter built on top of attention: an Attention Schema (AS), namely, a descriptive and predictive model of attention. In cognitive neuroscience, Attention Schema Theory (AST) supports this idea of distinguishing attention from AS. A strong prediction of this theory is that an agent can use its own AS to also infer the states of other agents' attention and consequently enhance coordination with other agents. As such, multi-agent reinforcement learning would be an ideal setting to experimentally test the validity of AST. We explore different ways in which attention and AS interact with each other. Our preliminary results indicate that agents that implement the AS as a recurrent internal control achieve the best performance. In general, these expl
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#21487;&#35299;&#37322;&#20851;&#31995;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#31070;&#32463;&#21644;&#31526;&#21495;&#19990;&#30028;&#30456;&#32467;&#21512;&#65292;&#21033;&#29992;&#31070;&#32463;&#31526;&#21495;&#21270;&#30340;&#26041;&#27861;&#25552;&#21462;&#20986;&#21487;&#35299;&#37322;&#30340;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#19981;&#21487;&#35299;&#37322;&#24615;&#21644;&#22788;&#29702;&#29615;&#22659;&#32467;&#26500;&#21464;&#21270;&#30340;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2304.08349</link><description>&lt;p&gt;
&#28145;&#24230;&#21487;&#35299;&#37322;&#20851;&#31995;&#24378;&#21270;&#23398;&#20064;&#65306;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Deep Explainable Relational Reinforcement Learning: A Neuro-Symbolic Approach. (arXiv:2304.08349v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08349
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#21487;&#35299;&#37322;&#20851;&#31995;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#31070;&#32463;&#21644;&#31526;&#21495;&#19990;&#30028;&#30456;&#32467;&#21512;&#65292;&#21033;&#29992;&#31070;&#32463;&#31526;&#21495;&#21270;&#30340;&#26041;&#27861;&#25552;&#21462;&#20986;&#21487;&#35299;&#37322;&#30340;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#19981;&#21487;&#35299;&#37322;&#24615;&#21644;&#22788;&#29702;&#29615;&#22659;&#32467;&#26500;&#21464;&#21270;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#21462;&#24471;&#20102;&#35768;&#22810;&#25104;&#21151;&#65292;&#23398;&#20064;&#21040;&#30340;&#31574;&#30053;&#20173;&#28982;&#26159;&#19981;&#21487;&#35299;&#37322;&#30340;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;DRL&#27809;&#26377;&#21033;&#29992;&#31526;&#21495;&#20851;&#31995;&#34920;&#31034;&#65292;&#23427;&#22312;&#22788;&#29702;&#29615;&#22659;&#32467;&#26500;&#21464;&#21270;&#65288;&#22914;&#23545;&#35937;&#25968;&#37327;&#22686;&#21152;&#65289;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20851;&#31995;&#24378;&#21270;&#23398;&#20064;&#20174;&#31526;&#21495;&#35745;&#21010;&#32487;&#25215;&#20102;&#20851;&#31995;&#34920;&#31034;&#65292;&#20197;&#23398;&#20064;&#21487;&#37325;&#29992;&#30340;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#20851;&#31995;&#24378;&#21270;&#23398;&#20064;&#26080;&#27861;&#25193;&#23637;&#24182;&#20805;&#20998;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#21487;&#35299;&#37322;&#20851;&#31995;&#24378;&#21270;&#23398;&#20064;&#65288;DERRL&#65289;&#26694;&#26550;&#65292;&#23427;&#32467;&#21512;&#20102;&#31070;&#32463;&#21644;&#31526;&#21495;&#19990;&#30028;&#30340;&#20248;&#21183;&#12290;&#36890;&#36807;&#37319;&#29992;&#31070;&#32463;&#31526;&#21495;&#21270;&#26041;&#27861;&#65292;DERRL&#23558;&#31526;&#21495;&#35745;&#21010;&#20013;&#30340;&#20851;&#31995;&#34920;&#31034;&#21644;&#32422;&#26463;&#19982;&#28145;&#24230;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#25552;&#21462;&#20986;&#21487;&#35299;&#37322;&#30340;&#31574;&#30053;&#12290;&#36825;&#20123;&#31574;&#30053;&#20197;&#36923;&#36753;&#35268;&#21017;&#30340;&#24418;&#24335;&#35299;&#37322;&#20102;&#27599;&#20010;&#20915;&#31574;&#65288;&#25110;&#21160;&#20316;&#65289;&#30340;&#20135;&#29983;&#36807;&#31243;&#12290;&#36890;&#36807;&#22810;&#20010;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;DERRL&#22312;&#22788;&#29702;&#29615;&#22659;&#32467;&#26500;&#21464;&#21270;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite numerous successes in Deep Reinforcement Learning (DRL), the learned policies are not interpretable. Moreover, since DRL does not exploit symbolic relational representations, it has difficulties in coping with structural changes in its environment (such as increasing the number of objects). Relational Reinforcement Learning, on the other hand, inherits the relational representations from symbolic planning to learn reusable policies. However, it has so far been unable to scale up and exploit the power of deep neural networks. We propose Deep Explainable Relational Reinforcement Learning (DERRL), a framework that exploits the best of both -- neural and symbolic worlds. By resorting to a neuro-symbolic approach, DERRL combines relational representations and constraints from symbolic planning with deep learning to extract interpretable policies. These policies are in the form of logical rules that explain how each decision (or action) is arrived at. Through several experiments, in 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#29702;&#35770;&#35777;&#26126;&#20102;&#25439;&#22833;&#20989;&#25968;&#30340;Lipschitz&#24120;&#25968;&#26159;&#38477;&#20302;Adam&#25110;AdamW&#33719;&#24471;&#36755;&#20986;&#27169;&#22411;&#30340;&#27867;&#21270;&#35823;&#24046;&#30340;&#19968;&#20010;&#37325;&#35201;&#22240;&#32032;&#12290;&#26412;&#25991;&#30340;&#36873;&#25321;&#25439;&#22833;&#20989;&#25968;&#26041;&#38024;&#20026;Adam&#25110;AdamW&#20248;&#21270;&#31639;&#27861;&#30340;&#20351;&#29992;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;Lipschitz&#24120;&#25968;&#36739;&#20302;&#19988;&#26368;&#22823;&#20540;&#36739;&#23567;&#30340;&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.16464</link><description>&lt;p&gt;
Adam&#21644;AdamW&#20248;&#21270;&#22120;&#35757;&#32451;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25439;&#22833;&#20989;&#25968;&#30340;Lipschitz&#25928;&#24212;&#23545;&#27867;&#21270;&#24615;&#33021;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Lipschitzness Effect of a Loss Function on Generalization Performance of Deep Neural Networks Trained by Adam and AdamW Optimizers. (arXiv:2303.16464v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16464
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#29702;&#35770;&#35777;&#26126;&#20102;&#25439;&#22833;&#20989;&#25968;&#30340;Lipschitz&#24120;&#25968;&#26159;&#38477;&#20302;Adam&#25110;AdamW&#33719;&#24471;&#36755;&#20986;&#27169;&#22411;&#30340;&#27867;&#21270;&#35823;&#24046;&#30340;&#19968;&#20010;&#37325;&#35201;&#22240;&#32032;&#12290;&#26412;&#25991;&#30340;&#36873;&#25321;&#25439;&#22833;&#20989;&#25968;&#26041;&#38024;&#20026;Adam&#25110;AdamW&#20248;&#21270;&#31639;&#27861;&#30340;&#20351;&#29992;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;Lipschitz&#24120;&#25968;&#36739;&#20302;&#19988;&#26368;&#22823;&#20540;&#36739;&#23567;&#30340;&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#19968;&#20010;&#20027;&#35201;&#20851;&#27880;&#28857;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#24615;&#33021;&#19982;&#20248;&#21270;&#31639;&#27861;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#25439;&#22833;&#20989;&#25968;&#30340;Lipschitz&#24120;&#25968;&#26159;&#38477;&#20302;Adam&#25110;AdamW&#33719;&#24471;&#36755;&#20986;&#27169;&#22411;&#30340;&#27867;&#21270;&#35823;&#24046;&#30340;&#19968;&#20010;&#37325;&#35201;&#22240;&#32032;&#12290;&#36825;&#20123;&#32467;&#26524;&#21487;&#20316;&#20026;&#36873;&#25321;&#25439;&#22833;&#20989;&#25968;&#26102;&#20248;&#21270;&#31639;&#27861;&#20026;Adam&#25110;AdamW&#30340;&#25351;&#23548;&#26041;&#38024;&#12290;&#26412;&#25991;&#36873;&#25321;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#20154;&#33080;&#24180;&#40836;&#35780;&#20272;&#38382;&#39064;&#26469;&#35780;&#20272;&#29702;&#35770;&#30028;&#38480;&#22312;&#23454;&#38469;&#29615;&#22659;&#19979;&#30340;&#34920;&#29616;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#35780;&#20272;&#27867;&#21270;&#33021;&#21147;&#65292;&#35757;&#32451;&#38598;&#21644;&#27979;&#35797;&#38598;&#20174;&#19981;&#21516;&#20998;&#24067;&#20013;&#36873;&#25321;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Lipschitz&#24120;&#25968;&#36739;&#20302;&#19988;&#26368;&#22823;&#20540;&#36739;&#23567;&#30340;&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#25552;&#39640;Adam&#25110;AdamW&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The generalization performance of deep neural networks with regard to the optimization algorithm is one of the major concerns in machine learning. This performance can be affected by various factors. In this paper, we theoretically prove that the Lipschitz constant of a loss function is an important factor to diminish the generalization error of the output model obtained by Adam or AdamW. The results can be used as a guideline for choosing the loss function when the optimization algorithm is Adam or AdamW. In addition, to evaluate the theoretical bound in a practical setting, we choose the human age estimation problem in computer vision. For assessing the generalization better, the training and test datasets are drawn from different distributions. Our experimental evaluation shows that the loss function with lower Lipschitz constant and maximum value improves the generalization of the model trained by Adam or AdamW.
&lt;/p&gt;</description></item><item><title>DiffTAD&#26159;&#19968;&#31181;&#26032;&#30340;&#26102;&#38388;&#21160;&#20316;&#26816;&#27979;&#26041;&#27861;&#65292;&#20351;&#29992;Proposal Denoising Diffusion&#30340;&#29983;&#25104;&#24314;&#27169;&#35270;&#35282;&#65292;&#24182;&#36890;&#36807;&#27491;&#21521;/&#22122;&#22768;&#36807;&#31243;&#21644;&#21453;&#21521;/&#21435;&#22122;&#36807;&#31243;&#23454;&#29616;&#20934;&#30830;&#30340;&#21160;&#20316;&#25552;&#35758;&#12290;</title><link>http://arxiv.org/abs/2303.14863</link><description>&lt;p&gt;
DiffTAD: &#20351;&#29992;Proposal Denoising Diffusion&#30340;&#26102;&#38388;&#21160;&#20316;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
DiffTAD: Temporal Action Detection with Proposal Denoising Diffusion. (arXiv:2303.14863v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14863
&lt;/p&gt;
&lt;p&gt;
DiffTAD&#26159;&#19968;&#31181;&#26032;&#30340;&#26102;&#38388;&#21160;&#20316;&#26816;&#27979;&#26041;&#27861;&#65292;&#20351;&#29992;Proposal Denoising Diffusion&#30340;&#29983;&#25104;&#24314;&#27169;&#35270;&#35282;&#65292;&#24182;&#36890;&#36807;&#27491;&#21521;/&#22122;&#22768;&#36807;&#31243;&#21644;&#21453;&#21521;/&#21435;&#22122;&#36807;&#31243;&#23454;&#29616;&#20934;&#30830;&#30340;&#21160;&#20316;&#25552;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#38388;&#21160;&#20316;&#26816;&#27979;&#65288;TAD&#65289;&#30340;&#34920;&#36798;&#24418;&#24335;&#65292;&#21363;DiffTAD&#12290;&#23427;&#37319;&#29992;&#38543;&#26426;&#26102;&#38388;&#25552;&#35758;&#20316;&#20026;&#36755;&#20837;&#65292;&#22312;&#32473;&#23450;&#26410;&#20462;&#21098;&#30340;&#38271;&#26102;&#38388;&#35270;&#39057;&#30340;&#24773;&#20917;&#19979;&#33021;&#22815;&#20934;&#30830;&#22320;&#20135;&#29983;&#21160;&#20316;&#25552;&#35758;&#12290;&#36825;&#26159;&#19968;&#31181;&#29983;&#25104;&#24314;&#27169;&#30340;&#35270;&#35282;&#65292;&#19982;&#20808;&#21069;&#30340;&#21028;&#21035;&#23398;&#20064;&#26041;&#24335;&#30456;&#23545;&#31435;&#12290;&#36890;&#36807;&#39318;&#20808;&#23558;&#22320;&#38754;&#30495;&#23454;&#25552;&#35758;&#25193;&#25955;&#21040;&#38543;&#26426;&#25552;&#35758;&#65288;&#21363;&#27491;&#21521;/&#22122;&#22768;&#36807;&#31243;&#65289;&#65292;&#28982;&#21518;&#23398;&#20064;&#36870;&#36716;&#22122;&#22768;&#36807;&#31243;&#65288;&#21363;&#21453;&#21521;/&#21435;&#22122;&#36807;&#31243;&#65289;&#26469;&#23454;&#29616;&#36825;&#31181;&#33021;&#21147;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;Transformer&#35299;&#30721;&#22120;&#65288;&#22914;DETR&#65289;&#20013;&#24341;&#20837;&#20855;&#26377;&#26356;&#24555;&#25910;&#25947;&#36895;&#24230;&#30340;&#26102;&#38388;&#20301;&#32622;&#26597;&#35810;&#35774;&#35745;&#26469;&#24314;&#31435;&#21435;&#22122;&#36807;&#31243;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25512;&#29702;&#21152;&#36895;&#30340;&#20132;&#21449;&#27493;&#36873;&#25321;&#24615;&#35843;&#33410;&#31639;&#27861;&#12290;&#23545;ActivityNet&#21644;THUMOS&#36827;&#34892;&#30340;&#24191;&#27867;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;DiffTAD&#30456;&#27604;&#20808;&#21069;&#30340;&#26367;&#20195;&#26041;&#26696;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;&#20195;&#30721;&#23558;&#22312;https://github.com/sauradip/Di&#19978;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new formulation of temporal action detection (TAD) with denoising diffusion, DiffTAD in short. Taking as input random temporal proposals, it can yield action proposals accurately given an untrimmed long video. This presents a generative modeling perspective, against previous discriminative learning manners. This capability is achieved by first diffusing the ground-truth proposals to random ones (i.e., the forward/noising process) and then learning to reverse the noising process (i.e., the backward/denoising process). Concretely, we establish the denoising process in the Transformer decoder (e.g., DETR) by introducing a temporal location query design with faster convergence in training. We further propose a cross-step selective conditioning algorithm for inference acceleration. Extensive evaluations on ActivityNet and THUMOS show that our DiffTAD achieves top performance compared to previous art alternatives. The code will be made available at https://github.com/sauradip/Di
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#26465;&#20214;&#20048;&#35266;&#25506;&#32034;(COE)&#30340;&#22522;&#20110;UCT&#30340;&#25506;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#26500;&#20381;&#36182;&#20851;&#31995;&#40723;&#21169;&#26234;&#33021;&#20307;&#36827;&#34892;&#22522;&#20110;&#20048;&#35266;&#20027;&#20041;&#30340;&#21327;&#21516;&#25506;&#32034;&#12290;</title><link>http://arxiv.org/abs/2303.09032</link><description>&lt;p&gt;
&#26465;&#20214;&#20048;&#35266;&#25506;&#32034;&#29992;&#20110;&#28145;&#24230;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Conditionally Optimistic Exploration for Cooperative Deep Multi-Agent Reinforcement Learning. (arXiv:2303.09032v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09032
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#26465;&#20214;&#20048;&#35266;&#25506;&#32034;(COE)&#30340;&#22522;&#20110;UCT&#30340;&#25506;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#26500;&#20381;&#36182;&#20851;&#31995;&#40723;&#21169;&#26234;&#33021;&#20307;&#36827;&#34892;&#22522;&#20110;&#20048;&#35266;&#20027;&#20041;&#30340;&#21327;&#21516;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#30340;&#25506;&#32034;&#23545;&#20110;&#21327;&#20316;&#28145;&#24230;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;(MARL)&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;UCT(&#24212;&#29992;&#20110;&#26641;&#30340;&#32622;&#20449;&#19978;&#38480;)&#30340;&#25506;&#32034;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#22320;&#40723;&#21169;&#21327;&#21516;&#25506;&#32034;&#12290;&#39640;&#23618;&#27425;&#30340;&#24605;&#36335;&#26159;&#65292;&#20026;&#20102;&#36827;&#34892;&#22522;&#20110;&#20048;&#35266;&#20027;&#20041;&#30340;&#25506;&#32034;&#65292;&#22914;&#26524;&#27599;&#20010;&#26234;&#33021;&#20307;&#30340;&#20048;&#35266;&#20272;&#35745;&#25429;&#33719;&#20102;&#19982;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#32467;&#26500;&#21270;&#20381;&#36182;&#20851;&#31995;&#65292;&#21017;&#26234;&#33021;&#20307;&#23558;&#23454;&#29616;&#21327;&#20316;&#31574;&#30053;&#12290;&#22312;&#25628;&#32034;&#26641;&#30340;&#27599;&#20010;&#33410;&#28857;&#65288;&#21363;&#21160;&#20316;&#65289;&#19978;&#65292;UCT&#20351;&#29992;&#36890;&#36807;&#23545;&#20854;&#29238;&#33410;&#28857;&#30340;&#35775;&#38382;&#35745;&#25968;&#36827;&#34892;&#26465;&#20214;&#25512;&#23548;&#30340;&#22870;&#21169;&#26469;&#25191;&#34892;&#22522;&#20110;&#20048;&#35266;&#20027;&#20041;&#30340;&#25506;&#32034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;MARL&#35270;&#20026;&#26641;&#25628;&#32034;&#36845;&#20195;&#30340;&#26041;&#27861;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;&#26465;&#20214;&#20048;&#35266;&#25506;&#32034;(COE)&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#20551;&#35774;&#26234;&#33021;&#20307;&#25353;&#39034;&#24207;&#25191;&#34892;&#21160;&#20316;&#65292;&#24182;&#23558;&#25628;&#32034;&#26641;&#21516;&#19968;&#28145;&#24230;&#30340;&#33410;&#28857;&#35270;&#20026;&#19968;&#20010;&#21333;&#29420;&#26234;&#33021;&#20307;&#30340;&#21160;&#20316;&#12290;COE&#35745;&#31639;&#22522;&#20110;&#20048;&#35266;&#20027;&#20041;&#30340;&#22870;&#21169;&#65292;&#20197;&#40723;&#21169;&#26234;&#33021;&#20307;&#36890;&#36807;&#23581;&#35797;&#26032;&#30340;&#34892;&#21160;&#26469;&#25506;&#32034;&#26410;&#30693;&#30340;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficient exploration is critical in cooperative deep Multi-Agent Reinforcement Learning (MARL). In this paper, we propose an exploration method that efficiently encourages cooperative exploration based on the idea of the theoretically justified tree search algorithm UCT (Upper Confidence bounds applied to Trees). The high-level intuition is that to perform optimism-based exploration, agents would achieve cooperative strategies if each agent's optimism estimate captures a structured dependency relationship with other agents. At each node (i.e., action) of the search tree, UCT performs optimism-based exploration using a bonus derived by conditioning on the visitation count of its parent node. We provide a perspective to view MARL as tree search iterations and develop a method called Conditionally Optimistic Exploration (COE). We assume agents take actions following a sequential order, and consider nodes at the same depth of the search tree as actions of one individual agent. COE compute
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21033;&#29992;&#36229;&#22768;&#24433;&#20687;&#39044;&#27979;&#20799;&#31185;&#30097;&#20284;&#38417;&#23614;&#28814;&#30340;&#35786;&#26029;&#12289;&#31649;&#29702;&#21644;&#20005;&#37325;&#31243;&#24230;&#12290;&#27169;&#22411;&#20351;&#29992;&#20102;&#36229;&#22768;&#24433;&#20687;&#21644;&#20020;&#24202;&#12289;&#23454;&#39564;&#23460;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#25512;&#24191;&#20102;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#21040;&#22810;&#35270;&#22270;&#21644;&#19981;&#23436;&#25972;&#27010;&#24565;&#38598;&#30340;&#39044;&#27979;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.14460</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#21644;&#33021;&#22815;&#24178;&#39044;&#30340;&#36229;&#22768;&#25104;&#20687;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#29992;&#20110;&#20799;&#31185;&#38417;&#23614;&#28814;
&lt;/p&gt;
&lt;p&gt;
Interpretable and Intervenable Ultrasonography-based Machine Learning Models for Pediatric Appendicitis. (arXiv:2302.14460v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14460
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21033;&#29992;&#36229;&#22768;&#24433;&#20687;&#39044;&#27979;&#20799;&#31185;&#30097;&#20284;&#38417;&#23614;&#28814;&#30340;&#35786;&#26029;&#12289;&#31649;&#29702;&#21644;&#20005;&#37325;&#31243;&#24230;&#12290;&#27169;&#22411;&#20351;&#29992;&#20102;&#36229;&#22768;&#24433;&#20687;&#21644;&#20020;&#24202;&#12289;&#23454;&#39564;&#23460;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#25512;&#24191;&#20102;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#21040;&#22810;&#35270;&#22270;&#21644;&#19981;&#23436;&#25972;&#27010;&#24565;&#38598;&#30340;&#39044;&#27979;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38417;&#23614;&#28814;&#26159;&#20799;&#31185;&#33145;&#37096;&#25163;&#26415;&#30340;&#26368;&#24120;&#35265;&#21407;&#22240;&#20043;&#19968;&#12290;&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25968;&#25454;&#39537;&#21160;&#30340;&#20915;&#31574;&#25903;&#25345;&#21487;&#20197;&#24110;&#21161;&#20020;&#24202;&#21307;&#29983;&#35786;&#26029;&#21644;&#31649;&#29702;&#24739;&#32773;&#65292;&#21516;&#26102;&#20943;&#23569;&#38750;&#20851;&#38190;&#25163;&#26415;&#30340;&#25968;&#37327;&#12290;&#20197;&#24448;&#30340;&#38417;&#23614;&#28814;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#20027;&#35201;&#20851;&#27880;&#20020;&#24202;&#12289;&#23454;&#39564;&#23460;&#12289;&#35780;&#20998;&#21644;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#25968;&#25454;&#65292;&#20027;&#35201;&#24573;&#35270;&#20102;&#33145;&#37096;&#36229;&#22768;&#25104;&#20687;&#65292;&#36825;&#26159;&#19968;&#31181;&#38750;&#20405;&#20837;&#24615;&#21644;&#26041;&#20415;&#33719;&#24471;&#30340;&#35786;&#26029;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#21644;&#39564;&#35777;&#20102;&#21033;&#29992;&#36229;&#22768;&#24433;&#20687;&#39044;&#27979;&#30097;&#20284;&#38417;&#23614;&#28814;&#35786;&#26029;&#12289;&#31649;&#29702;&#21644;&#20005;&#37325;&#31243;&#24230;&#30340;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20351;&#29992;&#20102;&#30001;579&#21517;&#20799;&#31185;&#24739;&#32773;&#30340;1709&#24133;&#36229;&#22768;&#24433;&#20687;&#65292;&#20197;&#21450;&#20020;&#24202;&#21644;&#23454;&#39564;&#23460;&#25968;&#25454;&#26500;&#25104;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23398;&#36129;&#29486;&#22312;&#20110;&#23558;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#25512;&#24191;&#21040;&#20855;&#26377;&#22810;&#20010;&#35270;&#22270;&#21644;&#19981;&#23436;&#25972;&#27010;&#24565;&#38598;&#30340;&#39044;&#27979;&#38382;&#39064;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#26679;&#30340;&#27169;&#22411;&#36866;&#29992;&#20110;&#24178;&#39044;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Appendicitis is among the most frequent reasons for pediatric abdominal surgeries. With recent advances in machine learning, data-driven decision support could help clinicians diagnose and manage patients while reducing the number of non-critical surgeries. Previous decision support systems for appendicitis focused on clinical, laboratory, scoring and computed tomography data, mainly ignoring abdominal ultrasound, a noninvasive and readily available diagnostic modality. To this end, we developed and validated interpretable machine learning models for predicting the diagnosis, management and severity of suspected appendicitis using ultrasound images. Our models were trained on a dataset comprising 579 pediatric patients with 1709 ultrasound images accompanied by clinical and laboratory data. Our methodological contribution is the generalization of concept bottleneck models to prediction problems with multiple views and incomplete concept sets. Notably, such models lend themselves to int
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#30123;&#24773;&#26399;&#38388;&#26816;&#27979;&#23398;&#29983;&#26159;&#21542;&#20018;&#36890;&#20316;&#24330;&#30340;&#25968;&#25454;&#25366;&#25496;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23545;&#36828;&#31243;&#32771;&#35797;&#20107;&#20214;&#26085;&#24535;&#25968;&#25454;&#30340;&#20998;&#26512;&#21457;&#29616;&#20102;&#32676;&#20307;&#20316;&#24330;&#34892;&#20026;&#25110;&#32773;&#24322;&#24120;&#65292;&#21516;&#26102;&#24314;&#31435;&#20102;&#19968;&#20010;&#35780;&#20272;&#24322;&#24120;&#26696;&#20363;&#30340;&#32463;&#39564;&#35268;&#21017;&#12290;</title><link>http://arxiv.org/abs/2302.07014</link><description>&lt;p&gt;
&#22312;&#32593;&#32476;&#32771;&#35797;&#20013;&#26816;&#27979;&#20018;&#36890;&#30340;&#25968;&#25454;&#25366;&#25496;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Data Mining Approach for Detecting Collusion in Unproctored Online Exams. (arXiv:2302.07014v2 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07014
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#30123;&#24773;&#26399;&#38388;&#26816;&#27979;&#23398;&#29983;&#26159;&#21542;&#20018;&#36890;&#20316;&#24330;&#30340;&#25968;&#25454;&#25366;&#25496;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23545;&#36828;&#31243;&#32771;&#35797;&#20107;&#20214;&#26085;&#24535;&#25968;&#25454;&#30340;&#20998;&#26512;&#21457;&#29616;&#20102;&#32676;&#20307;&#20316;&#24330;&#34892;&#20026;&#25110;&#32773;&#24322;&#24120;&#65292;&#21516;&#26102;&#24314;&#31435;&#20102;&#19968;&#20010;&#35780;&#20272;&#24322;&#24120;&#26696;&#20363;&#30340;&#32463;&#39564;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;COVID-19&#30123;&#24773;&#26399;&#38388;&#30340;&#39044;&#38450;&#24615;&#25514;&#26045;&#65292;&#35768;&#22810;&#22823;&#23398;&#25552;&#20379;&#20102;&#26080;&#30417;&#32771;&#30340;&#36828;&#31243;&#32771;&#35797;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#26816;&#27979;&#23398;&#29983;&#38388;&#21487;&#33021;&#30340;&#20018;&#36890;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#30123;&#24773;&#26399;&#38388;&#36828;&#31243;&#32771;&#35797;&#30340;&#20107;&#20214;&#26085;&#24535;&#25968;&#25454;&#20013;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20123;&#26377;&#30528;&#26126;&#26174;&#30456;&#20284;&#32771;&#35797;&#30340;&#23398;&#29983;&#32676;&#20307;&#65292;&#21516;&#26102;&#23558;&#25105;&#20204;&#30340;&#21457;&#29616;&#19982;&#34987;&#30417;&#32771;&#25511;&#21046;&#32452;&#30340;&#32467;&#26524;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#22522;&#20110;&#36825;&#20123;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#35780;&#20272;&#8220;&#26497;&#20854;&#30456;&#20284;&#8221;&#30340;&#24322;&#24120;&#26696;&#20363;&#30340;&#32463;&#39564;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the precautionary measures during the COVID-19 pandemic many universities offered unproctored take-home exams. We propose methods to detect potential collusion between students and apply our approach on event log data from take-home exams during the pandemic. We find groups of students with suspiciously similar exams. In addition, we compare our findings to a proctored control group. By this, we establish a rule of thumb for evaluating which cases are "outstandingly similar", i.e., suspicious cases.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#26032;&#26631;&#31614;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#25163;&#21160;&#26631;&#35760;&#30340;&#25968;&#25454;&#20013;&#23384;&#22312;&#22122;&#22768;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#27169;&#22411;&#39044;&#27979;&#26469;&#36741;&#21161;&#20154;&#31867;&#26631;&#35760;&#22122;&#22768;&#25968;&#25454;&#12290;&#23454;&#39564;&#35777;&#26126;&#27492;&#26041;&#27861;&#36866;&#29992;&#20110;&#22810;&#31867;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2302.04391</link><description>&lt;p&gt;
&#25968;&#25454;&#20013;&#24515;&#26426;&#22120;&#23398;&#20064;&#30340;&#37325;&#26032;&#26631;&#31614;&#27861;
&lt;/p&gt;
&lt;p&gt;
The Re-Label Method For Data-Centric Machine Learning. (arXiv:2302.04391v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#26032;&#26631;&#31614;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#25163;&#21160;&#26631;&#35760;&#30340;&#25968;&#25454;&#20013;&#23384;&#22312;&#22122;&#22768;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#27169;&#22411;&#39044;&#27979;&#26469;&#36741;&#21161;&#20154;&#31867;&#26631;&#35760;&#22122;&#22768;&#25968;&#25454;&#12290;&#23454;&#39564;&#35777;&#26126;&#27492;&#26041;&#27861;&#36866;&#29992;&#20110;&#22810;&#31867;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20013;&#65292;&#25163;&#21160;&#26631;&#35760;&#30340;&#25968;&#25454;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#23384;&#22312;&#22122;&#22768;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#22312;&#24320;&#21457;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;90&#20998;&#20197;&#19978;&#30340;&#25104;&#32489;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#25214;&#20986;&#22122;&#22768;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#37319;&#29992;&#27169;&#22411;&#39044;&#27979;&#20316;&#20026;&#20154;&#31867;&#26631;&#35760;&#30340;&#21442;&#32771;&#26469;&#37325;&#26032;&#26631;&#35760;&#22122;&#22768;&#25968;&#25454;&#12290;&#26412;&#25991;&#38416;&#36848;&#20102;&#25105;&#20204;&#22312;&#24191;&#27867;&#30340;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#24819;&#27861;&#65292;&#21253;&#25324;&#20998;&#31867;&#12289;&#24207;&#21015;&#26631;&#35760;&#12289;&#29289;&#20307;&#26816;&#27979;&#12289;&#24207;&#21015;&#29983;&#25104;&#12289;&#28857;&#20987;&#29575;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#21644;&#20154;&#31867;&#35780;&#20272;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#24819;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In industry deep learning application, our manually labeled data has a certain number of noisy data. To solve this problem and achieve more than 90 score in dev dataset, we present a simple method to find the noisy data and re-label the noisy data by human, given the model predictions as references in human labeling. In this paper, we illustrate our idea for a broad set of deep learning tasks, includes classification, sequence tagging, object detection, sequence generation, click-through rate prediction. The experimental results and human evaluation results verify our idea.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;CLIPood&#65292;&#19968;&#31181;&#23558;CLIP&#27867;&#21270;&#21040;&#36229;&#20986;&#20998;&#24067;&#27979;&#35797;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;CLIPood&#24341;&#20837;&#20102;&#26032;&#30340;&#35757;&#32451;&#30446;&#26631;MMS&#21644;&#20248;&#21270;&#31574;&#30053;BMA&#65292;&#20197;&#36866;&#24212;OOD&#24773;&#20917;&#24182;&#25552;&#21319;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.00864</link><description>&lt;p&gt;
CLIPood&#65306;&#23558;CLIP&#27867;&#21270;&#21040;&#36229;&#20986;&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;
CLIPood: Generalizing CLIP to Out-of-Distributions. (arXiv:2302.00864v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00864
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;CLIPood&#65292;&#19968;&#31181;&#23558;CLIP&#27867;&#21270;&#21040;&#36229;&#20986;&#20998;&#24067;&#27979;&#35797;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;CLIPood&#24341;&#20837;&#20102;&#26032;&#30340;&#35757;&#32451;&#30446;&#26631;MMS&#21644;&#20248;&#21270;&#31574;&#30053;BMA&#65292;&#20197;&#36866;&#24212;OOD&#24773;&#20917;&#24182;&#25552;&#21319;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#26159;&#22788;&#29702;&#35757;&#32451;&#36807;&#31243;&#20013;&#20998;&#24067;&#21457;&#29983;&#21464;&#21270;&#30340;&#24773;&#20917;&#65292;&#21363;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#27867;&#21270;&#38382;&#39064;&#12290;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#27169;&#22411;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#38646;&#26679;&#26412;&#33021;&#21147;&#65292;&#20294;&#22312;&#21518;&#32493;&#20219;&#21153;&#20013;&#36827;&#19968;&#27493;&#35843;&#25972;CLIP&#27169;&#22411;&#20250;&#19981;&#21487;&#36991;&#20813;&#22320;&#38477;&#20302;OOD&#24615;&#33021;&#12290;&#26412;&#25991;&#26088;&#22312;&#23558;CLIP&#27867;&#21270;&#21040;&#36229;&#20986;&#20998;&#24067;&#27979;&#35797;&#25968;&#25454;&#30340;&#21518;&#32493;&#20219;&#21153;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;CLIPood&#65292;&#19968;&#31181;&#21487;&#20197;&#36866;&#24212;OOD&#24773;&#20917;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#36825;&#20123;&#24773;&#20917;&#21487;&#33021;&#21253;&#25324;&#39046;&#22495;&#36716;&#31227;&#21644;&#26410;&#30693;&#31867;&#21035;&#22312;&#26410;&#35265;&#27979;&#35797;&#25968;&#25454;&#20013;&#20986;&#29616;&#12290;&#20026;&#20102;&#21033;&#29992;&#25991;&#26412;&#27169;&#24577;&#19979;&#31867;&#21035;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;CLIPood&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#35757;&#32451;&#30446;&#26631;&#65292;&#21363;&#36793;&#36317;&#24230;&#37327;&#36719;&#26368;&#22823;&#21270;&#65288;MMS&#65289;&#65292;&#20854;&#20855;&#26377;&#31867;&#21035;&#33258;&#36866;&#24212;&#36793;&#36317;&#29992;&#20110;&#24494;&#35843;&#12290;&#20026;&#20102;&#23558;&#39044;&#35757;&#32451;&#30340;&#38646;&#26679;&#26412;&#27169;&#22411;&#21644;&#24494;&#35843;&#30340;&#20219;&#21153;&#33258;&#36866;&#24212;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#65292;CLIPood&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#31574;&#30053;&#65292;&#21363;Beta&#31227;&#21160;&#24179;&#22343;&#65288;BMA&#65289;&#65292;&#20197;&#36890;&#36807;Beta&#20998;&#24067;&#32500;&#25345;&#19968;&#20010;&#26102;&#38388;&#19978;&#30340;&#38598;&#25104;&#21152;&#26435;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution (OOD) generalization, where the model needs to handle distribution shifts from training, is a major challenge of machine learning. Contrastive language-image pre-training (CLIP) models have shown impressive zero-shot ability, but the further adaptation of CLIP on downstream tasks undesirably degrades OOD performances. This paper aims at generalizing CLIP to out-of-distribution test data on downstream tasks. We propose CLIPood, a fine-tuning method that can adapt CLIP models to OOD situations where both domain shifts and open classes may occur on the unseen test data. To exploit the semantic relations between classes from the text modality, CLIPood introduces a new training objective, margin metric softmax (MMS), with class adaptive margins for fine-tuning. To incorporate both pre-trained zero-shot model and fine-tuned task-adaptive model, CLIPood leverages a new optimization strategy, Beta moving average (BMA), to maintain a temporal ensemble weighted by Beta distri
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26080;&#21442;&#25968;&#12289;&#39046;&#22495;&#26080;&#20851;&#30340;Shapley&#20540;&#36817;&#20284;&#31639;&#27861;SVARM&#21644;Stratified SVARM&#65292;&#23427;&#20204;&#22522;&#20110;&#19968;&#31181;&#19982;&#36793;&#38469;&#36129;&#29486;&#27010;&#24565;&#33073;&#38057;&#30340;Shapley&#20540;&#34920;&#31034;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23427;&#20204;&#22312;&#36817;&#20284;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#26080;&#19982;&#20262;&#27604;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#32467;&#26524;&#23558;&#20854;&#19982;&#21512;&#25104;&#28216;&#25103;&#21644;&#24120;&#35265;&#21487;&#35299;&#37322;&#24615;&#29992;&#20363;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2302.00736</link><description>&lt;p&gt;
&#19981;&#20351;&#29992;&#36793;&#38469;&#36129;&#29486;&#36817;&#20284;&#35745;&#31639;Shapley&#20540;
&lt;/p&gt;
&lt;p&gt;
Approximating the Shapley Value without Marginal Contributions. (arXiv:2302.00736v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00736
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26080;&#21442;&#25968;&#12289;&#39046;&#22495;&#26080;&#20851;&#30340;Shapley&#20540;&#36817;&#20284;&#31639;&#27861;SVARM&#21644;Stratified SVARM&#65292;&#23427;&#20204;&#22522;&#20110;&#19968;&#31181;&#19982;&#36793;&#38469;&#36129;&#29486;&#27010;&#24565;&#33073;&#38057;&#30340;Shapley&#20540;&#34920;&#31034;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23427;&#20204;&#22312;&#36817;&#20284;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#26080;&#19982;&#20262;&#27604;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#32467;&#26524;&#23558;&#20854;&#19982;&#21512;&#25104;&#28216;&#25103;&#21644;&#24120;&#35265;&#21487;&#35299;&#37322;&#24615;&#29992;&#20363;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Shapley&#20540;&#26159;&#20026;&#21512;&#20316;&#21338;&#24328;&#20013;&#30340;&#29609;&#23478;&#20998;&#37197;&#26377;&#24847;&#20041;&#30340;&#36129;&#29486;&#20540;&#30340;&#26368;&#27969;&#34892;&#26041;&#27861;&#65292;&#26368;&#36817;&#22312;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;Shapley&#20540;&#30340;&#26377;&#24847;&#20041;&#24615;&#28304;&#20110;&#20165;&#26377;Shapley&#20540;&#28385;&#36275;&#30340;&#20844;&#29702;&#23646;&#24615;&#65292;&#28982;&#32780;&#65292;&#30830;&#20999;&#35745;&#31639;&#30340;&#20195;&#20215;&#26159;&#38543;&#30528;&#29609;&#23478;&#25968;&#37327;&#25351;&#25968;&#32423;&#22686;&#38271;&#12290;&#22240;&#27492;&#65292;&#35768;&#22810;&#30740;&#31350;&#33268;&#21147;&#20110;&#39640;&#25928;&#36817;&#20284;Shapley&#20540;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#22260;&#32469;&#30528;&#29609;&#23478;&#30340;&#36793;&#38469;&#36129;&#29486;&#30340;&#27010;&#24565;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#19982;&#36793;&#38469;&#36129;&#29486;&#27010;&#24565;&#33073;&#38057;&#30340;Shapley&#20540;&#34920;&#31034;&#30340;&#26080;&#21442;&#25968;&#12289;&#39046;&#22495;&#26080;&#20851;&#30340;&#36817;&#20284;&#31639;&#27861;SVARM&#21644;Stratified SVARM&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23427;&#20204;&#22312;&#36817;&#20284;&#36136;&#37327;&#26041;&#38754;&#30340;&#26080;&#19982;&#20262;&#27604;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#25552;&#20379;&#20102;&#21253;&#25324;&#21512;&#25104;&#28216;&#25103;&#21644;&#24120;&#29992;&#21487;&#35299;&#37322;&#24615;&#29992;&#20363;&#30340;&#23454;&#35777;&#32467;&#26524;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Shapley value is arguably the most popular approach for assigning a meaningful contribution value to players in a cooperative game, which has recently been used intensively in explainable artificial intelligence. The meaningfulness is due to axiomatic properties that only the Shapley value satisfies, which, however, comes at the expense of an exact computation growing exponentially with the number of agents. Accordingly, a number of works are devoted to the efficient approximation of the Shapley values, most of them revolve around the notion of an agent's marginal contribution. In this paper, we propose with SVARM and Stratified SVARM two parameter-free and domain-independent approximation algorithms based on a representation of the Shapley value detached from the notion of marginal contributions. We prove unmatched theoretical guarantees regarding their approximation quality and provide empirical results including synthetic games as well as common explainability use cases comparin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;DoCoFL&#65292;&#19968;&#31181;&#29992;&#20110;&#36328;&#35774;&#22791;&#32852;&#21512;&#23398;&#20064;&#30340;&#19979;&#34892;&#21387;&#32553;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#38477;&#20302;&#21452;&#21521;&#24102;&#23485;&#30340;&#21516;&#26102;&#20445;&#25345;&#31454;&#20105;&#21147;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.00543</link><description>&lt;p&gt;
DoCoFL&#65306;&#29992;&#20110;&#36328;&#35774;&#22791;&#32852;&#21512;&#23398;&#20064;&#30340;&#19979;&#34892;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
DoCoFL: Downlink Compression for Cross-Device Federated Learning. (arXiv:2302.00543v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00543
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;DoCoFL&#65292;&#19968;&#31181;&#29992;&#20110;&#36328;&#35774;&#22791;&#32852;&#21512;&#23398;&#20064;&#30340;&#19979;&#34892;&#21387;&#32553;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#38477;&#20302;&#21452;&#21521;&#24102;&#23485;&#30340;&#21516;&#26102;&#20445;&#25345;&#31454;&#20105;&#21147;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#21387;&#32553;&#25216;&#26415;&#24050;&#34987;&#25552;&#20986;&#29992;&#20110;&#20943;&#23569;&#32852;&#21512;&#23398;&#20064;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#36890;&#20449;&#24320;&#38144;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25216;&#26415;&#36890;&#24120;&#29992;&#20110;&#21387;&#32553;&#27169;&#22411;&#26356;&#26032;&#65292;&#32780;&#27169;&#22411;&#26356;&#26032;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20250;&#36880;&#28176;&#20943;&#23569;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#26041;&#27861;&#19981;&#36866;&#29992;&#20110;&#36328;&#35774;&#22791;&#30340;&#19979;&#34892;&#65288;&#21363;&#20174;&#26381;&#21153;&#22120;&#21040;&#23458;&#25143;&#31471;&#65289;&#21387;&#32553;&#65292;&#22312;&#36825;&#31181;&#22330;&#26223;&#19979;&#65292;&#24322;&#26500;&#23458;&#25143;&#31471;&#22312;&#35757;&#32451;&#26399;&#38388;&#21487;&#33021;&#21482;&#20986;&#29616;&#19968;&#27425;&#65292;&#22240;&#27492;&#24517;&#39035;&#19979;&#36733;&#27169;&#22411;&#21442;&#25968;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DoCoFL&#8212;&#8212;&#19968;&#20010;&#26032;&#30340;&#29992;&#20110;&#36328;&#35774;&#22791;&#19979;&#34892;&#21387;&#32553;&#30340;&#26694;&#26550;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;DoCoFL&#21487;&#20197;&#26080;&#32541;&#22320;&#19982;&#35768;&#22810;&#19978;&#34892;&#21387;&#32553;&#26041;&#26696;&#32467;&#21512;&#20351;&#29992;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#21452;&#21521;&#21387;&#32553;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;DoCoFL&#22312;&#26174;&#33879;&#38477;&#20302;&#21452;&#21521;&#24102;&#23485;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#19982;&#27809;&#26377;&#20219;&#20309;&#21387;&#32553;&#30340;&#22522;&#20934;&#27169;&#22411;&#30456;&#24403;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many compression techniques have been proposed to reduce the communication overhead of Federated Learning training procedures. However, these are typically designed for compressing model updates, which are expected to decay throughout training. As a result, such methods are inapplicable to downlink (i.e., from the parameter server to clients) compression in the cross-device setting, where heterogeneous clients $\textit{may appear only once}$ during training and thus must download the model parameters. Accordingly, we propose $\textsf{DoCoFL}$ -- a new framework for downlink compression in the cross-device setting. Importantly, $\textsf{DoCoFL}$ can be seamlessly combined with many uplink compression schemes, rendering it suitable for bi-directional compression. Through extensive evaluation, we show that $\textsf{DoCoFL}$ offers significant bi-directional bandwidth reduction while achieving competitive accuracy to that of a baseline without any compression.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20855;&#26377;&#38543;&#26426;&#32422;&#26463;&#30340;&#22312;&#32447;&#20984;&#20248;&#21270;&#30340;&#28418;&#31227;&#21152;&#24809;&#32602;&#31639;&#27861;&#21464;&#20307;&#65292;&#20854;&#20445;&#35777;&#22312;&#22266;&#23450;&#36845;&#20195;&#27425;&#25968;&#21518;&#36798;&#21040;&#20102;$O(\sqrt{T})$&#30340;&#26399;&#26395;&#36951;&#25022;&#21644;&#38646;&#32422;&#26463;&#36829;&#35268;&#29575;&#12290;&#21516;&#26102;&#65292;&#23558;&#31639;&#27861;&#26694;&#26550;&#25193;&#23637;&#21040;&#20102;&#20855;&#26377;&#36172;&#21338;&#21453;&#39304;&#30340;&#24773;&#20917;&#19979;&#65292;&#20173;&#28982;&#33021;&#22815;&#23454;&#29616;$O(\sqrt{T})$&#30340;&#26399;&#26395;&#36951;&#25022;&#21644;&#38646;&#32422;&#26463;&#36829;&#35268;&#29575;&#12290;</title><link>http://arxiv.org/abs/2301.11267</link><description>&lt;p&gt;
&#22312;&#20855;&#26377;&#38543;&#26426;&#32422;&#26463;&#30340;&#22312;&#32447;&#20984;&#20248;&#21270;&#20013;&#38646;&#36829;&#32422;&#21644;&#36172;&#21338;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
Online Convex Optimization with Stochastic Constraints: Zero Constraint Violation and Bandit Feedback. (arXiv:2301.11267v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11267
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20855;&#26377;&#38543;&#26426;&#32422;&#26463;&#30340;&#22312;&#32447;&#20984;&#20248;&#21270;&#30340;&#28418;&#31227;&#21152;&#24809;&#32602;&#31639;&#27861;&#21464;&#20307;&#65292;&#20854;&#20445;&#35777;&#22312;&#22266;&#23450;&#36845;&#20195;&#27425;&#25968;&#21518;&#36798;&#21040;&#20102;$O(\sqrt{T})$&#30340;&#26399;&#26395;&#36951;&#25022;&#21644;&#38646;&#32422;&#26463;&#36829;&#35268;&#29575;&#12290;&#21516;&#26102;&#65292;&#23558;&#31639;&#27861;&#26694;&#26550;&#25193;&#23637;&#21040;&#20102;&#20855;&#26377;&#36172;&#21338;&#21453;&#39304;&#30340;&#24773;&#20917;&#19979;&#65292;&#20173;&#28982;&#33021;&#22815;&#23454;&#29616;$O(\sqrt{T})$&#30340;&#26399;&#26395;&#36951;&#25022;&#21644;&#38646;&#32422;&#26463;&#36829;&#35268;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#38543;&#26426;&#32422;&#26463;&#30340;&#22312;&#32447;&#20984;&#20248;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28418;&#31227;&#21152;&#24809;&#32602;&#31639;&#27861;&#30340;&#21464;&#20307;&#65292;&#22312;&#22266;&#23450;&#30340;&#36845;&#20195;&#27425;&#25968;&#21518;&#65292;&#23427;&#33021;&#22815;&#20445;&#35777;$O(\sqrt{T})$&#30340;&#26399;&#26395;&#36951;&#25022;&#21644;&#38646;&#32422;&#26463;&#36829;&#35268;&#29575;&#65292;&#36825;&#25913;&#36827;&#20102;&#26222;&#36890;&#30340;&#28418;&#31227;&#21152;&#24809;&#32602;&#26041;&#27861;&#65292;&#23427;&#30340;&#32422;&#26463;&#36829;&#35268;&#29575;&#20026;$O(\sqrt{T})$&#12290;&#19982;&#26222;&#36890;&#30340;&#28418;&#31227;&#21152;&#24809;&#32602;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#23545;&#20110;&#26102;&#38388;&#33539;&#22260;$T$&#30340;&#38271;&#24230;&#27809;&#26377;&#20219;&#20309;&#35201;&#27714;&#12290;&#36825;&#26159;&#22522;&#20110;&#25105;&#20204;&#30340;&#26032;&#22411;&#28418;&#31227;&#24341;&#29702;&#65292;&#23427;&#25552;&#20379;&#20102;&#26102;&#38388;&#21464;&#21270;&#30340;&#34394;&#25311;&#38431;&#21015;&#28418;&#31227;&#30028;&#38480;&#65292;&#20174;&#32780;&#23548;&#33268;&#20102;&#23545;&#26399;&#26395;&#34394;&#25311;&#38431;&#21015;&#38271;&#24230;&#30340;&#26102;&#38388;&#21464;&#21270;&#30028;&#38480;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26694;&#26550;&#25193;&#23637;&#21040;&#20102;&#20855;&#26377;&#20004;&#28857;&#36172;&#21338;&#21453;&#39304;&#30340;&#38543;&#26426;&#32422;&#26463;&#22312;&#32447;&#20984;&#20248;&#21270;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36890;&#36807;&#23558;&#25105;&#20204;&#30340;&#31639;&#27861;&#26694;&#26550;&#36866;&#24212;&#20110;&#36172;&#21338;&#21453;&#39304;&#22330;&#26223;&#65292;&#25105;&#20204;&#20173;&#28982;&#21487;&#20197;&#23454;&#29616;$O(\sqrt{T})$&#30340;&#26399;&#26395;&#36951;&#25022;&#21644;&#38646;&#32422;&#26463;&#36829;&#35268;&#29575;&#65292;&#25913;&#36827;&#20102;&#20808;&#21069;&#24037;&#20316;&#20013;&#20851;&#20110;&#30456;&#21516;&#32422;&#26463;&#24773;&#20917;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies online convex optimization with stochastic constraints. We propose a variant of the drift-plus-penalty algorithm that guarantees $O(\sqrt{T})$ expected regret and zero constraint violation, after a fixed number of iterations, which improves the vanilla drift-plus-penalty method with $O(\sqrt{T})$ constraint violation. Our algorithm is oblivious to the length of the time horizon $T$, in contrast to the vanilla drift-plus-penalty method. This is based on our novel drift lemma that provides time-varying bounds on the virtual queue drift and, as a result, leads to time-varying bounds on the expected virtual queue length. Moreover, we extend our framework to stochastic-constrained online convex optimization under two-point bandit feedback. We show that by adapting our algorithmic framework to the bandit feedback setting, we may still achieve $O(\sqrt{T})$ expected regret and zero constraint violation, improving upon the previous work for the case of identical constraint f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20840;&#23616;$k$-means\texttt{++}&#30340;&#32858;&#31867;&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20013;&#24515;&#36873;&#25321;&#27010;&#29575;&#23454;&#29616;&#23545;&#20840;&#23616;$k$-means&#30340;&#36136;&#37327;&#32858;&#31867;&#35299;&#30340;&#26377;&#25928;&#33719;&#21462;&#65292;&#24182;&#20943;&#23569;&#20102;&#35745;&#31639;&#36127;&#36733;&#12290;</title><link>http://arxiv.org/abs/2211.12271</link><description>&lt;p&gt;
&#20840;&#23616;$k$-means$++$: &#20840;&#23616;$k$-means&#32858;&#31867;&#31639;&#27861;&#30340;&#19968;&#31181;&#26377;&#25928;&#30340;&#31616;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Global $k$-means$++$: an effective relaxation of the global $k$-means clustering algorithm. (arXiv:2211.12271v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.12271
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20840;&#23616;$k$-means\texttt{++}&#30340;&#32858;&#31867;&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20013;&#24515;&#36873;&#25321;&#27010;&#29575;&#23454;&#29616;&#23545;&#20840;&#23616;$k$-means&#30340;&#36136;&#37327;&#32858;&#31867;&#35299;&#30340;&#26377;&#25928;&#33719;&#21462;&#65292;&#24182;&#20943;&#23569;&#20102;&#35745;&#31639;&#36127;&#36733;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
$k$-means&#31639;&#27861;&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#32858;&#31867;&#26041;&#27861;&#65292;&#22240;&#20854;&#31616;&#21333;&#12289;&#26377;&#25928;&#21644;&#24555;&#36895;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#20854;&#20027;&#35201;&#32570;&#28857;&#26159;&#23545;&#32858;&#31867;&#20013;&#24515;&#30340;&#21021;&#22987;&#20301;&#32622;&#38750;&#24120;&#25935;&#24863;&#12290;&#20840;&#23616;$k$-means&#26159;&#19968;&#31181;&#30830;&#23450;&#24615;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;$k$-means&#30340;&#38543;&#26426;&#21021;&#22987;&#21270;&#38382;&#39064;&#65292;&#20294;&#20247;&#25152;&#21608;&#30693;&#20854;&#35745;&#31639;&#25104;&#26412;&#24456;&#39640;&#12290;&#23427;&#36890;&#36807;&#36880;&#27493;&#35299;&#20915;&#25152;&#26377;$k$-means&#23376;&#38382;&#39064;&#65292;&#23558;&#25968;&#25454;&#21010;&#20998;&#20026;$K$&#20010;&#31751;&#65292;&#20854;&#20013;$k=1,\ldots, K$&#12290;&#23545;&#20110;&#27599;&#20010;$k$&#31751;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#25191;&#34892;$k$-means&#31639;&#27861;$N$&#27425;&#65292;&#20854;&#20013;$N$&#26159;&#25968;&#25454;&#28857;&#30340;&#25968;&#37327;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20840;&#23616;$k$-means\texttt{++}&#32858;&#31867;&#31639;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#33719;&#21462;&#36136;&#37327;&#32858;&#31867;&#35299;&#30340;&#26041;&#27861;&#65292;&#19982;&#20840;&#23616;$k$-means&#30456;&#27604;&#65292;&#35745;&#31639;&#36127;&#36733;&#20943;&#23569;&#20102;&#12290;&#36825;&#26159;&#36890;&#36807;&#21033;&#29992;&#22312;$k$-means\texttt{++}&#31639;&#27861;&#20013;&#26377;&#25928;&#20351;&#29992;&#30340;&#20013;&#24515;&#36873;&#25321;&#27010;&#29575;&#26469;&#23454;&#29616;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The $k$-means algorithm is a prevalent clustering method due to its simplicity, effectiveness, and speed. However, its main disadvantage is its high sensitivity to the initial positions of the cluster centers. The global $k$-means is a deterministic algorithm proposed to tackle the random initialization problem of k-means but its well-known that requires high computational cost. It partitions the data to $K$ clusters by solving all $k$-means sub-problems incrementally for all $k=1,\ldots, K$. For each $k$ cluster problem, the method executes the $k$-means algorithm $N$ times, where $N$ is the number of datapoints. In this paper, we propose the \emph{global $k$-means\texttt{++}} clustering algorithm, which is an effective way of acquiring quality clustering solutions akin to those of global $k$-means with a reduced computational load. This is achieved by exploiting the center selection probability that is effectively used in the $k$-means\texttt{++} algorithm. The proposed method has be
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#25351;&#23548;&#30340;&#21160;&#24577;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120; ($\Phi$-DVAE) &#29992;&#20110;&#23558;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#21516;&#21270;&#21040;&#29289;&#29702;&#27169;&#22411;&#20013;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#22312;&#26410;&#30693;&#26144;&#23556;&#24773;&#20917;&#19979;&#26080;&#27861;&#23454;&#29616;&#19968;&#33268;&#27169;&#22411;&#19982;&#25968;&#25454;&#32508;&#21512;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2209.15609</link><description>&lt;p&gt;
$\Phi$-DVAE: &#29289;&#29702;&#25351;&#23548;&#30340;&#21160;&#24577;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#29992;&#20110;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#21516;&#21270;
&lt;/p&gt;
&lt;p&gt;
$\Phi$-DVAE: Physics-Informed Dynamical Variational Autoencoders for Unstructured Data Assimilation. (arXiv:2209.15609v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.15609
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#25351;&#23548;&#30340;&#21160;&#24577;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120; ($\Phi$-DVAE) &#29992;&#20110;&#23558;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#21516;&#21270;&#21040;&#29289;&#29702;&#27169;&#22411;&#20013;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#22312;&#26410;&#30693;&#26144;&#23556;&#24773;&#20917;&#19979;&#26080;&#27861;&#23454;&#29616;&#19968;&#33268;&#27169;&#22411;&#19982;&#25968;&#25454;&#32508;&#21512;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#25454;&#21516;&#21270;&#20013;&#65292;&#23558;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#32435;&#20837;&#29289;&#29702;&#27169;&#22411;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#20256;&#32479;&#26041;&#27861;&#36890;&#24120;&#20851;&#27880;&#20855;&#26377;&#26126;&#30830;&#23450;&#20041;&#35266;&#27979;&#31639;&#23376;&#30340;&#24773;&#20917;&#65292;&#20854;&#20989;&#25968;&#24418;&#24335;&#36890;&#24120;&#34987;&#20551;&#23450;&#20026;&#24050;&#30693;&#12290;&#36825;&#38459;&#27490;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#20174;&#25968;&#25454;&#31354;&#38388;&#21040;&#27169;&#22411;&#31354;&#38388;&#30340;&#26144;&#23556;&#26410;&#30693;&#30340;&#37197;&#32622;&#20013;&#23454;&#29616;&#19968;&#33268;&#30340;&#27169;&#22411;&#19982;&#25968;&#25454;&#32508;&#21512;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#29289;&#29702;&#25351;&#23548;&#30340;&#21160;&#24577;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;($\Phi$-DVAE)&#65292;&#23558;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#27969;&#23884;&#20837;&#21040;&#30001;&#24494;&#20998;&#26041;&#31243;&#25551;&#36848;&#30340;&#26102;&#21464;&#29289;&#29702;&#31995;&#32479;&#20013;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#19968;&#20010;&#26631;&#20934;&#30340;&#12289;&#21487;&#33021;&#26159;&#38750;&#32447;&#24615;&#30340;&#28508;&#22312;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#28388;&#27874;&#22120;&#21644;&#19968;&#20010;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65292;&#23558;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#21516;&#21270;&#21040;&#28508;&#22312;&#30340;&#21160;&#24577;&#31995;&#32479;&#20013;&#12290;&#22312;&#25105;&#20204;&#30340;&#31034;&#20363;&#31995;&#32479;&#20013;&#65292;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#37319;&#29992;&#35270;&#39057;&#25968;&#25454;&#21644;&#36895;&#24230;&#22330;&#27979;&#37327;&#30340;&#24418;&#24335;&#65292;&#20294;&#35813;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#36275;&#22815;&#36890;&#29992;&#65292;&#21487;&#20197;&#20801;&#35768;&#20219;&#24847;&#26410;&#30693;&#30340;&#35266;&#27979;&#31639;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
Incorporating unstructured data into physical models is a challenging problem that is emerging in data assimilation. Traditional approaches focus on well-defined observation operators whose functional forms are typically assumed to be known. This prevents these methods from achieving a consistent model-data synthesis in configurations where the mapping from data-space to model-space is unknown. To address these shortcomings, in this paper we develop a physics-informed dynamical variational autoencoder ($\Phi$-DVAE) to embed diverse data streams into time-evolving physical systems described by differential equations. Our approach combines a standard, possibly nonlinear, filter for the latent state-space model and a VAE, to assimilate the unstructured data into the latent dynamical system. Unstructured data, in our example systems, comes in the form of video data and velocity field measurements, however the methodology is suitably generic to allow for arbitrary unknown observation operat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#20302;&#22122;&#22768;&#30340;&#24046;&#20998;&#38544;&#31169;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#27861;&#22312;&#28857;&#38382;&#39064;&#21644;&#25104;&#23545;&#23398;&#20064;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#25512;&#23548;&#20986;&#26356;&#31934;&#30830;&#30340;&#36807;&#21097;&#39118;&#38505;&#30028;&#38480;&#12290;&#25552;&#20986;&#30340;&#31639;&#27861;&#22522;&#20110;&#26799;&#24230;&#25200;&#21160;&#65292;&#20855;&#26377;&#20248;&#21270;&#36807;&#21097;&#39118;&#38505;&#29575;&#30340;&#26368;&#20339;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2209.04188</link><description>&lt;p&gt;
&#20855;&#26377;&#20302;&#22122;&#22768;&#30340;&#24046;&#20998;&#38544;&#31169;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#27861;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Stochastic Gradient Descent with Low-Noise. (arXiv:2209.04188v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.04188
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#20302;&#22122;&#22768;&#30340;&#24046;&#20998;&#38544;&#31169;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#27861;&#22312;&#28857;&#38382;&#39064;&#21644;&#25104;&#23545;&#23398;&#20064;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#25512;&#23548;&#20986;&#26356;&#31934;&#30830;&#30340;&#36807;&#21097;&#39118;&#38505;&#30028;&#38480;&#12290;&#25552;&#20986;&#30340;&#31639;&#27861;&#22522;&#20110;&#26799;&#24230;&#25200;&#21160;&#65292;&#20855;&#26377;&#20248;&#21270;&#36807;&#21097;&#39118;&#38505;&#29575;&#30340;&#26368;&#20339;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26088;&#22312;&#20174;&#25968;&#25454;&#20013;&#25552;&#21462;&#32454;&#31890;&#24230;&#20449;&#24687;&#65292;&#20197;&#25552;&#20379;&#20934;&#30830;&#30340;&#39044;&#27979;&#65292;&#20294;&#36825;&#24448;&#24448;&#19982;&#20445;&#25252;&#38544;&#31169;&#30340;&#30446;&#26631;&#30456;&#20914;&#31361;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#24320;&#21457;&#20445;&#25252;&#38544;&#31169;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#30830;&#20445;&#33391;&#22909;&#24615;&#33021;&#30340;&#21516;&#26102;&#20445;&#25252;&#38544;&#31169;&#30340;&#23454;&#38469;&#21644;&#29702;&#35770;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#22312;&#38543;&#26426;&#20984;&#20248;&#21270;&#35774;&#32622;&#20013;&#65292;&#20851;&#27880;&#24046;&#20998;&#38544;&#31169;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#31639;&#27861;&#22312;&#38544;&#31169;&#24615;&#21644;&#25928;&#29992;&#24615;&#65288;&#36890;&#36807;&#36807;&#21097;&#39118;&#38505;&#30028;&#38480;&#34913;&#37327;&#65289;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20302;&#22122;&#22768;&#35774;&#32622;&#19979;&#30340;&#28857;&#38382;&#39064;&#65292;&#24182;&#24471;&#20986;&#20102;&#24046;&#20998;&#38544;&#31169;SGD&#31639;&#27861;&#26356;&#31934;&#30830;&#30340;&#36807;&#21097;&#39118;&#38505;&#30028;&#38480;&#12290;&#22312;&#25104;&#23545;&#23398;&#20064;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#25200;&#21160;&#30340;&#31616;&#21333;&#24046;&#20998;&#38544;&#31169;SGD&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#25152;&#25552;&#31639;&#27861;&#30340;&#26032;&#22411;&#25928;&#29992;&#30028;&#38480;&#65292;&#35777;&#26126;&#23427;&#21363;&#20351;&#22312;&#38750;&#20809;&#28369;&#24773;&#20917;&#19979;&#20063;&#33021;&#36798;&#21040;&#26368;&#20248;&#30340;&#36807;&#21097;&#39118;&#38505;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern machine learning algorithms aim to extract fine-grained information from data to provide accurate predictions, which often conflicts with the goal of privacy protection. This paper addresses the practical and theoretical importance of developing privacy-preserving machine learning algorithms that ensure good performance while preserving privacy. In this paper, we focus on the privacy and utility (measured by excess risk bounds) performances of differentially private stochastic gradient descent (SGD) algorithms in the setting of stochastic convex optimization. Specifically, we examine the pointwise problem in the low-noise setting for which we derive sharper excess risk bounds for the differentially private SGD algorithm. In the pairwise learning setting, we propose a simple differentially private SGD algorithm based on gradient perturbation. Furthermore, we develop novel utility bounds for the proposed algorithm, proving that it achieves optimal excess risk rates even for non-sm
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;&#27010;&#29575;&#24314;&#27169;&#30340;&#20840;&#27010;&#29575;&#28145;&#24230;&#27169;&#22411;&#65292;&#29992;&#20110;&#23398;&#20064;&#21442;&#25968;&#21270;&#20559;&#24494;&#20998;&#26041;&#31243;&#20013;&#30340;&#21069;&#21521;&#21644;&#21453;&#21521;&#26144;&#23556;&#12290;&#27169;&#22411;&#36890;&#36807;&#26368;&#22823;&#21270;&#35266;&#23519;&#21040;&#30340;&#38646;&#27531;&#24046;&#30340;&#27010;&#29575;&#26469;&#36827;&#34892;&#35757;&#32451;&#65292;&#19981;&#38656;&#35201;&#29420;&#31435;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2208.04856</link><description>&lt;p&gt;
&#21442;&#25968;&#21270;&#20559;&#24494;&#20998;&#26041;&#31243;&#20013;&#21069;&#21521;&#21644;&#21453;&#21521;&#38382;&#39064;&#30340;&#20840;&#27010;&#29575;&#28145;&#24230;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Fully probabilistic deep models for forward and inverse problems in parametric PDEs. (arXiv:2208.04856v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.04856
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;&#27010;&#29575;&#24314;&#27169;&#30340;&#20840;&#27010;&#29575;&#28145;&#24230;&#27169;&#22411;&#65292;&#29992;&#20110;&#23398;&#20064;&#21442;&#25968;&#21270;&#20559;&#24494;&#20998;&#26041;&#31243;&#20013;&#30340;&#21069;&#21521;&#21644;&#21453;&#21521;&#26144;&#23556;&#12290;&#27169;&#22411;&#36890;&#36807;&#26368;&#22823;&#21270;&#35266;&#23519;&#21040;&#30340;&#38646;&#27531;&#24046;&#30340;&#27010;&#29575;&#26469;&#36827;&#34892;&#35757;&#32451;&#65292;&#19981;&#38656;&#35201;&#29420;&#31435;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#29289;&#29702;&#39537;&#21160;&#30340;&#28145;&#24230;&#28508;&#21464;&#37327;&#27169;&#22411;&#65288;PDDLVM&#65289;&#65292;&#29992;&#20110;&#21516;&#26102;&#23398;&#20064;&#21442;&#25968;&#21270;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#30340;&#21442;&#25968;&#21040;&#35299;&#65288;&#21069;&#21521;&#65289;&#21644;&#35299;&#21040;&#21442;&#25968;&#65288;&#21453;&#21521;&#65289;&#26144;&#23556;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#20256;&#32479;&#30340;PDE&#31163;&#25955;&#21270;&#25216;&#26415;&#12289;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12289;&#27010;&#29575;&#24314;&#27169;&#21644;&#21464;&#20998;&#25512;&#26029;&#65292;&#24418;&#25104;&#20102;&#19968;&#20010;&#23436;&#20840;&#27010;&#29575;&#19968;&#33268;&#30340;&#26694;&#26550;&#12290;&#22312;&#25152;&#20551;&#35774;&#30340;&#27010;&#29575;&#27169;&#22411;&#20013;&#65292;&#21069;&#21521;&#21644;&#21453;&#21521;&#26144;&#23556;&#22343;&#34987;&#36817;&#20284;&#20026;&#30001;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#30340;&#39640;&#26031;&#20998;&#24067;&#12290;&#25105;&#20204;&#20551;&#35774;PDE&#27531;&#24046;&#26159;&#19968;&#20010;&#35266;&#27979;&#21040;&#30340;&#38543;&#26426;&#21521;&#37327;&#65292;&#20540;&#20026;&#38646;&#65292;&#22240;&#27492;&#25105;&#20204;&#23558;&#20854;&#24314;&#27169;&#20026;&#19968;&#20010;&#22343;&#20540;&#20026;&#38646;&#12289;&#29992;&#25143;&#25351;&#23450;&#21327;&#26041;&#24046;&#30340;&#38543;&#26426;&#21521;&#37327;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#26368;&#22823;&#21270;&#35266;&#23519;&#21040;&#38646;&#27531;&#24046;&#30340;&#27010;&#29575;&#65288;&#21363;&#35777;&#25454;&#25110;&#36793;&#38469;&#20284;&#28982;&#65289;&#26469;&#36827;&#34892;&#35757;&#32451;&#65292;&#26368;&#22823;&#21270;&#35777;&#25454;&#19979;&#30028;&#65288;ELBO&#65289;&#12290;&#22240;&#27492;&#65292;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#20219;&#20309;&#29420;&#31435;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a physics-driven deep latent variable model (PDDLVM) to learn simultaneously parameter-to-solution (forward) and solution-to-parameter (inverse) maps of parametric partial differential equations (PDEs). Our formulation leverages conventional PDE discretization techniques, deep neural networks, probabilistic modelling, and variational inference to assemble a fully probabilistic coherent framework. In the posited probabilistic model, both the forward and inverse maps are approximated as Gaussian distributions with a mean and covariance parameterized by deep neural networks. The PDE residual is assumed to be an observed random vector of value zero, hence we model it as a random vector with a zero mean and a user-prescribed covariance. The model is trained by maximizing the probability, that is the evidence or marginal likelihood, of observing a residual of zero by maximizing the evidence lower bound (ELBO). Consequently, the proposed methodology does not require any independe
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#22238;&#39038;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#22522;&#20110;&#25490;&#21517;&#21487;&#20998;&#35299;&#25439;&#22833;&#20989;&#25968;&#30340;&#30740;&#31350;&#24182;&#25552;&#20379;&#20102;&#20998;&#31867;&#27861;&#12290;&#23427;&#35752;&#35770;&#20102;&#21487;&#20998;&#35299;&#24615;&#19982;&#20248;&#21270;&#31639;&#27861;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#21015;&#20030;&#20102;&#25490;&#21517;&#25439;&#22833;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#26368;&#26032;&#24212;&#29992;&#21644;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2207.08768</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#22522;&#20110;&#25490;&#21517;&#21487;&#20998;&#35299;&#25439;&#22833;&#20989;&#25968;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Rank-based Decomposable Losses in Machine Learning: A Survey. (arXiv:2207.08768v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.08768
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#22238;&#39038;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#22522;&#20110;&#25490;&#21517;&#21487;&#20998;&#35299;&#25439;&#22833;&#20989;&#25968;&#30340;&#30740;&#31350;&#24182;&#25552;&#20379;&#20102;&#20998;&#31867;&#27861;&#12290;&#23427;&#35752;&#35770;&#20102;&#21487;&#20998;&#35299;&#24615;&#19982;&#20248;&#21270;&#31639;&#27861;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#21015;&#20030;&#20102;&#25490;&#21517;&#25439;&#22833;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#26368;&#26032;&#24212;&#29992;&#21644;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#25439;&#22833;&#20989;&#25968;&#35774;&#35745;&#20013;&#21306;&#20998;&#20010;&#20307;&#25439;&#22833;&#19982;&#32858;&#21512;&#25439;&#22833;&#30340;&#37325;&#35201;&#33539;&#24335;&#12290;&#20010;&#20307;&#25439;&#22833;&#35780;&#20272;&#27169;&#22411;&#22312;&#26679;&#26412;&#19978;&#30340;&#36136;&#37327;&#65292;&#32780;&#32858;&#21512;&#25439;&#22833;&#23558;&#27599;&#20010;&#35757;&#32451;&#26679;&#26412;&#30340;&#20010;&#20307;&#25439;&#22833;/&#20998;&#25968;&#32452;&#21512;&#36215;&#26469;&#12290;&#20004;&#32773;&#37117;&#26377;&#19968;&#20010;&#20849;&#21516;&#30340;&#36807;&#31243;&#65292;&#23558;&#19968;&#32452;&#20010;&#20307;&#20540;&#32858;&#21512;&#25104;&#21333;&#20010;&#25968;&#23383;&#20540;&#12290;&#25490;&#21517;&#39034;&#24207;&#21453;&#26144;&#20102;&#35774;&#35745;&#25439;&#22833;&#20013;&#20010;&#20307;&#20540;&#20043;&#38388;&#26368;&#22522;&#26412;&#30340;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#21487;&#20998;&#35299;&#24615;&#65292;&#20854;&#20013;&#25439;&#22833;&#21487;&#20197;&#20998;&#35299;&#20026;&#19968;&#32452;&#20010;&#20307;&#39033;&#30340;&#38598;&#21512;&#65292;&#25104;&#20026;&#32452;&#32455;&#25439;&#22833;/&#20998;&#25968;&#30340;&#37325;&#35201;&#23646;&#24615;&#12290;&#26412;&#32508;&#36848;&#31995;&#32479;&#20840;&#38754;&#22320;&#22238;&#39038;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#22522;&#20110;&#25490;&#21517;&#21487;&#20998;&#35299;&#25439;&#22833;&#20989;&#25968;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#25353;&#32858;&#21512;&#25439;&#22833;&#21644;&#20010;&#20307;&#25439;&#22833;&#35270;&#35282;&#30340;&#26032;&#25439;&#22833;&#20989;&#25968;&#20998;&#31867;&#27861;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#24418;&#25104;&#36825;&#31181;&#25439;&#22833;&#30340;&#32858;&#21512;&#22120;&#65292;&#23427;&#20204;&#26159;&#38598;&#21512;&#20989;&#25968;&#30340;&#31034;&#20363;&#12290;&#25105;&#20204;&#32452;&#32455;&#20102;&#26377;&#30417;&#30563;&#23398;&#20064;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#22522;&#20110;&#25490;&#21517;&#30340;&#21487;&#20998;&#35299;&#25439;&#22833;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#25439;&#22833;&#30340;&#21487;&#20998;&#35299;&#24615;&#19982;&#20248;&#21270;&#31639;&#27861;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#25490;&#21517;&#25439;&#22833;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#26368;&#26032;&#24212;&#29992;&#21644;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works have revealed an essential paradigm in designing loss functions that differentiate individual losses vs. aggregate losses. The individual loss measures the quality of the model on a sample, while the aggregate loss combines individual losses/scores over each training sample. Both have a common procedure that aggregates a set of individual values to a single numerical value. The ranking order reflects the most fundamental relation among individual values in designing losses. In addition, decomposability, in which a loss can be decomposed into an ensemble of individual terms, becomes a significant property of organizing losses/scores. This survey provides a systematic and comprehensive review of rank-based decomposable losses in machine learning. Specifically, we provide a new taxonomy of loss functions that follows the perspectives of aggregate loss and individual loss. We identify the aggregator to form such losses, which are examples of set functions. We organize the rank
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#24352;&#37327;&#32593;&#32476;&#26550;&#26500;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#26426;&#22120;&#23398;&#20064;&#20013;&#20855;&#26377;&#28508;&#22312;&#20248;&#21183;&#65292;&#24182;&#25552;&#20986;&#20102;&#30830;&#20445;&#40065;&#26834;&#24615;&#30340;&#26126;&#30830;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2202.12319</link><description>&lt;p&gt;
&#20445;&#25252;&#38544;&#31169;&#30340;&#24352;&#37327;&#32593;&#32476;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Privacy-preserving machine learning with tensor networks. (arXiv:2202.12319v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.12319
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#24352;&#37327;&#32593;&#32476;&#26550;&#26500;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#26426;&#22120;&#23398;&#20064;&#20013;&#20855;&#26377;&#28508;&#22312;&#20248;&#21183;&#65292;&#24182;&#25552;&#20986;&#20102;&#30830;&#20445;&#40065;&#26834;&#24615;&#30340;&#26126;&#30830;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24352;&#37327;&#32593;&#32476;&#34987;&#24191;&#27867;&#29992;&#20110;&#25552;&#20379;&#20302;&#33021;&#37327;&#24577;&#30340;&#39640;&#25928;&#34920;&#31034;&#65292;&#26368;&#36817;&#34987;&#25552;&#20986;&#20316;&#20026;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#30340;&#19968;&#31181;&#26032;&#26041;&#27861;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#24352;&#37327;&#32593;&#32476;&#26550;&#26500;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#26426;&#22120;&#23398;&#20064;&#20013;&#20855;&#26377;&#28508;&#22312;&#20248;&#21183;&#65292;&#36825;&#23545;&#20110;&#22788;&#29702;&#21307;&#30103;&#35760;&#24405;&#31561;&#20219;&#21153;&#38750;&#24120;&#37325;&#35201;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#20013;&#23384;&#22312;&#30340;&#26032;&#38544;&#31169;&#28431;&#27934;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#20013;&#36827;&#34892;&#20102;&#35828;&#26126;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#30830;&#20445;&#23545;&#36825;&#31181;&#28431;&#27934;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#26126;&#30830;&#26465;&#20214;&#65292;&#36825;&#28041;&#21450;&#21040;&#22312;&#35268;&#33539;&#23545;&#31216;&#24615;&#19979;&#31561;&#20215;&#30340;&#27169;&#22411;&#30340;&#21051;&#30011;&#12290;&#25105;&#20204;&#20005;&#26684;&#35777;&#26126;&#20102;&#24352;&#37327;&#32593;&#32476;&#26550;&#26500;&#28385;&#36275;&#36825;&#20123;&#26465;&#20214;&#12290;&#22312;&#27492;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#30697;&#38453;&#20056;&#31215;&#24577;&#30340;&#35268;&#33539;&#24418;&#24335;&#65292;&#20855;&#26377;&#39640;&#24230;&#30340;&#35268;&#24459;&#24615;&#24182;&#20462;&#27491;&#20102;&#27531;&#20313;&#35268;&#33539;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tensor networks, widely used for providing efficient representations of low-energy states of local quantum many-body systems, have been recently proposed as machine learning architectures which could present advantages with respect to traditional ones. In this work we show that tensor network architectures have especially prospective properties for privacy-preserving machine learning, which is important in tasks such as the processing of medical records. First, we describe a new privacy vulnerability that is present in feedforward neural networks, illustrating it in synthetic and real-world datasets. Then, we develop well-defined conditions to guarantee robustness to such vulnerability, which involve the characterization of models equivalent under gauge symmetry. We rigorously prove that such conditions are satisfied by tensor-network architectures. In doing so, we define a novel canonical form for matrix product states, which has a high degree of regularity and fixes the residual gaug
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#22312;&#20195;&#29702;&#20351;&#29992;&#22797;&#26434;&#30340;&#8220;&#40657;&#30418;&#8221;&#39044;&#27979;&#20989;&#25968;&#36827;&#34892;&#20915;&#31574;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#31639;&#27861;&#20915;&#31574;&#36827;&#34892;&#26368;&#20248;&#35843;&#25511;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#38480;&#21046;&#20195;&#29702;&#20351;&#29992;&#36879;&#26126;&#24230;&#36275;&#22815;&#39640;&#30340;&#39044;&#27979;&#20989;&#25968;&#26159;&#20302;&#25928;&#30340;&#65292;&#32780;&#38024;&#23545;&#28608;&#21169;&#20559;&#24046;&#28304;&#22836;&#30340;&#30446;&#26631;&#21270;&#24037;&#20855;&#21487;&#20197;&#25552;&#20379;&#27425;&#20248;&#35299;&#20915;&#26041;&#26696;&#65292;&#20174;&#32780;&#25913;&#21892;&#31119;&#21033;&#12290;</title><link>http://arxiv.org/abs/2110.03443</link><description>&lt;p&gt;
&#25581;&#24320;&#40657;&#30418;&#23376;&#65306;&#35843;&#25511;&#31639;&#27861;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Unpacking the Black Box: Regulating Algorithmic Decisions. (arXiv:2110.03443v2 [econ.GN] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.03443
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#22312;&#20195;&#29702;&#20351;&#29992;&#22797;&#26434;&#30340;&#8220;&#40657;&#30418;&#8221;&#39044;&#27979;&#20989;&#25968;&#36827;&#34892;&#20915;&#31574;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#31639;&#27861;&#20915;&#31574;&#36827;&#34892;&#26368;&#20248;&#35843;&#25511;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#38480;&#21046;&#20195;&#29702;&#20351;&#29992;&#36879;&#26126;&#24230;&#36275;&#22815;&#39640;&#30340;&#39044;&#27979;&#20989;&#25968;&#26159;&#20302;&#25928;&#30340;&#65292;&#32780;&#38024;&#23545;&#28608;&#21169;&#20559;&#24046;&#28304;&#22836;&#30340;&#30446;&#26631;&#21270;&#24037;&#20855;&#21487;&#20197;&#25552;&#20379;&#27425;&#20248;&#35299;&#20915;&#26041;&#26696;&#65292;&#20174;&#32780;&#25913;&#21892;&#31119;&#21033;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#19968;&#20010;&#20195;&#29702;&#20351;&#29992;&#22797;&#26434;&#30340;&#8220;&#40657;&#30418;&#8221;&#39044;&#27979;&#20989;&#25968;&#36827;&#34892;&#20915;&#31574;&#65288;&#22914;&#36151;&#27454;&#12289;&#21307;&#30103;&#27979;&#35797;&#25110;&#25307;&#32856;&#65289;&#19988;&#22996;&#25176;&#20154;&#22312;&#20102;&#35299;&#20195;&#29702;&#30340;&#40657;&#30418;&#27169;&#22411;&#26041;&#38754;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#26368;&#20248;&#22320;&#35843;&#25511;&#39044;&#27979;&#31639;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#21482;&#35201;&#35825;&#23548;&#19981;&#36275;&#65292;&#19988;&#26368;&#20248;&#39044;&#27979;&#20989;&#25968;&#36275;&#22815;&#22797;&#26434;&#65292;&#23558;&#20195;&#29702;&#38480;&#21046;&#22312;&#36275;&#22815;&#36879;&#26126;&#30340;&#39044;&#27979;&#20989;&#25968;&#20013;&#26159;&#20302;&#25928;&#30340;&#12290;&#31639;&#27861;&#23457;&#35745;&#26377;&#21161;&#20110;&#25552;&#39640;&#31119;&#21033;&#65292;&#20294;&#20854;&#25910;&#30410;&#21462;&#20915;&#20110;&#23457;&#35745;&#24037;&#20855;&#30340;&#35774;&#35745;&#12290;&#35768;&#22810;&#35299;&#37322;&#24037;&#20855;&#20542;&#21521;&#20110;&#26368;&#23567;&#21270;&#25972;&#20307;&#20449;&#24687;&#25439;&#22833;&#65292;&#20294;&#36825;&#36890;&#24120;&#26159;&#20302;&#25928;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#38598;&#20013;&#20110;&#35299;&#37322;&#39044;&#27979;&#20989;&#25968;&#30340;&#24179;&#22343;&#34892;&#20026;&#12290;&#38024;&#23545;&#24615;&#30340;&#24037;&#20855;&#65292;&#22914;&#38024;&#23545;&#28608;&#21169;&#20559;&#24046;&#28304;&#22836;&#65288;&#22914;&#36807;&#22810;&#30340;&#20551;&#38451;&#24615;&#25110;&#31181;&#26063;&#24046;&#24322;&#65289;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#25552;&#20379;&#27425;&#20248;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#25105;&#20204;&#29702;&#35770;&#30340;&#23454;&#35777;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show how to optimally regulate prediction algorithms in a world where an agent uses complex 'black-box' prediction functions to make decisions such as lending, medical testing, or hiring, and where a principal is limited in how much she can learn about the agent's black-box model. We show that limiting agents to prediction functions that are simple enough to be fully transparent is inefficient as long as the misalignment is limited and first-best prediction functions are sufficiently complex. Algorithmic audits can improve welfare, but the gains depend on the design of the audit tools. Tools that focus on minimizing overall information loss, the focus of many explainer tools, will generally be inefficient since they focus on explaining the average behavior of the prediction function. Targeted tools that focus on the source of incentive misalignment, e.g., excess false positives or racial disparities, can provide second-best solutions. We provide empirical support for our theoretical
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#38543;&#26426;&#21333;&#32500;&#25628;&#32034;&#30340;&#25968;&#25454;&#22686;&#24378;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#20165;&#20351;&#29992;6&#27425;&#35757;&#32451;&#21363;&#21487;&#33719;&#24471;&#19982;&#20351;&#29992;100&#27425;&#35757;&#32451;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2106.08756</link><description>&lt;p&gt;
&#36890;&#36807;&#38543;&#26426;&#21333;&#32500;&#25628;&#32034;&#23454;&#29616;&#25968;&#25454;&#22686;&#24378;&#30340;&#33258;&#21160;&#21270;
&lt;/p&gt;
&lt;p&gt;
Automating Augmentation Through Random Unidimensional Search. (arXiv:2106.08756v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.08756
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#38543;&#26426;&#21333;&#32500;&#25628;&#32034;&#30340;&#25968;&#25454;&#22686;&#24378;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#20165;&#20351;&#29992;6&#27425;&#35757;&#32451;&#21363;&#21487;&#33719;&#24471;&#19982;&#20351;&#29992;100&#27425;&#35757;&#32451;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#32773;&#26469;&#35828;&#65292;&#25214;&#21040;&#35757;&#32451;&#26399;&#38388;&#26368;&#20339;&#30340;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#23545;&#20110;&#21462;&#24471;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#32780;&#35328;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#27492;&#65292;&#31038;&#21306;&#24050;&#32463;&#35265;&#35777;&#20102;&#35768;&#22810;&#21162;&#21147;&#26469;&#33258;&#21160;&#21270;&#25214;&#21040;&#36866;&#21512;&#20219;&#20309;&#20219;&#21153;&#30340;&#23436;&#32654;&#22686;&#24378;&#31243;&#24207;&#30340;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26159;&#26368;&#26032;&#30340;&#23574;&#31471;&#26041;&#27861;&#20063;&#20250;&#24102;&#26469;&#24040;&#22823;&#30340;&#35745;&#31639;&#24320;&#38144;&#65292;&#21487;&#33021;&#38656;&#35201;&#22810;&#36798;100&#20010;&#23436;&#25972;&#27169;&#22411;&#30340;&#35757;&#32451;&#26469;&#30830;&#23450;&#29702;&#24819;&#30340;&#37197;&#32622;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#20165;6&#27425;&#35757;&#32451;&#23601;&#33021;&#36798;&#21040;&#30456;&#21516;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#38543;&#26426;&#21333;&#32500;&#22686;&#24378;&#26041;&#27861;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#12290;&#28304;&#20195;&#30721;&#21487;&#22312;https://github.com/fastestimator/RUA/tree/v1.0&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is no secret amongst deep learning researchers that finding the optimal data augmentation strategy during training can mean the difference between state-of-the-art performance and a run-of-the-mill result. To that end, the community has seen many efforts to automate the process of finding the perfect augmentation procedure for any task at hand. Unfortunately, even recent cutting-edge methods bring massive computational overhead, requiring as many as 100 full model trainings to settle on an ideal configuration. We show how to achieve equivalent performance using just 6 trainings with Random Unidimensional Augmentation. Source code is available at https://github.com/fastestimator/RUA/tree/v1.0
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#26367;&#20154;&#21475;&#21644;&#20195;&#29702;&#25511;&#21046;&#31070;&#32463;&#32593;&#32476;&#65288;APAC-Net&#65289;&#26469;&#35299;&#20915;&#39640;&#32500;&#24230;&#38543;&#26426;&#22343;&#22330;&#21338;&#24328;&#65288;MFG&#65289;&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;&#21464;&#20998;&#21407;&#22987;-&#23545;&#20598;&#32467;&#26500;&#21644;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#20215;&#20540;&#21644;&#23494;&#24230;&#20989;&#25968;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#39640;&#32500;&#24230;MFG&#38382;&#39064;&#20013;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2002.10113</link><description>&lt;p&gt;
&#23558;&#20154;&#21475;&#21644;&#25511;&#21046;&#31070;&#32463;&#32593;&#32476;&#20132;&#26367;&#24212;&#29992;&#20110;&#35299;&#20915;&#39640;&#32500;&#24230;&#38543;&#26426;&#22343;&#22330;&#21338;&#24328;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Alternating the Population and Control Neural Networks to Solve High-Dimensional Stochastic Mean-Field Games. (arXiv:2002.10113v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2002.10113
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#26367;&#20154;&#21475;&#21644;&#20195;&#29702;&#25511;&#21046;&#31070;&#32463;&#32593;&#32476;&#65288;APAC-Net&#65289;&#26469;&#35299;&#20915;&#39640;&#32500;&#24230;&#38543;&#26426;&#22343;&#22330;&#21338;&#24328;&#65288;MFG&#65289;&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;&#21464;&#20998;&#21407;&#22987;-&#23545;&#20598;&#32467;&#26500;&#21644;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#20215;&#20540;&#21644;&#23494;&#24230;&#20989;&#25968;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#39640;&#32500;&#24230;MFG&#38382;&#39064;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#26367;&#20154;&#21475;&#21644;&#20195;&#29702;&#25511;&#21046;&#31070;&#32463;&#32593;&#32476;&#65288;APAC-Net&#65289;&#26469;&#35299;&#20915;&#38543;&#26426;&#22343;&#22330;&#21338;&#24328;&#65288;MFG&#65289;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#38024;&#23545;&#39640;&#32500;&#24230;&#30340;MFG&#23454;&#20363;&#65292;&#36825;&#20123;&#23454;&#20363;&#20351;&#29992;&#29616;&#26377;&#35299;&#20915;&#26041;&#27861;&#26080;&#27861;&#35299;&#20915;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#27493;&#39588;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21033;&#29992;MFG&#25152;&#23637;&#31034;&#30340;&#21464;&#20998;&#21407;&#22987;-&#23545;&#20598;&#32467;&#26500;&#65292;&#24182;&#23558;&#20854;&#24418;&#24335;&#21270;&#20026;&#20984;-&#20985;&#38797;&#28857;&#38382;&#39064;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20998;&#21035;&#36890;&#36807;&#20004;&#20010;&#31070;&#32463;&#32593;&#32476;&#23545;&#20215;&#20540;&#21644;&#23494;&#24230;&#20989;&#25968;&#36827;&#34892;&#21442;&#25968;&#21270;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#35299;&#20915;MFG&#21487;&#20197;&#34987;&#35299;&#37322;&#20026;&#35757;&#32451;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#30340;&#29305;&#20363;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#39640;&#36798;100&#32500;MFG&#38382;&#39064;&#19978;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present APAC-Net, an alternating population and agent control neural network for solving stochastic mean field games (MFGs). Our algorithm is geared toward high-dimensional instances of MFGs that are beyond reach with existing solution methods. We achieve this in two steps. First, we take advantage of the underlying variational primal-dual structure that MFGs exhibit and phrase it as a convex-concave saddle point problem. Second, we parameterize the value and density functions by two neural networks, respectively. By phrasing the problem in this manner, solving the MFG can be interpreted as a special case of training a generative adversarial network (GAN). We show the potential of our method on up to 100-dimensional MFG problems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26377;&#31526;&#21495;&#30340;&#36845;&#20195;&#38543;&#26426;&#26862;&#26519;&#65288;siRF&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#25512;&#26029;Drosophila melanogaster&#20013;&#22686;&#24378;&#23376;&#20803;&#32032;&#21608;&#22260;&#30340;&#36716;&#24405;&#22240;&#23376;&#20043;&#38388;&#30340;&#35843;&#25511;&#30456;&#20114;&#20316;&#29992;&#21644;&#21151;&#33021;&#32467;&#21512;&#31614;&#21517;&#12290;</title><link>http://arxiv.org/abs/1810.07287</link><description>&lt;p&gt;
&#29992;&#26377;&#31526;&#21495;&#30340;&#36845;&#20195;&#38543;&#26426;&#26862;&#26519;&#35782;&#21035;&#22686;&#24378;&#23376;&#30456;&#20851;&#30340;&#36716;&#24405;&#22240;&#23376;&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
Signed iterative random forests to identify enhancer-associated transcription factor binding. (arXiv:1810.07287v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1810.07287
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26377;&#31526;&#21495;&#30340;&#36845;&#20195;&#38543;&#26426;&#26862;&#26519;&#65288;siRF&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#25512;&#26029;Drosophila melanogaster&#20013;&#22686;&#24378;&#23376;&#20803;&#32032;&#21608;&#22260;&#30340;&#36716;&#24405;&#22240;&#23376;&#20043;&#38388;&#30340;&#35843;&#25511;&#30456;&#20114;&#20316;&#29992;&#21644;&#21151;&#33021;&#32467;&#21512;&#31614;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#20934;&#30340;ChIP-seq&#23792;&#20540;&#35843;&#29992;&#27969;&#31243;&#26088;&#22312;&#21306;&#20998;&#21508;&#20010;&#22522;&#22240;&#32452;&#20803;&#32032;&#30340;&#29983;&#21270;&#21487;&#37325;&#22797;&#20449;&#21495;&#21644;&#32972;&#26223;&#22122;&#22768;&#12290;&#28982;&#32780;&#65292;&#20165;&#20973;&#21487;&#37325;&#22797;&#24615;&#24182;&#19981;&#33021;&#26263;&#31034;&#21151;&#33021;&#35843;&#25511;&#65288;&#20363;&#22914;&#22686;&#24378;&#23376;&#27963;&#21270;&#12289;&#21487;&#36873;&#21098;&#25509;&#65289;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#12289;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65306;&#26377;&#31526;&#21495;&#30340;&#36845;&#20195;&#38543;&#26426;&#26862;&#26519;&#65288;siRF&#65289;&#65292;&#25105;&#20204;&#29992;&#23427;&#26469;&#25512;&#26029;Drosophila melanogaster&#20013;&#22686;&#24378;&#23376;&#20803;&#32032;&#21608;&#22260;&#30340;&#36716;&#24405;&#22240;&#23376;&#20043;&#38388;&#30340;&#35843;&#25511;&#30456;&#20114;&#20316;&#29992;&#21644;&#21151;&#33021;&#32467;&#21512;&#31614;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
Standard ChIP-seq peak calling pipelines seek to differentiate biochemically reproducible signals of individual genomic elements from background noise. However, reproducibility alone does not imply functional regulation (e.g., enhancer activation, alternative splicing). Here we present a general-purpose, interpretable machine learning method: signed iterative random forests (siRF), which we use to infer regulatory interactions among transcription factors and functional binding signatures surrounding enhancer elements in Drosophila melanogaster.
&lt;/p&gt;</description></item></channel></rss>