<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>TRABSA&#26159;&#19968;&#20010;&#38598;&#25104;&#20102;transformer&#26550;&#26500;&#12289;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;BiLSTM&#32593;&#32476;&#30340;&#28151;&#21512;&#26694;&#26550;&#65292;&#21033;&#29992;RoBERTa&#22312;&#22823;&#37327;&#25512;&#29305;&#19978;&#35757;&#32451;&#65292;&#22635;&#34917;&#20102;&#24773;&#24863;&#20998;&#26512;&#39046;&#22495;&#30340;&#24046;&#36317;&#65292;&#23454;&#29616;&#20102;94%&#30340;&#20934;&#30830;&#24615;&#21644;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2404.00297</link><description>&lt;p&gt;
TRABSA&#65306;&#20351;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;BiLSTM&#21644;Twitter-RoBERTa&#36827;&#34892;&#21487;&#35299;&#37322;&#30340;&#25512;&#25991;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
TRABSA: Interpretable Sentiment Analysis of Tweets using Attention-based BiLSTM and Twitter-RoBERTa
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00297
&lt;/p&gt;
&lt;p&gt;
TRABSA&#26159;&#19968;&#20010;&#38598;&#25104;&#20102;transformer&#26550;&#26500;&#12289;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;BiLSTM&#32593;&#32476;&#30340;&#28151;&#21512;&#26694;&#26550;&#65292;&#21033;&#29992;RoBERTa&#22312;&#22823;&#37327;&#25512;&#29305;&#19978;&#35757;&#32451;&#65292;&#22635;&#34917;&#20102;&#24773;&#24863;&#20998;&#26512;&#39046;&#22495;&#30340;&#24046;&#36317;&#65292;&#23454;&#29616;&#20102;94%&#30340;&#20934;&#30830;&#24615;&#21644;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#20998;&#26512;&#23545;&#20110;&#29702;&#35299;&#20844;&#20247;&#33286;&#35770;&#21644;&#28040;&#36153;&#32773;&#34892;&#20026;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#27169;&#22411;&#38754;&#20020;&#30528;&#35821;&#35328;&#22810;&#26679;&#24615;&#12289;&#27867;&#21270;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;TRABSA&#65292;&#36825;&#26159;&#19968;&#20010;&#38598;&#25104;&#20102;&#22522;&#20110;transformer&#30340;&#26550;&#26500;&#12289;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;BiLSTM&#32593;&#32476;&#30340;&#28151;&#21512;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#21033;&#29992;&#22312;124M&#26465;&#25512;&#25991;&#19978;&#35757;&#32451;&#30340;RoBERTa&#65292;&#25105;&#20204;&#22635;&#34917;&#20102;&#24773;&#24863;&#20998;&#26512;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#24046;&#36317;&#65292;&#30830;&#20445;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#23558;&#26469;&#33258;32&#20010;&#22269;&#23478;&#21644;&#32654;&#22269;&#21508;&#24030;&#30340;&#25512;&#25991;&#19982;&#25968;&#25454;&#38598;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20845;&#31181;&#35789;&#23884;&#20837;&#25216;&#26415;&#21644;&#19977;&#31181;&#22522;&#20110;&#35789;&#20856;&#30340;&#26631;&#27880;&#25216;&#26415;&#65292;&#24182;&#36873;&#25321;&#20102;&#26368;&#20339;&#25216;&#26415;&#20197;&#23454;&#29616;&#26368;&#20339;&#24773;&#24863;&#20998;&#26512;&#25928;&#26524;&#12290;TRABSA&#20197;94%&#30340;&#20934;&#30830;&#24615;&#21644;&#26174;&#33879;&#30340;&#31934;&#30830;&#24230;&#12289;&#21484;&#22238;&#29575;&#21644;F1&#24471;&#20998;&#22686;&#30410;&#65292;&#32988;&#36807;&#20102;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#26174;&#31034;&#20102;&#19968;&#33268;&#30340;&#20248;&#36234;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;SHAP&#21644;LIME&#20998;&#26512;&#25552;&#39640;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#22686;&#24378;&#20102;&#20449;&#24515;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00297v1 Announce Type: new  Abstract: Sentiment analysis is crucial for understanding public opinion and consumer behavior. Existing models face challenges with linguistic diversity, generalizability, and explainability. We propose TRABSA, a hybrid framework integrating transformer-based architectures, attention mechanisms, and BiLSTM networks to address this. Leveraging RoBERTa-trained on 124M tweets, we bridge gaps in sentiment analysis benchmarks, ensuring state-of-the-art accuracy. Augmenting datasets with tweets from 32 countries and US states, we compare six word-embedding techniques and three lexicon-based labeling techniques, selecting the best for optimal sentiment analysis. TRABSA outperforms traditional ML and deep learning models with 94% accuracy and significant precision, recall, and F1-score gains. Evaluation across diverse datasets demonstrates consistent superiority and generalizability. SHAP and LIME analyses enhance interpretability, improving confidence i
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28145;&#20837;&#30740;&#31350;Transformer-based&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#23454;&#22238;&#24518;&#20219;&#21153;&#20013;&#30340;&#26426;&#21046;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#38646;/&#23569;&#27425;&#26679;&#26412;&#24773;&#20917;&#19979;&#30340;&#29305;&#23450;&#20219;&#21153;&#22836;&#12289;MLP&#23618;&#21644;&#27531;&#24046;&#27969;&#30340;&#21151;&#33021;&#65292;&#20197;&#21450;&#25239;&#36807;&#24230;&#33258;&#20449;&#26426;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.19521</link><description>&lt;p&gt;
&#35299;&#37322;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#23454;&#22238;&#24518;&#20013;&#30340;&#20851;&#38190;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Interpreting Key Mechanisms of Factual Recall in Transformer-Based Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19521
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28145;&#20837;&#30740;&#31350;Transformer-based&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#23454;&#22238;&#24518;&#20219;&#21153;&#20013;&#30340;&#26426;&#21046;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#38646;/&#23569;&#27425;&#26679;&#26412;&#24773;&#20917;&#19979;&#30340;&#29305;&#23450;&#20219;&#21153;&#22836;&#12289;MLP&#23618;&#21644;&#27531;&#24046;&#27969;&#30340;&#21151;&#33021;&#65292;&#20197;&#21450;&#25239;&#36807;&#24230;&#33258;&#20449;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;Transformer-based&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#23454;&#22238;&#24518;&#20219;&#21153;&#20013;&#25152;&#37319;&#29992;&#30340;&#26426;&#21046;&#12290;&#22312;&#38646;&#27425;&#26679;&#26412;&#24773;&#20917;&#19979;&#65292;&#32473;&#23450;&#31867;&#20284;&#8220;&#27861;&#22269;&#30340;&#39318;&#37117;&#26159;&#8221;&#30340;&#25552;&#31034;&#65292;&#29305;&#23450;&#20219;&#21153;&#30340;&#27880;&#24847;&#21147;&#22836;&#20250;&#20174;&#19978;&#19979;&#25991;&#20013;&#25552;&#21462;&#20027;&#39064;&#23454;&#20307;&#65292;&#22914;&#8220;&#27861;&#22269;&#8221;&#65292;&#24182;&#23558;&#20854;&#20256;&#36882;&#32473;&#21518;&#32493;&#30340;MLP&#20197;&#22238;&#24518;&#25152;&#38656;&#30340;&#31572;&#26696;&#65292;&#22914;&#8220;&#24052;&#40654;&#8221;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#26088;&#22312;&#23558;MLP&#30340;&#36755;&#20986;&#20998;&#35299;&#20026;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#32452;&#20214;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#37327;&#21270;&#20102;&#36319;&#38543;&#36825;&#20123;&#29305;&#23450;&#20219;&#21153;&#22836;&#30340;MLP&#23618;&#30340;&#21151;&#33021;&#12290;&#22312;&#27531;&#24046;&#27969;&#20013;&#65292;&#23427;&#20250;&#25830;&#38500;&#25110;&#25918;&#22823;&#26469;&#33258;&#21508;&#20010;&#22836;&#30340;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#23427;&#20250;&#29983;&#25104;&#19968;&#20010;&#32452;&#20214;&#65292;&#23558;&#27531;&#24046;&#27969;&#37325;&#26032;&#23450;&#21521;&#21040;&#39044;&#26399;&#31572;&#26696;&#30340;&#26041;&#21521;&#12290;&#36825;&#20123;&#38646;&#27425;&#26426;&#21046;&#20063;&#36866;&#29992;&#20110;&#23569;&#27425;&#26679;&#26412;&#24773;&#20917;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#19968;&#31181;&#24191;&#27867;&#23384;&#22312;&#30340;&#25239;&#36807;&#24230;&#33258;&#20449;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19521v1 Announce Type: cross  Abstract: In this paper, we deeply explore the mechanisms employed by Transformer-based language models in factual recall tasks. In zero-shot scenarios, given a prompt like "The capital of France is," task-specific attention heads extract the topic entity, such as "France," from the context and pass it to subsequent MLPs to recall the required answer such as "Paris." We introduce a novel analysis method aimed at decomposing the outputs of the MLP into components understandable by humans. Through this method, we quantify the function of the MLP layer following these task-specific heads. In the residual stream, it either erases or amplifies the information originating from individual heads. Moreover, it generates a component that redirects the residual stream towards the direction of its expected answer. These zero-shot mechanisms are also employed in few-shot scenarios. Additionally, we observed a widely existent anti-overconfidence mechanism in 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#37327;&#23376;&#20551;&#35774;&#26816;&#39564;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#24471;&#20986;&#20102;&#23545;&#31216;&#21644;&#38750;&#23545;&#31216;&#35774;&#32622;&#20013;&#30340;&#20108;&#36827;&#21046;&#37327;&#23376;&#20551;&#35774;&#26816;&#39564;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#19982;&#21453;&#38169;&#35823;&#27010;&#29575;&#30340;&#23545;&#25968;&#21644;&#20445;&#30495;&#24230;&#30340;&#36127;&#23545;&#25968;&#30340;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2403.17868</link><description>&lt;p&gt;
&#37327;&#23376;&#20551;&#35774;&#26816;&#39564;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;
&lt;/p&gt;
&lt;p&gt;
Sample complexity of quantum hypothesis testing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17868
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#37327;&#23376;&#20551;&#35774;&#26816;&#39564;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#24471;&#20986;&#20102;&#23545;&#31216;&#21644;&#38750;&#23545;&#31216;&#35774;&#32622;&#20013;&#30340;&#20108;&#36827;&#21046;&#37327;&#23376;&#20551;&#35774;&#26816;&#39564;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#19982;&#21453;&#38169;&#35823;&#27010;&#29575;&#30340;&#23545;&#25968;&#21644;&#20445;&#30495;&#24230;&#30340;&#36127;&#23545;&#25968;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#20154;&#20204;&#20174;&#20449;&#24687;&#35770;&#30340;&#35282;&#24230;&#30740;&#31350;&#37327;&#23376;&#20551;&#35774;&#26816;&#39564;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20154;&#20204;&#23545;&#38169;&#35823;&#27010;&#29575;&#30340;&#26368;&#20248;&#34928;&#20943;&#36895;&#29575;&#24863;&#20852;&#36259;&#65292;&#36825;&#20010;&#36895;&#29575;&#26159;&#26410;&#30693;&#29366;&#24577;&#30340;&#26679;&#26412;&#25968;&#37327;&#20989;&#25968;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#37327;&#23376;&#20551;&#35774;&#26816;&#39564;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#26088;&#22312;&#30830;&#23450;&#36798;&#21040;&#25152;&#38656;&#38169;&#35823;&#27010;&#29575;&#25152;&#38656;&#30340;&#26368;&#23569;&#26679;&#26412;&#25968;&#37327;&#12290;&#36890;&#36807;&#21033;&#29992;&#24050;&#26377;&#25991;&#29486;&#20013;&#20851;&#20110;&#37327;&#23376;&#20551;&#35774;&#26816;&#39564;&#30340;&#20016;&#23500;&#30693;&#35782;&#65292;&#25105;&#20204;&#34920;&#24449;&#20102;&#23545;&#31216;&#21644;&#38750;&#23545;&#31216;&#35774;&#32622;&#20013;&#30340;&#20108;&#36827;&#21046;&#37327;&#23376;&#20551;&#35774;&#26816;&#39564;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#24182;&#25552;&#20379;&#20102;&#22810;&#20010;&#37327;&#23376;&#20551;&#35774;&#26816;&#39564;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#30028;&#38480;&#12290;&#26356;&#35814;&#32454;&#22320;&#35828;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#31216;&#20108;&#36827;&#21046;&#37327;&#23376;&#20551;&#35774;&#26816;&#39564;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#23545;&#21453;&#38169;&#35823;&#27010;&#29575;&#30340;&#23545;&#25968;&#21644;&#20445;&#30495;&#24230;&#30340;&#36127;&#23545;&#25968;&#30340;&#23545;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17868v1 Announce Type: cross  Abstract: Quantum hypothesis testing has been traditionally studied from the information-theoretic perspective, wherein one is interested in the optimal decay rate of error probabilities as a function of the number of samples of an unknown state. In this paper, we study the sample complexity of quantum hypothesis testing, wherein the goal is to determine the minimum number of samples needed to reach a desired error probability. By making use of the wealth of knowledge that already exists in the literature on quantum hypothesis testing, we characterize the sample complexity of binary quantum hypothesis testing in the symmetric and asymmetric settings, and we provide bounds on the sample complexity of multiple quantum hypothesis testing. In more detail, we prove that the sample complexity of symmetric binary quantum hypothesis testing depends logarithmically on the inverse error probability and inversely on the negative logarithm of the fidelity. 
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#23398;&#20064;&#26159;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#20840;&#38754;&#23457;&#35270;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21450;&#20854;&#24212;&#29992;&#30340;&#26368;&#26032;&#21457;&#23637;</title><link>https://arxiv.org/abs/2403.17561</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#21450;&#20854;&#26368;&#26032;&#24212;&#29992;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Deep Learning and State-of-the-arts Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17561
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26159;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#20840;&#38754;&#23457;&#35270;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21450;&#20854;&#24212;&#29992;&#30340;&#26368;&#26032;&#21457;&#23637;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;, &#26159;&#20154;&#24037;&#26234;&#33021;&#30340;&#19968;&#20010;&#20998;&#25903;&#65292;&#26159;&#19968;&#31181;&#21033;&#29992;&#22810;&#23618;&#20114;&#36830;&#21333;&#20803;&#65288;&#31070;&#32463;&#20803;&#65289;&#20174;&#21407;&#22987;&#36755;&#20837;&#25968;&#25454;&#20013;&#30452;&#25509;&#23398;&#20064;&#22797;&#26434;&#27169;&#24335;&#21644;&#34920;&#31034;&#30340;&#35745;&#31639;&#27169;&#22411;&#12290;&#21463;&#21040;&#36825;&#31181;&#23398;&#20064;&#33021;&#21147;&#30340;&#36171;&#33021;&#65292;&#28145;&#24230;&#23398;&#20064;&#24050;&#25104;&#20026;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#26159;&#35768;&#22810;&#31361;&#30772;&#24615;&#25216;&#26415;&#21644;&#21019;&#26032;&#30340;&#26680;&#24515;&#39537;&#21160;&#21147;&#12290;&#26500;&#24314;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#31639;&#27861;&#30340;&#22797;&#26434;&#24615;&#21644;&#29616;&#23454;&#38382;&#39064;&#30340;&#21160;&#24577;&#24615;&#12290;&#26377;&#20960;&#39033;&#30740;&#31350;&#22238;&#39038;&#20102;&#28145;&#24230;&#23398;&#20064;&#30340;&#27010;&#24565;&#21644;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#22823;&#22810;&#38598;&#20013;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#31867;&#22411;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21450;&#20854;&#22312;&#19981;&#21516;&#39046;&#22495;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#30340;&#26368;&#26032;&#21457;&#23637;&#30340;&#35206;&#30422;&#38754;&#26377;&#38480;&#12290;&#22240;&#27492;&#65292;&#21463;&#21040;&#36825;&#20123;&#38480;&#21046;&#30340;&#21551;&#21457;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#20840;&#38754;&#23457;&#35270;th
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17561v1 Announce Type: new  Abstract: Deep learning, a branch of artificial intelligence, is a computational model that uses multiple layers of interconnected units (neurons) to learn intricate patterns and representations directly from raw input data. Empowered by this learning capability, it has become a powerful tool for solving complex problems and is the core driver of many groundbreaking technologies and innovations. Building a deep learning model is a challenging task due to the algorithm`s complexity and the dynamic nature of real-world problems. Several studies have reviewed deep learning concepts and applications. However, the studies mostly focused on the types of deep learning models and convolutional neural network architectures, offering limited coverage of the state-of-the-art of deep learning models and their applications in solving complex problems across different domains. Therefore, motivated by the limitations, this study aims to comprehensively review th
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#34507;&#30333;&#36136;&#21644;&#23567;&#20998;&#23376;&#30456;&#20851;&#20219;&#21153;&#20013;&#36229;&#36234;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#36890;&#36807;&#32479;&#19968;&#20998;&#23376;&#24314;&#27169;&#23454;&#29616;&#20102;&#20805;&#20998;&#21033;&#29992;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.12995</link><description>&lt;p&gt;
&#22810;&#23610;&#24230;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#32479;&#19968;&#20998;&#23376;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Multi-Scale Protein Language Model for Unified Molecular Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12995
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#34507;&#30333;&#36136;&#21644;&#23567;&#20998;&#23376;&#30456;&#20851;&#20219;&#21153;&#20013;&#36229;&#36234;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#36890;&#36807;&#32479;&#19968;&#20998;&#23376;&#24314;&#27169;&#23454;&#29616;&#20102;&#20805;&#20998;&#21033;&#29992;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#22312;&#34507;&#30333;&#36136;&#24037;&#31243;&#39046;&#22495;&#23637;&#29616;&#20986;&#20102;&#26174;&#33879;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#20027;&#35201;&#22312;&#27531;&#22522;&#32423;&#21035;&#19978;&#36816;&#34892;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#21407;&#23376;&#27700;&#24179;&#25552;&#20379;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#36825;&#19968;&#38480;&#21046;&#38459;&#30861;&#20102;&#25105;&#20204;&#20805;&#20998;&#21033;&#29992;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#22312;&#28041;&#21450;&#34507;&#30333;&#36136;&#21644;&#23567;&#20998;&#23376;&#30340;&#24212;&#29992;&#20013;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;ms-ESM&#65288;&#22810;&#23610;&#24230;ESM&#65289;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22810;&#23610;&#24230;&#32479;&#19968;&#20998;&#23376;&#24314;&#27169;&#12290;ms-ESM&#36890;&#36807;&#39044;&#35757;&#32451;&#22810;&#23610;&#24230;&#20195;&#30721;&#20999;&#25442;&#34507;&#30333;&#36136;&#24207;&#21015;&#24182;&#21033;&#29992;&#22810;&#23610;&#24230;&#20301;&#32622;&#32534;&#30721;&#26469;&#25429;&#33719;&#27531;&#22522;&#21644;&#21407;&#23376;&#20043;&#38388;&#30340;&#20851;&#31995;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ms-ESM&#22312;&#34507;&#30333;&#36136;-&#20998;&#23376;&#20219;&#21153;&#20013;&#36229;&#36234;&#20102;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#23545;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#30340;&#20805;&#20998;&#21033;&#29992;&#12290;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#25581;&#31034;&#36890;&#36807;&#32479;&#19968;&#20998;&#23376;&#24314;&#27169;&#65292;ms-ESM&#19981;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12995v1 Announce Type: cross  Abstract: Protein language models have demonstrated significant potential in the field of protein engineering. However, current protein language models primarily operate at the residue scale, which limits their ability to provide information at the atom level. This limitation prevents us from fully exploiting the capabilities of protein language models for applications involving both proteins and small molecules. In this paper, we propose ms-ESM (multi-scale ESM), a novel approach that enables multi-scale unified molecular modeling. ms-ESM achieves this by pre-training on multi-scale code-switch protein sequences and utilizing a multi-scale position encoding to capture relationships among residues and atoms. Experimental results indicate that ms-ESM surpasses previous methods in protein-molecule tasks, demonstrating the full utilization of protein language models. Further investigations reveal that through unified molecular modeling, ms-ESM not 
&lt;/p&gt;</description></item><item><title>BurstAttention&#26159;&#19968;&#31181;&#29992;&#20110;&#20248;&#21270;&#20869;&#23384;&#35775;&#38382;&#21644;&#36890;&#20449;&#25805;&#20316;&#30340;&#20998;&#24067;&#24335;&#27880;&#24847;&#21147;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;&#22788;&#29702;&#26497;&#38271;&#24207;&#21015;&#12290;</title><link>https://arxiv.org/abs/2403.09347</link><description>&lt;p&gt;
BurstAttention&#65306;&#19968;&#31181;&#29992;&#20110;&#22788;&#29702;&#26497;&#38271;&#24207;&#21015;&#30340;&#39640;&#25928;&#20998;&#24067;&#24335;&#27880;&#24847;&#21147;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
BurstAttention: An Efficient Distributed Attention Framework for Extremely Long Sequences
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09347
&lt;/p&gt;
&lt;p&gt;
BurstAttention&#26159;&#19968;&#31181;&#29992;&#20110;&#20248;&#21270;&#20869;&#23384;&#35775;&#38382;&#21644;&#36890;&#20449;&#25805;&#20316;&#30340;&#20998;&#24067;&#24335;&#27880;&#24847;&#21147;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;&#22788;&#29702;&#26497;&#38271;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30340;&#27880;&#24847;&#21147;&#27169;&#22359;&#22312;&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25104;&#21151;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#20294;&#36825;&#20123;&#27880;&#24847;&#21147;&#27169;&#22359;&#30340;&#20108;&#27425;&#26102;&#38388;&#21644;&#20869;&#23384;&#22797;&#26434;&#24230;&#22312;&#22788;&#29702;&#38271;&#24207;&#21015;&#26102;&#20063;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;BurstAttention&#8221;&#30340;&#20998;&#24067;&#24335;&#27880;&#24847;&#21147;&#26694;&#26550;&#65292;&#20197;&#20248;&#21270;&#20840;&#23616;&#38598;&#32676;&#21644;&#26412;&#22320;&#35774;&#22791;&#32423;&#21035;&#30340;&#20869;&#23384;&#35775;&#38382;&#21644;&#36890;&#20449;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09347v1 Announce Type: cross  Abstract: Effective attention modules have played a crucial role in the success of Transformer-based large language models (LLMs), but the quadratic time and memory complexities of these attention modules also pose a challenge when processing long sequences. One potential solution for the long sequence problem is to utilize distributed clusters to parallelize the computation of attention modules across multiple devices (e.g., GPUs). However, adopting a distributed approach inevitably introduces extra memory overheads to store local attention results and incurs additional communication costs to aggregate local results into global ones. In this paper, we propose a distributed attention framework named ``BurstAttention'' to optimize memory access and communication operations at both the global cluster and local device levels. In our experiments, we compare BurstAttention with other competitive distributed attention solutions for long sequence proce
&lt;/p&gt;</description></item><item><title>&#20026;&#20102;&#22686;&#21152;&#27979;&#35797;&#26399;&#38388;&#27491;&#30830;&#39044;&#27979;&#30340;&#26426;&#20250;&#65292;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#22270;&#20687;&#21040;&#22270;&#20687;&#30340;&#36716;&#25442;&#32416;&#27491;FSL&#27169;&#22411;&#30340;&#27979;&#35797;&#36755;&#20837;&#65292;&#29983;&#25104;&#34987;&#27979;&#35797;&#31867;&#21035;&#30340;&#26032;&#26679;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.18292</link><description>&lt;p&gt;
FSL&#27169;&#22411;&#21487;&#20197;&#22240;&#20026;&#20854;&#20248;&#36234;&#24615;&#24471;&#20998;&#26356;&#39640;
&lt;/p&gt;
&lt;p&gt;
FSL Model can Score Higher as It Is
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18292
&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22686;&#21152;&#27979;&#35797;&#26399;&#38388;&#27491;&#30830;&#39044;&#27979;&#30340;&#26426;&#20250;&#65292;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#22270;&#20687;&#21040;&#22270;&#20687;&#30340;&#36716;&#25442;&#32416;&#27491;FSL&#27169;&#22411;&#30340;&#27979;&#35797;&#36755;&#20837;&#65292;&#29983;&#25104;&#34987;&#27979;&#35797;&#31867;&#21035;&#30340;&#26032;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#65292;&#20026;&#20102;&#22686;&#21152;&#34987;&#27491;&#30830;&#35782;&#21035;&#30340;&#26426;&#20250;&#65292;&#25105;&#20204;&#20542;&#21521;&#20110;&#38754;&#23545;&#38754;&#22320;&#30452;&#35270;&#38754;&#37096;&#35782;&#21035;&#26426;&#65292;&#32780;&#19981;&#26159;&#20391;&#30528;&#38754;&#23545;&#12290;&#23569;&#26679;&#26412;&#23398;&#20064;&#65288;FSL&#65289;&#20998;&#31867;&#26412;&#36523;&#23601;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#27169;&#22411;&#24517;&#39035;&#35782;&#21035;&#23646;&#20110;&#35757;&#32451;&#26102;&#26410;&#35265;&#30340;&#31867;&#21035;&#30340;&#22270;&#20687;&#12290;&#22240;&#27492;&#65292;&#22312;&#27979;&#35797;&#26399;&#38388;&#23545;&#25197;&#26354;&#21644;&#38750;&#20856;&#22411;&#30340;&#26597;&#35810;&#25110;&#25903;&#25345;&#22270;&#20687;&#20250;&#35753;&#27169;&#22411;&#26356;&#38590;&#27491;&#30830;&#39044;&#27979;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#20026;&#20102;&#22686;&#21152;&#27979;&#35797;&#26399;&#38388;&#27491;&#30830;&#39044;&#27979;&#30340;&#26426;&#20250;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#22270;&#20687;&#21040;&#22270;&#20687;&#30340;&#36716;&#25442;&#32416;&#27491;&#35757;&#32451;&#36807;&#30340;FSL&#27169;&#22411;&#30340;&#27979;&#35797;&#36755;&#20837;&#65292;&#29983;&#25104;&#34987;&#27979;&#35797;&#31867;&#21035;&#30340;&#26032;&#26679;&#26412;&#12290;FSL&#27169;&#22411;&#36890;&#24120;&#26159;&#22312;&#20855;&#26377;&#36275;&#22815;&#26679;&#26412;&#30340;&#31867;&#21035;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#20855;&#26377;&#23569;&#26679;&#26412;&#26679;&#26412;&#30340;&#31867;&#21035;&#19978;&#36827;&#34892;&#27979;&#35797;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#39318;&#20808;&#25429;&#25417;&#27979;&#35797;&#22270;&#20687;&#30340;&#39118;&#26684;&#25110;&#24418;&#29366;&#65292;&#28982;&#21518;&#35782;&#21035;&#19968;&#20010;&#36866;&#24403;&#30340;&#35757;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18292v1 Announce Type: cross  Abstract: In daily life, we tend to present the front of our faces by staring squarely at a facial recognition machine, instead of facing it sideways, in order to increase the chance of being correctly recognised. Few-shot-learning (FSL) classification is challenging in itself because a model has to identify images that belong to classes previously unseen during training. Therefore, a warped and non-typical query or support image during testing can make it even more challenging for a model to predict correctly. In our work, to increase the chance of correct prediction during testing, we aim to rectify the test input of a trained FSL model by generating new samples of the tested classes through image-to-image translation. An FSL model is usually trained on classes with sufficient samples, and then tested on classes with few-shot samples. Our proposed method first captures the style or shape of the test image, and then identifies a suitable traine
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;LLMs&#36755;&#20986;&#20998;&#24067;&#36827;&#34892;&#27745;&#26579;&#26816;&#27979;&#30340;&#26041;&#27861;CDD&#65292;&#20197;&#21450;&#19968;&#31181;&#22522;&#20110;LLMs&#36755;&#20986;&#20462;&#27491;&#30340;&#21487;&#20449;&#35780;&#20272;&#26041;&#27861;TED&#65292;&#20197;&#24212;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#25454;&#27745;&#26579;&#21644;&#21487;&#20449;&#35780;&#20272;&#26041;&#38754;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.15938</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#25110;&#35760;&#24518;&#65306;&#25968;&#25454;&#27745;&#26579;&#19982;&#21487;&#20449;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Generalization or Memorization: Data Contamination and Trustworthy Evaluation for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;LLMs&#36755;&#20986;&#20998;&#24067;&#36827;&#34892;&#27745;&#26579;&#26816;&#27979;&#30340;&#26041;&#27861;CDD&#65292;&#20197;&#21450;&#19968;&#31181;&#22522;&#20110;LLMs&#36755;&#20986;&#20462;&#27491;&#30340;&#21487;&#20449;&#35780;&#20272;&#26041;&#27861;TED&#65292;&#20197;&#24212;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#25454;&#27745;&#26579;&#21644;&#21487;&#20449;&#35780;&#20272;&#26041;&#38754;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20851;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#33021;&#21147;&#30340;&#35828;&#27861;&#36890;&#24120;&#26159;&#36890;&#36807;&#22312;&#24320;&#25918;&#33719;&#21462;&#30340;&#22522;&#20934;&#19978;&#36827;&#34892;&#35780;&#20272;&#26469;&#25903;&#25345;&#30340;&#12290;&#32771;&#34385;&#21040;LLMs&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#24222;&#22823;&#35268;&#27169;&#21644;&#24191;&#27867;&#26469;&#28304;&#65292;&#23427;&#21487;&#33021;&#26126;&#30830;&#25110;&#38544;&#21547;&#22320;&#21253;&#21547;&#27979;&#35797;&#25968;&#25454;&#65292;&#23548;&#33268;LLMs&#26356;&#23481;&#26131;&#21463;&#21040;&#25968;&#25454;&#27745;&#26579;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35757;&#32451;&#25968;&#25454;&#30340;&#19981;&#36879;&#26126;&#24615;&#12289;&#27169;&#22411;&#30340;&#40657;&#30418;&#35775;&#38382;&#20197;&#21450;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#30340;&#24555;&#36895;&#22686;&#38271;&#65292;&#23545;&#20110;LLMs&#26469;&#35828;&#26816;&#27979;&#21644;&#20943;&#36731;&#25968;&#25454;&#27745;&#26579;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CDD&#65292;&#21363;&#36890;&#36807;LLMs&#36755;&#20986;&#20998;&#24067;&#36827;&#34892;&#27745;&#26579;&#26816;&#27979;&#30340;CDD&#12290;CDD&#20165;&#38656;&#35201;&#37319;&#26679;&#25991;&#26412;&#26469;&#26816;&#27979;&#25968;&#25454;&#27745;&#26579;&#65292;&#36890;&#36807;&#35782;&#21035;LLMs&#36755;&#20986;&#20998;&#24067;&#30340;&#23792;&#20540;&#26469;&#36827;&#34892;&#26816;&#27979;&#12290;&#20026;&#20102;&#20943;&#36731;&#35780;&#20272;&#20013;&#25968;&#25454;&#27745;&#26579;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;TED&#65306;&#22522;&#20110;LLMs&#36755;&#20986;&#20462;&#27491;&#30340;&#21487;&#20449;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15938v1 Announce Type: cross  Abstract: Recent statements about the impressive capabilities of large language models (LLMs) are usually supported by evaluating on open-access benchmarks. Considering the vast size and wide-ranging sources of LLMs' training data, it could explicitly or implicitly include test data, leading to LLMs being more susceptible to data contamination. However, due to the opacity of training data, the black-box access of models, and the rapid growth of synthetic training data, detecting and mitigating data contamination for LLMs faces significant challenges. In this paper, we propose CDD, which stands for Contamination Detection via output Distribution for LLMs. CDD necessitates only the sampled texts to detect data contamination, by identifying the peakedness of LLM's output distribution. To mitigate the impact of data contamination in evaluation, we also present TED: Trustworthy Evaluation via output Distribution, based on the correction of LLM's outp
&lt;/p&gt;</description></item><item><title>GenTranslate&#26159;&#19968;&#20010;&#26032;&#30340;&#32763;&#35793;&#20219;&#21153;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20016;&#23500;&#35821;&#35328;&#30693;&#35782;&#21644;&#24378;&#22823;&#25512;&#29702;&#33021;&#21147;&#65292;&#21487;&#20197;&#20174;N-best&#21015;&#34920;&#20013;&#29983;&#25104;&#26356;&#39640;&#36136;&#37327;&#30340;&#32763;&#35793;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.06894</link><description>&lt;p&gt;
GenTranslate: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#29983;&#25104;&#30340;&#22810;&#35821;&#35328;&#35821;&#38899;&#21644;&#26426;&#22120;&#32763;&#35793;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
GenTranslate: Large Language Models are Generative Multilingual Speech and Machine Translators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06894
&lt;/p&gt;
&lt;p&gt;
GenTranslate&#26159;&#19968;&#20010;&#26032;&#30340;&#32763;&#35793;&#20219;&#21153;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20016;&#23500;&#35821;&#35328;&#30693;&#35782;&#21644;&#24378;&#22823;&#25512;&#29702;&#33021;&#21147;&#65292;&#21487;&#20197;&#20174;N-best&#21015;&#34920;&#20013;&#29983;&#25104;&#26356;&#39640;&#36136;&#37327;&#30340;&#32763;&#35793;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#36890;&#36807;&#20943;&#23569;&#34920;&#31034;&#35823;&#24046;&#21644;&#24341;&#20837;&#22806;&#37096;&#30693;&#35782;&#65292;&#25512;&#21160;&#20102;&#22810;&#35821;&#35328;&#35821;&#38899;&#21644;&#26426;&#22120;&#32763;&#35793;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#32763;&#35793;&#20219;&#21153;&#36890;&#24120;&#20351;&#29992;&#26463;&#25628;&#32034;&#35299;&#30721;&#21644;&#21069;k&#20010;&#20551;&#35774;&#36873;&#25321;&#36827;&#34892;&#25512;&#29702;&#12290;&#36825;&#20123;&#25216;&#26415;&#24448;&#24448;&#19981;&#33021;&#20805;&#20998;&#21033;&#29992;&#22810;&#26679;&#21270;&#30340;N-best&#20551;&#35774;&#20013;&#30340;&#20016;&#23500;&#20449;&#24687;&#65292;&#20351;&#24471;&#23427;&#20204;&#22312;&#38656;&#35201;&#21333;&#20010;&#39640;&#36136;&#37327;&#36755;&#20986;&#24207;&#21015;&#30340;&#32763;&#35793;&#20219;&#21153;&#20013;&#25928;&#26524;&#19981;&#20339;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#32763;&#35793;&#20219;&#21153;&#29983;&#25104;&#27169;&#22411;&#65292;&#21363;&#8220;GenTranslate&#8221;&#65292;&#23427;&#22522;&#20110;LLMs&#26469;&#20174;N-best&#21015;&#34920;&#20013;&#29983;&#25104;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#21033;&#29992;LLMs&#20016;&#23500;&#30340;&#35821;&#35328;&#30693;&#35782;&#21644;&#24378;&#22823;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#25105;&#20204;&#30340;&#26032;&#27169;&#22411;&#21487;&#20197;&#23558;N-best&#20505;&#36873;&#20154;&#20013;&#30340;&#20016;&#23500;&#20449;&#24687;&#25972;&#21512;&#36215;&#26469;&#65292;&#29983;&#25104;&#26356;&#39640;&#36136;&#37327;&#30340;&#32763;&#35793;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#25903;&#25345;LLM&#30340;&#24494;&#35843;&#65292;&#25105;&#20204;&#26500;&#24314;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;HypoTransla&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in large language models (LLMs) have stepped forward the development of multilingual speech and machine translation by its reduced representation errors and incorporated external knowledge. However, both translation tasks typically utilize beam search decoding and top-1 hypothesis selection for inference. These techniques struggle to fully exploit the rich information in the diverse N-best hypotheses, making them less optimal for translation tasks that require a single, high-quality output sequence. In this paper, we propose a new generative paradigm for translation tasks, namely "GenTranslate", which builds upon LLMs to generate better results from the diverse translation versions in N-best list. Leveraging the rich linguistic knowledge and strong reasoning abilities of LLMs, our new paradigm can integrate the rich information in N-best candidates to generate a higher-quality translation result. Furthermore, to support LLM finetuning, we build and release a HypoTransla
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;&#37096;&#20998;&#30417;&#25511;&#20013;&#22522;&#20110;&#38543;&#26426;&#21270;&#30340;&#20559;&#32622;&#30028;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#25193;&#23637;&#20102;&#29616;&#26377;&#38543;&#26426;&#31574;&#30053;&#26080;&#27861;&#24212;&#29992;&#30340;&#24773;&#22659;&#21644;&#38750;&#24773;&#22659;&#35774;&#32622;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#31574;&#30053;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.05002</link><description>&lt;p&gt;
&#38543;&#26426;&#20559;&#32622;&#30028;&#23545;&#38543;&#26426;&#37096;&#20998;&#30417;&#25511;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Randomized Confidence Bounds for Stochastic Partial Monitoring
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05002
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;&#37096;&#20998;&#30417;&#25511;&#20013;&#22522;&#20110;&#38543;&#26426;&#21270;&#30340;&#20559;&#32622;&#30028;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#25193;&#23637;&#20102;&#29616;&#26377;&#38543;&#26426;&#31574;&#30053;&#26080;&#27861;&#24212;&#29992;&#30340;&#24773;&#22659;&#21644;&#38750;&#24773;&#22659;&#35774;&#32622;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#31574;&#30053;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#20998;&#30417;&#25511; (PM) &#26694;&#26550;&#25552;&#20379;&#20102;&#36890;&#36807;&#19981;&#23436;&#25972;&#30340;&#21453;&#39304;&#36827;&#34892;&#39034;&#24207;&#23398;&#20064;&#38382;&#39064;&#30340;&#29702;&#35770;&#34920;&#36848;&#12290;&#22312;&#27599;&#20010;&#22238;&#21512;&#20013;&#65292;&#23398;&#20064;&#20195;&#29702;&#36873;&#25321;&#19968;&#20010;&#21160;&#20316;&#65292;&#32780;&#29615;&#22659;&#21516;&#26102;&#36873;&#25321;&#19968;&#20010;&#32467;&#26524;&#12290;&#28982;&#21518;&#20195;&#29702;&#35266;&#23519;&#21040;&#19968;&#20010;&#20165;&#37096;&#20998;&#25552;&#20379;&#20449;&#24687;&#20851;&#20110;&#65288;&#26410;&#35266;&#23519;&#21040;&#30340;&#65289;&#32467;&#26524;&#30340;&#21453;&#39304;&#20449;&#21495;&#12290;&#20195;&#29702;&#21033;&#29992;&#25509;&#25910;&#21040;&#30340;&#21453;&#39304;&#20449;&#21495;&#36873;&#25321;&#33021;&#22815;&#26368;&#23567;&#21270;&#65288;&#26410;&#35266;&#23519;&#21040;&#30340;&#65289;&#32047;&#35745;&#25439;&#22833;&#30340;&#21160;&#20316;&#12290;&#22312;&#24773;&#22659; PM &#20013;&#65292;&#32467;&#26524;&#20381;&#36182;&#20110;&#20195;&#29702;&#22312;&#27599;&#36718;&#36873;&#25321;&#21160;&#20316;&#20043;&#21069;&#21487;&#35266;&#23519;&#21040;&#30340;&#26576;&#20123;&#38468;&#21152;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20855;&#26377;&#38543;&#26426;&#32467;&#26524;&#30340;&#24773;&#22659;&#21644;&#38750;&#24773;&#22659;&#30340; PM &#35774;&#32622;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#30830;&#23450;&#24615;&#32622;&#20449;&#30028;&#30340;&#38543;&#26426;&#21270;&#31574;&#30053;&#30340;&#26032;&#31867;&#26041;&#27861;&#65292;&#23558;&#36951;&#25022;&#20445;&#35777;&#25193;&#23637;&#21040;&#29616;&#26377;&#30340;&#38543;&#26426;&#31574;&#30053;&#19981;&#36866;&#29992;&#30340;&#35774;&#32622;&#20013;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340; RandCBP &#21644; RandCBPside* &#31574;&#30053;&#25913;&#36827;&#20102;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The partial monitoring (PM) framework provides a theoretical formulation of sequential learning problems with incomplete feedback. On each round, a learning agent plays an action while the environment simultaneously chooses an outcome. The agent then observes a feedback signal that is only partially informative about the (unobserved) outcome. The agent leverages the received feedback signals to select actions that minimize the (unobserved) cumulative loss. In contextual PM, the outcomes depend on some side information that is observable by the agent before selecting the action on each round. In this paper, we consider the contextual and non-contextual PM settings with stochastic outcomes. We introduce a new class of strategies based on the randomization of deterministic confidence bounds, that extend regret guarantees to settings where existing stochastic strategies are not applicable. Our experiments show that the proposed RandCBP and RandCBPside* strategies improve state-of-the-art b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#24067;&#23616;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#32771;&#34385;&#32654;&#23398;&#32422;&#26463;&#65292;&#22312;&#22270;&#24418;&#35774;&#35745;&#20013;&#21019;&#24314;&#21512;&#29702;&#30340;&#20803;&#32032;&#21487;&#35270;&#25490;&#21015;&#12290;&#19982;&#20043;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#27169;&#22411;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#23545;&#40784;&#24615;&#33021;&#65292;&#24182;&#22312;&#21508;&#31181;&#24067;&#23616;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.04754</link><description>&lt;p&gt;
&#36890;&#36807;&#32771;&#34385;&#32654;&#23398;&#32422;&#26463;&#30340;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#23545;&#40784;&#24067;&#23616;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Towards Aligned Layout Generation via Diffusion Model with Aesthetic Constraints
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04754
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#24067;&#23616;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#32771;&#34385;&#32654;&#23398;&#32422;&#26463;&#65292;&#22312;&#22270;&#24418;&#35774;&#35745;&#20013;&#21019;&#24314;&#21512;&#29702;&#30340;&#20803;&#32032;&#21487;&#35270;&#25490;&#21015;&#12290;&#19982;&#20043;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#27169;&#22411;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#23545;&#40784;&#24615;&#33021;&#65292;&#24182;&#22312;&#21508;&#31181;&#24067;&#23616;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#25511;&#24067;&#23616;&#29983;&#25104;&#26159;&#25351;&#22312;&#20855;&#26377;&#20195;&#34920;&#35774;&#35745;&#24847;&#22270;&#30340;&#32422;&#26463;&#26465;&#20214;&#19979;&#65292;&#22312;&#22270;&#24418;&#35774;&#35745;&#65288;&#20363;&#22914;&#25991;&#26723;&#21644;&#32593;&#39029;&#35774;&#35745;&#65289;&#20013;&#21019;&#24314;&#19968;&#31181;&#21512;&#29702;&#30340;&#20803;&#32032;&#21487;&#35270;&#25490;&#21015;&#30340;&#36807;&#31243;&#12290;&#23613;&#31649;&#26368;&#36817;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;FID&#20998;&#25968;&#65292;&#20294;&#19982;&#20043;&#21069;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;&#23427;&#20204;&#24448;&#24448;&#34920;&#29616;&#20986;&#26356;&#26126;&#26174;&#30340;&#20559;&#31163;&#23545;&#40784;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#27169;&#22411;&#8212;&#8212;LACE&#65288;Layout Constraint Diffusion Model&#65289;&#65292;&#29992;&#20110;&#22788;&#29702;&#21508;&#31181;&#24067;&#23616;&#29983;&#25104;&#20219;&#21153;&#65292;&#20363;&#22914;&#26681;&#25454;&#25351;&#23450;&#23646;&#24615;&#25490;&#21015;&#20803;&#32032;&#12289;&#23436;&#21892;&#25110;&#23436;&#25104;&#31895;&#31961;&#30340;&#24067;&#23616;&#35774;&#35745;&#12290;&#35813;&#27169;&#22411;&#22522;&#20110;&#36830;&#32493;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;&#19982;&#20351;&#29992;&#31163;&#25955;&#25193;&#25955;&#27169;&#22411;&#30340;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#35774;&#35745;&#21487;&#20197;&#22312;&#35757;&#32451;&#20013;&#24341;&#20837;&#21487;&#24494;&#30340;&#32654;&#23398;&#32422;&#26463;&#20989;&#25968;&#12290;&#23545;&#20110;&#26465;&#20214;&#29983;&#25104;&#65292;&#25105;&#20204;&#36890;&#36807;&#25513;&#30721;&#36755;&#20837;&#24341;&#20837;&#26465;&#20214;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;LACE&#27169;&#22411;&#22312;&#19981;&#21516;&#24067;&#23616;&#29983;&#25104;&#20219;&#21153;&#19978;&#37117;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Controllable layout generation refers to the process of creating a plausible visual arrangement of elements within a graphic design (e.g., document and web designs) with constraints representing design intentions. Although recent diffusion-based models have achieved state-of-the-art FID scores, they tend to exhibit more pronounced misalignment compared to earlier transformer-based models. In this work, we propose the $\textbf{LA}$yout $\textbf{C}$onstraint diffusion mod$\textbf{E}$l (LACE), a unified model to handle a broad range of layout generation tasks, such as arranging elements with specified attributes and refining or completing a coarse layout design. The model is based on continuous diffusion models. Compared with existing methods that use discrete diffusion models, continuous state-space design can enable the incorporation of differentiable aesthetic constraint functions in training. For conditional generation, we introduce conditions via masked input. Extensive experiment re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;ConsiStory&#65292;&#36890;&#36807;&#20849;&#20139;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20869;&#37096;&#28608;&#27963;&#65292;&#23454;&#29616;&#20102;&#19968;&#33268;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#12290;&#24341;&#20837;&#20102;&#20027;&#39064;&#39537;&#21160;&#30340;&#20849;&#20139;&#27880;&#24847;&#21147;&#22359;&#21644;&#22522;&#20110;&#23545;&#24212;&#30340;&#29305;&#24449;&#27880;&#20837;&#65292;&#20419;&#36827;&#20102;&#22270;&#20687;&#20043;&#38388;&#30340;&#20027;&#39064;&#19968;&#33268;&#24615;&#65292;&#24182;&#37319;&#29992;&#20102;&#31574;&#30053;&#26469;&#20445;&#25345;&#24067;&#23616;&#22810;&#26679;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03286</link><description>&lt;p&gt;
&#26080;&#38656;&#35757;&#32451;&#30340;&#19968;&#33268;&#24615;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Training-Free Consistent Text-to-Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03286
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;ConsiStory&#65292;&#36890;&#36807;&#20849;&#20139;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20869;&#37096;&#28608;&#27963;&#65292;&#23454;&#29616;&#20102;&#19968;&#33268;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#12290;&#24341;&#20837;&#20102;&#20027;&#39064;&#39537;&#21160;&#30340;&#20849;&#20139;&#27880;&#24847;&#21147;&#22359;&#21644;&#22522;&#20110;&#23545;&#24212;&#30340;&#29305;&#24449;&#27880;&#20837;&#65292;&#20419;&#36827;&#20102;&#22270;&#20687;&#20043;&#38388;&#30340;&#20027;&#39064;&#19968;&#33268;&#24615;&#65292;&#24182;&#37319;&#29992;&#20102;&#31574;&#30053;&#26469;&#20445;&#25345;&#24067;&#23616;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#24341;&#23548;&#22270;&#20687;&#29983;&#25104;&#36807;&#31243;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#21019;&#36896;&#24615;&#28789;&#27963;&#24615;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#22312;&#22810;&#26679;&#21270;&#30340;&#25552;&#31034;&#19979;&#19968;&#33268;&#22320;&#25551;&#32472;&#30456;&#21516;&#30340;&#20027;&#39064;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#20248;&#21270;&#27169;&#22411;&#26469;&#25945;&#25480;&#23427;&#25551;&#36848;&#29305;&#23450;&#29992;&#25143;&#25552;&#20379;&#20027;&#39064;&#30340;&#26032;&#35789;&#27719;&#25110;&#32773;&#20026;&#27169;&#22411;&#28155;&#21152;&#22270;&#20687;&#26465;&#20214;&#12290;&#36825;&#20123;&#26041;&#27861;&#35201;&#27714;&#38024;&#23545;&#27599;&#20010;&#20027;&#39064;&#36827;&#34892;&#28459;&#38271;&#30340;&#20248;&#21270;&#25110;&#36827;&#34892;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#22312;&#23558;&#29983;&#25104;&#30340;&#22270;&#20687;&#19982;&#25991;&#26412;&#25552;&#31034;&#23545;&#40784;&#21644;&#25551;&#32472;&#22810;&#20010;&#20027;&#39064;&#26041;&#38754;&#36935;&#21040;&#22256;&#38590;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#35757;&#32451;&#26041;&#27861;ConsiStory&#65292;&#36890;&#36807;&#20849;&#20139;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20869;&#37096;&#28608;&#27963;&#26469;&#23454;&#29616;&#19968;&#33268;&#30340;&#20027;&#39064;&#29983;&#25104;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20027;&#39064;&#39537;&#21160;&#20849;&#20139;&#27880;&#24847;&#21147;&#22359;&#21644;&#22522;&#20110;&#23545;&#24212;&#30340;&#29305;&#24449;&#27880;&#20837;&#65292;&#20197;&#20419;&#36827;&#22270;&#20687;&#20043;&#38388;&#30340;&#20027;&#39064;&#19968;&#33268;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#31574;&#30053;&#20197;&#40723;&#21169;&#24067;&#23616;&#22810;&#26679;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#20027;&#39064;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image models offer a new level of creative flexibility by allowing users to guide the image generation process through natural language. However, using these models to consistently portray the same subject across diverse prompts remains challenging. Existing approaches fine-tune the model to teach it new words that describe specific user-provided subjects or add image conditioning to the model. These methods require lengthy per-subject optimization or large-scale pre-training. Moreover, they struggle to align generated images with text prompts and face difficulties in portraying multiple subjects. Here, we present ConsiStory, a training-free approach that enables consistent subject generation by sharing the internal activations of the pretrained model. We introduce a subject-driven shared attention block and correspondence-based feature injection to promote subject consistency between images. Additionally, we develop strategies to encourage layout diversity while maintaining su
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32447;&#21608;&#26399;&#24615;&#32447;&#24615;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#36951;&#25022;&#26368;&#23567;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#19982;&#21608;&#26399;&#25968;K&#25104;&#27604;&#29575;&#26368;&#20248;&#30340;&#36951;&#25022;&#25910;&#25947;&#29575;O(&#8730;K)&#12290;&#36825;&#26159;&#39318;&#20010;&#38024;&#23545;&#24102;&#26377;&#20048;&#35266;&#21453;&#39304;&#30340;&#38543;&#26426;&#35774;&#32622;&#20351;&#29992;&#22522;&#20110;&#31574;&#30053;&#20248;&#21270;&#30340;&#26041;&#27861;&#24182;&#24314;&#31435;&#19982;K&#26368;&#20248;&#25910;&#25947;&#36895;&#29575;&#30340;&#30740;&#31350;&#65292;&#20063;&#26159;&#39318;&#20010;&#38024;&#23545;&#20855;&#26377;&#20840;&#20449;&#24687;&#21453;&#39304;&#30340;&#23545;&#25239;&#35774;&#32622;&#24182;&#24314;&#31435;&#19982;K&#26368;&#20248;&#36895;&#29575;&#30340;&#30740;&#31350;&#65292;&#30446;&#21069;&#23578;&#26410;&#25214;&#21040;&#20855;&#26377;&#26368;&#20248;&#36895;&#29575;&#20445;&#35777;&#30340;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2308.14642</link><description>&lt;p&gt;
&#32447;&#24615;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#36895;&#29575;&#26368;&#20248;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Rate-Optimal Policy Optimization for Linear Markov Decision Processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.14642
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32447;&#21608;&#26399;&#24615;&#32447;&#24615;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#36951;&#25022;&#26368;&#23567;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#19982;&#21608;&#26399;&#25968;K&#25104;&#27604;&#29575;&#26368;&#20248;&#30340;&#36951;&#25022;&#25910;&#25947;&#29575;O(&#8730;K)&#12290;&#36825;&#26159;&#39318;&#20010;&#38024;&#23545;&#24102;&#26377;&#20048;&#35266;&#21453;&#39304;&#30340;&#38543;&#26426;&#35774;&#32622;&#20351;&#29992;&#22522;&#20110;&#31574;&#30053;&#20248;&#21270;&#30340;&#26041;&#27861;&#24182;&#24314;&#31435;&#19982;K&#26368;&#20248;&#25910;&#25947;&#36895;&#29575;&#30340;&#30740;&#31350;&#65292;&#20063;&#26159;&#39318;&#20010;&#38024;&#23545;&#20855;&#26377;&#20840;&#20449;&#24687;&#21453;&#39304;&#30340;&#23545;&#25239;&#35774;&#32622;&#24182;&#24314;&#31435;&#19982;K&#26368;&#20248;&#36895;&#29575;&#30340;&#30740;&#31350;&#65292;&#30446;&#21069;&#23578;&#26410;&#25214;&#21040;&#20855;&#26377;&#26368;&#20248;&#36895;&#29575;&#20445;&#35777;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#22312;&#32447;&#21608;&#26399;&#24615;&#32447;&#24615;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#26368;&#23567;&#21270;&#36951;&#25022;&#65292;&#24182;&#19988;&#24471;&#21040;&#20102;&#19982;$K$&#65288;&#34920;&#31034;&#21608;&#26399;&#25968;&#65289;&#25104;&#27604;&#29575;&#26368;&#20248;&#30340;$\widetilde{O}(\sqrt{K})$&#30340;&#36951;&#25022;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26159;&#39318;&#27425;&#22312;&#24102;&#26377;&#20048;&#35266;&#21453;&#39304;&#30340;&#38543;&#26426;&#35774;&#32622;&#20013;&#20351;&#29992;&#22522;&#20110;&#31574;&#30053;&#20248;&#21270;&#30340;&#26041;&#27861;&#24314;&#31435;&#20102;&#19982;$K$&#26368;&#20248;&#65288;&#30456;&#23545;&#20110;$K$&#65289;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#20063;&#26159;&#39318;&#27425;&#24314;&#31435;&#22312;&#23436;&#20840;&#20449;&#24687;&#21453;&#39304;&#30340;&#25932;&#23545;&#35774;&#32622;&#20013;&#19982;$K$&#26368;&#20248;&#30340;&#36895;&#29575;&#65292;&#36825;&#31181;&#24773;&#20917;&#19979;&#30446;&#21069;&#27809;&#26377;&#24050;&#30693;&#20855;&#26377;&#26368;&#20248;&#36895;&#29575;&#20445;&#35777;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2308.14642v2 Announce Type: replace  Abstract: We study regret minimization in online episodic linear Markov Decision Processes, and obtain rate-optimal $\widetilde O (\sqrt K)$ regret where $K$ denotes the number of episodes. Our work is the first to establish the optimal (w.r.t.~$K$) rate of convergence in the stochastic setting with bandit feedback using a policy optimization based approach, and the first to establish the optimal (w.r.t.~$K$) rate in the adversarial setup with full information feedback, for which no algorithm with an optimal rate guarantee is currently known.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Lookbehind-SAM&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#27425;&#19978;&#21319;&#27493;&#39588;&#21644;&#32447;&#24615;&#25554;&#20540;&#26469;&#22686;&#24378;&#26368;&#22823;&#21270;&#21644;&#26368;&#23567;&#21270;&#36807;&#31243;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#25439;&#22833;&#38160;&#24230;&#25240;&#34935;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#37117;&#26377;&#22810;&#31181;&#20248;&#28857;&#65292;&#21253;&#25324;&#25552;&#39640;&#30340;&#27867;&#21270;&#24615;&#33021;&#12289;&#26356;&#39640;&#30340;&#40065;&#26834;&#24615;&#21644;&#25913;&#36827;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2307.16704</link><description>&lt;p&gt;
Lookbehind-SAM: k&#27493;&#22238;&#26395;&#65292;1&#27493;&#21069;&#36827;
&lt;/p&gt;
&lt;p&gt;
Lookbehind-SAM: k steps back, 1 step forward
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2307.16704
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Lookbehind-SAM&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#27425;&#19978;&#21319;&#27493;&#39588;&#21644;&#32447;&#24615;&#25554;&#20540;&#26469;&#22686;&#24378;&#26368;&#22823;&#21270;&#21644;&#26368;&#23567;&#21270;&#36807;&#31243;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#25439;&#22833;&#38160;&#24230;&#25240;&#34935;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#37117;&#26377;&#22810;&#31181;&#20248;&#28857;&#65292;&#21253;&#25324;&#25552;&#39640;&#30340;&#27867;&#21270;&#24615;&#33021;&#12289;&#26356;&#39640;&#30340;&#40065;&#26834;&#24615;&#21644;&#25913;&#36827;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38160;&#24230;&#24863;&#30693;&#20248;&#21270;&#65288;SAM&#65289;&#26041;&#27861;&#36890;&#36807;&#23558;&#26368;&#23567;&#21270;&#25439;&#22833;&#20540;&#21644;&#25439;&#22833;&#38160;&#24230;&#38382;&#39064;&#34920;&#36848;&#20026;&#26497;&#23567;&#26497;&#22823;&#22411;&#30446;&#26631;&#65292;&#24471;&#21040;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#22686;&#21152;&#20102;SAM&#30446;&#26631;&#20013;&#26368;&#22823;&#21270;&#21644;&#26368;&#23567;&#21270;&#37096;&#20998;&#30340;&#25928;&#29575;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#25439;&#22833;&#38160;&#24230;&#25240;&#34935;&#12290;&#21463;Lookahead&#20248;&#21270;&#22120;&#30340;&#21551;&#21457;&#65292;&#35813;&#20248;&#21270;&#22120;&#20351;&#29992;&#22810;&#20010;&#21521;&#21069;&#30340;&#19979;&#38477;&#27493;&#39588;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Lookbehind&#65292;&#23427;&#22312;&#21518;&#38754;&#25191;&#34892;&#22810;&#20010;&#19978;&#21319;&#27493;&#39588;&#65292;&#22686;&#24378;&#20102;SAM&#30340;&#26368;&#22823;&#21270;&#27493;&#39588;&#65292;&#24182;&#25214;&#21040;&#20102;&#19968;&#20010;&#20855;&#26377;&#26356;&#39640;&#25439;&#22833;&#30340;&#26368;&#22351;&#24773;&#20917;&#25200;&#21160;&#12290;&#28982;&#21518;&#65292;&#20026;&#20102;&#20943;&#23567;&#30001;&#20110;&#25910;&#38598;&#21040;&#30340;&#22810;&#20010;&#19978;&#21319;&#27493;&#39588;&#30340;&#26799;&#24230;&#25152;&#24341;&#36215;&#30340;&#19979;&#38477;&#27493;&#39588;&#30340;&#26041;&#24046;&#65292;&#25105;&#20204;&#37319;&#29992;&#32447;&#24615;&#25554;&#20540;&#26469;&#25913;&#36827;&#26368;&#23567;&#21270;&#36807;&#31243;&#12290;Lookbehind&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#24102;&#26469;&#20102;&#35768;&#22810;&#22909;&#22788;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25552;&#39640;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#23545;&#22122;&#22768;&#26435;&#37325;&#30340;&#26356;&#39640;&#40065;&#26834;&#24615;&#65292;&#20197;&#21450;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#25913;&#36827;&#30340;&#25928;&#26524;&#21644;&#36739;&#23569;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sharpness-aware minimization (SAM) methods have gained increasing popularity by formulating the problem of minimizing both loss value and loss sharpness as a minimax objective. In this work, we increase the efficiency of the maximization and minimization parts of SAM's objective to achieve a better loss-sharpness trade-off. By taking inspiration from the Lookahead optimizer, which uses multiple descent steps ahead, we propose Lookbehind, which performs multiple ascent steps behind to enhance the maximization step of SAM and find a worst-case perturbation with higher loss. Then, to mitigate the variance in the descent step arising from the gathered gradients across the multiple ascent steps, we employ linear interpolation to refine the minimization step. Lookbehind leads to a myriad of benefits across a variety of tasks. Particularly, we show increased generalization performance, greater robustness against noisy weights, as well as improved learning and less catastrophic forgetting in l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#65292;&#21363;&#26368;&#23567;&#19981;&#19968;&#33268;&#24230;&#37327;&#65288;LDM&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#22797;&#26434;&#20915;&#31574;&#36793;&#30028;&#24773;&#20917;&#19979;&#30340;&#20027;&#21160;&#23398;&#20064;&#38382;&#39064;&#12290;&#36890;&#36807;&#26597;&#35810;&#20855;&#26377;&#26368;&#23567;LDM&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#65292;&#21487;&#20197;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.09787</link><description>&lt;p&gt;
&#26597;&#35810;&#26131;&#20110;&#32763;&#36716;&#26679;&#26412;&#30340;&#28145;&#24230;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Querying Easily Flip-flopped Samples for Deep Active Learning. (arXiv:2401.09787v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09787
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#65292;&#21363;&#26368;&#23567;&#19981;&#19968;&#33268;&#24230;&#37327;&#65288;LDM&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#22797;&#26434;&#20915;&#31574;&#36793;&#30028;&#24773;&#20917;&#19979;&#30340;&#20027;&#21160;&#23398;&#20064;&#38382;&#39064;&#12290;&#36890;&#36807;&#26597;&#35810;&#20855;&#26377;&#26368;&#23567;LDM&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#65292;&#21487;&#20197;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#23398;&#20064;&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#26088;&#22312;&#36890;&#36807;&#36873;&#25321;&#21644;&#26597;&#35810;&#26410;&#26631;&#35760;&#25968;&#25454;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#19968;&#31181;&#26377;&#25928;&#30340;&#36873;&#25321;&#31574;&#30053;&#26159;&#22522;&#20110;&#27169;&#22411;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#65292;&#36825;&#21487;&#20197;&#35299;&#37322;&#20026;&#26679;&#26412;&#30340;&#20449;&#24687;&#37327;&#24230;&#37327;&#12290;&#26679;&#26412;&#21040;&#20915;&#31574;&#36793;&#30028;&#30340;&#36317;&#31163;&#26159;&#19968;&#31181;&#33258;&#28982;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#65292;&#20294;&#36890;&#24120;&#38590;&#20197;&#35745;&#31639;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#22810;&#31867;&#20998;&#31867;&#20219;&#21153;&#20013;&#24418;&#25104;&#30340;&#22797;&#26434;&#20915;&#31574;&#36793;&#30028;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#8220;&#26368;&#23567;&#19981;&#19968;&#33268;&#24230;&#37327;&#8221;&#65288;LDM&#65289;&#65292;&#23450;&#20041;&#20026;&#39044;&#27979;&#26631;&#31614;&#19981;&#19968;&#33268;&#30340;&#26368;&#23567;&#27010;&#29575;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;LDM&#30340;&#20272;&#35745;&#22120;&#22312;&#28201;&#21644;&#20551;&#35774;&#19979;&#26159;&#28176;&#36817;&#19968;&#33268;&#30340;&#12290;&#35813;&#20272;&#35745;&#22120;&#35745;&#31639;&#25928;&#29575;&#39640;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#21442;&#25968;&#25200;&#21160;&#36731;&#26494;&#23454;&#29616;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#20351;&#29992;&#12290;&#22522;&#20110;LDM&#30340;&#20027;&#21160;&#23398;&#20064;&#36890;&#36807;&#26597;&#35810;&#20855;&#26377;&#26368;&#23567;LDM&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#26469;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Active learning is a machine learning paradigm that aims to improve the performance of a model by strategically selecting and querying unlabeled data. One effective selection strategy is to base it on the model's predictive uncertainty, which can be interpreted as a measure of how informative a sample is. The sample's distance to the decision boundary is a natural measure of predictive uncertainty, but it is often intractable to compute, especially for complex decision boundaries formed in multiclass classification tasks. To address this issue, this paper proposes the {\it least disagree metric} (LDM), defined as the smallest probability of disagreement of the predicted label, and an estimator for LDM proven to be asymptotically consistent under mild assumptions. The estimator is computationally efficient and can be easily implemented for deep learning models using parameter perturbation. The LDM-based active learning is performed by querying unlabeled data with the smallest LDM. Exper
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;(PINNs)&#23454;&#29616;&#25968;&#23383;&#23402;&#29983;(DT)&#30340;&#28508;&#21147;&#12290;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#26080;&#32593;&#26684;&#26694;&#26550;&#30340;&#33258;&#36866;&#24212;&#37319;&#26679;&#26041;&#27861;&#65292;&#24182;&#39564;&#35777;&#20102;PINNs&#22312;&#21442;&#25968;&#21270;&#30340;Navier-Stokes&#26041;&#31243;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#22810;&#20445;&#30495;&#24230;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2401.08667</link><description>&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65306;&#25968;&#23383;&#23402;&#29983;&#30340;&#35266;&#28857;
&lt;/p&gt;
&lt;p&gt;
Data-Driven Physics-Informed Neural Networks: A Digital Twin Perspective. (arXiv:2401.08667v1 [physics.flu-dyn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08667
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;(PINNs)&#23454;&#29616;&#25968;&#23383;&#23402;&#29983;(DT)&#30340;&#28508;&#21147;&#12290;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#26080;&#32593;&#26684;&#26694;&#26550;&#30340;&#33258;&#36866;&#24212;&#37319;&#26679;&#26041;&#27861;&#65292;&#24182;&#39564;&#35777;&#20102;PINNs&#22312;&#21442;&#25968;&#21270;&#30340;Navier-Stokes&#26041;&#31243;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#22810;&#20445;&#30495;&#24230;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#19981;&#21516;&#35282;&#24230;&#25506;&#32034;&#20102;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;(PINNs)&#23454;&#29616;&#25968;&#23383;&#23402;&#29983;(DT)&#30340;&#28508;&#21147;&#12290;&#39318;&#20808;&#65292;&#30740;&#31350;&#20102;&#29992;&#20110;&#37197;&#28857;&#30340;&#21508;&#31181;&#33258;&#36866;&#24212;&#37319;&#26679;&#26041;&#27861;&#65292;&#20197;&#39564;&#35777;&#23427;&#20204;&#22312;&#26080;&#32593;&#26684;&#26694;&#26550;&#30340;PINNs&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#35813;&#26694;&#26550;&#20801;&#35768;&#33258;&#21160;&#26500;&#24314;&#34394;&#25311;&#34920;&#31034;&#65292;&#26080;&#38656;&#25163;&#21160;&#29983;&#25104;&#32593;&#26684;&#12290;&#28982;&#21518;&#65292;&#26816;&#39564;&#20102;&#25968;&#25454;&#39537;&#21160;&#30340;PINNs(DD-PINNs)&#26694;&#26550;&#30340;&#25972;&#20307;&#24615;&#33021;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#21033;&#29992;&#22312;DT&#22330;&#26223;&#20013;&#33719;&#24471;&#30340;&#25968;&#25454;&#38598;&#12290;&#20854;&#23545;&#21442;&#25968;&#21270;&#30340;Navier-Stokes&#26041;&#31243;&#30340;&#26356;&#19968;&#33324;&#29289;&#29702;&#24615;&#30340;&#21487;&#25193;&#23637;&#24615;&#24471;&#21040;&#20102;&#39564;&#35777;&#65292;&#20854;&#20013;PINNs&#22312;&#38647;&#35834;&#25968;&#21464;&#21270;&#26102;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#23454;&#38469;&#19978;&#25968;&#25454;&#38598;&#32463;&#24120;&#21487;&#20197;&#22312;&#19981;&#21516;&#30340;&#20445;&#30495;&#24230;/&#31232;&#30095;&#24230;&#19979;&#25910;&#38598;&#65292;&#36824;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#22810;&#20445;&#30495;&#24230;&#30340;DD-PINNs&#12290;&#23427;&#20204;&#22312;&#22806;&#25512;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#30456;&#23545;&#20110;&#21333;&#20445;&#30495;&#24230;&#26041;&#27861;&#25552;&#39640;&#20102;42&#65285;&#21040;62&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study explores the potential of physics-informed neural networks (PINNs) for the realization of digital twins (DT) from various perspectives. First, various adaptive sampling approaches for collocation points are investigated to verify their effectiveness in the mesh-free framework of PINNs, which allows automated construction of virtual representation without manual mesh generation. Then, the overall performance of the data-driven PINNs (DD-PINNs) framework is examined, which can utilize the acquired datasets in DT scenarios. Its scalability to more general physics is validated within parametric Navier-Stokes equations, where PINNs do not need to be retrained as the Reynolds number varies. In addition, since datasets can be often collected from different fidelity/sparsity in practice, multi-fidelity DD-PINNs are also proposed and evaluated. They show remarkable prediction performance even in the extrapolation tasks, with $42\sim62\%$ improvement over the single-fidelity approach.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;$f$&#25955;&#24230;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#20351;&#29992;&#20132;&#21449;&#29109;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#25552;&#21462;&#22522;&#20110;&#21464;&#20998;&#34920;&#31034;&#30340;$f$&#25955;&#24230;&#30446;&#26631;&#20989;&#25968;&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#36125;&#21494;&#26031;&#35270;&#35282;&#23558;&#20998;&#31867;&#20219;&#21153;&#35270;&#20026;&#26368;&#22823;&#21518;&#39564;&#27010;&#29575;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20116;&#20010;&#37319;&#29992;&#19981;&#21516;$f$&#25955;&#24230;&#30340;&#21518;&#39564;&#27010;&#29575;&#20272;&#35745;&#22120;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#19968;&#31181;&#33258;&#24213;&#21521;&#19978;&#30340;&#26041;&#27861;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#24179;&#31227;&#23545;&#25968;&#30340;$f$&#25955;&#24230;&#30340;&#30446;&#26631;&#20989;&#25968;&#12290;&#36825;&#39033;&#30740;&#31350;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#25910;&#25947;&#24615;&#30340;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.01268</link><description>&lt;p&gt;
&#22522;&#20110;$f$&#25955;&#24230;&#30340;&#20998;&#31867;&#65306;&#36229;&#36234;&#20132;&#21449;&#29109;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
$f$-Divergence Based Classification: Beyond the Use of Cross-Entropy. (arXiv:2401.01268v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01268
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;$f$&#25955;&#24230;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#20351;&#29992;&#20132;&#21449;&#29109;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#25552;&#21462;&#22522;&#20110;&#21464;&#20998;&#34920;&#31034;&#30340;$f$&#25955;&#24230;&#30446;&#26631;&#20989;&#25968;&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#36125;&#21494;&#26031;&#35270;&#35282;&#23558;&#20998;&#31867;&#20219;&#21153;&#35270;&#20026;&#26368;&#22823;&#21518;&#39564;&#27010;&#29575;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20116;&#20010;&#37319;&#29992;&#19981;&#21516;$f$&#25955;&#24230;&#30340;&#21518;&#39564;&#27010;&#29575;&#20272;&#35745;&#22120;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#19968;&#31181;&#33258;&#24213;&#21521;&#19978;&#30340;&#26041;&#27861;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#24179;&#31227;&#23545;&#25968;&#30340;$f$&#25955;&#24230;&#30340;&#30446;&#26631;&#20989;&#25968;&#12290;&#36825;&#39033;&#30740;&#31350;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#25910;&#25947;&#24615;&#30340;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#65292;&#20998;&#31867;&#20219;&#21153;&#34987;&#24418;&#24335;&#21270;&#20026;&#36890;&#36807;&#26368;&#23567;&#21270;&#20132;&#21449;&#29109;&#26469;&#35299;&#20915;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#22312;&#30446;&#26631;&#20989;&#25968;&#35774;&#35745;&#26041;&#38754;&#30340;&#26368;&#26032;&#36827;&#23637;&#20351;&#24471;$f$&#25955;&#24230;&#24230;&#37327;&#21487;&#20197;&#25512;&#24191;&#20998;&#31867;&#38382;&#39064;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#37319;&#29992;&#36125;&#21494;&#26031;&#35270;&#35282;&#65292;&#24182;&#23558;&#20998;&#31867;&#20219;&#21153;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#26368;&#22823;&#21518;&#39564;&#27010;&#29575;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31867;&#22522;&#20110;$f$&#25955;&#24230;&#21464;&#20998;&#34920;&#31034;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#20174;&#20013;&#25552;&#21462;&#20102;&#19968;&#31995;&#21015;&#20116;&#20010;&#21518;&#39564;&#27010;&#29575;&#20272;&#35745;&#22120;&#65292;&#21033;&#29992;&#20102;&#20247;&#25152;&#21608;&#30693;&#30340;$f$&#25955;&#24230;&#12290;&#27492;&#22806;&#65292;&#21463;&#21040;&#25913;&#36827;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#25361;&#25112;&#30340;&#39537;&#21160;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#24213;&#21521;&#19978;&#30340;&#26041;&#27861;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#25552;&#20986;&#19968;&#20010;&#23545;&#24212;&#20110;&#19968;&#31181;&#26032;&#30340;&#34987;&#31216;&#20026;&#24179;&#31227;&#23545;&#25968; (SL) &#30340;$f$&#25955;&#24230;&#30340;&#26032;&#30446;&#26631;&#20989;&#25968;&#65288;&#21644;&#21518;&#39564;&#27010;&#29575;&#20272;&#35745;&#22120;&#65289;&#30340;&#20844;&#24335;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#25910;&#25947;&#24615;&#30340;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In deep learning, classification tasks are formalized as optimization problems solved via the minimization of the cross-entropy. However, recent advancements in the design of objective functions allow the $f$-divergence measure to generalize the formulation of the optimization problem for classification. With this goal in mind, we adopt a Bayesian perspective and formulate the classification task as a maximum a posteriori probability problem. We propose a class of objective functions based on the variational representation of the $f$-divergence, from which we extract a list of five posterior probability estimators leveraging well-known $f$-divergences. In addition, driven by the challenge of improving the state-of-the-art approach, we propose a bottom-up method that leads us to the formulation of a new objective function (and posterior probability estimator) corresponding to a novel $f$-divergence referred to as shifted log (SL). First, we theoretically prove the convergence property o
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#31639;&#27861;&#34920;&#31034;&#38598;&#25104;&#21040;&#31639;&#27861;&#36873;&#25321;&#20013;&#65292;&#20174;&#32780;&#22635;&#34917;&#20102;&#24403;&#21069;&#31639;&#27861;&#36873;&#25321;&#25216;&#26415;&#23545;&#31639;&#27861;&#29305;&#24449;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2311.13184</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#30340;&#31639;&#27861;&#36873;&#25321;&#65306;&#26397;&#30528;&#20840;&#38754;&#31639;&#27861;&#34920;&#31034;&#30340;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Large Language Model-Enhanced Algorithm Selection: Towards Comprehensive Algorithm Representation. (arXiv:2311.13184v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.13184
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#31639;&#27861;&#34920;&#31034;&#38598;&#25104;&#21040;&#31639;&#27861;&#36873;&#25321;&#20013;&#65292;&#20174;&#32780;&#22635;&#34917;&#20102;&#24403;&#21069;&#31639;&#27861;&#36873;&#25321;&#25216;&#26415;&#23545;&#31639;&#27861;&#29305;&#24449;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31639;&#27861;&#36873;&#25321;&#26088;&#22312;&#22312;&#25191;&#34892;&#20043;&#21069;&#35782;&#21035;&#35299;&#20915;&#29305;&#23450;&#38382;&#39064;&#30340;&#26368;&#21512;&#36866;&#31639;&#27861;&#65292;&#24050;&#25104;&#20026;&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20851;&#38190;&#36807;&#31243;&#12290;&#24403;&#21069;&#20027;&#27969;&#30340;&#31639;&#27861;&#36873;&#25321;&#25216;&#26415;&#20027;&#35201;&#20381;&#36182;&#20110;&#21508;&#31181;&#38382;&#39064;&#30340;&#29305;&#24449;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#27599;&#20010;&#31639;&#27861;&#30340;&#24615;&#33021;&#20316;&#20026;&#30417;&#30563;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23545;&#31639;&#27861;&#29305;&#24449;&#30340;&#32771;&#34385;&#23384;&#22312;&#37325;&#35201;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;&#36825;&#20027;&#35201;&#24402;&#22240;&#20110;&#31639;&#27861;&#30340;&#22266;&#26377;&#22797;&#26434;&#24615;&#65292;&#20351;&#24471;&#22312;&#19981;&#21516;&#31181;&#31867;&#30340;&#31639;&#27861;&#20013;&#25214;&#21040;&#19968;&#31181;&#26222;&#36866;&#26377;&#25928;&#30340;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#29305;&#21035;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#24573;&#35270;&#20102;&#36825;&#19968;&#26041;&#38754;&#26080;&#30097;&#20250;&#24433;&#21709;&#31639;&#27861;&#36873;&#25321;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#38388;&#25509;&#38656;&#35201;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#30340;&#25968;&#37327;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#19968;&#31354;&#30333;&#65292;&#21363;&#23558;&#31639;&#27861;&#34920;&#31034;&#38598;&#25104;&#21040;&#31639;&#27861;&#36873;&#25321;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algorithm selection aims to identify the most suitable algorithm for solving a specific problem before execution, which has become a critical process of the AutoML. Current mainstream algorithm selection techniques rely heavily on feature representations of various problems and employ the performance of each algorithm as supervised information. However, there is a significant research gap concerning the consideration of algorithm features. This gap is primarily attributed to the inherent complexity of algorithms, making it particularly challenging to find a universally effective feature extraction method that is applicable across a diverse range of algorithms. Unfortunately, neglecting this aspect undoubtedly impacts the accuracy of algorithm selection and indirectly necessitates an increased volume of problem data for training purposes. This paper takes a significant stride towards addressing this gap by proposing an approach that integrates algorithm representation into the algorithm
&lt;/p&gt;</description></item><item><title>GOPlan&#26159;&#19968;&#20010;&#20351;&#29992;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#35745;&#21010;&#30340;&#30446;&#26631;&#26465;&#20214;&#19979;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#20808;&#39564;&#31574;&#30053;&#21644;&#20351;&#29992;&#37325;&#26032;&#20998;&#26512;&#26041;&#27861;&#29983;&#25104;&#34394;&#26500;&#36712;&#36857;&#65292;&#29992;&#20197;&#25552;&#39640;&#24615;&#33021;&#21644;&#22788;&#29702;&#26377;&#38480;&#25968;&#25454;&#39044;&#31639;&#21644;&#26410;&#35265;&#30446;&#26631;&#27867;&#21270;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.20025</link><description>&lt;p&gt;
GOPlan:&#36890;&#36807;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#35745;&#21010;&#30340;&#30446;&#26631;&#26465;&#20214;&#19979;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
GOPlan: Goal-conditioned Offline Reinforcement Learning by Planning with Learned Models. (arXiv:2310.20025v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20025
&lt;/p&gt;
&lt;p&gt;
GOPlan&#26159;&#19968;&#20010;&#20351;&#29992;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#35745;&#21010;&#30340;&#30446;&#26631;&#26465;&#20214;&#19979;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#20808;&#39564;&#31574;&#30053;&#21644;&#20351;&#29992;&#37325;&#26032;&#20998;&#26512;&#26041;&#27861;&#29983;&#25104;&#34394;&#26500;&#36712;&#36857;&#65292;&#29992;&#20197;&#25552;&#39640;&#24615;&#33021;&#21644;&#22788;&#29702;&#26377;&#38480;&#25968;&#25454;&#39044;&#31639;&#21644;&#26410;&#35265;&#30446;&#26631;&#27867;&#21270;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#30446;&#26631;&#26465;&#20214;&#19979;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;GCRL&#65289;&#20026;&#20174;&#22810;&#26679;&#21270;&#21644;&#22810;&#20219;&#21153;&#30340;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#36890;&#29992;&#31574;&#30053;&#25552;&#20379;&#20102;&#21487;&#34892;&#30340;&#33539;&#20363;&#12290;&#23613;&#31649;&#36817;&#26399;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#20027;&#23548;&#30340;&#31163;&#32447;GCRL&#26041;&#27861;&#20173;&#28982;&#21463;&#38480;&#20110;&#26080;&#27169;&#22411;&#26041;&#27861;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#24212;&#23545;&#26377;&#38480;&#25968;&#25454;&#39044;&#31639;&#21644;&#26410;&#35265;&#30446;&#26631;&#27867;&#21270;&#30340;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20004;&#38454;&#27573;&#27169;&#22411;&#20026;&#22522;&#30784;&#30340;&#26694;&#26550;&#65292;Goal-conditioned Offline Planning&#65288;GOPlan&#65289;&#65292;&#21253;&#25324;&#65288;1&#65289;&#39044;&#35757;&#32451;&#19968;&#20010;&#33021;&#22815;&#25429;&#25417;&#22810;&#30446;&#26631;&#25968;&#25454;&#38598;&#20013;&#22810;&#27169;&#24577;&#21160;&#20316;&#20998;&#24067;&#30340;&#20808;&#39564;&#31574;&#30053;&#65307;&#65288;2&#65289;&#21033;&#29992;&#35268;&#21010;&#30340;&#37325;&#26032;&#20998;&#26512;&#26041;&#27861;&#20026;&#24494;&#35843;&#31574;&#30053;&#29983;&#25104;&#34394;&#26500;&#36712;&#36857;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20808;&#39564;&#31574;&#30053;&#22522;&#20110;&#19968;&#20010;&#20855;&#26377;&#26126;&#26174;&#27169;&#24335;&#20998;&#31163;&#30340;&#24102;&#20248;&#21183;&#26435;&#37325;&#30340;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#20197;&#20811;&#26381;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#21160;&#20316;&#30340;&#32570;&#28857;&#12290;&#20026;&#36827;&#19968;&#27493;&#20248;&#21270;&#31574;&#30053;&#65292;&#37325;&#26032;&#20998;&#26512;&#26041;&#27861;&#36890;&#36807;&#35268;&#21010;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#34394;&#26500;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline goal-conditioned RL (GCRL) offers a feasible paradigm to learn general-purpose policies from diverse and multi-task offline datasets. Despite notable recent progress, the predominant offline GCRL methods have been restricted to model-free approaches, constraining their capacity to tackle limited data budgets and unseen goal generalization. In this work, we propose a novel two-stage model-based framework, Goal-conditioned Offline Planning (GOPlan), including (1) pretraining a prior policy capable of capturing multi-modal action distribution within the multi-goal dataset; (2) employing the reanalysis method with planning to generate imagined trajectories for funetuning policies. Specifically, the prior policy is based on an advantage-weighted Conditioned Generative Adversarial Networks that exhibits distinct mode separation to overcome the pitfalls of out-of-distribution (OOD) actions. For further policy optimization, the reanalysis method generates high-quality imaginary data by
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LOOP-MAC&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#30340;&#20998;&#24067;&#24335;&#20248;&#21270;&#26469;&#21327;&#35843;&#34394;&#25311;&#30005;&#21378;&#65288;VPP&#65289;&#30340;&#36164;&#20135;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#22810;&#26234;&#33021;&#20307;&#21327;&#35843;&#35270;&#35282;&#65292;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#22120;&#21152;&#36895;&#35299;&#20915;&#26041;&#26696;&#25628;&#32034;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#27979;&#37327;&#22270;&#26469;&#20445;&#35777;&#26368;&#20339;&#35299;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2310.17882</link><description>&lt;p&gt;
&#20026;&#21327;&#35843;&#34394;&#25311;&#30005;&#21378;&#36164;&#20135;&#30340;&#20998;&#24067;&#24335;&#20248;&#21270;&#24341;&#20837;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Machine Learning Infused Distributed Optimization for Coordinating Virtual Power Plant Assets. (arXiv:2310.17882v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17882
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LOOP-MAC&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#30340;&#20998;&#24067;&#24335;&#20248;&#21270;&#26469;&#21327;&#35843;&#34394;&#25311;&#30005;&#21378;&#65288;VPP&#65289;&#30340;&#36164;&#20135;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#22810;&#26234;&#33021;&#20307;&#21327;&#35843;&#35270;&#35282;&#65292;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#22120;&#21152;&#36895;&#35299;&#20915;&#26041;&#26696;&#25628;&#32034;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#27979;&#37327;&#22270;&#26469;&#20445;&#35777;&#26368;&#20339;&#35299;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;&#20998;&#24067;&#24335;&#33021;&#28304;&#36164;&#28304;&#65288;DER&#65289;&#37096;&#32626;&#26085;&#30410;&#22686;&#21152;&#30340;&#20852;&#36259;&#65292;&#34394;&#25311;&#30005;&#21378;&#65288;VPP&#65289;&#24050;&#25104;&#20026;&#27719;&#38598;&#21508;&#31181;DER&#24182;&#20419;&#36827;&#20854;&#21442;&#19982;&#25209;&#21457;&#33021;&#28304;&#24066;&#22330;&#30340;&#20851;&#38190;&#24037;&#20855;&#12290;&#36825;&#20123;VPP&#37096;&#32626;&#24471;&#21040;&#20102;&#32852;&#37030;&#33021;&#28304;&#30417;&#31649;&#22996;&#21592;&#20250;&#31532;2222&#21495;&#21629;&#20196;&#30340;&#25512;&#21160;&#65292;&#35813;&#21629;&#20196;&#20351;DER&#21644;VPP&#22312;&#24066;&#22330;&#39046;&#22495;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#28982;&#32780;&#65292;DER&#30340;&#22810;&#26679;&#24615;&#21644;&#20998;&#25955;&#24615;&#36136;&#32473;VPP&#36164;&#20135;&#30340;&#21487;&#25193;&#23637;&#21327;&#35843;&#24102;&#26469;&#20102;&#26174;&#33879;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#25928;&#29575;&#21644;&#36895;&#24230;&#29942;&#39048;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#30340;&#20998;&#24067;&#24335;&#20248;&#21270;&#26041;&#27861;&#26469;&#21327;&#35843;VPP&#36164;&#20135;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21517;&#20026;LOOP-MAC&#65288;&#20026;&#22810;&#26234;&#33021;&#20307;&#21327;&#35843;&#20248;&#21270;&#36807;&#31243;&#23398;&#20064;&#20248;&#21270;&#65289;&#65292;&#37319;&#29992;&#22810;&#26234;&#33021;&#20307;&#21327;&#35843;&#35270;&#35282;&#65292;&#27599;&#20010;VPP&#20195;&#29702;&#31649;&#29702;&#22810;&#20010;DER&#24182;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#22120;&#21152;&#36895;&#35299;&#20915;&#26041;&#26696;&#25628;&#32034;&#12290;LOOP-MAC&#26041;&#27861;&#20351;&#29992;&#19968;&#20010;&#27979;&#37327;&#22270;&#30830;&#20445;&#20102;&#26368;&#20339;&#35299;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Amid the increasing interest in the deployment of Distributed Energy Resources (DERs), the Virtual Power Plant (VPP) has emerged as a pivotal tool for aggregating diverse DERs and facilitating their participation in wholesale energy markets. These VPP deployments have been fueled by the Federal Energy Regulatory Commission's Order 2222, which makes DERs and VPPs competitive across market segments. However, the diversity and decentralized nature of DERs present significant challenges to the scalable coordination of VPP assets. To address efficiency and speed bottlenecks, this paper presents a novel machine learning-assisted distributed optimization to coordinate VPP assets. Our method, named LOOP-MAC(Learning to Optimize the Optimization Process for Multi-agent Coordination), adopts a multi-agent coordination perspective where each VPP agent manages multiple DERs and utilizes neural network approximators to expedite the solution search. The LOOP-MAC method employs a gauge map to guarant
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32858;&#31867;&#26041;&#27861;&#65292;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#26469;&#24341;&#23548;&#32858;&#31867;&#12290;&#36890;&#36807;&#21033;&#29992;WordNet&#30340;&#25991;&#26412;&#35821;&#20041;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#22270;&#20687;&#32858;&#31867;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.11989</link><description>&lt;p&gt;
&#24102;&#26377;&#22806;&#37096;&#24341;&#23548;&#30340;&#22270;&#20687;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Image Clustering with External Guidance. (arXiv:2310.11989v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11989
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32858;&#31867;&#26041;&#27861;&#65292;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#26469;&#24341;&#23548;&#32858;&#31867;&#12290;&#36890;&#36807;&#21033;&#29992;WordNet&#30340;&#25991;&#26412;&#35821;&#20041;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#22270;&#20687;&#32858;&#31867;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32858;&#31867;&#30340;&#26680;&#24515;&#26159;&#34701;&#20837;&#20808;&#21069;&#30340;&#30693;&#35782;&#26469;&#26500;&#24314;&#30417;&#30563;&#20449;&#21495;&#12290;&#20174;&#22522;&#20110;&#25968;&#25454;&#32039;&#23494;&#24615;&#30340;&#32463;&#20856;k&#22343;&#20540;&#21040;&#26368;&#36817;&#30340;&#22522;&#20110;&#33258;&#30417;&#30563;&#24341;&#23548;&#30340;&#23545;&#27604;&#32858;&#31867;&#65292;&#32858;&#31867;&#26041;&#27861;&#30340;&#36827;&#27493;&#19982;&#30417;&#30563;&#20449;&#21495;&#30340;&#21457;&#23637;&#20869;&#22312;&#22320;&#30456;&#23545;&#24212;&#12290;&#30446;&#21069;&#65292;&#24456;&#22810;&#24037;&#20316;&#24050;&#32463;&#33268;&#21147;&#20110;&#20174;&#25968;&#25454;&#20013;&#25366;&#25496;&#20869;&#37096;&#30417;&#30563;&#20449;&#21495;&#12290;&#28982;&#32780;&#65292;&#20016;&#23500;&#30340;&#22806;&#37096;&#30693;&#35782;&#65292;&#20363;&#22914;&#35821;&#20041;&#25551;&#36848;&#65292;&#33258;&#28982;&#22320;&#20419;&#36827;&#20102;&#32858;&#31867;&#65292;&#21364;&#34987;&#36951;&#25022;&#22320;&#24573;&#35270;&#20102;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#20316;&#20026;&#26032;&#30340;&#30417;&#30563;&#20449;&#21495;&#26469;&#24341;&#23548;&#32858;&#31867;&#65292;&#21363;&#20351;&#23427;&#20284;&#20046;&#19982;&#32473;&#23450;&#30340;&#25968;&#25454;&#26080;&#20851;&#12290;&#20026;&#20102;&#23454;&#29616;&#21644;&#39564;&#35777;&#25105;&#20204;&#30340;&#24819;&#27861;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22806;&#37096;&#24341;&#23548;&#30340;&#32858;&#31867;&#26041;&#27861;&#65288;&#25991;&#26412;&#36741;&#21161;&#32858;&#31867;&#65292;TAC&#65289;&#65292;&#23427;&#21033;&#29992;WordNet&#30340;&#25991;&#26412;&#35821;&#20041;&#26469;&#20419;&#36827;&#22270;&#20687;&#32858;&#31867;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;TAC&#39318;&#20808;&#36873;&#25321;&#24182;&#26816;&#32034;&#26368;&#33021;&#21306;&#20998;&#22270;&#20687;&#30340;WordNet&#21517;&#35789;&#20197;&#22686;&#24378;&#32858;&#31867;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The core of clustering is incorporating prior knowledge to construct supervision signals. From classic k-means based on data compactness to recent contrastive clustering guided by self-supervision, the evolution of clustering methods intrinsically corresponds to the progression of supervision signals. At present, substantial efforts have been devoted to mining internal supervision signals from data. Nevertheless, the abundant external knowledge such as semantic descriptions, which naturally conduces to clustering, is regrettably overlooked. In this work, we propose leveraging external knowledge as a new supervision signal to guide clustering, even though it seems irrelevant to the given data. To implement and validate our idea, we design an externally guided clustering method (Text-Aided Clustering, TAC), which leverages the textual semantics of WordNet to facilitate image clustering. Specifically, TAC first selects and retrieves WordNet nouns that best distinguish images to enhance th
&lt;/p&gt;</description></item><item><title>ReMax&#26159;&#19968;&#31181;&#29992;&#20110;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21333;&#12289;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;PPO&#65292;ReMax&#31616;&#21270;&#20102;&#23454;&#29616;&#65292;&#20943;&#23569;&#20102;&#20869;&#23384;&#20351;&#29992;&#65292;&#24182;&#35299;&#20915;&#20102;fine-tuning&#26102;&#30340;&#20869;&#23384;&#28322;&#20986;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.10505</link><description>&lt;p&gt;
ReMax:&#19968;&#31181;&#29992;&#20110;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21333;&#12289;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ReMax: A Simple, Effective, and Efficient Reinforcement Learning Method for Aligning Large Language Models. (arXiv:2310.10505v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10505
&lt;/p&gt;
&lt;p&gt;
ReMax&#26159;&#19968;&#31181;&#29992;&#20110;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21333;&#12289;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;PPO&#65292;ReMax&#31616;&#21270;&#20102;&#23454;&#29616;&#65292;&#20943;&#23569;&#20102;&#20869;&#23384;&#20351;&#29992;&#65292;&#24182;&#35299;&#20915;&#20102;fine-tuning&#26102;&#30340;&#20869;&#23384;&#28322;&#20986;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#40784;&#23545;&#20110;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38750;&#24120;&#37325;&#35201;&#12290;&#30446;&#21069;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#20027;&#35201;&#31574;&#30053;&#26159;&#36890;&#36807;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#65292;&#20854;&#20013;PPO&#26159;&#20107;&#23454;&#19978;&#30340;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#20247;&#25152;&#21608;&#30693;&#65292;PPO&#22312;&#35745;&#31639;&#25928;&#29575;&#19978;&#23384;&#22312;&#38382;&#39064;&#65292;&#36825;&#26159;&#26412;&#35770;&#25991;&#35797;&#22270;&#35299;&#20915;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#22312;RLHF&#20219;&#21153;&#20013;&#30830;&#23450;&#20102;&#19977;&#20010;&#37325;&#35201;&#29305;&#24615;&#65306;&#24555;&#36895;&#27169;&#25311;&#12289;&#30830;&#23450;&#24615;&#36716;&#25442;&#21644;&#36712;&#36857;&#32423;&#22870;&#21169;&#65292;&#36825;&#20123;&#29305;&#24615;&#22312;PPO&#20013;&#27809;&#26377;&#24471;&#21040;&#20805;&#20998;&#21033;&#29992;&#12290;&#22522;&#20110;&#36825;&#20123;&#35266;&#23519;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#38024;&#23545;RLHF&#30340;&#26032;&#31639;&#27861;&#65292;&#31216;&#20026;ReMax&#12290;ReMax&#30340;&#31639;&#27861;&#35774;&#35745;&#26159;&#22522;&#20110;&#19968;&#31181;&#24191;&#20026;&#20351;&#29992;&#30340;&#31639;&#27861;REINFORCE&#65292;&#20294;&#37197;&#22791;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#24046;&#20943;&#23569;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;PPO&#20855;&#26377;&#19977;&#37325;&#20248;&#21183;&#65306;&#39318;&#20808;&#65292;ReMax&#23454;&#29616;&#31616;&#21333;&#65292;&#28040;&#38500;&#20102;PPO&#20013;&#30340;&#35768;&#22810;&#19982;&#35268;&#27169;&#30456;&#20851;&#19988;&#32321;&#29712;&#30340;&#36229;&#21442;&#25968;&#12290;&#20854;&#27425;&#65292;ReMax&#21407;&#21017;&#19978;&#21487;&#20197;&#33410;&#32422;&#32422;50%&#30340;&#20869;&#23384;&#20351;&#29992;&#12290;&#32467;&#26524;&#23548;&#33268;PPO&#22312;&#36827;&#34892;fine-tuning&#26102;&#20986;&#29616;&#20869;&#23384;&#28322;&#20986;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Alignment is of critical importance for training large language models (LLMs). The predominant strategy to address this is through Reinforcement Learning from Human Feedback (RLHF), where PPO serves as the de-facto algorithm. Yet, PPO is known to suffer from computational inefficiency, which is a challenge that this paper aims to address. We identify three important properties in RLHF tasks: fast simulation, deterministic transitions, and trajectory-level rewards, which are not leveraged in PPO. Based on such observations, we develop a new algorithm tailored for RLHF, called ReMax. The algorithm design of ReMax is built on a celebrated algorithm REINFORCE but is equipped with a new variance-reduction technique.  Our method has three-fold advantages over PPO: first, ReMax is simple to implement and removes many hyper-parameters in PPO, which are scale-sensitive and laborious to tune. Second, ReMax saves about 50% memory usage in principle. As a result, PPO runs out-of-memory when fine-t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SuperCalo&#30340;&#33021;&#37327;&#27785;&#31215;&#27169;&#25311;&#36229;&#20998;&#36776;&#29575;&#27169;&#22411;&#65292;&#33021;&#22815;&#24555;&#36895;&#19978;&#37319;&#26679;&#39640;&#32500;&#32454;&#31890;&#24230;&#30340;&#33021;&#37327;&#27785;&#31215;&#27169;&#25311;&#65292;&#20174;&#32780;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#21644;&#29983;&#25104;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2308.11700</link><description>&lt;p&gt;
SuperCalo: &#33021;&#37327;&#27785;&#31215;&#37327;&#27169;&#25311;&#30340;&#36229;&#20998;&#36776;&#29575;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
SuperCalo: Calorimeter shower super-resolution. (arXiv:2308.11700v1 [physics.ins-det])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11700
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SuperCalo&#30340;&#33021;&#37327;&#27785;&#31215;&#27169;&#25311;&#36229;&#20998;&#36776;&#29575;&#27169;&#22411;&#65292;&#33021;&#22815;&#24555;&#36895;&#19978;&#37319;&#26679;&#39640;&#32500;&#32454;&#31890;&#24230;&#30340;&#33021;&#37327;&#27785;&#31215;&#27169;&#25311;&#65292;&#20174;&#32780;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#21644;&#29983;&#25104;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#37327;&#27785;&#31215;&#27169;&#25311;&#26159;&#22823;&#22411;&#24378;&#23376;&#23545;&#25758;&#26426;&#35745;&#31639;&#27969;&#31243;&#20013;&#30340;&#20027;&#35201;&#29942;&#39048;&#12290;&#26368;&#36817;&#26377;&#19968;&#20123;&#24037;&#20316;&#20351;&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#26469;&#20811;&#26381;&#36825;&#20010;&#25361;&#25112;&#65292;&#20294;&#35768;&#22810;&#34920;&#29616;&#26368;&#20339;&#30340;&#27169;&#22411;&#22312;&#35757;&#32451;&#21644;&#29983;&#25104;&#26102;&#38388;&#19978;&#26080;&#27861;&#24456;&#22909;&#22320;&#36866;&#24212;&#39640;&#32500;&#33021;&#37327;&#27785;&#31215;&#27169;&#25311;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SuperCalo&#30340;&#22522;&#20110;&#27969;&#30340;&#36229;&#20998;&#36776;&#29575;&#27169;&#22411;&#65292;&#24182;&#35777;&#26126;&#20102;&#39640;&#32500;&#32454;&#31890;&#24230;&#30340;&#33021;&#37327;&#27785;&#31215;&#27169;&#25311;&#21487;&#20197;&#20174;&#31895;&#31890;&#24230;&#27169;&#25311;&#20013;&#24555;&#36895;&#19978;&#37319;&#26679;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#21487;&#20197;&#38477;&#20302;&#19982;&#24555;&#36895;&#33021;&#37327;&#27785;&#31215;&#27169;&#25311;&#27169;&#22411;&#30456;&#20851;&#30340;&#35745;&#31639;&#25104;&#26412;&#12289;&#20869;&#23384;&#38656;&#27714;&#21644;&#29983;&#25104;&#26102;&#38388;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#30001;SuperCalo&#19978;&#37319;&#26679;&#24471;&#21040;&#30340;&#33021;&#37327;&#27785;&#31215;&#27169;&#25311;&#20855;&#26377;&#39640;&#24230;&#21464;&#21270;&#30340;&#29305;&#28857;&#12290;&#36825;&#20351;&#24471;&#21487;&#20197;&#20174;&#36739;&#23569;&#30340;&#31895;&#31890;&#24230;&#27169;&#25311;&#20013;&#19978;&#37319;&#26679;&#20986;&#22810;&#20010;&#39640;&#32500;&#33021;&#37327;&#27785;&#31215;&#27169;&#25311;&#65292;&#20197;&#39640;&#20445;&#30495;&#24230;&#30340;&#26041;&#24335;&#36827;&#19968;&#27493;&#20943;&#23569;&#29983;&#25104;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Calorimeter shower simulation is a major bottleneck in the Large Hadron Collider computational pipeline. There have been recent efforts to employ deep-generative surrogate models to overcome this challenge. However, many of best performing models have training and generation times that do not scale well to high-dimensional calorimeter showers. In this work, we introduce SuperCalo, a flow-based super-resolution model, and demonstrate that high-dimensional fine-grained calorimeter showers can be quickly upsampled from coarse-grained showers. This novel approach presents a way to reduce computational cost, memory requirements and generation time associated with fast calorimeter simulation models. Additionally, we show that the showers upsampled by SuperCalo possess a high degree of variation. This allows a large number of high-dimensional calorimeter showers to be upsampled from much fewer coarse showers with high-fidelity, which results in additional reduction in generation time.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#39044;&#35757;&#32451;&#27169;&#22411;&#23545;&#31283;&#20581;&#24615;&#22270;&#20687;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#65292;&#30452;&#25509;&#27979;&#37327;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#25193;&#23637;&#22270;&#20687;&#25968;&#25454;&#38598;&#20197;&#23436;&#25104;&#36229;&#20986;&#22522;&#20934;&#33539;&#22260;&#30340;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2308.10632</link><description>&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#23548;&#21521;&#30340;&#31283;&#20581;&#24615;: &#36890;&#36807;&#39044;&#35757;&#32451;&#27169;&#22411;&#23545;&#31283;&#20581;&#24615;&#22270;&#20687;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Foundation Model-oriented Robustness: Robust Image Model Evaluation with Pretrained Models. (arXiv:2308.10632v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10632
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#39044;&#35757;&#32451;&#27169;&#22411;&#23545;&#31283;&#20581;&#24615;&#22270;&#20687;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#65292;&#30452;&#25509;&#27979;&#37327;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#25193;&#23637;&#22270;&#20687;&#25968;&#25454;&#38598;&#20197;&#23436;&#25104;&#36229;&#20986;&#22522;&#20934;&#33539;&#22260;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#26377;&#38480;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#28982;&#32780;&#65292;&#22266;&#23450;&#22522;&#20934;&#19978;&#30340;&#20998;&#25968;&#26159;&#21542;&#36275;&#20197;&#20805;&#20998;&#20307;&#29616;&#27169;&#22411;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#24615;&#33021;&#20173;&#26377;&#35752;&#35770;&#12290;&#23454;&#38469;&#19978;&#65292;&#29702;&#24819;&#30340;&#31283;&#20581;&#27169;&#22411;&#21487;&#33021;&#19982;&#31070;&#35861;&#65288;&#20363;&#22914;&#65292;&#20154;&#31867;&#29992;&#25143;&#65289;&#34920;&#29616;&#31867;&#20284;&#65292;&#22240;&#27492;&#19968;&#20010;&#22909;&#30340;&#35780;&#20272;&#21327;&#35758;&#21487;&#33021;&#26159;&#35780;&#20272;&#27169;&#22411;&#30456;&#23545;&#20110;&#31070;&#35861;&#30340;&#34892;&#20026;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31283;&#20581;&#24615;&#27979;&#37327;&#26041;&#27861;&#65292;&#30452;&#25509;&#27979;&#37327;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30456;&#23545;&#20110;&#26367;&#20195;&#31070;&#35861;&#65288;&#21363;&#22522;&#30784;&#27169;&#22411;&#65289;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22522;&#20934;&#33539;&#22260;&#20043;&#22806;&#23436;&#25104;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#28155;&#21152;&#20855;&#26377;&#36275;&#22815;&#25200;&#21160;&#30340;&#26032;&#26679;&#26412;&#26469;&#25193;&#23637;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#26679;&#26412;&#19982;&#21407;&#22987;&#38598;&#21512;&#20013;&#30340;&#26679;&#26412;&#26377;&#25152;&#19981;&#21516;&#65292;&#20294;&#20173;&#38480;&#21046;&#22312;&#21407;&#22987;&#27979;&#35797;&#22270;&#20687;&#25152;&#20195;&#34920;&#30340;&#30456;&#21516;&#22270;&#20687;-&#26631;&#31614;&#32467;&#26500;&#20869;&#65292;&#30001;&#39044;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;&#38480;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning has demonstrated remarkable performance over finite datasets, yet whether the scores over the fixed benchmarks can sufficiently indicate the model's performance in the real world is still in discussion. In reality, an ideal robust model will probably behave similarly to the oracle (e.g., the human users), thus a good evaluation protocol is probably to evaluate the models' behaviors in comparison to the oracle. In this paper, we introduce a new robustness measurement that directly measures the image classification model's performance compared with a surrogate oracle (i.e., a foundation model). Besides, we design a simple method that can accomplish the evaluation beyond the scope of the benchmarks. Our method extends the image datasets with new samples that are sufficiently perturbed to be distinct from the ones in the original sets, but are still bounded within the same image-label structure the original test image represents, constrained by a foundation model pretraine
&lt;/p&gt;</description></item><item><title>MIMIC&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#20687;&#23545;&#24212;&#20851;&#31995;&#30340;&#36974;&#34109;&#22270;&#20687;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#25366;&#25496;&#19981;&#38656;&#35201;&#20219;&#20309;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;&#22810;&#20010;&#33258;&#30417;&#30563;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#36798;&#21040;&#20102;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;&#20248;&#20110;&#20351;&#29992;&#27880;&#37322;&#25366;&#25496;&#30340;&#34920;&#31034;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.15128</link><description>&lt;p&gt;
MIMIC: &#22522;&#20110;&#22270;&#20687;&#23545;&#24212;&#20851;&#31995;&#30340;&#36974;&#34109;&#22270;&#20687;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
MIMIC: Masked Image Modeling with Image Correspondences. (arXiv:2306.15128v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15128
&lt;/p&gt;
&lt;p&gt;
MIMIC&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#20687;&#23545;&#24212;&#20851;&#31995;&#30340;&#36974;&#34109;&#22270;&#20687;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#25366;&#25496;&#19981;&#38656;&#35201;&#20219;&#20309;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;&#22810;&#20010;&#33258;&#30417;&#30563;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#36798;&#21040;&#20102;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;&#20248;&#20110;&#20351;&#29992;&#27880;&#37322;&#25366;&#25496;&#30340;&#34920;&#31034;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#20687;&#32032;&#32423;&#30340;&#23494;&#38598;&#39044;&#27979;&#20219;&#21153;&#8212;&#8212;&#22914;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#28145;&#24230;&#20272;&#35745;&#21644;&#35821;&#20041;&#20998;&#21106;&#8212;&#8212;&#22914;&#20170;&#20381;&#36182;&#20110;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#34920;&#31034;&#12290;&#22240;&#27492;&#65292;&#31579;&#36873;&#26377;&#25928;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#33267;&#20851;&#37325;&#35201;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#26377;&#25928;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20165;&#36890;&#36807;&#27169;&#25311;&#29615;&#22659;&#20013;&#30340;&#24102;&#26377;&#27880;&#37322;&#30340;3D&#32593;&#26684;&#12289;&#28857;&#20113;&#21644;&#30456;&#26426;&#21442;&#25968;&#31579;&#36873;&#32780;&#26469;&#65292;&#24182;&#19981;&#20855;&#22791;&#22810;&#35270;&#35282;&#22330;&#26223;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#20219;&#20309;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#31579;&#36873;&#26426;&#21046;&#12290;&#25105;&#20204;&#20174;&#24320;&#28304;&#35270;&#39057;&#25968;&#25454;&#38598;&#21644;&#21512;&#25104;&#30340;3D&#29615;&#22659;&#20013;&#25366;&#25496;&#20102;&#20004;&#20010;&#25968;&#25454;&#38598;&#65306;MIMIC-1M(&#21253;&#21547;1.3M&#20010;&#22810;&#35270;&#35282;&#22270;&#20687;&#23545;)&#21644;MIMIC-3M(&#21253;&#21547;3.1M&#20010;&#22810;&#35270;&#35282;&#22270;&#20687;&#23545;)&#12290;&#25105;&#20204;&#20351;&#29992;&#22810;&#20010;&#33258;&#30417;&#30563;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#37319;&#29992;&#19981;&#21516;&#30340;&#36974;&#34109;&#22270;&#20687;&#24314;&#27169;&#30446;&#26631;&#65292;&#23637;&#31034;&#20102;&#20197;&#19979;&#21457;&#29616;&#65306;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#65292;&#22522;&#20110;MIMIC-3M&#35757;&#32451;&#30340;&#34920;&#31034;&#20248;&#20110;&#20351;&#29992;&#27880;&#37322;&#25366;&#25496;&#30340;&#34920;&#31034;&#65292;&#21253;&#25324;&#28145;&#24230;&#20272;&#35745;&#12289;&#35821;&#20041;&#20998;&#21106;&#12289;&#34920;&#38754;&#27861;&#32447;&#21644;&#23039;&#24577;&#20272;&#35745;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many pixelwise dense prediction tasks-depth estimation and semantic segmentation in computer vision today rely on pretrained image representations. Therefore, curating effective pretraining datasets is vital. Unfortunately, the effective pretraining datasets are those with multi-view scenes and have only been curated using annotated 3D meshes, point clouds, and camera parameters from simulated environments. We propose a dataset-curation mechanism that does not require any annotations. We mine two datasets: MIMIC-1M with 1.3M and MIMIC-3M with 3.1M multi-view image pairs from open-sourced video datasets and from synthetic 3D environments. We train multiple self-supervised models with different masked image modeling objectives to showcase the following findings: Representations trained on MIMIC-3M outperform those mined using annotations on multiple downstream tasks, including depth estimation, semantic segmentation, surface normals, and pose estimation. They also outperform representati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#21151;&#33021;&#31561;&#20215;&#30340;&#35282;&#24230;&#20986;&#21457;&#30740;&#31350;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#30340;&#22797;&#26434;&#24615;&#65292;&#21457;&#29616;&#21033;&#29992;&#32622;&#25442;&#19981;&#21464;&#24615;&#30340;&#29305;&#24615;&#21487;&#20197;&#38477;&#20302;&#32593;&#32476;&#30340;&#22797;&#26434;&#24230;&#65292;&#36890;&#36807;&#36807;&#21442;&#25968;&#21270;&#21487;&#20197;&#22686;&#21152;&#35757;&#32451;&#32593;&#32476;&#30340;&#23481;&#26131;&#31243;&#24230;&#65292;&#24182;&#23545;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#20248;&#21270;&#21644;&#27867;&#21270;&#29702;&#35299;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2305.11417</link><description>&lt;p&gt;
&#20174;&#21151;&#33021;&#31561;&#20215;&#30340;&#35282;&#24230;&#30475;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Complexity of Feed-Forward Neural Networks from the Perspective of Functional Equivalence. (arXiv:2305.11417v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11417
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#21151;&#33021;&#31561;&#20215;&#30340;&#35282;&#24230;&#20986;&#21457;&#30740;&#31350;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#30340;&#22797;&#26434;&#24615;&#65292;&#21457;&#29616;&#21033;&#29992;&#32622;&#25442;&#19981;&#21464;&#24615;&#30340;&#29305;&#24615;&#21487;&#20197;&#38477;&#20302;&#32593;&#32476;&#30340;&#22797;&#26434;&#24230;&#65292;&#36890;&#36807;&#36807;&#21442;&#25968;&#21270;&#21487;&#20197;&#22686;&#21152;&#35757;&#32451;&#32593;&#32476;&#30340;&#23481;&#26131;&#31243;&#24230;&#65292;&#24182;&#23545;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#20248;&#21270;&#21644;&#27867;&#21270;&#29702;&#35299;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#32771;&#23519;&#21151;&#33021;&#31561;&#20215;&#30340;&#27010;&#24565;&#26469;&#30740;&#31350;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#30340;&#22797;&#26434;&#24615;&#65292;&#35813;&#27010;&#24565;&#34920;&#26126;&#19981;&#21516;&#30340;&#32593;&#32476;&#21442;&#25968;&#21270;&#21487;&#20197;&#23548;&#33268;&#30456;&#21516;&#30340;&#20989;&#25968;&#12290;&#25105;&#20204;&#21033;&#29992;&#32622;&#25442;&#19981;&#21464;&#24615;&#30340;&#29305;&#24615;&#20026;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#31867;&#23548;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35206;&#30422;&#25968;&#19978;&#30028;&#65292;&#21457;&#29616;&#21033;&#29992;&#35813;&#24615;&#36136;&#21487;&#20197;&#38477;&#20302;&#31070;&#32463;&#32593;&#32476;&#30340;&#22797;&#26434;&#24230;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#21442;&#25968;&#31354;&#38388;&#30340;&#23545;&#31216;&#32467;&#26500;&#65292;&#25105;&#20204;&#35777;&#26126;&#36866;&#24403;&#30340;&#38543;&#26426;&#21442;&#25968;&#21021;&#22987;&#21270;&#31574;&#30053;&#21487;&#20197;&#22686;&#21152;&#20248;&#21270;&#25910;&#25947;&#30340;&#27010;&#29575;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36807;&#21442;&#25968;&#21270;&#30340;&#32593;&#32476;&#24448;&#24448;&#26356;&#23481;&#26131;&#35757;&#32451;&#65292;&#21363;&#22686;&#21152;&#31070;&#32463;&#32593;&#32476;&#30340;&#23485;&#24230;&#20250;&#23548;&#33268;&#26377;&#25928;&#21442;&#25968;&#31354;&#38388;&#30340;&#20307;&#31215;&#36235;&#36817;&#20110;&#38646;&#12290;&#26412;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#36807;&#21442;&#25968;&#21270;&#30340;&#26032;&#35265;&#35299;&#65292;&#24182;&#23545;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#27867;&#21270;&#21644;&#20248;&#21270;&#29702;&#35299;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate the complexity of feed-forward neural networks by examining the concept of functional equivalence, which suggests that different network parameterizations can lead to the same function. We utilize the permutation invariance property to derive a novel covering number bound for the class of feedforward neural networks, which reveals that the complexity of a neural network can be reduced by exploiting this property. Furthermore, based on the symmetric structure of parameter space, we demonstrate that an appropriate strategy of random parameter initialization can increase the probability of convergence for optimization. We found that overparameterized networks tend to be easier to train in the sense that increasing the width of neural networks leads to a vanishing volume of the effective parameter space. Our findings offer new insights into overparameterization and have significant implications for understanding generalization and optimization in deep learning
&lt;/p&gt;</description></item><item><title>OpenBox&#26159;&#19968;&#20010;&#36890;&#29992;&#40657;&#30418;&#20248;&#21270;&#30340;Python&#24037;&#20855;&#21253;&#65292;&#25552;&#20379;&#20102;&#29992;&#25143;&#21451;&#22909;&#30340;&#25509;&#21475;&#21644;&#21487;&#35270;&#21270;&#21151;&#33021;&#65292;&#27169;&#22359;&#21270;&#35774;&#35745;&#33021;&#22815;&#22312;&#29616;&#26377;&#31995;&#32479;&#20013;&#28789;&#27963;&#37096;&#32626;&#65292;&#24182;&#19988;&#23454;&#39564;&#35777;&#26126;&#20854;&#27604;&#29616;&#26377;&#31995;&#32479;&#26356;&#26377;&#25928;&#21644;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2304.13339</link><description>&lt;p&gt;
OpenBox&#65306;&#36890;&#29992;&#40657;&#30418;&#20248;&#21270;&#30340; Python &#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
OpenBox: A Python Toolkit for Generalized Black-box Optimization. (arXiv:2304.13339v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13339
&lt;/p&gt;
&lt;p&gt;
OpenBox&#26159;&#19968;&#20010;&#36890;&#29992;&#40657;&#30418;&#20248;&#21270;&#30340;Python&#24037;&#20855;&#21253;&#65292;&#25552;&#20379;&#20102;&#29992;&#25143;&#21451;&#22909;&#30340;&#25509;&#21475;&#21644;&#21487;&#35270;&#21270;&#21151;&#33021;&#65292;&#27169;&#22359;&#21270;&#35774;&#35745;&#33021;&#22815;&#22312;&#29616;&#26377;&#31995;&#32479;&#20013;&#28789;&#27963;&#37096;&#32626;&#65292;&#24182;&#19988;&#23454;&#39564;&#35777;&#26126;&#20854;&#27604;&#29616;&#26377;&#31995;&#32479;&#26356;&#26377;&#25928;&#21644;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40657;&#30418;&#20248;&#21270;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#12289;&#23454;&#39564;&#35774;&#35745;&#21644;&#25968;&#25454;&#24211;&#21442;&#25968;&#35843;&#25972;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#29616;&#26377;&#36719;&#20214;&#21253;&#26102;&#65292;&#29992;&#25143;&#22312;&#36866;&#29992;&#24615;&#12289;&#24615;&#33021;&#21644;&#25928;&#29575;&#26041;&#38754;&#20173;&#38754;&#20020;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102; OpenBox&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#40657;&#30418;&#20248;&#21270;&#24037;&#20855;&#21253;&#65292;&#25552;&#39640;&#20102;&#20854;&#21487;&#29992;&#24615;&#12290;&#23427;&#23454;&#29616;&#20102;&#29992;&#25143;&#21451;&#22909;&#30340;&#25509;&#21475;&#21644;&#21487;&#35270;&#21270;&#21151;&#33021;&#65292;&#35753;&#29992;&#25143;&#33021;&#22815;&#23450;&#20041;&#21644;&#31649;&#29702;&#20219;&#21153;&#12290;OpenBox &#30340;&#27169;&#22359;&#21270;&#35774;&#35745;&#26377;&#21161;&#20110;&#22312;&#29616;&#26377;&#31995;&#32479;&#20013;&#28789;&#27963;&#37096;&#32626;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;OpenBox&#27604;&#29616;&#26377;&#31995;&#32479;&#26356;&#26377;&#25928;&#21644;&#39640;&#25928;&#12290;OpenBox &#30340;&#28304;&#20195;&#30721;&#21487;&#22312; https://github.com/PKU-DAIR/open-box &#20013;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Black-box optimization (BBO) has a broad range of applications, including automatic machine learning, experimental design, and database knob tuning. However, users still face challenges when applying BBO methods to their problems at hand with existing software packages in terms of applicability, performance, and efficiency. This paper presents OpenBox, an open-source BBO toolkit with improved usability. It implements user-friendly inferfaces and visualization for users to define and manage their tasks. The modular design behind OpenBox facilitates its flexible deployment in existing systems. Experimental results demonstrate the effectiveness and efficiency of OpenBox over existing systems. The source code of OpenBox is available at https://github.com/PKU-DAIR/open-box.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#20102;&#20302;&#24310;&#36831;&#27969;&#24335;&#20998;&#31163;&#24212;&#29992;&#20013;&#30340;&#22522;&#20110;&#35821;&#38899;&#20998;&#31163;&#30340;&#21457;&#35328;&#32773;&#20998;&#31163;&#65288;SSGD&#65289;&#22312;&#20250;&#35805;&#30005;&#35805;&#35821;&#38899;&#65288;CTS&#65289;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#20998;&#31163;&#35828;&#35805;&#20154;&#24182;&#22312;&#27599;&#20010;&#20998;&#31163;&#30340;&#27969;&#19978;&#24212;&#29992;&#35821;&#38899;&#27963;&#21160;&#26816;&#27979;&#65288;VAD&#65289;&#26469;&#25191;&#34892;&#21457;&#35328;&#32773;&#20998;&#31163;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#12289;&#22240;&#26524;&#21644;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#27844;&#28431;&#21435;&#38500;&#31639;&#27861;&#12290;&#22312;CALLHOME&#21644;Fisher&#35821;&#26009;&#24211;&#65288;&#31532;1&#21644;2&#37096;&#20998;&#65289;&#19978;&#30340;&#24615;&#33021;&#35780;&#20272;&#34920;&#26126;&#65292;SSGD&#31639;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#25552;&#39640;&#20998;&#31163;&#21644;&#21457;&#35328;&#32773;&#20998;&#31163;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.12002</link><description>&lt;p&gt;
&#30005;&#35805;&#20250;&#35805;&#30340;&#20302;&#24310;&#36831;&#21457;&#35328;&#20998;&#31163;&#21644;&#35821;&#38899;&#27963;&#21160;&#26816;&#27979;&#30340;&#31471;&#21040;&#31471;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
End-to-End Integration of Speech Separation and Voice Activity Detection for Low-Latency Diarization of Telephone Conversations. (arXiv:2303.12002v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12002
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#20102;&#20302;&#24310;&#36831;&#27969;&#24335;&#20998;&#31163;&#24212;&#29992;&#20013;&#30340;&#22522;&#20110;&#35821;&#38899;&#20998;&#31163;&#30340;&#21457;&#35328;&#32773;&#20998;&#31163;&#65288;SSGD&#65289;&#22312;&#20250;&#35805;&#30005;&#35805;&#35821;&#38899;&#65288;CTS&#65289;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#20998;&#31163;&#35828;&#35805;&#20154;&#24182;&#22312;&#27599;&#20010;&#20998;&#31163;&#30340;&#27969;&#19978;&#24212;&#29992;&#35821;&#38899;&#27963;&#21160;&#26816;&#27979;&#65288;VAD&#65289;&#26469;&#25191;&#34892;&#21457;&#35328;&#32773;&#20998;&#31163;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#12289;&#22240;&#26524;&#21644;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#27844;&#28431;&#21435;&#38500;&#31639;&#27861;&#12290;&#22312;CALLHOME&#21644;Fisher&#35821;&#26009;&#24211;&#65288;&#31532;1&#21644;2&#37096;&#20998;&#65289;&#19978;&#30340;&#24615;&#33021;&#35780;&#20272;&#34920;&#26126;&#65292;SSGD&#31639;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#25552;&#39640;&#20998;&#31163;&#21644;&#21457;&#35328;&#32773;&#20998;&#31163;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22522;&#20110;&#35821;&#38899;&#20998;&#31163;&#30340;&#21457;&#35328;&#32773;&#20998;&#31163;&#65288;SSGD&#65289;&#26159;&#19968;&#20010;&#36234;&#26469;&#36234;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#65292;&#36825;&#20027;&#35201;&#24471;&#30410;&#20110;&#35821;&#38899;&#20998;&#31163;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#23427;&#36890;&#36807;&#39318;&#20808;&#20998;&#31163;&#35828;&#35805;&#20154;&#65292;&#28982;&#21518;&#22312;&#27599;&#20010;&#20998;&#31163;&#30340;&#27969;&#19978;&#24212;&#29992;&#35821;&#38899;&#27963;&#21160;&#26816;&#27979;&#65288;VAD&#65289;&#26469;&#25191;&#34892;&#21457;&#35328;&#32773;&#20998;&#31163;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;&#20250;&#35805;&#30005;&#35805;&#35821;&#38899;&#65288;CTS&#65289;&#39046;&#22495;&#20013;&#30340;SSGD&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#65292;&#37325;&#28857;&#26159;&#20302;&#24310;&#36831;&#27969;&#24335;&#20998;&#31163;&#24212;&#29992;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19977;&#31181;&#26368;&#20808;&#36827;&#30340;&#35821;&#38899;&#20998;&#31163;&#65288;SSep&#65289;&#31639;&#27861;&#65292;&#24182;&#30740;&#31350;&#20102;&#23427;&#20204;&#22312;&#22312;&#32447;&#21644;&#31163;&#32447;&#22330;&#26223;&#19979;&#30340;&#24615;&#33021;&#65292;&#32771;&#34385;&#20102;&#38750;&#22240;&#26524;&#21644;&#22240;&#26524;&#23454;&#29616;&#20197;&#21450;&#36830;&#32493;SSep&#65288;CSS&#65289;&#31383;&#21475;&#25512;&#29702;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;SSGD&#31639;&#27861;&#22312;&#20004;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;CTS&#25968;&#25454;&#38598;CALLHOME&#21644;Fisher&#35821;&#26009;&#24211;&#65288;&#31532;1&#21644;2&#37096;&#20998;&#65289;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#35780;&#20272;&#20102;&#20998;&#31163;&#21644;&#21457;&#35328;&#32773;&#20998;&#31163;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#25552;&#39640;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#12289;&#22240;&#26524;&#21644;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#27844;&#28431;&#21435;&#38500;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works show that speech separation guided diarization (SSGD) is an increasingly promising direction, mainly thanks to the recent progress in speech separation. It performs diarization by first separating the speakers and then applying voice activity detection (VAD) on each separated stream. In this work we conduct an in-depth study of SSGD in the conversational telephone speech (CTS) domain, focusing mainly on low-latency streaming diarization applications. We consider three state-of-the-art speech separation (SSep) algorithms and study their performance both in online and offline scenarios, considering non-causal and causal implementations as well as continuous SSep (CSS) windowed inference. We compare different SSGD algorithms on two widely used CTS datasets: CALLHOME and Fisher Corpus (Part 1 and 2) and evaluate both separation and diarization performance. To improve performance, a novel, causal and computationally efficient leakage removal algorithm is proposed, which signifi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#29366;&#24577;&#31354;&#38388;&#21010;&#20998;&#23454;&#29616;&#30446;&#26631;&#23548;&#21521;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#37319;&#29992;&#34917;&#20805;&#30340;&#20248;&#21183;&#21152;&#26435;&#26041;&#26696;&#65292;&#20197;&#35299;&#20915;&#20998;&#24067;&#36716;&#31227;&#21644;&#22810;&#27169;&#24335;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.09367</link><description>&lt;p&gt;
&#36890;&#36807;&#29366;&#24577;&#31354;&#38388;&#21010;&#20998;&#23454;&#29616;&#30446;&#26631;&#23548;&#21521;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Goal-conditioned Offline Reinforcement Learning through State Space Partitioning. (arXiv:2303.09367v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09367
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#29366;&#24577;&#31354;&#38388;&#21010;&#20998;&#23454;&#29616;&#30446;&#26631;&#23548;&#21521;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#37319;&#29992;&#34917;&#20805;&#30340;&#20248;&#21183;&#21152;&#26435;&#26041;&#26696;&#65292;&#20197;&#35299;&#20915;&#20998;&#24067;&#36716;&#31227;&#21644;&#22810;&#27169;&#24335;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26088;&#22312;&#20165;&#20351;&#29992;&#31163;&#32447;&#25968;&#25454;&#38598;&#26469;&#25512;&#26029;&#20986;&#39034;&#24207;&#20915;&#31574;&#31574;&#30053;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24819;&#35201;&#23398;&#20064;&#23454;&#29616;&#22810;&#20010;&#19981;&#21516;&#30446;&#26631;&#25110;&#32467;&#26524;&#30340;&#30446;&#26631;&#23548;&#21521;&#20915;&#31574;&#31574;&#30053;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#35299;&#20915;&#20998;&#24067;&#36716;&#31227;&#21644;&#22810;&#27169;&#24335;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#34917;&#20805;&#30340;&#20248;&#21183;&#21152;&#26435;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) aims to infer sequential decision policies using only offline datasets. This is a particularly difficult setup, especially when learning to achieve multiple different goals or outcomes under a given scenario with only sparse rewards. For offline learning of goal-conditioned policies via supervised learning, previous work has shown that an advantage weighted log-likelihood loss guarantees monotonic policy improvement. In this work we argue that, despite its benefits, this approach is still insufficient to fully address the distribution shift and multi-modality problems. The latter is particularly severe in long-horizon tasks where finding a unique and optimal policy that goes from a state to the desired goal is challenging as there may be multiple and potentially conflicting solutions. To tackle these challenges, we propose a complementary advantage-based weighting scheme that introduces an additional source of inductive bias: given a value-based part
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#32479;&#19968;&#24754;&#35266;&#39118;&#38505;&#30340;&#32508;&#21512;$\alpha$-&#39118;&#38505;&#29256;&#26412;&#21644;&#22522;&#20110;&#39118;&#38505;&#33719;&#24471;&#26368;&#20248;&#32452;&#21512;&#30340;&#35745;&#31639;&#31639;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#20272;&#35745;&#38889;&#22269;&#32929;&#31080;&#30340;&#24754;&#35266;&#26368;&#20248;&#32452;&#21512;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.07158</link><description>&lt;p&gt;
&#32479;&#19968;&#24754;&#35266;&#39118;&#38505;&#21644;&#26368;&#20248;&#32452;&#21512;
&lt;/p&gt;
&lt;p&gt;
Uniform Pessimistic Risk and Optimal Portfolio. (arXiv:2303.07158v1 [q-fin.PM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#32479;&#19968;&#24754;&#35266;&#39118;&#38505;&#30340;&#32508;&#21512;$\alpha$-&#39118;&#38505;&#29256;&#26412;&#21644;&#22522;&#20110;&#39118;&#38505;&#33719;&#24471;&#26368;&#20248;&#32452;&#21512;&#30340;&#35745;&#31639;&#31639;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#20272;&#35745;&#38889;&#22269;&#32929;&#31080;&#30340;&#24754;&#35266;&#26368;&#20248;&#32452;&#21512;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a version of integrated $\alpha$-risk called the uniform pessimistic risk and a computational algorithm to obtain an optimal portfolio based on the risk. The proposed method can be used to estimate the pessimistic optimal portfolio models for Korean stocks.
&lt;/p&gt;
&lt;p&gt;
&#36164;&#20135;&#37197;&#32622;&#30340;&#26368;&#20248;&#24615;&#24050;&#32463;&#22312;&#39118;&#38505;&#24230;&#37327;&#30340;&#29702;&#35770;&#20998;&#26512;&#20013;&#24471;&#21040;&#24191;&#27867;&#35752;&#35770;&#12290;&#24754;&#35266;&#20027;&#20041;&#26159;&#19968;&#31181;&#36229;&#36234;&#20256;&#32479;&#26368;&#20248;&#32452;&#21512;&#27169;&#22411;&#30340;&#26368;&#26377;&#21560;&#24341;&#21147;&#30340;&#26041;&#27861;&#20043;&#19968;&#65292;$\alpha$-&#39118;&#38505;&#22312;&#25512;&#23548;&#20986;&#24191;&#27867;&#30340;&#24754;&#35266;&#26368;&#20248;&#32452;&#21512;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#30001;&#24754;&#35266;&#39118;&#38505;&#35780;&#20272;&#30340;&#26368;&#20248;&#32452;&#21512;&#30340;&#20272;&#35745;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#32570;&#20047;&#21487;&#29992;&#30340;&#20272;&#35745;&#27169;&#22411;&#21644;&#35745;&#31639;&#31639;&#27861;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#32479;&#19968;&#24754;&#35266;&#39118;&#38505;&#30340;&#32508;&#21512;$\alpha$-&#39118;&#38505;&#29256;&#26412;&#21644;&#22522;&#20110;&#39118;&#38505;&#33719;&#24471;&#26368;&#20248;&#32452;&#21512;&#30340;&#35745;&#31639;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20174;&#22810;&#20010;&#20998;&#20301;&#25968;&#22238;&#24402;&#12289;&#36866;&#24403;&#30340;&#35780;&#20998;&#35268;&#21017;&#21644;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#19977;&#20010;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#30740;&#31350;&#25152;&#25552;&#20986;&#30340;&#39118;&#38505;&#30340;&#29702;&#35770;&#24615;&#36136;&#12290;&#21516;&#26102;&#65292;&#32479;&#19968;&#24754;&#35266;&#39118;&#38505;&#34987;&#24212;&#29992;&#20110;&#20272;&#35745;&#38889;&#22269;&#32929;&#31080;&#30340;&#24754;&#35266;&#26368;&#20248;&#32452;&#21512;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The optimality of allocating assets has been widely discussed with the theoretical analysis of risk measures. Pessimism is one of the most attractive approaches beyond the conventional optimal portfolio model, and the $\alpha$-risk plays a crucial role in deriving a broad class of pessimistic optimal portfolios. However, estimating an optimal portfolio assessed by a pessimistic risk is still challenging due to the absence of an available estimation model and a computational algorithm. In this study, we propose a version of integrated $\alpha$-risk called the uniform pessimistic risk and the computational algorithm to obtain an optimal portfolio based on the risk. Further, we investigate the theoretical properties of the proposed risk in view of three different approaches: multiple quantile regression, the proper scoring rule, and distributionally robust optimization. Also, the uniform pessimistic risk is applied to estimate the pessimistic optimal portfolio models for the Korean stock 
&lt;/p&gt;</description></item><item><title>CaloFlow&#26159;&#19968;&#31181;&#26032;&#30340;&#12289;&#26377;&#21069;&#26223;&#30340;&#24555;&#36895;&#37327;&#33021;&#22120;&#27169;&#25311;&#26041;&#27861;&#65292;&#21487;&#20197;&#27604;Geant4&#24555;&#20960;&#20010;&#25968;&#37327;&#32423;&#29983;&#25104;&#39640;&#20445;&#30495;&#24230;&#30340;&#26679;&#26412;&#12290;&#20351;&#29992;&#37327;&#33021;&#22120;&#22270;&#29255;&#12289;&#30452;&#26041;&#22270;&#21644;&#20998;&#31867;&#22120;&#35777;&#26126;&#20102;&#20854;&#26679;&#26412;&#30340;&#20445;&#30495;&#24230;&#12290;</title><link>http://arxiv.org/abs/2210.14245</link><description>&lt;p&gt;
CaloFlow&#29992;&#20110;CaloChallenge&#25968;&#25454;&#38598;1&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
CaloFlow for CaloChallenge Dataset 1. (arXiv:2210.14245v2 [physics.ins-det] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14245
&lt;/p&gt;
&lt;p&gt;
CaloFlow&#26159;&#19968;&#31181;&#26032;&#30340;&#12289;&#26377;&#21069;&#26223;&#30340;&#24555;&#36895;&#37327;&#33021;&#22120;&#27169;&#25311;&#26041;&#27861;&#65292;&#21487;&#20197;&#27604;Geant4&#24555;&#20960;&#20010;&#25968;&#37327;&#32423;&#29983;&#25104;&#39640;&#20445;&#30495;&#24230;&#30340;&#26679;&#26412;&#12290;&#20351;&#29992;&#37327;&#33021;&#22120;&#22270;&#29255;&#12289;&#30452;&#26041;&#22270;&#21644;&#20998;&#31867;&#22120;&#35777;&#26126;&#20102;&#20854;&#26679;&#26412;&#30340;&#20445;&#30495;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
CaloFlow&#26159;&#19968;&#31181;&#22522;&#20110;&#24402;&#19968;&#21270;&#27969;&#30340;&#24555;&#36895;&#37327;&#33021;&#22120;&#27169;&#25311;&#30340;&#26032;&#26041;&#27861;&#12290;&#23558;CaloFlow&#24212;&#29992;&#20110;Fast Calorimeter Simulation Challenge 2022&#30340;&#20809;&#23376;&#21644;&#24102;&#30005;&#960;&#20171;&#23376;Geant4 shower&#30340;Dataset 1&#65292;&#25105;&#20204;&#23637;&#31034;&#23427;&#21487;&#20197;&#27604;Geant4&#24555;&#20960;&#20010;&#25968;&#37327;&#32423;&#20135;&#29983;&#39640;&#20445;&#30495;&#24230;&#30340;&#26679;&#26412;&#12290;&#25105;&#20204;&#20351;&#29992;&#37327;&#33021;&#22120;shower&#22270;&#20687;&#12289;&#39640;&#32423;&#29305;&#24449;&#30340;&#30452;&#26041;&#22270;&#20197;&#21450;&#20998;&#31867;&#22120;&#23558;CaloFlow&#19982;Geant4&#26679;&#26412;&#21306;&#20998;&#24320;&#26469;&#26469;&#35777;&#26126;&#26679;&#26412;&#30340;&#20445;&#30495;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
CaloFlow is a new and promising approach to fast calorimeter simulation based on normalizing flows. Applying CaloFlow to the photon and charged pion Geant4 showers of Dataset 1 of the Fast Calorimeter Simulation Challenge 2022, we show how it can produce high-fidelity samples with a sampling time that is several orders of magnitude faster than Geant4. We demonstrate the fidelity of the samples using calorimeter shower images, histograms of high-level features, and aggregate metrics such as a classifier trained to distinguish CaloFlow from Geant4 samples.
&lt;/p&gt;</description></item></channel></rss>