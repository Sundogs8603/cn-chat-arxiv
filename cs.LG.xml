<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#36890;&#36807;&#21487;&#23398;&#20064;&#24310;&#36831;&#32447;&#23454;&#29616;&#21487;&#24494;&#20998;&#21453;&#39304;&#24310;&#36831;&#32593;&#32476;&#30340;&#21442;&#25968;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#23545;&#23460;&#20869;&#22768;&#23398;&#29305;&#24615;&#30340;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#12290;</title><link>https://arxiv.org/abs/2404.00082</link><description>&lt;p&gt;
&#22522;&#20110;&#21487;&#24494;&#20998;&#21453;&#39304;&#24310;&#36831;&#32593;&#32476;&#21644;&#21487;&#23398;&#20064;&#24310;&#36831;&#32447;&#30340;&#25968;&#25454;&#39537;&#21160;&#23460;&#20869;&#22768;&#23398;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Data-Driven Room Acoustic Modeling Via Differentiable Feedback Delay Networks With Learnable Delay Lines
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00082
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21487;&#23398;&#20064;&#24310;&#36831;&#32447;&#23454;&#29616;&#21487;&#24494;&#20998;&#21453;&#39304;&#24310;&#36831;&#32593;&#32476;&#30340;&#21442;&#25968;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#23545;&#23460;&#20869;&#22768;&#23398;&#29305;&#24615;&#30340;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20960;&#21313;&#24180;&#20013;&#65292;&#20154;&#20204;&#33268;&#21147;&#20110;&#35774;&#35745;&#20154;&#24037;&#28151;&#21709;&#31639;&#27861;&#65292;&#26088;&#22312;&#27169;&#25311;&#29289;&#29702;&#29615;&#22659;&#30340;&#23460;&#20869;&#22768;&#23398;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#24310;&#36831;&#32593;&#32476;&#27169;&#22411;&#30340;&#33258;&#21160;&#21442;&#25968;&#35843;&#25972;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#24615;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#21487;&#24494;&#20998;&#21453;&#39304;&#24310;&#36831;&#32593;&#32476;&#65288;FDN&#65289;&#30340;&#21442;&#25968;&#65292;&#20351;&#20854;&#36755;&#20986;&#21576;&#29616;&#20986;&#25152;&#27979;&#24471;&#30340;&#23460;&#20869;&#33033;&#20914;&#21709;&#24212;&#30340;&#24863;&#30693;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00082v1 Announce Type: cross  Abstract: Over the past few decades, extensive research has been devoted to the design of artificial reverberation algorithms aimed at emulating the room acoustics of physical environments. Despite significant advancements, automatic parameter tuning of delay-network models remains an open challenge. We introduce a novel method for finding the parameters of a Feedback Delay Network (FDN) such that its output renders the perceptual qualities of a measured room impulse response. The proposed approach involves the implementation of a differentiable FDN with trainable delay lines, which, for the first time, allows us to simultaneously learn each and every delay-network parameter via backpropagation. The iterative optimization process seeks to minimize a time-domain loss function incorporating differentiable terms accounting for energy decay and echo density. Through experimental validation, we show that the proposed method yields time-invariant freq
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35797;&#39564;&#24615;&#30740;&#31350;&#39318;&#27425;&#23581;&#35797;&#26397;&#21521;&#26080;&#38656;&#20957;&#35270;&#30340;&#33041;&#26426;&#25509;&#21475;&#25340;&#20889;&#22120;&#65292;&#36890;&#36807;&#20351;&#29992;&#31354;&#38388;&#27880;&#24847;&#21147;&#32780;&#38750;&#30524;&#29699;&#31227;&#21160;&#26469;&#35299;&#30721;&#35270;&#35273;&#21050;&#28608;&#65292;&#21462;&#24471;&#20102;&#24456;&#39640;&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2404.00031</link><description>&lt;p&gt;
&#26397;&#21521;&#26080;&#38656;&#20957;&#35270;&#30340;c-VEP&#33041;&#26426;&#25509;&#21475;&#65306;&#19968;&#39033;&#35797;&#39564;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards gaze-independent c-VEP BCI: A pilot study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00031
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35797;&#39564;&#24615;&#30740;&#31350;&#39318;&#27425;&#23581;&#35797;&#26397;&#21521;&#26080;&#38656;&#20957;&#35270;&#30340;&#33041;&#26426;&#25509;&#21475;&#25340;&#20889;&#22120;&#65292;&#36890;&#36807;&#20351;&#29992;&#31354;&#38388;&#27880;&#24847;&#21147;&#32780;&#38750;&#30524;&#29699;&#31227;&#21160;&#26469;&#35299;&#30721;&#35270;&#35273;&#21050;&#28608;&#65292;&#21462;&#24471;&#20102;&#24456;&#39640;&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#26426;&#25509;&#21475;&#65288;BCI&#65289;&#25340;&#20889;&#22120;&#30340;&#19968;&#20010;&#23616;&#38480;&#24615;&#22312;&#20110;&#23427;&#20204;&#35201;&#27714;&#29992;&#25143;&#33021;&#22815;&#31227;&#21160;&#30524;&#30555;&#27880;&#35270;&#30446;&#26631;&#12290;&#23545;&#20110;&#26080;&#27861;&#33258;&#24895;&#25511;&#21046;&#30524;&#30555;&#31227;&#21160;&#30340;&#29992;&#25143;&#65288;&#20363;&#22914;&#24739;&#26377;&#26202;&#26399;&#32908;&#33806;&#32553;&#20391;&#32034;&#30828;&#21270;&#30151;&#65288;ALS&#65289;&#30340;&#20154;&#32676;&#65289;&#65292;&#36825;&#20250;&#36896;&#25104;&#38382;&#39064;&#12290;&#36825;&#39033;&#35797;&#39564;&#24615;&#30740;&#31350;&#39318;&#27425;&#36808;&#20986;&#20102;&#26397;&#21521;&#22522;&#20110;&#20195;&#30721;&#35843;&#21046;&#35270;&#35273;&#35825;&#21457;&#30005;&#20301;&#65288;c-VEP&#65289;&#30340;&#26080;&#38656;&#20957;&#35270;&#30340;&#25340;&#20889;&#22120;&#30340;&#31532;&#19968;&#27493;&#12290;&#21442;&#19982;&#32773;&#34987;&#21576;&#29616;&#20004;&#20010;&#21452;&#20391;&#20301;&#32622;&#30340;&#21050;&#28608;&#65292;&#20854;&#20013;&#19968;&#20010;&#22312;&#38378;&#28865;&#65292;&#24182;&#34987;&#35201;&#27714;&#19987;&#27880;&#20110;&#36825;&#20123;&#21050;&#28608;&#20013;&#30340;&#19968;&#20010;&#65292;&#21487;&#20197;&#36890;&#36807;&#30452;&#25509;&#30475;&#30528;&#21050;&#28608;&#65288;&#26126;&#26174;&#26465;&#20214;&#65289;&#25110;&#20351;&#29992;&#31354;&#38388;&#27880;&#24847;&#21147;&#26469;&#23436;&#25104;&#65292;&#28040;&#38500;&#20102;&#30524;&#29699;&#31227;&#21160;&#30340;&#38656;&#35201;&#65288;&#38544;&#34109;&#26465;&#20214;&#65289;&#12290;&#34987;&#19987;&#27880;&#30340;&#21050;&#28608;&#20174;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#20013;&#35299;&#30721;&#65292;&#38544;&#34109;&#21644;&#26126;&#26174;&#26465;&#20214;&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#20998;&#21035;&#20026;88%&#21644;100%&#12290;&#36825;&#20123;&#22522;&#30784;&#24615;&#35265;&#35299;&#23637;&#31034;&#20102;&#36825;&#19968;&#25216;&#26415;&#30340;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00031v1 Announce Type: cross  Abstract: A limitation of brain-computer interface (BCI) spellers is that they require the user to be able to move the eyes to fixate on targets. This poses an issue for users who cannot voluntarily control their eye movements, for instance, people living with late-stage amyotrophic lateral sclerosis (ALS). This pilot study makes the first step towards a gaze-independent speller based on the code-modulated visual evoked potential (c-VEP). Participants were presented with two bi-laterally located stimuli, one of which was flashing, and were tasked to attend to one of these stimuli either by directly looking at the stimuli (overt condition) or by using spatial attention, eliminating the need for eye movement (covert condition). The attended stimuli were decoded from electroencephalography (EEG) and classification accuracies of 88% and 100% were obtained for the covert and overt conditions, respectively. These fundamental insights show the promisin
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;IDGen&#65292;&#23558;&#27599;&#20010;&#25512;&#33616;&#39033;&#30446;&#34920;&#31034;&#20026;&#29420;&#29305;&#12289;&#31616;&#27905;&#12289;&#35821;&#20041;&#20016;&#23500;&#30340;&#25991;&#26412;ID&#65292;&#20174;&#32780;&#20351;&#24471;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#33616;&#26356;&#22909;&#22320;&#19982;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#23545;&#40784;&#12290;</title><link>https://arxiv.org/abs/2403.19021</link><description>&lt;p&gt;
&#26397;&#21521;LLM-RecSys&#23545;&#40784;&#19982;&#25991;&#26412;ID&#23398;&#20064;&#30340;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Towards LLM-RecSys Alignment with Textual ID Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19021
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;IDGen&#65292;&#23558;&#27599;&#20010;&#25512;&#33616;&#39033;&#30446;&#34920;&#31034;&#20026;&#29420;&#29305;&#12289;&#31616;&#27905;&#12289;&#35821;&#20041;&#20016;&#23500;&#30340;&#25991;&#26412;ID&#65292;&#20174;&#32780;&#20351;&#24471;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#33616;&#26356;&#22909;&#22320;&#19982;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#29983;&#25104;&#24335;&#25512;&#33616;&#24050;&#32463;&#23558;&#20256;&#32479;&#30340;&#22522;&#20110;&#25490;&#21517;&#30340;&#25512;&#33616;&#26041;&#24335;&#36716;&#21464;&#20026;&#25991;&#26412;&#29983;&#25104;&#33539;&#20363;&#12290;&#28982;&#32780;&#65292;&#19982;&#22266;&#26377;&#25805;&#20316;&#20154;&#31867;&#35789;&#27719;&#30340;&#26631;&#20934;NLP&#20219;&#21153;&#30456;&#21453;&#65292;&#30446;&#21069;&#29983;&#25104;&#24335;&#25512;&#33616;&#39046;&#22495;&#30340;&#30740;&#31350;&#22312;&#22914;&#20309;&#22312;&#25991;&#26412;&#29983;&#25104;&#33539;&#24335;&#20013;&#20197;&#31616;&#27905;&#32780;&#26377;&#24847;&#20041;&#30340;ID&#34920;&#31034;&#26377;&#25928;&#32534;&#30721;&#25512;&#33616;&#39033;&#30446;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#23545;&#40784;LLMs&#19982;&#25512;&#33616;&#38656;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;IDGen&#65292;&#20351;&#29992;&#20154;&#31867;&#35821;&#35328;&#26631;&#35760;&#23558;&#27599;&#20010;&#39033;&#30446;&#34920;&#31034;&#20026;&#29420;&#29305;&#12289;&#31616;&#27905;&#12289;&#35821;&#20041;&#20016;&#23500;&#12289;&#19982;&#24179;&#21488;&#26080;&#20851;&#30340;&#25991;&#26412;ID&#12290;&#36825;&#36890;&#36807;&#22312;&#22522;&#20110;LLM&#30340;&#25512;&#33616;&#31995;&#32479;&#26049;&#35757;&#32451;&#25991;&#26412;ID&#29983;&#25104;&#22120;&#26469;&#23454;&#29616;&#65292;&#20351;&#20010;&#24615;&#21270;&#25512;&#33616;&#33021;&#22815;&#26080;&#32541;&#38598;&#25104;&#21040;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20013;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#30001;&#20110;&#29992;&#25143;&#21382;&#21490;&#35760;&#24405;&#20197;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#24182;&#19982;&#21407;&#22987;&#25968;&#25454;&#38598;&#35299;&#32806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20986;&#20102;&#28508;&#22312;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19021v1 Announce Type: cross  Abstract: Generative recommendation based on Large Language Models (LLMs) have transformed the traditional ranking-based recommendation style into a text-to-text generation paradigm. However, in contrast to standard NLP tasks that inherently operate on human vocabulary, current research in generative recommendations struggles to effectively encode recommendation items within the text-to-text framework using concise yet meaningful ID representations. To better align LLMs with recommendation needs, we propose IDGen, representing each item as a unique, concise, semantically rich, platform-agnostic textual ID using human language tokens. This is achieved by training a textual ID generator alongside the LLM-based recommender, enabling seamless integration of personalized recommendations into natural language generation. Notably, as user history is expressed in natural language and decoupled from the original dataset, our approach suggests the potenti
&lt;/p&gt;</description></item><item><title>FairerCLIP&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;RKHSs&#20013;&#20351;&#29992;&#20989;&#25968;&#21435;&#38500;CLIP&#30340;&#38646;&#26679;&#26412;&#39044;&#27979;&#20559;&#35265;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#20351;&#24471;&#39044;&#27979;&#26356;&#20844;&#24179;&#19988;&#26356;&#33021;&#25269;&#25239;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.15593</link><description>&lt;p&gt;
FairerCLIP: &#22312;RKHSs&#20013;&#20351;&#29992;&#20989;&#25968;&#21435;&#38500;CLIP&#30340;&#38646;&#26679;&#26412;&#39044;&#27979;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
FairerCLIP: Debiasing CLIP's Zero-Shot Predictions using Functions in RKHSs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15593
&lt;/p&gt;
&lt;p&gt;
FairerCLIP&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;RKHSs&#20013;&#20351;&#29992;&#20989;&#25968;&#21435;&#38500;CLIP&#30340;&#38646;&#26679;&#26412;&#39044;&#27979;&#20559;&#35265;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#20351;&#24471;&#39044;&#27979;&#26356;&#20844;&#24179;&#19988;&#26356;&#33021;&#25269;&#25239;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;CLIP&#65289;&#25552;&#20379;&#20102;&#25991;&#26412;&#21644;&#22270;&#20687;&#30340;&#32039;&#20945;&#36890;&#29992;&#34920;&#31034;&#65292;&#24050;&#34987;&#35777;&#26126;&#22312;&#22810;&#20010;&#19979;&#28216;&#38646;&#26679;&#26412;&#39044;&#27979;&#20219;&#21153;&#20013;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23427;&#20204;&#35757;&#32451;&#36807;&#31243;&#30340;&#24615;&#36136;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#23384;&#22312;&#20197;&#19979;&#38382;&#39064;&#65306;1&#65289;&#20256;&#25773;&#25110;&#25918;&#22823;&#31038;&#20250;&#20559;&#35265;&#21644;2&#65289;&#23398;&#20064;&#20381;&#36182;&#34394;&#20551;&#29305;&#24449;&#12290;&#26412;&#25991;&#25552;&#20986;FairerCLIP&#65292;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#20351;CLIP&#30340;&#38646;&#26679;&#26412;&#39044;&#27979;&#26356;&#21152;&#20844;&#24179;&#19988;&#26356;&#33021;&#25269;&#25239;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#22312;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHSs&#65289;&#20013;&#32852;&#21512;&#21435;&#20559;CLIP&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#34920;&#31034;&#38382;&#39064;&#65292;&#36825;&#24102;&#26469;&#20102;&#22810;&#20010;&#22909;&#22788;&#65306;1&#65289;&#28789;&#27963;&#24615;&#65306;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#29616;&#26377;&#26041;&#27861;&#35201;&#20040;&#19987;&#38376;&#29992;&#20110;&#23398;&#20064;&#26377;&#22320;&#38754;&#30495;&#30456;&#26631;&#31614;&#30340;&#24773;&#20917;&#65292;&#35201;&#20040;&#19987;&#38376;&#29992;&#20110;&#23398;&#20064;&#27809;&#26377;&#22320;&#38754;&#30495;&#30456;&#26631;&#31614;&#30340;&#24773;&#20917;&#65292;FairerCLIP&#33021;&#22815;&#36866;&#24212;&#20004;&#31181;&#23398;&#20064;&#24773;&#20917;&#12290;2&#65289;&#20248;&#21270;&#20415;&#21033;&#24615;&#65306;FairerCLIP&#23545;&#20110;&#36845;&#20195;&#20248;&#21270;&#38750;&#24120;&#21512;&#36866;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15593v1 Announce Type: cross  Abstract: Large pre-trained vision-language models such as CLIP provide compact and general-purpose representations of text and images that are demonstrably effective across multiple downstream zero-shot prediction tasks. However, owing to the nature of their training process, these models have the potential to 1) propagate or amplify societal biases in the training data and 2) learn to rely on spurious features. This paper proposes FairerCLIP, a general approach for making zero-shot predictions of CLIP more fair and robust to spurious correlations. We formulate the problem of jointly debiasing CLIP's image and text representations in reproducing kernel Hilbert spaces (RKHSs), which affords multiple benefits: 1) Flexibility: Unlike existing approaches, which are specialized to either learn with or without ground-truth labels, FairerCLIP is adaptable to learning in both scenarios. 2) Ease of Optimization: FairerCLIP lends itself to an iterative o
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35797;&#28857;&#30740;&#31350;&#39318;&#27425;&#23581;&#35797;&#20351;&#29992;&#22122;&#22768;&#26631;&#35760;&#21050;&#28608;&#21327;&#35758;&#36827;&#34892;&#21548;&#35273;&#27880;&#24847;&#21147;&#35299;&#30721;&#65292;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.15523</link><description>&lt;p&gt;
&#37319;&#29992;&#22122;&#22768;&#26631;&#35760;&#30340;&#21548;&#35273;&#27880;&#24847;&#21147;&#35299;&#30721;&#30740;&#31350;&#65306;&#19968;&#39033;&#35797;&#28857;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards auditory attention decoding with noise-tagging: A pilot study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15523
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35797;&#28857;&#30740;&#31350;&#39318;&#27425;&#23581;&#35797;&#20351;&#29992;&#22122;&#22768;&#26631;&#35760;&#21050;&#28608;&#21327;&#35758;&#36827;&#34892;&#21548;&#35273;&#27880;&#24847;&#21147;&#35299;&#30721;&#65292;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21548;&#35273;&#27880;&#24847;&#21147;&#35299;&#30721;(AAD)&#26088;&#22312;&#20174;&#22823;&#33041;&#27963;&#21160;&#20013;&#25552;&#21462;&#34987;&#20851;&#27880;&#30340;&#35828;&#35805;&#32773;&#65292;&#25552;&#20379;&#20102;&#31070;&#32463;&#23548;&#21521;&#21548;&#35273;&#35774;&#22791;&#21644;&#33041;&#26426;&#25509;&#21475;&#31561;&#39046;&#22495;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;&#26412;&#35797;&#28857;&#30740;&#31350;&#39318;&#27425;&#23581;&#35797;&#20351;&#29992;&#22122;&#22768;&#26631;&#35760;&#21050;&#28608;&#21327;&#35758;&#36827;&#34892;AAD&#65292;&#35813;&#21327;&#35758;&#24341;&#21457;&#20102;&#21487;&#38752;&#30340;&#32534;&#30721;&#35843;&#21046;&#35825;&#21457;&#30005;&#20301;&#65292;&#20294;&#22312;&#21548;&#35273;&#27169;&#24335;&#19979;&#30340;&#25506;&#32034;&#36824;&#24456;&#26377;&#38480;&#12290;&#30740;&#31350;&#21442;&#19982;&#32773;&#20381;&#27425;&#21576;&#29616;&#20004;&#20010;&#33655;&#20848;&#35821;&#35328;&#35821;&#38899;&#21050;&#28608;&#65292;&#36825;&#20123;&#21050;&#28608;&#34987;&#24133;&#24230;&#35843;&#21046;&#20026;&#20855;&#26377;&#21807;&#19968;&#20108;&#36827;&#21046;&#20266;&#38543;&#26426;&#22122;&#22768;&#30721;&#65292;&#26377;&#25928;&#22320;&#20026;&#20854;&#26631;&#35760;&#20102;&#38468;&#21152;&#21487;&#35299;&#30721;&#20449;&#24687;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#26410;&#35843;&#21046;&#38899;&#39057;&#19982;&#20351;&#29992;&#19981;&#21516;&#35843;&#21046;&#28145;&#24230;&#35843;&#21046;&#30340;&#38899;&#39057;&#30340;&#35299;&#30721;&#65292;&#20197;&#21450;&#20256;&#32479;AAD&#26041;&#27861;&#19982;&#26631;&#20934;&#35299;&#30721;&#22122;&#22768;&#30721;&#26041;&#27861;&#30340;&#23545;&#27604;&#12290;&#25105;&#20204;&#30340;&#35797;&#28857;&#30740;&#31350;&#21457;&#29616;&#65292;&#19982;&#26410;&#35843;&#21046;&#38899;&#39057;&#30456;&#27604;&#65292;70&#33267;100%&#30340;&#35843;&#21046;&#28145;&#24230;&#30340;&#20256;&#32479;&#26041;&#27861;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15523v1 Announce Type: cross  Abstract: Auditory attention decoding (AAD) aims to extract from brain activity the attended speaker amidst candidate speakers, offering promising applications for neuro-steered hearing devices and brain-computer interfacing. This pilot study makes a first step towards AAD using the noise-tagging stimulus protocol, which evokes reliable code-modulated evoked potentials, but is minimally explored in the auditory modality. Participants were sequentially presented with two Dutch speech stimuli that were amplitude modulated with a unique binary pseudo-random noise-code, effectively tagging these with additional decodable information. We compared the decoding of unmodulated audio against audio modulated with various modulation depths, and a conventional AAD method against a standard method to decode noise-codes. Our pilot study revealed higher performances for the conventional method with 70 to 100 percent modulation depths compared to unmodulated au
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20107;&#20214;&#30456;&#20851;&#30005;&#20301;&#30340;&#26032;&#22411;&#26041;&#27861;&#65288;UMM&#65289;&#65292;&#19982;&#30446;&#21069;&#20808;&#36827;&#30340;c-VEP&#38646;&#35757;&#32451;&#26041;&#27861;&#65288;CCA&#65289;&#36827;&#34892;&#27604;&#36739;&#65292;&#35777;&#26126;&#20102;&#23427;&#20204;&#22312;&#26080;&#38656;&#26657;&#20934;&#30340;BCI&#31995;&#32479;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.15521</link><description>&lt;p&gt;
&#23398;&#20064;&#22312;&#26032;&#39046;&#22495;&#34892;&#36208;&#65306;&#38754;&#21521;c-VEP BCI&#30340;&#26080;&#26657;&#20934;&#35299;&#30721;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning to walk on new ground: Calibration-free decoding for c-VEP BCI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15521
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20107;&#20214;&#30456;&#20851;&#30005;&#20301;&#30340;&#26032;&#22411;&#26041;&#27861;&#65288;UMM&#65289;&#65292;&#19982;&#30446;&#21069;&#20808;&#36827;&#30340;c-VEP&#38646;&#35757;&#32451;&#26041;&#27861;&#65288;CCA&#65289;&#36827;&#34892;&#27604;&#36739;&#65292;&#35777;&#26126;&#20102;&#23427;&#20204;&#22312;&#26080;&#38656;&#26657;&#20934;&#30340;BCI&#31995;&#32479;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20004;&#31181;&#38646;&#35757;&#32451;&#26041;&#27861;&#65292;&#26088;&#22312;&#22686;&#24378;&#33041;&#26426;&#25509;&#21475;&#65288;BCI&#65289;&#30340;&#21487;&#29992;&#24615;&#65292;&#28040;&#38500;&#20102;&#26657;&#20934;&#20250;&#35805;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26681;&#26893;&#20110;&#20107;&#20214;&#30456;&#20851;&#30005;&#20301;&#65288;ERP&#65289;&#39046;&#22495;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#21363;&#26080;&#30417;&#30563;&#22343;&#20540;&#26368;&#22823;&#21270;&#65288;UMM&#65289;&#65292;&#29992;&#20110;&#24555;&#36895;&#32534;&#30721;&#35843;&#21046;&#35270;&#35273;&#35825;&#21457;&#30005;&#20301;&#65288;c-VEP&#65289;&#21050;&#28608;&#21327;&#35758;&#12290;&#25105;&#20204;&#23558;UMM&#19982;&#20351;&#29992;&#20856;&#22411;&#30456;&#20851;&#20998;&#26512;&#65288;CCA&#65289;&#30340;c-VEP&#38646;&#35757;&#32451;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;&#27604;&#36739;&#21253;&#25324;&#23545;CCA&#21644;UMM&#30340;&#21363;&#26102;&#20998;&#31867;&#20197;&#21450;&#23545;&#20808;&#21069;&#20998;&#31867;&#35797;&#39564;&#36827;&#34892;&#32047;&#31215;&#23398;&#20064;&#30340;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26174;&#31034;&#20102;&#20004;&#31181;&#26041;&#27861;&#22312;&#22788;&#29702;c-VEP&#25968;&#25454;&#38598;&#30340;&#22797;&#26434;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#31361;&#20986;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#24046;&#24322;&#21644;&#29420;&#29305;&#20248;&#21183;&#12290;&#36825;&#39033;&#30740;&#31350;&#19981;&#20165;&#20026;&#26080;&#26657;&#20934;BCI&#26041;&#27861;&#30340;&#23454;&#38469;&#23454;&#26045;&#25552;&#20379;&#20102;&#35265;&#35299;&#65292;&#36824;&#20026;&#36827;&#19968;&#27493;&#30340;&#25506;&#32034;&#21644;&#25913;&#36827;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15521v1 Announce Type: cross  Abstract: This study explores two zero-training methods aimed at enhancing the usability of brain-computer interfaces (BCIs) by eliminating the need for a calibration session. We introduce a novel method rooted in the event-related potential (ERP) domain, unsupervised mean maximization (UMM), to the fast code-modulated visual evoked potential (c-VEP) stimulus protocol. We compare UMM to the state-of-the-art c-VEP zero-training method that uses canonical correlation analysis (CCA). The comparison includes instantaneous classification and classification with cumulative learning from previously classified trials for both CCA and UMM. Our study shows the effectiveness of both methods in navigating the complexities of a c-VEP dataset, highlighting their differences and distinct strengths. This research not only provides insights into the practical implementation of calibration-free BCI methods but also paves the way for further exploration and refine
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#19981;&#21516;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#22312;&#22826;&#31354;&#33337;&#25968;&#25454;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.12864</link><description>&lt;p&gt;
&#22826;&#31354;&#33337;&#24322;&#24120;&#26816;&#27979;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
A Comparison of Deep Learning Architectures for Spacecraft Anomaly Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12864
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#19981;&#21516;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#22312;&#22826;&#31354;&#33337;&#25968;&#25454;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22826;&#31354;&#33337;&#25805;&#20316;&#20855;&#26377;&#26497;&#39640;&#30340;&#20851;&#38190;&#24615;&#65292;&#35201;&#27714;&#20855;&#26377;&#26080;&#21487;&#25361;&#21076;&#30340;&#21487;&#38752;&#24615;&#21644;&#23433;&#20840;&#24615;&#12290;&#30830;&#20445;&#22826;&#31354;&#33337;&#30340;&#26368;&#20339;&#24615;&#33021;&#38656;&#35201;&#21450;&#26089;&#26816;&#27979;&#21644;&#20943;&#36731;&#24322;&#24120;&#24773;&#20917;&#65292;&#21542;&#21017;&#21487;&#33021;&#23548;&#33268;&#37096;&#20214;&#25110;&#20219;&#21153;&#22833;&#36133;&#12290;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#20986;&#29616;&#65292;&#20154;&#20204;&#23545;&#21033;&#29992;&#36825;&#20123;&#22797;&#26434;&#31639;&#27861;&#22312;&#31354;&#38388;&#25805;&#20316;&#20013;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#34920;&#29616;&#20986;&#20102;&#36739;&#22823;&#20852;&#36259;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#27604;&#36739;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#22312;&#22826;&#31354;&#33337;&#25968;&#25454;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#27491;&#22312;&#30740;&#31350;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21253;&#25324;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#12289;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;RNNs&#65289;&#12289;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#32593;&#32476;&#20197;&#21450;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#12290;&#36825;&#20123;&#27169;&#22411;&#20013;&#30340;&#27599;&#19968;&#20010;&#37117;&#26159;&#20351;&#29992;&#26469;&#33258;&#22810;&#20010;&#22826;&#31354;&#33337;&#20219;&#21153;&#30340;&#20840;&#38754;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#21644;&#39564;&#35777;&#30340;&#65292;&#21253;&#25324;&#21508;&#31181;&#36816;&#34892;&#22330;&#26223;&#21644;&#24322;&#24120;&#31867;&#22411;&#12290;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;&#8230;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12864v1 Announce Type: new  Abstract: Spacecraft operations are highly critical, demanding impeccable reliability and safety. Ensuring the optimal performance of a spacecraft requires the early detection and mitigation of anomalies, which could otherwise result in unit or mission failures. With the advent of deep learning, a surge of interest has been seen in leveraging these sophisticated algorithms for anomaly detection in space operations. This study aims to compare the efficacy of various deep learning architectures in detecting anomalies in spacecraft data. The deep learning models under investigation include Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), Long Short-Term Memory (LSTM) networks, and Transformer-based architectures. Each of these models was trained and validated using a comprehensive dataset sourced from multiple spacecraft missions, encompassing diverse operational scenarios and anomaly types. Initial results indicate that while 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;3D&#24863;&#30693;&#30340;&#28508;&#31354;&#38388;&#21644;&#36328;&#22330;&#26223;&#20849;&#20139;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;NeRFs&#26377;&#25928;&#23398;&#20064;&#22823;&#37327;&#35821;&#20041;&#30456;&#20284;&#22330;&#26223;&#65292;&#24182;&#26174;&#33879;&#38477;&#20302;&#20102;&#35757;&#32451;&#26102;&#38388;&#21644;&#20869;&#23384;&#28040;&#32791;</title><link>https://arxiv.org/abs/2403.11678</link><description>&lt;p&gt;
&#25506;&#32034;&#22522;&#20110;3D&#24863;&#30693;&#30340;&#28508;&#31354;&#38388;&#65292;&#20197;&#26377;&#25928;&#23398;&#20064;&#22810;&#20010;&#22330;&#26223;
&lt;/p&gt;
&lt;p&gt;
Exploring 3D-aware Latent Spaces for Efficiently Learning Numerous Scenes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11678
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;3D&#24863;&#30693;&#30340;&#28508;&#31354;&#38388;&#21644;&#36328;&#22330;&#26223;&#20849;&#20139;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;NeRFs&#26377;&#25928;&#23398;&#20064;&#22823;&#37327;&#35821;&#20041;&#30456;&#20284;&#22330;&#26223;&#65292;&#24182;&#26174;&#33879;&#38477;&#20302;&#20102;&#35757;&#32451;&#26102;&#38388;&#21644;&#20869;&#23384;&#28040;&#32791;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#20004;&#31181;&#25216;&#26415;&#26469;&#25913;&#21892;&#27599;&#20010;&#22330;&#26223;&#25152;&#38656;&#30340;&#35757;&#32451;&#26102;&#38388;&#21644;&#20869;&#23384;&#25104;&#26412;&#65292;&#20174;&#32780;&#23454;&#29616;&#23558;NeRFs&#26080;&#32541;&#25193;&#23637;&#21040;&#23398;&#20064;&#22823;&#37327;&#35821;&#20041;&#30456;&#20284;&#30340;&#22330;&#26223;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23398;&#20064;&#20102;&#19968;&#20010;3D&#24863;&#30693;&#30340;&#28508;&#31354;&#38388;&#65292;&#22312;&#20854;&#20013;&#35757;&#32451;&#19977;&#24179;&#38754;&#22330;&#26223;&#34920;&#31034;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#23398;&#20064;&#22330;&#26223;&#30340;&#20998;&#36776;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#22330;&#26223;&#20849;&#20139;&#36890;&#29992;&#20449;&#24687;&#30340;&#26041;&#24335;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#23398;&#20064;&#29305;&#23450;&#22330;&#26223;&#30340;&#27169;&#22411;&#22797;&#26434;&#24615;&#12290;&#24403;&#35757;&#32451;1000&#20010;&#22330;&#26223;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#27599;&#20010;&#22330;&#26223;&#30340;&#20869;&#23384;&#25104;&#26412;&#38477;&#20302;&#20102;44%&#65292;&#26102;&#38388;&#25104;&#26412;&#38477;&#20302;&#20102;86%&#12290;&#25105;&#20204;&#30340;&#39033;&#30446;&#39029;&#38754;&#20301;&#20110;https://3da-ae.github.io&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11678v1 Announce Type: cross  Abstract: We present a method enabling the scaling of NeRFs to learn a large number of semantically-similar scenes. We combine two techniques to improve the required training time and memory cost per scene. First, we learn a 3D-aware latent space in which we train Tri-Plane scene representations, hence reducing the resolution at which scenes are learned. Moreover, we present a way to share common information across scenes, hence allowing for a reduction of model complexity to learn a particular scene. Our method reduces effective per-scene memory costs by 44% and per-scene time costs by 86% when training 1000 scenes. Our project page can be found at https://3da-ae.github.io .
&lt;/p&gt;</description></item><item><title>PeerAiD&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#24615;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#35753;&#21516;&#34892;&#32593;&#32476;&#23398;&#20064;&#23398;&#29983;&#32593;&#32476;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#32780;&#19981;&#26159;&#33258;&#36523;&#30340;&#31034;&#20363;&#65292;&#26469;&#25552;&#21319;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.06668</link><description>&lt;p&gt;
PeerAiD&#65306;&#25913;&#21892;&#19987;&#19994;&#21516;&#34892;&#23548;&#24072;&#30340;&#23545;&#25239;&#24615;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
PeerAiD: Improving Adversarial Distillation from a Specialized Peer Tutor
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06668
&lt;/p&gt;
&lt;p&gt;
PeerAiD&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#24615;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#35753;&#21516;&#34892;&#32593;&#32476;&#23398;&#20064;&#23398;&#29983;&#32593;&#32476;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#32780;&#19981;&#26159;&#33258;&#36523;&#30340;&#31034;&#20363;&#65292;&#26469;&#25552;&#21319;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#22312;&#24212;&#29992;&#20110;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#26102;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#23545;&#25239;&#24615;&#33976;&#39311;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#36873;&#25321;&#65292;&#26088;&#22312;&#25552;&#28860;&#25945;&#24072;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#65292;&#20197;&#25913;&#36827;&#23567;&#22411;&#23398;&#29983;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;PeerAiD&#65292;&#36890;&#36807;&#35753;&#21516;&#34892;&#32593;&#32476;&#23398;&#20064;&#23398;&#29983;&#32593;&#32476;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#32780;&#19981;&#26159;&#38024;&#23545;&#33258;&#36523;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#26469;&#25913;&#36827;&#23545;&#25239;&#24615;&#33976;&#39311;&#12290;PeerAiD&#26159;&#19968;&#31181;&#23545;&#25239;&#24615;&#33976;&#39311;&#65292;&#21516;&#26102;&#35757;&#32451;&#21516;&#34892;&#32593;&#32476;&#21644;&#23398;&#29983;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06668v1 Announce Type: new  Abstract: Adversarial robustness of the neural network is a significant concern when it is applied to security-critical domains. In this situation, adversarial distillation is a promising option which aims to distill the robustness of the teacher network to improve the robustness of a small student network. Previous works pretrain the teacher network to make it robust to the adversarial examples aimed at itself. However, the adversarial examples are dependent on the parameters of the target network. The fixed teacher network inevitably degrades its robustness against the unseen transferred adversarial examples which targets the parameters of the student network in the adversarial distillation process. We propose PeerAiD to make a peer network learn the adversarial examples of the student network instead of adversarial examples aimed at itself. PeerAiD is an adversarial distillation that trains the peer network and the student network simultaneousl
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;BioELQA&#65292;&#23558;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#38142;&#25509;&#30475;&#20316;&#26159;&#22810;&#39033;&#36873;&#25321;&#38382;&#31572;&#65292;&#36890;&#36807;&#20351;&#29992;&#24555;&#36895;&#26816;&#32034;&#22120;&#33719;&#24471;&#20505;&#36873;&#23454;&#20307;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#23454;&#20307;&#38142;&#25509;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.15189</link><description>&lt;p&gt;
&#23558;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#38142;&#25509;&#35270;&#20026;&#22810;&#39033;&#36873;&#25321;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Biomedical Entity Linking as Multiple Choice Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15189
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;BioELQA&#65292;&#23558;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#38142;&#25509;&#30475;&#20316;&#26159;&#22810;&#39033;&#36873;&#25321;&#38382;&#31572;&#65292;&#36890;&#36807;&#20351;&#29992;&#24555;&#36895;&#26816;&#32034;&#22120;&#33719;&#24471;&#20505;&#36873;&#23454;&#20307;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#23454;&#20307;&#38142;&#25509;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#38142;&#25509;&#65288;BioEL&#65289;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#23545;&#20110;&#32454;&#31890;&#24230;&#21644;&#38271;&#23614;&#23454;&#20307;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BioELQA&#65292;&#36825;&#26159;&#19968;&#31181;&#23558;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#38142;&#25509;&#35270;&#20026;&#22810;&#39033;&#36873;&#25321;&#38382;&#31572;&#30340;&#26032;&#39062;&#27169;&#22411;&#12290;BioELQA&#39318;&#20808;&#21033;&#29992;&#24555;&#36895;&#26816;&#32034;&#22120;&#33719;&#24471;&#20505;&#36873;&#23454;&#20307;&#65292;&#23558;&#25552;&#21450;&#21644;&#20505;&#36873;&#23454;&#20307;&#20849;&#21516;&#21576;&#29616;&#32473;&#29983;&#25104;&#22120;&#65292;&#28982;&#21518;&#36755;&#20986;&#19982;&#20854;&#36873;&#23450;&#23454;&#20307;&#30456;&#20851;&#30340;&#39044;&#27979;&#31526;&#21495;&#12290;&#36825;&#31181;&#20844;&#24335;&#20351;&#24471;&#19981;&#21516;&#20505;&#36873;&#23454;&#20307;&#20043;&#38388;&#30340;&#26126;&#30830;&#27604;&#36739;&#25104;&#20026;&#21487;&#33021;&#65292;&#20174;&#32780;&#25429;&#25417;&#20102;&#25552;&#21450;&#21644;&#23454;&#20307;&#20043;&#38388;&#20197;&#21450;&#23454;&#20307;&#20043;&#38388;&#30340;&#31934;&#32454;&#20132;&#20114;&#12290;&#20026;&#20102;&#25913;&#21892;&#38271;&#23614;&#23454;&#20307;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#25105;&#20204;&#26816;&#32034;&#30456;&#20284;&#30340;&#24050;&#26631;&#35760;&#35757;&#32451;&#23454;&#20363;&#20316;&#20026;&#32447;&#32034;&#65292;&#24182;&#23558;&#36755;&#20837;&#19982;&#26816;&#32034;&#23454;&#20363;&#36830;&#25509;&#21040;&#29983;&#25104;&#22120;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;BioELQA&#30340;&#34920;&#29616;&#20248;&#20110;&#32479;&#35745;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15189v1 Announce Type: cross  Abstract: Although biomedical entity linking (BioEL) has made significant progress with pre-trained language models, challenges still exist for fine-grained and long-tailed entities. To address these challenges, we present BioELQA, a novel model that treats Biomedical Entity Linking as Multiple Choice Question Answering. BioELQA first obtains candidate entities with a fast retriever, jointly presents the mention and candidate entities to a generator, and then outputs the predicted symbol associated with its chosen entity. This formulation enables explicit comparison of different candidate entities, thus capturing fine-grained interactions between mentions and entities, as well as among entities themselves. To improve generalization for long-tailed entities, we retrieve similar labeled training instances as clues and concatenate the input with retrieved instances for the generator. Extensive experimental results show that BioELQA outperforms stat
&lt;/p&gt;</description></item><item><title>&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#23545;&#36755;&#20837;&#20013;&#20559;&#35265;&#29305;&#24449;&#19982;&#26631;&#31614;&#20043;&#38388;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#25935;&#24863;&#65292;&#26412;&#25991;&#22238;&#39038;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#26368;&#26032;&#26041;&#27861;&#65292;&#21516;&#26102;&#24635;&#32467;&#20102;&#25968;&#25454;&#38598;&#12289;&#22522;&#20934;&#21644;&#24230;&#37327;&#26631;&#20934;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.12715</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Spurious Correlations in Machine Learning: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12715
&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#23545;&#36755;&#20837;&#20013;&#20559;&#35265;&#29305;&#24449;&#19982;&#26631;&#31614;&#20043;&#38388;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#25935;&#24863;&#65292;&#26412;&#25991;&#22238;&#39038;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#26368;&#26032;&#26041;&#27861;&#65292;&#21516;&#26102;&#24635;&#32467;&#20102;&#25968;&#25454;&#38598;&#12289;&#22522;&#20934;&#21644;&#24230;&#37327;&#26631;&#20934;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#25152;&#21608;&#30693;&#65292;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#23545;&#36755;&#20837;&#20013;&#20559;&#35265;&#29305;&#24449;&#65288;&#20363;&#22914;&#32972;&#26223;&#12289;&#32441;&#29702;&#21644;&#27425;&#35201;&#23545;&#35937;&#65289;&#19982;&#30456;&#24212;&#26631;&#31614;&#20043;&#38388;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#25935;&#24863;&#12290;&#36825;&#20123;&#29305;&#24449;&#21450;&#20854;&#19982;&#26631;&#31614;&#30340;&#30456;&#20851;&#24615;&#34987;&#31216;&#20026;&#8220;&#34394;&#20551;&#8221;&#65292;&#22240;&#20026;&#23427;&#20204;&#24448;&#24448;&#38543;&#30528;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#20998;&#24067;&#30340;&#21464;&#21270;&#32780;&#25913;&#21464;&#65292;&#36825;&#21487;&#33021;&#23545;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#40065;&#26834;&#24615;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#22312;&#36825;&#39033;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#23457;&#26597;&#20102;&#36825;&#19968;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#20851;&#20110;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#24403;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#20998;&#31867;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#29616;&#26377;&#30340;&#25968;&#25454;&#38598;&#12289;&#22522;&#20934;&#21644;&#24230;&#37327;&#26631;&#20934;&#65292;&#20197;&#24110;&#21161;&#26410;&#26469;&#30340;&#30740;&#31350;&#12290;&#26412;&#25991;&#26368;&#21518;&#35752;&#35770;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#26410;&#26469;&#30740;&#31350;&#25361;&#25112;&#65292;&#26088;&#22312;&#20026;&#30456;&#20851;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12715v1 Announce Type: new  Abstract: Machine learning systems are known to be sensitive to spurious correlations between biased features of the inputs (e.g., background, texture, and secondary objects) and the corresponding labels. These features and their correlations with the labels are known as "spurious" because they tend to change with shifts in real-world data distributions, which can negatively impact the model's generalization and robustness. In this survey, we provide a comprehensive review of this issue, along with a taxonomy of current state-of-the-art methods for addressing spurious correlations in machine learning models. Additionally, we summarize existing datasets, benchmarks, and metrics to aid future research. The paper concludes with a discussion of the recent advancements and future research challenges in this field, aiming to provide valuable insights for researchers in the related domains.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35777;&#26126;&#23433;&#20840;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#24320;&#25918;&#28023;&#22495;&#19978;&#30340;&#33337;&#21482;&#20013;&#36981;&#23432;&#20132;&#36890;&#35268;&#21017;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#39564;&#35777;&#26041;&#27861;&#26469;&#30830;&#23450;&#34892;&#20026;&#26159;&#21542;&#31526;&#21512;COLREGS&#35268;&#21017;&#12290;</title><link>https://arxiv.org/abs/2402.08502</link><description>&lt;p&gt;
&#22312;&#24320;&#25918;&#28023;&#22495;&#19978;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#20013;&#21487;&#35777;&#26126;&#30340;&#20132;&#36890;&#35268;&#21017;&#36981;&#23432;
&lt;/p&gt;
&lt;p&gt;
Provable Traffic Rule Compliance in Safe Reinforcement Learning on the Open Sea
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08502
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35777;&#26126;&#23433;&#20840;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#24320;&#25918;&#28023;&#22495;&#19978;&#30340;&#33337;&#21482;&#20013;&#36981;&#23432;&#20132;&#36890;&#35268;&#21017;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#39564;&#35777;&#26041;&#27861;&#26469;&#30830;&#23450;&#34892;&#20026;&#26159;&#21542;&#31526;&#21512;COLREGS&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#36710;&#36742;&#24517;&#39035;&#36981;&#23432;&#20132;&#36890;&#35268;&#21017;&#12290;&#36825;&#20123;&#35268;&#21017;&#36890;&#24120;&#20351;&#29992;&#26102;&#24577;&#36923;&#36753;&#36827;&#34892;&#24418;&#24335;&#21270;&#65292;&#23548;&#33268;&#20351;&#29992;&#22522;&#20110;&#20248;&#21270;&#30340;&#36816;&#21160;&#35268;&#21010;&#22120;&#35299;&#20915;&#36825;&#20123;&#32422;&#26463;&#21464;&#24471;&#22256;&#38590;&#12290;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#25214;&#21040;&#31526;&#21512;&#26102;&#24577;&#36923;&#36753;&#35268;&#33539;&#30340;&#36816;&#21160;&#35268;&#21010;&#12290;&#28982;&#32780;&#65292;&#32431;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22522;&#20110;&#38543;&#26426;&#25506;&#32034;&#65292;&#36825;&#22312;&#26412;&#36136;&#19978;&#26159;&#19981;&#23433;&#20840;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35777;&#26126;&#23433;&#20840;&#30340;RL&#26041;&#27861;&#65292;&#22987;&#32456;&#36981;&#23432;&#20132;&#36890;&#35268;&#21017;&#12290;&#20316;&#20026;&#19968;&#20010;&#29305;&#23450;&#30340;&#24212;&#29992;&#39046;&#22495;&#65292;&#25105;&#20204;&#32771;&#34385;&#22312;&#24320;&#25918;&#28023;&#22495;&#19978;&#30340;&#33337;&#21482;&#65292;&#36825;&#20123;&#33337;&#21482;&#24517;&#39035;&#36981;&#23432;&#12298;&#28023;&#19978;&#36991;&#30896;&#35268;&#21017;&#20844;&#32422;&#12299;&#65288;COLREGS&#65289;&#30340;&#35268;&#23450;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#39564;&#35777;&#26041;&#27861;&#65292;&#29992;&#20110;&#30830;&#23450;&#34892;&#20026;&#19982;&#20351;&#29992;&#26102;&#24577;&#36923;&#36753;&#24418;&#24335;&#21270;&#30340;COLREGS&#30340;&#31526;&#21512;&#24615;&#12290;&#25105;&#20204;&#30340;&#34892;&#20026;&#39564;&#35777;&#34987;&#38598;&#25104;&#21040;RL&#36807;&#31243;&#20013;&#65292;&#20197;&#20415;&#26234;&#33021;&#20307;&#21482;&#36873;&#25321;&#32463;&#36807;&#39564;&#35777;&#30340;&#34892;&#20026;&#12290;&#19982;&#21482;&#38598;&#25104;&#20132;&#36890;&#35268;&#21017;&#30340;&#26234;&#33021;&#20307;&#30456;&#27604;&#65292;
&lt;/p&gt;
&lt;p&gt;
Autonomous vehicles have to obey traffic rules. These rules are often formalized using temporal logic, resulting in constraints that are hard to solve using optimization-based motion planners. Reinforcement Learning (RL) is a promising method to find motion plans adhering to temporal logic specifications. However, vanilla RL algorithms are based on random exploration, which is inherently unsafe. To address this issue, we propose a provably safe RL approach that always complies with traffic rules. As a specific application area, we consider vessels on the open sea, which must adhere to the Convention on the International Regulations for Preventing Collisions at Sea (COLREGS). We introduce an efficient verification approach that determines the compliance of actions with respect to the COLREGS formalized using temporal logic. Our action verification is integrated into the RL process so that the agent only selects verified actions. In contrast to agents that only integrate the traffic rule
&lt;/p&gt;</description></item><item><title>&#39640;&#25928;ViT-SAM&#26159;&#19968;&#31181;&#26032;&#30340;&#21152;&#36895;&#30340;&#27573;&#33853;&#20219;&#24847;&#27169;&#22411;&#65292;&#36890;&#36807;&#20445;&#30041;&#36731;&#37327;&#32423;&#25552;&#31034;&#32534;&#30721;&#22120;&#21644;&#25513;&#30721;&#35299;&#30721;&#22120;&#65292;&#24182;&#26367;&#25442;&#27785;&#37325;&#30340;&#22270;&#20687;&#32534;&#30721;&#22120;&#65292;&#23454;&#29616;&#20102;48.9&#20493;&#30340;&#21152;&#36895;&#32780;&#19981;&#29306;&#29298;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.05008</link><description>&lt;p&gt;
&#39640;&#25928;ViT-SAM: &#22312;&#19981;&#25439;&#22833;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#21152;&#36895;&#30340;&#27573;&#33853;&#20219;&#24847;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
EfficientViT-SAM: Accelerated Segment Anything Model Without Performance Loss
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05008
&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;ViT-SAM&#26159;&#19968;&#31181;&#26032;&#30340;&#21152;&#36895;&#30340;&#27573;&#33853;&#20219;&#24847;&#27169;&#22411;&#65292;&#36890;&#36807;&#20445;&#30041;&#36731;&#37327;&#32423;&#25552;&#31034;&#32534;&#30721;&#22120;&#21644;&#25513;&#30721;&#35299;&#30721;&#22120;&#65292;&#24182;&#26367;&#25442;&#27785;&#37325;&#30340;&#22270;&#20687;&#32534;&#30721;&#22120;&#65292;&#23454;&#29616;&#20102;48.9&#20493;&#30340;&#21152;&#36895;&#32780;&#19981;&#29306;&#29298;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21152;&#36895;&#27573;&#33853;&#20219;&#24847;&#27169;&#22411;&#8212;&#8212;&#39640;&#25928;ViT-SAM&#12290;&#25105;&#20204;&#20445;&#30041;&#20102;SAM&#30340;&#36731;&#37327;&#32423;&#25552;&#31034;&#32534;&#30721;&#22120;&#21644;&#25513;&#30721;&#35299;&#30721;&#22120;&#65292;&#21516;&#26102;&#29992;&#39640;&#25928;ViT&#26367;&#25442;&#20102;&#27785;&#37325;&#30340;&#22270;&#20687;&#32534;&#30721;&#22120;&#12290;&#22312;&#35757;&#32451;&#26041;&#38754;&#65292;&#25105;&#20204;&#39318;&#20808;&#20174;SAM-ViT-H&#22270;&#20687;&#32534;&#30721;&#22120;&#21040;&#39640;&#25928;ViT&#36827;&#34892;&#30693;&#35782;&#33976;&#39311;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23545;SA-1B&#25968;&#25454;&#38598;&#36827;&#34892;&#31471;&#21040;&#31471;&#35757;&#32451;&#12290;&#30001;&#20110;&#39640;&#25928;ViT&#30340;&#25928;&#29575;&#21644;&#23481;&#37327;&#65292;&#39640;&#25928;ViT-SAM&#22312;A100 GPU&#19978;&#30456;&#27604;SAM-ViT-H&#23454;&#29616;&#20102;48.9&#20493;&#30340;TensorRT&#21152;&#36895;&#65292;&#32780;&#26080;&#38656;&#29306;&#29298;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#24050;&#22312;https://github.com/mit-han-lab/efficientvit&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present EfficientViT-SAM, a new family of accelerated segment anything models. We retain SAM's lightweight prompt encoder and mask decoder while replacing the heavy image encoder with EfficientViT. For the training, we begin with the knowledge distillation from the SAM-ViT-H image encoder to EfficientViT. Subsequently, we conduct end-to-end training on the SA-1B dataset. Benefiting from EfficientViT's efficiency and capacity, EfficientViT-SAM delivers 48.9x measured TensorRT speedup on A100 GPU over SAM-ViT-H without sacrificing performance. Our code and pre-trained models are released at https://github.com/mit-han-lab/efficientvit.
&lt;/p&gt;</description></item><item><title>SafEDMD&#26159;&#19968;&#31181;&#22522;&#20110;EDMD&#30340;&#23398;&#20064;&#26550;&#26500;&#65292;&#36890;&#36807;&#31283;&#23450;&#24615;&#21644;&#35748;&#35777;&#23548;&#21521;&#65292;&#29983;&#25104;&#21487;&#38752;&#30340;&#25968;&#25454;&#39537;&#21160;&#26367;&#20195;&#27169;&#22411;&#65292;&#24182;&#22522;&#20110;&#21322;&#23450;&#35268;&#21010;&#36827;&#34892;&#35748;&#35777;&#25511;&#21046;&#22120;&#35774;&#35745;&#12290;&#23427;&#22312;&#22810;&#20010;&#22522;&#20934;&#31034;&#20363;&#19978;&#23637;&#31034;&#20102;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.03145</link><description>&lt;p&gt;
SafEDMD&#65306;&#19968;&#31181;&#19987;&#20026;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#25968;&#25454;&#39537;&#21160;&#25511;&#21046;&#32780;&#35774;&#35745;&#30340;&#35748;&#35777;&#23398;&#20064;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
SafEDMD: A certified learning architecture tailored to data-driven control of nonlinear dynamical systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03145
&lt;/p&gt;
&lt;p&gt;
SafEDMD&#26159;&#19968;&#31181;&#22522;&#20110;EDMD&#30340;&#23398;&#20064;&#26550;&#26500;&#65292;&#36890;&#36807;&#31283;&#23450;&#24615;&#21644;&#35748;&#35777;&#23548;&#21521;&#65292;&#29983;&#25104;&#21487;&#38752;&#30340;&#25968;&#25454;&#39537;&#21160;&#26367;&#20195;&#27169;&#22411;&#65292;&#24182;&#22522;&#20110;&#21322;&#23450;&#35268;&#21010;&#36827;&#34892;&#35748;&#35777;&#25511;&#21046;&#22120;&#35774;&#35745;&#12290;&#23427;&#22312;&#22810;&#20010;&#22522;&#20934;&#31034;&#20363;&#19978;&#23637;&#31034;&#20102;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Koopman&#31639;&#23376;&#20316;&#20026;&#26426;&#22120;&#23398;&#20064;&#21160;&#24577;&#25511;&#21046;&#31995;&#32479;&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#20854;&#20013;&#31639;&#23376;&#36890;&#36807;&#25193;&#23637;&#21160;&#24577;&#27169;&#24577;&#20998;&#35299;&#65288;EDMD&#65289;&#21551;&#21457;&#24335;&#36817;&#20284;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31283;&#23450;&#24615;&#21644;&#35748;&#35777;&#23548;&#21521;&#30340;EDMD&#65288;SafEDMD&#65289;&#65306;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;EDMD&#30340;&#23398;&#20064;&#26550;&#26500;&#65292;&#23427;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#35777;&#20070;&#65292;&#20174;&#32780;&#20197;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#29983;&#25104;&#21487;&#38752;&#30340;&#26367;&#20195;&#27169;&#22411;&#12290;&#20026;&#20102;&#30830;&#20445;SafEDMD&#30340;&#21487;&#38752;&#24615;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#27604;&#20363;&#35823;&#24046;&#30028;&#38480;&#65292;&#36825;&#20123;&#30028;&#38480;&#22312;&#21407;&#28857;&#22788;&#28040;&#22833;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#25511;&#21046;&#20219;&#21153;&#65292;&#20174;&#32780;&#22522;&#20110;&#21322;&#23450;&#35268;&#21010;&#36827;&#34892;&#35748;&#35777;&#25511;&#21046;&#22120;&#35774;&#35745;&#12290;&#25105;&#20204;&#36890;&#36807;&#20960;&#20010;&#22522;&#20934;&#31034;&#20363;&#35828;&#26126;&#20102;&#25152;&#24320;&#21457;&#30340;&#26426;&#21046;&#65292;&#24182;&#24378;&#35843;&#20854;&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Koopman operator serves as the theoretical backbone for machine learning of dynamical control systems, where the operator is heuristically approximated by extended dynamic mode decomposition (EDMD). In this paper, we propose Stability- and certificate-oriented EDMD (SafEDMD): a novel EDMD-based learning architecture which comes along with rigorous certificates, resulting in a reliable surrogate model generated in a data-driven fashion. To ensure trustworthiness of SafEDMD, we derive proportional error bounds, which vanish at the origin and are tailored for control tasks, leading to certified controller design based on semi-definite programming. We illustrate the developed machinery by means of several benchmark examples and highlight the advantages over state-of-the-art methods.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#38543;&#26426;&#29305;&#24449;&#65292;&#25105;&#20204;&#21457;&#29616;&#27880;&#24847;&#21147;&#23618;&#20855;&#26377;&#36739;&#39640;&#30340;&#35789;&#25935;&#24863;&#24615;&#65292;&#36825;&#23545;&#20110;&#29702;&#35299;transformers&#30340;&#25104;&#21151;&#20197;&#21450;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#19978;&#19979;&#25991;&#21547;&#20041;&#38750;&#24120;&#37325;&#35201;&#12290;</title><link>https://arxiv.org/abs/2402.02969</link><description>&lt;p&gt;
&#20851;&#20110;&#27880;&#24847;&#21147;&#23618;&#30340;&#35789;&#25935;&#24863;&#24615;&#30340;&#29702;&#35299;&#65306;&#36890;&#36807;&#38543;&#26426;&#29305;&#24449;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Understanding the Word Sensitivity of Attention Layers: A Study via Random Features
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02969
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#38543;&#26426;&#29305;&#24449;&#65292;&#25105;&#20204;&#21457;&#29616;&#27880;&#24847;&#21147;&#23618;&#20855;&#26377;&#36739;&#39640;&#30340;&#35789;&#25935;&#24863;&#24615;&#65292;&#36825;&#23545;&#20110;&#29702;&#35299;transformers&#30340;&#25104;&#21151;&#20197;&#21450;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#19978;&#19979;&#25991;&#21547;&#20041;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25581;&#31034;transformers&#24322;&#24120;&#25104;&#21151;&#32972;&#21518;&#21407;&#22240;&#38656;&#35201;&#26356;&#22909;&#22320;&#29702;&#35299;&#20026;&#20160;&#20040;&#27880;&#24847;&#21147;&#23618;&#36866;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12290;&#29305;&#21035;&#26159;&#65292;&#36825;&#20123;&#20219;&#21153;&#35201;&#27714;&#39044;&#27979;&#27169;&#22411;&#25429;&#25417;&#19978;&#19979;&#25991;&#21547;&#20041;&#65292;&#21363;&#20351;&#21477;&#23376;&#24456;&#38271;&#65292;&#36825;&#24448;&#24448;&#21462;&#20915;&#20110;&#19968;&#20010;&#25110;&#20960;&#20010;&#35789;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#22312;&#38543;&#26426;&#29305;&#24449;&#30340;&#20856;&#22411;&#35774;&#32622;&#20013;&#30740;&#31350;&#20102;&#36825;&#19968;&#20851;&#38190;&#23646;&#24615;&#65292;&#31216;&#20026;&#35789;&#25935;&#24863;&#24615;&#65288;WS&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#27880;&#24847;&#21147;&#23618;&#20855;&#26377;&#36739;&#39640;&#30340;WS&#65292;&#21363;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#23384;&#22312;&#19968;&#20010;&#21521;&#37327;&#65292;&#33021;&#22815;&#22823;&#24133;&#25200;&#21160;&#38543;&#26426;&#27880;&#24847;&#21147;&#29305;&#24449;&#26144;&#23556;&#12290;&#36825;&#20010;&#35770;&#28857;&#20851;&#38190;&#22320;&#21033;&#29992;&#20102;&#27880;&#24847;&#21147;&#23618;&#20013;softmax&#30340;&#20316;&#29992;&#65292;&#31361;&#26174;&#20102;&#23427;&#30456;&#23545;&#20110;&#20854;&#20182;&#28608;&#27963;&#20989;&#25968;&#65288;&#22914;ReLU&#65289;&#30340;&#20248;&#21183;&#12290;&#30456;&#21453;&#65292;&#26631;&#20934;&#38543;&#26426;&#29305;&#24449;&#30340;WS&#26159;$1/\sqrt{n}$&#38454;&#30340;&#65292;$n$&#26159;&#25991;&#26412;&#26679;&#26412;&#20013;&#30340;&#21333;&#35789;&#25968;&#65292;&#22240;&#27492;&#23427;&#38543;&#19978;&#19979;&#25991;&#30340;&#38271;&#24230;&#32780;&#34928;&#20943;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#20851;&#20110;&#35789;&#25935;&#24863;&#24615;&#30340;&#32467;&#26524;&#36716;&#21270;&#20026;&#27867;&#21270;&#30028;&#65306;&#30001;&#20110;...
&lt;/p&gt;
&lt;p&gt;
Unveiling the reasons behind the exceptional success of transformers requires a better understanding of why attention layers are suitable for NLP tasks. In particular, such tasks require predictive models to capture contextual meaning which often depends on one or few words, even if the sentence is long. Our work studies this key property, dubbed word sensitivity (WS), in the prototypical setting of random features. We show that attention layers enjoy high WS, namely, there exists a vector in the space of embeddings that largely perturbs the random attention features map. The argument critically exploits the role of the softmax in the attention layer, highlighting its benefit compared to other activations (e.g., ReLU). In contrast, the WS of standard random features is of order $1/\sqrt{n}$, $n$ being the number of words in the textual sample, and thus it decays with the length of the context. We then translate these results on the word sensitivity into generalization bounds: due to th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#25935;&#25463;&#20294;&#23433;&#20840;&#65288;ABS&#65289;&#30340;&#23398;&#20064;&#25511;&#21046;&#26694;&#26550;&#65292;&#33021;&#22815;&#23454;&#29616;&#22235;&#36275;&#26426;&#22120;&#20154;&#30340;&#25935;&#25463;&#19988;&#26080;&#30896;&#25758;&#34892;&#36208;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#19968;&#20010;&#23398;&#20064;&#24471;&#21040;&#30340;&#25511;&#21046;&#35770;&#21040;&#36798;-&#36991;&#20813;&#20540;&#32593;&#32476;&#26469;&#23454;&#29616;&#31574;&#30053;&#20999;&#25442;&#65292;&#24182;&#36890;&#36807;&#21327;&#20316;&#36816;&#34892;&#30340;&#25935;&#25463;&#31574;&#30053;&#21644;&#24674;&#22797;&#31574;&#30053;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#39640;&#36895;&#19988;&#23433;&#20840;&#22320;&#23548;&#33322;&#12290;</title><link>https://arxiv.org/abs/2401.17583</link><description>&lt;p&gt;
&#25935;&#25463;&#20294;&#23433;&#20840;&#65306;&#23398;&#20064;&#26080;&#30896;&#25758;&#39640;&#36895;&#22235;&#36275;&#26426;&#22120;&#20154;&#34892;&#36208;
&lt;/p&gt;
&lt;p&gt;
Agile But Safe: Learning Collision-Free High-Speed Legged Locomotion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17583
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#25935;&#25463;&#20294;&#23433;&#20840;&#65288;ABS&#65289;&#30340;&#23398;&#20064;&#25511;&#21046;&#26694;&#26550;&#65292;&#33021;&#22815;&#23454;&#29616;&#22235;&#36275;&#26426;&#22120;&#20154;&#30340;&#25935;&#25463;&#19988;&#26080;&#30896;&#25758;&#34892;&#36208;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#19968;&#20010;&#23398;&#20064;&#24471;&#21040;&#30340;&#25511;&#21046;&#35770;&#21040;&#36798;-&#36991;&#20813;&#20540;&#32593;&#32476;&#26469;&#23454;&#29616;&#31574;&#30053;&#20999;&#25442;&#65292;&#24182;&#36890;&#36807;&#21327;&#20316;&#36816;&#34892;&#30340;&#25935;&#25463;&#31574;&#30053;&#21644;&#24674;&#22797;&#31574;&#30053;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#39640;&#36895;&#19988;&#23433;&#20840;&#22320;&#23548;&#33322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26434;&#20081;&#29615;&#22659;&#20013;&#34892;&#36208;&#30340;&#22235;&#36275;&#26426;&#22120;&#20154;&#24517;&#39035;&#26082;&#25935;&#25463;&#20197;&#25552;&#39640;&#20219;&#21153;&#25191;&#34892;&#25928;&#29575;&#65292;&#21448;&#35201;&#30830;&#20445;&#23433;&#20840;&#65292;&#36991;&#20813;&#19982;&#38556;&#30861;&#29289;&#25110;&#20154;&#30896;&#25758;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#35201;&#20040;&#24320;&#21457;&#20445;&#23432;&#30340;&#25511;&#21046;&#22120;&#65288;&#36895;&#24230;&#23567;&#20110;1.0 m/s&#65289;&#20197;&#30830;&#20445;&#23433;&#20840;&#65292;&#35201;&#20040;&#19987;&#27880;&#20110;&#25935;&#25463;&#24615;&#32780;&#26410;&#32771;&#34385;&#28508;&#22312;&#33268;&#21629;&#30340;&#30896;&#25758;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#25935;&#25463;&#20294;&#23433;&#20840;&#65288;ABS&#65289;&#30340;&#23398;&#20064;&#25511;&#21046;&#26694;&#26550;&#65292;&#20026;&#22235;&#36275;&#26426;&#22120;&#20154;&#23454;&#29616;&#20102;&#25935;&#25463;&#19988;&#26080;&#30896;&#25758;&#30340;&#34892;&#36208;&#12290;ABS&#21253;&#25324;&#19968;&#20010;&#25935;&#25463;&#31574;&#30053;&#26469;&#22312;&#38556;&#30861;&#29289;&#20013;&#25191;&#34892;&#28789;&#27963;&#30340;&#21160;&#20316;&#25216;&#33021;&#65292;&#24182;&#19988;&#26377;&#19968;&#20010;&#24674;&#22797;&#31574;&#30053;&#26469;&#36991;&#20813;&#22833;&#36133;&#65292;&#20849;&#21516;&#23454;&#29616;&#39640;&#36895;&#19988;&#26080;&#30896;&#25758;&#30340;&#23548;&#33322;&#12290;ABS&#20013;&#30340;&#31574;&#30053;&#20999;&#25442;&#30001;&#19968;&#20010;&#23398;&#20064;&#24471;&#21040;&#30340;&#25511;&#21046;&#35770;&#21040;&#36798;-&#36991;&#20813;&#20540;&#32593;&#32476;&#25511;&#21046;&#65292;&#35813;&#32593;&#32476;&#20063;&#25351;&#23548;&#24674;&#22797;&#31574;&#30053;&#20316;&#20026;&#30446;&#26631;&#20989;&#25968;&#65292;&#20174;&#32780;&#22312;&#38381;&#29615;&#20013;&#20445;&#25252;&#26426;&#22120;&#20154;&#12290;&#35757;&#32451;&#36807;&#31243;&#28041;&#21450;&#25935;&#25463;&#31574;&#30053;&#12289;&#21040;&#36798;-&#36991;&#20813;&#20540;&#32593;&#32476;&#12289;&#24674;&#22797;&#31574;&#30053;&#21644;&#22806;&#24863;&#30693;&#34920;&#24449;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Legged robots navigating cluttered environments must be jointly agile for efficient task execution and safe to avoid collisions with obstacles or humans. Existing studies either develop conservative controllers (&lt; 1.0 m/s) to ensure safety, or focus on agility without considering potentially fatal collisions. This paper introduces Agile But Safe (ABS), a learning-based control framework that enables agile and collision-free locomotion for quadrupedal robots. ABS involves an agile policy to execute agile motor skills amidst obstacles and a recovery policy to prevent failures, collaboratively achieving high-speed and collision-free navigation. The policy switch in ABS is governed by a learned control-theoretic reach-avoid value network, which also guides the recovery policy as an objective function, thereby safeguarding the robot in a closed loop. The training process involves the learning of the agile policy, the reach-avoid value network, the recovery policy, and an exteroception repre
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#24320;&#21457;&#20102;&#29992;&#20110;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#30340;&#24320;&#25918;&#24335;&#22522;&#30784;&#27169;&#22411;&#65292;&#20197;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#20026;&#20363;&#65292;&#21457;&#24067;&#22312;GitHub&#21644;Hugging Face&#19978;&#20379;&#31038;&#21306;&#20351;&#29992;&#21644;&#36827;&#19968;&#27493;&#24320;&#21457;&#12290;</title><link>http://arxiv.org/abs/2401.16640</link><description>&lt;p&gt;
TeenyTinyLlama&#65306;&#22522;&#20110;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#35757;&#32451;&#30340;&#24320;&#28304;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
TeenyTinyLlama: open-source tiny language models trained in Brazilian Portuguese. (arXiv:2401.16640v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16640
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#24320;&#21457;&#20102;&#29992;&#20110;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#30340;&#24320;&#25918;&#24335;&#22522;&#30784;&#27169;&#22411;&#65292;&#20197;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#20026;&#20363;&#65292;&#21457;&#24067;&#22312;GitHub&#21644;Hugging Face&#19978;&#20379;&#31038;&#21306;&#20351;&#29992;&#21644;&#36827;&#19968;&#27493;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#22312;&#21508;&#31181;&#35821;&#35328;&#20013;&#30340;&#36827;&#23637;&#36824;&#19981;&#24179;&#34913;&#12290;&#34429;&#28982;&#22823;&#22810;&#25968;LLMs&#26159;&#22312;&#20687;&#33521;&#35821;&#36825;&#26679;&#30340;&#39640;&#36164;&#28304;&#35821;&#35328;&#20013;&#35757;&#32451;&#30340;&#65292;&#20294;&#22810;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#27604;&#21333;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#31245;&#24046;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#30340;&#22810;&#35821;&#35328;&#22522;&#30784;&#26377;&#26102;&#20250;&#38480;&#21046;&#23427;&#20204;&#20135;&#29983;&#30340;&#21103;&#20135;&#21697;&#65292;&#22914;&#35745;&#31639;&#38656;&#27714;&#21644;&#35768;&#21487;&#21046;&#24230;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35760;&#24405;&#20102;&#20026;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#20351;&#29992;&#32780;&#37327;&#36523;&#23450;&#21046;&#30340;&#24320;&#25918;&#24335;&#22522;&#30784;&#27169;&#22411;&#30340;&#24320;&#21457;&#36807;&#31243;&#12289;&#20854;&#23616;&#38480;&#24615;&#21644;&#20248;&#21183;&#12290;&#36825;&#23601;&#26159;TeenyTinyLlama&#65306;&#20004;&#20010;&#29992;&#20110;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#25991;&#26412;&#29983;&#25104;&#30340;&#32039;&#20945;&#22411;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;GitHub&#21644;Hugging Face&#19978;&#20197;&#23485;&#26494;&#30340;Apache 2.0&#35768;&#21487;&#35777;&#21457;&#24067;&#23427;&#20204;&#65292;&#20379;&#31038;&#21306;&#20351;&#29992;&#21644;&#36827;&#19968;&#27493;&#24320;&#21457;&#12290;&#35814;&#35265;https://github.com/Nkluge-correa/TeenyTinyLlama
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have significantly advanced natural language processing, but their progress has yet to be equal across languages. While most LLMs are trained in high-resource languages like English, multilingual models generally underperform monolingual ones. Additionally, aspects of their multilingual foundation sometimes restrict the byproducts they produce, like computational demands and licensing regimes. In this study, we document the development of open-foundation models tailored for use in low-resource settings, their limitations, and their benefits. This is the TeenyTinyLlama pair: two compact models for Brazilian Portuguese text generation. We release them under the permissive Apache 2.0 license on GitHub and Hugging Face for community use and further development. See https://github.com/Nkluge-correa/TeenyTinyLlama
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;STG-LLM&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#29702;&#35299;&#26102;&#31354;&#25968;&#25454;&#24182;&#36827;&#34892;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;STG-Tokenizer&#23558;&#22797;&#26434;&#30340;&#22270;&#24418;&#25968;&#25454;&#36716;&#21270;&#20026;&#31616;&#27905;&#30340;&#26631;&#35760;&#65292;&#20877;&#36890;&#36807;STG-Adapter&#23558;&#26631;&#35760;&#21270;&#25968;&#25454;&#19982;LLM&#30340;&#29702;&#35299;&#33021;&#21147;&#36827;&#34892;&#36830;&#25509;&#12290;&#36890;&#36807;&#24494;&#35843;&#21442;&#25968;&#65292;STG-LLM&#33021;&#22815;&#26377;&#25928;&#22320;&#25226;&#25569;&#26631;&#35760;&#30340;&#35821;&#20041;&#65292;&#21516;&#26102;&#20445;&#30041;LLM&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;STG-LLM&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.14192</link><description>&lt;p&gt;
&#22914;&#20309;&#35753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29702;&#35299;&#26102;&#31354;&#25968;&#25454;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Can Large Language Models Understand Spatial-Temporal Data?. (arXiv:2401.14192v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14192
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;STG-LLM&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#29702;&#35299;&#26102;&#31354;&#25968;&#25454;&#24182;&#36827;&#34892;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;STG-Tokenizer&#23558;&#22797;&#26434;&#30340;&#22270;&#24418;&#25968;&#25454;&#36716;&#21270;&#20026;&#31616;&#27905;&#30340;&#26631;&#35760;&#65292;&#20877;&#36890;&#36807;STG-Adapter&#23558;&#26631;&#35760;&#21270;&#25968;&#25454;&#19982;LLM&#30340;&#29702;&#35299;&#33021;&#21147;&#36827;&#34892;&#36830;&#25509;&#12290;&#36890;&#36807;&#24494;&#35843;&#21442;&#25968;&#65292;STG-LLM&#33021;&#22815;&#26377;&#25928;&#22320;&#25226;&#25569;&#26631;&#35760;&#30340;&#35821;&#20041;&#65292;&#21516;&#26102;&#20445;&#30041;LLM&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;STG-LLM&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#31561;&#20219;&#21153;&#20013;&#21344;&#25454;&#20027;&#23548;&#22320;&#20301;&#65292;&#20294;&#21033;&#29992;&#23427;&#20204;&#30340;&#33021;&#21147;&#36827;&#34892;&#26102;&#31354;&#39044;&#27979;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26102;&#24207;&#25991;&#26412;&#19982;&#22797;&#26434;&#30340;&#26102;&#31354;&#25968;&#25454;&#20043;&#38388;&#30340;&#24046;&#24322;&#38459;&#30861;&#20102;&#35813;&#24212;&#29992;&#30340;&#23454;&#29616;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;STG-LLM&#65292;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#20026;LLM&#36171;&#20104;&#20102;&#26102;&#31354;&#39044;&#27979;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#20197;&#19979;&#26041;&#24335;&#35299;&#20915;&#25968;&#25454;&#19981;&#21305;&#37197;&#30340;&#38382;&#39064;&#65306;1&#65289;STG-Tokenizer&#65306;&#36825;&#20010;&#26102;&#31354;&#22270;&#24418;&#26631;&#35760;&#22120;&#23558;&#22797;&#26434;&#30340;&#22270;&#24418;&#25968;&#25454;&#36716;&#21270;&#20026;&#31616;&#27905;&#30340;&#26631;&#35760;&#65292;&#25429;&#25417;&#20102;&#31354;&#38388;&#21644;&#26102;&#38388;&#20851;&#31995;&#65307;2&#65289;STG-Adapter&#65306;&#36825;&#20010;&#31934;&#31616;&#30340;&#36866;&#37197;&#22120;&#30001;&#32447;&#24615;&#32534;&#30721;&#21644;&#35299;&#30721;&#23618;&#32452;&#25104;&#65292;&#22635;&#34917;&#20102;&#26631;&#35760;&#21270;&#25968;&#25454;&#21644;LLM&#29702;&#35299;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#36890;&#36807;&#20165;&#24494;&#35843;&#19968;&#23567;&#37096;&#20998;&#21442;&#25968;&#65292;&#23427;&#21487;&#20197;&#26377;&#25928;&#22320;&#25226;&#25569;STG-Tokenizer&#29983;&#25104;&#30340;&#26631;&#35760;&#30340;&#35821;&#20041;&#65292;&#21516;&#26102;&#20445;&#30041;LLM&#30340;&#21407;&#22987;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#12290;&#36890;&#36807;&#22312;&#22810;&#31181;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;STG-LLM&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Large Language Models (LLMs) dominate tasks like natural language processing and computer vision, harnessing their power for spatial-temporal forecasting remains challenging. The disparity between sequential text and complex spatial-temporal data hinders this application. To address this issue, this paper introduces STG-LLM, an innovative approach empowering LLMs for spatial-temporal forecasting. We tackle the data mismatch by proposing: 1) STG-Tokenizer: This spatial-temporal graph tokenizer transforms intricate graph data into concise tokens capturing both spatial and temporal relationships; 2) STG-Adapter: This minimalistic adapter, consisting of linear encoding and decoding layers, bridges the gap between tokenized data and LLM comprehension. By fine-tuning only a small set of parameters, it can effectively grasp the semantics of tokens generated by STG-Tokenizer, while preserving the original natural language understanding capabilities of LLMs. Extensive experiments on diver
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25506;&#32034;&#31639;&#27861;&#65292;&#31216;&#20026;&#20998;&#24067;&#24335;&#38543;&#26426;&#32593;&#32476;&#33976;&#39311;&#65288;DRND&#65289;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#33976;&#39311;&#38543;&#26426;&#32593;&#32476;&#30340;&#20998;&#24067;&#21644;&#38544;&#24335;&#34701;&#20837;&#20266;&#35745;&#25968;&#26469;&#25913;&#36827;&#22870;&#21169;&#20998;&#37197;&#30340;&#31934;&#24230;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#25506;&#32034;&#36807;&#31243;&#12290;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#32467;&#26524;&#22343;&#34920;&#26126;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.09750</link><description>&lt;p&gt;
&#20351;&#29992;&#20998;&#24067;&#24335;&#38543;&#26426;&#32593;&#32476;&#33976;&#39311;&#30340;&#25506;&#32034;&#21644;&#21453;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Exploration and Anti-Exploration with Distributional Random Network Distillation. (arXiv:2401.09750v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09750
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25506;&#32034;&#31639;&#27861;&#65292;&#31216;&#20026;&#20998;&#24067;&#24335;&#38543;&#26426;&#32593;&#32476;&#33976;&#39311;&#65288;DRND&#65289;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#33976;&#39311;&#38543;&#26426;&#32593;&#32476;&#30340;&#20998;&#24067;&#21644;&#38544;&#24335;&#34701;&#20837;&#20266;&#35745;&#25968;&#26469;&#25913;&#36827;&#22870;&#21169;&#20998;&#37197;&#30340;&#31934;&#24230;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#25506;&#32034;&#36807;&#31243;&#12290;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#32467;&#26524;&#22343;&#34920;&#26126;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#25506;&#32034;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65292;&#23545;&#20110;&#19968;&#20010;&#26234;&#33021;&#20307;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#21462;&#24471;&#39640;&#22238;&#25253;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#30446;&#21069;&#30340;&#25506;&#32034;&#38543;&#26426;&#32593;&#32476;&#33976;&#39311;&#65288;Random Network Distillation&#65292;RND&#65289;&#31639;&#27861;&#24050;&#22312;&#35768;&#22810;&#29615;&#22659;&#20013;&#35777;&#26126;&#26377;&#25928;&#65292;&#20294;&#23427;&#22312;&#22870;&#21169;&#20998;&#37197;&#19978;&#24448;&#24448;&#38656;&#35201;&#26356;&#39640;&#30340;&#21306;&#20998;&#33021;&#21147;&#12290;&#26412;&#25991;&#31361;&#20986;&#20102;RND&#20013;&#30340;&#8220;&#22870;&#21169;&#19981;&#19968;&#33268;&#8221;&#38382;&#39064;&#65292;&#24182;&#25351;&#20986;&#20102;&#20854;&#20027;&#35201;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20998;&#24067;&#24335;RND&#65288;DRND&#65289;&#65292;&#23427;&#26159;RND&#30340;&#19968;&#20010;&#21464;&#20307;&#12290;DRND&#36890;&#36807;&#33976;&#39311;&#38543;&#26426;&#32593;&#32476;&#30340;&#20998;&#24067;&#24182;&#38544;&#24335;&#22320;&#34701;&#20837;&#20266;&#35745;&#25968;&#26469;&#25913;&#36827;&#22870;&#21169;&#20998;&#37197;&#30340;&#31934;&#24230;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#25506;&#32034;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#32531;&#35299;&#20102;&#19981;&#19968;&#33268;&#38382;&#39064;&#65292;&#32780;&#19981;&#20250;&#24341;&#20837;&#26174;&#33879;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#32467;&#26524;&#22343;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploration remains a critical issue in deep reinforcement learning for an agent to attain high returns in unknown environments. Although the prevailing exploration Random Network Distillation (RND) algorithm has been demonstrated to be effective in numerous environments, it often needs more discriminative power in bonus allocation. This paper highlights the ``bonus inconsistency'' issue within RND, pinpointing its primary limitation. To address this issue, we introduce the Distributional RND (DRND), a derivative of the RND. DRND enhances the exploration process by distilling a distribution of random networks and implicitly incorporating pseudo counts to improve the precision of bonus allocation. This refinement encourages agents to engage in more extensive exploration. Our method effectively mitigates the inconsistency issue without introducing significant computational overhead. Both theoretical analysis and experimental results demonstrate the superiority of our approach over the or
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;(PINNs)&#23454;&#29616;&#25968;&#23383;&#23402;&#29983;(DT)&#30340;&#28508;&#21147;&#12290;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#26080;&#32593;&#26684;&#26694;&#26550;&#30340;&#33258;&#36866;&#24212;&#37319;&#26679;&#26041;&#27861;&#65292;&#24182;&#39564;&#35777;&#20102;PINNs&#22312;&#21442;&#25968;&#21270;&#30340;Navier-Stokes&#26041;&#31243;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#22810;&#20445;&#30495;&#24230;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2401.08667</link><description>&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65306;&#25968;&#23383;&#23402;&#29983;&#30340;&#35266;&#28857;
&lt;/p&gt;
&lt;p&gt;
Data-Driven Physics-Informed Neural Networks: A Digital Twin Perspective. (arXiv:2401.08667v1 [physics.flu-dyn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08667
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;(PINNs)&#23454;&#29616;&#25968;&#23383;&#23402;&#29983;(DT)&#30340;&#28508;&#21147;&#12290;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#26080;&#32593;&#26684;&#26694;&#26550;&#30340;&#33258;&#36866;&#24212;&#37319;&#26679;&#26041;&#27861;&#65292;&#24182;&#39564;&#35777;&#20102;PINNs&#22312;&#21442;&#25968;&#21270;&#30340;Navier-Stokes&#26041;&#31243;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#22810;&#20445;&#30495;&#24230;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#19981;&#21516;&#35282;&#24230;&#25506;&#32034;&#20102;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;(PINNs)&#23454;&#29616;&#25968;&#23383;&#23402;&#29983;(DT)&#30340;&#28508;&#21147;&#12290;&#39318;&#20808;&#65292;&#30740;&#31350;&#20102;&#29992;&#20110;&#37197;&#28857;&#30340;&#21508;&#31181;&#33258;&#36866;&#24212;&#37319;&#26679;&#26041;&#27861;&#65292;&#20197;&#39564;&#35777;&#23427;&#20204;&#22312;&#26080;&#32593;&#26684;&#26694;&#26550;&#30340;PINNs&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#35813;&#26694;&#26550;&#20801;&#35768;&#33258;&#21160;&#26500;&#24314;&#34394;&#25311;&#34920;&#31034;&#65292;&#26080;&#38656;&#25163;&#21160;&#29983;&#25104;&#32593;&#26684;&#12290;&#28982;&#21518;&#65292;&#26816;&#39564;&#20102;&#25968;&#25454;&#39537;&#21160;&#30340;PINNs(DD-PINNs)&#26694;&#26550;&#30340;&#25972;&#20307;&#24615;&#33021;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#21033;&#29992;&#22312;DT&#22330;&#26223;&#20013;&#33719;&#24471;&#30340;&#25968;&#25454;&#38598;&#12290;&#20854;&#23545;&#21442;&#25968;&#21270;&#30340;Navier-Stokes&#26041;&#31243;&#30340;&#26356;&#19968;&#33324;&#29289;&#29702;&#24615;&#30340;&#21487;&#25193;&#23637;&#24615;&#24471;&#21040;&#20102;&#39564;&#35777;&#65292;&#20854;&#20013;PINNs&#22312;&#38647;&#35834;&#25968;&#21464;&#21270;&#26102;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#23454;&#38469;&#19978;&#25968;&#25454;&#38598;&#32463;&#24120;&#21487;&#20197;&#22312;&#19981;&#21516;&#30340;&#20445;&#30495;&#24230;/&#31232;&#30095;&#24230;&#19979;&#25910;&#38598;&#65292;&#36824;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#22810;&#20445;&#30495;&#24230;&#30340;DD-PINNs&#12290;&#23427;&#20204;&#22312;&#22806;&#25512;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#30456;&#23545;&#20110;&#21333;&#20445;&#30495;&#24230;&#26041;&#27861;&#25552;&#39640;&#20102;42&#65285;&#21040;62&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study explores the potential of physics-informed neural networks (PINNs) for the realization of digital twins (DT) from various perspectives. First, various adaptive sampling approaches for collocation points are investigated to verify their effectiveness in the mesh-free framework of PINNs, which allows automated construction of virtual representation without manual mesh generation. Then, the overall performance of the data-driven PINNs (DD-PINNs) framework is examined, which can utilize the acquired datasets in DT scenarios. Its scalability to more general physics is validated within parametric Navier-Stokes equations, where PINNs do not need to be retrained as the Reynolds number varies. In addition, since datasets can be often collected from different fidelity/sparsity in practice, multi-fidelity DD-PINNs are also proposed and evaluated. They show remarkable prediction performance even in the extrapolation tasks, with $42\sim62\%$ improvement over the single-fidelity approach.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#35299;&#37322;&#26159;&#21542;&#21487;&#38752;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;AI&#23433;&#20840;&#32771;&#34385;&#22240;&#32032;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#33258;&#27965;&#24615;&#26816;&#27979;&#20316;&#20026;&#35780;&#20272;&#20854;&#21487;&#38752;&#24615;&#21644;&#35299;&#37322;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.07927</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#35299;&#37322;&#26159;&#21542;&#21487;&#38752;?
&lt;/p&gt;
&lt;p&gt;
Are self-explanations from Large Language Models faithful?. (arXiv:2401.07927v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.07927
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#35299;&#37322;&#26159;&#21542;&#21487;&#38752;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;AI&#23433;&#20840;&#32771;&#34385;&#22240;&#32032;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#33258;&#27965;&#24615;&#26816;&#27979;&#20316;&#20026;&#35780;&#20272;&#20854;&#21487;&#38752;&#24615;&#21644;&#35299;&#37322;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#36807;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35768;&#22810;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#29978;&#33267;&#33021;&#22815;&#25552;&#20379;&#20854;&#34892;&#20026;&#30340;&#35299;&#37322;&#12290;&#30001;&#20110;&#36825;&#20123;&#27169;&#22411;&#23545;&#20844;&#20247;&#26159;&#30452;&#25509;&#21487;&#35775;&#38382;&#30340;&#65292;&#22240;&#27492;&#23384;&#22312;&#36825;&#26679;&#30340;&#39118;&#38505;&#65292;&#21363;&#20196;&#20154;&#20449;&#26381;&#20294;&#38169;&#35823;&#30340;&#35299;&#37322;&#21487;&#33021;&#23548;&#33268;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26080;&#25903;&#25745;&#30340;&#33258;&#20449;&#12290;&#22240;&#27492;&#65292;&#35299;&#37322;&#33021;&#21147;&#21644;&#21487;&#38752;&#24615;&#26159;AI&#23433;&#20840;&#30340;&#37325;&#35201;&#32771;&#34385;&#22240;&#32032;&#12290;&#35780;&#20272;&#33258;&#25105;&#35299;&#37322;&#30340;&#21487;&#38752;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#20154;&#31867;&#26469;&#35828;&#36807;&#20110;&#22797;&#26434;&#65292;&#26080;&#27861;&#27880;&#37322;&#20160;&#20040;&#26159;&#27491;&#30830;&#30340;&#35299;&#37322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#33258;&#27965;&#24615;&#26816;&#27979;&#20316;&#20026;&#21487;&#38752;&#24615;&#30340;&#34913;&#37327;&#25351;&#26631;&#12290;&#20363;&#22914;&#65292;&#22914;&#26524;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35828;&#26576;&#32452;&#35789;&#23545;&#20110;&#20570;&#20986;&#39044;&#27979;&#24456;&#37325;&#35201;&#65292;&#37027;&#20040;&#22312;&#27809;&#26377;&#36825;&#20123;&#35789;&#30340;&#24773;&#20917;&#19979;&#65292;&#23427;&#24212;&#35813;&#26080;&#27861;&#20570;&#20986;&#30456;&#21516;&#30340;&#39044;&#27979;&#12290;&#34429;&#28982;&#33258;&#27965;&#24615;&#26816;&#27979;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#21487;&#38752;&#24615;&#26041;&#27861;&#65292;&#20294;&#20043;&#21069;&#23578;&#26410;&#24212;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#35299;&#37322;&#20013;&#12290;&#25105;&#20204;&#23558;&#33258;&#27965;&#24615;&#26816;&#27979;&#24212;&#29992;&#20110;...
&lt;/p&gt;
&lt;p&gt;
Instruction-tuned large language models (LLMs) excel at many tasks, and will even provide explanations for their behavior. Since these models are directly accessible to the public, there is a risk that convincing and wrong explanations can lead to unsupported confidence in LLMs. Therefore, interpretability-faithfulness of self-explanations is an important consideration for AI Safety. Assessing the interpretability-faithfulness of these explanations, termed self-explanations, is challenging as the models are too complex for humans to annotate what is a correct explanation. To address this, we propose employing self-consistency checks as a measure of faithfulness. For example, if an LLM says a set of words is important for making a prediction, then it should not be able to make the same prediction without these words. While self-consistency checks are a common approach to faithfulness, they have not previously been applied to LLM's self-explanations. We apply self-consistency checks to t
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32467;&#21512;&#36755;&#20837;&#20984;&#24615;&#21644;Lipschitz&#36830;&#32493;&#24615;&#30340;&#20248;&#21183;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;&#36755;&#20837;&#20984;&#24615;Lipschitz&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#22411;&#32593;&#32476;&#32467;&#26500;&#65292;&#22312;&#35745;&#31639;&#25928;&#29575;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#24490;&#29615;&#21333;&#20803;&#65292;&#24182;&#36866;&#29992;&#20110;&#22810;&#31181;&#24037;&#31243;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2401.07494</link><description>&lt;p&gt;
&#36755;&#20837;&#20984;&#24615;Lipschitz RNN: &#19968;&#31181;&#29992;&#20110;&#24037;&#31243;&#20219;&#21153;&#30340;&#24555;&#36895;&#21644;&#40065;&#26834;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Input Convex Lipschitz RNN: A Fast and Robust Approach for Engineering Tasks. (arXiv:2401.07494v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.07494
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32467;&#21512;&#36755;&#20837;&#20984;&#24615;&#21644;Lipschitz&#36830;&#32493;&#24615;&#30340;&#20248;&#21183;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;&#36755;&#20837;&#20984;&#24615;Lipschitz&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#22411;&#32593;&#32476;&#32467;&#26500;&#65292;&#22312;&#35745;&#31639;&#25928;&#29575;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#24490;&#29615;&#21333;&#20803;&#65292;&#24182;&#36866;&#29992;&#20110;&#22810;&#31181;&#24037;&#31243;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#25928;&#29575;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#26159;&#30495;&#23454;&#19990;&#30028;&#24037;&#31243;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#31070;&#32463;&#32593;&#32476;&#24448;&#24448;&#22312;&#21516;&#26102;&#25110;&#20998;&#21035;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#12290;&#36890;&#36807;&#20174;&#33258;&#28982;&#29289;&#29702;&#31995;&#32479;&#21644;&#29616;&#26377;&#25991;&#29486;&#20013;&#33719;&#21462;&#30340;&#35265;&#35299;&#65292;&#24050;&#30693;&#36755;&#20837;&#20984;&#24615;&#32467;&#26500;&#22686;&#24378;&#20102;&#35745;&#31639;&#25928;&#29575;&#65292;&#32780;Lipschitz&#32422;&#26463;&#32467;&#26500;&#22686;&#24378;&#20102;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;&#20984;&#24615;&#21644;Lipschitz&#36830;&#32493;&#24615;&#30340;&#20248;&#28857;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#32593;&#32476;&#32467;&#26500;&#65292;&#31216;&#20026;&#36755;&#20837;&#20984;&#24615;Lipschitz&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#12290;&#35813;&#27169;&#22411;&#22312;&#35745;&#31639;&#25928;&#29575;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#24490;&#29615;&#21333;&#20803;&#65292;&#36866;&#29992;&#20110;&#19968;&#31995;&#21015;&#24037;&#31243;&#20219;&#21153;&#65292;&#21253;&#25324;&#22522;&#20934;MNIST&#22270;&#20687;&#20998;&#31867;&#12289;&#26032;&#21152;&#22369;LHT Holdings&#20844;&#21496;&#30340;&#23454;&#38469;&#22826;&#38451;&#33021;&#20809;&#20239;&#31995;&#32479;&#35268;&#21010;&#20013;&#30340;&#23454;&#26102;&#22826;&#38451;&#36752;&#23556;&#39044;&#27979;&#65292;&#20197;&#21450;&#21270;&#23398;&#21453;&#24212;&#22120;&#30340;&#23454;&#26102;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#20248;&#21270;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computational efficiency and adversarial robustness are critical factors in real-world engineering applications. Yet, conventional neural networks often fall short in addressing both simultaneously, or even separately. Drawing insights from natural physical systems and existing literature, it is known that an input convex architecture enhances computational efficiency, while a Lipschitz-constrained architecture bolsters adversarial robustness. By leveraging the strengths of convexity and Lipschitz continuity, we develop a novel network architecture, termed Input Convex Lipschitz Recurrent Neural Networks. This model outperforms existing recurrent units across a spectrum of engineering tasks in terms of computational efficiency and adversarial robustness. These tasks encompass a benchmark MNIST image classification, real-world solar irradiance prediction for Solar PV system planning at LHT Holdings in Singapore, and real-time Model Predictive Control optimization for a chemical reactor.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20043;&#38388;&#39640;&#25928;&#36801;&#31227;&#27979;&#35797;&#38598;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#20855;&#26377;&#29992;&#25143;&#24863;&#20852;&#36259;&#30340;&#23646;&#24615;&#30340;&#33391;&#22909;&#27979;&#35797;&#38598;&#65292;&#20197;&#36798;&#21040;&#25913;&#21892;&#21487;&#39564;&#35777;&#24615;&#21644;&#27979;&#35797;&#24615;&#30340;&#30446;&#30340;&#12290;</title><link>http://arxiv.org/abs/2311.00801</link><description>&lt;p&gt;
GIST: &#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#29983;&#25104;&#36755;&#20837;&#38598;&#21512;&#30340;&#21487;&#36801;&#31227;&#24615;
&lt;/p&gt;
&lt;p&gt;
GIST: Generated Inputs Sets Transferability in Deep Learning. (arXiv:2311.00801v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00801
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20043;&#38388;&#39640;&#25928;&#36801;&#31227;&#27979;&#35797;&#38598;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#20855;&#26377;&#29992;&#25143;&#24863;&#20852;&#36259;&#30340;&#23646;&#24615;&#30340;&#33391;&#22909;&#27979;&#35797;&#38598;&#65292;&#20197;&#36798;&#21040;&#25913;&#21892;&#21487;&#39564;&#35777;&#24615;&#21644;&#27979;&#35797;&#24615;&#30340;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;&#31070;&#32463;&#32593;&#32476;&#21487;&#39564;&#35777;&#24615;&#21644;&#21487;&#27979;&#35797;&#24615;&#30340;&#38656;&#27714;&#19981;&#26029;&#22686;&#21152;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#29983;&#25104;&#27979;&#35797;&#38598;&#30340;&#26041;&#27861;&#34987;&#24320;&#21457;&#20986;&#26469;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25216;&#26415;&#20013;&#30340;&#27599;&#19968;&#31181;&#37117;&#20542;&#21521;&#20110;&#24378;&#35843;&#29305;&#23450;&#30340;&#27979;&#35797;&#26041;&#38754;&#65292;&#24182;&#19988;&#21487;&#33021;&#38750;&#24120;&#32791;&#26102;&#12290;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#20010;&#31616;&#21333;&#35299;&#20915;&#26041;&#26696;&#26159;&#26681;&#25454;&#24076;&#26395;&#36801;&#31227;&#30340;&#26399;&#26395;&#23646;&#24615;&#65292;&#22312;&#19968;&#20123;&#32463;&#36807;&#22522;&#20934;&#27979;&#35797;&#30340;&#27169;&#22411;&#21644;&#26032;&#27979;&#35797;&#27169;&#22411;&#20043;&#38388;&#36716;&#31227;&#27979;&#35797;&#38598;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;GIST&#65288;&#29983;&#25104;&#36755;&#20837;&#38598;&#21512;&#30340;&#21487;&#36801;&#31227;&#24615;&#65289;&#65292;&#19968;&#31181;&#29992;&#20110;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20043;&#38388;&#39640;&#25928;&#36801;&#31227;&#27979;&#35797;&#38598;&#30340;&#26032;&#26041;&#27861;&#12290;&#32473;&#23450;&#29992;&#25143;&#24076;&#26395;&#36801;&#31227;&#30340;&#19968;&#20010;&#24863;&#20852;&#36259;&#30340;&#23646;&#24615;&#65288;&#20363;&#22914;&#65292;&#35206;&#30422;&#20934;&#21017;&#65289;&#65292;GIST&#33021;&#22815;&#20174;&#22522;&#20934;&#25552;&#20379;&#30340;&#21487;&#29992;&#27979;&#35797;&#38598;&#20013;&#65292;&#20174;&#35813;&#23646;&#24615;&#30340;&#35282;&#24230;&#36873;&#25321;&#33391;&#22909;&#30340;&#27979;&#35797;&#38598;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#31181;&#27169;&#24577;&#21644;&#19981;&#21516;&#30340;&#27979;&#35797;&#38598;&#29983;&#25104;&#36807;&#31243;&#65292;&#22312;&#25925;&#38556;&#31867;&#22411;&#35206;&#30422;&#23646;&#24615;&#19978;&#23545;GIST&#36827;&#34892;&#32463;&#39564;&#35780;&#20272;&#65292;&#20197;&#35777;&#26126;&#35813;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the demand for verifiability and testability of neural networks continues to rise, an increasing number of methods for generating test sets are being developed. However, each of these techniques tends to emphasize specific testing aspects and can be quite time-consuming. A straightforward solution to mitigate this issue is to transfer test sets between some benchmarked models and a new model under test, based on a desirable property one wishes to transfer. This paper introduces GIST (Generated Inputs Sets Transferability), a novel approach for the efficient transfer of test sets among Deep Learning models. Given a property of interest that a user wishes to transfer (e.g., coverage criterion), GIST enables the selection of good test sets from the point of view of this property among available ones from a benchmark. We empirically evaluate GIST on fault types coverage property with two modalities and different test set generation procedures to demonstrate the approach's feasibility. E
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#22312;Wasserstein&#31354;&#38388;&#20013;&#24212;&#29992;&#36817;&#31471;&#26799;&#24230;&#19979;&#38477;&#65292;&#35777;&#26126;&#20102;&#22522;&#20110;&#27969;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#29983;&#25104;&#25968;&#25454;&#20998;&#24067;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2310.17582</link><description>&lt;p&gt;
&#22312;Wasserstein&#31354;&#38388;&#20013;&#36890;&#36807;&#36817;&#31471;&#26799;&#24230;&#19979;&#38477;&#23454;&#29616;&#22522;&#20110;&#27969;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
Convergence of flow-based generative models via proximal gradient descent in Wasserstein space. (arXiv:2310.17582v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17582
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22312;Wasserstein&#31354;&#38388;&#20013;&#24212;&#29992;&#36817;&#31471;&#26799;&#24230;&#19979;&#38477;&#65292;&#35777;&#26126;&#20102;&#22522;&#20110;&#27969;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#29983;&#25104;&#25968;&#25454;&#20998;&#24067;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27969;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#35745;&#31639;&#25968;&#25454;&#29983;&#25104;&#21644;&#20284;&#28982;&#20989;&#25968;&#26041;&#38754;&#20855;&#26377;&#19968;&#23450;&#30340;&#20248;&#21183;&#65292;&#24182;&#19988;&#26368;&#36817;&#22312;&#23454;&#35777;&#34920;&#29616;&#19978;&#26174;&#31034;&#20986;&#31454;&#20105;&#21147;&#12290;&#19982;&#30456;&#20851;&#22522;&#20110;&#20998;&#25968;&#25193;&#25955;&#27169;&#22411;&#30340;&#31215;&#32047;&#29702;&#35770;&#30740;&#31350;&#30456;&#27604;&#65292;&#23545;&#20110;&#22312;&#27491;&#21521;&#65288;&#25968;&#25454;&#21040;&#22122;&#22768;&#65289;&#21644;&#21453;&#21521;&#65288;&#22122;&#22768;&#21040;&#25968;&#25454;&#65289;&#26041;&#21521;&#19978;&#37117;&#26159;&#30830;&#23450;&#24615;&#30340;&#27969;&#27169;&#22411;&#30340;&#20998;&#26512;&#36824;&#24456;&#23569;&#12290;&#26412;&#25991;&#36890;&#36807;&#22312;&#24402;&#19968;&#21270;&#27969;&#32593;&#32476;&#20013;&#23454;&#26045;Jordan-Kinderleherer-Otto&#65288;JKO&#65289;&#26041;&#26696;&#30340;&#25152;&#35859;JKO&#27969;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#36890;&#36807;&#28176;&#36827;&#27969;&#27169;&#22411;&#29983;&#25104;&#25968;&#25454;&#20998;&#24067;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#21033;&#29992;Wasserstein&#31354;&#38388;&#20013;&#36817;&#31471;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#30340;&#25351;&#25968;&#25910;&#25947;&#24615;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36890;&#36807;JKO&#27969;&#27169;&#22411;&#29983;&#25104;&#25968;&#25454;&#30340;Kullback-Leibler&#65288;KL&#65289;&#20445;&#35777;&#20026;$O(\varepsilon^2)$&#65292;&#20854;&#20013;&#20351;&#29992;$N \lesssim \log (1/\varepsilon)$&#20010;JKO&#27493;&#39588;&#65288;&#27969;&#20013;&#30340;$N$&#20010;&#27531;&#24046;&#22359;&#65289;&#65292;&#20854;&#20013;$\varepsilon$&#26159;&#27599;&#27493;&#19968;&#38454;&#26465;&#20214;&#30340;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Flow-based generative models enjoy certain advantages in computing the data generation and the likelihood, and have recently shown competitive empirical performance. Compared to the accumulating theoretical studies on related score-based diffusion models, analysis of flow-based models, which are deterministic in both forward (data-to-noise) and reverse (noise-to-data) directions, remain sparse. In this paper, we provide a theoretical guarantee of generating data distribution by a progressive flow model, the so-called JKO flow model, which implements the Jordan-Kinderleherer-Otto (JKO) scheme in a normalizing flow network. Leveraging the exponential convergence of the proximal gradient descent (GD) in Wasserstein space, we prove the Kullback-Leibler (KL) guarantee of data generation by a JKO flow model to be $O(\varepsilon^2)$ when using $N \lesssim \log (1/\varepsilon)$ many JKO steps ($N$ Residual Blocks in the flow) where $\varepsilon $ is the error in the per-step first-order condit
&lt;/p&gt;</description></item><item><title>&#26412;&#27425;&#35843;&#26597;&#24635;&#32467;&#20102;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#22312;&#26234;&#33021;&#21046;&#36896;&#21644;&#20135;&#19994;4.0&#32972;&#26223;&#19979;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#31361;&#20986;&#20102;&#20854;&#22312;&#24037;&#19994;&#29983;&#20135;&#21644;&#21046;&#36896;&#39046;&#22495;&#30340;&#20851;&#38190;&#20248;&#21183;&#21644;&#25361;&#25112;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#20805;&#20998;&#21033;&#29992;&#30340;TDA&#26041;&#27861;&#21644;&#24050;&#35782;&#21035;&#30340;&#24212;&#29992;&#31867;&#22411;&#65292;&#20197;&#25512;&#21160;&#26356;&#22810;&#30340;&#30456;&#20851;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2310.09319</link><description>&lt;p&gt;
&#26234;&#33021;&#21046;&#36896;&#36807;&#31243;&#20013;&#30340;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;--&#20851;&#20110;&#29616;&#29366;&#30340;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Topological Data Analysis in smart manufacturing processes -- A survey on the state of the art. (arXiv:2310.09319v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09319
&lt;/p&gt;
&lt;p&gt;
&#26412;&#27425;&#35843;&#26597;&#24635;&#32467;&#20102;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#22312;&#26234;&#33021;&#21046;&#36896;&#21644;&#20135;&#19994;4.0&#32972;&#26223;&#19979;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#31361;&#20986;&#20102;&#20854;&#22312;&#24037;&#19994;&#29983;&#20135;&#21644;&#21046;&#36896;&#39046;&#22495;&#30340;&#20851;&#38190;&#20248;&#21183;&#21644;&#25361;&#25112;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#20805;&#20998;&#21033;&#29992;&#30340;TDA&#26041;&#27861;&#21644;&#24050;&#35782;&#21035;&#30340;&#24212;&#29992;&#31867;&#22411;&#65292;&#20197;&#25512;&#21160;&#26356;&#22810;&#30340;&#30456;&#20851;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#65288;TDA&#65289;&#26159;&#19968;&#31181;&#20351;&#29992;&#25299;&#25169;&#23398;&#25216;&#26415;&#23545;&#22797;&#26434;&#30340;&#22810;&#32500;&#25968;&#25454;&#36827;&#34892;&#20998;&#26512;&#30340;&#25968;&#23398;&#26041;&#27861;&#65292;&#24050;&#32463;&#22312;&#21307;&#23398;&#12289;&#26448;&#26009;&#31185;&#23398;&#12289;&#29983;&#29289;&#23398;&#31561;&#22810;&#20010;&#39046;&#22495;&#34987;&#24191;&#27867;&#32780;&#25104;&#21151;&#22320;&#24212;&#29992;&#12290;&#26412;&#35843;&#26597;&#24635;&#32467;&#20102;TDA&#22312;&#21478;&#19968;&#20010;&#24212;&#29992;&#39046;&#22495;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#65306;&#24037;&#19994;&#21046;&#36896;&#21644;&#20135;&#19994;4.0&#32972;&#26223;&#19979;&#30340;&#29983;&#20135;&#12290;&#25105;&#20204;&#23545;&#24037;&#19994;&#29983;&#20135;&#21644;&#21046;&#36896;&#39046;&#22495;&#20013;TDA&#24212;&#29992;&#36827;&#34892;&#20102;&#20005;&#35880;&#21487;&#37325;&#22797;&#30340;&#25991;&#29486;&#25628;&#32034;&#12290;&#36890;&#36807;&#23545;&#32467;&#26524;&#36827;&#34892;&#32858;&#31867;&#21644;&#20998;&#26512;&#65292;&#22522;&#20110;&#20854;&#22312;&#21046;&#36896;&#36807;&#31243;&#20013;&#30340;&#24212;&#29992;&#39046;&#22495;&#21644;&#36755;&#20837;&#25968;&#25454;&#31867;&#22411;&#36827;&#34892;&#35770;&#36848;&#12290;&#25105;&#20204;&#31361;&#20986;&#20102;TDA&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#20851;&#38190;&#20248;&#21183;&#21450;&#20854;&#24037;&#20855;&#65292;&#24182;&#25551;&#36848;&#20102;&#23427;&#30340;&#25361;&#25112;&#20197;&#21450;&#26410;&#26469;&#30340;&#28508;&#21147;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#22312;&#65288;&#29305;&#23450;&#39046;&#22495;&#30340;&#65289;&#24037;&#19994;&#20013;&#26410;&#20805;&#20998;&#21033;&#29992;&#30340;TDA&#26041;&#27861;&#21644;&#24050;&#35782;&#21035;&#30340;&#24212;&#29992;&#31867;&#22411;&#65292;&#26088;&#22312;&#20419;&#36827;&#26356;&#22810;&#30340;&#30740;&#31350;&#22312;&#24403;&#21069;&#39046;&#22495;&#20013;&#30340;&#24320;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Topological Data Analysis (TDA) is a mathematical method using techniques from topology for the analysis of complex, multi-dimensional data that has been widely and successfully applied in several fields such as medicine, material science, biology, and others. This survey summarizes the state of the art of TDA in yet another application area: industrial manufacturing and production in the context of Industry 4.0. We perform a rigorous and reproducible literature search of applications of TDA on the setting of industrial production and manufacturing. The resulting works are clustered and analyzed based on their application area within the manufacturing process and their input data type. We highlight the key benefits of TDA and their tools in this area and describe its challenges, as well as future potential. Finally, we discuss which TDA methods are underutilized in (the specific area of) industry and the identified types of application, with the goal of prompting more research in this 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;LM&#27169;&#25311;&#24037;&#20855;&#25191;&#34892;&#21644;&#24320;&#21457;&#22522;&#20110;LM&#30340;&#33258;&#21160;&#23433;&#20840;&#35780;&#20272;&#22120;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#27979;&#35797;LM&#20195;&#29702;&#30340;&#39640;&#25104;&#26412;&#21644;&#23547;&#25214;&#39640;&#39118;&#38505;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.15817</link><description>&lt;p&gt;
&#20351;&#29992;LM&#27169;&#25311;&#27801;&#30418;&#35782;&#21035;LM&#20195;&#29702;&#30340;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Identifying the Risks of LM Agents with an LM-Emulated Sandbox. (arXiv:2309.15817v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15817
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;LM&#27169;&#25311;&#24037;&#20855;&#25191;&#34892;&#21644;&#24320;&#21457;&#22522;&#20110;LM&#30340;&#33258;&#21160;&#23433;&#20840;&#35780;&#20272;&#22120;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#27979;&#35797;LM&#20195;&#29702;&#30340;&#39640;&#25104;&#26412;&#21644;&#23547;&#25214;&#39640;&#39118;&#38505;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#20195;&#29702;&#21644;&#24037;&#20855;&#20351;&#29992;&#30340;&#25216;&#26415;&#36827;&#27493;&#65292;&#20363;&#22914;ChatGPT&#25554;&#20214;&#65292;&#20351;&#24471;&#20195;&#29702;&#20855;&#22791;&#20102;&#20016;&#23500;&#30340;&#21151;&#33021;&#65292;&#20294;&#20063;&#25918;&#22823;&#20102;&#28508;&#22312;&#30340;&#39118;&#38505;&#65292;&#22914;&#27844;&#38706;&#31169;&#20154;&#25968;&#25454;&#25110;&#24341;&#21457;&#36130;&#21153;&#25439;&#22833;&#12290;&#35782;&#21035;&#36825;&#20123;&#39118;&#38505;&#26159;&#19968;&#39033;&#32791;&#26102;&#30340;&#24037;&#20316;&#65292;&#38656;&#35201;&#23454;&#26045;&#24037;&#20855;&#65292;&#25163;&#21160;&#35774;&#32622;&#27599;&#20010;&#27979;&#35797;&#22330;&#26223;&#30340;&#29615;&#22659;&#65292;&#24182;&#25214;&#21040;&#39118;&#38505;&#26696;&#20363;&#12290;&#38543;&#30528;&#24037;&#20855;&#21644;&#20195;&#29702;&#21464;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#65292;&#27979;&#35797;&#36825;&#20123;&#20195;&#29702;&#30340;&#39640;&#25104;&#26412;&#23558;&#20351;&#23547;&#25214;&#39640;&#39118;&#38505;&#12289;&#38271;&#23614;&#39118;&#38505;&#21464;&#24471;&#36234;&#26469;&#36234;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;ToolEmu&#65306;&#19968;&#20010;&#20351;&#29992;LM&#26469;&#27169;&#25311;&#24037;&#20855;&#25191;&#34892;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#25163;&#21160;&#23454;&#20363;&#21270;&#30340;&#24773;&#20917;&#19979;&#23545;LM&#20195;&#29702;&#36827;&#34892;&#21508;&#31181;&#24037;&#20855;&#21644;&#22330;&#26223;&#30340;&#27979;&#35797;&#12290;&#38500;&#20102;&#27169;&#25311;&#22120;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;LM&#30340;&#33258;&#21160;&#23433;&#20840;&#35780;&#20272;&#22120;&#65292;&#29992;&#20110;&#26816;&#26597;&#20195;&#29702;&#30340;&#22833;&#36133;&#24182;&#37327;&#21270;&#30456;&#20851;&#39118;&#38505;&#12290;&#25105;&#20204;&#36890;&#36807;&#20154;&#24037;&#35780;&#20272;&#27979;&#35797;&#20102;&#24037;&#20855;&#27169;&#25311;&#22120;&#21644;&#35780;&#20272;&#22120;&#65292;&#24182;&#21457;&#29616;&#20102;6&#20010;...
&lt;/p&gt;
&lt;p&gt;
Recent advances in Language Model (LM) agents and tool use, exemplified by applications like ChatGPT Plugins, enable a rich set of capabilities but also amplify potential risks - such as leaking private data or causing financial losses. Identifying these risks is labor-intensive, necessitating implementing the tools, manually setting up the environment for each test scenario, and finding risky cases. As tools and agents become more complex, the high cost of testing these agents will make it increasingly difficult to find high-stakes, long-tailed risks. To address these challenges, we introduce ToolEmu: a framework that uses an LM to emulate tool execution and enables the testing of LM agents against a diverse range of tools and scenarios, without manual instantiation. Alongside the emulator, we develop an LM-based automatic safety evaluator that examines agent failures and quantifies associated risks. We test both the tool emulator and evaluator through human evaluation and find that 6
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#20174;&#30495;&#23454;&#29992;&#25143;&#37027;&#37324;&#33719;&#21462;&#39640;&#36136;&#37327;&#30340;&#31532;&#19968;&#26041;&#25968;&#25454;&#26469;&#20934;&#30830;&#39044;&#27979;&#25628;&#32034;&#32773;&#30340;&#20559;&#22909;&#12290;</title><link>http://arxiv.org/abs/2309.10621</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#25628;&#32034;&#32773;&#30340;&#20559;&#22909;
&lt;/p&gt;
&lt;p&gt;
Large language models can accurately predict searcher preferences. (arXiv:2309.10621v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10621
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#20174;&#30495;&#23454;&#29992;&#25143;&#37027;&#37324;&#33719;&#21462;&#39640;&#36136;&#37327;&#30340;&#31532;&#19968;&#26041;&#25968;&#25454;&#26469;&#20934;&#30830;&#39044;&#27979;&#25628;&#32034;&#32773;&#30340;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#20851;&#24615;&#26631;&#31614;&#26159;&#35780;&#20272;&#21644;&#20248;&#21270;&#25628;&#32034;&#31995;&#32479;&#30340;&#20851;&#38190;&#12290;&#33719;&#21462;&#22823;&#37327;&#30456;&#20851;&#24615;&#26631;&#31614;&#36890;&#24120;&#38656;&#35201;&#31532;&#19977;&#26041;&#26631;&#27880;&#20154;&#21592;&#65292;&#20294;&#23384;&#22312;&#20302;&#36136;&#37327;&#25968;&#25454;&#30340;&#39118;&#38505;&#12290;&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25913;&#36827;&#26631;&#31614;&#36136;&#37327;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#30495;&#23454;&#29992;&#25143;&#37027;&#37324;&#33719;&#24471;&#20180;&#32454;&#21453;&#39304;&#26469;&#33719;&#21462;&#39640;&#36136;&#37327;&#30340;&#31532;&#19968;&#26041;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relevance labels, which indicate whether a search result is valuable to a searcher, are key to evaluating and optimising search systems. The best way to capture the true preferences of users is to ask them for their careful feedback on which results would be useful, but this approach does not scale to produce a large number of labels. Getting relevance labels at scale is usually done with third-party labellers, who judge on behalf of the user, but there is a risk of low-quality data if the labeller doesn't understand user needs. To improve quality, one standard approach is to study real users through interviews, user studies and direct feedback, find areas where labels are systematically disagreeing with users, then educate labellers about user needs through judging guidelines, training and monitoring. This paper introduces an alternate approach for improving label quality. It takes careful feedback from real users, which by definition is the highest-quality first-party gold data that 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;SignRound&#30340;&#20248;&#21270;&#26435;&#37325;&#33293;&#20837;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26377;&#31526;&#21495;&#26799;&#24230;&#36827;&#34892;&#36731;&#37327;&#32423;&#20998;&#22359;&#35843;&#25972;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#37327;&#21270;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.05516</link><description>&lt;p&gt;
&#36890;&#36807;&#26377;&#31526;&#21495;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;LLMs&#37327;&#21270;&#20013;&#30340;&#26435;&#37325;&#33293;&#20837;
&lt;/p&gt;
&lt;p&gt;
Optimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs. (arXiv:2309.05516v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;SignRound&#30340;&#20248;&#21270;&#26435;&#37325;&#33293;&#20837;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26377;&#31526;&#21495;&#26799;&#24230;&#36827;&#34892;&#36731;&#37327;&#32423;&#20998;&#22359;&#35843;&#25972;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#37327;&#21270;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#25191;&#34892;&#35821;&#35328;&#30456;&#20851;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#24040;&#22823;&#30340;&#20869;&#23384;&#21644;&#23384;&#20648;&#38656;&#27714;&#65292;&#23427;&#20204;&#30340;&#37096;&#32626;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20165;&#38024;&#23545;&#26435;&#37325;&#30340;&#37327;&#21270;&#65292;&#29305;&#21035;&#26159;3&#20301;&#21644;4&#20301;&#20165;&#38024;&#23545;&#26435;&#37325;&#30340;&#37327;&#21270;&#65292;&#24050;&#32463;&#25104;&#20026;&#26368;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#20043;&#19968;&#12290;&#38543;&#30528;&#20301;&#25968;&#30340;&#20943;&#23569;&#65292;&#37327;&#21270;&#32593;&#26684;&#21464;&#24471;&#26356;&#21152;&#23485;&#27867;&#65292;&#20174;&#32780;&#24378;&#35843;&#20102;&#19978;&#19979;&#33293;&#20837;&#30340;&#37325;&#35201;&#24615;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#28155;&#21152;&#25200;&#21160;&#32454;&#35843;&#19978;&#19979;&#33293;&#20837;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#25105;&#20204;&#30340;&#30740;&#31350;&#21463;&#21046;&#20110;&#36825;&#20123;&#25200;&#21160;&#30340;&#31934;&#30830;&#19988;&#26377;&#38480;&#30340;&#36793;&#30028;&#65292;&#21482;&#26377;&#25913;&#21464;&#33293;&#20837;&#20540;&#30340;&#38408;&#20540;&#25165;&#20855;&#26377;&#37325;&#35201;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#27905;&#39640;&#25928;&#30340;&#20248;&#21270;&#26435;&#37325;&#33293;&#20837;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21517;&#20026;SignRound&#65292;&#23427;&#28041;&#21450;&#20351;&#29992;&#26377;&#31526;&#21495;&#26799;&#24230;&#30340;&#36731;&#37327;&#32423;&#20998;&#22359;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have proven their exceptional capabilities in performing language-related tasks. However, their deployment poses significant challenges due to their considerable memory and storage requirements. In response to this issue, weight-only quantization, particularly 3 and 4-bit weight-only quantization, has emerged as one of the most viable solutions. As the number of bits decreases, the quantization grid broadens, thus emphasizing the importance of up and down rounding. While previous studies have demonstrated that fine-tuning up and down rounding with the addition of perturbations can enhance accuracy in some scenarios, our study is driven by the precise and limited boundary of these perturbations, where only the threshold for altering the rounding value is of significance. Consequently, we propose a concise and highly effective approach for optimizing the weight rounding task. Our method, named SignRound, involves lightweight block-wise tuning using signed gra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#38543;&#26426;&#32593;&#32476;&#19978;&#36827;&#34892;&#22312;&#32447;&#20998;&#24067;&#24335;&#23398;&#20064;&#30340;DOT-ADMM&#31639;&#27861;&#65292;&#36890;&#36807;&#35299;&#20915;&#22312;&#32447;&#23398;&#20064;&#12289;&#24322;&#27493;&#35745;&#31639;&#12289;&#19981;&#21487;&#38752;&#36890;&#20449;&#21644;&#19981;&#31934;&#30830;&#35745;&#31639;&#31561;&#25361;&#25112;&#65292;&#22312;&#19968;&#22823;&#31867;&#20984;&#23398;&#20064;&#38382;&#39064;&#19978;&#33719;&#24471;&#20102;&#32447;&#24615;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.00520</link><description>&lt;p&gt;
&#22312;&#38543;&#26426;&#32593;&#32476;&#19978;&#30340;&#22312;&#32447;&#20998;&#24067;&#24335;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Online Distributed Learning over Random Networks. (arXiv:2309.00520v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00520
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#38543;&#26426;&#32593;&#32476;&#19978;&#36827;&#34892;&#22312;&#32447;&#20998;&#24067;&#24335;&#23398;&#20064;&#30340;DOT-ADMM&#31639;&#27861;&#65292;&#36890;&#36807;&#35299;&#20915;&#22312;&#32447;&#23398;&#20064;&#12289;&#24322;&#27493;&#35745;&#31639;&#12289;&#19981;&#21487;&#38752;&#36890;&#20449;&#21644;&#19981;&#31934;&#30830;&#35745;&#31639;&#31561;&#25361;&#25112;&#65292;&#22312;&#19968;&#22823;&#31867;&#20984;&#23398;&#20064;&#38382;&#39064;&#19978;&#33719;&#24471;&#20102;&#32447;&#24615;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;&#21508;&#31181;&#22330;&#26223;&#20013;&#37096;&#32626;&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20351;&#24471;&#22312;&#20998;&#24067;&#24335;&#29615;&#22659;&#19979;&#35299;&#20915;&#23398;&#20064;&#38382;&#39064;&#25104;&#20026;&#21487;&#33021;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#26234;&#33021;&#20307;&#30340;&#20219;&#21153;&#26159;&#25910;&#38598;&#26412;&#22320;&#25968;&#25454;&#65292;&#28982;&#21518;&#21512;&#20316;&#35757;&#32451;&#27169;&#22411;&#65292;&#32780;&#19981;&#30452;&#25509;&#20849;&#20139;&#25968;&#25454;&#12290;&#34429;&#28982;&#20998;&#24067;&#24335;&#23398;&#20064;&#22312;&#20445;&#25252;&#26234;&#33021;&#20307;&#38544;&#31169;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#65292;&#20294;&#22312;&#35774;&#35745;&#21644;&#20998;&#26512;&#36866;&#24403;&#30340;&#31639;&#27861;&#26041;&#38754;&#20063;&#23384;&#22312;&#19968;&#20123;&#25361;&#25112;&#12290;&#26412;&#25991;&#29305;&#21035;&#20851;&#27880;&#20197;&#19979;&#30001;&#23454;&#38469;&#23454;&#26045;&#25152;&#39537;&#21160;&#30340;&#25361;&#25112;&#65306;&#65288;i&#65289;&#22312;&#32447;&#23398;&#20064;&#65292;&#20854;&#20013;&#26412;&#22320;&#25968;&#25454;&#38543;&#26102;&#38388;&#21464;&#21270;&#65307;&#65288;ii&#65289;&#24322;&#27493;&#26234;&#33021;&#20307;&#35745;&#31639;&#65307;&#65288;iii&#65289;&#19981;&#21487;&#38752;&#21644;&#26377;&#38480;&#30340;&#36890;&#20449;&#65307;&#65288;iv&#65289;&#19981;&#31934;&#30830;&#30340;&#26412;&#22320;&#35745;&#31639;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20998;&#24067;&#24335;&#25805;&#20316;&#29702;&#35770;&#65288;DOT&#65289;&#29256;&#26412;&#30340;&#20132;&#26367;&#26041;&#21521;&#20056;&#23376;&#27861;&#65288;ADMM&#65289;&#65292;&#31216;&#20043;&#20026;DOT-ADMM&#31639;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23427;&#22312;&#22823;&#31867;&#20984;&#23398;&#20064;&#38382;&#39064;&#19978;&#20855;&#26377;&#32447;&#24615;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent deployment of multi-agent systems in a wide range of scenarios has enabled the solution of learning problems in a distributed fashion. In this context, agents are tasked with collecting local data and then cooperatively train a model, without directly sharing the data. While distributed learning offers the advantage of preserving agents' privacy, it also poses several challenges in terms of designing and analyzing suitable algorithms. This work focuses specifically on the following challenges motivated by practical implementation: (i) online learning, where the local data change over time; (ii) asynchronous agent computations; (iii) unreliable and limited communications; and (iv) inexact local computations. To tackle these challenges, we introduce the Distributed Operator Theoretical (DOT) version of the Alternating Direction Method of Multipliers (ADMM), which we call the DOT-ADMM Algorithm. We prove that it converges with a linear rate for a large class of convex learning 
&lt;/p&gt;</description></item><item><title>AnyTeleop&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#35270;&#35273;&#23548;&#21521;&#30340;&#36828;&#31243;&#25805;&#20316;&#31995;&#32479;&#65292;&#25903;&#25345;&#22810;&#20010;&#19981;&#21516;&#30340;&#26426;&#26800;&#33218;&#12289;&#25163;&#37096;&#12289;&#29615;&#22659;&#21644;&#25668;&#20687;&#22836;&#37197;&#32622;&#65292;&#22312;&#23454;&#38469;&#23454;&#39564;&#21644;&#27169;&#25311;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2307.04577</link><description>&lt;p&gt;
AnyTeleop: &#36890;&#29992;&#35270;&#35273;&#23548;&#21521;&#30340;&#28789;&#24039;&#26426;&#26800;&#33218;&#25163;&#25805;&#20316;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
AnyTeleop: A General Vision-Based Dexterous Robot Arm-Hand Teleoperation System. (arXiv:2307.04577v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04577
&lt;/p&gt;
&lt;p&gt;
AnyTeleop&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#35270;&#35273;&#23548;&#21521;&#30340;&#36828;&#31243;&#25805;&#20316;&#31995;&#32479;&#65292;&#25903;&#25345;&#22810;&#20010;&#19981;&#21516;&#30340;&#26426;&#26800;&#33218;&#12289;&#25163;&#37096;&#12289;&#29615;&#22659;&#21644;&#25668;&#20687;&#22836;&#37197;&#32622;&#65292;&#22312;&#23454;&#38469;&#23454;&#39564;&#21644;&#27169;&#25311;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#35270;&#35273;&#30340;&#36828;&#31243;&#25805;&#20316;&#20026;&#26426;&#22120;&#20154;&#25552;&#20379;&#20102;&#19982;&#29615;&#22659;&#36827;&#34892;&#29289;&#29702;&#20132;&#20114;&#30340;&#21487;&#33021;&#24615;&#65292;&#21516;&#26102;&#21482;&#38656;&#35201;&#20302;&#25104;&#26412;&#30340;&#25668;&#20687;&#22836;&#20256;&#24863;&#22120;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#22522;&#20110;&#35270;&#35273;&#30340;&#36828;&#31243;&#25805;&#20316;&#31995;&#32479;&#26159;&#38024;&#23545;&#29305;&#23450;&#26426;&#22120;&#20154;&#27169;&#22411;&#21644;&#37096;&#32626;&#29615;&#22659;&#36827;&#34892;&#35774;&#35745;&#21644;&#24037;&#31243;&#21270;&#30340;&#65292;&#38543;&#30528;&#26426;&#22120;&#20154;&#27169;&#22411;&#30340;&#22686;&#21152;&#21644;&#25805;&#20316;&#29615;&#22659;&#30340;&#22810;&#26679;&#21270;&#65292;&#20854;&#25193;&#23637;&#24615;&#36739;&#24046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;AnyTeleop&#65292;&#19968;&#20010;&#32479;&#19968;&#21644;&#36890;&#29992;&#30340;&#36828;&#31243;&#25805;&#20316;&#31995;&#32479;&#65292;&#25903;&#25345;&#22312;&#21333;&#20010;&#31995;&#32479;&#20013;&#20351;&#29992;&#22810;&#20010;&#19981;&#21516;&#30340;&#26426;&#26800;&#33218;&#12289;&#25163;&#37096;&#12289;&#29616;&#23454;&#29615;&#22659;&#21644;&#25668;&#20687;&#22836;&#37197;&#32622;&#12290;&#23613;&#31649;&#35774;&#35745;&#20855;&#26377;&#36873;&#25321;&#27169;&#25311;&#22120;&#21644;&#30495;&#23454;&#30828;&#20214;&#30340;&#28789;&#27963;&#24615;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#20173;&#28982;&#21487;&#20197;&#23454;&#29616;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#22312;&#23454;&#38469;&#23454;&#39564;&#20013;&#65292;AnyTeleop&#21487;&#20197;&#20197;&#26356;&#39640;&#30340;&#25104;&#21151;&#29575;&#20987;&#36133;&#20026;&#29305;&#23450;&#26426;&#22120;&#20154;&#30828;&#20214;&#35774;&#35745;&#30340;&#20197;&#21069;&#31995;&#32479;&#65292;&#20351;&#29992;&#30456;&#21516;&#30340;&#26426;&#22120;&#20154;&#12290;&#22312;&#27169;&#25311;&#20013;&#36827;&#34892;&#36828;&#31243;&#25805;&#20316;&#26102;&#65292;AnyTeleop&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-based teleoperation offers the possibility to endow robots with human-level intelligence to physically interact with the environment, while only requiring low-cost camera sensors. However, current vision-based teleoperation systems are designed and engineered towards a particular robot model and deploy environment, which scales poorly as the pool of the robot models expands and the variety of the operating environment increases. In this paper, we propose AnyTeleop, a unified and general teleoperation system to support multiple different arms, hands, realities, and camera configurations within a single system. Although being designed to provide great flexibility to the choice of simulators and real hardware, our system can still achieve great performance. For real-world experiments, AnyTeleop can outperform a previous system that was designed for a specific robot hardware with a higher success rate, using the same robot. For teleoperation in simulation, AnyTeleop leads to better 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#24212;&#29992;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#20174;&#26377;&#38480;&#30340;&#25968;&#25454;&#26679;&#26412;&#20013;&#23398;&#20064;&#22522;&#22240;&#34920;&#36798;&#25968;&#25454;&#30340;&#33391;&#22909;&#29305;&#24449;&#34920;&#31034;&#65292;&#24182;&#24212;&#29992;&#20110;Cox&#27169;&#22411;&#20013;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#30284;&#30151;&#39044;&#21518;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.06276</link><description>&lt;p&gt;
&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#39044;&#27979;&#30284;&#30151;&#39044;&#21518;&#30340;&#22522;&#22240;&#34920;&#36798;&#20540;
&lt;/p&gt;
&lt;p&gt;
Contrastive Learning for Predicting Cancer Prognosis Using Gene Expression Values. (arXiv:2306.06276v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06276
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#24212;&#29992;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#20174;&#26377;&#38480;&#30340;&#25968;&#25454;&#26679;&#26412;&#20013;&#23398;&#20064;&#22522;&#22240;&#34920;&#36798;&#25968;&#25454;&#30340;&#33391;&#22909;&#29305;&#24449;&#34920;&#31034;&#65292;&#24182;&#24212;&#29992;&#20110;Cox&#27169;&#22411;&#20013;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#30284;&#30151;&#39044;&#21518;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#24320;&#21457;&#20102;&#22810;&#31181;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANNs&#65289;&#20316;&#20026;Cox&#27604;&#20363;&#39118;&#38505;&#27169;&#22411;&#65292;&#20197;&#22522;&#20110;&#32959;&#30244;&#36716;&#24405;&#32452;&#39044;&#27979;&#30284;&#30151;&#39044;&#21518;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24182;&#27809;&#26377;&#34920;&#29616;&#20986;&#27604;&#20256;&#32479;&#26377;&#35268;&#21017;&#21270;&#30340;Cox&#22238;&#24402;&#26174;&#30528;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#22312;&#26377;&#38480;&#25968;&#37327;&#30340;&#25968;&#25454;&#26679;&#26412;&#21644;&#39640;&#32500;&#29305;&#24449;&#31354;&#38388;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#35757;&#32451;&#20855;&#26377;&#39640;&#39044;&#27979;&#33021;&#21147;&#30340;ANN&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22270;&#20687;&#20998;&#31867;&#30340;&#26368;&#26032;&#36827;&#23637;&#34920;&#26126;&#65292;&#23545;&#27604;&#23398;&#20064;&#21487;&#20197;&#36890;&#36807;&#20174;&#26377;&#38480;&#25968;&#37327;&#30340;&#25968;&#25454;&#26679;&#26412;&#20013;&#23398;&#20064;&#33391;&#22909;&#30340;&#29305;&#24449;&#34920;&#31034;&#26469;&#20419;&#36827;&#36827;&#19968;&#27493;&#30340;&#23398;&#20064;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#24212;&#29992;&#20110;&#32959;&#30244;&#22522;&#22240;&#34920;&#36798;&#21644;&#20020;&#24202;&#25968;&#25454;&#65292;&#20197;&#22312;&#20302;&#32500;&#31354;&#38388;&#20013;&#23398;&#20064;&#29305;&#24449;&#34920;&#31034;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#23398;&#20064;&#21040;&#30340;&#29305;&#24449;&#26469;&#35757;&#32451;Cox&#27169;&#22411;&#65292;&#20197;&#39044;&#27979;&#30284;&#30151;&#39044;&#21518;&#12290;&#20351;&#29992;&#26469;&#33258;The Cancer Genome Atlas&#65288;TCGA&#65289;&#30340;&#25968;&#25454;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;Cox&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#23398;&#20064;&#22522;&#22240;&#34920;&#36798;&#25968;&#25454;&#30340;&#33391;&#22909;&#29305;&#24449;&#34920;&#24449;&#65292;&#26174;&#33879;&#25552;&#39640;&#30284;&#30151;&#39044;&#21518;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several artificial neural networks (ANNs) have recently been developed as the Cox proportional hazard model for predicting cancer prognosis based on tumor transcriptome. However, they have not demonstrated significantly better performance than the traditional Cox regression with regularization. Training an ANN with high prediction power is challenging in the presence of a limited number of data samples and a high-dimensional feature space. Recent advancements in image classification have shown that contrastive learning can facilitate further learning tasks by learning good feature representation from a limited number of data samples. In this paper, we applied supervised contrastive learning to tumor gene expression and clinical data to learn feature representations in a low-dimensional space. We then used these learned features to train the Cox model for predicting cancer prognosis. Using data from The Cancer Genome Atlas (TCGA), we demonstrated that our contrastive learning-based Cox 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;C(NN)FD&#65292;&#29992;&#20110;&#23454;&#26102;&#39044;&#27979;&#29123;&#27668;&#36718;&#26426;&#20013;&#36724;&#21521;&#21387;&#32553;&#26426;&#21046;&#36896;&#21644;&#32452;&#35013;&#21464;&#21270;&#23545;&#25972;&#20307;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#35813;&#26694;&#26550;&#21487;&#36807;&#28388;&#25481;CFD&#35299;&#20915;&#26041;&#26696;&#20013;&#30340;&#30456;&#20851;&#37096;&#20998;&#65292;&#22240;&#27492;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#65292;&#19988;&#23454;&#26102;&#31934;&#24230;&#21487;&#19982;CFD&#22522;&#20934;&#30456;&#23218;&#32654;&#12290;</title><link>http://arxiv.org/abs/2306.05889</link><description>&lt;p&gt;
C(NN)FD - &#19968;&#31181;&#29992;&#20110;&#28065;&#36718;&#26426;&#26800;CFD&#20998;&#26512;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
C(NN)FD -- a deep learning framework for turbomachinery CFD analysis. (arXiv:2306.05889v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05889
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;C(NN)FD&#65292;&#29992;&#20110;&#23454;&#26102;&#39044;&#27979;&#29123;&#27668;&#36718;&#26426;&#20013;&#36724;&#21521;&#21387;&#32553;&#26426;&#21046;&#36896;&#21644;&#32452;&#35013;&#21464;&#21270;&#23545;&#25972;&#20307;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#35813;&#26694;&#26550;&#21487;&#36807;&#28388;&#25481;CFD&#35299;&#20915;&#26041;&#26696;&#20013;&#30340;&#30456;&#20851;&#37096;&#20998;&#65292;&#22240;&#27492;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#65292;&#19988;&#23454;&#26102;&#31934;&#24230;&#21487;&#19982;CFD&#22522;&#20934;&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#24050;&#32463;&#22312;&#19981;&#21516;&#24037;&#19994;&#39046;&#22495;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#23545;&#20110;&#35832;&#22914;CFD&#65288;&#35745;&#31639;&#27969;&#20307;&#21147;&#23398;&#65289;&#30340;&#29289;&#29702;&#27169;&#25311;&#30340;&#24212;&#29992;&#20165;&#38480;&#20110;&#24037;&#19994;&#30456;&#20851;&#24615;&#36739;&#23567;&#30340;&#31616;&#21333;&#27979;&#35797;&#26696;&#20363;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#31181;&#26032;&#22411;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#30340;&#24320;&#21457;&#65292;&#29992;&#20110;&#23454;&#26102;&#39044;&#27979;&#29123;&#27668;&#36718;&#26426;&#20013;&#36724;&#21521;&#21387;&#32553;&#26426;&#21046;&#36896;&#21644;&#32452;&#35013;&#21464;&#21270;&#23545;&#25972;&#20307;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#37325;&#28857;&#20851;&#27880;&#20110;&#21494;&#29255;&#38388;&#38553;&#30340;&#21464;&#21270;&#12290;&#25928;&#29575;&#30340;&#25955;&#24067;&#21487;&#20197;&#26174;&#33879;&#22686;&#21152;$CO_2$&#25490;&#25918;&#37327;&#65292;&#22240;&#27492;&#20855;&#26377;&#37325;&#35201;&#30340;&#24037;&#19994;&#21644;&#29615;&#22659;&#24847;&#20041;&#12290;&#25152;&#25552;&#20986;&#30340;C(NN)FD&#26550;&#26500;&#23454;&#26102;&#31934;&#24230;&#21487;&#19982;CFD&#22522;&#20934;&#30456;&#23218;&#32654;&#12290;&#39044;&#27979;&#27969;&#22330;&#24182;&#20351;&#29992;&#20854;&#35745;&#31639;&#30456;&#24212;&#30340;&#25972;&#20307;&#24615;&#33021;&#20351;&#35813;&#26041;&#27861;&#20855;&#26377;&#26222;&#36866;&#24615;&#65292;&#32780;&#20165;&#36807;&#28388;CFD&#35299;&#20915;&#26041;&#26696;&#30340;&#30456;&#20851;&#37096;&#20998;&#20351;&#35813;&#26041;&#27861;&#21487;&#25193;&#23637;&#21040;&#23454;&#38469;&#24212;&#29992;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Learning methods have seen a wide range of successful applications across different industries. Up until now, applications to physical simulations such as CFD (Computational Fluid Dynamics), have been limited to simple test-cases of minor industrial relevance. This paper demonstrates the development of a novel deep learning framework for real-time predictions of the impact of manufacturing and build variations on the overall performance of axial compressors in gas turbines, with a focus on tip clearance variations. The associated scatter in efficiency can significantly increase the $CO_2$ emissions, thus being of great industrial and environmental relevance. The proposed \textit{C(NN)FD} architecture achieves in real-time accuracy comparable to the CFD benchmark. Predicting the flow field and using it to calculate the corresponding overall performance renders the methodology generalisable, while filtering only relevant parts of the CFD solution makes the methodology scalable to in
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;ERM&#35757;&#32451;&#27169;&#22411;&#23545;&#25239;&#24378;&#22823;&#40657;&#30418;&#25915;&#20987;&#30340;&#23433;&#20840;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20004;&#20010;&#25351;&#26631;&#37327;&#21270;&#27169;&#22411;&#23433;&#20840;&#24615;&#65306;&#21333;&#20010;&#26679;&#26412;&#30340;&#31283;&#23450;&#24615;&#21644;&#26597;&#35810;&#19982;&#21407;&#22987;&#25968;&#25454;&#29305;&#24449;&#30340;&#23545;&#40784;&#12290;&#22312;&#30740;&#31350;&#20013;&#65292;&#36890;&#36807;&#30740;&#31350;RF&#21644;NTK&#22238;&#24402;&#65292;&#35777;&#26126;&#38543;&#30528;&#27867;&#21270;&#33021;&#21147;&#30340;&#25552;&#39640;&#65292;&#38544;&#31169;&#20445;&#25252;&#21487;&#20197;&#24471;&#21040;&#21152;&#24378;&#12290;</title><link>http://arxiv.org/abs/2305.12100</link><description>&lt;p&gt;
&#31283;&#23450;&#24615;&#12289;&#27867;&#21270;&#24615;&#21644;&#38544;&#31169;&#20445;&#25252;&#65306;&#23545;&#20110;&#38543;&#26426;&#29305;&#24449;&#21644;NTK&#29305;&#24449;&#30340;&#31934;&#30830;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Stability, Generalization and Privacy: Precise Analysis for Random and NTK Features. (arXiv:2305.12100v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12100
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;ERM&#35757;&#32451;&#27169;&#22411;&#23545;&#25239;&#24378;&#22823;&#40657;&#30418;&#25915;&#20987;&#30340;&#23433;&#20840;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20004;&#20010;&#25351;&#26631;&#37327;&#21270;&#27169;&#22411;&#23433;&#20840;&#24615;&#65306;&#21333;&#20010;&#26679;&#26412;&#30340;&#31283;&#23450;&#24615;&#21644;&#26597;&#35810;&#19982;&#21407;&#22987;&#25968;&#25454;&#29305;&#24449;&#30340;&#23545;&#40784;&#12290;&#22312;&#30740;&#31350;&#20013;&#65292;&#36890;&#36807;&#30740;&#31350;RF&#21644;NTK&#22238;&#24402;&#65292;&#35777;&#26126;&#38543;&#30528;&#27867;&#21270;&#33021;&#21147;&#30340;&#25552;&#39640;&#65292;&#38544;&#31169;&#20445;&#25252;&#21487;&#20197;&#24471;&#21040;&#21152;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#24674;&#22797;&#25915;&#20987;&#65292;&#24341;&#36215;&#29992;&#25143;&#38544;&#31169;&#20445;&#25252;&#30340;&#25285;&#24551;&#12290;&#38024;&#23545;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;ERM&#65289;&#31561;&#24120;&#35265;&#31639;&#27861;&#36890;&#24120;&#19981;&#33021;&#30452;&#25509;&#23454;&#26045;&#23433;&#20840;&#20445;&#38556;&#30340;&#38382;&#39064;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;ERM&#35757;&#32451;&#27169;&#22411;&#23545;&#25239;&#29305;&#23450;&#24378;&#22823;&#40657;&#30418;&#23376;&#25915;&#20987;&#30340;&#23433;&#20840;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#36890;&#36807;&#20004;&#20010;&#30475;&#20284;&#19981;&#21516;&#20294;&#26377;&#32852;&#31995;&#30340;&#25351;&#26631;&#26469;&#37327;&#21270;&#27169;&#22411;&#23433;&#20840;&#24615;&#65306;&#19968;&#26159;&#30456;&#23545;&#20110;&#21333;&#20010;&#35757;&#32451;&#26679;&#26412;&#30340;&#27169;&#22411;&#31283;&#23450;&#24615;&#65292;&#21478;&#19968;&#20010;&#26159;&#25915;&#20987;&#26597;&#35810;&#21644;&#21407;&#22987;&#25968;&#25454;&#29305;&#24449;&#30340;&#29305;&#24449;&#23545;&#40784;&#12290;&#34429;&#28982;&#21069;&#32773;&#22312;&#23398;&#20064;&#29702;&#35770;&#20013;&#24050;&#32463;&#24471;&#21040;&#20102;&#24456;&#22909;&#30340;&#38416;&#36848;&#65292;&#24182;&#19982;&#32463;&#20856;&#24037;&#20316;&#20013;&#30340;&#27867;&#21270;&#35823;&#24046;&#30456;&#20851;&#65292;&#20294;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#31532;&#20108;&#31181;&#29305;&#24615;&#26159;&#26032;&#39062;&#30340;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#25216;&#26415;&#32467;&#26524;&#20026;&#20004;&#31181;&#21407;&#22411;&#35774;&#32622;&#25552;&#20379;&#20102;&#29305;&#24449;&#23545;&#40784;&#30340;&#31934;&#30830;&#21051;&#30011;&#65306;&#38543;&#26426;&#29305;&#24449;&#65288;RF&#65289;&#21644;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;NTK&#65289;&#22238;&#24402;&#12290;&#36825;&#35777;&#26126;&#65292;&#38543;&#30528;&#27867;&#21270;&#33021;&#21147;&#30340;&#25552;&#39640;&#65292;&#38544;&#31169;&#20445;&#25252;&#33021;&#22815;&#24471;&#21040;&#21152;&#24378;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#20854;&#20182;&#26377;&#36259;&#30340;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning models can be vulnerable to recovery attacks, raising privacy concerns to users, and widespread algorithms such as empirical risk minimization (ERM) often do not directly enforce safety guarantees. In this paper, we study the safety of ERM-trained models against a family of powerful black-box attacks. Our analysis quantifies this safety via two separate terms: (i) the model stability with respect to individual training samples, and (ii) the feature alignment between the attacker query and the original data. While the first term is well established in learning theory and it is connected to the generalization error in classical work, the second one is, to the best of our knowledge, novel. Our key technical result provides a precise characterization of the feature alignment for the two prototypical settings of random features (RF) and neural tangent kernel (NTK) regression. This proves that privacy strengthens with an increase in the generalization capability, unveiling also
&lt;/p&gt;</description></item><item><title>ACRoBat&#26159;&#19968;&#31181;&#21160;&#24577;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#21160;&#25209;&#22788;&#29702;&#26694;&#26550;&#65292;&#22312;&#32534;&#35793;&#26102;&#36827;&#34892;&#28151;&#21512;&#38745;&#24577;+&#21160;&#24577;&#32534;&#35793;&#22120;&#20248;&#21270;&#21644;&#31471;&#21040;&#31471;&#24352;&#37327;&#20195;&#30721;&#29983;&#25104;&#65292;&#21487;&#23558;&#24615;&#33021;&#25552;&#39640;&#22810;&#36798;8.5&#20493;&#12290;</title><link>http://arxiv.org/abs/2305.10611</link><description>&lt;p&gt;
ACRoBat&#65306;&#22312;&#32534;&#35793;&#26102;&#20248;&#21270;&#21160;&#24577;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#21160;&#25209;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
ACRoBat: Optimizing Auto-batching of Dynamic Deep Learning at Compile Time. (arXiv:2305.10611v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10611
&lt;/p&gt;
&lt;p&gt;
ACRoBat&#26159;&#19968;&#31181;&#21160;&#24577;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#21160;&#25209;&#22788;&#29702;&#26694;&#26550;&#65292;&#22312;&#32534;&#35793;&#26102;&#36827;&#34892;&#28151;&#21512;&#38745;&#24577;+&#21160;&#24577;&#32534;&#35793;&#22120;&#20248;&#21270;&#21644;&#31471;&#21040;&#31471;&#24352;&#37327;&#20195;&#30721;&#29983;&#25104;&#65292;&#21487;&#23558;&#24615;&#33021;&#25552;&#39640;&#22810;&#36798;8.5&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#25511;&#21046;&#27969;&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#25216;&#26415;&#65292;&#24120;&#29992;&#20110;&#35774;&#35745;&#34920;&#36798;&#21147;&#24378;&#21644;&#39640;&#25928;&#30340;&#28145;&#24230;&#23398;&#20064;&#35745;&#31639;&#65292;&#20363;&#22914;&#25991;&#26412;&#35299;&#26512;&#12289;&#26426;&#22120;&#32763;&#35793;&#12289;&#25552;&#21069;&#36864;&#20986;&#28145;&#24230;&#27169;&#22411;&#31561;&#12290;&#28982;&#32780;&#65292;&#30001;&#27492;&#20135;&#29983;&#30340;&#25511;&#21046;&#27969;&#20998;&#21449;&#20351;&#24471;&#25209;&#22788;&#29702;&#38590;&#20197;&#25163;&#21160;&#25191;&#34892;&#65292;&#36825;&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#24615;&#33021;&#20248;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102; ACRoBat &#26694;&#26550;&#65292;&#36890;&#36807;&#25191;&#34892;&#28151;&#21512;&#38745;&#24577;+&#21160;&#24577;&#32534;&#35793;&#22120;&#20248;&#21270;&#21644;&#31471;&#21040;&#31471;&#24352;&#37327;&#20195;&#30721;&#29983;&#25104;&#65292;&#23454;&#29616;&#20102;&#21160;&#24577;&#28145;&#24230;&#23398;&#20064;&#35745;&#31639;&#30340;&#39640;&#25928;&#33258;&#21160;&#25209;&#22788;&#29702;&#12290;&#22312; NVIDIA GeForce RTX 3070 GPU &#19978;&#65292;ACRoBat &#30340;&#24615;&#33021;&#27604;&#29616;&#26377;&#30340;&#33258;&#21160;&#25209;&#22788;&#29702;&#26694;&#26550; DyNet &#25552;&#39640;&#20102;&#22810;&#36798; 8.5 &#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamic control flow is an important technique often used to design expressive and efficient deep learning computations for applications such as text parsing, machine translation, exiting early out of deep models and so on. However, the resulting control flow divergence makes batching, an important performance optimization, difficult to perform manually. In this paper, we present ACRoBat, a framework that enables efficient automatic batching for dynamic deep learning computations by performing hybrid static+dynamic compiler optimizations and end-to-end tensor code generation. ACRoBat performs up to 8.5X better than DyNet, a state-of-the-art framework for automatic batching, on an Nvidia GeForce RTX 3070 GPU.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Grad-Align+&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#24378;&#33410;&#28857;&#29305;&#24449;&#26469;&#25191;&#34892;NA&#20219;&#21153;&#65292;&#24182;&#26368;&#22823;&#38480;&#24230;&#22320;&#21033;&#29992;&#22686;&#24378;&#30340;&#33410;&#28857;&#29305;&#24449;&#26469;&#35774;&#35745;NA&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;NA&#26041;&#27861;&#32570;&#20047;&#39069;&#22806;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.12751</link><description>&lt;p&gt;
&#33410;&#28857;&#29305;&#24449;&#22686;&#24378;&#25913;&#36827;&#32593;&#32476;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Node Feature Augmentation Vitaminizes Network Alignment. (arXiv:2304.12751v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Grad-Align+&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#24378;&#33410;&#28857;&#29305;&#24449;&#26469;&#25191;&#34892;NA&#20219;&#21153;&#65292;&#24182;&#26368;&#22823;&#38480;&#24230;&#22320;&#21033;&#29992;&#22686;&#24378;&#30340;&#33410;&#28857;&#29305;&#24449;&#26469;&#35774;&#35745;NA&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;NA&#26041;&#27861;&#32570;&#20047;&#39069;&#22806;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#23545;&#40784;&#65288;NA&#65289;&#26159;&#36890;&#36807;&#32473;&#23450;&#32593;&#32476;&#30340;&#25299;&#25169;&#21644;/&#25110;&#29305;&#24449;&#20449;&#24687;&#26469;&#21457;&#29616;&#22810;&#20010;&#32593;&#32476;&#20043;&#38388;&#30340;&#33410;&#28857;&#23545;&#24212;&#20851;&#31995;&#30340;&#20219;&#21153;&#12290;&#34429;&#28982;NA&#26041;&#27861;&#22312;&#21508;&#31181;&#22330;&#26223;&#19979;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#20854;&#26377;&#25928;&#24615;&#24182;&#19981;&#24635;&#26159;&#26377;&#39069;&#22806;&#20449;&#24687;&#65292;&#22914;&#20808;&#21069;&#30340;&#38170;&#28857;&#38142;&#25509;&#21644;/&#25110;&#33410;&#28857;&#29305;&#24449;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#23454;&#38469;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Grad-Align+&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;NA&#26041;&#27861;&#65292;&#24314;&#31435;&#22312;&#26368;&#36817;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;NA&#26041;&#27861;Grad-Align&#20043;&#19978;&#65292;Grad-Align+&#20165;&#36880;&#27493;&#21457;&#29616;&#37096;&#20998;&#33410;&#28857;&#23545;&#65292;&#30452;&#21040;&#25214;&#21040;&#25152;&#26377;&#33410;&#28857;&#23545;&#12290;&#22312;&#35774;&#35745;Grad-Align+&#26102;&#65292;&#25105;&#20204;&#32771;&#34385;&#22914;&#20309;&#36890;&#36807;&#22686;&#24378;&#33410;&#28857;&#29305;&#24449;&#26469;&#25191;&#34892;NA&#20219;&#21153;&#65292;&#24182;&#26368;&#22823;&#38480;&#24230;&#22320;&#21033;&#29992;&#22686;&#24378;&#30340;&#33410;&#28857;&#29305;&#24449;&#26469;&#35774;&#35745;NA&#26041;&#27861;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#30001;&#19977;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#32452;&#25104;&#30340;Grad-Align+&#65306;&#22522;&#20110;&#20013;&#24515;&#24615;&#30340;&#33410;&#28857;&#29305;&#24449;&#22686;&#24378;&#65288;CNFA&#65289;&#12289;&#22270;&#20999;&#29255;&#29983;&#25104;&#21644;&#20248;&#21270;&#33410;&#28857;&#23884;&#20837;&#29305;&#24449;&#65288;ONIFE&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Network alignment (NA) is the task of discovering node correspondences across multiple networks using topological and/or feature information of given networks. Although NA methods have achieved remarkable success in a myriad of scenarios, their effectiveness is not without additional information such as prior anchor links and/or node features, which may not always be available due to privacy concerns or access restrictions. To tackle this practical challenge, we propose Grad-Align+, a novel NA method built upon a recent state-of-the-art NA method, the so-called Grad-Align, that gradually discovers only a part of node pairs until all node pairs are found. In designing Grad-Align+, we account for how to augment node features in the sense of performing the NA task and how to design our NA method by maximally exploiting the augmented node features. To achieve this goal, we develop Grad-Align+ consisting of three key components: 1) centrality-based node feature augmentation (CNFA), 2) graph
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#32479;&#19968;&#24754;&#35266;&#39118;&#38505;&#30340;&#32508;&#21512;$\alpha$-&#39118;&#38505;&#29256;&#26412;&#21644;&#22522;&#20110;&#39118;&#38505;&#33719;&#24471;&#26368;&#20248;&#32452;&#21512;&#30340;&#35745;&#31639;&#31639;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#20272;&#35745;&#38889;&#22269;&#32929;&#31080;&#30340;&#24754;&#35266;&#26368;&#20248;&#32452;&#21512;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.07158</link><description>&lt;p&gt;
&#32479;&#19968;&#24754;&#35266;&#39118;&#38505;&#21644;&#26368;&#20248;&#32452;&#21512;
&lt;/p&gt;
&lt;p&gt;
Uniform Pessimistic Risk and Optimal Portfolio. (arXiv:2303.07158v1 [q-fin.PM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#32479;&#19968;&#24754;&#35266;&#39118;&#38505;&#30340;&#32508;&#21512;$\alpha$-&#39118;&#38505;&#29256;&#26412;&#21644;&#22522;&#20110;&#39118;&#38505;&#33719;&#24471;&#26368;&#20248;&#32452;&#21512;&#30340;&#35745;&#31639;&#31639;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#20272;&#35745;&#38889;&#22269;&#32929;&#31080;&#30340;&#24754;&#35266;&#26368;&#20248;&#32452;&#21512;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a version of integrated $\alpha$-risk called the uniform pessimistic risk and a computational algorithm to obtain an optimal portfolio based on the risk. The proposed method can be used to estimate the pessimistic optimal portfolio models for Korean stocks.
&lt;/p&gt;
&lt;p&gt;
&#36164;&#20135;&#37197;&#32622;&#30340;&#26368;&#20248;&#24615;&#24050;&#32463;&#22312;&#39118;&#38505;&#24230;&#37327;&#30340;&#29702;&#35770;&#20998;&#26512;&#20013;&#24471;&#21040;&#24191;&#27867;&#35752;&#35770;&#12290;&#24754;&#35266;&#20027;&#20041;&#26159;&#19968;&#31181;&#36229;&#36234;&#20256;&#32479;&#26368;&#20248;&#32452;&#21512;&#27169;&#22411;&#30340;&#26368;&#26377;&#21560;&#24341;&#21147;&#30340;&#26041;&#27861;&#20043;&#19968;&#65292;$\alpha$-&#39118;&#38505;&#22312;&#25512;&#23548;&#20986;&#24191;&#27867;&#30340;&#24754;&#35266;&#26368;&#20248;&#32452;&#21512;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#30001;&#24754;&#35266;&#39118;&#38505;&#35780;&#20272;&#30340;&#26368;&#20248;&#32452;&#21512;&#30340;&#20272;&#35745;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#32570;&#20047;&#21487;&#29992;&#30340;&#20272;&#35745;&#27169;&#22411;&#21644;&#35745;&#31639;&#31639;&#27861;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#32479;&#19968;&#24754;&#35266;&#39118;&#38505;&#30340;&#32508;&#21512;$\alpha$-&#39118;&#38505;&#29256;&#26412;&#21644;&#22522;&#20110;&#39118;&#38505;&#33719;&#24471;&#26368;&#20248;&#32452;&#21512;&#30340;&#35745;&#31639;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20174;&#22810;&#20010;&#20998;&#20301;&#25968;&#22238;&#24402;&#12289;&#36866;&#24403;&#30340;&#35780;&#20998;&#35268;&#21017;&#21644;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#19977;&#20010;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#30740;&#31350;&#25152;&#25552;&#20986;&#30340;&#39118;&#38505;&#30340;&#29702;&#35770;&#24615;&#36136;&#12290;&#21516;&#26102;&#65292;&#32479;&#19968;&#24754;&#35266;&#39118;&#38505;&#34987;&#24212;&#29992;&#20110;&#20272;&#35745;&#38889;&#22269;&#32929;&#31080;&#30340;&#24754;&#35266;&#26368;&#20248;&#32452;&#21512;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The optimality of allocating assets has been widely discussed with the theoretical analysis of risk measures. Pessimism is one of the most attractive approaches beyond the conventional optimal portfolio model, and the $\alpha$-risk plays a crucial role in deriving a broad class of pessimistic optimal portfolios. However, estimating an optimal portfolio assessed by a pessimistic risk is still challenging due to the absence of an available estimation model and a computational algorithm. In this study, we propose a version of integrated $\alpha$-risk called the uniform pessimistic risk and the computational algorithm to obtain an optimal portfolio based on the risk. Further, we investigate the theoretical properties of the proposed risk in view of three different approaches: multiple quantile regression, the proper scoring rule, and distributionally robust optimization. Also, the uniform pessimistic risk is applied to estimate the pessimistic optimal portfolio models for the Korean stock 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;ScionFL&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#39640;&#25928;&#36816;&#34892;&#22312;&#37327;&#21270;&#36755;&#20837;&#19978;&#24182;&#21516;&#26102;&#25552;&#20379;&#23545;&#24694;&#24847;&#23458;&#25143;&#31471;&#24378;&#20581;&#24615;&#30340;&#23433;&#20840;&#32858;&#21512;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2210.07376</link><description>&lt;p&gt;
ScionFL: &#39640;&#25928;&#19988;&#24378;&#20581;&#30340;&#23433;&#20840;&#37327;&#21270;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
ScionFL: Efficient and Robust Secure Quantized Aggregation. (arXiv:2210.07376v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07376
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;ScionFL&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#39640;&#25928;&#36816;&#34892;&#22312;&#37327;&#21270;&#36755;&#20837;&#19978;&#24182;&#21516;&#26102;&#25552;&#20379;&#23545;&#24694;&#24847;&#23458;&#25143;&#31471;&#24378;&#20581;&#24615;&#30340;&#23433;&#20840;&#32858;&#21512;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#32858;&#21512;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#65292;&#20197;&#20943;&#36731;&#19982;&#20013;&#22830;&#32858;&#21512;&#22120;&#30475;&#21040;&#25152;&#26377;&#21442;&#25968;&#26356;&#26032;&#26377;&#20851;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#23433;&#20840;&#32858;&#21512;&#26041;&#26696;&#24573;&#30053;&#20102;&#20004;&#20010;&#20851;&#38190;&#30340;&#27491;&#20132;&#30740;&#31350;&#26041;&#21521;&#65292;&#26088;&#22312;&#65288;i&#65289;&#26174;&#30528;&#20943;&#23569;&#23458;&#25143;&#31471;-&#26381;&#21153;&#22120;&#36890;&#20449;&#21644;&#65288;ii&#65289;&#20943;&#36731;&#24694;&#24847;&#23458;&#25143;&#31471;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#36825;&#20004;&#20010;&#39069;&#22806;&#30340;&#23646;&#24615;&#23545;&#20110;&#23454;&#29616;&#36328;&#35774;&#22791;&#30340;&#32852;&#37030;&#23398;&#20064;&#19982;&#25104;&#21315;&#19978;&#19975;&#29978;&#33267;&#30334;&#19975;&#65288;&#31227;&#21160;&#65289;&#21442;&#19982;&#32773;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;ScionFL&#65292;&#32852;&#21512;&#20102;&#20004;&#20010;&#30740;&#31350;&#26041;&#21521;&#65292;&#36825;&#26159;FL&#30340;&#31532;&#19968;&#20010;&#23433;&#20840;&#32858;&#21512;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#37327;&#21270;&#36755;&#20837;&#19978;&#26377;&#25928;&#36816;&#34892;&#65292;&#21516;&#26102;&#25552;&#20379;&#23545;&#24694;&#24847;&#23458;&#25143;&#31471;&#30340;&#24378;&#20581;&#24615;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21033;&#29992;&#65288;&#26032;&#39062;&#30340;&#65289;&#22810;&#26041;&#35745;&#31639;&#65288;MPC&#65289;&#25216;&#26415;&#65292;&#24182;&#25903;&#25345;&#22810;&#20010;&#32447;&#24615;&#65288;1&#27604;&#29305;&#65289;&#37327;&#21270;&#26041;&#26696;&#65292;&#21253;&#25324;&#21033;&#29992;&#38543;&#26426;&#21704;&#36798;&#29595;&#21464;&#25442;&#21644;&#21345;&#30003;&#34920;&#31034;&#30340;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Secure aggregation is commonly used in federated learning (FL) to alleviate privacy concerns related to the central aggregator seeing all parameter updates in the clear. Unfortunately, most existing secure aggregation schemes ignore two critical orthogonal research directions that aim to (i) significantly reduce client-server communication and (ii) mitigate the impact of malicious clients. However, both of these additional properties are essential to facilitate cross-device FL with thousands or even millions of (mobile) participants. In this paper, we unite both research directions by introducing ScionFL, the first secure aggregation framework for FL that operates efficiently on quantized inputs and simultaneously provides robustness against malicious clients. Our framework leverages (novel) multi-party computation (MPC) techniques and supports multiple linear (1-bit) quantization schemes, including ones that utilize the randomized Hadamard transform and Kashin's representation. Our th
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#23398;&#20064;&#26041;&#27861;&#65292;&#30001;&#20248;&#21270;&#22120;&#39537;&#21160;&#30340;&#37319;&#26679;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#26367;&#20195;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#27604;&#30446;&#21069;&#20351;&#29992;&#30340;&#26367;&#20195;&#27169;&#22411;&#26356;&#20934;&#30830;&#30340;&#26367;&#20195;&#27169;&#22411;&#65292;&#20026;&#22797;&#26434;&#31995;&#32479;&#27169;&#25311;&#25552;&#20379;&#20102;&#26356;&#21152;&#35745;&#31639;&#19978;&#39640;&#25928;&#30340;&#26367;&#20195;&#27169;&#22411;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2207.12855</link><description>&lt;p&gt;
&#29992;&#20110;&#22797;&#26434;&#31995;&#32479;&#27169;&#25311;&#30340;&#26377;&#25928;&#23398;&#20064;&#20934;&#30830;&#26367;&#20195;&#27169;&#22411;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient Learning of Accurate Surrogates for Simulations of Complex Systems. (arXiv:2207.12855v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.12855
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#23398;&#20064;&#26041;&#27861;&#65292;&#30001;&#20248;&#21270;&#22120;&#39537;&#21160;&#30340;&#37319;&#26679;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#26367;&#20195;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#27604;&#30446;&#21069;&#20351;&#29992;&#30340;&#26367;&#20195;&#27169;&#22411;&#26356;&#20934;&#30830;&#30340;&#26367;&#20195;&#27169;&#22411;&#65292;&#20026;&#22797;&#26434;&#31995;&#32479;&#27169;&#25311;&#25552;&#20379;&#20102;&#26356;&#21152;&#35745;&#31639;&#19978;&#39640;&#25928;&#30340;&#26367;&#20195;&#27169;&#22411;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36234;&#26469;&#36234;&#34987;&#29992;&#20110;&#26500;&#24314;&#35745;&#31639;&#19978;&#24265;&#20215;&#30340;&#22797;&#26434;&#29289;&#29702;&#27169;&#22411;&#30340;&#26367;&#20195;&#27169;&#22411;&#12290;&#24403;&#25968;&#25454;&#19981;&#31283;&#23450;&#12289;&#31232;&#30095;&#25110;&#26102;&#38388;&#30456;&#20851;&#26102;&#65292;&#36825;&#20123;&#26367;&#20195;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#20250;&#19979;&#38477;&#12290;&#20026;&#20102;&#25214;&#21040;&#19968;&#31181;&#21487;&#20197;&#25552;&#20379;&#20219;&#20309;&#28508;&#22312;&#26410;&#26469;&#27169;&#22411;&#35780;&#20272;&#30340;&#26377;&#25928;&#39044;&#27979;&#30340;&#26367;&#20195;&#27169;&#22411;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#30001;&#20248;&#21270;&#22120;&#39537;&#21160;&#30340;&#22312;&#32447;&#23398;&#20064;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#26377;&#20004;&#20010;&#20248;&#28857;&#65292;&#39318;&#20808;&#23427;&#30830;&#20445;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#21253;&#21547;&#27169;&#22411;&#21709;&#24212;&#26354;&#38754;&#19978;&#30340;&#25152;&#26377;&#25296;&#28857;&#12290;&#20854;&#27425;&#65292;&#22312;&#20219;&#20309;&#26032;&#30340;&#27169;&#22411;&#35780;&#20272;&#20043;&#21518;&#65292;&#22914;&#26524;&#8220;&#24471;&#20998;&#8221;&#19979;&#38477;&#21040;&#19979;&#38480;&#65292;&#21017;&#20250;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#8220;&#37325;&#26032;&#35757;&#32451;&#8221;&#65288;&#26356;&#26032;&#65289;&#12290;&#22312;&#22522;&#20934;&#20989;&#25968;&#19978;&#30340;&#27979;&#35797;&#34920;&#26126;&#65292;&#19982;&#20256;&#32479;&#30340;&#37319;&#26679;&#26041;&#27861;&#30456;&#27604;&#65292;&#30001;&#20248;&#21270;&#22120;&#24341;&#23548;&#30340;&#37319;&#26679;&#36890;&#24120;&#22312;&#23616;&#37096;&#26497;&#20540;&#38468;&#36817;&#30340;&#20934;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#20248;&#36234;&#65292;&#21363;&#20351;&#35780;&#20998;&#25351;&#26631;&#25903;&#25345;&#25972;&#20307;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#26680;&#29289;&#36136;&#30340;&#27169;&#25311;&#20013;&#65292;&#20197;&#35777;&#26126;&#23427;&#22788;&#29702;&#22024;&#26434;&#12289;&#31232;&#30095;&#21644;&#26102;&#38388;&#30456;&#20851;&#30340;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#24314;&#31435;&#30340;&#26367;&#20195;&#27169;&#22411;&#27604;&#21516;&#26679;&#25968;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#26500;&#24314;&#30340;&#30446;&#21069;&#20351;&#29992;&#30340;&#26367;&#20195;&#27169;&#22411;&#26356;&#20934;&#30830;&#12290;&#25105;&#20204;&#30340;&#22312;&#32447;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#35745;&#31639;&#19978;&#39640;&#25928;&#30340;&#26367;&#20195;&#27169;&#22411;&#65292;&#32780;&#20256;&#32479;&#30340;&#26367;&#20195;&#27169;&#22411;&#21017;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning methods are increasingly used to build computationally inexpensive surrogates for complex physical models. The predictive capability of these surrogates suffers when data are noisy, sparse, or time-dependent. As we are interested in finding a surrogate that provides valid predictions of any potential future model evaluations, we introduce an online learning method empowered by optimizer-driven sampling. The method has two advantages over current approaches. First, it ensures that all turning points on the model response surface are included in the training data. Second, after any new model evaluations, surrogates are tested and "retrained" (updated) if the "score" drops below a validity threshold. Tests on benchmark functions reveal that optimizer-directed sampling generally outperforms traditional sampling methods in terms of accuracy around local extrema, even when the scoring metric favors overall accuracy. We apply our method to simulations of nuclear matter to dem
&lt;/p&gt;</description></item></channel></rss>