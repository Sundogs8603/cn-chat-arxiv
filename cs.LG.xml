<rss version="2.0"><channel><title>Chinese Chat Arxiv LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>Exphormer&#26159;&#19968;&#31181;&#26032;&#30340;&#22270;&#24418;Transformer&#26550;&#26500;&#65292;&#37319;&#29992;&#34394;&#25311;&#20840;&#23616;&#33410;&#28857;&#21644;&#25193;&#23637;&#22270;&#24418;&#30340;&#31232;&#30095;&#27880;&#24847;&#26426;&#21046;&#65292;&#20855;&#26377;&#19982;&#28040;&#24687;&#20256;&#36882;&#32593;&#32476;&#30456;&#31454;&#20105;&#30340;&#20934;&#30830;&#24615;&#21644;&#32447;&#24615;&#30340;&#22797;&#26434;&#24230;&#12290;&#22312;&#24191;&#27867;&#30340;&#22270;&#24418;&#25968;&#25454;&#38598;&#19978;&#65292;Exphormer&#22312;&#26368;&#36817;&#25552;&#20986;&#30340;GraphGPS&#26694;&#26550;&#20013;&#23637;&#29616;&#20986;&#26377;&#31454;&#20105;&#21147;&#30340;&#23454;&#35777;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.06147</link><description>&lt;p&gt;
Exphormer&#65306;&#29992;&#20110;&#22270;&#24418;&#30340;&#31232;&#30095;Transformer
&lt;/p&gt;
&lt;p&gt;
Exphormer: Sparse Transformers for Graphs. (arXiv:2303.06147v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.06147](http://arxiv.org/abs/2303.06147)
&lt;/p&gt;
&lt;p&gt;
&#22270;&#24418;Transformer&#24050;&#25104;&#20026;&#21508;&#31181;&#22270;&#24418;&#23398;&#20064;&#21644;&#34920;&#31034;&#20219;&#21153;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#20307;&#31995;&#32467;&#26500;&#12290;&#23613;&#31649;&#23427;&#20204;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#22312;&#20445;&#25345;&#19982;&#28040;&#24687;&#20256;&#36882;&#32593;&#32476;&#31454;&#20105;&#21147;&#30340;&#31934;&#24230;&#30340;&#21516;&#26102;&#23558;&#22270;&#24418;Transformer&#25193;&#23637;&#21040;&#22823;&#22411;&#22270;&#24418;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Exphormer&#65292;&#36825;&#26159;&#19968;&#20010;&#26500;&#24314;&#24378;&#22823;&#32780;&#21487;&#25193;&#23637;&#30340;&#22270;&#24418;Transformer&#30340;&#26694;&#26550;&#12290;Exphormer&#30001;&#22522;&#20110;&#20004;&#20010;&#26426;&#21046;&#30340;&#31232;&#30095;&#27880;&#24847;&#26426;&#21046;&#32452;&#25104;&#65306;&#34394;&#25311;&#20840;&#23616;&#33410;&#28857;&#21644;&#25193;&#23637;&#22270;&#24418;&#65292;&#20854;&#25968;&#23398;&#29305;&#24449;&#65288;&#20363;&#22914;&#65292;&#35889;&#23637;&#24320;&#12289;&#20266;&#38543;&#26426;&#24615;&#21644;&#31232;&#30095;&#24615;&#65289;&#20135;&#29983;&#30340;&#22270;&#24418;Transformer&#30340;&#22797;&#26434;&#24230;&#20165;&#19982;&#22270;&#24418;&#30340;&#22823;&#23567;&#25104;&#32447;&#24615;&#20851;&#31995;&#65292;&#21516;&#26102;&#20801;&#35768;&#25105;&#20204;&#35777;&#26126;&#25152;&#24471;Transformer&#27169;&#22411;&#30340;&#29702;&#35770;&#24615;&#36136;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23558;Exphormer&#32435;&#20837;&#26368;&#36817;&#25552;&#20986;&#30340;GraphGPS&#26694;&#26550;&#20250;&#20135;&#29983;&#20855;&#26377;&#31454;&#20105;&#23454;&#35777;&#32467;&#26524;&#30340;&#27169;&#22411;&#65292;&#21253;&#25324;&#21508;&#31181;&#22270;&#24418;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#26032;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph transformers have emerged as a promising architecture for a variety of graph learning and representation tasks. Despite their successes, though, it remains challenging to scale graph transformers to large graphs while maintaining accuracy competitive with message-passing networks. In this paper, we introduce Exphormer, a framework for building powerful and scalable graph transformers. Exphormer consists of a sparse attention mechanism based on two mechanisms: virtual global nodes and expander graphs, whose mathematical characteristics, such as spectral expansion, pseduorandomness, and sparsity, yield graph transformers with complexity only linear in the size of the graph, while allowing us to prove desirable theoretical properties of the resulting transformer models. We show that incorporating \textsc{Exphormer} into the recently-proposed GraphGPS framework produces models with competitive empirical results on a wide variety of graph datasets, including state-of-the-art results o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20351;&#29992;&#25193;&#24352;&#21367;&#31215;&#26469;&#37325;&#26032;&#35843;&#25972;StyleGAN&#20013;&#27973;&#23618;&#30340;&#25509;&#21463;&#22495;&#65292;&#20174;&#32780;&#23558;StyleGAN&#30340;&#38754;&#37096;&#25805;&#20316;&#25193;&#23637;&#21040;&#38500;&#20102;&#35009;&#21098;&#23545;&#40784;&#30340;&#20154;&#33080;&#20043;&#22806;&#65292;&#20197;&#23454;&#29616;&#30495;&#23454;&#30340;&#38754;&#37096;&#21453;&#28436;&#21644;&#25805;&#20316;&#12290;</title><link>http://arxiv.org/abs/2303.06146</link><description>&lt;p&gt;
StyleGANEX&#65306;&#22522;&#20110;StyleGAN&#30340;&#38754;&#37096;&#25805;&#20316;&#25193;&#23637;&#21040;&#38500;&#20102;&#35009;&#21098;&#23545;&#40784;&#30340;&#20154;&#33080;&#20043;&#22806;
&lt;/p&gt;
&lt;p&gt;
StyleGANEX: StyleGAN-Based Manipulation Beyond Cropped Aligned Faces. (arXiv:2303.06146v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.06146](http://arxiv.org/abs/2303.06146)
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;StyleGAN&#36827;&#34892;&#38754;&#37096;&#25805;&#20316;&#30340;&#26368;&#26032;&#36827;&#23637;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;StyleGAN&#22312;&#22266;&#23450;&#22270;&#20687;&#20998;&#36776;&#29575;&#19979;&#65292;&#26412;&#36136;&#19978;&#20165;&#38480;&#20110;&#35009;&#21098;&#23545;&#40784;&#30340;&#20154;&#33080;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#25193;&#24352;&#21367;&#31215;&#26469;&#37325;&#26032;&#35843;&#25972;StyleGAN&#20013;&#27973;&#23618;&#30340;&#25509;&#21463;&#22495;&#65292;&#32780;&#19981;&#25913;&#21464;&#20219;&#20309;&#27169;&#22411;&#21442;&#25968;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#20351;&#24471;&#27973;&#23618;&#22266;&#23450;&#22823;&#23567;&#30340;&#23567;&#29305;&#24449;&#21487;&#20197;&#25193;&#23637;&#20026;&#21487;&#20197;&#36866;&#24212;&#21464;&#37327;&#20998;&#36776;&#29575;&#30340;&#36739;&#22823;&#29305;&#24449;&#65292;&#20351;&#23427;&#20204;&#33021;&#22815;&#26356;&#22909;&#22320;&#34920;&#24449;&#19981;&#23545;&#40784;&#30340;&#20154;&#33080;&#12290;&#20026;&#20102;&#23454;&#29616;&#30495;&#23454;&#30340;&#38754;&#37096;&#21453;&#28436;&#21644;&#25805;&#20316;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#30456;&#24212;&#30340;&#32534;&#30721;&#22120;&#65292;&#38500;&#20102;&#28508;&#22312;&#30340;&#26679;&#24335;&#20195;&#30721;&#22806;&#65292;&#36824;&#25552;&#20379;&#20102;&#25193;&#23637;StyleGAN&#30340;&#31532;&#19968;&#23618;&#29305;&#24449;&#12290;&#25105;&#20204;&#22312;&#21253;&#25324;&#38754;&#37096;&#23646;&#24615;&#32534;&#36753;&#12289;&#36229;&#20998;&#36776;&#29575;&#12289;&#33609;&#22270;/&#38754;&#20855;&#21040;&#38754;&#37096;&#31561;&#21508;&#31181;&#20998;&#36776;&#29575;&#30340;&#38750;&#23545;&#40784;&#38754;&#37096;&#36755;&#20837;&#20013;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in face manipulation using StyleGAN have produced impressive results. However, StyleGAN is inherently limited to cropped aligned faces at a fixed image resolution it is pre-trained on. In this paper, we propose a simple and effective solution to this limitation by using dilated convolutions to rescale the receptive fields of shallow layers in StyleGAN, without altering any model parameters. This allows fixed-size small features at shallow layers to be extended into larger ones that can accommodate variable resolutions, making them more robust in characterizing unaligned faces. To enable real face inversion and manipulation, we introduce a corresponding encoder that provides the first-layer feature of the extended StyleGAN in addition to the latent style code. We validate the effectiveness of our method using unaligned face inputs of various resolutions in a diverse set of face manipulation tasks, including facial attribute editing, super-resolution, sketch/mask-to-face 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;QD&#31639;&#27861;&#8212;&#8212;MAP-Elites-Multi-ES&#65288;MEMES&#65289;&#65292;&#35813;&#31639;&#27861;&#22522;&#20110;&#36827;&#21270;&#31574;&#30053;&#65288;ES&#65289;&#21644;&#24555;&#36895;&#24182;&#34892;&#35780;&#20272;&#65292;&#36890;&#36807;&#32500;&#25252;&#20855;&#26377;&#22823;&#35268;&#27169;&#24182;&#34892;&#24615;&#30340;&#22810;&#20010;&#29420;&#31435;ES&#32447;&#31243;&#26469;&#25193;&#23637;&#29616;&#26377;&#31639;&#27861;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#22312;&#20195;&#25968;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#65292;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#31639;&#27861;&#21019;&#26032;&#12290;</title><link>http://arxiv.org/abs/2303.06137</link><description>&lt;p&gt;
&#22810;&#25163;&#36731;&#26494;&#23436;&#25104;&#65306;&#20351;&#29992;&#22810;&#20010;&#24182;&#34892;&#36827;&#21270;&#31574;&#30053;&#30340;MAP-Elites&#22686;&#24378;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;
&lt;/p&gt;
&lt;p&gt;
Multiple Hands Make Light Work: Enhancing Quality and Diversity using MAP-Elites with Multiple Parallel Evolution Strategies. (arXiv:2303.06137v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.06137](http://arxiv.org/abs/2303.06137)
&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#30828;&#20214;&#21152;&#36895;&#22120;&#21450;&#20854;&#30456;&#24212;&#24037;&#20855;&#30340;&#21457;&#23637;&#65292;&#22312;&#26576;&#20123;&#24212;&#29992;&#20013;&#36890;&#36807;&#24555;&#36895;&#21644;&#22823;&#35268;&#27169;&#30340;&#35780;&#20272;&#65292;&#35780;&#20272;&#24050;&#21464;&#24471;&#26356;&#21152;&#32463;&#27982;&#23454;&#24800;&#12290;&#36825;&#31181;&#36827;&#23637;&#26497;&#22823;&#22320;&#21152;&#24555;&#20102;&#21551;&#21457;&#24335;&#31639;&#27861;&#65288;&#22914;&#36136;&#37327;-&#22810;&#26679;&#24615;&#20248;&#21270;&#65289;&#30340;&#36816;&#34892;&#26102;&#65292;&#24182;&#21019;&#36896;&#20102;&#36890;&#36807;&#35268;&#27169;&#36827;&#34892;&#31639;&#27861;&#21019;&#26032;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MAP-Elites-Multi-ES&#65288;MEMES&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#36827;&#21270;&#31574;&#30053;&#65288;ES&#65289;&#30340;&#26032;&#22411;QD&#31639;&#27861;&#65292;&#19987;&#20026;&#24555;&#36895;&#24182;&#34892;&#35780;&#20272;&#32780;&#35774;&#35745;&#12290; ME-Multi-ES&#22312;&#29616;&#26377;&#30340;MAP-Elites-ES&#31639;&#27861;&#22522;&#30784;&#19978;&#36827;&#34892;&#20102;&#25913;&#36827;&#65292;&#36890;&#36807;&#32500;&#25252;&#20855;&#26377;&#22823;&#35268;&#27169;&#24182;&#34892;&#24615;&#30340;&#22810;&#20010;&#29420;&#31435;ES&#32447;&#31243;&#26469;&#25193;&#23637;&#23427;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#21160;&#24577;&#37325;&#32622;&#31243;&#24207;&#65292;&#29992;&#20110;&#33258;&#20027;&#26368;&#22823;&#21270;QD&#32676;&#20307;&#30340;&#25913;&#36827;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#22522;&#20110;&#26799;&#24230;&#21644;&#23458;&#35266;&#19981;&#21487;&#30693;&#30340;QD&#31639;&#27861;&#30456;&#27604;&#65292;MEMES&#22312;&#20195;&#25968;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the development of hardware accelerators and their corresponding tools, evaluations have become more affordable through fast and massively parallel evaluations in some applications. This advancement has drastically sped up the runtime of evolution-inspired algorithms such as Quality-Diversity optimization, creating tremendous potential for algorithmic innovation through scale. In this work, we propose MAP-Elites-Multi-ES (MEMES), a novel QD algorithm based on Evolution Strategies (ES) designed for fast parallel evaluations. ME-Multi-ES builds on top of the existing MAP-Elites-ES algorithm, scaling it by maintaining multiple independent ES threads with massive parallelization. We also introduce a new dynamic reset procedure for the lifespan of the independent ES to autonomously maximize the improvement of the QD population. We show experimentally that MEMES outperforms existing gradient-based and objective-agnostic QD algorithms when compared in terms of generations. We perform thi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22870;&#21169;&#31995;&#32479;&#26469;&#35757;&#32451;&#20248;&#31168;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#21033;&#29992;&#29992;&#25143;&#21453;&#39304;&#25968;&#25454;&#21435;&#31579;&#36873;&#36755;&#20986;&#26469;&#25552;&#39640;&#20445;&#30041;&#29575;&#65292;A/B&#27979;&#35797;&#34920;&#26126;&#35813;&#26041;&#27861;&#33021;&#25552;&#39640;68%&#30340;&#20445;&#30041;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.06135</link><description>&lt;p&gt;
&#29992;&#20110;&#30334;&#19975;&#29992;&#25143;&#30495;&#23454;&#19990;&#30028;&#20114;&#21160;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#22870;&#21169;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Rewarding Chatbots for Real-World Engagement with Millions of Users. (arXiv:2303.06135v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.06135](http://arxiv.org/abs/2303.06135)
&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#23548;&#33268;&#20102;&#19968;&#31995;&#21015;&#30340;&#31038;&#20132;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#37096;&#32626;&#65292;&#29992;&#20110;&#38386;&#32842;&#12290;&#34429;&#28982;&#36825;&#20123;&#32842;&#22825;&#26426;&#22120;&#20154;&#23637;&#31034;&#20102;&#35821;&#35328;&#33021;&#21147;&#21644;&#27969;&#21033;&#24230;&#65292;&#20294;&#23427;&#20204;&#24182;&#19981;&#19968;&#23450;&#24341;&#20154;&#20837;&#32988;&#65292;&#26377;&#26102;&#20505;&#24456;&#38590;&#21560;&#24341;&#29992;&#25143;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#24320;&#21457;&#27880;&#37325;&#29992;&#25143;&#21442;&#19982;&#24230;&#30340;&#31038;&#20132;&#32842;&#22825;&#26426;&#22120;&#20154;&#20197;&#22686;&#24378;&#20445;&#30041;&#29575;&#65292;&#29305;&#21035;&#26159;&#32771;&#23519;&#20102;&#20351;&#29992;&#20154;&#31867;&#21453;&#39304;&#26469;&#39640;&#25928;&#24320;&#21457;&#26497;&#20855;&#21560;&#24341;&#21147;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;&#20174;&#29992;&#25143;&#20132;&#20114;&#20013;&#25910;&#38598;&#30340;&#33258;&#21160;&#20266;&#26631;&#31614;&#26469;&#35757;&#32451;&#19968;&#20010;&#22870;&#21169;&#27169;&#22411;&#65292;&#22312;&#25512;&#29702;&#26102;&#21487;&#20197;&#29992;&#26469;&#25298;&#32477;&#32842;&#22825;&#26426;&#22120;&#20154;&#27169;&#22411;&#20135;&#29983;&#30340;&#20302;&#20998;&#26679;&#26412;&#21709;&#24212;&#12290;&#24341;&#20837;&#20102;&#30452;&#35266;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#22914;&#24179;&#22343;&#23545;&#35805;&#38271;&#24230; (MCL)&#65292;&#20316;&#20026;&#34913;&#37327;&#37096;&#32626;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#21442;&#19982;&#27700;&#24179;&#30340;&#20195;&#29702;&#12290;&#22312;Chai Research&#24179;&#21488;&#19978;&#65292;&#23545;&#27599;&#26085;&#26032;&#30340;10,000&#20010;&#32842;&#22825;&#26426;&#22120;&#20154;&#29992;&#25143;&#32452;&#36827;&#34892;&#30340;A/B&#27979;&#35797;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#23558;MCL&#25552;&#39640;&#20102;&#26368;&#22810;70&#65285;&#65292;&#36825;&#30456;&#24403;&#20110;&#23558;&#20445;&#30041;&#29575;&#20174;40&#65285;&#22686;&#21152;&#21040;68&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of pretrained large language models has led to the deployment of a range of social chatbots for chitchat. Although these chatbots demonstrate language ability and fluency, they are not guaranteed to be engaging and can struggle to retain users. This work investigates the development of social chatbots that prioritize user engagement to enhance retention, specifically examining the use of human feedback to efficiently develop highly engaging chatbots. The proposed approach uses automatic pseudo-labels collected from user interactions to train a reward model that can be used to reject low-scoring sample responses generated by the chatbot model at inference time. Intuitive evaluation metrics, such as mean conversation length (MCL), are introduced as proxies to measure the level of engagement of deployed chatbots. A/B testing on groups of 10,000 new daily chatbot users on the Chai Research platform shows that this approach increases the MCL by up to 70%, which translates to a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20449;&#24687;&#38376;&#25511;&#30340;&#27010;&#24565;&#65292;&#21363;&#23398;&#20064;&#29992;&#20110;&#35299;&#20915;&#29305;&#23450;&#20219;&#21153;&#30340;&#26368;&#23567;&#20449;&#24687;&#25513;&#30721;&#65292;&#20197;&#23454;&#29616;&#26356;&#31616;&#27905;&#30340;&#34920;&#31034;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#21508;&#31181;&#30446;&#26631;&#65292;&#24182;&#19988;&#21487;&#20197;&#25913;&#21892;&#31639;&#27861;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.06121</link><description>&lt;p&gt;
&#26080;&#30693;&#21363;&#31119;&#65306;&#36890;&#36807;&#20449;&#24687;&#38376;&#25511;&#23454;&#29616;&#40065;&#26834;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Ignorance is Bliss: Robust Control via Information Gating. (arXiv:2303.06121v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.06121](http://arxiv.org/abs/2303.06121)
&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#31616;&#27905;&#24615;&#8212;&#8212;&#21363;&#20351;&#29992;&#20219;&#21153;&#25152;&#38656;&#30340;&#26368;&#23567;&#20449;&#24687;&#8212;&#8212;&#20026;&#23398;&#20064;&#34920;&#31034;&#25552;&#20379;&#20102;&#26377;&#29992;&#30340;&#24402;&#32435;&#20559;&#32622;&#65292;&#20174;&#32780;&#36890;&#36807;&#23545;&#22122;&#22768;&#21644;&#20266;&#30456;&#20851;&#24615;&#20855;&#26377;&#40065;&#26834;&#24615;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#27867;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20687;&#32032;&#31354;&#38388;&#20013;&#30340;&#20449;&#24687;&#38376;&#25511;&#20316;&#20026;&#23398;&#20064;&#26356;&#31616;&#27905;&#34920;&#31034;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;&#20449;&#24687;&#38376;&#25511;&#30340;&#20316;&#29992;&#26159;&#23398;&#20064;&#25429;&#25417;&#35299;&#20915;&#32473;&#23450;&#20219;&#21153;&#25152;&#38656;&#30340;&#26368;&#23567;&#20449;&#24687;&#30340;&#25513;&#30721;&#12290;&#30452;&#35266;&#22320;&#35828;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23398;&#20064;&#35782;&#21035;&#21738;&#20123;&#35270;&#35273;&#32447;&#32034;&#23545;&#20110;&#32473;&#23450;&#20219;&#21153;&#23454;&#38469;&#19978;&#24456;&#37325;&#35201;&#12290;&#25105;&#20204;&#20351;&#29992;&#20449;&#22122;&#27604;&#30340;&#21487;&#24494;&#21442;&#25968;&#21270;&#23545;&#20449;&#21495;&#36827;&#34892;&#38376;&#25511;&#65292;&#35813;&#20449;&#22122;&#27604;&#21487;&#24212;&#29992;&#20110;&#32593;&#32476;&#20013;&#30340;&#20219;&#24847;&#20540;&#65292;&#20363;&#22914;&#23631;&#34109;&#36755;&#20837;&#23618;&#30340;&#20687;&#32032;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#31216;&#20026; InfoGating&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#21508;&#31181;&#30446;&#26631;&#65292;&#20363;&#22914;&#65306;&#22810;&#27493;&#21069;&#21521;&#21644;&#36870;&#21521;&#21160;&#21147;&#23398;&#12289;Q&#23398;&#20064;&#12289;&#34892;&#20026;&#20811;&#38534;&#21644;&#26631;&#20934;&#33258;&#30417;&#30563;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#23398;&#20064;&#35782;&#21035;&#21644;&#20351;&#29992;&#26368;&#23567;&#20449;&#24687;&#26377;&#21161;&#20110;&#25913;&#21892;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Informational parsimony -- i.e., using the minimal information required for a task, -- provides a useful inductive bias for learning representations that achieve better generalization by being robust to noise and spurious correlations. We propose information gating in the pixel space as a way to learn more parsimonious representations. Information gating works by learning masks that capture only the minimal information required to solve a given task. Intuitively, our models learn to identify which visual cues actually matter for a given task. We gate information using a differentiable parameterization of the signal-to-noise ratio, which can be applied to arbitrary values in a network, e.g.~masking out pixels at the input layer. We apply our approach, which we call InfoGating, to various objectives such as: multi-step forward and inverse dynamics, Q-learning, behavior cloning, and standard self-supervised tasks. Our experiments show that learning to identify and use minimal information 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32771;&#34385;&#20102;&#32852;&#37030;&#20915;&#31574;&#20013;&#20449;&#24687;&#32858;&#21512;&#30340;&#38382;&#39064;&#65292;&#24314;&#31435;&#20102;&#27719;&#38598;&#31574;&#30053;&#37117;&#23548;&#33268;&#31995;&#32479;&#30340;&#28176;&#36817;&#27491;&#24577;&#24615;&#25551;&#36848;&#65292;&#29992;&#20110;&#35745;&#31639;&#38169;&#35823;&#27010;&#29575;&#65292;&#24182;&#36890;&#36807;&#27169;&#25311;&#39564;&#35777;&#20102;&#29702;&#35770;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.06109</link><description>&lt;p&gt;
&#20851;&#20110;&#32852;&#37030;&#20915;&#31574;&#20013;&#34701;&#21512;&#31574;&#30053;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Fusion Strategies for Federated Decision Making. (arXiv:2303.06109v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.06109](http://arxiv.org/abs/2303.06109)
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#32852;&#37030;&#20915;&#31574;&#20013;&#20449;&#24687;&#32858;&#21512;&#30340;&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#28041;&#21450;&#19968;&#32452;&#20195;&#29702;&#21327;&#20316;&#20197;&#25512;&#26029;&#33258;&#28982;&#29366;&#24577;&#65292;&#32780;&#19981;&#19982;&#20013;&#22830;&#22788;&#29702;&#22120;&#25110;&#24444;&#27492;&#20849;&#20139;&#31169;&#20154;&#25968;&#25454;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#38750;&#36125;&#21494;&#26031;&#31038;&#20132;&#23398;&#20064;&#31574;&#30053;&#65292;&#22312;&#35813;&#31574;&#30053;&#20013;&#65292;&#20195;&#29702;&#23558;&#20854;&#20010;&#20154;&#35266;&#23519;&#32467;&#26524;&#19982;&#36125;&#21494;&#26031;&#35268;&#21017;&#30456;&#32467;&#21512;&#65292;&#24418;&#25104;&#35266;&#28857;&#65288;&#21363;&#36719;&#20915;&#31574;&#65289;&#65292;&#24182;&#19988;&#20013;&#22830;&#22788;&#29702;&#22120;&#36890;&#36807;&#31639;&#26415;&#25110;&#20960;&#20309;&#24179;&#22343;&#32858;&#21512;&#36825;&#20123;&#35266;&#28857;&#12290;&#22522;&#20110;&#25105;&#20204;&#20043;&#21069;&#30340;&#24037;&#20316;&#65292;&#25105;&#20204;&#30830;&#23450;&#20004;&#31181;&#27719;&#38598;&#31574;&#30053;&#37117;&#23548;&#33268;&#31995;&#32479;&#30340;&#28176;&#36817;&#27491;&#24577;&#24615;&#25551;&#36848;&#65292;&#20363;&#22914;&#65292;&#21487;&#20197;&#21033;&#29992;&#36825;&#31181;&#25551;&#36848;&#36817;&#20284;&#35745;&#31639;&#38169;&#35823;&#27010;&#29575;&#30340;&#34920;&#36798;&#24335;&#12290;&#25105;&#20204;&#36890;&#36807;&#27169;&#25311;&#39564;&#35777;&#20102;&#29702;&#35770;&#32467;&#26524;&#24182;&#27604;&#36739;&#20102;&#20004;&#31181;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of information aggregation in federated decision making, where a group of agents collaborate to infer the underlying state of nature without sharing their private data with the central processor or each other. We analyze the non-Bayesian social learning strategy in which agents incorporate their individual observations into their opinions (i.e., soft-decisions) with Bayes rule, and the central processor aggregates these opinions by arithmetic or geometric averaging. Building on our previous work, we establish that both pooling strategies result in asymptotic normality characterization of the system, which, for instance, can be utilized in order to give approximate expressions for the error probability. We verify the theoretical findings with simulations and compare both strategies.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#20915;&#31574;&#29702;&#35770;&#30340;&#36890;&#29992;&#21644;&#21407;&#21017;&#24615;&#30340;&#38271;&#23614;&#20998;&#31867;&#26694;&#26550;&#65292;&#36890;&#36807;&#32508;&#21512;&#39118;&#38505;&#21644;&#36125;&#21494;&#26031;&#28145;&#24230;&#38598;&#25104;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#25152;&#26377;&#31867;&#21035;&#31934;&#24230;&#30340;&#26032;&#30446;&#26631;&#65292;&#29305;&#21035;&#26159;&#8220;&#23614;&#37096;&#8221;&#12290;&#27492;&#22806;&#65292;&#35813;&#26694;&#26550;&#20801;&#35768;&#20219;&#21153;&#33258;&#36866;&#24212;&#20915;&#31574;&#25439;&#22833;&#65292;&#20174;&#32780;&#22312;&#19981;&#21516;&#30340;&#20219;&#21153;&#20013;&#25552;&#20379;&#21487;&#35777;&#26126;&#30340;&#26368;&#20248;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2303.06075</link><description>&lt;p&gt;
&#22522;&#20110;&#36125;&#21494;&#26031;&#20915;&#31574;&#29702;&#35770;&#30340;&#38271;&#23614;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Long-tailed Classification from a Bayesian-decision-theory Perspective. (arXiv:2303.06075v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.06075](http://arxiv.org/abs/2303.06075)
&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#31867;&#27010;&#29575;&#30340;&#20005;&#37325;&#19981;&#24179;&#34913;&#21644;&#23614;&#37096;&#25935;&#24863;&#24615;&#39118;&#38505;&#20197;&#21450;&#19981;&#23545;&#31216;&#30340;&#38169;&#35823;&#39044;&#27979;&#25104;&#26412;&#65292;&#38271;&#23614;&#20998;&#31867;&#38754;&#20020;&#25361;&#25112;&#12290;&#26368;&#36817;&#30340;&#23581;&#35797;&#20351;&#29992;&#37325;&#26032;&#24179;&#34913;&#25439;&#22833;&#21644;&#38598;&#21512;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#26159;&#21551;&#21457;&#24335;&#30340;&#65292;&#24182;&#19988;&#20005;&#37325;&#20381;&#36182;&#20110;&#23454;&#35777;&#32467;&#26524;&#65292;&#32570;&#20047;&#29702;&#35770;&#35299;&#37322;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#26041;&#27861;&#24573;&#30053;&#20102;&#20915;&#31574;&#25439;&#22833;&#65292;&#20854;&#34920;&#24449;&#20102;&#19982;&#23614;&#37096;&#31867;&#21035;&#30456;&#20851;&#30340;&#19981;&#21516;&#25104;&#26412;&#12290;&#26412;&#25991;&#20174;&#36125;&#21494;&#26031;&#20915;&#31574;&#29702;&#35770;&#30340;&#35282;&#24230;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#21644;&#21407;&#21017;&#24615;&#30340;&#26694;&#26550;&#65292;&#32479;&#19968;&#20102;&#21253;&#25324;&#37325;&#26032;&#24179;&#34913;&#21644;&#38598;&#21512;&#26041;&#27861;&#22312;&#20869;&#30340;&#29616;&#26377;&#25216;&#26415;&#65292;&#24182;&#20026;&#20854;&#26377;&#25928;&#24615;&#25552;&#20379;&#20102;&#29702;&#35770;&#35777;&#26126;&#12290;&#20174;&#36825;&#20010;&#35282;&#24230;&#20986;&#21457;&#65292;&#25105;&#20204;&#22522;&#20110;&#32508;&#21512;&#39118;&#38505;&#21644;&#36125;&#21494;&#26031;&#28145;&#24230;&#38598;&#25104;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#25152;&#26377;&#31867;&#21035;&#31934;&#24230;&#30340;&#26032;&#30446;&#26631;&#65292;&#29305;&#21035;&#26159;&#8220;&#23614;&#37096;&#8221;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#20801;&#35768;&#20219;&#21153;&#33258;&#36866;&#24212;&#20915;&#31574;&#25439;&#22833;&#65292;&#20174;&#32780;&#22312;&#19981;&#21516;&#30340;&#20219;&#21153;&#20013;&#25552;&#20379;&#21487;&#35777;&#26126;&#30340;&#26368;&#20248;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
Long-tailed classification poses a challenge due to its heavy imbalance in class probabilities and tail-sensitivity risks with asymmetric misprediction costs. Recent attempts have used re-balancing loss and ensemble methods, but they are largely heuristic and depend heavily on empirical results, lacking theoretical explanation. Furthermore, existing methods overlook the decision loss, which characterizes different costs associated with tailed classes. This paper presents a general and principled framework from a Bayesian-decision-theory perspective, which unifies existing techniques including re-balancing and ensemble methods, and provides theoretical justifications for their effectiveness. From this perspective, we derive a novel objective based on the integrated risk and a Bayesian deep-ensemble approach to improve the accuracy of all classes, especially the ``tail". Besides, our framework allows for task-adaptive decision loss which provides provably optimal decisions in varying tas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#27010;&#29575;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#33041;&#30005;&#21512;&#25104;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#20174;&#24102;&#26377;&#24773;&#24863;&#26631;&#31614;&#30340;EEG&#35760;&#24405;&#30340;&#30005;&#26497;-&#39057;&#29575;&#20998;&#24067;&#22270;&#20013;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2303.06068</link><description>&lt;p&gt;
&#20351;&#29992;&#27010;&#29575;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#33041;&#30005;&#21512;&#25104;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
EEG Synthetic Data Generation Using Probabilistic Diffusion Models. (arXiv:2303.06068v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.06068](http://arxiv.org/abs/2303.06068)
&lt;/p&gt;
&lt;p&gt;
&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#30001;&#20110;&#20854;&#26080;&#21019;&#12289;&#20302;&#25104;&#26412;&#21644;&#26131;&#20110;&#20351;&#29992;&#30340;&#29305;&#28857;&#22312;&#22823;&#33041;&#35745;&#31639;&#26426;&#30028;&#39047;&#20855;&#24433;&#21709;&#21147;&#65292;&#36825;&#20351;&#20854;&#25104;&#20026;&#20844;&#20247;&#24191;&#27867;&#37319;&#29992;&#30340;&#39640;&#24230;&#29702;&#24819;&#30340;&#36873;&#39033;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65306;&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;EEG&#25968;&#25454;&#12290;&#21512;&#25104;&#25968;&#25454;&#26159;&#20174;&#24102;&#26377;&#24773;&#24863;&#26631;&#31614;&#30340;EEG&#35760;&#24405;&#30340;&#30005;&#26497;-&#39057;&#29575;&#20998;&#24067;&#22270;&#65288;EFDM&#65289;&#20013;&#29983;&#25104;&#30340;&#12290;&#20026;&#20102;&#35780;&#20272;&#25152;&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#30340;&#26377;&#25928;&#24615;&#65292;&#25104;&#21151;&#22320;&#36827;&#34892;&#20102;&#23450;&#24615;&#21644;&#23450;&#37327;&#19982;&#30495;&#23454;EEG&#25968;&#25454;&#30340;&#27604;&#36739;&#12290; 
&lt;/p&gt;
&lt;p&gt;
Electroencephalography (EEG) plays a significant role in the Brain Computer Interface (BCI) domain, due to its non-invasive nature, low cost, and ease of use, making it a highly desirable option for widespread adoption by the general public. This technology is commonly used in conjunction with deep learning techniques, the success of which is largely dependent on the quality and quantity of data used for training. To address the challenge of obtaining sufficient EEG data from individual participants while minimizing user effort and maintaining accuracy, this study proposes an advanced methodology for data augmentation: generating synthetic EEG data using denoising diffusion probabilistic models. The synthetic data are generated from electrode-frequency distribution maps (EFDMs) of emotionally labeled EEG recordings. To assess the validity of the synthetic data generated, both a qualitative and a quantitative comparison with real EEG data were successfully conducted. This study opens up
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#36890;&#36807;&#26102;&#38388;&#36807;&#31243;&#36827;&#34892;&#20107;&#20214;&#24207;&#21015;&#24314;&#27169;&#30340;&#27010;&#29575;&#27169;&#22411;&#65292;&#20998;&#31867;&#20026;&#31616;&#21333;&#30340;&#12289;&#26631;&#35760;&#30340;&#21644;&#26102;&#31354;&#28857;&#36807;&#31243;&#65292;&#24182;&#22238;&#39038;&#20102;&#29616;&#26377;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#21487;&#29992;&#20110;&#35299;&#20915;&#39044;&#27979;&#21644;&#24314;&#27169;&#26041;&#38754;&#30340;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2303.06067</link><description>&lt;p&gt;
&#36890;&#36807;&#26102;&#38388;&#36807;&#31243;&#24314;&#27169;&#20107;&#20214;&#19982;&#20132;&#20114;-&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Modeling Events and Interactions through Temporal Processes -- A Survey. (arXiv:2303.06067v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.06067](http://arxiv.org/abs/2303.06067)
&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24773;&#20917;&#19979;&#65292;&#35768;&#22810;&#29616;&#35937;&#20250;&#20135;&#29983;&#19968;&#31995;&#21015;&#36830;&#32493;&#21457;&#29983;&#30340;&#20107;&#20214;&#12290;&#28857;&#36807;&#31243;&#25552;&#20379;&#20102;&#19968;&#31181;&#33258;&#28982;&#30340;&#25968;&#23398;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#36825;&#20123;&#20107;&#20214;&#24207;&#21015;&#36827;&#34892;&#24314;&#27169;&#12290;&#22312;&#26412;&#27425;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#36890;&#36807;&#26102;&#38388;&#36807;&#31243;&#36827;&#34892;&#20107;&#20214;&#24207;&#21015;&#24314;&#27169;&#30340;&#27010;&#29575;&#27169;&#22411;&#12290;&#25105;&#20204;&#20462;&#35746;&#20107;&#20214;&#24314;&#27169;&#30340;&#27010;&#24565;&#65292;&#24182;&#25552;&#20379;&#20102;&#34920;&#24449;&#35813;&#20027;&#39064;&#25991;&#29486;&#30340;&#25968;&#23398;&#22522;&#30784;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#26412;&#20307;&#35770;&#65292;&#20197;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#19977;&#20010;&#31995;&#21015;(&#31616;&#21333;&#30340;&#65292;&#26631;&#35760;&#30340;&#21644;&#26102;&#31354;&#28857;&#36807;&#31243;)&#26469;&#20998;&#31867;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;&#23545;&#20110;&#27599;&#20010;&#31995;&#21015;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#29616;&#26377;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#25552;&#20986;&#30340;&#25216;&#26415;&#21487;&#20197;&#29992;&#20110;&#35299;&#20915;&#39044;&#27979;&#21644;&#24314;&#27169;&#26041;&#38754;&#30340;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
In real-world scenario, many phenomena produce a collection of events that occur in continuous time. Point Processes provide a natural mathematical framework for modeling these sequences of events. In this survey, we investigate probabilistic models for modeling event sequences through temporal processes. We revise the notion of event modeling and provide the mathematical foundations that characterize the literature on the topic. We define an ontology to categorize the existing approaches in terms of three families: simple, marked, and spatio-temporal point processes. For each family, we systematically review the existing approaches based based on deep learning. Finally, we analyze the scenarios where the proposed techniques can be used for addressing prediction and modeling aspects.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#26032;&#39062;&#30340;&#21160;&#24577;&#19979;&#21322;&#36523;&#36127;&#21387;&#27169;&#22411;&#27169;&#25311;&#25345;&#32493;&#24615;&#22833;&#34880;&#27700;&#24179;&#65292;&#24182;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#26694;&#26550;&#20998;&#31867;&#37492;&#21035;&#19981;&#21516;&#31243;&#24230;&#30340;&#22833;&#34880;&#27700;&#24179;&#65292;&#20026;&#24613;&#35786;&#20998;&#32423;&#25552;&#20379;&#26032;&#24605;&#36335;&#12290;</title><link>http://arxiv.org/abs/2303.06064</link><description>&lt;p&gt;
&#38750;&#20405;&#20837;&#24335;&#27874;&#24418;&#20998;&#26512;&#29992;&#20110;&#36890;&#36807;&#27169;&#25311;&#22833;&#34880;&#36827;&#34892;&#24613;&#35786;&#20998;&#32423;&#65306;&#19968;&#39033;&#20351;&#29992;&#26032;&#39062;&#30340;&#21160;&#24577;&#19979;&#21322;&#36523;&#36127;&#21387;&#27169;&#22411;&#30340;&#23454;&#39564;&#30740;&#31350;&#12290;(arXiv:2303.06064v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
Non-invasive Waveform Analysis for Emergency Triage via Simulated Hemorrhage: An Experimental Study using Novel Dynamic Lower Body Negative Pressure Model. (arXiv:2303.06064v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.06064](http://arxiv.org/abs/2303.06064)
&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#38750;&#20405;&#20837;&#24615;&#29983;&#29702;&#20449;&#21495;&#30340;&#20808;&#36827;&#27874;&#24418;&#20998;&#26512;&#33021;&#22815;&#35786;&#26029;&#22833;&#34880;&#31243;&#24230;&#30340;&#31243;&#24230;&#20173;&#19981;&#36275;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#26694;&#26550;&#20998;&#31867;&#36827;&#34892;&#19979;&#21322;&#36523;&#36127;&#21387;&#65288;LBNP&#65289;&#27169;&#25311;&#30340;&#25345;&#32493;&#24615;&#22833;&#34880;&#27700;&#24179;&#30340;&#37492;&#21035;&#33021;&#21147;&#12290;&#25105;&#20204;&#20351;&#29992;&#21160;&#24577;LBNP&#21327;&#35758;&#65292;&#32780;&#19981;&#26159;&#20256;&#32479;&#30340;&#27169;&#22411;&#65292;&#22312;&#35813;&#21327;&#35758;&#20013;&#65292;LBNP&#20197;&#21487;&#39044;&#27979;&#30340;&#36880;&#27493;&#19979;&#38477;&#30340;&#26041;&#24335;&#26045;&#21152;&#12290;&#35813;&#21160;&#24577;LBNP&#29256;&#26412;&#26377;&#21161;&#20110;&#35299;&#20915;&#26102;&#38388;&#20381;&#36182;&#24615;&#38382;&#39064;&#65292;&#22240;&#20026;&#22312;&#29616;&#23454;&#29983;&#27963;&#20013;&#30340;&#38498;&#21069;&#35774;&#32622;&#20013;&#65292;&#30001;&#20110;&#20307;&#31215;&#22797;&#33487;&#65292;&#34880;&#31649;&#20869;&#30340;&#34880;&#23481;&#37327;&#21487;&#33021;&#20250;&#27874;&#21160;&#12290;&#36890;&#36807;&#20998;&#21106;&#24213;&#23618;&#38750;&#20405;&#20837;&#24335;&#20449;&#21495;&#24182;&#20351;&#29992;&#30456;&#24212;&#30340;LBNP&#30446;&#26631;&#32423;&#21035;&#23545;&#29255;&#27573;&#36827;&#34892;&#26631;&#35760;&#65292;&#23454;&#29616;&#20102;&#29992;&#20110;&#19977;&#20803;&#20998;&#31867;&#30340;&#30417;&#30563;&#24335;DL&#26694;&#26550;&#12290;&#20855;&#26377;&#20004;&#20010;&#36755;&#20837;&#30340;DL&#27169;&#22411;&#32463;&#36807;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
The extent to which advanced waveform analysis of non-invasive physiological signals can diagnose levels of hypovolemia remains insufficiently explored. The present study explores the discriminative ability of a deep learning (DL) framework to classify levels of ongoing hypovolemia, simulated via novel dynamic lower body negative pressure (LBNP) model among healthy volunteers. We used a dynamic LBNP protocol as opposed to the traditional model, where LBNP is applied in a predictable step-wise, progressively descending manner. This dynamic LBNP version assists in circumventing the problem posed in terms of time dependency, as in real-life pre-hospital settings, intravascular blood volume may fluctuate due to volume resuscitation. A supervised DL-based framework for ternary classification was realized by segmenting the underlying noninvasive signal and labeling segments with corresponding LBNP target levels. The proposed DL model with two inputs was trained with respective time-frequency
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#26041;&#27861;&#26469;&#25512;&#23548;&#38543;&#26426;&#22810;&#33218;&#32769;&#34382;&#26426;&#31639;&#27861;&#30340;&#36951;&#25022;&#30028;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#22810;&#31181;&#20998;&#24067;&#27169;&#22411;&#65292;&#35777;&#26126;MED&#22312;&#25152;&#26377;&#27169;&#22411;&#19979;&#37117;&#26159;&#28176;&#36827;&#26368;&#20248;&#30340;&#65292;&#21516;&#26102;&#20026;&#26576;&#20123;TS&#31639;&#27861;&#25552;&#20379;&#20102;&#31616;&#21333;&#30340;&#36951;&#25022;&#20998;&#26512;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#20998;&#26512;&#26032;&#30340;&#38750;&#21442;&#25968;TS&#31639;&#27861;&#65288;h-NPTS&#65289;&#65292;&#26412;&#25991;&#36827;&#19968;&#27493;&#35828;&#26126;&#20102;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.06058</link><description>&lt;p&gt;
&#22810;&#33218;&#32769;&#34382;&#26426;&#31639;&#27861;&#20998;&#26512;&#30340;&#36890;&#29992;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A General Recipe for the Analysis of Randomized Multi-Armed Bandit Algorithms. (arXiv:2303.06058v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.06058](http://arxiv.org/abs/2303.06058)
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#26041;&#27861;&#26469;&#25512;&#23548;&#38543;&#26426;&#22810;&#33218;&#32769;&#34382;&#26426;&#31639;&#27861;&#30340;&#36951;&#25022;&#30028;&#12290; &#23427;&#21253;&#25324;&#26816;&#26597;&#27599;&#20010;&#33218;&#30340;&#37319;&#26679;&#27010;&#29575;&#21644;&#20998;&#24067;&#26063;&#19978;&#30340;&#19968;&#32452;&#20805;&#20998;&#26465;&#20214;&#65292;&#20197;&#35777;&#26126;&#23545;&#25968;&#36951;&#25022;&#12290; &#25105;&#20204;&#30452;&#25509;&#23558;&#20854;&#24212;&#29992;&#20110;&#20004;&#20010;&#33879;&#21517;&#30340;&#32769;&#34382;&#26426;&#31639;&#27861;&#65306;&#26368;&#23567;&#32463;&#39564;&#24046;&#24322;&#65288;MED&#65289;&#21644;&#27748;&#26222;&#26862;&#25277;&#26679;&#65288;TS&#65289;&#65292;&#21253;&#25324;&#21333;&#21442;&#25968;&#25351;&#25968;&#26063;&#65292;&#39640;&#26031;&#20998;&#24067;&#65292;&#26377;&#30028;&#20998;&#24067;&#25110;&#28385;&#36275;&#20854;&#30697;&#26465;&#20214;&#30340;&#20998;&#24067;&#30340;&#21508;&#31181;&#27169;&#22411;&#12290; &#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#35777;&#26126;MED&#22312;&#25152;&#26377;&#36825;&#20123;&#27169;&#22411;&#19979;&#37117;&#26159;&#28176;&#36827;&#26368;&#20248;&#30340;&#65292;&#20294;&#20063;&#20026;&#26576;&#20123;TS&#31639;&#27861;&#25552;&#20379;&#20102;&#31616;&#21333;&#30340;&#36951;&#25022;&#20998;&#26512;&#65292;&#20854;&#20013;&#20248;&#36234;&#24615;&#24050;&#30693;&#12290; &#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#26032;&#30340;&#38750;&#21442;&#25968;TS&#31639;&#27861;&#65288;h-NPTS&#65289;&#36827;&#19968;&#27493;&#35828;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#65292;&#35813;&#31639;&#27861;&#36866;&#29992;&#20110;&#20855;&#26377;&#26377;&#38480;h&#30697;&#30340;&#26576;&#20123;&#26410;&#38480;&#21046;&#22870;&#21169;&#20998;&#24067;&#26063;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we propose a general methodology to derive regret bounds for randomized multi-armed bandit algorithms. It consists in checking a set of sufficient conditions on the sampling probability of each arm and on the family of distributions to prove a logarithmic regret. As a direct application we revisit two famous bandit algorithms, Minimum Empirical Divergence (MED) and Thompson Sampling (TS), under various models for the distributions including single parameter exponential families, Gaussian distributions, bounded distributions, or distributions satisfying some conditions on their moments. In particular, we prove that MED is asymptotically optimal for all these models, but also provide a simple regret analysis of some TS algorithms for which the optimality is already known. We then further illustrate the interest of our approach, by analyzing a new Non-Parametric TS algorithm (h-NPTS), adapted to some families of unbounded reward distributions with a bounded h-moment. This mo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;MLP&#26550;&#26500;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;TSMixer&#65292;&#36890;&#36807;&#28151;&#21512;&#25805;&#20316;&#65292;&#21033;&#29992;&#26102;&#38388;&#21644;&#29305;&#24449;&#32500;&#24230;&#39640;&#25928;&#25552;&#21462;&#20449;&#24687;&#65292;&#22312;M5&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#20110;&#26368;&#26032;&#30340;&#26367;&#20195;&#26041;&#26696;</title><link>http://arxiv.org/abs/2303.06053</link><description>&lt;p&gt;
TSMixer&#65306;&#19968;&#31181;&#20840;MLP&#26550;&#26500;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
TSMixer: An all-MLP Architecture for Time Series Forecasting. (arXiv:2303.06053v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.06053](http://arxiv.org/abs/2303.06053)
&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#36890;&#24120;&#26159;&#22810;&#21464;&#37327;&#19988;&#20855;&#26377;&#22797;&#26434;&#30340;&#21160;&#24577;&#29305;&#24615;&#12290;&#20256;&#32479;&#30340;&#39640;&#23481;&#37327;&#20307;&#31995;&#32467;&#26500;&#65292;&#22914;&#22522;&#20110;&#36882;&#24402;&#25110;&#27880;&#24847;&#21147;&#30340;&#39034;&#24207;&#27169;&#22411;&#24050;&#32463;&#21464;&#24471;&#27969;&#34892;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#31616;&#21333;&#30340;&#21333;&#21464;&#37327;&#32447;&#24615;&#27169;&#22411;&#21487;&#20197;&#32988;&#36807;&#37027;&#20123;&#28145;&#24230;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#32447;&#24615;&#27169;&#22411;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#26102;&#38388;&#24207;&#21015;&#28151;&#21512;&#22120;&#65288;TSMixer&#65289;&#65292;&#19968;&#31181;&#36890;&#36807;&#22534;&#21472;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;MLP&#65289;&#35774;&#35745;&#30340;&#20307;&#31995;&#32467;&#26500;&#12290;TSMixer&#22522;&#20110;&#27839;&#26102;&#38388;&#21644;&#29305;&#24449;&#32500;&#24230;&#36827;&#34892;&#28151;&#21512;&#25805;&#20316;&#65292;&#20197;&#26377;&#25928;&#25552;&#21462;&#20449;&#24687;&#12290;&#22312;&#27969;&#34892;&#30340;&#23398;&#26415;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#26131;&#20110;&#23454;&#29616;&#30340;TSMixer&#19982;&#21033;&#29992;&#29305;&#23450;&#22522;&#20934;&#30340;&#24402;&#32435;&#20559;&#24046;&#30340;&#19987;&#19994;&#26368;&#26032;&#27169;&#22411;&#30456;&#24403;&#12290;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#22823;&#35268;&#27169;&#30340;M5&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#21363;&#23454;&#38469;&#30340;&#38646;&#21806;&#25968;&#25454;&#38598;&#19978;&#65292;TSMixer&#34920;&#29616;&#20248;&#20110;&#26368;&#26032;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#24378;&#35843;&#20102;&#39640;&#25928;&#25552;&#21462;&#20449;&#24687;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
Real-world time-series datasets are often multivariate with complex dynamics. Commonly-used high capacity architectures like recurrent- or attention-based sequential models have become popular. However, recent work demonstrates that simple univariate linear models can outperform those deep alternatives. In this paper, we investigate the capabilities of linear models for time-series forecasting and present Time-Series Mixer (TSMixer), an architecture designed by stacking multi-layer perceptrons (MLPs). TSMixer is based on mixing operations along time and feature dimensions to extract information efficiently. On popular academic benchmarks, the simple-to-implement TSMixer is comparable to specialized state-of-the-art models that leverage the inductive biases of specific benchmarks. On the challenging and large scale M5 benchmark, a real-world retail dataset, TSMixer demonstrates superior performance compared to the state-of-the-art alternatives. Our results underline the importance of ef
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#20102;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#21644;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#26469;&#39044;&#27979;&#33258;&#26432;&#39118;&#38505;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20915;&#31574;&#26641;&#65288;DT&#65289;&#12289;&#38543;&#26426;&#26862;&#26519;&#65288;RF&#65289;&#21644;&#26497;&#38480;&#26799;&#24230;&#25552;&#21319;&#65288;XGBoost&#65289;&#27169;&#22411;&#21462;&#24471;&#20102;&#26368;&#20339;&#32467;&#26524;&#12290;&#24868;&#24594;&#38382;&#39064;&#12289;&#25233;&#37057;&#30151;&#21644;&#31038;&#20132;&#38548;&#31163;&#26159;&#39044;&#27979;&#33258;&#26432;&#39118;&#38505;&#30340;&#20027;&#35201;&#21464;&#37327;&#65292;&#32780;&#25910;&#20837;&#22909;&#12289;&#32844;&#19994;&#21463;&#20154;&#23562;&#25964;&#21644;&#25509;&#21463;&#22823;&#23398;&#25945;&#32946;&#30340;&#24739;&#32773;&#39118;&#38505;&#26368;&#23567;&#12290;&#36825;&#20123;&#32467;&#26524;&#35777;&#26126;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;XAI&#26694;&#26550;&#22312;&#33258;&#26432;&#39118;&#38505;&#39044;&#27979;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#20197;&#21450;&#23427;&#20204;&#22312;&#21307;&#30103;&#20445;&#20581;&#21644;&#20844;&#20849;&#21355;&#29983;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.06052</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#22312;&#33258;&#26432;&#39118;&#38505;&#35780;&#20272;&#20013;&#30340;&#20998;&#26512;&#19982;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Analysis and Evaluation of Explainable Artificial Intelligence on Suicide Risk Assessment. (arXiv:2303.06052v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.06052](http://arxiv.org/abs/2303.06052)
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#25216;&#26415;&#22312;&#39044;&#27979;&#33258;&#26432;&#39118;&#38505;&#21644;&#30830;&#23450;&#20027;&#35201;&#21407;&#22240;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#21033;&#29992;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#39044;&#27979;&#30456;&#20851;&#39118;&#38505;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;SHapley Additive exPlanations&#65288;SHAP&#65289;&#21644;&#30456;&#20851;&#20998;&#26512;&#26469;&#25490;&#21517;&#39044;&#27979;&#20013;&#21464;&#37327;&#30340;&#37325;&#35201;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20915;&#31574;&#26641;&#65288;DT&#65289;&#12289;&#38543;&#26426;&#26862;&#26519;&#65288;RF&#65289;&#21644;&#26497;&#38480;&#26799;&#24230;&#25552;&#21319;&#65288;XGBoost&#65289;&#27169;&#22411;&#21462;&#24471;&#20102;&#26368;&#20339;&#32467;&#26524;&#65292;&#20854;&#20013;DT&#20855;&#26377;&#26368;&#20339;&#34920;&#29616;&#65292;&#20934;&#30830;&#29575;&#20026;95.23%&#65292;&#26354;&#32447;&#19979;&#38754;&#31215;&#65288;AUC&#65289;&#20026;0.95&#12290;&#26681;&#25454;SHAP&#30340;&#32467;&#26524;&#65292;&#24868;&#24594;&#38382;&#39064;&#12289;&#25233;&#37057;&#30151;&#21644;&#31038;&#20132;&#38548;&#31163;&#26159;&#39044;&#27979;&#33258;&#26432;&#39118;&#38505;&#30340;&#20027;&#35201;&#21464;&#37327;&#65292;&#32780;&#25910;&#20837;&#22909;&#12289;&#32844;&#19994;&#21463;&#20154;&#23562;&#25964;&#21644;&#25509;&#21463;&#22823;&#23398;&#25945;&#32946;&#30340;&#24739;&#32773;&#39118;&#38505;&#26368;&#23567;&#12290;&#32467;&#26524;&#35777;&#26126;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;XAI&#26694;&#26550;&#22312;&#33258;&#26432;&#39118;&#38505;&#39044;&#27979;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#20197;&#21450;&#23427;&#20204;&#22312;&#21307;&#30103;&#20445;&#20581;&#21644;&#20844;&#20849;&#21355;&#29983;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study investigates the effectiveness of Explainable Artificial Intelligence (XAI) techniques in predicting suicide risks and identifying the dominant causes for such behaviours. Data augmentation techniques and ML models are utilized to predict the associated risk. Furthermore, SHapley Additive exPlanations (SHAP) and correlation analysis are used to rank the importance of variables in predictions. Experimental results indicate that Decision Tree (DT), Random Forest (RF) and eXtreme Gradient Boosting (XGBoost) models achieve the best results while DT has the best performance with an accuracy of 95:23% and an Area Under Curve (AUC) of 0.95. As per SHAP results, anger problems, depression, and social isolation are the leading variables in predicting the risk of suicide, and patients with good incomes, respected occupations, and university education have the least risk. Results demonstrate the effectiveness of machine learning and XAI framework for suicide risk prediction, and they c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;VALERIAN&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#37326;&#22806;&#21487;&#31359;&#25140;&#24335;&#20256;&#24863;&#22120;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#30340;&#19981;&#21464;&#29305;&#24449;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20026;&#27599;&#20010;&#21463;&#35797;&#32773;&#35757;&#32451;&#20855;&#26377;&#21333;&#29420;&#20219;&#21153;&#29305;&#23450;&#23618;&#30340;&#22810;&#20219;&#21153;&#27169;&#22411;&#65292;VALERIAN&#20801;&#35768;&#21333;&#29420;&#22788;&#29702;&#22122;&#22768;&#26631;&#31614;&#12290;</title><link>http://arxiv.org/abs/2303.06048</link><description>&lt;p&gt;
VALERIAN&#65306;&#37326;&#22806;IMU&#20256;&#24863;&#22120;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#30340;&#19981;&#21464;&#29305;&#24449;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
VALERIAN: Invariant Feature Learning for IMU Sensor-based Human Activity Recognition in the Wild. (arXiv:2303.06048v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.06048](http://arxiv.org/abs/2303.06048)
&lt;/p&gt;
&lt;p&gt;
&#20174;&#33258;&#28982;&#29615;&#22659;&#20013;&#25910;&#38598;&#21040;&#30340;&#25968;&#25454;&#36890;&#24120;&#21253;&#21547;&#26174;&#30528;&#30340;&#26631;&#31614;&#22122;&#22768;&#65292;&#22240;&#27492;&#20174;&#21463;&#25511;&#30340;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#23454;&#38469;&#37096;&#32626;&#20013;&#30340;&#36866;&#29992;&#24615;&#36739;&#24046;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#26816;&#26597;&#20102;&#20004;&#20010;&#37326;&#22806;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#25968;&#25454;&#38598;&#21644;&#19968;&#31181;&#23398;&#20064;&#22122;&#22768;&#26631;&#31614;&#30340;&#26368;&#26032;&#26041;&#27861;DivideMix&#65292;&#20197;&#20102;&#35299;&#35757;&#32451;&#25968;&#25454;&#20013;&#22122;&#22768;&#26631;&#31614;&#30340;&#31243;&#24230;&#21644;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#20998;&#26512;&#34920;&#26126;&#65292;&#19981;&#21516;&#21463;&#35797;&#32773;&#20043;&#38388;&#30340;&#23454;&#36136;&#24615;&#39046;&#22495;&#24046;&#24322;&#23548;&#33268;&#23398;&#20064;&#22122;&#22768;&#26631;&#31614;&#30340;&#26041;&#27861;&#36829;&#21453;&#19968;&#20010;&#20851;&#38190;&#30340;&#22522;&#26412;&#20551;&#35774;&#65292;&#21363;&#31070;&#32463;&#32593;&#32476;&#20542;&#21521;&#20110;&#22312;&#26089;&#26399;&#35757;&#32451;&#26102;&#25311;&#21512;&#26356;&#31616;&#21333;&#65288;&#22240;&#27492;&#26356;&#24178;&#20928;&#65289;&#30340;&#25968;&#25454;&#12290;&#21463;&#36825;&#20123;&#35265;&#35299;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;VALERIAN&#65292;&#19968;&#31181;&#29992;&#20110;&#37326;&#22806;&#21487;&#31359;&#25140;&#24335;&#20256;&#24863;&#22120;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#30340;&#19981;&#21464;&#29305;&#24449;&#23398;&#20064;&#26041;&#27861;&#12290;&#36890;&#36807;&#20026;&#27599;&#20010;&#21463;&#35797;&#32773;&#35757;&#32451;&#20855;&#26377;&#21333;&#29420;&#20219;&#21153;&#29305;&#23450;&#23618;&#30340;&#22810;&#20219;&#21153;&#27169;&#22411;&#65292;VALERIAN&#20801;&#35768;&#21333;&#29420;&#22788;&#29702;&#22122;&#22768;&#26631;&#31614;
&lt;/p&gt;
&lt;p&gt;
Deep neural network models for IMU sensor-based human activity recognition (HAR) that are trained from controlled, well-curated datasets suffer from poor generalizability in practical deployments. However, data collected from naturalistic settings often contains significant label noise. In this work, we examine two in-the-wild HAR datasets and DivideMix, a state-of-the-art learning with noise labels (LNL) method to understand the extent and impacts of noisy labels in training data. Our empirical analysis reveals that the substantial domain gaps among diverse subjects cause LNL methods to violate a key underlying assumption, namely, neural networks tend to fit simpler (and thus clean) data in early training epochs. Motivated by the insights, we design VALERIAN, an invariant feature learning method for in-the-wild wearable sensor-based HAR. By training a multi-task model with separate task-specific layers for each subject, VALERIAN allows noisy labels to be dealt with individually while 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#29992;&#20110;&#30452;&#25509;&#36890;&#36807;&#33041;&#30005;&#22270;&#25968;&#25454;&#30830;&#23450;&#20957;&#35270;&#20301;&#32622;&#65292;&#20294;&#31934;&#24230;&#20173;&#20302;&#20110;&#22522;&#20110;&#22270;&#20687;&#30340;&#30524;&#21160;&#36861;&#36394;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2303.06039</link><description>&lt;p&gt;
&#19968;&#27493;&#25509;&#36817;&#22522;&#20110;&#33041;&#30005;&#22270;&#30340;&#30524;&#21160;&#36861;&#36394;
&lt;/p&gt;
&lt;p&gt;
One step closer to EEG based eye tracking. (arXiv:2303.06039v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.06039](http://arxiv.org/abs/2303.06039)
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#26041;&#27861;&#21644;&#31639;&#27861;&#65292;&#23427;&#20204;&#21487;&#20197;&#20351;&#24863;&#20852;&#36259;&#30340;&#21306;&#22495;&#36866;&#24212;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#65292;&#21487;&#20197;&#30452;&#25509;&#20351;&#29992;&#33041;&#30005;&#22270;&#25968;&#25454;&#26469;&#30830;&#23450;&#20957;&#35270;&#20301;&#32622;&#12290;&#22522;&#20110;&#33041;&#30005;&#22270;&#30340;&#30524;&#21160;&#36861;&#36394;&#26159;&#30524;&#21160;&#36861;&#36394;&#39046;&#22495;&#20013;&#19968;&#20010;&#26032;&#30340;&#19988;&#22256;&#38590;&#30340;&#30740;&#31350;&#35838;&#39064;&#65292;&#20294;&#23427;&#25552;&#20379;&#20102;&#19968;&#31181;&#26367;&#20195;&#22522;&#20110;&#22270;&#20687;&#30340;&#30524;&#21160;&#36861;&#36394;&#65292;&#19988;&#20854;&#36755;&#20837;&#25968;&#25454;&#38598;&#21487;&#19982;&#20256;&#32479;&#30340;&#22270;&#20687;&#22788;&#29702;&#30456;&#23218;&#32654;&#12290;&#25152;&#21576;&#29616;&#30340;DNN&#21033;&#29992;&#20102;&#33041;&#30005;&#22270;&#20449;&#21495;&#30340;&#31354;&#38388;&#20381;&#36182;&#24615;&#65292;&#24182;&#20351;&#29992;&#31867;&#20284;&#20110;&#31354;&#38388;&#28388;&#27874;&#30340;&#21367;&#31215;&#36827;&#34892;&#39044;&#22788;&#29702;&#33041;&#30005;&#22270;&#20449;&#21495;&#12290;&#19982;&#29616;&#26377;&#25216;&#26415;&#30456;&#27604;&#65292;&#25105;&#20204;&#20174;&#33041;&#30005;&#22270;&#20449;&#21495;&#20013;&#25552;&#39640;&#20102;&#30452;&#25509;&#20957;&#35270;&#30830;&#23450;&#30340;&#31934;&#24230;&#65292;MAE(&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;)&#25552;&#39640;&#20102;3.5&#21400;&#31859;&#65292;&#20294;&#19981;&#24184;&#30340;&#26159;&#65292;&#19982;&#22522;&#20110;&#22270;&#20687;&#30340;&#30524;&#21160;&#36861;&#36394;&#30456;&#27604;&#65292;&#31934;&#24230;&#20173;&#28982;&#26126;&#26174;&#36739;&#20302;&#65292;&#22240;&#27492;&#20173;&#19981;&#33021;&#23454;&#29616;&#30452;&#25509;&#36866;&#29992;&#30340;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present two approaches and algorithms that adapt areas of interest We present a new deep neural network (DNN) that can be used to directly determine gaze position using EEG data. EEG-based eye tracking is a new and difficult research topic in the field of eye tracking, but it provides an alternative to image-based eye tracking with an input data set comparable to conventional image processing. The presented DNN exploits spatial dependencies of the EEG signal and uses convolutions similar to spatial filtering, which is used for preprocessing EEG signals. By this, we improve the direct gaze determination from the EEG signal compared to the state of the art by 3.5 cm MAE (Mean absolute error), but unfortunately still do not achieve a directly applicable system, since the inaccuracy is still significantly higher compared to image-based eye trackers.  Link: https://es-cloud.cs.uni-tuebingen.de/d/8e2ab8c3fdd444e1a135/?p=%2FEEGGaze&amp;mode=list
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;&#35270;&#35273;&#30340;&#35302;&#35273;&#20256;&#24863;&#22120;&#36827;&#34892;&#22810;&#23545;&#35937;&#32452;&#35013;&#30340;&#20132;&#20114;&#24335;&#24863;&#30693;&#26041;&#27861;&#65292;&#20854;&#20013;&#26426;&#22120;&#20154;&#20351;&#29992;&#35302;&#35273;&#20256;&#24863;&#22120;&#21644;&#20351;&#29992;&#31890;&#23376;&#28388;&#27874;&#22120;&#30340;&#21453;&#39304;&#26426;&#21046;&#36880;&#27493;&#25913;&#36827;&#20854;&#23545;&#36866;&#21512;&#32452;&#35013;&#30340;&#23545;&#35937;&#30340;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2303.06034</link><description>&lt;p&gt;
Tactile-Filter: &#29992;&#20110;&#37197;&#20214;&#32452;&#35013;&#30340;&#20132;&#20114;&#24335;&#35302;&#35273;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Tactile-Filter: Interactive Tactile Perception for Part Mating. (arXiv:2303.06034v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.06034](http://arxiv.org/abs/2303.06034)
&lt;/p&gt;
&lt;p&gt;
&#20154;&#20204;&#20381;&#36182;&#35302;&#35273;&#21644;&#35302;&#35273;&#20256;&#24863;&#22120;&#23436;&#25104;&#35768;&#22810;&#29087;&#32451;&#30340;&#25805;&#20316;&#20219;&#21153;&#12290; &#35302;&#35273;&#20256;&#24863;&#22120;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#35768;&#22810;&#20851;&#20110;&#25509;&#35302;&#24418;&#24577;&#20197;&#21450;&#22312;&#20219;&#20309;&#20132;&#20114;&#26399;&#38388;&#20851;&#20110;&#23545;&#35937;&#30340;&#20960;&#20309;&#20449;&#24687;&#12290; &#20973;&#20511;&#36825;&#31181;&#21160;&#21147;&#65292;&#22522;&#20110;&#35270;&#35273;&#30340;&#35302;&#35273;&#20256;&#24863;&#22120;&#34987;&#24191;&#27867;&#29992;&#20110;&#21508;&#31181;&#26426;&#22120;&#20154;&#24863;&#30693;&#21644;&#25511;&#21046;&#20219;&#21153;&#20013;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;&#35270;&#35273;&#30340;&#35302;&#35273;&#20256;&#24863;&#22120;&#36827;&#34892;&#22810;&#23545;&#35937;&#32452;&#35013;&#30340;&#20132;&#20114;&#24335;&#24863;&#30693;&#26041;&#27861;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#23545;&#38646;&#20214;&#32452;&#35013;&#36807;&#31243;&#20013;&#30340;&#35302;&#35273;&#24863;&#30693;&#24863;&#20852;&#36259;&#65292;&#20854;&#20013;&#26426;&#22120;&#20154;&#21487;&#20197;&#20351;&#29992;&#35302;&#35273;&#20256;&#24863;&#22120;&#21644;&#20351;&#29992;&#31890;&#23376;&#28388;&#27874;&#22120;&#30340;&#21453;&#39304;&#26426;&#21046;&#36880;&#27493;&#25913;&#36827;&#20854;&#23545;&#36866;&#21512;&#32452;&#35013;&#30340;&#23545;&#35937;&#30340;&#20272;&#35745;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#35757;&#32451;&#20102;&#19968;&#20010;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#21033;&#29992;&#35302;&#35273;&#22270;&#20687;&#26469;&#39044;&#27979;&#24444;&#27492;&#22865;&#21512;&#30340;&#20219;&#24847;&#24418;&#29366;&#23545;&#35937;&#20043;&#38388;&#30340;&#27010;&#29575;&#23545;&#24212;&#20851;&#31995;&#12290;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#29992;&#20110;&#35774;&#35745;&#31890;&#23376;&#28388;&#27874;&#22120;&#65292;&#20854;&#20855;&#26377;&#20004;&#20010;&#29992;&#36884;&#12290; &#39318;&#20808;&#65292;&#32473;&#23450;
&lt;/p&gt;
&lt;p&gt;
Humans rely on touch and tactile sensing for a lot of dexterous manipulation tasks. Our tactile sensing provides us with a lot of information regarding contact formations as well as geometric information about objects during any interaction. With this motivation, vision-based tactile sensors are being widely used for various robotic perception and control tasks. In this paper, we present a method for interactive perception using vision-based tactile sensors for multi-object assembly. In particular, we are interested in tactile perception during part mating, where a robot can use tactile sensors and a feedback mechanism using particle filter to incrementally improve its estimate of objects that fit together for assembly. To do this, we first train a deep neural network that makes use of tactile images to predict the probabilistic correspondence between arbitrarily shaped objects that fit together. The trained model is used to design a particle filter which is used twofold. First, given 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;EEG&#20449;&#21495;&#36827;&#34892;&#25233;&#37057;&#30151;&#35786;&#26029;&#21644;&#39044;&#27979;&#33647;&#29289;&#21453;&#24212;&#30340;&#26041;&#27861;&#65292;&#37319;&#29992;transformers&#33021;&#22815;&#26377;&#25928;&#22320;&#35780;&#20272;&#26102;&#38388;&#24207;&#21015;&#30340;&#26102;&#38388;&#20381;&#36182;&#24615;&#65292;&#24182;&#33719;&#24471;&#20102;&#24456;&#39640;&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.06033</link><description>&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;EEG&#20449;&#21495;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#21644;transformers&#36827;&#34892;&#25233;&#37057;&#30151;&#35786;&#26029;&#21644;&#33647;&#29289;&#21453;&#24212;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Depression Diagnosis and Drug Response Prediction via Recurrent Neural Networks and Transformers Utilizing EEG Signals. (arXiv:2303.06033v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.06033](http://arxiv.org/abs/2303.06033)
&lt;/p&gt;
&lt;p&gt;
&#26089;&#26399;&#35786;&#26029;&#21644;&#27835;&#30103;&#25233;&#37057;&#30151;&#23545;&#20110;&#26377;&#25928;&#27835;&#30103;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#25233;&#37057;&#30151;&#20316;&#20026;&#26368;&#24120;&#35265;&#30340;&#24515;&#29702;&#30142;&#30149;&#20043;&#19968;&#65292;&#22312;&#30740;&#31350;&#21644;&#20020;&#24202;&#23454;&#36341;&#26041;&#38754;&#20173;&#28982;&#19981;&#22826;&#29702;&#35299;&#12290;&#22312;&#19981;&#21516;&#30340;&#27835;&#30103;&#26041;&#27861;&#20013;&#65292;&#33647;&#29289;&#22788;&#26041;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#28982;&#32780;&#36825;&#31181;&#33647;&#29289;&#27835;&#30103;&#23545;&#35768;&#22810;&#24739;&#32773;&#24182;&#19981;&#26377;&#25928;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25233;&#37057;&#30151;&#35786;&#26029;&#30340;&#26041;&#27861;&#20197;&#21450;&#19968;&#31181;&#29992;&#20110;&#39044;&#27979;&#24739;&#26377;&#25233;&#37057;&#30151;&#24739;&#32773;&#33647;&#29289;&#21453;&#24212;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;EEG&#20449;&#21495;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#37319;&#29992;transformers&#65292;&#36825;&#26159;&#20855;&#26377;&#26032;&#39062;&#26550;&#26500;&#30340;&#20462;&#25913;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#26377;&#25928;&#35780;&#20272;&#26102;&#38388;&#24207;&#21015;&#30340;&#26102;&#38388;&#20381;&#36182;&#24615;&#12290;&#25105;&#20204;&#36824;&#23558;&#27169;&#22411;&#19982;&#20247;&#25152;&#21608;&#30693;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#26696;&#65288;&#22914;CNN&#12289;LSTM&#21644;CNN-LSTM&#65289;&#36827;&#34892;&#27604;&#36739;&#12290;&#32467;&#26524;&#65306;transformers&#24179;&#22343;&#21484;&#22238;&#29575;&#20026;99.41%&#65292;&#20998;&#31867;&#27491;&#24120;&#21644;MDD&#21463;&#35797;&#32773;&#30340;&#20934;&#30830;&#24230;&#20026;97.14%&#12290;&#27492;&#22806;&#65292;transformers&#22312;&#20998;&#31867;&#23545;&#21709;&#24212;&#32773;&#21644;&#38750;&#21709;&#24212;&#32773;&#26041;&#38754;&#20063;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Early diagnosis and treatment of depression is essential for effective treatment. Depression, while being one of the most common mental illnesses, is still poorly understood in both research and clinical practice. Among different treatments, drug prescription is widely used, however the drug treatment is not effective for many patients. In this work, we propose a method for major depressive disorder (MDD) diagnosis as well as a method for predicting the drug response in patient with MDD using EEG signals. Method: We employ transformers, which are modified recursive neural networks with novel architecture to evaluate the time dependency of time series effectively. We also compare the model to the well-known deep learning schemes such as CNN, LSTM and CNN-LSTM. Results: The transformer achieves an average recall of 99.41% and accuracy of 97.14% for classifying normal and MDD subjects. Furthermore, the transformer also performed well in classifying responders and non-responders to the
&lt;/p&gt;</description></item><item><title>&#25991;&#31456;&#20351;&#29992;&#26799;&#24230;&#28909;&#22270;&#20998;&#26512;&#20102;VGG-16&#27169;&#22411;&#23545;&#25239;&#25915;&#20987;&#19979;&#30340;&#21709;&#24212;&#29305;&#24615;&#65292;&#21457;&#29616;&#19982;&#39640;&#26031;&#38543;&#26426;&#22122;&#22768;&#30456;&#27604;&#65292;&#23545;&#25239;&#22122;&#22768;&#20250;&#20998;&#25955;&#32593;&#32476;&#20013;&#30340;&#38598;&#20013;&#21306;&#22495;&#65292;&#23548;&#33268;&#20005;&#37325;&#30340;&#34892;&#20026;&#20559;&#24046;&#12290;&#27492;&#22806;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#23545;&#25239;&#24615;&#31034;&#20363;&#21482;&#38656;&#30772;&#22351;&#23569;&#25968;&#20013;&#38388;&#22359;&#21363;&#21487;&#35823;&#23548;&#26368;&#32456;&#20915;&#31574;&#65292;&#24182;&#19988;&#29305;&#23450;&#22359;&#26356;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2303.06032</link><description>&lt;p&gt;
&#25506;&#31350;&#31070;&#32463;&#32593;&#32476;&#30340;&#23545;&#25239;&#25915;&#20987;:&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Exploring Adversarial Attacks on Neural Networks: An Explainable Approach. (arXiv:2303.06032v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.06032](http://arxiv.org/abs/2303.06032)
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;(DL)&#34987;&#24212;&#29992;&#20110;&#21508;&#20010;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#22312;&#33258;&#21160;&#39550;&#39542;&#31561;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#12290;&#22240;&#27492;&#65292;&#30830;&#20445;&#36825;&#20123;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#24182;&#25269;&#28040;&#30001;&#23545;&#25239;&#25915;&#20987;&#24341;&#36215;&#30340;&#19981;&#30830;&#23450;&#34892;&#20026;&#38750;&#24120;&#37325;&#35201;&#12290;&#26412;&#25991;&#20351;&#29992;&#26799;&#24230;&#28909;&#22270;&#20998;&#26512;VGG-16&#27169;&#22411;&#30340;&#21709;&#24212;&#29305;&#24615;&#65292;&#24403;&#36755;&#20837;&#22270;&#20687;&#19982;&#23545;&#25239;&#22122;&#22768;&#21644;&#32479;&#35745;&#30456;&#20284;&#30340;&#39640;&#26031;&#38543;&#26426;&#22122;&#22768;&#28151;&#21512;&#26102;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#36880;&#23618;&#27604;&#36739;&#32593;&#32476;&#21709;&#24212;&#65292;&#20197;&#30830;&#23450;&#38169;&#35823;&#21457;&#29983;&#30340;&#20301;&#32622;&#12290;&#25105;&#20204;&#24471;&#20986;&#20102;&#20960;&#20010;&#26377;&#36259;&#30340;&#21457;&#29616;&#12290;&#39318;&#20808;&#65292;&#19982;&#39640;&#26031;&#38543;&#26426;&#22122;&#22768;&#30456;&#27604;&#65292;&#26377;&#24847;&#29983;&#25104;&#30340;&#23545;&#25239;&#22122;&#22768;&#36890;&#36807;&#20998;&#25955;&#32593;&#32476;&#20013;&#30340;&#38598;&#20013;&#21306;&#22495;&#24341;&#36215;&#20102;&#20005;&#37325;&#30340;&#34892;&#20026;&#20559;&#24046;&#12290;&#20854;&#27425;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#23545;&#25239;&#24615;&#31034;&#20363;&#21482;&#38656;&#35201;&#30772;&#22351;&#23569;&#25968;&#20013;&#38388;&#22359;&#21363;&#21487;&#35823;&#23548;&#26368;&#32456;&#20915;&#31574;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;&#20855;&#20307;&#30340;&#22359;&#26356;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#12290; 
&lt;/p&gt;
&lt;p&gt;
Deep Learning (DL) is being applied in various domains, especially in safety-critical applications such as autonomous driving. Consequently, it is of great significance to ensure the robustness of these methods and thus counteract uncertain behaviors caused by adversarial attacks. In this paper, we use gradient heatmaps to analyze the response characteristics of the VGG-16 model when the input images are mixed with adversarial noise and statistically similar Gaussian random noise. In particular, we compare the network response layer by layer to determine where errors occurred. Several interesting findings are derived. First, compared to Gaussian random noise, intentionally generated adversarial noise causes severe behavior deviation by distracting the area of concentration in the networks. Second, in many cases, adversarial examples only need to compromise a few intermediate blocks to mislead the final decision. Third, our experiments revealed that specific blocks are more vulnerable a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#21487;&#31359;&#25140;&#35774;&#22791;&#21644;&#35843;&#26597;&#25968;&#25454;&#65292;&#32467;&#21512;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12289;XGBoost&#21644;&#38543;&#26426;&#26862;&#26519;&#39044;&#27979;&#30561;&#30496;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2303.06028</link><description>&lt;p&gt;
&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#38598;&#25104;&#23398;&#20064;&#20174;&#21487;&#31359;&#25140;&#35774;&#22791;&#39044;&#27979;&#30561;&#30496;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
Sleep Quality Prediction from Wearables using Convolution Neural Networks and Ensemble Learning. (arXiv:2303.06028v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.06028](http://arxiv.org/abs/2303.06028)
&lt;/p&gt;
&lt;p&gt;
&#30561;&#30496;&#26159;&#24433;&#21709;&#20154;&#20204;&#26085;&#24120;&#34920;&#29616;&#65292;&#24184;&#31119;&#24863;&#21644;&#29983;&#27963;&#36136;&#37327;&#30340;&#26368;&#37325;&#35201;&#30340;&#22240;&#32032;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#29616;&#22312;&#21487;&#20197;&#36890;&#36807;&#20329;&#25140;&#35774;&#22791;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#26080;&#20260;&#23475;&#22320;&#27979;&#37327;&#23427;&#12290;&#19982;&#36890;&#36807;&#25668;&#20687;&#26426;&#35760;&#24405;&#21644;&#20174;&#22270;&#20687;&#20013;&#25552;&#21462;&#29366;&#24577;&#19981;&#21516;&#65292;&#25163;&#33109;&#20329;&#25140;&#30340;&#35774;&#22791;&#21487;&#20197;&#36890;&#36807;&#21152;&#36895;&#24230;&#35745;&#65292;&#24515;&#29575;&#21644;&#24515;&#29575;&#21464;&#24322;&#24615;&#20256;&#24863;&#22120;&#30452;&#25509;&#27979;&#37327;&#12290;&#19968;&#20123;&#27979;&#37327;&#21151;&#33021;&#22914;&#19979;&#65306;&#19978;&#24202;&#26102;&#38388;&#65292;&#36215;&#24202;&#26102;&#38388;&#65292;&#20837;&#30561;&#26102;&#38388;&#65292;&#20837;&#30561;&#26102;&#38388;&#65292;&#21796;&#37266;&#21518;&#20960;&#20998;&#38047;&#12290;&#25991;&#29486;&#20013;&#26377;&#20960;&#39033;&#20851;&#20110;&#30561;&#30496;&#36136;&#37327;&#21644;&#38454;&#27573;&#39044;&#27979;&#30340;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#20165;&#20351;&#29992;&#21487;&#31359;&#25140;&#35774;&#22791;&#25968;&#25454;&#26469;&#39044;&#27979;&#25110;&#20851;&#27880;&#30561;&#30496;&#38454;&#27573;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;NetHealth&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#20174;698&#21517;&#22823;&#23398;&#29983;&#30340;&#21487;&#31359;&#25140;&#35774;&#22791;&#21644;&#35843;&#26597;&#20013;&#25910;&#38598;&#32780;&#26469;&#12290;&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#23427;&#20204;&#36890;&#24120;&#27604;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#34920;&#29616;&#26356;&#22909;&#12290;&#20854;&#20013;&#65292;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#29305;&#24449;&#25552;&#21462;&#65292;&#28982;&#21518;&#19982;XGBoost&#21644;&#38543;&#26426;&#26862;&#26519;&#19968;&#36215;&#20351;&#29992;&#36827;&#34892;&#30561;&#30496;&#36136;&#37327;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sleep is among the most important factors affecting one's daily performance, well-being, and life quality. Nevertheless, it became possible to measure it in daily life in an unobtrusive manner with wearable devices. Rather than camera recordings and extraction of the state from the images, wrist-worn devices can measure directly via accelerometer, heart rate, and heart rate variability sensors. Some measured features can be as follows: time to bed, time out of bed, bedtime duration, minutes to fall asleep, and minutes after wake-up. There are several studies in the literature regarding sleep quality and stage prediction. However, they use only wearable data to predict or focus on the sleep stage. In this study, we use the NetHealth dataset, which is collected from 698 college students' via wearables, as well as surveys. Recently, there has been an advancement in deep learning algorithms, and they generally perform better than conventional machine learning techniques. Among them, Convol
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;wav2vec2&#30340;ASR&#27169;&#22411;&#22312;&#24503;&#35821;&#25991;&#21270;&#36951;&#20135;&#32034;&#24341;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#19982;&#21830;&#19994;&#20113;&#21644;&#19987;&#26377;&#26381;&#21153;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#30446;&#21069;&#21487;&#20197;&#23454;&#29616;90&#65285;&#20197;&#19978;&#30340;&#35782;&#21035;&#29575;&#65292;&#20294;&#36825;&#20123;&#25968;&#23383;&#24456;&#24555;&#23601;&#20250;&#38477;&#20302;&#65292;&#19968;&#26086;&#24405;&#38899;&#20855;&#26377;&#21463;&#38480;&#30340;&#38899;&#39057;&#36136;&#37327;&#25110;&#20351;&#29992;&#38750;&#26085;&#24120;&#25110;&#36807;&#26102;&#30340;&#35821;&#35328;&#12290;</title><link>http://arxiv.org/abs/2303.06026</link><description>&lt;p&gt;
wav2vec&#21450;&#20854;&#22312;&#24503;&#35821;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#28508;&#21147;&#65306;&#25991;&#21270;&#36951;&#20135;&#22330;&#26223;&#19979;&#21487;&#29992;ASR&#25216;&#26415;&#30340;&#27604;&#36739;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
wav2vec and its current potential to Automatic Speech Recognition in German for the usage in Digital History: A comparative assessment of available ASR-technologies for the use in cultural heritage contexts. (arXiv:2303.06026v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.06026](http://arxiv.org/abs/2303.06026)
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35757;&#32451;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#29992;&#20110;&#24503;&#35821;&#30340;&#26368;&#20808;&#36827;&#30340;&#24320;&#28304;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#27169;&#22411;&#65292;&#20197;&#35780;&#20272;&#35813;&#25216;&#26415;&#22312;&#25968;&#23383;&#20154;&#25991;&#21644;&#25991;&#21270;&#36951;&#20135;&#32034;&#24341;&#21270;&#30340;&#26356;&#22823;&#32972;&#26223;&#19979;&#30340;&#24403;&#21069;&#28508;&#21147;&#12290;&#38500;&#20102;&#26412;&#25991;&#65292;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#22522;&#20110;wav2vec2&#30340;&#35821;&#38899;&#36716;&#25991;&#23383;&#27169;&#22411;&#65292;&#21516;&#26102;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#19982;&#21830;&#29992;&#20113;&#21644;&#19987;&#26377;&#26381;&#21153;&#36827;&#34892;&#27604;&#36739;&#30340;&#21382;&#21490;&#24405;&#38899;&#35821;&#26009;&#24211;&#19978;&#30340;&#34920;&#29616;&#12290;&#34429;&#28982;&#25105;&#20204;&#30340;&#27169;&#22411;&#21462;&#24471;&#20102;&#20013;&#31561;&#30340;&#32467;&#26524;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#19987;&#26377;&#20113;&#26381;&#21153;&#34920;&#29616;&#26356;&#22909;&#12290;&#27491;&#22914;&#25105;&#20204;&#30340;&#32467;&#26524;&#25152;&#26174;&#31034;&#30340;&#65292;&#30446;&#21069;&#21487;&#20197;&#23454;&#29616;90&#65285;&#20197;&#19978;&#30340;&#35782;&#21035;&#29575;&#65292;&#20294;&#36825;&#20123;&#25968;&#23383;&#24456;&#24555;&#23601;&#20250;&#38477;&#20302;&#65292;&#19968;&#26086;&#24405;&#38899;&#20855;&#26377;&#21463;&#38480;&#30340;&#38899;&#39057;&#36136;&#37327;&#25110;&#20351;&#29992;&#38750;&#26085;&#24120;&#25110;&#36807;&#26102;&#30340;&#35821;&#35328;&#12290;&#24503;&#35821;&#20013;&#19981;&#21516;&#26041;&#35328;&#21644;&#21475;&#38899;&#30340;&#31181;&#31867;&#32321;&#22810;&#26159;&#19968;&#20010;&#22823;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#26412;&#25991;&#24378;&#35843;&#20102;&#30446;&#21069;&#21487;&#29992;&#30340;&#35782;&#21035;&#36136;&#37327;&#26159;
&lt;/p&gt;
&lt;p&gt;
In this case study we trained and published a state-of-the-art open-source model for Automatic Speech Recognition (ASR) for German to evaluate the current potential of this technology for the use in the larger context of Digital Humanities and cultural heritage indexation. Along with this paper we publish our wav2vec2 based speech to text model while we evaluate its performance on a corpus of historical recordings we assembled compared against commercial cloud-based and proprietary services. While our model achieves moderate results, we see that proprietary cloud services fare significantly better. As our results show, recognition rates over 90 percent can currently be achieved, however, these numbers drop quickly once the recordings feature limited audio quality or use of non-every day or outworn language. A big issue is the high variety of different dialects and accents in the German language. Nevertheless, this paper highlights that the currently available quality of recognition is 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#28145;&#24230;&#23398;&#20064;&#20803;&#21551;&#21457;&#24335;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#26497;&#30701;&#30340;&#26102;&#38388;&#20869;&#36817;&#20284;&#27714;&#35299;&#36947;&#36335;&#32593;&#32476;&#35774;&#35745;&#38382;&#39064;&#65292;&#36866;&#29992;&#20110;&#35768;&#22810;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2303.06024</link><description>&lt;p&gt;
&#19968;&#31181;&#28151;&#21512;&#28145;&#24230;&#23398;&#20064;&#20803;&#21551;&#21457;&#24335;&#26694;&#26550;&#29992;&#20110;&#36817;&#20284;&#31163;&#25955;&#36947;&#36335;&#32593;&#32476;&#35774;&#35745;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
A hybrid deep-learning-metaheuristic framework to approximate discrete road network design problems. (arXiv:2303.06024v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.06024](http://arxiv.org/abs/2303.06024)
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#28145;&#24230;&#23398;&#20064;&#20803;&#21551;&#21457;&#24335;&#26694;&#26550;&#65292;&#20855;&#26377;&#21452;&#23618;&#26550;&#26500;&#65292;&#29992;&#20110;&#35299;&#20915;&#36947;&#36335;&#32593;&#32476;&#35774;&#35745;&#38382;&#39064;&#65288;NDPs&#65289;&#12290;&#25105;&#20204;&#35757;&#32451;&#19968;&#20010;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26469;&#36817;&#20284;&#29992;&#25143;&#22343;&#34913;&#65288;UE&#65289;&#20132;&#36890;&#20998;&#37197;&#38382;&#39064;&#30340;&#35299;&#65292;&#24182;&#20351;&#29992;&#35757;&#32451;&#27169;&#22411;&#24471;&#20986;&#30340;&#25512;&#35770;&#26469;&#35745;&#31639;&#36951;&#20256;&#31639;&#27861;&#65288;GA&#65289;&#30340;&#36866;&#24212;&#20989;&#25968;&#20540;&#65292;&#20197;&#36817;&#20284;&#27714;&#35299;NDPs&#12290;&#20351;&#29992;&#20004;&#31181;NDP&#21464;&#20307;&#21644;&#19968;&#20010;&#31934;&#30830;&#27714;&#35299;&#22120;&#20316;&#20026;&#22522;&#20934;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#21487;&#20197;&#22312;&#19981;&#21040;&#23547;&#25214;&#26368;&#20248;&#32467;&#26524;&#25152;&#38656;&#26102;&#38388;&#30340;1&#65285;&#20869;&#25552;&#20379;&#22312;&#20840;&#23616;&#26368;&#20248;&#32467;&#26524;5&#65285;&#20043;&#20869;&#30340;&#35299;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#35768;&#22810;&#26377;&#36259;&#30340;&#26410;&#26469;&#26041;&#21521;&#65292;&#22240;&#27492;&#25105;&#20204;&#25552;&#20986;&#20102;&#36825;&#20010;&#20027;&#39064;&#30340;&#31616;&#35201;&#30740;&#31350;&#35758;&#31243;&#12290;&#20851;&#38190;&#35266;&#23519;&#26159;&#29992;GNN&#27169;&#22411;&#23545;&#36951;&#20256;&#31639;&#27861;&#30340;&#25512;&#35770;&#36827;&#34892;&#36866;&#24212;&#20989;&#25968;&#20540;&#35745;&#31639;&#30340;&#26102;&#38388;&#20026;&#27627;&#31186;&#32423;&#65292;&#36825;&#25552;&#20379;&#20102;&#19968;&#20010;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study proposes a hybrid deep-learning-metaheuristic framework with a bi-level architecture to solve road network design problems (NDPs). We train a graph neural network (GNN) to approximate the solution of the user equilibrium (UE) traffic assignment problem, and use inferences made by the trained model to calculate fitness function evaluations of a genetic algorithm (GA) to approximate solutions for NDPs. Using two NDP variants and an exact solver as benchmark, we show that our proposed framework can provide solutions within 5% gap of the global optimum results given less than 1% of the time required for finding the optimal results. Moreover, we observe many interesting future directions, thus we propose a brief research agenda for this topic. The key observation inspiring influential future research was that fitness function evaluation time using the inferences made by the GNN model for the genetic algorithm was in the order of milliseconds, which points to an opportunity and a 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#20307;&#32946;&#21338;&#24425;&#38382;&#39064;&#65292;&#25552;&#20986;&#23545;&#20110;&#36825;&#20010;&#38382;&#39064;&#65292;&#27169;&#22411;&#26657;&#20934;&#27604;&#20934;&#30830;&#24615;&#26356;&#37325;&#35201;&#65292;&#24182;&#20351;&#29992;NBA&#25968;&#25454;&#23637;&#31034;&#65292;&#23558;&#39044;&#27979;&#27169;&#22411;&#20248;&#21270;&#21040;&#26657;&#20934;&#27604;&#23558;&#20854;&#20248;&#21270;&#21040;&#20934;&#30830;&#24615;&#20135;&#29983;&#26356;&#39640;&#30340;&#25910;&#30410;</title><link>http://arxiv.org/abs/2303.06021</link><description>&lt;p&gt;
&#20307;&#32946;&#21338;&#24425;&#30340;&#26426;&#22120;&#23398;&#20064;&#65306;&#39044;&#27979;&#27169;&#22411;&#24212;&#35813;&#20248;&#21270;&#31934;&#24230;&#36824;&#26159;&#26657;&#20934;&#65311;
&lt;/p&gt;
&lt;p&gt;
Machine learning for sports betting: should forecasting models be optimised for accuracy or calibration?. (arXiv:2303.06021v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.06021](http://arxiv.org/abs/2303.06021)
&lt;/p&gt;
&lt;p&gt;
&#32654;&#22269;&#20307;&#32946;&#21338;&#24425;&#26368;&#36817;&#33719;&#24471;&#32852;&#37030;&#21512;&#27861;&#21270;&#65292;&#36825;&#19982;&#26426;&#22120;&#23398;&#20064;&#30340;&#40644;&#37329;&#26102;&#20195;&#30456;&#21563;&#21512;&#12290;&#22914;&#26524;&#21338;&#24425;&#32773;&#21487;&#20197;&#21033;&#29992;&#25968;&#25454;&#20934;&#30830;&#39044;&#27979;&#32467;&#26524;&#30340;&#27010;&#29575;&#65292;&#20182;&#20204;&#21487;&#20197;&#35782;&#21035;&#20986;&#20070;maker&#30340;&#36180;&#29575;&#26377;&#21033;&#12290;&#30001;&#20110;&#20165;&#22312;&#32654;&#22269;&#23601;&#26159;&#19968;&#20010;&#25968;&#21313;&#20159;&#32654;&#20803;&#30340;&#20135;&#19994;&#65292;&#22240;&#27492;&#35782;&#21035;&#36825;&#31181;&#26426;&#20250;&#21487;&#33021;&#38750;&#24120;&#26377;&#21033;&#21487;&#22270;&#12290;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#24050;&#23558;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#20307;&#32946;&#32467;&#26524;&#39044;&#27979;&#38382;&#39064;&#65292;&#36890;&#24120;&#20351;&#29992;&#20934;&#30830;&#24615;&#26469;&#35780;&#20272;&#39044;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#20551;&#35774;&#65292;&#22312;&#20307;&#32946;&#21338;&#24425;&#38382;&#39064;&#19978;&#65292;&#27169;&#22411;&#26657;&#20934;&#27604;&#20934;&#30830;&#24615;&#26356;&#37325;&#35201;&#12290;&#20026;&#20102;&#27979;&#35797;&#36825;&#20010;&#20551;&#35774;&#65292;&#25105;&#20204;&#22312;NBA&#25968;&#25454;&#19978;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#22312;&#19968;&#20010;&#36187;&#23395;&#19978;&#36816;&#34892;&#21338;&#24425;&#23454;&#39564;&#65292;&#20351;&#29992;&#24050;&#21457;&#24067;&#30340;&#36180;&#29575;&#12290;&#36890;&#36807;&#35780;&#20272;&#21508;&#31181;&#21338;&#24425;&#31995;&#32479;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#23558;&#39044;&#27979;&#27169;&#22411;&#20248;&#21270;&#21040;&#26657;&#20934;&#27604;&#23558;&#20854;&#20248;&#21270;&#21040;&#20934;&#30830;&#24615;&#20135;&#29983;&#26356;&#39640;&#30340;&#25910;&#30410;&#65292;&#24179;&#22343;&#22238;&#25253;&#29575;&#20026;110.42&#65285;
&lt;/p&gt;
&lt;p&gt;
Sports betting's recent federal legalisation in the USA coincides with the golden age of machine learning. If bettors can leverage data to accurately predict the probability of an outcome, they can recognise when the bookmaker's odds are in their favour. As sports betting is a multi-billion dollar industry in the USA alone, identifying such opportunities could be extremely lucrative. Many researchers have applied machine learning to the sports outcome prediction problem, generally using accuracy to evaluate the performance of forecasting models. We hypothesise that for the sports betting problem, model calibration is more important than accuracy. To test this hypothesis, we train models on NBA data over several seasons and run betting experiments on a single season, using published odds. Evaluating various betting systems, we show that optimising the forecasting model for calibration leads to greater returns than optimising for accuracy, on average (return on investment of $110.42\%$ v
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24515;&#24459;&#22833;&#24120;&#20998;&#31867;&#26041;&#27861;&#65292;&#31216;&#20026;HARDC&#65292;&#36890;&#36807;&#21033;&#29992;&#25193;&#24352;CNN&#21644;&#21452;&#21521;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#21333;&#20803;&#26469;&#29983;&#25104;&#34701;&#21512;&#29305;&#24449;&#65292;&#21152;&#19978;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.06020</link><description>&lt;p&gt;
HARDC&#65306;&#19968;&#31181;&#22522;&#20110;&#24515;&#30005;&#22270;&#30340;&#24515;&#36339;&#20998;&#31867;&#26041;&#27861;&#65292;&#20351;&#29992;&#20998;&#23618;&#27880;&#24847;&#21147;&#21452;&#32467;&#26500;RNN&#21644;&#25193;&#24352;CNN(arXiv&#65306;2303.06020v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
HARDC : A novel ECG-based heartbeat classification method to detect arrhythmia using hierarchical attention based dual structured RNN with dilated CNN. (arXiv:2303.06020v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.06020](http://arxiv.org/abs/2303.06020)
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#20998;&#23618;&#27880;&#24847;&#21147;&#21452;&#21521;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#19982;&#25193;&#24352;CNN&#65288;HARDC&#65289;&#26041;&#27861;&#29992;&#20110;&#24515;&#24459;&#22833;&#24120;&#20998;&#31867;&#12290;&#36825;&#35299;&#20915;&#20102;&#20256;&#32479;&#25193;&#24352;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#27169;&#22411;&#24573;&#30053;&#19978;&#19979;&#25991;&#20043;&#38388;&#30340;&#20851;&#32852;&#21644;&#26799;&#24230;&#20998;&#25955;&#30340;&#38382;&#39064;&#12290;&#25152;&#25552;&#20986;&#30340;HARDC&#20805;&#20998;&#21033;&#29992;&#20102;&#25193;&#24352;CNN&#21644;&#21452;&#21521;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#21333;&#20803;&#65288;BiGRU-BiLSTM&#65289;&#20307;&#31995;&#32467;&#26500;&#26469;&#29983;&#25104;&#34701;&#21512;&#29305;&#24449;&#12290;&#36890;&#36807;&#32467;&#21512;&#26412;&#22320;&#21644;&#20840;&#23616;&#29305;&#24449;&#20449;&#24687;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#35813;&#27169;&#22411;&#30340;&#39044;&#27979;&#24615;&#33021;&#24471;&#21040;&#20102;&#25552;&#39640;&#12290;&#36890;&#36807;&#23558;&#34701;&#21512;&#29305;&#24449;&#19982;&#25193;&#24352;CNN&#21644;&#20998;&#23618;&#27880;&#24847;&#21147;&#26426;&#21046;&#30456;&#32467;&#21512;&#65292;&#35757;&#32451;&#21518;&#30340;HARDC&#27169;&#22411;&#22312;PhysioNet 2017&#25361;&#25112;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20102;&#26126;&#26174;&#30340;&#25913;&#36827;&#20998;&#31867;&#32467;&#26524;&#21644;&#29305;&#24449;&#25552;&#21462;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#20351;&#29992;&#36830;&#32493;Z-Score&#24402;&#19968;&#21270;&#12289;&#28388;&#27874;&#12289;&#21435;&#22122;&#21644;&#20998;&#27573;&#26469;&#20934;&#22791;&#21407;&#22987;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper have developed a novel hybrid hierarchical attention-based bidirectional recurrent neural network with dilated CNN (HARDC) method for arrhythmia classification. This solves problems that arise when traditional dilated convolutional neural network (CNN) models disregard the correlation between contexts and gradient dispersion. The proposed HARDC fully exploits the dilated CNN and bidirectional recurrent neural network unit (BiGRU-BiLSTM) architecture to generate fusion features. As a result of incorporating both local and global feature information and an attention mechanism, the model's performance for prediction is improved.By combining the fusion features with a dilated CNN and a hierarchical attention mechanism, the trained HARDC model showed significantly improved classification results and interpretability of feature extraction on the PhysioNet 2017 challenge dataset. Sequential Z-Score normalization, filtering, denoising, and segmentation are used to prepare the raw
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31354;&#38388;&#28388;&#27874;&#26694;&#26550;scaCSP&#65292;&#33021;&#22815;&#22312;&#20108;&#20803;&#21644;&#22810;&#31867;&#38382;&#39064;&#20013;&#26222;&#36941;&#20351;&#29992;&#65292;&#24182;&#19988;&#21487;&#20197;&#36731;&#26494;&#22320;&#23558;&#21253;&#21547;&#22312;&#20854;&#20182;&#25955;&#28857;&#30697;&#38453;&#20013;&#30340;&#26356;&#20855;&#26377;&#21306;&#20998;&#24615;&#30340;&#20449;&#24687;&#32435;&#20837;&#20854;&#20013;</title><link>http://arxiv.org/abs/2303.06019</link><description>&lt;p&gt;
&#22522;&#20110;&#25955;&#28857;&#30340;&#20844;&#20849;&#31354;&#38388;&#27169;&#24335;-&#32479;&#19968;&#30340;&#31354;&#38388;&#28388;&#27874;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Scatter-based common spatial patterns -- a unified spatial filtering framework. (arXiv:2303.06019v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.06019](http://arxiv.org/abs/2303.06019)
&lt;/p&gt;
&lt;p&gt;
&#20844;&#20849;&#31354;&#38388;&#27169;&#24335;&#65288;CSP&#65289;&#26041;&#27861;&#26159;&#19968;&#31181;&#26368;&#21463;&#27426;&#36814;&#30340;EEG&#20998;&#31867;&#31354;&#38388;&#28388;&#27874;&#25216;&#26415;&#65292;&#29992;&#20110;&#22522;&#20110;&#36816;&#21160;&#24819;&#35937;&#65288;MI&#65289;&#30340;&#33041;&#26426;&#25509;&#21475;&#65288;BCIs&#65289;&#20013;&#12290;&#28982;&#32780;&#65292;&#23427;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#32570;&#28857;&#65292;&#20363;&#22914;&#23545;&#22122;&#22768;&#65292;&#38750;&#31283;&#24577;&#21644;&#20108;&#20803;&#20998;&#31867;&#30340;&#38480;&#21046;&#24615;&#25935;&#24863;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;EEG&#20449;&#21495;&#30340;&#31354;&#38388;&#21327;&#26041;&#24046;&#25955;&#28857;&#30697;&#38453;&#30340;&#26032;&#22411;&#31354;&#38388;&#28388;&#27874;&#26694;&#26550;&#65292;&#31216;&#20026;scaCSP&#65292;&#21487;&#20197;&#22312;&#20108;&#20803;&#21644;&#22810;&#31867;&#38382;&#39064;&#20013;&#26222;&#36941;&#20351;&#29992;&#65292;&#32780;CSP&#21482;&#33021;&#22312;&#20108;&#20803;&#24773;&#20917;&#19979;&#23558;&#20854;&#20316;&#20026;&#19968;&#31181;&#29305;&#27530;&#24773;&#20917;&#36827;&#34892;&#22788;&#29702;&#65292;&#21482;&#20351;&#29992;&#33539;&#22260;&#31354;&#38388;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#23376;&#31354;&#38388;&#22686;&#24378;&#30340;scaCSP&#31639;&#27861;&#65292;&#23427;&#21487;&#20197;&#36731;&#26494;&#22320;&#23558;&#21253;&#21547;&#22312;&#20854;&#20182;&#31867;&#38388;&#25955;&#28857;&#30697;&#38453;&#21644;&#31867;&#20869;&#25955;&#28857;&#30697;&#38453;&#30340;&#20854;&#20182;&#33539;&#22260;&#31354;&#38388;&#21644;&#38646;&#31354;&#38388;&#20013;&#30340;&#26356;&#20855;&#26377;&#21306;&#20998;&#24615;&#30340;&#20449;&#24687;&#32435;&#20837;&#20854;&#20013;&#65292;&#22312;&#20004;&#31181;&#24773;&#20917;&#19979;&#65292;&#21363;&#38646;&#31354;&#38388;&#20998;&#37327;&#20943;&#23569;&#22330;&#26223;&#21644;&#38468;&#21152;&#31354;&#38388;&#28388;&#27874;&#22120;&#23398;&#20064;&#22330;&#26223;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
The common spatial pattern (CSP) approach is known as one of the most popular spatial filtering techniques for EEG classification in motor imagery (MI) based brain-computer interfaces (BCIs). However, it still suffers some drawbacks such as sensitivity to noise, non-stationarity, and limitation to binary classification.Therefore, we propose a novel spatial filtering framework called scaCSP based on the scatter matrices of spatial covariances of EEG signals, which works generally in both binary and multi-class problems whereas CSP can be cast into our framework as a special case when only the range space of the between-class scatter matrix is used in binary cases.We further propose subspace enhanced scaCSP algorithms which easily permit incorporating more discriminative information contained in other range spaces and null spaces of the between-class and within-class scatter matrices in two scenarios: a nullspace components reduction scenario and an additional spatial filter learning sce
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#32452;&#21512;&#31243;&#24207;&#26469;&#21512;&#25104;&#31243;&#24207;&#30340;&#21487;&#25193;&#23637;&#31243;&#24207;&#21512;&#25104;&#26694;&#26550;&#65292;&#36825;&#33021;&#22815;&#36991;&#20813;&#20174;&#22836;&#21512;&#25104;&#38271;&#25110;&#26356;&#22797;&#26434;&#30340;&#31243;&#24207;&#12290;</title><link>http://arxiv.org/abs/2303.06018</link><description>&lt;p&gt;
&#20998;&#23618;&#31070;&#32463;&#31243;&#24207;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Neural Program Synthesis. (arXiv:2303.06018v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.06018](http://arxiv.org/abs/2303.06018)
&lt;/p&gt;
&lt;p&gt;
&#31243;&#24207;&#21512;&#25104;&#26088;&#22312;&#33258;&#21160;&#26500;&#24314;&#28385;&#36275;&#32473;&#23450;&#20219;&#21153;&#35268;&#33539;&#65288;&#20363;&#22914;&#36755;&#20837;/&#36755;&#20986;&#23545;&#25110;&#28436;&#31034;&#65289;&#30340;&#20154;&#31867;&#21487;&#35835;&#31243;&#24207;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#22312;&#21508;&#31181;&#39046;&#22495;&#21462;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#25104;&#26524;&#65292;&#22914;&#23383;&#31526;&#20018;&#36716;&#25442;&#12289;&#24352;&#37327;&#25805;&#20316;&#21644;&#25551;&#36848;&#20855;&#20307;&#21270;&#36523;&#20195;&#29702;&#30340;&#34892;&#20026;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#31243;&#24207;&#21512;&#25104;&#26041;&#27861;&#37117;&#26159;&#20174;&#22836;&#24320;&#22987;&#21512;&#25104;&#31243;&#24207;&#65292;&#36880;&#20010;&#20196;&#29260;&#12289;&#36880;&#34892;&#29983;&#25104;&#31243;&#24207;&#12290;&#36825;&#20174;&#26681;&#26412;&#19978;&#38459;&#27490;&#20102;&#36825;&#20123;&#26041;&#27861;&#25193;&#23637;&#21040;&#21512;&#25104;&#26356;&#38271;&#25110;&#26356;&#22797;&#26434;&#30340;&#31243;&#24207;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#31243;&#24207;&#21512;&#25104;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#20998;&#23618;&#32452;&#21512;&#31243;&#24207;&#26469;&#21512;&#25104;&#31243;&#24207;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#23398;&#20064;&#20219;&#21153;&#23884;&#20837;&#31354;&#38388;&#21644;&#31243;&#24207;&#35299;&#30721;&#22120;&#65292;&#35813;&#35299;&#30721;&#22120;&#21487;&#20197;&#23558;&#20219;&#21153;&#23884;&#20837;&#35299;&#30721;&#20026;&#31243;&#24207;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35757;&#32451;&#19968;&#20010;&#39640;&#23618;&#27169;&#22359;&#65292;&#20197;&#29702;&#35299;&#26469;&#33258;&#38271;&#31243;&#24207;&#30340;&#20219;&#21153;&#35268;&#33539;&#65288;&#20363;&#22914;&#36755;&#20837;/&#36755;&#20986;&#23545;&#25110;&#28436;&#31034;&#65289;&#24182;&#29983;&#25104;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
Program synthesis aims to automatically construct human-readable programs that satisfy given task specifications, such as input/output pairs or demonstrations. Recent works have demonstrated encouraging results in a variety of domains, such as string transformation, tensor manipulation, and describing behaviors of embodied agents. Most existing program synthesis methods are designed to synthesize programs from scratch, generating a program token by token, line by line. This fundamentally prevents these methods from scaling up to synthesize programs that are longer or more complex. In this work, we present a scalable program synthesis framework that instead synthesizes a program by hierarchically composing programs. Specifically, we first learn a task embedding space and a program decoder that can decode a task embedding into a program. Then, we train a high-level module to comprehend the task specification (e.g., input/output pairs or demonstrations) from long programs and produce a se
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#26377;&#25928;&#30340;&#22826;&#38451;&#36752;&#23556;&#39044;&#27979;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#65292;&#36890;&#36807;&#21033;&#29992;&#20998;&#24067;&#22312;&#33521;&#22269;&#21508;&#22320;&#30340;&#25968;&#25454;&#21644;&#21830;&#19994;&#21487;&#29992;&#30340;&#22825;&#27668;&#25968;&#25454;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#35266;&#27979;&#25968;&#25454;&#30340;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2303.06010</link><description>&lt;p&gt;
&#26080;&#38656;&#30452;&#25509;&#35266;&#27979;&#39044;&#27979;&#22826;&#38451;&#36752;&#23556;&#65306;&#19968;&#20010;&#32463;&#39564;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Forecasting Solar Irradiance without Direct Observation: An Empirical Analysis. (arXiv:2303.06010v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.06010](http://arxiv.org/abs/2303.06010)
&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22826;&#38451;&#33021;&#30340;&#20351;&#29992;&#22686;&#21152;&#65292;&#20934;&#30830;&#21644;&#21450;&#26102;&#30340;&#39044;&#27979;&#22120;&#23545;&#20110;&#39034;&#30021;&#30340;&#30005;&#32593;&#25805;&#20316;&#33267;&#20851;&#37325;&#35201;&#12290;&#26377;&#35768;&#22810;&#25552;&#20986;&#30340;&#26041;&#27861;&#26469;&#39044;&#27979;&#22826;&#38451;&#36752;&#23556;/&#22826;&#38451;&#33021;&#20135;&#37327;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#36825;&#20123;&#26041;&#27861;&#23558;&#38382;&#39064;&#21046;&#23450;&#20026;&#26102;&#38388;&#24207;&#21015;&#65292;&#20381;&#36182;&#20110;&#22312;&#24863;&#20852;&#36259;&#30340;&#20301;&#32622;&#36817;&#23454;&#26102;&#35775;&#38382;&#35266;&#27979;&#25968;&#25454;&#26469;&#29983;&#25104;&#39044;&#27979;&#12290;&#36825;&#38656;&#35201;&#35775;&#38382;&#23454;&#26102;&#25968;&#25454;&#27969;&#21644;&#36275;&#22815;&#30340;&#21382;&#21490;&#35266;&#27979;&#25968;&#25454;&#25165;&#33021;&#37096;&#32626;&#36825;&#20123;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#27604;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21644;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#30340;&#26377;&#25928;&#24418;&#24335;&#21270;&#39044;&#27979;&#38382;&#39064;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;&#20351;&#29992;&#20998;&#24067;&#22312;&#33521;&#22269;&#21508;&#22320;&#30340;20&#20010;&#20301;&#32622;&#30340;&#25968;&#25454;&#21644;&#21830;&#19994;&#21487;&#29992;&#30340;&#22825;&#27668;&#25968;&#25454;&#65292;&#25105;&#20204;&#34920;&#26126;&#21487;&#20197;&#26500;&#24314;&#19981;&#38656;&#35201;&#35775;&#38382;&#36825;&#20123;&#25968;&#25454;&#30340;&#31995;&#32479;&#12290;&#21033;&#29992;&#20854;&#20182;&#20301;&#32622;&#30340;&#22825;&#27668;&#35266;&#27979;&#21644;&#27979;&#37327;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#20197;&#21019;&#24314;&#20855;&#26377;&#39044;&#27979;&#33021;&#21147;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
As the use of solar power increases, having accurate and timely forecasters will be essential for smooth grid operators. There are many proposed methods for forecasting solar irradiance / solar power production. However, many of these methods formulate the problem as a time-series, relying on near real-time access to observations at the location of interest to generate forecasts. This requires both access to a real-time stream of data and enough historical observations for these methods to be deployed. In this paper, we conduct a thorough analysis of effective ways to formulate the forecasting problem comparing classical machine learning approaches to state-of-the-art deep learning. Using data from 20 locations distributed throughout the UK and commercially available weather data, we show that it is possible to build systems that do not require access to this data. Leveraging weather observations and measurements from other locations we show it is possible to create models capable of a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#21644;&#38543;&#26426;&#23567;&#25209;&#37327;&#20248;&#21270;&#35299;&#20915;Gromov-Wasserstein&#26368;&#20248;&#20256;&#36755;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#20855;&#26377;&#36739;&#24378;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#26679;&#26412;&#22806;&#20272;&#35745;&#33021;&#21147;&#65292;&#21487;&#24212;&#29992;&#20110;&#26080;&#30417;&#30563;&#23545;&#40784;&#35789;&#23884;&#20837;&#31561;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2303.05978</link><description>&lt;p&gt;
&#31070;&#32463;&#26684;&#32599;&#33707;&#22827;-&#21326;&#22622;&#26031;&#22374;&#26368;&#20248;&#20256;&#36755;
&lt;/p&gt;
&lt;p&gt;
Neural Gromov-Wasserstein Optimal Transport. (arXiv:2303.05978v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.05978](http://arxiv.org/abs/2303.05978)
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#20869;&#31215;&#25104;&#26412;&#30340;Gromov-Wasserstein&#65288;GW&#65289;&#26368;&#20248;&#20256;&#36755;&#65288;OT&#65289;&#38382;&#39064;&#12290;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#65292;&#32473;&#23450;&#25903;&#25345;&#22312;&#65288;&#21487;&#33021;&#19981;&#21516;&#30340;&#65289;&#31354;&#38388;&#19978;&#30340;&#20004;&#20010;&#20998;&#24067;&#65292;&#24517;&#39035;&#22312;&#23427;&#20204;&#20043;&#38388;&#25214;&#21040;&#26368;&#21516;&#26500;&#30340;&#26144;&#23556;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#21644;&#38543;&#26426;&#23567;&#25209;&#37327;&#20248;&#21270;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;GW&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#20363;&#22914;&#38543;&#30528;&#26679;&#26412;&#25968;&#37327;&#30340;&#22686;&#21152;&#32780;&#20986;&#29616;&#30340;&#21487;&#25193;&#23637;&#24615;&#24046;&#21644;&#32570;&#20047;&#26679;&#26412;&#22806;&#20272;&#35745;&#12290;&#20026;&#20102;&#23637;&#31034;&#25105;&#20204;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#24182;&#25506;&#35752;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26080;&#30417;&#30563;&#23545;&#40784;&#35789;&#23884;&#20837;&#30340;&#27969;&#34892;&#20219;&#21153;&#20013;&#30340;&#23454;&#38469;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a scalable neural method to solve the Gromov-Wasserstein (GW) Optimal Transport (OT) problem with the inner product cost. In this problem, given two distributions supported on (possibly different) spaces, one has to find the most isometric map between them. Our proposed approach uses neural networks and stochastic mini-batch optimization which allows to overcome the limitations of existing GW methods such as their poor scalability with the number of samples and the lack of out-of-sample estimation. To demonstrate the effectiveness of our proposed method, we conduct experiments on the synthetic data and explore the practical applicability of our method to the popular task of the unsupervised alignment of word embeddings.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37319;&#29992;&#21160;&#24577;&#36125;&#21494;&#26031;&#32593;&#32476;&#21644;&#31070;&#32463;&#32593;&#32476;&#65292;&#23545;COVID-19&#24739;&#32773;&#30340;&#30149;&#24773;&#36827;&#34892;&#39044;&#27979;&#20998;&#31867;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#36739;&#39640;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.05972</link><description>&lt;p&gt;
&#37319;&#29992;&#21160;&#24577;&#36125;&#21494;&#26031;&#32593;&#32476;&#21644;&#31070;&#32463;&#32593;&#32476;&#23545;COVID-19&#24739;&#32773;&#30149;&#24773;&#28436;&#21464;&#36827;&#34892;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Classifying the evolution of COVID-19 severity on patients with combined dynamic Bayesian networks and neural networks. (arXiv:2303.05972v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.05972](http://arxiv.org/abs/2303.05972)
&lt;/p&gt;
&lt;p&gt;
&#24403;&#25105;&#20204;&#38754;&#23545;&#21040;&#36798;&#21307;&#38498;&#30340;&#24739;&#32773;&#24739;&#30149;&#30340;&#24433;&#21709;&#26102;&#65292;&#25105;&#20204;&#20250;&#38754;&#20020;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064;&#65292;&#21363;&#35780;&#20272;&#36825;&#20123;&#24739;&#32773;&#26159;&#21542;&#22312;&#19981;&#20037;&#30340;&#23558;&#26469;&#38656;&#35201;&#37325;&#30151;&#30417;&#25252;&#12290;&#36825;&#31181;&#37325;&#30151;&#30417;&#25252;&#38656;&#35201;&#20998;&#37197;&#23453;&#36149;&#32780;&#31232;&#32570;&#30340;&#36164;&#28304;&#65292;&#39044;&#20808;&#30693;&#36947;&#24739;&#32773;&#30142;&#30149;&#30340;&#20005;&#37325;&#31243;&#24230;&#21487;&#20197;&#25913;&#21892;&#20854;&#27835;&#30103;&#21644;&#36164;&#28304;&#32452;&#32455;&#12290;&#25105;&#20204;&#22312;&#30001;&#35199;&#29677;&#29273;COVID-19&#24739;&#32773;&#32452;&#25104;&#30340;&#25968;&#25454;&#38598;&#20013;&#35828;&#26126;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;&#25105;&#20204;&#23558;&#24739;&#32773;&#26631;&#35760;&#20026;&#37325;&#30151;&#24739;&#32773;&#65292;&#24403;&#20182;&#20204;&#19981;&#24471;&#19981;&#36827;&#20837;&#37325;&#30151;&#30417;&#25252;&#23460;&#25110;&#21435;&#19990;&#26102;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#32467;&#21512;&#20351;&#29992;&#21160;&#24577;&#36125;&#21494;&#26031;&#32593;&#32476;&#65292;&#23545;&#26410;&#26469;40&#23567;&#26102;&#20869;&#24739;&#32773;&#30340;&#29983;&#21629;&#20307;&#24449;&#21644;&#34880;&#28082;&#20998;&#26512;&#32467;&#26524;&#36827;&#34892;&#39044;&#27979;&#65292;&#24182;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#35780;&#20272;&#24739;&#32773;&#22312;&#35813;&#26102;&#38388;&#38388;&#38548;&#20869;&#30340;&#30142;&#30149;&#20005;&#37325;&#31243;&#24230;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;DBN&#23558;&#24739;&#32773;&#24403;&#21069;&#29366;&#24577;&#36716;&#31227;&#21040;&#26410;&#26469;&#20540;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20855;&#26377;&#24456;&#39640;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
When we face patients arriving to a hospital suffering from the effects of some illness, one of the main problems we can encounter is evaluating whether or not said patients are going to require intensive care in the near future. This intensive care requires allotting valuable and scarce resources, and knowing beforehand the severity of a patients illness can improve both its treatment and the organization of resources. We illustrate this issue in a dataset consistent of Spanish COVID-19 patients from the sixth epidemic wave where we label patients as critical when they either had to enter the intensive care unit or passed away. We then combine the use of dynamic Bayesian networks, to forecast the vital signs and the blood analysis results of patients over the next 40 hours, and neural networks, to evaluate the severity of a patients disease in that interval of time. Our empirical results show that the transposition of the current state of a patient to future values with the DBN for it
&lt;/p&gt;</description></item><item><title>&#23545;&#27604;&#25439;&#22833;&#20351;&#27169;&#24577;&#21305;&#37197;&#65292;&#20294;&#23436;&#32654;&#30340;&#27169;&#24577;&#23545;&#40784;&#19981;&#21033;&#20110;&#19979;&#28216;&#39044;&#27979;&#20219;&#21153;&#12290; &#26412;&#25991;&#25552;&#20986;&#26377;&#24847;&#20041;&#30340;&#28508;&#22312;&#27169;&#24577;&#32467;&#26500;&#26159;&#26356;&#22909;&#30340;&#34920;&#29616;&#20851;&#38190;&#12290; &#20316;&#32773;&#35774;&#35745;&#20102;&#19977;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#20998;&#21035;&#26159;&#29992;&#20110;&#27169;&#20869;&#27491;&#21017;&#21270;&#30340;&#28145;&#24230;&#29305;&#24449;&#20998;&#31163;&#25439;&#22833;&#65292;&#29992;&#20110;&#27169;&#38388;&#27491;&#21017;&#21270;&#30340;&#24067;&#26391;&#36816;&#21160;&#26725;&#25439;&#22833;&#20197;&#21450;&#29992;&#20110;&#27169;&#20869;&#21644;&#27169;&#38388;&#27491;&#21017;&#21270;&#30340;&#20960;&#20309;&#19968;&#33268;&#24615;&#25439;&#22833;&#12290; &#32463;&#36807;&#24191;&#27867;&#23454;&#39564;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2303.05952</link><description>&lt;p&gt;
&#29702;&#35299;&#21644;&#26500;&#24314;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#28508;&#22312;&#27169;&#24577;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Understanding and Constructing Latent Modality Structures in Multi-modal Representation Learning. (arXiv:2303.05952v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.05952](http://arxiv.org/abs/2303.05952)
&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#25439;&#22833;&#22312;&#20174;&#22810;&#20010;&#27169;&#24577;&#23398;&#20064;&#34920;&#31034;&#20013;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#24191;&#27867;&#12290; &#22312;&#26497;&#38480;&#24773;&#20917;&#19979;&#65292;&#23545;&#27604;&#25439;&#22833;&#30340;&#26412;&#36136;&#20419;&#20351;&#27169;&#24577;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#23436;&#20840;&#21305;&#37197;&#12290; &#28982;&#32780;&#65292;&#27169;&#24577;&#23545;&#40784;&#22914;&#20309;&#24433;&#21709;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#38382;&#39064;&#12290; &#22522;&#20110;&#20449;&#24687;&#29702;&#35770;&#30340;&#35770;&#25454;&#65292;&#26412;&#25991;&#39318;&#20808;&#35777;&#26126;&#19968;&#33324;&#24773;&#20917;&#19979;&#31934;&#30830;&#30340;&#27169;&#24577;&#23545;&#40784;&#26159;&#27425;&#20248;&#30340;&#19979;&#28216;&#39044;&#27979;&#20219;&#21153;&#12290; &#22240;&#27492;&#65292;&#25105;&#20204;&#20027;&#24352;&#26356;&#22909;&#30340;&#34920;&#29616;&#20851;&#38190;&#22312;&#20110;&#26377;&#24847;&#20041;&#30340;&#28508;&#22312;&#27169;&#24577;&#32467;&#26500;&#65292;&#32780;&#19981;&#26159;&#23436;&#32654;&#30340;&#27169;&#24577;&#23545;&#40784;&#12290; &#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#26500;&#24314;&#28508;&#22312;&#27169;&#24577;&#32467;&#26500;&#30340;&#36890;&#29992;&#26041;&#27861;&#12290; &#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;1&#65289;&#29992;&#20110;&#27169;&#20869;&#27491;&#21017;&#21270;&#30340;&#28145;&#24230;&#29305;&#24449;&#20998;&#31163;&#25439;&#22833;&#65307; 2&#65289;&#29992;&#20110;&#27169;&#38388;&#27491;&#21017;&#21270;&#30340;&#24067;&#26391;&#36816;&#21160;&#26725;&#25439;&#22833;&#65307; 3&#65289;&#29992;&#20110;&#27169;&#20869;&#21644;&#27169;&#38388;&#27491;&#21017;&#21270;&#30340;&#20960;&#20309;&#19968;&#33268;&#24615;&#25439;&#22833;&#12290; &#22312;&#20004;&#20010;&#27969;&#34892;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive loss has been increasingly used in learning representations from multiple modalities. In the limit, the nature of the contrastive loss encourages modalities to exactly match each other in the latent space. Yet it remains an open question how the modality alignment affects the downstream task performance. In this paper, based on an information-theoretic argument, we first prove that exact modality alignment is sub-optimal in general for downstream prediction tasks. Hence we advocate that the key of better performance lies in meaningful latent modality structures instead of perfect modality alignment. To this end, we propose three general approaches to construct latent modality structures. Specifically, we design 1) a deep feature separation loss for intra-modality regularization; 2) a Brownian-bridge loss for inter-modality regularization; and 3) a geometric consistency loss for both intra- and inter-modality regularization. Extensive experiments are conducted on two popular
&lt;/p&gt;</description></item><item><title>&#27773;&#36710;&#24863;&#30693;&#36719;&#20214;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#35757;&#32451;&#21644;&#39564;&#35777;&#38656;&#35201;&#22823;&#37327;&#30340;&#27880;&#37322;&#25968;&#25454;&#38598;&#12290;&#38754;&#21521;&#27492;&#31867;&#25968;&#25454;&#23494;&#38598;&#22411;&#27773;&#36710;&#36719;&#20214;&#32452;&#20214;&#30340;&#25968;&#25454;&#21644;&#27880;&#37322;&#26381;&#21153;&#20135;&#19994;&#24050;&#32463;&#20986;&#29616;&#12290;&#38754;&#20020;&#25361;&#25112;&#30340;&#26159;&#35268;&#23450;&#25968;&#25454;&#21644;&#27880;&#37322;&#38656;&#27714;&#65292;&#25361;&#25112;OEM&#65288;&#21407;&#22987;&#35774;&#22791;&#21046;&#36896;&#21830;&#65289;&#19982;&#20182;&#20204;&#30340;&#36719;&#20214;&#32452;&#20214;&#12289;&#25968;&#25454;&#21644;&#27880;&#37322;&#20379;&#24212;&#21830;&#20043;&#38388;&#30340;&#21512;&#20316;&#12290;</title><link>http://arxiv.org/abs/2303.05947</link><description>&lt;p&gt;
&#27773;&#36710;&#24863;&#30693;&#36719;&#20214;&#24320;&#21457;&#65306;&#25968;&#25454;&#12289;&#27880;&#37322;&#21644;&#29983;&#24577;&#31995;&#32479;&#25361;&#25112;&#30340;&#23454;&#35777;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Automotive Perception Software Development: An Empirical Investigation into Data, Annotation, and Ecosystem Challenges. (arXiv:2303.05947v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.05947](http://arxiv.org/abs/2303.05947)
&lt;/p&gt;
&lt;p&gt;
&#21253;&#21547;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#36719;&#20214;&#26159;&#27773;&#36710;&#24863;&#30693;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#20363;&#22914;&#22312;&#39550;&#39542;&#33258;&#21160;&#21270;&#31995;&#32479;&#20013;&#12290;&#36825;&#31181;&#36719;&#20214;&#30340;&#24320;&#21457;&#65292;&#29305;&#21035;&#26159;&#26426;&#22120;&#23398;&#20064;&#32452;&#20214;&#30340;&#35757;&#32451;&#21644;&#39564;&#35777;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#27880;&#37322;&#25968;&#25454;&#38598;&#12290;&#19968;&#20010;&#25968;&#25454;&#21644;&#27880;&#37322;&#26381;&#21153;&#20135;&#19994;&#24050;&#32463;&#20986;&#29616;&#65292;&#20026;&#36825;&#31181;&#25968;&#25454;&#23494;&#38598;&#22411;&#27773;&#36710;&#36719;&#20214;&#32452;&#20214;&#30340;&#24320;&#21457;&#25552;&#20379;&#26381;&#21153;&#12290;&#24191;&#27867;&#23384;&#22312;&#30340;&#22256;&#38590;&#22312;&#20110;&#35268;&#23450;&#25968;&#25454;&#21644;&#27880;&#37322;&#38656;&#27714;&#65292;&#25361;&#25112;OEM&#65288;&#21407;&#22987;&#35774;&#22791;&#21046;&#36896;&#21830;&#65289;&#19982;&#20182;&#20204;&#30340;&#36719;&#20214;&#32452;&#20214;&#12289;&#25968;&#25454;&#21644;&#27880;&#37322;&#20379;&#24212;&#21830;&#20043;&#38388;&#30340;&#21512;&#20316;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#29790;&#20856;&#27773;&#36710;&#24037;&#19994;&#20174;&#19994;&#32773;&#22312;&#20026;&#25968;&#25454;&#21644;&#27880;&#37322;&#21046;&#23450;&#28165;&#26224;&#35268;&#33539;&#26041;&#38754;&#36935;&#21040;&#30340;&#22256;&#38590;&#30340;&#21407;&#22240;&#12290;&#19968;&#39033;&#35775;&#35848;&#30740;&#31350;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25968;&#25454;&#36136;&#37327;&#26041;&#38754;&#32570;&#20047;&#26377;&#25928;&#30340;&#25351;&#26631;&#12289;&#24037;&#20316;&#26041;&#24335;&#30340;&#27495;&#20041;&#12289;&#27880;&#37322;&#36136;&#37327;&#30340;&#19981;&#28165;&#26224;&#23450;&#20041;&#20197;&#21450;&#21830;&#19994;&#26041;&#38754;&#30340;&#32570;&#38519;&#37117;&#26159;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
Software that contains machine learning algorithms is an integral part of automotive perception, for example, in driving automation systems. The development of such software, specifically the training and validation of the machine learning components, require large annotated datasets. An industry of data and annotation services has emerged to serve the development of such data-intensive automotive software components. Wide-spread difficulties to specify data and annotation needs challenge collaborations between OEMs (Original Equipment Manufacturers) and their suppliers of software components, data, and annotations. This paper investigates the reasons for these difficulties for practitioners in the Swedish automotive industry to arrive at clear specifications for data and annotations. The results from an interview study show that a lack of effective metrics for data quality aspects, ambiguities in the way of working, unclear definitions of annotation quality, and deficits in the busine
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#25705;&#25830;&#20272;&#35745;&#38382;&#39064;&#21046;&#23450;&#20026;&#35270;&#35273;&#24863;&#30693;&#23398;&#20064;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#26816;&#27979;&#34920;&#38754;&#29305;&#24449;&#26469;&#39044;&#27979;&#25705;&#25830;&#21147;&#12290;&#35813;&#26041;&#27861;&#20026;&#20174;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#30340;&#22238;&#24402;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.05927</link><description>&lt;p&gt;
&#20351;&#29992;&#29983;&#25104;&#24314;&#27169;&#20272;&#35745;&#25705;&#25830;&#31995;&#25968;
&lt;/p&gt;
&lt;p&gt;
Estimating friction coefficient using generative modelling. (arXiv:2303.05927v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.05927](http://arxiv.org/abs/2303.05927)
&lt;/p&gt;
&lt;p&gt;
&#36890;&#24120;&#20351;&#29992;&#21160;&#24577;&#27169;&#22411;&#23454;&#26102;&#27979;&#37327;&#36718;&#32974;&#19982;&#36335;&#38754;&#20043;&#38388;&#30340;&#25705;&#25830;&#31995;&#25968;&#12290;&#30456;&#21453;&#65292;&#39044;&#27979;&#26041;&#27861;&#36890;&#36807;&#35782;&#21035;&#24433;&#21709;&#25705;&#25830;&#31995;&#25968;&#30340;&#29615;&#22659;&#22240;&#32032;&#26469;&#20272;&#35745;&#36718;&#32974;&#19982;&#36335;&#38754;&#20043;&#38388;&#30340;&#25705;&#25830;&#31995;&#25968;&#12290;&#26412;&#25991;&#26088;&#22312;&#23558;&#25705;&#25830;&#20272;&#35745;&#38382;&#39064;&#21046;&#23450;&#20026;&#35270;&#35273;&#30693;&#35273;&#23398;&#20064;&#20219;&#21153;&#12290;&#35813;&#38382;&#39064;&#36890;&#36807;&#24212;&#29992;&#35821;&#20041;&#20998;&#21106;&#26469;&#26816;&#27979;&#34920;&#38754;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#25552;&#21462;&#30340;&#29305;&#24449;&#26469;&#39044;&#27979;&#25705;&#25830;&#21147;&#12290;&#26412;&#30740;&#31350;&#39318;&#27425;&#23558;&#25705;&#25830;&#20272;&#35745;&#38382;&#39064;&#21046;&#23450;&#20026;&#20174;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#30340;&#22238;&#24402;&#38382;&#39064;&#12290;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20272;&#35745;&#25705;&#25830;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is common to utilise dynamic models to measure the tyre-road friction in real-time. Alternatively, predictive approaches estimate the tyre-road friction by identifying the environmental factors affecting it. This work aims to formulate the problem of friction estimation as a visual perceptual learning task. The problem is broken down into detecting surface characteristics by applying semantic segmentation and using the extracted features to predict the frictional force. This work for the first time formulates the friction estimation problem as a regression from the latent space of a semantic segmentation model. The preliminary results indicate that this approach can estimate frictional force.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;ODE-Net&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#24418;&#24335;&#21270;&#20026;&#22343;&#22330;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#24403;&#25551;&#36848;ODE-Net&#21521;&#37327;&#22330;&#30340;&#31070;&#32463;&#32593;&#32476;&#23545;&#20110;&#21487;&#23398;&#20064;&#21442;&#25968;&#26159;&#32447;&#24615;&#26102;&#30340;&#23384;&#22312;&#24615;&#32467;&#26524;</title><link>http://arxiv.org/abs/2303.05924</link><description>&lt;p&gt;
ODE-Net&#30340;&#21464;&#20998;&#24418;&#24335;&#20316;&#20026;&#19968;&#20010;&#22343;&#22330;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#21644;&#23384;&#22312;&#24615;&#32467;&#26524;&#30340;&#25968;&#23398;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Variational formulations of ODE-Net as a mean-field optimal control problem and existence results. (arXiv:2303.05924v1 [math.AP])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.05924](http://arxiv.org/abs/2303.05924)
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;ODE-Net&#30340;&#25968;&#23398;&#20998;&#26512;&#65292;&#36825;&#26159;&#19968;&#31181;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#36830;&#32493;&#27169;&#22411;&#12290;&#22312;&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#29992;ODE&#20316;&#20026;&#36830;&#32493;&#26497;&#38480;&#26469;&#26367;&#20195;DNN&#30340;&#28145;&#24230;&#32467;&#26500;&#30340;&#24819;&#27861;&#12290;&#36825;&#20123;&#30740;&#31350;&#23558;ODE-Net&#30340;&#8220;&#23398;&#20064;&#8221;&#35270;&#20026;&#36890;&#36807;&#19968;&#20010;&#21442;&#25968;ODE&#38480;&#21046;&#30340;&#8220;&#25439;&#22833;&#8221;&#30340;&#26368;&#23567;&#21270;&#12290;&#23613;&#31649;&#38656;&#35201;&#20551;&#35774;&#23384;&#22312;&#19968;&#20010;&#26368;&#23567;&#21270;&#38382;&#39064;&#30340;&#26368;&#23567;&#21270;&#22120;&#65292;&#20294;&#21482;&#26377;&#24456;&#23569;&#30340;&#30740;&#31350;&#22312;&#35814;&#32454;&#20998;&#26512;&#20854;&#23384;&#22312;&#24615;&#26041;&#38754;&#36827;&#34892;&#20102;&#35843;&#26597;&#12290;&#26412;&#25991;&#26681;&#25454;ODE-Net&#30340;&#27979;&#24230;&#29702;&#35770;&#22343;&#22330;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#30340;&#34920;&#36848;&#26469;&#35752;&#35770;&#26368;&#23567;&#21270;&#22120;&#30340;&#23384;&#22312;&#24615;&#32467;&#26524;&#12290;&#24403;&#25551;&#36848;ODE-Net&#21521;&#37327;&#22330;&#30340;&#31070;&#32463;&#32593;&#32476;&#23545;&#20110;&#21487;&#23398;&#20064;&#21442;&#25968;&#26159;&#32447;&#24615;&#26102;&#65292;&#35777;&#26126;&#20102;&#23384;&#22312;&#24615;&#32467;&#26524;&#12290;&#35777;&#26126;&#37319;&#29992;&#20102;&#27979;&#24230;&#29702;&#35770;&#30340;&#24418;&#24335;&#65292;&#32467;&#21512;&#21464;&#20998;&#27861;&#30340;&#30452;&#25509;&#26041;&#27861;&#12290;&#20854;&#27425;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#24819;&#21270;&#30340;&#26368;&#23567;&#21270;&#38382;&#39064;&#26469;&#28040;&#38500;......
&lt;/p&gt;
&lt;p&gt;
This paper presents a mathematical analysis of ODE-Net, a continuum model of deep neural networks (DNNs). In recent years, Machine Learning researchers have introduced ideas of replacing the deep structure of DNNs with ODEs as a continuum limit. These studies regard the "learning" of ODE-Net as the minimization of a "loss" constrained by a parametric ODE. Although the existence of a minimizer for this minimization problem needs to be assumed, only a few studies have investigated its existence analytically in detail. In the present paper, the existence of a minimizer is discussed based on a formulation of ODE-Net as a measure-theoretic mean-field optimal control problem. The existence result is proved when a neural network, which describes a vector field of ODE-Net, is linear with respect to learnable parameters. The proof employs the measure-theoretic formulation combined with the direct method of Calculus of Variations. Secondly, an idealized minimization problem is proposed to remove
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;eBPF&#30340;&#20869;&#23384;&#31649;&#29702;&#24037;&#20316;&#38598;&#22823;&#23567;&#20272;&#35745;&#26694;&#26550;&#65292;&#21487;&#26377;&#25928;&#20272;&#31639;WSS&#65292;&#36991;&#20813;&#20102;&#22522;&#20110;&#34394;&#25311;&#26426;&#30340;&#26041;&#27861;&#30340;&#36739;&#22823;&#24320;&#38144;&#12290;</title><link>http://arxiv.org/abs/2303.05919</link><description>&lt;p&gt;
&#22522;&#20110;eBPF&#30340;&#20869;&#23384;&#31649;&#29702;&#24037;&#20316;&#38598;&#22823;&#23567;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
eBPF-based Working Set Size Estimation in Memory Management. (arXiv:2303.05919v1 [cs.PF])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.05919](http://arxiv.org/abs/2303.05919)
&lt;/p&gt;
&lt;p&gt;
&#24037;&#20316;&#38598;&#22823;&#23567;&#20272;&#35745;(WSS)&#23545;&#20110;&#25552;&#39640;&#29616;&#20195;&#25805;&#20316;&#31995;&#32479;&#20013;&#31243;&#24207;&#25191;&#34892;&#21644;&#20869;&#23384;&#25490;&#24067;&#30340;&#25928;&#29575;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#20960;&#31181;&#20272;&#31639;WSS&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#33258;&#25105;&#20805;&#27668;&#12289;Z&#20805;&#27668;&#31561;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22522;&#20110;&#34394;&#25311;&#26426;&#30340;&#26041;&#27861;&#36890;&#24120;&#20250;&#23548;&#33268;&#36739;&#22823;&#30340;&#24320;&#38144;&#12290;&#22240;&#27492;&#65292;&#20351;&#29992;&#36825;&#20123;&#26041;&#27861;&#26469;&#20272;&#31639;WSS&#26159;&#19981;&#23454;&#38469;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;eBPF(&#25193;&#23637;&#20271;&#20811;&#21033;&#25968;&#25454;&#21253;&#36807;&#28388;&#22120;)&#65292;&#36825;&#26159;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#19982;&#20869;&#26680;&#36830;&#25509;&#26469;&#30417;&#25511;&#21644;&#36807;&#28388;&#25968;&#25454;&#12290;&#20351;&#29992;&#22266;&#23450;&#21040;&#20869;&#26680;&#30340;eBPF&#31243;&#24207;&#65292;&#25105;&#20204;&#21487;&#20197;&#24471;&#21040;&#39029;&#38754;&#38169;&#35823;&#31561;&#20869;&#23384;&#20998;&#37197;&#20449;&#24687;&#30340;&#26102;&#38388;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;vanilla&#24037;&#20855;&#25910;&#38598;WSS&#65292;&#29992;LightGBM&#35757;&#32451;&#39044;&#27979;&#27169;&#22411;&#26469;&#23436;&#25104;&#20272;&#31639;&#24037;&#20316;&#65292;LightGBM&#26159;&#19968;&#20010;&#22312;&#36830;&#32493;&#20540;&#19978;&#29983;&#25104;&#20915;&#31574;&#26641;&#30340;&#26377;&#29992;&#24037;&#20855;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#26377;&#25928;&#22320;&#20272;&#31639;WSS&#12290;
&lt;/p&gt;
&lt;p&gt;
Working set size estimation (WSS) is of great significance to improve the efficiency of program executing and memory arrangement in modern operating systems. Previous work proposed several methods to estimate WSS, including self-balloning, Zballoning and so on. However, these methods which are based on virtual machine usually cause a large overhead. Thus, using those methods to estimate WSS is impractical. In this paper, we propose a novel framework to efficiently estimate WSS with eBPF (extended Berkeley Packet Filter), a cutting-edge technology which monitors and filters data by being attached to the kernel. With an eBPF program pinned into the kernel, we get the times of page fault and other information of memory allocation. Moreover, we collect WSS via vanilla tool to train a predictive model to complete estimation work with LightGBM, a useful tool which performs well on generating decision trees over continuous value. The experimental results illustrate that our framework can esti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#32032;&#34701;&#21512;&#30340;&#22522;&#20110;&#21407;&#23376;&#20013;&#24515;&#30340;&#23545;&#31216;&#20989;&#25968;&#65292;&#23558;&#32467;&#26500;&#29305;&#24615;&#21644;&#20803;&#32032;&#20449;&#24687;&#30456;&#32467;&#21512;&#65292;&#21457;&#23637;&#20986;&#19968;&#31181;&#32456;&#36523;&#26426;&#22120;&#23398;&#20064;&#21183;&#33021;&#65288;lMLP&#65289;&#65292;&#33021;&#22815;&#36229;&#36234;&#22266;&#23450;&#30340;&#12289;&#39044;&#35757;&#32451;&#30340;MLP&#65292;&#23454;&#29616;&#19981;&#26029;&#36866;&#24212;&#12290;</title><link>http://arxiv.org/abs/2303.05911</link><description>&lt;p&gt;
&#32456;&#36523;&#26426;&#22120;&#23398;&#20064;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Lifelong Machine Learning Potentials. (arXiv:2303.05911v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.05911](http://arxiv.org/abs/2303.05911)
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20934;&#30830;&#30340;&#37327;&#23376;&#21270;&#23398;&#25968;&#25454;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#21183;&#33021;&#65288;MLP&#65289;&#21487;&#20197;&#20445;&#25345;&#39640;&#31934;&#24230;&#65292;&#32780;&#23545;&#35745;&#31639;&#35201;&#27714;&#24456;&#23567;&#12290;&#20294;&#26159;&#65292;&#23427;&#20204;&#38656;&#35201;&#38024;&#23545;&#27599;&#20010;&#21333;&#29420;&#30340;&#31995;&#32479;&#36827;&#34892;&#35757;&#32451;&#12290;&#36817;&#24180;&#26469;&#65292;&#24050;&#32463;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#20102;&#22823;&#37327;&#30340;MLP&#65292;&#22240;&#20026;&#23398;&#20064;&#39069;&#22806;&#25968;&#25454;&#36890;&#24120;&#38656;&#35201;&#37325;&#26032;&#23545;&#25152;&#26377;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#19981;&#24536;&#35760;&#20197;&#21069;&#33719;&#24471;&#30340;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;MLP&#30340;&#22823;&#22810;&#25968;&#24120;&#35265;&#32467;&#26500;&#25551;&#36848;&#31526;&#19981;&#33021;&#26377;&#25928;&#22320;&#34920;&#31034;&#22823;&#37327;&#19981;&#21516;&#30340;&#21270;&#23398;&#20803;&#32032;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#20803;&#32032;&#34701;&#21512;&#30340;&#22522;&#20110;&#21407;&#23376;&#20013;&#24515;&#30340;&#23545;&#31216;&#20989;&#25968;&#65288;eeACSFs&#65289;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#36825;&#20123;&#20989;&#25968;&#32467;&#21512;&#20102;&#21608;&#26399;&#34920;&#30340;&#32467;&#26500;&#29305;&#24615;&#21644;&#20803;&#32032;&#20449;&#24687;&#12290;&#36825;&#20123;eeACSFs&#26159;&#25105;&#20204;&#24320;&#21457;&#32456;&#36523;&#26426;&#22120;&#23398;&#20064;&#21183;&#33021;&#65288;lMLP&#65289;&#30340;&#20851;&#38190;&#12290;&#21487;&#20197;&#21033;&#29992;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26469;&#36229;&#36234;&#22266;&#23450;&#30340;&#12289;&#39044;&#35757;&#32451;&#30340;MLP&#65292;&#20197;&#21040;&#36798;&#19981;&#26029;&#36866;&#24212;&#30340;lMLP&#65292;&#22240;&#20026;&#39044;&#23450;&#20041;&#30340;&#32423;&#21035;
&lt;/p&gt;
&lt;p&gt;
Machine learning potentials (MLPs) trained on accurate quantum chemical data can retain the high accuracy, while inflicting little computational demands. On the downside, they need to be trained for each individual system. In recent years, a vast number of MLPs has been trained from scratch because learning additional data typically requires to train again on all data to not forget previously acquired knowledge. Additionally, most common structural descriptors of MLPs cannot represent efficiently a large number of different chemical elements. In this work, we tackle these problems by introducing element-embracing atom-centered symmetry functions (eeACSFs) which combine structural properties and element information from the periodic table. These eeACSFs are a key for our development of a lifelong machine learning potential (lMLP). Uncertainty quantification can be exploited to transgress a fixed, pre-trained MLP to arrive at a continuously adapting lMLP, because a predefined level of ac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#21463;BM&#21551;&#21457;&#30340;&#27169;&#22411;&#65306;&#20135;&#21697;Jacobi-Theta&#29627;&#23572;&#20857;&#26364;&#26426;(pJTBM)&#65292;&#24182;&#34920;&#26126;&#21487;&#20197;&#20351;&#29992;Fisher&#24046;&#24322;&#30340;&#24471;&#20998;&#21305;&#37197;&#26041;&#27861;&#26356;&#26377;&#25928;&#22320;&#20351;&#29992;pJTBM&#26469;&#36866;&#24212;&#27010;&#29575;&#23494;&#24230;&#65292;&#27604;&#20351;&#29992;&#21407;&#22987;RTBM&#26356;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2303.05910</link><description>&lt;p&gt;
&#20351;&#29992;&#24471;&#20998;&#21305;&#37197;&#30340;&#38597;&#21487;&#27604;-Theta&#29627;&#23572;&#20857;&#26364;&#26426;
&lt;/p&gt;
&lt;p&gt;
Product Jacobi-Theta Boltzmann machines with score matching. (arXiv:2303.05910v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.05910](http://arxiv.org/abs/2303.05910)
&lt;/p&gt;
&lt;p&gt;
&#20272;&#35745;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#26159;&#19968;&#39033;&#38750;&#24120;&#22256;&#38590;&#30340;&#20219;&#21153;&#65292;&#36817;&#24180;&#26469;&#24050;&#32463;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#26469;&#35299;&#20915;&#12290;&#36890;&#36807;&#21463;Boltzmann&#26426;(BM)&#26550;&#26500;&#21551;&#21457;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#33719;&#24471;&#25104;&#21151;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#20135;&#21697;Jacobi-Theta&#29627;&#23572;&#20857;&#26364;&#26426;(pJTBM)&#65292;&#20316;&#20026;&#20855;&#26377;&#23545;&#35282;&#32447;&#38544;&#34255;&#37096;&#20998;&#36830;&#25509;&#30697;&#38453;&#30340;Riemann-Theta&#29627;&#23572;&#20857;&#26364;&#26426;(RTBM)&#30340;&#38480;&#21046;&#29256;&#26412;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22522;&#20110;Fisher&#24046;&#24322;&#30340;&#24471;&#20998;&#21305;&#37197;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#20351;&#29992;pJTBM&#26469;&#36866;&#24212;&#27010;&#29575;&#23494;&#24230;&#65292;&#27604;&#20351;&#29992;&#21407;&#22987;RTBM&#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
The estimation of probability density functions is a non trivial task that over the last years has been tackled with machine learning techniques. Successful applications can be obtained using models inspired by the Boltzmann machine (BM) architecture. In this manuscript, the product Jacobi-Theta Boltzmann machine (pJTBM) is introduced as a restricted version of the Riemann-Theta Boltzmann machine (RTBM) with diagonal hidden sector connection matrix. We show that score matching, based on the Fisher divergence, can be used to fit probability densities with the pJTBM more efficiently than with the original RTBM.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#39318;&#27425;&#20840;&#38754;&#35780;&#20272;&#24182;&#20998;&#26512;&#20102;&#29992;&#20110;&#21270;&#23398;&#36807;&#31243;&#25968;&#25454;&#30340;&#29616;&#20195;&#65288;&#28145;&#24230;&#23398;&#20064;&#65289;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#12290;&#30740;&#31350;&#21457;&#29616;&#21487;&#22312;&#24037;&#19994;&#24212;&#29992;&#20013;&#36873;&#25321;&#36866;&#24403;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;</title><link>http://arxiv.org/abs/2303.05904</link><description>&lt;p&gt;
&#30000;&#32435;&#35199;&#183;&#20234;&#26031;&#26364;&#27969;&#31243;&#25968;&#25454;&#30340;&#28145;&#24230;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Deep Anomaly Detection on Tennessee Eastman Process Data. (arXiv:2303.05904v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.05904](http://arxiv.org/abs/2303.05904)
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#20840;&#38754;&#35780;&#20272;&#21644;&#20998;&#26512;&#20102;&#29992;&#20110;&#21270;&#23398;&#36807;&#31243;&#25968;&#25454;&#30340;&#29616;&#20195;&#65288;&#28145;&#24230;&#23398;&#20064;&#65289;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#12290;&#25105;&#20204;&#20851;&#27880;&#30000;&#32435;&#35199;&#183;&#20234;&#26031;&#26364;&#27969;&#31243;&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#26631;&#20934;&#30340;&#21270;&#39564;&#35797;&#39564;&#65292;&#29992;&#20110;&#35780;&#20272;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#24050;&#26377;&#36817;&#19977;&#21313;&#24180;&#12290;&#25105;&#20204;&#30340;&#24191;&#27867;&#30740;&#31350;&#23558;&#26377;&#21161;&#20110;&#22312;&#24037;&#19994;&#24212;&#29992;&#20013;&#36873;&#25321;&#36866;&#24403;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper provides the first comprehensive evaluation and analysis of modern (deep-learning) unsupervised anomaly detection methods for chemical process data. We focus on the Tennessee Eastman process dataset, which has been a standard litmus test to benchmark anomaly detection methods for nearly three decades. Our extensive study will facilitate choosing appropriate anomaly detection methods in industrial applications.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#39057;&#39044;&#27979;&#27169;&#22411;&#30340;&#20998;&#24067;&#20445;&#25345;&#28304;&#20998;&#31163;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#30340;&#24863;&#30693;&#32570;&#38519;&#65292;&#25552;&#39640;&#20998;&#31163;&#32467;&#26524;&#30340;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.05896</link><description>&lt;p&gt;
&#22522;&#20110;&#26102;&#39057;&#39044;&#27979;&#27169;&#22411;&#30340;&#20998;&#24067;&#20445;&#25345;&#28304;&#20998;&#31163;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Distribution Preserving Source Separation With Time Frequency Predictive Models. (arXiv:2303.05896v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.05896](http://arxiv.org/abs/2303.05896)
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20998;&#24067;&#20445;&#25345;&#30340;&#28304;&#20998;&#31163;&#26041;&#27861;&#30340;&#20363;&#23376;&#65292;&#26088;&#22312;&#35299;&#20915;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#24863;&#30693;&#32570;&#38519;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#20449;&#21495;&#28304;&#30340;&#26080;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#12290;&#36890;&#36807;&#23545;&#26465;&#20214;&#20110;&#28151;&#21512;&#23454;&#29616;&#30340;&#20998;&#24067;&#36827;&#34892;&#28151;&#21512;&#19968;&#33268;&#24615;&#37319;&#26679;&#65292;&#23454;&#29616;&#37325;&#26500;&#12290;&#20998;&#31163;&#30340;&#20449;&#21495;&#36981;&#24490;&#21508;&#33258;&#30340;&#28304;&#20998;&#24067;&#65292;&#22312;&#21548;&#27979;&#35797;&#20013;&#35780;&#20272;&#20998;&#31163;&#32467;&#26524;&#26102;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide an example of a distribution preserving source separation method, which aims at addressing perceptual shortcomings of state-of-the-art methods. Our approach uses unconditioned generative models of signal sources. Reconstruction is achieved by means of mix-consistent sampling from a distribution conditioned on a realization of a mix. The separated signals follow their respective source distributions, which provides an advantage when separation results are evaluated in a listening test.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22522;&#20110;&#20223;&#30495;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#35745;&#31639;&#26426;&#22120;&#20154;&#25235;&#21462;&#23039;&#24577;&#30340;&#26041;&#27861;&#65292;&#25104;&#21151;&#29575;&#39640;&#12290;</title><link>http://arxiv.org/abs/2303.05873</link><description>&lt;p&gt;
&#26426;&#22120;&#20154;&#25235;&#21462;&#30340;&#22522;&#20110;&#20223;&#30495;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Simulation-based Bayesian inference for robotic grasping. (arXiv:2303.05873v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.05873](http://arxiv.org/abs/2303.05873)
&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#38750;&#24179;&#28369;&#25509;&#35302;&#21160;&#21147;&#23398;&#21644;&#29615;&#22659;&#25110;&#20256;&#24863;&#22120;&#22122;&#22768;&#30340;&#35768;&#22810;&#19981;&#30830;&#23450;&#24615;&#65292;&#19968;&#33324;&#30340;&#26426;&#22120;&#20154;&#22841;&#29226;&#24456;&#38590;&#25511;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#22312;&#26426;&#22120;&#20154;&#22312;&#20854;&#29615;&#22659;&#20013;&#36827;&#34892;&#23436;&#20840;&#38543;&#26426;&#21069;&#21521;&#20223;&#30495;&#26469;&#40065;&#26834;&#22320;&#32771;&#34385;&#31995;&#32479;&#20013;&#30340;&#35768;&#22810;&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#32780;&#35745;&#31639;&#20986;6&#33258;&#30001;&#24230;&#25235;&#21462;&#23039;&#24577;&#12290;&#20351;&#29992;&#20445;&#30041;&#26059;&#36716;&#31354;&#38388;&#38750;&#32447;&#24615;&#24615;&#30340;&#40654;&#26364;&#27969;&#24418;&#20248;&#21270;&#31243;&#24207;&#35745;&#31639;&#26368;&#22823;&#21518;&#39564;&#25235;&#21462;&#23039;&#24577;&#12290;&#20223;&#30495;&#21644;&#29289;&#29702;&#22522;&#20934;&#27979;&#35797;&#26174;&#31034;&#20102;&#35813;&#26041;&#27861;&#39640;&#25104;&#21151;&#29575;&#30340;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
General robotic grippers are challenging to control because of their rich nonsmooth contact dynamics and the many sources of uncertainties due to the environment or sensor noise. In this work, we demonstrate how to compute 6-DoF grasp poses using simulation-based Bayesian inference through the full stochastic forward simulation of the robot in its environment while robustly accounting for many of the uncertainties in the system. A Riemannian manifold optimization procedure preserving the nonlinearity of the rotation space is used to compute the maximum a posteriori grasp pose. Simulation and physical benchmarks show the promising high success rate of the approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#29305;&#24449;&#20018;&#32852;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21069;&#19968;&#24103;&#30340;&#29305;&#24449;&#26144;&#23556;&#24182;&#20837;&#24403;&#21069;&#24103;&#26469;&#26816;&#27979;&#24687;&#32905;&#65292;&#20197;&#27492;&#25552;&#39640;&#35270;&#39057;&#20013;&#33258;&#21160;&#24687;&#32905;&#26816;&#27979;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.05871</link><description>&lt;p&gt;
&#35270;&#39057;&#20013;&#20934;&#30830;&#30340;&#23454;&#26102;&#24687;&#32905;&#26816;&#27979;&#65306;&#26469;&#33258;&#36830;&#32493;&#24103;&#25552;&#21462;&#30340;&#28508;&#22312;&#29305;&#24449;&#30340;&#20018;&#32852;
&lt;/p&gt;
&lt;p&gt;
Accurate Real-time Polyp Detection in Videos from Concatenation of Latent Features Extracted from Consecutive Frames. (arXiv:2303.05871v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.05871](http://arxiv.org/abs/2303.05871)
&lt;/p&gt;
&lt;p&gt;
&#23454;&#26102;&#30340;&#24687;&#32905;&#26816;&#27979;&#23545;&#20110;&#20943;&#23569;&#31579;&#26597;&#36807;&#31243;&#20013;&#30340;&#28431;&#35786;&#29575;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#23581;&#35797;&#36890;&#36807;&#22312;&#30456;&#37051;&#24103;&#20043;&#38388;&#25972;&#21512;&#26102;&#38388;&#20449;&#24687;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#29305;&#24449;&#20018;&#32852;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21069;&#19968;&#24103;&#30340;&#29305;&#24449;&#26144;&#23556;&#24182;&#20837;&#24403;&#21069;&#24103;&#26469;&#26816;&#27979;&#24687;&#32905;&#65292;&#20197;&#27492;&#19981;&#22686;&#21152;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#29305;&#24449;&#20018;&#32852;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#35270;&#39057;&#20013;&#33258;&#21160;&#24687;&#32905;&#26816;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
An efficient deep learning model that can be implemented in real-time for polyp detection is crucial to reducing polyp miss-rate during screening procedures. Convolutional neural networks (CNNs) are vulnerable to small changes in the input image. A CNN-based model may miss the same polyp appearing in a series of consecutive frames and produce unsubtle detection output due to changes in camera pose, lighting condition, light reflection, etc. In this study, we attempt to tackle this problem by integrating temporal information among neighboring frames. We propose an efficient feature concatenation method for a CNN-based encoder-decoder model without adding complexity to the model. The proposed method incorporates extracted feature maps of previous frames to detect polyps in the current frame. The experimental results demonstrate that the proposed method of feature concatenation improves the overall performance of automatic polyp detection in videos. The following results are obtained on a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;&#37327;&#23376;&#20248;&#21270;&#31639;&#27861;&#26469;&#25552;&#39640;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#21644;&#26102;&#38388;&#22797;&#26434;&#24230;&#65292;&#24182;&#19988;&#20351;&#29992;&#19968;&#20010;&#21464;&#20998;&#21442;&#25968;&#21270;&#30005;&#36335;&#20316;&#20026;&#36755;&#20837;&#23618;&#65292;&#21487;&#20197;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2303.05860</link><description>&lt;p&gt;
&#22522;&#20110;&#21464;&#20998;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#30340;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Variational Quantum Neural Networks (VQNNS) in Image Classification. (arXiv:2303.05860v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.05860](http://arxiv.org/abs/2303.05860)
&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#24050;&#25104;&#20026;&#19968;&#20010;&#36328;&#23398;&#31185;&#39046;&#22495;&#65292;&#26088;&#22312;&#20811;&#26381;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#38480;&#21046;&#12290;&#37327;&#23376;&#35745;&#31639;&#26426;&#33021;&#22815;&#35299;&#20915;&#36755;&#20837;&#20043;&#38388;&#23384;&#22312;&#22797;&#26434;&#30456;&#20851;&#24615;&#30340;&#38382;&#39064;&#65292;&#36825;&#34920;&#26126;&#22312;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#24314;&#31435;&#30340;&#23398;&#20064;&#27169;&#22411;&#21487;&#33021;&#26356;&#24378;&#22823;&#65292;&#33021;&#22815;&#22312;&#36739;&#23569;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#26356;&#24555;&#30340;&#35745;&#31639;&#21644;&#26356;&#22909;&#30340;&#27867;&#21270;&#12290;&#26412;&#25991;&#30340;&#30446;&#30340;&#26159;&#30740;&#31350;&#22914;&#20309;&#20351;&#29992;&#37327;&#23376;&#20248;&#21270;&#31639;&#27861;&#26469;&#35757;&#32451;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#65288;QNN&#65289;&#65292;&#20197;&#25552;&#39640;QNN&#30340;&#24615;&#33021;&#21644;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;&#21487;&#20197;&#37096;&#20998;&#37327;&#23376;&#21270;&#32463;&#20856;&#31070;&#32463;&#32593;&#32476;&#20197;&#21019;&#24314;&#28151;&#21512;&#37327;&#23376;-&#32463;&#20856;&#31070;&#32463;&#32593;&#32476;&#65292;&#20027;&#35201;&#29992;&#20110;&#20998;&#31867;&#21644;&#22270;&#20687;&#35782;&#21035;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;QNN&#32467;&#26500;&#65292;&#20854;&#20013;&#19968;&#20010;&#21464;&#20998;&#21442;&#25968;&#21270;&#30005;&#36335;&#34987;&#32435;&#20837;&#20316;&#20026;&#31216;&#20026;&#21464;&#20998;Q&#30340;&#36755;&#20837;&#23618;
&lt;/p&gt;
&lt;p&gt;
Quantum machine learning has established as an interdisciplinary field to overcome limitations of classical machine learning and neural networks. This is a field of research which can prove that quantum computers are able to solve problems with complex correlations between inputs that can be hard for classical computers. This suggests that learning models made on quantum computers may be more powerful for applications, potentially faster computation and better generalization on less data. The objective of this paper is to investigate how training of quantum neural network (QNNs) can be done using quantum optimization algorithms for improving the performance and time complexity of QNNs. A classical neural network can be partially quantized to create a hybrid quantum-classical neural network which is used mainly in classification and image recognition. In this paper, a QNN structure is made where a variational parameterized circuit is incorporated as an input layer named as Variational Q
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#36229;&#36234;&#20256;&#32479;&#27010;&#29575;&#29702;&#35299;&#30340;&#19981;&#30830;&#23450;&#24615;&#20915;&#31574;&#65292;&#20171;&#32461;&#20102;&#35299;&#20915;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#21644;&#23545;&#25239;&#34892;&#20026;&#30340;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#21450;&#20854;&#25193;&#23637;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#23637;&#31034;&#20102;&#22810;&#31181;&#35299;&#20915;&#31163;&#25955;&#21644;&#36830;&#32493;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.05848</link><description>&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#20915;&#31574;&#65306;&#36229;&#36234;&#27010;&#29575;
&lt;/p&gt;
&lt;p&gt;
Decision-Making Under Uncertainty: Beyond Probabilities. (arXiv:2303.05848v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.05848](http://arxiv.org/abs/2303.05848)
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#23545;&#19981;&#30830;&#23450;&#24615;&#20915;&#31574;&#30340;&#26368;&#26032;&#21457;&#23637;&#36827;&#34892;&#20102;&#22238;&#39038;&#12290;&#20256;&#32479;&#30340;&#20551;&#35774;&#26159;&#27010;&#29575;&#21487;&#20197;&#20805;&#20998;&#25429;&#25417;&#31995;&#32479;&#20013;&#30340;&#25152;&#26377;&#19981;&#30830;&#23450;&#24615;&#12290;&#26412;&#25991;&#20851;&#27880;&#36229;&#36234;&#36825;&#31181;&#20256;&#32479;&#35299;&#37322;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#28165;&#26224;&#21306;&#20998;&#27169;&#31946;&#19981;&#30830;&#23450;&#24615;&#21644;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#12290;&#25991;&#31456;&#27010;&#36848;&#20102;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#21450;&#20854;&#25193;&#23637;&#65292;&#20197;&#35299;&#20915;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#21644;&#23545;&#25239;&#34892;&#20026;&#30340;&#38382;&#39064;&#12290;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#24456;&#22909;&#22320;&#25429;&#25417;&#27169;&#31946;&#19981;&#30830;&#23450;&#24615;&#65292;&#20294;&#26410;&#33021;&#20805;&#20998;&#32771;&#34385;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23545;&#25152;&#35859;&#30340;&#8220;&#19981;&#30830;&#23450;&#24615;&#27169;&#22411;&#8221;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#27010;&#36848;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#20123;&#31163;&#25955;&#21644;&#36830;&#32493;&#27169;&#22411;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#20174;&#24418;&#24335;&#21270;&#39564;&#35777;&#12289;&#22522;&#20110;&#25511;&#21046;&#30340;&#25277;&#35937;&#21040;&#24378;&#21270;&#23398;&#20064;&#12290;&#20316;&#20026;&#26412;&#25991;&#30340;&#19968;&#20010;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#25105;&#20204;&#21015;&#20986;&#24182;&#35752;&#35770;&#20102;&#33509;&#24178;&#20010;&#20363;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
This position paper reflects on the state-of-the-art in decision-making under uncertainty. A classical assumption is that probabilities can sufficiently capture all uncertainty in a system. In this paper, the focus is on the uncertainty that goes beyond this classical interpretation, particularly by employing a clear distinction between aleatoric and epistemic uncertainty. The paper features an overview of Markov decision processes (MDPs) and extensions to account for partial observability and adversarial behavior. These models sufficiently capture aleatoric uncertainty but fail to account for epistemic uncertainty robustly. Consequently, we present a thorough overview of so-called uncertainty models that exhibit uncertainty in a more robust interpretation. We show several solution techniques for both discrete and continuous models, ranging from formal verification, over control-based abstractions, to reinforcement learning. As an integral part of this paper, we list and discuss severa
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#34920;&#26126;&#65292;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#27169;&#22411;&#26080;&#38656;&#20869;&#37096;&#20998;&#24067;&#24494;&#35843;&#21363;&#21487;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;&#36234;&#30028;&#26816;&#27979;&#34920;&#29616;&#65292;&#24182;&#25552;&#20986;&#26159;&#21542;&#38656;&#35201;&#26032;&#30340;&#35270;&#35273;&#24322;&#24120;&#26816;&#27979;&#22522;&#20934;</title><link>http://arxiv.org/abs/2303.05828</link><description>&lt;p&gt;
&#23545;&#27604;&#35821;&#35328;&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#27169;&#22411;&#26159;&#24378;&#22823;&#30340;&#36234;&#30028;&#26816;&#27979;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive Language-Image Pretrained (CLIP) Models are Powerful Out-of-Distribution Detectors. (arXiv:2303.05828v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.05828](http://arxiv.org/abs/2303.05828)
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#29992;&#20110;&#35270;&#35273;&#36234;&#30028;&#26816;&#27979;&#30340;&#39044;&#35757;&#32451;&#29305;&#24449;&#25552;&#21462;&#22120;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#30740;&#31350;&#12290; &#25105;&#20204;&#26816;&#26597;&#20102;&#20960;&#20010;&#35774;&#32622;&#65292;&#22522;&#20110;&#26631;&#31614;&#25110;&#22270;&#20687;&#26631;&#39064;&#30340;&#21487;&#29992;&#24615;&#65292;&#20351;&#29992;&#19981;&#21516;&#30340;&#20869;&#37096;&#21644;&#22806;&#37096;&#20998;&#24067;&#30340;&#32452;&#21512;&#12290; &#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#65288;i&#65289;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#27169;&#22411;&#20351;&#29992;&#26368;&#36817;&#37051;&#29305;&#24449;&#30456;&#20284;&#24615;&#20316;&#20026;&#36234;&#30028;&#26816;&#27979;&#20998;&#25968;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;&#36234;&#30028;&#26816;&#27979;&#34920;&#29616;&#65292;&#65288;ii&#65289;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#20869;&#37096;&#20998;&#24067;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#30417;&#30563;&#29366;&#24577;&#19979;&#30340;&#26368;&#20808;&#36827;&#36234;&#30028;&#26816;&#27979;&#24615;&#33021;&#65292;&#65288;iii&#65289;&#21363;&#20351;&#26159;&#38024;&#23545;&#33258;&#28982;&#35821;&#35328;&#30417;&#30563;&#36827;&#34892;&#35757;&#32451;&#30340;&#24615;&#33021;&#26368;&#20339;&#30340;&#21313;&#20159;&#32423;&#35270;&#35273;&#21464;&#25442;&#22120;&#20063;&#26080;&#27861;&#26816;&#27979;&#21040;&#32463;&#36807;&#25932;&#23545;&#25805;&#32437;&#30340;&#36234;&#30028;&#22270;&#20687;&#12290; &#26368;&#21518;&#65292;&#25105;&#20204;&#26681;&#25454;&#23454;&#39564;&#32467;&#26524;&#35752;&#35770;&#20102;&#26159;&#21542;&#38656;&#35201;&#26032;&#30340;&#22522;&#20110;&#35270;&#35273;&#24322;&#24120;&#26816;&#27979;&#30340;&#22522;&#20934;&#12290; &#20351;&#29992;&#26368;&#22823;&#30340;&#20844;&#24320;&#21487;&#29992;&#35270;&#35273;&#21464;&#25442;&#22120;&#65292;&#22312;&#25152;&#26377;18&#20010;&#25253;&#21578;&#30340;&#36234;&#30028;&#22522;&#20934;&#27979;&#35797;&#20013;&#37117;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
We present a comprehensive experimental study on pretrained feature extractors for visual out-of-distribution (OOD) detection. We examine several setups, based on the availability of labels or image captions and using different combinations of in- and out-distributions. Intriguingly, we find that (i) contrastive language-image pretrained models achieve state-of-the-art unsupervised out-of-distribution performance using nearest neighbors feature similarity as the OOD detection score, (ii) supervised state-of-the-art OOD detection performance can be obtained without in-distribution fine-tuning, (iii) even top-performing billion-scale vision transformers trained with natural language supervision fail at detecting adversarially manipulated OOD images. Finally, we argue whether new benchmarks for visual anomaly detection are needed based on our experiments. Using the largest publicly available vision transformer, we achieve state-of-the-art performance across all $18$ reported OOD benchmark
&lt;/p&gt;</description></item><item><title>&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#21830;&#21697;&#36741;&#21161;&#20449;&#24687;&#21644;&#26631;&#35760;&#30340;&#34917;&#20805;&#21830;&#21697;&#23545;&#26469;&#29983;&#25104;&#26377;&#25928;&#30340;&#34917;&#20805;&#25512;&#33616;&#65292;&#29992;&#20110;&#26080;&#20849;&#21516;&#36141;&#20080;&#32479;&#35745;&#25968;&#25454;&#30340;&#21830;&#21697;&#12290;&#35813;&#26041;&#27861;&#32500;&#25252;&#27599;&#20010;&#21830;&#21697;&#31867;&#21035;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#23398;&#20064;&#23558;&#20998;&#24067;&#24335;&#21830;&#21697;&#34920;&#31034;&#25237;&#24433;&#21040;&#36825;&#20123;&#31867;&#21035;&#31354;&#38388;&#20013;&#65292;&#20197;&#30830;&#23450;&#21512;&#36866;&#30340;&#25512;&#33616;&#12290;</title><link>http://arxiv.org/abs/2303.05812</link><description>&lt;p&gt;
&#21322;&#30417;&#30563;&#23545;&#25239;&#23398;&#20064;&#29992;&#20110;&#34917;&#20805;&#21830;&#21697;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised Adversarial Learning for Complementary Item Recommendation. (arXiv:2303.05812v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.05812](http://arxiv.org/abs/2303.05812)
&lt;/p&gt;
&lt;p&gt;
&#34917;&#20805;&#21830;&#21697;&#25512;&#33616;&#26159;&#29616;&#20195;&#30005;&#23376;&#21830;&#21153;&#32593;&#31449;&#30340;&#26222;&#36941;&#29305;&#24449;&#12290;&#24403;&#36825;&#20123;&#25512;&#33616;&#22522;&#20110;&#21327;&#20316;&#20449;&#21495;&#65292;&#22914;&#20849;&#21516;&#36141;&#20080;&#32479;&#35745;&#25968;&#25454;&#26102;&#65292;&#23427;&#20204;&#38750;&#24120;&#26377;&#25928;&#12290;&#20294;&#26159;&#65292;&#22312;&#26576;&#20123;&#22312;&#32447;&#24066;&#22330;&#19978;&#65292;&#20363;&#22914;&#22312;&#32447;&#25293;&#21334;&#32593;&#31449;&#65292;&#19981;&#26029;&#26377;&#26032;&#21830;&#21697;&#21152;&#20837;&#30446;&#24405;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#30001;&#20110;&#32570;&#20047;&#20132;&#20114;&#25968;&#25454;&#65292;&#34917;&#20805;&#21830;&#21697;&#25512;&#33616;&#36890;&#24120;&#22522;&#20110;&#21830;&#21697;&#36741;&#21161;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#21033;&#29992;&#21830;&#21697;&#36741;&#21161;&#20449;&#24687;&#21644;&#26631;&#35760;&#30340;&#34917;&#20805;&#21830;&#21697;&#23545;&#26469;&#29983;&#25104;&#26377;&#25928;&#30340;&#34917;&#20805;&#25512;&#33616;&#65292;&#29992;&#20110;&#26080;&#20919;&#21551;&#21160;&#21830;&#21697;&#65292;&#21363;&#23578;&#19981;&#23384;&#22312;&#20849;&#21516;&#36141;&#20080;&#32479;&#35745;&#25968;&#25454;&#30340;&#21830;&#21697;&#12290;&#32771;&#34385;&#21040;&#34917;&#20805;&#21830;&#21697;&#36890;&#24120;&#24517;&#39035;&#19982;&#31181;&#23376;&#21830;&#21697;&#19981;&#21516;&#31867;&#21035;&#65292;&#25105;&#20204;&#20026;&#27599;&#20010;&#21830;&#21697;&#31867;&#21035;&#25216;&#26415;&#32500;&#25252;&#19968;&#20010;&#28508;&#22312;&#31354;&#38388;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#23398;&#20064;&#23558;&#20998;&#24067;&#24335;&#21830;&#21697;&#34920;&#31034;&#25237;&#24433;&#21040;&#36825;&#20123;&#31867;&#21035;&#31354;&#38388;&#20013;&#65292;&#20197;&#30830;&#23450;&#21512;&#36866;&#30340;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Complementary item recommendations are a ubiquitous feature of modern e-commerce sites. Such recommendations are highly effective when they are based on collaborative signals like co-purchase statistics. In certain online marketplaces, however, e.g., on online auction sites, constantly new items are added to the catalog. In such cases, complementary item recommendations are often based on item side-information due to a lack of interaction data. In this work, we propose a novel approach that can leverage both item side-information and labeled complementary item pairs to generate effective complementary recommendations for cold items, i.e., for items for which no co-purchase statistics yet exist. Given that complementary items typically have to be of a different category than the seed item, we technically maintain a latent space for each item category. Simultaneously, we learn to project distributed item representations into these category spaces to determine suitable recommendations. Th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;PG-DRO&#26694;&#26550;&#65292;&#20197;&#27010;&#29575;&#32676;&#20307;&#25104;&#21592;&#36523;&#20221;&#20026;&#22522;&#30784;&#30340;&#20998;&#24067;&#24335;&#40065;&#26834;&#20248;&#21270;&#12290;&#19982;&#20197;&#24448;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#32771;&#34385;&#21040;&#20102;&#36719;&#32452;&#25104;&#21592;&#36523;&#20221;&#32780;&#19981;&#26159;&#30828;&#32452;&#27880;&#37322;&#65292;&#20855;&#26377;&#26356;&#24378;&#30340;&#28789;&#27963;&#24615;&#21644;&#36890;&#29992;&#24615;&#12290;&#35813;&#26041;&#27861;&#36866;&#24212;&#20855;&#26377;&#32452;&#25104;&#21592;&#36523;&#20221;&#27169;&#31946;&#24615;&#30340;&#26679;&#26412;&#65292;&#24182;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#26368;&#20339;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.05809</link><description>&lt;p&gt;
&#24102;&#26377;&#27010;&#29575;&#32452;&#30340;&#20998;&#24067;&#24335;&#40065;&#26834;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Distributionally Robust Optimization with Probabilistic Group. (arXiv:2303.05809v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.05809](http://arxiv.org/abs/2303.05809)
&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#33021;&#23481;&#26131;&#21463;&#21040;&#24179;&#22343;&#32780;&#38750;&#20856;&#22411;&#26679;&#26412;&#32676;&#20307;&#30340;&#34920;&#38754;&#30456;&#20851;&#24615;&#24178;&#25200;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20197;&#21069;&#30340;&#26041;&#27861;&#26368;&#23567;&#21270;&#32463;&#39564;&#26368;&#22351;&#32676;&#20307;&#39118;&#38505;&#12290;&#23613;&#31649;&#26377;&#21069;&#36884;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#20551;&#35774;&#27599;&#20010;&#26679;&#26412;&#23646;&#20110;&#19968;&#20010;&#19988;&#20165;&#19968;&#20010;&#32676;&#20307;&#65292;&#36825;&#19981;&#20801;&#35768;&#34920;&#36798;&#32676;&#20307;&#26631;&#27880;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;PG-DRO&#26694;&#26550;&#65292;&#25506;&#32034;&#22522;&#20110;&#27010;&#29575;&#32676;&#20307;&#25104;&#21592;&#36523;&#20221;&#30340;&#20998;&#24067;&#24335;&#40065;&#26834;&#20248;&#21270;&#30340;&#24605;&#24819;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#30340;&#20851;&#38190;&#26159;&#32771;&#34385;&#21040;&#36719;&#32452;&#25104;&#21592;&#36523;&#20221;&#32780;&#38750;&#30828;&#32452;&#27880;&#37322;&#12290;&#32452;&#27010;&#29575;&#21487;&#20197;&#20351;&#29992;&#30417;&#30563;&#23398;&#20064;&#25110;&#38646;&#26679;&#26412;&#26041;&#27861;&#28789;&#27963;&#29983;&#25104;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#36866;&#24212;&#20855;&#26377;&#32452;&#25104;&#21592;&#36523;&#20221;&#27169;&#31946;&#24615;&#30340;&#26679;&#26412;&#65292;&#27604;&#29616;&#26377;&#25216;&#26415;&#20855;&#26377;&#26356;&#24378;&#30340;&#28789;&#27963;&#24615;&#21644;&#36890;&#29992;&#24615;&#12290;&#25105;&#20204;&#20840;&#38754;&#35780;&#20272;&#20102;PG-DRO&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#34920;&#29616;&#65292;&#24314;&#31435;&#20102;&#36229;&#36807;&#20854;&#20182;&#26041;&#27861;&#30340;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern machine learning models may be susceptible to learning spurious correlations that hold on average but not for the atypical group of samples. To address the problem, previous approaches minimize the empirical worst-group risk. Despite the promise, they often assume that each sample belongs to one and only one group, which does not allow expressing the uncertainty in group labeling. In this paper, we propose a novel framework PG-DRO, which explores the idea of probabilistic group membership for distributionally robust optimization. Key to our framework, we consider soft group membership instead of hard group annotations. The group probabilities can be flexibly generated using either supervised learning or zero-shot approaches. Our framework accommodates samples with group membership ambiguity, offering stronger flexibility and generality than the prior art. We comprehensively evaluate PG-DRO on both image classification and natural language processing benchmarks, establishing supe
&lt;/p&gt;</description></item><item><title>&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22788;&#29702;&#21327;&#26041;&#24046;&#30697;&#38453;&#20998;&#24067;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;M/EEG&#20449;&#21495;&#19978;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#20102;Sliced-Wasserstein&#36317;&#31163;&#21644;&#26680;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#23427;&#26159;&#19968;&#20010;&#26377;&#25928;&#30340;&#26367;&#20195;&#21697;&#65292;&#22312;&#39046;&#22495;&#36866;&#24212;&#30340;Wasserstein&#36317;&#31163;&#20013;&#34987;&#35777;&#26126;&#26159;&#38750;&#24120;&#39640;&#25928;&#30340;&#12290;</title><link>http://arxiv.org/abs/2303.05798</link><description>&lt;p&gt;
&#23545;&#20110;M/EEG&#20449;&#21495;&#30340;&#23545;&#31216;&#27491;&#23450;&#30697;&#38453;&#19978;&#20999;&#29255;Wasserstein
&lt;/p&gt;
&lt;p&gt;
Sliced-Wasserstein on Symmetric Positive Definite Matrices for M/EEG Signals. (arXiv:2303.05798v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.05798](http://arxiv.org/abs/2303.05798)
&lt;/p&gt;
&lt;p&gt;
&#22788;&#29702;&#30005;&#25110;&#30913;&#22270;&#35760;&#24405;&#26102;&#65292;&#35768;&#22810;&#30417;&#30563;&#30340;&#39044;&#27979;&#20219;&#21153;&#36890;&#36807;&#20351;&#29992;&#21327;&#26041;&#24046;&#30697;&#38453;&#26469;&#24635;&#32467;&#20449;&#21495;&#26469;&#35299;&#20915;&#12290;&#20351;&#29992;&#36825;&#20123;&#30697;&#38453;&#30340;&#23398;&#20064;&#38656;&#35201;&#20351;&#29992;Riemanian&#20960;&#20309;&#26469;&#32771;&#34385;&#23427;&#20204;&#30340;&#32467;&#26500;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22788;&#29702;&#21327;&#26041;&#24046;&#30697;&#38453;&#20998;&#24067;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#22312;M / EEG&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#19978;&#23637;&#31034;&#20854;&#35745;&#31639;&#25928;&#29575;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#23545;&#31216;&#27491;&#23450;&#30697;&#38453;&#27979;&#37327;&#20043;&#38388;&#30340;&#20999;&#29255;Wasserstein&#36317;&#31163;&#65292;&#24182;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;&#20854;&#23646;&#24615;&#21644;&#26680;&#26041;&#27861;&#23558;&#27492;&#36317;&#31163;&#24212;&#29992;&#20110;&#20174;MEG&#25968;&#25454;&#20013;&#33041;&#24180;&#40836;&#39044;&#27979;&#65292;&#24182;&#23558;&#20854;&#19982;&#22522;&#20110;Riemannian&#20960;&#20309;&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#22312;&#39046;&#22495;&#36866;&#24212;&#30340;Wasserstein&#36317;&#31163;&#20013;&#26159;&#19968;&#20010;&#26377;&#25928;&#30340;&#26367;&#20195;&#21697;&#65292;&#29992;&#20110;&#33041;&#26426;&#25509;&#21475;&#24212;&#29992;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
When dealing with electro or magnetoencephalography records, many supervised prediction tasks are solved by working with covariance matrices to summarize the signals. Learning with these matrices requires using Riemanian geometry to account for their structure. In this paper, we propose a new method to deal with distributions of covariance matrices and demonstrate its computational efficiency on M/EEG multivariate time series. More specifically, we define a Sliced-Wasserstein distance between measures of symmetric positive definite matrices that comes with strong theoretical guarantees. Then, we take advantage of its properties and kernel methods to apply this distance to brain-age prediction from MEG data and compare it to state-of-the-art algorithms based on Riemannian geometry. Finally, we show that it is an efficient surrogate to the Wasserstein distance in domain adaptation for Brain Computer Interface applications.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#30740;&#31350;&#20102;&#30830;&#23450;&#24615;&#19981;&#30830;&#23450;&#24615;&#26041;&#27861;&#20013;&#30340;&#37325;&#35201;&#35774;&#35745;&#36873;&#25321;&#65292;&#21253;&#25324;&#35299;&#32806;&#26680;&#24515;&#26550;&#26500;&#21644;&#19981;&#30830;&#23450;&#24615;&#22836;&#26041;&#26696;&#30340;&#35757;&#32451;&#26041;&#26696;&#12289;&#26680;&#24515;&#26550;&#26500;&#30340;&#34920;&#36798;&#33021;&#21147;&#23545;&#19981;&#30830;&#23450;&#24615;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#21450;&#19982;&#20854;&#20182;&#36125;&#21494;&#26031;&#27169;&#22411;&#30456;&#21453;&#65292;DUMs&#23450;&#20041;&#30340;&#20808;&#39564;&#19981;&#20250;&#23545;&#26368;&#32456;&#24615;&#33021;&#20135;&#29983;&#24378;&#28872;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2303.05796</link><description>&lt;p&gt;
&#30830;&#23450;&#24615;&#19981;&#30830;&#23450;&#24615;&#26041;&#27861;&#30340;&#35757;&#32451;&#12289;&#26550;&#26500;&#21644;&#20808;&#39564;
&lt;/p&gt;
&lt;p&gt;
Training, Architecture, and Prior for Deterministic Uncertainty Methods. (arXiv:2303.05796v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.05796](http://arxiv.org/abs/2303.05796)
&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#39640;&#25928;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#23545;&#20110;&#26500;&#24314;&#21487;&#38752;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#38750;&#24120;&#37325;&#35201;&#65292;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#25552;&#20379;&#26657;&#20934;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12289;&#25512;&#24191;&#24182;&#26816;&#27979;&#36229;&#20986;&#20998;&#24067;&#30340;&#25968;&#25454;&#38598;&#12290;&#20026;&#27492;&#65292;&#30830;&#23450;&#24615;&#19981;&#30830;&#23450;&#24615;&#26041;&#27861;&#65288;DUMs&#65289;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#27169;&#22411;&#26063;&#65292;&#33021;&#22815;&#22312;&#21333;&#27425;&#21069;&#21521;&#20256;&#36882;&#20013;&#25191;&#34892;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;DUM&#20013;&#30340;&#37325;&#35201;&#35774;&#35745;&#36873;&#25321;&#65306;(1)&#25105;&#20204;&#34920;&#26126;&#65292;&#35299;&#32806;&#26680;&#24515;&#26550;&#26500;&#21644;&#19981;&#30830;&#23450;&#24615;&#22836;&#26041;&#26696;&#30340;&#35757;&#32451;&#26041;&#26696;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#19981;&#30830;&#23450;&#24615;&#24615;&#33021;&#12290;(2)&#25105;&#20204;&#35777;&#26126;&#20102;&#26680;&#24515;&#26550;&#26500;&#30340;&#34920;&#36798;&#33021;&#21147;&#23545;&#19981;&#30830;&#23450;&#24615;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#19988;&#36991;&#20813;&#29305;&#24449;&#23849;&#28291;&#30340;&#20854;&#20182;&#26550;&#26500;&#32422;&#26463;&#21487;&#33021;&#20250;&#24694;&#21270;OOD&#27010;&#25324;&#21644;&#26816;&#27979;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;(3)&#19982;&#20854;&#20182;&#36125;&#21494;&#26031;&#27169;&#22411;&#30456;&#21453;&#65292;&#25105;&#20204;&#34920;&#26126;DUMs&#23450;&#20041;&#30340;&#20808;&#39564;&#19981;&#20250;&#23545;&#26368;&#32456;&#24615;&#33021;&#20135;&#29983;&#24378;&#28872;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate and efficient uncertainty estimation is crucial to build reliable Machine Learning (ML) models capable to provide calibrated uncertainty estimates, generalize and detect Out-Of-Distribution (OOD) datasets. To this end, Deterministic Uncertainty Methods (DUMs) is a promising model family capable to perform uncertainty estimation in a single forward pass. This work investigates important design choices in DUMs: (1) we show that training schemes decoupling the core architecture and the uncertainty head schemes can significantly improve uncertainty performances. (2) we demonstrate that the core architecture expressiveness is crucial for uncertainty performance and that additional architecture constraints to avoid feature collapse can deteriorate the trade-off between OOD generalization and detection. (3) Contrary to other Bayesian models, we show that the prior defined by DUMs do not have a strong effect on the final performances.
&lt;/p&gt;</description></item><item><title>&#20256;&#32479;&#30340;&#33258;&#36866;&#24212;&#31639;&#27861;&#22312;&#22788;&#29702;&#21160;&#24577;&#22122;&#22768;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#30340;&#29983;&#25104;&#22266;&#23450;&#28388;&#27874;&#22120;&#26377;&#28304;&#22122;&#22768;&#25511;&#21046;&#65288;GFANC&#65289;&#26041;&#27861;&#65292;&#20165;&#38656;&#23569;&#37327;&#20808;&#39564;&#25968;&#25454;&#21363;&#21487;&#33258;&#21160;&#29983;&#25104;&#36866;&#29992;&#20110;&#21508;&#31181;&#22122;&#22768;&#30340;&#25511;&#21046;&#28388;&#27874;&#22120;&#65292;&#24182;&#33021;&#22815;&#21462;&#24471;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.05788</link><description>&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#22266;&#23450;&#28388;&#27874;&#22120;&#26377;&#28304;&#22122;&#22768;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Deep Generative Fixed-filter Active Noise Control. (arXiv:2303.05788v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.05788](http://arxiv.org/abs/2303.05788)
&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#25910;&#25947;&#36895;&#24230;&#24930;&#65292;&#36319;&#36394;&#33021;&#21147;&#36739;&#24046;&#65292;&#20256;&#32479;&#30340;LMS&#33258;&#36866;&#24212;&#31639;&#27861;&#19981;&#22826;&#33021;&#22815;&#22788;&#29702;&#21160;&#24577;&#22122;&#22768;&#12290;&#36873;&#25321;&#24615;&#22266;&#23450;&#28388;&#27874;&#22120;&#26377;&#28304;&#22122;&#22768;&#25511;&#21046;&#65288;SFANC&#65289;&#21487;&#20197;&#36890;&#36807;&#20026;&#19981;&#21516;&#30340;&#22122;&#22768;&#36873;&#25321;&#21512;&#36866;&#30340;&#39044;&#35757;&#32451;&#25511;&#21046;&#28388;&#27874;&#22120;&#65292;&#26174;&#33879;&#32553;&#30701;&#21709;&#24212;&#26102;&#38388;&#12290;&#28982;&#32780;&#65292;&#39044;&#35757;&#32451;&#25511;&#21046;&#28388;&#27874;&#22120;&#25968;&#37327;&#26377;&#38480;&#21487;&#33021;&#20250;&#24433;&#21709;&#22122;&#22768;&#38477;&#20302;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#24403;&#20837;&#23556;&#22122;&#22768;&#19982;&#39044;&#35757;&#32451;&#26399;&#38388;&#30340;&#21021;&#22987;&#22122;&#22768;&#24046;&#24322;&#24456;&#22823;&#26102;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;&#23436;&#32654;&#37325;&#26500;&#28388;&#27874;&#22120;&#32452;&#30340;&#29983;&#25104;&#22266;&#23450;&#28388;&#27874;&#22120;&#26377;&#28304;&#22122;&#22768;&#25511;&#21046;&#65288;GFANC&#65289;&#26041;&#27861;&#65292;&#21482;&#38656;&#35201;&#23569;&#37327;&#20808;&#39564;&#25968;&#25454;&#65288;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#23485;&#24102;&#25511;&#21046;&#28388;&#27874;&#22120;&#65289;&#21363;&#21487;&#33258;&#21160;&#29983;&#25104;&#36866;&#29992;&#20110;&#21508;&#31181;&#22122;&#22768;&#30340;&#25511;&#21046;&#28388;&#27874;&#22120;&#12290;&#36890;&#36807;&#23545;&#30495;&#23454;&#35760;&#24405;&#22122;&#22768;&#30340;&#25968;&#20540;&#27169;&#25311;&#65292;&#35777;&#26126;&#20102;GFANC&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the slow convergence and poor tracking ability, conventional LMS-based adaptive algorithms are less capable of handling dynamic noises. Selective fixed-filter active noise control (SFANC) can significantly reduce response time by selecting appropriate pre-trained control filters for different noises. Nonetheless, the limited number of pre-trained control filters may affect noise reduction performance, especially when the incoming noise differs much from the initial noises during pre-training. Therefore, a generative fixed-filter active noise control (GFANC) method is proposed in this paper to overcome the limitation. Based on deep learning and a perfect-reconstruction filter bank, the GFANC method only requires a few prior data (one pre-trained broadband control filter) to automatically generate suitable control filters for various noises. The efficacy of the GFANC method is demonstrated by numerical simulations on real-recorded noises.
&lt;/p&gt;</description></item></channel></rss>