<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#27169;&#24577;&#36335;&#24452;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20854;&#20182;&#27169;&#24577;&#30340;&#26080;&#20851;&#25968;&#25454;&#26469;&#25913;&#36827;&#29305;&#23450;&#27169;&#24577;&#30340;Transformer&#65292;&#23454;&#29616;&#20102;&#20004;&#20010;&#27169;&#22411;&#20043;&#38388;&#30340;&#32452;&#20214;&#36830;&#25509;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24207;&#21015;&#24314;&#27169;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.14405</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#36335;&#24452;&#65306;&#36890;&#36807;&#20854;&#20182;&#27169;&#24577;&#30340;&#26080;&#20851;&#25968;&#25454;&#26469;&#25913;&#36827;Transformer
&lt;/p&gt;
&lt;p&gt;
Multimodal Pathway: Improve Transformers with Irrelevant Data from Other Modalities. (arXiv:2401.14405v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14405
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#27169;&#24577;&#36335;&#24452;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20854;&#20182;&#27169;&#24577;&#30340;&#26080;&#20851;&#25968;&#25454;&#26469;&#25913;&#36827;&#29305;&#23450;&#27169;&#24577;&#30340;Transformer&#65292;&#23454;&#29616;&#20102;&#20004;&#20010;&#27169;&#22411;&#20043;&#38388;&#30340;&#32452;&#20214;&#36830;&#25509;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24207;&#21015;&#24314;&#27169;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#26469;&#33258;&#20854;&#20182;&#27169;&#24577;&#30340;&#26080;&#20851;&#25968;&#25454;&#26469;&#25913;&#36827;&#29305;&#23450;&#27169;&#24577;&#30340;Transformer&#65292;&#20363;&#22914;&#65292;&#20351;&#29992;&#38899;&#39057;&#25110;&#28857;&#20113;&#25968;&#25454;&#38598;&#26469;&#25913;&#36827;ImageNet&#27169;&#22411;&#12290;&#25105;&#20204;&#24378;&#35843;&#30446;&#26631;&#27169;&#24577;&#30340;&#25968;&#25454;&#26679;&#26412;&#19982;&#20854;&#20182;&#27169;&#24577;&#26080;&#20851;&#65292;&#36825;&#19982;&#21033;&#29992;&#19981;&#21516;&#27169;&#24577;&#30340;&#37197;&#23545;&#25968;&#25454;&#65288;&#22914;CLIP&#65289;&#25110;&#20132;&#38169;&#25968;&#25454;&#30340;&#20854;&#20182;&#26041;&#27861;&#19981;&#21516;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#27169;&#24577;&#36335;&#24452;&#30340;&#26041;&#27861;-&#32473;&#23450;&#30446;&#26631;&#27169;&#24577;&#21644;&#35774;&#35745;&#29992;&#20110;&#35813;&#27169;&#24577;&#30340;Transformer&#65292;&#25105;&#20204;&#20351;&#29992;&#20351;&#29992;&#21478;&#19968;&#20010;&#27169;&#24577;&#30340;&#25968;&#25454;&#35757;&#32451;&#30340;&#36741;&#21161;Transformer&#65292;&#24182;&#26500;&#24314;&#36335;&#24452;&#26469;&#36830;&#25509;&#20004;&#20010;&#27169;&#22411;&#30340;&#32452;&#20214;&#65292;&#20197;&#20415;&#30446;&#26631;&#27169;&#24577;&#30340;&#25968;&#25454;&#21487;&#20197;&#34987;&#20004;&#20010;&#27169;&#22411;&#22788;&#29702;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#20174;&#20004;&#20010;&#27169;&#24577;&#33719;&#24471;&#30340;Transformer&#30340;&#36890;&#29992;&#24207;&#21015;&#24314;&#27169;&#33021;&#21147;&#12290;&#20316;&#20026;&#20855;&#20307;&#23454;&#29616;&#65292;&#25105;&#20204;&#36890;&#24120;&#20351;&#29992;&#29305;&#23450;&#27169;&#24577;&#30340;tokenizer&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;head&#65292;&#20294;&#26159;&#21033;&#29992;&#36741;&#21161;&#27169;&#22411;&#30340;Transformer block&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose to improve transformers of a specific modality with irrelevant data from other modalities, e.g., improve an ImageNet model with audio or point cloud datasets. We would like to highlight that the data samples of the target modality are irrelevant to the other modalities, which distinguishes our method from other works utilizing paired (e.g., CLIP) or interleaved data of different modalities. We propose a methodology named Multimodal Pathway - given a target modality and a transformer designed for it, we use an auxiliary transformer trained with data of another modality and construct pathways to connect components of the two models so that data of the target modality can be processed by both models. In this way, we utilize the universal sequence-to-sequence modeling abilities of transformers obtained from two modalities. As a concrete implementation, we use a modality-specific tokenizer and task-specific head as usual but utilize the transformer blocks of the auxiliary model v
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#26368;&#21021;&#29992;&#20110;&#22270;&#20687;&#29983;&#25104;&#30340;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65288;DDM&#65289;&#36827;&#34892;&#20102;&#35299;&#26500;&#65292;&#21457;&#29616;&#21482;&#26377;&#24456;&#23569;&#30340;&#29616;&#20195;&#32452;&#20214;&#23545;&#20110;&#23398;&#20064;&#33391;&#22909;&#30340;&#34920;&#31034;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#36890;&#36807;&#23558;DDM&#36880;&#27493;&#36716;&#21270;&#20026;&#32463;&#20856;&#30340;&#21435;&#22122;&#33258;&#32534;&#30721;&#22120;&#65288;DAE&#65289;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#24230;&#31616;&#21270;&#19988;&#31867;&#20284;&#20110;&#32463;&#20856;DAE&#30340;&#26041;&#27861;&#12290;&#35813;&#30740;&#31350;&#24076;&#26395;&#37325;&#26032;&#24341;&#36215;&#20154;&#20204;&#23545;&#29616;&#20195;&#33258;&#30417;&#30563;&#23398;&#20064;&#39046;&#22495;&#20013;&#32463;&#20856;&#26041;&#27861;&#30340;&#20852;&#36259;&#12290;</title><link>http://arxiv.org/abs/2401.14404</link><description>&lt;p&gt;
&#35299;&#26500;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deconstructing Denoising Diffusion Models for Self-Supervised Learning. (arXiv:2401.14404v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14404
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#26368;&#21021;&#29992;&#20110;&#22270;&#20687;&#29983;&#25104;&#30340;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65288;DDM&#65289;&#36827;&#34892;&#20102;&#35299;&#26500;&#65292;&#21457;&#29616;&#21482;&#26377;&#24456;&#23569;&#30340;&#29616;&#20195;&#32452;&#20214;&#23545;&#20110;&#23398;&#20064;&#33391;&#22909;&#30340;&#34920;&#31034;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#36890;&#36807;&#23558;DDM&#36880;&#27493;&#36716;&#21270;&#20026;&#32463;&#20856;&#30340;&#21435;&#22122;&#33258;&#32534;&#30721;&#22120;&#65288;DAE&#65289;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#24230;&#31616;&#21270;&#19988;&#31867;&#20284;&#20110;&#32463;&#20856;DAE&#30340;&#26041;&#27861;&#12290;&#35813;&#30740;&#31350;&#24076;&#26395;&#37325;&#26032;&#24341;&#36215;&#20154;&#20204;&#23545;&#29616;&#20195;&#33258;&#30417;&#30563;&#23398;&#20064;&#39046;&#22495;&#20013;&#32463;&#20856;&#26041;&#27861;&#30340;&#20852;&#36259;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26368;&#21021;&#29992;&#20110;&#22270;&#20687;&#29983;&#25104;&#30340;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65288;DDM&#65289;&#30340;&#34920;&#31034;&#23398;&#20064;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#29702;&#24565;&#26159;&#36880;&#27493;&#22320;&#35299;&#26500;DDM&#65292;&#23558;&#20854;&#36716;&#21270;&#20026;&#32463;&#20856;&#30340;&#21435;&#22122;&#33258;&#32534;&#30721;&#22120;&#65288;DAE&#65289;&#12290;&#36825;&#31181;&#35299;&#26500;&#36807;&#31243;&#20801;&#35768;&#25105;&#20204;&#25506;&#32034;&#29616;&#20195;DDM&#30340;&#21508;&#20010;&#32452;&#20214;&#22914;&#20309;&#24433;&#21709;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#21482;&#26377;&#24456;&#23569;&#30340;&#29616;&#20195;&#32452;&#20214;&#23545;&#20110;&#23398;&#20064;&#33391;&#22909;&#30340;&#34920;&#31034;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#32780;&#20854;&#20182;&#35768;&#22810;&#32452;&#20214;&#21017;&#26159;&#38750;&#24517;&#35201;&#30340;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26368;&#32456;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#24230;&#31616;&#21270;&#19988;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#31867;&#20284;&#20110;&#32463;&#20856;DAE&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#30740;&#31350;&#33021;&#37325;&#26032;&#24341;&#36215;&#20154;&#20204;&#23545;&#29616;&#20195;&#33258;&#30417;&#30563;&#23398;&#20064;&#39046;&#22495;&#20013;&#19968;&#31867;&#32463;&#20856;&#26041;&#27861;&#30340;&#20852;&#36259;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we examine the representation learning abilities of Denoising Diffusion Models (DDM) that were originally purposed for image generation. Our philosophy is to deconstruct a DDM, gradually transforming it into a classical Denoising Autoencoder (DAE). This deconstructive procedure allows us to explore how various components of modern DDMs influence self-supervised representation learning. We observe that only a very few modern components are critical for learning good representations, while many others are nonessential. Our study ultimately arrives at an approach that is highly simplified and to a large extent resembles a classical DAE. We hope our study will rekindle interest in a family of classical methods within the realm of modern self-supervised learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#24320;&#25918;&#29615;&#22659;&#20013;&#20851;&#33410;&#29289;&#20307;&#25805;&#20316;&#30340;&#20840;&#26632;&#26041;&#27861;&#65292;&#26426;&#22120;&#20154;&#36890;&#36807;&#33258;&#36866;&#24212;&#23398;&#20064;&#26694;&#26550;&#20174;&#23569;&#37327;&#25968;&#25454;&#20013;&#23398;&#20064;&#65292;&#24182;&#36890;&#36807;&#22312;&#32447;&#23454;&#36341;&#23398;&#20064;&#36866;&#24212;&#35757;&#32451;&#20998;&#24067;&#20043;&#22806;&#30340;&#26032;&#23545;&#35937;&#12290;&#21516;&#26102;&#65292;&#36824;&#24320;&#21457;&#20102;&#20302;&#25104;&#26412;&#30340;&#31227;&#21160;&#25805;&#20316;&#30828;&#20214;&#24179;&#21488;&#12290;</title><link>http://arxiv.org/abs/2401.14403</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#31227;&#21160;&#25805;&#20316;&#22312;&#24320;&#25918;&#29615;&#22659;&#20013;&#30340;&#21487;&#20851;&#33410;&#29289;&#20307;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Adaptive Mobile Manipulation for Articulated Objects In the Open World. (arXiv:2401.14403v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#24320;&#25918;&#29615;&#22659;&#20013;&#20851;&#33410;&#29289;&#20307;&#25805;&#20316;&#30340;&#20840;&#26632;&#26041;&#27861;&#65292;&#26426;&#22120;&#20154;&#36890;&#36807;&#33258;&#36866;&#24212;&#23398;&#20064;&#26694;&#26550;&#20174;&#23569;&#37327;&#25968;&#25454;&#20013;&#23398;&#20064;&#65292;&#24182;&#36890;&#36807;&#22312;&#32447;&#23454;&#36341;&#23398;&#20064;&#36866;&#24212;&#35757;&#32451;&#20998;&#24067;&#20043;&#22806;&#30340;&#26032;&#23545;&#35937;&#12290;&#21516;&#26102;&#65292;&#36824;&#24320;&#21457;&#20102;&#20302;&#25104;&#26412;&#30340;&#31227;&#21160;&#25805;&#20316;&#30828;&#20214;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24320;&#25918;&#30340;&#26080;&#32467;&#26500;&#29615;&#22659;&#20013;&#37096;&#32626;&#26426;&#22120;&#20154;&#19968;&#30452;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#26426;&#22120;&#20154;&#36890;&#24120;&#21482;&#22312;&#23553;&#38381;&#30340;&#23454;&#39564;&#23460;&#29615;&#22659;&#20013;&#36827;&#34892;&#30740;&#31350;&#65292;&#20043;&#21069;&#30340;&#31227;&#21160;&#25805;&#20316;&#24037;&#20316;&#20063;&#20165;&#38480;&#20110;&#25342;&#21462;&#12289;&#31227;&#21160;&#12289;&#25918;&#32622;&#65292;&#36825;&#22312;&#36825;&#19968;&#39046;&#22495;&#20013;&#21482;&#26159;&#20912;&#23665;&#19968;&#35282;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#24320;&#25918;&#19990;&#30028;&#31227;&#21160;&#25805;&#20316;&#31995;&#32479;&#65292;&#37319;&#29992;&#20840;&#26632;&#26041;&#27861;&#26469;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#20013;&#21487;&#20851;&#33410;&#29289;&#20307;&#30340;&#25805;&#20316;&#65292;&#20363;&#22914;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#38376;&#12289;&#26588;&#23376;&#12289;&#25277;&#23625;&#21644;&#20912;&#31665;&#12290;&#26426;&#22120;&#20154;&#21033;&#29992;&#33258;&#36866;&#24212;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#34892;&#20026;&#20811;&#38534;&#20808;&#20174;&#19968;&#23567;&#32452;&#25968;&#25454;&#20013;&#23398;&#20064;&#65292;&#28982;&#21518;&#36890;&#36807;&#22312;&#32447;&#23454;&#36341;&#23398;&#20064;&#26469;&#22788;&#29702;&#35757;&#32451;&#20998;&#24067;&#20043;&#22806;&#30340;&#26032;&#23545;&#35937;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#20302;&#25104;&#26412;&#30340;&#31227;&#21160;&#25805;&#20316;&#30828;&#20214;&#24179;&#21488;&#65292;&#33021;&#22815;&#22312;&#26080;&#32467;&#26500;&#29615;&#22659;&#20013;&#36827;&#34892;&#23433;&#20840;&#21644;&#33258;&#20027;&#30340;&#22312;&#32447;&#36866;&#24212;&#65292;&#25104;&#26412;&#32422;&#20026;20,000&#32654;&#20803;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;20&#20010;&#21487;&#20851;&#33410;&#30340;&#29289;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deploying robots in open-ended unstructured environments such as homes has been a long-standing research problem. However, robots are often studied only in closed-off lab settings, and prior mobile manipulation work is restricted to pick-move-place, which is arguably just the tip of the iceberg in this area. In this paper, we introduce Open-World Mobile Manipulation System, a full-stack approach to tackle realistic articulated object operation, e.g. real-world doors, cabinets, drawers, and refrigerators in open-ended unstructured environments. The robot utilizes an adaptive learning framework to initially learns from a small set of data through behavior cloning, followed by learning from online practice on novel objects that fall outside the training distribution. We also develop a low-cost mobile manipulation hardware platform capable of safe and autonomous online adaptation in unstructured environments with a cost of around 20,000 USD. In our experiments we utilize 20 articulate obje
&lt;/p&gt;</description></item><item><title>pix2gestalt&#26159;&#19968;&#20010;&#29992;&#20110;&#38646;&#26679;&#26412;&#38750;&#27169;&#24577;&#20998;&#21106;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#23398;&#20064;&#20272;&#35745;&#34987;&#36974;&#25377;&#30340;&#25972;&#20010;&#23545;&#35937;&#30340;&#24418;&#29366;&#21644;&#22806;&#35266;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#35268;&#27169;&#25193;&#25955;&#27169;&#22411;&#65292;&#23398;&#20064;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#26469;&#37325;&#24314;&#25972;&#20010;&#23545;&#35937;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#20248;&#20110;&#26377;&#30417;&#30563;&#30340;&#22522;&#32447;&#26041;&#27861;&#65292;&#21516;&#26102;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22312;&#23384;&#22312;&#36974;&#25377;&#24773;&#20917;&#19979;&#30340;&#23545;&#35937;&#35782;&#21035;&#21644;3D&#37325;&#24314;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.14398</link><description>&lt;p&gt;
pix2gestalt&#65306;&#36890;&#36807;&#21512;&#25104;&#25972;&#20307;&#36827;&#34892;&#38750;&#27169;&#24577;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
pix2gestalt: Amodal Segmentation by Synthesizing Wholes. (arXiv:2401.14398v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14398
&lt;/p&gt;
&lt;p&gt;
pix2gestalt&#26159;&#19968;&#20010;&#29992;&#20110;&#38646;&#26679;&#26412;&#38750;&#27169;&#24577;&#20998;&#21106;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#23398;&#20064;&#20272;&#35745;&#34987;&#36974;&#25377;&#30340;&#25972;&#20010;&#23545;&#35937;&#30340;&#24418;&#29366;&#21644;&#22806;&#35266;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#35268;&#27169;&#25193;&#25955;&#27169;&#22411;&#65292;&#23398;&#20064;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#26469;&#37325;&#24314;&#25972;&#20010;&#23545;&#35937;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#20248;&#20110;&#26377;&#30417;&#30563;&#30340;&#22522;&#32447;&#26041;&#27861;&#65292;&#21516;&#26102;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22312;&#23384;&#22312;&#36974;&#25377;&#24773;&#20917;&#19979;&#30340;&#23545;&#35937;&#35782;&#21035;&#21644;3D&#37325;&#24314;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;pix2gestalt&#65292;&#19968;&#20010;&#29992;&#20110;&#38646;&#26679;&#26412;&#38750;&#27169;&#24577;&#20998;&#21106;&#30340;&#26694;&#26550;&#65292;&#23427;&#23398;&#20064;&#20272;&#35745;&#37096;&#20998;&#34987;&#36974;&#25377;&#30340;&#25972;&#20010;&#23545;&#35937;&#30340;&#24418;&#29366;&#21644;&#22806;&#35266;&#12290;&#36890;&#36807;&#21033;&#29992;&#22823;&#35268;&#27169;&#25193;&#25955;&#27169;&#22411;&#24182;&#23558;&#23427;&#20204;&#30340;&#34920;&#31034;&#36801;&#31227;&#21040;&#36825;&#20010;&#20219;&#21153;&#19978;&#65292;&#25105;&#20204;&#23398;&#20064;&#20102;&#19968;&#20010;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#37325;&#24314;&#25972;&#20010;&#23545;&#35937;&#65292;&#21253;&#25324;&#36829;&#21453;&#33258;&#28982;&#21644;&#29289;&#29702;&#20808;&#39564;&#30340;&#33402;&#26415;&#21697;&#31561;&#31034;&#20363;&#12290;&#20316;&#20026;&#35757;&#32451;&#25968;&#25454;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#32463;&#36807;&#21512;&#25104;&#31579;&#36873;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#24102;&#26377;&#36974;&#25377;&#23545;&#35937;&#21450;&#20854;&#25972;&#20010;&#23545;&#35937;&#23545;&#24212;&#29289;&#30340;&#37197;&#23545;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24050;&#24314;&#31435;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#20248;&#20110;&#26377;&#30417;&#30563;&#30340;&#22522;&#32447;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#22312;&#23384;&#22312;&#36974;&#25377;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#36824;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#29616;&#26377;&#23545;&#35937;&#35782;&#21035;&#21644;3D&#37325;&#24314;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce pix2gestalt, a framework for zero-shot amodal segmentation, which learns to estimate the shape and appearance of whole objects that are only partially visible behind occlusions. By capitalizing on large-scale diffusion models and transferring their representations to this task, we learn a conditional diffusion model for reconstructing whole objects in challenging zero-shot cases, including examples that break natural and physical priors, such as art. As training data, we use a synthetically curated dataset containing occluded objects paired with their whole counterparts. Experiments show that our approach outperforms supervised baselines on established benchmarks. Our model can furthermore be used to significantly improve the performance of existing object recognition and 3D reconstruction methods in the presence of occlusions.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21106;&#24179;&#38754;&#26041;&#27861;&#30340;&#24179;&#28369;&#25490;&#21517;SVM&#31639;&#27861;&#26469;&#26368;&#22823;&#21270;&#20998;&#31867;&#24615;&#33021;&#20013;&#30340;AUC&#25351;&#26631;&#12290;&#36890;&#36807;&#36845;&#20195;&#24341;&#20837;&#21106;&#24179;&#38754;&#20197;&#38450;&#27490;&#36807;&#25311;&#21512;&#65292;&#24182;&#24809;&#32602;&#27169;&#22411;&#30340;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2401.14388</link><description>&lt;p&gt;
&#21033;&#29992;&#21106;&#24179;&#38754;&#26041;&#27861;&#30340;&#24179;&#28369;&#25490;&#21517;SVM
&lt;/p&gt;
&lt;p&gt;
Smooth Ranking SVM via Cutting-Plane Method. (arXiv:2401.14388v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14388
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21106;&#24179;&#38754;&#26041;&#27861;&#30340;&#24179;&#28369;&#25490;&#21517;SVM&#31639;&#27861;&#26469;&#26368;&#22823;&#21270;&#20998;&#31867;&#24615;&#33021;&#20013;&#30340;AUC&#25351;&#26631;&#12290;&#36890;&#36807;&#36845;&#20195;&#24341;&#20837;&#21106;&#24179;&#38754;&#20197;&#38450;&#27490;&#36807;&#25311;&#21512;&#65292;&#24182;&#24809;&#32602;&#27169;&#22411;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#21463;&#27426;&#36814;&#30340;&#20998;&#31867;&#31639;&#27861;&#26088;&#22312;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26368;&#22823;&#21270;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#23384;&#22312;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#31574;&#30053;&#21487;&#33021;&#20250;&#22833;&#36133;&#65292;&#22240;&#20026;&#21487;&#20197;&#36890;&#36807;&#23545;&#22823;&#22810;&#25968;&#31867;&#21035;&#36807;&#24230;&#25311;&#21512;&#26469;&#35757;&#32451;&#39640;&#20934;&#30830;&#24615;&#30340;&#27169;&#22411;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22312;&#23384;&#22312;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#24773;&#20917;&#19979;&#65292;&#26354;&#32447;&#19979;&#38754;&#31215;&#65288;AUC&#65289;&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#29992;&#26469;&#27604;&#36739;&#19981;&#21516;&#31639;&#27861;&#30340;&#20998;&#31867;&#24615;&#33021;&#65292;&#24182;&#19988;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#19987;&#27880;&#20110;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#30452;&#25509;&#20248;&#21270;&#35813;&#24230;&#37327;&#30340;&#26041;&#27861;&#12290;&#20854;&#20013;&#65292;&#22522;&#20110;SVM&#30340;&#20844;&#24335;&#29305;&#21035;&#21463;&#27426;&#36814;&#65292;&#22240;&#20026;&#35813;&#20844;&#24335;&#21487;&#20197;&#36731;&#26494;&#22320;&#32467;&#21512;&#19981;&#21516;&#30340;&#27491;&#21017;&#21270;&#31574;&#30053;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#21106;&#24179;&#38754;&#26041;&#27861;&#30340;&#21407;&#22411;&#23398;&#20064;&#26041;&#27861;&#65292;&#31867;&#20284;&#20110;&#25490;&#21517;SVM&#65292;&#20197;&#26368;&#22823;&#21270;AUC&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#36890;&#36807;&#36845;&#20195;&#22320;&#24341;&#20837;&#21106;&#24179;&#38754;&#26469;&#23398;&#20064;&#26356;&#31616;&#21333;&#30340;&#27169;&#22411;&#65292;&#20174;&#32780;&#20197;&#38750;&#24120;&#35268;&#30340;&#26041;&#24335;&#38450;&#27490;&#36807;&#25311;&#21512;&#12290;&#27492;&#22806;&#65292;&#23427;&#24809;&#32602;&#20102;&#27169;&#22411;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
The most popular classification algorithms are designed to maximize classification accuracy during training. However, this strategy may fail in the presence of class imbalance since it is possible to train models with high accuracy by overfitting to the majority class. On the other hand, the Area Under the Curve (AUC) is a widely used metric to compare classification performance of different algorithms when there is a class imbalance, and various approaches focusing on the direct optimization of this metric during training have been proposed. Among them, SVM-based formulations are especially popular as this formulation allows incorporating different regularization strategies easily. In this work, we develop a prototype learning approach that relies on cutting-plane method, similar to Ranking SVM, to maximize AUC. Our algorithm learns simpler models by iteratively introducing cutting planes, thus overfitting is prevented in an unconventional way. Furthermore, it penalizes the changes in
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#27491;&#20132;&#22810;&#39033;&#24335;&#26680;&#21644;LS-SVR&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#24046;&#20998;&#20195;&#25968;&#26041;&#31243;&#31995;&#32479;&#30340;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#27169;&#25311;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.14382</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#27491;&#20132;&#22810;&#39033;&#24335;&#26680;&#30340;&#24046;&#20998;&#20195;&#25968;&#26041;&#31243;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
An Orthogonal Polynomial Kernel-Based Machine Learning Model for Differential-Algebraic Equations. (arXiv:2401.14382v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14382
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#27491;&#20132;&#22810;&#39033;&#24335;&#26680;&#21644;LS-SVR&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#24046;&#20998;&#20195;&#25968;&#26041;&#31243;&#31995;&#32479;&#30340;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#27169;&#25311;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#24341;&#20837;&#30340;&#26368;&#23567;&#20108;&#20056;&#25903;&#25345;&#21521;&#37327;&#22238;&#24402;&#65288;LS-SVR&#65289;&#31639;&#27861;&#29992;&#20110;&#35299;&#20915;&#24494;&#20998;&#21644;&#31215;&#20998;&#26041;&#31243;&#24341;&#36215;&#20102;&#20852;&#36259;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#35813;&#31639;&#27861;&#30340;&#24212;&#29992;&#65292;&#20197;&#35299;&#20915;&#24046;&#20998;&#20195;&#25968;&#26041;&#31243;&#31995;&#32479;&#65288;DAEs&#65289;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#36890;&#36807;&#24314;&#31435;LS-SVR&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12289;&#21152;&#26435;&#27531;&#24046;&#26041;&#27861;&#21644;&#21202;&#35753;&#24503;&#27491;&#20132;&#22810;&#39033;&#24335;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#36816;&#31639;&#31526;&#26684;&#24335;&#35299;&#20915;&#19968;&#33324;DAEs&#30340;&#26032;&#26041;&#27861;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#21508;&#31181;DAE&#22330;&#26223;&#30340;&#27169;&#25311;&#65292;&#22914;&#38750;&#32447;&#24615;&#31995;&#32479;&#12289;&#20998;&#25968;&#38454;&#23548;&#25968;&#12289;&#31215;&#20998;-&#24494;&#20998;&#21644;&#20559;&#24494;&#20998;DAEs&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;&#27604;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#19982;&#30446;&#21069;&#24050;&#24314;&#31435;&#30340;&#29616;&#26377;&#25216;&#26415;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#20854;&#21487;&#38752;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent introduction of the Least-Squares Support Vector Regression (LS-SVR) algorithm for solving differential and integral equations has sparked interest. In this study, we expand the application of this algorithm to address systems of differential-algebraic equations (DAEs). Our work presents a novel approach to solving general DAEs in an operator format by establishing connections between the LS-SVR machine learning model, weighted residual methods, and Legendre orthogonal polynomials. To assess the effectiveness of our proposed method, we conduct simulations involving various DAE scenarios, such as nonlinear systems, fractional-order derivatives, integro-differential, and partial DAEs. Finally, we carry out comparisons between our proposed method and currently established state-of-the-art approaches, demonstrating its reliability and effectiveness.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#20010;&#29992;&#20110;&#20855;&#26377;&#27969;&#24418;&#20540;&#29305;&#24449;&#30340;&#22270;&#30340;&#31070;&#32463;&#32593;&#32476;&#23618;&#12290;&#36825;&#20123;&#23618;&#20855;&#26377;&#23545;&#33410;&#28857;&#25490;&#21015;&#21644;&#29305;&#24449;&#27969;&#24418;&#30340;&#31561;&#21464;&#24615;&#65292;&#24182;&#22312;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#26377;&#30410;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2401.14381</link><description>&lt;p&gt;
&#38754;&#21521;&#27969;&#24418;&#20540;&#22270;&#30340;&#25193;&#25955;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65306;&#22810;&#37325;&#38590;&#39064;&#22270;&#31070;&#32463;&#32593;&#32476;&#23618;
&lt;/p&gt;
&lt;p&gt;
Manifold GCN: Diffusion-based Convolutional Neural Network for Manifold-valued Graphs. (arXiv:2401.14381v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14381
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#20010;&#29992;&#20110;&#20855;&#26377;&#27969;&#24418;&#20540;&#29305;&#24449;&#30340;&#22270;&#30340;&#31070;&#32463;&#32593;&#32476;&#23618;&#12290;&#36825;&#20123;&#23618;&#20855;&#26377;&#23545;&#33410;&#28857;&#25490;&#21015;&#21644;&#29305;&#24449;&#27969;&#24418;&#30340;&#31561;&#21464;&#24615;&#65292;&#24182;&#22312;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#26377;&#30410;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#29992;&#20110;&#20855;&#26377;Riemannian&#27969;&#24418;&#29305;&#24449;&#30340;&#22270;&#19978;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#23618;&#12290;&#31532;&#19968;&#65292;&#22522;&#20110;&#27969;&#24418;&#20540;&#22270;&#30340;&#25193;&#25955;&#26041;&#31243;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#25193;&#25955;&#23618;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#24847;&#25968;&#37327;&#30340;&#33410;&#28857;&#21644;&#22270;&#36830;&#25509;&#27169;&#24335;&#12290;&#31532;&#20108;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#21521;&#37327;&#31070;&#32463;&#20803;&#26694;&#26550;&#30340;&#24605;&#24819;&#36716;&#21270;&#21040;&#25105;&#20204;&#30340;&#19968;&#33324;&#35774;&#32622;&#20013;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#20999;&#32447;&#22810;&#23618;&#24863;&#30693;&#22120;&#12290;&#36825;&#20004;&#20010;&#23618;&#23545;&#33410;&#28857;&#25490;&#21015;&#21644;&#29305;&#24449;&#27969;&#24418;&#30340;&#31561;&#21464;&#20855;&#26377;&#21709;&#24212;&#65292;&#36825;&#20123;&#29305;&#24615;&#22312;&#35768;&#22810;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#20013;&#24050;&#34987;&#35777;&#26126;&#20855;&#26377;&#26377;&#30410;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#20197;&#21450;&#22312;&#21491;&#20391;&#28023;&#39532;&#19977;&#35282;&#32593;&#26684;&#19978;&#20998;&#31867;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30340;&#25968;&#20540;&#23454;&#20363;&#34920;&#26126;&#25105;&#20204;&#24314;&#31435;&#30340;&#23618;&#20855;&#26377;&#38750;&#24120;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose two graph neural network layers for graphs with features in a Riemannian manifold. First, based on a manifold-valued graph diffusion equation, we construct a diffusion layer that can be applied to an arbitrary number of nodes and graph connectivity patterns. Second, we model a tangent multilayer perceptron by transferring ideas from the vector neuron framework to our general setting. Both layers are equivariant with respect to node permutations and isometries of the feature manifold. These properties have been shown to lead to a beneficial inductive bias in many deep learning tasks. Numerical examples on synthetic data as well as on triangle meshes of the right hippocampus to classify Alzheimer's disease demonstrate the very good performance of our layers.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#20840;&#26223;&#20998;&#21106;&#21644;&#25193;&#25955;&#27169;&#22411;&#37325;&#26500;&#22478;&#24066;&#26223;&#35266;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#25972;&#21512;&#20808;&#36827;&#30340;&#22270;&#20687;&#20998;&#21106;&#21644;&#25193;&#25955;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#22478;&#24066;&#35774;&#35745;&#30340;&#20840;&#38754;&#22788;&#29702;&#65292;&#24182;&#22312;&#29289;&#20307;&#26816;&#27979;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.14379</link><description>&lt;p&gt;
UrbanGenAI: &#20351;&#29992;&#20840;&#26223;&#20998;&#21106;&#21644;&#25193;&#25955;&#27169;&#22411;&#37325;&#26500;&#22478;&#24066;&#26223;&#35266;
&lt;/p&gt;
&lt;p&gt;
UrbanGenAI: Reconstructing Urban Landscapes using Panoptic Segmentation and Diffusion Models. (arXiv:2401.14379v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14379
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#20840;&#26223;&#20998;&#21106;&#21644;&#25193;&#25955;&#27169;&#22411;&#37325;&#26500;&#22478;&#24066;&#26223;&#35266;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#25972;&#21512;&#20808;&#36827;&#30340;&#22270;&#20687;&#20998;&#21106;&#21644;&#25193;&#25955;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#22478;&#24066;&#35774;&#35745;&#30340;&#20840;&#38754;&#22788;&#29702;&#65292;&#24182;&#22312;&#29289;&#20307;&#26816;&#27979;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20195;&#35774;&#35745;&#23454;&#36341;&#20013;&#65292;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;(genAI)&#30340;&#25972;&#21512;&#20195;&#34920;&#20102;&#19968;&#31181;&#36716;&#21464;&#65292;&#26356;&#21152;&#20132;&#20114;&#21644;&#21253;&#23481;&#30340;&#36807;&#31243;&#12290;&#36825;&#20123;&#25216;&#26415;&#25552;&#20379;&#20102;&#22270;&#20687;&#20998;&#26512;&#21644;&#29983;&#25104;&#30340;&#26032;&#32500;&#24230;&#65292;&#22312;&#22478;&#24066;&#26223;&#35266;&#37325;&#24314;&#30340;&#32972;&#26223;&#19979;&#23588;&#20854;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24037;&#20316;&#27969;&#31243;&#65292;&#20197;&#21407;&#22411;&#24212;&#29992;&#31243;&#24207;&#30340;&#24418;&#24335;&#23637;&#31034;&#65292;&#26088;&#22312;&#21033;&#29992;&#20808;&#36827;&#30340;&#22270;&#20687;&#20998;&#21106;&#21644;&#25193;&#25955;&#27169;&#22411;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#23454;&#29616;&#23545;&#22478;&#24066;&#35774;&#35745;&#30340;&#20840;&#38754;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#35814;&#32454;&#30340;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;OneFormer&#21644;&#36890;&#36807;ControlNet&#23454;&#29616;&#30340;&#31283;&#23450;&#25193;&#25955;XL&#65288;SDXL&#65289;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#20174;&#25991;&#26412;&#25551;&#36848;&#20013;&#29983;&#25104;&#22270;&#20687;&#12290;&#39564;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#21407;&#22411;&#24212;&#29992;&#31243;&#24207;&#20855;&#26377;&#24456;&#39640;&#30340;&#24615;&#33021;&#65292;&#23637;&#31034;&#20102;&#22312;&#29289;&#20307;&#26816;&#27979;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#30340;&#26174;&#33879;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In contemporary design practices, the integration of computer vision and generative artificial intelligence (genAI) represents a transformative shift towards more interactive and inclusive processes. These technologies offer new dimensions of image analysis and generation, which are particularly relevant in the context of urban landscape reconstruction. This paper presents a novel workflow encapsulated within a prototype application, designed to leverage the synergies between advanced image segmentation and diffusion models for a comprehensive approach to urban design. Our methodology encompasses the OneFormer model for detailed image segmentation and the Stable Diffusion XL (SDXL) diffusion model, implemented through ControlNet, for generating images from textual descriptions. Validation results indicated a high degree of performance by the prototype application, showcasing significant accuracy in both object detection and text-to-image generation. This was evidenced by superior Inter
&lt;/p&gt;</description></item><item><title>TURNA&#26159;&#19968;&#31181;&#29992;&#20110;&#22303;&#32819;&#20854;&#35821;&#30340;&#20302;&#36164;&#28304;&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#22791;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#33021;&#21147;&#12290;TURNA&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;&#32534;&#30721;-&#35299;&#30721;&#26550;&#26500;&#22312;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#33021;&#19982;&#22303;&#32819;&#20854;&#35821;&#21333;&#35821;&#27169;&#22411;&#31454;&#20105;&#12290;</title><link>http://arxiv.org/abs/2401.14373</link><description>&lt;p&gt;
TURNA: &#19968;&#31181;&#29992;&#20110;&#22686;&#24378;&#29702;&#35299;&#21644;&#29983;&#25104;&#30340;&#22303;&#32819;&#20854;&#32534;&#30721;-&#35299;&#30721;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
TURNA: A Turkish Encoder-Decoder Language Model for Enhanced Understanding and Generation. (arXiv:2401.14373v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14373
&lt;/p&gt;
&lt;p&gt;
TURNA&#26159;&#19968;&#31181;&#29992;&#20110;&#22303;&#32819;&#20854;&#35821;&#30340;&#20302;&#36164;&#28304;&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#22791;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#33021;&#21147;&#12290;TURNA&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;&#32534;&#30721;-&#35299;&#30721;&#26550;&#26500;&#22312;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#33021;&#19982;&#22303;&#32819;&#20854;&#35821;&#21333;&#35821;&#27169;&#22411;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26368;&#26032;&#36827;&#23637;&#20027;&#35201;&#20559;&#21521;&#20110;&#36164;&#28304;&#20016;&#23500;&#19988;&#20197;&#33521;&#35821;&#20026;&#20013;&#24515;&#30340;&#27169;&#22411;&#65292;&#36825;&#23548;&#33268;&#20102;&#19982;&#36164;&#28304;&#31232;&#32570;&#30340;&#35821;&#35328;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#36317;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;TURNA&#35821;&#35328;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#38024;&#23545;&#36164;&#28304;&#31232;&#32570;&#30340;&#22303;&#32819;&#20854;&#35821;&#24320;&#21457;&#65292;&#33021;&#22815;&#36827;&#34892;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#12290;TURNA&#20351;&#29992;&#22522;&#20110;&#32479;&#19968;&#26694;&#26550;UL2&#30340;&#32534;&#30721;-&#35299;&#30721;&#26550;&#26500;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#19988;&#25105;&#20204;&#19987;&#38376;&#20026;&#27492;&#30446;&#30340;&#31579;&#36873;&#20102;&#19968;&#20010;&#22810;&#26679;&#30340;&#35821;&#26009;&#24211;&#12290;&#25105;&#20204;&#23545;TURNA&#22312;&#22303;&#32819;&#20854;&#35821;&#30340;&#19977;&#20010;&#29983;&#25104;&#20219;&#21153;&#21644;&#20116;&#20010;&#29702;&#35299;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;TURNA&#22312;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#19978;&#20248;&#20110;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#19982;&#22303;&#32819;&#20854;&#35821;&#21333;&#35821;&#27169;&#22411;&#22312;&#29702;&#35299;&#20219;&#21153;&#19978;&#31454;&#20105;&#12290;TURNA&#24050;&#22312;https://huggingface.co/boun-tabi-LMG/TURNA &#19978;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent advances in natural language processing have predominantly favored well-resourced English-centric models, resulting in a significant gap with low-resource languages. In this work, we introduce the language model TURNA, which is developed for the low-resource language Turkish and is capable of both natural language understanding and generation tasks. TURNA is pretrained with an encoder-decoder architecture based on the unified framework UL2 with a diverse corpus that we specifically curated for this purpose. We evaluated TURNA with three generation tasks and five understanding tasks for Turkish. The results show that TURNA outperforms several multilingual models in both understanding and generation tasks, and competes with monolingual Turkish models in understanding tasks. TURNA is made available at https://huggingface.co/boun-tabi-LMG/TURNA .
&lt;/p&gt;</description></item><item><title>Genie&#26159;&#19968;&#20010;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#39640;&#36136;&#37327;&#20869;&#23481;&#23548;&#21521;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19977;&#20010;&#38454;&#27573;&#23454;&#29616;&#65306;&#20869;&#23481;&#20934;&#22791;&#12289;&#29983;&#25104;&#21644;&#36807;&#28388;&#12290;&#22312;&#20154;&#31867;&#35780;&#20272;&#20013;&#65292;&#29983;&#25104;&#30340;&#25968;&#25454;&#34987;&#21457;&#29616;&#26159;&#33258;&#28982;&#19988;&#39640;&#36136;&#37327;&#30340;&#65292;&#24182;&#19988;&#36890;&#36807;&#19982;&#20351;&#29992;&#20154;&#24037;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#27604;&#36739;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#34920;&#29616;&#30456;&#24403;&#25110;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2401.14367</link><description>&lt;p&gt;
Genie&#65306;&#23454;&#29616;&#20869;&#23481;&#23548;&#21521;&#25968;&#25454;&#38598;&#29983;&#25104;&#30340;&#20154;&#31867;&#27700;&#24179;
&lt;/p&gt;
&lt;p&gt;
Genie: Achieving Human Parity in Content-Grounded Datasets Generation. (arXiv:2401.14367v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14367
&lt;/p&gt;
&lt;p&gt;
Genie&#26159;&#19968;&#20010;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#39640;&#36136;&#37327;&#20869;&#23481;&#23548;&#21521;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19977;&#20010;&#38454;&#27573;&#23454;&#29616;&#65306;&#20869;&#23481;&#20934;&#22791;&#12289;&#29983;&#25104;&#21644;&#36807;&#28388;&#12290;&#22312;&#20154;&#31867;&#35780;&#20272;&#20013;&#65292;&#29983;&#25104;&#30340;&#25968;&#25454;&#34987;&#21457;&#29616;&#26159;&#33258;&#28982;&#19988;&#39640;&#36136;&#37327;&#30340;&#65292;&#24182;&#19988;&#36890;&#36807;&#19982;&#20351;&#29992;&#20154;&#24037;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#27604;&#36739;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#34920;&#29616;&#30456;&#24403;&#25110;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20869;&#23481;&#23548;&#21521;&#29983;&#25104;&#20219;&#21153;&#65292;&#32570;&#20047;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#34987;&#35748;&#20026;&#26159;&#25512;&#21160;&#36825;&#20123;&#20219;&#21153;&#21457;&#23637;&#30340;&#20027;&#35201;&#38556;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Genie&#65292;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#39640;&#36136;&#37327;&#20869;&#23481;&#23548;&#21521;&#25968;&#25454;&#30340;&#26032;&#26041;&#27861;&#12290;&#23427;&#21253;&#25324;&#19977;&#20010;&#38454;&#27573;&#65306;&#65288;a&#65289;&#20869;&#23481;&#20934;&#22791;&#65292;&#65288;b&#65289;&#29983;&#25104;&#65306;&#20174;&#20869;&#23481;&#20013;&#21019;&#24314;&#29305;&#23450;&#20219;&#21153;&#30340;&#31034;&#20363;&#65288;&#20363;&#22914;&#38382;&#39064;-&#31572;&#26696;&#23545;&#25110;&#25688;&#35201;&#65289;&#65292;&#65288;c&#65289;&#36807;&#28388;&#26426;&#21046;&#65292;&#26088;&#22312;&#30830;&#20445;&#29983;&#25104;&#25968;&#25454;&#30340;&#36136;&#37327;&#21644;&#21487;&#20449;&#24230;&#12290;&#25105;&#20204;&#36890;&#36807;&#29983;&#25104;&#19977;&#20010;&#22823;&#35268;&#27169;&#30340;&#21512;&#25104;&#25968;&#25454;&#26469;&#23637;&#31034;&#36825;&#31181;&#26041;&#27861;&#65306;&#38271;&#22411;&#38382;&#39064;&#22238;&#31572;&#65288;LFQA&#65289;&#12289;&#25688;&#35201;&#21644;&#20449;&#24687;&#25552;&#21462;&#12290;&#22312;&#20154;&#31867;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#29983;&#25104;&#30340;&#25968;&#25454;&#34987;&#21457;&#29616;&#26159;&#33258;&#28982;&#19988;&#39640;&#36136;&#37327;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#20351;&#29992;&#25105;&#20204;&#30340;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#19982;&#20351;&#29992;&#20154;&#24037;&#32534;&#20889;&#30340;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739; - &#23545;&#20110;LFQA&#65292;&#25105;&#20204;&#19982;ELI5&#21644;ASQA&#36827;&#34892;&#27604;&#36739;&#65292;&#23545;&#20110;&#25688;&#35201;&#65292;&#25105;&#20204;&#19982;CNN-DailyMail&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#19982;&#25110;&#36229;&#36807;&#20351;&#29992;&#20154;&#24037;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The lack of high-quality data for content-grounded generation tasks has been identified as a major obstacle to advancing these tasks. To address this gap, we propose Genie, a novel method for automatically generating high-quality content-grounded data. It consists of three stages: (a) Content Preparation, (b) Generation: creating task-specific examples from the content (e.g., question-answer pairs or summaries). (c) Filtering mechanism aiming to ensure the quality and faithfulness of the generated data. We showcase this methodology by generating three large-scale synthetic data, making wishes, for Long-Form Question-Answering (LFQA), summarization, and information extraction. In a human evaluation, our generated data was found to be natural and of high quality. Furthermore, we compare models trained on our data with models trained on human-written data -- ELI5 and ASQA for LFQA and CNN-DailyMail for Summarization. We show that our models are on par with or outperforming models trained 
&lt;/p&gt;</description></item><item><title>MoE-Infinity&#26159;&#19968;&#31181;&#25104;&#26412;&#39640;&#25928;&#30340;MoE&#26381;&#21153;&#31995;&#32479;&#65292;&#36890;&#36807;&#28608;&#27963;&#24863;&#30693;&#30340;&#19987;&#23478;&#21368;&#36733;&#21644;&#32531;&#23384;&#25216;&#26415;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#24310;&#36831;&#65292;&#24182;&#25552;&#39640;&#20102;&#25104;&#26412;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.14361</link><description>&lt;p&gt;
MoE-Infinity&#65306;&#29992;&#20110;&#39640;&#25928;MoE&#26381;&#21153;&#30340;&#28608;&#27963;&#24863;&#30693;&#19987;&#23478;&#21368;&#36733;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
MoE-Infinity: Activation-Aware Expert Offloading for Efficient MoE Serving. (arXiv:2401.14361v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14361
&lt;/p&gt;
&lt;p&gt;
MoE-Infinity&#26159;&#19968;&#31181;&#25104;&#26412;&#39640;&#25928;&#30340;MoE&#26381;&#21153;&#31995;&#32479;&#65292;&#36890;&#36807;&#28608;&#27963;&#24863;&#30693;&#30340;&#19987;&#23478;&#21368;&#36733;&#21644;&#32531;&#23384;&#25216;&#26415;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#24310;&#36831;&#65292;&#24182;&#25552;&#39640;&#20102;&#25104;&#26412;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;MoE-Infinity&#65292;&#19968;&#31181;&#25104;&#26412;&#39640;&#25928;&#30340;&#19987;&#23478;&#28151;&#21512;(MoE)&#26381;&#21153;&#31995;&#32479;&#65292;&#23454;&#29616;&#20102;&#28608;&#27963;&#24863;&#30693;&#30340;&#19987;&#23478;&#21368;&#36733;&#12290;MoE-Infinity&#20855;&#26377;&#24207;&#21015;&#32423;&#19987;&#23478;&#28608;&#27963;&#36861;&#36394;&#30340;&#29305;&#28857;&#65292;&#36825;&#26159;&#19968;&#31181;&#25797;&#38271;&#35782;&#21035;&#31232;&#30095;&#28608;&#27963;&#24182;&#25429;&#25417;MoE&#25512;&#29702;&#30340;&#26102;&#38388;&#23616;&#37096;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#20998;&#26512;&#36825;&#20123;&#36861;&#36394;&#65292;MoE-Infinity&#25191;&#34892;&#20102;&#26032;&#39062;&#30340;&#28608;&#27963;&#24863;&#30693;&#19987;&#23478;&#39044;&#21462;&#21644;&#32531;&#23384;&#65292;&#22823;&#22823;&#38477;&#20302;&#20102;&#36890;&#24120;&#19982;&#21368;&#36733;&#19987;&#23478;&#30456;&#20851;&#30340;&#24310;&#36831;&#24320;&#38144;&#65292;&#25552;&#39640;&#20102;&#25104;&#26412;&#24615;&#33021;&#12290;&#22312;&#19968;&#20010;&#38598;&#32676;&#20013;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;MoE-Infinity&#20248;&#20110;&#35768;&#22810;&#29616;&#26377;&#30340;&#31995;&#32479;&#21644;&#26041;&#27861;&#65292;&#23545;&#20110;&#21508;&#31181;MoEs&#65292;&#23558;&#24310;&#36831;&#38477;&#20302;&#20102;420&#20493;&#65292;&#23558;&#37096;&#32626;&#25104;&#26412;&#38477;&#20302;&#20102;8&#20493;&#20197;&#19978;&#12290;MoE-Infinity&#30340;&#28304;&#20195;&#30721;&#21487;&#22312;https://github.com/TorchMoE/MoE-Infinity&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents MoE-Infinity, a cost-efficient mixture-of-expert (MoE) serving system that realizes activation-aware expert offloading. MoE-Infinity features sequence-level expert activation tracing, a new approach adept at identifying sparse activations and capturing the temporal locality of MoE inference. By analyzing these traces, MoE-Infinity performs novel activation-aware expert prefetching and caching, substantially reducing the latency overheads usually associated with offloading experts for improved cost performance. Extensive experiments in a cluster show that MoE-Infinity outperforms numerous existing systems and approaches, reducing latency by 4 20X and decreasing deployment costs by over 8X for various MoEs. MoE-Infinity's source code is publicly available at https://github.com/TorchMoE/MoE-Infinity
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#31216;&#20026;CAP&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#31867;&#21035;&#29305;&#23450;&#30340;&#23398;&#20064;&#31574;&#30053;&#26469;&#26356;&#22909;&#22320;&#36866;&#24212;&#24322;&#36136;&#24615;&#25968;&#25454;&#65292;&#24182;&#22312;&#25439;&#22833;&#20989;&#25968;&#35774;&#35745;&#21644;&#26631;&#31614;&#19981;&#24179;&#34913;&#38382;&#39064;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2401.14343</link><description>&lt;p&gt;
&#31867;&#23646;&#24615;&#20808;&#39564;&#65306;&#23558;&#20248;&#21270;&#26041;&#27861;&#24212;&#29992;&#20110;&#24322;&#36136;&#24615;&#21644;&#20844;&#24179;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;
Class-attribute Priors: Adapting Optimization to Heterogeneity and Fairness Objective. (arXiv:2401.14343v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14343
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#31216;&#20026;CAP&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#31867;&#21035;&#29305;&#23450;&#30340;&#23398;&#20064;&#31574;&#30053;&#26469;&#26356;&#22909;&#22320;&#36866;&#24212;&#24322;&#36136;&#24615;&#25968;&#25454;&#65292;&#24182;&#22312;&#25439;&#22833;&#20989;&#25968;&#35774;&#35745;&#21644;&#26631;&#31614;&#19981;&#24179;&#34913;&#38382;&#39064;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#20998;&#31867;&#38382;&#39064;&#22312;&#21508;&#20010;&#31867;&#21035;&#20043;&#38388;&#23384;&#22312;&#24322;&#36136;&#24615;&#65306;&#27599;&#20010;&#31867;&#21035;&#21487;&#33021;&#20855;&#26377;&#29420;&#29305;&#30340;&#23646;&#24615;&#65292;&#20363;&#22914;&#26679;&#26412;&#22823;&#23567;&#65292;&#26631;&#31614;&#36136;&#37327;&#25110;&#21487;&#39044;&#27979;&#24615;&#65288;&#26131; vs &#38590;&#65289;&#65292;&#20197;&#21450;&#22312;&#27979;&#35797;&#26102;&#30340;&#21464;&#37327;&#37325;&#35201;&#24615;&#12290;&#22914;&#26524;&#19981;&#27880;&#24847;&#22788;&#29702;&#65292;&#36825;&#20123;&#24322;&#36136;&#24615;&#20250;&#38459;&#30861;&#23398;&#20064;&#36807;&#31243;&#65292;&#23588;&#20854;&#26159;&#22312;&#20248;&#21270;&#20844;&#24179;&#30446;&#26631;&#26102;&#12290;&#22312;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#35774;&#23450;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20026;&#20102;&#36798;&#21040;&#24179;&#34913;&#20934;&#30830;&#24230;&#65292;&#26368;&#20248;&#30340;&#25903;&#25345;&#21521;&#37327;&#26426;&#20998;&#31867;&#22120;&#38656;&#35201;&#36866;&#24212;&#31867;&#21035;&#23646;&#24615;&#12290;&#36825;&#28608;&#21457;&#20102;&#25105;&#20204;&#25552;&#20986;&#20102;CAP&#65306;&#19968;&#31181;&#22522;&#20110;&#31867;&#21035;&#23646;&#24615;&#29983;&#25104;&#31867;&#21035;&#29305;&#23450;&#23398;&#20064;&#31574;&#30053;&#65288;&#20363;&#22914;&#36229;&#21442;&#25968;&#65289;&#30340;&#26377;&#25928;&#21644;&#36890;&#29992;&#26041;&#27861;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#20248;&#21270;&#36807;&#31243;&#26356;&#22909;&#22320;&#36866;&#24212;&#24322;&#36136;&#24615;&#12290;CAP&#30456;&#27604;&#20110;&#23558;&#19981;&#21516;&#30340;&#36229;&#21442;&#25968;&#20998;&#37197;&#32473;&#27599;&#20010;&#31867;&#21035;&#30340;&#26420;&#32032;&#26041;&#27861;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;&#25105;&#20204;&#23558;CAP&#23454;&#20363;&#21270;&#20026;&#25439;&#22833;&#20989;&#25968;&#35774;&#35745;&#21644;&#20107;&#21518;&#23545;&#25968;&#35843;&#25972;&#65292;&#37325;&#28857;&#20851;&#27880;&#26631;&#31614;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern classification problems exhibit heterogeneities across individual classes: Each class may have unique attributes, such as sample size, label quality, or predictability (easy vs difficult), and variable importance at test-time. Without care, these heterogeneities impede the learning process, most notably, when optimizing fairness objectives. Confirming this, under a gaussian mixture setting, we show that the optimal SVM classifier for balanced accuracy needs to be adaptive to the class attributes. This motivates us to propose CAP: An effective and general method that generates a class-specific learning strategy (e.g. hyperparameter) based on the attributes of that class. This way, optimization process better adapts to heterogeneities. CAP leads to substantial improvements over the naive approach of assigning separate hyperparameters to each class. We instantiate CAP for loss function design and post-hoc logit adjustment, with emphasis on label-imbalanced problems. We show that CA
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24471;&#20998;&#32467;&#26500;&#20808;&#39564;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#37096;&#20998;&#24050;&#30693;&#39640;&#26031;&#22270;&#27169;&#22411;&#12290;&#36890;&#36807;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#20272;&#35745;&#22270;&#30340;&#24471;&#20998;&#20989;&#25968;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;&#29983;&#25104;&#26679;&#26412;&#26102;&#21033;&#29992;&#36864;&#28779;&#26391;&#26684;&#32500;&#33021;&#25193;&#25955;&#65292;&#20174;&#32780;&#26356;&#20934;&#30830;&#22320;&#20272;&#35745;&#21518;&#39564;&#20998;&#24067;&#12290;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#26126;&#26174;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2401.14340</link><description>&lt;p&gt;
&#22522;&#20110;&#24471;&#20998;&#32467;&#26500;&#20808;&#39564;&#30340;&#37096;&#20998;&#24050;&#30693;&#39640;&#26031;&#22270;&#27169;&#22411;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Estimation of partially known Gaussian graphical models with score-based structural priors. (arXiv:2401.14340v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14340
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24471;&#20998;&#32467;&#26500;&#20808;&#39564;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#37096;&#20998;&#24050;&#30693;&#39640;&#26031;&#22270;&#27169;&#22411;&#12290;&#36890;&#36807;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#20272;&#35745;&#22270;&#30340;&#24471;&#20998;&#20989;&#25968;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;&#29983;&#25104;&#26679;&#26412;&#26102;&#21033;&#29992;&#36864;&#28779;&#26391;&#26684;&#32500;&#33021;&#25193;&#25955;&#65292;&#20174;&#32780;&#26356;&#20934;&#30830;&#22320;&#20272;&#35745;&#21518;&#39564;&#20998;&#24067;&#12290;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#26126;&#26174;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#25903;&#25345;&#20272;&#35745;&#37096;&#20998;&#24050;&#30693;&#30340;&#39640;&#26031;&#22270;&#27169;&#22411;&#65292;&#24182;&#19988;&#32467;&#21512;&#20102;&#20851;&#20110;&#24213;&#23618;&#22270;&#30340;&#20808;&#39564;&#20449;&#24687;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#20256;&#32479;&#26041;&#27861;&#20351;&#29992;&#28857;&#20272;&#35745;&#26041;&#27861;&#22522;&#20110;&#26368;&#22823;&#20284;&#28982;&#25110;&#26368;&#22823;&#21518;&#39564;&#20934;&#21017;&#65292;&#24182;&#20351;&#29992;&#65288;&#31616;&#21333;&#30340;&#65289;&#31934;&#24230;&#30697;&#38453;&#20808;&#39564;&#26469;&#25552;&#20379;&#28857;&#20272;&#35745;&#12290;&#25105;&#20204;&#32771;&#34385;&#23545;&#22270;&#36827;&#34892;&#20808;&#39564;&#65292;&#24182;&#20381;&#36182;&#36864;&#28779;&#26391;&#26684;&#32500;&#33021;&#25193;&#25955;&#20174;&#21518;&#39564;&#20998;&#24067;&#20013;&#29983;&#25104;&#26679;&#26412;&#12290;&#30001;&#20110;&#26391;&#26684;&#32500;&#33021;&#37319;&#26679;&#22120;&#38656;&#35201;&#35775;&#38382;&#24213;&#23618;&#22270;&#20808;&#39564;&#30340;&#24471;&#20998;&#20989;&#25968;&#65292;&#22240;&#27492;&#25105;&#20204;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#26377;&#25928;&#22320;&#20174;&#22270;&#25968;&#25454;&#38598;&#65288;&#20107;&#20808;&#21487;&#29992;&#25110;&#20174;&#24050;&#30693;&#20998;&#24067;&#29983;&#25104;&#65289;&#20272;&#35745;&#24471;&#20998;&#12290;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel algorithm for the support estimation of partially known Gaussian graphical models that incorporates prior information about the underlying graph. In contrast to classical approaches that provide a point estimate based on a maximum likelihood or a maximum a posteriori criterion using (simple) priors on the precision matrix, we consider a prior on the graph and rely on annealed Langevin diffusion to generate samples from the posterior distribution. Since the Langevin sampler requires access to the score function of the underlying graph prior, we use graph neural networks to effectively estimate the score from a graph dataset (either available beforehand or generated from a known distribution). Numerical experiments demonstrate the benefits of our approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#23478;&#24237;&#36335;&#30001;&#22120;&#19978;&#20351;&#29992;AI&#24037;&#20855;&#21644;&#32463;&#20856;&#27969;&#37327;&#36807;&#28388;&#31639;&#27861;&#36827;&#34892;&#26412;&#22320;&#26377;&#25928;&#30340;&#29289;&#32852;&#32593;&#23041;&#32961;&#26816;&#27979;&#65292;&#20854;&#32467;&#26524;&#34920;&#26126;&#35013;&#22791;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#20856;&#22411;&#23478;&#24237;&#36335;&#30001;&#22120;&#33021;&#22815;&#22312;&#20445;&#25252;&#29289;&#32852;&#32593;&#32593;&#32476;&#26041;&#38754;&#19982;&#29616;&#26377;&#30340;&#27969;&#34892;&#35299;&#20915;&#26041;&#26696;&#30456;&#24403;&#25110;&#20248;&#20110;&#20854;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.14332</link><description>&lt;p&gt;
SunBlock&#65306;&#26080;&#20113;&#20445;&#25252;&#29289;&#32852;&#32593;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
SunBlock: Cloudless Protection for IoT Systems. (arXiv:2401.14332v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14332
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#23478;&#24237;&#36335;&#30001;&#22120;&#19978;&#20351;&#29992;AI&#24037;&#20855;&#21644;&#32463;&#20856;&#27969;&#37327;&#36807;&#28388;&#31639;&#27861;&#36827;&#34892;&#26412;&#22320;&#26377;&#25928;&#30340;&#29289;&#32852;&#32593;&#23041;&#32961;&#26816;&#27979;&#65292;&#20854;&#32467;&#26524;&#34920;&#26126;&#35013;&#22791;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#20856;&#22411;&#23478;&#24237;&#36335;&#30001;&#22120;&#33021;&#22815;&#22312;&#20445;&#25252;&#29289;&#32852;&#32593;&#32593;&#32476;&#26041;&#38754;&#19982;&#29616;&#26377;&#30340;&#27969;&#34892;&#35299;&#20915;&#26041;&#26696;&#30456;&#24403;&#25110;&#20248;&#20110;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36234;&#26469;&#36234;&#22810;&#30340;&#29289;&#32852;&#32593;&#35774;&#22791;&#36827;&#20837;&#23478;&#24237;&#65292;&#28508;&#22312;&#30340;&#20449;&#24687;&#27844;&#28431;&#28192;&#36947;&#20197;&#21450;&#30456;&#20851;&#30340;&#23433;&#20840;&#23041;&#32961;&#21644;&#38544;&#31169;&#39118;&#38505;&#20063;&#22312;&#22686;&#21152;&#12290;&#23613;&#31649;&#22312;&#26080;&#20445;&#25252;&#30340;&#23478;&#24237;&#32593;&#32476;&#20013;&#23384;&#22312;&#23545;&#29289;&#32852;&#32593;&#35774;&#22791;&#30340;&#38271;&#26399;&#25915;&#20987;&#21382;&#21490;&#65292;&#20294;&#20934;&#30830;&#12289;&#24555;&#36895;&#22320;&#26816;&#27979;&#21644;&#39044;&#38450;&#27492;&#31867;&#25915;&#20987;&#30340;&#38382;&#39064;&#20173;&#28982;&#23384;&#22312;&#12290;&#35768;&#22810;&#29616;&#26377;&#30340;&#29289;&#32852;&#32593;&#20445;&#25252;&#35299;&#20915;&#26041;&#26696;&#26159;&#22522;&#20110;&#20113;&#30340;&#65292;&#26377;&#26102;&#25928;&#26524;&#19981;&#20339;&#65292;&#24182;&#19988;&#21487;&#33021;&#19982;&#26410;&#30693;&#31532;&#19977;&#26041;&#20849;&#20139;&#28040;&#36153;&#32773;&#25968;&#25454;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#23478;&#24237;&#36335;&#30001;&#22120;&#19978;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#32467;&#21512;&#32463;&#20856;&#22522;&#20110;&#35268;&#21017;&#30340;&#27969;&#37327;&#36807;&#28388;&#31639;&#27861;&#26469;&#26377;&#25928;&#26816;&#27979;&#29289;&#32852;&#32593;&#23041;&#32961;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#21644;&#27969;&#37327;&#36807;&#28388;&#36923;&#36753;&#23548;&#33268;&#36335;&#30001;&#22120;&#30828;&#20214;&#36164;&#28304;&#30053;&#24494;&#22686;&#21152;&#65292;&#35013;&#22791;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#20856;&#22411;&#23478;&#24237;&#36335;&#30001;&#22120;&#33021;&#22815;&#26377;&#25928;&#26816;&#27979;&#39118;&#38505;&#24182;&#20445;&#25252;&#20856;&#22411;&#30340;&#23478;&#24237;&#29289;&#32852;&#32593;&#32593;&#32476;&#65292;&#19982;&#29616;&#26377;&#30340;&#27969;&#34892;&#35299;&#20915;&#26041;&#26696;&#30456;&#24403;&#25110;&#20248;&#20110;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
With an increasing number of Internet of Things (IoT) devices present in homes, there is a rise in the number of potential information leakage channels and their associated security threats and privacy risks. Despite a long history of attacks on IoT devices in unprotected home networks, the problem of accurate, rapid detection and prevention of such attacks remains open. Many existing IoT protection solutions are cloud-based, sometimes ineffective, and might share consumer data with unknown third parties. This paper investigates the potential for effective IoT threat detection locally, on a home router, using AI tools combined with classic rule-based traffic-filtering algorithms. Our results show that with a slight rise of router hardware resources caused by machine learning and traffic filtering logic, a typical home router instrumented with our solution is able to effectively detect risks and protect a typical home IoT network, equaling or outperforming existing popular solutions, wi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;Spotify&#29992;&#25143;&#23646;&#24615;&#19982;&#20182;&#20204;&#20844;&#24320;&#25773;&#25918;&#21015;&#34920;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#29305;&#21035;&#20851;&#27880;&#35782;&#21035;&#19982;&#29992;&#25143;&#20010;&#20154;&#23646;&#24615;&#30456;&#20851;&#30340;&#38899;&#20048;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2401.14296</link><description>&lt;p&gt;
"All of Me": &#20174;&#20844;&#24320;&#30340;Spotify&#25773;&#25918;&#21015;&#34920;&#20013;&#25366;&#25496;&#29992;&#25143;&#23646;&#24615;
&lt;/p&gt;
&lt;p&gt;
"All of Me": Mining Users' Attributes from their Public Spotify Playlists. (arXiv:2401.14296v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14296
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;Spotify&#29992;&#25143;&#23646;&#24615;&#19982;&#20182;&#20204;&#20844;&#24320;&#25773;&#25918;&#21015;&#34920;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#29305;&#21035;&#20851;&#27880;&#35782;&#21035;&#19982;&#29992;&#25143;&#20010;&#20154;&#23646;&#24615;&#30456;&#20851;&#30340;&#38899;&#20048;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#23383;&#38899;&#20048;&#27969;&#23186;&#20307;&#26102;&#20195;&#65292;&#20687;Spotify&#36825;&#26679;&#30340;&#24179;&#21488;&#19978;&#30340;&#25773;&#25918;&#21015;&#34920;&#24050;&#32463;&#25104;&#20026;&#20010;&#20154;&#38899;&#20048;&#20307;&#39564;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#20154;&#20204;&#21019;&#24314;&#24182;&#20844;&#24320;&#20998;&#20139;&#33258;&#24049;&#30340;&#25773;&#25918;&#21015;&#34920;&#65292;&#20197;&#34920;&#36798;&#20182;&#20204;&#30340;&#38899;&#20048;&#21697;&#21619;&#65292;&#25512;&#24191;&#20182;&#20204;&#26368;&#21916;&#29233;&#30340;&#33402;&#26415;&#23478;&#30340;&#21457;&#29616;&#65292;&#24182;&#20419;&#36827;&#31038;&#20132;&#32852;&#31995;&#12290;&#36825;&#20123;&#21487;&#20197;&#20844;&#24320;&#35775;&#38382;&#30340;&#25773;&#25918;&#21015;&#34920;&#36229;&#36234;&#20102;&#20165;&#20165;&#38899;&#20048;&#20559;&#22909;&#30340;&#30028;&#38480;&#65306;&#23427;&#20204;&#26159;&#20016;&#23500;&#27934;&#23519;&#29992;&#25143;&#23646;&#24615;&#21644;&#36523;&#20221;&#30340;&#26469;&#28304;&#12290;&#20363;&#22914;&#65292;&#32769;&#24180;&#20154;&#30340;&#38899;&#20048;&#20559;&#22909;&#21487;&#33021;&#26356;&#20559;&#21521;&#20110;&#24343;&#20848;&#20811;&#183;&#36763;&#32435;&#23624;&#65292;&#32780;&#27604;&#33673;&#183;&#33406;&#21033;&#20160;&#20173;&#28982;&#26159;&#21313;&#20960;&#23681;&#38738;&#23569;&#24180;&#30340;&#39318;&#36873;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#25773;&#25918;&#21015;&#34920;&#25104;&#20026;&#20102;&#19968;&#25159;&#20102;&#35299;&#38899;&#20048;&#36523;&#20221;&#22810;&#26679;&#32780;&#19981;&#26029;&#28436;&#21464;&#30340;&#31383;&#21475;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;Spotify&#29992;&#25143;&#23646;&#24615;&#21644;&#20182;&#20204;&#30340;&#20844;&#24320;&#25773;&#25918;&#21015;&#34920;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#35782;&#21035;&#19982;&#29992;&#25143;&#20010;&#20154;&#23646;&#24615;&#30456;&#20851;&#30340;&#32463;&#24120;&#20986;&#29616;&#30340;&#38899;&#20048;&#29305;&#24449;&#65292;&#20363;&#22914;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#65292;&#20064;&#24815;&#25110;&#20010;&#24615;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the age of digital music streaming, playlists on platforms like Spotify have become an integral part of individuals' musical experiences. People create and publicly share their own playlists to express their musical tastes, promote the discovery of their favorite artists, and foster social connections. These publicly accessible playlists transcend the boundaries of mere musical preferences: they serve as sources of rich insights into users' attributes and identities. For example, the musical preferences of elderly individuals may lean more towards Frank Sinatra, while Billie Eilish remains a favored choice among teenagers. These playlists thus become windows into the diverse and evolving facets of one's musical identity.  In this work, we investigate the relationship between Spotify users' attributes and their public playlists. In particular, we focus on identifying recurring musical characteristics associated with users' individual attributes, such as demographics, habits, or perso
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#32467;&#21512;&#32467;&#26500;&#30340;&#25552;&#31034;&#24037;&#31243;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#24615;&#33021;&#26041;&#38754;&#30340;&#21069;&#26223;&#65292;&#36890;&#36807;&#24605;&#32500;&#38142;&#12289;&#24605;&#32500;&#26641;&#25110;&#24605;&#32500;&#22270;&#30340;&#35774;&#35745;&#26469;&#24341;&#23548;&#25972;&#20307;&#25512;&#29702;&#36807;&#31243;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#20363;&#65292;&#36825;&#31181;&#33539;&#24335;&#26174;&#33879;&#22686;&#24378;&#20102;&#27169;&#22411;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#34013;&#22270;&#65292;&#20026;&#26410;&#26469;&#30340;&#21457;&#23637;&#38138;&#24179;&#36947;&#36335;&#12290;</title><link>http://arxiv.org/abs/2401.14295</link><description>&lt;p&gt;
&#25512;&#29702;&#30340;&#25299;&#25169;&#23398;&#65306;&#25581;&#31192;&#24605;&#32500;&#38142;&#12289;&#26641;&#21644;&#22270;
&lt;/p&gt;
&lt;p&gt;
Topologies of Reasoning: Demystifying Chains, Trees, and Graphs of Thoughts. (arXiv:2401.14295v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14295
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#32467;&#21512;&#32467;&#26500;&#30340;&#25552;&#31034;&#24037;&#31243;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#24615;&#33021;&#26041;&#38754;&#30340;&#21069;&#26223;&#65292;&#36890;&#36807;&#24605;&#32500;&#38142;&#12289;&#24605;&#32500;&#26641;&#25110;&#24605;&#32500;&#22270;&#30340;&#35774;&#35745;&#26469;&#24341;&#23548;&#25972;&#20307;&#25512;&#29702;&#36807;&#31243;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#20363;&#65292;&#36825;&#31181;&#33539;&#24335;&#26174;&#33879;&#22686;&#24378;&#20102;&#27169;&#22411;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#34013;&#22270;&#65292;&#20026;&#26410;&#26469;&#30340;&#21457;&#23637;&#38138;&#24179;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#29305;&#21035;&#26159;&#22312;&#36890;&#36807;&#21019;&#26032;&#30340;&#25552;&#31034;&#25216;&#26415;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24615;&#33021;&#26041;&#38754;&#12290;&#20854;&#20013;&#65292;&#19982;&#32467;&#26500;&#30456;&#32467;&#21512;&#30340;&#25552;&#31034;&#24037;&#31243;&#34987;&#35270;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#33539;&#24335;&#65292;&#20854;&#35774;&#35745;&#22914;&#24605;&#32500;&#38142;&#12289;&#24605;&#32500;&#26641;&#25110;&#24605;&#32500;&#22270;&#31561;&#65292;&#36890;&#36807;&#32467;&#26500;&#25351;&#23548;&#25972;&#20307;LLM&#25512;&#29702;&#36807;&#31243;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#20363;&#30340;&#35828;&#26126;&#65292;&#36825;&#31181;&#33539;&#24335;&#26174;&#33879;&#22686;&#24378;&#20102;LLM&#22312;&#36923;&#36753;&#25110;&#25968;&#23398;&#25512;&#29702;&#12289;&#35268;&#21010;&#25110;&#21019;&#36896;&#24615;&#20889;&#20316;&#31561;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#26041;&#20415;&#29702;&#35299;&#36825;&#20010;&#19981;&#26029;&#21457;&#23637;&#30340;&#39046;&#22495;&#24182;&#20026;&#26410;&#26469;&#30340;&#21457;&#23637;&#38138;&#24179;&#36947;&#36335;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;LLM&#25512;&#29702;&#26041;&#26696;&#30340;&#36890;&#29992;&#34013;&#22270;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23545;&#25552;&#31034;&#25191;&#34892;&#27969;&#31243;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#28548;&#28165;&#24182;&#26126;&#30830;&#23450;&#20041;&#20102;&#19981;&#21516;&#30340;&#27010;&#24565;&#12290;&#28982;&#21518;&#25105;&#20204;&#24314;&#31435;&#31532;&#19968;&#20010;&#20998;&#31867;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
The field of natural language processing (NLP) has witnessed significant progress in recent years, with a notable focus on improving large language models' (LLM) performance through innovative prompting techniques. Among these, prompt engineering coupled with structures has emerged as a promising paradigm, with designs such as Chain-of-Thought, Tree of Thoughts, or Graph of Thoughts, in which the overall LLM reasoning is guided by a structure such as a graph. As illustrated with numerous examples, this paradigm significantly enhances the LLM's capability to solve numerous tasks, ranging from logical or mathematical reasoning to planning or creative writing. To facilitate the understanding of this growing field and pave the way for future developments, we devise a general blueprint for effective and efficient LLM reasoning schemes. For this, we conduct an in-depth analysis of the prompt execution pipeline, clarifying and clearly defining different concepts. We then build the first taxon
&lt;/p&gt;</description></item><item><title>&#23545;&#20110;&#21548;&#21147;&#21463;&#25439;&#21548;&#20247;&#26234;&#33021;&#39044;&#27979;&#24212;&#29992;&#65292;&#23545;&#35821;&#38899;&#22522;&#30784;&#27169;&#22411;&#65288;SFM&#65289;&#36827;&#34892;&#20102;&#31995;&#32479;&#35780;&#20272;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#22312;&#20923;&#32467;&#30340;SFM&#20043;&#19978;&#23398;&#20064;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#19987;&#38376;&#30340;&#39044;&#27979;&#22836;&#65292;&#36825;&#19968;&#26041;&#27861;&#22312;Clarity Prediction Challenge 2&#65288;CPC2&#65289;&#20013;&#21462;&#24471;&#20102;&#32988;&#21033;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#35821;&#38899;&#30693;&#35273;&#24212;&#29992;&#20013;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.14289</link><description>&lt;p&gt;
&#23545;&#21548;&#21147;&#21463;&#25439;&#21548;&#20247;&#26234;&#33021;&#39044;&#27979;&#30340;&#35821;&#38899;&#22522;&#30784;&#27169;&#22411;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Speech foundation models on intelligibility prediction for hearing-impaired listeners. (arXiv:2401.14289v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14289
&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#21548;&#21147;&#21463;&#25439;&#21548;&#20247;&#26234;&#33021;&#39044;&#27979;&#24212;&#29992;&#65292;&#23545;&#35821;&#38899;&#22522;&#30784;&#27169;&#22411;&#65288;SFM&#65289;&#36827;&#34892;&#20102;&#31995;&#32479;&#35780;&#20272;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#22312;&#20923;&#32467;&#30340;SFM&#20043;&#19978;&#23398;&#20064;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#19987;&#38376;&#30340;&#39044;&#27979;&#22836;&#65292;&#36825;&#19968;&#26041;&#27861;&#22312;Clarity Prediction Challenge 2&#65288;CPC2&#65289;&#20013;&#21462;&#24471;&#20102;&#32988;&#21033;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#35821;&#38899;&#30693;&#35273;&#24212;&#29992;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#22522;&#30784;&#27169;&#22411;&#65288;SFM&#65289;&#24050;&#32463;&#22312;&#35768;&#22810;&#35821;&#38899;&#22788;&#29702;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#24448;&#24448;&#20197;&#26368;&#23567;&#30340;&#35843;&#25972;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#19982;&#35821;&#38899;&#30693;&#35273;&#31038;&#21306;&#24863;&#20852;&#36259;&#30340;&#24212;&#29992;&#65292;SFM&#33539;&#24335;&#30340;&#30740;&#31350;&#26126;&#26174;&#36739;&#23569;&#12290;&#26412;&#25991;&#23545;10&#31181;SFM&#36827;&#34892;&#20102;&#31995;&#32479;&#35780;&#20272;&#65292;&#20197;&#22312;&#19968;&#20010;&#36825;&#26679;&#30340;&#24212;&#29992;&#20013;&#36827;&#34892;&#30740;&#31350;&#65306;&#35821;&#38899;&#28165;&#26224;&#24230;&#39044;&#27979;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;Clarity Prediction Challenge 2&#65288;CPC2&#65289;&#30340;&#38750;&#20405;&#20837;&#24615;&#35774;&#32622;&#65292;&#20854;&#20013;&#20219;&#21153;&#26159;&#39044;&#27979;&#21548;&#21147;&#21463;&#25439;&#21548;&#20247;&#20174;&#22122;&#22768;&#24405;&#38899;&#20013;&#27491;&#30830;&#29702;&#35299;&#30340;&#21333;&#35789;&#30334;&#20998;&#27604;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#20923;&#32467;&#30340;SFM&#20043;&#19978;&#23398;&#20064;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#19987;&#38376;&#30340;&#39044;&#27979;&#22836;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#20986;SFM&#20043;&#38388;&#24615;&#33021;&#24046;&#24322;&#26377;&#32479;&#35745;&#23398;&#19978;&#30340;&#26174;&#33879;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;CPC2&#20013;&#33719;&#24471;&#20102;&#33719;&#32988;&#30340;&#25552;&#20132;&#65292;&#23637;&#31034;&#20102;&#23427;&#22312;&#35821;&#38899;&#30693;&#35273;&#24212;&#29992;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech foundation models (SFMs) have been benchmarked on many speech processing tasks, often achieving state-of-the-art performance with minimal adaptation. However, the SFM paradigm has been significantly less explored for applications of interest to the speech perception community. In this paper we present a systematic evaluation of 10 SFMs on one such application: Speech intelligibility prediction. We focus on the non-intrusive setup of the Clarity Prediction Challenge 2 (CPC2), where the task is to predict the percentage of words correctly perceived by hearing-impaired listeners from speech-in-noise recordings. We propose a simple method that learns a lightweight specialized prediction head on top of frozen SFMs to approach the problem. Our results reveal statistically significant differences in performance across SFMs. Our method resulted in the winning submission in the CPC2, demonstrating its promise for speech perception applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#24314;&#31435;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#21033;&#29992;&#32479;&#35745;&#23398;&#20064;&#29702;&#35770;&#21644;&#20449;&#24687;&#35770;&#26469;&#20934;&#30830;&#37327;&#21270;&#21644;&#26816;&#27979;&#20449;&#24687;&#27844;&#28431;&#65292;&#36890;&#36807;&#36817;&#20284;&#36125;&#21494;&#26031;&#39044;&#27979;&#30340;&#23545;&#25968;&#25439;&#22833;&#21644;&#20934;&#30830;&#24615;&#26469;&#20934;&#30830;&#20272;&#35745;&#20114;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2401.14283</link><description>&lt;p&gt;
&#36890;&#36807;&#36817;&#20284;&#36125;&#21494;&#26031;&#26368;&#20248;&#39044;&#27979;&#26816;&#27979;&#20449;&#24687;&#27844;&#28431;
&lt;/p&gt;
&lt;p&gt;
Information Leakage Detection through Approximate Bayes-optimal Prediction. (arXiv:2401.14283v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14283
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#24314;&#31435;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#21033;&#29992;&#32479;&#35745;&#23398;&#20064;&#29702;&#35770;&#21644;&#20449;&#24687;&#35770;&#26469;&#20934;&#30830;&#37327;&#21270;&#21644;&#26816;&#27979;&#20449;&#24687;&#27844;&#28431;&#65292;&#36890;&#36807;&#36817;&#20284;&#36125;&#21494;&#26031;&#39044;&#27979;&#30340;&#23545;&#25968;&#25439;&#22833;&#21644;&#20934;&#30830;&#24615;&#26469;&#20934;&#30830;&#20272;&#35745;&#20114;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20170;&#22825;&#30340;&#20197;&#25968;&#25454;&#39537;&#21160;&#30340;&#19990;&#30028;&#20013;&#65292;&#20844;&#24320;&#21487;&#33719;&#24471;&#30340;&#20449;&#24687;&#30340;&#22686;&#21152;&#21152;&#21095;&#20102;&#20449;&#24687;&#27844;&#28431;&#65288;IL&#65289;&#30340;&#25361;&#25112;&#65292;&#24341;&#21457;&#20102;&#23433;&#20840;&#38382;&#39064;&#12290;IL&#28041;&#21450;&#36890;&#36807;&#31995;&#32479;&#30340;&#21487;&#35266;&#23519;&#20449;&#24687;&#26080;&#24847;&#22320;&#23558;&#31192;&#23494;&#65288;&#25935;&#24863;&#65289;&#20449;&#24687;&#26292;&#38706;&#32473;&#26410;&#32463;&#25480;&#26435;&#30340;&#26041;&#65292;&#20256;&#32479;&#30340;&#32479;&#35745;&#26041;&#27861;&#36890;&#36807;&#20272;&#35745;&#21487;&#35266;&#23519;&#20449;&#24687;&#21644;&#31192;&#23494;&#20449;&#24687;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#65288;MI&#65289;&#26469;&#26816;&#27979;IL&#65292;&#38754;&#20020;&#32500;&#24230;&#28798;&#38590;&#12289;&#25910;&#25947;&#12289;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;MI&#20272;&#35745;&#38169;&#35823;&#31561;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#34429;&#28982;&#26032;&#20852;&#30340;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#22312;&#20108;&#36827;&#21046;&#31995;&#32479;&#25935;&#24863;&#20449;&#24687;&#30340;&#26816;&#27979;&#19978;&#26377;&#25928;&#65292;&#20294;&#32570;&#20047;&#19968;&#20010;&#20840;&#38754;&#30340;&#29702;&#35770;&#26694;&#26550;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#20351;&#29992;&#32479;&#35745;&#23398;&#20064;&#29702;&#35770;&#21644;&#20449;&#24687;&#35770;&#24314;&#31435;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#20934;&#30830;&#37327;&#21270;&#21644;&#26816;&#27979;IL&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#21487;&#20197;&#36890;&#36807;&#36817;&#20284;&#36125;&#21494;&#26031;&#39044;&#27979;&#30340;&#23545;&#25968;&#25439;&#22833;&#21644;&#20934;&#30830;&#24615;&#26469;&#20934;&#30830;&#20272;&#35745;MI&#12290;
&lt;/p&gt;
&lt;p&gt;
In today's data-driven world, the proliferation of publicly available information intensifies the challenge of information leakage (IL), raising security concerns. IL involves unintentionally exposing secret (sensitive) information to unauthorized parties via systems' observable information. Conventional statistical approaches, which estimate mutual information (MI) between observable and secret information for detecting IL, face challenges such as the curse of dimensionality, convergence, computational complexity, and MI misestimation. Furthermore, emerging supervised machine learning (ML) methods, though effective, are limited to binary system-sensitive information and lack a comprehensive theoretical framework. To address these limitations, we establish a theoretical framework using statistical learning theory and information theory to accurately quantify and detect IL. We demonstrate that MI can be accurately estimated by approximating the log-loss and accuracy of the Bayes predict
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#28014;&#28216;&#29983;&#29289;&#20998;&#31867;&#22120;&#22312;&#25968;&#25454;&#38598;&#21464;&#21270;&#19979;&#24615;&#33021;&#19979;&#38477;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#25972;&#21512;&#19981;&#21516;&#37096;&#32626;&#26085;&#30340;&#25968;&#25454;&#38598;&#24182;&#35780;&#20272;&#36229;&#20986;&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#65292;&#25581;&#31034;&#20102;&#20998;&#31867;&#22120;&#22312;&#23454;&#38469;&#24773;&#26223;&#20013;&#36935;&#21040;&#30340;&#22833;&#36133;&#24773;&#20917;&#12290;&#22522;&#20110;&#23545;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#30340;&#26465;&#20214;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#38450;&#24615;&#35780;&#20272;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#26032;&#25968;&#25454;&#20998;&#31867;&#20013;&#21487;&#33021;&#20986;&#29616;&#30340;&#38382;&#39064;&#65292;&#24182;&#30830;&#23450;&#24433;&#21709;&#20998;&#31867;&#30340;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2401.14256</link><description>&lt;p&gt;
&#20135;&#29983;&#23545;&#25968;&#25454;&#38598;&#21464;&#21270;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#28014;&#28216;&#29983;&#29289;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Producing Plankton Classifiers that are Robust to Dataset Shift. (arXiv:2401.14256v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14256
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#28014;&#28216;&#29983;&#29289;&#20998;&#31867;&#22120;&#22312;&#25968;&#25454;&#38598;&#21464;&#21270;&#19979;&#24615;&#33021;&#19979;&#38477;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#25972;&#21512;&#19981;&#21516;&#37096;&#32626;&#26085;&#30340;&#25968;&#25454;&#38598;&#24182;&#35780;&#20272;&#36229;&#20986;&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#65292;&#25581;&#31034;&#20102;&#20998;&#31867;&#22120;&#22312;&#23454;&#38469;&#24773;&#26223;&#20013;&#36935;&#21040;&#30340;&#22833;&#36133;&#24773;&#20917;&#12290;&#22522;&#20110;&#23545;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#30340;&#26465;&#20214;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#38450;&#24615;&#35780;&#20272;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#26032;&#25968;&#25454;&#20998;&#31867;&#20013;&#21487;&#33021;&#20986;&#29616;&#30340;&#38382;&#39064;&#65292;&#24182;&#30830;&#23450;&#24433;&#21709;&#20998;&#31867;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#28014;&#28216;&#29983;&#29289;&#39640;&#36890;&#37327;&#30417;&#27979;&#20381;&#36182;&#20110;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#22120;&#23545;&#27700;&#29983;&#24577;&#31995;&#32479;&#20013;&#30340;&#29289;&#31181;&#36827;&#34892;&#35782;&#21035;&#12290;&#23613;&#31649;&#22312;&#29702;&#35770;&#24615;&#33021;&#26041;&#38754;&#20196;&#20154;&#28385;&#24847;&#65292;&#20294;&#25968;&#25454;&#38598;&#21464;&#21270;&#32473;&#37096;&#32626;&#36807;&#31243;&#20013;&#30340;&#24615;&#33021;&#24102;&#26469;&#20102;&#26174;&#33879;&#25361;&#25112;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23558;ZooLake&#25968;&#25454;&#38598;&#19982;10&#20010;&#29420;&#31435;&#37096;&#32626;&#26085;&#30340;&#25163;&#21160;&#27880;&#37322;&#22270;&#20687;&#38598;&#25104;&#65292;&#20316;&#20026;&#27979;&#35797;&#21333;&#20803;&#65292;&#20197;&#35780;&#20272;&#36229;&#20986;&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#20998;&#31867;&#22120;&#22312;&#29702;&#24819;&#24773;&#20917;&#19979;&#34920;&#29616;&#33391;&#22909;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#23454;&#38469;&#24773;&#26223;&#20013;&#36935;&#21040;&#26174;&#33879;&#22833;&#36133;&#30340;&#23454;&#20363;&#12290;&#20363;&#22914;&#65292;&#19968;&#20010;&#20855;&#26377;92&#65285;&#26631;&#31216;&#27979;&#35797;&#20934;&#30830;&#24230;&#30340;MobileNet&#27169;&#22411;&#26174;&#31034;&#20102;77&#65285;&#30340;&#36229;&#20986;&#25968;&#25454;&#38598;&#20934;&#30830;&#24230;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#23548;&#33268;&#36229;&#20986;&#25968;&#25454;&#38598;&#24615;&#33021;&#19979;&#38477;&#30340;&#26465;&#20214;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#38450;&#24615;&#35780;&#20272;&#26041;&#27861;&#65292;&#20197;&#35782;&#21035;&#23545;&#26032;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#26102;&#21487;&#33021;&#20986;&#29616;&#30340;&#38382;&#39064;&#65292;&#24182;&#30830;&#23450;&#24433;&#21709;&#20998;&#31867;&#30340;&#36229;&#20986;&#25968;&#25454;&#38598;&#22270;&#20687;&#20013;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#27493;&#27969;&#31243;&#65306;(i)&#35782;&#21035;&#36229;&#20986;&#25968;&#25454;&#38598;&#30340;&#23454;&#20363;&#65292;(ii)&#35780;&#20272;&#36229;&#20986;&#25968;&#25454;&#38598;&#24615;&#33021;&#30340;&#24433;&#21709;&#22240;&#32032;&#65292;(iii)&#39044;&#27979;&#26032;&#25968;&#25454;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern plankton high-throughput monitoring relies on deep learning classifiers for species recognition in water ecosystems. Despite satisfactory nominal performances, a significant challenge arises from Dataset Shift, which causes performances to drop during deployment. In our study, we integrate the ZooLake dataset with manually-annotated images from 10 independent days of deployment, serving as test cells to benchmark Out-Of-Dataset (OOD) performances. Our analysis reveals instances where classifiers, initially performing well in In-Dataset conditions, encounter notable failures in practical scenarios. For example, a MobileNet with a 92% nominal test accuracy shows a 77% OOD accuracy. We systematically investigate conditions leading to OOD performance drops and propose a preemptive assessment method to identify potential pitfalls when classifying new data, and pinpoint features in OOD images that adversely impact classification. We present a three-step pipeline: (i) identifying OOD d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#32467;&#21512;&#35821;&#27861;&#28436;&#21270;&#21644;&#25968;&#25454;&#22686;&#24378;&#26041;&#26696;&#65292;&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#25216;&#26415;STEM&#26469;&#35757;&#32451;&#21487;&#35299;&#37322;&#30340;&#20083;&#33146;&#30284;&#35786;&#26029;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#25968;&#25454;&#19981;&#24179;&#34913;&#21644;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.14255</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#27861;&#28436;&#21270;&#21644;&#25968;&#25454;&#22686;&#24378;&#30340;&#21487;&#35299;&#37322;&#20083;&#33146;&#30284;&#35786;&#26029;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Interpretable Solutions for Breast Cancer Diagnosis with Grammatical Evolution and Data Augmentation. (arXiv:2401.14255v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14255
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#32467;&#21512;&#35821;&#27861;&#28436;&#21270;&#21644;&#25968;&#25454;&#22686;&#24378;&#26041;&#26696;&#65292;&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#25216;&#26415;STEM&#26469;&#35757;&#32451;&#21487;&#35299;&#37322;&#30340;&#20083;&#33146;&#30284;&#35786;&#26029;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#25968;&#25454;&#19981;&#24179;&#34913;&#21644;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#24433;&#20687;&#35786;&#26029;&#36234;&#26469;&#36234;&#20381;&#36182;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#20219;&#21153;&#24120;&#24120;&#21463;&#21040;&#20005;&#37325;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;&#30340;&#22256;&#25200;&#65292;&#20854;&#20013;&#38451;&#24615;&#30149;&#20363;&#21487;&#33021;&#38750;&#24120;&#32597;&#35265;&#12290;&#32780;&#23427;&#20204;&#30340;&#21487;&#35299;&#37322;&#24615;&#21448;&#38750;&#24120;&#26377;&#38480;&#65292;&#36825;&#19968;&#28857;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#23613;&#31649;&#20107;&#21518;&#35299;&#37322;&#25216;&#26415;&#65288;&#22914;SHAP&#21644;LIME&#65289;&#22312;&#25152;&#35859;&#30340;&#40657;&#30418;&#27169;&#22411;&#19978;&#24050;&#32463;&#26377;&#19968;&#23450;&#30340;&#25104;&#21151;&#65292;&#20294;&#20351;&#29992;&#20869;&#22312;&#21487;&#29702;&#35299;&#30340;&#27169;&#22411;&#20250;&#20351;&#36825;&#20123;&#21162;&#21147;&#26356;&#21152;&#26377;&#25104;&#25928;&#12290;&#26412;&#25991;&#36890;&#36807;&#23637;&#31034;&#22914;&#20309;&#21033;&#29992;&#19968;&#31181;&#30456;&#23545;&#36739;&#26032;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#25216;&#26415;STEM&#65292;&#29992;&#20110;&#20135;&#29983;&#29992;&#35821;&#27861;&#28436;&#21270;&#20135;&#29983;&#30340;&#21487;&#35299;&#37322;&#27169;&#22411;&#25152;&#38656;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;STEM&#26159;&#19968;&#31181;&#26368;&#36817;&#24341;&#20837;&#30340;&#21512;&#25104;&#23569;&#25968;&#31867;&#36807;&#37319;&#26679;&#25216;&#26415;&#65288;SMOTE&#65289;&#12289;&#32534;&#36753;&#26368;&#36817;&#37051;&#65288;ENN&#65289;&#21644;&#28151;&#21512;&#30340;&#32452;&#21512;&#65307;&#23427;&#20808;&#21069;&#24050;&#25104;&#21151;&#29992;&#20110;&#35299;&#20915;&#31867;&#21035;&#38388;&#21644;&#31867;&#21035;&#20869;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical imaging diagnosis increasingly relies on Machine Learning (ML) models. This is a task that is often hampered by severely imbalanced datasets, where positive cases can be quite rare. Their use is further compromised by their limited interpretability, which is becoming increasingly important. While post-hoc interpretability techniques such as SHAP and LIME have been used with some success on so-called black box models, the use of inherently understandable models makes such endeavors more fruitful. This paper addresses these issues by demonstrating how a relatively new synthetic data generation technique, STEM, can be used to produce data to train models produced by Grammatical Evolution (GE) that are inherently understandable. STEM is a recently introduced combination of the Synthetic Minority Oversampling Technique (SMOTE), Edited Nearest Neighbour (ENN), and Mixup; it has previously been successfully used to tackle both between class and within class imbalance issues. We test o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#20986;&#30340;&#26631;&#27880;&#26041;&#27861;&#23545;Reddit&#25991;&#26412;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#32454;&#35843;&#20102;Longformer&#27169;&#22411;&#65292;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#22312;&#33521;&#35821;&#21644;&#21346;&#24178;&#36798;&#35821;&#30340;&#25233;&#37057;&#30151;&#20005;&#37325;&#31243;&#24230;&#20998;&#31867;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.14240</link><description>&lt;p&gt;
&#22522;&#20110;Reddit&#25991;&#26412;&#22686;&#24378;&#26631;&#27880;&#25216;&#26415;&#21644;&#32454;&#35843;Longformer&#27169;&#22411;&#30340;&#33521;&#35821;&#21644;&#21346;&#24178;&#36798;&#25233;&#37057;&#30151;&#20005;&#37325;&#31243;&#24230;&#20998;&#31867;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Enhanced Labeling Technique for Reddit Text and Fine-Tuned Longformer Models for Classifying Depression Severity in English and Luganda. (arXiv:2401.14240v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14240
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#20986;&#30340;&#26631;&#27880;&#26041;&#27861;&#23545;Reddit&#25991;&#26412;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#32454;&#35843;&#20102;Longformer&#27169;&#22411;&#65292;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#22312;&#33521;&#35821;&#21644;&#21346;&#24178;&#36798;&#35821;&#30340;&#25233;&#37057;&#30151;&#20005;&#37325;&#31243;&#24230;&#20998;&#31867;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25233;&#37057;&#30151;&#26159;&#20840;&#29699;&#36127;&#25285;&#37325;&#30340;&#12289;&#38590;&#20197;&#25511;&#21046;&#30340;&#24515;&#29702;&#20581;&#24247;&#38382;&#39064;&#20043;&#19968;&#12290;&#19987;&#23478;&#20204;&#21487;&#20197;&#20351;&#29992;&#36125;&#20811;&#25233;&#37057;&#37327;&#34920;&#65288;BDI&#65289;&#38382;&#21367;&#26089;&#26399;&#26816;&#27979;&#20854;&#20005;&#37325;&#31243;&#24230;&#65292;&#32473;&#24739;&#32773;&#26045;&#29992;&#36866;&#24403;&#33647;&#29289;&#65292;&#38459;&#27490;&#20854;&#36827;&#23637;&#12290;&#30001;&#20110;&#25285;&#24515;&#21487;&#33021;&#30340;&#27745;&#21517;&#21270;&#65292;&#35768;&#22810;&#24739;&#32773;&#36716;&#21521;Reddit&#31561;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#23547;&#27714;&#24314;&#35758;&#21644;&#24110;&#21161;&#12290;&#26412;&#30740;&#31350;&#20174;Reddit&#25552;&#21462;&#25991;&#26412;&#20197;&#20419;&#36827;&#35786;&#26029;&#36807;&#31243;&#12290;&#23427;&#37319;&#29992;&#20102;&#19968;&#31181;&#25552;&#20986;&#30340;&#26631;&#27880;&#26041;&#27861;&#23545;&#25991;&#26412;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#38543;&#21518;&#23545;Longformer&#27169;&#22411;&#36827;&#34892;&#20102;&#32454;&#35843;&#12290;&#23558;&#35813;&#27169;&#22411;&#19982;&#22522;&#20934;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#21253;&#25324;&#26420;&#32032;&#36125;&#21494;&#26031;&#12289;&#38543;&#26426;&#26862;&#26519;&#12289;&#25903;&#25345;&#21521;&#37327;&#26426;&#21644;&#26799;&#24230;&#25552;&#21319;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;Longformer&#27169;&#22411;&#22312;&#33521;&#35821;&#65288;48%&#65289;&#21644;&#21346;&#24178;&#36798;&#35821;&#65288;45%&#65289;&#30340;&#33258;&#23450;&#20041;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Depression is a global burden and one of the most challenging mental health conditions to control. Experts can detect its severity early using the Beck Depression Inventory (BDI) questionnaire, administer appropriate medication to patients, and impede its progression. Due to the fear of potential stigmatization, many patients turn to social media platforms like Reddit for advice and assistance at various stages of their journey. This research extracts text from Reddit to facilitate the diagnostic process. It employs a proposed labeling approach to categorize the text and subsequently fine-tunes the Longformer model. The model's performance is compared against baseline models, including Naive Bayes, Random Forest, Support Vector Machines, and Gradient Boosting. Our findings reveal that the Longformer model outperforms the baseline models in both English (48%) and Luganda (45%) languages on a custom-made dataset.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#38450;&#24481;&#26041;&#27861;AR-GAN&#65292;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;&#30340;&#20132;&#36890;&#26631;&#24535;&#20998;&#31867;&#31995;&#32479;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#22312;&#21508;&#31181;&#23545;&#25239;&#25915;&#20987;&#19979;&#20445;&#25345;&#39640;&#24615;&#33021;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.14232</link><description>&lt;p&gt;
AR-GAN: &#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20132;&#36890;&#26631;&#24535;&#20998;&#31867;&#31995;&#32479;&#30340;&#38450;&#24481;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
AR-GAN: Generative Adversarial Network-Based Defense Method Against Adversarial Attacks on the Traffic Sign Classification System of Autonomous Vehicles. (arXiv:2401.14232v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14232
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#38450;&#24481;&#26041;&#27861;AR-GAN&#65292;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;&#30340;&#20132;&#36890;&#26631;&#24535;&#20998;&#31867;&#31995;&#32479;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#22312;&#21508;&#31181;&#23545;&#25239;&#25915;&#20987;&#19979;&#20445;&#25345;&#39640;&#24615;&#33021;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;&#30340;&#20132;&#36890;&#26631;&#24535;&#20998;&#31867;&#65292;&#31216;&#20026;&#25915;&#20987;&#40065;&#26834;&#30340;GAN&#65288;AR-GAN&#65289;&#12290;AR-GAN&#30340;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#65288;i&#65289;&#20551;&#35774;&#23545;&#25239;&#25915;&#20987;&#27169;&#22411;&#21644;&#26679;&#26412;&#19968;&#26080;&#25152;&#30693;&#65292;&#65288;ii&#65289;&#22312;&#21508;&#31181;&#23545;&#25239;&#25915;&#20987;&#31867;&#22411;&#19979;&#22987;&#32456;&#25552;&#20379;&#39640;&#27700;&#24179;&#30340;&#20132;&#36890;&#26631;&#24535;&#20998;&#31867;&#24615;&#33021;&#12290;AR-GAN&#20998;&#31867;&#31995;&#32479;&#30001;&#19968;&#20010;&#36890;&#36807;&#37325;&#24314;&#21435;&#22122;&#22270;&#20687;&#30340;&#29983;&#25104;&#22120;&#21644;&#19968;&#20010;&#23545;&#37325;&#24314;&#22270;&#20687;&#36827;&#34892;&#20998;&#31867;&#30340;&#20998;&#31867;&#22120;&#32452;&#25104;&#12290;&#20316;&#32773;&#22312;&#27809;&#26377;&#25915;&#20987;&#21644;&#21508;&#31181;&#23545;&#25239;&#25915;&#20987;&#65288;&#22914;&#24555;&#36895;&#26799;&#24230;&#31526;&#21495;&#27861;&#65288;FGSM&#65289;&#65292;DeepFool&#65292;Carlini&#21644;Wagner&#65288;C&amp;W&#65289;&#65292;&#20197;&#21450;&#25237;&#24433;&#26799;&#24230;&#19979;&#38477;&#65288;PGD&#65289;&#65289;&#30340;&#24773;&#20917;&#19979;&#27979;&#35797;&#20102;AR-GAN&#12290;&#20316;&#32773;&#32771;&#34385;&#20102;&#36825;&#20123;&#25915;&#20987;&#30340;&#20004;&#31181;&#24418;&#24335;&#65292;&#21363;&#65288;i&#65289;&#40657;&#30418;&#25915;&#20987;&#65288;&#20551;&#35774;&#25915;&#20987;&#32773;&#23545;&#20998;&#31867;&#22120;&#27809;&#26377;&#20219;&#20309;&#20808;&#39564;&#30693;&#35782;&#65289;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#30333;&#30418;&#25915;&#20987;&#65288;&#20551;&#35774;&#25915;&#20987;&#32773;&#25317;&#26377;&#23436;&#20840;&#30693;&#35782;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study developed a generative adversarial network (GAN)-based defense method for traffic sign classification in an autonomous vehicle (AV), referred to as the attack-resilient GAN (AR-GAN). The novelty of the AR-GAN lies in (i) assuming zero knowledge of adversarial attack models and samples and (ii) providing consistently high traffic sign classification performance under various adversarial attack types. The AR-GAN classification system consists of a generator that denoises an image by reconstruction, and a classifier that classifies the reconstructed image. The authors have tested the AR-GAN under no-attack and under various adversarial attacks, such as Fast Gradient Sign Method (FGSM), DeepFool, Carlini and Wagner (C&amp;W), and Projected Gradient Descent (PGD). The authors considered two forms of these attacks, i.e., (i) black-box attacks (assuming the attackers possess no prior knowledge of the classifier), and (ii) white-box attacks (assuming the attackers possess full knowledge
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#35757;&#32451;&#30340;&#27169;&#22359;&#30340;&#21487;&#31227;&#26893;&#24615;&#65292;&#21457;&#29616;&#36825;&#20123;&#31227;&#26893;&#30340;&#27169;&#22359;&#22312;&#21508;&#31181;&#24773;&#26223;&#19979;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#37325;&#22797;&#21033;&#29992;&#20219;&#21153;&#29305;&#23450;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2401.14228</link><description>&lt;p&gt;
&#35780;&#20272;&#36890;&#36807;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#35757;&#32451;&#30340;&#21442;&#25968;&#30697;&#38453;&#30340;&#21487;&#31227;&#26893;&#24615;
&lt;/p&gt;
&lt;p&gt;
Assessing the Portability of Parameter Matrices Trained by Parameter-Efficient Finetuning Methods. (arXiv:2401.14228v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14228
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#35757;&#32451;&#30340;&#27169;&#22359;&#30340;&#21487;&#31227;&#26893;&#24615;&#65292;&#21457;&#29616;&#36825;&#20123;&#31227;&#26893;&#30340;&#27169;&#22359;&#22312;&#21508;&#31181;&#24773;&#26223;&#19979;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#37325;&#22797;&#21033;&#29992;&#20219;&#21153;&#29305;&#23450;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35757;&#32451;&#35268;&#27169;&#36234;&#26469;&#36234;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#25104;&#26412;&#22686;&#21152;&#65292;&#23545;&#37325;&#22797;&#21033;&#29992;&#20808;&#21069;&#23398;&#21040;&#30340;&#30693;&#35782;&#30340;&#20852;&#36259;&#20063;&#22312;&#22686;&#21152;&#12290;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#34920;&#26126;&#65292;&#37325;&#22797;&#21033;&#29992;&#38750;&#20219;&#21153;&#29305;&#23450;&#30693;&#35782;&#21487;&#20197;&#24110;&#21161;&#21518;&#32493;&#29305;&#23450;&#20219;&#21153;&#23398;&#20064;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#30456;&#21453;&#30340;&#24773;&#20917;&#65306;&#23558;&#20174;&#19968;&#20010;&#27169;&#22411;&#31227;&#26893;&#32534;&#30721;&#20219;&#21153;&#29305;&#23450;&#30693;&#35782;&#30340;&#23436;&#25972;&#21151;&#33021;&#27169;&#22359;&#21040;&#21478;&#19968;&#20010;&#27169;&#22411;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#39033;&#21253;&#25324;1,440&#20010;&#35757;&#32451;/&#27979;&#35797;&#36816;&#34892;&#30340;&#30740;&#31350;&#65292;&#20197;&#27979;&#35797;&#36890;&#36807;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;(PEFT)&#25216;&#26415;&#35757;&#32451;&#30340;&#27169;&#22359;&#30340;&#21487;&#31227;&#26893;&#24615;&#65292;&#20197;&#24773;&#24863;&#20998;&#26512;&#20026;&#31034;&#20363;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#22330;&#26223;&#20013;&#27979;&#35797;&#20102;&#21487;&#31227;&#26893;&#24615;&#65292;&#28041;&#21450;&#19981;&#21516;&#30340;PEFT&#25216;&#26415;&#21644;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#20027;&#26426;&#27169;&#22411;&#65292;&#31561;&#31561;&#12290;&#25105;&#20204;&#23558;&#31227;&#26893;&#30340;&#27169;&#22359;&#30340;&#24615;&#33021;&#19982;(i)&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#30340;&#30456;&#31561;&#27169;&#22359;&#30340;&#24615;&#33021;&#21644;(ii)&#20174;&#19982;&#31227;&#26893;&#30340;&#27169;&#22359;&#30456;&#21516;&#20998;&#24067;&#30340;&#21442;&#25968;&#20013;&#37319;&#26679;&#35757;&#32451;&#30340;&#27169;&#22359;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#31227;&#26893;&#30340;&#27169;&#22359;&#30340;&#24615;&#33021;&#36828;&#36828;&#36229;&#36807;&#25152;&#27979;&#35797;&#30340;&#20004;&#31181;&#26367;&#20195;&#26041;&#26696;&#30340;&#24615;&#33021;&#65292;&#20294;&#38656;&#27880;&#24847;&#19968;&#20123;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the cost of training ever larger language models has grown, so has the interest in reusing previously learnt knowledge. Transfer learning methods have shown how reusing non-task-specific knowledge can help in subsequent task-specific learning. In this paper, we investigate the inverse: porting whole functional modules that encode task-specific knowledge from one model to another. We designed a study comprising 1,440 training/testing runs to test the portability of modules trained by parameter-efficient finetuning (PEFT) techniques, using sentiment analysis as an example task. We test portability in a wide range of scenarios, involving different PEFT techniques and different pretrained host models, among other dimensions. We compare the performance of ported modules with that of equivalent modules trained (i) from scratch, and (ii) from parameters sampled from the same distribution as the ported module. We find that the ported modules far outperform the two alternatives tested, but t
&lt;/p&gt;</description></item><item><title>&#26412;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#26500;&#24314;&#22870;&#21169;&#20989;&#25968;&#26469;&#36873;&#25321;&#26368;&#20248;&#23376;&#20219;&#21153;&#65292;&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.14226</link><description>&lt;p&gt;
&#33258;&#21160;&#23398;&#20064;&#32452;&#21512;&#23376;&#20219;&#21153;&#30340;&#39640;&#26679;&#26412;&#25928;&#29575;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Sample Efficient Reinforcement Learning by Automatically Learning to Compose Subtasks. (arXiv:2401.14226v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14226
&lt;/p&gt;
&lt;p&gt;
&#26412;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#26500;&#24314;&#22870;&#21169;&#20989;&#25968;&#26469;&#36873;&#25321;&#26368;&#20248;&#23376;&#20219;&#21153;&#65292;&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#26159;&#26680;&#24515;&#38382;&#39064;&#65292;&#23588;&#20854;&#26159;&#22312;&#22870;&#21169;&#31232;&#30095;&#30340;&#29615;&#22659;&#20013;&#12290;&#26368;&#36817;&#30340;&#19968;&#20123;&#26041;&#27861;&#25552;&#20986;&#23558;&#22870;&#21169;&#20989;&#25968;&#20316;&#20026;&#25163;&#21160;&#35774;&#35745;&#25110;&#23398;&#20064;&#24471;&#21040;&#30340;&#22870;&#21169;&#32467;&#26500;&#26469;&#25913;&#36827;&#23398;&#20064;&#25928;&#29575;&#12290;&#25163;&#21160;&#35774;&#35745;&#30340;&#22870;&#21169;&#32467;&#26500;&#21487;&#33021;&#23384;&#22312;&#19981;&#20934;&#30830;&#24615;&#65292;&#32780;&#29616;&#26377;&#30340;&#33258;&#21160;&#23398;&#20064;&#26041;&#27861;&#23545;&#20110;&#22797;&#26434;&#20219;&#21153;&#32780;&#35328;&#24120;&#24120;&#35745;&#31639;&#38590;&#20197;&#22788;&#29702;&#12290;&#19981;&#20934;&#30830;&#25110;&#23616;&#37096;&#30340;&#22870;&#21169;&#32467;&#26500;&#30340;&#25972;&#21512;&#22312;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#26080;&#27861;&#23398;&#21040;&#26368;&#20248;&#31574;&#30053;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#33258;&#21160;&#26500;&#24314;&#22870;&#21169;&#20989;&#25968;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#32473;&#23450;&#19968;&#32452;&#34920;&#31034;&#23376;&#20219;&#21153;&#30340;&#26631;&#31614;&#12290;&#22312;&#23545;&#20219;&#21153;&#30340;&#26368;&#23567;&#20102;&#35299;&#19979;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#39640;&#23618;&#31574;&#30053;&#65292;&#22312;&#27599;&#20010;&#29366;&#24577;&#19979;&#36873;&#25321;&#26368;&#20248;&#30340;&#23376;&#20219;&#21153;&#65292;&#21516;&#26102;&#36824;&#26377;&#19968;&#20010;&#20302;&#23618;&#31574;&#30053;&#39640;&#25928;&#23398;&#20064;&#23436;&#25104;&#27599;&#20010;&#23376;&#20219;&#21153;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#19981;&#21516;&#29615;&#22659;&#19979;&#30340;&#34920;&#29616;&#65292;&#24182;&#19982;&#20854;&#20182;&#26041;&#27861;&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
Improving sample efficiency is central to Reinforcement Learning (RL), especially in environments where the rewards are sparse. Some recent approaches have proposed to specify reward functions as manually designed or learned reward structures whose integrations in the RL algorithms are claimed to significantly improve the learning efficiency. Manually designed reward structures can suffer from inaccuracy and existing automatically learning methods are often computationally intractable for complex tasks. The integration of inaccurate or partial reward structures in RL algorithms fail to learn optimal policies. In this work, we propose an RL algorithm that can automatically structure the reward function for sample efficiency, given a set of labels that signify subtasks. Given such minimal knowledge about the task, we train a high-level policy that selects optimal sub-tasks in each state together with a low-level policy that efficiently learns to complete each sub-task. We evaluate our al
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedCompress&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#26435;&#37325;&#32858;&#31867;&#21644;&#26381;&#21153;&#22120;&#31471;&#30693;&#35782;&#33976;&#39311;&#30340;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#36890;&#20449;&#30340;&#32852;&#37030;&#23398;&#20064;&#12290;&#35813;&#26041;&#27861;&#22312;&#38477;&#20302;&#36890;&#20449;&#25104;&#26412;&#30340;&#21516;&#26102;&#65292;&#33021;&#22815;&#23398;&#20064;&#21040;&#39640;&#24230;&#21487;&#27867;&#21270;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.14211</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#36866;&#24212;&#26435;&#37325;&#32858;&#31867;&#21644;&#26381;&#21153;&#22120;&#31471;&#33976;&#39311;&#23454;&#29616;&#39640;&#25928;&#36890;&#20449;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Communication-Efficient Federated Learning through Adaptive Weight Clustering and Server-Side Distillation. (arXiv:2401.14211v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14211
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedCompress&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#26435;&#37325;&#32858;&#31867;&#21644;&#26381;&#21153;&#22120;&#31471;&#30693;&#35782;&#33976;&#39311;&#30340;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#36890;&#20449;&#30340;&#32852;&#37030;&#23398;&#20064;&#12290;&#35813;&#26041;&#27861;&#22312;&#38477;&#20302;&#36890;&#20449;&#25104;&#26412;&#30340;&#21516;&#26102;&#65292;&#33021;&#22815;&#23398;&#20064;&#21040;&#39640;&#24230;&#21487;&#27867;&#21270;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26377;&#26395;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#21516;&#26102;&#65292;&#36890;&#36807;&#22810;&#20010;&#35774;&#22791;&#20849;&#21516;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35757;&#32451;&#36807;&#31243;&#20013;&#37325;&#22797;&#30340;&#26381;&#21153;&#22120;-&#23458;&#25143;&#31471;&#36890;&#20449;&#23548;&#33268;&#20102;&#36807;&#22810;&#30340;&#36890;&#20449;&#25104;&#26412;&#65292;&#36825;&#32473;&#32852;&#37030;&#23398;&#20064;&#24102;&#26469;&#20102;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#27169;&#22411;&#21387;&#32553;&#25216;&#26415;&#65292;&#20363;&#22914;&#31232;&#30095;&#21270;&#21644;&#26435;&#37325;&#32858;&#31867;&#65292;&#28982;&#32780;&#36825;&#20123;&#25216;&#26415;&#36890;&#24120;&#38656;&#35201;&#20462;&#25913;&#24213;&#23618;&#30340;&#27169;&#22411;&#32858;&#21512;&#26041;&#26696;&#25110;&#32773;&#28041;&#21450;&#32321;&#29712;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#65292;&#21518;&#32773;&#19981;&#20165;&#35843;&#25972;&#20102;&#27169;&#22411;&#30340;&#21387;&#32553;&#29575;&#65292;&#36824;&#38480;&#21046;&#20102;&#27169;&#22411;&#22312;&#19981;&#26029;&#22686;&#38271;&#30340;&#25968;&#25454;&#19978;&#30340;&#25345;&#32493;&#25913;&#36827;&#28508;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;FedCompress&#65292;&#23427;&#32467;&#21512;&#20102;&#21160;&#24577;&#26435;&#37325;&#32858;&#31867;&#21644;&#26381;&#21153;&#22120;&#31471;&#30693;&#35782;&#33976;&#39311;&#65292;&#20197;&#38477;&#20302;&#36890;&#20449;&#25104;&#26412;&#21516;&#26102;&#23398;&#20064;&#39640;&#24230;&#21487;&#27867;&#21270;&#30340;&#27169;&#22411;&#12290;&#36890;&#36807;&#23545;&#22810;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#27604;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a promising technique for the collaborative training of deep neural networks across multiple devices while preserving data privacy. Despite its potential benefits, FL is hindered by excessive communication costs due to repeated server-client communication during training. To address this challenge, model compression techniques, such as sparsification and weight clustering are applied, which often require modifying the underlying model aggregation schemes or involve cumbersome hyperparameter tuning, with the latter not only adjusts the model's compression rate but also limits model's potential for continuous improvement over growing data. In this paper, we propose FedCompress, a novel approach that combines dynamic weight clustering and server-side knowledge distillation to reduce communication costs while learning highly generalizable models. Through a comprehensive evaluation on diverse public datasets, we demonstrate the efficacy of our approach compared to
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#26497;&#20540;&#29702;&#35770;&#39537;&#21160;&#30340;&#27169;&#22411;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#28369;&#22369;&#28798;&#23475;&#12290;&#36825;&#31181;&#26041;&#27861;&#23558;&#28369;&#22369;&#20301;&#32622;&#30340;&#31354;&#38388;&#20449;&#24687;&#12289;&#23041;&#32961;&#31243;&#24230;&#21644;&#39057;&#29575;&#32467;&#21512;&#36215;&#26469;&#65292;&#22635;&#34917;&#20102;&#30446;&#21069;&#22312;&#22788;&#29702;&#22823;&#33539;&#22260;&#22320;&#21306;&#26102;&#20165;&#32771;&#34385;&#20004;&#20010;&#20803;&#32032;&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;</title><link>http://arxiv.org/abs/2401.14210</link><description>&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#19982;&#26497;&#20540;&#32479;&#35745;&#20043;&#38388;&#65306;&#23545;&#28369;&#22369;&#28798;&#23475;&#23450;&#20041;&#30340;&#24418;&#24335;&#21270;
&lt;/p&gt;
&lt;p&gt;
At the junction between deep learning and statistics of extremes: formalizing the landslide hazard definition. (arXiv:2401.14210v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14210
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#26497;&#20540;&#29702;&#35770;&#39537;&#21160;&#30340;&#27169;&#22411;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#28369;&#22369;&#28798;&#23475;&#12290;&#36825;&#31181;&#26041;&#27861;&#23558;&#28369;&#22369;&#20301;&#32622;&#30340;&#31354;&#38388;&#20449;&#24687;&#12289;&#23041;&#32961;&#31243;&#24230;&#21644;&#39057;&#29575;&#32467;&#21512;&#36215;&#26469;&#65292;&#22635;&#34917;&#20102;&#30446;&#21069;&#22312;&#22788;&#29702;&#22823;&#33539;&#22260;&#22320;&#21306;&#26102;&#20165;&#32771;&#34385;&#20004;&#20010;&#20803;&#32032;&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28369;&#22369;&#28798;&#23475;&#30340;&#26368;&#24120;&#29992;&#23450;&#20041;&#32467;&#21512;&#20102;&#28369;&#22369;&#20301;&#32622;&#30340;&#31354;&#38388;&#20449;&#24687;&#65288;&#26131;&#21457;&#24615;&#65289;&#12289;&#23041;&#32961;&#31243;&#24230;&#65288;&#24378;&#24230;&#65289;&#21644;&#39057;&#29575;&#65288;&#37325;&#29616;&#26399;&#65289;&#12290;&#22312;&#22788;&#29702;&#22823;&#33539;&#22260;&#22320;&#21306;&#26102;&#65292;&#36890;&#24120;&#21482;&#32771;&#34385;&#21644;&#20272;&#35745;&#21069;&#20004;&#20010;&#20803;&#32032;&#12290;&#21363;&#20415;&#22914;&#27492;&#65292;&#20998;&#31163;&#30340;&#27169;&#22411;&#20173;&#26159;&#26631;&#20934;&#65292;&#23545;&#39057;&#29575;&#30340;&#30740;&#31350;&#24456;&#23569;&#12290;&#39057;&#29575;&#21644;&#24378;&#24230;&#30456;&#20114;&#20132;&#32455;&#24182;&#30456;&#20114;&#20381;&#36182;&#65292;&#22240;&#20026;&#26356;&#22823;&#35268;&#27169;&#30340;&#20107;&#20214;&#21457;&#29983;&#30340;&#39057;&#29575;&#36739;&#20302;&#65292;&#21453;&#20043;&#20134;&#28982;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#22810;&#26102;&#26399;&#28165;&#21333;&#21644;&#32852;&#21512;&#32479;&#35745;&#27169;&#22411;&#65292;&#36890;&#36807;&#32479;&#19968;&#30340;&#28798;&#23475;&#27169;&#22411;&#23545;&#36825;&#20123;&#23646;&#24615;&#36827;&#34892;&#24314;&#27169;&#19968;&#30452;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#23578;&#26410;&#23581;&#35797;&#36807;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#32479;&#19968;&#27169;&#22411;&#65292;&#20197;&#21333;&#20803;&#22369;&#24230;&#27700;&#24179;&#20272;&#35745;&#28369;&#22369;&#28798;&#23475;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#24046;&#36317;&#12290;&#25105;&#20204;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#32467;&#21512;&#26497;&#20540;&#29702;&#35770;&#39537;&#21160;&#30340;&#27169;&#22411;&#65292;&#20998;&#26512;&#20102;&#23612;&#27850;&#23572;30&#24180;&#22303;&#22756;&#38477;&#38632;&#24341;&#21457;&#28369;&#22369;&#30340;&#28165;&#21333;&#65292;&#24182;&#35780;&#20272;&#20102;&#22810;&#20010;&#22320;&#21306;&#30340;&#28369;&#22369;&#28798;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;
The most adopted definition of landslide hazard combines spatial information about landslide location (susceptibility), threat (intensity), and frequency (return period). Only the first two elements are usually considered and estimated when working over vast areas. Even then, separate models constitute the standard, with frequency being rarely investigated. Frequency and intensity are intertwined and depend on each other because larger events occur less frequently and vice versa. However, due to the lack of multi-temporal inventories and joint statistical models, modelling such properties via a unified hazard model has always been challenging and has yet to be attempted. Here, we develop a unified model to estimate landslide hazard at the slope unit level to address such gaps. We employed deep learning, combined with a model motivated by extreme-value theory to analyse an inventory of 30 years of observed rainfall-triggered landslides in Nepal and assess landslide hazard for multiple r
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#8212;&#8212;&#22810;&#27169;&#24577;&#26102;&#38388;&#20851;&#31995;&#22270;&#23398;&#20064;&#65288;MTRGL&#65289;&#65292;&#36890;&#36807;&#23558;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#21644;&#31163;&#25955;&#29305;&#24449;&#32467;&#21512;&#25104;&#19968;&#20010;&#26102;&#38388;&#22270;&#65292;&#24182;&#37319;&#29992;&#35760;&#24518;&#26426;&#21046;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#36776;&#35782;&#23454;&#20307;&#38388;&#30340;&#26102;&#38388;&#30456;&#20851;&#24615;&#65292;&#21462;&#24471;&#20102;&#22312;&#23454;&#35777;&#23454;&#39564;&#20013;&#30340;&#20248;&#24322;&#34920;&#29616;&#12290;&#36825;&#19968;&#30740;&#31350;&#23545;&#20110;&#25552;&#21319;&#33258;&#21160;&#21270;&#37197;&#23545;&#20132;&#26131;&#31574;&#30053;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2401.14199</link><description>&lt;p&gt;
MTRGL&#65306;&#36890;&#36807;&#22810;&#27169;&#24577;&#26102;&#38388;&#20851;&#31995;&#22270;&#23398;&#20064;&#26377;&#25928;&#36776;&#35782;&#26102;&#38388;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
MTRGL:Effective Temporal Correlation Discerning through Multi-modal Temporal Relational Graph Learning. (arXiv:2401.14199v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14199
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#8212;&#8212;&#22810;&#27169;&#24577;&#26102;&#38388;&#20851;&#31995;&#22270;&#23398;&#20064;&#65288;MTRGL&#65289;&#65292;&#36890;&#36807;&#23558;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#21644;&#31163;&#25955;&#29305;&#24449;&#32467;&#21512;&#25104;&#19968;&#20010;&#26102;&#38388;&#22270;&#65292;&#24182;&#37319;&#29992;&#35760;&#24518;&#26426;&#21046;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#36776;&#35782;&#23454;&#20307;&#38388;&#30340;&#26102;&#38388;&#30456;&#20851;&#24615;&#65292;&#21462;&#24471;&#20102;&#22312;&#23454;&#35777;&#23454;&#39564;&#20013;&#30340;&#20248;&#24322;&#34920;&#29616;&#12290;&#36825;&#19968;&#30740;&#31350;&#23545;&#20110;&#25552;&#21319;&#33258;&#21160;&#21270;&#37197;&#23545;&#20132;&#26131;&#31574;&#30053;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#28145;&#24230;&#23398;&#20064;&#19982;&#37329;&#34701;&#24066;&#22330;&#24212;&#29992;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#37325;&#28857;&#20851;&#27880;&#37197;&#23545;&#20132;&#26131;&#12290;&#36825;&#31181;&#24066;&#22330;&#20013;&#24615;&#31574;&#30053;&#23545;&#37327;&#21270;&#37329;&#34701;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#12290;&#37197;&#23545;&#20132;&#26131;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#36776;&#35782;&#23454;&#20307;&#20043;&#38388;&#30340;&#26102;&#38388;&#30456;&#20851;&#24615;&#65292;&#36825;&#35201;&#27714;&#25972;&#21512;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#27169;&#24577;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21363;&#22810;&#27169;&#24577;&#26102;&#38388;&#20851;&#31995;&#22270;&#23398;&#20064;&#65288;MTRGL&#65289;&#12290;MTRGL&#23558;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#21644;&#31163;&#25955;&#29305;&#24449;&#32467;&#21512;&#21040;&#19968;&#20010;&#26102;&#38388;&#22270;&#20013;&#65292;&#24182;&#37319;&#29992;&#20102;&#22522;&#20110;&#35760;&#24518;&#30340;&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#12290;&#36825;&#31181;&#26041;&#27861;&#23558;&#26102;&#38388;&#30456;&#20851;&#24615;&#35782;&#21035;&#37325;&#26032;&#23450;&#20041;&#20026;&#19968;&#20010;&#26102;&#38388;&#22270;&#38142;&#36335;&#39044;&#27979;&#20219;&#21153;&#65292;&#21462;&#24471;&#20102;&#23454;&#35777;&#25104;&#21151;&#12290;&#25105;&#20204;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;MTRGL&#30340;&#21331;&#36234;&#24615;&#33021;&#65292;&#24378;&#35843;&#20854;&#22312;&#20248;&#21270;&#33258;&#21160;&#21270;&#37197;&#23545;&#20132;&#26131;&#31574;&#30053;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we explore the synergy of deep learning and financial market applications, focusing on pair trading. This market-neutral strategy is integral to quantitative finance and is apt for advanced deep-learning techniques. A pivotal challenge in pair trading is discerning temporal correlations among entities, necessitating the integration of diverse data modalities. Addressing this, we introduce a novel framework, Multi-modal Temporal Relation Graph Learning (MTRGL). MTRGL combines time series data and discrete features into a temporal graph and employs a memory-based temporal graph neural network. This approach reframes temporal correlation identification as a temporal graph link prediction task, which has shown empirical success. Our experiments on real-world datasets confirm the superior performance of MTRGL, emphasizing its promise in refining automated pair trading strategies.
&lt;/p&gt;</description></item><item><title>DeepSeek-Coder&#26159;&#19968;&#31995;&#21015;&#24320;&#28304;&#20195;&#30721;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#39640;&#36136;&#37327;&#39033;&#30446;&#32423;&#20195;&#30721;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#37319;&#29992;&#22635;&#31354;&#20219;&#21153;&#21644;16K&#31383;&#21475;&#26469;&#22686;&#24378;&#20195;&#30721;&#29983;&#25104;&#21644;&#22635;&#20805;&#65292;&#19981;&#20165;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#19982;&#24320;&#28304;&#20195;&#30721;&#27169;&#22411;&#21516;&#26679;&#30340;&#26368;&#26032;&#34920;&#29616;&#65292;&#32780;&#19988;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#38381;&#28304;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.14196</link><description>&lt;p&gt;
DeepSeek-Coder: &#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#32534;&#31243;&#30456;&#36935;&#30340;&#26102;&#20505;--&#20195;&#30721;&#26234;&#33021;&#30340;&#23835;&#36215;
&lt;/p&gt;
&lt;p&gt;
DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence. (arXiv:2401.14196v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14196
&lt;/p&gt;
&lt;p&gt;
DeepSeek-Coder&#26159;&#19968;&#31995;&#21015;&#24320;&#28304;&#20195;&#30721;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#39640;&#36136;&#37327;&#39033;&#30446;&#32423;&#20195;&#30721;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#37319;&#29992;&#22635;&#31354;&#20219;&#21153;&#21644;16K&#31383;&#21475;&#26469;&#22686;&#24378;&#20195;&#30721;&#29983;&#25104;&#21644;&#22635;&#20805;&#65292;&#19981;&#20165;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#19982;&#24320;&#28304;&#20195;&#30721;&#27169;&#22411;&#21516;&#26679;&#30340;&#26368;&#26032;&#34920;&#29616;&#65292;&#32780;&#19988;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#38381;&#28304;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24555;&#36895;&#21457;&#23637;&#20026;&#36719;&#20214;&#24320;&#21457;&#20013;&#30340;&#20195;&#30721;&#26234;&#33021;&#24102;&#26469;&#20102;&#38761;&#21629;&#12290;&#28982;&#32780;&#65292;&#38381;&#28304;&#27169;&#22411;&#30340;&#20027;&#23548;&#22320;&#20301;&#38480;&#21046;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;DeepSeek-Coder&#31995;&#21015;&#65292;&#36825;&#26159;&#19968;&#31995;&#21015;&#24320;&#28304;&#20195;&#30721;&#27169;&#22411;&#65292;&#22823;&#23567;&#20174;1.3B&#21040;33B&#65292;&#20174;&#22836;&#24320;&#22987;&#22312;2&#19975;&#20159;&#20010;&#26631;&#35760;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#39640;&#36136;&#37327;&#39033;&#30446;&#32423;&#20195;&#30721;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#65292;&#24182;&#37319;&#29992;&#22635;&#31354;&#20219;&#21153;&#21644;16K&#31383;&#21475;&#26469;&#22686;&#24378;&#20195;&#30721;&#29983;&#25104;&#21644;&#22635;&#20805;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;DeepSeek-Coder&#19981;&#20165;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#19982;&#24320;&#28304;&#20195;&#30721;&#27169;&#22411;&#21516;&#26679;&#30340;&#26368;&#26032;&#34920;&#29616;&#65292;&#32780;&#19988;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;Codex&#21644;GPT-3.5&#31561;&#38381;&#28304;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;DeepSeek-Coder&#27169;&#22411;&#37319;&#29992;&#20102;&#23485;&#26494;&#30340;&#35768;&#21487;&#35777;&#65292;&#26082;&#20801;&#35768;&#30740;&#31350;&#65292;&#20063;&#20801;&#35768;&#26080;&#38480;&#21046;&#30340;&#21830;&#19994;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid development of large language models has revolutionized code intelligence in software development. However, the predominance of closed-source models has restricted extensive research and development. To address this, we introduce the DeepSeek-Coder series, a range of open-source code models with sizes from 1.3B to 33B, trained from scratch on 2 trillion tokens. These models are pre-trained on a high-quality project-level code corpus and employ a fill-in-the-blank task with a 16K window to enhance code generation and infilling. Our extensive evaluations demonstrate that DeepSeek-Coder not only achieves state-of-the-art performance among open-source code models across multiple benchmarks but also surpasses existing closed-source models like Codex and GPT-3.5. Furthermore, DeepSeek-Coder models are under a permissive license that allows for both research and unrestricted commercial use.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;STG-LLM&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#29702;&#35299;&#26102;&#31354;&#25968;&#25454;&#24182;&#36827;&#34892;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;STG-Tokenizer&#23558;&#22797;&#26434;&#30340;&#22270;&#24418;&#25968;&#25454;&#36716;&#21270;&#20026;&#31616;&#27905;&#30340;&#26631;&#35760;&#65292;&#20877;&#36890;&#36807;STG-Adapter&#23558;&#26631;&#35760;&#21270;&#25968;&#25454;&#19982;LLM&#30340;&#29702;&#35299;&#33021;&#21147;&#36827;&#34892;&#36830;&#25509;&#12290;&#36890;&#36807;&#24494;&#35843;&#21442;&#25968;&#65292;STG-LLM&#33021;&#22815;&#26377;&#25928;&#22320;&#25226;&#25569;&#26631;&#35760;&#30340;&#35821;&#20041;&#65292;&#21516;&#26102;&#20445;&#30041;LLM&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;STG-LLM&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.14192</link><description>&lt;p&gt;
&#22914;&#20309;&#35753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29702;&#35299;&#26102;&#31354;&#25968;&#25454;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Can Large Language Models Understand Spatial-Temporal Data?. (arXiv:2401.14192v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14192
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;STG-LLM&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#29702;&#35299;&#26102;&#31354;&#25968;&#25454;&#24182;&#36827;&#34892;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;STG-Tokenizer&#23558;&#22797;&#26434;&#30340;&#22270;&#24418;&#25968;&#25454;&#36716;&#21270;&#20026;&#31616;&#27905;&#30340;&#26631;&#35760;&#65292;&#20877;&#36890;&#36807;STG-Adapter&#23558;&#26631;&#35760;&#21270;&#25968;&#25454;&#19982;LLM&#30340;&#29702;&#35299;&#33021;&#21147;&#36827;&#34892;&#36830;&#25509;&#12290;&#36890;&#36807;&#24494;&#35843;&#21442;&#25968;&#65292;STG-LLM&#33021;&#22815;&#26377;&#25928;&#22320;&#25226;&#25569;&#26631;&#35760;&#30340;&#35821;&#20041;&#65292;&#21516;&#26102;&#20445;&#30041;LLM&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;STG-LLM&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#31561;&#20219;&#21153;&#20013;&#21344;&#25454;&#20027;&#23548;&#22320;&#20301;&#65292;&#20294;&#21033;&#29992;&#23427;&#20204;&#30340;&#33021;&#21147;&#36827;&#34892;&#26102;&#31354;&#39044;&#27979;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26102;&#24207;&#25991;&#26412;&#19982;&#22797;&#26434;&#30340;&#26102;&#31354;&#25968;&#25454;&#20043;&#38388;&#30340;&#24046;&#24322;&#38459;&#30861;&#20102;&#35813;&#24212;&#29992;&#30340;&#23454;&#29616;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;STG-LLM&#65292;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#20026;LLM&#36171;&#20104;&#20102;&#26102;&#31354;&#39044;&#27979;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#20197;&#19979;&#26041;&#24335;&#35299;&#20915;&#25968;&#25454;&#19981;&#21305;&#37197;&#30340;&#38382;&#39064;&#65306;1&#65289;STG-Tokenizer&#65306;&#36825;&#20010;&#26102;&#31354;&#22270;&#24418;&#26631;&#35760;&#22120;&#23558;&#22797;&#26434;&#30340;&#22270;&#24418;&#25968;&#25454;&#36716;&#21270;&#20026;&#31616;&#27905;&#30340;&#26631;&#35760;&#65292;&#25429;&#25417;&#20102;&#31354;&#38388;&#21644;&#26102;&#38388;&#20851;&#31995;&#65307;2&#65289;STG-Adapter&#65306;&#36825;&#20010;&#31934;&#31616;&#30340;&#36866;&#37197;&#22120;&#30001;&#32447;&#24615;&#32534;&#30721;&#21644;&#35299;&#30721;&#23618;&#32452;&#25104;&#65292;&#22635;&#34917;&#20102;&#26631;&#35760;&#21270;&#25968;&#25454;&#21644;LLM&#29702;&#35299;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#36890;&#36807;&#20165;&#24494;&#35843;&#19968;&#23567;&#37096;&#20998;&#21442;&#25968;&#65292;&#23427;&#21487;&#20197;&#26377;&#25928;&#22320;&#25226;&#25569;STG-Tokenizer&#29983;&#25104;&#30340;&#26631;&#35760;&#30340;&#35821;&#20041;&#65292;&#21516;&#26102;&#20445;&#30041;LLM&#30340;&#21407;&#22987;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#12290;&#36890;&#36807;&#22312;&#22810;&#31181;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;STG-LLM&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Large Language Models (LLMs) dominate tasks like natural language processing and computer vision, harnessing their power for spatial-temporal forecasting remains challenging. The disparity between sequential text and complex spatial-temporal data hinders this application. To address this issue, this paper introduces STG-LLM, an innovative approach empowering LLMs for spatial-temporal forecasting. We tackle the data mismatch by proposing: 1) STG-Tokenizer: This spatial-temporal graph tokenizer transforms intricate graph data into concise tokens capturing both spatial and temporal relationships; 2) STG-Adapter: This minimalistic adapter, consisting of linear encoding and decoding layers, bridges the gap between tokenized data and LLM comprehension. By fine-tuning only a small set of parameters, it can effectively grasp the semantics of tokens generated by STG-Tokenizer, while preserving the original natural language understanding capabilities of LLMs. Extensive experiments on diver
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#21451;&#22909;&#25915;&#20987;&#8221;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20256;&#36755;&#21069;&#23545;&#30721;&#23383;&#36827;&#34892;&#36731;&#24494;&#25200;&#21160;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#32416;&#38169;&#20449;&#36947;&#32534;&#30721;&#30340;&#24615;&#33021;&#65292;&#25552;&#39640;&#20102;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.14184</link><description>&lt;p&gt;
&#36890;&#36807;&#21451;&#22909;&#25915;&#20987;&#26469;&#25552;&#39640;&#20449;&#36947;&#32534;&#30721;&#30340;&#21487;&#38752;&#24615;
&lt;/p&gt;
&lt;p&gt;
Friendly Attacks to Improve Channel Coding Reliability. (arXiv:2401.14184v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14184
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#21451;&#22909;&#25915;&#20987;&#8221;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20256;&#36755;&#21069;&#23545;&#30721;&#23383;&#36827;&#34892;&#36731;&#24494;&#25200;&#21160;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#32416;&#38169;&#20449;&#36947;&#32534;&#30721;&#30340;&#24615;&#33021;&#65292;&#25552;&#39640;&#20102;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#21451;&#22909;&#25915;&#20987;&#8221;&#30340;&#26032;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#32416;&#38169;&#20449;&#36947;&#32534;&#30721;&#30340;&#24615;&#33021;&#12290;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#27010;&#24565;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#23545;&#31070;&#32463;&#32593;&#32476;&#36755;&#20837;&#36827;&#34892;&#36731;&#24494;&#25200;&#21160;&#30340;&#24819;&#27861;&#65292;&#23545;&#32593;&#32476;&#24615;&#33021;&#20135;&#29983;&#20102;&#37325;&#22823;&#24433;&#21709;&#12290;&#36890;&#36807;&#22312;&#20256;&#36755;&#21069;&#21521;&#22266;&#23450;&#28857;&#35843;&#21046;&#30340;&#30721;&#23383;&#24341;&#20837;&#23567;&#30340;&#25200;&#21160;&#65292;&#25105;&#20204;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#35299;&#30721;&#22120;&#30340;&#24615;&#33021;&#65292;&#32780;&#19981;&#36829;&#21453;&#36755;&#20837;&#21151;&#29575;&#32422;&#26463;&#12290;&#25200;&#21160;&#35774;&#35745;&#30001;&#20462;&#25913;&#36807;&#30340;&#36845;&#20195;&#24555;&#36895;&#26799;&#24230;&#26041;&#27861;&#23436;&#25104;&#12290;&#26412;&#30740;&#31350;&#32771;&#23519;&#20102;&#36866;&#29992;&#20110;&#35745;&#31639;&#26799;&#24230;&#20197;&#33719;&#24471;&#25152;&#38656;&#25200;&#21160;&#30340;&#21508;&#31181;&#35299;&#30721;&#22120;&#26550;&#26500;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#22312;LDPC&#30721;&#20013;&#30340;&#32622;&#20449;&#20256;&#25773;&#65288;BP&#65289;&#65292;&#22312;&#26497;&#21270;&#30721;&#20013;&#30340;&#35823;&#24046;&#32416;&#27491;&#30721;&#21464;&#25442;&#22120;&#12289;BP&#21644;&#31070;&#32463;BP&#65288;NBP&#65289;&#65292;&#20197;&#21450;&#22312;&#21367;&#31215;&#30721;&#20013;&#30340;&#31070;&#32463;BCJR&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#21451;&#22909;&#25915;&#20987;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel approach called "friendly attack" aimed at enhancing the performance of error correction channel codes. Inspired by the concept of adversarial attacks, our method leverages the idea of introducing slight perturbations to the neural network input, resulting in a substantial impact on the network's performance. By introducing small perturbations to fixed-point modulated codewords before transmission, we effectively improve the decoder's performance without violating the input power constraint. The perturbation design is accomplished by a modified iterative fast gradient method. This study investigates various decoder architectures suitable for computing gradients to obtain the desired perturbations. Specifically, we consider belief propagation (BP) for LDPC codes; the error correcting code transformer, BP and neural BP (NBP) for polar codes, and neural BCJR for convolutional codes. We demonstrate that the proposed friendly attack method can improve the relia
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20174;&#29305;&#24449;&#35270;&#35282;&#35299;&#20915;&#20102;&#22270;&#24418;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#32467;&#26500;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#65292;&#36890;&#36807;&#25269;&#25239;&#24322;&#24120;&#33410;&#28857;&#30340;&#39640;&#24322;&#36136;&#24615;&#65292;&#21516;&#26102;&#20174;&#21516;&#36136;&#37051;&#23621;&#20013;&#21463;&#30410;&#20110;&#27491;&#24120;&#33410;&#28857;&#30340;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2401.14155</link><description>&lt;p&gt;
&#32531;&#35299;&#22270;&#24418;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#32467;&#26500;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Alleviating Structural Distribution Shift in Graph Anomaly Detection. (arXiv:2401.14155v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14155
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20174;&#29305;&#24449;&#35270;&#35282;&#35299;&#20915;&#20102;&#22270;&#24418;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#32467;&#26500;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#65292;&#36890;&#36807;&#25269;&#25239;&#24322;&#24120;&#33410;&#28857;&#30340;&#39640;&#24322;&#36136;&#24615;&#65292;&#21516;&#26102;&#20174;&#21516;&#36136;&#37051;&#23621;&#20013;&#21463;&#30410;&#20110;&#27491;&#24120;&#33410;&#28857;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#24418;&#24322;&#24120;&#26816;&#27979;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20108;&#20998;&#31867;&#38382;&#39064;&#65292;&#30001;&#20110;&#24322;&#24120;&#33410;&#28857;&#21644;&#27491;&#24120;&#33410;&#28857;&#20043;&#38388;&#30340;&#32467;&#26500;&#20998;&#24067;&#19981;&#21516;&#65292;&#20351;&#24471;&#35813;&#38382;&#39064;&#21464;&#24471;&#22256;&#38590;&#12290;&#24322;&#24120;&#33410;&#28857;&#26159;&#23569;&#25968;&#65292;&#22240;&#27492;&#19982;&#27491;&#24120;&#33410;&#28857;&#30456;&#27604;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#24322;&#36136;&#24615;&#21644;&#36739;&#20302;&#30340;&#21516;&#36136;&#24615;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#21508;&#31181;&#26102;&#38388;&#22240;&#32032;&#21644;&#20154;&#31867;&#19987;&#23478;&#30340;&#27880;&#37322;&#20559;&#22909;&#65292;&#24322;&#36136;&#24615;&#21644;&#21516;&#36136;&#24615;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#20043;&#38388;&#21487;&#33021;&#20250;&#21457;&#29983;&#21464;&#21270;&#65292;&#36825;&#22312;&#26412;&#25991;&#20013;&#34987;&#31216;&#20026;&#32467;&#26500;&#20998;&#24067;&#20559;&#31227;&#65288;SDS&#65289;&#12290;&#20027;&#27969;&#26041;&#27861;&#26159;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26500;&#24314;&#30340;&#65292;&#36890;&#36807;&#32858;&#21512;&#21516;&#36136;&#37051;&#23621;&#26377;&#21033;&#20110;&#27491;&#24120;&#33410;&#28857;&#30340;&#20998;&#31867;&#65292;&#20294;&#24573;&#35270;&#20102;&#24322;&#24120;&#33410;&#28857;&#30340;SDS&#38382;&#39064;&#65292;&#23548;&#33268;&#27867;&#21270;&#33021;&#21147;&#24046;&#12290;&#26412;&#30740;&#31350;&#20174;&#29305;&#24449;&#35270;&#35282;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;SDS&#30340;&#31243;&#24230;&#22312;&#24322;&#24120;&#33410;&#28857;&#21644;&#27491;&#24120;&#33410;&#28857;&#20043;&#38388;&#26377;&#25152;&#19981;&#21516;&#12290;&#22240;&#27492;&#65292;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#20851;&#38190;&#22312;&#20110;&#23545;&#24322;&#24120;&#33410;&#28857;&#25269;&#25239;&#39640;&#24322;&#36136;&#24615;&#30340;&#21516;&#26102;&#65292;&#20174;&#21516;&#36136;&#37051;&#23621;&#20013;&#21463;&#30410;&#20110;&#27491;&#24120;&#33410;&#28857;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph anomaly detection (GAD) is a challenging binary classification problem due to its different structural distribution between anomalies and normal nodes -- abnormal nodes are a minority, therefore holding high heterophily and low homophily compared to normal nodes. Furthermore, due to various time factors and the annotation preferences of human experts, the heterophily and homophily can change across training and testing data, which is called structural distribution shift (SDS) in this paper. The mainstream methods are built on graph neural networks (GNNs), benefiting the classification of normals from aggregating homophilous neighbors, yet ignoring the SDS issue for anomalies and suffering from poor generalization.  This work solves the problem from a feature view. We observe that the degree of SDS varies between anomalies and normal nodes. Hence to address the issue, the key lies in resisting high heterophily for anomalies meanwhile benefiting the learning of normals from homophi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#20915;&#31574;&#26234;&#33021;&#20307;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#19982;&#20855;&#36523;&#29615;&#22659;&#39640;&#25928;&#20114;&#21160;&#26469;&#35299;&#20915;LLMs&#19982;&#29615;&#22659;&#20043;&#38388;&#30693;&#35782;&#19981;&#23545;&#40784;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#26597;&#35810;LLMs&#30340;&#32852;&#21512;&#27010;&#29575;&#65292;&#24418;&#25104;&#34892;&#20026;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#20004;&#31181;&#24402;&#19968;&#21270;&#26041;&#27861;&#21644;&#22235;&#20010;&#25552;&#31034;&#35774;&#35745;&#21407;&#21017;&#25552;&#39640;&#31574;&#30053;&#30340;&#31283;&#23450;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#35774;&#35745;&#21442;&#25968;&#39640;&#25928;&#30340;&#35757;&#32451;&#26550;&#26500;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.14151</link><description>&lt;p&gt;
&#30495;&#30693;&#26469;&#28304;&#20110;&#23454;&#36341;&#65306;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#20351;LLMs&#19982;&#20855;&#36523;&#29615;&#22659;&#23545;&#40784;&#30340;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
True Knowledge Comes from Practice: Aligning LLMs with Embodied Environments via Reinforcement Learning. (arXiv:2401.14151v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14151
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#20915;&#31574;&#26234;&#33021;&#20307;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#19982;&#20855;&#36523;&#29615;&#22659;&#39640;&#25928;&#20114;&#21160;&#26469;&#35299;&#20915;LLMs&#19982;&#29615;&#22659;&#20043;&#38388;&#30693;&#35782;&#19981;&#23545;&#40784;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#26597;&#35810;LLMs&#30340;&#32852;&#21512;&#27010;&#29575;&#65292;&#24418;&#25104;&#34892;&#20026;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#20004;&#31181;&#24402;&#19968;&#21270;&#26041;&#27861;&#21644;&#22235;&#20010;&#25552;&#31034;&#35774;&#35745;&#21407;&#21017;&#25552;&#39640;&#31574;&#30053;&#30340;&#31283;&#23450;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#35774;&#35745;&#21442;&#25968;&#39640;&#25928;&#30340;&#35757;&#32451;&#26550;&#26500;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#20247;&#22810;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#34920;&#29616;&#65292;&#20294;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35299;&#20915;&#31616;&#21333;&#30340;&#20915;&#31574;&#20219;&#21153;&#19978;&#32463;&#24120;&#22833;&#36133;&#65292;&#21407;&#22240;&#26159;LLMs&#20013;&#30340;&#30693;&#35782;&#19982;&#29615;&#22659;&#19981;&#23545;&#40784;&#12290;&#30456;&#21453;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26234;&#33021;&#20307;&#20174;&#38646;&#24320;&#22987;&#23398;&#20064;&#31574;&#30053;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#22987;&#32456;&#19982;&#29615;&#22659;&#20445;&#25345;&#19968;&#33268;&#65292;&#20294;&#38590;&#20197;&#23558;&#20808;&#21069;&#30340;&#30693;&#35782;&#25972;&#21512;&#21040;&#20854;&#20013;&#20197;&#36827;&#34892;&#26377;&#25928;&#30340;&#25506;&#32034;&#12290;&#20026;&#20102;&#32553;&#23567;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TWOSOME&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22312;&#32447;&#26694;&#26550;&#65292;&#21033;&#29992;LLMs&#20316;&#20026;&#20915;&#31574;&#26234;&#33021;&#20307;&#65292;&#36890;&#36807;RL&#19982;&#20855;&#36523;&#29615;&#22659;&#39640;&#25928;&#20114;&#21160;&#24182;&#23454;&#29616;&#23545;&#40784;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#20934;&#22791;&#22909;&#30340;&#25968;&#25454;&#38598;&#25110;&#29615;&#22659;&#30340;&#20808;&#21069;&#30693;&#35782;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;LLMs&#26597;&#35810;&#27599;&#20010;&#26377;&#25928;&#21160;&#20316;&#30340;&#32852;&#21512;&#27010;&#29575;&#20197;&#24418;&#25104;&#34892;&#20026;&#31574;&#30053;&#12290;&#28982;&#21518;&#65292;&#20026;&#20102;&#22686;&#24378;&#31574;&#30053;&#30340;&#31283;&#23450;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#24402;&#19968;&#21270;&#26041;&#27861;&#65292;&#24182;&#24635;&#32467;&#20102;&#22235;&#20010;&#25552;&#31034;&#35774;&#35745;&#21407;&#21017;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21442;&#25968;&#39640;&#25928;&#30340;&#35757;&#32451;&#26550;&#26500;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#34892;&#20026;&#35780;&#20272;&#21644;&#36873;&#25321;&#31639;&#27861;&#26469;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the impressive performance across numerous tasks, large language models (LLMs) often fail in solving simple decision-making tasks due to the misalignment of the knowledge in LLMs with environments. On the contrary, reinforcement learning (RL) agents learn policies from scratch, which makes them always align with environments but difficult to incorporate prior knowledge for efficient explorations. To narrow the gap, we propose TWOSOME, a novel general online framework that deploys LLMs as decision-making agents to efficiently interact and align with embodied environments via RL without requiring any prepared datasets or prior knowledge of the environments. Firstly, we query the joint probabilities of each valid action with LLMs to form behavior policies. Then, to enhance the stability and robustness of the policies, we propose two normalization methods and summarize four prompt design principles. Finally, we design a novel parameter-efficient training architecture where the acto
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#33021;&#37327;&#30340;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#32479;&#19968;&#20102;&#39044;&#27979;&#12289;&#27010;&#24565;&#24178;&#39044;&#21644;&#26465;&#20214;&#35299;&#37322;&#30340;&#21151;&#33021;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#39640;&#38454;&#38750;&#32447;&#24615;&#30456;&#20114;&#20316;&#29992;&#21644;&#22797;&#26434;&#26465;&#20214;&#20381;&#36182;&#20851;&#31995;&#19978;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2401.14142</link><description>&lt;p&gt;
&#22522;&#20110;&#33021;&#37327;&#30340;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65306;&#32479;&#19968;&#39044;&#27979;&#12289;&#27010;&#24565;&#24178;&#39044;&#21644;&#26465;&#20214;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Energy-Based Concept Bottleneck Models: Unifying Prediction, Concept Intervention, and Conditional Interpretations. (arXiv:2401.14142v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14142
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#33021;&#37327;&#30340;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#32479;&#19968;&#20102;&#39044;&#27979;&#12289;&#27010;&#24565;&#24178;&#39044;&#21644;&#26465;&#20214;&#35299;&#37322;&#30340;&#21151;&#33021;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#39640;&#38454;&#38750;&#32447;&#24615;&#30456;&#20114;&#20316;&#29992;&#21644;&#22797;&#26434;&#26465;&#20214;&#20381;&#36182;&#20851;&#31995;&#19978;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#26041;&#27861;&#65292;&#22914;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411; (CBM)&#65292;&#22312;&#20026;&#40657;&#30418;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#22522;&#20110;&#27010;&#24565;&#30340;&#35299;&#37322;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#23427;&#20204;&#36890;&#24120;&#36890;&#36807;&#22312;&#32473;&#23450;&#36755;&#20837;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#27010;&#24565;&#65292;&#28982;&#21518;&#22312;&#32473;&#23450;&#39044;&#27979;&#30340;&#27010;&#24565;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#26368;&#32456;&#30340;&#31867;&#21035;&#26631;&#31614;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#32463;&#24120;&#26080;&#27861;&#25429;&#25417;&#21040;&#27010;&#24565;&#20043;&#38388;&#30340;&#39640;&#38454;&#38750;&#32447;&#24615;&#30456;&#20114;&#20316;&#29992;&#65292;&#20363;&#22914;&#32416;&#27491;&#19968;&#20010;&#39044;&#27979;&#30340;&#27010;&#24565;&#65288;&#20363;&#22914;&#8220;&#40644;&#33394;&#33016;&#37096;&#8221;&#65289;&#26080;&#27861;&#24110;&#21161;&#32416;&#27491;&#39640;&#24230;&#30456;&#20851;&#30340;&#27010;&#24565;&#65288;&#20363;&#22914;&#8220;&#40644;&#33394;&#33145;&#37096;&#8221;&#65289;&#65292;&#23548;&#33268;&#26368;&#32456;&#20934;&#30830;&#29575;&#19981;&#29702;&#24819;&#65307;&#23427;&#20204;&#26080;&#27861;&#33258;&#28982;&#22320;&#37327;&#21270;&#19981;&#21516;&#27010;&#24565;&#21644;&#31867;&#21035;&#26631;&#31614;&#20043;&#38388;&#30340;&#22797;&#26434;&#26465;&#20214;&#20381;&#36182;&#20851;&#31995;&#65288;&#20363;&#22914;&#23545;&#20110;&#19968;&#20010;&#24102;&#26377;&#31867;&#21035;&#26631;&#31614;&#8220;Kentucky Warbler&#8221;&#21644;&#27010;&#24565;&#8220;&#40657;&#33394;&#22068;&#24052;&#8221;&#30340;&#22270;&#20687;&#65292;&#27169;&#22411;&#33021;&#22815;&#27491;&#30830;&#39044;&#27979;&#21478;&#19968;&#20010;&#27010;&#24565;&#8220;&#40657;&#33394;&#20896;&#8221;&#30340;&#27010;&#29575;&#26159;&#22810;&#23569;&#65289;&#65292;&#22240;&#27492;&#26080;&#27861;&#25552;&#20379;&#20851;&#20110;&#40657;&#30418;&#27169;&#22411;&#24037;&#20316;&#21407;&#29702;&#26356;&#28145;&#23618;&#27425;&#30340;&#27934;&#23519;&#12290;&#38024;&#23545;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#33021;&#37327;&#30340;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65288;Energy-based Concept Bottleneck Models&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing methods, such as concept bottleneck models (CBMs), have been successful in providing concept-based interpretations for black-box deep learning models. They typically work by predicting concepts given the input and then predicting the final class label given the predicted concepts. However, (1) they often fail to capture the high-order, nonlinear interaction between concepts, e.g., correcting a predicted concept (e.g., "yellow breast") does not help correct highly correlated concepts (e.g., "yellow belly"), leading to suboptimal final accuracy; (2) they cannot naturally quantify the complex conditional dependencies between different concepts and class labels (e.g., for an image with the class label "Kentucky Warbler" and a concept "black bill", what is the probability that the model correctly predicts another concept "black crown"), therefore failing to provide deeper insight into how a black-box model works. In response to these limitations, we propose Energy-based Concept Bot
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#21360;&#24230;&#21360;&#22320;&#25991;&#27861;&#24459;&#25991;&#20214;&#19978;&#36827;&#34892;&#20108;&#20803;&#20445;&#37322;&#21028;&#26029;&#20998;&#31867;&#65292;&#21462;&#24471;&#20102;93&#65285;&#30340;&#20934;&#30830;&#29575;&#65292;&#25913;&#36827;&#20102;&#20043;&#21069;&#30340;&#20934;&#30830;&#29575;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2401.14135</link><description>&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#23454;&#29616;&#20108;&#20803;&#20445;&#37322;&#21028;&#26029;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Convolutional Neural Networks can achieve binary bail judgement classification. (arXiv:2401.14135v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14135
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#21360;&#24230;&#21360;&#22320;&#25991;&#27861;&#24459;&#25991;&#20214;&#19978;&#36827;&#34892;&#20108;&#20803;&#20445;&#37322;&#21028;&#26029;&#20998;&#31867;&#65292;&#21462;&#24471;&#20102;93&#65285;&#30340;&#20934;&#30830;&#29575;&#65292;&#25913;&#36827;&#20102;&#20043;&#21069;&#30340;&#20934;&#30830;&#29575;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21360;&#24230;&#27861;&#24459;&#39046;&#22495;&#20013;&#32570;&#20047;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#23454;&#26045;&#65292;&#35813;&#39046;&#22495;&#30340;&#20219;&#20309;&#30740;&#31350;&#36890;&#24120;&#26159;&#22522;&#20110;&#33521;&#35821;&#25968;&#25454;&#21644;&#39640;&#31561;&#27861;&#38498;&#30340;&#25968;&#25454;&#12290;&#24573;&#30053;&#20102;&#21360;&#24230;&#20302;&#32423;&#27861;&#38498;&#21644;&#19981;&#21516;&#22320;&#21306;&#35821;&#35328;&#30340;&#25968;&#25454;&#12290;&#26412;&#25991;&#22312;&#19968;&#32452;&#21360;&#22320;&#25991;&#27861;&#24459;&#25991;&#20214;&#19978;&#37096;&#32626;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#26550;&#26500;&#12290;&#25105;&#20204;&#20351;&#29992;CNN&#27169;&#22411;&#36827;&#34892;&#20445;&#37322;&#39044;&#27979;&#20219;&#21153;&#65292;&#21462;&#24471;&#20102;93&#65285;&#30340;&#24635;&#20307;&#20934;&#30830;&#29575;&#65292;&#36825;&#26159;&#23545;&#21360;&#24230;&#21271;&#26041;&#37030;20&#20010;&#22320;&#21306;&#25968;&#25454;&#30340;Kapoor&#31561;&#20154;&#65288;2022&#65289;&#35774;&#23450;&#30340;&#20934;&#30830;&#29575;&#22522;&#20934;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is an evident lack of implementation of Machine Learning (ML) in the legal domain in India, and any research that does take place in this domain is usually based on data from the higher courts of law and works with English data. The lower courts and data from the different regional languages of India are often overlooked. In this paper, we deploy a Convolutional Neural Network (CNN) architecture on a corpus of Hindi legal documents. We perform a bail Prediction task with the help of a CNN model and achieve an overall accuracy of 93\% which is an improvement on the benchmark accuracy, set by Kapoor et al. (2022), albeit in data from 20 districts of the Indian state of Uttar Pradesh.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31561;&#21464;&#27969;&#24418;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#20960;&#20309;&#26694;&#26550;&#65292;&#21033;&#29992;&#24494;&#20998;&#19981;&#21464;&#37327;&#21644;&#23884;&#20837;&#31561;&#21464;&#27969;&#24418;&#20013;&#30340;&#22686;&#24191;&#24418;&#24335;&#36827;&#34892;&#24314;&#27169;&#21644;&#36924;&#36817;&#12290;</title><link>http://arxiv.org/abs/2401.14131</link><description>&lt;p&gt;
&#31561;&#21464;&#27969;&#24418;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#19982;&#24494;&#20998;&#19981;&#21464;&#37327;
&lt;/p&gt;
&lt;p&gt;
Equivariant Manifold Neural ODEs and Differential Invariants. (arXiv:2401.14131v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14131
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31561;&#21464;&#27969;&#24418;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#20960;&#20309;&#26694;&#26550;&#65292;&#21033;&#29992;&#24494;&#20998;&#19981;&#21464;&#37327;&#21644;&#23884;&#20837;&#31561;&#21464;&#27969;&#24418;&#20013;&#30340;&#22686;&#24191;&#24418;&#24335;&#36827;&#34892;&#24314;&#27169;&#21644;&#36924;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20026;&#31561;&#21464;&#27969;&#24418;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;NODE&#65289;&#24320;&#21457;&#20102;&#19968;&#31181;&#26126;&#26174;&#20960;&#20309;&#30340;&#26694;&#26550;&#65292;&#24182;&#20351;&#29992;&#23427;&#26469;&#20998;&#26512;&#23427;&#20204;&#22312;&#23545;&#31216;&#25968;&#25454;&#24314;&#27169;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#32771;&#34385;Lie&#32676;G&#22312;&#20809;&#28369;&#27969;&#24418;M&#19978;&#30340;&#20316;&#29992;&#65292;&#24182;&#24314;&#31435;&#20102;&#21521;&#37327;&#22330;&#30340;&#31561;&#21464;&#24615;&#12289;&#30456;&#24212;&#26607;&#35199;&#38382;&#39064;&#30340;&#23545;&#31216;&#24615;&#20197;&#21450;&#20851;&#32852;&#30340;NODE&#30340;&#31561;&#21464;&#24615;&#20043;&#38388;&#30340;&#31561;&#20215;&#20851;&#31995;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#20110;G&#22312;M&#19978;&#20316;&#29992;&#30340;&#24494;&#20998;&#19981;&#21464;&#37327;&#30340;&#31561;&#21464;NODE&#26032;&#24418;&#24335;&#65292;&#22522;&#20110;Lie&#29702;&#35770;&#26469;&#25551;&#36848;&#24494;&#20998;&#26041;&#31243;&#23545;&#31216;&#24615;&#30340;&#21442;&#25968;&#21270;&#26041;&#27861;&#65292;&#26082;&#23545;M&#20063;&#23545;G&#37117;&#26159;&#19981;&#21487;&#30693;&#30340;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#36890;&#36807;&#23884;&#20837;&#31561;&#21464;&#27969;&#20013;&#26500;&#36896;&#20102;&#22686;&#24191;&#27969;&#24418;NODE&#65292;&#24182;&#35777;&#26126;&#23427;&#20204;&#26159;&#20219;&#20309;&#36335;&#24452;&#36830;&#36890;M&#19978;&#31561;&#21464;&#24494;&#20998;&#21516;&#32986;&#30340;&#36890;&#29992;&#36924;&#36817;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we develop a manifestly geometric framework for equivariant manifold neural ordinary differential equations (NODEs), and use it to analyse their modelling capabilities for symmetric data. First, we consider the action of a Lie group $G$ on a smooth manifold $M$ and establish the equivalence between equivariance of vector fields, symmetries of the corresponding Cauchy problems, and equivariance of the associated NODEs. We also propose a novel formulation of the equivariant NODEs in terms of the differential invariants of the action of $G$ on $M$, based on Lie theory for symmetries of differential equations, which provides an efficient parameterisation of the space of equivariant vector fields in a way that is agnostic to both the manifold $M$ and the symmetry group $G$. Second, we construct augmented manifold NODEs, through embeddings into equivariant flows, and show that they are universal approximators of equivariant diffeomorphisms on any path-connected $M$. Furthermore
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#39640;&#25928;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#20998;&#31867;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;ResNet&#32593;&#32476;&#20316;&#20026;&#39592;&#24178;&#65292;&#24182;&#32467;&#21512;&#21518;&#34701;&#21512;&#31639;&#27861;&#21644;&#27880;&#24847;&#26426;&#21046;&#65292;&#20934;&#30830;&#25429;&#25417;&#20102; MRI &#22270;&#20687;&#20013;&#30340;&#37325;&#35201;&#29305;&#24449;&#65292;&#20351;&#35786;&#26029;&#20934;&#30830;&#24615;&#24471;&#21040;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2401.14130</link><description>&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#39640;&#25928;&#20998;&#31867;&#31639;&#27861;&#29992;&#20110;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30340;3D MRI&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
Attention-based Efficient Classification for 3D MRI Image of Alzheimer's Disease. (arXiv:2401.14130v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14130
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#39640;&#25928;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#20998;&#31867;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;ResNet&#32593;&#32476;&#20316;&#20026;&#39592;&#24178;&#65292;&#24182;&#32467;&#21512;&#21518;&#34701;&#21512;&#31639;&#27861;&#21644;&#27880;&#24847;&#26426;&#21046;&#65292;&#20934;&#30830;&#25429;&#25417;&#20102; MRI &#22270;&#20687;&#20013;&#30340;&#37325;&#35201;&#29305;&#24449;&#65292;&#20351;&#35786;&#26029;&#20934;&#30830;&#24615;&#24471;&#21040;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26089;&#26399;&#35786;&#26029;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#20854;&#32454;&#24494;&#22797;&#26434;&#30340;&#20020;&#24202;&#30151;&#29366;&#12290;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#65292;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#36741;&#21161;&#30340;&#21307;&#23398;&#35786;&#26029;&#21644;&#22270;&#20687;&#35782;&#21035;&#25216;&#26415;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#35838;&#39064;&#12290;&#29305;&#24449;&#24517;&#39035;&#20934;&#30830;&#25429;&#25417;&#35299;&#21078;&#32467;&#26500;&#30340;&#20027;&#35201;&#21464;&#21270;&#65292;&#28982;&#32780;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#26159;&#26114;&#36149;&#19988;&#32791;&#26102;&#30340;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#22411;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#26816;&#27979;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;ResNet&#32593;&#32476;&#20316;&#20026;&#39592;&#24178;&#65292;&#32467;&#21512;&#21518;&#34701;&#21512;&#31639;&#27861;&#21644;&#27880;&#24847;&#26426;&#21046;&#26469;&#22788;&#29702;3D&#21307;&#23398;&#22270;&#20687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#37319;&#29992;&#30340;2D&#34701;&#21512;&#31639;&#27861;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#35757;&#32451;&#25928;&#29575;&#12290;&#24341;&#20837;&#30340;&#27880;&#24847;&#26426;&#21046;&#20934;&#30830;&#22320;&#23545;&#22270;&#20687;&#20013;&#30340;&#37325;&#35201;&#21306;&#22495;&#36827;&#34892;&#21152;&#26435;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#35786;&#26029;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Early diagnosis of Alzheimer Diagnostics (AD) is a challenging task due to its subtle and complex clinical symptoms. Deep learning-assisted medical diagnosis using image recognition techniques has become an important research topic in this field. The features have to accurately capture main variations of anatomical brain structures. However, time-consuming is expensive for feature extraction by deep learning training. This study proposes a novel Alzheimer's disease detection model based on Convolutional Neural Networks. The model utilizes a pre-trained ResNet network as the backbone, incorporating post-fusion algorithm for 3D medical images and attention mechanisms. The experimental results indicate that the employed 2D fusion algorithm effectively improves the model's training expense. And the introduced attention mechanism accurately weights important regions in images, further enhancing the model's diagnostic accuracy.
&lt;/p&gt;</description></item><item><title>FP6-LLM&#25552;&#20986;&#20102;&#19968;&#31181;&#25903;&#25345;&#20845;&#20301;&#37327;&#21270;&#30340;GPU&#31639;&#27861;-&#31995;&#32479;&#21327;&#21516;&#35774;&#35745;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25512;&#26029;&#25104;&#26412;&#21644;&#27169;&#22411;&#36136;&#37327;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2401.14112</link><description>&lt;p&gt;
FP6-LLM: &#36890;&#36807;FP6&#20013;&#24515;&#31639;&#27861;-&#31995;&#32479;&#21327;&#21516;&#35774;&#35745;&#39640;&#25928;&#25552;&#20379;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
FP6-LLM: Efficiently Serving Large Language Models Through FP6-Centric Algorithm-System Co-Design. (arXiv:2401.14112v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14112
&lt;/p&gt;
&lt;p&gt;
FP6-LLM&#25552;&#20986;&#20102;&#19968;&#31181;&#25903;&#25345;&#20845;&#20301;&#37327;&#21270;&#30340;GPU&#31639;&#27861;-&#31995;&#32479;&#21327;&#21516;&#35774;&#35745;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25512;&#26029;&#25104;&#26412;&#21644;&#27169;&#22411;&#36136;&#37327;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20845;&#20301;&#37327;&#21270;&#65288;FP6&#65289;&#21487;&#20197;&#26377;&#25928;&#22320;&#20943;&#23567;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#22823;&#23567;&#65292;&#24182;&#22312;&#19981;&#21516;&#24212;&#29992;&#20013;&#20445;&#25345;&#27169;&#22411;&#36136;&#37327;&#30340;&#19968;&#33268;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#31995;&#32479;&#19981;&#25552;&#20379;FP6&#37327;&#21270;&#30340;&#24352;&#37327;&#26680;&#24515;&#25903;&#25345;&#65292;&#24182;&#19988;&#22312;LLM&#25512;&#26029;&#36807;&#31243;&#20013;&#24456;&#38590;&#23454;&#29616;&#23454;&#38469;&#24615;&#33021;&#25913;&#36827;&#12290;&#30001;&#20110;&#65288;1&#65289;&#27169;&#22411;&#26435;&#37325;&#20855;&#26377;&#19981;&#35268;&#21017;&#20301;&#23485;&#30340;&#19981;&#21451;&#22909;&#20869;&#23384;&#35775;&#38382;&#21644;&#65288;2&#65289;&#26435;&#37325;&#21435;&#37327;&#21270;&#30340;&#39640;&#36816;&#34892;&#26102;&#24320;&#38144;&#65292;&#25903;&#25345;&#22312;GPU&#19978;&#36827;&#34892;FP6&#37327;&#21270;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TC-FPx&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20855;&#26377;&#32479;&#19968;&#24352;&#37327;&#26680;&#24515;&#25903;&#25345;&#30340;&#28014;&#28857;&#26435;&#37325;&#30340;&#23436;&#25972;GPU&#20869;&#26680;&#35774;&#35745;&#26041;&#26696;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#37327;&#21270;&#20301;&#23485;&#12290;&#25105;&#20204;&#23558;TC-FPx&#20869;&#26680;&#38598;&#25104;&#21040;&#29616;&#26377;&#25512;&#26029;&#31995;&#32479;&#20013;&#65292;&#25552;&#20379;&#20102;&#26032;&#30340;&#31471;&#21040;&#31471;&#25903;&#25345;&#65288;&#31216;&#20026;FP6-LLM&#65289;&#29992;&#20110;&#37327;&#21270;LLM&#25512;&#26029;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#25512;&#26029;&#25104;&#26412;&#21644;&#27169;&#22411;&#36136;&#37327;&#20043;&#38388;&#26356;&#22909;&#30340;&#24179;&#34913;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;FP6-LLM&#20165;&#20351;&#29992;&#19968;&#37096;&#20998;&#23384;&#20648;&#31354;&#38388;&#23601;&#21487;&#20197;&#36827;&#34892;LLaMA-70b&#30340;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
Six-bit quantization (FP6) can effectively reduce the size of large language models (LLMs) and preserve the model quality consistently across varied applications. However, existing systems do not provide Tensor Core support for FP6 quantization and struggle to achieve practical performance improvements during LLM inference. It is challenging to support FP6 quantization on GPUs due to (1) unfriendly memory access of model weights with irregular bit-width and (2) high runtime overhead of weight de-quantization. To address these problems, we propose TC-FPx, the first full-stack GPU kernel design scheme with unified Tensor Core support of float-point weights for various quantization bit-width. We integrate TC-FPx kernel into an existing inference system, providing new end-to-end support (called FP6-LLM) for quantized LLM inference, where better trade-offs between inference cost and model quality are achieved. Experiments show that FP6-LLM enables the inference of LLaMA-70b using only a sin
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#35757;&#32451;&#21644;&#24494;&#35843;&#39640;&#31471;DNN&#65292;&#23454;&#29616;&#20351;&#29992;&#26356;&#20415;&#23452;&#30340;12&#20301;&#32047;&#21152;&#22120;&#65292;&#32780;&#19981;&#20250;&#23548;&#33268;&#31934;&#24230;&#38477;&#20302;&#65292;&#24182;&#23637;&#31034;&#20102;&#20351;&#29992;&#32454;&#31890;&#24230;&#26799;&#24230;&#36817;&#20284;&#21487;&#20197;&#25552;&#39640;DNN&#30340;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.14110</link><description>&lt;p&gt;
&#20197;&#36739;&#20302;&#27604;&#29305;&#23485;&#24230;&#30340;&#32047;&#21152;&#22120;&#38477;&#20302;&#28145;&#24230;&#32593;&#32476;&#25512;&#29702;&#25104;&#26412;
&lt;/p&gt;
&lt;p&gt;
Towards Cheaper Inference in Deep Networks with Lower Bit-Width Accumulators. (arXiv:2401.14110v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14110
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#35757;&#32451;&#21644;&#24494;&#35843;&#39640;&#31471;DNN&#65292;&#23454;&#29616;&#20351;&#29992;&#26356;&#20415;&#23452;&#30340;12&#20301;&#32047;&#21152;&#22120;&#65292;&#32780;&#19981;&#20250;&#23548;&#33268;&#31934;&#24230;&#38477;&#20302;&#65292;&#24182;&#23637;&#31034;&#20102;&#20351;&#29992;&#32454;&#31890;&#24230;&#26799;&#24230;&#36817;&#20284;&#21487;&#20197;&#25552;&#39640;DNN&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#22823;&#22810;&#25968;&#20851;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#37327;&#21270;&#30340;&#30740;&#31350;&#37117;&#30528;&#37325;&#20110;&#38477;&#20302;&#39640;&#32423;&#26694;&#26550;&#21487;&#35265;&#30340;&#24352;&#37327;&#31934;&#24230;&#65288;&#20363;&#22914;&#26435;&#37325;&#12289;&#28608;&#27963;&#21644;&#26799;&#24230;&#65289;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#30828;&#20214;&#20173;&#28982;&#20381;&#36182;&#20110;&#39640;&#31934;&#24230;&#30340;&#26680;&#24515;&#25805;&#20316;&#65292;&#26368;&#37325;&#35201;&#30340;&#26159;&#32047;&#21152;&#20056;&#31215;&#30340;&#36816;&#31639;&#12290;&#36825;&#31181;&#39640;&#31934;&#24230;&#32047;&#21152;&#36816;&#31639;&#36880;&#28176;&#25104;&#20026;&#20027;&#35201;&#30340;&#35745;&#31639;&#29942;&#39048;&#12290;&#36825;&#26159;&#22240;&#20026;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#20302;&#31934;&#24230;&#32047;&#21152;&#22120;&#30340;&#20351;&#29992;&#23548;&#33268;&#20102;&#24615;&#33021;&#30340;&#26174;&#33879;&#38477;&#20302;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#35757;&#32451;&#21644;&#24494;&#35843;&#39640;&#31471;DNN&#65292;&#39318;&#27425;&#23454;&#29616;&#20351;&#29992;&#26356;&#20415;&#23452;&#30340;12&#20301;&#32047;&#21152;&#22120;&#65292;&#32780;&#19981;&#20250;&#23548;&#33268;&#26174;&#33879;&#30340;&#31934;&#24230;&#38477;&#20302;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20986;&#38543;&#30528;&#32047;&#21152;&#31934;&#24230;&#36827;&#19968;&#27493;&#38477;&#20302;&#65292;&#20351;&#29992;&#32454;&#31890;&#24230;&#26799;&#24230;&#36817;&#20284;&#21487;&#20197;&#25552;&#39640;DNN&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The majority of the research on the quantization of Deep Neural Networks (DNNs) is focused on reducing the precision of tensors visible by high-level frameworks (e.g., weights, activations, and gradients). However, current hardware still relies on high-accuracy core operations. Most significant is the operation of accumulating products. This high-precision accumulation operation is gradually becoming the main computational bottleneck. This is because, so far, the usage of low-precision accumulators led to a significant degradation in performance. In this work, we present a simple method to train and fine-tune high-end DNNs, to allow, for the first time, utilization of cheaper, $12$-bits accumulators, with no significant degradation in accuracy. Lastly, we show that as we decrease the accumulation precision further, using fine-grained gradient approximations can improve the DNN accuracy.
&lt;/p&gt;</description></item><item><title>CompactifAI&#26159;&#19968;&#31181;&#20351;&#29992;&#37327;&#23376;&#21551;&#21457;&#30340;&#24352;&#37327;&#32593;&#32476;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26497;&#21387;&#32553;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#21387;&#32553;&#26041;&#27861;&#65292;&#23427;&#26356;&#27880;&#37325;&#27169;&#22411;&#30340;&#30456;&#20851;&#31354;&#38388;&#65292;&#23454;&#29616;&#26356;&#21152;&#21487;&#25511;&#21644;&#31934;&#32454;&#30340;&#21387;&#32553;&#12290;</title><link>http://arxiv.org/abs/2401.14109</link><description>&lt;p&gt;
CompactifAI: &#20351;&#29992;&#37327;&#23376;&#21551;&#21457;&#30340;&#24352;&#37327;&#32593;&#32476;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26497;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
CompactifAI: Extreme Compression of Large Language Models using Quantum-Inspired Tensor Networks. (arXiv:2401.14109v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14109
&lt;/p&gt;
&lt;p&gt;
CompactifAI&#26159;&#19968;&#31181;&#20351;&#29992;&#37327;&#23376;&#21551;&#21457;&#30340;&#24352;&#37327;&#32593;&#32476;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26497;&#21387;&#32553;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#21387;&#32553;&#26041;&#27861;&#65292;&#23427;&#26356;&#27880;&#37325;&#27169;&#22411;&#30340;&#30456;&#20851;&#31354;&#38388;&#65292;&#23454;&#29616;&#26356;&#21152;&#21487;&#25511;&#21644;&#31934;&#32454;&#30340;&#21387;&#32553;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;ChatGPT&#21644;LlaMA&#22312;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26041;&#38754;&#21462;&#24471;&#20102;&#24555;&#36895;&#36827;&#23637;&#65292;&#20294;&#20854;&#24222;&#22823;&#30340;&#35268;&#27169;&#24102;&#26469;&#20102;&#37325;&#35201;&#25361;&#25112;&#65292;&#22914;&#24040;&#22823;&#30340;&#35757;&#32451;&#21644;&#25512;&#26029;&#25104;&#26412;&#12289;&#36739;&#22823;&#30340;&#33021;&#28304;&#38656;&#27714;&#20197;&#21450;&#29616;&#22330;&#37096;&#32626;&#30340;&#38480;&#21046;&#12290;&#20256;&#32479;&#30340;&#21387;&#32553;&#26041;&#27861;&#22914;&#21098;&#26525;&#12289;&#33976;&#39311;&#21644;&#20302;&#31209;&#36924;&#36817;&#20027;&#35201;&#20851;&#27880;&#20943;&#23569;&#32593;&#32476;&#20013;&#31070;&#32463;&#20803;&#30340;&#26377;&#25928;&#25968;&#37327;&#65292;&#32780;&#37327;&#21270;&#26041;&#27861;&#21017;&#20391;&#37325;&#20110;&#38477;&#20302;&#21333;&#20010;&#26435;&#37325;&#30340;&#25968;&#20540;&#31934;&#24230;&#65292;&#20197;&#20943;&#23567;&#27169;&#22411;&#22823;&#23567;&#21516;&#26102;&#20445;&#25345;&#31070;&#32463;&#20803;&#25968;&#30446;&#19981;&#21464;&#12290;&#34429;&#28982;&#36825;&#20123;&#21387;&#32553;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#21462;&#24471;&#20102;&#30456;&#23545;&#25104;&#21151;&#65292;&#20294;&#27809;&#26377;&#20196;&#20154;&#20449;&#26381;&#30340;&#29702;&#30001;&#35748;&#20026;&#25130;&#26029;&#31070;&#32463;&#20803;&#30340;&#25968;&#37327;&#26159;&#19968;&#31181;&#26368;&#20248;&#31574;&#30053;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;LLM&#21387;&#32553;&#26041;&#27861;CompactifAI&#65292;&#23427;&#20351;&#29992;&#37327;&#23376;&#21551;&#21457;&#30340;&#24352;&#37327;&#32593;&#32476;&#65292;&#32780;&#19981;&#26159;&#20256;&#32479;&#30340;&#21387;&#32553;&#26041;&#27861;&#65292;&#26356;&#27880;&#37325;&#27169;&#22411;&#30340;&#30456;&#20851;&#31354;&#38388;&#65292;&#23454;&#29616;&#26356;&#21152;&#21487;&#25511;&#21644;&#31934;&#32454;&#30340;&#21387;&#32553;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) such as ChatGPT and LlaMA are advancing rapidly in generative Artificial Intelligence (AI), but their immense size poses significant challenges, such as huge training and inference costs, substantial energy demands, and limitations for on-site deployment. Traditional compression methods such as pruning, distillation, and low-rank approximation focus on reducing the effective number of neurons in the network, while quantization focuses on reducing the numerical precision of individual weights to reduce the model size while keeping the number of neurons fixed. While these compression methods have been relatively successful in practice, there's no compelling reason to believe that truncating the number of neurons is an optimal strategy. In this context, this paper introduces CompactifAI, an innovative LLM compression approach using quantum-inspired Tensor Networks that focuses on the model's correlation space instead, allowing for a more controlled, refined an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23569;&#26679;&#26412;&#20154;&#26426;&#21327;&#20316;&#25913;&#36827;&#65288;FHLR&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#21487;&#31359;&#25140;&#25968;&#25454;&#20013;&#30340;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#24369;&#26631;&#31614;&#23398;&#20064;&#31181;&#23376;&#27169;&#22411;&#65292;&#20351;&#29992;&#23569;&#37327;&#30340;&#19987;&#23478;&#20462;&#27491;&#23545;&#31181;&#23376;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#36890;&#36807;&#21152;&#26435;&#21442;&#25968;&#24179;&#22343;&#23558;&#31181;&#23376;&#27169;&#22411;&#21644;&#24494;&#35843;&#27169;&#22411;&#21512;&#24182;&#65292;&#20197;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.14107</link><description>&lt;p&gt;
&#36890;&#36807;&#23569;&#26679;&#26412;&#20154;&#26426;&#21327;&#20316;&#25913;&#36827;&#23454;&#29616;&#22122;&#22768;&#26631;&#31614;&#19979;&#30340;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning under Label Noise through Few-Shot Human-in-the-Loop Refinement. (arXiv:2401.14107v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14107
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23569;&#26679;&#26412;&#20154;&#26426;&#21327;&#20316;&#25913;&#36827;&#65288;FHLR&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#21487;&#31359;&#25140;&#25968;&#25454;&#20013;&#30340;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#24369;&#26631;&#31614;&#23398;&#20064;&#31181;&#23376;&#27169;&#22411;&#65292;&#20351;&#29992;&#23569;&#37327;&#30340;&#19987;&#23478;&#20462;&#27491;&#23545;&#31181;&#23376;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#36890;&#36807;&#21152;&#26435;&#21442;&#25968;&#24179;&#22343;&#23558;&#31181;&#23376;&#27169;&#22411;&#21644;&#24494;&#35843;&#27169;&#22411;&#21512;&#24182;&#65292;&#20197;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#31359;&#25140;&#25216;&#26415;&#33021;&#22815;&#36830;&#32493;&#30417;&#27979;&#21508;&#31181;&#20581;&#24247;&#25351;&#26631;&#65292;&#22914;&#36523;&#20307;&#27963;&#21160;&#12289;&#24515;&#29575;&#12289;&#30561;&#30496;&#21644;&#21387;&#21147;&#27700;&#24179;&#12290;&#21487;&#31359;&#25140;&#25968;&#25454;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#33719;&#21462;&#39640;&#36136;&#37327;&#30340;&#26631;&#31614;&#12290;&#19982;&#35270;&#39057;&#31561;&#27169;&#24577;&#19981;&#21516;&#65292;&#21487;&#31359;&#25140;&#25968;&#25454;&#19981;&#21253;&#21547;&#20851;&#20110;&#29992;&#25143;&#30340;&#29289;&#29702;&#34920;&#29616;&#30340;&#26126;&#26174;&#32447;&#32034;&#65292;&#36890;&#24120;&#38656;&#35201;&#20016;&#23500;&#30340;&#20803;&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;&#22312;&#20026;&#36825;&#26679;&#30340;&#25968;&#25454;&#26631;&#27880;&#26102;&#65292;&#26631;&#31614;&#22122;&#22768;&#21487;&#33021;&#25104;&#20026;&#19968;&#20010;&#36234;&#26469;&#36234;&#26840;&#25163;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;&#23569;&#26679;&#26412;&#20154;&#26426;&#21327;&#20316;&#25913;&#36827;&#65288;FHLR&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#20351;&#29992;&#24369;&#26631;&#31614;&#23398;&#20064;&#19968;&#20010;&#31181;&#23376;&#27169;&#22411;&#12290;&#25509;&#19979;&#26469;&#65292;&#23427;&#20351;&#29992;&#23569;&#37327;&#30340;&#19987;&#23478;&#20462;&#27491;&#23545;&#31181;&#23376;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#21152;&#26435;&#21442;&#25968;&#24179;&#22343;&#23558;&#31181;&#23376;&#27169;&#22411;&#21644;&#24494;&#35843;&#27169;&#22411;&#21512;&#24182;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Wearable technologies enable continuous monitoring of various health metrics, such as physical activity, heart rate, sleep, and stress levels. A key challenge with wearable data is obtaining quality labels. Unlike modalities like video where the videos themselves can be effectively used to label objects or events, wearable data do not contain obvious cues about the physical manifestation of the users and usually require rich metadata. As a result, label noise can become an increasingly thorny issue when labeling such data. In this paper, we propose a novel solution to address noisy label learning, entitled Few-Shot Human-in-the-Loop Refinement (FHLR). Our method initially learns a seed model using weak labels. Next, it fine-tunes the seed model using a handful of expert corrections. Finally, it achieves better generalizability and robustness by merging the seed and fine-tuned models via weighted parameter averaging. We evaluate our approach on four challenging tasks and datasets, and c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#20013;&#24515;&#30340;&#26080;&#30417;&#30563;&#36864;&#21270;&#25351;&#31034;&#22120;McUDI&#65292;&#29992;&#20110;&#26816;&#27979;AIOps&#27169;&#22411;&#30001;&#20110;&#25968;&#25454;&#21464;&#21270;&#32780;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#30340;&#26102;&#26426;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;AIOps&#35299;&#20915;&#26041;&#26696;&#20013;&#24212;&#29992;McUDI&#21487;&#20197;&#20943;&#23569;&#26631;&#35760;&#26679;&#26412;&#25968;&#37327;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.14093</link><description>&lt;p&gt;
McUDI: &#27169;&#22411;&#20013;&#24515;&#30340;&#26080;&#30417;&#30563;&#36864;&#21270;&#25351;&#31034;&#22120;&#65292;&#29992;&#20110;&#25925;&#38556;&#39044;&#27979;AIOps&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
McUDI: Model-Centric Unsupervised Degradation Indicator for Failure Prediction AIOps Solutions. (arXiv:2401.14093v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14093
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#20013;&#24515;&#30340;&#26080;&#30417;&#30563;&#36864;&#21270;&#25351;&#31034;&#22120;McUDI&#65292;&#29992;&#20110;&#26816;&#27979;AIOps&#27169;&#22411;&#30001;&#20110;&#25968;&#25454;&#21464;&#21270;&#32780;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#30340;&#26102;&#26426;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;AIOps&#35299;&#20915;&#26041;&#26696;&#20013;&#24212;&#29992;McUDI&#21487;&#20197;&#20943;&#23569;&#26631;&#35760;&#26679;&#26412;&#25968;&#37327;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#36816;&#34892;&#25968;&#25454;&#19981;&#26029;&#21464;&#21270;&#65292;AIOps&#35299;&#20915;&#26041;&#26696;&#30340;&#24615;&#33021;&#20250;&#38543;&#26102;&#38388;&#19979;&#38477;&#12290;&#34429;&#28982;&#21608;&#26399;&#24615;&#37325;&#35757;&#32451;&#26159;&#20445;&#25345;&#25925;&#38556;&#39044;&#27979;AIOps&#27169;&#22411;&#24615;&#33021;&#30340;&#26368;&#26032;&#25216;&#26415;&#65292;&#20294;&#35813;&#25216;&#26415;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#12290;&#22312;AIOps&#20013;&#33719;&#21462;&#26631;&#35760;&#25968;&#25454;&#26159;&#26114;&#36149;&#30340;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#39046;&#22495;&#19987;&#23478;&#30340;&#25903;&#25345;&#36827;&#34892;&#23494;&#38598;&#30340;&#27880;&#37322;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#20013;&#24515;&#30340;&#26080;&#30417;&#30563;&#36864;&#21270;&#25351;&#31034;&#22120;McUDI&#65292;&#23427;&#33021;&#22815;&#26816;&#27979;&#21040;AIOps&#27169;&#22411;&#30001;&#20110;&#25968;&#25454;&#21464;&#21270;&#32780;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#30340;&#30830;&#20999;&#26102;&#21051;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;AIOps&#35299;&#20915;&#26041;&#26696;&#30340;&#32500;&#25252;&#27969;&#31243;&#20013;&#24212;&#29992;McUDI&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#21608;&#26399;&#24615;&#37325;&#35757;&#32451;&#30340;&#31867;&#20284;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#65292;&#20943;&#23569;&#38656;&#35201;&#36827;&#34892;&#27880;&#37322;&#30340;&#26679;&#26412;&#25968;&#65306;&#23545;&#20110;&#20316;&#19994;&#25925;&#38556;&#39044;&#27979;&#38656;&#35201;30k&#20010;&#26679;&#26412;&#65292;&#23545;&#20110;&#30913;&#30424;&#25925;&#38556;&#39044;&#27979;&#38656;&#35201;260k&#20010;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the continuous change in operational data, AIOps solutions suffer from performance degradation over time. Although periodic retraining is the state-of-the-art technique to preserve the failure prediction AIOps models' performance over time, this technique requires a considerable amount of labeled data to retrain. In AIOps obtaining label data is expensive since it requires the availability of domain experts to intensively annotate it. In this paper, we present McUDI, a model-centric unsupervised degradation indicator that is capable of detecting the exact moment the AIOps model requires retraining as a result of changes in data. We further show how employing McUDI in the maintenance pipeline of AIOps solutions can reduce the number of samples that require annotations with 30k for job failure prediction and 260k for disk failure prediction while achieving similar performance with periodic retraining.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22359;&#21270;&#26550;&#26500;&#65292;&#21033;&#29992;&#33286;&#24773;&#27744;&#26469;&#33258;&#21160;&#21270;&#32593;&#32476;&#23041;&#32961;&#24402;&#22240;&#36807;&#31243;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21305;&#37197;&#32858;&#21512;&#22120;&#20316;&#20026;&#32858;&#21512;&#26041;&#27861;&#65292;&#20197;&#22686;&#21152;&#21487;&#36861;&#36394;&#24615;&#21644;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.14090</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;&#32593;&#32476;&#23041;&#32961;&#24402;&#22240;&#30340;&#27169;&#22359;&#21270;&#26041;&#27861;&#65306;&#21033;&#29992;&#33286;&#24773;&#27744;
&lt;/p&gt;
&lt;p&gt;
A Modular Approach to Automatic Cyber Threat Attribution using Opinion Pools. (arXiv:2401.14090v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14090
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22359;&#21270;&#26550;&#26500;&#65292;&#21033;&#29992;&#33286;&#24773;&#27744;&#26469;&#33258;&#21160;&#21270;&#32593;&#32476;&#23041;&#32961;&#24402;&#22240;&#36807;&#31243;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21305;&#37197;&#32858;&#21512;&#22120;&#20316;&#20026;&#32858;&#21512;&#26041;&#27861;&#65292;&#20197;&#22686;&#21152;&#21487;&#36861;&#36394;&#24615;&#21644;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#23041;&#32961;&#24402;&#22240;&#22312;&#22686;&#21152;&#23545;&#25968;&#23383;&#23041;&#32961;&#30340;&#25269;&#24481;&#33021;&#21147;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20391;&#37325;&#20110;&#33258;&#21160;&#21270;&#23041;&#32961;&#24402;&#22240;&#36807;&#31243;&#24182;&#23558;&#20854;&#19982;&#20854;&#20182;&#24037;&#20316;&#38598;&#25104;&#22312;&#19968;&#36215;&#65292;&#22914;&#23041;&#32961;&#20390;&#27979;&#12290;&#20026;&#20102;&#25903;&#25345;&#32593;&#32476;&#23041;&#32961;&#24402;&#22240;&#36807;&#31243;&#30340;&#33258;&#21160;&#21270;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22359;&#21270;&#26550;&#26500;&#20316;&#20026;&#24403;&#21069;&#25972;&#20307;&#24335;&#33258;&#21160;&#21270;&#26041;&#27861;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#27169;&#22359;&#21270;&#26550;&#26500;&#21487;&#20197;&#21033;&#29992;&#33286;&#24773;&#27744;&#26469;&#32467;&#21512;&#20855;&#20307;&#24402;&#22240;&#22120;&#30340;&#36755;&#20986;&#12290;&#25152;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#22686;&#21152;&#20102;&#23041;&#32961;&#24402;&#22240;&#38382;&#39064;&#30340;&#21487;&#36861;&#36394;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#39640;&#30340;&#21487;&#29992;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#19982;&#25972;&#20307;&#24335;&#26041;&#27861;&#30456;&#27604;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21305;&#37197;&#32858;&#21512;&#22120;&#20316;&#20026;&#19968;&#31181;&#32858;&#21512;&#26041;&#27861;&#65292;&#26681;&#25454;&#19981;&#21516;&#30340;&#29305;&#24449;&#23545;&#24402;&#22240;&#22120;&#36827;&#34892;&#37197;&#23545;&#65292;&#29983;&#25104;&#20013;&#38388;&#32467;&#26524;&#65292;&#28982;&#21518;&#26368;&#32456;&#29983;&#25104;&#21333;&#20010;&#27010;&#29575;&#36136;&#37327;&#20989;&#25968; (Probability Mass Function, PMF) &#20316;&#20026;&#36755;&#20986;&#12290;&#21305;&#37197;&#32858;&#21512;&#22120;&#20381;&#27425;&#24212;&#29992;&#20102;&#23545;&#25968;&#33286;&#24773;&#21644;&#32047;&#21152;&#36816;&#31639;&#26469;&#29983;&#25104;&#26368;&#32456;&#30340;PMF&#12290;
&lt;/p&gt;
&lt;p&gt;
Cyber threat attribution can play an important role in increasing resilience against digital threats. Recent research focuses on automating the threat attribution process and on integrating it with other efforts, such as threat hunting. To support increasing automation of the cyber threat attribution process, this paper proposes a modular architecture as an alternative to current monolithic automated approaches. The modular architecture can utilize opinion pools to combine the output of concrete attributors. The proposed solution increases the tractability of the threat attribution problem and offers increased usability and interpretability, as opposed to monolithic alternatives. In addition, a Pairing Aggregator is proposed as an aggregation method that forms pairs of attributors based on distinct features to produce intermediary results before finally producing a single Probability Mass Function (PMF) as output. The Pairing Aggregator sequentially applies both the logarithmic opinion
&lt;/p&gt;</description></item><item><title>&#30001;&#20110;&#29992;&#25143;&#38656;&#27714;&#21644;&#26368;&#36817;&#30340;&#27861;&#35268;&#35201;&#27714;&#65292;&#38656;&#35201;&#23545;AI&#31995;&#32479;&#25152;&#20570;&#20986;&#30340;&#20915;&#31574;&#36827;&#34892;&#35299;&#37322;&#12290;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Sum-Product Networks&#27169;&#25311;&#23547;&#25214;&#39640;&#21487;&#33021;&#24615;&#21453;&#20107;&#23454;&#25512;&#29702;&#30340;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#25552;&#20379;&#28385;&#36275;&#22810;&#20010;&#24120;&#35265;&#35201;&#27714;&#30340;&#26368;&#20339;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2401.14086</link><description>&lt;p&gt;
&#20351;&#29992;Sum-Product Networks&#29983;&#25104;&#21487;&#33021;&#30340;&#21453;&#20107;&#23454;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Generating Likely Counterfactuals Using Sum-Product Networks. (arXiv:2401.14086v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14086
&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#29992;&#25143;&#38656;&#27714;&#21644;&#26368;&#36817;&#30340;&#27861;&#35268;&#35201;&#27714;&#65292;&#38656;&#35201;&#23545;AI&#31995;&#32479;&#25152;&#20570;&#20986;&#30340;&#20915;&#31574;&#36827;&#34892;&#35299;&#37322;&#12290;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Sum-Product Networks&#27169;&#25311;&#23547;&#25214;&#39640;&#21487;&#33021;&#24615;&#21453;&#20107;&#23454;&#25512;&#29702;&#30340;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#25552;&#20379;&#28385;&#36275;&#22810;&#20010;&#24120;&#35265;&#35201;&#27714;&#30340;&#26368;&#20339;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#29992;&#25143;&#38656;&#27714;&#21644;&#26368;&#36817;&#30340;&#27861;&#35268;&#65288;GDPR&#12289;AI&#27861;&#26696;&#65289;&#65292;&#38656;&#35201;&#35299;&#37322;AI&#31995;&#32479;&#25152;&#20570;&#20986;&#30340;&#20915;&#31574;&#12290;&#36825;&#20123;&#20915;&#31574;&#24448;&#24448;&#21482;&#33021;&#22312;&#20107;&#21518;&#35299;&#37322;&#65292;&#21453;&#20107;&#23454;&#25512;&#29702;&#25104;&#20026;&#24120;&#35265;&#30340;&#35299;&#37322;&#26041;&#24335;&#12290;&#20160;&#20040;&#26500;&#25104;&#20102;&#26368;&#20339;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#24517;&#39035;&#32771;&#34385;&#22810;&#20010;&#26041;&#38754;&#65292;&#20854;&#20013;&#8220;&#26679;&#26412;&#36317;&#31163;&#8221;&#26159;&#26368;&#24120;&#35265;&#30340;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#19968;&#35201;&#27714;&#32463;&#24120;&#20250;&#23548;&#33268;&#19981;&#22826;&#21487;&#33021;&#19988;&#22240;&#27492;&#20215;&#20540;&#26377;&#38480;&#30340;&#35299;&#37322;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33021;&#22815;&#25552;&#20379;&#39640;&#21487;&#33021;&#24615;&#35299;&#37322;&#30340;&#31995;&#32479;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;&#65288;MIO&#65289;&#27169;&#25311;&#23547;&#25214;&#28385;&#36275;&#21453;&#20107;&#23454;&#25512;&#29702;&#30340;&#35768;&#22810;&#24120;&#35265;&#35201;&#27714;&#30340;&#26368;&#26377;&#21487;&#33021;&#35299;&#37322;&#12290;&#22312;&#27492;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Sum-Product Network&#65288;SPN&#65289;&#30340;MIO&#34920;&#36798;&#65292;&#24182;&#20351;&#29992;SPN&#20272;&#35745;&#21453;&#20107;&#23454;&#30340;&#21487;&#33021;&#24615;&#65292;&#36825;&#23545;&#29420;&#31435;&#30340;&#20852;&#36259;&#20063;&#26377;&#29992;&#12290;&#19982;&#29983;&#25104;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#20960;&#31181;&#26041;&#27861;&#36827;&#34892;&#25968;&#20540;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to user demand and recent regulation (GDPR, AI Act), decisions made by AI systems need to be explained. These decisions are often explainable only post hoc, where counterfactual explanations are popular. The question of what constitutes the best counterfactual explanation must consider multiple aspects, where "distance from the sample" is the most common. We argue that this requirement frequently leads to explanations that are unlikely and, therefore, of limited value. Here, we present a system that provides high-likelihood explanations. We show that the search for the most likely explanations satisfying many common desiderata for counterfactual explanations can be modeled using mixed-integer optimization (MIO). In the process, we propose an MIO formulation of a Sum-Product Network (SPN) and use the SPN to estimate the likelihood of a counterfactual, which can be of independent interest. A numerical comparison against several methods for generating counterfactual explanations is pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25805;&#20316;&#30697;&#38453;&#26041;&#27861;&#26469;&#21152;&#36895;&#20998;&#25968;PINNs&#30340;&#35757;&#32451;&#65292;&#36890;&#36807;&#38750;&#22343;&#21248;&#31163;&#25955;&#21270;&#20998;&#25968;Caputo&#31639;&#23376;&#65292;&#23454;&#29616;&#20102;&#23545;&#20998;&#25968;&#23548;&#25968;&#30340;&#24555;&#36895;&#35745;&#31639;&#12290;&#20351;&#29992;Legendre&#31070;&#32463;&#22359;&#65288;LNB&#65289;&#26550;&#26500;&#25552;&#39640;&#20102;PINNs&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.14081</link><description>&lt;p&gt;
&#20351;&#29992;&#23548;&#25968;&#30340;&#25805;&#20316;&#30697;&#38453;&#21152;&#36895;&#20998;&#25968;PINNs
&lt;/p&gt;
&lt;p&gt;
Accelerating Fractional PINNs using Operational Matrices of Derivative. (arXiv:2401.14081v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14081
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25805;&#20316;&#30697;&#38453;&#26041;&#27861;&#26469;&#21152;&#36895;&#20998;&#25968;PINNs&#30340;&#35757;&#32451;&#65292;&#36890;&#36807;&#38750;&#22343;&#21248;&#31163;&#25955;&#21270;&#20998;&#25968;Caputo&#31639;&#23376;&#65292;&#23454;&#29616;&#20102;&#23545;&#20998;&#25968;&#23548;&#25968;&#30340;&#24555;&#36895;&#35745;&#31639;&#12290;&#20351;&#29992;Legendre&#31070;&#32463;&#22359;&#65288;LNB&#65289;&#26550;&#26500;&#25552;&#39640;&#20102;PINNs&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25805;&#20316;&#30697;&#38453;&#26041;&#27861;&#65292;&#29992;&#20110;&#21152;&#36895;&#20998;&#25968;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;fPINNs&#65289;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#23545;&#20998;&#25968;Caputo&#31639;&#23376;&#36827;&#34892;&#38750;&#22343;&#21248;&#31163;&#25955;&#21270;&#65292;&#20415;&#20110;&#22312;Caputo&#22411;&#20998;&#25968;&#24494;&#20998;&#38382;&#39064;&#20013;&#35745;&#31639;&#20998;&#25968;&#23548;&#25968;&#12290;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#65292;&#25805;&#20316;&#30697;&#38453;&#26159;&#39044;&#20808;&#35745;&#31639;&#30340;&#65292;&#22312;&#35757;&#32451;&#38454;&#27573;&#65292;&#33258;&#21160;&#24494;&#20998;&#34987;&#19968;&#20010;&#30697;&#38453;-&#21521;&#37327;&#20056;&#31215;&#26367;&#20195;&#12290;&#34429;&#28982;&#25105;&#20204;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#20219;&#20309;&#32593;&#32476;&#65292;&#20294;&#25105;&#20204;&#29305;&#21035;&#24378;&#35843;&#20854;&#22312;PINNs&#20013;&#25104;&#21151;&#23454;&#29616;&#30340;&#24773;&#20917;&#65292;&#24378;&#35843;&#22312;&#20351;&#29992;Legendre&#31070;&#32463;&#22359;&#65288;LNB&#65289;&#26550;&#26500;&#26102;&#25152;&#23454;&#29616;&#30340;&#22686;&#24378;&#20934;&#30830;&#24615;&#12290;LNB&#23558;Legendre&#22810;&#39033;&#24335;&#34701;&#20837;&#21040;PINN&#32467;&#26500;&#20013;&#65292;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#22312;&#21253;&#25324;&#26102;&#28382;&#24494;&#20998;&#26041;&#31243;&#21644;&#24494;&#20998;&#26041;&#31243;&#31995;&#32479;&#22312;&#20869;&#30340;&#22810;&#31181;&#24494;&#20998;&#26041;&#31243;&#20013;&#24471;&#21040;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel operational matrix method to accelerate the training of fractional Physics-Informed Neural Networks (fPINNs). Our approach involves a non-uniform discretization of the fractional Caputo operator, facilitating swift computation of fractional derivatives within Caputo-type fractional differential problems with $0&lt;\alpha&lt;1$. In this methodology, the operational matrix is precomputed, and during the training phase, automatic differentiation is replaced with a matrix-vector product. While our methodology is compatible with any network, we particularly highlight its successful implementation in PINNs, emphasizing the enhanced accuracy achieved when utilizing the Legendre Neural Block (LNB) architecture. LNB incorporates Legendre polynomials into the PINN structure, providing a significant boost in accuracy. The effectiveness of our proposed method is validated across diverse differential equations, including Delay Differential Equations (DDEs) and Systems of Diffe
&lt;/p&gt;</description></item><item><title>ProCNS&#26159;&#19968;&#31181;&#29992;&#20110;&#24369;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#26032;&#26041;&#27861;&#65292;&#37319;&#29992;&#28176;&#36827;&#24335;&#21407;&#22411;&#26657;&#20934;&#21644;&#22122;&#22768;&#25233;&#21046;&#30340;&#21407;&#21017;&#26469;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.14074</link><description>&lt;p&gt;
ProCNS: &#29992;&#20110;&#24369;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#28176;&#36827;&#24335;&#21407;&#22411;&#26657;&#20934;&#21644;&#22122;&#22768;&#25233;&#21046;
&lt;/p&gt;
&lt;p&gt;
ProCNS: Progressive Prototype Calibration and Noise Suppression for Weakly-Supervised Medical Image Segmentation. (arXiv:2401.14074v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14074
&lt;/p&gt;
&lt;p&gt;
ProCNS&#26159;&#19968;&#31181;&#29992;&#20110;&#24369;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#26032;&#26041;&#27861;&#65292;&#37319;&#29992;&#28176;&#36827;&#24335;&#21407;&#22411;&#26657;&#20934;&#21644;&#22122;&#22768;&#25233;&#21046;&#30340;&#21407;&#21017;&#26469;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24369;&#30417;&#30563;&#20998;&#21106;&#65288;WSS&#65289;&#20316;&#20026;&#32531;&#35299;&#27880;&#37322;&#25104;&#26412;&#21644;&#27169;&#22411;&#24615;&#33021;&#20043;&#38388;&#20914;&#31361;&#30340;&#35299;&#20915;&#26041;&#26696;&#32780;&#20986;&#29616;&#65292;&#37319;&#29992;&#31232;&#30095;&#30340;&#27880;&#37322;&#26684;&#24335;&#65288;&#20363;&#22914;&#28857;&#12289;&#28034;&#40486;&#12289;&#22359;&#31561;&#65289;&#12290;&#20856;&#22411;&#30340;&#26041;&#27861;&#35797;&#22270;&#21033;&#29992;&#35299;&#21078;&#21644;&#25299;&#25169;&#20808;&#39564;&#23558;&#31232;&#30095;&#27880;&#37322;&#30452;&#25509;&#25193;&#23637;&#20026;&#20266;&#26631;&#31614;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23545;&#21307;&#23398;&#22270;&#20687;&#20013;&#27169;&#31946;&#36793;&#32536;&#30340;&#20851;&#27880;&#19981;&#36275;&#21644;&#23545;&#31232;&#30095;&#30417;&#30563;&#30340;&#19981;&#20805;&#20998;&#25506;&#32034;&#65292;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#20250;&#22312;&#22122;&#22768;&#21306;&#22495;&#29983;&#25104;&#38169;&#35823;&#19988;&#36807;&#20110;&#33258;&#20449;&#30340;&#20266;&#24314;&#35758;&#65292;&#23548;&#33268;&#27169;&#22411;&#35823;&#24046;&#32047;&#31215;&#21644;&#24615;&#33021;&#19979;&#38477;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;WSS&#26041;&#27861;&#65292;&#21517;&#20026;ProCNS&#65292;&#23427;&#21253;&#21547;&#20004;&#20010;&#21327;&#21516;&#27169;&#22359;&#65292;&#35774;&#35745;&#21407;&#21017;&#26159;&#28176;&#36827;&#24335;&#21407;&#22411;&#26657;&#20934;&#21644;&#22122;&#22768;&#25233;&#21046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#21407;&#22411;&#30340;&#21306;&#22495;&#31354;&#38388;&#30456;&#20284;&#24615;&#65288;PRSA&#65289;&#25439;&#22833;&#20989;&#25968;&#65292;&#26368;&#22823;&#21270;&#31354;&#38388;&#21644;&#35821;&#20041;&#20803;&#32032;&#20043;&#38388;&#30340;&#25104;&#23545;&#30456;&#20284;&#24230;&#65292;&#20026;&#25105;&#20204;&#24863;&#20852;&#36259;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;
&lt;/p&gt;
&lt;p&gt;
Weakly-supervised segmentation (WSS) has emerged as a solution to mitigate the conflict between annotation cost and model performance by adopting sparse annotation formats (e.g., point, scribble, block, etc.). Typical approaches attempt to exploit anatomy and topology priors to directly expand sparse annotations into pseudo-labels. However, due to a lack of attention to the ambiguous edges in medical images and insufficient exploration of sparse supervision, existing approaches tend to generate erroneous and overconfident pseudo proposals in noisy regions, leading to cumulative model error and performance degradation. In this work, we propose a novel WSS approach, named ProCNS, encompassing two synergistic modules devised with the principles of progressive prototype calibration and noise suppression. Specifically, we design a Prototype-based Regional Spatial Affinity (PRSA) loss to maximize the pair-wise affinities between spatial and semantic elements, providing our model of interest 
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;Sinkhorn&#26799;&#24230;&#27969;&#27169;&#22411;&#36890;&#36807;&#21442;&#25968;&#21270;Wasserstein&#26799;&#24230;&#27969;&#20013;&#30340;&#36895;&#24230;&#22330;&#65292;&#24182;&#21033;&#29992;&#36895;&#24230;&#22330;&#21305;&#37197;&#35757;&#32451;&#26041;&#26696;&#23454;&#29616;&#39640;&#25928;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#20854;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#38543;&#30528;&#26679;&#26412;&#22823;&#23567;&#36235;&#36817;&#20110;&#26080;&#31351;&#22823;&#65292;&#32463;&#39564;&#36817;&#20284;&#20540;&#30340;&#22343;&#22330;&#26497;&#38480;&#20250;&#25910;&#25947;&#21040;&#30495;&#23454;&#30340;&#24213;&#23618;&#36895;&#24230;&#22330;&#12290;</title><link>http://arxiv.org/abs/2401.14069</link><description>&lt;p&gt;
&#31070;&#32463;Sinkhorn&#26799;&#24230;&#27969;
&lt;/p&gt;
&lt;p&gt;
Neural Sinkhorn Gradient Flow. (arXiv:2401.14069v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14069
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;Sinkhorn&#26799;&#24230;&#27969;&#27169;&#22411;&#36890;&#36807;&#21442;&#25968;&#21270;Wasserstein&#26799;&#24230;&#27969;&#20013;&#30340;&#36895;&#24230;&#22330;&#65292;&#24182;&#21033;&#29992;&#36895;&#24230;&#22330;&#21305;&#37197;&#35757;&#32451;&#26041;&#26696;&#23454;&#29616;&#39640;&#25928;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#20854;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#38543;&#30528;&#26679;&#26412;&#22823;&#23567;&#36235;&#36817;&#20110;&#26080;&#31351;&#22823;&#65292;&#32463;&#39564;&#36817;&#20284;&#20540;&#30340;&#22343;&#22330;&#26497;&#38480;&#20250;&#25910;&#25947;&#21040;&#30495;&#23454;&#30340;&#24213;&#23618;&#36895;&#24230;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#29305;&#23450;&#27867;&#20989;&#30340;Wasserstein&#26799;&#24230;&#27969;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#26368;&#36817;&#65292;&#20154;&#20204;&#37319;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#36817;&#20284;&#22788;&#29702;Wasserstein&#26799;&#24230;&#27969;&#20013;&#30340;&#19968;&#20123;&#38590;&#20197;&#35745;&#31639;&#30340;&#37096;&#20998;&#65292;&#24182;&#24471;&#21040;&#20102;&#39640;&#25928;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#31070;&#32463;Sinkhorn&#26799;&#24230;&#27969;&#65288;NSGF&#65289;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#23558;Wasserstein&#26799;&#24230;&#27969;&#30340;&#26102;&#38388;&#21464;&#21270;&#36895;&#24230;&#22330;&#21442;&#25968;&#21270;&#20026;Sinkhorn&#36317;&#31163;&#21040;&#30446;&#26631;&#20998;&#24067;&#19982;&#32473;&#23450;&#28304;&#20998;&#24067;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#22312;NSGF&#20013;&#21033;&#29992;&#36895;&#24230;&#22330;&#21305;&#37197;&#35757;&#32451;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#21482;&#38656;&#35201;&#26469;&#33258;&#28304;&#20998;&#24067;&#21644;&#30446;&#26631;&#20998;&#24067;&#30340;&#26679;&#26412;&#26469;&#35745;&#31639;&#32463;&#39564;&#36895;&#24230;&#22330;&#36817;&#20284;&#20540;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#38543;&#30528;&#26679;&#26412;&#22823;&#23567;&#36235;&#36817;&#20110;&#26080;&#31351;&#22823;&#65292;&#32463;&#39564;&#36817;&#20284;&#20540;&#30340;&#22343;&#22330;&#26497;&#38480;&#20250;&#25910;&#25947;&#21040;&#30495;&#23454;&#30340;&#24213;&#23618;&#36895;&#24230;&#22330;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#27169;&#22411;&#22312;&#39640;&#32500;&#20219;&#21153;&#19978;&#30340;&#25928;&#29575;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;NSGF
&lt;/p&gt;
&lt;p&gt;
Wasserstein Gradient Flows (WGF) with respect to specific functionals have been widely used in the machine learning literature. Recently, neural networks have been adopted to approximate certain intractable parts of the underlying Wasserstein gradient flow and result in efficient inference procedures. In this paper, we introduce the Neural Sinkhorn Gradient Flow (NSGF) model, which parametrizes the time-varying velocity field of the Wasserstein gradient flow w.r.t. the Sinkhorn divergence to the target distribution starting a given source distribution. We utilize the velocity field matching training scheme in NSGF, which only requires samples from the source and target distribution to compute an empirical velocity field approximation. Our theoretical analyses show that as the sample size increases to infinity, the mean-field limit of the empirical approximation converges to the true underlying velocity field. To further enhance model efficiency on high-dimensional tasks, a two-phase NS
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#37319;&#29992;&#20102;&#26032;&#39062;&#30340;Relief&#31639;&#27861;&#22312;&#32423;&#32852;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20013;&#36827;&#34892;&#39118;&#36895;&#39044;&#27979;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#35813;&#26041;&#27861;&#30340;&#20851;&#38190;&#22312;&#20110;&#36873;&#25321;&#26368;&#30456;&#20851;&#30340;&#36755;&#20837;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2401.14065</link><description>&lt;p&gt;
&#22312;&#21360;&#24230;&#39118;&#30005;&#36164;&#28304;&#35780;&#20272;&#20013;&#65292;&#37319;&#29992;&#32423;&#32852;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;Relief&#31639;&#27861;&#30340;&#26032;&#24212;&#29992;&#26469;&#39044;&#27979;&#39118;&#36895;
&lt;/p&gt;
&lt;p&gt;
Novel application of Relief Algorithm in cascaded artificial neural network to predict wind speed for wind power resource assessment in India. (arXiv:2401.14065v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14065
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#37319;&#29992;&#20102;&#26032;&#39062;&#30340;Relief&#31639;&#27861;&#22312;&#32423;&#32852;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20013;&#36827;&#34892;&#39118;&#36895;&#39044;&#27979;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#35813;&#26041;&#27861;&#30340;&#20851;&#38190;&#22312;&#20110;&#36873;&#25321;&#26368;&#30456;&#20851;&#30340;&#36755;&#20837;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39118;&#33021;&#21457;&#30005;&#30001;&#20110;&#27668;&#35937;&#21464;&#37327;&#30340;&#38543;&#26426;&#24615;&#32780;&#20855;&#26377;&#19981;&#21487;&#39044;&#27979;&#30340;&#29305;&#24615;&#12290;&#22240;&#27492;&#65292;&#33021;&#28304;&#19994;&#21153;&#21644;&#39118;&#33021;&#21457;&#30005;&#30340;&#25511;&#21046;&#38656;&#35201;&#20174;&#20960;&#31186;&#21040;&#19981;&#21516;&#26102;&#38388;&#27493;&#38271;&#25552;&#21069;&#39044;&#27979;&#39118;&#36895;&#12290;&#20026;&#20102;&#35299;&#20915;&#39044;&#27979;&#30340;&#19981;&#36275;&#65292;&#24050;&#32463;&#20351;&#29992;&#20102;&#21508;&#31181;&#39118;&#36895;&#39044;&#27979;&#26041;&#27861;&#12290;&#39044;&#27979;&#24615;&#25968;&#25454;&#25366;&#25496;&#25552;&#20379;&#20102;&#22810;&#31181;&#39118;&#36895;&#39044;&#27979;&#26041;&#27861;&#65292;&#20854;&#20013;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#26159;&#19968;&#31181;&#21487;&#38752;&#21644;&#20934;&#30830;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20256;&#32479;&#27169;&#22411;&#30456;&#27604;&#65292;ANN&#20855;&#26377;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#12290;&#39118;&#36895;&#39044;&#27979;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21457;&#29616;&#21462;&#20915;&#20110;&#36755;&#20837;&#21442;&#25968;&#21644;&#25152;&#20351;&#29992;&#30340;&#31639;&#27861;&#30340;&#26550;&#26500;&#31867;&#22411;&#12290;&#22240;&#27492;&#65292;&#36873;&#25321;&#26368;&#30456;&#20851;&#30340;&#36755;&#20837;&#21442;&#25968;&#26159;&#39118;&#36895;&#39044;&#27979;&#39046;&#22495;&#30340;&#37325;&#35201;&#30740;&#31350;&#39046;&#22495;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#26377;&#20004;&#20010;&#65306;&#39318;&#20808;&#23545;&#39118;&#33021;&#21644;&#39118;&#36895;&#39044;&#27979;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#24191;&#27867;&#30340;&#22238;&#39038;&#12290;&#35752;&#35770;&#21644;&#20998;&#26512;&#20351;&#29992;Relief&#31639;&#27861;&#36827;&#34892;&#29305;&#24449;&#36873;&#25321;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Wind power generated by wind has non-schedule nature due to stochastic nature of meteorological variable. Hence energy business and control of wind power generation requires prediction of wind speed (WS) from few seconds to different time steps in advance. To deal with prediction shortcomings, various WS prediction methods have been used. Predictive data mining offers variety of methods for WS predictions where artificial neural network (ANN) is one of the reliable and accurate methods. It is observed from the result of this study that ANN gives better accuracy in comparison conventional model. The accuracy of WS prediction models is found to be dependent on input parameters and architecture type algorithms utilized. So the selection of most relevant input parameters is important research area in WS predicton field. The objective of the paper is twofold: first extensive review of ANN for wind power and WS prediction is carried out. Discussion and analysis of feature selection using Rel
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35757;&#32451;&#19981;&#21516;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#23454;&#29616;&#20102;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#24038;&#21491;&#21322;&#29699;&#19987;&#38376;&#21270;&#25511;&#21046;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#22312;&#19981;&#21516;&#30340;&#36816;&#21160;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#21327;&#35843;&#24615;&#12289;&#36816;&#21160;&#25928;&#29575;&#21644;&#20301;&#32622;&#31283;&#23450;&#24615;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2401.14057</link><description>&lt;p&gt;
&#24038;/&#21491;&#33041;&#12289;&#20154;&#31867;&#36816;&#21160;&#25511;&#21046;&#21450;&#23545;&#26426;&#22120;&#20154;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Left/Right Brain, human motor control and the implications for robotics. (arXiv:2401.14057v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14057
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35757;&#32451;&#19981;&#21516;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#23454;&#29616;&#20102;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#24038;&#21491;&#21322;&#29699;&#19987;&#38376;&#21270;&#25511;&#21046;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#22312;&#19981;&#21516;&#30340;&#36816;&#21160;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#21327;&#35843;&#24615;&#12289;&#36816;&#21160;&#25928;&#29575;&#21644;&#20301;&#32622;&#31283;&#23450;&#24615;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#36816;&#21160;&#25511;&#21046;&#22120;&#30456;&#23545;&#20256;&#32479;&#25511;&#21046;&#26041;&#27861;&#20855;&#26377;&#21508;&#31181;&#20248;&#28857;&#65292;&#28982;&#32780;&#30001;&#20110;&#20854;&#26080;&#27861;&#20135;&#29983;&#21487;&#38752;&#30340;&#31934;&#30830;&#36816;&#21160;&#65292;&#22240;&#27492;&#23578;&#26410;&#24471;&#21040;&#24191;&#27867;&#37319;&#29992;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#31181;&#21452;&#20391;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20316;&#20026;&#36816;&#21160;&#20219;&#21153;&#30340;&#25511;&#21046;&#31995;&#32479;&#12290;&#25105;&#20204;&#26088;&#22312;&#23454;&#29616;&#31867;&#20284;&#20110;&#20154;&#31867;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#35266;&#23519;&#21040;&#30340;&#21322;&#29699;&#19987;&#38376;&#21270;&#65306;&#20248;&#21183;&#31995;&#32479;&#65288;&#36890;&#24120;&#26159;&#21491;&#25163;&#12289;&#24038;&#21322;&#29699;&#65289;&#25797;&#38271;&#21327;&#35843;&#21644;&#36816;&#21160;&#25928;&#29575;&#30340;&#20219;&#21153;&#65292;&#32780;&#38750;&#20248;&#21183;&#31995;&#32479;&#22312;&#38656;&#35201;&#20301;&#32622;&#31283;&#23450;&#24615;&#30340;&#20219;&#21153;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#30340;&#25439;&#22833;&#20989;&#25968;&#23545;&#21322;&#29699;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#19987;&#38376;&#21270;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#20855;&#26377;&#19987;&#38376;&#21270;&#21322;&#29699;&#21644;&#26080;&#19987;&#38376;&#21270;&#21322;&#29699;&#12289;&#20855;&#26377;&#21322;&#29699;&#38388;&#36830;&#25509;&#65288;&#20195;&#34920;&#29983;&#29289;&#23398;&#33041;&#26725;&#65289;&#21644;&#26080;&#21322;&#29699;&#38388;&#36830;&#25509;&#12289;&#20855;&#26377;&#19987;&#38376;&#21270;&#21644;&#26080;&#19987;&#38376;&#21270;&#30340;&#21333;&#20391;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Network movement controllers promise a variety of advantages over conventional control methods however they are not widely adopted due to their inability to produce reliably precise movements. This research explores a bilateral neural network architecture as a control system for motor tasks. We aimed to achieve hemispheric specialisation similar to what is observed in humans across different tasks; the dominant system (usually the right hand, left hemisphere) excels at tasks involving coordination and efficiency of movement, and the non-dominant system performs better at tasks requiring positional stability. Specialisation was achieved by training the hemispheres with different loss functions tailored toward the expected behaviour of the respective hemispheres. We compared bilateral models with and without specialised hemispheres, with and without inter-hemispheric connectivity (representing the biological Corpus Callosum), and unilateral models with and without specialisation. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#26032;&#30340;&#20108;&#27425;&#32422;&#26463;&#65292;&#23558;LipSDP&#26041;&#27861;&#25193;&#23637;&#21040;&#26012;&#29575;&#38480;&#21046;&#20043;&#22806;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#35813;&#26041;&#27861;&#30340;&#20027;&#35201;&#38480;&#21046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.14033</link><description>&lt;p&gt;
&#25193;&#23637;LipSDP&#20197;&#36229;&#36234;&#38480;&#21046;&#26012;&#29575;&#28608;&#27963;&#20989;&#25968;&#30340;&#26032;&#20108;&#27425;&#32422;&#26463;
&lt;/p&gt;
&lt;p&gt;
Novel Quadratic Constraints for Extending LipSDP beyond Slope-Restricted Activations. (arXiv:2401.14033v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14033
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26032;&#30340;&#20108;&#27425;&#32422;&#26463;&#65292;&#23558;LipSDP&#26041;&#27861;&#25193;&#23637;&#21040;&#26012;&#29575;&#38480;&#21046;&#20043;&#22806;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#35813;&#26041;&#27861;&#30340;&#20027;&#35201;&#38480;&#21046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#21322;&#23450;&#35268;&#21010;&#65288;SDP&#65289;&#25216;&#26415;&#22312;&#20026;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#20934;&#30830;&#30340;Lipschitz&#30028;&#38480;&#26041;&#38754;&#26174;&#31034;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;LipSDP&#26041;&#27861;&#65288;Fazlyab&#31561;&#65292;2019&#65289;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#20197;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20445;&#35777;&#19979;&#35745;&#31639;&#30340;&#26368;&#19981;&#20445;&#23432;&#30340;Lipschitz&#19978;&#30028;&#12290;&#28982;&#32780;&#65292;LipSDP&#30340;&#19968;&#20010;&#20027;&#35201;&#38480;&#21046;&#26159;&#20854;&#20844;&#24335;&#35201;&#27714;&#28608;&#27963;&#20989;&#25968;&#22312;[0,1]&#19978;&#20855;&#26377;&#38480;&#21046;&#26012;&#29575;&#65292;&#20174;&#32780;&#38459;&#27490;&#20854;&#36827;&#19968;&#27493;&#29992;&#20110;&#26356;&#19968;&#33324;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#22914;GroupSort&#12289;MaxMin&#21644;Householder&#12290;&#20363;&#22914;&#65292;&#21487;&#20197;&#23558;MaxMin&#28608;&#27963;&#37325;&#26032;&#20889;&#25104;&#27531;&#24046;ReLU&#32593;&#32476;&#12290;&#20294;&#26159;&#65292;&#23558;LipSDP&#30452;&#25509;&#24212;&#29992;&#20110;&#32467;&#26524;&#27531;&#24046;ReLU&#32593;&#32476;&#26159;&#20445;&#23432;&#30340;&#65292;&#29978;&#33267;&#26080;&#27861;&#24674;&#22797;MaxMin&#28608;&#27963;&#26159;1-Lipschitz&#30340;&#24050;&#30693;&#20107;&#23454;&#12290;&#26412;&#25991;&#24357;&#21512;&#20102;&#36825;&#19968;&#24046;&#36317;&#65292;&#23558;LipSDP&#25193;&#23637;&#21040;&#26012;&#29575;&#38480;&#21046;&#20043;&#22806;&#30340;&#28608;&#27963;&#20989;&#25968;&#19978;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;GroupSort&#12289;Ma&#30340;&#26032;&#20108;&#27425;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, semidefinite programming (SDP) techniques have shown great promise in providing accurate Lipschitz bounds for neural networks. Specifically, the LipSDP approach (Fazlyab et al., 2019) has received much attention and provides the least conservative Lipschitz upper bounds that can be computed with polynomial time guarantees. However, one main restriction of LipSDP is that its formulation requires the activation functions to be slope-restricted on $[0,1]$, preventing its further use for more general activation functions such as GroupSort, MaxMin, and Householder. One can rewrite MaxMin activations for example as residual ReLU networks. However, a direct application of LipSDP to the resultant residual ReLU networks is conservative and even fails in recovering the well-known fact that the MaxMin activation is 1-Lipschitz. Our paper bridges this gap and extends LipSDP beyond slope-restricted activation functions. To this end, we provide novel quadratic constraints for GroupSort, Ma
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;&#21487;&#20256;&#36882;&#30340;&#36890;&#29992;&#22855;&#24322;&#21521;&#37327;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#38544;&#34255;&#23618;&#20013;&#21033;&#29992;&#25130;&#26029;&#24130;&#36845;&#20195;&#26469;&#33719;&#24471;&#31232;&#30095;&#30340;$(p,q)$-&#22855;&#24322;&#21521;&#37327;&#12290;&#22312;ImageNet&#22522;&#20934;&#39564;&#35777;&#23376;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#20165;&#30772;&#22351;5%&#30340;&#20687;&#32032;&#21644;&#21033;&#29992;256&#20010;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#36798;&#21040;&#19982;&#23494;&#38598;&#22522;&#32447;&#30456;&#24403;&#30340;&#24858;&#24324;&#29575;&#36229;&#36807;50%&#30340;&#32467;&#26524;&#12290;&#21516;&#26102;&#65292;&#35813;&#25915;&#20987;&#26041;&#27861;&#19981;&#24433;&#21709;&#20154;&#31867;&#35299;&#20915;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#26500;&#36896;&#30340;&#25200;&#21160;&#26159;&#39640;&#24230;&#21487;&#20256;&#36882;&#30340;&#12290;</title><link>http://arxiv.org/abs/2401.14031</link><description>&lt;p&gt;
&#31232;&#30095;&#21487;&#20256;&#36882;&#30340;&#36890;&#29992;&#22855;&#24322;&#21521;&#37327;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Sparse and Transferable Universal Singular Vectors Attack. (arXiv:2401.14031v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14031
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;&#21487;&#20256;&#36882;&#30340;&#36890;&#29992;&#22855;&#24322;&#21521;&#37327;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#38544;&#34255;&#23618;&#20013;&#21033;&#29992;&#25130;&#26029;&#24130;&#36845;&#20195;&#26469;&#33719;&#24471;&#31232;&#30095;&#30340;$(p,q)$-&#22855;&#24322;&#21521;&#37327;&#12290;&#22312;ImageNet&#22522;&#20934;&#39564;&#35777;&#23376;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#20165;&#30772;&#22351;5%&#30340;&#20687;&#32032;&#21644;&#21033;&#29992;256&#20010;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#36798;&#21040;&#19982;&#23494;&#38598;&#22522;&#32447;&#30456;&#24403;&#30340;&#24858;&#24324;&#29575;&#36229;&#36807;50%&#30340;&#32467;&#26524;&#12290;&#21516;&#26102;&#65292;&#35813;&#25915;&#20987;&#26041;&#27861;&#19981;&#24433;&#21709;&#20154;&#31867;&#35299;&#20915;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#26500;&#36896;&#30340;&#25200;&#21160;&#26159;&#39640;&#24230;&#21487;&#20256;&#36882;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#25915;&#20987;&#21644;&#27169;&#22411;&#30340;&#33030;&#24369;&#24615;&#30740;&#31350;&#26159;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#22522;&#26412;&#26041;&#21521;&#20043;&#19968;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#27169;&#22411;&#30340;&#33030;&#24369;&#24615;&#29616;&#35937;&#65292;&#20102;&#35299;&#20854;&#32972;&#21518;&#30340;&#26426;&#21046;&#23545;&#20110;&#25913;&#21892;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31232;&#30095;&#36890;&#29992;&#30333;&#30418;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#25130;&#26029;&#24130;&#36845;&#20195;&#65292;&#20026;&#38597;&#21487;&#27604;&#30697;&#38453;&#30340;&#38544;&#34255;&#23618;&#30340;$(p,q)$-&#22855;&#24322;&#21521;&#37327;&#25552;&#20379;&#31232;&#30095;&#24615;&#12290;&#22312;ImageNet&#22522;&#20934;&#39564;&#35777;&#23376;&#38598;&#19978;&#65292;&#25105;&#20204;&#22312;&#21508;&#31181;&#35774;&#32622;&#19979;&#20998;&#26512;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#19982;&#23494;&#38598;&#22522;&#32447;&#30456;&#24403;&#30340;&#32467;&#26524;&#65292;&#24858;&#24324;&#29575;&#36229;&#36807;50%&#65292;&#20294;&#21482;&#30772;&#22351;&#20102;5%&#30340;&#20687;&#32032;&#65292;&#24182;&#21033;&#29992;256&#20010;&#26679;&#26412;&#36827;&#34892;&#25200;&#21160;&#25311;&#21512;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#25509;&#21463;&#26356;&#39640;&#30340;&#25915;&#20987;&#24133;&#24230;&#65292;&#32780;&#19981;&#24433;&#21709;&#20154;&#31867;&#35299;&#20915;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#26500;&#36896;&#30340;&#25200;&#21160;&#26159;&#39640;&#24230;&#21487;&#20256;&#36882;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The research in the field of adversarial attacks and models' vulnerability is one of the fundamental directions in modern machine learning. Recent studies reveal the vulnerability phenomenon, and understanding the mechanisms behind this is essential for improving neural network characteristics and interpretability. In this paper, we propose a novel sparse universal white-box adversarial attack. Our approach is based on truncated power iteration providing sparsity to $(p,q)$-singular vectors of the hidden layers of Jacobian matrices. Using the ImageNet benchmark validation subset, we analyze the proposed method in various settings, achieving results comparable to dense baselines with more than a 50% fooling rate while damaging only 5% of pixels and utilizing 256 samples for perturbation fitting. We also show that our algorithm admits higher attack magnitude without affecting the human ability to solve the task. Furthermore, we investigate that the constructed perturbations are highly tr
&lt;/p&gt;</description></item><item><title>&#21521;&#31639;&#27861;&#30340;&#31995;&#32479;&#29702;&#35770;&#36808;&#36827;&#65292;&#23558;&#31639;&#27861;&#35270;&#20026;&#24320;&#25918;&#30340;&#21160;&#24577;&#31995;&#32479;&#19982;&#20854;&#20182;&#31639;&#27861;&#12289;&#29289;&#29702;&#31995;&#32479;&#12289;&#20154;&#31867;&#25110;&#25968;&#25454;&#24211;&#20132;&#20114;&#65292;&#25552;&#20379;&#23453;&#36149;&#30340;&#27934;&#35265;&#12290;</title><link>http://arxiv.org/abs/2401.14029</link><description>&lt;p&gt;
&#21521;&#31639;&#27861;&#30340;&#31995;&#32479;&#29702;&#35770;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
Towards a Systems Theory of Algorithms. (arXiv:2401.14029v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14029
&lt;/p&gt;
&lt;p&gt;
&#21521;&#31639;&#27861;&#30340;&#31995;&#32479;&#29702;&#35770;&#36808;&#36827;&#65292;&#23558;&#31639;&#27861;&#35270;&#20026;&#24320;&#25918;&#30340;&#21160;&#24577;&#31995;&#32479;&#19982;&#20854;&#20182;&#31639;&#27861;&#12289;&#29289;&#29702;&#31995;&#32479;&#12289;&#20154;&#31867;&#25110;&#25968;&#25454;&#24211;&#20132;&#20114;&#65292;&#25552;&#20379;&#23453;&#36149;&#30340;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#25968;&#20540;&#31639;&#27861;&#34987;&#35270;&#20026;&#29420;&#31435;&#30340;&#20195;&#30721;&#29255;&#27573;&#65292;&#23616;&#38480;&#20110;``in silico''&#23384;&#22312;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#35266;&#28857;&#19981;&#36866;&#29992;&#20110;&#35768;&#22810;&#29616;&#20195;&#35745;&#31639;&#26041;&#27861;&#65292;&#22914;&#25511;&#21046;&#12289;&#23398;&#20064;&#25110;&#20248;&#21270;&#20013;&#65292;&#20854;&#20013;``in vivo''&#31639;&#27861;&#19982;&#20854;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#12290;&#36825;&#20123;``&#24320;&#25918;''&#30340;&#20363;&#23376;&#21253;&#25324;&#21508;&#31181;&#23454;&#26102;&#22522;&#20110;&#20248;&#21270;&#30340;&#25511;&#21046;&#31574;&#30053;&#12289;&#24378;&#21270;&#23398;&#20064;&#12289;&#20915;&#31574;&#26550;&#26500;&#12289;&#22312;&#32447;&#20248;&#21270;&#31561;&#31561;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#22312;&#23398;&#20064;&#25110;&#20248;&#21270;&#20013;&#65292;``&#38381;&#21512;''&#30340;&#31639;&#27861;&#20063;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#25277;&#35937;&#20026;&#20855;&#26377;&#30456;&#20114;&#20316;&#29992;&#30340;&#21160;&#24577;&#27169;&#22359;&#21644;&#31649;&#36947;&#30340;&#22359;&#22270;&#12290;&#22312;&#36825;&#31687;&#35266;&#28857;&#25991;&#31456;&#20013;&#65292;&#25105;&#20204;&#38416;&#36848;&#20102;&#25105;&#20204;&#23545;&#21363;&#23558;&#21457;&#23637;&#30340;``&#31639;&#27861;&#30340;&#31995;&#32479;&#29702;&#35770;''&#30340;&#24895;&#26223;&#65292;&#24182;&#20027;&#24352;&#23558;&#31639;&#27861;&#35270;&#20026;&#19982;&#20854;&#20182;&#31639;&#27861;&#12289;&#29289;&#29702;&#31995;&#32479;&#12289;&#20154;&#31867;&#25110;&#25968;&#25454;&#24211;&#20132;&#20114;&#30340;&#24320;&#25918;&#21160;&#24577;&#31995;&#32479;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#31995;&#32479;&#29702;&#35770;&#30340;&#20254;&#19979;&#24320;&#21457;&#30340;&#22810;&#31181;&#24037;&#20855;&#20063;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditionally, numerical algorithms are seen as isolated pieces of code confined to an {\em in silico} existence. However, this perspective is not appropriate for many modern computational approaches in control, learning, or optimization, wherein {\em in vivo} algorithms interact with their environment. Examples of such {\em open} include various real-time optimization-based control strategies, reinforcement learning, decision-making architectures, online optimization, and many more. Further, even {\em closed} algorithms in learning or optimization are increasingly abstracted in block diagrams with interacting dynamic modules and pipelines. In this opinion paper, we state our vision on a to-be-cultivated {\em systems theory of algorithms} and argue in favour of viewing algorithms as open dynamical systems interacting with other algorithms, physical systems, humans, or databases. Remarkably, the manifold tools developed under the umbrella of systems theory also provide valuable insights
&lt;/p&gt;</description></item><item><title>&#32852;&#37030;&#23398;&#20064;&#23384;&#22312;&#25197;&#26354;&#24494;&#35843;&#29305;&#24449;&#21644;&#25439;&#23475;&#27169;&#22411;&#25239;&#20998;&#24067;&#40065;&#26834;&#24615;&#30340;&#39118;&#38505;&#65292;&#20026;&#27492;&#25105;&#20204;&#24341;&#20837;&#20102;GNP&#31639;&#27861;&#26469;&#20445;&#35777;&#27169;&#22411;&#22312;&#30446;&#26631;&#20998;&#24067;&#19978;&#30340;&#20934;&#30830;&#24615;&#19981;&#20250;&#19979;&#38477;&#12290;</title><link>http://arxiv.org/abs/2401.14027</link><description>&lt;p&gt;
&#38754;&#21521;&#39046;&#22495;&#29305;&#23450;&#25968;&#25454;&#31232;&#32570;&#24615;&#21644;&#38544;&#31169;&#38382;&#39064;&#30340;&#32852;&#37030;&#23398;&#20064;&#30340;&#39118;&#38505;&#65306;&#25197;&#26354;&#24494;&#35843;&#29305;&#24449;&#24182;&#38477;&#20302;&#25239;&#20998;&#24067;&#40065;&#26834;&#24615;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Risk of Federated Learning to Skew Fine-Tuning Features and Underperform Out-of-Distribution Robustness. (arXiv:2401.14027v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14027
&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#23384;&#22312;&#25197;&#26354;&#24494;&#35843;&#29305;&#24449;&#21644;&#25439;&#23475;&#27169;&#22411;&#25239;&#20998;&#24067;&#40065;&#26834;&#24615;&#30340;&#39118;&#38505;&#65292;&#20026;&#27492;&#25105;&#20204;&#24341;&#20837;&#20102;GNP&#31639;&#27861;&#26469;&#20445;&#35777;&#27169;&#22411;&#22312;&#30446;&#26631;&#20998;&#24067;&#19978;&#30340;&#20934;&#30830;&#24615;&#19981;&#20250;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35299;&#20915;&#19982;&#39046;&#22495;&#29305;&#23450;&#25968;&#25454;&#38598;&#30456;&#20851;&#30340;&#31232;&#32570;&#24615;&#21644;&#38544;&#31169;&#38382;&#39064;&#65292;&#32852;&#37030;&#23398;&#20064;&#19982;&#24494;&#35843;&#30340;&#25972;&#21512;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#23454;&#38469;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#32852;&#37030;&#23398;&#20064;&#23384;&#22312;&#25197;&#26354;&#24494;&#35843;&#29305;&#24449;&#21644;&#25439;&#23475;&#27169;&#22411;&#30340;&#25239;&#20998;&#24067;&#40065;&#26834;&#24615;&#30340;&#39118;&#38505;&#12290;&#36890;&#36807;&#24341;&#20837;&#19977;&#20010;&#40065;&#26834;&#24615;&#25351;&#26631;&#24182;&#22312;&#19981;&#21516;&#30340;&#40065;&#26834;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#27169;&#22411;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#22810;&#26679;&#24615;&#12289;&#21487;&#36716;&#31227;&#24615;&#21644;&#20559;&#31163;&#31243;&#24230;&#26469;&#38416;&#26126;&#36825;&#20123;&#29616;&#35937;&#12290;&#20026;&#20102;&#20943;&#36731;&#32852;&#37030;&#23398;&#20064;&#23545;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;GNP&#65292;&#19968;&#31181;&#22522;&#20110;\textbf{G}eneral \textbf{N}oisy \textbf{P}rojection&#30340;&#40065;&#26834;&#31639;&#27861;&#65292;&#30830;&#20445;&#22312;&#30446;&#26631;&#20998;&#24067;&#19978;&#20934;&#30830;&#24615;&#19981;&#20250;&#19979;&#38477;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22686;&#24378;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#20851;&#38190;&#31574;&#30053;&#26159;&#23558;&#40065;&#26834;&#24615;&#20174;&#39044;&#35757;&#32451;&#27169;&#22411;&#36716;&#31227;&#21040;&#24494;&#35843;&#27169;&#22411;&#20013;&#65292;&#24182;&#32467;&#21512;&#28155;&#21152;&#19968;&#20010;&#23567;&#30340;...
&lt;/p&gt;
&lt;p&gt;
To tackle the scarcity and privacy issues associated with domain-specific datasets, the integration of federated learning in conjunction with fine-tuning has emerged as a practical solution. However, our findings reveal that federated learning has the risk of skewing fine-tuning features and compromising the out-of-distribution robustness of the model. By introducing three robustness indicators and conducting experiments across diverse robust datasets, we elucidate these phenomena by scrutinizing the diversity, transferability, and deviation within the model feature space. To mitigate the negative impact of federated learning on model robustness, we introduce GNP, a \underline{G}eneral \underline{N}oisy \underline{P}rojection-based robust algorithm, ensuring no deterioration of accuracy on the target distribution. Specifically, the key strategy for enhancing model robustness entails the transfer of robustness from the pre-trained model to the fine-tuned model, coupled with adding a sma
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;DNA&#24207;&#21015;&#20998;&#26512;&#30340;&#26032;&#22411;&#20998;&#31867;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#21387;&#32553;&#31639;&#27861;&#39640;&#25928;&#22788;&#29702;&#21644;&#20998;&#31867;&#22522;&#22240;&#32452;&#24207;&#21015;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#20934;&#30830;&#24615;&#39640;&#65292;&#32780;&#19988;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#33410;&#30465;&#35745;&#31639;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2401.14025</link><description>&lt;p&gt;
DNA&#24207;&#21015;&#21387;&#32553;&#22120;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
DNA Sequence Classification with Compressors. (arXiv:2401.14025v1 [q-bio.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14025
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;DNA&#24207;&#21015;&#20998;&#26512;&#30340;&#26032;&#22411;&#20998;&#31867;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#21387;&#32553;&#31639;&#27861;&#39640;&#25928;&#22788;&#29702;&#21644;&#20998;&#31867;&#22522;&#22240;&#32452;&#24207;&#21015;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#20934;&#30830;&#24615;&#39640;&#65292;&#32780;&#19988;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#33410;&#30465;&#35745;&#31639;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;DNA&#24207;&#21015;&#20998;&#31867;&#30740;&#31350;&#20511;&#37492;&#20102;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#22312;&#23545;&#22797;&#26434;&#30340;&#22522;&#22240;&#32452;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#26102;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#20934;&#30830;&#24615;&#12290;&#20854;&#20013;&#65292;&#20687;k-mer&#35745;&#25968;&#36825;&#26679;&#30340;&#26041;&#27861;&#24050;&#34987;&#35777;&#26126;&#22312;&#21306;&#20998;&#31867;&#20284;&#40657;&#29481;&#29481;&#12289;&#29399;&#21644;&#20154;&#31867;&#31561;&#19981;&#21516;&#29289;&#31181;&#30340;&#24207;&#21015;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#65292;&#22312;&#24403;&#20195;&#22522;&#22240;&#32452;&#30740;&#31350;&#20013;&#24050;&#32463;&#25104;&#20026;&#24120;&#29992;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#22312;&#21487;&#25193;&#23637;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#27743;&#31561;&#20154;&#22522;&#20110;&#21387;&#32553;&#22120;&#30340;&#26080;&#21442;&#25968;&#20998;&#31867;&#26041;&#27861;&#30340;&#26032;&#22411;&#25913;&#36827;&#65292;&#19987;&#38376;&#29992;&#20110;DNA&#24207;&#21015;&#20998;&#26512;&#12290;&#36825;&#31181;&#21019;&#26032;&#26041;&#27861;&#21033;&#29992;&#22810;&#31181;&#21387;&#32553;&#31639;&#27861;&#65288;&#22914;Gzip&#12289;Brotli&#21644;LZMA&#65289;&#26469;&#39640;&#25928;&#22788;&#29702;&#21644;&#20998;&#31867;&#22522;&#22240;&#32452;&#24207;&#21015;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#19982;&#29616;&#26377;&#25216;&#26415;&#20445;&#25345;&#19968;&#33268;&#65292;&#32780;&#19988;&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#21152;&#36164;&#28304;&#39640;&#25928;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies in DNA sequence classification have leveraged sophisticated machine learning techniques, achieving notable accuracy in categorizing complex genomic data. Among these, methods such as k-mer counting have proven effective in distinguishing sequences from varied species like chimpanzees, dogs, and humans, becoming a staple in contemporary genomic research. However, these approaches often demand extensive computational resources, posing a challenge in terms of scalability and efficiency. Addressing this issue, our study introduces a novel adaptation of Jiang et al.'s compressor-based, parameter-free classification method, specifically tailored for DNA sequence analysis. This innovative approach utilizes a variety of compression algorithms, such as Gzip, Brotli, and LZMA, to efficiently process and classify genomic sequences. Not only does this method align with the current state-of-the-art in terms of accuracy, but it also offers a more resource-efficient alternative to trad
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;RaLMSpec&#65292;&#36825;&#26159;&#19968;&#20010;&#20351;&#29992;&#25512;&#27979;&#21152;&#36895;&#26816;&#32034;&#22686;&#24378;&#22411;&#35821;&#35328;&#27169;&#22411;&#26381;&#21153;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25512;&#27979;&#24335;&#26816;&#32034;&#21644;&#25209;&#37327;&#39564;&#35777;&#25552;&#20379;&#20102;&#36890;&#29992;&#30340;&#21152;&#36895;&#25928;&#26524;&#65292;&#24182;&#36890;&#36807;&#36827;&#19968;&#27493;&#20248;&#21270;&#21644;&#24182;&#21457;&#22788;&#29702;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.14021</link><description>&lt;p&gt;
&#20351;&#29992;&#25512;&#27979;&#21152;&#36895;&#26816;&#32034;&#22686;&#24378;&#22411;&#35821;&#35328;&#27169;&#22411;&#26381;&#21153;
&lt;/p&gt;
&lt;p&gt;
Accelerating Retrieval-Augmented Language Model Serving with Speculation. (arXiv:2401.14021v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14021
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;RaLMSpec&#65292;&#36825;&#26159;&#19968;&#20010;&#20351;&#29992;&#25512;&#27979;&#21152;&#36895;&#26816;&#32034;&#22686;&#24378;&#22411;&#35821;&#35328;&#27169;&#22411;&#26381;&#21153;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25512;&#27979;&#24335;&#26816;&#32034;&#21644;&#25209;&#37327;&#39564;&#35777;&#25552;&#20379;&#20102;&#36890;&#29992;&#30340;&#21152;&#36895;&#25928;&#26524;&#65292;&#24182;&#36890;&#36807;&#36827;&#19968;&#27493;&#20248;&#21270;&#21644;&#24182;&#21457;&#22788;&#29702;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;RaLM&#65289;&#36890;&#36807;&#23558;&#38750;&#21442;&#25968;&#30340;&#30693;&#35782;&#24211;&#19982;&#21442;&#25968;&#21270;&#30340;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#24050;&#32463;&#23637;&#31034;&#20986;&#35299;&#20915;&#30693;&#35782;&#23494;&#38598;&#22411;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#30340;&#28508;&#21147;&#12290;&#19982;&#23545;&#23436;&#20840;&#21442;&#25968;&#21270;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#19981;&#21516;&#65292;RaLM&#22312;&#36866;&#24212;&#26368;&#26032;&#25968;&#25454;&#21644;&#26356;&#22909;&#30340;&#26469;&#28304;&#24402;&#23646;&#26426;&#21046;&#26041;&#38754;&#20855;&#26377;&#20302;&#25104;&#26412;&#30340;&#20248;&#21183;&#12290;&#22312;&#20247;&#22810;&#30340;RaLM&#26041;&#27861;&#20013;&#65292;&#36845;&#20195;&#24335;RaLM&#30001;&#20110;&#26816;&#32034;&#22120;&#19982;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#26356;&#39057;&#32321;&#30340;&#20114;&#21160;&#32780;&#20855;&#26377;&#26356;&#22909;&#30340;&#29983;&#25104;&#36136;&#37327;&#12290;&#23613;&#31649;&#26377;&#36825;&#20123;&#22909;&#22788;&#65292;&#36845;&#20195;&#24335;RaLM&#36890;&#24120;&#20250;&#22240;&#20026;&#39057;&#32321;&#30340;&#26816;&#32034;&#27493;&#39588;&#32780;&#36935;&#21040;&#39640;&#24320;&#38144;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RaLMSpec&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#25512;&#27979;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25512;&#27979;&#24335;&#26816;&#32034;&#21644;&#25209;&#37327;&#39564;&#35777;&#65292;&#33021;&#22815;&#22312;&#20445;&#25345;&#30456;&#21516;&#27169;&#22411;&#36755;&#20986;&#30340;&#21516;&#26102;&#65292;&#25552;&#20379;&#36890;&#29992;&#21152;&#36895;&#30340;&#25928;&#26524;&#12290;&#36890;&#36807;&#36827;&#19968;&#27493;&#32467;&#21512;&#39044;&#21462;&#12289;&#26368;&#20339;&#25512;&#27979;&#27493;&#24133;&#35843;&#24230;&#22120;&#21644;&#24322;&#27493;&#39564;&#35777;&#65292;RaLMSpec&#33021;&#22815;&#33258;&#21160;&#21033;&#29992;&#24182;&#21457;&#24615;&#21644;&#24182;&#34892;&#24615;&#26469;&#26368;&#22823;&#31243;&#24230;&#22320;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Retrieval-augmented language models (RaLM) have demonstrated the potential to solve knowledge-intensive natural language processing (NLP) tasks by combining a non-parametric knowledge base with a parametric language model. Instead of fine-tuning a fully parametric model, RaLM excels at its low-cost adaptation to the latest data and better source attribution mechanisms. Among various RaLM approaches, iterative RaLM delivers a better generation quality due to a more frequent interaction between the retriever and the language model. Despite the benefits, iterative RaLM usually encounters high overheads due to the frequent retrieval step. To this end, we propose RaLMSpec, a speculation-inspired framework that provides generic speed-up over iterative RaLM while preserving the same model outputs through speculative retrieval and batched verification. By further incorporating prefetching, optimal speculation stride scheduler, and asynchronous verification, RaLMSpec can automatically exploit t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ADAPTER&#30340;&#33258;&#36866;&#24212;&#21464;&#21387;&#22120;&#32593;&#32476;&#65292;&#29992;&#20110;&#36328;&#39046;&#22495;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#12290;&#20511;&#21161;&#21452;&#21521;&#20132;&#21449;&#27880;&#24847;&#21147;&#21644;&#26631;&#31614;&#24179;&#28369;&#26041;&#27861;&#65292;ADAPTER&#33021;&#22815;&#22312;&#23384;&#22312;&#39046;&#22495;&#36716;&#31227;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#29305;&#24449;&#65292;&#24182;&#36991;&#20813;&#30417;&#30563;&#23849;&#28291;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;BSCD-FSL&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#31168;&#65292;&#36229;&#36807;&#20102;&#20043;&#21069;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.13987</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#36866;&#24212;&#21464;&#21387;&#22120;&#32593;&#32476;&#36827;&#34892;&#36328;&#39046;&#22495;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Cross-Domain Few-Shot Learning via Adaptive Transformer Networks. (arXiv:2401.13987v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13987
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ADAPTER&#30340;&#33258;&#36866;&#24212;&#21464;&#21387;&#22120;&#32593;&#32476;&#65292;&#29992;&#20110;&#36328;&#39046;&#22495;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#12290;&#20511;&#21161;&#21452;&#21521;&#20132;&#21449;&#27880;&#24847;&#21147;&#21644;&#26631;&#31614;&#24179;&#28369;&#26041;&#27861;&#65292;ADAPTER&#33021;&#22815;&#22312;&#23384;&#22312;&#39046;&#22495;&#36716;&#31227;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#29305;&#24449;&#65292;&#24182;&#36991;&#20813;&#30417;&#30563;&#23849;&#28291;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;BSCD-FSL&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#31168;&#65292;&#36229;&#36807;&#20102;&#20043;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#20381;&#36182;&#20110;&#22522;&#20219;&#21153;&#21644;&#30446;&#26631;&#20219;&#21153;&#20043;&#38388;&#30340;&#30456;&#21516;&#39046;&#22495;&#30340;&#20551;&#35774;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#21464;&#21387;&#22120;&#32593;&#32476;&#65288;ADAPTER&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#22788;&#29702;&#22522;&#20219;&#21153;&#21644;&#30446;&#26631;&#20219;&#21153;&#20043;&#38388;&#23384;&#22312;&#22823;&#39046;&#22495;&#36716;&#31227;&#30340;&#36328;&#39046;&#22495;&#23569;&#26679;&#26412;&#23398;&#20064;&#12290;ADAPTER&#22522;&#20110;&#21452;&#21521;&#20132;&#21449;&#27880;&#24847;&#21147;&#30340;&#24605;&#24819;&#26500;&#24314;&#65292;&#20197;&#23398;&#20064;&#20004;&#20010;&#39046;&#22495;&#20043;&#38388;&#30340;&#21487;&#36716;&#31227;&#29305;&#24449;&#12290;&#25152;&#25552;&#20986;&#30340;&#26550;&#26500;&#20351;&#29992;DINO&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#20135;&#29983;&#22810;&#26679;&#30340;&#12289;&#36739;&#23569;&#20559;&#35265;&#30340;&#29305;&#24449;&#65292;&#20197;&#36991;&#20813;&#30417;&#30563;&#23849;&#28291;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#26631;&#31614;&#24179;&#28369;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#21516;&#26102;&#32771;&#34385;&#25509;&#36817;&#26679;&#26412;&#30340;&#39044;&#27979;&#26631;&#31614;&#65292;&#25552;&#39640;&#39044;&#27979;&#30340;&#19968;&#33268;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;ADAPTER&#30340;&#24615;&#33021;&#22312;BSCD-FSL&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#20005;&#26684;&#35780;&#20272;&#65292;&#23427;&#20197;&#26174;&#33879;&#30340;&#20248;&#21183;&#36229;&#36234;&#20102;&#20043;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most few-shot learning works rely on the same domain assumption between the base and the target tasks, hindering their practical applications. This paper proposes an adaptive transformer network (ADAPTER), a simple but effective solution for cross-domain few-shot learning where there exist large domain shifts between the base task and the target task. ADAPTER is built upon the idea of bidirectional cross-attention to learn transferable features between the two domains. The proposed architecture is trained with DINO to produce diverse, and less biased features to avoid the supervision collapse problem. Furthermore, the label smoothing approach is proposed to improve the consistency and reliability of the predictions by also considering the predicted labels of the close samples in the embedding space. The performance of ADAPTER is rigorously evaluated in the BSCD-FSL benchmarks in which it outperforms prior arts with significant margins.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35299;&#37322;&#19968;&#33268;&#24615;&#24494;&#35843;&#26041;&#27861;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#30456;&#20851;&#31034;&#20363;&#19978;&#29983;&#25104;&#26356;&#19968;&#33268;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#30456;&#23545;&#25552;&#39640;&#20102;10.0%&#30340;&#35299;&#37322;&#19968;&#33268;&#24615;&#65292;&#24182;&#19988;&#33021;&#22815;&#27867;&#21270;&#21040;&#20854;&#20182;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2401.13986</link><description>&lt;p&gt;
&#36890;&#36807;&#35299;&#37322;&#19968;&#33268;&#24615;&#24494;&#35843;&#23454;&#29616;&#19968;&#33268;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Towards Consistent Natural-Language Explanations via Explanation-Consistency Finetuning. (arXiv:2401.13986v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13986
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35299;&#37322;&#19968;&#33268;&#24615;&#24494;&#35843;&#26041;&#27861;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#30456;&#20851;&#31034;&#20363;&#19978;&#29983;&#25104;&#26356;&#19968;&#33268;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#30456;&#23545;&#25552;&#39640;&#20102;10.0%&#30340;&#35299;&#37322;&#19968;&#33268;&#24615;&#65292;&#24182;&#19988;&#33021;&#22815;&#27867;&#21270;&#21040;&#20854;&#20182;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#24120;&#33021;&#22815;&#29983;&#25104;&#20196;&#20154;&#20449;&#26381;&#12289;&#27969;&#30021;&#30340;&#35299;&#37322;&#12290;&#28982;&#32780;&#65292;&#19982;&#20154;&#31867;&#19981;&#21516;&#65292;&#23427;&#20204;&#22312;&#19981;&#21516;&#36755;&#20837;&#19978;&#29983;&#25104;&#30340;&#35299;&#37322;&#24120;&#24120;&#19981;&#19968;&#33268;&#12290;&#20363;&#22914;&#65292;LLM&#22312;&#22238;&#31572;&#38382;&#39064;&#8220;&#40635;&#38592;&#33021;&#39134;&#21527;&#65311;&#8221;&#26102;&#21487;&#33021;&#29983;&#25104;&#35299;&#37322;&#8220;&#25152;&#26377;&#40479;&#37117;&#33021;&#39134;&#8221;&#65292;&#20294;&#21516;&#26102;&#22312;&#22238;&#31572;&#19982;&#20043;&#30456;&#20851;&#30340;&#38382;&#39064;&#8220;&#20225;&#40517;&#33021;&#39134;&#21527;&#65311;&#8221;&#26102;&#22238;&#31572;&#8220;&#19981;&#34892;&#8221;&#12290;&#35299;&#37322;&#24212;&#35813;&#22312;&#30456;&#20851;&#31034;&#20363;&#20013;&#20445;&#25345;&#19968;&#33268;&#65292;&#20197;&#20415;&#35753;&#20154;&#31867;&#33021;&#22815;&#27169;&#25311;LLM&#22312;&#22810;&#20010;&#31034;&#20363;&#19978;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#35299;&#37322;&#19968;&#33268;&#24615;&#24494;&#35843;&#65288;EC-finetuning&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#36866;&#24212;LLM&#22312;&#30456;&#20851;&#31034;&#20363;&#19978;&#29983;&#25104;&#26356;&#19968;&#33268;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#12290;EC-finetuning&#21253;&#25324;&#22312;&#32463;&#36807;&#31934;&#24515;&#26500;&#24314;&#30340;&#21253;&#21547;&#19968;&#33268;&#35299;&#37322;&#30340;&#21512;&#25104;&#25968;&#25454;&#19978;&#24494;&#35843;LLM&#12290;&#22312;&#21508;&#31181;&#19981;&#21516;&#39046;&#22495;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#65292;EC-finetuning&#22312;&#22235;&#20010;&#24494;&#35843;&#25968;&#25454;&#38598;&#19978;&#30456;&#23545;&#25552;&#39640;&#20102;10.0%&#30340;&#35299;&#37322;&#19968;&#33268;&#24615;&#65292;&#24182;&#19988;&#22312;&#19971;&#20010;&#22806;&#37096;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) often generate convincing, fluent explanations. However, different from humans, they often generate inconsistent explanations on different inputs. For example, an LLM may generate the explanation "all birds can fly" when answering the question "Can sparrows fly?" but meanwhile answer "no" to the related question "Can penguins fly?". Explanations should be consistent across related examples so that they allow a human to simulate the LLM's decision process on multiple examples. We propose explanation-consistency finetuning (EC-finetuning), a method that adapts LLMs to generate more consistent natural-language explanations on related examples. EC-finetuning involves finetuning LLMs on synthetic data that is carefully constructed to contain consistent explanations. Across a variety of question-answering datasets in various domains, EC-finetuning yields a 10.0% relative explanation consistency improvement on four finetuning datasets, and generalizes to seven out
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Leeroo&#32534;&#25490;&#22120;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#38598;&#25104;&#22810;&#20010;&#35757;&#32451;&#36807;&#30340;LLMs&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#19968;&#20010;&#26032;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;&#35813;&#32534;&#25490;&#22120;&#22312;&#24615;&#33021;&#19978;&#19982;Mixtral&#27169;&#22411;&#30456;&#24403;&#65292;&#24182;&#19988;&#25104;&#26412;&#21482;&#26377;&#20854;&#19977;&#20998;&#20043;&#20108;&#12290;&#24403;&#20801;&#35768;&#26356;&#39640;&#30340;&#25104;&#26412;&#26102;&#65292;Leeroo&#32534;&#25490;&#22120;&#30340;&#20934;&#30830;&#24615;&#36229;&#36807;&#20102;Mixtral&#27169;&#22411;&#65292;&#24182;&#19988;&#24403;&#38598;&#25104;GPT4&#26102;&#36827;&#19968;&#27493;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2401.13979</link><description>&lt;p&gt;
Leeroo Orchestrator: &#36890;&#36807;&#27169;&#22411;&#38598;&#25104;&#25552;&#39640;LLMs&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Leeroo Orchestrator: Elevating LLMs Performance Through Model Integration. (arXiv:2401.13979v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13979
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Leeroo&#32534;&#25490;&#22120;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#38598;&#25104;&#22810;&#20010;&#35757;&#32451;&#36807;&#30340;LLMs&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#19968;&#20010;&#26032;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;&#35813;&#32534;&#25490;&#22120;&#22312;&#24615;&#33021;&#19978;&#19982;Mixtral&#27169;&#22411;&#30456;&#24403;&#65292;&#24182;&#19988;&#25104;&#26412;&#21482;&#26377;&#20854;&#19977;&#20998;&#20043;&#20108;&#12290;&#24403;&#20801;&#35768;&#26356;&#39640;&#30340;&#25104;&#26412;&#26102;&#65292;Leeroo&#32534;&#25490;&#22120;&#30340;&#20934;&#30830;&#24615;&#36229;&#36807;&#20102;Mixtral&#27169;&#22411;&#65292;&#24182;&#19988;&#24403;&#38598;&#25104;GPT4&#26102;&#36827;&#19968;&#27493;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26550;&#26500;&#65292;&#21033;&#29992;&#22810;&#20010;&#35757;&#32451;&#36807;&#30340;LLMs&#30340;&#38598;&#20307;&#30693;&#35782;&#65292;&#21019;&#24314;&#19968;&#20010;&#26032;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;&#35813;&#26694;&#26550;&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#22522;&#20110;LLM&#30340;&#32534;&#25490;&#22120;&#65292;&#33021;&#22815;&#36873;&#25321;&#26368;&#20339;&#30340;&#24213;&#23618;LLM&#19987;&#23478;&#36827;&#34892;&#20219;&#21153;&#25191;&#34892;&#12290;&#21463;&#21040;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#33258;&#25105;&#23545;&#24328;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#26597;&#35810;&#29983;&#25104;&#12289;&#32534;&#25490;&#21644;&#35780;&#20272;&#30340;&#24490;&#29615;&#65292;&#20026;&#32534;&#25490;&#22120;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#20027;&#35201;&#38024;&#23545;MMLU&#22522;&#20934;&#65292;&#22312;Hugging Face&#19978;&#20351;&#29992;&#20102;&#20855;&#26377;7B&#12289;13B&#21644;34B&#21442;&#25968;&#30340;&#27169;&#22411;&#12290;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;Leeroo&#32534;&#25490;&#22120;&#23454;&#29616;&#20102;&#19982;Mixtral&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#20294;&#21482;&#20135;&#29983;&#20102;&#20854;&#25104;&#26412;&#30340;&#19977;&#20998;&#20043;&#20108;&#12290;&#27492;&#22806;&#65292;&#22686;&#21152;&#20801;&#35768;&#30340;&#25104;&#26412;&#36229;&#36807;&#20102;Mixtral&#30340;&#20934;&#30830;&#24615;&#65292;&#36798;&#21040;&#20102;75.9%&#30340;&#20934;&#30830;&#24615;&#12290;&#24403;&#23558;GPT4&#38598;&#25104;&#21040;&#24213;&#23618;&#27169;&#22411;&#27744;&#20013;&#26102;&#65292;&#36827;&#19968;&#27493;&#25552;&#21319;&#20063;&#24471;&#21040;&#20102;&#35266;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose an architecture to harness the collective knowledge of multiple trained LLMs to create a new state-of-the-art. At the core of this framework is a LLM-based orchestrator that is adept at picking the right underlying LLM experts for optimal task execution. Inspired by self-play in reinforcement learning, we created a loop of query generation, orchestration, and evaluation to generate training data for the orchestrator. Our evaluation focused on the MMLU benchmark, employing models with 7B, 13B, and 34B parameters available on Hugging Face. The results demonstrate new state-of-the-art open-source models: Our Leeroo orchestrator achieves performance on par with the Mixtral model while incurring only two-thirds of its cost. Moreover, increasing the allowed cost surpasses Mixtral's accuracy by over 5% at the same cost level, reaching an accuracy of 75.9%. Further enhancements were observed when integrating GPT4 into the underlying model pool. The Leeroo orchestrator
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36816;&#29992;&#32479;&#35745;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#35843;&#26597;&#21360;&#24230;&#22478;&#24066;&#29677;&#21152;&#32599;&#23572;&#30340;&#20302;&#25910;&#20837;&#21644;&#20302;&#20013;&#20135;&#38454;&#32423;&#23478;&#24237;&#22312;&#20132;&#36890;&#26041;&#24335;&#36873;&#25321;&#26041;&#38754;&#30340;&#20915;&#31574;&#34892;&#20026;&#65292;&#24182;&#21457;&#29616;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#26368;&#20339;&#12290;</title><link>http://arxiv.org/abs/2401.13977</link><description>&lt;p&gt;
&#22312;&#21360;&#24230;&#22478;&#24066;&#29677;&#21152;&#32599;&#23572;&#20013;&#20351;&#29992;&#32479;&#35745;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#35780;&#20272;&#20132;&#36890;&#26041;&#24335;&#36873;&#25321;&#30340;&#20915;&#23450;&#22240;&#32032;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Determinants of Mode Choice Using Statistical and Machine Learning Techniques in the Indian Megacity of Bengaluru. (arXiv:2401.13977v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13977
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36816;&#29992;&#32479;&#35745;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#35843;&#26597;&#21360;&#24230;&#22478;&#24066;&#29677;&#21152;&#32599;&#23572;&#30340;&#20302;&#25910;&#20837;&#21644;&#20302;&#20013;&#20135;&#38454;&#32423;&#23478;&#24237;&#22312;&#20132;&#36890;&#26041;&#24335;&#36873;&#25321;&#26041;&#38754;&#30340;&#20915;&#31574;&#34892;&#20026;&#65292;&#24182;&#21457;&#29616;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#26041;&#24335;&#36873;&#25321;&#30340;&#20915;&#31574;&#23545;&#20132;&#36890;&#35268;&#21010;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#20256;&#32479;&#19978;&#20351;&#29992;&#31163;&#25955;&#36873;&#25321;&#27169;&#22411;&#31561;&#32479;&#35745;&#23398;&#20064;&#25216;&#26415;&#65292;&#20294;&#30001;&#20110;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#20855;&#26377;&#26356;&#39640;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#26368;&#36817;&#22312;&#20132;&#36890;&#35268;&#21010;&#32773;&#20013;&#24341;&#36215;&#20102;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;ML&#27169;&#22411;&#30340;&#40657;&#30418;&#29305;&#24615;&#22312;&#35299;&#37322;&#24615;&#26041;&#38754;&#23384;&#22312;&#26174;&#33879;&#25361;&#25112;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#20915;&#31574;&#21644;&#25919;&#31574;&#21046;&#23450;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#29677;&#21152;&#32599;&#23572;&#24066;$1350$&#20010;&#20302;&#25910;&#20837;&#21644;&#20302;&#20013;&#20135;&#38454;&#32423;&#23478;&#24237;&#30340;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;&#22810;&#39033;&#24335;&#36923;&#36753;&#27169;&#22411;&#21644;&#20915;&#31574;&#26641;&#12289;&#38543;&#26426;&#26862;&#26519;&#12289;&#26497;&#31471;&#26799;&#24230;&#25552;&#21319;&#21644;&#25903;&#25345;&#21521;&#37327;&#26426;&#31561;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#26469;&#35843;&#26597;&#20132;&#36890;&#26041;&#24335;&#36873;&#25321;&#34892;&#20026;&#30340;&#20915;&#31574;&#12290;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#65292;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#34920;&#29616;&#26368;&#20339;&#65288;&#35757;&#32451;&#25968;&#25454;&#20026;$0.788$&#65292;&#27979;&#35797;&#25968;&#25454;&#20026;$0.605$&#65289;&#65292;&#20248;&#20110;&#25152;&#26377;&#20854;&#20182;&#27169;&#22411;&#12290;&#36825;&#39033;&#30740;&#31350;&#37319;&#29992;&#20102;&#29616;&#20195;&#35299;&#37322;&#24615;&#24037;&#20855;&#65292;&#20026;&#20915;&#31574;&#21046;&#23450;&#32773;&#25552;&#20379;&#20102;&#26377;&#20851;&#20132;&#36890;&#26041;&#24335;&#36873;&#25321;&#34892;&#20026;&#30340;&#28145;&#20837;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
The decision making involved behind the mode choice is critical for transportation planning. While statistical learning techniques like discrete choice models have been used traditionally, machine learning (ML) models have gained traction recently among the transportation planners due to their higher predictive performance. However, the black box nature of ML models pose significant interpretability challenges, limiting their practical application in decision and policy making. This study utilised a dataset of $1350$ households belonging to low and low-middle income bracket in the city of Bengaluru to investigate mode choice decision making behaviour using Multinomial logit model and ML classifiers like decision trees, random forests, extreme gradient boosting and support vector machines. In terms of accuracy, random forest model performed the best ($0.788$ on training data and $0.605$ on testing data) compared to all the other models. This research has adopted modern interpretability 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#27809;&#26377;&#26631;&#20934;Lipschitz&#36830;&#32493;&#24615;&#20551;&#35774;&#30340;&#38543;&#26426;&#24369;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#33258;&#36866;&#24212;&#27491;&#21017;&#21270;&#31574;&#30053;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19968;&#31867;&#24191;&#27867;&#30340;&#38543;&#26426;&#31639;&#27861;&#20855;&#26377;$\mathcal{O} ( 1 / \sqrt{K})$&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#24120;&#25968;&#30340;&#22833;&#36133;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.13971</link><description>&lt;p&gt;
&#36229;&#36807;Lipschitz&#36830;&#32493;&#24615;&#30340;&#38543;&#26426;&#24369;&#20984;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Stochastic Weakly Convex Optimization Beyond Lipschitz Continuity. (arXiv:2401.13971v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13971
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#27809;&#26377;&#26631;&#20934;Lipschitz&#36830;&#32493;&#24615;&#20551;&#35774;&#30340;&#38543;&#26426;&#24369;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#33258;&#36866;&#24212;&#27491;&#21017;&#21270;&#31574;&#30053;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19968;&#31867;&#24191;&#27867;&#30340;&#38543;&#26426;&#31639;&#27861;&#20855;&#26377;$\mathcal{O} ( 1 / \sqrt{K})$&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#24120;&#25968;&#30340;&#22833;&#36133;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#27809;&#26377;&#26631;&#20934;Lipschitz&#36830;&#32493;&#24615;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#30340;&#38543;&#26426;&#24369;&#20984;&#20248;&#21270;&#12290;&#22522;&#20110;&#26032;&#30340;&#33258;&#36866;&#24212;&#27491;&#21017;&#21270;&#65288;&#27493;&#38271;&#65289;&#31574;&#30053;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#31867;&#24191;&#27867;&#30340;&#38543;&#26426;&#31639;&#27861;&#65292;&#21253;&#25324;&#38543;&#26426;&#27425;&#26799;&#24230;&#27861;&#65292;&#20855;&#26377;$\mathcal{O} ( 1 / \sqrt{K})$&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#22833;&#36133;&#29575;&#20026;&#24120;&#25968;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#22522;&#20110;&#30456;&#24403;&#24369;&#30340;&#20551;&#35774;&#65306;Lipschitz&#21442;&#25968;&#21487;&#20197;&#36890;&#36807;$\|x\|$&#30340;&#19968;&#33324;&#22686;&#38271;&#20989;&#25968;&#26469;&#38480;&#21046;&#65292;&#25110;&#32773;&#36890;&#36807;&#29420;&#31435;&#38543;&#26426;&#26679;&#26412;&#36827;&#34892;&#23616;&#37096;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper considers stochastic weakly convex optimization without the standard Lipschitz continuity assumption. Based on new adaptive regularization (stepsize) strategies, we show that a wide class of stochastic algorithms, including the stochastic subgradient method, preserve the $\mathcal{O} ( 1 / \sqrt{K})$ convergence rate with constant failure rate. Our analyses rest on rather weak assumptions: the Lipschitz parameter can be either bounded by a general growth function of $\|x\|$ or locally estimated through independent random samples.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Meta-Transformer Networks&#65288;MANTRA&#65289;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#21160;&#24577;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#24555;&#36895;&#21644;&#24930;&#36895;&#23398;&#20064;&#22120;&#30340;&#32467;&#21512;&#65292;&#20197;&#21450;&#20351;&#29992;&#36890;&#29992;&#34920;&#31034;&#36716;&#25442;&#23618;&#65292;&#23454;&#29616;&#23545;&#21160;&#24577;&#29615;&#22659;&#30340;&#24555;&#36895;&#36866;&#24212;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#20110;&#22522;&#20934;&#31639;&#27861;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2401.13968</link><description>&lt;p&gt;
&#21160;&#24577;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65306;&#22522;&#20110;&#20803;&#36716;&#25442;&#32593;&#32476;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Dynamic Long-Term Time-Series Forecasting via Meta Transformer Networks. (arXiv:2401.13968v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13968
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Meta-Transformer Networks&#65288;MANTRA&#65289;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#21160;&#24577;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#24555;&#36895;&#21644;&#24930;&#36895;&#23398;&#20064;&#22120;&#30340;&#32467;&#21512;&#65292;&#20197;&#21450;&#20351;&#29992;&#36890;&#29992;&#34920;&#31034;&#36716;&#25442;&#23618;&#65292;&#23454;&#29616;&#23545;&#21160;&#24577;&#29615;&#22659;&#30340;&#24555;&#36895;&#36866;&#24212;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#20110;&#22522;&#20934;&#31639;&#27861;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#36341;&#20013;&#65292;&#21487;&#38752;&#30340;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#38754;&#20020;&#35832;&#22810;&#25361;&#25112;&#65292;&#22914;&#20302;&#35745;&#31639;&#21644;&#20869;&#23384;&#21344;&#29992;&#20197;&#21450;&#23545;&#21160;&#24577;&#23398;&#20064;&#29615;&#22659;&#30340;&#40065;&#26834;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Meta-Transformer Networks&#65288;MANTRA&#65289;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#21160;&#24577;&#30340;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#12290;MANTRA&#20381;&#36182;&#20110;&#24555;&#36895;&#21644;&#24930;&#36895;&#23398;&#20064;&#22120;&#30340;&#27010;&#24565;&#65292;&#20854;&#20013;&#19968;&#32452;&#24555;&#36895;&#23398;&#20064;&#22120;&#23398;&#20064;&#25968;&#25454;&#20998;&#24067;&#30340;&#19981;&#21516;&#26041;&#38754;&#65292;&#21516;&#26102;&#24555;&#36895;&#36866;&#24212;&#21464;&#21270;&#12290;&#24930;&#36895;&#23398;&#20064;&#22120;&#20026;&#24555;&#36895;&#23398;&#20064;&#22120;&#23450;&#21046;&#36866;&#24403;&#30340;&#34920;&#31034;&#12290;&#36890;&#36807;&#20351;&#29992;&#36890;&#29992;&#34920;&#31034;&#36716;&#25442;&#23618;&#20135;&#29983;&#20219;&#21153;&#36866;&#24212;&#24615;&#34920;&#31034;&#65292;&#24182;&#20855;&#26377;&#23569;&#37327;&#21442;&#25968;&#65292;&#23454;&#29616;&#23545;&#21160;&#24577;&#29615;&#22659;&#30340;&#24555;&#36895;&#36866;&#24212;&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#20855;&#26377;&#19981;&#21516;&#39044;&#27979;&#38271;&#24230;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#21464;&#37327;&#21644;&#21333;&#21464;&#37327;&#35774;&#32622;&#19979;&#33267;&#23569;&#27604;&#22522;&#20934;&#31639;&#27861;&#25913;&#36827;&#20102;3&#65285;&#12290;MANTRA&#30340;&#28304;&#20195;&#30721;&#21487;&#22312;&#38142;&#25509;&#20013;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
A reliable long-term time-series forecaster is highly demanded in practice but comes across many challenges such as low computational and memory footprints as well as robustness against dynamic learning environments. This paper proposes Meta-Transformer Networks (MANTRA) to deal with the dynamic long-term time-series forecasting tasks. MANTRA relies on the concept of fast and slow learners where a collection of fast learners learns different aspects of data distributions while adapting quickly to changes. A slow learner tailors suitable representations to fast learners. Fast adaptations to dynamic environments are achieved using the universal representation transformer layers producing task-adapted representations with a small number of parameters. Our experiments using four datasets with different prediction lengths demonstrate the advantage of our approach with at least $3\%$ improvements over the baseline algorithms for both multivariate and univariate settings. Source codes of MANT
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#26469;&#23454;&#29616;&#28857;&#23545;&#28857;&#33021;&#28304;&#20132;&#26131;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#24110;&#21161;&#33258;&#21160;&#21270;&#28040;&#36153;&#32773;&#30340;&#31454;&#26631;&#21644;&#31649;&#29702;&#65292;&#24182;&#35299;&#20915;&#20102;&#21487;&#20877;&#29983;&#33021;&#28304;&#38646;&#36793;&#38469;&#25104;&#26412;&#21644;&#29289;&#29702;&#32422;&#26463;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.13947</link><description>&lt;p&gt;
&#32593;&#32476;&#21270;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#28857;&#23545;&#28857;&#33021;&#28304;&#20132;&#26131;
&lt;/p&gt;
&lt;p&gt;
Networked Multiagent Reinforcement Learning for Peer-to-Peer Energy Trading. (arXiv:2401.13947v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13947
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#26469;&#23454;&#29616;&#28857;&#23545;&#28857;&#33021;&#28304;&#20132;&#26131;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#24110;&#21161;&#33258;&#21160;&#21270;&#28040;&#36153;&#32773;&#30340;&#31454;&#26631;&#21644;&#31649;&#29702;&#65292;&#24182;&#35299;&#20915;&#20102;&#21487;&#20877;&#29983;&#33021;&#28304;&#38646;&#36793;&#38469;&#25104;&#26412;&#21644;&#29289;&#29702;&#32422;&#26463;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#20998;&#24067;&#24335;&#21487;&#20877;&#29983;&#33021;&#28304;&#21644;&#33021;&#37327;&#20648;&#23384;&#36164;&#28304;&#36827;&#34892;&#28857;&#23545;&#28857;&#33021;&#28304;&#20132;&#26131;&#34987;&#38271;&#26399;&#35748;&#20026;&#26159;&#25552;&#39640;&#33021;&#28304;&#31995;&#32479;&#24377;&#24615;&#21644;&#21487;&#25345;&#32493;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#28040;&#36153;&#32773;&#21644;&#33258;&#32473;&#33258;&#36275;&#32773;&#65288;&#20855;&#26377;&#33021;&#28304;&#21457;&#30005;&#36164;&#28304;&#30340;&#20154;&#65289;&#32570;&#20047;&#36827;&#34892;&#37325;&#22797;&#28857;&#23545;&#28857;&#20132;&#26131;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#24182;&#19988;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#38646;&#36793;&#38469;&#25104;&#26412;&#22312;&#30830;&#23450;&#20844;&#24179;&#24066;&#22330;&#20215;&#26684;&#26041;&#38754;&#23384;&#22312;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26694;&#26550;&#65292;&#20197;&#24110;&#21161;&#33258;&#21160;&#21270;&#28040;&#36153;&#32773;&#23545;&#22826;&#38451;&#33021;&#20809;&#20239;&#21644;&#33021;&#37327;&#20648;&#23384;&#36164;&#28304;&#30340;&#31454;&#26631;&#21644;&#31649;&#29702;&#65292;&#22312;&#19968;&#31181;&#21033;&#29992;&#20379;&#38656;&#27604;&#30340;&#28857;&#23545;&#28857;&#28165;&#31639;&#26426;&#21046;&#19979;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;MARL&#26694;&#26550;&#22914;&#20309;&#25972;&#21512;&#29289;&#29702;&#32593;&#32476;&#32422;&#26463;&#20197;&#23454;&#29616;&#30005;&#21387;&#25511;&#21046;&#65292;&#20174;&#32780;&#30830;&#20445;&#28857;&#23545;&#28857;&#33021;&#28304;&#20132;&#26131;&#30340;&#29289;&#29702;&#21487;&#34892;&#24615;&#65292;&#24182;&#20026;&#30495;&#23454;&#19990;&#30028;&#30340;&#23454;&#26045;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Utilizing distributed renewable and energy storage resources in local distribution networks via peer-to-peer (P2P) energy trading has long been touted as a solution to improve energy systems' resilience and sustainability. Consumers and prosumers (those who have energy generation resources), however, do not have the expertise to engage in repeated P2P trading, and the zero-marginal costs of renewables present challenges in determining fair market prices. To address these issues, we propose multi-agent reinforcement learning (MARL) frameworks to help automate consumers' bidding and management of their solar PV and energy storage resources, under a specific P2P clearing mechanism that utilizes the so-called supply-demand ratio. In addition, we show how the MARL frameworks can integrate physical network constraints to realize voltage control, hence ensuring physical feasibility of the P2P energy trading and paving way for real-world implementations.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#38024;&#23545;&#37325;&#24615;&#25233;&#37057;&#38556;&#30861;(MDD)&#20013;&#30340;&#22870;&#21169;&#22788;&#29702;&#24322;&#24120;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#21644;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#32467;&#21512;&#65292;&#25506;&#32034;&#20915;&#31574;&#21046;&#23450;&#36807;&#31243;&#20013;&#30340;&#23398;&#20064;&#31574;&#30053;&#21160;&#24577;&#23545;&#20010;&#20307;&#22870;&#21169;&#23398;&#20064;&#33021;&#21147;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.13929</link><description>&lt;p&gt;
&#20351;&#29992;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26469;&#21457;&#29616;&#20915;&#31574;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning with Hidden Markov Models for Discovering Decision-Making Dynamics. (arXiv:2401.13929v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#38024;&#23545;&#37325;&#24615;&#25233;&#37057;&#38556;&#30861;(MDD)&#20013;&#30340;&#22870;&#21169;&#22788;&#29702;&#24322;&#24120;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#21644;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#32467;&#21512;&#65292;&#25506;&#32034;&#20915;&#31574;&#21046;&#23450;&#36807;&#31243;&#20013;&#30340;&#23398;&#20064;&#31574;&#30053;&#21160;&#24577;&#23545;&#20010;&#20307;&#22870;&#21169;&#23398;&#20064;&#33021;&#21147;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#22797;&#26434;&#21644;&#24322;&#36136;&#24615;&#65292;&#37325;&#24615;&#25233;&#37057;&#38556;&#30861;(MDD)&#22312;&#35786;&#26029;&#21644;&#27835;&#30103;&#26041;&#38754;&#23384;&#22312;&#25361;&#25112;&#12290;&#26032;&#30340;&#35777;&#25454;&#34920;&#26126;&#22870;&#21169;&#22788;&#29702;&#24322;&#24120;&#21487;&#33021;&#20316;&#20026;MDD&#30340;&#34892;&#20026;&#26631;&#35760;&#12290;&#20026;&#20102;&#34913;&#37327;&#22870;&#21169;&#22788;&#29702;&#65292;&#24739;&#32773;&#25191;&#34892;&#28041;&#21450;&#20570;&#20986;&#36873;&#25321;&#25110;&#23545;&#19982;&#19981;&#21516;&#32467;&#26524;&#30456;&#20851;&#32852;&#30340;&#21050;&#28608;&#20316;&#20986;&#21453;&#24212;&#30340;&#22522;&#20110;&#35745;&#31639;&#26426;&#30340;&#34892;&#20026;&#20219;&#21153;&#12290;&#24378;&#21270;&#23398;&#20064;(RL)&#27169;&#22411;&#34987;&#25311;&#21512;&#20197;&#25552;&#21462;&#34913;&#37327;&#22870;&#21169;&#22788;&#29702;&#21508;&#20010;&#26041;&#38754;&#30340;&#21442;&#25968;&#65292;&#20197;&#34920;&#24449;&#24739;&#32773;&#22312;&#34892;&#20026;&#20219;&#21153;&#20013;&#30340;&#20915;&#31574;&#26041;&#24335;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#20165;&#22522;&#20110;&#21333;&#20010;RL&#27169;&#22411;&#30340;&#22870;&#21169;&#23398;&#20064;&#34920;&#24449;&#19981;&#36275;; &#30456;&#21453;&#65292;&#20915;&#31574;&#36807;&#31243;&#20013;&#21487;&#33021;&#23384;&#22312;&#22810;&#31181;&#31574;&#30053;&#20043;&#38388;&#30340;&#20999;&#25442;&#12290;&#19968;&#20010;&#37325;&#35201;&#30340;&#31185;&#23398;&#38382;&#39064;&#26159;&#20915;&#31574;&#21046;&#23450;&#20013;&#23398;&#20064;&#31574;&#30053;&#30340;&#21160;&#24577;&#22914;&#20309;&#24433;&#21709;MDD&#24739;&#32773;&#30340;&#22870;&#21169;&#23398;&#20064;&#33021;&#21147;&#12290;&#30001;&#27010;&#29575;&#22870;&#21169;&#20219;&#21153;(PRT)&#25152;&#21551;&#21457;
&lt;/p&gt;
&lt;p&gt;
Major depressive disorder (MDD) presents challenges in diagnosis and treatment due to its complex and heterogeneous nature. Emerging evidence indicates that reward processing abnormalities may serve as a behavioral marker for MDD. To measure reward processing, patients perform computer-based behavioral tasks that involve making choices or responding to stimulants that are associated with different outcomes. Reinforcement learning (RL) models are fitted to extract parameters that measure various aspects of reward processing to characterize how patients make decisions in behavioral tasks. Recent findings suggest the inadequacy of characterizing reward learning solely based on a single RL model; instead, there may be a switching of decision-making processes between multiple strategies. An important scientific question is how the dynamics of learning strategies in decision-making affect the reward learning ability of individuals with MDD. Motivated by the probabilistic reward task (PRT) wi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;3D-MoLM&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#32473;&#35821;&#35328;&#27169;&#22411;&#37197;&#22791;&#19968;&#20010;3D&#20998;&#23376;&#32534;&#30721;&#22120;&#65292;&#23454;&#29616;&#20102;&#23545;3D&#20998;&#23376;-&#25991;&#26412;&#30340;&#35299;&#37322;&#21644;&#20998;&#26512;&#65292;&#27492;&#27169;&#22411;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#12290;</title><link>http://arxiv.org/abs/2401.13923</link><description>&lt;p&gt;
&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#23545;3D&#20998;&#23376;-&#25991;&#26412;&#30340;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Towards 3D Molecule-Text Interpretation in Language Models. (arXiv:2401.13923v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13923
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;3D-MoLM&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#32473;&#35821;&#35328;&#27169;&#22411;&#37197;&#22791;&#19968;&#20010;3D&#20998;&#23376;&#32534;&#30721;&#22120;&#65292;&#23454;&#29616;&#20102;&#23545;3D&#20998;&#23376;-&#25991;&#26412;&#30340;&#35299;&#37322;&#21644;&#20998;&#26512;&#65292;&#27492;&#27169;&#22411;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#26377;&#30528;&#24456;&#22823;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23545;&#20110;&#29702;&#35299;3D&#20998;&#23376;&#32467;&#26500;&#30340;&#22266;&#26377;&#38480;&#21046;&#26497;&#22823;&#22320;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#29983;&#29289;&#20998;&#23376;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#20851;&#27880;&#20110;3D&#20998;&#23376;-&#25991;&#26412;&#35299;&#37322;&#65292;&#24182;&#25552;&#20986;3D-MoLM&#65306;3D&#20998;&#23376;&#35821;&#35328;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;3D-MoLM&#36890;&#36807;&#20026;LM&#37197;&#22791;&#19968;&#20010;3D&#20998;&#23376;&#32534;&#30721;&#22120;&#65292;&#20351;&#24471;LM&#33021;&#22815;&#35299;&#37322;&#21644;&#20998;&#26512;3D&#20998;&#23376;&#12290;&#36825;&#31181;&#38598;&#25104;&#26159;&#36890;&#36807;&#19968;&#20010;3D&#20998;&#23376;-&#25991;&#26412;&#25237;&#24433;&#22120;&#23454;&#29616;&#30340;&#65292;&#23427;&#36830;&#25509;&#20102;3D&#20998;&#23376;&#32534;&#30721;&#22120;&#30340;&#34920;&#31034;&#31354;&#38388;&#21644;LM&#30340;&#36755;&#20837;&#31354;&#38388;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#22686;&#24378;3D-MoLM&#22312;&#36328;&#27169;&#24577;&#20998;&#23376;&#29702;&#35299;&#21644;&#25351;&#20196;&#36319;&#38543;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#31934;&#24515;&#31574;&#21010;&#20102;&#19968;&#20010;&#20197;3D&#20998;&#23376;&#20026;&#20013;&#24515;&#30340;&#25351;&#24341;&#35843;&#25972;&#25968;&#25454;&#38598;--3D-MoIT&#12290;&#36890;&#36807;3D&#20998;&#23376;-&#25991;&#26412;&#23545;&#40784;&#21644;3D&#20998;&#23376;&#20013;&#24515;&#30340;&#25351;&#24341;&#35843;&#25972;&#65292;3D-MoLM&#24314;&#31435;&#20102;3D&#20998;&#23376;&#32534;&#30721;&#22120;&#21644;LM&#30340;&#38598;&#25104;&#12290;&#23427;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#26174;&#33879;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language Models (LMs) have greatly influenced diverse domains. However, their inherent limitation in comprehending 3D molecular structures has considerably constrained their potential in the biomolecular domain. To bridge this gap, we focus on 3D molecule-text interpretation, and propose 3D-MoLM: 3D-Molecular Language Modeling. Specifically, 3D-MoLM enables an LM to interpret and analyze 3D molecules by equipping the LM with a 3D molecular encoder. This integration is achieved by a 3D molecule-text projector, bridging the 3D molecular encoder's representation space and the LM's input space. Moreover, to enhance 3D-MoLM's ability of cross-modal molecular understanding and instruction following, we meticulously curated a 3D molecule-centric instruction tuning dataset -- 3D-MoIT. Through 3D molecule-text alignment and 3D molecule-centric instruction tuning, 3D-MoLM establishes an integration of 3D molecular encoder and LM. It significantly surpasses existing baselines on downstream tasks,
&lt;/p&gt;</description></item><item><title>LocMoE&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36335;&#30001;&#31574;&#30053;&#65292;&#36890;&#36807;&#23558;&#37096;&#20998;&#33410;&#28857;&#38388;&#36890;&#20449;&#36716;&#25442;&#20026;&#33410;&#28857;&#20869;&#36890;&#20449;&#65292;&#32467;&#21512;&#36127;&#36733;&#24179;&#34913;&#21644;&#23616;&#37096;&#24615;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.13920</link><description>&lt;p&gt;
LocMoE: &#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#30340;&#20302;&#24320;&#38144;MoE
&lt;/p&gt;
&lt;p&gt;
LocMoE: A Low-overhead MoE for Large Language Model Training. (arXiv:2401.13920v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13920
&lt;/p&gt;
&lt;p&gt;
LocMoE&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36335;&#30001;&#31574;&#30053;&#65292;&#36890;&#36807;&#23558;&#37096;&#20998;&#33410;&#28857;&#38388;&#36890;&#20449;&#36716;&#25442;&#20026;&#33410;&#28857;&#20869;&#36890;&#20449;&#65292;&#32467;&#21512;&#36127;&#36733;&#24179;&#34913;&#21644;&#23616;&#37096;&#24615;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#65288;MoE&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#37319;&#29992;&#30340;&#20998;&#24067;&#24335;&#21644;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#30001;&#20110;&#20854;&#33021;&#22815;&#26377;&#25928;&#31232;&#30095;&#21644;&#25193;&#23637;&#27169;&#22411;&#65292;&#22240;&#27492;&#22791;&#21463;&#38738;&#30544;&#12290;&#28982;&#32780;&#65292;MoE&#30340;&#24615;&#33021;&#21463;&#21040;&#36127;&#36733;&#19981;&#24179;&#34913;&#21644;&#20840;&#23545;&#20840;&#36890;&#20449;&#30340;&#39640;&#24310;&#36831;&#30340;&#38480;&#21046;&#65292;&#21516;&#26102;&#30001;&#20110;&#22823;&#37327;&#30340;&#19987;&#23478;&#23481;&#37327;&#23548;&#33268;&#30456;&#23545;&#20887;&#20313;&#30340;&#35745;&#31639;&#12290;&#36127;&#36733;&#19981;&#24179;&#34913;&#21487;&#33021;&#26159;&#30001;&#20110;&#29616;&#26377;&#36335;&#30001;&#31574;&#30053;&#22987;&#32456;&#20542;&#21521;&#20110;&#36873;&#25321;&#29305;&#23450;&#30340;&#19987;&#23478;&#23548;&#33268;&#30340;&#12290;&#20840;&#23545;&#20840;&#36807;&#31243;&#20013;&#39057;&#32321;&#30340;&#33410;&#28857;&#38388;&#36890;&#20449;&#20063;&#26174;&#33879;&#24310;&#38271;&#20102;&#35757;&#32451;&#26102;&#38388;&#12290;&#20026;&#20102;&#32531;&#35299;&#19978;&#36848;&#24615;&#33021;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36335;&#30001;&#31574;&#30053;&#65292;&#36890;&#36807;&#23558;&#37096;&#20998;&#33410;&#28857;&#38388;&#36890;&#20449;&#36716;&#25442;&#20026;&#33410;&#28857;&#20869;&#36890;&#20449;&#65292;&#32467;&#21512;&#36127;&#36733;&#24179;&#34913;&#21644;&#23616;&#37096;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#38416;&#26126;&#20102;&#19987;&#23478;&#23481;&#37327;&#30340;&#26368;&#23567;&#38408;&#20540;&#65292;&#36890;&#36807;&#23558;&#19987;&#23478;&#30340;&#38376;&#25511;&#26435;&#37325;&#19982;&#20998;&#37197;&#30340;&#26631;&#35760;&#20043;&#38388;&#30340;&#26368;&#22823;&#35282;&#20559;&#24046;&#35745;&#31639;&#20986;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Mixtures-of-Experts (MoE) model is a widespread distributed and integrated learning method for large language models (LLM), which is favored due to its ability to sparsify and expand models efficiently. However, the performance of MoE is limited by load imbalance and high latency of All-To-All communication, along with relatively redundant computation owing to large expert capacity. Load imbalance may result from existing routing policies that consistently tend to select certain experts. The frequent inter-node communication in the All-To-All procedure also significantly prolongs the training time. To alleviate the above performance problems, we propose a novel routing strategy that combines load balance and locality by converting partial inter-node communication to that of intra-node. Notably, we elucidate that there is a minimum threshold for expert capacity, calculated through the maximal angular deviation between the gating weights of the experts and the assigned tokens. We por
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35889;&#32858;&#31867;&#21644;&#20998;&#24067;&#30456;&#20284;&#24230;&#24230;&#37327;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#31163;&#25955;&#20998;&#24067;&#32858;&#31867;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#32447;&#24615;&#26368;&#20248;&#20256;&#36755;&#26500;&#24314;&#30456;&#20284;&#24230;&#30697;&#38453;&#65292;&#25105;&#20204;&#22312;&#32858;&#31867;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2401.13913</link><description>&lt;p&gt;
&#31163;&#25955;&#20998;&#24067;&#30340;&#35889;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Spectral Clustering for Discrete Distributions. (arXiv:2401.13913v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13913
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35889;&#32858;&#31867;&#21644;&#20998;&#24067;&#30456;&#20284;&#24230;&#24230;&#37327;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#31163;&#25955;&#20998;&#24067;&#32858;&#31867;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#32447;&#24615;&#26368;&#20248;&#20256;&#36755;&#26500;&#24314;&#30456;&#20284;&#24230;&#30697;&#38453;&#65292;&#25105;&#20204;&#22312;&#32858;&#31867;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#25955;&#20998;&#24067;&#32858;&#31867;&#65288;D2C&#65289;&#36890;&#24120;&#36890;&#36807;Wasserstein&#36136;&#24515;&#26041;&#27861;&#26469;&#35299;&#20915;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#19968;&#20010;&#20849;&#21516;&#30340;&#20551;&#35774;&#19979;&#24037;&#20316;&#65292;&#21363;&#32858;&#31867;&#21487;&#20197;&#36890;&#36807;&#36136;&#24515;&#24456;&#22909;&#22320;&#34920;&#31034;&#65292;&#20294;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#21487;&#33021;&#19981;&#25104;&#31435;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22522;&#20110;&#35889;&#32858;&#31867;&#21644;&#20998;&#24067;&#30456;&#20284;&#24230;&#24230;&#37327;&#65288;&#20363;&#22914;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#21644;Wasserstein&#36317;&#31163;&#65289;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;D2C&#38382;&#39064;&#12290;&#20026;&#20102;&#25552;&#39640;&#21487;&#25193;&#23637;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#32447;&#24615;&#26368;&#20248;&#20256;&#36755;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#39640;&#25928;&#22320;&#26500;&#24314;&#30456;&#20284;&#24230;&#30697;&#38453;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#65292;&#20445;&#35777;&#20102;&#25152;&#25552;&#26041;&#27861;&#22312;&#32858;&#31867;&#20998;&#24067;&#26041;&#38754;&#30340;&#25104;&#21151;&#12290;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#30495;&#23454;&#25968;&#25454;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#32858;&#31867;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#22823;&#22823;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discrete distribution clustering (D2C) was often solved by Wasserstein barycenter methods. These methods are under a common assumption that clusters can be well represented by barycenters, which may not hold in many real applications. In this work, we propose a simple yet effective framework based on spectral clustering and distribution affinity measures (e.g., maximum mean discrepancy and Wasserstein distance) for D2C. To improve the scalability, we propose to use linear optimal transport to construct affinity matrices efficiently on large datasets. We provide theoretical guarantees for the success of the proposed methods in clustering distributions. Experiments on synthetic and real data show that our methods outperform the baselines largely in terms of both clustering accuracy and computational efficiency.
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#23398;&#20064;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#26174;&#31034;&#20986;&#26174;&#33879;&#20248;&#21183;&#65292;&#20294;&#22312;&#30123;&#24773;&#39044;&#27979;&#26041;&#38754;&#20173;&#38754;&#20020;&#25361;&#25112;&#65292;&#22522;&#30784;&#27169;&#22411;&#30340;&#24320;&#21457;&#21487;&#20197;&#24110;&#21161;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.13912</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#21644;&#22522;&#30784;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey of Deep Learning and Foundation Models for Time Series Forecasting. (arXiv:2401.13912v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13912
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#26174;&#31034;&#20986;&#26174;&#33879;&#20248;&#21183;&#65292;&#20294;&#22312;&#30123;&#24773;&#39044;&#27979;&#26041;&#38754;&#20173;&#38754;&#20020;&#25361;&#25112;&#65292;&#22522;&#30784;&#27169;&#22411;&#30340;&#24320;&#21457;&#21487;&#20197;&#24110;&#21161;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#35768;&#22810;&#39046;&#22495;&#65292;&#28982;&#32780;&#20854;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#38754;&#30340;&#20248;&#21183;&#20986;&#29616;&#24471;&#27604;&#36739;&#24930;&#12290;&#26368;&#36817;&#65292;&#22312;&#33879;&#21517;&#30340;Makridakis (M)&#31454;&#36187;&#20013;&#65292;&#20256;&#32479;&#32479;&#35745;&#25110;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#28151;&#21512;&#27169;&#22411;&#20165;&#26368;&#36817;&#25104;&#20026;&#34920;&#29616;&#26368;&#20339;&#30340;&#27169;&#22411;&#12290;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#26550;&#26500;&#36827;&#27493;&#65288;&#20363;&#22914;&#65292;&#20855;&#26377;&#27880;&#24847;&#21147;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#12289;&#36716;&#25442;&#22120;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#65289;&#65292;&#28145;&#24230;&#23398;&#20064;&#24320;&#22987;&#23637;&#31034;&#20986;&#26174;&#33879;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#22312;&#30123;&#24773;&#39044;&#27979;&#39046;&#22495;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20173;&#38754;&#20020;&#25361;&#25112;&#65306;&#35757;&#32451;&#25968;&#25454;&#30340;&#26102;&#38388;&#24207;&#21015;&#38271;&#24230;&#19981;&#22815;&#65292;&#23545;&#32047;&#31215;&#30340;&#31185;&#23398;&#30693;&#35782;&#32570;&#20047;&#35748;&#35782;&#65292;&#20197;&#21450;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#20026;&#27492;&#65292;&#22522;&#30784;&#27169;&#22411;&#65288;&#20855;&#26377;&#24191;&#27867;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65289;&#30340;&#24320;&#21457;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#29702;&#35299;&#27169;&#24335;&#24182;&#33719;&#21462;&#21487;&#20197;&#24212;&#29992;&#20110;&#26032;&#30456;&#20851;&#38382;&#39064;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Learning has been successfully applied to many application domains, yet its advantages have been slow to emerge for time series forecasting. For example, in the well-known Makridakis (M) Competitions, hybrids of traditional statistical or machine learning techniques have only recently become the top performers. With the recent architectural advances in deep learning being applied to time series forecasting (e.g., encoder-decoders with attention, transformers, and graph neural networks), deep learning has begun to show significant advantages. Still, in the area of pandemic prediction, there remain challenges for deep learning models: the time series is not long enough for effective training, unawareness of accumulated scientific knowledge, and interpretability of the model. To this end, the development of foundation models (large deep learning models with extensive pre-training) allows models to understand patterns and acquire knowledge that can be applied to new related problems b
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#23618;&#27425;&#31526;&#21495;&#22238;&#24402;&#26041;&#27861;(UHiSR)&#65292;&#36890;&#36807;&#32467;&#21512;&#23618;&#27425;&#31070;&#32463;&#32593;&#32476;&#21644;&#31526;&#21495;&#22238;&#24402;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#25552;&#21462;&#21270;&#23398;&#30452;&#35266;&#30340;&#26497;&#24615;&#25351;&#25968;&#65292;&#24182;&#21457;&#29616;&#20102;&#23558;&#20998;&#23376;&#32467;&#26500;&#19982;&#33394;&#35889;&#34892;&#20026;&#32852;&#31995;&#36215;&#26469;&#30340;&#21487;&#35299;&#37322;&#26041;&#31243;&#12290;</title><link>http://arxiv.org/abs/2401.13904</link><description>&lt;p&gt;
&#36171;&#20104;&#26426;&#22120;&#20687;&#21270;&#23398;&#23478;&#19968;&#26679;&#24605;&#32771;&#30340;&#33021;&#21147;&#65306;&#29992;&#23618;&#27425;&#31526;&#21495;&#22238;&#24402;&#25581;&#31034;&#20998;&#23376;&#32467;&#26500;&#26497;&#24615;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Empowering Machines to Think Like Chemists: Unveiling Molecular Structure-Polarity Relationships with Hierarchical Symbolic Regression. (arXiv:2401.13904v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13904
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#23618;&#27425;&#31526;&#21495;&#22238;&#24402;&#26041;&#27861;(UHiSR)&#65292;&#36890;&#36807;&#32467;&#21512;&#23618;&#27425;&#31070;&#32463;&#32593;&#32476;&#21644;&#31526;&#21495;&#22238;&#24402;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#25552;&#21462;&#21270;&#23398;&#30452;&#35266;&#30340;&#26497;&#24615;&#25351;&#25968;&#65292;&#24182;&#21457;&#29616;&#20102;&#23558;&#20998;&#23376;&#32467;&#26500;&#19982;&#33394;&#35889;&#34892;&#20026;&#32852;&#31995;&#36215;&#26469;&#30340;&#21487;&#35299;&#37322;&#26041;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34180;&#23618;&#33394;&#35889;&#27861;(TLC)&#26159;&#20998;&#23376;&#26497;&#24615;&#20998;&#26512;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#25216;&#26415;&#12290;&#23613;&#31649;&#20854;&#37325;&#35201;&#24615;&#65292;&#23545;&#20110;TLC&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#23588;&#20854;&#26159;&#37027;&#20123;&#30001;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#20173;&#28982;&#38754;&#20020;&#25361;&#25112;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#35201;&#20040;&#21033;&#29992;&#39640;&#32500;&#20998;&#23376;&#25351;&#32441;&#65292;&#35201;&#20040;&#21033;&#29992;&#22522;&#20110;&#39046;&#22495;&#30693;&#35782;&#30340;&#29305;&#24449;&#24037;&#31243;&#65292;&#32463;&#24120;&#38754;&#20020;&#34920;&#36798;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#20043;&#38388;&#30340;&#20004;&#38590;&#36873;&#25321;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#26080;&#30417;&#30563;&#30340;&#23618;&#27425;&#31526;&#21495;&#22238;&#24402;(UHiSR)&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#23618;&#27425;&#31070;&#32463;&#32593;&#32476;&#21644;&#31526;&#21495;&#22238;&#24402;&#12290;UHiSR&#33258;&#21160;&#25552;&#21462;&#21270;&#23398;&#30452;&#35266;&#30340;&#26497;&#24615;&#25351;&#25968;&#65292;&#24182;&#21457;&#29616;&#23558;&#20998;&#23376;&#32467;&#26500;&#19982;&#33394;&#35889;&#34892;&#20026;&#32852;&#31995;&#36215;&#26469;&#30340;&#21487;&#35299;&#37322;&#26041;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Thin-layer chromatography (TLC) is a crucial technique in molecular polarity analysis. Despite its importance, the interpretability of predictive models for TLC, especially those driven by artificial intelligence, remains a challenge. Current approaches, utilizing either high-dimensional molecular fingerprints or domain-knowledge-driven feature engineering, often face a dilemma between expressiveness and interpretability. To bridge this gap, we introduce Unsupervised Hierarchical Symbolic Regression (UHiSR), combining hierarchical neural networks and symbolic regression. UHiSR automatically distills chemical-intuitive polarity indices, and discovers interpretable equations that link molecular structure to chromatographic behavior.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#20005;&#37325;&#32570;&#22833;&#27169;&#24577;&#30340;&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;MFCPL&#65292;&#36890;&#36807;&#23436;&#25972;&#30340;&#21407;&#22411;&#25552;&#20379;&#22810;&#26679;&#30340;&#27169;&#24577;&#30693;&#35782;&#65292;&#35299;&#20915;&#20102;&#25968;&#25454;&#24322;&#36136;&#24615;&#21644;&#32570;&#22833;&#27169;&#24577;&#24102;&#26469;&#30340;&#31283;&#20581;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.13898</link><description>&lt;p&gt;
&#36328;&#27169;&#24577;&#21407;&#22411;&#22522;&#30784;&#30340;&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#22312;&#20005;&#37325;&#32570;&#22833;&#27169;&#24577;&#19979;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Cross-Modal Prototype based Multimodal Federated Learning under Severely Missing Modality. (arXiv:2401.13898v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13898
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#20005;&#37325;&#32570;&#22833;&#27169;&#24577;&#30340;&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;MFCPL&#65292;&#36890;&#36807;&#23436;&#25972;&#30340;&#21407;&#22411;&#25552;&#20379;&#22810;&#26679;&#30340;&#27169;&#24577;&#30693;&#35782;&#65292;&#35299;&#20915;&#20102;&#25968;&#25454;&#24322;&#36136;&#24615;&#21644;&#32570;&#22833;&#27169;&#24577;&#24102;&#26469;&#30340;&#31283;&#20581;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#65288;MFL&#65289;&#20316;&#20026;&#19968;&#31181;&#21435;&#20013;&#24515;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539;&#20363;&#24050;&#32463;&#20986;&#29616;&#65292;&#23427;&#20801;&#35768;&#20855;&#26377;&#19981;&#21516;&#27169;&#24577;&#30340;&#22810;&#20010;&#23458;&#25143;&#31471;&#22312;&#19981;&#20849;&#20139;&#31169;&#20154;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21512;&#20316;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#36328;&#22810;&#26679;&#30340;&#25968;&#25454;&#28304;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#24322;&#36136;&#24615;&#21644;&#20005;&#37325;&#32570;&#22833;&#27169;&#24577;&#31561;&#25361;&#25112;&#32473;MFL&#30340;&#31283;&#20581;&#24615;&#24102;&#26469;&#37325;&#35201;&#38459;&#30861;&#65292;&#20005;&#37325;&#24433;&#21709;&#20840;&#23616;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#20005;&#37325;&#32570;&#22833;&#27169;&#24577;&#19979;&#30340;&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#21363;&#22810;&#27169;&#24577;&#32852;&#37030;&#20132;&#21449;&#21407;&#22411;&#23398;&#20064;&#65288;MFCPL&#65289;&#65292;&#36890;&#36807;&#23545;&#27169;&#24577;&#20849;&#20139;&#32423;&#21035;&#36827;&#34892;&#23436;&#25972;&#30340;&#21407;&#22411;&#26469;&#25552;&#20379;&#22810;&#26679;&#30340;&#27169;&#24577;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal federated learning (MFL) has emerged as a decentralized machine learning paradigm, allowing multiple clients with different modalities to collaborate on training a machine learning model across diverse data sources without sharing their private data. However, challenges, such as data heterogeneity and severely missing modalities, pose crucial hindrances to the robustness of MFL, significantly impacting the performance of global model. The absence of a modality introduces misalignment during the local training phase, stemming from zero-filling in the case of clients with missing modalities. Consequently, achieving robust generalization in global model becomes imperative, especially when dealing with clients that have incomplete data. In this paper, we propose Multimodal Federated Cross Prototype Learning (MFCPL), a novel approach for MFL under severely missing modalities by conducting the complete prototypes to provide diverse modality knowledge in modality-shared level with 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#30417;&#30563;&#24314;&#27169;&#22312;&#20083;&#33146;&#30284;&#30149;&#29702;&#20998;&#31867;&#19978;&#30340;&#38646;&#26679;&#26412;&#25512;&#26029;&#33021;&#21147;&#65292;&#21457;&#29616;GPT-4&#27169;&#22411;&#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#35201;&#20040;&#26126;&#26174;&#20248;&#20110;&#65292;&#35201;&#20040;&#19982;&#26368;&#20339;&#30340;&#30417;&#30563;&#27169;&#22411;LSTM-Att&#27169;&#22411;&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2401.13887</link><description>&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#30417;&#30563;&#24314;&#27169;&#30340;&#38646;&#26679;&#26412;&#25512;&#26029;&#36827;&#34892;&#20083;&#33146;&#30284;&#30149;&#29702;&#20998;&#31867;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A comparative study of zero-shot inference with large language models and supervised modeling in breast cancer pathology classification. (arXiv:2401.13887v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13887
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#30417;&#30563;&#24314;&#27169;&#22312;&#20083;&#33146;&#30284;&#30149;&#29702;&#20998;&#31867;&#19978;&#30340;&#38646;&#26679;&#26412;&#25512;&#26029;&#33021;&#21147;&#65292;&#21457;&#29616;GPT-4&#27169;&#22411;&#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#35201;&#20040;&#26126;&#26174;&#20248;&#20110;&#65292;&#35201;&#20040;&#19982;&#26368;&#20339;&#30340;&#30417;&#30563;&#27169;&#22411;LSTM-Att&#27169;&#22411;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#22312;&#20174;&#20020;&#24202;&#35760;&#24405;&#20013;&#25552;&#21462;&#20449;&#24687;&#26041;&#38754;&#21313;&#20998;&#27969;&#34892;&#65292;&#20294;&#21019;&#24314;&#22823;&#22411;&#27880;&#37322;&#25968;&#25454;&#38598;&#38656;&#35201;&#24191;&#27867;&#30340;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#21644;&#32791;&#36153;&#26102;&#38388;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#24456;&#24378;&#30340;&#36801;&#31227;&#23398;&#20064;&#33021;&#21147;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#26368;&#36817;&#30340;LLMs&#26159;&#21542;&#21487;&#20197;&#20943;&#23569;&#23545;&#22823;&#35268;&#27169;&#25968;&#25454;&#27880;&#37322;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#25972;&#29702;&#20102;&#19968;&#20010;&#25163;&#21160;&#26631;&#27880;&#30340;769&#20010;&#20083;&#33146;&#30284;&#30149;&#29702;&#25253;&#21578;&#30340;&#25968;&#25454;&#38598;&#65288;&#26631;&#27880;&#20102;13&#20010;&#31867;&#21035;&#65289;&#65292;&#26469;&#27604;&#36739;GPT-4&#27169;&#22411;&#21644;GPT-3.5&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#33021;&#21147;&#19982;&#19977;&#31181;&#27169;&#22411;&#26550;&#26500;&#30340;&#30417;&#30563;&#20998;&#31867;&#24615;&#33021;&#65306;&#38543;&#26426;&#26862;&#26519;&#20998;&#31867;&#22120;&#65292;&#20855;&#26377;&#27880;&#24847;&#21147;&#30340;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#65288;LSTM-Att&#65289;&#21644;UCSF-BERT&#27169;&#22411;&#12290;&#22312;&#25152;&#26377;13&#20010;&#20219;&#21153;&#20013;&#65292;GPT-4&#27169;&#22411;&#30340;&#24615;&#33021;&#35201;&#20040;&#26126;&#26174;&#20248;&#20110;&#26368;&#20339;&#30340;&#30417;&#30563;&#27169;&#22411;LSTM-Att&#27169;&#22411;&#65288;&#24179;&#22343;&#23439;F1&#24471;&#20998;&#20026;0.83 vs. 0.75&#65289;&#65292;&#35201;&#20040;&#19982;&#20854;&#30456;&#24403;&#12290;&#22312;&#23384;&#22312;&#26631;&#31614;&#20043;&#38388;&#39640;&#24230;&#19981;&#24179;&#34913;&#30340;&#20219;&#21153;&#20013;&#65292;di
&lt;/p&gt;
&lt;p&gt;
Although supervised machine learning is popular for information extraction from clinical notes, creating large annotated datasets requires extensive domain expertise and is time-consuming. Meanwhile, large language models (LLMs) have demonstrated promising transfer learning capability. In this study, we explored whether recent LLMs can reduce the need for large-scale data annotations. We curated a manually-labeled dataset of 769 breast cancer pathology reports, labeled with 13 categories, to compare zero-shot classification capability of the GPT-4 model and the GPT-3.5 model with supervised classification performance of three model architectures: random forests classifier, long short-term memory networks with attention (LSTM-Att), and the UCSF-BERT model. Across all 13 tasks, the GPT-4 model performed either significantly better than or as well as the best supervised model, the LSTM-Att model (average macro F1 score of 0.83 vs. 0.75). On tasks with high imbalance between labels, the di
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36793;&#32536;&#26465;&#20214;&#33410;&#28857;&#26356;&#26032;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;ECNU-GNN&#65289;&#65292;&#29992;&#20110;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#12290;&#35813;&#27169;&#22411;&#26681;&#25454;&#36830;&#25509;&#30340;&#36793;&#21160;&#24577;&#36716;&#25442;&#28304;&#33410;&#28857;&#34920;&#31034;&#20197;&#24688;&#24403;&#22320;&#34920;&#31034;&#30446;&#26631;&#33410;&#28857;&#65292;&#22312;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2401.13872</link><description>&lt;p&gt;
&#36793;&#32536;&#26465;&#20214;&#33410;&#28857;&#26356;&#26032;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Edge Conditional Node Update Graph Neural Network for Multi-variate Time Series Anomaly Detection. (arXiv:2401.13872v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13872
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36793;&#32536;&#26465;&#20214;&#33410;&#28857;&#26356;&#26032;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;ECNU-GNN&#65289;&#65292;&#29992;&#20110;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#12290;&#35813;&#27169;&#22411;&#26681;&#25454;&#36830;&#25509;&#30340;&#36793;&#21160;&#24577;&#36716;&#25442;&#28304;&#33410;&#28857;&#34920;&#31034;&#20197;&#24688;&#24403;&#22320;&#34920;&#31034;&#30446;&#26631;&#33410;&#28857;&#65292;&#22312;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26234;&#33021;&#21270;&#31995;&#32479;&#30340;&#36805;&#36895;&#21457;&#23637;&#65292;&#20256;&#24863;&#22120;&#30340;&#22686;&#21152;&#26174;&#33879;&#22797;&#26434;&#21270;&#20102;&#23545;&#31995;&#32479;&#29366;&#24577;&#30340;&#25163;&#21160;&#30417;&#27979;&#12290;&#22240;&#27492;&#65292;&#22522;&#20110;&#22270;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#26126;&#30830;&#34920;&#31034;&#20256;&#24863;&#22120;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#22312;&#26356;&#26032;&#19981;&#21516;&#30446;&#26631;&#33410;&#28857;&#34920;&#31034;&#26102;&#65292;&#23545;&#25152;&#26377;&#36830;&#25509;&#30340;&#30446;&#26631;&#33410;&#28857;&#24212;&#29992;&#32479;&#19968;&#30340;&#28304;&#33410;&#28857;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#24120;&#29992;&#20110;&#25512;&#26029;&#26410;&#30693;&#22270;&#32467;&#26500;&#30340;&#22270;&#27880;&#24847;&#21147;&#26426;&#21046;&#21487;&#33021;&#38480;&#21046;&#28304;&#33410;&#28857;&#34920;&#31034;&#30340;&#22810;&#26679;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#36793;&#32536;&#26465;&#20214;&#33410;&#28857;&#26356;&#26032;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;ECNU-GNN&#65289;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#37197;&#22791;&#20102;&#19968;&#20010;&#36793;&#32536;&#26465;&#20214;&#33410;&#28857;&#26356;&#26032;&#27169;&#22359;&#65292;&#26681;&#25454;&#36830;&#25509;&#30340;&#36793;&#21160;&#24577;&#36716;&#25442;&#28304;&#33410;&#28857;&#34920;&#31034;&#20197;&#24688;&#24403;&#22320;&#34920;&#31034;&#30446;&#26631;&#33410;&#28857;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#24615;&#33021;&#65306;SWaT&#12289;WADI&#21644;PSM&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#34920;&#29616;&#20986;5&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid advancement in cyber-physical systems, the increasing number of sensors has significantly complicated manual monitoring of system states. Consequently, graph-based time-series anomaly detection methods have gained attention due to their ability to explicitly represent relationships between sensors. However, these methods often apply a uniform source node representation across all connected target nodes, even when updating different target node representations. Moreover, the graph attention mechanism, commonly used to infer unknown graph structures, could constrain the diversity of source node representations. In this paper, we introduce the Edge Conditional Node-update Graph Neural Network (ECNU-GNN). Our model, equipped with an edge conditional node update module, dynamically transforms source node representations based on connected edges to represent target nodes aptly. We validate performance on three real-world datasets: SWaT, WADI, and PSM. Our model demonstrates 5.
&lt;/p&gt;</description></item><item><title>&#20511;&#21161;&#22810;&#26465;&#20214;&#25193;&#25955;&#24341;&#23548;&#30340;&#36870;&#20998;&#23376;&#35774;&#35745;&#27169;&#22411;&#22312;&#26448;&#26009;&#21644;&#33647;&#29289;&#21457;&#29616;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#36890;&#36807;&#24341;&#20837;Transformer-based&#21435;&#22122;&#27169;&#22411;&#21644;&#22270;&#20381;&#36182;&#30340;&#25193;&#25955;&#36807;&#31243;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#22312;&#22810;&#20010;&#26465;&#20214;&#32422;&#26463;&#19979;&#20934;&#30830;&#22320;&#29983;&#25104;&#32858;&#21512;&#29289;&#21644;&#23567;&#20998;&#23376;&#12290;</title><link>http://arxiv.org/abs/2401.13858</link><description>&lt;p&gt;
&#20511;&#21161;&#22810;&#26465;&#20214;&#25193;&#25955;&#24341;&#23548;&#30340;&#36870;&#20998;&#23376;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Inverse Molecular Design with Multi-Conditional Diffusion Guidance. (arXiv:2401.13858v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13858
&lt;/p&gt;
&lt;p&gt;
&#20511;&#21161;&#22810;&#26465;&#20214;&#25193;&#25955;&#24341;&#23548;&#30340;&#36870;&#20998;&#23376;&#35774;&#35745;&#27169;&#22411;&#22312;&#26448;&#26009;&#21644;&#33647;&#29289;&#21457;&#29616;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#36890;&#36807;&#24341;&#20837;Transformer-based&#21435;&#22122;&#27169;&#22411;&#21644;&#22270;&#20381;&#36182;&#30340;&#25193;&#25955;&#36807;&#31243;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#22312;&#22810;&#20010;&#26465;&#20214;&#32422;&#26463;&#19979;&#20934;&#30830;&#22320;&#29983;&#25104;&#32858;&#21512;&#29289;&#21644;&#23567;&#20998;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20511;&#21161;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#36870;&#20998;&#23376;&#35774;&#35745;&#22312;&#26448;&#26009;&#21644;&#33647;&#29289;&#21457;&#29616;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#34429;&#28982;&#22312;&#26080;&#26465;&#20214;&#20998;&#23376;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23558;&#21512;&#25104;&#35780;&#20998;&#21644;&#27668;&#20307;&#28183;&#36879;&#24615;&#31561;&#22810;&#20010;&#23646;&#24615;&#20316;&#20026;&#26465;&#20214;&#32422;&#26463;&#38598;&#25104;&#21040;&#25193;&#25955;&#27169;&#22411;&#20013;&#20173;&#26410;&#34987;&#25506;&#32034;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22810;&#26465;&#20214;&#25193;&#25955;&#24341;&#23548;&#12290;&#25152;&#25552;&#20986;&#30340;&#22522;&#20110;Transformer&#30340;&#21435;&#22122;&#27169;&#22411;&#20855;&#26377;&#19968;&#20010;&#26465;&#20214;&#32534;&#30721;&#22120;&#65292;&#35813;&#32534;&#30721;&#22120;&#23398;&#20064;&#20102;&#25968;&#20540;&#21644;&#20998;&#31867;&#26465;&#20214;&#30340;&#34920;&#31034;&#12290;&#32452;&#25104;&#32467;&#26500;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#30340;&#21435;&#22122;&#27169;&#22411;&#22312;&#26465;&#20214;&#34920;&#31034;&#19979;&#36827;&#34892;&#21435;&#22122;&#35757;&#32451;&#12290;&#25193;&#25955;&#36807;&#31243;&#21464;&#24471;&#20381;&#36182;&#20110;&#22270;&#26469;&#20934;&#30830;&#20272;&#35745;&#20998;&#23376;&#20013;&#19982;&#22270;&#30456;&#20851;&#30340;&#22122;&#22768;&#65292;&#32780;&#19981;&#20687;&#20197;&#21069;&#30340;&#27169;&#22411;&#20165;&#20851;&#27880;&#21407;&#23376;&#25110;&#38190;&#30340;&#36793;&#32536;&#20998;&#24067;&#12290;&#25105;&#20204;&#24191;&#27867;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#22810;&#26465;&#20214;&#32858;&#21512;&#29289;&#21644;&#23567;&#20998;&#23376;&#29983;&#25104;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#12290;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#22312;&#20998;&#24067;&#24230;&#37327;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inverse molecular design with diffusion models holds great potential for advancements in material and drug discovery. Despite success in unconditional molecule generation, integrating multiple properties such as synthetic score and gas permeability as condition constraints into diffusion models remains unexplored. We introduce multi-conditional diffusion guidance. The proposed Transformer-based denoising model has a condition encoder that learns the representations of numerical and categorical conditions. The denoising model, consisting of a structure encoder-decoder, is trained for denoising under the representation of conditions. The diffusion process becomes graph-dependent to accurately estimate graph-related noise in molecules, unlike the previous models that focus solely on the marginal distributions of atoms or bonds. We extensively validate our model for multi-conditional polymer and small molecule generation. Results demonstrate our superiority across metrics from distribution
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;NVIDIA&#24320;&#21457;&#30340;TTS&#27169;&#22411;&#65292;&#21033;&#29992;RAD-MMM&#21644;P-Flow&#23454;&#29616;&#20102;&#22810;&#35821;&#35328;TTS&#30340;&#35757;&#32451;&#65292;&#20854;&#20013;P-Flow&#22312;&#38646;&#26679;&#26412;TTS&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#22312;2024&#25361;&#25112;&#20013;&#33719;&#24471;&#20102;&#31532;&#19968;&#21517;&#12290;</title><link>http://arxiv.org/abs/2401.13851</link><description>&lt;p&gt;
&#21033;&#29992;&#22768;&#38899;&#20811;&#38534;&#23558;NVIDIA&#30340;&#22810;&#35821;&#35328;TTS&#31995;&#32479;&#25193;&#23637;&#21040;&#21360;&#24230;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
Scaling NVIDIA's multi-speaker multi-lingual TTS systems with voice cloning to Indic Languages. (arXiv:2401.13851v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13851
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;NVIDIA&#24320;&#21457;&#30340;TTS&#27169;&#22411;&#65292;&#21033;&#29992;RAD-MMM&#21644;P-Flow&#23454;&#29616;&#20102;&#22810;&#35821;&#35328;TTS&#30340;&#35757;&#32451;&#65292;&#20854;&#20013;P-Flow&#22312;&#38646;&#26679;&#26412;TTS&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#22312;2024&#25361;&#25112;&#20013;&#33719;&#24471;&#20102;&#31532;&#19968;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;NVIDIA&#20026;MMITS-VC&#65288;&#22810;&#35821;&#35328;&#12289;&#22810;&#35821;&#31181;&#21360;&#24230;TTS&#19982;&#22768;&#38899;&#20811;&#38534;&#65289;2024&#25361;&#25112;&#24320;&#21457;&#30340;TTS&#27169;&#22411;&#12290;&#22312;1&#21644;2&#36712;&#36947;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;RAD-MMM&#22312;&#30446;&#26631;&#35828;&#35805;&#20154;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#23569;&#26679;&#26412;TTS&#30340;&#35757;&#32451;&#12290;&#22312;&#31532;3&#36712;&#36947;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;P-Flow&#36827;&#34892;&#20102;&#38646;&#26679;&#26412;TTS&#30340;&#35757;&#32451;&#65292;&#21516;&#26102;&#20351;&#29992;&#20102;&#25361;&#25112;&#25968;&#25454;&#38598;&#21644;&#22806;&#37096;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23545;&#25152;&#26377;&#25552;&#20132;&#20351;&#29992;HiFi-GAN vocoders&#12290;RAD-MMM&#22312;1&#21644;2&#36712;&#36947;&#19978;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#65292;&#32780;P-Flow&#22312;&#31532;3&#36712;&#36947;&#19978;&#25490;&#21517;&#31532;&#19968;&#65292;&#24179;&#22343;&#24847;&#35265;&#20998;(MOS)&#20026;4.4&#65292;&#35828;&#35805;&#20154;&#30456;&#20284;&#24230;&#20998;&#25968;(SMOS)&#20026;3.62&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we describe the TTS models developed by NVIDIA for the MMITS-VC (Multi-speaker, Multi-lingual Indic TTS with Voice Cloning) 2024 Challenge. In Tracks 1 and 2, we utilize RAD-MMM to perform few-shot TTS by training additionally on 5 minutes of target speaker data. In Track 3, we utilize P-Flow to perform zero-shot TTS by training on the challenge dataset as well as external datasets. We use HiFi-GAN vocoders for all submissions. RAD-MMM performs competitively on Tracks 1 and 2, while P-Flow ranks first on Track 3, with mean opinion score (MOS) 4.4 and speaker similarity score (SMOS) of 3.62.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;V2X&#30340;&#38544;&#31169;&#20445;&#25252;&#32852;&#21512;&#27979;&#37327;&#21644;&#23398;&#20064;&#31995;&#32479;&#65292;&#36890;&#36807;V2V&#36890;&#20449;&#21521;&#20854;&#20182;&#36710;&#36742;&#25552;&#20379;&#23454;&#26102;&#25968;&#25454;&#65292;&#21516;&#26102;&#36890;&#36807;V2N&#38142;&#36335;&#19978;&#30340;FL&#26041;&#26696;&#21019;&#24314;&#20132;&#36890;&#32593;&#32476;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.13848</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;V2X&#30340;&#38544;&#31169;&#20445;&#25252;&#32852;&#21512;&#27979;&#37327;&#21644;&#23398;&#20064;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
A V2X-based Privacy Preserving Federated Measuring and Learning System. (arXiv:2401.13848v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13848
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;V2X&#30340;&#38544;&#31169;&#20445;&#25252;&#32852;&#21512;&#27979;&#37327;&#21644;&#23398;&#20064;&#31995;&#32479;&#65292;&#36890;&#36807;V2V&#36890;&#20449;&#21521;&#20854;&#20182;&#36710;&#36742;&#25552;&#20379;&#23454;&#26102;&#25968;&#25454;&#65292;&#21516;&#26102;&#36890;&#36807;V2N&#38142;&#36335;&#19978;&#30340;FL&#26041;&#26696;&#21019;&#24314;&#20132;&#36890;&#32593;&#32476;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26410;&#26469;&#30340;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#23558;&#20351;&#29992;&#21508;&#31181;&#20256;&#24863;&#22120;&#29983;&#25104;&#22823;&#37327;&#25968;&#25454;&#12290;&#36825;&#20123;&#25968;&#25454;&#19981;&#20165;&#21487;&#20197;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#31639;&#27861;&#65292;&#36824;&#21487;&#20197;&#24110;&#21161;&#20854;&#20182;&#36710;&#36742;&#25110;&#22522;&#30784;&#35774;&#26045;&#36827;&#34892;&#23454;&#26102;&#20915;&#31574;&#12290;&#22240;&#27492;&#65292;&#36710;&#36742;&#38656;&#35201;&#36890;&#36807;&#36710;&#36742;&#21040;&#19968;&#20999;(V2X)&#25216;&#26415;&#26469;&#20132;&#25442;&#27979;&#37327;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#39044;&#27979;&#36947;&#36335;&#32593;&#32476;&#30340;&#29366;&#24577;&#21487;&#33021;&#20063;&#20250;&#26377;&#30410;&#22788;&#12290;&#36890;&#36807;&#36825;&#31181;&#39044;&#27979;&#65292;&#25105;&#20204;&#21487;&#20197;&#20943;&#36731;&#36947;&#36335;&#25317;&#22581;&#12289;&#24179;&#34913;&#20572;&#36710;&#22330;&#20351;&#29992;&#24773;&#20917;&#25110;&#20248;&#21270;&#20132;&#36890;&#27969;&#21160;&#12290;&#36825;&#23558;&#38477;&#20302;&#36816;&#36755;&#25104;&#26412;&#65292;&#20943;&#23569;&#29615;&#22659;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#27979;&#37327;&#21644;&#23398;&#20064;&#31995;&#32479;&#65292;&#36890;&#36807;&#36710;&#36742;&#21040;&#36710;&#36742;(V2V)&#36890;&#20449;&#21521;&#20854;&#20182;&#36710;&#36742;&#25552;&#20379;&#23454;&#26102;&#25968;&#25454;&#65292;&#21516;&#26102;&#36890;&#36807;&#36710;&#36742;&#21040;&#32593;&#32476;(V2N)&#38142;&#36335;&#19978;&#30340;&#32852;&#21512;&#23398;&#20064;(FL)&#26041;&#26696;&#21019;&#24314;&#20132;&#36890;&#32593;&#32476;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;&#30001;&#20110;&#23578;&#26080;&#30495;&#23454;&#19990;&#30028;&#30340;&#33258;&#21160;&#39550;&#39542;&#25968;&#25454;&#65292;&#25105;&#20204;&#27169;&#25311;&#20102;&#25968;&#25454;&#65292;
&lt;/p&gt;
&lt;p&gt;
Future autonomous vehicles (AVs) will use a variety of sensors that generate a vast amount of data. Naturally, this data not only serves self-driving algorithms; but can also assist other vehicles or the infrastructure in real-time decision-making. Consequently, vehicles shall exchange their measurement data over Vehicle-to-Everything (V2X) technologies. Moreover, predicting the state of the road network might be beneficial too. With such a prediction, we might mitigate road congestion, balance parking lot usage, or optimize the traffic flow. That would decrease transportation costs as well as reduce its environmental impact.  In this paper, we propose a federated measurement and learning system that provides real-time data to fellow vehicles over Vehicle-to-Vehicle (V2V) communication while also operating a federated learning (FL) scheme over the Vehicle-to-Network (V2N) link to create a predictive model of the transportation network. As we are yet to have real-world AV data, we model
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#22810;&#31867;&#20998;&#31867;&#38382;&#39064;&#20013;&#26522;&#20030;k&#25240;&#37197;&#32622;&#30340;&#31639;&#27861;&#65292;&#29992;&#26469;&#35299;&#20915;&#22522;&#20110;k&#25240;&#20132;&#21449;&#39564;&#35777;&#30340;&#24615;&#33021;&#24471;&#20998;&#19981;&#21487;&#22797;&#29616;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.13843</link><description>&lt;p&gt;
&#22312;&#22810;&#31867;&#20998;&#31867;&#38382;&#39064;&#20013;&#26522;&#20030;k&#25240;&#37197;&#32622;
&lt;/p&gt;
&lt;p&gt;
Enumerating the k-fold configurations in multi-class classification problems. (arXiv:2401.13843v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13843
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#22810;&#31867;&#20998;&#31867;&#38382;&#39064;&#20013;&#26522;&#20030;k&#25240;&#37197;&#32622;&#30340;&#31639;&#27861;&#65292;&#29992;&#26469;&#35299;&#20915;&#22522;&#20110;k&#25240;&#20132;&#21449;&#39564;&#35777;&#30340;&#24615;&#33021;&#24471;&#20998;&#19981;&#21487;&#22797;&#29616;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
K&#25240;&#20132;&#21449;&#39564;&#35777;&#26159;&#35780;&#20272;&#20998;&#31867;&#22120;&#24615;&#33021;&#30340;&#24120;&#29992;&#24037;&#20855;&#12290;&#20154;&#24037;&#26234;&#33021;&#38754;&#20020;&#30340;&#21487;&#37325;&#22797;&#24615;&#21361;&#26426;&#37096;&#20998;&#28304;&#20110;&#25253;&#21578;&#30340;&#22522;&#20110;k&#25240;&#20132;&#21449;&#39564;&#35777;&#30340;&#24615;&#33021;&#24471;&#20998;&#19981;&#21487;&#22797;&#29616;&#12290;&#26368;&#36817;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#25968;&#20540;&#25216;&#26415;&#26469;&#27979;&#35797;&#25152;&#22768;&#31216;&#30340;&#24615;&#33021;&#24471;&#20998;&#21644;&#23454;&#39564;&#35774;&#32622;&#30340;&#19968;&#33268;&#24615;&#12290;&#22312;&#19968;&#20010;&#20851;&#38190;&#30340;&#20351;&#29992;&#26696;&#20363;&#20013;&#65292;&#35813;&#26041;&#27861;&#20381;&#36182;&#20110;&#23545;&#25152;&#26377;k&#25240;&#37197;&#32622;&#30340;&#32452;&#21512;&#26522;&#20030;&#65292;&#23545;&#20110;&#36825;&#31181;&#24773;&#20917;&#65292;&#25105;&#20204;&#22312;&#20108;&#20998;&#31867;&#38382;&#39064;&#20013;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
K-fold cross-validation is a widely used tool for assessing classifier performance. The reproducibility crisis faced by artificial intelligence partly results from the irreproducibility of reported k-fold cross-validation-based performance scores. Recently, we introduced numerical techniques to test the consistency of claimed performance scores and experimental setups. In a crucial use case, the method relies on the combinatorial enumeration of all k-fold configurations, for which we proposed an algorithm in the binary classification case.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#24037;&#19994;&#20256;&#24863;&#21644;&#25511;&#21046;&#20013;&#30340;&#24212;&#29992;&#65292;&#30830;&#23450;&#20102;&#22312;&#36807;&#31243;&#24037;&#19994;&#20013;&#23454;&#38469;&#25104;&#21151;&#30340;&#20851;&#38190;&#25216;&#26415;&#65292;&#24182;&#35752;&#35770;&#20102;&#36719;&#20256;&#24863;&#12289;&#36807;&#31243;&#20248;&#21270;&#21644;&#25511;&#21046;&#31561;&#26680;&#24515;&#24212;&#29992;&#39046;&#22495;&#30340;&#38656;&#27714;&#21644;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.13836</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#24037;&#19994;&#20256;&#24863;&#21644;&#25511;&#21046;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#39033;&#35843;&#26597;&#21644;&#23454;&#36341;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Machine learning for industrial sensing and control: A survey and practical perspective. (arXiv:2401.13836v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13836
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#24037;&#19994;&#20256;&#24863;&#21644;&#25511;&#21046;&#20013;&#30340;&#24212;&#29992;&#65292;&#30830;&#23450;&#20102;&#22312;&#36807;&#31243;&#24037;&#19994;&#20013;&#23454;&#38469;&#25104;&#21151;&#30340;&#20851;&#38190;&#25216;&#26415;&#65292;&#24182;&#35752;&#35770;&#20102;&#36719;&#20256;&#24863;&#12289;&#36807;&#31243;&#20248;&#21270;&#21644;&#25511;&#21046;&#31561;&#26680;&#24515;&#24212;&#29992;&#39046;&#22495;&#30340;&#38656;&#27714;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#20852;&#36215;&#65292;&#36807;&#31243;&#24037;&#19994;&#39046;&#22495;&#23545;&#20110;&#21033;&#29992;&#22823;&#35268;&#27169;&#38750;&#32447;&#24615;&#20256;&#24863;&#21644;&#25511;&#21046;&#38382;&#39064;&#30340;&#25968;&#25454;&#20135;&#29983;&#20102;&#26032;&#30340;&#20852;&#36259;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#22312;&#36807;&#31243;&#24037;&#19994;&#20013;&#21462;&#24471;&#23454;&#38469;&#25104;&#21151;&#30340;&#20851;&#38190;&#32479;&#35745;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20197;&#28151;&#21512;&#24314;&#27169;&#20026;&#36215;&#28857;&#65292;&#25552;&#20379;&#20102;&#25903;&#25745;&#26680;&#24515;&#24212;&#29992;&#39046;&#22495;&#30340;&#26041;&#27861;&#35770;&#26694;&#26550;&#65306;&#36719;&#20256;&#24863;&#12289;&#36807;&#31243;&#20248;&#21270;&#21644;&#25511;&#21046;&#12290;&#36719;&#20256;&#24863;&#21253;&#21547;&#20102;&#32479;&#35745;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#24037;&#19994;&#24212;&#29992;&#30340;&#20016;&#23500;&#26696;&#20363;&#12290;&#25105;&#20204;&#23450;&#37327;&#22320;&#35782;&#21035;&#20102;&#30740;&#31350;&#36235;&#21183;&#65292;&#20174;&#32780;&#25581;&#31034;&#20102;&#23454;&#36341;&#20013;&#26368;&#25104;&#21151;&#30340;&#25216;&#26415;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#25968;&#25454;&#39537;&#21160;&#20248;&#21270;&#21644;&#25511;&#21046;&#26041;&#27861;&#65306;&#28151;&#21512;&#24314;&#27169;&#32467;&#21512;&#25968;&#23398;&#35268;&#21010;&#25216;&#26415;&#20197;&#21450;&#24378;&#21270;&#23398;&#20064;&#12290;&#22312;&#36825;&#20123;&#24212;&#29992;&#39046;&#22495;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#23427;&#20204;&#21508;&#33258;&#30340;&#24037;&#19994;&#38656;&#27714;&#21644;&#25361;&#25112;&#12290;&#19968;&#20010;&#20849;&#21516;&#30340;&#25361;&#25112;&#26159;&#35299;&#37322;&#24615;&#21644;&#25928;&#29575;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rise of deep learning, there has been renewed interest within the process industries to utilize data on large-scale nonlinear sensing and control problems. We identify key statistical and machine learning techniques that have seen practical success in the process industries. To do so, we start with hybrid modeling to provide a methodological framework underlying core application areas: soft sensing, process optimization, and control. Soft sensing contains a wealth of industrial applications of statistical and machine learning methods. We quantitatively identify research trends, allowing insight into the most successful techniques in practice.  We consider two distinct flavors for data-driven optimization and control: hybrid modeling in conjunction with mathematical programming techniques and reinforcement learning. Throughout these application areas, we discuss their respective industrial requirements and challenges.  A common challenge is the interpretability and efficiency o
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20256;&#36798;&#32622;&#20449;&#24230;&#26041;&#38754;&#27169;&#22411;&#21644;&#20154;&#31867;&#20043;&#38388;&#23384;&#22312;&#30340;&#24046;&#36317;&#65292;&#24182;&#21457;&#29616;&#40664;&#35748;&#35299;&#37322;&#20250;&#23548;&#33268;&#29992;&#25143;&#36807;&#39640;&#20272;&#35745;&#27169;&#22411;&#32622;&#20449;&#24230;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.13835</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20013;&#27169;&#22411;&#21644;&#20154;&#31867;&#32622;&#20449;&#24230;&#20043;&#38388;&#30340;&#26657;&#20934;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
The Calibration Gap between Model and Human Confidence in Large Language Models. (arXiv:2401.13835v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13835
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20256;&#36798;&#32622;&#20449;&#24230;&#26041;&#38754;&#27169;&#22411;&#21644;&#20154;&#31867;&#20043;&#38388;&#23384;&#22312;&#30340;&#24046;&#36317;&#65292;&#24182;&#21457;&#29616;&#40664;&#35748;&#35299;&#37322;&#20250;&#23548;&#33268;&#29992;&#25143;&#36807;&#39640;&#20272;&#35745;&#27169;&#22411;&#32622;&#20449;&#24230;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#33021;&#22815;&#33719;&#24471;&#20154;&#31867;&#30340;&#20449;&#20219;&#65292;&#23427;&#20204;&#38656;&#35201;&#22312;&#26576;&#31181;&#24847;&#20041;&#19978;&#23454;&#29616;&#33391;&#22909;&#30340;&#26657;&#20934;&#65292;&#21363;&#33021;&#22815;&#20934;&#30830;&#35780;&#20272;&#21644;&#20256;&#36798;&#23427;&#20204;&#30340;&#39044;&#27979;&#27491;&#30830;&#30340;&#21487;&#33021;&#24615;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20851;&#27880;&#20102;LLM&#20869;&#37096;&#32622;&#20449;&#24230;&#35780;&#20272;&#30340;&#36136;&#37327;&#65292;&#20294;&#38382;&#39064;&#20173;&#28982;&#26159;LLM&#33021;&#22815;&#22914;&#20309;&#23558;&#36825;&#31181;&#20869;&#37096;&#27169;&#22411;&#32622;&#20449;&#24230;&#20256;&#36798;&#32473;&#20154;&#31867;&#29992;&#25143;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20154;&#31867;&#23545;LLM&#21709;&#24212;&#30340;&#22806;&#37096;&#32622;&#20449;&#24230;&#19982;&#27169;&#22411;&#20869;&#37096;&#32622;&#20449;&#24230;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#36890;&#36807;&#28041;&#21450;&#22810;&#39033;&#36873;&#25321;&#39064;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#26816;&#26597;&#20102;&#20154;&#31867;&#29992;&#25143;&#35782;&#21035;LLM&#36755;&#20986;&#21487;&#20449;&#24230;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#37325;&#28857;&#20998;&#20026;&#20004;&#20010;&#26041;&#38754;&#65306;&#65288;1&#65289;&#35780;&#20272;&#29992;&#25143;&#23545;&#30495;&#23454;LLM&#32622;&#20449;&#24230;&#30340;&#24863;&#30693;&#21644;&#65288;2&#65289;&#35843;&#26597;&#20010;&#24615;&#21270;&#35299;&#37322;&#23545;&#35813;&#24863;&#30693;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;LLM&#30340;&#40664;&#35748;&#35299;&#37322;&#24448;&#24448;&#20250;&#23548;&#33268;&#29992;&#25143;&#36807;&#39640;&#20272;&#35745;&#27169;&#22411;&#30340;&#32622;&#20449;&#24230;&#21644;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#20462;&#25913;&#35299;&#37322;&#30340;&#26041;&#24335;&#21487;&#20197;&#20943;&#23567;&#36825;&#31181;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
For large language models (LLMs) to be trusted by humans they need to be well-calibrated in the sense that they can accurately assess and communicate how likely it is that their predictions are correct. Recent work has focused on the quality of internal LLM confidence assessments, but the question remains of how well LLMs can communicate this internal model confidence to human users. This paper explores the disparity between external human confidence in an LLM's responses and the internal confidence of the model. Through experiments involving multiple-choice questions, we systematically examine human users' ability to discern the reliability of LLM outputs. Our study focuses on two key areas: (1) assessing users' perception of true LLM confidence and (2) investigating the impact of tailored explanations on this perception. The research highlights that default explanations from LLMs often lead to user overestimation of both the model's confidence and its' accuracy. By modifying the expl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20272;&#35745;&#22522;&#20110;&#39532;&#23572;&#21487;&#22827;&#20107;&#20214;&#30340;&#29289;&#32852;&#32593;&#35774;&#22791;&#30340;&#27969;&#37327;&#24773;&#20917;&#65292;&#24182;&#20248;&#21270;&#22810;&#20010;&#26080;&#20154;&#26426;&#30340;&#36712;&#36857;&#21644;&#35843;&#24230;&#31574;&#30053;&#65292;&#20197;&#38477;&#20302;&#20449;&#24687;&#26102;&#20195;&#12289;&#33410;&#30465;&#33021;&#37327;&#21644;&#25552;&#39640;&#21534;&#21520;&#37327;&#12290;</title><link>http://arxiv.org/abs/2401.13827</link><description>&lt;p&gt;
&#29992;&#20110;&#39532;&#23572;&#21487;&#22827;&#29289;&#32852;&#32593;&#27169;&#22411;&#20013;&#25968;&#25454;&#19978;&#34892;&#30340;&#27969;&#37327;&#23398;&#20064;&#21644;&#20027;&#21160;&#26080;&#20154;&#26426;&#36712;&#36857;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic Learning and Proactive UAV Trajectory Planning for Data Uplink in Markovian IoT Models. (arXiv:2401.13827v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13827
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20272;&#35745;&#22522;&#20110;&#39532;&#23572;&#21487;&#22827;&#20107;&#20214;&#30340;&#29289;&#32852;&#32593;&#35774;&#22791;&#30340;&#27969;&#37327;&#24773;&#20917;&#65292;&#24182;&#20248;&#21270;&#22810;&#20010;&#26080;&#20154;&#26426;&#30340;&#36712;&#36857;&#21644;&#35843;&#24230;&#31574;&#30053;&#65292;&#20197;&#38477;&#20302;&#20449;&#24687;&#26102;&#20195;&#12289;&#33410;&#30465;&#33021;&#37327;&#21644;&#25552;&#39640;&#21534;&#21520;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#26102;&#20195;(AoI)&#29992;&#20110;&#34913;&#37327;&#25968;&#25454;&#30340;&#26032;&#40092;&#24230;&#12290;&#22312;&#29289;&#32852;&#32593;&#32593;&#32476;&#20013;&#65292;&#20256;&#32479;&#30340;&#36164;&#28304;&#31649;&#29702;&#26041;&#26696;&#20381;&#36182;&#20110;&#35774;&#22791;&#21644;&#22522;&#31449;(BS)&#20043;&#38388;&#30340;&#28040;&#24687;&#20132;&#25442;&#65292;&#23548;&#33268;AoI&#39640;&#12289;&#33021;&#37327;&#28040;&#32791;&#39640;&#19988;&#21487;&#38752;&#24615;&#20302;&#12290;&#20316;&#20026;&#39134;&#34892;&#22522;&#31449;&#30340;&#26080;&#20154;&#26426;(UAV)&#22312;&#20943;&#23567;AoI&#12289;&#33410;&#30465;&#33021;&#37327;&#21644;&#25552;&#39640;&#21534;&#21520;&#37327;&#26041;&#38754;&#20855;&#26377;&#35768;&#22810;&#20248;&#21183;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#23398;&#20064;&#30340;&#26032;&#26694;&#26550;&#65292;&#26681;&#25454;&#39532;&#23572;&#31185;&#32500;&#20107;&#20214;&#20272;&#35745;&#29289;&#32852;&#32593;&#35774;&#22791;&#30340;&#27969;&#37327;&#21040;&#36798;&#24773;&#20917;&#12290;&#23398;&#20064;&#36807;&#31243;&#29992;&#20110;&#20248;&#21270;&#22810;&#20010;&#26080;&#20154;&#26426;&#30340;&#36712;&#36857;&#21644;&#35843;&#24230;&#31574;&#30053;&#12290;&#39318;&#20808;&#65292;BS&#39044;&#27979;&#35774;&#22791;&#26410;&#26469;&#30340;&#27969;&#37327;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#20004;&#31181;&#27969;&#37327;&#39044;&#27979;&#22120;&#65306;&#21069;&#21521;&#31639;&#27861;(FA)&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;(LSTM)&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20248;&#21270;&#27599;&#20010;&#26080;&#20154;&#26426;&#26368;&#20248;&#31574;&#30053;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#38024;&#23545;&#25552;&#20986;&#30340;&#20248;&#21270;&#22870;&#21169;&#20989;&#25968;&#36827;&#34892;&#20102;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
The age of information (AoI) is used to measure the freshness of the data. In IoT networks, the traditional resource management schemes rely on a message exchange between the devices and the base station (BS) before communication which causes high AoI, high energy consumption, and low reliability. Unmanned aerial vehicles (UAVs) as flying BSs have many advantages in minimizing the AoI, energy-saving, and throughput improvement. In this paper, we present a novel learning-based framework that estimates the traffic arrival of IoT devices based on Markovian events. The learning proceeds to optimize the trajectory of multiple UAVs and their scheduling policy. First, the BS predicts the future traffic of the devices. We compare two traffic predictors: the forward algorithm (FA) and the long short-term memory (LSTM). Afterward, we propose a deep reinforcement learning (DRL) approach to optimize the optimal policy of each UAV. Finally, we manipulate the optimum reward function for the proposed
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;Hugging Face&#19978;&#30340;&#25968;&#25454;&#38598;&#25991;&#26723;&#65292;&#25552;&#20379;&#20102;Hugging Face&#25968;&#25454;&#38598;&#29983;&#24577;&#31995;&#32479;&#30340;&#27010;&#35272;&#24182;&#27934;&#23519;&#21040;&#25968;&#25454;&#38598;&#25991;&#26723;&#23454;&#36341;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#25968;&#25454;&#38598;&#21345;&#29255;&#23436;&#25104;&#29575;&#19982;&#25968;&#25454;&#38598;&#21463;&#27426;&#36814;&#31243;&#24230;&#23384;&#22312;&#26174;&#33879;&#24322;&#36136;&#24615;&#65292;&#20174;&#19994;&#32773;&#22312;&#25968;&#25454;&#38598;&#25551;&#36848;&#21644;&#25968;&#25454;&#38598;&#32467;&#26500;&#37096;&#20998;&#26356;&#20026;&#20851;&#27880;&#65292;&#32780;&#23545;&#20110;&#20351;&#29992;&#25968;&#25454;&#30340;&#32771;&#34385;&#37096;&#20998;&#36739;&#20026;&#24573;&#35270;&#12290;</title><link>http://arxiv.org/abs/2401.13822</link><description>&lt;p&gt;
&#22312;AI&#20013;&#27983;&#35272;&#25968;&#25454;&#38598;&#25991;&#26723;&#65306;Hugging Face&#25968;&#25454;&#38598;&#21345;&#29255;&#30340;&#22823;&#35268;&#27169;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Navigating Dataset Documentations in AI: A Large-Scale Analysis of Dataset Cards on Hugging Face. (arXiv:2401.13822v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13822
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;Hugging Face&#19978;&#30340;&#25968;&#25454;&#38598;&#25991;&#26723;&#65292;&#25552;&#20379;&#20102;Hugging Face&#25968;&#25454;&#38598;&#29983;&#24577;&#31995;&#32479;&#30340;&#27010;&#35272;&#24182;&#27934;&#23519;&#21040;&#25968;&#25454;&#38598;&#25991;&#26723;&#23454;&#36341;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#25968;&#25454;&#38598;&#21345;&#29255;&#23436;&#25104;&#29575;&#19982;&#25968;&#25454;&#38598;&#21463;&#27426;&#36814;&#31243;&#24230;&#23384;&#22312;&#26174;&#33879;&#24322;&#36136;&#24615;&#65292;&#20174;&#19994;&#32773;&#22312;&#25968;&#25454;&#38598;&#25551;&#36848;&#21644;&#25968;&#25454;&#38598;&#32467;&#26500;&#37096;&#20998;&#26356;&#20026;&#20851;&#27880;&#65292;&#32780;&#23545;&#20110;&#20351;&#29992;&#25968;&#25454;&#30340;&#32771;&#34385;&#37096;&#20998;&#36739;&#20026;&#24573;&#35270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#30340;&#36827;&#23637;&#19982;&#25968;&#25454;&#38598;&#30340;&#21019;&#24314;&#23494;&#20999;&#30456;&#20851;&#12290;&#34429;&#28982;&#25968;&#25454;&#25991;&#26723;&#34987;&#24191;&#27867;&#35748;&#20026;&#23545;&#20110;ML&#30340;&#21487;&#38752;&#24615;&#12289;&#21487;&#37325;&#22797;&#24615;&#21644;&#36879;&#26126;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#25105;&#20204;&#32570;&#20047;&#23545;&#24403;&#21069;&#25968;&#25454;&#38598;&#25991;&#26723;&#23454;&#36341;&#30340;&#31995;&#32479;&#23454;&#35777;&#29702;&#35299;&#12290;&#20026;&#20102;&#25581;&#31034;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20197;Hugging Face&#20026;&#20363;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#20849;&#20139;&#21644;&#21327;&#20316;ML&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#30340;&#26368;&#22823;&#24179;&#21488;&#12290;&#36890;&#36807;&#20998;&#26512;Hugging Face&#19978;&#30340;7433&#20010;&#25968;&#25454;&#38598;&#25991;&#26723;&#65292;&#25105;&#20204;&#30340;&#35843;&#26597;&#25552;&#20379;&#20102;Hugging Face&#25968;&#25454;&#38598;&#29983;&#24577;&#31995;&#32479;&#30340;&#27010;&#35272;&#21644;&#23545;&#25968;&#25454;&#38598;&#25991;&#26723;&#23454;&#36341;&#30340;&#27934;&#23519;&#65292;&#24471;&#20986;&#20102;5&#20010;&#20027;&#35201;&#21457;&#29616;&#65306;&#65288;1&#65289;&#25968;&#25454;&#38598;&#21345;&#29255;&#23436;&#25104;&#29575;&#26174;&#31034;&#20986;&#19982;&#25968;&#25454;&#38598;&#30340;&#21463;&#27426;&#36814;&#31243;&#24230;&#30456;&#20851;&#30340;&#26174;&#33879;&#24322;&#36136;&#24615;&#12290;&#65288;2&#65289;&#23545;&#25968;&#25454;&#38598;&#21345;&#29255;&#20013;&#30340;&#27599;&#20010;&#37096;&#20998;&#36827;&#34892;&#32454;&#33268;&#30340;&#32771;&#23519;&#34920;&#26126;&#65292;&#20174;&#19994;&#32773;&#20284;&#20046;&#26356;&#37325;&#35270;&#25968;&#25454;&#38598;&#25551;&#36848;&#21644;&#25968;&#25454;&#38598;&#32467;&#26500;&#37096;&#20998;&#65292;&#32780;&#23545;&#20110;&#20351;&#29992;&#25968;&#25454;&#30340;&#32771;&#34385;&#37096;&#20998;&#26356;&#21152;&#24573;&#35270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advances in machine learning are closely tied to the creation of datasets. While data documentation is widely recognized as essential to the reliability, reproducibility, and transparency of ML, we lack a systematic empirical understanding of current dataset documentation practices. To shed light on this question, here we take Hugging Face -- one of the largest platforms for sharing and collaborating on ML models and datasets -- as a prominent case study. By analyzing all 7,433 dataset documentation on Hugging Face, our investigation provides an overview of the Hugging Face dataset ecosystem and insights into dataset documentation practices, yielding 5 main findings: (1) The dataset card completion rate shows marked heterogeneity correlated with dataset popularity. (2) A granular examination of each section within the dataset card reveals that the practitioners seem to prioritize Dataset Description and Dataset Structure sections, while the Considerations for Using the Data section rec
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#20811;&#38534;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.13802</link><description>&lt;p&gt;
&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#20811;&#38534;&#26816;&#27979;&#26041;&#38754;&#30340;&#21151;&#25928;
&lt;/p&gt;
&lt;p&gt;
Investigating the Efficacy of Large Language Models for Code Clone Detection. (arXiv:2401.13802v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13802
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#20811;&#38534;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#36719;&#20214;&#24037;&#31243;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20363;&#22914;&#20195;&#30721;&#29983;&#25104;&#12290;LLMs&#20027;&#35201;&#22312;&#22522;&#20110;&#25552;&#31034;&#30340;&#38646;/&#23569;&#26679;&#26412;&#33539;&#24335;&#20013;&#34987;&#29992;&#20110;&#25351;&#23548;&#27169;&#22411;&#23436;&#25104;&#20219;&#21153;&#12290;&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;LLMs&#22312;&#20195;&#30721;&#20811;&#38534;&#26816;&#27979;&#65288;CCD&#65289;&#36825;&#19968;&#38750;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated remarkable success in various natural language processing and software engineering tasks, such as code generation. The LLMs are mainly utilized in the prompt-based zero/few-shot paradigm to guide the model in accomplishing the task. %\textbf{Goal:} GPT-based models are one of the popular ones studied for tasks such as code comment generation or test generation. These tasks are `generative' tasks. However, there is limited research on the usage of LLMs for `non-generative' tasks such as classification using the prompt-based paradigm. In this preliminary exploratory study, we investigated the applicability of LLMs for Code Clone Detection (CCD), a non-generative task. %\textbf{Method:} By building a mono-lingual and cross-lingual CCD dataset derived from CodeNet, we first investigated two different prompts using ChatGPT to detect \textcolor{black}{Type-4} code clones in Java-Java and Java-Ruby pairs in a zero-shot setting. We \textcolor{blac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#27844;&#38706;&#38382;&#39064;&#65292;&#21363;&#26410;&#39044;&#26399;&#30340;&#20449;&#24687;&#27745;&#26579;&#35757;&#32451;&#25968;&#25454;&#65292;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#35780;&#20272;&#65292;&#29992;&#25143;&#21487;&#33021;&#30001;&#20110;&#32570;&#20047;&#29702;&#35299;&#32780;&#24573;&#35270;&#20851;&#38190;&#27493;&#39588;&#65292;&#23548;&#33268;&#20048;&#35266;&#30340;&#24615;&#33021;&#20272;&#35745;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#19981;&#25104;&#31435;&#12290;</title><link>http://arxiv.org/abs/2401.13796</link><description>&lt;p&gt;
&#19981;&#35201;&#25353;&#25353;&#38062;&#65281;&#25506;&#32034;&#26426;&#22120;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#27844;&#38706;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Don't Push the Button! Exploring Data Leakage Risks in Machine Learning and Transfer Learning. (arXiv:2401.13796v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13796
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#27844;&#38706;&#38382;&#39064;&#65292;&#21363;&#26410;&#39044;&#26399;&#30340;&#20449;&#24687;&#27745;&#26579;&#35757;&#32451;&#25968;&#25454;&#65292;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#35780;&#20272;&#65292;&#29992;&#25143;&#21487;&#33021;&#30001;&#20110;&#32570;&#20047;&#29702;&#35299;&#32780;&#24573;&#35270;&#20851;&#38190;&#27493;&#39588;&#65292;&#23548;&#33268;&#20048;&#35266;&#30340;&#24615;&#33021;&#20272;&#35745;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#19981;&#25104;&#31435;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#38761;&#21629;&#24615;&#30340;&#36827;&#23637;&#65292;&#20026;&#22810;&#20010;&#39046;&#22495;&#25552;&#20379;&#20102;&#39044;&#27979;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;ML&#24037;&#20855;&#30340;&#26085;&#30410;&#21487;&#33719;&#24471;&#24615;&#65292;&#35768;&#22810;&#20174;&#19994;&#32773;&#32570;&#20047;&#28145;&#20837;&#30340;ML&#19987;&#19994;&#30693;&#35782;&#65292;&#37319;&#29992;&#20102;&#8220;&#25353;&#25353;&#38062;&#8221;&#26041;&#27861;&#65292;&#21033;&#29992;&#29992;&#25143;&#21451;&#22909;&#30340;&#30028;&#38754;&#32780;&#24573;&#35270;&#20102;&#24213;&#23618;&#31639;&#27861;&#30340;&#28145;&#20837;&#29702;&#35299;&#12290;&#34429;&#28982;&#36825;&#31181;&#26041;&#27861;&#25552;&#20379;&#20102;&#20415;&#21033;&#65292;&#20294;&#23427;&#24341;&#21457;&#20102;&#23545;&#32467;&#26524;&#21487;&#38752;&#24615;&#30340;&#25285;&#24551;&#65292;&#23548;&#33268;&#20102;&#38169;&#35823;&#30340;&#24615;&#33021;&#35780;&#20272;&#31561;&#25361;&#25112;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;ML&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#21363;&#25968;&#25454;&#27844;&#38706;&#65292;&#20854;&#20013;&#26410;&#39044;&#26399;&#30340;&#20449;&#24687;&#27745;&#26579;&#20102;&#35757;&#32451;&#25968;&#25454;&#65292;&#24433;&#21709;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#35780;&#20272;&#12290;&#30001;&#20110;&#32570;&#20047;&#29702;&#35299;&#65292;&#29992;&#25143;&#21487;&#33021;&#20250;&#26080;&#24847;&#20013;&#24573;&#35270;&#20851;&#38190;&#27493;&#39588;&#65292;&#20174;&#32780;&#23548;&#33268;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#21487;&#33021;&#19981;&#25104;&#31435;&#30340;&#20048;&#35266;&#24615;&#33021;&#20272;&#35745;&#12290;&#35780;&#20272;&#24615;&#33021;&#19982;&#23454;&#38469;&#22312;&#26032;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#30340;&#24046;&#24322;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#20851;&#27880;&#28857;&#12290;&#26412;&#25991;&#29305;&#21035;&#23558;ML&#20013;&#30340;&#25968;&#25454;&#27844;&#38706;&#20998;&#20026;&#19981;&#21516;&#31867;&#21035;&#65292;&#24182;&#35752;&#35770;&#20102;&#30456;&#20851;&#35299;&#20915;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Learning (ML) has revolutionized various domains, offering predictive capabilities in several areas. However, with the increasing accessibility of ML tools, many practitioners, lacking deep ML expertise, adopt a "push the button" approach, utilizing user-friendly interfaces without a thorough understanding of underlying algorithms. While this approach provides convenience, it raises concerns about the reliability of outcomes, leading to challenges such as incorrect performance evaluation. This paper addresses a critical issue in ML, known as data leakage, where unintended information contaminates the training data, impacting model performance evaluation. Users, due to a lack of understanding, may inadvertently overlook crucial steps, leading to optimistic performance estimates that may not hold in real-world scenarios. The discrepancy between evaluated and actual performance on new data is a significant concern. In particular, this paper categorizes data leakage in ML, discussi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20351;&#29992;&#28145;&#24230;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#23545;&#26234;&#33021;&#22478;&#24066;&#20013;&#30340;&#20132;&#36890;&#27169;&#24335;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#20132;&#36890;&#27169;&#24335;&#21160;&#24577;&#21644;&#24207;&#21015;&#29305;&#24449;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#36229;&#36807;&#29616;&#26377;&#26041;&#27861;&#30340;&#20934;&#30830;&#29575;&#12289;&#31934;&#30830;&#29575;&#12289;&#21484;&#22238;&#29575;&#21644;F1&#20998;&#25968;&#12290;&#30740;&#31350;&#32467;&#26524;&#23545;&#26234;&#33021;&#22478;&#24066;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2401.13794</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#23545;&#26234;&#33021;&#22478;&#24066;&#20013;&#30340;&#20132;&#36890;&#27169;&#24335;&#36827;&#34892;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Traffic Pattern Classification in Smart Cities Using Deep Recurrent Neural Network. (arXiv:2401.13794v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13794
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20351;&#29992;&#28145;&#24230;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#23545;&#26234;&#33021;&#22478;&#24066;&#20013;&#30340;&#20132;&#36890;&#27169;&#24335;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#20132;&#36890;&#27169;&#24335;&#21160;&#24577;&#21644;&#24207;&#21015;&#29305;&#24449;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#36229;&#36807;&#29616;&#26377;&#26041;&#27861;&#30340;&#20934;&#30830;&#29575;&#12289;&#31934;&#30830;&#29575;&#12289;&#21484;&#22238;&#29575;&#21644;F1&#20998;&#25968;&#12290;&#30740;&#31350;&#32467;&#26524;&#23545;&#26234;&#33021;&#22478;&#24066;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#28145;&#24230;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#23545;&#26234;&#33021;&#22478;&#24066;&#20013;&#30340;&#20132;&#36890;&#27169;&#24335;&#36827;&#34892;&#20998;&#31867;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30340;&#20132;&#36890;&#27169;&#24335;&#20998;&#31867;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#20132;&#36890;&#27169;&#24335;&#30340;&#21160;&#24577;&#21644;&#24207;&#21015;&#29305;&#24449;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#32467;&#21512;&#20102;&#21367;&#31215;&#21644;&#36882;&#24402;&#23618;&#65292;&#20174;&#20132;&#36890;&#27169;&#24335;&#25968;&#25454;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;SoftMax&#23618;&#23545;&#20132;&#36890;&#27169;&#24335;&#36827;&#34892;&#20998;&#31867;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#20934;&#30830;&#29575;&#12289;&#31934;&#30830;&#29575;&#12289;&#21484;&#22238;&#29575;&#21644;F1&#20998;&#25968;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#32467;&#26524;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#24182;&#35752;&#35770;&#20102;&#25152;&#25552;&#20986;&#27169;&#22411;&#23545;&#26234;&#33021;&#22478;&#24066;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#33021;&#22815;&#20197;&#39640;&#36798;95%&#30340;&#31934;&#30830;&#29575;&#20934;&#30830;&#20998;&#31867;&#26234;&#33021;&#22478;&#24066;&#20013;&#30340;&#20132;&#36890;&#27169;&#24335;&#12290;&#35813;&#27169;&#22411;&#22312;&#23454;&#38469;&#20132;&#36890;&#27169;&#24335;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#19982;&#29616;&#26377;&#20998;&#31867;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper examines the use of deep recurrent neural networks to classify traffic patterns in smart cities. We propose a novel approach to traffic pattern classification based on deep recurrent neural networks, which can effectively capture traffic patterns' dynamic and sequential features. The proposed model combines convolutional and recurrent layers to extract features from traffic pattern data and a SoftMax layer to classify traffic patterns. Experimental results show that the proposed model outperforms existing methods regarding accuracy, precision, recall, and F1 score. Furthermore, we provide an in depth analysis of the results and discuss the implications of the proposed model for smart cities. The results show that the proposed model can accurately classify traffic patterns in smart cities with a precision of as high as 95%. The proposed model is evaluated on a real world traffic pattern dataset and compared with existing classification methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31038;&#20132;&#23186;&#20307;&#24433;&#21709;&#32773;&#22312;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#30340;&#21487;&#35265;&#24615;&#26041;&#38754;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;&#34987;&#36825;&#20123;&#24433;&#21709;&#32773;&#35748;&#21487;&#30340;&#35770;&#25991;&#24341;&#29992;&#27425;&#25968;&#26174;&#33879;&#22686;&#21152;&#65292;&#20013;&#20301;&#25968;&#24341;&#29992;&#27425;&#25968;&#27604;&#23545;&#29031;&#32452;&#39640;2-3&#20493;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#34987;&#23637;&#31034;&#20316;&#32773;&#30340;&#22320;&#29702;&#12289;&#24615;&#21035;&#21644;&#26426;&#26500;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.13782</link><description>&lt;p&gt;
&#20174;&#25512;&#29305;&#21040;&#24341;&#29992;&#65306;&#25581;&#31034;&#31038;&#20132;&#23186;&#20307;&#24433;&#21709;&#32773;&#23545;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#21487;&#35265;&#24615;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Tweets to Citations: Unveiling the Impact of Social Media Influencers on AI Research Visibility. (arXiv:2401.13782v1 [cs.DL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13782
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31038;&#20132;&#23186;&#20307;&#24433;&#21709;&#32773;&#22312;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#30340;&#21487;&#35265;&#24615;&#26041;&#38754;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;&#34987;&#36825;&#20123;&#24433;&#21709;&#32773;&#35748;&#21487;&#30340;&#35770;&#25991;&#24341;&#29992;&#27425;&#25968;&#26174;&#33879;&#22686;&#21152;&#65292;&#20013;&#20301;&#25968;&#24341;&#29992;&#27425;&#25968;&#27604;&#23545;&#29031;&#32452;&#39640;2-3&#20493;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#34987;&#23637;&#31034;&#20316;&#32773;&#30340;&#22320;&#29702;&#12289;&#24615;&#21035;&#21644;&#26426;&#26500;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#20250;&#35758;&#19978;&#34987;&#25509;&#21463;&#30340;&#35770;&#25991;&#25968;&#37327;&#36798;&#21040;&#25968;&#21315;&#31687;&#65292;&#30740;&#31350;&#20154;&#21592;&#22914;&#20309;&#33719;&#21462;&#21644;&#38405;&#35835;&#30740;&#31350;&#35770;&#25991;&#21464;&#24471;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#31038;&#20132;&#23186;&#20307;&#24433;&#21709;&#32773;&#22312;&#22686;&#24378;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#21487;&#35265;&#24615;&#20013;&#30340;&#20316;&#29992;&#65292;&#29305;&#21035;&#26159;&#20182;&#20204;&#20998;&#20139;&#30340;&#35770;&#25991;&#24341;&#29992;&#27425;&#25968;&#12290;&#25105;&#20204;&#32534;&#21046;&#20102;&#19968;&#20010;&#21253;&#25324;8000&#22810;&#31687;&#35770;&#25991;&#30340;&#20840;&#38754;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;2018&#24180;12&#26376;&#33267;2023&#24180;10&#26376;&#30340;&#25512;&#29305;&#65292;&#20197;&#21450;&#22522;&#20110;&#20986;&#29256;&#24180;&#20221;&#12289;&#20250;&#35758;&#22320;&#28857;&#21644;&#25688;&#35201;&#20027;&#39064;&#36827;&#34892;1&#65306;1&#21305;&#37197;&#30340;&#23545;&#29031;&#32452;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#36825;&#20123;&#24433;&#21709;&#32773;&#35748;&#21487;&#30340;&#35770;&#25991;&#24341;&#29992;&#27425;&#25968;&#26174;&#33879;&#22686;&#21152;&#65292;&#20013;&#20301;&#25968;&#24341;&#29992;&#27425;&#25968;&#27604;&#23545;&#29031;&#32452;&#39640;2-3&#20493;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#36824;&#28145;&#20837;&#30740;&#31350;&#20102;&#34987;&#23637;&#31034;&#20316;&#32773;&#30340;&#22320;&#29702;&#12289;&#24615;&#21035;&#21644;&#26426;&#26500;&#22810;&#26679;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#31361;&#26174;&#20102;&#31038;&#20132;&#23186;&#20307;&#22312;&#23398;&#26415;&#20132;&#27969;&#20013;&#30340;&#19981;&#26029;&#25193;&#22823;&#30340;&#24433;&#21709;&#21147;&#65292;&#24182;&#24378;&#35843;&#20102;&#24403;&#20170;&#25968;&#23383;&#21270;&#26102;&#20195;&#19981;&#26029;&#21457;&#23637;&#30340;&#29983;&#24577;&#31995;&#32479;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the number of accepted papers at AI and ML conferences reaches into the thousands, it has become unclear how researchers access and read research publications. In this paper, we investigate the role of social media influencers in enhancing the visibility of machine learning research, particularly the citation counts of papers they share. We have compiled a comprehensive dataset of over 8,000 papers, spanning tweets from December 2018 to October 2023, alongside 1:1 matched controls based on publication year, venue, and abstract topics. Our analysis reveals a significant increase in citations for papers endorsed by these influencers, with median citation counts 2-3 times higher than those of the control group. Additionally, the study delves into the geographic, gender, and institutional diversity of highlighted authors. These findings highlight the expanding influence of social media in scholarly communication and underscore the importance of an evolving ecosystem in today's digital a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BASS&#30340;&#22522;&#20110;&#24191;&#25773;&#30340;&#23376;&#22270;&#37319;&#26679;&#26041;&#27861;&#65292;&#29992;&#20110;&#21152;&#36895;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#20943;&#23569;&#36890;&#20449;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2401.13779</link><description>&lt;p&gt;
&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#26356;&#23569;&#30340;&#36890;&#20449;&#25104;&#26412;&#65306;&#29992;&#20110;&#26080;&#32447;&#32593;&#32476;&#30340;&#22522;&#20110;&#24191;&#25773;&#30340;&#23376;&#22270;&#37319;&#26679;&#30340;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Faster Convergence with Less Communication: Broadcast-Based Subgraph Sampling for Decentralized Learning over Wireless Networks. (arXiv:2401.13779v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13779
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BASS&#30340;&#22522;&#20110;&#24191;&#25773;&#30340;&#23376;&#22270;&#37319;&#26679;&#26041;&#27861;&#65292;&#29992;&#20110;&#21152;&#36895;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#20943;&#23569;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20849;&#35782;&#30340;&#21435;&#20013;&#24515;&#21270;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;(D-SGD)&#26159;&#19968;&#31181;&#24191;&#27867;&#37319;&#29992;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#32593;&#32476;&#20195;&#29702;&#20043;&#38388;&#30340;&#21435;&#20013;&#24515;&#21270;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#12290;D-SGD&#30340;&#19968;&#20010;&#20851;&#38190;&#37096;&#20998;&#26159;&#22522;&#20110;&#20849;&#35782;&#30340;&#27169;&#22411;&#24179;&#22343;&#65292;&#23427;&#20005;&#37325;&#20381;&#36182;&#20110;&#33410;&#28857;&#20043;&#38388;&#30340;&#20449;&#24687;&#20132;&#25442;&#21644;&#34701;&#21512;&#12290;&#29305;&#21035;&#22320;&#65292;&#23545;&#20110;&#22312;&#26080;&#32447;&#32593;&#32476;&#19978;&#30340;&#20849;&#35782;&#24179;&#22343;&#65292;&#36890;&#20449;&#21327;&#35843;&#26159;&#24517;&#35201;&#30340;&#65292;&#20197;&#30830;&#23450;&#33410;&#28857;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#35775;&#38382;&#20449;&#36947;&#65292;&#24182;&#23558;&#20449;&#24687;&#20256;&#36755;&#65288;&#25110;&#25509;&#25910;&#65289;&#32473;&#65288;&#25110;&#20174;&#65289;&#37051;&#23621;&#33410;&#28857;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BASS&#30340;&#22522;&#20110;&#24191;&#25773;&#30340;&#23376;&#22270;&#37319;&#26679;&#26041;&#27861;&#65292;&#26088;&#22312;&#21152;&#24555;D-SGD&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#32771;&#34385;&#27599;&#36718;&#36845;&#20195;&#30340;&#23454;&#38469;&#36890;&#20449;&#25104;&#26412;&#12290;BASS&#21019;&#24314;&#19968;&#32452;&#28151;&#21512;&#30697;&#38453;&#20505;&#36873;&#39033;&#65292;&#34920;&#31034;&#22522;&#30784;&#25299;&#25169;&#30340;&#31232;&#30095;&#23376;&#22270;&#12290;&#22312;&#27599;&#20010;&#20849;&#35782;&#36845;&#20195;&#20013;&#65292;&#23558;&#37319;&#26679;&#19968;&#20010;&#28151;&#21512;&#30697;&#38453;&#65292;&#20174;&#32780;&#20135;&#29983;&#19968;&#20010;&#29305;&#23450;&#30340;&#35843;&#24230;&#20915;&#31574;&#65292;&#28608;&#27963;&#22810;&#20010;&#26080;&#30896;&#25758;&#30340;&#33410;&#28857;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Consensus-based decentralized stochastic gradient descent (D-SGD) is a widely adopted algorithm for decentralized training of machine learning models across networked agents. A crucial part of D-SGD is the consensus-based model averaging, which heavily relies on information exchange and fusion among the nodes. Specifically, for consensus averaging over wireless networks, communication coordination is necessary to determine when and how a node can access the channel and transmit (or receive) information to (or from) its neighbors. In this work, we propose $\texttt{BASS}$, a broadcast-based subgraph sampling method designed to accelerate the convergence of D-SGD while considering the actual communication cost per iteration. $\texttt{BASS}$ creates a set of mixing matrix candidates that represent sparser subgraphs of the base topology. In each consensus iteration, one mixing matrix is sampled, leading to a specific scheduling decision that activates multiple collision-free subsets of node
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22810;&#35270;&#22270;&#22270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#33268;&#22270;&#26469;&#30830;&#20445;&#35270;&#22270;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#21644;&#20849;&#20139;&#32467;&#26500;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#25512;&#26029;&#20986;&#35270;&#22270;&#20849;&#20139;&#32467;&#26500;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.13769</link><description>&lt;p&gt;
&#22810;&#35270;&#22270;&#22270;&#23398;&#20064;&#19982;&#19968;&#33268;&#22270;
&lt;/p&gt;
&lt;p&gt;
Multiview Graph Learning with Consensus Graph. (arXiv:2401.13769v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13769
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22810;&#35270;&#22270;&#22270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#33268;&#22270;&#26469;&#30830;&#20445;&#35270;&#22270;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#21644;&#20849;&#20139;&#32467;&#26500;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#25512;&#26029;&#20986;&#35270;&#22270;&#20849;&#20139;&#32467;&#26500;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#25299;&#25169;&#25512;&#26029;&#65292;&#21363;&#20174;&#32473;&#23450;&#30340;&#33410;&#28857;&#35266;&#23519;&#25968;&#25454;&#20013;&#23398;&#20064;&#22270;&#65292;&#26159;&#35768;&#22810;&#24212;&#29992;&#39046;&#22495;&#20013;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#22823;&#22810;&#23616;&#38480;&#20110;&#23398;&#20064;&#21333;&#20010;&#22270;&#65292;&#20551;&#35774;&#35266;&#23519;&#25968;&#25454;&#26159;&#22343;&#21248;&#30340;&#12290;&#36825;&#26159;&#26377;&#38382;&#39064;&#30340;&#65292;&#22240;&#20026;&#35768;&#22810;&#29616;&#20195;&#25968;&#25454;&#38598;&#26159;&#24322;&#36136;&#30340;&#25110;&#28151;&#21512;&#30340;&#65292;&#24182;&#28041;&#21450;&#22810;&#20010;&#30456;&#20851;&#30340;&#22270;&#65292;&#21363;&#22810;&#35270;&#22270;&#22270;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#23398;&#20064;&#22810;&#35270;&#22270;&#22270;&#65292;&#36890;&#36807;&#25104;&#23545;&#27491;&#21017;&#21270;&#20445;&#35777;&#20102;&#23398;&#20064;&#30340;&#35270;&#22270;&#22270;&#30340;&#30456;&#20284;&#24615;&#65292;&#21363;&#40723;&#21169;&#27599;&#23545;&#35270;&#22270;&#20855;&#26377;&#30456;&#20284;&#30340;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#26080;&#27861;&#25512;&#26029;&#20986;&#35270;&#22270;&#20043;&#38388;&#30340;&#20849;&#20139;&#32467;&#26500;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19968;&#33268;&#27491;&#21017;&#21270;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#34920;&#31034;&#35270;&#22270;&#30340;&#20849;&#21516;&#32467;&#26500;&#30340;&#19968;&#33268;&#22270;&#65292;&#30830;&#20445;&#35270;&#22270;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#65292;&#20551;&#35774;&#22270;&#25968;&#25454;&#22312;&#22810;&#35270;&#22270;&#22270;&#19978;&#26159;&#24179;&#28369;&#30340;&#65292;&#24182;&#19988;&#22270;&#30340;&#25299;&#25169;&#32467;&#26500;&#22312;&#35270;&#22270;&#20043;&#38388;&#26159;&#19968;&#33268;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph topology inference, i.e., learning graphs from a given set of nodal observations, is a significant task in many application domains. Existing approaches are mostly limited to learning a single graph assuming that the observed data is homogeneous. This is problematic because many modern datasets are heterogeneous or mixed and involve multiple related graphs, i.e., multiview graphs. Recent work proposing to learn multiview graphs ensures the similarity of learned view graphs through pairwise regularization, where each pair of views is encouraged to have similar structures. However, this approach cannot infer the shared structure across views. In this work, we propose an alternative method based on consensus regularization, where views are ensured to be similar through a learned consensus graph representing the common structure of the views. In particular, we propose an optimization problem, where graph data is assumed to be smooth over the multiview graph and the topology of the in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#21021;&#32423;&#21307;&#30103;&#25252;&#29702;&#37492;&#21035;&#35786;&#26029;&#30340;&#21512;&#25104;&#30149;&#21382;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21307;&#23398;&#30693;&#35782;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#26500;&#24314;&#20102;&#20855;&#26377;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#24739;&#32773;&#35760;&#24405;&#65292;&#24182;&#23637;&#31034;&#20102;&#20351;&#29992;&#36825;&#20123;&#25968;&#25454;&#35757;&#32451;&#30142;&#30149;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.13756</link><description>&lt;p&gt;
NLICE: &#38024;&#23545;&#26377;&#25928;&#30340;&#21021;&#32423;&#21307;&#30103;&#25252;&#29702;&#37492;&#21035;&#35786;&#26029;&#30340;&#21512;&#25104;&#30149;&#21382;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
NLICE: Synthetic Medical Record Generation for Effective Primary Healthcare Differential Diagnosis. (arXiv:2401.13756v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13756
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#21021;&#32423;&#21307;&#30103;&#25252;&#29702;&#37492;&#21035;&#35786;&#26029;&#30340;&#21512;&#25104;&#30149;&#21382;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21307;&#23398;&#30693;&#35782;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#26500;&#24314;&#20102;&#20855;&#26377;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#24739;&#32773;&#35760;&#24405;&#65292;&#24182;&#23637;&#31034;&#20102;&#20351;&#29992;&#36825;&#20123;&#25968;&#25454;&#35757;&#32451;&#30142;&#30149;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#31995;&#32479;&#26041;&#27861;&#65292;&#29992;&#20110;&#21019;&#24314;&#22522;&#20110;&#21307;&#23398;&#30693;&#35782;&#30340;&#24739;&#32773;&#35760;&#24405;&#65292;&#29992;&#20110;&#37492;&#21035;&#35786;&#26029;&#30456;&#20851;&#27963;&#21160;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20379;&#20102;&#35780;&#20272;&#22522;&#20110;&#32473;&#23450;&#30151;&#29366;&#33021;&#22815;&#21306;&#20998;&#21508;&#31181;&#29366;&#20917;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;&#21517;&#20026;SymCat&#30340;&#20844;&#20849;&#30142;&#30149;-&#30151;&#29366;&#25968;&#25454;&#28304;&#32467;&#21512;Synthea&#26469;&#26500;&#24314;&#24739;&#32773;&#35760;&#24405;&#12290;&#20026;&#20102;&#22686;&#21152;&#21512;&#25104;&#25968;&#25454;&#30340;&#34920;&#36798;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#31181;&#21307;&#23398;&#26631;&#20934;&#21270;&#30340;&#30151;&#29366;&#24314;&#27169;&#26041;&#27861;NLICE&#65292;&#20026;&#27599;&#31181;&#29366;&#20917;&#30340;&#21512;&#25104;&#25968;&#25454;&#22686;&#21152;&#39069;&#22806;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#36824;&#23545;&#26420;&#32032;&#36125;&#21494;&#26031;&#21644;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#21644;&#27604;&#36739;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#25104;&#21151;&#26500;&#24314;&#22522;&#20110;SymCat&#21644;NLICE&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#23637;&#31034;&#20102;&#20351;&#29992;&#36825;&#20123;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#39044;&#27979;&#30142;&#30149;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper offers a systematic method for creating medical knowledge-grounded patient records for use in activities involving differential diagnosis. Additionally, an assessment of machine learning models that can differentiate between various conditions based on given symptoms is also provided. We use a public disease-symptom data source called SymCat in combination with Synthea to construct the patients records. In order to increase the expressive nature of the synthetic data, we use a medically-standardized symptom modeling method called NLICE to augment the synthetic data with additional contextual information for each condition. In addition, Naive Bayes and Random Forest models are evaluated and compared on the synthetic data. The paper shows how to successfully construct SymCat-based and NLICE-based datasets. We also show results for the effectiveness of using the datasets to train predictive disease models. The SymCat-based dataset is able to train a Naive Bayes and Random Fores
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#31995;&#32479;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#38024;&#23545;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#40065;&#26834;&#24615;&#24314;&#27169;&#12290;&#30740;&#31350;&#21457;&#29616;&#38544;&#34255;&#23618;&#25968;&#37327;&#23545;&#27169;&#22411;&#30340;&#25512;&#24191;&#24615;&#33021;&#26377;&#24433;&#21709;&#65292;&#21516;&#26102;&#36824;&#27979;&#35797;&#20102;&#27169;&#22411;&#22823;&#23567;&#12289;&#28014;&#28857;&#31934;&#24230;&#12289;&#35757;&#32451;&#25968;&#25454;&#21644;&#27169;&#22411;&#36755;&#20986;&#30340;&#22122;&#22768;&#27700;&#24179;&#31561;&#21442;&#25968;&#12290;&#20026;&#20102;&#25913;&#36827;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#21644;&#35745;&#31639;&#25104;&#26412;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35825;&#21457;&#25925;&#38556;&#26469;&#24314;&#27169;&#25925;&#38556;&#27010;&#29575;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.13751</link><description>&lt;p&gt;
&#19968;&#31181;&#38024;&#23545;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#24314;&#27169;&#30340;&#31995;&#32479;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Systematic Approach to Robustness Modelling for Deep Convolutional Neural Networks. (arXiv:2401.13751v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#31995;&#32479;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#38024;&#23545;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#40065;&#26834;&#24615;&#24314;&#27169;&#12290;&#30740;&#31350;&#21457;&#29616;&#38544;&#34255;&#23618;&#25968;&#37327;&#23545;&#27169;&#22411;&#30340;&#25512;&#24191;&#24615;&#33021;&#26377;&#24433;&#21709;&#65292;&#21516;&#26102;&#36824;&#27979;&#35797;&#20102;&#27169;&#22411;&#22823;&#23567;&#12289;&#28014;&#28857;&#31934;&#24230;&#12289;&#35757;&#32451;&#25968;&#25454;&#21644;&#27169;&#22411;&#36755;&#20986;&#30340;&#22122;&#22768;&#27700;&#24179;&#31561;&#21442;&#25968;&#12290;&#20026;&#20102;&#25913;&#36827;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#21644;&#35745;&#31639;&#25104;&#26412;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35825;&#21457;&#25925;&#38556;&#26469;&#24314;&#27169;&#25925;&#38556;&#27010;&#29575;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#26377;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#21487;&#29992;&#26102;&#65292;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#34987;&#35777;&#26126;&#22312;&#35768;&#22810;&#39046;&#22495;&#37117;&#21487;&#20197;&#24191;&#27867;&#24212;&#29992;&#12290;&#26368;&#36817;&#30340;&#36235;&#21183;&#26159;&#20351;&#29992;&#20855;&#26377;&#36234;&#26469;&#36234;&#22810;&#21487;&#35843;&#21442;&#25968;&#30340;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#65292;&#38477;&#20302;&#27169;&#22411;&#25439;&#22833;&#25110;&#21019;&#24314;&#26356;&#20855;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#27169;&#22411;&#65292;&#32780;&#36825;&#20123;&#30446;&#26631;&#36890;&#24120;&#30456;&#20114;&#30683;&#30462;&#12290;&#29305;&#21035;&#26159;&#65292;&#26368;&#36817;&#30340;&#29702;&#35770;&#30740;&#31350;&#25552;&#20986;&#20102;&#23545;&#26356;&#22823;&#27169;&#22411;&#33021;&#21542;&#25512;&#24191;&#21040;&#21463;&#25511;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#38598;&#20043;&#22806;&#30340;&#25968;&#25454;&#30340;&#30097;&#38382;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;ResNet&#27169;&#22411;&#20013;&#38544;&#34255;&#23618;&#30340;&#25968;&#37327;&#22312;MNIST&#12289;CIFAR10&#21644;CIFAR100&#25968;&#25454;&#38598;&#19978;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;&#21508;&#31181;&#21442;&#25968;&#65292;&#21253;&#25324;&#27169;&#22411;&#30340;&#22823;&#23567;&#12289;&#28014;&#28857;&#31934;&#24230;&#65292;&#20197;&#21450;&#35757;&#32451;&#25968;&#25454;&#21644;&#27169;&#22411;&#36755;&#20986;&#30340;&#22122;&#22768;&#27700;&#24179;&#12290;&#20026;&#20102;&#25913;&#36827;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#21644;&#35745;&#31639;&#25104;&#26412;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#20351;&#29992;&#35825;&#21457;&#25925;&#38556;&#26469;&#24314;&#27169;&#25925;&#38556;&#27010;&#29575;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Convolutional neural networks have shown to be widely applicable to a large number of fields when large amounts of labelled data are available. The recent trend has been to use models with increasingly larger sets of tunable parameters to increase model accuracy, reduce model loss, or create more adversarially robust models -- goals that are often at odds with one another. In particular, recent theoretical work raises questions about the ability for even larger models to generalize to data outside of the controlled train and test sets. As such, we examine the role of the number of hidden layers in the ResNet model, demonstrated on the MNIST, CIFAR10, CIFAR100 datasets. We test a variety of parameters including the size of the model, the floating point precision, and the noise level of both the training data and the model output. To encapsulate the model's predictive power and computational cost, we provide a method that uses induced failures to model the probability of failure as a fun
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#35268;&#33539;&#39044;&#27979;&#37327;&#21270;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#21487;&#20197;&#25552;&#39640;&#20154;&#31867;&#20915;&#31574;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#26524;&#65292;&#23545;&#20154;&#26426;&#21327;&#21516;&#20915;&#31574;&#20855;&#26377;&#23454;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2401.13744</link><description>&lt;p&gt;
&#12298;&#35268;&#33539;&#39044;&#27979;&#38598;&#25552;&#21319;&#20154;&#31867;&#20915;&#31574;&#33021;&#21147;&#12299;
&lt;/p&gt;
&lt;p&gt;
Conformal Prediction Sets Improve Human Decision Making. (arXiv:2401.13744v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13744
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#35268;&#33539;&#39044;&#27979;&#37327;&#21270;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#21487;&#20197;&#25552;&#39640;&#20154;&#31867;&#20915;&#31574;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#26524;&#65292;&#23545;&#20154;&#26426;&#21327;&#21516;&#20915;&#31574;&#20855;&#26377;&#23454;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#23545;&#26085;&#24120;&#26597;&#35810;&#30340;&#22238;&#24212;&#65292;&#20154;&#31867;&#26126;&#30830;&#22320;&#34920;&#36798;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#22312;&#19981;&#30830;&#23450;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#26367;&#20195;&#31572;&#26696;&#12290;&#36890;&#36807;&#35268;&#33539;&#39044;&#27979;&#36755;&#20986;&#26657;&#20934;&#30340;&#39044;&#27979;&#38598;&#65292;&#27169;&#20223;&#20102;&#20154;&#31867;&#30340;&#36825;&#31181;&#34892;&#20026;&#65307;&#26356;&#22823;&#30340;&#39044;&#27979;&#38598;&#34920;&#31034;&#26356;&#22823;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#26367;&#20195;&#26041;&#26696;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#26045;&#39044;&#27880;&#20876;&#30340;&#38543;&#26426;&#23545;&#29031;&#35797;&#39564;&#65292;&#24182;&#32473;&#20154;&#31867;&#21463;&#35797;&#32773;&#25552;&#20379;&#35268;&#33539;&#39044;&#27979;&#38598;&#65292;&#30740;&#31350;&#20102;&#35268;&#33539;&#39044;&#27979;&#38598;&#23545;&#20154;&#31867;&#20915;&#31574;&#30340;&#23454;&#29992;&#24615;&#12290;&#36890;&#36807;&#32479;&#35745;&#23398;&#26174;&#33879;&#24615;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#20154;&#31867;&#33719;&#24471;&#35268;&#33539;&#39044;&#27979;&#38598;&#26102;&#65292;&#20182;&#20204;&#22312;&#20219;&#21153;&#19978;&#30340;&#20934;&#30830;&#24615;&#27604;&#20351;&#29992;&#30456;&#21516;&#35206;&#30422;&#20445;&#35777;&#30340;&#22266;&#23450;&#23610;&#23544;&#39044;&#27979;&#38598;&#26102;&#26377;&#25152;&#25552;&#39640;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#29992;&#35268;&#33539;&#39044;&#27979;&#37327;&#21270;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#26377;&#21161;&#20110;&#20154;&#26426;&#21327;&#21516;&#20915;&#31574;&#21644;&#20154;&#24037;&#26234;&#33021;&#22242;&#38431;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
In response to everyday queries, humans explicitly signal uncertainty and offer alternative answers when they are unsure. Machine learning models that output calibrated prediction sets through conformal prediction mimic this human behaviour; larger sets signal greater uncertainty while providing alternatives. In this work, we study the usefulness of conformal prediction sets as an aid for human decision making by conducting a pre-registered randomized controlled trial with conformal prediction sets provided to human subjects. With statistical significance, we find that when humans are given conformal prediction sets their accuracy on tasks improves compared to fixed-size prediction sets with the same coverage guarantee. The results show that quantifying model uncertainty with conformal prediction is helpful for human-in-the-loop decision making and human-AI teams.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21516;&#26102;&#23637;&#31034;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#20010;&#22238;&#24212;&#65292;&#24182;&#35774;&#35745;&#20102;&#20116;&#20010;&#21151;&#33021;&#26469;&#25903;&#25345;&#29992;&#25143;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#36827;&#34892;&#29702;&#35299;&#12290;&#36890;&#36807;&#29992;&#25143;&#30740;&#31350;&#21644;&#26696;&#20363;&#30740;&#31350;&#65292;&#35813;&#30740;&#31350;&#21457;&#29616;&#36825;&#20123;&#21151;&#33021;&#33021;&#22815;&#25903;&#25345;&#21508;&#31181;&#29702;&#35299;&#20219;&#21153;&#65292;&#29978;&#33267;&#35299;&#20915;&#20102;&#20197;&#21069;&#34987;&#35748;&#20026;&#36807;&#20110;&#22256;&#38590;&#30340;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2401.13726</link><description>&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#20013;&#25903;&#25345;&#29702;&#35299;&#30340;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Supporting Sensemaking of Large Language Model Outputs at Scale. (arXiv:2401.13726v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13726
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21516;&#26102;&#23637;&#31034;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#20010;&#22238;&#24212;&#65292;&#24182;&#35774;&#35745;&#20102;&#20116;&#20010;&#21151;&#33021;&#26469;&#25903;&#25345;&#29992;&#25143;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#36827;&#34892;&#29702;&#35299;&#12290;&#36890;&#36807;&#29992;&#25143;&#30740;&#31350;&#21644;&#26696;&#20363;&#30740;&#31350;&#65292;&#35813;&#30740;&#31350;&#21457;&#29616;&#36825;&#20123;&#21151;&#33021;&#33021;&#22815;&#25903;&#25345;&#21508;&#31181;&#29702;&#35299;&#20219;&#21153;&#65292;&#29978;&#33267;&#35299;&#20915;&#20102;&#20197;&#21069;&#34987;&#35748;&#20026;&#36807;&#20110;&#22256;&#38590;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#23545;&#21333;&#20010;&#25552;&#31034;&#29983;&#25104;&#22810;&#31181;&#22238;&#24212;&#65292;&#28982;&#32780;&#24456;&#23569;&#26377;&#30740;&#31350;&#24037;&#20316;&#33457;&#36153;&#22312;&#24110;&#21161;&#26368;&#32456;&#29992;&#25143;&#25110;&#31995;&#32479;&#35774;&#35745;&#32773;&#21033;&#29992;&#36825;&#31181;&#33021;&#21147;&#19978;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#21516;&#26102;&#21576;&#29616;&#22810;&#20010;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#22238;&#24212;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#20116;&#20010;&#21151;&#33021;&#65292;&#20854;&#20013;&#21253;&#25324;&#35745;&#31639;&#25991;&#26412;&#25991;&#26723;&#20043;&#38388;&#30456;&#20284;&#24615;&#21644;&#24046;&#24322;&#24615;&#30340;&#29616;&#26377;&#21644;&#26032;&#39062;&#26041;&#27861;&#65292;&#20197;&#21450;&#22914;&#20309;&#21576;&#29616;&#23427;&#20204;&#30340;&#36755;&#20986;&#12290;&#25105;&#20204;&#25317;&#26377;&#19968;&#39033;&#21463;&#25511;&#29992;&#25143;&#30740;&#31350;&#65288;n=24&#65289;&#21644;&#20843;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#35780;&#20272;&#20102;&#36825;&#20123;&#21151;&#33021;&#20197;&#21450;&#23427;&#20204;&#22914;&#20309;&#25903;&#25345;&#29992;&#25143;&#36827;&#34892;&#19981;&#21516;&#20219;&#21153;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#21151;&#33021;&#25903;&#25345;&#21508;&#31181;&#29702;&#35299;&#20219;&#21153;&#65292;&#29978;&#33267;&#20351;&#20197;&#21069;&#34987;&#21442;&#19982;&#32773;&#35748;&#20026;&#36807;&#20110;&#22256;&#38590;&#30340;&#20219;&#21153;&#21464;&#24471;&#21487;&#34892;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35774;&#35745;&#25351;&#21335;&#65292;&#20197;&#25351;&#23548;&#26410;&#26469;&#23545;&#26032;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25509;&#21475;&#30340;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are capable of generating multiple responses to a single prompt, yet little effort has been expended to help end-users or system designers make use of this capability. In this paper, we explore how to present many LLM responses at once. We design five features, which include both pre-existing and novel methods for computing similarities and differences across textual documents, as well as how to render their outputs. We report on a controlled user study (n=24) and eight case studies evaluating these features and how they support users in different tasks. We find that the features support a wide variety of sensemaking tasks and even make tasks previously considered to be too difficult by our participants now tractable. Finally, we present design guidelines to inform future explorations of new LLM interfaces.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#19981;&#30830;&#23450;&#24615;&#24341;&#23548;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#22238;&#24402;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#19981;&#30830;&#23450;&#24615;&#20316;&#20026;&#32622;&#20449;&#24230;&#20272;&#35745;&#21644;&#23884;&#20837;&#31354;&#38388;&#30340;&#27491;&#21017;&#39033;&#26469;&#23454;&#29616;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2401.13721</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#22238;&#24402;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#24341;&#23548;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-Guided Alignment for Unsupervised Domain Adaptation in Regression. (arXiv:2401.13721v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13721
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#19981;&#30830;&#23450;&#24615;&#24341;&#23548;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#22238;&#24402;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#19981;&#30830;&#23450;&#24615;&#20316;&#20026;&#32622;&#20449;&#24230;&#20272;&#35745;&#21644;&#23884;&#20837;&#31354;&#38388;&#30340;&#27491;&#21017;&#39033;&#26469;&#23454;&#29616;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#22238;&#24402;&#65288;UDAR&#65289;&#26088;&#22312;&#23558;&#26469;&#33258;&#26377;&#26631;&#31614;&#28304;&#39046;&#22495;&#30340;&#27169;&#22411;&#35843;&#25972;&#21040;&#26080;&#26631;&#31614;&#30446;&#26631;&#39046;&#22495;&#65292;&#20197;&#23436;&#25104;&#22238;&#24402;&#20219;&#21153;&#12290;&#26368;&#36817;&#22312;UDAR&#39046;&#22495;&#21462;&#24471;&#30340;&#25104;&#21151;&#20027;&#35201;&#38598;&#20013;&#22312;&#23376;&#31354;&#38388;&#23545;&#40784;&#19978;&#65292;&#28041;&#21450;&#25972;&#20010;&#29305;&#24449;&#31354;&#38388;&#20013;&#25152;&#36873;&#25321;&#23376;&#31354;&#38388;&#30340;&#23545;&#40784;&#12290;&#36825;&#19982;&#29992;&#20110;&#20998;&#31867;&#30340;&#29305;&#24449;&#23545;&#40784;&#26041;&#27861;&#24418;&#25104;&#23545;&#27604;&#65292;&#21518;&#32773;&#26088;&#22312;&#23545;&#40784;&#25972;&#20010;&#29305;&#24449;&#31354;&#38388;&#65292;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#24050;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#22312;&#22238;&#24402;&#20219;&#21153;&#20013;&#25928;&#26524;&#36739;&#24046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20998;&#31867;&#20219;&#21153;&#26088;&#22312;&#22312;&#25972;&#20010;&#23884;&#20837;&#31354;&#38388;&#30340;&#32500;&#24230;&#19978;&#35782;&#21035;&#29420;&#31435;&#30340;&#31751;&#65292;&#32780;&#22238;&#24402;&#20219;&#21153;&#23545;&#25968;&#25454;&#34920;&#31034;&#30340;&#32467;&#26500;&#24615;&#35201;&#27714;&#36739;&#20302;&#65292;&#38656;&#35201;&#39069;&#22806;&#30340;&#25351;&#23548;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#23545;&#40784;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#24341;&#23548;&#30340;&#26377;&#25928;UDAR&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#21452;&#37325;&#20316;&#29992;&#65306;&#25552;&#20379;&#20102;&#23545;&#39044;&#27979;&#32467;&#26524;&#30340;&#32622;&#20449;&#24230;&#34913;&#37327;&#65292;&#24182;&#20316;&#20026;&#23884;&#20837;&#31354;&#38388;&#30340;&#27491;&#21017;&#21270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;&#28145;&#24230;&#35777;&#25454;&#27169;&#22411;&#26469;&#25552;&#20379;&#23545;&#39044;&#27979;&#30340;&#32622;&#20449;&#24230;&#20272;&#35745;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#23884;&#20837;&#31354;&#38388;&#30340;&#27491;&#21017;&#39033;&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Domain Adaptation for Regression (UDAR) aims to adapt a model from a labeled source domain to an unlabeled target domain for regression tasks. Recent successful works in UDAR mostly focus on subspace alignment, involving the alignment of a selected subspace within the entire feature space. This contrasts with the feature alignment methods used for classification, which aim at aligning the entire feature space and have proven effective but are less so in regression settings. Specifically, while classification aims to identify separate clusters across the entire embedding dimension, regression induces less structure in the data representation, necessitating additional guidance for efficient alignment. In this paper, we propose an effective method for UDAR by incorporating guidance from uncertainty. Our approach serves a dual purpose: providing a measure of confidence in predictions and acting as a regularization of the embedding space. Specifically, we leverage the Deep Evid
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#27809;&#26377;&#20998;&#31867;&#23618;&#30340;&#23454;&#38469;&#20154;&#33080;&#35782;&#21035;&#27169;&#22411;&#30340;&#26032;&#22411;&#25512;&#29702;&#25915;&#20987;&#65292;&#36890;&#36807;&#20998;&#26512;&#20013;&#38388;&#29305;&#24449;&#19982;&#25209;&#24402;&#19968;&#21270;&#21442;&#25968;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#25915;&#20987;&#27169;&#22411;&#65292;&#21487;&#20197;&#30830;&#23450;&#19968;&#20010;&#20154;&#33080;&#22270;&#20687;&#26159;&#21542;&#23646;&#20110;&#27169;&#22411;&#30340;&#25104;&#21592;&#12290;</title><link>http://arxiv.org/abs/2401.13719</link><description>&lt;p&gt;
&#19981;&#38656;&#35201;&#20998;&#31867;&#23618;&#30340;&#20154;&#33080;&#35782;&#21035;&#27169;&#22411;&#30340;&#25512;&#29702;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Inference Attacks Against Face Recognition Model without Classification Layers. (arXiv:2401.13719v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13719
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#27809;&#26377;&#20998;&#31867;&#23618;&#30340;&#23454;&#38469;&#20154;&#33080;&#35782;&#21035;&#27169;&#22411;&#30340;&#26032;&#22411;&#25512;&#29702;&#25915;&#20987;&#65292;&#36890;&#36807;&#20998;&#26512;&#20013;&#38388;&#29305;&#24449;&#19982;&#25209;&#24402;&#19968;&#21270;&#21442;&#25968;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#25915;&#20987;&#27169;&#22411;&#65292;&#21487;&#20197;&#30830;&#23450;&#19968;&#20010;&#20154;&#33080;&#22270;&#20687;&#26159;&#21542;&#23646;&#20110;&#27169;&#22411;&#30340;&#25104;&#21592;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#33080;&#35782;&#21035;(FR)&#24050;&#32463;&#24212;&#29992;&#20110;&#26085;&#24120;&#29983;&#27963;&#30340;&#20960;&#20046;&#25152;&#26377;&#26041;&#38754;&#65292;&#20294;&#23427;&#22987;&#32456;&#20276;&#38543;&#30528;&#27844;&#28431;&#31169;&#20154;&#20449;&#24687;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;&#30446;&#21069;&#65292;&#20960;&#20046;&#25152;&#26377;&#23545;FR&#30340;&#25915;&#20987;&#27169;&#22411;&#37117;&#20005;&#37325;&#20381;&#36182;&#20110;&#20998;&#31867;&#23618;&#30340;&#23384;&#22312;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;FR&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#27169;&#22411;&#39592;&#24178;&#33719;&#24471;&#36755;&#20837;&#30340;&#22797;&#26434;&#29305;&#24449;&#65292;&#28982;&#21518;&#19982;&#30446;&#26631;&#36827;&#34892;&#25512;&#29702;&#27604;&#36739;&#65292;&#36825;&#19982;&#26126;&#30830;&#37319;&#29992;&#23545;&#25968;&#25110;&#20854;&#20182;&#25439;&#22833;&#30340;&#20998;&#31867;&#23618;&#30340;&#36755;&#20986;&#26080;&#20851;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#27809;&#26377;&#20998;&#31867;&#23618;&#30340;&#23454;&#38469;FR&#27169;&#22411;&#30340;&#26032;&#22411;&#25512;&#29702;&#25915;&#20987;&#65292;&#30001;&#20004;&#20010;&#38454;&#27573;&#32452;&#25104;&#12290;&#31532;&#19968;&#38454;&#27573;&#26159;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20013;&#38388;&#29305;&#24449;&#21644;&#25209;&#24402;&#19968;&#21270;(BN)&#21442;&#25968;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20010;&#36317;&#31163;&#26159;&#25104;&#21592;&#25512;&#29702;&#30340;&#20851;&#38190;&#25351;&#26631;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#25915;&#20987;&#27169;&#22411;&#65292;&#21487;&#20197;&#30830;&#23450;&#19968;&#20010;&#20154;&#33080;&#22270;&#20687;&#26159;&#21542;&#23646;&#20110;&#27169;&#22411;&#30340;&#25104;&#21592;&#12290;
&lt;/p&gt;
&lt;p&gt;
Face recognition (FR) has been applied to nearly every aspect of daily life, but it is always accompanied by the underlying risk of leaking private information. At present, almost all attack models against FR rely heavily on the presence of a classification layer. However, in practice, the FR model can obtain complex features of the input via the model backbone, and then compare it with the target for inference, which does not explicitly involve the outputs of the classification layer adopting logit or other losses. In this work, we advocate a novel inference attack composed of two stages for practical FR models without a classification layer. The first stage is the membership inference attack. Specifically, We analyze the distances between the intermediate features and batch normalization (BN) parameters. The results indicate that this distance is a critical metric for membership inference. We thus design a simple but effective attack model that can determine whether a face image is f
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#32508;&#21512;&#36136;&#37327;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#21307;&#30103;&#39046;&#22495;&#20013;&#21512;&#25104;&#34920;&#26684;&#25968;&#25454;&#30340;&#36136;&#37327;&#65292;&#24182;&#35299;&#20915;&#20102;&#29616;&#26377;&#26694;&#26550;&#20013;&#23384;&#22312;&#30340;&#19968;&#20123;&#38382;&#39064;&#21644;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2401.13716</link><description>&lt;p&gt;
&#25105;&#33021;&#30456;&#20449;&#25105;&#30340;&#20551;&#25968;&#25454;&#21527;--&#19968;&#31181;&#32508;&#21512;&#36136;&#37327;&#35780;&#20272;&#26694;&#26550;&#29992;&#20110;&#21307;&#30103;&#39046;&#22495;&#30340;&#21512;&#25104;&#34920;&#26684;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can I trust my fake data -- A comprehensive quality assessment framework for synthetic tabular data in healthcare. (arXiv:2401.13716v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13716
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#32508;&#21512;&#36136;&#37327;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#21307;&#30103;&#39046;&#22495;&#20013;&#21512;&#25104;&#34920;&#26684;&#25968;&#25454;&#30340;&#36136;&#37327;&#65292;&#24182;&#35299;&#20915;&#20102;&#29616;&#26377;&#26694;&#26550;&#20013;&#23384;&#22312;&#30340;&#19968;&#20123;&#38382;&#39064;&#21644;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#39046;&#22495;&#65292;&#30830;&#20445;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#30340;&#23433;&#20840;&#37319;&#29992;&#21462;&#20915;&#20110;&#33719;&#24471;&#36275;&#22815;&#30340;&#35757;&#32451;&#12289;&#27979;&#35797;&#21644;&#39564;&#35777;&#25968;&#25454;&#12290;&#20026;&#20102;&#35299;&#20915;&#38544;&#31169;&#38382;&#39064;&#21644;&#30417;&#31649;&#35201;&#27714;&#65292;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#34987;&#25552;&#20986;&#12290;&#21512;&#25104;&#25968;&#25454;&#26159;&#36890;&#36807;&#22312;&#30495;&#23454;&#25968;&#25454;&#19978;&#35757;&#32451;&#29983;&#25104;&#22120;&#26469;&#21019;&#24314;&#20855;&#26377;&#31867;&#20284;&#32479;&#35745;&#23646;&#24615;&#30340;&#25968;&#25454;&#38598;&#12290;&#19981;&#21516;&#20998;&#31867;&#26631;&#20934;&#30340;&#31454;&#20105;&#24615;&#25351;&#26631;&#24050;&#34987;&#25552;&#20986;&#65292;&#23548;&#33268;&#20102;&#19968;&#20010;&#22797;&#26434;&#30340;&#24773;&#20917;&#12290;&#20248;&#21270;&#36136;&#37327;&#38656;&#35201;&#24179;&#34913;&#20351;&#25968;&#25454;&#36866;&#29992;&#20110;&#20351;&#29992;&#30340;&#32771;&#34385;&#22240;&#32032;&#65292;&#20294;&#29616;&#26377;&#26694;&#26550;&#20013;&#30041;&#19979;&#20102;&#19968;&#20123;&#30456;&#20851;&#30340;&#32500;&#24230;&#12290;&#25105;&#20204;&#23545;&#22312;&#34920;&#26684;&#21307;&#30103;&#25968;&#25454;&#33539;&#22260;&#20869;&#35780;&#20272;&#36136;&#37327;&#25351;&#26631;&#21644;&#20351;&#29992;&#28145;&#24230;&#29983;&#25104;&#26041;&#27861;&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#25991;&#29486;&#32508;&#36848;&#12290;&#22522;&#20110;&#27492;&#21644;&#22242;&#38431;&#30340;&#38598;&#20307;&#32463;&#39564;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#27010;&#24565;&#24615;&#30340;&#36136;&#37327;&#20445;&#35777;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#30340;&#36866;&#29992;&#24615;&#19982;&#33655;&#20848;&#22269;&#23478;&#30284;&#30151;&#30331;&#35760;&#20013;&#24515;&#30340;&#23454;&#38469;&#26696;&#20363;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ensuring safe adoption of AI tools in healthcare hinges on access to sufficient data for training, testing and validation. In response to privacy concerns and regulatory requirements, using synthetic data has been suggested. Synthetic data is created by training a generator on real data to produce a dataset with similar statistical properties. Competing metrics with differing taxonomies for quality evaluation have been suggested, resulting in a complex landscape. Optimising quality entails balancing considerations that make the data fit for use, yet relevant dimensions are left out of existing frameworks. We performed a comprehensive literature review on the use of quality evaluation metrics on SD within the scope of tabular healthcare data and SD made using deep generative methods. Based on this and the collective team experiences, we developed a conceptual framework for quality assurance. The applicability was benchmarked against a practical case from the Dutch National Cancer Regist
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;QuantMCU&#30340;&#22522;&#20110;&#20215;&#20540;&#39537;&#21160;&#30340;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#24494;&#25511;&#21046;&#22120;&#19978;&#36827;&#34892;&#22522;&#20110;&#22359;&#30340;&#25512;&#29702;&#12290;&#36890;&#36807;&#21033;&#29992;&#20215;&#20540;&#39537;&#21160;&#22359;&#20998;&#31867;(VDPC)&#23558;&#22359;&#20998;&#20026;&#20004;&#31867;&#65292;&#24182;&#23545;&#21253;&#21547;&#24322;&#24120;&#20540;&#30340;&#22359;&#36827;&#34892;8&#20301;&#37327;&#21270;&#65292;QuantMCU&#33021;&#22815;&#20943;&#23569;&#20887;&#20313;&#35745;&#31639;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#25191;&#34892;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.13714</link><description>&lt;p&gt;
&#22522;&#20110;&#20215;&#20540;&#39537;&#21160;&#30340;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#22312;&#24494;&#25511;&#21046;&#22120;&#19978;&#36827;&#34892;&#22522;&#20110;&#22359;&#30340;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Value-Driven Mixed-Precision Quantization for Patch-Based Inference on Microcontrollers. (arXiv:2401.13714v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13714
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;QuantMCU&#30340;&#22522;&#20110;&#20215;&#20540;&#39537;&#21160;&#30340;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#24494;&#25511;&#21046;&#22120;&#19978;&#36827;&#34892;&#22522;&#20110;&#22359;&#30340;&#25512;&#29702;&#12290;&#36890;&#36807;&#21033;&#29992;&#20215;&#20540;&#39537;&#21160;&#22359;&#20998;&#31867;(VDPC)&#23558;&#22359;&#20998;&#20026;&#20004;&#31867;&#65292;&#24182;&#23545;&#21253;&#21547;&#24322;&#24120;&#20540;&#30340;&#22359;&#36827;&#34892;8&#20301;&#37327;&#21270;&#65292;QuantMCU&#33021;&#22815;&#20943;&#23569;&#20887;&#20313;&#35745;&#31639;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#25191;&#34892;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24494;&#25511;&#21046;&#22120;&#21333;&#20803;&#65288;MCU&#65289;&#19978;&#37096;&#32626;&#31070;&#32463;&#32593;&#32476;&#38754;&#20020;&#30528;&#35745;&#31639;&#21644;&#23384;&#20648;&#36164;&#28304;&#26377;&#38480;&#30340;&#20005;&#37325;&#25361;&#25112;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#22522;&#20110;&#22359;&#30340;&#25512;&#29702;&#20316;&#20026;&#19968;&#31181;&#22312;&#19981;&#25439;&#22833;&#27169;&#22411;&#31934;&#24230;&#30340;&#24773;&#20917;&#19979;&#33410;&#30465;&#23384;&#20648;&#22120;&#30340;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#25216;&#26415;&#23384;&#22312;&#20005;&#37325;&#30340;&#20887;&#20313;&#35745;&#31639;&#24320;&#38144;&#65292;&#23548;&#33268;&#25191;&#34892;&#24310;&#36831;&#26174;&#33879;&#22686;&#21152;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#21487;&#34892;&#35299;&#20915;&#26041;&#26696;&#26159;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#65292;&#20294;&#23427;&#38754;&#20020;&#30528;&#31934;&#24230;&#38477;&#32423;&#21644;&#32791;&#26102;&#25628;&#32034;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;QuantMCU&#65292;&#19968;&#31181;&#21033;&#29992;&#22522;&#20110;&#20215;&#20540;&#39537;&#21160;&#30340;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#26469;&#20943;&#23569;&#20887;&#20313;&#35745;&#31639;&#30340;&#26032;&#22411;&#22522;&#20110;&#22359;&#30340;&#25512;&#29702;&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#21033;&#29992;&#22522;&#20110;&#20215;&#20540;&#39537;&#21160;&#30340;&#22359;&#20998;&#31867;&#65288;VDPC&#65289;&#26469;&#20445;&#25345;&#27169;&#22411;&#30340;&#31934;&#24230;&#12290;VDPC&#23558;&#22359;&#20998;&#20026;&#20004;&#31867;&#65292;&#26681;&#25454;&#23427;&#20204;&#26159;&#21542;&#21253;&#21547;&#24322;&#24120;&#20540;&#12290;&#23545;&#20110;&#21253;&#21547;&#24322;&#24120;&#20540;&#30340;&#22359;&#65292;&#25105;&#20204;&#23545;&#29305;&#24449;&#22270;&#36827;&#34892;8&#20301;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deploying neural networks on microcontroller units (MCUs) presents substantial challenges due to their constrained computation and memory resources. Previous researches have explored patch-based inference as a strategy to conserve memory without sacrificing model accuracy. However, this technique suffers from severe redundant computation overhead, leading to a substantial increase in execution latency. A feasible solution to address this issue is mixed-precision quantization, but it faces the challenges of accuracy degradation and a time-consuming search time. In this paper, we propose QuantMCU, a novel patch-based inference method that utilizes value-driven mixed-precision quantization to reduce redundant computation. We first utilize value-driven patch classification (VDPC) to maintain the model accuracy. VDPC classifies patches into two classes based on whether they contain outlier values. For patches containing outlier values, we apply 8-bit quantization to the feature maps on the 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;EMP&#26694;&#26550;&#65292;&#36890;&#36807;&#21516;&#26102;&#21464;&#21270;&#22810;&#20010;&#23610;&#24230;&#21442;&#25968;&#26469;&#25506;&#32034;&#25968;&#25454;&#65292;&#24182;&#23558;&#25551;&#36848;&#31526;&#20989;&#25968;&#25972;&#21512;&#21040;&#20998;&#26512;&#36807;&#31243;&#20013;&#65292;&#20174;&#32780;&#25552;&#20379;&#19968;&#20010;&#39640;&#24230;&#34920;&#36798;&#30340;&#25968;&#25454;&#25688;&#35201;&#12290;</title><link>http://arxiv.org/abs/2401.13713</link><description>&lt;p&gt;
EMP: &#26377;&#25928;&#30340;&#22810;&#32500;&#25345;&#20037;&#21270;&#29992;&#20110;&#22270;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
EMP: Effective Multidimensional Persistence for Graph Representation Learning. (arXiv:2401.13713v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13713
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;EMP&#26694;&#26550;&#65292;&#36890;&#36807;&#21516;&#26102;&#21464;&#21270;&#22810;&#20010;&#23610;&#24230;&#21442;&#25968;&#26469;&#25506;&#32034;&#25968;&#25454;&#65292;&#24182;&#23558;&#25551;&#36848;&#31526;&#20989;&#25968;&#25972;&#21512;&#21040;&#20998;&#26512;&#36807;&#31243;&#20013;&#65292;&#20174;&#32780;&#25552;&#20379;&#19968;&#20010;&#39640;&#24230;&#34920;&#36798;&#30340;&#25968;&#25454;&#25688;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;(TDA)&#22312;&#20174;&#27969;&#24418;&#23398;&#20064;&#21040;&#22270;&#20998;&#31867;&#30340;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;TDA&#20013;&#30340;&#20851;&#38190;&#25216;&#26415;&#26159;&#25345;&#20037;&#21270;&#21516;&#35843;(PH)&#65292;&#36890;&#36807;&#36319;&#36394;&#28508;&#22312;&#32467;&#26500;&#38543;&#23610;&#24230;&#21442;&#25968;&#21464;&#21270;&#30340;&#28436;&#21270;&#65292;&#20026;&#25968;&#25454;&#25552;&#20379;&#29420;&#29305;&#30340;&#25299;&#25169;&#29305;&#24449;&#12290;&#29616;&#26377;&#30340;PH&#24037;&#20855;&#20165;&#33021;&#36890;&#36807;&#21333;&#20010;&#36807;&#28388;&#21442;&#25968;&#23545;&#25968;&#25454;&#36827;&#34892;&#20998;&#26512;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#24773;&#20917;&#19979;&#38656;&#35201;&#32771;&#34385;&#22810;&#20010;&#30456;&#20851;&#21442;&#25968;&#20197;&#33719;&#21462;&#26356;&#35814;&#32454;&#30340;&#25968;&#25454;&#27934;&#23519;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#26377;&#25928;&#30340;&#22810;&#32500;&#25345;&#20037;&#21270;(EMP)&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#21516;&#26102;&#25913;&#21464;&#22810;&#20010;&#23610;&#24230;&#21442;&#25968;&#26469;&#25506;&#32034;&#25968;&#25454;&#12290;&#35813;&#26694;&#26550;&#23558;&#25551;&#36848;&#31526;&#20989;&#25968;&#25972;&#21512;&#21040;&#20998;&#26512;&#36807;&#31243;&#20013;&#65292;&#24471;&#21040;&#19968;&#20010;&#39640;&#24230;&#34920;&#36798;&#30340;&#25968;&#25454;&#25688;&#35201;&#12290;&#23427;&#26080;&#32541;&#22320;&#23558;&#24314;&#31435;&#30340;&#21333;&#19968;PH&#25688;&#35201;&#38598;&#25104;&#21040;&#22810;&#32500;&#23545;&#24212;&#29289;&#20013;&#65292;&#22914;EMP&#26223;&#35266;&#12289;&#36718;&#24275;&#32447;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Topological data analysis (TDA) is gaining prominence across a wide spectrum of machine learning tasks that spans from manifold learning to graph classification. A pivotal technique within TDA is persistent homology (PH), which furnishes an exclusive topological imprint of data by tracing the evolution of latent structures as a scale parameter changes. Present PH tools are confined to analyzing data through a single filter parameter. However, many scenarios necessitate the consideration of multiple relevant parameters to attain finer insights into the data. We address this issue by introducing the Effective Multidimensional Persistence (EMP) framework. This framework empowers the exploration of data by simultaneously varying multiple scale parameters. The framework integrates descriptor functions into the analysis process, yielding a highly expressive data summary. It seamlessly integrates established single PH summaries into multidimensional counterparts like EMP Landscapes, Silhouett
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21152;&#36895;&#21452;&#26354;&#32447;t-SNE&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26497;&#22352;&#26631;&#22235;&#21449;&#26641;&#30340;&#21152;&#36895;&#32467;&#26500;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#22788;&#29702;&#22823;&#35268;&#27169;&#36755;&#20837;&#25968;&#25454;&#26102;&#30340;&#25928;&#29575;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.13708</link><description>&lt;p&gt;
&#21152;&#36895;&#21452;&#26354;&#32447;t-SNE&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Accelerating hyperbolic t-SNE. (arXiv:2401.13708v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13708
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21152;&#36895;&#21452;&#26354;&#32447;t-SNE&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26497;&#22352;&#26631;&#22235;&#21449;&#26641;&#30340;&#21152;&#36895;&#32467;&#26500;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#22788;&#29702;&#22823;&#35268;&#27169;&#36755;&#20837;&#25968;&#25454;&#26102;&#30340;&#25928;&#29575;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#65292;&#29702;&#35299;&#23618;&#27425;&#21270;&#25110;&#39640;&#32500;&#25968;&#25454;&#30340;&#32467;&#26500;&#26159;&#24517;&#35201;&#30340;&#12290;&#21452;&#26354;&#31354;&#38388;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#23884;&#20837;&#35745;&#31639;&#21644;&#20998;&#26512;&#24037;&#20855;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#38750;&#32447;&#24615;&#29305;&#24615;&#24456;&#36866;&#21512;&#22788;&#29702;&#26641;&#29366;&#25110;&#22270;&#24418;&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#20063;&#34987;&#29992;&#20110;&#39640;&#32500;&#25968;&#25454;&#30340;&#21487;&#35270;&#21270;&#65292;&#20854;&#20013;&#23427;&#20204;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#23884;&#20837;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#23884;&#20837;&#21040;&#21452;&#26354;&#31354;&#38388;&#30340;&#38477;&#32500;&#26041;&#27861;&#37117;&#19981;&#33021;&#24456;&#22909;&#22320;&#22788;&#29702;&#36755;&#20837;&#25968;&#25454;&#30340;&#35268;&#27169;&#25193;&#23637;&#38382;&#39064;&#12290;&#36825;&#26159;&#22240;&#20026;&#23884;&#20837;&#26159;&#36890;&#36807;&#36845;&#20195;&#20248;&#21270;&#26041;&#26696;&#35745;&#31639;&#30340;&#65292;&#32780;&#27599;&#27425;&#36845;&#20195;&#30340;&#35745;&#31639;&#25104;&#26412;&#19982;&#36755;&#20837;&#35268;&#27169;&#30340;&#24179;&#26041;&#25104;&#27491;&#27604;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#21452;&#26354;&#31354;&#38388;&#30340;&#38750;&#32447;&#24615;&#29305;&#24615;&#65292;&#27431;&#20960;&#37324;&#24503;&#21152;&#36895;&#32467;&#26500;&#19981;&#33021;&#30452;&#25509;&#36716;&#21270;&#20026;&#21452;&#26354;&#35774;&#32622;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#21452;&#26354;&#23884;&#20837;&#21152;&#36895;&#30340;&#32467;&#26500;&#65292;&#22522;&#20110;&#26497;&#22352;&#26631;&#22235;&#21449;&#26641;&#12290;&#25105;&#20204;&#23558;&#20854;&#24212;&#29992;&#20110;t-SNE&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#21152;&#36895;&#21452;&#26354;&#32447;t-SNE&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The need to understand the structure of hierarchical or high-dimensional data is present in a variety of fields. Hyperbolic spaces have proven to be an important tool for embedding computations and analysis tasks as their non-linear nature lends itself well to tree or graph data. Subsequently, they have also been used in the visualization of high-dimensional data, where they exhibit increased embedding performance. However, none of the existing dimensionality reduction methods for embedding into hyperbolic spaces scale well with the size of the input data. That is because the embeddings are computed via iterative optimization schemes and the computation cost of every iteration is quadratic in the size of the input. Furthermore, due to the non-linear nature of hyperbolic spaces, Euclidean acceleration structures cannot directly be translated to the hyperbolic setting. This paper introduces the first acceleration structure for hyperbolic embeddings, building upon a polar quadtree. We com
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;&#22312;&#29289;&#32852;&#32593;&#20581;&#24247;&#25252;&#29702;&#20013;&#21033;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#20154;&#31867;&#25968;&#23383;&#23402;&#29983;&#30340;&#24212;&#29992;&#12290;&#20154;&#31867;&#25968;&#23383;&#23402;&#29983;&#20316;&#20026;&#22810;&#21151;&#33021;&#12289;&#29983;&#21160;&#30340;&#20154;&#31867;&#25968;&#23383;&#27979;&#35797;&#24179;&#21488;&#65292;&#21487;&#20197;&#27169;&#25311;&#32467;&#26524;&#24182;&#25351;&#23548;&#23454;&#38469;&#27835;&#30103;&#65292;&#20174;&#32780;&#25552;&#39640;&#29289;&#32852;&#32593;&#20581;&#24247;&#25252;&#29702;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.13699</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#29289;&#32852;&#32593;&#20581;&#24247;&#25252;&#29702;&#20013;&#30340;&#20154;&#31867;&#25968;&#23383;&#23402;&#29983;&#65306;&#19968;&#39033;&#32508;&#21512;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Generative AI-Driven Human Digital Twin in IoT-Healthcare: A Comprehensive Survey. (arXiv:2401.13699v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13699
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;&#22312;&#29289;&#32852;&#32593;&#20581;&#24247;&#25252;&#29702;&#20013;&#21033;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#20154;&#31867;&#25968;&#23383;&#23402;&#29983;&#30340;&#24212;&#29992;&#12290;&#20154;&#31867;&#25968;&#23383;&#23402;&#29983;&#20316;&#20026;&#22810;&#21151;&#33021;&#12289;&#29983;&#21160;&#30340;&#20154;&#31867;&#25968;&#23383;&#27979;&#35797;&#24179;&#21488;&#65292;&#21487;&#20197;&#27169;&#25311;&#32467;&#26524;&#24182;&#25351;&#23548;&#23454;&#38469;&#27835;&#30103;&#65292;&#20174;&#32780;&#25552;&#39640;&#29289;&#32852;&#32593;&#20581;&#24247;&#25252;&#29702;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#32852;&#32593;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20154;&#31867;&#29983;&#27963;&#30340;&#36136;&#37327;&#65292;&#29305;&#21035;&#26159;&#22312;&#20581;&#24247;&#25252;&#29702;&#26041;&#38754;&#21560;&#24341;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#12290;&#21516;&#26102;&#65292;&#20154;&#31867;&#25968;&#23383;&#23402;&#29983;&#34987;&#25552;&#20986;&#20316;&#20026;&#19968;&#31181;&#21019;&#26032;&#30340;&#33539;&#24335;&#65292;&#21487;&#20197;&#20840;&#38754;&#22320;&#25551;&#36848;&#20010;&#20307;&#20154;&#20307;&#22312;&#25968;&#23383;&#19990;&#30028;&#20013;&#30340;&#22797;&#21046;&#65292;&#24182;&#23454;&#26102;&#21453;&#26144;&#20854;&#29289;&#29702;&#29366;&#20917;&#12290;&#33258;&#28982;&#22320;&#65292;&#20154;&#31867;&#25968;&#23383;&#23402;&#29983;&#34987;&#35774;&#24819;&#20026;&#36890;&#36807;&#20805;&#24403;&#22810;&#21151;&#33021;&#12289;&#29983;&#21160;&#30340;&#20154;&#31867;&#25968;&#23383;&#27979;&#35797;&#24179;&#21488;&#26469;&#22686;&#24378;&#29289;&#32852;&#32593;&#20581;&#24247;&#25252;&#29702;&#30340;&#33021;&#21147;&#65292;&#27169;&#25311;&#32467;&#26524;&#24182;&#25351;&#23548;&#23454;&#38469;&#27835;&#30103;&#12290;&#28982;&#32780;&#65292;&#25104;&#21151;&#24314;&#31435;&#20154;&#31867;&#25968;&#23383;&#23402;&#29983;&#38656;&#35201;&#39640;&#20445;&#30495;&#24230;&#30340;&#34394;&#25311;&#24314;&#27169;&#21644;&#24378;&#22823;&#30340;&#20449;&#24687;&#20132;&#20114;&#65292;&#20294;&#21487;&#33021;&#23384;&#22312;&#31232;&#32570;&#12289;&#20559;&#20506;&#21644;&#22122;&#22768;&#25968;&#25454;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#26368;&#36817;&#27969;&#34892;&#30340;&#19968;&#31181;&#21517;&#20026;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GAI&#65289;&#30340;&#25216;&#26415;&#21487;&#33021;&#26159;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#21033;&#29992;&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#33258;&#21160;&#29983;&#25104;&#12289;&#25805;&#20316;&#21644;&#20462;&#25913;&#28176;&#21464;&#35270;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Internet of things (IoT) can significantly enhance the quality of human life, specifically in healthcare, attracting extensive attentions to IoT-healthcare services. Meanwhile, the human digital twin (HDT) is proposed as an innovative paradigm that can comprehensively characterize the replication of the individual human body in the digital world and reflect its physical status in real time. Naturally, HDT is envisioned to empower IoT-healthcare beyond the application of healthcare monitoring by acting as a versatile and vivid human digital testbed, simulating the outcomes and guiding the practical treatments. However, successfully establishing HDT requires high-fidelity virtual modeling and strong information interactions but possibly with scarce, biased and noisy data. Fortunately, a recent popular technology called generative artificial intelligence (GAI) may be a promising solution because it can leverage advanced AI algorithms to automatically create, manipulate, and modify val
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#21487;&#24494;&#20998;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#25311;&#22120;&#36827;&#34892;&#21453;&#28436;&#20998;&#26512;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#27169;&#25311;&#22120;&#35745;&#31639;&#24320;&#38144;&#22823;&#12289;&#19981;&#21487;&#24494;&#31561;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.13695</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#24494;&#20998;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#25311;&#22120;&#36827;&#34892;&#39063;&#31890;&#27969;&#30340;&#21453;&#28436;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Inverse analysis of granular flows using differentiable graph neural network simulator. (arXiv:2401.13695v1 [physics.geo-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13695
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#21487;&#24494;&#20998;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#25311;&#22120;&#36827;&#34892;&#21453;&#28436;&#20998;&#26512;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#27169;&#25311;&#22120;&#35745;&#31639;&#24320;&#38144;&#22823;&#12289;&#19981;&#21487;&#24494;&#31561;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39063;&#31890;&#27969;&#20013;&#30340;&#21453;&#28436;&#38382;&#39064;&#65292;&#22914;&#23665;&#20307;&#28369;&#22369;&#21644;&#30862;&#23633;&#27969;&#65292;&#28041;&#21450;&#22522;&#20110;&#30446;&#26631;&#27874;&#21160;&#21078;&#38754;&#20272;&#35745;&#26448;&#26009;&#21442;&#25968;&#25110;&#36793;&#30028;&#26465;&#20214;&#12290;&#20256;&#32479;&#30340;&#39640;&#20445;&#30495;&#24230;&#27169;&#25311;&#22120;&#23545;&#36825;&#20123;&#21453;&#28436;&#38382;&#39064;&#26159;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#65292;&#38480;&#21046;&#20102;&#21487;&#33021;&#30340;&#27169;&#25311;&#27425;&#25968;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#30340;&#19981;&#21487;&#24494;&#24615;&#20351;&#24471;&#26799;&#24230;&#20248;&#21270;&#26041;&#27861;&#26080;&#27861;&#24212;&#29992;&#65292;&#32780;&#36825;&#20123;&#26041;&#27861;&#22312;&#39640;&#32500;&#38382;&#39064;&#20013;&#20197;&#20854;&#25928;&#29575;&#32780;&#38395;&#21517;&#12290;&#34429;&#28982;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20195;&#29702;&#27169;&#22411;&#25552;&#20379;&#20102;&#35745;&#31639;&#25928;&#29575;&#21644;&#21487;&#24494;&#24615;&#65292;&#20294;&#30001;&#20110;&#20854;&#20381;&#36182;&#20110;&#20302;&#32500;&#36755;&#20837;-&#36755;&#20986;&#26144;&#23556;&#65292;&#26080;&#27861;&#25429;&#25417;&#21040;&#39063;&#31890;&#27969;&#30340;&#23436;&#25972;&#29289;&#29702;&#36807;&#31243;&#65292;&#22240;&#27492;&#24448;&#24448;&#38590;&#20197;&#25512;&#24191;&#21040;&#35757;&#32451;&#25968;&#25454;&#20043;&#22806;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#24494;&#20998;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#25311;&#22120;(GNS)&#65292;&#36890;&#36807;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21453;&#21521;&#27169;&#24335;&#33258;&#21160;&#24494;&#20998;&#19982;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#35299;&#20915;&#21453;&#28436;&#38382;&#39064;&#12290;GNS&#23398;&#20064;&#20102;&#39063;&#31890;&#27969;&#30340;&#21160;&#21147;&#23398;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inverse problems in granular flows, such as landslides and debris flows, involve estimating material parameters or boundary conditions based on target runout profile. Traditional high-fidelity simulators for these inverse problems are computationally demanding, restricting the number of simulations possible. Additionally, their non-differentiable nature makes gradient-based optimization methods, known for their efficiency in high-dimensional problems, inapplicable. While machine learning-based surrogate models offer computational efficiency and differentiability, they often struggle to generalize beyond their training data due to their reliance on low-dimensional input-output mappings that fail to capture the complete physics of granular flows. We propose a novel differentiable graph neural network simulator (GNS) by combining reverse mode automatic differentiation of graph neural networks with gradient-based optimization for solving inverse problems. GNS learns the dynamics of granula
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#23558;&#36807;&#31243;&#25366;&#25496;&#24212;&#29992;&#20110;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#21021;&#27493;&#35299;&#20915;&#26041;&#26696;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2401.13677</link><description>&lt;p&gt;
&#29992;&#20110;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#36807;&#31243;&#25366;&#25496;&#65306;&#25361;&#25112;&#19982;&#30740;&#31350;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Process Mining for Unstructured Data: Challenges and Research Directions. (arXiv:2401.13677v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13677
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#23558;&#36807;&#31243;&#25366;&#25496;&#24212;&#29992;&#20110;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#21021;&#27493;&#35299;&#20915;&#26041;&#26696;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#36807;&#31243;&#25366;&#25496;&#24212;&#29992;&#20110;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#21487;&#33021;&#26174;&#33879;&#25552;&#21319;&#23545;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#24120;&#35265;&#25968;&#25454;&#26684;&#24335;&#39046;&#22495;&#30340;&#26032;&#35265;&#35299;&#12290;&#35201;&#26377;&#25928;&#20998;&#26512;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#65292;&#24182;&#23545;&#20998;&#26512;&#32467;&#26524;&#20256;&#36798;&#20449;&#24515;&#65292;&#38656;&#35201;&#20811;&#26381;&#22810;&#20010;&#25361;&#25112;&#12290;&#26412;&#25991;&#26088;&#22312;&#35752;&#35770;&#36825;&#20123;&#25361;&#25112;&#65292;&#25552;&#20986;&#21021;&#27493;&#35299;&#20915;&#26041;&#26696;&#24182;&#25551;&#36848;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#31687;&#25991;&#31456;&#20026;&#26410;&#26469;&#22312;&#36825;&#20010;&#20027;&#39064;&#19978;&#30340;&#21512;&#20316;&#22880;&#23450;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
The application of process mining for unstructured data might significantly elevate novel insights into disciplines where unstructured data is a common data format. To efficiently analyze unstructured data by process mining and to convey confidence into the analysis result, requires bridging multiple challenges. The purpose of this paper is to discuss these challenges, present initial solutions and describe future research directions. We hope that this article lays the foundations for future collaboration on this topic.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;&#35782;&#21035;&#20102;&#39532;&#36798;&#21152;&#26031;&#21152;&#21487;&#20877;&#29983;&#33021;&#28304;&#28040;&#36153;&#30340;&#24433;&#21709;&#22240;&#32032;&#65292;&#21253;&#25324;&#32463;&#27982;&#12289;&#37329;&#34701;&#12289;&#31038;&#20250;&#21644;&#29615;&#22659;&#26041;&#38754;&#30340;&#22240;&#32032;&#65292;&#24182;&#36890;&#36807;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2401.13671</link><description>&lt;p&gt;
&#39532;&#36798;&#21152;&#26031;&#21152;&#21487;&#20877;&#29983;&#33021;&#28304;&#28040;&#36153;&#30340;&#20915;&#23450;&#22240;&#32032;&#65306;&#22522;&#20110;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Determinants of renewable energy consumption in Madagascar: Evidence from feature selection algorithms. (arXiv:2401.13671v1 [econ.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13671
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;&#35782;&#21035;&#20102;&#39532;&#36798;&#21152;&#26031;&#21152;&#21487;&#20877;&#29983;&#33021;&#28304;&#28040;&#36153;&#30340;&#24433;&#21709;&#22240;&#32032;&#65292;&#21253;&#25324;&#32463;&#27982;&#12289;&#37329;&#34701;&#12289;&#31038;&#20250;&#21644;&#29615;&#22659;&#26041;&#38754;&#30340;&#22240;&#32032;&#65292;&#24182;&#36890;&#36807;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30340;&#30446;&#30340;&#26159;&#35782;&#21035;&#39532;&#36798;&#21152;&#26031;&#21152;&#21487;&#20877;&#29983;&#33021;&#28304;&#28040;&#36153;&#30340;&#24433;&#21709;&#22240;&#32032;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;&#23439;&#35266;&#32463;&#27982;&#12289;&#37329;&#34701;&#12289;&#31038;&#20250;&#21644;&#29615;&#22659;&#31561;&#26041;&#38754;&#30340;12&#20010;&#29305;&#24449;&#65292;&#21253;&#25324;&#32463;&#27982;&#22686;&#38271;&#12289;&#22269;&#20869;&#25237;&#36164;&#12289;&#22806;&#22269;&#30452;&#25509;&#25237;&#36164;&#12289;&#37329;&#34701;&#21457;&#23637;&#12289;&#24037;&#19994;&#21457;&#23637;&#12289;&#36890;&#36135;&#33192;&#32960;&#12289;&#25910;&#20837;&#20998;&#37197;&#12289;&#36152;&#26131;&#24320;&#25918;&#24230;&#12289;&#27719;&#29575;&#12289;&#26053;&#28216;&#19994;&#21457;&#23637;&#12289;&#29615;&#22659;&#36136;&#37327;&#21644;&#22478;&#24066;&#21270;&#12290;&#20026;&#20102;&#35780;&#20272;&#23427;&#20204;&#30340;&#37325;&#35201;&#24615;&#65292;&#25105;&#20204;&#20551;&#23450;&#20877;&#29983;&#33021;&#28304;&#28040;&#36153;&#19982;&#36825;&#20123;&#29305;&#24449;&#20043;&#38388;&#23384;&#22312;&#32447;&#24615;&#20851;&#31995;&#65292;&#26102;&#38388;&#36328;&#24230;&#20026;1990&#24180;&#33267;2021&#24180;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;&#65292;&#21253;&#25324;&#22522;&#20110;&#36807;&#28388;&#22120;&#30340;&#65288;&#32447;&#24615;&#22238;&#24402;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#12289;&#30456;&#20851;&#26041;&#27861;&#65289;&#65292;&#23884;&#20837;&#24335;&#30340;&#65288;LASSO&#65289;&#65292;&#20197;&#21450;&#21253;&#35013;&#24335;&#30340;&#65288;&#26368;&#20339;&#23376;&#38598;&#22238;&#24402;&#12289;&#36880;&#27493;&#22238;&#24402;&#12289;&#36882;&#24402;&#29305;&#24449;&#28040;&#38500;&#12289;&#36845;&#20195;&#39044;&#27979;&#23376;&#21152;&#26435;&#20559;&#26368;&#23567;&#20108;&#20056;&#27861;&#12289;Boruta&#12289;&#27169;&#25311;&#36864;&#28779;&#21644;&#36951;&#20256;&#31639;&#27861;&#65289;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;...
&lt;/p&gt;
&lt;p&gt;
The aim of this note is to identify the factors influencing renewable energy consumption in Madagascar. We tested 12 features covering macroeconomic, financial, social, and environmental aspects, including economic growth, domestic investment, foreign direct investment, financial development, industrial development, inflation, income distribution, trade openness, exchange rate, tourism development, environmental quality, and urbanization. To assess their significance, we assumed a linear relationship between renewable energy consumption and these features over the 1990-2021 period. Next, we applied different machine learning feature selection algorithms classified as filter-based (relative importance for linear regression, correlation method), embedded (LASSO), and wrapper-based (best subset regression, stepwise regression, recursive feature elimination, iterative predictor weighting partial least squares, Boruta, simulated annealing, and genetic algorithms) methods. Our analysis revea
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#38543;&#26426;&#31070;&#32463;&#32593;&#32476;&#22312;&#20020;&#24202;&#24212;&#29992;&#20013;&#30340;&#21487;&#38752;&#24615;&#65292;&#24182;&#21457;&#29616;&#24120;&#35265;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#25968;&#25454;&#36716;&#31227;&#24773;&#20917;&#19979;&#36807;&#20110;&#33258;&#20449;&#12290;&#36825;&#24378;&#35843;&#20102;&#23545;&#26412;&#22320;&#19981;&#30830;&#23450;&#24615;&#21487;&#38752;&#20272;&#35745;&#21450;&#20854;&#21521;&#26368;&#32456;&#29992;&#25143;&#20256;&#36798;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.13657</link><description>&lt;p&gt;
&#24120;&#35265;&#30340;&#38543;&#26426;&#31070;&#32463;&#32593;&#32476;&#22312;&#21487;&#38752;&#24615;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#26041;&#38754;&#30340;&#19981;&#36275;
&lt;/p&gt;
&lt;p&gt;
Inadequacy of common stochastic neural networks for reliable clinical decision support. (arXiv:2401.13657v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13657
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#38543;&#26426;&#31070;&#32463;&#32593;&#32476;&#22312;&#20020;&#24202;&#24212;&#29992;&#20013;&#30340;&#21487;&#38752;&#24615;&#65292;&#24182;&#21457;&#29616;&#24120;&#35265;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#25968;&#25454;&#36716;&#31227;&#24773;&#20917;&#19979;&#36807;&#20110;&#33258;&#20449;&#12290;&#36825;&#24378;&#35843;&#20102;&#23545;&#26412;&#22320;&#19981;&#30830;&#23450;&#24615;&#21487;&#38752;&#20272;&#35745;&#21450;&#20854;&#21521;&#26368;&#32456;&#29992;&#25143;&#20256;&#36798;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20262;&#29702;&#21644;&#23433;&#20840;&#30456;&#20851;&#30340;&#20851;&#20999;&#65292;&#20154;&#24037;&#26234;&#33021;&#30340;&#24191;&#27867;&#24212;&#29992;&#20110;&#21307;&#23398;&#20915;&#31574;&#20173;&#28982;&#21463;&#38459;&#12290;&#23545;&#20110;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#21307;&#30103;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#26469;&#35828;&#65292;&#21487;&#38752;&#24615;&#21644;&#21487;&#20449;&#24230;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#24120;&#35265;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#25968;&#25454;&#36716;&#31227;&#19979;&#24448;&#24448;&#36807;&#20110;&#33258;&#20449;&#12290;&#36825;&#31181;&#22312;&#22522;&#20110;&#35777;&#25454;&#30340;&#22330;&#26223;&#20043;&#22806;&#19981;&#24688;&#24403;&#30340;&#25512;&#29702;&#21487;&#33021;&#20250;&#20135;&#29983;&#20005;&#37325;&#21518;&#26524;&#12290;&#36825;&#20984;&#26174;&#20102;&#23545;&#26412;&#22320;&#19981;&#30830;&#23450;&#24615;&#21487;&#38752;&#20272;&#35745;&#21450;&#20854;&#21521;&#26368;&#32456;&#29992;&#25143;&#20256;&#36798;&#30340;&#37325;&#35201;&#24615;&#12290;&#23613;&#31649;&#38543;&#26426;&#31070;&#32463;&#32593;&#32476;&#34987;&#35465;&#20026;&#36825;&#20123;&#38382;&#39064;&#30340;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#20854;&#22312;&#20020;&#24202;&#24212;&#29992;&#20013;&#30340;&#23454;&#38469;&#21487;&#38752;&#24615;&#12290;&#25105;&#20204;&#20197;&#20174;MIMIC3&#30740;&#31350;&#20013;&#20351;&#29992;&#30340;EHR&#30340;ICU&#20303;&#38498;&#30149;&#27515;&#29575;&#39044;&#27979;&#20026;&#20363;&#26469;&#36827;&#34892;&#20998;&#26512;&#12290;&#23545;&#20110;EHR&#26102;&#38388;&#24207;&#21015;&#30340;&#39044;&#27979;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#20165;&#32534;&#30721;&#22120;&#30340;Transformer&#27169;&#22411;&#12290;&#36890;&#36807;&#32435;&#20837;&#24120;&#35265;&#26041;&#27861;&#26469;&#23454;&#29616;&#27169;&#22411;&#20989;&#25968;&#30340;&#38543;&#26426;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Widespread adoption of AI for medical decision making is still hindered due to ethical and safety-related concerns. For AI-based decision support systems in healthcare settings it is paramount to be reliable and trustworthy. Common deep learning approaches, however, have the tendency towards overconfidence under data shift. Such inappropriate extrapolation beyond evidence-based scenarios may have dire consequences. This highlights the importance of reliable estimation of local uncertainty and its communication to the end user. While stochastic neural networks have been heralded as a potential solution to these issues, this study investigates their actual reliability in clinical applications. We centered our analysis on the exemplary use case of mortality prediction for ICU hospitalizations using EHR from MIMIC3 study. For predictions on the EHR time series, Encoder-Only Transformer models were employed. Stochasticity of model functions was achieved by incorporating common methods such 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22270;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#21644;&#31232;&#30095;&#32593;&#26684;&#26469;&#26816;&#27979;&#19981;&#36830;&#32493;&#20989;&#25968;&#19981;&#36830;&#32493;&#30028;&#38754;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#32500;&#24230;&#22823;&#20110;3&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#39640;&#25928;&#19988;&#20934;&#30830;&#30340;&#19981;&#36830;&#32493;&#24615;&#26816;&#27979;&#33021;&#21147;&#65292;&#22312;&#32500;&#24230;n = 2&#21644;n = 4&#30340;&#20989;&#25968;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#39640;&#25928;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#20855;&#26377;&#21487;&#31227;&#26893;&#24615;&#21644;&#22810;&#21151;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.13652</link><description>&lt;p&gt;
&#22522;&#20110;&#31232;&#30095;&#32593;&#26684;&#30340;&#19981;&#36830;&#32493;&#24615;&#26816;&#27979;&#30340;&#22270;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Graph-Informed Neural Networks for Sparse Grid-Based Discontinuity Detectors. (arXiv:2401.13652v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13652
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22270;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#21644;&#31232;&#30095;&#32593;&#26684;&#26469;&#26816;&#27979;&#19981;&#36830;&#32493;&#20989;&#25968;&#19981;&#36830;&#32493;&#30028;&#38754;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#32500;&#24230;&#22823;&#20110;3&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#39640;&#25928;&#19988;&#20934;&#30830;&#30340;&#19981;&#36830;&#32493;&#24615;&#26816;&#27979;&#33021;&#21147;&#65292;&#22312;&#32500;&#24230;n = 2&#21644;n = 4&#30340;&#20989;&#25968;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#39640;&#25928;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#20855;&#26377;&#21487;&#31227;&#26893;&#24615;&#21644;&#22810;&#21151;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#19981;&#36830;&#32493;&#20989;&#25968;&#30340;&#19981;&#36830;&#32493;&#30028;&#38754;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#22522;&#20110;&#22270;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;GINNs&#65289;&#21644;&#31232;&#30095;&#32593;&#26684;&#26469;&#35299;&#20915;&#32500;&#24230;&#22823;&#20110;3&#30340;&#24773;&#20917;&#19979;&#30340;&#19981;&#36830;&#32493;&#24615;&#26816;&#27979;&#12290;&#35757;&#32451;&#36807;&#30340;GINNs&#22312;&#31232;&#30095;&#32593;&#26684;&#19978;&#35782;&#21035;&#26377;&#38382;&#39064;&#30340;&#28857;&#65292;&#24182;&#21033;&#29992;&#26500;&#24314;&#22312;&#32593;&#26684;&#19978;&#30340;&#22270;&#32467;&#26500;&#23454;&#29616;&#39640;&#25928;&#20934;&#30830;&#30340;&#19981;&#36830;&#32493;&#24615;&#26816;&#27979;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#36882;&#24402;&#31639;&#27861;&#29992;&#20110;&#19968;&#33324;&#30340;&#22522;&#20110;&#31232;&#30095;&#32593;&#26684;&#30340;&#26816;&#27979;&#22120;&#65292;&#20855;&#26377;&#25910;&#25947;&#24615;&#21644;&#26131;&#20110;&#24212;&#29992;&#24615;&#12290;&#22312;&#32500;&#24230;n=2&#21644;n=4&#30340;&#20989;&#25968;&#19978;&#36827;&#34892;&#30340;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;GINNs&#22312;&#26816;&#27979;&#19981;&#36830;&#32493;&#30028;&#38754;&#26041;&#38754;&#30340;&#39640;&#25928;&#24615;&#21644;&#40065;&#26834;&#27867;&#21270;&#33021;&#21147;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#32463;&#36807;&#35757;&#32451;&#30340;GINNs&#20855;&#26377;&#21487;&#31227;&#26893;&#24615;&#21644;&#22810;&#21151;&#33021;&#24615;&#65292;&#21487;&#20197;&#38598;&#25104;&#21040;&#21508;&#31181;&#31639;&#27861;&#20013;&#24182;&#20849;&#20139;&#32473;&#29992;&#25143;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a novel approach for detecting the discontinuity interfaces of a discontinuous function. This approach leverages Graph-Informed Neural Networks (GINNs) and sparse grids to address discontinuity detection also in domains of dimension larger than 3. GINNs, trained to identify troubled points on sparse grids, exploit graph structures built on the grids to achieve efficient and accurate discontinuity detection performances. We also introduce a recursive algorithm for general sparse grid-based detectors, characterized by convergence properties and easy applicability. Numerical experiments on functions with dimensions n = 2 and n = 4 demonstrate the efficiency and robust generalization of GINNs in detecting discontinuity interfaces. Notably, the trained GINNs offer portability and versatility, allowing integration into various algorithms and sharing among users.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#36974;&#34109;&#31890;&#23376;&#24314;&#27169;&#65288;MPM&#65289;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#39640;&#33021;&#29289;&#29702;&#31185;&#23398;&#25968;&#25454;&#20013;&#26080;&#24207;&#36755;&#20837;&#30340;&#36890;&#29992;&#34920;&#31034;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#39044;&#35757;&#32451;&#23398;&#20064;&#32622;&#25442;&#19981;&#21464;&#30340;&#20989;&#25968;&#65292;&#22312;&#26500;&#24314;&#36866;&#29992;&#20110;&#22810;&#31181;&#20219;&#21153;&#30340;&#39640;&#33021;&#29289;&#29702;&#22522;&#30784;&#27169;&#22411;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.13537</link><description>&lt;p&gt;
&#22522;&#20110;&#38598;&#21512;&#30340;&#36974;&#34109;&#31890;&#23376;&#24314;&#27169;&#65306;&#36208;&#21521;&#33258;&#30417;&#30563;&#39640;&#33021;&#29289;&#29702;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Masked Particle Modeling on Sets: Towards Self-Supervised High Energy Physics Foundation Models. (arXiv:2401.13537v1 [hep-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13537
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#36974;&#34109;&#31890;&#23376;&#24314;&#27169;&#65288;MPM&#65289;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#39640;&#33021;&#29289;&#29702;&#31185;&#23398;&#25968;&#25454;&#20013;&#26080;&#24207;&#36755;&#20837;&#30340;&#36890;&#29992;&#34920;&#31034;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#39044;&#35757;&#32451;&#23398;&#20064;&#32622;&#25442;&#19981;&#21464;&#30340;&#20989;&#25968;&#65292;&#22312;&#26500;&#24314;&#36866;&#29992;&#20110;&#22810;&#31181;&#20219;&#21153;&#30340;&#39640;&#33021;&#29289;&#29702;&#22522;&#30784;&#27169;&#22411;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;"&#36974;&#34109;&#31890;&#23376;&#24314;&#27169;"&#65288;MPM&#65289;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#39640;&#33021;&#29289;&#29702;&#65288;HEP&#65289;&#31185;&#23398;&#25968;&#25454;&#20013;&#26080;&#24207;&#36755;&#20837;&#30340;&#36890;&#29992;&#12289;&#21487;&#36716;&#31227;&#21644;&#21487;&#37325;&#29992;&#34920;&#31034;&#12290;&#36825;&#39033;&#24037;&#20316;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#26696;&#65292;&#36890;&#36807;&#22522;&#20110;&#36974;&#34109;&#24314;&#27169;&#30340;&#39044;&#35757;&#32451;&#26469;&#23398;&#20064;&#38598;&#21512;&#19978;&#30340;&#32622;&#25442;&#19981;&#21464;&#20989;&#25968;&#12290;&#26356;&#19968;&#33324;&#22320;&#65292;&#36825;&#39033;&#24037;&#20316;&#22312;&#26500;&#24314;&#21487;&#20197;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#36827;&#34892;&#36890;&#29992;&#39044;&#35757;&#32451;&#24182;&#31245;&#21518;&#31934;&#35843;&#29992;&#20110;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#30340;HEP&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#26041;&#38754;&#36808;&#20986;&#20102;&#19968;&#27493;&#12290;&#22312;MPM&#20013;&#65292;&#38598;&#21512;&#20013;&#30340;&#31890;&#23376;&#34987;&#36974;&#34109;&#65292;&#35757;&#32451;&#30340;&#30446;&#26631;&#26159;&#24674;&#22797;&#23427;&#20204;&#30340;&#36523;&#20221;&#65292;&#36523;&#20221;&#30001;&#39044;&#35757;&#32451;&#30340;&#21521;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#31163;&#25955;&#21270;&#26631;&#35760;&#34920;&#31034;&#23450;&#20041;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#35813;&#26041;&#27861;&#22312;&#23545;&#25758;&#26426;&#29289;&#29702;&#23454;&#39564;&#20013;&#39640;&#33021;&#21943;&#27880;&#26679;&#26412;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#31163;&#25955;&#21270;&#12289;&#32622;&#25442;&#19981;&#21464;&#24615;&#21644;&#25490;&#24207;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose \textit{masked particle modeling} (MPM) as a self-supervised method for learning generic, transferable, and reusable representations on unordered sets of inputs for use in high energy physics (HEP) scientific data. This work provides a novel scheme to perform masked modeling based pre-training to learn permutation invariant functions on sets. More generally, this work provides a step towards building large foundation models for HEP that can be generically pre-trained with self-supervised learning and later fine-tuned for a variety of down-stream tasks. In MPM, particles in a set are masked and the training objective is to recover their identity, as defined by a discretized token representation of a pre-trained vector quantized variational autoencoder. We study the efficacy of the method in samples of high energy jets at collider physics experiments, including studies on the impact of discretization, permutation invariance, and ordering. We also study the fine-tuning capabili
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20013;&#23637;&#31034;&#20102;&#22312;&#39640;&#33021;&#29289;&#29702;&#23398;&#20013;&#65292;&#36890;&#36807;&#36229;&#36234;&#39034;&#24207;&#20248;&#21270;&#25110;&#37325;&#24314;&#21644;&#20998;&#26512;&#32452;&#20214;&#30340;&#26631;&#20934;&#33539; paradigm&#65292;&#21487;&#20197;&#23454;&#29616;&#24615;&#33021;&#21644;&#25968;&#25454;&#25928;&#29575;&#30340;&#26174;&#33879;&#25552;&#21319;&#12290;&#36890;&#36807;&#25628;&#32034;&#36890;&#36807;&#20013;&#38388; di-Higgs &#31995;&#32479;&#34928;&#21464;&#30340;&#37325;&#20849;&#25391;&#20307;&#20026;&#22235;&#20010; $b$-&#21943;&#27880;&#30340;&#31034;&#20363;&#29992;&#20363;&#65292;&#25105;&#20204;&#23558;&#39640;&#33021;&#29289;&#29702;&#23398;&#37325;&#24314;&#21644;&#20998;&#26512;&#19982;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#24037;&#20316;&#27969;&#36827;&#34892;&#20102;&#36830;&#25509;&#65292;&#24182;&#37327;&#21270;&#20102;&#20854;&#25910;&#30410;&#12290;</title><link>http://arxiv.org/abs/2401.13536</link><description>&lt;p&gt;
&#20026;&#32852;&#21512;&#20998;&#26512;&#20248;&#21270;&#35843;&#25972;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Finetuning Foundation Models for Joint Analysis Optimization. (arXiv:2401.13536v1 [hep-ex])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13536
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20013;&#23637;&#31034;&#20102;&#22312;&#39640;&#33021;&#29289;&#29702;&#23398;&#20013;&#65292;&#36890;&#36807;&#36229;&#36234;&#39034;&#24207;&#20248;&#21270;&#25110;&#37325;&#24314;&#21644;&#20998;&#26512;&#32452;&#20214;&#30340;&#26631;&#20934;&#33539; paradigm&#65292;&#21487;&#20197;&#23454;&#29616;&#24615;&#33021;&#21644;&#25968;&#25454;&#25928;&#29575;&#30340;&#26174;&#33879;&#25552;&#21319;&#12290;&#36890;&#36807;&#25628;&#32034;&#36890;&#36807;&#20013;&#38388; di-Higgs &#31995;&#32479;&#34928;&#21464;&#30340;&#37325;&#20849;&#25391;&#20307;&#20026;&#22235;&#20010; $b$-&#21943;&#27880;&#30340;&#31034;&#20363;&#29992;&#20363;&#65292;&#25105;&#20204;&#23558;&#39640;&#33021;&#29289;&#29702;&#23398;&#37325;&#24314;&#21644;&#20998;&#26512;&#19982;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#24037;&#20316;&#27969;&#36827;&#34892;&#20102;&#36830;&#25509;&#65292;&#24182;&#37327;&#21270;&#20102;&#20854;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#39640;&#33021;&#29289;&#29702;&#23398;&#20013;&#21487;&#20197;&#36890;&#36807;&#36229;&#36234;&#39034;&#24207;&#20248;&#21270;&#25110;&#37325;&#24314;&#21644;&#20998;&#26512;&#32452;&#20214;&#30340;&#26631;&#20934;&#33539; paradigm&#65292;&#23454;&#29616;&#24615;&#33021;&#21644;&#25968;&#25454;&#25928;&#29575;&#30340;&#26174;&#33879;&#25552;&#21319;&#12290;&#25105;&#20204;&#23558;&#39640;&#33021;&#29289;&#29702;&#23398;&#37325;&#24314;&#21644;&#20998;&#26512;&#19982;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#24037;&#20316;&#27969;&#65288;&#22914;&#39044;&#35757;&#32451;&#12289;&#24494;&#35843;&#12289;&#39046;&#22495;&#36866;&#24212;&#21644;&#39640;&#32500;&#23884;&#20837;&#31354;&#38388;&#65289;&#36827;&#34892;&#20102;&#27010;&#24565;&#19978;&#30340;&#36830;&#25509;&#65292;&#24182;&#37327;&#21270;&#20102;&#36890;&#36807;&#25628;&#32034;&#36890;&#36807;&#20013;&#38388; di-Higgs &#31995;&#32479;&#34928;&#21464;&#30340;&#37325;&#20849;&#25391;&#20307;&#20026;&#22235;&#20010; $b$-&#21943;&#27880;&#30340;&#31034;&#20363;&#29992;&#20363;&#20013;&#30340;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work we demonstrate that significant gains in performance and data efficiency can be achieved in High Energy Physics (HEP) by moving beyond the standard paradigm of sequential optimization or reconstruction and analysis components. We conceptually connect HEP reconstruction and analysis to modern machine learning workflows such as pretraining, finetuning, domain adaptation and high-dimensional embedding spaces and quantify the gains in the example usecase of searches of heavy resonances decaying via an intermediate di-Higgs system to four $b$-jets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;Wasserstein&#27010;&#29575;&#31354;&#38388;&#19978;&#30340;Riemannian SGD&#21644;SVRG&#27969;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#26469;&#20016;&#23500;Wasserstein&#31354;&#38388;&#20013;&#30340;&#36830;&#32493;&#20248;&#21270;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.13530</link><description>&lt;p&gt;
&#22312;Wasserstein&#27010;&#29575;&#31354;&#38388;&#19978;&#29702;&#35299;Riemannian SGD&#21644;SVRG&#27969;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Understanding the Riemannian SGD and SVRG Flows on Wasserstein Probabilistic Space. (arXiv:2401.13530v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13530
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;Wasserstein&#27010;&#29575;&#31354;&#38388;&#19978;&#30340;Riemannian SGD&#21644;SVRG&#27969;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#26469;&#20016;&#23500;Wasserstein&#31354;&#38388;&#20013;&#30340;&#36830;&#32493;&#20248;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#20110;Riemannian&#27969;&#24418;&#19978;&#30340;&#20248;&#21270;&#30740;&#31350;&#20026;&#20248;&#21270;&#39046;&#22495;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#27010;&#29575;&#27979;&#24230;&#24230;&#37327;&#31354;&#38388;&#20316;&#20026;&#27969;&#24418;&#65292;&#37197;&#22791;&#31532;&#20108;&#38454;Wasserstein&#36317;&#31163;&#65292;&#23588;&#20854;&#24341;&#20154;&#20851;&#27880;&#65292;&#22240;&#20026;&#22312;&#20854;&#19978;&#30340;&#20248;&#21270;&#21487;&#20197;&#19982;&#23454;&#38469;&#30340;&#37319;&#26679;&#36807;&#31243;&#30456;&#20851;&#32852;&#12290;&#19968;&#33324;&#26469;&#35828;&#65292;Wasserstein&#31354;&#38388;&#19978;&#30340;&#26368;&#20248;&#21270;&#26041;&#27861;&#26159;Riemannian&#26799;&#24230;&#27969;&#65288;&#21363;&#65292;&#22312;&#26368;&#23567;&#21270;KL&#25955;&#24230;&#26102;&#30340;Langevin&#21160;&#21147;&#23398;&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#23558;&#26799;&#24230;&#27969;&#24310;&#23637;&#21040;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#27969;&#21644;&#38543;&#26426;&#26041;&#24046;&#20943;&#23569;&#26799;&#24230;&#65288;SVRG&#65289;&#27969;&#65292;&#20016;&#23500;Wasserstein&#31354;&#38388;&#20013;&#30340;&#36830;&#32493;&#20248;&#21270;&#26041;&#27861;&#12290;Euclidean&#31354;&#38388;&#19978;&#30340;&#36825;&#20004;&#31181;&#27969;&#26159;&#26631;&#20934;&#30340;&#38543;&#26426;&#20248;&#21270;&#26041;&#27861;&#65292;&#32780;&#23427;&#20204;&#22312;Riemannian&#31354;&#38388;&#20013;&#30340;&#23545;&#24212;&#26041;&#27861;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#36890;&#36807;&#21033;&#29992;Wasserstein&#31354;&#38388;&#20013;&#30340;&#32467;&#26500;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDE&#65289;&#26469;&#36817;&#20284;&#31163;&#25955;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, optimization on the Riemannian manifold has provided new insights to the optimization community. In this regard, the manifold taken as the probability measure metric space equipped with the second-order Wasserstein distance is of particular interest, since optimization on it can be linked to practical sampling processes. In general, the oracle (continuous) optimization method on Wasserstein space is Riemannian gradient flow (i.e., Langevin dynamics when minimizing KL divergence). In this paper, we aim to enrich the continuous optimization methods in the Wasserstein space by extending the gradient flow into the stochastic gradient descent (SGD) flow and stochastic variance reduction gradient (SVRG) flow. The two flows on Euclidean space are standard stochastic optimization methods, while their Riemannian counterparts are not explored yet. By leveraging the structures in Wasserstein space, we construct a stochastic differential equation (SDE) to approximate the discrete dynamic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#22122;&#22768;&#19987;&#23478;&#27169;&#22411;&#65288;ITEM&#65289;&#26469;&#35299;&#20915;&#26679;&#26412;&#36873;&#25321;&#20013;&#30340;&#35757;&#32451;&#20559;&#24046;&#21644;&#25968;&#25454;&#20559;&#24046;&#38382;&#39064;&#12290;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#40065;&#26834;&#30340;&#32593;&#32476;&#26550;&#26500;&#26469;&#38598;&#25104;&#22810;&#20010;&#19987;&#23478;&#65292;&#21487;&#20197;&#20943;&#23569;&#36873;&#25321;&#38598;&#19981;&#24179;&#34913;&#21644;&#32047;&#31215;&#38169;&#35823;&#65292;&#24182;&#22312;&#20351;&#29992;&#26356;&#23569;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26356;&#22909;&#30340;&#36873;&#25321;&#21644;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.13360</link><description>&lt;p&gt;
&#23545;&#25239;&#22122;&#22768;&#26631;&#31614;&#30340;&#26080;&#20559;&#26679;&#26412;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Debiased Sample Selection for Combating Noisy Labels. (arXiv:2401.13360v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13360
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#22122;&#22768;&#19987;&#23478;&#27169;&#22411;&#65288;ITEM&#65289;&#26469;&#35299;&#20915;&#26679;&#26412;&#36873;&#25321;&#20013;&#30340;&#35757;&#32451;&#20559;&#24046;&#21644;&#25968;&#25454;&#20559;&#24046;&#38382;&#39064;&#12290;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#40065;&#26834;&#30340;&#32593;&#32476;&#26550;&#26500;&#26469;&#38598;&#25104;&#22810;&#20010;&#19987;&#23478;&#65292;&#21487;&#20197;&#20943;&#23569;&#36873;&#25321;&#38598;&#19981;&#24179;&#34913;&#21644;&#32047;&#31215;&#38169;&#35823;&#65292;&#24182;&#22312;&#20351;&#29992;&#26356;&#23569;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26356;&#22909;&#30340;&#36873;&#25321;&#21644;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#20351;&#29992;&#22122;&#22768;&#26631;&#31614;&#26088;&#22312;&#30830;&#20445;&#27169;&#22411;&#22312;&#26631;&#31614;&#38169;&#35823;&#30340;&#35757;&#32451;&#38598;&#19978;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#12290;&#26679;&#26412;&#36873;&#25321;&#31574;&#30053;&#36890;&#36807;&#36873;&#25321;&#21487;&#38752;&#30340;&#26631;&#31614;&#23376;&#38598;&#26469;&#23454;&#29616;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#23454;&#35777;&#34920;&#26126;&#65292;&#29616;&#26377;&#30340;&#26679;&#26412;&#36873;&#25321;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#23384;&#22312;&#25968;&#25454;&#21644;&#35757;&#32451;&#20559;&#24046;&#65292;&#20998;&#21035;&#34920;&#31034;&#20026;&#36873;&#25321;&#38598;&#19981;&#24179;&#34913;&#21644;&#32047;&#31215;&#38169;&#35823;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#21482;&#22788;&#29702;&#20102;&#35757;&#32451;&#20559;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#26679;&#26412;&#36873;&#25321;&#30340;&#26080;&#22122;&#22768;&#19987;&#23478;&#27169;&#22411;&#65288;ITEM&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#20026;&#20102;&#20943;&#36731;&#35757;&#32451;&#20559;&#24046;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#40065;&#26834;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#19982;&#22810;&#20010;&#19987;&#23478;&#38598;&#25104;&#12290;&#19982;&#30446;&#21069;&#30340;&#21452;&#20998;&#25903;&#32593;&#32476;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#32593;&#32476;&#22312;&#35757;&#32451;&#26356;&#23569;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#38598;&#25104;&#36825;&#20123;&#19987;&#23478;&#26469;&#23454;&#29616;&#26356;&#22909;&#30340;&#36873;&#25321;&#21644;&#39044;&#27979;&#24615;&#33021;&#12290;&#21516;&#26102;&#65292;&#20026;&#20102;&#20943;&#36731;&#25968;&#25454;&#20559;&#24046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#37319;&#26679;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning with noisy labels aims to ensure model generalization given a label-corrupted training set. The sample selection strategy achieves promising performance by selecting a label-reliable subset for model training. In this paper, we empirically reveal that existing sample selection methods suffer from both data and training bias that are represented as imbalanced selected sets and accumulation errors in practice, respectively. However, only the training bias was handled in previous studies. To address this limitation, we propose a noIse-Tolerant Expert Model (ITEM) for debiased learning in sample selection. Specifically, to mitigate the training bias, we design a robust network architecture that integrates with multiple experts. Compared with the prevailing double-branch network, our network exhibits better performance of selection and prediction by ensembling these experts while training with fewer parameters. Meanwhile, to mitigate the data bias, we propose a mixed sampling strat
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23398;&#20064;&#25511;&#21046;&#36719;&#24418;&#21464;&#26426;&#22120;&#20154;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#31995;&#32479;DittoGym&#65292;&#35813;&#31995;&#32479;&#38656;&#35201;&#23545;&#26426;&#22120;&#20154;&#30340;&#24418;&#24577;&#36827;&#34892;&#32454;&#31890;&#24230;&#21464;&#21270;&#26469;&#23436;&#25104;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2401.13231</link><description>&lt;p&gt;
DittoGym:&#23398;&#20064;&#25511;&#21046;&#36719;&#24418;&#21464;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
DittoGym: Learning to Control Soft Shape-Shifting Robots. (arXiv:2401.13231v1 [cs.RO] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13231
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23398;&#20064;&#25511;&#21046;&#36719;&#24418;&#21464;&#26426;&#22120;&#20154;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#31995;&#32479;DittoGym&#65292;&#35813;&#31995;&#32479;&#38656;&#35201;&#23545;&#26426;&#22120;&#20154;&#30340;&#24418;&#24577;&#36827;&#34892;&#32454;&#31890;&#24230;&#21464;&#21270;&#26469;&#23436;&#25104;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#20849;&#21516;&#35774;&#35745;&#65292;&#20854;&#20013;&#26426;&#22120;&#20154;&#30340;&#24418;&#24577;&#20248;&#21270;&#19982;&#23398;&#20064;&#30340;&#31574;&#30053;&#20849;&#21516;&#35299;&#20915;&#29305;&#23450;&#20219;&#21153;&#65292;&#26159;&#19968;&#20010;&#26032;&#20852;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#23545;&#20110;&#36719;&#26426;&#22120;&#20154;&#26469;&#35828;&#65292;&#36825;&#19968;&#39046;&#22495;&#20855;&#26377;&#29305;&#21035;&#30340;&#28508;&#21147;&#65292;&#22240;&#20026;&#36719;&#26426;&#22120;&#20154;&#21487;&#20197;&#36890;&#36807;&#26032;&#39062;&#30340;&#21046;&#36896;&#25216;&#26415;&#23454;&#29616;&#23398;&#20064;&#21040;&#30340;&#24418;&#24577;&#21644;&#25191;&#34892;&#22120;&#12290;&#21463;&#33258;&#28982;&#30028;&#21644;&#26368;&#36817;&#30340;&#26032;&#22411;&#26426;&#22120;&#20154;&#35774;&#35745;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#26356;&#36827;&#19968;&#27493;&#25506;&#32034;&#26032;&#22411;&#21487;&#37325;&#26500;&#26426;&#22120;&#20154;&#65292;&#21363;&#22312;&#20854;&#23551;&#21629;&#20869;&#21487;&#20197;&#25913;&#21464;&#24418;&#24577;&#30340;&#26426;&#22120;&#20154;&#12290;&#25105;&#20204;&#23558;&#21487;&#37325;&#26500;&#36719;&#26426;&#22120;&#20154;&#30340;&#25511;&#21046;&#24418;&#24335;&#21270;&#20026;&#39640;&#32500;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#21516;&#19968;action&#31354;&#38388;&#20013;&#32479;&#19968;&#24418;&#24577;&#21464;&#21270;&#12289;&#36816;&#21160;&#21644;&#19982;&#29615;&#22659;&#30340;&#20114;&#21160;&#65292;&#24182;&#24341;&#20837;&#21512;&#36866;&#30340;&#31895;&#21040;&#32454;&#30340;&#35838;&#31243;&#34920;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#21457;&#29616;&#23454;&#29616;&#23545;&#26368;&#32456;&#26426;&#22120;&#20154;&#36827;&#34892;&#32454;&#31890;&#24230;&#25511;&#21046;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;DittoGym&#65292;&#36825;&#26159;&#19968;&#20010;&#38024;&#23545;&#21487;&#37325;&#26500;&#36719;&#26426;&#22120;&#20154;&#30340;&#20840;&#38754;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#65292;&#38656;&#35201;&#23545;&#24418;&#24577;&#36827;&#34892;&#32454;&#31890;&#24230;&#21464;&#21270;&#26469;&#23436;&#25104;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robot co-design, where the morphology of a robot is optimized jointly with a learned policy to solve a specific task, is an emerging area of research. It holds particular promise for soft robots, which are amenable to novel manufacturing techniques that can realize learned morphologies and actuators. Inspired by nature and recent novel robot designs, we propose to go a step further and explore the novel reconfigurable robots, defined as robots that can change their morphology within their lifetime. We formalize control of reconfigurable soft robots as a high-dimensional reinforcement learning (RL) problem. We unify morphology change, locomotion, and environment interaction in the same action space, and introduce an appropriate, coarse-to-fine curriculum that enables us to discover policies that accomplish fine-grained control of the resulting robots. We also introduce DittoGym, a comprehensive RL benchmark for reconfigurable soft robots that require fine-grained morphology changes to a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20108;&#36827;&#21046;&#32467;&#26500;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#20108;&#36827;&#21046;&#32467;&#26500;&#26469;&#25429;&#25417;&#23616;&#37096;&#29305;&#24449;&#65292;&#24182;&#35299;&#20915;&#20102;&#20256;&#32479;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#22312;&#22788;&#29702;&#20855;&#26377;&#24555;&#36895;&#21464;&#21270;&#35299;&#30340;&#26041;&#31243;&#26102;&#30340;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2401.12806</link><description>&lt;p&gt;
&#20108;&#36827;&#21046;&#32467;&#26500;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#24555;&#36895;&#21464;&#21270;&#35299;&#30340;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Binary structured physics-informed neural networks for solving equations with rapidly changing solutions. (arXiv:2401.12806v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12806
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20108;&#36827;&#21046;&#32467;&#26500;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#20108;&#36827;&#21046;&#32467;&#26500;&#26469;&#25429;&#25417;&#23616;&#37096;&#29305;&#24449;&#65292;&#24182;&#35299;&#20915;&#20102;&#20256;&#32479;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#22312;&#22788;&#29702;&#20855;&#26377;&#24555;&#36895;&#21464;&#21270;&#35299;&#30340;&#26041;&#31243;&#26102;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;(PINNs)&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#65292;&#24050;&#25104;&#20026;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;(PDEs)&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;PDEs&#25551;&#36848;&#30340;&#29289;&#29702;&#20449;&#24687;&#23884;&#20837;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;PINNs&#34987;&#35757;&#32451;&#20026;&#26367;&#20195;&#27169;&#22411;&#65292;&#20197;&#36817;&#20284;&#35299;&#20915;&#26041;&#26696;&#32780;&#26080;&#38656;&#26631;&#31614;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;PINNs&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#22312;&#22788;&#29702;&#20855;&#26377;&#24555;&#36895;&#21464;&#21270;&#35299;&#30340;&#26041;&#31243;&#26102;&#21487;&#33021;&#20250;&#36935;&#21040;&#22256;&#38590;&#12290;&#36825;&#20123;&#22256;&#38590;&#21253;&#25324;&#25910;&#25947;&#36895;&#24230;&#24930;&#12289;&#26131;&#38519;&#20837;&#23616;&#37096;&#26368;&#23567;&#20540;&#21644;&#35299;&#20915;&#31934;&#24230;&#38477;&#20302;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20108;&#36827;&#21046;&#32467;&#26500;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;(BsPINN)&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;&#20108;&#36827;&#21046;&#32467;&#26500;&#30340;&#31070;&#32463;&#32593;&#32476;(BsNN)&#20316;&#20026;&#31070;&#32463;&#32593;&#32476;&#32452;&#20214;&#12290;&#36890;&#36807;&#21033;&#29992;&#20108;&#36827;&#21046;&#32467;&#26500;&#65292;BsPINNs&#22312;&#25429;&#25417;&#23616;&#37096;&#29305;&#24449;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;
&lt;/p&gt;
&lt;p&gt;
Physics-informed neural networks (PINNs), rooted in deep learning, have emerged as a promising approach for solving partial differential equations (PDEs). By embedding the physical information described by PDEs into feedforward neural networks, PINNs are trained as surrogate models to approximate solutions without the need for label data. Nevertheless, even though PINNs have shown remarkable performance, they can face difficulties, especially when dealing with equations featuring rapidly changing solutions. These difficulties encompass slow convergence, susceptibility to becoming trapped in local minima, and reduced solution accuracy. To address these issues, we propose a binary structured physics-informed neural network (BsPINN) framework, which employs binary structured neural network (BsNN) as the neural network component. By leveraging a binary structure that reduces inter-neuron connections compared to fully connected neural networks, BsPINNs excel in capturing the local features 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33021;&#37327;&#30340;&#33258;&#21160;&#21270;&#27169;&#22411;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#20851;&#20110;&#20010;&#20307;&#26679;&#26412;&#30456;&#20851;&#20449;&#24687;&#30340;&#20803;&#20998;&#24067;&#32479;&#35745;&#37327;&#65292;&#33021;&#22815;&#26356;&#39640;&#25928;&#21644;&#26377;&#25928;&#22320;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#35299;&#20915;&#20102;AutoEval&#26694;&#26550;&#20013;&#30340;&#36807;&#24230;&#33258;&#20449;&#12289;&#23384;&#20648;&#21644;&#35745;&#31639;&#25104;&#26412;&#39640;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.12689</link><description>&lt;p&gt;
&#22522;&#20110;&#33021;&#37327;&#30340;&#33258;&#21160;&#21270;&#27169;&#22411;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Energy-based Automated Model Evaluation. (arXiv:2401.12689v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12689
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33021;&#37327;&#30340;&#33258;&#21160;&#21270;&#27169;&#22411;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#20851;&#20110;&#20010;&#20307;&#26679;&#26412;&#30456;&#20851;&#20449;&#24687;&#30340;&#20803;&#20998;&#24067;&#32479;&#35745;&#37327;&#65292;&#33021;&#22815;&#26356;&#39640;&#25928;&#21644;&#26377;&#25928;&#22320;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#35299;&#20915;&#20102;AutoEval&#26694;&#26550;&#20013;&#30340;&#36807;&#24230;&#33258;&#20449;&#12289;&#23384;&#20648;&#21644;&#35745;&#31639;&#25104;&#26412;&#39640;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35780;&#20272;&#21327;&#35758;&#20381;&#36182;&#20110;&#26631;&#35760;&#30340;&#12289;&#20551;&#35774;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#27979;&#35797;&#25968;&#25454;&#38598;&#65292;&#32780;&#36825;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#24448;&#24448;&#24182;&#19981;&#24120;&#35265;&#12290;&#33258;&#21160;&#27169;&#22411;&#35780;&#20272;&#65288;AutoEval&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#20256;&#32479;&#24037;&#20316;&#27969;&#31243;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24418;&#25104;&#19968;&#20010;&#25509;&#36817;&#39044;&#27979;&#24615;&#33021;&#30340;&#27979;&#35797;&#31649;&#32447;&#65292;&#32780;&#26080;&#38656;&#30495;&#23454;&#26631;&#31614;&#30340;&#23384;&#22312;&#12290;&#23613;&#31649;AutoEval&#26694;&#26550;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#19968;&#20123;&#25104;&#21151;&#65292;&#20294;&#20173;&#23384;&#22312;&#36807;&#24230;&#33258;&#20449;&#12289;&#23384;&#20648;&#21644;&#35745;&#31639;&#25104;&#26412;&#39640;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24230;&#37327;&#26041;&#24335;&#8212;&#8212;&#20803;&#20998;&#24067;&#33021;&#37327;&#65288;MDE&#65289;&#65292;&#23427;&#21487;&#20197;&#20351;AutoEval&#26694;&#26550;&#26356;&#21152;&#39640;&#25928;&#21644;&#26377;&#25928;&#12290;MDE&#30340;&#26680;&#24515;&#26159;&#24314;&#31435;&#19968;&#20010;&#20851;&#20110;&#20010;&#20307;&#26679;&#26412;&#30456;&#20851;&#20449;&#24687;&#65288;&#33021;&#37327;&#65289;&#30340;&#20803;&#20998;&#24067;&#32479;&#35745;&#37327;&#65292;&#28982;&#21518;&#36890;&#36807;&#22522;&#20110;&#33021;&#37327;&#30340;&#23398;&#20064;&#25552;&#20379;&#26356;&#24179;&#28369;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;MDE&#19982;&#20998;&#31867;&#25439;&#22833;&#30456;&#36830;&#25509;&#65292;&#36827;&#19968;&#27493;&#25552;&#20379;&#20102;&#29702;&#35770;&#27934;&#35265;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#25454;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The conventional evaluation protocols on machine learning models rely heavily on a labeled, i.i.d-assumed testing dataset, which is not often present in real world applications. The Automated Model Evaluation (AutoEval) shows an alternative to this traditional workflow, by forming a proximal prediction pipeline of the testing performance without the presence of ground-truth labels. Despite its recent successes, the AutoEval frameworks still suffer from an overconfidence issue, substantial storage and computational cost. In that regard, we propose a novel measure -- Meta-Distribution Energy (MDE) -- that allows the AutoEval framework to be both more efficient and effective. The core of the MDE is to establish a meta-distribution statistic, on the information (energy) associated with individual samples, then offer a smoother representation enabled by energy-based learning. We further provide our theoretical insights by connecting the MDE with the classification loss. We provide extensive
&lt;/p&gt;</description></item><item><title>BiTA&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21452;&#21521;&#35843;&#25972;&#23454;&#29616;&#20102;&#26080;&#25439;&#21152;&#36895;&#12290;&#23427;&#37319;&#29992;&#31616;&#21270;&#30340;&#21322;&#33258;&#22238;&#24402;&#29983;&#25104;&#21644;&#33609;&#31295;&#39564;&#35777;&#65292;&#36890;&#36807;&#39640;&#25928;&#30340;&#22522;&#20110;&#26641;&#30340;&#35299;&#30721;&#21516;&#26102;&#36827;&#34892;&#20505;&#36873;&#29983;&#25104;&#21644;&#39564;&#35777;&#65292;&#25552;&#39640;&#20102;&#25512;&#29702;&#25928;&#29575;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#36741;&#21161;&#27169;&#22411;&#25110;&#26174;&#33879;&#30340;&#39069;&#22806;&#20869;&#23384;&#24320;&#38144;&#12290;</title><link>http://arxiv.org/abs/2401.12522</link><description>&lt;p&gt;
BiTA: &#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#26080;&#25439;&#21152;&#36895;&#30340;&#21452;&#21521;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
BiTA: Bi-Directional Tuning for Lossless Acceleration in Large Language Models. (arXiv:2401.12522v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12522
&lt;/p&gt;
&lt;p&gt;
BiTA&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21452;&#21521;&#35843;&#25972;&#23454;&#29616;&#20102;&#26080;&#25439;&#21152;&#36895;&#12290;&#23427;&#37319;&#29992;&#31616;&#21270;&#30340;&#21322;&#33258;&#22238;&#24402;&#29983;&#25104;&#21644;&#33609;&#31295;&#39564;&#35777;&#65292;&#36890;&#36807;&#39640;&#25928;&#30340;&#22522;&#20110;&#26641;&#30340;&#35299;&#30721;&#21516;&#26102;&#36827;&#34892;&#20505;&#36873;&#29983;&#25104;&#21644;&#39564;&#35777;&#65292;&#25552;&#39640;&#20102;&#25512;&#29702;&#25928;&#29575;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#36741;&#21161;&#27169;&#22411;&#25110;&#26174;&#33879;&#30340;&#39069;&#22806;&#20869;&#23384;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#24120;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#20351;&#29992;&#33258;&#22238;&#24402;&#29983;&#25104;&#65292;&#23548;&#33268;&#39640;&#20869;&#23384;&#24102;&#23485;&#38656;&#27714;&#21644;&#24310;&#36831;&#24310;&#38271;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#31181;&#25928;&#29575;&#20302;&#19979;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#8212;&#8212;&#21452;&#21521;&#35843;&#25972;&#20197;&#23454;&#29616;&#26080;&#25439;&#21152;&#36895;&#65288;BiTA&#65289;&#65292;&#36890;&#36807;&#31616;&#21270;&#30340;&#21322;&#33258;&#22238;&#24402;&#29983;&#25104;&#21644;&#33609;&#31295;&#39564;&#35777;&#26469;&#21152;&#36895;LLMs&#12290;&#21463;&#21551;&#21457;&#20110;&#25552;&#31034;&#35843;&#25972;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#35774;&#35745;&#65292;&#31216;&#20026;&#21452;&#21521;&#35843;&#25972;&#65292;&#26469;&#22686;&#24378;LLMs&#22312;&#21322;&#33258;&#22238;&#24402;&#29983;&#25104;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#37319;&#29992;&#39640;&#25928;&#30340;&#22522;&#20110;&#26641;&#30340;&#35299;&#30721;&#65292;&#27169;&#22411;&#21487;&#20197;&#21516;&#26102;&#36827;&#34892;&#33609;&#31295;&#20505;&#36873;&#29983;&#25104;&#21644;&#39564;&#35777;&#65292;&#30830;&#20445;&#36755;&#20986;&#32467;&#26524;&#19982;&#23427;&#20204;&#30340;&#33258;&#22238;&#24402;&#23545;&#24212;&#29289;&#22312;&#36138;&#23146;&#25277;&#26679;&#19979;&#23436;&#20840;&#30456;&#21516;&#12290;BiTA&#20316;&#20026;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#25554;&#20214;&#27169;&#22359;&#65292;&#21487;&#20197;&#26080;&#32541;&#22686;&#24378;&#29616;&#26377;LLMs&#30340;&#25512;&#29702;&#25928;&#29575;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#36741;&#21161;&#27169;&#22411;&#25110;&#25215;&#25285;&#26174;&#33879;&#30340;&#39069;&#22806;&#20869;&#23384;&#24320;&#38144;&#12290;&#36890;&#36807;&#24212;&#29992;&#25552;&#20986;&#30340;BiTA&#65292;LLaMA-2-70B-Chat&#23454;&#29616;&#20102;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) commonly employ autoregressive generation during inference, leading to high memory bandwidth demand and consequently extended latency. To mitigate this inefficiency, we present Bi-directional Tuning for lossless Acceleration (BiTA), an innovative method expediting LLMs via streamlined semi-autoregressive generation and draft verification. Inspired by the concept of prompt tuning, we enhance LLMs with a parameter-efficient design called bi-directional tuning for the capability in semi-autoregressive generation. Employing efficient tree-based decoding, the models perform draft candidate generation and verification in parallel, ensuring outputs identical to their autoregressive counterparts under greedy sampling. BiTA serves as a lightweight plug-in module, seamlessly boosting the inference efficiency of existing LLMs without requiring additional assistance models or incurring significant extra memory costs. Applying the proposed BiTA, LLaMA-2-70B-Chat achieve
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25351;&#32441;&#35782;&#21035;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36731;&#37327;&#32423;&#30340;&#25351;&#20196;&#35843;&#25972;&#65292;&#20445;&#25252;&#30693;&#35782;&#20135;&#26435;&#24182;&#30830;&#20445;&#36981;&#23432;&#35768;&#21487;&#26465;&#27454;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#19981;&#24433;&#21709;&#27169;&#22411;&#30340;&#27491;&#24120;&#34892;&#20026;&#65292;&#24182;&#19988;&#20855;&#26377;&#40065;&#26834;&#24615;&#21644;&#39640;&#25928;&#35757;&#32451;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2401.12255</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#25351;&#32441;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Instructional Fingerprinting of Large Language Models. (arXiv:2401.12255v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12255
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25351;&#32441;&#35782;&#21035;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36731;&#37327;&#32423;&#30340;&#25351;&#20196;&#35843;&#25972;&#65292;&#20445;&#25252;&#30693;&#35782;&#20135;&#26435;&#24182;&#30830;&#20445;&#36981;&#23432;&#35768;&#21487;&#26465;&#27454;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#19981;&#24433;&#21709;&#27169;&#22411;&#30340;&#27491;&#24120;&#34892;&#20026;&#65292;&#24182;&#19988;&#20855;&#26377;&#40065;&#26834;&#24615;&#21644;&#39640;&#25928;&#35757;&#32451;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#38646;&#24320;&#22987;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24040;&#22823;&#25104;&#26412;&#20351;&#24471;&#23545;&#27169;&#22411;&#36827;&#34892;&#25351;&#32441;&#35782;&#21035;&#20197;&#20445;&#25252;&#30693;&#35782;&#20135;&#26435;&#25104;&#20026;&#24517;&#35201;&#65292;&#36890;&#36807;&#25152;&#26377;&#26435;&#35748;&#35777;&#24182;&#30830;&#20445;&#19979;&#28216;&#29992;&#25143;&#21644;&#24320;&#21457;&#32773;&#36981;&#23432;&#35768;&#21487;&#26465;&#27454;&#65288;&#22914;&#38480;&#21046;&#21830;&#19994;&#20351;&#29992;&#65289;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LLM&#25351;&#32441;&#35782;&#21035;&#30340;&#35797;&#28857;&#30740;&#31350;&#65292;&#20316;&#20026;&#19968;&#31181;&#38750;&#24120;&#36731;&#37327;&#32423;&#30340;&#25351;&#20196;&#35843;&#25972;&#24418;&#24335;&#12290;&#27169;&#22411;&#21457;&#24067;&#32773;&#25351;&#23450;&#19968;&#20010;&#26426;&#23494;&#30340;&#31169;&#38053;&#65292;&#24182;&#23558;&#20854;&#26893;&#20837;&#20026;&#19968;&#20010;&#25351;&#20196;&#21518;&#38376;&#65292;&#24403;&#23494;&#38053;&#23384;&#22312;&#26102;&#65292;&#23548;&#33268;LLM&#29983;&#25104;&#29305;&#23450;&#30340;&#25991;&#26412;&#12290;&#23545;11&#20010;&#24120;&#29992;LLMs&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#36731;&#37327;&#32423;&#19988;&#19981;&#24433;&#21709;&#27169;&#22411;&#30340;&#27491;&#24120;&#34892;&#20026;&#12290;&#23427;&#36824;&#21487;&#20197;&#38450;&#27490;&#21457;&#24067;&#32773;&#36807;&#24230;&#23459;&#31216;&#65292;&#23545;&#25351;&#32441;&#29468;&#27979;&#21644;&#21442;&#25968;&#39640;&#25928;&#35757;&#32451;&#20445;&#25345;&#40065;&#26834;&#24615;&#65292;&#24182;&#25903;&#25345;&#31867;&#20284;&#20110;MIT&#35768;&#21487;&#35777;&#30340;&#22810;&#38454;&#27573;&#25351;&#32441;&#35782;&#21035;&#12290;&#20195;&#30721;&#21487;&#22312;https://cnut1648.github.io/Model-Fingerprint/&#20013;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
The exorbitant cost of training Large language models (LLMs) from scratch makes it essential to fingerprint the models to protect intellectual property via ownership authentication and to ensure downstream users and developers comply with their license terms (e.g. restricting commercial use). In this study, we present a pilot study on LLM fingerprinting as a form of very lightweight instruction tuning. Model publisher specifies a confidential private key and implants it as an instruction backdoor that causes the LLM to generate specific text when the key is present. Results on 11 popularly-used LLMs showed that this approach is lightweight and does not affect the normal behavior of the model. It also prevents publisher overclaim, maintains robustness against fingerprint guessing and parameter-efficient training, and supports multi-stage fingerprinting akin to MIT License. Code is available in https://cnut1648.github.io/Model-Fingerprint/.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35777;&#26126;&#20102;&#21363;&#20351;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#22122;&#22768;&#25968;&#25454;&#25311;&#21512;&#24471;&#24456;&#22909;&#65292;&#23545;&#25932;&#23545;&#31034;&#20363;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#20294;&#24403;&#38754;&#20020;&#25932;&#23545;&#25805;&#32437;&#30340;&#25968;&#25454;&#26102;&#65292;&#36807;&#24230;&#25311;&#21512;&#30340;&#27169;&#22411;&#21487;&#33021;&#20250;&#32473;&#31995;&#32479;&#24102;&#26469;&#24847;&#22806;&#30340;&#21361;&#23475;&#12290;</title><link>http://arxiv.org/abs/2401.12236</link><description>&lt;p&gt;
&#26080;&#23475;&#36807;&#24230;&#25311;&#21512;&#23545;&#25932;&#23545;&#40065;&#26834;&#24615;&#30340;&#24847;&#22806;&#21361;&#23475;
&lt;/p&gt;
&lt;p&gt;
The Surprising Harmfulness of Benign Overfitting for Adversarial Robustness. (arXiv:2401.12236v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12236
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35777;&#26126;&#20102;&#21363;&#20351;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#22122;&#22768;&#25968;&#25454;&#25311;&#21512;&#24471;&#24456;&#22909;&#65292;&#23545;&#25932;&#23545;&#31034;&#20363;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#20294;&#24403;&#38754;&#20020;&#25932;&#23545;&#25805;&#32437;&#30340;&#25968;&#25454;&#26102;&#65292;&#36807;&#24230;&#25311;&#21512;&#30340;&#27169;&#22411;&#21487;&#33021;&#20250;&#32473;&#31995;&#32479;&#24102;&#26469;&#24847;&#22806;&#30340;&#21361;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#23454;&#35777;&#21644;&#29702;&#35770;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#20102;&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#35757;&#32451;&#22122;&#22768;&#25968;&#25454;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#30340;&#32467;&#26524;&#65306;&#21363;&#20351;&#30495;&#27491;&#30340;&#25968;&#25454;&#26412;&#36523;&#23545;&#25932;&#23545;&#31034;&#20363;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#32780;&#19988;&#36807;&#24230;&#25311;&#21512;&#30340;&#27169;&#22411;&#22312;&#8220;&#26631;&#20934;&#8221;&#30340;&#26679;&#26412;&#22806;&#39118;&#38505;&#30446;&#26631;&#19978;&#26159;&#26080;&#23475;&#30340;&#65292;&#20294;&#22312;&#26679;&#26412;&#22806;&#25968;&#25454;&#21463;&#21040;&#25932;&#23545;&#25805;&#32437;&#26102;&#65292;&#36825;&#31181;&#26080;&#23475;&#30340;&#36807;&#24230;&#25311;&#21512;&#36807;&#31243;&#21487;&#33021;&#26159;&#26377;&#23475;&#30340;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#21253;&#21547;&#20004;&#20010;&#37096;&#20998;&#65306;&#65288;i&#65289;&#22312;&#36807;&#24230;&#21442;&#25968;&#21270;&#32447;&#24615;&#27169;&#22411;&#20013;&#65292;&#26368;&#23567;&#33539;&#25968;&#20272;&#35745;&#24635;&#26159;&#22312;&#8220;&#26080;&#23475;&#36807;&#24230;&#25311;&#21512;&#8221;&#35774;&#32622;&#20013;&#23548;&#33268;&#25932;&#23545;&#26131;&#21463;&#25915;&#20987;&#65307;&#65288;ii&#65289;&#25105;&#20204;&#39564;&#35777;&#20102;&#27599;&#20010;&#23725;&#22238;&#24402;&#20272;&#35745;&#22120;&#30340;&#26631;&#20934;&#39118;&#38505;&#21644;&#8220;&#25932;&#23545;&#8221;&#39118;&#38505;&#20043;&#38388;&#30340;&#28176;&#36827;&#26435;&#34913;&#32467;&#26524;&#65292;&#36825;&#24847;&#21619;&#30528;&#22312;&#36866;&#24403;&#30340;&#26465;&#20214;&#19979;&#65292;&#36825;&#20004;&#20010;&#39033;&#30446;&#19981;&#33021;&#21516;&#26102;&#36890;&#36807;&#20219;&#20309;&#21333;&#20010;&#23725;&#27491;&#21017;&#21270;&#21442;&#25968;&#30340;&#36873;&#25321;&#26469;&#20445;&#25345;&#24456;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent empirical and theoretical studies have established the generalization capabilities of large machine learning models that are trained to (approximately or exactly) fit noisy data. In this work, we prove a surprising result that even if the ground truth itself is robust to adversarial examples, and the benignly overfitted model is benign in terms of the ``standard'' out-of-sample risk objective, this benign overfitting process can be harmful when out-of-sample data are subject to adversarial manipulation. More specifically, our main results contain two parts: (i) the min-norm estimator in overparameterized linear model always leads to adversarial vulnerability in the ``benign overfitting'' setting; (ii) we verify an asymptotic trade-off result between the standard risk and the ``adversarial'' risk of every ridge regression estimator, implying that under suitable conditions these two items cannot both be small at the same time by any single choice of the ridge regularization parame
&lt;/p&gt;</description></item><item><title>TurboSVM-FL&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#32852;&#37030;&#32858;&#21512;&#31574;&#30053;&#65292;&#36890;&#36807;SVM&#32858;&#21512;&#20026;&#25042;&#24816;&#23458;&#25143;&#31471;&#22686;&#24378;&#32852;&#37030;&#23398;&#20064;&#12290;&#36825;&#31181;&#31574;&#30053;&#22312;&#19981;&#22686;&#21152;&#23458;&#25143;&#31471;&#35745;&#31639;&#36127;&#25285;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#25910;&#25947;&#36895;&#24230;&#24930;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.12012</link><description>&lt;p&gt;
TurboSVM-FL: &#36890;&#36807;SVM&#32858;&#21512;&#20026;&#25042;&#24816;&#23458;&#25143;&#31471;&#22686;&#24378;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
TurboSVM-FL: Boosting Federated Learning through SVM Aggregation for Lazy Clients. (arXiv:2401.12012v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12012
&lt;/p&gt;
&lt;p&gt;
TurboSVM-FL&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#32852;&#37030;&#32858;&#21512;&#31574;&#30053;&#65292;&#36890;&#36807;SVM&#32858;&#21512;&#20026;&#25042;&#24816;&#23458;&#25143;&#31471;&#22686;&#24378;&#32852;&#37030;&#23398;&#20064;&#12290;&#36825;&#31181;&#31574;&#30053;&#22312;&#19981;&#22686;&#21152;&#23458;&#25143;&#31471;&#35745;&#31639;&#36127;&#25285;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#25910;&#25947;&#36895;&#24230;&#24930;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#21327;&#20316;&#26426;&#22120;&#23398;&#20064;&#33539;&#20363;&#65292;&#22312;&#36817;&#24180;&#26469;&#33719;&#24471;&#20102;&#24378;&#28872;&#30340;&#25512;&#21160;&#21147;&#12290;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#20013;&#22830;&#26381;&#21153;&#22120;&#23450;&#26399;&#36890;&#36807;&#23458;&#25143;&#31471;&#21327;&#35843;&#27169;&#22411;&#65292;&#24182;&#32858;&#21512;&#30001;&#23458;&#25143;&#31471;&#22312;&#26412;&#22320;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#35775;&#38382;&#26412;&#22320;&#25968;&#25454;&#12290;&#23613;&#31649;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#32852;&#37030;&#23398;&#20064;&#30340;&#23454;&#26045;&#20173;&#28982;&#38754;&#20020;&#19968;&#20123;&#25361;&#25112;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#25968;&#25454;&#24322;&#36136;&#24615;&#23548;&#33268;&#30340;&#25910;&#25947;&#36895;&#24230;&#24930;&#12290;&#25910;&#25947;&#36895;&#24230;&#24930;&#22312;&#36328;&#35774;&#22791;&#32852;&#37030;&#23398;&#20064;&#22330;&#26223;&#20013;&#23588;&#20026;&#38382;&#39064;&#65292;&#20854;&#20013;&#23458;&#25143;&#31471;&#21487;&#33021;&#21463;&#21040;&#35745;&#31639;&#33021;&#21147;&#21644;&#23384;&#20648;&#31354;&#38388;&#30340;&#20005;&#37325;&#38480;&#21046;&#65292;&#22240;&#27492;&#23545;&#23458;&#25143;&#31471;&#20135;&#29983;&#39069;&#22806;&#35745;&#31639;&#25110;&#20869;&#23384;&#36127;&#25285;&#30340;&#26041;&#27861;&#65292;&#22914;&#36741;&#21161;&#30446;&#26631;&#39033;&#21644;&#26356;&#22823;&#30340;&#35757;&#32451;&#36845;&#20195;&#27425;&#25968;&#65292;&#21487;&#33021;&#19981;&#23454;&#38469;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32852;&#37030;&#32858;&#21512;&#31574;&#30053;TurboSVM-FL&#65292;&#23427;&#19981;&#20250;&#32473;&#23458;&#25143;&#31471;&#22686;&#21152;&#39069;&#22806;&#30340;&#35745;&#31639;&#36127;&#25285;
&lt;/p&gt;
&lt;p&gt;
Federated learning is a distributed collaborative machine learning paradigm that has gained strong momentum in recent years. In federated learning, a central server periodically coordinates models with clients and aggregates the models trained locally by clients without necessitating access to local data. Despite its potential, the implementation of federated learning continues to encounter several challenges, predominantly the slow convergence that is largely due to data heterogeneity. The slow convergence becomes particularly problematic in cross-device federated learning scenarios where clients may be strongly limited by computing power and storage space, and hence counteracting methods that induce additional computation or memory cost on the client side such as auxiliary objective terms and larger training iterations can be impractical. In this paper, we propose a novel federated aggregation strategy, TurboSVM-FL, that poses no additional computation burden on the client side and c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26368;&#20248;&#36755;&#36816;&#29702;&#35770;&#19982;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#12290;&#36890;&#36807;&#21033;&#29992;&#26368;&#20248;&#36755;&#36816;&#26469;&#22788;&#29702;&#20998;&#24067;&#21644;&#36816;&#36755;&#38382;&#39064;&#65292;&#22686;&#24378;&#20102;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#25928;&#29575;&#12289;&#21327;&#35843;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;&#36890;&#36807;&#22312;&#25919;&#31574;&#23545;&#40784;&#12289;&#20998;&#24067;&#24335;&#36164;&#28304;&#31649;&#29702;&#12289;&#24212;&#23545;&#38750;&#24179;&#31283;&#24615;&#12289;&#21487;&#25193;&#23637;&#30340;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#21644;&#25552;&#39640;&#33021;&#28304;&#25928;&#29575;&#20116;&#20010;&#26041;&#38754;&#24212;&#29992;&#26368;&#20248;&#36755;&#36816;&#29702;&#35770;&#65292;&#20026;&#35299;&#20915;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12289;&#20248;&#21270;&#36164;&#28304;&#20998;&#37197;&#21644;&#22312;&#21512;&#20316;&#29615;&#22659;&#20013;&#23545;&#40784;&#26234;&#33021;&#20307;&#31574;&#30053;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.10949</link><description>&lt;p&gt;
&#12298;&#26368;&#20248;&#36755;&#36816;&#29702;&#35770;&#19982;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#12299;
&lt;/p&gt;
&lt;p&gt;
The Synergy Between Optimal Transport Theory and Multi-Agent Reinforcement Learning. (arXiv:2401.10949v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10949
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26368;&#20248;&#36755;&#36816;&#29702;&#35770;&#19982;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#12290;&#36890;&#36807;&#21033;&#29992;&#26368;&#20248;&#36755;&#36816;&#26469;&#22788;&#29702;&#20998;&#24067;&#21644;&#36816;&#36755;&#38382;&#39064;&#65292;&#22686;&#24378;&#20102;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#25928;&#29575;&#12289;&#21327;&#35843;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;&#36890;&#36807;&#22312;&#25919;&#31574;&#23545;&#40784;&#12289;&#20998;&#24067;&#24335;&#36164;&#28304;&#31649;&#29702;&#12289;&#24212;&#23545;&#38750;&#24179;&#31283;&#24615;&#12289;&#21487;&#25193;&#23637;&#30340;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#21644;&#25552;&#39640;&#33021;&#28304;&#25928;&#29575;&#20116;&#20010;&#26041;&#38754;&#24212;&#29992;&#26368;&#20248;&#36755;&#36816;&#29702;&#35770;&#65292;&#20026;&#35299;&#20915;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12289;&#20248;&#21270;&#36164;&#28304;&#20998;&#37197;&#21644;&#22312;&#21512;&#20316;&#29615;&#22659;&#20013;&#23545;&#40784;&#26234;&#33021;&#20307;&#31574;&#30053;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#26368;&#20248;&#36755;&#36816;&#65288;OT&#65289;&#29702;&#35770;&#19982;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#30340;&#25972;&#21512;&#12290;&#35813;&#25972;&#21512;&#20351;&#29992;OT&#22788;&#29702;&#20998;&#24067;&#21644;&#36816;&#36755;&#38382;&#39064;&#65292;&#20197;&#25552;&#39640;MARL&#30340;&#25928;&#29575;&#12289;&#21327;&#35843;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;OT&#22312;&#20197;&#19979;&#20116;&#20010;&#20851;&#38190;&#39046;&#22495;&#21487;&#20197;&#24433;&#21709;MARL&#65306;&#65288;1&#65289;&#25919;&#31574;&#23545;&#40784;&#65292;&#21033;&#29992;OT&#30340;Wasserstein&#24230;&#37327;&#26469;&#23558;&#19981;&#21516;&#30340;&#26234;&#33021;&#20307;&#31574;&#30053;&#23545;&#40784;&#21040;&#32479;&#19968;&#30340;&#30446;&#26631;&#19978;&#65307;&#65288;2&#65289;&#20998;&#24067;&#24335;&#36164;&#28304;&#31649;&#29702;&#65292;&#21033;&#29992;OT&#26469;&#20248;&#21270;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#36164;&#28304;&#20998;&#37197;&#65307;&#65288;3&#65289;&#24212;&#23545;&#38750;&#24179;&#31283;&#24615;&#65292;&#21033;&#29992;OT&#36866;&#24212;&#21160;&#24577;&#29615;&#22659;&#21464;&#21270;&#65307;&#65288;4&#65289;&#21487;&#25193;&#23637;&#30340;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#65292;&#21033;&#29992;OT&#23558;&#22823;&#35268;&#27169;&#23398;&#20064;&#30446;&#26631;&#20998;&#35299;&#20026;&#21487;&#31649;&#29702;&#30340;&#20219;&#21153;&#65307;&#65288;5&#65289;&#25552;&#39640;&#33021;&#28304;&#25928;&#29575;&#65292;&#24212;&#29992;OT&#21407;&#21017;&#26469;&#24320;&#21457;&#21487;&#25345;&#32493;&#30340;MARL&#31995;&#32479;&#12290;&#26412;&#25991;&#38416;&#36848;&#20102;OT&#19982;MARL&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#22914;&#20309;&#35299;&#20915;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12289;&#20248;&#21270;&#36164;&#28304;&#20998;&#37197;&#12289;&#22312;&#21512;&#20316;&#29615;&#22659;&#20013;&#23545;&#40784;&#26234;&#33021;&#20307;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the integration of optimal transport (OT) theory with multi-agent reinforcement learning (MARL). This integration uses OT to handle distributions and transportation problems to enhance the efficiency, coordination, and adaptability of MARL. There are five key areas where OT can impact MARL: (1) policy alignment, where OT's Wasserstein metric is used to align divergent agent strategies towards unified goals; (2) distributed resource management, employing OT to optimize resource allocation among agents; (3) addressing non-stationarity, using OT to adapt to dynamic environmental shifts; (4) scalable multi-agent learning, harnessing OT for decomposing large-scale learning objectives into manageable tasks; and (5) enhancing energy efficiency, applying OT principles to develop sustainable MARL systems. This paper articulates how the synergy between OT and MARL can address scalability issues, optimize resource distribution, align agent policies in cooperative environments,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#21644;&#25991;&#29486;&#35745;&#37327;&#20998;&#26512;&#65292;&#22635;&#34917;&#20102;&#20379;&#24212;&#38142;&#39118;&#38505;&#35780;&#20272;&#20013;&#26032;&#20852;&#20154;&#24037;&#26234;&#33021;/&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#30740;&#31350;&#31354;&#30333;&#65292;&#20026;&#20102;&#35299;&#36825;&#20123;&#25216;&#26415;&#22312;&#23454;&#36341;&#20013;&#30340;&#23454;&#38469;&#24433;&#21709;&#25552;&#20379;&#20102;&#20851;&#38190;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2401.10895</link><description>&lt;p&gt;
&#20379;&#24212;&#38142;&#39118;&#38505;&#35780;&#20272;&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;&#65306;&#19968;&#39033;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#21644;&#25991;&#29486;&#35745;&#37327;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
AI in Supply Chain Risk Assessment: A Systematic Literature Review and Bibliometric Analysis. (arXiv:2401.10895v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10895
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#21644;&#25991;&#29486;&#35745;&#37327;&#20998;&#26512;&#65292;&#22635;&#34917;&#20102;&#20379;&#24212;&#38142;&#39118;&#38505;&#35780;&#20272;&#20013;&#26032;&#20852;&#20154;&#24037;&#26234;&#33021;/&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#30740;&#31350;&#31354;&#30333;&#65292;&#20026;&#20102;&#35299;&#36825;&#20123;&#25216;&#26415;&#22312;&#23454;&#36341;&#20013;&#30340;&#23454;&#38469;&#24433;&#21709;&#25552;&#20379;&#20102;&#20851;&#38190;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25972;&#21512;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#20379;&#24212;&#38142;&#39118;&#38505;&#35780;&#20272;(SCRA)&#32463;&#21382;&#20102;&#28145;&#21051;&#30340;&#28436;&#21464;&#65292;&#38761;&#26032;&#20102;&#39044;&#27979;&#33021;&#21147;&#21644;&#39118;&#38505;&#32531;&#35299;&#31574;&#30053;&#12290;&#36825;&#31181;&#28436;&#21464;&#30340;&#37325;&#35201;&#24615;&#22312;&#20110;&#22312;&#29616;&#20195;&#20379;&#24212;&#38142;&#20013;&#30830;&#20445;&#36816;&#33829;&#30340;&#38887;&#24615;&#21644;&#36830;&#32493;&#24615;&#65292;&#38656;&#35201;&#31283;&#20581;&#30340;&#39118;&#38505;&#31649;&#29702;&#31574;&#30053;&#12290;&#20197;&#24448;&#30340;&#32508;&#36848;&#24050;&#32463;&#27010;&#36848;&#20102;&#24050;&#24314;&#31435;&#30340;&#26041;&#27861;&#65292;&#20294;&#24573;&#35270;&#20102;&#26032;&#20852;&#30340;&#20154;&#24037;&#26234;&#33021;/&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#22312;&#29702;&#35299;&#20854;&#22312;SCRA&#20013;&#30340;&#23454;&#38469;&#24433;&#21709;&#26041;&#38754;&#23384;&#22312;&#26126;&#26174;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;&#26412;&#25991;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#25991;&#29486;&#32508;&#36848;&#65292;&#24182;&#32467;&#21512;&#20102;&#20840;&#38754;&#30340;&#25991;&#29486;&#35745;&#37327;&#20998;&#26512;&#12290;&#25105;&#20204;&#20180;&#32454;&#30740;&#31350;&#20102;1717&#31687;&#35770;&#25991;&#65292;&#24182;&#20174;2014&#24180;&#33267;2023&#24180;&#20043;&#38388;&#21457;&#34920;&#30340;48&#31687;&#25991;&#31456;&#20013;&#33719;&#24471;&#20102;&#20851;&#38190;&#35265;&#35299;&#12290;&#35813;&#32508;&#36848;&#22635;&#34917;&#20102;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#65292;&#36890;&#36807;&#22238;&#31572;&#20851;&#38190;&#30740;&#31350;&#38382;&#39064;&#65292;&#25506;&#31350;&#20102;&#29616;&#26377;&#30340;&#20154;&#24037;&#26234;&#33021;/&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#12289;&#26041;&#27861;&#35770;&#12289;&#30740;&#31350;&#32467;&#26524;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supply chain risk assessment (SCRA) has witnessed a profound evolution through the integration of artificial intelligence (AI) and machine learning (ML) techniques, revolutionizing predictive capabilities and risk mitigation strategies. The significance of this evolution stems from the critical role of robust risk management strategies in ensuring operational resilience and continuity within modern supply chains. Previous reviews have outlined established methodologies but have overlooked emerging AI/ML techniques, leaving a notable research gap in understanding their practical implications within SCRA. This paper conducts a systematic literature review combined with a comprehensive bibliometric analysis. We meticulously examined 1,717 papers and derived key insights from a select group of 48 articles published between 2014 and 2023. The review fills this research gap by addressing pivotal research questions, and exploring existing AI/ML techniques, methodologies, findings, and future 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#65292;&#26126;&#30830;&#25110;&#38544;&#21547;&#22320;&#24809;&#32602;&#20108;&#38454;&#20449;&#24687;&#21487;&#20197;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#65292;&#32780;&#26435;&#37325;&#22122;&#22768;&#21644;&#26799;&#24230;&#24809;&#32602;&#21017;&#24456;&#23569;&#33021;&#24102;&#26469;&#36825;&#26679;&#30340;&#22909;&#22788;&#12290;&#20316;&#32773;&#36890;&#36807;&#23545;&#25439;&#22833;&#30340;&#40657;&#22622;&#30697;&#38453;&#30340;&#32467;&#26500;&#36827;&#34892;&#35299;&#37322;&#65292;&#25552;&#20986;&#20102;&#29305;&#24449;&#30340;&#24320;&#21457;&#21644;&#29305;&#24449;&#30340;&#25506;&#32034;&#20043;&#38388;&#30340;&#37327;&#21270;&#20998;&#31163;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#21457;&#29616;&#24573;&#35270;&#30340;&#38750;&#32447;&#24615;&#24314;&#27169;&#35823;&#24046;&#30697;&#38453; (NME) &#23454;&#38469;&#19978;&#24456;&#37325;&#35201;&#65292;&#21487;&#20197;&#35299;&#37322;&#20026;&#20160;&#20040;&#26799;&#24230;&#24809;&#32602;&#23545;&#28608;&#27963;&#20989;&#25968;&#30340;&#36873;&#25321;&#38750;&#24120;&#25935;&#24863;&#12290;&#27492;&#22806;&#65292;&#20316;&#32773;&#36890;&#36807;&#35774;&#35745;&#24178;&#39044;&#25514;&#26045;&#26469;&#25913;&#36827;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#35777;&#25454;&#25361;&#25112;&#20102;&#20197;&#24448;&#30340;&#35266;&#28857;&#65292;&#35748;&#20026;&#26435;&#37325;&#22122;&#22768;&#21644;&#26799;&#24230;&#24809;&#32602;&#26159;&#31561;&#25928;&#30340;&#12290;</title><link>http://arxiv.org/abs/2401.10809</link><description>&lt;p&gt;
&#34987;&#24573;&#35270;&#30340;&#40657;&#22622; (Hessian) &#32452;&#20214;&#35299;&#37322;&#20102;&#38160;&#21270;&#27491;&#21017;&#21270;&#20013;&#30340;&#35868;&#22242;
&lt;/p&gt;
&lt;p&gt;
Neglected Hessian component explains mysteries in Sharpness regularization. (arXiv:2401.10809v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10809
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#65292;&#26126;&#30830;&#25110;&#38544;&#21547;&#22320;&#24809;&#32602;&#20108;&#38454;&#20449;&#24687;&#21487;&#20197;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#65292;&#32780;&#26435;&#37325;&#22122;&#22768;&#21644;&#26799;&#24230;&#24809;&#32602;&#21017;&#24456;&#23569;&#33021;&#24102;&#26469;&#36825;&#26679;&#30340;&#22909;&#22788;&#12290;&#20316;&#32773;&#36890;&#36807;&#23545;&#25439;&#22833;&#30340;&#40657;&#22622;&#30697;&#38453;&#30340;&#32467;&#26500;&#36827;&#34892;&#35299;&#37322;&#65292;&#25552;&#20986;&#20102;&#29305;&#24449;&#30340;&#24320;&#21457;&#21644;&#29305;&#24449;&#30340;&#25506;&#32034;&#20043;&#38388;&#30340;&#37327;&#21270;&#20998;&#31163;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#21457;&#29616;&#24573;&#35270;&#30340;&#38750;&#32447;&#24615;&#24314;&#27169;&#35823;&#24046;&#30697;&#38453; (NME) &#23454;&#38469;&#19978;&#24456;&#37325;&#35201;&#65292;&#21487;&#20197;&#35299;&#37322;&#20026;&#20160;&#20040;&#26799;&#24230;&#24809;&#32602;&#23545;&#28608;&#27963;&#20989;&#25968;&#30340;&#36873;&#25321;&#38750;&#24120;&#25935;&#24863;&#12290;&#27492;&#22806;&#65292;&#20316;&#32773;&#36890;&#36807;&#35774;&#35745;&#24178;&#39044;&#25514;&#26045;&#26469;&#25913;&#36827;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#35777;&#25454;&#25361;&#25112;&#20102;&#20197;&#24448;&#30340;&#35266;&#28857;&#65292;&#35748;&#20026;&#26435;&#37325;&#22122;&#22768;&#21644;&#26799;&#24230;&#24809;&#32602;&#26159;&#31561;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20687;SAM&#36825;&#26679;&#26126;&#30830;&#25110;&#38544;&#21547;&#22320;&#24809;&#32602;&#20108;&#38454;&#20449;&#24687;&#30340;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#31867;&#20284;&#30340;&#26041;&#27861;&#65292;&#22914;&#26435;&#37325;&#22122;&#22768;&#21644;&#26799;&#24230;&#24809;&#32602;&#65292;&#32463;&#24120;&#26080;&#27861;&#25552;&#20379;&#36825;&#26679;&#30340;&#22909;&#22788;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#24046;&#24322;&#21487;&#20197;&#36890;&#36807;&#25439;&#22833;&#30340;&#40657;&#22622;&#30697;&#38453;&#30340;&#32467;&#26500;&#26469;&#35299;&#37322;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24120;&#29992;&#30340;&#40657;&#22622;&#30697;&#38453;&#30340;&#20998;&#35299;&#21487;&#20197;&#23450;&#37327;&#35299;&#37322;&#20026;&#23558;&#29305;&#24449;&#30340;&#24320;&#21457;&#21644;&#29305;&#24449;&#30340;&#25506;&#32034;&#20998;&#24320;&#12290;&#29305;&#24449;&#30340;&#25506;&#32034;&#21487;&#20197;&#36890;&#36807;&#38750;&#32447;&#24615;&#24314;&#27169;&#35823;&#24046;&#30697;&#38453;(NME)&#26469;&#25551;&#36848;&#65292;&#36825;&#22312;&#25991;&#29486;&#20013;&#36890;&#24120;&#34987;&#24573;&#35270;&#65292;&#22240;&#20026;&#23427;&#22312;&#25554;&#20540;&#20013;&#28040;&#22833;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;NME&#20107;&#23454;&#19978;&#24456;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#35299;&#37322;&#20026;&#20160;&#20040;&#26799;&#24230;&#24809;&#32602;&#23545;&#28608;&#27963;&#20989;&#25968;&#30340;&#36873;&#25321;&#25935;&#24863;&#12290;&#21033;&#29992;&#36825;&#19968;&#35265;&#35299;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#25913;&#36827;&#24615;&#33021;&#30340;&#24178;&#39044;&#25514;&#26045;&#12290;&#25105;&#20204;&#20063;&#25552;&#20379;&#20102;&#35777;&#25454;&#26469;&#25361;&#25112;&#20102;&#38271;&#26399;&#20197;&#26469;&#26435;&#37325;&#22122;&#22768;&#21644;&#26799;&#24230;&#24809;&#32602;&#30340;&#31561;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work has shown that methods like SAM which either explicitly or implicitly penalize second order information can improve generalization in deep learning. Seemingly similar methods like weight noise and gradient penalties often fail to provide such benefits. We show that these differences can be explained by the structure of the Hessian of the loss. First, we show that a common decomposition of the Hessian can be quantitatively interpreted as separating the feature exploitation from feature exploration. The feature exploration, which can be described by the Nonlinear Modeling Error matrix (NME), is commonly neglected in the literature since it vanishes at interpolation. Our work shows that the NME is in fact important as it can explain why gradient penalties are sensitive to the choice of activation function. Using this insight we design interventions to improve performance. We also provide evidence that challenges the long held equivalence of weight noise and gradient penalties.
&lt;/p&gt;</description></item><item><title>Mementos&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22270;&#20687;&#24207;&#21015;&#25512;&#29702;&#20013;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#29616;&#26377;&#30340;MLLM&#22312;&#20934;&#30830;&#25551;&#36848;&#22270;&#20687;&#24207;&#21015;&#30340;&#21160;&#24577;&#20449;&#24687;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#23481;&#26131;&#23548;&#33268;&#29289;&#20307;&#21450;&#20854;&#34892;&#20026;&#30340;&#38169;&#35823;&#25551;&#36848;&#25110;&#38169;&#35273;&#12290;</title><link>http://arxiv.org/abs/2401.10529</link><description>&lt;p&gt;
Mementos: &#19968;&#31181;&#38024;&#23545;&#22270;&#20687;&#24207;&#21015;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences. (arXiv:2401.10529v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10529
&lt;/p&gt;
&lt;p&gt;
Mementos&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22270;&#20687;&#24207;&#21015;&#25512;&#29702;&#20013;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#29616;&#26377;&#30340;MLLM&#22312;&#20934;&#30830;&#25551;&#36848;&#22270;&#20687;&#24207;&#21015;&#30340;&#21160;&#24577;&#20449;&#24687;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#23481;&#26131;&#23548;&#33268;&#29289;&#20307;&#21450;&#20854;&#34892;&#20026;&#30340;&#38169;&#35823;&#25551;&#36848;&#25110;&#38169;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#22312;&#22788;&#29702;&#21508;&#31181;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#26041;&#38754;&#23637;&#31034;&#20102;&#39640;&#36229;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;MLLM&#22522;&#20934;&#27979;&#35797;&#20027;&#35201;&#29992;&#20110;&#35780;&#20272;&#22522;&#20110;&#21333;&#20010;&#22270;&#20687;&#30340;&#38745;&#24577;&#20449;&#24687;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#32780;&#29616;&#20195;MLLM&#22312;&#20174;&#22270;&#20687;&#24207;&#21015;&#20013;&#36827;&#34892;&#25512;&#26029;&#30340;&#33021;&#21147;&#65292;&#22312;&#29702;&#35299;&#19981;&#26029;&#21464;&#21270;&#30340;&#19990;&#30028;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#21364;&#34987;&#36739;&#23569;&#30740;&#31350;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;Mementos&#65292;&#29992;&#20110;&#35780;&#20272;MLLM&#30340;&#24207;&#21015;&#22270;&#20687;&#25512;&#29702;&#33021;&#21147;&#12290;Mementos&#21253;&#25324;4761&#20010;&#20855;&#26377;&#19981;&#21516;&#38271;&#24230;&#30340;&#22810;&#26679;&#30340;&#22270;&#20687;&#24207;&#21015;&#12290;&#25105;&#20204;&#36824;&#37319;&#29992;&#20102;GPT-4&#36741;&#21161;&#26041;&#27861;&#26469;&#35780;&#20272;MLLM&#30340;&#25512;&#29702;&#24615;&#33021;&#12290;&#36890;&#36807;&#23545;Mementos&#20013;&#21253;&#25324;GPT-4V&#21644;Gemini&#22312;&#20869;&#30340;&#20061;&#20010;&#26368;&#26032;MLLM&#36827;&#34892;&#20180;&#32454;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#23427;&#20204;&#22312;&#20934;&#30830;&#25551;&#36848;&#25152;&#32473;&#22270;&#20687;&#24207;&#21015;&#30340;&#21160;&#24577;&#20449;&#24687;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#24448;&#24448;&#23548;&#33268;&#23545;&#35937;&#21450;&#20854;&#23545;&#24212;&#34892;&#20026;&#30340;&#38169;&#35823;&#25551;&#36848;&#25110;&#38169;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal Large Language Models (MLLMs) have demonstrated proficiency in handling a variety of visual-language tasks. However, current MLLM benchmarks are predominantly designed to evaluate reasoning based on static information about a single image, and the ability of modern MLLMs to extrapolate from image sequences, which is essential for understanding our ever-changing world, has been less investigated. To address this challenge, this paper introduces Mementos, a new benchmark designed to assess MLLMs' sequential image reasoning abilities. Mementos features 4,761 diverse image sequences with varying lengths. We also employ a GPT-4 assisted method to evaluate MLLM reasoning performance. Through a careful evaluation of nine recent MLLMs on Mementos, including GPT-4V and Gemini, we find that they struggle to accurately describe dynamic information about given image sequences, often leading to hallucinations/misrepresentations of objects and their corresponding behaviors. Our quantitati
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25506;&#32034;&#31639;&#27861;&#65292;&#31216;&#20026;&#20998;&#24067;&#24335;&#38543;&#26426;&#32593;&#32476;&#33976;&#39311;&#65288;DRND&#65289;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#33976;&#39311;&#38543;&#26426;&#32593;&#32476;&#30340;&#20998;&#24067;&#21644;&#38544;&#24335;&#34701;&#20837;&#20266;&#35745;&#25968;&#26469;&#25913;&#36827;&#22870;&#21169;&#20998;&#37197;&#30340;&#31934;&#24230;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#25506;&#32034;&#36807;&#31243;&#12290;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#32467;&#26524;&#22343;&#34920;&#26126;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.09750</link><description>&lt;p&gt;
&#20351;&#29992;&#20998;&#24067;&#24335;&#38543;&#26426;&#32593;&#32476;&#33976;&#39311;&#30340;&#25506;&#32034;&#21644;&#21453;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Exploration and Anti-Exploration with Distributional Random Network Distillation. (arXiv:2401.09750v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09750
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25506;&#32034;&#31639;&#27861;&#65292;&#31216;&#20026;&#20998;&#24067;&#24335;&#38543;&#26426;&#32593;&#32476;&#33976;&#39311;&#65288;DRND&#65289;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#33976;&#39311;&#38543;&#26426;&#32593;&#32476;&#30340;&#20998;&#24067;&#21644;&#38544;&#24335;&#34701;&#20837;&#20266;&#35745;&#25968;&#26469;&#25913;&#36827;&#22870;&#21169;&#20998;&#37197;&#30340;&#31934;&#24230;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#25506;&#32034;&#36807;&#31243;&#12290;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#32467;&#26524;&#22343;&#34920;&#26126;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#25506;&#32034;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65292;&#23545;&#20110;&#19968;&#20010;&#26234;&#33021;&#20307;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#21462;&#24471;&#39640;&#22238;&#25253;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#30446;&#21069;&#30340;&#25506;&#32034;&#38543;&#26426;&#32593;&#32476;&#33976;&#39311;&#65288;Random Network Distillation&#65292;RND&#65289;&#31639;&#27861;&#24050;&#22312;&#35768;&#22810;&#29615;&#22659;&#20013;&#35777;&#26126;&#26377;&#25928;&#65292;&#20294;&#23427;&#22312;&#22870;&#21169;&#20998;&#37197;&#19978;&#24448;&#24448;&#38656;&#35201;&#26356;&#39640;&#30340;&#21306;&#20998;&#33021;&#21147;&#12290;&#26412;&#25991;&#31361;&#20986;&#20102;RND&#20013;&#30340;&#8220;&#22870;&#21169;&#19981;&#19968;&#33268;&#8221;&#38382;&#39064;&#65292;&#24182;&#25351;&#20986;&#20102;&#20854;&#20027;&#35201;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20998;&#24067;&#24335;RND&#65288;DRND&#65289;&#65292;&#23427;&#26159;RND&#30340;&#19968;&#20010;&#21464;&#20307;&#12290;DRND&#36890;&#36807;&#33976;&#39311;&#38543;&#26426;&#32593;&#32476;&#30340;&#20998;&#24067;&#24182;&#38544;&#24335;&#22320;&#34701;&#20837;&#20266;&#35745;&#25968;&#26469;&#25913;&#36827;&#22870;&#21169;&#20998;&#37197;&#30340;&#31934;&#24230;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#25506;&#32034;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#32531;&#35299;&#20102;&#19981;&#19968;&#33268;&#38382;&#39064;&#65292;&#32780;&#19981;&#20250;&#24341;&#20837;&#26174;&#33879;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#32467;&#26524;&#22343;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploration remains a critical issue in deep reinforcement learning for an agent to attain high returns in unknown environments. Although the prevailing exploration Random Network Distillation (RND) algorithm has been demonstrated to be effective in numerous environments, it often needs more discriminative power in bonus allocation. This paper highlights the ``bonus inconsistency'' issue within RND, pinpointing its primary limitation. To address this issue, we introduce the Distributional RND (DRND), a derivative of the RND. DRND enhances the exploration process by distilling a distribution of random networks and implicitly incorporating pseudo counts to improve the precision of bonus allocation. This refinement encourages agents to engage in more extensive exploration. Our method effectively mitigates the inconsistency issue without introducing significant computational overhead. Both theoretical analysis and experimental results demonstrate the superiority of our approach over the or
&lt;/p&gt;</description></item><item><title>SymTC&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#33136;&#26894;MR&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;Transformer&#21644;CNN&#30456;&#32467;&#21512;&#65292;&#24182;&#21033;&#29992;&#20301;&#32622;&#23884;&#20837;&#21644;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#30340;&#23454;&#20363;&#20998;&#21106;&#12290;</title><link>http://arxiv.org/abs/2401.09627</link><description>&lt;p&gt;
SymTC:&#19968;&#31181;&#29992;&#20110;&#33136;&#26894;MRI&#23454;&#20363;&#20998;&#21106;&#30340;&#20849;&#29983;Transformer-CNN&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
SymTC: A Symbiotic Transformer-CNN Net for Instance Segmentation of Lumbar Spine MRI. (arXiv:2401.09627v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09627
&lt;/p&gt;
&lt;p&gt;
SymTC&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#33136;&#26894;MR&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;Transformer&#21644;CNN&#30456;&#32467;&#21512;&#65292;&#24182;&#21033;&#29992;&#20301;&#32622;&#23884;&#20837;&#21644;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#30340;&#23454;&#20363;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26894;&#38388;&#30424;&#30142;&#30149;&#26159;&#19968;&#31181;&#24120;&#35265;&#30149;&#30151;&#65292;&#32463;&#24120;&#23548;&#33268;&#38388;&#27463;&#24615;&#25110;&#25345;&#32493;&#24615;&#30340;&#33136;&#32972;&#30140;&#30171;&#65292;&#23545;&#35813;&#30142;&#30149;&#30340;&#35786;&#26029;&#21644;&#35780;&#20272;&#20381;&#36182;&#20110;&#33136;&#26894;MR&#22270;&#20687;&#20013;&#26894;&#39592;&#21644;&#26894;&#38388;&#30424;&#20960;&#20309;&#24418;&#29366;&#30340;&#20934;&#30830;&#27979;&#37327;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#27169;&#22411;&#21487;&#20197;&#24110;&#21161;&#20020;&#24202;&#21307;&#29983;&#20197;&#26356;&#39640;&#25928;&#30340;&#26041;&#24335;&#33258;&#21160;&#21270;&#22320;&#23545;&#33136;&#26894;&#30340;&#20010;&#20307;&#23454;&#20363;&#65288;&#26894;&#39592;&#21644;&#26894;&#38388;&#30424;&#65289;&#36827;&#34892;&#22270;&#20687;&#20998;&#21106;&#65292;&#36825;&#34987;&#31216;&#20026;&#23454;&#20363;&#22270;&#20687;&#20998;&#21106;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SymTC&#65292;&#19968;&#31181;&#21019;&#26032;&#30340;&#33136;&#26894;MR&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#65292;&#23427;&#32467;&#21512;&#20102;Transformer&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30340;&#20248;&#21183;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#24182;&#34892;&#30340;&#21452;&#36335;&#24452;&#26550;&#26500;&#26469;&#34701;&#21512;CNN&#23618;&#21644;Transformer&#23618;&#65292;&#24182;&#22312;Transformer&#30340;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#20013;&#38598;&#25104;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20301;&#32622;&#23884;&#20837;&#65292;&#22686;&#24378;&#20102;&#20301;&#32622;&#20449;&#24687;&#30340;&#21033;&#29992;&#20197;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#20998;&#21106;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#23450;&#20301;&#31995;&#32479;&#36827;&#34892;&#27169;&#22411;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intervertebral disc disease, a prevalent ailment, frequently leads to intermittent or persistent low back pain, and diagnosing and assessing of this disease rely on accurate measurement of vertebral bone and intervertebral disc geometries from lumbar MR images. Deep neural network (DNN) models may assist clinicians with more efficient image segmentation of individual instances (disks and vertebrae) of the lumbar spine in an automated way, which is termed as instance image segmentation. In this work, we proposed SymTC, an innovative lumbar spine MR image segmentation model that combines the strengths of Transformer and Convolutional Neural Network (CNN). Specifically, we designed a parallel dual-path architecture to merge CNN layers and Transformer layers, and we integrated a novel position embedding into the self-attention module of Transformer, enhancing the utilization of positional information for more accurate segmentation. To further improves model performance, we introduced a new
&lt;/p&gt;</description></item><item><title>cedar&#26159;&#19968;&#20010;&#32534;&#31243;&#27169;&#22411;&#21644;&#26694;&#26550;&#65292;&#21487;&#20197;&#36731;&#26494;&#26500;&#24314;&#12289;&#20248;&#21270;&#21644;&#25191;&#34892;&#26426;&#22120;&#23398;&#20064;&#36755;&#20837;&#25968;&#25454;&#31649;&#36947;&#12290;&#23427;&#25552;&#20379;&#20102;&#26131;&#20110;&#20351;&#29992;&#30340;&#32534;&#31243;&#25509;&#21475;&#21644;&#21487;&#32452;&#21512;&#36816;&#31639;&#31526;&#65292;&#25903;&#25345;&#20219;&#24847;ML&#26694;&#26550;&#21644;&#24211;&#12290;&#36890;&#36807;&#35299;&#20915;&#24403;&#21069;&#36755;&#20837;&#25968;&#25454;&#31995;&#32479;&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;&#24615;&#33021;&#20248;&#21270;&#30340;&#38382;&#39064;&#65292;cedar&#25552;&#39640;&#20102;&#36164;&#28304;&#21033;&#29992;&#25928;&#29575;&#65292;&#28385;&#36275;&#20102;&#24222;&#22823;&#25968;&#25454;&#37327;&#21644;&#39640;&#35757;&#32451;&#21534;&#21520;&#37327;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2401.08895</link><description>&lt;p&gt;
cedar&#65306;&#21487;&#32452;&#21512;&#21644;&#20248;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#36755;&#20837;&#25968;&#25454;&#31649;&#36947;
&lt;/p&gt;
&lt;p&gt;
cedar: Composable and Optimized Machine Learning Input Data Pipelines. (arXiv:2401.08895v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08895
&lt;/p&gt;
&lt;p&gt;
cedar&#26159;&#19968;&#20010;&#32534;&#31243;&#27169;&#22411;&#21644;&#26694;&#26550;&#65292;&#21487;&#20197;&#36731;&#26494;&#26500;&#24314;&#12289;&#20248;&#21270;&#21644;&#25191;&#34892;&#26426;&#22120;&#23398;&#20064;&#36755;&#20837;&#25968;&#25454;&#31649;&#36947;&#12290;&#23427;&#25552;&#20379;&#20102;&#26131;&#20110;&#20351;&#29992;&#30340;&#32534;&#31243;&#25509;&#21475;&#21644;&#21487;&#32452;&#21512;&#36816;&#31639;&#31526;&#65292;&#25903;&#25345;&#20219;&#24847;ML&#26694;&#26550;&#21644;&#24211;&#12290;&#36890;&#36807;&#35299;&#20915;&#24403;&#21069;&#36755;&#20837;&#25968;&#25454;&#31995;&#32479;&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;&#24615;&#33021;&#20248;&#21270;&#30340;&#38382;&#39064;&#65292;cedar&#25552;&#39640;&#20102;&#36164;&#28304;&#21033;&#29992;&#25928;&#29575;&#65292;&#28385;&#36275;&#20102;&#24222;&#22823;&#25968;&#25454;&#37327;&#21644;&#39640;&#35757;&#32451;&#21534;&#21520;&#37327;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36755;&#20837;&#25968;&#25454;&#31649;&#36947;&#26159;&#27599;&#20010;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#35757;&#32451;&#20219;&#21153;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#23427;&#36127;&#36131;&#35835;&#21462;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20351;&#29992;&#22797;&#26434;&#30340;&#21464;&#25442;&#22788;&#29702;&#26679;&#26412;&#25209;&#27425;&#65292;&#24182;&#20197;&#20302;&#24310;&#36831;&#21644;&#39640;&#21534;&#21520;&#37327;&#23558;&#20854;&#21152;&#36733;&#21040;&#35757;&#32451;&#33410;&#28857;&#19978;&#12290;&#39640;&#24615;&#33021;&#30340;&#36755;&#20837;&#25968;&#25454;&#31995;&#32479;&#21464;&#24471;&#36234;&#26469;&#36234;&#20851;&#38190;&#65292;&#21407;&#22240;&#26159;&#25968;&#25454;&#37327;&#24613;&#21095;&#22686;&#21152;&#21644;&#35757;&#32451;&#21534;&#21520;&#37327;&#30340;&#35201;&#27714;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#36755;&#20837;&#25968;&#25454;&#31995;&#32479;&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;&#20851;&#38190;&#30340;&#24615;&#33021;&#20248;&#21270;&#65292;&#23548;&#33268;&#36164;&#28304;&#21033;&#29992;&#25928;&#29575;&#26497;&#20302;&#30340;&#22522;&#30784;&#35774;&#26045;&#65292;&#25110;&#32773;&#26356;&#31967;&#31957;&#22320;&#65292;&#28010;&#36153;&#26114;&#36149;&#30340;&#21152;&#36895;&#22120;&#12290;&#20026;&#20102;&#28385;&#36275;&#36825;&#20123;&#38656;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;cedar&#65292;&#19968;&#20010;&#32534;&#31243;&#27169;&#22411;&#21644;&#26694;&#26550;&#65292;&#20801;&#35768;&#29992;&#25143;&#36731;&#26494;&#26500;&#24314;&#12289;&#20248;&#21270;&#21644;&#25191;&#34892;&#36755;&#20837;&#25968;&#25454;&#31649;&#36947;&#12290;cedar&#25552;&#20379;&#20102;&#26131;&#20110;&#20351;&#29992;&#30340;&#32534;&#31243;&#25509;&#21475;&#65292;&#20801;&#35768;&#29992;&#25143;&#20351;&#29992;&#21487;&#32452;&#21512;&#36816;&#31639;&#31526;&#26469;&#23450;&#20041;&#25903;&#25345;&#20219;&#24847;ML&#26694;&#26550;&#21644;&#24211;&#30340;&#36755;&#20837;&#25968;&#25454;&#31649;&#36947;&#12290;
&lt;/p&gt;
&lt;p&gt;
The input data pipeline is an essential component of each machine learning (ML) training job. It is responsible for reading massive amounts of training data, processing batches of samples using complex of transformations, and loading them onto training nodes at low latency and high throughput. Performant input data systems are becoming increasingly critical, driven by skyrocketing data volumes and training throughput demands. Unfortunately, current input data systems cannot fully leverage key performance optimizations, resulting in hugely inefficient infrastructures that require significant resources -- or worse -- underutilize expensive accelerators.  To address these demands, we present cedar, a programming model and framework that allows users to easily build, optimize, and execute input data pipelines. cedar presents an easy-to-use programming interface, allowing users to define input data pipelines using composable operators that support arbitrary ML frameworks and libraries. Mean
&lt;/p&gt;</description></item><item><title>Shabari&#26159;&#19968;&#20010;&#24310;&#36831;&#20915;&#31574;&#30340;&#26080;&#26381;&#21153;&#22120;&#36164;&#28304;&#31649;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#20989;&#25968;&#36755;&#20837;&#30340;&#24310;&#36831;&#26469;&#20943;&#36731;&#26080;&#26381;&#21153;&#22120;&#31995;&#32479;&#20013;&#30340;&#24615;&#33021;&#21464;&#24322;&#24615;&#21644;&#36164;&#28304;&#20302;&#21033;&#29992;&#29575;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.08859</link><description>&lt;p&gt;
Shabari: &#24310;&#36831;&#20915;&#31574;&#20197;&#23454;&#29616;&#26356;&#24555;&#12289;&#26356;&#39640;&#25928;&#30340;&#26080;&#26381;&#21153;&#22120;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Shabari: Delayed Decision-Making for Faster and Efficient Serverless Function. (arXiv:2401.08859v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08859
&lt;/p&gt;
&lt;p&gt;
Shabari&#26159;&#19968;&#20010;&#24310;&#36831;&#20915;&#31574;&#30340;&#26080;&#26381;&#21153;&#22120;&#36164;&#28304;&#31649;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#20989;&#25968;&#36755;&#20837;&#30340;&#24310;&#36831;&#26469;&#20943;&#36731;&#26080;&#26381;&#21153;&#22120;&#31995;&#32479;&#20013;&#30340;&#24615;&#33021;&#21464;&#24322;&#24615;&#21644;&#36164;&#28304;&#20302;&#21033;&#29992;&#29575;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#26381;&#21153;&#22120;&#35745;&#31639;&#20943;&#36731;&#20102;&#24320;&#21457;&#20154;&#21592;&#23545;&#36164;&#28304;&#31649;&#29702;&#30340;&#36127;&#25285;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#20102;&#26131;&#29992;&#24615;&#65292;&#24182;&#20026;&#25552;&#20379;&#32773;&#20248;&#21270;&#36164;&#28304;&#21033;&#29992;&#29575;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;&#28982;&#32780;&#65292;&#20170;&#22825;&#30340;&#26080;&#26381;&#21153;&#22120;&#31995;&#32479;&#22312;&#20989;&#25968;&#35843;&#29992;&#26041;&#38754;&#32570;&#20047;&#24615;&#33021;&#20445;&#35777;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23545;&#24615;&#33021;&#20851;&#38190;&#24212;&#29992;&#31243;&#24207;&#30340;&#25903;&#25345;&#65306;&#25105;&#20204;&#35266;&#23519;&#21040;&#20005;&#37325;&#30340;&#24615;&#33021;&#21464;&#24322;&#24615;&#65288;&#39640;&#36798;6&#20493;&#65289;&#12290;&#25552;&#20379;&#32773;&#32570;&#20047;&#23545;&#29992;&#25143;&#20989;&#25968;&#30340;&#21487;&#35265;&#24615;&#65292;&#22240;&#27492;&#24456;&#38590;&#23545;&#20854;&#36827;&#34892;&#21512;&#36866;&#30340;&#36164;&#28304;&#35268;&#27169;&#21270;&#65306;&#25105;&#20204;&#35266;&#23519;&#21040;&#20005;&#37325;&#30340;&#36164;&#28304;&#20302;&#21033;&#29992;&#29575;&#65288;&#39640;&#36798;80%&#65289;&#12290;&#20026;&#20102;&#29702;&#35299;&#24615;&#33021;&#21464;&#24322;&#24615;&#21644;&#20302;&#21033;&#29992;&#29575;&#32972;&#21518;&#30340;&#21407;&#22240;&#65292;&#25105;&#20204;&#23545;&#24120;&#35265;&#37096;&#32626;&#30340;&#26080;&#26381;&#21153;&#22120;&#20989;&#25968;&#36827;&#34892;&#20102;&#27979;&#37327;&#30740;&#31350;&#65292;&#24182;&#20102;&#35299;&#21040;&#20989;&#25968;&#24615;&#33021;&#21644;&#36164;&#28304;&#21033;&#29992;&#29575;&#20851;&#38190;&#21462;&#20915;&#20110;&#20989;&#25968;&#35821;&#20041;&#21644;&#36755;&#20837;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#35748;&#35782;&#26159;&#22312;&#20989;&#25968;&#36755;&#20837;&#21487;&#29992;&#20043;&#21518;&#24310;&#36831;&#36164;&#28304;&#20998;&#37197;&#20915;&#31574;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;Shabari&#65292;&#19968;&#20010;&#29992;&#20110;&#26080;&#26381;&#21153;&#22120;&#30340;&#36164;&#28304;&#31649;&#29702;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Serverless computing relieves developers from the burden of resource management, thus providing ease-of-use to the users and the opportunity to optimize resource utilization for the providers. However, today's serverless systems lack performance guarantees for function invocations, thus limiting support for performance-critical applications: we observed severe performance variability (up to 6x). Providers lack visibility into user functions and hence find it challenging to right-size them: we observed heavy resource underutilization (up to 80%). To understand the causes behind the performance variability and underutilization, we conducted a measurement study of commonly deployed serverless functions and learned that the function performance and resource utilization depend crucially on function semantics and inputs. Our key insight is to delay making resource allocation decisions until after the function inputs are available. We introduce Shabari, a resource management framework for ser
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Decoupled Prototype Learning (DPL)&#30340;&#35299;&#32806;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26679;&#26412;&#21407;&#22411;&#20026;&#20013;&#24515;&#30340;&#25439;&#22833;&#35745;&#31639;&#65292;&#26469;&#35299;&#20915;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;&#20013;&#22122;&#22768;&#20266;&#26631;&#31614;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.08703</link><description>&lt;p&gt;
&#21487;&#38752;&#30340;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;&#30340;&#35299;&#32806;&#26679;&#26412;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Decoupled Prototype Learning for Reliable Test-Time Adaptation. (arXiv:2401.08703v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08703
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Decoupled Prototype Learning (DPL)&#30340;&#35299;&#32806;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26679;&#26412;&#21407;&#22411;&#20026;&#20013;&#24515;&#30340;&#25439;&#22833;&#35745;&#31639;&#65292;&#26469;&#35299;&#20915;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;&#20013;&#22122;&#22768;&#20266;&#26631;&#31614;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;&#65288;TTA&#65289;&#26159;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#25345;&#32493;&#23558;&#39044;&#35757;&#32451;&#30340;&#28304;&#27169;&#22411;&#36866;&#24212;&#21040;&#30446;&#26631;&#22495;&#30340;&#20219;&#21153;&#12290;&#19968;&#31181;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#20272;&#35745;&#30340;&#20266;&#26631;&#31614;&#20351;&#29992;&#20132;&#21449;&#29109;&#25439;&#22833;&#24494;&#35843;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#35813;&#26041;&#27861;&#30340;&#24615;&#33021;&#24456;&#23481;&#26131;&#21463;&#21040;&#22122;&#22768;&#20266;&#26631;&#31614;&#30340;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#23558;&#27599;&#20010;&#26679;&#26412;&#30340;&#20998;&#31867;&#38169;&#35823;&#26368;&#23567;&#21270;&#20250;&#20351;&#20132;&#21449;&#29109;&#25439;&#22833;&#23545;&#26631;&#31614;&#22122;&#22768;&#38750;&#24120;&#25935;&#24863;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#32806;&#26679;&#26412;&#23398;&#20064;&#65288;DPL&#65289;&#26041;&#27861;&#65292;&#37319;&#29992;&#26679;&#26412;&#21407;&#22411;&#20026;&#20013;&#24515;&#30340;&#25439;&#22833;&#35745;&#31639;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35299;&#32806;&#20102;&#31867;&#21407;&#22411;&#30340;&#20248;&#21270;&#12290;&#23545;&#20110;&#27599;&#20010;&#31867;&#21407;&#22411;&#65292;&#25105;&#20204;&#37319;&#29992;&#23545;&#27604;&#26041;&#24335;&#20943;&#23567;&#20854;&#19982;&#27491;&#26679;&#26412;&#30340;&#36317;&#31163;&#65292;&#24182;&#22686;&#21152;&#20854;&#19982;&#36127;&#26679;&#26412;&#30340;&#36317;&#31163;&#12290;&#36825;&#31181;&#31574;&#30053;&#21487;&#20197;&#38450;&#27490;&#27169;&#22411;&#23545;&#22122;&#22768;&#20266;&#26631;&#31614;&#36807;&#25311;&#21512;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35760;&#24518;&#30340;&#31574;&#30053;&#65292;&#20197;&#22686;&#24378;DPL&#22312;TTA&#20013;&#32463;&#24120;&#36935;&#21040;&#30340;&#23567;&#25209;&#37327;&#24773;&#20917;&#19979;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#26356;&#26032;&#27599;&#20010;&#31867;&#21035;&#30340;&#21407;&#22411;&#26102;&#20351;&#29992;&#35760;&#24518;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Test-time adaptation (TTA) is a task that continually adapts a pre-trained source model to the target domain during inference. One popular approach involves fine-tuning model with cross-entropy loss according to estimated pseudo-labels. However, its performance is significantly affected by noisy pseudo-labels. This study reveals that minimizing the classification error of each sample causes the cross-entropy loss's vulnerability to label noise. To address this issue, we propose a novel Decoupled Prototype Learning (DPL) method that features prototype-centric loss computation. First, we decouple the optimization of class prototypes. For each class prototype, we reduce its distance with positive samples and enlarge its distance with negative samples in a contrastive manner. This strategy prevents the model from overfitting to noisy pseudo-labels. Second, we propose a memory-based strategy to enhance DPL's robustness for the small batch sizes often encountered in TTA. We update each class
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#65288;SAiD&#65289;&#39537;&#21160;&#30340;&#35821;&#38899;&#39537;&#21160;&#30340;&#19977;&#32500;&#38754;&#37096;&#21160;&#30011;&#26041;&#27861;&#65292;&#36890;&#36807;&#36731;&#37327;&#32423;&#30340;Transformer-based U-Net&#27169;&#22411;&#21644;&#38899;&#39057;&#19982;&#35270;&#35273;&#30340;&#20132;&#21449;&#27169;&#24577;&#23545;&#40784;&#20559;&#24046;&#65292;&#23454;&#29616;&#20102;&#36739;&#22909;&#30340;&#21767;&#37096;&#21516;&#27493;&#21644;&#26356;&#22810;&#26679;&#21270;&#30340;&#21767;&#37096;&#36816;&#21160;&#12290;</title><link>http://arxiv.org/abs/2401.08655</link><description>&lt;p&gt;
SAiD: &#20351;&#29992;&#25193;&#25955;&#26041;&#27861;&#39537;&#21160;&#30340;&#35821;&#38899;&#39537;&#21160;&#34920;&#24773;&#21160;&#30011;
&lt;/p&gt;
&lt;p&gt;
SAiD: Speech-driven Blendshape Facial Animation with Diffusion. (arXiv:2401.08655v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08655
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#65288;SAiD&#65289;&#39537;&#21160;&#30340;&#35821;&#38899;&#39537;&#21160;&#30340;&#19977;&#32500;&#38754;&#37096;&#21160;&#30011;&#26041;&#27861;&#65292;&#36890;&#36807;&#36731;&#37327;&#32423;&#30340;Transformer-based U-Net&#27169;&#22411;&#21644;&#38899;&#39057;&#19982;&#35270;&#35273;&#30340;&#20132;&#21449;&#27169;&#24577;&#23545;&#40784;&#20559;&#24046;&#65292;&#23454;&#29616;&#20102;&#36739;&#22909;&#30340;&#21767;&#37096;&#21516;&#27493;&#21644;&#26356;&#22810;&#26679;&#21270;&#30340;&#21767;&#37096;&#36816;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#36827;&#34892;&#20102;&#22823;&#37327;&#30740;&#31350;&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#22823;&#35268;&#27169;&#30340;&#35270;&#21548;&#25968;&#25454;&#38598;&#65292;&#35821;&#38899;&#39537;&#21160;&#30340;&#19977;&#32500;&#38754;&#37096;&#21160;&#30011;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22823;&#22810;&#25968;&#36807;&#21435;&#30340;&#24037;&#20316;&#36890;&#24120;&#37319;&#29992;&#26368;&#23567;&#20108;&#20056;&#27861;&#22312;&#23567;&#25968;&#25454;&#38598;&#19978;&#23398;&#20064;&#22238;&#24402;&#27169;&#22411;&#65292;&#20294;&#22312;&#20174;&#35821;&#38899;&#29983;&#25104;&#21508;&#31181;&#21767;&#37096;&#21160;&#20316;&#26041;&#38754;&#36935;&#21040;&#22256;&#38590;&#65292;&#24182;&#19988;&#38656;&#35201;&#22823;&#37327;&#31934;&#32454;&#35843;&#25972;&#29983;&#25104;&#30340;&#36755;&#20986;&#32467;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#65288;SAiD&#65289;&#39537;&#21160;&#30340;&#35821;&#38899;&#39537;&#21160;&#30340;&#19977;&#32500;&#38754;&#37096;&#21160;&#30011;&#65292;&#36825;&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#22522;&#20110;Transformer&#30340;U-Net&#27169;&#22411;&#65292;&#20855;&#26377;&#38899;&#39057;&#21644;&#35270;&#35273;&#20043;&#38388;&#30340;&#20132;&#21449;&#27169;&#24577;&#23545;&#40784;&#20559;&#24046;&#65292;&#20197;&#22686;&#24378;&#21767;&#37096;&#21516;&#27493;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;BlendVOCA&#65292;&#36825;&#26159;&#19968;&#31181;&#35821;&#38899;&#38899;&#39057;&#21644;&#28151;&#21512;&#24418;&#29366;&#38754;&#37096;&#27169;&#22411;&#21442;&#25968;&#23545;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20197;&#35299;&#20915;&#20844;&#20849;&#36164;&#28304;&#30340;&#32570;&#20047;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#21767;&#37096;&#21516;&#27493;&#26041;&#38754;&#36798;&#21040;&#20102;&#19982;&#22522;&#32447;&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#30830;&#20445;&#20102;&#26356;&#22810;&#26679;&#21270;&#30340;&#21767;&#37096;&#36816;&#21160;&#65292;&#24182;&#31616;&#21270;&#20102;&#21160;&#30011;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech-driven 3D facial animation is challenging due to the scarcity of large-scale visual-audio datasets despite extensive research. Most prior works, typically focused on learning regression models on a small dataset using the method of least squares, encounter difficulties generating diverse lip movements from speech and require substantial effort in refining the generated outputs. To address these issues, we propose a speech-driven 3D facial animation with a diffusion model (SAiD), a lightweight Transformer-based U-Net with a cross-modality alignment bias between audio and visual to enhance lip synchronization. Moreover, we introduce BlendVOCA, a benchmark dataset of pairs of speech audio and parameters of a blendshape facial model, to address the scarcity of public resources. Our experimental results demonstrate that the proposed approach achieves comparable or superior performance in lip synchronization to baselines, ensures more diverse lip movements, and streamlines the animati
&lt;/p&gt;</description></item><item><title>DiConStruct&#26159;&#19968;&#31181;&#22522;&#20110;&#40657;&#30418;&#27169;&#22411;&#30340;&#22240;&#26524;&#27010;&#24565;&#35299;&#37322;&#26041;&#27861;&#65292;&#36890;&#36807;&#21019;&#24314;&#32467;&#26500;&#24615;&#22240;&#26524;&#27169;&#22411;&#21644;&#27010;&#24565;&#24402;&#22240;&#26041;&#24335;&#25552;&#20379;&#26356;&#20855;&#21487;&#35299;&#37322;&#24615;&#30340;&#23616;&#37096;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2401.08534</link><description>&lt;p&gt;
DiConStruct: &#22522;&#20110;&#40657;&#30418;&#31934;&#21326;&#30340;&#22240;&#26524;&#27010;&#24565;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
DiConStruct: Causal Concept-based Explanations through Black-Box Distillation. (arXiv:2401.08534v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08534
&lt;/p&gt;
&lt;p&gt;
DiConStruct&#26159;&#19968;&#31181;&#22522;&#20110;&#40657;&#30418;&#27169;&#22411;&#30340;&#22240;&#26524;&#27010;&#24565;&#35299;&#37322;&#26041;&#27861;&#65292;&#36890;&#36807;&#21019;&#24314;&#32467;&#26500;&#24615;&#22240;&#26524;&#27169;&#22411;&#21644;&#27010;&#24565;&#24402;&#22240;&#26041;&#24335;&#25552;&#20379;&#26356;&#20855;&#21487;&#35299;&#37322;&#24615;&#30340;&#23616;&#37096;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#22312;&#20154;&#24037;&#26234;&#33021;&#20915;&#31574;&#31995;&#32479;&#20013;&#36215;&#30528;&#26680;&#24515;&#20316;&#29992;&#12290;&#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#35299;&#37322;&#24212;&#35813;&#20351;&#29992;&#20154;&#21487;&#35299;&#37322;&#30340;&#35821;&#20041;&#27010;&#24565;&#26469;&#34920;&#36798;&#12290;&#27492;&#22806;&#65292;&#35299;&#37322;&#22120;&#24212;&#35813;&#25429;&#25417;&#36825;&#20123;&#27010;&#24565;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#20197;&#20415;&#23545;&#35299;&#37322;&#36827;&#34892;&#25512;&#29702;&#12290;&#26368;&#21518;&#65292;&#35299;&#37322;&#26041;&#27861;&#24212;&#35813;&#39640;&#25928;&#65292;&#24182;&#19981;&#25439;&#23475;&#39044;&#27979;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;AI&#35299;&#37322;&#24615;&#21462;&#24471;&#20102;&#24555;&#36895;&#36827;&#23637;&#65292;&#20294;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#33267;&#20170;&#27809;&#26377;&#19968;&#31181;&#26041;&#27861;&#28385;&#36275;&#36825;&#19977;&#20010;&#26465;&#20214;&#12290;&#20107;&#23454;&#19978;&#65292;&#20027;&#27969;&#30340;&#23616;&#37096;&#27010;&#24565;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#19981;&#20135;&#29983;&#22240;&#26524;&#35299;&#37322;&#65292;&#24182;&#22312;&#35299;&#37322;&#24615;&#21644;&#39044;&#27979;&#24615;&#33021;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;DiConStruct&#65292;&#19968;&#31181;&#26082;&#22522;&#20110;&#27010;&#24565;&#21448;&#20855;&#26377;&#22240;&#26524;&#24615;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#32467;&#26500;&#24615;&#22240;&#26524;&#27169;&#22411;&#21644;&#27010;&#24565;&#24402;&#22240;&#26041;&#24335;&#21019;&#24314;&#26356;&#20855;&#21487;&#35299;&#37322;&#24615;&#30340;&#23616;&#37096;&#35299;&#37322;&#12290;&#25105;&#20204;&#30340;&#35299;&#37322;&#22120;&#20316;&#20026;&#19968;&#20010;&#31934;&#21326;&#27169;&#22411;&#36866;&#29992;&#20110;&#20219;&#20309;&#40657;&#30418;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model interpretability plays a central role in human-AI decision-making systems. Ideally, explanations should be expressed using human-interpretable semantic concepts. Moreover, the causal relations between these concepts should be captured by the explainer to allow for reasoning about the explanations. Lastly, explanation methods should be efficient and not compromise the performance of the predictive task. Despite the rapid advances in AI explainability in recent years, as far as we know to date, no method fulfills these three properties. Indeed, mainstream methods for local concept explainability do not produce causal explanations and incur a trade-off between explainability and prediction performance. We present DiConStruct, an explanation method that is both concept-based and causal, with the goal of creating more interpretable local explanations in the form of structural causal models and concept attributions. Our explainer works as a distillation model to any black-box machine l
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#30740;&#31350;&#20102;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#30340;&#38598;&#25104;&#21040;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20197;&#35299;&#20915;&#20854;&#20135;&#29983;&#19981;&#21487;&#21462;&#20869;&#23481;&#30340;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#28165;&#27905;&#39046;&#22495;&#20013;&#26377;&#25928;&#20943;&#23569;&#26377;&#23475;&#20869;&#23481;&#29983;&#25104;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.08491</link><description>&lt;p&gt;
&#23545;&#27604;&#22256;&#24785;&#24230;&#22312;&#21463;&#25511;&#29983;&#25104;&#20013;&#30340;&#24212;&#29992;&#65306;&#28165;&#27905;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Contrastive Perplexity for Controlled Generation: An Application in Detoxifying Large Language Models. (arXiv:2401.08491v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08491
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#30740;&#31350;&#20102;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#30340;&#38598;&#25104;&#21040;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20197;&#35299;&#20915;&#20854;&#20135;&#29983;&#19981;&#21487;&#21462;&#20869;&#23481;&#30340;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#28165;&#27905;&#39046;&#22495;&#20013;&#26377;&#25928;&#20943;&#23569;&#26377;&#23475;&#20869;&#23481;&#29983;&#25104;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20135;&#29983;&#19981;&#21487;&#21462;&#21644;&#20107;&#23454;&#19981;&#27491;&#30830;&#30340;&#20869;&#23481;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#26159;&#19968;&#20010;&#25361;&#25112;&#21644;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#30340;&#38598;&#25104;&#65292;&#29992;&#20110;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#20197;&#36827;&#34892;&#38544;&#24335;&#30693;&#35782;&#32534;&#36753;&#21644;&#21463;&#25511;&#25991;&#26412;&#29983;&#25104;&#12290;&#36890;&#36807;&#23545;&#27604;&#26041;&#24335;&#20248;&#21270;&#35757;&#32451;&#30446;&#26631;&#65292;&#21363;&#23545;&#40784;&#25991;&#26412;&#30340;&#22256;&#24785;&#24230;&#12290;&#20026;&#20102;&#20197;&#33258;&#30417;&#30563;&#30340;&#26041;&#24335;&#35757;&#32451;&#27169;&#22411;&#65292;&#25105;&#20204;&#21033;&#29992;&#29616;&#25104;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#28165;&#27905;&#39046;&#22495;&#30340;&#36866;&#29992;&#24615;&#12290;&#22312;&#27492;&#36807;&#31243;&#20013;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26174;&#33879;&#20943;&#23569;&#20102;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#30340;&#25968;&#37327;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#23545;&#20110;&#24120;&#35782;&#25512;&#29702;&#21644;&#38405;&#35835;&#29702;&#35299;&#31561;&#19979;&#28216;&#20219;&#21153;&#30340;&#23454;&#29992;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#27010;&#24565;&#19978;&#31616;&#21333;&#20294;&#32463;&#39564;&#19978;&#24378;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;
The generation of undesirable and factually incorrect content of large language models poses a significant challenge and remains largely an unsolved issue. This paper studies the integration of a contrastive learning objective for fine-tuning LLMs for implicit knowledge editing and controlled text generation. Optimizing the training objective entails aligning text perplexities in a contrastive fashion. To facilitate training the model in a self-supervised fashion, we leverage an off-the-shelf LLM for training data generation. We showcase applicability in the domain of detoxification. Herein, the proposed approach leads to a significant decrease in the generation of toxic content while preserving general utility for downstream tasks such as commonsense reasoning and reading comprehension. The proposed approach is conceptually simple but empirically powerful.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25506;&#35752;&#20102;&#20869;&#23481;&#25512;&#33616;&#31995;&#32479;&#21487;&#33021;&#23545;&#29992;&#25143;&#20135;&#29983;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#24182;&#25351;&#20986;&#36825;&#31181;&#24433;&#21709;&#19981;&#20165;&#21487;&#33021;&#30001;&#24179;&#21488;&#30446;&#26631;&#19982;&#29992;&#25143;&#31119;&#21033;&#19981;&#19968;&#33268;&#24341;&#36215;&#65292;&#36824;&#21487;&#33021;&#30001;&#23398;&#20064;&#31639;&#27861;&#23545;&#19981;&#21516;&#20869;&#23481;&#30340;&#21453;&#39304;&#29575;&#24046;&#24322;&#36896;&#25104;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#22810;&#33218;&#36172;&#21338;&#26426;&#26694;&#26550;&#21644;&#27010;&#29575;&#21453;&#39304;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.05304</link><description>&lt;p&gt;
&#33021;&#21542;&#29992;&#27010;&#29575;&#21453;&#39304;&#25512;&#21160;&#22312;&#32447;&#24179;&#21488;&#23545;&#29992;&#25143;&#20135;&#29983;&#24433;&#21709;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Probabilistic Feedback Drive User Impacts in Online Platforms?. (arXiv:2401.05304v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05304
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25506;&#35752;&#20102;&#20869;&#23481;&#25512;&#33616;&#31995;&#32479;&#21487;&#33021;&#23545;&#29992;&#25143;&#20135;&#29983;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#24182;&#25351;&#20986;&#36825;&#31181;&#24433;&#21709;&#19981;&#20165;&#21487;&#33021;&#30001;&#24179;&#21488;&#30446;&#26631;&#19982;&#29992;&#25143;&#31119;&#21033;&#19981;&#19968;&#33268;&#24341;&#36215;&#65292;&#36824;&#21487;&#33021;&#30001;&#23398;&#20064;&#31639;&#27861;&#23545;&#19981;&#21516;&#20869;&#23481;&#30340;&#21453;&#39304;&#29575;&#24046;&#24322;&#36896;&#25104;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#22810;&#33218;&#36172;&#21338;&#26426;&#26694;&#26550;&#21644;&#27010;&#29575;&#21453;&#39304;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24120;&#35265;&#30340;&#35299;&#37322;&#26159;&#20869;&#23481;&#25512;&#33616;&#31995;&#32479;&#23545;&#29992;&#25143;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#26159;&#30001;&#20110;&#24179;&#21488;&#30446;&#26631;&#19982;&#29992;&#25143;&#31119;&#21033;&#20043;&#38388;&#30340;&#19981;&#23545;&#40784;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24179;&#21488;&#30446;&#26631;&#19981;&#19968;&#33268;&#24182;&#19981;&#26159;&#23545;&#29992;&#25143;&#20135;&#29983;&#24847;&#22806;&#24433;&#21709;&#30340;&#21807;&#19968;&#28508;&#22312;&#21407;&#22240;&#65306;&#21363;&#20351;&#24179;&#21488;&#30446;&#26631;&#23436;&#20840;&#19982;&#29992;&#25143;&#31119;&#21033;&#19968;&#33268;&#65292;&#23398;&#20064;&#31639;&#27861;&#20063;&#21487;&#33021;&#23545;&#29992;&#25143;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#36825;&#20123;&#29992;&#25143;&#24433;&#21709;&#30340;&#26469;&#28304;&#26159;&#19981;&#21516;&#20869;&#23481;&#21487;&#33021;&#20197;&#19981;&#21516;&#30340;&#36895;&#29575;&#20135;&#29983;&#21487;&#35266;&#23519;&#30340;&#29992;&#25143;&#21453;&#24212;&#65288;&#21453;&#39304;&#20449;&#24687;&#65289;&#65307;&#36825;&#20123;&#21453;&#39304;&#36895;&#29575;&#21487;&#33021;&#19982;&#24433;&#21709;&#29992;&#25143;&#20307;&#39564;&#30340;&#20869;&#23481;&#23646;&#24615;&#65288;&#22914;&#20105;&#35758;&#24615;&#25110;&#21019;&#20316;&#32773;&#30340;&#20154;&#21475;&#30456;&#20284;&#24230;&#65289;&#30456;&#20851;&#12290;&#30001;&#20110;&#21453;&#39304;&#36895;&#29575;&#30340;&#24046;&#24322;&#21487;&#33021;&#20250;&#24433;&#21709;&#23398;&#20064;&#31639;&#27861;&#19982;&#19981;&#21516;&#20869;&#23481;&#30340;&#20132;&#20114;&#39057;&#29575;&#65292;&#23398;&#20064;&#31639;&#27861;&#21487;&#33021;&#20250;&#26080;&#24847;&#20013;&#25512;&#24191;&#20855;&#26377;&#26576;&#20123;&#29305;&#23450;&#23646;&#24615;&#30340;&#20869;&#23481;&#12290;&#20351;&#29992;&#22810;&#33218;&#36172;&#21338;&#26426;&#26694;&#26550;&#19982;&#27010;&#29575;&#21453;&#39304;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
A common explanation for negative user impacts of content recommender systems is misalignment between the platform's objective and user welfare. In this work, we show that misalignment in the platform's objective is not the only potential cause of unintended impacts on users: even when the platform's objective is fully aligned with user welfare, the platform's learning algorithm can induce negative downstream impacts on users. The source of these user impacts is that different pieces of content may generate observable user reactions (feedback information) at different rates; these feedback rates may correlate with content properties, such as controversiality or demographic similarity of the creator, that affect the user experience. Since differences in feedback rates can impact how often the learning algorithm engages with different content, the learning algorithm may inadvertently promote content with certain such properties. Using the multi-armed bandit framework with probabilistic f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#26368;&#22823;&#21270;&#27169;&#22411;&#25910;&#25947;&#36895;&#29575;&#30340;Split Learning&#32908;&#30005;&#20551;&#32930;&#25511;&#21046;&#20013;&#30340;&#20999;&#23618;&#36873;&#25321;&#31639;&#27861;&#65292;&#36890;&#36807;&#21152;&#36895;&#25910;&#25947;&#36807;&#31243;&#25552;&#39640;&#20102;&#20551;&#32930;&#25511;&#21046;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.03233</link><description>&lt;p&gt;
&#22522;&#20110;Split Learning&#30340;&#32908;&#30005;&#20551;&#32930;&#25511;&#21046;&#20013;&#30340;&#25910;&#25947;&#36895;&#29575;&#26368;&#22823;&#21270;
&lt;/p&gt;
&lt;p&gt;
Convergence Rate Maximization for Split Learning-based Control of EMG Prosthetic Devices. (arXiv:2401.03233v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03233
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#26368;&#22823;&#21270;&#27169;&#22411;&#25910;&#25947;&#36895;&#29575;&#30340;Split Learning&#32908;&#30005;&#20551;&#32930;&#25511;&#21046;&#20013;&#30340;&#20999;&#23618;&#36873;&#25321;&#31639;&#27861;&#65292;&#36890;&#36807;&#21152;&#36895;&#25910;&#25947;&#36807;&#31243;&#25552;&#39640;&#20102;&#20551;&#32930;&#25511;&#21046;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Split Learning (SL)&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#29615;&#22659;&#20013;&#24212;&#29992;&#20110;&#22522;&#20110;&#32908;&#30005;&#30340;&#20551;&#32930;&#25511;&#21046;&#12290;&#19982;&#28145;&#24230;&#23398;&#20064;&#21644;&#32852;&#37030;&#23398;&#20064;&#31561;&#20854;&#20182;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;SL&#33021;&#22815;&#25552;&#20379;&#26356;&#20248;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#22240;&#20026;&#20551;&#32930;&#35774;&#22791;&#22312;&#22788;&#29702;&#33021;&#21147;&#21644;&#30005;&#27744;&#23551;&#21629;&#26041;&#38754;&#38750;&#24120;&#26377;&#38480;&#12290;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;SL&#30340;&#21487;&#34892;&#24615;&#28304;&#20110;&#20854;&#22266;&#26377;&#30340;&#27169;&#22411;&#20998;&#21106;&#65292;&#20854;&#20013;&#23458;&#25143;&#31471;&#25191;&#34892;&#36739;&#23567;&#30340;&#27169;&#22411;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#36873;&#25321;&#19981;&#24688;&#24403;&#30340;&#20999;&#23618;&#20250;&#38459;&#30861;SL&#31995;&#32479;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26368;&#22823;&#21270;&#27169;&#22411;&#25910;&#25947;&#36895;&#29575;&#30340;&#20999;&#23618;&#36873;&#25321;&#31639;&#27861;&#12290;&#24615;&#33021;&#35780;&#20272;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#25913;&#21892;&#20551;&#32930;&#25511;&#21046;&#30340;&#32908;&#30005;&#27169;&#24335;&#35782;&#21035;&#20219;&#21153;&#20013;&#26174;&#33879;&#21152;&#36895;&#20102;&#25910;&#25947;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Split Learning (SL) is a promising Distributed Learning approach in electromyography (EMG) based prosthetic control, due to its applicability within resource-constrained environments. Other learning approaches, such as Deep Learning and Federated Learning (FL), provide suboptimal solutions, since prosthetic devices are extremely limited in terms of processing power and battery life. The viability of implementing SL in such scenarios is caused by its inherent model partitioning, with clients executing the smaller model segment. However, selecting an inadequate cut layer hinders the training process in SL systems. This paper presents an algorithm for optimal cut layer selection in terms of maximizing the convergence rate of the model. The performance evaluation demonstrates that the proposed algorithm substantially accelerates the convergence in an EMG pattern recognition task for improving prosthetic device control.
&lt;/p&gt;</description></item><item><title>&#22312;&#28145;&#24230;&#21704;&#23494;&#39039;&#22238;&#24402;&#20013;&#65292;&#23454;&#29616;&#21327;&#26041;&#24046;&#21644;&#32593;&#32476;&#34920;&#36798;&#33021;&#21147;&#20043;&#38388;&#30340;&#24179;&#34913;&#19968;&#30452;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#32423;&#32852;&#22238;&#24402;&#26694;&#26550;&#65292;&#22312;&#31532;&#19968;&#38454;&#27573;&#36890;&#36807;&#21327;&#21464;&#31070;&#32463;&#32593;&#32476;&#24314;&#27169;&#24182;&#20135;&#29983;&#21327;&#21464;&#29305;&#24449;&#21644;&#22522;&#32447;&#39044;&#27979;&#65292;&#36741;&#21161;&#31532;&#20108;&#38454;&#27573;&#23398;&#20064;&#21327;&#26041;&#24046;&#12290;&#21516;&#26102;&#65292;&#31532;&#20108;&#38454;&#27573;&#20351;&#29992;&#38750;&#32447;&#24615;&#22270;&#24418;Transformer&#32593;&#32476;&#36827;&#34892;&#32467;&#26500;&#24314;&#27169;&#65292;&#25552;&#39640;&#20102;&#21704;&#23494;&#39039;&#39044;&#27979;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.00744</link><description>&lt;p&gt;
&#22312;&#26230;&#20307;&#26448;&#26009;&#30740;&#31350;&#20013;&#65292;&#23558;&#21327;&#26041;&#24046;&#21644;&#34920;&#36798;&#33021;&#21147;&#34701;&#21512;&#20026;&#28145;&#24230;&#21704;&#23494;&#39039;&#22238;&#24402;&#65306;&#19968;&#31181;&#28151;&#21512;&#32423;&#32852;&#22238;&#24402;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Harmonizing Covariance and Expressiveness for Deep Hamiltonian Regression in Crystalline Material Research: a Hybrid Cascaded Regression Framework. (arXiv:2401.00744v2 [physics.comp-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00744
&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#21704;&#23494;&#39039;&#22238;&#24402;&#20013;&#65292;&#23454;&#29616;&#21327;&#26041;&#24046;&#21644;&#32593;&#32476;&#34920;&#36798;&#33021;&#21147;&#20043;&#38388;&#30340;&#24179;&#34913;&#19968;&#30452;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#32423;&#32852;&#22238;&#24402;&#26694;&#26550;&#65292;&#22312;&#31532;&#19968;&#38454;&#27573;&#36890;&#36807;&#21327;&#21464;&#31070;&#32463;&#32593;&#32476;&#24314;&#27169;&#24182;&#20135;&#29983;&#21327;&#21464;&#29305;&#24449;&#21644;&#22522;&#32447;&#39044;&#27979;&#65292;&#36741;&#21161;&#31532;&#20108;&#38454;&#27573;&#23398;&#20064;&#21327;&#26041;&#24046;&#12290;&#21516;&#26102;&#65292;&#31532;&#20108;&#38454;&#27573;&#20351;&#29992;&#38750;&#32447;&#24615;&#22270;&#24418;Transformer&#32593;&#32476;&#36827;&#34892;&#32467;&#26500;&#24314;&#27169;&#65292;&#25552;&#39640;&#20102;&#21704;&#23494;&#39039;&#39044;&#27979;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26448;&#26009;&#30740;&#31350;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#29992;&#20110;&#21704;&#23494;&#39039;&#22238;&#24402;&#37327;&#23376;&#31995;&#32479;&#38656;&#35201;&#28385;&#36275;&#21327;&#26041;&#24046;&#23450;&#24459;&#65292;&#20854;&#20013;&#23454;&#29616;SO(3)&#31561;&#21464;&#24615;&#32780;&#19981;&#25439;&#22833;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#26159;&#19968;&#20010;&#38590;&#20197;&#35299;&#20915;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#38750;&#32447;&#24615;&#26144;&#23556;&#30340;&#29702;&#35770;&#31561;&#21464;&#24615;&#20445;&#35777;&#21463;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#21327;&#26041;&#24046;-&#34920;&#36798;&#33021;&#21147;&#22256;&#22659;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26694;&#26550;&#65292;&#20998;&#20026;&#20004;&#20010;&#32423;&#32852;&#22238;&#24402;&#38454;&#27573;&#12290;&#31532;&#19968;&#38454;&#27573;&#20351;&#29992;&#19968;&#20010;&#29702;&#35770;&#19978;&#20445;&#35777;&#30340;&#21327;&#21464;&#31070;&#32463;&#32593;&#32476;&#26469;&#24314;&#27169;&#19977;&#32500;&#21407;&#23376;&#31995;&#32479;&#30340;&#23545;&#31216;&#24615;&#65292;&#20135;&#29983;&#29702;&#35770;&#19978;&#30340;&#21327;&#21464;&#29305;&#24449;&#21644;&#22522;&#32447;&#21704;&#23494;&#39039;&#39044;&#27979;&#65292;&#24110;&#21161;&#31532;&#20108;&#38454;&#27573;&#23398;&#20064;&#21327;&#21464;&#24615;&#12290;&#21516;&#26102;&#65292;&#31532;&#20108;&#38454;&#27573;&#20351;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#38750;&#32447;&#24615;&#19977;&#32500;&#22270;&#24418;Transformer&#32593;&#32476;&#26469;&#36827;&#34892;&#19977;&#32500;&#21407;&#23376;&#31995;&#32479;&#30340;&#32467;&#26500;&#24314;&#27169;&#65292;&#23558;&#31532;&#19968;&#38454;&#27573;&#30340;&#36755;&#20986;&#31934;&#32454;&#21270;&#20026;&#20855;&#26377;&#26356;&#22909;&#34920;&#36798;&#33021;&#21147;&#30340;&#21704;&#23494;&#39039;&#39044;&#27979;&#12290;&#36890;&#36807;&#29702;&#35770;&#19978;&#30340;&#21327;&#21464;&#24615;&#21644;&#26356;&#22909;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#23545;&#21704;&#23494;&#39039;&#22238;&#24402;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning for Hamiltonian regression of quantum systems in material research necessitates satisfying the covariance laws, among which achieving SO(3)-equivariance without sacrificing the expressiveness of networks remains an elusive challenge due to the restriction to non-linear mappings on guaranteeing theoretical equivariance. To alleviate the covariance-expressiveness dilemma, we propose a hybrid framework with two cascaded regression stages. The first stage, with a theoretically-guaranteed covariant neural network modeling symmetry properties of 3D atom systems, yields theoretically covariant features and baseline Hamiltonian predictions, assisting the second stage in learning covariance. Meanwhile, the second stage, powered by a non-linear 3D graph Transformer network we propose for structural modeling of 3D atomic systems, refines the first stage's output as a fine-grained prediction of Hamiltonians with better expressiveness capability. The combination of a theoretically cov
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#21098;&#26525;&#36807;&#31243;&#20013;&#24212;&#29992;&#26799;&#24230;&#37319;&#26679;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#22330;&#26223;&#20013;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#27700;&#24179;&#30340;&#40065;&#26834;&#21098;&#26525;&#12290;&#30456;&#20851;&#23454;&#39564;&#35777;&#26126;&#65292;&#37319;&#29992;&#26799;&#24230;&#37319;&#26679;&#25216;&#26415;&#36827;&#34892;&#20248;&#21270;&#30340;&#27169;&#22411;&#27604;&#20256;&#32479;&#20248;&#21270;&#26041;&#27861;&#26356;&#33021;&#26377;&#25928;&#22320;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;&#36825;&#19968;&#21019;&#26032;&#26041;&#27861;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#21644;&#31070;&#32463;&#26550;&#26500;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#65292;&#24182;&#19988;&#29702;&#35770;&#19978;&#35299;&#37322;&#20102;&#26799;&#24230;&#37319;&#26679;&#25216;&#26415;&#23545;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2312.16020</link><description>&lt;p&gt;
&#22522;&#20110;&#26799;&#24230;&#37319;&#26679;&#20248;&#21270;&#30340;&#27531;&#24046;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#21098;&#26525;
&lt;/p&gt;
&lt;p&gt;
Robust Neural Pruning with Gradient Sampling Optimization for Residual Neural Networks. (arXiv:2312.16020v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.16020
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#21098;&#26525;&#36807;&#31243;&#20013;&#24212;&#29992;&#26799;&#24230;&#37319;&#26679;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#22330;&#26223;&#20013;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#27700;&#24179;&#30340;&#40065;&#26834;&#21098;&#26525;&#12290;&#30456;&#20851;&#23454;&#39564;&#35777;&#26126;&#65292;&#37319;&#29992;&#26799;&#24230;&#37319;&#26679;&#25216;&#26415;&#36827;&#34892;&#20248;&#21270;&#30340;&#27169;&#22411;&#27604;&#20256;&#32479;&#20248;&#21270;&#26041;&#27861;&#26356;&#33021;&#26377;&#25928;&#22320;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;&#36825;&#19968;&#21019;&#26032;&#26041;&#27861;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#21644;&#31070;&#32463;&#26550;&#26500;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#65292;&#24182;&#19988;&#29702;&#35770;&#19978;&#35299;&#37322;&#20102;&#26799;&#24230;&#37319;&#26679;&#25216;&#26415;&#23545;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#26041;&#27861;&#65292;&#37325;&#28857;&#26159;&#22312;&#21098;&#26525;&#36807;&#31243;&#20013;&#24212;&#29992;&#26799;&#24230;&#37319;&#26679;&#25216;&#26415;&#65292;&#31867;&#20284;&#20110;StochGradAdam&#20013;&#30340;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#22330;&#26223;&#20013;&#20445;&#25345;&#21098;&#26525;&#27169;&#22411;&#30340;&#39640;&#20934;&#30830;&#24615;&#27700;&#24179;&#65292;&#36825;&#26159;&#19968;&#20010;&#20851;&#38190;&#24615;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#37319;&#29992;&#26799;&#24230;&#37319;&#26679;&#25216;&#26415;&#20248;&#21270;&#30340;&#27169;&#22411;&#22312;&#21098;&#26525;&#36807;&#31243;&#20013;&#27604;&#20351;&#29992;&#20256;&#32479;&#20248;&#21270;&#26041;&#27861;&#26356;&#26377;&#25928;&#22320;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;&#36825;&#19968;&#21457;&#29616;&#24378;&#35843;&#20102;&#26799;&#24230;&#37319;&#26679;&#22312;&#20419;&#36827;&#40065;&#26834;&#23398;&#20064;&#21644;&#20351;&#32593;&#32476;&#22312;&#22797;&#26434;&#24230;&#22823;&#22823;&#38477;&#20302;&#21518;&#20173;&#33021;&#20445;&#30041;&#20851;&#38190;&#20449;&#24687;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#21508;&#31181;&#25968;&#25454;&#38598;&#21644;&#31070;&#32463;&#26550;&#26500;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#23427;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#21644;&#25928;&#26524;&#12290;&#35813;&#35770;&#25991;&#36824;&#28145;&#20837;&#25506;&#35752;&#20102;&#29702;&#35770;&#26041;&#38754;&#65292;&#35299;&#37322;&#20102;&#26799;&#24230;&#37319;&#26679;&#25216;&#26415;&#22914;&#20309;&#22686;&#24378;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we explore an innovative approach for neural network optimization, focusing on the application of gradient sampling techniques, similar to those in StochGradAdam, during the pruning process. Our primary objective is to maintain high accuracy levels in pruned models, a critical challenge in resource-limited scenarios. Our extensive experiments reveal that models optimized with gradient sampling techniques are more effective at preserving accuracy during pruning compared to those using traditional optimization methods. This finding underscores the significance of gradient sampling in facilitating robust learning and enabling networks to retain crucial information even after substantial reduction in their complexity. We validate our approach across various datasets and neural architectures, demonstrating its broad applicability and effectiveness. The paper also delves into the theoretical aspects, explaining how gradient sampling techniques contribute to the robustness of m
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#27169;&#22411;&#37096;&#32626;&#21644;&#24182;&#34892;&#26694;&#26550;&#65292;&#29992;&#20110;&#21152;&#36895;RLHF&#35757;&#32451;&#12290;&#35813;&#26694;&#26550;&#25552;&#20379;&#20102;&#20004;&#31181;&#28789;&#27963;&#30340;&#27169;&#22411;&#37096;&#32626;&#31574;&#30053;&#65292;&#20854;&#20013;&#20132;&#26367;&#31574;&#30053;&#26377;&#21161;&#20110;&#20943;&#23569;&#20869;&#23384;&#20887;&#20313;&#21644;&#36890;&#20449;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2312.11819</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#21152;&#36895;RLHF&#35757;&#32451;&#30340;&#33258;&#36866;&#24212;&#37096;&#32626;&#21644;&#24182;&#34892;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
An Adaptive Placement and Parallelism Framework for Accelerating RLHF Training. (arXiv:2312.11819v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.11819
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#27169;&#22411;&#37096;&#32626;&#21644;&#24182;&#34892;&#26694;&#26550;&#65292;&#29992;&#20110;&#21152;&#36895;RLHF&#35757;&#32451;&#12290;&#35813;&#26694;&#26550;&#25552;&#20379;&#20102;&#20004;&#31181;&#28789;&#27963;&#30340;&#27169;&#22411;&#37096;&#32626;&#31574;&#30053;&#65292;&#20854;&#20013;&#20132;&#26367;&#31574;&#30053;&#26377;&#21161;&#20110;&#20943;&#23569;&#20869;&#23384;&#20887;&#20313;&#21644;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20687;ChatGPT&#25110;InstructGPT&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20135;&#29983;&#20102;&#37325;&#22823;&#24433;&#21709;&#12290;&#35768;&#22810;&#30740;&#31350;&#23581;&#35797;&#22797;&#29616;&#22797;&#26434;&#30340;InstructGPT&#30340;&#35757;&#32451;&#27969;&#31243;&#65292;&#21363;&#22522;&#20110;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#12290;&#28982;&#32780;&#65292;&#20027;&#27969;&#30340;&#20998;&#24067;&#24335;RLHF&#35757;&#32451;&#26041;&#27861;&#36890;&#24120;&#37319;&#29992;&#22266;&#23450;&#30340;&#27169;&#22411;&#37096;&#32626;&#31574;&#30053;&#65292;&#31216;&#20026;Flattening&#31574;&#30053;&#12290;&#35813;&#31574;&#30053;&#23558;RLHF&#20013;&#28041;&#21450;&#30340;&#22235;&#20010;&#30456;&#20114;&#20381;&#36182;&#30340;&#27169;&#22411;&#35270;&#20026;&#21333;&#20010;&#23454;&#20307;&#65292;&#23558;&#23427;&#20204;&#20998;&#37197;&#21040;&#25152;&#26377;&#35774;&#22791;&#19978;&#65292;&#24182;&#24212;&#29992;&#20110;&#21333;&#20010;&#27169;&#22411;&#35774;&#35745;&#30340;&#24182;&#34892;&#25216;&#26415;&#65292;&#32780;&#19981;&#32771;&#34385;&#27599;&#20010;&#27169;&#22411;&#22266;&#26377;&#30340;&#19981;&#21516;&#24037;&#20316;&#36127;&#36733;&#12290;&#32467;&#26524;&#65292;&#35813;&#31574;&#30053;&#21152;&#21095;&#20102;RLHF&#35757;&#32451;&#20013;&#30340;&#29983;&#25104;&#29942;&#39048;&#65292;&#24182;&#38477;&#20302;&#20102;&#25972;&#20307;&#35757;&#32451;&#25928;&#29575;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#27169;&#22411;&#37096;&#32626;&#26694;&#26550;&#65292;&#25552;&#20379;&#20102;&#20004;&#31181;&#28789;&#27963;&#30340;&#27169;&#22411;&#37096;&#32626;&#31574;&#30053;&#12290;&#20132;&#26367;&#31574;&#30053;&#26377;&#21161;&#20110;&#20943;&#23569;&#20869;&#23384;&#20887;&#20313;&#21644;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, ChatGPT or InstructGPT like large language models (LLM) has made a significant impact in the AI world. Many works have attempted to reproduce the complex InstructGPT's training pipeline, namely Reinforcement Learning with Human Feedback (RLHF). However, the mainstream distributed RLHF training methods typically adopt a fixed model placement strategy, referred to as the Flattening strategy. This strategy treats all four interdependent models involved in RLHF as a single entity, distributing them across all devices and applying parallelism techniques designed for a single model, regardless of the different workloads inherent to each model. As a result, this strategy exacerbates the generation bottlenecks in the RLHF training and degrades the overall training efficiency. To address these issues, we propose an adaptive model placement framework that offers two flexible model placement strategies. The Interleaving strategy helps reduce memory redundancy and communication costs of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#20351;&#29992;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#25512;&#29702;&#30340;&#30740;&#31350;&#65292;&#20171;&#32461;&#20102;&#26368;&#26032;&#30340;&#25512;&#29702;&#20219;&#21153;&#12289;&#26041;&#27861;&#21644;&#22522;&#20934;&#65292;&#24182;&#35752;&#35770;&#20102;&#22522;&#30784;&#27169;&#22411;&#20013;&#25512;&#29702;&#33021;&#21147;&#30340;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2312.11562</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#25512;&#29702;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey of Reasoning with Foundation Models. (arXiv:2312.11562v5 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.11562
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#20351;&#29992;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#25512;&#29702;&#30340;&#30740;&#31350;&#65292;&#20171;&#32461;&#20102;&#26368;&#26032;&#30340;&#25512;&#29702;&#20219;&#21153;&#12289;&#26041;&#27861;&#21644;&#22522;&#20934;&#65292;&#24182;&#35752;&#35770;&#20102;&#22522;&#30784;&#27169;&#22411;&#20013;&#25512;&#29702;&#33021;&#21147;&#30340;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#29702;&#26159;&#22797;&#26434;&#38382;&#39064;&#35299;&#20915;&#30340;&#20851;&#38190;&#33021;&#21147;&#65292;&#22312;&#35848;&#21028;&#12289;&#21307;&#23398;&#35786;&#26029;&#21644;&#21009;&#20107;&#35843;&#26597;&#31561;&#21508;&#31181;&#29616;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#23427;&#22312;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#39046;&#22495;&#20013;&#20316;&#20026;&#19968;&#31181;&#22522;&#26412;&#26041;&#27861;&#23398;&#12290;&#38543;&#30528;&#22522;&#30784;&#27169;&#22411;&#65288;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65289;&#30340;&#19981;&#26029;&#21457;&#23637;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#32773;&#23545;&#23427;&#20204;&#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#20135;&#29983;&#20102;&#20852;&#36259;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#29992;&#20110;&#25512;&#29702;&#30340;&#24320;&#21019;&#24615;&#22522;&#30784;&#27169;&#22411;&#65292;&#24182;&#31361;&#20986;&#20102;&#21508;&#31181;&#25512;&#29702;&#20219;&#21153;&#12289;&#26041;&#27861;&#21644;&#22522;&#20934;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#25105;&#20204;&#36824;&#28145;&#20837;&#25506;&#35752;&#20102;&#22522;&#30784;&#27169;&#22411;&#20013;&#25512;&#29702;&#33021;&#21147;&#30340;&#28508;&#22312;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#65292;&#24182;&#35752;&#35770;&#20102;&#22810;&#27169;&#24335;&#23398;&#20064;&#12289;&#33258;&#20027;&#20195;&#29702;&#21644;&#36229;&#32423;&#23545;&#40784;&#22312;&#25512;&#29702;&#32972;&#26223;&#19979;&#30340;&#30456;&#20851;&#24615;&#12290;&#36890;&#36807;&#35752;&#35770;&#36825;&#20123;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#65292;&#25105;&#20204;&#24076;&#26395;&#28608;&#21457;&#30740;&#31350;&#32773;&#20204;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#25506;&#32034;&#65292;&#25512;&#21160;&#36827;&#19968;&#27493;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reasoning, a crucial ability for complex problem-solving, plays a pivotal role in various real-world settings such as negotiation, medical diagnosis, and criminal investigation. It serves as a fundamental methodology in the field of Artificial General Intelligence (AGI). With the ongoing development of foundation models, e.g., Large Language Models (LLMs), there is a growing interest in exploring their abilities in reasoning tasks. In this paper, we introduce seminal foundation models proposed or adaptable for reasoning, highlighting the latest advancements in various reasoning tasks, methods, and benchmarks. We then delve into the potential future directions behind the emergence of reasoning abilities within foundation models. We also discuss the relevance of multimodal learning, autonomous agents, and super alignment in the context of reasoning. By discussing these future research directions, we hope to inspire researchers in their exploration of this field, stimulate further advance
&lt;/p&gt;</description></item><item><title>TrojFST&#26159;&#19968;&#31181;&#22312;&#23569;&#26679;&#26412;&#25552;&#31034;&#35843;&#20248;&#26694;&#26550;&#20013;&#36827;&#34892;&#21518;&#38376;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#24179;&#34913;&#30340;&#27745;&#26579;&#23398;&#20064;&#12289;&#36873;&#25321;&#24615;&#20196;&#29260;&#27745;&#26579;&#21644;...&#31561;&#27169;&#22359;&#26469;&#35299;&#20915;&#26500;&#24314;&#22522;&#20110;&#25552;&#31034;&#30340;&#21518;&#38376;&#30340;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2312.10467</link><description>&lt;p&gt;
TrojFST: &#23558;&#29305;&#27931;&#20234;&#26408;&#39532;&#23884;&#20837;&#21040;&#23569;&#26679;&#26412;&#25552;&#31034;&#35843;&#20248;&#20013;
&lt;/p&gt;
&lt;p&gt;
TrojFST: Embedding Trojans in Few-shot Prompt Tuning. (arXiv:2312.10467v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.10467
&lt;/p&gt;
&lt;p&gt;
TrojFST&#26159;&#19968;&#31181;&#22312;&#23569;&#26679;&#26412;&#25552;&#31034;&#35843;&#20248;&#26694;&#26550;&#20013;&#36827;&#34892;&#21518;&#38376;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#24179;&#34913;&#30340;&#27745;&#26579;&#23398;&#20064;&#12289;&#36873;&#25321;&#24615;&#20196;&#29260;&#27745;&#26579;&#21644;...&#31561;&#27169;&#22359;&#26469;&#35299;&#20915;&#26500;&#24314;&#22522;&#20110;&#25552;&#31034;&#30340;&#21518;&#38376;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#35843;&#20248;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#38750;&#24120;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#36866;&#24212;&#20351;&#29992;&#26377;&#38480;&#36755;&#20837;&#26679;&#26412;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#26469;&#22788;&#29702;&#26032;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#25552;&#31034;&#35843;&#20248;&#30340;&#25104;&#21151;&#23548;&#33268;&#23545;&#25163;&#35797;&#22270;&#38024;&#23545;&#35813;&#25216;&#26415;&#36827;&#34892;&#21518;&#38376;&#25915;&#20987;&#12290;&#20043;&#21069;&#22522;&#20110;&#25552;&#31034;&#30340;&#21518;&#38376;&#25915;&#20987;&#22312;&#23569;&#26679;&#26412;&#25552;&#31034;&#35843;&#20248;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#65292;&#38656;&#35201;&#20840;&#27169;&#22411;&#24494;&#35843;&#25110;&#22823;&#35268;&#27169;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#20351;&#29992;&#23569;&#26679;&#26412;&#25552;&#31034;&#35843;&#20248;&#26500;&#24314;&#22522;&#20110;&#25552;&#31034;&#30340;&#21518;&#38376;&#30340;&#22256;&#38590;&#65292;&#36825;&#28041;&#21450;&#20923;&#32467;PLM&#24182;&#22312;&#19968;&#32452;&#21463;&#38480;&#21046;&#30340;&#36755;&#20837;&#26679;&#26412;&#19978;&#35843;&#20248;&#36719;&#25552;&#31034;&#12290;&#36825;&#31181;&#26041;&#27861;&#24341;&#20837;&#20102;&#19968;&#20010;&#19981;&#24179;&#34913;&#30340;&#27745;&#26579;&#25968;&#25454;&#38598;&#65292;&#23481;&#26131;&#36807;&#25311;&#21512;&#24182;&#19988;&#32570;&#20047;&#27880;&#24847;&#21147;&#24863;&#30693;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#22312;&#23569;&#26679;&#26412;&#25552;&#31034;&#35843;&#20248;&#26694;&#26550;&#20013;&#24341;&#20837;&#20102;TrojFST&#29992;&#20110;&#21518;&#38376;&#25915;&#20987;&#12290;TrojFST&#21253;&#25324;&#19977;&#20010;&#27169;&#22359;&#65306;&#24179;&#34913;&#30340;&#27745;&#26579;&#23398;&#20064;&#12289;&#36873;&#25321;&#24615;&#20196;&#29260;&#27745;&#26579;&#21644;...
&lt;/p&gt;
&lt;p&gt;
Prompt-tuning has emerged as a highly effective approach for adapting a pre-trained language model (PLM) to handle new natural language processing tasks with limited input samples. However, the success of prompt-tuning has led to adversaries attempting backdoor attacks against this technique. Previous prompt-based backdoor attacks faced challenges when implemented through few-shot prompt-tuning, requiring either full-model fine-tuning or a large training dataset. We observe the difficulty in constructing a prompt-based backdoor using few-shot prompt-tuning, which involves freezing the PLM and tuning a soft prompt with a restricted set of input samples. This approach introduces an imbalanced poisoned dataset, making it susceptible to overfitting and lacking attention awareness. To address these challenges, we introduce TrojFST for backdoor attacks within the framework of few-shot prompt-tuning. TrojFST comprises three modules: balanced poison learning, selective token poisoning, and tro
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#27604;&#36739;&#20102;&#26080;&#30417;&#30563;&#30340;&#24494;&#35843;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#36825;&#20004;&#31181;&#24120;&#35265;&#26041;&#27861;&#22312;LLMs&#20013;&#30340;&#24212;&#29992;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;RAG&#22312;&#29616;&#26377;&#30693;&#35782;&#21644;&#26032;&#30693;&#35782;&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#32780;LLMs&#36890;&#36807;&#26080;&#30417;&#30563;&#30340;&#24494;&#35843;&#23398;&#20064;&#26032;&#30340;&#20107;&#23454;&#20449;&#24687;&#36739;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2312.05934</link><description>&lt;p&gt;
Fine-Tuning&#36824;&#26159;&#26816;&#32034;&#65311;&#27604;&#36739;&#22312;LLMs&#20013;&#30340;&#30693;&#35782;&#27880;&#20837;
&lt;/p&gt;
&lt;p&gt;
Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs. (arXiv:2312.05934v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.05934
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#27604;&#36739;&#20102;&#26080;&#30417;&#30563;&#30340;&#24494;&#35843;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#36825;&#20004;&#31181;&#24120;&#35265;&#26041;&#27861;&#22312;LLMs&#20013;&#30340;&#24212;&#29992;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;RAG&#22312;&#29616;&#26377;&#30693;&#35782;&#21644;&#26032;&#30693;&#35782;&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#32780;LLMs&#36890;&#36807;&#26080;&#30417;&#30563;&#30340;&#24494;&#35843;&#23398;&#20064;&#26032;&#30340;&#20107;&#23454;&#20449;&#24687;&#36739;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20854;&#39044;&#35757;&#32451;&#30340;&#26435;&#37325;&#20013;&#23553;&#35013;&#20102;&#22823;&#37327;&#30340;&#20107;&#23454;&#20449;&#24687;&#65292;&#27491;&#22914;&#23427;&#20204;&#33021;&#22815;&#22312;&#19981;&#21516;&#39046;&#22495;&#22238;&#31572;&#21508;&#31181;&#38382;&#39064;&#25152;&#35777;&#26126;&#30340;&#37027;&#26679;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#30693;&#35782;&#26412;&#36136;&#19978;&#26159;&#26377;&#38480;&#30340;&#65292;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#35757;&#32451;&#25968;&#25454;&#30340;&#29305;&#24615;&#12290;&#22240;&#27492;&#65292;&#20351;&#29992;&#22806;&#37096;&#25968;&#25454;&#38598;&#26469;&#25972;&#21512;&#26032;&#30340;&#20449;&#24687;&#25110;&#25913;&#36827;LLMs&#22312;&#24050;&#35265;&#20449;&#24687;&#19978;&#30340;&#33021;&#21147;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#36825;&#20010;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20004;&#31181;&#24120;&#35265;&#30340;&#26041;&#27861;&#65306;&#26080;&#30417;&#30563;&#30340;&#24494;&#35843;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#20027;&#39064;&#30340;&#21508;&#31181;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#36825;&#20004;&#31181;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#34429;&#28982;&#26080;&#30417;&#30563;&#30340;&#24494;&#35843;&#33021;&#22815;&#25552;&#20379;&#19968;&#23450;&#30340;&#25913;&#36827;&#65292;&#20294;RAG&#22312;&#29616;&#26377;&#30693;&#35782;&#21644;&#23436;&#20840;&#26032;&#30693;&#35782;&#19978;&#22987;&#32456;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;LLMs&#24456;&#38590;&#36890;&#36807;&#26080;&#30417;&#30563;&#30340;&#24494;&#35843;&#26469;&#23398;&#20064;&#26032;&#30340;&#20107;&#23454;&#20449;&#24687;&#65292;&#24182;&#19988;&#26292;&#38706;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) encapsulate a vast amount of factual information within their pre-trained weights, as evidenced by their ability to answer diverse questions across different domains. However, this knowledge is inherently limited, relying heavily on the characteristics of the training data. Consequently, using external datasets to incorporate new information or refine the capabilities of LLMs on previously seen information poses a significant challenge. In this study, we compare two common approaches: unsupervised fine-tuning and retrieval-augmented generation (RAG). We evaluate both approaches on a variety of knowledge-intensive tasks across different topics. Our findings reveal that while unsupervised fine-tuning offers some improvement, RAG consistently outperforms it, both for existing knowledge encountered during training and entirely new knowledge. Moreover, we find that LLMs struggle to learn new factual information through unsupervised fine-tuning, and that exposing
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#36335;&#24452;&#35268;&#21010;&#30340;&#21322;&#30417;&#30563;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#36827;&#34892;&#35821;&#20041;&#20998;&#21106;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#20943;&#23569;&#20154;&#24037;&#26631;&#27880;&#37327;&#65292;&#25552;&#39640;&#20102;&#26426;&#22120;&#20154;&#30340;&#24863;&#30693;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2312.04402</link><description>&lt;p&gt;
&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#20351;&#29992;&#20449;&#24687;&#36335;&#24452;&#35268;&#21010;&#30340;&#21322;&#30417;&#30563;&#20027;&#21160;&#23398;&#20064;&#36827;&#34892;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Semi-Supervised Active Learning for Semantic Segmentation in Unknown Environments Using Informative Path Planning. (arXiv:2312.04402v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.04402
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#36335;&#24452;&#35268;&#21010;&#30340;&#21322;&#30417;&#30563;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#36827;&#34892;&#35821;&#20041;&#20998;&#21106;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#20943;&#23569;&#20154;&#24037;&#26631;&#27880;&#37327;&#65292;&#25552;&#39640;&#20102;&#26426;&#22120;&#20154;&#30340;&#24863;&#30693;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#20998;&#21106;&#20351;&#24471;&#26426;&#22120;&#20154;&#33021;&#22815;&#23545;&#20854;&#29615;&#22659;&#36827;&#34892;&#24863;&#30693;&#21644;&#25512;&#29702;&#65292;&#36229;&#20986;&#20102;&#20960;&#20309;&#23398;&#30340;&#33539;&#30068;&#12290;&#22823;&#22810;&#25968;&#36825;&#31867;&#31995;&#32479;&#24314;&#31435;&#22312;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#12290;&#30001;&#20110;&#33258;&#20027;&#26426;&#22120;&#20154;&#36890;&#24120;&#37096;&#32626;&#22312;&#21021;&#22987;&#26410;&#30693;&#29615;&#22659;&#20013;&#65292;&#23545;&#38745;&#24577;&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#35757;&#32451;&#19981;&#33021;&#24635;&#26159;&#25429;&#25417;&#21040;&#22810;&#26679;&#30340;&#39046;&#22495;&#65292;&#38480;&#21046;&#20102;&#26426;&#22120;&#20154;&#22312;&#20219;&#21153;&#20013;&#30340;&#24863;&#30693;&#24615;&#33021;&#12290;&#26368;&#36817;&#20986;&#29616;&#20102;&#19968;&#20123;&#33258;&#30417;&#30563;&#21644;&#23436;&#20840;&#30417;&#30563;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#26469;&#25913;&#36827;&#26426;&#22120;&#20154;&#30340;&#35270;&#35273;&#12290;&#36825;&#20123;&#26041;&#27861;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#30340;&#39046;&#22495;&#20869;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#25110;&#32773;&#38656;&#35201;&#22823;&#37327;&#20154;&#24037;&#26631;&#27880;&#30340;&#24037;&#20316;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22320;&#22270;&#30340;&#33258;&#36866;&#24212;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#29992;&#20110;&#21322;&#30417;&#30563;&#30340;&#35821;&#20041;&#20998;&#21106;&#20027;&#21160;&#23398;&#20064;&#65292;&#30456;&#27604;&#23436;&#20840;&#30417;&#30563;&#26041;&#27861;&#65292;&#26497;&#22823;&#22320;&#20943;&#23569;&#20102;&#20154;&#24037;&#26631;&#27880;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#21033;&#29992;&#33258;&#36866;&#24212;&#36335;&#24452;&#35268;&#21010;&#22120;&#26469;&#24341;&#23548;&#26426;&#22120;&#20154;&#25506;&#32034;&#26410;&#30693;&#31354;&#38388;&#30340;&#36793;&#30028;&#65292;&#24182;&#25910;&#38598;&#20855;&#26377;&#39640;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#20154;&#24037;&#26631;&#27880;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#26159;&#32467;&#21512;&#20102;&#31232;&#30095;&#30340;
&lt;/p&gt;
&lt;p&gt;
Semantic segmentation enables robots to perceive and reason about their environments beyond geometry. Most of such systems build upon deep learning approaches. As autonomous robots are commonly deployed in initially unknown environments, pre-training on static datasets cannot always capture the variety of domains and limits the robot's perception performance during missions. Recently, self-supervised and fully supervised active learning methods emerged to improve a robot's vision. These approaches rely on large in-domain pre-training datasets or require substantial human labelling effort. We propose a planning method for semi-supervised active learning of semantic segmentation that substantially reduces human labelling requirements compared to fully supervised approaches. We leverage an adaptive map-based planner guided towards the frontiers of unexplored space with high model uncertainty collecting training data for human labelling. A key aspect of our approach is to combine the spars
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#26680;&#26426;&#22120;&#39044;&#22788;&#29702;&#20013;&#20351;&#29992;Nystrom&#36924;&#36817;&#30340;&#26435;&#34913;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#23545;&#25968;&#22823;&#23567;&#30340;&#26679;&#26412;&#33021;&#22815;&#35753;Nystrom&#36924;&#36817;&#30340;&#39044;&#22788;&#29702;&#22120;&#20960;&#20046;&#19982;&#26799;&#24230;&#19979;&#38477;&#21516;&#26679;&#26377;&#25928;&#22320;&#21152;&#36895;&#12290;</title><link>http://arxiv.org/abs/2312.03311</link><description>&lt;p&gt;
&#23545;&#20110;&#26680;&#26426;&#22120;&#22312;&#39044;&#22788;&#29702;&#20013;&#30340;Nystrom&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
On the Nystrom Approximation for Preconditioning in Kernel Machines. (arXiv:2312.03311v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.03311
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#26680;&#26426;&#22120;&#39044;&#22788;&#29702;&#20013;&#20351;&#29992;Nystrom&#36924;&#36817;&#30340;&#26435;&#34913;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#23545;&#25968;&#22823;&#23567;&#30340;&#26679;&#26412;&#33021;&#22815;&#35753;Nystrom&#36924;&#36817;&#30340;&#39044;&#22788;&#29702;&#22120;&#20960;&#20046;&#19982;&#26799;&#24230;&#19979;&#38477;&#21516;&#26679;&#26377;&#25928;&#22320;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26680;&#26041;&#27861;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#19968;&#31867;&#27969;&#34892;&#30340;&#38750;&#32447;&#24615;&#39044;&#27979;&#27169;&#22411;&#12290;&#23398;&#20064;&#26680;&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#31639;&#27861;&#38656;&#35201;&#20855;&#26377;&#36845;&#20195;&#24615;&#36136;&#65292;&#20294;&#30001;&#20110;&#31967;&#31957;&#30340;&#26465;&#20214;&#65292;&#25910;&#25947;&#21487;&#33021;&#24456;&#24930;&#12290;&#35889;&#39044;&#22788;&#29702;&#26159;&#21152;&#24555;&#35757;&#32451;&#26680;&#27169;&#22411;&#36845;&#20195;&#31639;&#27861;&#25910;&#25947;&#36895;&#24230;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#35745;&#31639;&#21644;&#23384;&#20648;&#35889;&#39044;&#22788;&#29702;&#22120;&#21487;&#33021;&#20195;&#20215;&#39640;&#26114;&#65292;&#20250;&#23548;&#33268;&#22823;&#37327;&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#24320;&#38144;&#65292;&#38480;&#21046;&#20102;&#26680;&#26041;&#27861;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#38382;&#39064;&#19978;&#30340;&#24212;&#29992;&#12290;Nystrom&#36924;&#36817;&#30340;&#35889;&#39044;&#22788;&#29702;&#22120;&#36890;&#24120;&#26356;&#20415;&#23452;&#21644;&#26356;&#23481;&#26131;&#35745;&#31639;&#21644;&#23384;&#20648;&#65292;&#24182;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#20351;&#29992;&#36825;&#31181;&#36924;&#36817;&#39044;&#22788;&#29702;&#22120;&#30340;&#26435;&#34913;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#34920;&#26126;&#19982;&#25968;&#25454;&#38598;&#22823;&#23567;&#30456;&#20851;&#30340;&#23545;&#25968;&#26679;&#26412;&#25968;&#37327;&#33021;&#22815;&#35753;&#22522;&#20110;Nystrom&#36924;&#36817;&#30340;&#39044;&#22788;&#29702;&#22120;&#20960;&#20046;&#19982;&#26799;&#24230;&#19979;&#38477;&#21516;&#26679;&#26377;&#25928;&#22320;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;
Kernel methods are a popular class of nonlinear predictive models in machine learning. Scalable algorithms for learning kernel models need to be iterative in nature, but convergence can be slow due to poor conditioning. Spectral preconditioning is an important tool to speed-up the convergence of such iterative algorithms for training kernel models. However computing and storing a spectral preconditioner can be expensive which can lead to large computational and storage overheads, precluding the application of kernel methods to problems with large datasets. A Nystrom approximation of the spectral preconditioner is often cheaper to compute and store, and has demonstrated success in practical applications. In this paper we analyze the trade-offs of using such an approximated preconditioner. Specifically, we show that a sample of logarithmic size (as a function of the size of the dataset) enables the Nystrom-based approximated preconditioner to accelerate gradient descent nearly as well as
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;, Feedback-Driven Solution Synthesis (FDSS), &#26088;&#22312;&#36890;&#36807;&#23558;LLMs&#19982;&#38745;&#24577;&#20195;&#30721;&#20998;&#26512;&#24037;&#20855;Bandit&#32467;&#21512;&#65292;&#35299;&#20915;&#20195;&#30721;&#20013;&#30340;&#23433;&#20840;&#28431;&#27934;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#29616;&#26377;&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#26377;&#26174;&#33879;&#25913;&#36827;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;PythonSecurityEval&#12290;</title><link>http://arxiv.org/abs/2312.00024</link><description>&lt;p&gt;
LLMs&#33021;&#22815;&#20462;&#22797;&#23433;&#20840;&#38382;&#39064;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can LLMs Patch Security Issues?. (arXiv:2312.00024v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.00024
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;, Feedback-Driven Solution Synthesis (FDSS), &#26088;&#22312;&#36890;&#36807;&#23558;LLMs&#19982;&#38745;&#24577;&#20195;&#30721;&#20998;&#26512;&#24037;&#20855;Bandit&#32467;&#21512;&#65292;&#35299;&#20915;&#20195;&#30721;&#20013;&#30340;&#23433;&#20840;&#28431;&#27934;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#29616;&#26377;&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#26377;&#26174;&#33879;&#25913;&#36827;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;PythonSecurityEval&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#24320;&#21457;&#32773;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#20250;&#29983;&#25104;&#21253;&#21547;&#23433;&#20840;&#28431;&#27934;&#21644;&#32570;&#38519;&#30340;&#20195;&#30721;&#12290;&#32534;&#20889;&#23433;&#20840;&#20195;&#30721;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#28431;&#27934;&#36890;&#24120;&#22312;&#31243;&#24207;&#19982;&#22806;&#37096;&#31995;&#32479;&#25110;&#26381;&#21153;&#65288;&#22914;&#25968;&#25454;&#24211;&#21644;&#25805;&#20316;&#31995;&#32479;&#65289;&#20043;&#38388;&#30340;&#20132;&#20114;&#36807;&#31243;&#20013;&#20986;&#29616;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;&#22522;&#20110;&#21453;&#39304;&#30340;&#35299;&#20915;&#26041;&#26696;&#21512;&#25104;&#65288;FDSS&#65289;&#65292;&#26088;&#22312;&#25506;&#32034;&#20351;&#29992;LLMs&#25509;&#25910;&#26469;&#33258;&#38745;&#24577;&#20195;&#30721;&#20998;&#26512;&#24037;&#20855;Bandit&#30340;&#21453;&#39304;&#65292;&#28982;&#21518;LLMs&#29983;&#25104;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#26469;&#35299;&#20915;&#23433;&#20840;&#28431;&#27934;&#12290;&#27599;&#20010;&#35299;&#20915;&#26041;&#26696;&#20197;&#21450;&#26131;&#21463;&#25915;&#20987;&#30340;&#20195;&#30721;&#38543;&#21518;&#34987;&#36865;&#22238;LLMs&#36827;&#34892;&#20195;&#30721;&#23436;&#21892;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22522;&#32447;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#25913;&#36827;&#65292;&#24182;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;PythonSecurityEval&#65292;&#35813;&#25968;&#25454;&#38598;&#25910;&#38598;&#20102;&#26469;&#33258;Stack Overflow&#30340;&#30495;&#23454;&#22330;&#26223;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown impressive proficiency in code generation. Nonetheless, similar to human developers, these models might generate code that contains security vulnerabilities and flaws. Writing secure code remains a substantial challenge, as vulnerabilities often arise during interactions between programs and external systems or services, such as databases and operating systems. In this paper, we propose a novel approach, Feedback-Driven Solution Synthesis (FDSS), designed to explore the use of LLMs in receiving feedback from Bandit, which is a static code analysis tool, and then the LLMs generate potential solutions to resolve security vulnerabilities. Each solution, along with the vulnerable code, is then sent back to the LLM for code refinement. Our approach shows a significant improvement over the baseline and outperforms existing approaches. Furthermore, we introduce a new dataset, PythonSecurityEval, collected from real-world scenarios on Stack Overflow to e
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#30701;&#26399;&#35745;&#21010;&#29983;&#25104;&#21644;&#36873;&#25321;&#19982;&#20998;&#24067;&#24335;&#20248;&#21270;&#20197;&#21450;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#28176;&#36827;&#26041;&#27861;&#65292;&#29992;&#20110;&#26080;&#20154;&#26426;&#30340;&#21327;&#35843;&#21644;&#35268;&#21010;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.09852</link><description>&lt;p&gt;
&#30701;&#26399;&#19982;&#38271;&#26399;&#26080;&#20154;&#26426;&#21327;&#35843;&#65306;&#20998;&#24067;&#24335;&#20248;&#21270;&#19982;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#20132;&#27719;
&lt;/p&gt;
&lt;p&gt;
Short vs. Long-term Coordination of Drones: When Distributed Optimization Meets Deep Reinforcement Learning. (arXiv:2311.09852v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.09852
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#30701;&#26399;&#35745;&#21010;&#29983;&#25104;&#21644;&#36873;&#25321;&#19982;&#20998;&#24067;&#24335;&#20248;&#21270;&#20197;&#21450;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#28176;&#36827;&#26041;&#27861;&#65292;&#29992;&#20110;&#26080;&#20154;&#26426;&#30340;&#21327;&#35843;&#21644;&#35268;&#21010;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26234;&#33021;&#22478;&#24066;&#20013;&#65292;&#25903;&#25345;&#20805;&#30005;&#25216;&#26415;&#30340;&#33258;&#20027;&#20132;&#20114;&#24335;&#26080;&#20154;&#26426;&#32676;&#21487;&#20197;&#25552;&#20379;&#24341;&#20154;&#27880;&#30446;&#30340;&#24863;&#30693;&#33021;&#21147;&#65292;&#20363;&#22914;&#20132;&#36890;&#30417;&#27979;&#21644;&#28798;&#38590;&#21709;&#24212;&#12290;&#29616;&#26377;&#26041;&#27861;&#65292;&#21253;&#25324;&#20998;&#24067;&#24335;&#20248;&#21270;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#65292;&#26088;&#22312;&#21327;&#35843;&#26080;&#20154;&#26426;&#20197;&#23454;&#29616;&#25104;&#26412;&#25928;&#30410;&#39640;&#12289;&#39640;&#36136;&#37327;&#30340;&#23548;&#33322;&#12289;&#24863;&#30693;&#21644;&#20805;&#30005;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#65306;&#30701;&#26399;&#20248;&#21270;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#30340;&#24847;&#22806;&#21464;&#21270;&#19979;&#24182;&#19981;&#26377;&#25928;&#65292;&#32780;&#38271;&#26399;&#23398;&#20064;&#32570;&#20047;&#21487;&#25193;&#23637;&#24615;&#12289;&#38887;&#24615;&#21644;&#28789;&#27963;&#24615;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28176;&#36827;&#26041;&#27861;&#65292;&#23558;&#22522;&#20110;&#20998;&#24067;&#24335;&#20248;&#21270;&#30340;&#30701;&#26399;&#35745;&#21010;&#29983;&#25104;&#21644;&#36873;&#25321;&#19982;&#22522;&#20110;DRL&#30340;&#38271;&#26399;&#39134;&#34892;&#26041;&#21521;&#30340;&#25112;&#30053;&#35843;&#24230;&#30456;&#32467;&#21512;&#12290;&#36890;&#36807;&#23545;&#20174;&#29616;&#23454;&#22478;&#24066;&#31227;&#21160;&#20013;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#34920;&#26126;&#25152;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Swarms of autonomous interactive drones, with the support of recharging technology, can provide compelling sensing capabilities in Smart Cities, such as traffic monitoring and disaster response. Existing approaches, including distributed optimization and deep reinforcement learning (DRL), aim to coordinate drones to achieve cost-effective, high-quality navigation, sensing, and charging. However, they face grand challenges: short-term optimization is not effective in dynamic environments with unanticipated changes, while long-term learning lacks scalability, resilience, and flexibility. To bridge this gap, this paper introduces a new progressive approach that combines short-term plan generation and selection based on distributed optimization with a DRL-based long-term strategic scheduling of flying direction. Extensive experimentation with datasets generated from realistic urban mobility underscores an outstanding performance of the proposed solution compared to state-of-the-art. We als
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20108;&#32500;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#36816;&#21160;&#24615;&#22330;&#26223;&#19979;&#26080;&#32447;&#36890;&#20449;&#20013;&#30340;OTFS&#31526;&#21495;&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#23558;OTFS&#31995;&#32479;&#30340;&#39046;&#22495;&#30693;&#35782;&#19982;&#22312;&#32447;&#23376;&#24103;&#31526;&#21495;&#26816;&#27979;&#30340;&#35774;&#35745;&#32467;&#21512;&#36215;&#26469;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#20108;&#32500;&#24490;&#29615;&#22635;&#20805;&#21644;&#28388;&#27874;&#32467;&#26500;&#26469;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2311.08543</link><description>&lt;p&gt;
2D-RC: &#20108;&#32500;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#29992;&#20110;OTFS&#31526;&#21495;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
2D-RC: Two-Dimensional Neural Network Approach for OTFS Symbol Detection. (arXiv:2311.08543v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.08543
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20108;&#32500;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#36816;&#21160;&#24615;&#22330;&#26223;&#19979;&#26080;&#32447;&#36890;&#20449;&#20013;&#30340;OTFS&#31526;&#21495;&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#23558;OTFS&#31995;&#32479;&#30340;&#39046;&#22495;&#30693;&#35782;&#19982;&#22312;&#32447;&#23376;&#24103;&#31526;&#21495;&#26816;&#27979;&#30340;&#35774;&#35745;&#32467;&#21512;&#36215;&#26469;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#20108;&#32500;&#24490;&#29615;&#22635;&#20805;&#21644;&#28388;&#27874;&#32467;&#26500;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27491;&#20132;&#26102;&#39057;&#31354;&#38388;&#65288;OTFS&#65289;&#26159;&#39640;&#36816;&#21160;&#24615;&#22330;&#26223;&#19979;&#26080;&#32447;&#36890;&#20449;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35843;&#21046;&#26041;&#26696;&#12290;&#26368;&#36817;&#65292;&#19968;&#31181;&#22522;&#20110;&#20648;&#23618;&#35745;&#31639;&#65288;RC&#65289;&#30340;&#26041;&#27861;&#24050;&#32463;&#24341;&#20837;&#21040;OTFS&#31995;&#32479;&#20013;&#30340;&#22312;&#32447;&#23376;&#24103;&#31526;&#21495;&#26816;&#27979;&#20013;&#65292;&#35813;&#26041;&#27861;&#21482;&#21033;&#29992;&#20102;&#26377;&#38480;&#25968;&#37327;&#30340;&#31354;&#20013;&#65288;OTA&#65289;&#23548;&#39057;&#31526;&#21495;&#29992;&#20110;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;OTFS&#31995;&#32479;&#29305;&#23450;&#30340;&#39046;&#22495;&#30693;&#35782;&#65292;&#26080;&#27861;&#20805;&#20998;&#21457;&#25381;RC&#30340;&#28508;&#21147;&#12290;&#26412;&#25991;&#20171;&#32461;&#19968;&#31181;&#26032;&#39062;&#30340;&#20108;&#32500;RC&#65288;2D-RC&#65289;&#26041;&#27861;&#65292;&#23558;OTFS&#31995;&#32479;&#30340;&#39046;&#22495;&#30693;&#35782;&#32467;&#21512;&#21040;&#22312;&#32447;&#23376;&#24103;&#31526;&#21495;&#26816;&#27979;&#30340;&#35774;&#35745;&#20013;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#30001;&#20110;&#24310;&#36831;-&#22810;&#26222;&#21202;&#65288;DD&#65289;&#22495;&#20013;&#30340;&#20449;&#36947;&#30456;&#20114;&#20316;&#29992;&#26159;&#19968;&#20010;&#20108;&#32500;&#65288;2D&#65289;&#30340;&#24490;&#29615;&#25805;&#20316;&#65292;2D-RC&#34987;&#35774;&#35745;&#20026;&#20855;&#26377;2D&#24490;&#29615;&#22635;&#20805;&#36807;&#31243;&#21644;2D&#28388;&#27874;&#32467;&#26500;&#20197;&#23884;&#20837;&#27492;&#30693;&#35782;&#12290;&#36890;&#36807;&#24341;&#20837;&#36825;&#31181;&#26550;&#26500;&#65292;2D-RC&#21487;&#20197;&#22312;DD&#22495;&#20013;&#20165;&#20351;&#29992;&#19968;&#20010;&#21333;&#19968;&#30340;&#23548;&#39057;&#31526;&#21495;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Orthogonal time frequency space (OTFS) is a promising modulation scheme for wireless communication in high-mobility scenarios. Recently, a reservoir computing (RC) based approach has been introduced for online subframe-based symbol detection in the OTFS system, where only a limited number of over-the-air (OTA) pilot symbols are utilized for training. However, this approach does not leverage the domain knowledge specific to the OTFS system to fully unlock the potential of RC. This paper introduces a novel two-dimensional RC (2D-RC) method that incorporates the domain knowledge of the OTFS system into the design for symbol detection in an online subframe-based manner. Specifically, as the channel interaction in the delay-Doppler (DD) domain is a two-dimensional (2D) circular operation, the 2D-RC is designed to have the 2D circular padding procedure and the 2D filtering structure to embed this knowledge. With the introduced architecture, 2D-RC can operate in the DD domain with only a sing
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20803;&#23398;&#20064;&#23454;&#29616;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#32534;&#36753;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#36229;&#32593;&#32476;&#26469;&#29983;&#25104;&#21442;&#25968;&#21464;&#21270;&#65292;&#36890;&#36807;&#35299;&#20915;&#26368;&#23567;&#20108;&#20056;&#38382;&#39064;&#26469;&#26356;&#26032;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#12290;&#36890;&#36807;&#23558;&#35745;&#31639;&#20998;&#31163;&#22312;&#36229;&#32593;&#32476;&#21644;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#65292;&#20351;&#24471;&#21487;&#20197;&#21516;&#26102;&#32534;&#36753;&#22810;&#20010;&#20107;&#23454;&#12290;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#26550;&#26500;&#30340;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2311.04661</link><description>&lt;p&gt;
&#36890;&#36807;&#20803;&#23398;&#20064;&#23454;&#29616;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Massive Editing for Large Language Models via Meta Learning. (arXiv:2311.04661v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.04661
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20803;&#23398;&#20064;&#23454;&#29616;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#32534;&#36753;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#36229;&#32593;&#32476;&#26469;&#29983;&#25104;&#21442;&#25968;&#21464;&#21270;&#65292;&#36890;&#36807;&#35299;&#20915;&#26368;&#23567;&#20108;&#20056;&#38382;&#39064;&#26469;&#26356;&#26032;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#12290;&#36890;&#36807;&#23558;&#35745;&#31639;&#20998;&#31163;&#22312;&#36229;&#32593;&#32476;&#21644;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#65292;&#20351;&#24471;&#21487;&#20197;&#21516;&#26102;&#32534;&#36753;&#22810;&#20010;&#20107;&#23454;&#12290;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#26550;&#26500;&#30340;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36890;&#36807;&#23545;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#23398;&#20064;&#30693;&#35782;&#25104;&#20026;&#21487;&#33021;&#65292;&#20294;&#25152;&#33719;&#24471;&#30340;&#30693;&#35782;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#21487;&#33021;&#26159;&#22522;&#26412;&#19981;&#27491;&#30830;&#25110;&#36807;&#26102;&#30340;&#65292;&#36825;&#38656;&#35201;&#22312;&#35757;&#32451;&#21518;&#32416;&#27491;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#30340;&#30693;&#35782;&#12290;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#26159;&#21033;&#29992;&#36229;&#32593;&#32476;&#29983;&#25104;&#21442;&#25968;&#20559;&#31227;&#65292;&#28982;&#32780;&#29616;&#26377;&#30340;&#36229;&#32593;&#32476;&#22312;&#21516;&#27493;&#32534;&#36753;&#25805;&#20316;&#25968;&#37327;&#26041;&#38754;&#23384;&#22312;&#25193;&#23637;&#24615;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#32534;&#36753;&#32593;&#32476;&#65288;MALMEN&#65289;&#65292;&#23427;&#23558;&#21442;&#25968;&#20559;&#31227;&#32858;&#21512;&#24418;&#24335;&#21270;&#20026;&#26368;&#23567;&#20108;&#20056;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#27491;&#35268;&#26041;&#31243;&#26356;&#26032;LM&#21442;&#25968;&#12290;&#20026;&#36866;&#24212;&#22312;&#26377;&#38480;&#20869;&#23384;&#39044;&#31639;&#19979;&#21516;&#26102;&#32534;&#36753;&#22810;&#20010;&#20107;&#23454;&#65292;&#25105;&#20204;&#23558;&#36229;&#32593;&#32476;&#21644;LM&#19978;&#30340;&#35745;&#31639;&#20998;&#31163;&#65292;&#20351;&#24471;&#20004;&#20010;&#31070;&#32463;&#32593;&#32476;&#37117;&#21487;&#20197;&#20855;&#26377;&#20219;&#24847;&#25209;&#37327;&#22823;&#23567;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#23545;&#20855;&#26377;&#19981;&#21516;&#26550;&#26500;&#30340;LM&#65288;&#20363;&#22914;BERT-base&#65289;&#36827;&#34892;&#39640;&#36798;&#25968;&#21315;&#20010;&#20107;&#23454;&#30340;&#32534;&#36753;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
While large language models (LLMs) have enabled learning knowledge from the pre-training corpora, the acquired knowledge may be fundamentally incorrect or outdated over time, which necessitates rectifying the knowledge of the language model (LM) after the training. A promising approach involves employing a hyper-network to generate parameter shift, whereas existing hyper-networks suffer from inferior scalability in synchronous editing operation amount. To mitigate the problem, we propose the MAssive Language Model Editing Network (MALMEN), which formulates the parameter shift aggregation as the least square problem, subsequently updating the LM parameters using the normal equation. To accommodate editing multiple facts simultaneously with limited memory budgets, we separate the computation on the hyper-network and LM, enabling arbitrary batch size on both neural networks. Our method is evaluated by editing up to thousands of facts on LMs with different architectures, i.e., BERT-base, G
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39044;&#27979;fMRI&#20449;&#21495;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#27491;&#24358;&#34920;&#31034;&#32593;&#32476;&#20174;EEG&#20013;&#33719;&#21462;fMRI&#20449;&#24687;&#65292;&#24357;&#34917;&#20102;&#20004;&#32773;&#20043;&#38388;&#31354;&#38388;&#20998;&#36776;&#29575;&#21644;&#25104;&#26412;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2311.04234</link><description>&lt;p&gt;
&#21033;&#29992;&#27491;&#24358;&#34920;&#31034;&#32593;&#32476;&#20174;&#33041;&#30005;&#22270;&#39044;&#27979;&#21151;&#33021;&#30913;&#20849;&#25391;&#25104;&#20687;&#20449;&#21495;
&lt;/p&gt;
&lt;p&gt;
Leveraging sinusoidal representation networks to predict fMRI signals from EEG. (arXiv:2311.04234v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.04234
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39044;&#27979;fMRI&#20449;&#21495;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#27491;&#24358;&#34920;&#31034;&#32593;&#32476;&#20174;EEG&#20013;&#33719;&#21462;fMRI&#20449;&#24687;&#65292;&#24357;&#34917;&#20102;&#20004;&#32773;&#20043;&#38388;&#31354;&#38388;&#20998;&#36776;&#29575;&#21644;&#25104;&#26412;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#31070;&#32463;&#31185;&#23398;&#20013;&#65292;&#21151;&#33021;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;fMRI&#65289;&#26159;&#19968;&#31181;&#33267;&#20851;&#37325;&#35201;&#19988;&#19981;&#21487;&#26367;&#20195;&#30340;&#24037;&#20855;&#65292;&#21487;&#25552;&#20379;&#23545;&#25972;&#20010;&#22823;&#33041;&#27963;&#21160;&#21160;&#24577;&#30340;&#38750;&#20405;&#20837;&#24615;&#31383;&#21475;&#12290;&#28982;&#32780;&#65292;fMRI&#21463;&#21040;&#34880;&#31649;&#21160;&#21147;&#23398;&#27169;&#31946;&#12289;&#39640;&#25104;&#26412;&#12289;&#22266;&#23450;&#19981;&#21160;&#20197;&#21450;&#19982;&#37329;&#23646;&#26893;&#20837;&#29289;&#19981;&#20860;&#23481;&#31561;&#38480;&#21046;&#12290;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#19982;fMRI&#20114;&#34917;&#65292;&#22312;&#39640;&#26102;&#24207;&#20998;&#36776;&#29575;&#19979;&#21487;&#20197;&#30452;&#25509;&#35760;&#24405;&#30382;&#23618;&#30005;&#27963;&#21160;&#65292;&#20294;&#31354;&#38388;&#20998;&#36776;&#29575;&#36739;&#20302;&#65292;&#26080;&#27861;&#24674;&#22797;&#26377;&#20851;&#28145;&#23618;&#30382;&#23618;&#20197;&#19979;&#32467;&#26500;&#30340;&#20449;&#24687;&#12290;&#20174;EEG&#20013;&#33719;&#21462;fMRI&#20449;&#24687;&#23558;&#23454;&#29616;&#25104;&#26412;&#25928;&#30410;&#39640;&#65292;&#23545;&#26356;&#24191;&#27867;&#30340;&#33041;&#21306;&#36827;&#34892;&#25104;&#20687;&#12290;&#27492;&#22806;&#65292;&#36328;&#27169;&#24577;&#27169;&#22411;&#36824;&#23558;&#26377;&#21161;&#20110;&#35299;&#37322;fMRI&#20449;&#21495;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;EEG&#21644;fMRI&#37117;&#26159;&#39640;&#32500;&#19988;&#23481;&#26131;&#20135;&#29983;&#20266;&#24433;&#30340;&#65292;&#30446;&#21069;&#24456;&#38590;&#20174;EEG&#24314;&#27169;fMRI&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
In modern neuroscience, functional magnetic resonance imaging (fMRI) has been a crucial and irreplaceable tool that provides a non-invasive window into the dynamics of whole-brain activity. Nevertheless, fMRI is limited by hemodynamic blurring as well as high cost, immobility, and incompatibility with metal implants. Electroencephalography (EEG) is complementary to fMRI and can directly record the cortical electrical activity at high temporal resolution, but has more limited spatial resolution and is unable to recover information about deep subcortical brain structures. The ability to obtain fMRI information from EEG would enable cost-effective, imaging across a wider set of brain regions. Further, beyond augmenting the capabilities of EEG, cross-modality models would facilitate the interpretation of fMRI signals. However, as both EEG and fMRI are high-dimensional and prone to artifacts, it is currently challenging to model fMRI from EEG. To address this challenge, we propose a novel a
&lt;/p&gt;</description></item><item><title>&#21487;&#20449;&#20219;&#30340;&#36793;&#32536;&#26426;&#22120;&#23398;&#20064;&#26159;&#36793;&#32536;&#35745;&#31639;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#34701;&#21512;&#65292;&#38754;&#20020;&#21508;&#31181;&#25361;&#25112;&#65292;&#26412;&#35843;&#26597;&#24635;&#32467;&#20102;&#23545;&#20854;&#30340;&#23450;&#20041;&#12289;&#23646;&#24615;&#12289;&#26694;&#26550;&#12289;&#25216;&#26415;&#21644;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#24378;&#35843;&#20102;&#22312;6G&#32593;&#32476;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.17944</link><description>&lt;p&gt;
&#21487;&#20449;&#20219;&#30340;&#36793;&#32536;&#26426;&#22120;&#23398;&#20064;&#65306;&#19968;&#39033;&#35843;&#26597;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Trustworthy Edge Machine Learning: A Survey. (arXiv:2310.17944v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17944
&lt;/p&gt;
&lt;p&gt;
&#21487;&#20449;&#20219;&#30340;&#36793;&#32536;&#26426;&#22120;&#23398;&#20064;&#26159;&#36793;&#32536;&#35745;&#31639;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#34701;&#21512;&#65292;&#38754;&#20020;&#21508;&#31181;&#25361;&#25112;&#65292;&#26412;&#35843;&#26597;&#24635;&#32467;&#20102;&#23545;&#20854;&#30340;&#23450;&#20041;&#12289;&#23646;&#24615;&#12289;&#26694;&#26550;&#12289;&#25216;&#26415;&#21644;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#24378;&#35843;&#20102;&#22312;6G&#32593;&#32476;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36793;&#32536;&#35745;&#31639;&#65288;EC&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#34701;&#21512;&#65292;&#21363;&#36793;&#32536;&#26426;&#22120;&#23398;&#20064;&#65288;EML&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#20998;&#24067;&#24335;&#32593;&#32476;&#36164;&#28304;&#20197;&#21512;&#20316;&#26041;&#24335;&#36827;&#34892;&#32852;&#21512;&#35757;&#32451;&#21644;&#25512;&#26029;&#65292;&#24050;&#25104;&#20026;&#19968;&#20010;&#22791;&#21463;&#20851;&#27880;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;EML&#38754;&#20020;&#36164;&#28304;&#38480;&#21046;&#12289;&#24322;&#26500;&#32593;&#32476;&#29615;&#22659;&#20197;&#21450;&#19981;&#21516;&#24212;&#29992;&#30340;&#22810;&#26679;&#21270;&#26381;&#21153;&#38656;&#27714;&#31561;&#21508;&#31181;&#25361;&#25112;&#65292;&#36825;&#20123;&#22240;&#32032;&#20849;&#21516;&#24433;&#21709;&#30528;EML&#22312;&#21033;&#30410;&#30456;&#20851;&#32773;&#30524;&#20013;&#30340;&#21487;&#20449;&#24230;&#12290;&#26412;&#35843;&#26597;&#25552;&#20379;&#20102;&#23545;&#21487;&#20449;&#20219;&#30340;EML&#23450;&#20041;&#12289;&#23646;&#24615;&#12289;&#26694;&#26550;&#12289;&#25216;&#26415;&#21644;&#35299;&#20915;&#26041;&#26696;&#30340;&#20840;&#38754;&#24635;&#32467;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#24378;&#35843;&#20102;&#22312;&#31532;&#20845;&#20195;&#65288;6G&#65289;&#32593;&#32476;&#20013;&#21487;&#20449;&#20219;&#30340;EML&#30340;&#37325;&#35201;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20174;&#37096;&#32626;&#21644;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#30340;&#25361;&#25112;&#30340;&#35282;&#24230;&#35752;&#35770;&#20102;&#21487;&#20449;&#20219;&#24615;&#30340;&#24517;&#35201;&#24615;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#21487;&#20449;&#20219;&#30340;EML&#30340;&#21021;&#27493;&#23450;&#20041;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#20851;&#38190;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The convergence of Edge Computing (EC) and Machine Learning (ML), known as Edge Machine Learning (EML), has become a highly regarded research area by utilizing distributed network resources to perform joint training and inference in a cooperative manner. However, EML faces various challenges due to resource constraints, heterogeneous network environments, and diverse service requirements of different applications, which together affect the trustworthiness of EML in the eyes of its stakeholders. This survey provides a comprehensive summary of definitions, attributes, frameworks, techniques, and solutions for trustworthy EML. Specifically, we first emphasize the importance of trustworthy EML within the context of Sixth-Generation (6G) networks. We then discuss the necessity of trustworthiness from the perspective of challenges encountered during deployment and real-world application scenarios. Subsequently, we provide a preliminary definition of trustworthy EML and explore its key attrib
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#24067;&#26009;&#25805;&#25511;&#20013;&#30340;&#27169;&#25311;&#19982;&#29616;&#23454;&#24046;&#36317;&#12290;&#36890;&#36807;&#36827;&#34892;&#21160;&#24577;&#21644;&#20934;&#38745;&#24577;&#30340;&#24067;&#26009;&#25805;&#25511;&#20219;&#21153;&#20197;&#21450;&#19982;&#21018;&#24615;&#26700;&#23376;&#30340;&#25509;&#35302;&#26469;&#25910;&#38598;&#25968;&#25454;&#12290;&#32467;&#26524;&#35780;&#20272;&#20102;&#22235;&#20010;&#27969;&#34892;&#30340;&#21487;&#21464;&#24418;&#29289;&#20307;&#27169;&#25311;&#22120;&#30340;&#29616;&#23454;&#24046;&#36317;&#12289;&#35745;&#31639;&#26102;&#38388;&#21644;&#27169;&#25311;&#31283;&#23450;&#24615;&#65292;&#24182;&#35752;&#35770;&#20102;&#27599;&#20010;&#27169;&#25311;&#22120;&#30340;&#20248;&#32570;&#28857;&#12290;</title><link>http://arxiv.org/abs/2310.09543</link><description>&lt;p&gt;
&#22312;&#24067;&#26009;&#25805;&#25511;&#20013;&#23545;&#27169;&#25311;&#19982;&#29616;&#23454;&#24046;&#36317;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking the Sim-to-Real Gap in Cloth Manipulation. (arXiv:2310.09543v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09543
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#24067;&#26009;&#25805;&#25511;&#20013;&#30340;&#27169;&#25311;&#19982;&#29616;&#23454;&#24046;&#36317;&#12290;&#36890;&#36807;&#36827;&#34892;&#21160;&#24577;&#21644;&#20934;&#38745;&#24577;&#30340;&#24067;&#26009;&#25805;&#25511;&#20219;&#21153;&#20197;&#21450;&#19982;&#21018;&#24615;&#26700;&#23376;&#30340;&#25509;&#35302;&#26469;&#25910;&#38598;&#25968;&#25454;&#12290;&#32467;&#26524;&#35780;&#20272;&#20102;&#22235;&#20010;&#27969;&#34892;&#30340;&#21487;&#21464;&#24418;&#29289;&#20307;&#27169;&#25311;&#22120;&#30340;&#29616;&#23454;&#24046;&#36317;&#12289;&#35745;&#31639;&#26102;&#38388;&#21644;&#27169;&#25311;&#31283;&#23450;&#24615;&#65292;&#24182;&#35752;&#35770;&#20102;&#27599;&#20010;&#27169;&#25311;&#22120;&#30340;&#20248;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30495;&#23454;&#30340;&#29289;&#29702;&#24341;&#25806;&#23545;&#20110;&#23398;&#20064;&#22914;&#20309;&#22312;&#27169;&#25311;&#20013;&#25805;&#25511;&#21487;&#21464;&#24418;&#29289;&#20307;&#65288;&#22914;&#26381;&#35013;&#65289;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#36991;&#20813;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#24863;&#30693;&#29289;&#20307;&#30340;&#21464;&#24418;&#31561;&#25361;&#25112;&#12290;&#23613;&#31649;&#22823;&#37327;&#20351;&#29992;&#27169;&#25311;&#26469;&#23436;&#25104;&#36825;&#39033;&#20219;&#21153;&#65292;&#20294;&#24456;&#23569;&#26377;&#24037;&#20316;&#35780;&#20272;&#21487;&#21464;&#24418;&#29289;&#20307;&#27169;&#25311;&#22120;&#21644;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#20043;&#38388;&#30340;&#30495;&#23454;&#24046;&#36317;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#24067;&#26009;&#25805;&#25511;&#20013;&#30340;&#27169;&#25311;&#19982;&#29616;&#23454;&#24046;&#36317;&#12290;&#35813;&#25968;&#25454;&#38598;&#36890;&#36807;&#36827;&#34892;&#21160;&#24577;&#21644;&#20934;&#38745;&#24577;&#30340;&#24067;&#26009;&#25805;&#25511;&#20219;&#21153;&#20197;&#21450;&#19982;&#21018;&#24615;&#26700;&#23376;&#30340;&#25509;&#35302;&#26469;&#25910;&#38598;&#12290;&#25105;&#20204;&#20351;&#29992;&#35813;&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;&#22235;&#20010;&#27969;&#34892;&#30340;&#21487;&#21464;&#24418;&#29289;&#20307;&#27169;&#25311;&#22120;&#8212;&#8212;MuJoCo&#12289;Bullet&#12289;Flex&#21644;SOFA&#30340;&#29616;&#23454;&#24046;&#36317;&#12289;&#35745;&#31639;&#26102;&#38388;&#21644;&#27169;&#25311;&#31283;&#23450;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#27599;&#20010;&#27169;&#25311;&#22120;&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#12290;&#35813;&#22522;&#20934;&#25968;&#25454;&#38598;&#26159;&#24320;&#28304;&#30340;&#12290;&#38468;&#21152;&#26448;&#26009;&#12289;&#35270;&#39057;&#21644;&#20195;&#30721;&#21487;&#20197;&#22312; https://sites.google.com/view/cloth-sim2r &#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Realistic physics engines play a crucial role for learning to manipulate deformable objects such as garments in simulation. By doing so, researchers can circumvent challenges such as sensing the deformation of the object in the realworld. In spite of the extensive use of simulations for this task, few works have evaluated the reality gap between deformable object simulators and real-world data. We present a benchmark dataset to evaluate the sim-to-real gap in cloth manipulation. The dataset is collected by performing a dynamic as well as a quasi-static cloth manipulation task involving contact with a rigid table. We use the dataset to evaluate the reality gap, computational time, and simulation stability of four popular deformable object simulators: MuJoCo, Bullet, Flex, and SOFA. Additionally, we discuss the benefits and drawbacks of each simulator. The benchmark dataset is open-source. Supplementary material, videos, and code, can be found at https://sites.google.com/view/cloth-sim2r
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36801;&#31227;&#23398;&#20064;&#30340;EMR&#25968;&#25454;&#38598;&#20043;&#38388;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#30340;&#39044;&#21518;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#26500;&#24314;&#36807;&#28193;&#27169;&#22411;&#35299;&#20915;&#20102;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#29305;&#24449;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.07799</link><description>&lt;p&gt;
&#22522;&#20110;&#36801;&#31227;&#23398;&#20064;&#30340;EMR&#25968;&#25454;&#38598;&#20043;&#38388;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#30340;&#39044;&#21518;&#39044;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Transfer-Learning-Based Prognosis Prediction Paradigm that Bridges Data Distribution Shift across EMR Datasets. (arXiv:2310.07799v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07799
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36801;&#31227;&#23398;&#20064;&#30340;EMR&#25968;&#25454;&#38598;&#20043;&#38388;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#30340;&#39044;&#21518;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#26500;&#24314;&#36807;&#28193;&#27169;&#22411;&#35299;&#20915;&#20102;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#29305;&#24449;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#23545;&#26032;&#20852;&#30142;&#30149;&#30340;&#20449;&#24687;&#26377;&#38480;&#65292;&#30151;&#29366;&#24456;&#38590;&#34987;&#23519;&#35273;&#21644;&#35748;&#35782;&#21040;&#65292;&#22240;&#27492;&#21487;&#33021;&#24573;&#35270;&#20020;&#24202;&#24178;&#39044;&#30340;&#31383;&#21475;&#12290;&#26399;&#26395;&#33021;&#22815;&#24314;&#31435;&#19968;&#20010;&#26377;&#25928;&#30340;&#39044;&#21518;&#27169;&#22411;&#65292;&#36741;&#21161;&#21307;&#29983;&#36827;&#34892;&#27491;&#30830;&#35786;&#26029;&#21644;&#21046;&#23450;&#20010;&#24615;&#21270;&#27835;&#30103;&#26041;&#26696;&#65292;&#20174;&#32780;&#21450;&#26102;&#39044;&#38450;&#19981;&#21033;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22312;&#30142;&#30149;&#26089;&#26399;&#38454;&#27573;&#65292;&#30001;&#20110;&#25968;&#25454;&#25910;&#38598;&#21644;&#20020;&#24202;&#32463;&#39564;&#26377;&#38480;&#65292;&#20877;&#21152;&#19978;&#23545;&#38544;&#31169;&#21644;&#20262;&#29702;&#30340;&#32771;&#34385;&#65292;&#23548;&#33268;&#21487;&#20379;&#21442;&#32771;&#30340;&#25968;&#25454;&#21463;&#38480;&#65292;&#29978;&#33267;&#38590;&#20197;&#27491;&#30830;&#26631;&#35760;&#25968;&#25454;&#26631;&#31614;&#12290;&#27492;&#22806;&#65292;&#19981;&#21516;&#30142;&#30149;&#25110;&#21516;&#19968;&#30142;&#30149;&#19981;&#21516;&#26469;&#28304;&#30340;&#30005;&#23376;&#21307;&#30103;&#35760;&#24405;&#65288;EMR&#65289;&#25968;&#25454;&#21487;&#33021;&#23384;&#22312;&#20005;&#37325;&#30340;&#36328;&#25968;&#25454;&#38598;&#29305;&#24449;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#20005;&#37325;&#24433;&#21709;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#24314;&#31435;&#19968;&#20010;&#20174;&#28304;&#25968;&#25454;&#38598;&#21040;&#30446;&#26631;&#25968;&#25454;&#38598;&#30340;&#36807;&#28193;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#29305;&#24449;&#36827;&#34892;&#32422;&#26463;&#65292;&#26469;&#35299;&#20915;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the limited information about emerging diseases, symptoms are hard to be noticed and recognized, so that the window for clinical intervention could be ignored. An effective prognostic model is expected to assist doctors in making right diagnosis and designing personalized treatment plan, so to promptly prevent unfavorable outcomes. However, in the early stage of a disease, limited data collection and clinical experiences, plus the concern out of privacy and ethics, may result in restricted data availability for reference, to the extent that even data labels are difficult to mark correctly. In addition, Electronic Medical Record (EMR) data of different diseases or of different sources of the same disease can prove to be having serious cross-dataset feature misalignment problems, greatly mutilating the efficiency of deep learning models. This article introduces a transfer learning method to build a transition model from source dataset to target dataset. By way of constraining the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38754;&#37096;&#21160;&#20316;&#21333;&#20301;&#65288;AU&#65289;&#26816;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#20849;&#20139;&#21442;&#25968;&#21644;&#24341;&#20837;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#22312;&#38754;&#37096;&#26631;&#24535;&#26816;&#27979;&#21644;AU&#22495;&#20998;&#31163;&#19982;&#37325;&#24314;&#20043;&#38388;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#26041;&#27861;&#22312;&#37326;&#22806;AU&#26816;&#27979;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.05207</link><description>&lt;p&gt;
&#36890;&#36807;&#21516;&#26102;&#23398;&#20064;&#38754;&#37096;&#26631;&#24535;&#26816;&#27979;&#12289;&#22495;&#20998;&#31163;&#21644;&#37325;&#24314;&#26469;&#25552;&#39640;&#38754;&#37096;&#21160;&#20316;&#21333;&#20301;&#26816;&#27979;&#30340;&#31934;&#24230;
&lt;/p&gt;
&lt;p&gt;
Boosting Facial Action Unit Detection Through Jointly Learning Facial Landmark Detection and Domain Separation and Reconstruction. (arXiv:2310.05207v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05207
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38754;&#37096;&#21160;&#20316;&#21333;&#20301;&#65288;AU&#65289;&#26816;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#20849;&#20139;&#21442;&#25968;&#21644;&#24341;&#20837;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#22312;&#38754;&#37096;&#26631;&#24535;&#26816;&#27979;&#21644;AU&#22495;&#20998;&#31163;&#19982;&#37325;&#24314;&#20043;&#38388;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#26041;&#27861;&#22312;&#37326;&#22806;AU&#26816;&#27979;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22914;&#20309;&#23558;&#22823;&#37327;&#30340;&#22312;&#37326;&#38750;&#26631;&#35760;&#38754;&#37096;&#22270;&#20687;&#24341;&#20837;&#30417;&#30563;&#24335;&#38754;&#37096;&#21160;&#20316;&#21333;&#20301;&#65288;AU&#65289;&#26816;&#27979;&#26694;&#26550;&#20013;&#25104;&#20026;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;AU&#26816;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#20849;&#20139;&#21516;&#26500;&#38754;&#37096;&#25552;&#21462;&#27169;&#22359;&#30340;&#21442;&#25968;&#65292;&#24341;&#20837;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#21516;&#26102;&#23398;&#20064;AU&#22495;&#20998;&#31163;&#21644;&#37325;&#24314;&#20197;&#21450;&#38754;&#37096;&#26631;&#24535;&#26816;&#27979;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#26032;&#29305;&#24449;&#23545;&#40784;&#26041;&#26696;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#25237;&#24433;&#22120;&#21644;&#25913;&#36827;&#30340;&#23545;&#27604;&#25439;&#22833;&#28155;&#21152;&#20102;&#22235;&#20010;&#39069;&#22806;&#30340;&#20013;&#38388;&#30417;&#30563;&#22120;&#26469;&#20419;&#36827;&#29305;&#24449;&#37325;&#24314;&#30340;&#36807;&#31243;&#12290;&#22312;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#22312;&#37326;&#22806;AU&#26816;&#27979;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently how to introduce large amounts of unlabeled facial images in the wild into supervised Facial Action Unit (AU) detection frameworks has become a challenging problem. In this paper, we propose a new AU detection framework where multi-task learning is introduced to jointly learn AU domain separation and reconstruction and facial landmark detection by sharing the parameters of homostructural facial extraction modules. In addition, we propose a new feature alignment scheme based on contrastive learning by simple projectors and an improved contrastive loss, which adds four additional intermediate supervisors to promote the feature reconstruction process. Experimental results on two benchmarks demonstrate our superiority against the state-of-the-art methods for AU detection in the wild.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26426;&#23494;&#30340;&#25968;&#25454;&#36873;&#25321;&#21644;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;&#27969;&#31243;&#21644;&#31616;&#21270;&#30340;&#20302;&#32500;&#24230;&#25805;&#20316;&#26469;&#23454;&#29616;&#65292;&#20197;&#20445;&#25252;&#25968;&#25454;&#21644;&#27169;&#22411;&#30340;&#38544;&#31169;&#65292;&#24182;&#22312;&#22810;&#20010;Transformer&#27169;&#22411;&#21644;NLP/CV&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2310.02373</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#30340;&#23433;&#20840;&#26377;&#25928;&#25968;&#25454;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Secure and Effective Data Appraisal for Machine Learning. (arXiv:2310.02373v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02373
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26426;&#23494;&#30340;&#25968;&#25454;&#36873;&#25321;&#21644;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;&#27969;&#31243;&#21644;&#31616;&#21270;&#30340;&#20302;&#32500;&#24230;&#25805;&#20316;&#26469;&#23454;&#29616;&#65292;&#20197;&#20445;&#25252;&#25968;&#25454;&#21644;&#27169;&#22411;&#30340;&#38544;&#31169;&#65292;&#24182;&#22312;&#22810;&#20010;Transformer&#27169;&#22411;&#21644;NLP/CV&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#26080;&#25304;&#26080;&#26463;&#30340;&#25968;&#25454;&#24066;&#22330;&#38656;&#35201;&#22312;&#25968;&#25454;&#25152;&#26377;&#32773;&#21644;&#27169;&#22411;&#25152;&#26377;&#32773;&#26368;&#32456;&#20132;&#26131;&#21069;&#33021;&#22815;&#23545;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#31169;&#23494;&#36873;&#25321;&#21644;&#35780;&#20272;&#12290;&#20026;&#20102;&#20445;&#25252;&#25968;&#25454;&#21644;&#27169;&#22411;&#30340;&#38544;&#31169;&#65292;&#36825;&#20010;&#36807;&#31243;&#28041;&#21450;&#20351;&#29992;&#22810;&#26041;&#35745;&#31639;(MPC)&#26469;&#23457;&#26597;&#30446;&#26631;&#27169;&#22411;&#12290;&#23613;&#31649;&#20043;&#21069;&#30340;&#30740;&#31350;&#35748;&#20026;&#22522;&#20110;MPC&#30340;Transformer&#27169;&#22411;&#35780;&#20272;&#36807;&#20110;&#32791;&#36153;&#36164;&#28304;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#25968;&#25454;&#36873;&#25321;&#25104;&#20026;&#21487;&#34892;&#30340;&#12290;&#26412;&#30740;&#31350;&#30340;&#36129;&#29486;&#21253;&#25324;&#19977;&#20010;&#20851;&#38190;&#35201;&#32032;&#65306;(1)&#20351;&#29992;MPC&#36827;&#34892;&#26426;&#23494;&#25968;&#25454;&#36873;&#25321;&#30340;&#24320;&#21019;&#24615;&#27969;&#31243;&#65307;(2)&#36890;&#36807;&#22312;&#26377;&#38480;&#30340;&#30456;&#20851;&#25968;&#25454;&#23376;&#38598;&#19978;&#35757;&#32451;&#31616;&#21270;&#30340;&#20302;&#32500;&#24230;MLP&#26469;&#22797;&#21046;&#22797;&#26434;&#30340;&#39640;&#32500;&#24230;&#25805;&#20316;&#65307;(3)&#24182;&#21457;&#12289;&#22810;&#38454;&#27573;&#22320;&#23454;&#29616;MPC&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#19968;&#31995;&#21015;Transformer&#27169;&#22411;&#21644;NLP/CV&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#19982;&#30452;&#25509;&#22522;&#20110;MPC&#30340;&#35780;&#20272;&#30456;&#27604;
&lt;/p&gt;
&lt;p&gt;
Essential for an unfettered data market is the ability to discreetly select and evaluate training data before finalizing a transaction between the data owner and model owner. To safeguard the privacy of both data and model, this process involves scrutinizing the target model through Multi-Party Computation (MPC). While prior research has posited that the MPC-based evaluation of Transformer models is excessively resource-intensive, this paper introduces an innovative approach that renders data selection practical. The contributions of this study encompass three pivotal elements: (1) a groundbreaking pipeline for confidential data selection using MPC, (2) replicating intricate high-dimensional operations with simplified low-dimensional MLPs trained on a limited subset of pertinent data, and (3) implementing MPC in a concurrent, multi-phase manner. The proposed method is assessed across an array of Transformer models and NLP/CV benchmarks. In comparison to the direct MPC-based evaluation 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;MIML&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#23558;&#26080;&#26631;&#35760;&#32454;&#32990;&#22270;&#20687;&#19982;&#29983;&#29289;&#21147;&#23398;&#23646;&#24615;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#39640;&#31934;&#24230;&#32454;&#32990;&#20998;&#31867;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#24418;&#24577;&#20449;&#24687;&#65292;&#23558;&#32454;&#32990;&#23646;&#24615;&#29702;&#35299;&#24471;&#26356;&#20840;&#38754;&#65292;&#30456;&#36739;&#20110;&#20165;&#32771;&#34385;&#21333;&#19968;&#25968;&#25454;&#31867;&#22411;&#30340;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;98.3&#65285;&#30340;&#20998;&#31867;&#31934;&#24230;&#12290;&#35813;&#26041;&#27861;&#24050;&#22312;&#30333;&#32454;&#32990;&#21644;&#32959;&#30244;&#32454;&#32990;&#20998;&#31867;&#20013;&#24471;&#21040;&#35777;&#26126;&#65292;&#24182;&#20855;&#26377;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.08421</link><description>&lt;p&gt;
MIML: &#36890;&#36807;&#24494;&#27969;&#25511;&#31995;&#32479;&#20869;&#30340;&#26426;&#26800;&#29305;&#24615;&#23545;&#39640;&#31934;&#24230;&#32454;&#32990;&#20998;&#31867;&#36827;&#34892;&#22810;&#37325;&#22270;&#20687;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
MIML: Multiplex Image Machine Learning for High Precision Cell Classification via Mechanical Traits within Microfluidic Systems. (arXiv:2309.08421v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08421
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;MIML&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#23558;&#26080;&#26631;&#35760;&#32454;&#32990;&#22270;&#20687;&#19982;&#29983;&#29289;&#21147;&#23398;&#23646;&#24615;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#39640;&#31934;&#24230;&#32454;&#32990;&#20998;&#31867;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#24418;&#24577;&#20449;&#24687;&#65292;&#23558;&#32454;&#32990;&#23646;&#24615;&#29702;&#35299;&#24471;&#26356;&#20840;&#38754;&#65292;&#30456;&#36739;&#20110;&#20165;&#32771;&#34385;&#21333;&#19968;&#25968;&#25454;&#31867;&#22411;&#30340;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;98.3&#65285;&#30340;&#20998;&#31867;&#31934;&#24230;&#12290;&#35813;&#26041;&#27861;&#24050;&#22312;&#30333;&#32454;&#32990;&#21644;&#32959;&#30244;&#32454;&#32990;&#20998;&#31867;&#20013;&#24471;&#21040;&#35777;&#26126;&#65292;&#24182;&#20855;&#26377;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#26631;&#35760;&#32454;&#32990;&#20998;&#31867;&#26377;&#21161;&#20110;&#20026;&#36827;&#19968;&#27493;&#20351;&#29992;&#25110;&#26816;&#26597;&#25552;&#20379;&#21407;&#22987;&#32454;&#32990;&#65292;&#28982;&#32780;&#29616;&#26377;&#25216;&#26415;&#22312;&#29305;&#24322;&#24615;&#21644;&#36895;&#24230;&#26041;&#38754;&#24448;&#24448;&#19981;&#36275;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24320;&#21457;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;MIML&#26469;&#35299;&#20915;&#36825;&#20123;&#23616;&#38480;&#24615;&#12290;&#35813;&#26550;&#26500;&#23558;&#26080;&#26631;&#35760;&#32454;&#32990;&#22270;&#20687;&#19982;&#29983;&#29289;&#21147;&#23398;&#23646;&#24615;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#21033;&#29992;&#27599;&#20010;&#32454;&#32990;&#22266;&#26377;&#30340;&#24191;&#38420;&#19988;&#24120;&#24120;&#34987;&#20302;&#20272;&#30340;&#24418;&#24577;&#20449;&#24687;&#12290;&#36890;&#36807;&#25972;&#21512;&#36825;&#20004;&#31181;&#31867;&#22411;&#30340;&#25968;&#25454;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#23545;&#32454;&#32990;&#23646;&#24615;&#26356;&#20840;&#38754;&#30340;&#29702;&#35299;&#65292;&#21033;&#29992;&#20102;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#36890;&#24120;&#34987;&#20002;&#24323;&#30340;&#24418;&#24577;&#20449;&#24687;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#32454;&#32990;&#20998;&#31867;&#31934;&#24230;&#36798;&#21040;&#20102;&#24778;&#20154;&#30340;98.3&#65285;&#65292;&#22823;&#22823;&#20248;&#20110;&#20165;&#32771;&#34385;&#21333;&#19968;&#25968;&#25454;&#31867;&#22411;&#30340;&#27169;&#22411;&#12290;MIML&#24050;&#34987;&#35777;&#26126;&#22312;&#30333;&#32454;&#32990;&#21644;&#32959;&#30244;&#32454;&#32990;&#20998;&#31867;&#20013;&#26377;&#25928;&#65292;&#24182;&#20855;&#26377;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Label-free cell classification is advantageous for supplying pristine cells for further use or examination, yet existing techniques frequently fall short in terms of specificity and speed. In this study, we address these limitations through the development of a novel machine learning framework, Multiplex Image Machine Learning (MIML). This architecture uniquely combines label-free cell images with biomechanical property data, harnessing the vast, often underutilized morphological information intrinsic to each cell. By integrating both types of data, our model offers a more holistic understanding of the cellular properties, utilizing morphological information typically discarded in traditional machine learning models. This approach has led to a remarkable 98.3\% accuracy in cell classification, a substantial improvement over models that only consider a single data type. MIML has been proven effective in classifying white blood cells and tumor cells, with potential for broader applicatio
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#22823;&#19988;&#31616;&#21333;&#30340;&#28145;&#24230;&#23398;&#20064;&#22522;&#32447;EEG-SimpleConv&#65292;&#29992;&#20110;BCI&#20013;&#30340;&#36816;&#21160;&#24819;&#35937;&#35299;&#30721;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;EEG-SimpleConv&#34920;&#29616;&#33267;&#23569;&#21516;&#26679;&#22909;&#25110;&#26356;&#39640;&#25928;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#30693;&#35782;&#20256;&#36882;&#33021;&#21147;&#65292;&#25512;&#29702;&#26102;&#38388;&#36739;&#20302;&#12290;</title><link>http://arxiv.org/abs/2309.07159</link><description>&lt;p&gt;
&#19968;&#31181;&#24378;&#22823;&#19988;&#31616;&#21333;&#30340;BCI MI&#35299;&#30721;&#30340;&#28145;&#24230;&#23398;&#20064;&#22522;&#32447;
&lt;/p&gt;
&lt;p&gt;
A Strong and Simple Deep Learning Baseline for BCI MI Decoding. (arXiv:2309.07159v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07159
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#22823;&#19988;&#31616;&#21333;&#30340;&#28145;&#24230;&#23398;&#20064;&#22522;&#32447;EEG-SimpleConv&#65292;&#29992;&#20110;BCI&#20013;&#30340;&#36816;&#21160;&#24819;&#35937;&#35299;&#30721;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;EEG-SimpleConv&#34920;&#29616;&#33267;&#23569;&#21516;&#26679;&#22909;&#25110;&#26356;&#39640;&#25928;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#30693;&#35782;&#20256;&#36882;&#33021;&#21147;&#65292;&#25512;&#29702;&#26102;&#38388;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;EEG-SimpleConv&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;BCI&#20013;&#36816;&#21160;&#24819;&#35937;&#35299;&#30721;&#30340;&#30452;&#35266;&#30340;&#19968;&#32500;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#21160;&#26426;&#26159;&#25552;&#20986;&#19968;&#20010;&#38750;&#24120;&#31616;&#21333;&#30340;&#22522;&#32447;&#26469;&#36827;&#34892;&#27604;&#36739;&#65292;&#20165;&#20351;&#29992;&#25991;&#29486;&#20013;&#38750;&#24120;&#26631;&#20934;&#30340;&#20803;&#32032;&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;EEG&#36816;&#21160;&#24819;&#35937;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20854;&#24615;&#33021;&#65292;&#21253;&#25324;&#27169;&#25311;&#22312;&#32447;&#35774;&#32622;&#65292;&#24182;&#23558;&#20854;&#19982;&#26368;&#36817;&#30340;&#28145;&#24230;&#23398;&#20064;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;EEG-SimpleConv&#33267;&#23569;&#19982;&#20854;&#20182;&#26041;&#27861;&#19968;&#26679;&#22909;&#29978;&#33267;&#26356;&#39640;&#25928;&#65292;&#22312;&#20027;&#20307;&#38388;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#30693;&#35782;&#20256;&#36882;&#33021;&#21147;&#65292;&#21516;&#26102;&#20855;&#26377;&#20302;&#25512;&#29702;&#26102;&#38388;&#30340;&#25104;&#26412;&#12290;&#25105;&#20204;&#20027;&#24352;&#20351;&#29992;&#29616;&#25104;&#30340;&#20803;&#32032;&#32780;&#19981;&#26159;&#25552;&#20986;&#29305;&#23450;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#26174;&#33879;&#24110;&#21161;BCI&#20013;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#37319;&#29992;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#27169;&#22411;&#21644;&#23454;&#39564;&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose EEG-SimpleConv, a straightforward 1D convolutional neural network for Motor Imagery decoding in BCI. Our main motivation is to propose a very simple baseline to compare to, using only very standard ingredients from the literature. We evaluate its performance on four EEG Motor Imagery datasets, including simulated online setups, and compare it to recent Deep Learning and Machine Learning approaches. EEG-SimpleConv is at least as good or far more efficient than other approaches, showing strong knowledge-transfer capabilities across subjects, at the cost of a low inference time. We advocate that using off-the-shelf ingredients rather than coming with ad-hoc solutions can significantly help the adoption of Deep Learning approaches for BCI. We make the code of the models and the experiments accessible.
&lt;/p&gt;</description></item><item><title>&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32447;&#35774;&#32622;&#19979;&#23398;&#20064;&#26080;&#38480;&#32500;&#32447;&#24615;&#31639;&#23376;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#19968;&#23450;&#30340;&#26465;&#20214;&#19979;&#65292;&#32447;&#24615;&#31639;&#23376;&#26159;&#21487;&#20197;&#22312;&#32447;&#23398;&#20064;&#30340;&#65292;&#32780;&#22312;&#21478;&#19968;&#20123;&#26465;&#20214;&#19979;&#21017;&#19981;&#21487;&#20197;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#22312;&#32447;&#22343;&#19968;&#25910;&#25947;&#21644;&#23398;&#20064;&#33021;&#21147;&#20043;&#38388;&#30340;&#20998;&#31163;&#65292;&#24182;&#22312;PAC&#35774;&#32622;&#19979;&#24471;&#21040;&#20102;&#30456;&#21516;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.06548</link><description>&lt;p&gt;
&#22312;&#22312;&#32447;&#35774;&#32622;&#19979;&#23398;&#20064;&#32447;&#24615;&#31639;&#23376;&#30340;&#26080;&#38480;&#32500;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Online Infinite-Dimensional Regression: Learning Linear Operators. (arXiv:2309.06548v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06548
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32447;&#35774;&#32622;&#19979;&#23398;&#20064;&#26080;&#38480;&#32500;&#32447;&#24615;&#31639;&#23376;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#19968;&#23450;&#30340;&#26465;&#20214;&#19979;&#65292;&#32447;&#24615;&#31639;&#23376;&#26159;&#21487;&#20197;&#22312;&#32447;&#23398;&#20064;&#30340;&#65292;&#32780;&#22312;&#21478;&#19968;&#20123;&#26465;&#20214;&#19979;&#21017;&#19981;&#21487;&#20197;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#22312;&#32447;&#22343;&#19968;&#25910;&#25947;&#21644;&#23398;&#20064;&#33021;&#21147;&#20043;&#38388;&#30340;&#20998;&#31163;&#65292;&#24182;&#22312;PAC&#35774;&#32622;&#19979;&#24471;&#21040;&#20102;&#30456;&#21516;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#32447;&#35774;&#32622;&#19979;&#23398;&#20064;&#20004;&#20010;&#26080;&#38480;&#32500;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20043;&#38388;&#30340;&#32447;&#24615;&#31639;&#23376;&#38382;&#39064;&#65292;&#36890;&#36807;&#26368;&#23567;&#20108;&#20056;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#23398;&#20064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;$p \in [1, \infty)$&#33539;&#22260;&#20869;&#65292;&#20855;&#26377;&#22343;&#21248;&#26377;&#30028;$p$-Schatten&#33539;&#25968;&#30340;&#32447;&#24615;&#31639;&#23376;&#31867;&#26159;&#21487;&#20197;&#22312;&#32447;&#23398;&#20064;&#30340;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20855;&#26377;&#22343;&#21248;&#26377;&#30028;&#31639;&#23376;&#33539;&#25968;&#30340;&#32447;&#24615;&#31639;&#23376;&#31867;\textit{&#19981;}&#26159;&#21487;&#20197;&#22312;&#32447;&#23398;&#20064;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#25214;&#21040;&#19968;&#31867;&#26377;&#30028;&#32447;&#24615;&#31639;&#23376;&#65292;&#35777;&#26126;&#20102;&#22312;&#32447;&#22343;&#19968;&#25910;&#25947;&#21644;&#23398;&#20064;&#33021;&#21147;&#20043;&#38388;&#30340;&#20998;&#31163;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19981;&#21487;&#33021;&#24615;&#32467;&#26524;&#21644;&#22343;&#19968;&#25910;&#25947;&#19982;&#23398;&#20064;&#33021;&#21147;&#20043;&#38388;&#30340;&#20998;&#31163;&#22312;PAC&#35774;&#32622;&#19979;&#21516;&#26679;&#25104;&#31435;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of learning linear operators under squared loss between two infinite-dimensional Hilbert spaces in the online setting. We show that the class of linear operators with uniformly bounded $p$-Schatten norm is online learnable for any $p \in [1, \infty)$. On the other hand, we prove an impossibility result by showing that the class of uniformly bounded linear operators with respect to the operator norm is \textit{not} online learnable. Moreover, we show a separation between online uniform convergence and online learnability by identifying a class of bounded linear operators that is online learnable but uniform convergence does not hold. Finally, we prove that the impossibility result and the separation between uniform convergence and learnability also hold in the agnostic PAC setting.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21407;&#22411;&#24739;&#32773;&#34920;&#31034;&#21644;&#29305;&#24449;&#32570;&#22833;&#24863;&#30693;&#26657;&#20934;&#30340;&#38388;&#25509;&#25554;&#34917;&#26041;&#27861;&#65292;&#20197;&#32531;&#35299;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#31232;&#30095;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#33719;&#21462;&#26356;&#23494;&#38598;&#30340;&#23884;&#20837;&#26469;&#25552;&#39640;&#39044;&#27979;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.04160</link><description>&lt;p&gt;
&#21033;&#29992;&#29305;&#24449;&#32570;&#22833;&#24863;&#30693;&#26657;&#20934;&#30340;&#21407;&#22411;&#24739;&#32773;&#34920;&#31034;&#26469;&#32531;&#35299;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#31232;&#30095;&#24615;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Leveraging Prototype Patient Representations with Feature-Missing-Aware Calibration to Mitigate EHR Data Sparsity. (arXiv:2309.04160v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04160
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21407;&#22411;&#24739;&#32773;&#34920;&#31034;&#21644;&#29305;&#24449;&#32570;&#22833;&#24863;&#30693;&#26657;&#20934;&#30340;&#38388;&#25509;&#25554;&#34917;&#26041;&#27861;&#65292;&#20197;&#32531;&#35299;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#31232;&#30095;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#33719;&#21462;&#26356;&#23494;&#38598;&#30340;&#23884;&#20837;&#26469;&#25552;&#39640;&#39044;&#27979;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#25968;&#25454;&#32463;&#24120;&#21576;&#29616;&#31232;&#30095;&#29305;&#24449;&#65292;&#32473;&#39044;&#27979;&#24314;&#27169;&#24102;&#26469;&#25361;&#25112;&#12290;&#24403;&#21069;&#30340;&#30452;&#25509;&#25554;&#34917;&#26041;&#27861;&#65288;&#22914;&#30697;&#38453;&#25554;&#34917;&#26041;&#27861;&#65289;&#20381;&#36182;&#20110;&#21442;&#32771;&#31867;&#20284;&#34892;&#25110;&#21015;&#26469;&#23436;&#25104;&#21407;&#22987;&#32570;&#22833;&#25968;&#25454;&#65292;&#19981;&#21306;&#20998;&#25554;&#34917;&#21644;&#23454;&#38469;&#20540;&#12290;&#22240;&#27492;&#65292;&#27169;&#22411;&#21487;&#33021;&#20250;&#26080;&#24847;&#20013;&#23558;&#19982;&#39044;&#27979;&#30446;&#26631;&#26080;&#20851;&#30340;&#25110;&#20855;&#26377;&#27450;&#39575;&#24615;&#30340;&#20449;&#24687;&#32435;&#20837;&#20854;&#20013;&#65292;&#20174;&#32780;&#25439;&#23475;&#19979;&#28216;&#24615;&#33021;&#30340;&#25928;&#26524;&#12290;&#34429;&#28982;&#19968;&#20123;&#26041;&#27861;&#23581;&#35797;&#22312;&#30452;&#25509;&#25554;&#34917;&#21518;&#37325;&#26032;&#26657;&#20934;&#25110;&#22686;&#24378;EHR&#23884;&#20837;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#38169;&#35823;&#22320;&#20248;&#20808;&#32771;&#34385;&#25554;&#34917;&#29305;&#24449;&#12290;&#36825;&#31181;&#20248;&#20808;&#38169;&#35823;&#21487;&#33021;&#20250;&#32473;&#27169;&#22411;&#24341;&#20837;&#20559;&#35265;&#25110;&#19981;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#37319;&#29992;&#38388;&#25509;&#25554;&#34917;&#65292;&#21033;&#29992;&#31867;&#20284;&#24739;&#32773;&#30340;&#21407;&#22411;&#34920;&#31034;&#33719;&#21462;&#26356;&#23494;&#38598;&#30340;&#23884;&#20837;&#12290;&#35748;&#35782;&#21040;&#22312;&#34913;&#37327;&#26102;&#36890;&#24120;&#23558;&#32570;&#22833;&#29305;&#24449;&#19982;&#23384;&#22312;&#29305;&#24449;&#30456;&#21516;&#30340;&#38480;&#21046;&#26102;
&lt;/p&gt;
&lt;p&gt;
Electronic Health Record (EHR) data frequently exhibits sparse characteristics, posing challenges for predictive modeling. Current direct imputation such as matrix imputation approaches hinge on referencing analogous rows or columns to complete raw missing data and do not differentiate between imputed and actual values. As a result, models may inadvertently incorporate irrelevant or deceptive information with respect to the prediction objective, thereby compromising the efficacy of downstream performance. While some methods strive to recalibrate or augment EHR embeddings after direct imputation, they often mistakenly prioritize imputed features. This misprioritization can introduce biases or inaccuracies into the model. To tackle these issues, our work resorts to indirect imputation, where we leverage prototype representations from similar patients to obtain a denser embedding. Recognizing the limitation that missing features are typically treated the same as present ones when measurin
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20020;&#26102;&#24402;&#32435;&#36335;&#24452;&#31070;&#32463;&#32593;&#32476;&#65288;TiPNN&#65289;&#29992;&#20110;&#26102;&#38388;&#30693;&#35782;&#22270;&#30340;&#25512;&#29702;&#65292;&#37319;&#29992;&#23454;&#20307;&#29420;&#31435;&#30340;&#35282;&#24230;&#24314;&#27169;&#21382;&#21490;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#20020;&#26102;&#24402;&#32435;&#36335;&#24452;&#25552;&#21462;&#32467;&#26500;&#21644;&#26102;&#38388;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2309.03251</link><description>&lt;p&gt;
&#20020;&#26102;&#24402;&#32435;&#36335;&#24452;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#26102;&#38388;&#30693;&#35782;&#22270;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Temporal Inductive Path Neural Network for Temporal Knowledge Graph Reasoning. (arXiv:2309.03251v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03251
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20020;&#26102;&#24402;&#32435;&#36335;&#24452;&#31070;&#32463;&#32593;&#32476;&#65288;TiPNN&#65289;&#29992;&#20110;&#26102;&#38388;&#30693;&#35782;&#22270;&#30340;&#25512;&#29702;&#65292;&#37319;&#29992;&#23454;&#20307;&#29420;&#31435;&#30340;&#35282;&#24230;&#24314;&#27169;&#21382;&#21490;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#20020;&#26102;&#24402;&#32435;&#36335;&#24452;&#25552;&#21462;&#32467;&#26500;&#21644;&#26102;&#38388;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#30693;&#35782;&#22270;&#65288;TKG&#65289;&#26159;&#20256;&#32479;&#30693;&#35782;&#22270;&#65288;KG&#65289;&#30340;&#25193;&#23637;&#65292;&#34701;&#20837;&#20102;&#26102;&#38388;&#32500;&#24230;&#12290;&#22312;TKGs&#19978;&#36827;&#34892;&#25512;&#29702;&#26159;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#65292;&#26088;&#22312;&#22522;&#20110;&#21382;&#21490;&#20107;&#20214;&#39044;&#27979;&#26410;&#26469;&#20107;&#23454;&#12290;&#20851;&#38190;&#25361;&#25112;&#22312;&#20110;&#25581;&#31034;&#21382;&#21490;&#23376;&#22270;&#21644;&#26102;&#38388;&#27169;&#24335;&#20013;&#30340;&#32467;&#26500;&#20381;&#36182;&#20851;&#31995;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20381;&#38752;&#23454;&#20307;&#24314;&#27169;&#26469;&#27169;&#25311;TKGs&#65292;&#22240;&#20026;&#22270;&#20013;&#30340;&#33410;&#28857;&#22312;&#30693;&#35782;&#34920;&#31034;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#22330;&#26223;&#36890;&#24120;&#28041;&#21450;&#22823;&#37327;&#23454;&#20307;&#65292;&#24182;&#19988;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#20250;&#20986;&#29616;&#26032;&#23454;&#20307;&#12290;&#36825;&#20351;&#24471;&#20381;&#36182;&#20110;&#23454;&#20307;&#30340;&#26041;&#27861;&#24456;&#38590;&#24212;&#23545;&#22823;&#37327;&#23454;&#20307;&#65292;&#24182;&#19988;&#26377;&#25928;&#22788;&#29702;&#26032;&#20986;&#29616;&#30340;&#23454;&#20307;&#20063;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20020;&#26102;&#24402;&#32435;&#36335;&#24452;&#31070;&#32463;&#32593;&#32476;&#65288;TiPNN&#65289;&#65292;&#23427;&#20197;&#23454;&#20307;&#29420;&#31435;&#30340;&#35282;&#24230;&#23545;&#21382;&#21490;&#20449;&#24687;&#36827;&#34892;&#24314;&#27169;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;TiPNN&#37319;&#29992;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#22270;&#65292;&#21517;&#20026;&#21382;&#21490;&#26102;&#38388;&#22270;&#65292;&#26469;&#24314;&#27169;&#21382;&#21490;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#20020;&#26102;&#24402;&#32435;&#36335;&#24452;&#25552;&#21462;&#32467;&#26500;&#21644;&#26102;&#38388;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal Knowledge Graph (TKG) is an extension of traditional Knowledge Graph (KG) that incorporates the dimension of time. Reasoning on TKGs is a crucial task that aims to predict future facts based on historical occurrences. The key challenge lies in uncovering structural dependencies within historical subgraphs and temporal patterns. Most existing approaches model TKGs relying on entity modeling, as nodes in the graph play a crucial role in knowledge representation. However, the real-world scenario often involves an extensive number of entities, with new entities emerging over time. This makes it challenging for entity-dependent methods to cope with extensive volumes of entities, and effectively handling newly emerging entities also becomes a significant challenge. Therefore, we propose Temporal Inductive Path Neural Network (TiPNN), which models historical information in an entity-independent perspective. Specifically, TiPNN adopts a unified graph, namely history temporal graph, to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#32467;&#21512;&#20840;&#23616;&#20449;&#24687;&#21644;&#26412;&#22320;&#29305;&#24449;&#65292;&#23454;&#29616;&#23545;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#30340;&#36890;&#29992;&#31070;&#32463;&#27714;&#35299;&#22120;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#30340;&#22797;&#26434;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.14104</link><description>&lt;p&gt;
&#36890;&#36807;&#20855;&#26377;&#21487;&#36801;&#31227;&#26412;&#22320;&#31574;&#30053;&#30340;&#38598;&#25104;&#65292;&#23454;&#29616;&#23545;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#30340;&#36890;&#29992;&#31070;&#32463;&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
Towards Generalizable Neural Solvers for Vehicle Routing Problems via Ensemble with Transferrable Local Policy. (arXiv:2308.14104v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14104
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#32467;&#21512;&#20840;&#23616;&#20449;&#24687;&#21644;&#26412;&#22320;&#29305;&#24449;&#65292;&#23454;&#29616;&#23545;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#30340;&#36890;&#29992;&#31070;&#32463;&#27714;&#35299;&#22120;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#30340;&#22797;&#26434;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#24050;&#34987;&#24212;&#29992;&#20110;&#24110;&#21161;&#35299;&#20915;NP&#22256;&#38590;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#12290;&#30446;&#21069;&#19968;&#31181;&#24120;&#35265;&#30340;&#26041;&#24335;&#26159;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#26500;&#24314;&#35299;&#20915;&#26041;&#26696;&#65292;&#30001;&#20110;&#20854;&#39640;&#25928;&#24615;&#21644;&#23545;&#19987;&#19994;&#30693;&#35782;&#35201;&#27714;&#36739;&#23569;&#65292;&#22240;&#27492;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29992;&#20110;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#65288;VRPs&#65289;&#30340;&#31070;&#32463;&#26500;&#24314;&#26041;&#27861;&#37117;&#38598;&#20013;&#22312;&#20855;&#26377;&#25351;&#23450;&#33410;&#28857;&#20998;&#24067;&#21644;&#26377;&#38480;&#35268;&#27169;&#30340;&#21512;&#25104;&#38382;&#39064;&#23454;&#20363;&#19978;&#65292;&#23548;&#33268;&#22312;&#36890;&#24120;&#28041;&#21450;&#22797;&#26434;&#19988;&#26410;&#30693;&#33410;&#28857;&#20998;&#24067;&#20197;&#21450;&#22823;&#35268;&#27169;&#38382;&#39064;&#30340;&#29616;&#23454;&#19990;&#30028;&#38382;&#39064;&#19978;&#24615;&#33021;&#36739;&#24046;&#12290;&#20026;&#20102;&#20351;&#31070;&#32463;VRP&#27714;&#35299;&#22120;&#26356;&#23454;&#29992;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20174;&#26412;&#22320;&#21487;&#36801;&#31227;&#25299;&#25169;&#29305;&#24449;&#20013;&#23398;&#20064;&#30340;&#36741;&#21161;&#31574;&#30053;&#65292;&#31216;&#20026;&#26412;&#22320;&#31574;&#30053;&#65292;&#24182;&#19982;&#20856;&#22411;&#30340;&#26500;&#24314;&#31574;&#30053;&#65288;&#20174;VRP&#23454;&#20363;&#30340;&#20840;&#23616;&#20449;&#24687;&#20013;&#23398;&#20064;&#65289;&#30456;&#32467;&#21512;&#24418;&#25104;&#19968;&#20010;&#38598;&#25104;&#31574;&#30053;&#12290;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#65292;&#32858;&#21512;&#30340;&#31574;&#30053;&#30456;&#20114;&#21327;&#20316;&#21644;&#20114;&#34917;&#65292;&#20197;&#25552;&#21319;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning has been adapted to help solve NP-hard combinatorial optimization problems. One prevalent way is learning to construct solutions by deep neural networks, which has been receiving more and more attention due to the high efficiency and less requirement for expert knowledge. However, many neural construction methods for Vehicle Routing Problems (VRPs) focus on synthetic problem instances with specified node distributions and limited scales, leading to poor performance on real-world problems which usually involve complex and unknown node distributions together with large scales. To make neural VRP solvers more practical, we design an auxiliary policy that learns from the local transferable topological features, named local policy, and integrate it with a typical construction policy (which learns from the global information of VRP instances) to form an ensemble policy. With joint training, the aggregated policies perform cooperatively and complementarily to boost generaliza
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20010;&#24615;&#21270;&#29983;&#25104;&#32593;&#32476;&#23454;&#29616;&#20102;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#25968;&#25454;&#32479;&#35745;&#24322;&#36136;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23545;&#23458;&#25143;&#31471;&#20043;&#38388;&#30693;&#35782;&#20256;&#36882;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#20840;&#23616;&#27169;&#22411;&#30340;&#25910;&#25947;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.13265</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#29983;&#25104;&#32593;&#32476;&#23454;&#29616;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous Federated Learning via Personalized Generative Networks. (arXiv:2308.13265v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13265
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20010;&#24615;&#21270;&#29983;&#25104;&#32593;&#32476;&#23454;&#29616;&#20102;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#25968;&#25454;&#32479;&#35745;&#24322;&#36136;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23545;&#23458;&#25143;&#31471;&#20043;&#38388;&#30693;&#35782;&#20256;&#36882;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#20840;&#23616;&#27169;&#22411;&#30340;&#25910;&#25947;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20801;&#35768;&#22810;&#20010;&#23458;&#25143;&#31471;&#26500;&#24314;&#19968;&#20010;&#20849;&#21516;&#30340;&#20840;&#23616;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#20849;&#20139;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#32852;&#37030;&#23398;&#20064;&#38754;&#20020;&#23458;&#25143;&#31471;&#25968;&#25454;&#30340;&#32479;&#35745;&#24322;&#36136;&#24615;&#30340;&#25361;&#25112;&#65292;&#36825;&#38477;&#20302;&#20102;&#24615;&#33021;&#24182;&#20943;&#24930;&#20102;&#21521;&#20840;&#23616;&#27169;&#22411;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#29702;&#35770;&#35777;&#26126;&#65292;&#26368;&#23567;&#21270;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#24322;&#36136;&#24615;&#26377;&#21161;&#20110;&#27599;&#20010;&#21333;&#29420;&#23458;&#25143;&#31471;&#30340;&#20840;&#23616;&#27169;&#22411;&#30340;&#25910;&#25947;&#12290;&#36825;&#22312;&#23458;&#25143;&#31471;&#20043;&#38388;&#20986;&#29616;&#32463;&#39564;&#27010;&#24565;&#36716;&#21464;&#38750;&#24120;&#37325;&#35201;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#32771;&#34385;&#21040;&#24050;&#34987;&#30740;&#31350;&#36807;&#30340;&#19981;&#24179;&#34913;&#31867;&#21035;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#20256;&#36882;&#26041;&#27861;&#65292;&#20854;&#20013;&#26381;&#21153;&#22120;&#35757;&#32451;&#23458;&#25143;&#31471;&#29305;&#23450;&#30340;&#29983;&#25104;&#22120;&#12290;&#27599;&#20010;&#29983;&#25104;&#22120;&#20026;&#30456;&#24212;&#30340;&#23458;&#25143;&#31471;&#29983;&#25104;&#26679;&#26412;&#65292;&#20197;&#28040;&#38500;&#19982;&#20854;&#20182;&#23458;&#25143;&#31471;&#27169;&#22411;&#30340;&#20914;&#31361;&#12290;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#20197;&#21450;&#29702;&#35770;&#30740;&#31350;&#25903;&#25345;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) allows several clients to construct a common global machine-learning model without having to share their data. FL, however, faces the challenge of statistical heterogeneity between the client's data, which degrades performance and slows down the convergence toward the global model. In this paper, we provide theoretical proof that minimizing heterogeneity between clients facilitates the convergence of a global model for every single client. This becomes particularly important under empirical concept shifts among clients, rather than merely considering imbalanced classes, which have been studied until now. Therefore, we propose a method for knowledge transfer between clients where the server trains client-specific generators. Each generator generates samples for the corresponding client to remove the conflict with other clients' models. Experiments conducted on synthetic and real data, along with a theoretical study, support the effectiveness of our method in cons
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31232;&#30095;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#21152;&#26435;Chebyshev&#26631;&#37327;&#21270;&#21644;&#22686;&#24191;Lagrangian&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#21516;&#26102;&#20248;&#21270;&#22810;&#20010;&#20219;&#21153;&#26102;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.12243</link><description>&lt;p&gt;
&#29992;&#20110;&#31232;&#30095;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Multi-Objective Optimization for Sparse Deep Neural Network Training. (arXiv:2308.12243v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12243
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31232;&#30095;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#21152;&#26435;Chebyshev&#26631;&#37327;&#21270;&#21644;&#22686;&#24191;Lagrangian&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#21516;&#26102;&#20248;&#21270;&#22810;&#20010;&#20219;&#21153;&#26102;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#30340;&#21508;&#31181;&#22330;&#26223;&#20013;&#65292;&#20250;&#33258;&#28982;&#22320;&#20986;&#29616;&#19981;&#21516;&#30340;&#20914;&#31361;&#20248;&#21270;&#20934;&#21017;&#12290;&#36825;&#20123;&#20934;&#21017;&#21487;&#20197;&#35299;&#20915;&#19981;&#21516;&#30340;&#20027;&#20219;&#21153;&#65288;&#22914;&#22810;&#20219;&#21153;&#23398;&#20064;&#35774;&#32622;&#65289;&#65292;&#20063;&#21487;&#20197;&#35299;&#20915;&#20027;&#35201;&#20219;&#21153;&#21644;&#27425;&#35201;&#20219;&#21153;&#65292;&#20363;&#22914;&#25439;&#22833;&#26368;&#23567;&#21270;&#19982;&#31232;&#30095;&#24615;&#12290;&#36890;&#24120;&#30340;&#26041;&#27861;&#26159;&#31616;&#21333;&#22320;&#21152;&#26435;&#20934;&#21017;&#65292;&#20294;&#22312;&#20984;&#35774;&#32622;&#20013;&#25165;&#26377;&#25928;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#30446;&#26631;&#20248;&#21270;&#31639;&#27861;&#65292;&#23545;&#22810;&#20219;&#21153;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#36827;&#34892;&#35757;&#32451;&#65292;&#20351;&#29992;&#25913;&#36827;&#30340;&#21152;&#26435;Chebyshev&#26631;&#37327;&#21270;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#36825;&#31181;&#26631;&#37327;&#21270;&#25216;&#26415;&#65292;&#31639;&#27861;&#21487;&#20197;&#35782;&#21035;&#21407;&#22987;&#38382;&#39064;&#30340;&#25152;&#26377;&#26368;&#20248;&#35299;&#65292;&#21516;&#26102;&#23558;&#20854;&#22797;&#26434;&#24615;&#38477;&#20302;&#20026;&#19968;&#31995;&#21015;&#21333;&#30446;&#26631;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#22686;&#24191;Lagrangian&#26041;&#27861;&#26469;&#35299;&#20915;&#31616;&#21270;&#21518;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#21487;&#20197;&#20351;&#29992;&#24120;&#35265;&#30340;&#20248;&#21270;&#25216;&#26415;&#65292;&#22914;Adam&#21644;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65292;&#21516;&#26102;&#26377;&#25928;&#22320;&#22788;&#29702;&#32422;&#26463;&#26465;&#20214;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26088;&#22312;&#35299;&#20915;&#32463;&#27982;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Different conflicting optimization criteria arise naturally in various Deep Learning scenarios. These can address different main tasks (i.e., in the setting of Multi-Task Learning), but also main and secondary tasks such as loss minimization versus sparsity. The usual approach is a simple weighting of the criteria, which formally only works in the convex setting. In this paper, we present a Multi-Objective Optimization algorithm using a modified Weighted Chebyshev scalarization for training Deep Neural Networks (DNNs) with respect to several tasks. By employing this scalarization technique, the algorithm can identify all optimal solutions of the original problem while reducing its complexity to a sequence of single-objective problems. The simplified problems are then solved using an Augmented Lagrangian method, enabling the use of popular optimization techniques such as Adam and Stochastic Gradient Descent, while efficaciously handling constraints. Our work aims to address the (economi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28857;&#20113;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VF-Net&#65289;&#29992;&#20110;&#29273;&#31185;&#28857;&#20113;&#25968;&#25454;&#30340;&#22788;&#29702;&#65292;&#35813;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#20855;&#26377;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#32593;&#26684;&#29983;&#25104;&#12289;&#24418;&#29366;&#23436;&#25972;&#21644;&#34920;&#31034;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2307.10895</link><description>&lt;p&gt;
&#29273;&#31185;&#28857;&#20113;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Variational Autoencoding of Dental Point Clouds. (arXiv:2307.10895v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10895
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28857;&#20113;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VF-Net&#65289;&#29992;&#20110;&#29273;&#31185;&#28857;&#20113;&#25968;&#25454;&#30340;&#22788;&#29702;&#65292;&#35813;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#20855;&#26377;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#32593;&#26684;&#29983;&#25104;&#12289;&#24418;&#29366;&#23436;&#25972;&#21644;&#34920;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#29273;&#31185;&#23398;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#20173;&#38754;&#20020;&#35768;&#22810;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;FDI 16&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#20102;&#22823;&#37327;&#29273;&#40831;&#32593;&#26684;&#21644;&#28857;&#20113;&#30340;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65306;&#21464;&#20998;FoldingNet&#65288;VF-Net&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#19987;&#20026;&#28857;&#20113;&#35774;&#35745;&#30340;&#23436;&#20840;&#27010;&#29575;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#20808;&#21069;&#30340;&#28857;&#20113;&#28508;&#21464;&#37327;&#27169;&#22411;&#32570;&#20047;&#36755;&#20837;&#21644;&#36755;&#20986;&#28857;&#20043;&#38388;&#30340;&#19968;&#19968;&#23545;&#24212;&#20851;&#31995;&#12290;&#30456;&#21453;&#65292;&#23427;&#20204;&#20381;&#36182;&#20110;&#20248;&#21270;Chamfer&#36317;&#31163;&#65292;&#36825;&#26159;&#19968;&#31181;&#32570;&#20047;&#24402;&#19968;&#21270;&#20998;&#24067;&#23545;&#24212;&#30340;&#24230;&#37327;&#65292;&#22240;&#27492;&#19981;&#36866;&#21512;&#27010;&#29575;&#24314;&#27169;&#12290;&#25105;&#20204;&#29992;&#21512;&#36866;&#30340;&#32534;&#30721;&#22120;&#21462;&#20195;&#20102;&#26126;&#30830;&#30340;&#26368;&#23567;&#21270;Chamfer&#36317;&#31163;&#65292;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#65292;&#21516;&#26102;&#31616;&#21270;&#20102;&#27010;&#29575;&#25193;&#23637;&#12290;&#36825;&#20351;&#24471;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#37117;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#65292;&#21253;&#25324;&#32593;&#26684;&#29983;&#25104;&#12289;&#24418;&#29366;&#23436;&#25972;&#21644;&#34920;&#31034;&#23398;&#20064;&#12290;&#22312;&#23454;&#35777;&#26041;&#38754;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#29273;&#40831;&#37325;&#24314;&#20013;&#36739;&#20302;&#30340;&#37325;&#24314;&#35823;&#24046;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Digital dentistry has made significant advancements, yet numerous challenges remain. This paper introduces the FDI 16 dataset, an extensive collection of tooth meshes and point clouds. Additionally, we present a novel approach: Variational FoldingNet (VF-Net), a fully probabilistic variational autoencoder designed for point clouds. Notably, prior latent variable models for point clouds lack a one-to-one correspondence between input and output points. Instead, they rely on optimizing Chamfer distances, a metric that lacks a normalized distributional counterpart, rendering it unsuitable for probabilistic modeling. We replace the explicit minimization of Chamfer distances with a suitable encoder, increasing computational efficiency while simplifying the probabilistic extension. This allows for straightforward application in various tasks, including mesh generation, shape completion, and representation learning. Empirically, we provide evidence of lower reconstruction error in dental recon
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26465;&#20214;&#27133;&#27880;&#24847;&#21147;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#27010;&#29575;&#24615;&#27133;&#23383;&#20856;&#65288;PSD&#65289;&#23454;&#29616;&#20102;&#19987;&#38376;&#30340;&#27133;&#20301;&#32465;&#23450;&#21644;&#29289;&#20307;&#23618;&#27425;&#35843;&#33410;&#20998;&#24067;&#65292;&#22312;&#22810;&#20010;&#21518;&#32493;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20854;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2307.09437</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#26465;&#20214;&#27133;&#27880;&#24847;&#21147;&#29992;&#20110;&#29289;&#20307;&#20013;&#24515;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Conditional Slot Attention for Object Centric Learning. (arXiv:2307.09437v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09437
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26465;&#20214;&#27133;&#27880;&#24847;&#21147;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#27010;&#29575;&#24615;&#27133;&#23383;&#20856;&#65288;PSD&#65289;&#23454;&#29616;&#20102;&#19987;&#38376;&#30340;&#27133;&#20301;&#32465;&#23450;&#21644;&#29289;&#20307;&#23618;&#27425;&#35843;&#33410;&#20998;&#24067;&#65292;&#22312;&#22810;&#20010;&#21518;&#32493;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20854;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#21462;&#29289;&#20307;&#23618;&#27425;&#30340;&#34920;&#31034;&#20197;&#36827;&#34892;&#21518;&#32493;&#30340;&#25512;&#29702;&#20219;&#21153;&#26159;&#20154;&#24037;&#26234;&#33021;&#20013;&#28044;&#29616;&#30340;&#19968;&#20010;&#39046;&#22495;&#12290;&#22312;&#26080;&#30417;&#30563;&#30340;&#35774;&#32622;&#20013;&#23398;&#20064;&#29289;&#20307;&#20013;&#24515;&#34920;&#31034;&#38754;&#20020;&#22810;&#20010;&#25361;&#25112;&#65292;&#20854;&#20013;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#23558;&#20219;&#24847;&#25968;&#37327;&#30340;&#29289;&#20307;&#23454;&#20363;&#32465;&#23450;&#21040;&#19987;&#38376;&#30340;&#29289;&#20307;&#27133;&#20301;&#12290;&#26368;&#36817;&#30340;&#29289;&#20307;&#20013;&#24515;&#34920;&#31034;&#26041;&#27861;&#22914;&#27133;&#20301;&#27880;&#24847;&#21147;&#21033;&#29992;&#36845;&#20195;&#24335;&#27880;&#24847;&#21147;&#23398;&#20064;&#20855;&#26377;&#21160;&#24577;&#25512;&#29702;&#23618;&#32423;&#32465;&#23450;&#30340;&#21487;&#32452;&#21512;&#34920;&#31034;&#65292;&#20294;&#26410;&#33021;&#36798;&#21040;&#19987;&#38376;&#30340;&#27133;&#20301;&#32465;&#23450;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26032;&#39062;&#30340;&#27010;&#29575;&#24615;&#27133;&#23383;&#20856;&#65288;PSD&#65289;&#30340;&#26080;&#30417;&#30563;&#26465;&#20214;&#27133;&#27880;&#24847;&#21147;&#12290;&#25105;&#20204;&#23558;PSD&#23450;&#20041;&#20026;&#65288;i&#65289;&#25277;&#35937;&#30340;&#29289;&#20307;&#23618;&#27425;&#23646;&#24615;&#21521;&#37327;&#20316;&#20026;&#38190;&#65292;&#65288;ii&#65289;&#21442;&#25968;&#21270;&#39640;&#26031;&#20998;&#24067;&#20316;&#20026;&#30456;&#24212;&#30340;&#20540;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#21518;&#32493;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#23398;&#20064;&#21040;&#30340;&#20855;&#20307;&#29289;&#20307;&#23618;&#27425;&#35843;&#33410;&#20998;&#24067;&#30340;&#22909;&#22788;&#65292;&#21253;&#25324;&#29289;&#20307;&#21457;&#29616;&#12289;&#32452;&#21512;&#24335;&#22330;&#26223;&#29983;&#25104;&#21644;&#32452;&#21512;&#24335;&#35270;&#35273;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Extracting object-level representations for downstream reasoning tasks is an emerging area in AI. Learning object-centric representations in an unsupervised setting presents multiple challenges, a key one being binding an arbitrary number of object instances to a specialized object slot. Recent object-centric representation methods like Slot Attention utilize iterative attention to learn composable representations with dynamic inference level binding but fail to achieve specialized slot level binding. To address this, in this paper we propose Unsupervised Conditional Slot Attention using a novel Probabilistic Slot Dictionary (PSD). We define PSD with (i) abstract object-level property vectors as key and (ii) parametric Gaussian distribution as its corresponding value. We demonstrate the benefits of the learnt specific object-level conditioning distributions in multiple downstream tasks, namely object discovery, compositional scene generation, and compositional visual reasoning. We show
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;DyGATAD&#65292;&#29992;&#20110;&#22312;&#24322;&#26500;&#20256;&#24863;&#22120;&#32593;&#32476;&#20013;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#21160;&#24577;&#22270;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#35782;&#21035;&#38598;&#20307;&#24322;&#24120;&#34892;&#20026;&#65292;&#20854;&#20013;&#24322;&#24120;&#34892;&#20026;&#21487;&#33021;&#30001;&#20110;&#31995;&#32479;&#20869;&#37096;&#30456;&#20114;&#20851;&#31995;&#30340;&#21464;&#21270;&#24341;&#36215;&#12290;&#36825;&#22312;&#24037;&#19994;&#29289;&#32852;&#32593;&#30417;&#25511;&#31995;&#32479;&#20013;&#20855;&#26377;&#37325;&#35201;&#30340;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2307.03761</link><description>&lt;p&gt;
&#24322;&#26500;&#20256;&#24863;&#22120;&#32593;&#32476;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#30340;&#21160;&#24577;&#22270;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
Dynamic Graph Attention for Anomaly Detection in Heterogeneous Sensor Networks. (arXiv:2307.03761v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03761
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;DyGATAD&#65292;&#29992;&#20110;&#22312;&#24322;&#26500;&#20256;&#24863;&#22120;&#32593;&#32476;&#20013;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#21160;&#24577;&#22270;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#35782;&#21035;&#38598;&#20307;&#24322;&#24120;&#34892;&#20026;&#65292;&#20854;&#20013;&#24322;&#24120;&#34892;&#20026;&#21487;&#33021;&#30001;&#20110;&#31995;&#32479;&#20869;&#37096;&#30456;&#20114;&#20851;&#31995;&#30340;&#21464;&#21270;&#24341;&#36215;&#12290;&#36825;&#22312;&#24037;&#19994;&#29289;&#32852;&#32593;&#30417;&#25511;&#31995;&#32479;&#20013;&#20855;&#26377;&#37325;&#35201;&#30340;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#23383;&#21270;&#36716;&#22411;&#30340;&#26102;&#20195;&#65292;&#30001;&#24037;&#19994;&#29289;&#32852;&#32593; (IIoT) &#30417;&#25511;&#30340;&#31995;&#32479;&#36890;&#36807;&#24322;&#26500;&#20256;&#24863;&#22120;&#32593;&#32476;&#29983;&#25104;&#22823;&#37327;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015; (MTS) &#25968;&#25454;&#12290;&#34429;&#28982;&#36825;&#20123;&#25968;&#25454;&#26377;&#21161;&#20110;&#26465;&#20214;&#30417;&#25511;&#21644;&#24322;&#24120;&#26816;&#27979;&#65292;&#20294;&#26159;&#20256;&#24863;&#22120;&#32593;&#32476;&#20013;&#26085;&#30410;&#22797;&#26434;&#21644;&#30456;&#20114;&#20381;&#36182;&#30340;&#20851;&#31995;&#20063;&#32473;&#24322;&#24120;&#26816;&#27979;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#23613;&#31649;&#22312;&#36825;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#20027;&#35201;&#38598;&#20013;&#22312;&#28857;&#24322;&#24120;&#21644;&#32972;&#26223;&#24322;&#24120;&#65292;&#23545;&#38598;&#20307;&#24322;&#24120;&#30340;&#20851;&#27880;&#36739;&#23569;&#12290;&#38598;&#20307;&#24322;&#24120;&#30340;&#19968;&#31181;&#24120;&#35265;&#21464;&#31181;&#26159;&#24322;&#24120;&#38598;&#20307;&#34892;&#20026;&#30001;&#31995;&#32479;&#20869;&#37096;&#30340;&#30456;&#20114;&#20851;&#31995;&#21464;&#21270;&#24341;&#36215;&#12290;&#36825;&#21487;&#33021;&#26159;&#30001;&#20110;&#24322;&#24120;&#29615;&#22659;&#26465;&#20214;&#65288;&#22914;&#36807;&#28909;&#65289;&#12289;&#30001;&#20110;&#32593;&#32476;&#25915;&#20987;&#36896;&#25104;&#30340;&#19981;&#27491;&#30830;&#25805;&#20316;&#35774;&#32622;&#25110;&#31995;&#32479;&#32423;&#25925;&#38556;&#24341;&#36215;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102; DyGATAD&#65288;&#19968;&#31181;&#21160;&#24577;&#22270;&#27880;&#24847;&#21147;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65289;&#65292;&#37319;&#29992;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#20256;&#24863;&#22120;&#32593;&#32476;&#20013;&#30340;&#24322;&#24120;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the era of digital transformation, systems monitored by the Industrial Internet of Things (IIoTs) generate large amounts of Multivariate Time Series (MTS) data through heterogeneous sensor networks. While this data facilitates condition monitoring and anomaly detection, the increasing complexity and interdependencies within the sensor network pose significant challenges for anomaly detection. Despite progress in this field, much of the focus has been on point anomalies and contextual anomalies, with lesser attention paid to collective anomalies. A less addressed but common variant of collective anomalies is when the abnormal collective behavior is caused by shifts in interrelationships within the system. This can be due to abnormal environmental conditions like overheating, improper operational settings resulting from cyber-physical attacks, or system-level faults. To address these challenges, this paper proposes DyGATAD (Dynamic Graph Attention for Anomaly Detection), a graph-based
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20855;&#26377;&#32534;&#30721;&#25968;&#25454;&#32467;&#26500;&#30340;&#21464;&#20998;&#37327;&#23376;&#22238;&#24402;&#31639;&#27861;&#65292;&#22312;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#20855;&#26377;&#27169;&#22411;&#35299;&#37322;&#24615;&#65292;&#24182;&#33021;&#26377;&#25928;&#22320;&#22788;&#29702;&#20114;&#36830;&#24230;&#36739;&#39640;&#30340;&#37327;&#23376;&#27604;&#29305;&#12290;&#31639;&#27861;&#36890;&#36807;&#21387;&#32553;&#32534;&#30721;&#21644;&#25968;&#23383;-&#27169;&#25311;&#38376;&#25805;&#20316;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#22312;&#22122;&#22768;&#20013;&#23610;&#24230;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#30340;&#36816;&#34892;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2307.03334</link><description>&lt;p&gt;
&#20855;&#26377;&#32534;&#30721;&#25968;&#25454;&#32467;&#26500;&#30340;&#21464;&#20998;&#37327;&#23376;&#22238;&#24402;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Variational quantum regression algorithm with encoded data structure. (arXiv:2307.03334v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03334
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20855;&#26377;&#32534;&#30721;&#25968;&#25454;&#32467;&#26500;&#30340;&#21464;&#20998;&#37327;&#23376;&#22238;&#24402;&#31639;&#27861;&#65292;&#22312;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#20855;&#26377;&#27169;&#22411;&#35299;&#37322;&#24615;&#65292;&#24182;&#33021;&#26377;&#25928;&#22320;&#22788;&#29702;&#20114;&#36830;&#24230;&#36739;&#39640;&#30340;&#37327;&#23376;&#27604;&#29305;&#12290;&#31639;&#27861;&#36890;&#36807;&#21387;&#32553;&#32534;&#30721;&#21644;&#25968;&#23383;-&#27169;&#25311;&#38376;&#25805;&#20316;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#22312;&#22122;&#22768;&#20013;&#23610;&#24230;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#30340;&#36816;&#34892;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;(VQAs)&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#35299;&#20915;&#23454;&#38469;&#38382;&#39064;&#65292;&#22914;&#32452;&#21512;&#20248;&#21270;&#12289;&#37327;&#23376;&#21270;&#23398;&#27169;&#25311;&#12289;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#21644;&#22122;&#22768;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#30340;&#37327;&#23376;&#38169;&#35823;&#32416;&#27491;&#12290;&#23545;&#20110;&#21464;&#20998;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#65292;&#23578;&#26410;&#24320;&#21457;&#20986;&#23558;&#27169;&#22411;&#35299;&#37322;&#24615;&#20869;&#23884;&#21040;&#31639;&#27861;&#20013;&#30340;&#21464;&#20998;&#31639;&#27861;&#12290;&#26412;&#25991;&#26500;&#24314;&#20102;&#19968;&#20010;&#37327;&#23376;&#22238;&#24402;&#31639;&#27861;&#65292;&#24182;&#30830;&#23450;&#20102;&#21464;&#20998;&#21442;&#25968;&#19982;&#23398;&#20064;&#22238;&#24402;&#31995;&#25968;&#20043;&#38388;&#30340;&#30452;&#25509;&#20851;&#31995;&#65292;&#21516;&#26102;&#37319;&#29992;&#20102;&#23558;&#25968;&#25454;&#30452;&#25509;&#32534;&#30721;&#20026;&#21453;&#26144;&#32463;&#20856;&#25968;&#25454;&#34920;&#32467;&#26500;&#30340;&#37327;&#23376;&#24133;&#24230;&#30340;&#30005;&#36335;&#12290;&#35813;&#31639;&#27861;&#29305;&#21035;&#36866;&#29992;&#20110;&#20114;&#36830;&#24230;&#36739;&#39640;&#30340;&#37327;&#23376;&#27604;&#29305;&#12290;&#36890;&#36807;&#21387;&#32553;&#32534;&#30721;&#21644;&#25968;&#23383;-&#27169;&#25311;&#38376;&#25805;&#20316;&#65292;&#36816;&#34892;&#26102;&#38388;&#22797;&#26434;&#24230;&#22312;&#25968;&#25454;&#36755;&#20837;&#37327;&#32534;&#30721;&#30340;&#24773;&#20917;&#19979;&#23545;&#25968;&#32423;&#26356;&#26377;&#20248;&#21183;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#22122;&#22768;&#20013;&#23610;&#24230;&#37327;&#23376;&#35745;&#31639;&#26426;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variational quantum algorithms (VQAs) prevail to solve practical problems such as combinatorial optimization, quantum chemistry simulation, quantum machine learning, and quantum error correction on noisy quantum computers. For variational quantum machine learning, a variational algorithm with model interpretability built into the algorithm is yet to be exploited. In this paper, we construct a quantum regression algorithm and identify the direct relation of variational parameters to learned regression coefficients, while employing a circuit that directly encodes the data in quantum amplitudes reflecting the structure of the classical data table. The algorithm is particularly suitable for well-connected qubits. With compressed encoding and digital-analog gate operation, the run time complexity is logarithmically more advantageous than that for digital 2-local gate native hardware with the number of data entries encoded, a decent improvement in noisy intermediate-scale quantum computers a
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#33258;&#25105;&#30417;&#30563;&#30340;&#35821;&#38899;&#27169;&#22411;&#36827;&#34892;&#20998;&#26512;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#19981;&#21516;&#23618;&#20013;&#32534;&#30721;&#20102;&#19981;&#21516;&#30340;&#35821;&#35328;&#20449;&#24687;&#65292;&#20063;&#23398;&#20064;&#20102;&#31867;&#20284;&#38899;&#32032;&#30340;&#23376;&#35789;&#21333;&#20803;&#12290;&#19982;&#21333;&#35789;&#30456;&#20851;&#30340;&#20449;&#24687;&#20027;&#35201;&#22312;&#20013;&#38388;&#30340;&#27169;&#22411;&#23618;&#20013;&#65292;&#21516;&#26102;&#19968;&#20123;&#20302;&#32423;&#20449;&#24687;&#22312;&#26356;&#39640;&#30340;&#23618;&#20013;&#20063;&#24471;&#20197;&#20445;&#30041;&#12290;</title><link>http://arxiv.org/abs/2307.00162</link><description>&lt;p&gt;
&#33258;&#25105;&#30417;&#30563;&#30340;&#35821;&#38899;&#27169;&#22411;&#23545;&#21333;&#35789;&#30340;&#20102;&#35299;&#31243;&#24230;&#26159;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
What do self-supervised speech models know about words?. (arXiv:2307.00162v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00162
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#33258;&#25105;&#30417;&#30563;&#30340;&#35821;&#38899;&#27169;&#22411;&#36827;&#34892;&#20998;&#26512;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#19981;&#21516;&#23618;&#20013;&#32534;&#30721;&#20102;&#19981;&#21516;&#30340;&#35821;&#35328;&#20449;&#24687;&#65292;&#20063;&#23398;&#20064;&#20102;&#31867;&#20284;&#38899;&#32032;&#30340;&#23376;&#35789;&#21333;&#20803;&#12290;&#19982;&#21333;&#35789;&#30456;&#20851;&#30340;&#20449;&#24687;&#20027;&#35201;&#22312;&#20013;&#38388;&#30340;&#27169;&#22411;&#23618;&#20013;&#65292;&#21516;&#26102;&#19968;&#20123;&#20302;&#32423;&#20449;&#24687;&#22312;&#26356;&#39640;&#30340;&#23618;&#20013;&#20063;&#24471;&#20197;&#20445;&#30041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#35768;&#22810;&#33258;&#25105;&#30417;&#30563;&#30340;&#35821;&#38899;&#27169;&#22411;&#65288;S3Ms&#65289;&#34987;&#24341;&#20837;&#65292;&#20026;&#21508;&#31181;&#35821;&#38899;&#20219;&#21153;&#25552;&#20379;&#20102;&#24615;&#33021;&#21644;&#25968;&#25454;&#25928;&#29575;&#30340;&#25913;&#36827;&#12290;&#26377;&#35777;&#25454;&#34920;&#26126;&#65292;&#19981;&#21516;&#30340;S3Ms&#22312;&#19981;&#21516;&#30340;&#23618;&#20013;&#32534;&#30721;&#35821;&#35328;&#20449;&#24687;&#65292;&#32780;&#19988;&#19968;&#20123;S3Ms&#20284;&#20046;&#23398;&#20064;&#20102;&#31867;&#20284;&#20110;&#38899;&#32032;&#30340;&#23376;&#35789;&#21333;&#20803;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#25429;&#25417;&#26356;&#22823;&#30340;&#35821;&#35328;&#21333;&#20803;&#65288;&#22914;&#21333;&#35789;&#65289;&#30340;&#31243;&#24230;&#20197;&#21450;&#21333;&#35789;&#30456;&#20851;&#20449;&#24687;&#30340;&#32534;&#30721;&#20301;&#32622;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;&#26469;&#33258;&#19977;&#20010;S3Ms&#30340;&#19981;&#21516;&#23618;&#30340;&#21333;&#35789;&#29255;&#27573;&#34920;&#31034;&#36827;&#34892;&#20102;&#22810;&#31181;&#20998;&#26512;&#65306;wav2vec2&#12289;HuBERT&#21644;WavLM&#12290;&#25105;&#20204;&#21033;&#29992;&#35268;&#33539;&#30456;&#20851;&#20998;&#26512;&#65288;CCA&#65289;&#65292;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#20998;&#26512;&#24037;&#20855;&#65292;&#26469;&#34913;&#37327;&#36825;&#20123;&#34920;&#31034;&#19982;&#21333;&#35789;&#32423;&#35821;&#35328;&#23646;&#24615;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#26368;&#22823;&#30340;&#21333;&#35789;&#32423;&#35821;&#35328;&#20869;&#23481;&#24448;&#24448;&#20986;&#29616;&#22312;&#20013;&#38388;&#30340;&#27169;&#22411;&#23618;&#65292;&#32780;&#19968;&#20123;&#20302;&#32423;&#20449;&#24687;&#65288;&#22914;&#21457;&#38899;&#65289;&#20063;&#22312;&#26356;&#39640;&#30340;&#23618;&#20013;&#20445;&#30041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many self-supervised speech models (S3Ms) have been introduced over the last few years, producing performance and data efficiency improvements for a variety of speech tasks. Evidence is emerging that different S3Ms encode linguistic information in different layers, and also that some S3Ms appear to learn phone-like sub-word units. However, the extent to which these models capture larger linguistic units, such as words, and where word-related information is encoded, remains unclear. In this study, we conduct several analyses of word segment representations extracted from different layers of three S3Ms: wav2vec2, HuBERT, and WavLM. We employ canonical correlation analysis (CCA), a lightweight analysis tool, to measure the similarity between these representations and word-level linguistic properties. We find that the maximal word-level linguistic content tends to be found in intermediate model layers, while some lower-level information like pronunciation is also retained in higher layers 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#36924;&#30495;&#30340;&#21512;&#25104;&#37329;&#34701;&#20132;&#26131;&#25968;&#25454;&#38598;&#29983;&#25104;&#22120;&#21644;&#19968;&#32452;&#21512;&#25104;&#30340;&#21453;&#27927;&#38065;&#25968;&#25454;&#38598;&#65292;&#20197;&#28385;&#36275;&#35757;&#32451;&#27169;&#22411;&#21644;&#25512;&#36827;&#39046;&#22495;&#21457;&#23637;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2306.16424</link><description>&lt;p&gt;
&#36924;&#30495;&#30340;&#21512;&#25104;&#37329;&#34701;&#20132;&#26131;&#29992;&#20110;&#21453;&#27927;&#38065;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Realistic Synthetic Financial Transactions for Anti-Money Laundering Models. (arXiv:2306.16424v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#36924;&#30495;&#30340;&#21512;&#25104;&#37329;&#34701;&#20132;&#26131;&#25968;&#25454;&#38598;&#29983;&#25104;&#22120;&#21644;&#19968;&#32452;&#21512;&#25104;&#30340;&#21453;&#27927;&#38065;&#25968;&#25454;&#38598;&#65292;&#20197;&#28385;&#36275;&#35757;&#32451;&#27169;&#22411;&#21644;&#25512;&#36827;&#39046;&#22495;&#21457;&#23637;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#37329;&#34701;&#30340;&#24191;&#27867;&#25968;&#23383;&#21270;&#21644;&#21152;&#23494;&#36135;&#24065;&#30340;&#26085;&#30410;&#27969;&#34892;&#65292;&#32593;&#32476;&#29359;&#32618;&#20998;&#23376;&#35774;&#35745;&#30340;&#27450;&#35784;&#26041;&#26696;&#36234;&#26469;&#36234;&#22797;&#26434;&#12290;&#27927;&#38065;&#8212;&#8212;&#23558;&#38750;&#27861;&#36164;&#37329;&#31227;&#21160;&#20197;&#25513;&#30422;&#20854;&#26469;&#28304;&#8212;&#8212;&#21487;&#20197;&#36328;&#36234;&#38134;&#34892;&#21644;&#22269;&#30028;&#65292;&#20135;&#29983;&#22797;&#26434;&#30340;&#20132;&#26131;&#27169;&#24335;&#12290;&#32852;&#21512;&#22269;&#20272;&#35745;&#27599;&#24180;&#20840;&#29699;&#27927;&#38065;&#37329;&#39069;&#21344;&#20840;&#29699;GDP&#30340;2-5%&#65292;&#32422;&#20026;0.8-2.0&#19975;&#20159;&#32654;&#20803;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36890;&#24120;&#26080;&#27861;&#33719;&#24471;&#29992;&#20110;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#26816;&#27979;&#27927;&#38065;&#30340;&#30495;&#23454;&#25968;&#25454;&#65292;&#19988;&#20043;&#21069;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#22120;&#23384;&#22312;&#26174;&#33879;&#32570;&#38519;&#12290;&#20026;&#20102;&#27604;&#36739;&#27169;&#22411;&#24182;&#25512;&#36827;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#38656;&#35201;&#19968;&#20010;&#36924;&#30495;&#12289;&#26631;&#20934;&#21270;&#12289;&#20844;&#24320;&#21487;&#29992;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21512;&#25104;&#37329;&#34701;&#20132;&#26131;&#25968;&#25454;&#38598;&#29983;&#25104;&#22120;&#21644;&#19968;&#32452;&#21512;&#25104;&#30340;&#21453;&#27927;&#38065;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#26681;&#25454;&#23454;&#38469;&#20132;&#26131;&#23613;&#21487;&#33021;&#22320;&#26657;&#20934;&#20102;&#36825;&#20010;&#22522;&#20110;&#20195;&#29702;&#30340;&#29983;&#25104;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the widespread digitization of finance and the increasing popularity of cryptocurrencies, the sophistication of fraud schemes devised by cybercriminals is growing. Money laundering -- the movement of illicit funds to conceal their origins -- can cross bank and national boundaries, producing complex transaction patterns. The UN estimates 2-5\% of global GDP or \$0.8 - \$2.0 trillion dollars are laundered globally each year. Unfortunately, real data to train machine learning models to detect laundering is generally not available, and previous synthetic data generators have had significant shortcomings. A realistic, standardized, publicly-available benchmark is needed for comparing models and for the advancement of the area.  To this end, this paper contributes a synthetic financial transaction dataset generator and a set of synthetically generated AML (Anti-Money Laundering) datasets. We have calibrated this agent-based generator to match real transactions as closely as possible and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22788;&#29702;&#39034;&#24207;&#39044;&#27979;&#30340;&#27169;&#22411;&#65292;&#20801;&#35768;&#22312;&#19981;&#23545;&#23545;&#25239;&#24615;&#26679;&#20363;&#36827;&#34892;&#39044;&#27979;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#31639;&#27861;&#25239;&#23545;&#25239;&#25915;&#20987;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.13119</link><description>&lt;p&gt;
&#36890;&#36807;&#24323;&#26435;&#23454;&#29616;&#39034;&#24207;&#39044;&#27979;&#20013;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Adversarial Resilience in Sequential Prediction via Abstention. (arXiv:2306.13119v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13119
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22788;&#29702;&#39034;&#24207;&#39044;&#27979;&#30340;&#27169;&#22411;&#65292;&#20801;&#35768;&#22312;&#19981;&#23545;&#23545;&#25239;&#24615;&#26679;&#20363;&#36827;&#34892;&#39044;&#27979;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#31639;&#27861;&#25239;&#23545;&#25239;&#25915;&#20987;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24102;&#26377;&#20801;&#35768;&#27880;&#20837;&#24178;&#20928;&#26631;&#31614;&#23545;&#25239;&#24615;&#65288;&#25110;&#36229;&#20986;&#20998;&#24067;&#65289;&#31034;&#20363;&#30340;&#23545;&#25239;&#32773;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#38543;&#26426;&#35774;&#32622;&#19979;&#30340;&#39034;&#24207;&#39044;&#27979;&#38382;&#39064;&#12290;&#38024;&#23545;&#32431;&#38543;&#26426;&#25968;&#25454;&#30340;&#31639;&#27861;&#22312;&#23384;&#22312;&#27492;&#31867;&#23545;&#25239;&#24615;&#31034;&#20363;&#30340;&#24773;&#20917;&#19979;&#24448;&#24448;&#22833;&#36133;&#65292;&#20174;&#32780;&#23548;&#33268;&#38169;&#35823;&#30340;&#39044;&#27979;&#12290;&#36825;&#22312;&#35768;&#22810;&#39640;&#39118;&#38505;&#24212;&#29992;&#20013;&#26159;&#19981;&#21487;&#21462;&#30340;&#65292;&#20363;&#22914;&#21307;&#23398;&#24314;&#35758;&#65292;&#36825;&#37324;&#24323;&#26435;&#19981;&#36827;&#34892;&#23545;&#25239;&#24615;&#31034;&#20363;&#30340;&#39044;&#27979;&#20248;&#20110;&#35823;&#20998;&#31867;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20551;&#35774;&#23436;&#20840;&#23545;&#25239;&#24615;&#25968;&#25454;&#23548;&#33268;&#38750;&#24120;&#24754;&#35266;&#30340;&#30028;&#38480;&#65292;&#22312;&#23454;&#36341;&#20013;&#24448;&#24448;&#26159;&#31354;&#27934;&#30340;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39034;&#24207;&#39044;&#27979;&#27169;&#22411;&#65292;&#23427;&#20301;&#20110;&#32431;&#38543;&#26426;&#21644;&#23436;&#20840;&#23545;&#25239;&#24615;&#35774;&#32622;&#20043;&#38388;&#65292;&#36890;&#36807;&#20801;&#35768;&#23398;&#20064;&#22120;&#22312;&#23545;&#25239;&#26679;&#20363;&#19978;&#26080;&#20195;&#20215;&#22320;&#25918;&#24323;&#36827;&#34892;&#39044;&#27979;&#26469;&#23454;&#29616;&#12290;&#20551;&#35774;&#35775;&#38382;&#38750;&#23545;&#25239;&#26679;&#20363;&#30340;&#36793;&#38469;&#20998;&#24067;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#23398;&#20064;&#22120;&#65292;&#20854;&#35823;&#24046;&#38543;&#30528;VC&#32500;&#30340;&#21464;&#21270;&#32780;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of sequential prediction in the stochastic setting with an adversary that is allowed to inject clean-label adversarial (or out-of-distribution) examples. Algorithms designed to handle purely stochastic data tend to fail in the presence of such adversarial examples, often leading to erroneous predictions. This is undesirable in many high-stakes applications such as medical recommendations, where abstaining from predictions on adversarial examples is preferable to misclassification. On the other hand, assuming fully adversarial data leads to very pessimistic bounds that are often vacuous in practice.  To capture this motivation, we propose a new model of sequential prediction that sits between the purely stochastic and fully adversarial settings by allowing the learner to abstain from making a prediction at no cost on adversarial examples. Assuming access to the marginal distribution on the non-adversarial examples, we design a learner whose error scales with the VC 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#30456;&#23545;&#20110;&#32447;&#24615;&#32422;&#26463;&#19979;&#30340;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#37117;&#26159;&#28176;&#36827;&#26368;&#20248;&#30340;&#31639;&#27861;&#12290;&#36825;&#20004;&#31181;&#31639;&#27861;&#37117;&#35797;&#22270;&#36319;&#36394;&#22522;&#20110;&#19979;&#30028;&#35745;&#31639;&#21644;&#36890;&#36807;&#23545;&#27491;&#24120;&#38181;&#20307;&#36793;&#30028;&#36827;&#34892;&#21152;&#26435;&#25237;&#24433;&#25152;&#33719;&#24471;&#30340;&#26368;&#20248;&#20998;&#37197;&#12290;</title><link>http://arxiv.org/abs/2306.12774</link><description>&lt;p&gt;
&#32447;&#24615;&#32422;&#26463;&#19979;&#30340;&#22810;&#33218;&#36172;&#21338;&#32431;&#25506;&#32034;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Pure Exploration in Bandits with Linear Constraints. (arXiv:2306.12774v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12774
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#30456;&#23545;&#20110;&#32447;&#24615;&#32422;&#26463;&#19979;&#30340;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#37117;&#26159;&#28176;&#36827;&#26368;&#20248;&#30340;&#31639;&#27861;&#12290;&#36825;&#20004;&#31181;&#31639;&#27861;&#37117;&#35797;&#22270;&#36319;&#36394;&#22522;&#20110;&#19979;&#30028;&#35745;&#31639;&#21644;&#36890;&#36807;&#23545;&#27491;&#24120;&#38181;&#20307;&#36793;&#30028;&#36827;&#34892;&#21152;&#26435;&#25237;&#24433;&#25152;&#33719;&#24471;&#30340;&#26368;&#20248;&#20998;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#20013;&#23384;&#22312;&#32447;&#24615;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#20309;&#22312;&#19968;&#23450;&#32622;&#20449;&#24230;&#19979;&#30830;&#23450;&#26368;&#20248;&#31574;&#30053;&#30340;&#38382;&#39064;&#12290;&#19982;&#26631;&#20934;&#30340;&#26368;&#20248;&#33218;&#35782;&#21035;&#38382;&#39064;&#19981;&#21516;&#65292;&#36825;&#31181;&#24773;&#20917;&#19979;&#30340;&#26368;&#20248;&#31574;&#30053;&#21487;&#33021;&#19981;&#26159;&#30830;&#23450;&#24615;&#30340;&#65292;&#32780;&#26159;&#21487;&#33021;&#22312;&#22810;&#20010;&#33218;&#20043;&#38388;&#36827;&#34892;&#28151;&#21512;&#12290;&#36825;&#31181;&#24773;&#20917;&#25913;&#21464;&#20102;&#38382;&#39064;&#30340;&#20960;&#20309;&#24418;&#29366;&#65292;&#25105;&#20204;&#36890;&#36807;&#20449;&#24687;&#35770;&#19979;&#30028;&#36827;&#34892;&#20102;&#25551;&#36848;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#30456;&#23545;&#20110;&#27492;&#35774;&#32622;&#37117;&#26159;&#28176;&#36827;&#26368;&#20248;&#30340;&#31639;&#27861;&#65292;&#20854;&#20013;&#19968;&#20010;&#22522;&#20110;&#8220;&#36319;&#36394;&#20572;&#27490;&#8221;&#26041;&#27861;&#65292;&#21478;&#19968;&#20010;&#22522;&#20110;&#21338;&#24328;&#29702;&#35770;&#30340;&#26041;&#27861;&#12290;&#36825;&#20004;&#31181;&#31639;&#27861;&#37117;&#35797;&#22270;&#36319;&#36394;&#22522;&#20110;&#19979;&#30028;&#35745;&#31639;&#21644;&#36890;&#36807;&#23545;&#27491;&#24120;&#38181;&#20307;&#36793;&#30028;&#36827;&#34892;&#21152;&#26435;&#25237;&#24433;&#25152;&#33719;&#24471;&#30340;&#26368;&#20248;&#20998;&#37197;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23454;&#35777;&#32467;&#26524;&#65292;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#30028;&#38480;&#65292;&#24182;&#23637;&#31034;&#20102;&#32422;&#26463;&#22914;&#20309;&#25913;&#21464;&#38382;&#39064;&#30340;&#38590;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address the problem of identifying the optimal policy with a fixed confidence level in a multi-armed bandit setup, when \emph{the arms are subject to linear constraints}. Unlike the standard best-arm identification problem which is well studied, the optimal policy in this case may not be deterministic and could mix between several arms. This changes the geometry of the problem which we characterize via an information-theoretic lower bound. We introduce two asymptotically optimal algorithms for this setting, one based on the Track-and-Stop method and the other based on a game-theoretic approach. Both these algorithms try to track an optimal allocation based on the lower bound and computed by a weighted projection onto the boundary of a normal cone. Finally, we provide empirical results that validate our bounds and visualize how constraints change the hardness of the problem.
&lt;/p&gt;</description></item><item><title>GateON&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#35268;&#27169;&#36830;&#32493;&#23398;&#20064;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#36890;&#36807;&#21487;&#23398;&#20064;&#30340;&#27963;&#21160;&#38376;&#25511;&#21644;&#21442;&#25968;&#30456;&#20851;&#24615;&#30340;&#22312;&#32447;&#20272;&#35745;&#26469;&#38450;&#27490;&#37325;&#35201;&#30693;&#35782;&#34987;&#35206;&#30422;&#65292;&#21516;&#26102;&#36890;&#36807;&#23450;&#28857;&#31070;&#32463;&#20803;&#30340;&#37325;&#26032;&#28608;&#27963;&#26426;&#21046;&#35299;&#20915;&#20102;&#32593;&#32476;&#39281;&#21644;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.01690</link><description>&lt;p&gt;
GateON: &#19968;&#31181;&#29992;&#20110;&#22823;&#35268;&#27169;&#36830;&#32493;&#23398;&#20064;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
GateON: an unsupervised method for large scale continual learning. (arXiv:2306.01690v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01690
&lt;/p&gt;
&lt;p&gt;
GateON&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#35268;&#27169;&#36830;&#32493;&#23398;&#20064;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#36890;&#36807;&#21487;&#23398;&#20064;&#30340;&#27963;&#21160;&#38376;&#25511;&#21644;&#21442;&#25968;&#30456;&#20851;&#24615;&#30340;&#22312;&#32447;&#20272;&#35745;&#26469;&#38450;&#27490;&#37325;&#35201;&#30693;&#35782;&#34987;&#35206;&#30422;&#65292;&#21516;&#26102;&#36890;&#36807;&#23450;&#28857;&#31070;&#32463;&#20803;&#30340;&#37325;&#26032;&#28608;&#27963;&#26426;&#21046;&#35299;&#20915;&#20102;&#32593;&#32476;&#39281;&#21644;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#30340;&#30446;&#26631;&#26159;&#22312;&#19981;&#23545;&#26089;&#26399;&#20219;&#21153;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#25353;&#39034;&#24207;&#23398;&#20064;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#32463;&#36807;CL&#35757;&#32451;&#21518;&#20250;&#20986;&#29616;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;&#26377;&#38480;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;'Gate and Obstruct Network'&#65288;GateON&#65289;&#12290;GateON&#23558;&#21487;&#23398;&#20064;&#30340;&#27963;&#21160;&#38376;&#25511;&#19982;&#21442;&#25968;&#30456;&#20851;&#24615;&#30340;&#22312;&#32447;&#20272;&#35745;&#30456;&#32467;&#21512;&#65292;&#20197;&#38450;&#27490;&#37325;&#35201;&#30693;&#35782;&#34987;&#35206;&#30422;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20219;&#21153;&#20043;&#38388;&#29983;&#25104;&#37096;&#20998;&#37325;&#21472;&#30340;&#36335;&#24452;&#65292;&#20801;&#35768;&#22312;&#39034;&#24207;&#23398;&#20064;&#36807;&#31243;&#20013;&#36827;&#34892;&#27491;&#21521;&#21644;&#21453;&#21521;&#36716;&#31227;&#12290;GateON&#36890;&#36807;&#23450;&#28857;&#31070;&#32463;&#20803;&#30340;&#37325;&#26032;&#28608;&#27963;&#26426;&#21046;&#26469;&#35299;&#20915;&#21442;&#25968;&#22266;&#23450;&#21518;&#32593;&#32476;&#39281;&#21644;&#30340;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#22823;&#35268;&#27169;&#36830;&#32493;&#23398;&#20064;&#12290;GateON&#36866;&#29992;&#20110;&#21508;&#31181;&#32593;&#32476;&#65288;&#20840;&#36830;&#25509;&#12289;CNN&#12289;Transformers&#65289;&#65292;&#35745;&#31639;&#22797;&#26434;&#24230;&#20302;&#65292;&#26377;&#25928;&#22320;&#23398;&#20064;&#20102;&#39640;&#36798;100&#20010;MNIST&#23398;&#20064;&#20219;&#21153;&#65292;&#24182;&#22312;&#39044;&#35757;&#32451;BERT&#20013;&#21462;&#24471;&#20102;&#39030;&#23574;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The objective of continual learning (CL) is to learn tasks sequentially without retraining on earlier tasks. However, when subjected to CL, traditional neural networks exhibit catastrophic forgetting and limited generalization. To overcome these problems, we introduce a novel method called 'Gate and Obstruct Network' (GateON). GateON combines learnable gating of activity and online estimation of parameter relevance to safeguard crucial knowledge from being overwritten. Our method generates partially overlapping pathways between tasks which permits forward and backward transfer during sequential learning. GateON addresses the issue of network saturation after parameter fixation by a re-activation mechanism of fixed neurons, enabling large-scale continual learning. GateON is implemented on a wide range of networks (fully-connected, CNN, Transformers), has low computational complexity, effectively learns up to 100 MNIST learning tasks, and achieves top-tier results for pre-trained BERT in
&lt;/p&gt;</description></item><item><title>&#21518;&#32493;&#21069;&#23548;&#20869;&#22312;&#25506;&#32034;&#26159;&#19968;&#31181;&#22522;&#20110;&#26032;&#39062;&#20869;&#22312;&#22870;&#21169;&#30340;&#25506;&#32034;&#31639;&#27861;&#65292;&#23427;&#21033;&#29992;&#22238;&#39038;&#20449;&#24687;&#24182;&#32467;&#21512;&#21069;&#30651;&#20449;&#24687;&#65292;&#20197;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#23454;&#29616;&#39640;&#25928;&#19988;&#29983;&#24577;&#23398;&#21512;&#29702;&#30340;&#25506;&#32034;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2305.15277</link><description>&lt;p&gt;
&#21518;&#32493;&#21069;&#23548;&#20869;&#22312;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Successor-Predecessor Intrinsic Exploration. (arXiv:2305.15277v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15277
&lt;/p&gt;
&lt;p&gt;
&#21518;&#32493;&#21069;&#23548;&#20869;&#22312;&#25506;&#32034;&#26159;&#19968;&#31181;&#22522;&#20110;&#26032;&#39062;&#20869;&#22312;&#22870;&#21169;&#30340;&#25506;&#32034;&#31639;&#27861;&#65292;&#23427;&#21033;&#29992;&#22238;&#39038;&#20449;&#24687;&#24182;&#32467;&#21512;&#21069;&#30651;&#20449;&#24687;&#65292;&#20197;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#23454;&#29616;&#39640;&#25928;&#19988;&#29983;&#24577;&#23398;&#21512;&#29702;&#30340;&#25506;&#32034;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#25506;&#32034;&#23545;&#20110;&#37027;&#20123;&#22806;&#37096;&#22870;&#21169;&#31232;&#32570;&#30340;&#29615;&#22659;&#38750;&#24120;&#37325;&#35201;&#12290;&#26412;&#25991;&#20391;&#37325;&#20110;&#21033;&#29992;&#20869;&#22312;&#22870;&#21169;&#36827;&#34892;&#25506;&#32034;&#65292;&#21363;&#20195;&#29702;&#22120;&#20351;&#29992;&#33258;&#25105;&#29983;&#25104;&#30340;&#20869;&#22312;&#22870;&#21169;&#20020;&#26102;&#22686;&#21152;&#22806;&#37096;&#22870;&#21169;&#12290;&#23613;&#31649;&#20869;&#22312;&#22870;&#21169;&#30340;&#30740;&#31350;&#26377;&#30528;&#24736;&#20037;&#30340;&#21382;&#21490;&#65292;&#20294;&#29616;&#26377;&#30340;&#26041;&#27861;&#37117;&#38598;&#20013;&#20110;&#26681;&#25454;&#29366;&#24577;&#30340;&#26410;&#26469;&#21069;&#26223;&#24230;&#37327;&#26469;&#26500;&#25104;&#20869;&#22312;&#22870;&#21169;&#65292;&#24573;&#35270;&#20102;&#36716;&#31227;&#24207;&#21015;&#30340;&#22238;&#39038;&#32467;&#26500;&#20013;&#25152;&#34164;&#21547;&#30340;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#20195;&#29702;&#22120;&#21487;&#20197;&#21033;&#29992;&#22238;&#39038;&#20449;&#24687;&#26469;&#29983;&#25104;&#20855;&#26377;&#32467;&#26500;&#24863;&#30693;&#33021;&#21147;&#30340;&#25506;&#32034;&#34892;&#20026;&#65292;&#20197;&#22522;&#20110;&#25972;&#20307;&#32780;&#38750;&#23616;&#37096;&#20449;&#24687;&#30340;&#26041;&#24335;&#23454;&#29616;&#39640;&#25928;&#30340;&#25506;&#32034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26032;&#39062;&#20869;&#22312;&#22870;&#21169;&#30340;&#27169;&#22411;&#8212;&#8212;&#21518;&#32493;&#21069;&#23548;&#20869;&#22312;&#25506;&#32034; (SPIE) &#31639;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SPIE &#33021;&#22815;&#20135;&#29983;&#26356;&#21152;&#39640;&#25928;&#21644;&#29983;&#24577;&#23398;&#21512;&#29702;&#30340;&#25506;&#32034;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploration is essential in reinforcement learning, particularly in environments where external rewards are sparse. Here we focus on exploration with intrinsic rewards, where the agent transiently augments the external rewards with self-generated intrinsic rewards. Although the study of intrinsic rewards has a long history, existing methods focus on composing the intrinsic reward based on measures of future prospects of states, ignoring the information contained in the retrospective structure of transition sequences. Here we argue that the agent can utilise retrospective information to generate explorative behaviour with structure-awareness, facilitating efficient exploration based on global instead of local information. We propose Successor-Predecessor Intrinsic Exploration (SPIE), an exploration algorithm based on a novel intrinsic reward combining prospective and retrospective information. We show that SPIE yields more efficient and ethologically plausible exploratory behaviour in e
&lt;/p&gt;</description></item><item><title>Point2SSM&#21487;&#20197;&#30452;&#25509;&#20174;&#28857;&#20113;&#20013;&#26500;&#24314;&#20986;&#35299;&#21078;&#32479;&#35745;&#24418;&#24577;&#27169;&#22411;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#26041;&#27861;&#38656;&#35201;&#26080;&#22122;&#22768;&#34920;&#38754;&#32593;&#26684;&#25110;&#20108;&#36827;&#21046;&#20307;&#31215;&#65292;&#20381;&#36182;&#20110;&#20551;&#35774;&#25110;&#39044;&#23450;&#20041;&#27169;&#26495;&#65292;&#20197;&#21450;&#21516;&#26102;&#20248;&#21270;&#23548;&#33268;&#38271;&#26102;&#38388;&#25512;&#26029;&#26032;&#25968;&#25454;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.14486</link><description>&lt;p&gt;
Point2SSM&#65306;&#20174;&#28857;&#20113;&#23398;&#20064;&#35299;&#21078;&#32467;&#26500;&#30340;&#24418;&#24577;&#21464;&#24322;
&lt;/p&gt;
&lt;p&gt;
Point2SSM: Learning Morphological Variations of Anatomies from Point Cloud. (arXiv:2305.14486v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14486
&lt;/p&gt;
&lt;p&gt;
Point2SSM&#21487;&#20197;&#30452;&#25509;&#20174;&#28857;&#20113;&#20013;&#26500;&#24314;&#20986;&#35299;&#21078;&#32479;&#35745;&#24418;&#24577;&#27169;&#22411;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#26041;&#27861;&#38656;&#35201;&#26080;&#22122;&#22768;&#34920;&#38754;&#32593;&#26684;&#25110;&#20108;&#36827;&#21046;&#20307;&#31215;&#65292;&#20381;&#36182;&#20110;&#20551;&#35774;&#25110;&#39044;&#23450;&#20041;&#27169;&#26495;&#65292;&#20197;&#21450;&#21516;&#26102;&#20248;&#21270;&#23548;&#33268;&#38271;&#26102;&#38388;&#25512;&#26029;&#26032;&#25968;&#25454;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Point2SSM&#65292;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#30452;&#25509;&#20174;&#28857;&#20113;&#31934;&#30830;&#22320;&#26500;&#24314;&#22522;&#20110;&#23545;&#24212;&#20851;&#31995;&#30340;&#35299;&#21078;&#32479;&#35745;&#24418;&#24577;&#27169;&#22411;&#65288;SSM&#65289;&#12290; SSM&#22312;&#20020;&#24202;&#30740;&#31350;&#20013;&#38750;&#24120;&#20851;&#38190;&#65292;&#29992;&#20110;&#20998;&#26512;&#39592;&#39612;&#21644;&#22120;&#23448;&#20013;&#30340;&#32676;&#20307;&#27700;&#24179;&#24418;&#24577;&#21464;&#24322;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;SSM&#21019;&#24314;&#26041;&#27861;&#20855;&#26377;&#19968;&#23450;&#23616;&#38480;&#24615;&#65292;&#22914;&#38656;&#35201;&#26080;&#22122;&#22768;&#34920;&#38754;&#32593;&#26684;&#25110;&#20108;&#36827;&#21046;&#20307;&#31215;&#65292;&#20381;&#36182;&#20110;&#20551;&#35774;&#25110;&#39044;&#23450;&#20041;&#27169;&#26495;&#65292;&#20197;&#21450;&#38024;&#23545;&#25972;&#20010;&#38431;&#21015;&#30340;&#21516;&#26102;&#20248;&#21270;&#23548;&#33268;&#38271;&#26102;&#38388;&#25512;&#26029;&#26032;&#25968;&#25454;&#12290;Point2SSM&#36890;&#36807;&#25552;&#20379;&#25968;&#25454;&#39537;&#21160;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#20811;&#26381;&#36825;&#20123;&#38556;&#30861;&#65292;&#20174;&#21407;&#22987;&#28857;&#20113;&#20013;&#30452;&#25509;&#25512;&#26029;SSM&#65292;&#20943;&#23569;&#20102;&#25512;&#26029;&#36127;&#25285;&#65292;&#24182;&#25552;&#39640;&#20102;&#36866;&#29992;&#24615;&#65292;&#22240;&#20026;&#28857;&#20113;&#26356;&#23481;&#26131;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Point2SSM, a novel unsupervised learning approach that can accurately construct correspondence-based statistical shape models (SSMs) of anatomy directly from point clouds. SSMs are crucial in clinical research for analyzing the population-level morphological variation in bones and organs. However, traditional methods for creating SSMs have limitations that hinder their widespread adoption, such as the need for noise-free surface meshes or binary volumes, reliance on assumptions or predefined templates, and simultaneous optimization of the entire cohort leading to lengthy inference times given new data. Point2SSM overcomes these barriers by providing a data-driven solution that infers SSMs directly from raw point clouds, reducing inference burdens and increasing applicability as point clouds are more easily acquired. Deep learning on 3D point clouds has seen recent success in unsupervised representation learning, point-to-point matching, and shape correspondence; however, t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25968;&#25454;&#27169;&#31946;&#21270;&#26469;&#32531;&#35299;&#26631;&#31614;&#22122;&#22768;&#30340;&#26041;&#27861;&#65292;&#21363;&#28155;&#21152;&#39069;&#22806;&#30340;&#12289;&#20114;&#34917;&#30340;&#20505;&#36873;&#26631;&#31614;&#65292;&#21033;&#29992;&#25152;&#35859;&#30340;&#36229;&#38598;&#23398;&#20064;&#26694;&#26550;&#26500;&#24314;&#22522;&#20110;&#32622;&#20449;&#38408;&#20540;&#30340;&#38598;&#21512;&#20540;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2305.13764</link><description>&lt;p&gt;
&#36890;&#36807;&#25968;&#25454;&#27169;&#31946;&#21270;&#32531;&#35299;&#26631;&#31614;&#22122;&#22768;
&lt;/p&gt;
&lt;p&gt;
Mitigating Label Noise through Data Ambiguation. (arXiv:2305.13764v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13764
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25968;&#25454;&#27169;&#31946;&#21270;&#26469;&#32531;&#35299;&#26631;&#31614;&#22122;&#22768;&#30340;&#26041;&#27861;&#65292;&#21363;&#28155;&#21152;&#39069;&#22806;&#30340;&#12289;&#20114;&#34917;&#30340;&#20505;&#36873;&#26631;&#31614;&#65292;&#21033;&#29992;&#25152;&#35859;&#30340;&#36229;&#38598;&#23398;&#20064;&#26694;&#26550;&#26500;&#24314;&#22522;&#20110;&#32622;&#20449;&#38408;&#20540;&#30340;&#38598;&#21512;&#20540;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#31614;&#22122;&#22768;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#65292;&#20855;&#26377;&#39640;&#34920;&#29616;&#33021;&#21147;&#30340;&#22823;&#22411;&#27169;&#22411;&#20027;&#23548;&#20102;&#35813;&#39046;&#22495;&#12290;&#36825;&#31181;&#27169;&#22411;&#23481;&#26131;&#35760;&#24518;&#38169;&#35823;&#30340;&#26631;&#31614;&#65292;&#20174;&#32780;&#25439;&#23475;&#27867;&#21270;&#24615;&#33021;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21253;&#25324;&#24378;&#20581;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#26356;&#22797;&#26434;&#30340;&#26631;&#31614;&#26657;&#27491;&#26041;&#27861;&#12290;&#40065;&#26834;&#24615;&#25439;&#22833;&#20989;&#25968;&#30001;&#20110;&#31616;&#21333;&#32780;&#20855;&#26377;&#21560;&#24341;&#21147;&#65292;&#20294;&#36890;&#24120;&#32570;&#20047;&#28789;&#27963;&#24615;&#65292;&#32780;&#26631;&#31614;&#26657;&#27491;&#36890;&#24120;&#20250;&#22686;&#21152;&#35757;&#32451;&#35774;&#32622;&#30340;&#22797;&#26434;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#8220;&#27169;&#31946;&#21270;&#8221;&#30446;&#26631;&#20449;&#24687;&#26469;&#35299;&#20915;&#20004;&#31181;&#26041;&#27861;&#30340;&#32570;&#28857;&#65292;&#22312;&#35266;&#23519;&#21040;&#30340;&#35757;&#32451;&#26631;&#31614;&#19981;&#36275;&#22815;&#21487;&#20449;&#26102;&#65292;&#28155;&#21152;&#38468;&#21152;&#30340;&#12289;&#20114;&#34917;&#30340;&#20505;&#36873;&#26631;&#31614;&#12290;&#26356;&#30830;&#20999;&#22320;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#25152;&#35859;&#30340;&#36229;&#38598;&#23398;&#20064;&#26694;&#26550;&#26469;&#26500;&#24314;&#22522;&#20110;&#32622;&#20449;&#38408;&#20540;&#30340;&#38598;&#21512;&#20540;&#30446;&#26631;&#65292;&#36825;&#20123;&#30446;&#26631;&#25552;&#20379;&#19981;&#31934;&#30830;&#20294;&#26356;&#21487;&#38752;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Label noise poses an important challenge in machine learning, especially in deep learning, in which large models with high expressive power dominate the field. Models of that kind are prone to memorizing incorrect labels, thereby harming generalization performance. Many methods have been proposed to address this problem, including robust loss functions and more complex label correction approaches. Robust loss functions are appealing due to their simplicity, but typically lack flexibility, while label correction usually adds substantial complexity to the training setup. In this paper, we suggest to address the shortcomings of both methodologies by "ambiguating" the target information, adding additional, complementary candidate labels in case the learner is not sufficiently convinced of the observed training label. More precisely, we leverage the framework of so-called superset learning to construct set-valued targets based on a confidence threshold, which deliver imprecise yet more reli
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;CDCD&#26694;&#26550;&#30340;&#27665;&#20027;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#65288;DDLM&#65289;&#65292;&#24182;&#36890;&#36807;GLUE&#22522;&#20934;&#27979;&#35797;&#20102;&#20854;&#30693;&#35782;&#36716;&#31227;&#33021;&#21147;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;DDLM&#35757;&#32451;&#21644;&#35780;&#20272;&#27969;&#31243;&#20197;&#21450;&#24050;&#35757;&#32451;&#30340;DDLM&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.10818</link><description>&lt;p&gt;
&#27665;&#20027;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Democratized Diffusion Language Model. (arXiv:2305.10818v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10818
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;CDCD&#26694;&#26550;&#30340;&#27665;&#20027;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#65288;DDLM&#65289;&#65292;&#24182;&#36890;&#36807;GLUE&#22522;&#20934;&#27979;&#35797;&#20102;&#20854;&#30693;&#35782;&#36716;&#31227;&#33021;&#21147;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;DDLM&#35757;&#32451;&#21644;&#35780;&#20272;&#27969;&#31243;&#20197;&#21450;&#24050;&#35757;&#32451;&#30340;DDLM&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25193;&#25955;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#26377;&#28508;&#22312;&#22909;&#22788;&#65292;&#20294;&#30446;&#21069;&#20844;&#24320;&#30340;&#23454;&#29616;&#12289;&#35757;&#32451;&#27169;&#22411;&#25110;&#21487;&#37325;&#29616;&#30340;&#35757;&#32451;&#31243;&#24207;&#24182;&#19981;&#23384;&#22312;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;CDCD&#26694;&#26550;&#30340;&#27665;&#20027;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#65288;DDLM&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;C4&#25968;&#25454;&#38598;&#31616;&#21270;&#30340;DDLM&#35757;&#32451;&#27969;&#31243;&#65292;&#24182;&#23545;&#35757;&#32451;&#27169;&#22411;&#30340;&#34892;&#20026;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#29992;&#20110;&#36895;&#24230;&#26356;&#24555;&#30340;&#37319;&#26679;&#30340;&#26032;&#22411;&#26089;&#26399;&#36864;&#20986;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#38024;&#23545;&#20351;&#29992;&#24471;&#20998;&#25554;&#20540;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;&#30001;&#20110;&#27492;&#21069;&#27809;&#26377;&#30740;&#31350;&#26088;&#22312;&#20351;&#29992;&#39044;&#35757;&#32451;&#25193;&#25955;LM&#35299;&#20915;&#19979;&#28216;&#20219;&#21153;&#65288;&#20363;&#22914;&#20998;&#31867;&#20219;&#21153;&#65289;&#65292;&#25105;&#20204;&#22312;GLUE&#22522;&#20934;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#20197;&#30740;&#31350;DDLM&#30340;&#30693;&#35782;&#36716;&#31227;&#33021;&#21147;&#12290;&#36890;&#36807;&#26412;&#25991;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21487;&#20379;&#20854;&#20182;&#30740;&#31350;&#20154;&#21592;&#20351;&#29992;&#30340;DDLM&#35757;&#32451;&#21644;&#35780;&#20272;&#27969;&#31243;&#20197;&#21450;&#39044;&#20808;&#35757;&#32451;&#30340;DDLM&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#22312;&#26410;&#26469;&#30340;D&#30456;&#20851;&#30340;&#30740;&#31350;&#20013;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the potential benefits of Diffusion Models for NLP applications, publicly available implementations, trained models, or reproducible training procedures currently need to be publicly available. We present the Democratized Diffusion Language Model (DDLM), based on the Continuous Diffusion for Categorical Data (CDCD) framework, to address these challenges. We propose a simplified training procedure for DDLM using the C4 dataset and perform an in-depth analysis of the trained model's behavior. Furthermore, we introduce a novel early-exiting strategy for faster sampling with models trained with score interpolation. Since no previous works aimed at solving downstream tasks with pre-trained Diffusion LM (e.g., classification tasks), we experimented with GLUE Benchmark to study the ability of DDLM to transfer knowledge. With this paper, we propose available training and evaluation pipelines to other researchers and pre-trained DDLM models, which could be used in future research with D
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#35889;&#22270;&#29702;&#35770;&#30340;&#38142;&#25509;&#25512;&#33616;&#31639;&#27861;ERA-Link&#65292;&#26088;&#22312;&#32531;&#35299;&#29616;&#26377;&#25512;&#33616;&#31639;&#27861;&#24102;&#26469;&#30340;&#20449;&#24687;&#23396;&#23707;&#21644;&#31038;&#20250;&#25104;&#35265;&#65292;&#23454;&#29616;&#31038;&#20132;&#32593;&#32476;&#24179;&#21488;&#30340;&#31038;&#20250;&#27491;&#20041;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2305.03223</link><description>&lt;p&gt;
&#31038;&#20250;&#27491;&#20041;&#31639;&#27861;&#65306;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#24179;&#26435;&#34892;&#21160;
&lt;/p&gt;
&lt;p&gt;
Algorithms for Social Justice: Affirmative Action in Social Networks. (arXiv:2305.03223v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03223
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#35889;&#22270;&#29702;&#35770;&#30340;&#38142;&#25509;&#25512;&#33616;&#31639;&#27861;ERA-Link&#65292;&#26088;&#22312;&#32531;&#35299;&#29616;&#26377;&#25512;&#33616;&#31639;&#27861;&#24102;&#26469;&#30340;&#20449;&#24687;&#23396;&#23707;&#21644;&#31038;&#20250;&#25104;&#35265;&#65292;&#23454;&#29616;&#31038;&#20132;&#32593;&#32476;&#24179;&#21488;&#30340;&#31038;&#20250;&#27491;&#20041;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38142;&#25509;&#25512;&#33616;&#31639;&#27861;&#23545;&#20110;&#19990;&#30028;&#21508;&#22320;&#25968;&#21313;&#20159;&#29992;&#25143;&#30340;&#20154;&#38469;&#20851;&#31995;&#20135;&#29983;&#20102;&#24433;&#21709;&#12290;&#20026;&#20102;&#26368;&#22823;&#21270;&#30456;&#20851;&#24615;&#65292;&#23427;&#20204;&#36890;&#24120;&#24314;&#35758;&#36830;&#25509;&#30456;&#20114;&#30456;&#20284;&#30340;&#29992;&#25143;&#12290;&#28982;&#32780;&#65292;&#36825;&#34987;&#21457;&#29616;&#20250;&#20135;&#29983;&#20449;&#24687;&#23396;&#23707;&#65292;&#21152;&#21095;&#24369;&#21183;&#31361;&#20986;&#32676;&#20307;&#25152;&#36973;&#21463;&#30340;&#23396;&#31435;&#65292;&#24182;&#24310;&#32493;&#31038;&#20250;&#25104;&#35265;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38480;&#21046;&#65292;&#22823;&#37327;&#30740;&#31350;&#33268;&#21147;&#20110;&#23454;&#29616;&#20844;&#24179;&#30340;&#38142;&#25509;&#25512;&#33616;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#24182;&#19981;&#36136;&#30097;&#38142;&#25509;&#25512;&#33616;&#31639;&#27861;&#30340;&#26368;&#32456;&#30446;&#26631;&#65292;&#21363;&#25968;&#25454;&#20132;&#26131;&#30340;&#22797;&#26434;&#21830;&#19994;&#27169;&#22411;&#20013;&#29992;&#25143;&#21442;&#19982;&#30340;&#36135;&#24065;&#21270;&#12290;&#26412;&#25991;&#20027;&#24352;&#23454;&#29616;&#31038;&#20132;&#32593;&#32476;&#24179;&#21488;&#29609;&#23478;&#21644;&#30446;&#30340;&#30340;&#22810;&#26679;&#21270;&#65292;&#20197;&#23454;&#29616;&#31038;&#20250;&#27491;&#20041;&#12290;&#20026;&#20102;&#35828;&#26126;&#36825;&#19968;&#27010;&#24565;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ERA-Link&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#35889;&#22270;&#29702;&#35770;&#30340;&#26032;&#22411;&#38142;&#25509;&#25512;&#33616;&#31639;&#27861;&#65292;&#21487;&#20197;&#25269;&#28040;&#31995;&#32479;&#24615;&#30340;&#31038;&#20250;&#27495;&#35270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Link recommendation algorithms contribute to shaping human relations of billions of users worldwide in social networks. To maximize relevance, they typically propose connecting users that are similar to each other. This has been found to create information silos, exacerbating the isolation suffered by vulnerable salient groups and perpetuating societal stereotypes. To mitigate these limitations, a significant body of work has been devoted to the implementation of fair link recommendation methods. However, most approaches do not question the ultimate goal of link recommendation algorithms, namely the monetization of users' engagement in intricate business models of data trade. This paper advocates for a diversification of players and purposes of social network platforms, aligned with the pursue of social justice. To illustrate this conceptual goal, we present ERA-Link, a novel link recommendation algorithm based on spectral graph theory that counteracts the systemic societal discriminat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#30340;MAE&#25216;&#26415;&#39044;&#21069;&#32622;&#35757;&#32451;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#20159;&#32423;&#39044;&#35757;&#32451;&#35268;&#27169;&#65292;&#24182;&#21487;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#25910;&#25947;&#24615;&#21644;&#19979;&#28216;&#36716;&#31227;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.13496</link><description>&lt;p&gt;
MAE&#39044;&#21069;&#32622;&#35757;&#32451;&#23545;&#20110;&#20159;&#32423;&#39044;&#35757;&#32451;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
The effectiveness of MAE pre-pretraining for billion-scale pretraining. (arXiv:2303.13496v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13496
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#30340;MAE&#25216;&#26415;&#39044;&#21069;&#32622;&#35757;&#32451;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#20159;&#32423;&#39044;&#35757;&#32451;&#35268;&#27169;&#65292;&#24182;&#21487;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#25910;&#25947;&#24615;&#21644;&#19979;&#28216;&#36716;&#31227;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#29992;&#20110;&#35270;&#35273;&#35782;&#21035;&#20219;&#21153;&#30340;&#26631;&#20934;&#39044;&#35757;&#32451;-&#24494;&#35843;&#33539;&#24335;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#26368;&#20808;&#36827;&#30340;&#22522;&#30784;&#27169;&#22411;&#20351;&#29992;&#25968;&#21313;&#20159;&#24352;&#22270;&#20687;&#30340;&#22823;&#35268;&#27169;&#65288;&#24369;&#65289;&#30417;&#30563;&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#39069;&#22806;&#30340;&#39044;&#21069;&#32622;&#35757;&#32451;&#38454;&#27573;&#65292;&#23427;&#20351;&#29992;&#20102;&#33258;&#25105;&#30417;&#30563;&#30340;MAE&#25216;&#26415;&#26469;&#21021;&#22987;&#21270;&#27169;&#22411;&#12290;&#34429;&#28982;MAE&#25216;&#26415;&#20165;&#34987;&#35777;&#26126;&#33021;&#22815;&#19982;&#27169;&#22411;&#22823;&#23567;&#30456;&#32553;&#25918;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#23427;&#20063;&#21487;&#20197;&#38543;&#25968;&#25454;&#38598;&#22823;&#23567;&#32553;&#25918;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22522;&#20110;MAE&#30340;&#39044;&#21069;&#32622;&#35757;&#32451;&#21487;&#21516;&#26102;&#36866;&#29992;&#20110;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#35268;&#27169;&#12290;&#39044;&#21069;&#32622;&#35757;&#32451;&#22312;&#19968;&#31995;&#21015;&#27169;&#22411;&#35268;&#27169;&#65288;&#21442;&#25968;&#25968;&#30334;&#19975;&#21040;&#25968;&#21313;&#20159;&#65289;&#21644;&#25968;&#25454;&#38598;&#22823;&#23567;&#65288;&#22270;&#20687;&#25968;&#30334;&#19975;&#21040;&#25968;&#21313;&#20159;&#65289;&#19978;&#19968;&#33268;&#25552;&#39640;&#20102;&#27169;&#22411;&#25910;&#25947;&#24615;&#21644;&#19979;&#28216;&#36716;&#31227;&#24615;&#33021;&#65292;&#19988;&#25105;&#20204;&#36824;&#27979;&#37327;&#20102;&#20854;&#22312;10&#20010;&#19981;&#21516;&#30340;&#35270;&#35273;&#35782;&#21035;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#22270;&#20687;&#20998;&#31867;&#12289;&#35270;&#39057;&#35782;&#21035;&#21644;&#30446;&#26631;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper revisits the standard pretrain-then-finetune paradigm used in computer vision for visual recognition tasks. Typically, state-of-the-art foundation models are pretrained using large scale (weakly) supervised datasets with billions of images. We introduce an additional pre-pretraining stage that is simple and uses the self-supervised MAE technique to initialize the model. While MAE has only been shown to scale with the size of models, we find that it scales with the size of the training dataset as well. Thus, our MAE-based pre-pretraining scales with both model and data size making it applicable for training foundation models. Pre-pretraining consistently improves both the model convergence and the downstream transfer performance across a range of model scales (millions to billions of parameters), and dataset sizes (millions to billions of images). We measure the effectiveness of pre-pretraining on 10 different visual recognition tasks spanning image classification, video reco
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36880;&#23618;&#21442;&#25968;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#29616;&#20869;&#32622;&#40065;&#26834;&#24615;&#20445;&#35777;&#30340;1D&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;CNN&#29305;&#24449;&#30340;Lipschitz&#24120;&#25968;&#20316;&#20026;&#40065;&#26834;&#24615;&#24230;&#37327;&#65292;&#24182;&#20351;&#29992;Cayley&#21464;&#25442;&#21644;&#21487;&#25511;&#24615;Gram&#30697;&#26469;&#23454;&#29616;CNN&#30340;Lipschitz&#36830;&#32493;&#24615;&#21644;&#26080;&#32422;&#26463;&#35757;&#32451;&#65292;&#26368;&#21518;&#22312;&#24515;&#24459;&#22833;&#24120;&#25968;&#25454;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#25913;&#36827;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.11835</link><description>&lt;p&gt;
&#21033;&#29992;Cayley&#21464;&#25442;&#21644;&#21487;&#25511;&#24615;Gram&#30697;&#30340;Lipschitz-bounded 1D&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(arXiv:2303.11835v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
Lipschitz-bounded 1D convolutional neural networks using the Cayley transform and the controllability Gramian. (arXiv:2303.11835v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11835
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36880;&#23618;&#21442;&#25968;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#29616;&#20869;&#32622;&#40065;&#26834;&#24615;&#20445;&#35777;&#30340;1D&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;CNN&#29305;&#24449;&#30340;Lipschitz&#24120;&#25968;&#20316;&#20026;&#40065;&#26834;&#24615;&#24230;&#37327;&#65292;&#24182;&#20351;&#29992;Cayley&#21464;&#25442;&#21644;&#21487;&#25511;&#24615;Gram&#30697;&#26469;&#23454;&#29616;CNN&#30340;Lipschitz&#36830;&#32493;&#24615;&#21644;&#26080;&#32422;&#26463;&#35757;&#32451;&#65292;&#26368;&#21518;&#22312;&#24515;&#24459;&#22833;&#24120;&#25968;&#25454;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#25913;&#36827;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#31181;&#29992;&#20110;1D&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30340;&#36880;&#23618;&#21442;&#25968;&#21270;&#65292;&#20855;&#26377;&#20869;&#32622;&#30340;&#31471;&#21040;&#31471;&#40065;&#26834;&#24615;&#20445;&#35777;&#12290;&#25105;&#20204;&#20351;&#29992;CNN&#29305;&#24449;&#30340;Lipschitz&#24120;&#25968;&#20316;&#20026;&#40065;&#26834;&#24615;&#24230;&#37327;&#12290;&#25105;&#20204;&#22522;&#20110;Cayley&#21464;&#25442;&#23545;&#27491;&#20132;&#30697;&#38453;&#36827;&#34892;&#21442;&#25968;&#21270;&#20197;&#21450;&#23545;&#21367;&#31215;&#23618;&#30340;&#29366;&#24577;&#31354;&#38388;&#34920;&#24449;&#30340;&#21487;&#25511;&#24615;Gram&#30697;&#36827;&#34892;&#21442;&#25968;&#21270;&#12290;&#25152;&#25552;&#20986;&#30340;&#21442;&#25968;&#21270;&#35774;&#35745;&#28385;&#36275;&#32447;&#24615;&#30697;&#38453;&#19981;&#31561;&#24335;&#65292;&#20174;&#32780;&#23454;&#29616;CNN&#30340;Lipschitz&#36830;&#32493;&#24615;&#65292;&#36827;&#19968;&#27493;&#23454;&#29616;Lipschitz-bounded 1D CNNs&#30340;&#26080;&#32422;&#26463;&#35757;&#32451;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;&#24515;&#24459;&#22833;&#24120;&#25968;&#25454;&#36827;&#34892;Lipschitz-bounded 1D CNNs&#30340;&#20998;&#31867;&#35757;&#32451;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#25913;&#36827;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We establish a layer-wise parameterization for 1D convolutional neural networks (CNNs) with built-in end-to-end robustness guarantees. Herein, we use the Lipschitz constant of the input-output mapping characterized by a CNN as a robustness measure. We base our parameterization on the Cayley transform that parameterizes orthogonal matrices and the controllability Gramian for the state space representation of the convolutional layers. The proposed parameterization by design fulfills linear matrix inequalities that are sufficient for Lipschitz continuity of the CNN, which further enables unconstrained training of Lipschitz-bounded 1D CNNs. Finally, we train Lipschitz-bounded 1D CNNs for the classification of heart arrythmia data and show their improved robustness.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#23637;&#31034;&#20102;&#39046;&#22495;&#38543;&#26426;&#21270;&#26041;&#27861;&#22914;&#20309;&#36890;&#36807;&#22686;&#24378;&#36719;&#26426;&#22120;&#20154;&#30340;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#26469;&#35299;&#20915;&#27169;&#22411;&#19982;&#30495;&#23454;&#24179;&#21488;&#20043;&#38388;&#30340;&#39046;&#22495;&#24046;&#36317;&#65292;&#23454;&#29616;&#40065;&#26834;&#12289;&#32463;&#27982;&#21644;&#26377;&#25928;&#30340;&#38381;&#29615;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2303.04136</link><description>&lt;p&gt;
&#38754;&#21521;&#36719;&#26426;&#22120;&#20154;&#30340;&#39046;&#22495;&#38543;&#26426;&#21270;&#23454;&#29616;&#40065;&#26834;&#12289;&#32463;&#27982;&#21644;&#26377;&#25928;&#30340;&#38381;&#29615;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Domain Randomization for Robust, Affordable and Effective Closed-loop Control of Soft Robots. (arXiv:2303.04136v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04136
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#23637;&#31034;&#20102;&#39046;&#22495;&#38543;&#26426;&#21270;&#26041;&#27861;&#22914;&#20309;&#36890;&#36807;&#22686;&#24378;&#36719;&#26426;&#22120;&#20154;&#30340;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#26469;&#35299;&#20915;&#27169;&#22411;&#19982;&#30495;&#23454;&#24179;&#21488;&#20043;&#38388;&#30340;&#39046;&#22495;&#24046;&#36317;&#65292;&#23454;&#29616;&#40065;&#26834;&#12289;&#32463;&#27982;&#21644;&#26377;&#25928;&#30340;&#38381;&#29615;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36719;&#26426;&#22120;&#20154;&#30001;&#20110;&#20854;&#22266;&#26377;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#36866;&#24212;&#24615;&#32780;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#26080;&#38480;&#30340;&#33258;&#30001;&#24230;&#20351;&#24471;&#23427;&#20204;&#30340;&#24314;&#27169;&#21464;&#24471;&#22256;&#38590;&#65292;&#32780;&#19988;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#21482;&#33021;&#24471;&#21040;&#19968;&#20010;&#36817;&#20284;&#30340;&#25551;&#36848;&#12290;&#36825;&#20010;&#25361;&#25112;&#20351;&#24471;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#26041;&#27861;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#21464;&#24471;&#20302;&#25928;&#65292;&#22240;&#20026;&#27169;&#22411;&#19982;&#30495;&#23454;&#24179;&#21488;&#20043;&#38388;&#23384;&#22312;&#24040;&#22823;&#30340;&#39046;&#22495;&#24046;&#36317;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#23637;&#31034;&#20102;&#39046;&#22495;&#38543;&#26426;&#21270;&#65288;DR&#65289;&#22914;&#20309;&#36890;&#36807;&#22686;&#24378;&#36719;&#26426;&#22120;&#20154;&#30340;RL&#31574;&#30053;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65306;i&#65289;&#23545;&#26410;&#30693;&#21160;&#21147;&#23398;&#21442;&#25968;&#30340;&#40065;&#26834;&#24615;&#65307;ii&#65289;&#36890;&#36807;&#21033;&#29992;&#26356;&#31616;&#21270;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#36827;&#34892;&#23398;&#20064;&#65292;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#65307;iii&#65289;&#26356;&#22909;&#30340;&#29615;&#22659;&#25506;&#32034;&#65292;&#21487;&#20197;&#21033;&#29992;&#29615;&#22659;&#32422;&#26463;&#20197;&#23454;&#29616;&#26368;&#20339;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#31639;&#27861;&#25193;&#23637;&#26469;&#33258;&#20808;&#21069;&#36866;&#24212;&#24615;&#39046;&#22495;&#38543;&#26426;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#25512;&#26029;&#21160;&#24577;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Soft robots are gaining popularity thanks to their intrinsic safety to contacts and adaptability. However, the potentially infinite number of Degrees of Freedom makes their modeling a daunting task, and in many cases only an approximated description is available. This challenge makes reinforcement learning (RL) based approaches inefficient when deployed on a realistic scenario, due to the large domain gap between models and the real platform. In this work, we demonstrate, for the first time, how Domain Randomization (DR) can solve this problem by enhancing RL policies for soft robots with: i) robustness w.r.t. unknown dynamics parameters; ii) reduced training times by exploiting drastically simpler dynamic models for learning; iii) better environment exploration, which can lead to exploitation of environmental constraints for optimal performance. Moreover, we introduce a novel algorithmic extension to previous adaptive domain randomization methods for the automatic inference of dynamic
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26059;&#36716;&#19981;&#21464;&#37327;&#37327;&#21270;&#65288;RIQ&#65289;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#23618;&#27425;&#19978;&#23454;&#29616;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#65292;&#29992;&#20110;&#21518;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21387;&#32553;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#22312;&#21387;&#32553;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;&#22312;&#22810;&#31181;&#27169;&#22411;&#21644;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#20005;&#26684;&#35780;&#20272;&#65292;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.03106</link><description>&lt;p&gt;
&#26059;&#36716;&#19981;&#21464;&#37327;&#37327;&#21270;&#29992;&#20110;&#27169;&#22411;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Rotation Invariant Quantization for Model Compression. (arXiv:2303.03106v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03106
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26059;&#36716;&#19981;&#21464;&#37327;&#37327;&#21270;&#65288;RIQ&#65289;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#23618;&#27425;&#19978;&#23454;&#29616;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#65292;&#29992;&#20110;&#21518;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21387;&#32553;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#22312;&#21387;&#32553;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;&#22312;&#22810;&#31181;&#27169;&#22411;&#21644;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#20005;&#26684;&#35780;&#20272;&#65292;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#27169;&#22411;&#21387;&#32553;&#26159;&#19968;&#31181;&#23558;&#22823;&#22411;&#12289;&#28040;&#32791;&#20869;&#23384;&#30340;&#27169;&#22411;&#37096;&#32626;&#21040;&#20869;&#23384;&#36164;&#28304;&#26377;&#38480;&#35774;&#22791;&#19978;&#30340;&#21560;&#24341;&#20154;&#30340;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;NN&#27169;&#22411;&#21387;&#32553;&#30340;&#36895;&#29575;-&#22833;&#30495;&#26435;&#34913;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26059;&#36716;&#19981;&#21464;&#37327;&#37327;&#21270;&#65288;RIQ&#65289;&#25216;&#26415;&#65292;&#23427;&#21033;&#29992;&#19968;&#20010;&#21333;&#19968;&#21442;&#25968;&#37327;&#21270;&#25972;&#20010;NN&#27169;&#22411;&#65292;&#22312;&#27599;&#20010;&#23618;&#27425;&#19978;&#24471;&#21040;&#19981;&#21516;&#30340;&#36895;&#29575;&#65292;&#21363;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26059;&#36716;&#19981;&#21464;&#37327;&#26041;&#27861;&#22312;&#21387;&#32553;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#23545;RIQ&#36827;&#34892;&#20102;&#20005;&#26684;&#35780;&#20272;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#21508;&#31181;&#27169;&#22411;&#21644;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#12290;&#20363;&#22914;&#65292;RIQ&#22312;&#39044;&#35757;&#32451;&#30340;VGG&#31264;&#23494;&#21644;&#20462;&#21098;&#27169;&#22411;&#19978;&#20998;&#21035;&#23454;&#29616;&#20102;19.4&#20493;&#21644;52.9&#20493;&#30340;&#21387;&#32553;&#27604;&#65292;&#31934;&#24230;&#38477;&#20302;&#23567;&#20110;0.4%&#12290;&#20195;&#30721;&#21487;&#20197;&#22312;\url{https://github.com/ehaleva/RIQ}&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Post-training Neural Network (NN) model compression is an attractive approach for deploying large, memory-consuming models on devices with limited memory resources. In this study, we investigate the rate-distortion tradeoff for NN model compression. First, we suggest a Rotation-Invariant Quantization (RIQ) technique that utilizes a single parameter to quantize the entire NN model, yielding a different rate at each layer, i.e., mixed-precision quantization. Then, we prove that our rotation-invariant approach is optimal in terms of compression. We rigorously evaluate RIQ and demonstrate its capabilities on various models and tasks. For example, RIQ facilitates $\times 19.4$ and $\times 52.9$ compression ratios on pre-trained VGG dense and pruned models, respectively, with $&lt;0.4\%$ accuracy degradation. Code is available in \url{https://github.com/ehaleva/RIQ}.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#30456;&#20851;&#32858;&#31867;&#20013;&#25104;&#23545;&#30456;&#20284;&#24615;&#19981;&#20107;&#20808;&#32473;&#20986;&#30340;&#24773;&#20917;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#65292;&#36866;&#24212;&#21508;&#31181;&#30456;&#20851;&#32858;&#31867;&#31639;&#27861;&#21644;&#26597;&#35810;&#31574;&#30053;&#65292;&#21516;&#26102;&#20855;&#26377;&#36866;&#24212;&#24615;&#28789;&#27963;&#12289;&#22122;&#22768;&#40065;&#26834;&#24615;&#31561;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2302.10295</link><description>&lt;p&gt;
&#20351;&#29992;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#30340;&#30456;&#20851;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Correlation Clustering with Active Learning of Pairwise Similarities. (arXiv:2302.10295v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10295
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#30456;&#20851;&#32858;&#31867;&#20013;&#25104;&#23545;&#30456;&#20284;&#24615;&#19981;&#20107;&#20808;&#32473;&#20986;&#30340;&#24773;&#20917;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#65292;&#36866;&#24212;&#21508;&#31181;&#30456;&#20851;&#32858;&#31867;&#31639;&#27861;&#21644;&#26597;&#35810;&#31574;&#30053;&#65292;&#21516;&#26102;&#20855;&#26377;&#36866;&#24212;&#24615;&#28789;&#27963;&#12289;&#22122;&#22768;&#40065;&#26834;&#24615;&#31561;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#20851;&#32858;&#31867;&#26159;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#35774;&#32622;&#65292;&#22788;&#29702;&#27491;&#36127;&#30456;&#20284;&#24615;&#23545;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#24773;&#20917;&#65292;&#21363;&#25104;&#23545;&#30456;&#20284;&#24615;&#19981;&#20107;&#20808;&#32473;&#20986;&#65292;&#24517;&#39035;&#20197;&#39640;&#25928;&#30340;&#26041;&#24335;&#26597;&#35810;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#65292;&#38024;&#23545;&#36825;&#20010;&#20219;&#21153;&#20855;&#26377;&#22810;&#31181;&#20248;&#21183;&#65292;&#20363;&#22914;&#65292;&#29992;&#25143;/&#27880;&#37322;&#32773;&#21487;&#20197;&#25552;&#20379;&#21508;&#31181;&#21453;&#39304;&#31867;&#22411;&#12289;&#36866;&#24212;&#20219;&#20309;&#30456;&#20851;&#32858;&#31867;&#31639;&#27861;&#21644;&#26597;&#35810;&#31574;&#30053;&#20197;&#21450;&#23545;&#22122;&#22768;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#21644;&#20998;&#26512;&#20102;&#19968;&#20123;&#36866;&#21512;&#36825;&#31181;&#35774;&#32622;&#30340;&#26032;&#30340;&#26597;&#35810;&#31574;&#30053;&#12290;&#36890;&#36807;&#20960;&#20010;&#23454;&#39564;&#30740;&#31350;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#26694;&#26550;&#21644;&#25152;&#25552;&#20986;&#30340;&#26597;&#35810;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Correlation clustering is a well-known unsupervised learning setting that deals with positive and negative pairwise similarities. In this paper, we study the case where the pairwise similarities are not given in advance and must be queried in a cost-efficient way. Thereby, we develop a generic active learning framework for this task that benefits from several advantages, e.g., flexibility in the type of feedback that a user/annotator can provide, adaptation to any correlation clustering algorithm and query strategy, and robustness to noise. In addition, we propose and analyze a number of novel query strategies suited to this setting. We demonstrate the effectiveness of our framework and the proposed query strategies via several experimental studies.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#20998;&#24067;&#36716;&#21464;&#30340;&#20915;&#26007;&#23545;&#25239;&#38382;&#39064;&#65292;&#24182;&#25506;&#35752;&#20102;&#35774;&#35745;&#33258;&#36866;&#24212;&#31639;&#27861;&#20197;&#35299;&#20915;&#21160;&#24577;&#36951;&#25022;&#30340;&#38382;&#39064;&#65292;&#32467;&#26524;&#21457;&#29616;&#21462;&#20915;&#20110;&#24213;&#23618;&#20559;&#22909;&#20998;&#24067;&#30340;&#23646;&#24615;&#12290;&#36798;&#21040;$O(\sqrt{K\tilde{L}T})$&#30340;&#21160;&#24577;&#36951;&#25022;&#26159;&#19981;&#21487;&#33021;&#30340;&#65307;&#23545;&#20110;$\text{SST} \cap \text{STI}$&#24773;&#20917;&#65292;&#23384;&#22312;&#19968;&#31181;&#31639;&#27861;&#23454;&#29616;&#21160;&#24577;&#36951;&#25022;&#20026;$O(\sqrt{K\tilde{L}T})$&#12290;</title><link>http://arxiv.org/abs/2302.06595</link><description>&lt;p&gt;
&#20309;&#26102;&#21487;&#20197;&#36861;&#36394;&#21040;&#20915;&#26007;&#23545;&#25239;&#20013;&#30340;&#26174;&#33879;&#20559;&#22909;&#36716;&#21464;&#65311;
&lt;/p&gt;
&lt;p&gt;
When Can We Track Significant Preference Shifts in Dueling Bandits?. (arXiv:2302.06595v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06595
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#20998;&#24067;&#36716;&#21464;&#30340;&#20915;&#26007;&#23545;&#25239;&#38382;&#39064;&#65292;&#24182;&#25506;&#35752;&#20102;&#35774;&#35745;&#33258;&#36866;&#24212;&#31639;&#27861;&#20197;&#35299;&#20915;&#21160;&#24577;&#36951;&#25022;&#30340;&#38382;&#39064;&#65292;&#32467;&#26524;&#21457;&#29616;&#21462;&#20915;&#20110;&#24213;&#23618;&#20559;&#22909;&#20998;&#24067;&#30340;&#23646;&#24615;&#12290;&#36798;&#21040;$O(\sqrt{K\tilde{L}T})$&#30340;&#21160;&#24577;&#36951;&#25022;&#26159;&#19981;&#21487;&#33021;&#30340;&#65307;&#23545;&#20110;$\text{SST} \cap \text{STI}$&#24773;&#20917;&#65292;&#23384;&#22312;&#19968;&#31181;&#31639;&#27861;&#23454;&#29616;&#21160;&#24577;&#36951;&#25022;&#20026;$O(\sqrt{K\tilde{L}T})$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20449;&#24687;&#26816;&#32034;&#12289;&#25512;&#33616;&#31995;&#32479;&#31561;&#39046;&#22495;&#24212;&#29992;&#24191;&#27867;&#30340;$K$&#33218;&#20915;&#26007;&#23545;&#25239;&#38382;&#39064;&#20013;&#65292;&#21453;&#39304;&#20197;&#26377;&#22122;&#22768;&#30340;&#25104;&#23545;&#20559;&#22909;&#24418;&#24335;&#32473;&#20986;&#65292;&#22240;&#27492;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#12290;&#32771;&#34385;&#21040;&#29992;&#25143;&#30340;&#20559;&#22909;/&#21475;&#21619;&#21487;&#33021;&#38543;&#26102;&#38388;&#28436;&#21464;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#20998;&#24067;&#36716;&#21464;&#30340;&#20915;&#26007;&#23545;&#25239;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26368;&#36817;&#25552;&#20986;&#30340;&#26174;&#33879;&#36716;&#21464;&#27010;&#24565;&#65288;Suk&#21644;Kpotufe&#65292;2022&#65289;&#65292;&#24182;&#25552;&#20986;&#26159;&#21542;&#21487;&#20197;&#35774;&#35745;&#19968;&#31181;&#33258;&#36866;&#24212;&#31639;&#27861;&#26469;&#35299;&#20915;&#20855;&#26377;$O(\sqrt{K\tilde{L}T})$&#21160;&#24577;&#36951;&#25022;&#65288;regret&#65289;&#30340;&#20915;&#26007;&#38382;&#39064;&#65292;&#20854;&#20013;$\tilde{L}$&#26159;&#20559;&#22909;&#20013;&#26174;&#33879;&#36716;&#21464;&#30340;&#65288;&#26410;&#30693;&#65289;&#25968;&#37327;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36825;&#20010;&#38382;&#39064;&#30340;&#31572;&#26696;&#21462;&#20915;&#20110;&#24213;&#23618;&#20559;&#22909;&#20998;&#24067;&#30340;&#23646;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#19968;&#20010;&#19981;&#21487;&#33021;&#30340;&#32467;&#26524;&#65292;&#25490;&#38500;&#20102;&#22312;&#24191;&#21463;&#30740;&#31350;&#30340;Condorcet&#21644;SST&#20559;&#22909;&#20998;&#24067;&#31867;&#19979;&#20855;&#26377;$O(\sqrt{K\tilde{L}T})$&#21160;&#24577;&#36951;&#25022;&#30340;&#20219;&#20309;&#31639;&#27861;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#34920;&#26126;$\text{SST} \cap \text{STI}$&#26159;&#22823;&#35268;&#27169;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
The $K$-armed dueling bandits problem, where the feedback is in the form of noisy pairwise preferences, has been widely studied due its applications in information retrieval, recommendation systems, etc. Motivated by concerns that user preferences/tastes can evolve over time, we consider the problem of dueling bandits with distribution shifts. Specifically, we study the recent notion of significant shifts (Suk and Kpotufe, 2022), and ask whether one can design an adaptive algorithm for the dueling problem with $O(\sqrt{K\tilde{L}T})$ dynamic regret, where $\tilde{L}$ is the (unknown) number of significant shifts in preferences. We show that the answer to this question depends on the properties of underlying preference distributions.  Firstly, we give an impossibility result that rules out any algorithm with $O(\sqrt{K\tilde{L}T})$ dynamic regret under the well-studied Condorcet and SST classes of preference distributions. Secondly, we show that $\text{SST} \cap \text{STI}$ is the large
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24191;&#20041;&#34920;&#38754;&#25439;&#22833;&#20989;&#25968;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#38477;&#20302;Hausdorff&#36317;&#31163;&#12290;&#35813;&#26041;&#27861;&#32771;&#34385;&#20102;&#31867;&#21035;&#19981;&#24179;&#34913;&#65292;&#24182;&#20855;&#26377;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#25968;&#20540;&#29305;&#24615;&#12290;&#35813;&#26041;&#27861;&#23545;&#20110;&#32959;&#30244;&#20998;&#21106;&#31561;&#24212;&#29992;&#38750;&#24120;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2302.03868</link><description>&lt;p&gt;
&#38477;&#20302;&#21307;&#23398;&#25104;&#20687;&#20998;&#21106;&#20013;Hausdorff&#36317;&#31163;&#30340;&#24191;&#20041;&#34920;&#38754;&#25439;&#22833;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
A Generalized Surface Loss for Reducing the Hausdorff Distance in Medical Imaging Segmentation. (arXiv:2302.03868v3 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03868
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24191;&#20041;&#34920;&#38754;&#25439;&#22833;&#20989;&#25968;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#38477;&#20302;Hausdorff&#36317;&#31163;&#12290;&#35813;&#26041;&#27861;&#32771;&#34385;&#20102;&#31867;&#21035;&#19981;&#24179;&#34913;&#65292;&#24182;&#20855;&#26377;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#25968;&#20540;&#29305;&#24615;&#12290;&#35813;&#26041;&#27861;&#23545;&#20110;&#32959;&#30244;&#20998;&#21106;&#31561;&#24212;&#29992;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#23398;&#25104;&#20687;&#20998;&#21106;&#20013;&#65292;Dice&#31995;&#25968;&#21644;&#22522;&#20110;Hausdorff&#30340;&#24230;&#37327;&#26159;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25104;&#21151;&#30340;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#29616;&#20195;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#25439;&#22833;&#20989;&#25968;&#36890;&#24120;&#21482;&#32771;&#34385;Dice&#31995;&#25968;&#25110;&#31867;&#20284;&#30340;&#22522;&#20110;&#21306;&#22495;&#30340;&#24230;&#37327;&#12290;&#22240;&#27492;&#65292;&#20351;&#29992;&#36825;&#20123;&#25439;&#22833;&#20989;&#25968;&#35757;&#32451;&#30340;&#20998;&#21106;&#32467;&#26500;&#22312;Dice&#31995;&#25968;&#19978;&#21487;&#33021;&#20855;&#26377;&#39640;&#20934;&#30830;&#24230;&#65292;&#20294;&#22312;&#22522;&#20110;Hausdorff&#30340;&#24230;&#37327;&#19978;&#20934;&#30830;&#24230;&#20302;&#12290;&#22522;&#20110;Hausdorff&#30340;&#24230;&#37327;&#20934;&#30830;&#24230;&#20302;&#21487;&#33021;&#20250;&#23545;&#32959;&#30244;&#20998;&#21106;&#31561;&#24212;&#29992;&#36896;&#25104;&#38382;&#39064;&#65292;&#22240;&#20026;&#36825;&#20123;&#24230;&#37327;&#20540;&#33267;&#20851;&#37325;&#35201;&#12290;&#20363;&#22914;&#65292;&#39640;Dice&#35780;&#20998;&#20276;&#38543;&#30528;&#26174;&#33879;&#30340;Hausdorff&#38169;&#35823;&#21487;&#33021;&#24847;&#21619;&#30528;&#39044;&#27979;&#26410;&#33021;&#26816;&#27979;&#21040;&#23567;&#30340;&#32959;&#30244;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#24191;&#20041;&#34920;&#38754;&#25439;&#22833;&#20989;&#25968;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#26368;&#23567;&#21270;&#22522;&#20110;Hausdorff&#30340;&#24230;&#37327;&#65292;&#24182;&#20855;&#26377;&#27604;&#24403;&#21069;&#26041;&#27861;&#26356;&#29702;&#24819;&#30340;&#25968;&#20540;&#29305;&#24615;&#21644;&#29992;&#20110;&#22788;&#29702;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#21152;&#26435;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;
Within medical imaging segmentation, the Dice coefficient and Hausdorff-based metrics are standard measures of success for deep learning models. However, modern loss functions for medical image segmentation often only consider the Dice coefficient or similar region-based metrics during training. As a result, segmentation architectures trained over such loss functions run the risk of achieving high accuracy for the Dice coefficient but low accuracy for Hausdorff-based metrics. Low accuracy on Hausdorff-based metrics can be problematic for applications such as tumor segmentation, where such benchmarks are crucial. For example, high Dice scores accompanied by significant Hausdorff errors could indicate that the predictions fail to detect small tumors. We propose the Generalized Surface Loss function, a novel loss function to minimize Hausdorff-based metrics with more desirable numerical properties than current methods and with weighting terms for class imbalance. Our loss function outperf
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#20110;&#31163;&#25955;&#24207;&#21015;&#20998;&#31867;&#22120;&#30340;&#38543;&#26426;&#21024;&#38500;&#65288;RS-Del&#65289;&#24179;&#28369;&#26426;&#21046;&#65292;&#25552;&#20379;&#38024;&#23545;&#32534;&#36753;&#36317;&#31163;&#21463;&#38480;&#23545;&#25239;&#24615;&#30340;&#40065;&#26834;&#24615;&#35777;&#26126;&#12290;</title><link>http://arxiv.org/abs/2302.01757</link><description>&lt;p&gt;
RS-Del: &#38543;&#26426;&#21024;&#38500;&#23545;&#24207;&#21015;&#20998;&#31867;&#22120;&#30340;&#32534;&#36753;&#36317;&#31163;&#40065;&#26834;&#24615;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
RS-Del: Edit Distance Robustness Certificates for Sequence Classifiers via Randomized Deletion. (arXiv:2302.01757v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01757
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#20110;&#31163;&#25955;&#24207;&#21015;&#20998;&#31867;&#22120;&#30340;&#38543;&#26426;&#21024;&#38500;&#65288;RS-Del&#65289;&#24179;&#28369;&#26426;&#21046;&#65292;&#25552;&#20379;&#38024;&#23545;&#32534;&#36753;&#36317;&#31163;&#21463;&#38480;&#23545;&#25239;&#24615;&#30340;&#40065;&#26834;&#24615;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#24179;&#28369;&#26159;&#26500;&#24314;&#20855;&#26377;&#35748;&#35777;&#40065;&#26834;&#24615;&#30340;&#20998;&#31867;&#22120;&#30340;&#20027;&#35201;&#26041;&#27861;&#12290;&#29616;&#26377;&#30340;&#38543;&#26426;&#24179;&#28369;&#26041;&#27861;&#20027;&#35201;&#38024;&#23545;&#20855;&#26377;&#36830;&#32493;&#36755;&#20837;&#65288;&#22914;&#22270;&#20687;&#65289;&#30340;&#20998;&#31867;&#22120;&#65292;&#20854;&#20013;&#24120;&#24120;&#30740;&#31350;$\ell_p$&#33539;&#25968;&#21463;&#38480;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20855;&#26377;&#31163;&#25955;&#25110;&#21487;&#21464;&#22823;&#23567;&#36755;&#20837;&#65288;&#20363;&#22914;&#28304;&#20195;&#30721;&#65289;&#30340;&#20998;&#31867;&#22120;&#30340;&#30740;&#31350;&#36739;&#23569;&#65292;&#36825;&#20123;&#20998;&#31867;&#22120;&#38656;&#35201;&#19981;&#21516;&#30340;&#23041;&#32961;&#27169;&#22411;&#21644;&#24179;&#28369;&#26426;&#21046;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20462;&#25913;&#20102;&#38543;&#26426;&#24179;&#28369;&#26041;&#27861;&#65292;&#20197;&#36866;&#29992;&#20110;&#31163;&#25955;&#24207;&#21015;&#20998;&#31867;&#22120;&#65292;&#20197;&#25552;&#20379;&#38024;&#23545;&#32534;&#36753;&#36317;&#31163;&#21463;&#38480;&#30340;&#23545;&#25239;&#24615;&#30340;&#21487;&#35777;&#26126;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#24179;&#28369;&#26426;&#21046;&#38543;&#26426;&#21024;&#38500;&#65288;RS-Del&#65289;&#24212;&#29992;&#20102;&#38543;&#26426;&#21024;&#38500;&#32534;&#36753;&#65292;&#36825;&#31181;&#26041;&#24335;&#65288;&#20063;&#35768;&#20196;&#20154;&#24778;&#35766;&#22320;&#65289;&#36275;&#20197;&#25552;&#20379;&#38024;&#23545;&#23545;&#25239;&#24615;&#21024;&#38500;&#12289;&#25554;&#20837;&#21644;&#26367;&#25442;&#32534;&#36753;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#35748;&#35777;&#35777;&#26126;&#19981;&#21516;&#20110;&#20256;&#32479;&#30340;Neyman-Pearson&#26041;&#27861;&#65292;&#22312;&#25105;&#20204;&#30340;&#24773;&#20917;&#19979;&#26080;&#27861;&#35745;&#31639;&#65292;&#32780;&#26159;&#22260;&#32469;&#30528;&#21478;&#19968;&#31181;&#26041;&#24335;&#36827;&#34892;&#32452;&#32455;&#12290;
&lt;/p&gt;
&lt;p&gt;
Randomized smoothing is a leading approach for constructing classifiers that are certifiably robust against adversarial examples. Existing work on randomized smoothing has focused on classifiers with continuous inputs, such as images, where $\ell_p$-norm bounded adversaries are commonly studied. However, there has been limited work for classifiers with discrete or variable-size inputs, such as for source code, which require different threat models and smoothing mechanisms. In this work, we adapt randomized smoothing for discrete sequence classifiers to provide certified robustness against edit distance-bounded adversaries. Our proposed smoothing mechanism randomized deletion (RS-Del) applies random deletion edits, which are (perhaps surprisingly) sufficient to confer robustness against adversarial deletion, insertion and substitution edits. Our proof of certification deviates from the established Neyman-Pearson approach, which is intractable in our setting, and is instead organized aro
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#38544;&#31169;&#20445;&#25252;&#35757;&#32451;&#21307;&#23398;&#24433;&#20687;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#65292;&#24182;&#19982;&#38750;&#38544;&#31169;&#35757;&#32451;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#30740;&#31350;&#32467;&#26524;&#21487;&#20026;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#30340;&#24191;&#27867;&#24212;&#29992;&#25552;&#20379;&#37325;&#35201;&#21442;&#32771;&#12290;</title><link>http://arxiv.org/abs/2302.01622</link><description>&lt;p&gt;
&#31169;&#23494;&#12289;&#20844;&#24179;&#19988;&#31934;&#30830;&#65306;&#22312;&#21307;&#23398;&#24433;&#20687;&#20013;&#35757;&#32451;&#22823;&#35268;&#27169;&#38544;&#31169;&#20445;&#25252;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Private, fair and accurate: Training large-scale, privacy-preserving AI models in medical imaging. (arXiv:2302.01622v3 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01622
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#38544;&#31169;&#20445;&#25252;&#35757;&#32451;&#21307;&#23398;&#24433;&#20687;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#65292;&#24182;&#19982;&#38750;&#38544;&#31169;&#35757;&#32451;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#30740;&#31350;&#32467;&#26524;&#21487;&#20026;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#30340;&#24191;&#27867;&#24212;&#29992;&#25552;&#20379;&#37325;&#35201;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#22312;&#21307;&#23398;&#39046;&#22495;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#22810;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21307;&#23398;&#25968;&#25454;&#30340;&#39640;&#24230;&#25935;&#24863;&#24615;&#65292;&#38656;&#35201;&#37319;&#21462;&#29305;&#27530;&#25514;&#26045;&#30830;&#20445;&#20854;&#20445;&#25252;&#12290;&#20445;&#25252;&#38544;&#31169;&#30340;&#40644;&#37329;&#26631;&#20934;&#26159;&#24341;&#20837;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#26469;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;DP&#23545;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#26377;&#36127;&#38754;&#24433;&#21709;&#65292;&#36825;&#22312;&#21307;&#23398;&#20013;&#26159;&#19981;&#21487;&#25509;&#21463;&#30340;&#65292;&#24182;&#19988;&#26159;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#24191;&#27867;&#24212;&#29992;&#30340;&#20027;&#35201;&#38556;&#30861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#38544;&#31169;&#20445;&#25252;&#35757;&#32451;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#23545;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#65292;&#19982;&#38750;&#38544;&#31169;&#35757;&#32451;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#20004;&#20010;&#25968;&#25454;&#38598;&#65306;&#65288;1&#65289;&#19968;&#20010;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65288;N=193,311&#65289;&#30340;&#39640;&#36136;&#37327;&#20020;&#24202;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#65292;&#21644;&#65288;2&#65289;&#19968;&#20010;&#25968;&#25454;&#38598;&#65288;N=1,625&#65289;&#30340;3D&#33145;&#37096;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#22270;&#20687;&#65292;&#29992;&#20110;&#20998;&#31867;&#33008;&#33146;&#23548;&#31649;&#33146;&#30284;&#65288;PDAC&#65289;&#30340;&#23384;&#22312;&#12290;&#20004;&#20010;&#25968;&#25454;&#38598;&#22343;&#20026;&#22238;&#39038;&#24615;&#37319;&#38598;&#65292;&#24182;&#30001;&#32463;&#39564;&#20016;&#23500;&#30340;&#21307;&#23398;&#24433;&#20687;&#19987;&#23478;&#36827;&#34892;&#25163;&#21160;&#26631;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) models are increasingly used in the medical domain. However, as medical data is highly sensitive, special precautions to ensure its protection are required. The gold standard for privacy preservation is the introduction of differential privacy (DP) to model training. Prior work indicates that DP has negative implications on model accuracy and fairness, which are unacceptable in medicine and represent a main barrier to the widespread use of privacy-preserving techniques. In this work, we evaluated the effect of privacy-preserving training of AI models regarding accuracy and fairness compared to non-private training. For this, we used two datasets: (1) A large dataset (N=193,311) of high quality clinical chest radiographs, and (2) a dataset (N=1,625) of 3D abdominal computed tomography (CT) images, with the task of classifying the presence of pancreatic ductal adenocarcinoma (PDAC). Both were retrospectively collected and manually labeled by experienced radio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#27169;&#22411;&#26469;&#39044;&#27979;&#20056;&#23458;&#30340;&#36215;&#28857;-&#32456;&#28857;&#65288;OD&#65289;&#27969;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#21508;&#31181;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#25429;&#25417;&#37325;&#22797;&#27169;&#24335;&#21644;&#19978;&#19979;&#25991;&#25968;&#25454;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#20223;&#30495;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#27169;&#22411;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2301.02515</link><description>&lt;p&gt;
&#22522;&#20110;GNN&#30340;&#20056;&#23458;&#35831;&#27714;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
GNN-based Passenger Request Prediction. (arXiv:2301.02515v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.02515
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#27169;&#22411;&#26469;&#39044;&#27979;&#20056;&#23458;&#30340;&#36215;&#28857;-&#32456;&#28857;&#65288;OD&#65289;&#27969;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#21508;&#31181;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#25429;&#25417;&#37325;&#22797;&#27169;&#24335;&#21644;&#19978;&#19979;&#25991;&#25968;&#25454;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#20223;&#30495;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#27169;&#22411;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20056;&#23458;&#35831;&#27714;&#39044;&#27979;&#23545;&#20110;&#20056;&#36710;&#20849;&#20139;&#24179;&#21488;&#30340;&#36816;&#33829;&#35268;&#21010;&#12289;&#25511;&#21046;&#21644;&#31649;&#29702;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#38656;&#27714;&#39044;&#27979;&#38382;&#39064;&#24050;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#20056;&#23458;&#30340;&#36215;&#28857;-&#32456;&#28857;&#65288;OD&#65289;&#27969;&#39044;&#27979;&#21364;&#22312;&#30740;&#31350;&#30028;&#20013;&#21463;&#21040;&#36739;&#23569;&#20851;&#27880;&#12290;&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#27169;&#22411;&#26469;&#39044;&#27979;&#20056;&#23458;&#30340;OD&#27969;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#19981;&#21516;&#20301;&#32622;&#36215;&#22987;&#30340;&#35831;&#27714;&#20043;&#38388;&#20135;&#29983;&#30340;&#21508;&#31181;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#25429;&#25417;&#20102;&#35813;&#22320;&#28857;&#30340;&#37325;&#22797;&#27169;&#24335;&#21644;&#19978;&#19979;&#25991;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#30830;&#23450;&#20102;&#35206;&#30422;&#36947;&#36335;&#32593;&#32476;&#24182;&#20445;&#25345;&#27169;&#22411;&#22797;&#26434;&#24615;&#21644;&#20934;&#30830;&#24615;&#30340;&#32593;&#26684;&#21333;&#20803;&#30340;&#26368;&#20339;&#22823;&#23567;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#20223;&#30495;&#26469;&#26816;&#26597;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21450;&#20854;&#21508;&#20010;&#32452;&#20214;&#30340;&#29305;&#24449;&#12290;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#30456;&#27604;&#29616;&#26377;&#22522;&#32447;&#20855;&#26377;&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Passenger request prediction is essential for operations planning, control, and management in ride-sharing platforms. While the demand prediction problem has been studied extensively, the Origin-Destination (OD) flow prediction of passengers has received less attention from the research community. This paper develops a Graph Neural Network framework along with the Attention Mechanism to predict the OD flow of passengers. The proposed framework exploits various linear and non-linear dependencies that arise among requests originating from different locations and captures the repetition pattern and the contextual data of that place. Moreover, the optimal size of the grid cell that covers the road network and preserves the complexity and accuracy of the model is determined. Extensive simulations are conducted to examine the characteristics of our proposed approach and its various components. The results show the superior performance of our proposed model compared to the existing baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#23481;&#22120;&#20013;&#23384;&#22312;&#30340;&#33219;&#32959;&#38382;&#39064;&#65292;&#24182;&#24320;&#21457;&#20102;MMLB&#26694;&#26550;&#36827;&#34892;&#20998;&#26512;&#21644;&#37327;&#21270;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#33219;&#32959;&#20250;&#21344;&#25454;&#23481;&#22120;&#24635;&#22823;&#23567;&#30340;80&#65285;&#65292; &#26174;&#33879;&#22686;&#21152;&#20102;&#23481;&#22120;&#30340;&#20379;&#24212;&#26102;&#38388;&#65292;&#26368;&#22810;&#22686;&#21152;&#20102;370&#65285;&#65292;&#19988;&#23548;&#33268;&#28431;&#27934;&#24694;&#21270;&#26368;&#39640;&#36798;99&#65285;&#12290;</title><link>http://arxiv.org/abs/2212.09437</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#33219;&#32959;&#19988;&#23384;&#22312;&#28431;&#27934;
&lt;/p&gt;
&lt;p&gt;
Machine Learning Systems are Bloated and Vulnerable. (arXiv:2212.09437v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09437
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#23481;&#22120;&#20013;&#23384;&#22312;&#30340;&#33219;&#32959;&#38382;&#39064;&#65292;&#24182;&#24320;&#21457;&#20102;MMLB&#26694;&#26550;&#36827;&#34892;&#20998;&#26512;&#21644;&#37327;&#21270;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#33219;&#32959;&#20250;&#21344;&#25454;&#23481;&#22120;&#24635;&#22823;&#23567;&#30340;80&#65285;&#65292; &#26174;&#33879;&#22686;&#21152;&#20102;&#23481;&#22120;&#30340;&#20379;&#24212;&#26102;&#38388;&#65292;&#26368;&#22810;&#22686;&#21152;&#20102;370&#65285;&#65292;&#19988;&#23548;&#33268;&#28431;&#27934;&#24694;&#21270;&#26368;&#39640;&#36798;99&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20170;&#30340;&#36719;&#20214;&#20195;&#30721;&#21644;&#21151;&#33021;&#20887;&#20313;&#65292;&#24182;&#22823;&#22810;&#25968;&#29992;&#25143;&#24182;&#19981;&#20351;&#29992;&#12290;&#36825;&#31181;&#33219;&#32959;&#21344;&#25454;&#25972;&#20010;&#36719;&#20214;&#26632;&#65292;&#20174;&#25805;&#20316;&#31995;&#32479;&#21040;&#21518;&#31471;&#12289;&#21069;&#31471;&#21644;&#32593;&#39029;&#37117;&#26222;&#36941;&#23384;&#22312;&#12290;&#26412;&#25991;&#37325;&#28857;&#20998;&#26512;&#21644;&#37327;&#21270;&#26426;&#22120;&#23398;&#20064;&#23481;&#22120;&#20013;&#30340;&#33219;&#32959;&#38382;&#39064;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;MMLB&#26694;&#26550;&#26469;&#20998;&#26512;&#26426;&#22120;&#23398;&#20064;&#23481;&#22120;&#30340;&#33219;&#32959;&#65292;&#27979;&#37327;&#23481;&#22120;&#21644;&#36719;&#20214;&#21253;&#30340;&#33219;&#32959;&#31243;&#24230;&#12290;&#35813;&#24037;&#20855;&#37327;&#21270;&#20102;&#33219;&#32959;&#30340;&#26469;&#28304;&#65292;&#24182;&#19982;&#28431;&#27934;&#20998;&#26512;&#24037;&#20855;&#38598;&#25104;&#65292;&#20197;&#35780;&#20272;&#33219;&#32959;&#23545;&#23481;&#22120;&#28431;&#27934;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#23545;Tensorflow&#12289;Pytorch&#21644;NVIDIA&#30340;15&#20010;&#26426;&#22120;&#23398;&#20064;&#23481;&#22120;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#33219;&#32959;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21344;&#23481;&#22120;&#24635;&#22823;&#23567;&#30340;80%&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#33219;&#32959;&#26174;&#33879;&#22686;&#21152;&#20102;&#23481;&#22120;&#30340;&#20379;&#24212;&#26102;&#38388;&#65292;&#26368;&#22810;&#22686;&#21152;&#20102;370&#65285;&#65292;&#24182;&#23548;&#33268;&#28431;&#27934;&#24694;&#21270;&#65292;&#26368;&#39640;&#36798;99&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Today's software is bloated with both code and features that are not used by most users. This bloat is prevalent across the entire software stack, from the operating system, all the way to software backends, frontends, and web-pages. In this paper, we focus on analyzing and quantifying bloat in machine learning containers. We develop MMLB, a framework to analyze bloat in machine learning containers, measuring the amount of bloat that exists on the container and package levels. Our tool quantifies the sources of bloat and integrates with vulnerability analysis tools to evaluate the impact of bloat on container vulnerabilities. Through experimentation with 15 machine learning containers from Tensorflow, Pytorch, and NVIDIA, we show that bloat is a significant issue, accounting for up to 80% of the container size in some cases. Our results demonstrate that bloat significantly increases the container provisioning time by up to 370% and exacerbates vulnerabilities by up to 99%.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19978;&#19979;&#25991;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#20013;&#30340;&#36716;&#31227;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#26469;&#26368;&#23567;&#21270;&#36951;&#25022;&#65292;&#24182;&#37327;&#21270;&#20102;&#28304;&#39046;&#22495;&#25968;&#25454;&#23545;&#30446;&#26631;&#39046;&#22495;&#23398;&#20064;&#30340;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2211.12612</link><description>&lt;p&gt;
&#36716;&#31227;&#23398;&#20064;&#29992;&#20110;&#19978;&#19979;&#25991;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Transfer Learning for Contextual Multi-armed Bandits. (arXiv:2211.12612v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.12612
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19978;&#19979;&#25991;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#20013;&#30340;&#36716;&#31227;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#26469;&#26368;&#23567;&#21270;&#36951;&#25022;&#65292;&#24182;&#37327;&#21270;&#20102;&#28304;&#39046;&#22495;&#25968;&#25454;&#23545;&#30446;&#26631;&#39046;&#22495;&#23398;&#20064;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38754;&#23545;&#19968;&#31995;&#21015;&#24212;&#29992;&#30340;&#39537;&#21160;&#19979;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#21327;&#21464;&#37327;&#36716;&#31227;&#27169;&#22411;&#19979;&#38750;&#21442;&#25968;&#19978;&#19979;&#25991;&#22810;&#33218;&#36172;&#21338;&#26426;&#30340;&#36716;&#31227;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#22312;&#30446;&#26631;&#36172;&#21338;&#26426;&#24320;&#22987;&#23398;&#20064;&#20043;&#21069;&#65292;&#25105;&#20204;&#24050;&#32463;&#25910;&#38598;&#21040;&#20102;&#28304;&#36172;&#21338;&#26426;&#30340;&#25968;&#25454;&#12290;&#26412;&#25991;&#24314;&#31435;&#20102;&#32047;&#31215;&#36951;&#25022;&#30340;&#26368;&#23567;&#21270;&#36895;&#29575;&#25910;&#25947;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36716;&#31227;&#23398;&#20064;&#31639;&#27861;&#65292;&#23427;&#36798;&#21040;&#20102;&#26368;&#23567;&#21270;&#36951;&#25022;&#30340;&#26497;&#38480;&#12290;&#32467;&#26524;&#37327;&#21270;&#20102;&#28304;&#39046;&#22495;&#25968;&#25454;&#22312;&#19978;&#19979;&#25991;&#38750;&#21442;&#25968;&#22810;&#33218;&#36172;&#21338;&#26426;&#23398;&#20064;&#20013;&#23545;&#30446;&#26631;&#39046;&#22495;&#23398;&#20064;&#30340;&#36129;&#29486;&#12290;&#37492;&#20110;&#23545;&#26410;&#30693;&#24179;&#28369;&#24615;&#30340;&#19968;&#33324;&#19981;&#21487;&#33021;&#24615;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#31639;&#27861;&#65292;&#23427;&#22312;&#39069;&#22806;&#30340;&#33258;&#30456;&#20284;&#24615;&#20551;&#35774;&#19979;&#22312;&#22823;&#37327;&#21442;&#25968;&#31354;&#38388;&#20013;&#33258;&#21160;&#36866;&#24212;&#26410;&#30693;&#21442;&#25968;&#65292;&#24182;&#23454;&#29616;&#20102;&#25509;&#36817;&#26368;&#20248;&#30340;&#32479;&#35745;&#20445;&#35777;&#65288;&#38500;&#20197;&#23545;&#25968;&#22240;&#23376;&#65289;&#12290;&#36890;&#36807;&#27169;&#25311;&#23454;&#39564;&#35828;&#26126;&#20102;&#36716;&#31227;&#23398;&#20064;&#22312;&#19978;&#19979;&#25991;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#20013;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by a range of applications, we study in this paper the problem of transfer learning for nonparametric contextual multi-armed bandits under the covariate shift model, where we have data collected on source bandits before the start of the target bandit learning. The minimax rate of convergence for the cumulative regret is established and a novel transfer learning algorithm that attains the minimax regret is proposed. The results quantify the contribution of the data from the source domains for learning in the target domain in the context of nonparametric contextual multi-armed bandits.  In view of the general impossibility of adaptation to unknown smoothness, we develop a data-driven algorithm that achieves near-optimal statistical guarantees (up to a logarithmic factor) while automatically adapting to the unknown parameters over a large collection of parameter spaces under an additional self-similarity assumption. A simulation study is carried out to illustrate the benefits of
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;HyperSound&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#36229;&#32593;&#32476;&#29983;&#25104;&#38899;&#39057;&#20449;&#21495;&#30340;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#12290;&#19982;&#20854;&#20182;&#27169;&#22411;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#37325;&#26500;&#22768;&#27874;&#26041;&#38754;&#20855;&#26377;&#21487;&#27604;&#36739;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2211.01839</link><description>&lt;p&gt;
HyperSound&#65306;&#21033;&#29992;&#36229;&#32593;&#32476;&#29983;&#25104;&#38899;&#39057;&#20449;&#21495;&#30340;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
HyperSound: Generating Implicit Neural Representations of Audio Signals with Hypernetworks. (arXiv:2211.01839v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01839
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;HyperSound&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#36229;&#32593;&#32476;&#29983;&#25104;&#38899;&#39057;&#20449;&#21495;&#30340;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#12290;&#19982;&#20854;&#20182;&#27169;&#22411;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#37325;&#26500;&#22768;&#27874;&#26041;&#38754;&#20855;&#26377;&#21487;&#27604;&#36739;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#65288;INRs&#65289;&#26159;&#19968;&#20010;&#24555;&#36895;&#21457;&#23637;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#25552;&#20379;&#20102;&#22810;&#23186;&#20307;&#20449;&#21495;&#30340;&#26367;&#20195;&#34920;&#31034;&#26041;&#24335;&#12290;&#26368;&#36817;&#65292;INRs&#30340;&#24212;&#29992;&#21253;&#25324;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#12289;&#39640;&#32500;&#20449;&#21495;&#21387;&#32553;&#25110;3D&#28210;&#26579;&#31561;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#36890;&#24120;&#38598;&#20013;&#22312;&#35270;&#35273;&#25968;&#25454;&#19978;&#65292;&#23558;&#23427;&#20204;&#36866;&#24212;&#21040;&#38899;&#39057;&#39046;&#22495;&#24182;&#19981;&#23481;&#26131;&#12290;&#27492;&#22806;&#65292;&#36825;&#38656;&#35201;&#20026;&#27599;&#20010;&#25968;&#25454;&#26679;&#26412;&#21333;&#29420;&#35757;&#32451;&#27169;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HyperSound&#65292;&#19968;&#31181;&#21033;&#29992;&#36229;&#32593;&#32476;&#23454;&#29616;&#22312;&#35757;&#32451;&#26102;&#26410;&#35265;&#36807;&#30340;&#38899;&#39057;&#20449;&#21495;&#30340;INRs&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#37325;&#26500;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#27169;&#22411;&#30456;&#23218;&#32654;&#30340;&#22768;&#27874;&#12290;
&lt;/p&gt;
&lt;p&gt;
Implicit neural representations (INRs) are a rapidly growing research field, which provides alternative ways to represent multimedia signals. Recent applications of INRs include image super-resolution, compression of high-dimensional signals, or 3D rendering. However, these solutions usually focus on visual data, and adapting them to the audio domain is not trivial. Moreover, it requires a separately trained model for every data sample. To address this limitation, we propose HyperSound, a meta-learning method leveraging hypernetworks to produce INRs for audio signals unseen at training time. We show that our approach can reconstruct sound waves with quality comparable to other state-of-the-art models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#32593;&#32476;&#35266;&#27979;&#25968;&#25454;&#20013;&#20010;&#20307;&#27835;&#30103;&#25928;&#24212;&#30340;&#20272;&#35745;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#21152;&#26435;&#22238;&#24402;&#65288;DWR&#65289;&#31639;&#27861;&#65292;&#36890;&#36807;&#21516;&#26102;&#23398;&#20064;&#27880;&#24847;&#26435;&#37325;&#26469;&#35299;&#20915;&#32593;&#32476;&#24178;&#25200;&#21644;&#24322;&#36136;&#24178;&#39044;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2210.14080</link><description>&lt;p&gt;
&#22312;&#32593;&#32476;&#20013;&#23398;&#20064;&#24322;&#36136;&#24178;&#39044;&#19979;&#30340;&#20010;&#20307;&#27835;&#30103;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
Learning Individual Treatment Effects under Heterogeneous Interference in Networks. (arXiv:2210.14080v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14080
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#32593;&#32476;&#35266;&#27979;&#25968;&#25454;&#20013;&#20010;&#20307;&#27835;&#30103;&#25928;&#24212;&#30340;&#20272;&#35745;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#21152;&#26435;&#22238;&#24402;&#65288;DWR&#65289;&#31639;&#27861;&#65292;&#36890;&#36807;&#21516;&#26102;&#23398;&#20064;&#27880;&#24847;&#26435;&#37325;&#26469;&#35299;&#20915;&#32593;&#32476;&#24178;&#25200;&#21644;&#24322;&#36136;&#24178;&#39044;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20174;&#32593;&#32476;&#35266;&#27979;&#25968;&#25454;&#20013;&#20272;&#35745;&#20010;&#20307;&#27835;&#30103;&#25928;&#24212;&#24341;&#36215;&#20102;&#20154;&#20204;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#22312;&#32593;&#32476;&#22330;&#26223;&#20013;&#65292;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#31283;&#23450;&#21333;&#20803;&#27835;&#30103;&#20215;&#20540;&#20551;&#35774;&#65288;SUTVA&#65289;&#30340;&#36829;&#21453;&#65292;&#35813;&#20551;&#35774;&#35748;&#20026;&#19968;&#20010;&#21333;&#20301;&#30340;&#27835;&#30103;&#20998;&#37197;&#19981;&#20250;&#24433;&#21709;&#20854;&#20182;&#20154;&#30340;&#32467;&#26524;&#12290;&#22312;&#32593;&#32476;&#25968;&#25454;&#20013;&#65292;&#30001;&#20110;&#24178;&#25200;&#65292;&#19968;&#20010;&#21333;&#20301;&#30340;&#32467;&#26524;&#19981;&#20165;&#21463;&#21040;&#20854;&#27835;&#30103;&#30340;&#24433;&#21709;&#65288;&#21363;&#30452;&#25509;&#25928;&#24212;&#65289;&#65292;&#36824;&#21463;&#21040;&#20854;&#20182;&#20154;&#30340;&#27835;&#30103;&#24433;&#21709;&#65288;&#21363;&#28322;&#20986;&#25928;&#24212;&#65289;&#12290;&#27492;&#22806;&#65292;&#20854;&#20182;&#21333;&#20301;&#30340;&#24433;&#21709;&#22987;&#32456;&#26159;&#24322;&#36136;&#30340;&#65288;&#20363;&#22914;&#65292;&#19982;&#20852;&#36259;&#30456;&#20284;&#30340;&#26379;&#21451;&#23545;&#19968;&#20010;&#20154;&#30340;&#24433;&#21709;&#19982;&#20852;&#36259;&#19981;&#21516;&#30340;&#26379;&#21451;&#19981;&#21516;&#65289;&#12290;&#26412;&#25991;&#38024;&#23545;&#22312;&#24322;&#36136;&#24178;&#25200;&#19979;&#20272;&#35745;&#20010;&#20307;&#27835;&#30103;&#25928;&#24212;&#65288;&#21253;&#25324;&#30452;&#25509;&#25928;&#24212;&#21644;&#28322;&#20986;&#25928;&#24212;&#65289;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#21152;&#26435;&#22238;&#24402;&#65288;DWR&#65289;&#31639;&#27861;&#65292;&#36890;&#36807;&#21516;&#26102;&#23398;&#20064;&#25429;&#25417;&#24322;&#36136;&#24178;&#39044;&#30340;&#27880;&#24847;&#26435;&#37325;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimates of individual treatment effects from networked observational data are attracting increasing attention these days. One major challenge in network scenarios is the violation of the stable unit treatment value assumption (SUTVA), which assumes that the treatment assignment of a unit does not influence others' outcomes. In network data, due to interference, the outcome of a unit is influenced not only by its treatment (i.e., direct effects) but also by others' treatments (i.e., spillover effects). Furthermore, the influences from other units are always heterogeneous (e.g., friends with similar interests affect a person differently than friends with different interests). In this paper, we focus on the problem of estimating individual treatment effects (both direct and spillover effects) under heterogeneous interference. To address this issue, we propose a novel Dual Weighting Regression (DWR) algorithm by simultaneously learning attention weights that capture the heterogeneous int
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35777;&#26126;&#20102;&#20351;&#29992;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#39118;&#38505;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#30340;&#36951;&#25022;&#20445;&#35777;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#39062;&#31639;&#27861;&#65292;&#20854;&#36951;&#25022;&#19978;&#30028;&#19982;&#20808;&#21069;&#26041;&#27861;&#30456;&#21305;&#37197;&#12290;</title><link>http://arxiv.org/abs/2210.14051</link><description>&lt;p&gt;
&#35777;&#26126;&#20102;&#20998;&#24067;&#24335;&#39118;&#38505;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#19982;&#39118;&#38505;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#20043;&#38388;&#30340;&#21487;&#35777;&#26126;&#36951;&#25022;&#19978;&#30028;
&lt;/p&gt;
&lt;p&gt;
Bridging Distributional and Risk-sensitive Reinforcement Learning with Provable Regret Bounds. (arXiv:2210.14051v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14051
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35777;&#26126;&#20102;&#20351;&#29992;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#39118;&#38505;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#30340;&#36951;&#25022;&#20445;&#35777;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#39062;&#31639;&#27861;&#65292;&#20854;&#36951;&#25022;&#19978;&#30028;&#19982;&#20808;&#21069;&#26041;&#27861;&#30456;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#23545;&#39118;&#38505;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#65288;RSRL&#65289;&#30340;&#36951;&#25022;&#20445;&#35777;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#30446;&#26631;&#20026;&#22238;&#25253;&#30340;&#29109;&#39118;&#38505;&#27979;&#24230;&#65288;EntRM&#65289;&#30340;&#26377;&#38480;&#24773;&#33410;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#12290;&#36890;&#36807;&#21033;&#29992;EntRM&#30340;&#19968;&#20010;&#20851;&#38190;&#23646;&#24615;&#65292;&#29420;&#31435;&#24615;&#23646;&#24615;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#39118;&#38505;&#25935;&#24863;&#20998;&#24067;&#24335;&#21160;&#24577;&#35268;&#21010;&#26694;&#26550;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20004;&#31181;&#19981;&#21516;&#30340;&#26041;&#26696;&#23454;&#29616;&#20102;&#20048;&#35266;&#24615;&#65292;&#21253;&#25324;&#22522;&#20110;&#27169;&#22411;&#21644;&#26080;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20004;&#31181;&#31639;&#27861;&#37117;&#36798;&#21040;&#20102;$\tilde{\mathcal{O}}(\frac{\exp(|\beta| H)-1}{|\beta|}H\sqrt{S^2AK})$&#30340;&#36951;&#25022;&#19978;&#30028;&#65292;&#20854;&#20013;$S$&#65292;$A$&#65292;$K$&#21644;$H$&#20998;&#21035;&#34920;&#31034;&#29366;&#24577;&#30340;&#25968;&#37327;&#65292;&#21160;&#20316;&#30340;&#25968;&#37327;&#65292;&#24773;&#33410;&#30340;&#25968;&#37327;&#21644;&#26102;&#38388;&#30340;&#38271;&#24230;&#12290;&#36825;&#19982;\cite{fei2021exponential}&#20013;&#25552;&#20986;&#30340;RSVI2&#30456;&#19968;&#33268;&#65292;&#24182;&#36827;&#34892;&#20102;&#26032;&#39062;&#30340;&#20998;&#24067;&#24335;&#20998;&#26512;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20197;&#26679;&#26412;&#22797;&#26434;&#24230;&#26041;&#21521;&#23558;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#21644;&#39118;&#38505;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#32852;&#31995;&#36215;&#26469;&#30340;&#36951;&#25022;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the regret guarantee for risk-sensitive reinforcement learning (RSRL) via distributional reinforcement learning (DRL) methods. In particular, we consider finite episodic Markov decision processes whose objective is the entropic risk measure (EntRM) of return. By leveraging a key property of the EntRM, the independence property, we establish the risk-sensitive distributional dynamic programming framework. We then propose two novel DRL algorithms that implement optimism through two different schemes, including a model-free one and a model-based one.  We prove that they both attain $\tilde{\mathcal{O}}(\frac{\exp(|\beta| H)-1}{|\beta|}H\sqrt{S^2AK})$ regret upper bound, where $S$, $A$, $K$, and $H$ represent the number of states, actions, episodes, and the time horizon, respectively. It matches RSVI2 proposed in \cite{fei2021exponential}, with novel distributional analysis. To the best of our knowledge, this is the first regret analysis that bridges DRL and RSRL in terms of sampl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#21367;&#31215;&#22362;&#25345;&#36716;&#25442;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#19982;&#28388;&#27874;&#22120;&#36827;&#34892;&#21367;&#31215;&#35745;&#31639;&#26469;&#33719;&#21462;&#25299;&#25169;&#29305;&#24449;&#21270;&#12290;&#36890;&#36807;&#35777;&#26126;&#20854;&#25345;&#20037;&#22270;&#26159;&#19968;&#20010;&#27880;&#20837;&#19981;&#21464;&#37327;&#65292;&#35777;&#26126;&#20102;&#21367;&#31215;&#22362;&#25345;&#36716;&#25442;&#30340;&#29420;&#29305;&#24615;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#25913;&#21892;&#30340;&#31283;&#23450;&#24615;&#21644;&#26356;&#22823;&#30340;&#28789;&#27963;&#24615;&#12290;</title><link>http://arxiv.org/abs/2208.02107</link><description>&lt;p&gt;
&#21367;&#31215;&#22362;&#25345;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
Convolutional Persistence Transforms. (arXiv:2208.02107v2 [math.AT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.02107
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#21367;&#31215;&#22362;&#25345;&#36716;&#25442;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#19982;&#28388;&#27874;&#22120;&#36827;&#34892;&#21367;&#31215;&#35745;&#31639;&#26469;&#33719;&#21462;&#25299;&#25169;&#29305;&#24449;&#21270;&#12290;&#36890;&#36807;&#35777;&#26126;&#20854;&#25345;&#20037;&#22270;&#26159;&#19968;&#20010;&#27880;&#20837;&#19981;&#21464;&#37327;&#65292;&#35777;&#26126;&#20102;&#21367;&#31215;&#22362;&#25345;&#36716;&#25442;&#30340;&#29420;&#29305;&#24615;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#25913;&#21892;&#30340;&#31283;&#23450;&#24615;&#21644;&#26356;&#22823;&#30340;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#23450;&#20041;&#22312;&#31616;&#21333;&#22797;&#21512;&#20307;&#19978;&#30340;&#25968;&#25454;&#65288;&#22914;&#22270;&#20687;&#21644;&#26631;&#35760;&#22270;&#65289;&#30340;&#25299;&#25169;&#29305;&#24449;&#21270;&#65292;&#36890;&#36807;&#23558;&#36825;&#20123;&#25968;&#25454;&#19982;&#19981;&#21516;&#30340;&#28388;&#27874;&#22120;&#36827;&#34892;&#21367;&#31215;&#35745;&#31639;&#25345;&#32493;&#24615;&#12290;&#23558;&#21367;&#31215;&#28388;&#27874;&#22120;&#35270;&#20026;&#23616;&#37096;&#27169;&#24335;&#65292;&#25152;&#24471;&#21367;&#31215;&#30340;&#25345;&#20037;&#22270;&#25551;&#36848;&#20102;&#27169;&#24335;&#22312;&#31616;&#21333;&#22797;&#21512;&#20307;&#19978;&#30340;&#20998;&#24067;&#26041;&#24335;&#12290;&#36825;&#31181;&#27969;&#31243;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#21367;&#31215;&#22362;&#25345;&#36716;&#25442;&#65292;&#25193;&#23637;&#20102;&#25299;&#25169;&#23398;&#22312;&#27492;&#31867;&#25968;&#25454;&#20013;&#35266;&#23519;&#27169;&#24335;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#65288;&#19968;&#33324;&#24773;&#20917;&#19979;&#65289;&#65292;&#23545;&#20110;&#20219;&#24847;&#20004;&#20010;&#26631;&#35760;&#22797;&#21512;&#20307;&#65292;&#37117;&#21487;&#20197;&#25214;&#21040;&#26576;&#20010;&#28388;&#27874;&#22120;&#65292;&#20351;&#23427;&#20204;&#20135;&#29983;&#19981;&#21516;&#30340;&#25345;&#20037;&#22270;&#65292;&#20351;&#24471;&#25152;&#26377;&#21487;&#33021;&#30340;&#21367;&#31215;&#25345;&#20037;&#22270;&#30340;&#38598;&#21512;&#26159;&#19968;&#20010;&#27880;&#20837;&#19981;&#21464;&#37327;&#12290;&#36825;&#36890;&#36807;&#26174;&#31034;&#21367;&#31215;&#22362;&#25345;&#36716;&#25442;&#26159;&#21478;&#19968;&#20010;&#25299;&#25169;&#19981;&#21464;&#37327;&#8212;&#8212;&#25345;&#32493;&#21516;&#35843;&#21464;&#25442;&#30340;&#29305;&#27530;&#24773;&#20917;&#26469;&#35777;&#26126;&#12290;&#21367;&#31215;&#22362;&#25345;&#36716;&#25442;&#30340;&#20854;&#20182;&#20248;&#28857;&#26159;&#25913;&#21892;&#30340;&#31283;&#23450;&#24615;&#65292;&#26356;&#22823;&#30340;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we consider topological featurizations of data defined over simplicial complexes, like images and labeled graphs, obtained by convolving this data with various filters before computing persistence. Viewing a convolution filter as a local motif, the persistence diagram of the resulting convolution describes the way the motif is distributed across the simplicial complex. This pipeline, which we call convolutional persistence, extends the capacity of topology to observe patterns in such data. Moreover, we prove that (generically speaking) for any two labeled complexes one can find some filter for which they produce different persistence diagrams, so that the collection of all possible convolutional persistence diagrams is an injective invariant. This is proven by showing convolutional persistence to be a special case of another topological invariant, the Persistent Homology Transform. Other advantages of convolutional persistence are improved stability, greater flexibility 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20462;&#25913;&#37325;&#26500;&#35823;&#24046;&#30340;&#26041;&#24335;&#38598;&#20013;&#22312;&#25968;&#25454;&#27969;&#24418;&#19978;&#65292;&#20174;&#32780;&#35299;&#20915;&#28145;&#24230;&#21367;&#31215;&#33258;&#32534;&#30721;&#22120;&#22312;&#35270;&#35273;&#24322;&#24120;&#26816;&#27979;&#20013;&#23481;&#26131;&#20986;&#29616;&#30340;&#37325;&#26500;&#24322;&#24120;&#20449;&#21495;&#32780;&#23548;&#33268;&#26816;&#27979;&#25928;&#26524;&#19981;&#20339;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2206.11723</link><description>&lt;p&gt;
&#33258;&#32534;&#30721;&#22120;&#30340;&#33258;&#30417;&#30563;&#35757;&#32451;&#29992;&#20110;&#35270;&#35273;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Training with Autoencoders for Visual Anomaly Detection. (arXiv:2206.11723v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.11723
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20462;&#25913;&#37325;&#26500;&#35823;&#24046;&#30340;&#26041;&#24335;&#38598;&#20013;&#22312;&#25968;&#25454;&#27969;&#24418;&#19978;&#65292;&#20174;&#32780;&#35299;&#20915;&#28145;&#24230;&#21367;&#31215;&#33258;&#32534;&#30721;&#22120;&#22312;&#35270;&#35273;&#24322;&#24120;&#26816;&#27979;&#20013;&#23481;&#26131;&#20986;&#29616;&#30340;&#37325;&#26500;&#24322;&#24120;&#20449;&#21495;&#32780;&#23548;&#33268;&#26816;&#27979;&#25928;&#26524;&#19981;&#20339;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#21367;&#31215;&#33258;&#32534;&#30721;&#22120;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#23398;&#20064;&#38750;&#32447;&#24615;&#38477;&#32500;&#12290;&#26368;&#36817;&#65292;&#23427;&#20204;&#24050;&#34987;&#29992;&#20110;&#35270;&#35273;&#39046;&#22495;&#30340;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#12290;&#36890;&#36807;&#20351;&#29992;&#26080;&#24322;&#24120;&#30340;&#26679;&#26412;&#20248;&#21270;&#37325;&#26500;&#35823;&#24046;&#65292;&#26222;&#36941;&#35748;&#20026;&#30456;&#24212;&#30340;&#32593;&#32476;&#24212;&#22312;&#24212;&#29992;&#38454;&#27573;&#26410;&#33021;&#20934;&#30830;&#37325;&#26500;&#24322;&#24120;&#21306;&#22495;&#12290;&#36825;&#20010;&#30446;&#26631;&#36890;&#24120;&#36890;&#36807;&#25511;&#21046;&#32593;&#32476;&#30340;&#23481;&#37327;&#26469;&#35299;&#20915;&#65292;&#35201;&#20040;&#36890;&#36807;&#20943;&#23569;&#29942;&#39048;&#23618;&#30340;&#22823;&#23567;&#65292;&#35201;&#20040;&#36890;&#36807;&#23545;&#20854;&#28608;&#27963;&#26045;&#21152;&#31232;&#30095;&#32422;&#26463;&#12290;&#28982;&#32780;&#65292;&#36825;&#20004;&#31181;&#25216;&#26415;&#37117;&#27809;&#26377;&#26126;&#30830;&#24809;&#32602;&#24322;&#24120;&#20449;&#21495;&#30340;&#37325;&#26500;&#65292;&#36890;&#24120;&#23548;&#33268;&#26816;&#27979;&#25928;&#26524;&#19981;&#20339;&#12290;&#25105;&#20204;&#36890;&#36807;&#33258;&#36866;&#24212;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36825;&#31181;&#26041;&#27861;&#20801;&#35768;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#21028;&#21035;&#20449;&#24687;&#65292;&#36890;&#36807;&#20462;&#25913;&#30340;&#37325;&#26500;&#35823;&#24046;&#38598;&#20013;&#22312;&#25968;&#25454;&#27969;&#24418;&#19978;&#12290;&#36825;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#20135;&#29983;&#23616;&#37096;&#19968;&#33268;&#30340;&#37325;&#26500;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep convolutional autoencoders provide an effective tool for learning non-linear dimensionality reduction in an unsupervised way. Recently, they have been used for the task of anomaly detection in the visual domain. By optimising for the reconstruction error using anomaly-free examples, the common belief is that a corresponding network should fail to accurately reconstruct anomalous regions in the application phase. This goal is typically addressed by controlling the capacity of the network by either reducing the size of the bottleneck layer or enforcing sparsity constraints on its activations. However, neither of these techniques does explicitly penalize reconstruction of anomalous signals often resulting in poor detection. We tackle this problem by adapting a self-supervised learning regime, which allows to use discriminative information during training focusing on the data manifold by means of a modified reconstruction error. This regularizes the model to produce locally consistent
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23500;&#26377;&#20915;&#31574;&#23454;&#38469;&#24847;&#20041;&#30340;&#32858;&#21512;&#20989;&#25968;&#31867;&#65292;&#20197;&#26500;&#24314;&#19981;&#30830;&#23450;&#24615;&#22788;&#29702;&#30340;&#25968;&#23398;&#22522;&#30784;&#65292;&#36890;&#36807;&#20998;&#26512;&#35889;&#39118;&#38505;&#24230;&#37327;&#12289;Choquet&#31215;&#20998;&#21644;Lorentz&#33539;&#25968;&#30340;&#29305;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#39118;&#38505;&#24230;&#37327;&#36827;&#34892;&#20998;&#23618;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#26426;&#22120;&#23398;&#20064;&#23454;&#36341;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2206.03183</link><description>&lt;p&gt;
&#39118;&#38505;&#24230;&#37327;&#21644;&#19978;&#27010;&#29575;&#65306;&#36830;&#36143;&#24615;&#21644;&#20998;&#23618;
&lt;/p&gt;
&lt;p&gt;
Risk Measures and Upper Probabilities: Coherence and Stratification. (arXiv:2206.03183v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.03183
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23500;&#26377;&#20915;&#31574;&#23454;&#38469;&#24847;&#20041;&#30340;&#32858;&#21512;&#20989;&#25968;&#31867;&#65292;&#20197;&#26500;&#24314;&#19981;&#30830;&#23450;&#24615;&#22788;&#29702;&#30340;&#25968;&#23398;&#22522;&#30784;&#65292;&#36890;&#36807;&#20998;&#26512;&#35889;&#39118;&#38505;&#24230;&#37327;&#12289;Choquet&#31215;&#20998;&#21644;Lorentz&#33539;&#25968;&#30340;&#29305;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#39118;&#38505;&#24230;&#37327;&#36827;&#34892;&#20998;&#23618;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#26426;&#22120;&#23398;&#20064;&#23454;&#36341;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#36890;&#24120;&#20551;&#35774;&#32463;&#20856;&#27010;&#29575;&#35770;&#65292;&#36825;&#24847;&#21619;&#30528;&#32858;&#21512;&#24314;&#31435;&#22312;&#26399;&#26395;&#20043;&#19978;&#12290;&#29616;&#22312;&#26377;&#22810;&#20010;&#29702;&#30001;&#26469;&#23547;&#25214;&#27604;&#32463;&#20856;&#27010;&#29575;&#35770;&#26356;&#20016;&#23500;&#30340;&#26367;&#20195;&#25968;&#23398;&#22522;&#30784;&#65292;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#19968;&#31181;&#24378;&#22823;&#32780;&#20016;&#23500;&#30340;&#26367;&#20195;&#32858;&#21512;&#20989;&#25968;&#31867;&#65292;&#34987;&#31216;&#20026;&#35889;&#39118;&#38505;&#24230;&#37327;&#12289;Choquet&#31215;&#20998;&#25110;Lorentz&#33539;&#25968;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#34920;&#24449;&#32467;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#20010;&#35889;&#23478;&#26063;&#30340;&#29305;&#27530;&#20043;&#22788;&#12290;&#36890;&#36807;&#21033;&#29992;&#21487;&#37325;&#25490;&#19981;&#21464;Banach&#31354;&#38388;&#29702;&#35770;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#19968;&#31181;&#33258;&#28982;&#30340;&#23545;&#25152;&#26377;&#36830;&#36143;&#39118;&#38505;&#24230;&#37327;&#36827;&#34892;&#20998;&#23618;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#34920;&#31034;&#23427;&#20204;&#25152;&#24341;&#23548;&#30340;&#19978;&#27010;&#29575;&#12290;&#25105;&#20204;&#20174;&#23454;&#35777;&#35282;&#24230;&#28436;&#31034;&#20102;&#36825;&#31181;&#26032;&#30340;&#19981;&#30830;&#23450;&#24615;&#22788;&#29702;&#26041;&#27861;&#22914;&#20309;&#24110;&#21161;&#35299;&#20915;&#23454;&#38469;&#30340;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning typically presupposes classical probability theory which implies that aggregation is built upon expectation. There are now multiple reasons to motivate looking at richer alternatives to classical probability theory as a mathematical foundation for machine learning. We systematically examine a powerful and rich class of alternative aggregation functionals, known variously as spectral risk measures, Choquet integrals or Lorentz norms. We present a range of characterization results, and demonstrate what makes this spectral family so special. In doing so we arrive at a natural stratification of all coherent risk measures in terms of the upper probabilities that they induce by exploiting results from the theory of rearrangement invariant Banach spaces. We empirically demonstrate how this new approach to uncertainty helps tackling practical machine learning problems.
&lt;/p&gt;</description></item><item><title>MCCE&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#25311;&#21487;&#21464;&#29305;&#24449;&#21644;&#20915;&#31574;&#30340;&#32852;&#21512;&#20998;&#24067;&#65292;&#29983;&#25104;&#22788;&#20110;&#27969;&#24418;&#19978;&#12289;&#21487;&#34892;&#24182;&#19988;&#26377;&#25928;&#30340;&#21453;&#20107;&#23454;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;MCCE&#21487;&#20197;&#22788;&#29702;&#20219;&#20309;&#31867;&#22411;&#30340;&#39044;&#27979;&#27169;&#22411;&#21644;&#20855;&#26377;&#22810;&#20010;&#32423;&#21035;&#30340;&#20998;&#31867;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2111.09790</link><description>&lt;p&gt;
MCCE&#65306;&#33945;&#29305;&#21345;&#27931;&#37319;&#26679;&#30340;&#29616;&#23454;&#21453;&#20107;&#23454;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
MCCE: Monte Carlo sampling of realistic counterfactual explanations. (arXiv:2111.09790v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.09790
&lt;/p&gt;
&lt;p&gt;
MCCE&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#25311;&#21487;&#21464;&#29305;&#24449;&#21644;&#20915;&#31574;&#30340;&#32852;&#21512;&#20998;&#24067;&#65292;&#29983;&#25104;&#22788;&#20110;&#27969;&#24418;&#19978;&#12289;&#21487;&#34892;&#24182;&#19988;&#26377;&#25928;&#30340;&#21453;&#20107;&#23454;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;MCCE&#21487;&#20197;&#22788;&#29702;&#20219;&#20309;&#31867;&#22411;&#30340;&#39044;&#27979;&#27169;&#22411;&#21644;&#20855;&#26377;&#22810;&#20010;&#32423;&#21035;&#30340;&#20998;&#31867;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;MCCE: Monte Carlo&#37319;&#26679;&#30340;&#26377;&#25928;&#21644;&#29616;&#23454;&#30340;&#34920;&#26684;&#25968;&#25454;&#21453;&#20107;&#23454;&#35299;&#37322;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#32473;&#23450;&#19981;&#21487;&#21464;&#29305;&#24449;&#21644;&#20915;&#31574;&#30340;&#21487;&#21464;&#29305;&#24449;&#30340;&#32852;&#21512;&#20998;&#24067;&#36827;&#34892;&#24314;&#27169;&#65292;&#29983;&#25104;&#22788;&#20110;&#27969;&#24418;&#19978;&#12289;&#21487;&#34892;&#24182;&#19988;&#26377;&#25928;&#30340;&#21453;&#20107;&#23454;&#12290;&#19982;&#20854;&#20182;&#20381;&#36182;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#20855;&#26377;&#20005;&#26684;&#39044;&#27979;&#27169;&#22411;&#21644;&#25968;&#25454;&#35201;&#27714;&#30340;&#27969;&#24418;&#26041;&#27861;&#19981;&#21516;&#65292;MCCE&#21487;&#20197;&#22788;&#29702;&#20219;&#20309;&#31867;&#22411;&#30340;&#39044;&#27979;&#27169;&#22411;&#21644;&#20855;&#26377;&#20004;&#20010;&#20197;&#19978;&#32423;&#21035;&#30340;&#20998;&#31867;&#29305;&#24449;&#12290;MCCE&#39318;&#20808;&#20351;&#29992;&#33258;&#22238;&#24402;&#29983;&#25104;&#27169;&#22411;&#23545;&#29305;&#24449;&#21644;&#20915;&#31574;&#30340;&#32852;&#21512;&#20998;&#24067;&#36827;&#34892;&#24314;&#27169;&#65292;&#20854;&#20013;&#26465;&#20214;&#27010;&#29575;&#20351;&#29992;&#20915;&#31574;&#26641;&#36827;&#34892;&#20272;&#35745;&#12290;&#28982;&#21518;&#65292;&#23427;&#20174;&#35813;&#27169;&#22411;&#20013;&#37319;&#26679;&#19968;&#22823;&#32452;&#35266;&#27979;&#20540;&#65292;&#26368;&#21518;&#21024;&#38500;&#19981;&#31526;&#21512;&#29305;&#23450;&#26465;&#20214;&#30340;&#26679;&#26412;&#12290;&#25105;&#20204;&#20351;&#29992;&#22235;&#20010;&#30693;&#21517;&#25968;&#25454;&#38598;&#23558;MCCE&#19982;&#19968;&#31995;&#21015;&#26368;&#20808;&#36827;&#30340;&#27969;&#24418;&#21453;&#20107;&#23454;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#23637;&#31034;&#20102;MCCE&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce MCCE: Monte Carlo sampling of valid and realistic Counterfactual Explanations for tabular data, a novel counterfactual explanation method that generates on-manifold, actionable and valid counterfactuals by modeling the joint distribution of the mutable features given the immutable features and the decision. Unlike other on-manifold methods that tend to rely on variational autoencoders and have strict prediction model and data requirements, MCCE handles any type of prediction model and categorical features with more than two levels. MCCE first models the joint distribution of the features and the decision with an autoregressive generative model where the conditionals are estimated using decision trees. Then, it samples a large set of observations from this model, and finally, it removes the samples that do not obey certain criteria. We compare MCCE with a range of state-of-the-art on-manifold counterfactual methods using four well-known data sets and show that MCCE outperfo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#23454;&#38469;&#20351;&#29992;&#24182;&#21487;&#20197;&#26377;&#25928;&#36867;&#36991;&#40657;&#30418;Android&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#22120;&#30340;&#23545;&#25239;&#25915;&#20987;&#8212;&#8212;EvadeDroid&#65292;&#24182;&#19988;&#21487;&#20197;&#20445;&#30041;&#21407;&#22987;&#24694;&#24847;&#36719;&#20214;&#24212;&#29992;&#31243;&#24207;&#30340;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2110.03301</link><description>&lt;p&gt;
EvadeDroid&#65306;&#19968;&#31181;&#23545;&#40657;&#30418;Android&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#30340;&#23454;&#29992;&#36867;&#36920;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
EvadeDroid: A Practical Evasion Attack on Machine Learning for Black-box Android Malware Detection. (arXiv:2110.03301v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.03301
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#23454;&#38469;&#20351;&#29992;&#24182;&#21487;&#20197;&#26377;&#25928;&#36867;&#36991;&#40657;&#30418;Android&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#22120;&#30340;&#23545;&#25239;&#25915;&#20987;&#8212;&#8212;EvadeDroid&#65292;&#24182;&#19988;&#21487;&#20197;&#20445;&#30041;&#21407;&#22987;&#24694;&#24847;&#36719;&#20214;&#24212;&#29992;&#31243;&#24207;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#37324;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#24320;&#21457;&#36867;&#36991;&#25915;&#20987;&#26469;&#24191;&#27867;&#25506;&#32034;Android&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#22120;&#23545;&#23545;&#25239;&#24615;&#26679;&#26412;&#30340;&#28431;&#27934;&#65307;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#36825;&#20123;&#25915;&#20987;&#30340;&#23454;&#29992;&#24615;&#20173;&#28982;&#26377;&#20105;&#35758;&#12290;&#22823;&#22810;&#25968;&#30740;&#31350;&#20551;&#23450;&#25915;&#20987;&#32773;&#30693;&#36947;&#29992;&#20110;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#30340;&#30446;&#26631;&#20998;&#31867;&#22120;&#30340;&#35814;&#32454;&#20449;&#24687;&#65292;&#32780;&#22312;&#29616;&#23454;&#20013;&#65292;&#24694;&#24847;&#34892;&#20026;&#32773;&#21482;&#33021;&#33719;&#24471;&#26377;&#38480;&#30340;&#30446;&#26631;&#20998;&#31867;&#22120;&#35775;&#38382;&#26435;&#38480;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;EvadeDroid&#65292;&#19968;&#31181;&#23454;&#29992;&#30340;&#22522;&#20110;&#20915;&#31574;&#30340;&#23545;&#25239;&#25915;&#20987;&#65292;&#26088;&#22312;&#26377;&#25928;&#36867;&#36991;&#40657;&#30418;Android&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#22120;&#30340;&#26816;&#27979;&#12290;&#38500;&#20102;&#29983;&#25104;&#23454;&#38469;&#30340;&#23545;&#25239;&#24615;&#24694;&#24847;&#36719;&#20214;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;&#36867;&#36991;&#25915;&#20987;&#36824;&#21487;&#20197;&#20445;&#30041;&#21407;&#22987;&#24694;&#24847;&#36719;&#20214;&#24212;&#29992;&#31243;&#24207;&#30340;&#21151;&#33021;&#12290;&#36890;&#36807;&#21033;&#29992;&#22522;&#20110;n-gram&#30340;&#26041;&#27861;&#26500;&#24314;&#26469;&#33258;&#33391;&#24615;&#26679;&#26412;&#30340;&#21151;&#33021;&#20445;&#25345;&#36716;&#25442;&#65292;&#36825;&#20123;&#36716;&#25442;&#19982;&#24694;&#24847;&#36719;&#20214;&#24212;&#29992;&#31243;&#24207;&#20855;&#26377;&#25805;&#20316;&#30721;&#32423;&#21035;&#30340;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the last decade, researchers have extensively explored the vulnerabilities of Android malware detectors to adversarial examples through the development of evasion attacks; however, the practicality of these attacks in real-world scenarios remains arguable. The majority of studies have assumed attackers know the details of the target classifiers used for malware detection, while in reality, malicious actors have limited access to the target classifiers. This paper introduces EvadeDroid, a practical decision-based adversarial attack designed to effectively evade black-box Android malware detectors in real-world scenarios. In addition to generating real-world adversarial malware, the proposed evasion attack can also preserve the functionality of the original malware applications (apps). EvadeDroid constructs a collection of functionality-preserving transformations derived from benign donors that share opcode-level similarity with malware apps by leveraging an n-gram-based approach. T
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#38024;&#23545;&#38750;&#20984;-&#20985;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#30340;&#26080;&#23548;&#25968;&#20132;&#26367;&#25237;&#24433;&#31639;&#27861;&#65292;&#21253;&#25324;&#20809;&#28369;&#38382;&#39064;&#30340;&#20132;&#26367;&#38543;&#26426;&#26799;&#24230;&#25237;&#24433;&#31639;&#27861;&#65288;ZO-AGP&#65289;&#65292;&#20197;&#21450;&#22359;&#29366;&#38750;&#20809;&#28369;&#38382;&#39064;&#30340;&#20998;&#22359;&#20132;&#26367;&#38543;&#26426;&#36817;&#31471;&#26799;&#24230;&#31639;&#27861;&#65288;ZO-BAPG&#65289;&#12290;&#36825;&#20123;&#31639;&#27861;&#20855;&#26377;&#36739;&#23569;&#30340;&#20989;&#25968;&#20540;&#20272;&#35745;&#21644;&#36739;&#39640;&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2108.00473</link><description>&lt;p&gt;
&#38024;&#23545;&#38750;&#20984;-&#20985;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#30340;&#26080;&#23548;&#25968;&#20132;&#26367;&#25237;&#24433;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Derivative-free Alternating Projection Algorithms for General Nonconvex-Concave Minimax Problems. (arXiv:2108.00473v3 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.00473
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#38024;&#23545;&#38750;&#20984;-&#20985;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#30340;&#26080;&#23548;&#25968;&#20132;&#26367;&#25237;&#24433;&#31639;&#27861;&#65292;&#21253;&#25324;&#20809;&#28369;&#38382;&#39064;&#30340;&#20132;&#26367;&#38543;&#26426;&#26799;&#24230;&#25237;&#24433;&#31639;&#27861;&#65288;ZO-AGP&#65289;&#65292;&#20197;&#21450;&#22359;&#29366;&#38750;&#20809;&#28369;&#38382;&#39064;&#30340;&#20998;&#22359;&#20132;&#26367;&#38543;&#26426;&#36817;&#31471;&#26799;&#24230;&#31639;&#27861;&#65288;ZO-BAPG&#65289;&#12290;&#36825;&#20123;&#31639;&#27861;&#20855;&#26377;&#36739;&#23569;&#30340;&#20989;&#25968;&#20540;&#20272;&#35745;&#21644;&#36739;&#39640;&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#20984;-&#20985;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#30340;&#38646;&#38454;&#31639;&#27861;&#65292;&#36825;&#31867;&#38382;&#39064;&#36817;&#24180;&#22312;&#26426;&#22120;&#23398;&#20064;&#12289;&#20449;&#21495;&#22788;&#29702;&#31561;&#39046;&#22495;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#38454;&#20132;&#26367;&#38543;&#26426;&#26799;&#24230;&#25237;&#24433;&#65288;ZO-AGP&#65289;&#31639;&#27861;&#26469;&#35299;&#20915;&#20809;&#28369;&#30340;&#38750;&#20984;-&#20985;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#65292;&#20854;&#36845;&#20195;&#22797;&#26434;&#24230;&#20026; $\mathcal{O}(\varepsilon^{-4})$&#65292;&#27599;&#27425;&#36845;&#20195;&#30340;&#20989;&#25968;&#20540;&#20272;&#35745;&#27425;&#25968;&#20026; $\mathcal{O}(d_{x}+d_{y})$&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#38454;&#20998;&#22359;&#20132;&#26367;&#38543;&#26426;&#36817;&#31471;&#26799;&#24230;&#31639;&#27861;&#65288;ZO-BAPG&#65289;&#26469;&#35299;&#20915;&#22359;&#29366;&#38750;&#20809;&#28369;&#30340;&#38750;&#20984;-&#20985;&#26497;&#23567;&#26497;&#22823;&#20248;&#21270;&#38382;&#39064;&#65292;&#20854;&#36845;&#20195;&#22797;&#26434;&#24230;&#20026; $\mathcal{O}(\varepsilon^{-4})$&#65292;&#27599;&#27425;&#36845;&#20195;&#30340;&#20989;&#25968;&#20540;&#20272;&#35745;&#27425;&#25968;&#20026; $\mathcal{O}(K d_{x}+d_{y})$&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#39318;&#27425;&#25552;&#20986;&#36825;&#20123;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study zeroth-order algorithms for nonconvex-concave minimax problems, which have attracted widely attention in machine learning, signal processing and many other fields in recent years. We propose a zeroth-order alternating randomized gradient projection (ZO-AGP) algorithm for smooth nonconvex-concave minimax problems, and its iteration complexity to obtain an $\varepsilon$-stationary point is bounded by $\mathcal{O}(\varepsilon^{-4})$, and the number of function value estimation is bounded by $\mathcal{O}(d_{x}+d_{y})$ per iteration. Moreover, we propose a zeroth-order block alternating randomized proximal gradient algorithm (ZO-BAPG) for solving block-wise nonsmooth nonconvex-concave minimax optimization problems, and the iteration complexity to obtain an $\varepsilon$-stationary point is bounded by $\mathcal{O}(\varepsilon^{-4})$ and the number of function value estimation per iteration is bounded by $\mathcal{O}(K d_{x}+d_{y})$. To the best of our knowledge, this 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#32534;&#30721;&#29702;&#35770;&#19982;&#20132;&#21449;&#39564;&#35777;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#21457;&#29616;&#20102;&#23398;&#20064;&#31639;&#27861;&#22312;&#22266;&#23450;&#25968;&#25454;&#19978;&#33021;&#35299;&#20915;&#19981;&#21516;&#20108;&#36827;&#21046;&#20998;&#31867;&#38382;&#39064;&#30340;&#25968;&#37327;&#19982;&#35823;&#24046;&#26816;&#27979;&#30721;&#29702;&#35770;&#23494;&#20999;&#30456;&#20851;&#12290;&#25105;&#20204;&#36824;&#23545;&#19968;&#31181;&#29305;&#23450;&#30340;&#20132;&#21449;&#39564;&#35777;&#26041;&#27861;&#19979;&#30340;&#26368;&#22823;&#20998;&#31867;&#38382;&#39064;&#25968;&#37327;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#36825;&#21462;&#20915;&#20110;&#24120;&#26435;&#30721;&#30340;&#30721;&#23383;&#25968;&#37327;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25512;&#24191;&#20102;&#24120;&#26435;&#30721;&#30340;&#27010;&#24565;&#65292;&#24182;&#35777;&#26126;&#20102;&#31867;&#20284;&#30340;&#32467;&#26524;&#36866;&#29992;&#20110;&#20854;&#20182;&#20132;&#21449;&#39564;&#35777;&#38169;&#35823;&#21644;&#36731;&#37327;&#32423;&#24120;&#26435;&#30721;&#12290;</title><link>http://arxiv.org/abs/2103.11856</link><description>&lt;p&gt;
&#32534;&#30721;&#29702;&#35770;&#19982;&#20132;&#21449;&#39564;&#35777;&#30340;&#32852;&#31995;&#21450;&#20854;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
A Link between Coding Theory and Cross-Validation with Applications. (arXiv:2103.11856v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2103.11856
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#32534;&#30721;&#29702;&#35770;&#19982;&#20132;&#21449;&#39564;&#35777;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#21457;&#29616;&#20102;&#23398;&#20064;&#31639;&#27861;&#22312;&#22266;&#23450;&#25968;&#25454;&#19978;&#33021;&#35299;&#20915;&#19981;&#21516;&#20108;&#36827;&#21046;&#20998;&#31867;&#38382;&#39064;&#30340;&#25968;&#37327;&#19982;&#35823;&#24046;&#26816;&#27979;&#30721;&#29702;&#35770;&#23494;&#20999;&#30456;&#20851;&#12290;&#25105;&#20204;&#36824;&#23545;&#19968;&#31181;&#29305;&#23450;&#30340;&#20132;&#21449;&#39564;&#35777;&#26041;&#27861;&#19979;&#30340;&#26368;&#22823;&#20998;&#31867;&#38382;&#39064;&#25968;&#37327;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#36825;&#21462;&#20915;&#20110;&#24120;&#26435;&#30721;&#30340;&#30721;&#23383;&#25968;&#37327;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25512;&#24191;&#20102;&#24120;&#26435;&#30721;&#30340;&#27010;&#24565;&#65292;&#24182;&#35777;&#26126;&#20102;&#31867;&#20284;&#30340;&#32467;&#26524;&#36866;&#29992;&#20110;&#20854;&#20182;&#20132;&#21449;&#39564;&#35777;&#38169;&#35823;&#21644;&#36731;&#37327;&#32423;&#24120;&#26435;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#20010;&#23398;&#20064;&#31639;&#27861;&#22312;&#32473;&#23450;&#25968;&#25454;&#19978;&#33021;&#35299;&#20915;&#22810;&#23569;&#20010;&#19981;&#21516;&#30340;&#20108;&#36827;&#21046;&#20998;&#31867;&#38382;&#39064;&#65292;&#20854;&#20013;&#35201;&#27714;&#20132;&#21449;&#39564;&#35777;&#38169;&#35823;&#20026;&#38646;&#25110;&#26368;&#22810;&#32473;&#23450;&#25968;&#37327;&#65311;&#22312;&#21069;&#19968;&#31181;&#24773;&#20917;&#19979;&#65292;&#26681;&#25454;&#26080;&#20813;&#36153;&#21320;&#39184;&#23450;&#29702;&#65292;&#36825;&#20010;&#25968;&#37327;&#26159;&#26377;&#38480;&#30340;&#65292;&#25105;&#20204;&#34920;&#26126;&#31934;&#30830;&#31572;&#26696;&#30001;&#35823;&#24046;&#26816;&#27979;&#30721;&#29702;&#35770;&#32473;&#20986;&#12290;&#20316;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#20204;&#20851;&#27880;AUC&#24615;&#33021;&#24230;&#37327;&#21644;&#30041;&#19968;&#23545;&#20132;&#21449;&#39564;&#35777;(LPOCV)&#65292;&#20854;&#20013;&#27599;&#20010;&#21487;&#33021;&#20855;&#26377;&#19981;&#21516;&#31867;&#26631;&#31614;&#30340;&#25968;&#25454;&#23545;&#37117;&#20250;&#34987;&#26242;&#26102;&#20445;&#30041;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23545;&#20110;&#22266;&#23450;&#31867;&#27604;&#20363;&#30340;&#20998;&#31867;&#38382;&#39064;&#65292;&#23398;&#20064;&#31639;&#27861;&#33021;&#22815;&#23454;&#29616;&#38646;LPOCV&#38169;&#35823;&#30340;&#26368;&#22823;&#25968;&#37327;&#31561;&#20110;&#24120;&#26435;&#30721;(CWC)&#20013;&#30340;&#26368;&#22823;&#30721;&#23383;&#25968;&#37327;&#65292;&#20855;&#26377;&#19968;&#23450;&#30340;&#25216;&#26415;&#24615;&#36136;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#36731;&#37327;&#32423;&#24120;&#26435;&#30721;(light CWC)&#26469;&#25512;&#24191;CWC&#65292;&#24182;&#35777;&#26126;&#20102;&#23545;&#20110;&#38750;&#38646;LPOCV&#38169;&#35823;&#21644;&#36731;&#37327;&#32423;&#24120;&#26435;&#30721;&#30340;&#31867;&#20284;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#30721;&#23383;&#25968;&#37327;&#30340;&#26368;&#22823;&#19978;&#30028;&#21644;&#26368;&#22823;&#19979;&#30028;&#20063;&#36827;&#34892;&#20102;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
How many different binary classification problems a single learning algorithm can solve on a fixed data with exactly zero or at most a given number of cross-validation errors? While the number in the former case is known to be limited by the no-free-lunch theorem, we show that the exact answers are given by the theory of error detecting codes. As a case study, we focus on the AUC performance measure and leave-pair-out cross-validation (LPOCV), in which every possible pair of data with different class labels is held out at a time. We shown that the maximal number of classification problems with fixed class proportion, for which a learning algorithm can achieve zero LPOCV error, equals the maximal number of code words in a constant weight code (CWC), with certain technical properties. We then generalize CWCs by introducing light CWCs and prove an analogous result for nonzero LPOCV errors and light CWCs. Moreover, we prove both upper and lower bounds on the maximal numbers of code words i
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#26679;&#24615;&#20445;&#25345;&#30340;&#22270;&#32467;&#26500;&#32454;&#21270;&#30340;&#22270;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#24050;&#23398;&#20064;&#30340;&#33410;&#28857;&#34920;&#31034;&#26469;&#36880;&#27493;&#25913;&#21892;&#22270;&#24418;&#32467;&#26500;&#36136;&#37327;&#12290;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;&#65292;&#21253;&#25324;&#33410;&#28857;&#20998;&#31867;&#12289;&#38142;&#25509;&#39044;&#27979;&#21644;&#22270;&#32858;&#31867;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2103.07295</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#26679;&#24615;&#20445;&#25345;&#30340;&#22270;&#32467;&#26500;&#32454;&#21270;&#30340;&#22270;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Graph Representation Learning via Diversity-preserving Graph Refinement. (arXiv:2103.07295v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2103.07295
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#26679;&#24615;&#20445;&#25345;&#30340;&#22270;&#32467;&#26500;&#32454;&#21270;&#30340;&#22270;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#24050;&#23398;&#20064;&#30340;&#33410;&#28857;&#34920;&#31034;&#26469;&#36880;&#27493;&#25913;&#21892;&#22270;&#24418;&#32467;&#26500;&#36136;&#37327;&#12290;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;&#65292;&#21253;&#25324;&#33410;&#28857;&#20998;&#31867;&#12289;&#38142;&#25509;&#39044;&#27979;&#21644;&#22270;&#32858;&#31867;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#30495;&#23454;&#30340;&#22270;&#25968;&#25454;&#65292;&#33410;&#28857;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#36890;&#24120;&#34987;&#34920;&#31034;&#20026;&#30828;&#24615;&#20108;&#36827;&#21046;&#38142;&#25509;&#12290;&#26174;&#28982;&#65292;&#36825;&#26159;&#19968;&#31181;&#31163;&#25955;&#21644;&#31616;&#21270;&#30340;&#36830;&#32493;&#20851;&#31995;&#24418;&#24335;&#65292;&#20005;&#37325;&#38480;&#21046;&#20102;&#23398;&#20064;&#21040;&#30340;&#33410;&#28857;&#34920;&#31034;&#30340;&#21487;&#34920;&#36798;&#24615;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#23884;&#20837;&#31354;&#38388;&#20013;&#33719;&#24471;&#30340;&#33410;&#28857;&#34920;&#31034;&#21487;&#20197;&#21453;&#36807;&#26469;&#25581;&#31034;&#33410;&#28857;&#20043;&#38388;&#30340;&#20869;&#22312;&#20851;&#31995;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29305;&#24449;&#21270;&#33410;&#28857;&#20851;&#31995;&#24182;&#36827;&#19968;&#27493;&#20419;&#36827;&#33410;&#28857;&#34920;&#31034;&#30340;&#23398;&#20064;&#65292;&#19968;&#31181;&#30452;&#35266;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#23884;&#20837;&#30340;&#33410;&#28857;&#34920;&#31034;&#26469;&#32454;&#21270;&#26368;&#21021;&#32473;&#23450;&#30340;&#22270;&#32467;&#26500;&#12290;&#20294;&#26159;&#65292;&#20840;&#23616;&#32454;&#21270;&#25152;&#26377;&#33410;&#28857;&#20043;&#38388;&#30340;&#20851;&#31995;&#26080;&#27861;&#21306;&#20998;&#23558;&#19981;&#21487;&#36991;&#20813;&#22320;&#23548;&#33268;&#19968;&#20123;&#22122;&#22768;&#36793;&#32536;&#65292;&#36825;&#21487;&#33021;&#36827;&#19968;&#27493;&#28151;&#28102;&#33410;&#28857;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#22823;&#22411;&#22270;&#24418;&#19978;&#20063;&#23384;&#22312;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23616;&#37096;&#32467;&#26500;&#24863;&#30693;&#30340;&#22270;&#24418;&#32454;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#24050;&#32463;&#23398;&#21040;&#30340;&#33410;&#28857;&#34920;&#31034;&#36880;&#27493;&#25913;&#21892;&#22270;&#24418;&#32467;&#26500;&#36136;&#37327;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#23545;&#22270;&#20013;&#30340;&#38543;&#26426;&#28216;&#36208;&#27169;&#25311;&#29983;&#25104;&#19968;&#20010;&#22810;&#26679;&#21270;&#30340;&#37051;&#22495;&#32467;&#26500;&#38598;&#12290;&#28982;&#21518;&#65292;&#23545;&#20110;&#27599;&#20010;&#27169;&#25311;&#37051;&#22495;&#65292;&#25105;&#20204;&#22312;&#25972;&#20010;&#32454;&#21270;&#36807;&#31243;&#20013;&#20445;&#25345;&#37051;&#22495;&#32467;&#26500;&#30340;&#22810;&#26679;&#24615;&#65292;&#21516;&#26102;&#32454;&#21270;&#37051;&#22495;&#20869;&#30340;&#33410;&#28857;&#20851;&#31995;&#12290;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#35780;&#20272;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;&#65292;&#21253;&#25324;&#33410;&#28857;&#20998;&#31867;&#12289;&#38142;&#25509;&#39044;&#27979;&#21644;&#22270;&#32858;&#31867;&#65292;&#22343;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
For real-world graph data, the complex relationship between nodes is often represented as a hard binary link. Obviously, it is a discrete and simplified form of continuous relationship between nodes, which seriously limits the expressibility of the learned node representation. On the other hand, the node representation obtained in the embedding space can in turn be used to reveal the intrinsic relationship between nodes. To better characterize the node relationships and further facilitate the learning of node representation, an intuitive way is to refine the originally given graph structure with the embedded node representations. However, such global refinement of the relationships among all nodes without distinction will inevitably lead to some noisy edges, which may further confuse the training of the node representation learning model. In addition, it also has scalability problems on large graphs. To address these issues, we propose a local structure aware graph refinement to progre
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#27491;&#21017;&#21270;&#38543;&#26426;&#25511;&#21046;&#38382;&#39064;&#20013;&#28176;&#21464;&#27969;&#30340;&#24212;&#29992;&#65292;&#25214;&#21040;&#20102;&#36866;&#21512;&#30340;&#24230;&#37327;&#31354;&#38388;&#65292;&#22312;&#21487;&#34892;&#25511;&#21046;&#38598;&#20013;&#26500;&#24314;&#20102;&#28176;&#21464;&#27969;&#65292;&#20351;&#24471;&#25104;&#26412;&#20989;&#25968;&#36882;&#20943;&#65292;&#35777;&#26126;&#20102;&#28176;&#21464;&#27969;&#30340;&#19981;&#21464;&#27979;&#24230;&#28385;&#36275;&#26368;&#20248;&#24615;&#21407;&#21017;&#65292;&#19988;&#25351;&#25968;&#32423;&#25910;&#25947;&#12290;&#27492;&#22806;&#65292;&#26368;&#20248;&#27979;&#24230;&#20540;&#25511;&#21046;&#36807;&#31243;&#20855;&#26377;&#36125;&#21494;&#26031;&#35299;&#37322;&#65292;&#21487;&#20197;&#34701;&#20837;&#20808;&#39564;&#30693;&#35782;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25193;&#23637;&#22686;&#24378;&#23398;&#20064;&#20013;&#38543;&#26426;&#26799;&#24230;&#31639;&#27861;&#29992;&#20110;&#35299;&#20915;&#25511;&#21046;&#38382;&#39064;&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2006.05956</link><description>&lt;p&gt;
&#28176;&#21464;&#27969;&#29992;&#20110;&#27491;&#21017;&#21270;&#38543;&#26426;&#25511;&#21046;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Gradient Flows for Regularized Stochastic Control Problems. (arXiv:2006.05956v5 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2006.05956
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#27491;&#21017;&#21270;&#38543;&#26426;&#25511;&#21046;&#38382;&#39064;&#20013;&#28176;&#21464;&#27969;&#30340;&#24212;&#29992;&#65292;&#25214;&#21040;&#20102;&#36866;&#21512;&#30340;&#24230;&#37327;&#31354;&#38388;&#65292;&#22312;&#21487;&#34892;&#25511;&#21046;&#38598;&#20013;&#26500;&#24314;&#20102;&#28176;&#21464;&#27969;&#65292;&#20351;&#24471;&#25104;&#26412;&#20989;&#25968;&#36882;&#20943;&#65292;&#35777;&#26126;&#20102;&#28176;&#21464;&#27969;&#30340;&#19981;&#21464;&#27979;&#24230;&#28385;&#36275;&#26368;&#20248;&#24615;&#21407;&#21017;&#65292;&#19988;&#25351;&#25968;&#32423;&#25910;&#25947;&#12290;&#27492;&#22806;&#65292;&#26368;&#20248;&#27979;&#24230;&#20540;&#25511;&#21046;&#36807;&#31243;&#20855;&#26377;&#36125;&#21494;&#26031;&#35299;&#37322;&#65292;&#21487;&#20197;&#34701;&#20837;&#20808;&#39564;&#30693;&#35782;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25193;&#23637;&#22686;&#24378;&#23398;&#20064;&#20013;&#38543;&#26426;&#26799;&#24230;&#31639;&#27861;&#29992;&#20110;&#35299;&#20915;&#25511;&#21046;&#38382;&#39064;&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#30340;&#26159;&#23558;&#34892;&#21160;&#31354;&#38388;&#35774;&#23450;&#20026;&#27010;&#29575;&#27979;&#24230;&#12289;&#30446;&#26631;&#20989;&#25968;&#36890;&#36807;&#30456;&#23545;&#29109;&#24809;&#32602;&#30340;&#38543;&#26426;&#25511;&#21046;&#38382;&#39064;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#36866;&#21512;&#30340;&#24230;&#37327;&#31354;&#38388;&#65292;&#26500;&#24314;&#20102;&#23545;&#27979;&#24230;&#20540;&#25511;&#21046;&#36807;&#31243;&#30340;&#28176;&#21464;&#27969;&#65292;&#23427;&#22312;&#21487;&#34892;&#25511;&#21046;&#38598;&#20013;&#20445;&#35777;&#20102;&#25104;&#26412;&#20989;&#25968;&#30340;&#36882;&#20943;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#28176;&#21464;&#27969;&#30340;&#20219;&#20309;&#19981;&#21464;&#27979;&#24230;&#37117;&#28385;&#36275;&#24222;&#29305;&#37324;&#20122;&#37329;&#26368;&#20248;&#24615;&#21407;&#29702;&#12290;&#22914;&#26524;&#25152;&#22788;&#29702;&#30340;&#38382;&#39064;&#26159;&#36275;&#22815;&#20984;&#30340;&#65292;&#28176;&#21464;&#27969;&#23558;&#25351;&#25968;&#32423;&#25910;&#25947;&#12290;&#27492;&#22806;&#65292;&#26368;&#20248;&#27979;&#24230;&#20540;&#25511;&#21046;&#36807;&#31243;&#20855;&#26377;&#36125;&#21494;&#26031;&#35299;&#37322;&#65292;&#36825;&#24847;&#21619;&#30528;&#22312;&#35299;&#20915;&#27492;&#31867;&#38543;&#26426;&#25511;&#21046;&#38382;&#39064;&#26102;&#21487;&#20197;&#34701;&#20837;&#20808;&#39564;&#30693;&#35782;&#12290;&#26412;&#24037;&#20316;&#30340;&#21160;&#26426;&#26159;&#20026;&#20102;&#25193;&#23637;&#29992;&#20110;&#35299;&#20915;&#25511;&#21046;&#38382;&#39064;&#30340;&#22686;&#24378;&#23398;&#20064;&#31038;&#21306;&#24191;&#27867;&#37319;&#29992;&#30340;&#38543;&#26426;&#26799;&#24230;&#31867;&#22411;&#31639;&#27861;&#30340;&#25910;&#25947;&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies stochastic control problems with the action space taken to be probability measures, with the objective penalised by the relative entropy. We identify suitable metric space on which we construct a gradient flow for the measure-valued control process, in the set of admissible controls, along which the cost functional is guaranteed to decrease. It is shown that any invariant measure of this gradient flow satisfies the Pontryagin optimality principle. If the problem we work with is sufficiently convex, the gradient flow converges exponentially fast. Furthermore, the optimal measure-valued control process admits a Bayesian interpretation which means that one can incorporate prior knowledge when solving such stochastic control problems. This work is motivated by a desire to extend the theoretical underpinning for the convergence of stochastic gradient type algorithms widely employed in the reinforcement learning community to solve control problems.
&lt;/p&gt;</description></item></channel></rss>