<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#29702;&#26500;&#25104;&#30340;&#20195;&#29702;&#26694;&#26550;TrustAgent&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#39044;&#20808;&#35268;&#21010;&#12289;&#35268;&#21010;&#36807;&#31243;&#20013;&#21644;&#35745;&#21010;&#21518;&#26816;&#26597;&#19977;&#31181;&#31574;&#30053;&#26469;&#25552;&#39640;LLM&#20195;&#29702;&#30340;&#23433;&#20840;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#35782;&#21035;&#21644;&#39044;&#38450;&#28508;&#22312;&#21361;&#38505;&#12290;&#27492;&#22806;&#65292;&#36824;&#30740;&#31350;&#20102;&#23433;&#20840;&#24615;&#19982;&#20351;&#29992;&#32773;&#28385;&#24847;&#24230;&#20197;&#21450;&#27169;&#22411;&#25512;&#29702;&#33021;&#21147;&#19982;&#25928;&#29575;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01586</link><description>&lt;p&gt;
TrustAgent: &#36890;&#36807;&#20195;&#29702;&#26500;&#25104;&#23454;&#29616;&#23433;&#20840;&#21487;&#20449;&#36182;&#30340;LLM&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
TrustAgent: Towards Safe and Trustworthy LLM-based Agents through Agent Constitution
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01586
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#29702;&#26500;&#25104;&#30340;&#20195;&#29702;&#26694;&#26550;TrustAgent&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#39044;&#20808;&#35268;&#21010;&#12289;&#35268;&#21010;&#36807;&#31243;&#20013;&#21644;&#35745;&#21010;&#21518;&#26816;&#26597;&#19977;&#31181;&#31574;&#30053;&#26469;&#25552;&#39640;LLM&#20195;&#29702;&#30340;&#23433;&#20840;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#35782;&#21035;&#21644;&#39044;&#38450;&#28508;&#22312;&#21361;&#38505;&#12290;&#27492;&#22806;&#65292;&#36824;&#30740;&#31350;&#20102;&#23433;&#20840;&#24615;&#19982;&#20351;&#29992;&#32773;&#28385;&#24847;&#24230;&#20197;&#21450;&#27169;&#22411;&#25512;&#29702;&#33021;&#21147;&#19982;&#25928;&#29575;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20294;&#20854;&#21487;&#20449;&#24230;&#20173;&#26410;&#24471;&#21040;&#28145;&#20837;&#25506;&#32034;&#12290;&#30001;&#20110;&#20195;&#29702;&#21487;&#20197;&#30452;&#25509;&#19982;&#29289;&#29702;&#29615;&#22659;&#20132;&#20114;&#65292;&#20854;&#21487;&#38752;&#24615;&#21644;&#23433;&#20840;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#29702;&#26500;&#25104;&#30340;&#20195;&#29702;&#26694;&#26550;TrustAgent&#65292;&#23545;LLM&#20195;&#29702;&#30340;&#23433;&#20840;&#24615;&#32500;&#24230;&#36827;&#34892;&#20102;&#21021;&#27493;&#30740;&#31350;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#19977;&#31181;&#31574;&#30053;&#65306;&#39044;&#20808;&#35268;&#21010;&#31574;&#30053;&#65292;&#22312;&#29983;&#25104;&#35745;&#21010;&#20043;&#21069;&#21521;&#27169;&#22411;&#27880;&#20837;&#23433;&#20840;&#30693;&#35782;&#65307;&#35268;&#21010;&#36807;&#31243;&#20013;&#31574;&#30053;&#65292;&#22312;&#29983;&#25104;&#35745;&#21010;&#26102;&#22686;&#24378;&#23433;&#20840;&#24615;&#65307;&#35745;&#21010;&#21518;&#26816;&#26597;&#31574;&#30053;&#65292;&#36890;&#36807;&#35745;&#21010;&#21518;&#26816;&#26597;&#30830;&#20445;&#23433;&#20840;&#24615;&#12290;&#36890;&#36807;&#23454;&#39564;&#20998;&#26512;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#26041;&#27861;&#22914;&#20309;&#36890;&#36807;&#35782;&#21035;&#21644;&#39044;&#38450;&#28508;&#22312;&#21361;&#38505;&#26377;&#25928;&#25552;&#39640;LLM&#20195;&#29702;&#30340;&#23433;&#20840;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#23433;&#20840;&#24615;&#19982;&#20351;&#29992;&#32773;&#28385;&#24847;&#24230;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#65292;&#20197;&#21450;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#19982;&#20854;&#25928;&#29575;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of LLM-based agents has garnered considerable attention, yet their trustworthiness remains an under-explored area. As agents can directly interact with the physical environment, their reliability and safety is critical. This paper presents an Agent-Constitution-based agent framework, TrustAgent, an initial investigation into improving the safety dimension of trustworthiness in LLM-based agents. This framework consists of threefold strategies: pre-planning strategy which injects safety knowledge to the model prior to plan generation, in-planning strategy which bolsters safety during plan generation, and post-planning strategy which ensures safety by post-planning inspection. Through experimental analysis, we demonstrate how these approaches can effectively elevate an LLM agent's safety by identifying and preventing potential dangers. Furthermore, we explore the intricate relationships between safety and helpfulness, and between the model's reasoning ability and its efficac
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24314;&#31435;&#21644;&#20998;&#26512;&#22240;&#26524;&#29109;&#21644;&#22240;&#26524;&#20449;&#24687;&#22686;&#30410;&#30340;&#22522;&#26412;&#24615;&#36136;&#65292;&#21253;&#25324;&#30028;&#38480;&#21644;&#38142;&#35268;&#21017;&#65292;&#38416;&#26126;&#20102;&#22240;&#26524;&#29109;&#19982;&#38543;&#26426;&#24178;&#39044;&#30340;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#22240;&#26524;&#26465;&#20214;&#29109;&#21644;&#22240;&#26524;&#26465;&#20214;&#20449;&#24687;&#22686;&#30410;&#30340;&#23450;&#20041;&#65292;&#20026;&#25552;&#21319;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01341</link><description>&lt;p&gt;
&#22240;&#26524;&#29109;&#21644;&#20449;&#24687;&#22686;&#30410;&#30340;&#22522;&#26412;&#24615;&#36136;
&lt;/p&gt;
&lt;p&gt;
Fundamental Properties of Causal Entropy and Information Gain
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01341
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24314;&#31435;&#21644;&#20998;&#26512;&#22240;&#26524;&#29109;&#21644;&#22240;&#26524;&#20449;&#24687;&#22686;&#30410;&#30340;&#22522;&#26412;&#24615;&#36136;&#65292;&#21253;&#25324;&#30028;&#38480;&#21644;&#38142;&#35268;&#21017;&#65292;&#38416;&#26126;&#20102;&#22240;&#26524;&#29109;&#19982;&#38543;&#26426;&#24178;&#39044;&#30340;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#22240;&#26524;&#26465;&#20214;&#29109;&#21644;&#22240;&#26524;&#26465;&#20214;&#20449;&#24687;&#22686;&#30410;&#30340;&#23450;&#20041;&#65292;&#20026;&#25552;&#21319;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#21457;&#23637;&#20351;&#24471;&#33021;&#22815;&#37327;&#21270;&#22312;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;(SCM)&#19979;&#30340;&#22240;&#26524;&#25511;&#21046;&#12290;&#36825;&#26159;&#36890;&#36807;&#24341;&#20837;&#19968;&#20123;&#37327;&#26469;&#32534;&#30721;&#22312;&#24178;&#39044;&#21478;&#19968;&#20010;&#21464;&#37327;&#26102;&#26576;&#20010;&#21464;&#37327;&#29109;&#30340;&#21464;&#21270;&#26469;&#23454;&#29616;&#30340;&#12290;&#36825;&#20123;&#37327;&#34987;&#21629;&#21517;&#20026;&#22240;&#26524;&#29109;&#21644;&#22240;&#26524;&#20449;&#24687;&#22686;&#30410;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#20449;&#24687;&#35770;&#26041;&#27861;&#22312;&#22240;&#26524;&#24615;&#22312;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#36215;&#20851;&#38190;&#20316;&#29992;&#26102;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36890;&#36807;&#24314;&#31435;&#21644;&#20998;&#26512;&#36825;&#20123;&#27010;&#24565;&#30340;&#22522;&#26412;&#24615;&#36136;&#65292;&#21253;&#25324;&#30028;&#38480;&#21644;&#38142;&#35268;&#21017;&#65292;&#23545;&#22240;&#26524;&#29109;&#21644;&#22240;&#26524;&#20449;&#24687;&#22686;&#30410;&#30340;&#27010;&#24565;&#36827;&#34892;&#20102;&#24418;&#24335;&#19978;&#30340;&#29702;&#35299;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#38416;&#26126;&#20102;&#22240;&#26524;&#29109;&#19982;&#38543;&#26426;&#24178;&#39044;&#30340;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#22240;&#26524;&#26465;&#20214;&#29109;&#21644;&#22240;&#26524;&#26465;&#20214;&#20449;&#24687;&#22686;&#30410;&#30340;&#23450;&#20041;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#36825;&#20010;&#25506;&#32034;&#20026;&#25552;&#21319;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent developments enable the quantification of causal control given a structural causal model (SCM). This has been accomplished by introducing quantities which encode changes in the entropy of one variable when intervening on another. These measures, named causal entropy and causal information gain, aim to address limitations in existing information theoretical approaches for machine learning tasks where causality plays a crucial role. They have not yet been properly mathematically studied. Our research contributes to the formal understanding of the notions of causal entropy and causal information gain by establishing and analyzing fundamental properties of these concepts, including bounds and chain rules. Furthermore, we elucidate the relationship between causal entropy and stochastic interventions. We also propose definitions for causal conditional entropy and causal conditional information gain. Overall, this exploration paves the way for enhancing causal machine learning tasks th
&lt;/p&gt;</description></item><item><title>&#30123;&#33495;&#26159;&#19968;&#31181;&#38024;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#24178;&#25200;&#24863;&#30693;&#23545;&#40784;&#25216;&#26415;&#65292;&#36890;&#36807;&#36880;&#28176;&#28155;&#21152;&#25200;&#21160;&#20135;&#29983;&#19981;&#21464;&#30340;&#38544;&#34255;&#23884;&#20837;&#65292;&#25552;&#39640;&#23545;&#25239;&#26377;&#23475;&#25552;&#31034;&#24341;&#36215;&#30340;&#23884;&#20837;&#28418;&#31227;&#30340;&#23545;&#40784;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#20445;&#30041;&#23545;&#33391;&#24615;&#25552;&#31034;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01109</link><description>&lt;p&gt;
&#30123;&#33495;&#65306;&#38024;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#24178;&#25200;&#24863;&#30693;&#23545;&#40784;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Vaccine: Perturbation-aware Alignment for Large Language Model
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01109
&lt;/p&gt;
&lt;p&gt;
&#30123;&#33495;&#26159;&#19968;&#31181;&#38024;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#24178;&#25200;&#24863;&#30693;&#23545;&#40784;&#25216;&#26415;&#65292;&#36890;&#36807;&#36880;&#28176;&#28155;&#21152;&#25200;&#21160;&#20135;&#29983;&#19981;&#21464;&#30340;&#38544;&#34255;&#23884;&#20837;&#65292;&#25552;&#39640;&#23545;&#25239;&#26377;&#23475;&#25552;&#31034;&#24341;&#36215;&#30340;&#23884;&#20837;&#28418;&#31227;&#30340;&#23545;&#40784;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#20445;&#30041;&#23545;&#33391;&#24615;&#25552;&#31034;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#24494;&#35843;&#21363;&#26381;&#21153;&#33539; paradigm&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#20026;&#29992;&#25143;&#19978;&#20256;&#30340;&#19968;&#23567;&#37096;&#20998;&#26377;&#23475;&#25968;&#25454;&#25552;&#20379;&#20102;&#26032;&#30340;&#25915;&#20987;&#38754;&#65292;&#36825;&#20123;&#25968;&#25454;&#24456;&#23481;&#26131;&#27450;&#39575;&#24494;&#35843;&#36807;&#31243;&#20174;&#32780;&#20135;&#29983;&#23545;&#40784;&#22833;&#25928;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#19968;&#31181;&#21487;&#33021;&#23548;&#33268;&#23545;&#40784;&#22833;&#25928;&#30340;&#26377;&#23475;&#23884;&#20837;&#28418;&#31227;&#29616;&#35937;&#12290;&#21463;&#21040;&#25105;&#20204;&#30340;&#21457;&#29616;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#30123;&#33495; (Vaccine) &#65292;&#19968;&#31181;&#38024;&#23545;&#24178;&#25200;&#24863;&#30693;&#30340;&#23545;&#40784;&#25216;&#26415;&#65292;&#20197;&#20943;&#36731;&#29992;&#25143;&#24494;&#35843;&#30340;&#23433;&#20840;&#39118;&#38505;&#12290;&#30123;&#33495;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#36890;&#36807;&#22312;&#23545;&#40784;&#38454;&#27573;&#36880;&#28176;&#28155;&#21152;&#31934;&#24515;&#35774;&#35745;&#30340;&#25200;&#21160;&#65292;&#20135;&#29983;&#19981;&#21464;&#30340;&#38544;&#34255;&#23884;&#20837;&#65292;&#20174;&#32780;&#20351;&#23884;&#20837;&#33021;&#22815;&#25269;&#24481;&#26469;&#33258;&#26410;&#32463;&#28040;&#27602;&#30340;&#29992;&#25143;&#25968;&#25454;&#30340;&#26377;&#23475;&#25200;&#21160;&#12290;&#25105;&#20204;&#22312;&#24320;&#28304;&#20027;&#27969;LLM&#65288;&#22914;Llama2&#65292;Opt&#65292;Vicuna&#65289;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30123;&#33495;&#33021;&#22815;&#25552;&#39640;&#23545;&#25239;&#26377;&#23475;&#25552;&#31034;&#24341;&#36215;&#30340;&#23884;&#20837;&#28418;&#31227;&#30340;&#23545;&#40784;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#20445;&#30041;&#23545;&#33391;&#24615;&#25552;&#31034;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The new paradigm of finetuning-as-a-service introduces a new attack surface for Large Language Models (LLMs): a few harmful data uploaded by users can easily trick the finetuning to produce an alignment-broken model. We conduct an empirical analysis and uncover a \textit{harmful embedding drift} phenomenon, showing a probable cause of the alignment-broken effect. Inspired by our findings, we propose Vaccine, a perturbation-aware alignment technique to mitigate the security risk of users finetuning. The core idea of Vaccine is to produce invariant hidden embeddings by progressively adding crafted perturbation to them in the alignment phase. This enables the embeddings to withstand harmful perturbation from un-sanitized user data in the finetuning phase. Our results on open source mainstream LLMs (e.g., Llama2, Opt, Vicuna) demonstrate that Vaccine can boost the robustness of alignment against harmful prompts induced embedding drift while reserving reasoning ability towards benign prompt
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#35782;&#21035;TESS&#20840;&#30011;&#24133;&#22270;&#20687;&#20809;&#21464;&#26354;&#32447;&#20013;&#30340;&#30701;&#21608;&#26399;&#21464;&#37327;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#29575;&#30340;&#22823;&#35268;&#27169;&#26723;&#26696;&#25628;&#32034;&#12290;</title><link>https://arxiv.org/abs/2402.12369</link><description>&lt;p&gt;
&#36890;&#36807;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#35782;&#21035;TESS&#20840;&#30011;&#24133;&#22270;&#20687;&#20809;&#21464;&#26354;&#32447;&#20013;&#30340;&#30701;&#21608;&#26399;&#21464;&#37327;
&lt;/p&gt;
&lt;p&gt;
Short-Period Variables in TESS Full-Frame Image Light Curves Identified via Convolutional Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12369
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#35782;&#21035;TESS&#20840;&#30011;&#24133;&#22270;&#20687;&#20809;&#21464;&#26354;&#32447;&#20013;&#30340;&#30701;&#21608;&#26399;&#21464;&#37327;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#29575;&#30340;&#22823;&#35268;&#27169;&#26723;&#26696;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12369v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449; &#25277;&#35937;&#65306;&#36807;&#22659;&#31995;&#22806;&#34892;&#26143;&#21208;&#27979;&#21355;&#26143;&#65288;TESS&#65289;&#20219;&#21153;&#22312;&#20854;&#20026;&#26399;&#20004;&#24180;&#30340;&#20027;&#35201;&#20219;&#21153;&#26399;&#38388;&#27979;&#37327;&#20102;&#32422;85%&#30340;&#22825;&#31354;&#20013;&#30340;&#24658;&#26143;&#20809;&#65292;&#23548;&#33268;&#25968;&#30334;&#19975;TESS 30&#20998;&#38047;&#38388;&#38548;&#20809;&#21464;&#26354;&#32447;&#29992;&#20110;&#25628;&#32034;&#36807;&#22659;&#31995;&#22806;&#34892;&#26143;&#12290;&#20026;&#20102;&#25628;&#32034;&#36825;&#19968;&#24040;&#22823;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#26088;&#22312;&#25552;&#20379;&#19968;&#31181;&#26082;&#20855;&#26377;&#35745;&#31639;&#25928;&#29575;&#12289;&#29983;&#20135;&#24615;&#39044;&#27979;&#24615;&#65292;&#21448;&#26368;&#22823;&#31243;&#24230;&#20943;&#23569;&#25152;&#38656;&#20154;&#31867;&#25628;&#32034;&#24037;&#20316;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#35782;&#21035;&#30701;&#21608;&#26399;&#21464;&#37327;&#12290;&#20026;&#20102;&#23545;&#32473;&#23450;&#30340;&#20809;&#21464;&#26354;&#32447;&#36827;&#34892;&#39044;&#27979;&#65292;&#25105;&#20204;&#30340;&#32593;&#32476;&#19981;&#38656;&#35201;&#20351;&#29992;&#20854;&#20182;&#26041;&#27861;&#30830;&#23450;&#30340;&#20808;&#21069;&#30446;&#26631;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#32593;&#32476;&#22312;&#21333;&#20010;GPU&#19978;&#23545;TESS 30&#20998;&#38047;&#38388;&#38548;&#20809;&#21464;&#26354;&#32447;&#36827;&#34892;&#25512;&#26029;&#32422;5ms&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22823;&#35268;&#27169;&#30340;&#26723;&#26696;&#25628;&#32034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;14156&#20010;&#30001;&#25105;&#20204;&#30340;&#32593;&#32476;&#35782;&#21035;&#20986;&#30340;&#30701;&#21608;&#26399;&#21464;&#37327;&#30340;&#38598;&#21512;&#12290;&#25105;&#20204;&#35782;&#21035;&#30340;&#22823;&#22810;&#25968;&#21464;&#37327;&#23646;&#20110;&#20004;&#20010;&#31361;&#20986;&#30340;&#20154;&#32676;&#20043;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12369v1 Announce Type: cross  Abstract: The Transiting Exoplanet Survey Satellite (TESS) mission measured light from stars in ~85% of the sky throughout its two-year primary mission, resulting in millions of TESS 30-minute cadence light curves to analyze in the search for transiting exoplanets. To search this vast dataset, we aim to provide an approach that is both computationally efficient, produces highly performant predictions, and minimizes the required human search effort. We present a convolutional neural network that we train to identify short period variables. To make a prediction for a given light curve, our network requires no prior target parameters identified using other methods. Our network performs inference on a TESS 30-minute cadence light curve in ~5ms on a single GPU, enabling large scale archival searches. We present a collection of 14156 short-period variables identified by our network. The majority of our identified variables fall into two prominent popu
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#36136;&#30097;&#22797;&#26434;&#30340;&#24378;&#21270;&#23398;&#20064;&#22312;AI&#21453;&#39304;&#20013;&#30340;&#24517;&#35201;&#24615;&#65292;&#34920;&#26126;&#20351;&#29992;&#26356;&#24378;&#30340;&#25945;&#24072;&#27169;&#22411;&#36827;&#34892;&#30417;&#30563;&#24494;&#35843;&#21487;&#20197;&#36229;&#36234;&#29616;&#26377;&#30340;RLAIF&#31649;&#36947;&#12290;</title><link>https://arxiv.org/abs/2402.12366</link><description>&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;AI&#21453;&#39304;&#30340;&#20851;&#38190;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Critical Evaluation of AI Feedback for Aligning Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12366
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#36136;&#30097;&#22797;&#26434;&#30340;&#24378;&#21270;&#23398;&#20064;&#22312;AI&#21453;&#39304;&#20013;&#30340;&#24517;&#35201;&#24615;&#65292;&#34920;&#26126;&#20351;&#29992;&#26356;&#24378;&#30340;&#25945;&#24072;&#27169;&#22411;&#36827;&#34892;&#30417;&#30563;&#24494;&#35843;&#21487;&#20197;&#36229;&#36234;&#29616;&#26377;&#30340;RLAIF&#31649;&#36947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#19982;AI&#21453;&#39304;&#65288;RLAIF&#65289;&#26159;&#19968;&#31181;&#29992;&#20110;&#25552;&#39640;&#24378;&#22823;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#30340;&#27969;&#34892;&#33539;&#24335;&#12290; RLAIF&#39318;&#20808;&#20351;&#29992;&#26469;&#33258;&#25945;&#24072;&#27169;&#22411;&#30340;&#31034;&#33539;&#36827;&#34892;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#65292;&#28982;&#21518;&#20877;&#20351;&#29992;&#26469;&#33258;&#35780;&#35770;&#27169;&#22411;&#30340;&#21453;&#39304;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#36827;&#19968;&#27493;&#24494;&#35843;&#27169;&#22411;&#12290;&#23613;&#31649;&#26368;&#36817;&#27969;&#34892;&#30340;&#24320;&#28304;&#27169;&#22411;&#24050;&#32463;&#35777;&#26126;&#20102;&#20174;RL&#27493;&#39588;&#20013;&#33719;&#24471;&#30340;&#24615;&#33021;&#26174;&#30528;&#25552;&#39640;&#65292;&#20294;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36136;&#30097;&#26159;&#21542;&#22797;&#26434;&#30340;RL&#27493;&#39588;&#30495;&#27491;&#26377;&#24517;&#35201;&#20026;AI&#21453;&#39304;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;RL&#27493;&#39588;&#30340;&#25913;&#36827;&#20960;&#20046;&#23436;&#20840;&#26159;&#22240;&#20026;&#20351;&#29992;&#36739;&#24369;&#30340;&#25945;&#24072;&#27169;&#22411;&#65288;&#20363;&#22914;GPT-3.5&#65289;&#29992;&#20110;SFT&#25968;&#25454;&#25910;&#38598;&#32780;&#19981;&#26159;&#29992;&#20110;AI&#21453;&#39304;&#29983;&#25104;&#30340;&#35780;&#35770;&#32773;&#65288;&#20363;&#22914;GPT-4&#65289;&#30340;&#24191;&#27867;&#23454;&#36341;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#31616;&#21333;&#30340;&#20197;GPT-4&#20316;&#20026;&#25945;&#24072;&#30340;&#30417;&#30563;&#24494;&#35843;&#20248;&#20110;&#29616;&#26377;&#30340;RLAIF&#31649;&#36947;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12366v1 Announce Type: cross  Abstract: Reinforcement learning with AI feedback (RLAIF) is a popular paradigm for improving the instruction-following abilities of powerful pre-trained language models. RLAIF first performs supervised fine-tuning (SFT) using demonstrations from a teacher model and then further fine-tunes the model with reinforcement learning (RL), using feedback from a critic model. While recent popular open-source models have demonstrated substantial improvements in performance from the RL step, in this paper we question whether the complexity of this RL step is truly warranted for AI feedback. We show that the improvements of the RL step are virtually entirely due to the widespread practice of using a weaker teacher model (e.g. GPT-3.5) for SFT data collection than the critic (e.g., GPT-4) used for AI feedback generation. Specifically, we show that simple supervised fine-tuning with GPT-4 as the teacher outperforms existing RLAIF pipelines. More generally, w
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#36890;&#29992;&#29289;&#29702;&#21464;&#21387;&#22120;&#65288;UPTs&#65289;&#36825;&#19968;&#26032;&#39062;&#23398;&#20064;&#33539;&#24335;&#65292;&#33021;&#22815;&#27169;&#25311;&#24191;&#27867;&#30340;&#26102;&#31354;&#38382;&#39064;&#65292;&#21516;&#26102;&#36866;&#29992;&#20110;&#25289;&#26684;&#26391;&#26085;&#21644;&#27431;&#25289;&#31163;&#25955;&#21270;&#26041;&#26696;&#65292;&#26377;&#25928;&#22320;&#20256;&#25773;&#21160;&#24577;&#24182;&#20801;&#35768;&#26597;&#35810;&#28508;&#22312;&#31354;&#38388;</title><link>https://arxiv.org/abs/2402.12365</link><description>&lt;p&gt;
&#36890;&#29992;&#29289;&#29702;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
Universal Physics Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12365
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#36890;&#29992;&#29289;&#29702;&#21464;&#21387;&#22120;&#65288;UPTs&#65289;&#36825;&#19968;&#26032;&#39062;&#23398;&#20064;&#33539;&#24335;&#65292;&#33021;&#22815;&#27169;&#25311;&#24191;&#27867;&#30340;&#26102;&#31354;&#38382;&#39064;&#65292;&#21516;&#26102;&#36866;&#29992;&#20110;&#25289;&#26684;&#26391;&#26085;&#21644;&#27431;&#25289;&#31163;&#25955;&#21270;&#26041;&#26696;&#65292;&#26377;&#25928;&#22320;&#20256;&#25773;&#21160;&#24577;&#24182;&#20801;&#35768;&#26597;&#35810;&#28508;&#22312;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#26367;&#20195;&#32773;&#36817;&#26469;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#31867;&#20284;&#20110;&#23427;&#20204;&#30340;&#25968;&#20540;&#23545;&#24212;&#29289;&#65292;&#22312;&#19981;&#21516;&#24212;&#29992;&#20013;&#20351;&#29992;&#19981;&#21516;&#30340;&#25216;&#26415;&#65292;&#21363;&#20351;&#31995;&#32479;&#30340;&#22522;&#30784;&#21160;&#24577;&#30456;&#20284;&#12290;&#19968;&#20010;&#33879;&#21517;&#30340;&#20363;&#23376;&#26159;&#22312;&#35745;&#31639;&#27969;&#20307;&#21160;&#21147;&#23398;&#20013;&#30340;&#25289;&#26684;&#26391;&#26085;&#21644;&#27431;&#25289;&#34920;&#36848;&#65292;&#36825;&#20026;&#31070;&#32463;&#32593;&#32476;&#26377;&#25928;&#22320;&#24314;&#27169;&#22522;&#20110;&#31890;&#23376;&#32780;&#19981;&#26159;&#32593;&#26684;&#30340;&#21160;&#24577;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#36890;&#29992;&#29289;&#29702;&#21464;&#21387;&#22120;&#65288;UPTs&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#23427;&#27169;&#25311;&#20102;&#19968;&#31995;&#21015;&#26102;&#31354;&#38382;&#39064; - &#23545;&#25289;&#26684;&#26391;&#26085;&#21644;&#27431;&#25289;&#31163;&#25955;&#21270;&#26041;&#26696;&#12290;UPTs&#22312;&#27809;&#26377;&#22522;&#20110;&#32593;&#26684;&#25110;&#22522;&#20110;&#31890;&#23376;&#30340;&#28508;&#22312;&#32467;&#26500;&#30340;&#24773;&#20917;&#19979;&#36816;&#34892;&#65292;&#20174;&#32780;&#22312;&#32593;&#26684;&#21644;&#31890;&#23376;&#20043;&#38388;&#23454;&#29616;&#20102;&#28789;&#27963;&#24615;&#12290;UPTs&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#39640;&#25928;&#20256;&#25773;&#21160;&#24577;&#65292;&#24378;&#35843;&#20102;&#36870;&#32534;&#30721;&#21644;&#35299;&#30721;&#25216;&#26415;&#12290;&#26368;&#21518;&#65292;UPTs&#20801;&#35768;&#26597;&#35810;&#28508;&#22312;&#31354;&#38388;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12365v1 Announce Type: cross  Abstract: Deep neural network based surrogates for partial differential equations have recently gained increased interest. However, akin to their numerical counterparts, different techniques are used across applications, even if the underlying dynamics of the systems are similar. A prominent example is the Lagrangian and Eulerian specification in computational fluid dynamics, posing a challenge for neural networks to effectively model particle- as opposed to grid-based dynamics. We introduce Universal Physics Transformers (UPTs), a novel learning paradigm which models a wide range of spatio-temporal problems - both for Lagrangian and Eulerian discretization schemes. UPTs operate without grid- or particle-based latent structures, enabling flexibility across meshes and particles. UPTs efficiently propagate dynamics in the latent space, emphasized by inverse encoding and decoding techniques. Finally, UPTs allow for queries of the latent space repre
&lt;/p&gt;</description></item><item><title>LoRA+&#36890;&#36807;&#35774;&#32622;&#19981;&#21516;&#30340;&#23398;&#20064;&#29575;&#26469;&#25913;&#36827;&#21407;&#22987;LoRA&#30340;&#20302;&#25928;&#29575;&#38382;&#39064;&#65292;&#22312;&#20445;&#25345;&#35745;&#31639;&#25104;&#26412;&#19981;&#21464;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#21644;&#24494;&#35843;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.12354</link><description>&lt;p&gt;
LoRA+: &#22823;&#35268;&#27169;&#27169;&#22411;&#30340;&#39640;&#25928;&#20302;&#31209;&#36866;&#24212;&#24615;
&lt;/p&gt;
&lt;p&gt;
LoRA+: Efficient Low Rank Adaptation of Large Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12354
&lt;/p&gt;
&lt;p&gt;
LoRA+&#36890;&#36807;&#35774;&#32622;&#19981;&#21516;&#30340;&#23398;&#20064;&#29575;&#26469;&#25913;&#36827;&#21407;&#22987;LoRA&#30340;&#20302;&#25928;&#29575;&#38382;&#39064;&#65292;&#22312;&#20445;&#25345;&#35745;&#31639;&#25104;&#26412;&#19981;&#21464;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#21644;&#24494;&#35843;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#26368;&#21021;&#30001;&#32993;&#31561;&#20154;&#65288;2021&#24180;&#65289;&#24341;&#20837;&#65292;&#23548;&#33268;&#23545;&#20855;&#26377;&#22823;&#23485;&#24230;&#65288;&#23884;&#20837;&#32500;&#24230;&#65289;&#30340;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#26102;&#34920;&#29616;&#20122;&#20248;&#12290;&#36825;&#26159;&#22240;&#20026;LoRA&#20013;&#30340;&#36866;&#37197;&#22120;&#30697;&#38453;A&#21644;B&#20351;&#29992;&#30456;&#21516;&#30340;&#23398;&#20064;&#29575;&#36827;&#34892;&#26356;&#26032;&#12290;&#36890;&#36807;&#23545;&#22823;&#23485;&#24230;&#32593;&#32476;&#36827;&#34892;&#32553;&#25918;&#21442;&#25968;&#30340;&#35770;&#35777;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#36866;&#37197;&#22120;&#30697;&#38453;A&#21644;B&#20351;&#29992;&#30456;&#21516;&#30340;&#23398;&#20064;&#29575;&#19981;&#21033;&#20110;&#26377;&#25928;&#30340;&#29305;&#24449;&#23398;&#20064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#34920;&#26126;LoRA&#30340;&#36825;&#31181;&#27425;&#20248;&#24615;&#21487;&#20197;&#31616;&#21333;&#22320;&#36890;&#36807;&#20026;LoRA&#36866;&#37197;&#22120;&#30697;&#38453;A&#21644;B&#35774;&#32622;&#19981;&#21516;&#30340;&#23398;&#20064;&#29575;&#20197;&#21450;&#19968;&#20010;&#31934;&#24515;&#36873;&#25321;&#30340;&#27604;&#29575;&#26469;&#36827;&#34892;&#26657;&#27491;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#25552;&#20986;&#30340;&#31639;&#27861;&#31216;&#20026;LoRA$+$&#12290;&#22312;&#25105;&#20204;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#20013;&#65292;LoRA$+$&#22312;&#30456;&#21516;&#35745;&#31639;&#25104;&#26412;&#19979;&#25552;&#39640;&#20102;&#24615;&#33021;&#65288;1-2&#65285;&#30340;&#25913;&#36827;&#65289;&#21644;&#24494;&#35843;&#36895;&#24230;&#65288;&#26368;&#22810;&#25552;&#36895;&#32422;2&#20493;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12354v1 Announce Type: cross  Abstract: In this paper, we show that Low Rank Adaptation (LoRA) as originally introduced in Hu et al. (2021) leads to suboptimal finetuning of models with large width (embedding dimension). This is due to the fact that adapter matrices A and B in LoRA are updated with the same learning rate. Using scaling arguments for large width networks, we demonstrate that using the same learning rate for A and B does not allow efficient feature learning. We then show that this suboptimality of LoRA can be corrected simply by setting different learning rates for the LoRA adapter matrices A and B with a well-chosen ratio. We call this proposed algorithm LoRA$+$. In our extensive experiments, LoRA$+$ improves performance (1-2 $\%$ improvements) and finetuning speed (up to $\sim$ 2X SpeedUp), at the same computational cost as LoRA.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#21338;&#24328;&#35770;&#20219;&#21153;&#35780;&#20272;&#20102;LLMs&#22312;&#31454;&#20105;&#29615;&#22659;&#20013;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#35266;&#23519;&#21040;LLMs&#22312;&#19981;&#21516;&#28216;&#25103;&#22330;&#26223;&#19979;&#34920;&#29616;&#20986;&#19981;&#21516;&#34892;&#20026;&#65292;&#20855;&#26377;&#37325;&#35201;&#30340;&#25112;&#30053;&#25512;&#29702;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.12348</link><description>&lt;p&gt;
&#36890;&#36807;&#21338;&#24328;&#35770;&#35780;&#20272;&#25581;&#31034;LLM&#30340;&#25112;&#30053;&#25512;&#29702;&#23616;&#38480;&#24615;&#30340;GTBench
&lt;/p&gt;
&lt;p&gt;
GTBench: Uncovering the Strategic Reasoning Limitations of LLMs via Game-Theoretic Evaluations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12348
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#21338;&#24328;&#35770;&#20219;&#21153;&#35780;&#20272;&#20102;LLMs&#22312;&#31454;&#20105;&#29615;&#22659;&#20013;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#35266;&#23519;&#21040;LLMs&#22312;&#19981;&#21516;&#28216;&#25103;&#22330;&#26223;&#19979;&#34920;&#29616;&#20986;&#19981;&#21516;&#34892;&#20026;&#65292;&#20855;&#26377;&#37325;&#35201;&#30340;&#25112;&#30053;&#25512;&#29702;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34987;&#25972;&#21512;&#21040;&#20851;&#38190;&#30340;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#65292;&#23427;&#20204;&#30340;&#25112;&#30053;&#21644;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#21464;&#24471;&#36234;&#26469;&#36234;&#20851;&#38190;&#12290;&#26412;&#25991;&#36890;&#36807;&#21338;&#24328;&#35770;&#20219;&#21153;&#35780;&#20272;LLMs&#22312;&#31454;&#20105;&#29615;&#22659;&#20013;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20363;&#22914;&#65292;&#38656;&#35201;&#32431;&#36923;&#36753;&#21644;&#25112;&#30053;&#25512;&#29702;&#26469;&#19982;&#23545;&#25163;&#31454;&#20105;&#30340;&#26827;&#30424;&#28216;&#25103;&#21644;&#32440;&#29260;&#28216;&#25103;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;GTBench&#65292;&#36825;&#26159;&#19968;&#20010;&#20197;&#35821;&#35328;&#39537;&#21160;&#30340;&#29615;&#22659;&#65292;&#21253;&#25324;10&#20010;&#24191;&#27867;&#35748;&#21487;&#30340;&#20219;&#21153;&#65292;&#28085;&#30422;&#20102;&#20840;&#38754;&#30340;&#28216;&#25103;&#20998;&#31867;&#27861;&#65306;&#23436;&#25972;&#20449;&#24687;&#19982;&#19981;&#23436;&#25972;&#20449;&#24687;&#65292;&#21160;&#24577;&#19982;&#38745;&#24577;&#65292;&#20197;&#21450;&#27010;&#29575;&#19982;&#30830;&#23450;&#24615;&#22330;&#26223;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#65288;1&#65289;&#34920;&#24449;LLMs&#30340;&#21338;&#24328;&#35770;&#25512;&#29702;&#65307;&#65288;2&#65289;LLM&#23545;&#25239;LLM&#30340;&#27604;&#36187;&#20316;&#20026;&#25512;&#29702;&#35780;&#20272;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65288;1&#65289;LLMs&#22312;&#21508;&#31181;&#28216;&#25103;&#22330;&#26223;&#19979;&#26377;&#19981;&#21516;&#30340;&#34892;&#20026;&#65307;&#20363;&#22914;&#65292;LLMs&#22312;&#23436;&#25972;&#21644;&#30830;&#23450;&#24615;&#28216;&#25103;&#20013;&#22833;&#36133;&#65292;&#20294;&#23427;&#20204;&#22312;&#27010;&#29575;&#28216;&#25103;&#20013;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12348v1 Announce Type: cross  Abstract: As Large Language Models (LLMs) are integrated into critical real-world applications, their strategic and logical reasoning abilities are increasingly crucial. This paper evaluates LLMs' reasoning abilities in competitive environments through game-theoretic tasks, e.g., board and card games that require pure logic and strategic reasoning to compete with opponents. We first propose GTBench, a language-driven environment composing 10 widely-recognized tasks, across a comprehensive game taxonomy: complete versus incomplete information, dynamic versus static, and probabilistic versus deterministic scenarios. Then, we investigate two key problems: (1) Characterizing game-theoretic reasoning of LLMs; (2) LLM-vs-LLM competitions as reasoning evaluation. We observe that (1) LLMs have distinct behaviors regarding various gaming scenarios; for example, LLMs fail in complete and deterministic games yet they are competitive in probabilistic gaming
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#30340;&#27169;&#24577;&#20998;&#35299;&#26041;&#27861;&#29992;&#20110;&#25552;&#21462;&#20107;&#20214;&#20998;&#31867;&#29305;&#24449;&#65292;&#24182;&#35780;&#20272;&#20102;&#21487;&#35299;&#37322;&#20998;&#31867;&#22120;&#22312;&#38754;&#23545;&#23545;&#25239;&#31639;&#27861;&#26102;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.12338</link><description>&lt;p&gt;
&#19968;&#31181;&#23545;&#20107;&#20214;&#35782;&#21035;&#27169;&#22411;&#40065;&#26834;&#24615;&#36827;&#34892;&#35780;&#20272;&#30340;&#23545;&#25239;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Adversarial Approach to Evaluating the Robustness of Event Identification Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12338
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#30340;&#27169;&#24577;&#20998;&#35299;&#26041;&#27861;&#29992;&#20110;&#25552;&#21462;&#20107;&#20214;&#20998;&#31867;&#29305;&#24449;&#65292;&#24182;&#35780;&#20272;&#20102;&#21487;&#35299;&#37322;&#20998;&#31867;&#22120;&#22312;&#38754;&#23545;&#23545;&#25239;&#31639;&#27861;&#26102;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#27491;&#22312;&#31215;&#26497;&#29992;&#20110;&#20107;&#20214;&#26816;&#27979;&#21644;&#35782;&#21035;&#65292;&#21487;&#23454;&#26102;&#33719;&#24471;&#24577;&#21183;&#24863;&#30693;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#24050;&#34987;&#35777;&#26126;&#23481;&#26131;&#21463;&#21040;&#23545;&#20256;&#20837;&#36965;&#27979;&#25968;&#25454;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#30340;&#27169;&#24577;&#20998;&#35299;&#26041;&#27861;&#26469;&#25552;&#21462;&#20107;&#20214;&#20998;&#31867;&#30340;&#29305;&#24449;&#65292;&#24182;&#19987;&#27880;&#20110;&#21487;&#35299;&#37322;&#30340;&#20998;&#31867;&#22120;&#65292;&#21253;&#25324;&#36923;&#36753;&#22238;&#24402;&#21644;&#26799;&#24230;&#25552;&#21319;&#65292;&#20197;&#21306;&#20998;&#20004;&#31181;&#31867;&#22411;&#30340;&#20107;&#20214;&#65306;&#36127;&#36733;&#25439;&#22833;&#21644;&#21457;&#30005;&#25439;&#22833;&#12290;&#28982;&#21518;&#65292;&#23545;&#29983;&#25104;&#30340;&#20998;&#31867;&#22120;&#36827;&#34892;&#23545;&#25239;&#31639;&#27861;&#27979;&#35797;&#20197;&#35780;&#20272;&#20854;&#40065;&#26834;&#24615;&#12290;&#23545;&#25239;&#25915;&#20987;&#22312;&#20004;&#31181;&#24773;&#22659;&#19979;&#36827;&#34892;&#27979;&#35797;&#65306;&#30333;&#30418;&#35774;&#32622;&#65292;&#25915;&#20987;&#32773;&#23436;&#20840;&#20102;&#35299;&#20998;&#31867;&#27169;&#22411;&#65307;&#28784;&#30418;&#35774;&#32622;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#35775;&#38382;&#19982;&#29992;&#20110;&#35757;&#32451;&#20998;&#31867;&#22120;&#30340;&#30456;&#21516;&#32593;&#32476;&#30340;&#21382;&#21490;&#25968;&#25454;&#65292;&#20294;&#19981;&#30693;&#36947;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12338v1 Announce Type: cross  Abstract: Intelligent machine learning approaches are finding active use for event detection and identification that allow real-time situational awareness. Yet, such machine learning algorithms have been shown to be susceptible to adversarial attacks on the incoming telemetry data. This paper considers a physics-based modal decomposition method to extract features for event classification and focuses on interpretable classifiers including logistic regression and gradient boosting to distinguish two types of events: load loss and generation loss. The resulting classifiers are then tested against an adversarial algorithm to evaluate their robustness. The adversarial attack is tested in two settings: the white box setting, wherein the attacker knows exactly the classification model; and the gray box setting, wherein the attacker has access to historical data from the same network as was used to train the classifier, but does not know the classifica
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26080;&#30417;&#30563;&#23545;&#25239;&#24494;&#35843;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#22823;&#30340;CLIP&#35270;&#35273;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#22686;&#24378;&#21508;&#31181;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#24694;&#24847;&#31532;&#19977;&#26041;&#25552;&#20379;&#25805;&#32437;&#22270;&#20687;&#30340;&#29992;&#25143;&#38544;&#24418;&#25915;&#20987;&#24471;&#20197;&#26460;&#32477;&#12290;</title><link>https://arxiv.org/abs/2402.12336</link><description>&lt;p&gt;
Robust CLIP: &#23545;&#35270;&#35273;&#23884;&#20837;&#36827;&#34892;&#26080;&#30417;&#30563;&#23545;&#25239;&#24494;&#35843;&#20197;&#33719;&#24471;&#24378;&#22823;&#30340;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Robust CLIP: Unsupervised Adversarial Fine-Tuning of Vision Embeddings for Robust Large Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12336
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26080;&#30417;&#30563;&#23545;&#25239;&#24494;&#35843;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#22823;&#30340;CLIP&#35270;&#35273;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#22686;&#24378;&#21508;&#31181;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#24694;&#24847;&#31532;&#19977;&#26041;&#25552;&#20379;&#25805;&#32437;&#22270;&#20687;&#30340;&#29992;&#25143;&#38544;&#24418;&#25915;&#20987;&#24471;&#20197;&#26460;&#32477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35832;&#22914;OpenFlamingo&#12289;LLaVA&#21644;GPT-4&#20043;&#31867;&#30340;&#22810;&#27169;&#22411;&#22522;&#30784;&#27169;&#22411;&#36234;&#26469;&#36234;&#24191;&#27867;&#22320;&#29992;&#20110;&#21508;&#31181;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#35270;&#35273;&#27169;&#24577;&#19978;&#26497;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#25915;&#20987;&#21487;&#20197;&#29992;&#26469;&#20256;&#25773;&#34394;&#20551;&#20449;&#24687;&#25110;&#27450;&#39575;&#29992;&#25143;&#65292;&#22240;&#27492;&#26500;&#25104;&#20102;&#19968;&#20010;&#37325;&#22823;&#39118;&#38505;&#65292;&#36825;&#20351;&#24471;&#22823;&#22411;&#22810;&#27169;&#22411;&#22522;&#30784;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#25104;&#20026;&#19968;&#39033;&#32039;&#36843;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#23545;&#25239;&#24494;&#35843;&#26041;&#26696;&#65292;&#20197;&#33719;&#24471;&#24378;&#22823;&#30340;CLIP&#35270;&#35273;&#32534;&#30721;&#22120;&#65292;&#22312;&#25152;&#26377;&#20381;&#36182;&#20110;CLIP&#30340;&#35270;&#35273;&#19979;&#28216;&#20219;&#21153;&#65288;VLMs&#12289;&#38646;&#26679;&#26412;&#20998;&#31867;&#65289;&#19978;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#26086;&#26356;&#25442;&#21407;&#22987;&#30340;CLIP&#27169;&#22411;&#65292;&#29992;&#25143;&#22312;&#20351;&#29992;VLMs&#26102;&#20250;&#21463;&#21040;&#24694;&#24847;&#31532;&#19977;&#26041;&#25552;&#20379;&#30340;&#25805;&#32437;&#22270;&#20687;&#30340;&#28508;&#22312;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12336v1 Announce Type: cross  Abstract: Multi-modal foundation models like OpenFlamingo, LLaVA, and GPT-4 are increasingly used for various real-world tasks. Prior work has shown that these models are highly vulnerable to adversarial attacks on the vision modality. These attacks can be leveraged to spread fake information or defraud users, and thus pose a significant risk, which makes the robustness of large multi-modal foundation models a pressing problem. The CLIP model, or one of its variants, is used as a frozen vision encoder in many vision-language models (VLMs), e.g. LLaVA and OpenFlamingo. We propose an unsupervised adversarial fine-tuning scheme to obtain a robust CLIP vision encoder, which yields robustness on all vision down-stream tasks (VLMs, zero-shot classification) that rely on CLIP. In particular, we show that stealth-attacks on users of VLMs by a malicious third party providing manipulated images are no longer possible once one replaces the original CLIP mo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#29983;&#25104;&#29983;&#23384;&#36712;&#36857;&#21644;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#29305;&#23450;&#32467;&#26500;&#30340;&#33258;&#21160;&#32534;&#30721;&#22120;&#35299;&#20915;&#20102;&#39044;&#27979;&#12289;&#25968;&#25454;&#34917;&#20805;&#21644;&#29983;&#25104;&#21407;&#22411;&#26102;&#38388;&#30456;&#20851;&#36712;&#36857;&#31561;&#20219;&#21153;</title><link>https://arxiv.org/abs/2402.12331</link><description>&lt;p&gt;
&#29983;&#25104;&#29983;&#23384;&#21487;&#35299;&#37322;&#36712;&#36857;&#21644;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Generating Survival Interpretable Trajectories and Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12331
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#29983;&#25104;&#29983;&#23384;&#36712;&#36857;&#21644;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#29305;&#23450;&#32467;&#26500;&#30340;&#33258;&#21160;&#32534;&#30721;&#22120;&#35299;&#20915;&#20102;&#39044;&#27979;&#12289;&#25968;&#25454;&#34917;&#20805;&#21644;&#29983;&#25104;&#21407;&#22411;&#26102;&#38388;&#30456;&#20851;&#36712;&#36857;&#31561;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24212;&#29992;&#29305;&#23450;&#32467;&#26500;&#30340;&#33258;&#21160;&#32534;&#30721;&#22120;&#26469;&#29983;&#25104;&#29983;&#23384;&#36712;&#36857;&#21644;&#25968;&#25454;&#30340;&#26032;&#27169;&#22411;&#12290; &#23427;&#35299;&#20915;&#20102;&#19977;&#20010;&#20219;&#21153;&#12290; &#39318;&#20808;&#65292;&#23427;&#22522;&#20110;Beran&#20272;&#35745;&#22120;&#20026;&#26032;&#29983;&#25104;&#30340;&#29305;&#24449;&#21521;&#37327;&#25552;&#20379;&#20107;&#20214;&#26102;&#38388;&#30340;&#39044;&#27979;&#21644;&#29983;&#23384;&#20989;&#25968;&#30340;&#24418;&#24335;&#12290; &#31532;&#20108;&#65292;&#35813;&#27169;&#22411;&#22522;&#20110;&#32473;&#23450;&#30340;&#35757;&#32451;&#38598;&#29983;&#25104;&#39069;&#22806;&#25968;&#25454;&#65292;&#21487;&#20197;&#34917;&#20805;&#21407;&#22987;&#25968;&#25454;&#38598;&#12290; &#31532;&#19977;&#65292;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#23427;&#20026;&#23545;&#35937;&#29983;&#25104;&#20102;&#19968;&#20010;&#21407;&#22411;&#26102;&#38388;&#30456;&#20851;&#36712;&#36857;&#65292;&#25551;&#36848;&#20102;&#22914;&#20309;&#25913;&#21464;&#23545;&#35937;&#30340;&#29305;&#24449;&#20197;&#23454;&#29616;&#19981;&#21516;&#26102;&#38388;&#20107;&#20214;&#30340;&#26102;&#38388;&#12290; &#36712;&#36857;&#21487;&#20197;&#30475;&#20316;&#26159;&#19968;&#31181;&#21453;&#20107;&#23454;&#35299;&#37322;&#12290; &#30001;&#20110;&#23558;&#29305;&#23450;&#21152;&#26435;&#26041;&#26696;&#32435;&#20837;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#20013;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#20013;&#34920;&#29616;&#20986;&#40065;&#26834;&#24615;&#12290; &#35813;&#27169;&#22411;&#36824;&#36890;&#36807;&#35299;&#20915;&#20998;&#31867;&#38382;&#39064;&#30830;&#23450;&#20102;&#26032;&#29983;&#25104;&#25968;&#25454;&#30340;&#34987;&#23457;&#26597;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12331v1 Announce Type: cross  Abstract: A new model for generating survival trajectories and data based on applying an autoencoder of a specific structure is proposed. It solves three tasks. First, it provides predictions in the form of the expected event time and the survival function for a new generated feature vector on the basis of the Beran estimator. Second, the model generates additional data based on a given training set that would supplement the original dataset. Third, the most important, it generates a prototype time-dependent trajectory for an object, which characterizes how features of the object could be changed to achieve a different time to an event. The trajectory can be viewed as a type of the counterfactual explanation. The proposed model is robust during training and inference due to a specific weighting scheme incorporating into the variational autoencoder. The model also determines the censored indicators of new generated data by solving a classificatio
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26597;&#35810;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#36828;&#31243;&#35821;&#35328;&#27169;&#22411;&#30340; API &#35775;&#38382;&#26500;&#36896;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#20351;&#27169;&#22411;&#20197;&#26356;&#39640;&#27010;&#29575;&#21457;&#20986;&#26377;&#23475;&#23383;&#31526;&#20018;&#65292;&#32780;&#38750;&#20165;&#20165;&#22522;&#20110;&#27169;&#22411;&#20043;&#38388;&#30340;&#36716;&#31227;&#24615;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2402.12329</link><description>&lt;p&gt;
&#22522;&#20110;&#26597;&#35810;&#30340;&#23545;&#25239;&#24615;&#25552;&#31034;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Query-Based Adversarial Prompt Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12329
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26597;&#35810;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#36828;&#31243;&#35821;&#35328;&#27169;&#22411;&#30340; API &#35775;&#38382;&#26500;&#36896;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#20351;&#27169;&#22411;&#20197;&#26356;&#39640;&#27010;&#29575;&#21457;&#20986;&#26377;&#23475;&#23383;&#31526;&#20018;&#65292;&#32780;&#38750;&#20165;&#20165;&#22522;&#20110;&#27169;&#22411;&#20043;&#38388;&#30340;&#36716;&#31227;&#24615;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21487;&#20197;&#26500;&#36896;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#23548;&#33268;&#19968;&#20010;&#23545;&#20854;&#36827;&#34892;&#20102;&#35843;&#25972;&#30340;&#35821;&#35328;&#27169;&#22411;&#20135;&#29983;&#26377;&#23475;&#23383;&#31526;&#20018;&#25110;&#25191;&#34892;&#26377;&#23475;&#34892;&#20026;&#12290;&#29616;&#26377;&#30340;&#25915;&#20987;&#35201;&#20040;&#22312;&#30333;&#30418;&#35774;&#32622;&#20013;&#65288;&#23436;&#20840;&#35775;&#38382;&#27169;&#22411;&#26435;&#37325;&#65289;&#65292;&#35201;&#20040;&#36890;&#36807;&#21487;&#36716;&#31227;&#24615;&#65306;&#19968;&#31181;&#29616;&#35937;&#65292;&#21363;&#22312;&#19968;&#20010;&#27169;&#22411;&#19978;&#31934;&#24515;&#35774;&#35745;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#36890;&#24120;&#22312;&#20854;&#20182;&#27169;&#22411;&#19978;&#20173;&#28982;&#26377;&#25928;&#12290;&#25105;&#20204;&#36890;&#36807;&#22522;&#20110;&#26597;&#35810;&#30340;&#25915;&#20987;&#25913;&#36827;&#20197;&#21069;&#30340;&#24037;&#20316;&#65292;&#21033;&#29992; API &#35775;&#38382;&#36828;&#31243;&#35821;&#35328;&#27169;&#22411;&#26469;&#26500;&#36896;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#20351;&#27169;&#22411;&#20197;&#65288;&#26126;&#26174;&#65289;&#26356;&#39640;&#30340;&#27010;&#29575;&#21457;&#20986;&#26377;&#23475;&#23383;&#31526;&#20018;&#65292;&#32780;&#19981;&#33021;&#20165;&#20165;&#20351;&#29992;&#36716;&#31227;&#25915;&#20987;&#12290;&#25105;&#20204;&#22312; GPT-3.5 &#21644; OpenAI &#30340;&#23433;&#20840;&#20998;&#31867;&#22120;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#25915;&#20987;&#65307;&#25105;&#20204;&#33021;&#22815;&#35753; GPT-3.5 &#21457;&#20986;&#26377;&#23475;&#23383;&#31526;&#20018;&#65292;&#32780;&#30446;&#21069;&#30340;&#36716;&#31227;&#25915;&#20987;&#22833;&#36133;&#20102;&#65292;&#24182;&#19988;&#25105;&#20204;&#20960;&#20046;&#20197; 100% &#30340;&#27010;&#29575;&#35268;&#36991;&#20102;&#23433;&#20840;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12329v1 Announce Type: cross  Abstract: Recent work has shown it is possible to construct adversarial examples that cause an aligned language model to emit harmful strings or perform harmful behavior. Existing attacks work either in the white-box setting (with full access to the model weights), or through transferability: the phenomenon that adversarial examples crafted on one model often remain effective on other models. We improve on prior work with a query-based attack that leverages API access to a remote language model to construct adversarial examples that cause the model to emit harmful strings with (much) higher probability than with transfer-only attacks. We validate our attack on GPT-3.5 and OpenAI's safety classifier; we can cause GPT-3.5 to emit harmful strings that current transfer attacks fail at, and we can evade the safety classifier with nearly 100% probability.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;PsychoGAT&#65288;&#24515;&#29702;&#28216;&#25103;&#20195;&#29702;&#65289;&#20197;&#23454;&#29616;&#24515;&#29702;&#35780;&#20272;&#30340;&#36890;&#29992;&#28216;&#25103;&#21270;&#65292;&#36890;&#36807;&#23558;&#24378;&#22823;&#30340;LLM&#20195;&#29702;&#32435;&#20837;&#35282;&#33394;&#65292;&#23558;&#26631;&#20934;&#37327;&#34920;&#36716;&#21270;&#20026;&#20010;&#24615;&#21270;&#19988;&#20855;&#26377;&#21560;&#24341;&#21147;&#30340;&#20114;&#21160;&#23567;&#35828;&#28216;&#25103;&#12290;</title><link>https://arxiv.org/abs/2402.12326</link><description>&lt;p&gt;
&#22522;&#20110;LLM&#30340;&#24515;&#29702;&#23398;&#26234;&#33021;&#20195;&#29702;&#65306;&#19968;&#39033;&#20851;&#20110;&#28216;&#25103;&#21270;&#35780;&#20272;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
LLM Agents for Psychology: A Study on Gamified Assessments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;PsychoGAT&#65288;&#24515;&#29702;&#28216;&#25103;&#20195;&#29702;&#65289;&#20197;&#23454;&#29616;&#24515;&#29702;&#35780;&#20272;&#30340;&#36890;&#29992;&#28216;&#25103;&#21270;&#65292;&#36890;&#36807;&#23558;&#24378;&#22823;&#30340;LLM&#20195;&#29702;&#32435;&#20837;&#35282;&#33394;&#65292;&#23558;&#26631;&#20934;&#37327;&#34920;&#36716;&#21270;&#20026;&#20010;&#24615;&#21270;&#19988;&#20855;&#26377;&#21560;&#24341;&#21147;&#30340;&#20114;&#21160;&#23567;&#35828;&#28216;&#25103;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#29702;&#27979;&#37327;&#23545;&#20110;&#31934;&#31070;&#20581;&#24247;&#12289;&#33258;&#25105;&#29702;&#35299;&#21644;&#20010;&#20154;&#21457;&#23637;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#26041;&#27861;&#65292;&#22914;&#33258;&#25105;&#25253;&#21578;&#37327;&#34920;&#21644;&#24515;&#29702;&#23398;&#23478;&#35775;&#35848;&#65292;&#24120;&#24120;&#38754;&#20020;&#21442;&#19982;&#24230;&#21644;&#21487;&#33719;&#24471;&#24615;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;&#34429;&#28982;&#24050;&#32463;&#25506;&#35752;&#20102;&#22522;&#20110;&#28216;&#25103;&#21644;LLM&#30340;&#24037;&#20855;&#26469;&#25552;&#39640;&#29992;&#25143;&#20852;&#36259;&#24182;&#33258;&#21160;&#21270;&#35780;&#20272;&#65292;&#20294;&#23427;&#20204;&#38590;&#20197;&#24179;&#34913;&#21442;&#19982;&#24230;&#21644;&#26222;&#36866;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PsychoGAT&#65288;&#24515;&#29702;&#28216;&#25103;&#20195;&#29702;&#65289;&#65292;&#20197;&#23454;&#29616;&#24515;&#29702;&#35780;&#20272;&#30340;&#36890;&#29992;&#28216;&#25103;&#21270;&#12290;&#20027;&#35201;&#27934;&#23519;&#26159;&#24378;&#22823;&#30340;LLM&#26082;&#21487;&#20197;&#20805;&#24403;&#29087;&#32451;&#30340;&#24515;&#29702;&#23398;&#23478;&#65292;&#20063;&#21487;&#20197;&#26159;&#21019;&#26032;&#30340;&#28216;&#25103;&#35774;&#35745;&#24072;&#12290;&#36890;&#36807;&#23558;LLM&#20195;&#29702;&#32435;&#20837;&#25351;&#23450;&#35282;&#33394;&#24182;&#31934;&#24515;&#31649;&#29702;&#23427;&#20204;&#30340;&#20114;&#21160;&#65292;PsychoGAT&#21487;&#20197;&#23558;&#20219;&#20309;&#26631;&#20934;&#37327;&#34920;&#36716;&#21270;&#20026;&#20010;&#24615;&#21270;&#19988;&#20855;&#26377;&#21560;&#24341;&#21147;&#30340;&#20114;&#21160;&#23567;&#35828;&#28216;&#25103;&#12290;&#20026;&#39564;&#35777;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#36827;&#34892;&#24515;&#29702;&#24230;&#37327;&#35780;&#20272;&#20197;&#35780;&#20272;&#20854;&#26377;&#25928;&#24615;&#65292;&#24182;&#20351;&#29992;&#20154;&#31867;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12326v1 Announce Type: new  Abstract: Psychological measurement is essential for mental health, self-understanding, and personal development. Traditional methods, such as self-report scales and psychologist interviews, often face challenges with engagement and accessibility. While game-based and LLM-based tools have been explored to improve user interest and automate assessment, they struggle to balance engagement with generalizability. In this work, we propose PsychoGAT (Psychological Game AgenTs) to achieve a generic gamification of psychological assessment. The main insight is that powerful LLMs can function both as adept psychologists and innovative game designers. By incorporating LLM agents into designated roles and carefully managing their interactions, PsychoGAT can transform any standardized scales into personalized and engaging interactive fiction games. To validate the proposed method, we conduct psychometric evaluations to assess its effectiveness and employ huma
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#38750;GPS&#25112;&#22330;&#29615;&#22659;&#20013;&#36890;&#36807;&#20351;&#29992;&#22320;&#26631;&#38170;&#23450;&#33410;&#28857;&#23454;&#29616;&#31227;&#21160;&#37096;&#38431;&#25110;&#38450;&#24481;&#37096;&#38431;&#30340;&#34394;&#25311;&#22352;&#26631;&#33719;&#21462;&#30340;&#31574;&#30053;&#65292;&#24182;&#21033;&#29992;Yolov5&#27169;&#22411;&#36827;&#34892;&#22320;&#26631;&#35782;&#21035;&#21644;&#26377;&#25928;&#30340;&#31435;&#20307;&#21305;&#37197;&#31639;&#27861;&#36827;&#34892;&#22320;&#26631;&#36317;&#31163;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2402.12320</link><description>&lt;p&gt;
&#29992;&#20110;&#22312;&#38750;GPS&#25112;&#22330;&#29615;&#22659;&#20013;&#36827;&#34892;&#22320;&#26631;&#35782;&#21035;&#21644;&#31227;&#21160;&#33410;&#28857;&#23450;&#20301;&#30340;&#22320;&#26631;&#31435;&#20307;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Landmark Stereo Dataset for Landmark Recognition and Moving Node Localization in a Non-GPS Battlefield Environment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12320
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#38750;GPS&#25112;&#22330;&#29615;&#22659;&#20013;&#36890;&#36807;&#20351;&#29992;&#22320;&#26631;&#38170;&#23450;&#33410;&#28857;&#23454;&#29616;&#31227;&#21160;&#37096;&#38431;&#25110;&#38450;&#24481;&#37096;&#38431;&#30340;&#34394;&#25311;&#22352;&#26631;&#33719;&#21462;&#30340;&#31574;&#30053;&#65292;&#24182;&#21033;&#29992;Yolov5&#27169;&#22411;&#36827;&#34892;&#22320;&#26631;&#35782;&#21035;&#21644;&#26377;&#25928;&#30340;&#31435;&#20307;&#21305;&#37197;&#31639;&#27861;&#36827;&#34892;&#22320;&#26631;&#36317;&#31163;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#65292;&#21033;&#29992;&#22320;&#26631;&#38170;&#23450;&#33410;&#28857;&#32780;&#19981;&#26159;&#22522;&#20110;&#26080;&#32447;&#30005;&#30340;&#38170;&#23450;&#33410;&#28857;&#26469;&#33719;&#21462;&#31227;&#21160;&#37096;&#38431;&#25110;&#38450;&#24481;&#37096;&#38431;&#30340;&#34394;&#25311;&#22352;&#26631;&#65288;&#22320;&#26631;ID&#65292;&#36317;&#31163;&#65289;&#65292;&#20174;&#32780;&#24110;&#21161;&#22312;GPS&#21463;&#38480;&#30340;&#25112;&#22330;&#29615;&#22659;&#20013;&#36319;&#36394;&#21644;&#25805;&#32437;&#37096;&#38431;&#27839;&#30528;&#23433;&#20840;&#36335;&#24452;&#21069;&#36827;&#12290;&#25152;&#25552;&#20986;&#30340;&#31574;&#30053;&#37319;&#29992;Yolov5&#27169;&#22411;&#36827;&#34892;&#22320;&#26631;&#35782;&#21035;&#65292;&#24182;&#21033;&#29992;&#39640;&#25928;&#30340;&#31435;&#20307;&#21305;&#37197;&#31639;&#27861;&#36827;&#34892;&#22320;&#26631;&#36317;&#31163;&#20272;&#35745;&#12290;&#25105;&#20204;&#32771;&#34385;&#21040;&#65292;&#31227;&#21160;&#33410;&#28857;&#25658;&#24102;&#19968;&#21488;&#37197;&#22791;&#26657;&#20934;&#31435;&#20307;&#35270;&#35273;&#25668;&#20687;&#22836;&#30340;&#20302;&#21151;&#32791;&#31227;&#21160;&#35774;&#22791;&#65292;&#25293;&#25668;&#21253;&#21547;&#25112;&#22330;&#21306;&#22495;&#20869;&#22320;&#26631;&#30340;&#22330;&#26223;&#30340;&#31435;&#20307;&#22270;&#20687;&#65292;&#36825;&#20123;&#22320;&#26631;&#30340;&#20301;&#32622;&#23384;&#20648;&#22312;&#35774;&#22791;&#20869;&#30340;&#31163;&#32447;&#26381;&#21153;&#22120;&#20013;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#33258;&#23450;&#20041;&#30340;&#22320;&#26631;&#22270;&#20687;&#25968;&#25454;&#38598;MSTLandmarkv1&#65292;&#21253;&#21547;34&#20010;&#22320;&#26631;&#31867;&#21035;&#65292;&#20197;&#21450;&#21478;&#19968;&#20010;&#21253;&#21547;&#36825;34&#20010;&#22320;&#26631;&#31034;&#20363;&#30340;&#22320;&#26631;&#31435;&#20307;&#25968;&#25454;&#38598;MSTLandmarkSt&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12320v1 Announce Type: cross  Abstract: In this paper, we have proposed a new strategy of using the landmark anchor node instead of a radio-based anchor node to obtain the virtual coordinates (landmarkID, DISTANCE) of moving troops or defense forces that will help in tracking and maneuvering the troops along a safe path within a GPS-denied battlefield environment. The proposed strategy implements landmark recognition using the Yolov5 model and landmark distance estimation using an efficient Stereo Matching Algorithm. We consider that a moving node carrying a low-power mobile device facilitated with a calibrated stereo vision camera that captures stereo images of a scene containing landmarks within the battlefield region whose locations are stored in an offline server residing within the device itself. We created a custom landmark image dataset called MSTLandmarkv1 with 34 landmark classes and another landmark stereo dataset of those 34 landmark instances called MSTLandmarkSt
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;FairSAR&#65292;&#19968;&#31181;&#29420;&#29305;&#30340;&#36951;&#25022;&#24230;&#37327;&#65292;&#20197;&#35299;&#20915;&#21160;&#24577;&#29615;&#22659;&#19979;&#30340;&#20844;&#24179;&#24847;&#35782;&#22312;&#32447;&#23398;&#20064;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.12319</link><description>&lt;p&gt;
&#20855;&#26377;&#20844;&#24179;&#24847;&#35782;&#30340;&#21160;&#24577;&#29615;&#22659;&#21709;&#24212;&#22411;&#22312;&#32447;&#20803;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Dynamic Environment Responsive Online Meta-Learning with Fairness Awareness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12319
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;FairSAR&#65292;&#19968;&#31181;&#29420;&#29305;&#30340;&#36951;&#25022;&#24230;&#37327;&#65292;&#20197;&#35299;&#20915;&#21160;&#24577;&#29615;&#22659;&#19979;&#30340;&#20844;&#24179;&#24847;&#35782;&#22312;&#32447;&#23398;&#20064;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#20844;&#24179;&#24847;&#35782;&#30340;&#22312;&#32447;&#23398;&#20064;&#26694;&#26550;&#24050;&#32463;&#25104;&#20026;&#36830;&#32493;&#32456;&#36523;&#23398;&#20064;&#32972;&#26223;&#19979;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#23398;&#20064;&#32773;&#30340;&#30446;&#26631;&#26159;&#38543;&#30528;&#26102;&#38388;&#25512;&#31227;&#36880;&#28176;&#33719;&#21462;&#26032;&#20219;&#21153;&#65292;&#21516;&#26102;&#22312;&#24341;&#20837;&#26032;&#20219;&#21153;&#26102;&#30830;&#20445;&#21508;&#20010;&#21463;&#20445;&#25252;&#23376;&#20154;&#21475;&#65288;&#22914;&#31181;&#26063;&#21644;&#24615;&#21035;&#65289;&#20043;&#38388;&#30340;&#32479;&#35745;&#24179;&#31561;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#36951;&#25022;&#24230;&#37327;FairSAR&#65292;&#20197;&#24212;&#23545;&#19981;&#26029;&#21457;&#23637;&#30340;&#29615;&#22659;&#20013;&#30340;&#20844;&#24179;&#24847;&#35782;&#22312;&#32447;&#23398;&#20064;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12319v1 Announce Type: cross  Abstract: The fairness-aware online learning framework has emerged as a potent tool within the context of continuous lifelong learning. In this scenario, the learner's objective is to progressively acquire new tasks as they arrive over time, while also guaranteeing statistical parity among various protected sub-populations, such as race and gender, when it comes to the newly introduced tasks. A significant limitation of current approaches lies in their heavy reliance on the i.i.d (independent and identically distributed) assumption concerning data, leading to a static regret analysis of the framework. Nevertheless, it's crucial to note that achieving low static regret does not necessarily translate to strong performance in dynamic environments characterized by tasks sampled from diverse distributions. In this paper, to tackle the fairness-aware online learning challenge in evolving settings, we introduce a unique regret measure, FairSAR, by inco
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#24322;&#26500;&#20256;&#24863;&#22120;&#34701;&#21512;&#30340;&#22810;&#35270;&#35282;&#19968;&#33268;&#27169;&#22411;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;&#38598;&#21512;&#20132;&#38598;&#30340;&#22810;&#35270;&#35282;&#21322;&#19968;&#33268;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.12307</link><description>&lt;p&gt;
&#22810;&#35270;&#35282;&#19968;&#33268;&#23398;&#20064;&#29992;&#20110;&#24322;&#26500;&#20256;&#24863;&#22120;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
Multi-View Conformal Learning for Heterogeneous Sensor Fusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12307
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#24322;&#26500;&#20256;&#24863;&#22120;&#34701;&#21512;&#30340;&#22810;&#35270;&#35282;&#19968;&#33268;&#27169;&#22411;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;&#38598;&#21512;&#20132;&#38598;&#30340;&#22810;&#35270;&#35282;&#21322;&#19968;&#33268;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#22815;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#20010;&#21035;&#39044;&#27979;&#30340;&#32622;&#20449;&#24230;&#23545;&#20110;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#65292;&#23588;&#20854;&#26159;&#22312;&#35832;&#22914;&#21307;&#30103;&#35786;&#26029;&#12289;&#23433;&#20840;&#21644;&#26080;&#20154;&#36710;&#31561;&#20851;&#38190;&#24212;&#29992;&#20013;&#12290;&#22312;&#36807;&#21435;&#20960;&#24180;&#65292;&#22797;&#26434;&#30340;&#39044;&#27979;&#27169;&#22411;&#22312;&#35299;&#20915;&#22256;&#38590;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#27599;&#22825;&#37117;&#26377;&#26032;&#26041;&#27861;&#34987;&#25552;&#20986;&#12290;&#32780;&#22823;&#22810;&#25968;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26032;&#21457;&#23637;&#20027;&#35201;&#38598;&#20013;&#22312;&#25552;&#39640;&#25972;&#20307;&#24615;&#33021;&#19978;&#65292;&#23545;&#20010;&#21035;&#39044;&#27979;&#30340;&#21487;&#20449;&#24230;&#32771;&#37327;&#36739;&#23569;&#65292;&#29978;&#33267;&#22312;&#20256;&#24863;&#22120;&#34701;&#21512;&#30340;&#32972;&#26223;&#19979;&#26356;&#23569;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#26500;&#24314;&#24182;&#27979;&#35797;&#20102;&#29992;&#20110;&#24322;&#26500;&#20256;&#24863;&#22120;&#34701;&#21512;&#30340;&#22810;&#35270;&#35282;&#21644;&#21333;&#35270;&#35282;&#19968;&#33268;&#27169;&#22411;&#12290;&#30001;&#20110;&#22522;&#20110;&#19968;&#33268;&#24615;&#39044;&#27979;&#26694;&#26550;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#29702;&#35770;&#19978;&#30340;&#36793;&#38469;&#32622;&#20449;&#20445;&#35777;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#22522;&#20110;&#38598;&#21512;&#20132;&#38598;&#30340;&#22810;&#35270;&#35282;&#21322;&#19968;&#33268;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12307v1 Announce Type: cross  Abstract: Being able to assess the confidence of individual predictions in machine learning models is crucial for decision making scenarios. Specially, in critical applications such as medical diagnosis, security, and unmanned vehicles, to name a few. In the last years, complex predictive models have had great success in solving hard tasks and new methods are being proposed every day. While the majority of new developments in machine learning models focus on improving the overall performance, less effort is put on assessing the trustworthiness of individual predictions, and even to a lesser extent, in the context of sensor fusion. To this end, we build and test multi-view and single-view conformal models for heterogeneous sensor fusion. Our models provide theoretical marginal confidence guarantees since they are based on the conformal prediction framework. We also propose a multi-view semi-conformal model based on sets intersection. Through comp
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#35889;&#32858;&#31867;&#20013;&#29305;&#24449;&#21521;&#37327;&#30340;&#28176;&#36817;&#39640;&#26031;&#27874;&#21160;&#29616;&#35937;&#65292;&#20026;&#31934;&#30830;&#39044;&#27979;&#35889;&#32858;&#31867;&#30340;&#20998;&#31867;&#24615;&#33021;&#25552;&#20379;&#20102;&#37325;&#35201;&#20381;&#25454;&#12290;</title><link>https://arxiv.org/abs/2402.12302</link><description>&lt;p&gt;
&#35889;&#32858;&#31867;&#20013;&#29305;&#24449;&#21521;&#37327;&#30340;&#28176;&#36817;&#39640;&#26031;&#27874;&#21160;
&lt;/p&gt;
&lt;p&gt;
Asymptotic Gaussian Fluctuations of Eigenvectors in Spectral Clustering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12302
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#35889;&#32858;&#31867;&#20013;&#29305;&#24449;&#21521;&#37327;&#30340;&#28176;&#36817;&#39640;&#26031;&#27874;&#21160;&#29616;&#35937;&#65292;&#20026;&#31934;&#30830;&#39044;&#27979;&#35889;&#32858;&#31867;&#30340;&#20998;&#31867;&#24615;&#33021;&#25552;&#20379;&#20102;&#37325;&#35201;&#20381;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35889;&#32858;&#31867;&#30340;&#24615;&#33021;&#20381;&#36182;&#20110;&#30456;&#20284;&#30697;&#38453;&#30340;&#29305;&#24449;&#21521;&#37327;&#30340;&#26465;&#30446;&#27874;&#21160;&#65292;&#35813;&#27874;&#21160;&#30452;&#21040;&#29616;&#22312;&#20173;&#26410;&#24471;&#21040;&#25551;&#36848;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#19968;&#33324;&#23574;&#23792;&#38543;&#26426;&#30697;&#38453;&#27169;&#22411;&#30340;&#20449;&#21495;+&#22122;&#22768;&#32467;&#26500;&#34987;&#36716;&#31227;&#21040;&#30456;&#24212;&#30340;&#26684;&#25289;&#22982;&#26680;&#30697;&#38453;&#30340;&#29305;&#24449;&#21521;&#37327;&#19978;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#26465;&#30446;&#27874;&#21160;&#22312;&#22823;&#32500;&#24230;&#21306;&#22495;&#21576;&#39640;&#26031;&#20998;&#24067;&#12290;&#36825;&#31181;&#31867;&#20284;&#20110;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#30340;&#32467;&#26524;&#26159;&#20934;&#30830;&#39044;&#27979;&#35889;&#32858;&#31867;&#30340;&#20998;&#31867;&#24615;&#33021;&#30340;&#26368;&#21518;&#19968;&#22359;&#32570;&#22833;&#30340;&#25340;&#22270;&#12290;&#25552;&#20986;&#30340;&#35777;&#26126;&#38750;&#24120;&#36890;&#29992;&#65292;&#20165;&#20381;&#36182;&#20110;&#22122;&#22768;&#30340;&#26059;&#36716;&#19981;&#21464;&#24615;&#12290;&#23545;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#30340;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#20102;&#36825;&#20010;&#29616;&#35937;&#30340;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12302v1 Announce Type: cross  Abstract: The performance of spectral clustering relies on the fluctuations of the entries of the eigenvectors of a similarity matrix, which has been left uncharacterized until now. In this letter, it is shown that the signal $+$ noise structure of a general spike random matrix model is transferred to the eigenvectors of the corresponding Gram kernel matrix and the fluctuations of their entries are Gaussian in the large-dimensional regime. This CLT-like result was the last missing piece to precisely predict the classification performance of spectral clustering. The proposed proof is very general and relies solely on the rotational invariance of the noise. Numerical experiments on synthetic and real data illustrate the universality of this phenomenon.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#39537;&#21160;&#30340;&#27491;&#21017;&#21270;&#31574;&#30053;&#34701;&#20837;&#27010;&#29575;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#27491;&#21017;&#21270;&#21435;&#22122;&#30340;&#26041;&#27861;&#65292;&#24182;&#24212;&#29992;&#20110;&#22270;&#20687;&#21453;&#28436;&#20219;&#21153;&#20013;&#65292;&#22312;&#25104;&#20687;&#20013;&#25512;&#21160;&#20102;&#36125;&#21494;&#26031;&#25512;&#26029;&#12290;</title><link>https://arxiv.org/abs/2402.12292</link><description>&lt;p&gt;
&#27491;&#21017;&#21270;&#21435;&#22122;&#65306;&#36125;&#21494;&#26031;&#27169;&#22411;&#21644;&#38543;&#21518;&#30340;Langevin-within-split Gibbs&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Regularization by denoising: Bayesian model and Langevin-within-split Gibbs sampling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12292
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#39537;&#21160;&#30340;&#27491;&#21017;&#21270;&#31574;&#30053;&#34701;&#20837;&#27010;&#29575;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#27491;&#21017;&#21270;&#21435;&#22122;&#30340;&#26041;&#27861;&#65292;&#24182;&#24212;&#29992;&#20110;&#22270;&#20687;&#21453;&#28436;&#20219;&#21153;&#20013;&#65292;&#22312;&#25104;&#20687;&#20013;&#25512;&#21160;&#20102;&#36125;&#21494;&#26031;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#36890;&#36807;&#25512;&#23548;&#20986;&#19968;&#20010;&#27010;&#29575;&#21270;&#30340;&#30456;&#23545;&#24212;&#20110;&#27491;&#21017;&#21270;&#21435;&#22122;&#65288;RED&#65289;&#33539;&#24335;&#30340;&#26041;&#27861;&#23454;&#29616;&#23545;&#22270;&#20687;&#21453;&#28436;&#12290;&#27492;&#22806;&#65292;&#23427;&#23454;&#29616;&#20102;&#19968;&#20010;&#33945;&#29305;&#21345;&#27931;&#31639;&#27861;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#20174;&#25152;&#24471;&#30340;&#21518;&#39564;&#20998;&#24067;&#20013;&#37319;&#26679;&#65292;&#22522;&#20110;&#28176;&#36817;&#31934;&#30830;&#25968;&#25454;&#22686;&#24191;&#65288;AXDA&#65289;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#26159;&#19968;&#20010;&#23884;&#20837;&#20102;&#19968;&#20010;Langevin&#33945;&#29305;&#21345;&#27931;&#27493;&#39588;&#30340;&#25286;&#20998;Gibbs&#37319;&#26679;&#65288;SGS&#65289;&#30340;&#36817;&#20284;&#23454;&#20363;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#24120;&#35265;&#30340;&#25104;&#20687;&#20219;&#21153;&#65292;&#22914;&#21435;&#27169;&#31946;&#12289;&#20462;&#34917;&#21644;&#36229;&#20998;&#36776;&#29575;&#65292;&#36890;&#36807;&#22823;&#37327;&#30340;&#25968;&#20540;&#23454;&#39564;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#36825;&#20123;&#36129;&#29486;&#36890;&#36807;&#22312;&#27010;&#29575;&#26694;&#26550;&#20869;&#21033;&#29992;&#25968;&#25454;&#39537;&#21160;&#30340;&#27491;&#21017;&#21270;&#31574;&#30053;&#65292;&#20419;&#36827;&#20102;&#25104;&#20687;&#20013;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12292v1 Announce Type: cross  Abstract: This paper introduces a Bayesian framework for image inversion by deriving a probabilistic counterpart to the regularization-by-denoising (RED) paradigm. It additionally implements a Monte Carlo algorithm specifically tailored for sampling from the resulting posterior distribution, based on an asymptotically exact data augmentation (AXDA). The proposed algorithm is an approximate instance of split Gibbs sampling (SGS) which embeds one Langevin Monte Carlo step. The proposed method is applied to common imaging tasks such as deblurring, inpainting and super-resolution, demonstrating its efficacy through extensive numerical experiments. These contributions advance Bayesian inference in imaging by leveraging data-driven regularization strategies within a probabilistic framework.
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#36125;&#21494;&#26031;&#32423;&#21035;&#23436;&#32654;&#30340;MMR&#65288;BLP&#65289;&#65292;&#23427;&#26159;&#26497;&#23567;&#21270;&#36951;&#25022;&#30446;&#26631;&#30340;&#31934;&#30830;&#21270;&#65292;&#33021;&#22815;&#20811;&#26381;&#26497;&#23567;&#21270;&#36951;&#25022;&#31574;&#30053;&#22312;&#36951;&#25022;&#19978;&#30028;&#26102;&#23398;&#20064;&#20572;&#28382;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2402.12284</link><description>&lt;p&gt;
&#20026;&#26080;&#30417;&#30563;&#29615;&#22659;&#35774;&#35745;&#20248;&#21270;&#26497;&#23567;&#21270;&#36951;&#25022;
&lt;/p&gt;
&lt;p&gt;
Refining Minimax Regret for Unsupervised Environment Design
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12284
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#36125;&#21494;&#26031;&#32423;&#21035;&#23436;&#32654;&#30340;MMR&#65288;BLP&#65289;&#65292;&#23427;&#26159;&#26497;&#23567;&#21270;&#36951;&#25022;&#30446;&#26631;&#30340;&#31934;&#30830;&#21270;&#65292;&#33021;&#22815;&#20811;&#26381;&#26497;&#23567;&#21270;&#36951;&#25022;&#31574;&#30053;&#22312;&#36951;&#25022;&#19978;&#30028;&#26102;&#23398;&#20064;&#20572;&#28382;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26080;&#30417;&#30563;&#29615;&#22659;&#35774;&#35745;&#20013;&#65292;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#36890;&#36807;&#23545;&#23545;&#25163;&#26368;&#22823;&#21270;&#26576;&#20010;&#30446;&#26631;&#29983;&#25104;&#30340;&#29615;&#22659;&#37197;&#32622;&#65288;&#20851;&#21345;&#65289;&#36827;&#34892;&#35757;&#32451;&#12290;&#36951;&#25022;&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#30446;&#26631;&#65292;&#29702;&#35770;&#19978;&#23548;&#33268;&#20855;&#26377;&#33391;&#22909;&#40065;&#26834;&#24615;&#20445;&#35777;&#30340;&#26497;&#23567;&#21270;&#36951;&#25022;&#65288;MMR&#65289;&#31574;&#30053;&#65307;&#29305;&#21035;&#26159;&#65292;&#20195;&#29702;&#30340;&#26368;&#22823;&#36951;&#25022;&#26159;&#26377;&#30028;&#30340;&#12290;&#28982;&#32780;&#65292;&#19968;&#26086;&#20195;&#29702;&#22312;&#25152;&#26377;&#20851;&#21345;&#19978;&#36798;&#21040;&#20102;&#36825;&#20010;&#36951;&#25022;&#19978;&#30028;&#65292;&#23545;&#25163;&#23558;&#21482;&#20250;&#23545;&#26080;&#27861;&#36827;&#19968;&#27493;&#20943;&#23569;&#36951;&#25022;&#30340;&#20851;&#21345;&#36827;&#34892;&#37319;&#26679;&#12290;&#23613;&#31649;&#22312;&#36825;&#20123;&#26368;&#22823;&#21270;&#36951;&#25022;&#30340;&#20851;&#21345;&#20043;&#22806;&#21487;&#33021;&#23384;&#22312;&#24615;&#33021;&#25913;&#36827;&#31354;&#38388;&#65292;&#20294;&#23398;&#20064;&#20572;&#28382;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#36125;&#21494;&#26031;&#32423;&#21035;&#23436;&#32654;&#30340;MMR&#65288;BLP&#65289;&#65292;&#23427;&#26159;&#26497;&#23567;&#21270;&#36951;&#25022;&#30446;&#26631;&#30340;&#31934;&#30830;&#21270;&#12290;&#25105;&#20204;&#27491;&#24335;&#35777;&#26126;&#65292;&#35299;&#20915;&#36825;&#20010;&#30446;&#26631;&#23558;&#23548;&#33268;MMR&#31574;&#30053;&#30340;&#23376;&#38598;&#65292;&#24182;&#19988;BLP&#31574;&#30053;&#22312;&#25152;&#26377;&#20851;&#21345;&#19978;&#37117;&#19982;&#23436;&#32654;&#36125;&#21494;&#26031;&#31574;&#30053;&#19968;&#33268;&#34892;&#20107;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12284v1 Announce Type: cross  Abstract: In unsupervised environment design, reinforcement learning agents are trained on environment configurations (levels) generated by an adversary that maximises some objective. Regret is a commonly used objective that theoretically results in a minimax regret (MMR) policy with desirable robustness guarantees; in particular, the agent's maximum regret is bounded. However, once the agent reaches this regret bound on all levels, the adversary will only sample levels where regret cannot be further reduced. Although there are possible performance improvements to be made outside of these regret-maximising levels, learning stagnates. In this work, we introduce Bayesian level-perfect MMR (BLP), a refinement of the minimax regret objective that overcomes this limitation. We formally show that solving for this objective results in a subset of MMR policies, and that BLP policies act consistently with a Perfect Bayesian policy over all levels. We fur
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36328;&#24322;&#26500;&#20113;&#21644;&#39640;&#24615;&#33021;&#35745;&#31639;&#36164;&#28304;&#30340;&#23433;&#20840;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;Globus Compute&#21644;&#20122;&#39532;&#36874;&#20113;&#26381;&#21153;&#65292;&#23454;&#29616;&#20102;&#31471;&#21040;&#31471;&#30340;&#38544;&#31169;&#20445;&#25252;&#65292;&#25991;&#20013;&#20197;LLaMA 27B&#27169;&#22411;&#30340;&#32852;&#37030;&#24494;&#35843;&#20026;&#20363;&#12290;</title><link>https://arxiv.org/abs/2402.12271</link><description>&lt;p&gt;
&#36328;&#24322;&#26500;&#20113;&#21644;&#39640;&#24615;&#33021;&#35745;&#31639;&#36164;&#28304;&#30340;&#23433;&#20840;&#32852;&#37030;&#23398;&#20064;&#8212;&#8212;&#20197;LLaMA 2&#30340;&#32852;&#37030;&#24494;&#35843;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Secure Federated Learning Across Heterogeneous Cloud and High-Performance Computing Resources -- A Case Study on Federated Fine-tuning of LLaMA 2
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12271
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36328;&#24322;&#26500;&#20113;&#21644;&#39640;&#24615;&#33021;&#35745;&#31639;&#36164;&#28304;&#30340;&#23433;&#20840;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;Globus Compute&#21644;&#20122;&#39532;&#36874;&#20113;&#26381;&#21153;&#65292;&#23454;&#29616;&#20102;&#31471;&#21040;&#31471;&#30340;&#38544;&#31169;&#20445;&#25252;&#65292;&#25991;&#20013;&#20197;LLaMA 27B&#27169;&#22411;&#30340;&#32852;&#37030;&#24494;&#35843;&#20026;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20351;&#22810;&#20010;&#25968;&#25454;&#25152;&#26377;&#32773;&#33021;&#22815;&#21327;&#20316;&#35757;&#32451;&#20581;&#22766;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#36716;&#31227;&#22823;&#22411;&#25110;&#25935;&#24863;&#30340;&#26412;&#22320;&#25968;&#25454;&#38598;&#65292;&#21482;&#38656;&#20849;&#20139;&#26412;&#22320;&#35757;&#32451;&#27169;&#22411;&#30340;&#21442;&#25968;&#12290;&#26412;&#25991;&#35814;&#32454;&#38416;&#36848;&#20102;&#25105;&#20204;&#30340;&#39640;&#32423;&#38544;&#31169;&#20445;&#25252;&#32852;&#37030;&#23398;&#20064;&#65288;APPFL&#65289;&#26694;&#26550;&#30340;&#35774;&#35745;&#65292;&#36890;&#36807;&#21033;&#29992;Globus Compute&#65288;&#19968;&#31181;&#20998;&#24067;&#24335;&#20989;&#25968;&#21363;&#26381;&#21153;&#24179;&#21488;&#65289;&#21644;&#20122;&#39532;&#36874;&#20113;&#26381;&#21153;&#65292;&#22312;&#20113;&#35745;&#31639;&#35774;&#26045;&#21644;&#39640;&#24615;&#33021;&#35745;&#31639;&#36164;&#28304;&#20043;&#38388;&#31616;&#21270;&#31471;&#21040;&#31471;&#23433;&#20840;&#21487;&#38752;&#30340;&#32852;&#37030;&#23398;&#20064;&#23454;&#39564;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#28436;&#31034;&#20102;&#22312;&#22810;&#20010;&#20113;&#36164;&#28304;&#21644;&#36229;&#32423;&#35745;&#31639;&#26426;&#19978;&#20351;&#29992;APPFL&#26469;&#24494;&#35843;LLaMA 27B&#27169;&#22411;&#30340;&#29992;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12271v1 Announce Type: cross  Abstract: Federated learning enables multiple data owners to collaboratively train robust machine learning models without transferring large or sensitive local datasets by only sharing the parameters of the locally trained models. In this paper, we elaborate on the design of our Advanced Privacy-Preserving Federated Learning (APPFL) framework, which streamlines end-to-end secure and reliable federated learning experiments across cloud computing facilities and high-performance computing resources by leveraging Globus Compute, a distributed function as a service platform, and Amazon Web Services. We further demonstrate the use case of APPFL in fine-tuning a LLaMA 2 7B model using several cloud resources and supercomputers.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#21033;&#29992;&#37096;&#20998;&#25513;&#30721;&#34701;&#21512;&#30340;Gromov-Wasserstein&#21305;&#37197;&#36827;&#34892;&#20219;&#24847;&#22823;&#23567;&#22270;&#30340;&#31471;&#23545;&#31471;&#30417;&#30563;&#39044;&#27979;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#30456;&#27604;&#31454;&#20105;&#32773;&#26356;&#39640;&#30340;&#25928;&#29575;&#21644;&#22810;&#21151;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.12269</link><description>&lt;p&gt;
&#21033;&#29992;&#37096;&#20998;&#25513;&#30721;&#34701;&#21512;&#30340;Gromov-Wasserstein&#21305;&#37197;&#36827;&#34892;&#20219;&#24847;&#22823;&#23567;&#22270;&#30340;&#31471;&#23545;&#31471;&#30417;&#30563;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
End-to-end Supervised Prediction of Arbitrary-size Graphs with Partially-Masked Fused Gromov-Wasserstein Matching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12269
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#21033;&#29992;&#37096;&#20998;&#25513;&#30721;&#34701;&#21512;&#30340;Gromov-Wasserstein&#21305;&#37197;&#36827;&#34892;&#20219;&#24847;&#22823;&#23567;&#22270;&#30340;&#31471;&#23545;&#31471;&#30417;&#30563;&#39044;&#27979;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#30456;&#27604;&#31454;&#20105;&#32773;&#26356;&#39640;&#30340;&#25928;&#29575;&#21644;&#22810;&#21151;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#31471;&#21040;&#31471;&#28145;&#24230;&#23398;&#20064;&#30340;&#30417;&#30563;&#22270;&#39044;&#27979;&#65288;SGP&#65289;&#26041;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21407;&#22987;&#30340;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#65288;OT&#65289;&#30340;&#25439;&#22833;&#65292;&#37096;&#20998;&#25513;&#30721;&#34701;&#21512;&#30340;Gromov-Wasserstein&#25439;&#22833;&#65288;PM-FGW&#65289;&#65292;&#21487;&#20197;&#30452;&#25509;&#21033;&#29992;&#22270;&#34920;&#31034;&#65292;&#27604;&#22914;&#37051;&#25509;&#21644;&#29305;&#24449;&#30697;&#38453;&#12290;PM-FGW&#20855;&#26377;SGP&#30340;&#25152;&#26377;&#29702;&#24819;&#23646;&#24615;&#65306;&#33410;&#28857;&#25490;&#21015;&#19981;&#21464;&#24615;&#65292;&#21487;&#24494;&#20998;&#24615;&#65292;&#36890;&#36807;&#27604;&#36739;&#23427;&#20204;&#30340;&#22635;&#20805;&#34920;&#31034;&#20197;&#21450;&#23427;&#20204;&#30340;&#25513;&#30721;&#21521;&#37327;&#22788;&#29702;&#19981;&#21516;&#22823;&#23567;&#30340;&#22270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#22522;&#20110;transformer&#30340;&#26550;&#26500;&#65292;&#21487;&#20197;&#36731;&#26494;&#36866;&#24212;&#19981;&#21516;&#31867;&#22411;&#30340;&#36755;&#20837;&#25968;&#25454;&#12290;&#22312;&#23454;&#39564;&#37096;&#20998;&#65292;&#19977;&#20010;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#19968;&#20010;&#26032;&#39062;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65288;image2graph&#65289;&#21644;&#20004;&#20010;&#30495;&#23454;&#20219;&#21153;&#65292;&#22270;&#20687;&#21040;&#22320;&#22270;&#21644;&#25351;&#32441;&#21040;&#20998;&#23376; - &#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30456;&#27604;&#31454;&#20105;&#32773;&#30340;&#25928;&#29575;&#21644;&#22810;&#21151;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12269v1 Announce Type: new  Abstract: We present a novel end-to-end deep learning-based approach for Supervised Graph Prediction (SGP). We introduce an original Optimal Transport (OT)-based loss, the Partially-Masked Fused Gromov-Wasserstein loss (PM-FGW), that allows to directly leverage graph representations such as adjacency and feature matrices. PM-FGW exhibits all the desirable properties for SGP: it is node permutation invariant, sub-differentiable and handles graphs of different sizes by comparing their padded representations as well as their masking vectors. Moreover, we present a flexible transformer-based architecture that easily adapts to different types of input data. In the experimental section, three different tasks, a novel and challenging synthetic dataset (image2graph) and two real-world tasks, image2map and fingerprint2molecule - showcase the efficiency and versatility of the approach compared to competitors.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#33976;&#39311;&#30340;&#32852;&#37030;&#23398;&#20064;&#22312;&#25308;&#21344;&#24237;&#29615;&#22659;&#19979;&#34920;&#29616;&#20986;&#26497;&#24378;&#30340;&#24377;&#24615;&#65292;&#20171;&#32461;&#20102;&#20004;&#31181;&#26032;&#30340;&#25308;&#21344;&#24237;&#25915;&#20987;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#25308;&#21344;&#24237;&#24377;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.12265</link><description>&lt;p&gt;
&#35770;&#22522;&#20110;&#33976;&#39311;&#30340;&#32852;&#37030;&#23398;&#20064;&#22312;&#25308;&#21344;&#24237;&#29615;&#22659;&#19979;&#30340;&#24377;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Byzantine-Resilience of Distillation-Based Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12265
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#33976;&#39311;&#30340;&#32852;&#37030;&#23398;&#20064;&#22312;&#25308;&#21344;&#24237;&#29615;&#22659;&#19979;&#34920;&#29616;&#20986;&#26497;&#24378;&#30340;&#24377;&#24615;&#65292;&#20171;&#32461;&#20102;&#20004;&#31181;&#26032;&#30340;&#25308;&#21344;&#24237;&#25915;&#20987;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#25308;&#21344;&#24237;&#24377;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22312;&#38544;&#31169;&#12289;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#21644;&#36890;&#20449;&#25104;&#26412;&#26041;&#38754;&#30340;&#20248;&#21183;&#65292;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#31639;&#27861;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#25308;&#21344;&#24237;&#29615;&#22659;&#20013;&#30340;&#24615;&#33021;&#65292;&#23637;&#31034;&#20102;&#22522;&#20110;KD&#30340;FL&#31639;&#27861;&#30456;&#24403;&#20855;&#26377;&#24377;&#24615;&#65292;&#24182;&#20998;&#26512;&#20102;&#25308;&#21344;&#24237;&#23458;&#25143;&#31471;&#22914;&#20309;&#24433;&#21709;&#23398;&#20064;&#36807;&#31243;&#30456;&#23545;&#20110;&#32852;&#37030;&#24179;&#22343;&#31639;&#27861;&#12290;&#26681;&#25454;&#36825;&#20123;&#35265;&#35299;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20004;&#31181;&#26032;&#30340;&#25308;&#21344;&#24237;&#25915;&#20987;&#65292;&#24182;&#35777;&#26126;&#23427;&#20204;&#23545;&#20808;&#21069;&#30340;&#25308;&#21344;&#24237;&#24377;&#24615;&#26041;&#27861;&#26159;&#26377;&#25928;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FilterExp&#65292;&#19968;&#31181;&#26088;&#22312;&#22686;&#24378;&#25308;&#21344;&#24237;&#24377;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12265v1 Announce Type: cross  Abstract: Federated Learning (FL) algorithms using Knowledge Distillation (KD) have received increasing attention due to their favorable properties with respect to privacy, non-i.i.d. data and communication cost. These methods depart from transmitting model parameters and, instead, communicate information about a learning task by sharing predictions on a public dataset. In this work, we study the performance of such approaches in the byzantine setting, where a subset of the clients act in an adversarial manner aiming to disrupt the learning process. We show that KD-based FL algorithms are remarkably resilient and analyze how byzantine clients can influence the learning process compared to Federated Averaging. Based on these insights, we introduce two new byzantine attacks and demonstrate that they are effective against prior byzantine-resilient methods. Additionally, we propose FilterExp, a novel method designed to enhance the byzantine resilien
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;LoRA&#38598;&#25104;&#22312;&#31934;&#35843;LLMs&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#21407;&#21017;&#24615;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#19981;&#21516;&#25968;&#25454;&#22495;&#30340;&#20302;&#31209;&#36866;&#24212;&#38598;&#25104;&#20998;&#26512;&#65292;&#25512;&#27979;&#20102;&#27169;&#22411;&#23545;&#29305;&#23450;&#26550;&#26500;&#38590;&#20197;&#23398;&#20064;&#30340;&#25968;&#25454;&#39046;&#22495;&#30340;&#20449;&#21495;&#12290;</title><link>https://arxiv.org/abs/2402.12264</link><description>&lt;p&gt;
&#20351;&#29992;LoRA&#38598;&#25104;&#22312;&#31934;&#35843;LLMs&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Uncertainty quantification in fine-tuned LLMs using LoRA ensembles
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12264
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;LoRA&#38598;&#25104;&#22312;&#31934;&#35843;LLMs&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#21407;&#21017;&#24615;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#19981;&#21516;&#25968;&#25454;&#22495;&#30340;&#20302;&#31209;&#36866;&#24212;&#38598;&#25104;&#20998;&#26512;&#65292;&#25512;&#27979;&#20102;&#27169;&#22411;&#23545;&#29305;&#23450;&#26550;&#26500;&#38590;&#20197;&#23398;&#20064;&#30340;&#25968;&#25454;&#39046;&#22495;&#30340;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;&#29305;&#23450;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#23613;&#31649;&#23545;&#20110;&#31934;&#35843;&#27169;&#22411;&#23398;&#21040;&#20102;&#20160;&#20040;&#12289;&#36951;&#24536;&#20102;&#20160;&#20040;&#20197;&#21450;&#22914;&#20309;&#20449;&#20219;&#20854;&#39044;&#27979;&#20173;&#28982;&#32570;&#20047;&#19968;&#20010;&#19968;&#33324;&#30340;&#29702;&#35299;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#20302;&#31209;&#36866;&#24212;&#38598;&#25104;&#23545;&#31934;&#35843;LLMs&#36827;&#34892;&#22522;&#20110;&#21518;&#39564;&#36924;&#36817;&#30340;&#21407;&#21017;&#24615;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;Mistral-7b&#30340;&#20302;&#31209;&#36866;&#24212;&#38598;&#25104;&#20998;&#26512;&#20102;&#19977;&#20010;&#24120;&#35265;&#30340;&#22810;&#39033;&#36873;&#25321;&#25968;&#25454;&#38598;&#65292;&#24182;&#23545;&#20854;&#22312;&#31934;&#35843;&#36807;&#31243;&#20013;&#21644;&#20043;&#21518;&#23545;&#19981;&#21516;&#30446;&#26631;&#39046;&#22495;&#30340;&#24863;&#30693;&#22797;&#26434;&#24615;&#21644;&#27169;&#22411;&#25928;&#33021;&#36827;&#34892;&#20102;&#23450;&#37327;&#21644;&#23450;&#24615;&#30340;&#32467;&#35770;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22522;&#20110;&#25968;&#20540;&#23454;&#39564;&#25903;&#25345;&#65292;&#25105;&#20204;&#23545;&#37027;&#20123;&#23545;&#20110;&#32473;&#23450;&#26550;&#26500;&#38590;&#20197;&#23398;&#20064;&#30340;&#25968;&#25454;&#39046;&#22495;&#30340;&#29109;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#25552;&#20986;&#20102;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12264v1 Announce Type: cross  Abstract: Fine-tuning large language models can improve task specific performance, although a general understanding of what the fine-tuned model has learned, forgotten and how to trust its predictions is still missing. We derive principled uncertainty quantification for fine-tuned LLMs with posterior approximations using computationally efficient low-rank adaptation ensembles. We analyze three common multiple-choice datasets using low-rank adaptation ensembles based on Mistral-7b, and draw quantitative and qualitative conclusions on their perceived complexity and model efficacy on the different target domains during and after fine-tuning. In particular, backed by the numerical experiments, we hypothesise about signals from entropic uncertainty measures for data domains that are inherently difficult for a given architecture to learn.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#38754;&#21521;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#30340;&#23450;&#21046;&#28151;&#21512;&#31934;&#24230;&#20302;&#20110;8&#20301;&#37327;&#21270;&#26041;&#26696;&#65292;&#20351;&#29992;&#36951;&#20256;&#31639;&#27861;&#20248;&#21270;&#27169;&#22411;&#23610;&#23544;&#21644;&#20934;&#30830;&#24615;&#65292;&#22312;&#22235;&#20010;&#19981;&#21516;&#39034;&#24207;&#20219;&#21153;&#19978;&#23637;&#31034;&#28151;&#21512;&#31934;&#24230;&#35299;&#20915;&#26041;&#26696;&#20248;&#20110;&#21516;&#36136;&#31934;&#24230;&#35299;&#20915;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#27169;&#22411;&#23610;&#23544;&#32553;&#20943;25%&#33267;55%&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.12263</link><description>&lt;p&gt;
&#38754;&#21521;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#30340;&#23450;&#21046;&#28151;&#21512;&#31934;&#24230;&#20302;&#20110;8&#20301;&#37327;&#21270;&#26041;&#26696;&#30340;&#30740;&#31350;&#65292;&#20351;&#29992;&#36951;&#20256;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards a tailored mixed-precision sub-8bit quantization scheme for Gated Recurrent Units using Genetic Algorithms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12263
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#38754;&#21521;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#30340;&#23450;&#21046;&#28151;&#21512;&#31934;&#24230;&#20302;&#20110;8&#20301;&#37327;&#21270;&#26041;&#26696;&#65292;&#20351;&#29992;&#36951;&#20256;&#31639;&#27861;&#20248;&#21270;&#27169;&#22411;&#23610;&#23544;&#21644;&#20934;&#30830;&#24615;&#65292;&#22312;&#22235;&#20010;&#19981;&#21516;&#39034;&#24207;&#20219;&#21153;&#19978;&#23637;&#31034;&#28151;&#21512;&#31934;&#24230;&#35299;&#20915;&#26041;&#26696;&#20248;&#20110;&#21516;&#36136;&#31934;&#24230;&#35299;&#20915;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#27169;&#22411;&#23610;&#23544;&#32553;&#20943;25%&#33267;55%&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21387;&#32553;&#25216;&#26415;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#22312;&#36229;&#20302;&#21151;&#32791;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#37096;&#32626;&#36825;&#20123;&#27169;&#22411;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29305;&#21035;&#26159;&#38024;&#23545;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#65288;GRU&#65289;&#30340;&#37327;&#21270;&#26041;&#26696;&#24456;&#38590;&#35843;&#25972;&#65292;&#22240;&#20026;&#23427;&#20204;&#20381;&#36182;&#20110;&#20869;&#37096;&#29366;&#24577;&#65292;&#26080;&#27861;&#20805;&#20998;&#20174;&#20302;&#20110;8&#20301;&#30340;&#37327;&#21270;&#20013;&#33719;&#30410;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22359;&#21270;&#25972;&#25968;&#37327;&#21270;&#26041;&#26696;&#65292;&#20854;&#20013;&#27599;&#20010;&#36816;&#31639;&#31526;&#30340;&#20301;&#23485;&#21487;&#20197;&#29420;&#31435;&#36873;&#25321;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#37319;&#29992;&#36951;&#20256;&#31639;&#27861;&#65288;GA&#65289;&#26469;&#25506;&#32034;&#21487;&#33021;&#20301;&#23485;&#30340;&#24222;&#22823;&#25628;&#32034;&#31354;&#38388;&#65292;&#21516;&#26102;&#20248;&#21270;&#27169;&#22411;&#23610;&#23544;&#21644;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#19981;&#21516;&#30340;&#39034;&#24207;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#34920;&#26126;&#28151;&#21512;&#31934;&#24230;&#35299;&#20915;&#26041;&#26696;&#22312;Pareto&#25928;&#29575;&#26041;&#38754;&#36229;&#36807;&#21516;&#36136;&#31934;&#24230;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#25105;&#20204;&#30340;&#32467;&#26524;&#20013;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#27169;&#22411;&#23610;&#23544;&#22312;25%&#33267;55%&#20043;&#38388;&#30340;&#32553;&#20943;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#19982;t&#30456;&#24403;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12263v1 Announce Type: new  Abstract: Despite the recent advances in model compression techniques for deep neural networks, deploying such models on ultra-low-power embedded devices still proves challenging. In particular, quantization schemes for Gated Recurrent Units (GRU) are difficult to tune due to their dependence on an internal state, preventing them from fully benefiting from sub-8bit quantization. In this work, we propose a modular integer quantization scheme for GRUs where the bit width of each operator can be selected independently. We then employ Genetic Algorithms (GA) to explore the vast search space of possible bit widths, simultaneously optimising for model size and accuracy. We evaluate our methods on four different sequential tasks and demonstrate that mixed-precision solutions exceed homogeneous-precision ones in terms of Pareto efficiency. In our results, we achieve a model size reduction between 25% and 55% while maintaining an accuracy comparable with t
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#36710;&#32852;&#32593;&#20013;&#36827;&#34892;&#38750;&#27491;&#20132;&#26102;&#25928;&#26368;&#20248;&#20449;&#24687;&#20256;&#25773;&#65292;&#20197;&#20943;&#23569;&#20449;&#24687;&#26102;&#25928;&#21644;&#20256;&#36755;&#21151;&#32791;&#12290;</title><link>https://arxiv.org/abs/2402.12260</link><description>&lt;p&gt;
&#38750;&#27491;&#20132;&#26102;&#25928;&#26368;&#20248;&#20449;&#24687;&#20256;&#25773;&#22312;&#36710;&#32852;&#32593;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#31181;&#20803;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Non-orthogonal Age-Optimal Information Dissemination in Vehicular Networks: A Meta Multi-Objective Reinforcement Learning Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12260
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#36710;&#32852;&#32593;&#20013;&#36827;&#34892;&#38750;&#27491;&#20132;&#26102;&#25928;&#26368;&#20248;&#20449;&#24687;&#20256;&#25773;&#65292;&#20197;&#20943;&#23569;&#20449;&#24687;&#26102;&#25928;&#21644;&#20256;&#36755;&#21151;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#22312;&#36710;&#32852;&#32593;&#20013;&#20943;&#23569;&#20449;&#24687;&#26102;&#25928;&#65288;AoI&#65289;&#21644;&#20256;&#36755;&#21151;&#32791;&#65292;&#19968;&#20010;&#36947;&#36335;&#36793;&#32536;&#21333;&#20803;&#65288;RSU&#65289;&#21450;&#26102;&#21521;&#36710;&#36742;&#25552;&#20379;&#19968;&#32452;&#29289;&#29702;&#36807;&#31243;&#30340;&#26356;&#26032;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#22522;&#20110;RSU&#30340;&#21472;&#21152;&#28040;&#24687;&#20256;&#36755;&#21644;&#36710;&#36742;&#19978;&#30340;&#36830;&#32493;&#24178;&#25200;&#28040;&#38500;&#30340;&#38750;&#27491;&#20132;&#22810;&#27169;&#24335;&#20449;&#24687;&#20256;&#25773;&#12290;&#25152;&#36848;&#38382;&#39064;&#26159;&#19968;&#20010;&#22810;&#30446;&#26631;&#28151;&#21512;&#25972;&#25968;&#38750;&#32447;&#24615;&#35268;&#21010;&#38382;&#39064;&#65292;&#22240;&#27492;&#65292;&#33719;&#24471;&#24085;&#32047;&#25176;&#26368;&#20248;&#21069;&#27839;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21033;&#29992;&#21152;&#26435;&#21644;&#26041;&#27861;&#23558;&#22810;&#30446;&#26631;&#38382;&#39064;&#20998;&#35299;&#20026;&#19968;&#32452;&#23545;&#24212;&#20110;&#27599;&#20010;&#39044;&#23450;&#20041;&#30446;&#26631;&#20559;&#22909;&#26435;&#37325;&#30340;&#22810;&#20010;&#21333;&#30446;&#26631;&#23376;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#28151;&#21512;&#28145;&#24230;Q&#32593;&#32476;&#65288;DQN&#65289;-&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#65288;DDPG&#65289;&#27169;&#22411;&#26469;&#35299;&#20915;&#27599;&#20010;&#19982;&#39044;&#23450;&#20041;&#30446;&#26631;&#20559;&#22909;&#26435;&#37325;&#30456;&#23545;&#24212;&#30340;&#20248;&#21270;&#23376;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12260v1 Announce Type: new  Abstract: This paper considers minimizing the age-of-information (AoI) and transmit power consumption in a vehicular network, where a roadside unit (RSU) provides timely updates about a set of physical processes to vehicles. We consider non-orthogonal multi-modal information dissemination, which is based on superposed message transmission from RSU and successive interference cancellation (SIC) at vehicles. The formulated problem is a multi-objective mixed-integer nonlinear programming problem; thus, a Pareto-optimal front is very challenging to obtain. First, we leverage the weighted-sum approach to decompose the multi-objective problem into a set of multiple single-objective sub-problems corresponding to each predefined objective preference weight. Then, we develop a hybrid deep Q-network (DQN)-deep deterministic policy gradient (DDPG) model to solve each optimization sub-problem respective to predefined objective-preference weight. The DQN optim
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#36830;&#32493;&#25193;&#25955;&#36807;&#31243;&#21644;&#26144;&#23556;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20998;&#31867;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#20301;&#32622;&#36712;&#36857;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.12242</link><description>&lt;p&gt;
&#20351;&#29992;&#20998;&#31867;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#20301;&#32622;&#36712;&#36857;
&lt;/p&gt;
&lt;p&gt;
Synthetic location trajectory generation using categorical diffusion models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12242
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#36830;&#32493;&#25193;&#25955;&#36807;&#31243;&#21644;&#26144;&#23556;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20998;&#31867;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#20301;&#32622;&#36712;&#36857;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#29575;&#25193;&#25955;&#27169;&#22411;&#65288;DPMs&#65289;&#24050;&#36805;&#36895;&#21457;&#23637;&#25104;&#20026;&#27169;&#25311;&#21512;&#25104;&#25968;&#25454;&#30340;&#20027;&#35201;&#29983;&#25104;&#27169;&#22411;&#20043;&#19968;&#65292;&#20363;&#22914;&#29992;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#38899;&#39057;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25110;&#29983;&#29289;&#20998;&#23376;&#29983;&#25104;&#12290;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;DPMs&#26469;&#29983;&#25104;&#21512;&#25104;&#20010;&#20307;&#20301;&#32622;&#36712;&#36857;&#65288;ILTs&#65289;&#65292;ILTs&#26159;&#20195;&#34920;&#20010;&#20307;&#35775;&#38382;&#30340;&#29289;&#29702;&#20301;&#32622;&#30340;&#21464;&#37327;&#24207;&#21015;&#65292;&#23545;&#20110;&#31227;&#21160;&#30740;&#31350;&#33267;&#20851;&#37325;&#35201;&#65292;&#26377;&#21161;&#20110;&#29702;&#35299;&#20154;&#32676;&#30340;&#31227;&#21160;&#34892;&#20026;&#24182;&#26368;&#32456;&#20026;&#25919;&#27835;&#20915;&#31574;&#25552;&#20379;&#20449;&#24687;&#12290;&#25105;&#20204;&#23558;ILTs&#34920;&#31034;&#20026;&#22810;&#32500;&#20998;&#31867;&#38543;&#26426;&#21464;&#37327;&#65292;&#24182;&#24314;&#35758;&#20351;&#29992;&#36830;&#32493;DPM&#26469;&#24314;&#27169;&#23427;&#20204;&#30340;&#32852;&#21512;&#20998;&#24067;&#65292;&#39318;&#20808;&#22312;&#36830;&#32493;&#26080;&#32422;&#26463;&#31354;&#38388;&#20013;&#24212;&#29992;&#25193;&#25955;&#36807;&#31243;&#65292;&#28982;&#21518;&#23558;&#36830;&#32493;&#21464;&#37327;&#26144;&#23556;&#21040;&#31163;&#25955;&#31354;&#38388;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#26465;&#20214;&#27604;&#36739;&#21512;&#25104;&#20986;&#36924;&#30495;&#30340;ILPs&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12242v1 Announce Type: new  Abstract: Diffusion probabilistic models (DPMs) have rapidly evolved to be one of the predominant generative models for the simulation of synthetic data, for instance, for computer vision, audio, natural language processing, or biomolecule generation. Here, we propose using DPMs for the generation of synthetic individual location trajectories (ILTs) which are sequences of variables representing physical locations visited by individuals. ILTs are of major importance in mobility research to understand the mobility behavior of populations and to ultimately inform political decision-making. We represent ILTs as multi-dimensional categorical random variables and propose to model their joint distribution using a continuous DPM by first applying the diffusion process in a continuous unconstrained space and then mapping the continuous variables into a discrete space. We demonstrate that our model can synthesize realistic ILPs by comparing conditionally an
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20998;&#26512;&#20102;&#22312;&#21160;&#24577;&#31995;&#32479;&#20013;&#21033;&#29992;&#26799;&#24230;&#19979;&#38477;&#36827;&#34892;&#30417;&#30563;&#23398;&#20064;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#65292;&#24182;&#35777;&#26126;&#22312;&#19981;&#38656;&#35201;&#28023;&#37327;&#36807;&#21442;&#25968;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#26799;&#24230;&#19979;&#38477;&#21487;&#20197;&#36798;&#21040;&#26368;&#20248;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.12241</link><description>&lt;p&gt;
&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30340;&#26799;&#24230;&#19979;&#38477;&#25910;&#25947;&#24615;&#65306;&#38750;&#28176;&#36817;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Convergence of Gradient Descent for Recurrent Neural Networks: A Nonasymptotic Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12241
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20998;&#26512;&#20102;&#22312;&#21160;&#24577;&#31995;&#32479;&#20013;&#21033;&#29992;&#26799;&#24230;&#19979;&#38477;&#36827;&#34892;&#30417;&#30563;&#23398;&#20064;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#65292;&#24182;&#35777;&#26126;&#22312;&#19981;&#38656;&#35201;&#28023;&#37327;&#36807;&#21442;&#25968;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#26799;&#24230;&#19979;&#38477;&#21487;&#20197;&#36798;&#21040;&#26368;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20998;&#26512;&#22312;&#30417;&#30563;&#23398;&#20064;&#35774;&#32622;&#19979;&#21033;&#29992;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#22312;&#21160;&#24577;&#31995;&#32479;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#35777;&#26126;&#26799;&#24230;&#19979;&#38477;&#21487;&#20197;&#22312;\emph{&#19981;}&#38656;&#35201;&#28023;&#37327;&#36807;&#21442;&#25968;&#21270;&#30340;&#24773;&#20917;&#19979;&#36798;&#21040;&#26368;&#20248;&#24615;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#38750;&#28176;&#36817;&#24615;&#20998;&#26512;&#65292;(i)&#21033;&#29992;&#24207;&#21015;&#38271;&#24230;$T$&#12289;&#26679;&#26412;&#22823;&#23567;$n$&#21644;&#29615;&#22659;&#32500;&#24230;$d$&#32473;&#20986;&#20102;&#32593;&#32476;&#22823;&#23567;$m$&#21644;&#36845;&#20195;&#22797;&#26434;&#24230;$\tau$&#30340;&#23574;&#38160;&#30028;&#38480;&#65292;(ii)&#30830;&#23450;&#20102;&#21160;&#24577;&#31995;&#32479;&#20013;&#38271;&#26399;&#20381;&#36182;&#23545;&#25910;&#25947;&#21644;&#32593;&#32476;&#23485;&#24230;&#30028;&#38480;&#30340;&#26174;&#30528;&#24433;&#21709;&#65292;&#36825;&#20123;&#30028;&#38480;&#30001;&#28608;&#27963;&#20989;&#25968;&#30340;Lipschitz&#36830;&#32493;&#24615;&#20915;&#23450;&#30340;&#25130;&#27490;&#28857;&#26469;&#34920;&#24449;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#19968;&#20998;&#26512;&#25581;&#31034;&#20102;&#19968;&#20010;&#22949;&#21892;&#21021;&#22987;&#21270;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#22312;$n$&#20010;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#36890;&#36807;&#32593;&#32476;&#22823;&#23567;$m$&#20165;&#23545;&#25968;&#22320;&#38543;$n$&#25193;&#23637;&#23601;&#36798;&#21040;&#26368;&#20248;&#24615;&#12290;&#36825;&#19982;&#20197;&#21069;&#30340;&#24037;&#20316;&#24418;&#25104;&#40092;&#26126;&#23545;&#27604;&#65292;&#21069;&#32773;&#38656;&#35201;&#39640;&#38454;&#22810;&#39033;&#24335;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12241v1 Announce Type: new  Abstract: We analyze recurrent neural networks trained with gradient descent in the supervised learning setting for dynamical systems, and prove that gradient descent can achieve optimality \emph{without} massive overparameterization. Our in-depth nonasymptotic analysis (i) provides sharp bounds on the network size $m$ and iteration complexity $\tau$ in terms of the sequence length $T$, sample size $n$ and ambient dimension $d$, and (ii) identifies the significant impact of long-term dependencies in the dynamical system on the convergence and network width bounds characterized by a cutoff point that depends on the Lipschitz continuity of the activation function. Remarkably, this analysis reveals that an appropriately-initialized recurrent neural network trained with $n$ samples can achieve optimality with a network size $m$ that scales only logarithmically with $n$. This sharply contrasts with the prior works that require high-order polynomial dep
&lt;/p&gt;</description></item><item><title>BEARS&#26159;&#19968;&#31181;&#38598;&#25104;&#25216;&#26415;&#65292;&#21487;&#20197;&#35753;&#31070;&#32463;&#31526;&#21495;&#27169;&#22411;&#24847;&#35782;&#21040;&#23427;&#20204;&#23398;&#20064;&#30340;&#27010;&#24565;&#30340;&#35821;&#20041;&#27169;&#31946;&#24615;&#65292;&#24110;&#21161;&#29992;&#25143;&#35782;&#21035;&#21644;&#24576;&#30097;&#20302;&#36136;&#37327;&#27010;&#24565;&#12290;</title><link>https://arxiv.org/abs/2402.12240</link><description>&lt;p&gt;
BEARS &#35753;&#31070;&#32463;&#31526;&#21495;&#27169;&#22411;&#24847;&#35782;&#21040;&#23427;&#20204;&#30340;&#25512;&#29702;&#25463;&#24452;
&lt;/p&gt;
&lt;p&gt;
BEARS Make Neuro-Symbolic Models Aware of their Reasoning Shortcuts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12240
&lt;/p&gt;
&lt;p&gt;
BEARS&#26159;&#19968;&#31181;&#38598;&#25104;&#25216;&#26415;&#65292;&#21487;&#20197;&#35753;&#31070;&#32463;&#31526;&#21495;&#27169;&#22411;&#24847;&#35782;&#21040;&#23427;&#20204;&#23398;&#20064;&#30340;&#27010;&#24565;&#30340;&#35821;&#20041;&#27169;&#31946;&#24615;&#65292;&#24110;&#21161;&#29992;&#25143;&#35782;&#21035;&#21644;&#24576;&#30097;&#20302;&#36136;&#37327;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Neuro-Symbolic (NeSy)&#39044;&#27979;&#22120;&#31526;&#21512;&#31526;&#21495;&#30693;&#35782;-&#32534;&#30721;&#65292;&#20363;&#22914;&#23433;&#20840;&#32422;&#26463;&#65292;&#21487;&#33021;&#21463;&#21040;&#25512;&#29702;&#25463;&#24452;&#65288;RSs&#65289;&#30340;&#24433;&#21709;&#65306;&#23427;&#20204;&#36890;&#36807;&#21033;&#29992;&#38750;&#39044;&#26399;&#30340;&#35821;&#20041;&#26469;&#23398;&#20064;&#19982;&#31526;&#21495;&#30693;&#35782;&#19968;&#33268;&#30340;&#27010;&#24565;&#12290; RSs&#25439;&#23475;&#20102;&#21487;&#38752;&#24615;&#21644;&#27867;&#21270;&#65292;&#24182;&#19988;&#27491;&#22914;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#25152;&#23637;&#31034;&#30340;&#65292;&#23427;&#20204;&#19982;NeSy&#27169;&#22411;&#23545;&#39044;&#27979;&#27010;&#24565;&#36807;&#20110;&#33258;&#20449;&#26377;&#20851;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#21807;&#19968;&#21487;&#38752;&#30340;&#32531;&#35299;&#31574;&#30053;&#38656;&#35201;&#23545;&#27010;&#24565;&#36827;&#34892;&#26114;&#36149;&#30340;&#23494;&#38598;&#30417;&#30563;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#19981;&#26159;&#35797;&#22270;&#23436;&#20840;&#36991;&#20813;RSs&#65292;&#32780;&#26159;&#35201;&#30830;&#20445;NeSy&#27169;&#22411;&#24847;&#35782;&#21040;&#23427;&#20204;&#23398;&#20064;&#30340;&#27010;&#24565;&#30340;&#35821;&#20041;&#27169;&#31946;&#24615;&#65292;&#20174;&#32780;&#20351;&#29992;&#25143;&#33021;&#22815;&#35782;&#21035;&#21644;&#24576;&#30097;&#20302;&#36136;&#37327;&#30340;&#27010;&#24565;&#12290;&#20174;&#19977;&#20010;&#31616;&#21333;&#30340;&#35774;&#35745;&#35201;&#27714;&#24320;&#22987;&#65292;&#25105;&#20204;&#24471;&#20986;bears&#65288;BE Aware of Reasoning Shortcuts&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#38598;&#25104;&#25216;&#26415;&#65292;&#21487;&#20197;&#26657;&#20934;&#27169;&#22411;&#30340;&#27010;&#24565;&#32423;&#20449;&#24515;&#65292;&#32780;&#19981;&#20250;&#25439;&#23475;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12240v1 Announce Type: cross  Abstract: Neuro-Symbolic (NeSy) predictors that conform to symbolic knowledge - encoding, e.g., safety constraints - can be affected by Reasoning Shortcuts (RSs): They learn concepts consistent with the symbolic knowledge by exploiting unintended semantics. RSs compromise reliability and generalization and, as we show in this paper, they are linked to NeSy models being overconfident about the predicted concepts. Unfortunately, the only trustworthy mitigation strategy requires collecting costly dense supervision over the concepts. Rather than attempting to avoid RSs altogether, we propose to ensure NeSy models are aware of the semantic ambiguity of the concepts they learn, thus enabling their users to identify and distrust low-quality concepts. Starting from three simple desiderata, we derive bears (BE Aware of Reasoning Shortcuts), an ensembling technique that calibrates the model's concept-level confidence without compromising prediction accura
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#25429;&#25417;&#20869;&#23481;&#23457;&#26680;&#20013;&#20154;&#24037;&#26234;&#33021;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.12237</link><description>&lt;p&gt;
&#23398;&#20064;&#22312;&#20869;&#23481;&#23457;&#26680;&#20013;&#25512;&#36831;&#65306;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21327;&#21516;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Learning to Defer in Content Moderation: The Human-AI Interplay
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12237
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#25429;&#25417;&#20869;&#23481;&#23457;&#26680;&#20013;&#20154;&#24037;&#26234;&#33021;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25104;&#21151;&#30340;&#22312;&#32447;&#24179;&#21488;&#20869;&#23481;&#23457;&#26680;&#20381;&#36182;&#20110;&#20154;&#24037;&#26234;&#33021;&#21327;&#21516;&#26041;&#27861;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#25429;&#25417;&#20869;&#23481;&#23457;&#26680;&#20013;&#20154;&#24037;&#26234;&#33021;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#31639;&#27861;&#35266;&#23519;&#21040;&#21363;&#23558;&#21457;&#24067;&#30340;&#24086;&#23376;&#30340;&#32972;&#26223;&#20449;&#24687;&#65292;&#20570;&#20986;&#20998;&#31867;&#21644;&#20934;&#20837;&#20915;&#31574;&#65292;&#24182;&#23433;&#25490;&#24086;&#23376;&#36827;&#34892;&#20154;&#24037;&#23457;&#26680;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12237v1 Announce Type: cross  Abstract: Successful content moderation in online platforms relies on a human-AI collaboration approach. A typical heuristic estimates the expected harmfulness of a post and uses fixed thresholds to decide whether to remove it and whether to send it for human review. This disregards the prediction uncertainty, the time-varying element of human review capacity and post arrivals, and the selective sampling in the dataset (humans only review posts filtered by the admission algorithm).   In this paper, we introduce a model to capture the human-AI interplay in content moderation. The algorithm observes contextual information for incoming posts, makes classification and admission decisions, and schedules posts for human review. Only admitted posts receive human reviews on their harmfulness. These reviews help educate the machine-learning algorithms but are delayed due to congestion in the human review system. The classical learning-theoretic way to ca
&lt;/p&gt;</description></item><item><title>&#26368;&#23567;&#26435;&#38480;&#23398;&#20064;&#23384;&#22312;&#19968;&#20010;&#22522;&#26412;&#30340;&#26435;&#34913;&#65292;&#21363;&#34920;&#31034;&#23545;&#20110;&#32473;&#23450;&#20219;&#21153;&#30340;&#23454;&#29992;&#24615;&#21644;&#20854;&#27844;&#28431;&#21040;&#20219;&#21153;&#22806;&#23646;&#24615;&#20043;&#38388;&#23384;&#22312;&#26080;&#27861;&#36991;&#20813;&#30340;&#26435;&#34913;&#12290;</title><link>https://arxiv.org/abs/2402.12235</link><description>&lt;p&gt;
&#26368;&#23567;&#26435;&#38480;&#23398;&#20064;&#30340;&#22522;&#26412;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
The Fundamental Limits of Least-Privilege Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12235
&lt;/p&gt;
&lt;p&gt;
&#26368;&#23567;&#26435;&#38480;&#23398;&#20064;&#23384;&#22312;&#19968;&#20010;&#22522;&#26412;&#30340;&#26435;&#34913;&#65292;&#21363;&#34920;&#31034;&#23545;&#20110;&#32473;&#23450;&#20219;&#21153;&#30340;&#23454;&#29992;&#24615;&#21644;&#20854;&#27844;&#28431;&#21040;&#20219;&#21153;&#22806;&#23646;&#24615;&#20043;&#38388;&#23384;&#22312;&#26080;&#27861;&#36991;&#20813;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#23569;&#26435;&#38480;&#23398;&#20064;&#30340;&#25215;&#35834;&#26159;&#25214;&#21040;&#23545;&#20110;&#23398;&#20064;&#20219;&#21153;&#26377;&#29992;&#30340;&#29305;&#24449;&#34920;&#31034;&#65292;&#20294;&#21516;&#26102;&#38450;&#27490;&#25512;&#26029;&#19982;&#35813;&#20219;&#21153;&#26080;&#20851;&#30340;&#20219;&#20309;&#25935;&#24863;&#20449;&#24687;&#65292;&#36825;&#19968;&#28857;&#38750;&#24120;&#21560;&#24341;&#20154;&#12290;&#28982;&#32780;&#65292;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#36825;&#20010;&#27010;&#24565;&#21482;&#26159;&#20197;&#38750;&#27491;&#24335;&#30340;&#26041;&#24335;&#38472;&#36848;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20173;&#28982;&#19981;&#28165;&#26970;&#25105;&#20204;&#26159;&#21542;&#20197;&#21450;&#22914;&#20309;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#20026;&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#23567;&#26435;&#38480;&#21407;&#21017;&#25552;&#20379;&#20102;&#24418;&#24335;&#21270;&#65292;&#24182;&#25551;&#36848;&#20102;&#20854;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#34920;&#31034;&#23545;&#20110;&#32473;&#23450;&#20219;&#21153;&#30340;&#23454;&#29992;&#24615;&#21644;&#20854;&#27844;&#28431;&#21040;&#39044;&#26399;&#20219;&#21153;&#20043;&#22806;&#30340;&#23646;&#24615;&#20043;&#38388;&#23384;&#22312;&#22522;&#26412;&#26435;&#34913;&#65306;&#19981;&#21487;&#33021;&#23398;&#20064;&#21040;&#23545;&#20110;&#39044;&#26399;&#20219;&#21153;&#20855;&#26377;&#39640;&#23454;&#29992;&#24615;&#30340;&#34920;&#31034;&#65292;&#21516;&#26102;&#21448;&#38450;&#27490;&#25512;&#26029;&#38500;&#20219;&#21153;&#26631;&#31614;&#26412;&#36523;&#20043;&#22806;&#30340;&#20219;&#20309;&#23646;&#24615;&#12290;&#36825;&#31181;&#26435;&#34913;&#26159;&#26080;&#35770;&#20351;&#29992;&#20309;&#31181;&#25216;&#26415;&#26469;&#23398;&#20064;&#20135;&#29983;&#36825;&#20123;&#34920;&#31034;&#30340;&#29305;&#24449;&#26144;&#23556;&#37117;&#26159;&#25104;&#31435;&#30340;&#12290;&#25105;&#20204;&#32463;&#39564;&#24615;&#22320;&#39564;&#35777;&#20102;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12235v1 Announce Type: new  Abstract: The promise of least-privilege learning -- to find feature representations that are useful for a learning task but prevent inference of any sensitive information unrelated to this task -- is highly appealing. However, so far this concept has only been stated informally. It thus remains an open question whether and how we can achieve this goal. In this work, we provide the first formalisation of the least-privilege principle for machine learning and characterise its feasibility. We prove that there is a fundamental trade-off between a representation's utility for a given task and its leakage beyond the intended task: it is not possible to learn representations that have high utility for the intended task but, at the same time prevent inference of any attribute other than the task label itself. This trade-off holds regardless of the technique used to learn the feature mappings that produce these representations. We empirically validate thi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#30340;&#26080;&#30417;&#30563;&#20108;&#21449;&#26641;&#29992;&#20110;&#32858;&#31867;&#65292;&#31216;&#20026;Kauri&#65292;&#36890;&#36807;&#36138;&#23146;&#26368;&#22823;&#21270; kernel KMeans &#30446;&#26631;&#26469;&#25191;&#34892;&#65292;&#26080;&#38656;&#23450;&#20041;&#36136;&#24515;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20854;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.12232</link><description>&lt;p&gt;
&#23558; Kernel KMeans &#32858;&#31867;&#25286;&#20998;&#29992;&#20110;&#31471;&#21040;&#31471;&#26080;&#30417;&#30563;&#20915;&#31574;&#26641;
&lt;/p&gt;
&lt;p&gt;
Kernel KMeans clustering splits for end-to-end unsupervised decision trees
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12232
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#30340;&#26080;&#30417;&#30563;&#20108;&#21449;&#26641;&#29992;&#20110;&#32858;&#31867;&#65292;&#31216;&#20026;Kauri&#65292;&#36890;&#36807;&#36138;&#23146;&#26368;&#22823;&#21270; kernel KMeans &#30446;&#26631;&#26469;&#25191;&#34892;&#65292;&#26080;&#38656;&#23450;&#20041;&#36136;&#24515;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20854;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26641;&#26159;&#33719;&#21462;&#23545;&#30456;&#23545;&#36739;&#23567;&#25968;&#25454;&#38598;&#36827;&#34892;&#21487;&#35299;&#37322;&#39044;&#27979;&#30340;&#20415;&#21033;&#27169;&#22411;&#12290;&#34429;&#28982;&#26377;&#24456;&#22810;&#20851;&#20110;&#30417;&#30563;&#23398;&#20064;&#20013;&#31471;&#21040;&#31471;&#26500;&#24314;&#36825;&#31181;&#26641;&#30340;&#25552;&#35758;&#65292;&#20294;&#22312;&#27809;&#26377;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#29992;&#20110;&#32858;&#31867;&#30340;&#26641;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#25361;&#25112;&#12290;&#22823;&#22810;&#25968;&#20316;&#21697;&#20027;&#35201;&#38598;&#20013;&#20110;&#20351;&#29992;&#26641;&#26469;&#35299;&#37322;&#21478;&#19968;&#20010;&#32858;&#31867;&#31639;&#27861;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#22312;&#36825;&#37324;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#30340;&#26080;&#30417;&#30563;&#20108;&#21449;&#26641;&#29992;&#20110;&#32858;&#31867;&#65306;Kauri&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#36138;&#23146;&#26368;&#22823;&#21270; kernel KMeans &#30446;&#26631;&#26469;&#25191;&#34892;&#65292;&#32780;&#26080;&#38656;&#23450;&#20041;&#36136;&#24515;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23558;&#27492;&#27169;&#22411;&#19982;&#26368;&#36817;&#30340;&#26080;&#30417;&#30563;&#26641;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#23637;&#31034;&#24403;&#20351;&#29992;&#32447;&#24615;&#26680;&#26102;&#65292;Kauri &#30340;&#24615;&#33021;&#30456;&#21516;&#12290;&#23545;&#20110;&#20854;&#20182;&#20869;&#26680;&#65292;Kauri &#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#34920;&#29616;&#20248;&#20110;&#20869;&#26680; KMeans &#21644; CART &#20915;&#31574;&#26641;&#30340;&#20018;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12232v1 Announce Type: cross  Abstract: Trees are convenient models for obtaining explainable predictions on relatively small datasets. Although there are many proposals for the end-to-end construction of such trees in supervised learning, learning a tree end-to-end for clustering without labels remains an open challenge. As most works focus on interpreting with trees the result of another clustering algorithm, we present here a novel end-to-end trained unsupervised binary tree for clustering: Kauri. This method performs a greedy maximisation of the kernel KMeans objective without requiring the definition of centroids. We compare this model on multiple datasets with recent unsupervised trees and show that Kauri performs identically when using a linear kernel. For other kernels, Kauri often outperforms the concatenation of kernel KMeans and a CART decision tree.
&lt;/p&gt;</description></item><item><title>&#25193;&#25955;&#22238;&#28779;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#21487;&#25913;&#21892;&#27010;&#29575;&#25968;&#20540;&#26041;&#27861;&#22312;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#20013;&#30340;&#21442;&#25968;&#20248;&#21270;&#25910;&#25947;&#24615;&#65292;&#23454;&#29616;&#23545;&#22797;&#26434;&#21160;&#24577;&#31995;&#32479;&#20013;&#21442;&#25968;&#30340;&#21487;&#38752;&#20272;&#35745;</title><link>https://arxiv.org/abs/2402.12231</link><description>&lt;p&gt;
&#25193;&#25955;&#22238;&#28779;&#25913;&#21892;&#27010;&#29575;&#31215;&#20998;&#22120;&#23545;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#21442;&#25968;&#20272;&#35745;&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Diffusion Tempering Improves Parameter Estimation with Probabilistic Integrators for Ordinary Differential Equations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12231
&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#22238;&#28779;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#21487;&#25913;&#21892;&#27010;&#29575;&#25968;&#20540;&#26041;&#27861;&#22312;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#20013;&#30340;&#21442;&#25968;&#20248;&#21270;&#25910;&#25947;&#24615;&#65292;&#23454;&#29616;&#23545;&#22797;&#26434;&#21160;&#24577;&#31995;&#32479;&#20013;&#21442;&#25968;&#30340;&#21487;&#38752;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#65288;ODEs&#65289;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#25551;&#36848;&#31185;&#23398;&#20013;&#30340;&#21160;&#24577;&#31995;&#32479;&#65292;&#20294;&#30830;&#23450;&#35299;&#37322;&#23454;&#39564;&#27979;&#37327;&#32467;&#26524;&#30340;&#21442;&#25968;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#25193;&#25955;&#22238;&#28779;&#36825;&#19968;&#26032;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#23427;&#38024;&#23545;ODEs&#20013;&#30340;&#27010;&#29575;&#25968;&#20540;&#26041;&#27861;&#65292;&#25913;&#21892;&#20102;&#26799;&#24230;&#20248;&#21270;&#21442;&#25968;&#20272;&#35745;&#30340;&#25910;&#25947;&#24615;&#12290;&#36890;&#36807;&#36845;&#20195;&#20943;&#23569;&#27010;&#29575;&#31215;&#20998;&#22120;&#30340;&#19968;&#20010;&#22122;&#22768;&#21442;&#25968;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26356;&#21487;&#38752;&#22320;&#25910;&#25947;&#21040;&#30495;&#23454;&#21442;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#20110;&#19981;&#21516;&#22797;&#26434;&#24615;&#30340;&#21160;&#24577;&#31995;&#32479;&#26159;&#26377;&#25928;&#30340;&#65292;&#24182;&#23637;&#31034;&#23427;&#23545;&#20110;&#20855;&#26377;&#23454;&#38469;&#30456;&#20851;&#21442;&#25968;&#25968;&#37327;&#30340;Hodgkin-Huxley&#27169;&#22411;&#33719;&#24471;&#21487;&#38752;&#30340;&#21442;&#25968;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12231v1 Announce Type: new  Abstract: Ordinary differential equations (ODEs) are widely used to describe dynamical systems in science, but identifying parameters that explain experimental measurements is challenging. In particular, although ODEs are differentiable and would allow for gradient-based parameter optimization, the nonlinear dynamics of ODEs often lead to many local minima and extreme sensitivity to initial conditions. We therefore propose diffusion tempering, a novel regularization technique for probabilistic numerical methods which improves convergence of gradient-based parameter optimization in ODEs. By iteratively reducing a noise parameter of the probabilistic integrator, the proposed method converges more reliably to the true parameters. We demonstrate that our method is effective for dynamical systems of different complexity and show that it obtains reliable parameter estimates for a Hodgkin-Huxley model with a practically relevant number of parameters.
&lt;/p&gt;</description></item><item><title>AnyGPT&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#31163;&#25955;&#34920;&#31034;&#23454;&#29616;&#21508;&#31181;&#27169;&#24577;&#30340;&#32479;&#19968;&#22788;&#29702;&#65292;&#33021;&#22815;&#22312;&#19981;&#25913;&#21464;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#25110;&#35757;&#32451;&#26041;&#24335;&#30340;&#24773;&#20917;&#19979;&#31283;&#23450;&#35757;&#32451;&#65292;&#20026;&#26032;&#27169;&#24577;&#30340;&#26080;&#32541;&#25972;&#21512;&#25552;&#20379;&#20102;&#21487;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.12226</link><description>&lt;p&gt;
AnyGPT&#65306;&#32479;&#19968;&#30340;&#22810;&#27169;&#24335;&#31163;&#25955;&#24207;&#21015;&#24314;&#27169;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12226
&lt;/p&gt;
&lt;p&gt;
AnyGPT&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#31163;&#25955;&#34920;&#31034;&#23454;&#29616;&#21508;&#31181;&#27169;&#24577;&#30340;&#32479;&#19968;&#22788;&#29702;&#65292;&#33021;&#22815;&#22312;&#19981;&#25913;&#21464;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#25110;&#35757;&#32451;&#26041;&#24335;&#30340;&#24773;&#20917;&#19979;&#31283;&#23450;&#35757;&#32451;&#65292;&#20026;&#26032;&#27169;&#24577;&#30340;&#26080;&#32541;&#25972;&#21512;&#25552;&#20379;&#20102;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102; AnyGPT&#65292;&#36825;&#26159;&#19968;&#20010;&#20219;&#24847;&#22810;&#27169;&#24335;&#35821;&#35328;&#27169;&#22411;&#65292;&#21033;&#29992;&#31163;&#25955;&#34920;&#31034;&#32479;&#19968;&#22788;&#29702;&#21508;&#31181;&#27169;&#24577;&#65292;&#21253;&#25324;&#35821;&#38899;&#12289;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#38899;&#20048;&#12290;AnyGPT &#21487;&#20197;&#31283;&#23450;&#35757;&#32451;&#65292;&#26080;&#38656;&#23545;&#24403;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26550;&#26500;&#25110;&#35757;&#32451;&#33539;&#24335;&#36827;&#34892;&#20219;&#20309;&#25913;&#21160;&#12290;&#30456;&#21453;&#65292;&#23427;&#20165;&#20381;&#36182;&#20110;&#25968;&#25454;&#32423;&#39044;&#22788;&#29702;&#65292;&#20419;&#36827;&#20102;&#26032;&#27169;&#24577;&#30340;&#26080;&#32541;&#38598;&#25104;&#21040;LLM&#20013;&#65292;&#31867;&#20284;&#20110;&#26032;&#35821;&#35328;&#30340;&#25972;&#21512;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22810;&#27169;&#24335;&#25991;&#26412;&#20013;&#24515;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#22810;&#27169;&#24335;&#23545;&#40784;&#39044;&#35757;&#32451;&#12290;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#65292;&#25105;&#20204;&#21512;&#25104;&#20102;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#20219;&#24847;&#22810;&#27169;&#24335;&#25351;&#20196;&#25968;&#25454;&#38598;&#12290;&#23427;&#21253;&#25324;108k&#20010;&#22810;&#36718;&#23545;&#35805;&#31034;&#20363;&#65292;&#31934;&#32454;&#22320;&#20132;&#32455;&#21508;&#31181;&#27169;&#24577;&#65292;&#20174;&#32780;&#20351;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#22810;&#27169;&#24577;&#36755;&#20837;&#21644;&#36755;&#20986;&#30340;&#20219;&#24847;&#32452;&#21512;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;AnyGPT&#33021;&#22815;&#20419;&#36827;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12226v1 Announce Type: cross  Abstract: We introduce AnyGPT, an any-to-any multimodal language model that utilizes discrete representations for the unified processing of various modalities, including speech, text, images, and music. AnyGPT can be trained stably without any alterations to the current large language model (LLM) architecture or training paradigms. Instead, it relies exclusively on data-level preprocessing, facilitating the seamless integration of new modalities into LLMs, akin to the incorporation of new languages. We build a multimodal text-centric dataset for multimodal alignment pre-training. Utilizing generative models, we synthesize the first large-scale any-to-any multimodal instruction dataset. It consists of 108k samples of multi-turn conversations that intricately interweave various modalities, thus equipping the model to handle arbitrary combinations of multimodal inputs and outputs. Experimental results demonstrate that AnyGPT is capable of facilitat
&lt;/p&gt;</description></item><item><title>CovRL&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35206;&#30422;&#21453;&#39304;&#65292;&#36890;&#36807;&#26500;&#24314;&#21152;&#26435;&#35206;&#30422;&#26144;&#23556;&#24182;&#24212;&#29992;&#20110;&#22522;&#20110;LLM&#30340;&#21464;&#24322;&#22120;&#65292;&#20197;&#25552;&#21319;&#23545;JavaScript&#24341;&#25806;&#30340;&#27169;&#31946;&#27979;&#35797;&#25928;&#26524;</title><link>https://arxiv.org/abs/2402.12222</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#35206;&#30422;&#24341;&#23548;&#30340;&#24378;&#21270;&#23398;&#20064;&#23545;LLM&#22522;&#30784;&#21464;&#24322;&#36827;&#34892;JavaScript&#24341;&#25806;&#27169;&#31946;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
CovRL: Fuzzing JavaScript Engines with Coverage-Guided Reinforcement Learning for LLM-based Mutation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12222
&lt;/p&gt;
&lt;p&gt;
CovRL&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35206;&#30422;&#21453;&#39304;&#65292;&#36890;&#36807;&#26500;&#24314;&#21152;&#26435;&#35206;&#30422;&#26144;&#23556;&#24182;&#24212;&#29992;&#20110;&#22522;&#20110;LLM&#30340;&#21464;&#24322;&#22120;&#65292;&#20197;&#25552;&#21319;&#23545;JavaScript&#24341;&#25806;&#30340;&#27169;&#31946;&#27979;&#35797;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#31946;&#27979;&#35797;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#21457;&#29616;&#38169;&#35823;&#30340;&#25216;&#26415;&#65292;&#20294;&#22312;&#20687;&#38656;&#35201;&#31934;&#30830;&#35821;&#27861;&#36755;&#20837;&#30340;JavaScript&#24341;&#25806;&#36825;&#26679;&#30340;&#22797;&#26434;&#31995;&#32479;&#20013;&#24456;&#38590;&#24212;&#29992;&#12290;&#26368;&#36817;&#65292;&#30740;&#31350;&#20154;&#21592;&#37319;&#29992;&#35821;&#35328;&#27169;&#22411;&#23545;&#27169;&#31946;&#27979;&#35797;&#20013;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#21464;&#24322;&#36827;&#34892;&#20102;&#25913;&#36827;&#20197;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#25216;&#26415;&#22312;&#21033;&#29992;&#35206;&#30422;&#24341;&#23548;&#36827;&#34892;&#27169;&#31946;&#27979;&#35797;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#36890;&#24120;&#20197;&#40657;&#30418;&#26041;&#24335;&#25191;&#34892;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CovRL&#65288;&#22522;&#20110;&#35206;&#30422;&#24341;&#23548;&#30340;&#24378;&#21270;&#23398;&#20064;&#65289;&#30340;&#26032;&#25216;&#26415;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20174;&#35206;&#30422;&#21453;&#39304;&#20013;&#24471;&#21040;&#30340;&#24378;&#21270;&#23398;&#20064;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#30340;&#27169;&#31946;&#22120;CovRL-Fuzz&#36890;&#36807;&#21033;&#29992;&#35789;&#39057;-&#36870;&#25991;&#26723;&#39057;&#29575;&#65288;TF-IDF&#65289;&#26041;&#27861;&#23558;&#35206;&#30422;&#21453;&#39304;&#30452;&#25509;&#38598;&#25104;&#21040;LLM&#20013;&#65292;&#26500;&#24314;&#21152;&#26435;&#35206;&#30422;&#26144;&#23556;&#12290;&#35813;&#26144;&#23556;&#22312;&#35745;&#31639;&#27169;&#31946;&#27979;&#35797;&#22870;&#21169;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#28982;&#21518;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#20110;&#22522;&#20110;LLM&#30340;&#21464;&#24322;&#22120;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;CovRL-Fuzz&#23454;&#29616;&#20102;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12222v1 Announce Type: cross  Abstract: Fuzzing is an effective bug-finding technique but it struggles with complex systems like JavaScript engines that demand precise grammatical input. Recently, researchers have adopted language models for context-aware mutation in fuzzing to address this problem. However, existing techniques are limited in utilizing coverage guidance for fuzzing, which is rather performed in a black-box manner. This paper presents a novel technique called CovRL (Coverage-guided Reinforcement Learning) that combines Large Language Models (LLMs) with reinforcement learning from coverage feedback. Our fuzzer, CovRL-Fuzz, integrates coverage feedback directly into the LLM by leveraging the Term Frequency-Inverse Document Frequency (TF-IDF) method to construct a weighted coverage map. This map is key in calculating the fuzzing reward, which is then applied to the LLM-based mutator through reinforcement learning. CovRL-Fuzz, through this approach, enables the g
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#36125;&#21494;&#26031;&#23398;&#20064;&#25216;&#26415;&#24212;&#29992;&#20110;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65292;&#20197;&#38450;&#27490;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#23454;&#29616;&#20102;&#39044;&#35757;&#32451;&#30693;&#35782;&#30340;&#20445;&#30041;&#65292;&#24182;&#22312;&#35821;&#35328;&#24314;&#27169;&#21644;&#35821;&#38899;&#21512;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#25104;&#21151;&#12290;</title><link>https://arxiv.org/abs/2402.12220</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#20197;&#20811;&#26381;&#28798;&#38590;&#24615;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Bayesian Parameter-Efficient Fine-Tuning for Overcoming Catastrophic Forgetting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12220
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#36125;&#21494;&#26031;&#23398;&#20064;&#25216;&#26415;&#24212;&#29992;&#20110;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65292;&#20197;&#38450;&#27490;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#23454;&#29616;&#20102;&#39044;&#35757;&#32451;&#30693;&#35782;&#30340;&#20445;&#30041;&#65292;&#24182;&#22312;&#35821;&#35328;&#24314;&#27169;&#21644;&#35821;&#38899;&#21512;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#26368;&#21021;&#26159;&#34987;&#25991;&#26412;&#36716;&#35821;&#38899;&#21512;&#25104;&#27169;&#22411;&#30340;&#33258;&#36866;&#24212;&#25152;&#28608;&#21457;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#26356;&#36890;&#29992;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26159;&#36827;&#34892;&#36825;&#31181;&#33258;&#36866;&#24212;&#30340;&#36866;&#24403;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#28798;&#38590;&#24615;&#36951;&#24536;&#20173;&#28982;&#26159;PEFT&#38754;&#20020;&#30340;&#38382;&#39064;&#65292;&#23427;&#25439;&#23475;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#22266;&#26377;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#35777;&#26126;&#29616;&#26377;&#30340;&#36125;&#21494;&#26031;&#23398;&#20064;&#25216;&#26415;&#21487;&#20197;&#24212;&#29992;&#20110;PEFT&#65292;&#20197;&#38450;&#27490;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#21482;&#35201;&#33021;&#22815;&#21487;&#24494;&#22320;&#35745;&#31639;&#24494;&#35843;&#23618;&#30340;&#21442;&#25968;&#36716;&#25442;&#12290;&#22312;&#19968;&#31995;&#21015;&#20851;&#20110;&#35821;&#35328;&#24314;&#27169;&#21644;&#35821;&#38899;&#21512;&#25104;&#20219;&#21153;&#30340;&#22522;&#30784;&#24615;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#24314;&#31435;&#30340;&#25289;&#26222;&#25289;&#26031;&#36817;&#20284;&#65292;&#21253;&#25324;&#23545;&#35282;&#32447;&#21644;Kronecker&#20998;&#35299;&#26041;&#27861;&#65292;&#26469;&#27491;&#21017;&#21270;PEFT&#19982;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#24182;&#27604;&#36739;&#23427;&#20204;&#22312;&#20445;&#30041;&#39044;&#35757;&#32451;&#30693;&#35782;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20811;&#26381;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#32780;&#19981;&#20250;&#38477;&#20302;&#24494;&#35843;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12220v1 Announce Type: cross  Abstract: Although motivated by the adaptation of text-to-speech synthesis models, we argue that more generic parameter-efficient fine-tuning (PEFT) is an appropriate framework to do such adaptation. However, catastrophic forgetting remains an issue with PEFT, damaging the pre-trained model's inherent capabilities. We demonstrate that existing Bayesian learning techniques can be applied to PEFT to prevent catastrophic forgetting as long as the parameter shift of the fine-tuned layers can be calculated differentiably. In a principled series of experiments on language modeling and speech synthesis tasks, we utilize established Laplace approximations, including diagonal and Kronecker factored approaches, to regularize PEFT with the low-rank adaptation (LoRA) and compare their performance in pre-training knowledge preservation. Our results demonstrate that catastrophic forgetting can be overcome by our methods without degrading the fine-tuning perfo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ReAlign&#30340;&#31616;&#21333;&#26377;&#25928;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#26684;&#24335;&#21270;&#25351;&#23548;&#25968;&#25454;&#30340;&#21709;&#24212;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#23545;&#40784;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.12219</link><description>&lt;p&gt;
&#37325;&#26032;&#26684;&#24335;&#21270;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Reformatted Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12219
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ReAlign&#30340;&#31616;&#21333;&#26377;&#25928;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#26684;&#24335;&#21270;&#25351;&#23548;&#25968;&#25454;&#30340;&#21709;&#24212;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#23545;&#40784;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20248;&#21270;&#24494;&#35843;&#25968;&#25454;&#23545;&#20110;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#23545;&#40784;&#33267;&#20851;&#37325;&#35201;&#12290;&#24403;&#21069;&#25913;&#21892;&#25968;&#25454;&#36136;&#37327;&#30340;&#26041;&#27861;&#35201;&#20040;&#32791;&#26102;&#36153;&#21147;&#65292;&#35201;&#20040;&#23481;&#26131;&#21463;&#21040;LLM&#24187;&#35273;&#24341;&#36215;&#30340;&#20107;&#23454;&#38169;&#35823;&#24433;&#21709;&#12290;&#26412;&#25991;&#25506;&#35752;&#25552;&#21319;&#29616;&#26377;&#25351;&#23548;&#25968;&#25454;&#36136;&#37327;&#20197;&#26356;&#22909;&#22320;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;ReAlign&#30340;&#31616;&#21333;&#26377;&#25928;&#26041;&#27861;&#65292;&#23427;&#23558;&#25351;&#23548;&#25968;&#25454;&#30340;&#21709;&#24212;&#37325;&#26032;&#26684;&#24335;&#21270;&#20026;&#26356;&#31526;&#21512;&#39044;&#20808;&#24314;&#31435;&#26631;&#20934;&#21644;&#32534;&#35793;&#35777;&#25454;&#30340;&#26684;&#24335;&#12290;&#35813;&#26041;&#27861;&#26368;&#23567;&#21270;&#20102;&#20154;&#31867;&#27880;&#37322;&#12289;&#24187;&#35273;&#21644;&#25193;&#23637;&#22256;&#38590;&#65292;&#19982;&#29616;&#26377;&#23545;&#40784;&#25216;&#26415;&#27491;&#20132;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ReAlign&#26174;&#33879;&#25552;&#21319;&#20102;LLMs&#30340;&#25972;&#20307;&#23545;&#40784;&#33021;&#21147;&#12289;&#25968;&#23398;&#25512;&#29702;&#12289;&#20107;&#23454;&#24615;&#21644;&#21487;&#35835;&#24615;&#12290;&#20196;&#20154;&#40723;&#33310;&#30340;&#26159;&#65292;&#22312;&#19981;&#24341;&#20837;&#20219;&#20309;&#39069;&#22806;&#25968;&#25454;&#25110;&#20808;&#36827;&#35757;&#32451;&#25216;&#26415;&#30340;&#24773;&#20917;&#19979;&#65292;&#20165;&#36890;&#36807;&#37325;&#26032;&#26684;&#24335;&#21270;&#21709;&#24212;&#65292;LLaMA-2-13
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12219v1 Announce Type: cross  Abstract: The quality of finetuning data is crucial for aligning large language models (LLMs) with human values. Current methods to improve data quality are either labor-intensive or prone to factual errors caused by LLM hallucinations. This paper explores elevating the quality of existing instruction data to better align with human values, introducing a simple and effective approach named ReAlign, which reformats the responses of instruction data into a format that better aligns with pre-established criteria and the collated evidence. This approach minimizes human annotation, hallucination, and the difficulty in scaling, remaining orthogonal to existing alignment techniques. Experimentally, ReAlign significantly boosts the general alignment ability, math reasoning, factuality, and readability of the LLMs.   Encouragingly, without introducing any additional data or advanced training techniques, and merely by reformatting the response, LLaMA-2-13
&lt;/p&gt;</description></item><item><title>&#23383;&#20856;&#23398;&#20064;&#25216;&#26415;&#22312;&#26426;&#26800;&#35299;&#37322;&#20013;&#25915;&#20811;&#21472;&#21152;&#65292;&#24182;&#25552;&#21462;&#26356;&#26131;&#29702;&#35299;&#30340;&#29305;&#24449;&#12290;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30005;&#36335;&#21457;&#29616;&#26694;&#26550;&#65292;&#29992;&#20110;&#36830;&#25509;&#22823;&#37327;&#23383;&#20856;&#29305;&#24449;&#65292;&#30456;&#27604;&#20110;&#28608;&#27963;&#34917;&#19969;&#65292;&#35813;&#26694;&#26550;&#21463;&#36234;&#30028;&#20998;&#24067;&#24433;&#21709;&#36739;&#23567;&#65292;&#24182;&#22312;&#28176;&#36817;&#22797;&#26434;&#24230;&#26041;&#38754;&#26356;&#26377;&#25928;&#12290;</title><link>https://arxiv.org/abs/2402.12201</link><description>&lt;p&gt;
&#23383;&#20856;&#23398;&#20064;&#25913;&#36827;&#20102;&#26426;&#26800;&#35299;&#37322;&#20013;&#30340;&#26080;&#34917;&#19969;&#30005;&#36335;&#21457;&#29616;&#65306;&#20197;&#22885;&#36187;&#32599;-GPT&#20026;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Dictionary Learning Improves Patch-Free Circuit Discovery in Mechanistic Interpretability: A Case Study on Othello-GPT
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12201
&lt;/p&gt;
&lt;p&gt;
&#23383;&#20856;&#23398;&#20064;&#25216;&#26415;&#22312;&#26426;&#26800;&#35299;&#37322;&#20013;&#25915;&#20811;&#21472;&#21152;&#65292;&#24182;&#25552;&#21462;&#26356;&#26131;&#29702;&#35299;&#30340;&#29305;&#24449;&#12290;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30005;&#36335;&#21457;&#29616;&#26694;&#26550;&#65292;&#29992;&#20110;&#36830;&#25509;&#22823;&#37327;&#23383;&#20856;&#29305;&#24449;&#65292;&#30456;&#27604;&#20110;&#28608;&#27963;&#34917;&#19969;&#65292;&#35813;&#26694;&#26550;&#21463;&#36234;&#30028;&#20998;&#24067;&#24433;&#21709;&#36739;&#23567;&#65292;&#24182;&#22312;&#28176;&#36817;&#22797;&#26434;&#24230;&#26041;&#38754;&#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#23383;&#20856;&#23398;&#20064;&#26159;&#19968;&#31181;&#22312;&#26426;&#26800;&#35299;&#37322;&#20013;&#24555;&#36895;&#21457;&#23637;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#25915;&#20987;&#21472;&#21152;&#24182;&#20174;&#27169;&#22411;&#28608;&#27963;&#20013;&#25552;&#21462;&#26356;&#26131;&#29702;&#35299;&#30340;&#29305;&#24449;&#12290;&#26412;&#25991;&#22522;&#20110;&#25552;&#21462;&#30340;&#26356;&#21333;&#20041;&#29305;&#24449;&#36827;&#19968;&#27493;&#25552;&#20986;&#19968;&#20010;&#38382;&#39064;&#65306;&#25105;&#20204;&#22914;&#20309;&#35782;&#21035;&#36830;&#25509;&#22823;&#37327;&#23383;&#20856;&#29305;&#24449;&#30340;&#30005;&#36335;&#65311;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#30005;&#36335;&#21457;&#29616;&#26694;&#26550;&#65292;&#26367;&#20195;&#20102;&#28608;&#27963;&#34917;&#19969;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#36234;&#30028;&#20998;&#24067;&#26041;&#38754;&#36973;&#21463;&#36739;&#23567;&#65292;&#24182;&#22312;&#28176;&#36817;&#22797;&#26434;&#24230;&#26041;&#38754;&#35777;&#26126;&#26356;&#26377;&#25928;&#12290;&#25105;&#20204;&#26694;&#26550;&#20013;&#30340;&#22522;&#26412;&#21333;&#20803;&#26159;&#20174;&#25152;&#26377;&#27169;&#22359;&#20013;&#20889;&#20837;&#27531;&#20313;&#27969;&#30340;&#23383;&#20856;&#29305;&#24449;&#65292;&#21253;&#25324;&#23884;&#20837;&#12289;&#27880;&#24847;&#21147;&#36755;&#20986;&#21644;MLP&#36755;&#20986;&#12290;&#20174;&#20219;&#20309;&#23545;&#25968;&#12289;&#23383;&#20856;&#29305;&#24449;&#25110;&#27880;&#24847;&#21147;&#20998;&#25968;&#24320;&#22987;&#65292;&#25105;&#20204;&#25104;&#21151;&#36861;&#36394;&#21040;&#25152;&#26377;&#20196;&#29260;&#30340;&#36739;&#20302;&#32423;&#21035;&#23383;&#20856;&#29305;&#24449;&#65292;&#24182;&#35745;&#31639;&#23427;&#20204;&#23545;&#36825;&#20123;&#26356;&#20855;&#21487;&#35299;&#37322;&#24615;&#21644;&#23616;&#37096;&#27169;&#22411;&#34892;&#20026;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12201v1 Announce Type: new  Abstract: Sparse dictionary learning has been a rapidly growing technique in mechanistic interpretability to attack superposition and extract more human-understandable features from model activations. We ask a further question based on the extracted more monosemantic features: How do we recognize circuits connecting the enormous amount of dictionary features? We propose a circuit discovery framework alternative to activation patching. Our framework suffers less from out-of-distribution and proves to be more efficient in terms of asymptotic complexity. The basic unit in our framework is dictionary features decomposed from all modules writing to the residual stream, including embedding, attention output and MLP output. Starting from any logit, dictionary feature or attention score, we manage to trace down to lower-level dictionary features of all tokens and compute their contribution to these more interpretable and local model behaviors. We dig in a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#38646;-shot&#20998;&#31867;&#22312;&#22788;&#29702;&#22797;&#26434;&#20219;&#21153;&#22914;&#24694;&#24847;&#27169;&#22240;&#26816;&#27979;&#20013;&#30340;&#26377;&#25928;&#24615;</title><link>https://arxiv.org/abs/2402.12198</link><description>&lt;p&gt;
&#38646;-shot &#21487;&#35265;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#20167;&#24680;&#27169;&#22240;&#26816;&#27979;&#65306;&#25105;&#20204;&#24050;&#32463;&#21040;&#36798;&#30446;&#26631;&#20102;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Zero shot VLMs for hate meme detection: Are we there yet?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12198
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#38646;-shot&#20998;&#31867;&#22312;&#22788;&#29702;&#22797;&#26434;&#20219;&#21153;&#22914;&#24694;&#24847;&#27169;&#22240;&#26816;&#27979;&#20013;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#22810;&#23186;&#20307;&#20869;&#23481;&#27491;&#22312;&#36805;&#36895;&#21457;&#23637;&#65292;&#20854;&#20013;&#27169;&#22240;&#20316;&#20026;&#19968;&#31181;&#29420;&#29305;&#24418;&#24335;&#21464;&#24471;&#26085;&#30410;&#37325;&#35201;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#19968;&#20123;&#24694;&#24847;&#29992;&#25143;&#21033;&#29992;&#27169;&#22240;&#38024;&#23545;&#20010;&#20154;&#25110;&#26131;&#21463;&#25915;&#20987;&#30340;&#31038;&#21306;&#65292;&#22240;&#27492;&#26377;&#24517;&#35201;&#35782;&#21035;&#21644;&#35299;&#20915;&#27492;&#31867;&#24694;&#24847;&#27169;&#22240;&#12290;&#24050;&#32463;&#36827;&#34892;&#20102;&#22823;&#37327;&#30740;&#31350;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36890;&#36807;&#24320;&#21457;&#20167;&#24680;&#27169;&#22240;&#26816;&#27979;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;/&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#19968;&#20010;&#26174;&#33879;&#23616;&#38480;&#24615;&#26159;&#38656;&#35201;&#24102;&#26631;&#31614;&#30340;&#25968;&#25454;&#38598;&#25165;&#33021;&#36827;&#34892;&#20934;&#30830;&#20998;&#31867;&#12290;&#26368;&#36817;&#65292;&#30740;&#31350;&#30028;&#35265;&#35777;&#20102;&#20960;&#31181;&#21487;&#35265;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#35843;&#26597;&#36825;&#20123;&#21487;&#35265;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#35832;&#22914;&#20167;&#24680;&#27169;&#22240;&#26816;&#27979;&#31561;&#22797;&#26434;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#21508;&#31181;&#25552;&#31034;&#35774;&#32622;&#26469;&#19987;&#27880;&#20110;&#23545;&#24694;&#24847;/&#26377;&#23475;&#27169;&#22240;&#30340;&#38646;-shot &#20998;&#31867;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;o
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12198v1 Announce Type: new  Abstract: Multimedia content on social media is rapidly evolving, with memes gaining prominence as a distinctive form. Unfortunately, some malicious users exploit memes to target individuals or vulnerable communities, making it imperative to identify and address such instances of hateful memes. Extensive research has been conducted to address this issue by developing hate meme detection models. However, a notable limitation of traditional machine/deep learning models is the requirement for labeled datasets for accurate classification. Recently, the research community has witnessed the emergence of several visual language models that have exhibited outstanding performance across various tasks. In this study, we aim to investigate the efficacy of these visual language models in handling intricate tasks such as hate meme detection. We use various prompt settings to focus on zero-shot classification of hateful/harmful memes. Through our analysis, we o
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20010;&#24615;&#21270;&#21453;&#20107;&#23454;&#30284;&#30151;&#27835;&#30103;&#24314;&#35758;&#65292;&#38598;&#25104;&#20102;&#22810;&#31181;&#22810;&#32452;&#23398;&#25216;&#26415;&#30340;&#19987;&#23478;&#65292;&#21487;&#25552;&#20379;&#20248;&#36234;&#24615;&#33021;&#21644;&#20915;&#31574;&#35299;&#37322;&#12290;</title><link>https://arxiv.org/abs/2402.12190</link><description>&lt;p&gt;
&#22522;&#20110;AI&#30340;&#31934;&#20934;&#32959;&#30244;&#23398;&#65306;&#22522;&#20110;&#22810;&#32452;&#23398;&#25968;&#25454;&#30340;&#20010;&#24615;&#21270;&#21453;&#20107;&#23454;&#27835;&#30103;&#24314;&#35758;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Towards AI-Based Precision Oncology: A Machine Learning Framework for Personalized Counterfactual Treatment Suggestions based on Multi-Omics Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12190
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20010;&#24615;&#21270;&#21453;&#20107;&#23454;&#30284;&#30151;&#27835;&#30103;&#24314;&#35758;&#65292;&#38598;&#25104;&#20102;&#22810;&#31181;&#22810;&#32452;&#23398;&#25216;&#26415;&#30340;&#19987;&#23478;&#65292;&#21487;&#25552;&#20379;&#20248;&#36234;&#24615;&#33021;&#21644;&#20915;&#31574;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#39537;&#21160;&#30340;&#31934;&#20934;&#32959;&#30244;&#23398;&#20855;&#26377;&#36890;&#36807;&#21033;&#29992;AI&#27169;&#22411;&#20998;&#26512;&#22797;&#26434;&#24739;&#32773;&#29305;&#24449;&#19982;&#23545;&#24212;&#27835;&#30103;&#32467;&#26524;&#20043;&#38388;&#20114;&#21160;&#30340;&#28508;&#21147;&#65292;&#26377;&#26395;&#37325;&#22609;&#30284;&#30151;&#27835;&#30103;&#12290;&#26032;&#25216;&#26415;&#24179;&#21488;&#20419;&#36827;&#20102;&#21450;&#26102;&#33719;&#21462;&#22810;&#27169;&#24577;&#32959;&#30244;&#29983;&#29289;&#23398;&#25968;&#25454;&#65292;&#22914;&#21333;&#32454;&#32990;&#22810;&#32452;&#23398;&#25968;&#25454;&#65292;&#20351;&#24471;&#36825;&#31181;&#25968;&#25454;&#30340;&#36136;&#37327;&#21644;&#25968;&#37327;&#21487;&#29992;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#25913;&#36827;&#20020;&#24202;&#20915;&#31574;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22359;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#22522;&#20110;&#35757;&#32451;&#26377;&#20851;&#22810;&#31181;&#22810;&#32452;&#23398;&#25216;&#26415;&#30340;&#26426;&#22120;&#23398;&#20064;&#19987;&#23478;&#32452;&#25104;&#30340;&#38598;&#25104;&#26469;&#36827;&#34892;&#20010;&#24615;&#21270;&#21453;&#20107;&#23454;&#30284;&#30151;&#27835;&#30103;&#24314;&#35758;&#12290;&#36825;&#20123;&#19987;&#38376;&#30340;&#21453;&#20107;&#23454;&#19987;&#23478;&#26681;&#25454;&#25216;&#26415;&#19981;&#26029;&#32858;&#21512;&#20026;&#24615;&#33021;&#26356;&#20248;&#36234;&#30340;&#19987;&#23478;&#65292;&#21487;&#25552;&#20379;&#20915;&#31574;&#30340;&#32622;&#20449;&#24230;&#21644;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12190v1 Announce Type: cross  Abstract: AI-driven precision oncology has the transformative potential to reshape cancer treatment by leveraging the power of AI models to analyze the interaction between complex patient characteristics and their corresponding treatment outcomes. New technological platforms have facilitated the timely acquisition of multimodal data on tumor biology at an unprecedented resolution, such as single-cell multi-omics data, making this quality and quantity of data available for data-driven improved clinical decision-making. In this work, we propose a modular machine learning framework designed for personalized counterfactual cancer treatment suggestions based on an ensemble of machine learning experts trained on diverse multi-omics technologies. These specialized counterfactual experts per technology are consistently aggregated into a more powerful expert with superior performance and can provide both confidence and an explanation of its decision. The
&lt;/p&gt;</description></item><item><title>&#25915;&#20987;&#32773;&#36890;&#36807;&#23545;&#39044;&#35757;&#32451;LM&#36827;&#34892;&#23545;&#25239;&#24494;&#35843;&#65292;&#20197;&#25918;&#22823;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#30340;&#26333;&#20809;&#65292;&#37319;&#29992;&#20266;&#26631;&#31614;&#21644;&#26426;&#22120;&#29983;&#25104;&#27010;&#29575;&#26469;&#21152;&#24378;LM&#23545;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#20445;&#30041;&#12290;</title><link>https://arxiv.org/abs/2402.12189</link><description>&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#20266;&#26631;&#31614;&#25104;&#21592;&#36164;&#26684;&#36827;&#34892;&#24494;&#35843;&#26469;&#22686;&#24378;&#35757;&#32451;&#25968;&#25454;&#26333;&#20809;
&lt;/p&gt;
&lt;p&gt;
Amplifying Training Data Exposure through Fine-Tuning with Pseudo-Labeled Memberships
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12189
&lt;/p&gt;
&lt;p&gt;
&#25915;&#20987;&#32773;&#36890;&#36807;&#23545;&#39044;&#35757;&#32451;LM&#36827;&#34892;&#23545;&#25239;&#24494;&#35843;&#65292;&#20197;&#25918;&#22823;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#30340;&#26333;&#20809;&#65292;&#37319;&#29992;&#20266;&#26631;&#31614;&#21644;&#26426;&#22120;&#29983;&#25104;&#27010;&#29575;&#26469;&#21152;&#24378;LM&#23545;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#20445;&#30041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;(LMs)&#30001;&#20110;&#25968;&#25454;&#35760;&#24518;&#32780;&#23481;&#26131;&#21463;&#21040;&#35757;&#32451;&#25968;&#25454;&#25552;&#21462;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#22330;&#26223;&#65292;&#22312;&#36825;&#31181;&#22330;&#26223;&#20013;&#65292;&#25915;&#20987;&#32773;&#23545;&#39044;&#35757;&#32451;LM&#36827;&#34892;&#23545;&#25239;&#24494;&#35843;&#65292;&#20197;&#25918;&#22823;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#30340;&#26333;&#20809;&#12290;&#35813;&#31574;&#30053;&#19981;&#21516;&#20110;&#20808;&#21069;&#30340;&#30740;&#31350;&#65292;&#20854;&#30446;&#30340;&#26159;&#21152;&#24378;LM&#23545;&#20854;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#20445;&#30041;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25915;&#20987;&#32773;&#38656;&#35201;&#25910;&#38598;&#19982;&#39044;&#35757;&#32451;&#25968;&#25454;&#23494;&#20999;&#30456;&#20851;&#30340;&#29983;&#25104;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#27809;&#26377;&#23454;&#38469;&#25968;&#25454;&#38598;&#30340;&#30693;&#35782;&#65292;&#34913;&#37327;&#29983;&#25104;&#25991;&#26412;&#20013;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#37327;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#30446;&#26631;LM&#30340;&#26426;&#22120;&#29983;&#25104;&#27010;&#29575;&#25152;&#34920;&#31034;&#30340;&#25104;&#21592;&#36817;&#20284;&#20540;&#20026;&#36825;&#20123;&#29983;&#25104;&#25991;&#26412;&#20351;&#29992;&#20266;&#26631;&#31614;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#24494;&#35843;LM&#20197;&#25903;&#25345;&#37027;&#20123;&#26356;&#26377;&#21487;&#33021;&#28304;&#33258;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#29983;&#25104;&#25991;&#26412;&#65292;&#26681;&#25454;&#20854;&#25104;&#21592;&#36164;&#26684;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12189v1 Announce Type: new  Abstract: Neural language models (LMs) are vulnerable to training data extraction attacks due to data memorization. This paper introduces a novel attack scenario wherein an attacker adversarially fine-tunes pre-trained LMs to amplify the exposure of the original training data. This strategy differs from prior studies by aiming to intensify the LM's retention of its pre-training dataset. To achieve this, the attacker needs to collect generated texts that are closely aligned with the pre-training data. However, without knowledge of the actual dataset, quantifying the amount of pre-training data within generated texts is challenging. To address this, we propose the use of pseudo-labels for these generated texts, leveraging membership approximations indicated by machine-generated probabilities from the target LM. We subsequently fine-tune the LM to favor generations with higher likelihoods of originating from the pre-training data, based on their memb
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#65292;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#29305;&#24449;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#20197;&#24179;&#34913;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#65292;&#30740;&#31350;&#21457;&#29616;&#29305;&#24449;&#31354;&#38388;&#20869;&#37096;&#30340;&#19981;&#23545;&#40784;&#32463;&#24120;&#23548;&#33268; misclassification&#65292;&#36825;&#19968;&#26041;&#27861;&#26088;&#22312;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.12187</link><description>&lt;p&gt;
&#23545;&#25239;&#29305;&#24449;&#23545;&#40784;&#65306;&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#24179;&#34913;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Adversarial Feature Alignment: Balancing Robustness and Accuracy in Deep Learning via Adversarial Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12187
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#65292;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#29305;&#24449;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#20197;&#24179;&#34913;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#65292;&#30740;&#31350;&#21457;&#29616;&#29305;&#24449;&#31354;&#38388;&#20869;&#37096;&#30340;&#19981;&#23545;&#40784;&#32463;&#24120;&#23548;&#33268; misclassification&#65292;&#36825;&#19968;&#26041;&#27861;&#26088;&#22312;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#19981;&#26029;&#21462;&#24471;&#36827;&#23637;&#65292;&#20294;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#36825;&#32463;&#24120;&#23548;&#33268;&#23545;&#25239;&#24615;&#26679;&#26412;&#34987;&#38169;&#35823;&#20998;&#31867;&#12290;&#23545;&#25239;&#35757;&#32451;&#34987;&#29992;&#26469;&#36890;&#36807;&#22686;&#24378;&#23545;&#36825;&#20123;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#26469;&#20943;&#36731;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#36890;&#24120;&#20250;&#38477;&#20302;&#27169;&#22411;&#22312;&#24178;&#20928;&#30340;&#38750;&#23545;&#25239;&#24615;&#26679;&#26412;&#19978;&#30340;&#26631;&#20934;&#20934;&#30830;&#24615;&#12290;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#22312;&#23433;&#20840;&#24615;&#26041;&#38754;&#24179;&#34913;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#30340;&#24517;&#35201;&#24615;&#26159;&#26174;&#32780;&#26131;&#35265;&#30340;&#65292;&#20294;&#23454;&#29616;&#36825;&#31181;&#24179;&#34913;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#20854;&#28508;&#22312;&#21407;&#22240;&#23578;&#26410;&#26126;&#30830;&#38416;&#36848;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#65292;&#31216;&#20026;&#23545;&#25239;&#29305;&#24449;&#23545;&#40784;&#65288;AFA&#65289;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#35265;&#35299;&#65306;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#19981;&#23545;&#40784;&#32463;&#24120;&#23548;&#33268;&#38169;&#35823;&#20998;&#31867;&#65292;&#26080;&#35770;&#26679;&#26412;&#26159;&#33391;&#24615;&#36824;&#26159;&#23545;&#25239;&#24615;&#12290;AFA&#36890;&#36807;&#37319;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#20248;&#21270;&#31639;&#27861;&#26469;&#20943;&#36731;&#36825;&#31181;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12187v1 Announce Type: cross  Abstract: Deep learning models continue to advance in accuracy, yet they remain vulnerable to adversarial attacks, which often lead to the misclassification of adversarial examples. Adversarial training is used to mitigate this problem by increasing robustness against these attacks. However, this approach typically reduces a model's standard accuracy on clean, non-adversarial samples. The necessity for deep learning models to balance both robustness and accuracy for security is obvious, but achieving this balance remains challenging, and the underlying reasons are yet to be clarified. This paper proposes a novel adversarial training method called Adversarial Feature Alignment (AFA), to address these problems. Our research unveils an intriguing insight: misalignment within the feature space often leads to misclassification, regardless of whether the samples are benign or adversarial. AFA mitigates this risk by employing a novel optimization algor
&lt;/p&gt;</description></item><item><title>&#37325;&#26032;&#23457;&#35270;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#22686;&#24378;&#65292;&#20998;&#26512;&#19981;&#21516;&#26041;&#27861;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#22914;&#20309;&#26356;&#21152;&#21407;&#21017;&#22320;&#21033;&#29992;&#25968;&#25454;&#22686;&#24378;&#30340;&#24314;&#35758;&#12290;</title><link>https://arxiv.org/abs/2402.12181</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Revisiting Data Augmentation in Deep Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12181
&lt;/p&gt;
&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#22686;&#24378;&#65292;&#20998;&#26512;&#19981;&#21516;&#26041;&#27861;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#22914;&#20309;&#26356;&#21152;&#21407;&#21017;&#22320;&#21033;&#29992;&#25968;&#25454;&#22686;&#24378;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#22522;&#20110;&#22270;&#20687;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#20013;&#25552;&#20986;&#20102;&#21508;&#31181;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#12290;&#23613;&#31649;&#23427;&#20204;&#22312;&#23454;&#35777;&#20013;&#35777;&#26126;&#20102;&#25968;&#25454;&#22686;&#24378;&#23545;&#20110;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#25110;&#27867;&#21270;&#30340;&#26377;&#25928;&#24615;&#65292;&#20294;&#24182;&#19981;&#24635;&#26159;&#28165;&#26970;&#21738;&#31181;&#25216;&#26415;&#24212;&#35813;&#34987;&#20248;&#20808;&#36873;&#25321;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#29616;&#26377;&#26041;&#27861;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#23427;&#20204;&#24182;&#25581;&#31034;&#23427;&#20204;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36890;&#36807;&#34920;&#36798;&#36825;&#20123;&#26041;&#27861;&#30340;Q-targets&#21644;&#32463;&#39564;&#28436;&#21592;/&#35780;&#35770;&#23478;&#25439;&#22833;&#30340;&#26041;&#24046;&#65292;&#25105;&#20204;&#21487;&#20197;&#20998;&#26512;&#23427;&#20204;&#19981;&#21516;&#32452;&#25104;&#37096;&#20998;&#30340;&#24433;&#21709;&#24182;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#22914;&#20309;&#36873;&#25321;&#19981;&#21516;&#30340;&#25968;&#25454;&#22686;&#24378;&#36716;&#25442;&#26469;&#35745;&#31639;&#30446;&#26631;Q&#20540;&#21487;&#33021;&#20250;&#24433;&#21709;&#36825;&#20123;&#26041;&#27861;&#30340;&#35299;&#37322;&#12290;&#36825;&#39033;&#20998;&#26512;&#25552;&#20986;&#20102;&#22914;&#20309;&#26356;&#21152;&#21407;&#21017;&#22320;&#21033;&#29992;&#25968;&#25454;&#22686;&#24378;&#30340;&#24314;&#35758;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21253;&#25324;&#20102;&#19968;&#20010;&#31216;&#20026;&#20999;&#32447;prop&#30340;&#27491;&#21017;&#21270;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12181v1 Announce Type: cross  Abstract: Various data augmentation techniques have been recently proposed in image-based deep reinforcement learning (DRL). Although they empirically demonstrate the effectiveness of data augmentation for improving sample efficiency or generalization, which technique should be preferred is not always clear. To tackle this question, we analyze existing methods to better understand them and to uncover how they are connected. Notably, by expressing the variance of the Q-targets and that of the empirical actor/critic losses of these methods, we can analyze the effects of their different components and compare them. We furthermore formulate an explanation about how these methods may be affected by choosing different data augmentation transformations in calculating the target Q-values. This analysis suggests recommendations on how to exploit data augmentation in a more principled way. In addition, we include a regularization term called tangent prop,
&lt;/p&gt;</description></item><item><title>Mafin&#36890;&#36807;&#24341;&#20837;&#27169;&#22411;&#22686;&#24378;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#21482;&#26377;&#40657;&#30418;&#23884;&#20837;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.12177</link><description>&lt;p&gt;
Mafin: &#29992;&#27169;&#22411;&#22686;&#24378;&#24494;&#35843;&#26469;&#22686;&#24378;&#40657;&#30418;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Mafin: Enhancing Black-Box Embeddings with Model Augmented Fine-tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12177
&lt;/p&gt;
&lt;p&gt;
Mafin&#36890;&#36807;&#24341;&#20837;&#27169;&#22411;&#22686;&#24378;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#21482;&#26377;&#40657;&#30418;&#23884;&#20837;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#24050;&#32463;&#25104;&#20026;&#32531;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#24187;&#35273;&#30340;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;RAG&#20013;&#30340;&#26816;&#32034;&#38454;&#27573;&#36890;&#24120;&#28041;&#21450;&#39044;&#35757;&#32451;&#30340;&#23884;&#20837;&#27169;&#22411;&#65292;&#23558;&#26597;&#35810;&#21644;&#27573;&#33853;&#36716;&#25442;&#20026;&#21521;&#37327;&#20197;&#25429;&#33719;&#23427;&#20204;&#30340;&#35821;&#20041;&#12290;&#28982;&#32780;&#65292;&#24403;&#24212;&#29992;&#20110;&#29305;&#23450;&#39046;&#22495;&#30693;&#35782;&#26102;&#65292;&#26631;&#20934;&#30340;&#39044;&#35757;&#32451;&#23884;&#20837;&#27169;&#22411;&#21487;&#33021;&#34920;&#29616;&#20986;&#27425;&#20248;&#24615;&#33021;&#65292;&#38656;&#35201;&#36827;&#34892;&#24494;&#35843;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#20165;&#33021;&#20174;&#40657;&#30418;&#27169;&#22411;&#33719;&#21462;&#23884;&#20837;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#27169;&#22411;&#22686;&#24378;&#24494;&#35843;&#65288;Mafin&#65289;--&#19968;&#31181;&#36890;&#36807;&#29992;&#21487;&#35757;&#32451;&#30340;&#23884;&#20837;&#27169;&#22411;&#22686;&#24378;&#40657;&#30418;&#23884;&#20837;&#27169;&#22411;&#26469;&#36827;&#34892;&#24494;&#35843;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;Mafin&#20165;&#38656;&#35201;&#35757;&#32451;&#19968;&#20010;&#23567;&#30340;&#22686;&#24378;&#27169;&#22411;&#23601;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#40657;&#30418;&#23884;&#20837;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26377;&#26631;&#31614;&#21644;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12177v1 Announce Type: cross  Abstract: Retrieval Augmented Generation (RAG) has emerged as an effective solution for mitigating hallucinations in Large Language Models (LLMs). The retrieval stage in RAG typically involves a pre-trained embedding model, which converts queries and passages into vectors to capture their semantics. However, a standard pre-trained embedding model may exhibit sub-optimal performance when applied to specific domain knowledge, necessitating fine-tuning. This paper addresses scenarios where the embeddings are only available from a black-box model. We introduce Model augmented fine-tuning (Mafin) -- a novel approach for fine-tuning a black-box embedding model by augmenting it with a trainable embedding model. Our results demonstrate that Mafin significantly enhances the performance of the black-box embeddings by only requiring the training of a small augmented model. We validate the effectiveness of our method on both labeled and unlabeled datasets, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25193;&#23637;&#22522;&#20110;Gene-pool Optimal Mixing Evolutionary Algorithm (GOMEA)&#30340;&#26368;&#20808;&#36827;&#32467;&#26500;&#23398;&#20064;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#32852;&#21512;&#23398;&#20064;&#21464;&#37327;&#31163;&#25955;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.12175</link><description>&lt;p&gt;
&#20351;&#29992;GOMEA&#23398;&#20064;&#31163;&#25955;&#36125;&#21494;&#26031;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Learning Discretized Bayesian Networks with GOMEA
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12175
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25193;&#23637;&#22522;&#20110;Gene-pool Optimal Mixing Evolutionary Algorithm (GOMEA)&#30340;&#26368;&#20808;&#36827;&#32467;&#26500;&#23398;&#20064;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#32852;&#21512;&#23398;&#20064;&#21464;&#37327;&#31163;&#25955;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#32593;&#32476;&#27169;&#22411;&#25551;&#36848;&#20102;&#19981;&#30830;&#23450;&#24615;&#19979;&#38543;&#26426;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21487;&#29992;&#20110;&#39044;&#27979;&#20107;&#20214;&#21644;&#32467;&#26524;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#32467;&#21512;&#35266;&#23519;&#21040;&#30340;&#35777;&#25454;&#12290;&#20174;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#31181;&#27169;&#22411;&#20855;&#26377;&#32039;&#20945;&#24615;&#65292;&#32780;&#19988;&#25429;&#33719;&#30340;&#20851;&#31995;&#21487;&#20197;&#30452;&#25509;&#30001;&#39046;&#22495;&#19987;&#23478;&#26816;&#26597;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#25968;&#25454;&#24448;&#24448;&#26159;&#23454;&#20540;&#30340;&#12290;&#38500;&#38750;&#21487;&#20197;&#20551;&#35774;&#27491;&#24577;&#24615;&#65292;&#21542;&#21017;&#36890;&#24120;&#38656;&#35201;&#36827;&#34892;&#31163;&#25955;&#21270;&#12290;&#28982;&#32780;&#65292;&#26368;&#20339;&#31163;&#25955;&#21270;&#21462;&#20915;&#20110;&#27169;&#22411;&#21270;&#30340;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#36825;&#22686;&#21152;&#20102;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#22797;&#26434;&#24615;&#12290;&#22522;&#20110;Gene-pool Optimal Mixing Evolutionary Algorithm&#65288;GOMEA&#65289;&#30340;&#29616;&#26377;&#26368;&#20808;&#36827;&#32467;&#26500;&#23398;&#20064;&#26041;&#27861;&#65292;&#25105;&#20204;&#22312;&#26412;&#24037;&#20316;&#20013;&#25193;&#23637;&#20102;&#36825;&#19968;&#26041;&#27861;&#65292;&#20197;&#20849;&#21516;&#23398;&#20064;&#21464;&#37327;&#31163;&#25955;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12175v1 Announce Type: new  Abstract: Bayesian networks model relationships between random variables under uncertainty and can be used to predict the likelihood of events and outcomes while incorporating observed evidence. From an eXplainable AI (XAI) perspective, such models are interesting as they tend to be compact. Moreover, captured relations can be directly inspected by domain experts. In practice, data is often real-valued. Unless assumptions of normality can be made, discretization is often required. The optimal discretization, however, depends on the relations modelled between the variables. This complicates learning Bayesian networks from data. For this reason, most literature focuses on learning conditional dependencies between sets of variables, called structure learning. In this work, we extend an existing state-of-the-art structure learning approach based on the Gene-pool Optimal Mixing Evolutionary Algorithm (GOMEA) to jointly learn variable discretizations. T
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36866;&#37197;&#22120;&#35843;&#20248;&#26694;&#26550;&#65292;&#36171;&#20104;&#39044;&#35757;&#32451;&#22270;&#27169;&#22411;&#20855;&#26377;&#21487;&#35777;&#26126;&#30340;&#20844;&#24179;&#24615;</title><link>https://arxiv.org/abs/2402.12161</link><description>&lt;p&gt;
&#36171;&#20104;&#39044;&#35757;&#32451;&#22270;&#27169;&#22411;&#20855;&#26377;&#21487;&#35777;&#26126;&#30340;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Endowing Pre-trained Graph Models with Provable Fairness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12161
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36866;&#37197;&#22120;&#35843;&#20248;&#26694;&#26550;&#65292;&#36171;&#20104;&#39044;&#35757;&#32451;&#22270;&#27169;&#22411;&#20855;&#26377;&#21487;&#35777;&#26126;&#30340;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#22270;&#27169;&#22411;&#65288;PGMs&#65289;&#26088;&#22312;&#25429;&#25417;&#21487;&#36716;&#31227;&#30340;&#22266;&#26377;&#32467;&#26500;&#23646;&#24615;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#31867;&#20284;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;PGMs&#20063;&#20250;&#32487;&#25215;&#20154;&#31867;&#31038;&#20250;&#20013;&#30340;&#20559;&#35265;&#65292;&#23548;&#33268;&#22312;&#19979;&#28216;&#24212;&#29992;&#20013;&#20986;&#29616;&#27495;&#35270;&#34892;&#20026;&#12290;&#29616;&#26377;&#20844;&#24179;&#26041;&#27861;&#30340;&#21435;&#20559;&#35265;&#36807;&#31243;&#36890;&#24120;&#19982;GNNs&#30340;&#21442;&#25968;&#20248;&#21270;&#30456;&#32467;&#21512;&#12290;&#28982;&#32780;&#65292;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#22312;&#29616;&#23454;&#20013;&#21487;&#33021;&#19982;&#19981;&#21516;&#30340;&#25935;&#24863;&#23646;&#24615;&#30456;&#20851;&#32852;&#65292;&#30452;&#25509;&#37319;&#29992;&#29616;&#26377;&#26041;&#27861;&#25913;&#21892;PGMs&#30340;&#20844;&#24179;&#24615;&#26159;&#19981;&#28789;&#27963;&#19988;&#20302;&#25928;&#30340;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#32570;&#20047;&#29702;&#35770;&#20445;&#35777;&#65292;&#21363;&#23545;&#27169;&#22411;&#39044;&#27979;&#20844;&#24179;&#24615;&#30340;&#21487;&#35777;&#26126;&#19979;&#38480;&#65292;&#36825;&#30452;&#25509;&#25552;&#20379;&#20102;&#23454;&#38469;&#22330;&#26223;&#19979;&#30340;&#20445;&#35777;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36866;&#37197;&#22120;&#35843;&#20248;&#26694;&#26550;&#65292;&#36171;&#20104;&#39044;&#35757;&#32451;\textbf{&#22270;}&#27169;&#22411;&#20855;&#26377;\textbf{&#21487;&#35777;&#26126;}&#30340;\textbf{&#20844;}&#24179;\textbf{&#24615;}&#65288;&#31216;&#20026;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12161v1 Announce Type: cross  Abstract: Pre-trained graph models (PGMs) aim to capture transferable inherent structural properties and apply them to different downstream tasks. Similar to pre-trained language models, PGMs also inherit biases from human society, resulting in discriminatory behavior in downstream applications. The debiasing process of existing fair methods is generally coupled with parameter optimization of GNNs. However, different downstream tasks may be associated with different sensitive attributes in reality, directly employing existing methods to improve the fairness of PGMs is inflexible and inefficient. Moreover, most of them lack a theoretical guarantee, i.e., provable lower bounds on the fairness of model predictions, which directly provides assurance in a practical scenario. To overcome these limitations, we propose a novel adapter-tuning framework that endows pre-trained \textbf{Graph} models with \textbf{P}rovable f\textbf{A}i\textbf{R}ness (called
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#23450;&#20041;&#21644;&#37327;&#21270;&#21160;&#37327;&#65292;&#20026;&#32593;&#29699;&#27604;&#36187;&#30340;&#23454;&#26102;&#20998;&#26512;&#25552;&#20379;&#22522;&#30784;&#65292;&#36890;&#36807;&#24314;&#31435;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#21644;&#22522;&#20110;&#32463;&#39564;&#20844;&#24335;&#30340;&#27169;&#22411;&#65292;&#37319;&#29992;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#34701;&#21512;&#65292;&#20197;&#25506;&#32034;&#31454;&#25216;&#20307;&#32946;&#20013;&#30340;&#21160;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.12149</link><description>&lt;p&gt;
MLFEF: &#37319;&#29992;&#32463;&#39564;&#20844;&#24335;&#30340;&#26426;&#22120;&#23398;&#20064;&#34701;&#21512;&#27169;&#22411;&#25506;&#32034;&#31454;&#25216;&#20307;&#32946;&#20013;&#30340;&#21160;&#37327;
&lt;/p&gt;
&lt;p&gt;
MLFEF: Machine Learning Fusion Model with Empirical Formula to Explore the Momentum in Competitive Sports
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12149
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#23450;&#20041;&#21644;&#37327;&#21270;&#21160;&#37327;&#65292;&#20026;&#32593;&#29699;&#27604;&#36187;&#30340;&#23454;&#26102;&#20998;&#26512;&#25552;&#20379;&#22522;&#30784;&#65292;&#36890;&#36807;&#24314;&#31435;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#21644;&#22522;&#20110;&#32463;&#39564;&#20844;&#24335;&#30340;&#27169;&#22411;&#65292;&#37319;&#29992;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#34701;&#21512;&#65292;&#20197;&#25506;&#32034;&#31454;&#25216;&#20307;&#32946;&#20013;&#30340;&#21160;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#29699;&#38750;&#24120;&#21463;&#27426;&#36814;&#65292;&#25945;&#32451;&#21644;&#36816;&#21160;&#21592;&#23545;&#38500;&#20102;&#25216;&#33021;&#20043;&#22806;&#30340;&#22240;&#32032;&#65292;&#22914;&#21160;&#37327;&#65292;&#20063;&#24863;&#21040;&#22909;&#22855;&#12290;&#26412;&#25991;&#23558;&#23581;&#35797;&#23450;&#20041;&#21644;&#37327;&#21270;&#21160;&#37327;&#65292;&#20026;&#32593;&#29699;&#27604;&#36187;&#30340;&#23454;&#26102;&#20998;&#26512;&#25552;&#20379;&#22522;&#30784;&#12290;&#22522;&#20110;&#36817;&#24180;&#26469;&#32593;&#29699;&#22823;&#28385;&#36143;&#30007;&#23376;&#21333;&#25171;&#27604;&#36187;&#30340;&#25968;&#25454;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#20004;&#20010;&#27169;&#22411;&#65292;&#19968;&#20010;&#26159;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#27169;&#22411;&#65292;&#21478;&#19968;&#20010;&#26159;&#22522;&#20110;&#32463;&#39564;&#20844;&#24335;&#30340;&#27169;&#22411;&#12290;&#23545;&#20110;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#65292;&#25105;&#20204;&#39318;&#20808;&#25214;&#21040;&#20102;&#22823;&#37327;&#30340;&#20844;&#24320;&#25968;&#25454;&#65292;&#21253;&#25324;&#36807;&#21435;&#20116;&#24180;&#32593;&#29699;&#27604;&#36187;&#30340;&#20844;&#24320;&#25968;&#25454;&#21644;&#29699;&#21592;&#30340;&#20010;&#20154;&#20449;&#24687;&#25968;&#25454;&#12290;&#28982;&#21518;&#23545;&#25968;&#25454;&#36827;&#34892;&#39044;&#22788;&#29702;&#21644;&#29305;&#24449;&#24037;&#31243;&#22788;&#29702;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;SVM&#12289;Random Forrest&#31639;&#27861;&#21644;XGBoost&#30340;&#34701;&#21512;&#27169;&#22411;&#12290;&#23545;&#20110;&#26426;&#21046;&#20998;&#26512;&#27169;&#22411;&#65292;&#22522;&#20110;&#35768;&#22810;&#32593;&#29699;&#36816;&#21160;&#21592;&#21644;&#29233;&#22909;&#32773;&#30340;&#24314;&#35758;&#65292;&#36873;&#25321;&#20102;&#37325;&#35201;&#29305;&#24449;&#65292;&#20351;&#29992;&#28369;&#21160;&#31383;&#21475;&#31639;&#27861;&#35745;&#31639;&#26435;&#37325;&#65292;&#21644;&#19981;&#21516;&#30340;met
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12149v1 Announce Type: new  Abstract: Tennis is so popular that coaches and players are curious about factors other than skill, such as momentum. This article will try to define and quantify momentum, providing a basis for real-time analysis of tennis matches. Based on the tennis Grand Slam men's singles match data in recent years, we built two models, one is to build a model based on data-driven, and the other is to build a model based on empirical formulas. For the data-driven model, we first found a large amount of public data including public data on tennis matches in the past five years and personal information data of players. Then the data is preprocessed, and feature engineered, and a fusion model of SVM, Random Forrest algorithm and XGBoost was established. For the mechanism analysis model, important features were selected based on the suggestions of many tennis players and enthusiasts, the sliding window algorithm was used to calculate the weight, and different met
&lt;/p&gt;</description></item><item><title>Meta Ranking&#26041;&#27861;&#36890;&#36807;&#27604;&#36739;&#30446;&#26631;&#26597;&#35810;-&#21709;&#24212;&#23545;&#19982;&#21442;&#32771;&#26597;&#35810;-&#21709;&#24212;&#23545;&#26469;&#20351;&#36739;&#19981;&#20855;&#22791;&#33021;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#26377;&#25928;&#22320;&#35780;&#20272;&#21333;&#20010;&#21709;&#24212;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.12146</link><description>&lt;p&gt;
Meta Ranking: Less Capable Language Models are Capable for Single Response Judgement
&lt;/p&gt;
&lt;p&gt;
Meta Ranking: Less Capable Language Models are Capable for Single Response Judgement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12146
&lt;/p&gt;
&lt;p&gt;
Meta Ranking&#26041;&#27861;&#36890;&#36807;&#27604;&#36739;&#30446;&#26631;&#26597;&#35810;-&#21709;&#24212;&#23545;&#19982;&#21442;&#32771;&#26597;&#35810;-&#21709;&#24212;&#23545;&#26469;&#20351;&#36739;&#19981;&#20855;&#22791;&#33021;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#26377;&#25928;&#22320;&#35780;&#20272;&#21333;&#20010;&#21709;&#24212;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24191;&#27867;&#20219;&#21153;&#20013;&#23637;&#29616;&#24378;&#22823;&#24615;&#33021;&#65292;&#20294;&#20173;&#38754;&#20020;&#24187;&#35273;&#31561;&#21487;&#38752;&#24615;&#25361;&#25112;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20687;GPT-4&#36825;&#26679;&#39640;&#33021;&#21147;&#30340;LLMs&#22312;&#35780;&#20272;&#21333;&#20010;&#21709;&#24212;&#30340;&#21487;&#38752;&#24615;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#65292;&#32780;&#36739;&#19981;&#20855;&#22791;&#33021;&#21147;&#30340;LLMs&#36890;&#24120;&#34987;&#35843;&#25972;&#26469;&#35780;&#20272;&#23545;&#30456;&#21516;&#26597;&#35810;&#30340;&#21709;&#24212;&#30340;&#30456;&#23545;&#21487;&#38752;&#24615;&#12290;&#20026;&#20102;&#20351;&#36739;&#19981;&#20855;&#22791;&#33021;&#21147;&#30340;LLMs&#26377;&#25928;&#22320;&#35780;&#20272;&#21333;&#20010;&#21709;&#24212;&#30340;&#21487;&#38752;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;$\textit{Meta}$ $\textit{Ranking}$&#65288;MR&#65289;&#30340;&#26032;&#26041;&#27861;&#12290;&#19982;&#20808;&#21069;&#30452;&#25509;&#35780;&#20272;&#21709;&#24212;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#30446;&#26631;&#26597;&#35810;-&#21709;&#24212;&#23545;&#19982;&#21442;&#32771;&#26597;&#35810;-&#21709;&#24212;&#23545;&#36827;&#34892;&#27604;&#36739;&#26469;&#23454;&#29616;&#21028;&#26029;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#25512;&#29702;&#20219;&#21153;&#30340;LLM&#21709;&#24212;&#30340;&#38169;&#35823;&#26816;&#27979;&#20013;&#65292;MR&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#26377;&#25928;&#24615;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;&#36739;&#19981;&#20855;&#22791;&#33021;&#21147;&#30340;LLMs&#20063;&#33021;&#32988;&#36807;&#24378;&#22522;&#32447;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;MR&#21487;&#20197;&#34987;&#29992;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12146v1 Announce Type: cross  Abstract: Although Large Language Models (LLMs) have demonstrated strong performance on a wide range of tasks, they still face reliability challenges such as hallucination. Previous studies reveal that highly capable LLMs like GPT-4 are effective in judging the reliability of individual responses, while less capable ones are often tuned to evaluate the relative reliability of responses to the same query. To enable less capable LLMs to effectively judge the reliability of individual responses, we propose a novel method named $\textit{Meta}$ $\textit{Ranking}$ (MR). Unlike previous methods, which assess the response directly, we achieve the judgement by comparing the target query-response pair with reference query-response pairs. We found its remarkable effectiveness in error detection for LLM responses on reasoning tasks, where less capable LLMs could outperform strong baselines, even without fine-tuning. We further demonstrate that MR can be use
&lt;/p&gt;</description></item><item><title>&#32852;&#37030;&#36125;&#21494;&#26031;&#32593;&#32476;&#38598;&#25104;(FBNE)&#22312;&#32852;&#37030;&#35774;&#32622;&#19979;&#34920;&#29616;&#20248;&#24322;&#65292;&#30456;&#36739;&#20110;&#26412;&#22320;&#27169;&#22411;&#21644;VertiBayes&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#22312;&#20445;&#25345;&#31867;&#20284;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#20102;&#26174;&#33879;&#30340;&#35757;&#32451;&#36895;&#24230;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.12142</link><description>&lt;p&gt;
&#32852;&#37030;&#36125;&#21494;&#26031;&#32593;&#32476;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
Federated Bayesian Network Ensembles
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12142
&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#36125;&#21494;&#26031;&#32593;&#32476;&#38598;&#25104;(FBNE)&#22312;&#32852;&#37030;&#35774;&#32622;&#19979;&#34920;&#29616;&#20248;&#24322;&#65292;&#30456;&#36739;&#20110;&#26412;&#22320;&#27169;&#22411;&#21644;VertiBayes&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#22312;&#20445;&#25345;&#31867;&#20284;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#20102;&#26174;&#33879;&#30340;&#35757;&#32451;&#36895;&#24230;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12142v1 &#20844;&#24067;&#31867;&#22411;: &#26032;&#30340; &#25688;&#35201;: &#32852;&#37030;&#23398;&#20064;&#20801;&#35768;&#25105;&#20204;&#22312;&#21435;&#20013;&#24515;&#21270;&#25968;&#25454;&#19978;&#36816;&#34892;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#24403;&#30001;&#20110;&#38544;&#31169;&#38382;&#39064;&#32780;&#19981;&#20801;&#35768;&#25968;&#25454;&#20849;&#20139;&#26102;&#12290;&#22522;&#20110;&#38598;&#25104;&#30340;&#23398;&#20064;&#36890;&#36807;&#35757;&#32451;&#22810;&#20010;(&#24369;)&#20998;&#31867;&#22120;&#65292;&#28982;&#21518;&#23545;&#20854;&#36755;&#20986;&#36827;&#34892;&#32858;&#21512;&#12290;&#32852;&#37030;&#38598;&#25104;&#26159;&#24212;&#29992;&#20110;&#32852;&#37030;&#35774;&#32622;&#30340;&#38598;&#25104;&#65292;&#20854;&#20013;&#38598;&#25104;&#20013;&#30340;&#27599;&#20010;&#20998;&#31867;&#22120;&#37117;&#22312;&#19968;&#20010;&#25968;&#25454;&#20301;&#32622;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#19968;&#31995;&#21015;&#23454;&#39564;&#20013;&#20351;&#29992;&#32852;&#37030;&#36125;&#21494;&#26031;&#32593;&#32476;&#38598;&#25104;(FBNE)&#30340;&#24773;&#20917;&#65292;&#24182;&#23558;&#20854;&#24615;&#33021;&#19982;&#26412;&#22320;&#35757;&#32451;&#27169;&#22411;&#21644;&#20351;&#29992;VertiBayes&#36827;&#34892;&#35757;&#32451;&#30340;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#65292;&#21518;&#32773;&#26159;&#19968;&#31181;&#29992;&#20110;&#20174;&#21435;&#20013;&#24515;&#21270;&#25968;&#25454;&#35757;&#32451;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;FBNE&#30340;&#34920;&#29616;&#20248;&#20110;&#26412;&#22320;&#27169;&#22411;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#22312;&#32500;&#25345;&#31867;&#20284;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#27604;VertiBayes&#25552;&#20379;&#20102;&#26174;&#33879;&#30340;&#35757;&#32451;&#36895;&#24230;&#22686;&#21152;&#65292;&#21478;&#22806;&#36824;&#20855;&#26377;&#20854;&#20182;&#20248;&#21183;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;FBNE&#26159;&#32852;&#37030;&#29615;&#22659;&#20013;&#28508;&#22312;&#26377;&#29992;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12142v1 Announce Type: new  Abstract: Federated learning allows us to run machine learning algorithms on decentralized data when data sharing is not permitted due to privacy concerns. Ensemble-based learning works by training multiple (weak) classifiers whose output is aggregated. Federated ensembles are ensembles applied to a federated setting, where each classifier in the ensemble is trained on one data location.   In this article, we explore the use of federated ensembles of Bayesian networks (FBNE) in a range of experiments and compare their performance with locally trained models and models trained with VertiBayes, a federated learning algorithm to train Bayesian networks from decentralized data. Our results show that FBNE outperforms local models and provides a significant increase in training speed compared with VertiBayes while maintaining a similar performance in most settings, among other advantages. We show that FBNE is a potentially useful tool within the federat
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#20013;&#24515;&#30340;&#26041;&#27861;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#23558;&#39321;&#27700;&#20998;&#23376;&#32467;&#26500;&#19982;&#20154;&#31867;&#21957;&#35273;&#24863;&#30693;&#30456;&#36830;&#30340;&#28151;&#21512;&#27169;&#22411;&#65292;&#21033;&#29992;AI&#39537;&#21160;&#30340;&#20998;&#23376;&#29983;&#25104;&#22120;&#21644;&#28909;&#21147;&#23398;&#27169;&#22411;&#65292;&#20248;&#21270;&#28342;&#21058;&#21644;&#20998;&#23376;&#32452;&#21512;&#65292;&#26368;&#32456;&#36890;&#36807;&#25968;&#23398;&#20248;&#21270;&#38382;&#39064;&#26368;&#23567;&#21270;&#26032;&#26087;&#21957;&#35273;&#20307;&#39564;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2402.12134</link><description>&lt;p&gt;
&#20998;&#23376;&#29983;&#25104;&#21644;&#20248;&#21270;&#65292;&#29992;&#20110;&#39640;&#25928;&#39321;&#27675;&#21019;&#36896;
&lt;/p&gt;
&lt;p&gt;
Molecule Generation and Optimization for Efficient Fragrance Creation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12134
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#20013;&#24515;&#30340;&#26041;&#27861;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#23558;&#39321;&#27700;&#20998;&#23376;&#32467;&#26500;&#19982;&#20154;&#31867;&#21957;&#35273;&#24863;&#30693;&#30456;&#36830;&#30340;&#28151;&#21512;&#27169;&#22411;&#65292;&#21033;&#29992;AI&#39537;&#21160;&#30340;&#20998;&#23376;&#29983;&#25104;&#22120;&#21644;&#28909;&#21147;&#23398;&#27169;&#22411;&#65292;&#20248;&#21270;&#28342;&#21058;&#21644;&#20998;&#23376;&#32452;&#21512;&#65292;&#26368;&#32456;&#36890;&#36807;&#25968;&#23398;&#20248;&#21270;&#38382;&#39064;&#26368;&#23567;&#21270;&#26032;&#26087;&#21957;&#35273;&#20307;&#39564;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20197;&#26426;&#22120;&#23398;&#20064;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23454;&#39564;&#37327;&#21270;&#39321;&#27700;&#24863;&#30693;&#26469;&#22797;&#21046;&#21957;&#35273;&#20307;&#39564;&#12290;&#20851;&#38190;&#36129;&#29486;&#21253;&#25324;&#19968;&#20010;&#23558;&#39321;&#27700;&#20998;&#23376;&#32467;&#26500;&#19982;&#20154;&#31867;&#21957;&#35273;&#24863;&#30693;&#30456;&#36830;&#30340;&#28151;&#21512;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#21253;&#25324;&#19968;&#20010;&#21033;&#29992;&#22270;&#24418;&#21644;&#29983;&#25104;&#24335;&#31070;&#32463;&#32593;&#32476;&#30340;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#20998;&#23376;&#29983;&#25104;&#22120;&#65292;&#23545;&#27668;&#21619;&#24378;&#24230;&#36827;&#34892;&#37327;&#21270;&#21644;&#39044;&#27979;&#65292;&#20197;&#21450;&#20026;&#26399;&#26395;&#39321;&#27675;&#20248;&#21270;&#30340;&#26368;&#20339;&#28342;&#21058;&#21644;&#20998;&#23376;&#32452;&#21512;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#28909;&#21147;&#23398;&#30340;&#27169;&#22411;&#24314;&#31435;&#20102;&#21957;&#35273;&#24863;&#30693;&#19982;&#28082;&#30456;&#27987;&#24230;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#65292;&#22522;&#20110;&#33976;&#27773;&#21387;&#21644;&#39321;&#21619;&#38899;&#31526;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#20998;&#23376;&#12290;&#26368;&#32456;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#25968;&#23398;&#20248;&#21270;&#38382;&#39064;&#65292;&#20197;&#26368;&#23567;&#21270;&#26032;&#26087;&#21957;&#35273;&#20307;&#39564;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#37325;&#29616;&#20004;&#31181;&#29420;&#29305;&#39321;&#27700;&#30340;&#21957;&#35273;&#20307;&#39564;&#26469;&#36827;&#34892;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12134v1 Announce Type: cross  Abstract: This research introduces a Machine Learning-centric approach to replicate olfactory experiences, validated through experimental quantification of perfume perception. Key contributions encompass a hybrid model connecting perfume molecular structure to human olfactory perception. This model includes an AI-driven molecule generator (utilizing Graph and Generative Neural Networks), quantification and prediction of odor intensity, and refinery of optimal solvent and molecule combinations for desired fragrances. Additionally, a thermodynamic-based model establishes a link between olfactory perception and liquid-phase concentrations. The methodology employs Transfer Learning and selects the most suitable molecules based on vapor pressure and fragrance notes. Ultimately, a mathematical optimization problem is formulated to minimize discrepancies between new and target olfactory experiences. The methodology is validated by reproducing two disti
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;DualView&#65292;&#19968;&#31181;&#22522;&#20110;&#26367;&#20195;&#24314;&#27169;&#30340;&#21518;&#26399;&#25968;&#25454;&#24402;&#22240;&#26041;&#27861;&#65292;&#20855;&#26377;&#39640;&#25928;&#35745;&#31639;&#21644;&#20248;&#36136;&#35780;&#20272;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.12118</link><description>&lt;p&gt;
DualView&#65306;&#21452;&#37325;&#35270;&#35282;&#19979;&#30340;&#25968;&#25454;&#24402;&#22240;
&lt;/p&gt;
&lt;p&gt;
DualView: Data Attribution from the Dual Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12118
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;DualView&#65292;&#19968;&#31181;&#22522;&#20110;&#26367;&#20195;&#24314;&#27169;&#30340;&#21518;&#26399;&#25968;&#25454;&#24402;&#22240;&#26041;&#27861;&#65292;&#20855;&#26377;&#39640;&#25928;&#35745;&#31639;&#21644;&#20248;&#36136;&#35780;&#20272;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DualView&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#26367;&#20195;&#24314;&#27169;&#30340;&#21518;&#26399;&#25968;&#25454;&#24402;&#22240;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#39640;&#35745;&#31639;&#25928;&#29575;&#21644;&#33391;&#22909;&#30340;&#35780;&#20272;&#32467;&#26524;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#19982;&#25991;&#29486;&#30456;&#20851;&#30340;&#36866;&#24403;&#23450;&#37327;&#35780;&#20272;&#31574;&#30053;&#19979;&#35780;&#20272;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#25216;&#26415;&#65292;&#27604;&#36739;&#20102;&#19982;&#30456;&#20851;&#20027;&#35201;&#26412;&#22320;&#25968;&#25454;&#24402;&#22240;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12118v1 Announce Type: cross  Abstract: Local data attribution (or influence estimation) techniques aim at estimating the impact that individual data points seen during training have on particular predictions of an already trained Machine Learning model during test time. Previous methods either do not perform well consistently across different evaluation criteria from literature, are characterized by a high computational demand, or suffer from both. In this work we present DualView, a novel method for post-hoc data attribution based on surrogate modelling, demonstrating both high computational efficiency, as well as good evaluation results. With a focus on neural networks, we evaluate our proposed technique using suitable quantitative evaluation strategies from the literature against related principal local data attribution methods. We find that DualView requires considerably lower computational resources than other methods, while demonstrating comparable performance to comp
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#27010;&#36848;&#20102;&#20351;&#29992;&#21464;&#20998;&#26041;&#27861;&#21644;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#25104;&#20687;&#20013;&#36870;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#37325;&#28857;&#22312;&#20110;&#28857;&#20272;&#35745;&#22120;&#23545;&#25239;&#24615;&#25200;&#21160;&#19979;&#30340;&#40065;&#26834;&#24615;&#20197;&#21450;&#25506;&#32034;&#25968;&#25454;&#19968;&#33268;&#35299;&#23376;&#31354;&#38388;&#20197;&#28385;&#36275;&#29305;&#23450;&#35821;&#20041;&#25110;&#32441;&#29702;&#29305;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.12072</link><description>&lt;p&gt;
&#21464;&#20998;&#26041;&#27861;&#19982;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#36870;&#38382;&#39064;&#20013;&#30340;&#40065;&#26834;&#24615;&#21644;&#25506;&#32034;&#65306;&#27010;&#36848;
&lt;/p&gt;
&lt;p&gt;
Robustness and Exploration of Variational and Machine Learning Approaches to Inverse Problems: An Overview
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12072
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#27010;&#36848;&#20102;&#20351;&#29992;&#21464;&#20998;&#26041;&#27861;&#21644;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#25104;&#20687;&#20013;&#36870;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#37325;&#28857;&#22312;&#20110;&#28857;&#20272;&#35745;&#22120;&#23545;&#25239;&#24615;&#25200;&#21160;&#19979;&#30340;&#40065;&#26834;&#24615;&#20197;&#21450;&#25506;&#32034;&#25968;&#25454;&#19968;&#33268;&#35299;&#23376;&#31354;&#38388;&#20197;&#28385;&#36275;&#29305;&#23450;&#35821;&#20041;&#25110;&#32441;&#29702;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35797;&#22270;&#27010;&#36848;&#20351;&#29992;&#21464;&#20998;&#26041;&#27861;&#21644;&#26426;&#22120;&#23398;&#20064;&#26469;&#35299;&#20915;&#25104;&#20687;&#20013;&#36870;&#38382;&#39064;&#30340;&#24403;&#21069;&#26041;&#27861;&#12290;&#37325;&#28857;&#20851;&#27880;&#28857;&#20272;&#35745;&#22120;&#21450;&#20854;&#23545;&#25239;&#24615;&#25200;&#21160;&#19979;&#30340;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#19968;&#32500;&#31034;&#20363;&#38382;&#39064;&#30340;&#25968;&#20540;&#23454;&#39564;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;&#19981;&#21516;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#24182;&#22312;&#32463;&#39564;&#19978;&#39564;&#35777;&#20102;&#29702;&#35770;&#20445;&#35777;&#12290;&#35813;&#32508;&#36848;&#30340;&#21478;&#19968;&#20010;&#37325;&#28857;&#26159;&#36890;&#36807;&#26126;&#30830;&#25351;&#23548;&#26469;&#25506;&#32034;&#25968;&#25454;&#19968;&#33268;&#35299;&#30340;&#23376;&#31354;&#38388;&#65292;&#20197;&#28385;&#36275;&#29305;&#23450;&#35821;&#20041;&#25110;&#32441;&#29702;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12072v1 Announce Type: cross  Abstract: This paper attempts to provide an overview of current approaches for solving inverse problems in imaging using variational methods and machine learning. A special focus lies on point estimators and their robustness against adversarial perturbations. In this context results of numerical experiments for a one-dimensional toy problem are provided, showing the robustness of different approaches and empirically verifying theoretical guarantees. Another focus of this review is the exploration of the subspace of data consistent solutions through explicit guidance to satisfy specific semantic or textural properties.
&lt;/p&gt;</description></item><item><title>&#32463;&#30001;&#31867;&#33041;&#21551;&#21457;&#30340;&#24930;&#29305;&#24449;&#20998;&#26512;&#26041;&#27861;&#29983;&#25104;&#30340;&#21487;&#35299;&#37322;&#35270;&#35273;&#25968;&#25454;&#34920;&#31034;&#65292;&#33021;&#22815;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#25552;&#39640;&#23548;&#33322;&#20219;&#21153;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.12067</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#31867;&#33041;&#34920;&#31034;&#25552;&#21319;&#35270;&#35273;&#23548;&#33322;&#20219;&#21153;&#20013;&#24378;&#21270;&#23398;&#20064;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Interpretable Brain-Inspired Representations Improve RL Performance on Visual Navigation Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12067
&lt;/p&gt;
&lt;p&gt;
&#32463;&#30001;&#31867;&#33041;&#21551;&#21457;&#30340;&#24930;&#29305;&#24449;&#20998;&#26512;&#26041;&#27861;&#29983;&#25104;&#30340;&#21487;&#35299;&#37322;&#35270;&#35273;&#25968;&#25454;&#34920;&#31034;&#65292;&#33021;&#22815;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#25552;&#39640;&#23548;&#33322;&#20219;&#21153;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#23548;&#33322;&#38656;&#35201;&#19968;&#31995;&#21015;&#33021;&#21147;&#12290;&#20854;&#20013;&#19968;&#20010;&#20851;&#38190;&#33021;&#21147;&#26159;&#20195;&#29702;&#33021;&#22815;&#30830;&#23450;&#20854;&#22312;&#29615;&#22659;&#20013;&#30340;&#20301;&#32622;&#21644;&#26397;&#21521;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#36890;&#24120;&#20250;&#20551;&#35774;&#36825;&#20123;&#20449;&#24687;&#24050;&#30693;&#65292;&#25110;&#32773;&#20351;&#29992;&#32570;&#20047;&#36866;&#24403;&#24402;&#32435;&#20559;&#24046;&#24182;&#38543;&#26102;&#38388;&#32047;&#31215;&#35823;&#24046;&#30340;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21463;&#31070;&#32463;&#31185;&#23398;&#30740;&#31350;&#21551;&#21457;&#30340;&#24930;&#29305;&#24449;&#20998;&#26512;&#65288;SFA&#65289;&#26041;&#27861;&#22914;&#20309;&#20811;&#26381;&#36825;&#20004;&#20010;&#38480;&#21046;&#65292;&#36890;&#36807;&#29983;&#25104;&#21487;&#35299;&#37322;&#30340;&#35270;&#35273;&#25968;&#25454;&#34920;&#31034;&#26469;&#32534;&#30721;&#20195;&#29702;&#30340;&#20301;&#32622;&#21644;&#26397;&#21521;&#12290;&#25105;&#20204;&#22312;&#29616;&#20195;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#20013;&#24212;&#29992;SFA&#65292;&#20998;&#26512;&#21644;&#27604;&#36739;&#34920;&#31034;&#65292;&#24182;&#23637;&#31034;&#20102;&#23618;&#32423;SFA&#22312;&#23548;&#33322;&#20219;&#21153;&#19978;&#33021;&#22815;&#32988;&#36807;&#20854;&#20182;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12067v1 Announce Type: new  Abstract: Visual navigation requires a whole range of capabilities. A crucial one of these is the ability of an agent to determine its own location and heading in an environment. Prior works commonly assume this information as given, or use methods which lack a suitable inductive bias and accumulate error over time. In this work, we show how the method of slow feature analysis (SFA), inspired by neuroscience research, overcomes both limitations by generating interpretable representations of visual data that encode location and heading of an agent. We employ SFA in a modern reinforcement learning context, analyse and compare representations and illustrate where hierarchical SFA can outperform other feature extractors on navigation tasks.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;WKVQuant&#65292;&#19968;&#31181;&#19987;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#30340;&#37327;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#37327;&#21270;&#26435;&#37325;&#21644;&#38190;&#20540;&#32531;&#23384;&#26469;&#25913;&#21892;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.12065</link><description>&lt;p&gt;
WKVQuant&#65306;&#37327;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#26435;&#37325;&#21644;&#38190;&#20540;&#32531;&#23384;&#20197;&#25552;&#39640;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
WKVQuant: Quantizing Weight and Key/Value Cache for Large Language Models Gains More
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12065
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;WKVQuant&#65292;&#19968;&#31181;&#19987;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#30340;&#37327;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#37327;&#21270;&#26435;&#37325;&#21644;&#38190;&#20540;&#32531;&#23384;&#26469;&#25913;&#21892;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38754;&#20020;&#30528;&#37096;&#32626;&#25361;&#25112;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#20854;&#24040;&#22823;&#30340;&#20869;&#23384;&#38656;&#27714;&#21644;&#33258;&#22238;&#24402;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#30340;&#35745;&#31639;&#38656;&#27714;&#12290;&#26412;&#25991;&#36890;&#36807;&#20851;&#27880;LLMs&#30340;&#37327;&#21270;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#37327;&#21270;&#26159;&#19968;&#31181;&#36890;&#36807;&#23558;&#27169;&#22411;&#21442;&#25968;&#21644;&#28608;&#27963;&#36716;&#25442;&#20026;&#20302;&#27604;&#29305;&#25972;&#25968;&#26469;&#20943;&#23569;&#20869;&#23384;&#28040;&#32791;&#30340;&#25216;&#26415;&#12290;&#25105;&#20204;&#25209;&#21028;&#24615;&#22320;&#20998;&#26512;&#20102;&#29616;&#26377;&#30340;&#37327;&#21270;&#26041;&#27861;&#65292;&#35782;&#21035;&#20986;&#23427;&#20204;&#22312;&#24179;&#34913;&#37327;&#21270;LLMs&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#36229;&#36234;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;WKVQuant&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#20026;&#37327;&#21270;LLMs&#30340;&#21442;&#25968;&#26435;&#37325;&#21644;&#38190;&#20540;&#65288;KV&#65289;&#32531;&#23384;&#32780;&#35774;&#35745;&#30340;PTQ&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20165;&#32771;&#34385;&#36807;&#21435;&#30340;&#37327;&#21270;&#20197;&#25913;&#21892;&#27880;&#24847;&#21147;&#35745;&#31639;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#20108;&#32500;&#37327;&#21270;&#31574;&#30053;&#26469;&#22788;&#29702;KV&#32531;&#23384;&#30340;&#20998;&#24067;&#65292;&#20197;&#21450;&#19968;&#31181;&#36328;&#22359;&#37325;&#24314;&#27491;&#21017;&#21270;&#26041;&#27861;&#20197;&#24110;&#21161;&#27169;&#22411;&#21387;&#32553;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12065v1 Announce Type: cross  Abstract: Large Language Models (LLMs) face significant deployment challenges due to their substantial memory requirements and the computational demands of auto-regressive text generation process. This paper addresses these challenges by focusing on the quantization of LLMs, a technique that reduces memory consumption by converting model parameters and activations into low-bit integers. We critically analyze the existing quantization approaches, identifying their limitations in balancing the accuracy and efficiency of the quantized LLMs. To advance beyond these limitations, we propose WKVQuant, a PTQ framework especially designed for quantizing weights and the key/value (KV) cache of LLMs. Specifically, we incorporates past-only quantization to improve the computation of attention. Additionally, we introduce two-dimensional quantization strategy to handle the distribution of KV cache, along with a cross-block reconstruction regularization for pa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#20844;&#24179;&#24615;&#21407;&#21017;&#8212;&#8212;&#24179;&#31561;&#20445;&#25252;&#65292;&#20854;&#20851;&#38190;&#22312;&#20110;&#23558;&#38169;&#35823;&#20998;&#31867;&#30340;&#39118;&#38505;&#22343;&#31561;&#21270;&#65292;&#36991;&#20813;&#20102;&#35768;&#22810;&#23545;&#20256;&#32479;&#20998;&#31867;&#24179;&#31561;&#21407;&#21017;&#30340;&#21453;&#20363;&#12290;</title><link>https://arxiv.org/abs/2402.12062</link><description>&lt;p&gt;
&#22240;&#26524;&#24179;&#31561;&#20445;&#25252;&#19982;&#31639;&#27861;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Causal Equal Protection as Algorithmic Fairness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12062
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#20844;&#24179;&#24615;&#21407;&#21017;&#8212;&#8212;&#24179;&#31561;&#20445;&#25252;&#65292;&#20854;&#20851;&#38190;&#22312;&#20110;&#23558;&#38169;&#35823;&#20998;&#31867;&#30340;&#39118;&#38505;&#22343;&#31561;&#21270;&#65292;&#36991;&#20813;&#20102;&#35768;&#22810;&#23545;&#20256;&#32479;&#20998;&#31867;&#24179;&#31561;&#21407;&#21017;&#30340;&#21453;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#21313;&#24180;&#65292;&#35745;&#31639;&#26426;&#31185;&#23398;&#21644;&#21746;&#23398;&#30340;&#25991;&#29486;&#24418;&#25104;&#20102;&#19981;&#21516;&#30340;&#31639;&#27861;&#20844;&#24179;&#24615;&#26631;&#20934;&#12290;&#20854;&#20013;&#26368;&#21463;&#20105;&#35758;&#30340;&#20998;&#31867;&#24179;&#31561;&#35201;&#27714;&#65292;&#39044;&#27979;&#31639;&#27861;&#30340;&#38169;&#35823;&#20998;&#31867;&#22312;&#34987;&#20445;&#25252;&#29305;&#24449;&#25152;&#25351;&#31034;&#30340;&#32676;&#20307;&#20013;&#20197;&#30456;&#31561;&#39057;&#29575;&#21457;&#29983;&#12290;&#23613;&#31649;&#20998;&#31867;&#24179;&#31561;&#20855;&#26377;&#30452;&#35266;&#21560;&#24341;&#21147;&#65292;&#20294;&#24050;&#21463;&#21040;&#25915;&#20987;&#12290;&#25105;&#20204;&#36716;&#21521;&#19968;&#20010;&#30456;&#20851;&#21407;&#21017;&#65292;&#21363;&#24179;&#31561;&#20445;&#25252;&#65292;&#35813;&#21407;&#21017;&#26368;&#21021;&#26159;&#22312;&#21009;&#20107;&#21496;&#27861;&#39046;&#22495;&#21457;&#23637;&#36215;&#26469;&#30340;&#12290;&#24179;&#31561;&#20445;&#25252;&#30340;&#20851;&#38190;&#22312;&#20110;&#23558;&#38169;&#35823;&#20998;&#31867;&#30340;&#39118;&#38505;&#65288;&#23558;&#22312;&#35268;&#23450;&#30340;&#24847;&#20041;&#19978;&#20855;&#20307;&#35828;&#26126;&#65289;&#36827;&#34892;&#22343;&#31561;&#21270;&#65292;&#32780;&#19981;&#26159;&#23558;&#38169;&#35823;&#20998;&#31867;&#30340;&#27604;&#29575;&#22343;&#31561;&#21270;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24179;&#31561;&#20445;&#25252;&#36991;&#20813;&#20102;&#35768;&#22810;&#23545;&#20998;&#31867;&#24179;&#31561;&#30340;&#21453;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12062v1 Announce Type: cross  Abstract: Over the last ten years the literature in computer science and philosophy has formulated different criteria of algorithmic fairness. One of the most discussed, classification parity, requires that the erroneous classifications of a predictive algorithm occur with equal frequency for groups picked out by protected characteristics. Despite its intuitive appeal, classification parity has come under attack. Multiple scenarios can be imagined in which - intuitively - a predictive algorithm does not treat any individual unfairly, and yet classification parity is violated. To make progress, we turn to a related principle, equal protection, originally developed in the context of criminal justice. Key to equal protection is equalizing the risks of erroneous classifications (in a sense to be specified) as opposed to equalizing the rates of erroneous classifications. We show that equal protection avoids many of the counterexamples to classificati
&lt;/p&gt;</description></item><item><title>LONDI&#26694;&#26550;&#21487;&#20197;&#22312;&#38656;&#35201;&#22797;&#26434;&#20915;&#31574;&#21644;&#25512;&#29702;&#30340;&#22320;&#26041;&#36873;&#25321;&#24615;&#22320;&#20351;&#29992;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#26497;&#22823;&#22320;&#38477;&#20302;&#20102;&#36164;&#28304;&#28040;&#32791;&#12290;</title><link>https://arxiv.org/abs/2402.12061</link><description>&lt;p&gt;
&#25152;&#26377;&#35821;&#35328;&#27169;&#22411;&#30340;&#22823;&#23567;&#37117;&#19968;&#26679;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
All Language Models Large and Small
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12061
&lt;/p&gt;
&lt;p&gt;
LONDI&#26694;&#26550;&#21487;&#20197;&#22312;&#38656;&#35201;&#22797;&#26434;&#20915;&#31574;&#21644;&#25512;&#29702;&#30340;&#22320;&#26041;&#36873;&#25321;&#24615;&#22320;&#20351;&#29992;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#26497;&#22823;&#22320;&#38477;&#20302;&#20102;&#36164;&#28304;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#39046;&#20808;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#22312;&#35757;&#32451;&#21644;&#25191;&#34892;&#36807;&#31243;&#20013;&#20351;&#29992;&#39640;&#24378;&#24230;&#35745;&#31639;&#36164;&#28304;&#65292;&#36825;&#23545;&#20110;&#38477;&#20302;&#37096;&#32626;&#36164;&#28304;&#25104;&#26412;&#21644;&#26356;&#24555;&#25191;&#34892;&#20915;&#31574;&#20219;&#21153;&#31561;&#26041;&#38754;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#35821;&#35328;&#20248;&#21270;&#32593;&#32476;&#20998;&#24067;&#65288;LONDI&#65289;&#26694;&#26550;&#30340;&#26032;&#22411;&#21363;&#25554;&#21363;&#29992;LM&#26694;&#26550;&#12290; LONDI&#23398;&#20250;&#20102;&#22312;&#38656;&#35201;&#36827;&#34892;&#22797;&#26434;&#20915;&#31574;&#21644;&#25512;&#29702;&#30340;&#22320;&#26041;&#36873;&#25321;&#24615;&#22320;&#20351;&#29992;&#22823;&#30340;LM&#65292;&#32780;&#22312;&#20854;&#20182;&#22320;&#26041;&#20351;&#29992;&#20302;&#36164;&#28304;&#30340;LM&#12290; LONDI&#30001;&#20004;&#20010;&#65288;&#31163;&#32447;&#65289;&#31574;&#30053;&#32593;&#32476;&#31995;&#32479;&#12289;&#19968;&#20010;LM&#12289;&#19968;&#20010;&#22823;&#30340;LM&#65288;LLM)&#21644;&#19968;&#20010;&#20351;&#29992;&#24320;&#20851;&#25511;&#21046;&#24555;&#36895;&#23398;&#20064;&#20309;&#26102;&#35843;&#29992;LLM&#30340;&#24378;&#21270;&#23398;&#20064;&#27169;&#22359;&#32452;&#25104;&#12290; &#28982;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;LLM&#35843;&#29992;&#21644;&#36164;&#28304;&#20351;&#29992;&#26041;&#38754;&#20445;&#25345;&#39044;&#31639;&#32422;&#26463;&#30340;LONDI&#21464;&#20307;&#12290; &#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;LONDI&#23398;&#20064;&#28608;&#27963;&#25152;&#38656;&#35299;&#20915;&#20219;&#21153;&#30340;LLM&#30340;&#31995;&#32479;&#29366;&#24577;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12061v1 Announce Type: cross  Abstract: Many leading language models (LMs) use high-intensity computational resources both during training and execution. This poses the challenge of lowering resource costs for deployment and faster execution of decision-making tasks among others. We introduce a novel plug-and-play LM framework named Language Optimising Network Distribution (LONDI) framework. LONDI learns to selectively employ large LMs only where complex decision-making and reasoning are required while using low-resource LMs everywhere else. LONDI consists of a system of two (off-)policy networks, an LM, a large LM (LLM), and a reinforcement learning module that uses switching controls to quickly learn which system states to call the LLM. We then introduce a variant of LONDI that maintains budget constraints on LLM calls and hence its resource usage. Theoretically, we prove LONDI learns the subset of system states to activate the LLM required to solve the task. We then prove
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32447;&#24615;&#36172;&#21338;&#26426;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#32447;&#24615;&#38543;&#26426;&#36172;&#21338;&#26426;&#20013;&#26368;&#23567;&#26497;&#23567;&#36951;&#25022;&#30340;&#22810;&#23545;&#25968;&#32553;&#25918;&#38382;&#39064;&#65292;&#36890;&#36807;&#21152;&#26435;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#23454;&#29616;&#23545;&#35774;&#35745;&#30697;&#38453;&#29305;&#24449;&#20540;&#20851;&#31995;&#30340;&#25511;&#21046;&#65292;&#23454;&#29616;&#20102;&#32047;&#31215;&#36951;&#25022;&#30340;&#23545;&#25968;&#32553;&#25918;&#12290;</title><link>https://arxiv.org/abs/2402.12042</link><description>&lt;p&gt;
&#20855;&#26377;&#22810;&#23545;&#25968;&#26497;&#23567;&#26497;&#23567;&#36951;&#25022;&#30340;&#32447;&#24615;&#36172;&#21338;&#26426;
&lt;/p&gt;
&lt;p&gt;
Linear bandits with polylogarithmic minimax regret
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12042
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32447;&#24615;&#36172;&#21338;&#26426;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#32447;&#24615;&#38543;&#26426;&#36172;&#21338;&#26426;&#20013;&#26368;&#23567;&#26497;&#23567;&#36951;&#25022;&#30340;&#22810;&#23545;&#25968;&#32553;&#25918;&#38382;&#39064;&#65292;&#36890;&#36807;&#21152;&#26435;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#23454;&#29616;&#23545;&#35774;&#35745;&#30697;&#38453;&#29305;&#24449;&#20540;&#20851;&#31995;&#30340;&#25511;&#21046;&#65292;&#23454;&#29616;&#20102;&#32047;&#31215;&#36951;&#25022;&#30340;&#23545;&#25968;&#32553;&#25918;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#32447;&#24615;&#38543;&#26426;&#36172;&#21338;&#26426;&#30340;&#22122;&#22768;&#27169;&#22411;&#65292;&#23545;&#20110;&#35813;&#27169;&#22411;&#65292;&#24403;&#25105;&#20204;&#36873;&#25321;&#36234;&#26469;&#36234;&#25509;&#36817;&#26410;&#30693;&#21521;&#37327;&#30340;&#21333;&#20301;&#29699;&#19978;&#30340;&#21160;&#20316;&#26102;&#65292;&#20122;&#39640;&#26031;&#22122;&#22768;&#21442;&#25968;&#20197;&#32447;&#24615;&#26041;&#24335;&#28040;&#22833;&#12290;&#25105;&#20204;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#24341;&#20837;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#20854;&#22312;&#26102;&#38388;&#38271;&#24230;$T$&#30340;&#24773;&#20917;&#19979;&#21576;&#23545;&#25968;$^3&#65288;T&#65289;$&#30340;&#26368;&#23567;&#36951;&#25022;&#32553;&#25918;&#65292;&#19982;&#20856;&#22411;&#36172;&#21338;&#26426;&#31639;&#27861;&#30340;&#24179;&#26041;&#26681;&#36951;&#25022;&#32553;&#25918;&#24418;&#25104;&#40092;&#26126;&#23545;&#27604;&#12290;&#25105;&#20204;&#30340;&#31574;&#30053;&#22522;&#20110;&#21152;&#26435;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#65292;&#36890;&#36807;&#20960;&#20309;&#35770;&#35777;&#23454;&#29616;&#20102;&#35774;&#35745;&#30697;&#38453;$V_t$&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;$t$&#22788;&#30340;&#29305;&#24449;&#20540;&#20851;&#31995;$\lambda_{\min} ( V_t ) = \Omega (\sqrt{\lambda_{\max}(V_t ) })$&#65292;&#36825;&#20123;&#20960;&#20309;&#35770;&#35777;&#19982;&#22122;&#22768;&#27169;&#22411;&#26080;&#20851;&#65292;&#24182;&#21487;&#33021;&#20855;&#26377;&#29420;&#31435;&#30340;&#20852;&#36259;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#20005;&#26684;&#25511;&#21046;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#30340;&#26399;&#26395;&#36951;&#25022;&#20026;$O(\frac1{t})$&#30340;&#25968;&#37327;&#32423;&#65292;&#20174;&#32780;&#23548;&#33268;&#32047;&#31215;&#36951;&#25022;&#30340;&#23545;&#25968;&#32553;&#25918;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12042v1 Announce Type: cross  Abstract: We study a noise model for linear stochastic bandits for which the subgaussian noise parameter vanishes linearly as we select actions on the unit sphere closer and closer to the unknown vector. We introduce an algorithm for this problem that exhibits a minimax regret scaling as $\log^3(T)$ in the time horizon $T$, in stark contrast the square root scaling of this regret for typical bandit algorithms. Our strategy, based on weighted least-squares estimation, achieves the eigenvalue relation $\lambda_{\min} ( V_t ) = \Omega (\sqrt{\lambda_{\max}(V_t ) })$ for the design matrix $V_t$ at each time step $t$ through geometrical arguments that are independent of the noise model and might be of independent interest. This allows us to tightly control the expected regret in each time step to be of the order $O(\frac1{t})$, leading to the logarithmic scaling of the cumulative regret.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Self-AMPLIFY&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20107;&#21518;&#35299;&#37322;&#26041;&#27861;&#24212;&#29992;&#20110;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;SLMs&#65289;&#65292;&#33258;&#21160;&#29983;&#25104;&#22522;&#20110;&#21407;&#22240;&#30340;&#35299;&#37322;&#65292;&#20197;&#25552;&#39640;&#23427;&#20204;&#33258;&#36523;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.12038</link><description>&lt;p&gt;
Self-AMPLIFY&#65306;&#36890;&#36807;&#33258;&#25105;&#20107;&#21518;&#35299;&#37322;&#25913;&#36827;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Self-AMPLIFY: Improving Small Language Models with Self Post Hoc Explanations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12038
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Self-AMPLIFY&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20107;&#21518;&#35299;&#37322;&#26041;&#27861;&#24212;&#29992;&#20110;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;SLMs&#65289;&#65292;&#33258;&#21160;&#29983;&#25104;&#22522;&#20110;&#21407;&#22240;&#30340;&#35299;&#37322;&#65292;&#20197;&#25552;&#39640;&#23427;&#20204;&#33258;&#36523;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Self-AMPLIFY&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#24212;&#29992;&#20110;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;SLMs&#65289;&#30340;&#20107;&#21518;&#35299;&#37322;&#26041;&#27861;&#33258;&#21160;&#29983;&#25104;&#22522;&#20110;&#21407;&#22240;&#30340;&#35299;&#37322;&#65292;&#20174;&#32780;&#25552;&#39640;&#23427;&#20204;&#33258;&#36523;&#30340;&#24615;&#33021;&#12290;Self-AMPLIFY&#26159;&#19968;&#20010;3&#27493;&#39588;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#36873;&#25321;&#26679;&#26412;&#12289;&#29983;&#25104;&#29702;&#30001;&#21644;&#26500;&#24314;&#26368;&#32456;&#25552;&#31034;&#20197;&#21033;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#38656;&#35201;&#25512;&#29702;&#33021;&#21147;&#30340;SLMs&#21644;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;Self-AMPLIFY&#30340;&#24615;&#33021;&#65306;&#36825;&#20123;&#23454;&#39564;&#34920;&#26126;Self-AMPLIFY&#22312;&#19982;&#31454;&#20105;&#23545;&#25163;&#30456;&#27604;&#34920;&#29616;&#20986;&#33394;&#12290;Self-AMPLIFY&#26159;&#31532;&#19968;&#20010;&#23558;&#20107;&#21518;&#35299;&#37322;&#26041;&#27861;&#24212;&#29992;&#20110;SLMs&#30340;&#26041;&#27861;&#65292;&#20197;&#29983;&#25104;&#35299;&#37322;&#24182;&#25552;&#39640;&#23427;&#20204;&#33258;&#36523;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12038v1 Announce Type: new  Abstract: Incorporating natural language rationales in the prompt and In-Context Learning (ICL) has led to a significant improvement of Large Language Models (LLMs) performance. However, rationales currently require human-annotation or the use of auxiliary proxy models to target promising samples or generate high-quality rationales. In this work, we propose Self-AMPLIFY to generate automatically rationales from post hoc explanation methods applied to Small Language Models (SLMs) to improve their own performance. Self-AMPLIFY is a 3-step method that targets samples, generates rationales and builds a final prompt to leverage ICL. Self-AMPLIFY performance is evaluated on two SLMs and two datasets requiring reasoning abilities: these experiments show that Self-AMPLIFY achieves good results against competitors. Self-AMPLIFY is the first method to apply post hoc explanation methods to SLM to generate rationales to improve their own performance in a full
&lt;/p&gt;</description></item><item><title>&#26102;&#38388;&#24207;&#21015;&#22686;&#37327;&#23398;&#20064;&#38382;&#39064;&#22312;&#22270;&#20687;&#21644;&#35821;&#35328;&#39046;&#22495;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#26041;&#38754;&#20173;&#28982;&#30456;&#23545;&#36739;&#23569;&#30740;&#31350;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#35780;&#20272;&#21644;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.12035</link><description>&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#30340;&#22686;&#37327;&#24335;&#23398;&#20064;: &#22522;&#20934;&#21644;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Class-incremental Learning for Time Series: Benchmark and Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12035
&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#22686;&#37327;&#23398;&#20064;&#38382;&#39064;&#22312;&#22270;&#20687;&#21644;&#35821;&#35328;&#39046;&#22495;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#26041;&#38754;&#20173;&#28982;&#30456;&#23545;&#36739;&#23569;&#30740;&#31350;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#35780;&#20272;&#21644;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#23454;&#29615;&#22659;&#26412;&#36136;&#19978;&#26159;&#38750;&#24179;&#31283;&#30340;&#65292;&#32463;&#24120;&#20250;&#38543;&#26102;&#38388;&#24341;&#20837;&#26032;&#30340;&#31867;&#21035;&#12290;&#36825;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20013;&#23588;&#20026;&#24120;&#35265;&#65292;&#27604;&#22914;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20986;&#29616;&#26032;&#30340;&#30142;&#30149;&#20998;&#31867;&#65292;&#25110;&#32773;&#22312;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#20013;&#28155;&#21152;&#26032;&#30340;&#27963;&#21160;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#38656;&#35201;&#19968;&#20010;&#23398;&#20064;&#31995;&#32479;&#33021;&#22815;&#26377;&#25928;&#22320;&#21560;&#25910;&#26032;&#30340;&#31867;&#21035;&#65292;&#21516;&#26102;&#36991;&#20813;&#23545;&#26087;&#31867;&#21035;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#36825;&#23601;&#24341;&#21457;&#20102;&#22686;&#37327;&#24335;&#23398;&#20064;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#22312;&#22270;&#20687;&#21644;&#35821;&#35328;&#39046;&#22495;&#21462;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#36827;&#23637;&#65292;&#20294;&#38024;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#22686;&#37327;&#24335;&#23398;&#20064;&#20173;&#28982;&#30456;&#23545;&#36739;&#23569;&#30740;&#31350;&#12290;&#29616;&#26377;&#30740;&#31350;&#23384;&#22312;&#23454;&#39564;&#35774;&#35745;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#65292;&#38656;&#35201;&#23545;&#26041;&#27861;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#21644;&#22522;&#20934;&#27979;&#35797;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#27010;&#36848;&#20102;&#26102;&#38388;&#24207;&#21015;&#22686;&#37327;&#23398;&#20064;&#65288;TSCIL&#65289;&#38382;&#39064;&#65292;&#31361;&#20986;&#20102;&#20854;&#29420;&#29305;&#25361;&#25112;&#65292;&#24182;&#35206;&#30422;&#20102;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12035v1 Announce Type: cross  Abstract: Real-world environments are inherently non-stationary, frequently introducing new classes over time. This is especially common in time series classification, such as the emergence of new disease classification in healthcare or the addition of new activities in human activity recognition. In such cases, a learning system is required to assimilate novel classes effectively while avoiding catastrophic forgetting of the old ones, which gives rise to the Class-incremental Learning (CIL) problem. However, despite the encouraging progress in the image and language domains, CIL for time series data remains relatively understudied. Existing studies suffer from inconsistent experimental designs, necessitating a comprehensive evaluation and benchmarking of methods across a wide range of datasets. To this end, we first present an overview of the Time Series Class-incremental Learning (TSCIL) problem, highlight its unique challenges, and cover the 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#31163;&#31574;&#30053;&#21644;&#22312;&#31574;&#30053;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#39318;&#27425;&#25552;&#20986;&#20102;&#20943;&#23567;&#35813;&#24046;&#36317;&#30340;&#26465;&#20214;&#65292;&#21516;&#26102;&#21457;&#29616;&#22312;&#26465;&#20214;&#19981;&#28385;&#36275;&#26102;&#20250;&#20135;&#29983;&#30701;&#26495;&#12290;</title><link>https://arxiv.org/abs/2402.12034</link><description>&lt;p&gt;
&#31163;&#31574;&#30053;&#21644;&#22312;&#31574;&#30053;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#20309;&#26102;&#33021;&#22815;&#19968;&#33268;&#65311;
&lt;/p&gt;
&lt;p&gt;
When Do Off-Policy and On-Policy Policy Gradient Methods Align?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12034
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#31163;&#31574;&#30053;&#21644;&#22312;&#31574;&#30053;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#39318;&#27425;&#25552;&#20986;&#20102;&#20943;&#23567;&#35813;&#24046;&#36317;&#30340;&#26465;&#20214;&#65292;&#21516;&#26102;&#21457;&#29616;&#22312;&#26465;&#20214;&#19981;&#28385;&#36275;&#26102;&#20250;&#20135;&#29983;&#30701;&#26495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#26159;&#24191;&#27867;&#37319;&#29992;&#30340;&#22312;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#35768;&#22810;&#24212;&#29992;&#39046;&#22495;&#21462;&#24471;&#25104;&#21151;&#65292;&#28982;&#32780;&#30001;&#20110;&#20854;&#33261;&#21517;&#26157;&#33879;&#30340;&#26679;&#26412;&#25928;&#29575;&#20302;&#65292;&#23427;&#20204;&#30340;&#20351;&#29992;&#20173;&#28982;&#23616;&#38480;&#20110;&#21487;&#20197;&#24555;&#36895;&#20934;&#30830;&#27169;&#25311;&#30340;&#38382;&#39064;&#12290;&#25913;&#36827;&#26679;&#26412;&#25928;&#29575;&#30340;&#24120;&#35265;&#26041;&#27861;&#26159;&#20462;&#25913;&#23427;&#20204;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#20351;&#20043;&#33021;&#22815;&#20174;&#31163;&#31574;&#30053;&#26679;&#26412;&#20013;&#35745;&#31639;&#32780;&#26080;&#38656;&#37325;&#35201;&#24615;&#37319;&#26679;&#12290;&#19968;&#20010;&#25104;&#29087;&#30340;&#31163;&#31574;&#30053;&#30446;&#26631;&#23601;&#26159;&#28216;&#33633;&#30446;&#26631;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#28216;&#33633;&#30446;&#26631;&#19982;&#20256;&#32479;&#22312;&#31574;&#30053;&#30446;&#26631;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#22312;&#31163;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#29702;&#35770;&#20998;&#26512;&#65292;&#23637;&#31034;&#20102;&#20943;&#23569;&#22312;&#31163;&#24046;&#36317;&#30340;&#26465;&#20214;&#65292;&#21516;&#26102;&#24314;&#31435;&#20102;&#24403;&#36825;&#20123;&#26465;&#20214;&#26410;&#34987;&#28385;&#36275;&#26102;&#20986;&#29616;&#30340;&#32570;&#38519;&#30340;&#32463;&#39564;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12034v1 Announce Type: cross  Abstract: Policy gradient methods are widely adopted reinforcement learning algorithms for tasks with continuous action spaces. These methods succeeded in many application domains, however, because of their notorious sample inefficiency their use remains limited to problems where fast and accurate simulations are available. A common way to improve sample efficiency is to modify their objective function to be computable from off-policy samples without importance sampling. A well-established off-policy objective is the excursion objective. This work studies the difference between the excursion objective and the traditional on-policy objective, which we refer to as the on-off gap. We provide the first theoretical analysis showing conditions to reduce the on-off gap while establishing empirical evidence of shortfalls arising when these conditions are not met.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22270;&#27169;&#22411;&#30340;&#20248;&#21183;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#23558;LLMs&#30340;&#33021;&#21147;&#21387;&#32553;&#21040; TAG &#23398;&#20064;&#30340;&#26412;&#22320;&#22270;&#27169;&#22411;&#20013;&#65292;&#35299;&#20915;&#23427;&#20204;&#20043;&#38388;&#30340;&#22266;&#26377;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2402.12022</link><description>&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21387;&#32553;&#29992;&#20110;&#25991;&#26412;&#23646;&#24615;&#22270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Distilling Large Language Models for Text-Attributed Graph Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12022
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22270;&#27169;&#22411;&#30340;&#20248;&#21183;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#23558;LLMs&#30340;&#33021;&#21147;&#21387;&#32553;&#21040; TAG &#23398;&#20064;&#30340;&#26412;&#22320;&#22270;&#27169;&#22411;&#20013;&#65292;&#35299;&#20915;&#23427;&#20204;&#20043;&#38388;&#30340;&#22266;&#26377;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#23646;&#24615;&#22270;&#65288;TAGs&#65289;&#26159;&#36830;&#25509;&#30340;&#25991;&#26412;&#25991;&#26723;&#22270;&#12290;&#22270;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#23398;&#20064;TAGs&#65292;&#20294;&#23427;&#20204;&#30340;&#35757;&#32451;&#20005;&#37325;&#20381;&#36182;&#20110;&#20154;&#24037;&#26631;&#27880;&#30340;&#26631;&#31614;&#65292;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#36825;&#20123;&#26631;&#31614;&#24456;&#23569;&#25110;&#29978;&#33267;&#19981;&#21487;&#29992;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26368;&#36817;&#22312;&#23569;&#26679;&#26412;&#21644;&#38646;&#26679;&#26412;TAG&#23398;&#20064;&#20013;&#23637;&#31034;&#20102;&#26174;&#33879;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#23384;&#22312;&#21487;&#20280;&#32553;&#24615;&#12289;&#25104;&#26412;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#36890;&#36807;&#23558;LLMs&#30340;&#33021;&#21147;&#20256;&#25480;&#32473;TAG&#23398;&#20064;&#20013;&#30340;&#26412;&#22320;&#22270;&#27169;&#22411;&#65292;&#20174;&#32780;&#21327;&#21516;LLMs&#21644;&#22270;&#27169;&#22411;&#30340;&#20114;&#34917;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12022v1 Announce Type: new  Abstract: Text-Attributed Graphs (TAGs) are graphs of connected textual documents. Graph models can efficiently learn TAGs, but their training heavily relies on human-annotated labels, which are scarce or even unavailable in many applications. Large language models (LLMs) have recently demonstrated remarkable capabilities in few-shot and zero-shot TAG learning, but they suffer from scalability, cost, and privacy issues. Therefore, in this work, we focus on synergizing LLMs and graph models with their complementary strengths by distilling the power of LLMs to a local graph model on TAG learning. To address the inherent gaps between LLMs (generative models for texts) and graph models (discriminative models for graphs), we propose first to let LLMs teach an interpreter with rich textual rationale and then let a student model mimic the interpreter's reasoning without LLMs' textual rationale. Extensive experiments validate the efficacy of our proposed 
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;Sarsa&#21644;Q-learning&#30340;&#26234;&#33021;&#30446;&#26631;&#36319;&#36394;&#32034;&#24341;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#38647;&#36798;&#35843;&#24230;&#20013;&#30701;&#26399;&#24615;&#33021;&#21644;&#26410;&#26469;&#26426;&#21160;&#24615;&#30340;&#24179;&#34913;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.12015</link><description>&lt;p&gt;
&#22522;&#20110;Sarsa&#21644;Q-learning&#30340;&#24322;&#36136;&#26234;&#33021;&#30446;&#26631;&#36319;&#36394;&#32034;&#24341;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
An Index Policy Based on Sarsa and Q-learning for Heterogeneous Smart Target Tracking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12015
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Sarsa&#21644;Q-learning&#30340;&#26234;&#33021;&#30446;&#26631;&#36319;&#36394;&#32034;&#24341;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#38647;&#36798;&#35843;&#24230;&#20013;&#30701;&#26399;&#24615;&#33021;&#21644;&#26410;&#26469;&#26426;&#21160;&#24615;&#30340;&#24179;&#34913;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35299;&#20915;&#22810;&#20010;&#26234;&#33021;&#30446;&#26631;&#22312;&#20027;&#21160;&#21644;&#34987;&#21160;&#38647;&#36798;&#32593;&#32476;&#20013;&#30340;&#38750;&#36828;&#35265;&#24615;&#38647;&#36798;&#35843;&#24230;&#38382;&#39064;&#26102;&#65292;&#38656;&#35201;&#32771;&#34385;&#30701;&#26399;&#22686;&#24378;&#36319;&#36394;&#24615;&#33021;&#21644;&#26410;&#26469;&#20027;&#21160;&#36319;&#36394;&#20013;&#30446;&#26631;&#26426;&#21160;&#24615;&#30340;&#27010;&#29575;&#12290;&#22312;&#35843;&#24230;&#20027;&#21160;&#21644;&#34987;&#21160;&#38647;&#36798;&#30340;&#27874;&#26463;&#36164;&#28304;&#26102;&#33719;&#24471;&#38271;&#26399;&#36319;&#36394;&#24615;&#33021;&#24102;&#26469;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#38382;&#39064;&#24314;&#27169;&#20026;&#30001;&#24179;&#34892;&#19981;&#24179;&#38745;&#32769;&#34382;&#26426;&#36807;&#31243;&#32452;&#25104;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#12290;&#27599;&#20010;&#32769;&#34382;&#26426;&#36807;&#31243;&#19982;&#19968;&#20010;&#26234;&#33021;&#30446;&#26631;&#30456;&#20851;&#32852;&#65292;&#20854;&#20272;&#35745;&#29366;&#24577;&#26681;&#25454;&#19981;&#21516;&#30340;&#21160;&#20316;&#65288;&#30446;&#26631;&#26159;&#21542;&#34987;&#36319;&#36394;&#65289;&#32780;&#36981;&#24490;&#19981;&#21516;&#30340;&#31163;&#25955;&#21160;&#24577;&#27169;&#22411;&#32780;&#28436;&#21464;&#12290;&#31163;&#25955;&#29366;&#24577;&#30001;&#21160;&#24577;&#27169;&#24335;&#23450;&#20041;&#12290;&#35813;&#38382;&#39064;&#21576;&#29616;&#20986;&#32500;&#24230;&#35781;&#21650;&#65292;&#20854;&#20013;&#26368;&#20248;&#35299;&#36890;&#24120;&#38590;&#20197;&#22788;&#29702;&#12290;&#25105;&#20204;&#36890;&#36807;&#33879;&#21517;&#30340;&#19981;&#24179;&#38745;&#22810;&#33218;&#32769;&#34382;&#26426;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#27714;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12015v1 Announce Type: cross  Abstract: In solving the non-myopic radar scheduling for multiple smart target tracking within an active and passive radar network, we need to consider both short-term enhanced tracking performance and a higher probability of target maneuvering in the future with active tracking. Acquiring the long-term tracking performance while scheduling the beam resources of active and passive radars poses a challenge. To address this challenge, we model this problem as a Markov decision process consisting of parallel restless bandit processes. Each bandit process is associated with a smart target, of which the estimation state evolves according to different discrete dynamic models for different actions - whether or not the target is being tracked. The discrete state is defined by the dynamic mode. The problem exhibits the curse of dimensionality, where optimal solutions are in general intractable. We resort to heuristics through the famous restless multi-ar
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#36827;&#21270;&#30340;&#37319;&#26679;&#26694;&#26550;&#65292;&#26088;&#22312;&#35782;&#21035;&#31934;&#33521;&#35757;&#32451;&#26679;&#26412;&#65292;&#27604;&#36739;&#19982;&#20256;&#32479;&#26041;&#27861;&#30340;&#27169;&#22411;&#24615;&#33021;&#21644;&#33021;&#25928;&#20248;&#21183;&#65292;&#25506;&#35752;&#20854;&#23545;&#21487;&#25345;&#32493;&#27169;&#22411;&#35757;&#32451;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.12010</link><description>&lt;p&gt;
&#20351;&#29992;&#31934;&#33521;&#26679;&#26412;&#35757;&#32451;&#32511;&#33394;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Training Green AI Models Using Elite Samples
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12010
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#36827;&#21270;&#30340;&#37319;&#26679;&#26694;&#26550;&#65292;&#26088;&#22312;&#35782;&#21035;&#31934;&#33521;&#35757;&#32451;&#26679;&#26412;&#65292;&#27604;&#36739;&#19982;&#20256;&#32479;&#26041;&#27861;&#30340;&#27169;&#22411;&#24615;&#33021;&#21644;&#33021;&#25928;&#20248;&#21183;&#65292;&#25506;&#35752;&#20854;&#23545;&#21487;&#25345;&#32493;&#27169;&#22411;&#35757;&#32451;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#27169;&#22411;&#35757;&#32451;&#37327;&#30340;&#22823;&#24133;&#22686;&#21152;&#20855;&#26377;&#37325;&#35201;&#30340;&#29615;&#22659;&#24433;&#21709;&#65292;&#36825;&#38656;&#35201;&#26356;&#33410;&#33021;&#39640;&#25928;&#21644;&#21487;&#25345;&#32493;&#30340;&#20154;&#24037;&#26234;&#33021;&#23454;&#36341;&#12290;&#25968;&#25454;&#20013;&#24515;&#26041;&#27861;&#23637;&#29616;&#20986;&#35757;&#32451;&#33410;&#33021;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#32780;&#23454;&#20363;&#36873;&#25321;&#26041;&#27861;&#23637;&#31034;&#20102;&#20351;&#29992;&#26368;&#23567;&#21270;&#35757;&#32451;&#38598;&#35757;&#32451;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#33021;&#21147;&#19988;&#24615;&#33021;&#19979;&#38477;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#36827;&#21270;&#30340;&#37319;&#26679;&#26694;&#26550;&#65292;&#26088;&#22312;&#35782;&#21035;&#38024;&#23545;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#30340;&#31934;&#33521;&#35757;&#32451;&#26679;&#26412;&#65292;&#27604;&#36739;&#27169;&#22411;&#24615;&#33021;&#21644;&#33410;&#33021;&#25928;&#30410;&#19982;&#20856;&#22411;&#27169;&#22411;&#35757;&#32451;&#23454;&#36341;&#30340;&#24046;&#24322;&#65292;&#24182;&#30740;&#31350;&#36825;&#19968;&#26694;&#26550;&#23545;&#20419;&#36827;&#21487;&#25345;&#32493;&#27169;&#22411;&#35757;&#32451;&#23454;&#36341;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12010v1 Announce Type: cross  Abstract: The substantial increase in AI model training has considerable environmental implications, mandating more energy-efficient and sustainable AI practices. On the one hand, data-centric approaches show great potential towards training energy-efficient AI models. On the other hand, instance selection methods demonstrate the capability of training AI models with minimised training sets and negligible performance degradation. Despite the growing interest in both topics, the impact of data-centric training set selection on energy efficiency remains to date unexplored. This paper presents an evolutionary-based sampling framework aimed at (i) identifying elite training samples tailored for datasets and model pairs, (ii) comparing model performance and energy efficiency gains against typical model training practice, and (iii) investigating the feasibility of this framework for fostering sustainable model training practices. To evaluate the propo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#32676;&#38598;&#24615;&#33021;&#23545;&#28155;&#21152;&#21040;&#22522;&#32447;&#25968;&#25454;&#38598;&#20013;&#30340;&#22024;&#26434;&#19981;&#30456;&#20851;&#21464;&#37327;&#30340;&#25935;&#24863;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.12008</link><description>&lt;p&gt;
&#32676;&#38598;&#24230;&#37327;&#23545;&#26080;&#20851;&#29305;&#24449;&#30340;&#25935;&#24863;&#24230;
&lt;/p&gt;
&lt;p&gt;
Cluster Metric Sensitivity to Irrelevant Features
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12008
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#32676;&#38598;&#24615;&#33021;&#23545;&#28155;&#21152;&#21040;&#22522;&#32447;&#25968;&#25454;&#38598;&#20013;&#30340;&#22024;&#26434;&#19981;&#30456;&#20851;&#21464;&#37327;&#30340;&#25935;&#24863;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32858;&#31867;&#31639;&#27861;&#22312;&#25968;&#25454;&#20998;&#26512;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#29992;&#20110;&#25968;&#25454;&#25506;&#32034;&#21644;&#21457;&#29616;&#12290;&#25216;&#26415;&#36827;&#27493;&#23548;&#33268;&#25968;&#25454;&#22312;&#23481;&#37327;&#12289;&#32500;&#24230;&#21644;&#22797;&#26434;&#24615;&#26041;&#38754;&#19981;&#26029;&#22686;&#38271;&#12290;&#36825;&#20026;&#25968;&#25454;&#20998;&#26512;&#25552;&#20379;&#20102;&#24040;&#22823;&#26426;&#20250;&#65292;&#22240;&#20026;&#25968;&#25454;&#21487;&#20197;&#29992;&#20110;&#35768;&#22810;&#19981;&#21516;&#30446;&#30340;&#30340;&#35810;&#38382;&#12290;&#28982;&#32780;&#65292;&#36825;&#20063;&#24102;&#26469;&#20102;&#25361;&#25112;&#65292;&#27604;&#22914;&#22312;&#32473;&#23450;&#20219;&#21153;&#20013;&#35782;&#21035;&#30456;&#20851;&#29305;&#24449;&#12290;&#22312;&#30417;&#30563;&#20219;&#21153;&#20013;&#65292;&#21487;&#20197;&#21033;&#29992;&#21508;&#31181;&#26041;&#27861;&#20248;&#21270;&#20219;&#21153;&#30446;&#26631;&#65288;&#20363;&#22914;&#20998;&#31867;&#20934;&#30830;&#24615;&#65289;&#30340;&#36755;&#20837;&#29305;&#24449;&#12290;&#22312;&#26080;&#30417;&#30563;&#38382;&#39064;&#20013;&#65292;&#36825;&#20123;&#24037;&#20855;&#24182;&#19981;readily available&#65292;&#37096;&#20998;&#21407;&#22240;&#26159;&#26080;&#27861;&#23450;&#37327;&#22320;&#34913;&#37327;&#26080;&#26631;&#31614;&#20219;&#21153;&#20013;&#29305;&#24449;&#30340;&#30456;&#20851;&#24615;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#32676;&#38598;&#24615;&#33021;&#23545;&#22024;&#26434;&#30340;&#19981;&#30456;&#20851;&#21464;&#37327;&#36827;&#34892;&#36845;&#20195;&#28155;&#21152;&#21040;&#20855;&#26377;&#26126;&#30830;&#23450;&#20041;&#32676;&#38598;&#30340;&#22522;&#32447;&#25968;&#25454;&#38598;&#30340;&#25935;&#24863;&#24230;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19981;&#21516;&#31867;&#22411;&#30340;&#26080;&#20851;&#21464;&#37327;&#22914;&#20309;&#24433;&#21709;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12008v1 Announce Type: cross  Abstract: Clustering algorithms are used extensively in data analysis for data exploration and discovery. Technological advancements lead to continually growth of data in terms of volume, dimensionality and complexity. This provides great opportunities in data analytics as the data can be interrogated for many different purposes. This however leads challenges, such as identification of relevant features for a given task. In supervised tasks, one can utilise a number of methods to optimise the input features for the task objective (e.g. classification accuracy). In unsupervised problems, such tools are not readily available, in part due to an inability to quantify feature relevance in unlabeled tasks. In this paper, we investigate the sensitivity of clustering performance noisy uncorrelated variables iteratively added to baseline datasets with well defined clusters. We show how different types of irrelevant variables can impact the outcome of a c
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#26102;&#38388;&#20449;&#24687;&#21644;&#25512;&#29702;&#26041;&#38754;&#23384;&#22312;&#26174;&#33879;&#38480;&#21046;&#65292;&#38381;&#28304;&#27169;&#22411;&#21487;&#33021;&#26263;&#31034;&#20102;&#19981;&#30830;&#23450;&#24615;&#35748;&#35782;&#19982;&#38169;&#35823;&#22238;&#24212;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;</title><link>https://arxiv.org/abs/2402.11997</link><description>&lt;p&gt;
&#22238;&#24518;&#37027;&#19968;&#24180;&#21457;&#29983;&#30340;&#20107;&#20214;&#65311;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26102;&#38388;&#20449;&#24687;&#21644;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Remember This Event That Year? Assessing Temporal Information and Reasoning in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11997
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#26102;&#38388;&#20449;&#24687;&#21644;&#25512;&#29702;&#26041;&#38754;&#23384;&#22312;&#26174;&#33879;&#38480;&#21046;&#65292;&#38381;&#28304;&#27169;&#22411;&#21487;&#33021;&#26263;&#31034;&#20102;&#19981;&#30830;&#23450;&#24615;&#35748;&#35782;&#19982;&#38169;&#35823;&#22238;&#24212;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#20294;&#23427;&#20204;&#23545;&#20110;&#25512;&#29702;&#21644;&#20445;&#30041;&#26102;&#38388;&#20449;&#24687;&#30340;&#33021;&#21147;&#20173;&#28982;&#26377;&#38480;&#12290;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#29702;&#35299;&#20107;&#20214;&#30340;&#39034;&#24207;&#24615;&#23545;&#20851;&#38190;&#30340;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#22312;&#19968;&#20010;&#26032;&#39062;&#30340;&#22823;&#35268;&#27169;&#26102;&#38388;&#25968;&#25454;&#38598;\textbf{TempUN}&#19978;&#23545;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#65292;&#25581;&#31034;&#20102;&#26102;&#38388;&#20445;&#30041;&#21644;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#30340;&#26174;&#33879;&#38480;&#21046;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#38381;&#28304;&#27169;&#22411;&#26356;&#39057;&#32321;&#22320;&#26174;&#31034;&#20986;&#30693;&#35782;&#24046;&#36317;&#65292;&#21487;&#33021;&#26263;&#31034;&#20102;&#19981;&#30830;&#23450;&#24615;&#35748;&#35782;&#21644;&#38169;&#35823;&#22238;&#24212;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#27492;&#22806;&#65292;&#25506;&#32034;&#21508;&#31181;&#24494;&#35843;&#26041;&#27861;&#24182;&#27809;&#26377;&#24102;&#26469;&#20027;&#35201;&#24615;&#33021;&#25913;&#36827;&#12290;&#30456;&#20851;&#25968;&#25454;&#38598;&#21644;&#20195;&#30721;&#21487;&#22312;&#20197;&#19979;&#32593;&#22336;&#33719;&#24471;&#65288;https://github.com/lingoiitgn/TempUN&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11997v1 Announce Type: cross  Abstract: Large Language Models (LLMs) are increasingly becoming ubiquitous, yet their ability to reason about and retain temporal information remains limited. This hinders their application in real-world scenarios where understanding the sequential nature of events is crucial. This paper experiments with state-of-the-art models on a novel, large-scale temporal dataset, \textbf{TempUN}, to reveal significant limitations in temporal retention and reasoning abilities. Interestingly, closed-source models indicate knowledge gaps more frequently, potentially suggesting a trade-off between uncertainty awareness and incorrect responses. Further, exploring various fine-tuning approaches yielded no major performance improvements. The associated dataset and code are available at the following URL (https://github.com/lingoiitgn/TempUN).
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25991;&#26412;&#25552;&#31034;&#30340;DLO&#23454;&#20363;&#20998;&#21106;&#25216;&#26415;&#65292;&#32467;&#21512;&#20102;CLIPSeg&#27169;&#22411;&#30340;&#25991;&#26412;&#26465;&#20214;&#35821;&#20041;&#20998;&#21106;&#33021;&#21147;&#21644;Segment Anything Model&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#22312;&#24863;&#30693;&#21487;&#21464;&#24418;&#30452;&#32447;&#23545;&#35937;&#22914;&#30005;&#32447;&#12289;&#30005;&#32518;&#21644;&#26580;&#24615;&#31649;&#36947;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#24615;&#33021;&#36229;&#36234;&#20102;&#30446;&#21069;&#30340;&#25216;&#26415;&#27700;&#24179;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;DLO&#29305;&#23450;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2402.11996</link><description>&lt;p&gt;
ISCUTE&#65306;&#20351;&#29992;&#25991;&#26412;&#23884;&#20837;&#36827;&#34892;&#30005;&#32518;&#23454;&#20363;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
ISCUTE: Instance Segmentation of Cables Using Text Embedding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11996
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25991;&#26412;&#25552;&#31034;&#30340;DLO&#23454;&#20363;&#20998;&#21106;&#25216;&#26415;&#65292;&#32467;&#21512;&#20102;CLIPSeg&#27169;&#22411;&#30340;&#25991;&#26412;&#26465;&#20214;&#35821;&#20041;&#20998;&#21106;&#33021;&#21147;&#21644;Segment Anything Model&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#22312;&#24863;&#30693;&#21487;&#21464;&#24418;&#30452;&#32447;&#23545;&#35937;&#22914;&#30005;&#32447;&#12289;&#30005;&#32518;&#21644;&#26580;&#24615;&#31649;&#36947;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#24615;&#33021;&#36229;&#36234;&#20102;&#30446;&#21069;&#30340;&#25216;&#26415;&#27700;&#24179;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;DLO&#29305;&#23450;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#20154;&#25216;&#26415;&#21644;&#33258;&#21160;&#21270;&#39046;&#22495;&#65292;&#20256;&#32479;&#30340;&#23545;&#35937;&#35782;&#21035;&#21644;&#23454;&#20363;&#20998;&#21106;&#26041;&#27861;&#22312;&#24863;&#30693;&#31867;&#20284;&#30005;&#32447;&#12289;&#30005;&#32518;&#21644;&#26580;&#24615;&#31649;&#36947;&#31561;&#21487;&#21464;&#24418;&#30452;&#32447;&#23545;&#35937;&#65288;DLOs&#65289;&#26041;&#38754;&#38754;&#20020;&#30528;&#24040;&#22823;&#25361;&#25112;&#12290;&#36825;&#19968;&#25361;&#25112;&#20027;&#35201;&#28304;&#20110;&#32570;&#20047;&#24418;&#29366;&#12289;&#39068;&#33394;&#21644;&#32441;&#29702;&#31561;&#26126;&#26174;&#23646;&#24615;&#65292;&#36825;&#38656;&#35201;&#37327;&#36523;&#23450;&#21046;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#23454;&#29616;&#31934;&#30830;&#35782;&#21035;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22522;&#20110;&#25991;&#26412;&#25552;&#31034;&#30340;&#12289;&#29992;&#25143;&#21451;&#22909;&#30340;DLO&#23454;&#20363;&#20998;&#21106;&#25216;&#26415;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;CLIPSeg&#27169;&#22411;&#30340;&#25991;&#26412;&#26465;&#20214;&#35821;&#20041;&#20998;&#21106;&#33021;&#21147;&#21644;Segment Anything Model (SAM) &#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;DLO&#23454;&#20363;&#20998;&#21106;&#26041;&#38754;&#36229;&#36234;&#20102;&#26368;&#20808;&#36827;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;$91.21\%$&#30340;&#24179;&#22343;&#20132;&#24182;&#27604;&#65288;mIoU&#65289;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#20016;&#23500;&#22810;&#26679;&#30340;&#29992;&#20110;&#23454;&#20363;&#20998;&#21106;&#30340;DLO&#29305;&#23450;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11996v1 Announce Type: cross  Abstract: In the field of robotics and automation, conventional object recognition and instance segmentation methods face a formidable challenge when it comes to perceiving Deformable Linear Objects (DLOs) like wires, cables, and flexible tubes. This challenge arises primarily from the lack of distinct attributes such as shape, color, and texture, which calls for tailored solutions to achieve precise identification. In this work, we propose a foundation model-based DLO instance segmentation technique that is text-promptable and user-friendly. Specifically, our approach combines the text-conditioned semantic segmentation capabilities of CLIPSeg model with the zero-shot generalization capabilities of Segment Anything Model (SAM). We show that our method exceeds SOTA performance on DLO instance segmentation, achieving a mIoU of $91.21\%$. We also introduce a rich and diverse DLO-specific dataset for instance segmentation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#35757;&#32451;&#21518;&#30340;&#20108;&#20540;&#21270;&#31070;&#32463;&#32593;&#32476;&#32534;&#30721;&#65292;&#23454;&#29616;&#20102;&#23545;&#20854;&#36827;&#34892;&#21453;&#28418;&#30340;&#30446;&#30340;</title><link>https://arxiv.org/abs/2402.11995</link><description>&lt;p&gt;
&#20108;&#20540;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#32593;&#32476;&#21453;&#28418;
&lt;/p&gt;
&lt;p&gt;
Network Inversion of Binarised Neural Nets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11995
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#35757;&#32451;&#21518;&#30340;&#20108;&#20540;&#21270;&#31070;&#32463;&#32593;&#32476;&#32534;&#30721;&#65292;&#23454;&#29616;&#20102;&#23545;&#20854;&#36827;&#34892;&#21453;&#28418;&#30340;&#30446;&#30340;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#31070;&#32463;&#32593;&#32476;&#30340;&#37096;&#32626;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#20135;&#29983;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65292;&#20294;&#23427;&#20204;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#29702;&#35299;&#20381;&#28982;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#32593;&#32476;&#21453;&#28418;&#26159;&#19968;&#31181;&#26088;&#22312;&#20174;&#27169;&#22411;&#23398;&#20064;&#30340;&#20869;&#37096;&#34920;&#31034;&#20013;&#37325;&#24314;&#36755;&#20837;&#31354;&#38388;&#30340;&#25216;&#26415;&#65292;&#22312;&#25581;&#31034;&#31070;&#32463;&#32593;&#32476;&#20013;&#36755;&#20837;&#21040;&#36755;&#20986;&#26144;&#23556;&#30340;&#40657;&#21283;&#23376;&#29305;&#24615;&#26041;&#38754;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#22312;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;&#20013;&#65292;&#27169;&#22411;&#36755;&#20986;&#21487;&#33021;&#24433;&#21709;&#37325;&#35201;&#20915;&#31574;&#65292;&#30456;&#24212;&#36755;&#20837;&#31354;&#38388;&#30340;&#23436;&#25972;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#24517;&#39035;&#28040;&#38500;&#20219;&#20309;&#22810;&#20313;&#30340;&#8220;&#22403;&#22334;&#8221;&#20197;&#30830;&#20445;&#32593;&#32476;&#30340;&#21487;&#20449;&#24230;&#12290;&#20108;&#20540;&#31070;&#32463;&#32593;&#32476;&#65288;BNNs&#65289;&#20197;&#20108;&#20540;&#26435;&#37325;&#21644;&#28608;&#27963;&#20026;&#29305;&#24449;&#65292;&#25552;&#20379;&#20102;&#35745;&#31639;&#25928;&#29575;&#21644;&#20943;&#23569;&#20869;&#23384;&#38656;&#27714;&#65292;&#20351;&#23427;&#20204;&#36866;&#29992;&#20110;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#23558;&#35757;&#32451;&#21518;&#30340;BNN&#32534;&#30721;&#26469;&#21453;&#28418;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11995v1 Announce Type: new  Abstract: While the deployment of neural networks, yielding impressive results, becomes more prevalent in various applications, their interpretability and understanding remain a critical challenge. Network inversion, a technique that aims to reconstruct the input space from the model's learned internal representations, plays a pivotal role in unraveling the black-box nature of input to output mappings in neural networks. In safety-critical scenarios, where model outputs may influence pivotal decisions, the integrity of the corresponding input space is paramount, necessitating the elimination of any extraneous "garbage" to ensure the trustworthiness of the network. Binarised Neural Networks (BNNs), characterized by binary weights and activations, offer computational efficiency and reduced memory requirements, making them suitable for resource-constrained environments. This paper introduces a novel approach to invert a trained BNN by encoding it int
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#38544;&#31169;&#20445;&#25252;&#30340;&#20302;&#31209;&#36866;&#24212;&#35299;&#20915;&#26041;&#26696;PrivateLoRA&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#36866;&#24212;&#25439;&#22833;&#21644;&#20195;&#29702;&#25915;&#20987;&#27169;&#22411;&#30340;MI&#22686;&#30410;&#26469;&#25269;&#24481;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2402.11989</link><description>&lt;p&gt;
&#38544;&#31169;&#20445;&#25252;&#30340;&#20302;&#31209;&#36866;&#24212;Latent&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Privacy-Preserving Low-Rank Adaptation for Latent Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11989
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#38544;&#31169;&#20445;&#25252;&#30340;&#20302;&#31209;&#36866;&#24212;&#35299;&#20915;&#26041;&#26696;PrivateLoRA&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#36866;&#24212;&#25439;&#22833;&#21644;&#20195;&#29702;&#25915;&#20987;&#27169;&#22411;&#30340;MI&#22686;&#30410;&#26469;&#25269;&#24481;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#31574;&#30053;&#65292;&#29992;&#20110;&#36890;&#36807;&#26368;&#23567;&#21270;&#36866;&#24212;&#25439;&#22833;&#65292;&#33258;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#36866;&#24212;Latent&#25193;&#25955;&#27169;&#22411;&#65288;LDM&#65289;&#20197;&#29983;&#25104;&#29305;&#23450;&#23545;&#35937;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;LoRA&#36866;&#24212;&#30340;LDM&#23481;&#26131;&#21463;&#21040;&#25104;&#21592;&#25512;&#26029;&#65288;MI&#65289;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#36825;&#31181;&#25915;&#20987;&#21487;&#20197;&#21028;&#26029;&#29305;&#23450;&#25968;&#25454;&#28857;&#26159;&#21542;&#23646;&#20110;&#31169;&#20154;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#22240;&#27492;&#38754;&#20020;&#20005;&#37325;&#30340;&#38544;&#31169;&#27844;&#38706;&#39118;&#38505;&#12290;&#20026;&#20102;&#25269;&#24481;MI&#25915;&#20987;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#20010;&#30452;&#25509;&#30340;&#35299;&#20915;&#26041;&#26696;&#65306;&#38544;&#31169;&#20445;&#25252;&#30340;LoRA&#65288;PrivateLoRA&#65289;&#12290;PrivateLoRA&#34987;&#26500;&#24314;&#20026;&#19968;&#20010;&#26368;&#23567;&#26368;&#22823;&#20248;&#21270;&#38382;&#39064;&#65292;&#20854;&#20013;&#36890;&#36807;&#26368;&#22823;&#21270;MI&#22686;&#30410;&#26469;&#35757;&#32451;&#20195;&#29702;&#25915;&#20987;&#27169;&#22411;&#65292;&#32780;LDM&#21017;&#36890;&#36807;&#26368;&#23567;&#21270;&#36866;&#24212;&#25439;&#22833;&#21644;&#20195;&#29702;&#25915;&#20987;&#27169;&#22411;&#30340;MI&#22686;&#30410;&#20043;&#21644;&#26469;&#36827;&#34892;&#35843;&#25972;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#22312;&#23454;&#36341;&#20013;&#21457;&#29616;PrivateLoRA&#23384;&#22312;&#31283;&#23450;&#24615;&#20248;&#21270;&#38382;&#39064;&#65292;&#21363;&#30001;&#20110;&#26799;&#24230;&#35268;&#27169;&#30340;&#22823;&#24133;&#27874;&#21160;&#32780;&#22952;&#30861;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11989v1 Announce Type: new  Abstract: Low-rank adaptation (LoRA) is an efficient strategy for adapting latent diffusion models (LDMs) on a training dataset to generate specific objects by minimizing the adaptation loss. However, adapted LDMs via LoRA are vulnerable to membership inference (MI) attacks that can judge whether a particular data point belongs to private training datasets, thus facing severe risks of privacy leakage. To defend against MI attacks, we make the first effort to propose a straightforward solution: privacy-preserving LoRA (PrivateLoRA). PrivateLoRA is formulated as a min-max optimization problem where a proxy attack model is trained by maximizing its MI gain while the LDM is adapted by minimizing the sum of the adaptation loss and the proxy attack model's MI gain. However, we empirically disclose that PrivateLoRA has the issue of unstable optimization due to the large fluctuation of the gradient scale which impedes adaptation. To mitigate this issue, w
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24369;&#30417;&#30563;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#21487;&#24494;&#30340;ROI&#25552;&#35758;&#32593;&#32476;&#21644;&#36719;ROI&#27744;&#21270;&#65292;&#22312;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#20013;&#32988;&#36807;&#29616;&#26377;&#26041;&#27861;</title><link>https://arxiv.org/abs/2402.11985</link><description>&lt;p&gt;
&#20855;&#26377;&#21487;&#24494;&#21487;&#24863;&#30693;&#21306;&#22495;&#20852;&#36259;&#25552;&#35758;&#32593;&#32476;&#21644;&#36719;&#21306;&#22495;&#24863;&#30693;&#30340;&#33016;&#37096;X&#23556;&#32447;&#24369;&#30417;&#30563;&#30446;&#26631;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Weakly Supervised Object Detection in Chest X-Rays with Differentiable ROI Proposal Networks and Soft ROI Pooling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11985
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24369;&#30417;&#30563;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#21487;&#24494;&#30340;ROI&#25552;&#35758;&#32593;&#32476;&#21644;&#36719;ROI&#27744;&#21270;&#65292;&#22312;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#20013;&#32988;&#36807;&#29616;&#26377;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24369;&#30417;&#30563;&#30446;&#26631;&#26816;&#27979;&#65288;WSup-OD&#65289;&#25552;&#39640;&#20102;&#22270;&#20687;&#20998;&#31867;&#31639;&#27861;&#30340;&#26377;&#29992;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#26080;&#38656;&#39069;&#22806;&#30417;&#30563;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#19968;&#20219;&#21153;&#19978;&#22810;&#23454;&#20363;&#23398;&#20064;&#22312;&#33258;&#28982;&#22270;&#20687;&#20013;&#21462;&#24471;&#30340;&#25104;&#21151;&#24182;&#19981;&#33021;&#24456;&#22909;&#22320;&#36716;&#21270;&#20026;&#21307;&#23398;&#22270;&#20687;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#23545;&#35937;&#65288;&#21363;&#30149;&#29702;&#65289;&#20855;&#26377;&#38750;&#24120;&#19981;&#21516;&#30340;&#29305;&#24449;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#24369;&#30417;&#30563;&#21306;&#22495;&#20852;&#36259;&#25552;&#35758;&#32593;&#32476;&#65288;WSRPN&#65289;&#65292;&#19968;&#31181;&#20351;&#29992;&#19987;&#38376;&#30340;&#21306;&#22495;&#24863;&#30693;&#65288;ROI-attention&#65289;&#27169;&#22359;&#21160;&#24577;&#29983;&#25104;&#36793;&#30028;&#26694;&#25552;&#35758;&#30340;&#26032;&#26041;&#27861;&#12290;WSRPN&#19982;&#32463;&#20856;&#30340;&#39592;&#24178;-&#22836;&#37096;&#20998;&#31867;&#31639;&#27861;&#24456;&#22909;&#22320;&#38598;&#25104;&#22312;&#19968;&#36215;&#65292;&#21482;&#38656;&#22270;&#20687;&#26631;&#31614;&#30417;&#30563;&#21363;&#21487;&#36827;&#34892;&#31471;&#21040;&#31471;&#35757;&#32451;&#12290;&#25105;&#20204;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26032;&#26041;&#27861;&#22312;&#33016;&#37096;X&#20809;&#22270;&#20687;&#20013;&#30340;&#30142;&#30149;&#23450;&#20301;&#36825;&#19968;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#20013;&#32988;&#36807;&#20102;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11985v1 Announce Type: cross  Abstract: Weakly supervised object detection (WSup-OD) increases the usefulness and interpretability of image classification algorithms without requiring additional supervision. The successes of multiple instance learning in this task for natural images, however, do not translate well to medical images due to the very different characteristics of their objects (i.e. pathologies). In this work, we propose Weakly Supervised ROI Proposal Networks (WSRPN), a new method for generating bounding box proposals on the fly using a specialized region of interest-attention (ROI-attention) module. WSRPN integrates well with classic backbone-head classification algorithms and is end-to-end trainable with only image-label supervision. We experimentally demonstrate that our new method outperforms existing methods in the challenging task of disease localization in chest X-ray images. Code: https://github.com/philip-mueller/wsrpn
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27178;&#21521;&#36830;&#25509;&#21644;Hebbian&#23398;&#20064;&#30340;&#31070;&#32463;&#25805;&#20316;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#36890;&#36807;&#25237;&#24433;&#20445;&#25252;&#30693;&#35782;</title><link>https://arxiv.org/abs/2402.11984</link><description>&lt;p&gt;
&#22522;&#20110;Hebbian&#23398;&#20064;&#30340;&#27491;&#20132;&#25237;&#24433;&#29992;&#20110;&#25345;&#32493;&#23398;&#20064;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Hebbian Learning based Orthogonal Projection for Continual Learning of Spiking Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11984
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27178;&#21521;&#36830;&#25509;&#21644;Hebbian&#23398;&#20064;&#30340;&#31070;&#32463;&#25805;&#20316;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#36890;&#36807;&#25237;&#24433;&#20445;&#25252;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#65292;&#23545;&#20110;&#33021;&#25928;&#39640;&#30340;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20855;&#26377;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#19982;&#32456;&#36523;&#19981;&#26029;&#23398;&#20064;&#19981;&#21516;&#65292;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#23481;&#26131;&#20986;&#29616;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#31070;&#32463;&#25805;&#20316;&#22914;&#20309;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#26159;&#20154;&#24037;&#26234;&#33021;&#21644;&#31070;&#32463;&#31185;&#23398;&#20013;&#30340;&#37325;&#35201;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#27178;&#21521;&#36830;&#25509;&#21644;Hebbian&#23398;&#20064;&#30340;&#31070;&#32463;&#25805;&#20316;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#25237;&#24433;&#20445;&#25252;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11984v1 Announce Type: cross  Abstract: Neuromorphic computing with spiking neural networks is promising for energy-efficient artificial intelligence (AI) applications. However, different from humans who continually learn different tasks in a lifetime, neural network models suffer from catastrophic forgetting. How could neuronal operations solve this problem is an important question for AI and neuroscience. Many previous studies draw inspiration from observed neuroscience phenomena and propose episodic replay or synaptic metaplasticity, but they are not guaranteed to explicitly preserve knowledge for neuron populations. Other works focus on machine learning methods with more mathematical grounding, e.g., orthogonal projection on high dimensional spaces, but there is no neural correspondence for neuromorphic computing. In this work, we develop a new method with neuronal operations based on lateral connections and Hebbian learning, which can protect knowledge by projecting act
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#34987;&#23457;&#26597;&#22238;&#24402;&#20013;&#30340;&#36125;&#21494;&#26031;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;($\mathcal{C}$-BALD)&#65292;&#36890;&#36807;&#25512;&#23548;&#34987;&#23457;&#26597;&#20998;&#24067;&#30340;&#29109;&#21644;&#20114;&#20449;&#24687;&#65292;&#20248;&#21270;&#30446;&#26631;&#20989;&#25968;&#65292;&#22312;&#24191;&#27867;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#19979;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>https://arxiv.org/abs/2402.11973</link><description>&lt;p&gt;
&#34987;&#23457;&#26597;&#22238;&#24402;&#30340;&#36125;&#21494;&#26031;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Bayesian Active Learning for Censored Regression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11973
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#34987;&#23457;&#26597;&#22238;&#24402;&#20013;&#30340;&#36125;&#21494;&#26031;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;($\mathcal{C}$-BALD)&#65292;&#36890;&#36807;&#25512;&#23548;&#34987;&#23457;&#26597;&#20998;&#24067;&#30340;&#29109;&#21644;&#20114;&#20449;&#24687;&#65292;&#20248;&#21270;&#30446;&#26631;&#20989;&#25968;&#65292;&#22312;&#24191;&#27867;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#19979;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20027;&#21160;&#23398;&#20064;&#22522;&#20110;&#20449;&#24687;&#35770;&#26041;&#27861;&#65292;&#19987;&#27880;&#20110;&#26368;&#22823;&#21270;&#26032;&#35266;&#27979;&#25552;&#20379;&#32473;&#27169;&#22411;&#21442;&#25968;&#30340;&#20449;&#24687;&#12290;&#36890;&#24120;&#36890;&#36807;&#26368;&#22823;&#21270;&#36125;&#21494;&#26031;&#20027;&#21160;&#23398;&#20064;&#36890;&#36807;&#20998;&#27495;&#65288;BALD&#65289;&#33719;&#24471;&#20989;&#25968;&#26469;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#24378;&#35843;&#65292;&#22312;&#26032;&#25968;&#25454;&#28857;&#36973;&#21463;&#23457;&#26597;&#26102;&#20272;&#35745;BALD&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#20854;&#20013;&#21482;&#35266;&#23519;&#21040;&#30446;&#26631;&#30340;&#21098;&#36753;&#20540;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#34987;&#23457;&#26597;&#20998;&#24067;&#30340;&#29109;&#21644;&#20114;&#20449;&#24687;&#65292;&#24182;&#25512;&#23548;&#20102;&#34987;&#23457;&#26597;&#22238;&#24402;&#20013;&#30340;&#20027;&#21160;&#23398;&#20064;&#30340;BALD&#30446;&#26631;&#65288;$\mathcal{C}$-BALD&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24314;&#27169;&#26041;&#27861;&#26469;&#20272;&#35745;$\mathcal{C}$-BALD&#30446;&#26631;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#34987;&#23457;&#26597;&#35774;&#32622;&#20013;&#30340;&#20027;&#21160;&#23398;&#20064;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#24191;&#27867;&#30340;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#65292;&#25105;&#20204;&#35777;&#26126;$\mathcal{C}$-BALD&#22312;&#34987;&#23457;&#26597;&#22238;&#24402;&#20013;&#20248;&#20110;&#20854;&#20182;&#36125;&#21494;&#26031;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11973v1 Announce Type: new  Abstract: Bayesian active learning is based on information theoretical approaches that focus on maximising the information that new observations provide to the model parameters. This is commonly done by maximising the Bayesian Active Learning by Disagreement (BALD) acquisitions function. However, we highlight that it is challenging to estimate BALD when the new data points are subject to censorship, where only clipped values of the targets are observed. To address this, we derive the entropy and the mutual information for censored distributions and derive the BALD objective for active learning in censored regression ($\mathcal{C}$-BALD). We propose a novel modelling approach to estimate the $\mathcal{C}$-BALD objective and use it for active learning in the censored setting. Across a wide range of datasets and models, we demonstrate that $\mathcal{C}$-BALD outperforms other Bayesian active learning methods in censored regression.
&lt;/p&gt;</description></item><item><title>&#22238;&#24402;&#25968;&#25454;&#38598;&#20013;&#30340;&#19981;&#24179;&#34913;&#38382;&#39064;&#19968;&#30452;&#34987;&#24573;&#35270;&#65292;&#26412;&#25991;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#23450;&#20041;&#65292;&#23637;&#31034;&#20102;&#36825;&#19968;&#38382;&#39064;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#20849;&#21516;&#22522;&#30784;&#12290;</title><link>https://arxiv.org/abs/2402.11963</link><description>&lt;p&gt;
&#22238;&#24402;&#25968;&#25454;&#38598;&#20013;&#30340;&#19981;&#24179;&#34913;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Imbalance in Regression Datasets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11963
&lt;/p&gt;
&lt;p&gt;
&#22238;&#24402;&#25968;&#25454;&#38598;&#20013;&#30340;&#19981;&#24179;&#34913;&#38382;&#39064;&#19968;&#30452;&#34987;&#24573;&#35270;&#65292;&#26412;&#25991;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#23450;&#20041;&#65292;&#23637;&#31034;&#20102;&#36825;&#19968;&#38382;&#39064;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#20849;&#21516;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23601;&#20998;&#31867;&#32780;&#35328;&#65292;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#26159;&#20247;&#25152;&#21608;&#30693;&#30340;&#65292;&#24182;&#19988;&#24050;&#32463;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;&#22312;&#22238;&#24402;&#20013;&#23384;&#22312;&#30340;&#19981;&#24179;&#34913;&#38382;&#39064;&#21516;&#26679;&#37325;&#35201;&#65292;&#20294;&#36804;&#20170;&#20026;&#27490;&#34987;&#24573;&#35270;&#65306;&#30001;&#20110;&#25968;&#25454;&#38598;&#30446;&#26631;&#20998;&#24067;&#20013;&#30340;&#27424;&#34920;&#31034;&#21644;&#36807;&#22810;&#34920;&#31034;&#65292;&#22238;&#24402;&#22120;&#23481;&#26131;&#36864;&#21270;&#20026;&#26420;&#32032;&#27169;&#22411;&#65292;&#31995;&#32479;&#22320;&#24573;&#30053;&#19981;&#24120;&#35265;&#30340;&#35757;&#32451;&#25968;&#25454;&#24182;&#22312;&#35757;&#32451;&#26399;&#38388;&#32463;&#24120;&#35265;&#21040;&#30340;&#30446;&#26631;&#19978;&#36827;&#34892;&#36807;&#24230;&#34920;&#31034;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#24471;&#20986;&#30340;&#35265;&#35299;&#21046;&#23450;&#20102;&#23545;&#22238;&#24402;&#20013;&#19981;&#24179;&#34913;&#30340;&#39318;&#20010;&#23450;&#20041;&#65292;&#25105;&#20204;&#23637;&#31034;&#36825;&#26159;&#20998;&#31867;&#20013;&#24120;&#29992;&#30340;&#19981;&#24179;&#34913;&#24230;&#37327;&#30340;&#27867;&#21270;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#24076;&#26395;&#23558;&#20851;&#27880;&#28857;&#36716;&#21521;&#22238;&#24402;&#20013;&#34987;&#24573;&#35270;&#30340;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#20026;&#26410;&#26469;&#30740;&#31350;&#22880;&#23450;&#20849;&#21516;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11963v1 Announce Type: cross  Abstract: For classification, the problem of class imbalance is well known and has been extensively studied. In this paper, we argue that imbalance in regression is an equally important problem which has so far been overlooked: Due to under- and over-representations in a data set's target distribution, regressors are prone to degenerate to naive models, systematically neglecting uncommon training data and over-representing targets seen often during training. We analyse this problem theoretically and use resulting insights to develop a first definition of imbalance in regression, which we show to be a generalisation of the commonly employed imbalance measure in classification. With this, we hope to turn the spotlight on the overlooked problem of imbalance in regression and to provide common ground for future research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DB-LLM&#30340;&#26032;&#39062;&#21452;&#20108;&#20540;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#28789;&#27963;&#21452;&#20108;&#20540;&#21270;(FDB)&#26469;&#24179;&#34913;2&#20301;&#23485;&#24230;&#30340;&#31934;&#24230;&#20248;&#21183;&#21644;&#20108;&#20540;&#21270;&#30340;&#25928;&#29575;&#20248;&#21183;&#65292;&#20174;&#32780;&#22312;&#25552;&#39640;LLMs&#30340;&#35745;&#31639;&#25928;&#29575;&#30340;&#21516;&#26102;&#20445;&#25345;&#20102;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.11960</link><description>&lt;p&gt;
DB-LLM: &#39640;&#25928;LLM&#30340;&#20934;&#30830;&#21452;&#20108;&#20540;&#21270;
&lt;/p&gt;
&lt;p&gt;
DB-LLM: Accurate Dual-Binarization for Efficient LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11960
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DB-LLM&#30340;&#26032;&#39062;&#21452;&#20108;&#20540;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#28789;&#27963;&#21452;&#20108;&#20540;&#21270;(FDB)&#26469;&#24179;&#34913;2&#20301;&#23485;&#24230;&#30340;&#31934;&#24230;&#20248;&#21183;&#21644;&#20108;&#20540;&#21270;&#30340;&#25928;&#29575;&#20248;&#21183;&#65292;&#20174;&#32780;&#22312;&#25552;&#39640;LLMs&#30340;&#35745;&#31639;&#25928;&#29575;&#30340;&#21516;&#26102;&#20445;&#25345;&#20102;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26174;&#33879;&#25512;&#36827;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#28982;&#32780;&#39640;&#26114;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#24320;&#38144;&#38459;&#30861;&#20102;&#23427;&#20204;&#30340;&#23454;&#38469;&#37096;&#32626;&#12290;&#37327;&#21270;&#25104;&#20026;&#25913;&#21892;LLMs&#35745;&#31639;&#25928;&#29575;&#30340;&#26368;&#26377;&#25928;&#26041;&#27861;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#36229;&#20302;&#27604;&#29305;&#37327;&#21270;&#24635;&#26159;&#23548;&#33268;&#20005;&#37325;&#30340;&#31934;&#24230;&#19979;&#38477;&#12290;&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#32531;&#35299;&#20102;&#36229;&#20302;&#27604;&#29305;&#37327;&#21270;&#30340;&#24494;&#35266;&#21644;&#23439;&#35266;&#29305;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;LLMs&#21452;&#20108;&#20540;&#21270;&#26041;&#27861;&#65292;&#21363;DB-LLM&#12290;&#23545;&#20110;&#24494;&#35266;&#23618;&#38754;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;2&#20301;&#23485;&#24230;&#30340;&#20934;&#30830;&#24615;&#20248;&#21183;&#21644;&#20108;&#20540;&#21270;&#30340;&#25928;&#29575;&#20248;&#21183;&#65292;&#24341;&#20837;&#20102;&#28789;&#27963;&#21452;&#20108;&#20540;&#21270;(FDB)&#12290;&#36890;&#36807;&#23558;2&#20301;&#37327;&#21270;&#26435;&#37325;&#20998;&#20026;&#20004;&#32452;&#29420;&#31435;&#30340;&#20108;&#36827;&#21046;&#25968;&#38598;&#65292;FDB&#30830;&#20445;&#20102;&#34920;&#31034;&#30340;&#20934;&#30830;&#24615;&#24182;&#24341;&#20837;&#20102;&#28789;&#27963;&#24615;&#65292;&#21033;&#29992;&#20108;&#20540;&#21270;&#30340;&#39640;&#25928;&#20301;&#25805;&#20316;&#21516;&#26102;&#20445;&#30041;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11960v1 Announce Type: cross  Abstract: Large language models (LLMs) have significantly advanced the field of natural language processing, while the expensive memory and computation consumption impede their practical deployment. Quantization emerges as one of the most effective methods for improving the computational efficiency of LLMs. However, existing ultra-low-bit quantization always causes severe accuracy drops. In this paper, we empirically relieve the micro and macro characteristics of ultra-low bit quantization and present a novel Dual-Binarization method for LLMs, namely DB-LLM. For the micro-level, we take both the accuracy advantage of 2-bit-width and the efficiency advantage of binarization into account, introducing Flexible Dual Binarization (FDB). By splitting 2-bit quantized weights into two independent sets of binaries, FDB ensures the accuracy of representations and introduces flexibility, utilizing the efficient bitwise operations of binarization while reta
&lt;/p&gt;</description></item><item><title>&#23545;&#25239;&#26679;&#26412;&#30340;&#20998;&#31867;&#27169;&#24335;&#21644;&#26102;&#38388;&#20391;&#20449;&#36947;&#30340;&#32467;&#21512;&#21487;&#20197;&#23548;&#33268;&#31363;&#21462;&#39044;&#35757;&#32451;&#30340;CNN&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.11953</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#25239;&#26679;&#26412;&#21644;&#26102;&#38388;&#20391;&#20449;&#36947;&#25581;&#31034;&#39044;&#35757;&#32451;&#30340;CNN&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Stealing the Invisible: Unveiling Pre-Trained CNN Models through Adversarial Examples and Timing Side-Channels
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11953
&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#26679;&#26412;&#30340;&#20998;&#31867;&#27169;&#24335;&#21644;&#26102;&#38388;&#20391;&#20449;&#36947;&#30340;&#32467;&#21512;&#21487;&#20197;&#23548;&#33268;&#31363;&#21462;&#39044;&#35757;&#32451;&#30340;CNN&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#20854;&#20247;&#22810;&#24212;&#29992;&#20013;&#24050;&#25104;&#20026;&#35768;&#22810;&#25216;&#26415;&#31995;&#32479;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#24120;&#35265;&#20570;&#27861;&#26159;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#65292;&#21363;&#35843;&#25972;&#39044;&#20808;&#35757;&#32451;&#30340;&#27169;&#22411;&#26550;&#26500;&#20197;&#36866;&#24212;&#29305;&#23450;&#20219;&#21153;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#35266;&#23519;&#32467;&#26524;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#35748;&#20026;&#23545;&#25239;&#22270;&#20687;&#30340;&#20998;&#31867;&#27169;&#24335;&#21487;&#29992;&#20316;&#31363;&#21462;&#27169;&#22411;&#30340;&#25163;&#27573;&#12290;&#27492;&#22806;&#65292;&#23545;&#25239;&#22270;&#20687;&#30340;&#20998;&#31867;&#32467;&#21512;&#26102;&#38388;&#20391;&#20449;&#36947;&#21487;&#20197;&#23548;&#33268;&#27169;&#22411;&#31363;&#21462;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11953v1 Announce Type: cross  Abstract: Machine learning, with its myriad applications, has become an integral component of numerous technological systems. A common practice in this domain is the use of transfer learning, where a pre-trained model's architecture, readily available to the public, is fine-tuned to suit specific tasks. As Machine Learning as a Service (MLaaS) platforms increasingly use pre-trained models in their backends, it's crucial to safeguard these architectures and understand their vulnerabilities. In this work, we present an approach based on the observation that the classification patterns of adversarial images can be used as a means to steal the models. Furthermore, the adversarial image classifications in conjunction with timing side channels can lead to a model stealing method. Our approach, designed for typical user-level access in remote MLaaS environments exploits varying misclassifications of adversarial images across different models to fingerp
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;VAE&#19982;Transformer&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#26500;&#21644;&#21442;&#25968;&#20248;&#21270;&#65292;&#25104;&#21151;&#22788;&#29702;&#22810;&#26679;&#21270;&#20998;&#23376;&#30340;&#29983;&#25104;&#65292;&#29983;&#25104;&#30340;&#20998;&#23376;&#24615;&#33021;&#19982;&#29616;&#26377;&#27169;&#22411;&#30456;&#23218;&#32654;&#65292;&#23545;&#29983;&#25104;&#20855;&#26377;&#26410;&#30693;&#32467;&#26500;&#30340;&#20998;&#23376;&#20855;&#26377;&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.11950</link><description>&lt;p&gt;
&#19968;&#31181;&#23558;VAE&#19982;Transformer&#32467;&#21512;&#30340;&#26032;&#22411;&#20998;&#23376;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A novel molecule generative model of VAE combined with Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11950
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;VAE&#19982;Transformer&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#26500;&#21644;&#21442;&#25968;&#20248;&#21270;&#65292;&#25104;&#21151;&#22788;&#29702;&#22810;&#26679;&#21270;&#20998;&#23376;&#30340;&#29983;&#25104;&#65292;&#29983;&#25104;&#30340;&#20998;&#23376;&#24615;&#33021;&#19982;&#29616;&#26377;&#27169;&#22411;&#30456;&#23218;&#32654;&#65292;&#23545;&#29983;&#25104;&#20855;&#26377;&#26410;&#30693;&#32467;&#26500;&#30340;&#20998;&#23376;&#20855;&#26377;&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#20998;&#23376;&#29983;&#25104;&#12290;&#22312;&#35813;&#39046;&#22495;&#20013;&#65292;Transformer&#21644;VAE&#34987;&#24191;&#27867;&#20351;&#29992;&#20316;&#20026;&#24378;&#22823;&#30340;&#27169;&#22411;&#65292;&#20294;&#30001;&#20110;&#23427;&#20204;&#20043;&#38388;&#30340;&#32467;&#26500;&#21644;&#24615;&#33021;&#19981;&#21305;&#37197;&#65292;&#24456;&#23569;&#32452;&#21512;&#20351;&#29992;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#22788;&#29702;&#22810;&#26679;&#21270;&#20998;&#23376;&#30340;&#32467;&#26500;&#21644;&#21442;&#25968;&#20248;&#21270;&#65292;&#25552;&#20986;&#20102;&#23558;&#36825;&#20004;&#31181;&#27169;&#22411;&#32467;&#21512;&#30340;&#27169;&#22411;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#29983;&#25104;&#20998;&#23376;&#26041;&#38754;&#34920;&#29616;&#20986;&#19982;&#29616;&#26377;&#27169;&#22411;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#29983;&#25104;&#20855;&#26377;&#26410;&#30693;&#32467;&#26500;&#30340;&#20998;&#23376;&#26041;&#38754;&#34920;&#29616;&#20986;&#36804;&#20170;&#20026;&#27490;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#25104;&#21151;&#21033;&#29992;&#20102;VAE&#30340;&#28508;&#22312;&#34920;&#31034;&#26469;&#39044;&#27979;&#20998;&#23376;&#24615;&#36136;&#12290;&#28040;&#34701;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#29983;&#25104;&#26032;&#39062;&#20998;&#23376;&#26041;&#38754;&#65292;VAE&#27604;&#35821;&#35328;&#27169;&#22411;&#31561;&#20854;&#20182;&#29983;&#25104;&#27169;&#22411;&#20855;&#26377;&#20248;&#21183;&#65292;&#24182;&#19988;&#20998;&#23376;&#21487;&#20197;&#29992;&#32422;32&#20010;&#32500;&#24230;&#21464;&#37327;&#26469;&#25551;&#36848;&#65292;&#27604;&#29616;&#26377;&#25551;&#36848;&#31526;&#21644;&#27169;&#22411;&#35201;&#23567;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11950v1 Announce Type: cross  Abstract: Recently, molecule generation using deep learning has been actively investigated in drug discovery. In this field, Transformer and VAE are widely used as powerful models, but they are rarely used in combination due to structural and performance mismatch of them. This study proposes a model that combines these two models through structural and parameter optimization in handling diverse molecules. The proposed model shows comparable performance to existing models in generating molecules, and showed by far superior performance in generating molecules with unseen structures. In addition, the proposed model successfully predicted molecular properties using the latent representation of VAE. Ablation studies suggested the advantage of VAE over other generative models like language model in generating novel molecules, and that the molecules can be described by ~32 dimensional variables, much smaller than existing descriptors and models. This s
&lt;/p&gt;</description></item><item><title>Mini-Hes&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;mini-block&#23545;&#35282;&#40657;&#22622;&#26080;&#32422;&#26463;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#26500;&#24314;&#20108;&#38454;&#28508;&#22312;&#22240;&#23376;&#20998;&#26512;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#22823;&#25968;&#25454;&#37327;&#19979;&#20108;&#38454;&#31639;&#27861;&#21487;&#34892;&#24615;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.11948</link><description>&lt;p&gt;
Mini-Hes&#65306;&#19968;&#31181;&#21487;&#24182;&#34892;&#21270;&#30340;&#20108;&#38454;&#28508;&#22312;&#22240;&#23376;&#20998;&#26512;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Mini-Hes: A Parallelizable Second-order Latent Factor Analysis Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11948
&lt;/p&gt;
&lt;p&gt;
Mini-Hes&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;mini-block&#23545;&#35282;&#40657;&#22622;&#26080;&#32422;&#26463;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#26500;&#24314;&#20108;&#38454;&#28508;&#22312;&#22240;&#23376;&#20998;&#26512;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#22823;&#25968;&#25454;&#37327;&#19979;&#20108;&#38454;&#31639;&#27861;&#21487;&#34892;&#24615;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#22823;&#37327;&#23454;&#20307;&#20043;&#38388;&#30340;&#20132;&#20114;&#22312;&#35768;&#22810;&#19982;&#22823;&#25968;&#25454;&#30456;&#20851;&#30340;&#20219;&#21153;&#20013;&#33258;&#28982;&#26159;&#39640;&#32500;&#19988;&#19981;&#23436;&#25972;&#30340;&#65288;HDI&#65289;&#12290;&#29992;&#25143;&#30340;&#34892;&#20026;&#29305;&#24449;&#38544;&#34255;&#22312;&#36825;&#20123;&#20132;&#20114;&#20013;&#65292;&#22240;&#27492;&#65292;&#26377;&#25928;&#22320;&#34920;&#31034;HDI&#25968;&#25454;&#26159;&#29702;&#35299;&#29992;&#25143;&#34892;&#20026;&#30340;&#22522;&#26412;&#20219;&#21153;&#12290;&#28508;&#22312;&#22240;&#23376;&#20998;&#26512;&#65288;LFA&#65289;&#27169;&#22411;&#24050;&#34987;&#35777;&#26126;&#22312;&#34920;&#31034;HDI&#25968;&#25454;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290; LFA&#27169;&#22411;&#30340;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#20854;&#35757;&#32451;&#36807;&#31243;&#65292;&#36825;&#26159;&#19968;&#20010;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;&#24050;&#32463;&#35777;&#26126;&#65292;&#22312;&#20854;&#35757;&#32451;&#36807;&#31243;&#20013;&#21516;&#26102;&#21253;&#21547;&#23616;&#37096;&#26354;&#29575;&#21644;&#39044;&#22788;&#29702;&#26799;&#24230;&#21487;&#20197;&#27604;&#20351;&#29992;&#19968;&#38454;&#26041;&#27861;&#26500;&#24314;&#30340;LFA&#27169;&#22411;&#34920;&#29616;&#20986;&#26356;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#25968;&#25454;&#37327;&#30340;&#22686;&#21152;&#65292;&#20108;&#38454;&#31639;&#27861;&#30340;&#21487;&#34892;&#24615;&#38754;&#20020;&#25361;&#25112;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#20851;&#38190;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26500;&#24314;LFA&#27169;&#22411;&#30340;mini-block&#23545;&#35282;&#40657;&#22622;&#26080;&#32422;&#26463;&#65288;Mini-Hes&#65289;&#20248;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11948v1 Announce Type: cross  Abstract: Interactions among large number of entities is naturally high-dimensional and incomplete (HDI) in many big data related tasks. Behavioral characteristics of users are hidden in these interactions, hence, effective representation of the HDI data is a fundamental task for understanding user behaviors. Latent factor analysis (LFA) model has proven to be effective in representing HDI data. The performance of an LFA model relies heavily on its training process, which is a non-convex optimization. It has been proven that incorporating local curvature and preprocessing gradients during its training process can lead to superior performance compared to LFA models built with first-order family methods. However, with the escalation of data volume, the feasibility of second-order algorithms encounters challenges. To address this pivotal issue, this paper proposes a mini-block diagonal hessian-free (Mini-Hes) optimization for building an LFA model.
&lt;/p&gt;</description></item><item><title>Leaky ReLU&#21442;&#25968;$\alpha=-1$&#22312;&#35757;&#32451;&#35823;&#24046;&#21644;&#27867;&#21270;&#35823;&#24046;&#30028;&#26041;&#38754;&#26159;&#26368;&#20248;&#30340;&#36873;&#25321;&#12290;</title><link>https://arxiv.org/abs/2402.11942</link><description>&lt;p&gt;
Leaky ReLU&#23545;&#36229;&#21442;&#25968;&#32593;&#32476;&#30340;&#35757;&#32451;&#21644;&#27867;&#21270;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The effect of Leaky ReLUs on the training and generalization of overparameterized networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11942
&lt;/p&gt;
&lt;p&gt;
Leaky ReLU&#21442;&#25968;$\alpha=-1$&#22312;&#35757;&#32451;&#35823;&#24046;&#21644;&#27867;&#21270;&#35823;&#24046;&#30028;&#26041;&#38754;&#26159;&#26368;&#20248;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#21508;&#31181;&#27844;&#28431;&#20462;&#27491;&#32447;&#24615;&#21333;&#20803;&#65288;ReLU&#65289;&#20989;&#25968;&#30340;&#36229;&#21442;&#25968;&#31070;&#32463;&#32593;&#32476;&#65288;NNs&#65289;&#30340;&#35757;&#32451;&#21644;&#27867;&#21270;&#35823;&#24046;&#12290;&#26356;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#20180;&#32454;&#22320;&#23545;&#36825;&#20123;NNs&#30340;&#35757;&#32451;&#35823;&#24046;&#30340;&#25910;&#25947;&#36895;&#29575;&#21644;&#27867;&#21270;&#35823;&#24046;&#36827;&#34892;&#20102;&#19978;&#30028;&#20272;&#35745;&#65292;&#24182;&#30740;&#31350;&#20102;&#36825;&#20123;&#30028;&#38480;&#23545;Leaky ReLU&#21442;&#25968;$\alpha$&#30340;&#20381;&#36182;&#24615;&#12290;&#25105;&#20204;&#34920;&#26126;$\alpha=-1$&#65292;&#23545;&#24212;&#20110;&#32477;&#23545;&#20540;&#28608;&#27963;&#20989;&#25968;&#65292;&#23545;&#20110;&#35757;&#32451;&#35823;&#24046;&#30028;&#26159;&#26368;&#20248;&#30340;&#12290;&#27492;&#22806;&#65292;&#22312;&#29305;&#23450;&#35774;&#32622;&#20013;&#65292;&#36825;&#20063;&#26159;&#27867;&#21270;&#35823;&#24046;&#30028;&#30340;&#26368;&#20248;&#36873;&#25321;&#12290;&#25968;&#20540;&#23454;&#39564;&#22312;&#23454;&#36341;&#20013;&#25903;&#25345;&#20102;&#29702;&#35770;&#24341;&#23548;&#30340;&#23454;&#38469;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11942v1 Announce Type: new  Abstract: We investigate the training and generalization errors of overparameterized neural networks (NNs) with a wide class of leaky rectified linear unit (ReLU) functions. More specifically, we carefully upper bound both the convergence rate of the training error and the generalization error of such NNs and investigate the dependence of these bounds on the Leaky ReLU parameter, $\alpha$. We show that $\alpha =-1$, which corresponds to the absolute value activation function, is optimal for the training error bound. Furthermore, in special settings, it is also optimal for the generalization error bound. Numerical experiments empirically support the practical choices guided by the theory.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#25915;&#20987;&#31574;&#30053;AICAttack&#65292;&#26088;&#22312;&#36890;&#36807;&#24494;&#23567;&#30340;&#22270;&#20687;&#25200;&#21160;&#26469;&#25915;&#20987;&#22270;&#20687;&#23383;&#24149;&#27169;&#22411;&#65292;&#22312;&#40657;&#30418;&#25915;&#20987;&#24773;&#26223;&#19979;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.11940</link><description>&lt;p&gt;
AICAttack&#65306;&#22522;&#20110;&#27880;&#24847;&#21147;&#20248;&#21270;&#30340;&#23545;&#25239;&#24615;&#22270;&#20687;&#23383;&#24149;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
AICAttack: Adversarial Image Captioning Attack with Attention-Based Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11940
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#25915;&#20987;&#31574;&#30053;AICAttack&#65292;&#26088;&#22312;&#36890;&#36807;&#24494;&#23567;&#30340;&#22270;&#20687;&#25200;&#21160;&#26469;&#25915;&#20987;&#22270;&#20687;&#23383;&#24149;&#27169;&#22411;&#65292;&#22312;&#40657;&#30418;&#25915;&#20987;&#24773;&#26223;&#19979;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#21462;&#24471;&#20102;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#65288;CV&#65289;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#31561;&#35768;&#22810;&#20219;&#21153;&#19978;&#26174;&#33879;&#30340;&#25104;&#23601;&#12290;CV&#21644;NLP&#20132;&#21449;&#28857;&#19978;&#30340;&#22270;&#20687;&#23383;&#24149;&#38382;&#39064;&#20013;&#65292;&#30456;&#20851;&#27169;&#22411;&#23545;&#25239;&#25915;&#20987;&#30340;&#31283;&#20581;&#24615;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#25239;&#25915;&#20987;&#31574;&#30053;&#65292;&#31216;&#20026;AICAttack&#65288;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22270;&#20687;&#23383;&#24149;&#25915;&#20987;&#65289;&#65292;&#26088;&#22312;&#36890;&#36807;&#23545;&#22270;&#20687;&#36827;&#34892;&#24494;&#23567;&#25200;&#21160;&#26469;&#25915;&#20987;&#22270;&#20687;&#23383;&#24149;&#27169;&#22411;&#12290;&#22312;&#40657;&#30418;&#25915;&#20987;&#29615;&#22659;&#20013;&#36816;&#34892;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#19981;&#38656;&#35201;&#35775;&#38382;&#30446;&#26631;&#27169;&#22411;&#30340;&#26550;&#26500;&#12289;&#21442;&#25968;&#25110;&#26799;&#24230;&#20449;&#24687;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#20505;&#36873;&#36873;&#25321;&#26426;&#21046;&#65292;&#21487;&#35782;&#21035;&#26368;&#20339;&#20687;&#32032;&#36827;&#34892;&#25915;&#20987;&#65292;&#28982;&#21518;&#37319;&#29992;&#24046;&#20998;&#36827;&#21270;&#65288;DE&#65289;&#26469;&#25200;&#20081;&#20687;&#32032;&#30340;RGB&#20540;&#12290;&#36890;&#36807;&#23545;&#22522;&#20934;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;AICAttack&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11940v1 Announce Type: cross  Abstract: Recent advances in deep learning research have shown remarkable achievements across many tasks in computer vision (CV) and natural language processing (NLP). At the intersection of CV and NLP is the problem of image captioning, where the related models' robustness against adversarial attacks has not been well studied. In this paper, we present a novel adversarial attack strategy, which we call AICAttack (Attention-based Image Captioning Attack), designed to attack image captioning models through subtle perturbations on images. Operating within a black-box attack scenario, our algorithm requires no access to the target model's architecture, parameters, or gradient information. We introduce an attention-based candidate selection mechanism that identifies the optimal pixels to attack, followed by Differential Evolution (DE) for perturbing pixels' RGB values. We demonstrate AICAttack's effectiveness through extensive experiments on benchma
&lt;/p&gt;</description></item><item><title>SLADE&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#36793;&#32536;&#27969;&#20013;&#36805;&#36895;&#26816;&#27979;&#21160;&#24577;&#24322;&#24120;&#65292;&#26080;&#38656;&#20381;&#36182;&#26631;&#31614;&#65292;&#20027;&#35201;&#36890;&#36807;&#35266;&#23519;&#33410;&#28857;&#20132;&#20114;&#27169;&#24335;&#30340;&#20559;&#24046;&#26469;&#26816;&#27979;&#33410;&#28857;&#29366;&#24577;&#36716;&#21464;&#12290;</title><link>https://arxiv.org/abs/2402.11933</link><description>&lt;p&gt;
SLADE&#65306;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#36793;&#32536;&#27969;&#20013;&#26816;&#27979;&#21160;&#24577;&#24322;&#24120;
&lt;/p&gt;
&lt;p&gt;
SLADE: Detecting Dynamic Anomalies in Edge Streams without Labels via Self-Supervised Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11933
&lt;/p&gt;
&lt;p&gt;
SLADE&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#36793;&#32536;&#27969;&#20013;&#36805;&#36895;&#26816;&#27979;&#21160;&#24577;&#24322;&#24120;&#65292;&#26080;&#38656;&#20381;&#36182;&#26631;&#31614;&#65292;&#20027;&#35201;&#36890;&#36807;&#35266;&#23519;&#33410;&#28857;&#20132;&#20114;&#27169;&#24335;&#30340;&#20559;&#24046;&#26469;&#26816;&#27979;&#33410;&#28857;&#29366;&#24577;&#36716;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#26816;&#27979;&#30495;&#23454;&#19990;&#30028;&#22270;&#20013;&#30340;&#24322;&#24120;&#65292;&#22914;&#31038;&#20132;&#12289;&#30005;&#23376;&#37038;&#20214;&#21644;&#37329;&#34701;&#32593;&#32476;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#21508;&#31181;&#26041;&#27861;&#12290;&#22312;&#22823;&#22810;&#25968;&#30495;&#23454;&#19990;&#30028;&#22270;&#38543;&#26102;&#38388;&#22686;&#38271;&#65292;&#33258;&#28982;&#22320;&#34920;&#31034;&#20026;&#36793;&#32536;&#27969;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#65306;(a)&#22312;&#24322;&#24120;&#21457;&#29983;&#26102;&#21363;&#26102;&#26816;&#27979;&#24322;&#24120;&#65292;(b)&#36866;&#24212;&#21160;&#24577;&#21464;&#21270;&#30340;&#29366;&#24577;&#65292;(c)&#22788;&#29702;&#21160;&#24577;&#24322;&#24120;&#26631;&#31614;&#30340;&#31232;&#32570;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SLADE&#65288;&#36793;&#32536;&#27969;&#24322;&#24120;&#26816;&#27979;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#65289;&#65292;&#29992;&#20110;&#22312;&#36793;&#32536;&#27969;&#20013;&#24555;&#36895;&#26816;&#27979;&#21160;&#24577;&#24322;&#24120;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#26631;&#31614;&#12290;SLADE&#36890;&#36807;&#35266;&#23519;&#33410;&#28857;&#22312;&#26102;&#38388;&#19978;&#30456;&#20114;&#20316;&#29992;&#27169;&#24335;&#30340;&#20559;&#24046;&#26469;&#26816;&#27979;&#33410;&#28857;&#36827;&#20837;&#24322;&#24120;&#29366;&#24577;&#30340;&#36716;&#21464;&#12290;&#20026;&#27492;&#65292;&#23427;&#35757;&#32451;&#19968;&#20010;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25191;&#34892;&#20004;&#20010;&#33258;&#30417;&#30563;&#20219;&#21153;&#65306;(a)&#26368;&#23567;&#21270;&#33410;&#28857;&#34920;&#31034;&#20013;&#30340;&#28418;&#31227;&#65292;(b)&#20174;&#30701;&#26399;&#29983;&#25104;&#38271;&#26399;&#20132;&#20114;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11933v1 Announce Type: new  Abstract: To detect anomalies in real-world graphs, such as social, email, and financial networks, various approaches have been developed. While they typically assume static input graphs, most real-world graphs grow over time, naturally represented as edge streams. In this context, we aim to achieve three goals: (a) instantly detecting anomalies as they occur, (b) adapting to dynamically changing states, and (c) handling the scarcity of dynamic anomaly labels. In this paper, we propose SLADE (Self-supervised Learning for Anomaly Detection in Edge Streams) for rapid detection of dynamic anomalies in edge streams, without relying on labels. SLADE detects the shifts of nodes into abnormal states by observing deviations in their interaction patterns over time. To this end, it trains a deep neural network to perform two self-supervised tasks: (a) minimizing drift in node representations and (b) generating long-term interaction patterns from short-term 
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;JD2P&#30340;&#26032;&#22411;&#31163;&#32447;&#26550;&#26500;&#65292;&#36890;&#36807;&#32852;&#21512;&#25968;&#25454;&#28145;&#21270;&#21644;&#39044;&#21462;&#25216;&#26415;&#65292;&#25353;&#39034;&#24207;&#31163;&#32447;&#27599;&#20010;&#25968;&#25454;&#26679;&#26412;&#30340;&#29305;&#24449;&#65292;&#20197;&#20943;&#23569;&#29289;&#32852;&#32593;&#35774;&#22791;&#21521;&#36793;&#32536;&#26381;&#21153;&#22120;&#20256;&#36755;&#25968;&#25454;&#26102;&#25152;&#38656;&#30340;&#33021;&#37327;&#28040;&#32791;&#12290;</title><link>https://arxiv.org/abs/2402.11925</link><description>&lt;p&gt;
&#36890;&#36807;&#32852;&#21512;&#25968;&#25454;&#28145;&#21270;&#21644;&#39044;&#21462;&#23454;&#29616;&#33021;&#25928;&#36793;&#32536;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Energy-Efficient Edge Learning via Joint Data Deepening-and-Prefetching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11925
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;JD2P&#30340;&#26032;&#22411;&#31163;&#32447;&#26550;&#26500;&#65292;&#36890;&#36807;&#32852;&#21512;&#25968;&#25454;&#28145;&#21270;&#21644;&#39044;&#21462;&#25216;&#26415;&#65292;&#25353;&#39034;&#24207;&#31163;&#32447;&#27599;&#20010;&#25968;&#25454;&#26679;&#26412;&#30340;&#29305;&#24449;&#65292;&#20197;&#20943;&#23569;&#29289;&#32852;&#32593;&#35774;&#22791;&#21521;&#36793;&#32536;&#26381;&#21153;&#22120;&#20256;&#36755;&#25968;&#25454;&#26102;&#25152;&#38656;&#30340;&#33021;&#37327;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#29289;&#32852;&#32593;&#35774;&#22791;&#25910;&#38598;&#30340;&#23454;&#26102;&#25968;&#25454;&#23545;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#36827;&#34892;&#23454;&#26102;&#35757;&#32451;&#65292;&#23454;&#29616;&#26080;&#22788;&#19981;&#22312;&#30340;&#20154;&#24037;&#26234;&#33021;&#26381;&#21153;&#30340;&#24895;&#26223;&#12290;&#20026;&#20102;&#35299;&#20915;&#33021;&#28304;&#21463;&#38480;&#30340;&#29289;&#32852;&#32593;&#35774;&#22791;&#20256;&#36755;&#39640;&#32500;&#19988;&#24222;&#22823;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31163;&#32447;&#26550;&#26500;&#65292;&#31216;&#20026;&#32852;&#21512;&#25968;&#25454;&#28145;&#21270;&#21644;&#39044;&#21462;&#65288;JD2P&#65289;&#65292;&#20854;&#21253;&#21547;&#20004;&#20010;&#20851;&#38190;&#25216;&#26415;&#65306;&#25968;&#25454;&#28145;&#21270;&#21644;&#39044;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11925v1 Announce Type: cross  Abstract: The vision of pervasive artificial intelligence (AI) services can be realized by training an AI model on time using real-time data collected by internet of things (IoT) devices. To this end, IoT devices require offloading their data to an edge server in proximity. However, transmitting high-dimensional and voluminous data from energy-constrained IoT devices poses a significant challenge. To address this limitation, we propose a novel offloading architecture, called joint data deepening-and-prefetching (JD2P), which is feature-by-feature offloading comprising two key techniques. The first one is data deepening, where each data sample's features are sequentially offloaded in the order of importance determined by the data embedding technique such as principle component analysis (PCA). Offloading is terminated once the already transmitted features are sufficient for accurate data classification, resulting in a reduction in the amount of tr
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#26694;&#26550; GPDiff&#65292;&#36890;&#36807;&#22312;&#28304;&#22478;&#24066;&#25968;&#25454;&#20248;&#21270;&#30340;&#27169;&#22411;&#21442;&#25968;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#23558;STG&#36801;&#31227;&#23398;&#20064;&#36716;&#21270;&#20026;&#39044;&#35757;&#32451;&#29983;&#25104;&#24335;&#36229;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#23545;&#19981;&#21516;&#25968;&#25454;&#20998;&#24067;&#21644;&#29305;&#23450;&#22478;&#24066;&#29305;&#24449;&#30340;&#36866;&#24212;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.11922</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#26102;&#31354;&#22270;&#36801;&#31227;&#23398;&#20064;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Generative Pre-Training Framework for Spatio-Temporal Graph Transfer Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11922
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#26694;&#26550; GPDiff&#65292;&#36890;&#36807;&#22312;&#28304;&#22478;&#24066;&#25968;&#25454;&#20248;&#21270;&#30340;&#27169;&#22411;&#21442;&#25968;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#23558;STG&#36801;&#31227;&#23398;&#20064;&#36716;&#21270;&#20026;&#39044;&#35757;&#32451;&#29983;&#25104;&#24335;&#36229;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#23545;&#19981;&#21516;&#25968;&#25454;&#20998;&#24067;&#21644;&#29305;&#23450;&#22478;&#24066;&#29305;&#24449;&#30340;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#31354;&#22270;&#65288;STG&#65289;&#23398;&#20064;&#23545;&#20110;&#26234;&#24935;&#22478;&#24066;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#28982;&#32780;&#22312;&#35768;&#22810;&#22478;&#24066;&#21644;&#22320;&#21306;&#24448;&#24448;&#23384;&#22312;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#26694;&#26550; GPDiff&#65292;&#29992;&#20110;STG&#36801;&#31227;&#23398;&#20064;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#32463;&#36807;&#28304;&#22478;&#24066;&#25968;&#25454;&#20248;&#21270;&#30340;&#19968;&#31995;&#21015;&#27169;&#22411;&#21442;&#25968;&#19978;&#36827;&#34892;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#26469;&#25191;&#34892;STG&#36801;&#31227;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11922v1 Announce Type: new  Abstract: Spatio-temporal graph (STG) learning is foundational for smart city applications, yet it is often hindered by data scarcity in many cities and regions. To bridge this gap, we propose a novel generative pre-training framework, GPDiff, for STG transfer learning. Unlike conventional approaches that heavily rely on common feature extraction or intricate transfer learning designs, our solution takes a novel approach by performing generative pre-training on a collection of model parameters optimized with data from source cities. We recast STG transfer learning as pre-training a generative hypernetwork, which generates tailored model parameters guided by prompts, allowing for adaptability to diverse data distributions and city-specific characteristics. GPDiff employs a diffusion model with a transformer-based denoising network, which is model-agnostic to integrate with powerful STG models. By addressing challenges arising from data gaps and the
&lt;/p&gt;</description></item><item><title>&#23545;&#22312;&#21512;&#25104;&#25512;&#29702;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;Transformer&#36827;&#34892;&#30340;&#26426;&#29702;&#20998;&#26512;&#25581;&#31034;&#20854;&#23454;&#29616;&#20102;&#19968;&#20010;&#22312;&#24182;&#34892;&#36816;&#34892;&#30340;&#26377;&#30028;&#28145;&#24230;&#24490;&#29615;&#26426;&#21046;&#65292;&#24182;&#23558;&#20013;&#38388;&#32467;&#26524;&#23384;&#20648;&#22312;&#36873;&#23450;&#30340;&#20196;&#29260;&#20301;&#32622;</title><link>https://arxiv.org/abs/2402.11917</link><description>&lt;p&gt;
&#22312;&#31526;&#21495;&#21270;&#22810;&#27493;&#25512;&#29702;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;Transformer&#30340;&#26426;&#29702;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Mechanistic Analysis of a Transformer Trained on a Symbolic Multi-Step Reasoning Task
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11917
&lt;/p&gt;
&lt;p&gt;
&#23545;&#22312;&#21512;&#25104;&#25512;&#29702;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;Transformer&#36827;&#34892;&#30340;&#26426;&#29702;&#20998;&#26512;&#25581;&#31034;&#20854;&#23454;&#29616;&#20102;&#19968;&#20010;&#22312;&#24182;&#34892;&#36816;&#34892;&#30340;&#26377;&#30028;&#28145;&#24230;&#24490;&#29615;&#26426;&#21046;&#65292;&#24182;&#23558;&#20013;&#38388;&#32467;&#26524;&#23384;&#20648;&#22312;&#36873;&#23450;&#30340;&#20196;&#29260;&#20301;&#32622;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#22312;&#19968;&#31995;&#21015;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35780;&#20272;&#36825;&#20123;&#33021;&#21147;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#26159;&#23454;&#38469;&#25512;&#29702;&#30340;&#32467;&#26524;&#65292;&#29616;&#26377;&#24037;&#20316;&#38598;&#20013;&#20110;&#24320;&#21457;&#22797;&#26434;&#30340;&#34892;&#20026;&#30740;&#31350;&#22522;&#20934;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#24182;&#26410;&#25552;&#20379;&#20851;&#20110;&#39537;&#21160;&#35266;&#23519;&#21040;&#30340;&#33021;&#21147;&#30340;&#20869;&#37096;&#26426;&#21046;&#30340;&#35265;&#35299;&#12290;&#20026;&#20102;&#25913;&#21892;&#25105;&#20204;&#23545;Transformer&#20869;&#37096;&#26426;&#21046;&#30340;&#29702;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#19968;&#20010;&#22312;&#21512;&#25104;&#25512;&#29702;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;Transformer&#36827;&#34892;&#20840;&#38754;&#30340;&#26426;&#29702;&#20998;&#26512;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#27169;&#22411;&#29992;&#26469;&#35299;&#20915;&#20219;&#21153;&#30340;&#19968;&#32452;&#21487;&#35299;&#37322;&#26426;&#21046;&#65292;&#24182;&#21033;&#29992;&#30456;&#20851;&#21644;&#22240;&#26524;&#35777;&#25454;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23427;&#23454;&#29616;&#20102;&#19968;&#20010;&#22312;&#24182;&#34892;&#36816;&#34892;&#30340;&#26377;&#30028;&#28145;&#24230;&#24490;&#29615;&#26426;&#21046;&#65292;&#24182;&#23558;&#20013;&#38388;&#32467;&#26524;&#23384;&#20648;&#22312;&#36873;&#23450;&#30340;&#20196;&#29260;&#20301;&#32622;&#12290;&#25105;&#20204;&#39044;&#26399;&#25105;&#20204;&#22312;&#25105;&#20204;&#30340;&#21512;&#25104;&#29615;&#22659;&#20013;&#35782;&#21035;&#30340;&#20027;&#39064;&#21487;&#20197;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11917v1 Announce Type: new  Abstract: Transformers demonstrate impressive performance on a range of reasoning benchmarks. To evaluate the degree to which these abilities are a result of actual reasoning, existing work has focused on developing sophisticated benchmarks for behavioral studies. However, these studies do not provide insights into the internal mechanisms driving the observed capabilities. To improve our understanding of the internal mechanisms of transformers, we present a comprehensive mechanistic analysis of a transformer trained on a synthetic reasoning task. We identify a set of interpretable mechanisms the model uses to solve the task, and validate our findings using correlational and causal evidence. Our results suggest that it implements a depth-bounded recurrent mechanisms that operates in parallel and stores intermediate results in selected token positions. We anticipate that the motifs we identified in our synthetic setting can provide valuable insights
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#38646;&#38454;&#21644;&#19968;&#38454;&#20248;&#21270;&#26041;&#27861;&#65292;&#35774;&#35745;&#20102;&#21487;&#25193;&#23637;&#30340;&#34394;&#25311;&#20272;&#20540;&#32452;&#21512;&#25293;&#21334;&#65292;&#20197;&#35299;&#20915;&#32452;&#21512;&#20505;&#36873;&#20998;&#37197;&#30340;&#21487;&#32553;&#25918;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.11904</link><description>&lt;p&gt;
&#36890;&#36807;&#32467;&#21512;&#38646;&#38454;&#21644;&#19968;&#38454;&#20248;&#21270;&#26041;&#27861;&#35774;&#35745;&#21487;&#25193;&#23637;&#30340;&#34394;&#25311;&#20272;&#20540;&#32452;&#21512;&#25293;&#21334;
&lt;/p&gt;
&lt;p&gt;
Scalable Virtual Valuations Combinatorial Auction Design by Combining Zeroth-Order and First-Order Optimization Method
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11904
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#38646;&#38454;&#21644;&#19968;&#38454;&#20248;&#21270;&#26041;&#27861;&#65292;&#35774;&#35745;&#20102;&#21487;&#25193;&#23637;&#30340;&#34394;&#25311;&#20272;&#20540;&#32452;&#21512;&#25293;&#21334;&#65292;&#20197;&#35299;&#20915;&#32452;&#21512;&#20505;&#36873;&#20998;&#37197;&#30340;&#21487;&#32553;&#25918;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11904v1 &#20844;&#21578;&#31867;&#22411;: &#20132;&#21449;&#35770;&#22363; &#25688;&#35201;: &#33258;&#21160;&#21270;&#25293;&#21334;&#35774;&#35745;&#26088;&#22312;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21457;&#29616;&#39640;&#25910;&#20837;&#21644;&#28608;&#21169;&#20860;&#23481;&#30340;&#26426;&#21046;&#12290;&#30830;&#20445;&#20027;&#23548;&#25112;&#30053;&#28608;&#21169;&#20860;&#23481;&#24615;&#65288;DSIC&#65289;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#26368;&#26377;&#25928;&#30340;&#26041;&#27861;&#26159;&#23558;&#26426;&#21046;&#38480;&#21046;&#22312;&#20223;&#23556;&#26368;&#22823;&#21270;&#25293;&#21334;&#65288;AMAs&#65289;&#33539;&#22260;&#20869;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;AMA&#30340;&#26041;&#27861;&#38754;&#20020;&#25361;&#25112;&#65292;&#22914;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#65288;&#30001;&#32452;&#21512;&#20505;&#36873;&#20998;&#37197;&#23548;&#33268;&#65289;&#21644;&#25910;&#20837;&#30340;&#19981;&#21487;&#24494;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#20026;&#20102;&#23454;&#29616;&#21487;&#25193;&#23637;&#30340;AMA&#26041;&#27861;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#25293;&#21334;&#26426;&#21046;&#38480;&#21046;&#22312;&#34394;&#25311;&#20272;&#20540;&#32452;&#21512;&#25293;&#21334;&#65288;VVCAs&#65289;&#33539;&#22260;&#20869;&#65292;&#36825;&#26159;&#20855;&#26377;&#26356;&#23569;&#21442;&#25968;&#30340;AMAs&#23376;&#38598;&#12290;&#26368;&#21021;&#65292;&#25105;&#20204;&#20351;&#29992;&#21487;&#24182;&#34892;&#21270;&#30340;&#21160;&#24577;&#35268;&#21010;&#31639;&#27861;&#35745;&#31639;VVCA&#30340;&#33719;&#32988;&#20998;&#37197;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#38646;&#38454;&#21644;&#19968;&#38454;&#25216;&#26415;&#30340;&#26032;&#22411;&#20248;&#21270;&#26041;&#27861;&#26469;&#20248;&#21270;VVCA&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11904v1 Announce Type: cross  Abstract: Automated auction design seeks to discover empirically high-revenue and incentive-compatible mechanisms using machine learning. Ensuring dominant strategy incentive compatibility (DSIC) is crucial, and the most effective approach is to confine the mechanism to Affine Maximizer Auctions (AMAs). Nevertheless, existing AMA-based approaches encounter challenges such as scalability issues (arising from combinatorial candidate allocations) and the non-differentiability of revenue. In this paper, to achieve a scalable AMA-based method, we further restrict the auction mechanism to Virtual Valuations Combinatorial Auctions (VVCAs), a subset of AMAs with significantly fewer parameters. Initially, we employ a parallelizable dynamic programming algorithm to compute the winning allocation of a VVCA. Subsequently, we propose a novel optimization method that combines both zeroth-order and first-order techniques to optimize the VVCA parameters. Extens
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21322;&#30417;&#30563;&#22270;&#24322;&#24120;&#26816;&#27979;&#30340;&#29983;&#25104;&#24335;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#27169;&#25311;&#24322;&#24120;&#33410;&#28857;&#26469;&#35757;&#32451;&#21028;&#21035;&#24615;&#21333;&#31867;&#20998;&#31867;&#22120;&#65292;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#22270;&#20013;&#30340;&#24050;&#30693;&#27491;&#24120;&#33410;&#28857;&#12290;</title><link>https://arxiv.org/abs/2402.11887</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#21322;&#30417;&#30563;&#22270;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Generative Semi-supervised Graph Anomaly Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11887
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21322;&#30417;&#30563;&#22270;&#24322;&#24120;&#26816;&#27979;&#30340;&#29983;&#25104;&#24335;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#27169;&#25311;&#24322;&#24120;&#33410;&#28857;&#26469;&#35757;&#32451;&#21028;&#21035;&#24615;&#21333;&#31867;&#20998;&#31867;&#22120;&#65292;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#22270;&#20013;&#30340;&#24050;&#30693;&#27491;&#24120;&#33410;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#32771;&#34385;&#20102;&#19968;&#20010;&#23454;&#38469;&#24773;&#22659;&#19979;&#30340;&#21322;&#30417;&#30563;&#22270;&#24322;&#24120;&#26816;&#27979;&#65288;GAD&#65289;&#65292;&#22312;&#36825;&#20010;&#24773;&#22659;&#20013;&#65292;&#22270;&#20013;&#30340;&#37096;&#20998;&#33410;&#28857;&#34987;&#30693;&#26195;&#26159;&#27491;&#24120;&#30340;&#65292;&#19982;&#22823;&#22810;&#25968;GAD&#30740;&#31350;&#20013;&#20351;&#29992;&#23436;&#20840;&#26410;&#26631;&#35760;&#22270;&#30340;&#26080;&#30417;&#30563;&#24773;&#20917;&#24418;&#25104;&#23545;&#27604;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21487;&#20197;&#21033;&#29992;&#36825;&#20123;&#27491;&#24120;&#33410;&#28857;&#26377;&#21161;&#20110;&#25552;&#21319;&#29616;&#26377;&#26080;&#30417;&#30563;GAD&#26041;&#27861;&#22312;&#21322;&#30417;&#30563;&#24773;&#22659;&#19979;&#30340;&#26816;&#27979;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23545;&#36825;&#20123;&#27491;&#24120;&#33410;&#28857;&#30340;&#21033;&#29992;&#26159;&#26377;&#38480;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#21322;&#30417;&#30563;&#24773;&#22659;&#30340;&#29983;&#25104;&#24335;GAD&#26041;&#27861;&#65288;GGAD&#65289;&#65292;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#36825;&#20123;&#27491;&#24120;&#33410;&#28857;&#12290;&#20854;&#20851;&#38190;&#24605;&#24819;&#26159;&#29983;&#25104;&#27169;&#25311;&#24322;&#24120;&#33410;&#28857;&#30340;&#24322;&#24120;&#33410;&#28857;&#65292;&#23427;&#20204;&#34701;&#21512;&#20102;&#26412;&#22320;&#32467;&#26500;&#21644;&#33410;&#28857;&#34920;&#31034;&#65292;&#20026;&#35757;&#32451;&#21028;&#21035;&#22411;&#21333;&#31867;&#20998;&#31867;&#22120;&#25552;&#20379;&#26377;&#25928;&#30340;&#36127;&#38754;&#33410;&#28857;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11887v1 Announce Type: new  Abstract: This work considers a practical semi-supervised graph anomaly detection (GAD) scenario, where part of the nodes in a graph are known to be normal, contrasting to the unsupervised setting in most GAD studies with a fully unlabeled graph. As expected, we find that having access to these normal nodes helps enhance the detection performance of existing unsupervised GAD methods when they are adapted to the semi-supervised setting. However, their utilization of these normal nodes is limited. In this paper, we propose a novel Generative GAD approach (GGAD) for the semi-supervised scenario to better exploit the normal nodes. The key idea is to generate outlier nodes that assimilate anomaly nodes in both local structure and node representations for providing effective negative node samples in training a discriminative one-class classifier. There have been many generative anomaly detection approaches, but they are designed for non-graph data, and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#26377;&#38480;&#26102;&#38388;&#20998;&#26512;&#20197;&#21450;&#23454;&#35777;&#35780;&#20272;&#65292;&#25506;&#35752;&#20102;&#38598;&#25104;&#27169;&#22411;&#26041;&#27861;&#30340;Q&#23398;&#20064;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.11877</link><description>&lt;p&gt;
&#22312;&#20855;&#26377;&#25918;&#26494;&#37319;&#26679;&#27169;&#22411;&#30340;&#22312;&#32447;&#27169;&#22411;&#30340;&#26377;&#38480;&#26102;&#38388;&#35823;&#24046;&#20998;&#26512;&#19979;&#30340;Q&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Finite-Time Error Analysis of Online Model-Based Q-Learning with a Relaxed Sampling Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11877
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#26377;&#38480;&#26102;&#38388;&#20998;&#26512;&#20197;&#21450;&#23454;&#35777;&#35780;&#20272;&#65292;&#25506;&#35752;&#20102;&#38598;&#25104;&#27169;&#22411;&#26041;&#27861;&#30340;Q&#23398;&#20064;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#27169;&#22411;&#20026;&#22522;&#30784;&#30340;&#26041;&#27861;&#30340;&#20986;&#29616;&#19979;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#22312;&#36825;&#20123;&#26041;&#27861;&#20013;&#65292;Q&#23398;&#20064;&#22312;&#26080;&#27169;&#22411;&#35774;&#32622;&#20013;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#23558;Q&#23398;&#20064;&#25193;&#23637;&#21040;&#22522;&#20110;&#27169;&#22411;&#30340;&#26694;&#26550;&#20173;&#28982;&#30456;&#23545;&#26410;&#34987;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;Q&#23398;&#20064;&#19982;&#22522;&#20110;&#27169;&#22411;&#26041;&#27861;&#30456;&#32467;&#21512;&#26102;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#35777;&#35780;&#20272;&#65292;&#25105;&#20204;&#35797;&#22270;&#38416;&#26126;&#22312;&#21738;&#20123;&#26465;&#20214;&#19979;&#65292;&#22522;&#20110;&#27169;&#22411;&#30340;Q&#23398;&#20064;&#22312;&#26679;&#26412;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#20854;&#26080;&#27169;&#22411;&#23545;&#24212;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11877v1 Announce Type: cross  Abstract: Reinforcement learning has witnessed significant advancements, particularly with the emergence of model-based approaches. Among these, $Q$-learning has proven to be a powerful algorithm in model-free settings. However, the extension of $Q$-learning to a model-based framework remains relatively unexplored. In this paper, we delve into the sample complexity of $Q$-learning when integrated with a model-based approach. Through theoretical analyses and empirical evaluations, we seek to elucidate the conditions under which model-based $Q$-learning excels in terms of sample efficiency compared to its model-free counterpart.
&lt;/p&gt;</description></item><item><title>LoRA&#35757;&#32451;&#22312;NTK&#27169;&#24335;&#19979;&#28040;&#38500;&#20102;&#34394;&#20551;&#23616;&#37096;&#26368;&#23567;&#20540;&#65292;&#26377;&#21161;&#20110;&#26799;&#24230;&#19979;&#38477;&#25214;&#21040;&#20302;&#31209;&#35299;&#24182;&#23454;&#29616;&#33391;&#22909;&#30340;&#27867;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.11867</link><description>&lt;p&gt;
LoRA&#35757;&#32451;&#22312;NTK&#27169;&#24335;&#19979;&#27809;&#26377;&#34394;&#20551;&#23616;&#37096;&#26368;&#23567;&#20540;
&lt;/p&gt;
&lt;p&gt;
LoRA Training in the NTK Regime has No Spurious Local Minima
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11867
&lt;/p&gt;
&lt;p&gt;
LoRA&#35757;&#32451;&#22312;NTK&#27169;&#24335;&#19979;&#28040;&#38500;&#20102;&#34394;&#20551;&#23616;&#37096;&#26368;&#23567;&#20540;&#65292;&#26377;&#21161;&#20110;&#26799;&#24230;&#19979;&#38477;&#25214;&#21040;&#20302;&#31209;&#35299;&#24182;&#23454;&#29616;&#33391;&#22909;&#30340;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#24050;&#25104;&#20026;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26631;&#20934;&#26041;&#27861;&#65292;&#20294;&#25105;&#20204;&#23545;LoRA&#30340;&#29702;&#35770;&#29702;&#35299;&#26377;&#38480;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;&#22312;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;NTK&#65289;&#27169;&#24335;&#19979;&#20351;&#29992;LoRA&#24494;&#35843;&#65292;&#20854;&#20013;&#21253;&#21547;$N$&#20010;&#25968;&#25454;&#28857;&#65292;&#32467;&#26524;&#26174;&#31034;&#65306;(i) &#20840;&#38754;&#24494;&#35843;&#65288;&#19981;&#20351;&#29992;LoRA&#65289;&#20801;&#35768;&#31209;&#20026;$r\lesssim \sqrt{N}$&#30340;&#20302;&#31209;&#35299;; (ii) &#20351;&#29992;&#31209;&#20026;$r\gtrsim \sqrt{N}$&#30340;LoRA&#28040;&#38500;&#20102;&#34394;&#20551;&#30340;&#23616;&#37096;&#26368;&#23567;&#20540;&#65292;&#20351;&#26799;&#24230;&#19979;&#38477;&#21487;&#20197;&#25214;&#21040;&#20302;&#31209;&#35299;; (iii) &#20351;&#29992;LoRA&#25214;&#21040;&#30340;&#20302;&#31209;&#35299;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11867v1 Announce Type: new  Abstract: Low-rank adaptation (LoRA) has become the standard approach for parameter-efficient fine-tuning of large language models (LLM), but our theoretical understanding of LoRA has been limited. In this work, we theoretically analyze LoRA fine-tuning in the neural tangent kernel (NTK) regime with $N$ data points, showing: (i) full fine-tuning (without LoRA) admits a low-rank solution of rank $r\lesssim \sqrt{N}$; (ii) using LoRA with rank $r\gtrsim \sqrt{N}$ eliminates spurious local minima, allowing gradient descent to find the low-rank solutions; (iii) the low-rank solution found using LoRA generalizes well.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#38543;&#26426;Hessian-&#21521;&#37327;&#20056;&#31215;&#19978;&#25311;&#21512;Hessian&#25110;&#20854;&#36870;&#65292;&#25581;&#31034;&#20102;&#19981;&#21516;Hessian&#25311;&#21512;&#26041;&#27861;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#29305;&#23450;&#26446;&#32676;&#19978;&#30340;Hessian&#25311;&#21512;&#38382;&#39064;&#22312;&#36731;&#24494;&#26465;&#20214;&#19979;&#26159;&#24378;&#20984;&#30340;&#12290;</title><link>https://arxiv.org/abs/2402.11858</link><description>&lt;p&gt;
&#22312;&#26446;&#32676;&#19978;&#30340;&#38543;&#26426;Hessian&#25311;&#21512;
&lt;/p&gt;
&lt;p&gt;
Stochastic Hessian Fitting on Lie Group
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11858
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#38543;&#26426;Hessian-&#21521;&#37327;&#20056;&#31215;&#19978;&#25311;&#21512;Hessian&#25110;&#20854;&#36870;&#65292;&#25581;&#31034;&#20102;&#19981;&#21516;Hessian&#25311;&#21512;&#26041;&#27861;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#29305;&#23450;&#26446;&#32676;&#19978;&#30340;Hessian&#25311;&#21512;&#38382;&#39064;&#22312;&#36731;&#24494;&#26465;&#20214;&#19979;&#26159;&#24378;&#20984;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#38543;&#26426;Hessian-&#21521;&#37327;&#20056;&#31215;&#19978;&#25311;&#21512;Hessian&#25110;&#20854;&#36870;&#12290;&#20351;&#29992;&#20102;&#19968;&#20010;Hessian&#25311;&#21512;&#20934;&#21017;&#65292;&#21487;&#29992;&#20110;&#25512;&#23548;&#22823;&#37096;&#20998;&#24120;&#29992;&#26041;&#27861;&#65292;&#22914;BFGS&#12289;&#39640;&#26031;&#29275;&#39039;&#12289;AdaGrad&#31561;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#19981;&#21516;Hessian&#25311;&#21512;&#26041;&#27861;&#30340;&#19981;&#21516;&#25910;&#25947;&#36895;&#29575;&#65292;&#20363;&#22914;&#65292;&#22312;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#20013;&#30340;&#26799;&#24230;&#19979;&#38477;&#30340;&#27425;&#32447;&#24615;&#36895;&#29575;&#21644;&#23545;&#31216;&#27491;&#23450;&#65288;SPL&#65289;&#30697;&#38453;&#21644;&#26576;&#20123;&#26446;&#32676;&#19978;&#30340;&#26799;&#24230;&#19979;&#38477;&#30340;&#32447;&#24615;&#36895;&#29575;&#12290;&#22312;&#29305;&#23450;&#19988;&#36275;&#22815;&#19968;&#33324;&#30340;&#26446;&#32676;&#19978;&#30340;Hessian&#25311;&#21512;&#38382;&#39064;&#22312;&#36731;&#24494;&#26465;&#20214;&#19979;&#34987;&#35777;&#26126;&#26159;&#24378;&#20984;&#30340;&#12290;&#20026;&#20102;&#30830;&#35748;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#19981;&#21516;&#35774;&#32622;&#19979;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#22914;&#26377;&#22122;&#22768;&#30340;Hessian-&#21521;&#37327;&#20056;&#31215;&#12289;&#26102;&#21464;&#30340;Hessians&#21644;&#20302;&#31934;&#24230;&#31639;&#26415;&#12290;&#36825;&#20123;&#21457;&#29616;&#23545;&#20381;&#36182;&#20110;&#38543;&#26426;&#20108;&#38454;&#20248;&#21270;&#30340;&#26041;&#27861;&#26159;&#26377;&#29992;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11858v1 Announce Type: cross  Abstract: This paper studies the fitting of Hessian or its inverse with stochastic Hessian-vector products. A Hessian fitting criterion, which can be used to derive most of the commonly used methods, e.g., BFGS, Gaussian-Newton, AdaGrad, etc., is used for the analysis. Our studies reveal different convergence rates for different Hessian fitting methods, e.g., sublinear rates for gradient descent in the Euclidean space and a commonly used closed-form solution, linear rates for gradient descent on the manifold of symmetric positive definite (SPL) matrices and certain Lie groups. The Hessian fitting problem is further shown to be strongly convex under mild conditions on a specific yet general enough Lie group. To confirm our analysis, these methods are tested under different settings like noisy Hessian-vector products, time varying Hessians, and low precision arithmetic. These findings are useful for stochastic second order optimizations that rely 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Local Immediate Error Compensated SGD&#65288;LIEC-SGD&#65289;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#21452;&#21521;&#21387;&#32553;&#21644;&#31934;&#24515;&#35774;&#35745;&#30340;&#34917;&#20607;&#31574;&#30053;&#26469;&#20943;&#23569;&#36890;&#20449;&#25104;&#26412;&#65292;&#23454;&#26102;&#34917;&#20607;&#23616;&#37096;&#21387;&#32553;&#35823;&#24046;&#65292;&#20248;&#20110;&#29616;&#26377;&#24037;&#20316;&#12290;</title><link>https://arxiv.org/abs/2402.11857</link><description>&lt;p&gt;
&#20855;&#26377;&#26412;&#22320;&#21363;&#26102;&#35823;&#24046;&#34917;&#20607;&#30340;&#36890;&#20449;&#39640;&#25928;&#20998;&#24067;&#24335;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Communication-Efficient Distributed Learning with Local Immediate Error Compensation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11857
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Local Immediate Error Compensated SGD&#65288;LIEC-SGD&#65289;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#21452;&#21521;&#21387;&#32553;&#21644;&#31934;&#24515;&#35774;&#35745;&#30340;&#34917;&#20607;&#31574;&#30053;&#26469;&#20943;&#23569;&#36890;&#20449;&#25104;&#26412;&#65292;&#23454;&#26102;&#34917;&#20607;&#23616;&#37096;&#21387;&#32553;&#35823;&#24046;&#65292;&#20248;&#20110;&#29616;&#26377;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26799;&#24230;&#21387;&#32553;&#19982;&#35823;&#24046;&#34917;&#20607;&#24050;&#32463;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#37325;&#35270;&#65292;&#30446;&#30340;&#26159;&#20943;&#23569;&#20998;&#24067;&#24335;&#23398;&#20064;&#20013;&#27785;&#37325;&#30340;&#36890;&#20449;&#24320;&#38144;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#21387;&#32553;&#26041;&#27861;&#35201;&#20040;&#22312;&#19968;&#20010;&#36845;&#20195;&#20013;&#21482;&#25191;&#34892;&#21333;&#21521;&#21387;&#32553;&#65292;&#36896;&#25104;&#36739;&#39640;&#30340;&#36890;&#20449;&#25104;&#26412;&#65292;&#35201;&#20040;&#22312;&#25910;&#25947;&#36895;&#24230;&#36739;&#24930;&#30340;&#24773;&#20917;&#19979;&#25191;&#34892;&#21452;&#21521;&#21387;&#32553;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#21452;&#21521;&#21387;&#32553;&#21644;&#31934;&#24515;&#35774;&#35745;&#30340;&#34917;&#20607;&#31574;&#30053;&#26469;&#31361;&#30772;&#19978;&#36848;&#29942;&#39048;&#30340;&#26412;&#22320;&#21363;&#26102;&#35823;&#24046;&#34917;&#20607;SGD&#65288;LIEC-SGD&#65289;&#20248;&#21270;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11857v1 Announce Type: new  Abstract: Gradient compression with error compensation has attracted significant attention with the target of reducing the heavy communication overhead in distributed learning. However, existing compression methods either perform only unidirectional compression in one iteration with higher communication cost, or bidirectional compression with slower convergence rate. In this work, we propose the Local Immediate Error Compensated SGD (LIEC-SGD) optimization algorithm to break the above bottlenecks based on bidirectional compression and carefully designed compensation approaches. Specifically, the bidirectional compression technique is to reduce the communication cost, and the compensation technique compensates the local compression error to the model update immediately while only maintaining the global error variable on the server throughout the iterations to boost its efficacy. Theoretically, we prove that LIEC-SGD is superior to previous works in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22686;&#24378;&#30340;&#22522;&#20110;&#25945;&#23398;&#23398;&#20064;&#20248;&#21270;&#21644;&#28784;&#29436;&#20248;&#21270;&#22120;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#25991;&#26412;&#29305;&#24449;&#36873;&#25321;&#21644;&#32858;&#31867;&#65292;&#26088;&#22312;&#35299;&#20915;&#29305;&#24449;&#36873;&#25321;&#20013;&#30340;&#23616;&#37096;&#26368;&#20248;&#38519;&#38449;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.11839</link><description>&lt;p&gt;
&#19968;&#20010;&#22686;&#24378;&#30340;&#22522;&#20110;&#25945;&#23398;&#23398;&#20064;&#20248;&#21270;&#65288;TLBO&#65289;&#19982;&#28784;&#29436;&#20248;&#21270;&#22120;&#65288;GWO&#65289;&#30340;&#25991;&#26412;&#29305;&#24449;&#36873;&#25321;&#21644;&#32858;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An enhanced Teaching-Learning-Based Optimization (TLBO) with Grey Wolf Optimizer (GWO) for text feature selection and clustering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11839
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22686;&#24378;&#30340;&#22522;&#20110;&#25945;&#23398;&#23398;&#20064;&#20248;&#21270;&#21644;&#28784;&#29436;&#20248;&#21270;&#22120;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#25991;&#26412;&#29305;&#24449;&#36873;&#25321;&#21644;&#32858;&#31867;&#65292;&#26088;&#22312;&#35299;&#20915;&#29305;&#24449;&#36873;&#25321;&#20013;&#30340;&#23616;&#37096;&#26368;&#20248;&#38519;&#38449;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#25991;&#26723;&#32858;&#31867;&#22312;&#32452;&#32455;&#21644;&#22788;&#29702;&#26085;&#30410;&#22686;&#22810;&#30340;&#25991;&#26412;&#25991;&#26723;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#22823;&#22411;&#25991;&#26412;&#25991;&#26723;&#20013;&#21253;&#21547;&#30340;&#26080;&#20449;&#24687;&#21644;&#20887;&#20313;&#29305;&#24449;&#20250;&#38477;&#20302;&#32858;&#31867;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#29305;&#24449;&#36873;&#25321;&#65288;FS&#65289;&#26159;&#19968;&#31181;&#29992;&#20110;&#21435;&#38500;&#36825;&#20123;&#29305;&#24449;&#30340;&#20247;&#25152;&#21608;&#30693;&#30340;&#25216;&#26415;&#12290;&#30001;&#20110;FS&#21487;&#20197;&#34987;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#65292;&#22240;&#27492;&#21508;&#31181;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#34987;&#29992;&#26469;&#35299;&#20915;&#23427;&#12290;&#25945;&#23398;&#23398;&#20064;&#20248;&#21270;&#65288;TLBO&#65289;&#26159;&#19968;&#31181;&#21463;&#30410;&#20110;&#36739;&#23569;&#21442;&#25968;&#21644;&#24555;&#36895;&#25910;&#25947;&#30340;&#26032;&#22411;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#12290;&#36890;&#36807;&#25552;&#20986;TLBO&#65292;&#28784;&#29436;&#20248;&#21270;&#22120;&#65288;GWO&#65289;&#21644;&#36951;&#20256;&#31639;&#27861;&#65288;GA&#65289;&#31639;&#23376;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#36807;&#28388;&#30340;FS&#31639;&#27861;&#65288;TLBO-GWO&#65289;&#12290;&#36873;&#25321;&#20102;&#20845;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#23558;TLBO-GWO&#19982;&#19977;&#20010;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11839v1 Announce Type: cross  Abstract: Text document clustering can play a vital role in organizing and handling the everincreasing number of text documents. Uninformative and redundant features included in large text documents reduce the effectiveness of the clustering algorithm. Feature selection (FS) is a well-known technique for removing these features. Since FS can be formulated as an optimization problem, various meta-heuristic algorithms have been employed to solve it. Teaching-Learning-Based Optimization (TLBO) is a novel meta-heuristic algorithm that benefits from the low number of parameters and fast convergence. A hybrid method can simultaneously benefit from the advantages of TLBO and tackle the possible entrapment in the local optimum. By proposing a hybrid of TLBO, Grey Wolf Optimizer (GWO), and Genetic Algorithm (GA) operators, this paper suggests a filter-based FS algorithm (TLBO-GWO). Six benchmark datasets are selected, and TLBO-GWO is compared with three 
&lt;/p&gt;</description></item><item><title>UniST&#26159;&#19968;&#31181;&#20026;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#35774;&#35745;&#30340;&#36890;&#29992;&#27169;&#22411;&#65292;&#36890;&#36807;&#28789;&#27963;&#24615;&#12289;&#26377;&#25928;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#20197;&#21450;&#20016;&#23500;&#30340;&#25513;&#30721;&#31574;&#30053;&#25104;&#21151;&#25429;&#25417;&#22797;&#26434;&#30340;&#26102;&#31354;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.11838</link><description>&lt;p&gt;
UniST&#65306;&#19968;&#31181;&#20026;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#35774;&#35745;&#30340;&#25552;&#31034;&#22686;&#24378;&#22411;&#36890;&#29992;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
UniST: A Prompt-Empowered Universal Model for Urban Spatio-Temporal Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11838
&lt;/p&gt;
&lt;p&gt;
UniST&#26159;&#19968;&#31181;&#20026;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#35774;&#35745;&#30340;&#36890;&#29992;&#27169;&#22411;&#65292;&#36890;&#36807;&#28789;&#27963;&#24615;&#12289;&#26377;&#25928;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#20197;&#21450;&#20016;&#23500;&#30340;&#25513;&#30721;&#31574;&#30053;&#25104;&#21151;&#25429;&#25417;&#22797;&#26434;&#30340;&#26102;&#31354;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11838v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#30340; &#25688;&#35201;&#65306;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#23545;&#20110;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#65292;&#20363;&#22914;&#20132;&#36890;&#31649;&#29702;&#12289;&#36164;&#28304;&#20248;&#21270;&#21644;&#22478;&#24066;&#35268;&#21010;&#12290;&#23613;&#31649;&#33258;&#28982;&#35821;&#35328;&#30340;&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#31361;&#30772;&#65292;&#20854;&#20013;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;&#21487;&#20197;&#22788;&#29702;&#36328;&#22810;&#20010;&#39046;&#22495;&#30340;&#22810;&#20010;&#20219;&#21153;&#65292;&#20294;&#22478;&#24066;&#26102;&#31354;&#24314;&#27169;&#33853;&#21518;&#12290;&#29616;&#26377;&#30340;&#22478;&#24066;&#39044;&#27979;&#26041;&#27861;&#36890;&#24120;&#38024;&#23545;&#29305;&#23450;&#30340;&#26102;&#31354;&#22330;&#26223;&#36827;&#34892;&#23450;&#21046;&#65292;&#38656;&#35201;&#29305;&#23450;&#20219;&#21153;&#30340;&#27169;&#22411;&#35774;&#35745;&#21644;&#22823;&#37327;&#22495;&#20869;&#35757;&#32451;&#25968;&#25454;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#30340;&#36890;&#29992;&#27169;&#22411;UniST&#12290;&#20511;&#37492;&#33258;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;UniST&#36890;&#36807;&#20197;&#19979;&#26041;&#24335;&#21462;&#24471;&#25104;&#21151;&#65306;(i) &#23545;&#19981;&#21516;&#31354;&#38388;&#26102;&#38388;&#25968;&#25454;&#29305;&#24449;&#30340;&#28789;&#27963;&#24615;&#65292;(ii) &#26377;&#25928;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#65292;&#37319;&#29992;&#31934;&#24515;&#35774;&#35745;&#30340;&#25513;&#30721;&#31574;&#30053;&#26469;&#25429;&#25417;&#22797;&#26434;&#30340;&#31354;&#38388;&#26102;&#38388;&#20851;&#31995;&#65292;(iii) &#26102;&#31354;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11838v1 Announce Type: new  Abstract: Urban spatio-temporal prediction is crucial for informed decision-making, such as transportation management, resource optimization, and urban planning. Although pretrained foundation models for natural languages have experienced remarkable breakthroughs, wherein one general-purpose model can tackle multiple tasks across various domains, urban spatio-temporal modeling lags behind. Existing approaches for urban prediction are usually tailored for specific spatio-temporal scenarios, requiring task-specific model designs and extensive in-domain training data. In this work, we propose a universal model, UniST, for urban spatio-temporal prediction. Drawing inspiration from large language models, UniST achieves success through: (i) flexibility towards diverse spatio-temporal data characteristics, (ii) effective generative pre-training with elaborated masking strategies to capture complex spatio-temporal relationships, (iii) spatio-temporal know
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#20027;&#24341;&#23548;&#30340;GSR&#26694;&#26550;&#65288;SG-GSR&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#34987;&#25915;&#20987;&#22270;&#20013;&#21457;&#29616;&#30340;&#24178;&#20928;&#23376;&#22270;&#65292;&#24182;&#25552;&#20986;&#20102;&#22270;&#22686;&#24378;&#21644;&#20998;&#32452;&#35757;&#32451;&#31574;&#30053;&#65292;&#20197;&#24212;&#23545;&#29616;&#26377;GSR&#26041;&#27861;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#21463;&#38480;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.11837</link><description>&lt;p&gt;
&#33258;&#20027;&#24341;&#23548;&#30340;&#31283;&#20581;&#22270;&#32467;&#26500;&#32454;&#21270;
&lt;/p&gt;
&lt;p&gt;
Self-Guided Robust Graph Structure Refinement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11837
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#20027;&#24341;&#23548;&#30340;GSR&#26694;&#26550;&#65288;SG-GSR&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#34987;&#25915;&#20987;&#22270;&#20013;&#21457;&#29616;&#30340;&#24178;&#20928;&#23376;&#22270;&#65292;&#24182;&#25552;&#20986;&#20102;&#22270;&#22686;&#24378;&#21644;&#20998;&#32452;&#35757;&#32451;&#31574;&#30053;&#65292;&#20197;&#24212;&#23545;&#29616;&#26377;GSR&#26041;&#27861;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#21463;&#38480;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30740;&#31350;&#21457;&#29616;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;&#20026;&#20102;&#25269;&#24481;&#27492;&#31867;&#25915;&#20987;&#65292;&#31283;&#20581;&#22270;&#32467;&#26500;&#20462;&#27491;&#65288;GSR&#65289;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#33410;&#28857;&#29305;&#24449;&#12289;&#22270;&#32467;&#26500;&#25110;&#22806;&#37096;&#20449;&#24687;&#26469;&#26368;&#23567;&#21270;&#23545;&#25239;&#24615;&#36793;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;GSR&#26041;&#27861;&#21463;&#21040;&#29421;&#31364;&#20551;&#35774;&#30340;&#38480;&#21046;&#65292;&#27604;&#22914;&#20551;&#35774;&#24178;&#20928;&#30340;&#33410;&#28857;&#29305;&#24449;&#12289;&#36866;&#24230;&#30340;&#32467;&#26500;&#25915;&#20987;&#20197;&#21450;&#21487;&#29992;&#30340;&#22806;&#37096;&#24178;&#20928;&#22270;&#65292;&#23548;&#33268;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#24212;&#29992;&#21463;&#38480;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#20027;&#24341;&#23548;&#30340;GSR&#26694;&#26550;&#65288;SG-GSR&#65289;&#65292;&#20854;&#21033;&#29992;&#32473;&#23450;&#34987;&#25915;&#20987;&#22270;&#20013;&#21457;&#29616;&#30340;&#24178;&#20928;&#23376;&#22270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#22686;&#24378;&#21644;&#20998;&#32452;&#35757;&#32451;&#31574;&#30053;&#26469;&#22788;&#29702;&#22312;&#24178;&#20928;&#23376;&#22270;&#25552;&#21462;&#20013;&#30340;&#20004;&#20010;&#25216;&#26415;&#25361;&#25112;&#65306;1&#65289;&#32467;&#26500;&#20449;&#24687;&#30340;&#20002;&#22833;&#65292;2&#65289;&#33410;&#28857;&#24230;&#20998;&#24067;&#30340;&#19981;&#24179;&#34913;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11837v1 Announce Type: new  Abstract: Recent studies have revealed that GNNs are vulnerable to adversarial attacks. To defend against such attacks, robust graph structure refinement (GSR) methods aim at minimizing the effect of adversarial edges based on node features, graph structure, or external information. However, we have discovered that existing GSR methods are limited by narrowassumptions, such as assuming clean node features, moderate structural attacks, and the availability of external clean graphs, resulting in the restricted applicability in real-world scenarios. In this paper, we propose a self-guided GSR framework (SG-GSR), which utilizes a clean sub-graph found within the given attacked graph itself. Furthermore, we propose a novel graph augmentation and a group-training strategy to handle the two technical challenges in the clean sub-graph extraction: 1) loss of structural information, and 2) imbalanced node degree distribution. Extensive experiments demonstra
&lt;/p&gt;</description></item><item><title>ABCs&#31639;&#27861;&#32467;&#21512;&#20102;Boltzmann Q-learning&#21644;&#21453;&#20107;&#23454;&#36951;&#25022;&#26368;&#23567;&#21270;&#65292;&#36890;&#36807;&#27979;&#37327;&#29615;&#22659;&#31283;&#23450;&#24615;&#33258;&#36866;&#24212;&#36873;&#25321;&#25506;&#32034;&#27604;&#20363;&#65292;&#22312;&#21333;&#19968;&#26234;&#33021;&#20307;&#21644;&#22810;&#26234;&#33021;&#20307;&#39046;&#22495;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2402.11835</link><description>&lt;p&gt;
&#31616;&#21333;&#22914;ABC&#65306;&#32479;&#19968;Boltzmann Q-Learning&#21644;&#21453;&#20107;&#23454;&#36951;&#25022;&#26368;&#23567;&#21270;
&lt;/p&gt;
&lt;p&gt;
Easy as ABCs: Unifying Boltzmann Q-Learning and Counterfactual Regret Minimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11835
&lt;/p&gt;
&lt;p&gt;
ABCs&#31639;&#27861;&#32467;&#21512;&#20102;Boltzmann Q-learning&#21644;&#21453;&#20107;&#23454;&#36951;&#25022;&#26368;&#23567;&#21270;&#65292;&#36890;&#36807;&#27979;&#37327;&#29615;&#22659;&#31283;&#23450;&#24615;&#33258;&#36866;&#24212;&#36873;&#25321;&#25506;&#32034;&#27604;&#20363;&#65292;&#22312;&#21333;&#19968;&#26234;&#33021;&#20307;&#21644;&#22810;&#26234;&#33021;&#20307;&#39046;&#22495;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;ABCs&#65288;&#36890;&#36807;&#23376;&#31449;&#28857;&#31283;&#23450;&#24615;&#36827;&#34892;&#33258;&#36866;&#24212;&#20998;&#25903;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;Boltzmann Q-learning&#65288;&#19968;&#31181;&#32463;&#20856;&#30340;&#21333;&#19968;&#26234;&#33021;&#20307;&#39046;&#22495;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65289;&#21644;&#21453;&#20107;&#23454;&#36951;&#25022;&#26368;&#23567;&#21270;&#65288;CFR&#65289;&#30340;&#21452;&#36194;&#31639;&#27861;&#12290;ABCs&#36890;&#36807;&#27979;&#37327;&#29615;&#22659;&#22870;&#21169;&#21644;&#36716;&#25442;&#21160;&#24577;&#30340;&#31283;&#23450;&#24615;&#65292;&#33258;&#36866;&#24212;&#22320;&#36873;&#25321;&#27599;&#27425;&#36845;&#20195;&#35201;&#25506;&#32034;&#29615;&#22659;&#30340;&#27604;&#20363;&#12290;&#22312;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#65292;ABCs&#19982;BQL&#30456;&#27604;&#26368;&#22810;&#21482;&#24930;&#19968;&#20010;O&#65288;A&#65289;&#22240;&#23376;&#23601;&#33021;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#65292;&#20854;&#20013;A&#26159;&#29615;&#22659;&#20013;&#30340;&#21160;&#20316;&#25968;&#37327;&#12290;&#22312;&#20004;&#20154;&#38646;&#21644;&#21338;&#24328;&#20013;&#65292;ABCs&#26377;&#20445;&#35777;&#25910;&#25947;&#21040;&#32435;&#20160;&#22343;&#34913;&#65288;&#20551;&#35774;&#21487;&#20197;&#35775;&#38382;&#19968;&#20010;&#23436;&#32654;&#30340;&#31070;&#35861;&#26469;&#26816;&#27979;&#31283;&#23450;&#24615;&#65289;&#65292;&#32780;BQL&#27809;&#26377;&#36825;&#26679;&#30340;&#20445;&#35777;&#12290;&#20174;&#32463;&#39564;&#19978;&#30475;&#65292;ABCs&#22312;OpenSpiel&#28216;&#25103;&#24211;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#24378;&#21170;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11835v1 Announce Type: new  Abstract: We propose ABCs (Adaptive Branching through Child stationarity), a best-of-both-worlds algorithm combining Boltzmann Q-learning (BQL), a classic reinforcement learning algorithm for single-agent domains, and counterfactual regret minimization (CFR), a central algorithm for learning in multi-agent domains. ABCs adaptively chooses what fraction of the environment to explore each iteration by measuring the stationarity of the environment's reward and transition dynamics. In Markov decision processes, ABCs converges to the optimal policy with at most an O(A) factor slowdown compared to BQL, where A is the number of actions in the environment. In two-player zero-sum games, ABCs is guaranteed to converge to a Nash equilibrium (assuming access to a perfect oracle for detecting stationarity), while BQL has no such guarantees. Empirically, ABCs demonstrates strong performance when benchmarked across environments drawn from the OpenSpiel game libr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#31995;&#32479;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#22270;&#24418;&#21484;&#22238;&#30340;&#20934;&#30830;&#24615;&#21644;&#20559;&#35265;&#24494;&#32467;&#26500;&#65292;&#25506;&#35752;&#20102;&#23427;&#20204;&#19982;&#20154;&#31867;&#30340;&#24322;&#21516;&#20197;&#21450;&#23545;&#20854;&#20182;&#22270;&#24418;&#25512;&#29702;&#20219;&#21153;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.11821</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#22270;&#24418;&#21484;&#22238;&#30340;&#24494;&#32467;&#26500;&#21644;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Microstructures and Accuracy of Graph Recall by Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11821
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#31995;&#32479;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#22270;&#24418;&#21484;&#22238;&#30340;&#20934;&#30830;&#24615;&#21644;&#20559;&#35265;&#24494;&#32467;&#26500;&#65292;&#25506;&#35752;&#20102;&#23427;&#20204;&#19982;&#20154;&#31867;&#30340;&#24322;&#21516;&#20197;&#21450;&#23545;&#20854;&#20182;&#22270;&#24418;&#25512;&#29702;&#20219;&#21153;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#24418;&#25968;&#25454;&#23545;&#35768;&#22810;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#20854;&#20013;&#24456;&#22810;&#25968;&#25454;&#20197;&#25991;&#26412;&#26684;&#24335;&#25551;&#36848;&#20851;&#31995;&#12290;&#22240;&#27492;&#65292;&#20934;&#30830;&#22320;&#21484;&#22238;&#21644;&#32534;&#30721;&#20808;&#21069;&#25991;&#26412;&#20013;&#25551;&#36848;&#30340;&#22270;&#24418;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#38656;&#35201;&#23637;&#31034;&#30340;&#22522;&#26412;&#20294;&#20851;&#38190;&#33021;&#21147;&#65292;&#20197;&#25191;&#34892;&#28041;&#21450;&#22270;&#24418;&#32467;&#26500;&#20449;&#24687;&#30340;&#25512;&#29702;&#20219;&#21153;&#12290;&#20154;&#31867;&#22312;&#22270;&#24418;&#21484;&#22238;&#26041;&#38754;&#30340;&#34920;&#29616;&#24050;&#34987;&#35748;&#30693;&#31185;&#23398;&#23478;&#30740;&#31350;&#20102;&#20960;&#21313;&#24180;&#65292;&#21457;&#29616;&#20854;&#32463;&#24120;&#21576;&#29616;&#19982;&#20154;&#31867;&#22788;&#29702;&#31038;&#20250;&#20851;&#31995;&#19968;&#33268;&#30340;&#26576;&#20123;&#32467;&#26500;&#24615;&#20559;&#35265;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#25105;&#20204;&#24456;&#23569;&#20102;&#35299;LLMs&#22312;&#31867;&#20284;&#22270;&#24418;&#21484;&#22238;&#20219;&#21153;&#20013;&#30340;&#34892;&#20026;&#65306;&#23427;&#20204;&#21484;&#22238;&#30340;&#22270;&#24418;&#26159;&#21542;&#20063;&#21576;&#29616;&#26576;&#20123;&#20559;&#35265;&#27169;&#24335;&#65292;&#22914;&#26524;&#26159;&#65292;&#23427;&#20204;&#19982;&#20154;&#31867;&#30340;&#34920;&#29616;&#26377;&#20309;&#19981;&#21516;&#24182;&#22914;&#20309;&#24433;&#21709;&#20854;&#20182;&#22270;&#24418;&#25512;&#29702;&#20219;&#21153;&#65311;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#31532;&#19968;&#27425;&#23545;LLMs&#36827;&#34892;&#22270;&#24418;&#21484;&#22238;&#30340;&#31995;&#32479;&#30740;&#31350;&#65292;&#30740;&#31350;&#20854;&#20934;&#30830;&#24615;&#21644;&#20559;&#35265;&#24494;&#32467;&#26500;&#65288;&#23616;&#37096;&#32467;&#26500;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11821v1 Announce Type: cross  Abstract: Graphs data is crucial for many applications, and much of it exists in the relations described in textual format. As a result, being able to accurately recall and encode a graph described in earlier text is a basic yet pivotal ability that LLMs need to demonstrate if they are to perform reasoning tasks that involve graph-structured information. Human performance at graph recall by has been studied by cognitive scientists for decades, and has been found to often exhibit certain structural patterns of bias that align with human handling of social relationships. To date, however, we know little about how LLMs behave in analogous graph recall tasks: do their recalled graphs also exhibit certain biased patterns, and if so, how do they compare with humans and affect other graph reasoning tasks? In this work, we perform the first systematical study of graph recall by LLMs, investigating the accuracy and biased microstructures (local structura
&lt;/p&gt;</description></item><item><title>&#24320;&#21457;&#20102;&#19968;&#31181;&#22810;&#38454;&#23545;&#27604;&#23398;&#20064;&#65288;MCL&#65289;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#29305;&#24449;&#25233;&#21046;&#38382;&#39064;&#65292;&#24182;&#30830;&#20445;&#27169;&#22411;&#23398;&#20064;&#20840;&#38754;&#30340;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2402.11816</link><description>&lt;p&gt;
&#36991;&#20813;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#29305;&#24449;&#25233;&#21046;&#65306;&#23398;&#20064;&#20197;&#21069;&#26410;&#26366;&#23398;&#21040;&#30340;&#20869;&#23481;
&lt;/p&gt;
&lt;p&gt;
Avoiding Feature Suppression in Contrastive Learning: Learning What Has Not Been Learned Before
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11816
&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20102;&#19968;&#31181;&#22810;&#38454;&#23545;&#27604;&#23398;&#20064;&#65288;MCL&#65289;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#29305;&#24449;&#25233;&#21046;&#38382;&#39064;&#65292;&#24182;&#30830;&#20445;&#27169;&#22411;&#23398;&#20064;&#20840;&#38754;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#20174;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#33719;&#21462;&#39640;&#36136;&#37327;&#34920;&#31034;&#30340;&#24378;&#22823;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#22312;&#26631;&#20934;&#23545;&#27604;&#23398;&#20064;&#65288;&#22914;SimCLR&#12289;CLIP&#20013;&#65289;&#20013;&#21457;&#29616;&#20102;&#29305;&#24449;&#25233;&#21046;&#65306;&#22312;&#21333;&#20010;&#31471;&#21040;&#31471;&#35757;&#32451;&#38454;&#27573;&#65292;&#23545;&#27604;&#27169;&#22411;&#20165;&#25429;&#33719;&#23545;&#27604;&#35266;&#28857;&#20043;&#38388;&#30340;&#19968;&#37096;&#20998;&#20849;&#20139;&#20449;&#24687;&#65292;&#32780;&#24573;&#30053;&#20102;&#20854;&#20182;&#28508;&#22312;&#26377;&#29992;&#30340;&#20449;&#24687;&#12290;&#20855;&#26377;&#29305;&#24449;&#25233;&#21046;&#65292;&#23545;&#27604;&#27169;&#22411;&#36890;&#24120;&#26080;&#27861;&#23398;&#20064;&#36275;&#22815;&#36866;&#29992;&#20110;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#30340;&#34920;&#31034;&#12290;&#20026;&#20102;&#20943;&#36731;&#29305;&#24449;&#25233;&#21046;&#38382;&#39064;&#24182;&#30830;&#20445;&#23545;&#27604;&#27169;&#22411;&#23398;&#20064;&#20840;&#38754;&#30340;&#34920;&#31034;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#38454;&#23545;&#27604;&#23398;&#20064;&#65288;MCL&#65289;&#26694;&#26550;&#12290;&#19982;&#36890;&#24120;&#20250;&#23548;&#33268;&#29305;&#24449;&#25233;&#21046;&#30340;&#26631;&#20934;&#23545;&#27604;&#23398;&#20064;&#19981;&#21516;&#65292;MCL&#36880;&#28176;&#23398;&#20064;&#20197;&#21069;&#26410;&#25506;&#32034;&#36807;&#30340;&#26032;&#29305;&#24449;&#65292;&#21516;&#26102;&#20445;&#25345;&#24050;&#32463;&#23398;&#21040;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11816v1 Announce Type: cross  Abstract: Self-Supervised contrastive learning has emerged as a powerful method for obtaining high-quality representations from unlabeled data. However, feature suppression has recently been identified in standard contrastive learning ($e.g.$, SimCLR, CLIP): in a single end-to-end training stage, the contrastive model captures only parts of the shared information across contrasting views, while ignore the other potentially useful information. With feature suppression, contrastive models often fail to learn sufficient representations capable for various downstream tasks. To mitigate the feature suppression problem and ensure the contrastive model to learn comprehensive representations, we develop a novel Multistage Contrastive Learning (MCL) framework. Unlike standard contrastive learning that often result in feature suppression, MCL progressively learn new features that have not been explored in the previous stage, while maintaining the well-lea
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#21333;&#19968;&#27169;&#22411;&#65292;&#29992;&#36739;&#23569;&#30340;&#21442;&#25968;&#23454;&#29616;&#19982;&#22522;&#32447;&#30456;&#24403;&#30340;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#24615;&#33021;</title><link>https://arxiv.org/abs/2402.11815</link><description>&lt;p&gt;
HU&#22312;SemEval-2024&#20219;&#21153;8A&#20013;&#30340;&#34920;&#29616;&#65306;&#23545;&#27604;&#23398;&#20064;&#33021;&#21542;&#23398;&#20064;&#23884;&#20837;&#20197;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#65311;
&lt;/p&gt;
&lt;p&gt;
HU at SemEval-2024 Task 8A: Can Contrastive Learning Learn Embeddings to Detect Machine-Generated Text?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11815
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#21333;&#19968;&#27169;&#22411;&#65292;&#29992;&#36739;&#23569;&#30340;&#21442;&#25968;&#23454;&#29616;&#19982;&#22522;&#32447;&#30456;&#24403;&#30340;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25551;&#36848;&#20102;&#25105;&#20204;&#20026;SemEval-2024&#20219;&#21153;8&#8220;&#22810;&#29983;&#25104;&#22120;&#12289;&#22810;&#39046;&#22495;&#21644;&#22810;&#35821;&#35328;&#40657;&#21283;&#23376;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#8221;&#24320;&#21457;&#30340;&#31995;&#32479;&#12290;&#30001;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#34394;&#20551;&#25991;&#26412;&#29983;&#25104;&#12289;&#32593;&#32476;&#38035;&#40060;&#12289;&#32771;&#35797;&#20316;&#24330;&#29978;&#33267;&#25220;&#34989;&#29256;&#26435;&#26448;&#26009;&#20013;&#30340;&#20351;&#29992;&#65292;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#19968;&#30452;&#26159;&#20027;&#35201;&#20851;&#27880;&#30340;&#38382;&#39064;&#20043;&#19968;&#12290;&#35768;&#22810;&#31995;&#32479;&#24050;&#32463;&#34987;&#24320;&#21457;&#29992;&#20110;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31995;&#32479;&#20013;&#30340;&#22823;&#37096;&#20998;&#20381;&#36182;&#20110;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#19981;&#20999;&#23454;&#38469;&#30340;&#38480;&#21046;&#65292;&#22240;&#20026;&#36890;&#24120;&#19981;&#21487;&#33021;&#30693;&#36947;&#29992;&#25143;&#29992;&#20110;&#25991;&#26412;&#29983;&#25104;&#30340;&#20855;&#20307;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#21333;&#19968;&#27169;&#22411;&#65292;&#20854;&#20351;&#29992;&#22522;&#32447;&#21442;&#25968;&#30340;&#22823;&#32422;40%&#65288;149M&#27604;355M&#65289;&#65292;&#20294;&#22312;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20102;&#21487;&#27604;&#30340;&#24615;&#33021;&#65288;&#22312;137&#20010;&#21442;&#19982;&#32773;&#20013;&#25490;&#21517;&#31532;21&#65289;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#21457;&#29616;&#26159;&#65292;&#21363;&#20351;&#27809;&#26377;&#22810;&#20010;&#27169;&#22411;&#30340;&#38598;&#25104;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11815v1 Announce Type: cross  Abstract: This paper describes our system developed for SemEval-2024 Task 8, "Multigenerator, Multidomain, and Multilingual Black-Box Machine-Generated Text Detection." Machine-generated texts have been one of the main concerns due to the use of large language models (LLM) in fake text generation, phishing, cheating in exams, or even plagiarizing copyright materials. A lot of systems have been developed to detect machine-generated text. Nonetheless, the majority of these systems rely on the text-generating model, a limitation that is impractical in real-world scenarios, as it's often impossible to know which specific model the user has used for text generation. In this work, we propose a single model based on contrastive learning, which uses ~40% of the baseline's parameters (149M vs. 355M) but shows a comparable performance on the test dataset (21st out of 137 participants). Our key finding is that even without an ensemble of multiple models, a
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Smart Parallel Auto-Correct Decoding (SPACE)&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#21322;&#33258;&#22238;&#24402;&#25512;&#29702;&#21644;&#29468;&#27979;&#35299;&#30721;&#65292;&#23454;&#29616;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#21152;&#36895;&#21644;&#24182;&#34892;&#29983;&#25104;&#39564;&#35777;&#20196;&#29260;&#30340;&#21151;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.11809</link><description>&lt;p&gt;
&#26234;&#33021;&#24182;&#34892;&#33258;&#21160;&#32416;&#38169;&#35299;&#30721;&#65306;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Generation Meets Verification: Accelerating Large Language Model Inference with Smart Parallel Auto-Correct Decoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11809
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Smart Parallel Auto-Correct Decoding (SPACE)&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#21322;&#33258;&#22238;&#24402;&#25512;&#29702;&#21644;&#29468;&#27979;&#35299;&#30721;&#65292;&#23454;&#29616;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#21152;&#36895;&#21644;&#24182;&#34892;&#29983;&#25104;&#39564;&#35777;&#20196;&#29260;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#21152;&#36895;&#25317;&#26377;&#25968;&#21313;&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#36895;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#26234;&#33021;&#24182;&#34892;&#33258;&#21160;&#32416;&#38169;&#35299;&#30721;&#8221;&#65288;SPACE&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#26088;&#22312;&#23454;&#29616;LLMs&#30340;&#26080;&#25439;&#21152;&#36895;&#12290;&#36890;&#36807;&#38598;&#25104;&#21322;&#33258;&#22238;&#24402;&#25512;&#29702;&#21644;&#29468;&#27979;&#35299;&#30721;&#33021;&#21147;&#65292;SPACE&#29420;&#29305;&#22320;&#20351;&#33258;&#22238;&#24402;LLMs&#33021;&#22815;&#24182;&#34892;&#29983;&#25104;&#21644;&#39564;&#35777;&#20196;&#29260;&#12290;&#36825;&#26159;&#36890;&#36807;&#19987;&#38376;&#30340;&#21322;&#33258;&#22238;&#24402;&#30417;&#30563;&#24494;&#35843;&#36807;&#31243;&#23454;&#29616;&#30340;&#65292;&#35813;&#36807;&#31243;&#20351;&#29616;&#26377;LLMs&#20855;&#26377;&#21516;&#26102;&#39044;&#27979;&#22810;&#20010;&#20196;&#29260;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#19968;&#31181;&#33258;&#21160;&#32416;&#38169;&#35299;&#30721;&#31639;&#27861;&#20419;&#36827;&#20102;&#21333;&#20010;&#27169;&#22411;&#35843;&#29992;&#20869;&#20196;&#29260;&#24207;&#21015;&#30340;&#21516;&#26102;&#29983;&#25104;&#21644;&#39564;&#35777;&#12290;&#36890;&#36807;&#22312;&#19968;&#31995;&#21015;LLMs&#19978;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#35777;&#26126;&#65292;SPACE&#22312;HumanEval-X&#19978;&#34920;&#29616;&#20986;2.7&#20493;&#33267;4.0&#20493;&#30340;&#25512;&#29702;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11809v1 Announce Type: cross  Abstract: This research aims to accelerate the inference speed of large language models (LLMs) with billions of parameters. We propose \textbf{S}mart \textbf{P}arallel \textbf{A}uto-\textbf{C}orrect d\textbf{E}coding (SPACE), an innovative approach designed for achieving lossless acceleration of LLMs. By integrating semi-autoregressive inference and speculative decoding capabilities, SPACE uniquely enables autoregressive LLMs to parallelize token generation and verification. This is realized through a specialized semi-autoregressive supervised fine-tuning process that equips existing LLMs with the ability to simultaneously predict multiple tokens. Additionally, an auto-correct decoding algorithm facilitates the simultaneous generation and verification of token sequences within a single model invocation. Through extensive experiments on a range of LLMs, SPACE has demonstrated inference speedup ranging from 2.7x-4.0x on HumanEval-X while maintaini
&lt;/p&gt;</description></item><item><title>&#24310;&#36831;&#26356;&#26032;&#30340;&#38543;&#26426;&#36924;&#36817;&#26041;&#26696;&#22312;&#26102;&#38388;&#21464;&#21270;&#26377;&#30028;&#24310;&#36831;&#19979;&#65292;&#20445;&#35777;&#20102;&#27599;&#27425;&#36845;&#20195;&#24555;&#36895;&#25910;&#25947;&#21040;&#22266;&#23450;&#28857;&#21608;&#22260;&#30340;&#29699;&#20307;&#65292;&#30028;&#38480;&#20381;&#36182;&#20110;&#26368;&#22823;&#24310;&#36831;&#21644;&#28151;&#21512;&#26102;&#38388;&#12290;</title><link>https://arxiv.org/abs/2402.11800</link><description>&lt;p&gt;
&#20855;&#26377;&#24310;&#36831;&#26356;&#26032;&#30340;&#38543;&#26426;&#36924;&#36817;&#65306;&#39532;&#23572;&#31185;&#22827;&#37319;&#26679;&#19979;&#30340;&#26377;&#38480;&#26102;&#38388;&#36895;&#29575;
&lt;/p&gt;
&lt;p&gt;
Stochastic Approximation with Delayed Updates: Finite-Time Rates under Markovian Sampling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11800
&lt;/p&gt;
&lt;p&gt;
&#24310;&#36831;&#26356;&#26032;&#30340;&#38543;&#26426;&#36924;&#36817;&#26041;&#26696;&#22312;&#26102;&#38388;&#21464;&#21270;&#26377;&#30028;&#24310;&#36831;&#19979;&#65292;&#20445;&#35777;&#20102;&#27599;&#27425;&#36845;&#20195;&#24555;&#36895;&#25910;&#25947;&#21040;&#22266;&#23450;&#28857;&#21608;&#22260;&#30340;&#29699;&#20307;&#65292;&#30028;&#38480;&#20381;&#36182;&#20110;&#26368;&#22823;&#24310;&#36831;&#21644;&#28151;&#21512;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#22823;&#35268;&#27169;&#21644;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#39532;&#23572;&#31185;&#22827;&#37319;&#26679;&#19979;&#20855;&#26377;&#24310;&#36831;&#26356;&#26032;&#30340;&#38543;&#26426;&#36924;&#36817;&#65288;SA&#65289;&#26041;&#26696;&#30340;&#38750;&#28176;&#36817;&#24615;&#33021;&#12290;&#34429;&#28982;&#24310;&#36831;&#30340;&#24433;&#21709;&#22312;&#20248;&#21270;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#23427;&#20204;&#19982;&#24213;&#23618;&#39532;&#23572;&#31185;&#22827;&#36807;&#31243;&#30456;&#20114;&#20316;&#29992;&#20197;&#22609;&#36896;SA&#30340;&#26377;&#38480;&#26102;&#38388;&#24615;&#33021;&#30340;&#26041;&#24335;&#20173;&#28982;&#19981;&#22826;&#28165;&#26970;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#20027;&#35201;&#36129;&#29486;&#26159;&#35777;&#26126;&#22312;&#26102;&#38388;&#21464;&#21270;&#26377;&#30028;&#24310;&#36831;&#19979;&#65292;&#24310;&#36831;&#30340;SA&#26356;&#26032;&#35268;&#21017;&#30830;&#20445;&#26368;&#21518;&#36845;&#20195;&#25910;&#25947;&#21040;SA&#36816;&#31639;&#31526;&#22266;&#23450;&#28857;&#21608;&#22260;&#30340;&#29699;&#20307;&#20855;&#26377;&#25351;&#25968;&#24555;&#36895;&#30340;&#36895;&#24230;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#30028;&#38480;&#22312;&#20381;&#36182;&#20110;&#26368;&#22823;&#24310;&#36831;$\tau_{max}$&#21644;&#28151;&#21512;&#26102;&#38388;$\tau_{mix}$&#26041;&#38754;&#26159;\emph{&#32039;&#33268;&#30340;}&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#32039;&#23494;&#30028;&#38480;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24402;&#32435;&#35777;&#26126;&#25216;&#26415;&#65292;&#19982;&#21508;&#31181;&#29616;&#26377;&#24310;&#36831;&#20248;&#21270;&#20998;&#26512;&#19981;&#21516;&#65292;&#23427;&#20381;&#36182;&#20110;&#24314;&#31435;&#26410;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11800v1 Announce Type: cross  Abstract: Motivated by applications in large-scale and multi-agent reinforcement learning, we study the non-asymptotic performance of stochastic approximation (SA) schemes with delayed updates under Markovian sampling. While the effect of delays has been extensively studied for optimization, the manner in which they interact with the underlying Markov process to shape the finite-time performance of SA remains poorly understood. In this context, our first main contribution is to show that under time-varying bounded delays, the delayed SA update rule guarantees exponentially fast convergence of the \emph{last iterate} to a ball around the SA operator's fixed point. Notably, our bound is \emph{tight} in its dependence on both the maximum delay $\tau_{max}$, and the mixing time $\tau_{mix}$. To achieve this tight bound, we develop a novel inductive proof technique that, unlike various existing delayed-optimization analyses, relies on establishing un
&lt;/p&gt;</description></item><item><title>&#21457;&#29616;&#28145;&#23618;ReLU&#32593;&#32476;&#34920;&#29616;&#20986;&#36807;&#24230;&#27867;&#21270;&#29616;&#35937;&#65292;&#21033;&#29992;&#36825;&#19968;&#29305;&#24615;&#35774;&#35745;&#20102;&#8220;&#29983;&#25104;&#19975;&#33457;&#31570;&#32593;&#32476;&#8221;&#65292;&#36890;&#36807;&#36882;&#24402;&#26144;&#23556;&#38543;&#26426;&#36755;&#20837;&#22122;&#22768;&#29983;&#25104;&#26679;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.11793</link><description>&lt;p&gt;
&#29983;&#25104;&#19975;&#33457;&#31570;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Generative Kaleidoscopic Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11793
&lt;/p&gt;
&lt;p&gt;
&#21457;&#29616;&#28145;&#23618;ReLU&#32593;&#32476;&#34920;&#29616;&#20986;&#36807;&#24230;&#27867;&#21270;&#29616;&#35937;&#65292;&#21033;&#29992;&#36825;&#19968;&#29305;&#24615;&#35774;&#35745;&#20102;&#8220;&#29983;&#25104;&#19975;&#33457;&#31570;&#32593;&#32476;&#8221;&#65292;&#36890;&#36807;&#36882;&#24402;&#26144;&#23556;&#38543;&#26426;&#36755;&#20837;&#22122;&#22768;&#29983;&#25104;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#29616;&#28145;&#23618;ReLU&#32593;&#32476;&#65288;&#25110;&#22810;&#23618;&#24863;&#30693;&#22120;&#26550;&#26500;&#65289;&#34920;&#29616;&#20986;&#8220;&#36807;&#24230;&#27867;&#21270;&#8221;&#29616;&#35937;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#37027;&#20123;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#27809;&#26377;&#30475;&#21040;&#30340;&#36755;&#20837;&#30340;&#36755;&#20986;&#20540;&#34987;&#26144;&#23556;&#21040;&#20102;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#35266;&#23519;&#21040;&#30340;&#36755;&#20986;&#33539;&#22260;&#38468;&#36817;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#22810;&#23618;&#24863;&#30693;&#22120;&#23398;&#20064;&#20102;&#19968;&#23545;&#22810;&#30340;&#26144;&#23556;&#65292;&#36825;&#31181;&#25928;&#24212;&#22312;&#22686;&#21152;&#23618;&#25968;&#25110;&#22810;&#23618;&#24863;&#30693;&#22120;&#30340;&#28145;&#24230;&#26102;&#26356;&#20026;&#26126;&#26174;&#12290;&#25105;&#20204;&#21033;&#29992;&#20102;&#28145;&#23618;ReLU&#32593;&#32476;&#30340;&#36825;&#19968;&#29305;&#24615;&#26469;&#35774;&#35745;&#19968;&#20010;&#25968;&#25454;&#38598;&#19975;&#33457;&#31570;&#65292;&#31216;&#20026;&#8220;&#29983;&#25104;&#19975;&#33457;&#31570;&#32593;&#32476;&#8221;&#12290;&#31616;&#32780;&#35328;&#20043;&#65292;&#22914;&#26524;&#25105;&#20204;&#23398;&#20064;&#19968;&#20010;&#22810;&#23618;&#24863;&#30693;&#22120;&#23558;&#36755;&#20837; $x\in\mathbb{R}^D$ &#26144;&#23556;&#21040;&#33258;&#36523; $f_\mathcal{N}(x)\rightarrow x$&#65292;&#37027;&#20040;&#8220;&#19975;&#33457;&#31570;&#37319;&#26679;&#8221;&#36807;&#31243;&#23558;&#20174;&#38543;&#26426;&#36755;&#20837;&#22122;&#22768; $z\in\mathbb{R}^D$ &#24320;&#22987;&#65292;&#24182;&#36882;&#24402;&#22320;&#24212;&#29992; $f_\mathcal{N}(\cdots f_\mathcal{N}(z)\cdots )$&#12290;&#32463;&#36807;&#29123;&#28903;&#26399;&#21518;&#65292;&#25105;&#20204;&#24320;&#22987;&#35266;&#23519;&#26469;&#33258;&#36755;&#20837;&#20998;&#24067;&#30340;&#26679;&#26412;&#65292;&#25105;&#20204;&#21457;&#29616;&#26356;&#28145;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11793v1 Announce Type: cross  Abstract: We discovered that the Deep ReLU networks (or Multilayer Perceptron architecture) demonstrate an 'over-generalization' phenomenon. That is, the output values for the inputs that were not seen during training are mapped close to the output range that were observed during the learning process. In other words, the MLP learns a many-to-one mapping and this effect is more prominent as we increase the number of layers or depth of the MLP. We utilize this property of Deep ReLU networks to design a dataset kaleidoscope, termed as 'Generative Kaleidoscopic Networks'. Briefly, if we learn a MLP to map from input $x\in\mathbb{R}^D$ to itself $f_\mathcal{N}(x)\rightarrow x$, the 'Kaleidoscopic sampling' procedure starts with a random input noise $z\in\mathbb{R}^D$ and recursively applies $f_\mathcal{N}(\cdots f_\mathcal{N}(z)\cdots )$. After a burn-in period duration, we start observing samples from the input distribution and we found that deeper 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#35745;&#26816;&#39564;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#25512;&#26029;&#26694;&#26550;&#65292;&#22312;&#32771;&#34385;&#29983;&#25104;&#22270;&#20687;&#26159;&#30001;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#20135;&#29983;&#30340;&#26465;&#20214;&#19979;&#65292;&#37327;&#21270;&#21307;&#23398;&#22270;&#20687;&#35786;&#26029;&#32467;&#26524;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.11789</link><description>&lt;p&gt;
&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#30340;&#20551;&#35774;&#30340;&#32479;&#35745;&#26816;&#39564;
&lt;/p&gt;
&lt;p&gt;
Statistical Test for Generated Hypotheses by Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11789
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#35745;&#26816;&#39564;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#25512;&#26029;&#26694;&#26550;&#65292;&#22312;&#32771;&#34385;&#29983;&#25104;&#22270;&#20687;&#26159;&#30001;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#20135;&#29983;&#30340;&#26465;&#20214;&#19979;&#65292;&#37327;&#21270;&#21307;&#23398;&#22270;&#20687;&#35786;&#26029;&#32467;&#26524;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#30340;&#22686;&#24378;&#24615;&#33021;&#21152;&#36895;&#20102;&#20854;&#34701;&#20837;&#31185;&#23398;&#30740;&#31350;&#12290;&#29305;&#21035;&#26159;&#65292;&#21033;&#29992;&#29983;&#25104;&#24335;AI&#21019;&#24314;&#31185;&#23398;&#20551;&#35774;&#26159;&#24456;&#26377;&#21069;&#36884;&#30340;&#65292;&#24182;&#19988;&#27491;&#22312;&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#20110;&#21508;&#20010;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#24403;&#20351;&#29992;AI&#29983;&#25104;&#30340;&#20551;&#35774;&#36827;&#34892;&#20851;&#38190;&#20915;&#31574;&#65288;&#22914;&#21307;&#23398;&#35786;&#26029;&#65289;&#26102;&#65292;&#39564;&#35777;&#23427;&#20204;&#30340;&#21487;&#38752;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#30340;&#22270;&#20687;&#36827;&#34892;&#21307;&#23398;&#35786;&#26029;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#35745;&#26816;&#39564;&#26469;&#37327;&#21270;&#20854;&#21487;&#38752;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#32479;&#35745;&#26816;&#39564;&#30340;&#22522;&#26412;&#24605;&#24819;&#26159;&#20351;&#29992;&#36873;&#25321;&#24615;&#25512;&#26029;&#26694;&#26550;&#65292;&#25105;&#20204;&#32771;&#34385;&#22312;&#29983;&#25104;&#30340;&#22270;&#20687;&#26159;&#30001;&#32463;&#36807;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#20135;&#29983;&#30340;&#36825;&#19968;&#20107;&#23454;&#26465;&#20214;&#19979;&#30340;&#32479;&#35745;&#26816;&#39564;&#12290;&#21033;&#29992;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#21307;&#23398;&#22270;&#20687;&#35786;&#26029;&#32467;&#26524;&#30340;&#32479;&#35745;&#21487;&#38752;&#24615;&#21487;&#20197;&#20197;p&#20540;&#30340;&#24418;&#24335;&#37327;&#21270;&#65292;&#20174;&#32780;&#23454;&#29616;&#22312;&#25511;&#21046;&#38169;&#35823;&#29575;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11789v1 Announce Type: cross  Abstract: The enhanced performance of AI has accelerated its integration into scientific research. In particular, the use of generative AI to create scientific hypotheses is promising and is increasingly being applied across various fields. However, when employing AI-generated hypotheses for critical decisions, such as medical diagnoses, verifying their reliability is crucial. In this study, we consider a medical diagnostic task using generated images by diffusion models, and propose a statistical test to quantify its reliability. The basic idea behind the proposed statistical test is to employ a selective inference framework, where we consider a statistical test conditional on the fact that the generated images are produced by a trained diffusion model. Using the proposed method, the statistical reliability of medical image diagnostic results can be quantified in the form of a p-value, allowing for decision-making with a controlled error rate. 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26500;&#24314; ConflictingQA &#25968;&#25454;&#38598;&#65292;&#24182;&#36827;&#34892;&#25935;&#24863;&#24615;&#21644;&#21453;&#20107;&#23454;&#20998;&#26512;&#65292;&#30740;&#31350;&#21457;&#29616;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#27979;&#26102;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#32593;&#31449;&#19982;&#26597;&#35810;&#30340;&#30456;&#20851;&#24615;&#65292;&#32780;&#24573;&#35270;&#20102;&#20154;&#31867;&#35748;&#20026;&#37325;&#35201;&#30340;&#25991;&#26412;&#39118;&#26684;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2402.11782</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#35748;&#20026;&#21738;&#20123;&#35777;&#25454;&#20196;&#20154;&#20449;&#26381;&#65311;
&lt;/p&gt;
&lt;p&gt;
What Evidence Do Language Models Find Convincing?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11782
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26500;&#24314; ConflictingQA &#25968;&#25454;&#38598;&#65292;&#24182;&#36827;&#34892;&#25935;&#24863;&#24615;&#21644;&#21453;&#20107;&#23454;&#20998;&#26512;&#65292;&#30740;&#31350;&#21457;&#29616;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#27979;&#26102;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#32593;&#31449;&#19982;&#26597;&#35810;&#30340;&#30456;&#20851;&#24615;&#65292;&#32780;&#24573;&#35270;&#20102;&#20154;&#31867;&#35748;&#20026;&#37325;&#35201;&#30340;&#25991;&#26412;&#39118;&#26684;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#22411;&#35821;&#35328;&#27169;&#22411;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#36171;&#20104;&#20027;&#35266;&#12289;&#26377;&#20105;&#35758;&#21644;&#30683;&#30462;&#30340;&#26597;&#35810;&#20219;&#21153;&#65292;&#22914;&#8220;&#38463;&#26031;&#24052;&#29980;&#26159;&#21542;&#19982;&#30284;&#30151;&#26377;&#20851;&#8221;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#27169;&#31946;&#30340;&#26597;&#35810;&#65292;&#25105;&#20204;&#24517;&#39035;&#25628;&#32034;&#22823;&#37327;&#32593;&#31449;&#65292;&#24182;&#32771;&#34385;&#8220;&#25105;&#35748;&#20026;&#21738;&#20123;&#35777;&#25454;&#26159;&#20196;&#20154;&#20449;&#26381;&#30340;&#65311;&#8221;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#26159;&#22914;&#20309;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#30340;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026; ConflictingQA &#30340;&#25968;&#25454;&#38598;&#65292;&#23558;&#26377;&#20105;&#35758;&#30340;&#26597;&#35810;&#19982;&#19968;&#31995;&#21015;&#21253;&#21547;&#19981;&#21516;&#20107;&#23454;&#65288;&#22914;&#23450;&#37327;&#32467;&#26524;&#65289;&#12289;&#35770;&#35777;&#39118;&#26684;&#65288;&#22914;&#26435;&#23041;&#21628;&#22768;&#65289;&#21644;&#31572;&#26696;&#65288;&#26159;&#25110;&#21542;&#65289;&#30340;&#30495;&#23454;&#19990;&#30028;&#35777;&#25454;&#25991;&#26723;&#37197;&#23545;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#25935;&#24863;&#24615;&#21644;&#21453;&#20107;&#23454;&#20998;&#26512;&#65292;&#25506;&#35752;&#21738;&#20123;&#25991;&#26412;&#29305;&#24449;&#26368;&#24433;&#21709;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#27979;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#21069;&#27169;&#22411;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#32593;&#31449;&#19982;&#26597;&#35810;&#30340;&#30456;&#20851;&#24615;&#65292;&#32780;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#24573;&#35270;&#20102;&#20154;&#31867;&#35748;&#20026;&#37325;&#35201;&#30340;&#39118;&#26684;&#29305;&#24449;&#65292;&#27604;&#22914;&#25991;&#26412;&#26159;&#21542;&#26159;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11782v1 Announce Type: new  Abstract: Retrieval-augmented language models are being increasingly tasked with subjective, contentious, and conflicting queries such as "is aspartame linked to cancer". To resolve these ambiguous queries, one must search through a large range of websites and consider "which, if any, of this evidence do I find convincing?". In this work, we study how LLMs answer this question. In particular, we construct ConflictingQA, a dataset that pairs controversial queries with a series of real-world evidence documents that contain different facts (e.g., quantitative results), argument styles (e.g., appeals to authority), and answers (Yes or No). We use this dataset to perform sensitivity and counterfactual analyses to explore which text features most affect LLM predictions. Overall, we find that current models rely heavily on the relevance of a website to the query, while largely ignoring stylistic features that humans find important such as whether a text 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26500;&#24314;&#29702;&#35770;&#26694;&#26550;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#33258;&#28040;&#32791;&#24490;&#29615;&#20013;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#23545;&#25968;&#25454;&#20998;&#24067;&#23398;&#20064;&#30340;&#24433;&#21709;&#65292;&#35777;&#26126;&#20102;&#22312;&#36275;&#22815;&#22823;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#22823;&#23567;&#25110;&#30495;&#23454;&#25968;&#25454;&#27604;&#20363;&#26465;&#20214;&#19979;&#65292;&#21512;&#25104;&#25968;&#25454;&#20998;&#24067;&#19982;&#21407;&#22987;&#30495;&#23454;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#30340;&#24635;&#21464;&#24046;&#36317;&#31163;&#33021;&#22815;&#34987;&#26377;&#25928;&#25511;&#21046;&#12290;</title><link>https://arxiv.org/abs/2402.11778</link><description>&lt;p&gt;
&#26397;&#21521;&#33258;&#28040;&#32791;&#29983;&#25104;&#27169;&#22411;&#30340;&#29702;&#35770;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Towards Theoretical Understandings of Self-Consuming Generative Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11778
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26500;&#24314;&#29702;&#35770;&#26694;&#26550;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#33258;&#28040;&#32791;&#24490;&#29615;&#20013;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#23545;&#25968;&#25454;&#20998;&#24067;&#23398;&#20064;&#30340;&#24433;&#21709;&#65292;&#35777;&#26126;&#20102;&#22312;&#36275;&#22815;&#22823;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#22823;&#23567;&#25110;&#30495;&#23454;&#25968;&#25454;&#27604;&#20363;&#26465;&#20214;&#19979;&#65292;&#21512;&#25104;&#25968;&#25454;&#20998;&#24067;&#19982;&#21407;&#22987;&#30495;&#23454;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#30340;&#24635;&#21464;&#24046;&#36317;&#31163;&#33021;&#22815;&#34987;&#26377;&#25928;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#30340;&#26032;&#25361;&#25112;&#65292;&#21363;&#22312;&#19968;&#20010;&#33258;&#28040;&#32791;&#24490;&#29615;&#20013;&#35757;&#32451;&#27169;&#22411;&#65292;&#20854;&#20013;&#36830;&#32493;&#30340;&#27169;&#22411;&#19990;&#20195;&#36890;&#36807;&#28151;&#21512;&#20043;&#21069;&#19990;&#20195;&#30340;&#30495;&#23454;&#25968;&#25454;&#21644;&#21512;&#25104;&#25968;&#25454;&#26469;&#36827;&#34892;&#36882;&#24402;&#35757;&#32451;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#20197;&#20005;&#26684;&#35780;&#20272;&#36825;&#31181;&#35757;&#32451;&#26041;&#26696;&#23545;&#26410;&#26469;&#27169;&#22411;&#23398;&#20064;&#30340;&#25968;&#25454;&#20998;&#24067;&#20135;&#29983;&#30340;&#24433;&#21709;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#22312;&#19981;&#21516;&#28151;&#21512;&#35757;&#32451;&#22330;&#26223;&#19979;&#65292;&#26410;&#26469;&#27169;&#22411;&#20135;&#29983;&#30340;&#21512;&#25104;&#25968;&#25454;&#20998;&#24067;&#19982;&#21407;&#22987;&#30495;&#23454;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#30340;&#24635;&#21464;&#24046;&#65288;TV&#65289;&#36317;&#31163;&#30340;&#30028;&#38480;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#22312;&#28151;&#21512;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#25110;&#30495;&#23454;&#25968;&#25454;&#27604;&#20363;&#36275;&#22815;&#22823;&#30340;&#26465;&#20214;&#19979;&#65292;&#36825;&#31181;&#36317;&#31163;&#21487;&#20197;&#34987;&#26377;&#25928;&#25511;&#21046;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25581;&#31034;&#20102;&#30001;&#25193;&#22823;&#21512;&#25104;&#25968;&#25454;&#37327;&#24341;&#36215;&#30340;&#30456;&#21464;&#65292;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#34429;&#28982;TV&#36317;&#31163;&#34920;&#29616;&#20986;&#21021;&#22987;&#19978;&#21319;&#65292;&#20294;&#21364;&#36880;&#28176;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11778v1 Announce Type: cross  Abstract: This paper tackles the emerging challenge of training generative models within a self-consuming loop, wherein successive generations of models are recursively trained on mixtures of real and synthetic data from previous generations. We construct a theoretical framework to rigorously evaluate how this training regimen impacts the data distributions learned by future models. Specifically, we derive bounds on the total variation (TV) distance between the synthetic data distributions produced by future models and the original real data distribution under various mixed training scenarios. Our analysis demonstrates that this distance can be effectively controlled under the condition that mixed training dataset sizes or proportions of real data are large enough. Interestingly, we further unveil a phase transition induced by expanding synthetic data amounts, proving theoretically that while the TV distance exhibits an initial ascent, it declin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;ETHICS Utilitarianism&#20219;&#21153;&#21457;&#29616;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#31034;&#38544;&#21547;&#20102;&#23545;&#20154;&#31867;&#31119;&#31049;&#30340;&#29702;&#35299;&#65292;&#19988;&#27169;&#22411;&#35268;&#27169;&#22686;&#21152;&#26102;&#65292;&#20934;&#30830;&#29575;&#21576;&#38750;&#19979;&#38477;&#36235;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.11777</link><description>&lt;p&gt;
&#22312;&#35821;&#35328;&#27169;&#22411;&#23884;&#20837;&#20013;&#25581;&#31034;&#28508;&#22312;&#30340;&#20154;&#31867;&#31119;&#31049;
&lt;/p&gt;
&lt;p&gt;
Uncovering Latent Human Wellbeing in Language Model Embeddings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11777
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;ETHICS Utilitarianism&#20219;&#21153;&#21457;&#29616;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#31034;&#38544;&#21547;&#20102;&#23545;&#20154;&#31867;&#31119;&#31049;&#30340;&#29702;&#35299;&#65292;&#19988;&#27169;&#22411;&#35268;&#27169;&#22686;&#21152;&#26102;&#65292;&#20934;&#30830;&#29575;&#21576;&#38750;&#19979;&#38477;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#38544;&#21547;&#22320;&#23398;&#20064;&#20102;&#20154;&#31867;&#31119;&#31049;&#30340;&#27010;&#24565;&#65311;&#25105;&#20204;&#36890;&#36807;ETHICS&#21151;&#21033;&#20027;&#20041;&#20219;&#21153;&#36827;&#34892;&#25506;&#35752;&#65292;&#35780;&#20272;&#32553;&#25918;&#26159;&#21542;&#22686;&#24378;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#21457;&#29616;&#26174;&#31034;&#65292;&#26080;&#38656;&#20219;&#20309;&#25552;&#31034;&#24037;&#31243;&#25110;&#24494;&#35843;&#65292;OpenAI&#30340;text-embedding-ada-002&#30340;&#20027;&#25104;&#20998;&#36798;&#21040;73.9%&#30340;&#20934;&#30830;&#29575;&#12290;&#36825;&#19982;&#22312;&#25972;&#20010;ETHICS&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;&#30340;BERT-large&#27169;&#22411;&#30340;74.6%&#20934;&#30830;&#29575;&#38750;&#24120;&#25509;&#36817;&#65292;&#34920;&#26126;&#39044;&#35757;&#32451;&#20256;&#36798;&#20102;&#23545;&#20154;&#31867;&#31119;&#31049;&#30340;&#26576;&#31181;&#29702;&#35299;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#22235;&#31181;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#65292;&#35266;&#23519;&#21151;&#21033;&#20027;&#20041;&#20934;&#30830;&#29575;&#38543;&#21442;&#25968;&#22686;&#21152;&#32780;&#21464;&#21270;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20351;&#29992;&#36275;&#22815;&#25968;&#37327;&#30340;&#20027;&#25104;&#20998;&#26102;&#65292;&#24615;&#33021;&#38543;&#27169;&#22411;&#35268;&#27169;&#30340;&#22686;&#21152;&#32780;&#38750;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11777v1 Announce Type: cross  Abstract: Do language models implicitly learn a concept of human wellbeing? We explore this through the ETHICS Utilitarianism task, assessing if scaling enhances pretrained models' representations. Our initial finding reveals that, without any prompt engineering or finetuning, the leading principal component from OpenAI's text-embedding-ada-002 achieves 73.9% accuracy. This closely matches the 74.6% of BERT-large finetuned on the entire ETHICS dataset, suggesting pretraining conveys some understanding about human wellbeing. Next, we consider four language model families, observing how Utilitarianism accuracy varies with increased parameters. We find performance is nondecreasing with increased model size when using sufficient numbers of principal components.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#33258;&#21160;&#35282;&#24230;&#36229;&#20998;&#36776;&#29575;&#25216;&#26415;&#65292;&#23558;&#26469;&#33258;&#26356;&#24555;&#36895;&#37319;&#38598;&#30340;&#25968;&#25454;&#30340;&#35282;&#24230;&#20272;&#35745;&#25552;&#21319;&#21040;&#19982;&#38656;&#35201;&#36739;&#38271;&#26102;&#38388;&#33719;&#21462;&#30340;&#39640;&#20998;&#36776;&#29575;&#25968;&#25454;&#30456;&#24403;&#65292;&#20174;&#32780;&#22312;&#32420;&#32500;&#23450;&#21521;&#20998;&#24067;&#30340;&#20272;&#35745;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2402.11775</link><description>&lt;p&gt;
FOD-Swin-Net&#65306;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#28145;&#24230;&#27169;&#22411;&#23454;&#29616;&#32420;&#32500;&#23450;&#21521;&#20998;&#24067;&#30340;&#35282;&#24230;&#36229;&#20998;&#36776;&#29575;
&lt;/p&gt;
&lt;p&gt;
FOD-Swin-Net: angular super resolution of fiber orientation distribution using a transformer-based deep model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11775
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#33258;&#21160;&#35282;&#24230;&#36229;&#20998;&#36776;&#29575;&#25216;&#26415;&#65292;&#23558;&#26469;&#33258;&#26356;&#24555;&#36895;&#37319;&#38598;&#30340;&#25968;&#25454;&#30340;&#35282;&#24230;&#20272;&#35745;&#25552;&#21319;&#21040;&#19982;&#38656;&#35201;&#36739;&#38271;&#26102;&#38388;&#33719;&#21462;&#30340;&#39640;&#20998;&#36776;&#29575;&#25968;&#25454;&#30456;&#24403;&#65292;&#20174;&#32780;&#22312;&#32420;&#32500;&#23450;&#21521;&#20998;&#24067;&#30340;&#20272;&#35745;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35782;&#21035;&#21644;&#34920;&#24449;&#33041;&#37096;&#32420;&#32500;&#26463;&#21487;&#20197;&#24110;&#21161;&#29702;&#35299;&#35768;&#22810;&#30142;&#30149;&#21644;&#30149;&#20917;&#12290;&#22312;&#36825;&#19968;&#36807;&#31243;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#27493;&#39588;&#26159;&#21033;&#29992;&#25193;&#25955;&#21152;&#26435;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;DW-MRI&#65289;&#26469;&#20272;&#35745;&#32420;&#32500;&#26041;&#21521;&#12290;&#28982;&#32780;&#65292;&#33719;&#24471;&#31283;&#20581;&#30340;&#26041;&#21521;&#20272;&#35745;&#38656;&#35201;&#39640;&#20998;&#36776;&#29575;&#25968;&#25454;&#65292;&#23548;&#33268;&#33719;&#21462;&#26102;&#38388;&#38271;&#19988;&#22312;&#20020;&#24202;&#19978;&#24182;&#19981;&#24635;&#26159;&#21487;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#21033;&#29992;&#26356;&#24555;&#36895;&#33719;&#21462;&#30340;&#33258;&#21160;&#35282;&#24230;&#36229;&#20998;&#36776;&#29575;&#26469;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#12290;&#21033;&#29992;&#20844;&#24320;&#21487;&#29992;&#30340;Human Connectome Project&#65288;HCP&#65289;DW-MRI&#25968;&#25454;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#23454;&#29616;&#20102;&#32420;&#32500;&#23450;&#21521;&#20998;&#24067;&#65288;FOD&#65289;&#30340;&#35282;&#24230;&#36229;&#20998;&#36776;&#29575;&#12290;&#25105;&#20204;&#30340;&#22522;&#20110;&#22270;&#22359;&#30340;&#26041;&#27861;&#65292;FOD-Swin-Net&#65292;&#33021;&#22815;&#20351;&#20174;32&#20010;&#26041;&#21521;&#25512;&#21160;&#30340;&#21333;&#22771;&#37325;&#24314;&#19982;&#22810;&#22771;288&#20010;&#26041;&#21521;FOD&#37325;&#24314;&#30456;&#23218;&#32654;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#25152;&#38656;&#30340;&#25968;&#30446;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11775v1 Announce Type: cross  Abstract: Identifying and characterizing brain fiber bundles can help to understand many diseases and conditions. An important step in this process is the estimation of fiber orientations using Diffusion-Weighted Magnetic Resonance Imaging (DW-MRI). However, obtaining robust orientation estimates demands high-resolution data, leading to lengthy acquisitions that are not always clinically available. In this work, we explore the use of automated angular super resolution from faster acquisitions to overcome this challenge. Using the publicly available Human Connectome Project (HCP) DW-MRI data, we trained a transformer-based deep learning architecture to achieve angular super resolution in fiber orientation distribution (FOD). Our patch-based methodology, FOD-Swin-Net, is able to bring a single-shell reconstruction driven from 32 directions to be comparable to a multi-shell 288 direction FOD reconstruction, greatly reducing the number of required d
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;Dynamic Multi-network Mining (DMM)&#65292;&#33021;&#22815;&#23558;&#24352;&#37327;&#26102;&#38388;&#24207;&#21015;&#36716;&#25442;&#20026;&#19981;&#21516;&#38271;&#24230;&#30340;&#27573;&#32452;&#65292;&#36890;&#36807;&#31232;&#30095;&#20381;&#36182;&#32593;&#32476;&#25552;&#20379;&#32858;&#31867;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#31934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.11773</link><description>&lt;p&gt;
&#24352;&#37327;&#26102;&#38388;&#24207;&#21015;&#30340;&#21160;&#24577;&#22810;&#32593;&#32476;&#25366;&#25496;
&lt;/p&gt;
&lt;p&gt;
Dynamic Multi-Network Mining of Tensor Time Series
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11773
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;Dynamic Multi-network Mining (DMM)&#65292;&#33021;&#22815;&#23558;&#24352;&#37327;&#26102;&#38388;&#24207;&#21015;&#36716;&#25442;&#20026;&#19981;&#21516;&#38271;&#24230;&#30340;&#27573;&#32452;&#65292;&#36890;&#36807;&#31232;&#30095;&#20381;&#36182;&#32593;&#32476;&#25552;&#20379;&#32858;&#31867;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#31934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#30340;&#23376;&#24207;&#21015;&#32858;&#31867;&#26159;&#25968;&#25454;&#25366;&#25496;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#20219;&#21153;&#65292;&#35299;&#37322;&#32467;&#26524;&#32858;&#31867;&#20063;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#36890;&#24120;&#25105;&#20204;&#27809;&#26377;&#20851;&#20110;&#25968;&#25454;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#22240;&#27492;&#65292;&#38754;&#23545;&#30001;&#21253;&#21547;&#26102;&#38388;&#25139;&#22312;&#20869;&#30340;&#22810;&#31181;&#27169;&#24335;&#32452;&#25104;&#30340;&#22823;&#37327;&#24352;&#37327;&#26102;&#38388;&#24207;&#21015;&#65292;&#25105;&#20204;&#22914;&#20309;&#20026;&#24352;&#37327;&#26102;&#38388;&#24207;&#21015;&#23454;&#29616;&#23376;&#24207;&#21015;&#32858;&#31867;&#24182;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#35265;&#35299;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21363;&#21160;&#24577;&#22810;&#32593;&#32476;&#25366;&#25496;&#65288;DMM&#65289;&#65292;&#23427;&#23558;&#24352;&#37327;&#26102;&#38388;&#24207;&#21015;&#36716;&#25442;&#20026;&#30001;l1&#33539;&#25968;&#32422;&#26463;&#30340;&#19968;&#32452;&#21508;&#31181;&#38271;&#24230;&#30340;&#27573;&#32452;&#65288;&#21363;&#32858;&#31867;&#65289;&#29305;&#24449;&#21270;&#30340;&#20381;&#36182;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#20197;&#19979;&#29305;&#24615;&#12290;(a) &#21487;&#35299;&#37322;&#24615;&#65306;&#23427;&#20351;&#29992;&#22810;&#20010;&#32593;&#32476;&#23545;&#32858;&#31867;&#36827;&#34892;&#29305;&#24449;&#25551;&#36848;&#65292;&#27599;&#20010;&#32593;&#32476;&#26159;&#30456;&#24212;&#38750;&#26102;&#38388;&#27169;&#24335;&#30340;&#31232;&#30095;&#20381;&#36182;&#32593;&#32476;&#65292;&#20174;&#32780;&#25552;&#20379;&#21487;&#35265;&#19988;&#21487;&#35299;&#37322;&#30340;&#20851;&#38190;&#20851;&#31995;&#35265;&#35299;&#12290; (b) &#31934;&#30830;&#24615;&#65306;&#23427;&#21457;&#29616;&#20102;&#32858;&#31867;&#12290;&#12290;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11773v1 Announce Type: cross  Abstract: Subsequence clustering of time series is an essential task in data mining, and interpreting the resulting clusters is also crucial since we generally do not have prior knowledge of the data. Thus, given a large collection of tensor time series consisting of multiple modes, including timestamps, how can we achieve subsequence clustering for tensor time series and provide interpretable insights? In this paper, we propose a new method, Dynamic Multi-network Mining (DMM), that converts a tensor time series into a set of segment groups of various lengths (i.e., clusters) characterized by a dependency network constrained with l1-norm. Our method has the following properties. (a) Interpretable: it characterizes the cluster with multiple networks, each of which is a sparse dependency network of a corresponding non-temporal mode, and thus provides visible and interpretable insights into the key relationships. (b) Accurate: it discovers the clus
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35780;&#20272;&#22522;&#20110;&#25351;&#25968;&#30340;&#36164;&#28304;&#20998;&#37197;&#31574;&#30053;&#26377;&#25928;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32763;&#35793;&#21644;&#25193;&#23637;&#32479;&#35745;&#25991;&#29486;&#20013;&#30340;&#26368;&#26032;&#24605;&#24819;&#65292;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#20272;&#35745;&#22120;&#21644;&#35745;&#31639;&#28176;&#36817;&#27491;&#30830;&#32622;&#20449;&#21306;&#38388;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.11771</link><description>&lt;p&gt;
&#35780;&#20272;&#22522;&#20110;&#25351;&#25968;&#30340;&#27835;&#30103;&#20998;&#37197;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Effectiveness of Index-Based Treatment Allocation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11771
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35780;&#20272;&#22522;&#20110;&#25351;&#25968;&#30340;&#36164;&#28304;&#20998;&#37197;&#31574;&#30053;&#26377;&#25928;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32763;&#35793;&#21644;&#25193;&#23637;&#32479;&#35745;&#25991;&#29486;&#20013;&#30340;&#26368;&#26032;&#24605;&#24819;&#65292;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#20272;&#35745;&#22120;&#21644;&#35745;&#31639;&#28176;&#36817;&#27491;&#30830;&#32622;&#20449;&#21306;&#38388;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#36164;&#28304;&#31232;&#32570;&#26102;&#65292;&#38656;&#35201;&#19968;&#31181;&#20998;&#37197;&#31574;&#30053;&#26469;&#20915;&#23450;&#35841;&#33021;&#33719;&#24471;&#36164;&#28304;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35780;&#20272;&#22522;&#20110;&#25351;&#25968;&#30340;&#20998;&#37197;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#35813;&#31574;&#30053;&#36890;&#36807;&#20351;&#29992;&#38543;&#26426;&#23545;&#29031;&#35797;&#39564;&#30340;&#25968;&#25454;&#65292;&#23558;&#26377;&#38480;&#25968;&#37327;&#30340;&#36164;&#28304;&#20998;&#37197;&#32473;&#26368;&#38656;&#35201;&#30340;&#20154;&#12290;&#25105;&#20204;&#20174;&#32479;&#35745;&#25991;&#29486;&#20013;&#32763;&#35793;&#21644;&#25193;&#23637;&#20102;&#26368;&#36817;&#30340;&#24819;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20272;&#35745;&#22120;&#21644;&#35745;&#31639;&#28176;&#36817;&#27491;&#30830;&#32622;&#20449;&#21306;&#38388;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#24471;&#20986;&#26377;&#25928;&#30340;&#32479;&#35745;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11771v1 Announce Type: cross  Abstract: When resources are scarce, an allocation policy is needed to decide who receives a resource. This problem occurs, for instance, when allocating scarce medical resources and is often solved using modern ML methods. This paper introduces methods to evaluate index-based allocation policies -- that allocate a fixed number of resources to those who need them the most -- by using data from a randomized control trial. Such policies create dependencies between agents, which render the assumptions behind standard statistical tests invalid and limit the effectiveness of estimators. Addressing these challenges, we translate and extend recent ideas from the statistics literature to present an efficient estimator and methods for computing asymptotically correct confidence intervals. This enables us to effectively draw valid statistical conclusions, a critical gap in previous work. Our extensive experiments validate our methodology in practical sett
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#24378;&#21270;&#23398;&#20064;&#20316;&#20026;&#39044;&#27979;&#32423;&#32852;&#30340;&#31616;&#27905;&#26367;&#20195;&#26041;&#26696;&#65292;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.11760</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20316;&#20026;&#39044;&#27979;&#32423;&#32852;&#30340;&#31616;&#27905;&#26367;&#20195;&#26041;&#26696;&#65306;&#22522;&#20110;&#22270;&#20687;&#20998;&#21106;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning as a Parsimonious Alternative to Prediction Cascades: A Case Study on Image Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11760
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#24378;&#21270;&#23398;&#20064;&#20316;&#20026;&#39044;&#27979;&#32423;&#32852;&#30340;&#31616;&#27905;&#26367;&#20195;&#26041;&#26696;&#65292;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20363;&#22914;&#30446;&#26631;&#26816;&#27979;&#21644;&#22270;&#20687;&#20998;&#21106;&#12290;&#23613;&#31649;&#36825;&#20123;&#26550;&#26500;&#23548;&#33268;&#20102;&#31934;&#24230;&#30340;&#25552;&#39640;&#65292;&#20294;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#36890;&#24120;&#20250;&#20276;&#38543;&#30528;&#35745;&#31639;&#21644;&#20869;&#23384;&#38656;&#27714;&#30340;&#22823;&#24133;&#22686;&#21152;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#39044;&#27979;&#32423;&#32852;&#23548;&#33268;&#20102;&#30001;&#20110;&#20887;&#20313;&#20013;&#38388;&#27493;&#39588;&#32780;&#22686;&#21152;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11760v1 Announce Type: new  Abstract: Deep learning architectures have achieved state-of-the-art (SOTA) performance on computer vision tasks such as object detection and image segmentation. This may be attributed to the use of over-parameterized, monolithic deep learning architectures executed on large datasets. Although such architectures lead to increased accuracy, this is usually accompanied by a large increase in computation and memory requirements during inference. While this is a non-issue in traditional machine learning pipelines, the recent confluence of machine learning and fields like the Internet of Things has rendered such large architectures infeasible for execution in low-resource settings. In such settings, previous efforts have proposed decision cascades where inputs are passed through models of increasing complexity until desired performance is achieved. However, we argue that cascaded prediction leads to increased computational cost due to wasteful intermed
&lt;/p&gt;</description></item><item><title>MARS&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20998;&#20989;&#25968;MARS&#65292;&#32771;&#34385;&#20102;&#29983;&#25104;&#24207;&#21015;&#20013;&#27599;&#20010;&#26631;&#35760;&#30340;&#35821;&#20041;&#36129;&#29486;&#65292;&#35813;&#26041;&#27861;&#25913;&#36827;&#20102;&#29983;&#25104;&#24335;LLMs&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.11756</link><description>&lt;p&gt;
MARS&#65306;&#29992;&#20110;&#29983;&#25104;&#24335;LLMs&#20013;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#24847;&#20041;&#24863;&#30693;&#21709;&#24212;&#35780;&#20998;
&lt;/p&gt;
&lt;p&gt;
MARS: Meaning-Aware Response Scoring for Uncertainty Estimation in Generative LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11756
&lt;/p&gt;
&lt;p&gt;
MARS&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20998;&#20989;&#25968;MARS&#65292;&#32771;&#34385;&#20102;&#29983;&#25104;&#24207;&#21015;&#20013;&#27599;&#20010;&#26631;&#35760;&#30340;&#35821;&#20041;&#36129;&#29486;&#65292;&#35813;&#26041;&#27861;&#25913;&#36827;&#20102;&#29983;&#25104;&#24335;LLMs&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22240;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#21331;&#36234;&#34920;&#29616;&#32780;&#34987;&#24191;&#27867;&#21033;&#29992;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20135;&#29983;&#19981;&#20934;&#30830;&#25110;&#35823;&#23548;&#24615;&#36755;&#20986;&#30340;&#20542;&#21521;&#21487;&#33021;&#24102;&#26469;&#28508;&#22312;&#39118;&#38505;&#65292;&#23588;&#20854;&#26159;&#22312;&#39640;&#39118;&#38505;&#29615;&#22659;&#20013;&#12290;&#22240;&#27492;&#65292;&#20272;&#35745;&#29983;&#25104;&#24335;LLM&#36755;&#20986;&#30340;&#27491;&#30830;&#24615;&#26159;&#22686;&#24378;&#21487;&#38752;&#24615;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#29983;&#25104;&#24335;LLMs&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65288;UE&#65289;&#26159;&#19968;&#20010;&#19981;&#26029;&#21457;&#23637;&#30340;&#39046;&#22495;&#65292;&#20854;&#20013;SOTA&#22522;&#20110;&#27010;&#29575;&#30340;&#26041;&#27861;&#36890;&#24120;&#37319;&#29992;&#38271;&#24230;&#26631;&#20934;&#21270;&#35780;&#20998;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24847;&#20041;&#24863;&#30693;&#21709;&#24212;&#35780;&#20998;&#65288;MARS&#65289;&#30340;&#26367;&#20195;&#38271;&#24230;&#26631;&#20934;&#21270;&#35780;&#20998;&#30340;UE&#26041;&#27861;&#12290;MARS&#26159;&#19968;&#31181;&#32771;&#34385;&#22312;&#38382;&#39064;&#30340;&#19978;&#19979;&#25991;&#20013;&#29983;&#25104;&#24207;&#21015;&#20013;&#27599;&#20010;&#26631;&#35760;&#30340;&#35821;&#20041;&#36129;&#29486;&#30340;&#26032;&#22411;&#35780;&#20998;&#20989;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#23558;MARS&#25972;&#21512;&#21040;UE&#26041;&#27861;&#20013;&#20250;&#22312;UE&#24615;&#33021;&#19978;&#24102;&#26469;&#26222;&#36941;&#21644;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#20351;&#29992;&#19977;&#31181;&#19981;&#21516;&#30340;&#38381;&#21367;&#24335;&#38382;&#31572;&#26469;&#36827;&#34892;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11756v1 Announce Type: new  Abstract: Generative Large Language Models (LLMs) are widely utilized for their excellence in various tasks. However, their tendency to produce inaccurate or misleading outputs poses a potential risk, particularly in high-stakes environments. Therefore, estimating the correctness of generative LLM outputs is an important task for enhanced reliability. Uncertainty Estimation (UE) in generative LLMs is an evolving domain, where SOTA probability-based methods commonly employ length-normalized scoring. In this work, we propose Meaning-Aware Response Scoring (MARS) as an alternative to length-normalized scoring for UE methods. MARS is a novel scoring function that considers the semantic contribution of each token in the generated sequence in the context of the question. We demonstrate that integrating MARS into UE methods results in a universal and significant improvement in UE performance. We conduct experiments using three distinct closed-book questi
&lt;/p&gt;</description></item><item><title>SPML&#26159;&#19968;&#31181;&#29992;&#20110;&#20248;&#21270;&#25552;&#31034;&#24182;&#30417;&#25511;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32842;&#22825;&#26426;&#22120;&#20154;&#36755;&#20837;&#30340;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#65292;&#29992;&#20110;&#38450;&#24481;&#24694;&#24847;&#25915;&#20987;&#24182;&#20248;&#21270;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.11755</link><description>&lt;p&gt;
SPML: &#19968;&#31181;&#29992;&#20110;&#38450;&#24481;&#35821;&#35328;&#27169;&#22411;&#21463;&#21040;&#25552;&#31034;&#25915;&#20987;&#30340;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
SPML: A DSL for Defending Language Models Against Prompt Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11755
&lt;/p&gt;
&lt;p&gt;
SPML&#26159;&#19968;&#31181;&#29992;&#20110;&#20248;&#21270;&#25552;&#31034;&#24182;&#30417;&#25511;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32842;&#22825;&#26426;&#22120;&#20154;&#36755;&#20837;&#30340;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#65292;&#29992;&#20110;&#38450;&#24481;&#24694;&#24847;&#25915;&#20987;&#24182;&#20248;&#21270;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#28145;&#21051;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#24212;&#29992;&#65292;&#36234;&#26469;&#36234;&#22810;&#22320;&#20381;&#36182;&#20110;&#22522;&#20110;&#25351;&#20196;&#30340;&#23450;&#20041;&#26469;&#35774;&#35745;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;&#28982;&#32780;&#65292;&#37096;&#32626;&#21518;&#65292;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#23450;&#20041;&#26159;&#22266;&#23450;&#30340;&#65292;&#24182;&#19988;&#23481;&#26131;&#21463;&#21040;&#24694;&#24847;&#29992;&#25143;&#30340;&#25915;&#20987;&#65292;&#31361;&#20986;&#20102;&#38450;&#27490;&#19981;&#36947;&#24503;&#24212;&#29992;&#21644;&#36130;&#21153;&#25439;&#22833;&#30340;&#38656;&#35201;&#12290;&#29616;&#26377;&#30740;&#31350;&#25506;&#35752;&#20102;&#29992;&#25143;&#25552;&#31034;&#23545;&#22522;&#20110;LLM&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#24433;&#21709;&#65292;&#20294;&#23578;&#26410;&#25506;&#32034;&#21253;&#21547;&#24212;&#29992;&#29305;&#23450;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#25915;&#20987;&#30340;&#23454;&#29992;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#31995;&#32479;&#25552;&#31034;&#20803;&#35821;&#35328;&#65288;SPML&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#20248;&#21270;&#25552;&#31034;&#24182;&#30417;&#25511;&#22522;&#20110;LLM&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#36755;&#20837;&#30340;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#12290;SPML&#20027;&#21160;&#26816;&#26597;&#25915;&#20987;&#25552;&#31034;&#65292;&#30830;&#20445;&#29992;&#25143;&#36755;&#20837;&#19982;&#32842;&#22825;&#26426;&#22120;&#20154;&#23450;&#20041;&#30456;&#31526;&#65292;&#38450;&#27490;&#22312;LLM&#20027;&#24178;&#19978;&#23545;&#20854;&#36827;&#34892;&#24694;&#24847;&#25191;&#34892;&#65292;&#20248;&#21270;&#25104;&#26412;&#12290;&#23427;&#20063;&#36890;&#36807;&#32534;&#31243;&#35821;&#35328;&#33021;&#21147;&#31616;&#21270;&#20102;&#32842;&#22825;&#26426;&#22120;&#20154;&#23450;&#20041;&#30340;&#21046;&#20316;&#65292;&#20811;&#26381;&#20102;&#33258;&#28982;&#35821;&#35328;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11755v1 Announce Type: cross  Abstract: Large language models (LLMs) have profoundly transformed natural language applications, with a growing reliance on instruction-based definitions for designing chatbots. However, post-deployment the chatbot definitions are fixed and are vulnerable to attacks by malicious users, emphasizing the need to prevent unethical applications and financial losses. Existing studies explore user prompts' impact on LLM-based chatbots, yet practical methods to contain attacks on application-specific chatbots remain unexplored. This paper presents System Prompt Meta Language (SPML), a domain-specific language for refining prompts and monitoring the inputs to the LLM-based chatbots. SPML actively checks attack prompts, ensuring user inputs align with chatbot definitions to prevent malicious execution on the LLM backbone, optimizing costs. It also streamlines chatbot definition crafting with programming language capabilities, overcoming natural language 
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;Diagonalisation Stochastic Gradient Descent&#65288;&#23545;&#35282;&#21270;SGD&#65289;&#65292;&#36890;&#36807;&#37325;&#26032;&#21442;&#25968;&#21270;&#21644;&#24179;&#28369;&#23454;&#29616;&#38750;&#21487;&#24494;&#27169;&#22411;&#30340;&#24555;&#36895;&#25910;&#25947;SGD&#65292;&#22312;&#23454;&#35777;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#31616;&#21333;&#12289;&#24555;&#36895;&#12289;&#31283;&#23450;&#65292;&#24182;&#19988;&#21462;&#24471;&#20102;&#25968;&#37327;&#32423;&#30340;&#24037;&#20316;&#35268;&#33539;&#21270;&#26041;&#24046;&#38477;&#20302;&#12290;</title><link>https://arxiv.org/abs/2402.11752</link><description>&lt;p&gt;
&#23545;&#35282;&#21270;SGD&#65306;&#36890;&#36807;&#37325;&#26032;&#21442;&#25968;&#21270;&#21644;&#24179;&#28369;&#23454;&#29616;&#38750;&#21487;&#24494;&#27169;&#22411;&#30340;&#24555;&#36895;&#25910;&#25947;SGD
&lt;/p&gt;
&lt;p&gt;
Diagonalisation SGD: Fast &amp; Convergent SGD for Non-Differentiable Models via Reparameterisation and Smoothing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11752
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;Diagonalisation Stochastic Gradient Descent&#65288;&#23545;&#35282;&#21270;SGD&#65289;&#65292;&#36890;&#36807;&#37325;&#26032;&#21442;&#25968;&#21270;&#21644;&#24179;&#28369;&#23454;&#29616;&#38750;&#21487;&#24494;&#27169;&#22411;&#30340;&#24555;&#36895;&#25910;&#25947;SGD&#65292;&#22312;&#23454;&#35777;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#31616;&#21333;&#12289;&#24555;&#36895;&#12289;&#31283;&#23450;&#65292;&#24182;&#19988;&#21462;&#24471;&#20102;&#25968;&#37327;&#32423;&#30340;&#24037;&#20316;&#35268;&#33539;&#21270;&#26041;&#24046;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#25152;&#21608;&#30693;&#65292;&#23545;&#20110;&#38750;&#21487;&#24494;&#27169;&#22411;&#65292;&#23637;&#29616;&#20986;&#36739;&#20302;&#26041;&#24046;&#30340;&#37325;&#26032;&#21442;&#25968;&#21270;&#26799;&#24230;&#20272;&#35745;&#22120;&#22312;&#23454;&#36341;&#20013;&#23384;&#22312;&#20559;&#24046;&#12290;&#36825;&#21487;&#33021;&#21361;&#21450;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#26041;&#27861;&#65288;&#22914;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;SGD&#65289;&#30340;&#27491;&#30830;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#35821;&#27861;&#26694;&#26550;&#26469;&#20998;&#22359;&#22320;&#23450;&#20041;&#38750;&#21487;&#24494;&#20989;&#25968;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#26041;&#27861;&#65292;&#20197;&#33719;&#24471;&#20351;&#37325;&#26032;&#21442;&#25968;&#21270;&#26799;&#24230;&#20272;&#35745;&#22120;&#26080;&#20559;&#30340;&#24179;&#28369;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;SGD&#21464;&#20307;&#65292;&#23545;&#35282;&#21270;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65292;&#23427;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#36880;&#27493;&#25552;&#39640;&#24179;&#28369;&#36817;&#20284;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#35777;&#26126;&#25910;&#25947;&#21040;&#26410;&#24179;&#28369;&#65288;&#21407;&#22987;&#65289;&#30446;&#26631;&#30340;&#31283;&#23450;&#28857;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35780;&#20272;&#26174;&#31034;&#65292;&#19982;&#29616;&#26377;&#25216;&#26415;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#31616;&#21333;&#12289;&#24555;&#36895;&#12289;&#31283;&#23450;&#65292;&#24182;&#19988;&#22312;&#24037;&#20316;&#35268;&#33539;&#21270;&#26041;&#24046;&#19978;&#23454;&#29616;&#20102;&#25968;&#37327;&#32423;&#30340;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11752v1 Announce Type: cross  Abstract: It is well-known that the reparameterisation gradient estimator, which exhibits low variance in practice, is biased for non-differentiable models. This may compromise correctness of gradient-based optimisation methods such as stochastic gradient descent (SGD). We introduce a simple syntactic framework to define non-differentiable functions piecewisely and present a systematic approach to obtain smoothings for which the reparameterisation gradient estimator is unbiased. Our main contribution is a novel variant of SGD, Diagonalisation Stochastic Gradient Descent, which progressively enhances the accuracy of the smoothed approximation during optimisation, and we prove convergence to stationary points of the unsmoothed (original) objective. Our empirical evaluation reveals benefits over the state of the art: our approach is simple, fast, stable and attains orders of magnitude reduction in work-normalised variance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#29992;&#20110;&#35821;&#38899;&#24773;&#32490;&#35782;&#21035;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#21508;&#31181;PEFT&#36866;&#37197;&#22120;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#20998;&#31867;&#31163;&#25955;&#24773;&#32490;&#31867;&#21035;&#21644;&#39044;&#27979;&#24773;&#32490;&#23646;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#21516;&#26102;&#36890;&#36807;&#20943;&#23569;&#21487;&#35757;&#32451;&#21442;&#25968;&#25968;&#37327;&#36229;&#36234;&#20102;&#23436;&#25972;&#24494;&#35843;&#12290;&#36824;&#25552;&#20986;&#20102;&#20004;&#38454;&#27573;&#36866;&#24212;&#31574;&#30053;&#20197;&#25552;&#39640;&#27169;&#22411;&#23545;&#33258;&#28982;&#24773;&#24863;&#34920;&#36798;&#30340;&#25429;&#25417;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.11747</link><description>&lt;p&gt;
&#35821;&#38899;&#24773;&#32490;&#35782;&#21035;&#21644;&#39046;&#22495;&#36866;&#24212;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Parameter Efficient Finetuning for Speech Emotion Recognition and Domain Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11747
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29992;&#20110;&#35821;&#38899;&#24773;&#32490;&#35782;&#21035;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#21508;&#31181;PEFT&#36866;&#37197;&#22120;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#20998;&#31867;&#31163;&#25955;&#24773;&#32490;&#31867;&#21035;&#21644;&#39044;&#27979;&#24773;&#32490;&#23646;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#21516;&#26102;&#36890;&#36807;&#20943;&#23569;&#21487;&#35757;&#32451;&#21442;&#25968;&#25968;&#37327;&#36229;&#36234;&#20102;&#23436;&#25972;&#24494;&#35843;&#12290;&#36824;&#25552;&#20986;&#20102;&#20004;&#38454;&#27573;&#36866;&#24212;&#31574;&#30053;&#20197;&#25552;&#39640;&#27169;&#22411;&#23545;&#33258;&#28982;&#24773;&#24863;&#34920;&#36798;&#30340;&#25429;&#25417;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#22312;&#35821;&#38899;&#24773;&#32490;&#35782;&#21035;&#65288;SER&#65289;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#37492;&#20110;&#24773;&#32490;&#35821;&#26009;&#24211;&#20013;&#30340;&#25968;&#25454;&#26377;&#38480;&#65292;&#23545;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#25152;&#26377;&#21442;&#25968;&#36827;&#34892;&#24494;&#35843;&#26082;&#32791;&#36153;&#36164;&#28304;&#21448;&#23481;&#26131;&#36807;&#25311;&#21512;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#29992;&#20110;SER&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#12290;&#31995;&#32479;&#30740;&#31350;&#20102;&#21508;&#31181;PEFT&#36866;&#37197;&#22120;&#65292;&#26088;&#22312;&#23545;&#31163;&#25955;&#24773;&#32490;&#31867;&#21035;&#30340;&#20998;&#31867;&#21644;&#24773;&#32490;&#23646;&#24615;&#30340;&#32500;&#24230;&#39044;&#27979;&#36827;&#34892;&#30740;&#31350;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;PEFT&#26041;&#27861;&#30340;&#32452;&#21512;&#36229;&#36234;&#20102;&#20855;&#26377;&#21487;&#35757;&#32451;&#21442;&#25968;&#25968;&#37327;&#26174;&#33879;&#20943;&#23569;&#30340;&#23436;&#25972;&#24494;&#35843;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#36866;&#24212;&#31574;&#30053;&#65292;&#20197;&#36866;&#24212;&#22312;&#22330;&#26223;&#24773;&#32490;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#36825;&#31181;&#25968;&#25454;&#26356;&#26131;&#33719;&#24471;&#65292;&#20351;&#27169;&#22411;&#26356;&#25797;&#38271;&#25429;&#25417;&#33258;&#28982;&#24773;&#24863;&#34920;&#36798;&#12290;&#20869;&#37096;&#21644;&#36328;&#35821;&#26009;&#24211;&#23454;&#39564;&#39564;&#35777;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11747v1 Announce Type: cross  Abstract: Foundation models have shown superior performance for speech emotion recognition (SER). However, given the limited data in emotion corpora, finetuning all parameters of large pre-trained models for SER can be both resource-intensive and susceptible to overfitting. This paper investigates parameter-efficient finetuning (PEFT) for SER. Various PEFT adaptors are systematically studied for both classification of discrete emotion categories and prediction of dimensional emotional attributes. The results demonstrate that the combination of PEFT methods surpasses full finetuning with a significant reduction in the number of trainable parameters. Furthermore, a two-stage adaptation strategy is proposed to adapt models trained on acted emotion data, which is more readily available, to make the model more adept at capturing natural emotional expressions. Both intra- and cross-corpus experiments validate the efficacy of the proposed approach in e
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20809;&#35889;&#19981;&#24179;&#34913;&#30340;&#27010;&#24565;&#20316;&#20026;&#23548;&#33268;&#31867;&#21035;&#24046;&#24322;&#30340;&#28508;&#22312;&#26469;&#28304;&#65292;&#24182;&#30740;&#31350;&#20102;&#20809;&#35889;&#19981;&#24179;&#34913;&#19982;&#31867;&#21035;&#20559;&#35265;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#20026;&#29702;&#35770;&#21644;&#23454;&#36341;&#20013;&#30340;&#31867;&#21035;&#24046;&#24322;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#24182;&#22312;&#22810;&#20010;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;&#20013;&#39564;&#35777;&#20102;&#36825;&#31181;&#32852;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.11742</link><description>&lt;p&gt;
&#24179;&#34913;&#25968;&#25454;&#65292;&#19981;&#24179;&#34913;&#20809;&#35889;&#65306;&#25581;&#31034;&#20855;&#26377;&#20809;&#35889;&#19981;&#24179;&#34913;&#30340;&#31867;&#21035;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Balanced Data, Imbalanced Spectra: Unveiling Class Disparities with Spectral Imbalance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11742
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20809;&#35889;&#19981;&#24179;&#34913;&#30340;&#27010;&#24565;&#20316;&#20026;&#23548;&#33268;&#31867;&#21035;&#24046;&#24322;&#30340;&#28508;&#22312;&#26469;&#28304;&#65292;&#24182;&#30740;&#31350;&#20102;&#20809;&#35889;&#19981;&#24179;&#34913;&#19982;&#31867;&#21035;&#20559;&#35265;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#20026;&#29702;&#35770;&#21644;&#23454;&#36341;&#20013;&#30340;&#31867;&#21035;&#24046;&#24322;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#24182;&#22312;&#22810;&#20010;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;&#20013;&#39564;&#35777;&#20102;&#36825;&#31181;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#31867;&#27169;&#22411;&#34987;&#26399;&#26395;&#22312;&#19981;&#21516;&#31867;&#21035;&#19978;&#34920;&#29616;&#21516;&#26679;&#33391;&#22909;&#65292;&#20294;&#22312;&#23454;&#36341;&#20013;&#65292;&#23427;&#20204;&#30340;&#34920;&#29616;&#24448;&#24448;&#23384;&#22312;&#36739;&#22823;&#24046;&#36317;&#12290;&#36825;&#20010;&#31867;&#21035;&#20559;&#35265;&#38382;&#39064;&#22312;&#26679;&#26412;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#22312;&#24179;&#34913;&#25968;&#25454;&#38598;&#20013;&#30456;&#23545;&#34987;&#24573;&#35270;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29305;&#24449;&#20013;&#30340;&#20809;&#35889;&#19981;&#24179;&#34913;&#27010;&#24565;&#20316;&#20026;&#31867;&#21035;&#24046;&#24322;&#30340;&#28508;&#22312;&#26469;&#28304;&#65292;&#24182;&#30740;&#31350;&#20809;&#35889;&#19981;&#24179;&#34913;&#19982;&#31867;&#21035;&#20559;&#35265;&#22312;&#29702;&#35770;&#21644;&#23454;&#36341;&#20013;&#30340;&#32852;&#31995;&#12290;&#20026;&#20102;&#24314;&#31435;&#20809;&#35889;&#19981;&#24179;&#34913;&#19982;&#31867;&#21035;&#24046;&#36317;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#29992;&#20110;&#30740;&#31350;&#31867;&#21035;&#24046;&#24322;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#24182;&#25512;&#23548;&#20102;&#39640;&#32500;&#28151;&#21512;&#27169;&#22411;&#35774;&#23450;&#20013;&#27599;&#31867;&#38169;&#35823;&#30340;&#31934;&#30830;&#34920;&#36798;&#24335;&#12290;&#28982;&#21518;&#25105;&#20204;&#22312;11&#20010;&#19981;&#21516;&#30340;&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;&#20013;&#30740;&#31350;&#20102;&#36825;&#19968;&#29616;&#35937;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#22914;&#20309;&#29992;&#20110;&#27604;&#36739;&#32534;&#30721;&#22120;&#30340;&#36136;&#37327;&#65292;&#20197;&#21450;&#35780;&#20272;&#21644;&#32452;&#21512;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11742v1 Announce Type: new  Abstract: Classification models are expected to perform equally well for different classes, yet in practice, there are often large gaps in their performance. This issue of class bias is widely studied in cases of datasets with sample imbalance, but is relatively overlooked in balanced datasets. In this work, we introduce the concept of spectral imbalance in features as a potential source for class disparities and study the connections between spectral imbalance and class bias in both theory and practice. To build the connection between spectral imbalance and class gap, we develop a theoretical framework for studying class disparities and derive exact expressions for the per-class error in a high-dimensional mixture model setting. We then study this phenomenon in 11 different state-of-the-art pretrained encoders and show how our proposed framework can be used to compare the quality of encoders, as well as evaluate and combine data augmentation stra
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20351;&#29992;Koopman&#31639;&#23376;&#25552;&#21462;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#38750;&#32447;&#24615;&#65292;&#35777;&#26126;&#20102;&#21463;&#38480;&#38750;&#32447;&#24615;&#24050;&#36275;&#22815;&#36827;&#34892;&#25163;&#20889;&#25968;&#23383;&#20998;&#31867;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#65292;&#33021;&#22312;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#20013;&#22788;&#29702;&#22823;&#22411;&#32593;&#32476;&#12290;</title><link>https://arxiv.org/abs/2402.11740</link><description>&lt;p&gt;
&#21033;&#29992;Koopman&#31639;&#23376;&#25552;&#21462;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#38750;&#32447;&#24615;&#24182;&#36827;&#34892;&#27169;&#22411;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Extraction of nonlinearity in neural networks and model compression with Koopman operator
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11740
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20351;&#29992;Koopman&#31639;&#23376;&#25552;&#21462;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#38750;&#32447;&#24615;&#65292;&#35777;&#26126;&#20102;&#21463;&#38480;&#38750;&#32447;&#24615;&#24050;&#36275;&#22815;&#36827;&#34892;&#25163;&#20889;&#25968;&#23383;&#20998;&#31867;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#65292;&#33021;&#22312;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#20013;&#22788;&#29702;&#22823;&#22411;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#32447;&#24615;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#35843;&#26597;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#38750;&#32447;&#24615;&#23545;&#20998;&#31867;&#20219;&#21153;&#30340;&#20851;&#38190;&#31243;&#24230;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#37319;&#29992;Koopman&#31639;&#23376;&#12289;&#25193;&#23637;&#21160;&#24577;&#27169;&#24577;&#20998;&#35299;&#21644;&#24352;&#37327;&#21015;&#36710;&#26684;&#24335;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21463;&#38480;&#38750;&#32447;&#24615;&#24050;&#32463;&#36275;&#20197;&#36827;&#34892;&#25163;&#20889;&#25968;&#23383;&#30340;&#20998;&#31867;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#65292;&#36825;&#23545;&#20110;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#29615;&#22659;&#20013;&#22788;&#29702;&#22823;&#22411;&#32593;&#32476;&#21487;&#33021;&#26159;&#26377;&#30410;&#30340;&#12290;&#21033;&#29992;Koopman&#31639;&#23376;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#25105;&#20204;&#21487;&#20197;&#22312;&#31070;&#32463;&#32593;&#32476;&#30340;&#20869;&#37096;&#22788;&#29702;&#20013;&#20351;&#29992;&#32447;&#24615;&#20195;&#25968;&#12290;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#39640;&#24230;&#21387;&#32553;&#27169;&#22411;&#35774;&#32622;&#19979;&#22312;&#25163;&#20889;&#25968;&#23383;&#35782;&#21035;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#35201;&#20040;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#24403;&#65292;&#35201;&#20040;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11740v1 Announce Type: new  Abstract: Nonlinearity plays a crucial role in deep neural networks. In this paper, we first investigate the degree to which the nonlinearity of the neural network is essential. For this purpose, we employ the Koopman operator, extended dynamic mode decomposition, and the tensor-train format. The results imply that restricted nonlinearity is enough for the classification of handwritten numbers. Then, we propose a model compression method for deep neural networks, which could be beneficial to handling large networks in resource-constrained environments. Leveraging the Koopman operator, the proposed method enables us to use linear algebra in the internal processing of neural networks. We numerically show that the proposed method performs comparably or better than conventional methods in highly compressed model settings for the handwritten number recognition task.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#36807;&#28193;&#31995;&#32479;&#25277;&#35937;&#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#21160;&#21147;&#31995;&#32479;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#36890;&#36807;&#20154;&#31867;&#25163;&#20889;&#21160;&#21147;&#23398;&#23398;&#20064;&#21644;&#39564;&#35777;&#24212;&#29992;&#36827;&#34892;&#39564;&#35777;&#12290;</title><link>https://arxiv.org/abs/2402.11739</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#21160;&#21147;&#31995;&#32479;&#27169;&#22411;&#30340;&#36807;&#28193;&#31995;&#32479;&#25277;&#35937;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Transition System Abstraction Framework for Neural Network Dynamical System Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11739
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#36807;&#28193;&#31995;&#32479;&#25277;&#35937;&#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#21160;&#21147;&#31995;&#32479;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#36890;&#36807;&#20154;&#31867;&#25163;&#20889;&#21160;&#21147;&#23398;&#23398;&#20064;&#21644;&#39564;&#35777;&#24212;&#29992;&#36827;&#34892;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#21160;&#21147;&#31995;&#32479;&#27169;&#22411;&#30340;&#36807;&#28193;&#31995;&#32479;&#25277;&#35937;&#26694;&#26550;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#24212;&#29992;&#20110;&#22797;&#26434;&#21160;&#21147;&#31995;&#32479;&#65292;&#22914;&#20154;&#31867;&#34892;&#20026;&#23398;&#20064;&#21644;&#39564;&#35777;&#12290;&#39318;&#20808;&#65292;&#23558;&#23616;&#37096;&#24037;&#20316;&#21306;&#22495;&#26681;&#25454;&#25968;&#25454;&#39537;&#21160;&#30340;&#26368;&#22823;&#29109;&#65288;ME&#65289;&#21010;&#20998;&#26041;&#27861;&#20998;&#21106;&#20026;&#22810;&#20010;&#23616;&#37096;&#20998;&#21306;&#12290;&#28982;&#21518;&#65292;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#38598;&#20540;&#21487;&#36798;&#24615;&#20998;&#26512;&#33719;&#24471;&#36807;&#28193;&#30697;&#38453;&#12290;&#26368;&#21518;&#65292;&#32473;&#20986;&#20102;&#24212;&#29992;&#20110;&#20154;&#31867;&#25163;&#20889;&#21160;&#21147;&#23398;&#23398;&#20064;&#21644;&#39564;&#35777;&#30340;&#24212;&#29992;&#65292;&#20197;&#39564;&#35777;&#25105;&#20204;&#25552;&#20986;&#30340;&#25277;&#35937;&#26694;&#26550;&#65292;&#23637;&#31034;&#20102;&#22686;&#24378;&#40657;&#30418;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#30340;&#20248;&#21183;&#65292;&#21363;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#33021;&#22815;&#23558;&#25968;&#25454;&#39537;&#21160;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#25277;&#35937;&#20026;&#19968;&#20010;&#36807;&#28193;&#31995;&#32479;&#65292;&#36890;&#36807;&#39564;&#35777;&#22312;&#35745;&#31639;&#20013;&#25551;&#36848;&#30340;&#35268;&#33539;&#20351;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21464;&#24471;&#21487;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11739v1 Announce Type: cross  Abstract: This paper proposes a transition system abstraction framework for neural network dynamical system models to enhance the model interpretability, with applications to complex dynamical systems such as human behavior learning and verification. To begin with, the localized working zone will be segmented into multiple localized partitions under the data-driven Maximum Entropy (ME) partitioning method. Then, the transition matrix will be obtained based on the set-valued reachability analysis of neural networks. Finally, applications to human handwriting dynamics learning and verification are given to validate our proposed abstraction framework, which demonstrates the advantages of enhancing the interpretability of the black-box model, i.e., our proposed framework is able to abstract a data-driven neural network model into a transition system, making the neural network model interpretable through verifying specifications described in Computat
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#31561;&#25928;&#35780;&#20272;&#30340;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#20462;&#22797;&#26041;&#27861;&#65292;&#36890;&#36807;&#35745;&#31639;&#20004;&#20010;&#31070;&#32463;&#32593;&#32476;&#20043;&#38388;&#30340;&#36755;&#20986;&#24046;&#24322;&#65292;&#21021;&#22987;&#21270;&#26032;&#30340;&#35757;&#32451;&#38598;&#24182;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#26469;&#25913;&#36827;&#21387;&#32553;&#32593;&#32476;&#24615;&#33021;</title><link>https://arxiv.org/abs/2402.11737</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#22411;&#31561;&#25928;&#35780;&#20272;&#30340;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#20462;&#22797;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Compression Repair for Feedforward Neural Networks Based on Model Equivalence Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11737
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#31561;&#25928;&#35780;&#20272;&#30340;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#20462;&#22797;&#26041;&#27861;&#65292;&#36890;&#36807;&#35745;&#31639;&#20004;&#20010;&#31070;&#32463;&#32593;&#32476;&#20043;&#38388;&#30340;&#36755;&#20986;&#24046;&#24322;&#65292;&#21021;&#22987;&#21270;&#26032;&#30340;&#35757;&#32451;&#38598;&#24182;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#26469;&#25913;&#36827;&#21387;&#32553;&#32593;&#32476;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20004;&#20010;&#31070;&#32463;&#32593;&#32476;&#31561;&#25928;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20462;&#22797;&#21387;&#32553;&#30340;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#65288;FNNs&#65289;&#12290;&#22312;&#20462;&#22797;&#26694;&#26550;&#20013;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#31561;&#25928;&#35780;&#20272;&#26041;&#27861;&#26469;&#35745;&#31639;&#20004;&#20010;&#31070;&#32463;&#32593;&#32476;&#20043;&#38388;&#30340;&#36755;&#20986;&#24046;&#24322;&#12290;&#36755;&#20986;&#24046;&#24322;&#21487;&#20197;&#23450;&#37327;&#34920;&#24449;&#21387;&#32553;&#36807;&#31243;&#20135;&#29983;&#30340;&#36755;&#20986;&#24046;&#24322;&#12290;&#26681;&#25454;&#35745;&#31639;&#24471;&#21040;&#30340;&#36755;&#20986;&#24046;&#24322;&#65292;&#20462;&#22797;&#26041;&#27861;&#39318;&#20808;&#20026;&#21387;&#32553;&#32593;&#32476;&#21021;&#22987;&#21270;&#19968;&#20010;&#26032;&#30340;&#35757;&#32451;&#38598;&#65292;&#20197;&#32553;&#23567;&#20004;&#20010;&#31070;&#32463;&#32593;&#32476;&#38388;&#30340;&#24046;&#24322;&#24182;&#25913;&#36827;&#21387;&#32553;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#22522;&#20110;&#35757;&#32451;&#38598;&#30340;&#37325;&#26032;&#35757;&#32451;&#26469;&#20462;&#22797;&#21387;&#32553;&#30340; FNN&#12290;&#25105;&#20204;&#23558;&#25152;&#24320;&#21457;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110; MNIST &#25968;&#25454;&#38598;&#65292;&#20197;&#23637;&#31034;&#25105;&#20204;&#25552;&#20986;&#30340;&#20462;&#22797;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11737v1 Announce Type: cross  Abstract: In this paper, we propose a method of repairing compressed Feedforward Neural Networks (FNNs) based on equivalence evaluation of two neural networks. In the repairing framework, a novel neural network equivalence evaluation method is developed to compute the output discrepancy between two neural networks. The output discrepancy can quantitatively characterize the output difference produced by compression procedures. Based on the computed output discrepancy, the repairing method first initializes a new training set for the compressed networks to narrow down the discrepancy between the two neural networks and improve the performance of the compressed network. Then, we repair the compressed FNN by re-training based on the training set. We apply our developed method to the MNIST dataset to demonstrate the effectiveness and advantages of our proposed repair method.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#32852;&#21512;&#27010;&#29575;&#20998;&#24067;&#65292;&#20854;&#25903;&#25345;&#36235;&#20110;&#26368;&#23567;&#21270;&#26368;&#22351;&#24773;&#20917;&#35823;&#24046;&#65292;&#35777;&#26126;&#20102;&#23427;&#22312;&#26368;&#22351;&#24773;&#20917;&#31215;&#20998;&#35823;&#24046;&#38598;&#20013;&#19981;&#31561;&#24335;&#19978;&#20248;&#20110;i.i.d.&#33945;&#29305;&#21345;&#32599;&#12290;</title><link>https://arxiv.org/abs/2402.11736</link><description>&lt;p&gt;
&#22522;&#20110;&#26680;Gibbs&#27979;&#24230;&#30340;&#33945;&#29305;&#21345;&#32599;&#65306;&#27010;&#29575;&#38543;&#26426;&#25918;&#29287;&#30340;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Monte Carlo with kernel-based Gibbs measures: Guarantees for probabilistic herding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11736
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#32852;&#21512;&#27010;&#29575;&#20998;&#24067;&#65292;&#20854;&#25903;&#25345;&#36235;&#20110;&#26368;&#23567;&#21270;&#26368;&#22351;&#24773;&#20917;&#35823;&#24046;&#65292;&#35777;&#26126;&#20102;&#23427;&#22312;&#26368;&#22351;&#24773;&#20917;&#31215;&#20998;&#35823;&#24046;&#38598;&#20013;&#19981;&#31561;&#24335;&#19978;&#20248;&#20110;i.i.d.&#33945;&#29305;&#21345;&#32599;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Kernel herding&#23646;&#20110;&#19968;&#31867;&#30830;&#23450;&#24615;&#30340;&#22235;&#20301;&#25968;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHS&#65289;&#19978;&#30340;&#26368;&#22351;&#24773;&#20917;&#31215;&#20998;&#35823;&#24046;&#12290;&#23613;&#31649;&#26377;&#24456;&#24378;&#30340;&#23454;&#39564;&#25903;&#25345;&#65292;&#20294;&#22312;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#21363;RKHS&#26159;&#26080;&#38480;&#32500;&#26102;&#65292;&#35777;&#26126;&#36825;&#31181;&#26368;&#22351;&#24773;&#20917;&#35823;&#24046;&#20197;&#27604;&#26631;&#20934;&#31215;&#20998;&#33410;&#28857;&#25968;&#37327;&#30340;&#24179;&#26041;&#26681;&#26356;&#24555;&#30340;&#36895;&#29575;&#20943;&#23569;&#26159;&#22256;&#38590;&#30340;&#12290;&#22312;&#36825;&#31687;&#29702;&#35770;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#20851;&#20110;&#31215;&#20998;&#33410;&#28857;&#30340;&#32852;&#21512;&#27010;&#29575;&#20998;&#24067;&#65292;&#20854;&#25903;&#25345;&#36235;&#20110;&#26368;&#23567;&#21270;&#19982;&#26680;&#25918;&#29287;&#30456;&#21516;&#30340;&#26368;&#22351;&#24773;&#20917;&#35823;&#24046;&#12290;&#25105;&#20204;&#35777;&#26126;&#23427;&#20248;&#20110;i.i.d.&#33945;&#29305;&#21345;&#32599;&#65292;&#24847;&#21619;&#30528;&#22312;&#26368;&#22351;&#24773;&#20917;&#31215;&#20998;&#35823;&#24046;&#19978;&#20855;&#26377;&#26356;&#32039;&#30340;&#38598;&#20013;&#19981;&#31561;&#24335;&#12290;&#23613;&#31649;&#23578;&#26410;&#25552;&#39640;&#36895;&#29575;&#65292;&#20294;&#36825;&#34920;&#26126;&#20102;&#30740;&#31350;Gibbs&#27979;&#24230;&#30340;&#25968;&#23398;&#24037;&#20855;&#21487;&#20197;&#24110;&#21161;&#29702;&#35299;&#26680;&#25918;&#29287;&#21450;&#20854;&#21464;&#20307;&#22312;&#35745;&#31639;&#19978;&#30340;&#25913;&#36827;&#31243;&#24230;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11736v1 Announce Type: new  Abstract: Kernel herding belongs to a family of deterministic quadratures that seek to minimize the worst-case integration error over a reproducing kernel Hilbert space (RKHS). In spite of strong experimental support, it has revealed difficult to prove that this worst-case error decreases at a faster rate than the standard square root of the number of quadrature nodes, at least in the usual case where the RKHS is infinite-dimensional. In this theoretical paper, we study a joint probability distribution over quadrature nodes, whose support tends to minimize the same worst-case error as kernel herding. We prove that it does outperform i.i.d. Monte Carlo, in the sense of coming with a tighter concentration inequality on the worst-case integration error. While not improving the rate yet, this demonstrates that the mathematical tools of the study of Gibbs measures can help understand to what extent kernel herding and its variants improve on computation
&lt;/p&gt;</description></item><item><title>FOMO&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#36890;&#36807;&#38543;&#26426;&#36951;&#24536;&#37096;&#20998;&#26435;&#37325;&#26469;&#35843;&#33410;&#20449;&#24687;&#24182;&#24378;&#35843;&#23398;&#20064;&#21487;&#27867;&#21270;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#31070;&#32463;&#32593;&#32476;&#22312;&#23545;&#25239;&#25915;&#20987;&#19979;&#20986;&#29616;&#30340;&#31283;&#20581;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.11733</link><description>&lt;p&gt;
&#38543;&#26426;&#36951;&#24536;&#23545;&#31283;&#20581;&#27867;&#21270;&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
The Effectiveness of Random Forgetting for Robust Generalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11733
&lt;/p&gt;
&lt;p&gt;
FOMO&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#36890;&#36807;&#38543;&#26426;&#36951;&#24536;&#37096;&#20998;&#26435;&#37325;&#26469;&#35843;&#33410;&#20449;&#24687;&#24182;&#24378;&#35843;&#23398;&#20064;&#21487;&#27867;&#21270;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#31070;&#32463;&#32593;&#32476;&#22312;&#23545;&#25239;&#25915;&#20987;&#19979;&#20986;&#29616;&#30340;&#31283;&#20581;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#65292;&#36825;&#21487;&#33021;&#25439;&#23475;&#23427;&#20204;&#30340;&#24615;&#33021;&#21644;&#20934;&#30830;&#24615;&#12290;&#23545;&#25239;&#35757;&#32451;&#65288;AT&#65289;&#24050;&#32463;&#25104;&#20026;&#20445;&#25252;&#31070;&#32463;&#32593;&#32476;&#20813;&#21463;&#27492;&#31867;&#25915;&#20987;&#30340;&#24120;&#35265;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;AT&#30340;&#20851;&#38190;&#25361;&#25112;&#20043;&#19968;&#26159;&#31283;&#20581;&#36807;&#25311;&#21512;&#65292;&#21363;&#32593;&#32476;&#23545;&#27979;&#35797;&#25968;&#25454;&#30340;&#31283;&#20581;&#24615;&#33021;&#38543;&#30528;&#36827;&#19968;&#27493;&#35757;&#32451;&#32780;&#24694;&#21270;&#65292;&#20174;&#32780;&#22952;&#30861;&#27867;&#21270;&#12290;&#21463;&#22823;&#33041;&#20013;&#20027;&#21160;&#36951;&#24536;&#30340;&#27010;&#24565;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#36951;&#24536;&#26469;&#20943;&#36731;&#36807;&#25311;&#21512;&#65288;FOMO&#65289;&#8221;&#30340;&#26032;&#22411;&#23398;&#20064;&#33539;&#24335;&#12290;FOMO&#22312;&#36951;&#24536;&#38454;&#27573;&#38543;&#26426;&#36951;&#24536;&#37096;&#20998;&#26435;&#37325;&#24182;&#36890;&#36807;&#26435;&#37325;&#37325;&#26032;&#21021;&#22987;&#21270;&#35843;&#33410;&#27169;&#22411;&#30340;&#20449;&#24687;&#65292;&#28982;&#21518;&#22312;&#37325;&#26032;&#23398;&#20064;&#38454;&#27573;&#24378;&#35843;&#23398;&#20064;&#21487;&#27867;&#21270;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#23545;&#25239;&#25915;&#20987;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;FOMO&#36890;&#36807;&#26174;&#33879;&#20943;&#23569;&#26368;&#20339;&#32467;&#26524;&#21644;&#26368;&#32456;&#32467;&#26524;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#32531;&#35299;&#20102;&#31283;&#20581;&#36807;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11733v1 Announce Type: cross  Abstract: Deep neural networks are susceptible to adversarial attacks, which can compromise their performance and accuracy. Adversarial Training (AT) has emerged as a popular approach for protecting neural networks against such attacks. However, a key challenge of AT is robust overfitting, where the network's robust performance on test data deteriorates with further training, thus hindering generalization. Motivated by the concept of active forgetting in the brain, we introduce a novel learning paradigm called "Forget to Mitigate Overfitting (FOMO)". FOMO alternates between the forgetting phase, which randomly forgets a subset of weights and regulates the model's information through weight reinitialization, and the relearning phase, which emphasizes learning generalizable features. Our experiments on benchmark datasets and adversarial attacks show that FOMO alleviates robust overfitting by significantly reducing the gap between the best and last
&lt;/p&gt;</description></item><item><title>Prospector heads&#26159;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#29305;&#24449;&#24402;&#22240;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#32534;&#30721;&#22120;&#21644;&#20219;&#20309;&#25968;&#25454;&#24418;&#24577;&#65292;&#24182;&#19988;&#36890;&#36807;&#23545;&#19981;&#21516;&#25968;&#25454;&#24418;&#24577;&#30340;&#23454;&#39564;&#65292;&#34920;&#29616;&#20248;&#36234;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.11729</link><description>&lt;p&gt;
Prospector Heads:&#22823;&#35268;&#27169;&#27169;&#22411;&#21644;&#25968;&#25454;&#30340;&#24191;&#20041;&#29305;&#24449;&#24402;&#22240;
&lt;/p&gt;
&lt;p&gt;
Prospector Heads: Generalized Feature Attribution for Large Models &amp; Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11729
&lt;/p&gt;
&lt;p&gt;
Prospector heads&#26159;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#29305;&#24449;&#24402;&#22240;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#32534;&#30721;&#22120;&#21644;&#20219;&#20309;&#25968;&#25454;&#24418;&#24577;&#65292;&#24182;&#19988;&#36890;&#36807;&#23545;&#19981;&#21516;&#25968;&#25454;&#24418;&#24577;&#30340;&#23454;&#39564;&#65292;&#34920;&#29616;&#20248;&#36234;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#24402;&#22240;&#26159;&#19968;&#31181;&#23450;&#20301;&#36755;&#20837;&#25968;&#25454;&#20013;&#19982;&#20998;&#31867;&#30456;&#20851;&#30340;&#21306;&#22495;&#30340;&#33021;&#21147;&#65292;&#23545;&#20110;&#31185;&#23398;&#21644;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#32780;&#35328;&#65292;&#36825;&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#33021;&#21147;&#12290;&#24403;&#21069;&#30340;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#20381;&#36182;&#20110;&#8220;&#35299;&#37322;&#8221;&#31471;&#21040;&#31471;&#20998;&#31867;&#22120;&#30340;&#39044;&#27979;&#65292;&#23384;&#22312;&#29305;&#24449;&#23450;&#20301;&#19981;&#31934;&#30830;&#20197;&#21450;&#30001;&#20110;&#35745;&#31639;&#25361;&#25112;&#32780;&#26080;&#27861;&#22312;&#23567;&#26679;&#26412;&#23610;&#23544;&#21644;&#39640;&#32500;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#25506;&#23547;&#32773;&#22836;&#37096;&#65288;prospector heads&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#29305;&#24449;&#24402;&#22240;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#32534;&#30721;&#22120;&#21644;&#20219;&#20309;&#25968;&#25454;&#24418;&#24577;&#12290;&#36890;&#36807;&#23545;&#24207;&#21015;&#65288;&#25991;&#26412;&#65289;&#12289;&#22270;&#20687;&#65288;&#30149;&#29702;&#23398;&#65289;&#21644;&#22270;&#65288;&#34507;&#30333;&#36136;&#32467;&#26500;&#65289;&#30340;&#23454;&#39564;&#65292;&#25506;&#23547;&#32773;&#22836;&#37096;&#22312;&#27169;&#24577;&#20043;&#38388;&#36827;&#34892;&#27010;&#25324;&#65292;&#34920;&#29616;&#20248;&#20110;&#22522;&#32447;&#24402;&#22240;&#26041;&#27861;&#65292;&#24179;&#22343;&#23616;&#37096;&#21270;AUPRC&#24471;&#20998;&#25552;&#21319;&#20102;&#39640;&#36798;49&#28857;&#12290;&#25105;&#20204;&#36824;&#28436;&#31034;&#20102;&#25506;&#23547;&#32773;&#22836;&#37096;&#22914;&#20309;&#23454;&#29616;&#20102;&#25913;&#36827;&#30340;&#35299;&#37322;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11729v1 Announce Type: cross  Abstract: Feature attribution, the ability to localize regions of the input data that are relevant for classification, is an important capability for machine learning models in scientific and biomedical domains. Current methods for feature attribution, which rely on "explaining" the predictions of end-to-end classifiers, suffer from imprecise feature localization and are inadequate for use with small sample sizes and high-dimensional datasets due to computational challenges. We introduce prospector heads, an efficient and interpretable alternative to explanation-based methods for feature attribution that can be applied to any encoder and any data modality. Prospector heads generalize across modalities through experiments on sequences (text), images (pathology), and graphs (protein structures), outperforming baseline attribution methods by up to 49 points in mean localization AUPRC. We also demonstrate how prospector heads enable improved interpr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20998;&#26512;&#24072;&#25253;&#21578;&#21644;&#30408;&#21033;&#30005;&#35805;&#20013;&#30340;&#32034;&#36180;&#23545;&#37329;&#34701;&#24066;&#22330;&#22238;&#25253;&#30340;&#24433;&#21709;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#37329;&#34701;&#25968;&#25454;&#38598;&#29992;&#20110;&#32034;&#36180;&#26816;&#27979;&#20219;&#21153;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#20837;&#20027;&#39064;&#19987;&#23478;&#30693;&#35782;&#30340;&#26032;&#22411;&#24369;&#30417;&#30563;&#27169;&#22411;&#65292;&#36890;&#36807;&#26500;&#24314;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#8220;&#20048;&#35266;&#20027;&#20041;&#8221;&#23637;&#31034;&#20102;&#27169;&#22411;&#30340;&#23454;&#38469;&#25928;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.11728</link><description>&lt;p&gt;
&#37329;&#34701;&#39046;&#22495;&#30340;&#25968;&#23383;&#21270;&#32034;&#36180;&#26816;&#27979;&#65306;&#19968;&#20010;&#26032;&#30340;&#37329;&#34701;&#25968;&#25454;&#38598;&#12289;&#24369;&#30417;&#30563;&#27169;&#22411;&#21644;&#24066;&#22330;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Numerical Claim Detection in Finance: A New Financial Dataset, Weak-Supervision Model, and Market Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11728
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20998;&#26512;&#24072;&#25253;&#21578;&#21644;&#30408;&#21033;&#30005;&#35805;&#20013;&#30340;&#32034;&#36180;&#23545;&#37329;&#34701;&#24066;&#22330;&#22238;&#25253;&#30340;&#24433;&#21709;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#37329;&#34701;&#25968;&#25454;&#38598;&#29992;&#20110;&#32034;&#36180;&#26816;&#27979;&#20219;&#21153;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#20837;&#20027;&#39064;&#19987;&#23478;&#30693;&#35782;&#30340;&#26032;&#22411;&#24369;&#30417;&#30563;&#27169;&#22411;&#65292;&#36890;&#36807;&#26500;&#24314;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#8220;&#20048;&#35266;&#20027;&#20041;&#8221;&#23637;&#31034;&#20102;&#27169;&#22411;&#30340;&#23454;&#38469;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20998;&#26512;&#24072;&#25253;&#21578;&#21644;&#30408;&#21033;&#30005;&#35805;&#20013;&#30340;&#32034;&#36180;&#23545;&#37329;&#34701;&#24066;&#22330;&#22238;&#25253;&#30340;&#24433;&#21709;&#65292;&#23558;&#23427;&#20204;&#35270;&#20026;&#19978;&#24066;&#20844;&#21496;&#37325;&#35201;&#30340;&#23395;&#24230;&#20107;&#20214;&#12290;&#20026;&#20102;&#36827;&#34892;&#20840;&#38754;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#37329;&#34701;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#37329;&#34701;&#39046;&#22495;&#30340;&#32034;&#36180;&#26816;&#27979;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#23545;&#21508;&#31181;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#20837;&#20027;&#39064;&#19987;&#23478;&#65288;SMEs&#65289;&#30693;&#35782;&#30340;&#26032;&#22411;&#24369;&#30417;&#30563;&#27169;&#22411;&#65292;&#22312;&#32858;&#21512;&#20989;&#25968;&#20013;&#36229;&#36234;&#20102;&#29616;&#26377;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#26500;&#24314;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#8220;&#20048;&#35266;&#20027;&#20041;&#8221;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#30340;&#23454;&#38469;&#25928;&#29992;&#12290;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#30408;&#21033;&#24778;&#21916;&#21644;&#22238;&#25253;&#23545;&#25105;&#20204;&#30340;&#20048;&#35266;&#20027;&#20041;&#24230;&#37327;&#30340;&#20381;&#36182;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#21644;&#20195;&#30721;&#23558;&#22312;GitHub&#21644;Hugging Face&#19978;&#20844;&#24320;&#65288;&#36981;&#24490;CC BY 4.0&#35768;&#21487;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11728v1 Announce Type: new  Abstract: In this paper, we investigate the influence of claims in analyst reports and earnings calls on financial market returns, considering them as significant quarterly events for publicly traded companies. To facilitate a comprehensive analysis, we construct a new financial dataset for the claim detection task in the financial domain. We benchmark various language models on this dataset and propose a novel weak-supervision model that incorporates the knowledge of subject matter experts (SMEs) in the aggregation function, outperforming existing approaches. Furthermore, we demonstrate the practical utility of our proposed model by constructing a novel measure ``optimism". Furthermore, we observed the dependence of earnings surprise and return on our optimism measure. Our dataset, models, and code will be made publicly (under CC BY 4.0 license) available on GitHub and Hugging Face.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21487;&#36870;&#20613;&#31435;&#21494;&#31070;&#32463;&#31639;&#23376;(iFNO)&#65292;&#36890;&#36807;&#35774;&#35745;&#21487;&#36870;&#20613;&#31435;&#21494;&#22359;&#21644;&#38598;&#25104;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#23454;&#29616;&#21516;&#26102;&#22788;&#29702;&#21069;&#21521;&#19982;&#21453;&#21521;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#20026;&#21452;&#21521;&#20219;&#21153;&#30340;&#23398;&#20064;&#25552;&#20379;&#26377;&#25928;&#30340;&#21442;&#25968;&#20849;&#20139;&#21644;&#20449;&#24687;&#20132;&#25442;&#65292;&#20811;&#26381;&#20102;&#19981;&#36866;&#23450;&#24615;&#12289;&#25968;&#25454;&#30701;&#32570;&#21644;&#22122;&#22768;&#31561;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.11722</link><description>&lt;p&gt;
&#21487;&#36870;&#20613;&#31435;&#21494;&#31070;&#32463;&#31639;&#23376;&#22788;&#29702;&#21069;&#21521;&#21644;&#21453;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Invertible Fourier Neural Operators for Tackling Both Forward and Inverse Problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11722
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21487;&#36870;&#20613;&#31435;&#21494;&#31070;&#32463;&#31639;&#23376;(iFNO)&#65292;&#36890;&#36807;&#35774;&#35745;&#21487;&#36870;&#20613;&#31435;&#21494;&#22359;&#21644;&#38598;&#25104;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#23454;&#29616;&#21516;&#26102;&#22788;&#29702;&#21069;&#21521;&#19982;&#21453;&#21521;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#20026;&#21452;&#21521;&#20219;&#21153;&#30340;&#23398;&#20064;&#25552;&#20379;&#26377;&#25928;&#30340;&#21442;&#25968;&#20849;&#20139;&#21644;&#20449;&#24687;&#20132;&#25442;&#65292;&#20811;&#26381;&#20102;&#19981;&#36866;&#23450;&#24615;&#12289;&#25968;&#25454;&#30701;&#32570;&#21644;&#22122;&#22768;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20613;&#31435;&#21494;&#31070;&#32463;&#31639;&#23376;&#65288;FNO&#65289;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#31639;&#23376;&#23398;&#20064;&#26041;&#27861;&#65292;&#24050;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;FNO&#20027;&#35201;&#29992;&#20110;&#21069;&#21521;&#39044;&#27979;&#65292;&#32780;&#35768;&#22810;&#24212;&#29992;&#31243;&#24207;&#20381;&#36182;&#20110;&#35299;&#20915;&#21453;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#36870;&#20613;&#31435;&#21494;&#31070;&#32463;&#31639;&#23376;&#65288;iFNO&#65289;&#65292;&#26088;&#22312;&#35299;&#20915;&#21069;&#21521;&#21644;&#21453;&#21521;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#28508;&#22312;&#36890;&#36947;&#31354;&#38388;&#20013;&#35774;&#35745;&#20102;&#19968;&#31995;&#21015;&#21487;&#36870;&#20613;&#31435;&#21494;&#22359;&#65292;&#20197;&#20998;&#20139;&#27169;&#22411;&#21442;&#25968;&#65292;&#26377;&#25928;&#20132;&#25442;&#20449;&#24687;&#65292;&#24182;&#30456;&#20114;&#27491;&#35268;&#21270;&#21452;&#21521;&#20219;&#21153;&#30340;&#23398;&#20064;&#12290;&#25105;&#20204;&#38598;&#25104;&#20102;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#20197;&#25429;&#33719;&#36755;&#20837;&#31354;&#38388;&#20869;&#22312;&#32467;&#26500;&#65292;&#24182;&#23454;&#29616;&#21518;&#39564;&#25512;&#26029;&#65292;&#20197;&#20811;&#26381;&#19981;&#36866;&#23450;&#24615;&#12289;&#25968;&#25454;&#30701;&#32570;&#12289;&#22122;&#22768;&#31561;&#25361;&#25112;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#19977;&#27493;&#36807;&#31243;&#65292;&#29992;&#20110;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#20197;&#23454;&#29616;&#39640;&#25928;&#35757;&#32451;&#12290;&#23545;&#20116;&#20010;&#22522;&#20934;&#38382;&#39064;&#30340;&#35780;&#20272;&#24050;&#32463;&#35777;&#26126;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11722v1 Announce Type: new  Abstract: Fourier Neural Operator (FNO) is a popular operator learning method, which has demonstrated state-of-the-art performance across many tasks. However, FNO is mainly used in forward prediction, yet a large family of applications rely on solving inverse problems. In this paper, we propose an invertible Fourier Neural Operator (iFNO) that tackles both the forward and inverse problems. We designed a series of invertible Fourier blocks in the latent channel space to share the model parameters, efficiently exchange the information, and mutually regularize the learning for the bi-directional tasks. We integrated a variational auto-encoder to capture the intrinsic structures within the input space and to enable posterior inference so as to overcome challenges of illposedness, data shortage, noises, etc. We developed a three-step process for pre-training and fine tuning for efficient training. The evaluations on five benchmark problems have demonst
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#23398;&#20064;&#24191;&#20041;&#26391;&#20043;&#19975;&#26041;&#31243;&#20013;&#35760;&#24518;&#26680;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#27491;&#21017;&#21270;Prony&#26041;&#27861;&#20272;&#35745;&#30456;&#20851;&#20989;&#25968;&#24182;&#22312;Sobolev&#33539;&#25968;Loss&#20989;&#25968;&#21644;RKHS&#27491;&#21017;&#21270;&#19979;&#23454;&#29616;&#22238;&#24402;&#65292;&#22312;&#25351;&#25968;&#21152;&#26435;&#30340;$L^2$&#31354;&#38388;&#20869;&#33719;&#24471;&#25913;&#36827;&#24615;&#33021;&#65292;&#23545;&#27604;&#20854;&#20182;&#22238;&#24402;&#20272;&#35745;&#22120;&#23637;&#31034;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.11705</link><description>&lt;p&gt;
&#22312;&#24191;&#20041;&#26391;&#20043;&#19975;&#26041;&#31243;&#20013;&#23398;&#20064;&#35760;&#24518;&#26680;
&lt;/p&gt;
&lt;p&gt;
Learning Memory Kernels in Generalized Langevin Equations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11705
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#23398;&#20064;&#24191;&#20041;&#26391;&#20043;&#19975;&#26041;&#31243;&#20013;&#35760;&#24518;&#26680;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#27491;&#21017;&#21270;Prony&#26041;&#27861;&#20272;&#35745;&#30456;&#20851;&#20989;&#25968;&#24182;&#22312;Sobolev&#33539;&#25968;Loss&#20989;&#25968;&#21644;RKHS&#27491;&#21017;&#21270;&#19979;&#23454;&#29616;&#22238;&#24402;&#65292;&#22312;&#25351;&#25968;&#21152;&#26435;&#30340;$L^2$&#31354;&#38388;&#20869;&#33719;&#24471;&#25913;&#36827;&#24615;&#33021;&#65292;&#23545;&#27604;&#20854;&#20182;&#22238;&#24402;&#20272;&#35745;&#22120;&#23637;&#31034;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#24191;&#20041;&#26391;&#20043;&#19975;&#26041;&#31243;&#20013;&#30340;&#35760;&#24518;&#26680;&#12290;&#35813;&#26041;&#27861;&#26368;&#21021;&#21033;&#29992;&#27491;&#21017;&#21270;Prony&#26041;&#27861;&#20174;&#36712;&#36857;&#25968;&#25454;&#20013;&#20272;&#35745;&#30456;&#20851;&#20989;&#25968;&#65292;&#28982;&#21518;&#36890;&#36807;&#22522;&#20110;Sobolev&#33539;&#25968;&#30340;&#22238;&#24402;&#21644;RKHS&#27491;&#21017;&#21270;&#26469;&#36827;&#34892;&#22238;&#24402;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20445;&#35777;&#22312;&#25351;&#25968;&#21152;&#26435;&#30340;$L^2$&#31354;&#38388;&#20869;&#33719;&#24471;&#20102;&#25913;&#36827;&#30340;&#24615;&#33021;&#65292;&#26680;&#20272;&#35745;&#35823;&#24046;&#21463;&#25511;&#20110;&#20272;&#35745;&#30456;&#20851;&#20989;&#25968;&#30340;&#35823;&#24046;&#12290;&#25105;&#20204;&#36890;&#36807;&#25968;&#20540;&#31034;&#20363;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#20272;&#35745;&#22120;&#30456;&#23545;&#20110;&#20381;&#36182;&#20110;$L^2$&#25439;&#22833;&#20989;&#25968;&#30340;&#20854;&#20182;&#22238;&#24402;&#20272;&#35745;&#22120;&#20197;&#21450;&#20174;&#36870;&#25289;&#26222;&#25289;&#26031;&#21464;&#25442;&#25512;&#23548;&#20986;&#30340;&#20272;&#35745;&#22120;&#30340;&#20248;&#36234;&#24615;&#65292;&#36825;&#20123;&#31034;&#20363;&#31361;&#26174;&#20102;&#25105;&#20204;&#30340;&#20272;&#35745;&#22120;&#22312;&#21508;&#31181;&#26435;&#37325;&#21442;&#25968;&#36873;&#25321;&#19978;&#30340;&#25345;&#32493;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#21253;&#25324;&#21147;&#21644;&#28418;&#31227;&#39033;&#22312;&#26041;&#31243;&#20013;&#30340;&#24212;&#29992;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11705v1 Announce Type: cross  Abstract: We introduce a novel approach for learning memory kernels in Generalized Langevin Equations. This approach initially utilizes a regularized Prony method to estimate correlation functions from trajectory data, followed by regression over a Sobolev norm-based loss function with RKHS regularization. Our approach guarantees improved performance within an exponentially weighted $L^2$ space, with the kernel estimation error controlled by the error in estimated correlation functions. We demonstrate the superiority of our estimator compared to other regression estimators that rely on $L^2$ loss functions and also an estimator derived from the inverse Laplace transform, using numerical examples that highlight its consistent advantage across various weight parameter selections. Additionally, we provide examples that include the application of force and drift terms in the equation.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#26174;&#33879;&#33021;&#21147;&#65292;&#20294;&#30446;&#21069;&#20027;&#35201;&#29992;&#20110;&#23637;&#31034;&#27010;&#24565;&#25110;&#25552;&#20379;&#31034;&#20363;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#25165;&#33021;&#23454;&#29616;&#29983;&#20135;&#23601;&#32490;&#20195;&#30721;&#12290;</title><link>https://arxiv.org/abs/2402.11702</link><description>&lt;p&gt;
ChatGPT&#26159;&#21542;&#33021;&#25903;&#25345;&#24320;&#21457;&#32773;&#65311;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#30340;&#23454;&#35777;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Can ChatGPT Support Developers? An Empirical Evaluation of Large Language Models for Code Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11702
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#26174;&#33879;&#33021;&#21147;&#65292;&#20294;&#30446;&#21069;&#20027;&#35201;&#29992;&#20110;&#23637;&#31034;&#27010;&#24565;&#25110;&#25552;&#20379;&#31034;&#20363;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#25165;&#33021;&#23454;&#29616;&#29983;&#20135;&#23601;&#32490;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#23637;&#31034;&#20986;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#30340;&#26174;&#33879;&#33021;&#21147;&#65292;&#35768;&#22810;&#20808;&#21069;&#30340;&#30740;&#31350;&#26174;&#31034;&#23427;&#20204;&#22312;&#21508;&#31181;&#24320;&#21457;&#22330;&#26223;&#20013;&#20855;&#26377;&#24456;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#20027;&#35201;&#25552;&#20379;&#20102;&#22312;&#30740;&#31350;&#29615;&#22659;&#20013;&#30340;&#35780;&#20272;&#65292;&#36825;&#22312;&#29702;&#35299;LLM&#22312;&#23454;&#38469;&#19990;&#30028;&#20013;&#33021;&#26377;&#25928;&#25903;&#25345;&#24320;&#21457;&#32773;&#26041;&#38754;&#30041;&#19979;&#20102;&#26174;&#33879;&#30340;&#31354;&#30333;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23545;DevGPT&#20013;&#30340;&#23545;&#35805;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#65292;&#36825;&#26159;&#20174;&#24320;&#21457;&#32773;&#19982;ChatGPT&#30340;&#23545;&#35805;&#20013;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#65288;&#36890;&#36807;GitHub&#31561;&#24179;&#21488;&#19978;&#30340;Share Link&#21151;&#33021;&#25429;&#33719;&#65289;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#30446;&#21069;&#20351;&#29992;LLM&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#23454;&#36341;&#36890;&#24120;&#20165;&#38480;&#20110;&#23637;&#31034;&#39640;&#23618;&#27010;&#24565;&#25110;&#25552;&#20379;&#25991;&#26723;&#20013;&#30340;&#31034;&#20363;&#65292;&#32780;&#19981;&#26159;&#20316;&#20026;&#21487;&#29992;&#20110;&#29983;&#20135;&#30340;&#20195;&#30721;&#12290;&#36825;&#20123;&#21457;&#29616;&#34920;&#26126;&#65292;&#22312;LLM&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#36824;&#38656;&#35201;&#22823;&#37327;&#26410;&#26469;&#24037;&#20316;&#25165;&#33021;&#20351;&#20854;&#25104;&#20026;&#19981;&#21487;&#25110;&#32570;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11702v1 Announce Type: cross  Abstract: Large language models (LLMs) have demonstrated notable proficiency in code generation, with numerous prior studies showing their promising capabilities in various development scenarios. However, these studies mainly provide evaluations in research settings, which leaves a significant gap in understanding how effectively LLMs can support developers in real-world. To address this, we conducted an empirical analysis of conversations in DevGPT, a dataset collected from developers' conversations with ChatGPT (captured with the Share Link feature on platforms such as GitHub). Our empirical findings indicate that the current practice of using LLM-generated code is typically limited to either demonstrating high-level concepts or providing examples in documentation, rather than to be used as production-ready code. These findings indicate that there is much future work needed to improve LLMs in code generation before they can be integral parts o
&lt;/p&gt;</description></item><item><title>&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#21644;&#27169;&#22411;&#21704;&#23494;&#39039;&#37327;&#30340;&#23545;&#31216;&#24615;&#65292;&#35299;&#37322;&#38081;&#30913;&#20234;&#36763;&#27169;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.11701</link><description>&lt;p&gt;
&#35299;&#37322;&#20234;&#36763;&#27169;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Explaining the Machine Learning Solution of the Ising Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11701
&lt;/p&gt;
&lt;p&gt;
&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#21644;&#27169;&#22411;&#21704;&#23494;&#39039;&#37327;&#30340;&#23545;&#31216;&#24615;&#65292;&#35299;&#37322;&#38081;&#30913;&#20234;&#36763;&#27169;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#25216;&#26415;&#22312;&#35299;&#20915;&#28041;&#21450;&#22823;&#32500;&#25968;&#25454;&#30340;&#38382;&#39064;&#20013;&#38750;&#24120;&#24378;&#22823;&#65292;&#20294;&#35299;&#37322;&#20174;&#25311;&#21512;&#21442;&#25968;&#24471;&#20986;&#30340;&#32467;&#26524;&#20173;&#28982;&#26159;&#19968;&#39033;&#33267;&#20851;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#22312;&#29289;&#29702;&#24212;&#29992;&#20013;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#23454;&#29616;&#23545;&#38081;&#30913;&#20234;&#36763;&#27169;&#22411;&#30340;&#35299;&#37322;&#65292;&#36825;&#26159;&#36817;&#24180;&#26469;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#30340;&#37325;&#28857;&#23545;&#35937;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#27809;&#26377;&#20219;&#20309;&#38544;&#34255;&#23618;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#20197;&#21450;&#21704;&#23494;&#39039;&#37327;&#30340;&#23545;&#31216;&#24615;&#26469;&#25214;&#21040;&#27169;&#22411;&#36830;&#32493;&#30456;&#21464;&#30340;&#20020;&#30028;&#28201;&#24230;&#65292;&#25214;&#21040;&#20102;&#19968;&#31181;&#35299;&#37322;&#20854;&#31574;&#30053;&#30340;&#26041;&#27861;&#12290;&#36825;&#20351;&#24471;&#22312;&#23545;&#31216;&#24615;&#26410;&#30693;&#26102;&#21487;&#20197;&#39044;&#27979;&#35299;&#20915;&#38382;&#39064;&#25152;&#38656;&#30340;NN&#30340;&#26368;&#23567;&#25193;&#23637;&#65292;&#36825;&#20063;&#26159;&#21487;&#20197;&#35299;&#37322;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11701v1 Announce Type: cross  Abstract: As powerful as machine learning (ML) techniques are in solving problems involving data with large dimensionality, explaining the results from the fitted parameters remains a challenging task of utmost importance, especially in physics applications. Here it is shown how this can be accomplished for the ferromagnetic Ising model, the target of many ML studies in the last years. By using a neural network (NN) without any hidden layers and the symmetry of the Hamiltonian to find the critical temperature for the continuous phase transition of the model, an explanation of its strategy is found. This allows the prediction of the minimal extension of the NN to solve the problem when the symmetry is not known, which is also explainable.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#31363;&#21462;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#30828;&#20214;&#21464;&#21270;&#35825;&#21457;&#25200;&#21160;&#21644;&#30828;&#20214;&#19982;&#26550;&#26500;&#21464;&#21270;&#35825;&#21457;&#25200;&#21160;&#20316;&#20026;&#38450;&#24481;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.11687</link><description>&lt;p&gt;
&#35780;&#20272;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#31363;&#21462;&#25915;&#20987;&#21644;&#38450;&#24481;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating Efficacy of Model Stealing Attacks and Defenses on Quantum Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11687
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#31363;&#21462;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#30828;&#20214;&#21464;&#21270;&#35825;&#21457;&#25200;&#21160;&#21644;&#30828;&#20214;&#19982;&#26550;&#26500;&#21464;&#21270;&#35825;&#21457;&#25200;&#21160;&#20316;&#20026;&#38450;&#24481;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11687v1 &#36890;&#25253;&#31867;&#22411;&#65306;&#36328;&#39046;&#22495; &#25688;&#35201;&#65306;&#36890;&#36807;&#20113;&#25176;&#31649;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#65288;QML&#65289;&#27169;&#22411;&#20351;&#20854;&#38754;&#20020;&#19968;&#31995;&#21015;&#28431;&#27934;&#65292;&#20854;&#20013;&#26368;&#37325;&#35201;&#30340;&#26159;&#27169;&#22411;&#31363;&#21462;&#25915;&#20987;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22312;&#37327;&#23376;&#35745;&#31639;&#39046;&#22495;&#20013;&#27492;&#31867;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#23545;&#22810;&#20010;&#25968;&#25454;&#38598;&#20197;&#21450;&#22810;&#31181;QML&#27169;&#22411;&#26550;&#26500;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#20351;&#29992;Top-$1$&#21644;Top-$k$&#26631;&#31614;&#20998;&#21035;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#27169;&#22411;&#31363;&#21462;&#25915;&#20987;&#21487;&#20197;&#20135;&#29983;&#36798;&#21040;$0.9\times$&#21644;$0.99\times$&#20811;&#38534;&#27979;&#35797;&#31934;&#24230;&#30340;&#20811;&#38534;&#27169;&#22411;&#65288;$k:$ num\_classes&#65289;&#12290;&#20026;&#20102;&#38450;&#24481;&#36825;&#20123;&#25915;&#20987;&#65292;&#25105;&#20204;&#21033;&#29992;&#24403;&#21069;&#22024;&#26434;&#30828;&#20214;&#30340;&#29420;&#29305;&#29305;&#24615;&#25200;&#20081;&#21463;&#23475;&#27169;&#22411;&#36755;&#20986;&#24182;&#38459;&#30861;&#25915;&#20987;&#32773;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#65306;1&#65289;&#30828;&#20214;&#21464;&#21270;&#35825;&#21457;&#25200;&#21160;&#65288;HVIP&#65289;&#21644; 2&#65289;&#30828;&#20214;&#21644;&#26550;&#26500;&#21464;&#21270;&#35825;&#21457;&#25200;&#21160;&#65288;HAVIP&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11687v1 Announce Type: cross  Abstract: Cloud hosting of quantum machine learning (QML) models exposes them to a range of vulnerabilities, the most significant of which is the model stealing attack. In this study, we assess the efficacy of such attacks in the realm of quantum computing. We conducted comprehensive experiments on various datasets with multiple QML model architectures. Our findings revealed that model stealing attacks can produce clone models achieving up to $0.9\times$ and $0.99\times$ clone test accuracy when trained using Top-$1$ and Top-$k$ labels, respectively ($k:$ num\_classes). To defend against these attacks, we leverage the unique properties of current noisy hardware and perturb the victim model outputs and hinder the attacker's training process. In particular, we propose: 1) hardware variation-induced perturbation (HVIP) and 2) hardware and architecture variation-induced perturbation (HAVIP). Although noise and architectural variability can provide u
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#40657;&#30418;&#31163;&#25955;&#21160;&#21147;&#31995;&#32479;&#30340;&#34892;&#20026;&#21644;&#24213;&#23618;&#25299;&#25169;&#32467;&#26500;&#30340;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#36825;&#26159;&#19968;&#20010;&#35745;&#31639;&#19978;&#38590;&#20197;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#22312;&#26576;&#20123;&#26465;&#20214;&#19979;&#30340;&#39640;&#25928;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.11686</link><description>&lt;p&gt;
&#23398;&#20064;&#31163;&#25955;&#21160;&#21147;&#31995;&#32479;&#30340;&#25299;&#25169;&#32467;&#26500;&#21644;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Learning the Topology and Behavior of Discrete Dynamical Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11686
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#40657;&#30418;&#31163;&#25955;&#21160;&#21147;&#31995;&#32479;&#30340;&#34892;&#20026;&#21644;&#24213;&#23618;&#25299;&#25169;&#32467;&#26500;&#30340;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#36825;&#26159;&#19968;&#20010;&#35745;&#31639;&#19978;&#38590;&#20197;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#22312;&#26576;&#20123;&#26465;&#20214;&#19979;&#30340;&#39640;&#25928;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#25955;&#21160;&#21147;&#31995;&#32479;&#36890;&#24120;&#29992;&#20110;&#27169;&#25311;&#30495;&#23454;&#19990;&#30028;&#32593;&#32476;&#19978;&#20256;&#26579;&#30340;&#20256;&#25773;&#12290;&#29616;&#26377;&#30740;&#31350;&#24050;&#32463;&#22312;PAC&#26694;&#26550;&#19979;&#30740;&#31350;&#20102;&#22312;&#24050;&#30693;&#24213;&#23618;&#32593;&#32476;&#30340;&#26465;&#20214;&#19979;&#23398;&#20064;&#31995;&#32479;&#34892;&#20026;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#30528;&#30524;&#20110;&#19968;&#20010;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#24773;&#22659;&#65306;&#23398;&#20064;&#19968;&#20010;&#40657;&#30418;&#31995;&#32479;&#30340;&#34892;&#20026;&#21644;&#24213;&#23618;&#25299;&#25169;&#32467;&#26500;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#19968;&#33324;&#24773;&#20917;&#19979;&#65292;&#36825;&#20010;&#23398;&#20064;&#38382;&#39064;&#22312;&#35745;&#31639;&#19978;&#26159;&#38590;&#20197;&#35299;&#20915;&#30340;&#12290;&#22312;&#31215;&#26497;&#30340;&#19968;&#38754;&#65292;&#25105;&#20204;&#22312;PAC&#27169;&#22411;&#19979;&#38024;&#23545;&#26576;&#20123;&#31867;&#23646;&#20110;&#30340;&#21160;&#21147;&#31995;&#32479;&#22270;&#25552;&#20986;&#20102;&#39640;&#25928;&#30340;&#23398;&#20064;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#25918;&#26494;&#30340;&#24773;&#22659;&#65292;&#20854;&#20013;&#26410;&#30693;&#31995;&#32479;&#30340;&#25299;&#25169;&#32467;&#26500;&#37096;&#20998;&#21487;&#35266;&#27979;&#12290;&#38024;&#23545;&#36825;&#31181;&#24773;&#20917;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;PAC&#23398;&#20064;&#22120;&#26469;&#25512;&#26029;&#31995;&#32479;&#24182;&#30830;&#23450;&#26679;&#26412;&#22797;&#26434;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;&#21160;&#21147;&#31995;&#32479;&#30340;&#20551;&#35774;&#31867;&#30340;&#34920;&#36798;&#33021;&#21147;&#36827;&#34892;&#20102;&#24418;&#24335;&#21270;&#20998;&#26512;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#34892;&#20026;&#21644;&#25299;&#25169;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11686v1 Announce Type: new  Abstract: Discrete dynamical systems are commonly used to model the spread of contagions on real-world networks. Under the PAC framework, existing research has studied the problem of learning the behavior of a system, assuming that the underlying network is known. In this work, we focus on a more challenging setting: to learn both the behavior and the underlying topology of a black-box system. We show that, in general, this learning problem is computationally intractable. On the positive side, we present efficient learning methods under the PAC model when the underlying graph of the dynamical system belongs to some classes. Further, we examine a relaxed setting where the topology of an unknown system is partially observed. For this case, we develop an efficient PAC learner to infer the system and establish the sample complexity. Lastly, we present a formal analysis of the expressive power of the hypothesis class of dynamical systems where both the
&lt;/p&gt;</description></item><item><title>&#23398;&#20064;&#38750;&#20132;&#25442;&#24615;&#26465;&#20214;&#19981;&#21464;&#24615;&#26159;&#19968;&#31181;&#26356;&#20005;&#26684;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#23558;&#20248;&#21270;&#23548;&#21521;&#30446;&#26631;&#29305;&#23450;&#32534;&#30721;&#22120;&#26469;&#23454;&#29616;&#23545;&#30446;&#26631;&#39118;&#38505;&#30340;&#26356;&#20005;&#26684;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2402.11682</link><description>&lt;p&gt;
&#36890;&#36807;&#38750;&#20132;&#25442;&#24615;&#23398;&#20064;&#26465;&#20214;&#19981;&#21464;&#24615;
&lt;/p&gt;
&lt;p&gt;
Learning Conditional Invariances through Non-Commutativity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11682
&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#38750;&#20132;&#25442;&#24615;&#26465;&#20214;&#19981;&#21464;&#24615;&#26159;&#19968;&#31181;&#26356;&#20005;&#26684;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#23558;&#20248;&#21270;&#23548;&#21521;&#30446;&#26631;&#29305;&#23450;&#32534;&#30721;&#22120;&#26469;&#23454;&#29616;&#23545;&#30446;&#26631;&#39118;&#38505;&#30340;&#26356;&#20005;&#26684;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26465;&#20214;&#36807;&#28388;&#22120;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#26681;&#25454;&#25968;&#25454;&#35821;&#20041;&#32780;&#38750;&#30446;&#26631;&#22495;&#22312;&#35780;&#20272;&#20013;&#20165;&#22522;&#20110;&#30340;&#26041;&#27861;&#26469;&#36807;&#28388;&#25481;&#29305;&#23450;&#20110;&#22495;&#30340;&#38543;&#26426;&#21464;&#37327;&#20316;&#20026;&#24178;&#25200;&#22240;&#32032;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23398;&#20064;&#26465;&#20214;&#19981;&#21464;&#24615;&#30340;&#19968;&#31181;&#32463;&#36807;&#35777;&#26126;&#30340;&#26368;&#20248;&#21644;&#26679;&#26412;&#39640;&#25928;&#30340;&#26041;&#27861;&#26159;&#23558;&#19981;&#21464;&#24615;&#20934;&#21017;&#25918;&#23485;&#20026;&#38750;&#20132;&#25442;&#24615;&#65292;&#25351;&#21521;&#30446;&#26631;&#22495;&#12290;&#22312;&#23384;&#22312;&#39046;&#22495;&#19981;&#23545;&#31216;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#21363;&#24403;&#30446;&#26631;&#39046;&#22495;&#21253;&#21547;&#28304;&#39046;&#22495;&#20013;&#19981;&#21253;&#21547;&#30340;&#35821;&#20041;&#30456;&#20851;&#20449;&#24687;&#26102;&#65292;&#36328;&#39046;&#22495;&#24179;&#22343;&#26368;&#20248;&#32534;&#30721;&#22120;$\varphi^*$&#30340;&#39118;&#38505;&#20005;&#26684;&#22320;&#34987;&#30446;&#26631;&#29305;&#23450;&#26368;&#20248;&#32534;&#30721;&#22120;$\Phi^*_\tau$&#30340;&#39118;&#38505;&#19979;&#30028;&#38480;&#21046;&#12290;&#25105;&#20204;&#35777;&#26126;&#38750;&#20132;&#25442;&#24615;&#23558;&#20248;&#21270;&#23548;&#21521;$\Phi^*_\tau$&#32780;&#38750;$\varphi^*$&#65292;&#23558;&#39046;&#22495;&#20043;&#38388;&#30340;$\mathcal{H}$-&#25955;&#24230;&#38477;&#33267;&#38646;&#65292;&#20174;&#32780;&#23548;&#33268;&#23545;&#30446;&#26631;&#39118;&#38505;&#30340;&#26356;&#20005;&#26684;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#21644;&#23454;&#39564;&#22343;&#34920;&#26126;&#38750;&#20132;&#25442;&#24615;&#21487;&#20197;&#25913;&#36827;&#39118;&#38505;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11682v1 Announce Type: new  Abstract: Invariance learning algorithms that conditionally filter out domain-specific random variables as distractors, do so based only on the data semantics, and not the target domain under evaluation. We show that a provably optimal and sample-efficient way of learning conditional invariances is by relaxing the invariance criterion to be non-commutatively directed towards the target domain. Under domain asymmetry, i.e., when the target domain contains semantically relevant information absent in the source, the risk of the encoder $\varphi^*$ that is optimal on average across domains is strictly lower-bounded by the risk of the target-specific optimal encoder $\Phi^*_\tau$. We prove that non-commutativity steers the optimization towards $\Phi^*_\tau$ instead of $\varphi^*$, bringing the $\mathcal{H}$-divergence between domains down to zero, leading to a stricter bound on the target risk. Both our theory and experiments demonstrate that non-commu
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#29992;&#20110;&#27169;&#25311;&#38750;&#32447;&#24615;&#30005;&#38459;&#32593;&#32476;&#65292;&#36890;&#36807;&#23558;&#38382;&#39064;&#36716;&#21270;&#20026;&#20855;&#26377;&#32447;&#24615;&#19981;&#31561;&#24335;&#32422;&#26463;&#30340;&#20108;&#27425;&#35268;&#21010;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#24555;&#36895;&#12289;&#31934;&#30830;&#30340;&#22352;&#26631;&#19979;&#38477;&#31639;&#27861;&#36827;&#34892;&#27714;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.11674</link><description>&lt;p&gt;
&#19968;&#31181;&#24555;&#36895;&#27169;&#25311;&#38750;&#32447;&#24615;&#30005;&#38459;&#32593;&#32476;&#30340;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Fast Algorithm to Simulate Nonlinear Resistive Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11674
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#29992;&#20110;&#27169;&#25311;&#38750;&#32447;&#24615;&#30005;&#38459;&#32593;&#32476;&#65292;&#36890;&#36807;&#23558;&#38382;&#39064;&#36716;&#21270;&#20026;&#20855;&#26377;&#32447;&#24615;&#19981;&#31561;&#24335;&#32422;&#26463;&#30340;&#20108;&#27425;&#35268;&#21010;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#24555;&#36895;&#12289;&#31934;&#30830;&#30340;&#22352;&#26631;&#19979;&#38477;&#31639;&#27861;&#36827;&#34892;&#27714;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36861;&#27714;&#33021;&#25928;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#65292;&#30005;&#38459;&#32593;&#32476;&#20316;&#20026;&#26367;&#20195;&#20256;&#32479;&#22522;&#20110;GPU&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#19968;&#31181;&#26041;&#24335;&#22791;&#21463;&#20851;&#27880;&#12290;&#36825;&#20123;&#32593;&#32476;&#21033;&#29992;&#30005;&#36335;&#30340;&#29289;&#29702;&#29305;&#24615;&#36827;&#34892;&#25512;&#26029;&#65292;&#24182;&#21487;&#36890;&#36807;&#23616;&#37096;&#35757;&#32451;&#25216;&#26415;&#65288;&#22914;&#24179;&#34913;&#20256;&#25773;&#65289;&#36827;&#34892;&#20248;&#21270;&#12290;&#23613;&#31649;&#22312;&#21151;&#32791;&#26041;&#38754;&#20855;&#26377;&#28508;&#22312;&#20248;&#21183;&#65292;&#20294;&#39640;&#25928;&#27169;&#25311;&#36825;&#20123;&#30005;&#38459;&#32593;&#32476;&#30340;&#25361;&#25112;&#19968;&#30452;&#26159;&#35780;&#20272;&#20854;&#21487;&#25193;&#23637;&#24615;&#30340;&#37325;&#35201;&#29942;&#39048;&#65292;&#30446;&#21069;&#30340;&#26041;&#27861;&#35201;&#20040;&#23616;&#38480;&#20110;&#32447;&#24615;&#32593;&#32476;&#65292;&#35201;&#20040;&#20381;&#36182;&#20110;SPICE&#31561;&#29616;&#23454;&#20294;&#36895;&#24230;&#36739;&#24930;&#30340;&#30005;&#36335;&#27169;&#25311;&#22120;&#12290;&#22312;&#20551;&#23450;&#29702;&#24819;&#30005;&#36335;&#20803;&#20214;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38750;&#32447;&#24615;&#30005;&#38459;&#32593;&#32476;&#27169;&#25311;&#26041;&#27861;&#65292;&#23558;&#20854;&#26500;&#24314;&#20026;&#19968;&#20010;&#20855;&#26377;&#32447;&#24615;&#19981;&#31561;&#24335;&#32422;&#26463;&#30340;&#20108;&#27425;&#35268;&#21010;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#24555;&#36895;&#12289;&#31934;&#30830;&#30340;&#22352;&#26631;&#19979;&#38477;&#31639;&#27861;&#36827;&#34892;&#27714;&#35299;&#12290;&#25105;&#20204;&#30340;&#27169;&#25311;&#26041;&#27861;&#26174;&#33879;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11674v1 Announce Type: cross  Abstract: In the quest for energy-efficient artificial intelligence systems, resistor networks are attracting interest as an alternative to conventional GPU-based neural networks. These networks leverage the physics of electrical circuits for inference and can be optimized with local training techniques such as equilibrium propagation. Despite their potential advantage in terms of power consumption, the challenge of efficiently simulating these resistor networks has been a significant bottleneck to assess their scalability, with current methods either being limited to linear networks or relying on realistic, yet slow circuit simulators like SPICE. Assuming ideal circuit elements, we introduce a novel approach for the simulation of nonlinear resistive networks, which we frame as a quadratic programming problem with linear inequality constraints, and which we solve using a fast, exact coordinate descent algorithm. Our simulation methodology signif
&lt;/p&gt;</description></item><item><title>&#23545;&#20110;&#22312;&#20892;&#19994;&#21644;&#26519;&#19994;&#20013;CNN&#24212;&#29992;&#30340;&#24402;&#22240;&#22270;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#65292;&#21457;&#29616;&#36825;&#20123;&#22270;&#34920;&#24448;&#24448;&#26410;&#33021;&#20934;&#30830;&#31361;&#20986;&#20851;&#38190;&#29305;&#24449;&#65292;&#19982;&#39046;&#22495;&#19987;&#23478;&#35748;&#20026;&#37325;&#35201;&#30340;&#29305;&#24449;&#19981;&#19968;&#33268;&#65292;&#36825;&#24341;&#21457;&#20102;&#23545;&#20110;&#20854;&#22312;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#20915;&#31574;&#36807;&#31243;&#20013;&#23454;&#29992;&#24615;&#30340;&#37325;&#22823;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.11670</link><description>&lt;p&gt;
&#25361;&#25112;&#40657;&#21283;&#23376;&#65306;&#23545;&#22312;&#20892;&#19994;&#21644;&#26519;&#19994;&#20013;CNN&#24212;&#29992;&#30340;&#24402;&#22240;&#22270;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Challenging the Black Box: A Comprehensive Evaluation of Attribution Maps of CNN Applications in Agriculture and Forestry
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11670
&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22312;&#20892;&#19994;&#21644;&#26519;&#19994;&#20013;CNN&#24212;&#29992;&#30340;&#24402;&#22240;&#22270;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#65292;&#21457;&#29616;&#36825;&#20123;&#22270;&#34920;&#24448;&#24448;&#26410;&#33021;&#20934;&#30830;&#31361;&#20986;&#20851;&#38190;&#29305;&#24449;&#65292;&#19982;&#39046;&#22495;&#19987;&#23478;&#35748;&#20026;&#37325;&#35201;&#30340;&#29305;&#24449;&#19981;&#19968;&#33268;&#65292;&#36825;&#24341;&#21457;&#20102;&#23545;&#20110;&#20854;&#22312;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#20915;&#31574;&#36807;&#31243;&#20013;&#23454;&#29992;&#24615;&#30340;&#37325;&#22823;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#31070;&#32463;&#32593;&#32476;&#22312;&#20892;&#19994;&#21644;&#26519;&#19994;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#32933;&#26009;&#22788;&#29702;&#20998;&#31867;&#21644;&#26408;&#26448;&#35782;&#21035;&#26041;&#38754;&#12290;&#36890;&#36807;&#23545;&#26368;&#20808;&#36827;&#30340;&#24402;&#22240;&#22270;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#65292;&#20063;&#31216;&#20026;&#31867;&#28608;&#27963;&#22270;&#25110;&#26174;&#33879;&#24615;&#22270;&#65292;&#35299;&#20915;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#19981;&#36879;&#26126;&#24615;&#65292;&#36890;&#24120;&#34987;&#35748;&#20026;&#26159;&#8220;&#40657;&#21283;&#23376;&#8221;&#12290;&#25105;&#20204;&#23545;&#36825;&#20123;&#24402;&#22240;&#22270;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23450;&#24615;&#21644;&#23450;&#37327;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#20851;&#38190;&#23454;&#38469;&#23616;&#38480;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#24402;&#22240;&#22270;&#32463;&#24120;&#26080;&#27861;&#19968;&#33268;&#22320;&#31361;&#20986;&#20851;&#38190;&#29305;&#24449;&#65292;&#24182;&#19988;&#32463;&#24120;&#19982;&#39046;&#22495;&#19987;&#23478;&#35748;&#20026;&#37325;&#35201;&#30340;&#29305;&#24449;&#19981;&#19968;&#33268;&#12290;&#36825;&#20123;&#24046;&#24322;&#24341;&#21457;&#20102;&#20851;&#20110;&#24402;&#22240;&#22270;&#22312;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#20915;&#31574;&#36807;&#31243;&#20013;&#23454;&#29992;&#24615;&#30340;&#37325;&#22823;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#20892;&#19994;&#21644;&#26519;&#19994;&#37096;&#38376;&#20869;&#24402;&#22240;&#22270;&#30340;&#21487;&#20449;&#24230;&#21644;&#23454;&#29992;&#24615;&#25552;&#20379;&#20102;&#20851;&#38190;&#35265;&#35299;&#65292;&#20174;&#32780;&#20419;&#36827;&#20102;&#19968;&#31181;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11670v1 Announce Type: cross  Abstract: In this study, we explore the explainability of neural networks in agriculture and forestry, specifically in fertilizer treatment classification and wood identification. The opaque nature of these models, often considered 'black boxes', is addressed through an extensive evaluation of state-of-the-art Attribution Maps (AMs), also known as class activation maps (CAMs) or saliency maps. Our comprehensive qualitative and quantitative analysis of these AMs uncovers critical practical limitations. Findings reveal that AMs frequently fail to consistently highlight crucial features and often misalign with the features considered important by domain experts. These discrepancies raise substantial questions about the utility of AMs in understanding the decision-making process of neural networks. Our study provides critical insights into the trustworthiness and practicality of AMs within the agriculture and forestry sectors, thus facilitating a be
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#27599;&#20010;&#31070;&#32463;&#32593;&#32476;&#20851;&#27880;&#19968;&#20010;&#36755;&#20837;&#26102;&#38388;&#29305;&#24449;&#30340;&#32447;&#24615;&#32452;&#21512;&#65292;&#20197;&#21450;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#26102;&#38388;&#24207;&#21015;&#20998;&#35299;&#26041;&#27861;&#26469;&#22788;&#29702;&#22797;&#26434;&#30340;&#26102;&#38388;&#27169;&#24335;&#65292;&#22312;&#30005;&#21147;&#36127;&#33655;&#39044;&#27979;&#20013;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.11664</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#23610;&#24230;&#26102;&#38388;&#20998;&#35299;&#36827;&#34892;&#21487;&#35299;&#37322;&#30340;&#30701;&#26399;&#36127;&#33655;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Interpretable Short-Term Load Forecasting via Multi-Scale Temporal Decomposition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11664
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#27599;&#20010;&#31070;&#32463;&#32593;&#32476;&#20851;&#27880;&#19968;&#20010;&#36755;&#20837;&#26102;&#38388;&#29305;&#24449;&#30340;&#32447;&#24615;&#32452;&#21512;&#65292;&#20197;&#21450;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#26102;&#38388;&#24207;&#21015;&#20998;&#35299;&#26041;&#27861;&#26469;&#22788;&#29702;&#22797;&#26434;&#30340;&#26102;&#38388;&#27169;&#24335;&#65292;&#22312;&#30005;&#21147;&#36127;&#33655;&#39044;&#27979;&#20013;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#24555;&#36895;&#21457;&#23637;&#20351;&#24471;&#22312;&#30005;&#21147;&#31995;&#32479;&#36127;&#33655;&#39044;&#27979;&#20013;&#23454;&#29616;&#20102;&#24191;&#27867;&#24212;&#29992;&#65292;&#20363;&#22914;&#21333;&#21464;&#37327;&#21644;&#22810;&#21464;&#37327;&#30701;&#26399;&#36127;&#33655;&#39044;&#27979;&#12290;&#23613;&#31649;&#24050;&#32463;&#23454;&#29616;&#20102;&#23398;&#20064;&#36127;&#33655;&#27169;&#24335;&#38750;&#32447;&#24615;&#21644;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#24378;&#22823;&#33021;&#21147;&#65292;&#20294;&#23545;&#20110;&#30005;&#21147;&#36127;&#33655;&#39044;&#27979;&#30340;&#20856;&#22411;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#30740;&#31350;&#36739;&#23569;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23398;&#20064;&#20102;&#19968;&#20010;&#32447;&#24615;&#32452;&#21512;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#27599;&#20010;&#31070;&#32463;&#32593;&#32476;&#37117;&#20851;&#27880;&#19968;&#20010;&#36755;&#20837;&#26102;&#38388;&#29305;&#24449;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#26102;&#38388;&#24207;&#21015;&#20998;&#35299;&#26041;&#27861;&#26469;&#22788;&#29702;&#22797;&#26434;&#30340;&#26102;&#38388;&#27169;&#24335;&#12290;&#22312;&#27604;&#21033;&#26102;&#20013;&#22830;&#30005;&#32593;&#36127;&#33655;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#26696;&#20363;&#30740;&#31350;&#65292;&#24182;&#19988;&#25552;&#20986;&#30340;&#27169;&#22411;&#19982;&#24120;&#29992;&#22522;&#32447;&#27169;&#22411;&#30456;&#27604;&#23637;&#29616;&#20986;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11664v1 Announce Type: new  Abstract: Rapid progress in machine learning and deep learning has enabled a wide range of applications in the electricity load forecasting of power systems, for instance, univariate and multivariate short-term load forecasting. Though the strong capabilities of learning the non-linearity of the load patterns and the high prediction accuracy have been achieved, the interpretability of typical deep learning models for electricity load forecasting is less studied. This paper proposes an interpretable deep learning method, which learns a linear combination of neural networks that each attends to an input time feature. We also proposed a multi-scale time series decomposition method to deal with the complex time patterns. Case studies have been carried out on the Belgium central grid load dataset and the proposed model demonstrated better accuracy compared to the frequently applied baseline model. Specifically, the proposed multi-scale temporal decompo
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#22312;&#21160;&#24577;&#35268;&#21010;&#39046;&#22495;&#20013;&#27169;&#25311;&#24037;&#20855;&#20351;&#29992;&#30340;&#30446;&#26631;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#20027;&#21160;&#25512;&#26029;&#20013;&#30340;&#21160;&#24577;&#35268;&#21010;&#65292;&#35813;&#39046;&#22495;&#32771;&#34385;&#21040;&#29983;&#29289;&#30446;&#26631;&#23548;&#21521;&#34892;&#20026;&#30340;&#20004;&#20010;&#20851;&#38190;&#26041;&#38754;</title><link>https://arxiv.org/abs/2402.11658</link><description>&lt;p&gt;
&#20998;&#23618;&#20027;&#21160;&#25512;&#26029;&#20013;&#30340;&#21160;&#24577;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Dynamic planning in hierarchical active inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11658
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#22312;&#21160;&#24577;&#35268;&#21010;&#39046;&#22495;&#20013;&#27169;&#25311;&#24037;&#20855;&#20351;&#29992;&#30340;&#30446;&#26631;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#20027;&#21160;&#25512;&#26029;&#20013;&#30340;&#21160;&#24577;&#35268;&#21010;&#65292;&#35813;&#39046;&#22495;&#32771;&#34385;&#21040;&#29983;&#29289;&#30446;&#26631;&#23548;&#21521;&#34892;&#20026;&#30340;&#20004;&#20010;&#20851;&#38190;&#26041;&#38754;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21160;&#24577;&#35268;&#21010;&#65292;&#25105;&#20204;&#25351;&#30340;&#26159;&#20154;&#31867;&#22823;&#33041;&#25512;&#26029;&#21644;&#26045;&#21152;&#19982;&#35748;&#30693;&#20915;&#31574;&#30456;&#20851;&#30340;&#36816;&#21160;&#36712;&#36857;&#30340;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#19968;&#20010;&#33539;&#24335;&#65292;&#20027;&#21160;&#25512;&#26029;&#65292;&#20026;&#29983;&#29289;&#26377;&#26426;&#20307;&#36866;&#24212;&#24102;&#26469;&#20102;&#22522;&#26412;&#35265;&#35299;&#65292;&#19981;&#26029;&#21162;&#21147;&#26368;&#23567;&#21270;&#39044;&#27979;&#35823;&#24046;&#20197;&#23558;&#33258;&#24049;&#38480;&#21046;&#22312;&#19982;&#29983;&#21629;&#20860;&#23481;&#30340;&#29366;&#24577;&#12290;&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#37324;&#65292;&#35768;&#22810;&#30740;&#31350;&#34920;&#26126;&#20154;&#31867;&#21644;&#21160;&#29289;&#34892;&#20026;&#21487;&#20197;&#35299;&#37322;&#20026;&#20027;&#21160;&#25512;&#26029;&#36807;&#31243;&#65292;&#26080;&#35770;&#26159;&#20316;&#20026;&#31163;&#25955;&#20915;&#31574;&#36824;&#26159;&#36830;&#32493;&#36816;&#21160;&#25511;&#21046;&#65292;&#37117;&#28608;&#21457;&#20102;&#26426;&#22120;&#20154;&#25216;&#26415;&#21644;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#25991;&#29486;&#32570;&#20047;&#23545;&#22914;&#20309;&#26377;&#25928;&#22320;&#22312;&#21464;&#21270;&#29615;&#22659;&#20013;&#35268;&#21010;&#34892;&#21160;&#30340;&#20840;&#38754;&#23637;&#26395;&#12290;&#25105;&#20204;&#35774;&#23450;&#20102;&#23545;&#24037;&#20855;&#20351;&#29992;&#36827;&#34892;&#24314;&#27169;&#30340;&#30446;&#26631;&#65292;&#28145;&#20837;&#30740;&#31350;&#20102;&#20027;&#21160;&#25512;&#26029;&#20013;&#30340;&#21160;&#24577;&#35268;&#21010;&#20027;&#39064;&#65292;&#29282;&#35760;&#20004;&#20010;&#29983;&#29289;&#30446;&#26631;&#23548;&#21521;&#34892;&#20026;&#30340;&#20851;&#38190;&#26041;&#38754;&#65306;&#29702;&#35299;&#8230;&#8230;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11658v1 Announce Type: new  Abstract: By dynamic planning, we refer to the ability of the human brain to infer and impose motor trajectories related to cognitive decisions. A recent paradigm, active inference, brings fundamental insights into the adaptation of biological organisms, constantly striving to minimize prediction errors to restrict themselves to life-compatible states. Over the past years, many studies have shown how human and animal behavior could be explained in terms of an active inferential process -- either as discrete decision-making or continuous motor control -- inspiring innovative solutions in robotics and artificial intelligence. Still, the literature lacks a comprehensive outlook on how to effectively plan actions in changing environments. Setting ourselves the goal of modeling tool use, we delve into the topic of dynamic planning in active inference, keeping in mind two crucial aspects of biological goal-directed behavior: the capacity to understand a
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#20102;&#29289;&#29702;&#23618;&#36890;&#20449;&#21151;&#33021;&#30340;&#23454;&#29992;&#35774;&#22791;&#38388;&#20154;&#24037;&#26234;&#33021;&#36890;&#20449;&#26694;&#26550;&#65292;&#36890;&#36807;&#31471;&#21040;&#31471;&#35757;&#32451;&#32467;&#21512;&#20449;&#36947;&#22122;&#22768;&#20197;&#22686;&#24378;&#38887;&#24615;&#65292;&#37319;&#29992;VQ-VAE&#23454;&#29616;&#39640;&#25928;&#31283;&#20581;&#30340;&#36890;&#20449;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;Transformer&#25552;&#21319;&#36890;&#29992;&#24615;&#33021;</title><link>https://arxiv.org/abs/2402.11656</link><description>&lt;p&gt;
&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#19982;&#29289;&#29702;&#23618;&#36890;&#20449;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
Integrating Pre-Trained Language Model with Physical Layer Communications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11656
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#20102;&#29289;&#29702;&#23618;&#36890;&#20449;&#21151;&#33021;&#30340;&#23454;&#29992;&#35774;&#22791;&#38388;&#20154;&#24037;&#26234;&#33021;&#36890;&#20449;&#26694;&#26550;&#65292;&#36890;&#36807;&#31471;&#21040;&#31471;&#35757;&#32451;&#32467;&#21512;&#20449;&#36947;&#22122;&#22768;&#20197;&#22686;&#24378;&#38887;&#24615;&#65292;&#37319;&#29992;VQ-VAE&#23454;&#29616;&#39640;&#25928;&#31283;&#20581;&#30340;&#36890;&#20449;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;Transformer&#25552;&#21319;&#36890;&#29992;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35774;&#22791;&#38388;&#20154;&#24037;&#26234;&#33021;&#36890;&#20449;&#30340;&#26032;&#20852;&#39046;&#22495;&#20013;&#65292;&#35774;&#22791;&#30452;&#25509;&#36890;&#36807;&#23884;&#20837;&#24335;&#22522;&#30784;&#27169;&#22411;&#65288;&#22914;&#35821;&#35328;&#27169;&#22411;&#65289;&#20132;&#25442;&#20449;&#24687;&#65292;&#38656;&#35201;&#24378;&#22823;&#12289;&#39640;&#25928;&#19988;&#36890;&#29992;&#30340;&#36890;&#20449;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#20123;&#26694;&#26550;&#19982;&#29616;&#26377;&#26080;&#32447;&#31995;&#32479;&#38598;&#25104;&#24182;&#26377;&#25928;&#31649;&#29702;&#22122;&#22768;&#21644;&#27604;&#29305;&#35823;&#24046;&#37117;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#35774;&#22791;&#38388;&#20154;&#24037;&#26234;&#33021;&#36890;&#20449;&#26694;&#26550;&#65292;&#38598;&#25104;&#20102;&#29289;&#29702;&#23618;&#36890;&#20449;&#21151;&#33021;&#65292;&#24182;&#36890;&#36807;&#38142;&#36335;&#32423;&#27169;&#25311;&#22120;&#23637;&#31034;&#20102;&#20854;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#31471;&#21040;&#31471;&#35757;&#32451;&#32467;&#21512;&#20449;&#36947;&#22122;&#22768;&#20197;&#22686;&#24378;&#38887;&#24615;&#65292;&#37319;&#29992;&#21521;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VQ-VAE&#65289;&#23454;&#29616;&#39640;&#25928;&#31283;&#20581;&#30340;&#36890;&#20449;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#32534;&#30721;-&#35299;&#30721;Transformer&#25552;&#21319;&#36890;&#29992;&#24615;&#33021;&#12290;&#22312;&#21508;&#31181;&#36890;&#20449;&#22330;&#26223;&#30340;&#27169;&#25311;&#20013;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#23637;&#29616;&#20986;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11656v1 Announce Type: cross  Abstract: The burgeoning field of on-device AI communication, where devices exchange information directly through embedded foundation models, such as language models (LMs), requires robust, efficient, and generalizable communication frameworks. However, integrating these frameworks with existing wireless systems and effectively managing noise and bit errors pose significant challenges. In this work, we introduce a practical on-device AI communication framework, integrated with physical layer (PHY) communication functions, demonstrated through its performance on a link-level simulator. Our framework incorporates end-to-end training with channel noise to enhance resilience, incorporates vector quantized variational autoencoders (VQ-VAE) for efficient and robust communication, and utilizes pre-trained encoder-decoder transformers for improved generalization capabilities. Simulations, across various communication scenarios, reveal that our framework
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;&#19968;&#20010;&#31574;&#30053;&#20248;&#21270;&#35270;&#35282;&#65292;&#23558;&#22522;&#20110;&#23376;&#26799;&#24230;&#30340;&#25628;&#32034;&#26041;&#27861;&#25193;&#23637;&#21040;&#26080;&#27169;&#22411;&#35774;&#32622;&#65292;&#24182;&#30740;&#31350;&#20102;&#20004;&#31181;&#26080;&#27169;&#22411;&#31574;&#30053;&#20248;&#21270;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.11654</link><description>&lt;p&gt;
&#26080;&#27169;&#22411;$\mu$-&#32508;&#21512;: &#19968;&#20010;&#38750;&#20809;&#28369;&#20248;&#21270;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Model-Free $\mu$-Synthesis: A Nonsmooth Optimization Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11654
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;&#19968;&#20010;&#31574;&#30053;&#20248;&#21270;&#35270;&#35282;&#65292;&#23558;&#22522;&#20110;&#23376;&#26799;&#24230;&#30340;&#25628;&#32034;&#26041;&#27861;&#25193;&#23637;&#21040;&#26080;&#27169;&#22411;&#35774;&#32622;&#65292;&#24182;&#30740;&#31350;&#20102;&#20004;&#31181;&#26080;&#27169;&#22411;&#31574;&#30053;&#20248;&#21270;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#22312;&#19968;&#20010;&#37325;&#35201;&#30340;&#40065;&#26834;&#25511;&#21046;&#22522;&#20934;&#19978;&#30340;&#26080;&#27169;&#22411;&#31574;&#30053;&#25628;&#32034;&#65292;&#21363;$\mu$-&#32508;&#21512;&#12290;&#22312;&#19968;&#33324;&#30340;&#36755;&#20986;&#21453;&#39304;&#35774;&#32622;&#20013;&#65292;&#36825;&#20010;&#38382;&#39064;&#19981;&#23384;&#22312;&#20984;&#24418;&#24335;&#65292;&#22240;&#27492;&#19981;&#22826;&#21487;&#33021;&#23384;&#22312;&#20840;&#23616;&#26368;&#20248;&#24615;&#20445;&#35777;&#12290;Apkarian (2011)&#25552;&#20986;&#20102;&#19968;&#20010;&#38750;&#20984;&#38750;&#20809;&#28369;&#30340;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#19988;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#23376;&#26799;&#24230;&#30340;&#31574;&#30053;&#25628;&#32034;&#31639;&#27861;&#22312;&#19968;&#20010;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#24335;&#20013;&#29983;&#25104;&#26356;&#26032;&#26041;&#21521;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#35774;&#35745;&#32467;&#26524;&#12290;&#23613;&#31649;&#32570;&#20047;&#20984;&#24615;&#21644;&#20840;&#23616;&#26368;&#20248;&#24615;&#20445;&#35777;&#65292;&#20294;&#36825;&#20123;&#22522;&#20110;&#23376;&#26799;&#24230;&#30340;&#31574;&#30053;&#25628;&#32034;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25968;&#20540;&#32467;&#26524;&#12290;&#24314;&#31435;&#22312;&#36825;&#26679;&#19968;&#20010;&#31574;&#30053;&#20248;&#21270;&#35270;&#35282;&#20043;&#19978;&#65292;&#25105;&#20204;&#30340;&#35770;&#25991;&#23558;&#36825;&#20123;&#22522;&#20110;&#23376;&#26799;&#24230;&#30340;&#25628;&#32034;&#26041;&#27861;&#25193;&#23637;&#21040;&#20102;&#19968;&#20010;&#26080;&#27169;&#22411;&#30340;&#35774;&#23450;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;&#26080;&#27169;&#22411;&#31574;&#30053;&#20248;&#21270;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;: &#26080;&#27169;&#22411;&#38750;&#23548;&#25968;&#30340;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11654v1 Announce Type: cross  Abstract: In this paper, we revisit model-free policy search on an important robust control benchmark, namely $\mu$-synthesis. In the general output-feedback setting, there do not exist convex formulations for this problem, and hence global optimality guarantees are not expected. Apkarian (2011) presented a nonconvex nonsmooth policy optimization approach for this problem, and achieved state-of-the-art design results via using subgradient-based policy search algorithms which generate update directions in a model-based manner. Despite the lack of convexity and global optimality guarantees, these subgradient-based policy search methods have led to impressive numerical results in practice. Built upon such a policy optimization persepctive, our paper extends these subgradient-based search methods to a model-free setting. Specifically, we examine the effectiveness of two model-free policy optimization strategies: the model-free non-derivative samplin
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#31283;&#20581;&#30340;&#20272;&#35745;&#37327;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#29616;&#20195;&#25968;&#25454;&#20016;&#23500;&#30340;&#29615;&#22659;&#20013;&#20272;&#35745;&#23384;&#22312;&#26410;&#35266;&#23519;&#28151;&#26434;&#22240;&#32032;&#19979;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#26377;&#38480;&#26679;&#26412;&#21644;&#28176;&#36817;&#24615;&#36136;&#65292;&#24182;&#22312;&#21442;&#25968;&#36895;&#29575;&#19979;&#23558;&#20854;&#35823;&#24046;&#25910;&#25947;&#20026;&#38646;&#22343;&#20540;&#39640;&#26031;&#20998;&#24067;&#12290;</title><link>https://arxiv.org/abs/2402.11652</link><description>&lt;p&gt;
&#22240;&#26524;&#28508;&#22312;&#22240;&#23376;&#27169;&#22411;&#20013;&#30340;&#21452;&#37325;&#31283;&#20581;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Doubly Robust Inference in Causal Latent Factor Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11652
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#31283;&#20581;&#30340;&#20272;&#35745;&#37327;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#29616;&#20195;&#25968;&#25454;&#20016;&#23500;&#30340;&#29615;&#22659;&#20013;&#20272;&#35745;&#23384;&#22312;&#26410;&#35266;&#23519;&#28151;&#26434;&#22240;&#32032;&#19979;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#26377;&#38480;&#26679;&#26412;&#21644;&#28176;&#36817;&#24615;&#36136;&#65292;&#24182;&#22312;&#21442;&#25968;&#36895;&#29575;&#19979;&#23558;&#20854;&#35823;&#24046;&#25910;&#25947;&#20026;&#38646;&#22343;&#20540;&#39640;&#26031;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#29616;&#20195;&#25968;&#25454;&#20016;&#23500;&#29615;&#22659;&#20013;&#20272;&#35745;&#23384;&#22312;&#26410;&#35266;&#23519;&#28151;&#26434;&#22240;&#32032;&#19979;&#30340;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#30340;&#26032;&#26694;&#26550;&#65292;&#35813;&#29615;&#22659;&#20855;&#26377;&#22823;&#37327;&#21333;&#20301;&#21644;&#32467;&#26524;&#12290;&#25152;&#25552;&#20986;&#30340;&#20272;&#35745;&#37327;&#26159;&#21452;&#37325;&#31283;&#20581;&#30340;&#65292;&#32467;&#21512;&#20102;&#32467;&#26524;&#22635;&#34917;&#12289;&#20498;&#25968;&#27010;&#29575;&#21152;&#26435;&#20197;&#21450;&#19968;&#31181;&#29992;&#20110;&#30697;&#38453;&#34917;&#20840;&#30340;&#26032;&#22411;&#20132;&#21449;&#37197;&#23545;&#31243;&#24207;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#26377;&#38480;&#26679;&#26412;&#21644;&#28176;&#36817;&#20445;&#35777;&#65292;&#24182;&#23637;&#31034;&#20102;&#26032;&#20272;&#35745;&#37327;&#30340;&#35823;&#24046;&#25910;&#25947;&#21040;&#21442;&#25968;&#36895;&#29575;&#19979;&#30340;&#38646;&#22343;&#20540;&#39640;&#26031;&#20998;&#24067;&#12290;&#27169;&#25311;&#32467;&#26524;&#23637;&#31034;&#20102;&#26412;&#25991;&#20998;&#26512;&#30340;&#20272;&#35745;&#37327;&#30340;&#24418;&#24335;&#29305;&#24615;&#30340;&#23454;&#38469;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11652v1 Announce Type: cross  Abstract: This article introduces a new framework for estimating average treatment effects under unobserved confounding in modern data-rich environments featuring large numbers of units and outcomes. The proposed estimator is doubly robust, combining outcome imputation, inverse probability weighting, and a novel cross-fitting procedure for matrix completion. We derive finite-sample and asymptotic guarantees, and show that the error of the new estimator converges to a mean-zero Gaussian distribution at a parametric rate. Simulation results demonstrate the practical relevance of the formal properties of the estimators analyzed in this article.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#25506;&#35752;&#31243;&#24207;&#21270;&#24378;&#21270;&#23398;&#20064;&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#32473;&#20986;&#20102;&#23545;&#31243;&#24207;&#21270;&#24378;&#21270;&#23398;&#20064;&#20013;&#20851;&#38190;&#38382;&#39064;&#30340;&#21021;&#27493;&#22238;&#31572;</title><link>https://arxiv.org/abs/2402.11650</link><description>&lt;p&gt;
&#31243;&#24207;&#21270;&#24378;&#21270;&#23398;&#20064;&#30340;&#29702;&#35770;&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
Theoretical foundations for programmatic reinforcement learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11650
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#25506;&#35752;&#31243;&#24207;&#21270;&#24378;&#21270;&#23398;&#20064;&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#32473;&#20986;&#20102;&#23545;&#31243;&#24207;&#21270;&#24378;&#21270;&#23398;&#20064;&#20013;&#20851;&#38190;&#38382;&#39064;&#30340;&#21021;&#27493;&#22238;&#31572;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#33268;&#21147;&#20110;&#22312;&#26410;&#30693;&#30340;&#38543;&#26426;&#29615;&#22659;&#20013;&#23398;&#20064;&#26368;&#20248;&#31574;&#30053;&#30340;&#31639;&#27861;&#12290;&#31243;&#24207;&#21270;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;&#23558;&#31574;&#30053;&#34920;&#31034;&#20026;&#31243;&#24207;&#65292;&#21363;&#28041;&#21450;&#25511;&#21046;&#24490;&#29615;&#31561;&#39640;&#38454;&#26500;&#36896;&#12290;&#23613;&#31649;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#24418;&#24335;&#26041;&#27861;&#20132;&#21449;&#39046;&#22495;&#21560;&#24341;&#20102;&#24456;&#22810;&#20851;&#27880;&#65292;&#20294;&#22312;&#31243;&#24207;&#21270;&#24378;&#21270;&#23398;&#20064;&#30340;&#29702;&#35770;&#26041;&#38754;&#30693;&#20043;&#29978;&#23569;&#65306;&#20160;&#20040;&#26679;&#30340;&#31243;&#24207;&#21270;&#31574;&#30053;&#26159;&#22909;&#30340;&#65311;&#26368;&#20248;&#31243;&#24207;&#21270;&#31574;&#30053;&#26377;&#22810;&#22823;&#65311;&#25105;&#20204;&#22914;&#20309;&#23398;&#20064;&#23427;&#20204;&#65311;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#39318;&#27425;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#65292;&#21551;&#21160;&#23545;&#31243;&#24207;&#21270;&#24378;&#21270;&#23398;&#20064;&#30340;&#29702;&#35770;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11650v1 Announce Type: new  Abstract: The field of Reinforcement Learning (RL) is concerned with algorithms for learning optimal policies in unknown stochastic environments. Programmatic RL studies representations of policies as programs, meaning involving higher order constructs such as control loops. Despite attracting a lot of attention at the intersection of the machine learning and formal methods communities, very little is known on the theoretical front about programmatic RL: what are good classes of programmatic policies? How large are optimal programmatic policies? How can we learn them? The goal of this paper is to give first answers to these questions, initiating a theoretical study of programmatic RL.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#22810;&#21151;&#33021;&#22270;&#23398;&#20064;&#26041;&#27861;&#30340;&#26032;&#27010;&#24565;&#21407;&#22411;&#65292;&#37325;&#28857;&#20851;&#27880;&#8220;&#22312;&#21738;&#37324;&#8221;&#21644;&#8220;&#22914;&#20309;&#8221;&#30340;&#35282;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.11641</link><description>&lt;p&gt;
&#20174;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#35270;&#35282;&#25506;&#32034;&#22810;&#21151;&#33021;&#22270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Versatile Graph Learning Approach: from the Perspective of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11641
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#22810;&#21151;&#33021;&#22270;&#23398;&#20064;&#26041;&#27861;&#30340;&#26032;&#27010;&#24565;&#21407;&#22411;&#65292;&#37325;&#28857;&#20851;&#27880;&#8220;&#22312;&#21738;&#37324;&#8221;&#21644;&#8220;&#22914;&#20309;&#8221;&#30340;&#35282;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#32467;&#26500;&#25968;&#25454;&#26159;&#24120;&#29992;&#30340;&#65292;&#24182;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#22330;&#26223;&#12290;&#38754;&#23545;&#22810;&#26679;&#30340;&#23398;&#20064;&#20219;&#21153;&#12289;&#22270;&#39046;&#22495;&#21644;&#22797;&#26434;&#30340;&#22270;&#23398;&#20064;&#36807;&#31243;&#65292;&#20256;&#32479;&#30340;&#35774;&#35745;&#22810;&#21151;&#33021;&#22270;&#23398;&#20064;&#26041;&#27861;&#23545;&#20154;&#31867;&#19987;&#23478;&#25552;&#20986;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#27010;&#24565;&#21407;&#22411;&#65292;&#29992;&#20110;&#35774;&#35745;&#20855;&#26377;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#22810;&#21151;&#33021;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29305;&#21035;&#20851;&#27880;&#8220;&#22312;&#21738;&#37324;&#8221;&#21644;&#8220;&#22914;&#20309;&#8221;&#30340;&#35282;&#24230;&#12290;&#20174;&#8220;&#22312;&#21738;&#37324;&#8221;&#30340;&#35282;&#24230;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#22235;&#20010;&#20851;&#38190;&#30340;&#22270;&#23398;&#20064;&#36807;&#31243;&#65292;&#21253;&#25324;&#20219;&#21153;&#23450;&#20041;&#12289;&#22270;&#25968;&#25454;&#29305;&#24449;&#24037;&#31243;&#12289;&#27169;&#22411;&#36873;&#25321;&#19982;&#20248;&#21270;&#12289;&#37096;&#32626;&#19982;&#26381;&#21153;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;LLMs&#22312;&#36825;&#20123;&#36807;&#31243;&#20013;&#30340;&#24212;&#29992;&#22330;&#26223;&#12290;&#22312;&#8220;&#22914;&#20309;&#8221;&#30340;&#35282;&#24230;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11641v1 Announce Type: new  Abstract: Graph-structured data are the commonly used and have wide application scenarios in the real world. For these diverse applications, the vast variety of learning tasks, graph domains, and complex graph learning procedures present challenges for human experts when designing versatile graph learning approaches. Facing these challenges, large language models (LLMs) offer a potential solution due to the extensive knowledge and the human-like intelligence. This paper proposes a novel conceptual prototype for designing versatile graph learning methods with LLMs, with a particular focus on the ``where'' and ``how'' perspectives. From the ``where'' perspective, we summarize four key graph learning procedures, including task definition, graph data feature engineering, model selection and optimization, deployment and serving. We then explore the application scenarios of LLMs in these procedures across a wider spectrum. In the ``how'' perspective, we
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;Softmax&#27880;&#24847;&#21147;&#22312;&#36866;&#24212;&#39044;&#35757;&#32451;&#20219;&#21153;&#32972;&#26223;&#26102;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;&#27880;&#24847;&#21147;&#21333;&#20803;&#23398;&#20250;&#19982;Lipschitzness&#38477;&#20302;&#21644;&#26631;&#31614;&#22122;&#22768;&#22686;&#21152;&#30456;&#20851;&#30340;&#31383;&#21475;&#35843;&#25972;&#65292;&#20197;&#21450;&#22312;&#20302;&#32500;&#12289;&#32447;&#24615;&#38382;&#39064;&#19978;&#23398;&#20250;&#22312;&#25512;&#29702;&#21069;&#36827;&#34892;&#36866;&#24403;&#31354;&#38388;&#30340;&#25237;&#24433;&#12290;</title><link>https://arxiv.org/abs/2402.11639</link><description>&lt;p&gt;
&#20855;&#26377;Transformer&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#65306;Softmax&#27880;&#24847;&#21147;&#36866;&#24212;&#20989;&#25968;Lipschitz&#24615;&#36136;
&lt;/p&gt;
&lt;p&gt;
In-Context Learning with Transformers: Softmax Attention Adapts to Function Lipschitzness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11639
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;Softmax&#27880;&#24847;&#21147;&#22312;&#36866;&#24212;&#39044;&#35757;&#32451;&#20219;&#21153;&#32972;&#26223;&#26102;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;&#27880;&#24847;&#21147;&#21333;&#20803;&#23398;&#20250;&#19982;Lipschitzness&#38477;&#20302;&#21644;&#26631;&#31614;&#22122;&#22768;&#22686;&#21152;&#30456;&#20851;&#30340;&#31383;&#21475;&#35843;&#25972;&#65292;&#20197;&#21450;&#22312;&#20302;&#32500;&#12289;&#32447;&#24615;&#38382;&#39064;&#19978;&#23398;&#20250;&#22312;&#25512;&#29702;&#21069;&#36827;&#34892;&#36866;&#24403;&#31354;&#38388;&#30340;&#25237;&#24433;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#30340;&#19968;&#20010;&#26174;&#33879;&#29305;&#24615;&#26159;&#20854;&#33021;&#22815;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#65292;&#22312;&#36825;&#31181;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;&#23398;&#20064;&#32773;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#36890;&#36807;&#26576;&#20123;&#25968;&#25454;&#38544;&#24335;&#22320;&#34987;&#21576;&#29616;&#19968;&#20010;&#26032;&#39046;&#22495;&#30340;&#32972;&#26223;&#65292;&#24182;&#34987;&#35201;&#27714;&#22312;&#35813;&#32972;&#26223;&#19979;&#36827;&#34892;&#39044;&#27979;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#23398;&#20064;&#32773;&#24517;&#39035;&#22312;&#27809;&#26377;&#39069;&#22806;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#36866;&#24212;&#32972;&#26223;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;Softmax&#27880;&#24847;&#21147;&#22312;&#19968;&#20010;ICL&#35774;&#32622;&#20013;&#30340;&#20316;&#29992;&#65292;&#20854;&#20013;&#27599;&#20010;&#32972;&#26223;&#37117;&#32534;&#30721;&#20102;&#19968;&#20010;&#22238;&#24402;&#20219;&#21153;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#27880;&#24847;&#21147;&#21333;&#20803;&#23398;&#20064;&#19968;&#20010;&#31383;&#21475;&#65292;&#29992;&#20110;&#23454;&#29616;&#19968;&#20010;&#36866;&#24212;&#20110;&#39044;&#35757;&#32451;&#20219;&#21153;&#30340;&#26368;&#36817;&#37051;&#39044;&#27979;&#22120;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20010;&#31383;&#21475;&#38543;&#30528;Lipschitzness&#30340;&#38477;&#20302;&#21644;&#26631;&#31614;&#22122;&#22768;&#30340;&#22686;&#21152;&#32780;&#25193;&#22823;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22312;&#20302;&#31209;&#12289;&#32447;&#24615;&#38382;&#39064;&#19978;&#65292;&#27880;&#24847;&#21147;&#21333;&#20803;&#22312;&#25512;&#29702;&#20043;&#21069;&#23398;&#20250;&#20102;&#25237;&#24433;&#21040;&#36866;&#24403;&#30340;&#23376;&#31354;&#38388;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#36825;&#31181;&#36866;&#24212;&#24615;&#20851;&#38190;&#22320;&#20381;&#36182;&#20110;softmax&#28608;&#27963;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11639v1 Announce Type: new  Abstract: A striking property of transformers is their ability to perform in-context learning (ICL), a machine learning framework in which the learner is presented with a novel context during inference implicitly through some data, and tasked with making a prediction in that context. As such that learner must adapt to the context without additional training. We explore the role of softmax attention in an ICL setting where each context encodes a regression task. We show that an attention unit learns a window that it uses to implement a nearest-neighbors predictor adapted to the landscape of the pretraining tasks. Specifically, we show that this window widens with decreasing Lipschitzness and increasing label noise in the pretraining tasks. We also show that on low-rank, linear problems, the attention unit learns to project onto the appropriate subspace before inference. Further, we show that this adaptivity relies crucially on the softmax activatio
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#38656;&#39069;&#22806;&#20449;&#24687;&#30340;&#26032;&#22411;&#22522;&#20110;&#20551;&#29992;&#25143;&#30340;&#27602;&#21270;&#25915;&#20987;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#32852;&#37030;&#25512;&#33616;&#31995;&#32479;&#20013;&#25191;&#34892;&#25512;&#24191;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2402.11637</link><description>&lt;p&gt;
&#29992;&#20551;&#29992;&#25143;&#23545;&#32852;&#37030;&#25512;&#33616;&#31995;&#32479;&#36827;&#34892;&#27602;&#21270;
&lt;/p&gt;
&lt;p&gt;
Poisoning Federated Recommender Systems with Fake Users
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11637
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#38656;&#39069;&#22806;&#20449;&#24687;&#30340;&#26032;&#22411;&#22522;&#20110;&#20551;&#29992;&#25143;&#30340;&#27602;&#21270;&#25915;&#20987;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#32852;&#37030;&#25512;&#33616;&#31995;&#32479;&#20013;&#25191;&#34892;&#25512;&#24191;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#25512;&#33616;&#26159;&#32852;&#37030;&#23398;&#20064;&#20013;&#19968;&#20010;&#26174;&#33879;&#30340;&#29992;&#20363;&#65292;&#20294;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#21508;&#31181;&#25915;&#20987;&#65292;&#20174;&#29992;&#25143;&#21040;&#26381;&#21153;&#22120;&#31471;&#30340;&#28431;&#27934;&#12290;&#27602;&#21270;&#25915;&#20987;&#22312;&#29992;&#25143;&#31471;&#25915;&#20987;&#20013;&#29305;&#21035;&#24341;&#20154;&#27880;&#30446;&#65292;&#22240;&#20026;&#21442;&#19982;&#32773;&#19978;&#20256;&#24694;&#24847;&#27169;&#22411;&#26356;&#26032;&#26469;&#27450;&#39575;&#20840;&#23616;&#27169;&#22411;&#65292;&#36890;&#24120;&#24847;&#22270;&#25552;&#21319;&#25110;&#38477;&#20302;&#29305;&#23450;&#30446;&#26631;&#39033;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#32852;&#37030;&#25512;&#33616;&#31995;&#32479;&#20013;&#25191;&#34892;&#25512;&#24191;&#25915;&#20987;&#30340;&#31574;&#30053;&#12290;&#24403;&#21069;&#23545;&#32852;&#37030;&#25512;&#33616;&#31995;&#32479;&#30340;&#27602;&#21270;&#25915;&#20987;&#36890;&#24120;&#20381;&#36182;&#20110;&#39069;&#22806;&#20449;&#24687;&#65292;&#22914;&#30495;&#23454;&#29992;&#25143;&#30340;&#26412;&#22320;&#35757;&#32451;&#25968;&#25454;&#25110;&#29289;&#21697;&#27969;&#34892;&#24230;&#12290;&#28982;&#32780;&#65292;&#25915;&#20987;&#32773;&#24456;&#38590;&#33719;&#24471;&#36825;&#20123;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#24320;&#21457;&#19968;&#31181;&#25915;&#20987;&#65292;&#38500;&#20102;&#20174;&#26381;&#21153;&#22120;&#33719;&#21462;&#30340;&#29289;&#21697;&#23884;&#20837;&#20043;&#22806;&#65292;&#19981;&#38656;&#35201;&#39069;&#22806;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;PoisonFRS&#30340;&#26032;&#22411;&#22522;&#20110;&#20551;&#29992;&#25143;&#30340;&#27602;&#21270;&#25915;&#20987;&#65292;&#29992;&#20110;&#20419;&#38144;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11637v1 Announce Type: cross  Abstract: Federated recommendation is a prominent use case within federated learning, yet it remains susceptible to various attacks, from user to server-side vulnerabilities. Poisoning attacks are particularly notable among user-side attacks, as participants upload malicious model updates to deceive the global model, often intending to promote or demote specific targeted items. This study investigates strategies for executing promotion attacks in federated recommender systems.   Current poisoning attacks on federated recommender systems often rely on additional information, such as the local training data of genuine users or item popularity. However, such information is challenging for the potential attacker to obtain. Thus, there is a need to develop an attack that requires no extra information apart from item embeddings obtained from the server. In this paper, we introduce a novel fake user based poisoning attack named PoisonFRS to promote the
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#21046;&#31070;&#32463;&#25512;&#29702;&#22120;&#32500;&#25252;&#25191;&#34892;&#36712;&#36857;&#20316;&#20026;&#26377;&#38480;&#39044;&#23450;&#20041;&#29366;&#24577;&#32452;&#21512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#31639;&#27861;&#29366;&#24577;&#36716;&#25442;&#30340;&#30417;&#30563;&#35757;&#32451;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#19982;&#21407;&#22987;&#31639;&#27861;&#23436;&#32654;&#23545;&#40784;&#65292;&#24182;&#22312;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#23436;&#32654;&#30340;&#27979;&#35797;&#25104;&#32489;&#12290;</title><link>https://arxiv.org/abs/2402.11628</link><description>&lt;p&gt;
&#31163;&#25955;&#31070;&#32463;&#31639;&#27861;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Discrete Neural Algorithmic Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11628
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#21046;&#31070;&#32463;&#25512;&#29702;&#22120;&#32500;&#25252;&#25191;&#34892;&#36712;&#36857;&#20316;&#20026;&#26377;&#38480;&#39044;&#23450;&#20041;&#29366;&#24577;&#32452;&#21512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#31639;&#27861;&#29366;&#24577;&#36716;&#25442;&#30340;&#30417;&#30563;&#35757;&#32451;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#19982;&#21407;&#22987;&#31639;&#27861;&#23436;&#32654;&#23545;&#40784;&#65292;&#24182;&#22312;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#23436;&#32654;&#30340;&#27979;&#35797;&#25104;&#32489;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31639;&#27861;&#25512;&#29702;&#26088;&#22312;&#36890;&#36807;&#23398;&#20064;&#27169;&#20223;&#32463;&#20856;&#31639;&#27861;&#30340;&#25191;&#34892;&#26469;&#25429;&#25417;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#35745;&#31639;&#12290;&#23613;&#31649;&#24120;&#35265;&#30340;&#26550;&#26500;&#36275;&#22815;&#34920;&#36798;&#27491;&#30830;&#30340;&#27169;&#22411;&#22312;&#26435;&#37325;&#31354;&#38388;&#20013;&#65292;&#20294;&#24403;&#21069;&#30340;&#31070;&#32463;&#25512;&#29702;&#22120;&#22312;&#22788;&#29702;&#36229;&#20986;&#20998;&#24067;&#25968;&#25454;&#26102;&#38754;&#20020;&#27867;&#21270;&#22256;&#38590;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#32463;&#20856;&#35745;&#31639;&#19981;&#21463;&#20998;&#24067;&#21464;&#21270;&#30340;&#24433;&#21709;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#25551;&#36848;&#20026;&#31163;&#25955;&#35745;&#31639;&#29366;&#24577;&#20043;&#38388;&#30340;&#36716;&#25442;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#24378;&#21046;&#31070;&#32463;&#25512;&#29702;&#22120;&#23558;&#25191;&#34892;&#36712;&#36857;&#20316;&#20026;&#26377;&#38480;&#39044;&#23450;&#20041;&#29366;&#24577;&#30340;&#32452;&#21512;&#36827;&#34892;&#32500;&#25252;&#12290;&#36890;&#36807;&#23545;&#31639;&#27861;&#29366;&#24577;&#36716;&#25442;&#30340;&#30417;&#30563;&#35757;&#32451;&#65292;&#36825;&#31181;&#27169;&#22411;&#33021;&#22815;&#19982;&#21407;&#22987;&#31639;&#27861;&#23436;&#32654;&#23545;&#40784;&#12290;&#20026;&#20102;&#35777;&#26126;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#22312;SALSA-CLRS&#22522;&#20934;&#27979;&#35797;&#19978;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#22312;&#37027;&#37324;&#25105;&#20204;&#20026;&#25152;&#26377;&#20219;&#21153;&#33719;&#24471;&#20102;&#23436;&#32654;&#30340;&#27979;&#35797;&#25104;&#32489;&#12290;&#27492;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;&#26550;&#26500;&#36873;&#25321;&#20351;&#25105;&#20204;&#33021;&#22815;&#35777;&#26126;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11628v1 Announce Type: new  Abstract: Neural algorithmic reasoning aims to capture computations with neural networks via learning the models to imitate the execution of classical algorithms. While common architectures are expressive enough to contain the correct model in the weights space, current neural reasoners are struggling to generalize well on out-of-distribution data. On the other hand, classical computations are not affected by distribution shifts as they can be described as transitions between discrete computational states. In this work, we propose to force neural reasoners to maintain the execution trajectory as a combination of finite predefined states. Trained with supervision on the algorithm's state transitions, such models are able to perfectly align with the original algorithm. To show this, we evaluate our approach on the SALSA-CLRS benchmark, where we get perfect test scores for all tasks. Moreover, the proposed architectural choice allows us to prove the 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36923;&#36753;&#38381;&#29615;&#30340;&#26694;&#26550;&#65288;LogicCheckGPT&#65289;&#65292;&#21033;&#29992;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#26412;&#36523;&#26469;&#26816;&#27979;&#21644;&#20943;&#36731;&#23545;&#35937;&#24187;&#35273;&#12290;</title><link>https://arxiv.org/abs/2402.11622</link><description>&lt;p&gt;
&#36923;&#36753;&#38381;&#29615;&#65306;&#25581;&#31034;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#23545;&#35937;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Logical Closed Loop: Uncovering Object Hallucinations in Large Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11622
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36923;&#36753;&#38381;&#29615;&#30340;&#26694;&#26550;&#65288;LogicCheckGPT&#65289;&#65292;&#21033;&#29992;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#26412;&#36523;&#26469;&#26816;&#27979;&#21644;&#20943;&#36731;&#23545;&#35937;&#24187;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35937;&#24187;&#35273;&#19968;&#30452;&#26159;&#38459;&#30861;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#26356;&#24191;&#27867;&#24212;&#29992;&#30340;&#36719;&#32907;&#12290;&#23545;&#35937;&#24187;&#35273;&#26159;&#25351;LVLMs&#22312;&#22270;&#20687;&#20013;&#22768;&#31216;&#19981;&#23384;&#22312;&#30340;&#23545;&#35937;&#30340;&#29616;&#35937;&#12290;&#20026;&#20102;&#20943;&#36731;&#23545;&#35937;&#24187;&#35273;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#25351;&#23548;&#35843;&#25972;&#21644;&#22522;&#20110;&#22806;&#37096;&#27169;&#22411;&#30340;&#26816;&#27979;&#26041;&#27861;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#35201;&#20040;&#38656;&#35201;&#22823;&#35268;&#27169;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#35201;&#20040;&#20381;&#36182;&#20110;&#22806;&#37096;&#27169;&#22411;&#30340;&#26816;&#27979;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#20173;&#28982;&#23384;&#22312;&#19968;&#20010;&#26410;&#28145;&#20837;&#25506;&#35752;&#30340;&#39046;&#22495;&#65292;&#21363;&#21033;&#29992;LVLM&#26412;&#36523;&#26469;&#20943;&#36731;&#23545;&#35937;&#24187;&#35273;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#36825;&#26679;&#30340;&#30452;&#35273;&#65292;&#21363;LVLM&#20542;&#21521;&#20110;&#23545;&#23384;&#22312;&#30340;&#23545;&#35937;&#20570;&#20986;&#36923;&#36753;&#19968;&#33268;&#30340;&#21453;&#24212;&#65292;&#20294;&#23545;&#24187;&#35273;&#23545;&#35937;&#20570;&#20986;&#19981;&#19968;&#33268;&#30340;&#21453;&#24212;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#36923;&#36753;&#38381;&#29615;&#30340;&#23545;&#35937;&#24187;&#35273;&#26816;&#27979;&#21644;&#20943;&#36731;&#26694;&#26550;&#65292;&#21363;LogicCheckGPT&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#36923;&#36753;&#19968;&#33268;&#24615;&#25506;&#27979;&#26469;&#25552;&#20986;&#20855;&#26377;&#36923;&#36753;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11622v1 Announce Type: cross  Abstract: Object hallucination has been an Achilles' heel which hinders the broader applications of large vision-language models (LVLMs). Object hallucination refers to the phenomenon that the LVLMs claim non-existent objects in the image. To mitigate the object hallucinations, instruction tuning and external model-based detection methods have been proposed, which either require large-scare computational resources or depend on the detection result of external models. However, there remains an under-explored field to utilize the LVLM itself to alleviate object hallucinations. In this work, we adopt the intuition that the LVLM tends to respond logically consistently for existent objects but inconsistently for hallucinated objects. Therefore, we propose a Logical Closed Loop-based framework for Object Hallucination Detection and Mitigation, namely LogicCheckGPT. In specific, we devise logical consistency probing to raise questions with logical corr
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#33258;&#36827;&#21270;&#33258;&#21160;&#32534;&#30721;&#22120;&#23884;&#20837; Q &#32593;&#32476;&#20197;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#25506;&#32034;&#33021;&#21147;&#30340;&#26032;&#26041;&#27861;</title><link>https://arxiv.org/abs/2402.11604</link><description>&lt;p&gt;
&#33258;&#36827;&#21270;&#33258;&#21160;&#32534;&#30721;&#22120;&#23884;&#20837;&#30340; Q &#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Self-evolving Autoencoder Embedded Q-Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11604
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#33258;&#36827;&#21270;&#33258;&#21160;&#32534;&#30721;&#22120;&#23884;&#20837; Q &#32593;&#32476;&#20197;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#25506;&#32034;&#33021;&#21147;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24207;&#36143;&#20915;&#31574;&#20219;&#21153;&#39046;&#22495;&#65292;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#25506;&#32034;&#33021;&#21147;&#23545;&#36890;&#36807;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#33719;&#24471;&#39640;&#22870;&#21169;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#22686;&#24378;&#36825;&#19968;&#20851;&#38190;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861; SAQN&#65292;&#22312;&#20854;&#20013;&#23558;&#19968;&#20010;&#33258;&#36827;&#21270;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;SA&#65289;&#23884;&#20837;&#21040;&#19968;&#20010; Q &#32593;&#32476;&#65288;QN&#65289;&#20013;&#12290;&#22312; SAQN &#20013;&#65292;&#33258;&#36827;&#21270;&#33258;&#21160;&#32534;&#30721;&#22120;&#26550;&#26500;&#38543;&#30528;&#20195;&#29702;&#25506;&#32034;&#29615;&#22659;&#32780;&#35843;&#25972;&#21644;&#36827;&#21270;&#12290;&#36825;&#31181;&#36827;&#21270;&#20351;&#24471;&#33258;&#21160;&#32534;&#30721;&#22120;&#33021;&#22815;&#25429;&#25417;&#21508;&#31181;&#21407;&#22987;&#35266;&#27979;&#24182;&#26377;&#25928;&#22320;&#22312;&#20854;&#28508;&#22312;&#31354;&#38388;&#20013;&#34920;&#31034;&#23427;&#20204;&#12290;&#36890;&#36807;&#21033;&#29992;&#20174;&#32534;&#30721;&#22120;&#29983;&#25104;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#25552;&#21462;&#30340;&#20998;&#35299;&#29366;&#24577;&#65292;QN &#34987;&#35757;&#32451;&#20197;&#30830;&#23450;&#25913;&#21892;&#22870;&#21169;&#30340;&#26368;&#20339;&#34892;&#21160;&#12290;&#22312;&#33258;&#21160;&#32534;&#30721;&#22120;&#26550;&#26500;&#30340;&#36827;&#21270;&#36807;&#31243;&#20013;&#65292;&#37319;&#29992;&#20559;&#24046;-&#26041;&#24046;&#35843;&#33410;&#31574;&#30053;&#26469;&#24341;&#23548;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#20570;&#20986;&#26368;&#20339;&#21453;&#24212;&#12290;&#36825;&#20010;&#31574;&#30053;&#28041;&#21450;&#20004;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65306;&#65288;i&#65289;&#20419;&#36827;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11604v1 Announce Type: new  Abstract: In the realm of sequential decision-making tasks, the exploration capability of a reinforcement learning (RL) agent is paramount for achieving high rewards through interactions with the environment. To enhance this crucial ability, we propose SAQN, a novel approach wherein a self-evolving autoencoder (SA) is embedded with a Q-Network (QN). In SAQN, the self-evolving autoencoder architecture adapts and evolves as the agent explores the environment. This evolution enables the autoencoder to capture a diverse range of raw observations and represent them effectively in its latent space. By leveraging the disentangled states extracted from the encoder generated latent space, the QN is trained to determine optimal actions that improve rewards. During the evolution of the autoencoder architecture, a bias-variance regulatory strategy is employed to elicit the optimal response from the RL agent. This strategy involves two key components: (i) fost
&lt;/p&gt;</description></item><item><title>`spotRiverGUI`&#26159;&#19968;&#20010;&#20026;&#22312;&#32447;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#36229;&#21442;&#25968;&#35843;&#20248;&#30340;&#22270;&#24418;&#29992;&#25143;&#30028;&#38754;&#65292;&#31616;&#21270;&#20102;&#29992;&#25143;&#25163;&#21160;&#25628;&#32034;&#26368;&#20339;&#36229;&#21442;&#25968;&#35774;&#32622;&#30340;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2402.11594</link><description>&lt;p&gt;
&#22312;&#32447;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#36229;&#21442;&#25968;&#35843;&#20248;&#31616;&#21270;--spotRiverGUI
&lt;/p&gt;
&lt;p&gt;
Simplifying Hyperparameter Tuning in Online Machine Learning -- The spotRiverGUI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11594
&lt;/p&gt;
&lt;p&gt;
`spotRiverGUI`&#26159;&#19968;&#20010;&#20026;&#22312;&#32447;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#36229;&#21442;&#25968;&#35843;&#20248;&#30340;&#22270;&#24418;&#29992;&#25143;&#30028;&#38754;&#65292;&#31616;&#21270;&#20102;&#29992;&#25143;&#25163;&#21160;&#25628;&#32034;&#26368;&#20339;&#36229;&#21442;&#25968;&#35774;&#32622;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25209;&#37327;&#26426;&#22120;&#23398;&#20064;(BML)&#22312;&#22788;&#29702;&#22823;&#37327;&#27969;&#25968;&#25454;&#26102;&#23384;&#22312;&#20869;&#23384;&#12289;&#25968;&#25454;&#27969;&#28418;&#31227;&#22788;&#29702;&#21644;&#22788;&#29702;&#26032;&#30340;&#26410;&#30693;&#25968;&#25454;&#31561;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;&#22312;&#32447;&#26426;&#22120;&#23398;&#20064;(OML)&#26159;BML&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#33021;&#22815;&#20197;&#39034;&#24207;&#26041;&#24335;&#22788;&#29702;&#25968;&#25454;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#25968;&#25454;&#27969;&#12290;`river`&#21253;&#26159;&#19968;&#20010;Python OML&#24211;&#65292;&#25552;&#20379;&#20102;&#21508;&#31181;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#21253;&#25324;&#20998;&#31867;&#12289;&#22238;&#24402;&#12289;&#32858;&#31867;&#12289;&#24322;&#24120;&#26816;&#27979;&#31561;&#12290;`spotRiver`&#21253;&#20026;OML&#27169;&#22411;&#25552;&#20379;&#20102;&#36229;&#21442;&#25968;&#35843;&#20248;&#30340;&#26694;&#26550;&#12290;`spotRiverGUI`&#26159;`spotRiver`&#21253;&#30340;&#22270;&#24418;&#29992;&#25143;&#30028;&#38754;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#20102;&#20174;&#24378;&#22823;&#30340;&#31639;&#27861;&#20013;&#36873;&#25321;&#26368;&#20248;&#36229;&#21442;&#25968;&#35774;&#32622;&#30340;&#20415;&#21033;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11594v1 Announce Type: cross  Abstract: Batch Machine Learning (BML) reaches its limits when dealing with very large amounts of streaming data. This is especially true for available memory, handling drift in data streams, and processing new, unknown data. Online Machine Learning (OML) is an alternative to BML that overcomes the limitations of BML. OML is able to process data in a sequential manner, which is especially useful for data streams. The `river` package is a Python OML-library, which provides a variety of online learning algorithms for classification, regression, clustering, anomaly detection, and more. The `spotRiver` package provides a framework for hyperparameter tuning of OML models. The `spotRiverGUI` is a graphical user interface for the `spotRiver` package. The `spotRiverGUI` releases the user from the burden of manually searching for the optimal hyperparameter setting. After the data is provided, users can compare different OML algorithms from the powerful `
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#20351;&#29992;&#21453;&#21521;&#20256;&#25773;&#30340;&#38646;&#38454;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#38477;&#20302;LLM&#24494;&#35843;&#20013;&#30340;&#20869;&#23384;&#25104;&#26412;&#65292;&#36890;&#36807;&#20840;&#38754;&#30340;&#22522;&#20934;&#30740;&#31350;&#25193;&#23637;&#20102;&#23545;&#19981;&#21516;&#30340;ZO&#20248;&#21270;&#25216;&#26415;&#30340;&#25506;&#32034;&#12290;</title><link>https://arxiv.org/abs/2402.11592</link><description>&lt;p&gt;
&#37325;&#26032;&#25506;&#35752;&#38646;&#38454;&#20248;&#21270;&#22312;&#20869;&#23384;&#39640;&#25928;LLM&#24494;&#35843;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#20010;&#22522;&#20934;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11592
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#20351;&#29992;&#21453;&#21521;&#20256;&#25773;&#30340;&#38646;&#38454;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#38477;&#20302;LLM&#24494;&#35843;&#20013;&#30340;&#20869;&#23384;&#25104;&#26412;&#65292;&#36890;&#36807;&#20840;&#38754;&#30340;&#22522;&#20934;&#30740;&#31350;&#25193;&#23637;&#20102;&#23545;&#19981;&#21516;&#30340;ZO&#20248;&#21270;&#25216;&#26415;&#30340;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#30340;&#19981;&#26029;&#21457;&#23637;&#20013;&#65292;&#20351;&#29992;SGD&#21644;Adam&#31561;&#19968;&#38454;&#65288;FO&#65289;&#20248;&#21270;&#22120;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#25104;&#20026;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;LLMs&#20307;&#31215;&#30340;&#22686;&#38271;&#65292;&#30001;&#20110;FO&#26799;&#24230;&#35745;&#31639;&#30340;&#21453;&#21521;&#20256;&#25773;&#65288;BP&#65289;&#24102;&#26469;&#30340;&#24040;&#22823;&#20869;&#23384;&#24320;&#38144;&#26500;&#25104;&#20102;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#65292;&#23588;&#20854;&#23545;&#20110;&#20869;&#23384;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#30340;&#35774;&#22791;&#31471;&#35757;&#32451;&#31561;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36716;&#21521;&#19981;&#20351;&#29992;BP&#30340;&#38646;&#38454;&#65288;ZO&#65289;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;LLM&#24494;&#35843;&#36807;&#31243;&#20013;&#38477;&#20302;&#20869;&#23384;&#25104;&#26412;&#65292;&#26500;&#24314;&#22312;MeZO&#25552;&#20986;&#30340;&#27010;&#24565;&#22522;&#30784;&#19978;&#12290;&#19982;&#20256;&#32479;&#30340;ZO&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#23558;&#25506;&#32034;&#25193;&#23637;&#21040;&#26356;&#24191;&#27867;&#30340;ZO&#20248;&#21270;&#25216;&#26415;&#65292;&#36890;&#36807;&#20840;&#38754;&#30340;&#12289;&#39318;&#27425;&#25512;&#20986;&#30340;&#22522;&#20934;&#30740;&#31350;&#36328;&#36234;&#20116;&#20010;LLM&#31995;&#21015;&#65288;Roberta&#65292;OPT&#65292;LLaMA&#65292;Vicuna&#65292;Mistral&#65289;&#65292;&#19977;&#31181;&#20219;&#21153;&#22797;&#26434;&#24615;&#21644;&#20116;&#31181;&#24494;&#35843;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11592v1 Announce Type: new  Abstract: In the evolving landscape of natural language processing (NLP), fine-tuning pre-trained Large Language Models (LLMs) with first-order (FO) optimizers like SGD and Adam has become standard. Yet, as LLMs grow {in size}, the substantial memory overhead from back-propagation (BP) for FO gradient computation presents a significant challenge. Addressing this issue is crucial, especially for applications like on-device training where memory efficiency is paramount. This paper proposes a shift towards BP-free, zeroth-order (ZO) optimization as a solution for reducing memory costs during LLM fine-tuning, building on the initial concept introduced by MeZO. Unlike traditional ZO-SGD methods, our work expands the exploration to a wider array of ZO optimization techniques, through a comprehensive, first-of-its-kind benchmarking study across five LLM families (Roberta, OPT, LLaMA, Vicuna, Mistral), three task complexities, and five fine-tuning schemes
&lt;/p&gt;</description></item><item><title>PolypNextLSTM&#26159;&#19968;&#20010;&#36731;&#37327;&#19988;&#24555;&#36895;&#30340;&#24687;&#32905;&#35270;&#39057;&#20998;&#21106;&#32593;&#32476;&#65292;&#20351;&#29992;ConvNext&#21644;ConvLSTM&#65292;&#26368;&#22823;&#30340;&#21019;&#26032;&#22312;&#20110;&#21442;&#25968;&#26368;&#23569;&#19988;&#36895;&#24230;&#26368;&#24555;&#65292;&#24615;&#33021;&#36229;&#36234;&#20102;&#20116;&#31181;&#20808;&#36827;&#30340;&#22522;&#20110;&#22270;&#20687;&#21644;&#35270;&#39057;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.11585</link><description>&lt;p&gt;
PolypNextLSTM&#65306;&#20351;&#29992;ConvNext&#21644;ConvLSTM&#30340;&#36731;&#37327;&#32423;&#24555;&#36895;&#24687;&#32905;&#35270;&#39057;&#20998;&#21106;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
PolypNextLSTM: A lightweight and fast polyp video segmentation network using ConvNext and ConvLSTM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11585
&lt;/p&gt;
&lt;p&gt;
PolypNextLSTM&#26159;&#19968;&#20010;&#36731;&#37327;&#19988;&#24555;&#36895;&#30340;&#24687;&#32905;&#35270;&#39057;&#20998;&#21106;&#32593;&#32476;&#65292;&#20351;&#29992;ConvNext&#21644;ConvLSTM&#65292;&#26368;&#22823;&#30340;&#21019;&#26032;&#22312;&#20110;&#21442;&#25968;&#26368;&#23569;&#19988;&#36895;&#24230;&#26368;&#24555;&#65292;&#24615;&#33021;&#36229;&#36234;&#20102;&#20116;&#31181;&#20808;&#36827;&#30340;&#22522;&#20110;&#22270;&#20687;&#21644;&#35270;&#39057;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#24120;&#29992;&#20110;&#24687;&#32905;&#20998;&#21106;&#30340;&#21333;&#22270;&#20687;UNet&#26550;&#26500;&#32570;&#20047;&#20020;&#24202;&#21307;&#29983;&#22312;&#35786;&#26029;&#24687;&#32905;&#26102;&#20174;&#35270;&#39057;&#25968;&#25454;&#20013;&#33719;&#24471;&#30340;&#26102;&#38388;&#27934;&#23519;&#12290;&#20026;&#20102;&#26356;&#24544;&#23454;&#22320;&#21453;&#26144;&#20020;&#24202;&#23454;&#36341;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;PolypNextLSTM&#21033;&#29992;&#22522;&#20110;&#35270;&#39057;&#30340;&#28145;&#24230;&#23398;&#20064;&#65292;&#21033;&#29992;&#26102;&#38388;&#20449;&#24687;&#23454;&#29616;&#20102;&#21331;&#36234;&#30340;&#20998;&#21106;&#24615;&#33021;&#65292;&#21442;&#25968;&#24320;&#38144;&#26368;&#23567;&#65292;&#21487;&#33021;&#36866;&#29992;&#20110;&#36793;&#32536;&#35774;&#22791;&#12290;PolypNextLSTM&#37319;&#29992;&#31867;&#20284;UNet&#30340;&#32467;&#26500;&#65292;ConvNext-Tiny&#20316;&#20026;&#20854;&#20027;&#24178;&#65292;&#31574;&#30053;&#24615;&#22320;&#30465;&#30053;&#26368;&#21518;&#20004;&#23618;&#20197;&#20943;&#23569;&#21442;&#25968;&#24320;&#38144;&#12290;&#25105;&#20204;&#30340;&#26102;&#38388;&#34701;&#21512;&#27169;&#22359;&#65292;&#19968;&#20010;&#21367;&#31215;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;ConvLSTM&#65289;&#65292;&#26377;&#25928;&#22320;&#21033;&#29992;&#26102;&#38388;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#21019;&#26032;&#22312;&#20110;PolypNextLSTM&#65292;&#22312;&#21442;&#25968;&#19978;&#26368;&#30246;&#19988;&#36895;&#24230;&#26368;&#24555;&#65292;&#36229;&#36234;&#20102;&#20116;&#31181;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#22270;&#20687;&#21644;&#35270;&#39057;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;SUN-SEG&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#36328;&#36234;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11585v1 Announce Type: cross  Abstract: Commonly employed in polyp segmentation, single image UNet architectures lack the temporal insight clinicians gain from video data in diagnosing polyps. To mirror clinical practices more faithfully, our proposed solution, PolypNextLSTM, leverages video-based deep learning, harnessing temporal information for superior segmentation performance with the least parameter overhead, making it possibly suitable for edge devices. PolypNextLSTM employs a UNet-like structure with ConvNext-Tiny as its backbone, strategically omitting the last two layers to reduce parameter overhead. Our temporal fusion module, a Convolutional Long Short Term Memory (ConvLSTM), effectively exploits temporal features. Our primary novelty lies in PolypNextLSTM, which stands out as the leanest in parameters and the fastest model, surpassing the performance of five state-of-the-art image and video-based deep learning models. The evaluation of the SUN-SEG dataset spans 
&lt;/p&gt;</description></item><item><title>&#23545;&#22270;&#19978;&#25345;&#32493;&#23398;&#20064;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#21644;&#20998;&#31867;&#65292;&#24357;&#34917;&#20102;&#27431;&#20960;&#37324;&#24471;&#25968;&#25454;&#19978;&#25345;&#32493;&#23398;&#20064;&#30740;&#31350;&#30340;&#19981;&#36275;&#12290;</title><link>https://arxiv.org/abs/2402.11565</link><description>&lt;p&gt;
&#22270;&#19978;&#30340;&#25345;&#32493;&#23398;&#20064;&#65306;&#25361;&#25112;&#12289;&#35299;&#20915;&#26041;&#26696;&#21644;&#26426;&#20250;
&lt;/p&gt;
&lt;p&gt;
Continual Learning on Graphs: Challenges, Solutions, and Opportunities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11565
&lt;/p&gt;
&lt;p&gt;
&#23545;&#22270;&#19978;&#25345;&#32493;&#23398;&#20064;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#21644;&#20998;&#31867;&#65292;&#24357;&#34917;&#20102;&#27431;&#20960;&#37324;&#24471;&#25968;&#25454;&#19978;&#25345;&#32493;&#23398;&#20064;&#30740;&#31350;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26469;&#65292;&#22270;&#25968;&#25454;&#19978;&#30340;&#25345;&#32493;&#23398;&#20064;&#22240;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#20219;&#21153;&#19978;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#20197;&#21450;&#23558;&#39034;&#24207;&#26356;&#26032;&#30340;&#27169;&#22411;&#36866;&#24212;&#26032;&#20986;&#29616;&#30340;&#22270;&#20219;&#21153;&#32780;&#24341;&#36215;&#20102;&#26497;&#22823;&#20851;&#27880;&#12290;&#34429;&#28982;&#20154;&#20204;&#21162;&#21147;&#24635;&#32467;&#20102;&#22312;&#27431;&#20960;&#37324;&#24471;&#25968;&#25454;&#65288;&#20363;&#22914;&#22270;&#20687;&#21644;&#25991;&#26412;&#65289;&#19978;&#25345;&#32493;&#23398;&#20064;&#30740;&#31350;&#30340;&#36827;&#23637;&#65292;&#20294;&#23545;&#20110;&#22270;&#19978;&#30340;&#25345;&#32493;&#23398;&#20064;&#65292;&#21363;&#25345;&#32493;&#22270;&#23398;&#20064;&#65288;CGL&#65289;&#25110;&#32456;&#36523;&#22270;&#23398;&#20064;&#65292;&#20173;&#28982;&#38656;&#35201;&#36827;&#34892;&#31995;&#32479;&#24615;&#23457;&#26597;&#12290;&#22270;&#25968;&#25454;&#22312;&#25968;&#25454;&#32467;&#26500;&#21644;&#24212;&#29992;&#22330;&#26223;&#26041;&#38754;&#35201;&#22797;&#26434;&#24471;&#22810;&#65292;&#36825;&#20351;&#24471;CGL&#20219;&#21153;&#35774;&#32622;&#12289;&#27169;&#22411;&#35774;&#35745;&#21644;&#24212;&#29992;&#21464;&#24471;&#26497;&#20855;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#24357;&#21512;&#24046;&#36317;&#65292;&#25105;&#20204;&#36890;&#36807;&#38416;&#26126;&#19981;&#21516;&#30340;&#20219;&#21153;&#35774;&#32622;&#24182;&#26681;&#25454;&#29305;&#24615;&#23545;&#29616;&#26377;&#30340;&#25345;&#32493;&#22270;&#23398;&#20064;&#65288;CGL&#65289;&#31639;&#27861;&#36827;&#34892;&#20998;&#31867;&#65292;&#25552;&#20379;&#20102;&#23545;&#29616;&#26377;&#25345;&#32493;&#22270;&#23398;&#20064;&#65288;CGL&#65289;&#31639;&#27861;&#30340;&#20840;&#38754;&#35780;&#20272;&#12290;&#25105;&#20204;&#23558;CGL&#26041;&#27861;&#19982;&#20256;&#32479;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11565v1 Announce Type: cross  Abstract: Continual learning on graph data has recently attracted paramount attention for its aim to resolve the catastrophic forgetting problem on existing tasks while adapting the sequentially updated model to newly emerged graph tasks. While there have been efforts to summarize progress on continual learning research over Euclidean data, e.g., images and texts, a systematic review of progress in continual learning on graphs, a.k.a, continual graph learning (CGL) or lifelong graph learning, is still demanding. Graph data are far more complex in terms of data structures and application scenarios, making CGL task settings, model designs, and applications extremely challenging. To bridge the gap, we provide a comprehensive review of existing continual graph learning (CGL) algorithms by elucidating the different task settings and categorizing the existing methods based on their characteristics. We compare the CGL methods with traditional continual
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26102;&#38388;&#35299;&#32806;&#23545;&#27604;&#25193;&#25955;&#27169;&#22411;&#24212;&#29992;&#20110;&#26102;&#31354;&#25554;&#34917;&#65292;&#26088;&#22312;&#36890;&#36807;&#29983;&#25104;&#27169;&#22411;&#26469;&#25552;&#39640;&#39044;&#27979;&#25928;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.11558</link><description>&lt;p&gt;
&#26102;&#38388;&#35299;&#32806;&#23545;&#27604;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#26102;&#31354;&#25554;&#34917;
&lt;/p&gt;
&lt;p&gt;
Temporal Disentangled Contrastive Diffusion Model for Spatiotemporal Imputation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11558
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26102;&#38388;&#35299;&#32806;&#23545;&#27604;&#25193;&#25955;&#27169;&#22411;&#24212;&#29992;&#20110;&#26102;&#31354;&#25554;&#34917;&#65292;&#26088;&#22312;&#36890;&#36807;&#29983;&#25104;&#27169;&#22411;&#26469;&#25552;&#39640;&#39044;&#27979;&#25928;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11558v1 &#21457;&#24067;&#31867;&#22411;: &#26032;&#20869;&#23481; &#25688;&#35201;: &#26102;&#31354;&#25968;&#25454;&#20998;&#26512;&#22312;&#21508;&#20010;&#39046;&#22495;&#33267;&#20851;&#37325;&#35201;&#65292;&#21253;&#25324;&#20132;&#36890;&#12289;&#27668;&#35937;&#21644;&#21307;&#30103;&#20445;&#20581;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#25910;&#38598;&#30340;&#25968;&#25454;&#24448;&#24448;&#22240;&#20256;&#24863;&#22120;&#25925;&#38556;&#21644;&#32593;&#32476;&#20256;&#36755;&#38169;&#35823;&#32780;&#19981;&#23436;&#25972;&#12290;&#26102;&#31354;&#25554;&#34917;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#35266;&#27979;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#22266;&#26377;&#31354;&#38388;&#21644;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#26469;&#39044;&#27979;&#32570;&#22833;&#20540;&#12290;&#20256;&#32479;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#32463;&#20856;&#32479;&#35745;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#24120;&#19981;&#36275;&#22815;&#65292;&#29305;&#21035;&#26159;&#24403;&#25968;&#25454;&#26410;&#33021;&#31526;&#21512;&#20005;&#26684;&#30340;&#20998;&#24067;&#20551;&#35774;&#26102;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#26368;&#36817;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22270;&#24418;&#21644;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65292;&#24050;&#32463;&#34920;&#29616;&#20986;&#22686;&#24378;&#30340;&#21151;&#25928;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#23481;&#26131;&#31215;&#32047;&#35823;&#24046;&#12290;&#29983;&#25104;&#27169;&#22411;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#37319;&#29992;&#65292;&#20197;&#35268;&#36991;&#20381;&#36182;&#20110;&#28508;&#22312;&#19981;&#20934;&#30830;&#30340;&#21382;&#21490;&#25554;&#34917;&#20540;&#36827;&#34892;&#26410;&#26469;&#39044;&#27979;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11558v1 Announce Type: new  Abstract: Spatiotemporal data analysis is pivotal across various domains, including transportation, meteorology, and healthcare. However, the data collected in real-world scenarios often suffers incompleteness due to sensor malfunctions and network transmission errors. Spatiotemporal imputation endeavours to predict missing values by exploiting the inherent spatial and temporal dependencies present in the observed data. Traditional approaches, which rely on classical statistical and machine learning techniques, are often inadequate, particularly when the data fails to meet strict distributional assumptions. In contrast, recent deep learning-based methods, leveraging graph and recurrent neural networks, have demonstrated enhanced efficacy. Nonetheless, these approaches are prone to error accumulation. Generative models have been increasingly adopted to circumvent the reliance on potentially inaccurate historical imputed values for future prediction
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#26679;&#26465;&#25311;&#25554;&#20540;&#36827;&#34892;&#21333;&#21464;&#37327;&#23494;&#24230;&#20272;&#35745;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#32858;&#31867;&#24314;&#27169;&#65292;&#20026;&#26500;&#24314;&#36866;&#29992;&#30340;&#22810;&#20803;&#20998;&#24067;&#25552;&#20379;&#20102;&#26032;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.11552</link><description>&lt;p&gt;
&#22522;&#20110;&#26679;&#26465;&#25311;&#25554;&#20540;&#30340;&#32463;&#39564;&#23494;&#24230;&#20272;&#35745;&#21450;&#20854;&#22312;Copulas&#32858;&#31867;&#24314;&#27169;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Empirical Density Estimation based on Spline Quasi-Interpolation with applications to Copulas clustering modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11552
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#26679;&#26465;&#25311;&#25554;&#20540;&#36827;&#34892;&#21333;&#21464;&#37327;&#23494;&#24230;&#20272;&#35745;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#32858;&#31867;&#24314;&#27169;&#65292;&#20026;&#26500;&#24314;&#36866;&#29992;&#30340;&#22810;&#20803;&#20998;&#24067;&#25552;&#20379;&#20102;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23494;&#24230;&#20272;&#35745;&#26159;&#21508;&#20010;&#39046;&#22495;&#20869;&#29992;&#20110;&#24314;&#27169;&#21644;&#29702;&#35299;&#25968;&#25454;&#22522;&#30784;&#20998;&#24067;&#30340;&#22522;&#26412;&#25216;&#26415;&#12290;&#23494;&#24230;&#20272;&#35745;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#20272;&#35745;&#38543;&#26426;&#21464;&#37327;&#30340;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#12290;&#22312;&#22788;&#29702;&#21333;&#21464;&#37327;&#25110;&#22810;&#21464;&#37327;&#25968;&#25454;&#26102;&#65292;&#36825;&#19968;&#36807;&#31243;&#23588;&#20026;&#37325;&#35201;&#65292;&#23545;&#20110;&#32858;&#31867;&#12289;&#24322;&#24120;&#26816;&#27979;&#21644;&#29983;&#25104;&#24314;&#27169;&#31561;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#26679;&#26465;&#25311;&#25554;&#20540;&#26469;&#36817;&#20284;&#21333;&#21464;&#37327;&#23494;&#24230;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#32858;&#31867;&#24314;&#27169;&#30340;&#26041;&#27861;&#12290;&#25152;&#20351;&#29992;&#30340;&#32858;&#31867;&#25216;&#26415;&#22522;&#20110;&#26500;&#24314;&#36866;&#29992;&#30340;&#22810;&#20803;&#20998;&#24067;&#65292;&#36825;&#21462;&#20915;&#20110;&#23545;&#21333;&#21464;&#37327;&#32463;&#39564;&#23494;&#24230;&#65288;&#36793;&#38469;&#23494;&#24230;&#65289;&#30340;&#20272;&#35745;&#12290;&#35813;&#36924;&#36817;&#26159;&#36890;&#36807;&#20351;&#29992;&#25552;&#20986;&#30340;&#26679;&#26465;&#25311;&#25554;&#20540;&#23454;&#29616;&#30340;&#65292;&#21516;&#26102;&#29992;&#20110;&#24314;&#27169;&#25152;&#23547;&#27714;&#30340;&#32858;&#31867;&#21010;&#20998;&#30340;&#32852;&#21512;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11552v1 Announce Type: cross  Abstract: Density estimation is a fundamental technique employed in various fields to model and to understand the underlying distribution of data. The primary objective of density estimation is to estimate the probability density function of a random variable. This process is particularly valuable when dealing with univariate or multivariate data and is essential for tasks such as clustering, anomaly detection, and generative modeling. In this paper we propose the mono-variate approximation of the density using spline quasi interpolation and we applied it in the context of clustering modeling. The clustering technique used is based on the construction of suitable multivariate distributions which rely on the estimation of the monovariate empirical densities (marginals). Such an approximation is achieved by using the proposed spline quasi-interpolation, while the joint distributions to model the sought clustering partition is constructed with the 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;PASCL&#31639;&#27861;&#65292;&#22312;&#39640;&#33021;&#29289;&#29702;&#23398;&#20013;&#20351;&#29992;&#24102;&#25200;&#21160;&#22686;&#24378;&#30340;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#65292;&#21033;&#29992;&#22270;&#32467;&#26500;&#37325;&#24314;&#31890;&#23376;&#34928;&#21464;&#23618;&#27425;&#26641;&#12290;</title><link>https://arxiv.org/abs/2402.11538</link><description>&lt;p&gt;
PASCL: &#24102;&#25200;&#21160;&#22686;&#24378;&#30340;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#31890;&#23376;&#34928;&#21464;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
PASCL: Supervised Contrastive Learning with Perturbative Augmentation for Particle Decay Reconstruction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11538
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;PASCL&#31639;&#27861;&#65292;&#22312;&#39640;&#33021;&#29289;&#29702;&#23398;&#20013;&#20351;&#29992;&#24102;&#25200;&#21160;&#22686;&#24378;&#30340;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#65292;&#21033;&#29992;&#22270;&#32467;&#26500;&#37325;&#24314;&#31890;&#23376;&#34928;&#21464;&#23618;&#27425;&#26641;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#33021;&#29289;&#29702;&#23398;&#20013;&#65292;&#22312;&#30896;&#25758;&#20107;&#20214;&#20013;&#20135;&#29983;&#30340;&#31890;&#23376;&#20197;&#23618;&#27425;&#26641;&#32467;&#26500;&#30340;&#24418;&#24335;&#34928;&#21464;&#65292;&#21482;&#26377;&#26368;&#32456;&#30340;&#34928;&#21464;&#20135;&#29289;&#21487;&#20197;&#36890;&#36807;&#25506;&#27979;&#22120;&#35266;&#27979;&#21040;&#12290;&#28982;&#32780;&#65292;&#21487;&#33021;&#30340;&#26641;&#32467;&#26500;&#24040;&#22823;&#30340;&#25490;&#21015;&#31354;&#38388;&#20351;&#24471;&#22312;&#32473;&#23450;&#19968;&#32452;&#26368;&#32456;&#31890;&#23376;&#30340;&#24773;&#20917;&#19979;&#37325;&#24314;&#23454;&#38469;&#34928;&#21464;&#36807;&#31243;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#20998;&#26512;&#23618;&#27425;&#26641;&#32467;&#26500;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22270;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#25512;&#26029;&#26641;&#32467;&#26500;&#20197;&#37325;&#24314;&#30896;&#25758;&#20107;&#20214;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#32039;&#20945;&#30340;&#30697;&#38453;&#34920;&#31034;&#65292;&#31216;&#20026;&#26368;&#20302;&#20844;&#20849;&#31062;&#20808;&#20195;&#25968;&#65288;LCAG&#65289;&#30697;&#38453;&#65292;&#26469;&#32534;&#30721;&#31890;&#23376;&#34928;&#21464;&#26641;&#32467;&#26500;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#24212;&#29992;&#20110;&#33410;&#28857;&#29305;&#24449;&#30340;&#25200;&#21160;&#22686;&#24378;&#25216;&#26415;&#65292;&#26088;&#22312;&#27169;&#25311;&#23454;&#39564;&#19981;&#30830;&#23450;&#24615;&#24182;&#22686;&#21152;&#25968;&#25454;&#22810;&#26679;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30417;&#30563;&#22270;&#23545;&#27604;&#23398;&#20064;&#31639;&#27861;&#65292;&#21033;&#29992;&#26469;&#33258;&#22810;&#20010;&#31890;&#23376;&#20043;&#38388;&#20851;&#31995;&#30340;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11538v1 Announce Type: cross  Abstract: In high-energy physics, particles produced in collision events decay in a format of a hierarchical tree structure, where only the final decay products can be observed using detectors. However, the large combinatorial space of possible tree structures makes it challenging to recover the actual decay process given a set of final particles. To better analyse the hierarchical tree structure, we propose a graph-based deep learning model to infer the tree structure to reconstruct collision events. In particular, we use a compact matrix representation termed as lowest common ancestor generations (LCAG) matrix, to encode the particle decay tree structure. Then, we introduce a perturbative augmentation technique applied to node features, aiming to mimic experimental uncertainties and increase data diversity. We further propose a supervised graph contrastive learning algorithm to utilize the information of inter-particle relations from multiple 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#26469;&#25913;&#36827;&#32763;&#35793;&#36136;&#37327;&#30340;&#25104;&#26412;&#25928;&#30410;&#20559;&#22909;&#23398;&#20064;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#36890;&#36807;&#20248;&#21270;&#22870;&#21169;&#27169;&#22411;&#26469;&#21306;&#20998;&#20154;&#31867;&#21644;&#26426;&#22120;&#32763;&#35793;&#65292;&#20174;&#32780;&#25351;&#23548;&#25913;&#36827;&#26426;&#22120;&#32763;&#35793;&#12290;</title><link>https://arxiv.org/abs/2402.11525</link><description>&lt;p&gt;
&#29992;RLHF&#25512;&#36827;&#32763;&#35793;&#20559;&#22909;&#24314;&#27169;&#65306;&#36808;&#21521;&#25104;&#26412;&#25928;&#30410;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Advancing Translation Preference Modeling with RLHF: A Step Towards Cost-Effective Solution
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11525
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#26469;&#25913;&#36827;&#32763;&#35793;&#36136;&#37327;&#30340;&#25104;&#26412;&#25928;&#30410;&#20559;&#22909;&#23398;&#20064;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#36890;&#36807;&#20248;&#21270;&#22870;&#21169;&#27169;&#22411;&#26469;&#21306;&#20998;&#20154;&#31867;&#21644;&#26426;&#22120;&#32763;&#35793;&#65292;&#20174;&#32780;&#25351;&#23548;&#25913;&#36827;&#26426;&#22120;&#32763;&#35793;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11525v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032; &#30495;&#23454;&#24615;&#12289;&#34920;&#36798;&#21147;&#21644;&#20248;&#38597;&#26159;&#26426;&#22120;&#32763;&#35793;&#20013;&#19981;&#26029;&#36861;&#27714;&#30340;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#24230;&#37327;&#26631;&#20934;&#22914;BLEU&#24182;&#19981;&#20005;&#26684;&#31526;&#21512;&#20154;&#31867;&#23545;&#32763;&#35793;&#36136;&#37327;&#30340;&#20559;&#22909;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#26469;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#12290;&#25910;&#38598;&#20154;&#31867;&#23545;&#32763;&#35793;&#20043;&#38388;&#30340;&#27604;&#36739;&#30340;&#22823;&#35268;&#27169;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#24182;&#19981;&#23481;&#26131;&#65292;&#23588;&#20854;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25104;&#26412;&#25928;&#30410;&#30340;&#20559;&#22909;&#23398;&#20064;&#31574;&#30053;&#65292;&#36890;&#36807;&#21306;&#20998;&#20154;&#31867;&#21644;&#26426;&#22120;&#32763;&#35793;&#26469;&#20248;&#21270;&#22870;&#21169;&#27169;&#22411;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#22870;&#21169;&#27169;&#22411;&#23398;&#20064;&#26426;&#22120;&#32763;&#35793;&#19982;&#20154;&#31867;&#20043;&#38388;&#30340;&#19981;&#36275;&#20043;&#22788;&#65292;&#24182;&#25351;&#23548;&#38543;&#21518;&#23545;&#26426;&#22120;&#32763;&#35793;&#30340;&#25913;&#36827;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;RLHF&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#21319;&#32763;&#35793;&#36136;&#37327;&#65292;&#36825;&#31181;&#25913;&#36827;&#20063;&#26377;&#30410;&#20110;&#20854;&#20182;&#32763;&#35793;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11525v1 Announce Type: new  Abstract: Faithfulness, expressiveness, and elegance is the constant pursuit in machine translation. However, traditional metrics like \textit{BLEU} do not strictly align with human preference of translation quality. In this paper, we explore leveraging reinforcement learning with human feedback (\textit{RLHF}) to improve translation quality. It is non-trivial to collect a large high-quality dataset of human comparisons between translations, especially for low-resource languages. To address this issue, we propose a cost-effective preference learning strategy, optimizing reward models by distinguishing between human and machine translations. In this manner, the reward model learns the deficiencies of machine translation compared to human and guides subsequent improvements in machine translation. Experimental results demonstrate that \textit{RLHF} can effectively enhance translation quality and this improvement benefits other translation directions 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#20803;&#32467;&#26500;&#25628;&#32034;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#25163;&#24037;&#35774;&#35745;&#20803;&#32467;&#26500;&#19981;&#26131;&#25193;&#23637;&#20197;&#21450;&#24573;&#35270;&#21487;&#35299;&#37322;&#24615;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2402.11518</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#24322;&#36136;&#20449;&#24687;&#32593;&#32476;&#20013;&#20803;&#32467;&#26500;&#30340;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Large Language Model-driven Meta-structure Discovery in Heterogeneous Information Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11518
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#20803;&#32467;&#26500;&#25628;&#32034;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#25163;&#24037;&#35774;&#35745;&#20803;&#32467;&#26500;&#19981;&#26131;&#25193;&#23637;&#20197;&#21450;&#24573;&#35270;&#21487;&#35299;&#37322;&#24615;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#36136;&#20449;&#24687;&#32593;&#32476;&#65288;HIN&#65289;&#22240;&#33021;&#22815;&#25429;&#25417;&#19981;&#21516;&#31867;&#22411;&#33410;&#28857;&#20043;&#38388;&#22797;&#26434;&#20851;&#31995;&#32780;&#26085;&#30410;&#21463;&#21040;&#38738;&#30544;&#12290;&#20803;&#32467;&#26500;&#34987;&#25552;&#20986;&#29992;&#20110;&#35782;&#21035;HIN&#19978;&#30340;&#37325;&#35201;&#20851;&#31995;&#27169;&#24335;&#65292;&#24050;&#35777;&#26126;&#22312;&#25552;&#21462;&#20016;&#23500;&#35821;&#20041;&#20449;&#24687;&#21644;&#20419;&#36827;&#22270;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#34920;&#36798;&#21147;&#34920;&#31034;&#26041;&#38754;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#25163;&#24037;&#35774;&#35745;&#30340;&#20803;&#32467;&#26500;&#22312;&#25193;&#23637;&#24615;&#26041;&#38754;&#23384;&#22312;&#25361;&#25112;&#65292;&#36825;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20197;&#21457;&#23637;&#33258;&#21160;&#20803;&#32467;&#26500;&#25628;&#32034;&#31639;&#27861;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#38598;&#20013;&#20110;&#23547;&#25214;&#20855;&#26377;&#33391;&#22909;&#32463;&#39564;&#39044;&#27979;&#24615;&#33021;&#30340;&#20803;&#32467;&#26500;&#65292;&#32780;&#24573;&#35270;&#20102;&#21487;&#35299;&#37322;&#24615;&#12290;&#22240;&#27492;&#65292;&#20182;&#20204;&#24448;&#24448;&#20135;&#29983;&#26131;&#20110;&#36807;&#24230;&#25311;&#21512;&#21644;&#20154;&#31867;&#38590;&#20197;&#29702;&#35299;&#30340;&#20803;&#32467;&#26500;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26032;&#20852;&#30340;&#25512;&#29702;&#33021;&#21147;&#20013;&#33719;&#21462;&#21551;&#31034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;REasoning meta-STRUCTure search&#65288;ReStruct&#65289;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11518v1 Announce Type: new  Abstract: Heterogeneous information networks (HIN) have gained increasing popularity for being able to capture complex relations between nodes of diverse types. Meta-structure was proposed to identify important patterns of relations on HIN, which has been proven effective for extracting rich semantic information and facilitating graph neural networks to learn expressive representations. However, hand-crafted meta-structures pose challenges for scaling up, which draws wide research attention for developing automatic meta-structure search algorithms. Previous efforts concentrate on searching for meta-structures with good empirical prediction performance, overlooking explainability. Thus, they often produce meta-structures prone to overfitting and incomprehensible to humans. To address this, we draw inspiration from the emergent reasoning abilities of large language models (LLMs). We propose a novel REasoning meta-STRUCTure search (ReStruct) framewor
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#19987;&#27880;&#20110;&#20248;&#21270;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#27969;&#20307;&#21147;&#23398;&#20013;&#20027;&#21160;&#27969;&#25511;&#21046;&#20013;&#30340;&#24182;&#34892;&#35774;&#32622;&#65292;&#36890;&#36807;&#25286;&#35299;DRL&#26694;&#26550;&#12289;&#36827;&#34892;&#25193;&#23637;&#24615;&#22522;&#20934;&#27979;&#35797;&#12289;&#25552;&#20986;&#28151;&#21512;&#24182;&#34892;&#21270;&#37197;&#32622;&#24182;&#20248;&#21270;&#22810;&#29615;&#22659;DRL&#35757;&#32451;&#20013;&#30340;I/O&#25805;&#20316;&#65292;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#24182;&#34892;&#21270;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.11515</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#27969;&#20307;&#21147;&#23398;&#20013;&#20027;&#21160;&#27969;&#25511;&#21046;&#20013;&#30340;&#26368;&#20339;&#24182;&#34892;&#21270;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Optimal Parallelization Strategies for Active Flow Control in Deep Reinforcement Learning-Based Computational Fluid Dynamics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11515
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#19987;&#27880;&#20110;&#20248;&#21270;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#27969;&#20307;&#21147;&#23398;&#20013;&#20027;&#21160;&#27969;&#25511;&#21046;&#20013;&#30340;&#24182;&#34892;&#35774;&#32622;&#65292;&#36890;&#36807;&#25286;&#35299;DRL&#26694;&#26550;&#12289;&#36827;&#34892;&#25193;&#23637;&#24615;&#22522;&#20934;&#27979;&#35797;&#12289;&#25552;&#20986;&#28151;&#21512;&#24182;&#34892;&#21270;&#37197;&#32622;&#24182;&#20248;&#21270;&#22810;&#29615;&#22659;DRL&#35757;&#32451;&#20013;&#30340;I/O&#25805;&#20316;&#65292;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#24182;&#34892;&#21270;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#24050;&#34987;&#35777;&#26126;&#26159;&#22788;&#29702;&#39640;&#21160;&#24577;&#21644;&#38750;&#32447;&#24615;&#20027;&#21160;&#27969;&#25511;&#21046;&#65288;AFC&#65289;&#38382;&#39064;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#19982;&#35757;&#32451;DRL&#27169;&#22411;&#30456;&#20851;&#30340;&#35745;&#31639;&#25104;&#26412;&#26500;&#25104;&#20102;&#37325;&#35201;&#30340;&#24615;&#33021;&#29942;&#39048;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#24182;&#22312;&#39640;&#24615;&#33021;&#35745;&#31639;&#26550;&#26500;&#19978;&#23454;&#29616;&#26377;&#25928;&#30340;&#25193;&#23637;&#65292;&#26412;&#30740;&#31350;&#20391;&#37325;&#20110;&#20248;&#21270;&#24182;&#34892;&#35774;&#32622;&#20013;&#30340;&#22522;&#20110;DRL&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;&#29992;&#20110;AFC&#38382;&#39064;&#30340;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;DRL&#26694;&#26550;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#25928;&#29575;&#29942;&#39048;&#12290;&#38543;&#21518;&#65292;&#36890;&#36807;&#25286;&#35299;&#25972;&#20307;&#26694;&#26550;&#65292;&#24182;&#20026;&#21508;&#20010;&#32452;&#20214;&#36827;&#34892;&#24191;&#27867;&#30340;&#21487;&#25193;&#23637;&#24615;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21508;&#31181;&#28151;&#21512;&#24182;&#34892;&#21270;&#37197;&#32622;&#65292;&#24182;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#24182;&#34892;&#21270;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20248;&#21270;&#20102;&#22810;&#29615;&#22659;DRL&#35757;&#32451;&#20013;&#30340;&#36755;&#20837;/&#36755;&#20986;&#65288;I/O&#65289;&#25805;&#20316;&#65292;&#20197;&#35299;&#20915;&#19982;&#25968;&#25454;&#31227;&#21160;&#30456;&#20851;&#30340;&#20851;&#38190;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11515v1 Announce Type: new  Abstract: Deep Reinforcement Learning (DRL) has emerged as a promising approach for handling highly dynamic and nonlinear Active Flow Control (AFC) problems. However, the computational cost associated with training DRL models presents a significant performance bottleneck. To address this challenge and enable efficient scaling on high-performance computing architectures, this study focuses on optimizing DRL-based algorithms in parallel settings. We validate an existing state-of-the-art DRL framework used for AFC problems and discuss its efficiency bottlenecks. Subsequently, by deconstructing the overall framework and conducting extensive scalability benchmarks for individual components, we investigate various hybrid parallelization configurations and propose efficient parallelization strategies. Moreover, we refine input/output (I/O) operations in multi-environment DRL training to tackle critical overhead associated with data movement. Finally, we 
&lt;/p&gt;</description></item><item><title>URLBERT&#26159;&#31532;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;URL&#20998;&#31867;&#25110;&#26816;&#27979;&#20219;&#21153;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24341;&#20837;&#20102;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#21644;&#34394;&#25311;&#23545;&#25239;&#35757;&#32451;&#20004;&#31181;&#26032;&#39062;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#20197;&#21152;&#24378;&#27169;&#22411;&#23545;URL&#32467;&#26500;&#30340;&#29702;&#35299;&#21644;&#25552;&#39640;&#20174;URL&#20013;&#25552;&#21462;&#35821;&#20041;&#29305;&#24449;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.11495</link><description>&lt;p&gt;
URLBERT&#65306;&#19968;&#31181;&#29992;&#20110;URL&#20998;&#31867;&#30340;&#23545;&#27604;&#21644;&#23545;&#25239;&#39044;&#35757;&#32451;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
URLBERT:A Contrastive and Adversarial Pre-trained Model for URL Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11495
&lt;/p&gt;
&lt;p&gt;
URLBERT&#26159;&#31532;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;URL&#20998;&#31867;&#25110;&#26816;&#27979;&#20219;&#21153;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24341;&#20837;&#20102;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#21644;&#34394;&#25311;&#23545;&#25239;&#35757;&#32451;&#20004;&#31181;&#26032;&#39062;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#20197;&#21152;&#24378;&#27169;&#22411;&#23545;URL&#32467;&#26500;&#30340;&#29702;&#35299;&#21644;&#25552;&#39640;&#20174;URL&#20013;&#25552;&#21462;&#35821;&#20041;&#29305;&#24449;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv&#65306;2402.11495v1 &#21457;&#34920;&#31867;&#22411;&#65306;&#36328;&#39046;&#22495;&#25688;&#35201;&#65306;URL&#22312;&#29702;&#35299;&#21644;&#20998;&#31867;&#32593;&#32476;&#20869;&#23481;&#26041;&#38754;&#21457;&#25381;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#19982;&#23433;&#20840;&#25511;&#21046;&#21644;&#22312;&#32447;&#25512;&#33616;&#30456;&#20851;&#30340;&#20219;&#21153;&#20013;&#12290;&#23613;&#31649;&#39044;&#35757;&#32451;&#27169;&#22411;&#30446;&#21069;&#22312;&#21508;&#20010;&#39046;&#22495;&#21344;&#25454;&#20027;&#23548;&#22320;&#20301;&#65292;&#20294;URL&#20998;&#26512;&#39046;&#22495;&#20173;&#32570;&#20047;&#19987;&#38376;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;URLBERT&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#24212;&#29992;&#20110;&#21508;&#31181;URL&#20998;&#31867;&#25110;&#26816;&#27979;&#20219;&#21153;&#30340;&#39044;&#35757;&#32451;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#25968;&#21313;&#20159;&#20010;URL&#30340;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#20102;&#19968;&#20010;URL&#26631;&#35760;&#22120;&#65292;&#20197;&#35299;&#20915;URL&#25968;&#25454;&#30340;&#26631;&#35760;&#21270;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#65306;&#65288;1&#65289;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#20219;&#21153;&#65292;&#36890;&#36807;&#21306;&#20998;&#30456;&#21516;URL&#30340;&#19981;&#21516;&#21464;&#20307;&#26469;&#22686;&#24378;&#27169;&#22411;&#23545;URL&#32467;&#26500;&#30340;&#29702;&#35299;&#21644;&#23545;&#31867;&#21035;&#24046;&#24322;&#30340;&#25429;&#25417;&#65307;&#65288;2&#65289;&#34394;&#25311;&#23545;&#25239;&#35757;&#32451;&#65292;&#26088;&#22312;&#25552;&#39640;&#27169;&#22411;&#20174;URL&#20013;&#25552;&#21462;&#35821;&#20041;&#29305;&#24449;&#30340;&#40065;&#26834;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11495v1 Announce Type: cross  Abstract: URLs play a crucial role in understanding and categorizing web content, particularly in tasks related to security control and online recommendations. While pre-trained models are currently dominating various fields, the domain of URL analysis still lacks specialized pre-trained models. To address this gap, this paper introduces URLBERT, the first pre-trained representation learning model applied to a variety of URL classification or detection tasks. We first train a URL tokenizer on a corpus of billions of URLs to address URL data tokenization. Additionally, we propose two novel pre-training tasks: (1) self-supervised contrastive learning tasks, which strengthen the model's understanding of URL structure and the capture of category differences by distinguishing different variants of the same URL; (2) virtual adversarial training, aimed at improving the model's robustness in extracting semantic features from URLs. Finally, our proposed 
&lt;/p&gt;</description></item><item><title>GNN&#22312;&#31163;&#32676;&#20998;&#24067;&#27867;&#21270;&#20013;&#30340;&#22833;&#36133;&#20851;&#38190;&#22312;&#20110;&#26469;&#33258;&#29615;&#22659;&#30340;&#28508;&#22312;&#28151;&#26434;&#20559;&#24046;&#65292;&#22240;&#27492;&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#21407;&#21017;&#24615;&#30340;&#26041;&#27861;&#26469;&#35757;&#32451;&#31283;&#20581;GNN&#12290;</title><link>https://arxiv.org/abs/2402.11494</link><description>&lt;p&gt;
&#36890;&#36807;&#22240;&#26524;&#24178;&#39044;&#23454;&#29616;&#22270;&#24418;&#30340;&#31163;&#32676;&#20998;&#24067;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Graph Out-of-Distribution Generalization via Causal Intervention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11494
&lt;/p&gt;
&lt;p&gt;
GNN&#22312;&#31163;&#32676;&#20998;&#24067;&#27867;&#21270;&#20013;&#30340;&#22833;&#36133;&#20851;&#38190;&#22312;&#20110;&#26469;&#33258;&#29615;&#22659;&#30340;&#28508;&#22312;&#28151;&#26434;&#20559;&#24046;&#65292;&#22240;&#27492;&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#21407;&#21017;&#24615;&#30340;&#26041;&#27861;&#26469;&#35757;&#32451;&#31283;&#20581;GNN&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32676;&#20998;&#24067;&#65288;OOD&#65289;&#27867;&#21270;&#22312;&#22270;&#24418;&#23398;&#20064;&#20013;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#22240;&#20026;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#20998;&#24067;&#36716;&#31227;&#26102;&#36890;&#24120;&#20250;&#34920;&#29616;&#20986;&#24615;&#33021;&#19979;&#38477;&#12290;&#26412;&#25991;&#20174;&#33258;&#19979;&#32780;&#19978;&#30340;&#25968;&#25454;&#29983;&#25104;&#35282;&#24230;&#20986;&#21457;&#65292;&#36890;&#36807;&#22240;&#26524;&#20998;&#26512;&#25581;&#31034;&#20102;&#19968;&#20010;&#20851;&#38190;&#35266;&#23519;&#65306;GNN&#22312;OOD&#27867;&#21270;&#20013;&#22833;&#36133;&#30340;&#20851;&#38190;&#22312;&#20110;&#26469;&#33258;&#29615;&#22659;&#30340;&#28508;&#22312;&#28151;&#26434;&#20559;&#24046;&#12290;&#21518;&#32773;&#35823;&#23548;&#27169;&#22411;&#21033;&#29992;&#33258;&#25105;&#22270;&#29305;&#24449;&#21644;&#30446;&#26631;&#33410;&#28857;&#26631;&#31614;&#20043;&#38388;&#30340;&#29615;&#22659;&#25935;&#24863;&#30456;&#20851;&#24615;&#65292;&#23548;&#33268;&#22312;&#26032;&#30340;&#26410;&#35265;&#33410;&#28857;&#19978;&#19981;&#33391;&#27867;&#21270;&#12290;&#22522;&#20110;&#36825;&#19968;&#20998;&#26512;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22312;&#33410;&#28857;&#32423;&#21035;&#20998;&#24067;&#36716;&#31227;&#19979;&#35757;&#32451;&#31283;&#20581;GNN&#30340;&#27010;&#24565;&#31616;&#21333;&#32780;&#21448;&#21407;&#21017;&#24615;&#30340;&#26041;&#27861;&#65292;&#32780;&#19981;&#38656;&#35201;&#29615;&#22659;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11494v1 Announce Type: new  Abstract: Out-of-distribution (OOD) generalization has gained increasing attentions for learning on graphs, as graph neural networks (GNNs) often exhibit performance degradation with distribution shifts. The challenge is that distribution shifts on graphs involve intricate interconnections between nodes, and the environment labels are often absent in data. In this paper, we adopt a bottom-up data-generative perspective and reveal a key observation through causal analysis: the crux of GNNs' failure in OOD generalization lies in the latent confounding bias from the environment. The latter misguides the model to leverage environment-sensitive correlations between ego-graph features and target nodes' labels, resulting in undesirable generalization on new unseen nodes. Built upon this analysis, we introduce a conceptually simple yet principled approach for training robust GNNs under node-level distribution shifts, without prior knowledge of environment
&lt;/p&gt;</description></item><item><title>LEIA&#26159;&#19968;&#31181;&#35821;&#35328;&#36866;&#24212;&#35843;&#25972;&#26041;&#27861;&#65292;&#21033;&#29992;&#32500;&#22522;&#30334;&#31185;&#23454;&#20307;&#21517;&#31216;&#36328;&#35821;&#35328;&#22686;&#24378;&#30446;&#26631;&#35821;&#35328;&#35821;&#26009;&#24211;&#65292;&#36890;&#36807;&#24038;&#21040;&#21491;&#30340;&#35821;&#35328;&#24314;&#27169;&#35757;&#32451;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#21508;&#31181;&#38750;&#33521;&#35821;&#35821;&#35328;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.11485</link><description>&lt;p&gt;
LEIA: &#21033;&#29992;&#22522;&#20110;&#23454;&#20307;&#30340;&#25968;&#25454;&#22686;&#24378;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#20419;&#36827;&#36328;&#35821;&#35328;&#30693;&#35782;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
LEIA: Facilitating Cross-Lingual Knowledge Transfer in Language Models with Entity-based Data Augmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11485
&lt;/p&gt;
&lt;p&gt;
LEIA&#26159;&#19968;&#31181;&#35821;&#35328;&#36866;&#24212;&#35843;&#25972;&#26041;&#27861;&#65292;&#21033;&#29992;&#32500;&#22522;&#30334;&#31185;&#23454;&#20307;&#21517;&#31216;&#36328;&#35821;&#35328;&#22686;&#24378;&#30446;&#26631;&#35821;&#35328;&#35821;&#26009;&#24211;&#65292;&#36890;&#36807;&#24038;&#21040;&#21491;&#30340;&#35821;&#35328;&#24314;&#27169;&#35757;&#32451;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#21508;&#31181;&#38750;&#33521;&#35821;&#35821;&#35328;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22522;&#20110;&#33521;&#35821;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36866;&#24212;&#20854;&#20182;&#35821;&#35328;&#30340;&#25805;&#20316;&#30001;&#20110;&#36328;&#35821;&#35328;&#36716;&#31227;&#30340;&#25928;&#29575;&#21644;&#28508;&#21147;&#32780;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35821;&#35328;&#36866;&#24212;&#26041;&#27861;&#24120;&#24120;&#24573;&#35270;&#36328;&#35821;&#35328;&#30417;&#30563;&#30340;&#22909;&#22788;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;LEIA&#65292;&#19968;&#31181;&#21033;&#29992;&#36328;&#35821;&#35328;&#23545;&#40784;&#30340;&#32500;&#22522;&#30334;&#31185;&#23454;&#20307;&#21517;&#31216;&#30340;&#35821;&#35328;&#36866;&#24212;&#35843;&#25972;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#28041;&#21450;&#20351;&#29992;&#33521;&#35821;&#23454;&#20307;&#21517;&#31216;&#22686;&#24378;&#30446;&#26631;&#35821;&#35328;&#35821;&#26009;&#24211;&#65292;&#24182;&#20351;&#29992;&#20174;&#24038;&#21040;&#21491;&#30340;&#35821;&#35328;&#24314;&#27169;&#35757;&#32451;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#22810;&#26679;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;LEIA&#65292;&#20351;&#29992;7B&#21442;&#25968;&#30340;LLMs&#65292;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#38750;&#33521;&#35821;&#35821;&#35328;&#19978;&#30340;&#26174;&#33879;&#24615;&#33021;&#22686;&#30410;&#12290;&#28304;&#20195;&#30721;&#21487;&#22312;https://github.com/studio-ousia/leia&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11485v1 Announce Type: cross  Abstract: Adapting English-based large language models (LLMs) to other languages has become increasingly popular due to the efficiency and potential of cross-lingual transfer. However, existing language adaptation methods often overlook the benefits of cross-lingual supervision. In this study, we introduce LEIA, a language adaptation tuning method that utilizes Wikipedia entity names aligned across languages. This method involves augmenting the target language corpus with English entity names and training the model using left-to-right language modeling. We assess LEIA on diverse question answering datasets using 7B-parameter LLMs, demonstrating significant performance gains across various non-English languages. The source code is available at https://github.com/studio-ousia/leia.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22270;&#25552;&#31034;&#23398;&#20064;&#30340;DDIPrompt&#26694;&#26550;&#26088;&#22312;&#35299;&#20915;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#20107;&#20214;&#39044;&#27979;&#20013;&#30340;&#39640;&#24230;&#19981;&#24179;&#34913;&#20107;&#20214;&#20998;&#24067;&#21644;&#32597;&#35265;&#20107;&#20214;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.11472</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#25552;&#31034;&#23398;&#20064;&#30340;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#20107;&#20214;&#39044;&#27979;&#65306;DDIPrompt
&lt;/p&gt;
&lt;p&gt;
DDIPrompt: Drug-Drug Interaction Event Prediction based on Graph Prompt Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11472
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22270;&#25552;&#31034;&#23398;&#20064;&#30340;DDIPrompt&#26694;&#26550;&#26088;&#22312;&#35299;&#20915;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#20107;&#20214;&#39044;&#27979;&#20013;&#30340;&#39640;&#24230;&#19981;&#24179;&#34913;&#20107;&#20214;&#20998;&#24067;&#21644;&#32597;&#35265;&#20107;&#20214;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30001;&#20110;&#20854;&#22312;&#24314;&#27169;&#33647;&#29289;&#20998;&#23376;&#20869;&#37096;&#21644;&#20043;&#38388;&#21407;&#23376;&#21644;&#21151;&#33021;&#22242;&#20043;&#38388;&#22797;&#26434;&#20851;&#32852;&#26041;&#38754;&#30340;&#29087;&#32451;&#34920;&#29616;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#39044;&#27979;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#20107;&#20214;&#65288;DDI&#65289;&#26041;&#38754;&#21464;&#24471;&#26085;&#30410;&#26222;&#36941;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20173;&#28982;&#21463;&#21040;&#20004;&#20010;&#37325;&#22823;&#25361;&#25112;&#30340;&#21046;&#32422;&#65306;&#65288;1&#65289;&#39640;&#24230;&#19981;&#24179;&#34913;&#20107;&#20214;&#20998;&#24067;&#30340;&#38382;&#39064;&#65292;&#22312;&#21307;&#23398;&#25968;&#25454;&#38598;&#20013;&#36825;&#26159;&#19968;&#20010;&#24120;&#35265;&#20294;&#20851;&#38190;&#30340;&#38382;&#39064;&#65292;&#26576;&#20123;&#30456;&#20114;&#20316;&#29992;&#34987;&#24191;&#27867;&#22320;&#20302;&#20272;&#12290;&#36825;&#31181;&#19981;&#24179;&#34913;&#23545;&#23454;&#29616;&#20934;&#30830;&#21487;&#38752;&#30340;DDI&#39044;&#27979;&#26500;&#25104;&#20102;&#37325;&#22823;&#38556;&#30861;&#12290;&#65288;2&#65289;&#32597;&#35265;&#20107;&#20214;&#26631;&#35760;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#65292;&#22312;&#21307;&#23398;&#39046;&#22495;&#26159;&#19968;&#20010;&#26222;&#36941;&#38382;&#39064;&#65292;&#30001;&#20110;&#25968;&#25454;&#26377;&#38480;&#65292;&#24448;&#24448;&#24573;&#35270;&#25110;&#30740;&#31350;&#19981;&#36275;&#30340;&#32597;&#35265;&#20294;&#28508;&#22312;&#20851;&#38190;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DDIPrompt&#65292;&#36825;&#26159;&#19968;&#31181;&#21463;&#26368;&#36817;&#22270;&#25552;&#31034;&#23398;&#36827;&#23637;&#21551;&#21457;&#30340;&#21019;&#26032;&#33391;&#26041;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#26088;&#22312;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11472v1 Announce Type: cross  Abstract: Recently, Graph Neural Networks have become increasingly prevalent in predicting adverse drug-drug interactions (DDI) due to their proficiency in modeling the intricate associations between atoms and functional groups within and across drug molecules. However, they are still hindered by two significant challenges: (1) the issue of highly imbalanced event distribution, which is a common but critical problem in medical datasets where certain interactions are vastly underrepresented. This imbalance poses a substantial barrier to achieving accurate and reliable DDI predictions. (2) the scarcity of labeled data for rare events, which is a pervasive issue in the medical field where rare yet potentially critical interactions are often overlooked or under-studied due to limited available data. In response, we offer DDIPrompt, an innovative panacea inspired by the recent advancements in graph prompting. Our framework aims to address these issue
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35757;&#32451;&#25968;&#25454;&#19982;&#27169;&#22411;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#25552;&#20986;&#36890;&#36807;&#25552;&#21462;&#19981;&#21516;&#29305;&#24449;&#26469;&#39044;&#27979;Transformer&#25991;&#26412;&#27169;&#22411;&#30340;&#23545;&#25239;&#24615;&#31283;&#20581;&#24615;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.11469</link><description>&lt;p&gt;
&#22312;&#25628;&#32034;&#35757;&#32451;&#25968;&#25454;&#19982;Transformer&#25991;&#26412;&#27169;&#22411;&#23545;&#25239;&#24615;&#31283;&#20581;&#24615;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#26102;&#30340;&#19968;&#20010;&#26377;&#36259;&#26696;&#20363;
&lt;/p&gt;
&lt;p&gt;
A Curious Case of Searching for the Correlation between Training Data and Adversarial Robustness of Transformer Textual Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11469
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35757;&#32451;&#25968;&#25454;&#19982;&#27169;&#22411;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#25552;&#20986;&#36890;&#36807;&#25552;&#21462;&#19981;&#21516;&#29305;&#24449;&#26469;&#39044;&#27979;Transformer&#25991;&#26412;&#27169;&#22411;&#30340;&#23545;&#25239;&#24615;&#31283;&#20581;&#24615;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30740;&#31350;&#34920;&#26126;&#65292;&#32463;&#36807;&#24494;&#35843;&#30340;&#25991;&#26412;Transformer&#27169;&#22411;&#21487;&#20197;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#20294;&#20063;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#25991;&#26412;&#25200;&#21160;&#30340;&#24433;&#21709;&#12290;&#20256;&#32479;&#30340;&#23545;&#25239;&#24615;&#35780;&#20272;&#36890;&#24120;&#22312;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#20043;&#21518;&#25165;&#36827;&#34892;&#65292;&#24573;&#30053;&#20102;&#35757;&#32451;&#25968;&#25454;&#12290;&#26412;&#25991;&#26088;&#22312;&#35777;&#26126;&#35757;&#32451;&#25968;&#25454;&#21644;&#27169;&#22411;&#40065;&#26834;&#24615;&#20043;&#38388;&#20063;&#23384;&#22312;&#30528;&#24378;&#20851;&#32852;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#21462;&#20102;&#20195;&#34920;&#24191;&#27867;&#36755;&#20837;&#24494;&#35843;&#35821;&#26009;&#24211;&#23646;&#24615;&#30340;13&#31181;&#19981;&#21516;&#29305;&#24449;&#65292;&#24182;&#29992;&#23427;&#20204;&#26469;&#39044;&#27979;&#32463;&#36807;&#24494;&#35843;&#30340;&#27169;&#22411;&#30340;&#23545;&#25239;&#24615;&#31283;&#20581;&#24615;&#12290;&#25105;&#20204;&#20027;&#35201;&#20851;&#27880;&#20165;&#32534;&#30721;&#22120;&#30340;Transformer&#27169;&#22411;BERT&#21644;RoBERTa&#65292;&#24182;&#38468;&#21152;&#20102;BART&#12289;ELECTRA&#21644;GPT2&#30340;&#20854;&#20182;&#32467;&#26524;&#65292;&#20026;&#25105;&#20204;&#30340;&#35770;&#28857;&#25552;&#20379;&#22810;&#26679;&#30340;&#35777;&#25454;&#12290;&#39318;&#20808;&#65292;&#32463;&#39564;&#35777;&#26126;&#65292;(a)&#25552;&#21462;&#30340;&#29305;&#24449;&#21487;&#19982;&#36731;&#37327;&#32423;&#20998;&#31867;&#22120;&#65288;&#22914;&#38543;&#26426;&#26862;&#26519;&#65289;&#19968;&#36215;&#26377;&#25928;&#22320;&#39044;&#27979;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11469v1 Announce Type: cross  Abstract: Existing works have shown that fine-tuned textual transformer models achieve state-of-the-art prediction performances but are also vulnerable to adversarial text perturbations. Traditional adversarial evaluation is often done \textit{only after} fine-tuning the models and ignoring the training data. In this paper, we want to prove that there is also a strong correlation between training data and model robustness. To this end, we extract 13 different features representing a wide range of input fine-tuning corpora properties and use them to predict the adversarial robustness of the fine-tuned models. Focusing mostly on encoder-only transformer models BERT and RoBERTa with additional results for BART, ELECTRA and GPT2, we provide diverse evidence to support our argument. First, empirical analyses show that (a) extracted features can be used with a lightweight classifier such as Random Forest to effectively predict the attack success rate 
&lt;/p&gt;</description></item><item><title>Attraos&#27169;&#22411;&#22522;&#20110;&#28151;&#27788;&#29702;&#35770;&#65292;&#22312;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#21033;&#29992;&#22810;&#23610;&#24230;&#21160;&#24577;&#35760;&#24518;&#21333;&#20803;&#21644;&#23616;&#37096;&#28436;&#21270;&#31574;&#30053;&#65292;&#34920;&#29616;&#20248;&#24322;&#20110;&#20854;&#20182;LTSF&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.11463</link><description>&lt;p&gt;
&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#21560;&#24341;&#23376;&#35760;&#24518;&#65306;&#28151;&#27788;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Attractor Memory for Long-Term Time Series Forecasting: A Chaos Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11463
&lt;/p&gt;
&lt;p&gt;
Attraos&#27169;&#22411;&#22522;&#20110;&#28151;&#27788;&#29702;&#35770;&#65292;&#22312;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#21033;&#29992;&#22810;&#23610;&#24230;&#21160;&#24577;&#35760;&#24518;&#21333;&#20803;&#21644;&#23616;&#37096;&#28436;&#21270;&#31574;&#30053;&#65292;&#34920;&#29616;&#20248;&#24322;&#20110;&#20854;&#20182;LTSF&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65288;LTSF&#65289;&#20219;&#21153;&#20013;&#65292;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24573;&#35270;&#20102;&#31163;&#25955;&#26102;&#38388;&#24207;&#21015;&#28304;&#33258;&#28508;&#22312;&#36830;&#32493;&#21160;&#24577;&#31995;&#32479;&#30340;&#20851;&#38190;&#29305;&#24449;&#65292;&#23548;&#33268;&#32570;&#20047;&#22806;&#25512;&#21644;&#28436;&#21270;&#33021;&#21147;&#12290; &#37492;&#21035;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#28151;&#27788;&#24615;&#36136;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;\textbf{\textit{Attraos}}&#23558;&#28151;&#27788;&#29702;&#35770;&#34701;&#20837;&#21040;LTSF&#20013;&#65292;&#23558;&#23454;&#38469;&#26102;&#38388;&#24207;&#21015;&#35270;&#20026;&#26410;&#30693;&#39640;&#32500;&#28151;&#27788;&#21160;&#24577;&#31995;&#32479;&#30340;&#35266;&#27979;&#12290; &#22312;&#21560;&#24341;&#23376;&#19981;&#21464;&#24615;&#30340;&#27010;&#24565;&#19979;&#65292;Attraos&#21033;&#29992;&#25552;&#20986;&#30340;&#22810;&#23610;&#24230;&#21160;&#24577;&#35760;&#24518;&#21333;&#20803;&#26469;&#35760;&#24518;&#21382;&#21490;&#21160;&#24577;&#32467;&#26500;&#65292;&#24182;&#36890;&#36807;&#39057;&#29575;&#22686;&#24378;&#30340;&#23616;&#37096;&#28436;&#21270;&#31574;&#30053;&#36827;&#34892;&#39044;&#27979;&#12290; &#35814;&#32454;&#30340;&#29702;&#35770;&#20998;&#26512;&#21644;&#20016;&#23500;&#30340;&#32463;&#39564;&#35777;&#25454;&#19968;&#33268;&#34920;&#26126;&#65292;Attraos&#22312;&#20027;&#27969;LTSF&#25968;&#25454;&#38598;&#21644;&#28151;&#27788;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#21508;&#31181;LTSF&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11463v1 Announce Type: cross  Abstract: In long-term time series forecasting (LTSF) tasks, existing deep learning models overlook the crucial characteristic that discrete time series originate from underlying continuous dynamic systems, resulting in a lack of extrapolation and evolution capabilities. Recognizing the chaotic nature of real-world data, our model, \textbf{\textit{Attraos}}, incorporates chaos theory into LTSF, perceiving real-world time series as observations from unknown high-dimensional chaotic dynamic systems. Under the concept of attractor invariance, Attraos utilizes the proposed multi-scale dynamic memory unit to memorize historical dynamics structure and predicts by a frequency-enhanced local evolution strategy. Detailed theoretical analysis and abundant empirical evidence consistently show that Attraos outperforms various LTSF methods on mainstream LTSF datasets and chaotic datasets.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25193;&#25955;&#26725;&#29983;&#25104;&#27169;&#22411; Re-Dock&#65292;&#29992;&#20110;&#28789;&#27963;&#21644;&#29616;&#23454;&#30340;&#20998;&#23376;&#23545;&#25509;&#65292;&#36890;&#36807;&#33021;&#37327;&#21040;&#20960;&#20309;&#26144;&#23556;&#26469;&#20849;&#21516;&#24314;&#27169;&#32467;&#21512;&#33021;&#21644;&#26500;&#35937;&#65292;&#22635;&#34917;&#20102;&#23545;&#25509;&#20013;&#30340;&#23454;&#29992;&#24615;&#21644;&#26500;&#35937;&#39044;&#27979;&#26041;&#38754;&#30340;&#24046;&#36317;</title><link>https://arxiv.org/abs/2402.11459</link><description>&lt;p&gt;
Re-Dock: &#26397;&#21521;&#20855;&#26377;&#25193;&#25955;&#26725;&#30340;&#28789;&#27963;&#21644;&#29616;&#23454;&#20998;&#23376;&#23545;&#25509;
&lt;/p&gt;
&lt;p&gt;
Re-Dock: Towards Flexible and Realistic Molecular Docking with Diffusion Bridge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11459
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25193;&#25955;&#26725;&#29983;&#25104;&#27169;&#22411; Re-Dock&#65292;&#29992;&#20110;&#28789;&#27963;&#21644;&#29616;&#23454;&#30340;&#20998;&#23376;&#23545;&#25509;&#65292;&#36890;&#36807;&#33021;&#37327;&#21040;&#20960;&#20309;&#26144;&#23556;&#26469;&#20849;&#21516;&#24314;&#27169;&#32467;&#21512;&#33021;&#21644;&#26500;&#35937;&#65292;&#22635;&#34917;&#20102;&#23545;&#25509;&#20013;&#30340;&#23454;&#29992;&#24615;&#21644;&#26500;&#35937;&#39044;&#27979;&#26041;&#38754;&#30340;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#39044;&#27979;&#34507;&#30333;&#36136;-&#37197;&#20307;&#32467;&#21512;&#32467;&#26500;&#65292;&#21363;&#20998;&#23376;&#23545;&#25509;&#20219;&#21153;&#23545;&#20110;&#33647;&#29289;&#35774;&#35745;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#26174;&#31034;&#20986;&#20102;&#28508;&#21147;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#23436;&#25972;&#34507;&#30333;&#36136;&#32467;&#26500;&#65288;&#23545;&#25509;&#65292;&#19988;&#22312;&#29616;&#23454;&#20219;&#21153;&#20013;&#19981;&#21487;&#36798;&#65289;&#25110;&#24573;&#30053;&#21475;&#34955;&#20391;&#38142;&#26500;&#35937;&#65292;&#23548;&#33268;&#26377;&#38480;&#30340;&#23454;&#29992;&#24615;&#21644;&#19981;&#20999;&#23454;&#38469;&#30340;&#26500;&#35937;&#39044;&#27979;&#12290;&#20026;&#22635;&#34917;&#36825;&#20123;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26410;&#32463;&#25506;&#32034;&#30340;&#20219;&#21153;&#65292;&#21629;&#21517;&#20026;&#26580;&#24615;&#23545;&#25509;&#65292;&#20197;&#21516;&#26102;&#39044;&#27979;&#37197;&#20307;&#21644;&#21475;&#34955;&#20391;&#38142;&#30340;&#23039;&#21183;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#25193;&#23637;&#21040;&#20960;&#20309;&#27969;&#24418;&#30340;&#26032;&#22411;&#25193;&#25955;&#26725;&#29983;&#25104;&#27169;&#22411; Re-Dock&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21463;&#29275;&#39039;-&#27431;&#25289;&#26041;&#31243;&#21551;&#21457;&#30340;&#33021;&#37327;&#21040;&#20960;&#20309;&#26144;&#23556;&#65292;&#20197;&#20849;&#21516;&#24314;&#27169;&#32467;&#21512;&#33021;&#21644;&#26500;&#35937;&#65292;&#20197;&#21453;&#26144;&#33021;&#37327;&#32422;&#26463;&#23545;&#25509;&#29983;&#25104;&#36807;&#31243;&#12290;&#25105;&#20204;&#22312;&#35774;&#35745;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#21253;&#25324;apo-dock&#21644;cross-dock d
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11459v1 Announce Type: cross  Abstract: Accurate prediction of protein-ligand binding structures, a task known as molecular docking is crucial for drug design but remains challenging. While deep learning has shown promise, existing methods often depend on holo-protein structures (docked, and not accessible in realistic tasks) or neglect pocket sidechain conformations, leading to limited practical utility and unrealistic conformation predictions. To fill these gaps, we introduce an under-explored task, named flexible docking to predict poses of ligand and pocket sidechains simultaneously and introduce Re-Dock, a novel diffusion bridge generative model extended to geometric manifolds. Specifically, we propose energy-to-geometry mapping inspired by the Newton-Euler equation to co-model the binding energy and conformations for reflecting the energy-constrained docking generative process. Comprehensive experiments on designed benchmark datasets including apo-dock and cross-dock d
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;Infuser-Guided Knowledge Integration&#65288;InfuserKI&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;transformer&#20869;&#37096;&#29366;&#24577;&#26377;&#25928;&#22320;&#23558;&#26410;&#30693;&#30693;&#35782;&#38598;&#25104;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20174;&#32780;&#32531;&#35299;&#30693;&#35782;&#36951;&#24536;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.11441</link><description>&lt;p&gt;
InfuserKI&#65306;&#36890;&#36807;Infuser&#24341;&#23548;&#30340;&#30693;&#35782;&#38598;&#25104;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#30693;&#35782;&#22270;&#35889;
&lt;/p&gt;
&lt;p&gt;
InfuserKI: Enhancing Large Language Models with Knowledge Graphs via Infuser-Guided Knowledge Integration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11441
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;Infuser-Guided Knowledge Integration&#65288;InfuserKI&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;transformer&#20869;&#37096;&#29366;&#24577;&#26377;&#25928;&#22320;&#23558;&#26410;&#30693;&#30693;&#35782;&#38598;&#25104;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20174;&#32780;&#32531;&#35299;&#30693;&#35782;&#36951;&#24536;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24320;&#25918;&#24335;&#29983;&#25104;&#33021;&#21147;&#65292;&#20294;&#22312;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#30693;&#35782;&#38598;&#25104;&#26041;&#27861;&#65292;&#21033;&#29992;&#22806;&#37096;&#27169;&#22359;&#23558;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#22270;&#35889;&#19982;LLMs&#32467;&#21512;&#36215;&#26469;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23384;&#22312;&#25968;&#25454;&#25928;&#29575;&#20302;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#24050;&#30693;&#21644;&#26410;&#30693;&#30340;&#30693;&#35782;&#26469;&#36827;&#34892;&#24494;&#35843;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#38382;&#39064;&#65292;&#21363;&#22914;&#20309;&#22312;&#19981;&#37325;&#22797;&#24050;&#30693;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#22320;&#23558;&#26410;&#30693;&#30693;&#35782;&#38598;&#25104;&#21040;LLMs&#20013;&#12290;&#27880;&#20837;&#26032;&#30693;&#35782;&#20250;&#23548;&#33268;&#36951;&#24536;&#20808;&#21069;&#33719;&#24471;&#30340;&#30693;&#35782;&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Infuser-Guided Knowledge Integration&#65288;InfuserKI&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;transformer&#20869;&#37096;&#29366;&#24577;&#26469;&#30830;&#23450;&#26159;&#21542;&#24212;&#35813;&#22686;&#24378;&#21407;&#22987;LLM&#36755;&#20986;&#20449;&#24687;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#20943;&#36731;&#30693;&#35782;&#36951;&#24536;&#38382;&#39064;&#12290;&#22312;UMLS-2.5k&#21644;MetaQ&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11441v1 Announce Type: cross  Abstract: Though Large Language Models (LLMs) have shown remarkable open-generation capabilities across diverse domains, they struggle with knowledge-intensive tasks. To alleviate this issue, knowledge integration methods have been proposed to enhance LLMs with domain-specific knowledge graphs using external modules. However, they suffer from data inefficiency as they require both known and unknown knowledge for fine-tuning. Thus, we study a novel problem of integrating unknown knowledge into LLMs efficiently without unnecessary overlap of known knowledge. Injecting new knowledge poses the risk of forgetting previously acquired knowledge. To tackle this, we propose a novel Infuser-Guided Knowledge Integration (InfuserKI) framework that utilizes transformer internal states to determine whether to enhance the original LLM output with additional information, thereby effectively mitigating knowledge forgetting. Evaluations on the UMLS-2.5k and MetaQ
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#25913;&#21892;&#22522;&#20110;RSSI&#30340;&#23460;&#20869;&#23450;&#20301;&#65292;&#24341;&#20837;&#21152;&#26435;&#26368;&#23567;&#20108;&#20056;&#25216;&#26415;&#21644;&#20266;&#32447;&#24615;&#35299;&#20915;&#26041;&#26696;&#26041;&#27861;&#20197;&#35299;&#20915;&#38750;&#32447;&#24615;RSSI&#27979;&#37327;&#26041;&#31243;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.11433</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#25913;&#21892;&#29289;&#32852;&#32593;&#24212;&#29992;&#30340;&#23460;&#20869;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Improved Indoor Localization with Machine Learning Techniques for IoT applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11433
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#25913;&#21892;&#22522;&#20110;RSSI&#30340;&#23460;&#20869;&#23450;&#20301;&#65292;&#24341;&#20837;&#21152;&#26435;&#26368;&#23567;&#20108;&#20056;&#25216;&#26415;&#21644;&#20266;&#32447;&#24615;&#35299;&#20915;&#26041;&#26696;&#26041;&#27861;&#20197;&#35299;&#20915;&#38750;&#32447;&#24615;RSSI&#27979;&#37327;&#26041;&#31243;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20114;&#32852;&#32593;&#29289;&#32852;&#32593;&#21644;&#31227;&#21160;&#20114;&#32852;&#32593;&#24212;&#29992;&#30340;&#20852;&#36215;&#25512;&#21160;&#20102;&#21830;&#19994;&#12289;&#20891;&#20107;&#21644;&#31038;&#20250;&#24212;&#29992;&#20013;&#22522;&#20110;&#20301;&#32622;&#30340;&#26381;&#21153;&#30340;&#20852;&#36259;&#12290;&#23460;&#20869;&#23450;&#20301;&#31995;&#32479;&#21033;&#29992;&#35832;&#22914;Wi-Fi&#12289;ZigBee&#12289;&#34013;&#29273;&#12289;UWB&#31561;&#26080;&#32447;&#25216;&#26415;&#65292;&#26681;&#25454;&#19978;&#19979;&#25991;&#36827;&#34892;&#36873;&#25321;&#12290;&#25509;&#25910;&#20449;&#21495;&#24378;&#24230;&#25351;&#31034;&#22120;&#65288;RSSI&#65289;&#25216;&#26415;&#20197;&#20854;&#20934;&#30830;&#24615;&#21644;&#31616;&#21333;&#24615;&#32780;&#34987;&#24191;&#27867;&#37319;&#29992;&#12290;&#35813;&#30740;&#31350;&#22312;&#19977;&#20010;&#38454;&#27573;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65306;&#30417;&#30563;&#22238;&#24402;&#22120;&#12289;&#30417;&#30563;&#20998;&#31867;&#22120;&#21644;&#22522;&#20110;RSSI&#30340;&#23460;&#20869;&#23450;&#20301;&#30340;&#38598;&#25104;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#23427;&#24341;&#20837;&#20102;&#21152;&#26435;&#26368;&#23567;&#20108;&#20056;&#25216;&#26415;&#21644;&#20266;&#32447;&#24615;&#35299;&#20915;&#26041;&#26696;&#26041;&#27861;&#65292;&#36890;&#36807;&#29992;&#32447;&#24615;&#26041;&#31243;&#26469;&#36817;&#20284;&#38750;&#32447;&#24615;RSSI&#27979;&#37327;&#26041;&#31243;&#65292;&#26469;&#35299;&#20915;&#38750;&#32447;&#24615;RSSI&#27979;&#37327;&#26041;&#31243;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11433v1 Announce Type: new  Abstract: The rise of the Internet of Things (IoT) and mobile internet applications has spurred interest in location-based services (LBS) for commercial, military, and social applications. While the global positioning system (GPS) dominates outdoor localization, its efficacy wanes indoors due to signal challenges. Indoor localization systems leverage wireless technologies like Wi-Fi, ZigBee, Bluetooth, UWB, selecting based on context. Received signal strength indicator (RSSI) technology, known for its accuracy and simplicity, is widely adopted. This study employs machine learning algorithms in three phases: supervised regressors, supervised classifiers, and ensemble methods for RSSI-based indoor localization. Additionally, it introduces a weighted least squares technique and pseudo-linear solution approach to address non-linear RSSI measurement equations by approximating them with linear equations. An experimental testbed, utilizing diverse wirele
&lt;/p&gt;</description></item><item><title>OptEx&#26159;&#31532;&#19968;&#20010;&#36890;&#36807;&#21033;&#29992;&#24182;&#34892;&#35745;&#31639;&#26469;&#20943;&#36731;&#19968;&#38454;&#20248;&#21270;&#30340;&#36845;&#20195;&#29942;&#39048;&#24182;&#22686;&#24378;&#25928;&#29575;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#26680;&#21270;&#26799;&#24230;&#20272;&#35745;&#23454;&#29616;&#36845;&#20195;&#30340;&#24182;&#34892;&#21270;&#65292;&#25552;&#20379;&#29702;&#35770;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2402.11427</link><description>&lt;p&gt;
OptEx: &#21033;&#29992;&#36817;&#20284;&#24182;&#34892;&#21270;&#36845;&#20195;&#21152;&#36895;&#19968;&#38454;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
OptEx: Expediting First-Order Optimization with Approximately Parallelized Iterations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11427
&lt;/p&gt;
&lt;p&gt;
OptEx&#26159;&#31532;&#19968;&#20010;&#36890;&#36807;&#21033;&#29992;&#24182;&#34892;&#35745;&#31639;&#26469;&#20943;&#36731;&#19968;&#38454;&#20248;&#21270;&#30340;&#36845;&#20195;&#29942;&#39048;&#24182;&#22686;&#24378;&#25928;&#29575;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#26680;&#21270;&#26799;&#24230;&#20272;&#35745;&#23454;&#29616;&#36845;&#20195;&#30340;&#24182;&#34892;&#21270;&#65292;&#25552;&#20379;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31532;&#19968;&#38454;&#20248;&#21270;&#65288;FOO&#65289;&#31639;&#27861;&#22312;&#35832;&#22914;&#26426;&#22120;&#23398;&#20064;&#21644;&#20449;&#21495;&#21435;&#22122;&#31561;&#20247;&#22810;&#35745;&#31639;&#39046;&#22495;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#23558;&#23427;&#20204;&#24212;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#31561;&#22797;&#26434;&#20219;&#21153;&#24448;&#24448;&#23548;&#33268;&#26174;&#33879;&#30340;&#20302;&#25928;&#65292;&#22240;&#20026;&#38656;&#35201;&#35768;&#22810;&#39034;&#24207;&#36845;&#20195;&#20197;&#23454;&#29616;&#25910;&#25947;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31532;&#19968;&#38454;&#20248;&#21270;&#21152;&#36895;&#36817;&#20284;&#24182;&#34892;&#36845;&#20195;&#65288;OptEx&#65289;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#36890;&#36807;&#21033;&#29992;&#24182;&#34892;&#35745;&#31639;&#26469;&#20943;&#36731;&#20854;&#36845;&#20195;&#29942;&#39048;&#32780;&#22686;&#24378;FOO&#25928;&#29575;&#30340;&#26694;&#26550;&#12290;OptEx&#37319;&#29992;&#26680;&#21270;&#26799;&#24230;&#20272;&#35745;&#26469;&#21033;&#29992;&#26799;&#24230;&#21382;&#21490;&#36827;&#34892;&#26410;&#26469;&#26799;&#24230;&#39044;&#27979;&#65292;&#23454;&#29616;&#20102;&#36845;&#20195;&#30340;&#24182;&#34892;&#21270; -- &#36825;&#26159;&#19968;&#31181;&#26366;&#32463;&#34987;&#35748;&#20026;&#30001;&#20110;FOO&#20013;&#22266;&#26377;&#30340;&#36845;&#20195;&#20381;&#36182;&#32780;&#19981;&#20999;&#23454;&#38469;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#20026;&#25105;&#20204;&#30340;&#26680;&#21270;&#26799;&#24230;&#20272;&#35745;&#30340;&#21487;&#38752;&#24615;&#21644;&#22522;&#20110;SGD&#30340;OptEx&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#25552;&#20379;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#30830;&#35748;&#20102;&#20854;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11427v1 Announce Type: cross  Abstract: First-order optimization (FOO) algorithms are pivotal in numerous computational domains such as machine learning and signal denoising. However, their application to complex tasks like neural network training often entails significant inefficiencies due to the need for many sequential iterations for convergence. In response, we introduce first-order optimization expedited with approximately parallelized iterations (OptEx), the first framework that enhances the efficiency of FOO by leveraging parallel computing to mitigate its iterative bottleneck. OptEx employs kernelized gradient estimation to make use of gradient history for future gradient prediction, enabling parallelization of iterations -- a strategy once considered impractical because of the inherent iterative dependency in FOO. We provide theoretical guarantees for the reliability of our kernelized gradient estimation and the iteration complexity of SGD-based OptEx, confirming t
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#23616;&#37096;&#34394;&#21457;&#29616;&#29575;&#25511;&#21046;&#30340;&#36164;&#28304;&#20998;&#37197;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;$O(\sqrt{T})$&#30340;&#21518;&#24724;&#29575;&#65292;&#24182;&#25351;&#20986;&#36825;&#31181;&#21518;&#24724;&#29575;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#26159;&#19981;&#21487;&#25913;&#36827;&#30340;&#12290;</title><link>https://arxiv.org/abs/2402.11425</link><description>&lt;p&gt;
&#22312;&#32447;&#23616;&#37096;&#34394;&#21457;&#29616;&#29575;&#25511;&#21046;&#65306;&#19968;&#31181;&#36164;&#28304;&#20998;&#37197;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Online Local False Discovery Rate Control: A Resource Allocation Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11425
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#23616;&#37096;&#34394;&#21457;&#29616;&#29575;&#25511;&#21046;&#30340;&#36164;&#28304;&#20998;&#37197;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;$O(\sqrt{T})$&#30340;&#21518;&#24724;&#29575;&#65292;&#24182;&#25351;&#20986;&#36825;&#31181;&#21518;&#24724;&#29575;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#26159;&#19981;&#21487;&#25913;&#36827;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#32447;&#23616;&#37096;&#34394;&#21457;&#29616;&#29575;&#65288;FDR&#65289;&#25511;&#21046;&#38382;&#39064;&#65292;&#20854;&#20013;&#22810;&#20010;&#27979;&#35797;&#34987;&#39034;&#24207;&#36827;&#34892;&#65292;&#30446;&#26631;&#26159;&#26368;&#22823;&#21270;&#24635;&#26399;&#26395;&#30340;&#21457;&#29616;&#27425;&#25968;&#12290;&#25105;&#20204;&#23558;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#19968;&#31181;&#22312;&#32447;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;&#65292;&#28041;&#21450;&#25509;&#21463;/&#25298;&#32477;&#20915;&#31574;&#65292;&#20174;&#39640;&#23618;&#27425;&#26469;&#30475;&#65292;&#36825;&#21487;&#20197;&#34987;&#35270;&#20026;&#19968;&#20010;&#24102;&#26377;&#39069;&#22806;&#19981;&#30830;&#23450;&#24615;&#30340;&#22312;&#32447;&#32972;&#21253;&#38382;&#39064;&#65292;&#21363;&#38543;&#26426;&#39044;&#31639;&#34917;&#20805;&#12290;&#25105;&#20204;&#20174;&#19968;&#33324;&#30340;&#21040;&#36798;&#20998;&#24067;&#24320;&#22987;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;$O(\sqrt{T})$&#30340;&#21518;&#24724;&#12290;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#36825;&#31181;&#21518;&#24724;&#29575;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#26159;&#19981;&#21487;&#25913;&#36827;&#30340;&#26469;&#34917;&#20805;&#36825;&#19968;&#32467;&#26524;&#12290;&#28982;&#21518;&#25105;&#20204;&#23558;&#28966;&#28857;&#36716;&#21521;&#31163;&#25955;&#21040;&#36798;&#20998;&#24067;&#12290;&#25105;&#20204;&#21457;&#29616;&#35768;&#22810;&#29616;&#26377;&#30340;&#22312;&#32447;&#36164;&#28304;&#20998;&#37197;&#25991;&#29486;&#20013;&#30340;&#37325;&#26032;&#35299;&#20915;&#21551;&#21457;&#24335;&#34429;&#28982;&#22312;&#20856;&#22411;&#35774;&#32622;&#20013;&#23454;&#29616;&#20102;&#26377;&#30028;&#30340;&#25439;&#22833;&#65292;&#20294;&#21487;&#33021;&#20250;&#36896;&#25104;$\Omega(\sqrt{T})$&#29978;&#33267;$\Omega(T)$&#30340;&#21518;&#24724;&#12290;&#36890;&#36807;&#35266;&#23519;&#21040;&#20856;&#22411;&#31574;&#30053;&#24448;&#24448;&#22826;&#36807;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11425v1 Announce Type: cross  Abstract: We consider the problem of online local false discovery rate (FDR) control where multiple tests are conducted sequentially, with the goal of maximizing the total expected number of discoveries. We formulate the problem as an online resource allocation problem with accept/reject decisions, which from a high level can be viewed as an online knapsack problem, with the additional uncertainty of random budget replenishment. We start with general arrival distributions and propose a simple policy that achieves a $O(\sqrt{T})$ regret. We complement the result by showing that such regret rate is in general not improvable. We then shift our focus to discrete arrival distributions. We find that many existing re-solving heuristics in the online resource allocation literature, albeit achieve bounded loss in canonical settings, may incur a $\Omega(\sqrt{T})$ or even a $\Omega(T)$ regret. With the observation that canonical policies tend to be too op
&lt;/p&gt;</description></item><item><title>LoRETTA&#26159;&#19968;&#20010;&#36890;&#36807;&#24352;&#37327;&#35757;&#32451;&#20998;&#35299;&#26174;&#33879;&#20943;&#23569;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#36229;&#20302;&#21442;&#25968;&#39640;&#25928;&#26694;&#26550;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#20013;&#34920;&#29616;&#20986;&#19982;&#22823;&#22810;&#25968;PEFT&#26041;&#27861;&#30456;&#23218;&#32654;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.11417</link><description>&lt;p&gt;
LoRETTA: &#20302;&#31209;&#32463;&#27982;&#24352;&#37327;&#35757;&#32451;&#36866;&#24212;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36229;&#20302;&#21442;&#25968;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
LoRETTA: Low-Rank Economic Tensor-Train Adaptation for Ultra-Low-Parameter Fine-Tuning of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11417
&lt;/p&gt;
&lt;p&gt;
LoRETTA&#26159;&#19968;&#20010;&#36890;&#36807;&#24352;&#37327;&#35757;&#32451;&#20998;&#35299;&#26174;&#33879;&#20943;&#23569;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#36229;&#20302;&#21442;&#25968;&#39640;&#25928;&#26694;&#26550;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#20013;&#34920;&#29616;&#20986;&#19982;&#22823;&#22810;&#25968;PEFT&#26041;&#27861;&#30456;&#23218;&#32654;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21508;&#31181;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#25216;&#26415;&#34987;&#25552;&#20986;&#20197;&#23454;&#29616;&#22312;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#35745;&#31639;&#39640;&#25928;&#30340;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#37096;&#32626;&#65292;&#29616;&#26377;&#30340;PEFT&#26041;&#27861;&#20173;&#28982;&#21463;&#21040;&#21487;&#35757;&#32451;&#21442;&#25968;&#25968;&#37327;&#22686;&#38271;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LoRETTA&#65292;&#36825;&#26159;&#19968;&#20010;&#36229;&#21442;&#25968;&#39640;&#25928;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24352;&#37327;&#35757;&#32451;&#20998;&#35299;&#26174;&#33879;&#20943;&#23569;&#21487;&#35757;&#32451;&#21442;&#25968;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#65292;&#20998;&#21035;&#21629;&#21517;&#20026;{LoRETTA}$_{adp}$&#21644;{LoRETTA}$_{rep}$&#12290;&#21069;&#32773;&#37319;&#29992;&#24352;&#37327;&#21270;&#36866;&#37197;&#22120;&#65292;&#20026;LLMs&#30340;&#24494;&#35843;&#25552;&#20379;&#20102;&#39640;&#24615;&#33021;&#19988;&#36731;&#37327;&#32423;&#30340;&#26041;&#27861;&#12290;&#21518;&#32773;&#24378;&#35843;&#36890;&#36807;&#19968;&#32452;&#23567;&#24352;&#37327;&#22240;&#23376;&#36827;&#34892;&#26435;&#37325;&#21442;&#25968;&#21270;&#30340;&#24494;&#35843;&#12290;LoRETTA&#22312;LLaMA-2-7B&#27169;&#22411;&#19978;&#27604;&#22823;&#22810;&#25968;&#24191;&#27867;&#20351;&#29992;&#30340;PEFT&#26041;&#27861;&#20855;&#26377;&#21487;&#27604;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#21442;&#25968;&#23569;&#36798;&#21040;100&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11417v1 Announce Type: cross  Abstract: Various parameter-efficient fine-tuning (PEFT) techniques have been proposed to enable computationally efficient fine-tuning while maintaining model performance. However, existing PEFT methods are still limited by the growing number of trainable parameters with the rapid deployment of Large Language Models (LLMs). To address this challenge, we present LoRETTA, an ultra-parameter-efficient framework that significantly reduces trainable parameters through tensor-train decomposition. Specifically, we propose two methods, named {LoRETTA}$_{adp}$ and {LoRETTA}$_{rep}$. The former employs tensorized adapters, offering a high-performance yet lightweight approach for the fine-tuning of LLMs. The latter emphasizes fine-tuning via weight parameterization with a set of small tensor factors. LoRETTA achieves comparable or better performance than most widely used PEFT methods with up to $100\times$ fewer parameters on the LLaMA-2-7B models. Further
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#20809;&#35889;&#33258;&#21160;&#36716;&#31227;&#25216;&#26415;&#65288;MATT&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;RGB&#22270;&#20687;&#36716;&#32622;SAM&#20998;&#21106;&#25513;&#27169;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#20809;&#35889;&#22270;&#20687;&#30340;&#39640;&#31934;&#24230;&#21644;&#39640;&#25928;&#33258;&#21160;&#20998;&#21106;&#26631;&#35760;&#12290;</title><link>https://arxiv.org/abs/2402.11413</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#21033;&#29992;&#8220;Segment Anything Model (SAM)&#8221;&#36827;&#34892;&#33258;&#21160;&#22270;&#20687;&#20998;&#21106;&#30340;&#22810;&#20809;&#35889;&#33258;&#21160;&#36716;&#31227;&#25216;&#26415;&#65288;MATT&#65289;
&lt;/p&gt;
&lt;p&gt;
A Multispectral Automated Transfer Technique (MATT) for machine-driven image labeling utilizing the Segment Anything Model (SAM)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11413
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#20809;&#35889;&#33258;&#21160;&#36716;&#31227;&#25216;&#26415;&#65288;MATT&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;RGB&#22270;&#20687;&#36716;&#32622;SAM&#20998;&#21106;&#25513;&#27169;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#20809;&#35889;&#22270;&#20687;&#30340;&#39640;&#31934;&#24230;&#21644;&#39640;&#25928;&#33258;&#21160;&#20998;&#21106;&#26631;&#35760;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11413v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495;  &#25688;&#35201;: Segment Anything Model (SAM)&#22823;&#22823;&#21152;&#24555;&#20102;&#33258;&#21160;&#20998;&#21106;&#21644;&#26631;&#35760;&#22823;&#22411;&#32418;&#32511;&#34013;&#65288;RGB&#65289;&#22270;&#20687;&#25968;&#25454;&#38598;&#30340;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;SAM&#26080;&#27861;&#23545;&#21487;&#35265;&#20809;&#35889;&#20043;&#22806;&#30340;&#22270;&#20687;&#36827;&#34892;&#20998;&#21106;&#21644;&#26631;&#35760;&#65292;&#20363;&#22914;&#65292;&#23545;&#20110;&#22810;&#20809;&#35889;&#25110;&#39640;&#20809;&#35889;&#22270;&#20687;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#27010;&#36848;&#20102;&#19968;&#31181;&#25105;&#20204;&#31216;&#20043;&#20026;&#22810;&#20809;&#35889;&#33258;&#21160;&#36716;&#31227;&#25216;&#26415;&#65288;MATT&#65289;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20174;RGB&#22270;&#20687;&#20013;&#36716;&#32622;SAM&#20998;&#21106;&#25513;&#27169;&#65292;&#25105;&#20204;&#21487;&#20197;&#39640;&#31934;&#24230;&#21644;&#39640;&#25928;&#22320;&#33258;&#21160;&#20998;&#21106;&#21644;&#26631;&#35760;&#22810;&#20809;&#35889;&#22270;&#20687;&#12290;&#20363;&#22914;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#21033;&#29992;MATT&#23545;&#19968;&#20010;2,400&#24352;&#22270;&#20687;&#25968;&#25454;&#38598;&#36827;&#34892;&#20998;&#21106;&#21644;&#26631;&#35760;&#65292;&#22312;&#24320;&#21457;&#35757;&#32451;&#27169;&#22411;&#26041;&#38754;&#33410;&#30465;&#20102;87.8%&#30340;&#26102;&#38388;&#65292;&#23558;&#22823;&#32422;20&#23567;&#26102;&#30340;&#25163;&#21160;&#26631;&#27880;&#20943;&#23569;&#21040;&#20165;2.4&#23567;&#26102;&#12290;&#22312;&#36890;&#36807;MATT&#35757;&#32451;&#22810;&#20809;&#35889;&#27169;&#22411;&#26102;&#65292;&#36825;&#31181;&#25928;&#29575;&#25552;&#39640;&#20165;&#19982;&#25972;&#20307;&#24179;&#22343;&#31934;&#24230;&#65288;mAP&#65289;&#38477;&#20302;&#20102;6.7%&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11413v1 Announce Type: cross  Abstract: Segment Anything Model (SAM) is drastically accelerating the speed and accuracy of automatically segmenting and labeling large Red-Green-Blue (RGB) imagery datasets. However, SAM is unable to segment and label images outside of the visible light spectrum, for example, for multispectral or hyperspectral imagery. Therefore, this paper outlines a method we call the Multispectral Automated Transfer Technique (MATT). By transposing SAM segmentation masks from RGB images we can automatically segment and label multispectral imagery with high precision and efficiency. For example, the results demonstrate that segmenting and labeling a 2,400-image dataset utilizing MATT achieves a time reduction of 87.8% in developing a trained model, reducing roughly 20 hours of manual labeling, to only 2.4 hours. This efficiency gain is associated with only a 6.7% decrease in overall mean average precision (mAP) when training multispectral models via MATT, co
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#24187;&#35273;&#38382;&#39064;&#35270;&#20026;&#23545;&#40784;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20559;&#22909;&#35843;&#25972;&#35299;&#20915;&#65292;&#25552;&#20986;&#20102;POVID&#26041;&#27861;&#26469;&#29983;&#25104;&#21453;&#39304;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2402.11411</link><description>&lt;p&gt;
&#36890;&#36807;&#20559;&#22909;&#24494;&#35843;&#22312;&#35270;&#35273;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#23545;&#40784;&#27169;&#24577;
&lt;/p&gt;
&lt;p&gt;
Aligning Modalities in Vision Large Language Models via Preference Fine-tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11411
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#24187;&#35273;&#38382;&#39064;&#35270;&#20026;&#23545;&#40784;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20559;&#22909;&#35843;&#25972;&#35299;&#20915;&#65292;&#25552;&#20986;&#20102;POVID&#26041;&#27861;&#26469;&#29983;&#25104;&#21453;&#39304;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#31034;&#36319;&#38543;&#30340;&#35270;&#35273;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;VLLMs&#65289;&#26368;&#36817;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#36825;&#20123;&#26041;&#27861;&#21512;&#24182;&#20102;&#24378;&#22823;&#30340;&#39044;&#35757;&#32451;&#35270;&#35273;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;&#30001;&#20110;&#36825;&#20123;&#32452;&#20214;&#26159;&#20998;&#21035;&#35757;&#32451;&#30340;&#65292;&#25152;&#20197;&#38656;&#35201;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#39069;&#22806;&#30340;&#22270;&#20687;-&#35821;&#35328;&#23545;&#26469;&#23545;&#23398;&#20064;&#30340;&#34920;&#31034;&#36827;&#34892;&#23545;&#40784;&#12290;&#36825;&#20010;&#36807;&#31243;&#24182;&#19981;&#23436;&#32654;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#27169;&#22411;&#20135;&#29983;&#24187;&#35273;-&#21363;&#20351;&#26680;&#24515;LLM&#38750;&#24120;&#23458;&#35266;&#65292;&#35270;&#35273;&#25903;&#25745;&#20855;&#26377;&#20805;&#20998;&#23436;&#25972;&#30340;&#34920;&#31034;&#65292;&#20063;&#20250;&#25552;&#20379;&#19982;&#22270;&#20687;&#19981;&#31526;&#21512;&#30340;&#31572;&#26696;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#24187;&#35273;&#38382;&#39064;&#23450;&#20041;&#20026;&#19968;&#20010;&#23545;&#40784;&#38382;&#39064;&#65292;&#36890;&#36807;&#20559;&#22909;&#35843;&#25972;&#26469;&#35299;&#20915;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;POVID&#26469;&#20135;&#29983;AI&#27169;&#22411;&#30340;&#21453;&#39304;&#25968;&#25454;&#12290;&#25105;&#20204;&#20351;&#29992;&#22320;&#38754;&#30495;&#23454;&#25351;&#31034;&#20316;&#20026;&#39318;&#36873;&#21709;&#24212;&#65292;&#37319;&#29992;&#20004;&#38454;&#27573;&#26041;&#27861;&#29983;&#25104;&#19981;&#21463;&#27426;&#36814;&#30340;&#25968;&#25454;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#31034;GPT-4V&#27880;&#20837;&#21512;&#29702;&#30340;&#24187;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11411v1 Announce Type: cross  Abstract: Instruction-following Vision Large Language Models (VLLMs) have achieved significant progress recently on a variety of tasks. These approaches merge strong pre-trained vision models and large language models (LLMs). Since these components are trained separately, the learned representations need to be aligned with joint training on additional image-language pairs. This procedure is not perfect and can cause the model to hallucinate - provide answers that do not accurately reflect the image, even when the core LLM is highly factual and the vision backbone has sufficiently complete representations. In this work, we frame the hallucination problem as an alignment issue, tackle it with preference tuning. Specifically, we propose POVID to generate feedback data with AI models. We use ground-truth instructions as the preferred response and a two-stage approach to generate dispreferred data. First, we prompt GPT-4V to inject plausible hallucin
&lt;/p&gt;</description></item><item><title>&#32473;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#12289;&#39640;&#25928;&#12289;&#30830;&#23450;&#24615;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#30340;&#26657;&#20934;&#36317;&#31163;&#35823;&#24046;&#26368;&#22810;&#20026;$2\sqrt{T}$</title><link>https://arxiv.org/abs/2402.11410</link><description>&lt;p&gt;
&#33719;&#24471;$2\sqrt{T}$&#21040;&#26657;&#20934;&#30340;&#22522;&#26412;&#39044;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
An Elementary Predictor Obtaining $2\sqrt{T}$ Distance to Calibration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11410
&lt;/p&gt;
&lt;p&gt;
&#32473;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#12289;&#39640;&#25928;&#12289;&#30830;&#23450;&#24615;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#30340;&#26657;&#20934;&#36317;&#31163;&#35823;&#24046;&#26368;&#22810;&#20026;$2\sqrt{T}$
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Blasiok&#31561;&#20154;[2023]&#25552;&#20986;&#20102;&#26657;&#20934;&#36317;&#31163;&#20316;&#20026;&#19968;&#31181;&#33258;&#28982;&#30340;&#26657;&#20934;&#35823;&#24046;&#24230;&#37327;&#65292;&#19982;&#39044;&#26399;&#30340;&#26657;&#20934;&#35823;&#24046;(ECE)&#19981;&#21516;&#65292;&#23427;&#26159;&#36830;&#32493;&#30340;&#12290;&#26368;&#36817;&#65292;Qiao&#21644;Zheng [2024]&#32473;&#20986;&#20102;&#19968;&#20010;&#38750;&#26500;&#36896;&#24615;&#30340;&#35770;&#35777;&#65292;&#24314;&#31435;&#20102;&#19968;&#31181;&#22312;&#32447;&#39044;&#27979;&#22120;&#30340;&#23384;&#22312;&#65292;&#35813;&#39044;&#27979;&#22120;&#21487;&#20197;&#22312;&#23545;&#25239;&#35774;&#32622;&#20013;&#33719;&#24471;$O(\sqrt{T})$&#30340;&#26657;&#20934;&#36317;&#31163;&#65292;&#32780;&#23545;&#20110;ECE&#26469;&#35828;&#26159;&#19981;&#21487;&#33021;&#30340;&#12290;&#20182;&#20204;&#23558;&#25214;&#21040;&#19968;&#31181;&#26126;&#30830;&#30340;&#12289;&#39640;&#25928;&#30340;&#31639;&#27861;&#20316;&#20026;&#19968;&#20010;&#38656;&#35201;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#32473;&#20986;&#20102;&#19968;&#20010;&#38750;&#24120;&#31616;&#21333;&#12289;&#39640;&#25928;&#12289;&#30830;&#23450;&#24615;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#30340;&#26657;&#20934;&#36317;&#31163;&#35823;&#24046;&#26368;&#22810;&#20026;$2\sqrt{T}$&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11410v1 Announce Type: new  Abstract: Blasiok et al. [2023] proposed distance to calibration as a natural measure of calibration error that unlike expected calibration error (ECE) is continuous. Recently, Qiao and Zheng [2024] gave a non-constructive argument establishing the existence of an online predictor that can obtain $O(\sqrt{T})$ distance to calibration in the adversarial setting, which is known to be impossible for ECE. They leave as an open problem finding an explicit, efficient algorithm. We resolve this problem and give an extremely simple, efficient, deterministic algorithm that obtains distance to calibration error at most $2\sqrt{T}$.
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#28508;&#22312;&#29305;&#24449;&#31354;&#38388;&#31283;&#23450;&#24615;&#30340;&#26032;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#21487;&#20197;&#30830;&#20445;&#19968;&#33268;&#24615;&#21644;&#21487;&#38752;&#24615;&#30340;&#31283;&#23450;&#24615;&#35780;&#20272;&#24037;&#20316;&#27969;&#31243;&#65292;&#21253;&#25324;&#20102;&#19977;&#31181;&#31283;&#23450;&#24615;&#31867;&#22411;&#21644;&#19968;&#22871;&#20840;&#38754;&#35780;&#20272;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;</title><link>https://arxiv.org/abs/2402.11404</link><description>&lt;p&gt;
&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#28508;&#22312;&#29305;&#24449;&#31354;&#38388;&#30340;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Stability of Deep Learning Latent Feature Spaces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11404
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#28508;&#22312;&#29305;&#24449;&#31354;&#38388;&#31283;&#23450;&#24615;&#30340;&#26032;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#21487;&#20197;&#30830;&#20445;&#19968;&#33268;&#24615;&#21644;&#21487;&#38752;&#24615;&#30340;&#31283;&#23450;&#24615;&#35780;&#20272;&#24037;&#20316;&#27969;&#31243;&#65292;&#21253;&#25324;&#20102;&#19977;&#31181;&#31283;&#23450;&#24615;&#31867;&#22411;&#21644;&#19968;&#22871;&#20840;&#38754;&#35780;&#20272;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#32500;&#25968;&#25454;&#38598;&#22312;&#21508;&#20010;&#23398;&#31185;&#30340;&#32479;&#35745;&#24314;&#27169;&#20013;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#38656;&#35201;&#26377;&#25928;&#30340;&#38477;&#32500;&#26041;&#27861;&#12290;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#20197;&#20174;&#22797;&#26434;&#25968;&#25454;&#20013;&#25552;&#28860;&#20851;&#38190;&#29305;&#24449;&#30340;&#33021;&#21147;&#32780;&#33879;&#31216;&#65292;&#36890;&#36807;&#38477;&#32500;&#30340;&#28508;&#22312;&#29305;&#24449;&#31354;&#38388;&#23454;&#29616;&#24314;&#27169;&#12289;&#21487;&#35270;&#21270;&#21644;&#21387;&#32553;&#65292;&#22312;&#29983;&#29289;&#20449;&#24687;&#23398;&#21040;&#22320;&#29699;&#31185;&#23398;&#31561;&#39046;&#22495;&#26377;&#24191;&#27867;&#24212;&#29992;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#24037;&#20316;&#27969;&#31243;&#65292;&#29992;&#20110;&#35780;&#20272;&#36825;&#20123;&#28508;&#22312;&#31354;&#38388;&#30340;&#31283;&#23450;&#24615;&#65292;&#30830;&#20445;&#21518;&#32493;&#20998;&#26512;&#30340;&#19968;&#33268;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;&#31283;&#23450;&#24615;&#34987;&#23450;&#20041;&#20026;&#28508;&#22312;&#31354;&#38388;&#23545;&#20110;&#24494;&#23567;&#25968;&#25454;&#12289;&#35757;&#32451;&#23454;&#29616;&#21644;&#21442;&#25968;&#25200;&#21160;&#30340;&#19981;&#21464;&#24615;&#65292;&#26159;&#33267;&#20851;&#37325;&#35201;&#21364;&#32463;&#24120;&#34987;&#24573;&#35270;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#35770;&#30028;&#23450;&#20102;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#19977;&#31181;&#31283;&#23450;&#24615;&#31867;&#22411;&#65292;&#26679;&#26412;&#31283;&#23450;&#24615;&#12289;&#32467;&#26500;&#31283;&#23450;&#24615;&#21644;&#25512;&#26029;&#31283;&#23450;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#22871;&#29992;&#20110;&#20840;&#38754;&#35780;&#20272;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;&#36825;&#20010;&#24037;&#20316;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11404v1 Announce Type: new  Abstract: High-dimensional datasets present substantial challenges in statistical modeling across various disciplines, necessitating effective dimensionality reduction methods. Deep learning approaches, notable for their capacity to distill essential features from complex data, facilitate modeling, visualization, and compression through reduced dimensionality latent feature spaces, have wide applications from bioinformatics to earth sciences. This study introduces a novel workflow to evaluate the stability of these latent spaces, ensuring consistency and reliability in subsequent analyses. Stability, defined as the invariance of latent spaces to minor data, training realizations, and parameter perturbations, is crucial yet often overlooked.   Our proposed methodology delineates three stability types, sample, structural, and inferential, within latent spaces, and introduces a suite of metrics for comprehensive evaluation. We implement this workflow
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#25991;&#26723;&#22270;&#20687;&#20013;&#35782;&#21035;&#21644;&#23450;&#20301;&#25991;&#26723;&#23545;&#35937;&#65292;&#20197;&#20943;&#23569;&#22823;&#22411;&#27169;&#22411;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#30340;&#37096;&#32626;&#25104;&#26412;</title><link>https://arxiv.org/abs/2402.11401</link><description>&lt;p&gt;
GraphKD&#65306;&#25506;&#32034;&#30693;&#35782;&#33976;&#39311;&#22312;&#25991;&#26723;&#30446;&#26631;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#36890;&#36807;&#32467;&#26500;&#21270;&#22270;&#21019;&#24314;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
GraphKD: Exploring Knowledge Distillation Towards Document Object Detection with Structured Graph Creation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11401
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#25991;&#26723;&#22270;&#20687;&#20013;&#35782;&#21035;&#21644;&#23450;&#20301;&#25991;&#26723;&#23545;&#35937;&#65292;&#20197;&#20943;&#23569;&#22823;&#22411;&#27169;&#22411;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#30340;&#37096;&#32626;&#25104;&#26412;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25991;&#26723;&#20013;&#36827;&#34892;&#30446;&#26631;&#26816;&#27979;&#26159;&#33258;&#21160;&#21270;&#35782;&#21035;&#25968;&#23383;&#25110;&#25195;&#25551;&#25991;&#26723;&#20013;&#30340;&#32467;&#26500;&#20803;&#32032;&#30340;&#20851;&#38190;&#27493;&#39588;&#65292;&#36890;&#36807;&#29702;&#35299;&#19981;&#21516;&#20803;&#32032;&#20043;&#38388;&#30340;&#20998;&#23618;&#32467;&#26500;&#21644;&#20851;&#31995;&#12290;&#22823;&#22411;&#21644;&#22797;&#26434;&#30340;&#27169;&#22411;&#34429;&#28982;&#21487;&#20197;&#23454;&#29616;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#22312;&#35745;&#31639;&#19978;&#26114;&#36149;&#19988;&#21344;&#29992;&#20869;&#23384;&#65292;&#20351;&#20854;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#37096;&#32626;&#21464;&#24471;&#19981;&#20999;&#23454;&#38469;&#12290;&#30693;&#35782;&#33976;&#39311;&#20801;&#35768;&#25105;&#20204;&#21019;&#24314;&#23567;&#22411;&#19988;&#26356;&#39640;&#25928;&#30340;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#20445;&#30041;&#20102;&#22823;&#22411;&#27169;&#22411;&#30340;&#22823;&#37096;&#20998;&#24615;&#33021;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65292;&#20197;&#27491;&#30830;&#35782;&#21035;&#24182;&#23450;&#20301;&#25991;&#26723;&#22270;&#20687;&#20013;&#30340;&#25991;&#26723;&#23545;&#35937;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20855;&#26377;&#24314;&#35758;&#32423;&#29305;&#24449;&#30340;&#32467;&#26500;&#21270;&#22270;&#65292;&#36793;&#34920;&#31034;&#19981;&#21516;&#24314;&#35758;&#21306;&#22495;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20943;&#23569;&#25991;&#26412;&#20559;&#35265;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#33410;&#28857;&#25277;&#26679;&#31574;&#30053;&#26469;&#20462;&#21098;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11401v1 Announce Type: cross  Abstract: Object detection in documents is a key step to automate the structural elements identification process in a digital or scanned document through understanding the hierarchical structure and relationships between different elements. Large and complex models, while achieving high accuracy, can be computationally expensive and memory-intensive, making them impractical for deployment on resource constrained devices. Knowledge distillation allows us to create small and more efficient models that retain much of the performance of their larger counterparts. Here we present a graph-based knowledge distillation framework to correctly identify and localize the document objects in a document image. Here, we design a structured graph with nodes containing proposal-level features and edges representing the relationship between the different proposal regions. Also, to reduce text bias an adaptive node sampling strategy is designed to prune the weight
&lt;/p&gt;</description></item><item><title>k-SemStamp&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35821;&#20041;&#27700;&#21360;&#26041;&#26696;&#65292;&#36890;&#36807;&#21033;&#29992;k&#22343;&#20540;&#32858;&#31867;&#20195;&#26367;LSH&#26469;&#25552;&#39640;&#40065;&#26834;&#24615;&#21644;&#25277;&#26679;&#25928;&#29575;&#65292;&#21516;&#26102;&#20445;&#25345;&#29983;&#25104;&#36136;&#37327;&#65292;&#20026;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#25552;&#20379;&#20102;&#26356;&#26377;&#25928;&#30340;&#24037;&#20855;&#12290;</title><link>https://arxiv.org/abs/2402.11399</link><description>&lt;p&gt;
k-SemStamp&#65306;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#30340;&#35821;&#20041;&#27700;&#21360;&#29992;&#20110;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
k-SemStamp: A Clustering-Based Semantic Watermark for Detection of Machine-Generated Text
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11399
&lt;/p&gt;
&lt;p&gt;
k-SemStamp&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35821;&#20041;&#27700;&#21360;&#26041;&#26696;&#65292;&#36890;&#36807;&#21033;&#29992;k&#22343;&#20540;&#32858;&#31867;&#20195;&#26367;LSH&#26469;&#25552;&#39640;&#40065;&#26834;&#24615;&#21644;&#25277;&#26679;&#25928;&#29575;&#65292;&#21516;&#26102;&#20445;&#25345;&#29983;&#25104;&#36136;&#37327;&#65292;&#20026;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#25552;&#20379;&#20102;&#26356;&#26377;&#25928;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#27700;&#21360;&#29983;&#25104;&#31639;&#27861;&#22312;&#35821;&#35328;&#29983;&#25104;&#36807;&#31243;&#20013;&#27880;&#20837;&#21487;&#26816;&#27979;&#30340;&#31614;&#21517;&#65292;&#20197;&#20415;&#36827;&#34892;&#20107;&#21518;&#26816;&#27979;&#12290;&#34429;&#28982;&#22522;&#20110;&#26631;&#35760;&#32423;&#21035;&#30340;&#27700;&#21360;&#23481;&#26131;&#21463;&#21040;&#25913;&#20889;&#25915;&#20987;&#65292;&#20294;SemStamp (Hou&#31561;&#20154;&#65292;2023)&#22312;&#21477;&#23376;&#30340;&#35821;&#20041;&#34920;&#31034;&#19978;&#24212;&#29992;&#27700;&#21360;&#65292;&#24182;&#23637;&#31034;&#20986;&#24456;&#22909;&#30340;&#40065;&#26834;&#24615;&#12290;SemStamp&#21033;&#29992;&#23616;&#37096;&#25935;&#24863;&#21704;&#24076;&#65288;LSH&#65289;&#26469;&#21033;&#29992;&#20219;&#24847;&#36229;&#24179;&#38754;&#23545;&#35821;&#20041;&#31354;&#38388;&#36827;&#34892;&#20998;&#21306;&#65292;&#23548;&#33268;&#22312;&#40065;&#26834;&#24615;&#21644;&#36895;&#24230;&#20043;&#38388;&#23384;&#22312;&#27425;&#20248;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#25552;&#20986;k-SemStamp&#65292;&#36825;&#26159;SemStamp&#30340;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22686;&#24378;&#29256;&#65292;&#21033;&#29992;k&#22343;&#20540;&#32858;&#31867;&#20316;&#20026;&#23616;&#37096;&#25935;&#24863;&#21704;&#24076;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#20197;&#20102;&#35299;&#20869;&#22312;&#30340;&#35821;&#20041;&#32467;&#26500;&#26469;&#20998;&#21306;&#23884;&#20837;&#31354;&#38388;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;k-SemStamp&#26174;&#33879;&#25552;&#39640;&#20102;&#20854;&#40065;&#26834;&#24615;&#21644;&#25277;&#26679;&#25928;&#29575;&#65292;&#21516;&#26102;&#20445;&#25345;&#29983;&#25104;&#36136;&#37327;&#65292;&#25512;&#36827;&#20102;&#26356;&#26377;&#25928;&#30340;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11399v1 Announce Type: new  Abstract: Recent watermarked generation algorithms inject detectable signatures during language generation to facilitate post-hoc detection. While token-level watermarks are vulnerable to paraphrase attacks, SemStamp (Hou et al., 2023) applies watermark on the semantic representation of sentences and demonstrates promising robustness. SemStamp employs locality-sensitive hashing (LSH) to partition the semantic space with arbitrary hyperplanes, which results in a suboptimal tradeoff between robustness and speed. We propose k-SemStamp, a simple yet effective enhancement of SemStamp, utilizing k-means clustering as an alternative of LSH to partition the embedding space with awareness of inherent semantic structure. Experimental results indicate that k-SemStamp saliently improves its robustness and sampling efficiency while preserving the generation quality, advancing a more effective tool for machine-generated text detection.
&lt;/p&gt;</description></item><item><title>RPNNs&#20855;&#26377;&#22266;&#23450;&#20869;&#37096;&#26435;&#37325;&#21644;&#20559;&#32622;&#65292;&#36890;&#36807;&#36873;&#25321;&#22806;&#37096;&#26435;&#37325;&#65292;&#23427;&#20204;&#23637;&#29616;&#20986;&#25351;&#25968;&#25910;&#25947;&#36895;&#29575;&#26469;&#36924;&#36817;&#20219;&#24847;&#26080;&#31351;&#21487;&#24494;&#20989;&#25968;&#65292;&#22312;&#20989;&#25968;&#36924;&#36817;&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#39640;&#25928;&#20934;&#30830;&#30340;&#28508;&#21147;</title><link>https://arxiv.org/abs/2402.11397</link><description>&lt;p&gt;
&#26368;&#20339;&#36924;&#36817;&#30340;&#38543;&#26426;&#25237;&#24433;&#31070;&#32463;&#32593;&#32476;&#65306;&#25910;&#25947;&#29702;&#35770;&#21644;&#23454;&#38469;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Random Projection Neural Networks of Best Approximation: Convergence theory and practical applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11397
&lt;/p&gt;
&lt;p&gt;
RPNNs&#20855;&#26377;&#22266;&#23450;&#20869;&#37096;&#26435;&#37325;&#21644;&#20559;&#32622;&#65292;&#36890;&#36807;&#36873;&#25321;&#22806;&#37096;&#26435;&#37325;&#65292;&#23427;&#20204;&#23637;&#29616;&#20986;&#25351;&#25968;&#25910;&#25947;&#36895;&#29575;&#26469;&#36924;&#36817;&#20219;&#24847;&#26080;&#31351;&#21487;&#24494;&#20989;&#25968;&#65292;&#22312;&#20989;&#25968;&#36924;&#36817;&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#39640;&#25928;&#20934;&#30830;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#65288;FNN&#65289;&#30340;&#26368;&#20339;&#36924;&#36817;&#27010;&#24565;&#65292;&#24182;&#36890;&#36807;&#38543;&#26426;&#25237;&#24433;&#31070;&#32463;&#32593;&#32476;&#65288;RPNNs&#65289;&#30340;&#35270;&#35282;&#25506;&#35752;&#23427;&#20204;&#30340;&#25910;&#25947;&#24615;&#36136;&#12290;RPNNs&#20855;&#26377;&#39044;&#20808;&#30830;&#23450;&#19988;&#22266;&#23450;&#30340;&#20869;&#37096;&#26435;&#37325;&#21644;&#20559;&#32622;&#65292;&#25552;&#20379;&#20102;&#35745;&#31639;&#25928;&#29575;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#20219;&#20309;&#19968;&#26063;&#20855;&#26377;&#38750;&#22810;&#39033;&#24335;&#26080;&#31351;&#21487;&#24494;&#28608;&#27963;&#20989;&#25968;&#30340;RPNNs&#65292;&#23384;&#22312;&#22806;&#37096;&#26435;&#37325;&#30340;&#36873;&#25321;&#65292;&#23637;&#29616;&#20986;&#25351;&#25968;&#25910;&#25947;&#36895;&#29575;&#26469;&#36924;&#36817;&#20219;&#24847;&#26080;&#31351;&#21487;&#24494;&#20989;&#25968;&#12290;&#20026;&#20102;&#35828;&#26126;&#65292;&#25105;&#20204;&#22312;&#20116;&#20010;&#22522;&#20934;&#20989;&#25968;&#36924;&#36817;&#38382;&#39064;&#19978;&#27979;&#35797;&#20102;&#22522;&#20110;RPNN&#30340;&#20989;&#25968;&#36924;&#36817;&#65292;&#36873;&#25321;&#20102;&#31616;&#27905;&#30340;&#22522;&#20989;&#25968;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;RPNNs&#36798;&#21040;&#20102;&#19982;&#21202;&#35753;&#24503;&#22810;&#39033;&#24335;&#31561;&#24050;&#24314;&#31435;&#26041;&#27861;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#65292;&#31361;&#26174;&#20102;&#23427;&#20204;&#22312;&#39640;&#25928;&#20934;&#30830;&#20989;&#25968;&#36924;&#36817;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11397v1 Announce Type: new  Abstract: We investigate the concept of Best Approximation for Feedforward Neural Networks (FNN) and explore their convergence properties through the lens of Random Projection (RPNNs). RPNNs have predetermined and fixed, once and for all, internal weights and biases, offering computational efficiency. We demonstrate that there exists a choice of external weights, for any family of such RPNNs, with non-polynomial infinitely differentiable activation functions, that exhibit an exponential convergence rate when approximating any infinitely differentiable function. For illustration purposes, we test the proposed RPNN-based function approximation, with parsimoniously chosen basis functions, across five benchmark function approximation problems. Results show that RPNNs achieve comparable performance to established methods such as Legendre Polynomials, highlighting their potential for efficient and accurate function approximation.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#25511;&#21046;&#39118;&#21147;&#21457;&#30005;&#20197;&#26368;&#22823;&#21270;&#33021;&#37327;&#20135;&#29983;&#30340;&#26041;&#27861;&#65292;&#34920;&#29616;&#20986;&#22312;&#21508;&#31181;&#29615;&#22659;&#19979;&#27604;&#20256;&#32479;PID&#25511;&#21046;&#26356;&#20248;&#24322;&#65292;&#24182;&#22312;&#30495;&#23454;&#39118;&#26465;&#20214;&#19979;&#36827;&#34892;&#20102;&#23545;&#27604;&#39564;&#35777;&#12290;</title><link>https://arxiv.org/abs/2402.11384</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#26368;&#22823;&#21270;&#39118;&#21147;&#21457;&#30005;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning to maximise wind turbine energy generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11384
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#25511;&#21046;&#39118;&#21147;&#21457;&#30005;&#20197;&#26368;&#22823;&#21270;&#33021;&#37327;&#20135;&#29983;&#30340;&#26041;&#27861;&#65292;&#34920;&#29616;&#20986;&#22312;&#21508;&#31181;&#29615;&#22659;&#19979;&#27604;&#20256;&#32479;PID&#25511;&#21046;&#26356;&#20248;&#24322;&#65292;&#24182;&#22312;&#30495;&#23454;&#39118;&#26465;&#20214;&#19979;&#36827;&#34892;&#20102;&#23545;&#27604;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#65292;&#36890;&#36807;&#20027;&#21160;&#35843;&#25972;&#36716;&#23376;&#36895;&#24230;&#12289;&#36716;&#23376;&#20559;&#33322;&#35282;&#21644;&#21494;&#29255;&#20542;&#35282;&#26469;&#25511;&#21046;&#39118;&#21147;&#21457;&#30005;&#12290;&#25105;&#20204;&#23558;&#21452;&#28145;&#24230;Q&#23398;&#20064;&#19982;&#20248;&#20808;&#32463;&#39564;&#37325;&#25773;&#20195;&#29702;&#19982;&#21494;&#29255;&#20803;&#32032;&#21160;&#37327;&#27169;&#22411;&#32806;&#21512;&#65292;&#24182;&#35757;&#32451;&#35813;&#20195;&#29702;&#20197;&#20801;&#35768;&#25511;&#21046;&#19981;&#26029;&#21464;&#21270;&#30340;&#39118;&#12290;&#35813;&#20195;&#29702;&#32463;&#36807;&#35757;&#32451;&#65292;&#21487;&#20197;&#22312;&#31616;&#21333;&#31283;&#23450;&#39118;&#20013;&#20915;&#23450;&#26368;&#20339;&#25511;&#21046;&#65288;&#36895;&#24230;&#12289;&#20559;&#33322;&#12289;&#20542;&#35282;&#65289;&#65292;&#38543;&#21518;&#25361;&#25112;&#30495;&#23454;&#30340;&#21160;&#24577;&#32010;&#27969;&#39118;&#65292;&#34920;&#29616;&#33391;&#22909;&#12290;&#21452;&#28145;&#24230;Q&#23398;&#20064;&#19982;&#32463;&#20856;&#20540;&#36845;&#20195;&#24378;&#21270;&#23398;&#20064;&#25511;&#21046;&#36827;&#34892;&#27604;&#36739;&#65292;&#22312;&#25152;&#26377;&#29615;&#22659;&#20013;&#37117;&#20248;&#20110;&#32463;&#20856;PID&#25511;&#21046;&#12290;&#27492;&#22806;&#65292;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#38750;&#24120;&#36866;&#21512;&#21464;&#21270;&#30340;&#29615;&#22659;&#65292;&#21253;&#25324;&#32010;&#20081;/&#38453;&#39118;&#39118;&#65292;&#34920;&#29616;&#20986;&#24456;&#24378;&#30340;&#36866;&#24212;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#25152;&#26377;&#25511;&#21046;&#31574;&#30053;&#19982;&#30495;&#23454;&#39118;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#35745;&#31639;&#24180;&#24230;&#33021;&#28304;&#20135;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11384v1 Announce Type: new  Abstract: We propose a reinforcement learning strategy to control wind turbine energy generation by actively changing the rotor speed, the rotor yaw angle and the blade pitch angle. A double deep Q-learning with a prioritized experience replay agent is coupled with a blade element momentum model and is trained to allow control for changing winds. The agent is trained to decide the best control (speed, yaw, pitch) for simple steady winds and is subsequently challenged with real dynamic turbulent winds, showing good performance. The double deep Q- learning is compared with a classic value iteration reinforcement learning control and both strategies outperform a classic PID control in all environments. Furthermore, the reinforcement learning approach is well suited to changing environments including turbulent/gusty winds, showing great adaptability. Finally, we compare all control strategies with real winds and compute the annual energy production. I
&lt;/p&gt;</description></item><item><title>&#23558;&#22870;&#21169;&#20998;&#35299;&#20026;&#20219;&#21153;&#29305;&#23450;&#22870;&#21169;&#21644;&#24120;&#35782;&#22870;&#21169;&#65292;&#25506;&#32034;&#22914;&#20309;&#20174;&#19987;&#23478;&#28436;&#31034;&#20013;&#23398;&#20064;&#24120;&#35782;&#22870;&#21169;&#65307;&#30740;&#31350;&#21457;&#29616;&#65292;&#36870;&#24378;&#21270;&#23398;&#20064;&#25104;&#21151;&#35757;&#32451;&#20195;&#29702;&#21518;&#65292;&#24182;&#19981;&#20250;&#23398;&#21040;&#26377;&#29992;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;</title><link>https://arxiv.org/abs/2402.11367</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#36870;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#24120;&#35782;&#22870;&#21169;
&lt;/p&gt;
&lt;p&gt;
Multi Task Inverse Reinforcement Learning for Common Sense Reward
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11367
&lt;/p&gt;
&lt;p&gt;
&#23558;&#22870;&#21169;&#20998;&#35299;&#20026;&#20219;&#21153;&#29305;&#23450;&#22870;&#21169;&#21644;&#24120;&#35782;&#22870;&#21169;&#65292;&#25506;&#32034;&#22914;&#20309;&#20174;&#19987;&#23478;&#28436;&#31034;&#20013;&#23398;&#20064;&#24120;&#35782;&#22870;&#21169;&#65307;&#30740;&#31350;&#21457;&#29616;&#65292;&#36870;&#24378;&#21270;&#23398;&#20064;&#25104;&#21151;&#35757;&#32451;&#20195;&#29702;&#21518;&#65292;&#24182;&#19981;&#20250;&#23398;&#21040;&#26377;&#29992;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23558;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#20110;&#22797;&#26434;&#30340;&#29616;&#23454;&#29615;&#22659;&#20013;&#65292;&#19968;&#20010;&#25361;&#25112;&#22312;&#20110;&#20026;agent&#25552;&#20379;&#36275;&#22815;&#35814;&#32454;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#22870;&#21169;&#19982;&#26399;&#26395;&#34892;&#20026;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#21487;&#33021;&#23548;&#33268;&#24847;&#22806;&#32467;&#26524;&#65292;&#22914;&#8220;&#22870;&#21169;&#31713;&#25913;&#8221;&#65292;agent&#36890;&#36807;&#24847;&#22806;&#34892;&#20026;&#26368;&#22823;&#21270;&#22870;&#21169;&#12290;&#26412;&#25991;&#25552;&#20986;&#23558;&#22870;&#21169;&#20998;&#35299;&#20026;&#20004;&#20010;&#26126;&#30830;&#37096;&#20998;&#65306;&#19968;&#20010;&#31616;&#21333;&#30340;&#20219;&#21153;&#29305;&#23450;&#22870;&#21169;&#65292;&#27010;&#36848;&#20102;&#24403;&#21069;&#20219;&#21153;&#30340;&#32454;&#33410;&#65307;&#20197;&#21450;&#19968;&#20010;&#26410;&#30693;&#30340;&#24120;&#35782;&#22870;&#21169;&#65292;&#25351;&#31034;agent&#22312;&#29615;&#22659;&#20013;&#30340;&#39044;&#26399;&#34892;&#20026;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#22914;&#20309;&#20174;&#19987;&#23478;&#28436;&#31034;&#20013;&#23398;&#20064;&#36825;&#31181;&#24120;&#35782;&#22870;&#21169;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#65292;&#21363;&#20351;&#36870;&#24378;&#21270;&#23398;&#20064;&#25104;&#21151;&#35757;&#32451;&#20102;&#19968;&#20010;&#20195;&#29702;&#65292;&#20063;&#19981;&#20250;&#23398;&#21040;&#19968;&#20010;&#26377;&#29992;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#20351;&#29992;&#23398;&#21040;&#30340;&#22870;&#21169;&#35757;&#32451;&#26032;&#20195;&#29702;&#19981;&#20250;&#24433;&#21709;&#26399;&#26395;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11367v1 Announce Type: new  Abstract: One of the challenges in applying reinforcement learning in a complex real-world environment lies in providing the agent with a sufficiently detailed reward function. Any misalignment between the reward and the desired behavior can result in unwanted outcomes. This may lead to issues like "reward hacking" where the agent maximizes rewards by unintended behavior. In this work, we propose to disentangle the reward into two distinct parts. A simple task-specific reward, outlining the particulars of the task at hand, and an unknown common-sense reward, indicating the expected behavior of the agent within the environment. We then explore how this common-sense reward can be learned from expert demonstrations. We first show that inverse reinforcement learning, even when it succeeds in training an agent, does not learn a useful reward function. That is, training a new agent with the learned reward does not impair the desired behaviors. We then d
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#30340;&#25968;&#25454;&#39537;&#21160;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#38543;&#26426;&#20132;&#27969;&#65288;AC&#65289;&#27010;&#29575;&#32422;&#26463;&#65288;CC&#65289;&#26368;&#20248;&#28526;&#27969;&#65288;OPF&#65289;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#22810;&#20010;IEEE&#27979;&#35797;&#26696;&#20363;&#23637;&#31034;&#20102;&#20854;&#23454;&#35777;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.11365</link><description>&lt;p&gt;
&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#30340;&#25968;&#25454;&#39537;&#21160;&#38543;&#26426;&#20132;&#27969;&#20248;&#21270;&#28526;&#27969;
&lt;/p&gt;
&lt;p&gt;
Data-Driven Stochastic AC-OPF using Gaussian Processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11365
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#30340;&#25968;&#25454;&#39537;&#21160;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#38543;&#26426;&#20132;&#27969;&#65288;AC&#65289;&#27010;&#29575;&#32422;&#26463;&#65288;CC&#65289;&#26368;&#20248;&#28526;&#27969;&#65288;OPF&#65289;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#22810;&#20010;IEEE&#27979;&#35797;&#26696;&#20363;&#23637;&#31034;&#20102;&#20854;&#23454;&#35777;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#32858;&#28966;&#20110;&#21457;&#23637;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#25968;&#25454;&#39537;&#21160;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#38543;&#26426;&#20132;&#27969;&#65288;AC&#65289;&#27010;&#29575;&#32422;&#26463;&#65288;CC&#65289;&#26368;&#20248;&#28526;&#27969;&#65288;OPF&#65289;&#38382;&#39064;&#12290;&#34429;&#28982;AC CC-OPF&#38382;&#39064;&#22312;&#23398;&#26415;&#30028;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#30001;&#20110;&#39640;&#24230;&#38750;&#32447;&#24615;&#21644;&#35745;&#31639;&#35201;&#27714;&#24456;&#39640;&#65292;&#38480;&#21046;&#20102;&#20854;&#23454;&#38469;&#24433;&#21709;&#12290;&#35813;&#26041;&#27861;&#26088;&#22312;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#24182;&#36890;&#36807;&#24212;&#29992;&#20110;&#22810;&#20010;IEEE&#27979;&#35797;&#26696;&#20363;&#26469;&#23637;&#31034;&#20854;&#23454;&#35777;&#25928;&#29575;&#12290;&#20026;&#20102;&#35299;&#20915;&#38750;&#20984;&#21644;&#35745;&#31639;&#22797;&#26434;&#30340;CC AC-OPF&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#20381;&#36182;&#20110;&#26426;&#22120;&#23398;&#20064;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#65288;GPR&#65289;&#27169;&#22411;&#12290;&#23436;&#25972;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#26041;&#27861;&#33021;&#22815;&#23398;&#20064;&#19968;&#20010;&#31616;&#21333;&#20294;&#38750;&#20984;&#30340;&#25968;&#25454;&#39537;&#21160;&#36817;&#20284;&#33267;AC&#28526;&#27969;&#26041;&#31243;&#65292;&#33021;&#22815;&#32435;&#20837;&#19981;&#30830;&#23450;&#36755;&#20837;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#21508;&#31181;&#36817;&#20284;&#26469;&#20256;&#25773;GP&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11365v1 Announce Type: new  Abstract: The thesis focuses on developing a data-driven algorithm, based on machine learning, to solve the stochastic alternating current (AC) chance-constrained (CC) Optimal Power Flow (OPF) problem. Although the AC CC-OPF problem has been successful in academic circles, it is highly nonlinear and computationally demanding, which limits its practical impact. The proposed approach aims to address this limitation and demonstrate its empirical efficiency through applications to multiple IEEE test cases. To solve the non-convex and computationally challenging CC AC-OPF problem, the proposed approach relies on a machine learning Gaussian process regression (GPR) model. The full Gaussian process (GP) approach is capable of learning a simple yet non-convex data-driven approximation to the AC power flow equations that can incorporate uncertain inputs. The proposed approach uses various approximations for GP-uncertainty propagation. The full GP CC-OPF ap
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22914;&#20309;&#23450;&#20041;&#20869;&#23384;&#39640;&#25928;&#30340;T-&#33539;&#25968;&#25439;&#22833;&#65292;&#20801;&#35768;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#21033;&#29992;T-&#33539;&#25968;&#36827;&#34892;&#20107;&#20214;&#26816;&#27979;&#20219;&#21153;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#22312;GPU&#19978;&#36816;&#34892;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.11362</link><description>&lt;p&gt;
&#21033;&#29992;T-&#33539;&#25968;&#36827;&#34892;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Exploiting T-norms for Deep Learning in Autonomous Driving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11362
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22914;&#20309;&#23450;&#20041;&#20869;&#23384;&#39640;&#25928;&#30340;T-&#33539;&#25968;&#25439;&#22833;&#65292;&#20801;&#35768;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#21033;&#29992;T-&#33539;&#25968;&#36827;&#34892;&#20107;&#20214;&#26816;&#27979;&#20219;&#21153;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#22312;GPU&#19978;&#36816;&#34892;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#19968;&#30452;&#26159;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#21457;&#23637;&#30340;&#26680;&#24515;&#65292;&#30001;&#20110;&#31070;&#32463;&#32593;&#32476;&#22312;&#21457;&#29616;&#21407;&#22987;&#25968;&#25454;&#20013;&#30340;&#27169;&#24335;&#24182;&#23558;&#20854;&#36716;&#21270;&#20026;&#20934;&#30830;&#39044;&#27979;&#26041;&#38754;&#21462;&#24471;&#30340;&#25104;&#21151;&#12290;&#27492;&#22806;&#65292;&#26368;&#36817;&#30340;&#31070;&#32463;&#31526;&#21495;&#21270;&#24037;&#20316;&#34920;&#26126;&#65292;&#36890;&#36807;T-&#33539;&#25968;&#22312;&#25439;&#22833;&#20989;&#25968;&#20013;&#21512;&#24182;&#20851;&#20110;&#25163;&#22836;&#38382;&#39064;&#30340;&#21487;&#29992;&#32972;&#26223;&#30693;&#35782;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;T-&#33539;&#25968;&#30340;&#25439;&#22833;&#21487;&#33021;&#20855;&#26377;&#38750;&#24120;&#39640;&#30340;&#20869;&#23384;&#38656;&#27714;&#65292;&#22240;&#27492;&#21487;&#33021;&#26080;&#27861;&#24212;&#29992;&#20110;&#20687;&#33258;&#21160;&#39550;&#39542;&#36825;&#26679;&#30340;&#22797;&#26434;&#24212;&#29992;&#39046;&#22495;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23450;&#20041;&#20869;&#23384;&#39640;&#25928;&#30340;T-&#33539;&#25968;&#25439;&#22833;&#65292;&#20174;&#32780;&#20801;&#35768;&#21033;&#29992;T-&#33539;&#25968;&#26469;&#36827;&#34892;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#20107;&#20214;&#26816;&#27979;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;ROAD-R&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#20998;&#26512;&#65292;&#24182;&#23637;&#31034;&#20102;&#65288;i&#65289;&#25105;&#20204;&#30340;&#25552;&#35758;&#21487;&#20197;&#22312;&#20855;&#26377;&#23567;&#20110;25 GiB&#21487;&#29992;&#20869;&#23384;&#30340;GPU&#19978;&#23454;&#29616;&#21644;&#36816;&#34892;&#65292;&#32780;&#26631;&#20934;&#30340;t-
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11362v1 Announce Type: new  Abstract: Deep learning has been at the core of the autonomous driving field development, due to the neural networks' success in finding patterns in raw data and turning them into accurate predictions. Moreover, recent neuro-symbolic works have shown that incorporating the available background knowledge about the problem at hand in the loss function via t-norms can further improve the deep learning models' performance. However, t-norm-based losses may have very high memory requirements and, thus, they may be impossible to apply in complex application domains like autonomous driving. In this paper, we show how it is possible to define memory-efficient t-norm-based losses, allowing for exploiting t-norms for the task of event detection in autonomous driving. We conduct an extensive experimental analysis on the ROAD-R dataset and show (i) that our proposal can be implemented and run on GPUs with less than 25 GiB of available memory, while standard t-
&lt;/p&gt;</description></item><item><title>&#23558;&#34920;&#24449;&#31354;&#38388;&#30340;&#21453;&#20107;&#23454;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#65292;&#20197;&#20998;&#26512;&#21644;&#35299;&#37322;&#27169;&#22411;&#24178;&#39044;&#25152;&#24341;&#36215;&#30340;&#35821;&#35328;&#21464;&#21270;&#65292;&#24182;&#20943;&#36731;&#20998;&#31867;&#20013;&#30340;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2402.11355</link><description>&lt;p&gt;
&#25913;&#21464;&#20102;&#20160;&#20040;&#65311;&#23558;&#34920;&#24449;&#24178;&#39044;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
What Changed? Converting Representational Interventions to Natural Language
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11355
&lt;/p&gt;
&lt;p&gt;
&#23558;&#34920;&#24449;&#31354;&#38388;&#30340;&#21453;&#20107;&#23454;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#65292;&#20197;&#20998;&#26512;&#21644;&#35299;&#37322;&#27169;&#22411;&#24178;&#39044;&#25152;&#24341;&#36215;&#30340;&#35821;&#35328;&#21464;&#21270;&#65292;&#24182;&#20943;&#36731;&#20998;&#31867;&#20013;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#34920;&#24449;&#31354;&#38388;&#30340;&#24178;&#39044;&#26041;&#27861;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#24433;&#21709;&#27169;&#22411;&#34892;&#20026;&#30340;&#26377;&#25928;&#25163;&#27573;&#12290;&#36825;&#20123;&#26041;&#27861;&#34987;&#29992;&#26469;&#28040;&#38500;&#25110;&#25913;&#21464;&#27169;&#22411;&#34920;&#31034;&#20013;&#30340;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#65288;&#22914;&#24615;&#21035;&#65289;&#30340;&#32534;&#30721;&#65292;&#21019;&#24314;&#19968;&#20010;&#21453;&#20107;&#23454;&#30340;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24178;&#39044;&#25805;&#20316;&#22312;&#34920;&#31034;&#31354;&#38388;&#20869;&#65292;&#20934;&#30830;&#29702;&#35299;&#23427;&#20462;&#25913;&#20102;&#21738;&#20123;&#29305;&#24449;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#34920;&#24449;&#31354;&#38388;&#30340;&#21453;&#20107;&#23454;&#21487;&#20197;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#30340;&#21453;&#20107;&#23454;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#20998;&#26512;&#23545;&#24212;&#20110;&#32473;&#23450;&#34920;&#31034;&#31354;&#38388;&#24178;&#39044;&#30340;&#35821;&#35328;&#21464;&#21270;&#65292;&#24182;&#35299;&#37322;&#29992;&#20110;&#32534;&#30721;&#29305;&#23450;&#27010;&#24565;&#30340;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#30001;&#27492;&#20135;&#29983;&#30340;&#21453;&#20107;&#23454;&#21487;&#20197;&#29992;&#20110;&#20943;&#36731;&#20998;&#31867;&#20013;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11355v1 Announce Type: new  Abstract: Interventions targeting the representation space of language models (LMs) have emerged as effective means to influence model behavior. These methods are employed, for example, to eliminate or alter the encoding of demographic information such as gender within the model's representations, creating a counterfactual representation. However, since the intervention operates within the representation space, understanding precisely which features it modifies poses a challenge. We show that representation-space counterfactuals can be converted into natural language counterfactuals. We demonstrate that this approach enables us to analyze the linguistic alterations corresponding to a given representation-space intervention and to interpret the features utilized for encoding a specific concept. Moreover, the resulting counterfactuals can be used to mitigate bias in classification.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#36335;&#30001;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;PEOs&#26377;&#25928;&#35782;&#21035;&#22270;&#20013;&#38656;&#35201;&#32771;&#34385;&#36827;&#34892;&#31934;&#30830;&#36317;&#31163;&#35745;&#31639;&#30340;&#37051;&#23621;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#22522;&#20110;&#22270;&#30340;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;&#30340;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.11354</link><description>&lt;p&gt;
&#22522;&#20110;&#27010;&#29575;&#36335;&#30001;&#30340;&#22522;&#20110;&#22270;&#30340;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Routing for Graph-Based Approximate Nearest Neighbor Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11354
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#36335;&#30001;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;PEOs&#26377;&#25928;&#35782;&#21035;&#22270;&#20013;&#38656;&#35201;&#32771;&#34385;&#36827;&#34892;&#31934;&#30830;&#36317;&#31163;&#35745;&#31639;&#30340;&#37051;&#23621;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#22522;&#20110;&#22270;&#30340;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv&#65306;2402.11354v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449; &#25688;&#35201;&#65306;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#39640;&#32500;&#31354;&#38388;&#20013;&#30340;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;(ANNS)&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#24050;&#32463;&#25104;&#20026;ANNS&#30340;&#20248;&#36234;&#26041;&#27861;&#65292;&#24314;&#31435;&#20102;&#19968;&#31181;&#26032;&#30340;&#25216;&#26415;&#27700;&#24179;&#12290;&#23613;&#31649;&#24341;&#20837;&#20102;&#21508;&#31181;&#22522;&#20110;&#22270;&#30340;ANNS&#20248;&#21270;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#20027;&#35201;&#20381;&#36182;&#20110;&#32570;&#20047;&#27491;&#24335;&#29702;&#35770;&#25903;&#25345;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26041;&#27861;&#26469;&#22686;&#24378;&#22522;&#20110;&#22270;&#30340;ANNS&#20013;&#30340;&#36335;&#30001;&#65292;&#35813;&#26041;&#27861;&#22312;&#25506;&#32034;&#22270;&#20013;&#33410;&#28857;&#30340;&#37051;&#23621;&#26102;&#25552;&#20379;&#27010;&#29575;&#20445;&#35777;&#12290;&#25105;&#20204;&#23558;&#38382;&#39064;&#24314;&#27169;&#20026;&#27010;&#29575;&#36335;&#30001;&#65292;&#24182;&#36890;&#36807;&#32467;&#21512;&#23616;&#37096;&#25935;&#24863;&#25216;&#26415;&#24320;&#21457;&#20102;&#20004;&#31181;&#22522;&#20934;&#31574;&#30053;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;PEOs&#65292;&#36825;&#26159;&#19968;&#31181;&#26377;&#25928;&#35782;&#21035;&#22270;&#20013;&#24212;&#32771;&#34385;&#36827;&#34892;&#31934;&#30830;&#36317;&#31163;&#35745;&#31639;&#30340;&#37051;&#23621;&#30340;&#26032;&#26041;&#27861;&#65292;&#20174;&#32780;&#22312;&#23454;&#36341;&#20013;&#26174;&#33879;&#25552;&#39640;&#20102;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11354v1 Announce Type: cross  Abstract: Approximate nearest neighbor search (ANNS) in high-dimensional spaces is a pivotal challenge in the field of machine learning. In recent years, graph-based methods have emerged as the superior approach to ANNS, establishing a new state of the art. Although various optimizations for graph-based ANNS have been introduced, they predominantly rely on heuristic methods that lack formal theoretical backing. This paper aims to enhance routing within graph-based ANNS by introducing a method that offers a probabilistic guarantee when exploring a node's neighbors in the graph. We formulate the problem as probabilistic routing and develop two baseline strategies by incorporating locality-sensitive techniques. Subsequently, we introduce PEOs, a novel approach that efficiently identifies which neighbors in the graph should be considered for exact distance computation, thus significantly improving efficiency in practice. Our experiments demonstrate 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#21464;&#20998;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#23558;&#26399;&#26395;&#25913;&#36827;&#65288;EI&#65289;&#35270;&#20026;&#26368;&#22823;&#20540;&#29109;&#25628;&#32034;&#65288;MES&#65289;&#30340;&#29305;&#27530;&#24773;&#20917;&#65292;&#25552;&#20986;&#20102;&#21464;&#20998;&#29109;&#25628;&#32034;&#65288;VES&#65289;&#26041;&#27861;&#21644; VES-Gamma &#31639;&#27861;&#65292;&#25104;&#21151;&#35843;&#25972; EI &#24182;&#23637;&#31034;&#20854;&#22312;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#38754;&#30340;&#23454;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.11345</link><description>&lt;p&gt;
&#21464;&#20998;&#29109;&#25628;&#32034;&#29992;&#20110;&#35843;&#25972;&#26399;&#26395;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Variational Entropy Search for Adjusting Expected Improvement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11345
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#21464;&#20998;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#23558;&#26399;&#26395;&#25913;&#36827;&#65288;EI&#65289;&#35270;&#20026;&#26368;&#22823;&#20540;&#29109;&#25628;&#32034;&#65288;MES&#65289;&#30340;&#29305;&#27530;&#24773;&#20917;&#65292;&#25552;&#20986;&#20102;&#21464;&#20998;&#29109;&#25628;&#32034;&#65288;VES&#65289;&#26041;&#27861;&#21644; VES-Gamma &#31639;&#27861;&#65292;&#25104;&#21151;&#35843;&#25972; EI &#24182;&#23637;&#31034;&#20854;&#22312;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#38754;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Bayesian optimization &#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#20248;&#21270;&#40657;&#30418;&#20989;&#25968;&#30340;&#25216;&#26415;&#65292;&#26399;&#26395;&#25913;&#36827;&#65288;EI&#65289;&#26159;&#35813;&#39046;&#22495;&#20013;&#26368;&#24120;&#29992;&#30340;&#33719;&#21462;&#20989;&#25968;&#12290;&#34429;&#28982; EI &#36890;&#24120;&#34987;&#35270;&#20026;&#19982;&#20854;&#20182;&#20449;&#24687;&#29702;&#35770;&#33719;&#21462;&#20989;&#25968;&#65288;&#22914;&#29109;&#25628;&#32034;&#65288;ES&#65289;&#21644;&#26368;&#22823;&#20540;&#29109;&#25628;&#32034;&#65288;MES&#65289;&#65289;&#19981;&#21516;&#65292;&#20294;&#25105;&#20204;&#30340;&#24037;&#20316;&#25581;&#31034;&#20102;&#65292;&#36890;&#36807;&#21464;&#20998;&#25512;&#26029;&#65288;VI&#65289;&#26041;&#27861;&#65292;EI &#21487;&#20197;&#34987;&#35270;&#20026; MES &#30340;&#19968;&#31181;&#29305;&#27530;&#24773;&#20917;&#12290;&#22312;&#36825;&#19968;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#21464;&#20998;&#29109;&#25628;&#32034;&#65288;VES&#65289;&#26041;&#27861;&#21644; VES-Gamma &#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#20449;&#24687;&#29702;&#35770;&#27010;&#24565;&#30340;&#21407;&#21017;&#25972;&#21512;&#21040; EI &#20013;&#26469;&#35843;&#25972; EI&#12290;VES-Gamma &#30340;&#26377;&#25928;&#24615;&#22312;&#21508;&#31181;&#27979;&#35797;&#20989;&#25968;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#20013;&#24471;&#21040;&#20102;&#35777;&#26126;&#65292;&#31361;&#20986;&#20102;&#23427;&#22312;&#36125;&#21494;&#26031;&#20248;&#21270;&#22330;&#26223;&#20013;&#30340;&#29702;&#35770;&#21644;&#23454;&#38469;&#29992;&#36884;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11345v1 Announce Type: cross  Abstract: Bayesian optimization is a widely used technique for optimizing black-box functions, with Expected Improvement (EI) being the most commonly utilized acquisition function in this domain. While EI is often viewed as distinct from other information-theoretic acquisition functions, such as entropy search (ES) and max-value entropy search (MES), our work reveals that EI can be considered a special case of MES when approached through variational inference (VI). In this context, we have developed the Variational Entropy Search (VES) methodology and the VES-Gamma algorithm, which adapts EI by incorporating principles from information-theoretic concepts. The efficacy of VES-Gamma is demonstrated across a variety of test functions and read datasets, highlighting its theoretical and practical utilities in Bayesian optimization scenarios.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#19968;&#31181;&#20351;&#29992;&#22534;&#21472;&#33258;&#32534;&#30721;&#22120;&#36827;&#34892;&#29305;&#24449;&#36873;&#25321;&#30340;&#20808;&#36827;&#21202;&#32034;&#36719;&#20214;&#26816;&#27979;&#21644;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#22534;&#21472;&#33258;&#32534;&#30721;&#22120;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#20998;&#31867;&#22120;&#65292;&#23454;&#29616;&#20102;&#25552;&#39640;&#21202;&#32034;&#36719;&#20214;&#20998;&#23618;&#20934;&#30830;&#24615;&#30340;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2402.11342</link><description>&lt;p&gt;
&#20351;&#29992;&#22534;&#21472;&#33258;&#32534;&#30721;&#22120;&#36827;&#34892;&#29305;&#24449;&#36873;&#25321;&#30340;&#21202;&#32034;&#36719;&#20214;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Ransomware detection using stacked autoencoder for feature selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11342
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#19968;&#31181;&#20351;&#29992;&#22534;&#21472;&#33258;&#32534;&#30721;&#22120;&#36827;&#34892;&#29305;&#24449;&#36873;&#25321;&#30340;&#20808;&#36827;&#21202;&#32034;&#36719;&#20214;&#26816;&#27979;&#21644;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#22534;&#21472;&#33258;&#32534;&#30721;&#22120;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#20998;&#31867;&#22120;&#65292;&#23454;&#29616;&#20102;&#25552;&#39640;&#21202;&#32034;&#36719;&#20214;&#20998;&#23618;&#20934;&#30830;&#24615;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25552;&#20986;&#24182;&#35780;&#20272;&#19968;&#31181;&#20808;&#36827;&#30340;&#21202;&#32034;&#36719;&#20214;&#26816;&#27979;&#21644;&#20998;&#31867;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#22534;&#21472;&#33258;&#32534;&#30721;&#22120;&#65288;SAE&#65289;&#19982;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#20998;&#31867;&#22120;&#32467;&#21512;&#65292;&#29992;&#20110;&#31934;&#30830;&#29305;&#24449;&#36873;&#25321;&#65292;&#20197;&#22686;&#24378;&#21202;&#32034;&#36719;&#20214;&#20998;&#23618;&#20934;&#30830;&#24615;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#23545; UGRansome &#25968;&#25454;&#38598;&#36827;&#34892;&#24443;&#24213;&#30340;&#39044;&#22788;&#29702;&#65292;&#35757;&#32451;&#19968;&#20010;&#26080;&#30417;&#30563;&#30340; SAE &#36827;&#34892;&#26368;&#20339;&#29305;&#24449;&#36873;&#25321;&#65292;&#25110;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#36827;&#34892;&#24494;&#35843;&#65292;&#25552;&#39640; LSTM &#27169;&#22411;&#30340;&#20998;&#31867;&#33021;&#21147;&#12290;&#35813;&#30740;&#31350;&#35814;&#32454;&#20998;&#26512;&#20102;&#33258;&#32534;&#30721;&#22120;&#23398;&#20064;&#21040;&#30340;&#26435;&#37325;&#21644;&#28608;&#27963;&#65292;&#20197;&#35782;&#21035;&#29992;&#20110;&#23558;&#21202;&#32034;&#36719;&#20214;&#23478;&#26063;&#19982;&#20854;&#20182;&#24694;&#24847;&#36719;&#20214;&#21306;&#20998;&#24320;&#30340;&#37325;&#35201;&#29305;&#24449;&#65292;&#24182;&#20026;&#31934;&#30830;&#20998;&#31867;&#21019;&#24314;&#20102;&#31934;&#31616;&#30340;&#29305;&#24449;&#38598;&#12290;&#36890;&#36807;&#36827;&#34892;&#22810;&#36798; 400 &#20010; epochs &#21644;&#19981;&#21516;&#30340;&#23398;&#20064;&#29575;&#31561;&#24191;&#27867;&#23454;&#39564;&#65292;&#20248;&#21270;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;SAE-LSTM &#27169;&#22411;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11342v1 Announce Type: new  Abstract: The aim of this study is to propose and evaluate an advanced ransomware detection and classification method that combines a Stacked Autoencoder (SAE) for precise feature selection with a Long Short Term Memory (LSTM) classifier to enhance ransomware stratification accuracy. The proposed approach involves thorough pre processing of the UGRansome dataset and training an unsupervised SAE for optimal feature selection or fine tuning via supervised learning to elevate the LSTM model's classification capabilities. The study meticulously analyzes the autoencoder's learned weights and activations to identify essential features for distinguishing ransomware families from other malware and creates a streamlined feature set for precise classification. Extensive experiments, including up to 400 epochs and varying learning rates, are conducted to optimize the model's performance. The results demonstrate the outstanding performance of the SAE-LSTM mod
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#39044;&#22788;&#29702;&#31639;&#27861;&#35782;&#21035;&#23637;&#29616;&#23545;&#31216;&#24615;&#30340;&#27491;&#21017;&#23376;&#36229;&#22270;&#65292;&#20174;&#32780;&#25552;&#39640;&#36229;&#22270;&#22312;&#39640;&#38454;&#38142;&#25509;&#39044;&#27979;&#20013;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#21306;&#20998;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.11339</link><description>&lt;p&gt;
&#36890;&#36807;&#36229;&#22270;&#23545;&#31216;&#24615;&#25171;&#30772;&#36827;&#34892;&#39640;&#38454;&#38142;&#25509;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Expressive Higher-Order Link Prediction through Hypergraph Symmetry Breaking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11339
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#39044;&#22788;&#29702;&#31639;&#27861;&#35782;&#21035;&#23637;&#29616;&#23545;&#31216;&#24615;&#30340;&#27491;&#21017;&#23376;&#36229;&#22270;&#65292;&#20174;&#32780;&#25552;&#39640;&#36229;&#22270;&#22312;&#39640;&#38454;&#38142;&#25509;&#39044;&#27979;&#20013;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#21306;&#20998;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#36229;&#22270;&#30001;&#19968;&#32452;&#33410;&#28857;&#20197;&#21450;&#31216;&#20026;&#36229;&#36793;&#30340;&#33410;&#28857;&#23376;&#38598;&#21512;&#32452;&#25104;&#12290;&#26356;&#39640;&#38454;&#38142;&#25509;&#39044;&#27979;&#26159;&#39044;&#27979;&#19968;&#20010;&#36229;&#22270;&#20013;&#26159;&#21542;&#23384;&#22312;&#32570;&#22833;&#30340;&#36229;&#36793;&#30340;&#20219;&#21153;&#12290;&#20026;&#39640;&#38454;&#38142;&#25509;&#39044;&#27979;&#23398;&#20064;&#30340;&#36229;&#36793;&#34920;&#31034;&#22312;&#21516;&#26500;&#19979;&#19981;&#22833;&#21435;&#21306;&#20998;&#33021;&#21147;&#26102;&#20855;&#26377;&#23436;&#20840;&#34920;&#36798;&#24615;&#12290;&#35768;&#22810;&#29616;&#26377;&#30340;&#36229;&#22270;&#34920;&#31034;&#23398;&#20064;&#22120;&#21463;&#21040;&#24191;&#20041;Weisfeiler Lehman-1&#65288;GWL-1&#65289;&#31639;&#27861;&#30340;&#34920;&#36798;&#33021;&#21147;&#38480;&#21046;&#65292;&#23427;&#26159;Weisfeiler Lehman-1&#31639;&#27861;&#30340;&#25512;&#24191;&#12290;&#28982;&#32780;&#65292;GWL-1&#30340;&#34920;&#36798;&#33021;&#21147;&#26377;&#38480;&#12290;&#20107;&#23454;&#19978;&#65292;&#20855;&#26377;&#30456;&#21516;GWL-1&#20540;&#33410;&#28857;&#30340;&#35825;&#23548;&#23376;&#36229;&#22270;&#26159;&#26080;&#27861;&#21306;&#20998;&#30340;&#12290;&#27492;&#22806;&#65292;&#22312;&#36229;&#22270;&#19978;&#36827;&#34892;&#28040;&#24687;&#20256;&#36882;&#21487;&#33021;&#24050;&#32463;&#22312;GPU&#20869;&#23384;&#19978;&#21464;&#24471;&#35745;&#31639;&#26114;&#36149;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21487;&#20197;&#35782;&#21035;&#20986;&#23637;&#29616;&#23545;&#31216;&#24615;&#30340;&#29305;&#23450;&#27491;&#21017;&#23376;&#36229;&#22270;&#30340;&#39044;&#22788;&#29702;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11339v1 Announce Type: new  Abstract: A hypergraph consists of a set of nodes along with a collection of subsets of the nodes called hyperedges. Higher-order link prediction is the task of predicting the existence of a missing hyperedge in a hypergraph. A hyperedge representation learned for higher order link prediction is fully expressive when it does not lose distinguishing power up to an isomorphism. Many existing hypergraph representation learners, are bounded in expressive power by the Generalized Weisfeiler Lehman-1 (GWL-1) algorithm, a generalization of the Weisfeiler Lehman-1 algorithm. However, GWL-1 has limited expressive power. In fact, induced subhypergraphs with identical GWL-1 valued nodes are indistinguishable. Furthermore, message passing on hypergraphs can already be computationally expensive, especially on GPU memory. To address these limitations, we devise a preprocessing algorithm that can identify certain regular subhypergraphs exhibiting symmetry. Our p
&lt;/p&gt;</description></item><item><title>&#35813;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25506;&#32034;&#30340;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#32570;&#20047;&#37096;&#20998;&#21453;&#39304;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#20998;&#31867;&#22120;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#31574;&#30053;&#26469;&#30830;&#20445;&#25152;&#26377;&#23376;&#32676;&#20307;&#37117;&#34987;&#25506;&#32034;&#12289;&#38450;&#27490;&#38169;&#35823;&#20998;&#31867;&#12289;&#20197;&#21450;&#25910;&#25947;&#21040;&#26399;&#26395;&#30340;&#20998;&#31867;&#22120;&#12290;</title><link>https://arxiv.org/abs/2402.11338</link><description>&lt;p&gt;
&#20855;&#26377;&#37096;&#20998;&#21453;&#39304;&#30340;&#20844;&#24179;&#20998;&#31867;&#65306;&#19968;&#31181;&#22522;&#20110;&#25506;&#32034;&#30340;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Fair Classification with Partial Feedback: An Exploration-Based Data-Collection Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11338
&lt;/p&gt;
&lt;p&gt;
&#35813;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25506;&#32034;&#30340;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#32570;&#20047;&#37096;&#20998;&#21453;&#39304;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#20998;&#31867;&#22120;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#31574;&#30053;&#26469;&#30830;&#20445;&#25152;&#26377;&#23376;&#32676;&#20307;&#37117;&#34987;&#25506;&#32034;&#12289;&#38450;&#27490;&#38169;&#35823;&#20998;&#31867;&#12289;&#20197;&#21450;&#25910;&#25947;&#21040;&#26399;&#26395;&#30340;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#39044;&#27979;&#22330;&#26223;&#65288;&#20363;&#22914;&#20449;&#36151;&#25918;&#27454;&#65289;&#20013;&#65292;&#21482;&#26377;&#36807;&#21435;&#34987;&#31215;&#26497;&#20998;&#31867;&#30340;&#26679;&#26412;&#25165;&#20250;&#35266;&#23519;&#21040;&#30495;&#23454;&#32467;&#26524;&#12290;&#36825;&#20123;&#36807;&#21435;&#30340;&#35266;&#23519;&#32467;&#26524;&#24418;&#25104;&#20102;&#29992;&#20110;&#35757;&#32451;&#20998;&#31867;&#22120;&#20197;&#36827;&#34892;&#26410;&#26469;&#39044;&#27979;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#32570;&#20047;&#20851;&#20110;&#36807;&#21435;&#65288;&#38169;&#35823;&#22320;&#65289;&#34987;&#36127;&#38754;&#20998;&#31867;&#30340;&#26679;&#26412;&#32467;&#26524;&#30340;&#20449;&#24687;&#65292;&#21487;&#33021;&#23548;&#33268;&#38169;&#35823;&#30340;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#21487;&#29992;&#25968;&#25454;&#35757;&#32451;&#20998;&#31867;&#22120;&#65292;&#24182;&#25552;&#20379;&#19968;&#31995;&#21015;&#25506;&#32034;&#31574;&#30053;&#26469;&#25910;&#38598;&#20851;&#20110;&#21542;&#21017;&#20250;&#34987;&#24573;&#30053;&#30340;&#23376;&#32676;&#20307;&#30340;&#32467;&#26524;&#25968;&#25454;&#12290;&#23545;&#20110;&#20219;&#20309;&#25506;&#32034;&#31574;&#30053;&#65292;&#35813;&#26041;&#27861;&#37117;&#20855;&#26377;&#20197;&#19979;&#20445;&#35777;&#65306;&#65288;1&#65289;&#25152;&#26377;&#23376;&#32676;&#20307;&#37117;&#24471;&#21040;&#20102;&#25506;&#32034;&#65292;&#65288;2&#65289;&#20551;&#38451;&#24615;&#30340;&#27604;&#20363;&#21463;&#21040;&#20102;&#38480;&#21046;&#65292;&#65288;3&#65289;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#25910;&#25947;&#21040;&#19968;&#20010;&#8220;&#26399;&#26395;&#8221;&#30340;&#20998;&#31867;&#22120;&#12290;&#27491;&#30830;&#30340;&#25506;&#32034;&#31574;&#30053;&#21462;&#20915;&#20110;&#19978;&#19979;&#25991;&#65307;&#23427;&#21487;&#20197;&#36873;&#25321;&#20197;&#25913;&#21892;&#23398;&#20064;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11338v1 Announce Type: cross  Abstract: In many predictive contexts (e.g., credit lending), true outcomes are only observed for samples that were positively classified in the past. These past observations, in turn, form training datasets for classifiers that make future predictions. However, such training datasets lack information about the outcomes of samples that were (incorrectly) negatively classified in the past and can lead to erroneous classifiers. We present an approach that trains a classifier using available data and comes with a family of exploration strategies to collect outcome data about subpopulations that otherwise would have been ignored. For any exploration strategy, the approach comes with guarantees that (1) all sub-populations are explored, (2) the fraction of false positives is bounded, and (3) the trained classifier converges to a "desired" classifier. The right exploration strategy is context-dependent; it can be chosen to improve learning guarantees 
&lt;/p&gt;</description></item><item><title>SpikeNAS&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#20869;&#23384;&#24863;&#30693;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26694;&#26550;&#65292;&#26088;&#22312;&#24110;&#21161;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#31995;&#32479;&#24555;&#36895;&#25214;&#21040;&#22312;&#32473;&#23450;&#20869;&#23384;&#39044;&#31639;&#19979;&#39640;&#20934;&#30830;&#24615;&#30340;&#36866;&#24403;&#26550;&#26500;&#12290;</title><link>https://arxiv.org/abs/2402.11322</link><description>&lt;p&gt;
SpikeNAS: &#19968;&#31181;&#38754;&#21521;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#31995;&#32479;&#30340;&#24555;&#36895;&#20869;&#23384;&#24863;&#30693;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
SpikeNAS: A Fast Memory-Aware Neural Architecture Search Framework for Spiking Neural Network Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11322
&lt;/p&gt;
&lt;p&gt;
SpikeNAS&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#20869;&#23384;&#24863;&#30693;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26694;&#26550;&#65292;&#26088;&#22312;&#24110;&#21161;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#31995;&#32479;&#24555;&#36895;&#25214;&#21040;&#22312;&#32473;&#23450;&#20869;&#23384;&#39044;&#31639;&#19979;&#39640;&#20934;&#30830;&#24615;&#30340;&#36866;&#24403;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#20026;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#25552;&#20379;&#20102;&#23454;&#29616;&#36229;&#20302;&#21151;&#32791;&#35745;&#31639;&#30340;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#30446;&#21069;&#65292;&#22823;&#22810;&#25968;SNN&#26550;&#26500;&#37117;&#28304;&#33258;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#31070;&#32463;&#20803;&#30340;&#26550;&#26500;&#21644;&#25805;&#20316;&#19982;SNN&#19981;&#21516;&#65292;&#25110;&#32773;&#22312;&#19981;&#32771;&#34385;&#26469;&#33258;&#24213;&#23618;&#22788;&#29702;&#30828;&#20214;&#30340;&#20869;&#23384;&#39044;&#31639;&#30340;&#24773;&#20917;&#19979;&#24320;&#21457;&#12290;&#36825;&#20123;&#38480;&#21046;&#38459;&#30861;&#20102;SNN&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#20805;&#20998;&#21457;&#25381;&#28508;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SpikeNAS&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#20869;&#23384;&#24863;&#30693;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#26694;&#26550;&#65292;&#21487;&#22312;&#32473;&#23450;&#20869;&#23384;&#39044;&#31639;&#19979;&#24555;&#36895;&#25214;&#21040;&#19968;&#20010;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#30340;&#36866;&#24403;SNN&#26550;&#26500;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#30340;SpikeNAS&#37319;&#29992;&#20102;&#20960;&#20010;&#20851;&#38190;&#27493;&#39588;&#65306;&#20998;&#26512;&#32593;&#32476;&#25805;&#20316;&#23545;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#65292;&#22686;&#24378;&#32593;&#32476;&#26550;&#26500;&#20197;&#25552;&#39640;&#23398;&#20064;&#36136;&#37327;&#65292;&#24182;&#24320;&#21457;&#24555;&#36895;&#20869;&#23384;&#24863;&#30693;&#25628;&#32034;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11322v1 Announce Type: cross  Abstract: Spiking Neural Networks (SNNs) offer a promising solution to achieve ultra low-power/energy computation for solving machine learning tasks. Currently, most of the SNN architectures are derived from Artificial Neural Networks whose neurons' architectures and operations are different from SNNs, or developed without considering memory budgets from the underlying processing hardware. These limitations hinder the SNNs from reaching their full potential in accuracy and efficiency. Towards this, we propose SpikeNAS, a novel memory-aware neural architecture search (NAS) framework for SNNs that can quickly find an appropriate SNN architecture with high accuracy under the given memory budgets. To do this, our SpikeNAS employs several key steps: analyzing the impacts of network operations on the accuracy, enhancing the network architecture to improve the learning quality, and developing a fast memory-aware search algorithm. The experimental resul
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24212;&#23545;&#20351;&#29992;&#20559;&#20506;&#20301;&#32622;&#25968;&#25454;&#35745;&#31639;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#36896;&#25104;&#30340;&#19981;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#25968;&#25454;&#38598;&#22312;&#25935;&#24863;&#20915;&#31574;&#20013;&#21487;&#33021;&#20135;&#29983;&#30340;&#20005;&#37325;&#21518;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.11318</link><description>&lt;p&gt;
BiasBuster&#65306;&#20351;&#29992;&#20559;&#20506;&#20301;&#32622;&#25968;&#25454;&#20934;&#30830;&#20272;&#35745;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#30340;&#31070;&#32463;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
BiasBuster: a Neural Approach for Accurate Estimation of Population Statistics using Biased Location Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11318
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24212;&#23545;&#20351;&#29992;&#20559;&#20506;&#20301;&#32622;&#25968;&#25454;&#35745;&#31639;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#36896;&#25104;&#30340;&#19981;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#25968;&#25454;&#38598;&#22312;&#25935;&#24863;&#20915;&#31574;&#20013;&#21487;&#33021;&#20135;&#29983;&#30340;&#20005;&#37325;&#21518;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#31227;&#21160;&#35774;&#22791;&#25910;&#38598;&#30340;&#20301;&#32622;&#25968;&#25454;&#38750;&#24120;&#26377;&#29992;&#65288;&#20363;&#22914;&#65292;&#29992;&#20110;COVID-19&#39044;&#27979;&#21644;&#25919;&#31574;&#21046;&#23450;&#12289;&#22478;&#24066;&#27969;&#21160;&#24615;&#20998;&#26512;&#21644;&#33829;&#38144;&#12289;&#20197;&#21450;&#33719;&#21462;&#21830;&#19994;&#27934;&#35265;&#65289;&#65292;&#20294;&#36825;&#20123;&#25968;&#25454;&#36890;&#24120;&#26469;&#33258;&#20110;&#19968;&#20010;&#24102;&#26377;&#20559;&#21521;&#24615;&#30340;&#20154;&#21475;&#23376;&#38598;&#65292;&#26576;&#20123;&#31038;&#21306;&#22312;&#25910;&#38598;&#30340;&#25968;&#25454;&#20013;&#34987;&#36807;&#24230;&#25110;&#32773;&#20302;&#20272;&#20102;&#12290;&#22240;&#27492;&#65292;&#20174;&#36825;&#26679;&#30340;&#25968;&#25454;&#38598;&#35745;&#31639;&#30340;&#27719;&#24635;&#32479;&#35745;&#25968;&#25454;&#65288;&#22914;Safegraph&#12289;Google&#21644;Facebook&#31561;&#21508;&#31181;&#20844;&#21496;&#25152;&#20570;&#30340;&#37027;&#26679;&#65289;&#65292;&#22914;&#26524;&#24573;&#35270;&#20102;&#20559;&#35265;&#65292;&#20250;&#23548;&#33268;&#23545;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#30340;&#19981;&#20934;&#30830;&#34920;&#31034;&#12290;&#36825;&#26679;&#30340;&#32479;&#35745;&#25968;&#25454;&#19981;&#20165;&#26222;&#36941;&#19981;&#20934;&#30830;&#65292;&#32780;&#19988;&#38169;&#35823;&#23558;&#19981;&#25104;&#27604;&#20363;&#22320;&#24433;&#21709;&#19981;&#21516;&#30340;&#20154;&#21475;&#20122;&#32452;&#65288;&#20363;&#22914;&#65292;&#22240;&#20026;&#23427;&#20204;&#24573;&#35270;&#20102;&#34987;&#20302;&#20272;&#30340;&#31038;&#21306;&#65289;&#12290;&#36825;&#20250;&#24102;&#26469;&#20005;&#37325;&#21518;&#26524;&#65292;&#22240;&#20026;&#36825;&#20123;&#25968;&#25454;&#38598;&#34987;&#29992;&#20110;&#20687;COVID-19&#25919;&#31574;&#21046;&#23450;&#36825;&#26679;&#30340;&#25935;&#24863;&#20915;&#31574;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#21033;&#29992;&#36825;&#31181;&#26377;&#20559;&#24046;&#25968;&#25454;&#25552;&#20379;&#20934;&#30830;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11318v1 Announce Type: new  Abstract: While extremely useful (e.g., for COVID-19 forecasting and policy-making, urban mobility analysis and marketing, and obtaining business insights), location data collected from mobile devices often contain data from a biased population subset, with some communities over or underrepresented in the collected datasets. As a result, aggregate statistics calculated from such datasets (as is done by various companies including Safegraph, Google, and Facebook), while ignoring the bias, leads to an inaccurate representation of population statistics. Such statistics will not only be generally inaccurate, but the error will disproportionately impact different population subgroups (e.g., because they ignore the underrepresented communities). This has dire consequences, as these datasets are used for sensitive decision-making such as COVID-19 policymaking. This paper tackles the problem of providing accurate population statistics using such biased da
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DORA&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20449;&#24687;&#29942;&#39048;&#21407;&#29702;&#22312;&#31163;&#32447;&#35774;&#32622;&#20013;&#23398;&#20064;&#36866;&#24212;&#24615;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#21160;&#24577;&#32534;&#30721;&#19982;&#29615;&#22659;&#25968;&#25454;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#19982;&#19982;&#34892;&#20026;&#31574;&#30053;&#30340;&#20114;&#20449;&#24687;&#20043;&#38388;&#30340;&#38590;&#39064;</title><link>https://arxiv.org/abs/2402.11317</link><description>&lt;p&gt;
&#38024;&#23545;&#38750;&#38745;&#24577;&#21160;&#24577;&#30340;&#24555;&#36895;&#22312;&#32447;&#35843;&#25972;&#30340;&#21435;&#20559;&#32622;&#31163;&#32447;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Debiased Offline Representation Learning for Fast Online Adaptation in Non-stationary Dynamics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11317
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DORA&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20449;&#24687;&#29942;&#39048;&#21407;&#29702;&#22312;&#31163;&#32447;&#35774;&#32622;&#20013;&#23398;&#20064;&#36866;&#24212;&#24615;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#21160;&#24577;&#32534;&#30721;&#19982;&#29615;&#22659;&#25968;&#25454;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#19982;&#19982;&#34892;&#20026;&#31574;&#30053;&#30340;&#20114;&#20449;&#24687;&#20043;&#38388;&#30340;&#38590;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#33021;&#22815;&#36866;&#24212;&#38750;&#38745;&#24577;&#29615;&#22659;&#30340;&#31574;&#30053;&#23545;&#20110;&#29616;&#23454;&#19990;&#30028;&#30340;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22312;&#20165;&#26377;&#19968;&#32452;&#26377;&#38480;&#30340;&#39044;&#20808;&#25910;&#38598;&#30340;&#36712;&#36857;&#30340;&#31163;&#32447;&#35774;&#32622;&#20013;&#23398;&#20064;&#36825;&#31181;&#36866;&#24212;&#24615;&#31574;&#30053;&#23384;&#22312;&#26174;&#33879;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#21435;&#20559;&#32622;&#31163;&#32447;&#34920;&#31034;&#24555;&#36895;&#22312;&#32447;&#35843;&#25972;&#65288;DORA&#65289;&#30340;&#26032;&#26041;&#27861;&#12290;DORA&#34701;&#20837;&#20102;&#20449;&#24687;&#29942;&#39048;&#21407;&#29702;&#65292;&#26368;&#22823;&#21270;&#20102;&#21160;&#24577;&#32534;&#30721;&#19982;&#29615;&#22659;&#25968;&#25454;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#20102;&#21160;&#24577;&#32534;&#30721;&#19982;&#34892;&#20026;&#31574;&#30053;&#30340;&#20114;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;DORA&#30340;&#23454;&#38469;&#23454;&#29616;&#65292;&#21033;&#29992;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11317v1 Announce Type: cross  Abstract: Developing policies that can adjust to non-stationary environments is essential for real-world reinforcement learning applications. However, learning such adaptable policies in offline settings, with only a limited set of pre-collected trajectories, presents significant challenges. A key difficulty arises because the limited offline data makes it hard for the context encoder to differentiate between changes in the environment dynamics and shifts in the behavior policy, often leading to context misassociations. To address this issue, we introduce a novel approach called Debiased Offline Representation for fast online Adaptation (DORA). DORA incorporates an information bottleneck principle that maximizes mutual information between the dynamics encoding and the environmental data, while minimizing mutual information between the dynamics encoding and the actions of the behavior policy. We present a practical implementation of DORA, leverag
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#23454;&#39564;&#35780;&#20272;&#20102;O-Cloud&#30340;&#33021;&#32791;&#21450;&#20854;&#19982;&#26381;&#21153;&#22120;&#30828;&#20214;&#12289;&#23481;&#37327;&#21644;&#25968;&#25454;&#27969;&#37327;&#29305;&#24615;&#30340;&#20851;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#31574;&#30053;&#21644;&#26080;&#32447;&#31574;&#30053;&#65292;&#24179;&#34913;&#33021;&#28304;&#33410;&#32422;&#21644;&#24615;&#33021;&#65292;&#30830;&#20445;&#23427;&#20204;&#22312;&#26381;&#21153;&#22120;&#21644;&#29992;&#25143;&#20043;&#38388;&#20844;&#24179;&#20998;&#37197;&#12290;</title><link>https://arxiv.org/abs/2402.11285</link><description>&lt;p&gt;
&#22312;&#34394;&#25311;&#21270;O-RAN&#24179;&#21488;&#20013;&#30340;&#20844;&#24179;&#36164;&#28304;&#20998;&#37197;
&lt;/p&gt;
&lt;p&gt;
Fair Resource Allocation in Virtualized O-RAN Platforms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11285
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#23454;&#39564;&#35780;&#20272;&#20102;O-Cloud&#30340;&#33021;&#32791;&#21450;&#20854;&#19982;&#26381;&#21153;&#22120;&#30828;&#20214;&#12289;&#23481;&#37327;&#21644;&#25968;&#25454;&#27969;&#37327;&#29305;&#24615;&#30340;&#20851;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#31574;&#30053;&#21644;&#26080;&#32447;&#31574;&#30053;&#65292;&#24179;&#34913;&#33021;&#28304;&#33410;&#32422;&#21644;&#24615;&#33021;&#65292;&#30830;&#20445;&#23427;&#20204;&#22312;&#26381;&#21153;&#22120;&#21644;&#29992;&#25143;&#20043;&#38388;&#20844;&#24179;&#20998;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
O-RAN&#31995;&#32479;&#21450;&#20854;&#22312;&#34394;&#25311;&#21270;&#36890;&#29992;&#35745;&#31639;&#24179;&#21488;&#65288;O-Cloud&#65289;&#20013;&#30340;&#37096;&#32626;&#26500;&#25104;&#20102;&#19968;&#20010;&#39044;&#35745;&#23558;&#24102;&#26469;&#21069;&#25152;&#26410;&#26377;&#30340;&#24615;&#33021;&#22686;&#30410;&#30340;&#33539;&#24335;&#36716;&#21464;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26550;&#26500;&#24102;&#26469;&#20102;&#26032;&#30340;&#23454;&#26045;&#25361;&#25112;&#65292;&#24182;&#23041;&#32961;&#30528;&#21152;&#21095;&#31227;&#21160;&#32593;&#32476;&#24050;&#32463;&#39640;&#33021;&#32791;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102;O-Cloud&#30340;&#33021;&#32791;&#21450;&#20854;&#23545;&#26381;&#21153;&#22120;&#30828;&#20214;&#12289;&#23481;&#37327;&#21644;&#25968;&#25454;&#27969;&#37327;&#29305;&#24615;&#30340;&#20381;&#36182;&#24615;&#65292;&#36825;&#20123;&#29305;&#24615;&#36890;&#24120;&#20250;&#38543;&#26102;&#38388;&#25913;&#21464;&#12290;&#25509;&#19979;&#26469;&#65292;&#23427;&#25552;&#20986;&#20102;&#19968;&#20010;&#35745;&#31639;&#31574;&#30053;&#65292;&#20197;&#33021;&#25928;&#30340;&#26041;&#24335;&#23558;&#22522;&#31449;&#25968;&#25454;&#36127;&#36733;&#20998;&#37197;&#21040;O-Cloud&#26381;&#21153;&#22120;&#65307;&#20197;&#21450;&#19968;&#20010;&#26080;&#32447;&#31574;&#30053;&#65292;&#21487;&#23454;&#26102;&#30830;&#23450;&#27599;&#20010;&#29992;&#25143;&#30340;&#26368;&#23567;&#20256;&#36755;&#22359;&#22823;&#23567;&#65292;&#20174;&#32780;&#36991;&#20813;&#19981;&#24517;&#35201;&#30340;&#33021;&#28304;&#25104;&#26412;&#12290;&#36825;&#20123;&#31574;&#30053;&#24179;&#34913;&#20102;&#33021;&#28304;&#33410;&#32422;&#21644;&#24615;&#33021;&#65292;&#24182;&#30830;&#20445;&#23427;&#20204;&#22312;&#26381;&#21153;&#22120;&#21644;&#29992;&#25143;&#20043;&#38388;&#20998;&#25955;&#20844;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11285v1 Announce Type: cross  Abstract: O-RAN systems and their deployment in virtualized general-purpose computing platforms (O-Cloud) constitute a paradigm shift expected to bring unprecedented performance gains. However, these architectures raise new implementation challenges and threaten to worsen the already-high energy consumption of mobile networks. This paper presents first a series of experiments which assess the O-Cloud's energy costs and their dependency on the servers' hardware, capacity and data traffic properties which, typically, change over time. Next, it proposes a compute policy for assigning the base station data loads to O-Cloud servers in an energy-efficient fashion; and a radio policy that determines at near-real-time the minimum transmission block size for each user so as to avoid unnecessary energy costs. The policies balance energy savings with performance, and ensure that both of them are dispersed fairly across the servers and users, respectively. 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;MRI&#37325;&#24314;&#26041;&#27861;TC-DiffRecon&#65292;&#26088;&#22312;&#35299;&#20915;&#25193;&#25955;&#27169;&#22411;&#23548;&#33268;&#30340;&#22270;&#20687;&#30862;&#35010;&#21644;&#19981;&#19968;&#33268;&#24615;&#20197;&#21450;&#29983;&#25104;&#22270;&#20687;&#36807;&#24230;&#24179;&#28369;&#31561;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.11274</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#21644;&#25913;&#36827;&#30340;MF-UNet&#26041;&#27861;&#30340;&#32441;&#29702;&#21327;&#35843;MRI&#37325;&#24314;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
TC-DiffRecon: Texture coordination MRI reconstruction method based on diffusion model and modified MF-UNet method
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11274
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;MRI&#37325;&#24314;&#26041;&#27861;TC-DiffRecon&#65292;&#26088;&#22312;&#35299;&#20915;&#25193;&#25955;&#27169;&#22411;&#23548;&#33268;&#30340;&#22270;&#20687;&#30862;&#35010;&#21644;&#19981;&#19968;&#33268;&#24615;&#20197;&#21450;&#29983;&#25104;&#22270;&#20687;&#36807;&#24230;&#24179;&#28369;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#19968;&#31181;&#26032;&#22411;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#29983;&#25104;&#26041;&#27861;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#36825;&#20123;&#27169;&#22411;&#23581;&#35797;&#20174;&#31526;&#21512;&#30446;&#26631;&#20998;&#24067;&#30340;&#39640;&#26031;&#20998;&#24067;&#20013;&#23545;&#25968;&#25454;&#36827;&#34892;&#37319;&#26679;&#65292;&#24182;&#24050;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;MRI&#25968;&#25454;&#30340;&#37325;&#24314;&#12290;&#28982;&#32780;&#65292;&#20316;&#20026;&#19968;&#31181;&#26080;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#65292;&#25193;&#25955;&#27169;&#22411;&#36890;&#24120;&#30001;&#20110;&#26465;&#20214;&#24341;&#23548;&#24341;&#20837;&#30340;&#19968;&#33268;&#25968;&#25454;&#25237;&#24433;&#32780;&#25171;&#30772;&#22270;&#20687;&#21327;&#35843;&#24615;&#12290;&#36825;&#32463;&#24120;&#23548;&#33268;&#22270;&#20687;&#30340;&#30862;&#35010;&#21644;&#19981;&#19968;&#33268;&#24615;&#12290;&#27492;&#22806;&#65292;&#25193;&#25955;&#27169;&#22411;&#30340;&#22266;&#26377;&#23616;&#38480;&#24615;&#36890;&#24120;&#20250;&#23548;&#33268;&#29983;&#25104;&#22270;&#20687;&#36807;&#24230;&#24179;&#28369;&#12290;&#22312;&#21516;&#19968;&#24605;&#36335;&#19978;&#65292;&#19968;&#20123;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27169;&#22411;&#32463;&#24120;&#34920;&#29616;&#20986;&#36739;&#24046;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24847;&#21619;&#30528;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#21463;&#21040;&#19981;&#21516;&#21152;&#36895;&#22240;&#23376;&#30340;&#26497;&#22823;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TC-DiffRecon&#30340;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;MRI&#37325;&#24314;&#26041;&#27861;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11274v1 Announce Type: cross  Abstract: Recently, diffusion models have gained significant attention as a novel set of deep learning-based generative methods. These models attempt to sample data from a Gaussian distribution that adheres to a target distribution, and have been successfully adapted to the reconstruction of MRI data. However, as an unconditional generative model, the diffusion model typically disrupts image coordination because of the consistent projection of data introduced by conditional bootstrap. This often results in image fragmentation and incoherence. Furthermore, the inherent limitations of the diffusion model often lead to excessive smoothing of the generated images. In the same vein, some deep learning-based models often suffer from poor generalization performance, meaning their effectiveness is greatly affected by different acceleration factors. To address these challenges, we propose a novel diffusion model-based MRI reconstruction method, named TC-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#24179;&#32531;&#23616;&#37096;&#26368;&#23567;&#20540;&#30340;&#35282;&#24230;&#20998;&#26512;&#22810;&#27169;&#24577;&#25512;&#33616;&#31995;&#32479;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#38236;&#20687;&#26799;&#24230;&#65288;MG&#65289;&#30340;&#26799;&#24230;&#31574;&#30053;&#65292;&#21487;&#20197;&#22686;&#24378;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#65292;&#32531;&#35299;&#26469;&#33258;&#22810;&#27169;&#24577;&#20449;&#24687;&#36755;&#20837;&#30340;&#19981;&#31283;&#23450;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2402.11262</link><description>&lt;p&gt;
&#38236;&#20687;&#26799;&#24230;&#65306;&#36890;&#36807;&#25506;&#32034;&#24179;&#32531;&#23616;&#37096;&#26368;&#23567;&#20540;&#23454;&#29616;&#40065;&#26834;&#30340;&#22810;&#27169;&#24335;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Mirror Gradient: Towards Robust Multimodal Recommender Systems via Exploring Flat Local Minima
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11262
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#24179;&#32531;&#23616;&#37096;&#26368;&#23567;&#20540;&#30340;&#35282;&#24230;&#20998;&#26512;&#22810;&#27169;&#24577;&#25512;&#33616;&#31995;&#32479;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#38236;&#20687;&#26799;&#24230;&#65288;MG&#65289;&#30340;&#26799;&#24230;&#31574;&#30053;&#65292;&#21487;&#20197;&#22686;&#24378;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#65292;&#32531;&#35299;&#26469;&#33258;&#22810;&#27169;&#24577;&#20449;&#24687;&#36755;&#20837;&#30340;&#19981;&#31283;&#23450;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#25512;&#33616;&#31995;&#32479;&#21033;&#29992;&#21508;&#31181;&#20449;&#24687;&#26469;&#24314;&#27169;&#29992;&#25143;&#20559;&#22909;&#21644;&#29289;&#21697;&#29305;&#24449;&#65292;&#24110;&#21161;&#29992;&#25143;&#21457;&#29616;&#31526;&#21512;&#20854;&#20852;&#36259;&#30340;&#29289;&#21697;&#12290;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#25972;&#21512;&#22810;&#27169;&#24577;&#20449;&#24687;&#21487;&#20197;&#32531;&#35299;&#22266;&#26377;&#30340;&#25361;&#25112;&#65292;&#20363;&#22914;&#25968;&#25454;&#31232;&#30095;&#38382;&#39064;&#21644;&#20919;&#21551;&#21160;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23427;&#21516;&#26102;&#20250;&#25918;&#22823;&#26469;&#33258;&#22810;&#27169;&#24577;&#20449;&#24687;&#36755;&#20837;&#30340;&#26576;&#20123;&#39118;&#38505;&#65292;&#22914;&#20449;&#24687;&#35843;&#25972;&#39118;&#38505;&#21644;&#22266;&#26377;&#22122;&#22768;&#39118;&#38505;&#12290;&#36825;&#20123;&#39118;&#38505;&#23545;&#25512;&#33616;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#26500;&#25104;&#37325;&#35201;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24179;&#32531;&#23616;&#37096;&#26368;&#23567;&#20540;&#30340;&#26032;&#39062;&#35270;&#35282;&#20998;&#26512;&#22810;&#27169;&#24577;&#25512;&#33616;&#31995;&#32479;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#31616;&#27905;&#32780;&#26377;&#25928;&#30340;&#26799;&#24230;&#31574;&#30053;&#65292;&#31216;&#20026;&#38236;&#20687;&#26799;&#24230;&#65288;MG&#65289;&#12290;&#36825;&#31181;&#31574;&#30053;&#21487;&#20197;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#38544;&#24335;&#22686;&#24378;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#65292;&#32531;&#35299;&#30001;&#22810;&#27169;&#24577;&#20449;&#24687;&#36755;&#20837;&#24341;&#36215;&#30340;&#19981;&#31283;&#23450;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11262v1 Announce Type: cross  Abstract: Multimodal recommender systems utilize various types of information to model user preferences and item features, helping users discover items aligned with their interests. The integration of multimodal information mitigates the inherent challenges in recommender systems, e.g., the data sparsity problem and cold-start issues. However, it simultaneously magnifies certain risks from multimodal information inputs, such as information adjustment risk and inherent noise risk. These risks pose crucial challenges to the robustness of recommendation models. In this paper, we analyze multimodal recommender systems from the novel perspective of flat local minima and propose a concise yet effective gradient strategy called Mirror Gradient (MG). This strategy can implicitly enhance the model's robustness during the optimization process, mitigating instability risks arising from multimodal information inputs. We also provide strong theoretical evide
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#23545;&#40784;&#26694;&#26550;SELF-JUDGE&#65292;&#36890;&#36807;&#22686;&#21152;&#24335;&#30417;&#30563;&#24494;&#35843;&#65288;JSFT&#65289;&#35757;&#32451;&#19968;&#20010;&#21516;&#26102;&#20805;&#24403;&#31574;&#30053;&#21644;&#35780;&#21028;&#22120;&#30340;&#21333;&#19968;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#21442;&#25968;&#39640;&#25928;&#30340;&#22522;&#20110;&#25919;&#31574;&#23398;&#20064;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#22870;&#21169;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.11253</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#25919;&#31574;&#30340;&#33258;&#25105;&#21028;&#26029;&#26469;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Aligning Large Language Models by On-Policy Self-Judgment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11253
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#23545;&#40784;&#26694;&#26550;SELF-JUDGE&#65292;&#36890;&#36807;&#22686;&#21152;&#24335;&#30417;&#30563;&#24494;&#35843;&#65288;JSFT&#65289;&#35757;&#32451;&#19968;&#20010;&#21516;&#26102;&#20805;&#24403;&#31574;&#30053;&#21644;&#35780;&#21028;&#22120;&#30340;&#21333;&#19968;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#21442;&#25968;&#39640;&#25928;&#30340;&#22522;&#20110;&#25919;&#31574;&#23398;&#20064;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#22870;&#21169;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#20445;&#25345;&#19968;&#33268;&#65292;&#29616;&#26377;&#30740;&#31350;&#35201;&#20040;&#21033;&#29992;&#21333;&#29420;&#30340;&#22870;&#21169;&#27169;&#22411;&#65288;RM&#65289;&#25191;&#34892;&#22522;&#20110;&#25919;&#31574;&#30340;&#23398;&#20064;&#65292;&#35201;&#20040;&#36890;&#36807;&#25918;&#24323;&#22522;&#20110;&#25919;&#31574;&#30340;&#23398;&#20064;&#21644;&#23545;&#29420;&#31435;RM&#30340;&#38656;&#27714;&#31616;&#21270;&#35757;&#32451;&#36807;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#23545;&#40784;&#26694;&#26550;SELF-JUDGE&#65292;&#23427;&#26082;&#26159;(1) &#22522;&#20110;&#25919;&#31574;&#30340;&#23398;&#20064;&#65292;&#21448;&#26159;(2) &#21442;&#25968;&#39640;&#25928;&#30340;&#65292;&#22240;&#20026;&#23427;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;RM&#26469;&#35780;&#20272;&#26679;&#26412;&#36827;&#34892;&#22522;&#20110;&#25919;&#31574;&#30340;&#23398;&#20064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22686;&#24378;&#24335;&#30417;&#30563;&#24494;&#35843;&#65288;JSFT&#65289;&#26469;&#35757;&#32451;&#19968;&#20010;&#21333;&#19968;&#27169;&#22411;&#65292;&#20316;&#20026;&#31574;&#30053;&#21644;&#35780;&#21028;&#22120;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#19968;&#23545;&#19968;&#21028;&#26029;&#20219;&#21153;&#35270;&#20026;&#25351;&#23548;&#24335;&#20219;&#21153;&#30340;&#29305;&#27530;&#24773;&#20917;&#65292;&#20174;&#21709;&#24212;&#23545;&#20013;&#36873;&#25321;&#26356;&#22909;&#30340;&#21709;&#24212;&#12290;&#22240;&#27492;&#65292;&#24471;&#21040;&#30340;&#27169;&#22411;&#21487;&#20197;&#35780;&#21028;&#24403;&#21069;&#31574;&#30053;&#30340;&#21363;&#26102;&#21709;&#24212;&#20559;&#22909;&#65292;&#20174;&#33258;&#36523;&#21021;&#22987;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20102;SELF-JUDGE&#30340;&#26377;&#25928;&#24615;&#65292;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11253v1 Announce Type: cross  Abstract: To align large language models with human preferences, existing research either utilizes a separate reward model (RM) to perform on-policy learning or simplifies the training procedure by discarding the on-policy learning and the need for a separate RM. In this paper, we present a novel alignment framework, SELF-JUDGE that is (1) on-policy learning and 2) parameter efficient, as it does not require an additional RM for evaluating the samples for on-policy learning. To this end, we propose Judge-augmented Supervised Fine-Tuning (JSFT) to train a single model acting as both a policy and a judge. Specifically, we view the pairwise judgment task as a special case of the instruction-following task, choosing the better response from a response pair. Thus, the resulting model can judge preferences of on-the-fly responses from current policy initialized from itself. Experimental results show the efficacy of SELF-JUDGE, outperforming baselines 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22788;&#29702;&#19981;&#24179;&#34913;&#25968;&#25454;&#20013;&#22024;&#26434;&#26631;&#31614;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;Class-Balance-based Sample Selection (CBS)&#38450;&#27490;&#24573;&#35270;&#23614;&#37096;&#31867;&#21035;&#26679;&#26412;&#65292;&#24182;&#36890;&#36807;Confidence-based Sample Augmentation (CSA)&#22686;&#24378;&#24178;&#20928;&#26679;&#26412;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.11242</link><description>&lt;p&gt;
&#36890;&#36807;&#38450;&#27490;&#26679;&#26412;&#36873;&#25321;&#20559;&#24046;&#26469;&#23398;&#20064;&#19981;&#24179;&#34913;&#22024;&#26434;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Learning with Imbalanced Noisy Data by Preventing Bias in Sample Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11242
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22788;&#29702;&#19981;&#24179;&#34913;&#25968;&#25454;&#20013;&#22024;&#26434;&#26631;&#31614;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;Class-Balance-based Sample Selection (CBS)&#38450;&#27490;&#24573;&#35270;&#23614;&#37096;&#31867;&#21035;&#26679;&#26412;&#65292;&#24182;&#36890;&#36807;Confidence-based Sample Augmentation (CSA)&#22686;&#24378;&#24178;&#20928;&#26679;&#26412;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#20855;&#26377;&#22024;&#26434;&#26631;&#31614;&#30340;&#26041;&#27861;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#22240;&#20026;&#29616;&#23454;&#22330;&#26223;&#20013;&#19981;&#21487;&#36991;&#20813;&#30340;&#19981;&#23436;&#32654;&#26631;&#31614;&#20250;&#20005;&#37325;&#24433;&#21709;&#28145;&#24230;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20542;&#21521;&#20110;&#23558;&#20302;&#25439;&#22833;&#26679;&#26412;&#35270;&#20026;&#24178;&#20928;&#26679;&#26412;&#65292;&#20002;&#24323;&#39640;&#25439;&#22833;&#26679;&#26412;&#20197;&#20943;&#36731;&#22024;&#26434;&#26631;&#31614;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19981;&#20165;&#21253;&#21547;&#22024;&#26434;&#26631;&#31614;&#65292;&#36824;&#21253;&#21547;&#31867;&#21035;&#19981;&#24179;&#34913;&#12290;&#19981;&#24179;&#34913;&#38382;&#39064;&#23481;&#26131;&#23548;&#33268;&#25439;&#22833;&#36739;&#22823;&#30340;&#23614;&#37096;&#31867;&#21035;&#30340;&#23398;&#20064;&#19981;&#36275;&#65292;&#20174;&#32780;&#20135;&#29983;&#39640;&#25439;&#22833;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#19981;&#24179;&#34913;&#25968;&#25454;&#20013;&#30340;&#22024;&#26434;&#26631;&#31614;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#31867;&#24179;&#34913;&#30340;&#26679;&#26412;&#36873;&#25321;&#65288;CBS&#65289;&#26469;&#38450;&#27490;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24573;&#35270;&#23614;&#37096;&#31867;&#21035;&#26679;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#32622;&#20449;&#24230;&#30340;&#26679;&#26412;&#22686;&#24378;&#65288;CSA&#65289;&#20197;&#21152;&#24378;&#25152;&#36873;&#24178;&#20928;&#26679;&#26412;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11242v1 Announce Type: cross  Abstract: Learning with noisy labels has gained increasing attention because the inevitable imperfect labels in real-world scenarios can substantially hurt the deep model performance. Recent studies tend to regard low-loss samples as clean ones and discard high-loss ones to alleviate the negative impact of noisy labels. However, real-world datasets contain not only noisy labels but also class imbalance. The imbalance issue is prone to causing failure in the loss-based sample selection since the under-learning of tail classes also leans to produce high losses. To this end, we propose a simple yet effective method to address noisy labels in imbalanced datasets. Specifically, we propose Class-Balance-based sample Selection (CBS) to prevent the tail class samples from being neglected during training. We propose Confidence-based Sample Augmentation (CSA) for the chosen clean samples to enhance their reliability in the training process. To exploit sel
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#25552;&#20986;&#19968;&#20010;&#32479;&#19968;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#26816;&#27979;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#24555;&#25463;&#23398;&#20064;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.11237</link><description>&lt;p&gt;
&#23545;&#25239;&#28145;&#24230;&#23398;&#20064;&#20013;&#24555;&#25463;&#26041;&#24335;&#30340;&#32479;&#19968;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Be Persistent: Towards a Unified Solution for Mitigating Shortcuts in Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11237
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#25552;&#20986;&#19968;&#20010;&#32479;&#19968;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#26816;&#27979;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#24555;&#25463;&#23398;&#20064;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#23481;&#26131;&#21463;&#21040;&#24555;&#25463;&#23398;&#20064;&#30340;&#24433;&#21709;&#65306;&#23427;&#20204;&#20542;&#21521;&#20110;&#24314;&#31435;&#36755;&#20837;&#21644;&#36755;&#20986;&#20043;&#38388;&#26080;&#20851;&#30340;&#20851;&#31995;&#65292;&#32780;&#19981;&#26159;&#23398;&#20064;&#39044;&#26399;&#30340;&#20219;&#21153;&#12290;&#24555;&#25463;&#23398;&#20064;&#22312;&#31070;&#32463;&#32593;&#32476;&#35768;&#22810;&#22833;&#36133;&#26696;&#20363;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#36825;&#19968;&#29616;&#35937;&#30340;&#30165;&#36857;&#21487;&#35265;&#20110;&#20854;&#27867;&#21270;&#38382;&#39064;&#12289;&#39046;&#22495;&#36716;&#31227;&#12289;&#23545;&#25239;&#24615;&#33030;&#24369;&#24615;&#65292;&#29978;&#33267;&#23545;&#22810;&#25968;&#32676;&#20307;&#30340;&#20559;&#35265;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;&#21508;&#31181;DNN&#38382;&#39064;&#30340;&#20849;&#21516;&#21407;&#22240;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#37325;&#35201;&#26426;&#20250;&#65292;&#24212;&#35813;&#21033;&#29992;&#36825;&#19968;&#28857;&#25214;&#21040;&#23545;&#25239;&#24555;&#25463;&#23398;&#20064;&#30340;&#32479;&#19968;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;(TDA)&#29305;&#21035;&#26159;&#25345;&#32493;&#21516;&#35843;(PH)&#26041;&#38754;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#20026;&#25506;&#27979;&#28145;&#24230;&#23398;&#20064;&#20013;&#24555;&#25463;&#26041;&#24335;&#21246;&#30011;&#20102;&#32479;&#19968;&#30340;&#36335;&#32447;&#22270;&#12290;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;DNNs&#20013;&#35745;&#31639;&#22270;&#30340;&#25299;&#25169;&#29305;&#24449;&#65292;&#20351;&#29992;&#26080;&#27861;&#23398;&#20064;&#30340;&#31034;&#20363;&#21644;&#20559;&#35265;&#20026;&#20004;&#31181;&#24773;&#20917;&#65292;&#26469;&#35777;&#26126;&#25105;&#20204;&#30340;&#35770;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11237v1 Announce Type: new  Abstract: Deep neural networks (DNNs) are vulnerable to shortcut learning: rather than learning the intended task, they tend to draw inconclusive relationships between their inputs and outputs. Shortcut learning is ubiquitous among many failure cases of neural networks, and traces of this phenomenon can be seen in their generalizability issues, domain shift, adversarial vulnerability, and even bias towards majority groups. In this paper, we argue that this commonality in the cause of various DNN issues creates a significant opportunity that should be leveraged to find a unified solution for shortcut learning. To this end, we outline the recent advances in topological data analysis~(TDA), and persistent homology~(PH) in particular, to sketch a unified roadmap for detecting shortcuts in deep learning. We demonstrate our arguments by investigating the topological features of computational graphs in DNNs using two cases of unlearnable examples and bia
&lt;/p&gt;</description></item><item><title>ZeroG&#26159;&#19968;&#20010;&#26032;&#26694;&#26550;&#65292;&#26088;&#22312;&#23454;&#29616;&#22270;&#20013;&#36328;&#25968;&#25454;&#38598;&#38646;&#23556;&#20987;&#36801;&#31227;&#65292;&#35299;&#20915;&#20102;&#29305;&#24449;&#19981;&#23545;&#40784;&#12289;&#26631;&#31614;&#31354;&#38388;&#19981;&#21305;&#37197;&#21644;&#36127;&#36801;&#31227;&#31561;&#25361;&#25112;</title><link>https://arxiv.org/abs/2402.11235</link><description>&lt;p&gt;
ZeroG&#65306;&#25506;&#31350;&#22270;&#20013;&#36328;&#25968;&#25454;&#38598;&#38646;&#23556;&#20987;&#21487;&#36801;&#31227;&#24615;
&lt;/p&gt;
&lt;p&gt;
ZeroG: Investigating Cross-dataset Zero-shot Transferability in Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11235
&lt;/p&gt;
&lt;p&gt;
ZeroG&#26159;&#19968;&#20010;&#26032;&#26694;&#26550;&#65292;&#26088;&#22312;&#23454;&#29616;&#22270;&#20013;&#36328;&#25968;&#25454;&#38598;&#38646;&#23556;&#20987;&#36801;&#31227;&#65292;&#35299;&#20915;&#20102;&#29305;&#24449;&#19981;&#23545;&#40784;&#12289;&#26631;&#31614;&#31354;&#38388;&#19981;&#21305;&#37197;&#21644;&#36127;&#36801;&#31227;&#31561;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22914;GPT-4&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#38646;&#23556;&#20987;&#36801;&#31227;&#23398;&#20064;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;NLP&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#21644;CV&#27169;&#22411;&#30340;&#22522;&#20110;&#26816;&#32034;&#30340;&#26041;&#27861;&#31361;&#26174;&#20102;&#36825;&#19968;&#28857;&#65292;&#20108;&#32773;&#26377;&#25928;&#22320;&#24357;&#21512;&#20102;&#24050;&#35265;&#25968;&#25454;&#21644;&#26410;&#35265;&#25968;&#25454;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#22312;&#22270;&#23398;&#20064;&#39046;&#22495;&#65292;&#26032;&#22270;&#30340;&#19981;&#26029;&#28044;&#29616;&#21644;&#20154;&#31867;&#26631;&#27880;&#30340;&#25361;&#25112;&#20063;&#21152;&#21095;&#20102;&#38646;&#23556;&#20987;&#36801;&#31227;&#23398;&#20064;&#30340;&#24517;&#35201;&#24615;&#65292;&#25512;&#21160;&#20102;&#25506;&#32034;&#33021;&#22815;&#22312;&#19981;&#38656;&#35201;&#29305;&#23450;&#25968;&#25454;&#38598;&#21644;&#26631;&#31614;&#29305;&#23450;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#27867;&#21270;&#36328;&#22810;&#26679;&#22270;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;ZeroG&#65292;&#19968;&#20010;&#26088;&#22312;&#23454;&#29616;&#36328;&#25968;&#25454;&#38598;&#27867;&#21270;&#30340;&#26032;&#26694;&#26550;&#65292;&#23558;&#36825;&#26679;&#30340;&#33539;&#20363;&#25193;&#23637;&#21040;&#20102;&#22270;&#20013;&#30340;&#38646;&#23556;&#20987;&#36801;&#31227;&#24615;&#12290;&#35299;&#20915;&#35832;&#22914;&#29305;&#24449;&#19981;&#23545;&#40784;&#12289;&#19981;&#21305;&#37197;&#30340;&#26631;&#31614;&#31354;&#38388;&#21644;&#36127;&#36801;&#31227;&#31561;&#22266;&#26377;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11235v1 Announce Type: new  Abstract: With the development of foundation models such as large language models, zero-shot transfer learning has become increasingly significant. This is highlighted by the generative capabilities of NLP models like GPT-4, and the retrieval-based approaches of CV models like CLIP, both of which effectively bridge the gap between seen and unseen data. In the realm of graph learning, the continuous emergence of new graphs and the challenges of human labeling also amplify the necessity for zero-shot transfer learning, driving the exploration of approaches that can generalize across diverse graph data without necessitating dataset-specific and label-specific fine-tuning. In this study, we extend such paradigms to zero-shot transferability in graphs by introducing ZeroG, a new framework tailored to enable cross-dataset generalization. Addressing the inherent challenges such as feature misalignment, mismatched label spaces, and negative transfer, we l
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#33258;&#36866;&#24212;&#20998;&#21106;&#24179;&#34913;&#26862;&#26519;&#65288;ASBF&#65289;&#65292;&#21487;&#22312;&#23398;&#20064;&#26641;&#34920;&#31034;&#30340;&#21516;&#26102;&#65292;&#22312;&#22797;&#26434;&#24773;&#20917;&#19979;&#23454;&#29616;&#26497;&#23567;&#26497;&#20248;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26412;&#22320;&#21270;&#29256;&#26412;&#65292;&#22312;H\"older&#31867;&#19979;&#36798;&#21040;&#26368;&#23567;&#26497;&#20248;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.11228</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#20998;&#21106;&#24179;&#34913;&#20248;&#21270;&#38543;&#26426;&#26862;&#26519;
&lt;/p&gt;
&lt;p&gt;
Adaptive Split Balancing for Optimal Random Forest
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11228
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#33258;&#36866;&#24212;&#20998;&#21106;&#24179;&#34913;&#26862;&#26519;&#65288;ASBF&#65289;&#65292;&#21487;&#22312;&#23398;&#20064;&#26641;&#34920;&#31034;&#30340;&#21516;&#26102;&#65292;&#22312;&#22797;&#26434;&#24773;&#20917;&#19979;&#23454;&#29616;&#26497;&#23567;&#26497;&#20248;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26412;&#22320;&#21270;&#29256;&#26412;&#65292;&#22312;H\"older&#31867;&#19979;&#36798;&#21040;&#26368;&#23567;&#26497;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#38543;&#26426;&#26862;&#26519;&#36890;&#24120;&#29992;&#20110;&#22238;&#24402;&#38382;&#39064;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#22312;&#22797;&#26434;&#24773;&#20917;&#19979;&#32570;&#20047;&#36866;&#24212;&#24615;&#65292;&#25110;&#22312;&#31616;&#21333;&#12289;&#24179;&#28369;&#24773;&#26223;&#19979;&#22833;&#21435;&#26368;&#20248;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#33258;&#36866;&#24212;&#20998;&#21106;&#24179;&#34913;&#26862;&#26519;&#65288;ASBF&#65289;&#65292;&#33021;&#22815;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#26641;&#34920;&#31034;&#65292;&#21516;&#26102;&#22312;Lipschitz&#31867;&#19979;&#23454;&#29616;&#26497;&#23567;&#26497;&#20248;&#24615;&#12290;&#20026;&#20102;&#21033;&#29992;&#26356;&#39640;&#38454;&#30340;&#24179;&#28369;&#24615;&#27700;&#24179;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;&#26412;&#22320;&#21270;&#29256;&#26412;&#65292;&#35813;&#29256;&#26412;&#22312;&#20219;&#24847;$q \in \mathbb{N}$&#21644;$\beta \in (0,1]$&#30340;H&#246;lder&#31867;$\mathcal{H}^{q,\beta}$&#19979;&#36798;&#21040;&#26368;&#23567;&#26497;&#20248;&#24615;&#12290;&#19982;&#24191;&#27867;&#20351;&#29992;&#30340;&#38543;&#26426;&#29305;&#24449;&#36873;&#25321;&#19981;&#21516;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#23545;&#29616;&#26377;&#26041;&#27861;&#30340;&#24179;&#34913;&#20462;&#25913;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36807;&#24230;&#20381;&#36182;&#36741;&#21161;&#38543;&#26426;&#24615;&#21487;&#33021;&#20250;&#25439;&#23475;&#26641;&#27169;&#22411;&#30340;&#36924;&#36817;&#33021;&#21147;&#65292;&#23548;&#33268;&#27425;&#20248;&#32467;&#26524;&#12290;&#30456;&#21453;&#65292;&#19968;&#20010;&#26356;&#24179;&#34913;&#12289;&#26356;&#23569;&#38543;&#26426;&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11228v1 Announce Type: cross  Abstract: While random forests are commonly used for regression problems, existing methods often lack adaptability in complex situations or lose optimality under simple, smooth scenarios. In this study, we introduce the adaptive split balancing forest (ASBF), capable of learning tree representations from data while simultaneously achieving minimax optimality under the Lipschitz class. To exploit higher-order smoothness levels, we further propose a localized version that attains the minimax rate under the H\"older class $\mathcal{H}^{q,\beta}$ for any $q\in\mathbb{N}$ and $\beta\in(0,1]$. Rather than relying on the widely-used random feature selection, we consider a balanced modification to existing approaches. Our results indicate that an over-reliance on auxiliary randomness may compromise the approximation power of tree models, leading to suboptimal results. Conversely, a less random, more balanced approach demonstrates optimality. Additionall
&lt;/p&gt;</description></item><item><title>&#22351;&#28436;&#21592;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#20266;&#35013;&#26679;&#26412;&#32469;&#36807;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#65292;&#30740;&#31350;&#25506;&#35752;&#20102;&#25968;&#23383;&#31614;&#21517;&#21644;&#26426;&#22120;&#23398;&#20064;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#36890;&#36807;&#30456;&#20284;&#24615;&#21644;&#32858;&#31867;&#26469;&#25913;&#36827;&#23433;&#20840;&#35299;&#20915;&#26041;&#26696;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.11227</link><description>&lt;p&gt;
&#20851;&#20110;&#30456;&#20284;&#24615;&#22312;&#26816;&#27979;&#20266;&#35013;&#25991;&#20214;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
On the Role of Similarity in Detecting Masquerading Files
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11227
&lt;/p&gt;
&lt;p&gt;
&#22351;&#28436;&#21592;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#20266;&#35013;&#26679;&#26412;&#32469;&#36807;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#65292;&#30740;&#31350;&#25506;&#35752;&#20102;&#25968;&#23383;&#31614;&#21517;&#21644;&#26426;&#22120;&#23398;&#20064;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#36890;&#36807;&#30456;&#20284;&#24615;&#21644;&#32858;&#31867;&#26469;&#25913;&#36827;&#23433;&#20840;&#35299;&#20915;&#26041;&#26696;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#20284;&#24615;&#24050;&#32463;&#24212;&#29992;&#20110;&#24191;&#27867;&#30340;&#23433;&#20840;&#24212;&#29992;&#39046;&#22495;&#65292;&#36890;&#24120;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#30001;&#22351;&#28436;&#21592;&#21046;&#20316;&#30340;&#20266;&#35013;&#26679;&#26412;&#25152;&#25552;&#20986;&#30340;&#38382;&#39064;; &#36825;&#20123;&#26679;&#26412;&#34987;&#21046;&#20316;&#24471;&#19982;&#21512;&#27861;&#26679;&#26412;&#30456;&#20284;&#25110;&#25509;&#36817;&#30456;&#21516;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#26679;&#26412;&#21487;&#33021;&#20250;&#32473;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#24102;&#26469;&#37325;&#22823;&#38382;&#39064;&#12290;&#20027;&#35201;&#38382;&#39064;&#22312;&#20110;&#65292;&#22351;&#28436;&#21592;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#20266;&#35013;&#26679;&#26412;&#32469;&#36807;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#25968;&#23383;&#31614;&#21517;&#21644;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#21487;&#25191;&#34892;&#25991;&#20214;&#21644;&#20195;&#30721;&#31614;&#21517;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20266;&#35013;&#25991;&#20214;&#30340;&#20998;&#31867;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#30456;&#20284;&#24615;&#21644;&#32858;&#31867;&#30340;&#32452;&#21512;&#26469;&#26597;&#25214;&#20266;&#35013;&#25991;&#20214;&#12290;&#25105;&#20204;&#21033;&#29992;&#22312;&#36825;&#20010;&#36807;&#31243;&#20013;&#25910;&#38598;&#21040;&#30340;&#35265;&#35299;&#26469;&#25913;&#36827;&#22522;&#20110;&#30456;&#20284;&#24615;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#23433;&#20840;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11227v1 Announce Type: cross  Abstract: Similarity has been applied to a wide range of security applications, typically used in machine learning models. We examine the problem posed by masquerading samples; that is samples crafted by bad actors to be similar or near identical to legitimate samples. We find that these samples potentially create significant problems for machine learning solutions. The primary problem being that bad actors can circumvent machine learning solutions by using masquerading samples.   We then examine the interplay between digital signatures and machine learning solutions. In particular, we focus on executable files and code signing. We offer a taxonomy for masquerading files. We use a combination of similarity and clustering to find masquerading files. We use the insights gathered in this process to offer improvements to similarity based and machine learning security solutions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#39033;&#24335;&#36924;&#36817;&#20316;&#20026;&#19968;&#31181;&#29420;&#31435;&#23545;&#35937;&#30340;&#30740;&#31350;&#65292;&#21457;&#29616;PANN&#23545;&#26576;&#20123;&#31867;&#22411;&#30340;&#36924;&#36817;&#35823;&#24046;...</title><link>https://arxiv.org/abs/2402.11224</link><description>&lt;p&gt;
&#20855;&#26377;&#65288;&#20302;&#31934;&#24230;&#65289;&#22810;&#39033;&#24335;&#36924;&#36817;&#30340;&#31070;&#32463;&#32593;&#32476;&#65306;&#20934;&#30830;&#24615;&#25552;&#39640;&#30340;&#26032;&#35265;&#35299;&#21644;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Neural Networks with (Low-Precision) Polynomial Approximations: New Insights and Techniques for Accuracy Improvement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11224
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#39033;&#24335;&#36924;&#36817;&#20316;&#20026;&#19968;&#31181;&#29420;&#31435;&#23545;&#35937;&#30340;&#30740;&#31350;&#65292;&#21457;&#29616;PANN&#23545;&#26576;&#20123;&#31867;&#22411;&#30340;&#36924;&#36817;&#35823;&#24046;...
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#29992;&#22810;&#39033;&#24335;&#36924;&#36817;&#26367;&#25442;&#38750;&#22810;&#39033;&#24335;&#20989;&#25968;&#65288;&#20363;&#22914;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#65292;&#22914;ReLU&#65289;&#26159;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#26631;&#20934;&#20570;&#27861;&#12290;&#26412;&#25991;&#20013;&#31216;&#20043;&#20026;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#39033;&#24335;&#36924;&#36817;&#65288;PANN&#65289;&#30340;&#32467;&#26524;&#31070;&#32463;&#32593;&#32476;&#19982;&#20808;&#36827;&#30340;&#23494;&#30721;&#31995;&#32479;&#20860;&#23481;&#65292;&#23454;&#29616;&#38544;&#31169;&#20445;&#25252;&#27169;&#22411;&#25512;&#26029;&#12290;&#21033;&#29992;&#8220;&#39640;&#31934;&#24230;&#8221;&#36924;&#36817;&#65292;&#26368;&#20808;&#36827;&#30340;PANN&#25552;&#20379;&#20102;&#19982;&#22522;&#30784;&#39592;&#24178;&#27169;&#22411;&#30456;&#20284;&#30340;&#25512;&#26029;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#36924;&#36817;&#30340;&#24433;&#21709;&#30693;&#20043;&#29978;&#23569;&#65292;&#24182;&#19988;&#29616;&#26377;&#25991;&#29486;&#36890;&#24120;&#26159;&#36890;&#36807;&#23454;&#35777;&#30830;&#23450;&#25152;&#38656;&#30340;&#36924;&#36817;&#31934;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#22987;&#23545;PANN&#36827;&#34892;&#29420;&#31435;&#23545;&#35937;&#30340;&#30740;&#31350;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#36129;&#29486;&#26159;&#21452;&#37325;&#30340;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35299;&#37322;&#20102;PANN&#20013;&#36817;&#20284;&#35823;&#24046;&#30340;&#24433;&#21709;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;PANN&#23481;&#26131;&#21463;&#21040;&#26576;&#31181;&#31867;&#22411;&#30340;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11224v1 Announce Type: new  Abstract: Replacing non-polynomial functions (e.g., non-linear activation functions such as ReLU) in a neural network with their polynomial approximations is a standard practice in privacy-preserving machine learning. The resulting neural network, called polynomial approximation of neural network (PANN) in this paper, is compatible with advanced cryptosystems to enable privacy-preserving model inference. Using ``highly precise'' approximation, state-of-the-art PANN offers similar inference accuracy as the underlying backbone model. However, little is known about the effect of approximation, and existing literature often determined the required approximation precision empirically. In this paper, we initiate the investigation of PANN as a standalone object. Specifically, our contribution is two-fold. Firstly, we provide an explanation on the effect of approximate error in PANN. In particular, we discovered that (1) PANN is susceptible to some type o
&lt;/p&gt;</description></item><item><title>HEAL&#26159;&#19968;&#31181;&#19987;&#20026;HDC&#20998;&#31867;&#37327;&#36523;&#23450;&#21046;&#30340;&#26032;&#22411;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#21644;&#22810;&#26679;&#24615;&#24341;&#23548;&#30340;&#33719;&#21462;&#20027;&#21160;&#20026;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#28857;&#36827;&#34892;&#27880;&#37322;&#65292;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#25968;&#25454;&#38598;&#27880;&#37322;&#21644;&#38477;&#20302;&#21171;&#21160;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.11223</link><description>&lt;p&gt;
HEAL&#65306;&#21551;&#21457;&#20110;&#22823;&#33041;&#30340;&#39640;&#32500;&#24230;&#39640;&#25928;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
HEAL: Brain-inspired Hyperdimensional Efficient Active Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11223
&lt;/p&gt;
&lt;p&gt;
HEAL&#26159;&#19968;&#31181;&#19987;&#20026;HDC&#20998;&#31867;&#37327;&#36523;&#23450;&#21046;&#30340;&#26032;&#22411;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#21644;&#22810;&#26679;&#24615;&#24341;&#23548;&#30340;&#33719;&#21462;&#20027;&#21160;&#20026;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#28857;&#36827;&#34892;&#27880;&#37322;&#65292;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#25968;&#25454;&#38598;&#27880;&#37322;&#21644;&#38477;&#20302;&#21171;&#21160;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20154;&#31867;&#22823;&#33041;&#20986;&#33394;&#30340;&#23398;&#20064;&#33021;&#21147;&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#36229;&#39640;&#32500;&#24230;&#35745;&#31639;&#65288;HDC&#65289;&#20316;&#20026;&#19968;&#31181;&#26032;&#39062;&#30340;&#35745;&#31639;&#33539;&#24335;&#20986;&#29616;&#65292;&#21033;&#29992;&#39640;&#32500;&#21521;&#37327;&#34920;&#31034;&#21644;&#25805;&#20316;&#36827;&#34892;&#31867;&#20284;&#22823;&#33041;&#30340;&#36731;&#37327;&#32423;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#12290;HDC&#30340;&#23454;&#38469;&#37096;&#32626;&#26174;&#33879;&#25552;&#39640;&#20102;&#23398;&#20064;&#25928;&#29575;&#65292;&#19982;&#24403;&#21069;&#30340;&#28145;&#24230;ML&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#24191;&#27867;&#30340;&#24212;&#29992;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#25552;&#39640;HDC&#20998;&#31867;&#22120;&#22312;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#25928;&#29575;&#20173;&#28982;&#26159;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Hyperdimensional Efficient Active Learning&#65288;HEAL&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#19987;&#20026;HDC&#20998;&#31867;&#37327;&#36523;&#23450;&#21046;&#30340;&#26032;&#22411;&#20027;&#21160;&#23398;&#20064;&#65288;AL&#65289;&#26694;&#26550;&#12290; HEAL&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#21644;&#22810;&#26679;&#24615;&#24341;&#23548;&#30340;&#33719;&#21462;&#20027;&#21160;&#20026;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#28857;&#36827;&#34892;&#27880;&#37322;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#25968;&#25454;&#38598;&#27880;&#37322;&#21644;&#38477;&#20302;&#21171;&#21160;&#25104;&#26412;&#12290;&#19982;&#20165;&#25903;&#25345;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#31867;&#22120;&#30340;&#20256;&#32479;AL&#26041;&#27861;&#19981;&#21516;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11223v1 Announce Type: new  Abstract: Drawing inspiration from the outstanding learning capability of our human brains, Hyperdimensional Computing (HDC) emerges as a novel computing paradigm, and it leverages high-dimensional vector presentation and operations for brain-like lightweight Machine Learning (ML). Practical deployments of HDC have significantly enhanced the learning efficiency compared to current deep ML methods on a broad spectrum of applications. However, boosting the data efficiency of HDC classifiers in supervised learning remains an open question. In this paper, we introduce Hyperdimensional Efficient Active Learning (HEAL), a novel Active Learning (AL) framework tailored for HDC classification. HEAL proactively annotates unlabeled data points via uncertainty and diversity-guided acquisition, leading to a more efficient dataset annotation and lowering labor costs. Unlike conventional AL methods that only support classifiers built upon deep neural networks (D
&lt;/p&gt;</description></item><item><title>AdAdaGrad&#21644;AdAdaGradNorm&#26159;&#19968;&#20010;&#33258;&#36866;&#24212;&#22686;&#21152;&#25209;&#22823;&#23567;&#30340;&#26041;&#27861;&#65292;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#24341;&#20837;&#20102;&#33258;&#36866;&#24212;&#25209;&#22823;&#23567;&#31574;&#30053;&#65292;&#35777;&#26126;AdaGradNorm&#20197;&#39640;&#27010;&#29575;&#22312;$O(1/K)$&#36895;&#24230;&#19979;&#25910;&#25947;&#12290;</title><link>https://arxiv.org/abs/2402.11215</link><description>&lt;p&gt;
AdAdaGrad&#65306;&#33258;&#36866;&#24212;&#26799;&#24230;&#26041;&#27861;&#30340;&#33258;&#36866;&#24212;&#25209;&#22823;&#23567;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
AdAdaGrad: Adaptive Batch Size Schemes for Adaptive Gradient Methods
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11215
&lt;/p&gt;
&lt;p&gt;
AdAdaGrad&#21644;AdAdaGradNorm&#26159;&#19968;&#20010;&#33258;&#36866;&#24212;&#22686;&#21152;&#25209;&#22823;&#23567;&#30340;&#26041;&#27861;&#65292;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#24341;&#20837;&#20102;&#33258;&#36866;&#24212;&#25209;&#22823;&#23567;&#31574;&#30053;&#65292;&#35777;&#26126;AdaGradNorm&#20197;&#39640;&#27010;&#29575;&#22312;$O(1/K)$&#36895;&#24230;&#19979;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#26799;&#24230;&#20248;&#21270;&#22120;&#20013;&#25209;&#37327;&#22823;&#23567;&#30340;&#36873;&#25321;&#23545;&#27169;&#22411;&#35757;&#32451;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21464;&#21270;&#25209;&#22823;&#23567;&#30340;&#23454;&#36341;&#30456;&#23545;&#20854;&#20182;&#36229;&#21442;&#25968;&#36739;&#23569;&#25506;&#35752;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20174;&#33258;&#36866;&#24212;&#37319;&#26679;&#26041;&#27861;&#20013;&#23548;&#20986;&#30340;&#33258;&#36866;&#24212;&#25209;&#22823;&#23567;&#31574;&#30053;&#65292;&#20256;&#32479;&#19978;&#20165;&#24212;&#29992;&#20110;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#12290;&#32771;&#34385;&#21040;&#23398;&#20064;&#36895;&#29575;&#21644;&#25209;&#22823;&#23567;&#20043;&#38388;&#30340;&#26174;&#33879;&#30456;&#20114;&#20316;&#29992;&#65292;&#20197;&#21450;&#33258;&#36866;&#24212;&#26799;&#24230;&#26041;&#27861;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#26222;&#21450;&#65292;&#25105;&#20204;&#24378;&#35843;&#22312;&#36825;&#20123;&#24773;&#22659;&#20013;&#38656;&#35201;&#33258;&#36866;&#24212;&#25209;&#22823;&#23567;&#31574;&#30053;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;AdAdaGrad&#21450;&#20854;&#26631;&#37327;&#21464;&#20307;AdAdaGradNorm&#65292;&#23427;&#20204;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36880;&#28176;&#22686;&#21152;&#25209;&#22823;&#23567;&#65292;&#21516;&#26102;&#20351;&#29992;AdaGrad&#21644;AdaGradNorm&#36827;&#34892;&#27169;&#22411;&#26356;&#26032;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;AdaGradNorm&#20197;&#39640;&#27010;&#29575;&#20197;$O(1/K)$&#30340;&#36895;&#24230;&#25910;&#25947;&#65292;&#29992;&#20110;&#25214;&#21040;&#20809;&#28369;&#38750;&#20984;&#20989;&#25968;&#30340;&#19968;&#38454;&#31283;&#23450;&#28857;&#22312;$K$&#27425;&#36845;&#20195;&#20869;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11215v1 Announce Type: new  Abstract: The choice of batch sizes in stochastic gradient optimizers is critical for model training. However, the practice of varying batch sizes throughout the training process is less explored compared to other hyperparameters. We investigate adaptive batch size strategies derived from adaptive sampling methods, traditionally applied only in stochastic gradient descent. Given the significant interplay between learning rates and batch sizes, and considering the prevalence of adaptive gradient methods in deep learning, we emphasize the need for adaptive batch size strategies in these contexts. We introduce AdAdaGrad and its scalar variant AdAdaGradNorm, which incrementally increase batch sizes during training, while model updates are performed using AdaGrad and AdaGradNorm. We prove that AdaGradNorm converges with high probability at a rate of $\mathscr{O}(1/K)$ for finding a first-order stationary point of smooth nonconvex functions within $K$ i
&lt;/p&gt;</description></item><item><title>ChatGPT&#20316;&#20026;&#20449;&#24687;&#26816;&#32034;&#39046;&#22495;&#30340;&#20851;&#38190;&#25216;&#26415;&#65292;&#19981;&#26029;&#25361;&#25112;&#20256;&#32479;&#33539;&#24335;&#65292;&#24102;&#26469;&#20102;&#26032;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#65292;&#21516;&#26102;&#36229;&#36234;&#20102;&#20043;&#21069;&#30340;GPT-3&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.11203</link><description>&lt;p&gt;
&#25506;&#32034;ChatGPT&#22312;&#19979;&#19968;&#20195;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#24212;&#29992;&#65306;&#26426;&#36935;&#19982;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Exploring ChatGPT for Next-generation Information Retrieval: Opportunities and Challenges
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11203
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#20316;&#20026;&#20449;&#24687;&#26816;&#32034;&#39046;&#22495;&#30340;&#20851;&#38190;&#25216;&#26415;&#65292;&#19981;&#26029;&#25361;&#25112;&#20256;&#32479;&#33539;&#24335;&#65292;&#24102;&#26469;&#20102;&#26032;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#65292;&#21516;&#26102;&#36229;&#36234;&#20102;&#20043;&#21069;&#30340;GPT-3&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#20984;&#26174;&#20102;ChatGPT&#20316;&#20026;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#39046;&#22495;&#20013;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#19982;&#20043;&#21069;&#30340;&#27169;&#22411;&#19981;&#21516;&#65292;ChatGPT&#25552;&#20379;&#20102;&#26174;&#33879;&#30340;&#22909;&#22788;&#65292;&#21560;&#24341;&#20102;&#34892;&#19994;&#21644;&#23398;&#26415;&#30028;&#30340;&#20851;&#27880;&#12290;&#19968;&#20123;&#20154;&#35748;&#20026;ChatGPT&#26159;&#19968;&#39033;&#24320;&#21019;&#24615;&#30340;&#21019;&#26032;&#65292;&#32780;&#21478;&#19968;&#20123;&#20154;&#23558;&#20854;&#25104;&#21151;&#24402;&#22240;&#20110;&#20135;&#21697;&#24320;&#21457;&#21644;&#24066;&#22330;&#31574;&#30053;&#30340;&#26377;&#25928;&#25972;&#21512;&#12290;ChatGPT&#30340;&#20986;&#29616;&#65292;&#20197;&#21450;&#19982;OpenAI&#30340;GPT-4&#19968;&#36215;&#65292;&#26631;&#24535;&#30528;&#29983;&#25104;&#24335;AI&#30340;&#26032;&#38454;&#27573;&#65292;&#20135;&#29983;&#30340;&#20869;&#23481;&#19982;&#35757;&#32451;&#26679;&#26412;&#26377;&#25152;&#19981;&#21516;&#65292;&#24182;&#36229;&#36234;&#20102;&#20197;&#24448;&#30340;GPT-3&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#19982;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#30340;&#20256;&#32479;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#19981;&#21516;&#65292;ChatGPT&#25361;&#25112;&#20102;&#29616;&#26377;&#30340;&#33539;&#24335;&#65292;&#24102;&#26469;&#20102;&#20851;&#20110;&#25991;&#26412;&#36136;&#37327;&#20445;&#35777;&#12289;&#27169;&#22411;&#20559;&#24046;&#21644;&#25928;&#29575;&#26041;&#38754;&#30340;&#26032;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;ChatGPT&#23545;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20379;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11203v1 Announce Type: cross  Abstract: The rapid advancement of artificial intelligence (AI) has highlighted ChatGPT as a pivotal technology in the field of information retrieval (IR). Distinguished from its predecessors, ChatGPT offers significant benefits that have attracted the attention of both the industry and academic communities. While some view ChatGPT as a groundbreaking innovation, others attribute its success to the effective integration of product development and market strategies. The emergence of ChatGPT, alongside GPT-4, marks a new phase in Generative AI, generating content that is distinct from training examples and exceeding the capabilities of the prior GPT-3 model by OpenAI. Unlike the traditional supervised learning approach in IR tasks, ChatGPT challenges existing paradigms, bringing forth new challenges and opportunities regarding text quality assurance, model bias, and efficiency. This paper seeks to examine the impact of ChatGPT on IR tasks and offe
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;(AFL)&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#20855;&#26377;&#24322;&#26500;&#23458;&#25143;&#31471;&#30340;&#32852;&#37030;&#23398;&#20064;&#20013;&#21516;&#27493;&#38382;&#39064;&#65292;&#23454;&#29616;&#32447;&#24615;&#21152;&#36895;&#12290;</title><link>https://arxiv.org/abs/2402.11198</link><description>&lt;p&gt;
&#22312;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;&#20013;&#23454;&#29616;&#20855;&#26377;&#24322;&#26500;&#23458;&#25143;&#31471;&#30340;&#32447;&#24615;&#21152;&#36895;&#24230;
&lt;/p&gt;
&lt;p&gt;
Achieving Linear Speedup in Asynchronous Federated Learning with Heterogeneous Clients
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11198
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;(AFL)&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#20855;&#26377;&#24322;&#26500;&#23458;&#25143;&#31471;&#30340;&#32852;&#37030;&#23398;&#20064;&#20013;&#21516;&#27493;&#38382;&#39064;&#65292;&#23454;&#29616;&#32447;&#24615;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;(FL)&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#20998;&#24067;&#24335;&#35757;&#32451;&#33539;&#24335;&#65292;&#26088;&#22312;&#23398;&#20064;&#19968;&#20010;&#36890;&#29992;&#30340;&#20840;&#23616;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#20132;&#25442;&#25110;&#20256;&#36755;&#23384;&#20648;&#22312;&#19981;&#21516;&#23458;&#25143;&#31471;&#26412;&#22320;&#30340;&#25968;&#25454;&#12290;&#22522;&#20110;Federated Averaging (FedAvg)&#30340;&#31639;&#27861;&#22312;FL&#20013;&#22791;&#21463;&#27426;&#36814;&#65292;&#20197;&#20943;&#23569;&#36890;&#20449;&#24320;&#38144;&#65292;&#20854;&#20013;&#27599;&#20010;&#23458;&#25143;&#31471;&#22312;&#19982;&#20013;&#22830;&#26381;&#21153;&#22120;&#36890;&#20449;&#20043;&#21069;&#36827;&#34892;&#22810;&#20010;&#26412;&#22320;&#21270;&#36845;&#20195;&#12290;&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#20102;&#20855;&#26377;&#19981;&#21516;&#35745;&#31639;&#21644;/&#25110;&#36890;&#20449;&#33021;&#21147;&#30340;&#23458;&#25143;&#31471;&#30340;FL&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;FedAvg&#21487;&#33021;&#19981;&#37027;&#20040;&#39640;&#25928;&#65292;&#22240;&#20026;&#23427;&#35201;&#27714;&#21442;&#19982;&#20840;&#23616;&#32858;&#21512;&#30340;&#25152;&#26377;&#23458;&#25143;&#31471;&#22312;&#19968;&#20010;&#22238;&#21512;&#20013;&#20174;&#26368;&#26032;&#30340;&#20840;&#23616;&#27169;&#22411;&#24320;&#22987;&#36845;&#20195;&#65292;&#22240;&#27492;&#24555;&#36895;&#23458;&#25143;&#31471;&#21644;&#28382;&#21518;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#21516;&#27493;&#20250;&#20005;&#37325;&#25302;&#24930;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#39640;&#25928;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;(AFL)&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11198v1 Announce Type: new  Abstract: Federated learning (FL) is an emerging distributed training paradigm that aims to learn a common global model without exchanging or transferring the data that are stored locally at different clients. The Federated Averaging (FedAvg)-based algorithms have gained substantial popularity in FL to reduce the communication overhead, where each client conducts multiple localized iterations before communicating with a central server. In this paper, we focus on FL where the clients have diverse computation and/or communication capabilities. Under this circumstance, FedAvg can be less efficient since it requires all clients that participate in the global aggregation in a round to initiate iterations from the latest global model, and thus the synchronization among fast clients and straggler clients can severely slow down the overall training process. To address this issue, we propose an efficient asynchronous federated learning (AFL) framework call
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21452;&#26799;&#24230;&#25237;&#24433;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#26799;&#24230;&#25237;&#24433;&#21040;&#20004;&#20010;&#20851;&#38190;&#23376;&#31354;&#38388;&#26469;&#23454;&#29616;&#25345;&#32493;&#40065;&#26834;&#23398;&#20064;&#65292;&#26377;&#25928;&#22320;&#32500;&#25345;&#20102;&#31070;&#32463;&#32593;&#32476;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.11196</link><description>&lt;p&gt;
&#22312;&#36830;&#32493;&#23398;&#20064;&#20013;&#20445;&#25345;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Maintaining Adversarial Robustness in Continuous Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11196
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21452;&#26799;&#24230;&#25237;&#24433;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#26799;&#24230;&#25237;&#24433;&#21040;&#20004;&#20010;&#20851;&#38190;&#23376;&#31354;&#38388;&#26469;&#23454;&#29616;&#25345;&#32493;&#40065;&#26834;&#23398;&#20064;&#65292;&#26377;&#25928;&#22320;&#32500;&#25345;&#20102;&#31070;&#32463;&#32593;&#32476;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#38752;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#22797;&#26434;&#30340;&#38450;&#24481;&#31639;&#27861;&#33719;&#24471;&#30340;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#22312;&#31070;&#32463;&#32593;&#32476;&#19981;&#26029;&#28436;&#21270;&#20197;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#24456;&#23481;&#26131;&#34987;&#25273;&#21435;&#12290;&#36825;&#31181;&#33030;&#24369;&#24615;&#21487;&#20197;&#36890;&#36807;&#22521;&#20859;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#33021;&#21147;&#26469;&#35299;&#20915;&#65292;&#31216;&#20026;&#25345;&#32493;&#40065;&#26834;&#23398;&#20064;&#65292;&#23427;&#22312;&#36830;&#32493;&#23398;&#20064;&#36807;&#31243;&#20013;&#20851;&#27880;&#21069;&#26399;&#20219;&#21153;&#30340;(&#20998;&#31867;)&#24615;&#33021;&#21644;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#12290;&#20026;&#20102;&#23454;&#29616;&#25345;&#32493;&#40065;&#26834;&#23398;&#20064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21452;&#26799;&#24230;&#25237;&#24433;&#30340;&#26041;&#27861;&#65292;&#23558;&#29992;&#20110;&#26435;&#37325;&#26356;&#26032;&#30340;&#26799;&#24230;&#27491;&#20132;&#25237;&#24433;&#21040;&#20004;&#20010;&#20851;&#38190;&#23376;&#31354;&#38388;&#19978; -- &#19968;&#20010;&#29992;&#20110;&#31283;&#23450;&#24179;&#28369;&#26679;&#26412;&#26799;&#24230;&#65292;&#21478;&#19968;&#20010;&#29992;&#20110;&#31283;&#23450;&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#32456;&#36755;&#20986;&#12290;&#22312;&#22235;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#32500;&#25345;&#20102;&#23545;&#24378;&#23545;&#25239;&#24615;&#30340;&#25345;&#32493;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11196v1 Announce Type: cross  Abstract: Adversarial robustness is essential for security and reliability of machine learning systems. However, the adversarial robustness gained by sophisticated defense algorithms is easily erased as the neural network evolves to learn new tasks. This vulnerability can be addressed by fostering a novel capability for neural networks, termed continual robust learning, which focuses on both the (classification) performance and adversarial robustness on previous tasks during continuous learning. To achieve continuous robust learning, we propose an approach called Double Gradient Projection that projects the gradients for weight updates orthogonally onto two crucial subspaces -- one for stabilizing the smoothed sample gradients and another for stabilizing the final outputs of the neural network. The experimental results on four benchmarks demonstrate that the proposed approach effectively maintains continuous robustness against strong adversarial
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#23567;&#30417;&#30563;&#33258;&#32452;&#32455;&#26144;&#23556;&#26041;&#27861;&#65292;&#21033;&#29992;&#26368;&#36817;&#37051;&#22810;&#25968;&#25237;&#31080;&#26469;&#36827;&#34892;&#31867;&#21035;&#20272;&#35745;&#65292;&#22312;&#39134;&#34892;&#38454;&#27573;&#35782;&#21035;&#26041;&#38754;&#34920;&#29616;&#20986;&#24456;&#22909;&#30340;&#25928;&#26524;&#65292;&#24182;&#19988;&#23545;&#31867;&#21035;&#19981;&#24179;&#34913;&#26356;&#21152;&#31283;&#20581;</title><link>https://arxiv.org/abs/2402.11185</link><description>&lt;p&gt;
&#33258;&#32452;&#32455;&#26144;&#23556;&#30340;&#26368;&#23567;&#30417;&#30563;&#25299;&#25169;&#25237;&#24433;&#29992;&#20110;&#39134;&#34892;&#38454;&#27573;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Minimally Supervised Topological Projections of Self-Organizing Maps for Phase of Flight Identification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11185
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#23567;&#30417;&#30563;&#33258;&#32452;&#32455;&#26144;&#23556;&#26041;&#27861;&#65292;&#21033;&#29992;&#26368;&#36817;&#37051;&#22810;&#25968;&#25237;&#31080;&#26469;&#36827;&#34892;&#31867;&#21035;&#20272;&#35745;&#65292;&#22312;&#39134;&#34892;&#38454;&#27573;&#35782;&#21035;&#26041;&#38754;&#34920;&#29616;&#20986;&#24456;&#22909;&#30340;&#25928;&#26524;&#65292;&#24182;&#19988;&#23545;&#31867;&#21035;&#19981;&#24179;&#34913;&#26356;&#21152;&#31283;&#20581;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36890;&#29992;&#33322;&#31354;&#39046;&#22495;&#65292;&#35782;&#21035;&#39134;&#34892;&#38454;&#27573;&#26159;&#37325;&#35201;&#30340;&#65292;&#22240;&#20026;&#20102;&#35299;&#20174;&#39134;&#34892;&#22120;&#39134;&#34892;&#25968;&#25454;&#35760;&#24405;&#22120;&#25910;&#38598;&#30340;&#25968;&#25454;&#22788;&#20110;&#21738;&#20010;&#39134;&#34892;&#38454;&#27573;&#65292;&#21487;&#20197;&#24110;&#21161;&#26356;&#26377;&#25928;&#22320;&#26816;&#27979;&#23433;&#20840;&#25110;&#21361;&#38505;&#20107;&#20214;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#30340;&#26368;&#23567;&#30417;&#30563;&#33258;&#32452;&#32455;&#26144;&#23556;&#65288;MS-SOMs&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;SOM U-&#30697;&#38453;&#20013;&#30340;&#26368;&#36817;&#37051;&#22810;&#25968;&#25237;&#31080;&#36827;&#34892;&#31867;&#20272;&#35745;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#36798;&#21040;&#25110;&#36229;&#36807;&#20351;&#29992;&#23436;&#25972;&#26631;&#35760;&#25968;&#25454;&#25991;&#20214;&#30340;&#26420;&#32032;SOM&#26041;&#27861;&#65292;&#27599;&#20010;&#31867;&#21035;&#20165;&#38656;&#35201;30&#20010;&#26631;&#35760;&#25968;&#25454;&#28857;&#12290;&#27492;&#22806;&#65292;&#26368;&#23567;&#30417;&#30563;SOM&#23545;&#31867;&#21035;&#19981;&#24179;&#34913;&#26356;&#20855;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11185v1 Announce Type: new  Abstract: Identifying phases of flight is important in the field of general aviation, as knowing which phase of flight data is collected from aircraft flight data recorders can aid in the more effective detection of safety or hazardous events. General aviation flight data for phase of flight identification is usually per-second data, comes on a large scale, and is class imbalanced. It is expensive to manually label the data and training classification models usually faces class imbalance problems. This work investigates the use of a novel method for minimally supervised self-organizing maps (MS-SOMs) which utilize nearest neighbor majority votes in the SOM U-matrix for class estimation. Results show that the proposed method can reach or exceed a naive SOM approach which utilized a full data file of labeled data, with only 30 labeled datapoints per class. Additionally, the minimally supervised SOM is significantly more robust to the class imbalance
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20351;&#29992;&#21704;&#23494;&#39039;&#33945;&#29305;&#21345;&#27931;&#21644;&#26031;&#22374;&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;&#31561;&#26041;&#27861;&#65292;&#23545;&#24314;&#27169;&#22797;&#26434;&#26102;&#31354;&#36807;&#31243;&#30340;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21442;&#25968;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#29305;&#21035;&#26159;&#24212;&#29992;&#20110;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#28436;&#21270;&#36807;&#31243;&#65292;&#23637;&#31034;&#20102;...</title><link>https://arxiv.org/abs/2402.11179</link><description>&lt;p&gt;
&#28436;&#21270;&#36807;&#31243;&#30340;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Uncertainty Quantification of Graph Convolution Neural Network Models of Evolving Processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11179
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20351;&#29992;&#21704;&#23494;&#39039;&#33945;&#29305;&#21345;&#27931;&#21644;&#26031;&#22374;&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;&#31561;&#26041;&#27861;&#65292;&#23545;&#24314;&#27169;&#22797;&#26434;&#26102;&#31354;&#36807;&#31243;&#30340;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21442;&#25968;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#29305;&#21035;&#26159;&#24212;&#29992;&#20110;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#28436;&#21270;&#36807;&#31243;&#65292;&#23637;&#31034;&#20102;...
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#19981;&#26029;&#22686;&#21152;&#12290;&#29305;&#21035;&#26159;&#65292;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#24050;&#34987;&#35777;&#26126;&#22312;&#24314;&#27169;&#20855;&#26377;&#26102;&#31354;&#22797;&#26434;&#24615;&#30340;&#36807;&#31243;&#26041;&#38754;&#38750;&#24120;&#25797;&#38271;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#39640;&#24230;&#21442;&#25968;&#21270;&#30340;&#27169;&#22411;&#22312;&#33021;&#22815;&#22312;&#24863;&#20852;&#36259;&#30340;&#21306;&#22495;&#20869;&#20135;&#29983;&#20855;&#26377;&#37327;&#21270;&#35823;&#24046;&#30028;&#38480;&#30340;&#36755;&#20986;&#26041;&#38754;&#24341;&#36215;&#20102;&#24576;&#30097;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#25214;&#21040;&#36866;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;&#20351;&#29992;&#21704;&#23494;&#39039;&#33945;&#29305;&#21345;&#27931;&#21644;&#26031;&#22374;&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;&#20197;&#21450;&#20854;&#25237;&#24433;&#21464;&#20307;&#23545;&#24314;&#27169;&#22797;&#26434;&#26102;&#31354;&#36807;&#31243;&#30340;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21442;&#25968;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#27604;&#36739;&#36827;&#34892;&#20102;&#23637;&#31034;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#26041;&#27861;&#24212;&#29992;&#20110;&#20351;&#29992;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#21644;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#26550;&#26500;&#24314;&#27169;&#30340;&#28436;&#21270;&#31995;&#32479;&#30340;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;S
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11179v1 Announce Type: new  Abstract: The application of neural network models to scientific machine learning tasks has proliferated in recent years. In particular, neural network models have proved to be adept at modeling processes with spatial-temporal complexity. Nevertheless, these highly parameterized models have garnered skepticism in their ability to produce outputs with quantified error bounds over the regimes of interest. Hence there is a need to find uncertainty quantification methods that are suitable for neural networks. In this work we present comparisons of the parametric uncertainty quantification of neural networks modeling complex spatial-temporal processes with Hamiltonian Monte Carlo and Stein variational gradient descent and its projected variant. Specifically we apply these methods to graph convolutional neural network models of evolving systems modeled with recurrent neural network and neural ordinary differential equations architectures. We show that S
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#35774;&#35745;&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#30340;&#31616;&#21333;&#28789;&#27963;&#26694;&#26550;&#65292;&#29992;&#20110;&#23547;&#25214;&#38750;&#20984;&#25439;&#22833;&#20989;&#25968;&#30340;&#36817;&#20284;&#31283;&#23450;&#28857;&#65292;&#24182;&#33719;&#24471;&#20102;&#25913;&#36827;&#21644;&#26377;&#26102;&#26159;&#26368;&#20248;&#30340;&#36895;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.11173</link><description>&lt;p&gt;
&#22914;&#20309;&#22312;&#38544;&#31169;&#26465;&#20214;&#19979;&#20351;&#26799;&#24230;&#21464;&#24471;&#26356;&#23567;&#65306;&#25913;&#36827;&#30340;&#24046;&#20998;&#38544;&#31169;&#38750;&#20984;&#20248;&#21270;&#36895;&#29575;
&lt;/p&gt;
&lt;p&gt;
How to Make the Gradients Small Privately: Improved Rates for Differentially Private Non-Convex Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11173
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#35774;&#35745;&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#30340;&#31616;&#21333;&#28789;&#27963;&#26694;&#26550;&#65292;&#29992;&#20110;&#23547;&#25214;&#38750;&#20984;&#25439;&#22833;&#20989;&#25968;&#30340;&#36817;&#20284;&#31283;&#23450;&#28857;&#65292;&#24182;&#33719;&#24471;&#20102;&#25913;&#36827;&#21644;&#26377;&#26102;&#26159;&#26368;&#20248;&#30340;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21333;&#28789;&#27963;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35774;&#35745;&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#65292;&#20197;&#25214;&#21040;&#38750;&#20984;&#25439;&#22833;&#20989;&#25968;&#30340;&#36817;&#20284;&#31283;&#23450;&#28857;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#22522;&#20110;&#20351;&#29992;&#31169;&#26377;&#30340;&#36817;&#20284;&#39118;&#38505;&#26368;&#23567;&#21270;&#22120;&#26469;&#8220;&#28909;&#21551;&#21160;&#8221;&#21478;&#19968;&#20010;&#29992;&#20110;&#23547;&#25214;&#31283;&#23450;&#28857;&#30340;&#31169;&#26377;&#31639;&#27861;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#26694;&#26550;&#26469;&#33719;&#24471;&#23545;&#20960;&#31867;&#38750;&#20984;&#25439;&#22833;&#20989;&#25968;&#30340;&#25913;&#36827;&#29978;&#33267;&#26159;&#26368;&#20248;&#36895;&#29575;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#23547;&#25214;&#24179;&#28369;&#38750;&#20984;&#32463;&#39564;&#25439;&#22833;&#20989;&#25968;&#31283;&#23450;&#28857;&#30340;&#36895;&#29575;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#19987;&#38376;&#38024;&#23545;&#22840;&#33832;-&#20984;&#20989;&#25968;&#65292;&#36825;&#31181;&#20989;&#25968;&#27010;&#25324;&#20102;&#26143;-&#20984;&#20989;&#25968;&#65292;&#24182;&#22312;&#23398;&#20064;&#21160;&#24577;&#31995;&#32479;&#21644;&#35757;&#32451;&#19968;&#20123;&#31070;&#32463;&#32593;&#32476;&#26102;&#20986;&#29616;&#12290;&#25105;&#20204;&#20026;&#36825;&#20010;&#31867;&#21035;&#23454;&#29616;&#20102;&#26368;&#20248;&#36895;&#29575;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#23545;&#28385;&#36275;Kurdyka-Lojasiewicz&#65288;KL&#65289;&#26465;&#20214;&#30340;&#20989;&#25968;&#23547;&#25214;&#31283;&#23450;&#28857;&#30340;&#26368;&#20248;&#31639;&#27861;&#12290;&#20363;&#22914;&#65292;&#36229;&#21442;&#25968;&#21270;&#31070;&#32463;&#32593;&#32476;&#32463;&#24120;&#28385;&#36275;&#36825;&#20010;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11173v1 Announce Type: new  Abstract: We provide a simple and flexible framework for designing differentially private algorithms to find approximate stationary points of non-convex loss functions. Our framework is based on using a private approximate risk minimizer to "warm start" another private algorithm for finding stationary points. We use this framework to obtain improved, and sometimes optimal, rates for several classes of non-convex loss functions. First, we obtain improved rates for finding stationary points of smooth non-convex empirical loss functions. Second, we specialize to quasar-convex functions, which generalize star-convex functions and arise in learning dynamical systems and training some neural nets. We achieve the optimal rate for this class. Third, we give an optimal algorithm for finding stationary points of functions satisfying the Kurdyka-Lojasiewicz (KL) condition. For example, over-parameterized neural networks often satisfy this condition. Fourth, 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#40657;&#30418;&#27010;&#29575;&#35748;&#35777;&#35299;&#37322;&#30340;&#20449;&#20219;&#21306;&#22495;&#33021;&#22815;&#26377;&#25928;&#22320;&#27934;&#23519;&#27169;&#22411;&#34892;&#20026;&#12289;&#20445;&#35777;&#35299;&#37322;&#30340;&#31283;&#23450;&#24615;&#65292;&#24182;&#23454;&#29616;&#35299;&#37322;&#30340;&#37325;&#29992;</title><link>https://arxiv.org/abs/2402.11168</link><description>&lt;p&gt;
&#22522;&#20110;&#20449;&#20219;&#21306;&#22495;&#30340;&#40657;&#30418;&#27010;&#29575;&#35748;&#35777;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Trust Regions for Explanations via Black-Box Probabilistic Certification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11168
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#40657;&#30418;&#27010;&#29575;&#35748;&#35777;&#35299;&#37322;&#30340;&#20449;&#20219;&#21306;&#22495;&#33021;&#22815;&#26377;&#25928;&#22320;&#27934;&#23519;&#27169;&#22411;&#34892;&#20026;&#12289;&#20445;&#35777;&#35299;&#37322;&#30340;&#31283;&#23450;&#24615;&#65292;&#24182;&#23454;&#29616;&#35299;&#37322;&#30340;&#37325;&#29992;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#40657;&#30418;&#24615;&#36136;&#65292;&#20154;&#20204;&#24320;&#21457;&#20102;&#22823;&#37327;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#26469;&#35299;&#26512;&#20010;&#21035;&#20915;&#31574;&#32972;&#21518;&#30340;&#22240;&#32032;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#40657;&#30418;&#65288;&#27010;&#29575;&#24615;&#65289;&#35299;&#37322;&#35748;&#35777;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#32473;&#23450;&#19968;&#20010;&#40657;&#30418;&#27169;&#22411;&#65292;&#21482;&#26377;&#26597;&#35810;&#35775;&#38382;&#26435;&#65292;&#19968;&#20010;&#31034;&#20363;&#30340;&#35299;&#37322;&#20197;&#21450;&#19968;&#20010;&#36136;&#37327;&#24230;&#37327;&#65288;&#22914;&#36924;&#30495;&#24230;&#12289;&#31283;&#23450;&#24615;&#65289;&#65292;&#25105;&#20204;&#26159;&#21542;&#33021;&#25214;&#21040;&#26368;&#22823;&#30340;&#36229;&#31435;&#26041;&#20307;&#65288;&#21363; $\ell_{\infty}$ &#29699;&#65289;&#65292;&#20197;&#31034;&#20363;&#20026;&#20013;&#24515;&#65292;&#20351;&#24471;&#24403;&#35299;&#37322;&#34987;&#24212;&#29992;&#20110;&#36229;&#31435;&#26041;&#20307;&#20869;&#30340;&#25152;&#26377;&#31034;&#20363;&#26102;&#65288;&#39640;&#27010;&#29575;&#19979;&#65289;&#36136;&#37327;&#26631;&#20934;&#24471;&#21040;&#28385;&#36275;&#65288;&#27604;&#22914;&#36924;&#30495;&#24230;&#39640;&#20110;&#26576;&#20010;&#20540;&#65289;&#65311;&#33021;&#22815;&#39640;&#25928;&#22320;&#25214;&#21040;&#36825;&#26679;&#19968;&#20010;&#20449;&#20219;&#21306;&#22495;&#26377;&#22810;&#37325;&#22909;&#22788;&#65306;i&#65289;&#27934;&#23519;&#27169;&#22411;&#22312;&#19968;&#20010;&#21306;&#22495;&#20869;&#30340;&#34892;&#20026;&#65292;&#20855;&#26377;&#20445;&#35777;&#65307;ii&#65289;&#35299;&#37322;&#30340;&#31283;&#23450;&#24615;&#24471;&#21040;&#20445;&#35777;&#65307;iii&#65289;&#35299;&#37322;&#30340;&#37325;&#29992;&#65292;&#21487;&#20197;&#33410;&#30465;&#26102;&#38388;&#12289;&#31934;&#21147;&#21644;&#37329;&#38065;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11168v1 Announce Type: cross  Abstract: Given the black box nature of machine learning models, a plethora of explainability methods have been developed to decipher the factors behind individual decisions. In this paper, we introduce a novel problem of black box (probabilistic) explanation certification. We ask the question: Given a black box model with only query access, an explanation for an example and a quality metric (viz. fidelity, stability), can we find the largest hypercube (i.e., $\ell_{\infty}$ ball) centered at the example such that when the explanation is applied to all examples within the hypercube, (with high probability) a quality criterion is met (viz. fidelity greater than some value)? Being able to efficiently find such a \emph{trust region} has multiple benefits: i) insight into model behavior in a \emph{region}, with a \emph{guarantee}; ii) ascertained \emph{stability} of the explanation; iii) \emph{explanation reuse}, which can save time, energy and mone
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#22411;&#20302;&#31209;&#30697;&#38453;&#20272;&#35745;&#26041;&#27861;LowPopArt&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#37327;B(Q)&#25552;&#20379;&#26356;&#32039;&#23494;&#30340;&#24674;&#22797;&#20445;&#35777;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23454;&#39564;&#35774;&#35745;&#26631;&#20934;&#65292;&#20197;&#21450;&#20004;&#31181;&#36866;&#29992;&#20110;&#19968;&#33324;Arm&#38598;&#30340;&#20302;&#31209;&#32447;&#24615;&#36172;&#21338;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.11156</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#20302;&#31209;&#30697;&#38453;&#20272;&#35745;&#12289;&#23454;&#39564;&#35774;&#35745;&#21644;&#22522;&#20110;Arm&#38598;&#30340;&#20302;&#31209;&#36172;&#21338;&#26426;
&lt;/p&gt;
&lt;p&gt;
Efficient Low-Rank Matrix Estimation, Experimental Design, and Arm-Set-Dependent Low-Rank Bandits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11156
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#22411;&#20302;&#31209;&#30697;&#38453;&#20272;&#35745;&#26041;&#27861;LowPopArt&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#37327;B(Q)&#25552;&#20379;&#26356;&#32039;&#23494;&#30340;&#24674;&#22797;&#20445;&#35777;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23454;&#39564;&#35774;&#35745;&#26631;&#20934;&#65292;&#20197;&#21450;&#20004;&#31181;&#36866;&#29992;&#20110;&#19968;&#33324;Arm&#38598;&#30340;&#20302;&#31209;&#32447;&#24615;&#36172;&#21338;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20302;&#31209;&#30697;&#38453;&#36857;&#22238;&#24402;&#21644;&#30456;&#20851;&#30340;&#20302;&#31209;&#30697;&#38453;&#36172;&#21338;&#38382;&#39064;&#12290;&#20551;&#35774;&#21487;&#20197;&#35775;&#38382;&#21327;&#21464;&#37327;&#30340;&#20998;&#24067;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LowPopArt&#30340;&#26032;&#22411;&#20302;&#31209;&#30697;&#38453;&#20272;&#35745;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#20854;&#20381;&#36182;&#20110;&#19968;&#20010;&#26032;&#39062;&#25968;&#37327;B(Q)&#30340;&#24674;&#22797;&#20445;&#35777;&#65292;&#35813;&#25968;&#37327;&#34920;&#24449;&#20102;&#38382;&#39064;&#30340;&#38590;&#24230;&#65292;&#20854;&#20013;Q&#26159;&#27979;&#37327;&#20998;&#24067;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20960;&#20010;&#38382;&#39064;&#20013;&#21487;&#20197;&#25552;&#20379;&#27604;&#32463;&#20856;&#30340;&#26680;&#33539;&#25968;&#24809;&#32602;&#26368;&#23567;&#20108;&#20056;&#27861;&#65288;Koltchinskii&#31561;&#20154;&#65292;2011&#65289;&#26356;&#32039;&#23494;&#30340;&#24674;&#22797;&#20445;&#35777;&#12290;&#20026;&#20102;&#22312;&#20174;&#20219;&#24847;&#32473;&#23450;&#30340;&#27979;&#37327;&#38598;&#21512;A&#20013;&#36827;&#34892;&#26377;&#38480;&#27979;&#37327;&#30340;&#24773;&#20917;&#19979;&#25191;&#34892;&#39640;&#25928;&#20272;&#35745;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23454;&#39564;&#35774;&#35745;&#26631;&#20934;&#65292;&#35813;&#26631;&#20934;&#20197;&#35745;&#31639;&#25928;&#29575;&#26368;&#23567;&#21270;B(Q)&#12290;&#25105;&#20204;&#21033;&#29992;&#25105;&#20204;&#30340;&#26032;&#39062;&#20272;&#35745;&#22120;&#21644;&#23454;&#39564;&#35774;&#35745;&#25512;&#23548;&#20102;&#20004;&#31181;&#36866;&#29992;&#20110;&#19968;&#33324;Arm&#38598;&#30340;&#20302;&#31209;&#32447;&#24615;&#36172;&#21338;&#31639;&#27861;&#65292;&#20854;&#20139;&#26377;&#25913;&#36827;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11156v1 Announce Type: cross  Abstract: We study low-rank matrix trace regression and the related problem of low-rank matrix bandits. Assuming access to the distribution of the covariates, we propose a novel low-rank matrix estimation method called LowPopArt and provide its recovery guarantee that depends on a novel quantity denoted by B(Q) that characterizes the hardness of the problem, where Q is the covariance matrix of the measurement distribution. We show that our method can provide tighter recovery guarantees than classical nuclear norm penalized least squares (Koltchinskii et al., 2011) in several problems. To perform efficient estimation with a limited number of measurements from an arbitrarily given measurement set A, we also propose a novel experimental design criterion that minimizes B(Q) with computational efficiency. We leverage our novel estimator and design of experiments to derive two low-rank linear bandit algorithms for general arm sets that enjoy improved 
&lt;/p&gt;</description></item><item><title>&#35813;&#35843;&#26597;&#32508;&#36848;&#20102;&#22270;&#20687;&#19978;&#30340;&#20998;&#24067;&#36866;&#24212;&#38382;&#39064;&#65292;&#25552;&#20379;&#26368;&#26032;&#30340;&#22270;OOD&#36866;&#24212;&#26041;&#27861;&#30340;&#22238;&#39038;&#65292;&#35206;&#30422;&#20102;&#35757;&#32451;&#26102;&#21644;&#27979;&#35797;&#26102;&#30340;&#20004;&#31181;&#20027;&#35201;&#38382;&#39064;&#22330;&#26223;&#65292;&#24182;&#25552;&#20986;&#20102;&#20998;&#31867;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.11153</link><description>&lt;p&gt;
&#36229;&#36234;&#27867;&#21270;&#65306;&#22270;&#19978;&#30340;&#20998;&#24067;&#36866;&#24212;&#24615;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Beyond Generalization: A Survey of Out-Of-Distribution Adaptation on Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11153
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35843;&#26597;&#32508;&#36848;&#20102;&#22270;&#20687;&#19978;&#30340;&#20998;&#24067;&#36866;&#24212;&#38382;&#39064;&#65292;&#25552;&#20379;&#26368;&#26032;&#30340;&#22270;OOD&#36866;&#24212;&#26041;&#27861;&#30340;&#22238;&#39038;&#65292;&#35206;&#30422;&#20102;&#35757;&#32451;&#26102;&#21644;&#27979;&#35797;&#26102;&#30340;&#20004;&#31181;&#20027;&#35201;&#38382;&#39064;&#22330;&#26223;&#65292;&#24182;&#25552;&#20986;&#20102;&#20998;&#31867;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22270;&#19978;&#30340;&#20998;&#24067;&#36716;&#31227;&#8212;&#8212;&#21363;&#35757;&#32451;&#21644;&#27979;&#35797;&#22270;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20043;&#38388;&#30340;&#25968;&#25454;&#20998;&#24067;&#24046;&#24322;&#65292;&#24448;&#24448;&#26222;&#36941;&#19988;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#19981;&#21487;&#36991;&#20813;&#12290;&#36825;&#31181;&#36716;&#31227;&#21487;&#33021;&#20005;&#37325;&#24694;&#21270;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#32473;&#21487;&#38752;&#30340;&#22270;&#26426;&#22120;&#23398;&#20064;&#24102;&#26469;&#37325;&#22823;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#22270;&#22270;&#20687;&#30340;Out-Of-Distribution&#65288;OOD&#65289;&#36866;&#24212;&#26041;&#27861;&#30340;&#30740;&#31350;&#28608;&#22686;&#65292;&#20854;&#26088;&#22312;&#20943;&#36731;&#20998;&#24067;&#36716;&#31227;&#65292;&#24182;&#23558;&#19968;&#20010;&#20998;&#24067;&#30340;&#30693;&#35782;&#36866;&#24212;&#21040;&#21478;&#19968;&#20010;&#20998;&#24067;&#12290; &#22312;&#25105;&#20204;&#30340;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22270;OOD&#36866;&#24212;&#26041;&#27861;&#30340;&#26368;&#26032;&#21644;&#21069;&#30651;&#24615;&#22238;&#39038;&#65292;&#28085;&#30422;&#20004;&#20010;&#20027;&#35201;&#38382;&#39064;&#22330;&#26223;&#65292;&#21253;&#25324;&#35757;&#32451;&#26102;&#21644;&#27979;&#35797;&#26102;&#30340;&#22270;OOD&#36866;&#24212;&#12290; &#25105;&#20204;&#39318;&#20808;&#27491;&#24335;&#25551;&#36848;&#20102;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#28982;&#21518;&#35752;&#35770;&#20102;&#22270;&#19978;&#30340;&#19981;&#21516;&#31867;&#22411;&#30340;&#20998;&#24067;&#36716;&#31227;&#12290;&#22522;&#20110;&#25105;&#20204;&#23545;&#22270;OOD&#36866;&#24212;&#30340;&#25552;&#20986;&#30340;&#20998;&#31867;&#27861;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#23545;&#29616;&#26377;&#30340;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11153v1 Announce Type: new  Abstract: Distribution shifts on graphs -- the data distribution discrepancies between training and testing a graph machine learning model, are often ubiquitous and unavoidable in real-world scenarios. Such shifts may severely deteriorate the performance of the model, posing significant challenges for reliable graph machine learning. Consequently, there has been a surge in research on graph Out-Of-Distribution (OOD) adaptation methods that aim to mitigate the distribution shifts and adapt the knowledge from one distribution to another. In our survey, we provide an up-to-date and forward-looking review of graph OOD adaptation methods, covering two main problem scenarios including training-time as well as test-time graph OOD adaptation. We start by formally formulating the two problems and then discuss different types of distribution shifts on graphs. Based on our proposed taxonomy for graph OOD adaptation, we systematically categorize the existing 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25918;&#24323;&#23398;&#29983;&#31471;&#30340;&#28201;&#24230;&#32553;&#25918;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#36716;&#25442;&#25945;&#24072;&#21305;&#37197;&#65288;TTM&#65289;&#30340;&#30693;&#35782;&#33976;&#39311;&#21464;&#20307;&#65292;&#36890;&#36807;&#23545;&#28201;&#24230;&#32553;&#25918;&#30340;&#37325;&#26032;&#35299;&#37322;&#65292;TTM&#22312;&#30446;&#26631;&#20989;&#25968;&#20013;&#24341;&#20837;&#20102;&#22266;&#26377;&#30340;R&#233;nyi&#29109;&#39033;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#23398;&#29983;&#27867;&#21270;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.11148</link><description>&lt;p&gt;
&#22522;&#20110;&#36716;&#25442;&#25945;&#24072;&#21305;&#37197;&#30340;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Knowledge Distillation Based on Transformed Teacher Matching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11148
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25918;&#24323;&#23398;&#29983;&#31471;&#30340;&#28201;&#24230;&#32553;&#25918;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#36716;&#25442;&#25945;&#24072;&#21305;&#37197;&#65288;TTM&#65289;&#30340;&#30693;&#35782;&#33976;&#39311;&#21464;&#20307;&#65292;&#36890;&#36807;&#23545;&#28201;&#24230;&#32553;&#25918;&#30340;&#37325;&#26032;&#35299;&#37322;&#65292;TTM&#22312;&#30446;&#26631;&#20989;&#25968;&#20013;&#24341;&#20837;&#20102;&#22266;&#26377;&#30340;R&#233;nyi&#29109;&#39033;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#23398;&#29983;&#27867;&#21270;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#36830;&#25509;&#36923;&#36753;&#21305;&#37197;&#21644;&#27010;&#29575;&#20998;&#24067;&#21305;&#37197;&#30340;&#25216;&#26415;&#65292;&#28201;&#24230;&#32553;&#25918;&#22312;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#20256;&#32479;&#19978;&#65292;&#22312;KD&#20013;&#65292;&#28201;&#24230;&#32553;&#25918;&#34987;&#24212;&#29992;&#20110;&#25945;&#24072;&#30340;logits&#21644;&#23398;&#29983;&#30340;logits&#12290;&#21463;&#19968;&#20123;&#26368;&#36817;&#30340;&#30740;&#31350;&#21551;&#21457;&#65292;&#26412;&#25991;&#25918;&#24323;&#20102;&#22312;&#23398;&#29983;&#31471;&#30340;&#28201;&#24230;&#32553;&#25918;&#65292;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#30001;&#27492;&#20135;&#29983;&#30340;KD&#21464;&#20307;&#65292;&#31216;&#20026;&#36716;&#25442;&#25945;&#24072;&#21305;&#37197;&#65288;TTM&#65289;&#12290;&#36890;&#36807;&#37325;&#26032;&#35299;&#37322;&#28201;&#24230;&#32553;&#25918;&#20316;&#20026;&#27010;&#29575;&#20998;&#24067;&#30340;&#24130;&#21464;&#25442;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#19982;&#21407;&#22987;&#30340;KD&#30456;&#27604;&#65292;TTM&#22312;&#20854;&#30446;&#26631;&#20989;&#25968;&#20013;&#20855;&#26377;&#22266;&#26377;&#30340;R&#233;nyi&#29109;&#39033;&#65292;&#36825;&#20805;&#24403;&#20102;&#39069;&#22806;&#30340;&#27491;&#21017;&#21270;&#39033;&#12290;&#22823;&#37327;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30001;&#20110;&#36825;&#31181;&#22266;&#26377;&#30340;&#27491;&#21017;&#21270;&#65292;TTM&#23548;&#33268;&#35757;&#32451;&#33391;&#22909;&#30340;&#23398;&#29983;&#27604;&#21407;&#22987;KD&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11148v1 Announce Type: new  Abstract: As a technique to bridge logit matching and probability distribution matching, temperature scaling plays a pivotal role in knowledge distillation (KD). Conventionally, temperature scaling is applied to both teacher's logits and student's logits in KD. Motivated by some recent works, in this paper, we drop instead temperature scaling on the student side, and systematically study the resulting variant of KD, dubbed transformed teacher matching (TTM). By reinterpreting temperature scaling as a power transform of probability distribution, we show that in comparison with the original KD, TTM has an inherent R\'enyi entropy term in its objective function, which serves as an extra regularization term. Extensive experiment results demonstrate that thanks to this inherent regularization, TTM leads to trained students with better generalization than the original KD. To further enhance student's capability to match teacher's power transformed proba
&lt;/p&gt;</description></item><item><title>Providence&#26159;&#19968;&#20010;&#22522;&#20110;&#35270;&#35273;&#32534;&#31243;&#30340;&#22810;&#27169;&#24335;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#65292;&#26088;&#22312;&#24110;&#21161;&#19987;&#23478;&#25429;&#33719;&#20154;&#31867;&#34892;&#20026;&#32447;&#32034;&#65292;&#26080;&#38656;&#32534;&#20889;&#20195;&#30721;&#65292;&#20855;&#26377;&#21487;&#21462;&#30340;&#21487;&#29992;&#24615;&#21644;&#20196;&#20154;&#28385;&#24847;&#30340;&#36755;&#20986;&#12290;</title><link>https://arxiv.org/abs/2402.11145</link><description>&lt;p&gt;
&#20026;&#20154;&#31867;&#34892;&#20026;&#20998;&#26512;&#23545;&#35805;&#35270;&#39057;&#25903;&#25345;&#19987;&#23478;&#30340;&#22810;&#27169;&#24335;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Supporting Experts with a Multimodal Machine-Learning-Based Tool for Human Behavior Analysis of Conversational Videos
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11145
&lt;/p&gt;
&lt;p&gt;
Providence&#26159;&#19968;&#20010;&#22522;&#20110;&#35270;&#35273;&#32534;&#31243;&#30340;&#22810;&#27169;&#24335;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#65292;&#26088;&#22312;&#24110;&#21161;&#19987;&#23478;&#25429;&#33719;&#20154;&#31867;&#34892;&#20026;&#32447;&#32034;&#65292;&#26080;&#38656;&#32534;&#20889;&#20195;&#30721;&#65292;&#20855;&#26377;&#21487;&#21462;&#30340;&#21487;&#29992;&#24615;&#21644;&#20196;&#20154;&#28385;&#24847;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#23398;&#31185;&#23545;&#35805;&#30340;&#22810;&#27169;&#24335;&#22330;&#26223;&#25628;&#32034;&#23545;&#35299;&#38145;&#26377;&#20215;&#20540;&#30340;&#31038;&#20250;&#21160;&#24577;&#35265;&#35299;&#21644;&#22686;&#24378;&#27807;&#36890;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;Providence&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#35270;&#35273;&#32534;&#31243;&#30340;&#24037;&#20855;&#65292;&#26088;&#22312;&#25552;&#20379;&#20174;&#19987;&#23478;&#30740;&#31350;&#20013;&#24471;&#20986;&#30340;&#35774;&#35745;&#32771;&#34385;&#65292;&#20351;&#19987;&#23478;&#33021;&#22815;&#32467;&#21512;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#25429;&#33719;&#20154;&#31867;&#34892;&#20026;&#32447;&#32034;&#65292;&#32780;&#26080;&#38656;&#32534;&#20889;&#20195;&#30721;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#35813;&#24037;&#20855;&#20855;&#26377;&#21487;&#21462;&#30340;&#21487;&#29992;&#24615;&#21644;&#20196;&#20154;&#28385;&#24847;&#30340;&#36755;&#20986;&#65292;&#25191;&#34892;&#23545;&#35805;&#22330;&#26223;&#25628;&#32034;&#20219;&#21153;&#26102;&#25152;&#26045;&#21152;&#30340;&#35748;&#30693;&#36127;&#33655;&#36739;&#23567;&#65292;&#39564;&#35777;&#20102;&#20854;&#21487;&#23450;&#21046;&#24615;&#21644;&#36879;&#26126;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11145v1 Announce Type: cross  Abstract: Multimodal scene search of conversations is essential for unlocking valuable insights into social dynamics and enhancing our communication. While experts in conversational analysis have their own knowledge and skills to find key scenes, a lack of comprehensive, user-friendly tools that streamline the processing of diverse multimodal queries impedes efficiency and objectivity. To solve it, we developed Providence, a visual-programming-based tool based on design considerations derived from a formative study with experts. It enables experts to combine various machine learning algorithms to capture human behavioral cues without writing code. Our study showed its preferable usability and satisfactory output with less cognitive load imposed in accomplishing scene search tasks of conversations, verifying the importance of its customizability and transparency. Furthermore, through the in-the-wild trial, we confirmed the objectivity and reusabi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#39046;&#33521;&#19978;&#24320;&#21457;&#21644;&#37096;&#32626;&#30340;LiGNN&#26694;&#26550;&#65292;&#21253;&#25324;&#23545;GNN&#34920;&#31034;&#23398;&#20064;&#30340;&#31639;&#27861;&#25913;&#36827;&#21644;&#22823;&#35268;&#27169;&#35757;&#32451;&#20248;&#21270;&#65292;&#20026;&#24037;&#20316;&#30003;&#35831;&#22238;&#22797;&#29575;&#12289;&#24191;&#21578;&#28857;&#20987;&#29575;&#21644;Feed&#27599;&#26085;&#27963;&#36291;&#29992;&#25143;&#25552;&#39640;&#24102;&#26469;&#20102;&#32422;1%-2%&#30340;&#30456;&#23545;&#25913;&#21892;&#12290;</title><link>https://arxiv.org/abs/2402.11139</link><description>&lt;p&gt;
LiGNN: &#39046;&#33521;&#19978;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
LiGNN: Graph Neural Networks at LinkedIn
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11139
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#39046;&#33521;&#19978;&#24320;&#21457;&#21644;&#37096;&#32626;&#30340;LiGNN&#26694;&#26550;&#65292;&#21253;&#25324;&#23545;GNN&#34920;&#31034;&#23398;&#20064;&#30340;&#31639;&#27861;&#25913;&#36827;&#21644;&#22823;&#35268;&#27169;&#35757;&#32451;&#20248;&#21270;&#65292;&#20026;&#24037;&#20316;&#30003;&#35831;&#22238;&#22797;&#29575;&#12289;&#24191;&#21578;&#28857;&#20987;&#29575;&#21644;Feed&#27599;&#26085;&#27963;&#36291;&#29992;&#25143;&#25552;&#39640;&#24102;&#26469;&#20102;&#32422;1%-2%&#30340;&#30456;&#23545;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LiGNN&#65292;&#19968;&#31181;&#24050;&#37096;&#32626;&#30340;&#22823;&#35268;&#27169;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26694;&#26550;&#12290;&#25105;&#20204;&#20998;&#20139;&#20102;&#22312;&#39046;&#33521;&#19978;&#24320;&#21457;&#21644;&#37096;&#32626;GNNs&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#32452;&#31639;&#27861;&#25913;&#36827;&#65292;&#21253;&#25324;&#20855;&#26377;&#38271;&#26399;&#25439;&#22833;&#30340;&#26102;&#38388;&#22270;&#26550;&#26500;&#65292;&#36890;&#36807;&#22270;&#23494;&#38598;&#21270;&#23454;&#29616;&#30340;&#26377;&#25928;&#20919;&#21551;&#21160;&#35299;&#20915;&#26041;&#26696;&#65292;ID&#23884;&#20837;&#21644;&#22810;&#36339;&#37051;&#23621;&#37319;&#26679;&#65292;&#20197;&#25913;&#36827;GNN&#34920;&#31034;&#23398;&#20064;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#35299;&#37322;&#20102;&#22914;&#20309;&#36890;&#36807;&#37051;&#23621;&#30340;&#33258;&#36866;&#24212;&#37319;&#26679;&#65292;&#23545;&#35757;&#32451;&#25968;&#25454;&#25209;&#27425;&#36827;&#34892;&#20998;&#32452;&#21644;&#20999;&#29255;&#65292;&#19987;&#38376;&#30340;&#20849;&#20139;&#20869;&#23384;&#38431;&#21015;&#21644;&#26412;&#22320;&#26799;&#24230;&#20248;&#21270;&#23558;LinkedIn&#22270;&#30340;&#22823;&#35268;&#27169;&#35757;&#32451;&#21152;&#24555;7&#20493;&#12290;&#25105;&#20204;&#24635;&#32467;&#20102;&#20174;A/B&#27979;&#35797;&#23454;&#39564;&#20013;&#33719;&#24471;&#30340;&#37096;&#32626;&#32463;&#39564;&#21644;&#25945;&#35757;&#12290;&#26412;&#25991;&#20171;&#32461;&#30340;&#25216;&#26415;&#24050;&#32463;&#20026;&#24037;&#20316;&#30003;&#35831;&#22238;&#22797;&#29575;&#30340;&#30456;&#23545;&#25913;&#21892;&#29575;&#32422;&#20026;1&#65285;&#65292;&#24191;&#21578;&#28857;&#20987;&#29575;&#25552;&#21319;2&#65285;&#65292;Feed&#27599;&#26085;&#27963;&#36291;&#29992;&#25143;&#25552;&#39640;0.5&#65285;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11139v1 Announce Type: cross  Abstract: In this paper, we present LiGNN, a deployed large-scale Graph Neural Networks (GNNs) Framework. We share our insight on developing and deployment of GNNs at large scale at LinkedIn. We present a set of algorithmic improvements to the quality of GNN representation learning including temporal graph architectures with long term losses, effective cold start solutions via graph densification, ID embeddings and multi-hop neighbor sampling. We explain how we built and sped up by 7x our large-scale training on LinkedIn graphs with adaptive sampling of neighbors, grouping and slicing of training data batches, specialized shared-memory queue and local gradient optimization. We summarize our deployment lessons and learnings gathered from A/B test experiments. The techniques presented in this work have contributed to an approximate relative improvements of 1% of Job application hearing back rate, 2% Ads CTR lift, 0.5% of Feed engaged daily active 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#23545;&#27604;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#30456;&#20284;&#24615;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#26410;&#30693;&#20219;&#21153;&#25351;&#20196;&#30340;&#31283;&#20581;&#24615;</title><link>https://arxiv.org/abs/2402.11138</link><description>&lt;p&gt;
&#23545;&#27604;&#25351;&#20196;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Contrastive Instruction Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11138
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#23545;&#27604;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#30456;&#20284;&#24615;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#26410;&#30693;&#20219;&#21153;&#25351;&#20196;&#30340;&#31283;&#20581;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#19968;&#30452;&#34987;&#29992;&#20316;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#26410;&#30693;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;LLMs&#22312;&#38754;&#20020;&#26410;&#30693;&#25351;&#20196;&#26102;&#34920;&#29616;&#20986;&#26377;&#38480;&#30340;&#31283;&#20581;&#24615;&#65292;&#24403;&#30456;&#21516;&#30340;&#25351;&#20196;&#20197;&#31245;&#24494;&#21464;&#21270;&#30340;&#24418;&#24335;&#25110;&#35821;&#35328;&#39118;&#26684;&#25552;&#20986;&#26102;&#20250;&#20135;&#29983;&#19981;&#19968;&#33268;&#30340;&#36755;&#20986;&#12290;&#36825;&#31181;&#34892;&#20026;&#34920;&#26126;LLMs&#23545;&#25991;&#26412;&#21464;&#21270;&#30340;&#31283;&#20581;&#24615;&#21644;&#23545;&#26410;&#30693;&#25351;&#20196;&#30340;&#27867;&#21270;&#33021;&#21147;&#19981;&#36275;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#21487;&#20449;&#24230;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#27604;&#25351;&#20196;&#35843;&#25972;&#65292;&#35813;&#26041;&#27861;&#22312;&#26368;&#22823;&#21270;&#35821;&#20041;&#19978;&#31561;&#20215;&#30340;&#25351;&#20196;-&#23454;&#20363;&#23545;&#30340;&#38544;&#34255;&#34920;&#31034;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#30340;&#21516;&#26102;&#65292;&#26368;&#23567;&#21270;&#35821;&#20041;&#19978;&#19981;&#21516;&#30340;&#23545;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#20026;&#20102;&#20419;&#36827;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#36890;&#36807;&#37322;&#20041;&#20219;&#21153;&#25351;&#20196;&#65292;&#25193;&#20805;&#29616;&#26377;&#30340;FLAN&#38598;&#21512;&#12290;&#22312;PromptBench&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#23545;&#27604;&#25351;&#20196;&#35843;&#25972;&#65288;CoIN&#65289;&#19968;&#30452;&#25552;&#39640;&#20102;LLMs&#23545;&#26410;&#30693;&#25351;&#20196;&#30340;&#31283;&#20581;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11138v1 Announce Type: cross  Abstract: Instruction tuning has been used as a promising approach to improve the performance of large language models (LLMs) on unseen tasks. However, current LLMs exhibit limited robustness to unseen instructions, generating inconsistent outputs when the same instruction is phrased with slightly varied forms or language styles. This behavior indicates LLMs' lack of robustness to textual variations and generalizability to unseen instructions, potentially leading to trustworthiness issues. Accordingly, we propose Contrastive Instruction Tuning, which maximizes the similarity between the hidden representations of semantically equivalent instruction-instance pairs while minimizing the similarity between semantically different ones. To facilitate this approach, we augment the existing FLAN collection by paraphrasing task instructions. Experiments on the PromptBench benchmark show that CoIN consistently improves LLMs' robustness to unseen instructio
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;TuneTables&#19978;&#19979;&#25991;&#20248;&#21270;&#25216;&#26415;&#65292;&#36890;&#36807;&#24320;&#21457;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#35843;&#25972;&#31574;&#30053;&#65292;&#23558;TabPFN&#25193;&#23637;&#21040;&#19982;&#26356;&#22823;&#25968;&#25454;&#19978;&#30340;&#26368;&#20808;&#36827;&#34920;&#26684;&#20998;&#31867;&#26041;&#27861;&#30456;&#31454;&#20105;&#12290;</title><link>https://arxiv.org/abs/2402.11137</link><description>&lt;p&gt;
TuneTables&#65306;&#21487;&#25193;&#23637;&#20808;&#39564;&#25968;&#25454;&#25311;&#21512;&#32593;&#32476;&#30340;&#19978;&#19979;&#25991;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
TuneTables: Context Optimization for Scalable Prior-Data Fitted Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11137
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;TuneTables&#19978;&#19979;&#25991;&#20248;&#21270;&#25216;&#26415;&#65292;&#36890;&#36807;&#24320;&#21457;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#35843;&#25972;&#31574;&#30053;&#65292;&#23558;TabPFN&#25193;&#23637;&#21040;&#19982;&#26356;&#22823;&#25968;&#25454;&#19978;&#30340;&#26368;&#20808;&#36827;&#34920;&#26684;&#20998;&#31867;&#26041;&#27861;&#30456;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#34920;&#26684;&#20998;&#31867;&#20256;&#32479;&#19978;&#20381;&#36182;&#20110;&#20174;&#38646;&#24320;&#22987;&#35757;&#32451;&#30340;&#38382;&#39064;&#65292;&#26368;&#36817;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#20808;&#39564;&#25968;&#25454;&#25311;&#21512;&#32593;&#32476;&#65288;PFN&#65289;&#30340;&#31361;&#30772;&#24615;&#26041;&#27861;&#65292;&#25361;&#25112;&#20102;&#36825;&#31181;&#26041;&#27861;&#12290;&#31867;&#20284;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;PFN&#21033;&#29992;&#39044;&#35757;&#32451;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#22312;&#21333;&#27425;&#21069;&#21521;&#20256;&#36882;&#20013;&#22312;&#26032;&#20219;&#21153;&#19978;&#21462;&#24471;&#24378;&#22823;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;PFN&#23384;&#22312;&#38480;&#21046;&#65292;&#38459;&#30861;&#20102;&#23427;&#20204;&#30340;&#24191;&#27867;&#37319;&#29992;&#12290;&#29305;&#21035;&#26159;&#65292;TabPFN&#22312;&#23567;&#22411;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#38750;&#24120;&#24378;&#21170;&#30340;&#24615;&#33021;&#65292;&#20294;&#24182;&#19981;&#36866;&#29992;&#20110;&#25968;&#25454;&#38598;&#22823;&#23567;&#22823;&#20110;1000&#30340;&#39044;&#27979;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20026;PFN&#24320;&#21457;&#19978;&#19979;&#25991;&#20248;&#21270;&#25216;&#26415;&#65292;&#20811;&#26381;&#20102;&#36825;&#20123;&#38480;&#21046;&#65292;&#22823;&#24133;&#25552;&#39640;&#20102;PFN&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TuneTables&#65292;&#19968;&#31181;&#23558;&#22823;&#22411;&#25968;&#25454;&#38598;&#21387;&#32553;&#20026;&#36739;&#23567;&#23398;&#20064;&#19978;&#19979;&#25991;&#30340;&#26032;&#22411;&#25552;&#31034;&#35843;&#25972;&#31574;&#30053;&#12290;TuneTables&#23558;TabPFN&#25193;&#23637;&#21040;&#19982;&#26356;&#22823;&#25968;&#25454;&#19978;&#30340;&#26368;&#20808;&#36827;&#34920;&#26684;&#20998;&#31867;&#26041;&#27861;&#30456;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11137v1 Announce Type: new  Abstract: While tabular classification has traditionally relied on from-scratch training, a recent breakthrough called prior-data fitted networks (PFNs) challenges this approach. Similar to large language models, PFNs make use of pretraining and in-context learning to achieve strong performance on new tasks in a single forward pass. However, current PFNs have limitations that prohibit their widespread adoption. Notably, TabPFN achieves very strong performance on small tabular datasets but is not designed to make predictions for datasets of size larger than 1000. In this work, we overcome these limitations and substantially improve the performance of PFNs by developing context optimization techniques for PFNs. Specifically, we propose TuneTables, a novel prompt-tuning strategy that compresses large datasets into a smaller learned context. TuneTables scales TabPFN to be competitive with state-of-the-art tabular classification methods on larger datas
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;Speculative Streaming&#26041;&#27861;&#65292;&#23558;&#33609;&#31295;&#27169;&#22411;&#34701;&#20837;&#30446;&#26631;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#23558;&#24494;&#35843;&#30446;&#26631;&#20174;&#19979;&#19968;&#20010;&#20196;&#29260;&#39044;&#27979;&#26356;&#25913;&#20026;&#26410;&#26469;&#30340;n-gram&#39044;&#27979;&#65292;&#21152;&#36895;&#35299;&#30721;1.8-3.1&#20493;&#65292;&#21516;&#26102;&#20445;&#25345;&#29983;&#25104;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.11131</link><description>&lt;p&gt;
&#25512;&#27979;&#24335;&#27969;&#24335;&#22788;&#29702;: &#26080;&#38656;&#36741;&#21161;&#27169;&#22411;&#30340;&#24555;&#36895;LLM&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Speculative Streaming: Fast LLM Inference without Auxiliary Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11131
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;Speculative Streaming&#26041;&#27861;&#65292;&#23558;&#33609;&#31295;&#27169;&#22411;&#34701;&#20837;&#30446;&#26631;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#23558;&#24494;&#35843;&#30446;&#26631;&#20174;&#19979;&#19968;&#20010;&#20196;&#29260;&#39044;&#27979;&#26356;&#25913;&#20026;&#26410;&#26469;&#30340;n-gram&#39044;&#27979;&#65292;&#21152;&#36895;&#35299;&#30721;1.8-3.1&#20493;&#65292;&#21516;&#26102;&#20445;&#25345;&#29983;&#25104;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#27979;&#24335;&#35299;&#30721;&#26159;&#19968;&#31181;&#31361;&#20986;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#25552;&#39640;&#22522;&#20110;&#36741;&#21161;&#33609;&#31295;&#27169;&#22411;&#39044;&#27979;&#30340;&#22823;&#22411;&#30446;&#26631;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#36895;&#24230;&#12290;&#34429;&#28982;&#22312;&#29305;&#23450;&#24212;&#29992;&#35774;&#32622;&#20013;&#26377;&#25928;&#65292;&#20294;&#36890;&#24120;&#38656;&#35201;&#24494;&#35843;&#33609;&#31295;&#21644;&#30446;&#26631;&#27169;&#22411;&#20197;&#23454;&#29616;&#36739;&#39640;&#30340;&#25509;&#21463;&#29575;&#12290;&#38543;&#30528;&#19979;&#28216;&#20219;&#21153;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#36825;&#20123;&#33609;&#31295;&#27169;&#22411;&#32473;&#25512;&#29702;&#31995;&#32479;&#22686;&#21152;&#20102;&#26174;&#33879;&#30340;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Speculative Streaming&#65292;&#19968;&#31181;&#21333;&#27169;&#22411;&#30340;&#25512;&#27979;&#24335;&#35299;&#30721;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#33609;&#25311;&#34701;&#20837;&#30446;&#26631;&#27169;&#22411;&#65292;&#23558;&#24494;&#35843;&#30446;&#26631;&#20174;&#19979;&#19968;&#20010;&#20196;&#29260;&#39044;&#27979;&#23545;&#35937;&#26356;&#25913;&#20026;&#26410;&#26469;&#30340;n-gram&#39044;&#27979;&#12290; Speculative Streaming&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21152;&#36895;&#35299;&#30721;1.8-3.1&#20493;&#65292;&#22914;&#25688;&#35201;&#12289;&#32467;&#26500;&#21270;&#26597;&#35810;&#21644;&#24847;&#20041;&#34920;&#36798;&#65292;&#21516;&#26102;&#19981;&#38477;&#20302;&#29983;&#25104;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;Speculative Streaming&#21442;&#25968;&#26377;&#25928;&#12290;&#23427;&#23454;&#29616;&#20102;&#19982;Medusa&#39118;&#26684;&#26550;&#26500;&#30456;&#23218;&#32654;/&#26356;&#39640;&#30340;&#21152;&#36895;&#24230;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11131v1 Announce Type: cross  Abstract: Speculative decoding is a prominent technique to speed up the inference of a large target language model based on predictions of an auxiliary draft model. While effective, in application-specific settings, it often involves fine-tuning both draft and target models to achieve high acceptance rates. As the number of downstream tasks grows, these draft models add significant complexity to inference systems. We propose Speculative Streaming, a single-model speculative decoding method that fuses drafting into the target model by changing the fine-tuning objective from next token prediction to future n-gram prediction. Speculative Streaming speeds up decoding by 1.8 - 3.1X in a diverse set of tasks, such as Summarization, Structured Queries, and Meaning Representation, without sacrificing generation quality. Additionally, Speculative Streaming is parameter-efficient. It achieves on-par/higher speed-ups than Medusa-style architectures while u
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;Kolmogorov n-&#23485;&#24230;&#20316;&#20026;&#35780;&#20272;&#22810;&#20219;&#21153;PIML&#26550;&#26500;&#30340;&#26377;&#25928;&#24615;&#30340;&#26041;&#27861;&#65292;&#20998;&#26512;&#27169;&#22411;&#22312;&#21508;&#31181;PDE&#38382;&#39064;&#19978;&#23398;&#21040;&#30340;&#22522;&#20989;&#25968;&#12290;</title><link>https://arxiv.org/abs/2402.11126</link><description>&lt;p&gt;
Kolmogorov n-&#23485;&#24230;&#29992;&#20110;&#22810;&#20219;&#21153;&#29289;&#29702;&#20449;&#24687;&#26426;&#22120;&#23398;&#20064;&#65288;PIML&#65289;&#26041;&#27861;&#65306;&#26397;&#21521;&#31283;&#20581;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
Kolmogorov n-Widths for Multitask Physics-Informed Machine Learning (PIML) Methods: Towards Robust Metrics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;Kolmogorov n-&#23485;&#24230;&#20316;&#20026;&#35780;&#20272;&#22810;&#20219;&#21153;PIML&#26550;&#26500;&#30340;&#26377;&#25928;&#24615;&#30340;&#26041;&#27861;&#65292;&#20998;&#26512;&#27169;&#22411;&#22312;&#21508;&#31181;PDE&#38382;&#39064;&#19978;&#23398;&#21040;&#30340;&#22522;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11126v1 &#22768;&#26126;&#31867;&#22411;&#65306;&#26032;&#25688;&#35201;&#65306;&#20316;&#20026;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#30340;&#25163;&#27573;&#65292;&#29289;&#29702;&#20449;&#24687;&#26426;&#22120;&#23398;&#20064;&#65288;PIML&#65289;&#22312;&#35745;&#31639;&#31185;&#23398;&#19982;&#24037;&#31243;&#65288;CS&amp;E&#65289;&#39046;&#22495;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290; &#36825;&#20010;&#35805;&#39064;&#28085;&#30422;&#20102;&#26088;&#22312;&#35299;&#20915;&#21333;&#20010;&#25110;&#22810;&#20010;PDE&#38382;&#39064;&#30340;&#21508;&#31181;&#26041;&#27861;&#21644;&#27169;&#22411;&#65292;&#31216;&#20026;&#22810;&#20219;&#21153;&#23398;&#20064;&#12290; PIML &#30340;&#29305;&#28857;&#26159;&#22312;&#35299;&#20915; PDE &#38382;&#39064;&#26102;&#65292;&#23558;&#29289;&#29702;&#23450;&#24459;&#32435;&#20837;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#32780;&#19981;&#26159;&#22823;&#25968;&#25454;&#12290; &#23613;&#31649;&#36825;&#19968;&#31995;&#21015;&#26041;&#27861;&#30340;&#25972;&#20307;&#25104;&#21151;&#65292;&#20294;&#20998;&#26512;&#12289;&#22522;&#20934;&#27979;&#35797;&#21644;&#36890;&#24120;&#27604;&#36739;&#19968;&#31181;&#26041;&#27861;&#19982;&#21478;&#19968;&#31181;&#26041;&#27861;&#20173;&#28982;&#38750;&#24120;&#22256;&#38590;&#12290; &#25105;&#20204;&#20351;&#29992; Kolmogorov n-&#23485;&#24230;&#20316;&#20026;&#36817;&#20284;&#20989;&#25968;&#26377;&#25928;&#24615;&#30340;&#34913;&#37327;&#26631;&#20934;&#65292;&#23457;&#24910;&#22320;&#23558;&#36825;&#19968;&#25351;&#26631;&#24212;&#29992;&#20110;&#27604;&#36739;&#21508;&#31181;&#22810;&#20219;&#21153; PIML &#32467;&#26500;&#12290; &#25105;&#20204;&#35745;&#31639;&#36739;&#20302;&#30340;&#20934;&#30830;&#24230;&#19979;&#30028;&#65292;&#24182;&#20998;&#26512;&#27169;&#22411;&#22312;&#21508;&#31181; PDE &#38382;&#39064;&#19978;&#23398;&#21040;&#30340;&#22522;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11126v1 Announce Type: new  Abstract: Physics-informed machine learning (PIML) as a means of solving partial differential equations (PDE) has garnered much attention in the Computational Science and Engineering (CS&amp;E) world. This topic encompasses a broad array of methods and models aimed at solving a single or a collection of PDE problems, called multitask learning. PIML is characterized by the incorporation of physical laws into the training process of machine learning models in lieu of large data when solving PDE problems. Despite the overall success of this collection of methods, it remains incredibly difficult to analyze, benchmark, and generally compare one approach to another. Using Kolmogorov n-widths as a measure of effectiveness of approximating functions, we judiciously apply this metric in the comparison of various multitask PIML architectures. We compute lower accuracy bounds and analyze the model's learned basis functions on various PDE problems. This is the fi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#36719;&#24178;&#39044;&#22788;&#29702;&#38544;&#24335;&#28508;&#22312;&#22240;&#26524;&#34920;&#24449;&#23398;&#20064;&#65292;&#22312; Variational Autoencoder (VAE) &#26694;&#26550;&#20013;&#24341;&#20837;&#20102;&#22240;&#26524;&#26426;&#21046;&#24320;&#20851;&#21464;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.11124</link><description>&lt;p&gt;
&#36890;&#36807;&#24320;&#20851;&#21464;&#37327;&#22312;&#38544;&#24335;&#22240;&#26524;&#27169;&#22411;&#20013;&#35299;&#24320;&#32416;&#32544;
&lt;/p&gt;
&lt;p&gt;
Disentanglement in Implicit Causal Models via Switch Variable
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11124
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#36719;&#24178;&#39044;&#22788;&#29702;&#38544;&#24335;&#28508;&#22312;&#22240;&#26524;&#34920;&#24449;&#23398;&#20064;&#65292;&#22312; Variational Autoencoder (VAE) &#26694;&#26550;&#20013;&#24341;&#20837;&#20102;&#22240;&#26524;&#26426;&#21046;&#24320;&#20851;&#21464;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35266;&#27979;&#21644;&#24178;&#39044;&#25968;&#25454;&#20013;&#23398;&#20064;&#22240;&#26524;&#34920;&#24449;&#65292;&#22312;&#27809;&#26377;&#24050;&#30693;&#30340;&#22320;&#38754;&#30495;&#23454;&#22270;&#32467;&#26500;&#30340;&#24773;&#20917;&#19979;&#65292;&#38656;&#35201;&#38544;&#24335;&#28508;&#22312;&#22240;&#26524;&#34920;&#24449;&#23398;&#20064;&#12290;&#38544;&#24335;&#23398;&#20064;&#22240;&#26524;&#26426;&#21046;&#36890;&#24120;&#28041;&#21450;&#20004;&#31867;&#24178;&#39044;&#25968;&#25454;&#65306;&#30828;&#24178;&#39044;&#21644;&#36719;&#24178;&#39044;&#12290;&#22312;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#65292;&#36719;&#24178;&#39044;&#36890;&#24120;&#27604;&#30828;&#24178;&#39044;&#26356;&#29616;&#23454;&#65292;&#22240;&#20026;&#21518;&#32773;&#38656;&#35201;&#23436;&#20840;&#21463;&#25511;&#30340;&#29615;&#22659;&#12290;&#19982;&#30452;&#25509;&#24378;&#21046;&#25913;&#21464;&#22240;&#26524;&#21464;&#37327;&#30340;&#30828;&#24178;&#39044;&#19981;&#21516;&#65292;&#36719;&#24178;&#39044;&#36890;&#36807;&#24433;&#21709;&#22240;&#26524;&#26426;&#21046;&#38388;&#25509;&#22320;&#20135;&#29983;&#24433;&#21709;&#12290;&#26412;&#25991;&#36890;&#36807;&#36719;&#24178;&#39044;&#22312;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#26694;&#26550;&#20013;&#22788;&#29702;&#38544;&#24335;&#28508;&#22312;&#22240;&#26524;&#34920;&#24449;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#26088;&#22312;&#22312;&#19981;&#21516;&#22240;&#26524;&#26426;&#21046;&#20043;&#38388;&#20999;&#25442;&#30340;&#22240;&#26524;&#26426;&#21046;&#24320;&#20851;&#21464;&#37327;&#26469;&#24314;&#27169;&#36719;&#24178;&#39044;&#25928;&#26524;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#22987;&#32456;&#20445;&#25345;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11124v1 Announce Type: new  Abstract: Learning causal representations from observational and interventional data in the absence of known ground-truth graph structures necessitates implicit latent causal representation learning. Implicitly learning causal mechanisms typically involves two categories of interventional data: hard and soft interventions. In real-world scenarios, soft interventions are often more realistic than hard interventions, as the latter require fully controlled environments. Unlike hard interventions, which directly force changes in a causal variable, soft interventions exert influence indirectly by affecting the causal mechanism. In this paper, we tackle implicit latent causal representation learning in a Variational Autoencoder (VAE) framework through soft interventions. Our approach models soft interventions effects by employing a causal mechanism switch variable designed to toggle between different causal mechanisms. In our experiments, we consistentl
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24773;&#22659;&#33218;&#21644;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35266;&#23519;&#25968;&#25454;&#24314;&#31435;&#20010;&#24615;&#21270;&#30340;&#21326;&#27861;&#26519;&#21058;&#37327;&#31574;&#30053;&#65292;&#21363;&#20351;&#22312;&#32570;&#20047;&#22522;&#22240;&#22411;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#20063;&#21487;&#20197;&#36229;&#36234;&#22522;&#32447;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.11123</link><description>&lt;p&gt;
&#20351;&#29992;&#24773;&#22659;&#33218;&#30740;&#31350;&#20248;&#21270;&#21326;&#27861;&#26519;&#29992;&#37327;&#65306;&#19968;&#31181;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#21644;&#35780;&#20272;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Optimizing Warfarin Dosing Using Contextual Bandit: An Offline Policy Learning and Evaluation Method
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11123
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24773;&#22659;&#33218;&#21644;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35266;&#23519;&#25968;&#25454;&#24314;&#31435;&#20010;&#24615;&#21270;&#30340;&#21326;&#27861;&#26519;&#21058;&#37327;&#31574;&#30053;&#65292;&#21363;&#20351;&#22312;&#32570;&#20047;&#22522;&#22240;&#22411;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#20063;&#21487;&#20197;&#36229;&#36234;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21326;&#27861;&#26519;&#26159;&#19968;&#31181;&#25239;&#20957;&#33647;&#29289;&#65292;&#26088;&#22312;&#39044;&#38450;&#21644;&#27835;&#30103;&#19982;&#24322;&#24120;&#34880;&#28082;&#20957;&#32467;&#30456;&#20851;&#30340;&#30142;&#30149;&#65292;&#26159;&#20840;&#29699;&#26368;&#24120;&#24320;&#22788;&#26041;&#30340;&#33647;&#29289;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20010;&#20307;&#21453;&#24212;&#21464;&#21270;&#65292;&#30830;&#23450;&#21512;&#36866;&#30340;&#21058;&#37327;&#20173;&#20855;&#25361;&#25112;&#24615;&#65292;&#38169;&#35823;&#30340;&#21058;&#37327;&#21487;&#33021;&#23548;&#33268;&#20005;&#37325;&#21518;&#26524;&#12290;&#24773;&#22659;&#33218;&#21644;&#24378;&#21270;&#23398;&#20064;&#22312;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#19978;&#26174;&#31034;&#20986;&#20102;&#28508;&#21147;&#12290;&#37492;&#20110;&#21382;&#21490;&#25919;&#31574;&#30340;&#35266;&#23519;&#25968;&#25454;&#24191;&#27867;&#21487;&#24471;&#19988;&#21307;&#30103;&#20915;&#31574;&#30340;&#23433;&#20840;&#24615;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#20351;&#29992;&#21382;&#21490;&#25919;&#31574;&#20316;&#20026;&#28436;&#31034;&#30340;&#35266;&#27979;&#25968;&#25454;&#65292;&#36890;&#36807;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#21644;&#35780;&#20272;&#22312;&#24773;&#22659;&#33218;&#29615;&#22659;&#20013;&#24314;&#31435;&#26368;&#20339;&#20010;&#24615;&#21270;&#21058;&#37327;&#31574;&#30053;&#12290;&#25105;&#20204;&#23398;&#20064;&#21040;&#30340;&#31574;&#30053;&#22312;&#27809;&#26377;&#22522;&#22240;&#22411;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#36229;&#36234;&#20102;&#36825;&#20123;&#22522;&#32447;&#26041;&#27861;&#65292;&#29978;&#33267;&#22312;&#32473;&#23450;&#27425;&#20248;&#28436;&#31034;&#30340;&#24773;&#20917;&#19979;&#20063;&#34920;&#29616;&#20986;&#33394;&#65292;&#23637;&#31034;&#20102;&#24040;&#22823;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11123v1 Announce Type: cross  Abstract: Warfarin, an anticoagulant medication, is formulated to prevent and address conditions associated with abnormal blood clotting, making it one of the most prescribed drugs globally. However, determining the suitable dosage remains challenging due to individual response variations, and prescribing an incorrect dosage may lead to severe consequences. Contextual bandit and reinforcement learning have shown promise in addressing this issue. Given the wide availability of observational data and safety concerns of decision-making in healthcare, we focused on using exclusively observational data from historical policies as demonstrations to derive new policies; we utilized offline policy learning and evaluation in a contextual bandit setting to establish the optimal personalized dosage strategy. Our learned policies surpassed these baseline approaches without genotype inputs, even when given a suboptimal demonstration, showcasing promising app
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#20013;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#24314;&#31435;&#23545;&#25239;&#30446;&#26631;&#25439;&#22833;&#30340;&#27867;&#21270;&#30028;&#38480;&#26469;&#35299;&#20915;&#30446;&#26631;&#22495;&#26631;&#31614;&#32570;&#22833;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.11120</link><description>&lt;p&gt;
DART: &#19968;&#31181;&#38754;&#21521;&#23545;&#25239;&#40065;&#26834;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#21407;&#21017;&#24615;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DART: A Principled Approach to Adversarially Robust Unsupervised Domain Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11120
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#20013;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#24314;&#31435;&#23545;&#25239;&#30446;&#26631;&#25439;&#22833;&#30340;&#27867;&#21270;&#30028;&#38480;&#26469;&#35299;&#20915;&#30446;&#26631;&#22495;&#26631;&#31614;&#32570;&#22833;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#36716;&#31227;&#21644;&#23545;&#25239;&#26679;&#26412;&#26159;&#37096;&#32626;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#38754;&#20020;&#30340;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#12290;&#34429;&#28982;&#36825;&#20123;&#25361;&#25112;&#24050;&#34987;&#20998;&#21035;&#30740;&#31350;&#65292;&#20294;&#23427;&#20204;&#30340;&#32467;&#21512;&#20173;&#28982;&#26159;&#19968;&#20010;&#30456;&#23545;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#30340;&#37325;&#35201;&#20027;&#39064;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#19968;&#20010;&#24120;&#35265;&#30340;&#20998;&#24067;&#36716;&#31227;&#35774;&#32622;&#19979;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#38382;&#39064;&#65292;&#21363;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;UDA&#65289;&#12290;&#20855;&#20307;&#22320;&#65292;&#32473;&#23450;&#19968;&#20010;&#24102;&#26631;&#31614;&#30340;&#28304;&#22495; $D_S$ &#21644;&#19968;&#20010;&#24102;&#26377;&#30456;&#20851;&#20294;&#19981;&#21516;&#20998;&#24067;&#30340;&#26410;&#26631;&#35760;&#30446;&#26631;&#22495; $D_T$&#65292;&#30446;&#26631;&#26159;&#20026; $D_T$ &#33719;&#24471;&#19968;&#20010;&#23545;&#25239;&#40065;&#26834;&#30340;&#27169;&#22411;&#12290;&#30446;&#26631;&#22495;&#26631;&#31614;&#30340;&#32570;&#22833;&#25552;&#20986;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#20256;&#32479;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#38450;&#24481;&#19981;&#33021;&#30452;&#25509;&#24212;&#29992;&#20110; $D_T$&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#39318;&#20808;&#24314;&#31435;&#20102;&#23545;&#25239;&#30446;&#26631;&#25439;&#22833;&#30340;&#27867;&#21270;&#30028;&#38480;&#65292;&#20854;&#20013;&#21253;&#25324;&#19982;&#25968;&#25454;&#25439;&#22833;&#30456;&#20851;&#30340;&#39033;&#21644;&#26368;&#22351;&#24773;&#20917;&#22495;&#20998;&#27495;&#30340;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11120v1 Announce Type: new  Abstract: Distribution shifts and adversarial examples are two major challenges for deploying machine learning models. While these challenges have been studied individually, their combination is an important topic that remains relatively under-explored. In this work, we study the problem of adversarial robustness under a common setting of distribution shift - unsupervised domain adaptation (UDA). Specifically, given a labeled source domain $D_S$ and an unlabeled target domain $D_T$ with related but different distributions, the goal is to obtain an adversarially robust model for $D_T$. The absence of target domain labels poses a unique challenge, as conventional adversarial robustness defenses cannot be directly applied to $D_T$. To address this challenge, we first establish a generalization bound for the adversarial target loss, which consists of (i) terms related to the loss on the data, and (ii) a measure of worst-case domain divergence. Motivat
&lt;/p&gt;</description></item><item><title>&#31169;&#26377;PAC&#23398;&#20064;&#21644;&#22312;&#32447;&#23398;&#20064;&#20043;&#38388;&#30340;&#36716;&#25442;&#24182;&#19981;&#24635;&#26159;&#21487;&#20197;&#20445;&#25345;&#35745;&#31639;&#25928;&#29575;&#65292;&#22312;&#19968;&#20123;&#24773;&#20917;&#19979;&#26159;&#22256;&#38590;&#30340;&#12290;</title><link>https://arxiv.org/abs/2402.11119</link><description>&lt;p&gt;
&#31169;&#26377;PAC&#23398;&#20064;&#21487;&#33021;&#27604;&#22312;&#32447;&#23398;&#20064;&#26356;&#22256;&#38590;
&lt;/p&gt;
&lt;p&gt;
Private PAC Learning May be Harder than Online Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11119
&lt;/p&gt;
&lt;p&gt;
&#31169;&#26377;PAC&#23398;&#20064;&#21644;&#22312;&#32447;&#23398;&#20064;&#20043;&#38388;&#30340;&#36716;&#25442;&#24182;&#19981;&#24635;&#26159;&#21487;&#20197;&#20445;&#25345;&#35745;&#31639;&#25928;&#29575;&#65292;&#22312;&#19968;&#20123;&#24773;&#20917;&#19979;&#26159;&#22256;&#38590;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32487;&#32493;&#30740;&#31350;&#19981;&#21516;ially private PAC&#23398;&#20064;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#20197;&#21450;&#23427;&#22312;&#26426;&#22120;&#23398;&#20064;&#22522;&#30784;&#20013;&#30340;&#23450;&#20301;&#12290;&#26368;&#36817;&#30340;&#19968;&#31995;&#21015;&#24037;&#20316;&#25581;&#31034;&#20102;&#31169;&#26377;PAC&#27169;&#22411;&#19982;Littlestone&#30340;&#38169;&#35823;&#30028;&#22312;&#32447;&#23398;&#20064;&#27169;&#22411;&#20043;&#38388;&#30340;&#23450;&#24615;&#31561;&#20215;&#24615;&#65292;&#29305;&#21035;&#26159;&#23637;&#31034;&#20102;&#20219;&#20309;Littlestone&#32500;&#24230;&#20026;$d$&#30340;&#27010;&#24565;&#31867;&#37117;&#21487;&#20197;&#20351;&#29992;$\mathrm{poly}(d)$&#20010;&#26679;&#26412;&#26469;&#36827;&#34892;&#31169;&#26377;PAC&#23398;&#20064;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#33258;&#28982;&#38382;&#39064;&#65292;&#21363;&#26159;&#21542;&#21487;&#33021;&#20174;&#22312;&#32447;&#23398;&#20064;&#32773;&#36716;&#25442;&#20026;&#31169;&#26377;PAC&#23398;&#20064;&#32773;&#65292;&#24182;&#19988;&#36824;&#33021;&#20445;&#25345;&#35745;&#31639;&#25928;&#29575;&#12290;&#25105;&#20204;&#22312;&#21512;&#29702;&#30340;&#21152;&#23494;&#20551;&#35774;&#19979;&#65288;&#22823;&#33268;&#26469;&#33258;&#20110;&#21487;&#20197;&#20026;&#25152;&#26377;&#30005;&#36335;&#26500;&#24314;&#19981;&#21487;&#21306;&#20998;&#28151;&#28102;&#30340;&#20551;&#35774;&#65289;&#23545;&#36825;&#20010;&#38382;&#39064;&#32473;&#20986;&#20102;&#21542;&#23450;&#31572;&#26696;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#27010;&#24565;&#31867;&#65292;&#21487;&#20197;&#25509;&#21463;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#36816;&#34892;&#19988;&#20855;&#26377;&#22810;&#39033;&#24335;&#38169;&#35823;&#30028;&#38480;&#30340;&#22312;&#32447;&#23398;&#20064;&#22120;&#65292;&#20294;&#23545;&#20110;&#36825;&#26679;&#19968;&#20010;&#27010;&#24565;&#31867;&#65292;&#19981;&#23384;&#22312;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11119v1 Announce Type: new  Abstract: We continue the study of the computational complexity of differentially private PAC learning and how it is situated within the foundations of machine learning. A recent line of work uncovered a qualitative equivalence between the private PAC model and Littlestone's mistake-bounded model of online learning, in particular, showing that any concept class of Littlestone dimension $d$ can be privately PAC learned using $\mathrm{poly}(d)$ samples. This raises the natural question of whether there might be a generic conversion from online learners to private PAC learners that also preserves computational efficiency.   We give a negative answer to this question under reasonable cryptographic assumptions (roughly, those from which it is possible to build indistinguishability obfuscation for all circuits). We exhibit a concept class that admits an online learner running in polynomial time with a polynomial mistake bound, but for which there is no 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#39044;&#27979;&#26032;&#35199;&#20848;&#22269;&#23478;&#28201;&#23460;&#27668;&#20307;&#25490;&#25918;&#30340;&#26032;&#26041;&#27861;&#65292;&#20351;&#24471;&#39044;&#27979;&#33021;&#25552;&#21069;&#33267;&#22269;&#23478;&#25490;&#25918;&#28165;&#21333;&#21457;&#24067;&#20043;&#21069;&#65292;&#24182;&#23637;&#29616;&#20102;&#36739;&#20302;&#35823;&#24046;&#30340;&#27425;&#24180;&#20272;&#35745;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.11107</link><description>&lt;p&gt;
&#26032;&#35199;&#20848;&#28201;&#23460;&#27668;&#20307;&#28165;&#21333;&#30340;&#21160;&#24577;&#29616;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Dynamic nowcast of the New Zealand greenhouse gas inventory
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11107
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#39044;&#27979;&#26032;&#35199;&#20848;&#22269;&#23478;&#28201;&#23460;&#27668;&#20307;&#25490;&#25918;&#30340;&#26032;&#26041;&#27861;&#65292;&#20351;&#24471;&#39044;&#27979;&#33021;&#25552;&#21069;&#33267;&#22269;&#23478;&#25490;&#25918;&#28165;&#21333;&#21457;&#24067;&#20043;&#21069;&#65292;&#24182;&#23637;&#29616;&#20102;&#36739;&#20302;&#35823;&#24046;&#30340;&#27425;&#24180;&#20272;&#35745;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20943;&#32531;&#27668;&#20505;&#21464;&#21270;&#25928;&#26524;&#30340;&#21162;&#21147;&#21152;&#24378;&#65292;&#21487;&#38752;&#21644;&#20840;&#38754;&#30340;&#28201;&#23460;&#27668;&#20307;&#25490;&#25918;&#25253;&#21578;&#23545;&#20110;&#34913;&#37327;&#22269;&#38469;&#21644;&#22269;&#20869;&#20943;&#25490;&#30446;&#26631;&#30340;&#36827;&#23637;&#33267;&#20851;&#37325;&#35201;&#12290;&#30446;&#21069;&#65292;&#26032;&#35199;&#20848;&#30340;&#22269;&#23478;&#25490;&#25918;&#28165;&#21333;&#25253;&#21578;&#28382;&#21518;&#20110;15&#33267;27&#20010;&#26376;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#26377;&#20004;&#20010;&#26376;&#30340;&#24310;&#36831;&#65288;&#30001;&#20110;&#24403;&#21069;&#25968;&#25454;&#21487;&#29992;&#24615;&#65289;&#65292;&#22312;&#26032;&#35199;&#20848;&#22269;&#23478;&#25490;&#25918;&#28165;&#21333;&#21457;&#24067;&#20043;&#21069;&#29616;&#22312;&#39044;&#27979;&#22269;&#23478;&#28201;&#23460;&#27668;&#20307;&#25490;&#25918;&#12290;&#20027;&#35201;&#21457;&#29616;&#21253;&#25324;&#33258;2020&#24180;&#20197;&#26469;&#22269;&#23478;&#24635;&#25490;&#25918;&#37327;&#30340;&#20272;&#35745;&#20943;&#23569;0.2%&#65288;&#25130;&#33267;2022&#24180;7&#26376;&#65289;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#31361;&#26174;&#20102;&#21160;&#24577;&#35270;&#35282;&#23545;&#25490;&#25918;&#23494;&#38598;&#22411;&#27963;&#21160;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#35770;&#26159;&#19968;&#20010;&#27010;&#24565;&#39564;&#35777;&#65292;&#21363;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#36739;&#20302;&#30340;&#35823;&#24046;&#23545;&#21508;&#20010;&#37096;&#38376;&#30340;&#22269;&#23478;&#28201;&#23460;&#27668;&#20307;&#25490;&#25918;&#36827;&#34892;&#27425;&#24180;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11107v1 Announce Type: new  Abstract: As efforts to mitigate the effects of climate change grow, reliable and thorough reporting of greenhouse gas emissions are essential for measuring progress towards international and domestic emissions reductions targets. New Zealand's national emissions inventories are currently reported between 15 to 27 months out-of-date. We present a machine learning approach to nowcast (dynamically estimate) national greenhouse gas emissions in New Zealand in advance of the national emissions inventory's release, with just a two month latency due to current data availability. Key findings include an estimated 0.2% decrease in national gross emissions since 2020 (as at July 2022). Our study highlights the predictive power of a dynamic view of emissions intensive activities. This methodology is a proof of concept that a machine learning approach can make sub-annual estimates of national greenhouse gas emissions by sector with a relatively low error tha
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#31354;&#38388;&#32479;&#35745;&#31354;&#38388;&#20013;&#26368;&#23567;&#21270;&#21407;&#22987;&#21644;&#37325;&#24314;&#22270;&#20687;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#23454;&#29616;&#23398;&#20064;&#24494;&#35266;&#32467;&#26500;&#30340;&#20302;&#32500;&#34920;&#31034;</title><link>https://arxiv.org/abs/2402.11103</link><description>&lt;p&gt;
&#36890;&#36807;&#22312;&#31354;&#38388;&#32479;&#35745;&#31354;&#38388;&#20248;&#21270;&#23398;&#20064;&#24494;&#35266;&#32467;&#26500;&#30340;&#28508;&#21464;&#37327;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Toward Learning Latent-Variable Representations of Microstructures by Optimizing in Spatial Statistics Space
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11103
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#31354;&#38388;&#32479;&#35745;&#31354;&#38388;&#20013;&#26368;&#23567;&#21270;&#21407;&#22987;&#21644;&#37325;&#24314;&#22270;&#20687;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#23454;&#29616;&#23398;&#20064;&#24494;&#35266;&#32467;&#26500;&#30340;&#20302;&#32500;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26448;&#26009;&#31185;&#23398;&#20013;&#65292;&#26448;&#26009;&#24320;&#21457;&#28041;&#21450;&#35780;&#20272;&#21644;&#20248;&#21270;&#26448;&#26009;&#30340;&#20869;&#37096;&#32467;&#26500;&#65292;&#19968;&#33324;&#31216;&#20026;&#24494;&#35266;&#32467;&#26500;&#12290;&#24494;&#35266;&#32467;&#26500;&#26159;&#38543;&#26426;&#30340;&#65292;&#31867;&#20284;&#20110;&#22270;&#20687;&#32441;&#29702;&#12290;&#19968;&#20010;&#29305;&#23450;&#30340;&#24494;&#35266;&#32467;&#26500;&#21487;&#20197;&#36890;&#36807;&#20854;&#31354;&#38388;&#32479;&#35745;&#29305;&#24449;&#36827;&#34892;&#33391;&#22909;&#30340;&#34920;&#24449;&#65292;&#31867;&#20284;&#20110;&#22270;&#20687;&#32441;&#29702;&#36890;&#36807;&#23545;&#20613;&#37324;&#21494;&#28388;&#27874;&#22120;&#32452;&#30340;&#21709;&#24212;&#36827;&#34892;&#34920;&#24449;&#12290;&#26448;&#26009;&#35774;&#35745;&#23558;&#21463;&#30410;&#20110;&#24494;&#35266;&#32467;&#26500;&#30340;&#20302;&#32500;&#34920;&#31034;&#65288;Paulson&#31561;&#20154;&#65292;2017&#65289;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35757;&#32451;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#20197;&#29983;&#25104;&#20445;&#30041;&#21407;&#22987;&#32441;&#29702;&#30340;&#31354;&#38388;&#32479;&#35745;&#30340;&#37325;&#24314;&#32441;&#29702;&#65292;&#32780;&#19981;&#19968;&#23450;&#22312;&#25968;&#25454;&#31354;&#38388;&#20013;&#37325;&#24314;&#30456;&#21516;&#30340;&#22270;&#20687;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#25104;&#26412;&#20989;&#25968;&#20013;&#28155;&#21152;&#21487;&#24494;&#20998;&#39033;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#20197;&#26368;&#23567;&#21270;&#21407;&#22987;&#22270;&#20687;&#21644;&#37325;&#24314;&#22270;&#20687;&#20043;&#38388;&#22312;&#31354;&#38388;&#32479;&#35745;&#31354;&#38388;&#20013;&#30340;&#36317;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11103v1 Announce Type: new  Abstract: In Materials Science, material development involves evaluating and optimizing the internal structures of the material, generically referred to as microstructures. Microstructures structure is stochastic, analogously to image textures. A particular microstructure can be well characterized by its spatial statistics, analogously to image texture being characterized by the response to a Fourier-like filter bank. Material design would benefit from low-dimensional representation of microstructures Paulson et al. (2017).   In this work, we train a Variational Autoencoders (VAE) to produce reconstructions of textures that preserve the spatial statistics of the original texture, while not necessarily reconstructing the same image in data space. We accomplish this by adding a differentiable term to the cost function in order to minimize the distance between the original and the reconstruction in spatial statistics space.   Our experiments indicate
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#36125;&#21494;&#26031;&#20248;&#21270;&#24320;&#21457;&#20102;&#19968;&#20010;&#20998;&#26512;&#24179;&#21488;&#65292;&#21487;&#20197;&#20174;&#38041;&#38043;&#30719;&#23454;&#39564;&#20013;&#25552;&#21462;&#22810;&#20010;&#22522;&#26412;&#26448;&#26009;&#21442;&#25968;&#65292;&#21152;&#36895;&#26448;&#26009;&#21457;&#29616;&#21644;&#21322;&#23548;&#20307;&#20248;&#21270;</title><link>https://arxiv.org/abs/2402.11101</link><description>&lt;p&gt;
&#36890;&#36807;&#36125;&#21494;&#26031;&#20248;&#21270;&#20174;&#38041;&#38043;&#30719;&#23454;&#39564;&#20013;&#25552;&#21462;&#22522;&#20110;&#29289;&#29702;&#30340;&#26448;&#26009;&#21442;&#25968;
&lt;/p&gt;
&lt;p&gt;
Physics-based material parameters extraction from perovskite experiments via Bayesian optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11101
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#36125;&#21494;&#26031;&#20248;&#21270;&#24320;&#21457;&#20102;&#19968;&#20010;&#20998;&#26512;&#24179;&#21488;&#65292;&#21487;&#20197;&#20174;&#38041;&#38043;&#30719;&#23454;&#39564;&#20013;&#25552;&#21462;&#22810;&#20010;&#22522;&#26412;&#26448;&#26009;&#21442;&#25968;&#65292;&#21152;&#36895;&#26448;&#26009;&#21457;&#29616;&#21644;&#21322;&#23548;&#20307;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#23450;&#37327;&#23454;&#39564;&#20998;&#26512;&#20013;&#25552;&#21462;&#26448;&#26009;&#21442;&#25968;&#30340;&#33021;&#21147;&#23545;&#20110;&#21512;&#29702;&#35774;&#35745;&#21644;&#29702;&#35770;&#36827;&#27493;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#29702;&#35770;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#21644;&#26448;&#26009;&#21442;&#25968;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#36825;&#31181;&#20998;&#26512;&#30340;&#38590;&#24230;&#26174;&#30528;&#22686;&#21152;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20351;&#29992;&#36125;&#21494;&#26031;&#20248;&#21270;&#24320;&#21457;&#20102;&#19968;&#20010;&#20998;&#26512;&#24179;&#21488;&#65292;&#21487;&#20197;&#20174;&#30636;&#24577;&#20809;&#33268;&#21457;&#20809;&#23454;&#39564;&#20013;&#25552;&#21462;&#19968;&#20010;&#26377;&#26426;&#37329;&#23646;&#38041;&#38043;&#30719;&#21322;&#23548;&#20307;&#30340;8&#20010;&#22522;&#26412;&#26448;&#26009;&#21442;&#25968;&#65292;&#22522;&#20110;&#19968;&#20010;&#21253;&#25324;&#36733;&#27969;&#23376;&#28418;&#31227;&#25193;&#25955;&#21644;&#21160;&#24577;&#32570;&#38519;&#21344;&#25454;&#30340;&#22797;&#26434;&#20840;&#29289;&#29702;&#27169;&#22411;&#12290;&#28909;&#38477;&#35299;&#30340;&#19968;&#20010;&#31034;&#20363;&#30740;&#31350;&#34920;&#26126;&#65292;&#25530;&#26434;&#27987;&#24230;&#21644;&#36733;&#27969;&#23376;&#36801;&#31227;&#29575;&#30340;&#21464;&#21270;&#20027;&#23548;&#65292;&#32780;&#32570;&#38519;&#33021;&#32423;&#20960;&#20046;&#20445;&#25345;&#19981;&#21464;&#12290;&#36825;&#20010;&#24179;&#21488;&#21487;&#20197;&#26041;&#20415;&#22320;&#24212;&#29992;&#20110;&#20854;&#20182;&#23454;&#39564;&#25110;&#23454;&#39564;&#32452;&#21512;&#65292;&#21152;&#36895;&#26448;&#26009;&#21457;&#29616;&#21644;&#21322;&#23548;&#20307;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11101v1 Announce Type: cross  Abstract: The ability to extract material parameters from quantitative experimental analysis is essential for rational design and theory advancement. However, the difficulty of this analysis increases significantly with the complexity of the theoretical model and the number of material parameters. Here we use Bayesian optimization to develop an analysis platform that can extract up to 8 fundamental material parameters of an organometallic perovskite semiconductor from a transient photoluminescence experiment, based on a complex full physics model that includes drift-diffusion of carriers and dynamic defect occupation. An example study of thermal degradation reveals that changes in doping concentration and carrier mobility dominate, while the defect energy level remains nearly unchanged. This platform can be conveniently applied to other experiments or to combinations of experiments, accelerating materials discovery and optimization of semiconduc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#27169;&#22359;&#21270;&#30340;&#31471;&#21040;&#31471;&#25163;&#20889;&#30005;&#36335;&#22270;&#20687;&#30340;&#22270;&#24418;&#25552;&#21462;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#20174;&#26629;&#26684;&#22270;&#20687;&#20013;&#25552;&#21462;&#30005;&#27668;&#22270;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.11093</link><description>&lt;p&gt;
&#29992;&#20110;&#25163;&#20889;&#30005;&#36335;&#22270;&#20687;&#30340;&#27169;&#22359;&#21270;&#22270;&#24418;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Modular Graph Extraction for Handwritten Circuit Diagram Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11093
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#27169;&#22359;&#21270;&#30340;&#31471;&#21040;&#31471;&#25163;&#20889;&#30005;&#36335;&#22270;&#20687;&#30340;&#22270;&#24418;&#25552;&#21462;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#20174;&#26629;&#26684;&#22270;&#20687;&#20013;&#25552;&#21462;&#30005;&#27668;&#22270;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#24037;&#31243;&#39046;&#22495;&#30340;&#25968;&#23383;&#21270;&#36827;&#23637;&#65292;&#30005;&#36335;&#22270;&#65288;&#20063;&#31216;&#20026;&#21407;&#29702;&#22270;&#65289;&#36890;&#24120;&#26159;&#22312;&#35745;&#31639;&#26426;&#36741;&#21161;&#24037;&#31243;&#65288;CAE&#65289;&#31995;&#32479;&#20013;&#24320;&#21457;&#21644;&#32500;&#25252;&#30340;&#65292;&#20174;&#32780;&#23454;&#29616;&#33258;&#21160;&#21270;&#39564;&#35777;&#12289;&#27169;&#25311;&#21644;&#22312;&#19979;&#28216;&#24037;&#31243;&#27493;&#39588;&#20013;&#36827;&#19968;&#27493;&#21152;&#24037;&#12290;&#28982;&#32780;&#65292;&#38500;&#20102;&#21360;&#21047;&#30340;&#20256;&#32479;&#21407;&#29702;&#22270;&#22806;&#65292;&#22312;&#25945;&#32946;&#39046;&#22495;&#20013;&#20170;&#22825;&#20173;&#28982;&#20351;&#29992;&#25163;&#32472;&#30005;&#36335;&#22270;&#65292;&#23427;&#20204;&#20316;&#20026;&#23398;&#21592;&#21644;&#23398;&#29983;&#23398;&#20064;&#32472;&#21046;&#27492;&#31867;&#22270;&#34920;&#30340;&#19968;&#31181;&#23481;&#26131;&#33719;&#21462;&#30340;&#25163;&#27573;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#27861;&#24459;&#32422;&#26463;&#65292;&#25163;&#32472;&#21407;&#29702;&#22270;&#36890;&#24120;&#29992;&#20110;&#32771;&#35797;&#12290;&#20026;&#20102;&#21033;&#29992;&#25968;&#23383;&#30005;&#36335;&#34920;&#31034;&#30340;&#21151;&#33021;&#65292;&#38656;&#35201;&#33258;&#21160;&#25552;&#21462;&#26629;&#26684;&#22270;&#24418;&#20013;&#30340;&#30005;&#36335;&#22270;&#30340;&#26041;&#27861;&#12290;&#34429;&#28982;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#30456;&#24212;&#30340;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#26159;&#22312;&#23567;&#22411;&#25110;&#26410;&#20844;&#24320;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#27169;&#22359;&#21270;&#30340;&#31471;&#21040;&#31471;s
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11093v1 Announce Type: cross  Abstract: As digitization in engineering progressed, circuit diagrams (also referred to as schematics) are typically developed and maintained in computer-aided engineering (CAE) systems, thus allowing for automated verification, simulation and further processing in downstream engineering steps. However, apart from printed legacy schematics, hand-drawn circuit diagrams are still used today in the educational domain, where they serve as an easily accessible mean for trainees and students to learn drawing this type of diagrams. Furthermore, hand-drawn schematics are typically used in examinations due to legal constraints. In order to harness the capabilities of digital circuit representations, automated means for extracting the electrical graph from raster graphics are required.   While respective approaches have been proposed in literature, they are typically conducted on small or non-disclosed datasets. This paper describes a modular end-to-end s
&lt;/p&gt;</description></item><item><title>&#32431;&#24494;&#35843;&#36890;&#36807;&#20248;&#21270;&#26465;&#20214;&#20284;&#28982;&#12289;&#22686;&#21152;&#38543;&#26426;&#37322;&#20041;&#21644;&#20107;&#23454;&#30340;&#25968;&#25454;&#65292;&#22312;&#27169;&#22411;&#32534;&#36753;&#20013;&#21462;&#24471;&#20102;&#19981;&#20439;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.11078</link><description>&lt;p&gt;
&#36890;&#36807;&#32431;&#24494;&#35843;&#36827;&#34892;&#27169;&#22411;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Model Editing by Pure Fine-Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11078
&lt;/p&gt;
&lt;p&gt;
&#32431;&#24494;&#35843;&#36890;&#36807;&#20248;&#21270;&#26465;&#20214;&#20284;&#28982;&#12289;&#22686;&#21152;&#38543;&#26426;&#37322;&#20041;&#21644;&#20107;&#23454;&#30340;&#25968;&#25454;&#65292;&#22312;&#27169;&#22411;&#32534;&#36753;&#20013;&#21462;&#24471;&#20102;&#19981;&#20439;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#32454;&#35843;&#25972;&#34987;&#35748;&#20026;&#22312;&#27169;&#22411;&#32534;&#36753;&#20013;&#19981;&#22815;&#26377;&#25928;&#65292;&#22240;&#20026;&#30456;&#23545;&#26356;&#19987;&#19994;&#30340;&#26041;&#27861;&#32780;&#35328;&#65292;&#23427;&#30340;&#34920;&#29616;&#36739;&#24046;&#12290;&#28982;&#32780;&#65292;&#24494;&#35843;&#26159;&#31616;&#21333;&#30340;&#65292;&#19981;&#20851;&#24515;&#34987;&#32534;&#36753;&#27169;&#22411;&#30340;&#20307;&#31995;&#32467;&#26500;&#32454;&#33410;&#65292;&#24182;&#19988;&#33021;&#22815;&#21033;&#29992;&#26631;&#20934;&#35757;&#32451;&#26041;&#27861;&#30340;&#19981;&#26029;&#36827;&#23637;&#65288;&#20363;&#22914;PEFT&#65289;&#65292;&#20351;&#20854;&#25104;&#20026;&#27169;&#22411;&#32534;&#36753;&#22120;&#30340;&#21560;&#24341;&#36873;&#25321;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#32431;&#31929;&#30340;&#24494;&#35843;&#21487;&#20197;&#26159;&#19968;&#31181;&#21487;&#34892;&#30340;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#26420;&#32032;&#24494;&#35843;&#36827;&#34892;&#36731;&#24494;&#20462;&#25913;&#30340;&#20004;&#20010;&#20851;&#38190;&#22240;&#32032;&#12290;&#31532;&#19968;&#65292;&#25105;&#20204;&#20248;&#21270;&#26465;&#20214;&#20284;&#28982;&#32780;&#38750;&#23436;&#25972;&#20284;&#28982;&#12290;&#31532;&#20108;&#65292;&#25105;&#20204;&#20351;&#29992;&#38543;&#26426;&#37322;&#20041;&#21644;&#20107;&#23454;&#26469;&#22686;&#21152;&#25968;&#25454;&#65292;&#20197;&#40723;&#21169;&#27867;&#21270;&#21644;&#23616;&#37096;&#24615;&#12290;&#25105;&#20204;&#22312;ZsRE&#21644;CounterFact&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#19968;&#31616;&#21333;&#20462;&#25913;&#20351;&#24471;&#24494;&#35843;&#36890;&#24120;&#21487;&#20197;&#19982;&#19987;&#19994;&#32534;&#36753;&#22120;&#22312;&#32534;&#36753;&#20998;&#25968;&#26041;&#38754;&#21305;&#25932;&#29978;&#33267;&#36229;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11078v1 Announce Type: cross  Abstract: Fine-tuning is dismissed as not effective for model editing due to its poor performance compared to more specialized methods. However, fine-tuning is simple, agnostic to the architectural details of the model being edited, and able to leverage ongoing advances in standard training methods (e.g., PEFT), making it an appealing choice for a model editor. In this work, we show that pure fine-tuning can be a viable approach to model editing. We propose a slight modification of naive fine-tuning with two key ingredients. First, we optimize the conditional likelihood rather than the full likelihood. Second, we augment the data with random paraphrases and facts to encourage generalization and locality. Our experiments on ZsRE and CounterFact show that this simple modification allows fine-tuning to often match or outperform specialized editors in the edit score.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#32858;&#31867;&#65292;&#35299;&#20915;&#37329;&#34701;&#21253;&#23481;&#24615;&#20449;&#36151;&#20135;&#21697;&#20013;&#23458;&#25143;&#20998;&#21106;&#30340;&#25361;&#25112;</title><link>https://arxiv.org/abs/2402.11066</link><description>&lt;p&gt;
&#36890;&#36807;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#32858;&#31867;&#23454;&#29616;&#37329;&#34701;&#21253;&#23481;&#24615;&#20449;&#36151;&#20135;&#21697;
&lt;/p&gt;
&lt;p&gt;
Towards Financially Inclusive Credit Products Through Financial Time Series Clustering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11066
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#32858;&#31867;&#65292;&#35299;&#20915;&#37329;&#34701;&#21253;&#23481;&#24615;&#20449;&#36151;&#20135;&#21697;&#20013;&#23458;&#25143;&#20998;&#21106;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37329;&#34701;&#21253;&#23481;&#24615;&#30830;&#20445;&#20010;&#20154;&#33021;&#22815;&#33719;&#24471;&#28385;&#36275;&#20854;&#38656;&#27714;&#30340;&#37329;&#34701;&#20135;&#21697;&#21644;&#26381;&#21153;&#12290;&#21033;&#29992;&#28040;&#36153;&#32773;&#20132;&#26131;&#25968;&#25454;&#36827;&#34892;&#23458;&#25143;&#20998;&#21106;&#26159;&#19968;&#31181;&#20419;&#36827;&#37329;&#34701;&#21253;&#23481;&#30340;&#24120;&#29992;&#31574;&#30053;&#65292;&#28982;&#32780;&#65292;&#23545;&#25968;&#25454;&#26631;&#27880;&#30340;&#38656;&#27714;&#36890;&#24120;&#38590;&#20197;&#33719;&#21462;&#65292;&#36825;&#23548;&#33268;&#20102;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#27169;&#22411;&#22312;&#22522;&#20110;&#39046;&#22495;&#19987;&#23478;&#30693;&#35782;&#36827;&#34892;&#23458;&#25143;&#20998;&#21106;&#26102;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11066v1 Announce Type: new  Abstract: Financial inclusion ensures that individuals have access to financial products and services that meet their needs. As a key contributing factor to economic growth and investment opportunity, financial inclusion increases consumer spending and consequently business development. It has been shown that institutions are more profitable when they provide marginalised social groups access to financial services. Customer segmentation based on consumer transaction data is a well-known strategy used to promote financial inclusion. While the required data is available to modern institutions, the challenge remains that segment annotations are usually difficult and/or expensive to obtain. This prevents the usage of time series classification models for customer segmentation based on domain expert knowledge. As a result, clustering is an attractive alternative to partition customers into homogeneous groups based on the spending behaviour encoded with
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RAD&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39046;&#22495;&#26631;&#27880;&#30340;&#27491;&#21017;&#21270;&#26469;&#35757;&#32451;&#40065;&#26834;&#30340;&#26368;&#21518;&#19968;&#23618;&#20998;&#31867;&#22120;&#65292;&#26080;&#38656;&#26174;&#24335;&#30340;&#39046;&#22495;&#26631;&#27880;&#65292;&#22312;&#20855;&#26377;&#39046;&#22495;&#26631;&#31614;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20248;&#36234;&#12290;</title><link>https://arxiv.org/abs/2402.11039</link><description>&lt;p&gt;
&#20855;&#26377;&#39046;&#22495;&#26631;&#31614;&#22122;&#22768;&#30340;&#20122;&#32676;&#20307;&#36716;&#31227;&#40065;&#26834;&#24615;&#36890;&#36807;&#39046;&#22495;&#26631;&#27880;&#30340;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Robustness to Subpopulation Shift with Domain Label Noise via Regularized Annotation of Domains
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11039
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RAD&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39046;&#22495;&#26631;&#27880;&#30340;&#27491;&#21017;&#21270;&#26469;&#35757;&#32451;&#40065;&#26834;&#30340;&#26368;&#21518;&#19968;&#23618;&#20998;&#31867;&#22120;&#65292;&#26080;&#38656;&#26174;&#24335;&#30340;&#39046;&#22495;&#26631;&#27880;&#65292;&#22312;&#20855;&#26377;&#39046;&#22495;&#26631;&#31614;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#38024;&#23545;&#26368;&#20248;&#32452;&#20934;&#30830;&#24615;(WGA)&#36827;&#34892;&#26368;&#21518;&#19968;&#23618;&#37325;&#26032;&#35757;&#32451;&#30340;&#26041;&#27861;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#36807;&#20110;&#20381;&#36182;&#20110;&#33391;&#22909;&#26631;&#27880;&#30340;&#32452;&#12290;&#25105;&#20204;&#29702;&#35770;&#19978;&#21644;&#23454;&#36341;&#20013;&#23637;&#31034;&#20102;&#65292;&#22522;&#20110;&#27880;&#37322;&#30340;&#25968;&#25454;&#22686;&#24378;&#20351;&#29992;&#19979;&#37319;&#26679;&#25110;&#19978;&#21152;&#26435;&#29992;&#20110;WGA&#26159;&#23481;&#26131;&#21463;&#21040;&#39046;&#22495;&#26631;&#27880;&#22122;&#22768;&#24178;&#25200;&#65292;&#22312;&#39640;&#22122;&#22768;&#24773;&#20917;&#19979;&#25509;&#36817;&#20351;&#29992;&#21407;&#22987;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;WGA&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#39046;&#22495;&#26631;&#27880;&#27491;&#21017;&#21270;(RAD)&#26469;&#35757;&#32451;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#26368;&#21518;&#19968;&#23618;&#20998;&#31867;&#22120;&#65292;&#32780;&#26080;&#38656;&#26126;&#30830;&#30340;&#39046;&#22495;&#26631;&#27880;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;RAD&#19982;&#20854;&#20182;&#26368;&#36817;&#25552;&#20986;&#30340;&#26080;&#39046;&#22495;&#26631;&#27880;&#25216;&#26415;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#21363;&#20351;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#20165;&#26377;5%&#30340;&#22122;&#22768;&#65292;RAD&#20063;&#22312;&#20960;&#20010;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#38598;&#19978;&#32988;&#36807;&#26368;&#20808;&#36827;&#30340;&#20381;&#36182;&#27880;&#37322;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11039v1 Announce Type: new  Abstract: Existing methods for last layer retraining that aim to optimize worst-group accuracy (WGA) rely heavily on well-annotated groups in the training data. We show, both in theory and practice, that annotation-based data augmentations using either downsampling or upweighting for WGA are susceptible to domain annotation noise, and in high-noise regimes approach the WGA of a model trained with vanilla empirical risk minimization. We introduce Regularized Annotation of Domains (RAD) in order to train robust last layer classifiers without the need for explicit domain annotations. Our results show that RAD is competitive with other recently proposed domain annotation-free techniques. Most importantly, RAD outperforms state-of-the-art annotation-reliant methods even with only 5% noise in the training data for several publicly available datasets.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#36523;&#20307;&#24314;&#27169;&#20026;&#26102;&#31354;&#22270;&#24182;&#24341;&#20837;&#19968;&#20010;&#31934;&#28860;&#32593;&#32476;&#65292;&#26469;&#23454;&#29616;&#23545;&#36974;&#25377;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;3D&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2402.11036</link><description>&lt;p&gt;
&#20855;&#26377;&#36974;&#25377;&#40065;&#26834;&#24615;&#30340;3D&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Occlusion Resilient 3D Human Pose Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11036
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#36523;&#20307;&#24314;&#27169;&#20026;&#26102;&#31354;&#22270;&#24182;&#24341;&#20837;&#19968;&#20010;&#31934;&#28860;&#32593;&#32476;&#65292;&#26469;&#23454;&#29616;&#23545;&#36974;&#25377;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;3D&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36974;&#25377;&#20173;&#28982;&#26159;&#21333;&#25668;&#20687;&#22836;&#35270;&#39057;&#24207;&#21015;&#20013;3D&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#30340;&#20851;&#38190;&#25361;&#25112;&#20043;&#19968;&#12290;&#34429;&#28982;&#26102;&#38388;&#19968;&#33268;&#24615;&#24050;&#34987;&#24191;&#27867;&#29992;&#20110;&#20943;&#36731;&#20854;&#24433;&#21709;&#65292;&#20294;&#25991;&#29486;&#20013;&#30340;&#29616;&#26377;&#31639;&#27861;&#24182;&#27809;&#26377;&#26126;&#30830;&#23545;&#20854;&#36827;&#34892;&#24314;&#27169;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#21464;&#24418;&#30340;&#36523;&#20307;&#34920;&#31034;&#20026;&#26102;&#31354;&#22270;&#26469;&#24212;&#29992;&#36825;&#19968;&#28857;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#19968;&#20010;&#31934;&#28860;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#23545;&#35813;&#22270;&#25191;&#34892;&#22270;&#21367;&#31215;&#20197;&#36755;&#20986;3D&#23039;&#21183;&#12290;&#20026;&#20102;&#30830;&#20445;&#23545;&#36974;&#25377;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#32452;&#20108;&#36827;&#21046;&#25513;&#30721;&#26469;&#35757;&#32451;&#36825;&#20010;&#32593;&#32476;&#65292;&#25105;&#20204;&#23558;&#20854;&#29992;&#20110;&#31105;&#29992;&#19968;&#20123;&#36793;&#65292;&#22914;&#21435;&#38500;&#25216;&#26415;&#20013;&#12290;&#23454;&#38469;&#19978;&#65292;&#25105;&#20204;&#27169;&#25311;&#20102;&#19968;&#20123;&#20851;&#33410;&#21487;&#33021;&#22312;&#19968;&#27573;&#26102;&#38388;&#20869;&#34987;&#38544;&#34255;&#30340;&#20107;&#23454;&#65292;&#24182;&#35757;&#32451;&#32593;&#32476;&#20813;&#30123;&#20110;&#27492;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19982;&#20174;&#21333;&#25668;&#20687;&#22836;&#24207;&#21015;&#25512;&#26029;&#23039;&#21183;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#30456;&#27604;&#65292;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11036v1 Announce Type: cross  Abstract: Occlusions remain one of the key challenges in 3D body pose estimation from single-camera video sequences. Temporal consistency has been extensively used to mitigate their impact but the existing algorithms in the literature do not explicitly model them.   Here, we apply this by representing the deforming body as a spatio-temporal graph. We then introduce a refinement network that performs graph convolutions over this graph to output 3D poses. To ensure robustness to occlusions, we train this network with a set of binary masks that we use to disable some of the edges as in drop-out techniques.   In effect, we simulate the fact that some joints can be hidden for periods of time and train the network to be immune to that. We demonstrate the effectiveness of this approach compared to state-of-the-art techniques that infer poses from single-camera sequences.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#31232;&#30095;&#23376;&#31354;&#38388;&#21464;&#20998;&#25512;&#26029;&#65288;SSVI&#65289;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22312;&#35757;&#32451;&#21644;&#25512;&#26029;&#38454;&#27573;&#22987;&#32456;&#20445;&#25345;&#39640;&#24230;&#31232;&#30095;&#30340;&#36125;&#21494;&#26031;&#27169;&#22411;&#30340;&#20840;&#31232;&#30095;BNN&#26694;&#26550;</title><link>https://arxiv.org/abs/2402.11025</link><description>&lt;p&gt;
&#20351;&#29992;&#31232;&#30095;&#23376;&#31354;&#38388;&#21464;&#20998;&#25512;&#26029;&#35757;&#32451;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Training Bayesian Neural Networks with Sparse Subspace Variational Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11025
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#31232;&#30095;&#23376;&#31354;&#38388;&#21464;&#20998;&#25512;&#26029;&#65288;SSVI&#65289;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22312;&#35757;&#32451;&#21644;&#25512;&#26029;&#38454;&#27573;&#22987;&#32456;&#20445;&#25345;&#39640;&#24230;&#31232;&#30095;&#30340;&#36125;&#21494;&#26031;&#27169;&#22411;&#30340;&#20840;&#31232;&#30095;BNN&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;BNN&#65289;&#25552;&#20379;&#20102;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#20294;&#20195;&#20215;&#26159;&#22823;&#24133;&#22686;&#21152;&#35757;&#32451;&#21644;&#25512;&#26029;&#25104;&#26412;&#12290;&#31232;&#30095;BNN&#24050;&#34987;&#30740;&#31350;&#29992;&#20110;&#39640;&#25928;&#25512;&#26029;&#65292;&#36890;&#24120;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36880;&#28176;&#24341;&#20837;&#31232;&#30095;&#24615;&#25110;&#36890;&#36807;&#21518;&#32493;&#23545;&#23494;&#38598;BNN&#36827;&#34892;&#21387;&#32553;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#38477;&#20302;&#24040;&#22823;&#30340;&#35757;&#32451;&#25104;&#26412;&#20173;&#28982;&#26159;&#19968;&#20010;&#38590;&#39064;&#65292;&#29305;&#21035;&#26159;&#32771;&#34385;&#21040;&#38656;&#35201;&#23398;&#20064;&#19981;&#30830;&#23450;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31232;&#30095;&#23376;&#31354;&#38388;&#21464;&#20998;&#25512;&#26029;&#65288;SSVI&#65289;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22312;&#35757;&#32451;&#21644;&#25512;&#26029;&#38454;&#27573;&#22987;&#32456;&#20445;&#25345;&#39640;&#24230;&#31232;&#30095;&#30340;&#36125;&#21494;&#26031;&#27169;&#22411;&#30340;&#20840;&#31232;&#30095;BNN&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20174;&#19968;&#20010;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#20302;&#32500;&#31232;&#30095;&#23376;&#31354;&#38388;&#24320;&#22987;&#65292;&#20132;&#26367;&#20248;&#21270;&#31232;&#30095;&#23376;&#31354;&#38388;&#22522;&#21521;&#37327;&#30340;&#36873;&#25321;&#20197;&#21450;&#30456;&#20851;&#21442;&#25968;&#12290;&#23613;&#31649;&#22522;&#21521;&#37327;&#36873;&#25321;&#34987;&#25551;&#36848;&#20026;&#19968;&#20010;&#19981;&#21487;&#24494;&#20998;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#36817;&#20284;&#27714;&#35299;&#35813;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11025v1 Announce Type: new  Abstract: Bayesian neural networks (BNNs) offer uncertainty quantification but come with the downside of substantially increased training and inference costs. Sparse BNNs have been investigated for efficient inference, typically by either slowly introducing sparsity throughout the training or by post-training compression of dense BNNs. The dilemma of how to cut down massive training costs remains, particularly given the requirement to learn about the uncertainty. To solve this challenge, we introduce Sparse Subspace Variational Inference (SSVI), the first fully sparse BNN framework that maintains a consistently highly sparse Bayesian model throughout the training and inference phases. Starting from a randomly initialized low-dimensional sparse subspace, our approach alternately optimizes the sparse subspace basis selection and its associated parameters. While basis selection is characterized as a non-differentiable problem, we approximate the opti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#30495;&#23454;&#19990;&#30028;&#35821;&#26009;&#24211;&#33258;&#21160;&#26816;&#27979;&#21644;&#20998;&#26512;&#38544;&#31169;&#25919;&#31574;&#20013;&#25968;&#25454;&#23454;&#36341;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#21644;&#26696;&#20363;&#30740;&#31350;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.11006</link><description>&lt;p&gt;
&#20351;&#29992;&#30495;&#23454;&#19990;&#30028;&#35821;&#26009;&#24211;&#33258;&#21160;&#26816;&#27979;&#21644;&#20998;&#26512;&#25968;&#25454;&#23454;&#36341;
&lt;/p&gt;
&lt;p&gt;
Automated Detection and Analysis of Data Practices Using A Real-World Corpus
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11006
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#30495;&#23454;&#19990;&#30028;&#35821;&#26009;&#24211;&#33258;&#21160;&#26816;&#27979;&#21644;&#20998;&#26512;&#38544;&#31169;&#25919;&#31574;&#20013;&#25968;&#25454;&#23454;&#36341;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#21644;&#26696;&#20363;&#30740;&#31350;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#31169;&#25919;&#31574;&#23545;&#20110;&#21578;&#30693;&#29992;&#25143;&#25968;&#25454;&#23454;&#36341;&#33267;&#20851;&#37325;&#35201;&#65292;&#28982;&#32780;&#20854;&#38271;&#24230;&#21644;&#22797;&#26434;&#24615;&#32463;&#24120;&#38459;&#27490;&#29992;&#25143;&#38405;&#35835;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#21644;&#21487;&#35270;&#21270;&#38544;&#31169;&#25919;&#31574;&#20013;&#19981;&#21516;&#32454;&#33410;&#32423;&#21035;&#30340;&#25968;&#25454;&#23454;&#36341;&#12290;&#21033;&#29992;&#26469;&#33258;ToS;DR&#24179;&#21488;&#30340;&#20247;&#21253;&#27880;&#37322;&#65292;&#25105;&#20204;&#23581;&#35797;&#20351;&#29992;&#21508;&#31181;&#26041;&#27861;&#21305;&#37197;&#25919;&#31574;&#25688;&#24405;&#19982;&#39044;&#23450;&#20041;&#30340;&#25968;&#25454;&#23454;&#36341;&#25551;&#36848;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36827;&#34892;&#20102;&#26696;&#20363;&#30740;&#31350;&#65292;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#30495;&#23454;&#25919;&#31574;&#20013;&#30340;&#25928;&#26524;&#65292;&#35777;&#26126;&#20854;&#22312;&#31616;&#21270;&#22797;&#26434;&#25919;&#31574;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#20934;&#30830;&#21305;&#37197;&#25968;&#25454;&#23454;&#36341;&#25551;&#36848;&#21644;&#25919;&#31574;&#25688;&#24405;&#65292;&#26377;&#21161;&#20110;&#21521;&#29992;&#25143;&#21576;&#29616;&#31616;&#21270;&#30340;&#38544;&#31169;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11006v1 Announce Type: cross  Abstract: Privacy policies are crucial for informing users about data practices, yet their length and complexity often deter users from reading them. In this paper, we propose an automated approach to identify and visualize data practices within privacy policies at different levels of detail. Leveraging crowd-sourced annotations from the ToS;DR platform, we experiment with various methods to match policy excerpts with predefined data practice descriptions. We further conduct a case study to evaluate our approach on a real-world policy, demonstrating its effectiveness in simplifying complex policies. Experiments show that our approach accurately matches data practice descriptions with policy excerpts, facilitating the presentation of simplified privacy information to users.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#36890;&#36807;&#35757;&#32451;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#20934;&#30830;&#39044;&#27979;&#19979;&#19968;&#20010;&#26631;&#35760;&#27010;&#29575;&#65292;&#24182;&#21457;&#29616;&#20102;&#19968;&#20010;&#22810;&#38454;&#27573;&#30340;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2402.11004</link><description>&lt;p&gt;
&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#39532;&#23572;&#21487;&#22827;&#38142;&#30340;&#32479;&#35745;&#24402;&#32435;&#22836;&#30340;&#28436;&#21464;
&lt;/p&gt;
&lt;p&gt;
The Evolution of Statistical Induction Heads: In-Context Learning Markov Chains
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11004
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#36890;&#36807;&#35757;&#32451;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#20934;&#30830;&#39044;&#27979;&#19979;&#19968;&#20010;&#26631;&#35760;&#27010;&#29575;&#65292;&#24182;&#21457;&#29616;&#20102;&#19968;&#20010;&#22810;&#38454;&#27573;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#27169;&#20223;&#20854;&#36755;&#20837;&#27169;&#24335;&#30340;&#25991;&#26412;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#39532;&#23572;&#21487;&#22827;&#38142;&#24207;&#21015;&#24314;&#27169;&#20219;&#21153;&#65292;&#20197;&#30740;&#31350;&#36825;&#31181;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#33021;&#21147;&#26159;&#22914;&#20309;&#20986;&#29616;&#30340;&#12290;&#22312;&#25105;&#20204;&#30340;&#35774;&#23450;&#20013;&#65292;&#27599;&#20010;&#20363;&#23376;&#26159;&#20174;&#19968;&#20010;&#20174;&#39532;&#23572;&#21487;&#22827;&#38142;&#20808;&#39564;&#20998;&#24067;&#20013;&#25277;&#21462;&#30340;&#39532;&#23572;&#21487;&#22827;&#38142;&#20013;&#25277;&#21462;&#30340;&#12290;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#35757;&#32451;&#30340;Transformer&#24418;&#25104;&#20102;&#35745;&#31639;&#32473;&#23450;&#19978;&#19979;&#25991;&#21452;&#23383;&#27597;&#32479;&#35745;&#30340;&#20934;&#30830;&#19979;&#19968;&#20010;&#26631;&#35760;&#27010;&#29575;&#30340;\emph{&#32479;&#35745;&#24402;&#32435;&#22836;}&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#27169;&#22411;&#32463;&#36807;&#22810;&#20010;&#38454;&#27573;&#65306;&#22312;&#21021;&#22987;&#38454;&#27573;&#65292;&#39044;&#27979;&#26159;&#22343;&#21248;&#30340;&#65307;&#20182;&#20204;&#23398;&#20250;&#20351;&#29992;&#19978;&#19979;&#25991;&#21333;&#26631;&#35760;&#32479;&#35745;&#65288;&#19968;&#20803;&#32452;&#65289;&#36827;&#34892;&#27425;&#20248;&#39044;&#27979;&#65307;&#28982;&#21518;&#65292;&#24555;&#36895;&#36807;&#28193;&#21040;&#27491;&#30830;&#30340;&#19978;&#19979;&#25991;&#21452;&#23383;&#27597;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#23545;&#36825;&#20010;&#22810;&#38454;&#27573;&#36807;&#31243;&#36827;&#34892;&#20102;&#32463;&#39564;&#21644;&#29702;&#35770;&#35843;&#26597;&#65292;&#26174;&#31034;&#20102;&#25104;&#21151;&#23398;&#20064;&#26159;&#22914;&#20309;&#26469;&#33258;&#20110;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11004v1 Announce Type: new  Abstract: Large language models have the ability to generate text that mimics patterns in their inputs. We introduce a simple Markov Chain sequence modeling task in order to study how this in-context learning (ICL) capability emerges. In our setting, each example is sampled from a Markov chain drawn from a prior distribution over Markov chains. Transformers trained on this task form \emph{statistical induction heads} which compute accurate next-token probabilities given the bigram statistics of the context. During the course of training, models pass through multiple phases: after an initial stage in which predictions are uniform, they learn to sub-optimally predict using in-context single-token statistics (unigrams); then, there is a rapid phase transition to the correct in-context bigram solution. We conduct an empirical and theoretical investigation of this multi-phase process, showing how successful learning results from the interaction between
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#27835;&#30103;&#26041;&#26696;&#38656;&#27880;&#24847;&#24739;&#32773;&#21097;&#20313;&#29983;&#21629;&#21644;&#21512;&#24182;&#30151;&#65292;&#30740;&#31350;&#21033;&#29992;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#26500;&#24314;&#22810;&#31867;&#20998;&#31867;&#27169;&#22411;&#26469;&#39044;&#27979;&#32769;&#24180;2&#22411;&#31958;&#23615;&#30149;&#24739;&#32773;&#30340;&#27515;&#20129;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.10999</link><description>&lt;p&gt;
&#20998;&#26512;&#21644;&#39044;&#27979;2&#22411;&#31958;&#23615;&#30149;&#32769;&#24180;&#24739;&#32773;&#30340;&#22810;&#31867;&#20998;&#31867;&#27515;&#20129;&#29575;
&lt;/p&gt;
&lt;p&gt;
Analysis and Mortality Prediction using Multiclass Classification for Older Adults with Type 2 Diabetes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10999
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#27835;&#30103;&#26041;&#26696;&#38656;&#27880;&#24847;&#24739;&#32773;&#21097;&#20313;&#29983;&#21629;&#21644;&#21512;&#24182;&#30151;&#65292;&#30740;&#31350;&#21033;&#29992;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#26500;&#24314;&#22810;&#31867;&#20998;&#31867;&#27169;&#22411;&#26469;&#39044;&#27979;&#32769;&#24180;2&#22411;&#31958;&#23615;&#30149;&#24739;&#32773;&#30340;&#27515;&#20129;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#21512;&#36866;&#30340;&#27835;&#30103;&#26041;&#26696;&#26469;&#31649;&#29702;&#31958;&#23615;&#30149;&#35201;&#27714;&#21307;&#25252;&#20154;&#21592;&#27880;&#24847;&#24739;&#32773;&#21097;&#20313;&#30340;&#29983;&#21629;&#20197;&#21450;&#24433;&#21709;&#20182;&#20204;&#30340;&#21512;&#24182;&#30151;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#20102;&#19968;&#20010;&#21253;&#21547;68&#20010;&#28508;&#22312;&#27515;&#20129;&#39044;&#27979;&#22240;&#23376;&#30340;&#32467;&#26500;&#21270;&#25968;&#25454;&#38598;&#65292;&#38024;&#23545;275,190&#21517;&#24180;&#40836;&#22312;65&#23681;&#25110;&#20197;&#19978;&#30340;&#32654;&#22269;&#36864;&#20237;&#20891;&#20154;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#36890;&#36807;&#23558;&#20004;&#20010;&#21407;&#22987;&#30446;&#26631;&#21464;&#37327;&#32452;&#21512;&#36215;&#26469;&#21019;&#36896;&#20102;&#19968;&#20010;&#26032;&#30340;&#30446;&#26631;&#21464;&#37327;&#12290;&#36890;&#36807;&#31163;&#25955;&#21270;&#36830;&#32493;&#21464;&#37327;&#26469;&#22788;&#29702;&#24322;&#24120;&#20540;&#65292;&#23545;&#20998;&#31867;&#21464;&#37327;&#36827;&#34892;&#20102;&#34394;&#25311;&#32534;&#30721;&#12290;&#36890;&#36807;&#38543;&#26426;&#27424;&#37319;&#26679;&#23454;&#29616;&#20102;&#31867;&#24179;&#34913;&#12290;&#20351;&#29992;&#24102;LASSO&#30340;&#22810;&#39033;&#24335;&#36923;&#36753;&#22238;&#24402;&#24314;&#31435;&#20102;&#22522;&#20934;&#22238;&#24402;&#27169;&#22411;&#12290;&#37319;&#29992;&#21345;&#26041;&#26816;&#39564;&#21644;&#20449;&#24687;&#22686;&#30410;&#20316;&#20026;&#22522;&#20110;&#36807;&#28388;&#30340;&#29305;&#24449;&#36873;&#25321;&#25216;&#26415;&#12290;&#20998;&#31867;&#22120;&#21253;&#25324;&#22810;&#39033;&#24335;&#36923;&#36753;&#22238;&#24402;&#12289;&#38543;&#26426;&#26862;&#26519;&#12289;&#26497;&#31471;&#26799;&#24230;&#25552;&#21319;&#65288;XGB&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10999v1 Announce Type: cross  Abstract: Designing proper treatment plans to manage diabetes requires health practitioners to pay heed to the individuals remaining life along with the comorbidities affecting them. Older adults with Type 2 Diabetes Mellitus (T2DM) are prone to experience premature death or even hypoglycaemia. The structured dataset utilized has 68 potential mortality predictors for 275,190 diabetic U.S. military Veterans aged 65 years or older. A new target variable is invented by combining the two original target variables. Outliers are handled by discretizing the continuous variables. Categorical variables have been dummy encoded. Class balancing is achieved by random under-sampling. A benchmark regression model is built using Multinomial Logistic Regression with LASSO. Chi-Squared and Information Gain are the filter-based feature selection techniques utilized. Classifiers such as Multinomial Logistic Regression, Random Forest, Extreme Gradient Boosting (XGB
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24046;&#20998;&#21160;&#24577;&#36923;&#36753;&#19982;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#30456;&#32467;&#21512;&#30340;VerSAILLE&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#31995;&#32479;&#22312;&#26080;&#38480;&#26102;&#38388;&#33539;&#22260;&#19978;&#30340;&#23433;&#20840;&#24615;&#35777;&#26126;&#12290;</title><link>https://arxiv.org/abs/2402.10998</link><description>&lt;p&gt;
&#36890;&#36807;&#24046;&#20998;&#21160;&#24577;&#36923;&#36753;&#30830;&#20445;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#30340;&#23433;&#20840;&#24615;
&lt;/p&gt;
&lt;p&gt;
Provably Safe Neural Network Controllers via Differential Dynamic Logic
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10998
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24046;&#20998;&#21160;&#24577;&#36923;&#36753;&#19982;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#30456;&#32467;&#21512;&#30340;VerSAILLE&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#31995;&#32479;&#22312;&#26080;&#38480;&#26102;&#38388;&#33539;&#22260;&#19978;&#30340;&#23433;&#20840;&#24615;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#20316;&#20026;&#38754;&#21521;&#30446;&#26631;&#30340;&#25511;&#21046;&#22120;&#22312;&#32593;&#32476;&#29289;&#29702;&#31995;&#32479;&#20013;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#39564;&#35777;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#25511;&#21046;&#31995;&#32479;&#65288;NNCS&#65289;&#30340;&#23433;&#20840;&#24615;&#23545;&#20110;&#23454;&#38469;&#24212;&#29992;NN&#26469;&#35828;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#24403;&#38656;&#35201;&#23545;&#26080;&#30028;&#26102;&#38388;&#33539;&#22260;&#36827;&#34892;&#23433;&#20840;&#24615;&#39564;&#35777;&#26102;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;VerSAILLE&#65288;&#36890;&#36807;&#36923;&#36753;&#38142;&#25509;&#21253;&#39564;&#35777;&#30340;&#21487;&#39564;&#35777;&#23433;&#20840;&#20154;&#24037;&#26234;&#33021;&#65289;&#65306;&#36825;&#26159;&#24046;&#20998;&#21160;&#24577;&#36923;&#36753;&#65288;dL&#65289;&#21644;NN&#39564;&#35777;&#32452;&#21512;&#30340;&#31532;&#19968;&#31181;&#26041;&#27861;&#12290;&#36890;&#36807;&#21512;&#20316;&#65292;&#25105;&#20204;&#21487;&#20197;&#21033;&#29992;NN&#39564;&#35777;&#24037;&#20855;&#30340;&#25928;&#29575;&#65292;&#21516;&#26102;&#20445;&#30041;dL&#30340;&#20005;&#35880;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25511;&#21046;&#22120;&#20449;&#23553;&#30340;&#23433;&#20840;&#24615;&#35777;&#26126;&#65292;&#20197;&#35777;&#26126;&#26080;&#38480;&#26102;&#38388;&#33539;&#22260;&#19978;&#20855;&#20307;NNCS&#30340;&#23433;&#20840;&#24615;&#12290;VerSAILLE&#23548;&#33268;&#30340;NN&#39564;&#35777;&#23646;&#24615;&#36890;&#24120;&#38656;&#35201;&#38750;&#32447;&#24615;&#31639;&#26415;&#65292;&#32780;&#39640;&#25928;&#30340;NN&#39564;&#35777;&#24037;&#20855;&#20165;&#25903;&#25345;&#32447;&#24615;&#31639;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10998v1 Announce Type: cross  Abstract: While neural networks (NNs) have a large potential as goal-oriented controllers for Cyber-Physical Systems, verifying the safety of neural network based control systems (NNCSs) poses significant challenges for the practical use of NNs -- especially when safety is needed for unbounded time horizons. One reason for this is the intractability of NN and hybrid system analysis. We introduce VerSAILLE (Verifiably Safe AI via Logically Linked Envelopes): The first approach for the combination of differential dynamic logic (dL) and NN verification. By joining forces, we can exploit the efficiency of NN verification tools while retaining the rigor of dL. We reflect a safety proof for a controller envelope in an NN to prove the safety of concrete NNCS on an infinite-time horizon. The NN verification properties resulting from VerSAILLE typically require nonlinear arithmetic while efficient NN verification tools merely support linear arithmetic. T
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#36129;&#29486;&#30340;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#21160;&#24577;&#35843;&#25972;&#25509;&#25910;&#21040;&#30340;&#26356;&#26032;&#30340;&#22788;&#29702;&#26041;&#24335;&#65292;&#20197;&#35299;&#20915;&#29616;&#23454;&#24773;&#20917;&#19979;&#21516;&#27493;&#19978;&#20256;&#25968;&#25454;&#21487;&#33021;&#20986;&#29616;&#30340;&#32531;&#24930;&#21644;&#19981;&#21487;&#38752;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.10991</link><description>&lt;p&gt;
&#21152;&#36895;&#21322;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Accelerating Semi-Asynchronous Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10991
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#36129;&#29486;&#30340;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#21160;&#24577;&#35843;&#25972;&#25509;&#25910;&#21040;&#30340;&#26356;&#26032;&#30340;&#22788;&#29702;&#26041;&#24335;&#65292;&#20197;&#35299;&#20915;&#29616;&#23454;&#24773;&#20917;&#19979;&#21516;&#27493;&#19978;&#20256;&#25968;&#25454;&#21487;&#33021;&#20986;&#29616;&#30340;&#32531;&#24930;&#21644;&#19981;&#21487;&#38752;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#33539;&#20363;&#65292;&#20801;&#35768;&#23458;&#25143;&#31471;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21516;&#26102;&#22312;&#20854;&#25968;&#25454;&#19978;&#35757;&#32451;&#27169;&#22411;&#12290;&#29616;&#26377;&#30340;FL&#31639;&#27861;&#65292;&#22914;Federated Averaging&#65288;FedAvg&#65289;&#21450;&#20854;&#21464;&#31181;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#24050;&#32463;&#34987;&#35777;&#26126;&#25910;&#25947;&#33391;&#22909;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#23458;&#25143;&#31471;&#20197;&#21516;&#27493;&#26041;&#24335;&#23558;&#20854;&#26412;&#22320;&#26356;&#26032;&#19978;&#20256;&#33267;&#26381;&#21153;&#22120;&#65292;&#36825;&#22312;&#29616;&#23454;&#24773;&#20917;&#19979;&#21487;&#33021;&#20250;&#21464;&#24471;&#32531;&#24930;&#21644;&#19981;&#21487;&#38752;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#20102;&#24322;&#27493;FL&#26041;&#27861;&#65292;&#20801;&#35768;&#23458;&#25143;&#31471;&#32487;&#32493;&#20351;&#29992;&#38472;&#26087;&#30340;&#20840;&#23616;&#27169;&#22411;&#23545;&#20854;&#26412;&#22320;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;&#26041;&#27861;&#20165;&#20165;&#32858;&#21512;&#20102;&#25152;&#26377;&#25509;&#25910;&#21040;&#30340;&#26356;&#26032;&#65292;&#32780;&#27809;&#26377;&#32771;&#34385;&#20854;&#30456;&#23545;&#36129;&#29486;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#25910;&#25947;&#36895;&#24230;&#21464;&#24930;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#36129;&#29486;&#30340;&#24322;&#27493;FL&#26041;&#27861;&#65292;&#32771;&#34385;&#20102;&#25509;&#25910;&#21040;&#30340;&#26356;&#26032;&#30340;&#38472;&#26087;&#31243;&#24230;&#21644;&#32479;&#35745;&#24322;&#36136;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21160;&#24577;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10991v1 Announce Type: cross  Abstract: Federated Learning (FL) is a distributed machine learning paradigm that allows clients to train models on their data while preserving their privacy. FL algorithms, such as Federated Averaging (FedAvg) and its variants, have been shown to converge well in many scenarios. However, these methods require clients to upload their local updates to the server in a synchronous manner, which can be slow and unreliable in realistic FL settings. To address this issue, researchers have developed asynchronous FL methods that allow clients to continue training on their local data using a stale global model. However, most of these methods simply aggregate all of the received updates without considering their relative contributions, which can slow down convergence. In this paper, we propose a contribution-aware asynchronous FL method that takes into account the staleness and statistical heterogeneity of the received updates. Our method dynamically adju
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#32593;&#32476;&#28431;&#27934;&#30340;&#37327;&#23376;&#21551;&#21457;&#24335;&#20998;&#26512;&#25581;&#31034;&#20102;&#36755;&#20837;&#20849;&#36717;&#22312;&#31995;&#32479;&#25915;&#20987;&#20013;&#30340;&#20316;&#29992;&#65292;&#25581;&#31034;&#20102;&#32593;&#32476;&#32467;&#26500;&#20013;&#30340;&#31995;&#32479;&#33030;&#24369;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#19982;&#37327;&#23376;&#29289;&#29702;&#19981;&#30830;&#23450;&#24615;&#21407;&#29702;&#20043;&#38388;&#30340;&#25968;&#23398;&#19968;&#33268;&#24615;&#65292;&#31361;&#26174;&#20102;&#31070;&#32463;&#32593;&#32476;&#31995;&#32479;&#30340;&#22266;&#26377;&#33030;&#24369;&#24615;&#21644;&#28508;&#22312;&#30340;&#36328;&#23398;&#31185;&#39046;&#22495;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2402.10983</link><description>&lt;p&gt;
&#37327;&#23376;&#21551;&#21457;&#24335;&#20998;&#26512;&#31070;&#32463;&#32593;&#32476;&#28431;&#27934;&#65306;&#21327;&#21464;&#37327;&#22312;&#31995;&#32479;&#25915;&#20987;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Quantum-Inspired Analysis of Neural Network Vulnerabilities: The Role of Conjugate Variables in System Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10983
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#28431;&#27934;&#30340;&#37327;&#23376;&#21551;&#21457;&#24335;&#20998;&#26512;&#25581;&#31034;&#20102;&#36755;&#20837;&#20849;&#36717;&#22312;&#31995;&#32479;&#25915;&#20987;&#20013;&#30340;&#20316;&#29992;&#65292;&#25581;&#31034;&#20102;&#32593;&#32476;&#32467;&#26500;&#20013;&#30340;&#31995;&#32479;&#33030;&#24369;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#19982;&#37327;&#23376;&#29289;&#29702;&#19981;&#30830;&#23450;&#24615;&#21407;&#29702;&#20043;&#38388;&#30340;&#25968;&#23398;&#19968;&#33268;&#24615;&#65292;&#31361;&#26174;&#20102;&#31070;&#32463;&#32593;&#32476;&#31995;&#32479;&#30340;&#22266;&#26377;&#33030;&#24369;&#24615;&#21644;&#28508;&#22312;&#30340;&#36328;&#23398;&#31185;&#39046;&#22495;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#23545;&#20110;&#24494;&#23567;&#30340;&#12289;&#38750;&#38543;&#26426;&#30340;&#25200;&#21160;&#23637;&#31034;&#20986;&#22266;&#26377;&#30340;&#33030;&#24369;&#24615;&#65292;&#36825;&#20123;&#28431;&#27934;&#34987;&#31216;&#20026;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;&#36825;&#20123;&#25915;&#20987;&#28304;&#33258;&#25439;&#22833;&#20989;&#25968;&#30456;&#23545;&#36755;&#20837;&#30340;&#26799;&#24230;&#65292;&#34987;&#35782;&#21035;&#20026;&#36755;&#20837;&#20849;&#36717;&#65292;&#25581;&#31034;&#20102;&#32593;&#32476;&#32467;&#26500;&#20869;&#37096;&#30340;&#31995;&#32479;&#33030;&#24369;&#24615;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#36825;&#31181;&#26426;&#21046;&#19982;&#37327;&#23376;&#29289;&#29702;&#30340;&#19981;&#30830;&#23450;&#24615;&#21407;&#29702;&#20043;&#38388;&#21576;&#29616;&#20986;&#25968;&#23398;&#19978;&#30340;&#19968;&#33268;&#24615;&#65292;&#20026;&#27492;&#31181;&#36328;&#23398;&#31185;&#24615;&#36136;&#25237;&#19979;&#20102;&#19968;&#36947;&#21069;&#25152;&#26410;&#26377;&#30340;&#20809;&#12290;&#31070;&#32463;&#32593;&#32476;&#31995;&#32479;&#20869;&#22312;&#30340;&#25935;&#24863;&#24615;&#36890;&#24120;&#26159;&#22266;&#26377;&#30340;&#65292;&#19981;&#20165;&#31361;&#26174;&#20102;&#36825;&#20123;&#32593;&#32476;&#30340;&#22266;&#26377;&#33030;&#24369;&#24615;&#65292;&#26356;&#26263;&#31034;&#30528;&#22312;&#29702;&#35299;&#36825;&#20123;&#40657;&#21283;&#32593;&#32476;&#26041;&#38754;&#30340;&#36328;&#23398;&#31185;&#39046;&#22495;&#30340;&#28508;&#22312;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10983v1 Announce Type: new  Abstract: Neural networks demonstrate inherent vulnerability to small, non-random perturbations, emerging as adversarial attacks. Such attacks, born from the gradient of the loss function relative to the input, are discerned as input conjugates, revealing a systemic fragility within the network structure. Intriguingly, a mathematical congruence manifests between this mechanism and the quantum physics' uncertainty principle, casting light on a hitherto unanticipated interdisciplinarity. This inherent susceptibility within neural network systems is generally intrinsic, highlighting not only the innate vulnerability of these networks but also suggesting potential advancements in the interdisciplinary area for understanding these black-box networks.
&lt;/p&gt;</description></item><item><title>&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#39044;&#27979;&#30701;&#26399;&#30005;&#21147;&#38656;&#27714;&#30340;MATLAB&#24037;&#20855;&#31665;&#65292;&#21487;&#20197;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2402.10982</link><description>&lt;p&gt;
mshw&#65292;&#19968;&#31181;&#22522;&#20110;&#22810;&#23395;&#33410;Holt-Winters&#30340;&#39044;&#27979;&#24211;&#65292;&#29992;&#20110;&#39044;&#27979;&#30701;&#26399;&#30005;&#21147;&#38656;&#27714;
&lt;/p&gt;
&lt;p&gt;
mshw, a forecasting library to predict short-term electricity demand based on multiple seasonal Holt-Winters
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10982
&lt;/p&gt;
&lt;p&gt;
&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#39044;&#27979;&#30701;&#26399;&#30005;&#21147;&#38656;&#27714;&#30340;MATLAB&#24037;&#20855;&#31665;&#65292;&#21487;&#20197;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36755;&#30005;&#31995;&#32479;&#36816;&#33829;&#21830;&#36234;&#26469;&#36234;&#38656;&#35201;&#26356;&#20934;&#30830;&#30340;&#30005;&#21147;&#38656;&#27714;&#39044;&#27979;&#12290;&#30446;&#21069;&#30340;&#30005;&#21147;&#31995;&#32479;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#38656;&#35201;&#38656;&#27714;&#39044;&#27979;&#65292;&#20197;&#20415;&#30005;&#21147;&#24066;&#22330;&#21046;&#23450;&#30005;&#21147;&#20215;&#26684;&#20197;&#21450;&#29983;&#20135;&#21333;&#20803;&#30340;&#32534;&#31243;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#30005;&#21147;&#38656;&#27714;&#39044;&#27979;&#30340;MATLAB&#24037;&#20855;&#31665;&#65292;&#23454;&#29616;&#20102;&#22810;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10982v1 Announce Type: new  Abstract: Transmission system operators have a growing need for more accurate forecasting of electricity demand. Current electricity systems largely require demand forecasting so that the electricity market establishes electricity prices as well as the programming of production units. The companies that are part of the electrical system use exclusive software to obtain predictions, based on the use of time series and prediction tools, whether statistical or artificial intelligence. However, the most common form of prediction is based on hybrid models that use both technologies. In any case, it is software with a complicated structure, with a large number of associated variables and that requires a high computational load to make predictions. The predictions they can offer are not much better than those that simple models can offer. In this paper we present a MATLAB toolbox created for the prediction of electrical demand. The toolbox implements mul
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;ReRAM&#31070;&#32463;&#24418;&#24577;&#30005;&#36335;&#20013;&#21345;&#20303;&#25925;&#38556;&#23545;&#25512;&#29702;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#21345;&#20303;&#21644;&#21345;&#20303;&#32570;&#38519;&#23545;&#25512;&#29702;&#20934;&#30830;&#24615;&#26377;&#31867;&#20284;&#30340;&#24433;&#21709;&#65292;&#20294;&#22914;&#26524;&#22312;&#21015;&#20043;&#38388;&#23384;&#22312;&#31354;&#38388;&#32570;&#38519;&#21464;&#21270;&#65292;&#21017;&#25512;&#29702;&#20934;&#30830;&#24615;&#20250;&#21463;&#21040;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.10981</link><description>&lt;p&gt;
ReRAM&#31070;&#32463;&#24418;&#24577;&#30005;&#36335;&#38453;&#21015;&#20013;&#30340;&#21345;&#20303;&#25925;&#38556;&#21450;&#20854;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#30340;&#32416;&#27491;
&lt;/p&gt;
&lt;p&gt;
Stuck-at Faults in ReRAM Neuromorphic Circuit Array and their Correction through Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10981
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;ReRAM&#31070;&#32463;&#24418;&#24577;&#30005;&#36335;&#20013;&#21345;&#20303;&#25925;&#38556;&#23545;&#25512;&#29702;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#21345;&#20303;&#21644;&#21345;&#20303;&#32570;&#38519;&#23545;&#25512;&#29702;&#20934;&#30830;&#24615;&#26377;&#31867;&#20284;&#30340;&#24433;&#21709;&#65292;&#20294;&#22914;&#26524;&#22312;&#21015;&#20043;&#38388;&#23384;&#22312;&#31354;&#38388;&#32570;&#38519;&#21464;&#21270;&#65292;&#21017;&#25512;&#29702;&#20934;&#30830;&#24615;&#20250;&#21463;&#21040;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#30001;&#20110;&#21345;&#20303;&#25925;&#38556;&#65288;&#21345;&#20303;&#12289;&#21345;&#20303;&#21644;&#21345;&#20303;&#22312;&#26576;&#19968;&#30005;&#38459;&#20540;&#65289;&#23548;&#33268;&#30340;&#21487;&#32534;&#31243;&#38459;&#24615;&#38543;&#26426;&#23384;&#21462;&#20869;&#23384;&#65288;ReRAM&#65289;&#31070;&#32463;&#24418;&#24577;&#30005;&#36335;&#30340;&#25512;&#29702;&#20934;&#30830;&#24615;&#12290; &#20351;&#29992;Python&#25191;&#34892;&#20102;&#19968;&#20010;&#27169;&#25311;&#26694;&#26550;&#65292;&#36827;&#34892;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#65288;&#20855;&#26377;3&#20010;&#38544;&#34255;&#23618;&#12289;1&#20010;&#36755;&#20837;&#23618;&#21644;1&#20010;&#36755;&#20986;&#23618;&#30340;&#31070;&#32463;&#32593;&#32476;&#65289;&#26469;&#35782;&#21035;&#25163;&#20889;&#25968;&#23383;&#65292;&#24182;&#26500;&#24314;&#30456;&#24212;&#30340;&#23436;&#20840;&#27169;&#25311;&#31070;&#32463;&#24418;&#24577;&#30005;&#36335;&#65288;&#30001;Spectre&#27169;&#25311;&#30340;4&#20010;&#31361;&#35302;&#38453;&#21015;&#65289;&#12290; &#20351;&#29992;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;45nm&#24037;&#33402;&#24320;&#21457;&#22871;&#20214;&#65288;PDK&#65289;&#12290; &#30740;&#31350;&#20102;&#21345;&#20303;&#21644;&#21345;&#20303;&#32570;&#38519;&#23548;&#33268;&#30340;&#25512;&#29702;&#20934;&#30830;&#24615;&#38477;&#32423;&#30340;&#24046;&#24322;&#12290; &#30740;&#31350;&#20102;&#21508;&#31181;&#32570;&#38519;&#27169;&#24335;&#65292;&#21253;&#25324;&#22278;&#24418;&#12289;&#29615;&#24418;&#12289;&#34892;&#12289;&#21015;&#21644;&#22278;&#24418;&#20114;&#34917;&#32570;&#38519;&#12290; &#21457;&#29616;&#21345;&#20303;&#21644;&#21345;&#20303;&#32570;&#38519;&#23545;&#25512;&#29702;&#20934;&#30830;&#24615;&#20855;&#26377;&#31867;&#20284;&#30340;&#24433;&#21709;&#12290; &#20294;&#36824;&#21457;&#29616;&#65292;&#22914;&#26524;&#22312;&#21015;&#20043;&#38388;&#23384;&#22312;&#31354;&#38388;&#32570;&#38519;&#21464;&#21270;&#65292;&#21017;&#25512;&#29702;&#20934;&#30830;&#24615;&#23558;&#21463;&#21040;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10981v1 Announce Type: cross  Abstract: In this paper, we study the inference accuracy of the Resistive Random Access Memory (ReRAM) neuromorphic circuit due to stuck-at faults (stuck-on, stuck-off, and stuck at a certain resistive value). A simulation framework using Python is used to perform supervised machine learning (neural network with 3 hidden layers, 1 input layer, and 1 output layer) of handwritten digits and construct a corresponding fully analog neuromorphic circuit (4 synaptic arrays) simulated by Spectre. A generic 45nm Process Development Kit (PDK) was used. We study the difference in the inference accuracy degradation due to stuck-on and stuck-off defects. Various defect patterns are studied including circular, ring, row, column, and circular-complement defects. It is found that stuck-on and stuck-off defects have a similar effect on inference accuracy. However, it is also found that if there is a spatial defect variation across the columns, the inference accu
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#19982;&#37327;&#23376;&#21270;&#23398;&#21453;&#39304;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;AI&#24341;&#23548;&#30340;&#35745;&#31639;&#31579;&#36873;&#26694;&#26550;&#65292;&#23558;&#20652;&#21270;&#21058;&#21457;&#29616;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#19981;&#30830;&#23450;&#29615;&#22659;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#25928;&#20652;&#21270;&#21058;&#30340;&#31215;&#26497;&#25628;&#32034;</title><link>https://arxiv.org/abs/2402.10980</link><description>&lt;p&gt;
CHEMREASONER&#65306;&#20351;&#29992;&#37327;&#23376;&#21270;&#23398;&#21453;&#39304;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#31354;&#38388;&#20013;&#36827;&#34892;&#21551;&#21457;&#24335;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
CHEMREASONER: Heuristic Search over a Large Language Model's Knowledge Space using Quantum-Chemical Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10980
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#19982;&#37327;&#23376;&#21270;&#23398;&#21453;&#39304;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;AI&#24341;&#23548;&#30340;&#35745;&#31639;&#31579;&#36873;&#26694;&#26550;&#65292;&#23558;&#20652;&#21270;&#21058;&#21457;&#29616;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#19981;&#30830;&#23450;&#29615;&#22659;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#25928;&#20652;&#21270;&#21058;&#30340;&#31215;&#26497;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10980v1 &#31867;&#22411;&#20844;&#21578;&#65306;&#36328;&#39046;&#22495; &#25688;&#35201;&#65306;&#21457;&#29616;&#26032;&#30340;&#20652;&#21270;&#21058;&#23545;&#20110;&#35774;&#35745;&#26032;&#30340;&#26356;&#39640;&#25928;&#30340;&#21270;&#23398;&#36807;&#31243;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#23454;&#29616;&#21521;&#21487;&#25345;&#32493;&#26410;&#26469;&#30340;&#36807;&#28193;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20154;&#24037;&#26234;&#33021;&#24341;&#23548;&#30340;&#35745;&#31639;&#31579;&#36873;&#26694;&#26550;&#65292;&#23558;&#35821;&#35328;&#25512;&#29702;&#19982;&#22522;&#20110;&#37327;&#23376;&#21270;&#23398;&#30340;&#19977;&#32500;&#21407;&#23376;&#34920;&#31034;&#30340;&#21453;&#39304;&#32479;&#19968;&#36215;&#26469;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#20652;&#21270;&#21058;&#21457;&#29616;&#26500;&#24314;&#20026;&#19968;&#20010;&#19981;&#30830;&#23450;&#29615;&#22659;&#65292;&#20854;&#20013;&#19968;&#20010;&#20195;&#29702;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25512;&#23548;&#30340;&#20551;&#35774;&#19982;&#22522;&#20110;&#21407;&#23376;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#21453;&#39304;&#30340;&#36845;&#20195;&#32452;&#21512;&#65292;&#31215;&#26497;&#25628;&#32034;&#39640;&#25928;&#20652;&#21270;&#21058;&#12290;&#22312;&#20013;&#38388;&#25628;&#32034;&#27493;&#39588;&#30830;&#23450;&#30340;&#20652;&#21270;&#21058;&#32463;&#36807;&#22522;&#20110;&#31354;&#38388;&#23450;&#21521;&#12289;&#21453;&#24212;&#36884;&#24452;&#21644;&#31283;&#23450;&#24615;&#30340;&#32467;&#26500;&#35780;&#20272;&#12290;&#22522;&#20110;&#21560;&#38468;&#33021;&#21644;&#21183;&#22418;&#30340;&#35780;&#20998;&#20989;&#25968;&#24341;&#23548;&#22312;LLM&#30340;&#30693;&#35782;&#31354;&#38388;&#20013;&#21521;&#33021;&#37327;&#26377;&#21033;&#12289;&#39640;&#25928;&#30340;&#20652;&#21270;&#21058;&#25506;&#32034;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#21487;&#20197;&#33258;&#21160;&#35268;&#21010;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10980v1 Announce Type: cross  Abstract: The discovery of new catalysts is essential for the design of new and more efficient chemical processes in order to transition to a sustainable future. We introduce an AI-guided computational screening framework unifying linguistic reasoning with quantum-chemistry based feedback from 3D atomistic representations. Our approach formulates catalyst discovery as an uncertain environment where an agent actively searches for highly effective catalysts via the iterative combination of large language model (LLM)-derived hypotheses and atomistic graph neural network (GNN)-derived feedback. Identified catalysts in intermediate search steps undergo structural evaluation based on spatial orientation, reaction pathways, and stability. Scoring functions based on adsorption energies and barriers steer the exploration in the LLM's knowledge space toward energetically favorable, high-efficiency catalysts. We introduce planning methods that automaticall
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#36890;&#36807;&#36830;&#25509;&#35821;&#35328;&#24314;&#27169;&#21644;&#31526;&#21512;&#39044;&#27979;&#20026;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#39640;&#27010;&#29575;&#27491;&#30830;&#24615;&#20445;&#35777;&#30340;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2402.10978</link><description>&lt;p&gt;
&#20855;&#26377;&#31526;&#21512;&#20107;&#23454;&#24615;&#20445;&#35777;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Language Models with Conformal Factuality Guarantees
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10978
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#36890;&#36807;&#36830;&#25509;&#35821;&#35328;&#24314;&#27169;&#21644;&#31526;&#21512;&#39044;&#27979;&#20026;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#39640;&#27010;&#29575;&#27491;&#30830;&#24615;&#20445;&#35777;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#36755;&#20986;&#30340;&#27491;&#30830;&#24615;&#21644;&#20107;&#23454;&#24615;&#20445;&#35777;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#24320;&#25918;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31526;&#21512;&#20107;&#23454;&#24615;&#65292;&#36825;&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#36830;&#25509;&#35821;&#35328;&#24314;&#27169;&#21644;&#31526;&#21512;&#39044;&#27979;&#65292;&#30830;&#20445;LM&#30340;&#39640;&#27010;&#29575;&#27491;&#30830;&#24615;&#20445;&#35777;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;LM&#36755;&#20986;&#30340;&#27491;&#30830;&#24615;&#31561;&#20215;&#20110;&#19968;&#20010;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#38382;&#39064;&#65292;&#20854;&#20013;&#19981;&#30830;&#23450;&#24615;&#38598;&#34987;&#23450;&#20041;&#20026;LM&#36755;&#20986;&#30340;&#34164;&#21547;&#38598;&#12290;&#21033;&#29992;&#36825;&#31181;&#32852;&#31995;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31526;&#21512;&#39044;&#27979;&#23545;&#24212;&#20110;&#19968;&#31181;&#21518;&#36864;&#31639;&#27861;&#65292;&#36890;&#36807;&#36880;&#28176;&#20351;LM&#36755;&#20986;&#21464;&#24471;&#19981;&#22826;&#20855;&#20307;&#65288;&#24182;&#25193;&#22823;&#30456;&#20851;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#65289;&#25552;&#20379;&#39640;&#27010;&#29575;&#30340;&#27491;&#30830;&#24615;&#20445;&#35777;&#12290;&#36825;&#31181;&#26041;&#27861;&#36866;&#29992;&#20110;&#20219;&#20309;&#40657;&#30418;LM&#65292;&#24182;&#19988;&#38656;&#35201;&#24456;&#23569;&#30340;&#20154;&#24037;&#27880;&#37322;&#26679;&#26412;&#12290;&#25105;&#20204;&#22312;&#23553;&#38381;&#20070;&#31821;QA&#65288;FActScore&#65292;NaturalQuestions&#65289;&#21644;&#25512;&#29702;&#20219;&#21153;&#65288;MATH&#65289;&#19978;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;p
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10978v1 Announce Type: cross  Abstract: Guaranteeing the correctness and factuality of language model (LM) outputs is a major open problem. In this work, we propose conformal factuality, a framework that can ensure high probability correctness guarantees for LMs by connecting language modeling and conformal prediction. We observe that the correctness of an LM output is equivalent to an uncertainty quantification problem, where the uncertainty sets are defined as the entailment set of an LM's output. Using this connection, we show that conformal prediction in language models corresponds to a back-off algorithm that provides high probability correctness guarantees by progressively making LM outputs less specific (and expanding the associated uncertainty sets). This approach applies to any black-box LM and requires very few human-annotated samples. Evaluations of our approach on closed book QA (FActScore, NaturalQuestions) and reasoning tasks (MATH) show that our approach can p
&lt;/p&gt;</description></item><item><title>&#26032;&#20852;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65288;&#22914;&#22522;&#30784;&#27169;&#22411;&#65289;&#22312;&#36807;&#31243;&#31995;&#32479;&#24037;&#31243;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20379;&#20102;&#22810;&#21151;&#33021;&#30340;&#36866;&#24212;&#24615;&#65292;&#23545;&#21512;&#25104;&#19982;&#35774;&#35745;&#12289;&#20248;&#21270;&#19982;&#38598;&#25104;&#20197;&#21450;&#36807;&#31243;&#30417;&#25511;&#19982;&#25511;&#21046;&#31561;&#20851;&#38190;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.10977</link><description>&lt;p&gt;
&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#19982;&#36807;&#31243;&#31995;&#32479;&#24037;&#31243;&#65306;&#19979;&#19968;&#20010;&#21069;&#27839;
&lt;/p&gt;
&lt;p&gt;
Generative AI and Process Systems Engineering: The Next Frontier
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10977
&lt;/p&gt;
&lt;p&gt;
&#26032;&#20852;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65288;&#22914;&#22522;&#30784;&#27169;&#22411;&#65289;&#22312;&#36807;&#31243;&#31995;&#32479;&#24037;&#31243;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20379;&#20102;&#22810;&#21151;&#33021;&#30340;&#36866;&#24212;&#24615;&#65292;&#23545;&#21512;&#25104;&#19982;&#35774;&#35745;&#12289;&#20248;&#21270;&#19982;&#38598;&#25104;&#20197;&#21450;&#36807;&#31243;&#30417;&#25511;&#19982;&#25511;&#21046;&#31561;&#20851;&#38190;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#26032;&#20852;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#27169;&#22411;&#65292;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;&#20309;&#22686;&#24378;&#36807;&#31243;&#31995;&#32479;&#24037;&#31243;&#65288;PSE&#65289;&#20013;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;&#36825;&#20123;&#26368;&#21069;&#27839;&#30340;GenAI&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#22522;&#30784;&#27169;&#22411;&#65288;FMs&#65289;&#65292;&#23427;&#20204;&#22312;&#24191;&#27867;&#30340;&#36890;&#29992;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#65292;&#20026;&#28041;&#21450;&#26597;&#35810;&#21709;&#24212;&#12289;&#22270;&#20687;&#29983;&#25104;&#21644;&#22797;&#26434;&#20915;&#31574;&#31561;&#24191;&#27867;&#20219;&#21153;&#25552;&#20379;&#20102;&#22810;&#21151;&#33021;&#30340;&#36866;&#24212;&#24615;&#12290;&#37492;&#20110;PSE&#30340;&#36827;&#23637;&#19982;&#35745;&#31639;&#21644;&#31995;&#32479;&#25216;&#26415;&#30340;&#21457;&#23637;&#20043;&#38388;&#23494;&#20999;&#20851;&#31995;&#65292;&#25506;&#32034;GenAI&#21644;PSE&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#25105;&#20204;&#20174;&#32463;&#20856;&#21644;&#26032;&#20852;&#30340;GenAI&#27169;&#22411;&#65292;&#21253;&#25324;FMs&#30340;&#31616;&#35201;&#27010;&#36848;&#24320;&#22987;&#35752;&#35770;&#65292;&#28982;&#21518;&#28145;&#20837;&#25506;&#35752;&#23427;&#20204;&#22312;&#20851;&#38190;PSE&#39046;&#22495;&#20869;&#30340;&#24212;&#29992;&#65306;&#21512;&#25104;&#19982;&#35774;&#35745;&#12289;&#20248;&#21270;&#19982;&#38598;&#25104;&#65292;&#20197;&#21450;&#36807;&#31243;&#30417;&#25511;&#19982;&#25511;&#21046;&#12290;&#22312;&#27599;&#20010;&#39046;&#22495;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;GenAI&#27169;&#22411;&#22914;&#20309;&#21487;&#20197;&#20419;&#36827;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10977v1 Announce Type: new  Abstract: This article explores how emerging generative artificial intelligence (GenAI) models, such as large language models (LLMs), can enhance solution methodologies within process systems engineering (PSE). These cutting-edge GenAI models, particularly foundation models (FMs), which are pre-trained on extensive, general-purpose datasets, offer versatile adaptability for a broad range of tasks, including responding to queries, image generation, and complex decision-making. Given the close relationship between advancements in PSE and developments in computing and systems technologies, exploring the synergy between GenAI and PSE is essential. We begin our discussion with a compact overview of both classic and emerging GenAI models, including FMs, and then dive into their applications within key PSE domains: synthesis and design, optimization and integration, and process monitoring and control. In each domain, we explore how GenAI models could pot
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#22312;&#36328;&#25968;&#25454;&#38598;&#26694;&#26550;&#20013;&#36827;&#34892;&#23454;&#39564;&#65292;&#23545;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#30340;&#27867;&#21270;&#33021;&#21147;&#36827;&#34892;&#20102;&#32508;&#21512;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#25968;&#25454;&#38598;&#23545;&#20854;&#36827;&#34892;&#39564;&#35777;&#12290;</title><link>https://arxiv.org/abs/2402.10974</link><description>&lt;p&gt;
&#35770;&#26426;&#22120;&#23398;&#20064;&#22312;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#20013;&#30340;&#36328;&#25968;&#25454;&#38598;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
On the Cross-Dataset Generalization of Machine Learning for Network Intrusion Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10974
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#22312;&#36328;&#25968;&#25454;&#38598;&#26694;&#26550;&#20013;&#36827;&#34892;&#23454;&#39564;&#65292;&#23545;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#30340;&#27867;&#21270;&#33021;&#21147;&#36827;&#34892;&#20102;&#32508;&#21512;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#25968;&#25454;&#38598;&#23545;&#20854;&#36827;&#34892;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;(NIDS)&#26159;&#32593;&#32476;&#23433;&#20840;&#20013;&#30340;&#22522;&#26412;&#24037;&#20855;&#12290;&#23427;&#20204;&#22312;&#19981;&#21516;&#32593;&#32476;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#26159;&#20854;&#26377;&#25928;&#24615;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#20063;&#26159;&#23454;&#38469;&#24212;&#29992;&#30340;&#20808;&#20915;&#26465;&#20214;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#36328;&#25968;&#25454;&#38598;&#26694;&#26550;&#20013;&#36827;&#34892;&#20840;&#38754;&#23454;&#39564;&#65292;&#23545;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;NIDS&#30340;&#27867;&#21270;&#33021;&#21147;&#36827;&#34892;&#20102;&#32508;&#21512;&#20998;&#26512;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#22235;&#31181;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#65292;&#24182;&#21033;&#29992;&#20102;&#26469;&#33258;&#19981;&#21516;&#32593;&#32476;&#30340;&#22235;&#20010;&#25968;&#25454;&#38598;&#65306;CIC-IDS-2017&#12289;CSE-CIC-IDS2018&#12289;LycoS-IDS2017&#21644;LycoS-Unicas-IDS2018&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#26368;&#21518;&#19968;&#20010;&#25968;&#25454;&#38598;&#26159;&#19968;&#20010;&#21019;&#26032;&#36129;&#29486;&#65292;&#25105;&#20204;&#22312;&#36825;&#20010;&#25968;&#25454;&#38598;&#19978;&#22522;&#20110;LycoS-IDS2017&#30340;&#20462;&#27491;&#23545;&#30693;&#21517;&#30340;CSE-CIC-IDS2018&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#24212;&#29992;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#24403;&#27169;&#22411;&#22312;&#21516;&#19968;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#21644;&#27979;&#35797;&#26102;&#65292;&#20960;&#20046;&#23436;&#32654;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#24403;&#20197;&#36328;&#25968;&#25454;&#38598;&#30340;&#26041;&#24335;&#35757;&#32451;&#21644;&#27979;&#35797;&#27169;&#22411;&#26102;&#65292;&#20998;&#31867;&#20934;&#30830;&#24230;&#20250;&#22823;&#24133;&#31243;&#24230;&#22320;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10974v1 Announce Type: cross  Abstract: Network Intrusion Detection Systems (NIDS) are a fundamental tool in cybersecurity. Their ability to generalize across diverse networks is a critical factor in their effectiveness and a prerequisite for real-world applications. In this study, we conduct a comprehensive analysis on the generalization of machine-learning-based NIDS through an extensive experimentation in a cross-dataset framework. We employ four machine learning classifiers and utilize four datasets acquired from different networks: CIC-IDS-2017, CSE-CIC-IDS2018, LycoS-IDS2017, and LycoS-Unicas-IDS2018. Notably, the last dataset is a novel contribution, where we apply corrections based on LycoS-IDS2017 to the well-known CSE-CIC-IDS2018 dataset. The results show nearly perfect classification performance when the models are trained and tested on the same dataset. However, when training and testing the models in a cross-dataset fashion, the classification accuracy is largel
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#38024;&#23545;&#20559;&#22836;&#30171;&#31561;&#24930;&#24615;&#30142;&#30149;&#30340;&#39044;&#27979;&#27169;&#22411;&#24314;&#35774;&#26041;&#27861;&#65292;&#25506;&#35752;&#20102;&#26368;&#22823;&#39044;&#27979;&#26102;&#38388;&#36328;&#24230;&#21450;&#20854;&#23545;&#36873;&#23450;&#29305;&#24449;&#30340;&#20381;&#36182;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.10972</link><description>&lt;p&gt;
&#24930;&#24615;&#30142;&#30149;&#30151;&#29366;&#20107;&#20214;&#20934;&#30830;&#21450;&#21450;&#26102;&#39044;&#27979;&#30340;&#24314;&#27169;&#26041;&#27861;&#35770;
&lt;/p&gt;
&lt;p&gt;
Modeling methodology for the accurate and prompt prediction of symptomatic events in chronic diseases
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10972
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#38024;&#23545;&#20559;&#22836;&#30171;&#31561;&#24930;&#24615;&#30142;&#30149;&#30340;&#39044;&#27979;&#27169;&#22411;&#24314;&#35774;&#26041;&#27861;&#65292;&#25506;&#35752;&#20102;&#26368;&#22823;&#39044;&#27979;&#26102;&#38388;&#36328;&#24230;&#21450;&#20854;&#23545;&#36873;&#23450;&#29305;&#24449;&#30340;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#24930;&#24615;&#30142;&#30149;&#20013;&#30340;&#30151;&#29366;&#21361;&#26426;&#21487;&#20197;&#22312;&#30151;&#29366;&#21457;&#29983;&#20043;&#21069;&#20570;&#20986;&#20915;&#31574;&#65292;&#27604;&#22914;&#26381;&#29992;&#33647;&#29289;&#20197;&#36991;&#20813;&#30151;&#29366;&#25110;&#28608;&#27963;&#21307;&#30103;&#35686;&#25253;&#12290;&#26412;&#25991;&#38024;&#23545;&#19968;&#31181;&#24930;&#24615;&#30149;&#8212;&#20559;&#22836;&#30171;&#36827;&#34892;&#39044;&#27979;&#26497;&#38480;&#30740;&#31350;&#12290;&#20026;&#27492;&#65292;&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#26500;&#24314;&#20559;&#22836;&#30171;&#39044;&#27979;&#27169;&#22411;&#24182;&#25913;&#21892;&#36825;&#20123;&#39044;&#27979;&#36229;&#20986;&#21407;&#22987;&#27169;&#22411;&#38480;&#21046;&#30340;&#26041;&#27861;&#35770;&#12290;&#20998;&#26512;&#20102;&#26368;&#22823;&#39044;&#27979;&#26102;&#38388;&#36328;&#24230;&#65292;&#24182;&#30740;&#31350;&#20102;&#20854;&#23545;&#25152;&#36873;&#29305;&#24449;&#30340;&#20381;&#36182;&#24615;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#36873;&#25321;&#31574;&#30053;&#65292;&#20197;&#35299;&#20915;&#23545;&#20445;&#23432;&#20294;&#31283;&#20581;&#30340;&#39044;&#27979;&#27169;&#22411;&#21644;&#23545;&#31934;&#24230;&#36739;&#20302;&#20294;&#26356;&#22823;&#26102;&#38388;&#36328;&#24230;&#30340;&#39044;&#27979;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#24471;&#21040;&#30340;&#32467;&#26524;&#26174;&#31034;&#20102;&#19968;&#31181;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10972v1 Announce Type: cross  Abstract: Prediction of symptomatic crises in chronic diseases allows to take decisions before the symptoms occur, such as the intake of drugs to avoid the symptoms or the activation of medical alarms. The prediction horizon is in this case an important parameter in order to fulfill the pharmacokinetics of medications, or the time response of medical services. This paper presents a study about the prediction limits of a chronic disease with symptomatic crises: the migraine. For that purpose, this work develops a methodology to build predictive migraine models and to improve these predictions beyond the limits of the initial models. The maximum prediction horizon is analyzed, and its dependency on the selected features is studied. A strategy for model selection is proposed to tackle the trade off between conservative but robust predictive models, with respect to less accurate predictions with higher horizons. The obtained results show a predictio
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#30103;&#20581;&#24247;&#39046;&#22495;&#26377;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#28982;&#32780;&#23427;&#20204;&#30340;&#27867;&#21270;&#25928;&#26524;&#21462;&#20915;&#20110;&#22312;&#19981;&#21516;&#20020;&#24202;&#29615;&#22659;&#21644;&#20154;&#32676;&#20013;&#30340;&#34920;&#29616;&#65292;&#23545;&#20110;&#27867;&#21270;&#33021;&#21147;&#19981;&#36275;&#30340;&#21407;&#22240;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#21457;&#29616;&#22312;&#26679;&#26412;&#36739;&#23569;&#30340;&#21307;&#38498;&#21644;&#29305;&#23450;&#20154;&#32676;&#20013;&#23384;&#22312;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.10965</link><description>&lt;p&gt;
&#21307;&#30103;AI&#20013;&#30340;&#27867;&#21270;&#24615;&#33021;&#65306;&#20020;&#24202;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Generalization in Healthcare AI: Evaluation of a Clinical Large Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10965
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#30103;&#20581;&#24247;&#39046;&#22495;&#26377;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#28982;&#32780;&#23427;&#20204;&#30340;&#27867;&#21270;&#25928;&#26524;&#21462;&#20915;&#20110;&#22312;&#19981;&#21516;&#20020;&#24202;&#29615;&#22659;&#21644;&#20154;&#32676;&#20013;&#30340;&#34920;&#29616;&#65292;&#23545;&#20110;&#27867;&#21270;&#33021;&#21147;&#19981;&#36275;&#30340;&#21407;&#22240;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#21457;&#29616;&#22312;&#26679;&#26412;&#36739;&#23569;&#30340;&#21307;&#38498;&#21644;&#29305;&#23450;&#20154;&#32676;&#20013;&#23384;&#22312;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#20026;&#21307;&#30103;&#20581;&#24247;&#39046;&#22495;&#25552;&#20379;&#20102;&#26032;&#26426;&#36935;&#65292;&#21487;&#20197;&#25913;&#21892;&#24739;&#32773;&#25252;&#29702;&#12289;&#20020;&#24202;&#20915;&#31574;&#20197;&#21450;&#25552;&#21319;&#21307;&#24072;&#21644;&#31649;&#29702;&#20154;&#21592;&#30340;&#24037;&#20316;&#27969;&#31243;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#28508;&#21147;&#37325;&#35201;&#21462;&#20915;&#20110;&#23427;&#20204;&#22312;&#20020;&#24202;&#29615;&#22659;&#21644;&#20154;&#32676;&#20013;&#26377;&#25928;&#27867;&#21270;&#30340;&#33021;&#21147;&#65292;&#36825;&#26159;&#22312;&#26089;&#26399;&#24320;&#21457;&#20013;&#32463;&#24120;&#34987;&#20302;&#20272;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#20123;&#25361;&#25112;&#30340;&#21407;&#22240;&#24182;&#21046;&#23450;&#32531;&#35299;&#26041;&#27861;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;ClinicLLM&#65292;&#36825;&#26159;&#19968;&#20010;&#22312; [HOSPITAL] &#30340;&#20020;&#24202;&#31508;&#35760;&#19978;&#35757;&#32451;&#30340;LLM&#27169;&#22411;&#65292;&#23545;&#20854;&#22312;30&#22825;&#20840;&#22240;&#32032;&#20877;&#20837;&#38498;&#39044;&#27979;&#20013;&#30340;&#34920;&#29616;&#36827;&#34892;&#20998;&#26512;&#65292;&#20851;&#27880;&#36328;&#21307;&#38498;&#21644;&#24739;&#32773;&#29305;&#24449;&#30340;&#21464;&#24322;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#26679;&#26412;&#36739;&#23569;&#30340;&#21307;&#38498;&#12289;&#25919;&#24220;&#21644;&#26410;&#25351;&#23450;&#20445;&#38505;&#30340;&#24739;&#32773;&#12289;&#32769;&#24180;&#20154;&#20197;&#21450;&#39640;&#20849;&#30149;&#24615;&#24739;&#32773;&#20013;&#65292;&#27867;&#21270;&#25928;&#26524;&#36739;&#24046;&#12290;&#20026;&#20102;&#20102;&#35299;&#27867;&#21270;&#19981;&#24432;&#30340;&#21407;&#22240;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#26679;&#26412;&#37327;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10965v1 Announce Type: new  Abstract: Advances in large language models (LLMs) provide new opportunities in healthcare for improved patient care, clinical decision-making, and enhancement of physician and administrator workflows. However, the potential of these models importantly depends on their ability to generalize effectively across clinical environments and populations, a challenge often underestimated in early development. To better understand reasons for these challenges and inform mitigation approaches, we evaluated ClinicLLM, an LLM trained on [HOSPITAL]'s clinical notes, analyzing its performance on 30-day all-cause readmission prediction focusing on variability across hospitals and patient characteristics. We found poorer generalization particularly in hospitals with fewer samples, among patients with government and unspecified insurance, the elderly, and those with high comorbidities. To understand reasons for lack of generalization, we investigated sample sizes 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36951;&#20256;&#31639;&#27861;&#36827;&#34892;&#36755;&#20837;&#29305;&#24449;&#30340;&#26368;&#20339;&#37325;&#32553;&#25918;&#26469;&#25913;&#21892;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#25928;&#29575;&#21644;&#27867;&#21270;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.10964</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#26368;&#20339;&#29305;&#24449;&#37325;&#32553;&#25918;
&lt;/p&gt;
&lt;p&gt;
Optimal feature rescaling in machine learning based on neural networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10964
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36951;&#20256;&#31639;&#27861;&#36827;&#34892;&#36755;&#20837;&#29305;&#24449;&#30340;&#26368;&#20339;&#37325;&#32553;&#25918;&#26469;&#25913;&#21892;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#25928;&#29575;&#21644;&#27867;&#21270;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36951;&#20256;&#31639;&#27861;&#65288;GA&#65289;&#36827;&#34892;&#36755;&#20837;&#29305;&#24449;&#30340;&#26368;&#20339;&#37325;&#32553;&#25918;&#65288;OFR&#65289;&#26469;&#25913;&#21892;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#65288;FFNNs&#65289;&#30340;&#35757;&#32451;&#25928;&#29575;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;OFR&#37325;&#26032;&#22609;&#36896;&#20102;&#36755;&#20837;&#31354;&#38388;&#65292;&#25913;&#21892;&#20102;&#29992;&#20110;&#35757;&#32451;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#31639;&#27861;&#30340;&#26465;&#20214;&#12290;&#27492;&#22806;&#65292;GA&#36827;&#34892;&#30340;&#27604;&#20363;&#22240;&#23376;&#25506;&#32034;&#21644;&#36873;&#25321;&#23545;&#24212;&#20110;&#27599;&#27425;&#35757;&#32451;&#23581;&#35797;&#20013;&#31532;&#19968;&#23618;&#26435;&#37325;&#30340;&#19981;&#21516;&#21021;&#22987;&#21270;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#19968;&#20010;&#22810;&#36215;&#28857;&#20840;&#23616;&#25628;&#32034;&#31639;&#27861;&#65288;&#23613;&#31649;&#20165;&#38480;&#20110;&#23569;&#37327;&#26435;&#37325;&#65289;&#65292;&#20174;&#32780;&#20419;&#36827;&#20102;&#20840;&#23616;&#26368;&#23567;&#20540;&#30340;&#23454;&#29616;&#12290;&#35813;&#26041;&#27861;&#24050;&#22312;&#27169;&#25311;&#23454;&#38469;&#24037;&#19994;&#36807;&#31243;&#65288;&#26080;&#24515;&#30952;&#21066;&#65289;&#32467;&#26524;&#30340;FFNN&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10964v1 Announce Type: new  Abstract: This paper proposes a novel approach to improve the training efficiency and the generalization performance of Feed Forward Neural Networks (FFNNs) resorting to an optimal rescaling of input features (OFR) carried out by a Genetic Algorithm (GA). The OFR reshapes the input space improving the conditioning of the gradient-based algorithm used for the training. Moreover, the scale factors exploration entailed by GA trials and selection corresponds to different initialization of the first layer weights at each training attempt, thus realizing a multi-start global search algorithm (even though restrained to few weights only) which fosters the achievement of a global minimum. The approach has been tested on a FFNN modeling the outcome of a real industrial process (centerless grinding).
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Stepwise ORMs (SORMs)&#65292;&#23427;&#20204;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#35757;&#32451;&#65292;&#20197;&#36817;&#20284;&#39044;&#27979;&#26368;&#20248;&#31574;&#30053;&#30340;&#26410;&#26469;&#39044;&#26399;&#22870;&#21169;</title><link>https://arxiv.org/abs/2402.10963</link><description>&lt;p&gt;
GLoRe: &#20309;&#26102;&#12289;&#20309;&#22320;&#20197;&#21450;&#22914;&#20309;&#36890;&#36807;&#20840;&#23616;&#21644;&#23616;&#37096;&#30340;&#25913;&#36827;&#26469;&#25552;&#39640;LLM&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
GLoRe: When, Where, and How to Improve LLM Reasoning via Global and Local Refinements
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10963
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Stepwise ORMs (SORMs)&#65292;&#23427;&#20204;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#35757;&#32451;&#65292;&#20197;&#36817;&#20284;&#39044;&#27979;&#26368;&#20248;&#31574;&#30053;&#30340;&#26410;&#26469;&#39044;&#26399;&#22870;&#21169;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#23398;&#12289;&#31185;&#23398;&#25110;&#32534;&#30721;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25512;&#29702;&#25913;&#36827;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#26368;&#22909;&#30340;&#27169;&#22411;&#20063;&#24456;&#38590;&#22312;&#27809;&#26377;&#22806;&#37096;&#21453;&#39304;&#30340;&#24773;&#20917;&#19979;&#30830;&#23450;&#20309;&#26102;&#20309;&#22320;&#36827;&#34892;&#25913;&#36827;&#12290;&#22522;&#20110;&#32467;&#26524;&#30340;&#22870;&#21169;&#27169;&#22411;(ORMs)&#65292;&#34987;&#35757;&#32451;&#26469;&#39044;&#27979;&#26368;&#32456;&#31572;&#26696;&#30340;&#27491;&#30830;&#24615;&#65292;&#25351;&#31034;&#20309;&#26102;&#36827;&#34892;&#25913;&#36827;&#65292;&#20026;&#20915;&#23450;&#20309;&#26102;&#36827;&#34892;&#25913;&#36827;&#25552;&#20379;&#20102;&#19968;&#31181;&#20415;&#21033;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22522;&#20110;&#36807;&#31243;&#30340;&#22870;&#21169;&#27169;&#22411;(PRMs)&#21463;&#36807;&#35757;&#32451;&#65292;&#29992;&#20197;&#39044;&#27979;&#20013;&#38388;&#27493;&#39588;&#30340;&#27491;&#30830;&#24615;&#65292;&#28982;&#21518;&#21487;&#20197;&#29992;&#26469;&#25351;&#31034;&#20309;&#22788;&#36827;&#34892;&#25913;&#36827;&#12290;&#20294;&#23427;&#20204;&#24456;&#26114;&#36149;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#24037;&#27880;&#37322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36880;&#27493;ORMs(SORMs)&#65292;&#23427;&#20204;&#21482;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#21463;&#36807;&#35757;&#32451;&#65292;&#20197;&#36817;&#20284;&#39044;&#27979;&#26368;&#20248;&#31574;&#30053;&#25110;$V^{\star}$&#30340;&#26410;&#26469;&#39044;&#26399;&#22870;&#21169;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;SORMs&#21463;&#35757;&#32451;&#26469;&#39044;&#27979;&#24403;&#21462;&#26679;&#26102;&#26368;&#32456;&#31572;&#26696;&#30340;&#27491;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10963v1 Announce Type: new  Abstract: State-of-the-art language models can exhibit impressive reasoning refinement capabilities on math, science or coding tasks. However, recent work demonstrates that even the best models struggle to identify \textit{when and where to refine} without access to external feedback. Outcome-based Reward Models (\textbf{ORMs}), trained to predict correctness of the final answer indicating when to refine, offer one convenient solution for deciding when to refine. Process Based Reward Models (\textbf{PRMs}), trained to predict correctness of intermediate steps, can then be used to indicate where to refine. But they are expensive to train, requiring extensive human annotations. In this paper, we propose Stepwise ORMs (\textbf{SORMs}) which are trained, only on synthetic data, to approximate the expected future reward of the optimal policy or $V^{\star}$. More specifically, SORMs are trained to predict the correctness of the final answer when samplin
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#22522;&#20934;&#26469;&#27979;&#37327;&#35821;&#35328;&#27169;&#22411;&#23545;&#35805;&#20013;&#30340;&#8220;&#20154;&#35774;&#8221;&#28418;&#31227;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;split-softmax&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#26469;&#23545;&#25239;&#27880;&#24847;&#21147;&#34928;&#20943;&#21644;&#8220;&#20154;&#35774;&#8221;&#28418;&#31227;</title><link>https://arxiv.org/abs/2402.10962</link><description>&lt;p&gt;
&#22312;&#35821;&#35328;&#27169;&#22411;&#23545;&#35805;&#20013;&#27979;&#37327;&#21644;&#25511;&#21046;&#8220;&#20154;&#35774;&#8221;&#28418;&#31227;
&lt;/p&gt;
&lt;p&gt;
Measuring and Controlling Persona Drift in Language Model Dialogs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10962
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#22522;&#20934;&#26469;&#27979;&#37327;&#35821;&#35328;&#27169;&#22411;&#23545;&#35805;&#20013;&#30340;&#8220;&#20154;&#35774;&#8221;&#28418;&#31227;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;split-softmax&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#26469;&#23545;&#25239;&#27880;&#24847;&#21147;&#34928;&#20943;&#21644;&#8220;&#20154;&#35774;&#8221;&#28418;&#31227;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#26159;&#23450;&#21046;&#35821;&#35328;&#27169;&#22411;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#26631;&#20934;&#24037;&#20855;&#65292;&#20351;&#20854;&#33021;&#22815;&#25215;&#25285;&#29305;&#23450;&#30340;&#8220;&#20154;&#35774;&#8221;&#12290;&#22312;&#20351;&#29992;&#25552;&#31034;&#26102;&#30340;&#19968;&#20010;&#38544;&#21547;&#20551;&#35774;&#26159;&#65292;&#23427;&#20204;&#23558;&#26159;&#31283;&#23450;&#30340;&#65292;&#22240;&#27492;&#32842;&#22825;&#26426;&#22120;&#20154;&#23558;&#22312;&#25972;&#20010;&#23545;&#35805;&#36807;&#31243;&#20013;&#32487;&#32493;&#26681;&#25454;&#35268;&#23450;&#30340;&#8220;&#20154;&#35774;&#8221;&#29983;&#25104;&#25991;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#37327;&#21270;&#22522;&#20934;&#26469;&#27979;&#35797;&#36825;&#19968;&#20551;&#35774;&#65292;&#36890;&#36807;&#20004;&#20010;&#20010;&#24615;&#21270;&#32842;&#22825;&#26426;&#22120;&#20154;&#20043;&#38388;&#30340;&#33258;&#25105;&#23545;&#35805;&#26469;&#35780;&#20272;&#8220;&#20154;&#35774;&#8221;&#30340;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#23545;&#27969;&#34892;&#27169;&#22411;&#22914;LLaMA2-chat-70B&#36827;&#34892;&#27979;&#35797;&#65292;&#21457;&#29616;&#22312;&#20843;&#36718;&#23545;&#35805;&#20013;&#23384;&#22312;&#26174;&#33879;&#30340;&#8220;&#20154;&#35774;&#8221;&#28418;&#31227;&#12290;&#23545;&#36825;&#19968;&#29616;&#35937;&#30340;&#23454;&#35777;&#21644;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#30001;&#20110;&#38271;&#23545;&#35805;&#20013;&#30340;&#27880;&#24847;&#21147;&#34928;&#20943;&#65292;&#21464;&#21387;&#22120;&#27880;&#24847;&#21147;&#26426;&#21046;&#36215;&#21040;&#20102;&#19968;&#23450;&#20316;&#29992;&#12290;&#20026;&#20102;&#23545;&#25239;&#27880;&#24847;&#21147;&#34928;&#20943;&#21644;&#8220;&#20154;&#35774;&#8221;&#28418;&#31227;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;split-softmax&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#65292;&#19982;&#20004;&#20010;&#24378;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10962v1 Announce Type: cross  Abstract: Prompting is a standard tool for customizing language-model chatbots, enabling them to take on a specific "persona". An implicit assumption in the use of prompts is that they will be stable, so the chatbot will continue to generate text according to the stipulated persona for the duration of a conversation. We propose a quantitative benchmark to test this assumption, evaluating persona stability via self-chats between two personalized chatbots. Testing popular models like LLaMA2-chat-70B, we reveal a significant persona drift within eight rounds of conversations. An empirical and theoretical analysis of this phenomenon suggests the transformer attention mechanism plays a role, due to attention decay over long exchanges. To combat attention decay and persona drift, we propose a lightweight method called split-softmax, which compares favorably against two strong baselines.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#30456;&#23545;&#20248;&#20808;&#26435;&#20248;&#21270;&#65288;RPO&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#21306;&#20998;&#26469;&#33258;&#30456;&#21516;&#21644;&#30456;&#20851;&#25552;&#31034;&#30340;&#26356;&#21463;&#38738;&#30544;&#30340;&#21709;&#24212;&#21644;&#26356;&#19981;&#21463;&#38738;&#30544;&#30340;&#21709;&#24212;&#65292;&#25193;&#23637;&#20102;&#27169;&#22411;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.10958</link><description>&lt;p&gt;
&#30456;&#23545;&#20248;&#20808;&#26435;&#20248;&#21270;: &#36890;&#36807;&#23545;&#30456;&#21516;&#21644;&#19981;&#21516;&#25552;&#31034;&#30340;&#23545;&#27604;&#21709;&#24212;&#22686;&#24378;LLM&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Relative Preference Optimization: Enhancing LLM Alignment through Contrasting Responses across Identical and Diverse Prompts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10958
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#30456;&#23545;&#20248;&#20808;&#26435;&#20248;&#21270;&#65288;RPO&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#21306;&#20998;&#26469;&#33258;&#30456;&#21516;&#21644;&#30456;&#20851;&#25552;&#31034;&#30340;&#26356;&#21463;&#38738;&#30544;&#30340;&#21709;&#24212;&#21644;&#26356;&#19981;&#21463;&#38738;&#30544;&#30340;&#21709;&#24212;&#65292;&#25193;&#23637;&#20102;&#27169;&#22411;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#39046;&#22495;&#65292;&#23558;&#27169;&#22411;&#19982;&#29992;&#25143;&#30340;&#22810;&#26679;&#21270;&#20559;&#22909;&#30456;&#19968;&#33268;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#30452;&#25509;&#20248;&#20808;&#26435;&#20248;&#21270;&#65288;DPO&#65289;&#22312;&#36825;&#19968;&#39046;&#22495;&#36215;&#21040;&#20102;&#20851;&#38190;&#20316;&#29992;&#12290;DPO&#36890;&#36807;&#20351;&#29992;&#20174;&#30456;&#21516;&#25552;&#31034;&#20013;&#27966;&#29983;&#30340;&#20559;&#22909;&#23545;&#26469;&#24037;&#20316;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#22870;&#21169;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;DPO&#24182;&#19981;&#33021;&#23436;&#20840;&#21453;&#26144;&#20154;&#31867;&#23398;&#20064;&#30340;&#22797;&#26434;&#24615;&#65292;&#36825;&#31181;&#23398;&#20064;&#24448;&#24448;&#28041;&#21450;&#23545;&#19981;&#20165;&#30456;&#21516;&#32780;&#19988;&#30456;&#20284;&#38382;&#39064;&#30340;&#23545;&#27604;&#21709;&#24212;&#30340;&#29702;&#35299;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#19981;&#36275;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#30456;&#23545;&#20248;&#20808;&#26435;&#20248;&#21270;&#65288;RPO&#65289;&#12290;RPO&#26088;&#22312;&#21306;&#20998;&#26469;&#33258;&#30456;&#21516;&#21644;&#30456;&#20851;&#25552;&#31034;&#30340;&#26356;&#21463;&#38738;&#30544;&#30340;&#21709;&#24212;&#21644;&#26356;&#19981;&#21463;&#38738;&#30544;&#30340;&#21709;&#24212;&#12290;&#23427;&#24341;&#20837;&#20102;&#23545;&#27604;&#21152;&#26435;&#26426;&#21046;&#65292;&#20351;LLMs&#33021;&#22815;&#20351;&#29992;&#26356;&#24191;&#27867;&#30340;&#20559;&#22909;&#25968;&#25454;&#36827;&#34892;&#35843;&#25972;&#65292;&#21253;&#25324;&#25104;&#23545;&#21644;&#19981;&#25104;&#23545;&#30340;&#25968;&#25454;&#38598;&#12290;&#36825;&#31181;&#26041;&#27861;&#25193;&#23637;&#20102;&#27169;&#22411;&#30340;&#23398;&#20064;&#33021;&#21147;&#65292;&#20351;&#20854;&#33021;&#22815;&#21033;&#29992;&#26356;&#22810;&#30340;&#20559;&#22909;&#25968;&#25454;&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10958v1 Announce Type: cross  Abstract: In the field of large language models (LLMs), aligning models with the diverse preferences of users is a critical challenge. Direct Preference Optimization (DPO) has played a key role in this area. It works by using pairs of preferences derived from the same prompts, and it functions without needing an additional reward model. However, DPO does not fully reflect the complex nature of human learning, which often involves understanding contrasting responses to not only identical but also similar questions. To overcome this shortfall, we propose Relative Preference Optimization (RPO). RPO is designed to discern between more and less preferred responses derived from both identical and related prompts. It introduces a contrastive weighting mechanism, enabling the tuning of LLMs using a broader range of preference data, including both paired and unpaired sets. This approach expands the learning capabilities of the model, allowing it to lever
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#31867;&#20284;&#20110;&#30561;&#30496;&#30340;&#26080;&#30417;&#30563;&#37325;&#25773;&#38454;&#27573;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22312;&#26377;&#38480;&#25968;&#25454;&#24773;&#20917;&#19979;&#35757;&#32451;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#20934;&#30830;&#24615;&#65292;&#36825;&#20026;&#35299;&#20915;&#25968;&#25454;&#26377;&#38480;&#25110;&#19981;&#24179;&#34913;&#26102;&#30340;&#24615;&#33021;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.10956</link><description>&lt;p&gt;
&#31867;&#20284;&#20110;&#30561;&#30496;&#30340;&#26080;&#30417;&#30563;&#37325;&#25773;&#22312;&#25968;&#25454;&#26377;&#38480;&#25110;&#19981;&#24179;&#34913;&#26102;&#25913;&#21892;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Sleep-Like Unsupervised Replay Improves Performance when Data are Limited or Unbalanced
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10956
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#31867;&#20284;&#20110;&#30561;&#30496;&#30340;&#26080;&#30417;&#30563;&#37325;&#25773;&#38454;&#27573;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22312;&#26377;&#38480;&#25968;&#25454;&#24773;&#20917;&#19979;&#35757;&#32451;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#20934;&#30830;&#24615;&#65292;&#36825;&#20026;&#35299;&#20915;&#25968;&#25454;&#26377;&#38480;&#25110;&#19981;&#24179;&#34913;&#26102;&#30340;&#24615;&#33021;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANNs&#65289;&#22312;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#25110;&#19981;&#24179;&#34913;&#26102;&#24615;&#33021;&#19979;&#38477;&#12290;&#30456;&#21453;&#65292;&#20154;&#33041;&#21487;&#20197;&#24555;&#36895;&#20174;&#23569;&#37327;&#31034;&#20363;&#20013;&#23398;&#20064;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;MNIST&#21644;Fashion MNIST&#25968;&#25454;&#38598;&#19978;&#29992;&#26377;&#38480;&#25968;&#25454;&#35757;&#32451;&#30340;ANNs&#24615;&#33021;&#25913;&#21892;&#20316;&#29992;&#12290;&#30561;&#30496;&#34987;&#23454;&#29616;&#20026;&#20855;&#26377;&#26412;&#22320;Hebbian&#31867;&#22411;&#23398;&#20064;&#35268;&#21017;&#30340;&#26080;&#30417;&#30563;&#38454;&#27573;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#20351;&#29992;MNIST&#25110;Fashion MNIST&#25968;&#25454;&#38598;&#24635;&#37327;&#30340;0.5-10%&#26377;&#38480;&#25968;&#25454;&#35757;&#32451;&#27169;&#22411;&#21518;&#65292;&#32463;&#36807;&#30561;&#30496;&#38454;&#27573;&#21518;&#20934;&#30830;&#24615;&#26174;&#33879;&#25552;&#39640;&#12290;&#24403;&#20351;&#29992;&#36229;&#36807;&#24635;&#25968;&#25454;&#37327;&#30340;10%&#26102;&#65292;&#20165;&#30561;&#30496;&#23545;&#24615;&#33021;&#26377;&#36731;&#24494;&#36127;&#38754;&#24433;&#21709;&#65292;&#20294;&#36825;&#21487;&#20197;&#36890;&#36807;&#23545;&#21407;&#22987;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#26469;&#32416;&#27491;&#12290;&#35813;&#30740;&#31350;&#25581;&#31034;&#20102;&#22823;&#33041;&#22312;&#30561;&#30496;&#26102;&#21033;&#29992;&#30340;&#28508;&#22312;&#31361;&#35302;&#26435;&#37325;&#21160;&#24577;&#31574;&#30053;&#65292;&#20197;&#22686;&#24378;&#22312;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#25110;&#19981;&#24179;&#34913;&#24773;&#20917;&#19979;&#30340;&#35760;&#24518;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10956v1 Announce Type: cross  Abstract: The performance of artificial neural networks (ANNs) degrades when training data are limited or imbalanced. In contrast, the human brain can learn quickly from just a few examples. Here, we investigated the role of sleep in improving the performance of ANNs trained with limited data on the MNIST and Fashion MNIST datasets. Sleep was implemented as an unsupervised phase with local Hebbian type learning rules. We found a significant boost in accuracy after the sleep phase for models trained with limited data in the range of 0.5-10% of total MNIST or Fashion MNIST datasets. When more than 10% of the total data was used, sleep alone had a slight negative impact on performance, but this was remedied by fine-tuning on the original data. This study sheds light on a potential synaptic weight dynamics strategy employed by the brain during sleep to enhance memory performance when training data are limited or imbalanced.
&lt;/p&gt;</description></item><item><title>DAEDRA&#26159;&#19968;&#31181;&#26088;&#22312;&#22312;&#34987;&#21160;&#25253;&#21578;&#20013;&#26816;&#27979;&#30417;&#31649;&#30456;&#20851;&#32467;&#26524;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24357;&#34917;&#20102;&#36890;&#29992;&#27169;&#22411;&#26080;&#27861;&#25429;&#25417;&#20020;&#24202;&#32500;&#24230;&#19982;&#19987;&#19994;&#27169;&#22411;&#22312;&#38750;&#19987;&#19994;&#25253;&#21578;&#19978;&#34920;&#29616;&#19981;&#20339;&#30340;&#32570;&#38519;</title><link>https://arxiv.org/abs/2402.10951</link><description>&lt;p&gt;
DAEDRA&#65306;&#29992;&#20110;&#39044;&#27979;&#34987;&#21160;&#33647;&#29289;&#35686;&#25106;&#25253;&#21578;&#32467;&#26524;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DAEDRA: A language model for predicting outcomes in passive pharmacovigilance reporting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10951
&lt;/p&gt;
&lt;p&gt;
DAEDRA&#26159;&#19968;&#31181;&#26088;&#22312;&#22312;&#34987;&#21160;&#25253;&#21578;&#20013;&#26816;&#27979;&#30417;&#31649;&#30456;&#20851;&#32467;&#26524;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24357;&#34917;&#20102;&#36890;&#29992;&#27169;&#22411;&#26080;&#27861;&#25429;&#25417;&#20020;&#24202;&#32500;&#24230;&#19982;&#19987;&#19994;&#27169;&#22411;&#22312;&#38750;&#19987;&#19994;&#25253;&#21578;&#19978;&#34920;&#29616;&#19981;&#20339;&#30340;&#32570;&#38519;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#23548;&#33268;&#20102;&#29305;&#23450;&#39046;&#22495;&#27169;&#22411;&#30340;&#28608;&#22686;&#65292;&#36825;&#20123;&#27169;&#22411;&#26088;&#22312;&#21453;&#26144;&#26469;&#28304;&#39046;&#22495;&#30340;&#35821;&#35328;&#29615;&#22659;&#21644;&#20869;&#23481;&#30340;&#29305;&#27530;&#24615;&#12290;&#26412;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;DAEDRA&#30340;&#26500;&#24605;&#12289;&#35774;&#35745;&#12289;&#35757;&#32451;&#21644;&#35780;&#20272;&#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#26816;&#27979;&#34987;&#21160;&#25253;&#21578;&#65288;PR&#65289;&#20013;&#30340;&#30417;&#31649;&#30456;&#20851;&#32467;&#26524;&#65288;&#27515;&#20129;&#12289;&#24613;&#35786;&#23601;&#35786;&#21644;&#20303;&#38498;&#65289;&#30340;LLM&#12290;&#34429;&#28982;PR&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#20174;&#24191;&#27867;&#21644;&#22810;&#26679;&#21270;&#21463;&#20247;&#65288;&#36890;&#24120;&#19981;&#20165;&#21253;&#25324;&#21307;&#29983;&#21644;&#21307;&#25252;&#20154;&#21592;&#65292;&#36824;&#21253;&#25324;&#24739;&#32773;&#12289;&#23478;&#24237;&#25104;&#21592;&#21644;&#20854;&#20182;&#38750;&#19987;&#19994;&#21033;&#30410;&#30456;&#20851;&#32773;&#65289;&#37027;&#37324;&#33719;&#21462;&#20449;&#24687;&#30340;&#26041;&#24335;&#65292;&#20294;&#26159;&#36825;&#31181;&#22810;&#26679;&#24615;&#20351;PR&#35821;&#26009;&#24211;&#38590;&#20197;&#20998;&#26512;&#12290;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#26080;&#27861;&#25429;&#25417;&#22797;&#26434;&#30340;&#20020;&#24202;&#32500;&#24230;&#65292;&#32780;&#29305;&#23450;&#30340;&#20020;&#24202;&#25110;&#29983;&#29289;&#21307;&#23398;&#27169;&#22411;&#21487;&#33021;&#22312;&#38750;&#19987;&#19994;&#25253;&#21578;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10951v1 Announce Type: new  Abstract: Over the recent years, the emergence of large language models (LLMs) has given rise to a proliferation of domain-specific models that are intended to reflect the particularities of linguistic context and content as a correlate of the originating domain. This paper details the conception, design, training and evaluation of DAEDRA, a LLM designed to detect regulatory-relevant outcomes (mortality, ER attendance and hospitalisation) in adverse event reports elicited through passive reporting (PR). While PR is a highly cost-efficient way of eliciting information from a wide and diverse audience -- typically including not only physicians and healthcare providers but also patients, family members and other lay stakeholders --, this diversity makes PR corpora difficult to analyse. Generic language models may not capture the complex clinical dimensions while specific clinical or biomedical models may not perform well on lay reports. To evaluate t
&lt;/p&gt;</description></item><item><title>&#24322;&#31867;&#33258;&#21160;&#25552;&#31034;&#30340;&#19981;&#21512;&#29702;&#26377;&#25928;&#24615;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#21508;&#31181;&#25552;&#31034;&#26102;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#21253;&#25324;&#8220;&#31215;&#26497;&#24605;&#32771;&#8221;&#25552;&#31034;&#20250;&#23545;&#27169;&#22411;&#24615;&#33021;&#20135;&#29983;&#31215;&#26497;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.10949</link><description>&lt;p&gt;
&#24322;&#31867;&#33258;&#21160;&#25552;&#31034;&#30340;&#19981;&#21512;&#29702;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Unreasonable Effectiveness of Eccentric Automatic Prompts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10949
&lt;/p&gt;
&lt;p&gt;
&#24322;&#31867;&#33258;&#21160;&#25552;&#31034;&#30340;&#19981;&#21512;&#29702;&#26377;&#25928;&#24615;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#21508;&#31181;&#25552;&#31034;&#26102;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#21253;&#25324;&#8220;&#31215;&#26497;&#24605;&#32771;&#8221;&#25552;&#31034;&#20250;&#23545;&#27169;&#22411;&#24615;&#33021;&#20135;&#29983;&#31215;&#26497;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#38382;&#39064;&#35299;&#20915;&#21644;&#22522;&#26412;&#25968;&#23398;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#21151;&#25928;&#39640;&#24230;&#20381;&#36182;&#20110;&#25552;&#31034;&#30340;&#21046;&#23450;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#37327;&#21270;&#23558;&#8220;&#31215;&#26497;&#24605;&#32771;&#8221;&#32435;&#20837;&#31995;&#32479;&#25552;&#31034;&#28040;&#24687;&#30340;&#24433;&#21709;&#65292;&#28982;&#21518;&#23558;&#20854;&#19982;&#31995;&#32479;&#21270;&#25552;&#31034;&#20248;&#21270;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;60&#31181;&#31995;&#32479;&#28040;&#24687;&#29255;&#27573;&#30340;&#24615;&#33021;&#65292;&#20998;&#21035;&#20351;&#29992;&#21644;&#19981;&#20351;&#29992;Chain of Thought&#25552;&#31034;&#65292;&#36328;&#19977;&#20010;&#21442;&#25968;&#33539;&#22260;&#20174;70&#20159;&#21040;70&#20159;&#20010;&#21464;&#37327;&#30340;&#27169;&#22411;&#65292;&#22312;GSM8K&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#32467;&#26524;&#24182;&#19981;&#22312;&#25152;&#26377;&#27169;&#22411;&#20013;&#26222;&#36941;&#36866;&#29992;&#12290;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#21253;&#25324;&#8220;&#31215;&#26497;&#24605;&#32771;&#8221;&#25552;&#31034;&#20250;&#31215;&#26497;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;Llama2-70B&#22312;&#19981;&#20351;&#29992;Chain of Thought&#26102;&#26159;&#20010;&#20363;&#22806;&#65292;&#22240;&#20026;&#21457;&#29616;&#26368;&#20339;&#31995;&#32479;&#28040;&#24687;&#23454;&#38469;&#19978;&#26159;&#27809;&#26377;&#28040;&#24687;&#12290;&#32771;&#34385;&#21040;&#32452;&#21512;&#22797;&#26434;&#24615;&#65292;&#20197;&#21450;&#20854;&#23548;&#33267;&#30340;&#21152;# Truncated due to exceeding character limit.
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10949v1 Announce Type: new  Abstract: Large Language Models (LLMs) have demonstrated remarkable problem-solving and basic mathematics abilities. However, their efficacy is highly contingent on the formulation of the prompt. This study endeavors to quantify the influence of incorporating "positive thinking" into the system message of the prompt, then compare that to systematic prompt optimization. We assess the performance of 60 combinations of system message snippets, tested with and without Chain of Thought prompting, across three models with parameters ranging from 7 to 70 billion on the GSM8K dataset. Our findings reveal that results do not universally generalize across models. In most instances, the inclusion of "positive thinking" prompts positively affected model performance. Notably, however, Llama2-70B exhibited an exception when not utilizing Chain of Thought, as the optimal system message was found to be none at all. Given the combinatorial complexity, and thus com
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CultureLLM&#30340;&#25104;&#26412;&#25928;&#30410;&#39640;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20351;&#29992;&#19990;&#30028;&#20215;&#20540;&#35843;&#26597;&#65288;WVS&#65289;&#20316;&#20026;&#31181;&#23376;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#25552;&#20986;&#30340;&#35821;&#20041;&#25968;&#25454;&#22686;&#24378;&#26469;&#23558;&#25991;&#21270;&#24046;&#24322;&#32435;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#25104;&#21151;&#24494;&#35843;&#24471;&#21040;&#20102;&#28085;&#30422;&#23500;&#35029;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;9&#31181;&#25991;&#21270;&#29305;&#23450;LLMs&#20197;&#21450;&#19968;&#20010;&#32479;&#19968;&#27169;&#22411;&#65288;CultureLLM-One&#65289;&#12290;</title><link>https://arxiv.org/abs/2402.10946</link><description>&lt;p&gt;
&#23558;&#25991;&#21270;&#24046;&#24322;&#32435;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
CultureLLM: Incorporating Cultural Differences into Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10946
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CultureLLM&#30340;&#25104;&#26412;&#25928;&#30410;&#39640;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20351;&#29992;&#19990;&#30028;&#20215;&#20540;&#35843;&#26597;&#65288;WVS&#65289;&#20316;&#20026;&#31181;&#23376;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#25552;&#20986;&#30340;&#35821;&#20041;&#25968;&#25454;&#22686;&#24378;&#26469;&#23558;&#25991;&#21270;&#24046;&#24322;&#32435;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#25104;&#21151;&#24494;&#35843;&#24471;&#21040;&#20102;&#28085;&#30422;&#23500;&#35029;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;9&#31181;&#25991;&#21270;&#29305;&#23450;LLMs&#20197;&#21450;&#19968;&#20010;&#32479;&#19968;&#27169;&#22411;&#65288;CultureLLM-One&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34987;&#25253;&#36947;&#20559;&#21521;&#20110;&#26576;&#20123;&#25991;&#21270;&#65292;&#22240;&#20026;&#35757;&#32451;&#25968;&#25454;&#20027;&#35201;&#26469;&#33258;&#33521;&#35821;&#35821;&#26009;&#24211;&#12290;&#30001;&#20110;&#22810;&#35821;&#31181;&#25991;&#21270;&#25968;&#25454;&#36890;&#24120;&#36739;&#38590;&#25910;&#38598;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#25110;&#29305;&#23450;&#25991;&#21270;&#30340;&#39044;&#35757;&#32451;&#26469;&#22788;&#29702;&#36825;&#19968;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#21487;&#33021;&#24573;&#35270;&#20102;&#20302;&#36164;&#28304;&#25991;&#21270;&#30340;&#30693;&#35782;&#32570;&#20047;&#65292;&#24182;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;CultureLLM&#65292;&#36825;&#26159;&#19968;&#20010;&#25104;&#26412;&#25928;&#30410;&#39640;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#23558;&#25991;&#21270;&#24046;&#24322;&#32435;&#20837;LLMs&#20013;&#12290;CultureLLM&#37319;&#29992;&#19990;&#30028;&#20215;&#20540;&#35843;&#26597;&#65288;WVS&#65289;&#20316;&#20026;&#31181;&#23376;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#25552;&#20986;&#30340;&#35821;&#20041;&#25968;&#25454;&#22686;&#24378;&#29983;&#25104;&#35821;&#20041;&#31561;&#25928;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#20165;&#20351;&#29992;&#26469;&#33258;WVS&#30340;50&#20010;&#31181;&#23376;&#26679;&#26412;&#21644;&#22686;&#24378;&#25968;&#25454;&#65292;&#25105;&#20204;&#23545;9&#31181;&#21253;&#25324;&#23500;&#35029;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#25991;&#21270;&#29305;&#23450;LLMs&#21644;&#19968;&#20010;&#32479;&#19968;&#27169;&#22411;&#65288;CultureLLM-One&#65289;&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;&#23545;60&#20010;&#19982;&#25991;&#21270;&#30456;&#20851;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;CultureLLM&#22312;&#22686;&#24378;LLM&#30340;&#25991;&#21270;&#29305;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10946v1 Announce Type: cross  Abstract: Large language models (LLMs) are reported to be partial to certain cultures owing to the training data dominance from the English corpora. Since multilingual cultural data are often expensive to collect, existing efforts handle this by prompt engineering or culture-specific pre-training. However, they might overlook the knowledge deficiency of low-resource culture and require extensive computing resources. In this paper, we propose CultureLLM, a cost-effective solution to incorporate cultural differences into LLMs. CultureLLM adopts World Value Survey (WVS) as seed data and generates semantically equivalent training data via the proposed semantic data augmentation. Using only 50 seed samples from WVS with augmented data, we fine-tune culture-specific LLMs and one unified model (CultureLLM-One) for 9 cultures covering rich and low-resource languages. Extensive experiments on 60 culture-related datasets demonstrate that CultureLLM signif
&lt;/p&gt;</description></item><item><title>Text2Data&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#36890;&#36807;&#26080;&#30417;&#30563;&#25193;&#25955;&#27169;&#22411;&#26469;&#29702;&#35299;&#22522;&#30784;&#25968;&#25454;&#20998;&#24067;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#32570;&#20047;&#25991;&#26412;&#26631;&#31614;&#30340;&#25991;&#26412;&#21040;&#25968;&#25454;&#20219;&#21153;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.10941</link><description>&lt;p&gt;
Text2Data&#65306;&#20351;&#29992;&#25991;&#26412;&#25511;&#21046;&#30340;&#20302;&#36164;&#28304;&#25968;&#25454;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Text2Data: Low-Resource Data Generation with Textual Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10941
&lt;/p&gt;
&lt;p&gt;
Text2Data&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#36890;&#36807;&#26080;&#30417;&#30563;&#25193;&#25955;&#27169;&#22411;&#26469;&#29702;&#35299;&#22522;&#30784;&#25968;&#25454;&#20998;&#24067;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#32570;&#20047;&#25991;&#26412;&#26631;&#31614;&#30340;&#25991;&#26412;&#21040;&#25968;&#25454;&#20219;&#21153;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#20316;&#20026;&#20154;&#31867;&#19982;&#26426;&#22120;&#26080;&#32541;&#20132;&#20114;&#30340;&#19968;&#31181;&#24120;&#35265;&#30452;&#25509;&#25511;&#21046;&#20449;&#21495;&#12290;&#24847;&#35782;&#21040;&#36825;&#19968;&#25509;&#21475;&#30340;&#37325;&#35201;&#24615;&#65292;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#27491;&#22312;&#25237;&#20837;&#22823;&#37327;&#31934;&#21147;&#29983;&#25104;&#19982;&#25991;&#26412;&#25351;&#20196;&#22312;&#35821;&#20041;&#19978;&#19968;&#33268;&#30340;&#25968;&#25454;&#12290;&#34429;&#28982;&#22312;&#28085;&#30422;&#22270;&#20687;&#32534;&#36753;&#12289;&#38899;&#39057;&#21512;&#25104;&#12289;&#35270;&#39057;&#29983;&#25104;&#31561;&#39046;&#22495;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#20302;&#36164;&#28304;&#39046;&#22495;&#30001;&#20110;&#26114;&#36149;&#27880;&#37322;&#25110;&#22797;&#26434;&#25968;&#25454;&#32467;&#26500;&#65288;&#22914;&#20998;&#23376;&#12289;&#36816;&#21160;&#21160;&#24577;&#21644;&#26102;&#24207;&#65289;&#31561;&#29305;&#28857;&#65292;&#24448;&#24448;&#32570;&#20047;&#25991;&#26412;&#26631;&#31614;&#12290;&#36825;&#31181;&#19981;&#36275;&#38459;&#30861;&#20102;&#30417;&#30563;&#23398;&#20064;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23558;&#20808;&#36827;&#29983;&#25104;&#27169;&#22411;&#24212;&#29992;&#20110;&#25991;&#26412;&#21040;&#25968;&#25454;&#20219;&#21153;&#30340;&#21487;&#33021;&#24615;&#12290;&#20026;&#20102;&#24212;&#23545;&#20302;&#36164;&#28304;&#22330;&#26223;&#20013;&#30340;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Text2Data&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#36890;&#36807;&#26080;&#30417;&#30563;&#25193;&#25955;&#27169;&#22411;&#26469;&#29702;&#35299;&#22522;&#30784;&#25968;&#25454;&#20998;&#24067;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10941v1 Announce Type: cross  Abstract: Natural language serves as a common and straightforward control signal for humans to interact seamlessly with machines. Recognizing the importance of this interface, the machine learning community is investing considerable effort in generating data that is semantically coherent with textual instructions. While strides have been made in text-to-data generation spanning image editing, audio synthesis, video creation, and beyond, low-resource areas characterized by expensive annotations or complex data structures, such as molecules, motion dynamics, and time series, often lack textual labels. This deficiency impedes supervised learning, thereby constraining the application of advanced generative models for text-to-data tasks. In response to these challenges in the low-resource scenario, we propose Text2Data, a novel approach that utilizes unlabeled data to understand the underlying data distribution through an unsupervised diffusion model
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#24341;&#20837;&#20102;&#21307;&#23398;&#29109;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#22522;&#20110;ICD-9&#20195;&#30721;&#30340;&#24739;&#32773;&#39044;&#27979;&#32467;&#26524;&#65292;&#37327;&#21270;&#20102;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.10940</link><description>&lt;p&gt;
&#20020;&#24202;&#31243;&#24207;&#20195;&#30721;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#29992;&#20110;&#21307;&#23398;&#35786;&#26029;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Neural machine translation of clinical procedure codes for medical diagnosis and uncertainty quantification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10940
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#24341;&#20837;&#20102;&#21307;&#23398;&#29109;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#22522;&#20110;ICD-9&#20195;&#30721;&#30340;&#24739;&#32773;&#39044;&#27979;&#32467;&#26524;&#65292;&#37327;&#21270;&#20102;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65288;CDSS&#65289;&#26088;&#22312;&#36890;&#36807;&#23558;&#31995;&#32479;&#29983;&#25104;&#30340;&#24314;&#35758;&#19982;&#21307;&#23398;&#19987;&#19994;&#30693;&#35782;&#32467;&#21512;&#26469;&#22686;&#24378;&#20020;&#24202;&#21307;&#29983;&#30340;&#20915;&#31574;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#21307;&#23398;&#29109;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#22522;&#20110;&#25163;&#26415;ICD-9&#20195;&#30721;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#26469;&#37327;&#21270;&#24739;&#32773;&#39044;&#27979;&#32467;&#26524;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#19981;&#20165;&#23637;&#31034;&#20102;&#31243;&#24207;&#20195;&#30721;&#19982;&#23454;&#38469;&#21307;&#30103;&#32467;&#26524;&#20043;&#38388;&#30340;&#24378;&#30456;&#20851;&#24615;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10940v1 Announce Type: new  Abstract: A Clinical Decision Support System (CDSS) is designed to enhance clinician decision-making by combining system-generated recommendations with medical expertise. Given the high costs, intensive labor, and time-sensitive nature of medical treatments, there is a pressing need for efficient decision support, especially in complex emergency scenarios. In these scenarios, where information can be limited, an advanced CDSS framework that leverages AI (artificial intelligence) models to effectively reduce diagnostic uncertainty has utility. Such an AI-enabled CDSS framework with quantified uncertainty promises to be practical and beneficial in the demanding context of real-world medical care. In this study, we introduce the concept of Medical Entropy, quantifying uncertainties in patient outcomes predicted by neural machine translation based on the ICD-9 code of procedures. Our experimental results not only show strong correlations between proce
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;Inception&#22686;&#24378;&#30340;U-Net&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#24067;&#32447;&#25317;&#22622;&#21644;&#35774;&#35745;&#35268;&#21017;&#26816;&#26597;&#28909;&#28857;&#65292;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.10937</link><description>&lt;p&gt;
&#19968;&#31181;&#36731;&#37327;&#32423;Inception&#22686;&#24378;U-Net&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#24067;&#32447;&#21487;&#39044;&#27979;&#24615;
&lt;/p&gt;
&lt;p&gt;
A Lightweight Inception Boosted U-Net Neural Network for Routability Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10937
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;Inception&#22686;&#24378;&#30340;U-Net&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#24067;&#32447;&#25317;&#22622;&#21644;&#35774;&#35745;&#35268;&#21017;&#26816;&#26597;&#28909;&#28857;&#65292;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29616;&#20195;CPU&#12289;GPU&#21644;NPU&#33455;&#29255;&#35774;&#35745;&#22797;&#26434;&#24615;&#21644;&#26230;&#20307;&#31649;&#25968;&#37327;&#30340;&#19981;&#26029;&#22686;&#21152;&#65292;&#20197;&#21450;&#21322;&#23548;&#20307;&#25216;&#26415;&#33410;&#28857;&#19981;&#26029;&#32553;&#23567;&#33267;&#36817;1&#32435;&#31859;&#65292;&#24067;&#23616;&#21644;&#24067;&#32447;&#36880;&#28176;&#25104;&#20026;&#29616;&#20195;&#36229;&#22823;&#35268;&#27169;&#38598;&#25104;&#65288;VLSI&#65289;&#30005;&#36335;&#21518;&#31471;&#35774;&#35745;&#20013;&#26368;&#20851;&#38190;&#30340;&#20004;&#20010;&#36807;&#31243;&#12290;&#22914;&#20309;&#26377;&#25928;&#20934;&#30830;&#22320;&#22312;&#20808;&#26399;&#35780;&#20272;&#21487;&#36335;&#30001;&#24615;&#65288;&#22312;&#24067;&#23616;&#21644;&#20840;&#23616;&#24067;&#32447;&#38454;&#27573;&#65289;&#24050;&#25104;&#20026;&#20154;&#24037;&#26234;&#33021;&#36741;&#21161;&#30005;&#23376;&#35774;&#35745;&#33258;&#21160;&#21270;&#65288;EDA&#65289;&#39046;&#22495;&#30340;&#20851;&#38190;&#30740;&#31350;&#39046;&#22495;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;U-Net&#21464;&#20307;&#27169;&#22411;&#65292;&#36890;&#36807;&#23884;&#20837;Inception&#27169;&#22359;&#26469;&#39044;&#27979;&#36335;&#30001;&#25317;&#22622;&#65288;RC&#65289;&#21644;&#35774;&#35745;&#35268;&#21017;&#26816;&#26597;&#65288;DRC&#65289;&#28909;&#28857;&#12290;&#26368;&#36817;&#21457;&#24067;&#30340;CircuitNet&#25968;&#25454;&#38598;&#22522;&#20934;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;Avg-NRMSE&#65288;&#24179;&#22343;&#24402;&#19968;&#21270;&#26681;&#22343;&#26041;&#35823;&#24046;&#65289;&#26041;&#38754;&#23454;&#29616;&#20102;&#39640;&#36798;5%&#65288;RC&#65289;&#21644;20%&#65288;DRC&#65289;&#30340;&#38477;&#20302;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10937v1 Announce Type: cross  Abstract: As the modern CPU, GPU, and NPU chip design complexity and transistor counts keep increasing, and with the relentless shrinking of semiconductor technology nodes to nearly 1 nanometer, the placement and routing have gradually become the two most pivotal processes in modern very-large-scale-integrated (VLSI) circuit back-end design. How to evaluate routability efficiently and accurately in advance (at the placement and global routing stages) has grown into a crucial research area in the field of artificial intelligence (AI) assisted electronic design automation (EDA). In this paper, we propose a novel U-Net variant model boosted by an Inception embedded module to predict Routing Congestion (RC) and Design Rule Checking (DRC) hotspots. Experimental results on the recently published CircuitNet dataset benchmark show that our proposed method achieves up to 5% (RC) and 20% (DRC) rate reduction in terms of Avg-NRMSE (Average Normalized Root 
&lt;/p&gt;</description></item><item><title>ConSmax&#26159;&#19968;&#31181;&#30828;&#20214;&#21451;&#22909;&#22411;Softmax&#26367;&#20195;&#26041;&#26696;&#65292;&#36890;&#36807;&#24341;&#20837;&#21487;&#23398;&#20064;&#21442;&#25968;&#65292;&#22312;&#19981;&#24433;&#21709;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#23545;&#21407;Softmax&#20851;&#38190;&#20219;&#21153;&#30340;&#39640;&#25928;&#22788;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.10930</link><description>&lt;p&gt;
ConSmax: &#20855;&#26377;&#21487;&#23398;&#20064;&#21442;&#25968;&#30340;&#30828;&#20214;&#21451;&#22909;&#22411;Softmax&#26367;&#20195;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
ConSmax: Hardware-Friendly Alternative Softmax with Learnable Parameters
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10930
&lt;/p&gt;
&lt;p&gt;
ConSmax&#26159;&#19968;&#31181;&#30828;&#20214;&#21451;&#22909;&#22411;Softmax&#26367;&#20195;&#26041;&#26696;&#65292;&#36890;&#36807;&#24341;&#20837;&#21487;&#23398;&#20064;&#21442;&#25968;&#65292;&#22312;&#19981;&#24433;&#21709;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#23545;&#21407;Softmax&#20851;&#38190;&#20219;&#21153;&#30340;&#39640;&#25928;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#27880;&#24847;&#26426;&#21046;&#23558;&#22522;&#20110;transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#21367;&#31215;&#21644;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21306;&#20998;&#24320;&#26469;&#12290;&#23613;&#31649;&#24615;&#33021;&#26377;&#25152;&#25552;&#21319;&#65292;&#20294;&#30001;&#20110;&#33258;&#27880;&#24847;&#20013;&#24191;&#27867;&#20351;&#29992;Softmax&#65292;&#22312;&#30789;&#19978;&#23454;&#29616;&#23454;&#26102;LLM&#25512;&#26029;&#20173;&#20855;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Constant Softmax&#65288;ConSmax&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;Softmax&#26367;&#20195;&#26041;&#26696;&#65292;&#37319;&#29992;&#21487;&#24494;&#30340;&#35268;&#33539;&#21270;&#21442;&#25968;&#26469;&#28040;&#38500;Softmax&#20013;&#30340;&#26368;&#22823;&#25628;&#32034;&#21644;&#20998;&#27597;&#27714;&#21644;&#65292;&#23454;&#29616;&#20102;&#22823;&#35268;&#27169;&#24182;&#34892;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10930v1 Announce Type: cross  Abstract: The self-attention mechanism sets transformer-based large language model (LLM) apart from the convolutional and recurrent neural networks. Despite the performance improvement, achieving real-time LLM inference on silicon is challenging due to the extensively used Softmax in self-attention. Apart from the non-linearity, the low arithmetic intensity greatly reduces the processing parallelism, which becomes the bottleneck especially when dealing with a longer context. To address this challenge, we propose Constant Softmax (ConSmax), a software-hardware co-design as an efficient Softmax alternative. ConSmax employs differentiable normalization parameters to remove the maximum searching and denominator summation in Softmax. It allows for massive parallelization while performing the critical tasks of Softmax. In addition, a scalable ConSmax hardware utilizing a bitwidth-split look-up table (LUT) can produce lossless non-linear operation and 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20840;&#38754;&#22238;&#39038;&#20102;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#21450;&#30456;&#20851;&#27169;&#22411;&#22312;&#29289;&#29702;&#20449;&#24687;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#25968;&#20540;&#20998;&#26512;&#32467;&#26524;&#65292;&#25552;&#20379;&#20102;&#23545;PINNs&#36817;&#20284;PDE&#36807;&#31243;&#20013;&#35823;&#24046;&#25104;&#20998;&#30340;&#32479;&#19968;&#26694;&#26550;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2402.10926</link><description>&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#21450;&#30456;&#20851;&#27169;&#22411;&#22312;&#29289;&#29702;&#20449;&#24687;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#25968;&#20540;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Numerical analysis of physics-informed neural networks and related models in physics-informed machine learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10926
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20840;&#38754;&#22238;&#39038;&#20102;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#21450;&#30456;&#20851;&#27169;&#22411;&#22312;&#29289;&#29702;&#20449;&#24687;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#25968;&#20540;&#20998;&#26512;&#32467;&#26524;&#65292;&#25552;&#20379;&#20102;&#23545;PINNs&#36817;&#20284;PDE&#36807;&#31243;&#20013;&#35823;&#24046;&#25104;&#20998;&#30340;&#32479;&#19968;&#26694;&#26550;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#21450;&#20854;&#21464;&#31181;&#36817;&#24180;&#26469;&#22312;&#20559;&#24494;&#20998;&#26041;&#31243;&#27491;&#28436;&#21644;&#21453;&#38382;&#39064;&#30340;&#25968;&#20540;&#27169;&#25311;&#20013;&#22791;&#21463;&#38738;&#30544;&#12290;&#26412;&#25991;&#26088;&#22312;&#20840;&#38754;&#22238;&#39038;&#30446;&#21069;&#20851;&#20110;PINNs&#21450;&#20854;&#30456;&#20851;&#27169;&#22411;&#30340;&#25968;&#20540;&#20998;&#26512;&#32467;&#26524;&#65292;&#36825;&#20123;&#27169;&#22411;&#26500;&#25104;&#20102;&#29289;&#29702;&#20449;&#24687;&#26426;&#22120;&#23398;&#20064;&#30340;&#22522;&#30784;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20998;&#26512;PINNs&#22312;&#36924;&#36817;PDE&#26102;&#20135;&#29983;&#30340;&#35823;&#24046;&#30340;&#21508;&#20010;&#32452;&#25104;&#37096;&#20998;&#12290;&#37325;&#28857;&#22238;&#39038;&#20102;&#20851;&#20110;&#36924;&#36817;&#35823;&#24046;&#12289;&#27867;&#21270;&#35823;&#24046;&#21644;&#35757;&#32451;&#35823;&#24046;&#30340;&#21487;&#29992;&#32467;&#26524;&#65292;&#20197;&#21450;&#23427;&#20204;&#22312;&#23545;&#24212;PDE&#31867;&#22411;&#21644;&#24213;&#23618;&#22495;&#30340;&#32500;&#24230;&#26041;&#38754;&#30340;&#34892;&#20026;&#12290;&#29305;&#21035;&#22320;&#65292;&#38416;&#26126;&#20102;&#35299;&#20915;&#26041;&#26696;&#30340;&#27491;&#21017;&#24615;&#21450;&#20854;&#23545;&#35823;&#24046;&#20998;&#26512;&#20013;&#30340;&#25200;&#21160;&#30340;&#31283;&#23450;&#24615;&#12290;&#25968;&#20540;&#32467;&#26524;&#20063;&#24471;&#21040;&#20102;&#23637;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10926v1 Announce Type: cross  Abstract: Physics-informed neural networks (PINNs) and their variants have been very popular in recent years as algorithms for the numerical simulation of both forward and inverse problems for partial differential equations. This article aims to provide a comprehensive review of currently available results on the numerical analysis of PINNs and related models that constitute the backbone of physics-informed machine learning. We provide a unified framework in which analysis of the various components of the error incurred by PINNs in approximating PDEs can be effectively carried out. A detailed review of available results on approximation, generalization and training errors and their behavior with respect to the type of the PDE and the dimension of the underlying domain is presented. In particular, the role of the regularity of the solutions and their stability to perturbations in the error analysis is elucidated. Numerical results are also presen
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#33258;&#36866;&#24212;&#32570;&#22833;&#27169;&#24577;&#24773;&#32490;&#35782;&#21035;, &#35813;&#27169;&#22411;&#21253;&#25324;&#26597;&#35810;&#33258;&#36866;&#24212;&#34701;&#21512;&#21644;&#22810;&#27169;&#24577;&#32852;&#21512;&#23884;&#20837;&#23398;&#20064;&#20004;&#22823;&#29305;&#28857;&#65292;&#26088;&#22312;&#25552;&#39640;&#24773;&#32490;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.10921</link><description>&lt;p&gt;
AM^2-EmoJE&#65306;&#36890;&#36807;&#32852;&#21512;&#23884;&#20837;&#23398;&#20064;&#23454;&#29616;&#23545;&#35805;&#20013;&#30340;&#33258;&#36866;&#24212;&#32570;&#22833;&#27169;&#24577;&#24773;&#32490;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
AM^2-EmoJE: Adaptive Missing-Modality Emotion Recognition in Conversation via Joint Embedding Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10921
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#33258;&#36866;&#24212;&#32570;&#22833;&#27169;&#24577;&#24773;&#32490;&#35782;&#21035;, &#35813;&#27169;&#22411;&#21253;&#25324;&#26597;&#35810;&#33258;&#36866;&#24212;&#34701;&#21512;&#21644;&#22810;&#27169;&#24577;&#32852;&#21512;&#23884;&#20837;&#23398;&#20064;&#20004;&#22823;&#29305;&#28857;&#65292;&#26088;&#22312;&#25552;&#39640;&#24773;&#32490;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#24773;&#32490;&#21487;&#20197;&#36890;&#36807;&#19981;&#21516;&#27169;&#24335;&#34920;&#36798;&#65292;&#20363;&#22914;&#38899;&#39057;&#12289;&#35270;&#39057;&#21644;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#27599;&#31181;&#27169;&#24335;&#22312;&#23637;&#31034;&#24773;&#32490;&#26102;&#30340;&#36129;&#29486;&#24182;&#19981;&#22343;&#21248;&#12290;&#27492;&#22806;&#65292;&#22312;&#27979;&#35797;&#26102;&#65292;&#19981;&#19968;&#23450;&#24635;&#26159;&#33021;&#22815;&#20445;&#35777;&#23436;&#25972;&#30340;&#27169;&#24335;&#29305;&#23450;&#32454;&#33410;&#21487;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AM^2-EmoJE&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#32852;&#21512;&#23884;&#20837;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#23545;&#35805;&#20013;&#23454;&#29616;&#33258;&#36866;&#24212;&#32570;&#22833;&#27169;&#24577;&#24773;&#32490;&#35782;&#21035;&#65292;&#35813;&#27169;&#22411;&#22522;&#20110;&#20004;&#26041;&#38754;&#30340;&#36129;&#29486;&#65306;&#39318;&#20808;&#65292;&#26597;&#35810;&#33258;&#36866;&#24212;&#34701;&#21512;&#21487;&#20197;&#33258;&#21160;&#23398;&#20064;&#20854;&#27169;&#24335;&#29305;&#23450;&#34920;&#31034;&#22312;&#26597;&#35810;&#29305;&#23450;&#26041;&#24335;&#19979;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#27169;&#22411;&#26088;&#22312;&#20248;&#20808;&#32771;&#34385;&#24773;&#32490;&#27169;&#24335;&#30340;&#27169;&#24335;&#19981;&#21464;&#31354;&#38388;&#26597;&#35810;&#32454;&#33410;&#65292;&#21516;&#26102;&#22312;&#23398;&#20064;&#30340;&#22810;&#27169;&#24335;&#26597;&#35810;&#25551;&#36848;&#31526;&#20013;&#20445;&#30041;&#20854;&#29420;&#21344;&#27169;&#24335;&#26041;&#38754;&#12290;&#20854;&#27425;&#65292;&#22810;&#27169;&#24577;&#32852;&#21512;&#23884;&#20837;&#23398;&#20064;&#27169;&#22359;&#26126;&#30830;&#35299;&#20915;&#20102;&#27979;&#35797;&#26102;&#30340;&#21508;&#31181;&#32570;&#22833;&#27169;&#24577;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10921v1 Announce Type: new  Abstract: Human emotion can be presented in different modes i.e., audio, video, and text. However, the contribution of each mode in exhibiting each emotion is not uniform. Furthermore, the availability of complete mode-specific details may not always be guaranteed in the test time. In this work, we propose AM^2-EmoJE, a model for Adaptive Missing-Modality Emotion Recognition in Conversation via Joint Embedding Learning model that is grounded on two-fold contributions: First, a query adaptive fusion that can automatically learn the relative importance of its mode-specific representations in a query-specific manner. By this the model aims to prioritize the mode-invariant spatial query details of the emotion patterns, while also retaining its mode-exclusive aspects within the learned multimodal query descriptor. Second the multimodal joint embedding learning module that explicitly addresses various missing modality scenarios in test-time. By this, th
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;LLAMA2&#35821;&#35328;&#27169;&#22411;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#33021;&#22815;&#20174;&#31038;&#20132;&#23186;&#20307;&#21644;&#32039;&#24613;&#28040;&#24687;&#20013;&#35782;&#21035;&#21644;&#20998;&#31867;&#32039;&#24613;&#24773;&#20917;&#30340;&#26041;&#27861;&#65292;&#21487;&#21327;&#21161;&#22312;&#20840;&#22269;&#33539;&#22260;&#20869;&#30340;&#32039;&#24613;&#24773;&#20917;&#19979;&#20844;&#20849;&#23433;&#20840;&#35805;&#21153;&#21592;&#21644;&#22823;&#20247;&#65292;&#25552;&#20379;&#30456;&#20851;&#25351;&#23548;&#24182;&#36890;&#30693;&#25919;&#24220;&#26426;&#26500;&#12290;</title><link>https://arxiv.org/abs/2402.10908</link><description>&lt;p&gt;
LLM&#36741;&#21161;&#21361;&#26426;&#31649;&#29702;&#65306;&#26500;&#24314;&#29992;&#20110;&#26377;&#25928;&#24212;&#24613;&#21709;&#24212;&#21644;&#20844;&#20247;&#21327;&#20316;&#30340;&#20808;&#36827;LLM&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
LLM-Assisted Crisis Management: Building Advanced LLM Platforms for Effective Emergency Response and Public Collaboration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10908
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;LLAMA2&#35821;&#35328;&#27169;&#22411;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#33021;&#22815;&#20174;&#31038;&#20132;&#23186;&#20307;&#21644;&#32039;&#24613;&#28040;&#24687;&#20013;&#35782;&#21035;&#21644;&#20998;&#31867;&#32039;&#24613;&#24773;&#20917;&#30340;&#26041;&#27861;&#65292;&#21487;&#21327;&#21161;&#22312;&#20840;&#22269;&#33539;&#22260;&#20869;&#30340;&#32039;&#24613;&#24773;&#20917;&#19979;&#20844;&#20849;&#23433;&#20840;&#35805;&#21153;&#21592;&#21644;&#22823;&#20247;&#65292;&#25552;&#20379;&#30456;&#20851;&#25351;&#23548;&#24182;&#36890;&#30693;&#25919;&#24220;&#26426;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32039;&#24613;&#24773;&#20917;&#21644;&#37325;&#22823;&#20107;&#20214;&#24448;&#24448;&#36805;&#36895;&#21457;&#23637;&#65292;&#38656;&#35201;&#36805;&#36895;&#26377;&#25928;&#30340;&#21709;&#24212;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLAMA2&#65292;&#20174;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#21644;&#30452;&#25509;&#30340;&#32039;&#24613;&#28040;&#24687;&#20013;&#35782;&#21035;&#21644;&#20998;&#31867;&#32039;&#24613;&#24773;&#20917;&#12290;&#26088;&#22312;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#21147;&#37327;&#65292;&#21327;&#21161;&#20844;&#20849;&#23433;&#20840;&#35805;&#21153;&#21592;&#21644;&#22823;&#37327;&#27665;&#20247;&#22312;&#20840;&#22269;&#33539;&#22260;&#20869;&#30340;&#32039;&#24613;&#24773;&#20917;&#20013;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#38598;&#20013;&#20110;&#24320;&#21457;&#19968;&#31181;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#22815;&#29702;&#35299;&#29992;&#25143;&#22312;911&#21628;&#21483;&#20013;&#25551;&#36848;&#33258;&#24049;&#30340;&#24773;&#20917;&#65292;&#20351;LLAMA2&#33021;&#22815;&#20998;&#26512;&#20869;&#23481;&#24182;&#20026;&#35805;&#21153;&#21592;&#25552;&#20379;&#30456;&#20851;&#25351;&#23548;&#65292;&#21516;&#26102;&#21019;&#24314;&#24037;&#20316;&#27969;&#31243;&#65292;&#22312;&#24517;&#35201;&#26102;&#23558;&#21628;&#21483;&#32773;&#20449;&#24687;&#36890;&#30693;&#25919;&#24220;&#26426;&#26500;&#12290;&#35813;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#30340;&#21478;&#19968;&#20010;&#22909;&#22788;&#26159;&#65292;&#24403;911&#31995;&#32479;&#19981;&#22570;&#37325;&#36127;&#26102;&#65292;&#23427;&#33021;&#22815;&#22312;&#37325;&#22823;&#32039;&#24613;&#20107;&#20214;&#20013;&#21327;&#21161;&#20154;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10908v1 Announce Type: cross  Abstract: Emergencies and critical incidents often unfold rapidly, necessitating a swift and effective response. In this research, we introduce a novel approach to identify and classify emergency situations from social media posts and direct emergency messages using an open source Large Language Model, LLAMA2. The goal is to harness the power of natural language processing and machine learning to assist public safety telecommunicators and huge crowds during countrywide emergencies. Our research focuses on developing a language model that can understand users describe their situation in the 911 call, enabling LLAMA2 to analyze the content and offer relevant instructions to the telecommunicator, while also creating workflows to notify government agencies with the caller's information when necessary. Another benefit this language model provides is its ability to assist people during a significant emergency incident when the 911 system is overwhelme
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#28151;&#21512;&#31070;&#32463;&#32593;&#32476;&#32467;&#21512;Hermite&#20989;&#25968;&#26681;&#20316;&#20026;&#37197;&#28857;&#65292;&#25552;&#39640;&#20102;&#35299;&#20915;&#20108;&#32500;&#34203;&#23450;&#35860;&#26041;&#31243;&#30340;&#25928;&#29575;&#21644;&#31934;&#24230;&#65292;&#19982;&#20854;&#20182;&#31070;&#32463;&#32593;&#32476;&#21644;&#26041;&#27861;&#30456;&#27604;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.10649</link><description>&lt;p&gt;
Hermite&#31070;&#32463;&#32593;&#32476;&#27169;&#25311;&#35299;&#20915;&#20108;&#32500;&#34203;&#23450;&#35860;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Hermite Neural Network Simulation for Solving the 2D Schrodinger Equation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10649
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#28151;&#21512;&#31070;&#32463;&#32593;&#32476;&#32467;&#21512;Hermite&#20989;&#25968;&#26681;&#20316;&#20026;&#37197;&#28857;&#65292;&#25552;&#39640;&#20102;&#35299;&#20915;&#20108;&#32500;&#34203;&#23450;&#35860;&#26041;&#31243;&#30340;&#25928;&#29575;&#21644;&#31934;&#24230;&#65292;&#19982;&#20854;&#20182;&#31070;&#32463;&#32593;&#32476;&#21644;&#26041;&#27861;&#30456;&#27604;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv: 2402.10649v1 &#20844;&#21578;&#31867;&#22411;: &#20132;&#21449;&#25688;&#35201;: &#34203;&#23450;&#35860;&#26041;&#31243;&#26159;&#25551;&#36848;&#37327;&#23376;&#21147;&#23398;&#31995;&#32479;&#20013;&#27874;&#20989;&#25968;&#34892;&#20026;&#30340;&#25968;&#23398;&#26041;&#31243;&#12290;&#23427;&#26159;&#19968;&#20010;&#20559;&#24494;&#20998;&#26041;&#31243;&#65292;&#25552;&#20379;&#20102;&#26377;&#20851;&#37327;&#23376;&#21147;&#23398;&#22522;&#26412;&#21407;&#29702;&#30340;&#23453;&#36149;&#35265;&#35299;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#28151;&#21512;&#31070;&#32463;&#32593;&#32476;&#19982;&#22522;&#20110;Hermite&#20989;&#25968;&#30340;&#37197;&#28857;&#26041;&#27861;&#26469;&#20197;&#36275;&#22815;&#30340;&#31934;&#24230;&#35299;&#20915;&#34203;&#23450;&#35860;&#26041;&#31243;&#12290;&#26368;&#21021;&#65292;Hermite&#20989;&#25968;&#30340;&#26681;&#34987;&#29992;&#20316;&#37197;&#28857;&#65292;&#25552;&#39640;&#20102;&#35299;&#20915;&#26041;&#26696;&#30340;&#25928;&#29575;&#12290;&#34203;&#23450;&#35860;&#26041;&#31243;&#22312;&#26080;&#38480;&#22495;&#20013;&#23450;&#20041;&#65292;&#20351;&#29992;Hermite&#20989;&#25968;&#20316;&#20026;&#28608;&#27963;&#20989;&#25968;&#23548;&#33268;&#20102;&#20986;&#33394;&#30340;&#31934;&#24230;&#12290;&#26368;&#21518;&#65292;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#29992;MATLAB&#30340;Simulink&#24037;&#20855;&#36827;&#34892;&#20102;&#27169;&#25311;&#12290;&#28982;&#21518;&#23558;&#32467;&#26524;&#19982;&#20351;&#29992;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#31070;&#32463;&#32593;&#32476;&#21644;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#33719;&#24471;&#30340;&#32467;&#26524;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10649v1 Announce Type: cross  Abstract: The Schrodinger equation is a mathematical equation describing the wave function's behavior in a quantum-mechanical system. It is a partial differential equation that provides valuable insights into the fundamental principles of quantum mechanics. In this paper, the aim was to solve the Schrodinger equation with sufficient accuracy by using a mixture of neural networks with the collocation method base Hermite functions. Initially, the Hermite functions roots were employed as collocation points, enhancing the efficiency of the solution. The Schrodinger equation is defined in an infinite domain, the use of Hermite functions as activation functions resulted in excellent precision. Finally, the proposed method was simulated using MATLAB's Simulink tool. The results were then compared with those obtained using Physics-informed neural networks and the presented method.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Rewards-in-Context&#65288;RiC&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#22810;&#20010;&#22870;&#21169;&#26465;&#20214;&#25511;&#21046;&#22522;&#30784;&#27169;&#22411;&#30340;&#21709;&#24212;&#65292;&#24182;&#24212;&#29992;&#26377;&#30417;&#30563;&#30340;&#24494;&#35843;&#36827;&#34892;&#23545;&#40784;&#12290;&#23427;&#20855;&#26377;&#31616;&#21333;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#24182;&#25903;&#25345;&#22312;&#25512;&#29702;&#26102;&#21160;&#24577;&#35843;&#25972;&#29992;&#25143;&#20559;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.10207</link><description>&lt;p&gt;
&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#22870;&#21169;&#65306;&#22522;&#20110;&#21160;&#24577;&#20559;&#22909;&#35843;&#25972;&#30340;&#22810;&#30446;&#26631;&#22522;&#30784;&#27169;&#22411;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Rewards-in-Context: Multi-objective Alignment of Foundation Models with Dynamic Preference Adjustment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10207
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Rewards-in-Context&#65288;RiC&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#22810;&#20010;&#22870;&#21169;&#26465;&#20214;&#25511;&#21046;&#22522;&#30784;&#27169;&#22411;&#30340;&#21709;&#24212;&#65292;&#24182;&#24212;&#29992;&#26377;&#30417;&#30563;&#30340;&#24494;&#35843;&#36827;&#34892;&#23545;&#40784;&#12290;&#23427;&#20855;&#26377;&#31616;&#21333;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#24182;&#25903;&#25345;&#22312;&#25512;&#29702;&#26102;&#21160;&#24577;&#35843;&#25972;&#29992;&#25143;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#22522;&#20110;&#20154;&#31867;&#20559;&#22909;&#30340;&#22522;&#30784;&#27169;&#22411;&#22810;&#30446;&#26631;&#23545;&#40784;&#38382;&#39064;&#65292;&#36825;&#26159;&#23454;&#29616;&#26377;&#30410;&#21644;&#26080;&#23475;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#23545;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#36890;&#24120;&#26159;&#26114;&#36149;&#19988;&#19981;&#31283;&#23450;&#30340;&#65292;&#24182;&#19988;&#20154;&#31867;&#20559;&#22909;&#30340;&#22810;&#32500;&#24230;&#12289;&#24322;&#36136;&#24615;&#21644;&#20914;&#31361;&#24615;&#36827;&#19968;&#27493;&#22797;&#26434;&#21270;&#20102;&#23545;&#40784;&#36807;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Rewards-in-Context&#65288;RiC&#65289;&#26041;&#27861;&#65292;&#23427;&#20351;&#24471;&#22522;&#30784;&#27169;&#22411;&#30340;&#21709;&#24212;&#21462;&#20915;&#20110;&#20854;&#25552;&#31034;&#19978;&#19979;&#25991;&#20013;&#30340;&#22810;&#20010;&#22870;&#21169;&#65292;&#24182;&#24212;&#29992;&#26377;&#30417;&#30563;&#30340;&#24494;&#35843;&#26469;&#36827;&#34892;&#23545;&#40784;&#12290;RiC&#30340;&#26174;&#33879;&#29305;&#28857;&#26159;&#31616;&#21333;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#22240;&#20026;&#23427;&#21482;&#38656;&#35201;&#23545;&#21333;&#20010;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#26377;&#30417;&#30563;&#30340;&#24494;&#35843;&#65292;&#24182;&#25903;&#25345;&#22312;&#25512;&#29702;&#26102;&#21160;&#24577;&#35843;&#25972;&#29992;&#25143;&#20559;&#22909;&#12290;&#21463;&#21040;&#25277;&#35937;&#30340;&#20984;&#20248;&#21270;&#38382;&#39064;&#30340;&#35299;&#26512;&#35299;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#25512;&#29702;&#26102;&#35843;&#25972;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10207v1 Announce Type: cross  Abstract: We consider the problem of multi-objective alignment of foundation models with human preferences, which is a critical step towards helpful and harmless AI systems. However, it is generally costly and unstable to fine-tune large foundation models using reinforcement learning (RL), and the multi-dimensionality, heterogeneity, and conflicting nature of human preferences further complicate the alignment process. In this paper, we introduce Rewards-in-Context (RiC), which conditions the response of a foundation model on multiple rewards in its prompt context and applies supervised fine-tuning for alignment. The salient features of RiC are simplicity and adaptivity, as it only requires supervised fine-tuning of a single foundation model and supports dynamic adjustment for user preferences during inference time. Inspired by the analytical solution of an abstracted convex optimization problem, our dynamic inference-time adjustment method appro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;Transformer&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#23616;&#38480;&#24615;&#65292;&#21457;&#29616;&#20854;&#27880;&#24847;&#21147;&#26426;&#21046;&#26159;&#27867;&#21270;&#33021;&#21147;&#19981;&#36275;&#30340;&#21407;&#22240;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#27973;&#23618;&#36731;&#37327;&#32423;&#30340;Transformer&#27169;&#22411;SAMformer&#65292;&#36890;&#36807;&#38160;&#24230;&#24863;&#30693;&#20248;&#21270;&#36991;&#20813;&#20102;&#38519;&#20837;&#22351;&#30340;&#23616;&#37096;&#26368;&#23567;&#20540;&#65292;&#24182;&#22312;&#24120;&#29992;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#19978;&#36229;&#36807;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;TSMixer&#12290;</title><link>https://arxiv.org/abs/2402.10198</link><description>&lt;p&gt;
&#20351;&#29992;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#21644;&#36890;&#36947;&#27880;&#24847;&#21147;&#35299;&#38145;Transformer&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Unlocking the Potential of Transformers in Time Series Forecasting with Sharpness-Aware Minimization and Channel-Wise Attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10198
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Transformer&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#23616;&#38480;&#24615;&#65292;&#21457;&#29616;&#20854;&#27880;&#24847;&#21147;&#26426;&#21046;&#26159;&#27867;&#21270;&#33021;&#21147;&#19981;&#36275;&#30340;&#21407;&#22240;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#27973;&#23618;&#36731;&#37327;&#32423;&#30340;Transformer&#27169;&#22411;SAMformer&#65292;&#36890;&#36807;&#38160;&#24230;&#24863;&#30693;&#20248;&#21270;&#36991;&#20813;&#20102;&#38519;&#20837;&#22351;&#30340;&#23616;&#37096;&#26368;&#23567;&#20540;&#65292;&#24182;&#22312;&#24120;&#29992;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#19978;&#36229;&#36807;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;TSMixer&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#26550;&#26500;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#21462;&#24471;&#20102;&#31361;&#30772;&#24615;&#30340;&#24615;&#33021;&#65292;&#20294;&#22312;&#22810;&#20803;&#38271;&#26399;&#39044;&#27979;&#26041;&#38754;&#65292;&#23427;&#20204;&#20173;&#28982;&#19981;&#22914;&#26356;&#31616;&#21333;&#30340;&#32447;&#24615;&#22522;&#32447;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#19968;&#29616;&#35937;&#65292;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;&#19968;&#20010;&#29609;&#20855;&#32447;&#24615;&#39044;&#27979;&#38382;&#39064;&#65292;&#23637;&#31034;&#20102;&#23613;&#31649;Transformer&#20855;&#26377;&#39640;&#34920;&#36798;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#26080;&#27861;&#25910;&#25947;&#21040;&#30495;&#27491;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30830;&#23450;Transformer&#30340;&#27880;&#24847;&#21147;&#26159;&#36896;&#25104;&#20854;&#20302;&#27867;&#21270;&#33021;&#21147;&#30340;&#21407;&#22240;&#12290;&#22522;&#20110;&#36825;&#19968;&#35748;&#35782;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27973;&#23618;&#36731;&#37327;&#32423;&#30340;Transformer&#27169;&#22411;&#65292;&#22312;&#38160;&#24230;&#24863;&#30693;&#20248;&#21270;&#30340;&#24773;&#20917;&#19979;&#25104;&#21151;&#36991;&#20813;&#20102;&#22351;&#30340;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20010;&#32467;&#26524;&#36866;&#29992;&#20110;&#25152;&#26377;&#24120;&#29992;&#30340;&#23454;&#38469;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#12290;&#29305;&#21035;&#26159;&#65292;&#30456;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;TSMixer&#65292;SAMformer&#30340;&#24179;&#22343;&#24615;&#33021;&#25552;&#39640;&#20102;14.33%&#65292;&#24182;&#19988;&#21442;&#25968;&#25968;&#37327;&#20943;&#23569;&#20102;&#32422;4&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10198v1 Announce Type: new  Abstract: Transformer-based architectures achieved breakthrough performance in natural language processing and computer vision, yet they remain inferior to simpler linear baselines in multivariate long-term forecasting. To better understand this phenomenon, we start by studying a toy linear forecasting problem for which we show that transformers are incapable of converging to their true solution despite their high expressive power. We further identify the attention of transformers as being responsible for this low generalization capacity. Building upon this insight, we propose a shallow lightweight transformer model that successfully escapes bad local minima when optimized with sharpness-aware optimization. We empirically demonstrate that this result extends to all commonly used real-world multivariate time series datasets. In particular, SAMformer surpasses the current state-of-the-art model TSMixer by 14.33% on average, while having ~4 times few
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35774;&#35745;&#22870;&#21169;&#24314;&#27169;&#36807;&#31243;&#20013;&#30340;&#25968;&#25454;&#38598;&#20449;&#24687;&#32467;&#26500;&#65292;&#20174;&#22270;&#35770;&#30340;&#35270;&#35282;&#25552;&#20986;&#20102;RLHF&#20013;&#22870;&#21169;&#27867;&#21270;&#30340;&#38382;&#39064;&#65292;&#20197;&#35299;&#20915;&#22810;&#26679;&#30340;&#29615;&#22659;&#12289;&#20302;&#25104;&#26412;&#26631;&#27880;&#21644;&#21487;&#38752;&#30340;&#23545;&#40784;&#24615;&#33021;&#38388;&#30340;&#19981;&#20860;&#23481;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.10184</link><description>&lt;p&gt;
&#37325;&#22609;RLHF&#20013;&#30340;&#20449;&#24687;&#32467;&#26500;&#65306;&#22522;&#20110;&#22270;&#35770;&#30340;&#22870;&#21169;&#27867;&#21270;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Rethinking Information Structures in RLHF: Reward Generalization from a Graph Theory Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10184
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35774;&#35745;&#22870;&#21169;&#24314;&#27169;&#36807;&#31243;&#20013;&#30340;&#25968;&#25454;&#38598;&#20449;&#24687;&#32467;&#26500;&#65292;&#20174;&#22270;&#35770;&#30340;&#35270;&#35282;&#25552;&#20986;&#20102;RLHF&#20013;&#22870;&#21169;&#27867;&#21270;&#30340;&#38382;&#39064;&#65292;&#20197;&#35299;&#20915;&#22810;&#26679;&#30340;&#29615;&#22659;&#12289;&#20302;&#25104;&#26412;&#26631;&#27880;&#21644;&#21487;&#38752;&#30340;&#23545;&#40784;&#24615;&#33021;&#38388;&#30340;&#19981;&#20860;&#23481;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#65288;RLHF&#65289;&#23384;&#22312;&#19968;&#20010;&#19977;&#38590;&#38382;&#39064;&#65306;&#39640;&#24230;&#22810;&#26679;&#30340;&#29615;&#22659;&#12289;&#20302;&#26631;&#27880;&#25104;&#26412;&#21644;&#21487;&#38752;&#30340;&#23545;&#40784;&#24615;&#33021;&#20043;&#38388;&#30340;&#19981;&#20860;&#23481;&#24615;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#35774;&#35745;&#22870;&#21169;&#24314;&#27169;&#36807;&#31243;&#20013;&#30340;&#25968;&#25454;&#38598;&#20449;&#24687;&#32467;&#26500;&#26469;&#32531;&#35299;&#36825;&#31181;&#19981;&#20860;&#23481;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;RLHF&#36807;&#31243;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#23558;&#20854;&#25551;&#32472;&#20026;&#25991;&#26412;&#20998;&#24067;&#19978;&#30340;&#33258;&#21160;&#32534;&#30721;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#24418;&#24335;&#21270;&#20102;RLHF&#30446;&#26631;&#65292;&#21363;&#30830;&#20445;&#20154;&#31867;&#20559;&#22909;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#34892;&#20026;&#20043;&#38388;&#30340;&#20998;&#24067;&#19968;&#33268;&#24615;&#12290;&#22522;&#20110;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;RLHF&#22870;&#21169;&#24314;&#27169;&#38454;&#27573;&#20013;&#20449;&#24687;&#32467;&#26500;&#30340;&#24615;&#33021;&#24433;&#21709;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#29702;&#35299;&#22870;&#21169;&#24314;&#27169;&#38454;&#27573;&#20013;&#30340;&#22870;&#21169;&#27867;&#21270;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#22270;&#35770;&#30340;&#26041;&#27861;&#26469;&#24314;&#27169;&#35821;&#20041;&#31354;&#38388;&#20013;&#30340;&#27867;&#21270;&#12290;&#20854;&#20013;&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10184v1 Announce Type: cross  Abstract: There is a trilemma in reinforcement learning from human feedback (RLHF): the incompatibility between highly diverse contexts, low labeling cost, and reliable alignment performance. Here we aim to mitigate such incompatibility through the design of dataset information structures during reward modeling. Specifically, we first reexamine the RLHF process and propose a theoretical framework portraying it as an autoencoding process over text distributions. Our framework formalizes the RLHF objective of ensuring distributional consistency between human preference and large language model (LLM) behavior. Building on this framework, we then systematically investigate the performance impact of information structure in the reward modeling stage of RLHF. To further understand reward generalization in the reward modeling stage, we introduce a new method based on random graph theory that models generalization in the semantic space. A key insight of
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#20020;&#24202;&#35774;&#32622;&#20013;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#38899;&#39057;&#20998;&#31867;&#30340;&#25928;&#26524;&#65292;&#24182;&#21457;&#29616;&#22312;&#24494;&#35843;&#20043;&#21069;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#22823;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#23545;&#20020;&#24202;&#25968;&#25454;&#30340;&#24433;&#21709;&#36739;&#22909;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;CNN&#27169;&#22411;&#21487;&#20197;&#22312;&#23567;&#25968;&#25454;&#38598;&#29615;&#22659;&#20013;&#19982;&#36716;&#25442;&#27169;&#22411;&#30456;&#23218;&#32654;&#25110;&#36229;&#36234;&#12290;</title><link>https://arxiv.org/abs/2402.10100</link><description>&lt;p&gt;
&#35843;&#35856;&#65306;&#22312;&#20020;&#24202;&#35774;&#32622;&#20013;&#20351;&#29992;&#26377;&#38480;&#25968;&#25454;&#30340;&#38899;&#39057;&#20998;&#31867;&#22120;&#24615;&#33021;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Tuning In: Analysis of Audio Classifier Performance in Clinical Settings with Limited Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10100
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#20020;&#24202;&#35774;&#32622;&#20013;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#38899;&#39057;&#20998;&#31867;&#30340;&#25928;&#26524;&#65292;&#24182;&#21457;&#29616;&#22312;&#24494;&#35843;&#20043;&#21069;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#22823;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#23545;&#20020;&#24202;&#25968;&#25454;&#30340;&#24433;&#21709;&#36739;&#22909;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;CNN&#27169;&#22411;&#21487;&#20197;&#22312;&#23567;&#25968;&#25454;&#38598;&#29615;&#22659;&#20013;&#19982;&#36716;&#25442;&#27169;&#22411;&#30456;&#23218;&#32654;&#25110;&#36229;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#20020;&#24202;&#35774;&#32622;&#20013;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#38899;&#39057;&#20998;&#31867;&#30340;&#25928;&#26524;&#65292;&#38480;&#21046;&#26465;&#20214;&#26159;&#20197;&#21453;&#26144;&#23454;&#38469;&#19990;&#30028;&#25968;&#25454;&#25910;&#38598;&#30340;&#23567;&#25968;&#25454;&#38598;&#20026;&#22522;&#30784;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#21253;&#25324;DenseNet&#21644;ConvNeXt&#22312;&#20869;&#30340;CNN&#27169;&#22411;&#65292;&#20197;&#21450;ViT&#12289;SWIN&#21644;AST&#31561;&#36716;&#25442;&#27169;&#22411;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#35832;&#22914;YAMNet&#21644;VGGish&#30340;&#39044;&#35757;&#32451;&#38899;&#39057;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24378;&#35843;&#20102;&#22312;&#29305;&#23450;&#20020;&#24202;&#25968;&#25454;&#19978;&#24494;&#35843;&#20043;&#21069;&#65292;&#22312;&#22823;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#22909;&#22788;&#12290;&#25105;&#20204;&#20174;&#21330;&#20013;&#24739;&#32773;&#20013;&#26032;&#25910;&#38598;&#20102;&#20004;&#20010;&#21069;&#25152;&#26410;&#26377;&#30340;&#24739;&#32773;&#38899;&#39057;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#21508;&#31181;&#39044;&#22788;&#29702;&#25216;&#26415;&#65292;&#21457;&#29616;&#22522;&#20110;&#23427;&#20204;&#20174;&#39044;&#35757;&#32451;&#20013;&#23398;&#20064;&#21040;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;RGB&#21644;&#28784;&#24230;&#35889;&#22270;&#36716;&#25442;&#23545;&#27169;&#22411;&#24615;&#33021;&#20135;&#29983;&#20102;&#19981;&#21516;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;CNN&#27169;&#22411;&#22312;&#23567;&#25968;&#25454;&#38598;&#29615;&#22659;&#20013;&#21487;&#20197;&#19982;&#36716;&#25442;&#27169;&#22411;&#30456;&#23218;&#32654;&#25110;&#36229;&#36234;&#65292;&#20854;&#20013;DenseNet-Contrastive&#21644;AST&#27169;&#22411;&#34920;&#29616;&#31361;&#20986;&#12290;&#26412;&#30740;&#31350;&#24378;&#35843;&#20102;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10100v1 Announce Type: cross  Abstract: This study assesses deep learning models for audio classification in a clinical setting with the constraint of small datasets reflecting real-world prospective data collection. We analyze CNNs, including DenseNet and ConvNeXt, alongside transformer models like ViT, SWIN, and AST, and compare them against pre-trained audio models such as YAMNet and VGGish. Our method highlights the benefits of pre-training on large datasets before fine-tuning on specific clinical data. We prospectively collected two first-of-their-kind patient audio datasets from stroke patients. We investigated various preprocessing techniques, finding that RGB and grayscale spectrogram transformations affect model performance differently based on the priors they learn from pre-training. Our findings indicate CNNs can match or exceed transformer models in small dataset contexts, with DenseNet-Contrastive and AST models showing notable performance. This study highlights
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#31867;&#24179;&#34913;&#20027;&#21160;&#23398;&#20064;&#30340;&#26694;&#26550;GraphCBAL&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#23398;&#20064;&#19968;&#31181;&#26368;&#20339;&#31574;&#30053;&#65292;&#36873;&#25321;&#31867;&#24179;&#34913;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#33410;&#28857;&#36827;&#34892;&#27880;&#37322;&#65292;&#20197;&#26368;&#22823;&#21270;GNNs&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.10074</link><description>&lt;p&gt;
GraphCBAL: &#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#31867;&#24179;&#34913;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
GraphCBAL: Class-Balanced Active Learning for Graph Neural Networks via Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10074
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#31867;&#24179;&#34913;&#20027;&#21160;&#23398;&#20064;&#30340;&#26694;&#26550;GraphCBAL&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#23398;&#20064;&#19968;&#31181;&#26368;&#20339;&#31574;&#30053;&#65292;&#36873;&#25321;&#31867;&#24179;&#34913;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#33410;&#28857;&#36827;&#34892;&#27880;&#37322;&#65292;&#20197;&#26368;&#22823;&#21270;GNNs&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;GNNs&#30340;&#20027;&#21160;&#23398;&#20064;&#26088;&#22312;&#20174;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#20013;&#26597;&#35810;&#26377;&#20215;&#20540;&#30340;&#26679;&#26412;&#36827;&#34892;&#27880;&#37322;&#65292;&#20197;&#26368;&#22823;&#38480;&#24230;&#22320;&#38477;&#20302;&#25104;&#26412;&#24182;&#25552;&#39640;GNNs&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;GNNs&#20013;&#30340;&#24378;&#21270;&#20027;&#21160;&#23398;&#20064;&#65292;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#39640;&#24230;&#19981;&#24179;&#34913;&#30340;&#31867;&#20998;&#24067;&#65292;&#23588;&#20854;&#26159;&#22312;&#39640;&#24230;&#20542;&#26012;&#30340;&#31867;&#21035;&#22330;&#26223;&#19979;&#12290;&#36825;&#36827;&#19968;&#27493;&#23545;&#20998;&#31867;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22686;&#24378;&#31867;&#24179;&#34913;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;GraphCBAL&#65292;&#29992;&#20110;GNNs&#12290;&#23427;&#23398;&#20064;&#19968;&#31181;&#26368;&#20339;&#31574;&#30053;&#65292;&#20197;&#33719;&#21462;&#31867;&#24179;&#34913;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#33410;&#28857;&#36827;&#34892;&#27880;&#37322;&#65292;&#20174;&#32780;&#26368;&#22823;&#21270;&#36873;&#25321;&#30340;&#26631;&#35760;&#33410;&#28857;&#35757;&#32451;&#30340;GNNs&#30340;&#24615;&#33021;&#12290;GraphCBAL&#35774;&#35745;&#20102;&#31867;&#24179;&#34913;&#24863;&#30693;&#29366;&#24577;&#21644;&#22870;&#21169;&#20989;&#25968;&#65292;&#23454;&#29616;&#27169;&#22411;&#24615;&#33021;&#21644;&#31867;&#24179;&#34913;&#20043;&#38388;&#30340;&#25240;&#34935;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25913;&#36827;&#20102;GraphCBAL&#65292;&#24471;&#21040;GraphCBAL++&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10074v1 Announce Type: new  Abstract: Graph neural networks (GNNs) have recently demonstrated significant success. Active learning for GNNs aims to query the valuable samples from the unlabeled data for annotation to maximize the GNNs' performance at a low cost. However, most existing methods for reinforced active learning in GNNs may lead to a highly imbalanced class distribution, especially in highly skewed class scenarios. This further adversely affects the classification performance. To tackle this issue, in this paper, we propose a novel reinforced class-balanced active learning framework for GNNs, namely, GraphCBAL. It learns an optimal policy to acquire class-balanced and informative nodes for annotation, maximizing the performance of GNNs trained with selected labeled nodes. GraphCBAL designs class-balance-aware states, as well as a reward function that achieves trade-off between model performance and class balance. We further upgrade GraphCBAL to GraphCBAL++ by intr
&lt;/p&gt;</description></item><item><title>LoraRetriever&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#36755;&#20837;&#30340;LoRA&#26816;&#32034;&#19982;&#21512;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#24357;&#21512;&#23454;&#38469;&#24773;&#20917;&#19979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25509;&#25910;&#21040;&#19981;&#21516;&#20219;&#21153;&#25552;&#31034;&#30340;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2402.09997</link><description>&lt;p&gt;
LoraRetriever: &#36866;&#24212;&#36755;&#20837;&#30340;LoRA&#26816;&#32034;&#19982;&#21512;&#25104;&#26041;&#27861;&#29992;&#20110;&#28151;&#21512;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
LoraRetriever: Input-Aware LoRA Retrieval and Composition for Mixed Tasks in the Wild
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09997
&lt;/p&gt;
&lt;p&gt;
LoraRetriever&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#36755;&#20837;&#30340;LoRA&#26816;&#32034;&#19982;&#21512;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#24357;&#21512;&#23454;&#38469;&#24773;&#20917;&#19979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25509;&#25910;&#21040;&#19981;&#21516;&#20219;&#21153;&#25552;&#31034;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Low-Rank Adaptation (LoRA)&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24494;&#35843;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#32780;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;LoRA&#30340;&#27169;&#22359;&#21270;&#21644;&#21363;&#25554;&#21363;&#29992;&#30340;&#29305;&#24615;&#20351;&#24471;&#33021;&#22815;&#38598;&#25104;&#21508;&#31181;&#39046;&#22495;&#29305;&#23450;&#30340;LoRA&#65292;&#20197;&#22686;&#24378;LLM&#30340;&#33021;&#21147;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#35201;&#20040;&#19987;&#27880;&#20110;&#29305;&#23450;&#30340;&#38548;&#31163;&#19979;&#28216;&#20219;&#21153;&#65292;&#35201;&#20040;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#22266;&#23450;LoRA&#30340;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24773;&#20917;&#20013;&#65292;LLM&#25509;&#25910;&#21040;&#28085;&#30422;&#19981;&#21516;&#20219;&#21153;&#30340;&#21508;&#31181;&#25552;&#31034;&#65292;&#24182;&#19988;&#20505;&#36873;LoRA&#30340;&#27744;&#32463;&#24120;&#21160;&#24577;&#26356;&#26032;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LoraRetriever&#65292;&#19968;&#31181;&#26681;&#25454;&#36755;&#20837;&#25552;&#31034;&#33258;&#36866;&#24212;&#26816;&#32034;&#21644;&#21512;&#25104;&#22810;&#20010;LoRA&#30340;&#26694;&#26550;&#12290;LoraRetriever&#21253;&#21547;&#19977;&#20010;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#65306;&#39318;&#20808;&#65292;&#35782;&#21035;&#21644;&#26816;&#32034;&#19982;&#32473;&#23450;&#36755;&#20837;&#30456;&#20851;&#30340;LoRA&#65307;&#20854;&#27425;&#65292;&#21046;&#23450;&#26377;&#25928;&#25972;&#21512;&#26816;&#32034;&#21040;&#30340;LoRA&#30340;&#31574;&#30053;&#65307;&#26368;&#21518;&#65292;&#24320;&#21457;&#39640;&#25928;&#30340;&#26041;&#27861;&#29992;&#20110;&#23454;&#29616;LoRA&#30340;&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09997v1 Announce Type: new  Abstract: Low-Rank Adaptation (LoRA) provides an effective yet efficient solution for fine-tuning large language models (LLM). The modular and plug-and-play nature of LoRA enables the integration of diverse domain-specific LoRAs to enhance the capabilities of LLMs. Previous research on exploiting multiple LoRAs either focuses on specific isolated downstream tasks or fixes the selection of LoRAs during training. However, in real-world scenarios, LLMs receive diverse prompts covering different tasks, and the pool of candidate LoRAs is often dynamically updated. To bridge this gap, we propose LoraRetriever, a retrieve-then-compose framework that adaptively retrieves and composes multiple LoRAs according to the input prompts. LoraRetriever contains three main components: firstly, identifying and retrieving LoRAs relevant to the given input; secondly, formulating strategies for effectively integrating the retrieved LoRAs; and thirdly, developing effici
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22522;&#20110;&#35282;&#33394;&#29983;&#25104;&#23545;&#35805;&#26041;&#38754;&#36827;&#34892;&#23454;&#39564;&#65292;&#21457;&#29616;&#35843;&#25972;&#25552;&#31034;&#25351;&#20196;&#21487;&#20197;&#26368;&#30452;&#25509;&#26377;&#25928;&#19988;&#32463;&#27982;&#22320;&#25552;&#39640;&#29983;&#25104;&#36136;&#37327;&#65292;&#24182;&#19988;&#38543;&#26426;&#26816;&#32034;&#31034;&#33539;&#20250;&#21462;&#24471;&#26368;&#20339;&#32467;&#26524;&#65292;&#32780;&#26597;&#35810;&#30456;&#21516;&#19978;&#19979;&#25991;&#30340;&#31034;&#33539;&#26816;&#32034;&#25928;&#26524;&#26368;&#24046;&#12290;&#21363;&#20351;&#30772;&#22351;&#20102;&#31034;&#33539;&#20013;&#30340;&#22810;&#22238;&#21512;&#20851;&#32852;&#21644;&#21333;&#22238;&#21512;&#35821;&#20041;&#65292;&#23545;&#35805;&#29983;&#25104;&#20173;&#28982;&#26377;&#25928;&#12290;</title><link>https://arxiv.org/abs/2402.09954</link><description>&lt;p&gt;
&#21046;&#23450;&#33391;&#22909;&#25552;&#31034;&#36824;&#26159;&#25552;&#20379;&#20986;&#33394;&#30340;&#23545;&#35805;&#65311;&#20851;&#20110;&#22522;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#35282;&#33394;&#29983;&#25104;&#23545;&#35805;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Crafting a Good Prompt or Providing Exemplary Dialogues? A Study of In-Context Learning for Persona-based Dialogue Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09954
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22522;&#20110;&#35282;&#33394;&#29983;&#25104;&#23545;&#35805;&#26041;&#38754;&#36827;&#34892;&#23454;&#39564;&#65292;&#21457;&#29616;&#35843;&#25972;&#25552;&#31034;&#25351;&#20196;&#21487;&#20197;&#26368;&#30452;&#25509;&#26377;&#25928;&#19988;&#32463;&#27982;&#22320;&#25552;&#39640;&#29983;&#25104;&#36136;&#37327;&#65292;&#24182;&#19988;&#38543;&#26426;&#26816;&#32034;&#31034;&#33539;&#20250;&#21462;&#24471;&#26368;&#20339;&#32467;&#26524;&#65292;&#32780;&#26597;&#35810;&#30456;&#21516;&#19978;&#19979;&#25991;&#30340;&#31034;&#33539;&#26816;&#32034;&#25928;&#26524;&#26368;&#24046;&#12290;&#21363;&#20351;&#30772;&#22351;&#20102;&#31034;&#33539;&#20013;&#30340;&#22810;&#22238;&#21512;&#20851;&#32852;&#21644;&#21333;&#22238;&#21512;&#35821;&#20041;&#65292;&#23545;&#35805;&#29983;&#25104;&#20173;&#28982;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#20851;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#30340;&#30740;&#31350;&#20027;&#35201;&#20391;&#37325;&#20110;&#20998;&#31867;&#12289;&#26426;&#22120;&#32763;&#35793;&#12289;&#25991;&#26412;&#21040;&#34920;&#26684;&#31561;&#20219;&#21153;&#65292;&#32780;&#23545;&#20110;ICL&#33021;&#21542;&#25913;&#36827;&#29983;&#25104;&#31867;&#20284;&#20154;&#31867;&#23545;&#35805;&#30340;&#30740;&#31350;&#24456;&#23569;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#36890;&#36807;&#22312;&#39640;&#36136;&#37327;&#30340;&#30495;&#23454;&#20154;&#31867;&#23545;&#35805;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22522;&#20110;&#35282;&#33394;&#29983;&#25104;&#23545;&#35805;&#26041;&#38754;&#30340;ICL&#33021;&#21147;&#12290;&#26681;&#25454;&#23454;&#39564;&#32467;&#26524;&#65292;&#25105;&#20204;&#24471;&#20986;&#19977;&#20010;&#32467;&#35770;&#65306;1&#65289;&#35843;&#25972;&#25552;&#31034;&#25351;&#20196;&#26159;&#25552;&#39640;&#29983;&#25104;&#36136;&#37327;&#26368;&#30452;&#25509;&#12289;&#26377;&#25928;&#21644;&#32463;&#27982;&#30340;&#26041;&#27861;&#65307;2&#65289;&#38543;&#26426;&#26816;&#32034;&#31034;&#33539;&#21487;&#20197;&#21462;&#24471;&#26368;&#20339;&#30340;&#32467;&#26524;&#65292;&#21487;&#33021;&#26159;&#22240;&#20026;&#20855;&#26377;&#26356;&#22810;&#26679;&#21270;&#21644;&#26377;&#25928;&#20449;&#24687;&#30340;&#21407;&#22240;&#65307;&#19982;&#26597;&#35810;&#30456;&#21516;&#19978;&#19979;&#25991;&#30340;&#31034;&#33539;&#26816;&#32034;&#32467;&#26524;&#26368;&#24046;&#65307;3&#65289;&#21363;&#20351;&#30772;&#22351;&#20102;&#31034;&#33539;&#20013;&#30340;&#22810;&#22238;&#21512;&#20851;&#32852;&#21644;&#21333;&#22238;&#21512;&#35821;&#20041;&#65292;&#23545;&#35805;&#29983;&#25104;&#20173;&#28982;&#21487;&#20197;&#23454;&#29616;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09954v1 Announce Type: new  Abstract: Previous in-context learning (ICL) research has focused on tasks such as classification, machine translation, text2table, etc., while studies on whether ICL can improve human-like dialogue generation are scarce. Our work fills this gap by systematically investigating the ICL capabilities of large language models (LLMs) in persona-based dialogue generation, conducting extensive experiments on high-quality real human Chinese dialogue datasets. From experimental results, we draw three conclusions: 1) adjusting prompt instructions is the most direct, effective, and economical way to improve generation quality; 2) randomly retrieving demonstrations (demos) achieves the best results, possibly due to the greater diversity and the amount of effective information; counter-intuitively, retrieving demos with a context identical to the query performs the worst; 3) even when we destroy the multi-turn associations and single-turn semantics in the demo
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26469;&#22686;&#24378;&#37329;&#34701;&#34892;&#19994;&#30340;&#32593;&#32476;&#23433;&#20840;&#38887;&#24615;&#65292;&#24182;&#23454;&#29616;&#39640;&#32423;&#23041;&#32961;&#26816;&#27979;&#12290;&#30446;&#21069;&#30340;&#32593;&#32476;&#23041;&#32961;&#26816;&#27979;&#26041;&#27861;&#24448;&#24448;&#22522;&#20110;&#35268;&#21017;&#21644;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#26080;&#27861;&#36866;&#29992;&#22823;&#35268;&#27169;&#25968;&#25454;&#24212;&#29992;&#65292;&#24182;&#19988;&#26080;&#27861;&#26377;&#25928;&#26816;&#27979;&#26410;&#30693;&#23041;&#32961;&#12290;</title><link>https://arxiv.org/abs/2402.09820</link><description>&lt;p&gt;
&#29992;&#28145;&#24230;&#23398;&#20064;&#22686;&#24378;&#37329;&#34701;&#34892;&#19994;&#30340;&#32593;&#32476;&#23433;&#20840;&#38887;&#24615;&#65292;&#23454;&#29616;&#39640;&#32423;&#23041;&#32961;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Enhancing Cybersecurity Resilience in Finance with Deep Learning for Advanced Threat Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09820
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26469;&#22686;&#24378;&#37329;&#34701;&#34892;&#19994;&#30340;&#32593;&#32476;&#23433;&#20840;&#38887;&#24615;&#65292;&#24182;&#23454;&#29616;&#39640;&#32423;&#23041;&#32961;&#26816;&#27979;&#12290;&#30446;&#21069;&#30340;&#32593;&#32476;&#23041;&#32961;&#26816;&#27979;&#26041;&#27861;&#24448;&#24448;&#22522;&#20110;&#35268;&#21017;&#21644;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#26080;&#27861;&#36866;&#29992;&#22823;&#35268;&#27169;&#25968;&#25454;&#24212;&#29992;&#65292;&#24182;&#19988;&#26080;&#27861;&#26377;&#25928;&#26816;&#27979;&#26410;&#30693;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20114;&#32852;&#32593;&#26102;&#20195;&#65292;&#20154;&#20204;&#30340;&#29983;&#27963;&#36234;&#26469;&#36234;&#20381;&#36182;&#20110;&#20170;&#22825;&#30340;&#32593;&#32476;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#32593;&#32476;&#25216;&#26415;&#26159;&#19968;&#25226;&#21452;&#20995;&#21073;&#65292;&#32473;&#20154;&#20204;&#24102;&#26469;&#20415;&#21033;&#30340;&#21516;&#26102;&#20063;&#24102;&#26469;&#20102;&#35768;&#22810;&#23433;&#20840;&#25361;&#25112;&#12290;&#20445;&#25345;&#32593;&#32476;&#23433;&#20840;&#21644;&#20445;&#25252;&#29992;&#25143;&#30340;&#21512;&#27861;&#21033;&#30410;&#26159;&#32593;&#32476;&#24314;&#35774;&#30340;&#26680;&#24515;&#12290;&#23041;&#32961;&#26816;&#27979;&#26159;&#19968;&#20010;&#23436;&#25972;&#26377;&#25928;&#30340;&#38450;&#24481;&#31995;&#32479;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#22312;&#32593;&#32476;&#20449;&#24687;&#23433;&#20840;&#39046;&#22495;&#65292;&#32593;&#32476;&#25915;&#20987;&#21644;&#32593;&#32476;&#38450;&#25252;&#30340;&#25216;&#26415;&#26356;&#26032;&#26085;&#30410;&#36805;&#29467;&#12290;&#22914;&#20309;&#26377;&#25928;&#22320;&#26816;&#27979;&#26410;&#30693;&#23041;&#32961;&#26159;&#32593;&#32476;&#38450;&#25252;&#30340;&#20851;&#27880;&#28966;&#28857;&#20043;&#19968;&#12290;&#30446;&#21069;&#65292;&#32593;&#32476;&#23041;&#32961;&#26816;&#27979;&#36890;&#24120;&#22522;&#20110;&#35268;&#21017;&#21644;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#21019;&#24314;&#20154;&#24037;&#35268;&#21017;&#25110;&#25552;&#21462;&#24120;&#35265;&#30340;&#26102;&#31354;&#29305;&#24449;&#65292;&#19981;&#33021;&#24212;&#29992;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#24212;&#29992;&#65292;&#24182;&#19988;&#26410;&#30693;&#23041;&#32961;&#30340;&#20986;&#29616;&#23548;&#33268;&#20102;&#31995;&#32479;&#30340;&#26816;&#27979;&#20934;&#30830;&#24615;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09820v1 Announce Type: cross  Abstract: In the age of the Internet, people's lives are increasingly dependent on today's network technology. However, network technology is a double-edged sword, bringing convenience to people but also posing many security challenges. Maintaining network security and protecting the legitimate interests of users is at the heart of network construction. Threat detection is an important part of a complete and effective defense system. In the field of network information security, the technical update of network attack and network protection is spiraling. How to effectively detect unknown threats is one of the concerns of network protection. Currently, network threat detection is usually based on rules and traditional machine learning methods, which create artificial rules or extract common spatiotemporal features, which cannot be applied to large-scale data applications, and the emergence of unknown threats causes the detection accuracy of the or
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#32593;&#32476;&#29305;&#24449;&#22312;&#27450;&#35784;&#26816;&#27979;&#20013;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#20351;&#29992;&#20010;&#24615;&#21270;&#30340;PageRank&#31639;&#27861;&#26469;&#25429;&#25417;&#27450;&#35784;&#30340;&#31038;&#20250;&#21160;&#24577;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#38598;&#25104;PPR&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#24182;&#25552;&#20379;&#29420;&#29305;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2402.09495</link><description>&lt;p&gt;
&#20851;&#20110;&#22522;&#20110;&#32593;&#32476;&#29305;&#24449;&#22312;&#27450;&#35784;&#26816;&#27979;&#20013;&#28508;&#21147;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Potential of Network-Based Features for Fraud Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09495
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#32593;&#32476;&#29305;&#24449;&#22312;&#27450;&#35784;&#26816;&#27979;&#20013;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#20351;&#29992;&#20010;&#24615;&#21270;&#30340;PageRank&#31639;&#27861;&#26469;&#25429;&#25417;&#27450;&#35784;&#30340;&#31038;&#20250;&#21160;&#24577;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#38598;&#25104;PPR&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#24182;&#25552;&#20379;&#29420;&#29305;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#20132;&#26131;&#27450;&#35784;&#32473;&#20225;&#19994;&#21644;&#28040;&#36153;&#32773;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#38754;&#20020;&#30528;&#37325;&#22823;&#30340;&#32463;&#27982;&#25439;&#22833;&#12290;&#20256;&#32479;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#31995;&#32479;&#38590;&#20197;&#36319;&#19978;&#27450;&#35784;&#25112;&#26415;&#30340;&#28436;&#21464;&#65292;&#23548;&#33268;&#39640;&#35823;&#25253;&#29575;&#21644;&#28431;&#25253;&#29575;&#12290;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#36890;&#36807;&#21033;&#29992;&#21382;&#21490;&#25968;&#25454;&#35782;&#21035;&#27450;&#35784;&#27169;&#24335;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#25506;&#35752;&#20351;&#29992;&#20010;&#24615;&#21270;&#30340;PageRank&#65288;PPR&#65289;&#31639;&#27861;&#36890;&#36807;&#20998;&#26512;&#37329;&#34701;&#36134;&#25143;&#20043;&#38388;&#30340;&#20851;&#31995;&#26469;&#25429;&#25417;&#27450;&#35784;&#30340;&#31038;&#20250;&#21160;&#24577;&#12290;&#20027;&#35201;&#30446;&#26631;&#26159;&#27604;&#36739;&#20256;&#32479;&#29305;&#24449;&#19982;&#28155;&#21152;PPR&#22312;&#27450;&#35784;&#26816;&#27979;&#27169;&#22411;&#20013;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#38598;&#25104;PPR&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#36229;&#36807;&#22522;&#20934;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;PPR&#29305;&#24449;&#25552;&#20379;&#20102;&#29420;&#29305;&#32780;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#65292;&#36890;&#36807;&#20854;&#39640;&#29305;&#24449;&#37325;&#35201;&#24615;&#24471;&#20998;&#24471;&#20197;&#35777;&#26126;&#12290;&#29305;&#24449;&#31283;&#23450;&#24615;&#20998;&#26512;&#35777;&#23454;&#20102;&#19968;&#33268;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09495v1 Announce Type: cross  Abstract: Online transaction fraud presents substantial challenges to businesses and consumers, risking significant financial losses. Conventional rule-based systems struggle to keep pace with evolving fraud tactics, leading to high false positive rates and missed detections. Machine learning techniques offer a promising solution by leveraging historical data to identify fraudulent patterns. This article explores using the personalised PageRank (PPR) algorithm to capture the social dynamics of fraud by analysing relationships between financial accounts. The primary objective is to compare the performance of traditional features with the addition of PPR in fraud detection models. Results indicate that integrating PPR enhances the model's predictive power, surpassing the baseline model. Additionally, the PPR feature provides unique and valuable information, evidenced by its high feature importance score. Feature stability analysis confirms consist
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#23545;&#27604;&#20102;&#20154;&#31867;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#34892;&#20026;&#24046;&#24322;&#65292;&#21457;&#29616;&#20154;&#31867;&#20855;&#26377;&#21363;&#26102;&#27010;&#25324;&#33021;&#21147;&#65292;&#32780;DNNs&#23384;&#22312;&#28382;&#21518;&#27010;&#25324;&#29616;&#35937;&#65292;&#36825;&#34920;&#26126;&#20102;&#34920;&#31034;&#20998;&#27495;&#30340;&#23384;&#22312;&#12290;</title><link>https://arxiv.org/abs/2402.09303</link><description>&lt;p&gt;
&#20154;&#31867;&#20013;&#30340;&#21363;&#26102;&#27010;&#25324;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#28382;&#21518;&#27010;&#25324;&#8212;&#8212;&#34920;&#31034;&#20998;&#27495;&#30340;&#35777;&#25454;&#65311;
&lt;/p&gt;
&lt;p&gt;
Immediate generalisation in humans but a generalisation lag in deep neural networks$\unicode{x2014}$evidence for representational divergence?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09303
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#23545;&#27604;&#20102;&#20154;&#31867;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#34892;&#20026;&#24046;&#24322;&#65292;&#21457;&#29616;&#20154;&#31867;&#20855;&#26377;&#21363;&#26102;&#27010;&#25324;&#33021;&#21147;&#65292;&#32780;DNNs&#23384;&#22312;&#28382;&#21518;&#27010;&#25324;&#29616;&#35937;&#65292;&#36825;&#34920;&#26126;&#20102;&#34920;&#31034;&#20998;&#27495;&#30340;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#30740;&#31350;&#22312;&#22270;&#20687;&#20998;&#31867;&#39046;&#22495;&#20013;&#23545;&#27604;&#20102;&#20154;&#31867;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30340;&#35768;&#22810;&#34892;&#20026;&#27604;&#36739;&#12290;&#36890;&#24120;&#65292;&#27604;&#36739;&#30740;&#31350;&#20851;&#27880;&#30340;&#26159;&#23398;&#20064;&#36807;&#31243;&#30340;&#26368;&#32456;&#32467;&#26524;&#65292;&#36890;&#36807;&#27979;&#37327;&#21644;&#27604;&#36739;&#30446;&#26631;&#31867;&#21035;&#34920;&#31034;&#30340;&#30456;&#20284;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#34920;&#31034;&#22914;&#20309;&#24418;&#25104;&#21363;&#20854;&#36807;&#31243;&#8212;&#8212;&#21363;&#22312;&#33719;&#21462;&#36807;&#31243;&#20013;&#35266;&#23519;&#21040;&#30340;&#34892;&#20026;&#21464;&#21270;&#21644;&#20013;&#38388;&#38454;&#27573;&#8212;&#8212;&#24448;&#24448;&#23569;&#26377;&#30452;&#25509;&#21644;&#23454;&#35777;&#30340;&#27604;&#36739;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25253;&#21578;&#20102;&#23545;&#20154;&#31867;&#35266;&#23519;&#32773;&#21644;&#19981;&#21516;&#32463;&#20856;&#19982;&#26368;&#26032;&#25216;&#26415;&#30340;DNNs&#20013;&#21487;&#36716;&#31227;&#34920;&#31034;&#26159;&#22914;&#20309;&#34987;&#33719;&#21462;&#30340;&#30340;&#35814;&#32454;&#35843;&#26597;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21463;&#38480;&#30340;&#30417;&#30563;&#23398;&#20064;&#29615;&#22659;&#65292;&#35813;&#29615;&#22659;&#20013;&#25105;&#20204;&#23545;&#40784;&#20102;&#23398;&#20064;&#30456;&#20851;&#30340;&#21442;&#25968;&#65292;&#22914;&#36215;&#22987;&#28857;&#12289;&#36755;&#20837;&#27169;&#24335;&#12289;&#21487;&#29992;&#36755;&#20837;&#25968;&#25454;&#20197;&#21450;&#25552;&#20379;&#30340;&#21453;&#39304;&#12290;&#22312;&#25972;&#20010;&#23398;&#20064;&#36807;&#31243;&#20013;&#25105;&#20204;&#35780;&#20272;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09303v1 Announce Type: cross Abstract: Recent research has seen many behavioral comparisons between humans and deep neural networks (DNNs) in the domain of image classification. Often, comparison studies focus on the end-result of the learning process by measuring and comparing the similarities in the representations of object categories once they have been formed. However, the process of how these representations emerge$\unicode{x2014}$that is, the behavioral changes and intermediate stages observed during the acquisition$\unicode{x2014}$is less often directly and empirically compared.   Here we report a detailed investigation of how transferable representations are acquired in human observers and various classic and state-of-the-art DNNs. We develop a constrained supervised learning environment in which we align learning-relevant parameters such as starting point, input modality, available input data and the feedback provided. Across the whole learning process we evaluate 
&lt;/p&gt;</description></item><item><title>UR2M&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#36164;&#28304;&#24863;&#30693;&#30340;&#20107;&#20214;&#26816;&#27979;&#26694;&#26550;&#65292;&#38024;&#23545;&#24494;&#25511;&#21046;&#22120;&#19978;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#35780;&#20272;&#27169;&#22411;&#36755;&#20986;&#30340;&#21487;&#38752;&#24615;&#26469;&#35299;&#20915;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#26102;&#20135;&#29983;&#19981;&#20934;&#30830;&#39044;&#27979;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.09264</link><description>&lt;p&gt;
UR2M: &#24494;&#25511;&#21046;&#22120;&#19978;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#36164;&#28304;&#24863;&#30693;&#20107;&#20214;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
UR2M: Uncertainty and Resource-Aware Event Detection on Microcontrollers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09264
&lt;/p&gt;
&lt;p&gt;
UR2M&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#36164;&#28304;&#24863;&#30693;&#30340;&#20107;&#20214;&#26816;&#27979;&#26694;&#26550;&#65292;&#38024;&#23545;&#24494;&#25511;&#21046;&#22120;&#19978;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#35780;&#20272;&#27169;&#22411;&#36755;&#20986;&#30340;&#21487;&#38752;&#24615;&#26469;&#35299;&#20915;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#26102;&#20135;&#29983;&#19981;&#20934;&#30830;&#39044;&#27979;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#38454;&#27573;&#30340;&#25968;&#25454;&#20998;&#24067;&#21457;&#29983;&#21464;&#21270;&#26102;&#23481;&#26131;&#20135;&#29983;&#19981;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;&#36825;&#31181;&#33030;&#24369;&#24615;&#21487;&#33021;&#23548;&#33268;&#20005;&#37325;&#21518;&#26524;&#65292;&#29305;&#21035;&#26159;&#22312;&#31227;&#21160;&#21307;&#30103;&#31561;&#24212;&#29992;&#20013;&#12290;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26377;&#28508;&#21147;&#36890;&#36807;&#35780;&#20272;&#27169;&#22411;&#36755;&#20986;&#30340;&#21487;&#38752;&#24615;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#25216;&#26415;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#21644;&#20869;&#23384;&#65292;&#20351;&#23427;&#20204;&#22312;&#24494;&#25511;&#21046;&#22120;&#65288;MCU&#65289;&#19978;&#30340;&#23454;&#26045;&#21464;&#24471;&#19981;&#20999;&#23454;&#38469;&#12290;&#36825;&#20010;&#38480;&#21046;&#38459;&#30861;&#20102;&#35768;&#22810;&#37325;&#35201;&#30340;&#35774;&#22791;&#19978;&#21487;&#31359;&#25140;&#20107;&#20214;&#26816;&#27979;&#65288;WED&#65289;&#24212;&#29992;&#30340;&#21487;&#34892;&#24615;&#65292;&#22914;&#24515;&#33039;&#30149;&#21457;&#20316;&#26816;&#27979;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;UR2M&#65292;&#19968;&#20010;&#38024;&#23545;MCU&#30340;&#26032;&#39062;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#36164;&#28304;&#24863;&#30693;&#20107;&#20214;&#26816;&#27979;&#26694;&#26550;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#65288;i&#65289;&#22522;&#20110;&#35777;&#25454;&#29702;&#35770;&#24320;&#21457;&#20102;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;WED&#65292;&#29992;&#20110;&#20934;&#30830;&#30340;&#20107;&#20214;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09264v1 Announce Type: new Abstract: Traditional machine learning techniques are prone to generating inaccurate predictions when confronted with shifts in the distribution of data between the training and testing phases. This vulnerability can lead to severe consequences, especially in applications such as mobile healthcare. Uncertainty estimation has the potential to mitigate this issue by assessing the reliability of a model's output. However, existing uncertainty estimation techniques often require substantial computational resources and memory, making them impractical for implementation on microcontrollers (MCUs). This limitation hinders the feasibility of many important on-device wearable event detection (WED) applications, such as heart attack detection.   In this paper, we present UR2M, a novel Uncertainty and Resource-aware event detection framework for MCUs. Specifically, we (i) develop an uncertainty-aware WED based on evidential theory for accurate event detection
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20351;&#29992;Transformer&#26550;&#26500;&#36827;&#34892;&#20154;&#20307;&#23039;&#24577;&#20272;&#35745;&#30340;&#26368;&#20339;IMU&#25918;&#32622;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#23039;&#24577;&#37325;&#24314;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#30340;&#21452;&#21521;RNN&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;&#20351;&#29992;&#21482;&#26377;6&#20010;IMU&#26102;&#65292;Transformer&#26550;&#26500;&#23558;24&#20010;IMU&#20301;&#32622;&#33719;&#21462;&#30340;&#25968;&#25454;&#19982;&#21452;&#21521;RNN&#20855;&#26377;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#36825;&#19968;&#20248;&#21270;&#36873;&#25321;&#30340;IMU&#25918;&#32622;&#31574;&#30053;&#32467;&#21512;&#20102;Transformer&#30340;&#24182;&#34892;&#24615;&#21644;&#24615;&#33021;&#20248;&#21183;&#65292;&#23545;&#22522;&#20110;IMU&#30340;&#23039;&#24577;&#20272;&#35745;&#39046;&#22495;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.08923</link><description>&lt;p&gt;
IMUOptimize: &#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20351;&#29992;Transformer&#26550;&#26500;&#36827;&#34892;&#20154;&#20307;&#23039;&#24577;&#20272;&#35745;&#30340;&#26368;&#20339;IMU&#25918;&#32622;
&lt;/p&gt;
&lt;p&gt;
IMUOptimize: A Data-Driven Approach to Optimal IMU Placement for Human Pose Estimation with Transformer Architecture
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08923
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20351;&#29992;Transformer&#26550;&#26500;&#36827;&#34892;&#20154;&#20307;&#23039;&#24577;&#20272;&#35745;&#30340;&#26368;&#20339;IMU&#25918;&#32622;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#23039;&#24577;&#37325;&#24314;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#30340;&#21452;&#21521;RNN&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;&#20351;&#29992;&#21482;&#26377;6&#20010;IMU&#26102;&#65292;Transformer&#26550;&#26500;&#23558;24&#20010;IMU&#20301;&#32622;&#33719;&#21462;&#30340;&#25968;&#25454;&#19982;&#21452;&#21521;RNN&#20855;&#26377;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#36825;&#19968;&#20248;&#21270;&#36873;&#25321;&#30340;IMU&#25918;&#32622;&#31574;&#30053;&#32467;&#21512;&#20102;Transformer&#30340;&#24182;&#34892;&#24615;&#21644;&#24615;&#33021;&#20248;&#21183;&#65292;&#23545;&#22522;&#20110;IMU&#30340;&#23039;&#24577;&#20272;&#35745;&#39046;&#22495;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;IMU&#25968;&#25454;&#39044;&#27979;&#20154;&#20307;&#23039;&#24577;&#30340;&#26032;&#26041;&#27861;&#65292;&#19981;&#21516;&#20110;&#20043;&#21069;&#30340;&#30740;&#31350;DIP-IMU&#12289;IMUPoser&#21644;TransPose&#65292;&#23427;&#20204;&#20351;&#29992;&#26368;&#22810;6&#20010;IMU&#19982;&#21452;&#21521;RNN&#32467;&#21512;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#20027;&#35201;&#21019;&#26032;&#65306;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26368;&#20339;IMU&#25918;&#32622;&#31574;&#30053;&#21644;&#22522;&#20110;Transformer&#30340;&#26102;&#24207;&#20998;&#26512;&#27169;&#22411;&#26550;&#26500;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#22312;&#20256;&#32479;&#30340;&#20197;6&#20010;IMU&#20026;&#22522;&#30784;&#30340;&#21452;&#21521;RNN&#27169;&#22411;&#19978;&#34920;&#29616;&#20248;&#36234;&#65292;&#32780;&#19988;&#20351;&#29992;&#21482;&#26377;6&#20010;IMU&#26102;&#65292;&#19982;&#21452;&#21521;RNN&#30456;&#27604;&#65292;Transformer&#26550;&#26500;&#26174;&#33879;&#25552;&#39640;&#20102;&#20174;24&#20010;IMU&#20301;&#32622;&#33719;&#21462;&#30340;&#25968;&#25454;&#30340;&#23039;&#24577;&#37325;&#24314;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#20248;&#21270;&#36873;&#25321;&#30340;&#20301;&#32622;&#25552;&#20379;&#20102;&#26174;&#33879;&#30340;IMU&#23039;&#24577;&#20272;&#35745;&#39046;&#22495;&#30340;&#25913;&#36827;&#65292;&#32467;&#21512;Transformer&#30340;&#24182;&#34892;&#24615;&#21644;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08923v1 Announce Type: new Abstract: This paper presents a novel approach for predicting human poses using IMU data, diverging from previous studies such as DIP-IMU, IMUPoser, and TransPose, which use up to 6 IMUs in conjunction with bidirectional RNNs. We introduce two main innovations: a data-driven strategy for optimal IMU placement and a transformer-based model architecture for time series analysis. Our findings indicate that our approach not only outperforms traditional 6 IMU-based biRNN models but also that the transformer architecture significantly enhances pose reconstruction from data obtained from 24 IMU locations, with equivalent performance to biRNNs when using only 6 IMUs. The enhanced accuracy provided by our optimally chosen locations, when coupled with the parallelizability and performance of transformers, provides significant improvements to the field of IMU-based pose estimation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#20010;&#26234;&#33021;&#20307;&#22312;&#20849;&#21516;&#29615;&#22659;&#20013;&#30340;&#20132;&#20114;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#26041;&#27861;&#26469;&#25512;&#26029;&#28216;&#25103;&#21442;&#25968;&#12290;&#37319;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#26469;&#26500;&#24314;&#28216;&#25103;&#21442;&#25968;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#26080;&#27861;&#23545;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#23450;&#37327;&#21270;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.08902</link><description>&lt;p&gt;
&#33258;&#21160;&#32534;&#30721;&#36125;&#21494;&#26031;&#21453;&#21521;&#28216;&#25103;
&lt;/p&gt;
&lt;p&gt;
Auto-Encoding Bayesian Inverse Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08902
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#20010;&#26234;&#33021;&#20307;&#22312;&#20849;&#21516;&#29615;&#22659;&#20013;&#30340;&#20132;&#20114;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#26041;&#27861;&#26469;&#25512;&#26029;&#28216;&#25103;&#21442;&#25968;&#12290;&#37319;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#26469;&#26500;&#24314;&#28216;&#25103;&#21442;&#25968;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#26080;&#27861;&#23545;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#23450;&#37327;&#21270;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#22810;&#20010;&#26234;&#33021;&#20307;&#22312;&#20849;&#21516;&#29615;&#22659;&#20013;&#20114;&#21160;&#26102;&#65292;&#27599;&#20010;&#26234;&#33021;&#20307;&#30340;&#34892;&#21160;&#20250;&#24433;&#21709;&#20854;&#20182;&#26234;&#33021;&#20307;&#26410;&#26469;&#30340;&#20915;&#31574;&#65292;&#32780;&#38750;&#21512;&#20316;&#21160;&#24577;&#28216;&#25103;&#33258;&#28982;&#22320;&#25429;&#25417;&#21040;&#20102;&#36825;&#31181;&#32806;&#21512;&#12290;&#28982;&#32780;&#65292;&#22312;&#20132;&#20114;&#24335;&#36816;&#21160;&#35268;&#21010;&#20013;&#65292;&#26234;&#33021;&#20307;&#36890;&#24120;&#27809;&#26377;&#23436;&#25972;&#30340;&#28216;&#25103;&#27169;&#22411;&#65292;&#20363;&#22914;&#30001;&#20110;&#20854;&#20182;&#29609;&#23478;&#30340;&#30446;&#26631;&#26159;&#26410;&#30693;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#36870;&#21521;&#28216;&#25103;&#38382;&#39064;&#65292;&#20854;&#20013;&#28216;&#25103;&#30340;&#26576;&#20123;&#23646;&#24615;&#26159;&#20808;&#39564;&#26410;&#30693;&#30340;&#65292;&#24517;&#39035;&#26681;&#25454;&#35266;&#27979;&#32467;&#26524;&#36827;&#34892;&#25512;&#26029;&#12290;&#29616;&#26377;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65288;MLE&#65289;&#26041;&#27861;&#35299;&#20915;&#36870;&#21521;&#28216;&#25103;&#38382;&#39064;&#26102;&#20165;&#25552;&#20379;&#26410;&#30693;&#21442;&#25968;&#30340;&#28857;&#20272;&#35745;&#32780;&#19981;&#23545;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#23450;&#37327;&#21270;&#65292;&#24182;&#19988;&#22312;&#35768;&#22810;&#21442;&#25968;&#20540;&#33021;&#35299;&#37322;&#35266;&#27979;&#34892;&#20026;&#26102;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#37319;&#29992;&#36125;&#21494;&#26031;&#35266;&#28857;&#26500;&#24314;&#20102;&#28216;&#25103;&#21442;&#25968;&#30340;&#21518;&#39564;&#20998;&#24067;&#12290;&#20026;&#20102;&#20351;&#25512;&#26029;&#21487;&#34892;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#20855;&#26377;&#20869;&#23884;&#21487;&#24494;&#20998;&#28216;&#25103;&#30340;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08902v1 Announce Type: cross Abstract: When multiple agents interact in a common environment, each agent's actions impact others' future decisions, and noncooperative dynamic games naturally capture this coupling. In interactive motion planning, however, agents typically do not have access to a complete model of the game, e.g., due to unknown objectives of other players. Therefore, we consider the inverse game problem, in which some properties of the game are unknown a priori and must be inferred from observations. Existing maximum likelihood estimation (MLE) approaches to solve inverse games provide only point estimates of unknown parameters without quantifying uncertainty, and perform poorly when many parameter values explain the observed behavior. To address these limitations, we take a Bayesian perspective and construct posterior distributions of game parameters. To render inference tractable, we employ a variational autoencoder (VAE) with an embedded differentiable game
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36873;&#25321;&#24615;SSMs&#30340;&#26032;&#31867;GNNs&#26694;&#26550;&#8212;&#8212;&#22270;&#39532;&#24052;&#32593;&#32476;&#65288;GMNs&#65289;&#65292;&#36890;&#36807;&#19981;&#20381;&#36182;&#20110;Transformer&#12289;&#22797;&#26434;&#30340;&#28040;&#24687;&#20256;&#36882;&#21644;&#20301;&#32622;/&#32467;&#26500;&#32534;&#30721;&#65288;SE/PE&#65289;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;GNNs&#30340;&#36807;&#24230;&#21387;&#32553;&#21644;&#26080;&#27861;&#25429;&#25417;&#38271;&#31243;&#20381;&#36182;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.08678</link><description>&lt;p&gt;
&#22270;&#39532;&#24052;&#65306;&#38754;&#21521;&#22522;&#20110;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#22270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Graph Mamba: Towards Learning on Graphs with State Space Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08678
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36873;&#25321;&#24615;SSMs&#30340;&#26032;&#31867;GNNs&#26694;&#26550;&#8212;&#8212;&#22270;&#39532;&#24052;&#32593;&#32476;&#65288;GMNs&#65289;&#65292;&#36890;&#36807;&#19981;&#20381;&#36182;&#20110;Transformer&#12289;&#22797;&#26434;&#30340;&#28040;&#24687;&#20256;&#36882;&#21644;&#20301;&#32622;/&#32467;&#26500;&#32534;&#30721;&#65288;SE/PE&#65289;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;GNNs&#30340;&#36807;&#24230;&#21387;&#32553;&#21644;&#26080;&#27861;&#25429;&#25417;&#38271;&#31243;&#20381;&#36182;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#22270;&#34920;&#31034;&#23398;&#20064;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;&#22823;&#22810;&#25968;GNNs&#23450;&#20041;&#20102;&#19968;&#31181;&#23616;&#37096;&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#65292;&#36890;&#36807;&#22534;&#21472;&#22810;&#20010;&#23618;&#22312;&#22270;&#19978;&#20256;&#25773;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24050;&#30693;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#38480;&#21046;&#65306;&#36807;&#24230;&#21387;&#32553;&#21644;&#26080;&#27861;&#25429;&#25417;&#38271;&#31243;&#20381;&#36182;&#12290;&#26368;&#36817;&#65292;&#22270;&#36716;&#25442;&#22120;&#65288;GTs&#65289;&#20316;&#20026;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNNs&#65289;&#30340;&#19968;&#31181;&#24378;&#22823;&#26367;&#20195;&#26041;&#27861;&#20986;&#29616;&#12290;&#28982;&#32780;&#65292;GTs&#20855;&#26377;&#20108;&#27425;&#35745;&#31639;&#25104;&#26412;&#65292;&#22312;&#22270;&#32467;&#26500;&#19978;&#32570;&#20047;&#24402;&#32435;&#20559;&#24046;&#65292;&#24182;&#19988;&#20381;&#36182;&#22797;&#26434;&#30340;&#20301;&#32622;/&#32467;&#26500;&#32534;&#30721;&#65288;SE/PE&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#23454;&#36341;&#20013;&#65292;&#23613;&#31649;Transformer&#12289;&#22797;&#26434;&#30340;&#28040;&#24687;&#20256;&#36882;&#21644;SE/PE&#23545;&#20110;&#33391;&#22909;&#24615;&#33021;&#32780;&#35328;&#26159;&#36275;&#22815;&#30340;&#65292;&#20294;&#24182;&#38750;&#24517;&#38656;&#12290;&#21463;&#21040;&#26368;&#36817;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSMs&#65289;&#65288;&#20363;&#22914;Mamba&#65289;&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22270;&#39532;&#24052;&#32593;&#32476;&#65288;GMNs&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#36873;&#25321;&#24615;SSMs&#30340;&#26032;&#31867;GNNs&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;&#25105;&#20204;&#35752;&#35770;&#24182;&#23545;&#26032;&#30340;c&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have shown promising potential in graph representation learning. The majority of GNNs define a local message-passing mechanism, propagating information over the graph by stacking multiple layers. These methods, however, are known to suffer from two major limitations: over-squashing and poor capturing of long-range dependencies. Recently, Graph Transformers (GTs) emerged as a powerful alternative to Message-Passing Neural Networks (MPNNs). GTs, however, have quadratic computational cost, lack inductive biases on graph structures, and rely on complex Positional/Structural Encodings (SE/PE). In this paper, we show that while Transformers, complex message-passing, and SE/PE are sufficient for good performance in practice, neither is necessary. Motivated by the recent success of State Space Models (SSMs), such as Mamba, we present Graph Mamba Networks (GMNs), a general framework for a new class of GNNs based on selective SSMs. We discuss and categorize the new c
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#23558;&#30456;&#20301;&#35266;&#23519;&#12289;&#31616;&#21333;&#30340;&#30456;&#20301;&#22870;&#21169;&#21644;&#23616;&#37096;&#21453;&#39304;&#21160;&#21147;&#23398;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20998;&#25955;&#30456;&#20301;&#25391;&#33633;&#22120;&#23398;&#20064;&#21160;&#29289;&#27493;&#24577;&#30340;&#27169;&#22411;&#65292;&#26377;&#25928;&#22320;&#23454;&#29616;&#20102;&#26032;&#29983;&#27493;&#24577;&#20559;&#22909;&#30340;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.08662</link><description>&lt;p&gt;
&#20351;&#29992;&#20998;&#25955;&#30340;&#30456;&#20301;&#25391;&#33633;&#22120;&#23398;&#20064;&#26032;&#29983;&#27493;&#24577;&#65306;&#20851;&#20110;&#35266;&#23519;&#12289;&#22870;&#21169;&#21644;&#21453;&#39304;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Learning Emergent Gaits with Decentralized Phase Oscillators: on the role of Observations, Rewards, and Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08662
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#23558;&#30456;&#20301;&#35266;&#23519;&#12289;&#31616;&#21333;&#30340;&#30456;&#20301;&#22870;&#21169;&#21644;&#23616;&#37096;&#21453;&#39304;&#21160;&#21147;&#23398;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20998;&#25955;&#30456;&#20301;&#25391;&#33633;&#22120;&#23398;&#20064;&#21160;&#29289;&#27493;&#24577;&#30340;&#27169;&#22411;&#65292;&#26377;&#25928;&#22320;&#23454;&#29616;&#20102;&#26032;&#29983;&#27493;&#24577;&#20559;&#22909;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26368;&#23567;&#21270;&#30456;&#20301;&#25391;&#33633;&#22120;&#27169;&#22411;&#26469;&#23398;&#20064;&#22235;&#36275;&#21160;&#29289;&#30340;&#36816;&#21160;&#12290;&#27599;&#20010;&#25391;&#33633;&#22120;&#20165;&#19982;&#33258;&#36523;&#21644;&#30456;&#24212;&#30340;&#33151;&#20043;&#38388;&#36890;&#36807;&#22320;&#38754;&#21453;&#20316;&#29992;&#21147;&#30340;&#23616;&#37096;&#21453;&#39304;&#30456;&#36830;&#65292;&#36825;&#21487;&#20197;&#34987;&#35299;&#37322;&#20026;&#19968;&#20010;&#35266;&#23519;&#32773;&#21453;&#39304;&#22686;&#30410;&#12290;&#25105;&#20204;&#23558;&#25391;&#33633;&#22120;&#26412;&#36523;&#35299;&#37322;&#20026;&#28508;&#22312;&#30340;&#25509;&#35302;&#29366;&#24577;&#20272;&#35745;&#22120;&#12290;&#36890;&#36807;&#31995;&#32479;&#30340;&#28040;&#38500;&#30740;&#31350;&#65292;&#25105;&#20204;&#34920;&#26126;&#30456;&#20301;&#35266;&#23519;&#12289;&#31616;&#21333;&#30340;&#22522;&#20110;&#30456;&#20301;&#30340;&#22870;&#21169;&#21644;&#23616;&#37096;&#21453;&#39304;&#21160;&#21147;&#23398;&#30340;&#32467;&#21512;&#24341;&#23548;&#20986;&#23637;&#29616;&#26032;&#29983;&#27493;&#24577;&#20559;&#22909;&#30340;&#31574;&#30053;&#65292;&#21516;&#26102;&#20351;&#29992;&#20102;&#19968;&#32452;&#31616;&#21270;&#30340;&#22870;&#21169;&#65292;&#32780;&#27809;&#26377;&#39044;&#20808;&#25351;&#23450;&#29305;&#23450;&#30340;&#27493;&#24577;&#12290;&#20195;&#30721;&#26159;&#24320;&#28304;&#30340;&#65292;&#35270;&#39057;&#25688;&#35201;&#21487;&#22312;https://youtu.be/1NKQ0rSV3jU&#22788;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a minimal phase oscillator model for learning quadrupedal locomotion. Each of the four oscillators is coupled only to itself and its corresponding leg through local feedback of the ground reaction force, which can be interpreted as an observer feedback gain. We interpret the oscillator itself as a latent contact state-estimator. Through a systematic ablation study, we show that the combination of phase observations, simple phase-based rewards, and the local feedback dynamics induces policies that exhibit emergent gait preferences, while using a reduced set of simple rewards, and without prescribing a specific gait. The code is open-source, and a video synopsis available at https://youtu.be/1NKQ0rSV3jU.
&lt;/p&gt;</description></item><item><title>SAGMAN&#26159;&#19968;&#31181;&#29992;&#20110;&#26816;&#39564;&#22270;&#31070;&#32463;&#32593;&#32476;&#31283;&#23450;&#24615;&#30340;&#35889;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#35780;&#20272;&#38750;&#32447;&#24615;&#26144;&#23556;&#20013;&#30340;&#36317;&#31163;&#22833;&#30495;&#26469;&#34913;&#37327;GNN&#30340;&#31283;&#23450;&#24615;&#12290;&#20026;&#20102;&#36827;&#34892;&#26377;&#24847;&#20041;&#30340;&#31283;&#23450;&#24615;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36317;&#31163;&#20445;&#25345;&#30340;&#22270;&#38477;&#32500;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.08653</link><description>&lt;p&gt;
SAGMAN: &#29992;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#27969;&#24418;&#19978;&#30340;&#31283;&#23450;&#24615;&#20998;&#26512;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SAGMAN: Stability Analysis of Graph Neural Networks on the Manifolds
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08653
&lt;/p&gt;
&lt;p&gt;
SAGMAN&#26159;&#19968;&#31181;&#29992;&#20110;&#26816;&#39564;&#22270;&#31070;&#32463;&#32593;&#32476;&#31283;&#23450;&#24615;&#30340;&#35889;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#35780;&#20272;&#38750;&#32447;&#24615;&#26144;&#23556;&#20013;&#30340;&#36317;&#31163;&#22833;&#30495;&#26469;&#34913;&#37327;GNN&#30340;&#31283;&#23450;&#24615;&#12290;&#20026;&#20102;&#36827;&#34892;&#26377;&#24847;&#20041;&#30340;&#31283;&#23450;&#24615;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36317;&#31163;&#20445;&#25345;&#30340;&#22270;&#38477;&#32500;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#23545;&#36755;&#20837;&#22270;&#32467;&#26500;&#21644;&#33410;&#28857;&#29305;&#24449;&#30340;&#21464;&#21270;&#25935;&#24863;&#65292;&#21487;&#33021;&#23548;&#33268;&#19981;&#21487;&#39044;&#27979;&#30340;&#34892;&#20026;&#21644;&#24615;&#33021;&#19979;&#38477;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;SAGMAN&#30340;&#35889;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#39564;GNN&#30340;&#31283;&#23450;&#24615;&#12290;&#35813;&#26694;&#26550;&#35780;&#20272;&#38750;&#32447;&#24615;&#26144;&#23556;&#20013;GNN&#22312;&#36755;&#20837;&#21644;&#36755;&#20986;&#27969;&#24418;&#20043;&#38388;&#24341;&#36215;&#30340;&#36317;&#31163;&#22833;&#30495;: &#24403;&#36755;&#20837;&#27969;&#34892;&#20013;&#20004;&#20010;&#38468;&#36817;&#30340;&#33410;&#28857;&#65288;&#36890;&#36807;GNN&#27169;&#22411;&#65289;&#34987;&#26144;&#23556;&#21040;&#36755;&#20986;&#27969;&#34892;&#19978;&#30340;&#20004;&#20010;&#36828;&#31163;&#30340;&#33410;&#28857;&#26102;&#65292;&#24847;&#21619;&#30528;&#23384;&#22312;&#36739;&#22823;&#30340;&#36317;&#31163;&#22833;&#30495;&#65292;&#20174;&#32780;&#23548;&#33268;GNN&#30340;&#31283;&#23450;&#24615;&#36739;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36317;&#31163;&#20445;&#25345;&#30340;&#22270;&#38477;&#32500;&#65288;GDR&#65289;&#26041;&#27861;&#65292;&#21033;&#29992;&#35889;&#22270;&#23884;&#20837;&#21644;&#27010;&#29575;&#22270;&#27169;&#22411;&#65288;PGMs&#65289;&#26469;&#21019;&#24314;&#20302;&#32500;&#30340;&#36755;&#20837;/&#36755;&#20986;&#22522;&#20110;&#22270;&#30340;&#27969;&#24418;&#65292;&#20197;&#36827;&#34892;&#26377;&#24847;&#20041;&#30340;&#31283;&#23450;&#24615;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;SAGMAN&#33021;&#22815;&#26377;&#25928;&#35780;&#20272;&#27599;&#20010;&#33410;&#28857;&#22312;&#38754;&#23545;&#19981;&#21516;&#36793;&#32536;&#25110;&#29305;&#24449;&#25200;&#21160;&#26102;&#30340;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern graph neural networks (GNNs) can be sensitive to changes in the input graph structure and node features, potentially resulting in unpredictable behavior and degraded performance. In this work, we introduce a spectral framework known as SAGMAN for examining the stability of GNNs. This framework assesses the distance distortions that arise from the nonlinear mappings of GNNs between the input and output manifolds: when two nearby nodes on the input manifold are mapped (through a GNN model) to two distant ones on the output manifold, it implies a large distance distortion and thus a poor GNN stability. We propose a distance-preserving graph dimension reduction (GDR) approach that utilizes spectral graph embedding and probabilistic graphical models (PGMs) to create low-dimensional input/output graph-based manifolds for meaningful stability analysis. Our empirical evaluations show that SAGMAN effectively assesses the stability of each node when subjected to various edge or feature pe
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;&#40657;&#30418;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#30693;&#35782;&#32534;&#36753;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#22810;&#35282;&#24230;&#35780;&#20272;&#26694;&#26550;&#21644;&#19968;&#31181;&#26032;&#30340;postEdit&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#38544;&#31169;&#21644;&#39118;&#26684;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.08631</link><description>&lt;p&gt;
&#22312;&#40657;&#30418;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#30693;&#35782;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Knowledge Editing on Black-box Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08631
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;&#40657;&#30418;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#30693;&#35782;&#32534;&#36753;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#22810;&#35282;&#24230;&#35780;&#20272;&#26694;&#26550;&#21644;&#19968;&#31181;&#26032;&#30340;postEdit&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#38544;&#31169;&#21644;&#39118;&#26684;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#32534;&#36753;&#26088;&#22312;&#39640;&#25928;&#12289;&#31934;&#30830;&#22320;&#20462;&#25913;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#20197;&#26356;&#26032;&#29305;&#23450;&#30340;&#30693;&#35782;&#65292;&#32780;&#19981;&#23545;&#20854;&#20182;&#30693;&#35782;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#24403;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#30333;&#30418;&#35821;&#35328;&#27169;&#22411;&#32534;&#36753;&#19978;&#65292;&#24573;&#35270;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#22330;&#26223;&#65306;&#40657;&#30418;&#35821;&#35328;&#27169;&#22411;&#32534;&#36753;&#65292;&#21363;&#36890;&#36807;&#25509;&#21475;&#35775;&#38382;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#20165;&#21487;&#29992;&#25991;&#26412;&#36755;&#20986;&#12290;&#20026;&#20102;&#35299;&#20915;&#29616;&#26377;&#35780;&#20272;&#22312;&#40657;&#30418;&#35821;&#35328;&#27169;&#22411;&#32534;&#36753;&#19978;&#19981;&#36866;&#29992;&#19988;&#32570;&#20047;&#20840;&#38754;&#24615;&#30340;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35282;&#24230;&#35780;&#20272;&#26694;&#26550;&#65292;&#39318;&#27425;&#23558;&#39118;&#26684;&#20445;&#30041;&#30340;&#35780;&#20272;&#32435;&#20837;&#20854;&#20013;&#12290;&#20026;&#20102;&#35299;&#20915;&#24403;&#21069;&#26041;&#27861;&#20013;&#30340;&#32534;&#36753;&#25968;&#25454;&#38544;&#31169;&#27844;&#28431;&#21644;&#39118;&#26684;&#36807;&#24230;&#32534;&#36753;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;postEdit&#26694;&#26550;&#65292;&#36890;&#36807;&#19979;&#28216;&#21518;&#22788;&#29702;&#35299;&#20915;&#38544;&#31169;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23545;&#21407;&#22987;&#22238;&#31572;&#36827;&#34892;&#32454;&#31890;&#24230;&#32534;&#36753;&#26469;&#20445;&#25345;&#25991;&#26412;&#39118;&#26684;&#19968;&#33268;&#24615;&#12290;&#22312;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#19982;&#20998;&#26512;&#34920;&#26126;&#65292;postEdit&#30340;&#24615;&#33021;&#36229;&#36807;&#20102;&#25152;&#26377;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge editing (KE) aims to efficiently and precisely modify the behavior of large language models (LLMs) to update specific knowledge without negatively influencing other knowledge. Current research primarily focuses on white-box LLMs editing, overlooking an important scenario: black-box LLMs editing, where LLMs are accessed through interfaces and only textual output is available. To address the limitations of existing evaluations that are not inapplicable to black-box LLM editing and lack comprehensiveness, we propose a multi-perspective evaluation framework, incorporating the assessment of style retention for the first time. To tackle privacy leaks of editing data and style over-editing in current methods, we introduce a novel postEdit framework, resolving privacy concerns through downstream post-processing and maintaining textual style consistency via fine-grained editing to original responses. Experiments and analysis on two benchmarks demonstrate that postEdit outperforms all 
&lt;/p&gt;</description></item><item><title>ChatCell&#26159;&#19968;&#20010;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#20419;&#36827;&#21333;&#32454;&#32990;&#20998;&#26512;&#30340;&#24037;&#20855;&#65292;&#36890;&#36807;&#35789;&#27719;&#36866;&#24212;&#21644;&#32479;&#19968;&#24207;&#21015;&#29983;&#25104;&#65292;&#23427;&#20855;&#22791;&#28145;&#21402;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#36866;&#24212;&#21508;&#31181;&#20998;&#26512;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.08303</link><description>&lt;p&gt;
ChatCell: &#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#20419;&#36827;&#21333;&#32454;&#32990;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
ChatCell: Facilitating Single-Cell Analysis with Natural Language
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08303
&lt;/p&gt;
&lt;p&gt;
ChatCell&#26159;&#19968;&#20010;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#20419;&#36827;&#21333;&#32454;&#32990;&#20998;&#26512;&#30340;&#24037;&#20855;&#65292;&#36890;&#36807;&#35789;&#27719;&#36866;&#24212;&#21644;&#32479;&#19968;&#24207;&#21015;&#29983;&#25104;&#65292;&#23427;&#20855;&#22791;&#28145;&#21402;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#36866;&#24212;&#21508;&#31181;&#20998;&#26512;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#23427;&#20204;&#22312;&#31185;&#23398;&#20013;&#30340;&#24433;&#21709;&#26085;&#30410;&#31361;&#20986;&#12290;LLMs&#22312;&#20219;&#21153;&#27867;&#21270;&#21644;&#33258;&#30001;&#23545;&#35805;&#26041;&#38754;&#30340;&#26032;&#20852;&#33021;&#21147;&#21487;&#20197;&#26497;&#22823;&#22320;&#25512;&#36827;&#21270;&#23398;&#21644;&#29983;&#29289;&#23398;&#31561;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#21333;&#32454;&#32990;&#29983;&#29289;&#23398;&#36825;&#20010;&#26500;&#25104;&#29983;&#29289;&#20307;&#22522;&#30784;&#26500;&#20214;&#30340;&#39046;&#22495;&#20173;&#38754;&#20020;&#19968;&#20123;&#25361;&#25112;&#12290;&#24403;&#21069;&#26041;&#27861;&#22312;&#30693;&#35782;&#38376;&#27099;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#65292;&#38459;&#30861;&#20102;LLMs&#22312;&#25484;&#25569;&#21333;&#32454;&#32990;&#25968;&#25454;&#26041;&#38754;&#30340;&#20805;&#20998;&#21033;&#29992;&#65292;&#24433;&#21709;&#20102;&#30452;&#25509;&#21487;&#35775;&#38382;&#21644;&#24555;&#36895;&#36845;&#20195;&#30340;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;ChatCell&#65292;&#36890;&#36807;&#21033;&#29992;&#35789;&#27719;&#36866;&#24212;&#21644;&#32479;&#19968;&#24207;&#21015;&#29983;&#25104;&#65292;&#23427;&#22312;&#21333;&#32454;&#32990;&#29983;&#29289;&#23398;&#39046;&#22495;&#33719;&#24471;&#20102;&#28145;&#21402;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#36866;&#24212;&#21508;&#31181;&#20998;&#26512;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#26631;&#24535;&#30528;&#19968;&#31181;&#33539;&#24335;&#36716;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;
As Large Language Models (LLMs) rapidly evolve, their influence in science is becoming increasingly prominent. The emerging capabilities of LLMs in task generalization and free-form dialogue can significantly advance fields like chemistry and biology. However, the field of single-cell biology, which forms the foundational building blocks of living organisms, still faces several challenges. High knowledge barriers and limited scalability in current methods restrict the full exploitation of LLMs in mastering single-cell data, impeding direct accessibility and rapid iteration. To this end, we introduce ChatCell, which signifies a paradigm shift by facilitating single-cell analysis with natural language. Leveraging vocabulary adaptation and unified sequence generation, ChatCell has acquired profound expertise in single-cell biology and the capability to accommodate a diverse range of analysis tasks. Extensive experiments further demonstrate ChatCell's robust performance and potential to de
&lt;/p&gt;</description></item><item><title>LLaGA&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#27169;&#22411;&#65292;&#23427;&#26377;&#25928;&#22320;&#25972;&#21512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33021;&#21147;&#65292;&#20197;&#22788;&#29702;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#12290;&#36890;&#36807;&#37325;&#26032;&#32452;&#32455;&#22270;&#33410;&#28857;&#20197;&#20316;&#20026;&#32467;&#26500;&#24863;&#30693;&#30340;&#24207;&#21015;&#65292;&#24182;&#36890;&#36807;&#19968;&#20010;&#22810;&#21151;&#33021;&#25237;&#24433;&#20202;&#23558;&#20854;&#26144;&#23556;&#21040;&#26631;&#35760;&#23884;&#20837;&#31354;&#38388;&#20013;&#65292;LLaGA&#22312;&#22810;&#26679;&#24615;&#12289;&#27867;&#21270;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2402.08170</link><description>&lt;p&gt;
LLaGA: &#22823;&#22411;&#35821;&#35328;&#21644;&#22270;&#24418;&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
LLaGA: Large Language and Graph Assistant
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08170
&lt;/p&gt;
&lt;p&gt;
LLaGA&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#27169;&#22411;&#65292;&#23427;&#26377;&#25928;&#22320;&#25972;&#21512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33021;&#21147;&#65292;&#20197;&#22788;&#29702;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#12290;&#36890;&#36807;&#37325;&#26032;&#32452;&#32455;&#22270;&#33410;&#28857;&#20197;&#20316;&#20026;&#32467;&#26500;&#24863;&#30693;&#30340;&#24207;&#21015;&#65292;&#24182;&#36890;&#36807;&#19968;&#20010;&#22810;&#21151;&#33021;&#25237;&#24433;&#20202;&#23558;&#20854;&#26144;&#23556;&#21040;&#26631;&#35760;&#23884;&#20837;&#31354;&#38388;&#20013;&#65292;LLaGA&#22312;&#22810;&#26679;&#24615;&#12289;&#27867;&#21270;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24050;&#32463;&#25512;&#21160;&#20102;&#22270;&#32467;&#26500;&#25968;&#25454;&#20998;&#26512;&#30340;&#36827;&#27493;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;GPT-4&#30340;&#23835;&#36215;&#39044;&#31034;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#19968;&#20010;&#26032;&#26102;&#20195;&#12290;&#28982;&#32780;&#65292;&#23558;&#23427;&#20204;&#24212;&#29992;&#20110;&#22270;&#25968;&#25454;&#36824;&#38754;&#20020;&#30528;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#30001;&#20110;&#23558;&#22270;&#32467;&#26500;&#36716;&#21270;&#20026;&#25991;&#26412;&#30340;&#22266;&#26377;&#38590;&#24230;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21019;&#26032;&#27169;&#22411;&#8212;&#8212;&#22823;&#22411;&#35821;&#35328;&#21644;&#22270;&#24418;&#21161;&#25163;&#65288;LLaGA&#65289;&#65292;&#23427;&#26377;&#25928;&#22320;&#25972;&#21512;&#20102;LLM&#30340;&#33021;&#21147;&#65292;&#20197;&#22788;&#29702;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#12290;LLaGA&#20445;&#30041;&#20102;LLM&#30340;&#36890;&#29992;&#24615;&#65292;&#21516;&#26102;&#23558;&#22270;&#25968;&#25454;&#36716;&#21270;&#20026;&#19982;LLM&#36755;&#20837;&#20860;&#23481;&#30340;&#26684;&#24335;&#12290;LLaGA&#36890;&#36807;&#37325;&#26032;&#32452;&#32455;&#22270;&#33410;&#28857;&#20197;&#20316;&#20026;&#32467;&#26500;&#24863;&#30693;&#30340;&#24207;&#21015;&#65292;&#28982;&#21518;&#36890;&#36807;&#19968;&#20010;&#22810;&#21151;&#33021;&#25237;&#24433;&#20202;&#23558;&#20854;&#26144;&#23556;&#21040;&#26631;&#35760;&#23884;&#20837;&#31354;&#38388;&#20013;&#12290;LLaGA&#22312;&#22810;&#26679;&#24615;&#12289;&#27867;&#21270;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#19968;&#33268;&#30340;&#33391;&#22909;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have empowered the advance in graph-structured data analysis. Recently, the rise of Large Language Models (LLMs) like GPT-4 has heralded a new era in deep learning. However, their application to graph data poses distinct challenges due to the inherent difficulty of translating graph structures to language. To this end, we introduce the \textbf{L}arge \textbf{L}anguage \textbf{a}nd \textbf{G}raph \textbf{A}ssistant (\textbf{LLaGA}), an innovative model that effectively integrates LLM capabilities to handle the complexities of graph-structured data. LLaGA retains the general-purpose nature of LLMs while adapting graph data into a format compatible with LLM input. LLaGA achieves this by reorganizing graph nodes to structure-aware sequences and then mapping these into the token embedding space through a versatile projector. LLaGA excels in versatility, generalizability and interpretability, allowing it to perform consistently well across different datasets and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#19968;&#33324;&#20215;&#20540;&#20989;&#25968;&#30340;&#24773;&#22659;&#22810;&#39033;&#24335;&#36923;&#36753;&#22238;&#24402;&#36172;&#21338;&#26426;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#22871;&#31639;&#27861;&#26469;&#22788;&#29702;&#32447;&#24615;&#24773;&#20917;&#65292;&#36825;&#20123;&#31639;&#27861;&#20855;&#26377;&#35745;&#31639;&#25928;&#29575;&#39640;&#12289;&#26080;&#32500;&#24230;&#30340;&#36951;&#25022;&#30028;&#38480;&#20197;&#21450;&#22788;&#29702;&#23436;&#20840;&#23545;&#25239;&#24615;&#29615;&#22659;&#21644;&#22870;&#21169;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.08126</link><description>&lt;p&gt;
&#20855;&#26377;&#19968;&#33324;&#20215;&#20540;&#20989;&#25968;&#30340;&#24773;&#22659;&#22810;&#39033;&#24335;&#36923;&#36753;&#22238;&#24402;&#36172;&#21338;&#26426;
&lt;/p&gt;
&lt;p&gt;
Contextual Multinomial Logit Bandits with General Value Functions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#19968;&#33324;&#20215;&#20540;&#20989;&#25968;&#30340;&#24773;&#22659;&#22810;&#39033;&#24335;&#36923;&#36753;&#22238;&#24402;&#36172;&#21338;&#26426;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#22871;&#31639;&#27861;&#26469;&#22788;&#29702;&#32447;&#24615;&#24773;&#20917;&#65292;&#36825;&#20123;&#31639;&#27861;&#20855;&#26377;&#35745;&#31639;&#25928;&#29575;&#39640;&#12289;&#26080;&#32500;&#24230;&#30340;&#36951;&#25022;&#30028;&#38480;&#20197;&#21450;&#22788;&#29702;&#23436;&#20840;&#23545;&#25239;&#24615;&#29615;&#22659;&#21644;&#22870;&#21169;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#22659;&#22810;&#39033;&#24335;&#36923;&#36753;&#22238;&#24402; (MNL) &#36172;&#21338;&#26426;&#21487;&#20197;&#35299;&#20915;&#35768;&#22810;&#23454;&#38469;&#20013;&#30340;&#25512;&#33616;&#38382;&#39064;&#65292;&#27604;&#22914;&#22312;&#32447;&#38646;&#21806;/&#24191;&#21578;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#20165;&#32771;&#34385;&#20102;&#65288;&#24191;&#20041;&#30340;&#65289;&#32447;&#24615;&#20215;&#20540;&#20989;&#25968;&#65292;&#36825;&#20005;&#37325;&#38480;&#21046;&#20102;&#20854;&#36866;&#29992;&#24615;&#12290;&#37492;&#20110;&#36825;&#19968;&#20107;&#23454;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#21253;&#21547;&#30495;&#23454;&#24773;&#20917;&#30340;&#24773;&#22659;MNL&#36172;&#21338;&#26426;&#65292;&#20511;&#37492;&#20102;&#26368;&#36817;&#23545;&#24773;&#22659;&#36172;&#21338;&#26426;&#30740;&#31350;&#30340;&#36235;&#21183;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#38543;&#26426;&#21644;&#23545;&#25239;&#24615;&#29615;&#22659;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#22871;&#31639;&#27861;&#65292;&#27599;&#20010;&#31639;&#27861;&#22312;&#35745;&#31639;&#21644;&#36951;&#25022;&#20043;&#38388;&#26377;&#19981;&#21516;&#30340;&#26435;&#34913;&#12290;&#24403;&#24212;&#29992;&#20110;&#32447;&#24615;&#24773;&#20917;&#26102;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#19981;&#20165;&#26159;&#31532;&#19968;&#20010;&#19981;&#20381;&#36182;&#20110;&#26576;&#20010;&#21487;&#33021;&#25351;&#25968;&#22686;&#38271;&#30340;&#38382;&#39064;&#30456;&#20851;&#24120;&#25968;&#30340;&#32467;&#26524;&#65292;&#36824;&#20855;&#26377;&#20854;&#20182;&#20248;&#21183;&#65292;&#22914;&#35745;&#31639;&#25928;&#29575;&#12289;&#26080;&#32500;&#24230;&#30340;&#36951;&#25022;&#30028;&#38480;&#20197;&#21450;&#22788;&#29702;&#23436;&#20840;&#23545;&#25239;&#24615;&#29615;&#22659;&#21644;&#22870;&#21169;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contextual multinomial logit (MNL) bandits capture many real-world assortment recommendation problems such as online retailing/advertising. However, prior work has only considered (generalized) linear value functions, which greatly limits its applicability. Motivated by this fact, in this work, we consider contextual MNL bandits with a general value function class that contains the ground truth, borrowing ideas from a recent trend of studies on contextual bandits. Specifically, we consider both the stochastic and the adversarial settings, and propose a suite of algorithms, each with different computation-regret trade-off. When applied to the linear case, our results not only are the first ones with no dependence on a certain problem-dependent constant that can be exponentially large, but also enjoy other advantages such as computational efficiency, dimension-free regret bounds, or the ability to handle completely adversarial contexts and rewards.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#19979;&#19968;&#20010;&#26354;&#32447;&#24418;&#29366;&#39044;&#27979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#32534;&#30721;&#22120;&#30340;&#38646;&#26679;&#26412;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;GTT&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#36890;&#36947;&#32423;&#21035;&#30340;&#26354;&#32447;&#24418;&#29366;&#39044;&#27979;&#65292;&#23637;&#29616;&#20986;&#20248;&#31168;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#29978;&#33267;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#26377;&#30417;&#30563;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.07570</link><description>&lt;p&gt;
&#21482;&#26377;&#26354;&#32447;&#24418;&#29366;&#26377;&#20851;&#65306;&#36890;&#36807;&#19979;&#19968;&#20010;&#26354;&#32447;&#24418;&#29366;&#39044;&#27979;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#38646;&#26679;&#26412;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Only the Curve Shape Matters: Training Foundation Models for Zero-Shot Multivariate Time Series Forecasting through Next Curve Shape Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07570
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#19979;&#19968;&#20010;&#26354;&#32447;&#24418;&#29366;&#39044;&#27979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#32534;&#30721;&#22120;&#30340;&#38646;&#26679;&#26412;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;GTT&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#36890;&#36947;&#32423;&#21035;&#30340;&#26354;&#32447;&#24418;&#29366;&#39044;&#27979;&#65292;&#23637;&#29616;&#20986;&#20248;&#31168;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#29978;&#33267;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#26377;&#30417;&#30563;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;General Time Transformer (GTT)&#65292;&#19968;&#31181;&#20165;&#26377;&#32534;&#30721;&#22120;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;GTT&#22312;&#19968;&#20010;&#21253;&#21547;2&#20159;&#20010;&#39640;&#36136;&#37327;&#26102;&#38388;&#24207;&#21015;&#26679;&#26412;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28085;&#30422;&#20102;&#19981;&#21516;&#39046;&#22495;&#12290;&#22312;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#20013;&#65292;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#20219;&#21153;&#34987;&#24314;&#27169;&#20026;&#19968;&#20010;&#36880;&#36890;&#36947;&#30340;&#19979;&#19968;&#20010;&#26354;&#32447;&#24418;&#29366;&#39044;&#27979;&#38382;&#39064;&#65292;&#20854;&#20013;&#27599;&#20010;&#26102;&#38388;&#24207;&#21015;&#26679;&#26412;&#34920;&#31034;&#20026;&#19968;&#31995;&#21015;&#38750;&#37325;&#21472;&#30340;&#26354;&#32447;&#24418;&#29366;&#65292;&#20855;&#26377;&#32479;&#19968;&#30340;&#25968;&#20540;&#22823;&#23567;&#12290;GTT&#22312;&#36890;&#36947;&#32423;&#21035;&#19978;&#36890;&#36807;&#39044;&#27979;&#36807;&#21435;&#26354;&#32447;&#24418;&#29366;&#30340;&#31383;&#21475;&#26469;&#39044;&#27979;&#19979;&#19968;&#20010;&#26354;&#32447;&#24418;&#29366;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;GTT&#22312;&#26410;&#35265;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#20248;&#31168;&#30340;&#38646;&#26679;&#26412;&#22810;&#20803;&#39044;&#27979;&#33021;&#21147;&#65292;&#29978;&#33267;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#26377;&#30417;&#30563;&#22522;&#32447;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;GTT&#27169;&#22411;&#21442;&#25968;&#21644;&#35757;&#32451;&#25968;&#25454;&#38598;&#35268;&#27169;&#21464;&#21270;&#30340;&#24433;&#21709;&#65292;&#35266;&#23519;&#21040;&#22312;&#38646;&#26679;&#26412;&#22810;&#20803;&#39044;&#27979;&#30340;&#32972;&#26223;&#19979;&#65292;&#35268;&#27169;&#23450;&#24459;&#20063;&#25104;&#31435;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present General Time Transformer (GTT), an encoder-only style foundation model for zero-shot multivariate time series forecasting. GTT is pretrained on a large dataset of 200M high-quality time series samples spanning diverse domains. In our proposed framework, the task of multivariate time series forecasting is formulated as a channel-wise next curve shape prediction problem, where each time series sample is represented as a sequence of non-overlapping curve shapes with a unified numerical magnitude. GTT is trained to predict the next curve shape based on a window of past curve shapes in a channel-wise manner. Experimental results demonstrate that GTT exhibits superior zero-shot multivariate forecasting capabilities on unseen time series datasets, even surpassing state-of-the-art supervised baselines. Additionally, we investigate the impact of varying GTT model parameters and training dataset scales, observing that the scaling law also holds in the context of zero-shot multivariate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#22343;&#22330;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243; (SDE) &#30340;&#31283;&#24577;&#20998;&#24067;&#20013;&#37319;&#26679;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#32806;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#22810;&#31181;&#24773;&#20917;&#19979;&#25552;&#20379;&#25913;&#36827;&#30340;&#20445;&#35777;&#65292;&#21253;&#25324;&#22312;&#22343;&#22330;&#21306;&#22495;&#20248;&#21270;&#26576;&#20123;&#21452;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#26356;&#22909;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2402.07355</link><description>&lt;p&gt;
&#20174;&#22343;&#22330;&#31283;&#24577;&#20998;&#24067;&#20013;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Sampling from the Mean-Field Stationary Distribution
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07355
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#22343;&#22330;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243; (SDE) &#30340;&#31283;&#24577;&#20998;&#24067;&#20013;&#37319;&#26679;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#32806;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#22810;&#31181;&#24773;&#20917;&#19979;&#25552;&#20379;&#25913;&#36827;&#30340;&#20445;&#35777;&#65292;&#21253;&#25324;&#22312;&#22343;&#22330;&#21306;&#22495;&#20248;&#21270;&#26576;&#20123;&#21452;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#26356;&#22909;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20174;&#22343;&#22330;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243; (SDE) &#30340;&#31283;&#24577;&#20998;&#24067;&#20013;&#37319;&#26679;&#30340;&#22797;&#26434;&#24615;&#65292;&#25110;&#32773;&#31561;&#20215;&#22320;&#65292;&#21363;&#21253;&#21547;&#20132;&#20114;&#39033;&#30340;&#27010;&#29575;&#27979;&#24230;&#31354;&#38388;&#19978;&#30340;&#26368;&#23567;&#21270;&#20989;&#25968;&#30340;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#27934;&#23519;&#26159;&#23558;&#36825;&#20010;&#38382;&#39064;&#30340;&#20004;&#20010;&#20851;&#38190;&#26041;&#38754;&#35299;&#32806;&#65306;(1) &#36890;&#36807;&#26377;&#38480;&#31890;&#23376;&#31995;&#32479;&#36924;&#36817;&#22343;&#22330;SDE&#65292;&#36890;&#36807;&#26102;&#38388;&#22343;&#21248;&#20256;&#25773;&#28151;&#27788;&#65292;&#21644;(2) &#36890;&#36807;&#26631;&#20934;&#23545;&#25968;&#20985;&#25277;&#26679;&#22120;&#20174;&#26377;&#38480;&#31890;&#23376;&#31283;&#24577;&#20998;&#24067;&#20013;&#37319;&#26679;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27010;&#24565;&#19978;&#26356;&#31616;&#21333;&#65292;&#20854;&#28789;&#27963;&#24615;&#20801;&#35768;&#32467;&#21512;&#29992;&#20110;&#31639;&#27861;&#21644;&#29702;&#35770;&#30340;&#26368;&#26032;&#25216;&#26415;&#12290;&#36825;&#23548;&#33268;&#22312;&#35768;&#22810;&#35774;&#32622;&#20013;&#25552;&#20379;&#20102;&#25913;&#36827;&#30340;&#20445;&#35777;&#65292;&#21253;&#25324;&#22312;&#22343;&#22330;&#21306;&#22495;&#20248;&#21270;&#26576;&#20123;&#21452;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#26356;&#22909;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the complexity of sampling from the stationary distribution of a mean-field SDE, or equivalently, the complexity of minimizing a functional over the space of probability measures which includes an interaction term.   Our main insight is to decouple the two key aspects of this problem: (1) approximation of the mean-field SDE via a finite-particle system, via uniform-in-time propagation of chaos, and (2) sampling from the finite-particle stationary distribution, via standard log-concave samplers. Our approach is conceptually simpler and its flexibility allows for incorporating the state-of-the-art for both algorithms and theory. This leads to improved guarantees in numerous settings, including better guarantees for optimizing certain two-layer neural networks in the mean-field regime.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#31995;&#32479;&#35843;&#26597;&#65292;&#21457;&#29616;&#25972;&#21512;&#39046;&#22495;&#30693;&#35782;&#21487;&#20197;&#25552;&#39640;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#34701;&#21512;&#21487;&#20197;&#20135;&#29983;&#26356;&#31934;&#30830;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.07249</link><description>&lt;p&gt;
&#39046;&#22495;&#30693;&#35782;&#21644;&#22810;&#27169;&#24577;&#23545;&#26234;&#33021;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#30340;&#24433;&#21709;&#65306;&#19968;&#39033;&#31995;&#32479;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
The Impact of Domain Knowledge and Multi-Modality on Intelligent Molecular Property Prediction: A Systematic Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07249
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#31995;&#32479;&#35843;&#26597;&#65292;&#21457;&#29616;&#25972;&#21512;&#39046;&#22495;&#30693;&#35782;&#21487;&#20197;&#25552;&#39640;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#34701;&#21512;&#21487;&#20197;&#20135;&#29983;&#26356;&#31934;&#30830;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#39044;&#27979;&#20998;&#23376;&#24615;&#36136;&#23545;&#20110;&#33647;&#29289;&#24320;&#21457;&#23588;&#20854;&#26159;&#34394;&#25311;&#31579;&#36873;&#21644;&#21270;&#21512;&#29289;&#20248;&#21270;&#30340;&#36827;&#23637;&#33267;&#20851;&#37325;&#35201;&#12290;&#36817;&#24180;&#26469;&#24341;&#20837;&#20102;&#35768;&#22810;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#22312;&#22686;&#24378;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#65288;MPP&#65289;&#26041;&#38754;&#26174;&#31034;&#20986;&#26174;&#33879;&#28508;&#21147;&#65292;&#29305;&#21035;&#26159;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#21644;&#23545;&#20998;&#23376;&#32467;&#26500;&#30340;&#27934;&#23519;&#21147;&#12290;&#28982;&#32780;&#65292;&#26377;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#39046;&#22495;&#30693;&#35782;&#30340;&#25972;&#21512;&#26159;&#21542;&#22686;&#24378;&#20102;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#20351;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#34701;&#21512;&#26159;&#21542;&#27604;&#21333;&#19968;&#25968;&#25454;&#26469;&#28304;&#26041;&#27861;&#20135;&#29983;&#26356;&#31934;&#30830;&#30340;&#32467;&#26524;&#65311;&#20026;&#20102;&#25506;&#31350;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#20840;&#38754;&#22238;&#39038;&#21644;&#23450;&#37327;&#20998;&#26512;&#20102;&#22522;&#20110;&#21508;&#31181;&#22522;&#20934;&#30340;&#26368;&#26032;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#25972;&#21512;&#20998;&#23376;&#20449;&#24687;&#23558;&#20998;&#21035;&#25552;&#39640;MPP&#22238;&#24402;&#21644;&#20998;&#31867;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#65292;&#20998;&#21035;&#39640;&#36798;3.98&#65285;&#21644;1.72&#65285;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#20351;&#29992;&#19977;&#32500;&#20449;&#24687;&#19982;&#19968;&#32500;&#21644;&#20108;&#32500;&#20449;&#24687;&#30456;&#32467;&#21512;&#20250;&#20135;&#29983;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The precise prediction of molecular properties is essential for advancements in drug development, particularly in virtual screening and compound optimization. The recent introduction of numerous deep learning-based methods has shown remarkable potential in enhancing molecular property prediction (MPP), especially improving accuracy and insights into molecular structures. Yet, two critical questions arise: does the integration of domain knowledge augment the accuracy of molecular property prediction and does employing multi-modal data fusion yield more precise results than unique data source methods? To explore these matters, we comprehensively review and quantitatively analyze recent deep learning methods based on various benchmarks. We discover that integrating molecular information will improve both MPP regression and classification tasks by upto 3.98% and 1.72%, respectively. We also discover that the utilizing 3-dimensional information with 1-dimensional and 2-dimensional informati
&lt;/p&gt;</description></item><item><title>&#21160;&#24577;&#22270;&#20449;&#24687;&#29942;&#39048;&#26694;&#26550;&#65288;DGIB&#65289;&#33021;&#22815;&#23398;&#20064;&#40065;&#26834;&#19988;&#26377;&#21306;&#20998;&#24615;&#30340;&#21160;&#24577;&#22270;&#34920;&#31034;&#12290;&#21033;&#29992;&#20449;&#24687;&#29942;&#39048;&#21407;&#29702;&#65292;&#36890;&#36807;&#36845;&#20195;&#24341;&#23548;&#21644;&#25913;&#36827;&#22270;&#24555;&#29031;&#20256;&#36882;&#30340;&#32467;&#26500;&#21644;&#29305;&#24449;&#20449;&#24687;&#27969;&#65292;&#21387;&#32553;&#20887;&#20313;&#20449;&#24687;&#24182;&#20445;&#30041;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#12290;&#35813;&#26694;&#26550;&#33021;&#28385;&#36275;&#26368;&#23567;-&#20840;&#23616;-&#19968;&#33268;&#26465;&#20214;&#65292;&#25552;&#39640;&#20102;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.06716</link><description>&lt;p&gt;
&#21160;&#24577;&#22270;&#20449;&#24687;&#29942;&#39048;
&lt;/p&gt;
&lt;p&gt;
Dynamic Graph Information Bottleneck
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06716
&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#22270;&#20449;&#24687;&#29942;&#39048;&#26694;&#26550;&#65288;DGIB&#65289;&#33021;&#22815;&#23398;&#20064;&#40065;&#26834;&#19988;&#26377;&#21306;&#20998;&#24615;&#30340;&#21160;&#24577;&#22270;&#34920;&#31034;&#12290;&#21033;&#29992;&#20449;&#24687;&#29942;&#39048;&#21407;&#29702;&#65292;&#36890;&#36807;&#36845;&#20195;&#24341;&#23548;&#21644;&#25913;&#36827;&#22270;&#24555;&#29031;&#20256;&#36882;&#30340;&#32467;&#26500;&#21644;&#29305;&#24449;&#20449;&#24687;&#27969;&#65292;&#21387;&#32553;&#20887;&#20313;&#20449;&#24687;&#24182;&#20445;&#30041;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#12290;&#35813;&#26694;&#26550;&#33021;&#28385;&#36275;&#26368;&#23567;-&#20840;&#23616;-&#19968;&#33268;&#26465;&#20214;&#65292;&#25552;&#39640;&#20102;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#22270;&#24191;&#27867;&#23384;&#22312;&#20110;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#23427;&#20204;&#25658;&#24102;&#30528;&#22797;&#26434;&#30340;&#26102;&#31354;&#29305;&#24449;&#27169;&#24335;&#65292;&#23545;&#20110;&#23427;&#20204;&#30340;&#34920;&#31034;&#23398;&#20064;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;DGNNs&#65289;&#36890;&#36807;&#21033;&#29992;&#20869;&#22312;&#30340;&#21160;&#24577;&#24615;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;DGNNs&#23637;&#31034;&#20102;&#26377;&#38480;&#30340;&#40065;&#26834;&#24615;&#65292;&#26131;&#21463;&#23545;&#25239;&#25915;&#20987;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21160;&#24577;&#22270;&#20449;&#24687;&#29942;&#39048;&#65288;DGIB&#65289;&#26694;&#26550;&#26469;&#23398;&#20064;&#40065;&#26834;&#19988;&#26377;&#21306;&#20998;&#24615;&#30340;&#34920;&#31034;&#12290;&#20511;&#21161;&#20449;&#24687;&#29942;&#39048;&#65288;IB&#65289;&#21407;&#29702;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#26399;&#26395;&#30340;&#26368;&#20248;&#34920;&#31034;&#24212;&#28385;&#36275;&#26368;&#23567;-&#20840;&#23616;-&#19968;&#33268;&#65288;MSC&#65289;&#26465;&#20214;&#12290;&#20026;&#20102;&#22312;&#28508;&#22312;&#34920;&#31034;&#20013;&#21387;&#32553;&#20887;&#20313;&#20449;&#24687;&#21644;&#20445;&#30041;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#65292;DGIB&#36845;&#20195;&#22320;&#24341;&#23548;&#21644;&#25913;&#36827;&#36890;&#36807;&#22270;&#24555;&#29031;&#20256;&#36882;&#30340;&#32467;&#26500;&#21644;&#29305;&#24449;&#20449;&#24687;&#27969;&#12290;&#20026;&#20102;&#28385;&#36275;MSC&#26465;&#20214;&#65292;&#25105;&#20204;&#23558;&#25972;&#20307;IB&#30446;&#26631;&#20998;&#35299;&#20026;DGIB$_{MS}$&#21644;DGIB$_C$&#65292;&#20854;&#20013;DGIB$_{MS}$&#36890;&#36947;&#30340;&#30446;&#26631;&#26159;...
&lt;/p&gt;
&lt;p&gt;
Dynamic Graphs widely exist in the real world, which carry complicated spatial and temporal feature patterns, challenging their representation learning. Dynamic Graph Neural Networks (DGNNs) have shown impressive predictive abilities by exploiting the intrinsic dynamics. However, DGNNs exhibit limited robustness, prone to adversarial attacks. This paper presents the novel Dynamic Graph Information Bottleneck (DGIB) framework to learn robust and discriminative representations. Leveraged by the Information Bottleneck (IB) principle, we first propose the expected optimal representations should satisfy the Minimal-Sufficient-Consensual (MSC) Condition. To compress redundant as well as conserve meritorious information into latent representation, DGIB iteratively directs and refines the structural and feature information flow passing through graph snapshots. To meet the MSC Condition, we decompose the overall IB objectives into DGIB$_{MS}$ and DGIB$_C$, in which the DGIB$_{MS}$ channel aims 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20869;&#30465;&#35268;&#21010;&#30340;&#27010;&#24565;&#65292;&#20316;&#20026;&#19968;&#31181;&#24341;&#23548;&#35821;&#35328;&#39537;&#21160;&#30340;&#20195;&#29702;&#26426;&#22120;&#20154;&#25913;&#36827;&#33258;&#36523;&#19981;&#30830;&#23450;&#24615;&#30340;&#31995;&#32479;&#26041;&#27861;&#12290;&#36890;&#36807;&#35782;&#21035;&#20219;&#21153;&#19981;&#30830;&#23450;&#24615;&#24182;&#20027;&#21160;&#23547;&#27714;&#28548;&#28165;&#65292;&#20869;&#30465;&#26174;&#33879;&#25552;&#39640;&#20102;&#26426;&#22120;&#20154;&#20219;&#21153;&#35268;&#21010;&#30340;&#25104;&#21151;&#29575;&#21644;&#23433;&#20840;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.06529</link><description>&lt;p&gt;
&#20869;&#30465;&#35268;&#21010;&#65306;&#24341;&#23548;&#35821;&#35328;&#39537;&#21160;&#30340;&#20195;&#29702;&#26426;&#22120;&#20154;&#25913;&#36827;&#33258;&#36523;&#30340;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Introspective Planning: Guiding Language-Enabled Agents to Refine Their Own Uncertainty
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06529
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20869;&#30465;&#35268;&#21010;&#30340;&#27010;&#24565;&#65292;&#20316;&#20026;&#19968;&#31181;&#24341;&#23548;&#35821;&#35328;&#39537;&#21160;&#30340;&#20195;&#29702;&#26426;&#22120;&#20154;&#25913;&#36827;&#33258;&#36523;&#19981;&#30830;&#23450;&#24615;&#30340;&#31995;&#32479;&#26041;&#27861;&#12290;&#36890;&#36807;&#35782;&#21035;&#20219;&#21153;&#19981;&#30830;&#23450;&#24615;&#24182;&#20027;&#21160;&#23547;&#27714;&#28548;&#28165;&#65292;&#20869;&#30465;&#26174;&#33879;&#25552;&#39640;&#20102;&#26426;&#22120;&#20154;&#20219;&#21153;&#35268;&#21010;&#30340;&#25104;&#21151;&#29575;&#21644;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#20808;&#36827;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20351;&#24471;&#26426;&#22120;&#20154;&#33021;&#22815;&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#65292;&#24182;&#36890;&#36807;&#36866;&#24403;&#30340;&#22522;&#30784;&#22609;&#36896;&#26469;&#31574;&#30053;&#24615;&#22320;&#36827;&#34892;&#39640;&#32423;&#34892;&#21160;&#35268;&#21010;&#12290;&#28982;&#32780;&#65292;LLM&#20135;&#29983;&#30340;&#24187;&#35273;&#21487;&#33021;&#23548;&#33268;&#26426;&#22120;&#20154;&#33258;&#20449;&#22320;&#25191;&#34892;&#19982;&#29992;&#25143;&#30446;&#26631;&#19981;&#31526;&#25110;&#22312;&#26497;&#31471;&#24773;&#20917;&#19979;&#19981;&#23433;&#20840;&#30340;&#35745;&#21010;&#12290;&#27492;&#22806;&#65292;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#20013;&#30340;&#22266;&#26377;&#27495;&#20041;&#21487;&#33021;&#24341;&#21457;&#20219;&#21153;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#23588;&#20854;&#26159;&#22312;&#23384;&#22312;&#22810;&#20010;&#26377;&#25928;&#36873;&#39033;&#30340;&#24773;&#20917;&#19979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;LLMs&#24517;&#39035;&#35782;&#21035;&#27492;&#31867;&#19981;&#30830;&#23450;&#24615;&#24182;&#20027;&#21160;&#23547;&#27714;&#28548;&#28165;&#12290;&#26412;&#25991;&#25506;&#32034;&#20102;&#20869;&#30465;&#35268;&#21010;&#30340;&#27010;&#24565;&#65292;&#20316;&#20026;&#19968;&#31181;&#31995;&#32479;&#26041;&#27861;&#65292;&#24341;&#23548;LLMs&#22312;&#26080;&#38656;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#24418;&#25104;&#24847;&#35782;&#21040;&#19981;&#30830;&#23450;&#24615;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#25191;&#34892;&#35745;&#21010;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20219;&#21153;&#32423;&#26426;&#22120;&#20154;&#35268;&#21010;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#24182;&#35777;&#26126;&#19982;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;LLM&#30340;&#35268;&#21010;&#26041;&#27861;&#30456;&#27604;&#65292;&#20869;&#30465;&#26174;&#33879;&#25552;&#39640;&#20102;&#25104;&#21151;&#29575;&#21644;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) exhibit advanced reasoning skills, enabling robots to comprehend natural language instructions and strategically plan high-level actions through proper grounding. However, LLM hallucination may result in robots confidently executing plans that are misaligned with user goals or, in extreme cases, unsafe. Additionally, inherent ambiguity in natural language instructions can induce task uncertainty, particularly in situations where multiple valid options exist. To address this issue, LLMs must identify such uncertainty and proactively seek clarification. This paper explores the concept of introspective planning as a systematic method for guiding LLMs in forming uncertainty--aware plans for robotic task execution without the need for fine-tuning. We investigate uncertainty quantification in task-level robot planning and demonstrate that introspection significantly improves both success rates and safety compared to state-of-the-art LLM-based planning approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LIFTED&#30340;&#22810;&#27169;&#24577;&#20020;&#24202;&#35797;&#39564;&#32467;&#26524;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#19981;&#21516;&#27169;&#24577;&#25968;&#25454;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#26469;&#32479;&#19968;&#25968;&#25454;&#65292;&#24182;&#26500;&#24314;&#32479;&#19968;&#30340;&#25239;&#22122;&#22768;&#32534;&#30721;&#22120;&#36827;&#34892;&#20449;&#24687;&#25552;&#21462;&#12290;</title><link>https://arxiv.org/abs/2402.06512</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#20020;&#24202;&#35797;&#39564;&#32467;&#26524;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Multimodal Clinical Trial Outcome Prediction with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06512
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LIFTED&#30340;&#22810;&#27169;&#24577;&#20020;&#24202;&#35797;&#39564;&#32467;&#26524;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#19981;&#21516;&#27169;&#24577;&#25968;&#25454;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#26469;&#32479;&#19968;&#25968;&#25454;&#65292;&#24182;&#26500;&#24314;&#32479;&#19968;&#30340;&#25239;&#22122;&#22768;&#32534;&#30721;&#22120;&#36827;&#34892;&#20449;&#24687;&#25552;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#35797;&#39564;&#26159;&#19968;&#20010;&#20851;&#38190;&#19988;&#26114;&#36149;&#30340;&#36807;&#31243;&#65292;&#36890;&#24120;&#38656;&#35201;&#22810;&#24180;&#26102;&#38388;&#21644;&#22823;&#37327;&#36130;&#21147;&#36164;&#28304;&#12290;&#22240;&#27492;&#65292;&#24320;&#21457;&#20020;&#24202;&#35797;&#39564;&#32467;&#26524;&#39044;&#27979;&#27169;&#22411;&#26088;&#22312;&#25490;&#38500;&#21487;&#33021;&#22833;&#36133;&#30340;&#33647;&#29289;&#65292;&#24182;&#20855;&#26377;&#26174;&#33879;&#30340;&#25104;&#26412;&#33410;&#32422;&#28508;&#21147;&#12290;&#26368;&#36817;&#30340;&#25968;&#25454;&#39537;&#21160;&#23581;&#35797;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#25972;&#21512;&#22810;&#27169;&#24577;&#25968;&#25454;&#26469;&#39044;&#27979;&#20020;&#24202;&#35797;&#39564;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20381;&#36182;&#20110;&#25163;&#21160;&#35774;&#35745;&#30340;&#27169;&#24577;&#29305;&#23450;&#32534;&#30721;&#22120;&#65292;&#36825;&#38480;&#21046;&#20102;&#36866;&#24212;&#26032;&#27169;&#24577;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#35782;&#21035;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30456;&#20284;&#20449;&#24687;&#27169;&#24335;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#19987;&#23478;&#28151;&#21512;&#65288;LIFTED&#65289;&#26041;&#27861;&#29992;&#20110;&#20020;&#24202;&#35797;&#39564;&#32467;&#26524;&#39044;&#27979;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;LIFTED&#36890;&#36807;&#23558;&#19981;&#21516;&#27169;&#24577;&#30340;&#25968;&#25454;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#26469;&#32479;&#19968;&#19981;&#21516;&#27169;&#24577;&#25968;&#25454;&#12290;&#28982;&#21518;&#65292;LIFTED&#26500;&#24314;&#32479;&#19968;&#30340;&#25239;&#22122;&#22768;&#32534;&#30721;&#22120;&#65292;&#20174;&#27169;&#24577;&#29305;&#23450;&#30340;&#35821;&#35328;&#25551;&#36848;&#20013;&#25552;&#21462;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
The clinical trial is a pivotal and costly process, often spanning multiple years and requiring substantial financial resources. Therefore, the development of clinical trial outcome prediction models aims to exclude drugs likely to fail and holds the potential for significant cost savings. Recent data-driven attempts leverage deep learning methods to integrate multimodal data for predicting clinical trial outcomes. However, these approaches rely on manually designed modal-specific encoders, which limits both the extensibility to adapt new modalities and the ability to discern similar information patterns across different modalities. To address these issues, we propose a multimodal mixture-of-experts (LIFTED) approach for clinical trial outcome prediction. Specifically, LIFTED unifies different modality data by transforming them into natural language descriptions. Then, LIFTED constructs unified noise-resilient encoders to extract information from modal-specific language descriptions. S
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PEAK&#30340;&#26032;&#22411;&#38750;&#21442;&#25968;&#39034;&#24207;&#22797;&#21512;&#20551;&#35774;&#26816;&#39564;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#22810;&#20010;&#25968;&#25454;&#27969;&#30340;&#22343;&#20540;&#26816;&#39564;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#27979;&#35797;&#21363;&#21338;&#24328;&#30340;&#26694;&#26550;&#65292;&#22312;&#20219;&#20309;&#20572;&#27490;&#26102;&#38388;&#19978;&#25552;&#20379;&#20102;&#38750;&#28176;&#36827;&#945;&#27700;&#24179;&#30340;&#26816;&#39564;&#12290;PEAK&#33021;&#22815;&#26377;&#25928;&#25298;&#32477;&#22312;&#28385;&#36275;&#38750;&#21442;&#25968;&#20551;&#35774;&#26465;&#20214;&#30340;&#25152;&#26377;&#28508;&#22312;&#20998;&#24067;&#20013;&#38169;&#35823;&#30340;&#20551;&#35774;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#22810;&#20010;&#25968;&#25454;&#27969;&#30340;&#32852;&#21512;&#22797;&#21512;&#20551;&#35774;&#26816;&#39564;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#36739;&#39640;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.06122</link><description>&lt;p&gt;
&#20351;&#29992;PEAK&#36827;&#34892;&#31397;&#25506;&#65306;&#22810;&#20010;&#25968;&#25454;&#27969;&#22343;&#20540;&#30340;&#39034;&#24207;&#12289;&#38750;&#21442;&#25968;&#22797;&#21512;&#20551;&#35774;&#26816;&#39564;
&lt;/p&gt;
&lt;p&gt;
Peeking with PEAK: Sequential, Nonparametric Composite Hypothesis Tests for Means of Multiple Data Streams
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06122
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PEAK&#30340;&#26032;&#22411;&#38750;&#21442;&#25968;&#39034;&#24207;&#22797;&#21512;&#20551;&#35774;&#26816;&#39564;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#22810;&#20010;&#25968;&#25454;&#27969;&#30340;&#22343;&#20540;&#26816;&#39564;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#27979;&#35797;&#21363;&#21338;&#24328;&#30340;&#26694;&#26550;&#65292;&#22312;&#20219;&#20309;&#20572;&#27490;&#26102;&#38388;&#19978;&#25552;&#20379;&#20102;&#38750;&#28176;&#36827;&#945;&#27700;&#24179;&#30340;&#26816;&#39564;&#12290;PEAK&#33021;&#22815;&#26377;&#25928;&#25298;&#32477;&#22312;&#28385;&#36275;&#38750;&#21442;&#25968;&#20551;&#35774;&#26465;&#20214;&#30340;&#25152;&#26377;&#28508;&#22312;&#20998;&#24067;&#20013;&#38169;&#35823;&#30340;&#20551;&#35774;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#22810;&#20010;&#25968;&#25454;&#27969;&#30340;&#32852;&#21512;&#22797;&#21512;&#20551;&#35774;&#26816;&#39564;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#36739;&#39640;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38750;&#21442;&#25968;&#39034;&#24207;&#22797;&#21512;&#20551;&#35774;&#26816;&#39564;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#20010;&#25968;&#25454;&#27969;&#30340;&#22343;&#20540;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21517;&#20026;PEAK&#65288;&#22522;&#20110;&#26399;&#26395;&#24179;&#22343;&#36164;&#20135;&#30340;&#31397;&#25506;&#65289;&#65292;&#22522;&#20110;&#27979;&#35797;&#21363;&#21338;&#24328;&#30340;&#26694;&#26550;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#22312;&#20219;&#20309;&#20572;&#27490;&#26102;&#38388;&#19978;&#30340;&#38750;&#28176;&#36827;&#945;&#27700;&#24179;&#27979;&#35797;&#12290;PEAK&#22312;&#35745;&#31639;&#19978;&#21487;&#34892;&#65292;&#24182;&#19988;&#33021;&#22815;&#26377;&#25928;&#25298;&#32477;&#22312;&#28385;&#36275;&#25105;&#20204;&#30340;&#38750;&#21442;&#25968;&#20551;&#35774;&#26465;&#20214;&#30340;&#25152;&#26377;&#28508;&#22312;&#20998;&#24067;&#20013;&#38169;&#35823;&#30340;&#20551;&#35774;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#22810;&#20010;&#25968;&#25454;&#27969;&#30340;&#32852;&#21512;&#22797;&#21512;&#20551;&#35774;&#26816;&#39564;&#12290;&#25105;&#20204;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26368;&#20339;&#33218;&#35782;&#21035;&#21644;&#38408;&#20540;&#35782;&#21035;&#20219;&#21153;&#20013;&#23545;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#36827;&#34892;&#20102;&#25968;&#20540;&#39564;&#35777;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35745;&#31639;&#25928;&#29575;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#27979;&#35797;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel nonparametric sequential test for composite hypotheses for means of multiple data streams. Our proposed method, \emph{peeking with expectation-based averaged capital} (PEAK), builds upon the testing-as-betting framework and provides a non-asymptotic $\alpha$-level test across any stopping time. PEAK is computationally tractable and efficiently rejects hypotheses that are incorrect across all potential distributions that satisfy our nonparametric assumption, enabling joint composite hypothesis testing on multiple streams of data. We numerically validate our theoretical findings under the best arm identification and threshold identification in the bandit setting, illustrating the computational efficiency of our method against state-of-the-art testing methods.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;TASER&#26041;&#27861;&#65292;&#23427;&#26159;&#38024;&#23545;&#21160;&#24577;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#26102;&#38388;&#33258;&#36866;&#24212;&#37319;&#26679;&#25216;&#26415;&#65292;&#22312;&#20934;&#30830;&#24615;&#12289;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#35299;&#20915;&#20102;&#29616;&#23454;&#19990;&#30028;&#21160;&#24577;&#22270;&#20013;&#23384;&#22312;&#30340;&#22122;&#22768;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.05396</link><description>&lt;p&gt;
TASER: &#26102;&#38388;&#33258;&#36866;&#24212;&#37319;&#26679;&#30340;&#24555;&#36895;&#20934;&#30830;&#21160;&#24577;&#22270;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
TASER: Temporal Adaptive Sampling for Fast and Accurate Dynamic Graph Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05396
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;TASER&#26041;&#27861;&#65292;&#23427;&#26159;&#38024;&#23545;&#21160;&#24577;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#26102;&#38388;&#33258;&#36866;&#24212;&#37319;&#26679;&#25216;&#26415;&#65292;&#22312;&#20934;&#30830;&#24615;&#12289;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#35299;&#20915;&#20102;&#29616;&#23454;&#19990;&#30028;&#21160;&#24577;&#22270;&#20013;&#23384;&#22312;&#30340;&#22122;&#22768;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;TGNN&#65289;&#22312;&#21253;&#25324;&#27450;&#35784;&#26816;&#27979;&#21644;&#20869;&#23481;&#25512;&#33616;&#22312;&#20869;&#30340;&#21508;&#31181;&#37325;&#35201;&#24212;&#29992;&#20013;&#23637;&#31034;&#20986;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;TGNN&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#23481;&#26131;&#21463;&#21040;&#29616;&#23454;&#19990;&#30028;&#21160;&#24577;&#22270;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#22122;&#22768;&#30340;&#24433;&#21709;&#65292;&#20363;&#22914;&#26102;&#38388;&#36807;&#26102;&#30340;&#38142;&#25509;&#21644;&#20559;&#26012;&#30340;&#20132;&#20114;&#20998;&#24067;&#12290;&#36825;&#20123;&#22122;&#22768;&#23548;&#33268;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#20005;&#37325;&#25439;&#23475;&#20102;TGNN&#30340;&#20934;&#30830;&#24615;&#65306;&#65288;1&#65289;&#27169;&#22411;&#21463;&#21040;&#36739;&#24046;&#20132;&#20114;&#30340;&#30417;&#30563;&#65292;&#65288;2&#65289;&#22122;&#22768;&#36755;&#20837;&#23548;&#33268;&#32858;&#21512;&#28040;&#24687;&#30340;&#39640;&#26041;&#24046;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;TGNN&#21435;&#22122;&#25216;&#26415;&#24182;&#26410;&#32771;&#34385;&#27599;&#20010;&#33410;&#28857;&#30340;&#22810;&#26679;&#21270;&#21644;&#21160;&#24577;&#30340;&#22122;&#22768;&#27169;&#24335;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#36824;&#38754;&#20020;&#30528;&#36941;&#21382;&#26356;&#22810;&#37051;&#23621;&#23548;&#33268;&#20135;&#29983;&#36807;&#22810;&#23567;&#25209;&#37327;&#30340;&#24320;&#38144;&#12290;&#25105;&#20204;&#30456;&#20449;&#24555;&#36895;&#20934;&#30830;&#30340;TGNN&#30340;&#35299;&#20915;&#26041;&#27861;&#22312;&#20110;&#26102;&#38388;&#33258;&#36866;&#24212;&#37319;&#26679;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TASER&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#38024;&#23545;&#20934;&#30830;&#24615;&#12289;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#36827;&#34892;&#20248;&#21270;&#30340;TGNN&#33258;&#36866;&#24212;&#37319;&#26679;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Temporal Graph Neural Networks (TGNNs) have demonstrated state-of-the-art performance in various high-impact applications, including fraud detection and content recommendation. Despite the success of TGNNs, they are prone to the prevalent noise found in real-world dynamic graphs like time-deprecated links and skewed interaction distribution. The noise causes two critical issues that significantly compromise the accuracy of TGNNs: (1) models are supervised by inferior interactions, and (2) noisy input induces high variance in the aggregated messages. However, current TGNN denoising techniques do not consider the diverse and dynamic noise pattern of each node. In addition, they also suffer from the excessive mini-batch generation overheads caused by traversing more neighbors. We believe the remedy for fast and accurate TGNNs lies in temporal adaptive sampling. In this work, we propose TASER, the first adaptive sampling method for TGNNs optimized for accuracy, efficiency, and sc
&lt;/p&gt;</description></item><item><title>E(3)-&#31561;&#21464;Mesh&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#25193;&#23637;E(n)-&#31561;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26356;&#26032;&#26041;&#31243;&#20197;&#21253;&#25324;&#32593;&#26684;&#38754;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#23618;&#27425;&#21270;&#36827;&#19968;&#27493;&#25913;&#36827;&#20197;&#32771;&#34385;&#38271;&#31243;&#30456;&#20114;&#20316;&#29992;&#65292;&#23454;&#29616;&#20102;&#22312;&#32593;&#26684;&#20219;&#21153;&#19978;&#30340;&#20248;&#36234;&#24615;&#33021;&#65292;&#20855;&#26377;&#24555;&#36895;&#36816;&#34892;&#26102;&#38388;&#21644;&#26080;&#38656;&#26114;&#36149;&#39044;&#22788;&#29702;&#30340;&#29305;&#28857;&#12290;</title><link>https://arxiv.org/abs/2402.04821</link><description>&lt;p&gt;
E(3)-&#31561;&#21464;Mesh&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
E(3)-Equivariant Mesh Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04821
&lt;/p&gt;
&lt;p&gt;
E(3)-&#31561;&#21464;Mesh&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#25193;&#23637;E(n)-&#31561;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26356;&#26032;&#26041;&#31243;&#20197;&#21253;&#25324;&#32593;&#26684;&#38754;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#23618;&#27425;&#21270;&#36827;&#19968;&#27493;&#25913;&#36827;&#20197;&#32771;&#34385;&#38271;&#31243;&#30456;&#20114;&#20316;&#29992;&#65292;&#23454;&#29616;&#20102;&#22312;&#32593;&#26684;&#20219;&#21153;&#19978;&#30340;&#20248;&#36234;&#24615;&#33021;&#65292;&#20855;&#26377;&#24555;&#36895;&#36816;&#34892;&#26102;&#38388;&#21644;&#26080;&#38656;&#26114;&#36149;&#39044;&#22788;&#29702;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19977;&#35282;&#32593;&#26684;&#34987;&#24191;&#27867;&#29992;&#20110;&#34920;&#31034;&#19977;&#32500;&#29289;&#20307;&#12290;&#22240;&#27492;&#65292;&#35768;&#22810;&#26368;&#36817;&#30340;&#30740;&#31350;&#37117;&#33268;&#21147;&#20110;&#22312;3D&#32593;&#26684;&#19978;&#36827;&#34892;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#35768;&#22810;&#36825;&#20123;&#26550;&#26500;&#30340;&#22797;&#26434;&#24615;&#19982;&#23454;&#38469;&#24615;&#33021;&#20043;&#38388;&#24182;&#27809;&#26377;&#30452;&#25509;&#20851;&#32852;&#65292;&#24182;&#19988;&#22312;&#23454;&#36341;&#20013;&#65292;&#31616;&#21333;&#30340;&#28145;&#24230;&#27169;&#22411;&#23545;&#20110;&#20960;&#20309;&#22270;&#34920;&#29616;&#31454;&#20105;&#21147;&#12290;&#21463;&#21040;&#36825;&#19968;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#26368;&#23567;&#38480;&#24230;&#22320;&#25193;&#23637;&#20102;E(n)-&#31561;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;EGNNs&#65289;&#65288;Satorras&#31561;&#65292;2021&#65289;&#30340;&#26356;&#26032;&#26041;&#31243;&#65292;&#20197;&#21253;&#25324;&#32593;&#26684;&#38754;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#23618;&#27425;&#21270;&#36827;&#19968;&#27493;&#25913;&#36827;&#20102;&#35813;&#26041;&#31243;&#20197;&#32771;&#34385;&#38271;&#31243;&#30456;&#20114;&#20316;&#29992;&#12290;&#30001;&#27492;&#24471;&#21040;&#30340;&#26550;&#26500;&#65292;&#21363;&#31561;&#21464;Mesh&#31070;&#32463;&#32593;&#32476;&#65288;EMNN&#65289;&#65292;&#22312;&#32593;&#26684;&#20219;&#21153;&#19978;&#20248;&#20110;&#20854;&#20182;&#26356;&#22797;&#26434;&#30340;&#31561;&#21464;&#26041;&#27861;&#65292;&#20855;&#26377;&#24555;&#36895;&#30340;&#36816;&#34892;&#26102;&#38388;&#21644;&#26080;&#38656;&#26114;&#36149;&#30340;&#39044;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Triangular meshes are widely used to represent three-dimensional objects. As a result, many recent works have address the need for geometric deep learning on 3D mesh. However, we observe that the complexities in many of these architectures does not translate to practical performance, and simple deep models for geometric graphs are competitive in practice. Motivated by this observation, we minimally extend the update equations of E(n)-Equivariant Graph Neural Networks (EGNNs) (Satorras et al., 2021) to incorporate mesh face information, and further improve it to account for long-range interactions through hierarchy. The resulting architecture, Equivariant Mesh Neural Network (EMNN), outperforms other, more complicated equivariant methods on mesh tasks, with a fast run-time and no expensive pre-processing.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20809;HGNN&#26041;&#27861;&#65292;&#23558;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;(HGNNs)&#36716;&#21270;&#20026;Multi-Layer Perceptron (MLPs)&#20197;&#25552;&#39640;&#25512;&#26029;&#36895;&#24230;&#12290;LightHGNN&#36890;&#36807;&#36719;&#26631;&#31614;&#23558;&#30693;&#35782;&#20174;teacher HGNN&#33976;&#39311;&#21040;student MLPs&#65292;&#32780;LightHGNN$^+$&#21017;&#27880;&#20837;&#20102;&#21487;&#38752;&#30340;&#39640;&#38454;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.04296</link><description>&lt;p&gt;
&#20809;HGNN&#65306;&#23558;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#33976;&#39311;&#25104;MLPs&#65292;&#25512;&#26029;&#36895;&#24230;&#25552;&#21319;100&#20493;
&lt;/p&gt;
&lt;p&gt;
LightHGNN: Distilling Hypergraph Neural Networks into MLPs for $100\times$ Faster Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04296
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20809;HGNN&#26041;&#27861;&#65292;&#23558;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;(HGNNs)&#36716;&#21270;&#20026;Multi-Layer Perceptron (MLPs)&#20197;&#25552;&#39640;&#25512;&#26029;&#36895;&#24230;&#12290;LightHGNN&#36890;&#36807;&#36719;&#26631;&#31614;&#23558;&#30693;&#35782;&#20174;teacher HGNN&#33976;&#39311;&#21040;student MLPs&#65292;&#32780;LightHGNN$^+$&#21017;&#27880;&#20837;&#20102;&#21487;&#38752;&#30340;&#39640;&#38454;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30001;&#20110;&#20854;&#22312;&#39640;&#38454;&#30456;&#20851;&#24615;&#24314;&#27169;&#26041;&#38754;&#30340;&#20248;&#21183;&#65292;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;(HGNNs)&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#24182;&#23637;&#29616;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#27880;&#24847;&#21040;&#65292;&#36229;&#22270;&#30340;&#39640;&#38454;&#24314;&#27169;&#33021;&#21147;&#20063;&#24102;&#26469;&#20102;&#22686;&#21152;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#36825;&#38459;&#30861;&#20102;&#20854;&#22312;&#23454;&#38469;&#24037;&#19994;&#37096;&#32626;&#20013;&#30340;&#24212;&#29992;&#12290;&#23454;&#38469;&#19978;&#65292;&#25105;&#20204;&#21457;&#29616;HGNNs&#39640;&#38454;&#32467;&#26500;&#20381;&#36182;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#26159;&#39640;&#25928;&#37096;&#32626;&#30340;&#19968;&#20010;&#20851;&#38190;&#38556;&#30861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;HGNNs&#21644;&#39640;&#25928;&#25512;&#26029;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;(MLPs)&#32852;&#31995;&#36215;&#26469;&#65292;&#20197;&#28040;&#38500;HGNNs&#30340;&#36229;&#22270;&#20381;&#36182;&#24615;&#65292;&#20174;&#32780;&#38477;&#20302;&#35745;&#31639;&#22797;&#26434;&#24615;&#24182;&#25913;&#21892;&#25512;&#26029;&#36895;&#24230;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;LightHGNN&#21644;LightHGNN$^+$&#65292;&#20197;&#23454;&#29616;&#24555;&#36895;&#25512;&#26029;&#21644;&#20302;&#22797;&#26434;&#24615;&#12290;LightHGNN&#36890;&#36807;&#36719;&#26631;&#31614;&#23558;&#30693;&#35782;&#30452;&#25509;&#20174;teacher HGNN&#33976;&#39311;&#21040;student MLPs&#20013;&#65292;&#32780;LightHGNN$^+$&#21017;&#36827;&#19968;&#27493;&#26174;&#24335;&#22320;&#23558;&#21487;&#38752;&#30340;&#39640;&#38454;&#30456;&#20851;&#24615;&#27880;&#20837;&#20854;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hypergraph Neural Networks (HGNNs) have recently attracted much attention and exhibited satisfactory performance due to their superiority in high-order correlation modeling. However, it is noticed that the high-order modeling capability of hypergraph also brings increased computation complexity, which hinders its practical industrial deployment. In practice, we find that one key barrier to the efficient deployment of HGNNs is the high-order structural dependencies during inference. In this paper, we propose to bridge the gap between the HGNNs and inference-efficient Multi-Layer Perceptron (MLPs) to eliminate the hypergraph dependency of HGNNs and thus reduce computational complexity as well as improve inference speed. Specifically, we introduce LightHGNN and LightHGNN$^+$ for fast inference with low complexity. LightHGNN directly distills the knowledge from teacher HGNNs to student MLPs via soft labels, and LightHGNN$^+$ further explicitly injects reliable high-order correlations into 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22522;&#20110;&#25490;&#21015;&#30340;&#26435;&#37325;&#21305;&#37197;&#20998;&#26512;&#32447;&#24615;&#27169;&#24335;&#36830;&#25509;&#24615;&#65292;&#25105;&#20204;&#23454;&#39564;&#35777;&#26126;&#20102;&#36890;&#36807;&#26435;&#37325;&#21305;&#37197;&#25214;&#21040;&#30340;&#25490;&#21015;&#21487;&#20197;&#25913;&#21464;&#26435;&#37325;&#30697;&#38453;&#22855;&#24322;&#21521;&#37327;&#30340;&#26041;&#21521;&#65292;&#20294;&#19981;&#33021;&#25913;&#21464;&#22855;&#24322;&#20540;&#12290;&#36825;&#19968;&#21457;&#29616;&#23545;&#20110;&#29702;&#35299;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#26377;&#25928;&#24615;&#21450;&#20854;&#22312;&#27169;&#22411;&#21512;&#24182;&#31561;&#39046;&#22495;&#30340;&#24212;&#29992;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2402.04051</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#25490;&#21015;&#30340;&#26435;&#37325;&#21305;&#37197;&#20998;&#26512;&#32447;&#24615;&#27169;&#24335;&#36830;&#25509;&#24615;
&lt;/p&gt;
&lt;p&gt;
Analysis of Linear Mode Connectivity via Permutation-Based Weight Matching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04051
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#25490;&#21015;&#30340;&#26435;&#37325;&#21305;&#37197;&#20998;&#26512;&#32447;&#24615;&#27169;&#24335;&#36830;&#25509;&#24615;&#65292;&#25105;&#20204;&#23454;&#39564;&#35777;&#26126;&#20102;&#36890;&#36807;&#26435;&#37325;&#21305;&#37197;&#25214;&#21040;&#30340;&#25490;&#21015;&#21487;&#20197;&#25913;&#21464;&#26435;&#37325;&#30697;&#38453;&#22855;&#24322;&#21521;&#37327;&#30340;&#26041;&#21521;&#65292;&#20294;&#19981;&#33021;&#25913;&#21464;&#22855;&#24322;&#20540;&#12290;&#36825;&#19968;&#21457;&#29616;&#23545;&#20110;&#29702;&#35299;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#26377;&#25928;&#24615;&#21450;&#20854;&#22312;&#27169;&#22411;&#21512;&#24182;&#31561;&#39046;&#22495;&#30340;&#24212;&#29992;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;Ainsworth&#31561;&#20154;&#23637;&#31034;&#20102;&#20351;&#29992;&#26435;&#37325;&#21305;&#37197;&#65288;WM&#65289;&#26469;&#26368;&#23567;&#21270;&#25490;&#21015;&#25628;&#32034;&#27169;&#22411;&#21442;&#25968;&#20013;&#30340;$L_2$&#36317;&#31163;&#26377;&#25928;&#22320;&#35782;&#21035;&#28385;&#36275;&#32447;&#24615;&#27169;&#24335;&#36830;&#25509;&#24615;&#65288;LMC&#65289;&#30340;&#25490;&#21015;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#65292;&#22312;&#20004;&#20010;&#20855;&#26377;&#19981;&#21516;&#31181;&#23376;&#30340;&#29420;&#31435;&#35757;&#32451;&#27169;&#22411;&#20043;&#38388;&#30340;&#32447;&#24615;&#36335;&#24452;&#19978;&#30340;&#25439;&#22833;&#20445;&#25345;&#20960;&#20046;&#24658;&#23450;&#12290;&#26412;&#25991;&#36890;&#36807;WM&#25552;&#20379;&#20102;LMC&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#36825;&#23545;&#20110;&#29702;&#35299;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#26377;&#25928;&#24615;&#21450;&#20854;&#22312;&#27169;&#22411;&#21512;&#24182;&#31561;&#39046;&#22495;&#30340;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#23454;&#39564;&#21644;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;WM&#25214;&#21040;&#30340;&#25490;&#21015;&#24182;&#19981;&#26174;&#30528;&#20943;&#23569;&#20004;&#20010;&#27169;&#22411;&#20043;&#38388;&#30340;$L_2$&#36317;&#31163;&#65292;&#32780;LMC&#30340;&#20986;&#29616;&#24182;&#19981;&#20165;&#20165;&#26159;&#30001;&#20110;WM&#26412;&#36523;&#30340;&#36317;&#31163;&#20943;&#23567;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#27934;&#35265;&#65292;&#34920;&#26126;&#25490;&#21015;&#21487;&#20197;&#25913;&#21464;&#27599;&#23618;&#26435;&#37325;&#30697;&#38453;&#30340;&#22855;&#24322;&#21521;&#37327;&#30340;&#26041;&#21521;&#65292;&#20294;&#19981;&#33021;&#25913;&#21464;&#22855;&#24322;&#20540;&#12290;&#36825;&#19968;&#21457;&#29616;&#34920;&#26126;&#65292;WM&#25214;&#21040;&#30340;&#25490;&#21015;&#20027;&#35201;&#25913;&#21464;&#20102;&#26435;&#37325;&#30697;&#38453;&#30340;&#26041;&#21521;&#65292;&#32780;&#19981;&#26159;&#22855;&#24322;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Ainsworth et al. showed that using weight matching (WM) to minimize the $L_2$ distance in a permutation search of model parameters effectively identifies permutations that satisfy linear mode connectivity (LMC), in which the loss along a linear path between two independently trained models with different seeds remains nearly constant. This paper provides a theoretical analysis of LMC using WM, which is crucial for understanding stochastic gradient descent's effectiveness and its application in areas like model merging. We first experimentally and theoretically show that permutations found by WM do not significantly reduce the $L_2$ distance between two models and the occurrence of LMC is not merely due to distance reduction by WM in itself. We then provide theoretical insights showing that permutations can change the directions of the singular vectors, but not the singular values, of the weight matrices in each layer. This finding shows that permutations found by WM mainly al
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#35843;&#30740;&#20102;&#22270;&#32553;&#20943;&#26041;&#27861;&#65292;&#21253;&#25324;&#31232;&#30095;&#21270;&#12289;&#31895;&#21270;&#21644;&#27987;&#32553;&#65292;&#22312;&#35299;&#20915;&#22823;&#22411;&#22270;&#24418;&#25968;&#25454;&#20998;&#26512;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#26041;&#38754;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#35843;&#30740;&#23545;&#36825;&#20123;&#26041;&#27861;&#30340;&#25216;&#26415;&#32454;&#33410;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#22238;&#39038;&#65292;&#24182;&#24378;&#35843;&#20102;&#23427;&#20204;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#24615;&#12290;&#21516;&#26102;&#65292;&#35843;&#30740;&#36824;&#25552;&#20986;&#20102;&#20445;&#35777;&#22270;&#32553;&#20943;&#25216;&#26415;&#25345;&#32493;&#26377;&#25928;&#24615;&#30340;&#20851;&#38190;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2402.03358</link><description>&lt;p&gt;
&#22270;&#32553;&#20943;&#30340;&#32508;&#21512;&#35843;&#30740;&#65306;&#31232;&#30095;&#21270;&#12289;&#31895;&#21270;&#21644;&#27987;&#32553;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey on Graph Reduction: Sparsification, Coarsening, and Condensation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03358
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#35843;&#30740;&#20102;&#22270;&#32553;&#20943;&#26041;&#27861;&#65292;&#21253;&#25324;&#31232;&#30095;&#21270;&#12289;&#31895;&#21270;&#21644;&#27987;&#32553;&#65292;&#22312;&#35299;&#20915;&#22823;&#22411;&#22270;&#24418;&#25968;&#25454;&#20998;&#26512;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#26041;&#38754;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#35843;&#30740;&#23545;&#36825;&#20123;&#26041;&#27861;&#30340;&#25216;&#26415;&#32454;&#33410;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#22238;&#39038;&#65292;&#24182;&#24378;&#35843;&#20102;&#23427;&#20204;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#24615;&#12290;&#21516;&#26102;&#65292;&#35843;&#30740;&#36824;&#25552;&#20986;&#20102;&#20445;&#35777;&#22270;&#32553;&#20943;&#25216;&#26415;&#25345;&#32493;&#26377;&#25928;&#24615;&#30340;&#20851;&#38190;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#21487;&#20197;&#33258;&#28982;&#22320;&#34920;&#31034;&#20026;&#22270;&#65292;&#28085;&#30422;&#20102;&#24191;&#27867;&#30340;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#22270;&#25968;&#25454;&#38598;&#30340;&#22797;&#26434;&#24615;&#21644;&#35268;&#27169;&#30340;&#22686;&#21152;&#20026;&#20998;&#26512;&#21644;&#35745;&#31639;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#22270;&#32553;&#20943;&#25216;&#26415;&#22312;&#20445;&#30041;&#20851;&#38190;&#23646;&#24615;&#30340;&#21516;&#26102;&#31616;&#21270;&#22823;&#22411;&#22270;&#24418;&#25968;&#25454;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#12290;&#22312;&#26412;&#35843;&#30740;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#25552;&#20379;&#23545;&#22270;&#32553;&#20943;&#26041;&#27861;&#30340;&#20840;&#38754;&#29702;&#35299;&#65292;&#21253;&#25324;&#22270;&#31232;&#30095;&#21270;&#12289;&#22270;&#31895;&#21270;&#21644;&#22270;&#27987;&#32553;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#32479;&#19968;&#23450;&#20041;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#20998;&#23618;&#20998;&#31867;&#27861;&#26469;&#20998;&#31867;&#36825;&#20123;&#26041;&#27861;&#25152;&#35299;&#20915;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#35843;&#30740;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#25216;&#26415;&#32454;&#33410;&#65292;&#24182;&#24378;&#35843;&#20102;&#23427;&#20204;&#22312;&#21508;&#31181;&#22330;&#26223;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#27010;&#36848;&#20102;&#20445;&#35777;&#22270;&#32553;&#20943;&#25216;&#26415;&#25345;&#32493;&#26377;&#25928;&#24615;&#30340;&#20851;&#38190;&#30740;&#31350;&#26041;&#21521;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#35814;&#32454;&#30340;&#35770;&#25991;&#21015;&#34920;&#38142;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many real-world datasets can be naturally represented as graphs, spanning a wide range of domains. However, the increasing complexity and size of graph datasets present significant challenges for analysis and computation. In response, graph reduction techniques have gained prominence for simplifying large graphs while preserving essential properties. In this survey, we aim to provide a comprehensive understanding of graph reduction methods, including graph sparsification, graph coarsening, and graph condensation. Specifically, we establish a unified definition for these methods and introduce a hierarchical taxonomy to categorize the challenges they address. Our survey then systematically reviews the technical details of these methods and emphasizes their practical applications across diverse scenarios. Furthermore, we outline critical research directions to ensure the continued effectiveness of graph reduction techniques, as well as provide a comprehensive paper list at https://github.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35814;&#32454;&#20998;&#26512;&#20102;&#22522;&#20110;&#24494;&#29615;&#30340;&#30456;&#24178;&#20809;&#23398;GEMM&#21152;&#36895;&#22120;&#30340;&#32452;&#32455;&#65292;&#20854;&#36890;&#36807;&#20998;&#35010;&#12289;&#32858;&#21512;&#12289;&#35843;&#21046;&#12289;&#21152;&#26435;&#21644;&#27714;&#21644;&#31561;&#26041;&#24335;&#25805;&#20316;&#20809;&#20449;&#21495;&#20197;&#21152;&#36895;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#30697;&#38453;-&#30697;&#38453;&#20056;&#27861;&#65292;&#25552;&#39640;&#21534;&#21520;&#37327;&#21644;&#33021;&#37327;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.03149</link><description>&lt;p&gt;
&#22522;&#20110;&#24494;&#29615;&#30340;&#30456;&#24178;&#20809;&#23398;GEMM&#21152;&#36895;&#22120;&#30340;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Comparative Analysis of Microrings Based Incoherent Photonic GEMM Accelerators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03149
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35814;&#32454;&#20998;&#26512;&#20102;&#22522;&#20110;&#24494;&#29615;&#30340;&#30456;&#24178;&#20809;&#23398;GEMM&#21152;&#36895;&#22120;&#30340;&#32452;&#32455;&#65292;&#20854;&#36890;&#36807;&#20998;&#35010;&#12289;&#32858;&#21512;&#12289;&#35843;&#21046;&#12289;&#21152;&#26435;&#21644;&#27714;&#21644;&#31561;&#26041;&#24335;&#25805;&#20316;&#20809;&#20449;&#21495;&#20197;&#21152;&#36895;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#30697;&#38453;-&#30697;&#38453;&#20056;&#27861;&#65292;&#25552;&#39640;&#21534;&#21520;&#37327;&#21644;&#33021;&#37327;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20960;&#31181;&#22522;&#20110;&#24494;&#29615;&#35856;&#25391;&#22120;&#65288;MRR&#65289;&#30340;&#27169;&#25311;&#20809;&#23398;&#26550;&#26500;&#65292;&#20197;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#21152;&#36895;&#36890;&#29992;&#30340;&#30697;&#38453;-&#30697;&#38453;&#20056;&#27861;&#65288;GEMM&#65289;&#65292;&#20855;&#26377;&#20986;&#33394;&#30340;&#21534;&#21520;&#37327;&#21644;&#33021;&#37327;&#25928;&#29575;&#12290;&#20026;&#20102;&#23454;&#29616;GEMM&#21151;&#33021;&#65292;&#36825;&#20123;&#22522;&#20110;MRR&#30340;&#26550;&#26500;&#19968;&#33324;&#36890;&#36807;&#20116;&#31181;&#19981;&#21516;&#30340;&#26041;&#24335;&#25805;&#20316;&#20809;&#20449;&#21495;&#65306;&#65288;i&#65289;&#23558;&#22810;&#20010;&#20809;&#20449;&#21495;&#20998;&#35010;&#65288;&#22797;&#21046;&#65289;&#20197;&#36798;&#21040;&#26576;&#31181;&#22810;&#20998;&#25903;&#65292;&#65288;ii&#65289;&#23558;&#22810;&#20010;&#20809;&#20449;&#21495;&#32858;&#21512;&#65288;&#22797;&#29992;&#65289;&#20197;&#36798;&#21040;&#26576;&#31181;&#22810;&#36755;&#20837;&#65292;&#65288;iii&#65289;&#35843;&#21046;&#20809;&#20449;&#21495;&#20197;&#23558;&#36755;&#20837;&#20540;&#21360;&#32622;&#20110;&#27169;&#25311;&#20449;&#21495;&#24133;&#24230;&#19978;&#65292;&#65288;iv&#65289;&#23545;&#35843;&#21046;&#30340;&#20809;&#20449;&#21495;&#36827;&#34892;&#21152;&#26435;&#65292;&#20197;&#23454;&#29616;&#27169;&#25311;&#36755;&#20837;&#26435;&#37325;&#30456;&#20056;&#65292;&#65288;v&#65289;&#23545;&#20809;&#20449;&#21495;&#36827;&#34892;&#27714;&#21644;&#12290;MRR&#22522;&#20110;&#30340;GEMM&#21152;&#36895;&#22120;&#20197;&#20219;&#24847;&#39034;&#24207;&#25191;&#34892;&#21069;&#22235;&#31181;&#20449;&#21495;&#25805;&#20316;&#65292;&#24573;&#30053;&#20102;&#36825;&#20123;&#25805;&#20316;&#39034;&#24207;&#23545;&#20854;&#24615;&#33021;&#30340;&#21487;&#33021;&#24433;&#21709;&#12290;&#26412;&#25991;&#23545;&#21152;&#36895;&#22120;&#32452;&#32455;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several microring resonator (MRR) based analog photonic architectures have been proposed to accelerate general matrix-matrix multiplications (GEMMs) in deep neural networks with exceptional throughput and energy efficiency. To implement GEMM functions, these MRR-based architectures, in general, manipulate optical signals in five different ways: (i) Splitting (copying) of multiple optical signals to achieve a certain fan-out, (ii) Aggregation (multiplexing) of multiple optical signals to achieve a certain fan-in, (iii) Modulation of optical signals to imprint input values onto analog signal amplitude, (iv) Weighting of modulated optical signals to achieve analog input-weight multiplication, (v) Summation of optical signals. The MRR-based GEMM accelerators undertake the first four ways of signal manipulation in an arbitrary order ignoring the possible impact of the order of these manipulations on their performance. In this paper, we conduct a detailed analysis of accelerator organization
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25991;&#26412;&#24341;&#23548;&#30340;&#22270;&#20687;&#32858;&#31867;&#26041;&#27861;&#65292;&#20351;&#29992;&#22270;&#20687;&#23383;&#24149;&#21644;&#35270;&#35273;&#38382;&#31572;&#27169;&#22411;&#29983;&#25104;&#25991;&#26412;&#65292;&#28982;&#21518;&#23545;&#29983;&#25104;&#30340;&#25991;&#26412;&#36827;&#34892;&#32858;&#31867;&#65292;&#24182;&#36890;&#36807;&#27880;&#20837;&#20219;&#21153;&#25110;&#39046;&#22495;&#30693;&#35782;&#26469;&#25913;&#36827;&#32858;&#31867;&#32467;&#26524;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#33719;&#24471;&#30340;&#25991;&#26412;&#34920;&#31034;&#36890;&#24120;&#20248;&#20110;&#22270;&#20687;&#29305;&#24449;&#65292;&#32780;&#22522;&#20110;&#20851;&#38190;&#35789;&#30340;&#35299;&#37322;&#21487;&#20197;&#26356;&#22909;&#22320;&#25551;&#36848;&#32858;&#31867;&#12290;</title><link>https://arxiv.org/abs/2402.02996</link><description>&lt;p&gt;
&#25991;&#26412;&#24341;&#23548;&#30340;&#22270;&#20687;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Text-Guided Image Clustering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02996
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25991;&#26412;&#24341;&#23548;&#30340;&#22270;&#20687;&#32858;&#31867;&#26041;&#27861;&#65292;&#20351;&#29992;&#22270;&#20687;&#23383;&#24149;&#21644;&#35270;&#35273;&#38382;&#31572;&#27169;&#22411;&#29983;&#25104;&#25991;&#26412;&#65292;&#28982;&#21518;&#23545;&#29983;&#25104;&#30340;&#25991;&#26412;&#36827;&#34892;&#32858;&#31867;&#65292;&#24182;&#36890;&#36807;&#27880;&#20837;&#20219;&#21153;&#25110;&#39046;&#22495;&#30693;&#35782;&#26469;&#25913;&#36827;&#32858;&#31867;&#32467;&#26524;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#33719;&#24471;&#30340;&#25991;&#26412;&#34920;&#31034;&#36890;&#24120;&#20248;&#20110;&#22270;&#20687;&#29305;&#24449;&#65292;&#32780;&#22522;&#20110;&#20851;&#38190;&#35789;&#30340;&#35299;&#37322;&#21487;&#20197;&#26356;&#22909;&#22320;&#25551;&#36848;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#32858;&#31867;&#23558;&#19968;&#32452;&#22270;&#20687;&#20998;&#25104;&#26377;&#24847;&#20041;&#30340;&#32452;&#65292;&#36890;&#24120;&#36890;&#36807;&#20154;&#24037;&#32473;&#20986;&#30340;&#27880;&#37322;&#36827;&#34892;&#35299;&#37322;&#12290;&#36825;&#20123;&#27880;&#37322;&#36890;&#24120;&#20197;&#25991;&#26412;&#24418;&#24335;&#23384;&#22312;&#65292;&#24341;&#21457;&#20102;&#20351;&#29992;&#25991;&#26412;&#20316;&#20026;&#22270;&#20687;&#32858;&#31867;&#30340;&#25277;&#35937;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#22270;&#20687;&#32858;&#31867;&#26041;&#27861;&#24573;&#35270;&#20102;&#29983;&#25104;&#30340;&#25991;&#26412;&#25551;&#36848;&#30340;&#20351;&#29992;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25991;&#26412;&#24341;&#23548;&#30340;&#22270;&#20687;&#32858;&#31867;&#26041;&#27861;&#65292;&#21363;&#20351;&#29992;&#22270;&#20687;&#23383;&#24149;&#21644;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#27169;&#22411;&#29983;&#25104;&#25991;&#26412;&#65292;&#28982;&#21518;&#23545;&#29983;&#25104;&#30340;&#25991;&#26412;&#36827;&#34892;&#32858;&#31867;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#25552;&#31034;VQA&#27169;&#22411;&#26469;&#27880;&#20837;&#20219;&#21153;&#25110;&#39046;&#22495;&#30693;&#35782;&#29992;&#20110;&#32858;&#31867;&#30340;&#26032;&#26041;&#27861;&#12290;&#22312;&#20843;&#20010;&#19981;&#21516;&#30340;&#22270;&#20687;&#32858;&#31867;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#33719;&#24471;&#30340;&#25991;&#26412;&#34920;&#31034;&#36890;&#24120;&#20248;&#20110;&#22270;&#20687;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35745;&#25968;&#30340;&#32858;&#31867;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#20851;&#38190;&#35789;&#30340;&#35299;&#37322;&#27604;&#30456;&#24212;&#30340;&#32858;&#31867;&#20934;&#30830;&#24615;&#26356;&#22909;&#22320;&#25551;&#36848;&#20102;&#32858;&#31867;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;
&lt;/p&gt;
&lt;p&gt;
Image clustering divides a collection of images into meaningful groups, typically interpreted post-hoc via human-given annotations. Those are usually in the form of text, begging the question of using text as an abstraction for image clustering. Current image clustering methods, however, neglect the use of generated textual descriptions. We, therefore, propose Text-Guided Image Clustering, i.e., generating text using image captioning and visual question-answering (VQA) models and subsequently clustering the generated text. Further, we introduce a novel approach to inject task- or domain knowledge for clustering by prompting VQA models. Across eight diverse image clustering datasets, our results show that the obtained text representations often outperform image features. Additionally, we propose a counting-based cluster explainability method. Our evaluations show that the derived keyword-based explanations describe clusters better than the respective cluster accuracy suggests. Overall, 
&lt;/p&gt;</description></item><item><title>TopoX&#26159;&#19968;&#20010;&#29992;&#20110;&#22312;&#25299;&#25169;&#22495;&#19978;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#30340;Python&#36719;&#20214;&#21253;&#22871;&#20214;&#65292;&#21253;&#21547;&#20102;&#26500;&#24314;&#12289;&#35745;&#31639;&#21644;&#23884;&#20837;&#25299;&#25169;&#22495;&#30340;&#21151;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#22871;&#20840;&#38754;&#30340;&#39640;&#38454;&#28040;&#24687;&#20256;&#36882;&#21151;&#33021;&#24037;&#20855;&#31665;&#12290;</title><link>https://arxiv.org/abs/2402.02441</link><description>&lt;p&gt;
TopoX: &#19968;&#20010;&#29992;&#20110;&#25299;&#25169;&#22495;&#19978;&#30340;&#26426;&#22120;&#23398;&#20064;&#30340;Python&#36719;&#20214;&#21253;&#22871;&#20214;
&lt;/p&gt;
&lt;p&gt;
TopoX: A Suite of Python Packages for Machine Learning on Topological Domains
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02441
&lt;/p&gt;
&lt;p&gt;
TopoX&#26159;&#19968;&#20010;&#29992;&#20110;&#22312;&#25299;&#25169;&#22495;&#19978;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#30340;Python&#36719;&#20214;&#21253;&#22871;&#20214;&#65292;&#21253;&#21547;&#20102;&#26500;&#24314;&#12289;&#35745;&#31639;&#21644;&#23884;&#20837;&#25299;&#25169;&#22495;&#30340;&#21151;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#22871;&#20840;&#38754;&#30340;&#39640;&#38454;&#28040;&#24687;&#20256;&#36882;&#21151;&#33021;&#24037;&#20855;&#31665;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;topox&#65292;&#19968;&#20010;&#25552;&#20379;&#21487;&#38752;&#19988;&#29992;&#25143;&#21451;&#22909;&#30340;Python&#36719;&#20214;&#21253;&#22871;&#20214;&#65292;&#29992;&#20110;&#22312;&#25299;&#25169;&#22495;&#65288;&#25193;&#23637;&#20102;&#22270;&#30340;&#39046;&#22495;&#65289;&#19978;&#36827;&#34892;&#35745;&#31639;&#21644;&#26426;&#22120;&#23398;&#20064;&#65306;&#36229;&#22270;&#12289;&#21333;&#32431;&#12289;&#32990;&#33108;&#12289;&#36335;&#24452;&#21644;&#32452;&#21512;&#22797;&#21512;&#20307;&#12290;topox&#30001;&#19977;&#20010;&#36719;&#20214;&#21253;&#32452;&#25104;&#65306;toponetx&#29992;&#20110;&#26500;&#24314;&#21644;&#35745;&#31639;&#36825;&#20123;&#22495;&#65292;&#21253;&#25324;&#33410;&#28857;&#12289;&#36793;&#21644;&#39640;&#38454;&#21333;&#20803;&#30340;&#22788;&#29702;&#65307;topoembedx&#25552;&#20379;&#20102;&#23558;&#25299;&#25169;&#22495;&#23884;&#20837;&#21040;&#21521;&#37327;&#31354;&#38388;&#30340;&#26041;&#27861;&#65292;&#31867;&#20284;&#20110;&#27969;&#34892;&#30340;&#22522;&#20110;&#22270;&#30340;&#23884;&#20837;&#31639;&#27861;&#65292;&#22914;node2vec&#65307;topomodelx&#24314;&#31435;&#22312;PyTorch&#20043;&#19978;&#65292;&#20026;&#25299;&#25169;&#22495;&#19978;&#30340;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#20102;&#19968;&#22871;&#20840;&#38754;&#30340;&#39640;&#38454;&#28040;&#24687;&#20256;&#36882;&#21151;&#33021;&#24037;&#20855;&#31665;&#12290;topox&#30340;&#28304;&#20195;&#30721;&#32463;&#36807;&#24191;&#27867;&#30340;&#25991;&#26723;&#21270;&#21644;&#21333;&#20803;&#27979;&#35797;&#65292;&#24182;&#22312;https://github.com/pyt-team&#20197;MIT&#35768;&#21487;&#35777;&#30340;&#24418;&#24335;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce topox, a Python software suite that provides reliable and user-friendly building blocks for computing and machine learning on topological domains that extend graphs: hypergraphs, simplicial, cellular, path and combinatorial complexes. topox consists of three packages: toponetx facilitates constructing and computing on these domains, including working with nodes, edges and higher-order cells; topoembedx provides methods to embed topological domains into vector spaces, akin to popular graph-based embedding algorithms such as node2vec; topomodelx is built on top of PyTorch and offers a comprehensive toolbox of higher-order message passing functions for neural networks on topological domains. The extensively documented and unit-tested source code of topox is available under MIT license at https://github.com/pyt-team.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28508;&#22312;IPS&#65288;LIPS&#65289;&#30340;&#26032;&#30340;Slate Bandit OPE&#20272;&#35745;&#22120;&#65292;&#36890;&#36807;&#22312;&#20302;&#32500;&#24230;&#30340;Slate&#25277;&#35937;&#31354;&#38388;&#20013;&#23450;&#20041;&#37325;&#35201;&#24615;&#26435;&#37325;&#65292;&#24182;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#20248;&#21270;Slate&#25277;&#35937;&#26469;&#20943;&#23567;&#20559;&#24046;&#21644;&#26041;&#24046;&#12290;</title><link>https://arxiv.org/abs/2402.02171</link><description>&lt;p&gt;
&#36890;&#36807;&#20248;&#21270;&#25277;&#35937;&#30340;&#26041;&#24335;&#36827;&#34892;Slate Bandit&#31574;&#30053;&#30340;&#31163;&#31574;&#30053;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Off-Policy Evaluation of Slate Bandit Policies via Optimizing Abstraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02171
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28508;&#22312;IPS&#65288;LIPS&#65289;&#30340;&#26032;&#30340;Slate Bandit OPE&#20272;&#35745;&#22120;&#65292;&#36890;&#36807;&#22312;&#20302;&#32500;&#24230;&#30340;Slate&#25277;&#35937;&#31354;&#38388;&#20013;&#23450;&#20041;&#37325;&#35201;&#24615;&#26435;&#37325;&#65292;&#24182;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#20248;&#21270;Slate&#25277;&#35937;&#26469;&#20943;&#23567;&#20559;&#24046;&#21644;&#26041;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;Slate&#19978;&#19979;&#25991;&#24378;&#30423;&#38382;&#39064;&#20013;&#30340;&#31163;&#31574;&#30053;&#35780;&#20272;&#65288;OPE&#65289;&#65292;&#20854;&#20013;&#19968;&#20010;&#31574;&#30053;&#36873;&#25321;&#31216;&#20026;slates&#30340;&#22810;&#32500;&#21160;&#20316;&#12290;&#36825;&#20010;&#38382;&#39064;&#22312;&#25512;&#33616;&#31995;&#32479;&#12289;&#25628;&#32034;&#24341;&#25806;&#12289;&#33829;&#38144;&#20197;&#21450;&#21307;&#30103;&#24212;&#29992;&#20013;&#24191;&#27867;&#23384;&#22312;&#65292;&#28982;&#32780;&#65292;&#30001;&#20110;&#21160;&#20316;&#31354;&#38388;&#22823;&#65292;&#20856;&#22411;&#30340;&#36870;&#20542;&#21521;&#35780;&#20998;&#65288;IPS&#65289;&#20272;&#35745;&#22120;&#23384;&#22312;&#36739;&#22823;&#30340;&#26041;&#24046;&#65292;&#20351;&#24471;&#26377;&#25928;&#30340;OPE&#25104;&#20026;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#20266;&#36870;&#65288;PI&#65289;&#20272;&#35745;&#22120;&#24050;&#34987;&#24341;&#20837;&#20197;&#20943;&#23567;&#26041;&#24046;&#38382;&#39064;&#65292;&#36890;&#36807;&#20551;&#35774;&#22870;&#21169;&#20989;&#25968;&#32447;&#24615;&#65292;&#20294;&#36825;&#21487;&#33021;&#23548;&#33268;&#26174;&#33879;&#30340;&#20559;&#24046;&#65292;&#22240;&#20026;&#36825;&#20010;&#20551;&#35774;&#22312;&#35266;&#27979;&#25968;&#25454;&#20013;&#24456;&#38590;&#39564;&#35777;&#24182;&#19988;&#32463;&#24120;&#20250;&#34987;&#23454;&#36136;&#24615;&#36829;&#21453;&#12290;&#20026;&#20102;&#35299;&#20915;&#20043;&#21069;&#20272;&#35745;&#22120;&#30340;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;Slate Bandit OPE&#20272;&#35745;&#22120;&#65292;&#31216;&#20026;&#28508;&#22312;IPS&#65288;LIPS&#65289;&#65292;&#23427;&#22312;&#20302;&#32500;&#24230;&#30340;Slate&#25277;&#35937;&#31354;&#38388;&#20013;&#23450;&#20041;&#20102;&#37325;&#35201;&#24615;&#26435;&#37325;&#65292;&#25105;&#20204;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#20248;&#21270;Slate&#25277;&#35937;&#26469;&#26368;&#23567;&#21270;LIPS&#30340;&#20559;&#24046;&#21644;&#26041;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study off-policy evaluation (OPE) in the problem of slate contextual bandits where a policy selects multi-dimensional actions known as slates. This problem is widespread in recommender systems, search engines, marketing, to medical applications, however, the typical Inverse Propensity Scoring (IPS) estimator suffers from substantial variance due to large action spaces, making effective OPE a significant challenge. The PseudoInverse (PI) estimator has been introduced to mitigate the variance issue by assuming linearity in the reward function, but this can result in significant bias as this assumption is hard-to-verify from observed data and is often substantially violated. To address the limitations of previous estimators, we develop a novel estimator for OPE of slate bandits, called Latent IPS (LIPS), which defines importance weights in a low-dimensional slate abstraction space where we optimize slate abstractions to minimize the bias and variance of LIPS in a data-driven way. By do
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#29983;&#25104;&#20195;&#30721;&#30340;&#36136;&#37327;&#21644;&#20449;&#20219;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#26657;&#20934;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#30830;&#23450;&#27169;&#22411;&#29983;&#25104;&#20195;&#30721;&#30340;&#27491;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02047</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#29983;&#25104;&#20195;&#30721;&#30340;&#36136;&#37327;&#21644;&#20449;&#20219;
&lt;/p&gt;
&lt;p&gt;
Quality and Trust in LLM-generated Code
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02047
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#29983;&#25104;&#20195;&#30721;&#30340;&#36136;&#37327;&#21644;&#20449;&#20219;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#26657;&#20934;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#30830;&#23450;&#27169;&#22411;&#29983;&#25104;&#20195;&#30721;&#30340;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#24120;&#24120;&#20250;&#20986;&#38169;&#12290;&#29992;&#25143;&#38656;&#35201;&#21487;&#38752;&#30340;&#25351;&#31034;&#65292;&#20197;&#30830;&#23450;&#32473;&#23450;&#27169;&#22411;&#30340;&#36755;&#20986;&#26159;&#21542;&#21487;&#20449;&#65292;&#20174;&#32780;&#21487;&#20197;&#20570;&#20986;&#29702;&#24615;&#20915;&#31574;&#26159;&#21542;&#20351;&#29992;&#35813;&#36755;&#20986;&#12290;&#20363;&#22914;&#65292;&#21487;&#20197;&#23558;&#36755;&#20986;&#19982;&#32622;&#20449;&#24230;&#30456;&#20851;&#32852;&#65307;&#22914;&#26524;&#32622;&#20449;&#24230;&#19982;&#27491;&#30830;&#24615;&#30340;&#21487;&#33021;&#24615;&#24378;&#30456;&#20851;&#65292;&#21017;&#31216;&#35813;&#27169;&#22411;&#20026;&#33391;&#22909;&#26657;&#20934;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#39640;&#32622;&#20449;&#24230;&#30340;&#36755;&#20986;&#21487;&#20197;&#23433;&#20840;&#25509;&#21463;&#65292;&#20302;&#32622;&#20449;&#24230;&#30340;&#36755;&#20986;&#21487;&#20197;&#25298;&#32477;&#12290;&#26657;&#20934;&#36804;&#20170;&#20027;&#35201;&#22312;&#38750;&#29983;&#25104;&#24615;&#65288;&#20363;&#22914;&#20998;&#31867;&#65289;&#29615;&#22659;&#20013;&#36827;&#34892;&#30740;&#31350;&#65292;&#29305;&#21035;&#26159;&#22312;&#36719;&#20214;&#24037;&#31243;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#20195;&#30721;&#24456;&#23481;&#26131;&#20986;&#38169;&#65306;&#24320;&#21457;&#20154;&#21592;&#38656;&#35201;&#30693;&#36947;&#20309;&#26102;&#30452;&#25509;&#20351;&#29992;&#12289;&#32463;&#36807;&#20180;&#32454;&#23457;&#26597;&#21518;&#20351;&#29992;&#25110;&#20002;&#24323;&#27169;&#22411;&#29983;&#25104;&#30340;&#20195;&#30721;&#65292;&#22240;&#27492;&#22312;&#29983;&#25104;&#29615;&#22659;&#20013;&#65292;&#26657;&#20934;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#20195;&#30721;&#30340;&#27491;&#30830;&#24615;&#27010;&#24565;&#24182;&#19981;&#31616;&#21333;&#65292;&#22240;&#27492;&#26657;&#20934;&#20063;&#26159;&#22914;&#27492;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models are widely used but can also often be wrong. Users would benefit from a reliable indication of whether a given output from a given model should be trusted, so a rational decision can be made whether to use the output or not. For example, outputs can be associated with a confidence measure; if this confidence measure is strongly associated with likelihood of correctness, then the model is said to be well-calibrated. In this case, for example, high-confidence outputs could be safely accepted, and low-confidence outputs rejected.   Calibration has so far been studied in non-generative (e.g., classification) settings, especially in Software Engineering. However, generated code can quite often be wrong: Developers need to know when they should e.g., directly use, use after careful review, or discard model-generated code; thus Calibration is vital in generative settings. However, the notion of correctness of generated code is non-trivial, and thus so is Calibration. I
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24212;&#29992;&#20110;&#31070;&#32463;&#23494;&#24230;&#27604;&#20272;&#35745;&#30340;$\alpha$-&#25955;&#24230;&#25439;&#22833;&#20989;&#25968;($\alpha$-Div)&#65292;&#36890;&#36807;&#31616;&#27905;&#23454;&#29616;&#21644;&#31283;&#23450;&#20248;&#21270;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#25439;&#22833;&#20989;&#25968;&#30340;&#31283;&#23450;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;DRE&#20219;&#21153;&#30340;&#20272;&#35745;&#20934;&#30830;&#24615;&#30340;&#30740;&#31350;&#65292;&#21516;&#26102;&#32473;&#20986;&#20102;&#26679;&#26412;&#35201;&#27714;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.02041</link><description>&lt;p&gt;
&#29992;&#20110;&#31070;&#32463;&#23494;&#24230;&#27604;&#20272;&#35745;&#30340;$\alpha$-&#25955;&#24230;&#25439;&#22833;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
$\alpha$-Divergence Loss Function for Neural Density Ratio Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02041
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24212;&#29992;&#20110;&#31070;&#32463;&#23494;&#24230;&#27604;&#20272;&#35745;&#30340;$\alpha$-&#25955;&#24230;&#25439;&#22833;&#20989;&#25968;($\alpha$-Div)&#65292;&#36890;&#36807;&#31616;&#27905;&#23454;&#29616;&#21644;&#31283;&#23450;&#20248;&#21270;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#25439;&#22833;&#20989;&#25968;&#30340;&#31283;&#23450;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;DRE&#20219;&#21153;&#30340;&#20272;&#35745;&#20934;&#30830;&#24615;&#30340;&#30740;&#31350;&#65292;&#21516;&#26102;&#32473;&#20986;&#20102;&#26679;&#26412;&#35201;&#27714;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#31070;&#32463;&#32593;&#32476;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#22522;&#30784;&#25216;&#26415;&#23494;&#24230;&#27604;&#20272;&#35745;(DRE)&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#22240;DRE&#30340;&#25439;&#22833;&#20989;&#25968;&#32780;&#20986;&#29616;&#20102;&#20248;&#21270;&#38382;&#39064;&#65306;KL&#25955;&#24230;&#38656;&#35201;&#22823;&#26679;&#26412;&#65292;&#35757;&#32451;&#25439;&#22833;&#26799;&#24230;&#28040;&#22833;&#65292;&#25439;&#22833;&#20989;&#25968;&#26799;&#24230;&#26377;&#20559;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25552;&#20379;&#31616;&#27905;&#23454;&#29616;&#21644;&#31283;&#23450;&#20248;&#21270;&#30340;$\alpha$-&#25955;&#24230;&#25439;&#22833;&#20989;&#25968;($\alpha$-Div)&#12290;&#27492;&#22806;&#65292;&#36824;&#32473;&#20986;&#20102;&#23545;&#25152;&#25552;&#20986;&#30340;&#25439;&#22833;&#20989;&#25968;&#30340;&#25216;&#26415;&#39564;&#35777;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#25439;&#22833;&#20989;&#25968;&#30340;&#31283;&#23450;&#24615;&#65292;&#24182;&#30740;&#31350;&#20102;DRE&#20219;&#21153;&#30340;&#20272;&#35745;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#36824;&#25552;&#20986;&#20102;&#20351;&#29992;&#25152;&#25552;&#20986;&#30340;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;DRE&#30340;&#26679;&#26412;&#35201;&#27714;&#65292;&#20197;$L_1$&#35823;&#24046;&#30340;&#19978;&#30028;&#32852;&#31995;&#36215;&#26469;&#65292;&#35813;&#19978;&#30028;&#23558;&#39640;&#32500;&#24230;DRE&#20219;&#21153;&#20013;&#30340;&#32500;&#24230;&#35781;&#21650;&#20316;&#20026;&#19968;&#20010;&#20849;&#21516;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, neural networks have produced state-of-the-art results for density-ratio estimation (DRE), a fundamental technique in machine learning. However, existing methods bear optimization issues that arise from the loss functions of DRE: a large sample requirement of Kullback--Leibler (KL)-divergence, vanishing of train loss gradients, and biased gradients of the loss functions. Thus, an $\alpha$-divergence loss function ($\alpha$-Div) that offers concise implementation and stable optimization is proposed in this paper. Furthermore, technical justifications for the proposed loss function are presented. The stability of the proposed loss function is empirically demonstrated and the estimation accuracy of DRE tasks is investigated. Additionally, this study presents a sample requirement for DRE using the proposed loss function in terms of the upper bound of $L_1$ error, which connects a curse of dimensionality as a common problem in high-dimensional DRE tasks.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25972;&#21512;&#36827;&#21270;&#31639;&#27861;&#19982;&#24378;&#21270;&#23398;&#20064;&#65292;&#36827;&#21270;&#24378;&#21270;&#23398;&#20064;&#65288;ERL&#65289;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#26412;&#32508;&#36848;&#21576;&#29616;&#20102;ERL&#39046;&#22495;&#30340;&#21508;&#20010;&#30740;&#31350;&#20998;&#25903;&#65292;&#31361;&#20986;&#20102;EA&#36741;&#21161;RL&#30340;&#20248;&#21270;&#12289;RL&#36741;&#21161;EA&#30340;&#20248;&#21270;&#20197;&#21450;EA&#21644;RL&#30340;&#21327;&#21516;&#20248;&#21270;&#36825;&#19977;&#20010;&#20027;&#35201;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2401.11963</link><description>&lt;p&gt;
&#36328;&#36234;&#36827;&#21270;&#31639;&#27861;&#21644;&#24378;&#21270;&#23398;&#20064;&#65306;&#19968;&#39033;&#20840;&#38754;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Bridging Evolutionary Algorithms and Reinforcement Learning: A Comprehensive Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.11963
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25972;&#21512;&#36827;&#21270;&#31639;&#27861;&#19982;&#24378;&#21270;&#23398;&#20064;&#65292;&#36827;&#21270;&#24378;&#21270;&#23398;&#20064;&#65288;ERL&#65289;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#26412;&#32508;&#36848;&#21576;&#29616;&#20102;ERL&#39046;&#22495;&#30340;&#21508;&#20010;&#30740;&#31350;&#20998;&#25903;&#65292;&#31361;&#20986;&#20102;EA&#36741;&#21161;RL&#30340;&#20248;&#21270;&#12289;RL&#36741;&#21161;EA&#30340;&#20248;&#21270;&#20197;&#21450;EA&#21644;RL&#30340;&#21327;&#21516;&#20248;&#21270;&#36825;&#19977;&#20010;&#20027;&#35201;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36827;&#21270;&#24378;&#21270;&#23398;&#20064;&#65288;ERL&#65289;&#23558;&#36827;&#21270;&#31639;&#27861;&#65288;EAs&#65289;&#21644;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30456;&#32467;&#21512;&#36827;&#34892;&#20248;&#21270;&#65292;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#36890;&#36807;&#34701;&#21512;&#20004;&#31181;&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;ERL&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#26412;&#35843;&#26597;&#32508;&#36848;&#20102;ERL&#20013;&#19981;&#21516;&#30740;&#31350;&#20998;&#25903;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#31995;&#32479;&#24635;&#32467;&#20102;&#30456;&#20851;&#31639;&#27861;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#30830;&#23450;&#20102;&#19977;&#20010;&#20027;&#35201;&#30740;&#31350;&#26041;&#21521;&#65306;EA&#36741;&#21161;RL&#30340;&#20248;&#21270;&#65292;RL&#36741;&#21161;EA&#30340;&#20248;&#21270;&#65292;&#20197;&#21450;EA&#21644;RL&#30340;&#21327;&#21516;&#20248;&#21270;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#28145;&#20837;&#20998;&#26512;&#20102;&#27599;&#20010;&#30740;&#31350;&#26041;&#21521;&#65292;&#32452;&#32455;&#20102;&#22810;&#20010;&#30740;&#31350;&#20998;&#25903;&#12290;&#25105;&#20204;&#38416;&#26126;&#20102;&#27599;&#20010;&#20998;&#25903;&#33268;&#21147;&#20110;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;&#20197;&#21450;EA&#21644;RL&#30340;&#25972;&#21512;&#22914;&#20309;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#28508;&#22312;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.11963v2 Announce Type: replace-cross  Abstract: Evolutionary Reinforcement Learning (ERL), which integrates Evolutionary Algorithms (EAs) and Reinforcement Learning (RL) for optimization, has demonstrated remarkable performance advancements. By fusing the strengths of both approaches, ERL has emerged as a promising research direction. This survey offers a comprehensive overview of the diverse research branches in ERL. Specifically, we systematically summarize recent advancements in relevant algorithms and identify three primary research directions: EA-assisted optimization of RL, RL-assisted optimization of EA, and synergistic optimization of EA and RL. Following that, we conduct an in-depth analysis of each research direction, organizing multiple research branches. We elucidate the problems that each branch aims to tackle and how the integration of EA and RL addresses these challenges. In conclusion, we discuss potential challenges and prospective future research directions
&lt;/p&gt;</description></item><item><title>LightDiC&#26159;&#22522;&#20110;&#30913;&#24615;&#25289;&#26222;&#25289;&#26031;&#30340;&#21487;&#25193;&#23637;&#26377;&#21521;&#22270;&#21367;&#31215;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#31163;&#32447;&#39044;&#22788;&#29702;&#20013;&#36827;&#34892;&#25299;&#25169;&#30456;&#20851;&#35745;&#31639;&#65292;&#23454;&#29616;&#20102;&#21487;&#25193;&#23637;&#24615;&#65292;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#24211;&#12290;</title><link>https://arxiv.org/abs/2401.11772</link><description>&lt;p&gt;
LightDiC: &#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22823;&#35268;&#27169;&#26377;&#21521;&#22270;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
LightDiC: A Simple yet Effective Approach for Large-scale Digraph Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.11772
&lt;/p&gt;
&lt;p&gt;
LightDiC&#26159;&#22522;&#20110;&#30913;&#24615;&#25289;&#26222;&#25289;&#26031;&#30340;&#21487;&#25193;&#23637;&#26377;&#21521;&#22270;&#21367;&#31215;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#31163;&#32447;&#39044;&#22788;&#29702;&#20013;&#36827;&#34892;&#25299;&#25169;&#30456;&#20851;&#35745;&#31639;&#65292;&#23454;&#29616;&#20102;&#21487;&#25193;&#23637;&#24615;&#65292;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#23616;&#38480;&#20110;&#26080;&#21521;&#22270;&#65292;&#20854;&#25429;&#25417;&#20851;&#31995;&#20449;&#24687;&#30340;&#33539;&#22260;&#21463;&#38480;&#21046;&#65292;&#38480;&#21046;&#20102;&#20854;&#34920;&#36798;&#33021;&#21147;&#21644;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#37096;&#32626;&#12290; &#30456;&#36739;&#20110;&#26080;&#21521;&#22270;&#65292;&#26377;&#21521;&#22270;&#65288;&#26377;&#21521;&#22270;&#65289;&#26356;&#36866;&#21512;&#24314;&#27169;&#26356;&#22797;&#26434;&#30340;&#25299;&#25169;&#31995;&#32479;&#30340;&#38656;&#27714;&#65292;&#36890;&#36807;&#25429;&#25417;&#33410;&#28857;&#20043;&#38388;&#26356;&#22797;&#26434;&#30340;&#20851;&#31995;&#65292;&#22914;&#21046;&#23450;&#20132;&#36890;&#21644;&#37329;&#34701;&#32593;&#32476;&#12290; &#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#26377;&#21521;GNN&#65292;&#20294;&#23427;&#20204;&#30340;&#28789;&#24863;&#20027;&#35201;&#26469;&#33258;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#36825;&#23548;&#33268;&#20102;&#20887;&#20313;&#30340;&#22797;&#26434;&#24615;&#21644;&#35745;&#31639;&#37327;&#65292;&#20351;&#20854;&#26080;&#27861;&#24212;&#29992;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#24211;&#12290; &#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30913;&#24615;&#25289;&#26222;&#25289;&#26031;&#30340;&#21487;&#25193;&#23637;&#21464;&#31181;&#26377;&#21521;&#22270;&#21367;&#31215;&#65292;LightDiC&#12290; &#30001;&#20110;&#25299;&#25169;&#30456;&#20851;&#30340;&#35745;&#31639;&#20165;&#22312;&#31163;&#32447;&#39044;&#22788;&#29702;&#36807;&#31243;&#20013;&#36827;&#34892;&#65292;LightDiC&#23454;&#29616;&#20102;&#20986;&#33394;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#21521;&#19979;&#30340;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.11772v2 Announce Type: replace-cross  Abstract: Most existing graph neural networks (GNNs) are limited to undirected graphs, whose restricted scope of the captured relational information hinders their expressive capabilities and deployments in real-world scenarios. Compared with undirected graphs, directed graphs (digraphs) fit the demand for modeling more complex topological systems by capturing more intricate relationships between nodes, such as formulating transportation and financial networks. While some directed GNNs have been introduced, their inspiration mainly comes from deep learning architectures, which lead to redundant complexity and computation, making them inapplicable to large-scale databases. To address these issues, we propose LightDiC, a scalable variant of the digraph convolution based on the magnetic Laplacian. Since topology-related computations are conducted solely during offline pre-processing, LightDiC achieves exceptional scalability, enabling downst
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#21160;&#21270;&#24037;&#20855;PRewrite&#65292;&#29992;&#20110;&#37325;&#20889;&#25552;&#31034;&#33609;&#26696;&#24182;&#29983;&#25104;&#39640;&#25928;&#30340;&#26032;&#25552;&#31034;&#65292;&#20197;&#35299;&#20915;&#25552;&#31034;&#24037;&#31243;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2401.08189</link><description>&lt;p&gt;
PRewrite: &#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#25552;&#31034;&#37325;&#20889;
&lt;/p&gt;
&lt;p&gt;
PRewrite: Prompt Rewriting with Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.08189
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#21160;&#21270;&#24037;&#20855;PRewrite&#65292;&#29992;&#20110;&#37325;&#20889;&#25552;&#31034;&#33609;&#26696;&#24182;&#29983;&#25104;&#39640;&#25928;&#30340;&#26032;&#25552;&#31034;&#65292;&#20197;&#35299;&#20915;&#25552;&#31034;&#24037;&#31243;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2401.08189v2 &#20844;&#21578;&#31867;&#22411;: &#26367;&#25442; &#25688;&#35201;: &#25552;&#31034;&#24037;&#31243;&#23545;&#20110;&#22522;&#20110;LLM&#30340;&#24212;&#29992;&#31243;&#24207;&#30340;&#24320;&#21457;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#36890;&#24120;&#20197;&#8220;&#35797;&#38169;&#8221;&#30340;&#26041;&#24335;&#25163;&#21160;&#23436;&#25104;&#12290;&#36825;&#31181;&#25163;&#21160;&#31243;&#24207;&#21487;&#33021;&#32791;&#26102;&#65292;&#25928;&#26524;&#19981;&#20339;&#65292;&#24182;&#19988;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#29983;&#25104;&#30340;&#25552;&#31034;&#37117;&#26159;&#27425;&#20248;&#30340;&#12290;&#21363;&#20351;&#23545;&#37027;&#20123;&#30475;&#20284;&#36816;&#20316;&#33391;&#22909;&#30340;&#25552;&#31034;&#65292;&#22987;&#32456;&#23384;&#22312;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#65306;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#36827;&#19968;&#27493;&#20462;&#25913;&#20351;&#25552;&#31034;&#21464;&#24471;&#26356;&#22909;&#21602;&#65311;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#25552;&#31034;&#24037;&#31243;&#33258;&#21160;&#21270;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#29305;&#23450;&#30340;&#20351;&#29992;&#24773;&#26223;&#65292;&#21363;&#24320;&#21457;&#32773;/&#29992;&#25143;&#24050;&#32463;&#36215;&#33609;&#20102;&#21021;&#22987;&#25552;&#31034;&#65292;&#20294;&#32570;&#20047;&#26102;&#38388;/&#19987;&#19994;&#30693;&#35782;&#26469;&#20248;&#21270;&#23427;&#20204;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;PRewrite&#65292;&#19968;&#20010;&#33258;&#21160;&#21270;&#24037;&#20855;&#65292;&#21487;&#37325;&#20889;&#36825;&#20123;&#33609;&#26696;&#65292;&#24182;&#29983;&#25104;&#39640;&#25928;&#30340;&#26032;&#25552;&#31034;&#12290;PRewrite&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26694;&#26550;&#65292;&#20801;&#35768;&#31471;&#21040;&#31471;&#20248;&#21270;&#65292;&#25105;&#20204;&#30340;&#35774;&#35745;&#20801;&#35768;RL&#25628;&#32034;&#22312;&#22823;&#21160;&#20316;&#31354;&#38388;&#20013;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.08189v2 Announce Type: replace  Abstract: Prompt engineering is critical for the development of LLM-based applications. However, it is usually done manually in a "trial and error" fashion. This manual procedure can be time consuming, ineffective, and the generated prompts are, in a lot of cases, sub-optimal. Even for the prompts which seemingly work well, there is always a lingering question: can the prompts be made better with further modifications?   To address these questions, in this paper, we investigate prompt engineering automation. We consider a specific use case scenario in which developers/users have drafted initial prompts, but lack the time/expertise to optimize them. We propose PRewrite, an automated tool to rewrite these drafts and to generate highly effective new prompts. PRewrite is based on the Reinforcement Learning (RL) framework which allows for end-to-end optimization and our design allows the RL search to happen in a large action space. The automated to
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#30740;&#31350;&#20102;&#23558;&#26426;&#22120;&#23398;&#20064;&#21644;&#26412;&#20307;&#25972;&#21512;&#30340;&#36807;&#31243;&#65292;&#21457;&#29616;&#20102;&#22686;&#24378;&#22411;&#26412;&#20307;&#23398;&#20064;&#12289;&#35821;&#20041;&#25968;&#25454;&#25366;&#25496;&#21644;&#23398;&#20064;&#19982;&#25512;&#29702;&#31995;&#32479;&#36825;&#19977;&#31181;&#20027;&#35201;&#30340;&#28151;&#21512;&#31867;&#21035;&#65292;&#27604;&#36739;&#20102;&#19981;&#21516;&#30740;&#31350;&#20013;&#20351;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2401.07744</link><description>&lt;p&gt;
&#32467;&#21512;&#26426;&#22120;&#23398;&#20064;&#21644;&#26412;&#20307;&#35770;&#65306;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Combining Machine Learning and Ontology: A Systematic Literature Review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.07744
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#30740;&#31350;&#20102;&#23558;&#26426;&#22120;&#23398;&#20064;&#21644;&#26412;&#20307;&#25972;&#21512;&#30340;&#36807;&#31243;&#65292;&#21457;&#29616;&#20102;&#22686;&#24378;&#22411;&#26412;&#20307;&#23398;&#20064;&#12289;&#35821;&#20041;&#25968;&#25454;&#25366;&#25496;&#21644;&#23398;&#20064;&#19982;&#25512;&#29702;&#31995;&#32479;&#36825;&#19977;&#31181;&#20027;&#35201;&#30340;&#28151;&#21512;&#31867;&#21035;&#65292;&#27604;&#36739;&#20102;&#19981;&#21516;&#30740;&#31350;&#20013;&#20351;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#23545;&#23558;&#24402;&#32435;&#25512;&#29702;&#21644;&#28436;&#32462;&#25512;&#29702;&#32467;&#21512;&#30340;&#25506;&#32034;&#36807;&#31243;&#30340;&#28212;&#26395;&#39537;&#20351;&#65292;&#25105;&#20204;&#23545;&#35843;&#26597;&#26426;&#22120;&#23398;&#20064;&#21644;&#26412;&#20307;&#19968;&#20307;&#21270;&#30340;&#25991;&#31456;&#36827;&#34892;&#20102;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#12290;&#20854;&#30446;&#26631;&#26159;&#35782;&#21035;&#28085;&#30422;&#24402;&#32435;&#25512;&#29702;&#65288;&#30001;&#26426;&#22120;&#23398;&#20064;&#25191;&#34892;&#65289;&#21644;&#28436;&#32462;&#25512;&#29702;&#65288;&#30001;&#26412;&#20307;&#25191;&#34892;&#65289;&#30340;&#22810;&#31181;&#25216;&#26415;&#20197;&#25972;&#21512;&#21040;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#12290;&#25105;&#20204;&#30340;&#32508;&#36848;&#21253;&#25324;&#23545;128&#39033;&#30740;&#31350;&#30340;&#20998;&#26512;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#30830;&#23450;&#26426;&#22120;&#23398;&#20064;&#21644;&#26412;&#20307;&#20043;&#38388;&#30340;&#19977;&#20010;&#20027;&#35201;&#28151;&#21512;&#31867;&#21035;&#65306;&#22686;&#24378;&#22411;&#26412;&#20307;&#23398;&#20064;&#12289;&#35821;&#20041;&#25968;&#25454;&#25366;&#25496;&#20197;&#21450;&#23398;&#20064;&#19982;&#25512;&#29702;&#31995;&#32479;&#12290;&#25105;&#20204;&#23545;&#25152;&#26377;&#36825;&#20123;&#31867;&#21035;&#36827;&#34892;&#20102;&#20840;&#38754;&#26816;&#26597;&#65292;&#24378;&#35843;&#20102;&#30740;&#31350;&#20013;&#20351;&#29992;&#30340;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#20998;&#31867;&#19982;&#39046;&#22495;&#20869;&#31867;&#20284;&#30340;&#36817;&#26399;&#24037;&#20316;&#20197;&#21450;&#28151;&#21512;&#20154;&#24037;&#26234;&#33021;&#21644;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.07744v2 Announce Type: replace  Abstract: Motivated by the desire to explore the process of combining inductive and deductive reasoning, we conducted a systematic literature review of articles that investigate the integration of machine learning and ontologies. The objective was to identify diverse techniques that incorporate both inductive reasoning (performed by machine learning) and deductive reasoning (performed by ontologies) into artificial intelligence systems. Our review, which included the analysis of 128 studies, allowed us to identify three main categories of hybridization between machine learning and ontologies: learning-enhanced ontologies, semantic data mining, and learning and reasoning systems. We provide a comprehensive examination of all these categories, emphasizing the various machine learning algorithms utilized in the studies. Furthermore, we compared our classification with similar recent work in the field of hybrid AI and neuro-symbolic approaches.
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22270;&#35821;&#35328;&#27169;&#22411;&#65288;GLM&#65289;&#65292;&#32467;&#21512;&#32447;&#24615;&#21270;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#28857;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#22312;&#22788;&#29702;&#32467;&#26500;&#21270;&#30693;&#35782;&#22270;&#35889;&#26102;&#30340;&#24369;&#28857;&#12290;</title><link>https://arxiv.org/abs/2401.07105</link><description>&lt;p&gt;
&#22270;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Graph Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.07105
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22270;&#35821;&#35328;&#27169;&#22411;&#65288;GLM&#65289;&#65292;&#32467;&#21512;&#32447;&#24615;&#21270;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#28857;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#22312;&#22788;&#29702;&#32467;&#26500;&#21270;&#30693;&#35782;&#22270;&#35889;&#26102;&#30340;&#24369;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#20027;&#21147;&#20891;&#65292;&#23427;&#20204;&#19982;&#32467;&#26500;&#21270;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#30340;&#30456;&#20114;&#20316;&#29992;&#20173;&#22312;&#31215;&#26497;&#30740;&#31350;&#20013;&#12290;&#24403;&#21069;&#29992;&#20110;&#32534;&#30721;&#36825;&#20123;&#22270;&#24418;&#30340;&#26041;&#27861;&#36890;&#24120;&#35201;&#20040;&#65288;i&#65289;&#23558;&#23427;&#20204;&#32447;&#24615;&#21270;&#20197;&#20379;LM&#23884;&#20837;--&#36825;&#26679;&#20250;&#20302;&#25928;&#21033;&#29992;&#32467;&#26500;&#20449;&#24687;&#65292;&#35201;&#20040;&#65288;ii&#65289;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26469;&#20445;&#30041;&#22270;&#32467;&#26500;--&#20294;GNNs&#26080;&#27861;&#20687;&#39044;&#35757;&#32451;&#30340;LM&#19968;&#26679;&#24456;&#22909;&#22320;&#34920;&#31034;&#25991;&#26412;&#29305;&#24449;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#22411;LM&#31867;&#22411;&#65292;&#21363;&#22270;&#35821;&#35328;&#27169;&#22411;&#65288;GLM&#65289;&#65292;&#23427;&#25972;&#21512;&#20102;&#20004;&#31181;&#26041;&#27861;&#30340;&#20248;&#28857;&#24182;&#20943;&#36731;&#20102;&#23427;&#20204;&#30340;&#24369;&#28857;&#12290;GLM&#21442;&#25968;&#20174;&#39044;&#35757;&#32451;&#30340;LM&#20013;&#21021;&#22987;&#21270;&#65292;&#20197;&#22686;&#24378;&#23545;&#20010;&#21035;&#22270;&#27010;&#24565;&#21644;&#19977;&#20803;&#32452;&#30340;&#29702;&#35299;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#35774;&#35745;GLM&#30340;&#26550;&#26500;&#20197;&#25972;&#21512;&#22270;&#20559;&#24046;&#65292;&#20174;&#32780;&#20419;&#36827;&#22270;&#20869;&#30340;&#30693;&#35782;&#20998;&#24067;&#12290;&#36825;&#20351;GLM&#33021;&#22815;&#22788;&#29702;&#22270;&#24418;&#12289;&#25991;&#26412;&#20197;&#21450;&#20004;&#32773;&#30340;&#20132;&#32455;&#36755;&#20837;&#12290;&#23454;&#35777;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.07105v2 Announce Type: replace-cross  Abstract: While Language Models (LMs) are the workhorses of NLP, their interplay with structured knowledge graphs (KGs) is still actively researched. Current methods for encoding such graphs typically either (i) linearize them for embedding with LMs -- which underutilize structural information, or (ii) use Graph Neural Networks (GNNs) to preserve the graph structure -- but GNNs cannot represent text features as well as pretrained LMs. In our work we introduce a novel LM type, the Graph Language Model (GLM), that integrates the strengths of both approaches and mitigates their weaknesses. The GLM parameters are initialized from a pretrained LM to enhance understanding of individual graph concepts and triplets. Simultaneously, we design the GLM's architecture to incorporate graph biases, thereby promoting effective knowledge distribution within the graph. This enables GLMs to process graphs, texts, and interleaved inputs of both. Empirical 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#23618;ReLU&#32593;&#32476;&#20013;&#30340;&#38544;&#34255;&#26497;&#23567;&#20540;&#29616;&#35937;&#65292;&#24182;&#25552;&#20986;&#26041;&#27861;&#26469;&#30740;&#31350;&#36825;&#20123;&#38544;&#34255;&#26497;&#23567;&#20540;&#30340;&#29420;&#29305;&#35299;&#26512;&#24615;&#36136;&#12290;</title><link>https://arxiv.org/abs/2312.16819</link><description>&lt;p&gt;
&#20004;&#23618;ReLU&#32593;&#32476;&#20013;&#30340;&#38544;&#34255;&#26497;&#23567;&#20540;
&lt;/p&gt;
&lt;p&gt;
Hidden Minima in Two-Layer ReLU Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.16819
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#23618;ReLU&#32593;&#32476;&#20013;&#30340;&#38544;&#34255;&#26497;&#23567;&#20540;&#29616;&#35937;&#65292;&#24182;&#25552;&#20986;&#26041;&#27861;&#26469;&#30740;&#31350;&#36825;&#20123;&#38544;&#34255;&#26497;&#23567;&#20540;&#30340;&#29420;&#29305;&#35299;&#26512;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#25311;&#21512;&#20855;&#26377;$d$&#20010;&#36755;&#20837;&#12289;$k$&#20010;&#31070;&#32463;&#20803;&#20197;&#21450;&#30001;&#30446;&#26631;&#32593;&#32476;&#29983;&#25104;&#30340;&#26631;&#31614;&#30340;&#20004;&#23618;ReLU&#32593;&#32476;&#25152;&#28041;&#21450;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#26368;&#36817;&#21457;&#29616;&#20102;&#20004;&#31181;&#26080;&#31351;&#26063;&#30340;&#34394;&#20551;&#26497;&#23567;&#20540;&#65292;&#27599;&#20010;$d$&#23545;&#24212;&#19968;&#20010;&#26497;&#23567;&#20540;&#12290;&#23646;&#20110;&#31532;&#19968;&#31867;&#30340;&#26497;&#23567;&#20540;&#30340;&#25439;&#22833;&#22312;$d$&#22686;&#21152;&#26102;&#25910;&#25947;&#20110;&#38646;&#12290;&#22312;&#31532;&#20108;&#31867;&#20013;&#65292;&#25439;&#22833;&#20445;&#25345;&#36828;&#31163;&#20110;&#38646;&#12290;&#37027;&#20040;&#65292;&#22914;&#20309;&#36991;&#20813;&#23646;&#20110;&#21518;&#19968;&#31867;&#30340;&#26497;&#23567;&#20540;&#21602;&#65311;&#24184;&#36816;&#30340;&#26159;&#65292;&#36825;&#26679;&#30340;&#26497;&#23567;&#20540;&#20174;&#19981;&#20250;&#34987;&#26631;&#20934;&#20248;&#21270;&#26041;&#27861;&#26816;&#27979;&#21040;&#12290;&#21463;&#21040;&#27492;&#29616;&#35937;&#24615;&#36136;&#30340;&#38382;&#39064;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#30740;&#31350;&#38544;&#34255;&#26497;&#23567;&#20540;&#29420;&#29305;&#35299;&#26512;&#24615;&#36136;&#30340;&#26041;&#27861;&#12290;&#26681;&#25454;&#29616;&#26377;&#30340;&#20998;&#26512;&#65292;&#20004;&#31181;&#31867;&#22411;&#30340;Hessian&#35889;&#22312;$O(d^{-1/2})$&#39033;&#27169;&#24847;&#20041;&#19979;&#19968;&#33268; -- &#19981;&#22826;&#20048;&#35266;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#36890;&#36807;&#30740;&#31350;&#25439;&#22833;&#34987;&#26368;&#23567;&#21270;&#25110;&#26368;&#22823;&#21270;&#30340;&#26354;&#32447;&#36827;&#34892;&#65292;&#36890;&#24120;&#31216;&#20026;&#20999;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.16819v2 Announce Type: replace  Abstract: The optimization problem associated to fitting two-layer ReLU networks having $d$~inputs, $k$~neurons, and labels generated by a target network, is considered. Two types of infinite families of spurious minima, giving one minimum per $d$, were recently found. The loss at minima belonging to the first type converges to zero as $d$ increases. In the second type, the loss remains bounded away from zero. That being so, how may one avoid minima belonging to the latter type? Fortunately, such minima are never detected by standard optimization methods. Motivated by questions concerning the nature of this phenomenon, we develop methods to study distinctive analytic properties of hidden minima.   By existing analyses, the Hessian spectrum of both types agree modulo $O(d^{-1/2})$-terms -- not promising. Thus, rather, our investigation proceeds by studying curves along which the loss is minimized or maximized, generally referred to as tangency 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#32447;&#24615;&#24773;&#22659;&#36172;&#21338;&#38382;&#39064;&#30340;&#20004;&#20840;&#20854;&#32654;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#23545;&#25239;&#24615;&#21644;&#38543;&#26426;&#24773;&#20917;&#19979;&#25509;&#36817;&#26368;&#20248;&#30340;&#36951;&#25022;&#30028;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;&#38024;&#23545;&#26368;&#23567;&#27425;&#20248;&#24046;&#36317;&#30340;&#22810;&#23545;&#25968;&#32423;&#21035;&#36895;&#29575;&#21644;&#22312;&#23545;&#25239;&#24615;&#24773;&#20917;&#19979;&#30340;&#31532;&#19968;&#38454;&#25110;&#31532;&#20108;&#38454;&#30028;&#20197;&#21450;&#22522;&#20110;Shannon&#29109;&#27491;&#21017;&#39033;&#30340;FTRL&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2312.15433</link><description>&lt;p&gt;
&#32447;&#24615;&#24773;&#22659;&#36172;&#21338;&#38382;&#39064;&#30340;&#20004;&#20840;&#20854;&#32654;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Best-of-Both-Worlds Algorithms for Linear Contextual Bandits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.15433
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#32447;&#24615;&#24773;&#22659;&#36172;&#21338;&#38382;&#39064;&#30340;&#20004;&#20840;&#20854;&#32654;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#23545;&#25239;&#24615;&#21644;&#38543;&#26426;&#24773;&#20917;&#19979;&#25509;&#36817;&#26368;&#20248;&#30340;&#36951;&#25022;&#30028;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;&#38024;&#23545;&#26368;&#23567;&#27425;&#20248;&#24046;&#36317;&#30340;&#22810;&#23545;&#25968;&#32423;&#21035;&#36895;&#29575;&#21644;&#22312;&#23545;&#25239;&#24615;&#24773;&#20917;&#19979;&#30340;&#31532;&#19968;&#38454;&#25110;&#31532;&#20108;&#38454;&#30028;&#20197;&#21450;&#22522;&#20110;Shannon&#29109;&#27491;&#21017;&#39033;&#30340;FTRL&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#38024;&#23545;$K$&#33218;&#32447;&#24615;&#24773;&#22659;&#36172;&#21338;&#38382;&#39064;&#30340;&#20004;&#20840;&#20854;&#32654;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#23545;&#25239;&#24615;&#21644;&#38543;&#26426;&#24773;&#20917;&#19979;&#22343;&#20855;&#26377;&#25509;&#36817;&#26368;&#20248;&#30340;&#36951;&#25022;&#30028;&#65292;&#32780;&#26080;&#38656;&#20851;&#20110;&#29615;&#22659;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#22312;&#38543;&#26426;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#22810;&#23545;&#25968;&#32423;&#21035;&#30340;&#36895;&#29575;$\frac{(dK)^2\mathrm{poly}\log(dKT)}{\Delta_{\min}}$&#65292;&#20854;&#20013;$\Delta_{\min}$&#26159;$d$&#32500;&#24773;&#22659;&#31354;&#38388;&#20013;&#30340;&#26368;&#23567;&#27425;&#20248;&#24046;&#36317;&#12290;&#22312;&#23545;&#25239;&#24615;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#31532;&#19968;&#38454;$\widetilde{O}(dK\sqrt{L^*})$&#30028;&#25110;&#32773;&#31532;&#20108;&#38454;$\widetilde{O}(dK\sqrt{\Lambda^*})$&#30028;&#65292;&#20854;&#20013;$L^*$&#26159;&#26368;&#20339;&#25805;&#20316;&#30340;&#32047;&#31215;&#25439;&#22833;&#65292;$\Lambda^*$&#26159;&#31639;&#27861;&#20135;&#29983;&#30340;&#25439;&#22833;&#30340;&#32047;&#31215;&#20108;&#27425;&#30697;&#30340;&#19968;&#31181;&#27010;&#24565;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22522;&#20110;&#24102;&#26377;Shannon&#29109;&#27491;&#21017;&#39033;&#30340;FTRL&#31639;&#27861;&#24320;&#21457;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#30693;&#36947;&#21327;&#26041;&#24046;&#30697;&#38453;&#36870;&#30340;&#31639;&#27861;&#65292;&#24182;&#23454;&#29616;&#20102;&#22810;&#23545;&#25968;&#36951;&#25022;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.15433v2 Announce Type: replace  Abstract: We study best-of-both-worlds algorithms for $K$-armed linear contextual bandits. Our algorithms deliver near-optimal regret bounds in both the adversarial and stochastic regimes, without prior knowledge about the environment. In the stochastic regime, we achieve the polylogarithmic rate $\frac{(dK)^2\mathrm{poly}\log(dKT)}{\Delta_{\min}}$, where $\Delta_{\min}$ is the minimum suboptimality gap over the $d$-dimensional context space. In the adversarial regime, we obtain either the first-order $\widetilde{O}(dK\sqrt{L^*})$ bound, or the second-order $\widetilde{O}(dK\sqrt{\Lambda^*})$ bound, where $L^*$ is the cumulative loss of the best action and $\Lambda^*$ is a notion of the cumulative second moment for the losses incurred by the algorithm. Moreover, we develop an algorithm based on FTRL with Shannon entropy regularizer that does not require the knowledge of the inverse of the covariance matrix, and achieves a polylogarithmic regre
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#21033;&#29992;&#25945;&#24072;&#21644;&#23398;&#29983;&#30340;&#27491;&#30830;&#24615;&#20197;&#21450;&#23398;&#29983;&#23545;&#25945;&#24072;&#27169;&#20223;&#31243;&#24230;&#26469;&#23398;&#20064;&#27599;&#20010;&#26679;&#26412;&#30340;&#30693;&#35782;&#34701;&#21512;&#27604;&#29575;&#65292;&#20174;&#32780;&#24341;&#20837;&#20102;&#26679;&#26412;&#20869;&#19977;&#36793;&#20960;&#20309;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2312.15112</link><description>&lt;p&gt;
&#20174;&#25945;&#24072;&#37027;&#37324;&#23569;&#25110;&#22810;&#65306;&#21033;&#29992;&#19977;&#36793;&#20960;&#20309;&#36827;&#34892;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Less or More From Teacher: Exploiting Trilateral Geometry For Knowledge Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.15112
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#21033;&#29992;&#25945;&#24072;&#21644;&#23398;&#29983;&#30340;&#27491;&#30830;&#24615;&#20197;&#21450;&#23398;&#29983;&#23545;&#25945;&#24072;&#27169;&#20223;&#31243;&#24230;&#26469;&#23398;&#20064;&#27599;&#20010;&#26679;&#26412;&#30340;&#30693;&#35782;&#34701;&#21512;&#27604;&#29575;&#65292;&#20174;&#32780;&#24341;&#20837;&#20102;&#26679;&#26412;&#20869;&#19977;&#36793;&#20960;&#20309;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#26088;&#22312;&#20351;&#29992;&#26469;&#33258;&#36739;&#22823;&#25945;&#24072;&#32593;&#32476;&#30340;&#36719;&#30417;&#30563;&#21644;&#26469;&#33258;&#30495;&#23454;&#25968;&#25454;&#30340;&#30828;&#30417;&#30563;&#26469;&#35757;&#32451;&#19968;&#20010;&#32039;&#20945;&#30340;&#23398;&#29983;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;&#30830;&#23450;&#19968;&#20010;&#24179;&#34913;&#36825;&#20123;&#30417;&#30563;&#20449;&#21495;&#30340;&#26368;&#20339;&#30693;&#35782;&#34701;&#21512;&#27604;&#29575;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#27599;&#20010;&#26679;&#26412;&#30340;&#30693;&#35782;&#34701;&#21512;&#27604;&#29575;&#65292;&#21033;&#29992;&#25945;&#24072;&#21644;&#23398;&#29983;&#30340;&#27491;&#30830;&#24615;&#65292;&#20197;&#21450;&#23398;&#29983;&#22312;&#27599;&#20010;&#26679;&#26412;&#19978;&#27169;&#20223;&#25945;&#24072;&#30340;&#31243;&#24230;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#33258;&#28982;&#22320;&#23548;&#33268;&#20102;&#23398;&#29983;&#39044;&#27979;($S$)&#12289;&#25945;&#24072;&#39044;&#27979;($T$)&#21644;&#22320;&#38754;&#30495;&#30456;($G$)&#20043;&#38388;&#30340;&#26679;&#26412;&#20869;&#19977;&#36793;&#20960;&#20309;&#20851;&#31995;&#12290;&#20026;&#20102;&#25269;&#28040;&#24322;&#24120;&#20540;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25193;&#23637;&#21040;&#26679;&#26412;&#38388;&#30340;&#20851;&#31995;&#65292;&#23558;&#25945;&#24072;&#30340;&#20840;&#23616;&#24179;&#22343;&#39044;&#27979;$\bar{T}$&#32435;&#20837;&#32771;&#34385;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.15112v3 Announce Type: replace  Abstract: Knowledge distillation aims to train a compact student network using soft supervision from a larger teacher network and hard supervision from ground truths. However, determining an optimal knowledge fusion ratio that balances these supervisory signals remains challenging. Prior methods generally resort to a constant or heuristic-based fusion ratio, which often falls short of a proper balance. In this study, we introduce a novel adaptive method for learning a sample-wise knowledge fusion ratio, exploiting both the correctness of teacher and student, as well as how well the student mimics the teacher on each sample. Our method naturally leads to the intra-sample trilateral geometric relations among the student prediction ($S$), teacher prediction ($T$), and ground truth ($G$). To counterbalance the impact of outliers, we further extend to the inter-sample relations, incorporating the teacher's global average prediction $\bar{T}$ for sa
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#32593;&#32476;&#22312;&#39640;&#32500;&#25968;&#25454;&#20013;&#21457;&#29616;&#32479;&#35745;&#27169;&#24335;&#65292;&#30740;&#31350;&#20102;&#22914;&#20309;&#39640;&#25928;&#22320;&#20174;&#39640;&#38454;&#32047;&#31215;&#37327;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#24182;&#25506;&#35752;&#20102;&#22312;&#23574;&#23792;&#32047;&#31215;&#37327;&#27169;&#22411;&#20013;&#30340;&#32479;&#35745;&#21644;&#35745;&#31639;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2312.14922</link><description>&lt;p&gt;
&#20174;&#39640;&#38454;&#32479;&#35745;&#37327;&#20013;&#39640;&#25928;&#23398;&#20064;&#65306;&#20551;&#35774;&#26816;&#39564;&#12289;&#38543;&#26426;&#29305;&#24449;&#21644;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Learning from higher-order statistics, efficiently: hypothesis tests, random features, and neural networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.14922
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#22312;&#39640;&#32500;&#25968;&#25454;&#20013;&#21457;&#29616;&#32479;&#35745;&#27169;&#24335;&#65292;&#30740;&#31350;&#20102;&#22914;&#20309;&#39640;&#25928;&#22320;&#20174;&#39640;&#38454;&#32047;&#31215;&#37327;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#24182;&#25506;&#35752;&#20102;&#22312;&#23574;&#23792;&#32047;&#31215;&#37327;&#27169;&#22411;&#20013;&#30340;&#32479;&#35745;&#21644;&#35745;&#31639;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#25797;&#38271;&#21457;&#29616;&#39640;&#32500;&#25968;&#25454;&#38598;&#20013;&#30340;&#32479;&#35745;&#27169;&#24335;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#24230;&#37327;&#19977;&#20010;&#25110;&#26356;&#22810;&#21464;&#37327;&#38388;&#30340;&#38750;&#39640;&#26031;&#30456;&#20851;&#24615;&#30340;&#39640;&#38454;&#32047;&#31215;&#37327;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#29305;&#21035;&#37325;&#35201;&#12290;&#20294;&#31070;&#32463;&#32593;&#32476;&#26377;&#22810;&#26377;&#25928;&#22320;&#20174;&#39640;&#38454;&#32047;&#31215;&#37327;&#20013;&#25552;&#21462;&#29305;&#24449;&#65311;&#25105;&#20204;&#22312;&#23574;&#23792;&#32047;&#31215;&#37327;&#27169;&#22411;&#20013;&#25506;&#35752;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#36825;&#37324;&#32479;&#35745;&#23398;&#23478;&#38656;&#35201;&#20174;$d$&#32500;&#36755;&#20837;&#30340;&#38454;-$p\ge 4$&#32047;&#31215;&#37327;&#20013;&#24674;&#22797;&#20986;&#19968;&#20010;&#29305;&#26435;&#26041;&#21521;&#25110;&#8220;&#23574;&#23792;&#8221;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#20998;&#26512;&#25152;&#38656;&#26679;&#26412;&#25968;$n$&#26469;&#34920;&#24449;&#24674;&#22797;&#23574;&#23792;&#30340;&#22522;&#26412;&#32479;&#35745;&#21644;&#35745;&#31639;&#38480;&#21046;&#65292;&#20197;&#24378;&#28872;&#21306;&#20998;&#26469;&#33258;&#23574;&#23792;&#32047;&#31215;&#37327;&#27169;&#22411;&#21644;&#21508;&#21521;&#21516;&#24615;&#39640;&#26031;&#36755;&#20837;&#30340;&#36755;&#20837;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#32479;&#35745;&#19978;&#30340;&#21487;&#21306;&#20998;&#24615;&#38656;&#35201;$n\gtrsim d$&#20010;&#26679;&#26412;&#65292;&#32780;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#21306;&#20998;&#36825;&#20004;&#20010;&#20998;&#24067;&#21017;&#38656;&#35201;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.14922v2 Announce Type: replace-cross  Abstract: Neural networks excel at discovering statistical patterns in high-dimensional data sets. In practice, higher-order cumulants, which quantify the non-Gaussian correlations between three or more variables, are particularly important for the performance of neural networks. But how efficient are neural networks at extracting features from higher-order cumulants? We study this question in the spiked cumulant model, where the statistician needs to recover a privileged direction or "spike" from the order-$p\ge 4$ cumulants of $d$-dimensional inputs. We first characterise the fundamental statistical and computational limits of recovering the spike by analysing the number of samples $n$ required to strongly distinguish between inputs from the spiked cumulant model and isotropic Gaussian inputs. We find that statistical distinguishability requires $n\gtrsim d$ samples, while distinguishing the two distributions in polynomial time require
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22522;&#20110;&#29983;&#25104;&#30340;&#26041;&#27861;SIG&#65292;&#22312;&#25991;&#23398;&#20316;&#21697;&#20013;&#23454;&#29616;&#20102;&#35828;&#35805;&#32773;&#35782;&#21035;&#20219;&#21153;&#65292;&#25903;&#25345;&#36328;&#39046;&#22495;&#35780;&#20272;&#21644;&#24320;&#25918;&#19990;&#30028;&#20998;&#31867;&#33539;&#24335;&#12290;</title><link>https://arxiv.org/abs/2312.14590</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#25552;&#31034;&#29983;&#25104;&#30340;&#26041;&#27861;&#22312;&#25991;&#23398;&#20316;&#21697;&#20013;&#36827;&#34892;&#35828;&#35805;&#32773;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
SIG: Speaker Identification in Literature via Prompt-Based Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.14590
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#29983;&#25104;&#30340;&#26041;&#27861;SIG&#65292;&#22312;&#25991;&#23398;&#20316;&#21697;&#20013;&#23454;&#29616;&#20102;&#35828;&#35805;&#32773;&#35782;&#21035;&#20219;&#21153;&#65292;&#25903;&#25345;&#36328;&#39046;&#22495;&#35780;&#20272;&#21644;&#24320;&#25918;&#19990;&#30028;&#20998;&#31867;&#33539;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25991;&#23398;&#20998;&#26512;&#20013;&#65292;&#35782;&#21035;&#21465;&#36848;&#20013;&#24341;&#29992;&#30340;&#21457;&#35328;&#32773;&#26159;&#19968;&#39033;&#37325;&#35201;&#20219;&#21153;&#65292;&#25361;&#25112;&#24615;&#24773;&#26223;&#21253;&#25324;&#23545;&#30475;&#19981;&#35265;&#21457;&#35328;&#32773;&#30340;&#36328;&#39046;&#22495;&#25512;&#26029;&#65292;&#20197;&#21450;&#21608;&#22260;&#29615;&#22659;&#20013;&#27809;&#26377;&#25552;&#21040;&#21457;&#35328;&#32773;&#30340;&#38750;&#26126;&#30830;&#24773;&#20917;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#26377;&#25928;&#30340;&#26041;&#27861;SIG&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#26681;&#25454;&#35774;&#35745;&#30340;&#25552;&#31034;&#27169;&#26495;&#23545;&#20219;&#21153;&#21644;&#24341;&#35821;&#36755;&#20837;&#36827;&#34892;&#35821;&#35328;&#21270;&#22788;&#29702;&#65292;&#36824;&#21487;&#20197;&#36731;&#26494;&#38598;&#25104;&#20854;&#20182;&#36827;&#19968;&#27493;&#22686;&#24378;&#35828;&#35805;&#32773;&#35782;&#21035;&#24615;&#33021;&#30340;&#36741;&#21161;&#20219;&#21153;&#12290;&#39044;&#27979;&#21487;&#20197;&#26469;&#33258;&#27169;&#22411;&#30340;&#30452;&#25509;&#29983;&#25104;&#65292;&#20063;&#21487;&#20197;&#30001;&#27599;&#20010;&#21457;&#35328;&#32773;&#20505;&#36873;&#20154;&#30340;&#26368;&#39640;&#29983;&#25104;&#27010;&#29575;&#30830;&#23450;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#26041;&#27861;&#35774;&#35745;&#65292;SIG&#25903;&#25345;&#36328;&#39046;&#22495;&#35780;&#20272;&#65292;&#24182;&#23454;&#29616;&#20102;&#33021;&#22815;&#25509;&#21463;&#20219;&#20309;&#20505;&#36873;&#36755;&#20837;&#24418;&#24335;&#30340;&#24320;&#25918;&#19990;&#30028;&#20998;&#31867;&#33539;&#24335;&#12290;&#25105;&#20204;&#22312;PDNC&#19978;&#36827;&#34892;&#20102;&#36328;&#39046;&#22495;&#35780;&#20272;&#21644;&#20869;&#39046;&#22495;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.14590v2 Announce Type: replace  Abstract: Identifying speakers of quotations in narratives is an important task in literary analysis, with challenging scenarios including the out-of-domain inference for unseen speakers, and non-explicit cases where there are no speaker mentions in surrounding context. In this work, we propose a simple and effective approach SIG, a generation-based method that verbalizes the task and quotation input based on designed prompt templates, which also enables easy integration of other auxiliary tasks that further bolster the speaker identification performance. The prediction can either come from direct generation by the model, or be determined by the highest generation probability of each speaker candidate. Based on our approach design, SIG supports out-of-domain evaluation, and achieves open-world classification paradigm that is able to accept any forms of candidate input. We perform both cross-domain evaluation and in-domain evaluation on PDNC, t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20219;&#21153;&#33258;&#30001;&#25345;&#32493;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#36755;&#20837;&#25968;&#25454;&#21644;&#20915;&#31574;&#21046;&#23450;&#19978;&#27880;&#20837;&#25932;&#23545;&#25200;&#21160;&#65292;&#25552;&#39640;&#20102;&#26410;&#26469;&#26679;&#26412;&#32771;&#34385;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2312.13027</link><description>&lt;p&gt;
&#21452;&#37325;&#25200;&#21160;&#20219;&#21153;&#33258;&#30001;&#30340;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Doubly Perturbed Task Free Continual Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.13027
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20219;&#21153;&#33258;&#30001;&#25345;&#32493;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#36755;&#20837;&#25968;&#25454;&#21644;&#20915;&#31574;&#21046;&#23450;&#19978;&#27880;&#20837;&#25932;&#23545;&#25200;&#21160;&#65292;&#25552;&#39640;&#20102;&#26410;&#26469;&#26679;&#26412;&#32771;&#34385;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Task Free online continual learning (TF-CL)&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#27169;&#22411;&#22312;&#27809;&#26377;&#26174;&#24335;&#20219;&#21153;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#36880;&#27493;&#23398;&#20064;&#20219;&#21153;&#12290;&#23613;&#31649;&#20351;&#29992;&#26469;&#33258;&#36807;&#21435;&#12289;&#29616;&#22312;&#20197;&#21450;&#26410;&#26469;&#30340;&#25152;&#26377;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#34987;&#35748;&#20026;&#26159;&#40644;&#37329;&#26631;&#20934;&#65292;&#20294;&#22312;TF-CL&#20013;&#20351;&#29992;&#24403;&#21069;&#26679;&#26412;&#30340;&#24188;&#31258;&#26041;&#27861;&#21487;&#33021;&#20250;&#19982;&#26410;&#26469;&#26679;&#26412;&#30340;&#23398;&#20064;&#21457;&#29983;&#20914;&#31361;&#65292;&#23548;&#33268;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;&#21487;&#22609;&#24615;&#19981;&#20339;&#12290;&#22240;&#27492;&#65292;&#22312;TF-CL&#20013;&#31215;&#26497;&#32771;&#34385;&#26410;&#26469;&#26679;&#26412;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#21463;&#21040;&#36825;&#31181;&#30452;&#35273;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32771;&#34385;&#26410;&#26469;&#26679;&#26412;&#30340;&#26032;&#39062;TF-CL&#26694;&#26550;&#65292;&#24182;&#23637;&#31034;&#22312;&#36755;&#20837;&#25968;&#25454;&#21644;&#20915;&#31574;&#21046;&#23450;&#19978;&#27880;&#20837;&#25932;&#23545;&#25200;&#21160;&#26159;&#26377;&#25928;&#30340;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Doubly Perturbed Continual Learning (DPCL)&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#26377;&#25928;&#22320;&#23454;&#26045;&#36825;&#20123;&#36755;&#20837;&#21644;&#20915;&#31574;&#25200;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.13027v2 Announce Type: replace  Abstract: Task Free online continual learning (TF-CL) is a challenging problem where the model incrementally learns tasks without explicit task information. Although training with entire data from the past, present as well as future is considered as the gold standard, naive approaches in TF-CL with the current samples may be conflicted with learning with samples in the future, leading to catastrophic forgetting and poor plasticity. Thus, a proactive consideration of an unseen future sample in TF-CL becomes imperative. Motivated by this intuition, we propose a novel TF-CL framework considering future samples and show that injecting adversarial perturbations on both input data and decision-making is effective. Then, we propose a novel method named Doubly Perturbed Continual Learning (DPCL) to efficiently implement these input and decision-making perturbations. Specifically, for input perturbation, we propose an approximate perturbation method th
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#22312;PPO-Clip&#31639;&#27861;&#26041;&#38754;&#20570;&#20986;&#36129;&#29486;&#65292;&#24314;&#31435;&#20102;&#20854;&#22312;&#34920;&#26684;&#21644;&#31070;&#32463;&#20989;&#25968;&#36924;&#36817;&#35774;&#32622;&#20013;&#30340;&#20840;&#23616;&#25910;&#25947;&#32467;&#26524;&#65292;&#29305;&#21035;&#31361;&#20986;&#20102;&#22312;&#31070;&#32463;&#20989;&#25968;&#36924;&#36817;&#24773;&#22659;&#19979;&#30340;$O(1/\sqrt{T})$&#26368;&#23567;&#36845;&#20195;&#25910;&#25947;&#36895;&#29575;&#12290;</title><link>https://arxiv.org/abs/2312.12065</link><description>&lt;p&gt;
PPO-Clip&#23454;&#29616;&#20840;&#23616;&#26368;&#20248;&#24615;&#65306;&#26356;&#28145;&#20837;&#29702;&#35299;&#20462;&#21098;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
PPO-Clip Attains Global Optimality: Towards Deeper Understandings of Clipping
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.12065
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#22312;PPO-Clip&#31639;&#27861;&#26041;&#38754;&#20570;&#20986;&#36129;&#29486;&#65292;&#24314;&#31435;&#20102;&#20854;&#22312;&#34920;&#26684;&#21644;&#31070;&#32463;&#20989;&#25968;&#36924;&#36817;&#35774;&#32622;&#20013;&#30340;&#20840;&#23616;&#25910;&#25947;&#32467;&#26524;&#65292;&#29305;&#21035;&#31361;&#20986;&#20102;&#22312;&#31070;&#32463;&#20989;&#25968;&#36924;&#36817;&#24773;&#22659;&#19979;&#30340;$O(1/\sqrt{T})$&#26368;&#23567;&#36845;&#20195;&#25910;&#25947;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#24314;&#31435;&#20102;PPO-Clip&#21464;&#20307;&#22312;&#34920;&#26684;&#21644;&#31070;&#32463;&#20989;&#25968;&#36924;&#36817;&#35774;&#32622;&#20013;&#20855;&#26377;&#20840;&#23616;&#25910;&#25947;&#24615;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#29305;&#21035;&#31361;&#20986;&#20102;&#22312;&#31070;&#32463;&#20989;&#25968;&#36924;&#36817;&#24773;&#22659;&#19979;&#30340;$O(1/\sqrt{T})$&#26368;&#23567;&#36845;&#20195;&#25910;&#25947;&#36895;&#29575;&#12290;&#36890;&#36807;&#24341;&#20837;PPO-Clip&#30340;&#24191;&#20041;&#29256;&#26412;&#65292;&#32467;&#21512;&#20854;&#19982;&#38128;&#38142;&#25439;&#22833;&#30340;&#20851;&#31995;&#65292;&#37319;&#29992;&#29109;&#38236;&#20687;&#19979;&#38477;&#65292;&#25105;&#20204;&#20026;&#30452;&#25509;&#31574;&#30053;&#21442;&#25968;&#21270;&#30340;&#34920;&#26684;PPO-Clip&#24314;&#31435;&#20102;&#28176;&#36817;&#25910;&#25947;&#12290;&#21463;&#34920;&#26684;&#20998;&#26512;&#21551;&#21457;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.12065v2 Announce Type: replace-cross  Abstract: Proximal Policy Optimization algorithm employing a clipped surrogate objective (PPO-Clip) is a prominent exemplar of the policy optimization methods. However, despite its remarkable empirical success, PPO-Clip lacks theoretical substantiation to date. In this paper, we contribute to the field by establishing the first global convergence results of a PPO-Clip variant in both tabular and neural function approximation settings. Our findings highlight the $O(1/\sqrt{T})$ min-iterate convergence rate specifically in the context of neural function approximation. We tackle the inherent challenges in analyzing PPO-Clip through three central concepts: (i) We introduce a generalized version of the PPO-Clip objective, illuminated by its connection with the hinge loss. (ii) Employing entropic mirror descent, we establish asymptotic convergence for tabular PPO-Clip with direct policy parameterization. (iii) Inspired by the tabular analysis,
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BSARec&#30340;&#26032;&#26041;&#27861;&#65292;&#36229;&#36234;&#20102;&#33258;&#27880;&#24847;&#21147;&#65292;&#22312;&#24207;&#21015;&#25512;&#33616;&#20013;&#27880;&#20837;&#20102;&#24402;&#32435;&#20559;&#24046;&#65292;&#24182;&#38598;&#25104;&#20102;&#20302;&#39057;&#21644;&#39640;&#39057;&#20449;&#24687;&#20197;&#20943;&#36731;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;</title><link>https://arxiv.org/abs/2312.10325</link><description>&lt;p&gt;
&#36229;&#36234;&#33258;&#27880;&#24847;&#21147;&#30340;&#24207;&#21015;&#25512;&#33616;&#20013;&#30340;&#20851;&#27880;&#24402;&#32435;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
An Attentive Inductive Bias for Sequential Recommendation beyond the Self-Attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.10325
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BSARec&#30340;&#26032;&#26041;&#27861;&#65292;&#36229;&#36234;&#20102;&#33258;&#27880;&#24847;&#21147;&#65292;&#22312;&#24207;&#21015;&#25512;&#33616;&#20013;&#27880;&#20837;&#20102;&#24402;&#32435;&#20559;&#24046;&#65292;&#24182;&#38598;&#25104;&#20102;&#20302;&#39057;&#21644;&#39640;&#39057;&#20449;&#24687;&#20197;&#20943;&#36731;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#24207;&#21015;&#25512;&#33616;&#65288;SR&#65289;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290; Transformer&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#36935;&#21040;&#20102;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#65292;&#21363;&#38544;&#34255;&#34920;&#31034;&#21464;&#24471;&#31867;&#20284;&#20110;&#26631;&#35760;&#12290; &#22312;SR&#39046;&#22495;&#65292;&#25105;&#20204;&#39318;&#27425;&#23637;&#31034;&#20102;&#30456;&#21516;&#38382;&#39064;&#30340;&#21457;&#29983;&#12290; &#25105;&#20204;&#36827;&#34892;&#20102;&#24320;&#21019;&#24615;&#30340;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#33258;&#27880;&#24847;&#22312;SR&#20013;&#30340;&#20302;&#36890;&#28388;&#27874;&#29305;&#24615;&#65292;&#23548;&#33268;&#20102;&#36807;&#24230;&#24179;&#28369;&#12290; &#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;$\textbf{B}$eyond $\textbf{S}$elf-$\textbf{A}$ttention for Sequential $\textbf{Rec}$ommendation&#65288;BSARec&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#20613;&#37324;&#21494;&#21464;&#25442;&#26469; i&#65289;&#36890;&#36807;&#32771;&#34385;&#32454;&#31890;&#24230;&#30340;&#24207;&#21015;&#27169;&#24335;&#27880;&#20837;&#24402;&#32435;&#20559;&#24046;&#21644; ii&#65289;&#38598;&#25104;&#20302;&#39057;&#21644;&#39640;&#39057;&#20449;&#24687;&#20197;&#20943;&#36731;&#36807;&#24230;&#24179;&#28369;&#12290; &#25105;&#20204;&#30340;&#21457;&#29616;&#22312;SR&#39046;&#22495;&#26174;&#31034;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#24182;&#26377;&#26395;&#25645;&#36215;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.10325v2 Announce Type: replace-cross  Abstract: Sequential recommendation (SR) models based on Transformers have achieved remarkable successes. The self-attention mechanism of Transformers for computer vision and natural language processing suffers from the oversmoothing problem, i.e., hidden representations becoming similar to tokens. In the SR domain, we, for the first time, show that the same problem occurs. We present pioneering investigations that reveal the low-pass filtering nature of self-attention in the SR, which causes oversmoothing. To this end, we propose a novel method called $\textbf{B}$eyond $\textbf{S}$elf-$\textbf{A}$ttention for Sequential $\textbf{Rec}$ommendation (BSARec), which leverages the Fourier transform to i) inject an inductive bias by considering fine-grained sequential patterns and ii) integrate low and high-frequency information to mitigate oversmoothing. Our discovery shows significant advancements in the SR domain and is expected to bridge t
&lt;/p&gt;</description></item><item><title>&#24212;&#29992;&#26465;&#20214;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;Siamese&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36827;&#34892;&#30417;&#30563;&#24335;&#24322;&#24120;&#26816;&#27979;&#65292;&#25552;&#39640;&#20102;&#31890;&#23376;&#21152;&#36895;&#22120;&#38169;&#35823;&#26463;&#39044;&#27979;&#30340;&#20581;&#22766;&#24615;&#21644;&#24635;&#20307;&#21487;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.10040</link><description>&lt;p&gt;
&#20855;&#26377;&#26465;&#20214;&#24314;&#27169;&#30340;&#31890;&#23376;&#21152;&#36895;&#22120;&#20581;&#22766;&#30340;&#38169;&#35823;&#26463;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Robust Errant Beam Prognostics with Conditional Modeling for Particle Accelerators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.10040
&lt;/p&gt;
&lt;p&gt;
&#24212;&#29992;&#26465;&#20214;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;Siamese&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36827;&#34892;&#30417;&#30563;&#24335;&#24322;&#24120;&#26816;&#27979;&#65292;&#25552;&#39640;&#20102;&#31890;&#23376;&#21152;&#36895;&#22120;&#38169;&#35823;&#26463;&#39044;&#27979;&#30340;&#20581;&#22766;&#24615;&#21644;&#24635;&#20307;&#21487;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31890;&#23376;&#21152;&#36895;&#22120;&#22797;&#26434;&#19988;&#30001;&#25104;&#21315;&#19978;&#19975;&#20010;&#32452;&#20214;&#32452;&#25104;&#65292;&#35768;&#22810;&#35774;&#22791;&#36816;&#34892;&#22312;&#20854;&#23792;&#20540;&#21151;&#29575;&#19979;&#12290;&#22240;&#27492;&#65292;&#31890;&#23376;&#21152;&#36895;&#22120;&#21487;&#33021;&#22240;&#20026;&#22810;&#31181;&#21407;&#22240;&#20986;&#29616;&#25925;&#38556;&#24182;&#20013;&#27490;&#36816;&#34892;&#12290;&#36825;&#20123;&#25925;&#38556;&#20250;&#24433;&#21709;&#35745;&#21010;&#36816;&#34892;&#26102;&#31890;&#23376;&#21152;&#36895;&#22120;&#30340;&#21487;&#29992;&#24615;&#65292;&#38477;&#20302;&#25928;&#29575;&#21644;&#25972;&#20307;&#31185;&#23398;&#20135;&#20986;&#12290;&#20026;&#20102;&#36991;&#20813;&#36825;&#20123;&#25925;&#38556;&#65292;&#25105;&#20204;&#24212;&#29992;&#24322;&#24120;&#26816;&#27979;&#25216;&#26415;&#26469;&#39044;&#27979;&#20219;&#20309;&#19981;&#23547;&#24120;&#30340;&#34892;&#20026;&#65292;&#24182;&#37319;&#21462;&#39044;&#38450;&#24615;&#25514;&#26045;&#26469;&#25552;&#39640;&#31890;&#23376;&#21152;&#36895;&#22120;&#30340;&#24635;&#20307;&#21487;&#29992;&#24615;&#12290;&#22522;&#20110;&#21322;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#22914;&#33258;&#32534;&#30721;&#22120;&#21644;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65292;&#36890;&#24120;&#29992;&#20110;&#36825;&#20123;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#30417;&#30563;&#24335;ML&#25216;&#26415;&#65292;&#22914;Siamese&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#27169;&#22411;&#65292;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#26631;&#31614;&#20449;&#24687;&#20248;&#20110;&#26080;&#30417;&#30563;&#25110;&#21322;&#30417;&#30563;&#26041;&#27861;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#12290;&#20854;&#20013;&#19968;&#20010;&#29305;&#23450;&#30340;&#25361;&#25112;&#26159;&#24322;&#24120;&#26816;&#27979;&#36807;&#31243;&#20013;&#30340;&#26631;&#35760;&#25968;&#25454;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.10040v2 Announce Type: replace-cross  Abstract: Particle accelerators are complex and comprise thousands of components, with many pieces of equipment running at their peak power. Consequently, particle accelerators can fault and abort operations for numerous reasons. These faults impact the availability of particle accelerators during scheduled run-time and hamper the efficiency and the overall science output. To avoid these faults, we apply anomaly detection techniques to predict any unusual behavior and perform preemptive actions to improve the total availability of particle accelerators. Semi-supervised Machine Learning (ML) based anomaly detection approaches such as autoencoders and variational autoencoders are often used for such tasks. However, supervised ML techniques such as Siamese Neural Network (SNN) models can outperform unsupervised or semi-supervised approaches for anomaly detection by leveraging the label information. One of the challenges specific to anomaly 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#32479;&#19968;&#30340;&#26465;&#20214;&#25193;&#25955;&#24314;&#27169;&#26694;&#26550;&#65292;&#22522;&#20110;Doob's h-transform&#65292;&#29992;&#20110;&#35299;&#20915;&#34507;&#30333;&#35774;&#35745;&#20013;&#30340;&#22522;&#24207;&#25903;&#26550;&#38382;&#39064;</title><link>https://arxiv.org/abs/2312.09236</link><description>&lt;p&gt;
&#19968;&#31181;&#20855;&#26377;&#26465;&#20214;&#25193;&#25955;&#24314;&#27169;&#26694;&#26550;&#30340;&#24212;&#29992;&#20110;&#34507;&#30333;&#35774;&#35745;&#20013;&#30340;&#22522;&#24207;&#25903;&#26550;
&lt;/p&gt;
&lt;p&gt;
A framework for conditional diffusion modelling with applications in motif scaffolding for protein design
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.09236
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#32479;&#19968;&#30340;&#26465;&#20214;&#25193;&#25955;&#24314;&#27169;&#26694;&#26550;&#65292;&#22522;&#20110;Doob's h-transform&#65292;&#29992;&#20110;&#35299;&#20915;&#34507;&#30333;&#35774;&#35745;&#20013;&#30340;&#22522;&#24207;&#25903;&#26550;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#34507;&#30333;&#35774;&#35745;&#24212;&#29992;&#65292;&#22914;&#32467;&#21512;&#29289;&#25110;&#37238;&#30340;&#35774;&#35745;&#65292;&#38656;&#35201;&#20197;&#39640;&#31934;&#24230;&#25645;&#24314;&#20855;&#26377;&#32467;&#26500;&#22522;&#24207;&#30340;&#34507;&#30333;&#36136;&#12290;&#22522;&#20110;&#21435;&#22122;&#25193;&#25955;&#36807;&#31243;&#30340;&#29983;&#25104;&#24314;&#27169;&#33539;&#24335;&#24050;&#25104;&#20026;&#35299;&#20915;&#36825;&#19968;&#22522;&#24207;&#25903;&#26550;&#38382;&#39064;&#30340;&#20027;&#35201;&#20505;&#36873;&#26041;&#26696;&#65292;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#26174;&#31034;&#20986;&#26089;&#26399;&#23454;&#39564;&#25104;&#21151;&#12290;&#22312;&#25193;&#25955;&#33539;&#24335;&#20013;&#65292;&#22522;&#24207;&#25903;&#26550;&#34987;&#35270;&#20026;&#19968;&#31181;&#26465;&#20214;&#29983;&#25104;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;&#20960;&#31181;&#26465;&#20214;&#29983;&#25104;&#21327;&#35758;&#25110;&#20174;&#35745;&#31639;&#26426;&#35270;&#35273;&#25991;&#29486;&#20013;&#23548;&#20837;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#21327;&#35758;&#22823;&#22810;&#22522;&#20110;&#21551;&#21457;&#24615;&#21160;&#26426;&#65292;&#20363;&#22914;&#36890;&#36807;&#23545;&#26391;&#20043;&#19975;&#21160;&#21147;&#23398;&#30340;&#31867;&#27604;&#65292;&#24182;&#32570;&#20047;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#20351;&#24471;&#19981;&#21516;&#26041;&#27861;&#20043;&#38388;&#30340;&#32852;&#31995;&#21464;&#24471;&#27169;&#31946;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#22522;&#20110;&#25968;&#23398;&#19978;&#29702;&#35299;&#33391;&#22909;&#30340;Doob's h-transform&#30340;&#20849;&#21516;&#26694;&#26550;&#19979;&#32479;&#19968;&#20102;&#26465;&#20214;&#35757;&#32451;&#21644;&#26465;&#20214;&#25277;&#26679;&#31243;&#24207;&#12290;&#36825;&#31181;&#26032;&#30340;&#35270;&#35282;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#19981;&#21516;&#26041;&#27861;&#20043;&#38388;&#24314;&#31435;&#32852;&#31995;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.09236v2 Announce Type: replace  Abstract: Many protein design applications, such as binder or enzyme design, require scaffolding a structural motif with high precision. Generative modelling paradigms based on denoising diffusion processes emerged as a leading candidate to address this motif scaffolding problem and have shown early experimental success in some cases. In the diffusion paradigm, motif scaffolding is treated as a conditional generation task, and several conditional generation protocols were proposed or imported from the Computer Vision literature. However, most of these protocols are motivated heuristically, e.g. via analogies to Langevin dynamics, and lack a unifying framework, obscuring connections between the different approaches. In this work, we unify conditional training and conditional sampling procedures under one common framework based on the mathematically well-understood Doob's h-transform. This new perspective allows us to draw connections between ex
&lt;/p&gt;</description></item><item><title>Math-Shepherd&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#23398;&#22870;&#21169;&#27169;&#22411;&#65292;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#30340;&#36807;&#31243;&#30417;&#30563;&#25968;&#25454;&#23454;&#29616;LLMs&#30340;&#36880;&#27493;&#39564;&#35777;&#21644;&#21152;&#24378;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.08935</link><description>&lt;p&gt;
Math-Shepherd: &#22312;&#19981;&#38656;&#35201;&#20154;&#24037;&#26631;&#27880;&#30340;&#24773;&#20917;&#19979;&#36880;&#27493;&#39564;&#35777;&#21644;&#21152;&#24378;LLMs
&lt;/p&gt;
&lt;p&gt;
Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.08935
&lt;/p&gt;
&lt;p&gt;
Math-Shepherd&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#23398;&#22870;&#21169;&#27169;&#22411;&#65292;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#30340;&#36807;&#31243;&#30417;&#30563;&#25968;&#25454;&#23454;&#29616;LLMs&#30340;&#36880;&#27493;&#39564;&#35777;&#21644;&#21152;&#24378;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Math-Shepherd&#30340;&#21019;&#26032;&#36807;&#31243;&#23548;&#21521;&#25968;&#23398;&#22870;&#21169;&#27169;&#22411;&#65292;&#20026;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#30340;&#27599;&#19968;&#27493;&#20998;&#37197;&#22870;&#21169;&#20998;&#25968;&#12290;Math-Shepherd&#30340;&#35757;&#32451;&#26159;&#20351;&#29992;&#33258;&#21160;&#26500;&#24314;&#30340;&#22522;&#20110;&#36807;&#31243;&#30340;&#30417;&#30563;&#25968;&#25454;&#23436;&#25104;&#30340;&#65292;&#25171;&#30772;&#20102;&#29616;&#26377;&#24037;&#20316;&#20013;&#23545;&#25163;&#21160;&#26631;&#27880;&#30340;&#20005;&#37325;&#20381;&#36182;&#29942;&#39048;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;Math-Shepherd&#22312;&#20004;&#31181;&#22330;&#26223;&#20013;&#30340;&#26377;&#25928;&#24615;&#65306;1&#65289;\textit{&#39564;&#35777;}&#65306;&#21033;&#29992;Math-Shepherd&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#29983;&#25104;&#30340;&#22810;&#20010;&#36755;&#20986;&#36827;&#34892;&#37325;&#26032;&#25490;&#24207;&#65307;2&#65289;\textit{&#24378;&#21270;&#23398;&#20064;}&#65306;&#20351;&#29992;Math-Shepherd&#36890;&#36807;&#36880;&#27493;&#30340;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;(PPO)&#21152;&#24378;LLMs&#12290;&#36890;&#36807;Math-Shepherd&#65292;&#19968;&#31995;&#21015;&#24320;&#28304;LLMs&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#20363;&#22914;&#65292;&#20351;&#29992;Math-Shepherd&#30340;&#36880;&#27493;PPO&#26174;&#33879;&#25552;&#39640;&#20102;Mistral-7B&#30340;&#20934;&#30830;&#29575;(GSM8K&#30001;77.9%&#25552;&#39640;&#21040;84.1%&#65292;MATH&#30001;28.6%&#25552;&#39640;&#21040;33.0%)
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.08935v3 Announce Type: replace  Abstract: In this paper, we present an innovative process-oriented math process reward model called \textbf{Math-Shepherd}, which assigns a reward score to each step of math problem solutions. The training of Math-Shepherd is achieved using automatically constructed process-wise supervision data, breaking the bottleneck of heavy reliance on manual annotation in existing work. We explore the effectiveness of Math-Shepherd in two scenarios: 1) \textit{Verification}: Math-Shepherd is utilized for reranking multiple outputs generated by Large Language Models (LLMs); 2) \textit{Reinforcement Learning}: Math-Shepherd is employed to reinforce LLMs with step-by-step Proximal Policy Optimization (PPO). With Math-Shepherd, a series of open-source LLMs demonstrates exceptional performance. For instance, the step-by-step PPO with Math-Shepherd significantly improves the accuracy of Mistral-7B (77.9\%$\to$84.1\% on GSM8K and 28.6\%$\to$33.0\% on MATH). The
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;PDE&#27714;&#35299;&#22120;&#65292;&#36890;&#36807;&#26080;&#25968;&#25454;&#31070;&#32463;&#32593;&#26684;&#36866;&#37197;&#22120;&#65288;DMM&#65289;&#35299;&#20915;&#20102;&#31070;&#32463;PDE&#27714;&#35299;&#22120;&#38656;&#35201;&#26114;&#36149;&#32593;&#26684;&#25968;&#25454;&#21644;&#35299;&#20915;&#31354;&#38388;&#30340;&#33258;&#30001;&#24230;&#21644;&#25299;&#25169;&#32467;&#26500;&#21464;&#21270;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2312.05583</link><description>&lt;p&gt;
&#36890;&#36807;&#26080;&#25968;&#25454;&#32593;&#26684;&#31227;&#21160;&#22120;&#23454;&#29616;&#26356;&#22909;&#30340;&#31070;&#32463;PDE&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
Better Neural PDE Solvers Through Data-Free Mesh Movers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.05583
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;PDE&#27714;&#35299;&#22120;&#65292;&#36890;&#36807;&#26080;&#25968;&#25454;&#31070;&#32463;&#32593;&#26684;&#36866;&#37197;&#22120;&#65288;DMM&#65289;&#35299;&#20915;&#20102;&#31070;&#32463;PDE&#27714;&#35299;&#22120;&#38656;&#35201;&#26114;&#36149;&#32593;&#26684;&#25968;&#25454;&#21644;&#35299;&#20915;&#31354;&#38388;&#30340;&#33258;&#30001;&#24230;&#21644;&#25299;&#25169;&#32467;&#26500;&#21464;&#21270;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#31070;&#32463;&#32593;&#32476;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#35299;&#20915;&#29289;&#29702;&#31995;&#32479;&#24314;&#27169;&#20013;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#12290;&#34429;&#28982;&#20027;&#35201;&#30740;&#31350;&#38598;&#20013;&#22312;&#23398;&#20064;&#39044;&#23450;&#20041;&#38745;&#24577;&#32593;&#26684;&#31163;&#25955;&#21270;&#19978;&#30340;&#31995;&#32479;&#28436;&#21270;&#65292;&#20294;&#19968;&#20123;&#26041;&#27861;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#25110;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#26469;&#21019;&#24314;&#36866;&#24212;&#24615;&#21644;&#21160;&#24577;&#32593;&#26684;&#65292;&#30001;&#20110;&#36825;&#20123;&#31995;&#32479;&#30340;&#21160;&#24577;&#29305;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#38754;&#20020;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#65288;1&#65289;&#38656;&#35201;&#26114;&#36149;&#30340;&#26368;&#20339;&#32593;&#26684;&#25968;&#25454;&#65292;&#21644;&#65288;2&#65289;&#22312;&#32593;&#26684;&#32454;&#21270;&#36807;&#31243;&#20013;&#35299;&#20915;&#31354;&#38388;&#30340;&#33258;&#30001;&#24230;&#21644;&#25299;&#25169;&#32467;&#26500;&#30340;&#21464;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#31070;&#32463;&#32593;&#26684;&#36866;&#37197;&#22120;&#30340;&#31070;&#32463;PDE&#27714;&#35299;&#22120;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#25968;&#25454;&#31070;&#32463;&#32593;&#26684;&#36866;&#37197;&#22120;&#65292;&#31216;&#20026;Data-free Mesh Mover&#65288;DMM&#65289;&#65292;&#20855;&#26377;&#20004;&#20010;&#20027;&#35201;&#21019;&#26032;&#28857;&#12290;&#39318;&#20808;&#65292;&#23427;&#26159;&#19968;&#20010;&#25805;&#20316;&#31526;&#65292;&#23558;&#35299;&#26144;&#23556;&#21040;&#33258;&#36866;&#24212;&#32593;&#26684;&#19978;&#65292;&#24182;&#20351;&#29992;Monge-Amp\`ere&#26041;&#31243;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.05583v2 Announce Type: replace-cross  Abstract: Recently, neural networks have been extensively employed to solve partial differential equations (PDEs) in physical system modeling. While major studies focus on learning system evolution on predefined static mesh discretizations, some methods utilize reinforcement learning or supervised learning techniques to create adaptive and dynamic meshes, due to the dynamic nature of these systems. However, these approaches face two primary challenges: (1) the need for expensive optimal mesh data, and (2) the change of the solution space's degree of freedom and topology during mesh refinement. To address these challenges, this paper proposes a neural PDE solver with a neural mesh adapter. To begin with, we introduce a novel data-free neural mesh adaptor, called Data-free Mesh Mover (DMM), with two main innovations. Firstly, it is an operator that maps the solution to adaptive meshes and is trained using the Monge-Amp\`ere equation withou
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#27604;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#23545;&#39118;&#26292;Ciar&#225;n&#30340;&#39044;&#27979;&#65292;&#21457;&#29616;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#20934;&#30830;&#27169;&#25311;&#39640;&#24433;&#21709;&#22825;&#27668;&#20107;&#20214;&#30340;&#22823;&#23610;&#24230;&#32467;&#26500;&#21644;&#21160;&#21147;&#39537;&#21160;&#22120;&#12290;</title><link>https://arxiv.org/abs/2312.02658</link><description>&lt;p&gt;
AI&#27169;&#22411;&#26159;&#21542;&#27604;&#22522;&#20110;&#29289;&#29702;&#30340;&#27169;&#22411;&#33021;&#22815;&#25552;&#20379;&#26356;&#22909;&#30340;&#22825;&#27668;&#39044;&#25253;&#65311;&#23545;&#39118;&#26292;Ciar&#225;n&#30340;&#23450;&#37327;&#35780;&#20272;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Do AI models produce better weather forecasts than physics-based models? A quantitative evaluation case study of Storm Ciar\'an
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.02658
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#27604;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#23545;&#39118;&#26292;Ciar&#225;n&#30340;&#39044;&#27979;&#65292;&#21457;&#29616;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#20934;&#30830;&#27169;&#25311;&#39640;&#24433;&#21709;&#22825;&#27668;&#20107;&#20214;&#30340;&#22823;&#23610;&#24230;&#32467;&#26500;&#21644;&#21160;&#21147;&#39537;&#21160;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#25805;&#20316;&#24615;&#22825;&#27668;&#39044;&#25253;&#30340;&#28508;&#21147;&#22791;&#21463;&#20851;&#27880;&#12290;&#38543;&#30528;&#23427;&#20204;&#25104;&#20026;&#22825;&#27668;&#39044;&#25253;&#24037;&#20855;&#31665;&#30340;&#19968;&#37096;&#20998;&#65292;&#36843;&#20999;&#38656;&#35201;&#20102;&#35299;&#24403;&#21069;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33021;&#22815;&#22810;&#22909;&#22320;&#27169;&#25311;&#39640;&#24433;&#21709;&#22825;&#27668;&#20107;&#20214;&#12290;&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#30001;&#26426;&#22120;&#23398;&#20064;&#21644;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#21046;&#20316;&#30340;&#39118;&#26292;Ciar&#225;n&#30340;&#39044;&#25253;&#65292;&#36825;&#26159;&#19968;&#22330;&#24341;&#21457;&#21271;&#27431;16&#20154;&#27515;&#20129;&#24182;&#36896;&#25104;&#24191;&#27867;&#30772;&#22351;&#30340;&#27431;&#27954;&#39118;&#26292;&#12290;&#32771;&#34385;&#30340;&#22235;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65288;FourCastNet&#65292;Pangu-Weather&#65292;GraphCast&#21644;FourCastNet-v2&#65289;&#20135;&#29983;&#30340;&#39044;&#25253;&#33021;&#22815;&#20934;&#30830;&#25429;&#25417;&#27668;&#26059;&#30340;&#22823;&#23610;&#24230;&#32467;&#26500;&#65292;&#21253;&#25324;&#20113;&#22836;&#20301;&#32622;&#12289;&#26262;&#21306;&#24418;&#29366;&#21644;&#26262;&#27668;&#22260;&#24102;&#21943;&#27969;&#20301;&#32622;&#65292;&#20197;&#21450;&#23545;&#24555;&#36895;&#39118;&#26292;&#21457;&#23637;&#37325;&#35201;&#30340;&#22823;&#23610;&#24230;&#21160;&#21147;&#39537;&#21160;&#22120;&#65292;&#20363;&#22914;&#26292;&#39118;&#30456;&#23545;&#20110;&#39640;&#23618;&#21943;&#27969;&#20986;&#21475;&#30340;&#20301;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.02658v2 Announce Type: replace  Abstract: There has been huge recent interest in the potential of making operational weather forecasts using machine learning techniques. As they become a part of the weather forecasting toolbox, there is a pressing need to understand how well current machine learning models can simulate high-impact weather events. We compare forecasts of Storm Ciar\'an, a European windstorm that caused sixteen deaths and extensive damage in Northern Europe, made by machine learning and numerical weather prediction models. The four machine learning models considered (FourCastNet, Pangu-Weather, GraphCast and FourCastNet-v2) produce forecasts that accurately capture the synoptic-scale structure of the cyclone including the position of the cloud head, shape of the warm sector and location of warm conveyor belt jet, and the large-scale dynamical drivers important for the rapid storm development such as the position of the storm relative to the upper-level jet exi
&lt;/p&gt;</description></item><item><title>LEMR&#26694;&#26550;&#36890;&#36807;&#22312;&#26410;&#26631;&#35760;&#30340;&#39564;&#35777;&#38598;&#20013;&#31574;&#30053;&#24615;&#22320;&#26631;&#27880;&#23454;&#20363;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#27169;&#22411;&#36873;&#25321;&#20013;&#30340;&#26631;&#27880;&#25104;&#26412;&#65292;&#24182;&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#20854;&#39640;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.01619</link><description>&lt;p&gt;
&#20320;&#38656;&#35201;&#22810;&#23569;&#39564;&#35777;&#26631;&#31614;&#65311;&#25506;&#32034;&#26631;&#31614;&#39640;&#25928;&#27169;&#22411;&#25490;&#21517;&#30340;&#35774;&#35745;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
How Many Validation Labels Do You Need? Exploring the Design Space of Label-Efficient Model Ranking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.01619
&lt;/p&gt;
&lt;p&gt;
LEMR&#26694;&#26550;&#36890;&#36807;&#22312;&#26410;&#26631;&#35760;&#30340;&#39564;&#35777;&#38598;&#20013;&#31574;&#30053;&#24615;&#22320;&#26631;&#27880;&#23454;&#20363;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#27169;&#22411;&#36873;&#25321;&#20013;&#30340;&#26631;&#27880;&#25104;&#26412;&#65292;&#24182;&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#20854;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;LEMR&#65288;&#26631;&#31614;&#39640;&#25928;&#27169;&#22411;&#25490;&#21517;&#65289;&#24182;&#20171;&#32461;&#20102;MoraBench&#22522;&#20934;&#27979;&#35797;&#12290;LEMR&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20174;&#26410;&#26631;&#35760;&#30340;&#39564;&#35777;&#38598;&#20013;&#31574;&#30053;&#24615;&#22320;&#26631;&#27880;&#23454;&#20363;&#65292;&#26368;&#22823;&#31243;&#24230;&#22320;&#20943;&#23569;&#20102;&#22312;&#27169;&#22411;&#36873;&#25321;&#20013;&#26114;&#36149;&#27880;&#37322;&#30340;&#38656;&#27714;&#12290;&#20026;&#20102;&#35780;&#20272;LEMR&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;MoraBench&#22522;&#20934;&#27979;&#35797;&#65292;&#36825;&#26159;&#19968;&#20010;&#28085;&#30422;&#20102;&#22810;&#31181;&#22330;&#26223;&#30340;&#27169;&#22411;&#36755;&#20986;&#30340;&#32508;&#21512;&#25910;&#38598;&#12290;&#25105;&#20204;&#22312;&#21322;&#30417;&#30563;&#23398;&#20064;&#12289;&#24369;&#30417;&#30563;&#21644;&#25552;&#31034;&#36873;&#25321;&#20219;&#21153;&#30340;23&#20010;&#19981;&#21516;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;LEMR&#22312;&#26174;&#33879;&#38477;&#20302;&#26631;&#27880;&#25104;&#26412;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#20851;&#38190;&#21457;&#29616;&#31361;&#20986;&#20102;&#21512;&#36866;&#30340;&#38598;&#25104;&#26041;&#27861;&#12289;&#19981;&#30830;&#23450;&#24615;&#37319;&#26679;&#31574;&#30053;&#21644;&#27169;&#22411;&#22996;&#21592;&#20250;&#36873;&#25321;&#23545;&#25552;&#39640;&#27169;&#22411;&#25490;&#21517;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#12290;LEMR&#32467;&#21512;&#20102;MoraBench&#30340;&#35265;&#35299;&#65292;&#20026;&#27169;&#22411;&#36873;&#25321;&#25552;&#20379;&#20102;&#19968;&#31181;&#25104;&#26412;&#25928;&#30410;&#39640;&#19988;&#20934;&#30830;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.01619v3 Announce Type: replace  Abstract: This paper presents LEMR (Label-Efficient Model Ranking) and introduces the MoraBench Benchmark. LEMR is a novel framework that minimizes the need for costly annotations in model selection by strategically annotating instances from an unlabeled validation set. To evaluate LEMR, we leverage the MoraBench Benchmark, a comprehensive collection of model outputs across diverse scenarios. Our extensive evaluation across 23 different NLP tasks in semi-supervised learning, weak supervision, and prompt selection tasks demonstrates LEMR's effectiveness in significantly reducing labeling costs. Key findings highlight the impact of suitable ensemble methods, uncertainty sampling strategies, and model committee selection in enhancing model ranking accuracy. LEMR, supported by the insights from MoraBench, provides a cost-effective and accurate solution for model selection, especially valuable in resource-constrained environments.
&lt;/p&gt;</description></item><item><title>&#26354;&#29575;&#26041;&#21521;&#30340;&#20007;&#22833;&#34987;&#35748;&#20026;&#26159;&#23548;&#33268;&#31070;&#32463;&#32593;&#32476;&#21487;&#22609;&#24615;&#20007;&#22833;&#30340;&#19968;&#20010;&#37325;&#35201;&#21407;&#22240;&#65292;&#24182;&#19988;&#25105;&#20204;&#36890;&#36807;&#31995;&#32479;&#35843;&#26597;&#21644;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#30340;&#30740;&#31350;&#32467;&#26524;&#25903;&#25345;&#20102;&#36825;&#19968;&#35266;&#28857;&#12290;</title><link>https://arxiv.org/abs/2312.00246</link><description>&lt;p&gt;
&#26354;&#29575;&#26041;&#21521;&#20316;&#20026;&#22833;&#21435;&#21487;&#22609;&#24615;&#30340;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Directions of Curvature as an Explanation for Loss of Plasticity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.00246
&lt;/p&gt;
&lt;p&gt;
&#26354;&#29575;&#26041;&#21521;&#30340;&#20007;&#22833;&#34987;&#35748;&#20026;&#26159;&#23548;&#33268;&#31070;&#32463;&#32593;&#32476;&#21487;&#22609;&#24615;&#20007;&#22833;&#30340;&#19968;&#20010;&#37325;&#35201;&#21407;&#22240;&#65292;&#24182;&#19988;&#25105;&#20204;&#36890;&#36807;&#31995;&#32479;&#35843;&#26597;&#21644;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#30340;&#30740;&#31350;&#32467;&#26524;&#25903;&#25345;&#20102;&#36825;&#19968;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#22609;&#24615;&#30340;&#20007;&#22833;&#26159;&#31070;&#32463;&#32593;&#32476;&#20007;&#22833;&#20174;&#26032;&#32463;&#39564;&#23398;&#20064;&#33021;&#21147;&#30340;&#29616;&#35937;&#12290;&#23613;&#31649;&#22312;&#20960;&#31181;&#38382;&#39064;&#35774;&#32622;&#20013;&#32463;&#39564;&#19978;&#35266;&#23519;&#21040;&#65292;&#20294;&#23545;&#23548;&#33268;&#21487;&#22609;&#24615;&#20007;&#22833;&#30340;&#26426;&#21046;&#20102;&#35299;&#29978;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#21487;&#22609;&#24615;&#20007;&#22833;&#30340;&#19968;&#33268;&#35299;&#37322;&#65306;&#31070;&#32463;&#32593;&#32476;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20007;&#22833;&#20102;&#26354;&#29575;&#26041;&#21521;&#65292;&#21487;&#23558;&#21487;&#22609;&#24615;&#30340;&#20007;&#22833;&#24402;&#22240;&#20110;&#36825;&#31181;&#26354;&#29575;&#20943;&#23569;&#12290;&#20026;&#20102;&#25903;&#25345;&#36825;&#26679;&#30340;&#35828;&#27861;&#65292;&#25105;&#20204;&#23545;&#22312;MNIST&#12289;CIFAR-10&#21644;ImageNet&#20013;&#20351;&#29992;&#30340;&#19981;&#26029;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#21487;&#22609;&#24615;&#20007;&#22833;&#36827;&#34892;&#20102;&#31995;&#32479;&#35843;&#26597;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#26354;&#29575;&#26041;&#21521;&#30340;&#20007;&#22833;&#19982;&#21487;&#22609;&#24615;&#30340;&#20007;&#22833;&#30456;&#21563;&#21512;&#65292;&#21516;&#26102;&#36824;&#34920;&#26126;&#20197;&#21069;&#30340;&#35299;&#37322;&#19981;&#36275;&#20197;&#35299;&#37322;&#25152;&#26377;&#24773;&#20917;&#19979;&#30340;&#21487;&#22609;&#24615;&#20007;&#22833;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#32531;&#35299;&#21487;&#22609;&#24615;&#20007;&#22833;&#30340;&#27491;&#21017;&#21270;&#22120;&#20063;&#20250;&#20445;&#30041;&#26354;&#29575;&#65292;&#20419;&#20351;&#37319;&#29992;&#31616;&#21333;&#30340;&#20998;&#24067;&#24335;&#27491;&#21017;&#21270;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.00246v2 Announce Type: replace  Abstract: Loss of plasticity is a phenomenon in which neural networks lose their ability to learn from new experience. Despite being empirically observed in several problem settings, little is understood about the mechanisms that lead to loss of plasticity. In this paper, we offer a consistent explanation for loss of plasticity: Neural networks lose directions of curvature during training and that loss of plasticity can be attributed to this reduction in curvature. To support such a claim, we provide a systematic investigation of loss of plasticity across continual learning tasks using MNIST, CIFAR-10 and ImageNet. Our findings illustrate that loss of curvature directions coincides with loss of plasticity, while also showing that previous explanations are insufficient to explain loss of plasticity in all settings. Lastly, we show that regularizers which mitigate loss of plasticity also preserve curvature, motivating a simple distributional reg
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PA-RL&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#29109;&#29575;&#26469;&#24341;&#23548;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#23637;&#29616;&#21487;&#39044;&#27979;&#30340;&#34892;&#20026;&#12290;&#30740;&#31350;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#24179;&#22343;&#26367;&#20195;&#22870;&#21169;&#23454;&#29616;&#30830;&#23450;&#24615;&#31574;&#30053;&#65292;&#24182;&#22312;&#21160;&#24577;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#36817;&#20284;&#35745;&#31639;&#20540;&#20989;&#25968;&#12290;</title><link>https://arxiv.org/abs/2311.18703</link><description>&lt;p&gt;
&#36890;&#36807;&#29109;&#29575;&#26368;&#23567;&#21270;&#23454;&#29616;&#21487;&#39044;&#27979;&#30340;&#24378;&#21270;&#23398;&#20064;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Predictable Reinforcement Learning Dynamics through Entropy Rate Minimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.18703
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PA-RL&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#29109;&#29575;&#26469;&#24341;&#23548;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#23637;&#29616;&#21487;&#39044;&#27979;&#30340;&#34892;&#20026;&#12290;&#30740;&#31350;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#24179;&#22343;&#26367;&#20195;&#22870;&#21169;&#23454;&#29616;&#30830;&#23450;&#24615;&#31574;&#30053;&#65292;&#24182;&#22312;&#21160;&#24577;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#36817;&#20284;&#35745;&#31639;&#20540;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#26234;&#33021;&#20307;&#27809;&#26377;&#21160;&#26426;&#23637;&#31034;&#21487;&#39044;&#27979;&#30340;&#34892;&#20026;&#65292;&#36890;&#24120;&#36890;&#36807;&#31574;&#30053;&#29109;&#27491;&#21017;&#21270;&#25512;&#21160;&#26234;&#33021;&#20307;&#22312;&#25506;&#32034;&#19978;&#38543;&#26426;&#21270;&#20854;&#34892;&#20026;&#12290;&#20174;&#20154;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#20351;&#24471;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#24456;&#38590;&#35299;&#37322;&#21644;&#39044;&#27979;&#65307;&#20174;&#23433;&#20840;&#35282;&#24230;&#26469;&#30475;&#65292;&#26356;&#38590;&#20197;&#36827;&#34892;&#24418;&#24335;&#21270;&#39564;&#35777;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#21487;&#39044;&#27979;&#24615;&#24863;&#30693;&#24378;&#21270;&#23398;&#20064;&#65288;PA-RL&#65289;&#65292;&#29992;&#20110;&#24341;&#23548;&#26234;&#33021;&#20307;&#23637;&#29616;&#21487;&#39044;&#27979;&#30340;&#34892;&#20026;&#65292;&#20854;&#21033;&#29992;&#29366;&#24577;&#24207;&#21015;&#29109;&#29575;&#20316;&#20026;&#21487;&#39044;&#27979;&#24615;&#24230;&#37327;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#29109;&#29575;&#21046;&#23450;&#20026;&#24179;&#22343;&#22870;&#21169;&#30446;&#26631;&#65292;&#24182;&#19988;&#30001;&#20110;&#20854;&#29109;&#22870;&#21169;&#20989;&#25968;&#20381;&#36182;&#20110;&#31574;&#30053;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21160;&#20316;&#30456;&#20851;&#30340;&#26367;&#20195;&#29109;&#65292;&#20197;&#21033;&#29992;PG&#26041;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#26368;&#23567;&#21270;&#24179;&#22343;&#26367;&#20195;&#22870;&#21169;&#30340;&#30830;&#23450;&#24615;&#31574;&#30053;&#23384;&#22312;&#65292;&#24182;&#19988;&#26368;&#23567;&#21270;&#20102;&#23454;&#38469;&#29109;&#29575;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#23398;&#20064;&#21040;&#30340;&#21160;&#24577;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#36817;&#20284;&#35745;&#31639;&#19982;&#20540;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Reinforcement Learning (RL), agents have no incentive to exhibit predictable behaviors, and are often pushed (through e.g. policy entropy regularization) to randomize their actions in favor of exploration. From a human perspective, this makes RL agents hard to interpret and predict, and from a safety perspective, even harder to formally verify. We propose a novel method to induce predictable behavior in RL agents, referred to as Predictability-Aware RL (PA-RL), which employs the state sequence entropy rate as a predictability measure. We show how the entropy rate can be formulated as an average reward objective, and since its entropy reward function is policy-dependent, we introduce an action-dependent surrogate entropy enabling the use of PG methods. We prove that deterministic policies minimizing the average surrogate reward exist and also minimize the actual entropy rate, and show how, given a learned dynamical model, we are able to approximate the value function associated to th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#29305;&#24449;&#30697;&#38453;&#30340;&#25968;&#25454;&#39537;&#21160;&#26500;&#36896;&#65292;&#29992;&#20110;&#35299;&#20915;&#38750;&#32467;&#26500;&#31232;&#30095;&#24674;&#22797;&#38382;&#39064;&#65292;&#23545;&#20110;&#26679;&#26412;&#20540;&#20013;&#30340;&#22122;&#22768;&#21644;&#26679;&#26412;&#20301;&#32622;&#30340;&#38750;&#32467;&#26500;&#24615;&#36136;&#20855;&#26377;&#24456;&#22909;&#30340;&#36866;&#24212;&#24615;&#12290;</title><link>https://arxiv.org/abs/2311.16609</link><description>&lt;p&gt;
&#29992;&#20110;&#38750;&#32467;&#26500;&#31232;&#30095;&#24674;&#22797;&#30340;&#29305;&#24449;&#30697;&#38453;
&lt;/p&gt;
&lt;p&gt;
Eigenmatrix for unstructured sparse recovery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.16609
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#29305;&#24449;&#30697;&#38453;&#30340;&#25968;&#25454;&#39537;&#21160;&#26500;&#36896;&#65292;&#29992;&#20110;&#35299;&#20915;&#38750;&#32467;&#26500;&#31232;&#30095;&#24674;&#22797;&#38382;&#39064;&#65292;&#23545;&#20110;&#26679;&#26412;&#20540;&#20013;&#30340;&#22122;&#22768;&#21644;&#26679;&#26412;&#20301;&#32622;&#30340;&#38750;&#32467;&#26500;&#24615;&#36136;&#20855;&#26377;&#24456;&#22909;&#30340;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#33324;&#24418;&#24335;&#30340;&#38750;&#32467;&#26500;&#31232;&#30095;&#24674;&#22797;&#38382;&#39064;&#65292;&#21253;&#25324;&#26377;&#29702;&#36924;&#36817;&#12289;&#35889;&#20989;&#25968;&#20272;&#35745;&#12289;&#20613;&#37324;&#21494;&#21453;&#28436;&#12289;&#25289;&#26222;&#25289;&#26031;&#21453;&#28436;&#21644;&#31232;&#30095;&#21453;&#21367;&#31215;&#31561;&#12290;&#20027;&#35201;&#25361;&#25112;&#26159;&#26679;&#26412;&#20540;&#20013;&#30340;&#22122;&#22768;&#21644;&#26679;&#26412;&#20301;&#32622;&#30340;&#38750;&#32467;&#26500;&#24615;&#36136;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#29305;&#24449;&#30697;&#38453;&#65292;&#19968;&#31181;&#20855;&#26377;&#25152;&#38656;&#36817;&#20284;&#29305;&#24449;&#20540;&#21644;&#29305;&#24449;&#21521;&#37327;&#30340;&#25968;&#25454;&#39537;&#21160;&#26500;&#36896;&#65292;&#20026;&#36825;&#20123;&#31232;&#30095;&#24674;&#22797;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#12290;&#25968;&#20540;&#32467;&#26524;&#35777;&#26126;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper considers the unstructured sparse recovery problems in a general form. Examples include rational approximation, spectral function estimation, Fourier inversion, Laplace inversion, and sparse deconvolution. The main challenges are the noise in the sample values and the unstructured nature of the sample locations. This paper proposes the eigenmatrix, a data-driven construction with desired approximate eigenvalues and eigenvectors. The eigenmatrix offers a new way for these sparse recovery problems. Numerical results are provided to demonstrate the efficiency of the proposed method.
&lt;/p&gt;</description></item><item><title>ColaBO&#26159;&#31532;&#19968;&#20010;&#36125;&#21494;&#26031;&#21407;&#29702;&#26694;&#26550;&#65292;&#20801;&#35768;&#39046;&#22495;&#19987;&#23478;&#23450;&#21046;&#20248;&#21270;&#31243;&#24207;&#65292;&#25972;&#21512;&#20808;&#39564;&#20449;&#24565;&#20197;&#21152;&#36895;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2311.14645</link><description>&lt;p&gt;
&#19968;&#33324;&#26694;&#26550;&#29992;&#20110;&#29992;&#25143;&#24341;&#23548;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
A General Framework for User-Guided Bayesian Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.14645
&lt;/p&gt;
&lt;p&gt;
ColaBO&#26159;&#31532;&#19968;&#20010;&#36125;&#21494;&#26031;&#21407;&#29702;&#26694;&#26550;&#65292;&#20801;&#35768;&#39046;&#22495;&#19987;&#23478;&#23450;&#21046;&#20248;&#21270;&#31243;&#24207;&#65292;&#25972;&#21512;&#20808;&#39564;&#20449;&#24565;&#20197;&#21152;&#36895;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26114;&#36149;&#30340;&#40657;&#30418;&#20989;&#25968;&#20248;&#21270;&#22312;&#21508;&#31181;&#31185;&#23398;&#23398;&#31185;&#20013;&#26222;&#36941;&#23384;&#22312;&#12290;&#36125;&#21494;&#26031;&#20248;&#21270;&#26159;&#19968;&#31181;&#33258;&#21160;&#12289;&#36890;&#29992;&#19988;&#26679;&#26412;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26368;&#23567;&#20102;&#35299;&#22522;&#30784;&#20989;&#25968;&#21160;&#24577;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36125;&#21494;&#26031;&#20248;&#21270;&#33021;&#22815;&#25972;&#21512;&#20851;&#20110;&#24453;&#20248;&#21270;&#20989;&#25968;&#30340;&#20808;&#39564;&#30693;&#35782;&#25110;&#20449;&#24565;&#20197;&#21152;&#36895;&#20248;&#21270;&#30340;&#33021;&#21147;&#26377;&#38480;&#65292;&#36825;&#38477;&#20302;&#20102;&#23545;&#20855;&#26377;&#39044;&#31639;&#32039;&#36843;&#30693;&#35782;&#28170;&#21338;&#30340;&#23454;&#36341;&#32773;&#30340;&#21560;&#24341;&#21147;&#12290;&#20026;&#20102;&#20801;&#35768;&#39046;&#22495;&#19987;&#23478;&#23450;&#21046;&#20248;&#21270;&#31243;&#24207;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ColaBO&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#36125;&#21494;&#26031;&#21407;&#29702;&#26694;&#26550;&#65292;&#29992;&#20110;&#25972;&#21512;&#36229;&#20986;&#20856;&#22411;&#26680;&#32467;&#26500;&#30340;&#20808;&#39564;&#20449;&#24565;&#65292;&#22914;&#20248;&#21270;&#22120;&#30340;&#21487;&#33021;&#20301;&#32622;&#25110;&#26368;&#20339;&#20540;&#12290;ColaBO&#30340;&#36890;&#29992;&#24615;&#20351;&#20854;&#36866;&#29992;&#20110;&#19981;&#21516;&#33945;&#29305;&#21345;&#27931;&#25910;&#33719;&#20989;&#25968;&#21644;&#29992;&#25143;&#20449;&#24565;&#30340;&#31867;&#22411;&#12290;&#25105;&#20204;&#32463;&#39564;&#24615;&#22320;&#23637;&#31034;&#20102;ColaBO&#30340;&#33021;&#21147;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.14645v2 Announce Type: replace  Abstract: The optimization of expensive-to-evaluate black-box functions is prevalent in various scientific disciplines. Bayesian optimization is an automatic, general and sample-efficient method to solve these problems with minimal knowledge of the underlying function dynamics. However, the ability of Bayesian optimization to incorporate prior knowledge or beliefs about the function at hand in order to accelerate the optimization is limited, which reduces its appeal for knowledgeable practitioners with tight budgets. To allow domain experts to customize the optimization routine, we propose ColaBO, the first Bayesian-principled framework for incorporating prior beliefs beyond the typical kernel structure, such as the likely location of the optimizer or the optimal value. The generality of ColaBO makes it applicable across different Monte Carlo acquisition functions and types of user beliefs. We empirically demonstrate ColaBO's ability to substa
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#20110;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#24378;&#23545;&#25968;&#20985;&#25968;&#25454;&#20998;&#24067;&#20551;&#35774;&#19979;&#30340;&#23436;&#25972;&#25910;&#25947;&#29702;&#35770;&#20445;&#35777;&#65292;&#33719;&#24471;&#20102;&#23545;&#20110;&#21442;&#25968;&#20272;&#35745;&#21644;&#37319;&#26679;&#31639;&#27861;&#30340;&#26368;&#20248;&#19978;&#38480;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2311.13584</link><description>&lt;p&gt;
&#20851;&#20110;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#21450;&#20854;&#35823;&#24046;&#30028;&#38480;&#65306;&#23436;&#20840;&#25910;&#25947;&#20272;&#35745;&#19979;&#30340;&#23545;&#25968;&#20985;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;
On diffusion-based generative models and their error bounds: The log-concave case with full convergence estimates
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.13584
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#20110;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#24378;&#23545;&#25968;&#20985;&#25968;&#25454;&#20998;&#24067;&#20551;&#35774;&#19979;&#30340;&#23436;&#25972;&#25910;&#25947;&#29702;&#35770;&#20445;&#35777;&#65292;&#33719;&#24471;&#20102;&#23545;&#20110;&#21442;&#25968;&#20272;&#35745;&#21644;&#37319;&#26679;&#31639;&#27861;&#30340;&#26368;&#20248;&#19978;&#38480;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#24378;&#23545;&#25968;&#20985;&#25968;&#25454;&#20998;&#24067;&#30340;&#20551;&#35774;&#19979;&#20026;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#25910;&#25947;&#34892;&#20026;&#25552;&#20379;&#20102;&#23436;&#25972;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#32780;&#25105;&#20204;&#29992;&#20110;&#24471;&#20998;&#20272;&#35745;&#30340;&#36924;&#36817;&#20989;&#25968;&#31867;&#30001;Lipschitz&#36830;&#32493;&#20989;&#25968;&#32452;&#25104;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#28608;&#21169;&#24615;&#20363;&#23376;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#24378;&#22823;&#20043;&#22788;&#65292;&#21363;&#20174;&#20855;&#26377;&#26410;&#30693;&#22343;&#20540;&#30340;&#39640;&#26031;&#20998;&#24067;&#20013;&#36827;&#34892;&#37319;&#26679;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23545;&#30456;&#20851;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#21363;&#24471;&#20998;&#20272;&#35745;&#65292;&#25552;&#20379;&#20102;&#26126;&#30830;&#30340;&#20272;&#35745;&#65292;&#21516;&#26102;&#23558;&#20854;&#19982;&#30456;&#24212;&#30340;&#37319;&#26679;&#20272;&#35745;&#32467;&#21512;&#36215;&#26469;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#26368;&#22909;&#30340;&#24050;&#30693;&#19978;&#38480;&#20272;&#35745;&#65292;&#28041;&#21450;&#20851;&#38190;&#24863;&#20852;&#36259;&#30340;&#25968;&#37327;&#65292;&#22914;&#25968;&#25454;&#20998;&#24067;&#65288;&#20855;&#26377;&#26410;&#30693;&#22343;&#20540;&#30340;&#39640;&#26031;&#20998;&#24067;&#65289;&#19982;&#25105;&#20204;&#30340;&#37319;&#26679;&#31639;&#27861;&#20043;&#38388;&#30340;Wasserstein-2&#36317;&#31163;&#30340;&#32500;&#24230;&#21644;&#25910;&#25947;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.13584v2 Announce Type: replace  Abstract: We provide full theoretical guarantees for the convergence behaviour of diffusion-based generative models under the assumption of strongly log-concave data distributions while our approximating class of functions used for score estimation is made of Lipschitz continuous functions. We demonstrate via a motivating example, sampling from a Gaussian distribution with unknown mean, the powerfulness of our approach. In this case, explicit estimates are provided for the associated optimization problem, i.e. score approximation, while these are combined with the corresponding sampling estimates. As a result, we obtain the best known upper bound estimates in terms of key quantities of interest, such as the dimension and rates of convergence, for the Wasserstein-2 distance between the data distribution (Gaussian with unknown mean) and our sampling algorithm.   Beyond the motivating example and in order to allow for the use of a diverse range o
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;LLMs&#21644;VLMs&#30340;&#26694;&#26550;&#65292;&#20197;&#25214;&#21040;&#26368;&#20339;&#30340;&#31867;&#21035;&#25551;&#36848;&#31526;&#65292;&#35299;&#20915;&#20102;&#22270;&#20687;&#20998;&#31867;&#20013;&#22312;&#31934;&#30830;&#26500;&#24314;&#25991;&#26412;&#34920;&#31034;&#21644;&#21306;&#20998;&#30456;&#20284;&#31867;&#21035;&#26041;&#38754;&#30340;&#25361;&#25112;</title><link>https://arxiv.org/abs/2311.11904</link><description>&lt;p&gt;
LLMs&#20316;&#20026;&#35270;&#35273;&#35299;&#37322;&#22120;&#65306;&#36890;&#36807;&#19981;&#26029;&#28436;&#36827;&#30340;&#35270;&#35273;&#25551;&#36848;&#25552;&#21319;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
LLMs as Visual Explainers: Advancing Image Classification with Evolving Visual Descriptions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.11904
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;LLMs&#21644;VLMs&#30340;&#26694;&#26550;&#65292;&#20197;&#25214;&#21040;&#26368;&#20339;&#30340;&#31867;&#21035;&#25551;&#36848;&#31526;&#65292;&#35299;&#20915;&#20102;&#22270;&#20687;&#20998;&#31867;&#20013;&#22312;&#31934;&#30830;&#26500;&#24314;&#25991;&#26412;&#34920;&#31034;&#21644;&#21306;&#20998;&#30456;&#20284;&#31867;&#21035;&#26041;&#38754;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#36890;&#36807;&#27604;&#36739;&#22270;&#20687;&#19982;&#31867;&#21035;&#23884;&#20837;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#20026;&#22270;&#20687;&#20998;&#31867;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#33539;&#24335;&#12290;&#19968;&#20010;&#20851;&#38190;&#30340;&#25361;&#25112;&#22312;&#20110;&#20026;&#31867;&#21035;&#21517;&#31216;&#26500;&#24314;&#31934;&#30830;&#30340;&#25991;&#26412;&#34920;&#31034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;LLMs&#21644;VLMs&#30340;&#26032;&#26694;&#26550;&#65292;&#20197;&#25214;&#21040;&#26368;&#20339;&#30340;&#31867;&#21035;&#25551;&#36848;&#31526;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.11904v2 Announce Type: replace-cross  Abstract: Vision-language models (VLMs) offer a promising paradigm for image classification by comparing the similarity between images and class embeddings. A critical challenge lies in crafting precise textual representations for class names. While previous studies have leveraged recent advancements in large language models (LLMs) to enhance these descriptors, their outputs often suffer from ambiguity and inaccuracy. We attribute this to two primary factors: 1) the reliance on single-turn textual interactions with LLMs, leading to a mismatch between generated text and visual concepts for VLMs; 2) the oversight of the inter-class relationships, resulting in descriptors that fail to differentiate similar classes effectively. In this paper, we propose a novel framework that integrates LLMs and VLMs to find the optimal class descriptors. Our training-free approach develops an LLM-based agent with an evolutionary optimization strategy to ite
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#20196;&#29260;&#32423;&#21035;&#26816;&#27979;&#23545;&#25239;&#24615;&#25552;&#31034;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#39044;&#27979;&#19979;&#19968;&#20010;&#26631;&#35760;&#30340;&#27010;&#29575;&#12290;</title><link>https://arxiv.org/abs/2311.11509</link><description>&lt;p&gt;
&#22522;&#20110;&#22256;&#24785;&#24230;&#37327;&#21644;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#20196;&#29260;&#32423;&#23545;&#25239;&#25552;&#31034;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Token-Level Adversarial Prompt Detection Based on Perplexity Measures and Contextual Information
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.11509
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#20196;&#29260;&#32423;&#21035;&#26816;&#27979;&#23545;&#25239;&#24615;&#25552;&#31034;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#39044;&#27979;&#19979;&#19968;&#20010;&#26631;&#35760;&#30340;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#24050;&#25104;&#20026;&#21508;&#31181;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25552;&#31034;&#25915;&#20987;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#31934;&#24515;&#31574;&#21010;&#36755;&#20837;&#23383;&#31526;&#20018;&#65292;&#35823;&#23548;LLM&#29983;&#25104;&#19981;&#27491;&#30830;&#25110;&#19981;&#24076;&#26395;&#30340;&#36755;&#20986;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#21033;&#29992;&#31163;&#25955;&#20248;&#21270;&#30340;&#30456;&#23545;&#31616;&#21333;&#21364;&#26377;&#25928;&#30340;&#25915;&#20987;&#26041;&#24335;&#21487;&#20197;&#29983;&#25104;&#32469;&#36807;&#27169;&#22411;&#30340;&#35843;&#25972;&#21644;&#23545;&#40784;&#30340;&#23545;&#25239;&#24615;&#25552;&#31034;&#12290;&#23545;&#23545;&#25239;&#24615;&#25552;&#31034;&#30340;&#33030;&#24369;&#24615;&#20984;&#26174;&#20102;&#23545;LLM&#20581;&#22766;&#24615;&#21644;&#21487;&#38752;&#24615;&#30340;&#37325;&#35201;&#20851;&#27880;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#65292;&#22312;&#20196;&#29260;&#32423;&#21035;&#26816;&#27979;&#23545;&#25239;&#24615;&#25552;&#31034;&#65292;&#21033;&#29992;LLM&#39044;&#27979;&#19979;&#19968;&#20010;&#26631;&#35760;&#30340;&#27010;&#29575;&#33021;&#21147;&#12290;&#25105;&#20204;&#27979;&#37327;&#27169;&#22411;&#22256;&#24785;&#24230;&#30340;&#31243;&#24230;&#65292;&#20854;&#20013;&#39640;&#27010;&#29575;&#39044;&#27979;&#30340;&#20196;&#29260;&#34987;&#35270;&#20026;&#27491;&#24120;&#65292;&#32780;&#37027;&#20123;&#34920;&#29616;&#24322;&#24120;&#30340;&#21017;&#21487;&#33021;&#26159;&#23545;&#25239;&#24615;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.11509v3 Announce Type: replace  Abstract: In recent years, Large Language Models (LLM) have emerged as pivotal tools in various applications. However, these models are susceptible to adversarial prompt attacks, where attackers can carefully curate input strings that mislead LLMs into generating incorrect or undesired outputs. Previous work has revealed that with relatively simple yet effective attacks based on discrete optimization, it is possible to generate adversarial prompts that bypass moderation and alignment of the models. This vulnerability to adversarial prompts underscores a significant concern regarding the robustness and reliability of LLMs. Our work aims to address this concern by introducing a novel approach to detecting adversarial prompts at a token level, leveraging the LLM's capability to predict the next token's probability. We measure the degree of the model's perplexity, where tokens predicted with high probability are considered normal, and those exhibi
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;EarnMore&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;RL&#26041;&#27861;&#65292;&#21487;&#20197;&#20801;&#35768;RL&#20195;&#29702;&#19982;&#21487;&#23450;&#21046;&#32929;&#31080;&#27744;&#65288;CSPs&#65289;&#20132;&#20114;&#65292;&#32780;&#19981;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2311.10801</link><description>&lt;p&gt;
&#20351;&#29992;&#21487;&#23631;&#34109;&#32929;&#31080;&#34920;&#31034;&#30340;&#24378;&#21270;&#23398;&#20064;&#22312;&#21487;&#23450;&#21046;&#32929;&#31080;&#27744;&#20013;&#36827;&#34892;&#25237;&#36164;&#32452;&#21512;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning with Maskable Stock Representation for Portfolio Management in Customizable Stock Pools
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.10801
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;EarnMore&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;RL&#26041;&#27861;&#65292;&#21487;&#20197;&#20801;&#35768;RL&#20195;&#29702;&#19982;&#21487;&#23450;&#21046;&#32929;&#31080;&#27744;&#65288;CSPs&#65289;&#20132;&#20114;&#65292;&#32780;&#19981;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25237;&#36164;&#32452;&#21512;&#31649;&#29702;&#65288;PM&#65289;&#26159;&#19968;&#39033;&#22522;&#26412;&#30340;&#37329;&#34701;&#20132;&#26131;&#20219;&#21153;&#65292;&#25506;&#32034;&#23450;&#26399;&#23558;&#36164;&#37329;&#37325;&#26032;&#37197;&#32622;&#21040;&#19981;&#21516;&#32929;&#31080;&#20013;&#20197;&#36861;&#27714;&#38271;&#26399;&#21033;&#28070;&#12290;&#26368;&#36817;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26174;&#31034;&#20986;&#20854;&#28508;&#21147;&#65292;&#36890;&#36807;&#19982;&#37329;&#34701;&#24066;&#22330;&#20114;&#21160;&#26469;&#35757;&#32451;&#20855;&#26377;&#30408;&#21033;&#33021;&#21147;&#30340;PM&#20195;&#29702;&#12290;&#20294;&#26159;&#65292;&#29616;&#26377;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#22266;&#23450;&#32929;&#31080;&#27744;&#19978;&#65292;&#36825;&#19982;&#25237;&#36164;&#32773;&#30340;&#23454;&#38469;&#38656;&#27714;&#19981;&#19968;&#33268;&#12290;&#20026;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;EarnMore&#65292;&#19968;&#31181;&#26032;&#30340;RL&#26041;&#27861;&#65292;&#21487;&#20197;&#20801;&#35768;RL&#20195;&#29702;&#19982;&#21487;&#23450;&#21046;&#32929;&#31080;&#27744;&#65288;CSPs&#65289;&#20132;&#20114;&#65292;&#32780;&#19981;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.10801v3 Announce Type: replace-cross  Abstract: Portfolio management (PM) is a fundamental financial trading task, which explores the optimal periodical reallocation of capitals into different stocks to pursue long-term profits. Reinforcement learning (RL) has recently shown its potential to train profitable agents for PM through interacting with financial markets. However, existing work mostly focuses on fixed stock pools, which is inconsistent with investors' practical demand. Specifically, the target stock pool of different investors varies dramatically due to their discrepancy on market states and individual investors may temporally adjust stocks they desire to trade (e.g., adding one popular stocks), which lead to customizable stock pools (CSPs). Existing RL methods require to retrain RL agents even with a tiny change of the stock pool, which leads to high computational cost and unstable performance. To tackle this challenge, we propose EarnMore, a rEinforcement leARNin
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#31639;&#27861;FedFish&#65292;&#36890;&#36807;&#21033;&#29992;&#23458;&#25143;&#31471;&#23398;&#20064;&#20989;&#25968;&#30340;&#23616;&#37096;&#36817;&#20284;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#36153;&#33293;&#23572;&#20449;&#24687;&#30340;&#20272;&#35745;&#65292;&#23454;&#29616;&#20102;&#23545;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#20989;&#25968;&#31354;&#38388;&#32858;&#21512;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#20132;&#21449;&#35774;&#22791;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2311.10291</link><description>&lt;p&gt;
&#21033;&#29992;&#20989;&#25968;&#31354;&#38388;&#32858;&#21512;&#36827;&#34892;&#22823;&#35268;&#27169;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Leveraging Function Space Aggregation for Federated Learning at Scale
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.10291
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#31639;&#27861;FedFish&#65292;&#36890;&#36807;&#21033;&#29992;&#23458;&#25143;&#31471;&#23398;&#20064;&#20989;&#25968;&#30340;&#23616;&#37096;&#36817;&#20284;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#36153;&#33293;&#23572;&#20449;&#24687;&#30340;&#20272;&#35745;&#65292;&#23454;&#29616;&#20102;&#23545;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#20989;&#25968;&#31354;&#38388;&#32858;&#21512;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#20132;&#21449;&#35774;&#22791;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#33539;&#24335;&#28608;&#21457;&#20102;&#23558;&#22810;&#20010;&#23458;&#25143;&#31471;&#26356;&#26032;&#32858;&#21512;&#21040;&#20840;&#23616;&#26381;&#21153;&#22120;&#27169;&#22411;&#20013;&#30340;&#26041;&#27861;&#30340;&#21457;&#23637;&#65292;&#32780;&#26080;&#38656;&#20849;&#20139;&#23458;&#25143;&#31471;&#25968;&#25454;&#12290;&#35768;&#22810;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#21253;&#25324;&#32463;&#20856;&#30340;&#32852;&#37030;&#24179;&#22343;&#65288;FedAvg&#65289;&#65292;&#37319;&#29992;&#20102;&#23545;&#23458;&#25143;&#31471;&#21442;&#25968;&#26356;&#26032;&#30340;&#30452;&#25509;&#65288;&#21487;&#33021;&#21152;&#26435;&#65289;&#24179;&#22343;&#20540;&#65292;&#36825;&#26159;&#22522;&#20110;&#20998;&#24067;&#24335;&#20248;&#21270;&#32467;&#26524;&#30340;&#21160;&#26426;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#20989;&#25968;&#31354;&#38388;&#35282;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;FedFish&#65292;&#23427;&#32858;&#21512;&#20102;&#23458;&#25143;&#31471;&#23398;&#20064;&#21040;&#30340;&#20989;&#25968;&#30340;&#23616;&#37096;&#36817;&#20284;&#65292;&#20351;&#29992;&#22522;&#20110;&#36153;&#33293;&#23572;&#20449;&#24687;&#30340;&#20272;&#35745;&#12290;&#25105;&#20204;&#22312;&#23454;&#38469;&#30340;&#22823;&#35268;&#27169;&#36328;&#35774;&#22791;&#22522;&#20934;&#19978;&#35780;&#20272;&#20102;FedFish&#12290;&#34429;&#28982;&#24403;&#23458;&#25143;&#31471;&#27169;&#22411;&#28418;&#31163;&#26102;FedAvg&#30340;&#24615;&#33021;&#21487;&#33021;&#20250;&#21463;&#21040;&#24433;&#21709;&#65292;&#20294;&#25105;&#20204;&#35777;&#26126;FedFish&#23545;&#26356;&#38271;&#30340;&#23616;&#37096;&#35757;&#32451;&#26356;&#20855;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#23545;&#22270;&#20687;&#21644;&#35821;&#35328;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#20960;&#20010;&#35774;&#32622;&#36827;&#34892;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;FedFish&#22312;&#23616;&#37096;&#35757;&#32451;&#20013;&#20248;&#20110;FedAvg&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.10291v2 Announce Type: replace  Abstract: The federated learning paradigm has motivated the development of methods for aggregating multiple client updates into a global server model, without sharing client data. Many federated learning algorithms, including the canonical Federated Averaging (FedAvg), take a direct (possibly weighted) average of the client parameter updates, motivated by results in distributed optimization. In this work, we adopt a function space perspective and propose a new algorithm, FedFish, that aggregates local approximations to the functions learned by clients, using an estimate based on their Fisher information. We evaluate FedFish on realistic, large-scale cross-device benchmarks. While the performance of FedAvg can suffer as client models drift further apart, we demonstrate that FedFish is more robust to longer local training. Our evaluation across several settings in image and language benchmarks shows that FedFish outperforms FedAvg as local train
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#20027;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#26080;&#38656;&#20219;&#21153;&#29305;&#23450;&#30693;&#35782;&#21363;&#21487;&#29983;&#25104;&#36866;&#24212;&#26234;&#33021;&#20307;&#23398;&#20064;&#36827;&#23637;&#30340;&#35838;&#31243;</title><link>https://arxiv.org/abs/2311.09195</link><description>&lt;p&gt;
&#26080;&#38656;&#20219;&#21153;&#29305;&#23450;&#30693;&#35782;&#30340;&#33258;&#30417;&#30563;&#35838;&#31243;&#29983;&#25104;&#29992;&#20110;&#33258;&#20027;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Curriculum Generation for Autonomous Reinforcement Learning without Task-Specific Knowledge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09195
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#20027;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#26080;&#38656;&#20219;&#21153;&#29305;&#23450;&#30693;&#35782;&#21363;&#21487;&#29983;&#25104;&#36866;&#24212;&#26234;&#33021;&#20307;&#23398;&#20064;&#36827;&#23637;&#30340;&#35838;&#31243;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23558;&#24403;&#21069;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#24212;&#29992;&#20110;&#29616;&#23454;&#22330;&#26223;&#26102;&#65292;&#19968;&#20010;&#37325;&#35201;&#29942;&#39048;&#26159;&#38656;&#35201;&#22312;&#27599;&#20010;&#22238;&#21512;&#20043;&#38388;&#37325;&#32622;&#29615;&#22659;&#12290;&#37325;&#32622;&#36807;&#31243;&#38656;&#35201;&#22823;&#37327;&#20154;&#24037;&#24178;&#39044;&#65292;&#20351;&#24471;&#26234;&#33021;&#20307;&#38590;&#20197;&#36830;&#32493;&#21644;&#33258;&#20027;&#23398;&#20064;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#20027;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#29983;&#25104;&#36866;&#24212;&#26234;&#33021;&#20307;&#23398;&#20064;&#36827;&#23637;&#30340;&#35838;&#31243;&#65292;&#32780;&#26080;&#38656;&#20219;&#21153;&#29305;&#23450;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09195v2 Announce Type: replace  Abstract: A significant bottleneck in applying current reinforcement learning algorithms to real-world scenarios is the need to reset the environment between every episode. This reset process demands substantial human intervention, making it difficult for the agent to learn continuously and autonomously. Several recent works have introduced autonomous reinforcement learning (ARL) algorithms that generate curricula for jointly training reset and forward policies. While their curricula can reduce the number of required manual resets by taking into account the agent's learning progress, they rely on task-specific knowledge, such as predefined initial states or reset reward functions. In this paper, we propose a novel ARL algorithm that can generate a curriculum adaptive to the agent's learning progress without task-specific knowledge. Our curriculum empowers the agent to autonomously reset to diverse and informative initial states. To achieve thi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#20559;&#22909;&#20248;&#21270;&#65288;APO&#65289;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#22312;&#27809;&#26377;&#39069;&#22806;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#23545;&#25239;&#23398;&#20064;&#33258;&#36866;&#24212;&#20110;&#29983;&#25104;&#20998;&#24067;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2311.08045</link><description>&lt;p&gt;
&#23545;&#25239;&#20559;&#22909;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Adversarial Preference Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08045
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#20559;&#22909;&#20248;&#21270;&#65288;APO&#65289;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#22312;&#27809;&#26377;&#39069;&#22806;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#23545;&#25239;&#23398;&#20064;&#33258;&#36866;&#24212;&#20110;&#29983;&#25104;&#20998;&#24067;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#20559;&#22909;&#35843;&#25972;&#26159;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20132;&#20114;&#36136;&#37327;&#30340;&#20851;&#38190;&#12290;&#29616;&#26377;&#30340;&#23545;&#40784;&#26041;&#27861;&#20381;&#36182;&#20110;&#25163;&#21160;&#27880;&#37322;&#30340;&#20559;&#22909;&#25968;&#25454;&#26469;&#25351;&#23548;LLM&#30340;&#20248;&#21270;&#26041;&#21521;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#25345;&#32493;&#26356;&#26032;LLMs&#20250;&#23548;&#33268;&#27169;&#22411;&#29983;&#25104;&#26679;&#26412;&#19982;&#20154;&#31867;&#39318;&#36873;&#21709;&#24212;&#20043;&#38388;&#23384;&#22312;&#20998;&#24067;&#24046;&#36317;&#65292;&#36825;&#38459;&#30861;&#20102;&#27169;&#22411;&#24494;&#35843;&#30340;&#25928;&#29575;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#20808;&#21069;&#30340;&#26041;&#27861;&#38656;&#35201;&#22312;&#29983;&#25104;&#30340;&#26679;&#26412;&#19978;&#39069;&#22806;&#36827;&#34892;&#20559;&#22909;&#27880;&#37322;&#65292;&#20197;&#36866;&#24212;&#36716;&#31227;&#20998;&#24067;&#65292;&#36825;&#38656;&#35201;&#22823;&#37327;&#30340;&#27880;&#37322;&#36164;&#28304;&#12290;&#38024;&#23545;&#26356;&#39640;&#25928;&#30340;&#20154;&#31867;&#20559;&#22909;&#20248;&#21270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#20559;&#22909;&#20248;&#21270;&#65288;APO&#65289;&#26694;&#26550;&#65292;&#20854;&#20013;LLM&#20195;&#29702;&#21644;&#20559;&#22909;&#27169;&#22411;&#36890;&#36807;&#26497;&#23567;-&#26497;&#22823;&#21338;&#24328;&#20132;&#26367;&#26356;&#26032;&#12290;&#22312;&#27809;&#26377;&#39069;&#22806;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;APO&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#23545;&#25239;&#23398;&#20064;&#33258;&#36866;&#24212;&#20110;&#29983;&#25104;&#20998;&#24067;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.08045v2 Announce Type: replace-cross  Abstract: Human preference alignment is essential to improve the interaction quality of large language models (LLMs). Existing aligning methods depend on manually annotated preference data to guide the LLM optimization directions. However, in practice, continuously updating LLMs raises a distribution gap between model-generated samples and human-preferred responses, which hinders model fine-tuning efficiency. To mitigate this issue, previous methods require additional preference annotation on generated samples to adapt the shifted distribution, which consumes a large amount of annotation resources. Targeting more efficient human preference optimization, we propose an adversarial preference optimization (APO) framework, where the LLM agent and the preference model update alternatively via a min-max game. Without additional annotation, our APO method can make a self-adaption to the generation distribution gap through the adversarial learni
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Residual Message Graph Convolution Network&#65288;ResMGCN&#65289;&#65292;&#29992;&#20110;&#24555;&#36895;&#32780;&#20934;&#30830;&#22320;&#39044;&#27979;&#29983;&#29289;&#21307;&#23398;&#30456;&#20114;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2311.07632</link><description>&lt;p&gt;
ResMGCN&#65306;&#29992;&#20110;&#24555;&#36895;&#29983;&#29289;&#21307;&#23398;&#30456;&#20114;&#20316;&#29992;&#21457;&#29616;&#30340;&#27531;&#24046;&#28040;&#24687;&#22270;&#21367;&#31215;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
ResMGCN: Residual Message Graph Convolution Network for Fast Biomedical Interactions Discovering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.07632
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Residual Message Graph Convolution Network&#65288;ResMGCN&#65289;&#65292;&#29992;&#20110;&#24555;&#36895;&#32780;&#20934;&#30830;&#22320;&#39044;&#27979;&#29983;&#29289;&#21307;&#23398;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#20449;&#24687;&#22270;&#23545;&#20110;&#29616;&#20195;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20013;&#30340;&#29983;&#29289;&#21307;&#23398;&#20449;&#24687;&#30456;&#20114;&#20316;&#29992;&#30340;&#21457;&#29616;&#33267;&#20851;&#37325;&#35201;&#65292;&#27604;&#22914;&#22810;&#26679;&#21270;&#20998;&#23376;&#30456;&#20114;&#20316;&#29992;&#30340;&#35782;&#21035;&#21644;&#33647;&#29289;&#21457;&#29616;&#65292;&#22312;&#29983;&#29289;&#21307;&#23398;&#12289;&#29983;&#29289;&#20449;&#24687;&#23398;&#21644;&#20154;&#31867;&#20581;&#24247;&#31038;&#21306;&#20013;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#30446;&#21069;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#34987;&#25552;&#20986;&#26469;&#23398;&#20064;&#29983;&#29289;&#21307;&#23398;&#20449;&#24687;&#23454;&#20307;&#65292;&#24182;&#36890;&#36807;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#20934;&#30830;&#25581;&#31034;&#29983;&#29289;&#21307;&#23398;&#20998;&#23376;&#30456;&#20114;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#34429;&#28982;&#24357;&#34917;&#20102;&#36828;&#36317;&#31163;&#29305;&#24449;&#30340;&#34928;&#20943;&#65292;&#20294;&#21364;&#20197;&#20887;&#20313;&#30340;&#20869;&#23384;&#21644;&#26102;&#38388;&#20026;&#20195;&#20215;&#12290;&#22312;&#25105;&#20204;&#30340;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Residual Message Graph Convolution Network&#65288;ResMGCN&#65289;&#65292;&#29992;&#20110;&#20197;&#19968;&#31181;&#19981;&#21516;&#30340;&#24605;&#36335;&#24555;&#36895;&#32780;&#20934;&#30830;&#22320;&#39044;&#27979;&#29983;&#29289;&#21307;&#23398;&#30456;&#20114;&#20316;&#29992;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;ResMGCN&#19981;&#26159;&#22686;&#24378;&#36828;&#31243;&#33410;&#28857;&#30340;&#28040;&#24687;&#65292;&#32780;&#26159;&#19982;&#19979;&#19968;&#36718;&#39640;&#38454;&#20449;&#24687;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.07632v2 Announce Type: replace-cross  Abstract: Biomedical information graphs are crucial for interaction discovering of biomedical information in modern age, such as identification of multifarious molecular interactions and drug discovery, which attracts increasing interests in biomedicine, bioinformatics, and human healthcare communities. Nowadays, more and more graph neural networks have been proposed to learn the entities of biomedical information and precisely reveal biomedical molecule interactions with state-of-the-art results. These methods remedy the fading of features from a far distance but suffer from remedying such problem at the expensive cost of redundant memory and time. In our paper, we propose a novel Residual Message Graph Convolution Network (ResMGCN) for fast and precise biomedical interaction prediction in a different idea. Specifically, instead of enhancing the message from far nodes, ResMGCN aggregates lower-order information with the next round highe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#20195;&#25968;&#20219;&#21153;&#20013;&#23398;&#21040;&#30340;&#29305;&#24449;&#65292;&#21457;&#29616;&#36793;&#30028;&#26368;&#22823;&#21270;&#21407;&#21017;&#21487;&#20197;&#23436;&#20840;&#25351;&#23450;&#32593;&#32476;&#23398;&#21040;&#30340;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2311.07568</link><description>&lt;p&gt;
&#29305;&#24449;&#36890;&#36807;&#36793;&#30028;&#26368;&#22823;&#21270;&#30340;&#20986;&#29616;&#65306;&#20195;&#25968;&#20219;&#21153;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Feature emergence via margin maximization: case studies in algebraic tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.07568
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#20195;&#25968;&#20219;&#21153;&#20013;&#23398;&#21040;&#30340;&#29305;&#24449;&#65292;&#21457;&#29616;&#36793;&#30028;&#26368;&#22823;&#21270;&#21407;&#21017;&#21487;&#20197;&#23436;&#20840;&#25351;&#23450;&#32593;&#32476;&#23398;&#21040;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#30340;&#20869;&#37096;&#34920;&#31034;&#26159;&#26426;&#22120;&#23398;&#20064;&#31185;&#23398;&#20013;&#30340;&#19968;&#20010;&#22522;&#30707;&#24615;&#25361;&#25112;&#12290;&#34429;&#28982;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36817;&#26399;&#24050;&#32463;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20197;&#20102;&#35299;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#23454;&#29616;&#29305;&#23450;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#20294;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#20010;&#20114;&#34917;&#30340;&#38382;&#39064;&#8212;&#8212;&#32593;&#32476;&#20026;&#20309;&#20250;&#37319;&#29992;&#29305;&#23450;&#30340;&#35745;&#31639;&#31574;&#30053;&#65311;&#25105;&#20204;&#30340;&#30740;&#31350;&#37325;&#28857;&#25918;&#22312;&#27169;&#22359;&#21270;&#21152;&#27861;&#12289;&#31232;&#30095;&#22855;&#20598;&#24615;&#21644;&#26377;&#38480;&#32676;&#25805;&#20316;&#30340;&#20195;&#25968;&#23398;&#20064;&#20219;&#21153;&#19978;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#29702;&#35770;&#21457;&#29616;&#36890;&#36807;&#20998;&#26512;&#30340;&#26041;&#27861;&#23545;&#36825;&#20123;&#20195;&#25968;&#20219;&#21153;&#30340;&#31070;&#32463;&#32593;&#32476;&#23398;&#21040;&#30340;&#29305;&#24449;&#36827;&#34892;&#20102;&#34920;&#24449;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#20027;&#35201;&#25216;&#26415;&#23637;&#31034;&#20102;&#36793;&#30028;&#26368;&#22823;&#21270;&#21407;&#21017;&#22914;&#20309;&#21333;&#29420;&#29992;&#20110;&#23436;&#20840;&#25351;&#23450;&#32593;&#32476;&#23398;&#21040;&#30340;&#29305;&#24449;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35777;&#26126;&#35757;&#32451;&#21518;&#30340;&#32593;&#32476;&#21033;&#29992;&#20613;&#37324;&#21494;&#29305;&#24449;&#25191;&#34892;&#27169;&#22359;&#21270;&#21152;&#27861;&#65292;&#24182;&#20351;&#29992;&#19982;&#19981;&#21487;&#32422; gr
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.07568v2 Announce Type: replace  Abstract: Understanding the internal representations learned by neural networks is a cornerstone challenge in the science of machine learning. While there have been significant recent strides in some cases towards understanding how neural networks implement specific target functions, this paper explores a complementary question -- why do networks arrive at particular computational strategies? Our inquiry focuses on the algebraic learning tasks of modular addition, sparse parities, and finite group operations. Our primary theoretical findings analytically characterize the features learned by stylized neural networks for these algebraic tasks. Notably, our main technique demonstrates how the principle of margin maximization alone can be used to fully specify the features learned by the network. Specifically, we prove that the trained networks utilize Fourier features to perform modular addition and employ features corresponding to irreducible gr
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#27491;&#24120;&#32467;&#26500;&#35268;&#33539;&#21270;&#26041;&#27861;&#65292;&#23454;&#29616;&#24320;&#25918;&#22270;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#23545;&#26410;&#30693;&#24322;&#24120;&#30340;&#24191;&#20041;&#26816;&#27979;&#33021;&#21147;</title><link>https://arxiv.org/abs/2311.06835</link><description>&lt;p&gt;
&#36890;&#36807;&#27491;&#24120;&#32467;&#26500;&#35268;&#33539;&#21270;&#23454;&#29616;&#24320;&#25918;&#22270;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Open-Set Graph Anomaly Detection via Normal Structure Regularisation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.06835
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27491;&#24120;&#32467;&#26500;&#35268;&#33539;&#21270;&#26041;&#27861;&#65292;&#23454;&#29616;&#24320;&#25918;&#22270;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#23545;&#26410;&#30693;&#24322;&#24120;&#30340;&#24191;&#20041;&#26816;&#27979;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#22270;&#24322;&#24120;&#26816;&#27979;&#65288;GAD&#65289;&#20219;&#21153;&#65292;&#21363;&#24320;&#25918;&#24335;GAD&#65292;&#26088;&#22312;&#20351;&#29992;&#23569;&#37327;&#26631;&#35760;&#30340;&#35757;&#32451;&#27491;&#24120;&#33410;&#28857;&#21644;&#24322;&#24120;&#33410;&#28857;&#65288;&#31216;&#20026;&#24050;&#30693;&#24322;&#24120;&#65289;&#26469;&#26816;&#27979;&#24322;&#24120;&#33410;&#28857;&#65292;&#36825;&#20123;&#33410;&#28857;&#26080;&#27861;&#23637;&#31034;&#25152;&#26377;&#21487;&#33021;&#30340;&#25512;&#29702;&#26102;&#24322;&#24120;&#12290;&#24050;&#26631;&#35760;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#20026;GAD&#27169;&#22411;&#25552;&#20379;&#20102;&#20851;&#38190;&#30340;&#24322;&#24120;&#20808;&#39564;&#30693;&#35782;&#65292;&#21487;&#22823;&#22823;&#38477;&#20302;&#26816;&#27979;&#38169;&#35823;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#26041;&#27861;&#24448;&#24448;&#36807;&#20998;&#24378;&#35843;&#25311;&#21512;&#24050;&#30693;&#24322;&#24120;&#65292;&#23548;&#33268;&#23545;&#26410;&#30693;&#24322;&#24120;&#65288;&#21363;&#26410;&#34987;&#26631;&#35760;&#30340;&#24322;&#24120;&#33410;&#28857;&#65289;&#30340;&#24369;&#27867;&#21270;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#34987;&#24341;&#20837;&#20197;&#22788;&#29702;&#27431;&#20960;&#37324;&#24503;&#25968;&#25454;&#65292;&#26410;&#33021;&#26377;&#25928;&#25429;&#25417;GAD&#30340;&#37325;&#35201;&#38750;&#27431;&#20960;&#37324;&#24503;&#29305;&#24449;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24320;&#25918;&#24335;GAD&#26041;&#27861;&#65292;&#21363;&#27491;&#24120;&#32467;&#26500;&#35268;&#33539;&#21270;&#65288;NSReg&#65289;&#65292;&#20197;&#23454;&#29616;&#23545;&#26410;&#30693;&#24322;&#24120;&#30340;&#24191;&#20041;&#26816;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.06835v2 Announce Type: replace-cross  Abstract: This paper considers an important Graph Anomaly Detection (GAD) task, namely open-set GAD, which aims to detect anomalous nodes using a small number of labelled training normal and anomaly nodes (known as seen anomalies) that cannot illustrate all possible inference-time abnormalities. The availability of that labelled data provides crucial prior knowledge about abnormalities for GAD models, enabling substantially reduced detection errors. However, current methods tend to over-emphasise fitting the seen anomalies, leading to a weak generalisation ability to detect unseen anomalies, i.e., those that are not illustrated by the labelled anomaly nodes. Further, they were introduced to handle Euclidean data, failing to effectively capture important non-Euclidean features for GAD. In this work, we propose a novel open-set GAD approach, namely Normal Structure Regularisation (NSReg), to achieve generalised detection ability to unseen 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#19988;&#36890;&#29992;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#29992;&#25143;&#19982;&#25628;&#32034;&#24341;&#25806;&#30340;&#20132;&#20114;&#21382;&#21490;&#20013;&#25552;&#21462;&#30456;&#20851;&#19978;&#19979;&#25991;&#26469;&#20010;&#24615;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#25913;&#36827;&#32593;&#32476;&#25628;&#32034;&#20307;&#39564;&#12290;</title><link>https://arxiv.org/abs/2311.06318</link><description>&lt;p&gt;
&#30693;&#35782;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#20010;&#24615;&#21270;&#19978;&#19979;&#25991;&#26597;&#35810;&#24314;&#35758;
&lt;/p&gt;
&lt;p&gt;
Knowledge-Augmented Large Language Models for Personalized Contextual Query Suggestion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.06318
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#19988;&#36890;&#29992;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#29992;&#25143;&#19982;&#25628;&#32034;&#24341;&#25806;&#30340;&#20132;&#20114;&#21382;&#21490;&#20013;&#25552;&#21462;&#30456;&#20851;&#19978;&#19979;&#25991;&#26469;&#20010;&#24615;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#25913;&#36827;&#32593;&#32476;&#25628;&#32034;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25797;&#38271;&#35299;&#20915;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#37325;&#26032;&#35757;&#32451;&#25110;&#24494;&#35843;&#23427;&#20204;&#25152;&#28041;&#21450;&#30340;&#25104;&#26412;&#24040;&#22823;&#65292;&#23427;&#20204;&#20173;&#28982;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#26159;&#38745;&#24577;&#30340;&#65292;&#24182;&#19988;&#38590;&#20197;&#20010;&#24615;&#21270;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#35768;&#22810;&#24212;&#29992;&#31243;&#24207;&#21487;&#20197;&#20174;&#26681;&#25454;&#29992;&#25143;&#30340;&#20559;&#22909;&#12289;&#30446;&#26631;&#21644;&#30693;&#35782;&#37327;&#23450;&#21046;&#30340;&#29983;&#25104;&#20013;&#21463;&#30410;&#12290;&#20854;&#20013;&#20043;&#19968;&#26159;&#32593;&#32476;&#25628;&#32034;&#65292;&#20102;&#35299;&#29992;&#25143;&#35797;&#22270;&#20570;&#20160;&#20040;&#12289;&#20851;&#24515;&#20160;&#20040;&#20197;&#21450;&#20182;&#20204;&#30693;&#36947;&#20160;&#20040;&#21487;&#20197;&#25552;&#39640;&#25628;&#32034;&#20307;&#39564;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#36890;&#29992;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#29992;&#25143;&#19982;&#25628;&#32034;&#24341;&#25806;&#30340;&#20132;&#20114;&#21382;&#21490;&#20013;&#30340;&#30456;&#20851;&#19978;&#19979;&#25991;&#26469;&#22686;&#24378;LLM&#20197;&#20010;&#24615;&#21270;&#20854;&#36755;&#20986;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26681;&#25454;&#29992;&#25143;&#22312;&#32593;&#32476;&#19978;&#30340;&#25628;&#32034;&#21644;&#27983;&#35272;&#27963;&#21160;&#26500;&#24314;&#20102;&#27599;&#20010;&#29992;&#25143;&#30340;&#20197;&#23454;&#20307;&#20026;&#20013;&#24515;&#30340;&#30693;&#35782;&#23384;&#20648;&#65292;&#28982;&#21518;&#21033;&#29992;&#36825;&#20123;&#30693;&#35782;&#20026;LLM&#25552;&#20379;&#20855;&#26377;&#19978;&#19979;&#25991;&#30456;&#20851;&#24615;&#30340;&#25552;&#31034;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.06318v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) excel at tackling various natural language tasks. However, due to the significant costs involved in re-training or fine-tuning them, they remain largely static and difficult to personalize. Nevertheless, a variety of applications could benefit from generations that are tailored to users' preferences, goals, and knowledge. Among them is web search, where knowing what a user is trying to accomplish, what they care about, and what they know can lead to improved search experiences. In this work, we propose a novel and general approach that augments an LLM with relevant context from users' interaction histories with a search engine in order to personalize its outputs. Specifically, we construct an entity-centric knowledge store for each user based on their search and browsing activities on the web, which is then leveraged to provide contextually relevant LLM prompt augmentations. This knowledge store is 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#29369;&#35947;&#27169;&#31946;&#38598;&#30340;&#22810;&#31181;&#21253;&#21547;&#20851;&#31995;&#23450;&#20041;&#12289;&#29369;&#35947;&#27169;&#31946;&#20449;&#24687;&#31995;&#32479;&#30340;&#22522;&#30784;&#21629;&#39064;&#21644;&#22522;&#20110;&#22810;&#24378;&#24230;&#26234;&#33021;&#20998;&#31867;&#22120;&#30340;&#20581;&#24247;&#29366;&#24577;&#35786;&#26029;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2311.04256</link><description>&lt;p&gt;
&#29369;&#35947;&#27169;&#31946;&#38598;&#21450;&#20854;&#24212;&#29992;&#20110;&#22810;&#24378;&#24230;&#26234;&#33021;&#20998;&#31867;&#22120;&#30340;&#22522;&#30784;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Foundational theories of hesitant fuzzy sets and hesitant fuzzy information systems and their applications for multi-strength intelligent classifiers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.04256
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#29369;&#35947;&#27169;&#31946;&#38598;&#30340;&#22810;&#31181;&#21253;&#21547;&#20851;&#31995;&#23450;&#20041;&#12289;&#29369;&#35947;&#27169;&#31946;&#20449;&#24687;&#31995;&#32479;&#30340;&#22522;&#30784;&#21629;&#39064;&#21644;&#22522;&#20110;&#22810;&#24378;&#24230;&#26234;&#33021;&#20998;&#31867;&#22120;&#30340;&#20581;&#24247;&#29366;&#24577;&#35786;&#26029;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29369;&#35947;&#27169;&#31946;&#38598;&#22312;&#26576;&#20123;&#19981;&#30830;&#23450;&#21644;&#29369;&#35947;&#30340;&#24773;&#20917;&#19979;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#22312;&#38598;&#21512;&#20013;&#65292;&#21253;&#21547;&#20851;&#31995;&#26159;&#19968;&#20010;&#37325;&#35201;&#19988;&#22522;&#30784;&#30340;&#23450;&#20041;&#12290;&#22240;&#27492;&#65292;&#20316;&#20026;&#19968;&#31181;&#38598;&#21512;&#65292;&#29369;&#35947;&#27169;&#31946;&#38598;&#38656;&#35201;&#19968;&#20010;&#26126;&#30830;&#30340;&#21253;&#21547;&#20851;&#31995;&#23450;&#20041;&#12290;&#22522;&#20110;&#31163;&#25955;&#24418;&#24335;&#30340;&#29369;&#35947;&#27169;&#31946;&#38582;&#23646;&#24230;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#20960;&#31181;&#36866;&#29992;&#20110;&#29369;&#35947;&#27169;&#31946;&#38598;&#30340;&#21253;&#21547;&#20851;&#31995;&#12290;&#38543;&#21518;&#65292;&#20171;&#32461;&#20102;&#19968;&#20123;&#29369;&#35947;&#27169;&#31946;&#38598;&#30340;&#22522;&#30784;&#21629;&#39064;&#65292;&#20197;&#21450;&#29369;&#35947;&#27169;&#31946;&#38598;&#26063;&#30340;&#21629;&#39064;&#12290;&#38024;&#23545;&#21442;&#25968;&#20943;&#23569;&#65292;&#25552;&#20986;&#20102;&#29369;&#35947;&#27169;&#31946;&#20449;&#24687;&#31995;&#32479;&#30340;&#19968;&#20123;&#22522;&#30784;&#21629;&#39064;&#65292;&#24182;&#32473;&#20986;&#20102;&#19968;&#20010;&#31034;&#20363;&#21644;&#31639;&#27861;&#26469;&#35828;&#26126;&#21442;&#25968;&#20943;&#23569;&#30340;&#36807;&#31243;&#12290;&#26368;&#21518;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#24378;&#24230;&#26234;&#33021;&#20998;&#31867;&#22120;&#65292;&#29992;&#20110;&#23545;&#22797;&#26434;&#31995;&#32479;&#36827;&#34892;&#20581;&#24247;&#29366;&#24577;&#35786;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.04256v3 Announce Type: replace  Abstract: Hesitant fuzzy sets are widely used in certain instances of uncertainty and hesitation. In sets, the inclusion relationship is an important and foundational definition. Thus, as a kind of set, hesitant fuzzy sets require an explicit definition of inclusion relationship. Based on the hesitant fuzzy membership degree of discrete form, several kinds of inclusion relationships for hesitant fuzzy sets are proposed in this work. Then, some foundational propositions of hesitant fuzzy sets are presented, along with propositions of families of hesitant fuzzy sets. Some foundational propositions of hesitant fuzzy information systems are proposed with respect to parameter reductions and an example and an algorithm are given to illustrate the processes of parameter reduction. Finally, a multi-strength intelligent classifier is proposed to make health state diagnoses for complex systems.
&lt;/p&gt;</description></item><item><title>DreamSmooth&#36890;&#36807;&#23398;&#20064;&#39044;&#27979;&#26102;&#38388;&#24179;&#28369;&#22870;&#21169;&#32780;&#38750;&#31934;&#30830;&#22870;&#21169;&#65292;&#20248;&#21270;&#20102;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#65292;&#22312;&#31232;&#30095;&#22870;&#21169;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2311.01450</link><description>&lt;p&gt;
DreamSmooth&#65306;&#36890;&#36807;&#22870;&#21169;&#24179;&#28369;&#25913;&#36827;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DreamSmooth: Improving Model-based Reinforcement Learning via Reward Smoothing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.01450
&lt;/p&gt;
&lt;p&gt;
DreamSmooth&#36890;&#36807;&#23398;&#20064;&#39044;&#27979;&#26102;&#38388;&#24179;&#28369;&#22870;&#21169;&#32780;&#38750;&#31934;&#30830;&#22870;&#21169;&#65292;&#20248;&#21270;&#20102;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#65292;&#22312;&#31232;&#30095;&#22870;&#21169;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25688;&#35201;: &#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;MBRL&#65289;&#22240;&#20854;&#20197;&#33410;&#32422;&#26679;&#26412;&#30340;&#26041;&#24335;&#23398;&#20064;&#22797;&#26434;&#34892;&#20026;&#30340;&#33021;&#21147;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#65306;&#36890;&#36807;&#29983;&#25104;&#20855;&#26377;&#39044;&#27979;&#22870;&#21169;&#30340;&#34394;&#25311;&#36712;&#36857;&#26469;&#35268;&#21010;&#21160;&#20316;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#22870;&#21169;&#39044;&#27979;&#36890;&#24120;&#26159;MBRL&#30340;&#29942;&#39048;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#38590;&#20197;&#39044;&#27979;&#30340;&#31232;&#30095;&#22870;&#21169;&#12290;&#21463;&#21040;&#20154;&#31867;&#20174;&#31895;&#31961;&#22870;&#21169;&#20272;&#35745;&#20013;&#23398;&#20064;&#30340;&#30452;&#35273;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22870;&#21169;&#24179;&#28369;&#26041;&#27861;DreamSmooth&#65292;&#23427;&#23398;&#20064;&#39044;&#27979;&#19968;&#20010;&#22312;&#32473;&#23450;&#26102;&#38388;&#27493;&#30340;&#22870;&#21169;&#30340;&#26102;&#38388;&#24179;&#28369;&#29256;&#26412;&#65292;&#32780;&#19981;&#26159;&#31934;&#30830;&#22870;&#21169;&#12290;&#25105;&#20204;&#22312;&#23454;&#35777;&#19978;&#23637;&#31034;DreamSmooth&#22312;&#38271;&#35270;&#37326;&#31232;&#30095;&#22870;&#21169;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#26082;&#22312;&#26679;&#26412;&#25928;&#29575;&#21644;&#26368;&#32456;&#24615;&#33021;&#19978;&#65292;&#21448;&#19981;&#25439;&#22833;&#22312;&#24120;&#35265;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#24615;&#33021;&#65292;&#22914;Deepmind Control Suite&#21644;Atari&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.01450v2 Announce Type: replace-cross  Abstract: Model-based reinforcement learning (MBRL) has gained much attention for its ability to learn complex behaviors in a sample-efficient way: planning actions by generating imaginary trajectories with predicted rewards. Despite its success, we found that surprisingly, reward prediction is often a bottleneck of MBRL, especially for sparse rewards that are challenging (or even ambiguous) to predict. Motivated by the intuition that humans can learn from rough reward estimates, we propose a simple yet effective reward smoothing approach, DreamSmooth, which learns to predict a temporally-smoothed reward, instead of the exact reward at the given timestep. We empirically show that DreamSmooth achieves state-of-the-art performance on long-horizon sparse-reward tasks both in sample efficiency and final performance without losing performance on common benchmarks, such as Deepmind Control Suite and Atari benchmarks.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;Syntheseus&#24314;&#31435;&#30340;&#22522;&#20934;&#24211;&#37325;&#26032;&#35780;&#20272;&#20102;&#22238;&#28335;&#21512;&#25104;&#31639;&#27861;&#65292;&#25581;&#31034;&#20102;&#29616;&#26377;&#25216;&#26415;&#27169;&#22411;&#30340;&#31995;&#32479;&#24615;&#32570;&#38519;&#24182;&#25552;&#20379;&#20102;&#23545;&#26410;&#26469;&#24037;&#20316;&#30340;&#25351;&#23548;&#24314;&#35758;&#12290;</title><link>https://arxiv.org/abs/2310.19796</link><description>&lt;p&gt;
&#20351;&#29992;Syntheseus&#37325;&#26032;&#35780;&#20272;&#22238;&#28335;&#21512;&#25104;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Re-evaluating Retrosynthesis Algorithms with Syntheseus
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.19796
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;Syntheseus&#24314;&#31435;&#30340;&#22522;&#20934;&#24211;&#37325;&#26032;&#35780;&#20272;&#20102;&#22238;&#28335;&#21512;&#25104;&#31639;&#27861;&#65292;&#25581;&#31034;&#20102;&#29616;&#26377;&#25216;&#26415;&#27169;&#22411;&#30340;&#31995;&#32479;&#24615;&#32570;&#38519;&#24182;&#25552;&#20379;&#20102;&#23545;&#26410;&#26469;&#24037;&#20316;&#30340;&#25351;&#23548;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#20960;&#24180;&#65292;&#20998;&#23376;&#21512;&#25104;&#35268;&#21010;&#65292;&#20063;&#31216;&#20026;&#22238;&#28335;&#21512;&#25104;&#65292;&#24050;&#32463;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#21644;&#21270;&#23398;&#30028;&#20851;&#27880;&#30340;&#28966;&#28857;&#12290;&#23613;&#31649;&#30475;&#20284;&#21462;&#24471;&#20102;&#31283;&#23450;&#30340;&#36827;&#23637;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#23384;&#22312;&#19981;&#23436;&#21892;&#30340;&#22522;&#20934;&#21644;&#19981;&#19968;&#33268;&#30340;&#27604;&#36739;&#25513;&#30422;&#20102;&#29616;&#26377;&#25216;&#26415;&#30340;&#31995;&#32479;&#24615;&#32570;&#38519;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;syntheseus&#30340;&#22522;&#20934;&#24211;&#65292;&#36890;&#36807;&#40664;&#35748;&#25512;&#24191;&#26368;&#20339;&#23454;&#36341;&#65292;&#23454;&#29616;&#20102;&#23545;&#21333;&#27493;&#21644;&#22810;&#27493;&#22238;&#28335;&#21512;&#25104;&#31639;&#27861;&#30340;&#19968;&#33268;&#32780;&#26377;&#24847;&#20041;&#30340;&#35780;&#20272;&#12290;&#25105;&#20204;&#20351;&#29992;syntheseus&#37325;&#26032;&#35780;&#20272;&#20102;&#33509;&#24178;&#20808;&#21069;&#30340;&#22238;&#28335;&#21512;&#25104;&#31639;&#27861;&#65292;&#24182;&#21457;&#29616;&#22312;&#20180;&#32454;&#35780;&#20272;&#26102;&#65292;&#29616;&#26377;&#25216;&#26415;&#27169;&#22411;&#30340;&#25490;&#21517;&#20250;&#21457;&#29983;&#21464;&#21270;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#36825;&#19968;&#39046;&#22495;&#26410;&#26469;&#24037;&#20316;&#30340;&#25351;&#23548;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.19796v2 Announce Type: replace-cross  Abstract: The planning of how to synthesize molecules, also known as retrosynthesis, has been a growing focus of the machine learning and chemistry communities in recent years. Despite the appearance of steady progress, we argue that imperfect benchmarks and inconsistent comparisons mask systematic shortcomings of existing techniques. To remedy this, we present a benchmarking library called syntheseus which promotes best practice by default, enabling consistent meaningful evaluation of single-step and multi-step retrosynthesis algorithms. We use syntheseus to re-evaluate a number of previous retrosynthesis algorithms, and find that the ranking of state-of-the-art models changes when evaluated carefully. We end with guidance for future works in this area.
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#19981;&#30830;&#23450;&#24615;&#21152;&#26435;&#36845;&#20195;&#36807;&#31243;&#20197;&#25552;&#39640;&#32784;&#33104;&#36133;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#25928;&#29575;&#65292;&#24182;&#22312;&#21333;&#31574;&#30053;&#35206;&#30422;&#21644;&#24050;&#30693;&#33104;&#36133;&#27700;&#24179;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#27425;&#20248;&#24615;&#30028;&#12290;</title><link>https://arxiv.org/abs/2310.14550</link><description>&lt;p&gt;
&#20855;&#26377;&#36890;&#29992;&#20989;&#25968;&#36924;&#36817;&#30340;&#32784;&#33104;&#36133;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Corruption-Robust Offline Reinforcement Learning with General Function Approximation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.14550
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#19981;&#30830;&#23450;&#24615;&#21152;&#26435;&#36845;&#20195;&#36807;&#31243;&#20197;&#25552;&#39640;&#32784;&#33104;&#36133;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#25928;&#29575;&#65292;&#24182;&#22312;&#21333;&#31574;&#30053;&#35206;&#30422;&#21644;&#24050;&#30693;&#33104;&#36133;&#27700;&#24179;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#27425;&#20248;&#24615;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#36890;&#29992;&#20989;&#25968;&#36924;&#36817;&#30340;&#32784;&#33104;&#36133;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#33104;&#36133;&#40065;&#26834;&#24615;&#38382;&#39064;&#65292;&#20854;&#20013;&#23545;&#25163;&#21487;&#20197;&#30772;&#22351;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#30340;&#27599;&#20010;&#26679;&#26412;&#65292;&#32780;&#33104;&#36133;&#27700;&#24179; $\zeta\geq0$ &#37327;&#21270;&#20102; $n$ &#20010;&#21608;&#26399;&#21644; $H$ &#27493;&#20013;&#30340;&#32047;&#31215;&#30772;&#22351;&#37327;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25214;&#21040;&#19968;&#31181;&#23545;&#27492;&#31867;&#30772;&#22351;&#20855;&#26377;&#40065;&#26834;&#24615;&#24182;&#26368;&#23567;&#21270;&#30456;&#23545;&#20110;&#26410;&#21463;&#25439;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#26368;&#20248;&#31574;&#30053;&#30340;&#27425;&#20248;&#24046;&#36317;&#30340;&#31574;&#30053;&#12290;&#21463;&#21040;&#40065;&#26834;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#21152;&#26435;&#25216;&#26415;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#19981;&#30830;&#23450;&#24615;&#21152;&#26435;&#36845;&#20195;&#36807;&#31243;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#22312;&#25209;&#37327;&#26679;&#26412;&#19978;&#35745;&#31639;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32784;&#33104;&#36133;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#21333;&#31574;&#30053;&#35206;&#30422;&#21644; $\zeta$ &#30693;&#35782;&#20551;&#35774;&#30340;&#21069;&#25552;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#36798;&#21040;&#20102;&#19968;&#20010;&#27425;&#20248;&#24615;&#30028;&#65292;&#35813;&#30028;&#26159;&#24694;&#21270;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.14550v3 Announce Type: replace  Abstract: We investigate the problem of corruption robustness in offline reinforcement learning (RL) with general function approximation, where an adversary can corrupt each sample in the offline dataset, and the corruption level $\zeta\geq0$ quantifies the cumulative corruption amount over $n$ episodes and $H$ steps. Our goal is to find a policy that is robust to such corruption and minimizes the suboptimality gap with respect to the optimal policy for the uncorrupted Markov decision processes (MDPs). Drawing inspiration from the uncertainty-weighting technique from the robust online RL setting \citep{he2022nearly,ye2022corruptionrobust}, we design a new uncertainty weight iteration procedure to efficiently compute on batched samples and propose a corruption-robust algorithm for offline RL. Notably, under the assumption of single policy coverage and the knowledge of $\zeta$, our proposed algorithm achieves a suboptimality bound that is worsen
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Energy-Optimized Semantic Loss&#8221;&#65288;EOSL&#65289;&#30340;&#26032;&#22411;&#22810;&#30446;&#26631;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#23545;&#21464;&#21387;&#22120;&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#23454;&#39564;&#65292;&#23454;&#29616;&#20102;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#33410;&#30465;&#39640;&#36798;90%&#33021;&#37327;&#24182;&#25552;&#39640;44%&#35821;&#20041;&#30456;&#20284;&#24615;&#34920;&#29616;&#30340;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2310.07592</link><description>&lt;p&gt;
&#21464;&#21387;&#22120;&#29992;&#20110;&#32511;&#33394;&#35821;&#20041;&#36890;&#20449;&#65306;&#26356;&#23569;&#33021;&#37327;&#65292;&#26356;&#22810;&#35821;&#20041;
&lt;/p&gt;
&lt;p&gt;
Transformers for Green Semantic Communication: Less Energy, More Semantics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.07592
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Energy-Optimized Semantic Loss&#8221;&#65288;EOSL&#65289;&#30340;&#26032;&#22411;&#22810;&#30446;&#26631;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#23545;&#21464;&#21387;&#22120;&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#23454;&#39564;&#65292;&#23454;&#29616;&#20102;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#33410;&#30465;&#39640;&#36798;90%&#33021;&#37327;&#24182;&#25552;&#39640;44%&#35821;&#20041;&#30456;&#20284;&#24615;&#34920;&#29616;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#36890;&#20449;&#26088;&#22312;&#20256;&#36755;&#26377;&#24847;&#20041;&#19988;&#26377;&#25928;&#30340;&#20449;&#24687;&#65292;&#32780;&#19981;&#26159;&#19987;&#27880;&#20110;&#21333;&#20010;&#31526;&#21495;&#25110;&#20301;&#12290;&#19982;&#20256;&#32479;&#36890;&#20449;&#30456;&#27604;&#65292;&#36825;&#26679;&#20570;&#24102;&#26469;&#20102;&#35832;&#22914;&#38477;&#20302;&#24310;&#36831;&#12289;&#24102;&#23485;&#20351;&#29992;&#21644;&#26356;&#39640;&#21534;&#21520;&#37327;&#31561;&#22909;&#22788;&#12290;&#28982;&#32780;&#65292;&#35821;&#20041;&#36890;&#20449;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#38656;&#35201;&#36890;&#29992;&#24230;&#37327;&#26469;&#34913;&#37327;&#35821;&#20041;&#20449;&#24687;&#20002;&#22833;&#21644;&#23454;&#38469;&#33021;&#37327;&#28040;&#32791;&#30340;&#32852;&#21512;&#25928;&#26524;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;Energy-Optimized Semantic Loss&#8221;&#65288;EOSL&#65289;&#30340;&#26032;&#22411;&#22810;&#30446;&#26631;&#25439;&#22833;&#20989;&#25968;&#65292;&#35299;&#20915;&#20102;&#24179;&#34913;&#35821;&#20041;&#20449;&#24687;&#20002;&#22833;&#21644;&#33021;&#37327;&#28040;&#32791;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#23545;&#21464;&#21387;&#22120;&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#23454;&#39564;&#65292;&#21253;&#25324;CPU&#21644;GPU&#33021;&#37327;&#20351;&#29992;&#65292;&#35777;&#26126;&#22522;&#20110;EOSL&#30340;&#32534;&#30721;&#22120;&#27169;&#22411;&#36873;&#25321;&#22312;&#35813;&#23454;&#39564;&#20013;&#25512;&#26029;&#36807;&#31243;&#20013;&#33021;&#33410;&#30465;&#39640;&#36798;90%&#30340;&#33021;&#37327;&#65292;&#21516;&#26102;&#22312;&#35821;&#20041;&#30456;&#20284;&#24615;&#24615;&#33021;&#26041;&#38754;&#23454;&#29616;&#20102;44%&#30340;&#25913;&#36827;&#12290;&#36825;&#39033;&#24037;&#20316;&#20026;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.07592v2 Announce Type: replace  Abstract: Semantic communication aims to transmit meaningful and effective information, rather than focusing on individual symbols or bits. This results in benefits like reduced latency, bandwidth usage, and higher throughput compared with traditional communication. However, semantic communication poses significant challenges due to the need for universal metrics to benchmark the joint effects of semantic information loss and practical energy consumption. This research presents a novel multi-objective loss function named "Energy-Optimized Semantic Loss" (EOSL), addressing the challenge of balancing semantic information loss and energy consumption. Through comprehensive experiments on transformer models, including CPU and GPU energy usage, it is demonstrated that EOSL-based encoder model selection can save up to 90% of energy while achieving a 44% improvement in semantic similarity performance during inference in this experiment. This work pave
&lt;/p&gt;</description></item><item><title>&#26631;&#31614;&#24179;&#28369;&#26041;&#27861;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#65292;&#26082;&#33021;&#25552;&#21319;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#21644;&#26657;&#20934;&#24615;&#65292;&#21448;&#21487;&#33021;&#25104;&#20026;&#27169;&#22411;&#38544;&#31169;&#27844;&#38706;&#30340;&#22240;&#32032;&#12290;&#30740;&#31350;&#25581;&#31034;&#20102;&#32467;&#21512;&#36127;&#22240;&#23376;&#36827;&#34892;&#24179;&#28369;&#21487;&#26377;&#25928;&#38459;&#27490;&#27169;&#22411;&#21453;&#25512;&#25915;&#20987;&#65292;&#25552;&#21319;&#38544;&#31169;&#20445;&#25252;&#25928;&#26524;&#65292;&#36229;&#36234;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#38450;&#24481;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2310.06549</link><description>&lt;p&gt;
&#35880;&#24910;&#24179;&#28369;&#26631;&#31614;&#65306;&#26631;&#31614;&#24179;&#28369;&#26082;&#21487;&#20197;&#20316;&#20026;&#38544;&#31169;&#23631;&#38556;&#65292;&#21448;&#21487;&#20197;&#25104;&#20026;&#27169;&#22411;&#21453;&#25512;&#25915;&#20987;&#30340;&#20652;&#21270;&#21058;
&lt;/p&gt;
&lt;p&gt;
Be Careful What You Smooth For: Label Smoothing Can Be a Privacy Shield but Also a Catalyst for Model Inversion Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.06549
&lt;/p&gt;
&lt;p&gt;
&#26631;&#31614;&#24179;&#28369;&#26041;&#27861;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#65292;&#26082;&#33021;&#25552;&#21319;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#21644;&#26657;&#20934;&#24615;&#65292;&#21448;&#21487;&#33021;&#25104;&#20026;&#27169;&#22411;&#38544;&#31169;&#27844;&#38706;&#30340;&#22240;&#32032;&#12290;&#30740;&#31350;&#25581;&#31034;&#20102;&#32467;&#21512;&#36127;&#22240;&#23376;&#36827;&#34892;&#24179;&#28369;&#21487;&#26377;&#25928;&#38459;&#27490;&#27169;&#22411;&#21453;&#25512;&#25915;&#20987;&#65292;&#25552;&#21319;&#38544;&#31169;&#20445;&#25252;&#25928;&#26524;&#65292;&#36229;&#36234;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#38450;&#24481;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#31614;&#24179;&#28369;&#8212;&#8212;&#20351;&#29992;&#36719;&#21270;&#30340;&#26631;&#31614;&#32780;&#19981;&#26159;&#30828;&#26631;&#31614;&#8212;&#8212;&#26159;&#28145;&#24230;&#23398;&#20064;&#20013;&#34987;&#24191;&#27867;&#37319;&#29992;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#34920;&#29616;&#20986;&#22686;&#24378;&#27867;&#21270;&#21644;&#26657;&#20934;&#31561;&#22810;&#26679;&#30410;&#22788;&#12290;&#28982;&#32780;&#65292;&#23427;&#23545;&#20110;&#20445;&#25252;&#27169;&#22411;&#38544;&#31169;&#30340;&#24433;&#21709;&#20173;&#28982;&#27809;&#26377;&#34987;&#25506;&#32034;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#26631;&#31614;&#24179;&#28369;&#23545;&#27169;&#22411;&#21453;&#25512;&#25915;&#20987;&#65288;MIAs&#65289;&#30340;&#24433;&#21709;&#65292;&#36825;&#20123;&#25915;&#20987;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#20998;&#31867;&#22120;&#20013;&#32534;&#30721;&#30340;&#30693;&#35782;&#29983;&#25104;&#20855;&#26377;&#31867;&#20195;&#34920;&#24615;&#30340;&#26679;&#26412;&#65292;&#20174;&#32780;&#25512;&#26029;&#26377;&#20851;&#20854;&#35757;&#32451;&#25968;&#25454;&#30340;&#25935;&#24863;&#20449;&#24687;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#20256;&#32479;&#26631;&#31614;&#24179;&#28369;&#20419;&#36827;&#20102;MIAs&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#27169;&#22411;&#30340;&#38544;&#31169;&#27844;&#38706;&#12290;&#26356;&#29978;&#32773;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#29992;&#36127;&#22240;&#23376;&#36827;&#34892;&#24179;&#28369;&#21487;&#20197;&#25269;&#21046;&#36825;&#19968;&#36235;&#21183;&#65292;&#38459;&#30861;&#25552;&#21462;&#19982;&#31867;&#30456;&#20851;&#30340;&#20449;&#24687;&#65292;&#23454;&#29616;&#38544;&#31169;&#20445;&#25252;&#65292;&#32988;&#36807;&#26368;&#20808;&#36827;&#30340;&#38450;&#24481;&#26041;&#27861;&#12290;&#36825;&#30830;&#31435;&#20102;&#19968;&#31181;&#23454;&#29992;&#19988;&#24378;&#22823;&#30340;&#26032;&#30340;&#22686;&#24378;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.06549v2 Announce Type: replace  Abstract: Label smoothing -- using softened labels instead of hard ones -- is a widely adopted regularization method for deep learning, showing diverse benefits such as enhanced generalization and calibration. Its implications for preserving model privacy, however, have remained unexplored. To fill this gap, we investigate the impact of label smoothing on model inversion attacks (MIAs), which aim to generate class-representative samples by exploiting the knowledge encoded in a classifier, thereby inferring sensitive information about its training data. Through extensive analyses, we uncover that traditional label smoothing fosters MIAs, thereby increasing a model's privacy leakage. Even more, we reveal that smoothing with negative factors counters this trend, impeding the extraction of class-related information and leading to privacy preservation, beating state-of-the-art defenses. This establishes a practical and powerful novel way for enhanc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;LLMs&#20013;&#30340;&#28608;&#27963;&#24322;&#24120;&#20540;&#19982;&#32593;&#32476;&#23618;&#31232;&#30095;&#24230;&#30340;&#38750;&#22343;&#21248;&#24615;&#30456;&#20851;&#65292;&#24182;&#25552;&#20986;&#20102;Outlier Weighed Layerwise Sparsity&#65288;OWL&#65289;&#20316;&#20026;&#21098;&#26525;LLMs&#21040;&#39640;&#31232;&#30095;&#24230;&#30340;&#31192;&#23494;&#35843;&#21619;&#26009;&#12290;</title><link>https://arxiv.org/abs/2310.05175</link><description>&lt;p&gt;
Outlier Weighed Layerwise Sparsity (OWL): &#20026;&#21098;&#26525;LLMs&#36798;&#21040;&#39640;&#31232;&#30095;&#24230;&#25552;&#20379;&#32570;&#22833;&#30340;&#31192;&#23494;&#35843;&#21619;&#26009;
&lt;/p&gt;
&lt;p&gt;
Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.05175
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;LLMs&#20013;&#30340;&#28608;&#27963;&#24322;&#24120;&#20540;&#19982;&#32593;&#32476;&#23618;&#31232;&#30095;&#24230;&#30340;&#38750;&#22343;&#21248;&#24615;&#30456;&#20851;&#65292;&#24182;&#25552;&#20986;&#20102;Outlier Weighed Layerwise Sparsity&#65288;OWL&#65289;&#20316;&#20026;&#21098;&#26525;LLMs&#21040;&#39640;&#31232;&#30095;&#24230;&#30340;&#31192;&#23494;&#35843;&#21619;&#26009;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20197;&#22312;&#21508;&#20010;&#39046;&#22495;&#23637;&#29616;&#20986;&#30340;&#21331;&#36234;&#24615;&#33021;&#32780;&#38395;&#21517;&#65292;&#22312;&#23454;&#38469;&#37096;&#32626;&#26102;&#30001;&#20110;&#27169;&#22411;&#24222;&#22823;&#32780;&#38754;&#20020;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#20154;&#20204;&#21162;&#21147;&#23558;&#20256;&#32479;&#30340;&#32593;&#32476;&#21098;&#26525;&#25216;&#26415;&#24212;&#29992;&#20110;LLMs&#65292;&#21457;&#29616;&#21487;&#20197;&#22312;&#19981;&#24433;&#21709;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#19968;&#27425;&#24615;&#21098;&#25481;&#22823;&#37327;&#21442;&#25968;&#12290;&#29616;&#26377;&#30340;LLM&#21098;&#26525;&#31574;&#30053;&#19968;&#30452;&#22362;&#25345;&#20197;&#31561;&#20215;&#31232;&#30095;&#24230;&#22343;&#21248;&#21098;&#35009;&#25152;&#26377;&#23618;&#30340;&#20570;&#27861;&#65292;&#32467;&#26524;&#34920;&#29616;&#24378;&#21170;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#35266;&#23519;&#32467;&#26524;&#19982;&#22312;&#35270;&#35273;&#27169;&#22411;&#39046;&#22495;&#35266;&#23519;&#21040;&#30340;&#38750;&#22343;&#21248;&#36880;&#23618;&#31232;&#30095;&#30340;&#20027;&#27969;&#36235;&#21183;&#30456;&#30683;&#30462;&#65292;&#21518;&#32773;&#36890;&#24120;&#20250;&#20135;&#29983;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#20102;&#35299;&#36825;&#31181;&#24046;&#24322;&#32972;&#21518;&#30340;&#21407;&#22240;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#65292;&#24182;&#21457;&#29616;&#19982;LLMs&#20013;&#24322;&#24120;&#20540;&#30340;&#20986;&#29616;&#24378;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.05175v2 Announce Type: replace  Abstract: Large Language Models (LLMs), renowned for their remarkable performance across diverse domains, present a challenge when it comes to practical deployment due to their colossal model size. In response to this challenge, efforts have been directed toward the application of traditional network pruning techniques to LLMs, uncovering a massive number of parameters that can be pruned in one-shot without hurting performance. Prevailing LLM pruning strategies have consistently adhered to the practice of uniformly pruning all layers at equivalent sparsity, resulting in robust performance. However, this observation stands in contrast to the prevailing trends observed in the field of vision models, where non-uniform layerwise sparsity typically yields stronger results. To understand the underlying reasons for this disparity, we conduct a comprehensive study and discover a strong correlation with the emergence of activation outliers in LLMs. Ins
&lt;/p&gt;</description></item><item><title>CoDi&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36866;&#24212;&#39044;&#20808;&#35757;&#32451;&#30340;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#25509;&#21463;&#39069;&#22806;&#30340;&#22270;&#20687;&#26465;&#20214;&#36755;&#20837;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#29983;&#25104;&#39640;&#36136;&#37327;&#32467;&#26524;&#25152;&#38656;&#30340;&#37319;&#26679;&#27493;&#39588;&#12290;</title><link>https://arxiv.org/abs/2310.01407</link><description>&lt;p&gt;
CoDi: &#26465;&#20214;&#25193;&#25955;&#33976;&#39311;&#65292;&#29992;&#20110;&#26356;&#39640;&#20445;&#30495;&#24230;&#21644;&#26356;&#24555;&#30340;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
CoDi: Conditional Diffusion Distillation for Higher-Fidelity and Faster Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.01407
&lt;/p&gt;
&lt;p&gt;
CoDi&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36866;&#24212;&#39044;&#20808;&#35757;&#32451;&#30340;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#25509;&#21463;&#39069;&#22806;&#30340;&#22270;&#20687;&#26465;&#20214;&#36755;&#20837;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#29983;&#25104;&#39640;&#36136;&#37327;&#32467;&#26524;&#25152;&#38656;&#30340;&#37319;&#26679;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#29983;&#25104;&#24615;&#25193;&#25955;&#27169;&#22411;&#24050;&#32463;&#38761;&#26032;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#29983;&#25104;&#65292;&#24182;&#20026;&#20687;&#22270;&#20687;&#22686;&#24378;&#12289;&#24674;&#22797;&#12289;&#32534;&#36753;&#21644;&#21512;&#25104;&#31561;&#26465;&#20214;&#29983;&#25104;&#20219;&#21153;&#25552;&#20379;&#20102;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#24191;&#27867;&#24212;&#29992;&#21463;&#21040;&#39640;&#35745;&#31639;&#25104;&#26412;&#30340;&#38480;&#21046;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#23454;&#26102;&#24212;&#29992;&#20013;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;CoDi&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#20351;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#25509;&#21463;&#39069;&#22806;&#30340;&#22270;&#20687;&#26465;&#20214;&#36755;&#20837;&#65292;&#21516;&#26102;&#26174;&#33879;&#20943;&#23569;&#20102;&#38656;&#35201;&#36798;&#21040;&#39640;&#36136;&#37327;&#32467;&#26524;&#25152;&#38656;&#30340;&#37319;&#26679;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.01407v2 Announce Type: replace-cross  Abstract: Large generative diffusion models have revolutionized text-to-image generation and offer immense potential for conditional generation tasks such as image enhancement, restoration, editing, and compositing. However, their widespread adoption is hindered by the high computational cost, which limits their real-time application. To address this challenge, we introduce a novel method dubbed CoDi, that adapts a pre-trained latent diffusion model to accept additional image conditioning inputs while significantly reducing the sampling steps required to achieve high-quality results. Our method can leverage architectures such as ControlNet to incorporate conditioning inputs without compromising the model's prior knowledge gained during large scale pre-training. Additionally, a conditional consistency loss enforces consistent predictions across diffusion steps, effectively compelling the model to generate high-quality images with conditio
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20449;&#29992;&#35780;&#20998;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#36890;&#29992;&#33021;&#21147;&#65292;&#36890;&#36807;&#39318;&#20010;&#24320;&#28304;&#26694;&#26550;&#21644;CALM&#27169;&#22411;&#65292;&#21487;&#20197;&#24110;&#21161;&#35299;&#20915;&#20256;&#32479;&#20449;&#29992;&#35780;&#20998;&#26041;&#27861;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#21516;&#26102;&#35299;&#20915;&#28508;&#22312;&#30340;&#20559;&#35265;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2310.00566</link><description>&lt;p&gt;
&#36171;&#33021;&#20247;&#22810;&#65292;&#20559;&#34962;&#23569;&#25968;&#65306;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#36890;&#29992;&#20449;&#29992;&#35780;&#20998;
&lt;/p&gt;
&lt;p&gt;
Empowering Many, Biasing a Few: Generalist Credit Scoring through Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.00566
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20449;&#29992;&#35780;&#20998;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#36890;&#29992;&#33021;&#21147;&#65292;&#36890;&#36807;&#39318;&#20010;&#24320;&#28304;&#26694;&#26550;&#21644;CALM&#27169;&#22411;&#65292;&#21487;&#20197;&#24110;&#21161;&#35299;&#20915;&#20256;&#32479;&#20449;&#29992;&#35780;&#20998;&#26041;&#27861;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#21516;&#26102;&#35299;&#20915;&#28508;&#22312;&#30340;&#20559;&#35265;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37329;&#34701;&#34892;&#19994;&#65292;&#20449;&#29992;&#35780;&#20998;&#26159;&#19968;&#20010;&#22522;&#30784;&#35201;&#32032;&#65292;&#22609;&#36896;&#30528;&#20010;&#20154;&#21644;&#20225;&#19994;&#30340;&#20449;&#36151;&#20934;&#20837;&#65292;&#20915;&#23450;&#30528;&#36151;&#27454;&#26465;&#20214;&#12290;&#26412;&#30740;&#31350;&#35748;&#20026;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20449;&#29992;&#35780;&#20998;&#20219;&#21153;&#20013;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#33021;&#22815;&#24378;&#22823;&#22320;&#36328;&#22810;&#20010;&#20219;&#21153;&#36827;&#34892;&#27867;&#21270;&#12290;&#20026;&#20102;&#31995;&#32479;&#22320;&#25506;&#32034;LLMs&#22312;&#20449;&#29992;&#35780;&#20998;&#20013;&#30340;&#24212;&#29992;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#24320;&#28304;&#20840;&#38754;&#26694;&#26550;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#28085;&#30422;9&#20010;&#25968;&#25454;&#38598;&#12289;1.4K&#26679;&#26412;&#30340;&#26032;&#22411;&#22522;&#20934;&#65292;&#19987;&#38376;&#29992;&#20110;&#20449;&#29992;&#35780;&#20272;&#65292;&#24182;&#23545;LLMs&#20869;&#28508;&#22312;&#20559;&#35265;&#36827;&#34892;&#20102;&#37325;&#35201;&#26816;&#26597;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#36229;&#36807;45K&#26679;&#26412;&#30340;&#26032;&#22411;&#25351;&#23548;&#35843;&#25972;&#25968;&#25454;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#25351;&#23548;&#35843;&#25972;&#25552;&#20986;&#20102;&#39318;&#20010;&#20449;&#36151;&#19982;&#39118;&#38505;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;CALM&#65289;&#65292;&#20197;&#38024;&#23545;&#19981;&#21516;&#30340;&#24494;&#22937;&#38656;&#27714;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.00566v3 Announce Type: replace-cross  Abstract: In the financial industry, credit scoring is a fundamental element, shaping access to credit and determining the terms of loans for individuals and businesses alike. Traditional credit scoring methods, however, often grapple with challenges such as narrow knowledge scope and isolated evaluation of credit tasks. Our work posits that Large Language Models (LLMs) have great potential for credit scoring tasks, with strong generalization ability across multiple tasks. To systematically explore LLMs for credit scoring, we propose the first open-source comprehensive framework. We curate a novel benchmark covering 9 datasets with 14K samples, tailored for credit assessment and a critical examination of potential biases within LLMs, and the novel instruction tuning data with over 45k samples. We then propose the first Credit and Risk Assessment Large Language Model (CALM) by instruction tuning, tailored to the nuanced demands of various
&lt;/p&gt;</description></item><item><title>&#25351;&#20196;&#35843;&#25972;&#23545;LLMs&#20135;&#29983;&#20102;&#19977;&#20010;&#37325;&#35201;&#24433;&#21709;&#65306;1&#65289;&#20351;&#20854;&#33021;&#22815;&#35782;&#21035;&#29992;&#25143;&#25552;&#31034;&#20013;&#30340;&#25351;&#20196;&#37096;&#20998;&#65307;2&#65289;&#20419;&#36827;&#21709;&#24212;&#29983;&#25104;&#30340;&#19981;&#26029;&#35843;&#25972;</title><link>https://arxiv.org/abs/2310.00492</link><description>&lt;p&gt;
&#20174;&#35821;&#35328;&#24314;&#27169;&#21040;&#25351;&#20196;&#36319;&#38543;&#65306;&#29702;&#35299;&#25351;&#20196;&#35843;&#25972;&#21518;LLMs&#20013;&#34892;&#20026;&#30340;&#36716;&#21464;
&lt;/p&gt;
&lt;p&gt;
From Language Modeling to Instruction Following: Understanding the Behavior Shift in LLMs after Instruction Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.00492
&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#23545;LLMs&#20135;&#29983;&#20102;&#19977;&#20010;&#37325;&#35201;&#24433;&#21709;&#65306;1&#65289;&#20351;&#20854;&#33021;&#22815;&#35782;&#21035;&#29992;&#25143;&#25552;&#31034;&#20013;&#30340;&#25351;&#20196;&#37096;&#20998;&#65307;2&#65289;&#20419;&#36827;&#21709;&#24212;&#29983;&#25104;&#30340;&#19981;&#26029;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20854;&#20013;&#25351;&#20196;&#35843;&#25972;&#26159;&#23558;LLMs&#19982;&#29992;&#25143;&#24847;&#22270;&#23545;&#40784;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#25351;&#20196;&#35843;&#25972;&#22914;&#20309;&#35843;&#25972;&#32463;&#36807;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#37325;&#28857;&#20851;&#27880;&#20869;&#22312;&#21464;&#21270;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#24320;&#21457;&#20102;&#20960;&#31181;&#26412;&#22320;&#21644;&#20840;&#23616;&#35299;&#37322;&#26041;&#27861;&#65292;&#21253;&#25324;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#36755;&#20837;&#36755;&#20986;&#24402;&#22240;&#26041;&#27861;&#65292;&#20197;&#21450;&#29992;&#20110;&#35299;&#37322;&#33258;&#27880;&#24847;&#21147;&#21644;&#21069;&#39304;&#23618;&#20013;&#30340;&#27169;&#24335;&#21644;&#27010;&#24565;&#30340;&#25216;&#26415;&#12290;&#28982;&#21518;&#36890;&#36807;&#27604;&#36739;&#20174;&#39044;&#35757;&#32451;&#21644;&#25351;&#20196;&#35843;&#25972;&#27169;&#22411;&#20013;&#24471;&#20986;&#30340;&#35299;&#37322;&#26469;&#30740;&#31350;&#25351;&#20196;&#35843;&#25972;&#30340;&#24433;&#21709;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#20154;&#21487;&#29702;&#35299;&#30340;&#27700;&#24179;&#19978;&#25552;&#20379;&#20102;&#27169;&#22411;&#36716;&#21464;&#30340;&#20869;&#37096;&#35270;&#35282;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#20102;&#25351;&#20196;&#35843;&#25972;&#30340;&#19977;&#20010;&#37325;&#35201;&#24433;&#21709;&#65306;1&#65289;&#23427;&#20351;LLMs&#33021;&#22815;&#35782;&#21035;&#29992;&#25143;&#25552;&#31034;&#20013;&#30340;&#25351;&#20196;&#37096;&#20998;&#65292;&#24182;&#19981;&#26029;&#20419;&#36827;&#21709;&#24212;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.00492v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) have achieved remarkable success, where instruction tuning is the critical step in aligning LLMs with user intentions. In this work, we investigate how the instruction tuning adjusts pre-trained models with a focus on intrinsic changes. Specifically, we first develop several local and global explanation methods, including a gradient-based method for input-output attribution and techniques for interpreting patterns and concepts in self-attention and feed-forward layers. The impact of instruction tuning is then studied by comparing the explanations derived from the pre-trained and instruction-tuned models. This approach provides an internal perspective of the model shifts on a human-comprehensible level. Our findings reveal three significant impacts of instruction tuning: 1) It empowers LLMs to recognize the instruction parts from user prompts, and promotes the response generation constantly condition
&lt;/p&gt;</description></item><item><title>SINCERE&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#19978;&#21512;&#29702;&#30340;&#30417;&#30563;&#25193;&#23637;&#65292;&#36991;&#20813;&#20102;&#21516;&#19968;&#31867;&#21035;&#30340;&#22270;&#20687;&#30456;&#20114;&#25490;&#26021;&#65292;&#36890;&#36807;&#26356;&#22909;&#22320;&#20998;&#31163;&#19981;&#21516;&#31867;&#21035;&#30340;&#23884;&#20837;&#65292;&#22312;&#20445;&#25345;&#31454;&#20105;&#24615;&#20998;&#31867;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2309.14277</link><description>&lt;p&gt;
SINCERE: &#30417;&#30563;&#20449;&#24687;&#22122;&#22768;-&#23545;&#27604;&#20272;&#35745;&#20877;&#23457;
&lt;/p&gt;
&lt;p&gt;
SINCERE: Supervised Information Noise-Contrastive Estimation REvisited
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.14277
&lt;/p&gt;
&lt;p&gt;
SINCERE&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#19978;&#21512;&#29702;&#30340;&#30417;&#30563;&#25193;&#23637;&#65292;&#36991;&#20813;&#20102;&#21516;&#19968;&#31867;&#21035;&#30340;&#22270;&#20687;&#30456;&#20114;&#25490;&#26021;&#65292;&#36890;&#36807;&#26356;&#22909;&#22320;&#20998;&#31163;&#19981;&#21516;&#31867;&#21035;&#30340;&#23884;&#20837;&#65292;&#22312;&#20445;&#25345;&#31454;&#20105;&#24615;&#20998;&#31867;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#65288;InfoNCE&#65289;&#25439;&#22833;&#20989;&#25968;&#30001;&#20110;&#20854;&#24378;&#22823;&#30340;&#23454;&#35777;&#32467;&#26524;&#21644;&#29702;&#35770;&#21160;&#26426;&#65292;&#20026;&#35768;&#22810;&#33258;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#30417;&#30563;&#23545;&#27604;&#65288;SupCon&#65289;&#25439;&#22833;&#21487;&#25193;&#23637;InfoNCE&#20197;&#20174;&#21487;&#29992;&#31867;&#26631;&#31614;&#20013;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20808;&#21069;&#30340;SupCon&#25439;&#22833;&#20844;&#24335;&#23384;&#22312;&#30097;&#38382;&#30340;&#29702;&#30001;&#65292;&#22240;&#20026;&#23427;&#21487;&#33021;&#20250;&#20419;&#20351;&#26469;&#33258;&#21516;&#19968;&#31867;&#21035;&#30340;&#26576;&#20123;&#22270;&#20687;&#22312;&#23398;&#20064;&#21040;&#30340;&#23884;&#20837;&#31354;&#38388;&#20013;&#30456;&#20114;&#25490;&#26021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#30417;&#30563;&#20449;&#24687;&#22122;&#22768;-&#23545;&#27604;&#20272;&#35745;&#20877;&#23457;&#65288;SINCERE&#65289;&#25439;&#22833;&#65292;&#20316;&#20026;&#20449;&#24687;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#30340;&#29702;&#35770;&#19978;&#21512;&#29702;&#30340;&#30417;&#30563;&#25193;&#23637;&#65292;&#23427;&#27704;&#36828;&#19981;&#20250;&#23548;&#33268;&#26469;&#33258;&#21516;&#19968;&#31867;&#21035;&#30340;&#22270;&#20687;&#30456;&#20114;&#25490;&#26021;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;SINCERE&#23548;&#33268;&#19981;&#21516;&#31867;&#21035;&#30340;&#23884;&#20837;&#26356;&#22909;&#22320;&#20998;&#31163;&#65292;&#21516;&#26102;&#23545;&#20110;&#30417;&#30563;&#21644;&#36801;&#31227;&#23398;&#20064;&#25552;&#20379;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#19968;&#20010;&#20449;&#24687;&#35770;&#19978;&#30340;&#19979;&#30028;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.14277v2 Announce Type: replace-cross  Abstract: The information noise-contrastive estimation (InfoNCE) loss function provides the basis of many self-supervised deep learning methods due to its strong empirical results and theoretic motivation. Previous work suggests a supervised contrastive (SupCon) loss to extend InfoNCE to learn from available class labels. However, in this work we find that the prior SupCon loss formulation has questionable justification because it can encourage some images from the same class to repel one another in the learned embedding space. We propose the Supervised InfoNCE REvisited (SINCERE) loss as a theoretically-justified supervised extension of InfoNCE that never causes images from the same class to repel one another. Experiments show that SINCERE leads to better separation of embeddings from different classes while delivering competitive classification accuracy for supervised and transfer learning. We further show an information-theoretic boun
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31867;&#21035;&#25490;&#38500;&#30340;&#21160;&#24577;&#25512;&#29702;&#26089;&#26399;&#36864;&#20986;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20013;&#38388;&#23618;&#21033;&#29992;&#23398;&#21040;&#30340;&#29305;&#24449;&#25490;&#38500;&#22823;&#37327;&#19981;&#30456;&#20851;&#31867;&#21035;&#65292;&#20174;&#32780;&#26174;&#33879;&#38477;&#20302;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2309.13443</link><description>&lt;p&gt;
&#20351;&#29992;&#31867;&#21035;&#25490;&#38500;&#30340;&#26089;&#26399;&#36864;&#20986;&#23454;&#29616;&#31070;&#32463;&#32593;&#32476;&#26377;&#25928;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Early-Exit with Class Exclusion for Efficient Inference of Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.13443
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31867;&#21035;&#25490;&#38500;&#30340;&#21160;&#24577;&#25512;&#29702;&#26089;&#26399;&#36864;&#20986;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20013;&#38388;&#23618;&#21033;&#29992;&#23398;&#21040;&#30340;&#29305;&#24449;&#25490;&#38500;&#22823;&#37327;&#19981;&#30456;&#20851;&#31867;&#21035;&#65292;&#20174;&#32780;&#26174;&#33879;&#38477;&#20302;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#20010;&#39046;&#22495;&#24050;&#32463;&#34987;&#25104;&#21151;&#24212;&#29992;&#12290;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#38656;&#35201;&#25191;&#34892;&#22823;&#37327;&#30340;&#20056;&#27861;&#32047;&#21152;&#65288;MAC&#65289;&#36816;&#31639;&#65292;&#36825;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24179;&#21488;&#65288;&#22914;&#36793;&#32536;&#35774;&#22791;&#65289;&#20013;&#24212;&#29992;&#23427;&#20204;&#26102;&#20250;&#24102;&#26469;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31867;&#21035;&#30340;&#21160;&#24577;&#25512;&#29702;&#26089;&#26399;&#36864;&#20986;&#26041;&#27861;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#23618;&#20013;&#23398;&#21040;&#30340;&#29305;&#24449;&#26469;&#25490;&#38500;&#23613;&#21487;&#33021;&#22810;&#30340;&#19981;&#30456;&#20851;&#31867;&#21035;&#65292;&#20351;&#24471;&#21518;&#32493;&#23618;&#21482;&#38656;&#35201;&#22312;&#21097;&#20313;&#31867;&#21035;&#20013;&#30830;&#23450;&#30446;&#26631;&#31867;&#21035;&#12290;&#24403;&#22312;&#26576;&#19968;&#23618;&#20013;&#20165;&#21097;&#19979;&#19968;&#20010;&#31867;&#21035;&#26102;&#65292;&#36825;&#20010;&#31867;&#21035;&#23601;&#26159;&#30456;&#24212;&#30340;&#20998;&#31867;&#32467;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25552;&#20986;&#30340;&#26089;&#26399;&#36864;&#20986;&#25216;&#26415;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;DNN&#22312;&#25512;&#29702;&#20013;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.13443v2 Announce Type: replace  Abstract: Deep neural networks (DNNs) have been successfully applied in various fields. In DNNs, a large number of multiply-accumulate (MAC) operations are required to be performed, posing critical challenges in applying them in resource-constrained platforms, e.g., edge devices. To address this challenge, in this paper, we propose a class-based early-exit for dynamic inference. Instead of pushing DNNs to make a dynamic decision at intermediate layers, we take advantage of the learned features in these layers to exclude as many irrelevant classes as possible, so that later layers only have to determine the target class among the remaining classes. When only one class remains at a layer, this class is the corresponding classification result. Experimental results demonstrate the computational cost of DNNs in inference can be reduced significantly with the proposed early-exit technique. The codes can be found at https://github.com/HWAI-TUDa/Early
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;&#34701;&#21512;&#35268;&#33539;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26694;&#26550;&#65292;&#22312;&#39046;&#22495;&#27867;&#21270;&#21644;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#36234;&#12290;</title><link>https://arxiv.org/abs/2308.03321</link><description>&lt;p&gt;
AFN: &#33258;&#36866;&#24212;&#34701;&#21512;&#35268;&#33539;&#21270;&#22312;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26694;&#26550;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
AFN: Adaptive Fusion Normalization via an Encoder-Decoder Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.03321
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;&#34701;&#21512;&#35268;&#33539;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26694;&#26550;&#65292;&#22312;&#39046;&#22495;&#27867;&#21270;&#21644;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#25104;&#21151;&#19982;&#35268;&#33539;&#21270;&#23618;&#23494;&#19981;&#21487;&#20998;&#12290;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#21508;&#31181;&#35268;&#33539;&#21270;&#21151;&#33021;&#65292;&#27599;&#31181;&#37117;&#26377;&#20854;&#20248;&#21183;&#21644;&#21155;&#21183;&#12290;&#20026;&#27492;&#65292;&#20154;&#20204;&#33268;&#21147;&#20110;&#35774;&#35745;&#19968;&#20010;&#32479;&#19968;&#30340;&#35268;&#33539;&#21270;&#21151;&#33021;&#65292;&#23558;&#25152;&#26377;&#35268;&#33539;&#21270;&#36807;&#31243;&#32467;&#21512;&#36215;&#26469;&#24182;&#20943;&#36731;&#23427;&#20204;&#30340;&#24369;&#28857;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35268;&#33539;&#21270;&#21151;&#33021;&#65292;&#31216;&#20026;&#33258;&#36866;&#24212;&#34701;&#21512;&#35268;&#33539;&#21270;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;AFN&#22312;&#39046;&#22495;&#27867;&#21270;&#21644;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#20248;&#20110;&#20808;&#21069;&#30340;&#35268;&#33539;&#21270;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2308.03321v4 Announce Type: replace  Abstract: The success of deep learning is inseparable from normalization layers. Researchers have proposed various normalization functions, and each of them has both advantages and disadvantages. In response, efforts have been made to design a unified normalization function that combines all normalization procedures and mitigates their weaknesses. We also proposed a new normalization function called Adaptive Fusion Normalization. Through experiments, we demonstrate AFN outperforms the previous normalization techniques in domain generalization and image classification tasks.
&lt;/p&gt;</description></item><item><title>CroSSL&#26159;&#19968;&#31181;&#36328;&#27169;&#24577;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#38544;&#34255;&#25513;&#30721;&#21644;&#36328;&#27169;&#24577;&#32858;&#21512;&#22120;&#23454;&#29616;&#23545;&#26102;&#38388;&#24207;&#21015;&#30340;&#23398;&#20064;&#65292;&#26080;&#38656;&#36127;&#26679;&#26412;&#23545;&#21644;&#25968;&#25454;&#39044;&#22788;&#29702;&#12290;</title><link>https://arxiv.org/abs/2307.16847</link><description>&lt;p&gt;
CroSSL: &#36328;&#27169;&#24577;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#26102;&#38388;&#24207;&#21015;&#19978;&#30340;&#24212;&#29992;&#36890;&#36807;&#38544;&#34255;&#25513;&#30721;
&lt;/p&gt;
&lt;p&gt;
CroSSL: Cross-modal Self-Supervised Learning for Time-series through Latent Masking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2307.16847
&lt;/p&gt;
&lt;p&gt;
CroSSL&#26159;&#19968;&#31181;&#36328;&#27169;&#24577;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#38544;&#34255;&#25513;&#30721;&#21644;&#36328;&#27169;&#24577;&#32858;&#21512;&#22120;&#23454;&#29616;&#23545;&#26102;&#38388;&#24207;&#21015;&#30340;&#23398;&#20064;&#65292;&#26080;&#38656;&#36127;&#26679;&#26412;&#23545;&#21644;&#25968;&#25454;&#39044;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22810;&#27169;&#24577;&#26102;&#38388;&#24207;&#21015;&#30340;&#26631;&#27880;&#25968;&#25454;&#30340;&#26377;&#38480;&#21487;&#29992;&#24615;&#20005;&#37325;&#38459;&#30861;&#20102;&#35813;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26159;&#19968;&#31181;&#26080;&#38656;&#20381;&#36182;&#26631;&#31614;&#23398;&#20064;&#25968;&#25454;&#34920;&#31034;&#30340;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;SSL&#26041;&#27861;&#38656;&#35201;&#35745;&#31639;&#26114;&#36149;&#30340;&#36127;&#26679;&#26412;&#23545;&#65292;&#24182;&#19988;&#36890;&#24120;&#20165;&#36866;&#29992;&#20110;&#21333;&#27169;&#24577;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#22810;&#21151;&#33021;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;CroSSL&#65288;&#36328;&#27169;&#24577;&#33258;&#30417;&#30563;&#23398;&#20064;&#65289;&#65292;&#23427;&#25552;&#20986;&#20102;&#20004;&#20010;&#21019;&#26032;&#27010;&#24565;&#65306;&#36890;&#36807;&#27169;&#24577;&#29305;&#23450;&#32534;&#30721;&#22120;&#20135;&#29983;&#30340;&#20013;&#38388;&#23884;&#20837;&#30340;&#38544;&#34255;&#25513;&#30721;&#65292;&#20197;&#21450;&#36890;&#36807;&#36328;&#27169;&#24577;&#32858;&#21512;&#22120;&#23558;&#20854;&#32858;&#21512;&#20026;&#20840;&#23616;&#23884;&#20837;&#65292;&#21487;&#20197;&#25552;&#20379;&#32473;&#19979;&#28216;&#20998;&#31867;&#22120;&#12290;CroSSL&#20801;&#35768;&#22788;&#29702;&#32570;&#22833;&#27169;&#24577;&#21644;&#31471;&#21040;&#31471;&#30340;&#36328;&#27169;&#24577;&#23398;&#20064;&#65292;&#26080;&#38656;&#36827;&#34892;&#39044;&#22788;&#29702;&#20197;&#22788;&#29702;&#32570;&#22833;&#36755;&#20837;&#25110;&#36827;&#34892;&#23545;&#27604;&#23398;&#20064;&#30340;&#36127;&#26679;&#26412;&#37319;&#26679;&#12290;&#25105;&#20204;&#23545;&#21508;&#31181;&#25968;&#25454;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21253;&#25324;&#21152;&#36895;&#24230;&#35745;&#25110;&#38464;&#34746;&#20202;&#31561;&#36816;&#21160;&#20256;&#24863;&#22120;&#21644;&#29983;&#29289;&#20256;&#24863;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Limited availability of labeled data for machine learning on multimodal time-series extensively hampers progress in the field. Self-supervised learning (SSL) is a promising approach to learning data representations without relying on labels. However, existing SSL methods require expensive computations of negative pairs and are typically designed for single modalities, which limits their versatility. We introduce CroSSL (Cross-modal SSL), which puts forward two novel concepts: masking intermediate embeddings produced by modality-specific encoders, and their aggregation into a global embedding through a cross-modal aggregator that can be fed to down-stream classifiers. CroSSL allows for handling missing modalities and end-to-end cross-modal learning without requiring prior data preprocessing for handling missing inputs or negative-pair sampling for contrastive learning. We evaluate our method on a wide range of data, including motion sensors such as accelerometers or gyroscopes and biosi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#25193;&#25955;&#37319;&#26679;&#21644;Krylov&#23376;&#31354;&#38388;&#26041;&#27861;&#21327;&#21516;&#32467;&#21512;&#30340;&#26032;&#22411;&#39640;&#25928;&#37319;&#26679;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2303.05754</link><description>&lt;p&gt;
&#20998;&#35299;&#25193;&#25955;&#37319;&#26679;&#22120;&#29992;&#20110;&#21152;&#36895;&#22823;&#35268;&#27169;&#36870;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Decomposed Diffusion Sampler for Accelerating Large-Scale Inverse Problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2303.05754
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#25193;&#25955;&#37319;&#26679;&#21644;Krylov&#23376;&#31354;&#38388;&#26041;&#27861;&#21327;&#21516;&#32467;&#21512;&#30340;&#26032;&#22411;&#39640;&#25928;&#37319;&#26679;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Krylov&#23376;&#31354;&#38388;&#26159;&#36890;&#36807;&#23558;&#32473;&#23450;&#21521;&#37327;&#19982;&#32447;&#24615;&#21464;&#25442;&#30697;&#38453;&#21450;&#20854;&#36830;&#32493;&#24130;&#30456;&#20056;&#32780;&#29983;&#25104;&#30340;&#65292;&#24191;&#27867;&#30740;&#31350;&#30340;&#32463;&#20856;&#20248;&#21270;&#25991;&#29486;&#20013;&#21033;&#29992;Krylov&#23376;&#31354;&#38388;&#35774;&#35745;&#31639;&#27861;&#20197;&#24555;&#36895;&#25910;&#25947;&#22823;&#35268;&#27169;&#32447;&#24615;&#36870;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#39640;&#25928;&#30340;&#25193;&#25955;&#37319;&#26679;&#31574;&#30053;&#65292;&#23558;&#25193;&#25955;&#37319;&#26679;&#19982;Krylov&#23376;&#31354;&#38388;&#26041;&#27861;&#21327;&#21516;&#32467;&#21512;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2303.05754v3 Announce Type: replace-cross  Abstract: Krylov subspace, which is generated by multiplying a given vector by the matrix of a linear transformation and its successive powers, has been extensively studied in classical optimization literature to design algorithms that converge quickly for large linear inverse problems. For example, the conjugate gradient method (CG), one of the most popular Krylov subspace methods, is based on the idea of minimizing the residual error in the Krylov subspace. However, with the recent advancement of high-performance diffusion solvers for inverse problems, it is not clear how classical wisdom can be synergistically combined with modern diffusion models. In this study, we propose a novel and efficient diffusion sampling strategy that synergistically combines the diffusion sampling and Krylov subspace methods. Specifically, we prove that if the tangent space at a denoised sample by Tweedie's formula forms a Krylov subspace, then the CG initi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#39532;&#23572;&#21487;&#22827;&#38142;&#30340;&#36317;&#31163;&#65292;&#21253;&#25324;&#24102;&#33410;&#28857;&#23646;&#24615;&#30340;&#26377;&#21521;&#22270;&#65292;&#36825;&#20010;&#26694;&#26550;&#31216;&#20026;&#26368;&#20248;&#36755;&#36816;&#39532;&#23572;&#21487;&#22827;&#65288;OTM&#65289;&#36317;&#31108;&#12290;</title><link>https://arxiv.org/abs/2302.08621</link><description>&lt;p&gt;
&#39532;&#23572;&#21487;&#22827;&#38142;&#30340;&#36317;&#31163;&#21450;&#20854;&#24046;&#24322;&#21270;
&lt;/p&gt;
&lt;p&gt;
Distances for Markov Chains, and Their Differentiation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.08621
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#39532;&#23572;&#21487;&#22827;&#38142;&#30340;&#36317;&#31163;&#65292;&#21253;&#25324;&#24102;&#33410;&#28857;&#23646;&#24615;&#30340;&#26377;&#21521;&#22270;&#65292;&#36825;&#20010;&#26694;&#26550;&#31216;&#20026;&#26368;&#20248;&#36755;&#36816;&#39532;&#23572;&#21487;&#22827;&#65288;OTM&#65289;&#36317;&#31108;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#65288;&#26377;&#21521;&#65289;&#24102;&#33410;&#28857;&#23646;&#24615;&#30340;&#22270;&#26159;&#21508;&#31181;&#24212;&#29992;&#20013;&#24120;&#35265;&#30340;&#19968;&#31181;&#25968;&#25454;&#31867;&#22411;&#65292;&#20851;&#20110;&#24320;&#21457;&#29992;&#20110;&#27604;&#36739;&#23427;&#20204;&#30340;&#24230;&#37327;&#26631;&#20934;&#21644;&#39640;&#25928;&#31639;&#27861;&#26377;&#22823;&#37327;&#25991;&#29486;&#12290;&#26368;&#36817;&#65292;&#22312;&#22270;&#23398;&#20064;&#21644;&#20248;&#21270;&#39046;&#22495;&#65292;&#38024;&#23545;&#24102;&#33410;&#28857;&#23646;&#24615;&#30340;&#22270;&#27604;&#36739;&#24050;&#32463;&#20986;&#29616;&#20102;&#19968;&#31995;&#21015;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#26368;&#20248;&#36755;&#36816;&#65288;OT&#65289;&#21644;Weisfeiler-Lehman&#65288;WL&#65289;&#22270;&#21516;&#26500;&#27979;&#35797;&#31561;&#24605;&#24819;&#12290;&#20004;&#20010;&#26368;&#20808;&#36827;&#30340;&#20195;&#34920;&#26159;(O'Connor&#31561;&#20154;&#65292;2022)&#25552;&#20986;&#30340;OTC&#36317;&#31163;&#21644;(Chen&#31561;&#20154;&#65292;2022)&#25552;&#20986;&#30340;WL&#36317;&#31163;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#23613;&#31649;&#36825;&#20004;&#31181;&#36317;&#31163;&#22522;&#20110;&#19981;&#21516;&#30340;&#24605;&#24819;&#24320;&#21457;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#23427;&#20204;&#37117;&#23558;&#22270;&#35270;&#20026;&#39532;&#23572;&#21487;&#22827;&#38142;&#65292;&#24182;&#19988;&#26377;&#30528;&#28145;&#21051;&#30340;&#32852;&#31995;&#12290;&#23454;&#38469;&#19978;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#26469;&#29983;&#25104;&#39532;&#23572;&#21487;&#22827;&#38142;&#30340;&#36317;&#31163;&#65288;&#20174;&#32780;&#21253;&#25324;&#24102;&#33410;&#28857;&#23646;&#24615;&#30340;&#65288;&#26377;&#21521;&#65289;&#22270;&#65289;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#26368;&#20248;&#36755;&#36816;&#39532;&#23572;&#21487;&#22827;&#65288;OTM&#65289;d
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.08621v2 Announce Type: replace  Abstract: (Directed) graphs with node attributes are a common type of data in various applications and there is a vast literature on developing metrics and efficient algorithms for comparing them. Recently, in the graph learning and optimization communities, a range of new approaches have been developed for comparing graphs with node attributes, leveraging ideas such as the Optimal Transport (OT) and the Weisfeiler-Lehman (WL) graph isomorphism test. Two state-of-the-art representatives are the OTC distance proposed in (O'Connor et al., 2022) and the WL distance in (Chen et al., 2022). Interestingly, while these two distances are developed based on different ideas, we observe that they both view graphs as Markov chains, and are deeply connected. Indeed, in this paper, we propose a unified framework to generate distances for Markov chains (thus including (directed) graphs with node attributes), which we call the Optimal Transport Markov (OTM) d
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#33521;&#22269;&#29983;&#29289;&#24211;&#30524;&#24213;&#25104;&#20687;&#20013;&#34987;&#29992;&#20110;&#35786;&#26029;&#24085;&#37329;&#26862;&#30149;&#65292;&#21487;&#22312;&#30142;&#30149;&#30151;&#29366;&#21457;&#20316;&#20043;&#21069;&#23454;&#29616;&#20934;&#30830;&#31579;&#26597;&#12290;</title><link>https://arxiv.org/abs/2302.06727</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20174;&#33521;&#22269;&#29983;&#29289;&#24211;&#30524;&#24213;&#25104;&#20687;&#20013;&#39044;&#27979;&#24085;&#37329;&#26862;&#30149;&#30340;&#29616;&#26377;&#21644;&#26032;&#21457;&#30149;&#20363;
&lt;/p&gt;
&lt;p&gt;
Deep Learning Predicts Prevalent and Incident Parkinson's Disease From UK Biobank Fundus Imaging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.06727
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#33521;&#22269;&#29983;&#29289;&#24211;&#30524;&#24213;&#25104;&#20687;&#20013;&#34987;&#29992;&#20110;&#35786;&#26029;&#24085;&#37329;&#26862;&#30149;&#65292;&#21487;&#22312;&#30142;&#30149;&#30151;&#29366;&#21457;&#20316;&#20043;&#21069;&#23454;&#29616;&#20934;&#30830;&#31579;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24085;&#37329;&#26862;&#30149;&#26159;&#19990;&#30028;&#19978;&#22686;&#38271;&#26368;&#24555;&#30340;&#31070;&#32463;&#31995;&#32479;&#30142;&#30149;&#12290;&#30740;&#31350;&#38416;&#26126;&#24085;&#37329;&#26862;&#30149;&#30340;&#26426;&#21046;&#24182;&#33258;&#21160;&#36827;&#34892;&#35786;&#26029;&#23558;&#26497;&#22823;&#22320;&#25913;&#21892;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#30340;&#27835;&#30103;&#12290;&#24403;&#21069;&#30340;&#35786;&#26029;&#26041;&#27861;&#26114;&#36149;&#19988;&#20379;&#24212;&#26377;&#38480;&#12290;&#32771;&#34385;&#21040;&#30142;&#30149;&#30340;&#38544;&#21311;&#24615;&#21644;&#20020;&#24202;&#21069;&#26399;&#30340;&#21457;&#23637;&#65292;&#29702;&#24819;&#30340;&#31579;&#26597;&#24212;&#35813;&#20855;&#26377;&#35786;&#26029;&#20934;&#30830;&#24615;&#65292;&#29978;&#33267;&#22312;&#30151;&#29366;&#21457;&#20316;&#20043;&#21069;&#23601;&#33021;&#20801;&#35768;&#21307;&#23398;&#24178;&#39044;&#12290;&#25105;&#20204;&#24378;&#35843;&#35270;&#32593;&#33180;&#30524;&#24213;&#25104;&#20687;&#20316;&#20026;&#24085;&#37329;&#26862;&#30149;&#30340;&#35786;&#26029;&#31579;&#26597;&#27169;&#24335;&#65292;&#36890;&#24120;&#34987;&#31216;&#20026;&#22823;&#33041;&#20043;&#31383;&#12290;&#25105;&#20204;&#23545;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#20102;&#31995;&#32479;&#35780;&#20272;&#65292;&#20197;&#20174;&#33521;&#22269;&#29983;&#29289;&#24211;&#30524;&#24213;&#25104;&#20687;&#20013;&#23545;&#24085;&#37329;&#26862;&#30149;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#21487;&#20197;&#19982;&#24180;&#40836;&#21644;&#24615;&#21035;&#21305;&#37197;&#30340;&#20581;&#24247;&#21463;&#35797;&#32773;&#21306;&#20998;&#24320;&#26469;&#65292;&#38754;&#31215;&#22312;&#26354;&#32447;&#19979;
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.06727v3 Announce Type: replace  Abstract: Parkinson's disease is the world's fastest-growing neurological disorder. Research to elucidate the mechanisms of Parkinson's disease and automate diagnostics would greatly improve the treatment of patients with Parkinson's disease. Current diagnostic methods are expensive and have limited availability. Considering the insidious and preclinical onset and progression of the disease, a desirable screening should be diagnostically accurate even before the onset of symptoms to allow medical interventions. We highlight retinal fundus imaging, often termed a window to the brain, as a diagnostic screening modality for Parkinson's disease. We conducted a systematic evaluation of conventional machine learning and deep learning techniques to classify Parkinson's disease from UK Biobank fundus imaging. Our results show that Parkinson's disease individuals can be differentiated from age and gender-matched healthy subjects with an Area Under the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#20998;&#24067;&#24335;&#37327;&#21270;&#27969;&#30340;GFlowNets&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#27969;&#20989;&#25968;&#36716;&#21270;&#20026;&#20998;&#24067;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#25552;&#20379;&#26356;&#22810;&#20449;&#24687;&#30340;&#23398;&#20064;&#20449;&#21495;&#12290;&#36890;&#36807;&#37327;&#21270;&#20989;&#25968;&#21442;&#25968;&#21270;&#27599;&#20010;&#36793;&#27969;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#21487;&#20197;&#23398;&#20064;&#39118;&#38505;&#25935;&#24863;&#30340;&#31574;&#30053;&#65292;&#23454;&#29616;&#23545;&#39118;&#38505;&#19981;&#30830;&#23450;&#24615;&#22330;&#26223;&#30340;&#22788;&#29702;&#65292;&#24182;&#22312;&#29616;&#26377;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2302.05793</link><description>&lt;p&gt;
&#24102;&#26377;&#20998;&#24067;&#24335;&#37327;&#21270;&#27969;&#30340;GFlowNets
&lt;/p&gt;
&lt;p&gt;
Distributional GFlowNets with Quantile Flows
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.05793
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#20998;&#24067;&#24335;&#37327;&#21270;&#27969;&#30340;GFlowNets&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#27969;&#20989;&#25968;&#36716;&#21270;&#20026;&#20998;&#24067;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#25552;&#20379;&#26356;&#22810;&#20449;&#24687;&#30340;&#23398;&#20064;&#20449;&#21495;&#12290;&#36890;&#36807;&#37327;&#21270;&#20989;&#25968;&#21442;&#25968;&#21270;&#27599;&#20010;&#36793;&#27969;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#21487;&#20197;&#23398;&#20064;&#39118;&#38505;&#25935;&#24863;&#30340;&#31574;&#30053;&#65292;&#23454;&#29616;&#23545;&#39118;&#38505;&#19981;&#30830;&#23450;&#24615;&#22330;&#26223;&#30340;&#22788;&#29702;&#65292;&#24182;&#22312;&#29616;&#26377;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#27969;&#32593;&#32476;&#65288;GFlowNets&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#27010;&#29575;&#37319;&#26679;&#22120;&#31995;&#21015;&#65292;&#20854;&#20013;&#20195;&#29702;&#36890;&#36807;&#19968;&#31995;&#21015;&#20915;&#31574;&#27493;&#39588;&#23398;&#20064;&#29983;&#25104;&#22797;&#26434;&#32452;&#21512;&#32467;&#26500;&#30340;&#38543;&#26426;&#31574;&#30053;&#12290;&#23613;&#31649;&#21463;&#24378;&#21270;&#23398;&#20064;&#21551;&#21457;&#65292;&#24403;&#21069;&#30340;GFlowNet&#26694;&#26550;&#22312;&#36866;&#29992;&#24615;&#19978;&#30456;&#23545;&#26377;&#38480;&#65292;&#26080;&#27861;&#22788;&#29702;&#22870;&#21169;&#20989;&#25968;&#20013;&#30340;&#38543;&#26426;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20998;&#24067;&#24335;&#33539;&#24335;&#26469;&#22788;&#29702;GFlowNets&#65292;&#23558;&#27599;&#20010;&#27969;&#20989;&#25968;&#36716;&#21270;&#20026;&#19968;&#20010;&#20998;&#24067;&#65292;&#20174;&#32780;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#25552;&#20379;&#26356;&#22810;&#20449;&#24687;&#30340;&#23398;&#20064;&#20449;&#21495;&#12290;&#36890;&#36807;&#36890;&#36807;&#37327;&#21270;&#20989;&#25968;&#23545;&#27599;&#20010;&#36793;&#27969;&#36827;&#34892;&#21442;&#25968;&#21270;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#8220;&#37327;&#21270;&#21305;&#37197;&#8221; GFlowNet&#23398;&#20064;&#31639;&#27861;&#33021;&#22815;&#23398;&#20064;&#39118;&#38505;&#25935;&#24863;&#30340;&#31574;&#30053;&#65292;&#36825;&#26159;&#22788;&#29702;&#39118;&#38505;&#19981;&#30830;&#23450;&#24615;&#22330;&#26223;&#30340;&#22522;&#26412;&#32452;&#25104;&#37096;&#20998;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#19982;&#20043;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#20998;&#24067;&#24335;&#26041;&#27861;&#30001;&#20110;&#25105;&#20204;&#22686;&#24378;&#30340;&#35757;&#32451;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#29616;&#26377;&#22522;&#20934;&#19978;&#23454;&#29616;&#26174;&#30528;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Flow Networks (GFlowNets) are a new family of probabilistic samplers where an agent learns a stochastic policy for generating complex combinatorial structure through a series of decision-making steps. Despite being inspired from reinforcement learning, the current GFlowNet framework is relatively limited in its applicability and cannot handle stochasticity in the reward function. In this work, we adopt a distributional paradigm for GFlowNets, turning each flow function into a distribution, thus providing more informative learning signals during training. By parameterizing each edge flow through their quantile functions, our proposed \textit{quantile matching} GFlowNet learning algorithm is able to learn a risk-sensitive policy, an essential component for handling scenarios with risk uncertainty. Moreover, we find that the distributional approach can achieve substantial improvement on existing benchmarks compared to prior methods due to our enhanced training algorithm, even i
&lt;/p&gt;</description></item><item><title>STLGRU&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#20132;&#36890;&#39044;&#27979;&#27169;&#22411;&#65292;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#20132;&#36890;&#32593;&#32476;&#30340;&#21160;&#24577;&#23616;&#37096;&#21644;&#20840;&#23616;&#26102;&#31354;&#20851;&#31995;&#65292;&#36866;&#29992;&#20110;&#20302;&#21151;&#32791;&#35774;&#22791;</title><link>https://arxiv.org/abs/2212.04548</link><description>&lt;p&gt;
STLGRU&#65306;&#26102;&#31354;&#36731;&#37327;&#32423;&#22270;GRU&#29992;&#20110;&#20132;&#36890;&#27969;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
STLGRU: Spatio-Temporal Lightweight Graph GRU for Traffic Flow Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2212.04548
&lt;/p&gt;
&lt;p&gt;
STLGRU&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#20132;&#36890;&#39044;&#27979;&#27169;&#22411;&#65292;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#20132;&#36890;&#32593;&#32476;&#30340;&#21160;&#24577;&#23616;&#37096;&#21644;&#20840;&#23616;&#26102;&#31354;&#20851;&#31995;&#65292;&#36866;&#29992;&#20110;&#20302;&#21151;&#32791;&#35774;&#22791;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#38752;&#30340;&#20132;&#36890;&#27969;&#39044;&#27979;&#38656;&#35201;&#23545;&#20132;&#36890;&#25968;&#25454;&#36827;&#34892;&#39640;&#25928;&#24314;&#27169;&#12290;&#30001;&#20110;&#21160;&#24577;&#20132;&#36890;&#32593;&#32476;&#20013;&#20986;&#29616;&#19981;&#21516;&#30340;&#30456;&#20851;&#24615;&#21644;&#24433;&#21709;&#65292;&#20351;&#24471;&#24314;&#27169;&#25104;&#20026;&#19968;&#39033;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#29616;&#26377;&#25991;&#29486;&#25552;&#20986;&#20102;&#35768;&#22810;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#25429;&#25417;&#20132;&#36890;&#32593;&#32476;&#30340;&#22797;&#26434;&#26102;&#31354;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#37492;&#20110;&#20132;&#36890;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#65292;&#22987;&#32456;&#25429;&#25417;&#31354;&#38388;&#21644;&#26102;&#38388;&#20381;&#36182;&#24615;&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#38543;&#30528;&#36234;&#26469;&#36234;&#22810;&#22797;&#26434;&#26041;&#27861;&#30340;&#25552;&#20986;&#65292;&#27169;&#22411;&#21464;&#24471;&#36234;&#26469;&#36234;&#28040;&#32791;&#20869;&#23384;&#65292;&#22240;&#27492;&#19981;&#36866;&#21512;&#20302;&#21151;&#32791;&#35774;&#22791;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;STLGRU&#30340;&#26102;&#31354;&#36731;&#37327;&#32423;&#22270;GRU&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#20934;&#30830;&#39044;&#27979;&#20132;&#36890;&#27969;&#37327;&#30340;&#26032;&#22411;&#20132;&#36890;&#39044;&#27979;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;STLGRU&#21487;&#20197;&#26377;&#25928;&#22320;&#25429;&#33719;&#20132;&#36890;&#32593;&#32476;&#30340;&#21160;&#24577;&#23616;&#37096;&#21644;&#20840;&#23616;&#26102;&#31354;&#20851;&#31995;&#65292;&#21033;&#29992;&#22686;&#24378;&#35760;&#24518;&#30340;&#27880;&#24847;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
arXiv:2212.04548v3 Announce Type: replace  Abstract: Reliable forecasting of traffic flow requires efficient modeling of traffic data. Indeed, different correlations and influences arise in a dynamic traffic network, making modeling a complicated task. Existing literature has proposed many different methods to capture traffic networks' complex underlying spatial-temporal relations. However, given the heterogeneity of traffic data, consistently capturing both spatial and temporal dependencies presents a significant challenge. Also, as more and more sophisticated methods are being proposed, models are increasingly becoming memory-heavy and, thus, unsuitable for low-powered devices. To this end, we propose Spatio-Temporal Lightweight Graph GRU, namely STLGRU, a novel traffic forecasting model for predicting traffic flow accurately. Specifically, our proposed STLGRU can effectively capture dynamic local and global spatial-temporal relations of traffic networks using memory-augmented attent
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;&#21453;&#20107;&#23454;&#39118;&#38505;&#26368;&#23567;&#21270;&#26694;&#26550;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#25209;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22312;&#24050;&#35760;&#24405;&#25968;&#25454;&#20013;&#21453;&#39304;&#32570;&#22833;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#19978;&#30028;&#26469;&#22788;&#29702;&#36825;&#31181;&#23398;&#20064;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2209.07148</link><description>&lt;p&gt;
&#20174;&#24050;&#35760;&#24405;&#25968;&#25454;&#20013;&#36827;&#34892;&#21322;&#30417;&#30563;&#25209;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised Batch Learning From Logged Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2209.07148
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;&#21453;&#20107;&#23454;&#39118;&#38505;&#26368;&#23567;&#21270;&#26694;&#26550;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#25209;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22312;&#24050;&#35760;&#24405;&#25968;&#25454;&#20013;&#21453;&#39304;&#32570;&#22833;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#19978;&#30028;&#26469;&#22788;&#29702;&#36825;&#31181;&#23398;&#20064;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#31574;&#30053;&#23398;&#20064;&#26041;&#27861;&#26088;&#22312;&#20174;&#24050;&#35760;&#24405;&#25968;&#25454;&#20013;&#23398;&#20064;&#31574;&#30053;&#65292;&#35813;&#25968;&#25454;&#21253;&#25324;&#27599;&#20010;&#26679;&#26412;&#28857;&#30340;&#29615;&#22659;&#12289;&#21160;&#20316;&#21644;&#21453;&#39304;&#65288;&#25104;&#26412;&#25110;&#22870;&#21169;&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22522;&#20110;&#21453;&#20107;&#23454;&#39118;&#38505;&#26368;&#23567;&#21270;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36824;&#20551;&#35774;&#33021;&#22815;&#35775;&#38382;&#27010;&#29575;&#24471;&#20998;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;&#19968;&#20123;&#26679;&#26412;&#32570;&#22833;&#21453;&#39304;&#30340;&#38382;&#39064;&#25552;&#20986;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#22240;&#27492;&#22312;&#24050;&#35760;&#24405;&#25968;&#25454;&#20013;&#26377;&#20123;&#26679;&#26412;&#26377;&#21453;&#39304;&#65292;&#26377;&#20123;&#26679;&#26412;&#32570;&#22833;&#21453;&#39304;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#31867;&#22411;&#30340;&#23398;&#20064;&#31216;&#20026;&#20174;&#24050;&#35760;&#24405;&#25968;&#25454;&#20013;&#30340;&#21322;&#30417;&#30563;&#25209;&#37327;&#23398;&#20064;&#65292;&#36825;&#22312;&#24191;&#27867;&#30340;&#24212;&#29992;&#39046;&#22495;&#20013;&#20986;&#29616;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#31181;&#23398;&#20064;&#38382;&#39064;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#30495;&#23454;&#39118;&#38505;&#30340;&#26032;&#19978;&#30028;&#65292;&#37319;&#29992;&#20498;&#25968;&#27010;&#29575;&#24471;&#20998;&#20272;&#35745;&#22120;&#12290;&#21033;&#29992;&#36825;&#20010;&#19978;&#30028;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#24050;&#35760;&#24405;&#25968;&#25454;&#30340;&#27491;&#21017;&#21270;&#21322;&#30417;&#30563;&#25209;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#20854;&#20013;&#27491;&#21017;&#21270;&#39033;&#19982;&#21453;&#39304;&#26080;&#20851;&#65292;&#32467;&#26524;&#21487;&#20197;&#20351;&#29992;&#24050;&#35760;&#24405;&#30340;&#32570;&#22833;&#21453;&#39304;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2209.07148v3 Announce Type: replace-cross  Abstract: Off-policy learning methods are intended to learn a policy from logged data, which includes context, action, and feedback (cost or reward) for each sample point. In this work, we build on the counterfactual risk minimization framework, which also assumes access to propensity scores. We propose learning methods for problems where feedback is missing for some samples, so there are samples with feedback and samples missing-feedback in the logged data. We refer to this type of learning as semi-supervised batch learning from logged data, which arises in a wide range of application domains. We derive a novel upper bound for the true risk under the inverse propensity score estimator to address this kind of learning problem. Using this bound, we propose a regularized semi-supervised batch learning method with logged data where the regularization term is feedback-independent and, as a result, can be evaluated using the logged missing-fe
&lt;/p&gt;</description></item><item><title>&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#24182;&#25253;&#21578;&#20102;DL&#23454;&#29616;&#20013;&#30340;&#19968;&#31181;&#26032;&#22411;&#25968;&#25454;&#20381;&#36182;&#24615;&#26102;&#24207;&#20391;&#20449;&#36947;&#27844;&#28431;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#25512;&#29702;&#26102;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2208.01113</link><description>&lt;p&gt;
&#23545;&#20351;&#29992;&#26102;&#38388;&#20391;&#20449;&#36947;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#35780;&#20272;&#29992;&#25143;&#38544;&#31169;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Evaluation of User Privacy in Deep Neural Networks using Timing Side Channel
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2208.01113
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#24182;&#25253;&#21578;&#20102;DL&#23454;&#29616;&#20013;&#30340;&#19968;&#31181;&#26032;&#22411;&#25968;&#25454;&#20381;&#36182;&#24615;&#26102;&#24207;&#20391;&#20449;&#36947;&#27844;&#28431;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#25512;&#29702;&#26102;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#22312;&#35299;&#20915;&#22797;&#26434;&#29616;&#23454;&#19990;&#30028;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#30340;&#36827;&#23637;&#23548;&#33268;&#20854;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#24471;&#21040;&#24191;&#27867;&#37319;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#26426;&#20250;&#20276;&#38543;&#30528;&#37325;&#35201;&#30340;&#28508;&#22312;&#39118;&#38505;&#65292;&#22240;&#20026;&#35768;&#22810;&#27169;&#22411;&#20381;&#36182;&#20110;&#29992;&#20110;&#35757;&#32451;&#30340;&#28041;&#21450;&#38544;&#31169;&#30340;&#25968;&#25454;&#65292;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#20351;&#20854;&#25104;&#20026;&#36807;&#24230;&#26292;&#38706;&#30340;&#38544;&#31169;&#20405;&#29359;&#23041;&#32961;&#38754;&#12290;&#27492;&#22806;&#65292;&#24191;&#27867;&#20351;&#29992;&#22522;&#20110;&#20113;&#30340;&#26426;&#22120;&#23398;&#20064;&#21363;&#26381;&#21153;&#65288;MLaaS&#65289;&#20197;&#33719;&#21462;&#20854;&#24378;&#22823;&#30340;&#22522;&#30784;&#35774;&#26045;&#25903;&#25345;&#65292;&#23558;&#23041;&#32961;&#38754;&#25193;&#23637;&#21040;&#21253;&#25324;&#21508;&#31181;&#36828;&#31243;&#20391;&#20449;&#36947;&#25915;&#20987;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#35782;&#21035;&#24182;&#25253;&#21578;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#30456;&#20851;&#30340;DL&#23454;&#29616;&#20013;&#30340;&#26102;&#24207;&#20391;&#20449;&#36947;&#27844;&#28431;&#65288;&#31216;&#20026;&#31867;&#27844;&#28431;&#65289;&#65292;&#28304;&#33258;&#20110;&#24191;&#27867;&#20351;&#29992;&#30340;DL&#26694;&#26550;PyTorch&#20013;&#30340;&#38750;&#24120;&#37327;&#26102;&#38388;&#20998;&#25903;&#25805;&#20316;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#25512;&#29702;&#26102;&#25915;&#20987;&#65292;&#20854;&#20013;&#25317;&#26377;&#29992;&#25143;&#29305;&#26435;&#21644;&#30828;&#26631;&#31614;&#40657;&#30418;&#35775;&#38382;&#26435;&#38480;&#30340;&#23545;&#25163;
&lt;/p&gt;
&lt;p&gt;
arXiv:2208.01113v3 Announce Type: replace-cross  Abstract: Recent Deep Learning (DL) advancements in solving complex real-world tasks have led to its widespread adoption in practical applications. However, this opportunity comes with significant underlying risks, as many of these models rely on privacy-sensitive data for training in a variety of applications, making them an overly-exposed threat surface for privacy violations. Furthermore, the widespread use of cloud-based Machine-Learning-as-a-Service (MLaaS) for its robust infrastructure support has broadened the threat surface to include a variety of remote side-channel attacks. In this paper, we first identify and report a novel data-dependent timing side-channel leakage (termed Class Leakage) in DL implementations originating from non-constant time branching operation in a widely used DL framework PyTorch. We further demonstrate a practical inference-time attack where an adversary with user privilege and hard-label black-box acces
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#22312;&#19968;&#33324;&#22270;&#32467;&#26500;&#38382;&#39064;&#20013;&#30340;&#26041;&#24046;&#20272;&#35745;&#65292;&#24320;&#21457;&#20102;&#32447;&#24615;&#26102;&#38388;&#20272;&#35745;&#22120;&#24182;&#25552;&#20379;&#20102;&#19978;&#30028;&#65292;&#20801;&#35768;&#25512;&#24191;&#21040;&#26356;&#24191;&#27867;&#30340;&#20998;&#24067;&#31867;&#12290;</title><link>https://arxiv.org/abs/2207.12638</link><description>&lt;p&gt;
&#20855;&#26377;&#34701;&#21512;&#22871;&#32034;&#30340;&#22270;&#20013;&#26041;&#24046;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Variance estimation in graphs with the fused lasso
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2207.12638
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#22312;&#19968;&#33324;&#22270;&#32467;&#26500;&#38382;&#39064;&#20013;&#30340;&#26041;&#24046;&#20272;&#35745;&#65292;&#24320;&#21457;&#20102;&#32447;&#24615;&#26102;&#38388;&#20272;&#35745;&#22120;&#24182;&#25552;&#20379;&#20102;&#19978;&#30028;&#65292;&#20801;&#35768;&#25512;&#24191;&#21040;&#26356;&#24191;&#27867;&#30340;&#20998;&#24067;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#19968;&#33324;&#22270;&#32467;&#26500;&#38382;&#39064;&#20013;&#30340;&#26041;&#24046;&#20272;&#35745;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20026;&#21516;&#26041;&#24046;&#24773;&#20917;&#24320;&#21457;&#20102;&#19968;&#20010;&#32447;&#24615;&#26102;&#38388;&#20272;&#35745;&#22120;&#65292;&#21487;&#20197;&#22312;&#19968;&#33324;&#22270;&#20013;&#19968;&#33268;&#20272;&#35745;&#26041;&#24046;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;&#22343;&#20540;&#20449;&#21495;&#20855;&#26377;&#24635;&#21464;&#21270;&#19982;&#26631;&#20934;&#23610;&#24230;&#26102;&#65292;&#25105;&#20204;&#30340;&#20272;&#35745;&#22120;&#22312;&#38142;&#24335;&#22270;&#21644;&#20108;&#32500;&#32593;&#26684;&#22270;&#19978;&#36798;&#21040;&#26368;&#23567;&#26368;&#22823;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#19968;&#33324;&#22270;&#20013;&#25552;&#20379;&#20102;&#34701;&#21512;&#22871;&#32034;&#20272;&#35745;&#22120;&#30340;&#22343;&#26041;&#35823;&#24046;&#34920;&#29616;&#30340;&#19968;&#33324;&#19978;&#30028;&#65292;&#26681;&#25454;&#30697;&#26465;&#20214;&#21644;&#35823;&#24046;&#23614;&#37096;&#34892;&#20026;&#30340;&#30028;&#38480;&#12290;&#36825;&#20123;&#19978;&#30028;&#20351;&#25105;&#20204;&#33021;&#22815;&#25512;&#24191;&#21040;&#26356;&#24191;&#27867;&#30340;&#20998;&#24067;&#31867;&#65292;&#20363;&#22914;&#20122;&#25351;&#25968;&#20998;&#24067;&#65292;&#35768;&#22810;&#29616;&#26377;&#20851;&#20110;&#34701;&#21512;&#22871;&#32034;&#30340;&#32467;&#26524;&#20165;&#22312;&#20551;&#35774;&#38169;&#35823;&#20026;&#20122;&#39640;&#26031;&#38543;&#26426;&#21464;&#37327;&#30340;&#24773;&#20917;&#19979;&#25104;&#31435;&#12290;&#21033;&#29992;&#25105;&#20204;&#30340;&#19978;&#30028;&#65292;&#25105;&#20204;&#38543;&#21518;&#30740;&#31350;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#24635;&#21464;&#24046;&#27491;&#21017;&#21270;&#20272;&#35745;&#22120;&#65292;&#29992;&#20110;&#20272;&#31639;&#26041;&#24046;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2207.12638v3 Announce Type: replace-cross  Abstract: We study the problem of variance estimation in general graph-structured problems. First, we develop a linear time estimator for the homoscedastic case that can consistently estimate the variance in general graphs. We show that our estimator attains minimax rates for the chain and 2D grid graphs when the mean signal has total variation with canonical scaling. Furthermore, we provide general upper bounds on the mean squared error performance of the fused lasso estimator in general graphs under a moment condition and a bound on the tail behavior of the errors. These upper bounds allow us to generalize for broader classes of distributions, such as sub-exponential, many existing results on the fused lasso that are only known to hold with the assumption that errors are sub-Gaussian random variables. Exploiting our upper bounds, we then study a simple total variation regularization estimator for estimating the signal of variances in t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#33258;&#20027;&#24335;MARL&#65288;SPMARL&#65289;&#20197;&#35299;&#20915;&#24403;&#21069;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#35838;&#31243;&#29983;&#25104;&#30340;&#38382;&#39064;&#65292;&#20248;&#20808;&#32771;&#34385;&#22522;&#20110;&#20219;&#21153;&#30340;&#20248;&#20808;&#32423;&#12290;</title><link>https://arxiv.org/abs/2205.10016</link><description>&lt;p&gt;
&#23398;&#20064;&#36827;&#24230;&#39537;&#21160;&#30340;&#22810;&#26234;&#33021;&#20307;&#35838;&#31243;
&lt;/p&gt;
&lt;p&gt;
Learning Progress Driven Multi-Agent Curriculum
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2205.10016
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#33258;&#20027;&#24335;MARL&#65288;SPMARL&#65289;&#20197;&#35299;&#20915;&#24403;&#21069;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#35838;&#31243;&#29983;&#25104;&#30340;&#38382;&#39064;&#65292;&#20248;&#20808;&#32771;&#34385;&#22522;&#20110;&#20219;&#21153;&#30340;&#20248;&#20808;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#65288;CRL&#65289;&#26088;&#22312;&#36890;&#36807;&#36880;&#28176;&#22686;&#21152;&#20219;&#21153;&#30340;&#38590;&#24230;&#65288;&#36890;&#24120;&#30001;&#21487;&#23454;&#29616;&#30340;&#39044;&#26399;&#22238;&#25253;&#37327;&#21270;&#65289;&#26469;&#21152;&#24555;&#23398;&#20064;&#36895;&#24230;&#12290;&#21463;CRL&#22312;&#21333;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#19968;&#20123;&#30740;&#31350;&#23581;&#35797;&#23558;CRL&#24212;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#65292;&#20351;&#29992;&#26234;&#33021;&#20307;&#25968;&#37327;&#26469;&#25511;&#21046;&#20219;&#21153;&#38590;&#24230;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#36890;&#24120;&#20351;&#29992;&#25163;&#21160;&#23450;&#20041;&#30340;&#35838;&#31243;&#65292;&#22914;&#32447;&#24615;&#26041;&#26696;&#12290;&#26412;&#25991;&#39318;&#20808;&#23558;&#26368;&#20808;&#36827;&#30340;&#21333;&#26234;&#33021;&#20307;&#33258;&#20027;&#24335;CRL&#24212;&#29992;&#20110;&#31232;&#30095;&#22870;&#21169;MARL&#12290;&#34429;&#28982;&#34920;&#29616;&#20196;&#20154;&#28385;&#24847;&#65292;&#20294;&#25105;&#20204;&#30830;&#23450;&#20102;&#29616;&#26377;&#22522;&#20110;&#22870;&#21169;&#30340;CRL&#26041;&#27861;&#29983;&#25104;&#30340;&#35838;&#31243;&#23384;&#22312;&#20004;&#20010;&#28508;&#22312;&#32570;&#38519;&#65306;&#65288;1&#65289;&#39640;&#22238;&#25253;&#30340;&#20219;&#21153;&#21487;&#33021;&#19981;&#25552;&#20379;&#20449;&#24687;&#37327;&#22823;&#30340;&#23398;&#20064;&#20449;&#21495;&#65292;&#65288;2&#65289;&#22312;&#22810;&#26234;&#33021;&#20307;&#20135;&#29983;&#26356;&#39640;&#22238;&#25253;&#30340;&#20219;&#21153;&#20013;&#65292;&#21152;&#21095;&#20102;&#23398;&#20998;&#20998;&#37197;&#22256;&#38590;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#33258;&#20027;&#24335;MARL&#65288;SPMARL&#65289;&#65292;&#20197;&#22522;&#20110;&#20219;&#21153;&#30340;&#20248;&#20808;&#32423;&#36827;&#34892;&#23433;&#25490;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2205.10016v2 Announce Type: replace  Abstract: Curriculum reinforcement learning (CRL) aims to speed up learning by gradually increasing the difficulty of a task, usually quantified by the achievable expected return. Inspired by the success of CRL in single-agent settings, a few works have attempted to apply CRL to multi-agent reinforcement learning (MARL) using the number of agents to control task difficulty. However, existing works typically use manually defined curricula such as a linear scheme. In this paper, we first apply state-of-the-art single-agent self-paced CRL to sparse reward MARL. Although with satisfying performance, we identify two potential flaws of the curriculum generated by existing reward-based CRL methods: (1) tasks with high returns may not provide informative learning signals and (2) the exacerbated credit assignment difficulty in tasks where more agents yield higher returns. Thereby, we further propose self-paced MARL (SPMARL) to prioritize tasks based on
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#65292;&#33021;&#22815;&#25429;&#33719;&#32454;&#31890;&#24230;&#30340;&#20960;&#20309;&#20449;&#24687;&#65292;&#32780;&#26080;&#38656;&#20381;&#36182;&#20809;&#28369;&#24615;&#20551;&#35774;&#12290;</title><link>https://arxiv.org/abs/2205.05795</link><description>&lt;p&gt;
&#20195;&#25968;&#26426;&#22120;&#23398;&#20064;&#21450;&#20854;&#22312;&#21270;&#23398;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Algebraic Machine Learning with an Application to Chemistry
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2205.05795
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#65292;&#33021;&#22815;&#25429;&#33719;&#32454;&#31890;&#24230;&#30340;&#20960;&#20309;&#20449;&#24687;&#65292;&#32780;&#26080;&#38656;&#20381;&#36182;&#20809;&#28369;&#24615;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31185;&#23398;&#24212;&#29992;&#20013;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#21464;&#24471;&#26356;&#21152;&#22797;&#26434;&#65292;&#30740;&#31350;&#25968;&#25454;&#30340;&#20960;&#20309;&#21644;&#25299;&#25169;&#32467;&#26500;&#24050;&#25104;&#20026;&#25968;&#25454;&#20998;&#26512;&#36807;&#31243;&#20013;&#26085;&#30410;&#37325;&#35201;&#30340;&#19968;&#37096;&#20998;&#12290;&#36825;&#21487;&#20197;&#20174;&#23545;&#25345;&#20037;&#21516;&#35843;&#31561;&#25299;&#25169;&#24037;&#20855;&#26085;&#30410;&#22686;&#38271;&#30340;&#20852;&#36259;&#20013;&#30475;&#20986;&#12290;&#28982;&#32780;&#65292;&#19968;&#26041;&#38754;&#65292;&#25299;&#25169;&#24037;&#20855;&#26412;&#36136;&#19978;&#20165;&#25552;&#20379;&#20851;&#20110;&#25968;&#25454;&#22522;&#26412;&#31354;&#38388;&#30340;&#31895;&#30053;&#20449;&#24687;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#26356;&#22810;&#20960;&#20309;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#27969;&#24418;&#20551;&#35774;&#65292;&#21363;&#22522;&#26412;&#31354;&#38388;&#26159;&#20809;&#28369;&#27969;&#24418;&#12290;&#36825;&#19968;&#20551;&#35774;&#23545;&#20110;&#35768;&#22810;&#21253;&#21547;&#22855;&#28857;&#30340;&#29289;&#29702;&#27169;&#22411;&#26159;&#19981;&#25104;&#31435;&#30340;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#65292;&#33021;&#22815;&#25429;&#33719;&#32454;&#31890;&#24230;&#30340;&#20960;&#20309;&#20449;&#24687;&#65292;&#32780;&#26080;&#38656;&#20381;&#36182;&#20219;&#20309;&#20809;&#28369;&#24615;&#20551;&#35774;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#20195;&#25968;&#20960;&#20309;&#21644;&#20195;&#25968;&#22810;&#39033;&#24335;&#30340;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2205.05795v3 Announce Type: replace-cross  Abstract: As datasets used in scientific applications become more complex, studying the geometry and topology of data has become an increasingly prevalent part of the data analysis process. This can be seen for example with the growing interest in topological tools such as persistent homology. However, on the one hand, topological tools are inherently limited to providing only coarse information about the underlying space of the data. On the other hand, more geometric approaches rely predominately on the manifold hypothesis, which asserts that the underlying space is a smooth manifold. This assumption fails for many physical models where the underlying space contains singularities.   In this paper we develop a machine learning pipeline that captures fine-grain geometric information without having to rely on any smoothness assumptions. Our approach involves working within the scope of algebraic geometry and algebraic varieties instead of 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26041;&#31243;&#21457;&#29616;&#26041;&#27861;&#65292;&#32467;&#21512;&#20256;&#32479;&#38543;&#26426;&#24494;&#31215;&#20998;&#26041;&#27861;&#21644;&#26368;&#20808;&#36827;&#30340;&#26041;&#31243;&#21457;&#29616;&#25216;&#26415;&#65292;&#20174;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#20998;&#26512;&#27874;&#21160;&#24182;&#36755;&#20986;&#21487;&#35299;&#37322;&#30340;SDE&#65292;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#26041;&#27861;&#30340;&#26222;&#36866;&#24615;&#21644;&#36866;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2205.02645</link><description>&lt;p&gt;
&#20174;&#29983;&#29289;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#21457;&#29616;&#38543;&#26426;&#21160;&#21147;&#23398;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Discovering stochastic dynamical equations from biological time series data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2205.02645
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26041;&#31243;&#21457;&#29616;&#26041;&#27861;&#65292;&#32467;&#21512;&#20256;&#32479;&#38543;&#26426;&#24494;&#31215;&#20998;&#26041;&#27861;&#21644;&#26368;&#20808;&#36827;&#30340;&#26041;&#31243;&#21457;&#29616;&#25216;&#26415;&#65292;&#20174;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#20998;&#26512;&#27874;&#21160;&#24182;&#36755;&#20986;&#21487;&#35299;&#37322;&#30340;SDE&#65292;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#26041;&#27861;&#30340;&#26222;&#36866;&#24615;&#21644;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDEs&#65289;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#27169;&#25311;&#20855;&#26377;&#38543;&#26426;&#24615;&#30340;&#21160;&#24577;&#65292;&#36825;&#22312;&#22823;&#22810;&#25968;&#29983;&#29289;&#31995;&#32479;&#20013;&#37117;&#24456;&#24120;&#35265;&#12290;&#23558;&#36825;&#20123;&#27169;&#22411;&#19982;&#23454;&#35777;&#25968;&#25454;&#36827;&#34892;&#25972;&#21512;&#30340;&#36870;&#38382;&#39064;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#31243;&#21457;&#29616;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20316;&#20026;&#36755;&#20837;&#65292;&#20998;&#26512;&#32454;&#31890;&#24230;&#27874;&#21160;&#24182;&#36755;&#20986;&#19968;&#20010;&#21487;&#20197;&#27491;&#30830;&#25429;&#25417;&#25968;&#25454;&#38271;&#26102;&#38388;&#21160;&#24577;&#30340;&#21487;&#35299;&#37322;SDE&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#26469;&#33258;&#38543;&#26426;&#24494;&#31215;&#20998;&#25991;&#29486;&#30340;&#20256;&#32479;&#26041;&#27861;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#31243;&#21457;&#29616;&#25216;&#26415;&#30456;&#32467;&#21512;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#20004;&#20010;&#26102;&#31354;&#23610;&#24230;&#36837;&#24322;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26222;&#36866;&#24615;&#21644;&#36866;&#29992;&#24615;&#65306;&#65288;i&#65289;&#40060;&#32676;&#30340;&#38598;&#20307;&#36816;&#21160;&#65292;&#20854;&#20013;&#38543;&#26426;&#24615;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#21644;&#65288;ii&#65289;&#21333;&#20010;&#32454;&#32990;&#21463;&#38480;&#36801;&#31227;&#65292;&#20027;&#35201;&#26159;&#36981;&#24490;&#25918;&#26494;&#25391;&#33633;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2205.02645v5 Announce Type: replace-cross  Abstract: Stochastic differential equations (SDEs) are an important framework to model dynamics with randomness, as is common in most biological systems. The inverse problem of integrating these models with empirical data remains a major challenge. Here, we present an equation discovery methodology that takes time series data as an input, analyses fine scale fluctuations and outputs an interpretable SDE that can correctly capture long-time dynamics of data. We achieve this by combining traditional approaches from stochastic calculus literature with state-of-the-art equation discovery techniques. We validate our approach on synthetic datasets, and demonstrate the generality and applicability of the method on two real-world datasets of vastly different spatiotemporal scales: (i) collective movement of fish school where stochasticity plays a crucial role, and (ii) confined migration of a single cell, primarily following a relaxed oscillatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#23548;&#33268;&#31070;&#32463;&#32593;&#32476;&#31232;&#30095;&#24615;&#21644;&#36873;&#25321;&#24615;&#22686;&#21152;&#30340;&#21508;&#31181;&#26465;&#20214;&#12290;</title><link>https://arxiv.org/abs/2201.11653</link><description>&lt;p&gt;
&#12298;SGD&#21644;&#33258;&#36866;&#24212;&#23398;&#20064;&#35268;&#21017;&#23398;&#21040;&#30340;&#34920;&#31034;&#65306;&#21464;&#21270;&#31232;&#30095;&#24615;&#21644;&#36873;&#25321;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#26465;&#20214;&#12299;
&lt;/p&gt;
&lt;p&gt;
Representations learnt by SGD and Adaptive learning rules: Conditions that vary sparsity and selectivity in neural network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2201.11653
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#23548;&#33268;&#31070;&#32463;&#32593;&#32476;&#31232;&#30095;&#24615;&#21644;&#36873;&#25321;&#24615;&#22686;&#21152;&#30340;&#21508;&#31181;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20154;&#33041;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36830;&#32493;&#23398;&#20064;&#21487;&#20197;&#25191;&#34892;&#21508;&#31181;&#20219;&#21153;&#32780;&#20114;&#19981;&#24178;&#25200;&#12290;&#20943;&#23569;&#20114;&#30456;&#24178;&#25200;&#30340;&#26377;&#25928;&#26041;&#24335;&#21487;&#20197;&#22312;&#31070;&#32463;&#20803;&#30340;&#31232;&#30095;&#24615;&#21644;&#36873;&#25321;&#24615;&#20013;&#25214;&#21040;&#12290;&#26681;&#25454;Aljundi&#31561;&#20154;&#21644;Hadsell&#31561;&#20154;&#30340;&#35266;&#28857;&#65292;&#22312;&#34920;&#31034;&#27700;&#24179;&#26045;&#21152;&#31232;&#30095;&#24615;&#23545;&#36830;&#32493;&#23398;&#20064;&#26159;&#26377;&#21033;&#30340;&#65292;&#22240;&#20026;&#31232;&#30095;&#30340;&#31070;&#32463;&#20803;&#28608;&#27963;&#40723;&#21169;&#21442;&#25968;&#20043;&#38388;&#30340;&#23569;&#37325;&#21472;&#65292;&#23548;&#33268;&#26356;&#23569;&#30340;&#24178;&#25200;&#12290;&#21516;&#26679;&#65292;&#39640;&#24230;&#36873;&#25321;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#21487;&#33021;&#20250;&#24341;&#36215;&#36739;&#23569;&#30340;&#24178;&#25200;&#65292;&#22240;&#20026;&#31070;&#32463;&#20803;&#20013;&#30340;&#29305;&#23450;&#21709;&#24212;&#23558;&#20943;&#23569;&#19982;&#20854;&#20182;&#21442;&#25968;&#30340;&#37325;&#21472;&#26426;&#20250;&#12290;&#32771;&#34385;&#21040;&#20154;&#33041;&#22312;&#19968;&#29983;&#20013;&#25191;&#34892;&#36830;&#32493;&#23398;&#20064;&#65292;&#25214;&#21040;&#33258;&#28982;&#22686;&#21152;&#31232;&#30095;&#24615;&#21644;&#36873;&#25321;&#24615;&#30340;&#26465;&#20214;&#21487;&#33021;&#20026;&#20102;&#35299;&#22823;&#33041;&#21151;&#33021;&#25552;&#20379;&#35265;&#35299;&#12290;&#26412;&#25991;&#35843;&#26597;&#20102;&#33258;&#28982;&#22686;&#21152;&#31070;&#32463;&#32593;&#32476;&#31232;&#30095;&#24615;&#21644;&#36873;&#25321;&#24615;&#30340;&#21508;&#31181;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2201.11653v2 Announce Type: replace  Abstract: From the point of view of the human brain, continual learning can perform various tasks without mutual interference. An effective way to reduce mutual interference can be found in sparsity and selectivity of neurons. According to Aljundi et al. and Hadsell et al., imposing sparsity at the representational level is advantageous for continual learning because sparse neuronal activations encourage less overlap between parameters, resulting in less interference. Similarly, highly selective neural networks are likely to induce less interference since particular response in neurons will reduce the chance of overlap with other parameters. Considering that the human brain performs continual learning over the lifespan, finding conditions where sparsity and selectivity naturally arises may provide insight for understanding how the brain functions. This paper investigates various conditions that naturally increase sparsity and selectivity in a 
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#22522;&#20110;&#26041;&#24046;&#20943;&#23569;&#30340;&#32463;&#39564;&#22238;&#25918;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#36873;&#25321;&#24615;&#37325;&#22797;&#21033;&#29992;&#30456;&#20851;&#26679;&#26412;&#26469;&#25913;&#21892;&#31574;&#30053;&#26799;&#24230;&#20272;&#35745;&#65292;&#24182;&#26500;&#24314;&#20102;&#39640;&#25928;&#30340;&#31163;&#31574;&#30053;&#31639;&#27861;PG-VRER&#12290;</title><link>https://arxiv.org/abs/2110.08902</link><description>&lt;p&gt;
&#22522;&#20110;&#26041;&#24046;&#20943;&#23569;&#30340;&#32463;&#39564;&#22238;&#25918;&#29992;&#20110;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Variance Reduction Based Experience Replay for Policy Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2110.08902
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#22522;&#20110;&#26041;&#24046;&#20943;&#23569;&#30340;&#32463;&#39564;&#22238;&#25918;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#36873;&#25321;&#24615;&#37325;&#22797;&#21033;&#29992;&#30456;&#20851;&#26679;&#26412;&#26469;&#25913;&#21892;&#31574;&#30053;&#26799;&#24230;&#20272;&#35745;&#65292;&#24182;&#26500;&#24314;&#20102;&#39640;&#25928;&#30340;&#31163;&#31574;&#30053;&#31639;&#27861;PG-VRER&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22797;&#26434;&#38543;&#26426;&#31995;&#32479;&#19978;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#26102;&#65292;&#26377;&#25928;&#21033;&#29992;&#21382;&#21490;&#26679;&#26412;&#20013;&#30340;&#20449;&#24687;&#20197;&#21152;&#36895;&#31574;&#30053;&#20248;&#21270;&#26159;&#24456;&#26377;&#24517;&#35201;&#30340;&#12290;&#20256;&#32479;&#30340;&#32463;&#39564;&#22238;&#25918;&#34429;&#28982;&#26377;&#25928;&#65292;&#20294;&#26159;&#23558;&#25152;&#26377;&#35266;&#27979;&#37117;&#35270;&#20026;&#30456;&#21516;&#65292;&#24573;&#30053;&#20102;&#23427;&#20204;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#24046;&#20943;&#23569;&#32463;&#39564;&#22238;&#25918;&#65288;VRER&#65289;&#26694;&#26550;&#65292;&#23454;&#29616;&#23545;&#30456;&#20851;&#26679;&#26412;&#30340;&#36873;&#25321;&#24615;&#37325;&#22797;&#21033;&#29992;&#65292;&#20174;&#32780;&#25913;&#21892;&#31574;&#30053;&#26799;&#24230;&#20272;&#35745;&#12290;VRER&#20316;&#20026;&#19968;&#31181;&#36866;&#24212;&#24615;&#26041;&#27861;&#65292;&#21487;&#20197;&#26080;&#32541;&#38598;&#25104;&#21040;&#19981;&#21516;&#30340;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#20013;&#65292;&#26500;&#24314;&#20102;&#25105;&#20204;&#39640;&#25928;&#30340;&#31163;&#31574;&#30053;&#31639;&#27861;Policy Optimization with VRER (PG-VRER)&#12290;&#27492;&#22806;&#65292;&#25991;&#29486;&#20013;&#23545;&#32463;&#39564;&#22238;&#25918;&#26041;&#27861;&#32570;&#20047;&#20005;&#26684;&#30340;&#29702;&#35770;&#29702;&#35299;&#65292;&#36825;&#20419;&#20351;&#25105;&#20204;&#24341;&#20837;&#19968;&#20010;&#26032;&#39062;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#32771;&#34385;&#26679;&#26412;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2110.08902v3 Announce Type: replace-cross  Abstract: For reinforcement learning on complex stochastic systems, it is desirable to effectively leverage the information from historical samples collected in previous iterations to accelerate policy optimization. Classical experience replay, while effective, treats all observations uniformly, neglecting their relative importance. To address this limitation, we introduce a novel Variance Reduction Experience Replay (VRER) framework, enabling the selective reuse of relevant samples to improve policy gradient estimation. VRER, as an adaptable method that can seamlessly integrate with different policy optimization algorithms, forms the foundation of our sample-efficient off-policy algorithm known as Policy Optimization with VRER (PG-VRER). Furthermore, the lack of a rigorous theoretical understanding of the experience replay method in the literature motivates us to introduce a novel theoretical framework that accounts for sample dependenc
&lt;/p&gt;</description></item><item><title>HCR-Net&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36801;&#31227;&#23398;&#20064;&#30340;&#33073;&#26426;&#25163;&#20889;&#23383;&#31526;&#35782;&#21035;&#32593;&#32476;&#65292;&#36890;&#36807;&#37096;&#20998;&#21033;&#29992;&#39044;&#35757;&#32451;&#32593;&#32476;&#30340;&#29305;&#24449;&#25552;&#21462;&#23618;&#65292;&#23454;&#29616;&#20102;&#26356;&#24555;&#12289;&#26356;&#39640;&#25928;&#30340;&#35757;&#32451;&#65292;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2108.06663</link><description>&lt;p&gt;
HCR-Net&#65306;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#33073;&#26426;&#25163;&#20889;&#23383;&#31526;&#35782;&#21035;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
HCR-Net: A deep learning based script independent handwritten character recognition network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2108.06663
&lt;/p&gt;
&lt;p&gt;
HCR-Net&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36801;&#31227;&#23398;&#20064;&#30340;&#33073;&#26426;&#25163;&#20889;&#23383;&#31526;&#35782;&#21035;&#32593;&#32476;&#65292;&#36890;&#36807;&#37096;&#20998;&#21033;&#29992;&#39044;&#35757;&#32451;&#32593;&#32476;&#30340;&#29305;&#24449;&#25552;&#21462;&#23618;&#65292;&#23454;&#29616;&#20102;&#26356;&#24555;&#12289;&#26356;&#39640;&#25928;&#30340;&#35757;&#32451;&#65292;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#32463;&#36807;&#20960;&#21313;&#24180;&#30340;&#30740;&#31350;&#65292;&#25163;&#20889;&#23383;&#31526;&#35782;&#21035;&#65288;HCR&#65289;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#27169;&#24335;&#35782;&#21035;&#38382;&#39064;&#65292;&#24182;&#19988;&#32570;&#20047;&#33073;&#26426;&#35782;&#21035;&#25216;&#26415;&#30340;&#30740;&#31350;&#12290;&#36825;&#20027;&#35201;&#26159;&#30001;&#20110;&#30456;&#20284;&#30340;&#23383;&#31526;&#32467;&#26500;&#12289;&#19981;&#21516;&#30340;&#25163;&#20889;&#39118;&#26684;&#12289;&#19981;&#21516;&#30340;&#20070;&#20889;&#31995;&#32479;&#12289;&#25163;&#24037;&#29305;&#24449;&#25552;&#21462;&#25216;&#26415;&#12289;&#25968;&#25454;&#21644;&#20195;&#30721;&#30340;&#19981;&#21487;&#29992;&#24615;&#65292;&#20197;&#21450;&#33050;&#26412;&#29305;&#23450;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#21457;&#23637;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;HCR-Net&#30340;&#33073;&#26426;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#65292;&#20026;HCR&#30740;&#31350;&#24320;&#36767;&#20102;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;HCR-Net&#22522;&#20110;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;HCR&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;\textit{&#37096;&#20998;&#21033;&#29992;}&#20102;&#39044;&#35757;&#32451;&#32593;&#32476;&#30340;&#29305;&#24449;&#25552;&#21462;&#23618;&#12290;&#30001;&#20110;&#36801;&#31227;&#23398;&#20064;&#21644;&#22270;&#20687;&#22686;&#24378;&#65292;HCR-Net&#25552;&#20379;&#20102;&#26356;&#24555;&#12289;&#26356;&#39640;&#25928;&#30340;&#35757;&#32451;&#65292;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#19988;&#21487;&#20197;&#22788;&#29702;&#23569;&#37327;&#25968;&#25454;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2108.06663v4 Announce Type: replace-cross  Abstract: Handwritten character recognition (HCR) remains a challenging pattern recognition problem despite decades of research, and lacks research on script independent recognition techniques. {\color{black}This is mainly because of similar character structures, different handwriting styles, diverse scripts, handcrafted feature extraction techniques, unavailability of data and code, and the development of script-specific deep learning techniques. To address these limitations, we have proposed a script independent deep learning network for HCR research, called HCR-Net, that sets a new research direction for the field. HCR-Net is based on a novel transfer learning approach for HCR, which \textit{partly utilizes} feature extraction layers of a pre-trained network.} Due to transfer learning and image augmentation, HCR-Net provides faster and computationally efficient training, better performance and generalizations, and can work with small 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32676;&#31232;&#30095;&#30697;&#38453;&#20998;&#35299;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#26032;&#39046;&#22495;&#36827;&#34892;&#35789;&#23884;&#20837;&#30340;&#20256;&#36882;&#23398;&#20064;&#65292;&#20197;&#35299;&#20915;&#19981;&#21516;&#39046;&#22495;&#21333;&#35789;&#21547;&#20041;&#24046;&#24322;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2104.08928</link><description>&lt;p&gt;
&#22522;&#20110;&#32676;&#31232;&#30095;&#30697;&#38453;&#20998;&#35299;&#30340;&#35789;&#23884;&#20837;&#20256;&#36882;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Group-Sparse Matrix Factorization for Transfer Learning of Word Embeddings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2104.08928
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32676;&#31232;&#30095;&#30697;&#38453;&#20998;&#35299;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#26032;&#39046;&#22495;&#36827;&#34892;&#35789;&#23884;&#20837;&#30340;&#20256;&#36882;&#23398;&#20064;&#65292;&#20197;&#35299;&#20915;&#19981;&#21516;&#39046;&#22495;&#21333;&#35789;&#21547;&#20041;&#24046;&#24322;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20026;&#35768;&#22810;&#39046;&#22495;&#30340;&#20915;&#31574;&#32773;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#25968;&#25454;&#28304;&#65292;&#28085;&#30422;&#33539;&#22260;&#20174;&#38646;&#21806;&#20013;&#30340;&#20135;&#21697;&#35780;&#35770;&#21040;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#25252;&#29702;&#35760;&#24405;&#12290;&#20026;&#20102;&#21033;&#29992;&#36825;&#20123;&#20449;&#24687;&#65292;&#36890;&#24120;&#20250;&#36890;&#36807;&#26080;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#65288;&#22914;&#30697;&#38453;&#20998;&#35299;&#65289;&#23558;&#21333;&#35789;&#36716;&#25442;&#20026;&#35789;&#23884;&#20837;&#8212;&#8212;&#32534;&#30721;&#21333;&#35789;&#20043;&#38388;&#35821;&#20041;&#20851;&#31995;&#30340;&#21521;&#37327;&#12290;&#28982;&#32780;&#65292;&#20174;&#20855;&#26377;&#26377;&#38480;&#35757;&#32451;&#25968;&#25454;&#30340;&#26032;&#39046;&#22495;&#23398;&#20064;&#21333;&#35789;&#23884;&#20837;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#22312;&#26032;&#39046;&#22495;&#20013;&#65292;&#21333;&#35789;&#30340;&#21547;&#20041;/&#29992;&#27861;&#21487;&#33021;&#19981;&#21516;&#65292;&#20363;&#22914;&#65292;&#8220;positive&#8221;&#19968;&#35789;&#36890;&#24120;&#20855;&#26377;&#27491;&#38754;&#24773;&#32490;&#65292;&#20294;&#22312;&#21307;&#30103;&#35760;&#24405;&#20013;&#24448;&#24448;&#20855;&#26377;&#36127;&#38754;&#24773;&#32490;&#65292;&#22240;&#20026;&#23427;&#21487;&#33021;&#24847;&#21619;&#30528;&#24739;&#32773;&#26816;&#27979;&#21576;&#38451;&#24615;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#25105;&#20204;&#39044;&#35745;&#21482;&#26377;&#23569;&#37327;&#39046;&#22495;&#29305;&#23450;&#21333;&#35789;&#21487;&#33021;&#20855;&#26377;&#26032;&#21547;&#20041;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#30452;&#35266;&#30340;&#20004;&#38454;&#27573;&#20272;&#35745;&#22120;&#65292;&#36890;&#36807;&#32676;&#31232;&#30095;&#24809;&#32602;&#26469;&#26377;&#25928;&#22320;&#20256;&#36882;&#23398;&#20064;&#39046;&#22495;&#29305;&#23450;&#30340;&#26032;&#21547;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2104.08928v3 Announce Type: replace-cross  Abstract: Unstructured text provides decision-makers with a rich data source in many domains, ranging from product reviews in retail to nursing notes in healthcare. To leverage this information, words are typically translated into word embeddings -- vectors that encode the semantic relationships between words -- through unsupervised learning algorithms such as matrix factorization. However, learning word embeddings from new domains with limited training data can be challenging, because the meaning/usage may be different in the new domain, e.g., the word ``positive'' typically has positive sentiment, but often has negative sentiment in medical notes since it may imply that a patient tested positive for a disease. In practice, we expect that only a small number of domain-specific words may have new meanings. We propose an intuitive two-stage estimator that exploits this structure via a group-sparse penalty to efficiently transfer learn dom
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#27773;&#36710;&#38647;&#36798;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#24320;&#21457;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#38647;&#36798;&#24863;&#30693;&#31639;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#31227;&#21160;&#36947;&#36335;&#29992;&#25143;&#12290;</title><link>https://arxiv.org/abs/2104.02493</link><description>&lt;p&gt;
RadarScenes: &#29992;&#20110;&#27773;&#36710;&#24212;&#29992;&#30340;&#23454;&#38469;&#38647;&#36798;&#28857;&#20113;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
RadarScenes: A Real-World Radar Point Cloud Data Set for Automotive Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2104.02493
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#27773;&#36710;&#38647;&#36798;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#24320;&#21457;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#38647;&#36798;&#24863;&#30693;&#31639;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#31227;&#21160;&#36947;&#36335;&#29992;&#25143;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#27773;&#36710;&#38647;&#36798;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#36229;&#36807;&#22235;&#20010;&#23567;&#26102;&#39550;&#39542;&#30340;&#27979;&#37327;&#25968;&#25454;&#21644;&#28857;&#32423;&#26631;&#27880;&#12290;&#26469;&#33258;&#19968;&#36742;&#27979;&#35797;&#36710;&#19978;&#23433;&#35013;&#30340;&#22235;&#20010;&#31995;&#21015;&#38647;&#36798;&#20256;&#24863;&#22120;&#25552;&#20379;&#30340;&#25968;&#25454;&#34987;&#35760;&#24405;&#19979;&#26469;&#65292;&#21160;&#24577;&#30446;&#26631;&#30340;&#27599;&#20010;&#26816;&#27979;&#37117;&#34987;&#25163;&#21160;&#20998;&#32452;&#21040;&#31751;&#20013;&#65292;&#24182;&#38543;&#21518;&#36827;&#34892;&#26631;&#35760;&#12290;&#35813;&#25968;&#25454;&#38598;&#30340;&#30446;&#30340;&#26159;&#20026;&#20102;&#20419;&#36827;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26032;&#22411;&#38647;&#36798;&#24863;&#30693;&#31639;&#27861;&#30340;&#24320;&#21457;&#65292;&#37325;&#28857;&#25918;&#22312;&#31227;&#21160;&#36947;&#36335;&#29992;&#25143;&#19978;&#12290;&#35760;&#24405;&#24207;&#21015;&#30340;&#22270;&#20687;&#26159;&#20351;&#29992;&#35760;&#24405;&#25668;&#20687;&#26426;&#25429;&#33719;&#30340;&#12290;&#20026;&#20102;&#35780;&#20272;&#26410;&#26469;&#30340;&#30446;&#26631;&#26816;&#27979;&#21644;&#20998;&#31867;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#24471;&#20998;&#35745;&#31639;&#30340;&#24314;&#35758;&#65292;&#20197;&#20415;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#22312;&#19968;&#20010;&#20849;&#21516;&#30340;&#22522;&#30784;&#19978;&#35780;&#20272;&#20182;&#20204;&#30340;&#31639;&#27861;&#12290;&#39069;&#22806;&#20449;&#24687;&#20197;&#21450;&#19979;&#36733;&#35828;&#26126;&#21487;&#20197;&#22312;&#25968;&#25454;&#38598;&#30340;&#32593;&#31449;&#19978;&#25214;&#21040;&#65306;www.radar-scenes.com&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2104.02493v2 Announce Type: replace  Abstract: A new automotive radar data set with measurements and point-wise annotations from more than four hours of driving is presented. Data provided by four series radar sensors mounted on one test vehicle were recorded and the individual detections of dynamic objects were manually grouped to clusters and labeled afterwards. The purpose of this data set is to enable the development of novel (machine learning-based) radar perception algorithms with the focus on moving road users. Images of the recorded sequences were captured using a documentary camera. For the evaluation of future object detection and classification algorithms, proposals for score calculation are made so that researchers can evaluate their algorithms on a common basis. Additional information as well as download instructions can be found on the website of the data set: www.radar-scenes.com.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Deep-Lock&#30340;&#36890;&#29992;&#21644;&#36731;&#37327;&#32423;&#22522;&#20110;&#23494;&#38053;&#30340;&#27169;&#22411;&#38145;&#23450;&#26041;&#26696;&#65292;&#36890;&#36807;&#20351;&#29992;S-&#30418;&#23545;&#35757;&#32451;&#23436;&#27605;&#30340;DNN&#27169;&#22411;&#30340;&#27599;&#20010;&#21442;&#25968;&#36827;&#34892;&#21152;&#23494;&#65292;&#24182;&#30830;&#20445;&#21482;&#26377;&#22312;&#24212;&#29992;&#27491;&#30830;&#30340;&#31192;&#23494;&#23494;&#38053;&#26102;&#27169;&#22411;&#25165;&#33021;&#27491;&#30830;&#36816;&#34892;&#65292;&#20174;&#32780;&#38450;&#27490;&#20102;DNN&#27169;&#22411;&#30340;&#26410;&#32463;&#25480;&#26435;&#20351;&#29992;&#12290;</title><link>https://arxiv.org/abs/2008.05966</link><description>&lt;p&gt;
Deep-Lock: &#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#23433;&#20840;&#25480;&#26435;
&lt;/p&gt;
&lt;p&gt;
Deep-Lock: Secure Authorization for Deep Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2008.05966
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Deep-Lock&#30340;&#36890;&#29992;&#21644;&#36731;&#37327;&#32423;&#22522;&#20110;&#23494;&#38053;&#30340;&#27169;&#22411;&#38145;&#23450;&#26041;&#26696;&#65292;&#36890;&#36807;&#20351;&#29992;S-&#30418;&#23545;&#35757;&#32451;&#23436;&#27605;&#30340;DNN&#27169;&#22411;&#30340;&#27599;&#20010;&#21442;&#25968;&#36827;&#34892;&#21152;&#23494;&#65292;&#24182;&#30830;&#20445;&#21482;&#26377;&#22312;&#24212;&#29992;&#27491;&#30830;&#30340;&#31192;&#23494;&#23494;&#38053;&#26102;&#27169;&#22411;&#25165;&#33021;&#27491;&#30830;&#36816;&#34892;&#65292;&#20174;&#32780;&#38450;&#27490;&#20102;DNN&#27169;&#22411;&#30340;&#26410;&#32463;&#25480;&#26435;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#23436;&#27605;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#27169;&#22411;&#34987;&#35270;&#20026;&#22810;&#31181;&#21830;&#19994;&#27169;&#24335;&#20013;&#30340;&#26377;&#20215;&#20540;&#30340;&#30693;&#35782;&#20135;&#26435;&#65288;IP&#65289;&#12290;&#38450;&#27490;IP&#30423;&#31363;&#21644;&#26410;&#32463;&#25480;&#26435;&#20351;&#29992;&#36825;&#20123;DNN&#27169;&#22411;&#24050;&#34987;&#19994;&#30028;&#25552;&#20986;&#20316;&#20026;&#19968;&#20010;&#37325;&#22823;&#20851;&#27880;&#28857;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#36890;&#29992;&#19988;&#36731;&#37327;&#30340;&#22522;&#20110;&#23494;&#38053;&#30340;&#27169;&#22411;&#38145;&#23450;&#26041;&#26696;&#8212;Deep-Lock&#65292;&#35299;&#20915;&#20102;&#38450;&#27490;DNN&#27169;&#22411;&#26410;&#32463;&#25480;&#26435;&#20351;&#29992;&#30340;&#38382;&#39064;&#65292;&#30830;&#20445;&#34987;&#38145;&#23450;&#30340;&#27169;&#22411;&#21482;&#26377;&#22312;&#24212;&#29992;&#27491;&#30830;&#30340;&#31192;&#23494;&#23494;&#38053;&#26102;&#25165;&#33021;&#27491;&#30830;&#36816;&#34892;&#12290;Deep-Lock&#26041;&#26696;&#21033;&#29992;&#20855;&#26377;&#33391;&#22909;&#23433;&#20840;&#24615;&#36136;&#30340;S-&#30418;&#23545;&#32463;&#36807;&#35757;&#32451;&#30340;DNN&#27169;&#22411;&#30340;&#27599;&#20010;&#21442;&#25968;&#36827;&#34892;&#21152;&#23494;&#65292;&#20351;&#29992;&#20027;&#23494;&#38053;&#36890;&#36807;&#23494;&#38053;&#35843;&#24230;&#31639;&#27861;&#29983;&#25104;&#31192;&#23494;&#23494;&#38053;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#21152;&#23494;&#26435;&#37325;&#30340;&#23494;&#38598;&#32593;&#32476;&#34987;&#21457;&#29616;&#33021;&#22815;&#25269;&#25239;&#27169;&#22411;&#24494;&#35843;&#25915;&#20987;&#12290;&#26368;&#21518;&#65292;Deep-Lock&#19981;&#38656;&#35201;&#23545;DNN&#27169;&#22411;&#30340;&#32467;&#26500;&#21644;&#35757;&#32451;&#36827;&#34892;&#20219;&#20309;&#24178;&#39044;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#25152;&#26377;&#24050;&#23384;&#22312;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2008.05966v2 Announce Type: replace  Abstract: Trained Deep Neural Network (DNN) models are considered valuable Intellectual Properties (IP) in several business models. Prevention of IP theft and unauthorized usage of such DNN models has been raised as of significant concern by industry. In this paper, we address the problem of preventing unauthorized usage of DNN models by proposing a generic and lightweight key-based model-locking scheme, which ensures that a locked model functions correctly only upon applying the correct secret key. The proposed scheme, known as Deep-Lock, utilizes S-Boxes with good security properties to encrypt each parameter of a trained DNN model with secret keys generated from a master key via a key scheduling algorithm. The resulting dense network of encrypted weights is found robust against model fine-tuning attacks. Finally, Deep-Lock does not require any intervention in the structure and training of the DNN models, making it applicable for all existin
&lt;/p&gt;</description></item><item><title>&#35813;&#26041;&#27861;&#25552;&#20986;&#20102;SplitSGD&#65292;&#36890;&#36807;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#31283;&#24577;&#26816;&#27979;&#65292;&#22312;&#26816;&#27979;&#21040;&#31283;&#24577;&#38454;&#27573;&#26102;&#38477;&#20302;&#23398;&#20064;&#36895;&#29575;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#20984;&#38382;&#39064;&#21644;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#65292;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#38543;&#26426;&#20248;&#21270;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/1910.08597</link><description>&lt;p&gt;
&#36890;&#36807;&#20998;&#35010;&#35786;&#26029;&#23454;&#29616;&#38543;&#26426;&#20248;&#21270;&#30340;&#31283;&#20581;&#23398;&#20064;&#29575;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Robust Learning Rate Selection for Stochastic Optimization via Splitting Diagnostic
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/1910.08597
&lt;/p&gt;
&lt;p&gt;
&#35813;&#26041;&#27861;&#25552;&#20986;&#20102;SplitSGD&#65292;&#36890;&#36807;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#31283;&#24577;&#26816;&#27979;&#65292;&#22312;&#26816;&#27979;&#21040;&#31283;&#24577;&#38454;&#27573;&#26102;&#38477;&#20302;&#23398;&#20064;&#36895;&#29575;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#20984;&#38382;&#39064;&#21644;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#65292;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#38543;&#26426;&#20248;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;SplitSGD&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#38543;&#26426;&#20248;&#21270;&#30340;&#26032;&#30340;&#21160;&#24577;&#23398;&#20064;&#29575;&#35843;&#24230;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#26816;&#27979;&#21040;&#31283;&#24577;&#38454;&#27573;&#26102;&#38477;&#20302;&#23398;&#20064;&#36895;&#29575;&#65292;&#20197;&#26356;&#22909;&#22320;&#36866;&#24212;&#30446;&#26631;&#20989;&#25968;&#30340;&#23616;&#37096;&#20960;&#20309;&#32467;&#26500;&#65292;&#21363;&#24403;&#36845;&#20195;&#22788;&#20110;&#23616;&#37096;&#26368;&#23567;&#20540;&#38468;&#36817;&#26102;&#21487;&#33021;&#20250;&#20986;&#29616;&#21453;&#24377;&#12290;&#36890;&#36807;&#23558;&#21333;&#32447;&#31243;&#20998;&#25104;&#20004;&#20010;&#37096;&#20998;&#65292;&#24182;&#20351;&#29992;&#20004;&#20010;&#32447;&#31243;&#26799;&#24230;&#30340;&#20869;&#31215;&#20316;&#20026;&#31283;&#24577;&#24230;&#37327;&#26469;&#25191;&#34892;&#26816;&#27979;&#12290;&#22522;&#20110;&#36825;&#20010;&#31616;&#21333;&#20294;&#32463;&#36807;&#39564;&#35777;&#26377;&#25928;&#30340;&#31283;&#24577;&#26816;&#27979;&#65292;SplitSGD&#26131;&#20110;&#23454;&#29616;&#65292;&#24182;&#19988;&#22522;&#26412;&#19981;&#20250;&#27604;&#26631;&#20934;SGD&#20135;&#29983;&#39069;&#22806;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#26082;&#36866;&#29992;&#20110;&#20984;&#38382;&#39064;&#65292;&#20063;&#36866;&#29992;&#20110;&#35757;&#32451;&#65288;&#38750;&#20984;&#65289;&#31070;&#32463;&#32593;&#32476;&#65292;&#34920;&#29616;&#27604;&#20854;&#20182;&#38543;&#26426;&#20248;&#21270;&#26041;&#27861;&#26356;&#22909;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#35266;&#23519;&#21040;&#35813;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:1910.08597v5 Announce Type: replace-cross  Abstract: This paper proposes SplitSGD, a new dynamic learning rate schedule for stochastic optimization. This method decreases the learning rate for better adaptation to the local geometry of the objective function whenever a stationary phase is detected, that is, the iterates are likely to bounce at around a vicinity of a local minimum. The detection is performed by splitting the single thread into two and using the inner product of the gradients from the two threads as a measure of stationarity. Owing to this simple yet provably valid stationarity detection, SplitSGD is easy-to-implement and essentially does not incur additional computational cost than standard SGD. Through a series of extensive experiments, we show that this method is appropriate for both convex problems and training (non-convex) neural networks, with performance compared favorably to other stochastic optimization methods. Importantly, this method is observed to be v
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#32416;&#27491;&#20154;&#31867;&#38169;&#35823;&#26041;&#38754;&#36215;&#21040;&#20102;&#31215;&#26497;&#20316;&#29992;&#65292;&#20294;&#27492;&#20030;&#20063;&#28508;&#22312;&#23548;&#33268;&#24515;&#29702;&#25104;&#26412;&#65292;&#24182;&#24433;&#21709;&#20154;&#30340;&#20915;&#31574;&#12290;&#36890;&#36807;&#30740;&#31350;&#32593;&#29699;&#27604;&#36187;&#20013;&#30340;Hawk-Eye&#23457;&#26597;&#31995;&#32479;&#65292;&#25105;&#20204;&#21457;&#29616;&#24341;&#20837;AI&#30417;&#30563;&#21518;&#65292;&#35009;&#21028;&#21592;&#30340;&#38169;&#35823;&#29575;&#19979;&#38477;&#65292;&#24515;&#29702;&#25104;&#26412;&#23548;&#33268;&#20182;&#20204;&#26356;&#20542;&#21521;&#20110;&#23558;&#29699;&#21028;&#20026;&#36827;&#30028;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#31867;&#22411;&#38169;&#21028;&#30340;&#36716;&#21464;&#12290;</title><link>http://arxiv.org/abs/2401.16754</link><description>&lt;p&gt;
AI&#30417;&#30563;&#21644;&#20154;&#31867;&#38169;&#35823;&#65306;&#26469;&#33258;&#20013;&#24515;&#27861;&#24237;&#30340;&#35777;&#25454;
&lt;/p&gt;
&lt;p&gt;
AI Oversight and Human Mistakes: Evidence from Centre Court. (arXiv:2401.16754v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16754
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#32416;&#27491;&#20154;&#31867;&#38169;&#35823;&#26041;&#38754;&#36215;&#21040;&#20102;&#31215;&#26497;&#20316;&#29992;&#65292;&#20294;&#27492;&#20030;&#20063;&#28508;&#22312;&#23548;&#33268;&#24515;&#29702;&#25104;&#26412;&#65292;&#24182;&#24433;&#21709;&#20154;&#30340;&#20915;&#31574;&#12290;&#36890;&#36807;&#30740;&#31350;&#32593;&#29699;&#27604;&#36187;&#20013;&#30340;Hawk-Eye&#23457;&#26597;&#31995;&#32479;&#65292;&#25105;&#20204;&#21457;&#29616;&#24341;&#20837;AI&#30417;&#30563;&#21518;&#65292;&#35009;&#21028;&#21592;&#30340;&#38169;&#35823;&#29575;&#19979;&#38477;&#65292;&#24515;&#29702;&#25104;&#26412;&#23548;&#33268;&#20182;&#20204;&#26356;&#20542;&#21521;&#20110;&#23558;&#29699;&#21028;&#20026;&#36827;&#30028;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#31867;&#22411;&#38169;&#21028;&#30340;&#36716;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#19981;&#26029;&#25552;&#21319;&#30340;&#39537;&#21160;&#19979;&#65292;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#31995;&#32479;&#24050;&#32463;&#24320;&#22987;&#22312;&#35768;&#22810;&#22330;&#21512;&#29992;&#20110;&#32416;&#27491;&#20154;&#31867;&#38169;&#35823;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#39318;&#20010;&#23454;&#22320;&#35777;&#25454;&#65292;&#35777;&#26126;&#36825;&#31181;AI&#30417;&#30563;&#20250;&#20135;&#29983;&#24515;&#29702;&#25104;&#26412;&#65292;&#24433;&#21709;&#20154;&#30340;&#20915;&#31574;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;AI&#30417;&#30563;&#21457;&#29983;&#30340;&#26368;&#39640;&#21487;&#35265;&#24615;&#22330;&#26223;&#20043;&#19968;&#65306;&#39030;&#32423;&#32593;&#29699;&#27604;&#36187;&#20013;&#35009;&#21028;&#30340;Hawk-Eye&#23457;&#26597;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24341;&#20837;Hawk-Eye&#23457;&#26597;&#21518;&#65292;&#35009;&#21028;&#30340;&#25972;&#20307;&#38169;&#35823;&#29575;&#38477;&#20302;&#65292;&#31526;&#21512;&#24515;&#29702;&#25104;&#26412;&#34987;AI&#21542;&#23450;&#30340;&#21512;&#29702;&#24573;&#35270;&#29616;&#35937;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#35009;&#21028;&#22686;&#21152;&#20102;&#23545;&#29699;&#20837;&#20869;&#30340;&#21028;&#23450;&#29575;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#20174;II&#31867;&#38169;&#35823;&#65288;&#23558;&#29699;&#21028;&#20026;&#20986;&#30028;&#65292;&#23454;&#38469;&#19978;&#26159;&#36827;&#30028;&#65289;&#21040;I&#31867;&#38169;&#35823;&#65288;&#23558;&#29699;&#21028;&#20026;&#36827;&#30028;&#65292;&#23454;&#38469;&#19978;&#26159;&#20986;&#30028;&#65289;&#30340;&#36716;&#21464;&#12290;&#36890;&#36807;&#23545;&#29702;&#24615;&#19981;&#27880;&#24847;&#30340;&#35009;&#21028;&#27169;&#22411;&#36827;&#34892;&#24515;&#29702;&#25104;&#26412;&#30340;&#32467;&#26500;&#20272;&#35745;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#30001;&#20110;AI&#21542;&#23450;&#30340;&#24515;&#29702;&#25104;&#26412;&#65292;&#35009;&#21028;&#21592;&#38477;&#20302;&#20102;&#38169;&#35823;&#21028;&#23450;&#30340;&#39118;&#38505;&#24182;&#25552;&#39640;&#20102;&#29699;&#20837;&#20869;&#30340;&#21028;&#23450;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Powered by the increasing predictive capabilities of machine learning algorithms, artificial intelligence (AI) systems have begun to be used to overrule human mistakes in many settings. We provide the first field evidence this AI oversight carries psychological costs that can impact human decision-making. We investigate one of the highest visibility settings in which AI oversight has occurred: the Hawk-Eye review of umpires in top tennis tournaments. We find that umpires lowered their overall mistake rate after the introduction of Hawk-Eye review, in line with rational inattention given psychological costs of being overruled by AI. We also find that umpires increased the rate at which they called balls in, which produced a shift from making Type II errors (calling a ball out when in) to Type I errors (calling a ball in when out). We structurally estimate the psychological costs of being overruled by AI using a model of rational inattentive umpires, and our results suggest that because 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#21487;&#25511;&#38376;&#36866;&#37197;&#22120;&#65288;ConGater&#65289;&#65292;&#19968;&#31181;&#20855;&#26377;&#21487;&#35843;&#33410;&#25935;&#24863;&#24615;&#21442;&#25968;&#30340;&#26032;&#39062;&#27169;&#22359;&#21270;&#38376;&#26426;&#21046;&#65292;&#21487;&#22312;&#25512;&#29702;&#26102;&#36880;&#28176;&#36807;&#28193;&#20174;&#27169;&#22411;&#30340;&#20559;&#21521;&#29366;&#24577;&#21040;&#23436;&#20840;&#21435;&#20559;&#30340;&#29256;&#26412;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#20998;&#31867;&#21644;&#26816;&#32034;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.16457</link><description>&lt;p&gt;
&#26377;&#25928;&#30340;&#21487;&#25511;&#20559;&#24046;&#32531;&#35299;&#26041;&#27861;&#65292;&#21033;&#29992;&#38376;&#36866;&#37197;&#22120;&#36827;&#34892;&#20998;&#31867;&#21644;&#26816;&#32034;&#12290;(arXiv:2401.16457v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
Effective Controllable Bias Mitigation for Classification and Retrieval using Gate Adapters. (arXiv:2401.16457v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16457
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#21487;&#25511;&#38376;&#36866;&#37197;&#22120;&#65288;ConGater&#65289;&#65292;&#19968;&#31181;&#20855;&#26377;&#21487;&#35843;&#33410;&#25935;&#24863;&#24615;&#21442;&#25968;&#30340;&#26032;&#39062;&#27169;&#22359;&#21270;&#38376;&#26426;&#21046;&#65292;&#21487;&#22312;&#25512;&#29702;&#26102;&#36880;&#28176;&#36807;&#28193;&#20174;&#27169;&#22411;&#30340;&#20559;&#21521;&#29366;&#24577;&#21040;&#23436;&#20840;&#21435;&#20559;&#30340;&#29256;&#26412;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#20998;&#31867;&#21644;&#26816;&#32034;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#20559;&#24046;&#32531;&#35299;&#24050;&#32463;&#25104;&#20026;&#35768;&#22810;&#30740;&#31350;&#30340;&#20027;&#39064;&#65292;&#26368;&#36817;&#20851;&#27880;&#30340;&#28966;&#28857;&#26159;&#23398;&#20064;&#29420;&#31435;&#30340;&#27169;&#22359;&#65292;&#20363;&#22914;&#36866;&#37197;&#22120;&#36827;&#34892;&#25353;&#38656;&#21435;&#20559;&#12290;&#38500;&#20102;&#20248;&#21270;&#27169;&#22359;&#21270;&#21435;&#20559;&#27169;&#22411;&#22806;&#65292;&#22312;&#23454;&#36341;&#20013;&#36890;&#24120;&#38656;&#35201;&#22312;&#25512;&#29702;&#26102;&#25511;&#21046;&#20559;&#24046;&#20943;&#23569;&#30340;&#31243;&#24230;&#65292;&#20363;&#22914;&#65292;&#20026;&#20102;&#22312;&#25628;&#32034;&#32467;&#26524;&#20013;&#35843;&#25972;&#26399;&#26395;&#30340;&#24615;&#33021;-&#20844;&#24179;&#24615;&#26435;&#34913;&#25110;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#25511;&#21046;&#21435;&#20559;&#30340;&#24378;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21487;&#25511;&#38376;&#36866;&#37197;&#22120;&#65288;ConGater&#65289;&#65292;&#19968;&#31181;&#20855;&#26377;&#21487;&#35843;&#33410;&#25935;&#24863;&#24615;&#21442;&#25968;&#30340;&#26032;&#39062;&#27169;&#22359;&#21270;&#38376;&#26426;&#21046;&#65292;&#20801;&#35768;&#22312;&#25512;&#29702;&#26102;&#20174;&#27169;&#22411;&#30340;&#20559;&#21521;&#29366;&#24577;&#36880;&#28176;&#36807;&#28193;&#21040;&#23436;&#20840;&#21435;&#20559;&#30340;&#29256;&#26412;&#12290;&#36890;&#36807;&#22312;&#19977;&#20010;&#20998;&#31867;&#20219;&#21153;&#19978;&#23545;&#19977;&#20010;&#19981;&#21516;&#27169;&#22411;&#36827;&#34892;&#23545;&#25239;&#24615;&#21435;&#20559;&#23454;&#39564;&#65292;&#24182;&#36890;&#36807;&#20844;&#24179;&#24615;&#21015;&#34920;&#27491;&#21017;&#21270;&#26469;&#20943;&#23569;&#25628;&#32034;&#32467;&#26524;&#30340;&#20559;&#24046;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;ConGater&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bias mitigation of Language Models has been the topic of many studies with a recent focus on learning separate modules like adapters for on-demand debiasing. Besides optimizing for a modularized debiased model, it is often critical in practice to control the degree of bias reduction at inference time, e.g., in order to tune for a desired performance-fairness trade-off in search results or to control the strength of debiasing in classification tasks. In this paper, we introduce Controllable Gate Adapter (ConGater), a novel modular gating mechanism with adjustable sensitivity parameters, which allows for a gradual transition from the biased state of the model to the fully debiased version at inference time. We demonstrate ConGater performance by (1) conducting adversarial debiasing experiments with three different models on three classification tasks with four protected attributes, and (2) reducing the bias of search results through fairness list-wise regularization to enable adjusting a
&lt;/p&gt;</description></item><item><title>LLM&#25351;&#20196;&#24494;&#35843;&#20013;&#65292;&#23545;&#20110;&#30701;&#25552;&#31034;&#23436;&#25104;&#25968;&#25454;&#38598;&#65292;&#25552;&#31034;&#35789;&#26631;&#35760;&#20998;&#31867;&#25439;&#22833;&#21152;&#26435;&#65288;PLW&#65289;&#19982;&#24615;&#33021;&#21576;&#36127;&#20108;&#27425;&#20851;&#31995;&#65292;&#32780;&#38271;&#25552;&#31034;&#23436;&#25104;&#25968;&#25454;&#38598;&#21017;&#19981;&#21463;PLW&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.13586</link><description>&lt;p&gt;
LLM&#25351;&#20196;&#24494;&#35843;&#20013;&#30340;&#25552;&#31034;&#26435;&#37325;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
Prompt Weight Experiments for LLM Instruction Fine-Tuning. (arXiv:2401.13586v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13586
&lt;/p&gt;
&lt;p&gt;
LLM&#25351;&#20196;&#24494;&#35843;&#20013;&#65292;&#23545;&#20110;&#30701;&#25552;&#31034;&#23436;&#25104;&#25968;&#25454;&#38598;&#65292;&#25552;&#31034;&#35789;&#26631;&#35760;&#20998;&#31867;&#25439;&#22833;&#21152;&#26435;&#65288;PLW&#65289;&#19982;&#24615;&#33021;&#21576;&#36127;&#20108;&#27425;&#20851;&#31995;&#65292;&#32780;&#38271;&#25552;&#31034;&#23436;&#25104;&#25968;&#25454;&#38598;&#21017;&#19981;&#21463;PLW&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#23567;&#22411;&#30740;&#31350;&#65292;&#20998;&#26512;&#20102;&#25552;&#31034;&#35789;&#26631;&#35760;&#20998;&#31867;&#25439;&#22833;&#21152;&#26435;&#65288;PLW&#65289;&#22914;&#20309;&#24433;&#21709;&#22312;&#25351;&#20196;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#30340;7B&#22823;&#23567;&#30340;LLaMA&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#20351;&#29992;&#22810;&#20010;&#25351;&#20196;&#25968;&#25454;&#38598;&#37325;&#29616;&#20102;&#26031;&#22374;&#31119;&#22823;&#23398;&#30340;Alpaca&#23454;&#39564;&#65292;&#20854;&#20013;&#21253;&#25324;LLaMA 1&#21644;LLaMA 2&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#25105;&#20204;&#30340;&#30701;&#25552;&#31034;&#23436;&#25104;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;&#30340;&#27169;&#22411;&#19982;PLW&#20043;&#38388;&#23384;&#22312;&#36127;&#20108;&#27425;&#20851;&#31995;&#65292;&#32780;&#22312;&#38271;&#25552;&#31034;&#23436;&#25104;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;&#30340;&#27169;&#22411;&#19981;&#21463;PLW&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a small study analyzing how prompt token classification loss weighting (PLW) affects the performance of 7B-size LLaMA models fine-tuned on instruction tasks. We recreated Stanford's Alpaca experiment with both LLaMA 1 and LLaMA 2 using multiple instruction datasets. We found that models fine-tuned on our short-completion dataset have a negative quadratic relationship with PLW while models fine-tuned on long-completion datasets were unaffected by PLW.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27424;&#38459;&#23612; Langevin Monte Carlo &#21152;&#36895;&#30340;&#36817;&#20284; Thompson &#37319;&#26679;&#31574;&#30053;&#65292;&#36890;&#36807;&#29305;&#23450;&#21183;&#20989;&#25968;&#30340;&#35774;&#35745;&#25913;&#21892;&#20102;&#39640;&#32500;&#38382;&#39064;&#20013;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#24182;&#22312;&#39640;&#32500;&#36172;&#21338;&#26426;&#38382;&#39064;&#20013;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2401.11665</link><description>&lt;p&gt;
&#20351;&#29992;&#27424;&#38459;&#23612; Langevin Monte Carlo &#21152;&#36895;&#36817;&#20284; Thompson &#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Accelerating Approximate Thompson Sampling with Underdamped Langevin Monte Carlo. (arXiv:2401.11665v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11665
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27424;&#38459;&#23612; Langevin Monte Carlo &#21152;&#36895;&#30340;&#36817;&#20284; Thompson &#37319;&#26679;&#31574;&#30053;&#65292;&#36890;&#36807;&#29305;&#23450;&#21183;&#20989;&#25968;&#30340;&#35774;&#35745;&#25913;&#21892;&#20102;&#39640;&#32500;&#38382;&#39064;&#20013;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#24182;&#22312;&#39640;&#32500;&#36172;&#21338;&#26426;&#38382;&#39064;&#20013;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#27424;&#38459;&#23612; Langevin Monte Carlo &#30340;&#36817;&#20284; Thompson &#37319;&#26679;&#26041;&#27861;&#25193;&#23637;&#20102;&#20854;&#36866;&#29992;&#33539;&#22260;&#65292;&#20174;&#39640;&#26031;&#21518;&#39564;&#37319;&#26679;&#25193;&#23637;&#21040;&#26356;&#19968;&#33324;&#30340;&#24179;&#28369;&#21518;&#39564;&#12290;&#28982;&#32780;&#65292;&#22312;&#39640;&#32500;&#38382;&#39064;&#20013;&#35201;&#27714;&#39640;&#20934;&#30830;&#24615;&#26102;&#65292;&#20173;&#28982;&#38754;&#20020;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36817;&#20284; Thompson &#37319;&#26679;&#31574;&#30053;&#65292;&#21033;&#29992;&#27424;&#38459;&#23612; Langevin Monte Carlo&#65292;&#21518;&#32773;&#26159;&#27169;&#25311;&#39640;&#32500;&#21518;&#39564;&#30340;&#36890;&#29992;&#24037;&#20855;&#12290;&#22522;&#20110;&#26631;&#20934;&#30340;&#24179;&#28369;&#24615;&#21644;&#23545;&#25968;&#20985;&#24615;&#26465;&#20214;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#29305;&#23450;&#21183;&#20989;&#25968;&#30340;&#21152;&#36895;&#21518;&#39564;&#38598;&#20013;&#21644;&#37319;&#26679;&#12290;&#35813;&#35774;&#35745;&#25913;&#36827;&#20102;&#23454;&#29616;&#23545;&#25968;&#36951;&#25022;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#20174;$\mathcal{\tilde O}(d)$&#25913;&#36827;&#21040;$\mathcal{\tilde O}(\sqrt{d})$&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#21512;&#25104;&#23454;&#39564;&#22312;&#39640;&#32500;&#36172;&#21338;&#26426;&#38382;&#39064;&#20013;&#32463;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#31639;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Approximate Thompson sampling with Langevin Monte Carlo broadens its reach from Gaussian posterior sampling to encompass more general smooth posteriors. However, it still encounters scalability issues in high-dimensional problems when demanding high accuracy. To address this, we propose an approximate Thompson sampling strategy, utilizing underdamped Langevin Monte Carlo, where the latter is the go-to workhorse for simulations of high-dimensional posteriors. Based on the standard smoothness and log-concavity conditions, we study the accelerated posterior concentration and sampling using a specific potential function. This design improves the sample complexity for realizing logarithmic regrets from $\mathcal{\tilde O}(d)$ to $\mathcal{\tilde O}(\sqrt{d})$. The scalability and robustness of our algorithm are also empirically validated through synthetic experiments in high-dimensional bandit problems.
&lt;/p&gt;</description></item><item><title>SymTC&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#33136;&#26894;MR&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;Transformer&#21644;CNN&#30456;&#32467;&#21512;&#65292;&#24182;&#21033;&#29992;&#20301;&#32622;&#23884;&#20837;&#21644;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#30340;&#23454;&#20363;&#20998;&#21106;&#12290;</title><link>http://arxiv.org/abs/2401.09627</link><description>&lt;p&gt;
SymTC:&#19968;&#31181;&#29992;&#20110;&#33136;&#26894;MRI&#23454;&#20363;&#20998;&#21106;&#30340;&#20849;&#29983;Transformer-CNN&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
SymTC: A Symbiotic Transformer-CNN Net for Instance Segmentation of Lumbar Spine MRI. (arXiv:2401.09627v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09627
&lt;/p&gt;
&lt;p&gt;
SymTC&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#33136;&#26894;MR&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;Transformer&#21644;CNN&#30456;&#32467;&#21512;&#65292;&#24182;&#21033;&#29992;&#20301;&#32622;&#23884;&#20837;&#21644;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#30340;&#23454;&#20363;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26894;&#38388;&#30424;&#30142;&#30149;&#26159;&#19968;&#31181;&#24120;&#35265;&#30149;&#30151;&#65292;&#32463;&#24120;&#23548;&#33268;&#38388;&#27463;&#24615;&#25110;&#25345;&#32493;&#24615;&#30340;&#33136;&#32972;&#30140;&#30171;&#65292;&#23545;&#35813;&#30142;&#30149;&#30340;&#35786;&#26029;&#21644;&#35780;&#20272;&#20381;&#36182;&#20110;&#33136;&#26894;MR&#22270;&#20687;&#20013;&#26894;&#39592;&#21644;&#26894;&#38388;&#30424;&#20960;&#20309;&#24418;&#29366;&#30340;&#20934;&#30830;&#27979;&#37327;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#27169;&#22411;&#21487;&#20197;&#24110;&#21161;&#20020;&#24202;&#21307;&#29983;&#20197;&#26356;&#39640;&#25928;&#30340;&#26041;&#24335;&#33258;&#21160;&#21270;&#22320;&#23545;&#33136;&#26894;&#30340;&#20010;&#20307;&#23454;&#20363;&#65288;&#26894;&#39592;&#21644;&#26894;&#38388;&#30424;&#65289;&#36827;&#34892;&#22270;&#20687;&#20998;&#21106;&#65292;&#36825;&#34987;&#31216;&#20026;&#23454;&#20363;&#22270;&#20687;&#20998;&#21106;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SymTC&#65292;&#19968;&#31181;&#21019;&#26032;&#30340;&#33136;&#26894;MR&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#65292;&#23427;&#32467;&#21512;&#20102;Transformer&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30340;&#20248;&#21183;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#24182;&#34892;&#30340;&#21452;&#36335;&#24452;&#26550;&#26500;&#26469;&#34701;&#21512;CNN&#23618;&#21644;Transformer&#23618;&#65292;&#24182;&#22312;Transformer&#30340;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#20013;&#38598;&#25104;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20301;&#32622;&#23884;&#20837;&#65292;&#22686;&#24378;&#20102;&#20301;&#32622;&#20449;&#24687;&#30340;&#21033;&#29992;&#20197;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#20998;&#21106;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#23450;&#20301;&#31995;&#32479;&#36827;&#34892;&#27169;&#22411;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intervertebral disc disease, a prevalent ailment, frequently leads to intermittent or persistent low back pain, and diagnosing and assessing of this disease rely on accurate measurement of vertebral bone and intervertebral disc geometries from lumbar MR images. Deep neural network (DNN) models may assist clinicians with more efficient image segmentation of individual instances (disks and vertebrae) of the lumbar spine in an automated way, which is termed as instance image segmentation. In this work, we proposed SymTC, an innovative lumbar spine MR image segmentation model that combines the strengths of Transformer and Convolutional Neural Network (CNN). Specifically, we designed a parallel dual-path architecture to merge CNN layers and Transformer layers, and we integrated a novel position embedding into the self-attention module of Transformer, enhancing the utilization of positional information for more accurate segmentation. To further improves model performance, we introduced a new
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#21160;&#24577;&#32593;&#32476;&#25299;&#25169;&#19979;&#65292;&#19981;&#21487;&#35775;&#38382;&#33410;&#28857;&#23545;&#27969;&#35328;&#23398;&#20064;&#30340;&#25910;&#25947;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2401.09498</link><description>&lt;p&gt;
&#25216;&#26415;&#25253;&#21578;&#65306;&#20851;&#20110;&#33410;&#28857;&#19981;&#21487;&#35775;&#38382;&#24773;&#20917;&#19979;&#27969;&#35328;&#23398;&#20064;&#25910;&#25947;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Technical Report: On the Convergence of Gossip Learning in the Presence of Node Inaccessibility. (arXiv:2401.09498v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09498
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#21160;&#24577;&#32593;&#32476;&#25299;&#25169;&#19979;&#65292;&#19981;&#21487;&#35775;&#38382;&#33410;&#28857;&#23545;&#27969;&#35328;&#23398;&#20064;&#30340;&#25910;&#25947;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Gossip learning&#65288;GL&#65289;&#20316;&#20026;&#20998;&#25955;&#24335;&#23398;&#20064;&#30340;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65292;&#26356;&#36866;&#29992;&#20110;&#36164;&#28304;&#21463;&#38480;&#30340;&#26080;&#32447;&#32593;&#32476;&#65292;&#22914;&#30001;&#26080;&#20154;&#26426;&#65288;UAV&#65289;&#32452;&#25104;&#30340;FANETs&#12290;GL&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;UAV&#32593;&#32476;&#30340;&#25928;&#29575;&#24182;&#24310;&#38271;&#30005;&#27744;&#23551;&#21629;&#12290;&#23613;&#31649;&#20855;&#26377;&#36825;&#20123;&#20248;&#21183;&#65292;&#20294;GL&#30340;&#24615;&#33021;&#21463;&#25968;&#25454;&#20998;&#24067;&#12289;&#36890;&#20449;&#36895;&#24230;&#21644;&#32593;&#32476;&#36830;&#25509;&#24615;&#30340;&#24433;&#21709;&#36739;&#22823;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22240;&#32032;&#22914;&#20309;&#24433;&#21709;GL&#30340;&#25910;&#25947;&#24615;&#20173;&#19981;&#28165;&#26970;&#12290;&#29616;&#26377;&#30740;&#31350;&#22522;&#20110;&#34394;&#25311;&#25968;&#37327;&#26469;&#30740;&#31350;GL&#30340;&#25910;&#25947;&#24615;&#65292;&#20197;&#26041;&#20415;&#24615;&#32780;&#24573;&#30053;&#20102;&#24403;&#19968;&#20123;&#33410;&#28857;&#19981;&#21487;&#35775;&#38382;&#26102;&#32593;&#32476;&#30340;&#30495;&#23454;&#29366;&#24577;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#21160;&#24577;&#32593;&#32476;&#25299;&#25169;&#19979;&#19981;&#21487;&#35775;&#38382;&#33410;&#28857;&#23545;GL&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#24314;&#27169;&#21644;&#30740;&#31350;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#26435;&#37325;&#21457;&#25955;&#20998;&#35299;&#20026;&#33410;&#28857;&#26159;&#21542;&#21487;&#35775;&#38382;&#30340;&#24773;&#20917;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#33410;&#28857;&#21487;&#35775;&#38382;&#24615;&#30340;&#21160;&#24577;&#19979;GL&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#25552;&#20379;&#20102;
&lt;/p&gt;
&lt;p&gt;
Gossip learning (GL), as a decentralized alternative to federated learning (FL), is more suitable for resource-constrained wireless networks, such as FANETs that are formed by unmanned aerial vehicles (UAVs). GL can significantly enhance the efficiency and extend the battery life of UAV networks. Despite the advantages, the performance of GL is strongly affected by data distribution, communication speed, and network connectivity. However, how these factors influence the GL convergence is still unclear. Existing work studied the convergence of GL based on a virtual quantity for the sake of convenience, which fail to reflect the real state of the network when some nodes are inaccessible. In this paper, we formulate and investigate the impact of inaccessible nodes to GL under a dynamic network topology. We first decompose the weight divergence by whether the node is accessible or not. Then, we investigate the GL convergence under the dynamic of node accessibility and theoretically provide
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#37096;&#20998;&#35266;&#27979;&#20013;&#36827;&#34892;&#36830;&#32493;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#29289;&#29702;&#27169;&#25311;&#65292;&#24182;&#35299;&#20915;&#20102;&#22522;&#20110;&#22266;&#23450;&#25903;&#25345;&#32593;&#26684;&#30340;&#20256;&#32479;&#26041;&#27861;&#30340;&#32570;&#28857;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#22312;&#31232;&#30095;&#35266;&#27979;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#21033;&#29992;&#20004;&#20010;&#30456;&#20114;&#20851;&#32852;&#30340;&#21160;&#21147;&#31995;&#32479;&#22312;&#31232;&#30095;&#20301;&#32622;&#21644;&#36830;&#32493;&#22495;&#19978;&#36827;&#34892;&#39044;&#27979;&#21644;&#25554;&#20540;&#27714;&#35299;&#12290;</title><link>http://arxiv.org/abs/2401.09198</link><description>&lt;p&gt;
&#20174;&#37096;&#20998;&#35266;&#27979;&#20013;&#36827;&#34892;&#31354;&#38388;&#21644;&#26102;&#38388;&#36830;&#32493;&#30340;&#29289;&#29702;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Space and Time Continuous Physics Simulation From Partial Observations. (arXiv:2401.09198v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09198
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#37096;&#20998;&#35266;&#27979;&#20013;&#36827;&#34892;&#36830;&#32493;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#29289;&#29702;&#27169;&#25311;&#65292;&#24182;&#35299;&#20915;&#20102;&#22522;&#20110;&#22266;&#23450;&#25903;&#25345;&#32593;&#26684;&#30340;&#20256;&#32479;&#26041;&#27861;&#30340;&#32570;&#28857;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#22312;&#31232;&#30095;&#35266;&#27979;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#21033;&#29992;&#20004;&#20010;&#30456;&#20114;&#20851;&#32852;&#30340;&#21160;&#21147;&#31995;&#32479;&#22312;&#31232;&#30095;&#20301;&#32622;&#21644;&#36830;&#32493;&#22495;&#19978;&#36827;&#34892;&#39044;&#27979;&#21644;&#25554;&#20540;&#27714;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#29289;&#29702;&#27169;&#25311;&#25216;&#26415;&#20381;&#36182;&#20110;&#25968;&#20540;&#26041;&#26696;&#21644;&#32593;&#26684;&#32454;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;&#31934;&#24230;&#21644;&#22797;&#26434;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#20294;&#36825;&#20123;&#25163;&#24037;&#35299;&#20915;&#26041;&#26696;&#32321;&#29712;&#19988;&#38656;&#35201;&#39640;&#35745;&#31639;&#33021;&#21147;&#12290;&#22522;&#20110;&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#36890;&#36807;&#26356;&#30452;&#25509;&#21644;&#39640;&#25928;&#22320;&#38598;&#25104;&#38271;&#36317;&#31163;&#20381;&#36182;&#26469;&#23454;&#29616;&#39640;&#36866;&#24212;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20027;&#35201;&#20851;&#27880;&#27969;&#20307;&#21160;&#21147;&#23398;&#65292;&#24182;&#35299;&#20915;&#20102;&#22823;&#37096;&#20998;&#25991;&#29486;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#21363;&#35745;&#31639;&#21644;&#39044;&#27979;&#24418;&#24335;&#20026;&#24120;&#35268;&#25110;&#38750;&#35268;&#21017;&#32593;&#26684;&#30340;&#22266;&#23450;&#25903;&#25345;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35774;&#32622;&#65292;&#21487;&#20197;&#22312;&#36830;&#32493;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#22495;&#20013;&#36827;&#34892;&#39044;&#27979;&#65292;&#21516;&#26102;&#22312;&#31232;&#30095;&#35266;&#27979;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#20219;&#21153;&#24418;&#24335;&#21270;&#20026;&#21452;&#35266;&#27979;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#20854;&#20013;&#22312;&#31232;&#30095;&#20301;&#32622;&#21644;&#36830;&#32493;&#22495;&#19978;&#23450;&#20041;&#20102;&#20004;&#20010;&#30456;&#20114;&#20851;&#32852;&#30340;&#21160;&#21147;&#31995;&#32479;&#65292;&#21487;&#20197;&#20174;&#21021;&#22987;&#26465;&#20214;&#36827;&#34892;&#39044;&#27979;&#21644;&#25554;&#20540;&#27714;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern techniques for physical simulations rely on numerical schemes and mesh-refinement methods to address trade-offs between precision and complexity, but these handcrafted solutions are tedious and require high computational power. Data-driven methods based on large-scale machine learning promise high adaptivity by integrating long-range dependencies more directly and efficiently. In this work, we focus on fluid dynamics and address the shortcomings of a large part of the literature, which are based on fixed support for computations and predictions in the form of regular or irregular grids. We propose a novel setup to perform predictions in a continuous spatial and temporal domain while being trained on sparse observations. We formulate the task as a double observation problem and propose a solution with two interlinked dynamical systems defined on, respectively, the sparse positions and the continuous domain, which allows to forecast and interpolate a solution from the initial cond
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#31526;&#21512;&#39044;&#27979;&#38598;&#22312;AI&#36741;&#21161;&#22270;&#20687;&#26631;&#27880;&#20013;&#30340;&#25928;&#29992;&#65292;&#21457;&#29616;&#23545;&#20110;&#31616;&#21333;&#22270;&#20687;&#65292;&#39044;&#27979;&#38598;&#19982;Top-1&#21644;Top-k&#26174;&#31034;&#30340;&#20934;&#30830;&#24615;&#30456;&#24403;&#65292;&#20294;&#22312;&#26631;&#35760;&#20998;&#24067;&#22806;&#22270;&#20687;&#26102;&#29305;&#21035;&#26377;&#25928;&#65292;&#23588;&#20854;&#26159;&#38598;&#21512;&#22823;&#23567;&#36739;&#23567;&#26102;&#12290;</title><link>http://arxiv.org/abs/2401.08876</link><description>&lt;p&gt;
&#35780;&#20272;&#29992;&#20110;AI&#36741;&#21161;&#22270;&#20687;&#26631;&#27880;&#30340;&#31526;&#21512;&#39044;&#27979;&#38598;&#30340;&#25928;&#29992;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Utility of Conformal Prediction Sets for AI-Advised Image Labeling. (arXiv:2401.08876v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08876
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#31526;&#21512;&#39044;&#27979;&#38598;&#22312;AI&#36741;&#21161;&#22270;&#20687;&#26631;&#27880;&#20013;&#30340;&#25928;&#29992;&#65292;&#21457;&#29616;&#23545;&#20110;&#31616;&#21333;&#22270;&#20687;&#65292;&#39044;&#27979;&#38598;&#19982;Top-1&#21644;Top-k&#26174;&#31034;&#30340;&#20934;&#30830;&#24615;&#30456;&#24403;&#65292;&#20294;&#22312;&#26631;&#35760;&#20998;&#24067;&#22806;&#22270;&#20687;&#26102;&#29305;&#21035;&#26377;&#25928;&#65292;&#23588;&#20854;&#26159;&#38598;&#21512;&#22823;&#23567;&#36739;&#23567;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#39640;&#39118;&#38505;&#39046;&#22495;&#20013;&#36234;&#26469;&#36234;&#24120;&#35265;&#65292;&#23427;&#20204;&#30340;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#20351;&#24471;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#29992;&#20110;&#34920;&#31034;AI&#36741;&#21161;&#20915;&#31574;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#30340;&#31526;&#21512;&#39044;&#27979;&#38598;&#30340;&#25928;&#26524;&#12290;&#36890;&#36807;&#19968;&#39033;&#22823;&#22411;&#39044;&#27880;&#20876;&#23454;&#39564;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#31526;&#21512;&#39044;&#27979;&#38598;&#21644;&#26174;&#31034;Top-1&#21644;Top-k&#39044;&#27979;&#22312;AI&#36741;&#21161;&#22270;&#20687;&#26631;&#27880;&#20013;&#30340;&#25928;&#29992;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23545;&#20110;&#31616;&#21333;&#30340;&#22270;&#20687;&#65292;&#39044;&#27979;&#38598;&#30340;&#20934;&#30830;&#24615;&#19982;Top-1&#21644;Top-k&#26174;&#31034;&#30456;&#24403;&#25110;&#31245;&#20302;&#65292;&#20294;&#22312;&#26631;&#35760;&#20998;&#24067;&#22806;&#65288;OOD&#65289;&#22270;&#20687;&#26102;&#65292;&#23588;&#20854;&#26159;&#24403;&#38598;&#21512;&#22823;&#23567;&#36739;&#23567;&#26102;&#65292;&#39044;&#27979;&#38598;&#22312;&#36741;&#21161;&#20154;&#31867;&#26631;&#27880;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#22312;&#23454;&#36341;&#20013;&#24378;&#35843;&#20102;&#31526;&#21512;&#39044;&#27979;&#38598;&#30340;&#23454;&#38469;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#20851;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
As deep neural networks are more commonly deployed in high-stakes domains, their lack of interpretability makes uncertainty quantification challenging. We investigate the effects of presenting conformal prediction sets$\unicode{x2013}$a method for generating valid confidence sets in distribution-free uncertainty quantification$\unicode{x2013}$to express uncertainty in AI-advised decision-making. Through a large pre-registered experiment, we compare the utility of conformal prediction sets to displays of Top-1 and Top-k predictions for AI-advised image labeling. We find that the utility of prediction sets for accuracy varies with the difficulty of the task: while they result in accuracy on par with or less than Top-1 and Top-k displays for easy images, prediction sets excel at assisting humans in labeling out-of-distribution (OOD) images especially when the set size is small. Our results empirically pinpoint the practical challenges of conformal prediction sets and provide implications 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#30693;&#35782;&#33976;&#39311;&#20013;&#21442;&#25968;&#36873;&#25321;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#23454;&#35777;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#19981;&#21516;&#36873;&#39033;&#23545;&#23398;&#29983;&#24615;&#33021;&#30340;&#25972;&#20307;&#24433;&#21709;&#65292;&#24182;&#25214;&#21040;&#20102;&#22312;&#21508;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#30340;&#21333;&#19968;&#37197;&#32622;&#12290;</title><link>http://arxiv.org/abs/2401.06356</link><description>&lt;p&gt;
&#23545;&#30693;&#35782;&#33976;&#39311;&#20013;&#21442;&#25968;&#36873;&#25321;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Empirical Investigation into the Effect of Parameter Choices in Knowledge Distillation. (arXiv:2401.06356v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06356
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#30693;&#35782;&#33976;&#39311;&#20013;&#21442;&#25968;&#36873;&#25321;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#23454;&#35777;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#19981;&#21516;&#36873;&#39033;&#23545;&#23398;&#29983;&#24615;&#33021;&#30340;&#25972;&#20307;&#24433;&#21709;&#65292;&#24182;&#25214;&#21040;&#20102;&#22312;&#21508;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#30340;&#21333;&#19968;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#39033;&#20851;&#20110;&#21442;&#25968;&#37197;&#32622;&#36873;&#25321;&#23545;&#30693;&#35782;&#33976;&#39311;&#24615;&#33021;&#24433;&#21709;&#30340;&#22823;&#35268;&#27169;&#23454;&#35777;&#30740;&#31350;&#12290;&#20854;&#20013;&#19968;&#20010;&#31034;&#20363;&#26159;&#25945;&#24072;&#21644;&#23398;&#29983;&#39044;&#27979;&#20043;&#38388;&#36317;&#31163;&#30340;&#24230;&#37327;&#65292;&#22312;&#27492;&#26041;&#38754;&#24120;&#35265;&#30340;&#36873;&#25321;&#21253;&#25324;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#21644;KL&#25955;&#24230;&#12290;&#23613;&#31649;&#24050;&#32463;&#36827;&#34892;&#20102;&#19968;&#20123;&#25955;&#20081;&#30340;&#21162;&#21147;&#26469;&#29702;&#35299;&#36825;&#20123;&#36873;&#39033;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#20294;&#26159;&#30693;&#35782;&#33976;&#39311;&#39046;&#22495;&#20173;&#28982;&#32570;&#20047;&#19968;&#20010;&#23545;&#23427;&#20204;&#23545;&#23398;&#29983;&#24615;&#33021;&#30340;&#25972;&#20307;&#24433;&#21709;&#36827;&#34892;&#31995;&#32479;&#30740;&#31350;&#30340;&#24037;&#20316;&#12290;&#25105;&#20204;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#37319;&#29992;&#23454;&#35777;&#26041;&#27861;&#26469;&#25506;&#32034;&#36825;&#20010;&#38382;&#39064;&#65292;&#35797;&#22270;&#25214;&#20986;&#36825;&#20123;&#36873;&#25321;&#22312;&#21253;&#25324;4&#20010;NLP&#20219;&#21153;&#21644;3&#31181;&#23398;&#29983;&#35268;&#27169;&#30340;13&#20010;&#25968;&#25454;&#38598;&#19978;&#23545;&#23398;&#29983;&#24615;&#33021;&#30340;&#24433;&#21709;&#31243;&#24230;&#12290;&#25105;&#20204;&#34913;&#37327;&#20102;&#20570;&#20986;&#27425;&#20248;&#36873;&#25321;&#30340;&#20195;&#20215;&#65292;&#24182;&#30830;&#23450;&#20102;&#19968;&#20010;&#22312;&#21508;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#30340;&#21333;&#19968;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a large-scale empirical study of how choices of configuration parameters affect performance in knowledge distillation (KD). An example of such a KD parameter is the measure of distance between the predictions of the teacher and the student, common choices for which include the mean squared error (MSE) and the KL-divergence. Although scattered efforts have been made to understand the differences between such options, the KD literature still lacks a systematic study on their general effect on student performance. We take an empirical approach to this question in this paper, seeking to find out the extent to which such choices influence student performance across 13 datasets from 4 NLP tasks and 3 student sizes. We quantify the cost of making sub-optimal choices and identify a single configuration that performs well across the board.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#36845;&#20195;&#27491;&#21017;&#21270;&#31639;&#27861;IRKSN&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;$k$&#25903;&#25745;&#33539;&#25968;&#27491;&#21017;&#21270;&#23454;&#29616;&#31232;&#30095;&#24674;&#22797;&#65292;&#24182;&#25552;&#20379;&#20102;&#26465;&#20214;&#12290;&#36825;&#26159;&#23545;&#22522;&#20110;$\ell_1$&#33539;&#25968;&#30340;&#36845;&#20195;&#26041;&#27861;&#30340;&#19968;&#31181;&#37325;&#35201;&#34917;&#20805;&#12290;</title><link>http://arxiv.org/abs/2401.05394</link><description>&lt;p&gt;
&#36845;&#20195;&#27491;&#21017;&#21270;&#19982;k&#25903;&#25745;&#33539;&#25968;&#65306;&#31232;&#30095;&#24674;&#22797;&#30340;&#37325;&#35201;&#34917;&#20805;
&lt;/p&gt;
&lt;p&gt;
Iterative Regularization with k-Support Norm: an Important Complement to Sparse Recovery. (arXiv:2401.05394v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05394
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#36845;&#20195;&#27491;&#21017;&#21270;&#31639;&#27861;IRKSN&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;$k$&#25903;&#25745;&#33539;&#25968;&#27491;&#21017;&#21270;&#23454;&#29616;&#31232;&#30095;&#24674;&#22797;&#65292;&#24182;&#25552;&#20379;&#20102;&#26465;&#20214;&#12290;&#36825;&#26159;&#23545;&#22522;&#20110;$\ell_1$&#33539;&#25968;&#30340;&#36845;&#20195;&#26041;&#27861;&#30340;&#19968;&#31181;&#37325;&#35201;&#34917;&#20805;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#24674;&#22797;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#20449;&#21495;&#22788;&#29702;&#20013;&#26080;&#22788;&#19981;&#22312;&#12290;&#30001;&#20110;&#31232;&#30095;&#24674;&#22797;&#30340;NP&#22256;&#38590;&#24615;&#36136;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#35201;&#20040;&#21463;&#38480;&#20110;&#36866;&#29992;&#26465;&#20214;&#65288;&#29978;&#33267;&#26410;&#30693;&#65289;&#65292;&#35201;&#20040;&#35745;&#31639;&#25104;&#26412;&#39640;&#12290;&#26368;&#36817;&#65292;&#36845;&#20195;&#27491;&#21017;&#21270;&#26041;&#27861;&#20316;&#20026;&#19968;&#31181;&#24555;&#36895;&#26041;&#27861;&#20986;&#29616;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#36890;&#36807;&#25552;&#21069;&#20572;&#27490;&#19968;&#27425;&#36890;&#36807;&#26469;&#23454;&#29616;&#31232;&#30095;&#24674;&#22797;&#65292;&#32780;&#19981;&#26159;&#20256;&#32479;&#26041;&#27861;&#20013;&#32321;&#29712;&#30340;&#32593;&#26684;&#25628;&#32034;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;&#36845;&#20195;&#26041;&#27861;&#37117;&#22522;&#20110;$\ell_1$&#33539;&#25968;&#65292;&#38656;&#35201;&#21463;&#38480;&#30340;&#36866;&#29992;&#26465;&#20214;&#65292;&#24182;&#19988;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#21487;&#33021;&#20250;&#22833;&#36133;&#12290;&#22240;&#27492;&#65292;&#36845;&#20195;&#27491;&#21017;&#21270;&#26041;&#27861;&#22312;&#26356;&#24191;&#27867;&#30340;&#26465;&#20214;&#19979;&#23454;&#29616;&#31232;&#30095;&#24674;&#22797;&#20173;&#38656;&#36827;&#19968;&#27493;&#25506;&#32034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36845;&#20195;&#27491;&#21017;&#21270;&#31639;&#27861;IRKSN&#65292;&#23427;&#22522;&#20110;$k$&#25903;&#25745;&#33539;&#25968;&#27491;&#21017;&#21270;&#32780;&#19981;&#26159;$\ell_1$&#33539;&#25968;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20351;&#29992;IRKSN&#36827;&#34892;&#31232;&#30095;&#24674;&#22797;&#30340;&#26465;&#20214;&#65292;&#24182;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sparse recovery is ubiquitous in machine learning and signal processing. Due to the NP-hard nature of sparse recovery, existing methods are known to suffer either from restrictive (or even unknown) applicability conditions, or high computational cost. Recently, iterative regularization methods have emerged as a promising fast approach because they can achieve sparse recovery in one pass through early stopping, rather than the tedious grid-search used in the traditional methods. However, most of those iterative methods are based on the $\ell_1$ norm which requires restrictive applicability conditions and could fail in many cases. Therefore, achieving sparse recovery with iterative regularization methods under a wider range of conditions has yet to be further explored. To address this issue, we propose a novel iterative regularization algorithm, IRKSN, based on the $k$-support norm regularizer rather than the $\ell_1$ norm. We provide conditions for sparse recovery with IRKSN, and compar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;&#20154;&#26426;&#21327;&#20316;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20154;&#31867;&#26234;&#33021;&#27880;&#20837;&#21040;AI&#20013;&#23454;&#29616;&#28151;&#21512;&#20132;&#36890;&#32534;&#38431;&#20013;&#30340;&#23433;&#20840;&#39640;&#25928;&#33258;&#21160;&#39550;&#39542;&#12290;&#35813;&#26041;&#27861;&#23558;&#20154;&#31867;&#19987;&#23478;&#20316;&#20026;&#23548;&#24072;&#65292;&#20801;&#35768;&#20195;&#29702;&#22312;&#19981;&#30830;&#23450;&#29615;&#22659;&#20013;&#36827;&#34892;&#25506;&#32034;&#65292;&#21516;&#26102;&#22312;&#21361;&#38505;&#24773;&#20917;&#19979;&#25509;&#31649;&#25511;&#21046;&#20197;&#36991;&#20813;&#20107;&#25925;&#65292;&#24182;&#25351;&#23548;&#20195;&#29702;&#20943;&#23567;&#20132;&#36890;&#27969;&#24178;&#25200;&#65292;&#20248;&#21270;&#20132;&#36890;&#27969;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.03160</link><description>&lt;p&gt;
&#20154;&#20316;&#20026;AI&#23548;&#24072;&#65306;&#22686;&#24378;&#20154;&#26426;&#21327;&#20316;&#24378;&#21270;&#23398;&#20064;&#20197;&#23454;&#29616;&#23433;&#20840;&#39640;&#25928;&#30340;&#33258;&#21160;&#39550;&#39542;
&lt;/p&gt;
&lt;p&gt;
Human as AI Mentor: Enhanced Human-in-the-loop Reinforcement Learning for Safe and Efficient Autonomous Driving. (arXiv:2401.03160v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03160
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;&#20154;&#26426;&#21327;&#20316;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20154;&#31867;&#26234;&#33021;&#27880;&#20837;&#21040;AI&#20013;&#23454;&#29616;&#28151;&#21512;&#20132;&#36890;&#32534;&#38431;&#20013;&#30340;&#23433;&#20840;&#39640;&#25928;&#33258;&#21160;&#39550;&#39542;&#12290;&#35813;&#26041;&#27861;&#23558;&#20154;&#31867;&#19987;&#23478;&#20316;&#20026;&#23548;&#24072;&#65292;&#20801;&#35768;&#20195;&#29702;&#22312;&#19981;&#30830;&#23450;&#29615;&#22659;&#20013;&#36827;&#34892;&#25506;&#32034;&#65292;&#21516;&#26102;&#22312;&#21361;&#38505;&#24773;&#20917;&#19979;&#25509;&#31649;&#25511;&#21046;&#20197;&#36991;&#20813;&#20107;&#25925;&#65292;&#24182;&#25351;&#23548;&#20195;&#29702;&#20943;&#23567;&#20132;&#36890;&#27969;&#24178;&#25200;&#65292;&#20248;&#21270;&#20132;&#36890;&#27969;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;AVs&#65289;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#30830;&#20445;AVs&#30340;&#23433;&#20840;&#24615;&#21644;&#20132;&#36890;&#27969;&#25928;&#29575;&#30340;&#39550;&#39542;&#31574;&#30053;&#30340;&#21457;&#23637;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;&#20154;&#26426;&#21327;&#20316;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;&#22522;&#20110;&#20154;&#20316;&#20026;AI&#23548;&#24072;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;HAIM-DRL&#65289;&#26694;&#26550;&#65292;&#20197;&#22312;&#28151;&#21512;&#20132;&#36890;&#32534;&#38431;&#20013;&#23454;&#29616;&#23433;&#20840;&#39640;&#25928;&#30340;&#33258;&#21160;&#39550;&#39542;&#12290;&#20174;&#20154;&#31867;&#23398;&#20064;&#36807;&#31243;&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#26377;&#25928;&#22320;&#23558;&#20154;&#31867;&#26234;&#33021;&#27880;&#20837;&#21040;AI&#20013;&#65292;&#31216;&#20026;&#20154;&#20316;&#20026;AI&#23548;&#24072;&#65288;HAIM&#65289;&#12290;&#22312;&#36825;&#20010;&#33539;&#24335;&#20013;&#65292;&#20154;&#31867;&#19987;&#23478;&#20316;&#20026;&#23548;&#24072;&#20026;AI&#20195;&#29702;&#25552;&#20379;&#24110;&#21161;&#12290;&#22312;&#20801;&#35768;&#20195;&#29702;&#22312;&#19981;&#30830;&#23450;&#29615;&#22659;&#20013;&#36827;&#34892;&#20805;&#20998;&#25506;&#32034;&#30340;&#21516;&#26102;&#65292;&#20154;&#31867;&#19987;&#23478;&#21487;&#20197;&#22312;&#21361;&#38505;&#24773;&#20917;&#19979;&#25509;&#31649;&#25511;&#21046;&#65292;&#24182;&#23637;&#31034;&#27491;&#30830;&#30340;&#34892;&#21160;&#20197;&#36991;&#20813;&#28508;&#22312;&#20107;&#25925;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#21487;&#20197;&#25351;&#23548;&#20195;&#29702;&#20943;&#23567;&#20132;&#36890;&#27969;&#24178;&#25200;&#65292;&#20174;&#32780;&#20248;&#21270;&#20132;&#36890;&#27969;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite significant progress in autonomous vehicles (AVs), the development of driving policies that ensure both the safety of AVs and traffic flow efficiency has not yet been fully explored. In this paper, we propose an enhanced human-in-the-loop reinforcement learning method, termed the Human as AI mentor-based deep reinforcement learning (HAIM-DRL) framework, which facilitates safe and efficient autonomous driving in mixed traffic platoon. Drawing inspiration from the human learning process, we first introduce an innovative learning paradigm that effectively injects human intelligence into AI, termed Human as AI mentor (HAIM). In this paradigm, the human expert serves as a mentor to the AI agent. While allowing the agent to sufficiently explore uncertain environments, the human expert can take control in dangerous situations and demonstrate correct actions to avoid potential accidents. On the other hand, the agent could be guided to minimize traffic flow disturbance, thereby optimizi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21435;&#22122;&#25193;&#25955;&#21464;&#20998;&#25512;&#26029;&#65288;DDVI&#65289;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#34920;&#36798;&#24615;&#21464;&#20998;&#21518;&#39564;&#65292;&#24182;&#36890;&#36807;&#21453;&#36716;&#21152;&#22122;&#36807;&#31243;&#22312;&#28508;&#31354;&#38388;&#20013;&#36827;&#34892;&#25193;&#25955;&#12290;&#35813;&#26041;&#27861;&#26131;&#20110;&#23454;&#29616;&#65292;&#20860;&#23481;&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;&#65292;&#24182;&#22312;&#28145;&#24230;&#28508;&#21464;&#37327;&#27169;&#22411;&#20013;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2401.02739</link><description>&lt;p&gt;
&#25193;&#25955;&#21464;&#20998;&#25512;&#26029;&#65306;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#34920;&#36798;&#24615;&#21464;&#20998;&#21518;&#39564;
&lt;/p&gt;
&lt;p&gt;
Diffusion Variational Inference: Diffusion Models as Expressive Variational Posteriors. (arXiv:2401.02739v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02739
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21435;&#22122;&#25193;&#25955;&#21464;&#20998;&#25512;&#26029;&#65288;DDVI&#65289;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#34920;&#36798;&#24615;&#21464;&#20998;&#21518;&#39564;&#65292;&#24182;&#36890;&#36807;&#21453;&#36716;&#21152;&#22122;&#36807;&#31243;&#22312;&#28508;&#31354;&#38388;&#20013;&#36827;&#34892;&#25193;&#25955;&#12290;&#35813;&#26041;&#27861;&#26131;&#20110;&#23454;&#29616;&#65292;&#20860;&#23481;&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;&#65292;&#24182;&#22312;&#28145;&#24230;&#28508;&#21464;&#37327;&#27169;&#22411;&#20013;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#21435;&#22122;&#25193;&#25955;&#21464;&#20998;&#25512;&#26029;&#65288;DDVI&#65289;&#65292;&#19968;&#31181;&#29992;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#34920;&#36798;&#24615;&#21464;&#20998;&#21518;&#39564;&#30340;&#28508;&#21464;&#37327;&#27169;&#22411;&#30340;&#36817;&#20284;&#25512;&#26029;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#36741;&#21161;&#28508;&#21464;&#37327;&#22686;&#21152;&#20102;&#21464;&#20998;&#21518;&#39564;&#65292;&#20174;&#32780;&#24471;&#21040;&#19968;&#20010;&#34920;&#36798;&#24615;&#30340;&#27169;&#22411;&#31867;&#65292;&#36890;&#36807;&#21453;&#36716;&#29992;&#25143;&#25351;&#23450;&#30340;&#21152;&#22122;&#36807;&#31243;&#22312;&#28508;&#31354;&#38388;&#20013;&#36827;&#34892;&#25193;&#25955;&#12290;&#25105;&#20204;&#36890;&#36807;&#20248;&#21270;&#19968;&#20010;&#21463;&#21040;&#35273;&#37266;-&#30561;&#30496;&#31639;&#27861;&#21551;&#21457;&#30340;&#36793;&#38469;&#20284;&#28982;&#26032;&#19979;&#30028;&#26469;&#25311;&#21512;&#36825;&#20123;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26131;&#20110;&#23454;&#29616;&#65288;&#23427;&#36866;&#37197;&#20102;&#27491;&#21017;&#21270;&#30340;ELBO&#25193;&#23637;&#65289;&#65292;&#19982;&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;&#20860;&#23481;&#65292;&#24182;&#19988;&#34920;&#29616;&#20248;&#20110;&#22522;&#20110;&#24402;&#19968;&#21270;&#27969;&#25110;&#23545;&#25239;&#32593;&#32476;&#30340;&#26367;&#20195;&#36817;&#20284;&#21518;&#39564;&#31867;&#21035;&#12290;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#28145;&#24230;&#28508;&#21464;&#37327;&#27169;&#22411;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#24471;&#21040;&#20102;&#21435;&#22122;&#25193;&#25955;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;DD-VAE&#65289;&#31639;&#27861;&#12290;&#25105;&#20204;&#23558;&#35813;&#31639;&#27861;&#24212;&#29992;&#20110;&#29983;&#29289;&#23398;&#20013;&#30340;&#19968;&#20010;&#28608;&#21169;&#20219;&#21153; -- &#20174;&#20154;&#31867;&#22522;&#22240;&#32452;&#20013;&#25512;&#26029;&#28508;&#22312;&#34880;&#32479; -- &#36229;&#36807;&#20102;&#24378;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose denoising diffusion variational inference (DDVI), an approximate inference algorithm for latent variable models which relies on diffusion models as expressive variational posteriors. Our method augments variational posteriors with auxiliary latents, which yields an expressive class of models that perform diffusion in latent space by reversing a user-specified noising process. We fit these models by optimizing a novel lower bound on the marginal likelihood inspired by the wake-sleep algorithm. Our method is easy to implement (it fits a regularized extension of the ELBO), is compatible with black-box variational inference, and outperforms alternative classes of approximate posteriors based on normalizing flows or adversarial networks. When applied to deep latent variable models, our method yields the denoising diffusion VAE (DD-VAE) algorithm. We use this algorithm on a motivating task in biology -- inferring latent ancestry from human genomes -- outperforming strong baselines
&lt;/p&gt;</description></item><item><title>&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#28040;&#38500;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#26088;&#22312;&#35299;&#20915;&#29615;&#22659;&#25152;&#26377;&#32773;&#26377;&#26435;&#25764;&#38144;&#26234;&#33021;&#20307;&#35757;&#32451;&#25968;&#25454;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;&#35813;&#39046;&#22495;&#38754;&#20020;&#19977;&#20010;&#20027;&#35201;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2312.15910</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#28040;&#38500;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Unlearning. (arXiv:2312.15910v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.15910
&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#28040;&#38500;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#26088;&#22312;&#35299;&#20915;&#29615;&#22659;&#25152;&#26377;&#32773;&#26377;&#26435;&#25764;&#38144;&#26234;&#33021;&#20307;&#35757;&#32451;&#25968;&#25454;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;&#35813;&#39046;&#22495;&#38754;&#20020;&#19977;&#20010;&#20027;&#35201;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#28040;&#38500;&#23398;&#20064;&#25351;&#30340;&#26159;&#26681;&#25454;&#25968;&#25454;&#25152;&#26377;&#32773;&#30340;&#35831;&#27714;&#65292;&#38477;&#20302;&#29305;&#23450;&#35757;&#32451;&#25968;&#25454;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24433;&#21709;&#30340;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#22312;&#28040;&#38500;&#23398;&#20064;&#30340;&#30740;&#31350;&#20013;&#65292;&#19968;&#20010;&#37325;&#35201;&#30340;&#39046;&#22495;&#24448;&#24448;&#34987;&#24573;&#35270;&#65292;&#37027;&#23601;&#26159;&#24378;&#21270;&#23398;&#20064;&#12290;&#24378;&#21270;&#23398;&#20064;&#26088;&#22312;&#35757;&#32451;&#19968;&#20010;&#26234;&#33021;&#20307;&#22312;&#29615;&#22659;&#20013;&#20570;&#20986;&#26368;&#20248;&#20915;&#31574;&#20197;&#26368;&#22823;&#21270;&#32047;&#31215;&#22870;&#21169;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#26234;&#33021;&#20307;&#24448;&#24448;&#20250;&#35760;&#24518;&#29615;&#22659;&#30340;&#29305;&#24449;&#65292;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#37325;&#22823;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;&#26681;&#25454;&#25968;&#25454;&#20445;&#25252;&#27861;&#35268;&#65292;&#29615;&#22659;&#30340;&#25152;&#26377;&#32773;&#26377;&#26435;&#25764;&#38144;&#26234;&#33021;&#20307;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#35775;&#38382;&#26435;&#38480;&#65292;&#22240;&#27492;&#38656;&#35201;&#24320;&#23637;&#19968;&#20010;&#26032;&#39062;&#19988;&#32039;&#36843;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#21363;&#8220;&#24378;&#21270;&#28040;&#38500;&#23398;&#20064;&#8221;&#12290;&#24378;&#21270;&#28040;&#38500;&#23398;&#20064;&#20391;&#37325;&#20110;&#25764;&#38144;&#25972;&#20010;&#29615;&#22659;&#32780;&#19981;&#26159;&#21333;&#20010;&#25968;&#25454;&#26679;&#26412;&#12290;&#36825;&#19968;&#29420;&#29305;&#29305;&#24449;&#24102;&#26469;&#20102;&#19977;&#20010;&#19981;&#21516;&#30340;&#25361;&#25112;&#65306;1&#65289;&#22914;&#20309;&#25552;&#20986;&#28040;&#38500;&#23398;&#20064;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Machine unlearning refers to the process of mitigating the influence of specific training data on machine learning models based on removal requests from data owners. However, one important area that has been largely overlooked in the research of unlearning is reinforcement learning. Reinforcement learning focuses on training an agent to make optimal decisions within an environment to maximize its cumulative rewards. During the training, the agent tends to memorize the features of the environment, which raises a significant concern about privacy. As per data protection regulations, the owner of the environment holds the right to revoke access to the agent's training data, thus necessitating the development of a novel and pressing research field, known as \emph{reinforcement unlearning}. Reinforcement unlearning focuses on revoking entire environments rather than individual data samples. This unique characteristic presents three distinct challenges: 1) how to propose unlearning schemes f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;, Feedback-Driven Solution Synthesis (FDSS), &#26088;&#22312;&#36890;&#36807;&#23558;LLMs&#19982;&#38745;&#24577;&#20195;&#30721;&#20998;&#26512;&#24037;&#20855;Bandit&#32467;&#21512;&#65292;&#35299;&#20915;&#20195;&#30721;&#20013;&#30340;&#23433;&#20840;&#28431;&#27934;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#29616;&#26377;&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#26377;&#26174;&#33879;&#25913;&#36827;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;PythonSecurityEval&#12290;</title><link>http://arxiv.org/abs/2312.00024</link><description>&lt;p&gt;
LLMs&#33021;&#22815;&#20462;&#22797;&#23433;&#20840;&#38382;&#39064;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can LLMs Patch Security Issues?. (arXiv:2312.00024v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.00024
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;, Feedback-Driven Solution Synthesis (FDSS), &#26088;&#22312;&#36890;&#36807;&#23558;LLMs&#19982;&#38745;&#24577;&#20195;&#30721;&#20998;&#26512;&#24037;&#20855;Bandit&#32467;&#21512;&#65292;&#35299;&#20915;&#20195;&#30721;&#20013;&#30340;&#23433;&#20840;&#28431;&#27934;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#29616;&#26377;&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#26377;&#26174;&#33879;&#25913;&#36827;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;PythonSecurityEval&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#24320;&#21457;&#32773;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#20250;&#29983;&#25104;&#21253;&#21547;&#23433;&#20840;&#28431;&#27934;&#21644;&#32570;&#38519;&#30340;&#20195;&#30721;&#12290;&#32534;&#20889;&#23433;&#20840;&#20195;&#30721;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#28431;&#27934;&#36890;&#24120;&#22312;&#31243;&#24207;&#19982;&#22806;&#37096;&#31995;&#32479;&#25110;&#26381;&#21153;&#65288;&#22914;&#25968;&#25454;&#24211;&#21644;&#25805;&#20316;&#31995;&#32479;&#65289;&#20043;&#38388;&#30340;&#20132;&#20114;&#36807;&#31243;&#20013;&#20986;&#29616;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;&#22522;&#20110;&#21453;&#39304;&#30340;&#35299;&#20915;&#26041;&#26696;&#21512;&#25104;&#65288;FDSS&#65289;&#65292;&#26088;&#22312;&#25506;&#32034;&#20351;&#29992;LLMs&#25509;&#25910;&#26469;&#33258;&#38745;&#24577;&#20195;&#30721;&#20998;&#26512;&#24037;&#20855;Bandit&#30340;&#21453;&#39304;&#65292;&#28982;&#21518;LLMs&#29983;&#25104;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#26469;&#35299;&#20915;&#23433;&#20840;&#28431;&#27934;&#12290;&#27599;&#20010;&#35299;&#20915;&#26041;&#26696;&#20197;&#21450;&#26131;&#21463;&#25915;&#20987;&#30340;&#20195;&#30721;&#38543;&#21518;&#34987;&#36865;&#22238;LLMs&#36827;&#34892;&#20195;&#30721;&#23436;&#21892;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22522;&#32447;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#25913;&#36827;&#65292;&#24182;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;PythonSecurityEval&#65292;&#35813;&#25968;&#25454;&#38598;&#25910;&#38598;&#20102;&#26469;&#33258;Stack Overflow&#30340;&#30495;&#23454;&#22330;&#26223;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown impressive proficiency in code generation. Nonetheless, similar to human developers, these models might generate code that contains security vulnerabilities and flaws. Writing secure code remains a substantial challenge, as vulnerabilities often arise during interactions between programs and external systems or services, such as databases and operating systems. In this paper, we propose a novel approach, Feedback-Driven Solution Synthesis (FDSS), designed to explore the use of LLMs in receiving feedback from Bandit, which is a static code analysis tool, and then the LLMs generate potential solutions to resolve security vulnerabilities. Each solution, along with the vulnerable code, is then sent back to the LLM for code refinement. Our approach shows a significant improvement over the baseline and outperforms existing approaches. Furthermore, we introduce a new dataset, PythonSecurityEval, collected from real-world scenarios on Stack Overflow to e
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SalUn&#30340;&#26426;&#22120;&#36951;&#24536;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;"&#26435;&#37325;&#26174;&#33879;&#24615;"&#30340;&#27010;&#24565;&#65292;&#23558;&#20851;&#27880;&#28857;&#20174;&#25972;&#20010;&#27169;&#22411;&#24341;&#23548;&#21040;&#20855;&#20307;&#30340;&#27169;&#22411;&#26435;&#37325;&#19978;&#65292;&#25552;&#39640;&#20102;&#36951;&#24536;&#30340;&#25928;&#26524;&#21644;&#25928;&#29575;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#26377;&#25928;&#28040;&#38500;&#36951;&#24536;&#25968;&#25454;&#12289;&#31867;&#21035;&#25110;&#27010;&#24565;&#24433;&#21709;&#30340;&#26377;&#21407;&#21017;&#30340;&#26426;&#22120;&#36951;&#24536;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.12508</link><description>&lt;p&gt;
SalUn&#65306;&#36890;&#36807;&#22522;&#20110;&#26799;&#24230;&#30340;&#26435;&#37325;&#26174;&#33879;&#24615;&#22686;&#24378;&#26426;&#22120;&#36951;&#24536;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#29983;&#25104;&#20013;&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
SalUn: Empowering Machine Unlearning via Gradient-based Weight Saliency in Both Image Classification and Generation. (arXiv:2310.12508v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12508
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SalUn&#30340;&#26426;&#22120;&#36951;&#24536;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;"&#26435;&#37325;&#26174;&#33879;&#24615;"&#30340;&#27010;&#24565;&#65292;&#23558;&#20851;&#27880;&#28857;&#20174;&#25972;&#20010;&#27169;&#22411;&#24341;&#23548;&#21040;&#20855;&#20307;&#30340;&#27169;&#22411;&#26435;&#37325;&#19978;&#65292;&#25552;&#39640;&#20102;&#36951;&#24536;&#30340;&#25928;&#26524;&#21644;&#25928;&#29575;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#26377;&#25928;&#28040;&#38500;&#36951;&#24536;&#25968;&#25454;&#12289;&#31867;&#21035;&#25110;&#27010;&#24565;&#24433;&#21709;&#30340;&#26377;&#21407;&#21017;&#30340;&#26426;&#22120;&#36951;&#24536;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25968;&#25454;&#27861;&#35268;&#30340;&#19981;&#26029;&#21457;&#23637;&#65292;&#26426;&#22120;&#36951;&#24536;&#65288;MU&#65289;&#24050;&#25104;&#20026;&#22686;&#24378;&#24403;&#21069;AI&#27169;&#22411;&#30340;&#20449;&#20219;&#21644;&#23433;&#20840;&#24615;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;MU&#26041;&#27861;&#36890;&#24120;&#22312;&#36951;&#24536;&#31934;&#24230;&#12289;&#31283;&#23450;&#24615;&#21644;&#36328;&#39046;&#22495;&#36866;&#29992;&#24615;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MU&#20013;&#30340;&#8220;&#26435;&#37325;&#26174;&#33879;&#24615;&#8221;&#27010;&#24565;&#65292;&#20511;&#37492;&#20102;&#27169;&#22411;&#35299;&#37322;&#20013;&#30340;&#36755;&#20837;&#26174;&#33879;&#24615;&#12290;&#36825;&#19968;&#21019;&#26032;&#23558;MU&#30340;&#20851;&#27880;&#28857;&#20174;&#25972;&#20010;&#27169;&#22411;&#24341;&#23548;&#21040;&#20102;&#20855;&#20307;&#30340;&#27169;&#22411;&#26435;&#37325;&#19978;&#65292;&#25552;&#39640;&#20102;&#20854;&#25928;&#26524;&#21644;&#25928;&#29575;&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;&#26174;&#33879;&#24615;&#36951;&#24536;&#65288;SalUn&#65289;&#30340;&#26041;&#27861;&#23558;&#20854;&#19982;&#8220;&#31934;&#30830;&#8221;&#36951;&#24536;&#65288;&#22312;&#21024;&#38500;&#36951;&#24536;&#25968;&#25454;&#38598;&#21518;&#20174;&#22836;&#24320;&#22987;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#65289;&#30340;&#24615;&#33021;&#24046;&#36317;&#32553;&#23567;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;SalUn&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#29983;&#25104;&#20013;&#26377;&#25928;&#28040;&#38500;&#36951;&#24536;&#25968;&#25454;&#12289;&#31867;&#21035;&#25110;&#27010;&#24565;&#24433;&#21709;&#30340;&#26377;&#21407;&#21017;&#30340;MU&#26041;&#27861;&#12290;&#20363;&#22914;&#65292;SalUn&#21487;&#22312;&#22270;&#29255;&#20998;&#31867;&#21644;&#29983;&#25104;&#20219;&#21153;&#20013;&#25830;&#38500;&#36951;&#24536;&#25968;&#25454;&#12289;&#31867;&#21035;&#25110;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
With evolving data regulations, machine unlearning (MU) has become an important tool for fostering trust and safety in today's AI models. However, existing MU methods focusing on data and/or weight perspectives often grapple with limitations in unlearning accuracy, stability, and cross-domain applicability. To address these challenges, we introduce the concept of 'weight saliency' in MU, drawing parallels with input saliency in model explanation. This innovation directs MU's attention toward specific model weights rather than the entire model, improving effectiveness and efficiency. The resultant method that we call saliency unlearning (SalUn) narrows the performance gap with 'exact' unlearning (model retraining from scratch after removing the forgetting dataset). To the best of our knowledge, SalUn is the first principled MU approach adaptable enough to effectively erase the influence of forgetting data, classes, or concepts in both image classification and generation. For example, Sa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#21152;&#36895;&#31574;&#30053;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24212;&#29992;Nesterov&#21160;&#37327;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#23454;&#29616;&#26356;&#24555;&#30340;&#20840;&#23616;&#25910;&#25947;&#36895;&#24230;&#12290;&#36890;&#36807;&#22312;softmax&#31574;&#30053;&#21442;&#25968;&#21270;&#20013;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#65292;&#23427;&#20197; $\tilde{O}(1/t^2)$ &#30340;&#36895;&#29575;&#25910;&#25947;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;Nesterov&#21152;&#36895;&#26799;&#24230;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#20840;&#23616;&#25910;&#25947;&#36895;&#29575;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2310.11897</link><description>&lt;p&gt;
&#21152;&#36895;&#31574;&#30053;&#26799;&#24230;&#65306;&#20851;&#20110;&#24212;&#29992;Nesterov&#21160;&#37327;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
Accelerated Policy Gradient: On the Nesterov Momentum for Reinforcement Learning. (arXiv:2310.11897v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11897
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#21152;&#36895;&#31574;&#30053;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24212;&#29992;Nesterov&#21160;&#37327;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#23454;&#29616;&#26356;&#24555;&#30340;&#20840;&#23616;&#25910;&#25947;&#36895;&#24230;&#12290;&#36890;&#36807;&#22312;softmax&#31574;&#30053;&#21442;&#25968;&#21270;&#20013;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#65292;&#23427;&#20197; $\tilde{O}(1/t^2)$ &#30340;&#36895;&#29575;&#25910;&#25947;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;Nesterov&#21152;&#36895;&#26799;&#24230;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#20840;&#23616;&#25910;&#25947;&#36895;&#29575;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30740;&#31350;&#34920;&#26126;&#65292;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#22312;&#38750;&#27491;&#21017;&#21270;&#34920;&#26684;softmax&#35774;&#32622;&#20013;&#20197;&#920;(1/t)&#30340;&#36895;&#29575;&#20840;&#23616;&#25910;&#25947;&#12290;&#22240;&#27492;&#65292;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#38382;&#39064;&#26159;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#20165;&#20351;&#29992;&#19968;&#38454;&#26356;&#26032;&#36827;&#19968;&#27493;&#25913;&#36827;&#36825;&#31181;&#25910;&#25947;&#36895;&#24230;&#12290;&#26412;&#25991;&#20174;&#21160;&#37327;&#30340;&#35282;&#24230;&#22238;&#31572;&#20102;&#19978;&#36848;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#33879;&#21517;&#30340;Nesterov&#21152;&#36895;&#26799;&#24230;&#65288;NAG&#65289;&#26041;&#27861;&#24212;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#65292;&#31216;&#20043;&#20026; \textit{&#21152;&#36895;&#31574;&#30053;&#26799;&#24230;}&#65288;APG&#65289;&#12290;&#20026;&#20102;&#23637;&#31034;APG&#22312;&#23454;&#29616;&#26356;&#24555;&#20840;&#23616;&#25910;&#25947;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#25105;&#20204;&#27491;&#24335;&#35777;&#26126;&#20102;&#20351;&#29992;&#30495;&#23454;&#26799;&#24230;&#26102;&#65292;&#20855;&#26377; softmax &#31574;&#30053;&#21442;&#25968;&#21270;&#30340;APG&#20197; $\tilde{O}(1/t^2)$ &#30340;&#36895;&#29575;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;NAG&#22312;RL&#39046;&#22495;&#20013;&#20840;&#23616;&#25910;&#25947;&#36895;&#29575;&#30340;&#31532;&#19968;&#20010;&#34920;&#24449;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#20381;&#36182;&#20110;&#19968;&#20010;&#26377;&#36259;&#30340;&#21457;&#29616;&#65306;&#19981;&#35770;&#21021;&#22987;&#21270;&#22914;&#20309;&#65292;APG&#26368;&#32456;&#21487;&#20197;&#36798;&#21040;&#36817;&#20046;&#23616;&#37096;&#25910;&#25947;&#30340;&#22320;&#26041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Policy gradient methods have recently been shown to enjoy global convergence at a $\Theta(1/t)$ rate in the non-regularized tabular softmax setting. Accordingly, one important research question is whether this convergence rate can be further improved, with only first-order updates. In this paper, we answer the above question from the perspective of momentum by adapting the celebrated Nesterov's accelerated gradient (NAG) method to reinforcement learning (RL), termed \textit{Accelerated Policy Gradient} (APG). To demonstrate the potential of APG in achieving faster global convergence, we formally show that with the true gradient, APG with softmax policy parametrization converges to an optimal policy at a $\tilde{O}(1/t^2)$ rate. To the best of our knowledge, this is the first characterization of the global convergence rate of NAG in the context of RL. Notably, our analysis relies on one interesting finding: Regardless of the initialization, APG could end up reaching a locally nearly-con
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#36890;&#36807;&#23545;&#24378;&#21270;&#23398;&#20064;&#20013;17&#31181;&#30446;&#26631;&#35268;&#33539;&#24418;&#24335;&#30340;&#34920;&#36798;&#33021;&#21147;&#36827;&#34892;&#20840;&#38754;&#27604;&#36739;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#25991;&#29486;&#20013;&#30340;&#31354;&#30333;&#12290;&#36890;&#36807;&#23558;&#36825;&#20123;&#24418;&#24335;&#21270;&#26041;&#27861;&#36827;&#34892;&#39044;&#25490;&#24207;&#65292;&#24182;&#21576;&#29616;&#20026;&#21704;&#26031;&#22270;&#65292;&#25105;&#20204;&#21457;&#29616;&#19981;&#21516;&#24418;&#24335;&#21270;&#26041;&#27861;&#23384;&#22312;&#21508;&#31181;&#38480;&#21046;&#65292;&#24182;&#19988;&#27809;&#26377;&#19968;&#31181;&#24418;&#24335;&#21270;&#26041;&#27861;&#26082;&#20855;&#26377;&#20027;&#23548;&#24615;&#30340;&#34920;&#36798;&#33021;&#21147;&#21448;&#23481;&#26131;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2310.11840</link><description>&lt;p&gt;
&#20851;&#20110;&#24378;&#21270;&#23398;&#20064;&#20013;&#30446;&#26631;&#35268;&#33539;&#24418;&#24335;&#30340;&#34920;&#36798;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
On The Expressivity of Objective-Specification Formalisms in Reinforcement Learning. (arXiv:2310.11840v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11840
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#36890;&#36807;&#23545;&#24378;&#21270;&#23398;&#20064;&#20013;17&#31181;&#30446;&#26631;&#35268;&#33539;&#24418;&#24335;&#30340;&#34920;&#36798;&#33021;&#21147;&#36827;&#34892;&#20840;&#38754;&#27604;&#36739;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#25991;&#29486;&#20013;&#30340;&#31354;&#30333;&#12290;&#36890;&#36807;&#23558;&#36825;&#20123;&#24418;&#24335;&#21270;&#26041;&#27861;&#36827;&#34892;&#39044;&#25490;&#24207;&#65292;&#24182;&#21576;&#29616;&#20026;&#21704;&#26031;&#22270;&#65292;&#25105;&#20204;&#21457;&#29616;&#19981;&#21516;&#24418;&#24335;&#21270;&#26041;&#27861;&#23384;&#22312;&#21508;&#31181;&#38480;&#21046;&#65292;&#24182;&#19988;&#27809;&#26377;&#19968;&#31181;&#24418;&#24335;&#21270;&#26041;&#27861;&#26082;&#20855;&#26377;&#20027;&#23548;&#24615;&#30340;&#34920;&#36798;&#33021;&#21147;&#21448;&#23481;&#26131;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35201;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#65292;&#24517;&#39035;&#23545;&#35813;&#20219;&#21153;&#30340;&#30446;&#26631;&#36827;&#34892;&#24418;&#24335;&#21270;&#35268;&#23450;&#12290;&#23613;&#31649;&#22823;&#22810;&#25968;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#35201;&#27714;&#23558;&#30446;&#26631;&#24418;&#24335;&#21270;&#20026;&#39532;&#23572;&#21487;&#22827;&#22870;&#21169;&#20989;&#25968;&#65292;&#20294;&#24050;&#32463;&#24320;&#21457;&#20986;&#20102;&#20854;&#20182;&#26367;&#20195;&#26041;&#27861;&#65288;&#22914;&#32447;&#24615;&#26102;&#38388;&#36923;&#36753;&#21644;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#65289;&#12290;&#27492;&#22806;&#65292;&#20247;&#25152;&#21608;&#30693;&#65292;&#20854;&#20013;&#19968;&#20123;&#24418;&#24335;&#21270;&#26041;&#27861;&#33021;&#22815;&#34920;&#36798;&#20854;&#20182;&#24418;&#24335;&#21270;&#26041;&#27861;&#26080;&#27861;&#34920;&#36798;&#30340;&#29305;&#23450;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#23545;&#36825;&#20123;&#24418;&#24335;&#21270;&#26041;&#27861;&#22312;&#34920;&#36798;&#33021;&#21147;&#26041;&#38754;&#22914;&#20309;&#30456;&#20114;&#20851;&#32852;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#24378;&#21270;&#23398;&#20064;&#20013;17&#31181;&#30446;&#26631;&#35268;&#33539;&#24418;&#24335;&#30340;&#34920;&#36798;&#33021;&#21147;&#36827;&#34892;&#20840;&#38754;&#27604;&#36739;&#22635;&#34917;&#20102;&#29616;&#26377;&#25991;&#29486;&#20013;&#30340;&#31354;&#30333;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#24418;&#24335;&#21270;&#26041;&#27861;&#26681;&#25454;&#20854;&#34920;&#36798;&#33021;&#21147;&#36827;&#34892;&#39044;&#25490;&#24207;&#65292;&#24182;&#23558;&#35813;&#39044;&#25490;&#24207;&#21576;&#29616;&#20026;&#21704;&#26031;&#22270;&#12290;&#25105;&#20204;&#21457;&#29616;&#19981;&#21516;&#24418;&#24335;&#21270;&#26041;&#27861;&#23384;&#22312;&#21508;&#31181;&#38480;&#21046;&#65292;&#24182;&#19988;&#27809;&#26377;&#19968;&#31181;&#24418;&#24335;&#21270;&#26041;&#27861;&#26082;&#20855;&#26377;&#20027;&#23548;&#24615;&#30340;&#34920;&#36798;&#33021;&#21147;&#21448;&#23481;&#26131;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
To solve a task with reinforcement learning (RL), it is necessary to formally specify the goal of that task. Although most RL algorithms require that the goal is formalised as a Markovian reward function, alternatives have been developed (such as Linear Temporal Logic and Multi-Objective Reinforcement Learning). Moreover, it is well known that some of these formalisms are able to express certain tasks that other formalisms cannot express. However, there has not yet been any thorough analysis of how these formalisms relate to each other in terms of expressivity. In this work, we fill this gap in the existing literature by providing a comprehensive comparison of the expressivities of 17 objective-specification formalisms in RL. We place these formalisms in a preorder based on their expressive power, and present this preorder as a Hasse diagram. We find a variety of limitations for the different formalisms, and that no formalism is both dominantly expressive and straightforward to optimis
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;FedHGNN&#65289;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#20998;&#24067;&#24335;&#30340;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#19978;&#21327;&#21516;&#35757;&#32451;&#25512;&#33616;&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2310.11730</link><description>&lt;p&gt;
&#38754;&#21521;&#38544;&#31169;&#20445;&#25252;&#25512;&#33616;&#30340;&#32852;&#37030;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Federated Heterogeneous Graph Neural Network for Privacy-preserving Recommendation. (arXiv:2310.11730v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11730
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;FedHGNN&#65289;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#20998;&#24067;&#24335;&#30340;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#19978;&#21327;&#21516;&#35757;&#32451;&#25512;&#33616;&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#65288;HIN&#65289;&#36890;&#36807;&#20803;&#36335;&#24452;&#25551;&#36848;&#20016;&#23500;&#30340;&#35821;&#20041;&#65292;&#24050;&#25104;&#20026;&#32531;&#35299;&#25512;&#33616;&#31995;&#32479;&#25968;&#25454;&#31232;&#30095;&#24615;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;HIN&#30340;&#25512;&#33616;&#31995;&#32479;&#25345;&#26377;&#25968;&#25454;&#30340;&#38598;&#20013;&#23384;&#20648;&#20551;&#35774;&#65292;&#24182;&#36827;&#34892;&#38598;&#20013;&#24335;&#27169;&#22411;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38544;&#31169;&#38382;&#39064;&#65292;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#24448;&#24448;&#20197;&#20998;&#24067;&#24335;&#26041;&#24335;&#23384;&#20648;&#65292;&#23548;&#33268;&#38598;&#20013;&#24335;HIN&#25512;&#33616;&#26080;&#27861;&#23454;&#29616;&#12290;&#26412;&#25991;&#25552;&#20986;&#23558;HIN&#20998;&#20026;&#23458;&#25143;&#31471;&#23384;&#20648;&#30340;&#31169;&#26377;HIN&#21644;&#26381;&#21153;&#22120;&#31471;&#30340;&#20849;&#20139;HIN&#12290;&#22312;&#27492;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;FedHGNN&#65289;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#20998;&#24067;&#24335;HIN&#19978;&#21327;&#20316;&#35757;&#32451;&#25512;&#33616;&#27169;&#22411;&#65292;&#21516;&#26102;&#19981;&#27844;&#38706;&#29992;&#25143;&#38544;&#31169;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#38024;&#23545;&#22522;&#20110;HIN&#30340;&#32852;&#21512;&#25512;&#33616;&#65292;&#22522;&#20110;&#24046;&#20998;&#38544;&#31169;&#30340;&#20809;&#19979;&#30830;&#23450;&#20102;&#38544;&#31169;&#23450;&#20041;&#65292;&#26088;&#22312;&#20445;&#25252;&#31169;&#26377;HIN&#30340;&#29992;&#25143;-&#21830;&#21697;&#20132;&#20114;&#65292;&#20197;&#21450;&#29992;&#25143;&#30340;&#38544;&#31169;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous information network (HIN), which contains rich semantics depicted by meta-paths, has become a powerful tool to alleviate data sparsity in recommender systems. Existing HIN-based recommendations hold the data centralized storage assumption and conduct centralized model training. However, the real-world data is often stored in a distributed manner for privacy concerns, resulting in the failure of centralized HIN-based recommendations. In this paper, we suggest the HIN is partitioned into private HINs stored in the client side and shared HINs in the server. Following this setting, we propose a federated heterogeneous graph neural network (FedHGNN) based framework, which can collaboratively train a recommendation model on distributed HINs without leaking user privacy. Specifically, we first formalize the privacy definition in the light of differential privacy for HIN-based federated recommendation, which aims to protect user-item interactions of private HIN as well as user's 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25935;&#24863;&#24615;&#24863;&#30693;&#30340;&#25674;&#38144;&#36125;&#21494;&#26031;&#25512;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#26435;&#37325;&#20849;&#20139;&#21644;&#31070;&#32463;&#32593;&#32476;&#26469;&#36827;&#34892;&#20284;&#28982;&#21644;&#20808;&#39564;&#35268;&#33539;&#30340;&#35757;&#32451;&#65292;&#20197;&#21450;&#23545;&#25968;&#25454;&#25200;&#21160;&#21644;&#39044;&#22788;&#29702;&#31243;&#24207;&#30340;&#25935;&#24863;&#24615;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2310.11122</link><description>&lt;p&gt;
&#25935;&#24863;&#24615;&#24863;&#30693;&#30340;&#25674;&#38144;&#36125;&#21494;&#26031;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Sensitivity-Aware Amortized Bayesian Inference. (arXiv:2310.11122v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11122
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25935;&#24863;&#24615;&#24863;&#30693;&#30340;&#25674;&#38144;&#36125;&#21494;&#26031;&#25512;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#26435;&#37325;&#20849;&#20139;&#21644;&#31070;&#32463;&#32593;&#32476;&#26469;&#36827;&#34892;&#20284;&#28982;&#21644;&#20808;&#39564;&#35268;&#33539;&#30340;&#35757;&#32451;&#65292;&#20197;&#21450;&#23545;&#25968;&#25454;&#25200;&#21160;&#21644;&#39044;&#22788;&#29702;&#31243;&#24207;&#30340;&#25935;&#24863;&#24615;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#25512;&#26029;&#26159;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#36827;&#34892;&#27010;&#29575;&#25512;&#29702;&#21644;&#20915;&#31574;&#30340;&#24378;&#22823;&#26694;&#26550;&#12290;&#29616;&#20195;&#36125;&#21494;&#26031;&#24037;&#20316;&#27969;&#31243;&#20013;&#30340;&#22522;&#26412;&#36873;&#25321;&#28041;&#21450;&#20284;&#28982;&#20989;&#25968;&#21644;&#20808;&#39564;&#20998;&#24067;&#30340;&#35268;&#33539;&#12289;&#21518;&#39564;&#36924;&#36817;&#22120;&#21644;&#25968;&#25454;&#12290;&#27599;&#20010;&#36873;&#25321;&#37117;&#21487;&#20197;&#26174;&#30528;&#24433;&#21709;&#22522;&#20110;&#27169;&#22411;&#30340;&#25512;&#26029;&#21644;&#21518;&#32493;&#20915;&#31574;&#65292;&#22240;&#27492;&#38656;&#35201;&#36827;&#34892;&#25935;&#24863;&#24615;&#20998;&#26512;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26041;&#38754;&#30340;&#26041;&#27861;&#65292;&#23558;&#25935;&#24863;&#24615;&#20998;&#26512;&#25972;&#21512;&#21040;&#25674;&#38144;&#36125;&#21494;&#26031;&#25512;&#26029;&#65288;ABI&#65292;&#21363;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#25311;&#25512;&#26029;&#65289;&#20013;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21033;&#29992;&#26435;&#37325;&#20849;&#20139;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#32534;&#30721;&#26367;&#20195;&#20284;&#28982;&#21644;&#20808;&#39564;&#35268;&#33539;&#20043;&#38388;&#30340;&#32467;&#26500;&#30456;&#20284;&#24615;&#65292;&#20197;&#26368;&#23567;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#24555;&#36895;&#25512;&#26029;&#26469;&#35780;&#20272;&#23545;&#21508;&#31181;&#25968;&#25454;&#25200;&#21160;&#25110;&#39044;&#22788;&#29702;&#31243;&#24207;&#30340;&#25935;&#24863;&#24615;&#12290;&#19982;&#22823;&#22810;&#25968;&#20854;&#20182;&#36125;&#21494;&#26031;&#26041;&#27861;&#30456;&#27604;&#65292;&#36825;&#20004;&#20010;&#27493;&#39588;&#37117;&#36991;&#20813;&#20102;&#26114;&#36149;&#30340;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian inference is a powerful framework for making probabilistic inferences and decisions under uncertainty. Fundamental choices in modern Bayesian workflows concern the specification of the likelihood function and prior distributions, the posterior approximator, and the data. Each choice can significantly influence model-based inference and subsequent decisions, thereby necessitating sensitivity analysis. In this work, we propose a multifaceted approach to integrate sensitivity analyses into amortized Bayesian inference (ABI, i.e., simulation-based inference with neural networks). First, we utilize weight sharing to encode the structural similarities between alternative likelihood and prior specifications in the training process with minimal computational overhead. Second, we leverage the rapid inference of neural networks to assess sensitivity to various data perturbations or pre-processing procedures. In contrast to most other Bayesian approaches, both steps circumvent the costly
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21435;&#23398;&#20064;&#26159;&#19968;&#20010;&#30740;&#31350;&#30340;&#26032;&#39046;&#22495;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19977;&#20010;&#22330;&#26223;&#65292;&#21487;&#20197;&#36890;&#36807;&#21435;&#23398;&#20064;&#35753;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#20445;&#25345;&#19968;&#33268;&#12290;&#21435;&#23398;&#20064;&#20855;&#26377;&#19977;&#20010;&#20248;&#21183;&#65292;&#21482;&#38656;&#35201;&#36127;&#38754;&#31034;&#20363;&#65292;&#35745;&#31639;&#25928;&#29575;&#39640;&#65292;&#29305;&#21035;&#23545;&#20110;&#30693;&#36947;&#20855;&#20307;&#23548;&#33268;&#19981;&#33391;&#34892;&#20026;&#30340;&#35757;&#32451;&#26679;&#26412;&#26356;&#20026;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2310.10683</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21435;&#23398;&#20064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Large Language Model Unlearning. (arXiv:2310.10683v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10683
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21435;&#23398;&#20064;&#26159;&#19968;&#20010;&#30740;&#31350;&#30340;&#26032;&#39046;&#22495;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19977;&#20010;&#22330;&#26223;&#65292;&#21487;&#20197;&#36890;&#36807;&#21435;&#23398;&#20064;&#35753;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#20445;&#25345;&#19968;&#33268;&#12290;&#21435;&#23398;&#20064;&#20855;&#26377;&#19977;&#20010;&#20248;&#21183;&#65292;&#21482;&#38656;&#35201;&#36127;&#38754;&#31034;&#20363;&#65292;&#35745;&#31639;&#25928;&#29575;&#39640;&#65292;&#29305;&#21035;&#23545;&#20110;&#30693;&#36947;&#20855;&#20307;&#23548;&#33268;&#19981;&#33391;&#34892;&#20026;&#30340;&#35757;&#32451;&#26679;&#26412;&#26356;&#20026;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#21435;&#23398;&#20064;&#65292;&#21363;&#24536;&#35760;&#19981;&#21463;&#27426;&#36814;&#30340;&#65288;&#38750;&#65289;&#34892;&#20026;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#33267;&#23569;&#19977;&#31181;&#24773;&#22659;&#21487;&#20197;&#20174;&#21435;&#23398;&#20064;&#20013;&#20351;LLMs&#19982;&#20154;&#31867;&#20559;&#22909;&#20445;&#25345;&#19968;&#33268;&#65306;&#65288;1&#65289;&#21024;&#38500;&#26377;&#23475;&#22238;&#22797;&#65292;&#65288;2&#65289;&#25353;&#35201;&#27714;&#21024;&#38500;&#21463;&#29256;&#26435;&#20445;&#25252;&#30340;&#20869;&#23481;&#65292;&#20197;&#21450;&#65288;3&#65289;&#28040;&#38500;&#24187;&#35273;&#12290;&#20316;&#20026;&#23545;&#40784;&#25216;&#26415;&#30340;&#19968;&#31181;&#65292;&#21435;&#23398;&#20064;&#20855;&#26377;&#19977;&#20010;&#20248;&#28857;&#65306;&#65288;1&#65289;&#21482;&#38656;&#35201;&#36127;&#38754;&#65288;&#20363;&#22914;&#26377;&#23475;&#65289;&#31034;&#20363;&#65292;&#36825;&#27604;&#22312;RLHF&#65288;&#22522;&#20110;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65289;&#20013;&#25152;&#38656;&#30340;&#27491;&#38754;&#65288;&#20363;&#22914;&#26377;&#24110;&#21161;&#19988;&#36890;&#24120;&#30001;&#20154;&#31867;&#32534;&#20889;&#65289;&#31034;&#20363;&#26356;&#23481;&#26131;&#21644;&#26356;&#20415;&#23452;&#22320;&#25910;&#38598;&#65288;&#20363;&#22914;&#36890;&#36807;&#32418;&#38431;&#27979;&#35797;&#25110;&#29992;&#25143;&#25253;&#21578;&#65289;&#65307;&#65288;2&#65289;&#35745;&#31639;&#25928;&#29575;&#39640;&#65307;&#65288;3&#65289;&#24403;&#25105;&#20204;&#30693;&#36947;&#21738;&#20123;&#35757;&#32451;&#26679;&#26412;&#23548;&#33268;&#20102;&#19981;&#33391;&#34892;&#20026;&#26102;&#65292;&#23427;&#29305;&#21035;&#26377;&#25928;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#26159;&#39318;&#27425;&#25506;&#32034;LLM&#21435;&#23398;&#20064;&#30340;&#24037;&#20316;&#20043;&#19968;&#12290;&#25105;&#20204;&#20063;&#26159;&#39318;&#27425;&#22312;LLM&#21435;&#23398;&#20064;&#20013;&#21046;&#23450;&#20102;&#35774;&#32622;&#12289;&#30446;&#26631;&#21644;&#35780;&#20272;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22914;&#26524;&#20174;&#19994;&#32773;&#21482;&#26377;&#26377;&#38480;&#30340;
&lt;/p&gt;
&lt;p&gt;
We study how to perform unlearning, i.e. forgetting undesirable (mis)behaviors, on large language models (LLMs). We show at least three scenarios of aligning LLMs with human preferences can benefit from unlearning: (1) removing harmful responses, (2) erasing copyright-protected content as requested, and (3) eliminating hallucinations. Unlearning, as an alignment technique, has three advantages. (1) It only requires negative (e.g. harmful) examples, which are much easier and cheaper to collect (e.g. via red teaming or user reporting) than positive (e.g. helpful and often human-written) examples required in RLHF (RL from human feedback). (2) It is computationally efficient. (3) It is especially effective when we know which training samples cause the misbehavior. To the best of our knowledge, our work is among the first to explore LLM unlearning. We are also among the first to formulate the settings, goals, and evaluations in LLM unlearning. We show that if practitioners only have limited
&lt;/p&gt;</description></item><item><title>&#32452;&#21512;&#33021;&#21147;&#20197;&#20056;&#27861;&#26041;&#24335;&#20986;&#29616;&#65306;&#30740;&#31350;&#20102;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#22312;&#21512;&#25104;&#20219;&#21153;&#20013;&#30340;&#32452;&#21512;&#27867;&#21270;&#33021;&#21147;&#65292;&#32467;&#26524;&#26174;&#31034;&#36825;&#31181;&#33021;&#21147;&#21463;&#21040;&#24213;&#23618;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#30340;&#32467;&#26500;&#24433;&#21709;&#65292;&#19988;&#27169;&#22411;&#22312;&#23398;&#20064;&#21040;&#26356;&#39640;&#32423;&#30340;&#32452;&#21512;&#26102;&#23384;&#22312;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2310.09336</link><description>&lt;p&gt;
&#32452;&#21512;&#33021;&#21147;&#20197;&#20056;&#27861;&#26041;&#24335;&#20986;&#29616;&#65306;&#22312;&#21512;&#25104;&#20219;&#21153;&#20013;&#25506;&#32034;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Compositional Abilities Emerge Multiplicatively: Exploring Diffusion Models on a Synthetic Task. (arXiv:2310.09336v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09336
&lt;/p&gt;
&lt;p&gt;
&#32452;&#21512;&#33021;&#21147;&#20197;&#20056;&#27861;&#26041;&#24335;&#20986;&#29616;&#65306;&#30740;&#31350;&#20102;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#22312;&#21512;&#25104;&#20219;&#21153;&#20013;&#30340;&#32452;&#21512;&#27867;&#21270;&#33021;&#21147;&#65292;&#32467;&#26524;&#26174;&#31034;&#36825;&#31181;&#33021;&#21147;&#21463;&#21040;&#24213;&#23618;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#30340;&#32467;&#26500;&#24433;&#21709;&#65292;&#19988;&#27169;&#22411;&#22312;&#23398;&#20064;&#21040;&#26356;&#39640;&#32423;&#30340;&#32452;&#21512;&#26102;&#23384;&#22312;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#29983;&#25104;&#27169;&#22411;&#23637;&#31034;&#20986;&#20102;&#20135;&#29983;&#26497;&#20026;&#36924;&#30495;&#25968;&#25454;&#30340;&#21069;&#25152;&#26410;&#26377;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#32771;&#34385;&#21040;&#29616;&#23454;&#19990;&#30028;&#30340;&#33258;&#28982;&#32452;&#21512;&#24615;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21487;&#38752;&#20351;&#29992;&#38656;&#35201;&#23637;&#31034;&#20986;&#33021;&#22815;&#32452;&#21512;&#26032;&#30340;&#27010;&#24565;&#38598;&#21512;&#20197;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#26410;&#35265;&#30340;&#36755;&#20986;&#30340;&#33021;&#21147;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#26368;&#36817;&#30340;&#25193;&#25955;&#27169;&#22411;&#30830;&#23454;&#34920;&#29616;&#20986;&#20102;&#26377;&#36259;&#30340;&#32452;&#21512;&#27867;&#21270;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#20063;&#20250;&#20986;&#29616;&#26080;&#27861;&#39044;&#27979;&#30340;&#22833;&#36133;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#22312;&#21512;&#25104;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#26377;&#25511;&#21046;&#24615;&#30340;&#30740;&#31350;&#65292;&#20197;&#20102;&#35299;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#30340;&#32452;&#21512;&#27867;&#21270;&#33021;&#21147;&#65292;&#25105;&#20204;&#21464;&#21270;&#20102;&#35757;&#32451;&#25968;&#25454;&#30340;&#19981;&#21516;&#23646;&#24615;&#24182;&#27979;&#37327;&#20102;&#27169;&#22411;&#29983;&#25104;&#36234;&#30028;&#26679;&#26412;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65306;&#65288;i&#65289;&#20174;&#19968;&#20010;&#27010;&#24565;&#29983;&#25104;&#26679;&#26412;&#30340;&#33021;&#21147;&#21644;&#23558;&#23427;&#20204;&#32452;&#21512;&#36215;&#26469;&#30340;&#33021;&#21147;&#30340;&#20986;&#29616;&#39034;&#24207;&#21463;&#21040;&#20102;&#24213;&#23618;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#30340;&#32467;&#26500;&#30340;&#24433;&#21709;&#65307;&#65288;ii&#65289;&#22312;&#32452;&#21512;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#34920;&#26126;&#27169;&#22411;&#22312;&#23398;&#20064;&#21040;&#26356;&#39640;&#32423;&#30340;&#32452;&#21512;&#26102;&#23384;&#22312;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern generative models exhibit unprecedented capabilities to generate extremely realistic data. However, given the inherent compositionality of the real world, reliable use of these models in practical applications requires that they exhibit the capability to compose a novel set of concepts to generate outputs not seen in the training data set. Prior work demonstrates that recent diffusion models do exhibit intriguing compositional generalization abilities, but also fail unpredictably. Motivated by this, we perform a controlled study for understanding compositional generalization in conditional diffusion models in a synthetic setting, varying different attributes of the training data and measuring the model's ability to generate samples out-of-distribution. Our results show: (i) the order in which the ability to generate samples from a concept and compose them emerges is governed by the structure of the underlying data-generating process; (ii) performance on compositional tasks exhib
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#28145;&#20837;&#20998;&#26512;&#20102;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27599;&#20010;&#38454;&#27573;&#23545;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;&#21644;&#36755;&#20986;&#22810;&#26679;&#24615;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.06452</link><description>&lt;p&gt;
&#29702;&#35299;RLHF&#23545;LLM&#27867;&#21270;&#21644;&#22810;&#26679;&#24615;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Understanding the Effects of RLHF on LLM Generalisation and Diversity. (arXiv:2310.06452v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06452
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#28145;&#20837;&#20998;&#26512;&#20102;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27599;&#20010;&#38454;&#27573;&#23545;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;&#21644;&#36755;&#20986;&#22810;&#26679;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;AI&#27169;&#22411;&#20013;&#65292;&#22914;OpenAI&#30340;ChatGPT&#25110;Anthropic&#30340;Claude&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#12290;&#23613;&#31649;&#22312;&#36825;&#20123;&#26041;&#27861;&#30340;&#24320;&#21457;&#26041;&#38754;&#26377;&#22823;&#37327;&#30340;&#30740;&#31350;&#65292;&#20294;&#25105;&#20204;&#23545;RLHF&#36807;&#31243;&#20013;&#27599;&#20010;&#38454;&#27573;&#30340;&#21033;&#19982;&#24330;&#30340;&#29702;&#35299;&#20173;&#28982;&#26377;&#38480;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#23545;&#27599;&#20010;&#38454;&#27573;&#65288;&#21363;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#65292;&#22870;&#21169;&#24314;&#27169;&#21644;RLHF&#65289;&#22914;&#20309;&#24433;&#21709;&#20004;&#20010;&#20851;&#38190;&#23646;&#24615;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65306;&#36229;&#20986;&#20998;&#24067;&#30340;&#27867;&#21270;&#21644;&#36755;&#20986;&#22810;&#26679;&#24615;&#12290;&#22312;&#36825;&#20123;&#27169;&#22411;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#21508;&#31181;&#24773;&#26223;&#30340;&#32972;&#26223;&#19979;&#65292;&#36229;&#20986;&#20998;&#24067;&#30340;&#27867;&#21270;&#38750;&#24120;&#37325;&#35201;&#65292;&#32780;&#36755;&#20986;&#22810;&#26679;&#24615;&#25351;&#30340;&#26159;&#27169;&#22411;&#29983;&#25104;&#21508;&#31181;&#19981;&#21516;&#36755;&#20986;&#30340;&#33021;&#21147;&#65292;&#23545;&#20110;&#21508;&#31181;&#29992;&#20363;&#26469;&#35828;&#37117;&#38750;&#24120;&#37325;&#35201;&#12290;&#25105;&#20204;&#22312;&#25688;&#35201;&#21644;&#25351;&#20196;&#36981;&#24490;&#20219;&#21153;&#20013;&#23545;&#20004;&#20010;&#22522;&#26412;&#27169;&#22411;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#21518;&#32773;&#38750;&#24120;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) fine-tuned with reinforcement learning from human feedback (RLHF) have been used in some of the most widely deployed AI models to date, such as OpenAI's ChatGPT or Anthropic's Claude. % , or Meta's LLaMA-2. While there has been significant work developing these methods, our understanding of the benefits and downsides of each stage in RLHF is still limited. To fill this gap, we present an extensive analysis of how each stage of the process (i.e.~supervised fine-tuning (SFT), reward modelling, and RLHF) affects two key properties: out-of-distribution (OOD) generalisation and output diversity. OOD generalisation is crucial given the wide range of real-world scenarios in which these models are being used, while output diversity refers to the model's ability to generate varied outputs and is important for a variety of use cases. We perform our analysis across two base models on both summarisation and instruction following tasks, the latter being highly relevant 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#32593;&#32476;&#22823;&#23567;&#21644;L2&#27491;&#21017;&#21270;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#35266;&#23519;&#21040;&#20102;&#21452;&#19979;&#38477;&#29616;&#35937;&#12290;&#36890;&#36807;&#20351;&#29992;&#38543;&#26426;&#29305;&#24449;&#21644;&#25042;&#24816;&#35757;&#32451;&#31574;&#30053;&#65292;&#22312;&#21442;&#25968;&#21644;&#29366;&#24577;&#25968;&#26080;&#38480;&#22823;&#30340;&#24773;&#20917;&#19979;&#30740;&#31350;&#20102;&#27491;&#21017;&#21270;&#30340;&#26368;&#23567;&#20108;&#20056;&#26102;&#38388;&#24046;&#20998;&#31639;&#27861;&#65292;&#24471;&#20986;&#20102;&#20854;&#25910;&#25947;&#24615;&#21644;&#26368;&#20248;&#24615;&#65292;&#24182;&#38416;&#36848;&#20102;&#21452;&#19979;&#38477;&#29616;&#35937;&#22312;&#35813;&#31639;&#27861;&#20013;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.05518</link><description>&lt;p&gt;
&#20851;&#20110;&#20351;&#29992;LSTD&#21644;&#38543;&#26426;&#29305;&#24449;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21452;&#19979;&#38477;&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;
On Double-Descent in Reinforcement Learning with LSTD and Random Features. (arXiv:2310.05518v1 [cs.LG] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05518
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#32593;&#32476;&#22823;&#23567;&#21644;L2&#27491;&#21017;&#21270;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#35266;&#23519;&#21040;&#20102;&#21452;&#19979;&#38477;&#29616;&#35937;&#12290;&#36890;&#36807;&#20351;&#29992;&#38543;&#26426;&#29305;&#24449;&#21644;&#25042;&#24816;&#35757;&#32451;&#31574;&#30053;&#65292;&#22312;&#21442;&#25968;&#21644;&#29366;&#24577;&#25968;&#26080;&#38480;&#22823;&#30340;&#24773;&#20917;&#19979;&#30740;&#31350;&#20102;&#27491;&#21017;&#21270;&#30340;&#26368;&#23567;&#20108;&#20056;&#26102;&#38388;&#24046;&#20998;&#31639;&#27861;&#65292;&#24471;&#20986;&#20102;&#20854;&#25910;&#25947;&#24615;&#21644;&#26368;&#20248;&#24615;&#65292;&#24182;&#38416;&#36848;&#20102;&#21452;&#19979;&#38477;&#29616;&#35937;&#22312;&#35813;&#31639;&#27861;&#20013;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24046;&#20998;&#31639;&#27861;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20854;&#24615;&#33021;&#21463;&#31070;&#32463;&#32593;&#32476;&#22823;&#23567;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#22312;&#30417;&#30563;&#23398;&#20064;&#20013;&#36807;&#21442;&#25968;&#21270;&#21644;&#20854;&#24102;&#26469;&#30340;&#22909;&#22788;&#24050;&#32463;&#24471;&#21040;&#20102;&#24456;&#22909;&#30340;&#29702;&#35299;&#65292;&#20294;&#26159;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#24773;&#20917;&#21017;&#19981;&#22826;&#28165;&#26970;&#12290;&#26412;&#25991;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#25506;&#35752;&#20102;&#32593;&#32476;&#22823;&#23567;&#21644;L2&#27491;&#21017;&#21270;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#23558;&#21442;&#25968;&#20010;&#25968;&#19982;&#35775;&#38382;&#29366;&#24577;&#20010;&#25968;&#20043;&#27604;&#23450;&#20041;&#20026;&#20851;&#38190;&#22240;&#32032;&#65292;&#24403;&#35813;&#27604;&#20540;&#22823;&#20110;1&#26102;&#31216;&#20026;&#36807;&#21442;&#25968;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20102;&#21452;&#19979;&#38477;&#29616;&#35937;&#65292;&#21363;&#22312;&#21442;&#25968;/&#29366;&#24577;&#27604;&#20026;1&#38468;&#36817;&#20250;&#31361;&#28982;&#24615;&#33021;&#19979;&#38477;&#12290;&#36890;&#36807;&#21033;&#29992;&#38543;&#26426;&#29305;&#24449;&#21644;&#25042;&#24816;&#35757;&#32451;&#31574;&#30053;&#65292;&#25105;&#20204;&#22312;&#26080;&#38480;&#22823;&#30340;&#21442;&#25968;&#21644;&#29366;&#24577;&#25968;&#19979;&#30740;&#31350;&#20102;&#27491;&#21017;&#21270;&#30340;&#26368;&#23567;&#20108;&#20056;&#26102;&#38388;&#24046;&#20998;&#31639;&#27861;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#20854;&#25910;&#25947;&#24615;&#21644;&#26368;&#20248;&#24615;&#65292;&#24182;&#38416;&#36848;&#20102;&#21452;&#19979;&#38477;&#29616;&#35937;&#22312;&#35813;&#31639;&#27861;&#20013;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal Difference (TD) algorithms are widely used in Deep Reinforcement Learning (RL). Their performance is heavily influenced by the size of the neural network. While in supervised learning, the regime of over-parameterization and its benefits are well understood, the situation in RL is much less clear. In this paper, we present a theoretical analysis of the influence of network size and $l_2$-regularization on performance. We identify the ratio between the number of parameters and the number of visited states as a crucial factor and define over-parameterization as the regime when it is larger than one. Furthermore, we observe a double-descent phenomenon, i.e., a sudden drop in performance around the parameter/state ratio of one. Leveraging random features and the lazy training regime, we study the regularized Least-Square Temporal Difference (LSTD) algorithm in an asymptotic regime, as both the number of parameters and states go to infinity, maintaining a constant ratio. We derive 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Entropy-MCMC&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#36741;&#21161;&#30340;&#24341;&#23548;&#21464;&#37327;&#26469;&#22312;&#24179;&#22374;&#30406;&#22320;&#20013;&#36827;&#34892;&#37319;&#26679;&#65292;&#20197;&#35299;&#20915;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21518;&#39564;&#20998;&#24067;&#30340;&#22810;&#27169;&#24577;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.05401</link><description>&lt;p&gt;
Entropy-MCMC: &#36731;&#26494;&#20174;&#24179;&#22374;&#30406;&#22320;&#36827;&#34892;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Entropy-MCMC: Sampling from Flat Basins with Ease. (arXiv:2310.05401v1 [cs.LG] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05401
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Entropy-MCMC&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#36741;&#21161;&#30340;&#24341;&#23548;&#21464;&#37327;&#26469;&#22312;&#24179;&#22374;&#30406;&#22320;&#20013;&#36827;&#34892;&#37319;&#26679;&#65292;&#20197;&#35299;&#20915;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21518;&#39564;&#20998;&#24067;&#30340;&#22810;&#27169;&#24577;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#20381;&#36182;&#20110;&#23545;&#21518;&#39564;&#20998;&#24067;&#30340;&#36136;&#37327;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21518;&#39564;&#20998;&#24067;&#22312;&#24615;&#36136;&#19978;&#26159;&#39640;&#24230;&#22810;&#27169;&#24577;&#30340;&#65292;&#23616;&#37096;&#27169;&#24335;&#34920;&#29616;&#20986;&#19981;&#21516;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#22312;&#26377;&#38480;&#30340;&#35745;&#31639;&#36164;&#28304;&#19979;&#65292;&#20174;&#21407;&#22987;&#21518;&#39564;&#20998;&#24067;&#20013;&#36827;&#34892;&#37319;&#26679;&#21487;&#33021;&#20250;&#23548;&#33268;&#27425;&#20248;&#24615;&#33021;&#65292;&#22240;&#20026;&#19968;&#20123;&#26679;&#26412;&#21487;&#33021;&#20250;&#38519;&#20837;&#8220;&#22351;&#8221;&#27169;&#24335;&#24182;&#20986;&#29616;&#36807;&#25311;&#21512;&#12290;&#22522;&#20110;&#35266;&#23519;&#21040;&#20302;&#27867;&#21270;&#35823;&#24046;&#30340;&#8220;&#22909;&#8221;&#27169;&#24335;&#36890;&#24120;&#23384;&#22312;&#20110;&#33021;&#37327;&#26223;&#35266;&#30340;&#24179;&#22374;&#30406;&#22320;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#20559;&#32622;&#37319;&#26679;&#26397;&#21521;&#36825;&#20123;&#24179;&#22374;&#21306;&#22495;&#30340;&#21518;&#39564;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#36741;&#21161;&#24341;&#23548;&#21464;&#37327;&#65292;&#20854;&#31283;&#24577;&#20998;&#24067;&#31867;&#20284;&#20110;&#24179;&#28369;&#21518;&#39564;&#20998;&#24067;&#65292;&#24182;&#19988;&#27809;&#26377;&#23574;&#38160;&#30340;&#27169;&#24577;&#65292;&#20197;&#24341;&#23548;MCMC&#37319;&#26679;&#22120;&#22312;&#24179;&#22374;&#30340;&#30406;&#22320;&#20013;&#37319;&#26679;&#12290;&#36890;&#36807;&#23558;&#27492;&#24341;&#23548;&#21464;&#37327;&#19982;&#27169;&#22411;&#21442;&#25968;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#32852;&#21512;&#20998;&#24067;&#65292;&#21487;&#20197;&#22312;&#26368;&#23567;&#35745;&#31639;&#24320;&#38144;&#19979;&#23454;&#29616;&#39640;&#25928;&#37319;&#26679;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#20803;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian deep learning counts on the quality of posterior distribution estimation. However, the posterior of deep neural networks is highly multi-modal in nature, with local modes exhibiting varying generalization performance. Given a practical budget, sampling from the original posterior can lead to suboptimal performance, as some samples may become trapped in "bad" modes and suffer from overfitting. Leveraging the observation that "good" modes with low generalization error often reside in flat basins of the energy landscape, we propose to bias sampling on the posterior toward these flat regions. Specifically, we introduce an auxiliary guiding variable, the stationary distribution of which resembles a smoothed posterior free from sharp modes, to lead the MCMC sampler to flat basins. By integrating this guiding variable with the model parameter, we create a simple joint distribution that enables efficient sampling with minimal computational overhead. We prove the convergence of our met
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#36890;&#36807;&#31232;&#30095;&#24615;&#20998;&#26512;LLM&#39044;&#35757;&#32451;&#26435;&#37325;&#30340;&#20219;&#21153;&#20013;&#24515;&#35282;&#24230;&#65292;&#25361;&#25112;&#20102;&#20256;&#32479;&#23545;&#20110;&#26435;&#37325;&#20013;&#20887;&#20313;&#24615;&#30340;&#35266;&#28857;&#65292;&#24182;&#25552;&#20986;&#20102;"&#22403;&#22334;DNA&#20551;&#35774;"&#12290;</title><link>http://arxiv.org/abs/2310.02277</link><description>&lt;p&gt;
"&#22403;&#22334;DNA&#20551;&#35774;&#65306;&#36890;&#36807;&#31232;&#30095;&#24615;&#23545;LLM&#39044;&#35757;&#32451;&#26435;&#37325;&#36827;&#34892;&#20219;&#21153;&#20013;&#24515;&#35282;&#24230;&#20998;&#26512;"
&lt;/p&gt;
&lt;p&gt;
Junk DNA Hypothesis: A Task-Centric Angle of LLM Pre-trained Weights through Sparsity. (arXiv:2310.02277v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02277
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#36890;&#36807;&#31232;&#30095;&#24615;&#20998;&#26512;LLM&#39044;&#35757;&#32451;&#26435;&#37325;&#30340;&#20219;&#21153;&#20013;&#24515;&#35282;&#24230;&#65292;&#25361;&#25112;&#20102;&#20256;&#32479;&#23545;&#20110;&#26435;&#37325;&#20013;&#20887;&#20313;&#24615;&#30340;&#35266;&#28857;&#65292;&#24182;&#25552;&#20986;&#20102;"&#22403;&#22334;DNA&#20551;&#35774;"&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#23545;"&#22403;&#22334;DNA"&#30340;&#27010;&#24565;&#38271;&#26399;&#20197;&#26469;&#19982;&#20154;&#31867;&#22522;&#22240;&#32452;&#20013;&#30340;&#38750;&#32534;&#30721;&#29255;&#27573;&#30456;&#20851;&#32852;&#65292;&#21344;&#20854;&#32452;&#25104;&#30340;&#22823;&#32422;98%&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#19968;&#20123;&#36825;&#20123;&#30475;&#20284;&#26080;&#21151;&#33021;&#30340;DNA&#24207;&#21015;&#22312;&#32454;&#32990;&#36807;&#31243;&#20013;&#36215;&#21040;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#26435;&#37325;&#19982;&#20154;&#31867;&#22522;&#22240;&#20013;&#35266;&#23519;&#21040;&#30340;&#20887;&#20313;&#24615;&#26377;&#30528;&#26174;&#33879;&#30340;&#30456;&#20284;&#24615;&#12290;&#20154;&#20204;&#35748;&#20026;&#65292;&#24222;&#22823;&#27169;&#22411;&#20013;&#30340;&#26435;&#37325;&#21253;&#21547;&#20102;&#36807;&#22810;&#30340;&#20887;&#20313;&#65292;&#21487;&#20197;&#22312;&#19981;&#24433;&#21709;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#21435;&#38500;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#20196;&#20154;&#20449;&#26381;&#30340;&#21453;&#35770;&#26469;&#25361;&#25112;&#36825;&#20010;&#20256;&#32479;&#35266;&#28857;&#12290;&#25105;&#20204;&#20351;&#29992;&#31232;&#30095;&#24615;&#20316;&#20026;&#19968;&#31181;&#24037;&#20855;&#65292;&#26469;&#29420;&#31435;&#32780;&#20934;&#30830;&#22320;&#37327;&#21270;&#39044;&#35757;&#32451;&#22823;&#35821;&#35328;&#27169;&#22411;(LLM)&#20013;&#20302;&#24133;&#24230;&#26435;&#37325;&#30340;&#32454;&#24494;&#37325;&#35201;&#24615;&#65292;&#20174;&#19979;&#28216;&#20219;&#21153;&#20013;&#24515;&#30340;&#35282;&#24230;&#29702;&#35299;&#23427;&#20204;&#21253;&#21547;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#25903;&#25345;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#30340;"&#22403;&#22334;DNA&#20551;&#35774;"&#12290;
&lt;/p&gt;
&lt;p&gt;
The traditional notion of "Junk DNA" has long been linked to non-coding segments within the human genome, constituting roughly 98% of its composition. However, recent research has unveiled the critical roles some of these seemingly non-functional DNA sequences play in cellular processes. Intriguingly, the weights within deep neural networks exhibit a remarkable similarity to the redundancy observed in human genes. It was believed that weights in gigantic models contained excessive redundancy, and could be removed without compromising performance. This paper challenges this conventional wisdom by presenting a compelling counter-argument. We employ sparsity as a tool to isolate and quantify the nuanced significance of low-magnitude weights in pre-trained large language models (LLMs). Our study demonstrates a strong correlation between these weight magnitudes and the knowledge they encapsulate, from a downstream task-centric angle. we raise the "Junk DNA Hypothesis" backed by our in-depth
&lt;/p&gt;</description></item><item><title>&#33258;&#32534;&#30721;&#22120;&#26159;&#19968;&#31181;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#35843;&#25972;&#21442;&#25968;&#23454;&#29616;&#36755;&#20837;&#25968;&#25454;&#21644;&#37325;&#26500;&#36755;&#20986;&#20043;&#38388;&#30340;&#26368;&#23567;&#24046;&#24322;&#65292;&#29992;&#20110;&#35782;&#21035;&#25968;&#25454;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#30340;&#20869;&#22312;&#32500;&#24230;&#65292;&#24182;&#19988;&#23545;&#20110;&#26576;&#20123;&#25299;&#25169;&#32467;&#26500;&#65292;&#23384;&#22312;&#38590;&#20197;&#25214;&#21040;&#23436;&#32654;&#37325;&#26500;&#32593;&#32476;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2310.02250</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;&#33258;&#32534;&#30721;&#22120;&#36215;&#20316;&#29992;&#65311;
&lt;/p&gt;
&lt;p&gt;
Why do autoencoders work?. (arXiv:2310.02250v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02250
&lt;/p&gt;
&lt;p&gt;
&#33258;&#32534;&#30721;&#22120;&#26159;&#19968;&#31181;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#35843;&#25972;&#21442;&#25968;&#23454;&#29616;&#36755;&#20837;&#25968;&#25454;&#21644;&#37325;&#26500;&#36755;&#20986;&#20043;&#38388;&#30340;&#26368;&#23567;&#24046;&#24322;&#65292;&#29992;&#20110;&#35782;&#21035;&#25968;&#25454;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#30340;&#20869;&#22312;&#32500;&#24230;&#65292;&#24182;&#19988;&#23545;&#20110;&#26576;&#20123;&#25299;&#25169;&#32467;&#26500;&#65292;&#23384;&#22312;&#38590;&#20197;&#25214;&#21040;&#23436;&#32654;&#37325;&#26500;&#32593;&#32476;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#33258;&#32534;&#30721;&#22120;&#34987;&#24191;&#27867;&#29992;&#20110;&#27169;&#22411;&#21387;&#32553;&#12290;&#23427;&#20204;&#21487;&#20197;&#35782;&#21035;&#25968;&#25454;&#22312;&#36755;&#20837;&#30340;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;R^n&#20013;&#65292;&#20301;&#20110;k&#32500;&#23376;&#38598;K&#20013;&#30340;&#20869;&#22312;&#32500;&#24230;&#12290;&#20854;&#22522;&#26412;&#24605;&#24819;&#26159;&#33719;&#24471;&#19968;&#20010;&#23558;R^n&#26144;&#23556;&#20026;R^k&#30340;&#32534;&#30721;&#23618;&#65288;&#31216;&#20026;&#29942;&#39048;&#23618;&#25110;&#28508;&#21464;&#37327;&#31354;&#38388;&#65289;&#65292;&#20197;&#21450;&#19968;&#20010;&#23558;R^k&#26144;&#23556;&#22238;R^n&#30340;&#35299;&#30721;&#23618;&#65292;&#20351;&#24471;&#22312;&#32452;&#21512;&#36825;&#20004;&#20010;&#26144;&#23556;&#26102;&#21487;&#20197;&#24674;&#22797;&#26469;&#33258;&#38598;&#21512;K&#30340;&#36755;&#20837;&#25968;&#25454;&#12290;&#36825;&#36890;&#36807;&#35843;&#25972;&#32593;&#32476;&#20013;&#30340;&#21442;&#25968;&#65288;&#26435;&#37325;&#65289;&#26469;&#26368;&#23567;&#21270;&#36755;&#20837;&#21644;&#37325;&#26500;&#36755;&#20986;&#20043;&#38388;&#30340;&#24046;&#24322;&#26469;&#23454;&#29616;&#12290;&#30001;&#20110;&#31070;&#32463;&#32593;&#32476;&#65288;&#20855;&#26377;&#36830;&#32493;&#28608;&#27963;&#20989;&#25968;&#65289;&#35745;&#31639;&#36830;&#32493;&#26144;&#23556;&#65292;&#23454;&#29616;&#23436;&#32654;&#37325;&#26500;&#30340;&#32593;&#32476;&#30340;&#23384;&#22312;&#23558;&#24847;&#21619;&#30528;K&#22312;R^k&#20013;&#26159;&#19968;&#20010;k&#32500;&#23376;&#38598;&#30340;&#21516;&#32986;&#65292;&#22240;&#27492;&#26126;&#26174;&#23384;&#22312;&#25299;&#25169;&#38556;&#30861;&#26469;&#23547;&#25214;&#36825;&#26679;&#30340;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural network autoencoders are routinely used computationally for model reduction. They allow recognizing the intrinsic dimension of data that lie in a $k$-dimensional subset $K$ of an input Euclidean space $\mathbb{R}^n$. The underlying idea is to obtain both an encoding layer that maps $\mathbb{R}^n$ into $\mathbb{R}^k$ (called the bottleneck layer or the space of latent variables) and a decoding layer that maps $\mathbb{R}^k$ back into $\mathbb{R}^n$, in such a way that the input data from the set $K$ is recovered when composing the two maps. This is achieved by adjusting parameters (weights) in the network to minimize the discrepancy between the input and the reconstructed output. Since neural networks (with continuous activation functions) compute continuous maps, the existence of a network that achieves perfect reconstruction would imply that $K$ is homeomorphic to a $k$-dimensional subset of $\mathbb{R}^k$, so clearly there are topological obstructions to finding such a ne
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#36234;&#29087;&#24713;&#29305;&#24449;&#30340;&#28145;&#24230;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21487;&#35299;&#37322;&#24615;&#22312;&#36755;&#20837;&#31354;&#38388;&#20013;&#25429;&#25417;&#26032;&#39062;&#29305;&#24449;&#26469;&#36991;&#20813;&#20551;&#38452;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#24191;&#27867;&#30340;&#24322;&#24120;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#28040;&#38500;&#20102;&#26114;&#36149;&#30340;&#32972;&#26223;&#27169;&#22411;&#21644;&#23494;&#38598;&#21305;&#37197;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2310.00797</link><description>&lt;p&gt;
&#36229;&#36234;&#29087;&#24713;&#29305;&#24449;&#30340;&#28145;&#24230;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Going Beyond Familiar Features for Deep Anomaly Detection. (arXiv:2310.00797v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00797
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#36234;&#29087;&#24713;&#29305;&#24449;&#30340;&#28145;&#24230;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21487;&#35299;&#37322;&#24615;&#22312;&#36755;&#20837;&#31354;&#38388;&#20013;&#25429;&#25417;&#26032;&#39062;&#29305;&#24449;&#26469;&#36991;&#20813;&#20551;&#38452;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#24191;&#27867;&#30340;&#24322;&#24120;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#28040;&#38500;&#20102;&#26114;&#36149;&#30340;&#32972;&#26223;&#27169;&#22411;&#21644;&#23494;&#38598;&#21305;&#37197;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#65288;AD&#65289;&#26159;&#19968;&#39033;&#37325;&#35201;&#20219;&#21153;&#65292;&#28041;&#21450;&#35782;&#21035;&#19981;&#31526;&#21512;&#24050;&#23398;&#20064;&#30340;&#27491;&#24120;&#27169;&#22411;&#30340;&#35266;&#23519;&#32467;&#26524;&#12290;&#20043;&#21069;&#30340;&#28145;&#24230;AD&#24037;&#20316;&#20027;&#35201;&#22522;&#20110;&#29087;&#24713;&#24615;&#20551;&#35774;&#65292;&#22312;&#39044;&#35757;&#32451;&#30340;&#23884;&#20837;&#31354;&#38388;&#20013;&#20351;&#29992;&#29087;&#24713;&#29305;&#24449;&#20316;&#20026;&#21442;&#32771;&#12290;&#34429;&#28982;&#36825;&#31181;&#31574;&#30053;&#24050;&#34987;&#35777;&#26126;&#38750;&#24120;&#25104;&#21151;&#65292;&#20294;&#23454;&#38469;&#19978;&#65292;&#22312;&#24322;&#24120;&#21253;&#21547;&#26410;&#34987;&#39044;&#35757;&#32451;&#32534;&#30721;&#24456;&#22909;&#25429;&#25417;&#21040;&#30340;&#20840;&#26032;&#29305;&#24449;&#26102;&#65292;&#23427;&#20250;&#23548;&#33268;&#25345;&#32493;&#30340;&#20551;&#38452;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;AD&#26041;&#27861;&#65292;&#21033;&#29992;&#21487;&#35299;&#37322;&#24615;&#22312;&#36755;&#20837;&#31354;&#38388;&#20013;&#25429;&#25417;&#26032;&#39062;&#29305;&#24449;&#20316;&#20026;&#26410;&#35299;&#37322;&#30340;&#35266;&#23519;&#32467;&#26524;&#12290;&#36890;&#36807;&#22312;&#28151;&#21512;&#26041;&#27861;&#20013;&#32467;&#21512;&#30456;&#20284;&#24615;&#21644;&#26032;&#39062;&#24615;&#65292;&#25105;&#20204;&#22312;&#24191;&#27867;&#30340;&#24322;&#24120;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#24456;&#24378;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#24314;&#31435;&#20102;&#26032;&#30340;&#26368;&#26032;&#25216;&#26415;&#65292;&#22788;&#29702;&#22810;&#26679;&#21270;&#30340;&#24322;&#24120;&#31867;&#22411;&#65292;&#21516;&#26102;&#28040;&#38500;&#20102;&#26114;&#36149;&#30340;&#32972;&#26223;&#27169;&#22411;&#21644;&#23494;&#38598;&#21305;&#37197;&#30340;&#38656;&#27714;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#32771;&#34385;&#26032;&#39062;&#29305;&#24449;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#35299;&#20915;&#20808;&#21069;&#30340;&#20551;&#38452;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly Detection (AD) is a critical task that involves identifying observations that do not conform to a learned model of normality. Prior work in deep AD is predominantly based on a familiarity hypothesis, where familiar features serve as the reference in a pre-trained embedding space. While this strategy has proven highly successful, it turns out that it causes consistent false negatives when anomalies consist of truly novel features that are not well captured by the pre-trained encoding. We propose a novel approach to AD using explainability to capture novel features as unexplained observations in the input space. We achieve strong performance across a wide range of anomaly benchmarks by combining similarity and novelty in a hybrid approach. Our approach establishes a new state-of-the-art across multiple benchmarks, handling diverse anomaly types while eliminating the need for expensive background models and dense matching. In particular, we show that by taking account of novel fea
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#39640;&#20998;&#36776;&#29575;&#24433;&#20687;&#21644;&#28151;&#21512;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22270;&#20687;&#25968;&#25454;&#20013;&#30340;&#29305;&#24449;&#25552;&#21462;&#21644;&#27169;&#24335;&#26816;&#27979;&#65292;&#20272;&#35745;&#20102;&#20010;&#21035;&#31038;&#21306;&#30340;&#20154;&#21475;&#23494;&#24230;&#12289;&#23478;&#24237;&#25910;&#20837;&#20013;&#20301;&#25968;&#21644;&#25945;&#32946;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.16808</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#31890;&#24230;&#65306;&#21033;&#29992;&#39640;&#20998;&#36776;&#29575;&#27491;&#23556;&#24433;&#20687;&#21644;&#28151;&#21512;&#23398;&#20064;&#20272;&#35745;&#23621;&#27665;&#31038;&#21306;&#30340;&#31119;&#31049;
&lt;/p&gt;
&lt;p&gt;
Granularity at Scale: Estimating Neighborhood Well-Being from High-Resolution Orthographic Imagery and Hybrid Learning. (arXiv:2309.16808v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16808
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#39640;&#20998;&#36776;&#29575;&#24433;&#20687;&#21644;&#28151;&#21512;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22270;&#20687;&#25968;&#25454;&#20013;&#30340;&#29305;&#24449;&#25552;&#21462;&#21644;&#27169;&#24335;&#26816;&#27979;&#65292;&#20272;&#35745;&#20102;&#20010;&#21035;&#31038;&#21306;&#30340;&#20154;&#21475;&#23494;&#24230;&#12289;&#23478;&#24237;&#25910;&#20837;&#20013;&#20301;&#25968;&#21644;&#25945;&#32946;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#29616;&#26377;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#19990;&#30028;&#19978;&#35768;&#22810;&#22320;&#21306;&#32570;&#20047;&#26377;&#20851;&#23621;&#27665;&#31119;&#31049;&#30340;&#22522;&#26412;&#20449;&#24687;&#12290;&#36890;&#36807;&#36965;&#24863;&#33719;&#21462;&#30340;&#39640;&#31354;&#24433;&#20687;&#65292;&#22914;&#21355;&#26143;&#25110;&#39134;&#26426;&#65292;&#21487;&#20197;&#20316;&#20026;&#31397;&#35270;&#22320;&#38754;&#19978;&#29983;&#27963;&#29366;&#20917;&#30340;&#31383;&#21475;&#65292;&#24182;&#24110;&#21161;&#22635;&#34917;&#31038;&#21306;&#20449;&#24687;&#31232;&#32570;&#30340;&#22320;&#26041;&#65292;&#36739;&#23567;&#22320;&#29702;&#23610;&#24230;&#30340;&#20272;&#35745;&#38656;&#35201;&#26356;&#39640;&#20998;&#36776;&#29575;&#30340;&#20256;&#24863;&#22120;&#12290;&#38543;&#30528;&#20256;&#24863;&#22120;&#20998;&#36776;&#29575;&#30340;&#25552;&#39640;&#65292;&#26426;&#22120;&#23398;&#20064;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#26368;&#26032;&#36827;&#23637;&#20351;&#24471;&#33021;&#22815;&#24555;&#36895;&#20174;&#22270;&#20687;&#25968;&#25454;&#20013;&#25552;&#21462;&#29305;&#24449;&#24182;&#26816;&#27979;&#27169;&#24335;&#65292;&#20174;&#32780;&#23558;&#36825;&#20123;&#29305;&#24449;&#19982;&#20854;&#20182;&#20449;&#24687;&#30456;&#20851;&#32852;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20004;&#31181;&#26041;&#27861;&#65288;&#30417;&#30563;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#22522;&#20110;&#35270;&#35273;&#35789;&#34955;&#30340;&#21322;&#30417;&#30563;&#32858;&#31867;&#65289;&#22914;&#20309;&#20174;&#20844;&#24320;&#21487;&#29992;&#30340;&#39640;&#20998;&#36776;&#29575;&#24433;&#20687;&#20013;&#20272;&#35745;&#20010;&#21035;&#31038;&#21306;&#30340;&#20154;&#21475;&#23494;&#24230;&#12289;&#23478;&#24237;&#25910;&#20837;&#20013;&#20301;&#25968;&#21644;&#25945;&#32946;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many areas of the world are without basic information on the well-being of the residing population due to limitations in existing data collection methods. Overhead images obtained remotely, such as from satellite or aircraft, can help serve as windows into the state of life on the ground and help "fill in the gaps" where community information is sparse, with estimates at smaller geographic scales requiring higher resolution sensors. Concurrent with improved sensor resolutions, recent advancements in machine learning and computer vision have made it possible to quickly extract features from and detect patterns in image data, in the process correlating these features with other information. In this work, we explore how well two approaches, a supervised convolutional neural network and semi-supervised clustering based on bag-of-visual-words, estimate population density, median household income, and educational attainment of individual neighborhoods from publicly available high-resolution 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#21435;&#22122;&#20013;&#30340;&#21518;&#39564;&#20998;&#24067;&#21450;&#20854;&#19982;&#21518;&#39564;&#22343;&#20540;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#24212;&#29992;&#20110;&#39044;&#35757;&#32451;&#21435;&#22122;&#22120;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#35745;&#31639;&#21518;&#39564;&#20998;&#24067;&#20027;&#25104;&#20998;&#21644;&#36817;&#20284;&#36793;&#38469;&#20998;&#24067;&#30340;&#26041;&#27861;&#12290;&#19981;&#38656;&#35201;&#26174;&#24335;&#35745;&#31639;&#39640;&#38454;&#30697;&#24352;&#37327;&#25110;&#36827;&#34892;&#35757;&#32451;&#25110;&#24494;&#35843;&#12290;</title><link>http://arxiv.org/abs/2309.13598</link><description>&lt;p&gt;
&#20851;&#20110;&#21435;&#22122;&#20013;&#30340;&#21518;&#39564;&#20998;&#24067;&#65306;&#22312;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
On the Posterior Distribution in Denoising: Application to Uncertainty Quantification. (arXiv:2309.13598v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13598
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#21435;&#22122;&#20013;&#30340;&#21518;&#39564;&#20998;&#24067;&#21450;&#20854;&#19982;&#21518;&#39564;&#22343;&#20540;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#24212;&#29992;&#20110;&#39044;&#35757;&#32451;&#21435;&#22122;&#22120;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#35745;&#31639;&#21518;&#39564;&#20998;&#24067;&#20027;&#25104;&#20998;&#21644;&#36817;&#20284;&#36793;&#38469;&#20998;&#24067;&#30340;&#26041;&#27861;&#12290;&#19981;&#38656;&#35201;&#26174;&#24335;&#35745;&#31639;&#39640;&#38454;&#30697;&#24352;&#37327;&#25110;&#36827;&#34892;&#35757;&#32451;&#25110;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21435;&#22122;&#31639;&#27861;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#36215;&#30528;&#26680;&#24515;&#20316;&#29992;&#65292;&#20174;&#38477;&#22122;&#20302;&#32423;&#21035;&#25104;&#20687;&#20256;&#24863;&#22120;&#21040;&#25552;&#21319;&#22522;&#20110;&#35780;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#21518;&#19968;&#31867;&#26041;&#27861;&#20351;&#29992;Tweedie&#20844;&#24335;&#65292;&#23558;&#39640;&#26031;&#21435;&#22122;&#30340;&#21518;&#39564;&#22343;&#20540;&#65288;&#21363;&#26368;&#23567;&#22343;&#26041;&#35823;&#24046;&#21435;&#22122;&#22120;&#65289;&#19982;&#25968;&#25454;&#20998;&#24067;&#30340;&#35780;&#20998;&#38142;&#25509;&#36215;&#26469;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#25512;&#23548;&#20102;&#21518;&#39564;&#20998;&#24067;&#30340;&#39640;&#38454;&#20013;&#24515;&#30697;&#19982;&#21518;&#39564;&#22343;&#20540;&#30340;&#39640;&#38454;&#23548;&#25968;&#20043;&#38388;&#30340;&#22522;&#26412;&#20851;&#31995;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#32467;&#26524;&#36827;&#34892;&#39044;&#35757;&#32451;&#21435;&#22122;&#22120;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#39640;&#25928;&#35745;&#31639;&#22270;&#20687;&#20219;&#20309;&#25152;&#38656;&#21306;&#22495;&#30340;&#21518;&#39564;&#20998;&#24067;&#30340;&#20027;&#25104;&#20998;&#65292;&#20197;&#21450;&#22914;&#20309;&#36817;&#20284;&#27839;&#36825;&#20123;&#65288;&#25110;&#20219;&#20309;&#20854;&#20182;&#65289;&#19968;&#32500;&#26041;&#21521;&#30340;&#23436;&#25972;&#36793;&#38469;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24555;&#36895;&#19988;&#20869;&#23384;&#39640;&#25928;&#65292;&#22240;&#20026;&#23427;&#19981;&#38656;&#35201;&#26174;&#24335;&#35745;&#31639;&#25110;&#23384;&#20648;&#39640;&#38454;&#30697;&#24352;&#37327;&#65292;&#24182;&#19988;&#26080;&#38656;&#35757;&#32451;&#25110;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
Denoisers play a central role in many applications, from noise suppression in low-grade imaging sensors, to empowering score-based generative models. The latter category of methods makes use of Tweedie's formula, which links the posterior mean in Gaussian denoising (i.e., the minimum MSE denoiser) with the score of the data distribution. Here, we derive a fundamental relation between the higher-order central moments of the posterior distribution, and the higher-order derivatives of the posterior mean. We harness this result for uncertainty quantification of pre-trained denoisers. Particularly, we show how to efficiently compute the principal components of the posterior distribution for any desired region of an image, as well as to approximate the full marginal distribution along those (or any other) one-dimensional directions. Our method is fast and memory efficient, as it does not explicitly compute or store the high-order moment tensors and it requires no training or fine tuning of t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;3D-U-SAM&#32593;&#32476;&#65292;&#29992;&#20110;&#23569;&#26679;&#26412;CBCT&#22270;&#20687;&#30340;&#29273;&#40831;&#20998;&#21106;&#12290;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;SAM&#21644;&#21367;&#31215;&#36924;&#36817;&#26041;&#27861;&#65292;&#20197;&#21450;&#36339;&#36291;&#36830;&#25509;&#34701;&#21512;&#29305;&#24449;&#65292;&#26412;&#26041;&#27861;&#22312;&#35299;&#20915;&#23567;&#26679;&#26412;&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.11015</link><description>&lt;p&gt;
&#36890;&#36807;3D-U-SAM&#32593;&#32476;&#36827;&#34892;&#23569;&#26679;&#26412;CBCT&#22270;&#20687;&#30340;&#29273;&#40831;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
3D-U-SAM Network For Few-shot Tooth Segmentation in CBCT Images. (arXiv:2309.11015v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11015
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;3D-U-SAM&#32593;&#32476;&#65292;&#29992;&#20110;&#23569;&#26679;&#26412;CBCT&#22270;&#20687;&#30340;&#29273;&#40831;&#20998;&#21106;&#12290;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;SAM&#21644;&#21367;&#31215;&#36924;&#36817;&#26041;&#27861;&#65292;&#20197;&#21450;&#36339;&#36291;&#36830;&#25509;&#34701;&#21512;&#29305;&#24449;&#65292;&#26412;&#26041;&#27861;&#22312;&#35299;&#20915;&#23567;&#26679;&#26412;&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29273;&#40831;&#20301;&#32622;&#30340;&#20934;&#30830;&#34920;&#31034;&#22312;&#27835;&#30103;&#20013;&#38750;&#24120;&#37325;&#35201;&#12290;3D&#29273;&#40831;&#22270;&#20687;&#20998;&#21106;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#26041;&#27861;&#65292;&#28982;&#32780;&#26631;&#27880;&#30340;3D&#29273;&#40831;&#25968;&#25454;&#38598;&#26159;&#31232;&#32570;&#30340;&#36164;&#28304;&#65292;&#36825;&#23548;&#33268;&#20102;&#36825;&#20010;&#20219;&#21153;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#38754;&#20020;&#23567;&#26679;&#26412;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#37319;&#29992;&#20102;&#39044;&#35757;&#32451;&#30340;SAM&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;3D-U-SAM&#32593;&#32476;&#29992;&#20110;3D&#29273;&#40831;&#22270;&#20687;&#20998;&#21106;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20026;&#20102;&#35299;&#20915;&#22312;3D&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;2D&#39044;&#35757;&#32451;&#26435;&#37325;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#21367;&#31215;&#36924;&#36817;&#26041;&#27861;&#65307;&#20026;&#20102;&#20445;&#30041;&#26356;&#22810;&#32454;&#33410;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#36339;&#36291;&#36830;&#25509;&#65292;&#20197;&#21442;&#32771;U-Net&#22312;&#25152;&#26377;&#23618;&#32423;&#19978;&#34701;&#21512;&#29305;&#24449;&#12290;&#36890;&#36807;&#28040;&#34701;&#23454;&#39564;&#12289;&#23545;&#27604;&#23454;&#39564;&#21644;&#26679;&#26412;&#22823;&#23567;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate representation of tooth position is extremely important in treatment. 3D dental image segmentation is a widely used method, however labelled 3D dental datasets are a scarce resource, leading to the problem of small samples that this task faces in many cases. To this end, we address this problem with a pretrained SAM and propose a novel 3D-U-SAM network for 3D dental image segmentation. Specifically, in order to solve the problem of using 2D pre-trained weights on 3D datasets, we adopted a convolution approximation method; in order to retain more details, we designed skip connections to fuse features at all levels with reference to U-Net. The effectiveness of the proposed method is demonstrated in ablation experiments, comparison experiments, and sample size experiments.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#35299;&#32806;&#35757;&#32451;&#65288;D-Train&#65289;&#30340;&#20196;&#20154;&#27822;&#20007;&#30340;&#12289;&#26080;&#36229;&#21442;&#25968;&#30340;&#22810;&#39046;&#22495;&#23398;&#20064;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#20102;&#19968;&#31181;&#19977;&#38454;&#27573;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#39318;&#20808;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#27599;&#20010;&#39046;&#22495;&#19978;&#36827;&#34892;&#21518;&#35757;&#32451;&#65292;&#26368;&#21518;&#36827;&#34892;&#22836;&#37096;&#24494;&#35843;&#65292;&#23454;&#29616;&#35299;&#32806;&#35757;&#32451;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.10302</link><description>&lt;p&gt;
&#35299;&#32806;&#35757;&#32451;&#65306;&#20196;&#20154;&#27822;&#20007;&#30340;&#31616;&#21333;&#22810;&#39046;&#22495;&#23398;&#20064;&#30340;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Decoupled Training: Return of Frustratingly Easy Multi-Domain Learning. (arXiv:2309.10302v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10302
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#35299;&#32806;&#35757;&#32451;&#65288;D-Train&#65289;&#30340;&#20196;&#20154;&#27822;&#20007;&#30340;&#12289;&#26080;&#36229;&#21442;&#25968;&#30340;&#22810;&#39046;&#22495;&#23398;&#20064;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#20102;&#19968;&#31181;&#19977;&#38454;&#27573;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#39318;&#20808;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#27599;&#20010;&#39046;&#22495;&#19978;&#36827;&#34892;&#21518;&#35757;&#32451;&#65292;&#26368;&#21518;&#36827;&#34892;&#22836;&#37096;&#24494;&#35843;&#65292;&#23454;&#29616;&#35299;&#32806;&#35757;&#32451;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#39046;&#22495;&#23398;&#20064;&#65288;MDL&#65289;&#26088;&#22312;&#35757;&#32451;&#19968;&#20010;&#27169;&#22411;&#65292;&#22312;&#22810;&#20010;&#37325;&#21472;&#20294;&#38750;&#30456;&#21516;&#30340;&#39046;&#22495;&#20013;&#20855;&#26377;&#26368;&#23567;&#30340;&#24179;&#22343;&#39118;&#38505;&#12290;&#20026;&#20102;&#35299;&#20915;&#25968;&#25454;&#38598;&#20559;&#24046;&#21644;&#39046;&#22495;&#20248;&#21183;&#30340;&#25361;&#25112;&#65292;&#20174;&#23545;&#40784;&#20998;&#24067;&#20943;&#23569;&#39046;&#22495;&#24046;&#36317;&#30340;&#35282;&#24230;&#25110;&#36890;&#36807;&#23454;&#26045;&#39046;&#22495;&#29305;&#23450;&#30340;&#22612;&#12289;&#38376;&#29978;&#33267;&#19987;&#23478;&#26469;&#20445;&#30041;&#24046;&#24322;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;MDL&#26041;&#27861;&#12290;MDL&#27169;&#22411;&#21464;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#65292;&#20855;&#26377;&#22797;&#26434;&#30340;&#32593;&#32476;&#26550;&#26500;&#25110;&#25439;&#22833;&#20989;&#25968;&#65292;&#24341;&#20837;&#39069;&#22806;&#30340;&#21442;&#25968;&#24182;&#22686;&#21152;&#35745;&#31639;&#25104;&#26412;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20196;&#20154;&#27822;&#20007;&#30340;&#12289;&#26080;&#36229;&#21442;&#25968;&#30340;&#22810;&#39046;&#22495;&#23398;&#20064;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;&#35299;&#32806;&#35757;&#32451;&#65288;D-Train&#65289;&#12290;D-Train&#26159;&#19968;&#31181;&#19977;&#38454;&#27573;&#30340;&#20174;&#19968;&#33324;&#21040;&#29305;&#27530;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#39318;&#20808;&#22312;&#25152;&#26377;&#39046;&#22495;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#20197;&#28909;&#36523;&#19968;&#20010;&#26681;&#27169;&#22411;&#65292;&#28982;&#21518;&#36890;&#36807;&#23558;&#20854;&#25286;&#20998;&#20026;&#22810;&#20010;&#22836;&#37096;&#22312;&#27599;&#20010;&#39046;&#22495;&#19978;&#36827;&#34892;&#21518;&#35757;&#32451;&#65292;&#26368;&#21518;&#36890;&#36807;&#22266;&#23450;&#39592;&#24178;&#36827;&#34892;&#22836;&#37096;&#24494;&#35843;&#65292;&#23454;&#29616;&#35299;&#32806;&#35757;&#32451;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-domain learning (MDL) aims to train a model with minimal average risk across multiple overlapping but non-identical domains. To tackle the challenges of dataset bias and domain domination, numerous MDL approaches have been proposed from the perspectives of seeking commonalities by aligning distributions to reduce domain gap or reserving differences by implementing domain-specific towers, gates, and even experts. MDL models are becoming more and more complex with sophisticated network architectures or loss functions, introducing extra parameters and enlarging computation costs. In this paper, we propose a frustratingly easy and hyperparameter-free multi-domain learning method named Decoupled Training(D-Train). D-Train is a tri-phase general-to-specific training strategy that first pre-trains on all domains to warm up a root model, then post-trains on each domain by splitting into multi heads, and finally fine-tunes the heads by fixing the backbone, enabling decouple training to ac
&lt;/p&gt;</description></item><item><title>RaTrack&#26159;&#19968;&#31181;&#38024;&#23545;&#38647;&#36798;&#36319;&#36394;&#30340;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#36816;&#21160;&#20998;&#21106;&#21644;&#32858;&#31867;&#20197;&#21450;&#36816;&#21160;&#20272;&#35745;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#23545;&#31227;&#21160;&#29289;&#20307;&#30340;&#31934;&#30830;&#36319;&#36394;&#65292;&#20248;&#20110;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.09737</link><description>&lt;p&gt;
RaTrack: &#24102;&#26377;4D&#38647;&#36798;&#28857;&#20113;&#30340;&#36816;&#21160;&#29289;&#20307;&#26816;&#27979;&#19982;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
RaTrack: Moving Object Detection and Tracking with 4D Radar Point Cloud. (arXiv:2309.09737v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09737
&lt;/p&gt;
&lt;p&gt;
RaTrack&#26159;&#19968;&#31181;&#38024;&#23545;&#38647;&#36798;&#36319;&#36394;&#30340;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#36816;&#21160;&#20998;&#21106;&#21644;&#32858;&#31867;&#20197;&#21450;&#36816;&#21160;&#20272;&#35745;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#23545;&#31227;&#21160;&#29289;&#20307;&#30340;&#31934;&#30830;&#36319;&#36394;&#65292;&#20248;&#20110;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;&#33258;&#20027;&#24615;&#20381;&#36182;&#20110;&#23545;&#21160;&#24577;&#29615;&#22659;&#30340;&#31934;&#30830;&#24863;&#30693;&#12290;&#22312;3D&#19990;&#30028;&#20013;&#31283;&#23450;&#22320;&#36319;&#36394;&#31227;&#21160;&#29289;&#20307;&#22240;&#27492;&#23545;&#20110;&#36712;&#36857;&#39044;&#27979;&#12289;&#36991;&#38556;&#21644;&#36335;&#24452;&#35268;&#21010;&#31561;&#24212;&#29992;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#34429;&#28982;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#21033;&#29992;LiDAR&#25110;&#30456;&#26426;&#36827;&#34892;&#22810;&#30446;&#26631;&#36319;&#36394;&#65288;MOT&#65289;&#65292;&#20294;4D&#25104;&#20687;&#38647;&#36798;&#30340;&#33021;&#21147;&#20173;&#28982;&#24456;&#23569;&#34987;&#25506;&#32034;&#12290;&#35748;&#35782;&#21040;4D&#38647;&#36798;&#25968;&#25454;&#20013;&#30340;&#38647;&#36798;&#22122;&#22768;&#21644;&#28857;&#31232;&#30095;&#24615;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;RaTrack&#65292;&#36825;&#26159;&#19968;&#31181;&#19987;&#38376;&#38024;&#23545;&#22522;&#20110;&#38647;&#36798;&#30340;&#36319;&#36394;&#30340;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25682;&#24323;&#20102;&#23545;&#29305;&#23450;&#23545;&#35937;&#31867;&#22411;&#21644;3D&#36793;&#30028;&#26694;&#30340;&#20381;&#36182;&#65292;&#32780;&#26159;&#19987;&#27880;&#20110;&#36816;&#21160;&#20998;&#21106;&#21644;&#32858;&#31867;&#65292;&#24182;&#37197;&#20197;&#36816;&#21160;&#20272;&#35745;&#27169;&#22359;&#12290;&#22312;View-of-Delft&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#26102;&#65292;RaTrack&#23637;&#31034;&#20986;&#20102;&#20248;&#20110;&#26368;&#20808;&#36827;&#24615;&#33021;&#30340;&#36816;&#21160;&#29289;&#20307;&#36319;&#36394;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mobile autonomy relies on the precise perception of dynamic environments. Robustly tracking moving objects in 3D world thus plays a pivotal role for applications like trajectory prediction, obstacle avoidance, and path planning. While most current methods utilize LiDARs or cameras for Multiple Object Tracking (MOT), the capabilities of 4D imaging radars remain largely unexplored. Recognizing the challenges posed by radar noise and point sparsity in 4D radar data, we introduce RaTrack, an innovative solution tailored for radar-based tracking. Bypassing the typical reliance on specific object types and 3D bounding boxes, our method focuses on motion segmentation and clustering, enriched by a motion estimation module. Evaluated on the View-of-Delft dataset, RaTrack showcases superior tracking precision of moving objects, largely surpassing the performance of the state of the art.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#23454;&#26102;&#25512;&#29702;&#33021;&#21147;&#30340;&#27969;&#24335;&#32456;&#36523;&#23398;&#20064;&#26041;&#27861;&#65292;&#37319;&#29992;&#34394;&#25311;&#26799;&#24230;&#36827;&#34892;&#36830;&#32493;&#34920;&#31034;&#23398;&#20064;&#65292;&#20511;&#21161;&#35821;&#20041;&#35760;&#24518;&#26469;&#25233;&#21046;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#22312;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2309.08227</link><description>&lt;p&gt;
VERSE&#65306;&#20855;&#26377;&#23454;&#26102;&#25512;&#29702;&#33021;&#21147;&#30340;&#34394;&#25311;&#26799;&#24230;&#24863;&#30693;&#27969;&#36716;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
VERSE: Virtual-Gradient Aware Streaming Lifelong Learning with Anytime Inference. (arXiv:2309.08227v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08227
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#23454;&#26102;&#25512;&#29702;&#33021;&#21147;&#30340;&#27969;&#24335;&#32456;&#36523;&#23398;&#20064;&#26041;&#27861;&#65292;&#37319;&#29992;&#34394;&#25311;&#26799;&#24230;&#36827;&#34892;&#36830;&#32493;&#34920;&#31034;&#23398;&#20064;&#65292;&#20511;&#21161;&#35821;&#20041;&#35760;&#24518;&#26469;&#25233;&#21046;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#22312;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32456;&#36523;&#23398;&#20064;&#26159;&#25351;&#22312;&#35757;&#32451;AI&#20195;&#29702;&#30340;&#21516;&#26102;&#65292;&#38450;&#27490;&#20854;&#36951;&#24536;&#20197;&#21069;&#33719;&#24471;&#30340;&#30693;&#35782;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#22823;&#22810;&#20851;&#27880;&#22312;&#38745;&#24577;&#29615;&#22659;&#19979;&#30340;&#32456;&#36523;&#23398;&#20064;&#65292;&#24182;&#19988;&#32570;&#20047;&#22312;&#24555;&#36895;&#21464;&#21270;&#30340;&#21160;&#24577;&#29615;&#22659;&#20013;&#20943;&#36731;&#36951;&#24536;&#30340;&#33021;&#21147;&#12290;&#27969;&#24335;&#32456;&#36523;&#23398;&#20064;&#26159;&#32456;&#36523;&#23398;&#20064;&#20013;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35774;&#32622;&#65292;&#20854;&#30446;&#26631;&#26159;&#22312;&#21160;&#24577;&#30340;&#38750;&#24179;&#31283;&#29615;&#22659;&#20013;&#36827;&#34892;&#36830;&#32493;&#23398;&#20064;&#32780;&#19981;&#36951;&#24536;&#12290;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#32456;&#36523;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26159;&#27969;&#24335;&#30340;&#65292;&#20165;&#38656;&#35201;&#23545;&#25968;&#25454;&#36827;&#34892;&#19968;&#27425;&#36941;&#21382;&#65292;&#21487;&#20197;&#20197;&#31867;&#22686;&#37327;&#30340;&#26041;&#24335;&#23398;&#20064;&#65292;&#24182;&#19988;&#21487;&#20197;&#36827;&#34892;&#21363;&#26102;&#35780;&#20272;&#65288;&#23454;&#26102;&#25512;&#29702;&#65289;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20123;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#36830;&#32493;&#34920;&#31034;&#23398;&#20064;&#30340;&#34394;&#25311;&#26799;&#24230;&#65292;&#20197;&#38450;&#27490;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#20511;&#21161;&#22522;&#20110;&#25351;&#25968;&#31227;&#21160;&#24179;&#22343;&#30340;&#35821;&#20041;&#35760;&#24518;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lifelong learning, also referred to as continual learning, is the problem of training an AI agent continuously while also preventing it from forgetting its previously acquired knowledge. Most of the existing methods primarily focus on lifelong learning within a static environment and lack the ability to mitigate forgetting in a quickly-changing dynamic environment. Streaming lifelong learning is a challenging setting of lifelong learning with the goal of continuous learning in a dynamic non-stationary environment without forgetting. We introduce a novel approach to lifelong learning, which is streaming, requires a single pass over the data, can learn in a class-incremental manner, and can be evaluated on-the-fly (anytime inference). To accomplish these, we propose virtual gradients for continual representation learning to prevent catastrophic forgetting and leverage an exponential-moving-average-based semantic memory to further enhance performance. Extensive experiments on diverse data
&lt;/p&gt;</description></item><item><title>DePT&#36890;&#36807;&#23558;&#36719;&#25552;&#31034;&#20998;&#35299;&#20026;&#36739;&#30701;&#30340;&#36719;&#25552;&#31034;&#21644;&#19968;&#23545;&#20302;&#31209;&#30697;&#38453;&#65292;&#24182;&#29992;&#20004;&#20010;&#19981;&#21516;&#30340;&#23398;&#20064;&#29575;&#26469;&#20248;&#21270;&#65292;&#20197;&#35299;&#20915;&#25552;&#31034;&#35843;&#25972;&#23545;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#20197;&#21450;&#20869;&#23384;&#20351;&#29992;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.05173</link><description>&lt;p&gt;
DePT:&#20998;&#35299;&#25552;&#31034;&#35843;&#25972;&#20197;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning. (arXiv:2309.05173v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05173
&lt;/p&gt;
&lt;p&gt;
DePT&#36890;&#36807;&#23558;&#36719;&#25552;&#31034;&#20998;&#35299;&#20026;&#36739;&#30701;&#30340;&#36719;&#25552;&#31034;&#21644;&#19968;&#23545;&#20302;&#31209;&#30697;&#38453;&#65292;&#24182;&#29992;&#20004;&#20010;&#19981;&#21516;&#30340;&#23398;&#20064;&#29575;&#26469;&#20248;&#21270;&#65292;&#20197;&#35299;&#20915;&#25552;&#31034;&#35843;&#25972;&#23545;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#20197;&#21450;&#20869;&#23384;&#20351;&#29992;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#35843;&#25972;&#65288;PT&#65289;&#26159;&#19968;&#31181;&#23558;&#21487;&#35757;&#32451;&#30340;&#23569;&#37327;&#36719;&#25552;&#31034;&#21521;&#37327;&#38468;&#21152;&#21040;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#36755;&#20837;&#20013;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#65292;&#24050;&#22312;&#21508;&#31181;&#20219;&#21153;&#21644;&#27169;&#22411;&#20013;&#26174;&#31034;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290; &#19982;&#20854;&#20182;PEFT&#26041;&#27861;&#30456;&#27604;&#65292;PT&#30340;&#31454;&#20105;&#24615;&#33021;&#21487;&#20197;&#22312;&#21487;&#35757;&#32451;&#21442;&#25968;&#26356;&#23569;&#30340;&#24773;&#20917;&#19979;&#20445;&#25345;&#65292;&#24182;&#19988;&#38543;&#30528;&#27169;&#22411;&#35268;&#27169;&#30340;&#25193;&#22823;&#65292;&#20854;&#21442;&#25968;&#24182;&#19981;&#20250;&#26174;&#33879;&#22686;&#21152;&#12290; &#20294;&#26159;&#65292;PT&#24341;&#20837;&#20102;&#39069;&#22806;&#30340;&#36719;&#25552;&#31034;&#26631;&#35760;&#65292;&#23548;&#33268;&#36755;&#20837;&#24207;&#21015;&#21464;&#38271;&#65292;&#36825;&#23545;&#20110;Transformer&#30340;&#20108;&#27425;&#22797;&#26434;&#24230;&#32780;&#35328;&#65292;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#20197;&#21450;&#20869;&#23384;&#20351;&#29992;&#26041;&#38754;&#20250;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#12290; &#36825;&#23545;&#20110;&#38754;&#20020;&#22823;&#37327;&#27599;&#26085;&#26597;&#35810;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23588;&#20854;&#20196;&#20154;&#25285;&#24551;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt tuning (PT), where a small amount of trainable soft (continuous) prompt vectors is affixed to the input of language models (LM), has shown promising results across various tasks and models for parameter-efficient fine-tuning (PEFT). PT stands out from other PEFT approaches because it maintains competitive performance with fewer trainable parameters and does not drastically scale up its parameters as the model size expands. However, PT introduces additional soft prompt tokens, leading to longer input sequences, which significantly impacts training and inference time and memory usage due to the Transformer's quadratic complexity. Particularly concerning for Large Language Models (LLMs) that face heavy daily querying. To address this issue, we propose Decomposed Prompt Tuning (DePT), which decomposes the soft prompt into a shorter soft prompt and a pair of low-rank matrices that are then optimised with two different learning rates. This allows DePT to achieve better performance whi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#31471;&#21040;&#31471;&#21487;&#23398;&#20064;&#30340;&#28145;&#24230;&#35270;&#39057;&#32534;&#30721;&#25511;&#21046;&#26041;&#27861;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#24102;&#23485;&#38480;&#21046;&#21644;&#19979;&#28216;&#35270;&#35273;&#24615;&#33021;&#65292;&#24182;&#22312;&#19981;&#30772;&#22351;&#29616;&#26377;&#26631;&#20934;&#21270;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#20445;&#25252;&#28145;&#24230;&#35270;&#35273;&#27169;&#22411;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2308.16215</link><description>&lt;p&gt;
&#28145;&#24230;&#35270;&#39057;&#32534;&#30721;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Deep Video Codec Control. (arXiv:2308.16215v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16215
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#31471;&#21040;&#31471;&#21487;&#23398;&#20064;&#30340;&#28145;&#24230;&#35270;&#39057;&#32534;&#30721;&#25511;&#21046;&#26041;&#27861;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#24102;&#23485;&#38480;&#21046;&#21644;&#19979;&#28216;&#35270;&#35273;&#24615;&#33021;&#65292;&#24182;&#22312;&#19981;&#30772;&#22351;&#29616;&#26377;&#26631;&#20934;&#21270;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#20445;&#25252;&#28145;&#24230;&#35270;&#35273;&#27169;&#22411;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20002;&#22833;&#29575;&#35270;&#39057;&#21387;&#32553;&#36890;&#24120;&#29992;&#20110;&#20256;&#36755;&#21644;&#23384;&#20648;&#35270;&#39057;&#25968;&#25454;&#12290;&#23613;&#31649;&#23384;&#22312;&#36827;&#38454;&#65288;&#31070;&#32463;&#65289;&#21387;&#32553;&#26041;&#27861;&#65292;&#20294;&#32479;&#19968;&#35270;&#39057;&#32534;&#30721;&#22120;&#65288;&#22914;H.264&#25110;H.265&#65289;&#20173;&#28982;&#26159;&#20107;&#23454;&#19978;&#30340;&#26631;&#20934;&#12290;&#22312;&#38754;&#23545;&#21160;&#24577;&#32593;&#32476;&#24102;&#23485;&#26465;&#20214;&#30340;&#35270;&#39057;&#20256;&#36755;&#20013;&#65292;&#35270;&#39057;&#32534;&#30721;&#22120;&#38656;&#35201;&#36866;&#24212;&#38750;&#24120;&#19981;&#21516;&#30340;&#21387;&#32553;&#24378;&#24230;&#12290;&#36895;&#29575;&#25511;&#21046;&#27169;&#22359;&#22686;&#24378;&#32534;&#35299;&#30721;&#22120;&#30340;&#21387;&#32553;&#33021;&#21147;&#65292;&#20197;&#28385;&#36275;&#24102;&#23485;&#38480;&#21046;&#24182;&#23613;&#37327;&#20943;&#23569;&#35270;&#39057;&#22833;&#30495;&#12290;&#28982;&#32780;&#65292;&#26631;&#20934;&#35270;&#39057;&#32534;&#30721;&#22120;&#21450;&#20854;&#36895;&#29575;&#25511;&#21046;&#27169;&#22359;&#26159;&#20026;&#20102;&#26368;&#23567;&#21270;&#20154;&#31867;&#36136;&#37327;&#35780;&#20272;&#32780;&#24320;&#21457;&#30340;&#65292;&#21364;&#27809;&#26377;&#32771;&#34385;&#20445;&#25252;&#28145;&#24230;&#35270;&#35273;&#27169;&#22411;&#30340;&#19979;&#28216;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#31471;&#21040;&#31471;&#21487;&#23398;&#20064;&#30340;&#28145;&#24230;&#35270;&#39057;&#32534;&#30721;&#25511;&#21046;&#26041;&#27861;&#65292;&#32771;&#34385;&#20102;&#24102;&#23485;&#38480;&#21046;&#21644;&#19979;&#28216;&#35270;&#35273;&#24615;&#33021;&#65292;&#24182;&#19981;&#30772;&#22351;&#29616;&#26377;&#30340;&#26631;&#20934;&#21270;&#12290;&#25105;&#20204;&#38024;&#23545;&#20004;&#20010;&#24120;&#35265;&#30340;&#35270;&#35273;&#20219;&#21153;&#65288;&#35821;&#20041;&#20998;&#21106;...
&lt;/p&gt;
&lt;p&gt;
Lossy video compression is commonly used when transmitting and storing video data. Unified video codecs (e.g., H.264 or H.265) remain the \emph{de facto} standard, despite the availability of advanced (neural) compression approaches. Transmitting videos in the face of dynamic network bandwidth conditions requires video codecs to adapt to vastly different compression strengths. Rate control modules augment the codec's compression such that bandwidth constraints are satisfied and video distortion is minimized. While, both standard video codes and their rate control modules are developed to minimize video distortion w.r.t. human quality assessment, preserving the downstream performance of deep vision models is not considered. In this paper, we present the first end-to-end learnable deep video codec control considering both bandwidth constraints and downstream vision performance, while not breaking existing standardization. We demonstrate for two common vision tasks (semantic segmentation 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#27169;&#24577;&#23884;&#20837;&#20013;&#30340;&#23545;&#25239;&#24187;&#35273;&#38382;&#39064;&#12290;&#23545;&#25163;&#21487;&#20197;&#25200;&#21160;&#36755;&#20837;&#30340;&#20219;&#24847;&#27169;&#24577;&#65292;&#20351;&#20854;&#23884;&#20837;&#19982;&#20854;&#20182;&#27169;&#24577;&#30340;&#20219;&#24847;&#36755;&#20837;&#25509;&#36817;&#65292;&#20174;&#32780;&#23454;&#29616;&#20219;&#24847;&#22270;&#20687;&#19982;&#20219;&#24847;&#25991;&#26412;&#12289;&#20219;&#24847;&#25991;&#26412;&#19982;&#20219;&#24847;&#22768;&#38899;&#30340;&#23545;&#40784;&#12290;&#35813;&#38382;&#39064;&#19982;&#19979;&#28216;&#20219;&#21153;&#26080;&#20851;&#65292;&#23545;&#29983;&#25104;&#21644;&#20998;&#31867;&#20219;&#21153;&#20250;&#20135;&#29983;&#35823;&#23548;&#12290;</title><link>http://arxiv.org/abs/2308.11804</link><description>&lt;p&gt;
&#36825;&#19981;&#26159;&#19968;&#20010;&#33529;&#26524;&#65306;&#22810;&#27169;&#24577;&#23884;&#20837;&#20013;&#30340;&#23545;&#25239;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Ceci n'est pas une pomme: Adversarial Illusions in Multi-Modal Embeddings. (arXiv:2308.11804v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11804
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#27169;&#24577;&#23884;&#20837;&#20013;&#30340;&#23545;&#25239;&#24187;&#35273;&#38382;&#39064;&#12290;&#23545;&#25163;&#21487;&#20197;&#25200;&#21160;&#36755;&#20837;&#30340;&#20219;&#24847;&#27169;&#24577;&#65292;&#20351;&#20854;&#23884;&#20837;&#19982;&#20854;&#20182;&#27169;&#24577;&#30340;&#20219;&#24847;&#36755;&#20837;&#25509;&#36817;&#65292;&#20174;&#32780;&#23454;&#29616;&#20219;&#24847;&#22270;&#20687;&#19982;&#20219;&#24847;&#25991;&#26412;&#12289;&#20219;&#24847;&#25991;&#26412;&#19982;&#20219;&#24847;&#22768;&#38899;&#30340;&#23545;&#40784;&#12290;&#35813;&#38382;&#39064;&#19982;&#19979;&#28216;&#20219;&#21153;&#26080;&#20851;&#65292;&#23545;&#29983;&#25104;&#21644;&#20998;&#31867;&#20219;&#21153;&#20250;&#20135;&#29983;&#35823;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#32534;&#30721;&#22120;&#23558;&#22270;&#20687;&#12289;&#22768;&#38899;&#12289;&#25991;&#26412;&#12289;&#35270;&#39057;&#31561;&#26144;&#23556;&#21040;&#19968;&#20010;&#21333;&#19968;&#30340;&#23884;&#20837;&#31354;&#38388;&#20013;&#65292;&#36890;&#36807;&#23545;&#40784;&#19981;&#21516;&#27169;&#24577;&#30340;&#34920;&#31034;&#65288;&#20363;&#22914;&#23558;&#19968;&#24352;&#29399;&#30340;&#22270;&#20687;&#19982;&#19968;&#31181;&#21483;&#22768;&#30456;&#20851;&#32852;&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22810;&#27169;&#24577;&#23884;&#20837;&#21487;&#20197;&#21463;&#21040;&#19968;&#31181;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#23545;&#25239;&#24187;&#35273;&#8221;&#30340;&#25915;&#20987;&#12290;&#32473;&#23450;&#20219;&#24847;&#27169;&#24577;&#30340;&#36755;&#20837;&#65292;&#23545;&#25163;&#21487;&#20197;&#25200;&#21160;&#23427;&#65292;&#20351;&#20854;&#23884;&#20837;&#25509;&#36817;&#20110;&#21478;&#19968;&#27169;&#24577;&#20013;&#20219;&#24847;&#23545;&#25163;&#36873;&#25321;&#30340;&#36755;&#20837;&#30340;&#23884;&#20837;&#12290;&#24187;&#35273;&#20351;&#23545;&#25163;&#33021;&#22815;&#23558;&#20219;&#24847;&#22270;&#20687;&#19982;&#20219;&#24847;&#25991;&#26412;&#12289;&#20219;&#24847;&#25991;&#26412;&#19982;&#20219;&#24847;&#22768;&#38899;&#31561;&#36827;&#34892;&#23545;&#40784;&#12290;&#23545;&#25239;&#24187;&#35273;&#21033;&#29992;&#20102;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#25509;&#36817;&#24615;&#65292;&#22240;&#27492;&#19982;&#19979;&#28216;&#20219;&#21153;&#26080;&#20851;&#12290;&#20351;&#29992;ImageBind&#23884;&#20837;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#22312;&#27809;&#26377;&#20855;&#20307;&#19979;&#28216;&#20219;&#21153;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#23545;&#25239;&#24615;&#23545;&#40784;&#30340;&#36755;&#20837;&#22914;&#20309;&#35823;&#23548;&#22270;&#20687;&#29983;&#25104;&#12289;&#25991;&#26412;&#29983;&#25104;&#21644;&#38646;&#26679;&#20363;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-modal encoders map images, sounds, texts, videos, etc. into a single embedding space, aligning representations across modalities (e.g., associate an image of a dog with a barking sound). We show that multi-modal embeddings can be vulnerable to an attack we call "adversarial illusions." Given an input in any modality, an adversary can perturb it so as to make its embedding close to that of an arbitrary, adversary-chosen input in another modality. Illusions thus enable the adversary to align any image with any text, any text with any sound, etc.  Adversarial illusions exploit proximity in the embedding space and are thus agnostic to downstream tasks. Using ImageBind embeddings, we demonstrate how adversarially aligned inputs, generated without knowledge of specific downstream tasks, mislead image generation, text generation, and zero-shot classification.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22522;&#20110;&#23545;&#27604;&#25439;&#22833;&#30340;&#31163;&#32447;&#25163;&#20889;&#31614;&#21517;&#39564;&#35777;&#27169;&#22411;&#30340;&#30333;&#30418;&#35823;&#25253;&#23545;&#25239;&#25915;&#20987;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25915;&#20987;&#35270;&#20026;&#20070;&#20889;&#39118;&#26684;&#20043;&#38388;&#30340;&#39118;&#26684;&#36716;&#25442;&#65292;&#24341;&#20837;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#29983;&#25104;&#27450;&#39575;&#24615;&#22270;&#20687;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.08925</link><description>&lt;p&gt;
&#22522;&#20110;&#23545;&#27604;&#25439;&#22833;&#30340;&#31163;&#32447;&#25163;&#20889;&#31614;&#21517;&#39564;&#35777;&#27169;&#22411;&#30340;&#30333;&#30418;&#35823;&#25253;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A White-Box False Positive Adversarial Attack Method on Contrastive Loss-Based Offline Handwritten Signature Verification Models. (arXiv:2308.08925v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08925
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22522;&#20110;&#23545;&#27604;&#25439;&#22833;&#30340;&#31163;&#32447;&#25163;&#20889;&#31614;&#21517;&#39564;&#35777;&#27169;&#22411;&#30340;&#30333;&#30418;&#35823;&#25253;&#23545;&#25239;&#25915;&#20987;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25915;&#20987;&#35270;&#20026;&#20070;&#20889;&#39118;&#26684;&#20043;&#38388;&#30340;&#39118;&#26684;&#36716;&#25442;&#65292;&#24341;&#20837;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#29983;&#25104;&#27450;&#39575;&#24615;&#22270;&#20687;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#22522;&#20110;&#23545;&#27604;&#25439;&#22833;&#30340;&#31163;&#32447;&#25163;&#20889;&#31614;&#21517;&#39564;&#35777;&#27169;&#22411;&#30340;&#30333;&#30418;&#35823;&#25253;&#23545;&#25239;&#25915;&#20987;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#23558;&#25915;&#20987;&#35270;&#20026;&#22312;&#23494;&#20999;&#30456;&#20851;&#20294;&#19981;&#21516;&#30340;&#20070;&#20889;&#39118;&#26684;&#20043;&#38388;&#36827;&#34892;&#39118;&#26684;&#36716;&#25442;&#12290;&#20026;&#20102;&#24341;&#23548;&#27450;&#39575;&#24615;&#22270;&#20687;&#30340;&#29983;&#25104;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#25200;&#21160;&#21407;&#22987;&#26679;&#26412;&#19982;&#21512;&#25104;&#26679;&#26412;&#30340;&#23884;&#20837;&#21521;&#37327;&#20043;&#38388;&#30340;&#27431;&#27663;&#36317;&#31163;&#26469;&#25552;&#39640;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#21516;&#26102;&#36890;&#36807;&#20943;&#23567;&#29983;&#25104;&#22270;&#20687;&#19982;&#21407;&#22987;&#22270;&#20687;&#20043;&#38388;&#30340;&#24046;&#24322;&#26469;&#20445;&#35777;&#26368;&#23567;&#30340;&#25200;&#21160;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23545;&#27604;&#25439;&#22833;&#30340;&#31163;&#32447;&#25163;&#20889;&#31614;&#21517;&#39564;&#35777;&#27169;&#22411;&#30340;&#30333;&#30418;&#25915;&#20987;&#20013;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#30340;&#20851;&#38190;&#36129;&#29486;&#21253;&#25324;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35823;&#25253;&#25915;&#20987;&#26041;&#27861;&#12289;&#20004;&#20010;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#12289;&#26377;&#25928;&#30340;&#20070;&#20889;&#39118;&#26684;&#36716;&#25442;&#20197;&#21450;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we tackle the challenge of white-box false positive adversarial attacks on contrastive loss-based offline handwritten signature verification models. We propose a novel attack method that treats the attack as a style transfer between closely related but distinct writing styles. To guide the generation of deceptive images, we introduce two new loss functions that enhance the attack success rate by perturbing the Euclidean distance between the embedding vectors of the original and synthesized samples, while ensuring minimal perturbations by reducing the difference between the generated image and the original image. Our method demonstrates state-of-the-art performance in white-box attacks on contrastive loss-based offline handwritten signature verification models, as evidenced by our experiments. The key contributions of this paper include a novel false positive attack method, two new loss functions, effective style transfer in handwriting styles, and superior performance in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#21453;&#21521;&#25512;&#29702;&#36827;&#34892;&#39564;&#35777;&#30340;&#26041;&#27861;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#23631;&#34109;&#38382;&#39064;&#20013;&#30340;&#19968;&#20010;&#26631;&#35760;&#65292;&#24182;&#35201;&#27714;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#34987;&#23631;&#34109;&#30340;&#26631;&#35760;&#26469;&#39564;&#35777;&#20505;&#36873;&#31572;&#26696;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#27491;&#21521;&#21644;&#21453;&#21521;&#25512;&#29702;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#20505;&#36873;&#31572;&#26696;&#30340;&#27010;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.07758</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#21453;&#21521;&#25512;&#29702;&#36827;&#34892;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Backward Reasoning in Large Language Models for Verification. (arXiv:2308.07758v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07758
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#21453;&#21521;&#25512;&#29702;&#36827;&#34892;&#39564;&#35777;&#30340;&#26041;&#27861;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#23631;&#34109;&#38382;&#39064;&#20013;&#30340;&#19968;&#20010;&#26631;&#35760;&#65292;&#24182;&#35201;&#27714;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#34987;&#23631;&#34109;&#30340;&#26631;&#35760;&#26469;&#39564;&#35777;&#20505;&#36873;&#31572;&#26696;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#27491;&#21521;&#21644;&#21453;&#21521;&#25512;&#29702;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#20505;&#36873;&#31572;&#26696;&#30340;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38142;&#24335;&#24605;&#32771;&#65288;Chain-of-Though, CoT&#65289;&#25552;&#31034;&#22312;&#21508;&#31181;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;&#26368;&#36817;&#65292;Self-Consistency&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#37319;&#26679;&#19968;&#32452;&#19981;&#21516;&#30340;&#25512;&#29702;&#38142;&#65292;&#36825;&#20123;&#38142;&#21487;&#33021;&#23548;&#33268;&#19981;&#21516;&#30340;&#31572;&#26696;&#65292;&#28982;&#21518;&#36873;&#25321;&#24471;&#31080;&#26368;&#22810;&#30340;&#31572;&#26696;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;&#22312;&#39564;&#35777;&#20505;&#36873;&#31572;&#26696;&#26102;&#20351;&#29992;&#21453;&#21521;&#25512;&#29702;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#31616;&#21333;&#30340;&#27169;&#26495;&#65292;&#21363;``&#22914;&#26524;&#25105;&#20204;&#30693;&#36947;&#19978;&#36848;&#38382;&#39064;&#30340;&#31572;&#26696;&#26159;&#20505;&#36873;&#31572;&#26696;&#65292;&#37027;&#20040;&#26410;&#30693;&#21464;&#37327;x&#30340;&#20540;&#26159;&#22810;&#23569;&#65311;''&#65292;&#23558;&#38382;&#39064;&#20013;&#30340;&#19968;&#20010;&#26631;&#35760;&#23631;&#34109;&#65292;&#24182;&#35201;&#27714;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#34987;&#23631;&#34109;&#30340;&#26631;&#35760;&#12290;&#30452;&#35266;&#19978;&#35762;&#65292;&#22914;&#26524;&#25552;&#20379;&#30340;&#20505;&#36873;&#31572;&#26696;&#26159;&#27491;&#30830;&#30340;&#65292;&#35821;&#35328;&#27169;&#22411;&#24212;&#35813;&#33021;&#22815;&#25104;&#21151;&#39044;&#27979;&#34987;&#23631;&#34109;&#30340;&#26631;&#35760;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;FOBAR&#26041;&#27861;&#65292;&#23558;&#27491;&#21521;&#21644;&#21453;&#21521;&#25512;&#29702;&#32467;&#21512;&#36215;&#26469;&#20272;&#35745;&#20505;&#36873;&#31572;&#26696;&#30340;&#27010;&#29575;&#12290;&#25105;&#20204;&#22312;&#20845;&#20010;&#25968;&#25454;&#38598;&#21644;&#19977;&#20010;&#23454;&#39564;&#20013;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Though (CoT) prompting has shown promising performance in various reasoning tasks. Recently, Self-Consistency \citep{wang2023selfconsistency} proposes to sample a diverse set of reasoning chains which may lead to different answers while the answer that receives the most votes is selected. In this paper, we propose a novel method to use backward reasoning in verifying candidate answers. We mask a token in the question by ${\bf x}$ and ask the LLM to predict the masked token when a candidate answer is provided by \textit{a simple template}, i.e., ``\textit{\textbf{If we know the answer of the above question is \{a candidate answer\}, what is the value of unknown variable ${\bf x}$?}}'' Intuitively, the LLM is expected to predict the masked token successfully if the provided candidate answer is correct. We further propose FOBAR to combine forward and backward reasoning for estimating the probability of candidate answers. We conduct extensive experiments on six data sets and three
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26377;&#24207;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#23398;&#20064;&#22240;&#26524;&#22270;&#65292;&#24182;&#22312;&#22810;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#31034;&#20986;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.07480</link><description>&lt;p&gt;
OCDaf: &#26377;&#24207;&#22240;&#26524;&#25512;&#26029;&#19982;&#33258;&#22238;&#24402;&#27969;
&lt;/p&gt;
&lt;p&gt;
OCDaf: Ordered Causal Discovery with Autoregressive Flows. (arXiv:2308.07480v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07480
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26377;&#24207;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#23398;&#20064;&#22240;&#26524;&#22270;&#65292;&#24182;&#22312;&#22810;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#31034;&#20986;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#39034;&#24207;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#23398;&#20064;&#22240;&#26524;&#22270;&#12290;&#25105;&#20204;&#22312;&#22810;&#21464;&#37327;&#24322;&#26041;&#24046;&#22122;&#22768;&#27169;&#22411;&#20013;&#24314;&#31435;&#20102;&#22240;&#26524;&#22270;&#30340;&#21487;&#35782;&#21035;&#24615;&#65292;&#36825;&#26159;&#23545;&#21152;&#24615;&#22122;&#22768;&#27169;&#22411;&#30340;&#25512;&#24191;&#65292;&#20801;&#35768;&#38750;&#24120;&#25968;&#22122;&#22768;&#26041;&#24046;&#12290;&#20511;&#37492;&#36825;&#20123;&#27169;&#22411;&#19982;&#20223;&#23556;&#33258;&#22238;&#24402;&#24402;&#19968;&#20114;&#34917;&#35268;&#33539;&#27969;&#30340;&#32467;&#26500;&#30456;&#20284;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#36830;&#32493;&#25628;&#32034;&#31639;&#27861;&#26469;&#23547;&#25214;&#22240;&#26524;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;Sachs&#21644;SynTReN&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#22312;&#32467;&#26500;&#27721;&#26126;&#36317;&#31163;&#65288;SHD&#65289;&#21644;&#32467;&#26500;&#24178;&#39044;&#36317;&#31163;&#65288;SID&#65289;&#26041;&#38754;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#21487;&#35782;&#21035;&#24615;&#29702;&#35770;&#22312;&#21508;&#31181;&#21442;&#25968;&#21644;&#38750;&#21442;&#25968;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#65292;&#24182;&#23637;&#31034;&#20102;&#19982;&#29616;&#26377;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose OCDaf, a novel order-based method for learning causal graphs from observational data. We establish the identifiability of causal graphs within multivariate heteroscedastic noise models, a generalization of additive noise models that allow for non-constant noise variances. Drawing upon the structural similarities between these models and affine autoregressive normalizing flows, we introduce a continuous search algorithm to find causal structures. Our experiments demonstrate state-of-the-art performance across the Sachs and SynTReN benchmarks in Structural Hamming Distance (SHD) and Structural Intervention Distance (SID). Furthermore, we validate our identifiability theory across various parametric and nonparametric synthetic datasets and showcase superior performance compared to existing baselines.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#21160;&#24577;&#28608;&#27963;&#20989;&#25968;&#20248;&#21270;&#21069;&#39304;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#20013;&#65292;&#22797;&#26434;&#30340;&#20998;&#27573;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#27604;ReLU&#28608;&#27963;&#20989;&#25968;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2308.05724</link><description>&lt;p&gt;
&#36890;&#36807;&#21160;&#24577;&#28608;&#27963;&#20989;&#25968;&#20248;&#21270;&#21069;&#39304;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Optimizing Performance of Feedforward and Convolutional Neural Networks through Dynamic Activation Functions. (arXiv:2308.05724v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05724
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#21160;&#24577;&#28608;&#27963;&#20989;&#25968;&#20248;&#21270;&#21069;&#39304;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#20013;&#65292;&#22797;&#26434;&#30340;&#20998;&#27573;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#27604;ReLU&#28608;&#27963;&#20989;&#25968;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#31639;&#27861;&#22312;&#35821;&#38899;&#12289;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#35270;&#39057;&#31561;&#35768;&#22810;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#20154;&#20204;&#25552;&#20986;&#20102;&#36234;&#26469;&#36234;&#28145;&#30340;&#32593;&#32476;&#23618;&#27425;&#32467;&#26500;&#65292;&#22914;&#20855;&#26377;&#32422;152&#23618;&#30340;ResNet&#32467;&#26500;&#12290;&#27973;&#23618;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#20173;&#28982;&#26159;&#19968;&#20010;&#27963;&#36291;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#20854;&#20013;&#19968;&#20123;&#29616;&#35937;&#20173;&#28982;&#27809;&#26377;&#24471;&#21040;&#35299;&#37322;&#12290;&#32593;&#32476;&#20013;&#20351;&#29992;&#30340;&#28608;&#27963;&#20989;&#25968;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#20204;&#20026;&#32593;&#32476;&#25552;&#20379;&#20102;&#38750;&#32447;&#24615;&#12290;ReLU&#26159;&#26368;&#24120;&#29992;&#30340;&#28608;&#27963;&#20989;&#25968;&#12290;&#25105;&#20204;&#22312;&#38544;&#34255;&#23618;&#20351;&#29992;&#20102;&#22797;&#26434;&#30340;&#20998;&#27573;&#32447;&#24615;&#65288;PWL&#65289;&#28608;&#27963;&#20989;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;PWL&#28608;&#27963;&#20989;&#25968;&#22312;&#25105;&#20204;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#20013;&#27604;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#24615;&#33021;&#26356;&#22909;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#22312;PyTorch&#20013;&#27604;&#36739;&#27973;&#23618;&#21644;&#28145;&#24230;CNN&#30340;&#32467;&#26524;&#65292;&#20197;&#36827;&#19968;&#27493;&#35777;&#23454;&#25105;&#20204;&#30340;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning training training algorithms are a huge success in recent years in many fields including speech, text,image video etc. Deeper and deeper layers are proposed with huge success with resnet structures having around 152 layers. Shallow convolution neural networks(CNN's) are still an active research, where some phenomena are still unexplained. Activation functions used in the network are of utmost importance, as they provide non linearity to the networks. Relu's are the most commonly used activation function.We show a complex piece-wise linear(PWL) activation in the hidden layer. We show that these PWL activations work much better than relu activations in our networks for convolution neural networks and multilayer perceptrons. Result comparison in PyTorch for shallow and deep CNNs are given to further strengthen our case.
&lt;/p&gt;</description></item><item><title>CoRe&#20248;&#21270;&#22120;&#26159;&#19968;&#31181;&#39640;&#24615;&#33021;&#30340;&#26426;&#22120;&#23398;&#20064;&#20248;&#21270;&#22120;&#65292;&#20855;&#26377;&#24555;&#36895;&#12289;&#24179;&#28369;&#25910;&#25947;&#12289;&#20302;&#35745;&#31639;&#38656;&#27714;&#21644;&#36890;&#29992;&#36866;&#29992;&#24615;&#30340;&#29305;&#28857;&#65292;&#22312;&#35757;&#32451;&#32456;&#36523;&#26426;&#22120;&#23398;&#20064;&#28508;&#21147;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2307.15663</link><description>&lt;p&gt;
CoRe&#20248;&#21270;&#22120;&#65306;&#26426;&#22120;&#23398;&#20064;&#30340;&#19968;&#20307;&#21270;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
CoRe Optimizer: An All-in-One Solution for Machine Learning. (arXiv:2307.15663v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15663
&lt;/p&gt;
&lt;p&gt;
CoRe&#20248;&#21270;&#22120;&#26159;&#19968;&#31181;&#39640;&#24615;&#33021;&#30340;&#26426;&#22120;&#23398;&#20064;&#20248;&#21270;&#22120;&#65292;&#20855;&#26377;&#24555;&#36895;&#12289;&#24179;&#28369;&#25910;&#25947;&#12289;&#20302;&#35745;&#31639;&#38656;&#27714;&#21644;&#36890;&#29992;&#36866;&#29992;&#24615;&#30340;&#29305;&#28857;&#65292;&#22312;&#35757;&#32451;&#32456;&#36523;&#26426;&#22120;&#23398;&#20064;&#28508;&#21147;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20248;&#21270;&#31639;&#27861;&#21450;&#20854;&#36229;&#21442;&#25968;&#22312;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#20250;&#26174;&#33879;&#24433;&#21709;&#35757;&#32451;&#36895;&#24230;&#21644;&#27169;&#22411;&#20934;&#30830;&#24230;&#12290;&#29702;&#24819;&#20248;&#21270;&#22120;&#30340;&#24895;&#26395;&#28165;&#21333;&#21253;&#25324;&#24555;&#36895;&#12289;&#24179;&#28369;&#22320;&#25910;&#25947;&#21040;&#20302;&#35823;&#24046;&#12289;&#20302;&#35745;&#31639;&#38656;&#27714;&#21644;&#36890;&#29992;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#26368;&#36817;&#24341;&#20837;&#30340;&#25345;&#32493;&#24377;&#24615;&#65288;CoRe&#65289;&#20248;&#21270;&#22120;&#22312;&#35757;&#32451;&#32456;&#36523;&#26426;&#22120;&#23398;&#20064;&#28508;&#21147;&#26041;&#38754;&#27604;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#19968;&#38454;&#26799;&#24230;&#20248;&#21270;&#22120;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;CoRe&#20248;&#21270;&#22120;&#36827;&#34892;&#20102;&#19982;&#20854;&#20182;&#20061;&#31181;&#20248;&#21270;&#31639;&#27861;&#30340;&#24191;&#27867;&#24615;&#33021;&#23545;&#27604;&#65292;&#21253;&#25324;Adam&#20248;&#21270;&#22120;&#21644;&#24377;&#24615;&#21453;&#21521;&#20256;&#25773;&#65288;RPROP&#65289;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#19981;&#21516;&#36229;&#21442;&#25968;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20379;&#20102;&#36890;&#29992;&#36866;&#29992;&#30340;&#20540;&#12290;CoRe&#20248;&#21270;&#22120;&#22312;&#27599;&#20010;&#30740;&#31350;&#24212;&#29992;&#20013;&#37117;&#21462;&#24471;&#20102;&#26368;&#20339;&#25110;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#65292;&#21482;&#38656;&#35201;&#26356;&#25913;&#19968;&#20010;&#36229;&#21442;&#25968;&#65292;&#20855;&#20307;&#21462;&#20915;&#20110;&#23567;&#25209;&#37327;
&lt;/p&gt;
&lt;p&gt;
The optimization algorithm and its hyperparameters can significantly affect the training speed and resulting model accuracy in machine learning applications. The wish list for an ideal optimizer includes fast and smooth convergence to low error, low computational demand, and general applicability. Our recently introduced continual resilient (CoRe) optimizer has shown superior performance compared to other state-of-the-art first-order gradient-based optimizers for training lifelong machine learning potentials. In this work we provide an extensive performance comparison of the CoRe optimizer and nine other optimization algorithms including the Adam optimizer and resilient backpropagation (RPROP) for diverse machine learning tasks. We analyze the influence of different hyperparameters and provide generally applicable values. The CoRe optimizer yields best or competitive performance in every investigated application, while only one hyperparameter needs to be changed depending on mini-batch
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20855;&#26377;&#30830;&#23450;&#24615;&#28436;&#21270;&#29366;&#24577;&#30340;&#24378;&#30423;&#27169;&#22411;&#65292;&#29992;&#20110;&#23398;&#20064;&#24102;&#26377;&#24378;&#30423;&#21453;&#39304;&#30340;&#25512;&#33616;&#31995;&#32479;&#21644;&#22312;&#32447;&#24191;&#21578;&#12290;&#35813;&#27169;&#22411;&#32771;&#34385;&#20102;&#29366;&#24577;&#28436;&#21270;&#30340;&#19981;&#21516;&#36895;&#29575;&#65292;&#33021;&#20934;&#30830;&#35780;&#20272;&#22870;&#21169;&#19982;&#31995;&#32479;&#20581;&#24247;&#31243;&#24230;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2307.11655</link><description>&lt;p&gt;
&#20855;&#26377;&#30830;&#23450;&#24615;&#28436;&#21270;&#29366;&#24577;&#30340;&#24378;&#30423;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Bandits with Deterministically Evolving States. (arXiv:2307.11655v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11655
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20855;&#26377;&#30830;&#23450;&#24615;&#28436;&#21270;&#29366;&#24577;&#30340;&#24378;&#30423;&#27169;&#22411;&#65292;&#29992;&#20110;&#23398;&#20064;&#24102;&#26377;&#24378;&#30423;&#21453;&#39304;&#30340;&#25512;&#33616;&#31995;&#32479;&#21644;&#22312;&#32447;&#24191;&#21578;&#12290;&#35813;&#27169;&#22411;&#32771;&#34385;&#20102;&#29366;&#24577;&#28436;&#21270;&#30340;&#19981;&#21516;&#36895;&#29575;&#65292;&#33021;&#20934;&#30830;&#35780;&#20272;&#22870;&#21169;&#19982;&#31995;&#32479;&#20581;&#24247;&#31243;&#24230;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#19982;&#24378;&#30423;&#21453;&#39304;&#32467;&#21512;&#30340;&#27169;&#22411;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#30830;&#23450;&#24615;&#28436;&#21270;&#21644;&#19981;&#21487;&#35266;&#27979;&#30340;&#29366;&#24577;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#20855;&#26377;&#30830;&#23450;&#24615;&#28436;&#21270;&#29366;&#24577;&#30340;&#24378;&#30423;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20027;&#35201;&#24212;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#21644;&#22312;&#32447;&#24191;&#21578;&#30340;&#23398;&#20064;&#12290;&#22312;&#36825;&#20004;&#31181;&#24773;&#20917;&#19979;&#65292;&#31639;&#27861;&#22312;&#27599;&#19968;&#36718;&#33719;&#24471;&#30340;&#22870;&#21169;&#26159;&#36873;&#25321;&#34892;&#21160;&#30340;&#30701;&#26399;&#22870;&#21169;&#21644;&#31995;&#32479;&#30340;&#8220;&#20581;&#24247;&#8221;&#31243;&#24230;&#65288;&#21363;&#36890;&#36807;&#20854;&#29366;&#24577;&#27979;&#37327;&#65289;&#30340;&#20989;&#25968;&#12290;&#20363;&#22914;&#65292;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#24179;&#21488;&#20174;&#29992;&#25143;&#23545;&#29305;&#23450;&#31867;&#22411;&#20869;&#23481;&#30340;&#21442;&#19982;&#20013;&#33719;&#24471;&#30340;&#22870;&#21169;&#19981;&#20165;&#21462;&#20915;&#20110;&#20855;&#20307;&#20869;&#23481;&#30340;&#22266;&#26377;&#29305;&#24449;&#65292;&#36824;&#21462;&#20915;&#20110;&#29992;&#25143;&#19982;&#24179;&#21488;&#19978;&#20854;&#20182;&#31867;&#22411;&#20869;&#23481;&#20114;&#21160;&#21518;&#20854;&#20559;&#22909;&#30340;&#28436;&#21270;&#12290;&#25105;&#20204;&#30340;&#36890;&#29992;&#27169;&#22411;&#32771;&#34385;&#20102;&#29366;&#24577;&#28436;&#21270;&#30340;&#19981;&#21516;&#36895;&#29575;&#955;&#8712;[0,1]&#65288;&#20363;&#22914;&#65292;&#29992;&#25143;&#30340;&#20559;&#22909;&#22240;&#20808;&#21069;&#20869;&#23481;&#28040;&#36153;&#32780;&#24555;&#36895;&#21464;&#21270;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a model for learning with bandit feedback while accounting for deterministically evolving and unobservable states that we call Bandits with Deterministically Evolving States. The workhorse applications of our model are learning for recommendation systems and learning for online ads. In both cases, the reward that the algorithm obtains at each round is a function of the short-term reward of the action chosen and how ``healthy'' the system is (i.e., as measured by its state). For example, in recommendation systems, the reward that the platform obtains from a user's engagement with a particular type of content depends not only on the inherent features of the specific content, but also on how the user's preferences have evolved as a result of interacting with other types of content on the platform. Our general model accounts for the different rate $\lambda \in [0,1]$ at which the state evolves (e.g., how fast a user's preferences shift as a result of previous content consumption
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31687;&#31995;&#32479;&#32508;&#36848;&#65292;&#20174;&#26032;&#30340;&#35270;&#35282;&#25506;&#35752;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#24212;&#35813;&#20998;&#20139;&#20160;&#20040;&#65292;&#27880;&#37325;&#27169;&#22411;&#25928;&#29992;&#12289;&#38544;&#31169;&#27844;&#38706;&#21644;&#36890;&#20449;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.10655</link><description>&lt;p&gt;
&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20998;&#20139;&#20160;&#20040;&#65306;&#27169;&#22411;&#25928;&#29992;&#12289;&#38544;&#31169;&#27844;&#38706;&#21644;&#36890;&#20449;&#25928;&#29575;&#30340;&#35270;&#35282;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey of What to Share in Federated Learning: Perspectives on Model Utility, Privacy Leakage, and Communication Efficiency. (arXiv:2307.10655v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10655
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31687;&#31995;&#32479;&#32508;&#36848;&#65292;&#20174;&#26032;&#30340;&#35270;&#35282;&#25506;&#35752;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#24212;&#35813;&#20998;&#20139;&#20160;&#20040;&#65292;&#27880;&#37325;&#27169;&#22411;&#25928;&#29992;&#12289;&#38544;&#31169;&#27844;&#38706;&#21644;&#36890;&#20449;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#24050;&#25104;&#20026;&#19968;&#31181;&#39640;&#25928;&#30340;&#38544;&#31169;&#20445;&#25252;&#21512;&#20316;&#35757;&#32451;&#33539;&#24335;&#65292;&#21487;&#20197;&#22312;&#19981;&#26292;&#38706;&#31169;&#26377;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;&#20801;&#35768;&#23458;&#25143;&#31471;&#20849;&#20139;&#38544;&#31169;&#20445;&#25252;&#20449;&#24687;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#20445;&#35777;&#20102;&#22686;&#24378;&#30340;&#38544;&#31169;&#20445;&#25252;&#65292;&#32780;&#19988;&#20419;&#36827;&#20102;&#22810;&#26041;&#20043;&#38388;&#26356;&#39640;&#25928;&#12289;&#26356;&#23433;&#20840;&#30340;&#21512;&#20316;&#12290;&#22240;&#27492;&#65292;FL&#24341;&#36215;&#20102;&#30740;&#31350;&#20154;&#21592;&#30340;&#24191;&#27867;&#20851;&#27880;&#65292;&#25512;&#21160;&#20102;&#35768;&#22810;&#32508;&#36848;&#24615;&#25991;&#31456;&#23545;&#30456;&#20851;&#24037;&#20316;&#36827;&#34892;&#24635;&#32467;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#32508;&#36848;&#38598;&#20013;&#20110;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20849;&#20139;&#27169;&#22411;&#21442;&#25968;&#30340;&#26041;&#27861;&#65292;&#32780;&#24573;&#35270;&#20102;&#20849;&#20139;&#20854;&#20182;&#24418;&#24335;&#30340;&#26412;&#22320;&#20449;&#24687;&#30340;&#28508;&#21147;&#12290;&#26412;&#25991;&#20174;&#19968;&#31181;&#26032;&#30340;&#35270;&#35282;&#20986;&#21457;&#65292;&#21363;&#22312;FL&#20013;&#20998;&#20139;&#20160;&#20040;&#65292;&#37325;&#28857;&#20851;&#27880;&#27169;&#22411;&#25928;&#29992;&#12289;&#38544;&#31169;&#27844;&#38706;&#21644;&#36890;&#20449;&#25928;&#29575;&#65292;&#36827;&#34892;&#20102;&#31995;&#32479;&#32508;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) has emerged as a highly effective paradigm for privacy-preserving collaborative training among different parties. Unlike traditional centralized learning, which requires collecting data from each party, FL allows clients to share privacy-preserving information without exposing private datasets. This approach not only guarantees enhanced privacy protection but also facilitates more efficient and secure collaboration among multiple participants. Therefore, FL has gained considerable attention from researchers, promoting numerous surveys to summarize the related works. However, the majority of these surveys concentrate on methods sharing model parameters during the training process, while overlooking the potential of sharing other forms of local information. In this paper, we present a systematic survey from a new perspective, i.e., what to share in FL, with an emphasis on the model utility, privacy leakage, and communication efficiency. This survey differs from pr
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#36830;&#32493;&#26102;&#38388;&#21160;&#24577;&#22270;&#19978;&#20855;&#26377;&#20302;&#24310;&#36831;&#30340;&#33410;&#28857;&#23884;&#20837;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#20986;&#27969;&#24335;&#20302;&#24310;&#36831;&#30340;&#36817;&#20284;&#38543;&#26426;&#28216;&#36208;&#29305;&#24449;&#65292;&#35745;&#31639;&#26102;&#38388;&#24863;&#30693;&#33410;&#28857;&#23884;&#20837;&#20197;&#24635;&#32467;&#22810;&#36339;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2307.08433</link><description>&lt;p&gt;
&#20174;&#38543;&#26426;&#28216;&#36208;&#21040;&#22270;&#24418;&#24555;&#36305;&#65306;&#19968;&#31181;&#22312;&#36830;&#32493;&#26102;&#38388;&#21160;&#24577;&#22270;&#19978;&#20855;&#26377;&#20302;&#24310;&#36831;&#30340;&#33410;&#28857;&#23884;&#20837;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
From random-walks to graph-sprints: a low-latency node embedding framework on continuous-time dynamic graphs. (arXiv:2307.08433v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08433
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#36830;&#32493;&#26102;&#38388;&#21160;&#24577;&#22270;&#19978;&#20855;&#26377;&#20302;&#24310;&#36831;&#30340;&#33410;&#28857;&#23884;&#20837;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#20986;&#27969;&#24335;&#20302;&#24310;&#36831;&#30340;&#36817;&#20284;&#38543;&#26426;&#28216;&#36208;&#29305;&#24449;&#65292;&#35745;&#31639;&#26102;&#38388;&#24863;&#30693;&#33410;&#28857;&#23884;&#20837;&#20197;&#24635;&#32467;&#22810;&#36339;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#20855;&#26377;&#22522;&#30784;&#30340;&#21160;&#24577;&#22270;&#32467;&#26500;&#65292;&#20854;&#20013;&#23454;&#20307;&#21644;&#23427;&#20204;&#30340;&#30456;&#20114;&#20316;&#29992;&#38543;&#26102;&#38388;&#28436;&#21464;&#12290;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24212;&#32771;&#34385;&#36825;&#20123;&#21160;&#24577;&#22240;&#32032;&#65292;&#20197;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#20805;&#20998;&#21457;&#25381;&#20854;&#28508;&#21147;&#12290;&#20197;&#21069;&#29992;&#20110;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#35201;&#20040;&#20391;&#37325;&#20110;&#25277;&#26679;k-&#36339;&#37051;&#22495;&#65292;&#31867;&#20284;&#20110;&#24191;&#24230;&#20248;&#20808;&#25628;&#32034;&#65292;&#35201;&#20040;&#20391;&#37325;&#20110;&#38543;&#26426;&#28216;&#36208;&#65292;&#31867;&#20284;&#20110;&#28145;&#24230;&#20248;&#20808;&#25628;&#32034;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#23454;&#26102;&#21160;&#24577;&#22270;&#19978;&#36827;&#34892;&#20302;&#24310;&#36831;&#25512;&#26029;&#26159;&#35745;&#31639;&#19978;&#26114;&#36149;&#19988;&#19981;&#36866;&#29992;&#30340;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22270;&#24418;&#24555;&#36305;&#65292;&#36825;&#26159;&#19968;&#20010;&#36866;&#29992;&#20110;&#36830;&#32493;&#26102;&#38388;&#21160;&#24577;&#22270;&#65288;CTDGs&#65289;&#30340;&#36890;&#29992;&#29305;&#24449;&#25552;&#21462;&#26694;&#26550;&#65292;&#20855;&#26377;&#20302;&#24310;&#36831;&#65292;&#24182;&#19988;&#19982;&#39640;&#24310;&#36831;&#27169;&#22411;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27969;&#24335;&#12289;&#20302;&#24310;&#36831;&#30340;&#36817;&#20284;&#38543;&#26426;&#28216;&#36208;&#29305;&#24449;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#20351;&#29992;&#20165;&#21333;&#36339;&#25805;&#20316;&#35745;&#31639;&#24635;&#32467;&#22810;&#36339;&#20449;&#24687;&#30340;&#26102;&#38388;&#24863;&#30693;&#33410;&#28857;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many real-world datasets have an underlying dynamic graph structure, where entities and their interactions evolve over time. Machine learning models should consider these dynamics in order to harness their full potential in downstream tasks. Previous approaches for graph representation learning have focused on either sampling k-hop neighborhoods, akin to breadth-first search, or random walks, akin to depth-first search. However, these methods are computationally expensive and unsuitable for real-time, low-latency inference on dynamic graphs. To overcome these limitations, we propose graph-sprints a general purpose feature extraction framework for continuous-time-dynamic-graphs (CTDGs) that has low latency and is competitive with state-of-the-art, higher latency models. To achieve this, a streaming, low latency approximation to the random-walk based features is proposed. In our framework, time-aware node embeddings summarizing multi-hop information are computed using only single-hop ope
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22870;&#21169;&#26426;&#22120;&#25277;&#35937;&#26469;&#34920;&#31034;&#24403;&#21069;&#20219;&#21153;&#65292;&#24182;&#22312;&#36801;&#31227;&#23398;&#20064;&#20013;&#25552;&#21319;DRL&#20195;&#29702;&#30340;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#24182;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#36827;&#34892;&#23569;&#26679;&#26412;&#36801;&#31227;&#12290;</title><link>http://arxiv.org/abs/2307.05209</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#22522;&#20110;&#22870;&#21169;&#26426;&#22120;&#25277;&#35937;&#30340;&#19978;&#19979;&#25991;&#39044;&#35268;&#21010;&#20197;&#22686;&#24378;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Contextual Pre-Planning on Reward Machine Abstractions for Enhanced Transfer in Deep Reinforcement Learning. (arXiv:2307.05209v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05209
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22870;&#21169;&#26426;&#22120;&#25277;&#35937;&#26469;&#34920;&#31034;&#24403;&#21069;&#20219;&#21153;&#65292;&#24182;&#22312;&#36801;&#31227;&#23398;&#20064;&#20013;&#25552;&#21319;DRL&#20195;&#29702;&#30340;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#24182;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#36827;&#34892;&#23569;&#26679;&#26412;&#36801;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#20195;&#29702;&#20542;&#21521;&#20110;&#36807;&#25311;&#21512;&#35757;&#32451;&#20219;&#21153;&#65292;&#24182;&#19988;&#26080;&#27861;&#36866;&#24212;&#36731;&#24494;&#30340;&#29615;&#22659;&#21464;&#21270;&#12290;&#20026;&#20102;&#22312;&#36716;&#31227;&#21040;&#26410;&#35265;&#20219;&#21153;&#26102;&#21152;&#24555;&#23398;&#20064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22870;&#21169;&#26426;&#22120;&#65288;RM&#65289;&#26469;&#34920;&#31034;&#24403;&#21069;&#20219;&#21153;&#30340;&#26032;&#26041;&#27861;&#65292;&#22870;&#21169;&#26426;&#22120;&#26159;&#22522;&#20110;&#24403;&#21069;&#20219;&#21153;&#30340;&#22870;&#21169;&#21644;&#21160;&#24577;&#29983;&#25104;&#23376;&#20219;&#21153;&#30340;&#29366;&#24577;&#26426;&#25277;&#35937;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20026;&#20195;&#29702;&#25552;&#20379;&#20102;&#24403;&#21069;&#25277;&#35937;&#29366;&#24577;&#30340;&#31526;&#21495;&#34920;&#31034;&#65292;&#24182;&#22870;&#21169;&#23427;&#20204;&#36798;&#25104;&#36825;&#20123;&#36716;&#25442;&#12290;&#36825;&#20123;&#34920;&#31034;&#22312;&#20219;&#21153;&#20043;&#38388;&#20849;&#20139;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#21033;&#29992;&#20808;&#21069;&#36935;&#21040;&#30340;&#31526;&#21495;&#21644;&#36716;&#25442;&#30340;&#30693;&#35782;&#65292;&#20174;&#32780;&#22686;&#24378;&#36801;&#31227;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#34920;&#31034;&#22312;&#21508;&#31181;&#39046;&#22495;&#20013;&#25552;&#39640;&#20102;&#26679;&#26412;&#25928;&#29575;&#21644;&#23569;&#26679;&#26412;&#36801;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies show that deep reinforcement learning (DRL) agents tend to overfit to the task on which they were trained and fail to adapt to minor environment changes. To expedite learning when transferring to unseen tasks, we propose a novel approach to representing the current task using reward machines (RM), state machine abstractions that induce subtasks based on the current task's rewards and dynamics. Our method provides agents with symbolic representations of optimal transitions from their current abstract state and rewards them for achieving these transitions. These representations are shared across tasks, allowing agents to exploit knowledge of previously encountered symbols and transitions, thus enhancing transfer. Our empirical evaluation shows that our representations improve sample efficiency and few-shot transfer in a variety of domains.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#33136;&#37096;&#20329;&#25140;&#30340;&#21152;&#36895;&#35745;&#33258;&#21160;&#26816;&#27979;&#27493;&#24577;&#20107;&#20214;&#21644;&#34892;&#36208;&#36317;&#31163;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#24066;&#21806;&#26234;&#33021;&#25163;&#26426;&#21152;&#36895;&#35745;&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#20174;&#24191;&#27867;&#30340;&#27493;&#24577;&#36895;&#24230;&#33539;&#22260;&#20013;&#25552;&#21462;&#27493;&#24577;&#29305;&#24449;&#65292;&#21487;&#29992;&#20110;&#23545;Duchenne&#32908;&#32905;&#33806;&#32553;&#24739;&#20799;&#21644;&#20856;&#22411;&#21457;&#32946;&#27491;&#24120;&#24739;&#32773;&#30340;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2307.04866</link><description>&lt;p&gt;
&#20351;&#29992;&#33136;&#37096;&#20329;&#25140;&#30340;&#21152;&#36895;&#35745;&#22312;&#20856;&#22411;&#30340;&#34892;&#36208;&#21644;&#36305;&#27493;&#36895;&#24230;&#33539;&#22260;&#20869;&#33258;&#21160;&#26816;&#27979;&#27493;&#24577;&#20107;&#20214;&#21644;&#34892;&#36208;&#36317;&#31163;
&lt;/p&gt;
&lt;p&gt;
Automated Detection of Gait Events and Travel Distance Using Waist-worn Accelerometers Across a Typical Range of Walking and Running Speeds. (arXiv:2307.04866v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04866
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#33136;&#37096;&#20329;&#25140;&#30340;&#21152;&#36895;&#35745;&#33258;&#21160;&#26816;&#27979;&#27493;&#24577;&#20107;&#20214;&#21644;&#34892;&#36208;&#36317;&#31163;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#24066;&#21806;&#26234;&#33021;&#25163;&#26426;&#21152;&#36895;&#35745;&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#20174;&#24191;&#27867;&#30340;&#27493;&#24577;&#36895;&#24230;&#33539;&#22260;&#20013;&#25552;&#21462;&#27493;&#24577;&#29305;&#24449;&#65292;&#21487;&#29992;&#20110;&#23545;Duchenne&#32908;&#32905;&#33806;&#32553;&#24739;&#20799;&#21644;&#20856;&#22411;&#21457;&#32946;&#27491;&#24120;&#24739;&#32773;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#20272;&#35745;&#27493;&#24577;&#65288;CFs&#65289;&#30340;&#26102;&#38388;&#31354;&#38388;&#20020;&#24202;&#29305;&#24449;&#65292;&#22914;&#27493;&#25968;&#21644;&#38271;&#24230;&#12289;&#27493;&#38271;&#12289;&#27493;&#39057;&#12289;&#27493;&#36895;&#21644;&#34892;&#36208;&#36317;&#31163;&#31561;&#65292;&#22312;&#20351;&#29992;&#21487;&#31359;&#25140;&#24335;&#21152;&#36895;&#35745;&#36827;&#34892;&#22522;&#20110;&#31038;&#21306;&#30340;&#31227;&#21160;&#24615;&#35780;&#20272;&#20013;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35774;&#22791;&#22797;&#26434;&#24615;&#21644;&#21487;&#29992;&#24615;&#12289;&#25104;&#26412;&#21644;&#20998;&#26512;&#26041;&#27861;&#23398;&#24341;&#36215;&#30340;&#25361;&#25112;&#38480;&#21046;&#20102;&#27492;&#31867;&#24037;&#20855;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#30740;&#31350;&#38382;&#39064;&#65306;&#33021;&#21542;&#20351;&#29992;&#24066;&#21806;&#26234;&#33021;&#25163;&#26426;&#30340;&#21152;&#36895;&#35745;&#25968;&#25454;&#26469;&#25552;&#21462;Duchenne&#32908;&#32905;&#33806;&#32553;&#65288;DMD&#65289;&#24739;&#20799;&#21644;&#20856;&#22411;&#21457;&#32946;&#27491;&#24120;&#65288;TDs&#65289;&#24739;&#32773;&#22312;&#24191;&#27867;&#27493;&#24577;&#36895;&#24230;&#33539;&#22260;&#20869;&#30340;&#27493;&#24577;CFs&#65292;&#24182;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#12290;&#26041;&#27861;&#65306;15&#21517;DMD&#24739;&#20799;&#21644;15&#21517;TDs&#34987;&#35201;&#27714;&#22312;10MRW&#12289;25MRW&#12289;100MRW&#12289;6MWT&#21644;FW&#35780;&#20272;&#20013;&#20197;&#19968;&#31995;&#21015;&#27493;&#24577;&#36895;&#24230;&#36827;&#34892;&#30417;&#30563;&#24615;&#20020;&#24202;&#27979;&#35797;&#65292;&#21516;&#26102;&#20329;&#25140;&#25163;&#26426;&#22522;&#30784;&#21152;&#36895;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Background: Estimation of temporospatial clinical features of gait (CFs), such as step count and length, step duration, step frequency, gait speed and distance traveled is an important component of community-based mobility evaluation using wearable accelerometers. However, challenges arising from device complexity and availability, cost and analytical methodology have limited widespread application of such tools. Research Question: Can accelerometer data from commercially-available smartphones be used to extract gait CFs across a broad range of attainable gait velocities in children with Duchenne muscular dystrophy (DMD) and typically developing controls (TDs) using machine learning (ML)-based methods Methods: Fifteen children with DMD and 15 TDs underwent supervised clinical testing across a range of gait speeds using 10 or 25m run/walk (10MRW, 25MRW), 100m run/walk (100MRW), 6-minute walk (6MWT) and free-walk (FW) evaluations while wearing a mobile phone-based accelerometer at the wa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21367;&#31215;&#27531;&#24046;&#32593;&#32476;&#22312;&#38750;&#21442;&#25968;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;&#26435;&#37325;&#34928;&#20943;&#30340;ConvResNeXts&#65292;&#21487;&#20197;&#38544;&#21547;&#22320;&#23454;&#29616;&#23545;&#27169;&#22359;&#30340;&#31232;&#30095;&#24615;&#65292;&#20174;&#32780;&#20351;&#32593;&#32476;&#33021;&#22815;&#36866;&#24212;&#20302;&#32500;&#27969;&#24418;&#30340;&#24179;&#28369;&#24615;&#21644;&#32467;&#26500;&#65292;&#24182;&#39640;&#25928;&#22320;&#23398;&#20064;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2307.01649</link><description>&lt;p&gt;
&#20302;&#32500;&#27969;&#24418;&#19978;&#36807;&#21442;&#25968;&#21270;&#21367;&#31215;&#27531;&#24046;&#32593;&#32476;&#30340;&#38750;&#21442;&#25968;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Nonparametric Classification on Low Dimensional Manifolds using Overparameterized Convolutional Residual Networks. (arXiv:2307.01649v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01649
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21367;&#31215;&#27531;&#24046;&#32593;&#32476;&#22312;&#38750;&#21442;&#25968;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;&#26435;&#37325;&#34928;&#20943;&#30340;ConvResNeXts&#65292;&#21487;&#20197;&#38544;&#21547;&#22320;&#23454;&#29616;&#23545;&#27169;&#22359;&#30340;&#31232;&#30095;&#24615;&#65292;&#20174;&#32780;&#20351;&#32593;&#32476;&#33021;&#22815;&#36866;&#24212;&#20302;&#32500;&#27969;&#24418;&#30340;&#24179;&#28369;&#24615;&#21644;&#32467;&#26500;&#65292;&#24182;&#39640;&#25928;&#22320;&#23398;&#20064;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#27531;&#24046;&#31070;&#32463;&#32593;&#32476;(ConvResNets)&#34429;&#28982;&#36807;&#21442;&#25968;&#21270;&#65292;&#20294;&#22312;&#23454;&#36341;&#20013;&#33021;&#22815;&#33719;&#24471;&#26174;&#33879;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#36825;&#19981;&#33021;&#34987;&#24120;&#35268;&#26234;&#24935;&#24456;&#22909;&#22320;&#35299;&#37322;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#20174;&#38750;&#21442;&#25968;&#20998;&#31867;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#20351;&#29992;&#26435;&#37325;&#34928;&#20943;&#35757;&#32451;&#30340;ConvResNeXts&#65288;&#35206;&#30422;ConvResNets&#20316;&#20026;&#19968;&#31181;&#29305;&#27530;&#24773;&#20917;&#65289;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#20801;&#35768;ConvResNeXts&#20013;&#26377;&#26080;&#38480;&#22810;&#30340;&#26500;&#24314;&#27169;&#22359;&#65292;&#24182;&#26174;&#31034;&#26435;&#37325;&#34928;&#20943;&#38544;&#21547;&#22320;&#24378;&#21046;&#36825;&#20123;&#27169;&#22359;&#30340;&#31232;&#30095;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#22312;&#20302;&#32500;&#27969;&#24418;&#19978;&#30340;&#24179;&#28369;&#30446;&#26631;&#20989;&#25968;&#65292;&#28982;&#21518;&#35777;&#26126;ConvResNeXts&#21487;&#20197;&#36866;&#24212;&#20989;&#25968;&#30340;&#24179;&#28369;&#24615;&#21644;&#20302;&#32500;&#32467;&#26500;&#65292;&#24182;&#19988;&#33021;&#22815;&#39640;&#25928;&#22320;&#23398;&#20064;&#20989;&#25968;&#32780;&#19981;&#21463;&#32500;&#24230;&#35781;&#21650;&#30340;&#22256;&#25200;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#37096;&#20998;&#35777;&#26126;&#20102;&#36807;&#21442;&#25968;&#21270;&#30340;ConvResNeXts&#30456;&#23545;&#20110;&#24120;&#35268;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Convolutional residual neural networks (ConvResNets), though overparameterized, can achieve remarkable prediction performance in practice, which cannot be well explained by conventional wisdom. To bridge this gap, we study the performance of ConvResNeXts, which cover ConvResNets as a special case, trained with weight decay from the perspective of nonparametric classification. Our analysis allows for infinitely many building blocks in ConvResNeXts, and shows that weight decay implicitly enforces sparsity on these blocks. Specifically, we consider a smooth target function supported on a low-dimensional manifold, then prove that ConvResNeXts can adapt to the function smoothness and low-dimensional structures and efficiently learn the function without suffering from the curse of dimensionality. Our findings partially justify the advantage of overparameterized ConvResNeXts over conventional machine learning models.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35266;&#23519;&#35757;&#32451;&#25968;&#25454;&#30340;&#20316;&#29992;&#65292;&#30740;&#31350;&#27169;&#22411;&#19981;&#20844;&#24179;&#24615;&#30340;&#26469;&#28304;&#21644;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#25913;&#21464;&#26679;&#26412;&#30340;&#23646;&#24615;&#26469;&#35745;&#31639;&#35757;&#32451;&#26679;&#26412;&#23545;&#27169;&#22411;&#30340;&#19981;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.17828</link><description>&lt;p&gt;
&#36890;&#36807;&#35757;&#32451;&#27010;&#24565;&#24433;&#21709;&#29702;&#35299;&#19981;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Understanding Unfairness via Training Concept Influence. (arXiv:2306.17828v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17828
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35266;&#23519;&#35757;&#32451;&#25968;&#25454;&#30340;&#20316;&#29992;&#65292;&#30740;&#31350;&#27169;&#22411;&#19981;&#20844;&#24179;&#24615;&#30340;&#26469;&#28304;&#21644;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#25913;&#21464;&#26679;&#26412;&#30340;&#23646;&#24615;&#26469;&#35745;&#31639;&#35757;&#32451;&#26679;&#26412;&#23545;&#27169;&#22411;&#30340;&#19981;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20102;&#35299;&#27169;&#22411;&#19981;&#20844;&#24179;&#24615;&#30340;&#21407;&#22240;&#26377;&#21161;&#20110;&#20174;&#19994;&#20154;&#21592;&#26356;&#22909;&#22320;&#29702;&#35299;&#20182;&#20204;&#30340;&#25968;&#25454;&#21644;&#31639;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#22521;&#35757;&#25968;&#25454;&#36825;&#19968;&#20027;&#35201;&#19981;&#20844;&#24179;&#26469;&#28304;&#30340;&#35270;&#35282;&#26469;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20197;&#19979;&#38382;&#39064;&#65306;&#22914;&#26524;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#26377;&#20123;&#26679;&#26412;&#65288;1&#65289;&#26469;&#33258;&#19981;&#21516;&#30340;&#65288;&#20363;&#22914;&#20154;&#21475;&#32479;&#35745;&#23398;&#65289;&#32676;&#20307;&#65292;&#65288;2&#65289;&#26631;&#35760;&#26041;&#24335;&#19981;&#21516;&#65292;&#25110;&#32773;&#65288;3&#65289;&#26576;&#20123;&#29305;&#24449;&#21457;&#29983;&#20102;&#21464;&#21270;&#65292;&#37027;&#20040;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#34920;&#29616;&#20250;&#21457;&#29983;&#24590;&#26679;&#30340;&#21464;&#21270;&#65311;&#25442;&#21477;&#35805;&#35828;&#65292;&#25105;&#20204;&#36890;&#36807;&#21453;&#20107;&#23454;&#22320;&#23545;&#22522;&#20110;&#39044;&#23450;&#20041;&#27010;&#24565;&#30340;&#26679;&#26412;&#36827;&#34892;&#24178;&#39044;&#21644;&#25913;&#21464;&#65292;&#37327;&#21270;&#35757;&#32451;&#26679;&#26412;&#23545;&#27169;&#22411;&#30340;&#19981;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#12290;&#35745;&#31639;&#35757;&#32451;&#26679;&#26412;&#23545;&#27169;&#22411;&#30456;&#23545;&#20110;&#27010;&#24565;&#30340;&#19981;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#26102;&#65292;&#25105;&#20204;&#39318;&#20808;&#22522;&#20110;&#27010;&#24565;&#29983;&#25104;&#21453;&#20107;&#23454;&#29256;&#26412;&#30340;&#26679;&#26412;&#65292;&#21363;&#22914;&#26524;&#27010;&#24565;&#21457;&#29983;&#21464;&#21270;&#65292;&#26679;&#26412;&#30340;&#21453;&#20107;&#23454;&#29256;&#26412;&#12290;&#28982;&#21518;&#25105;&#20204;&#35745;&#31639;&#37325;&#26032;
&lt;/p&gt;
&lt;p&gt;
Knowing the causes of a model's unfairness helps practitioners better understand their data and algorithms. This is an important yet relatively unexplored task. We look into this problem through the lens of the training data - one of the major sources of unfairness. We ask the following questions: how would a model's fairness performance change if, in its training data, some samples (1) were collected from a different (e.g. demographic) group, (2) were labeled differently, or (3) some features were changed? In other words, we quantify the fairness influence of training samples by counterfactually intervening and changing samples based on predefined concepts, i.e. data attributes such as features (X), labels (Y), or sensitive attributes (A). To calculate a training sample's influence on the model's unfairness w.r.t a concept, we first generate counterfactual samples based on the concept, i.e. the counterfactual versions of the sample if the concept were changed. We then calculate the re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#25972;&#21512;&#33258;&#36866;&#24212;&#30340;&#32422;&#26463;&#26465;&#20214;&#65292;&#29992;&#36739;&#23569;&#30340;&#20256;&#24863;&#22120;&#23454;&#29616;&#37325;&#24314;&#21453;&#24212;&#22534;&#27969;&#22330;&#24182;&#21019;&#24314;&#26680;&#25968;&#23383;&#23402;&#29983;&#20307;&#12290;</title><link>http://arxiv.org/abs/2306.13637</link><description>&lt;p&gt;
&#25918;&#32622;&#26377;&#33258;&#36866;&#24212;&#32422;&#26463;&#26465;&#20214;&#30340;&#20256;&#24863;&#22120;&#20197;&#21019;&#24314;&#26680;&#25968;&#23383;&#23402;&#29983;&#20307;&#30340;&#26368;&#20248;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Optimal Sensor Placement with Adaptive Constraints for Nuclear Digital Twins. (arXiv:2306.13637v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13637
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#25972;&#21512;&#33258;&#36866;&#24212;&#30340;&#32422;&#26463;&#26465;&#20214;&#65292;&#29992;&#36739;&#23569;&#30340;&#20256;&#24863;&#22120;&#23454;&#29616;&#37325;&#24314;&#21453;&#24212;&#22534;&#27969;&#22330;&#24182;&#21019;&#24314;&#26680;&#25968;&#23383;&#23402;&#29983;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#21453;&#24212;&#22534;&#30340;&#24694;&#21155;&#36816;&#34892;&#26465;&#20214;&#21644;&#29289;&#29702;&#38480;&#21046;&#65292;&#26680;&#24212;&#29992;&#19981;&#33021;&#23558;&#29289;&#29702;&#36164;&#20135;&#37197;&#22791;&#22823;&#37327;&#20256;&#24863;&#22120;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#20180;&#32454;&#30830;&#23450;&#22312;&#32473;&#23450;&#30340;&#31354;&#38388;&#38480;&#21046;&#20869;&#20256;&#24863;&#22120;&#30340;&#25918;&#32622;&#20301;&#32622;&#65292;&#20197;&#23454;&#29616;&#21453;&#24212;&#22534;&#27969;&#22330;&#30340;&#37325;&#24314;&#21644;&#26680;&#25968;&#23383;&#23402;&#29983;&#20307;&#30340;&#21019;&#24314;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#25216;&#26415;&#65292;&#23558;&#32422;&#26463;&#26465;&#20214;&#25972;&#21512;&#21040;&#20256;&#24863;&#22120;&#25918;&#32622;&#20248;&#21270;&#36807;&#31243;&#20013;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;&#37325;&#24314;&#35823;&#24046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#36138;&#24515;&#31639;&#27861;&#65292;&#22312;&#32593;&#26684;&#19978;&#20248;&#21270;&#20256;&#24863;&#22120;&#20301;&#32622;&#65292;&#36981;&#24490;&#29992;&#25143;&#23450;&#20041;&#30340;&#32422;&#26463;&#26465;&#20214;&#12290;&#25105;&#20204;&#36890;&#36807;&#35745;&#31639;&#36873;&#25321;&#20256;&#24863;&#22120;&#20301;&#32622;&#30340;&#25152;&#26377;&#21487;&#33021;&#37197;&#32622;&#24182;&#23558;&#20854;&#19982;&#25105;&#20204;&#30340;&#26041;&#27861;&#24471;&#20986;&#30340;&#35299;&#36827;&#34892;&#27604;&#36739;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#31639;&#27861;&#30340;&#36817;&#20046;&#26368;&#20248;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#65292;&#19988;&#25152;&#38656;&#20256;&#24863;&#22120;&#25968;&#37327;&#26497;&#23569;&#65292;&#22240;&#27492;&#20026;&#26680;&#25968;&#23383;&#23402;&#29983;&#20307;&#30340;&#21019;&#24314;&#25552;&#20379;&#20102;&#26356;&#20026;&#32463;&#27982;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given harsh operating conditions and physical constraints in reactors, nuclear applications cannot afford to equip the physical asset with a large array of sensors. Therefore, it is crucial to carefully determine the placement of sensors within the given spatial limitations, enabling the reconstruction of reactor flow fields and the creation of nuclear digital twins. Various design considerations are imposed, such as predetermined sensor locations, restricted areas within the reactor, a fixed number of sensors allocated to a specific region, or sensors positioned at a designated distance from one another. We develop a data-driven technique that integrates constraints into an optimization procedure for sensor placement, aiming to minimize reconstruction errors. Our approach employs a greedy algorithm that can optimize sensor locations on a grid, adhering to user-defined constraints. We demonstrate the near optimality of our algorithm by computing all possible configurations for selectin
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#28145;&#20837;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#39044;&#27979;&#22303;&#22756;&#19982;&#26893;&#29289;&#34920;&#22411;&#20043;&#38388;&#32852;&#31995;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#35777;&#26126;&#21152;&#20837;&#22303;&#22756;&#29289;&#29702;&#21270;&#23398;&#24615;&#36136;&#21644;&#24494;&#29983;&#29289;&#31181;&#32676;&#23494;&#24230;&#31561;&#29615;&#22659;&#29305;&#24449;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.11157</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20154;&#31867;&#38480;&#21046;&#65306;&#21033;&#29992;&#22303;&#22756;&#24494;&#29983;&#29289;&#25968;&#25454;&#39044;&#27979;&#26893;&#29289;&#34920;&#22411;
&lt;/p&gt;
&lt;p&gt;
Human Limits in Machine Learning: Prediction of Plant Phenotypes Using Soil Microbiome Data. (arXiv:2306.11157v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11157
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#28145;&#20837;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#39044;&#27979;&#22303;&#22756;&#19982;&#26893;&#29289;&#34920;&#22411;&#20043;&#38388;&#32852;&#31995;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#35777;&#26126;&#21152;&#20837;&#22303;&#22756;&#29289;&#29702;&#21270;&#23398;&#24615;&#36136;&#21644;&#24494;&#29983;&#29289;&#31181;&#32676;&#23494;&#24230;&#31561;&#29615;&#22659;&#29305;&#24449;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20445;&#25252;&#22303;&#22756;&#20581;&#24247;&#34987;&#35748;&#20026;&#26159;21&#19990;&#32426;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#65292;&#22240;&#20026;&#23427;&#22312;&#20892;&#19994;&#12289;&#20154;&#31867;&#20581;&#24247;&#21644;&#29983;&#29289;&#22810;&#26679;&#24615;&#26041;&#38754;&#20855;&#26377;&#24191;&#27867;&#65288;&#21487;&#33021;&#20855;&#26377;&#23041;&#32961;&#24615;&#30340;&#65289;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#20004;&#31181;&#27169;&#22411;&#65288;&#38543;&#26426;&#26862;&#26519;&#21644;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65289;&#25506;&#32034;&#20102;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#29702;&#35299;&#22303;&#22756;&#21644;&#29983;&#29289;&#34920;&#22411;&#20043;&#38388;&#32852;&#31995;&#30340;&#39044;&#27979;&#28508;&#21147;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#27169;&#22411;&#20013;&#21152;&#20837;&#22303;&#22756;&#29289;&#29702;&#21270;&#23398;&#24615;&#36136;&#21644;&#24494;&#29983;&#29289;&#31181;&#32676;&#23494;&#24230;&#31561;&#29615;&#22659;&#29305;&#24449;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#25506;&#32034;&#22810;&#31181;&#25968;&#25454;&#39044;&#22788;&#29702;&#31574;&#30053;&#65292;&#22914;&#24402;&#19968;&#21270;&#12289;&#38646;&#26367;&#25442;&#21644;&#25968;&#25454;&#22686;&#24378;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The preservation of soil health has been identified as one of the main challenges of the XXI century given its vast (and potentially threatening) ramifications in agriculture, human health and biodiversity. Here, we provide the first deep investigation of the predictive potential of machine-learning models to understand the connections between soil and biological phenotypes. Indeed, we investigate an integrative framework performing accurate machine-learning-based prediction of plant phenotypes from biological, chemical and physical properties of the soil via two models: random forest and Bayesian neural network. We show that prediction is improved, as evidenced by higher weighted F1 scores, when incorporating into the models environmental features like soil physicochemical properties and microbial population density in addition to the microbiome information. Furthermore, by exploring multiple data preprocessing strategies such as normalization, zero replacement, and data augmentation,
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#20256;&#24863;&#22120;&#30340;&#26631;&#20934;&#25968;&#25454;&#34920;&#27169;&#26495;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#36825;&#20123;&#25968;&#25454;&#34920;&#21487;&#20197;&#20419;&#36827;&#23545;&#20256;&#24863;&#22120;&#25968;&#25454;&#22312;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#29702;&#35299;&#21644;&#21033;&#29992;&#65292;&#24182;&#25552;&#20379;&#20102;&#23458;&#35266;&#30340;&#24615;&#33021;&#35780;&#20272;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2306.08848</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20256;&#24863;&#22120;&#30340;&#25968;&#25454;&#34920;
&lt;/p&gt;
&lt;p&gt;
Datasheets for Machine Learning Sensors. (arXiv:2306.08848v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08848
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#20256;&#24863;&#22120;&#30340;&#26631;&#20934;&#25968;&#25454;&#34920;&#27169;&#26495;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#36825;&#20123;&#25968;&#25454;&#34920;&#21487;&#20197;&#20419;&#36827;&#23545;&#20256;&#24863;&#22120;&#25968;&#25454;&#22312;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#29702;&#35299;&#21644;&#21033;&#29992;&#65292;&#24182;&#25552;&#20379;&#20102;&#23458;&#35266;&#30340;&#24615;&#33021;&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#20256;&#24863;&#22120;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#24863;&#30693;&#33539;&#24335;&#65292;&#33021;&#22815;&#22312;&#36793;&#32536;&#36827;&#34892;&#26234;&#33021;&#21270;&#65292;&#21516;&#26102;&#36171;&#20104;&#32456;&#31471;&#29992;&#25143;&#26356;&#22810;&#23545;&#20854;&#25968;&#25454;&#30340;&#25511;&#21046;&#26435;&#12290;&#30001;&#20110;&#36825;&#20123;ML&#20256;&#24863;&#22120;&#22312;&#26234;&#33021;&#35774;&#22791;&#30340;&#21457;&#23637;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#28165;&#26224;&#22320;&#35760;&#24405;&#20854;&#35268;&#26684;&#12289;&#21151;&#33021;&#21644;&#38480;&#21046;&#38750;&#24120;&#20851;&#38190;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;ML&#20256;&#24863;&#22120;&#30340;&#26631;&#20934;&#25968;&#25454;&#34920;&#27169;&#26495;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#21253;&#25324;&#31995;&#32479;&#30340;&#30828;&#20214;&#12289;ML&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#23646;&#24615;&#12289;&#31471;&#21040;&#31471;&#24615;&#33021;&#25351;&#26631;&#20197;&#21450;&#29615;&#22659;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#25105;&#20204;&#33258;&#24049;ML&#20256;&#24863;&#22120;&#30340;&#31034;&#20363;&#25968;&#25454;&#34920;&#65292;&#24182;&#35814;&#32454;&#35752;&#35770;&#20102;&#27599;&#20010;&#37096;&#20998;&#12290;&#25105;&#20204;&#24378;&#35843;&#36825;&#20123;&#25968;&#25454;&#34920;&#22914;&#20309;&#20419;&#36827;&#23545;ML&#24212;&#29992;&#20013;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#26356;&#22909;&#29702;&#35299;&#21644;&#21033;&#29992;&#65292;&#24182;&#25552;&#20379;&#20102;&#23458;&#35266;&#30340;&#34913;&#37327;&#31995;&#32479;&#24615;&#33021;&#30340;&#25351;&#26631;&#36827;&#34892;&#35780;&#20272;&#21644;&#27604;&#36739;&#12290;ML&#20256;&#24863;&#22120;&#21450;&#20854;&#25968;&#25454;&#34920;&#20849;&#21516;&#25552;&#20379;&#20102;&#26356;&#39640;&#30340;&#38544;&#31169;&#12289;&#23433;&#20840;&#24615;&#12289;&#36879;&#26126;&#24230;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#21487;&#23457;&#35745;&#24615;&#21644;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) sensors offer a new paradigm for sensing that enables intelligence at the edge while empowering end-users with greater control of their data. As these ML sensors play a crucial role in the development of intelligent devices, clear documentation of their specifications, functionalities, and limitations is pivotal. This paper introduces a standard datasheet template for ML sensors and discusses its essential components including: the system's hardware, ML model and dataset attributes, end-to-end performance metrics, and environmental impact. We provide an example datasheet for our own ML sensor and discuss each section in detail. We highlight how these datasheets can facilitate better understanding and utilization of sensor data in ML applications, and we provide objective measures upon which system performance can be evaluated and compared. Together, ML sensors and their datasheets provide greater privacy, security, transparency, explainability, auditability, and u
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102; BEE &#25805;&#20316;&#31526;&#65292;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#36807;&#21435;&#30340;&#25104;&#21151;&#32463;&#39564;&#65292;&#24182;&#20445;&#25345;&#25506;&#32034;&#20048;&#35266;&#24615;&#65292;&#35299;&#20915;&#20102;&#31163;&#32447;&#31574;&#30053;&#28436;&#21592;-&#35780;&#35770;&#23478;&#20013; Q &#20540;&#39640;&#20272;&#19982;&#20302;&#20272;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#31574;&#30053;&#23398;&#20064;&#21644;&#26679;&#26412;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.02865</link><description>&lt;p&gt;
&#25235;&#20303;&#24847;&#22806;&#25910;&#33719;&#65306;&#22312;&#31163;&#32447;&#31574;&#30053;&#28436;&#21592;-&#35780;&#35770;&#23478;&#20013;&#21033;&#29992;&#36807;&#21435;&#25104;&#21151;&#30340;&#20215;&#20540;(arXiv:2306.02865v2 [cs.LG]&#24050;&#26356;&#26032;)
&lt;/p&gt;
&lt;p&gt;
Seizing Serendipity: Exploiting the Value of Past Success in Off-Policy Actor-Critic. (arXiv:2306.02865v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02865
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102; BEE &#25805;&#20316;&#31526;&#65292;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#36807;&#21435;&#30340;&#25104;&#21151;&#32463;&#39564;&#65292;&#24182;&#20445;&#25345;&#25506;&#32034;&#20048;&#35266;&#24615;&#65292;&#35299;&#20915;&#20102;&#31163;&#32447;&#31574;&#30053;&#28436;&#21592;-&#35780;&#35770;&#23478;&#20013; Q &#20540;&#39640;&#20272;&#19982;&#20302;&#20272;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#31574;&#30053;&#23398;&#20064;&#21644;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#39640;&#36136;&#37327;&#30340; Q &#20540;&#20989;&#25968;&#22312;&#35768;&#22810;&#29616;&#20195;&#31163;&#32447;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064; (RL) &#31639;&#27861;&#30340;&#25104;&#21151;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#38598;&#20013;&#35299;&#20915;&#37319;&#29992;&#20989;&#25968;&#36924;&#36817;&#22120;&#21644;&#31163;&#32447;&#23398;&#20064;&#25152;&#23548;&#33268;&#30340;&#20540;&#36807;&#39640;&#30340;&#38382;&#39064;&#12290;&#19982;&#36825;&#31181;&#26222;&#36941;&#35266;&#28857;&#19981;&#21516;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040; Q &#20540;&#22312; RL &#35757;&#32451;&#36807;&#31243;&#30340;&#21518;&#26399;&#23454;&#38469;&#19978;&#34987;&#20302;&#20272;&#20102;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#36125;&#23572;&#26364;&#26356;&#26032;&#20013;&#65292;&#24403;&#21069;&#31574;&#30053;&#20351;&#29992;&#27604;&#22238;&#25918;&#32531;&#20914;&#21306;&#20013;&#26356;&#20248;&#30340;&#21160;&#20316;&#26679;&#26412;&#24046;&#12290;&#25105;&#20204;&#20551;&#35774;&#36825;&#20010;&#38271;&#26399;&#34987;&#24573;&#35270;&#30340;&#29616;&#35937;&#21487;&#33021;&#38459;&#30861;&#20102;&#31574;&#30053;&#23398;&#20064;&#65292;&#38477;&#20302;&#20102;&#26679;&#26412;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#24819;&#27861;&#26159;&#22312;&#20445;&#25345;&#25506;&#32034;&#20048;&#35266;&#24615;&#30340;&#21516;&#26102;&#65292;&#32467;&#21512;&#20805;&#20998;&#21033;&#29992;&#36807;&#21435;&#25104;&#21151;&#30340;&#32463;&#39564;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#28151;&#21512;&#21033;&#29992;&#21644;&#25506;&#32034; (BEE) &#25805;&#20316;&#31526;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#21382;&#21490;&#19978;&#34920;&#29616;&#26368;&#20339;&#30340;&#21160;&#20316;&#21644;&#24403;&#21069;&#31574;&#30053;&#29983;&#25104;&#30340;&#21160;&#20316;&#26469;&#26356;&#26032; Q &#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning high-quality Q-value functions plays a key role in the success of many modern off-policy deep reinforcement learning (RL) algorithms. Previous works focus on addressing the value overestimation issue, an outcome of adopting function approximators and off-policy learning. Deviating from the common viewpoint, we observe that Q-values are indeed underestimated in the latter stage of the RL training process, primarily related to the use of inferior actions from the current policy in Bellman updates as compared to the more optimal action samples in the replay buffer. We hypothesize that this long-neglected phenomenon potentially hinders policy learning and reduces sample efficiency. Our insight to address this issue is to incorporate sufficient exploitation of past successes while maintaining exploration optimism. We propose the Blended Exploitation and Exploration (BEE) operator, a simple yet effective approach that updates Q-value using both historical best-performing actions and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#36890;&#29992;&#35782;&#21035;&#31995;&#32479;AGNet&#65292;&#36890;&#36807;&#36716;&#25442;&#21487;&#23398;&#20064;&#30340;&#24494;&#35266;&#21442;&#25968;&#65292;&#23398;&#20064;&#20102;&#19981;&#21516;&#39057;&#29575;&#19979;&#27700;&#19979;&#22768;&#38899;&#30340;&#29305;&#24615;&#65292;&#20197;&#35299;&#20915;&#22312;&#21487;&#21464;&#27700;&#19979;&#29615;&#22659;&#20013;&#35782;&#21035;&#33337;&#33334;&#36752;&#23556;&#22122;&#38899;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.01002</link><description>&lt;p&gt;
&#36866;&#24212;&#24615;&#33337;&#33334;&#36752;&#23556;&#22122;&#22768;&#35782;&#21035;&#19982;&#21487;&#23398;&#20064;&#24494;&#35266;&#23567;&#27874;&#21464;&#25442;
&lt;/p&gt;
&lt;p&gt;
Adaptive ship-radiated noise recognition with learnable fine-grained wavelet transform. (arXiv:2306.01002v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01002
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#36890;&#29992;&#35782;&#21035;&#31995;&#32479;AGNet&#65292;&#36890;&#36807;&#36716;&#25442;&#21487;&#23398;&#20064;&#30340;&#24494;&#35266;&#21442;&#25968;&#65292;&#23398;&#20064;&#20102;&#19981;&#21516;&#39057;&#29575;&#19979;&#27700;&#19979;&#22768;&#38899;&#30340;&#29305;&#24615;&#65292;&#20197;&#35299;&#20915;&#22312;&#21487;&#21464;&#27700;&#19979;&#29615;&#22659;&#20013;&#35782;&#21035;&#33337;&#33334;&#36752;&#23556;&#22122;&#38899;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#26512;&#28023;&#27915;&#22768;&#23398;&#29615;&#22659;&#26159;&#19968;&#20010;&#26840;&#25163;&#30340;&#20219;&#21153;&#12290;&#32972;&#26223;&#22122;&#22768;&#21644;&#21487;&#21464;&#30340;&#20449;&#36947;&#20256;&#36755;&#29615;&#22659;&#20351;&#24471;&#20934;&#30830;&#30340;&#33337;&#33334;&#36752;&#23556;&#22122;&#22768;&#35782;&#21035;&#21464;&#24471;&#22797;&#26434;&#12290;&#29616;&#26377;&#30340;&#35782;&#21035;&#31995;&#32479;&#22312;&#22788;&#29702;&#21487;&#21464;&#27700;&#19979;&#29615;&#22659;&#26041;&#38754;&#36739;&#20026;&#34180;&#24369;&#65292;&#22240;&#27492;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#34920;&#29616;&#20196;&#20154;&#22833;&#26395;&#12290;&#20026;&#20102;&#20445;&#25345;&#35782;&#21035;&#31995;&#32479;&#22312;&#21508;&#31181;&#27700;&#19979;&#29615;&#22659;&#19979;&#30340;&#40065;&#26834;&#24615;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#36890;&#29992;&#35782;&#21035;&#31995;&#32479;&#8212;&#8212;AGNet&#65288;&#33258;&#36866;&#24212;&#36890;&#29992;&#32593;&#32476;&#65289;&#12290;&#36890;&#36807;&#23558;&#22266;&#23450;&#30340;&#23567;&#27874;&#21442;&#25968;&#36716;&#25442;&#20026;&#21487;&#23398;&#20064;&#30340;&#24494;&#35266;&#21442;&#25968;&#65292;AGNet&#23398;&#20064;&#20102;&#19981;&#21516;&#39057;&#29575;&#19979;&#27700;&#19979;&#22768;&#38899;&#30340;&#29305;&#24615;&#12290;&#20854;&#28789;&#27963;&#24494;&#35266;&#30340;&#35774;&#35745;&#26377;&#21161;&#20110;&#25429;&#33719;&#26356;&#22810;&#32972;&#26223;&#22768;&#23398;&#20449;&#24687;&#65288;&#20363;&#22914;&#32972;&#26223;&#22122;&#22768;&#12289;&#27700;&#19979;&#20256;&#36755;&#36890;&#36947;&#65289;&#12290;&#20026;&#20102;&#21033;&#29992;&#23567;&#27874;&#35889;&#22270;&#20013;&#30340;&#38544;&#24335;&#20449;&#24687;&#65292;AGNet&#37319;&#29992;&#24182;&#34892;&#21367;&#31215;&#27880;&#24847;&#21147;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;Convolutional Neural Network with Parallel Convolution Attention M&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Analyzing the ocean acoustic environment is a tricky task. Background noise and variable channel transmission environment make it complicated to implement accurate ship-radiated noise recognition. Existing recognition systems are weak in addressing the variable underwater environment, thus leading to disappointing performance in practical application. In order to keep the recognition system robust in various underwater environments, this work proposes an adaptive generalized recognition system - AGNet (Adaptive Generalized Network). By converting fixed wavelet parameters into fine-grained learnable parameters, AGNet learns the characteristics of underwater sound at different frequencies. Its flexible and fine-grained design is conducive to capturing more background acoustic information (e.g., background noise, underwater transmission channel). To utilize the implicit information in wavelet spectrograms, AGNet adopts the convolutional neural network with parallel convolution attention m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#33021;&#37327;&#22522;&#27169;&#22411;&#30340;&#35757;&#32451;&#31639;&#27861;&#65292;&#20351;&#29992;&#24402;&#19968;&#21270;&#27969;&#36827;&#34892;&#37319;&#26679;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#32479;&#35745;&#31934;&#24230;&#21644;&#29983;&#25104;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.00684</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#36866;&#24212;&#27969;&#37319;&#26679;&#24179;&#34913;&#35757;&#32451;&#33021;&#37327;&#22522;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Balanced Training of Energy-Based Models with Adaptive Flow Sampling. (arXiv:2306.00684v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00684
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#33021;&#37327;&#22522;&#27169;&#22411;&#30340;&#35757;&#32451;&#31639;&#27861;&#65292;&#20351;&#29992;&#24402;&#19968;&#21270;&#27969;&#36827;&#34892;&#37319;&#26679;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#32479;&#35745;&#31934;&#24230;&#21644;&#29983;&#25104;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#37327;&#22522;&#27169;&#22411; (EBM) &#26159;&#19968;&#31181;&#30452;&#25509;&#21442;&#25968;&#21270;&#26410;&#26631;&#20934;&#21270;&#23545;&#25968;&#23494;&#24230;&#30340;&#22810;&#21151;&#33021;&#23494;&#24230;&#20272;&#35745;&#27169;&#22411;&#12290;EBM &#38750;&#24120;&#28789;&#27963;&#65292;&#20294;&#32570;&#20047;&#27169;&#22411;&#30340;&#35268;&#33539;&#21270;&#24120;&#37327;&#65292;&#20351;&#27169;&#22411;&#30340;&#20284;&#28982;&#20989;&#25968;&#35745;&#31639;&#19981;&#21487;&#34892;&#12290;&#36817;&#24180;&#26469;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#36817;&#20284;&#37319;&#26679;&#22120;&#21644;&#21464;&#20998;&#25512;&#29702;&#25216;&#26415;&#26469;&#20272;&#35745;&#20284;&#28982;&#20989;&#25968;&#26799;&#24230;&#36827;&#34892;&#35757;&#32451;&#12290;&#36825;&#20123;&#25216;&#26415;&#22312;&#29983;&#25104;&#26679;&#26412;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23545;&#20110;&#20272;&#35745;&#23494;&#24230;&#30340;&#32479;&#35745;&#31934;&#24230;&#65292;&#20363;&#22914;&#30830;&#23450;&#25968;&#25454;&#38598;&#20013;&#19981;&#21516;&#31867;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#65292;&#21364;&#20184;&#20986;&#20102;&#24456;&#23569;&#30340;&#20851;&#27880;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26368;&#22823;&#20284;&#28982;&#35757;&#32451;&#31639;&#27861;&#65292;&#20351;&#29992;&#19968;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#24402;&#19968;&#21270;&#27969; (NF)&#65292;&#36825;&#31181;&#27169;&#22411;&#26368;&#36817;&#34987;&#25552;&#20986;&#20197;&#20415;&#20110;&#37319;&#26679;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23558; NF &#25311;&#21512;&#21040; EBM &#19978;&#65292;&#20197;&#20415; NF &#36741;&#21161;&#19979;&#30340;&#37319;&#26679;&#26041;&#26696;&#33021;&#22815;&#22987;&#32456;&#20026; EBM &#25552;&#20379;&#20934;&#30830;&#30340;&#26799;&#24230;&#65292;&#26368;&#32456;&#25552;&#39640;&#27169;&#22411;&#30340;&#32479;&#35745;&#31934;&#24230;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20256;&#32479; EBM &#35757;&#32451;&#25216;&#26415;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20135;&#29983;&#20102;&#26356;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#21644;&#26356;&#22909;&#30340;&#29983;&#25104;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Energy-based models (EBMs) are versatile density estimation models that directly parameterize an unnormalized log density. Although very flexible, EBMs lack a specified normalization constant of the model, making the likelihood of the model computationally intractable. Several approximate samplers and variational inference techniques have been proposed to estimate the likelihood gradients for training. These techniques have shown promising results in generating samples, but little attention has been paid to the statistical accuracy of the estimated density, such as determining the relative importance of different classes in a dataset. In this work, we propose a new maximum likelihood training algorithm for EBMs that uses a different type of generative model, normalizing flows (NF), which have recently been proposed to facilitate sampling. Our method fits an NF to an EBM during training so that an NF-assisted sampling scheme provides an accurate gradient for the EBMs at all times, ultim
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26102;&#38388;&#39044;&#27979;&#22312;&#38654;&#29615;&#22659;&#19979;&#39044;&#27979;&#22797;&#21046;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;Holder-Winter&#25351;&#25968;&#24179;&#28369;&#27861;&#36827;&#34892;&#26102;&#38388;&#39044;&#27979;&#30340;&#26032;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#20943;&#23569;&#22810;&#20313;&#25968;&#25454;&#30340;&#21516;&#26102;&#65292;&#21482;&#26377;&#24494;&#23567;&#30340;&#25968;&#25454;&#21487;&#29992;&#24615;&#38477;&#20302;&#12290;</title><link>http://arxiv.org/abs/2306.00575</link><description>&lt;p&gt;
&#22312;&#38654;&#29615;&#22659;&#20013;&#36827;&#34892;&#39044;&#27979;&#22797;&#21046;&#30340;&#26102;&#38388;&#36816;&#21160;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Predicting Temporal Aspects of Movement for Predictive Replication in Fog Environments. (arXiv:2306.00575v2 [cs.DC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00575
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26102;&#38388;&#39044;&#27979;&#22312;&#38654;&#29615;&#22659;&#19979;&#39044;&#27979;&#22797;&#21046;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;Holder-Winter&#25351;&#25968;&#24179;&#28369;&#27861;&#36827;&#34892;&#26102;&#38388;&#39044;&#27979;&#30340;&#26032;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#20943;&#23569;&#22810;&#20313;&#25968;&#25454;&#30340;&#21516;&#26102;&#65292;&#21482;&#26377;&#24494;&#23567;&#30340;&#25968;&#25454;&#21487;&#29992;&#24615;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20805;&#20998;&#21033;&#29992;&#38654;&#29615;&#22659;&#30340;&#22909;&#22788;&#65292;&#26377;&#25928;&#22320;&#31649;&#29702;&#25968;&#25454;&#20301;&#32622;&#38750;&#24120;&#37325;&#35201;&#12290;&#30450;&#30446;&#25110;&#21453;&#24212;&#24335;&#30340;&#25968;&#25454;&#22797;&#21046;&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;&#38654;&#35745;&#31639;&#30340;&#28508;&#21147;&#65292;&#38656;&#35201;&#26356;&#20808;&#36827;&#30340;&#25216;&#26415;&#26469;&#39044;&#27979;&#23458;&#25143;&#31471;&#20309;&#26102;&#20309;&#22320;&#36830;&#25509;&#12290;&#34429;&#28982;&#31354;&#38388;&#39044;&#27979;&#21463;&#21040;&#20102;&#30456;&#24403;&#22810;&#30340;&#20851;&#27880;&#65292;&#20294;&#26102;&#38388;&#39044;&#27979;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#30740;&#31350;&#12290;&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#23558;&#26102;&#38388;&#39044;&#27979;&#32435;&#20837;&#29616;&#26377;&#31354;&#38388;&#39044;&#27979;&#27169;&#22411;&#30340;&#20248;&#21183;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#36824;&#22312;&#39044;&#27979;&#22797;&#21046;&#30340;&#32972;&#26223;&#19979;&#23545;&#26102;&#31354;&#39044;&#27979;&#27169;&#22411;&#65288;&#22914;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#65289;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;Holder-Winter&#25351;&#25968;&#24179;&#28369;&#27861;&#36827;&#34892;&#26102;&#38388;&#39044;&#27979;&#30340;&#26032;&#27169;&#22411;&#65292;&#21033;&#29992;&#39034;&#24207;&#21644;&#21608;&#26399;&#24615;&#29992;&#25143;&#31227;&#21160;&#27169;&#24335;&#12290;&#22312;&#27169;&#25311;&#30495;&#23454;&#29992;&#25143;&#36712;&#36857;&#30340;&#38654;&#32593;&#32476;&#20013;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#22810;&#20313;&#25968;&#25454;&#19978;&#23454;&#29616;&#20102;15&#65285;&#30340;&#38477;&#20302;&#65292;&#32780;&#25968;&#25454;&#21487;&#29992;&#24615;&#21482;&#26377;1&#65285;&#30340;&#24494;&#23567;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
To fully exploit the benefits of the fog environment, efficient management of data locality is crucial. Blind or reactive data replication falls short in harnessing the potential of fog computing, necessitating more advanced techniques for predicting where and when clients will connect. While spatial prediction has received considerable attention, temporal prediction remains understudied.  Our paper addresses this gap by examining the advantages of incorporating temporal prediction into existing spatial prediction models. We also provide a comprehensive analysis of spatio-temporal prediction models, such as Deep Neural Networks and Markov models, in the context of predictive replication. We propose a novel model using Holt-Winter's Exponential Smoothing for temporal prediction, leveraging sequential and periodical user movement patterns. In a fog network simulation with real user trajectories our model achieves a 15% reduction in excess data with a marginal 1% decrease in data availabi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25991;&#26412;&#27169;&#26495;&#30340;&#27700;&#19979;&#22768;&#23398;&#30446;&#26631;&#35782;&#21035;&#26041;&#27861;&#65288;UART&#65289;&#65292;&#20174;&#19981;&#21516;&#35270;&#35282;&#25972;&#21512;&#30456;&#20851;&#20449;&#24687;&#65292;&#36890;&#36807;&#38899;&#39057;-&#39057;&#35889;&#22270;-&#25991;&#26412;&#19977;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#36171;&#20104;UART&#29992;&#33258;&#28982;&#35821;&#35328;&#25351;&#23548;&#22768;&#23398;&#34920;&#31034;&#23398;&#20064;&#30340;&#33021;&#21147;&#65292;&#26174;&#33879;&#25552;&#39640;&#35782;&#21035;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.19612</link><description>&lt;p&gt;
&#25299;&#23637;&#27700;&#19979;&#22768;&#23398;&#30446;&#26631;&#35782;&#21035;&#20449;&#24687;&#35270;&#35282;&#30340;&#25991;&#26412;&#27169;&#26495;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Underwater-Art: Expanding Information Perspectives With Text Templates For Underwater Acoustic Target Recognition. (arXiv:2305.19612v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19612
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25991;&#26412;&#27169;&#26495;&#30340;&#27700;&#19979;&#22768;&#23398;&#30446;&#26631;&#35782;&#21035;&#26041;&#27861;&#65288;UART&#65289;&#65292;&#20174;&#19981;&#21516;&#35270;&#35282;&#25972;&#21512;&#30456;&#20851;&#20449;&#24687;&#65292;&#36890;&#36807;&#38899;&#39057;-&#39057;&#35889;&#22270;-&#25991;&#26412;&#19977;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#36171;&#20104;UART&#29992;&#33258;&#28982;&#35821;&#35328;&#25351;&#23548;&#22768;&#23398;&#34920;&#31034;&#23398;&#20064;&#30340;&#33021;&#21147;&#65292;&#26174;&#33879;&#25552;&#39640;&#35782;&#21035;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27700;&#19979;&#22768;&#23398;&#30446;&#26631;&#35782;&#21035;&#26159;&#19968;&#39033;&#26840;&#25163;&#30340;&#20219;&#21153;&#65292;&#30001;&#20110;&#22797;&#26434;&#30340;&#22768;&#28304;&#29305;&#24449;&#21644;&#22768;&#27874;&#20256;&#25773;&#27169;&#24335;&#32780;&#21464;&#24471;&#21313;&#20998;&#22256;&#38590;&#12290;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35782;&#21035;&#27169;&#22411;&#65292;&#21463;&#38480;&#20110;&#25968;&#25454;&#37327;&#19981;&#36275;&#21644;&#29421;&#31364;&#20449;&#24687;&#35270;&#35282;&#65292;&#22312;&#23454;&#38469;&#27700;&#19979;&#22330;&#26223;&#20013;&#20284;&#20046;&#36828;&#26410;&#36798;&#21040;&#28385;&#24847;&#31243;&#24230;&#12290;&#23613;&#31649;&#27700;&#19979;&#22768;&#23398;&#20449;&#21495;&#21463;&#36317;&#31163;&#12289;&#27700;&#28145;&#25110;&#20854;&#20182;&#22240;&#32032;&#30340;&#20005;&#37325;&#24433;&#21709;&#65292;&#20294;&#30456;&#20851;&#20449;&#24687;&#30340;&#27880;&#37322;&#24448;&#24448;&#26159;&#19981;&#22343;&#21248;&#30340;&#12289;&#19981;&#23436;&#25972;&#30340;&#12289;&#38590;&#20197;&#20351;&#29992;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25991;&#26412;&#27169;&#26495;&#30340;&#27700;&#19979;&#22768;&#23398;&#30446;&#26631;&#35782;&#21035;&#26041;&#27861;&#65288;UART&#65289;&#65292;&#36890;&#36807;&#23558;&#26469;&#33258;&#19981;&#21516;&#35270;&#35282;&#30340;&#30456;&#20851;&#20449;&#24687;&#25972;&#21512;&#25104;&#33258;&#28982;&#35821;&#35328;&#26469;&#35774;&#35745;&#27169;&#26495;&#12290;UART&#37319;&#29992;&#38899;&#39057;-&#39057;&#35889;&#22270;-&#25991;&#26412;&#19977;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#36171;&#20104;UART&#29992;&#33258;&#28982;&#35821;&#35328;&#25351;&#23548;&#22768;&#23398;&#34920;&#31034;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;UART&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#21508;&#31181;&#27700;&#19979;&#22768;&#23398;&#25968;&#25454;&#38598;&#19978;&#35782;&#21035;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Underwater acoustic target recognition is an intractable task due to the complex acoustic source characteristics and sound propagation patterns. Limited by insufficient data and narrow information perspective, recognition models based on deep learning seem far from satisfactory in practical underwater scenarios. Although underwater acoustic signals are severely influenced by distance, channel depth, or other factors, annotations of relevant information are often non-uniform, incomplete, and hard to use. In our work, we propose to implement Underwater Acoustic Recognition based on Templates made up of rich relevant information (hereinafter called "UART"). We design templates to integrate relevant information from different perspectives into descriptive natural language. UART adopts an audio-spectrogram-text tri-modal contrastive learning framework, which endows UART with the ability to guide the learning of acoustic representations by descriptive natural language. Our experiments reveal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;SpikeGCL&#65292;&#19968;&#31181;&#29992;&#20108;&#20540;&#21270;1&#27604;&#29305;&#34920;&#31034;&#26469;&#25552;&#39640;&#25928;&#29575;&#21644;&#33410;&#32422;&#36164;&#28304;&#30340;&#22270;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#21487;&#20197;&#20197;&#36817;32&#20493;&#30340;&#34920;&#31034;&#23384;&#20648;&#21387;&#32553;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2305.19306</link><description>&lt;p&gt;
&#19968;&#24352;&#22270;&#20540;&#24471;&#19968;&#27604;&#29305;&#30340;&#24046;&#24322;&#24615;&#65306;&#24403;&#22270;&#30340;&#23545;&#27604;&#23398;&#20064;&#36935;&#21040;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
A Graph is Worth 1-bit Spikes: When Graph Contrastive Learning Meets Spiking Neural Networks. (arXiv:2305.19306v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;SpikeGCL&#65292;&#19968;&#31181;&#29992;&#20108;&#20540;&#21270;1&#27604;&#29305;&#34920;&#31034;&#26469;&#25552;&#39640;&#25928;&#29575;&#21644;&#33410;&#32422;&#36164;&#28304;&#30340;&#22270;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#21487;&#20197;&#20197;&#36817;32&#20493;&#30340;&#34920;&#31034;&#23384;&#20648;&#21387;&#32553;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20107;&#23454;&#19978;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#20294;&#23545;&#39640;&#20219;&#21153;&#20934;&#30830;&#24615;&#30340;&#36861;&#27714;&#38656;&#35201;&#22823;&#30340;&#38544;&#34255;&#32500;&#24230;&#26469;&#23398;&#20064;&#20449;&#24687;&#20016;&#23500;&#12289;&#26377;&#21306;&#21035;&#24615;&#30340;&#20840;&#31934;&#24230;&#34920;&#31034;&#65292;&#36825;&#24341;&#21457;&#20102;&#23545;&#35745;&#31639;&#12289;&#23384;&#20648;&#21644;&#33021;&#28304;&#28040;&#32791;&#36127;&#25285;&#65288;&#22312;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#22823;&#22810;&#34987;&#24573;&#30053;&#65289;&#30340;&#25285;&#24551;&#12290;&#26412;&#25991;&#25506;&#32034;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#65292;&#21363;&#29992;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#36827;&#34892;&#22270;&#30340;&#23545;&#27604;&#23398;&#20064;&#65288;GCL&#65289;&#65292;&#21033;&#29992;&#31232;&#30095;&#21644;&#20108;&#20803;&#29305;&#24615;&#26469;&#23398;&#20064;&#26356;&#20855;&#29983;&#29289;&#21487;&#34892;&#24615;&#21644;&#32039;&#20945;&#24615;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SpikeGCL&#65292;&#19968;&#31181;&#23398;&#20064;&#22270;&#30340;&#20108;&#20540;&#21270;1&#27604;&#29305;&#34920;&#31034;&#30340;&#26032;&#22411;GCL&#26694;&#26550;&#65292;&#24179;&#34913;&#20102;&#25928;&#29575;&#21644;&#24615;&#33021;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#65292;&#35777;&#26126;SpikeGCL&#22312;&#34920;&#36798;&#33021;&#21147;&#19978;&#19982;&#20854;&#20840;&#31934;&#24230;&#23545;&#24212;&#29289;&#20855;&#26377;&#21487;&#27604;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#23558;&#34920;&#31034;&#23384;&#20648;&#21387;&#32553;&#36817;32&#20493;&#65292;SpikeGCL&#22312;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#21487;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
While contrastive self-supervised learning has become the de-facto learning paradigm for graph neural networks, the pursuit of high task accuracy requires a large hidden dimensionality to learn informative and discriminative full-precision representations, raising concerns about computation, memory footprint, and energy consumption burden (largely overlooked) for real-world applications. This paper explores a promising direction for graph contrastive learning (GCL) with spiking neural networks (SNNs), which leverage sparse and binary characteristics to learn more biologically plausible and compact representations. We propose SpikeGCL, a novel GCL framework to learn binarized 1-bit representations for graphs, making balanced trade-offs between efficiency and performance. We provide theoretical guarantees to demonstrate that SpikeGCL has comparable expressiveness with its full-precision counterparts. Experimental results demonstrate that, with nearly 32x representation storage compressio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#37096;&#20998;&#20687;&#32032;&#31354;&#38388;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65292;&#32467;&#21512;&#20102;&#33258;&#32534;&#30721;&#22120;&#19982;&#26465;&#20214;&#31070;&#32463;&#36807;&#31243;&#65292;&#21487;&#20197;&#23398;&#20064;&#21040;&#19968;&#31995;&#21015;&#22522;&#26412;&#29289;&#29702;&#21644;&#25991;&#21270;&#27010;&#24565;&#30340;&#34920;&#31034;&#65292;&#24182;&#19988;&#21487;&#20197;&#25552;&#39640;&#19978;&#19979;&#25991;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.18485</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#30340;&#26465;&#20214;&#31070;&#32463;&#36807;&#31243;&#29992;&#20110;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Autoencoding Conditional Neural Processes for Representation Learning. (arXiv:2305.18485v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18485
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#37096;&#20998;&#20687;&#32032;&#31354;&#38388;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65292;&#32467;&#21512;&#20102;&#33258;&#32534;&#30721;&#22120;&#19982;&#26465;&#20214;&#31070;&#32463;&#36807;&#31243;&#65292;&#21487;&#20197;&#23398;&#20064;&#21040;&#19968;&#31995;&#21015;&#22522;&#26412;&#29289;&#29702;&#21644;&#25991;&#21270;&#27010;&#24565;&#30340;&#34920;&#31034;&#65292;&#24182;&#19988;&#21487;&#20197;&#25552;&#39640;&#19978;&#19979;&#25991;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26465;&#20214;&#31070;&#32463;&#36807;&#31243;(CNPs)&#26159;&#19968;&#31181;&#28789;&#27963;&#39640;&#25928;&#30340;&#27169;&#22411;&#26063;&#32676;&#65292;&#21487;&#20197;&#20174;&#35266;&#27979;&#20540;&#20013;&#23398;&#20064;&#20986;&#19968;&#20010;&#38543;&#26426;&#36807;&#31243;&#12290;&#22312;&#35270;&#35273;&#39046;&#22495;&#20013;&#65292;CNPs &#22312;&#19978;&#19979;&#25991;&#22270;&#20687;&#34917;&#20840;&#20013;&#24471;&#21040;&#20102;&#29305;&#21035;&#30340;&#24212;&#29992;&#65292;&#21363;&#36890;&#36807;&#35266;&#23519;&#26576;&#20123;&#20301;&#32622;&#30340;&#20687;&#32032;&#20540;&#26469;&#39044;&#27979;&#20854;&#20182;&#26410;&#35266;&#23519;&#20301;&#32622;&#19978;&#30340;&#20540;&#30340;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#23398;&#20064;&#36825;&#26679;&#19968;&#20010; CNP &#30340;&#20687;&#32032;&#36873;&#25321;&#36890;&#24120;&#26159;&#38543;&#26426;&#30340;&#25110;&#32773;&#26159;&#36890;&#36807;&#19968;&#20010;&#31616;&#21333;&#30340;&#32479;&#35745;&#37327;(&#20363;&#22914;&#20687;&#32032;&#26041;&#24046;)&#23548;&#20986;&#30340;&#12290;&#26412;&#25991;&#23558;&#38382;&#39064;&#36716;&#21464;&#19968;&#19979;&#65306;&#19968;&#20010; CNP &#24819;&#35201;&#35266;&#23519;&#21738;&#20123;&#20687;&#32032;&#65311;&#20063;&#23601;&#26159;&#35828;&#65292;&#21738;&#20123;&#20687;&#32032;&#20801;&#35768;&#25311;&#21512; CNP&#65292;&#36825;&#26679;&#30340;&#20687;&#32032;&#33021;&#21578;&#35785;&#25105;&#20204;&#19968;&#20123;&#20851;&#20110;&#28508;&#22312;&#22270;&#20687;&#30340;&#20449;&#24687;&#21527;&#65311;&#23558;&#25552;&#20379;&#32473; CNP &#30340;&#19978;&#19979;&#25991;&#35270;&#20026;&#22266;&#23450;&#22823;&#23567;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#19968;&#27425;&#24615;&#21464;&#20998;&#26694;&#26550;&#65292;&#37096;&#20998;&#20687;&#32032;&#31354;&#38388;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;(Partical Pixel Space VAE, PPS-VAE)&#65292;&#21516;&#26102;&#39044;&#27979;&#36825;&#20010;&#19978;&#19979;&#25991;&#65292;&#24182;&#23398;&#20064;&#19968;&#20010; CNP&#12290;&#25105;&#20204;&#22312;&#19968;&#32452;&#35270;&#35273;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102; PPS-VAE&#65292;&#21457;&#29616;&#36890;&#36807;&#30456;&#23545;&#22823;&#23567;&#25110;&#21464;&#21270;&#39044;&#27979;&#20687;&#32032;&#30340;&#36873;&#25321;&#21487;&#20197;&#23433;&#25490;&#23398;&#20064;&#65292;&#19988;&#26356;&#20934;&#30830;&#22320;&#36827;&#34892;&#20102;&#19978;&#19979;&#25991;&#39044;&#27979;&#65292;&#24182;&#19988;&#21487;&#20197;&#23545;&#22522;&#26412;&#29289;&#29702;&#21644;&#25991;&#21270;&#27010;&#24565;&#36827;&#34892;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conditional neural processes (CNPs) are a flexible and efficient family of models that learn to learn a stochastic process from observations. In the visual domain, they have seen particular application in contextual image completion - observing pixel values at some locations to predict a distribution over values at other unobserved locations. However, the choice of pixels in learning such a CNP is typically either random or derived from a simple statistical measure (e.g. pixel variance). Here, we turn the problem on its head and ask: which pixels would a CNP like to observe? That is, which pixels allow fitting CNP, and do such pixels tell us something about the underlying image? Viewing the context provided to the CNP as fixed-size latent representations, we construct an amortised variational framework, Partial Pixel Space Variational Autoencoder (PPS-VAE), for predicting this context simultaneously with learning a CNP. We evaluate PPS-VAE on a set of vision datasets, and find that not
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#21464;&#21387;&#22120;&#20551;&#35774;&#31354;&#38388;&#30340;&#26222;&#36941;&#36924;&#36817;&#23450;&#29702;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35268;&#24459;&#27010;&#24565;&#29992;&#20110;&#31934;&#30830;&#36924;&#36817;&#36895;&#29575;&#20272;&#35745;&#65292;&#25581;&#31034;&#20102;&#21464;&#21387;&#22120;&#36866;&#29992;&#20110;&#36924;&#36817;&#21738;&#20123;&#31867;&#22411;&#30340;&#24207;&#21015;&#20851;&#31995;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#19982;&#20256;&#32479;&#24207;&#21015;&#24314;&#27169;&#26041;&#27861;&#20043;&#38388;&#30340;&#32467;&#26500;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2305.18475</link><description>&lt;p&gt;
&#24207;&#21015;&#24314;&#27169;&#30340;&#21464;&#21387;&#22120;&#32593;&#32476;&#30340;&#36924;&#36817;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Approximation theory of transformer networks for sequence modeling. (arXiv:2305.18475v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18475
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#21464;&#21387;&#22120;&#20551;&#35774;&#31354;&#38388;&#30340;&#26222;&#36941;&#36924;&#36817;&#23450;&#29702;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35268;&#24459;&#27010;&#24565;&#29992;&#20110;&#31934;&#30830;&#36924;&#36817;&#36895;&#29575;&#20272;&#35745;&#65292;&#25581;&#31034;&#20102;&#21464;&#21387;&#22120;&#36866;&#29992;&#20110;&#36924;&#36817;&#21738;&#20123;&#31867;&#22411;&#30340;&#24207;&#21015;&#20851;&#31995;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#19982;&#20256;&#32479;&#24207;&#21015;&#24314;&#27169;&#26041;&#27861;&#20043;&#38388;&#30340;&#32467;&#26500;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#21387;&#22120;&#26159;&#24207;&#21015;&#24314;&#27169;&#24212;&#29992;&#20013;&#24191;&#27867;&#24212;&#29992;&#30340;&#26550;&#26500;&#65292;&#20294;&#20854;&#24037;&#20316;&#21407;&#29702;&#30340;&#29702;&#35770;&#29702;&#35299;&#26377;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21464;&#21387;&#22120;&#36924;&#36817;&#24207;&#21015;&#20851;&#31995;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#21464;&#21387;&#22120;&#20551;&#35774;&#31354;&#38388;&#30340;&#26222;&#36941;&#36924;&#36817;&#23450;&#29702;&#12290;&#36890;&#36807;&#25512;&#23548;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#31181;&#26032;&#30340;&#35268;&#24459;&#27010;&#24565;&#65292;&#22312;&#27492;&#27010;&#24565;&#19979;&#65292;&#25105;&#20204;&#21487;&#20197;&#35777;&#26126;&#19968;&#20010;&#26126;&#30830;&#30340;&#36924;&#36817;&#36895;&#29575;&#20272;&#35745;&#12290;&#36825;&#20010;&#20272;&#35745;&#25581;&#31034;&#20102;&#21464;&#21387;&#22120;&#30340;&#20851;&#38190;&#32467;&#26500;&#29305;&#24615;&#65292;&#24182;&#26263;&#31034;&#20102;&#21464;&#21387;&#22120;&#36866;&#29992;&#20110;&#36924;&#36817;&#21738;&#20123;&#31867;&#22411;&#30340;&#24207;&#21015;&#20851;&#31995;&#12290;&#29305;&#21035;&#22320;&#65292;&#23427;&#20351;&#25105;&#20204;&#33021;&#22815;&#20855;&#20307;&#22320;&#35752;&#35770;&#21464;&#21387;&#22120;&#19982;&#20256;&#32479;&#24207;&#21015;&#24314;&#27169;&#26041;&#27861;&#65288;&#22914;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65289;&#20043;&#38388;&#30340;&#32467;&#26500;&#20559;&#24046;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#24471;&#21040;&#20102;&#25968;&#23383;&#23454;&#39564;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
The transformer is a widely applied architecture in sequence modeling applications, but the theoretical understanding of its working principles is limited. In this work, we investigate the ability of transformers to approximate sequential relationships. We first prove a universal approximation theorem for the transformer hypothesis space. From its derivation, we identify a novel notion of regularity under which we can prove an explicit approximation rate estimate. This estimate reveals key structural properties of the transformer and suggests the types of sequence relationships that the transformer is adapted to approximating. In particular, it allows us to concretely discuss the structural bias between the transformer and classical sequence modeling methods, such as recurrent neural networks. Our findings are supported by numerical experiments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36991;&#20813;&#38480;&#21046;&#24615;&#20551;&#35774;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#29305;&#24615;&#26469;&#25552;&#39640;&#23398;&#20064;&#25439;&#22833;&#20989;&#25968;&#30340;&#26679;&#26412;&#25928;&#29575;&#65292;&#22312;&#39044;&#27979;&#20248;&#21270;&#38382;&#39064;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.16830</link><description>&lt;p&gt;
&#31163;&#24034;&#65306;&#36229;&#36234;&#26412;&#22320;&#25439;&#22833;&#20989;&#25968;&#30340;&#39044;&#27979;&#20248;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Leaving the Nest: Going Beyond Local Loss Functions for Predict-Then-Optimize. (arXiv:2305.16830v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16830
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36991;&#20813;&#38480;&#21046;&#24615;&#20551;&#35774;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#29305;&#24615;&#26469;&#25552;&#39640;&#23398;&#20064;&#25439;&#22833;&#20989;&#25968;&#30340;&#26679;&#26412;&#25928;&#29575;&#65292;&#22312;&#39044;&#27979;&#20248;&#21270;&#38382;&#39064;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#20248;&#21270;&#38382;&#39064;&#26159;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#22312;&#19981;&#30830;&#23450;&#24615;&#26465;&#20214;&#19979;&#36827;&#34892;&#20915;&#31574;&#21046;&#23450;&#30340;&#26694;&#26550;&#12290;&#23427;&#30340;&#20013;&#24515;&#30740;&#31350;&#38382;&#39064;&#26159;&#65292;&#8220;&#22914;&#20309;&#21033;&#29992;&#20915;&#31574;&#20219;&#21153;&#30340;&#32467;&#26500;&#26469;&#23450;&#21046;&#29305;&#23450;&#20219;&#21153;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65311;&#8221;&#20026;&#27492;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#25552;&#20986;&#20102;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#25429;&#25417;&#36825;&#31181;&#28508;&#22312;&#30340;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#23545;&#36825;&#20123;&#25439;&#22833;&#30340;&#24418;&#24335;&#21644;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#34892;&#20026;&#30340;&#24433;&#21709;&#20570;&#20986;&#20102;&#38480;&#21046;&#24615;&#30340;&#20551;&#35774;&#12290;&#36825;&#20123;&#20551;&#35774;&#26082;&#23548;&#33268;&#20102;&#39640;&#35745;&#31639;&#25104;&#26412;&#30340;&#26041;&#27861;&#65292;&#20063;&#22312;&#23454;&#36341;&#20013;&#34987;&#36829;&#21453;&#26102;&#23548;&#33268;&#20102;&#24615;&#33021;&#19979;&#38477;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36991;&#20813;&#20102;&#19978;&#36848;&#20551;&#35774;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#29305;&#24615;&#26469;&#25552;&#39640;&#23398;&#20064;&#25439;&#22833;&#20989;&#25968;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;&#25105;&#20204;&#20174;&#25991;&#29486;&#20013;&#30340;&#22235;&#20010;&#39046;&#22495;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#36890;&#24120;&#38656;&#35201;&#27604;&#21487;&#27604;&#26041;&#27861;&#23569;&#19968;&#20010;&#25968;&#37327;&#32423;&#30340;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predict-then-Optimize is a framework for using machine learning to perform decision-making under uncertainty. The central research question it asks is, "How can the structure of a decision-making task be used to tailor ML models for that specific task?" To this end, recent work has proposed learning task-specific loss functions that capture this underlying structure. However, current approaches make restrictive assumptions about the form of these losses and their impact on ML model behavior. These assumptions both lead to approaches with high computational cost, and when they are violated in practice, poor performance. In this paper, we propose solutions to these issues, avoiding the aforementioned assumptions and utilizing the ML model's features to increase the sample efficiency of learning loss functions. We empirically show that our method achieves state-of-the-art results in four domains from the literature, often requiring an order of magnitude fewer samples than comparable metho
&lt;/p&gt;</description></item><item><title>vFedSec&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#30340;&#26032;&#22411;Secure Layer&#65292;&#26088;&#22312;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#23433;&#20840;&#27169;&#22359;&#65292;&#23454;&#29616;&#23433;&#20840;&#39640;&#25928;&#30340;&#32852;&#21512;&#35757;&#32451;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#25928;&#26524;&#26174;&#33879;&#65292;&#19981;&#20250;&#24433;&#21709;&#35757;&#32451;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.16794</link><description>&lt;p&gt;
&#36890;&#36807;&#23433;&#20840;&#23618;&#23454;&#29616;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#30340;&#39640;&#25928;&#23433;&#20840;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
vFedSec: Efficient Secure Aggregation for Vertical Federated Learning via Secure Layer. (arXiv:2305.16794v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16794
&lt;/p&gt;
&lt;p&gt;
vFedSec&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#30340;&#26032;&#22411;Secure Layer&#65292;&#26088;&#22312;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#23433;&#20840;&#27169;&#22359;&#65292;&#23454;&#29616;&#23433;&#20840;&#39640;&#25928;&#30340;&#32852;&#21512;&#35757;&#32451;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#25928;&#26524;&#26174;&#33879;&#65292;&#19981;&#20250;&#24433;&#21709;&#35757;&#32451;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#31169;&#20445;&#25252;&#32852;&#37030;&#23398;&#20064;&#20027;&#35201;&#20851;&#27880;&#27178;&#21521;&#21010;&#20998;&#30340;&#25968;&#25454;&#38598;&#65292;&#32780;&#22312;&#35768;&#22810;&#26377;&#36259;&#30340;&#38382;&#39064;&#20013;&#65292;&#20010;&#20307;&#25968;&#25454;&#28857;&#20998;&#25955;&#22312;&#22402;&#30452;&#30340;&#23458;&#25143;&#31471;/&#32452;&#32455;&#20013;&#12290;&#36825;&#31181;&#24773;&#20917;&#19979;&#30340;&#32852;&#37030;&#23398;&#20064;&#38656;&#35201;&#21442;&#19982;&#32773;&#20043;&#38388;&#20132;&#25442;&#20013;&#38388;&#36755;&#20986;&#21644;&#26799;&#24230;&#65292;&#33509;&#19981;&#32771;&#34385;&#38544;&#31169;&#21644;&#23433;&#20840;&#38382;&#39064;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#38544;&#31169;&#27844;&#38706;&#30340;&#39118;&#38505;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;vFedSec&#65292;&#36890;&#36807;&#21019;&#26032;&#24615;&#30340;&#23433;&#20840;&#23618;&#35774;&#35745;&#21644;&#26368;&#20808;&#36827;&#30340;&#23433;&#20840;&#27169;&#22359;&#65292;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#30340;&#23433;&#20840;&#21644;&#39640;&#25928;&#12290;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#24433;&#21709;&#35757;&#32451;&#32489;&#25928;&#65292;&#21516;&#26102;&#26377;&#25928;&#20445;&#25252;&#31169;&#20154;&#25968;&#25454;&#12290;&#23454;&#39564;&#32467;&#26524;&#20063;&#34920;&#26126;&#20102;&#25105;&#20204;&#30340;&#35774;&#35745;&#30340;&#24212;&#29992;&#24615;&#21644;&#20445;&#25252;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most work in privacy-preserving federated learning (FL) has been focusing on horizontally partitioned datasets where clients share the same sets of features and can train complete models independently. However, in many interesting problems, individual data points are scattered across different clients/organizations in a vertical setting. Solutions for this type of FL require the exchange of intermediate outputs and gradients between participants, posing a potential risk of privacy leakage when privacy and security concerns are not considered. In this work, we present vFedSec - a novel design with an innovative Secure Layer for training vertical FL securely and efficiently using state-of-the-art security modules in secure aggregation. We theoretically demonstrate that our method does not impact the training performance while protecting private data effectively. Empirically results also show its applicability with extensive experiments that our design can achieve the protection with negl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25991;&#29486;&#30340;&#21457;&#29616;&#26041;&#27861;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#21270;&#30340;&#23398;&#20064;&#29983;&#25104;&#26032;&#30340;&#31185;&#23398;&#26041;&#21521;&#65292;&#20811;&#26381;&#20102;&#26631;&#20934;&#26041;&#27861;&#22312;&#39044;&#27979;&#20851;&#32852;&#12289;&#24573;&#30053;&#19978;&#19979;&#25991;&#31561;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;&#27169;&#22411;&#20351;&#29992;&#20102;&#24341;&#25991;&#21644;&#30693;&#35782;&#22270;&#20851;&#31995;&#30340;&#32593;&#32476;&#65292;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616;GPT4&#22312;&#29983;&#25104;&#21019;&#26032;&#24605;&#24819;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2305.14259</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#25991;&#29486;&#30340;&#35821;&#22659;&#21270;&#23398;&#20064;&#29983;&#25104;&#26032;&#30340;&#31185;&#23398;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Learning to Generate Novel Scientific Directions with Contextualized Literature-based Discovery. (arXiv:2305.14259v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14259
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25991;&#29486;&#30340;&#21457;&#29616;&#26041;&#27861;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#21270;&#30340;&#23398;&#20064;&#29983;&#25104;&#26032;&#30340;&#31185;&#23398;&#26041;&#21521;&#65292;&#20811;&#26381;&#20102;&#26631;&#20934;&#26041;&#27861;&#22312;&#39044;&#27979;&#20851;&#32852;&#12289;&#24573;&#30053;&#19978;&#19979;&#25991;&#31561;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;&#27169;&#22411;&#20351;&#29992;&#20102;&#24341;&#25991;&#21644;&#30693;&#35782;&#22270;&#20851;&#31995;&#30340;&#32593;&#32476;&#65292;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616;GPT4&#22312;&#29983;&#25104;&#21019;&#26032;&#24605;&#24819;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25991;&#29486;&#30340;&#21457;&#29616;&#65288;LBD&#65289;&#26088;&#22312;&#36890;&#36807;&#25366;&#25496;&#35770;&#25991;&#24182;&#29983;&#25104;&#20551;&#35774;&#26469;&#21457;&#29616;&#26032;&#30340;&#31185;&#23398;&#30693;&#35782;&#12290;&#26631;&#20934;&#30340;LBD&#20165;&#38480;&#20110;&#39044;&#27979;&#31163;&#25955;&#27010;&#24565;&#20043;&#38388;&#30340;&#20004;&#20004;&#20851;&#31995;&#65288;&#20363;&#22914;&#65292;&#33647;&#29289;&#21644;&#30142;&#30149;&#30340;&#20851;&#32852;&#65289;&#12290;LBD&#36824;&#24573;&#30053;&#20102;&#20851;&#38190;&#30340;&#19978;&#19979;&#25991;&#65292;&#20363;&#22914;&#23454;&#39564;&#35774;&#32622;&#65288;&#20363;&#22914;&#65292;&#33647;&#29289;&#35780;&#20272;&#30340;&#29305;&#23450;&#24739;&#32773;&#32676;&#20307;&#65289;&#21644;&#20154;&#31867;&#31185;&#23398;&#23478;&#32771;&#34385;&#30340;&#32972;&#26223;&#30693;&#35782;&#21644;&#21160;&#26426;&#65288;&#20363;&#22914;&#65292;&#25214;&#21040;&#27809;&#26377;&#29305;&#23450;&#21103;&#20316;&#29992;&#30340;&#33647;&#29289;&#20505;&#36873;&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#19978;&#19979;&#25991;&#21270;LBD&#65288;C-LBD&#65289;&#34920;&#36848;&#26469;&#35299;&#20915;&#36825;&#20123;&#23616;&#38480;&#24615;&#65306;&#20197;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#31185;&#23398;&#20551;&#35774;&#65292;&#21516;&#26102;&#23558;&#23427;&#20204;&#32852;&#31995;&#21040;&#25511;&#21046;&#20551;&#35774;&#25628;&#32034;&#31354;&#38388;&#30340;&#19978;&#19979;&#25991;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24314;&#27169;&#26694;&#26550;&#65292;&#20351;&#29992;&#33719;&#24471;&#30340;&#24341;&#25991;&#21644;&#30693;&#35782;&#22270;&#20851;&#31995;&#30340;&#24322;&#26500;&#32593;&#32476;&#20013;&#30340;&#8220;&#28789;&#24863;&#8221;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#20174;&#35770;&#25991;&#20013;&#27966;&#29983;&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20351;&#29992;&#24378;&#22823;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616;GPT4&#20542;&#21521;&#20110;&#29983;&#25104;&#20855;&#26377;&#21019;&#26032;&#24615;&#30340;&#24605;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;
Literature-Based Discovery (LBD) aims to discover new scientific knowledge by mining papers and generating hypotheses. Standard LBD is limited to predicting pairwise relations between discrete concepts (e.g., drug-disease links). LBD also ignores critical contexts like experimental settings (e.g., a specific patient population where a drug is evaluated) and background knowledge and motivations that human scientists consider (e.g., to find a drug candidate without specific side effects). We address these limitations with a novel formulation of contextualized-LBD (C-LBD): generating scientific hypotheses in natural language, while grounding them in a context that controls the hypothesis search space. We present a modeling framework using retrieval of ``inspirations'' from a heterogeneous network of citations and knowledge graph relations, and create a new dataset derived from papers. Our evaluations with powerful large language models (LLMs) reveal that GPT4 tends to generate ideas with 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#25968;&#25454;&#39537;&#21160;&#31163;&#32447;&#35757;&#32451;&#30340; Web &#20195;&#29702;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#25351;&#20196;&#36319;&#38543;&#22810;&#27169;&#24577;&#20195;&#29702;WebGUM&#65292;&#23558;&#24494;&#35843;&#25351;&#20196;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#36716;&#25442;&#22120;&#65292;&#33021;&#22815;&#26377;&#25928;&#25552;&#39640;&#20195;&#29702;&#30340;&#22522;&#20110;&#35270;&#35273;&#24863;&#30693;&#12289;HTML &#29702;&#35299;&#21644;&#22810;&#27493;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.11854</link><description>&lt;p&gt;
&#20351;&#29992;&#25351;&#20196;&#24494;&#35843;&#22522;&#30784;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577; Web &#23548;&#33322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal Web Navigation with Instruction-Finetuned Foundation Models. (arXiv:2305.11854v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11854
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#25968;&#25454;&#39537;&#21160;&#31163;&#32447;&#35757;&#32451;&#30340; Web &#20195;&#29702;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#25351;&#20196;&#36319;&#38543;&#22810;&#27169;&#24577;&#20195;&#29702;WebGUM&#65292;&#23558;&#24494;&#35843;&#25351;&#20196;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#36716;&#25442;&#22120;&#65292;&#33021;&#22815;&#26377;&#25928;&#25552;&#39640;&#20195;&#29702;&#30340;&#22522;&#20110;&#35270;&#35273;&#24863;&#30693;&#12289;HTML &#29702;&#35299;&#21644;&#22810;&#27493;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027; Web &#23548;&#33322;&#30340;&#36827;&#23637;&#21463;&#21040;&#20102;&#20381;&#36182;&#25968;&#21313;&#20159;&#27425;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#25506;&#32034;&#24615;&#20132;&#20114;&#21644;&#20855;&#26377;&#39046;&#22495;&#29305;&#23450;&#27169;&#22411;&#35774;&#35745;&#30340;&#24433;&#21709;&#65292;&#36825;&#20351;&#24471;&#38590;&#20197;&#21033;&#29992;&#26469;&#33258;&#20016;&#23500;&#39046;&#22495;&#22806;&#25968;&#25454;&#30340;&#27867;&#21270;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#33073;&#26426;&#35757;&#32451;&#65292;&#29992;&#20110;&#20351;&#29992;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#30340; Web &#20195;&#29702;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25351;&#20196;&#36319;&#38543;&#22810;&#27169;&#24577;&#20195;&#29702;&#65292; WebGUM&#65292;&#23427;&#35266;&#23519;&#20102;&#32593;&#39029;&#25130;&#22270;&#21644; HTML &#39029;&#38754;&#65292;&#24182;&#36755;&#20986; Web &#23548;&#33322;&#25805;&#20316;&#65292;&#22914;&#21333;&#20987;&#21644;&#36755;&#20837;&#12290;WebGUM &#26159;&#36890;&#36807;&#32852;&#21512;&#24494;&#35843;&#25351;&#20196;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#36716;&#25442;&#22120;&#22312;&#22823;&#37327;&#30340;&#28436;&#31034;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#30340;&#12290;&#25105;&#20204;&#20973;&#32463;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#20195;&#29702;&#30340;&#22522;&#20110;&#35270;&#35273;&#24863;&#30693;&#12289;HTML &#29702;&#35299;&#21644;&#22810;&#27493;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#26126;&#26174;&#20248;&#20110;&#20043;&#21069;&#30340;&#24037;&#20316;&#12290;&#22312; MiniWoB &#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#36229;&#36807;&#20043;&#21069;&#26368;&#20339;&#33073;&#26426;&#26041;&#27861; 31.9% &#20197;&#19978;&#65292;&#25509;&#36817;&#23454;&#29616;&#22312;&#32447;&#20132;&#20114;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
The progress of autonomous web navigation has been hindered by the dependence on billions of exploratory interactions via online reinforcement learning, and domain-specific model designs that make it difficult to leverage generalization from rich out-of-domain data. In this work, we study data-driven offline training for web agents with vision-language foundation models. We propose an instruction-following multimodal agent, WebGUM, that observes both webpage screenshots and HTML pages and outputs web navigation actions, such as click and type. WebGUM is trained by jointly finetuning an instruction-finetuned language model and a vision transformer on a large corpus of demonstrations. We empirically demonstrate this recipe improves the agent's ability of grounded visual perception, HTML comprehension and multi-step reasoning, outperforming prior works by a significant margin. On the MiniWoB benchmark, we improve over the previous best offline methods by more than 31.9%, being close to re
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DeformerNet&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#23398;&#20064;&#19977;&#32500;&#21487;&#22609;&#29289;&#20307;&#30340;&#20302;&#32500;&#34920;&#31034;&#26469;&#23454;&#29616;&#26426;&#22120;&#20154;&#23545;&#29289;&#20307;&#24418;&#29366;&#30340;&#25805;&#32437;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#38656;&#35201;&#25163;&#24037;&#29305;&#24449;&#21644;&#29289;&#20307;&#29305;&#23450;&#30340;&#25511;&#21046;&#27169;&#22411;&#65292;&#21487;&#22312;&#20223;&#30495;&#21644;&#30495;&#23454;&#26426;&#22120;&#20154;&#19978;&#36827;&#34892;&#28436;&#31034;&#21644;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.04449</link><description>&lt;p&gt;
DeformerNet: &#23398;&#20064;&#19977;&#32500;&#21487;&#22609;&#29289;&#20307;&#30340;&#21452;&#25163;&#25805;&#32437;
&lt;/p&gt;
&lt;p&gt;
DeformerNet: Learning Bimanual Manipulation of 3D Deformable Objects. (arXiv:2305.04449v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04449
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DeformerNet&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#23398;&#20064;&#19977;&#32500;&#21487;&#22609;&#29289;&#20307;&#30340;&#20302;&#32500;&#34920;&#31034;&#26469;&#23454;&#29616;&#26426;&#22120;&#20154;&#23545;&#29289;&#20307;&#24418;&#29366;&#30340;&#25805;&#32437;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#38656;&#35201;&#25163;&#24037;&#29305;&#24449;&#21644;&#29289;&#20307;&#29305;&#23450;&#30340;&#25511;&#21046;&#27169;&#22411;&#65292;&#21487;&#22312;&#20223;&#30495;&#21644;&#30495;&#23454;&#26426;&#22120;&#20154;&#19978;&#36827;&#34892;&#28436;&#31034;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#23478;&#24237;&#25252;&#29702;&#21040;&#20179;&#24211;&#37197;&#36865;&#20877;&#21040;&#22806;&#31185;&#25163;&#26415;&#21161;&#29702;&#31561;&#39046;&#22495;&#65292;&#24212;&#29992;&#38656;&#35201;&#26426;&#22120;&#20154;&#21487;&#38752;&#22320;&#25805;&#32437;&#19977;&#32500;&#21487;&#22609;&#29289;&#20307;&#30340;&#24418;&#29366;&#12290;&#24377;&#24615;&#19977;&#32500;&#21487;&#22609;&#29289;&#20307;&#30340;&#20998;&#26512;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#21442;&#25968;&#26469;&#25551;&#36848;&#20915;&#23450;&#29289;&#20307;&#24418;&#29366;&#30340;&#21487;&#33021;&#26080;&#38480;&#33258;&#30001;&#24230;&#12290;&#20197;&#24448;&#30340;3D&#24418;&#29366;&#25511;&#21046;&#23581;&#35797;&#20381;&#36182;&#20110;&#25163;&#24037;&#29305;&#24449;&#26469;&#34920;&#31034;&#29289;&#20307;&#24418;&#29366;&#65292;&#24182;&#38656;&#35201;&#35757;&#32451;&#29289;&#20307;&#29305;&#23450;&#30340;&#25511;&#21046;&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#25105;&#20204;&#30340;&#26032;&#22411;DeformerNet&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26469;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#65292;&#35813;&#26550;&#26500;&#22312;&#34987;&#25805;&#32437;&#29289;&#20307;&#30340;&#37096;&#20998;&#35270;&#22270;&#28857;&#20113;&#21644;&#30446;&#26631;&#24418;&#29366;&#30340;&#28857;&#20113;&#19978;&#36816;&#34892;&#65292;&#23398;&#20064;&#29289;&#20307;&#24418;&#29366;&#30340;&#20302;&#32500;&#34920;&#31034;&#12290;&#36825;&#20010;&#24418;&#29366;&#23884;&#20837;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#23398;&#20064;&#19968;&#31181;&#35270;&#35273;&#20282;&#26381;&#25511;&#21046;&#22120;&#65292;&#35813;&#25511;&#21046;&#22120;&#35745;&#31639;&#20986;&#25152;&#38656;&#30340;&#26426;&#22120;&#20154;&#26411;&#31471;&#25191;&#34892;&#22120;&#21160;&#20316;&#65292;&#23558;&#29289;&#20307;&#36845;&#20195;&#22320;&#21464;&#24418;&#21521;&#30446;&#26631;&#24418;&#29366;&#12290;&#25105;&#20204;&#22312;&#20223;&#30495;&#21644;&#30495;&#23454;&#26426;&#22120;&#20154;&#19978;&#28436;&#31034;&#20102;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Applications in fields ranging from home care to warehouse fulfillment to surgical assistance require robots to reliably manipulate the shape of 3D deformable objects. Analytic models of elastic, 3D deformable objects require numerous parameters to describe the potentially infinite degrees of freedom present in determining the object's shape. Previous attempts at performing 3D shape control rely on hand-crafted features to represent the object shape and require training of object-specific control models. We overcome these issues through the use of our novel DeformerNet neural network architecture, which operates on a partial-view point cloud of the manipulated object and a point cloud of the goal shape to learn a low-dimensional representation of the object shape. This shape embedding enables the robot to learn a visual servo controller that computes the desired robot end-effector action to iteratively deform the object toward the target shape. We demonstrate both in simulation and on 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21452;&#26102;&#38388;&#23610;&#24230;&#26694;&#26550;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#28201;&#21644;&#26465;&#20214;&#19979;&#25910;&#25947;&#24615;&#65292;&#35813;&#26694;&#26550;&#21253;&#25324;&#20102;&#21508;&#31181;&#27969;&#34892;&#30340;Adam&#23478;&#26063;&#31639;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#26080;&#24179;&#28369;&#31070;&#32463;&#32593;&#32476;&#21644;&#24212;&#23545;&#37325;&#23614;&#22122;&#22768;&#30340;&#38656;&#27714;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#34920;&#26126;&#20102;&#20854;&#25928;&#29575;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.03938</link><description>&lt;p&gt;
Adam&#23478;&#26063;&#31639;&#27861;&#22312;&#26080;&#24179;&#28369;&#20248;&#21270;&#20013;&#30340;&#25910;&#25947;&#24615;&#20445;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Adam-family Methods for Nonsmooth Optimization with Convergence Guarantees. (arXiv:2305.03938v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21452;&#26102;&#38388;&#23610;&#24230;&#26694;&#26550;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#28201;&#21644;&#26465;&#20214;&#19979;&#25910;&#25947;&#24615;&#65292;&#35813;&#26694;&#26550;&#21253;&#25324;&#20102;&#21508;&#31181;&#27969;&#34892;&#30340;Adam&#23478;&#26063;&#31639;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#26080;&#24179;&#28369;&#31070;&#32463;&#32593;&#32476;&#21644;&#24212;&#23545;&#37325;&#23614;&#22122;&#22768;&#30340;&#38656;&#27714;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#34920;&#26126;&#20102;&#20854;&#25928;&#29575;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;Adam&#23478;&#26063;&#31639;&#27861;&#22312;&#26080;&#24179;&#28369;&#20248;&#21270;&#20013;&#30340;&#25910;&#25947;&#24615;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#65292;&#29305;&#21035;&#26159;&#22312;&#26080;&#24179;&#28369;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21452;&#26102;&#38388;&#23610;&#24230;&#26694;&#26550;&#65292;&#37319;&#29992;&#21452;&#26102;&#38388;&#23610;&#24230;&#26356;&#26032;&#26041;&#26696;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#28201;&#21644;&#26465;&#20214;&#19979;&#30340;&#25910;&#25947;&#24615;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;&#20102;&#21508;&#31181;&#27969;&#34892;&#30340;Adam&#23478;&#26063;&#31639;&#27861;&#65292;&#22312;&#35757;&#32451;&#26080;&#24179;&#28369;&#31070;&#32463;&#32593;&#32476;&#20013;&#25552;&#20379;&#20102;&#25910;&#25947;&#24615;&#20445;&#35777;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#38543;&#26426;&#27425;&#26799;&#24230;&#26041;&#27861;&#65292;&#32467;&#21512;&#26799;&#24230;&#35009;&#21098;&#25216;&#26415;&#65292;&#29992;&#20110;&#35757;&#32451;&#20855;&#26377;&#37325;&#23614;&#22122;&#22768;&#30340;&#26080;&#24179;&#28369;&#31070;&#32463;&#32593;&#32476;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#29978;&#33267;&#22312;&#20165;&#20551;&#23450;&#35780;&#20272;&#22122;&#22768;&#21487;&#31215;&#30340;&#24773;&#20917;&#19979;&#20063;&#20250;&#25910;&#25947;&#12290;&#24191;&#27867;&#30340;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#39640;&#25928;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a comprehensive study on the convergence properties of Adam-family methods for nonsmooth optimization, especially in the training of nonsmooth neural networks. We introduce a novel two-timescale framework that adopts a two-timescale updating scheme, and prove its convergence properties under mild assumptions. Our proposed framework encompasses various popular Adam-family methods, providing convergence guarantees for these methods in training nonsmooth neural networks. Furthermore, we develop stochastic subgradient methods that incorporate gradient clipping techniques for training nonsmooth neural networks with heavy-tailed noise. Through our framework, we show that our proposed methods converge even when the evaluation noises are only assumed to be integrable. Extensive numerical experiments demonstrate the high efficiency and robustness of our proposed methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#26694;&#26550;MLCopilot&#65292;&#36890;&#36807;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#25193;&#23637;&#20854;&#33021;&#21147;&#20197;&#29702;&#35299;&#32467;&#26500;&#21270;&#36755;&#20837;&#24182;&#36827;&#34892;&#28145;&#20837;&#25512;&#29702;&#20197;&#35299;&#20915;&#26032;&#22411;ML&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#23637;&#31034;&#20102;MLCopilot&#22312;&#35299;&#20915;&#22270;&#20687;&#20998;&#31867;&#65292;&#25991;&#26412;&#20998;&#31867;&#21644;&#34920;&#26684;&#20998;&#31867;&#19977;&#39033;&#20219;&#21153;&#26041;&#38754;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.14979</link><description>&lt;p&gt;
MLCopilot&#65306;&#37322;&#25918;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
MLCopilot: Unleashing the Power of Large Language Models in Solving Machine Learning Tasks. (arXiv:2304.14979v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14979
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#26694;&#26550;MLCopilot&#65292;&#36890;&#36807;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#25193;&#23637;&#20854;&#33021;&#21147;&#20197;&#29702;&#35299;&#32467;&#26500;&#21270;&#36755;&#20837;&#24182;&#36827;&#34892;&#28145;&#20837;&#25512;&#29702;&#20197;&#35299;&#20915;&#26032;&#22411;ML&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#23637;&#31034;&#20102;MLCopilot&#22312;&#35299;&#20915;&#22270;&#20687;&#20998;&#31867;&#65292;&#25991;&#26412;&#20998;&#31867;&#21644;&#34920;&#26684;&#20998;&#31867;&#19977;&#39033;&#20219;&#21153;&#26041;&#38754;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#39046;&#22495;&#21463;&#21040;&#20102;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#22240;&#27492;&#36880;&#28176;&#24341;&#21457;&#20102;&#23558;ML&#24212;&#29992;&#20110;&#29305;&#23450;&#22330;&#26223;&#30340;&#38656;&#27714;&#65292;&#20294;&#23454;&#29616;&#36215;&#26469;&#32791;&#26102;&#19988;&#19981;&#26131;&#12290; &#33258;&#21160;&#21270;&#35299;&#20915;ML&#20219;&#21153;&#65288;&#20363;&#22914;AutoML&#65289;&#30340;&#20027;&#35201;&#26041;&#27861;&#36890;&#24120;&#32791;&#36153;&#26102;&#38388;&#19988;&#38590;&#20197;&#29702;&#35299;&#12290; &#32780;&#19982;&#20043;&#30456;&#21453;&#65292;&#34429;&#28982;&#20154;&#31867;&#24037;&#31243;&#24072;&#20855;&#26377;&#29702;&#35299;&#20219;&#21153;&#21644;&#25512;&#29702;&#35299;&#20915;&#26041;&#26696;&#30340;&#38590;&#20197;&#32622;&#20449;&#30340;&#33021;&#21147;&#65292;&#20294;&#20182;&#20204;&#30340;&#32463;&#39564;&#21644;&#30693;&#35782;&#24448;&#24448;&#19981;&#20805;&#20998;&#19988;&#38590;&#20197;&#20511;&#21161;&#23450;&#37327;&#26041;&#27861;&#21033;&#29992;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;MLCopilot&#26469;&#24357;&#21512;&#26426;&#22120;&#26234;&#33021;&#21644;&#20154;&#31867;&#30693;&#35782;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;LLM&#26469;&#24320;&#21457;&#26032;&#22411;&#20219;&#21153;&#30340;ML&#35299;&#20915;&#26041;&#26696;&#12290; &#25105;&#20204;&#23637;&#31034;&#20102;&#25193;&#23637;LLM&#30340;&#33021;&#21147;&#20197;&#29702;&#35299;&#32467;&#26500;&#21270;&#36755;&#20837;&#24182;&#36827;&#34892;&#28145;&#20837;&#25512;&#29702;&#20197;&#35299;&#20915;&#26032;&#22411;ML&#20219;&#21153;&#30340;&#21487;&#33021;&#24615;&#12290; &#32463;&#36807;&#19968;&#20123;&#19987;&#38376;&#35774;&#35745;&#21518;&#65292;&#25105;&#20204;&#21457;&#29616;LLM&#21487;&#20197;&#65288;i&#65289;&#20174;&#20154;&#31867;&#32534;&#20889;&#30340;&#25991;&#20214;&#20013;&#35266;&#23519;&#29616;&#26377;&#30693;&#35782;&#65292;&#65288;ii&#65289;&#21046;&#23450;&#35299;&#20915;ML&#20219;&#21153;&#30340;&#20855;&#20307;&#27493;&#39588;&#12290; &#25105;&#20204;&#23545;&#22270;&#20687;&#20998;&#31867;&#65292;&#25991;&#26412;&#20998;&#31867;&#21644;&#34920;&#26684;&#20998;&#31867;&#19977;&#39033;&#20219;&#21153;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;MLCopilot&#22312;&#35299;&#20915;&#23454;&#38469;ML&#38382;&#39064;&#26041;&#38754;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of machine learning (ML) has gained widespread adoption, leading to a significant demand for adapting ML to specific scenarios, which is yet expensive and non-trivial. The predominant approaches towards the automation of solving ML tasks (e.g., AutoML) are often time consuming and hard to understand for human developers. In contrast, though human engineers have the incredible ability to understand tasks and reason about solutions, their experience and knowledge are often sparse and difficult to utilize by quantitative approaches. In this paper, we aim to bridge the gap between machine intelligence and human knowledge by introducing a novel framework MLCopilot, which leverages the state-of-the-art LLMs to develop ML solutions for novel tasks. We showcase the possibility of extending the capability of LLMs to comprehend structured inputs and perform thorough reasoning for solving novel ML tasks. And we find that, after some dedicated design, the LLM can (i) observe from the exi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#21033;&#29992;&#33258;&#26059;&#30005;&#23376;&#29289;&#29702;&#27700;&#24211;&#36827;&#34892;&#33258;&#27835;&#22411;&#38271;&#26399;&#39044;&#27979;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#24314;&#27169;&#28151;&#27788;&#26102;&#38388;&#24207;&#21015;&#21644;&#21160;&#24577;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#26159;&#36866;&#21512;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#36827;&#34892;&#23454;&#26102;&#23398;&#20064;&#30340;&#12290;&#36825;&#37324;&#25552;&#20986;&#30340;&#22522;&#20110;&#24494;&#26059;&#30913;&#38567;&#31359;&#32467;&#30340;&#28065;&#26059;&#23376;&#21487;&#20197;&#20316;&#20026;&#23454;&#29616;&#27492;&#31181;RC&#30340;&#21407;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.03343</link><description>&lt;p&gt;
&#33258;&#20027;&#39044;&#27979;&#21644;&#38271;&#26399;&#23478;&#24237;&#33021;&#28304;&#36127;&#33655;&#39044;&#27979;&#30340;&#33258;&#26059;&#30005;&#23376;&#29289;&#29702;&#27700;&#24211;
&lt;/p&gt;
&lt;p&gt;
Spintronic Physical Reservoir for Autonomous Prediction and Long-Term Household Energy Load Forecasting. (arXiv:2304.03343v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03343
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#21033;&#29992;&#33258;&#26059;&#30005;&#23376;&#29289;&#29702;&#27700;&#24211;&#36827;&#34892;&#33258;&#27835;&#22411;&#38271;&#26399;&#39044;&#27979;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#24314;&#27169;&#28151;&#27788;&#26102;&#38388;&#24207;&#21015;&#21644;&#21160;&#24577;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#26159;&#36866;&#21512;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#36827;&#34892;&#23454;&#26102;&#23398;&#20064;&#30340;&#12290;&#36825;&#37324;&#25552;&#20986;&#30340;&#22522;&#20110;&#24494;&#26059;&#30913;&#38567;&#31359;&#32467;&#30340;&#28065;&#26059;&#23376;&#21487;&#20197;&#20316;&#20026;&#23454;&#29616;&#27492;&#31181;RC&#30340;&#21407;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#33258;&#26059;&#30005;&#23376;&#29289;&#29702;&#27700;&#24211;&#36827;&#34892;&#20102;&#33258;&#27835;&#22411;&#38271;&#26399;&#39044;&#27979;&#12290;&#30001;&#20110;&#30913;&#21270;&#21160;&#21147;&#23398;&#30340;&#30701;&#26399;&#35760;&#24518;&#29305;&#24615;&#65292;&#27700;&#24211;&#29366;&#24577;&#20013;&#20135;&#29983;&#20102;&#38750;&#32447;&#24615;&#65292;&#21487;&#29992;&#20110;&#20351;&#29992;&#31616;&#21333;&#32447;&#24615;&#22238;&#24402;&#36827;&#34892;&#22312;&#32447;&#35757;&#32451;&#30340;&#38271;&#26399;&#39044;&#27979;&#20219;&#21153;&#12290;&#22312;&#39044;&#27979;&#38454;&#27573;&#65292;&#36755;&#20986;&#30452;&#25509;&#39304;&#20837;&#27700;&#24211;&#30340;&#36755;&#20837;&#20013;&#36827;&#34892;&#33258;&#27835;&#22411;&#39044;&#27979;&#12290;&#25105;&#20204;&#23558;&#25152;&#25552;&#20986;&#30340;&#27700;&#24211;&#29992;&#20110;&#24314;&#27169;&#35832;&#22914;Mackey-Glass&#31561;&#28151;&#27788;&#26102;&#38388;&#24207;&#21015;&#21644;&#21160;&#24577;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#22914;&#23478;&#24237;&#24314;&#31569;&#33021;&#32791;&#12290;&#30001;&#20110;&#21482;&#26377;RC&#30340;&#26368;&#21518;&#19968;&#23618;&#38656;&#35201;&#20351;&#29992;&#32447;&#24615;&#22238;&#24402;&#36827;&#34892;&#35757;&#32451;&#65292;&#22240;&#27492;&#23427;&#38750;&#24120;&#36866;&#21512;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#36827;&#34892;&#23454;&#26102;&#23398;&#20064;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22522;&#20110;&#24494;&#26059;&#30913;&#38567;&#31359;&#32467;&#30340;&#28065;&#26059;&#23376;&#21487;&#33021;&#29992;&#20316;&#21407;&#22411;RC&#65292;&#20294;&#20219;&#20309;&#20855;&#26377;&#38750;&#32447;&#24615;&#30913;&#21270;&#34892;&#20026;&#30340;&#32435;&#31859;&#30913;&#38567;&#36947;&#32467;&#37117;&#21487;&#20197;&#23454;&#29616;&#27492;&#31181;RC&#12290;&#36890;&#36807;&#27604;&#36739;&#25105;&#20204;&#30340;&#33258;&#26059;&#30005;&#23376;&#29289;&#29702;RC&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
In this study, we have shown autonomous long-term prediction with a spintronic physical reservoir. Due to the short-term memory property of the magnetization dynamics, non-linearity arises in the reservoir states which could be used for long-term prediction tasks using simple linear regression for online training. During the prediction stage, the output is directly fed to the input of the reservoir for autonomous prediction. We employ our proposed reservoir for the modeling of the chaotic time series such as Mackey-Glass and dynamic time-series data, such as household building energy loads. Since only the last layer of a RC needs to be trained with linear regression, it is well suited for learning in real time on edge devices. Here we show that a skyrmion based magnetic tunnel junction can potentially be used as a prototypical RC but any nanomagnetic magnetic tunnel junction with nonlinear magnetization behavior can implement such a RC. By comparing our spintronic physical RC approach 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#27969;&#24418;&#23398;&#20064;&#20013;&#24212;&#29992;&#26080;&#30896;&#25758;&#36816;&#36755;&#22270;&#30340;&#26041;&#27861;&#65292;&#20854;&#21487;&#20197;&#27604;OT&#22270;&#26356;&#20415;&#23452;&#22320;&#35745;&#31639;&#36317;&#31163;&#65292;&#24182;&#25552;&#20379;&#21333;&#20010;&#27010;&#29575;&#27979;&#24230;&#30340;&#24179;&#31227;&#21644;&#20280;&#32553;&#30340;&#31561;&#36317;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.00199</link><description>&lt;p&gt;
&#26080;&#30896;&#25758;&#36816;&#36755;&#22270;&#22312;&#27969;&#34892;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Applications of No-Collision Transportation Maps in Manifold Learning. (arXiv:2304.00199v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00199
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#27969;&#24418;&#23398;&#20064;&#20013;&#24212;&#29992;&#26080;&#30896;&#25758;&#36816;&#36755;&#22270;&#30340;&#26041;&#27861;&#65292;&#20854;&#21487;&#20197;&#27604;OT&#22270;&#26356;&#20415;&#23452;&#22320;&#35745;&#31639;&#36317;&#31163;&#65292;&#24182;&#25552;&#20379;&#21333;&#20010;&#27010;&#29575;&#27979;&#24230;&#30340;&#24179;&#31227;&#21644;&#20280;&#32553;&#30340;&#31561;&#36317;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24341;&#20837;&#20110;[Nurbekyan et al.&#65292;2020]&#30340;&#26080;&#30896;&#25758;&#36816;&#36755;&#22270;&#22312;&#22270;&#20687;&#25968;&#25454;&#30340;&#27969;&#24418;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#12290;&#36817;&#24180;&#26469;&#65292;&#22312;&#34920;&#31034;&#31867;&#20284;&#36816;&#21160;&#25110;&#21464;&#24418;&#29616;&#35937;&#30340;&#25968;&#25454;&#20013;&#65292;&#24212;&#29992;&#22522;&#20110;&#36816;&#36755;&#30340;&#36317;&#31163;&#21644;&#29305;&#24449;&#30340;&#30740;&#31350;&#22823;&#24133;&#22686;&#21152;&#12290;&#20107;&#23454;&#19978;&#65292;&#22266;&#23450;&#20301;&#32622;&#27604;&#36739;&#24378;&#24230;&#36890;&#24120;&#26080;&#27861;&#26174;&#31034;&#25968;&#25454;&#32467;&#26500;&#12290;&#22312;[Nurbekyan et al.&#65292;2020]&#20013;&#24320;&#21457;&#30340;&#26080;&#30896;&#25758;&#22270;&#21644;&#36317;&#31163;&#31867;&#20284;&#20110;&#26368;&#20248;&#20256;&#36755;(OT)&#22270;&#30340;&#20960;&#20309;&#29305;&#24449;&#20294;&#30001;&#20110;&#26080;&#38656;&#20248;&#21270;&#65292;&#35745;&#31639;&#25104;&#26412;&#35201;&#20415;&#23452;&#24471;&#22810;&#12290;&#26412;&#25991;&#35777;&#26126;&#26080;&#30896;&#25758;&#36317;&#31163;&#25552;&#20379;&#21333;&#20010;&#27010;&#29575;&#27979;&#24230;&#30340;&#24179;&#31227;(&#20998;&#21035;&#26159;&#20280;&#32553;)&#21644;&#35013;&#22791;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#30340;&#24179;&#31227;(&#20998;&#21035;&#26159;&#20280;&#32553;)&#21521;&#37327;&#20043;&#38388;&#30340;&#31561;&#36317;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#65292;&#26080;&#30896;&#25758;&#36816;&#36755;&#22270;&#20197;&#21450;OT&#21644;&#32447;&#24615;OT&#22270;&#65292;&#19968;&#33324;&#26469;&#35828;&#19981;&#33021;&#20026;&#26059;&#36716;&#25552;&#20379;&#31561;&#36317;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we investigate applications of no-collision transportation maps introduced in [Nurbekyan et. al., 2020] in manifold learning for image data. Recently, there has been a surge in applying transportation-based distances and features for data representing motion-like or deformation-like phenomena. Indeed, comparing intensities at fixed locations often does not reveal the data structure. No-collision maps and distances developed in [Nurbekyan et. al., 2020] are sensitive to geometric features similar to optimal transportation (OT) maps but much cheaper to compute due to the absence of optimization. In this work, we prove that no-collision distances provide an isometry between translations (respectively dilations) of a single probability measure and the translation (respectively dilation) vectors equipped with a Euclidean distance. Furthermore, we prove that no-collision transportation maps, as well as OT and linearized OT maps, do not in general provide an isometry for rotatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24369;&#21442;&#25968;&#32467;&#26500;&#20551;&#35774;&#30340;&#40657;&#30418;&#31243;&#24207;&#65292;&#29992;&#20110;&#20272;&#35745;&#32479;&#35745;&#27169;&#22411;&#21442;&#25968;&#12290;&#35813;&#31243;&#24207;&#21487;&#20197;&#25104;&#21151;&#22320;&#20174;&#20855;&#26377;&#22797;&#26434;&#31354;&#38388;&#30456;&#20851;&#30340;&#38750;&#39640;&#26031;&#27169;&#22411;&#20013;&#20272;&#35745;&#21644;&#37327;&#21270;&#21442;&#25968;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.15041</link><description>&lt;p&gt;
&#26397;&#40657;&#30418;&#21442;&#25968;&#20272;&#35745;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
Towards black-box parameter estimation. (arXiv:2303.15041v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15041
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24369;&#21442;&#25968;&#32467;&#26500;&#20551;&#35774;&#30340;&#40657;&#30418;&#31243;&#24207;&#65292;&#29992;&#20110;&#20272;&#35745;&#32479;&#35745;&#27169;&#22411;&#21442;&#25968;&#12290;&#35813;&#31243;&#24207;&#21487;&#20197;&#25104;&#21151;&#22320;&#20174;&#20855;&#26377;&#22797;&#26434;&#31354;&#38388;&#30456;&#20851;&#30340;&#38750;&#39640;&#26031;&#27169;&#22411;&#20013;&#20272;&#35745;&#21644;&#37327;&#21270;&#21442;&#25968;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#26368;&#36817;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#20272;&#35745;&#32479;&#35745;&#27169;&#22411;&#21442;&#25968;&#30340;&#25104;&#21151;&#24037;&#20855;&#65292;&#27169;&#25311;&#23481;&#26131;&#20294;&#20284;&#28982;&#35745;&#31639;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20294;&#36825;&#20123;&#26041;&#27861;&#30340;&#25104;&#21151;&#21462;&#20915;&#20110;&#27169;&#25311;&#20986;&#21487;&#20197;&#20805;&#20998;&#22797;&#21046;&#35266;&#23519;&#25968;&#25454;&#30340;&#21442;&#25968;&#65292;&#24182;&#19988;&#30446;&#21069;&#32570;&#20047;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#20135;&#29983;&#36825;&#20123;&#27169;&#25311;&#25968;&#25454;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#22522;&#20110;&#24369;&#21442;&#25968;&#32467;&#26500;&#20551;&#35774;&#20272;&#35745;&#32479;&#35745;&#27169;&#22411;&#21442;&#25968;&#30340;&#26032;&#30340;&#40657;&#30418;&#31243;&#24207;&#12290;&#23545;&#20110;&#20284;&#28982;&#20989;&#25968;&#26377;&#36739;&#39057;&#32321;&#20986;&#29616;&#30340;&#33391;&#22909;&#32467;&#26500;&#30340;&#24773;&#20917;&#65292;&#22914;&#26102;&#38388;&#24207;&#21015;&#65292;&#36825;&#26159;&#36890;&#36807;&#22312;&#24191;&#27867;&#30340;&#27169;&#25311;&#25968;&#25454;&#24211;&#19978;&#39044;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26469;&#23454;&#29616;&#30340;&#65292;&#35813;&#25968;&#25454;&#24211;&#28085;&#30422;&#20102;&#21508;&#31181;&#25968;&#25454;&#22823;&#23567;&#30340;&#33539;&#22260;&#12290;&#23545;&#20110;&#20854;&#20182;&#31867;&#22411;&#30340;&#22797;&#26434;&#20381;&#36182;&#20851;&#31995;&#65292;&#21017;&#38656;&#35201;&#19968;&#20010;&#36845;&#20195;&#30340;&#31639;&#27861;&#26469;&#25351;&#23548;&#22810;&#36718;&#27491;&#30830;&#21442;&#25968;&#21306;&#22495;&#30340;&#27169;&#25311;&#12290;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#25104;&#21151;&#22320;&#20174;&#20855;&#26377;&#22797;&#26434;&#31354;&#38388;&#30456;&#20851;&#30340;&#38750;&#39640;&#26031;&#27169;&#22411;&#20013;&#20272;&#35745;&#21644;&#37327;&#21270;&#21442;&#25968;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning algorithms have recently shown to be a successful tool in estimating parameters of statistical models for which simulation is easy, but likelihood computation is challenging. But the success of these approaches depends on simulating parameters that sufficiently reproduce the observed data, and, at present, there is a lack of efficient methods to produce these simulations. We develop new black-box procedures to estimate parameters of statistical models based only on weak parameter structure assumptions. For well-structured likelihoods with frequent occurrences, such as in time series, this is achieved by pre-training a deep neural network on an extensive simulated database that covers a wide range of data sizes. For other types of complex dependencies, an iterative algorithm guides simulations to the correct parameter region in multiple rounds. These approaches can successfully estimate and quantify the uncertainty of parameters from non-Gaussian models with complex spatia
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#35777;&#21644;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#20960;&#31181;AI&#25991;&#26412;&#26816;&#27979;&#22120;&#19981;&#21487;&#38752;&#12290;&#25913;&#20889;&#25915;&#20987;&#21487;&#20197;&#30772;&#35299;&#22810;&#31181;&#26816;&#27979;&#22120;&#65292;&#21253;&#25324;&#27700;&#21360;&#26041;&#26696;&#12289;&#31070;&#32463;&#32593;&#32476;&#26816;&#27979;&#22120;&#21644;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#12290;&#21363;&#20351;&#26159;&#26368;&#22909;&#30340;&#26816;&#27979;&#22120;&#65292;&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#19968;&#27493;&#25552;&#21319;&#65292;&#24615;&#33021;&#20063;&#20250;&#19979;&#38477;&#12290;&#22240;&#27492;&#65292;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#21487;&#38752;&#26816;&#27979;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2303.11156</link><description>&lt;p&gt;
AI&#29983;&#25104;&#30340;&#25991;&#26412;&#26159;&#21542;&#21487;&#38752;&#22320;&#26816;&#27979;&#20986;&#26469;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can AI-Generated Text be Reliably Detected?. (arXiv:2303.11156v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11156
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#35777;&#21644;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#20960;&#31181;AI&#25991;&#26412;&#26816;&#27979;&#22120;&#19981;&#21487;&#38752;&#12290;&#25913;&#20889;&#25915;&#20987;&#21487;&#20197;&#30772;&#35299;&#22810;&#31181;&#26816;&#27979;&#22120;&#65292;&#21253;&#25324;&#27700;&#21360;&#26041;&#26696;&#12289;&#31070;&#32463;&#32593;&#32476;&#26816;&#27979;&#22120;&#21644;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#12290;&#21363;&#20351;&#26159;&#26368;&#22909;&#30340;&#26816;&#27979;&#22120;&#65292;&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#19968;&#27493;&#25552;&#21319;&#65292;&#24615;&#33021;&#20063;&#20250;&#19979;&#38477;&#12290;&#22240;&#27492;&#65292;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#21487;&#38752;&#26816;&#27979;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#23454;&#35777;&#21644;&#29702;&#35770;&#20004;&#20010;&#26041;&#38754;&#34920;&#26126;&#65292;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#20960;&#31181;AI&#25991;&#26412;&#26816;&#27979;&#22120;&#24182;&#19981;&#21487;&#38752;&#12290;&#20174;&#23454;&#36341;&#19978;&#26469;&#35828;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36731;&#37327;&#32423;&#30340;&#25913;&#20889;&#22120;&#24212;&#29992;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19978;&#21487;&#20197;&#30772;&#35299;&#19968;&#31995;&#21015;&#30340;&#26816;&#27979;&#22120;&#65292;&#21253;&#25324;&#20351;&#29992;&#27700;&#21360;&#26041;&#26696;&#12289;&#31070;&#32463;&#32593;&#32476;&#26816;&#27979;&#22120;&#21644;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#26088;&#22312;&#36530;&#36991;&#25913;&#20889;&#25915;&#20987;&#30340;&#22522;&#20110;&#26816;&#32034;&#30340;&#26816;&#27979;&#22120;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#36882;&#24402;&#25913;&#20889;&#30340;&#25915;&#20987;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#19978;&#30340;&#19981;&#21487;&#33021;&#32467;&#26524;&#65292;&#25351;&#20986;&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#21464;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#21644;&#26356;&#25797;&#38271;&#27169;&#20223;&#20154;&#31867;&#25991;&#26412;&#65292;&#22312;&#26368;&#22909;&#30340;&#26816;&#27979;&#22120;&#24615;&#33021;&#20250;&#19979;&#38477;&#12290;&#23545;&#20110;&#19968;&#20010;&#36275;&#22815;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#27169;&#20223;&#20154;&#31867;&#25991;&#26412;&#65292;&#21363;&#20351;&#26368;&#20339;&#30340;&#26816;&#27979;&#22120;&#30340;&#34920;&#29616;&#21482;&#27604;&#38543;&#26426;&#20998;&#31867;&#22120;&#22909;&#19978;&#19968;&#28857;&#28857;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36275;&#22815;&#27010;&#25324;&#29305;&#23450;&#30340;&#22330;&#26223;&#65292;&#22914;&#25913;&#20889;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, both empirically and theoretically, we show that several AI-text detectors are not reliable in practical scenarios. Empirically, we show that paraphrasing attacks, where a light paraphraser is applied on top of a large language model (LLM), can break a whole range of detectors, including ones using watermarking schemes as well as neural network-based detectors and zero-shot classifiers. Our experiments demonstrate that retrieval-based detectors, designed to evade paraphrasing attacks, are still vulnerable to recursive paraphrasing. We then provide a theoretical impossibility result indicating that as language models become more sophisticated and better at emulating human text, the performance of even the best-possible detector decreases. For a sufficiently advanced language model seeking to imitate human text, even the best-possible detector may only perform marginally better than a random classifier. Our result is general enough to capture specific scenarios such as par
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#39038;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#38543;&#26426;&#25511;&#21046;&#38382;&#39064;&#21644;&#21338;&#24328;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#30528;&#37325;&#20171;&#32461;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#35299;&#20915;&#39640;&#32500;&#24230;&#21644;&#38750;&#24120;&#22797;&#26434;&#32467;&#26500;&#24773;&#20917;&#19979;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.10257</link><description>&lt;p&gt;
&#29992;&#20110;&#38543;&#26426;&#25511;&#21046;&#21644;&#21338;&#24328;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#26368;&#26032;&#21457;&#23637;
&lt;/p&gt;
&lt;p&gt;
Recent Developments in Machine Learning Methods for Stochastic Control and Games. (arXiv:2303.10257v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10257
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#38543;&#26426;&#25511;&#21046;&#38382;&#39064;&#21644;&#21338;&#24328;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#30528;&#37325;&#20171;&#32461;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#35299;&#20915;&#39640;&#32500;&#24230;&#21644;&#38750;&#24120;&#22797;&#26434;&#32467;&#26500;&#24773;&#20917;&#19979;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#26368;&#20248;&#25511;&#21046;&#21644;&#21338;&#24328;&#24050;&#32463;&#22312;&#37329;&#34701;&#12289;&#32463;&#27982;&#23398;&#12289;&#31038;&#20250;&#31185;&#23398;&#12289;&#26426;&#22120;&#20154;&#21644;&#33021;&#28304;&#31649;&#29702;&#31561;&#39046;&#22495;&#20013;&#25214;&#21040;&#20102;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#35768;&#22810;&#30495;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#37117;&#28041;&#21450;&#21040;&#22797;&#26434;&#30340;&#27169;&#22411;&#65292;&#36825;&#25512;&#21160;&#20102;&#20808;&#36827;&#30340;&#25968;&#20540;&#26041;&#27861;&#30340;&#21457;&#23637;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#35745;&#31639;&#26041;&#27861;&#24050;&#32463;&#21457;&#23637;&#29992;&#20110;&#38543;&#26426;&#25511;&#21046;&#38382;&#39064;&#21644;&#21338;&#24328;&#12290;&#25105;&#20204;&#22238;&#39038;&#36825;&#20123;&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#24050;&#32463;&#35299;&#38145;&#20102;&#39640;&#32500;&#24230;&#21644;&#38750;&#24120;&#22797;&#26434;&#32467;&#26500;&#24773;&#20917;&#19979;&#35299;&#20915;&#27492;&#31867;&#38382;&#39064;&#30340;&#21487;&#33021;&#24615;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#36825;&#26159;&#20256;&#32479;&#25968;&#20540;&#26041;&#27861;&#26080;&#27861;&#23436;&#25104;&#30340;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20027;&#35201;&#32771;&#34385;&#36830;&#32493;&#26102;&#38388;&#21644;&#36830;&#32493;&#31354;&#38388;&#35774;&#32622;&#12290;&#35768;&#22810;&#26032;&#26041;&#27861;&#22522;&#20110;&#26368;&#36817;&#29992;&#20110;&#39640;&#32500;&#20559;&#24494;&#20998;&#26041;&#31243;&#25110;&#21453;&#21521;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#25110;&#32773;&#22522;&#20110;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#30340;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#36825;&#23548;&#33268;&#20102;&#31361;&#30772;&#24615;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic optimal control and games have found a wide range of applications, from finance and economics to social sciences, robotics and energy management. Many real-world applications involve complex models which have driven the development of sophisticated numerical methods. Recently, computational methods based on machine learning have been developed for stochastic control problems and games. We review such methods, with a focus on deep learning algorithms that have unlocked the possibility to solve such problems even when the dimension is high or when the structure is very complex, beyond what is feasible with traditional numerical methods. Here, we consider mostly the continuous time and continuous space setting. Many of the new approaches build on recent neural-network based methods for high-dimensional partial differential equations or backward stochastic differential equations, or on model-free reinforcement learning for Markov decision processes that have led to breakthrough 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#20013;&#26631;&#35760;&#19981;&#20934;&#30830;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#23545;&#27604;&#26631;&#31614;&#26657;&#27491;&#26041;&#27861;&#12290;&#26041;&#27861;&#21253;&#25324;&#23558;&#23545;&#27604;&#23398;&#20064;&#31639;&#27861;&#34701;&#20837;&#20803;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;&#36890;&#36807;Gramian angular field&#21644;&#20195;&#34920;&#23398;&#20064;&#23558;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#29983;&#25104;&#22270;&#20687;&#65292;&#20174;&#32780;&#33258;&#21160;&#29983;&#25104;&#20934;&#30830;&#26631;&#31614;&#65292;&#24182;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.08103</link><description>&lt;p&gt;
&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#30340;&#20803;&#23545;&#27604;&#26631;&#31614;&#26657;&#27491;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Meta contrastive label correction for financial time series. (arXiv:2303.08103v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08103
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#20013;&#26631;&#35760;&#19981;&#20934;&#30830;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#23545;&#27604;&#26631;&#31614;&#26657;&#27491;&#26041;&#27861;&#12290;&#26041;&#27861;&#21253;&#25324;&#23558;&#23545;&#27604;&#23398;&#20064;&#31639;&#27861;&#34701;&#20837;&#20803;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;&#36890;&#36807;Gramian angular field&#21644;&#20195;&#34920;&#23398;&#20064;&#23558;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#29983;&#25104;&#22270;&#20687;&#65292;&#20174;&#32780;&#33258;&#21160;&#29983;&#25104;&#20934;&#30830;&#26631;&#31614;&#65292;&#24182;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37329;&#34701;&#24212;&#29992;&#65288;&#22914;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#65289;&#36890;&#24120;&#38754;&#20020;&#26631;&#35760;&#19981;&#20934;&#30830;&#30340;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#20803;&#23545;&#27604;&#26631;&#31614;&#26657;&#27491;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#29983;&#25104;&#20934;&#30830;&#26631;&#31614;&#65292;&#24182;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#23558;&#23545;&#27604;&#23398;&#20064;&#31639;&#27861;&#34701;&#20837;&#20803;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;&#36890;&#36807;Gramian angular field&#21644;&#20195;&#34920;&#23398;&#20064;&#23558;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#29983;&#25104;&#22270;&#20687;&#65292;&#20174;&#32780;&#33258;&#21160;&#29983;&#25104;&#20934;&#30830;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;
Financial applications such as stock price forecasting, usually face an issue that under the predefined labeling rules, it is hard to accurately predict the directions of stock movement. This is because traditional ways of labeling, taking Triple Barrier Method, for example, usually gives us inaccurate or even corrupted labels. To address this issue, we focus on two main goals. One is that our proposed method can automatically generate correct labels for noisy time series patterns, while at the same time, the method is capable of boosting classification performance on this new labeled dataset. Based on the aforementioned goals, our approach has the following three novelties: First, we fuse a new contrastive learning algorithm into the meta-learning framework to estimate correct labels iteratively when updating the classification model inside. Moreover, we utilize images generated from time series data through Gramian angular field and representative learning. Most important of all, we 
&lt;/p&gt;</description></item><item><title>Lumos&#26159;&#31532;&#19968;&#20010;&#25903;&#25345;&#33410;&#28857;&#32423;&#32852;&#37030;&#22270;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#20855;&#26377;&#29305;&#24449;&#21644;&#24230;&#25968;&#20445;&#25252;&#21151;&#33021;&#12290;&#23427;&#37319;&#29992;&#20102;&#26641;&#26500;&#36896;&#22120;&#21644;&#21435;&#20013;&#24515;&#21270;&#33410;&#28857;&#32858;&#21512;&#31574;&#30053;&#26469;&#25552;&#39640;&#22270;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.00492</link><description>&lt;p&gt;
Lumos: &#38024;&#23545;&#20998;&#24067;&#24335;&#35774;&#22791;&#30340;&#24322;&#26500;&#24863;&#30693;&#32852;&#37030;&#22270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Lumos: Heterogeneity-aware Federated Graph Learning over Decentralized Devices. (arXiv:2303.00492v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00492
&lt;/p&gt;
&lt;p&gt;
Lumos&#26159;&#31532;&#19968;&#20010;&#25903;&#25345;&#33410;&#28857;&#32423;&#32852;&#37030;&#22270;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#20855;&#26377;&#29305;&#24449;&#21644;&#24230;&#25968;&#20445;&#25252;&#21151;&#33021;&#12290;&#23427;&#37319;&#29992;&#20102;&#26641;&#26500;&#36896;&#22120;&#21644;&#21435;&#20013;&#24515;&#21270;&#33410;&#28857;&#32858;&#21512;&#31574;&#30053;&#26469;&#25552;&#39640;&#22270;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#33021;&#22815;&#22788;&#29702;&#22270;&#32467;&#26500;&#25968;&#25454;&#65292;&#22240;&#27492;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#32593;&#32476;&#24212;&#29992;&#21644;&#31995;&#32479;&#20013;&#24191;&#27867;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#23545;&#25968;&#25454;&#38544;&#31169;&#30340;&#26085;&#30410;&#20851;&#27880;&#20005;&#37325;&#25361;&#25112;&#20102;&#20256;&#32479;&#30340;&#38598;&#20013;&#24335;&#27169;&#22411;&#35757;&#32451;&#33539;&#24335;&#65292;&#20854;&#20013;&#26381;&#21153;&#22120;&#25345;&#26377;&#25152;&#26377;&#22270;&#20449;&#24687;&#12290;&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#21327;&#20316;&#35745;&#31639;&#33539;&#24335;&#65292;&#21487;&#20197;&#22312;&#19981;&#38598;&#20013;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#12290;&#29616;&#26377;&#30340;&#32852;&#37030;GNN&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#23458;&#25143;&#31471;&#25345;&#26377;&#19981;&#21516;&#30340;&#22270;&#25110;&#23376;&#22270;&#30340;&#31995;&#32479;&#19978;&#12290;&#28982;&#32780;&#65292;&#36824;&#27809;&#26377;&#23545;&#27599;&#20010;&#23458;&#25143;&#31471;&#20165;&#30693;&#36947;&#20854;&#30452;&#25509;&#37051;&#23621;&#30340;&#23454;&#38469;&#33410;&#28857;&#32423;&#32852;&#37030;&#24773;&#20917;&#36827;&#34892;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#25903;&#25345;&#33410;&#28857;&#32423;&#32852;&#37030;&#22270;&#30340;&#26377;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#32852;&#37030;GNN&#26694;&#26550;Lumos&#65292;&#23427;&#20855;&#26377;&#29305;&#24449;&#21644;&#24230;&#25968;&#20445;&#25252;&#21151;&#33021;&#12290;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#20010;&#26641;&#26500;&#36896;&#22120;&#65292;&#20197;&#25552;&#39640;&#22312;&#26377;&#38480;&#32467;&#26500;&#20449;&#24687;&#19979;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37051;&#23621;&#20449;&#24687;&#30340;&#21435;&#20013;&#24515;&#21270;&#33410;&#28857;&#32858;&#21512;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNN) have been widely deployed in real-world networked applications and systems due to their capability to handle graph-structured data. However, the growing awareness of data privacy severely challenges the traditional centralized model training paradigm, where a server holds all the graph information. Federated learning is an emerging collaborative computing paradigm that allows model training without data centralization. Existing federated GNN studies mainly focus on systems where clients hold distinctive graphs or sub-graphs. The practical node-level federated situation, where each client is only aware of its direct neighbors, has yet to be studied. In this paper, we propose the first federated GNN framework called Lumos that supports supervised and unsupervised learning with feature and degree protection on node-level federated graphs. We first design a tree constructor to improve the representation capability given the limited structural information. We fur
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#23618;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#31639;&#27861;&#65292;&#20351;&#29992;&#30340;&#26799;&#24230;&#35745;&#31639;&#27425;&#25968; $O((n+m)^{\frac{1}{2}}\varepsilon^{-1})$&#65292;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;&#26041;&#38754;&#26159;&#26368;&#20248;&#30340;&#12290;</title><link>http://arxiv.org/abs/2302.08766</link><description>&lt;p&gt;
&#19968;&#31181;&#21452;&#23618;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#31639;&#27861;&#30340;&#19979;&#30028;&#21644;&#36817;&#20284;&#26368;&#20248;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Lower Bound and a Near-Optimal Algorithm for Bilevel Empirical Risk Minimization. (arXiv:2302.08766v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08766
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#23618;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#31639;&#27861;&#65292;&#20351;&#29992;&#30340;&#26799;&#24230;&#35745;&#31639;&#27425;&#25968; $O((n+m)^{\frac{1}{2}}\varepsilon^{-1})$&#65292;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;&#26041;&#38754;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21452;&#23618;&#26368;&#20248;&#21270;&#38382;&#39064;&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#20013;&#12290;&#22312;&#35768;&#22810;&#23454;&#38469;&#24773;&#20917;&#19979;&#65292;&#19978;&#23618;&#21644;&#19979;&#23618;&#30446;&#26631;&#23545;&#24212;&#20110;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#38382;&#39064;&#65292;&#24182;&#22240;&#27492;&#20855;&#26377;&#24635;&#21644;&#32467;&#26500;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33879;&#21517;&#30340;SARAH&#31639;&#27861;&#30340;&#21452;&#23618;&#25193;&#23637;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#38656;&#35201;$\mathcal {O}((n+m)^{\frac{1}{2}}\varepsilon ^{-1})$&#27425;&#26799;&#24230;&#35745;&#31639;&#25165;&#33021;&#23454;&#29616;$\varepsilon$&#31283;&#23450;&#24615;&#65292;&#20854;&#20013;$n+m$&#26159;&#26679;&#26412;&#24635;&#25968;&#65292;&#36825;&#27604;&#20808;&#21069;&#25152;&#26377;&#30340;&#21452;&#23618;&#31639;&#27861;&#37117;&#35201;&#22909;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#19979;&#30028;&#65292;&#29992;&#20110;&#24471;&#21040;&#21452;&#23618;&#38382;&#39064;&#30340;&#30446;&#26631;&#20989;&#25968;&#30340;&#36817;&#20284;&#31283;&#23450;&#28857;&#25152;&#38656;&#30340;oracle&#35843;&#29992;&#27425;&#25968;&#12290;&#36825;&#20010;&#19979;&#30028;&#27491;&#26159;&#25105;&#20204;&#30340;&#31639;&#27861;&#25152;&#36798;&#21040;&#30340;&#65292;&#22240;&#27492;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;&#26041;&#38754;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bilevel optimization problems, which are problems where two optimization problems are nested, have more and more applications in machine learning. In many practical cases, the upper and the lower objectives correspond to empirical risk minimization problems and therefore have a sum structure. In this context, we propose a bilevel extension of the celebrated SARAH algorithm. We demonstrate that the algorithm requires $\mathcal{O}((n+m)^{\frac12}\varepsilon^{-1})$ gradient computations to achieve $\varepsilon$-stationarity with $n+m$ the total number of samples, which improves over all previous bilevel algorithms. Moreover, we provide a lower bound on the number of oracle calls required to get an approximate stationary point of the objective function of the bilevel problem. This lower bound is attained by our algorithm, which is therefore optimal in terms of sample complexity.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20004;&#31181;&#22522;&#20110;&#20256;&#36755;&#26144;&#23556;&#30340;&#25277;&#26679;&#26041;&#27861;&#65292;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#27969;&#30340;&#25552;&#35758;&#21487;&#20197;&#22788;&#29702;&#22810;&#23792;&#30446;&#26631;&#65292;&#22312;&#39640;&#32500;&#24230;&#21644;&#35757;&#32451;&#19981;&#33391;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#20381;&#36182;&#20110;&#37325;&#26032;&#21442;&#25968;&#21270;&#30340;&#26041;&#27861;&#26356;&#21152;&#31283;&#20581;&#12290;</title><link>http://arxiv.org/abs/2302.04763</link><description>&lt;p&gt;
&#20851;&#20110;&#20351;&#29992;&#36817;&#20284;&#20256;&#36755;&#26144;&#23556;&#36827;&#34892;&#25277;&#26679;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Sampling with Approximate Transport Maps. (arXiv:2302.04763v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04763
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20004;&#31181;&#22522;&#20110;&#20256;&#36755;&#26144;&#23556;&#30340;&#25277;&#26679;&#26041;&#27861;&#65292;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#27969;&#30340;&#25552;&#35758;&#21487;&#20197;&#22788;&#29702;&#22810;&#23792;&#30446;&#26631;&#65292;&#22312;&#39640;&#32500;&#24230;&#21644;&#35757;&#32451;&#19981;&#33391;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#20381;&#36182;&#20110;&#37325;&#26032;&#21442;&#25968;&#21270;&#30340;&#26041;&#27861;&#26356;&#21152;&#31283;&#20581;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#20998;&#24067;&#36716;&#21270;&#20026;&#26131;&#20110;&#22788;&#29702;&#30340;&#20998;&#24067;&#65292;&#20256;&#36755;&#26144;&#23556;&#21487;&#20197;&#31616;&#21270;&#20855;&#26377;&#38750;&#24179;&#20961;&#20960;&#20309;&#32467;&#26500;&#30340;&#20998;&#24067;&#30340;&#25277;&#26679;&#12290;&#38543;&#30528;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#30340;&#20256;&#32479;&#27969;&#65288;NF&#65289;&#30340;&#21457;&#23637;&#65292;&#36825;&#31181;&#26041;&#27861;&#30340;&#28508;&#21147;&#19981;&#26029;&#25552;&#39640;&#12290;NF&#22686;&#24378;&#37319;&#26679;&#22120;&#26368;&#36817;&#25552;&#20986;&#20102;&#23558;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#26041;&#27861;&#19982;&#65288;i&#65289;&#26469;&#33258;&#27969;&#30340;&#25552;&#35758;&#32472;&#21046;&#25110;&#65288;ii&#65289;&#22522;&#20110;&#27969;&#30340;&#37325;&#26032;&#21442;&#25968;&#21270;&#30456;&#32467;&#21512;&#12290;&#22312;&#36825;&#20004;&#31181;&#24773;&#20917;&#19979;&#65292;&#23398;&#20064;&#21040;&#30340;&#20256;&#36755;&#30340;&#36136;&#37327;&#20250;&#24433;&#21709;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#39318;&#27425;&#38416;&#26126;&#20102;&#36825;&#20004;&#31181;&#26041;&#27861;&#30340;&#30456;&#23545;&#20248;&#21183;&#21644;&#21155;&#21183;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24471;&#20986;&#32467;&#35770;&#65306;&#30452;&#21040;&#20013;&#31561;&#32500;&#24230;&#65292;&#21487;&#20197;&#21487;&#38752;&#22320;&#20351;&#29992;&#22522;&#20110;&#27969;&#30340;&#25552;&#35758;&#22788;&#29702;&#22810;&#23792;&#30446;&#26631;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22312;&#39640;&#32500;&#24230;&#21644;&#35757;&#32451;&#19981;&#33391;&#30340;&#24773;&#20917;&#19979;&#65292;&#20381;&#36182;&#20110;&#37325;&#26032;&#21442;&#25968;&#21270;&#30340;&#26041;&#27861;&#22312;&#22810;&#27169;&#24335;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#20294;&#20854;&#20182;&#26041;&#38754;&#26356;&#20026;&#31283;&#20581;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transport maps can ease the sampling of distributions with non-trivial geometries by transforming them into distributions that are easier to handle. The potential of this approach has risen with the development of Normalizing Flows (NF) which are maps parameterized with deep neural networks trained to push a reference distribution towards a target. NF-enhanced samplers recently proposed blend (Markov chain) Monte Carlo methods with either (i) proposal draws from the flow or (ii) a flow-based reparametrization. In both cases, the quality of the learned transport conditions performance. The present work clarifies for the first time the relative strengths and weaknesses of these two approaches. Our study concludes that multimodal targets can be reliably handled with flow-based proposals up to moderately high dimensions. In contrast, methods relying on reparametrization struggle with multimodality but are more robust otherwise in high-dimensional settings and under poor training. To furthe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;&#22270;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#65292;&#20027;&#35201;&#25506;&#35752;&#20102;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#28508;&#21147;&#21644;&#26368;&#20808;&#36827;&#30340;&#22270;&#24322;&#24120;&#26816;&#27979;&#25216;&#26415;&#22312;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2302.00058</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#65306;&#32508;&#36848;(arXiv&#65306;2302.00058v2 [cs.LG]&#26356;&#26032;)
&lt;/p&gt;
&lt;p&gt;
Graph-based Time-Series Anomaly Detection: A Survey. (arXiv:2302.00058v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00058
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;&#22270;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#65292;&#20027;&#35201;&#25506;&#35752;&#20102;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#28508;&#21147;&#21644;&#26368;&#20808;&#36827;&#30340;&#22270;&#24322;&#24120;&#26816;&#27979;&#25216;&#26415;&#22312;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25216;&#26415;&#30340;&#21457;&#23637;&#65292;&#35768;&#22810;&#31995;&#32479;&#25345;&#32493;&#25910;&#38598;&#22823;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#22914;&#30005;&#23376;&#21830;&#21153;&#12289;&#32593;&#32476;&#23433;&#20840;&#12289;&#36710;&#36742;&#32500;&#25252;&#21644;&#21307;&#30103;&#30417;&#27979;&#31561;&#39046;&#22495;&#65292;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#24050;&#25104;&#20026;&#37325;&#35201;&#30340;&#20219;&#21153;&#12290;&#20294;&#30001;&#20110;&#38656;&#35201;&#21516;&#26102;&#32771;&#34385;&#21464;&#37327;&#20869;&#37096;&#21644;&#21464;&#37327;&#38388;&#30340;&#20381;&#36182;&#24615;&#65292;&#36825;&#19968;&#20219;&#21153;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#22312;&#35299;&#20915;&#35813;&#39046;&#22495;&#30340;&#38590;&#39064;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;&#26412;&#32508;&#36848;&#20840;&#38754;&#32780;&#26368;&#26032;&#22320;&#22238;&#39038;&#20102;&#22522;&#20110;&#22270;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;(G-TSAD)&#12290;&#39318;&#20808;&#25506;&#35752;&#20102;&#22270;&#34920;&#31034;&#23398;&#20064;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#28982;&#21518;&#22312;&#26102;&#38388;&#24207;&#21015;&#32972;&#26223;&#19979;&#22238;&#39038;&#20102;&#26368;&#20808;&#36827;&#30340;&#22270;&#24322;&#24120;&#26816;&#27979;&#25216;&#26415;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#12290;&#26368;&#21518;&#65292;&#35752;&#35770;&#20102;&#36825;&#20123;&#25216;&#26415;&#22914;&#20309;&#24212;&#29992;&#20110;&#23454;&#38469;&#31995;&#32479;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the recent advances in technology, a wide range of systems continue to collect a large amount of data over time and thus generate time series. Time-Series Anomaly Detection (TSAD) is an important task in various time-series applications such as e-commerce, cybersecurity, vehicle maintenance, and healthcare monitoring. However, this task is very challenging as it requires considering both the intra-variable dependency and the inter-variable dependency, where a variable can be defined as an observation in time series data. Recent graph-based approaches have made impressive progress in tackling the challenges of this field. In this survey, we conduct a comprehensive and up-to-date review of Graph-based TSAD (G-TSAD). First, we explore the significant potential of graph representation learning for time-series data. Then, we review state-of-the-art graph anomaly detection techniques in the context of time series and discuss their strengths and drawbacks. Finally, we discuss the technic
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#36870;&#21487;&#35299;&#24615;&#21644;&#23433;&#20840;&#24615;&#30340;&#27010;&#24565;&#65292;&#20197;&#21450;&#20854;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#12290;&#35770;&#25991;&#25552;&#20379;&#20102;&#27169;&#22411;&#31034;&#20363;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#22686;&#21152;&#29992;&#25143;&#25968;&#37327;&#26469;&#22686;&#21152;&#21487;&#35299;&#24615;&#21644;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.14115</link><description>&lt;p&gt;
&#36870;&#21487;&#35299;&#24615;&#21644;&#23433;&#20840;&#24615;&#21450;&#20854;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Inverse Solvability and Security with Applications to Federated Learning. (arXiv:2211.14115v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14115
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#36870;&#21487;&#35299;&#24615;&#21644;&#23433;&#20840;&#24615;&#30340;&#27010;&#24565;&#65292;&#20197;&#21450;&#20854;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#12290;&#35770;&#25991;&#25552;&#20379;&#20102;&#27169;&#22411;&#31034;&#20363;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#22686;&#21152;&#29992;&#25143;&#25968;&#37327;&#26469;&#22686;&#21152;&#21487;&#35299;&#24615;&#21644;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#36870;&#21487;&#35299;&#24615;&#21644;&#23433;&#20840;&#24615;&#30340;&#27010;&#24565;&#65292;&#36866;&#29992;&#20110;&#19968;&#33324;&#32447;&#24615;&#21069;&#21521;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#20854;&#24212;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#20013;&#20351;&#29992;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#36825;&#26679;&#30340;&#27169;&#22411;&#30340;&#31034;&#20363;&#65292;&#20854;&#36870;&#21487;&#35299;&#24615;&#21644;&#23433;&#20840;&#24615;&#22312;&#26412;&#25991;&#20013;&#24471;&#21040;&#23450;&#20041;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#21442;&#19982;&#32473;&#23450;&#36845;&#20195;&#30340;&#22823;&#37327;&#29992;&#25143;&#26469;&#22686;&#21152;&#21487;&#35299;&#24615;&#21644;&#23433;&#20840;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#25152;&#25552;&#20986;&#27010;&#24565;&#30340;&#21487;&#33021;&#25193;&#23637;&#65292;&#21253;&#25324;&#38750;&#32447;&#24615;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the concepts of inverse solvability and security for a generic linear forward model and demonstrate how they can be applied to models used in federated learning. We provide examples of such models which differ in the resulting inverse solvability and security as defined in this paper. We also show how the large number of users participating in a given iteration of federated learning can be leveraged to increase both solvability and security. Finally, we discuss possible extensions of the presented concepts including the nonlinear case.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#23558;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#25512;&#24191;&#21040;&#24191;&#27867;&#30340;&#31354;&#38388;&#20013;&#65292;&#24182;&#23548;&#33268;&#20998;&#25968;&#21305;&#37197;&#30340;&#21407;&#22987;&#25193;&#23637;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#24212;&#29992;&#31243;&#24207;&#12290;</title><link>http://arxiv.org/abs/2211.03595</link><description>&lt;p&gt;
&#20174;&#21435;&#22122;&#25193;&#25955;&#21040;&#21435;&#22122;&#39532;&#23572;&#31185;&#22827;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
From Denoising Diffusions to Denoising Markov Models. (arXiv:2211.03595v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.03595
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#23558;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#25512;&#24191;&#21040;&#24191;&#27867;&#30340;&#31354;&#38388;&#20013;&#65292;&#24182;&#23548;&#33268;&#20998;&#25968;&#21305;&#37197;&#30340;&#21407;&#22987;&#25193;&#23637;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#24212;&#29992;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21435;&#22122;&#25193;&#25955;&#26159;&#23637;&#29616;&#20986;&#21331;&#36234;&#23454;&#39564;&#24615;&#33021;&#30340;&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#20182;&#20204;&#36890;&#36807;&#23558;&#25968;&#25454;&#20998;&#24067;&#25193;&#25955;&#21040;&#39640;&#26031;&#20998;&#24067;&#65292;&#28982;&#21518;&#23398;&#20064;&#36870;&#36716;&#36825;&#20010;&#22122;&#22768;&#36807;&#31243;&#20197;&#33719;&#21462;&#21512;&#25104;&#25968;&#25454;&#28857;&#12290;&#21435;&#22122;&#25193;&#25955;&#20381;&#36182;&#20110;&#20351;&#29992;&#20998;&#25968;&#21305;&#37197;&#23545;&#22122;&#22768;&#25968;&#25454;&#23494;&#24230;&#30340;&#23545;&#25968;&#23548;&#25968;&#30340;&#36924;&#36817;&#12290;&#24403;&#21482;&#33021;&#20174;&#20808;&#39564;&#20998;&#24067;&#21644;&#20284;&#28982;&#20989;&#25968;&#20013;&#36827;&#34892;&#25277;&#26679;&#26102;&#65292;&#36825;&#31181;&#27169;&#22411;&#20063;&#21487;&#29992;&#20110;&#25191;&#34892;&#36817;&#20284;&#21518;&#39564;&#27169;&#25311;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#23558;&#27492;&#26041;&#27861;&#25512;&#24191;&#21040;&#19968;&#31867;&#24191;&#27867;&#30340;&#31354;&#38388;&#65292;&#24182;&#23548;&#33268;&#20998;&#25968;&#21305;&#37197;&#30340;&#21407;&#22987;&#25193;&#23637;&#12290;&#25105;&#20204;&#36890;&#36807;&#21508;&#31181;&#24212;&#29992;&#31243;&#24207;&#35828;&#26126;&#20102;&#25152;&#24471;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Denoising diffusions are state-of-the-art generative models exhibiting remarkable empirical performance. They work by diffusing the data distribution into a Gaussian distribution and then learning to reverse this noising process to obtain synthetic datapoints. The denoising diffusion relies on approximations of the logarithmic derivatives of the noised data densities using score matching. Such models can also be used to perform approximate posterior simulation when one can only sample from the prior and likelihood. We propose a unifying framework generalising this approach to a wide class of spaces and leading to an original extension of score matching. We illustrate the resulting models on various applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#35777;&#26126;&#20102;&#25968;&#25454;&#19981;&#24179;&#34913;&#23545;&#23398;&#20064;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#35828;&#26126;&#22312;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#26102;&#65292;&#23569;&#25968;&#21644;&#22810;&#25968;&#31867;&#30340;&#23398;&#20064;&#26354;&#32447;&#20250;&#36981;&#24490;&#27425;&#20248;&#36712;&#36857;&#65292;&#21516;&#26102;&#25552;&#20986;&#23545;&#27599;&#31181;&#31867;&#21035;&#26799;&#24230;&#20570;&#20986;&#36129;&#29486;&#30340;&#24402;&#19968;&#21270;&#21464;&#20307;&#65292;&#20197;&#35299;&#20915;&#20248;&#21270;&#19981;&#21516;&#31867;&#21035;&#20043;&#38388;&#30340;&#31454;&#20105;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2207.00391</link><description>&lt;p&gt;
&#31867;&#21035;&#19981;&#24179;&#34913;&#19979;&#30340;&#23398;&#20064;&#21160;&#24577;&#29702;&#35770;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Theoretical Analysis of the Learning Dynamics under Class Imbalance. (arXiv:2207.00391v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.00391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#35777;&#26126;&#20102;&#25968;&#25454;&#19981;&#24179;&#34913;&#23545;&#23398;&#20064;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#35828;&#26126;&#22312;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#26102;&#65292;&#23569;&#25968;&#21644;&#22810;&#25968;&#31867;&#30340;&#23398;&#20064;&#26354;&#32447;&#20250;&#36981;&#24490;&#27425;&#20248;&#36712;&#36857;&#65292;&#21516;&#26102;&#25552;&#20986;&#23545;&#27599;&#31181;&#31867;&#21035;&#26799;&#24230;&#20570;&#20986;&#36129;&#29486;&#30340;&#24402;&#19968;&#21270;&#21464;&#20307;&#65292;&#20197;&#35299;&#20915;&#20248;&#21270;&#19981;&#21516;&#31867;&#21035;&#20043;&#38388;&#30340;&#31454;&#20105;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#19981;&#24179;&#34913;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#24120;&#35265;&#30340;&#38382;&#39064;&#65292;&#20250;&#20005;&#37325;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#12290;&#34429;&#28982;&#26377;&#21508;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#23427;&#20204;&#23545;&#23398;&#20064;&#21160;&#24577;&#30340;&#25910;&#25947;&#24433;&#21709;&#23578;&#26410;&#34987;&#29702;&#35299;&#12290;&#26412;&#25991;&#38416;&#26126;&#20102;&#25968;&#25454;&#19981;&#24179;&#34913;&#23545;&#23398;&#20064;&#30340;&#26174;&#33879;&#36127;&#38754;&#24433;&#21709;&#65292;&#24403;&#20351;&#29992;&#26799;&#24230;&#20248;&#21270;&#22120;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#23569;&#25968;&#31867;&#21644;&#22810;&#25968;&#31867;&#30340;&#23398;&#20064;&#26354;&#32447;&#20250;&#36981;&#24490;&#27425;&#20248;&#36712;&#36857;&#12290;&#36825;&#31181;&#25918;&#32531;&#19982;&#19981;&#24179;&#34913;&#27604;&#30456;&#20851;&#65292;&#21487;&#20197;&#36861;&#28335;&#21040;&#20248;&#21270;&#19981;&#21516;&#31867;&#21035;&#20043;&#38388;&#30340;&#31454;&#20105;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#22312;&#20110;&#20998;&#26512;&#20102;&#20840;&#25209;&#27425;&#65288;GD&#65289;&#21644;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#30340;&#25910;&#25947;&#21644;&#21508;&#31181;&#23545;&#27599;&#31181;&#31867;&#21035;&#26799;&#24230;&#20570;&#20986;&#36129;&#29486;&#30340;&#24402;&#19968;&#21270;&#21464;&#20307;&#12290;&#25105;&#20204;&#21457;&#29616;GD&#19981;&#33021;&#20445;&#35777;&#38477;&#20302;&#27599;&#20010;&#31867;&#21035;&#30340;&#25439;&#22833;&#65292;&#20294;&#21487;&#20197;&#36890;&#36807;&#25191;&#34892;&#21508;&#33258;&#24402;&#19968;&#21270;&#26799;&#24230;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#20351;&#29992;SGD&#26102;, &#31867;&#21035;&#19981;&#24179;&#34913;&#20250;&#23545;&#31639;&#27861;&#20135;&#29983;&#39069;&#22806;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data imbalance is a common problem in machine learning that can have a critical effect on the performance of a model. Various solutions exist but their impact on the convergence of the learning dynamics is not understood. Here, we elucidate the significant negative impact of data imbalance on learning, showing that the learning curves for minority and majority classes follow sub-optimal trajectories when training with a gradient-based optimizer. This slowdown is related to the imbalance ratio and can be traced back to a competition between the optimization of different classes. Our main contribution is the analysis of the convergence of full-batch (GD) and stochastic gradient descent (SGD), and of variants that renormalize the contribution of each per-class gradient. We find that GD is not guaranteed to decrease the loss for each class but that this problem can be addressed by performing a per-class normalization of the gradient. With SGD, class imbalance has an additional effect on th
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#21453;&#20107;&#23454;&#24490;&#29615;&#32593;&#32476;&#65292;&#29992;&#20110;&#22312;&#22797;&#26434;&#30340;&#22810;&#26234;&#33021;&#20307;&#22330;&#26223;&#20013;&#20272;&#35745;&#24178;&#39044;&#25928;&#26524;&#12290;&#35813;&#27169;&#22411;&#32771;&#34385;&#20102;&#26102;&#38388;&#21464;&#21270;&#30340;&#22810;&#26234;&#33021;&#20307;&#20851;&#31995;&#21644;&#21327;&#21464;&#37327;&#21453;&#20107;&#23454;&#39044;&#27979;&#30340;&#22797;&#26434;&#32467;&#26500;&#65292;&#33021;&#22815;&#20934;&#30830;&#35780;&#20272;&#20010;&#20307;&#27835;&#30103;&#25928;&#26524;&#65292;&#24182;&#25552;&#20379;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2206.01900</link><description>&lt;p&gt;
&#22312;&#22797;&#26434;&#30340;&#22810;&#26234;&#33021;&#20307;&#22330;&#26223;&#20013;&#20272;&#35745;&#21453;&#20107;&#23454;&#27835;&#30103;&#32467;&#26524;&#30340;&#26102;&#38388;&#21464;&#21270;
&lt;/p&gt;
&lt;p&gt;
Estimating counterfactual treatment outcomes over time in complex multi-agent scenarios. (arXiv:2206.01900v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.01900
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#21453;&#20107;&#23454;&#24490;&#29615;&#32593;&#32476;&#65292;&#29992;&#20110;&#22312;&#22797;&#26434;&#30340;&#22810;&#26234;&#33021;&#20307;&#22330;&#26223;&#20013;&#20272;&#35745;&#24178;&#39044;&#25928;&#26524;&#12290;&#35813;&#27169;&#22411;&#32771;&#34385;&#20102;&#26102;&#38388;&#21464;&#21270;&#30340;&#22810;&#26234;&#33021;&#20307;&#20851;&#31995;&#21644;&#21327;&#21464;&#37327;&#21453;&#20107;&#23454;&#39044;&#27979;&#30340;&#22797;&#26434;&#32467;&#26500;&#65292;&#33021;&#22815;&#20934;&#30830;&#35780;&#20272;&#20010;&#20307;&#27835;&#30103;&#25928;&#26524;&#65292;&#24182;&#25552;&#20379;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#31181;&#24037;&#31243;&#21644;&#31185;&#23398;&#39046;&#22495;&#20013;&#65292;&#35780;&#20272;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#30340;&#24178;&#39044;&#34892;&#20026;&#65288;&#20363;&#22914;&#65292;&#20154;&#31867;&#20309;&#26102;&#24212;&#35813;&#24178;&#39044;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#65292;&#20309;&#26102;&#29699;&#21592;&#24212;&#35813;&#20256;&#32473;&#38431;&#21451;&#36827;&#34892;&#22909;&#23556;&#38376;&#65289;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20351;&#29992;&#21453;&#20107;&#23454;&#30340;&#38271;&#26399;&#39044;&#27979;&#26469;&#20272;&#35745;&#20010;&#20307;&#27835;&#30103;&#25928;&#26524;&#65288;ITE&#65289;&#26159;&#35780;&#20272;&#27492;&#31867;&#24178;&#39044;&#25514;&#26045;&#30340;&#23454;&#29992;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20256;&#32479;&#26694;&#26550;&#27809;&#26377;&#32771;&#34385;&#21040;&#22810;&#26234;&#33021;&#20307;&#20851;&#31995;&#30340;&#26102;&#38388;&#21464;&#21270;&#21644;&#21327;&#21464;&#37327;&#21453;&#20107;&#23454;&#39044;&#27979;&#30340;&#22797;&#26434;&#32467;&#26500;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;ITE&#30340;&#38169;&#35823;&#35780;&#20272;&#21644;&#35299;&#37322;&#22256;&#38590;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#21453;&#20107;&#23454;&#24490;&#29615;&#32593;&#32476;&#65292;&#29992;&#20110;&#20272;&#35745;&#24178;&#39044;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21033;&#29992;&#22270;&#24418;&#21464;&#20998;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21644;&#22522;&#20110;&#39046;&#22495;&#30693;&#35782;&#30340;&#35745;&#31639;&#26469;&#36827;&#34892;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#21327;&#21464;&#37327;&#21644;&#32467;&#26524;&#30340;&#38271;&#26399;&#39044;&#27979;&#30340;ITE&#20272;&#35745;&#26694;&#26550;&#65292;&#33021;&#22815;&#30830;&#35748;&#24490;&#29615;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluation of intervention in a multi-agent system, e.g., when humans should intervene in autonomous driving systems and when a player should pass to teammates for a good shot, is challenging in various engineering and scientific fields. Estimating the individual treatment effect (ITE) using counterfactual long-term prediction is practical to evaluate such interventions. However, most of the conventional frameworks did not consider the time-varying complex structure of multi-agent relationships and covariate counterfactual prediction. This may lead to erroneous assessments of ITE and difficulty in interpretation. Here we propose an interpretable, counterfactual recurrent network in multi-agent systems to estimate the effect of the intervention. Our model leverages graph variational recurrent neural networks and theory-based computation with domain knowledge for the ITE estimation framework based on long-term prediction of multi-agent covariates and outcomes, which can confirm the circu
&lt;/p&gt;</description></item></channel></rss>