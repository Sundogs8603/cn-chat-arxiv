<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20379;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#32447;&#24615;&#32452;&#21512;&#26367;&#25442;&#31070;&#32463;&#20803;&#65292;&#23454;&#29616;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#21477;&#27861;&#21644;&#35821;&#20041;&#25277;&#35937;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#25913;&#36827;&#26041;&#27861;&#26469;&#24179;&#34913;&#20943;&#23569;&#21644;&#31934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2307.10891</link><description>&lt;p&gt;
&#21477;&#27861;&#19982;&#35821;&#20041;&#32447;&#24615;&#25277;&#35937;&#21644;&#32454;&#21270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Syntactic vs Semantic Linear Abstraction and Refinement of Neural Networks. (arXiv:2307.10891v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10891
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20379;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#32447;&#24615;&#32452;&#21512;&#26367;&#25442;&#31070;&#32463;&#20803;&#65292;&#23454;&#29616;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#21477;&#27861;&#21644;&#35821;&#20041;&#25277;&#35937;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#25913;&#36827;&#26041;&#27861;&#26469;&#24179;&#34913;&#20943;&#23569;&#21644;&#31934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25277;&#35937;&#26159;&#19968;&#31181;&#25552;&#39640;&#21487;&#25193;&#23637;&#24615;&#30340;&#20851;&#38190;&#39564;&#35777;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#20854;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20351;&#29992;&#36804;&#20170;&#38750;&#24120;&#26377;&#38480;&#12290;&#20197;&#21069;&#30340;&#20998;&#31867;&#32593;&#32476;&#25277;&#35937;&#26041;&#27861;&#23558;&#22810;&#20010;&#31070;&#32463;&#20803;&#26367;&#25442;&#20026;&#36275;&#22815;&#30456;&#20284;&#30340;&#20854;&#20013;&#19968;&#20010;&#31070;&#32463;&#20803;&#12290;&#25105;&#20204;&#21487;&#20197;&#23558;&#30456;&#20284;&#24615;&#23450;&#20041;&#20026;&#21477;&#27861;&#19978;&#65288;&#20351;&#29992;&#31070;&#32463;&#20803;&#20043;&#38388;&#30340;&#36830;&#25509;&#25968;&#37327;&#65289;&#25110;&#35821;&#20041;&#19978;&#65288;&#23545;&#20110;&#21508;&#31181;&#36755;&#20837;&#30340;&#31070;&#32463;&#20803;&#28608;&#27963;&#20540;&#65289;&#30340;&#30456;&#20284;&#24615;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#26041;&#27861;&#20165;&#33021;&#36798;&#21040;&#36866;&#24230;&#30340;&#20943;&#23569;&#65292;&#32780;&#19988;&#23454;&#29616;&#36215;&#26469;&#22256;&#38590;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26356;&#28789;&#27963;&#30340;&#26694;&#26550;&#65292;&#20854;&#20013;&#31070;&#32463;&#20803;&#21487;&#20197;&#34987;&#20854;&#20182;&#31070;&#32463;&#20803;&#30340;&#32447;&#24615;&#32452;&#21512;&#26367;&#25442;&#65292;&#20174;&#32780;&#25913;&#21892;&#20943;&#23569;&#25928;&#26524;&#12290;&#25105;&#20204;&#22312;&#21477;&#27861;&#21644;&#35821;&#20041;&#25277;&#35937;&#19978;&#24212;&#29992;&#20102;&#36825;&#31181;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#36827;&#34892;&#20102;&#23454;&#29616;&#21644;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#25913;&#36827;&#25105;&#20204;&#25277;&#35937;&#30340;&#26041;&#27861;&#65292;&#20197;&#23547;&#25214;&#26356;&#22909;&#30340;&#20943;&#23569;&#21644;&#31934;&#30830;&#24230;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Abstraction is a key verification technique to improve scalability. However, its use for neural networks is so far extremely limited. Previous approaches for abstracting classification networks replace several neurons with one of them that is similar enough. We can classify the similarity as defined either syntactically (using quantities on the connections between neurons) or semantically (on the activation values of neurons for various inputs). Unfortunately, the previous approaches only achieve moderate reductions, when implemented at all. In this work, we provide a more flexible framework where a neuron can be replaced with a linear combination of other neurons, improving the reduction. We apply this approach both on syntactic and semantic abstractions, and implement and evaluate them experimentally. Further, we introduce a refinement method for our abstractions, allowing for finding a better balance between reduction and precision.
&lt;/p&gt;</description></item><item><title>&#22312;&#21305;&#37197;&#24066;&#22330;&#20013;&#65292;&#36890;&#36807;&#22312;&#32447;&#23398;&#20064;&#65292;&#29609;&#23478;&#21487;&#20197;&#23454;&#29616;&#29609;&#23478;&#26368;&#20248;&#31283;&#23450;&#21305;&#37197;&#65292;&#20174;&#32780;&#26368;&#22823;&#21270;&#20182;&#20204;&#30340;&#21033;&#28070;&#12290;</title><link>http://arxiv.org/abs/2307.10890</link><description>&lt;p&gt;
&#22312;&#21305;&#37197;&#24066;&#22330;&#20013;&#30340;&#36172;&#21338;&#23398;&#20064;&#20013;&#30340;&#29609;&#23478;&#26368;&#20248;&#31283;&#23450;&#36951;&#25022;
&lt;/p&gt;
&lt;p&gt;
Player-optimal Stable Regret for Bandit Learning in Matching Markets. (arXiv:2307.10890v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10890
&lt;/p&gt;
&lt;p&gt;
&#22312;&#21305;&#37197;&#24066;&#22330;&#20013;&#65292;&#36890;&#36807;&#22312;&#32447;&#23398;&#20064;&#65292;&#29609;&#23478;&#21487;&#20197;&#23454;&#29616;&#29609;&#23478;&#26368;&#20248;&#31283;&#23450;&#21305;&#37197;&#65292;&#20174;&#32780;&#26368;&#22823;&#21270;&#20182;&#20204;&#30340;&#21033;&#28070;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#24191;&#27867;&#30340;&#24212;&#29992;&#33539;&#22260;&#65292;&#21305;&#37197;&#24066;&#22330;&#30340;&#38382;&#39064;&#24050;&#32463;&#22312;&#25991;&#29486;&#20013;&#30740;&#31350;&#20102;&#24456;&#38271;&#26102;&#38388;&#12290;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#65292;&#25214;&#21040;&#19968;&#20010;&#31283;&#23450;&#30340;&#21305;&#37197;&#26159;&#19968;&#20010;&#24120;&#35265;&#30340;&#22343;&#34913;&#30446;&#26631;&#12290;&#30001;&#20110;&#24066;&#22330;&#21442;&#19982;&#32773;&#36890;&#24120;&#23545;&#20182;&#20204;&#30340;&#20559;&#22909;&#24863;&#21040;&#19981;&#30830;&#23450;&#65292;&#26368;&#36817;&#26377;&#35768;&#22810;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#32447;&#35774;&#32622;&#65292;&#20854;&#20013;&#21333;&#26041;&#21442;&#19982;&#32773;&#65288;&#29609;&#23478;&#65289;&#36890;&#36807;&#19982;&#21478;&#19968;&#26041;&#65288;&#23545;&#25163;&#65289;&#30340;&#36845;&#20195;&#20132;&#20114;&#26469;&#23398;&#20064;&#20182;&#20204;&#26410;&#30693;&#30340;&#20559;&#22909;&#12290;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#24037;&#20316;&#21482;&#33021;&#25512;&#23548;&#20986;&#29609;&#23478;&#26368;&#24046;&#31283;&#23450;&#36951;&#25022;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#36825;&#26159;&#30456;&#23545;&#20110;&#29609;&#23478;&#26368;&#19981;&#21916;&#27426;&#30340;&#31283;&#23450;&#21305;&#37197;&#23450;&#20041;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#26368;&#24046;&#31283;&#23450;&#21305;&#37197;&#19979;&#65292;&#29609;&#23478;&#21482;&#33021;&#33719;&#24471;&#25152;&#26377;&#31283;&#23450;&#21305;&#37197;&#20013;&#30340;&#26368;&#20302;&#22870;&#21169;&#12290;&#20026;&#20102;&#26368;&#22823;&#21270;&#29609;&#23478;&#30340;&#21033;&#28070;&#65292;&#29609;&#23478;&#26368;&#20248;&#31283;&#23450;&#21305;&#37197;&#23558;&#26159;&#26368;&#29702;&#24819;&#30340;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;Basu&#31561;&#20154;&#25104;&#21151;&#22320;&#25552;&#20379;&#20102;&#29609;&#23478;&#26368;&#20248;&#31283;&#23450;&#36951;&#25022;&#30340;&#19978;&#30028;&#65292;&#20294;&#20182;&#20204;&#30340;&#32467;&#26524;&#22312;&#29609;&#23478;&#20559;&#22909;&#24456;&#22823;&#30340;&#24773;&#20917;&#19979;&#21487;&#33021;&#26159;&#25351;&#25968;&#32423;&#21035;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The problem of matching markets has been studied for a long time in the literature due to its wide range of applications. Finding a stable matching is a common equilibrium objective in this problem. Since market participants are usually uncertain of their preferences, a rich line of recent works study the online setting where one-side participants (players) learn their unknown preferences from iterative interactions with the other side (arms). Most previous works in this line are only able to derive theoretical guarantees for player-pessimal stable regret, which is defined compared with the players' least-preferred stable matching. However, under the pessimal stable matching, players only obtain the least reward among all stable matchings. To maximize players' profits, player-optimal stable matching would be the most desirable. Though \citet{basu21beyond} successfully bring an upper bound for player-optimal stable regret, their result can be exponentially large if players' preference g
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#31283;&#20581;&#28857;&#20113;&#20998;&#31867;&#30340;&#39118;&#38505;&#20248;&#21270;&#24322;&#24120;&#20540;&#21435;&#38500;&#26041;&#27861;&#65292;&#21033;&#29992;&#26222;&#36890;&#35757;&#32451;&#30340;&#27169;&#22411;&#28040;&#38500;&#39069;&#22806;&#30340;&#24322;&#24120;&#20540;&#24182;&#24674;&#22797;&#25968;&#25454;&#12290;&#26041;&#27861;&#36890;&#36807;&#24402;&#22240;&#20998;&#26512;&#30830;&#23450;&#27599;&#20010;&#28857;&#23545;&#27169;&#22411;&#36755;&#20986;&#30340;&#24433;&#21709;&#65292;&#20351;&#29992;&#26465;&#20214;&#39118;&#38505;&#20215;&#20540;&#20248;&#21270;&#39640;&#39118;&#38505;&#28857;&#30340;&#36807;&#28388;&#36807;&#31243;&#12290;&#35813;&#26041;&#27861;&#22312;&#19981;&#38656;&#35201;&#39069;&#22806;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#33021;&#22815;&#20135;&#29983;&#20986;&#33394;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.10875</link><description>&lt;p&gt;
&#38754;&#21521;&#31283;&#20581;&#28857;&#20113;&#20998;&#31867;&#30340;&#39118;&#38505;&#20248;&#21270;&#24322;&#24120;&#20540;&#21435;&#38500;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Risk-optimized Outlier Removal for Robust Point Cloud Classification. (arXiv:2307.10875v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10875
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#31283;&#20581;&#28857;&#20113;&#20998;&#31867;&#30340;&#39118;&#38505;&#20248;&#21270;&#24322;&#24120;&#20540;&#21435;&#38500;&#26041;&#27861;&#65292;&#21033;&#29992;&#26222;&#36890;&#35757;&#32451;&#30340;&#27169;&#22411;&#28040;&#38500;&#39069;&#22806;&#30340;&#24322;&#24120;&#20540;&#24182;&#24674;&#22797;&#25968;&#25454;&#12290;&#26041;&#27861;&#36890;&#36807;&#24402;&#22240;&#20998;&#26512;&#30830;&#23450;&#27599;&#20010;&#28857;&#23545;&#27169;&#22411;&#36755;&#20986;&#30340;&#24433;&#21709;&#65292;&#20351;&#29992;&#26465;&#20214;&#39118;&#38505;&#20215;&#20540;&#20248;&#21270;&#39640;&#39118;&#38505;&#28857;&#30340;&#36807;&#28388;&#36807;&#31243;&#12290;&#35813;&#26041;&#27861;&#22312;&#19981;&#38656;&#35201;&#39069;&#22806;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#33021;&#22815;&#20135;&#29983;&#20986;&#33394;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28857;&#20113;&#28145;&#24230;&#27169;&#22411;&#22312;&#23433;&#20840;&#20851;&#38190;&#20219;&#21153;&#20013;&#30340;&#20351;&#29992;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#20294;&#26159;&#28857;&#20113;&#22122;&#22768;&#21487;&#33021;&#20250;&#24433;&#21709;&#36825;&#20123;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#21644;&#23433;&#20840;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28857;&#20113;&#24322;&#24120;&#20540;&#21435;&#38500;&#26041;&#27861;(PointCVaR)&#65292;&#23427;&#21487;&#20197;&#20351;&#26631;&#20934;&#35757;&#32451;&#30340;&#27169;&#22411;&#28040;&#38500;&#39069;&#22806;&#30340;&#24322;&#24120;&#20540;&#24182;&#24674;&#22797;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#36827;&#34892;&#24402;&#22240;&#20998;&#26512;&#65292;&#20197;&#30830;&#23450;&#27599;&#20010;&#28857;&#23545;&#27169;&#22411;&#36755;&#20986;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#28857;&#30340;&#39118;&#38505;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#26465;&#20214;&#39118;&#38505;&#20215;&#20540; (CVaR) &#20316;&#20026;&#30446;&#26631;&#65292;&#20248;&#21270;&#39640;&#39118;&#38505;&#28857;&#30340;&#36807;&#28388;&#36807;&#31243;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#22522;&#26412;&#21407;&#29702;&#26159;&#35266;&#23519;&#21040;&#28857;&#20113;&#22122;&#22768;&#28857;&#24448;&#24448;&#32858;&#38598;&#22312;&#39118;&#38505;&#20998;&#24067;&#30340;&#23614;&#37096;&#65292;&#39057;&#29575;&#20302;&#20294;&#39118;&#38505;&#27700;&#24179;&#39640;&#65292;&#20174;&#32780;&#23545;&#20998;&#31867;&#32467;&#26524;&#20135;&#29983;&#37325;&#22823;&#24178;&#25200;&#12290;&#23613;&#31649;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21364;&#33021;&#20135;&#29983;&#20986;&#33394;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The popularity of point cloud deep models for safety-critical purposes has increased, but the reliability and security of these models can be compromised by intentional or naturally occurring point cloud noise. To combat this issue, we present a novel point cloud outlier removal method called PointCVaR, which empowers standard-trained models to eliminate additional outliers and restore the data. Our approach begins by conducting attribution analysis to determine the influence of each point on the model output, which we refer to as point risk. We then optimize the process of filtering high-risk points using Conditional Value at Risk (CVaR) as the objective. The rationale for this approach is based on the observation that noise points in point clouds tend to cluster in the tail of the risk distribution, with a low frequency but a high level of risk, resulting in significant interference with classification results. Despite requiring no additional training effort, our method produces exce
&lt;/p&gt;</description></item><item><title>&#38750;&#32447;&#24615;&#20803;&#23398;&#20064;&#21487;&#20197;&#20445;&#35777;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2307.10870</link><description>&lt;p&gt;
&#38750;&#32447;&#24615;&#20803;&#23398;&#20064;&#21487;&#20197;&#20445;&#35777;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;
&lt;/p&gt;
&lt;p&gt;
Nonlinear Meta-Learning Can Guarantee Faster Rates. (arXiv:2307.10870v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10870
&lt;/p&gt;
&lt;p&gt;
&#38750;&#32447;&#24615;&#20803;&#23398;&#20064;&#21487;&#20197;&#20445;&#35777;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#35768;&#22810;&#20851;&#20110;&#20803;&#23398;&#20064;&#30340;&#29702;&#35770;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#30456;&#20851;&#20219;&#21153;&#20013;&#30340;&#30456;&#20284;&#34920;&#31034;&#32467;&#26500;&#26469;&#31616;&#21270;&#30446;&#26631;&#20219;&#21153;&#65292;&#24182;&#23454;&#29616;&#25910;&#25947;&#36895;&#29575;&#30340;&#20445;&#35777;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#34920;&#31034;&#24448;&#24448;&#26159;&#39640;&#24230;&#38750;&#32447;&#24615;&#30340;&#65292;&#24341;&#20837;&#20102;&#27599;&#20010;&#20219;&#21153;&#20013;&#19981;&#21487;&#31616;&#21333;&#24179;&#22343;&#30340;&#38750;&#24179;&#20961;&#20559;&#24046;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#38750;&#32447;&#24615;&#34920;&#31034;&#25512;&#23548;&#20986;&#20803;&#23398;&#20064;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many recent theoretical works on \emph{meta-learning} aim to achieve guarantees in leveraging similar representational structures from related tasks towards simplifying a target task. Importantly, the main aim in theory works on the subject is to understand the extent to which convergence rates -- in learning a common representation -- \emph{may scale with the number $N$ of tasks} (as well as the number of samples per task). First steps in this setting demonstrate this property when both the shared representation amongst tasks, and task-specific regression functions, are linear. This linear setting readily reveals the benefits of aggregating tasks, e.g., via averaging arguments. In practice, however, the representation is often highly nonlinear, introducing nontrivial biases in each task that cannot easily be averaged out as in the linear case. In the present work, we derive theoretical guarantees for meta-learning with nonlinear representations. In particular, assuming the shared nonl
&lt;/p&gt;</description></item><item><title>&#20113;&#31995;&#32479;&#20013;&#30340;&#24615;&#33021;&#38382;&#39064;&#35782;&#21035;&#23384;&#22312;&#25361;&#25112;&#65292;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#29420;&#31435;&#20998;&#26512;&#27599;&#20010;&#25351;&#26631;&#30340;&#24322;&#24120;&#26080;&#27861;&#35299;&#20915;&#38382;&#39064;&#65292;&#38656;&#35201;&#32771;&#34385;&#25351;&#26631;&#20043;&#38388;&#30340;&#20851;&#32852;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.10869</link><description>&lt;p&gt;
&#20113;&#31995;&#32479;&#20013;&#30340;&#24615;&#33021;&#38382;&#39064;&#35782;&#21035;&#19982;&#20851;&#31995;-&#26102;&#38388;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Performance Issue Identification in Cloud Systems with Relational-Temporal Anomaly Detection. (arXiv:2307.10869v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10869
&lt;/p&gt;
&lt;p&gt;
&#20113;&#31995;&#32479;&#20013;&#30340;&#24615;&#33021;&#38382;&#39064;&#35782;&#21035;&#23384;&#22312;&#25361;&#25112;&#65292;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#29420;&#31435;&#20998;&#26512;&#27599;&#20010;&#25351;&#26631;&#30340;&#24322;&#24120;&#26080;&#27861;&#35299;&#20915;&#38382;&#39064;&#65292;&#38656;&#35201;&#32771;&#34385;&#25351;&#26631;&#20043;&#38388;&#30340;&#20851;&#32852;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24615;&#33021;&#38382;&#39064;&#22312;&#22823;&#35268;&#27169;&#20113;&#26381;&#21153;&#31995;&#32479;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#21487;&#33021;&#23548;&#33268;&#24040;&#39069;&#25910;&#20837;&#25439;&#22833;&#12290;&#20026;&#20102;&#30830;&#20445;&#21487;&#38752;&#30340;&#24615;&#33021;&#65292;&#20351;&#29992;&#26381;&#21153;&#30417;&#25511;&#25351;&#26631;&#20934;&#30830;&#22320;&#35782;&#21035;&#21644;&#23450;&#20301;&#36825;&#20123;&#38382;&#39064;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#37492;&#20110;&#29616;&#20195;&#20113;&#31995;&#32479;&#30340;&#22797;&#26434;&#24615;&#21644;&#35268;&#27169;&#65292;&#36825;&#39033;&#20219;&#21153;&#21487;&#33021;&#20250;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#24182;&#19988;&#21487;&#33021;&#38656;&#35201;&#36229;&#20986;&#20010;&#20154;&#33021;&#21147;&#30340;&#24191;&#27867;&#19987;&#19994;&#30693;&#35782;&#21644;&#36164;&#28304;&#12290;&#19968;&#20123;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#36807;&#20998;&#26512;&#27599;&#20010;&#25351;&#26631;&#29420;&#31435;&#22320;&#26816;&#27979;&#24322;&#24120;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#38590;&#20197;&#30001;&#24037;&#31243;&#24072;&#25163;&#21160;&#35786;&#26029;&#30340;&#21387;&#20498;&#24615;&#35686;&#25253;&#39118;&#26292;&#12290;&#20026;&#20102;&#36861;&#27714;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#19981;&#20165;&#24212;&#32771;&#34385;&#25351;&#26631;&#30340;&#26102;&#38388;&#27169;&#24335;&#65292;&#36824;&#24212;&#32771;&#34385;&#25351;&#26631;&#20043;&#38388;&#30340;&#20851;&#32852;&#24615;&#27169;&#24335;&#65292;&#36825;&#21487;&#20197;&#34987;&#24418;&#24335;&#21270;&#20026;&#22810;&#21464;&#37327;&#25351;&#26631;&#24322;&#24120;&#26816;&#27979;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#22312;&#26126;&#30830;&#25552;&#21462;&#36825;&#20004;&#31181;&#31867;&#22411;&#30340;&#29305;&#24449;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#12290;&#27492;&#22806;&#65292;&#23384;&#22312;&#19968;&#20123;&#26410;&#26631;&#35760;&#30340;&#24322;&#24120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Performance issues permeate large-scale cloud service systems, which can lead to huge revenue losses. To ensure reliable performance, it's essential to accurately identify and localize these issues using service monitoring metrics. Given the complexity and scale of modern cloud systems, this task can be challenging and may require extensive expertise and resources beyond the capacity of individual humans. Some existing methods tackle this problem by analyzing each metric independently to detect anomalies. However, this could incur overwhelming alert storms that are difficult for engineers to diagnose manually. To pursue better performance, not only the temporal patterns of metrics but also the correlation between metrics (i.e., relational patterns) should be considered, which can be formulated as a multivariate metrics anomaly detection problem. However, most of the studies fall short of extracting these two types of features explicitly. Moreover, there exist some unlabeled anomalies m
&lt;/p&gt;</description></item><item><title>FigCaps-HF&#26159;&#19968;&#20010;&#22270;&#20687;&#29983;&#25104;&#26631;&#39064;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#34701;&#20837;&#39046;&#22495;&#19987;&#23478;&#30340;&#21453;&#39304;&#24847;&#35265;&#65292;&#29983;&#25104;&#31526;&#21512;&#35835;&#32773;&#20559;&#22909;&#30340;&#39640;&#36136;&#37327;&#22270;&#20687;&#26631;&#39064;&#12290;&#23558;&#33258;&#21160;&#35780;&#20272;&#21644;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#25913;&#21892;&#29983;&#25104;&#30340;&#26631;&#39064;&#19982;&#35835;&#32773;&#20559;&#22909;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.10867</link><description>&lt;p&gt;
FigCaps-HF:&#19968;&#20010;&#22522;&#20110;&#20154;&#31867;&#21453;&#39304;&#30340;&#22270;&#20687;&#29983;&#25104;&#26631;&#39064;&#26694;&#26550;&#21644;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
FigCaps-HF: A Figure-to-Caption Generative Framework and Benchmark with Human Feedback. (arXiv:2307.10867v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10867
&lt;/p&gt;
&lt;p&gt;
FigCaps-HF&#26159;&#19968;&#20010;&#22270;&#20687;&#29983;&#25104;&#26631;&#39064;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#34701;&#20837;&#39046;&#22495;&#19987;&#23478;&#30340;&#21453;&#39304;&#24847;&#35265;&#65292;&#29983;&#25104;&#31526;&#21512;&#35835;&#32773;&#20559;&#22909;&#30340;&#39640;&#36136;&#37327;&#22270;&#20687;&#26631;&#39064;&#12290;&#23558;&#33258;&#21160;&#35780;&#20272;&#21644;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#25913;&#21892;&#29983;&#25104;&#30340;&#26631;&#39064;&#19982;&#35835;&#32773;&#20559;&#22909;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#39064;&#23545;&#20110;&#29702;&#35299;&#31185;&#23398;&#21487;&#35270;&#21270;&#21644;&#25991;&#26723;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#31185;&#23398;&#22270;&#20687;&#29983;&#25104;&#26631;&#39064;&#26041;&#27861;&#20381;&#36182;&#20110;&#20174;&#25991;&#26723;&#20013;&#25552;&#21462;&#30340;&#22270;&#20687;-&#26631;&#39064;&#37197;&#23545;&#36827;&#34892;&#35757;&#32451;&#65292;&#20294;&#20854;&#20013;&#35768;&#22810;&#37197;&#23545;&#22312;&#24110;&#21161;&#24615;&#12289;&#35299;&#37322;&#24615;&#21644;&#35270;&#35273;&#25551;&#36848;&#24615;&#31561;&#25351;&#26631;&#19978;&#23384;&#22312;&#19981;&#36275;&#65292;&#23548;&#33268;&#29983;&#25104;&#30340;&#26631;&#39064;&#19982;&#35835;&#32773;&#20559;&#22909;&#19981;&#19968;&#33268;&#12290;&#20026;&#20102;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#26631;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;FigCaps-HF&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#22270;&#20687;&#29983;&#25104;&#26631;&#39064;&#26694;&#26550;&#65292;&#21487;&#20197;&#34701;&#20837;&#39046;&#22495;&#19987;&#23478;&#30340;&#21453;&#39304;&#24847;&#35265;&#65292;&#20197;&#29983;&#25104;&#20248;&#21270;&#20102;&#35835;&#32773;&#20559;&#22909;&#30340;&#26631;&#39064;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#21547;1&#65289;&#19968;&#31181;&#35780;&#20272;&#22270;&#20687;-&#26631;&#39064;&#37197;&#23545;&#36136;&#37327;&#30340;&#33258;&#21160;&#26041;&#27861;&#65292;2&#65289;&#19968;&#31181;&#22522;&#20110;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#20248;&#21270;&#29983;&#25104;&#24335;&#22270;&#20687;&#29983;&#25104;&#26631;&#39064;&#27169;&#22411;&#20197;&#31526;&#21512;&#35835;&#32773;&#20559;&#22909;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#27169;&#22411;&#19978;&#25913;&#36827;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#31616;&#21333;&#30340;&#23398;&#20064;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Captions are crucial for understanding scientific visualizations and documents. Existing captioning methods for scientific figures rely on figure-caption pairs extracted from documents for training, many of which fall short with respect to metrics like helpfulness, explainability, and visual-descriptiveness [15] leading to generated captions being misaligned with reader preferences. To enable the generation of high-quality figure captions, we introduce FigCaps-HF a new framework for figure-caption generation that can incorporate domain expert feedback in generating captions optimized for reader preferences. Our framework comprises of 1) an automatic method for evaluating quality of figure-caption pairs, 2) a novel reinforcement learning with human feedback (RLHF) method to optimize a generative figure-to-caption model for reader preferences. We demonstrate the effectiveness of our simple learning framework by improving performance over standard fine-tuning across different types of mod
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21457;&#29616;&#32593;&#32476;&#26435;&#37325;&#30340;&#26041;&#24046;&#21644;&#22823;&#26435;&#37325;&#30340;&#31354;&#38388;&#38598;&#20013;&#26159;&#24433;&#21709;&#31070;&#32463;&#25345;&#20037;&#24615;&#30340;&#20027;&#35201;&#22240;&#32032;&#65292;&#24182;&#25552;&#20986;&#20102;&#23558;&#31070;&#32463;&#25345;&#20037;&#24615;&#25193;&#23637;&#21040;&#25972;&#20010;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#22270;&#25345;&#20037;&#24615;&#27979;&#37327;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.10865</link><description>&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#22270;&#30340;&#25345;&#20037;&#24615;&#35299;&#20915;&#31070;&#32463;&#25345;&#20037;&#24615;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Addressing caveats of neural persistence with deep graph persistence. (arXiv:2307.10865v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21457;&#29616;&#32593;&#32476;&#26435;&#37325;&#30340;&#26041;&#24046;&#21644;&#22823;&#26435;&#37325;&#30340;&#31354;&#38388;&#38598;&#20013;&#26159;&#24433;&#21709;&#31070;&#32463;&#25345;&#20037;&#24615;&#30340;&#20027;&#35201;&#22240;&#32032;&#65292;&#24182;&#25552;&#20986;&#20102;&#23558;&#31070;&#32463;&#25345;&#20037;&#24615;&#25193;&#23637;&#21040;&#25972;&#20010;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#22270;&#25345;&#20037;&#24615;&#27979;&#37327;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#25345;&#20037;&#24615;&#26159;&#19968;&#31181;&#29992;&#20110;&#37327;&#21270;&#31070;&#32463;&#32593;&#32476;&#22797;&#26434;&#24615;&#30340;&#37325;&#35201;&#25351;&#26631;&#65292;&#25552;&#20986;&#20110;&#28145;&#24230;&#23398;&#20064;&#20013;&#26032;&#20852;&#30340;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#22312;&#29702;&#35770;&#21644;&#23454;&#35777;&#19978;&#25105;&#20204;&#21457;&#29616;&#65292;&#32593;&#32476;&#26435;&#37325;&#30340;&#26041;&#24046;&#21644;&#22823;&#26435;&#37325;&#30340;&#31354;&#38388;&#38598;&#20013;&#26159;&#24433;&#21709;&#31070;&#32463;&#25345;&#20037;&#24615;&#30340;&#20027;&#35201;&#22240;&#32032;&#12290;&#34429;&#28982;&#36825;&#23545;&#20110;&#32447;&#24615;&#20998;&#31867;&#22120;&#26377;&#29992;&#30340;&#20449;&#24687;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21518;&#20960;&#23618;&#20013;&#27809;&#26377;&#30456;&#20851;&#30340;&#31354;&#38388;&#32467;&#26500;&#65292;&#20351;&#24471;&#31070;&#32463;&#25345;&#20037;&#24615;&#22823;&#33268;&#31561;&#20110;&#26435;&#37325;&#30340;&#26041;&#24046;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#25152;&#25552;&#20986;&#30340;&#23618;&#38388;&#24179;&#22343;&#36807;&#31243;&#27809;&#26377;&#32771;&#34385;&#23618;&#38388;&#30340;&#20132;&#20114;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#31070;&#32463;&#25345;&#20037;&#24615;&#22522;&#30784;&#32467;&#26500;&#30340;&#25193;&#23637;&#65292;&#20174;&#21333;&#23618;&#25913;&#20026;&#25972;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#36825;&#30456;&#24403;&#20110;&#22312;&#19968;&#20010;&#29305;&#23450;&#30697;&#38453;&#19978;&#35745;&#31639;&#31070;&#32463;&#25345;&#20037;&#24615;&#12290;&#36825;&#24471;&#21040;&#20102;&#25105;&#20204;&#30340;&#28145;&#24230;&#22270;&#25345;&#20037;&#24615;&#27979;&#37327;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Persistence is a prominent measure for quantifying neural network complexity, proposed in the emerging field of topological data analysis in deep learning. In this work, however, we find both theoretically and empirically that the variance of network weights and spatial concentration of large weights are the main factors that impact neural persistence. Whilst this captures useful information for linear classifiers, we find that no relevant spatial structure is present in later layers of deep neural networks, making neural persistence roughly equivalent to the variance of weights. Additionally, the proposed averaging procedure across layers for deep neural networks does not consider interaction between layers. Based on our analysis, we propose an extension of the filtration underlying neural persistence to the whole neural network instead of single layers, which is equivalent to calculating neural persistence on one particular matrix. This yields our deep graph persistence measur
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"&#20998;&#21106;&#19982;&#32465;&#23450;"&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#25913;&#36827;&#29983;&#25104;&#35821;&#20041;&#25252;&#29702;&#30340;&#25928;&#26524;&#12290;&#35813;&#26041;&#27861;&#24341;&#20837;&#20102;&#26032;&#30340;&#25439;&#22833;&#30446;&#26631;&#65292;&#21253;&#25324;&#20851;&#27880;&#20002;&#22833;&#21644;&#32465;&#23450;&#20002;&#22833;&#65292;&#20197;&#35299;&#20915;&#22797;&#26434;&#25552;&#31034;&#21644;&#19981;&#36866;&#24403;&#23646;&#24615;&#32465;&#23450;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.10864</link><description>&lt;p&gt;
&#23558;&#27880;&#24847;&#21147;&#20998;&#21106;&#19982;&#32465;&#23450;&#29992;&#20110;&#25913;&#36827;&#29983;&#25104;&#35821;&#20041;&#25252;&#29702;
&lt;/p&gt;
&lt;p&gt;
Divide &amp; Bind Your Attention for Improved Generative Semantic Nursing. (arXiv:2307.10864v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10864
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"&#20998;&#21106;&#19982;&#32465;&#23450;"&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#25913;&#36827;&#29983;&#25104;&#35821;&#20041;&#25252;&#29702;&#30340;&#25928;&#26524;&#12290;&#35813;&#26041;&#27861;&#24341;&#20837;&#20102;&#26032;&#30340;&#25439;&#22833;&#30446;&#26631;&#65292;&#21253;&#25324;&#20851;&#27880;&#20002;&#22833;&#21644;&#32465;&#23450;&#20002;&#22833;&#65292;&#20197;&#35299;&#20915;&#22797;&#26434;&#25552;&#31034;&#21644;&#19981;&#36866;&#24403;&#23646;&#24615;&#32465;&#23450;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#20852;&#30340;&#22823;&#35268;&#27169;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65292;&#22914;&#31283;&#23450;&#25193;&#25955;&#65288;SD&#65289;&#65292;&#23637;&#31034;&#20102;&#39640;&#24230;&#36924;&#30495;&#30340;&#21387;&#20498;&#24615;&#32467;&#26524;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#20173;&#28982;&#38590;&#20197;&#23436;&#20840;&#20381;&#29031;&#36755;&#20837;&#25552;&#31034;&#29983;&#25104;&#22270;&#20687;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#8212;&#8212;&#20851;&#27880;&#19982;&#28608;&#21457;&#65292;&#24341;&#20837;&#20102;&#29983;&#25104;&#35821;&#20041;&#25252;&#29702;&#65288;GSN&#65289;&#30340;&#27010;&#24565;&#65292;&#26088;&#22312;&#22312;&#25512;&#26029;&#26102;&#20248;&#21270;&#36328;&#27880;&#24847;&#21147;&#20197;&#26356;&#22909;&#22320;&#34701;&#20837;&#35821;&#20041;&#12290;&#23427;&#22312;&#29983;&#25104;&#31616;&#21333;&#25552;&#31034;&#65292;&#22914;&#8220;&#19968;&#21482;&#29483;&#21644;&#19968;&#21482;&#29399;&#8221;&#65292;&#26041;&#38754;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#23427;&#22312;&#22788;&#29702;&#26356;&#22797;&#26434;&#30340;&#25552;&#31034;&#20197;&#21450;&#35299;&#20915;&#19981;&#36866;&#24403;&#30340;&#23646;&#24615;&#32465;&#23450;&#38382;&#39064;&#26041;&#38754;&#30340;&#21151;&#25928;&#26377;&#25152;&#19979;&#38477;&#12290;&#20026;&#20102;&#24212;&#23545;&#22797;&#26434;&#25552;&#31034;&#25110;&#28041;&#21450;&#22810;&#20010;&#23454;&#20307;&#30340;&#22330;&#26223;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#24182;&#23454;&#29616;&#25913;&#36827;&#30340;&#23646;&#24615;&#32465;&#23450;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20998;&#21106;&#19982;&#32465;&#23450;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#30340;GSN&#25439;&#22833;&#30446;&#26631;&#65306;&#19968;&#31181;&#26032;&#30340;&#20851;&#27880;&#20002;&#22833;&#21644;&#19968;&#31181;&#32465;&#23450;&#20002;&#22833;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20854;&#33021;&#22815;&#26356;&#22909;&#22320;&#23558;&#35821;&#20041;&#32435;&#20837;&#22270;&#20687;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#29305;&#28857;&#19978;&#33073;&#39062;&#32780;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emerging large-scale text-to-image generative models, e.g., Stable Diffusion (SD), have exhibited overwhelming results with high fidelity. Despite the magnificent progress, current state-of-the-art models still struggle to generate images fully adhering to the input prompt. Prior work, Attend &amp; Excite, has introduced the concept of Generative Semantic Nursing (GSN), aiming to optimize cross-attention during inference time to better incorporate the semantics. It demonstrates promising results in generating simple prompts, e.g., ``a cat and a dog''. However, its efficacy declines when dealing with more complex prompts, and it does not explicitly address the problem of improper attribute binding. To address the challenges posed by complex prompts or scenarios involving multiple entities and to achieve improved attribute binding, we propose Divide &amp; Bind. We introduce two novel loss objectives for GSN: a novel attendance loss and a binding loss. Our approach stands out in its ability to fa
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#26435;&#37325;&#24041;&#22266;&#65288;spWC&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#35780;&#20272;&#20808;&#21069;&#20219;&#21153;&#30340;&#21306;&#20998;&#24615;&#36129;&#29486;&#65292;&#23454;&#29616;&#31283;&#20581;&#30340;&#25345;&#32493;&#23398;&#20064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#26681;&#25454;&#20851;&#38190;&#32489;&#25928;&#25351;&#26631;&#65288;&#21363;&#20934;&#30830;&#24230;&#65289;&#26469;&#34913;&#37327;&#38590;&#24230;&#65292;&#21453;&#26144;&#36807;&#21435;&#20219;&#21153;&#30340;&#20248;&#20808;&#32423;&#65292;&#24182;&#23545;&#20808;&#21069;&#20219;&#21153;&#36827;&#34892;&#25490;&#24207;&#12290;</title><link>http://arxiv.org/abs/2307.10845</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#26435;&#37325;&#24041;&#22266;&#29992;&#20110;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Self-paced Weight Consolidation for Continual Learning. (arXiv:2307.10845v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10845
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#26435;&#37325;&#24041;&#22266;&#65288;spWC&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#35780;&#20272;&#20808;&#21069;&#20219;&#21153;&#30340;&#21306;&#20998;&#24615;&#36129;&#29486;&#65292;&#23454;&#29616;&#31283;&#20581;&#30340;&#25345;&#32493;&#23398;&#20064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#26681;&#25454;&#20851;&#38190;&#32489;&#25928;&#25351;&#26631;&#65288;&#21363;&#20934;&#30830;&#24230;&#65289;&#26469;&#34913;&#37327;&#38590;&#24230;&#65292;&#21453;&#26144;&#36807;&#21435;&#20219;&#21153;&#30340;&#20248;&#20808;&#32423;&#65292;&#24182;&#23545;&#20808;&#21069;&#20219;&#21153;&#36827;&#34892;&#25490;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#31639;&#27861;&#22312;&#38450;&#27490;&#39034;&#24207;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#26041;&#38754;&#24456;&#21463;&#27426;&#36814;&#65292;&#23427;&#20204;&#20351;&#24471;&#26032;&#20219;&#21153;&#30340;&#21442;&#25968;&#20445;&#25345;&#25509;&#36817;&#20110;&#20808;&#21069;&#20219;&#21153;&#30340;&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;(1)&#22914;&#26524;&#19981;&#21306;&#20998;&#20808;&#21069;&#23398;&#20064;&#20219;&#21153;&#30340;&#36129;&#29486;&#65292;&#26032;&#30340;&#25345;&#32493;&#23398;&#20064;&#22120;&#30340;&#24615;&#33021;&#23558;&#38477;&#20302;&#65307;(2)&#38543;&#30528;&#20219;&#21153;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#35745;&#31639;&#25104;&#26412;&#23558;&#22823;&#22823;&#22686;&#21152;&#65292;&#22240;&#20026;&#22823;&#22810;&#25968;&#29616;&#26377;&#31639;&#27861;&#22312;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#38656;&#35201;&#23545;&#25152;&#26377;&#20808;&#21069;&#20219;&#21153;&#36827;&#34892;&#27491;&#21017;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#20197;&#19978;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#33258;&#25105;&#23398;&#20064;&#26435;&#37325;&#24041;&#22266;&#65288;spWC&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#35780;&#20272;&#20808;&#21069;&#20219;&#21153;&#30340;&#21306;&#20998;&#24615;&#36129;&#29486;&#65292;&#23454;&#29616;&#31283;&#20581;&#30340;&#25345;&#32493;&#23398;&#20064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#20851;&#38190;&#32489;&#25928;&#25351;&#26631;&#65288;&#21363;&#20934;&#30830;&#24230;&#65289;&#26469;&#34913;&#37327;&#38590;&#24230;&#65292;&#20197;&#21453;&#26144;&#36807;&#21435;&#20219;&#21153;&#30340;&#20248;&#20808;&#32423;&#12290;&#22312;&#36935;&#21040;&#26032;&#20219;&#21153;&#26102;&#65292;&#26681;&#25454;&#20248;&#20808;&#32423;&#65292;&#23545;&#25152;&#26377;&#20808;&#21069;&#20219;&#21153;&#36827;&#34892;&#20174;&#8220;&#22256;&#38590;&#8221;&#21040;&#8220;&#31616;&#21333;&#8221;&#30340;&#25490;&#24207;&#12290;&#28982;&#21518;&#65292;p
&lt;/p&gt;
&lt;p&gt;
Continual learning algorithms which keep the parameters of new tasks close to that of previous tasks, are popular in preventing catastrophic forgetting in sequential task learning settings. However, 1) the performance for the new continual learner will be degraded without distinguishing the contributions of previously learned tasks; 2) the computational cost will be greatly increased with the number of tasks, since most existing algorithms need to regularize all previous tasks when learning new tasks. To address the above challenges, we propose a self-paced Weight Consolidation (spWC) framework to attain robust continual learning via evaluating the discriminative contributions of previous tasks. To be specific, we develop a self-paced regularization to reflect the priorities of past tasks via measuring difficulty based on key performance indicator (i.e., accuracy). When encountering a new task, all previous tasks are sorted from "difficult" to "easy" based on the priorities. Then the p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;U-Net&#21644;&#21367;&#31215;LSTM&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#20840;&#29699;&#38477;&#27700;&#30340;&#21363;&#26102;&#24773;&#20917;&#65292;&#23588;&#20854;&#22312;&#26497;&#31471;&#38477;&#27700;&#26041;&#38754;&#30340;&#39044;&#27979;&#20934;&#30830;&#29575;&#26356;&#39640;&#12290;</title><link>http://arxiv.org/abs/2307.10843</link><description>&lt;p&gt;
&#20840;&#29699;&#38477;&#27700;&#21363;&#26102;&#39044;&#27979;&#65306;&#22522;&#20110;GPM&#30340;&#38598;&#25104;&#22810;&#21355;&#26143;&#26816;&#32034;&#30340;U-Net&#21367;&#31215;LSTM&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Global Precipitation Nowcasting of Integrated Multi-satellitE Retrievals for GPM: A U-Net Convolutional LSTM Architecture. (arXiv:2307.10843v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10843
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;U-Net&#21644;&#21367;&#31215;LSTM&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#20840;&#29699;&#38477;&#27700;&#30340;&#21363;&#26102;&#24773;&#20917;&#65292;&#23588;&#20854;&#22312;&#26497;&#31471;&#38477;&#27700;&#26041;&#38754;&#30340;&#39044;&#27979;&#20934;&#30830;&#29575;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#29992;&#20110;&#27599;30&#20998;&#38047;&#20840;&#29699;&#36817;4&#23567;&#26102;&#30340;&#38477;&#27700;&#21363;&#26102;&#39044;&#27979;&#12290;&#35813;&#26550;&#26500;&#34701;&#21512;&#20102;U-Net&#21644;&#21367;&#31215;&#38271;&#30701;&#26399;&#35760;&#24518;(LSTM)&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#20351;&#29992;&#26469;&#33258;&#38598;&#25104;&#22810;&#21355;&#26143;&#26816;&#32034;(GPM)&#21644;&#20840;&#29699;&#39044;&#27979;&#31995;&#32479;(GFS)&#30340;&#20851;&#38190;&#38477;&#27700;&#39537;&#21160;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#30740;&#31350;&#20102;&#19981;&#21516;&#35757;&#32451;&#25439;&#22833;&#20989;&#25968;&#65288;&#21253;&#25324;&#22343;&#26041;&#24046;&#22238;&#24402;&#21644;&#32858;&#28966;&#25439;&#22833;&#20998;&#31867;&#65289;&#23545;&#38477;&#27700;&#29616;&#22312;&#39044;&#27979;&#36136;&#37327;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22238;&#24402;&#32593;&#32476;&#22312;&#25429;&#25417;&#36731;&#24230;&#38477;&#27700;&#65288;&#23567;&#20110;1.6 mm/hr&#65289;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#20998;&#31867;&#32593;&#32476;&#22312;&#26497;&#31471;&#38477;&#27700;&#65288;&#22823;&#20110;8 mm/hr&#65289;&#30340;&#39044;&#27979;&#26041;&#38754;&#65292;&#20197;&#20851;&#38190;&#25104;&#21151;&#25351;&#25968;(CSI)&#34913;&#37327;&#65292;&#21487;&#20197;&#32988;&#36807;&#22238;&#24402;&#32593;&#32476;&#12290;&#20351;&#29992;Wasserstein&#36317;&#31163;&#34920;&#26126;&#65292;&#20998;&#31867;&#32593;&#32476;&#39044;&#27979;&#30340;&#38477;&#27700;&#20855;&#26377;&#26356;&#25509;&#36817;&#30340;&#31867;&#21035;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a deep learning architecture for nowcasting of precipitation almost globally every 30 min with a 4-hour lead time. The architecture fuses a U-Net and a convolutional long short-term memory (LSTM) neural network and is trained using data from the Integrated MultisatellitE Retrievals for GPM (IMERG) and a few key precipitation drivers from the Global Forecast System (GFS). The impacts of different training loss functions, including the mean-squared error (regression) and the focal-loss (classification), on the quality of precipitation nowcasts are studied. The results indicate that the regression network performs well in capturing light precipitation (below 1.6 mm/hr), but the classification network can outperform the regression network for nowcasting of precipitation extremes (&gt;8 mm/hr), in terms of the critical success index (CSI).. Using the Wasserstein distance, it is shown that the predicted precipitation by the classification network has a closer class probabili
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#22495;&#20559;&#31227;&#19979;&#20351;&#29992;&#26631;&#31614;&#26657;&#20934;&#26041;&#27861;&#26469;&#25552;&#21319;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#24615;&#33021;&#30340;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2307.10842</link><description>&lt;p&gt;
&#26631;&#31614;&#26657;&#20934;&#22312;&#22495;&#20559;&#31227;&#19979;&#30340;&#35821;&#20041;&#20998;&#21106;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Label Calibration for Semantic Segmentation Under Domain Shift. (arXiv:2307.10842v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10842
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#22495;&#20559;&#31227;&#19979;&#20351;&#29992;&#26631;&#31614;&#26657;&#20934;&#26041;&#27861;&#26469;&#25552;&#21319;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#24615;&#33021;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#22312;&#26032;&#39046;&#22495;&#30340;&#25968;&#25454;&#19978;&#34920;&#29616;&#24456;&#21487;&#33021;&#22823;&#24133;&#24230;&#38477;&#20302;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#22495;&#20559;&#31227;&#19979;&#35745;&#31639;&#36719;&#26631;&#31614;&#21407;&#22411;&#65292;&#24182;&#26681;&#25454;&#19982;&#39044;&#27979;&#31867;&#21035;&#27010;&#29575;&#26368;&#25509;&#36817;&#30340;&#21407;&#22411;&#36827;&#34892;&#39044;&#27979;&#65292;&#26469;&#36866;&#24212;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#39046;&#22495;&#25968;&#25454;&#12290;&#25152;&#25552;&#20986;&#30340;&#36866;&#24212;&#36807;&#31243;&#24555;&#36895;&#19988;&#20960;&#20046;&#27809;&#26377;&#35745;&#31639;&#36164;&#28304;&#25104;&#26412;&#65292;&#24182;&#19988;&#33021;&#22815;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#39640;&#24230;&#23454;&#29992;&#30340;&#20174;&#21512;&#25104;&#21040;&#30495;&#23454;&#30340;&#35821;&#20041;&#20998;&#21106;&#38382;&#39064;&#19978;&#23637;&#31034;&#20102;&#26631;&#31614;&#26657;&#20934;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Performance of a pre-trained semantic segmentation model is likely to substantially decrease on data from a new domain. We show a pre-trained model can be adapted to unlabelled target domain data by calculating soft-label prototypes under the domain shift and making predictions according to the prototype closest to the vector with predicted class probabilities. The proposed adaptation procedure is fast, comes almost for free in terms of computational resources and leads to considerable performance improvements. We demonstrate the benefits of such label calibration on the highly-practical synthetic-to-real semantic segmentation problem.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#26041;&#27861;&#32467;&#21512;&#22810;&#20010;&#19987;&#23478;&#31034;&#33539;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#27169;&#20223;&#23398;&#20064;&#20013;&#25552;&#20379;&#20102;&#26356;&#21512;&#29702;&#30340;&#31034;&#33539;&#20960;&#20309;&#24179;&#22343;&#20540;&#12290;</title><link>http://arxiv.org/abs/2307.10810</link><description>&lt;p&gt;
&#20851;&#20110;&#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#32467;&#21512;&#19987;&#23478;&#31034;&#33539;&#22312;&#27169;&#20223;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Combining Expert Demonstrations in Imitation Learning via Optimal Transport. (arXiv:2307.10810v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10810
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#26041;&#27861;&#32467;&#21512;&#22810;&#20010;&#19987;&#23478;&#31034;&#33539;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#27169;&#20223;&#23398;&#20064;&#20013;&#25552;&#20379;&#20102;&#26356;&#21512;&#29702;&#30340;&#31034;&#33539;&#20960;&#20309;&#24179;&#22343;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#20223;&#23398;&#20064;&#26088;&#22312;&#36890;&#36807;&#19987;&#23478;&#31034;&#33539;&#26469;&#25945;&#25480;&#26234;&#33021;&#20307;&#29305;&#23450;&#20219;&#21153;&#12290;&#27169;&#20223;&#23398;&#20064;&#30340;&#20851;&#38190;&#26041;&#27861;&#20043;&#19968;&#26159;&#23450;&#20041;&#26234;&#33021;&#20307;&#21644;&#19987;&#23478;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#24182;&#25214;&#21040;&#20351;&#35813;&#36317;&#31163;&#26368;&#23567;&#21270;&#30340;&#26234;&#33021;&#20307;&#31574;&#30053;&#12290;&#26368;&#20248;&#20256;&#36755;&#26041;&#27861;&#22312;&#27169;&#20223;&#23398;&#20064;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#25552;&#20379;&#20102;&#34913;&#37327;&#26234;&#33021;&#20307;&#21644;&#19987;&#23478;&#36712;&#36857;&#20043;&#38388;&#26377;&#24847;&#20041;&#36317;&#31163;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#26368;&#20339;&#22320;&#32467;&#21512;&#22810;&#20010;&#19987;&#23478;&#31034;&#33539;&#30340;&#38382;&#39064;&#24182;&#27809;&#26377;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#12290;&#26631;&#20934;&#26041;&#27861;&#26159;&#31616;&#21333;&#22320;&#20018;&#32852;&#29366;&#24577;&#65288;-&#21160;&#20316;&#65289;&#36712;&#36857;&#65292;&#20294;&#22312;&#36712;&#36857;&#20026;&#22810;&#27169;&#24577;&#26102;&#23384;&#22312;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65292;&#20351;&#29992;&#22810;&#36793;&#38469;&#26368;&#20248;&#20256;&#36755;&#36317;&#31163;&#65292;&#33021;&#22815;&#22312;&#26368;&#20248;&#20256;&#36755;&#30340;&#24847;&#20041;&#19979;&#32467;&#21512;&#22810;&#20010;&#21644;&#22810;&#26679;&#21270;&#30340;&#29366;&#24577;&#36712;&#36857;&#65292;&#25552;&#20379;&#26356;&#21512;&#29702;&#30340;&#31034;&#33539;&#20960;&#20309;&#24179;&#22343;&#20540;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#26234;&#33021;&#20307;&#33021;&#22815;&#20174;&#22810;&#20010;&#19987;&#23478;&#20013;&#23398;&#20064;&#65292;&#24182;&#22312;OpenAI Gym&#25511;&#21046;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#25928;&#29575;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imitation learning (IL) seeks to teach agents specific tasks through expert demonstrations. One of the key approaches to IL is to define a distance between agent and expert and to find an agent policy that minimizes that distance. Optimal transport methods have been widely used in imitation learning as they provide ways to measure meaningful distances between agent and expert trajectories. However, the problem of how to optimally combine multiple expert demonstrations has not been widely studied. The standard method is to simply concatenate state (-action) trajectories, which is problematic when trajectories are multi-modal. We propose an alternative method that uses a multi-marginal optimal transport distance and enables the combination of multiple and diverse state-trajectories in the OT sense, providing a more sensible geometric average of the demonstrations. Our approach enables an agent to learn from several experts, and its efficiency is analyzed on OpenAI Gym control environment
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SplitFC&#30340;&#36890;&#20449;&#39640;&#25928;&#30340;&#20998;&#21106;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20004;&#31181;&#33258;&#36866;&#24212;&#21387;&#32553;&#31574;&#30053;&#26469;&#20943;&#23569;&#20013;&#38388;&#29305;&#24449;&#21644;&#26799;&#24230;&#21521;&#37327;&#30340;&#36890;&#20449;&#24320;&#38144;&#65292;&#36825;&#20123;&#31574;&#30053;&#20998;&#21035;&#26159;&#33258;&#36866;&#24212;&#29305;&#24449;&#36880;&#28176;&#25481;&#33853;&#21644;&#33258;&#36866;&#24212;&#29305;&#24449;&#36880;&#28176;&#37327;&#21270;&#12290;</title><link>http://arxiv.org/abs/2307.10805</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#36866;&#24212;&#29305;&#24449;&#36880;&#28176;&#21387;&#32553;&#23454;&#29616;&#39640;&#25928;&#30340;&#20998;&#21106;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Communication-Efficient Split Learning via Adaptive Feature-Wise Compression. (arXiv:2307.10805v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10805
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SplitFC&#30340;&#36890;&#20449;&#39640;&#25928;&#30340;&#20998;&#21106;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20004;&#31181;&#33258;&#36866;&#24212;&#21387;&#32553;&#31574;&#30053;&#26469;&#20943;&#23569;&#20013;&#38388;&#29305;&#24449;&#21644;&#26799;&#24230;&#21521;&#37327;&#30340;&#36890;&#20449;&#24320;&#38144;&#65292;&#36825;&#20123;&#31574;&#30053;&#20998;&#21035;&#26159;&#33258;&#36866;&#24212;&#29305;&#24449;&#36880;&#28176;&#25481;&#33853;&#21644;&#33258;&#36866;&#24212;&#29305;&#24449;&#36880;&#28176;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SplitFC&#30340;&#26032;&#39062;&#30340;&#36890;&#20449;&#39640;&#25928;&#30340;&#20998;&#21106;&#23398;&#20064;&#65288;SL&#65289;&#26694;&#26550;&#65292;&#23427;&#20943;&#23569;&#20102;&#22312;SL&#22521;&#35757;&#36807;&#31243;&#20013;&#20256;&#36755;&#20013;&#38388;&#29305;&#24449;&#21644;&#26799;&#24230;&#21521;&#37327;&#25152;&#38656;&#30340;&#36890;&#20449;&#24320;&#38144;&#12290;SplitFC&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#21033;&#29992;&#30697;&#38453;&#30340;&#21015;&#25152;&#23637;&#31034;&#30340;&#19981;&#21516;&#30340;&#31163;&#25955;&#31243;&#24230;&#12290;SplitFC&#25972;&#21512;&#20102;&#20004;&#31181;&#21387;&#32553;&#31574;&#30053;&#65306;&#65288;i&#65289;&#33258;&#36866;&#24212;&#29305;&#24449;&#36880;&#28176;&#25481;&#33853;&#21644;&#65288;ii&#65289;&#33258;&#36866;&#24212;&#29305;&#24449;&#36880;&#28176;&#37327;&#21270;&#12290;&#22312;&#31532;&#19968;&#31181;&#31574;&#30053;&#20013;&#65292;&#20013;&#38388;&#29305;&#24449;&#21521;&#37327;&#26681;&#25454;&#36825;&#20123;&#21521;&#37327;&#30340;&#26631;&#20934;&#20559;&#24046;&#30830;&#23450;&#33258;&#36866;&#24212;&#25481;&#33853;&#27010;&#29575;&#36827;&#34892;&#25481;&#33853;&#12290;&#28982;&#21518;&#65292;&#30001;&#20110;&#38142;&#24335;&#35268;&#21017;&#65292;&#19982;&#34987;&#20002;&#24323;&#30340;&#29305;&#24449;&#21521;&#37327;&#30456;&#20851;&#32852;&#30340;&#20013;&#38388;&#26799;&#24230;&#21521;&#37327;&#20063;&#20250;&#34987;&#20002;&#24323;&#12290;&#22312;&#31532;&#20108;&#31181;&#31574;&#30053;&#20013;&#65292;&#38750;&#20002;&#24323;&#30340;&#20013;&#38388;&#29305;&#24449;&#21644;&#26799;&#24230;&#21521;&#37327;&#20351;&#29992;&#22522;&#20110;&#21521;&#37327;&#33539;&#22260;&#30830;&#23450;&#30340;&#33258;&#36866;&#24212;&#37327;&#21270;&#32423;&#21035;&#36827;&#34892;&#37327;&#21270;&#12290;&#20026;&#20102;&#23613;&#37327;&#20943;&#23567;&#37327;&#21270;&#35823;&#24046;&#65292;&#26368;&#20248;&#37327;&#21270;&#26159;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel communication-efficient split learning (SL) framework, named SplitFC, which reduces the communication overhead required for transmitting intermediate feature and gradient vectors during the SL training process. The key idea of SplitFC is to leverage different dispersion degrees exhibited in the columns of the matrices. SplitFC incorporates two compression strategies: (i) adaptive feature-wise dropout and (ii) adaptive feature-wise quantization. In the first strategy, the intermediate feature vectors are dropped with adaptive dropout probabilities determined based on the standard deviation of these vectors. Then, by the chain rule, the intermediate gradient vectors associated with the dropped feature vectors are also dropped. In the second strategy, the non-dropped intermediate feature and gradient vectors are quantized using adaptive quantization levels determined based on the ranges of the vectors. To minimize the quantization error, the optimal quantizatio
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24635;&#32467;&#20102;&#28023;&#27915;&#31185;&#23398;&#39046;&#22495;&#30340;&#26102;&#31354;&#25968;&#25454;&#25366;&#25496;&#30740;&#31350;&#65292;&#21253;&#25324;&#24191;&#27867;&#20351;&#29992;&#30340;ST&#28023;&#27915;&#25968;&#25454;&#38598;&#21644;&#20854;&#29305;&#28857;&#65292;&#20197;&#21450;ST&#28023;&#27915;&#25968;&#25454;&#36136;&#37327;&#22686;&#24378;&#25216;&#26415;&#21644;&#29616;&#26377;STDM&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2307.10803</link><description>&lt;p&gt;
&#28023;&#27915;&#31185;&#23398;&#30340;&#26102;&#31354;&#25968;&#25454;&#25366;&#25496;&#65306;&#25968;&#25454;&#12289;&#26041;&#27861;&#21644;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
Spatial-Temporal Data Mining for Ocean Science: Data, Methodologies, and Opportunities. (arXiv:2307.10803v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10803
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24635;&#32467;&#20102;&#28023;&#27915;&#31185;&#23398;&#39046;&#22495;&#30340;&#26102;&#31354;&#25968;&#25454;&#25366;&#25496;&#30740;&#31350;&#65292;&#21253;&#25324;&#24191;&#27867;&#20351;&#29992;&#30340;ST&#28023;&#27915;&#25968;&#25454;&#38598;&#21644;&#20854;&#29305;&#28857;&#65292;&#20197;&#21450;ST&#28023;&#27915;&#25968;&#25454;&#36136;&#37327;&#22686;&#24378;&#25216;&#26415;&#21644;&#29616;&#26377;STDM&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28023;&#27915;&#26102;&#31354;&#25968;&#25454;&#37327;&#30340;&#22686;&#21152;&#65292;&#24050;&#32463;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#31354;&#38388;-&#26102;&#38388;&#25968;&#25454;&#25366;&#25496;&#65288;STDM&#65289;&#30740;&#31350;&#26469;&#35299;&#20915;&#21508;&#31181;&#28023;&#27915;&#38382;&#39064;&#65292;&#22914;&#27668;&#20505;&#39044;&#27979;&#21644;&#28798;&#23475;&#35686;&#31034;&#12290;&#19982;&#20856;&#22411;&#30340;ST&#25968;&#25454;&#65288;&#22914;&#20132;&#36890;&#25968;&#25454;&#65289;&#30456;&#27604;&#65292;ST&#28023;&#27915;&#25968;&#25454;&#26356;&#21152;&#22797;&#26434;&#65292;&#20855;&#26377;&#19968;&#20123;&#29420;&#29305;&#30340;&#29305;&#24449;&#65292;&#20363;&#22914;&#22810;&#26679;&#21270;&#30340;&#21306;&#22495;&#24615;&#21644;&#39640;&#31232;&#30095;&#24615;&#12290;&#36825;&#20123;&#29305;&#28857;&#20351;&#24471;&#35774;&#35745;&#21644;&#35757;&#32451;STDM&#27169;&#22411;&#21464;&#24471;&#22256;&#38590;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36824;&#32570;&#20047;&#23545;&#36825;&#20123;&#30740;&#31350;&#30340;&#27010;&#36848;&#65292;&#36825;&#38459;&#30861;&#20102;&#35745;&#31639;&#26426;&#31185;&#23398;&#23478;&#22312;&#28023;&#27915;&#39046;&#22495;&#35782;&#21035;&#30740;&#31350;&#38382;&#39064;&#65292;&#21516;&#26102;&#20063;&#20351;&#28023;&#27915;&#31185;&#23398;&#30740;&#31350;&#20154;&#21592;&#19981;&#24895;&#24212;&#29992;&#20808;&#36827;&#30340;STDM&#25216;&#26415;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20221;&#32508;&#21512;&#35843;&#26597;&#65292;&#24635;&#32467;&#20102;&#28023;&#27915;&#39046;&#22495;&#20013;&#29616;&#26377;&#30340;STDM&#30740;&#31350;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#24635;&#32467;&#20102;&#24191;&#27867;&#20351;&#29992;&#30340;ST&#28023;&#27915;&#25968;&#25454;&#38598;&#24182;&#30830;&#23450;&#20102;&#23427;&#20204;&#30340;&#29420;&#29305;&#29305;&#28857;&#12290;&#28982;&#21518;&#65292;&#35752;&#35770;&#20102;&#20856;&#22411;&#30340;ST&#28023;&#27915;&#25968;&#25454;&#36136;&#37327;&#22686;&#24378;&#25216;&#26415;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#23545;&#29616;&#26377;&#30340;STDM&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increasing amount of spatial-temporal~(ST) ocean data, numerous spatial-temporal data mining (STDM) studies have been conducted to address various oceanic issues, e.g., climate forecasting and disaster warning. Compared with typical ST data (e.g., traffic data), ST ocean data is more complicated with some unique characteristics, e.g., diverse regionality and high sparsity. These characteristics make it difficult to design and train STDM models. Unfortunately, an overview of these studies is still missing, hindering computer scientists to identify the research issues in ocean while discouraging researchers in ocean science from applying advanced STDM techniques. To remedy this situation, we provide a comprehensive survey to summarize existing STDM studies in ocean. Concretely, we first summarize the widely-used ST ocean datasets and identify their unique characteristics. Then, typical ST ocean data quality enhancement techniques are discussed. Next, we classify existing STDM st
&lt;/p&gt;</description></item><item><title>Meta-Transformer&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#19968;&#20010;&#20923;&#32467;&#30340;&#32534;&#30721;&#22120;&#36827;&#34892;&#22810;&#27169;&#24577;&#24863;&#30693;&#65292;&#22312;&#27809;&#26377;&#25104;&#23545;&#22810;&#27169;&#24577;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#22788;&#29702;&#21508;&#31181;&#27169;&#24577;&#65292;&#24182;&#19988;&#33021;&#22815;&#25552;&#21462;&#36755;&#20837;&#25968;&#25454;&#30340;&#39640;&#32423;&#35821;&#20041;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2307.10802</link><description>&lt;p&gt;
Meta-Transformer: &#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Meta-Transformer: A Unified Framework for Multimodal Learning. (arXiv:2307.10802v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10802
&lt;/p&gt;
&lt;p&gt;
Meta-Transformer&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#19968;&#20010;&#20923;&#32467;&#30340;&#32534;&#30721;&#22120;&#36827;&#34892;&#22810;&#27169;&#24577;&#24863;&#30693;&#65292;&#22312;&#27809;&#26377;&#25104;&#23545;&#22810;&#27169;&#24577;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#22788;&#29702;&#21508;&#31181;&#27169;&#24577;&#65292;&#24182;&#19988;&#33021;&#22815;&#25552;&#21462;&#36755;&#20837;&#25968;&#25454;&#30340;&#39640;&#32423;&#35821;&#20041;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#23398;&#20064;&#26088;&#22312;&#26500;&#24314;&#33021;&#22815;&#22788;&#29702;&#21644;&#20851;&#32852;&#22810;&#31181;&#27169;&#24577;&#30340;&#20449;&#24687;&#30340;&#27169;&#22411;&#12290;&#23613;&#31649;&#22312;&#36825;&#20010;&#39046;&#22495;&#24050;&#32463;&#26377;&#22810;&#24180;&#30340;&#21457;&#23637;&#65292;&#20294;&#30001;&#20110;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#22266;&#26377;&#24046;&#36317;&#65292;&#35774;&#35745;&#19968;&#20010;&#29992;&#20110;&#22788;&#29702;&#21508;&#31181;&#27169;&#24577;&#30340;&#32479;&#19968;&#32593;&#32476;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Meta-Transformer&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#19968;&#20010;&#20923;&#32467;&#30340;&#32534;&#30721;&#22120;&#22312;&#27809;&#26377;&#25104;&#23545;&#22810;&#27169;&#24577;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#22810;&#27169;&#24577;&#24863;&#30693;&#12290;&#22312;Meta-Transformer&#20013;&#65292;&#26469;&#33258;&#21508;&#31181;&#27169;&#24577;&#30340;&#21407;&#22987;&#36755;&#20837;&#25968;&#25454;&#34987;&#26144;&#23556;&#21040;&#19968;&#20010;&#20849;&#20139;&#30340;&#26631;&#35760;&#31354;&#38388;&#20013;&#65292;&#20351;&#24471;&#21518;&#32493;&#30340;&#32534;&#30721;&#22120;&#21487;&#20197;&#25552;&#21462;&#36755;&#20837;&#25968;&#25454;&#30340;&#39640;&#32423;&#35821;&#20041;&#29305;&#24449;&#12290;&#30001;&#19977;&#20010;&#20027;&#35201;&#32452;&#20214;&#32452;&#25104;&#65306;&#19968;&#20010;&#32479;&#19968;&#30340;&#25968;&#25454;&#26631;&#35760;&#22120;&#65292;&#19968;&#20010;&#27169;&#24577;&#20849;&#20139;&#30340;&#32534;&#30721;&#22120;&#21644;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#29305;&#23450;&#20219;&#21153;&#22836;&#65292;Meta-Transformer&#26159;&#31532;&#19968;&#20010;&#22312;12&#31181;&#27169;&#24577;&#19978;&#36827;&#34892;&#32479;&#19968;&#23398;&#20064;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal learning aims to build models that can process and relate information from multiple modalities. Despite years of development in this field, it still remains challenging to design a unified network for processing various modalities ($\textit{e.g.}$ natural language, 2D images, 3D point clouds, audio, video, time series, tabular data) due to the inherent gaps among them. In this work, we propose a framework, named Meta-Transformer, that leverages a $\textbf{frozen}$ encoder to perform multimodal perception without any paired multimodal training data. In Meta-Transformer, the raw input data from various modalities are mapped into a shared token space, allowing a subsequent encoder with frozen parameters to extract high-level semantic features of the input data. Composed of three main components: a unified data tokenizer, a modality-shared encoder, and task-specific heads for downstream tasks, Meta-Transformer is the first framework to perform unified learning across 12 modaliti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#23569;&#37327;&#26679;&#26412;&#21644;&#22823;&#37327;&#26679;&#26412;&#35774;&#32622;&#19979;&#65292;PatchCore&#31639;&#27861;&#22312;&#24322;&#24120;&#26816;&#27979;/&#24322;&#24120;&#20998;&#21106;&#24615;&#33021;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;&#36890;&#36807;&#20248;&#21270;&#36229;&#21442;&#25968;&#21644;&#20511;&#37492;&#23569;&#37327;&#26679;&#26412;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.10792</link><description>&lt;p&gt;
&#20248;&#21270;PatchCore&#20197;&#29992;&#20110;&#23569;&#37327;/&#22823;&#37327;&#26679;&#26412;&#30340;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Optimizing PatchCore for Few/many-shot Anomaly Detection. (arXiv:2307.10792v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10792
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#23569;&#37327;&#26679;&#26412;&#21644;&#22823;&#37327;&#26679;&#26412;&#35774;&#32622;&#19979;&#65292;PatchCore&#31639;&#27861;&#22312;&#24322;&#24120;&#26816;&#27979;/&#24322;&#24120;&#20998;&#21106;&#24615;&#33021;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;&#36890;&#36807;&#20248;&#21270;&#36229;&#21442;&#25968;&#21644;&#20511;&#37492;&#23569;&#37327;&#26679;&#26412;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23569;&#37327;&#26679;&#26412;&#30340;&#24322;&#24120;&#26816;&#27979;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#24322;&#24120;&#26816;&#27979;&#23376;&#39046;&#22495;&#65292;&#23427;&#35797;&#22270;&#36890;&#36807;&#20165;&#20351;&#29992;&#23569;&#37327;&#36873;&#23450;&#30340;&#26679;&#26412;&#26469;&#21306;&#20998;&#27491;&#24120;&#21644;&#24322;&#24120;&#25968;&#25454;&#12290;&#23613;&#31649;&#26032;&#25552;&#20986;&#30340;&#23569;&#37327;&#26679;&#26412;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#19982;&#29992;&#20110;&#23436;&#20840;&#26679;&#26412;&#39046;&#22495;&#30340;&#39044;&#20808;&#23384;&#22312;&#30340;&#31639;&#27861;&#36827;&#34892;&#27604;&#36739;&#20316;&#20026;&#22522;&#32447;&#65292;&#20294;&#23427;&#20204;&#24182;&#27809;&#26377;&#19987;&#38376;&#38024;&#23545;&#23569;&#37327;&#26679;&#26412;&#36827;&#34892;&#20248;&#21270;&#12290;&#22240;&#27492;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#36825;&#20123;&#39044;&#20808;&#23384;&#22312;&#30340;&#31639;&#27861;&#30340;&#24615;&#33021;&#26159;&#21542;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#35813;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23545;PatchCore&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#35813;&#31639;&#27861;&#26159;&#30446;&#21069;&#29366;&#24577;&#26368;&#20339;&#30340;&#23436;&#20840;&#26679;&#26412;&#24322;&#24120;&#26816;&#27979;/&#24322;&#24120;&#20998;&#21106;&#31639;&#27861;&#65292;&#30740;&#31350;&#20854;&#22312;&#23569;&#37327;&#26679;&#26412;&#21644;&#22823;&#37327;&#26679;&#26412;&#35774;&#32622;&#19979;&#30340;&#24322;&#24120;&#26816;&#27979;/&#24322;&#24120;&#20998;&#21106;&#24615;&#33021;&#12290;&#25105;&#20204;&#20551;&#35774;&#36890;&#36807;&#65288;I&#65289;&#20248;&#21270;&#20854;&#21508;&#31181;&#36229;&#21442;&#25968;&#21644;&#65288;II&#65289;&#36716;&#31227;&#24050;&#30693;&#21487;&#25913;&#21892;&#23569;&#37327;&#26679;&#26412;&#30417;&#30563;&#23398;&#20064;&#30340;&#25216;&#26415;&#21040;&#24322;&#24120;&#26816;&#27979;&#39046;&#22495;&#65292;&#21487;&#20197;&#23454;&#29616;&#36827;&#19968;&#27493;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#23545;&#20844;&#20849;VisA&#21644;MVTec&#24322;&#24120;&#26816;&#27979;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#35814;&#23613;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#65288;I&#65289;sign
&lt;/p&gt;
&lt;p&gt;
Few-shot anomaly detection (AD) is an emerging sub-field of general AD, and tries to distinguish between normal and anomalous data using only few selected samples. While newly proposed few-shot AD methods do compare against pre-existing algorithms developed for the full-shot domain as baselines, they do not dedicatedly optimize them for the few-shot setting. It thus remains unclear if the performance of such pre-existing algorithms can be further improved. We address said question in this work. Specifically, we present a study on the AD/anomaly segmentation (AS) performance of PatchCore, the current state-of-the-art full-shot AD/AS algorithm, in both the few-shot and the many-shot settings. We hypothesize that further performance improvements can be realized by (I) optimizing its various hyperparameters, and by (II) transferring techniques known to improve few-shot supervised learning to the AD domain. Exhaustive experiments on the public VisA and MVTec AD datasets reveal that (I) sign
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#25915;&#20987;&#28151;&#21512;&#20998;&#31867;&#22120;&#30340;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#20010;&#21487;&#21462;&#24615;&#36136;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#29616;&#26377;&#30340;&#25915;&#20987;&#26041;&#27861;&#19981;&#33021;&#21516;&#26102;&#28385;&#36275;&#36825;&#20004;&#20010;&#24615;&#36136;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#24615;&#33021;&#22909;&#12290;</title><link>http://arxiv.org/abs/2307.10788</link><description>&lt;p&gt;
&#25915;&#20987;&#28151;&#21512;&#20998;&#31867;&#22120;&#30340;&#23545;&#25239;&#24615;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Adversarial attacks for mixtures of classifiers. (arXiv:2307.10788v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10788
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#25915;&#20987;&#28151;&#21512;&#20998;&#31867;&#22120;&#30340;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#20010;&#21487;&#21462;&#24615;&#36136;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#29616;&#26377;&#30340;&#25915;&#20987;&#26041;&#27861;&#19981;&#33021;&#21516;&#26102;&#28385;&#36275;&#36825;&#20004;&#20010;&#24615;&#36136;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#24615;&#33021;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#20998;&#31867;&#22120;&#65288;&#21363;&#38543;&#26426;&#38598;&#25104;&#65289;&#34987;&#25552;&#20986;&#20316;&#20026;&#25552;&#39640;&#23545;&#25239;&#24615;&#25915;&#20987;&#40065;&#26834;&#24615;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24050;&#32463;&#35777;&#26126;&#29616;&#26377;&#30340;&#25915;&#20987;&#26041;&#27861;&#24182;&#19981;&#36866;&#29992;&#20110;&#36825;&#31181;&#31867;&#22411;&#30340;&#20998;&#31867;&#22120;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#20197;&#19968;&#31181;&#21407;&#21017;&#24615;&#30340;&#26041;&#24335;&#25915;&#20987;&#28151;&#21512;&#20998;&#31867;&#22120;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23545;&#38382;&#39064;&#30340;&#20960;&#20309;&#20998;&#26512;&#24341;&#20837;&#20102;&#25915;&#20987;&#30340;&#20004;&#20010;&#21487;&#21462;&#24615;&#36136;&#65288;&#25928;&#21147;&#21644;&#26497;&#22823;&#24615;&#65289;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#29616;&#26377;&#30340;&#25915;&#20987;&#26041;&#27861;&#19981;&#33021;&#21516;&#26102;&#28385;&#36275;&#36825;&#20004;&#20010;&#24615;&#36136;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#26230;&#26684;&#25856;&#30331;&#25915;&#20987;&#30340;&#26032;&#25915;&#20987;&#26041;&#27861;&#65292;&#22312;&#20108;&#20803;&#32447;&#24615;&#35774;&#32622;&#19979;&#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#36890;&#36807;&#23545;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#23637;&#31034;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mixtures of classifiers (a.k.a. randomized ensembles) have been proposed as a way to improve robustness against adversarial attacks. However, it has been shown that existing attacks are not well suited for this kind of classifiers. In this paper, we discuss the problem of attacking a mixture in a principled way and introduce two desirable properties of attacks based on a geometrical analysis of the problem (effectiveness and maximality). We then show that existing attacks do not meet both of these properties. Finally, we introduce a new attack called lattice climber attack with theoretical guarantees on the binary linear setting, and we demonstrate its performance by conducting experiments on synthetic and real datasets.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21069;&#39304;&#26041;&#27861;&#26469;&#35299;&#20915;&#28304;&#26080;&#20851;&#22495;&#36866;&#24212;&#38382;&#39064;&#65292;&#36890;&#36807;&#35745;&#31639;&#31867;&#30340;&#21407;&#22411;&#26469;&#22788;&#29702;&#22495;&#20559;&#31227;&#65292;&#30456;&#36739;&#20110;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#22312;&#31934;&#24230;&#21644;&#26102;&#38388;&#19978;&#37117;&#26377;&#26174;&#33879;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2307.10787</link><description>&lt;p&gt;
&#36890;&#36807;&#31867;&#21407;&#22411;&#23454;&#29616;&#30340;&#21069;&#39304;&#28304;&#26080;&#20851;&#22495;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Feed-Forward Source-Free Domain Adaptation via Class Prototypes. (arXiv:2307.10787v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10787
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21069;&#39304;&#26041;&#27861;&#26469;&#35299;&#20915;&#28304;&#26080;&#20851;&#22495;&#36866;&#24212;&#38382;&#39064;&#65292;&#36890;&#36807;&#35745;&#31639;&#31867;&#30340;&#21407;&#22411;&#26469;&#22788;&#29702;&#22495;&#20559;&#31227;&#65292;&#30456;&#36739;&#20110;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#22312;&#31934;&#24230;&#21644;&#26102;&#38388;&#19978;&#37117;&#26377;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27809;&#26377;&#35775;&#38382;&#28304;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#28304;&#26080;&#20851;&#22495;&#36866;&#24212;&#22240;&#20854;&#23454;&#29992;&#24615;&#21644;&#26080;&#38656;&#35775;&#38382;&#28304;&#25968;&#25454;&#32780;&#21464;&#24471;&#27969;&#34892;&#12290;&#28982;&#32780;&#65292;&#36866;&#24212;&#36807;&#31243;&#20173;&#28982;&#38656;&#35201;&#30456;&#24403;&#38271;&#30340;&#26102;&#38388;&#65292;&#24182;&#19988;&#20027;&#35201;&#22522;&#20110;&#20381;&#36182;&#20110;&#21453;&#21521;&#20256;&#25773;&#30340;&#20248;&#21270;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#21069;&#39304;&#26041;&#27861;&#65292;&#25361;&#25112;&#20102;&#22522;&#20110;&#21453;&#21521;&#20256;&#25773;&#30340;&#36866;&#24212;&#30340;&#24517;&#35201;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#35745;&#31639;&#20986;&#30340;&#31867;&#22312;&#22495;&#20559;&#31227;&#19979;&#30340;&#21407;&#22411;&#12290;&#19982;&#39044;&#35757;&#32451;&#27169;&#22411;&#30456;&#27604;&#65292;&#23427;&#22312;&#31934;&#24230;&#19978;&#21462;&#24471;&#20102;&#24378;&#22823;&#30340;&#25913;&#36827;&#65292;&#24182;&#19988;&#21482;&#38656;&#35201;&#29616;&#26377;&#22495;&#36866;&#24212;&#26041;&#27861;&#25152;&#38656;&#26102;&#38388;&#30340;&#19968;&#23567;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Source-free domain adaptation has become popular because of its practical usefulness and no need to access source data. However, the adaptation process still takes a considerable amount of time and is predominantly based on optimization that relies on back-propagation. In this work we present a simple feed-forward approach that challenges the need for back-propagation based adaptation. Our approach is based on computing prototypes of classes under the domain shift using a pre-trained model. It achieves strong improvements in accuracy compared to the pre-trained model and requires only a small fraction of time of existing domain adaptation methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;Beam Tree&#36882;&#24402;&#31639;&#27861;&#65288;BT-RvNN&#65289;&#65292;&#36890;&#36807;&#35299;&#20915;&#35780;&#20998;&#20989;&#25968;&#21644;&#36882;&#24402;&#21333;&#20803;&#20989;&#25968;&#30340;&#32416;&#32544;&#38382;&#39064;&#20197;&#21450;&#31616;&#21270;&#20869;&#23384;&#20351;&#29992;&#65292;&#25104;&#21151;&#38477;&#20302;&#20102;BT-RvNN&#30340;&#20869;&#23384;&#20351;&#29992;&#12290;&#36825;&#20010;&#31639;&#27861;&#22312;ListOps&#20219;&#21153;&#20013;&#36798;&#21040;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#27700;&#24179;&#65292;&#24182;&#22312;&#20854;&#20182;&#20219;&#21153;&#20013;&#20445;&#25345;&#20102;&#31867;&#20284;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.10779</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;Beam Tree&#36882;&#24402;
&lt;/p&gt;
&lt;p&gt;
Efficient Beam Tree Recursion. (arXiv:2307.10779v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10779
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;Beam Tree&#36882;&#24402;&#31639;&#27861;&#65288;BT-RvNN&#65289;&#65292;&#36890;&#36807;&#35299;&#20915;&#35780;&#20998;&#20989;&#25968;&#21644;&#36882;&#24402;&#21333;&#20803;&#20989;&#25968;&#30340;&#32416;&#32544;&#38382;&#39064;&#20197;&#21450;&#31616;&#21270;&#20869;&#23384;&#20351;&#29992;&#65292;&#25104;&#21151;&#38477;&#20302;&#20102;BT-RvNN&#30340;&#20869;&#23384;&#20351;&#29992;&#12290;&#36825;&#20010;&#31639;&#27861;&#22312;ListOps&#20219;&#21153;&#20013;&#36798;&#21040;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#27700;&#24179;&#65292;&#24182;&#22312;&#20854;&#20182;&#20219;&#21153;&#20013;&#20445;&#25345;&#20102;&#31867;&#20284;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25552;&#20986;&#30340;Beam Tree&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;BT-RvNN&#65289;&#26159;Gumbel Tree RvNN&#30340;&#31616;&#21333;&#25193;&#23637;&#65292;&#24050;&#32463;&#22312;ListOps&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#38271;&#24230;&#27867;&#21270;&#24615;&#33021;&#65292;&#21516;&#26102;&#22312;&#20854;&#20182;&#20219;&#21153;&#19978;&#20445;&#25345;&#20102;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#19981;&#26159;&#26368;&#24046;&#30340;&#65292;&#20294;BT-RvNN&#30340;&#20869;&#23384;&#20351;&#29992;&#20173;&#28982;&#38750;&#24120;&#26114;&#36149;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;BT-RvNN&#20869;&#23384;&#20351;&#29992;&#30340;&#20027;&#35201;&#29942;&#39048;&#26159;&#35780;&#20998;&#20989;&#25968;&#21644;&#36882;&#24402;&#21333;&#20803;&#20989;&#25968;&#30340;&#32416;&#32544;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31574;&#30053;&#26469;&#35299;&#20915;&#36825;&#20010;&#29942;&#39048;&#65292;&#24182;&#36827;&#19968;&#27493;&#31616;&#21270;&#20869;&#23384;&#20351;&#29992;&#12290;&#24635;&#20307;&#19978;&#65292;&#25105;&#20204;&#30340;&#31574;&#30053;&#19981;&#20165;&#23558;BT-RvNN&#30340;&#20869;&#23384;&#20351;&#29992;&#20943;&#23569;&#20102;10-16&#20493;&#65292;&#32780;&#19988;&#22312;ListOps&#20013;&#21019;&#36896;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#27700;&#24179;&#65292;&#21516;&#26102;&#22312;&#20854;&#20182;&#20219;&#21153;&#20013;&#20445;&#25345;&#20102;&#31867;&#20284;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;BT-RvNN&#20135;&#29983;&#30340;&#24341;&#23548;&#38544;&#23618;&#26641;&#33410;&#28857;&#34920;&#31034;&#30340;&#31574;&#30053;&#65292;&#23558;BT-RvNN&#20174;&#24418;&#24335;&#20026;f&#65306;R^n&#215;d -&gt;&#30340;&#21477;&#23376;&#32534;&#30721;&#22120;&#36716;&#25442;&#25104;&#12290;&#8221;
&lt;/p&gt;
&lt;p&gt;
Beam Tree Recursive Neural Network (BT-RvNN) was recently proposed as a simple extension of Gumbel Tree RvNN and it was shown to achieve state-of-the-art length generalization performance in ListOps while maintaining comparable performance on other tasks. However, although not the worst in its kind, BT-RvNN can be still exorbitantly expensive in memory usage. In this paper, we identify the main bottleneck in BT-RvNN's memory usage to be the entanglement of the scorer function and the recursive cell function. We propose strategies to remove this bottleneck and further simplify its memory usage. Overall, our strategies not only reduce the memory usage of BT-RvNN by $10$-$16$ times but also create a new state-of-the-art in ListOps while maintaining similar performance in other tasks. In addition, we also propose a strategy to utilize the induced latent-tree node representations produced by BT-RvNN to turn BT-RvNN from a sentence encoder of the form $f:\mathbb{R}^{n \times d} \rightarrow \
&lt;/p&gt;</description></item><item><title>AutoML&#20316;&#20026;&#19968;&#31181;&#33258;&#21160;&#21270;&#26500;&#24314;&#31471;&#21040;&#31471;AI/ML&#27969;&#27700;&#32447;&#30340;&#35299;&#20915;&#26041;&#26696;&#34987;&#24191;&#27867;&#20851;&#27880;&#65292;&#20294;&#30446;&#21069;&#23545;&#20854;&#22312;&#24320;&#21457;AI/ML&#31995;&#32479;&#30340;&#22242;&#38431;&#20013;&#30340;&#37319;&#29992;&#31243;&#24230;&#21644;&#24863;&#30693;&#31243;&#24230;&#32570;&#20047;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2307.10774</link><description>&lt;p&gt;
&#35780;&#20272;AutoML&#22312;&#25968;&#25454;&#39537;&#21160;&#36719;&#20214;&#24037;&#31243;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Assessing the Use of AutoML for Data-Driven Software Engineering. (arXiv:2307.10774v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10774
&lt;/p&gt;
&lt;p&gt;
AutoML&#20316;&#20026;&#19968;&#31181;&#33258;&#21160;&#21270;&#26500;&#24314;&#31471;&#21040;&#31471;AI/ML&#27969;&#27700;&#32447;&#30340;&#35299;&#20915;&#26041;&#26696;&#34987;&#24191;&#27867;&#20851;&#27880;&#65292;&#20294;&#30446;&#21069;&#23545;&#20854;&#22312;&#24320;&#21457;AI/ML&#31995;&#32479;&#30340;&#22242;&#38431;&#20013;&#30340;&#37319;&#29992;&#31243;&#24230;&#21644;&#24863;&#30693;&#31243;&#24230;&#32570;&#20047;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#30001;&#20110;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#22312;&#26500;&#24314;&#36719;&#20214;&#24212;&#29992;&#26041;&#38754;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#20844;&#21496;&#27491;&#21162;&#21147;&#25307;&#32856;&#20855;&#26377;&#28145;&#20837;&#20102;&#35299;&#36825;&#20123;&#25216;&#26415;&#30340;&#21592;&#24037;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;AutoML&#20316;&#20026;&#22635;&#34917;AI / ML&#25216;&#33021;&#32570;&#21475;&#30340;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#22823;&#21463;&#27426;&#36814;&#65292;&#22240;&#20026;&#23427;&#25215;&#35834;&#33258;&#21160;&#21270;&#26500;&#24314;&#31471;&#21040;&#31471;AI / ML&#27969;&#27700;&#32447;&#65292;&#36825;&#20123;&#27969;&#27700;&#32447;&#36890;&#24120;&#30001;&#19987;&#38376;&#30340;&#22242;&#38431;&#25104;&#21592;&#35774;&#35745;&#12290;&#30446;&#26631;&#65306;&#23613;&#31649;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#21644;&#39640;&#26399;&#26395;&#65292;&#20294;&#30446;&#21069;&#23545;&#20110;&#24320;&#21457;AI / ML&#31995;&#32479;&#30340;&#22242;&#38431;&#24403;&#21069;&#37319;&#29992;AutoML&#30340;&#31243;&#24230;&#20197;&#21450;&#20174;&#23454;&#36341;&#32773;&#21644;&#30740;&#31350;&#32773;&#30340;&#35270;&#35282;&#26469;&#30475;&#23427;&#30340;&#24863;&#30693;&#31243;&#24230;&#32570;&#20047;&#20449;&#24687;&#12290;&#26041;&#27861;&#65306;&#20026;&#20102;&#22635;&#34917;&#36825;&#20123;&#31354;&#30333;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#39033;&#28151;&#21512;&#26041;&#27861;&#30740;&#31350;&#65292;&#21253;&#25324;&#23545;&#20004;&#20010;&#36719;&#20214;&#24037;&#31243;&#25968;&#25454;&#38598;&#20013;12&#20010;&#31471;&#21040;&#31471;AutoML&#24037;&#20855;&#30340;&#22522;&#20934;&#27979;&#35797;&#20197;&#21450;&#29992;&#25143;&#35843;&#26597;&#21644;&#21518;&#32493;&#35775;&#35848;&#65292;&#20197;&#36827;&#19968;&#27493;&#20102;&#35299;AutoML&#30340;&#37319;&#29992;&#21644;&#24863;&#30693;&#12290;&#32467;&#26524;&#65306;&#25105;&#20204;&#21457;&#29616;AutoML&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;
&lt;/p&gt;
&lt;p&gt;
Background. Due to the widespread adoption of Artificial Intelligence (AI) and Machine Learning (ML) for building software applications, companies are struggling to recruit employees with a deep understanding of such technologies. In this scenario, AutoML is soaring as a promising solution to fill the AI/ML skills gap since it promises to automate the building of end-to-end AI/ML pipelines that would normally be engineered by specialized team members. Aims. Despite the growing interest and high expectations, there is a dearth of information about the extent to which AutoML is currently adopted by teams developing AI/ML-enabled systems and how it is perceived by practitioners and researchers. Method. To fill these gaps, in this paper, we present a mixed-method study comprising a benchmark of 12 end-to-end AutoML tools on two SE datasets and a user survey with follow-up interviews to further our understanding of AutoML adoption and perception. Results. We found that AutoML solutions can 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;ResNet&#21644;Bi-GRU&#32467;&#26500;&#65292;&#24182;&#21033;&#29992;&#35270;&#35273;&#39057;&#35889;&#22270;&#36827;&#34892;&#38899;&#20048;&#27969;&#27966;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#38899;&#20048;&#25512;&#33616;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.10773</link><description>&lt;p&gt;
&#20351;&#29992;ResNet&#21644;Bi-GRU&#21033;&#29992;&#35270;&#35273;&#39057;&#35889;&#22270;&#36827;&#34892;&#38899;&#20048;&#27969;&#27966;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Music Genre Classification with ResNet and Bi-GRU Using Visual Spectrograms. (arXiv:2307.10773v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10773
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;ResNet&#21644;Bi-GRU&#32467;&#26500;&#65292;&#24182;&#21033;&#29992;&#35270;&#35273;&#39057;&#35889;&#22270;&#36827;&#34892;&#38899;&#20048;&#27969;&#27966;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#38899;&#20048;&#25512;&#33616;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#20048;&#25512;&#33616;&#31995;&#32479;&#24050;&#25104;&#20026;&#22686;&#24378;&#38899;&#20048;&#27969;&#23186;&#20307;&#26381;&#21153;&#29992;&#25143;&#20307;&#39564;&#21644;&#28385;&#24847;&#24230;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#25913;&#36827;&#36825;&#20123;&#25512;&#33616;&#31995;&#32479;&#30340;&#20851;&#38190;&#25361;&#25112;&#22312;&#20110;&#29702;&#35299;&#38899;&#20048;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#65292;&#29305;&#21035;&#26159;&#38899;&#20048;&#27969;&#27966;&#20998;&#31867;&#12290;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#27969;&#27966;&#20998;&#31867;&#26041;&#38754;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#20294;&#36807;&#20110;&#20381;&#36182;&#25163;&#21160;&#35774;&#35745;&#30340;&#29305;&#24449;&#21644;&#29305;&#24449;&#36873;&#25321;&#65292;&#26080;&#27861;&#25429;&#25417;&#38899;&#20048;&#25968;&#25454;&#30340;&#23436;&#25972;&#22797;&#26434;&#24615;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#26550;&#26500;&#22914;&#20256;&#32479;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#22312;&#25429;&#25417;&#31354;&#38388;&#23618;&#27425;&#32467;&#26500;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#22312;&#25429;&#25417;&#38899;&#20048;&#25968;&#25454;&#20013;&#22266;&#26377;&#30340;&#26102;&#38388;&#21160;&#24577;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;
&lt;/p&gt;
&lt;p&gt;
Music recommendation systems have emerged as a vital component to enhance user experience and satisfaction for the music streaming services, which dominates music consumption. The key challenge in improving these recommender systems lies in comprehending the complexity of music data, specifically for the underpinning music genre classification. The limitations of manual genre classification have highlighted the need for a more advanced system, namely the Automatic Music Genre Classification (AMGC) system. While traditional machine learning techniques have shown potential in genre classification, they heavily rely on manually engineered features and feature selection, failing to capture the full complexity of music data. On the other hand, deep learning classification architectures like the traditional Convolutional Neural Networks (CNN) are effective in capturing the spatial hierarchies but struggle to capture the temporal dynamics inherent in music data. To address these challenges, t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#24037;&#20316;&#35760;&#24518;&#22522;&#20934;&#25968;&#25454;&#38598;&#65288;WorM&#65289;&#65292;&#36890;&#36807;&#35780;&#20272;4&#20010;&#21151;&#33021;&#12289;3&#20010;&#39046;&#22495;&#21644;11&#20010;&#34892;&#20026;&#21644;&#31070;&#32463;&#29305;&#24449;&#30340;WM&#20219;&#21153;&#26469;&#24320;&#21457;&#21644;&#35780;&#20272;AI WM&#27169;&#22411;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;AI&#27169;&#22411;&#33021;&#22815;&#27169;&#25311;&#20986;&#33041;&#20013;&#24037;&#20316;&#35760;&#24518;&#30340;&#19968;&#20123;&#29305;&#24449;&#65292;&#22914;&#20248;&#21183;&#25928;&#24212;&#21644;&#26368;&#26032;&#24615;&#25928;&#24212;&#65292;&#20197;&#21450;&#19987;&#38376;&#29992;&#20110;&#19981;&#21516;&#39046;&#22495;&#21644;&#21151;&#33021;&#30340;&#24037;&#20316;&#35760;&#24518;&#30340;&#31070;&#32463;&#32676;&#38598;&#21644;&#30456;&#20851;&#29289;&#12290;</title><link>http://arxiv.org/abs/2307.10768</link><description>&lt;p&gt;
&#35299;&#30721;&#35868;&#22242;&#65306;&#22312;&#24037;&#20316;&#35760;&#24518;&#30340;&#22810;&#20010;&#26041;&#38754;&#19978;&#23545;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Decoding the Enigma: Benchmarking Humans and AIs on the Many Facets of Working Memory. (arXiv:2307.10768v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10768
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#24037;&#20316;&#35760;&#24518;&#22522;&#20934;&#25968;&#25454;&#38598;&#65288;WorM&#65289;&#65292;&#36890;&#36807;&#35780;&#20272;4&#20010;&#21151;&#33021;&#12289;3&#20010;&#39046;&#22495;&#21644;11&#20010;&#34892;&#20026;&#21644;&#31070;&#32463;&#29305;&#24449;&#30340;WM&#20219;&#21153;&#26469;&#24320;&#21457;&#21644;&#35780;&#20272;AI WM&#27169;&#22411;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;AI&#27169;&#22411;&#33021;&#22815;&#27169;&#25311;&#20986;&#33041;&#20013;&#24037;&#20316;&#35760;&#24518;&#30340;&#19968;&#20123;&#29305;&#24449;&#65292;&#22914;&#20248;&#21183;&#25928;&#24212;&#21644;&#26368;&#26032;&#24615;&#25928;&#24212;&#65292;&#20197;&#21450;&#19987;&#38376;&#29992;&#20110;&#19981;&#21516;&#39046;&#22495;&#21644;&#21151;&#33021;&#30340;&#24037;&#20316;&#35760;&#24518;&#30340;&#31070;&#32463;&#32676;&#38598;&#21644;&#30456;&#20851;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#20316;&#35760;&#24518;&#65288;WM&#65289;&#26159;&#19968;&#31181;&#22522;&#26412;&#30340;&#35748;&#30693;&#36807;&#31243;&#65292;&#23427;&#20419;&#36827;&#20102;&#20449;&#24687;&#30340;&#20020;&#26102;&#23384;&#20648;&#12289;&#25972;&#21512;&#12289;&#25805;&#20316;&#21644;&#26816;&#32034;&#65292;&#22312;&#25512;&#29702;&#21644;&#20915;&#31574;&#20219;&#21153;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#25429;&#25417;&#24037;&#20316;&#35760;&#24518;&#22810;&#26041;&#38754;&#29305;&#24449;&#30340;&#21487;&#38752;&#22522;&#20934;&#25968;&#25454;&#38598;&#23545;&#20110;&#26377;&#25928;&#22320;&#24320;&#21457;&#21644;&#35780;&#20272;AI&#24037;&#20316;&#35760;&#24518;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#24037;&#20316;&#35760;&#24518;&#65288;WorM&#65289;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20197;&#23454;&#29616;&#36825;&#20010;&#30446;&#30340;&#12290;WorM&#21253;&#25324;10&#20010;&#20219;&#21153;&#21644;&#24635;&#20849;100&#19975;&#27425;&#35797;&#39564;&#65292;&#35780;&#20272;&#20102;WM&#30340;4&#20010;&#21151;&#33021;&#12289;3&#20010;&#39046;&#22495;&#21644;11&#20010;&#34892;&#20026;&#21644;&#31070;&#32463;&#29305;&#24449;&#12290;&#25105;&#20204;&#22312;&#25152;&#26377;&#36825;&#20123;&#20219;&#21153;&#19978;&#20849;&#21516;&#35757;&#32451;&#21644;&#27979;&#35797;&#20102;&#26368;&#20808;&#36827;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer&#12290;&#25105;&#20204;&#36824;&#21253;&#25324;&#20154;&#31867;&#34892;&#20026;&#22522;&#20934;&#20316;&#20026;&#23545;&#27604;&#30340;&#19978;&#38480;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;AI&#27169;&#22411;&#27169;&#25311;&#20102;&#33041;&#20013;&#24037;&#20316;&#35760;&#24518;&#30340;&#19968;&#20123;&#29305;&#24449;&#65292;&#29305;&#21035;&#26159;&#20248;&#21183;&#25928;&#24212;&#21644;&#26368;&#26032;&#24615;&#25928;&#24212;&#65292;&#20197;&#21450;&#19987;&#38376;&#29992;&#20110;&#19981;&#21516;&#39046;&#22495;&#21644;&#21151;&#33021;&#24615;&#30340;&#24037;&#20316;&#35760;&#24518;&#30340;&#31070;&#32463;&#32676;&#38598;&#21644;&#30456;&#20851;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Working memory (WM), a fundamental cognitive process facilitating the temporary storage, integration, manipulation, and retrieval of information, plays a vital role in reasoning and decision-making tasks. Robust benchmark datasets that capture the multifaceted nature of WM are crucial for the effective development and evaluation of AI WM models. Here, we introduce a comprehensive Working Memory (WorM) benchmark dataset for this purpose. WorM comprises 10 tasks and a total of 1 million trials, assessing 4 functionalities, 3 domains, and 11 behavioral and neural characteristics of WM. We jointly trained and tested state-of-the-art recurrent neural networks and transformers on all these tasks. We also include human behavioral benchmarks as an upper bound for comparison. Our results suggest that AI models replicate some characteristics of WM in the brain, most notably primacy and recency effects, and neural clusters and correlates specialized for different domains and functionalities of WM
&lt;/p&gt;</description></item><item><title>MSQNet&#26159;&#19968;&#31181;&#26080;&#20851;&#28436;&#21592;&#30340;&#22810;&#27169;&#24577;&#22810;&#26631;&#31614;&#21160;&#20316;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#35270;&#35273;&#21644;&#25991;&#26412;&#27169;&#24577;&#26469;&#26356;&#22909;&#22320;&#34920;&#31034;&#21160;&#20316;&#31867;&#21035;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#38024;&#23545;&#29305;&#23450;&#28436;&#21592;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2307.10763</link><description>&lt;p&gt;
MSQNet: &#26080;&#20851;&#28436;&#21592;&#30340;&#22810;&#27169;&#24577;&#21160;&#20316;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
MSQNet: Actor-agnostic Action Recognition with Multi-modal Query. (arXiv:2307.10763v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10763
&lt;/p&gt;
&lt;p&gt;
MSQNet&#26159;&#19968;&#31181;&#26080;&#20851;&#28436;&#21592;&#30340;&#22810;&#27169;&#24577;&#22810;&#26631;&#31614;&#21160;&#20316;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#35270;&#35273;&#21644;&#25991;&#26412;&#27169;&#24577;&#26469;&#26356;&#22909;&#22320;&#34920;&#31034;&#21160;&#20316;&#31867;&#21035;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#38024;&#23545;&#29305;&#23450;&#28436;&#21592;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#21160;&#20316;&#35782;&#21035;&#26041;&#27861;&#36890;&#24120;&#26159;&#38024;&#23545;&#29305;&#23450;&#28436;&#21592;&#30340;&#65292;&#22240;&#20026;&#28436;&#21592;&#20043;&#38388;&#20855;&#26377;&#22266;&#26377;&#30340;&#25299;&#25169;&#21644;&#26174;&#30528;&#24046;&#24322;&#12290;&#36825;&#23601;&#38656;&#35201;&#29305;&#23450;&#28436;&#21592;&#30340;&#23039;&#24577;&#20272;&#35745;&#65288;&#20363;&#22914;&#20154;&#31867;&#19982;&#21160;&#29289;&#65289;&#65292;&#23548;&#33268;&#27169;&#22411;&#35774;&#35745;&#22797;&#26434;&#24615;&#21644;&#39640;&#32500;&#25252;&#25104;&#26412;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#36890;&#24120;&#21482;&#20851;&#27880;&#23398;&#20064;&#35270;&#35273;&#27169;&#24577;&#21644;&#21333;&#26631;&#31614;&#20998;&#31867;&#65292;&#24573;&#35270;&#20102;&#20854;&#20182;&#21487;&#29992;&#20449;&#24687;&#28304;&#65288;&#20363;&#22914;&#31867;&#21517;&#25991;&#26412;&#65289;&#21644;&#22810;&#20010;&#21160;&#20316;&#30340;&#21516;&#26102;&#21457;&#29983;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#8220;&#26080;&#20851;&#28436;&#21592;&#30340;&#22810;&#27169;&#24577;&#22810;&#26631;&#31614;&#21160;&#20316;&#35782;&#21035;&#8221;&#65292;&#20026;&#21253;&#25324;&#20154;&#31867;&#21644;&#21160;&#29289;&#22312;&#20869;&#30340;&#21508;&#31181;&#31867;&#22411;&#30340;&#28436;&#21592;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#22312;&#22522;&#20110;Transformer&#30340;&#30446;&#26631;&#26816;&#27979;&#26694;&#26550;&#65288;&#20363;&#22914;DETR&#65289;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#35821;&#20041;&#26597;&#35810;&#32593;&#32476;&#65288;MSQNet&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#35270;&#35273;&#21644;&#25991;&#26412;&#27169;&#24577;&#26356;&#22909;&#22320;&#34920;&#31034;&#21160;&#20316;&#31867;&#21035;&#12290;&#28040;&#38500;&#20102;&#28436;&#21592;&#29305;&#23450;&#24615;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing action recognition methods are typically actor-specific due to the intrinsic topological and apparent differences among the actors. This requires actor-specific pose estimation (e.g., humans vs. animals), leading to cumbersome model design complexity and high maintenance costs. Moreover, they often focus on learning the visual modality alone and single-label classification whilst neglecting other available information sources (e.g., class name text) and the concurrent occurrence of multiple actions. To overcome these limitations, we propose a new approach called 'actor-agnostic multi-modal multi-label action recognition,' which offers a unified solution for various types of actors, including humans and animals. We further formulate a novel Multi-modal Semantic Query Network (MSQNet) model in a transformer-based object detection framework (e.g., DETR), characterized by leveraging visual and textual modalities to represent the action classes better. The elimination of actor-spec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#32771;&#34385;&#36873;&#27665;&#23646;&#24615;&#26469;&#23454;&#29616;&#20844;&#27491;&#30340;&#24847;&#35265;&#27719;&#24635;&#30340;&#26041;&#27861;&#65292;&#24182;&#35780;&#20272;&#20102;&#27719;&#24635;&#32467;&#26524;&#30340;&#20844;&#27491;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.10749</link><description>&lt;p&gt;
&#20943;&#36731;&#36873;&#27665;&#23646;&#24615;&#20559;&#35265;&#20197;&#23454;&#29616;&#20844;&#27491;&#30340;&#24847;&#35265;&#27719;&#24635;
&lt;/p&gt;
&lt;p&gt;
Mitigating Voter Attribute Bias for Fair Opinion Aggregation. (arXiv:2307.10749v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10749
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#32771;&#34385;&#36873;&#27665;&#23646;&#24615;&#26469;&#23454;&#29616;&#20844;&#27491;&#30340;&#24847;&#35265;&#27719;&#24635;&#30340;&#26041;&#27861;&#65292;&#24182;&#35780;&#20272;&#20102;&#27719;&#24635;&#32467;&#26524;&#30340;&#20844;&#27491;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20010;&#24847;&#35265;&#30340;&#27719;&#24635;&#22312;&#20915;&#31574;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#20363;&#22914;&#22312;&#25307;&#32856;&#21644;&#36151;&#27454;&#23457;&#26680;&#20013;&#65292;&#20197;&#21450;&#22312;&#20026;&#30417;&#30563;&#23398;&#20064;&#26631;&#35760;&#25968;&#25454;&#26102;&#12290;&#34429;&#28982;&#22810;&#25968;&#25237;&#31080;&#21644;&#29616;&#26377;&#30340;&#24847;&#35265;&#27719;&#24635;&#27169;&#22411;&#23545;&#20110;&#31616;&#21333;&#20219;&#21153;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#22312;&#27809;&#26377;&#23458;&#35266;&#30495;&#23454;&#26631;&#31614;&#30340;&#20219;&#21153;&#20013;&#65292;&#23427;&#20204;&#24182;&#19981;&#36866;&#29992;&#65292;&#22240;&#20026;&#21487;&#33021;&#20250;&#20986;&#29616;&#20998;&#27495;&#12290;&#29305;&#21035;&#26159;&#65292;&#24403;&#36873;&#27665;&#23646;&#24615;&#65288;&#22914;&#24615;&#21035;&#25110;&#31181;&#26063;&#65289;&#24341;&#20837;&#20559;&#35265;&#26102;&#65292;&#27719;&#24635;&#32467;&#26524;&#21487;&#33021;&#20250;&#22240;&#36873;&#27665;&#23646;&#24615;&#30340;&#32452;&#25104;&#32780;&#24322;&#12290;&#19968;&#20010;&#24179;&#34913;&#30340;&#36873;&#27665;&#32676;&#20307;&#23545;&#20110;&#20844;&#24179;&#30340;&#27719;&#24635;&#32467;&#26524;&#26159;&#29702;&#24819;&#30340;&#65292;&#20294;&#21487;&#33021;&#38590;&#20197;&#20934;&#22791;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#22522;&#20110;&#36873;&#27665;&#23646;&#24615;&#23454;&#29616;&#20844;&#27491;&#24847;&#35265;&#27719;&#24635;&#30340;&#26041;&#27861;&#65292;&#24182;&#35780;&#20272;&#20102;&#27719;&#24635;&#32467;&#26524;&#30340;&#20844;&#27491;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#23558;&#22810;&#25968;&#25237;&#31080;&#21644;Dawid&#21644;Skene&#27169;&#22411;&#65288;D&amp;S&#27169;&#22411;&#65289;&#31561;&#24847;&#35265;&#27719;&#24635;&#27169;&#22411;&#19982;&#37319;&#26679;&#21152;&#26435;&#31561;&#20844;&#24179;&#36873;&#39033;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#35780;&#20272;&#27719;&#24635;&#32467;&#26524;&#30340;&#20844;&#27491;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The aggregation of multiple opinions plays a crucial role in decision-making, such as in hiring and loan review, and in labeling data for supervised learning. Although majority voting and existing opinion aggregation models are effective for simple tasks, they are inappropriate for tasks without objectively true labels in which disagreements may occur. In particular, when voter attributes such as gender or race introduce bias into opinions, the aggregation results may vary depending on the composition of voter attributes. A balanced group of voters is desirable for fair aggregation results but may be difficult to prepare. In this study, we consider methods to achieve fair opinion aggregation based on voter attributes and evaluate the fairness of the aggregated results. To this end, we consider an approach that combines opinion aggregation models such as majority voting and the Dawid and Skene model (D&amp;S model) with fairness options such as sample weighting. To evaluate the fairness of 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20844;&#24179;&#24863;&#30693;&#30340;&#32852;&#37030;&#23458;&#25143;&#31471;&#36873;&#25321;&#65288;FairFedCS&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#32852;&#37030;&#23398;&#20064;&#23458;&#25143;&#31471;&#30340;&#36873;&#25321;&#27010;&#29575;&#65292;&#21516;&#26102;&#32771;&#34385;&#23458;&#25143;&#31471;&#30340;&#22768;&#35465;&#12289;&#21442;&#19982;&#27425;&#25968;&#21644;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#36129;&#29486;&#65292;&#35299;&#20915;&#20102;&#24179;&#34913;&#24615;&#33021;&#21644;&#20844;&#24179;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.10738</link><description>&lt;p&gt;
&#20844;&#24179;&#24863;&#30693;&#30340;&#32852;&#37030;&#23398;&#20064;&#23458;&#25143;&#31471;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Fairness-Aware Client Selection for Federated Learning. (arXiv:2307.10738v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10738
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20844;&#24179;&#24863;&#30693;&#30340;&#32852;&#37030;&#23458;&#25143;&#31471;&#36873;&#25321;&#65288;FairFedCS&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#32852;&#37030;&#23398;&#20064;&#23458;&#25143;&#31471;&#30340;&#36873;&#25321;&#27010;&#29575;&#65292;&#21516;&#26102;&#32771;&#34385;&#23458;&#25143;&#31471;&#30340;&#22768;&#35465;&#12289;&#21442;&#19982;&#27425;&#25968;&#21644;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#36129;&#29486;&#65292;&#35299;&#20915;&#20102;&#24179;&#34913;&#24615;&#33021;&#21644;&#20844;&#24179;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20351;&#24471;&#22810;&#20010;&#25968;&#25454;&#25152;&#26377;&#32773;&#65288;&#21363;&#32852;&#37030;&#23398;&#20064;&#23458;&#25143;&#31471;&#65289;&#33021;&#22815;&#22312;&#19981;&#27844;&#38706;&#31169;&#20154;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21327;&#20316;&#35757;&#32451;&#12290;&#30001;&#20110;&#32852;&#37030;&#23398;&#20064;&#26381;&#21153;&#22120;&#27599;&#36718;&#35757;&#32451;&#21482;&#33021;&#36873;&#25321;&#26377;&#38480;&#25968;&#37327;&#30340;&#23458;&#25143;&#31471;&#65292;&#23458;&#25143;&#31471;&#36873;&#25321;&#25104;&#20026;&#37325;&#35201;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#20851;&#27880;&#20110;&#25552;&#39640;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#25110;&#25552;&#39640;&#23458;&#25143;&#31471;&#30340;&#20844;&#24179;&#24453;&#36935;&#12290;&#22312;&#36873;&#25321;&#32852;&#37030;&#23398;&#20064;&#23458;&#25143;&#31471;&#26102;&#24179;&#34913;&#24615;&#33021;&#21644;&#20844;&#24179;&#24615;&#32771;&#34385;&#30340;&#38382;&#39064;&#20173;&#28982;&#23384;&#22312;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20844;&#24179;&#24863;&#30693;&#30340;&#32852;&#37030;&#23458;&#25143;&#31471;&#36873;&#25321;&#65288;FairFedCS&#65289;&#26041;&#27861;&#12290;&#22522;&#20110;&#26446;&#38597;&#26222;&#35834;&#22827;&#20248;&#21270;&#65292;&#23427;&#36890;&#36807;&#21516;&#26102;&#32771;&#34385;&#23458;&#25143;&#31471;&#30340;&#22768;&#35465;&#12289;&#21442;&#19982;&#32852;&#37030;&#23398;&#20064;&#20219;&#21153;&#30340;&#27425;&#25968;&#21644;&#23545;&#26368;&#32456;&#27169;&#22411;&#24615;&#33021;&#30340;&#36129;&#29486;&#65292;&#21160;&#24577;&#35843;&#25972;&#32852;&#37030;&#23398;&#20064;&#23458;&#25143;&#31471;&#30340;&#36873;&#25321;&#27010;&#29575;&#12290;&#36890;&#36807;&#19981;&#20351;&#29992;&#22522;&#20110;&#38408;&#20540;&#30340;&#22768;&#35465;&#36807;&#28388;&#65292;&#23427;&#20026;&#32852;&#37030;&#23398;&#20064;&#23458;&#25143;&#31471;&#25552;&#20379;&#20102;&#36174;&#22238;&#22768;&#35465;&#30340;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) has enabled multiple data owners (a.k.a. FL clients) to train machine learning models collaboratively without revealing private data. Since the FL server can only engage a limited number of clients in each training round, FL client selection has become an important research problem. Existing approaches generally focus on either enhancing FL model performance or enhancing the fair treatment of FL clients. The problem of balancing performance and fairness considerations when selecting FL clients remains open. To address this problem, we propose the Fairness-aware Federated Client Selection (FairFedCS) approach. Based on Lyapunov optimization, it dynamically adjusts FL clients' selection probabilities by jointly considering their reputations, times of participation in FL tasks and contributions to the resulting model performance. By not using threshold-based reputation filtering, it provides FL clients with opportunities to redeem their reputations after a perceive
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65292;&#31526;&#21512;Feldman&#30340;&#38271;&#23614;&#29702;&#35770;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#38271;&#23614;&#20998;&#24067;&#24773;&#20917;&#19979;&#65292;&#38750;&#32447;&#24615;&#20998;&#31867;&#22120;&#21487;&#20197;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#65292;&#32780;&#32447;&#24615;&#20998;&#31867;&#22120;&#19981;&#33021;&#12290;&#35813;&#32467;&#26524;&#24378;&#35843;&#20102;&#23545;&#20110;&#38271;&#23614;&#20998;&#24067;&#65292;&#38656;&#35201;&#32771;&#34385;&#32597;&#35265;&#30340;&#35757;&#32451;&#26679;&#26412;&#20197;&#23454;&#29616;&#26368;&#20339;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.10736</link><description>&lt;p&gt;
&#39640;&#26031;&#28151;&#21512;&#19979;&#30340;&#38271;&#23614;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Long-Tail Theory under Gaussian Mixtures. (arXiv:2307.10736v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10736
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65292;&#31526;&#21512;Feldman&#30340;&#38271;&#23614;&#29702;&#35770;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#38271;&#23614;&#20998;&#24067;&#24773;&#20917;&#19979;&#65292;&#38750;&#32447;&#24615;&#20998;&#31867;&#22120;&#21487;&#20197;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#65292;&#32780;&#32447;&#24615;&#20998;&#31867;&#22120;&#19981;&#33021;&#12290;&#35813;&#32467;&#26524;&#24378;&#35843;&#20102;&#23545;&#20110;&#38271;&#23614;&#20998;&#24067;&#65292;&#38656;&#35201;&#32771;&#34385;&#32597;&#35265;&#30340;&#35757;&#32451;&#26679;&#26412;&#20197;&#23454;&#29616;&#26368;&#20339;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#26469;&#29983;&#25104;&#36981;&#24490;Feldman&#30340;&#38271;&#23614;&#29702;&#35770;&#65288;2020&#65289;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#25552;&#20986;&#30340;&#27169;&#22411;&#20013;&#65292;&#32447;&#24615;&#20998;&#31867;&#22120;&#26080;&#27861;&#23558;&#27867;&#21270;&#35823;&#24046;&#38477;&#20302;&#21040;&#19968;&#23450;&#27700;&#24179;&#20197;&#19979;&#65292;&#32780;&#20855;&#26377;&#35760;&#24518;&#33021;&#21147;&#30340;&#38750;&#32447;&#24615;&#20998;&#31867;&#22120;&#21487;&#20197;&#12290;&#36825;&#35777;&#23454;&#20102;&#23545;&#20110;&#38271;&#23614;&#20998;&#24067;&#65292;&#24517;&#39035;&#32771;&#34385;&#32597;&#35265;&#30340;&#35757;&#32451;&#26679;&#26412;&#20197;&#23454;&#29616;&#23545;&#26032;&#25968;&#25454;&#30340;&#26368;&#20339;&#27867;&#21270;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#24403;&#23376;&#32676;&#20307;&#39057;&#29575;&#20998;&#24067;&#30340;&#23614;&#37096;&#21464;&#30701;&#26102;&#65292;&#32447;&#24615;&#27169;&#22411;&#21644;&#38750;&#32447;&#24615;&#27169;&#22411;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#21487;&#20197;&#20943;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
We suggest a simple Gaussian mixture model for data generation that complies with Feldman's long tail theory (2020). We demonstrate that a linear classifier cannot decrease the generalization error below a certain level in the proposed model, whereas a nonlinear classifier with a memorization capacity can. This confirms that for long-tailed distributions, rare training examples must be considered for optimal generalization to new data. Finally, we show that the performance gap between linear and nonlinear models can be lessened as the tail becomes shorter in the subpopulation frequency distribution, as confirmed by experiments on synthetic and real data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#25506;&#35752;&#20102;&#22256;&#38590;&#26679;&#26412;&#21644;&#22122;&#22768;&#26679;&#26412;&#20043;&#38388;&#30340;&#21306;&#21035;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#21487;&#20197;&#21306;&#20998;&#24182;&#36807;&#28388;&#25481;&#22122;&#22768;&#26631;&#35760;&#30340;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2307.10718</link><description>&lt;p&gt;
&#22256;&#38590;&#26679;&#26412;&#21644;&#22122;&#22768;&#26679;&#26412;&#20043;&#38388;&#30340;&#21306;&#21035;&#65306;&#19968;&#39033;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Differences Between Hard and Noisy-labeled Samples: An Empirical Study. (arXiv:2307.10718v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10718
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#25506;&#35752;&#20102;&#22256;&#38590;&#26679;&#26412;&#21644;&#22122;&#22768;&#26679;&#26412;&#20043;&#38388;&#30340;&#21306;&#21035;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#21487;&#20197;&#21306;&#20998;&#24182;&#36807;&#28388;&#25481;&#22122;&#22768;&#26631;&#35760;&#30340;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20855;&#26377;&#22256;&#38590;/&#38590;&#20197;&#22788;&#29702;&#30340;&#26679;&#26412;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#22122;&#22768;&#25110;&#26631;&#31614;&#38169;&#35823;&#30340;&#26679;&#26412;&#26159;&#19968;&#20010;&#37325;&#35201;&#20294;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#30340;&#20027;&#39064;&#12290;&#29616;&#26377;&#26041;&#27861;&#22823;&#22810;&#23558;&#36825;&#20004;&#31181;&#31867;&#22411;&#30340;&#25968;&#25454;&#21516;&#31561;&#23545;&#24453;&#65292;&#23548;&#33268;&#27169;&#22411;&#25972;&#20307;&#24615;&#33021;&#19979;&#38477;&#12290;&#26412;&#25991;&#39318;&#20808;&#35774;&#35745;&#20102;&#19981;&#21516;&#26679;&#26412;&#30340;&#33258;&#23450;&#20041;&#30828;&#24230;&#21644;&#22122;&#22768;&#27700;&#24179;&#30340;&#21508;&#31181;&#21512;&#25104;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#20351;&#25105;&#20204;&#33021;&#22815;&#26356;&#22909;&#22320;&#29702;&#35299;&#38590;&#20197;&#23398;&#20064;&#30340;&#26679;&#26412;&#21644;&#38169;&#35823;&#26631;&#35760;&#30340;&#26679;&#26412;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#21644;&#21306;&#21035;&#12290;&#36825;&#20123;&#21487;&#25511;&#23454;&#39564;&#20026;&#21306;&#20998;&#22256;&#38590;&#26679;&#26412;&#21644;&#22122;&#22768;&#26679;&#26412;&#30340;&#26041;&#27861;&#30340;&#21457;&#23637;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#21487;&#20197;&#36807;&#28388;&#25481;&#22122;&#22768;&#26631;&#35760;&#30340;&#26679;&#26412;&#65292;&#21516;&#26102;&#20445;&#30041;&#22256;&#38590;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Extracting noisy or incorrectly labeled samples from a labeled dataset with hard/difficult samples is an important yet under-explored topic. Two general and often independent lines of work exist, one focuses on addressing noisy labels, and another deals with hard samples. However, when both types of data are present, most existing methods treat them equally, which results in a decline in the overall performance of the model. In this paper, we first design various synthetic datasets with custom hardness and noisiness levels for different samples. Our proposed systematic empirical study enables us to better understand the similarities and more importantly the differences between hard-to-learn samples and incorrectly-labeled samples. These controlled experiments pave the way for the development of methods that distinguish between hard and noisy samples. Through our study, we introduce a simple yet effective metric that filters out noisy-labeled samples while keeping the hard samples. We s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#39640;&#32500;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#20013;&#30340;&#22686;&#24378;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#31574;&#30053;&#21442;&#25968;&#21270;&#30340;&#37325;&#21442;&#25968;&#21270;&#26694;&#26550;&#65292;&#24182;&#22522;&#20110;&#27492;&#26694;&#26550;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#21442;&#25968;&#21270;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#20219;&#21153;&#20013;&#36991;&#20813;&#23616;&#37096;&#26368;&#20248;&#65292;&#35299;&#20915;&#31232;&#30095;&#22870;&#21169;&#29615;&#22659;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.10710</link><description>&lt;p&gt;
&#37325;&#21442;&#25968;&#21270;&#31574;&#30053;&#23398;&#20064;&#29992;&#20110;&#22810;&#27169;&#24577;&#36712;&#36857;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Reparameterized Policy Learning for Multimodal Trajectory Optimization. (arXiv:2307.10710v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10710
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#39640;&#32500;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#20013;&#30340;&#22686;&#24378;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#31574;&#30053;&#21442;&#25968;&#21270;&#30340;&#37325;&#21442;&#25968;&#21270;&#26694;&#26550;&#65292;&#24182;&#22522;&#20110;&#27492;&#26694;&#26550;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#21442;&#25968;&#21270;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#20219;&#21153;&#20013;&#36991;&#20813;&#23616;&#37096;&#26368;&#20248;&#65292;&#35299;&#20915;&#31232;&#30095;&#22870;&#21169;&#29615;&#22659;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#39640;&#32500;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#20013;&#20026;&#22686;&#24378;&#23398;&#20064; (RL) &#21442;&#25968;&#21270;&#31574;&#30053;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#24320;&#21457;&#19968;&#31181;&#22810;&#27169;&#24577;&#31574;&#30053;&#65292;&#20811;&#26381;&#20102;&#24120;&#29992;&#30340;&#39640;&#26031;&#21442;&#25968;&#21270;&#30340;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21407;&#21017;&#24615;&#30340;&#26694;&#26550;&#65292;&#23558;&#36830;&#32493;RL&#31574;&#30053;&#24314;&#27169;&#20026;&#26368;&#20248;&#36712;&#36857;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#36890;&#36807;&#23558;&#31574;&#30053;&#26465;&#20214;&#20110;&#28508;&#21464;&#37327;&#65292;&#25105;&#20204;&#23548;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#21464;&#20998;&#19978;&#30028;&#20316;&#20026;&#20248;&#21270;&#30446;&#26631;&#65292;&#20174;&#32780;&#20419;&#36827;&#20102;&#23545;&#29615;&#22659;&#30340;&#25506;&#32034;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;RL&#26041;&#27861;&#65292;&#31216;&#20026;&#37325;&#21442;&#25968;&#21270;&#31574;&#30053;&#26799;&#24230;(RPG)&#65292;&#23427;&#21033;&#29992;&#20102;&#22810;&#27169;&#24577;&#31574;&#30053;&#21442;&#25968;&#21270;&#21644;&#23398;&#24471;&#30340;&#19990;&#30028;&#27169;&#22411;&#20197;&#23454;&#29616;&#24378;&#22823;&#30340;&#25506;&#32034;&#33021;&#21147;&#21644;&#39640;&#25928;&#30340;&#25968;&#25454;&#21033;&#29992;&#29575;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#24110;&#21161;&#20195;&#29702;&#22312;&#20855;&#26377;&#23494;&#38598;&#22870;&#21169;&#30340;&#20219;&#21153;&#20013;&#36991;&#20813;&#23616;&#37096;&#26368;&#20248;&#65292;&#24182;&#36890;&#36807;&#25972;&#21512;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#32534;&#30721;&#65292;&#35299;&#20915;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#31232;&#30095;&#22870;&#21169;&#29615;&#22659;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the challenge of parametrizing policies for reinforcement learning (RL) in high-dimensional continuous action spaces. Our objective is to develop a multimodal policy that overcomes limitations inherent in the commonly-used Gaussian parameterization. To achieve this, we propose a principled framework that models the continuous RL policy as a generative model of optimal trajectories. By conditioning the policy on a latent variable, we derive a novel variational bound as the optimization objective, which promotes exploration of the environment. We then present a practical model-based RL method, called Reparameterized Policy Gradient (RPG), which leverages the multimodal policy parameterization and learned world model to achieve strong exploration capabilities and high data efficiency. Empirical results demonstrate that our method can help agents evade local optima in tasks with dense rewards and solve challenging sparse-reward environments by incorporating an object-centric
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#27169;&#22411;TwinLiteNet&#65292;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;&#30340;&#21487;&#39537;&#21160;&#21306;&#22495;&#21644;&#36710;&#36947;&#20998;&#21106;&#12290;&#35813;&#27169;&#22411;&#25104;&#26412;&#20302;&#24265;&#19988;&#39640;&#25928;&#20934;&#30830;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#26126;&#26174;&#30340;&#35745;&#31639;&#36164;&#28304;&#33410;&#32422;&#12290;</title><link>http://arxiv.org/abs/2307.10705</link><description>&lt;p&gt;
TwinLiteNet&#65306;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#20013;&#21487;&#39537;&#21160;&#21306;&#22495;&#21644;&#36710;&#36947;&#20998;&#21106;&#30340;&#39640;&#25928;&#36731;&#37327;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
TwinLiteNet: An Efficient and Lightweight Model for Driveable Area and Lane Segmentation in Self-Driving Cars. (arXiv:2307.10705v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10705
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#27169;&#22411;TwinLiteNet&#65292;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;&#30340;&#21487;&#39537;&#21160;&#21306;&#22495;&#21644;&#36710;&#36947;&#20998;&#21106;&#12290;&#35813;&#27169;&#22411;&#25104;&#26412;&#20302;&#24265;&#19988;&#39640;&#25928;&#20934;&#30830;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#26126;&#26174;&#30340;&#35745;&#31639;&#36164;&#28304;&#33410;&#32422;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#20998;&#21106;&#26159;&#33258;&#21160;&#39550;&#39542;&#20013;&#19968;&#20010;&#24120;&#35265;&#30340;&#20219;&#21153;&#65292;&#29992;&#20110;&#29702;&#35299;&#21608;&#22260;&#29615;&#22659;&#12290;&#23545;&#20110;&#36947;&#36335;&#19978;&#30340;&#23433;&#20840;&#21644;&#39640;&#25928;&#23548;&#33322;&#26469;&#35828;&#65292;&#21487;&#39537;&#21160;&#21306;&#22495;&#20998;&#21106;&#21644;&#36710;&#36947;&#26816;&#27979;&#23588;&#20026;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#21407;&#22987;&#30340;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#35745;&#31639;&#24320;&#38144;&#22823;&#65292;&#38656;&#35201;&#39640;&#31471;&#30828;&#20214;&#65292;&#36825;&#23545;&#20110;&#23884;&#20837;&#24335;&#31995;&#32479;&#30340;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#26469;&#35828;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#21487;&#39537;&#21160;&#21306;&#22495;&#21644;&#36710;&#36947;&#32447;&#20998;&#21106;&#27169;&#22411;&#12290;TwinLiteNet&#35774;&#35745;&#25104;&#25104;&#26412;&#20302;&#24265;&#65292;&#20294;&#33021;&#22815;&#23454;&#29616;&#20934;&#30830;&#21644;&#39640;&#25928;&#30340;&#20998;&#21106;&#32467;&#26524;&#12290;&#25105;&#20204;&#22312;BDD100K&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;TwinLiteNet&#65292;&#24182;&#19982;&#29616;&#20195;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;TwinLiteNet&#19982;&#29616;&#26377;&#26041;&#27861;&#34920;&#29616;&#30456;&#20284;&#65292;&#20294;&#25152;&#38656;&#30340;&#35745;&#31639;&#36164;&#28304;&#26174;&#33879;&#20943;&#23569;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;TwinLiteNet&#22312;&#21487;&#39537;&#21160;&#21306;&#22495;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;91.3%&#30340;mIoU&#35780;&#20998;&#65292;&#22312;&#36710;&#36947;&#26816;&#27979;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;31.08%&#30340;IoU&#35780;&#20998;&#65292;&#20165;&#20351;&#29992;&#20102;40&#19975;&#20010;&#21442;&#25968;&#65292;&#22312;GPU RTX&#19978;&#23454;&#29616;&#20102;415 FPS&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic segmentation is a common task in autonomous driving to understand the surrounding environment. Driveable Area Segmentation and Lane Detection are particularly important for safe and efficient navigation on the road. However, original semantic segmentation models are computationally expensive and require high-end hardware, which is not feasible for embedded systems in autonomous vehicles. This paper proposes a lightweight model for the driveable area and lane line segmentation. TwinLiteNet is designed cheaply but achieves accurate and efficient segmentation results. We evaluate TwinLiteNet on the BDD100K dataset and compare it with modern models. Experimental results show that our TwinLiteNet performs similarly to existing approaches, requiring significantly fewer computational resources. Specifically, TwinLiteNet achieves a mIoU score of 91.3% for the Drivable Area task and 31.08% IoU for the Lane Detection task with only 0.4 million parameters and achieves 415 FPS on GPU RTX 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#25955;&#24335;&#26234;&#33021;&#20805;&#30005;&#31995;&#32479;&#65292;&#21033;&#29992;&#33258;&#36866;&#24212;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#26041;&#27861;&#36827;&#34892;&#25511;&#21046;&#65292;&#24182;&#37319;&#29992;&#22810;&#33218;&#36172;&#21338;&#23398;&#20064;&#22788;&#29702;&#19981;&#30830;&#23450;&#24615;&#12290;&#35813;&#31995;&#32479;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#12289;&#23454;&#26102;&#24615;&#12289;&#26080;&#27169;&#22411;&#24615;&#65292;&#24182;&#19988;&#32771;&#34385;&#20102;&#19981;&#21516;&#21442;&#19982;&#32773;&#20043;&#38388;&#30340;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.10704</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#30005;&#21160;&#27773;&#36710;&#30340;&#33258;&#36866;&#24212;&#22810;&#26234;&#33021;&#20307;&#22810;&#33218;&#36172;&#21338;&#35299;&#20915;&#26041;&#26696;&#19979;&#30340;&#20998;&#25955;&#24335;&#26234;&#33021;&#20805;&#30005;
&lt;/p&gt;
&lt;p&gt;
Decentralized Smart Charging of Large-Scale EVs using Adaptive Multi-Agent Multi-Armed Bandits. (arXiv:2307.10704v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10704
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#25955;&#24335;&#26234;&#33021;&#20805;&#30005;&#31995;&#32479;&#65292;&#21033;&#29992;&#33258;&#36866;&#24212;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#26041;&#27861;&#36827;&#34892;&#25511;&#21046;&#65292;&#24182;&#37319;&#29992;&#22810;&#33218;&#36172;&#21338;&#23398;&#20064;&#22788;&#29702;&#19981;&#30830;&#23450;&#24615;&#12290;&#35813;&#31995;&#32479;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#12289;&#23454;&#26102;&#24615;&#12289;&#26080;&#27169;&#22411;&#24615;&#65292;&#24182;&#19988;&#32771;&#34385;&#20102;&#19981;&#21516;&#21442;&#19982;&#32773;&#20043;&#38388;&#30340;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#21160;&#27773;&#36710;&#21644;&#20809;&#20239;&#30340;&#24555;&#36895;&#22686;&#38271;&#20250;&#24102;&#26469;&#26032;&#30340;&#25361;&#25112;&#65292;&#22914;&#30005;&#27969;&#25317;&#22581;&#21644;&#30005;&#21387;&#38480;&#21046;&#36829;&#35268;&#12290;&#36890;&#36807;&#25511;&#21046;&#30005;&#21160;&#27773;&#36710;&#30340;&#36816;&#34892;&#21363;&#26234;&#33021;&#20805;&#30005;&#65292;&#21487;&#20197;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#12290;&#25991;&#29486;&#20013;&#24050;&#25552;&#20986;&#38598;&#20013;&#24335;&#26234;&#33021;&#20805;&#30005;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#36825;&#26679;&#30340;&#35299;&#20915;&#26041;&#26696;&#21487;&#33021;&#32570;&#20047;&#21487;&#25193;&#23637;&#24615;&#65292;&#24182;&#19988;&#21463;&#21040;&#38598;&#20013;&#21270;&#30340;&#22266;&#26377;&#32570;&#28857;&#30340;&#24433;&#21709;&#65292;&#22914;&#21333;&#19968;&#25925;&#38556;&#28857;&#21644;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#12290;&#20998;&#25955;&#21270;&#21487;&#20197;&#24110;&#21161;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#20840;&#20998;&#25955;&#21270;&#30340;&#26234;&#33021;&#20805;&#30005;&#31995;&#32479;&#65292;&#37319;&#29992;&#33258;&#36866;&#24212;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#29702;&#24565;&#12290;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#21033;&#29992;&#22810;&#33218;&#36172;&#21338;&#23398;&#20064;&#26469;&#22788;&#29702;&#31995;&#32479;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#35813;&#31995;&#32479;&#20855;&#26377;&#20998;&#25955;&#21270;&#12289;&#21487;&#25193;&#23637;&#24615;&#12289;&#23454;&#26102;&#24615;&#12289;&#26080;&#27169;&#22411;&#24615;&#65292;&#24182;&#32771;&#34385;&#20102;&#19981;&#21516;&#21442;&#19982;&#32773;&#20043;&#38388;&#30340;&#20844;&#24179;&#24615;&#12290;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#35814;&#32454;&#30340;&#26696;&#20363;&#30740;&#31350;&#20197;&#36827;&#34892;&#24615;&#33021;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
The drastic growth of electric vehicles and photovoltaics can introduce new challenges, such as electrical current congestion and voltage limit violations due to peak load demands. These issues can be mitigated by controlling the operation of electric vehicles i.e., smart charging. Centralized smart charging solutions have already been proposed in the literature. But such solutions may lack scalability and suffer from inherent drawbacks of centralization, such as a single point of failure, and data privacy concerns. Decentralization can help tackle these challenges. In this paper, a fully decentralized smart charging system is proposed using the philosophy of adaptive multi-agent systems. The proposed system utilizes multi-armed bandit learning to handle uncertainties in the system. The presented system is decentralized, scalable, real-time, model-free, and takes fairness among different players into account. A detailed case study is also presented for performance evaluation.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#20013;&#30340;&#22270;&#24418;&#36827;&#34892;Granger&#22240;&#26524;&#24615;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;GraphEM&#31639;&#27861;&#20272;&#35745;&#32447;&#24615;&#39640;&#26031;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#20013;&#29366;&#24577;&#26041;&#31243;&#20013;&#30340;&#32447;&#24615;&#30697;&#38453;&#36816;&#31639;&#31526;&#65292;&#24182;&#22312;M&#27493;&#20013;&#21253;&#25324;Lasso&#27491;&#21017;&#21270;&#36827;&#34892;&#27714;&#35299;&#12290;&#36890;&#36807;&#22312;&#29609;&#20855;&#20363;&#23376;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#27668;&#20505;&#38382;&#39064;&#19978;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30456;&#23545;&#20110;&#26631;&#20934;&#30340;Granger&#22240;&#26524;&#24615;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2307.10703</link><description>&lt;p&gt;
&#22312;&#27668;&#20505;&#31185;&#23398;&#20013;&#20351;&#29992;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#20013;&#30340;&#22270;&#24418;&#36827;&#34892;Granger&#22240;&#26524;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Graphs in State-Space Models for Granger Causality in Climate Science. (arXiv:2307.10703v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10703
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#20013;&#30340;&#22270;&#24418;&#36827;&#34892;Granger&#22240;&#26524;&#24615;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;GraphEM&#31639;&#27861;&#20272;&#35745;&#32447;&#24615;&#39640;&#26031;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#20013;&#29366;&#24577;&#26041;&#31243;&#20013;&#30340;&#32447;&#24615;&#30697;&#38453;&#36816;&#31639;&#31526;&#65292;&#24182;&#22312;M&#27493;&#20013;&#21253;&#25324;Lasso&#27491;&#21017;&#21270;&#36827;&#34892;&#27714;&#35299;&#12290;&#36890;&#36807;&#22312;&#29609;&#20855;&#20363;&#23376;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#27668;&#20505;&#38382;&#39064;&#19978;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30456;&#23545;&#20110;&#26631;&#20934;&#30340;Granger&#22240;&#26524;&#24615;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Granger&#22240;&#26524;&#24615;(GC)&#32463;&#24120;&#34987;&#35748;&#20026;&#19981;&#26159;&#19968;&#31181;&#30495;&#27491;&#30340;&#22240;&#26524;&#20851;&#31995;&#24418;&#24335;&#12290;&#28982;&#32780;&#65292;&#23427;&#21487;&#20197;&#35828;&#26159;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#35780;&#20272;&#19968;&#20010;&#26102;&#38388;&#24207;&#21015;&#20174;&#21478;&#19968;&#20010;&#26102;&#38388;&#24207;&#21015;&#20013;&#21487;&#39044;&#27979;&#24615;&#30340;&#26041;&#27861;&#12290;Granger&#22240;&#26524;&#24615;&#22312;&#35768;&#22810;&#24212;&#29992;&#23398;&#31185;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20174;&#31070;&#32463;&#31185;&#23398;&#21644;&#35745;&#37327;&#32463;&#27982;&#23398;&#21040;&#22320;&#29699;&#31185;&#23398;&#12290;&#25105;&#20204;&#20174;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#22270;&#24418;&#35282;&#24230;&#37325;&#26032;&#23457;&#35270;Granger&#22240;&#26524;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;GraphEM&#65292;&#19968;&#31181;&#26368;&#36817;&#25552;&#20986;&#30340;&#29992;&#20110;&#20272;&#35745;&#32447;&#24615;&#39640;&#26031;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#29366;&#24577;&#26041;&#31243;&#20013;&#30340;&#32447;&#24615;&#30697;&#38453;&#31639;&#23376;&#30340;&#26399;&#26395;&#26368;&#22823;&#21270;&#31639;&#27861;&#12290;&#22312;M&#27493;&#20013;&#21253;&#25324;Lasso&#27491;&#21017;&#21270;&#65292;&#20351;&#29992;&#19968;&#31181;&#36817;&#31471;&#25286;&#20998;Douglas-Rachford&#31639;&#27861;&#36827;&#34892;&#27714;&#35299;&#12290;&#29609;&#20855;&#20363;&#23376;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#27668;&#20505;&#38382;&#39064;&#19978;&#30340;&#23454;&#39564;&#35828;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#21644;&#25512;&#26029;&#25216;&#26415;&#30456;&#23545;&#20110;&#26631;&#20934;Granger&#22240;&#26524;&#24615;&#26041;&#27861;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Granger causality (GC) is often considered not an actual form of causality. Still, it is arguably the most widely used method to assess the predictability of a time series from another one. Granger causality has been widely used in many applied disciplines, from neuroscience and econometrics to Earth sciences. We revisit GC under a graphical perspective of state-space models. For that, we use GraphEM, a recently presented expectation-maximisation algorithm for estimating the linear matrix operator in the state equation of a linear-Gaussian state-space model. Lasso regularisation is included in the M-step, which is solved using a proximal splitting Douglas-Rachford algorithm. Experiments in toy examples and challenging climate problems illustrate the benefits of the proposed model and inference technique over standard Granger causality methods.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21333;&#22270;&#20687;&#21435;&#22122;&#26041;&#27861;&#65292;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#25439;&#22833;&#65292;&#21482;&#20351;&#29992;&#22122;&#22768;&#36755;&#20837;&#36827;&#34892;&#32593;&#32476;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;&#24179;&#22343;&#19981;&#21516;&#23454;&#20363;&#30340;&#39044;&#27979;&#32467;&#26524;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.10695</link><description>&lt;p&gt;
&#33258;&#25105;&#33258;&#25105;+: &#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#25439;&#22833;&#30340;&#21333;&#22270;&#20687;&#21435;&#22122;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Self2Self+: Single-Image Denoising with Self-Supervised Learning and Image Quality Assessment Loss. (arXiv:2307.10695v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10695
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21333;&#22270;&#20687;&#21435;&#22122;&#26041;&#27861;&#65292;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#25439;&#22833;&#65292;&#21482;&#20351;&#29992;&#22122;&#22768;&#36755;&#20837;&#36827;&#34892;&#32593;&#32476;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;&#24179;&#22343;&#19981;&#21516;&#23454;&#20363;&#30340;&#39044;&#27979;&#32467;&#26524;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#30417;&#30563;&#23398;&#20064;&#30340;&#21435;&#22122;&#26041;&#27861;&#23637;&#29616;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23545;&#21253;&#21547;&#22122;&#22768;-&#24178;&#20928;&#22270;&#20687;&#23545;&#30340;&#22806;&#37096;&#25968;&#25454;&#38598;&#30340;&#20381;&#36182;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#36866;&#29992;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#22987;&#20851;&#27880;&#20351;&#29992;&#20165;&#21253;&#21547;&#22122;&#22768;&#36755;&#20837;&#30340;&#19968;&#32452;&#36755;&#20837;&#26469;&#35757;&#32451;&#21435;&#22122;&#32593;&#32476;&#12290;&#20026;&#20102;&#25913;&#21892;&#21435;&#22122;&#36807;&#31243;&#30340;&#21487;&#34892;&#24615;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21333;&#22270;&#20687;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#21482;&#20351;&#29992;&#22122;&#22768;&#36755;&#20837;&#22270;&#20687;&#36827;&#34892;&#32593;&#32476;&#35757;&#32451;&#12290;&#20351;&#29992;&#38376;&#25511;&#21367;&#31215;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#65292;&#20351;&#29992;&#26080;&#21442;&#32771;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#26469;&#25351;&#23548;&#35757;&#32451;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#29992;&#20271;&#21162;&#21033;&#37319;&#26679;&#20174;&#36755;&#20837;&#22270;&#20687;&#25968;&#25454;&#38598;&#20013;&#36873;&#25321;&#23454;&#20363;&#36827;&#34892;&#35757;&#32451;&#65292;&#37319;&#29992;&#19968;&#23450;&#30340;dropout&#29575;&#12290;&#36890;&#36807;&#23545;&#35757;&#32451;&#36807;&#30340;&#32593;&#32476;&#30340;&#19981;&#21516;&#23454;&#20363;&#36827;&#34892;&#39044;&#27979;&#65292;&#24182;&#36827;&#34892;&#24179;&#22343;&#65292;&#29983;&#25104;&#30456;&#24212;&#30340;&#32467;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25552;&#20986;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, denoising methods based on supervised learning have exhibited promising performance. However, their reliance on external datasets containing noisy-clean image pairs restricts their applicability. To address this limitation, researchers have focused on training denoising networks using solely a set of noisy inputs. To improve the feasibility of denoising procedures, in this study, we proposed a single-image self-supervised learning method in which only the noisy input image is used for network training. Gated convolution was used for feature extraction and no-reference image quality assessment was used for guiding the training process. Moreover, the proposed method sampled instances from the input image dataset using Bernoulli sampling with a certain dropout rate for training. The corresponding result was produced by averaging the generated predictions from various instances of the trained network with dropouts. The experimental results indicated that the proposed method achie
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#25968;&#38477;&#22122;&#31639;&#27861;&#65292;&#29992;&#20110;3D&#20998;&#23376;&#39044;&#35757;&#32451;&#12290;&#36890;&#36807;&#28151;&#21512;&#22122;&#22768;&#31574;&#30053;&#35299;&#20915;&#20102;&#26679;&#26412;&#35206;&#30422;&#29575;&#20302;&#21644;&#21508;&#21521;&#21516;&#24615;&#21147;&#22330;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#35299;&#32806;&#20004;&#31181;&#31867;&#22411;&#30340;&#22122;&#22768;&#20811;&#26381;&#20102;&#20256;&#32479;&#38477;&#22122;&#26041;&#27861;&#26080;&#27861;&#23398;&#20064;&#21147;&#22330;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.10683</link><description>&lt;p&gt;
&#20998;&#25968;&#38477;&#22122;&#29992;&#20110;3D&#20998;&#23376;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Fractional Denoising for 3D Molecular Pre-training. (arXiv:2307.10683v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10683
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#25968;&#38477;&#22122;&#31639;&#27861;&#65292;&#29992;&#20110;3D&#20998;&#23376;&#39044;&#35757;&#32451;&#12290;&#36890;&#36807;&#28151;&#21512;&#22122;&#22768;&#31574;&#30053;&#35299;&#20915;&#20102;&#26679;&#26412;&#35206;&#30422;&#29575;&#20302;&#21644;&#21508;&#21521;&#21516;&#24615;&#21147;&#22330;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#35299;&#32806;&#20004;&#31181;&#31867;&#22411;&#30340;&#22122;&#22768;&#20811;&#26381;&#20102;&#20256;&#32479;&#38477;&#22122;&#26041;&#27861;&#26080;&#27861;&#23398;&#20064;&#21147;&#22330;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22352;&#26631;&#38477;&#22122;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;3D&#20998;&#23376;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#22312;&#21508;&#31181;&#19979;&#28216;&#33647;&#29289;&#21457;&#29616;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#20854;&#30446;&#26631;&#31561;&#21516;&#20110;&#23398;&#20064;&#21147;&#22330;&#65292;&#24182;&#19988;&#23545;&#19979;&#28216;&#20219;&#21153;&#26377;&#24110;&#21161;&#12290;&#28982;&#32780;&#65292;&#22352;&#26631;&#38477;&#22122;&#23398;&#20064;&#26377;&#25928;&#21147;&#22330;&#38754;&#20020;&#20004;&#20010;&#25361;&#25112;&#65292;&#21363;&#26679;&#26412;&#35206;&#30422;&#29575;&#20302;&#21644;&#21508;&#21521;&#21516;&#24615;&#21147;&#22330;&#12290;&#26681;&#26412;&#21407;&#22240;&#22312;&#20110;&#29616;&#26377;&#38477;&#22122;&#26041;&#27861;&#25152;&#20551;&#35774;&#30340;&#20998;&#23376;&#20998;&#24067;&#19981;&#33021;&#25429;&#25417;&#20998;&#23376;&#30340;&#21508;&#21521;&#24322;&#24615;&#29305;&#24449;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#22122;&#22768;&#31574;&#30053;&#65292;&#21253;&#25324;&#20108;&#38754;&#35282;&#21644;&#22352;&#26631;&#30340;&#22122;&#22768;&#12290;&#28982;&#32780;&#65292;&#20197;&#20256;&#32479;&#26041;&#24335;&#38477;&#22122;&#36825;&#31181;&#28151;&#21512;&#22122;&#22768;&#19981;&#20877;&#31561;&#21516;&#20110;&#23398;&#20064;&#21147;&#22330;&#12290;&#36890;&#36807;&#29702;&#35770;&#25512;&#23548;&#65292;&#25105;&#20204;&#21457;&#29616;&#38382;&#39064;&#26159;&#30001;&#20110;&#36755;&#20837;&#26500;&#35937;&#23545;&#21327;&#26041;&#24046;&#30340;&#20381;&#36182;&#24615;&#25152;&#23548;&#33268;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#36825;&#20004;&#31181;&#31867;&#22411;&#30340;&#22122;&#22768;&#35299;&#32806;
&lt;/p&gt;
&lt;p&gt;
Coordinate denoising is a promising 3D molecular pre-training method, which has achieved remarkable performance in various downstream drug discovery tasks. Theoretically, the objective is equivalent to learning the force field, which is revealed helpful for downstream tasks. Nevertheless, there are two challenges for coordinate denoising to learn an effective force field, i.e. low coverage samples and isotropic force field. The underlying reason is that molecular distributions assumed by existing denoising methods fail to capture the anisotropic characteristic of molecules. To tackle these challenges, we propose a novel hybrid noise strategy, including noises on both dihedral angel and coordinate. However, denoising such hybrid noise in a traditional way is no more equivalent to learning the force field. Through theoretical deductions, we find that the problem is caused by the dependency of the input conformation for covariance. To this end, we propose to decouple the two types of nois
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#24212;&#29992;&#20110;&#25277;&#35937;&#22270;&#20687;&#20998;&#31867;&#26102;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#32463;&#20856;&#20998;&#31867;&#27169;&#22411;&#30340;&#38480;&#21046;&#12290;&#36890;&#36807;&#23558;&#35813;&#27169;&#22411;&#24212;&#29992;&#20110;&#22122;&#22768;&#20108;&#32500;&#30721;&#30340;&#20998;&#31867;&#65292;&#30740;&#31350;&#32773;&#21457;&#29616;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#29702;&#35299;&#25277;&#35937;&#22270;&#20687;&#26041;&#38754;&#20855;&#26377;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.10677</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#29992;&#20110;&#22122;&#22768;&#20108;&#32500;&#30721;&#30340;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Deep learning for classification of noisy QR codes. (arXiv:2307.10677v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10677
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#24212;&#29992;&#20110;&#25277;&#35937;&#22270;&#20687;&#20998;&#31867;&#26102;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#32463;&#20856;&#20998;&#31867;&#27169;&#22411;&#30340;&#38480;&#21046;&#12290;&#36890;&#36807;&#23558;&#35813;&#27169;&#22411;&#24212;&#29992;&#20110;&#22122;&#22768;&#20108;&#32500;&#30721;&#30340;&#20998;&#31867;&#65292;&#30740;&#31350;&#32773;&#21457;&#29616;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#29702;&#35299;&#25277;&#35937;&#22270;&#20687;&#26041;&#38754;&#20855;&#26377;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24076;&#26395;&#23450;&#20041;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#32463;&#20856;&#20998;&#31867;&#27169;&#22411;&#22312;&#24212;&#29992;&#20110;&#25277;&#35937;&#22270;&#20687;&#65288;&#19981;&#20195;&#34920;&#21487;&#35270;&#21487;&#35782;&#21035;&#23545;&#35937;&#30340;&#22270;&#20687;&#65289;&#26102;&#30340;&#38480;&#21046;&#12290;&#20108;&#32500;&#30721;&#23646;&#20110;&#36825;&#31181;&#25277;&#35937;&#22270;&#20687;&#30340;&#19968;&#31867;&#65306;&#19968;&#20010;&#27604;&#29305;&#23545;&#24212;&#19968;&#20010;&#32534;&#30721;&#23383;&#31526;&#65292;&#20108;&#32500;&#30721;&#24182;&#19981;&#26159;&#20026;&#20102;&#25163;&#21160;&#35299;&#30721;&#32780;&#35774;&#35745;&#30340;&#12290;&#20026;&#20102;&#29702;&#35299;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#25277;&#35937;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;&#20581;&#24247;&#36890;&#34892;&#35777;&#19978;&#30340;&#20449;&#24687;&#29983;&#25104;&#20102;&#20108;&#32500;&#30721;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#35757;&#32451;&#12290;&#25105;&#20204;&#23558;&#20998;&#31867;&#27169;&#22411;&#19982;&#32463;&#20856;&#30340;&#65288;&#30830;&#23450;&#24615;&#65289;&#35299;&#30721;&#26041;&#27861;&#22312;&#23384;&#22312;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#27604;&#36739;&#12290;&#36825;&#39033;&#30740;&#31350;&#20351;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65306;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27169;&#22411;&#23545;&#20110;&#29702;&#35299;&#25277;&#35937;&#22270;&#20687;&#26159;&#30456;&#20851;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We wish to define the limits of a classical classification model based on deep learning when applied to abstract images, which do not represent visually identifiable objects.QR codes (Quick Response codes) fall into this category of abstract images: one bit corresponding to one encoded character, QR codes were not designed to be decoded manually. To understand the limitations of a deep learning-based model for abstract image classification, we train an image classification model on QR codes generated from information obtained when reading a health pass. We compare a classification model with a classical (deterministic) decoding method in the presence of noise. This study allows us to conclude that a model based on deep learning can be relevant for the understanding of abstract images.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35745;&#31639;&#26465;&#20214;&#29256;&#26412;&#30340;&#65288;&#20195;&#29702;&#65289;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#21644;&#20854;&#20182;&#22238;&#24402;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#24182;&#32771;&#34385;&#29305;&#24449;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#21516;&#26102;&#65292;&#35813;&#26041;&#27861;&#36824;&#21487;&#20197;&#24212;&#29992;&#20110;&#22797;&#26434;&#22238;&#24402;&#27169;&#22411;&#30340;&#20998;&#26512;&#65292;&#24182;&#25552;&#20379;&#27491;&#30830;&#30340;&#20559;&#20381;&#36182;&#22270;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2307.10654</link><description>&lt;p&gt;
&#26465;&#20214;&#26399;&#26395;&#32593;&#32476;&#29992;&#20110;SHAP
&lt;/p&gt;
&lt;p&gt;
Conditional expectation network for SHAP. (arXiv:2307.10654v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10654
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35745;&#31639;&#26465;&#20214;&#29256;&#26412;&#30340;&#65288;&#20195;&#29702;&#65289;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#21644;&#20854;&#20182;&#22238;&#24402;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#24182;&#32771;&#34385;&#29305;&#24449;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#21516;&#26102;&#65292;&#35813;&#26041;&#27861;&#36824;&#21487;&#20197;&#24212;&#29992;&#20110;&#22797;&#26434;&#22238;&#24402;&#27169;&#22411;&#30340;&#20998;&#26512;&#65292;&#24182;&#25552;&#20379;&#27491;&#30830;&#30340;&#20559;&#20381;&#36182;&#22270;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
SHAP&#26159;&#19968;&#31181;&#38750;&#24120;&#27969;&#34892;&#30340;&#27169;&#22411;&#26080;&#20851;&#25216;&#26415;&#65292;&#29992;&#20110;&#35299;&#37322;&#39044;&#27979;&#27169;&#22411;&#12290;SHAP&#30340;&#20004;&#20010;&#26368;&#21463;&#27426;&#36814;&#30340;&#29256;&#26412;&#26159;&#26465;&#20214;&#26399;&#26395;&#29256;&#26412;&#21644;&#26080;&#26465;&#20214;&#26399;&#26395;&#29256;&#26412;&#65288;&#21518;&#32773;&#20063;&#31216;&#20026;&#24178;&#39044;SHAP&#65289;&#12290;&#38500;&#20102;&#22522;&#20110;&#26641;&#30340;&#26041;&#27861;&#20043;&#22806;&#65292;&#36890;&#24120;&#20351;&#29992;&#26080;&#26465;&#20214;&#29256;&#26412;&#65288;&#20986;&#20110;&#35745;&#31639;&#21407;&#22240;&#65289;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#65288;&#20195;&#29702;&#65289;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#35745;&#31639;&#31070;&#32463;&#32593;&#32476;&#21644;&#20854;&#20182;&#22238;&#24402;&#27169;&#22411;&#30340;&#26465;&#20214;&#29256;&#26412;&#65292;&#24182;&#27491;&#30830;&#32771;&#34385;&#29305;&#24449;&#32452;&#20214;&#20043;&#38388;&#30340;&#20381;&#36182;&#32467;&#26500;&#12290;&#36825;&#20010;&#26041;&#27861;&#36824;&#21487;&#20197;&#29992;&#20110;&#25552;&#20379;&#19982;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#65288;GLM&#65289;&#31867;&#20284;&#30340;&#22797;&#26434;&#22238;&#24402;&#27169;&#22411;&#30340;drop1&#21644;anova&#20998;&#26512;&#65292;&#24182;&#25552;&#20379;&#32771;&#34385;&#29305;&#24449;&#32452;&#20214;&#20013;&#27491;&#30830;&#20381;&#36182;&#32467;&#26500;&#30340;&#20559;&#20381;&#36182;&#22270;&#65288;PDP&#65289;&#30340;&#23545;&#24212;&#29256;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
A very popular model-agnostic technique for explaining predictive models is the SHapley Additive exPlanation (SHAP). The two most popular versions of SHAP are a conditional expectation version and an unconditional expectation version (the latter is also known as interventional SHAP). Except for tree-based methods, usually the unconditional version is used (for computational reasons). We provide a (surrogate) neural network approach which allows us to efficiently calculate the conditional version for both neural networks and other regression models, and which properly considers the dependence structure in the feature components. This proposal is also useful to provide drop1 and anova analyses in complex regression models which are similar to their generalized linear model (GLM) counterparts, and we provide a partial dependence plot (PDP) counterpart that considers the right dependence structure in the feature components.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#20013;&#33258;&#21160;&#21442;&#25968;&#20248;&#21270;&#12290;&#26694;&#26550;&#24341;&#20837;&#20102;&#19977;&#20010;&#20248;&#21270;&#30446;&#26631;&#65306;&#39044;&#27979;&#24471;&#20998;&#12289;&#24418;&#29366;&#24471;&#20998;&#21644;&#25935;&#24863;&#24230;&#24471;&#20998;&#65292;&#24182;&#25104;&#21151;&#22312;&#32447;&#24212;&#29992;&#20102;&#20845;&#20010;&#26376;&#20197;&#19978;&#65292;&#27599;&#20998;&#38047;&#20026;&#36229;&#36807;50,000&#20010;&#26102;&#38388;&#24207;&#21015;&#25552;&#20379;&#26381;&#21153;&#12290;&#35813;&#26694;&#26550;&#31616;&#21270;&#20102;&#29992;&#25143;&#20307;&#39564;&#65292;&#25552;&#20379;&#20102;&#29992;&#25143;&#21451;&#22909;&#30340;&#30028;&#38754;&#65292;&#24182;&#23454;&#29616;&#20102;&#26399;&#26395;&#30340;&#26816;&#27979;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.10653</link><description>&lt;p&gt;
&#33258;&#21160;&#21333;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#22312;&#30417;&#25511;&#26381;&#21153;&#20013;&#30340;&#20248;&#21270;&#30446;&#26631;&#32454;&#21270;
&lt;/p&gt;
&lt;p&gt;
Refining the Optimization Target for Automatic Univariate Time Series Anomaly Detection in Monitoring Services. (arXiv:2307.10653v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10653
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#20013;&#33258;&#21160;&#21442;&#25968;&#20248;&#21270;&#12290;&#26694;&#26550;&#24341;&#20837;&#20102;&#19977;&#20010;&#20248;&#21270;&#30446;&#26631;&#65306;&#39044;&#27979;&#24471;&#20998;&#12289;&#24418;&#29366;&#24471;&#20998;&#21644;&#25935;&#24863;&#24230;&#24471;&#20998;&#65292;&#24182;&#25104;&#21151;&#22312;&#32447;&#24212;&#29992;&#20102;&#20845;&#20010;&#26376;&#20197;&#19978;&#65292;&#27599;&#20998;&#38047;&#20026;&#36229;&#36807;50,000&#20010;&#26102;&#38388;&#24207;&#21015;&#25552;&#20379;&#26381;&#21153;&#12290;&#35813;&#26694;&#26550;&#31616;&#21270;&#20102;&#29992;&#25143;&#20307;&#39564;&#65292;&#25552;&#20379;&#20102;&#29992;&#25143;&#21451;&#22909;&#30340;&#30028;&#38754;&#65292;&#24182;&#23454;&#29616;&#20102;&#26399;&#26395;&#30340;&#26816;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#23545;&#20110;&#22788;&#29702;&#22823;&#37327;&#25968;&#25454;&#12289;&#30830;&#20445;&#21487;&#38752;&#24615;&#21644;&#20248;&#21270;&#31995;&#32479;&#24615;&#33021;&#30340;&#24037;&#19994;&#30417;&#25511;&#26381;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#26631;&#27880;&#36164;&#28304;&#21644;&#25163;&#21160;&#21442;&#25968;&#36873;&#25321;&#65292;&#24378;&#35843;&#20102;&#33258;&#21160;&#21270;&#30340;&#38656;&#27714;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#20013;&#33258;&#21160;&#21442;&#25968;&#20248;&#21270;&#12290;&#35813;&#26694;&#26550;&#24341;&#20837;&#20102;&#19977;&#20010;&#20248;&#21270;&#30446;&#26631;&#65306;&#39044;&#27979;&#24471;&#20998;&#12289;&#24418;&#29366;&#24471;&#20998;&#21644;&#25935;&#24863;&#24230;&#24471;&#20998;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#20808;&#39564;&#30693;&#35782;&#25110;&#25163;&#21160;&#26631;&#27880;&#30340;&#24773;&#20917;&#19979;&#36731;&#26494;&#36866;&#24212;&#19981;&#21516;&#30340;&#27169;&#22411;&#32972;&#26223;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#24050;&#25104;&#21151;&#22312;&#32447;&#24212;&#29992;&#20102;&#20845;&#20010;&#26376;&#20197;&#19978;&#65292;&#27599;&#20998;&#38047;&#20026;&#36229;&#36807;50,000&#20010;&#26102;&#38388;&#24207;&#21015;&#25552;&#20379;&#26381;&#21153;&#12290;&#23427;&#36890;&#36807;&#21482;&#38656;&#19968;&#20010;&#39044;&#26399;&#25935;&#24863;&#20540;&#31616;&#21270;&#20102;&#29992;&#25143;&#30340;&#20307;&#39564;&#65292;&#25552;&#20379;&#20102;&#29992;&#25143;&#21451;&#22909;&#30340;&#30028;&#38754;&#65292;&#24182;&#23454;&#29616;&#20102;&#26399;&#26395;&#30340;&#26816;&#27979;&#32467;&#26524;&#12290;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#35780;&#20272;&#21644;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series anomaly detection is crucial for industrial monitoring services that handle a large volume of data, aiming to ensure reliability and optimize system performance. Existing methods often require extensive labeled resources and manual parameter selection, highlighting the need for automation. This paper proposes a comprehensive framework for automatic parameter optimization in time series anomaly detection models. The framework introduces three optimization targets: prediction score, shape score, and sensitivity score, which can be easily adapted to different model backbones without prior knowledge or manual labeling efforts. The proposed framework has been successfully applied online for over six months, serving more than 50,000 time series every minute. It simplifies the user's experience by requiring only an expected sensitive value, offering a user-friendly interface, and achieving desired detection results. Extensive evaluations conducted on public datasets and comparison
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#30340;&#26080;&#32447;&#32593;&#32476;&#24310;&#36831;&#27010;&#29575;&#39044;&#27979;&#65292;&#37325;&#28857;&#20851;&#27880;&#23614;&#37096;&#27010;&#29575;&#20998;&#24067;&#12290;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#20934;&#30830;&#20272;&#35745;&#32597;&#35265;&#24310;&#36831;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#21046;&#23450;&#32593;&#32476;&#36164;&#28304;&#37197;&#32622;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2307.10648</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26080;&#32447;&#32593;&#32476;&#24310;&#36831;&#27010;&#29575;&#39044;&#27979;&#65306;&#37325;&#28857;&#20851;&#27880;&#23614;&#37096;&#27010;&#29575;
&lt;/p&gt;
&lt;p&gt;
Data-Driven Latency Probability Prediction for Wireless Networks: Focusing on Tail Probabilities. (arXiv:2307.10648v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10648
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#30340;&#26080;&#32447;&#32593;&#32476;&#24310;&#36831;&#27010;&#29575;&#39044;&#27979;&#65292;&#37325;&#28857;&#20851;&#27880;&#23614;&#37096;&#27010;&#29575;&#20998;&#24067;&#12290;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#20934;&#30830;&#20272;&#35745;&#32597;&#35265;&#24310;&#36831;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#21046;&#23450;&#32593;&#32476;&#36164;&#28304;&#37197;&#32622;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26032;&#30340;&#24212;&#29992;&#39046;&#22495;&#30340;&#20986;&#29616;&#65292;&#20363;&#22914;&#32593;&#32476;&#29289;&#29702;&#31995;&#32479;&#21644;&#20154;&#26426;&#21327;&#20316;&#24212;&#29992;&#65292;&#38656;&#35201;&#20197;&#26497;&#39640;&#30340;&#21487;&#38752;&#24615;&#20445;&#35777;&#19968;&#23450;&#27700;&#24179;&#30340;&#31471;&#21040;&#31471;&#32593;&#32476;&#24310;&#36831;&#65292;&#20363;&#22914;99.999%&#12290;&#34429;&#28982;IEEE 802.1as&#26102;&#25935;&#32593;&#32476;&#65288;TSN&#65289;&#35268;&#33539;&#20013;&#30340;&#26426;&#21046;&#21487;&#20197;&#29992;&#26469;&#23454;&#29616;&#20197;&#22826;&#32593;&#20132;&#25442;&#32593;&#32476;&#30340;&#36825;&#20123;&#35201;&#27714;&#65292;&#20294;&#22312;&#26080;&#32447;&#32593;&#32476;&#20013;&#23454;&#29616;TSN&#26426;&#21046;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#20854;&#20855;&#26377;&#38543;&#26426;&#24615;&#36136;&#12290;&#20026;&#20102;&#23558;&#26080;&#32447;&#38142;&#36335;&#21512;&#35268;&#21040;99.999%&#30340;&#21487;&#38752;&#24230;&#27700;&#24179;&#65292;&#24517;&#39035;&#20998;&#26512;&#21644;&#25511;&#21046;&#24310;&#36831;&#27010;&#29575;&#20998;&#24067;&#20013;&#26497;&#20026;&#32597;&#35265;&#30340;&#24322;&#24120;&#20540;&#65292;&#21363;&#20998;&#24067;&#30340;&#23614;&#37096;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#22914;&#28151;&#21512;&#23494;&#24230;&#32593;&#32476;&#65288;MDN&#65289;&#21644;&#26497;&#20540;&#28151;&#21512;&#27169;&#22411;&#65292;&#23545;&#24310;&#36831;&#20998;&#24067;&#30340;&#23614;&#37096;&#36827;&#34892;&#39044;&#27979;&#65292;&#20197;&#20272;&#35745;&#22522;&#20110;&#32593;&#32476;&#21442;&#25968;&#30340;&#32597;&#35265;&#24310;&#36831;&#30340;&#21487;&#33021;&#24615;&#65292;&#20174;&#32780;&#21487;&#20197;&#21046;&#23450;&#26356;&#21152;&#31934;&#30830;&#30340;&#32593;&#32476;&#36164;&#28304;&#37197;&#32622;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the emergence of new application areas, such as cyber-physical systems and human-in-the-loop applications, there is a need to guarantee a certain level of end-to-end network latency with extremely high reliability, e.g., 99.999%. While mechanisms specified under IEEE 802.1as time-sensitive networking (TSN) can be used to achieve these requirements for switched Ethernet networks, implementing TSN mechanisms in wireless networks is challenging due to their stochastic nature. To conform the wireless link to a reliability level of 99.999%, the behavior of extremely rare outliers in the latency probability distribution, or the tail of the distribution, must be analyzed and controlled. This work proposes predicting the tail of the latency distribution using state-of-the-art data-driven approaches, such as mixture density networks (MDN) and extreme value mixture models, to estimate the likelihood of rare latencies conditioned on the network parameters, which can be used to make more info
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#21644;&#40065;&#26834;&#30340;&#26041;&#27861;&#26469;&#36817;&#20284;&#35745;&#31639;&#22810;&#20803;&#27491;&#24577;&#20998;&#24067;&#20043;&#38388;&#30340;Fisher-Rao&#36317;&#31163;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31867;&#22522;&#20110;&#27491;&#24577;&#27969;&#24418;&#23884;&#20837;&#21040;&#39640;&#32500;&#23545;&#31216;&#27491;&#23450;&#38181;&#23376;&#27969;&#24418;&#30340;&#36317;&#31163;&#12290;</title><link>http://arxiv.org/abs/2307.10644</link><description>&lt;p&gt;
Fisher-Rao&#36317;&#31163;&#21644;&#36870;&#25512;&#21040;SPD&#38181;&#36317;&#31163;&#22312;&#22810;&#20803;&#27491;&#24577;&#20998;&#24067;&#20043;&#38388;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Fisher-Rao distance and pullback SPD cone distances between multivariate normal distributions. (arXiv:2307.10644v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10644
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#21644;&#40065;&#26834;&#30340;&#26041;&#27861;&#26469;&#36817;&#20284;&#35745;&#31639;&#22810;&#20803;&#27491;&#24577;&#20998;&#24067;&#20043;&#38388;&#30340;Fisher-Rao&#36317;&#31163;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31867;&#22522;&#20110;&#27491;&#24577;&#27969;&#24418;&#23884;&#20837;&#21040;&#39640;&#32500;&#23545;&#31216;&#27491;&#23450;&#38181;&#23376;&#27969;&#24418;&#30340;&#36317;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#31185;&#23398;&#39046;&#22495;&#65292;&#22914;&#25193;&#25955;&#24352;&#37327;&#25104;&#20687;&#12289;&#32467;&#26500;&#24352;&#37327;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#38647;&#36798;&#20449;&#21495;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#31561;&#65292;&#37117;&#23384;&#22312;&#30528;&#22810;&#20803;&#27491;&#24577;&#20998;&#24067;&#30340;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#22788;&#29702;&#36825;&#20123;&#27491;&#24577;&#25968;&#25454;&#38598;&#20197;&#36827;&#34892;&#36807;&#28388;&#12289;&#20998;&#31867;&#25110;&#32858;&#31867;&#31561;&#19979;&#28216;&#20219;&#21153;&#65292;&#38656;&#35201;&#23450;&#20041;&#21512;&#36866;&#30340;&#27491;&#24577;&#21644;&#23427;&#20204;&#20043;&#38388;&#30340;&#36335;&#24452;&#20043;&#38388;&#30340;&#24046;&#24322;&#24230;&#37327;&#12290;Fisher-Rao&#36317;&#31163;&#65292;&#20316;&#20026;Fisher&#20449;&#24687;&#24230;&#37327;&#24341;&#36215;&#30340;Riemann&#20960;&#20309;&#36317;&#31163;&#65292;&#26159;&#19968;&#31181;&#21512;&#29702;&#30340;&#24230;&#37327;&#36317;&#31163;&#65292;&#20294;&#38500;&#20102;&#19968;&#20123;&#29305;&#27530;&#24773;&#20917;&#22806;&#65292;&#24182;&#27809;&#26377;&#38381;&#24335;&#27714;&#35299;&#12290;&#26412;&#25991;&#39318;&#20808;&#25253;&#21578;&#20102;&#19968;&#31181;&#24555;&#36895;&#19988;&#40065;&#26834;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#31934;&#30830;&#22320;&#36817;&#20284;&#35745;&#31639;&#22810;&#20803;&#27491;&#24577;&#20998;&#24067;&#20043;&#38388;&#30340;Fisher-Rao&#36317;&#31163;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31867;&#22522;&#20110;&#27491;&#24577;&#27969;&#24418;&#21040;&#39640;&#32500;&#23545;&#31216;&#27491;&#23450;&#38181;&#30340;&#23376;&#27969;&#24418;&#30340;&#24494;&#20998;&#21516;&#32986;&#23884;&#20837;&#30340;&#36317;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data sets of multivariate normal distributions abound in many scientific areas like diffusion tensor imaging, structure tensor computer vision, radar signal processing, machine learning, just to name a few. In order to process those normal data sets for downstream tasks like filtering, classification or clustering, one needs to define proper notions of dissimilarities between normals and paths joining them. The Fisher-Rao distance defined as the Riemannian geodesic distance induced by the Fisher information metric is such a principled metric distance which however is not known in closed-form excepts for a few particular cases. In this work, we first report a fast and robust method to approximate arbitrarily finely the Fisher-Rao distance between multivariate normal distributions. Second, we introduce a class of distances based on diffeomorphic embeddings of the normal manifold into a submanifold of the higher-dimensional symmetric positive-definite cone corresponding to the manifold of
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;SciBench&#30340;&#22522;&#20934;&#22871;&#20214;&#65292;&#26088;&#22312;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22823;&#23398;&#27700;&#24179;&#31185;&#23398;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#36827;&#34892;&#35780;&#20272;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#24403;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#25552;&#20379;&#22797;&#26434;&#31185;&#23398;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#26041;&#38754;&#36824;&#26377;&#19981;&#36275;&#20043;&#22788;&#12290;</title><link>http://arxiv.org/abs/2307.10635</link><description>&lt;p&gt;
SciBench: &#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#22823;&#23398;&#27700;&#24179;&#30340;&#31185;&#23398;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models. (arXiv:2307.10635v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10635
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;SciBench&#30340;&#22522;&#20934;&#22871;&#20214;&#65292;&#26088;&#22312;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22823;&#23398;&#27700;&#24179;&#31185;&#23398;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#36827;&#34892;&#35780;&#20272;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#24403;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#25552;&#20379;&#22797;&#26434;&#31185;&#23398;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#26041;&#38754;&#36824;&#26377;&#19981;&#36275;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#36827;&#23637;&#22312;&#35768;&#22810;&#25968;&#23398;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#27493;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22522;&#20934;&#22823;&#22810;&#21482;&#21253;&#21547;&#21021;&#39640;&#20013;&#31185;&#30446;&#30340;&#38382;&#39064;&#65292;&#20165;&#21253;&#21547;&#22810;&#39033;&#36873;&#25321;&#39064;&#65292;&#24182;&#19988;&#20165;&#38480;&#20110;&#22522;&#26412;&#31639;&#26415;&#36816;&#31639;&#33539;&#22260;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#24191;&#27867;&#30340;&#22522;&#20934;&#22871;&#20214;SciBench&#65292;&#26088;&#22312;&#31995;&#32479;&#22320;&#26816;&#27979;&#22797;&#26434;&#31185;&#23398;&#38382;&#39064;&#35299;&#20915;&#25152;&#38656;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;SciBench&#21253;&#21547;&#20004;&#20010;&#32463;&#36807;&#31934;&#24515;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#65306;&#19968;&#20010;&#24320;&#25918;&#38598;&#65292;&#21253;&#25324;&#20174;&#25968;&#23398;&#12289;&#21270;&#23398;&#21644;&#29289;&#29702;&#25945;&#31185;&#20070;&#20013;&#25688;&#24405;&#30340;&#22823;&#23398;&#27700;&#24179;&#30340;&#31185;&#23398;&#38382;&#39064;&#65292;&#20197;&#21450;&#19968;&#20010;&#23553;&#38381;&#38598;&#65292;&#21253;&#21547;&#26469;&#33258;&#35745;&#31639;&#26426;&#31185;&#23398;&#21644;&#25968;&#23398;&#26412;&#31185;&#32771;&#35797;&#30340;&#38382;&#39064;&#12290;&#22522;&#20110;&#36825;&#20004;&#20010;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#23545;&#20004;&#20010;&#20195;&#34920;&#24615;&#30340;LLM&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#22522;&#20934;&#30740;&#31350;&#65292;&#24182;&#37319;&#29992;&#19981;&#21516;&#30340;&#25552;&#31034;&#31574;&#30053;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#21069;&#30340;LLMs&#22312;&#25552;&#20379;&#22797;&#26434;&#31185;&#23398;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#26041;&#38754;&#36824;&#23384;&#22312;&#19981;&#36275;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in large language models (LLMs) have demonstrated notable progress on many mathematical benchmarks. However, most of these benchmarks only feature problems grounded in junior and senior high school subjects, contain only multiple-choice questions, and are confined to a limited scope of elementary arithmetic operations. To address these issues, this paper introduces an expansive benchmark suite SciBench that aims to systematically examine the reasoning capabilities required for complex scientific problem solving. SciBench contains two carefully curated datasets: an open set featuring a range of collegiate-level scientific problems drawn from mathematics, chemistry, and physics textbooks, and a closed set comprising problems from undergraduate-level exams in computer science and mathematics. Based on the two datasets, we conduct an in-depth benchmark study of two representative LLMs with various prompting strategies. The results reveal that current LLMs fall short of deli
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#20154;&#31867;&#22522;&#22240;&#30340;&#26680;&#33527;&#37240;&#24207;&#21015;&#65292;&#22635;&#34917;&#20102;DNA&#24207;&#21015;&#29983;&#25104;&#27169;&#22411;&#30740;&#31350;&#30340;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2307.10634</link><description>&lt;p&gt;
&#20154;&#31867;&#22522;&#22240;&#26680;&#33527;&#37240;&#24207;&#21015;&#30340;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Generative Language Models on Nucleotide Sequences of Human Genes. (arXiv:2307.10634v1 [q-bio.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10634
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#20154;&#31867;&#22522;&#22240;&#30340;&#26680;&#33527;&#37240;&#24207;&#21015;&#65292;&#22635;&#34917;&#20102;DNA&#24207;&#21015;&#29983;&#25104;&#27169;&#22411;&#30740;&#31350;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22312;DNA&#30456;&#20851;&#30340;&#29983;&#29289;&#20449;&#24687;&#23398;&#39046;&#22495;&#65292;&#29983;&#25104;&#27169;&#22411;&#30340;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#31867;&#20284;&#20110;GPT-3&#30340;&#33258;&#22238;&#24402;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#20154;&#31867;&#22522;&#22240;&#30340;&#26680;&#33527;&#37240;&#24207;&#21015;&#12290;&#32771;&#34385;&#21040;&#22788;&#29702;&#25972;&#20010;DNA&#24207;&#21015;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#65292;&#25105;&#20204;&#20915;&#23450;&#22312;&#26356;&#23567;&#30340;&#23610;&#24230;&#19978;&#36827;&#34892;&#30740;&#31350;&#65292;&#37325;&#28857;&#20851;&#27880;&#20154;&#31867;&#22522;&#22240;&#30340;&#26680;&#33527;&#37240;&#24207;&#21015;&#65292;&#32780;&#19981;&#26159;&#25972;&#20010;DNA&#12290;&#36825;&#20010;&#20915;&#31574;&#24182;&#19981;&#25913;&#21464;&#38382;&#39064;&#30340;&#32467;&#26500;&#65292;&#22240;&#20026;DNA&#21644;&#22522;&#22240;&#37117;&#21487;&#20197;&#30475;&#20316;&#30001;&#22235;&#31181;&#19981;&#21516;&#30340;&#26680;&#33527;&#37240;&#32452;&#25104;&#30340;&#19968;&#32500;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models, primarily transformer-based ones, obtained colossal success in NLP. To be more precise, studies like BERT in NLU and works such as GPT-3 for NLG are very crucial. DNA sequences are very close to natural language in terms of structure, so if the DNA-related bioinformatics domain is concerned, discriminative models, like DNABert, exist. Yet, the generative side of the coin is mainly unexplored to the best of our knowledge. Consequently, we focused on developing an autoregressive generative language model like GPT-3 for DNA sequences. Because working with whole DNA sequences is challenging without substantial computational resources, we decided to carry out our study on a smaller scale, focusing on nucleotide sequences of human genes, unique parts in DNA with specific functionalities, instead of the whole DNA. This decision did not change the problem structure a lot due to the fact that both DNA and genes can be seen as 1D sequences consisting of four different nucleotide
&lt;/p&gt;</description></item><item><title>&#22810;&#26041;&#27861;&#33258;&#35757;&#32451;&#21487;&#20197;&#36890;&#36807;&#22312;&#19981;&#21516;&#26041;&#27861;&#20043;&#38388;&#35757;&#32451;&#21644;&#29983;&#25104;&#25968;&#25454;&#26469;&#25913;&#21892;&#20195;&#30721;&#29983;&#25104;&#30340;&#24615;&#33021;&#65292;&#20351;&#27169;&#22411;&#26356;&#26131;&#20110;&#20351;&#29992;&#65292;&#24182;&#25552;&#39640;&#30456;&#20851;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.10633</link><description>&lt;p&gt;
&#22810;&#26041;&#27861;&#33258;&#35757;&#32451;&#65306;&#36890;&#36807;&#25991;&#26412;&#25913;&#36827;&#20195;&#30721;&#29983;&#25104;&#65292;&#21453;&#20043;&#20134;&#28982;
&lt;/p&gt;
&lt;p&gt;
Multi-Method Self-Training: Improving Code Generation With Text, And Vice Versa. (arXiv:2307.10633v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10633
&lt;/p&gt;
&lt;p&gt;
&#22810;&#26041;&#27861;&#33258;&#35757;&#32451;&#21487;&#20197;&#36890;&#36807;&#22312;&#19981;&#21516;&#26041;&#27861;&#20043;&#38388;&#35757;&#32451;&#21644;&#29983;&#25104;&#25968;&#25454;&#26469;&#25913;&#21892;&#20195;&#30721;&#29983;&#25104;&#30340;&#24615;&#33021;&#65292;&#20351;&#27169;&#22411;&#26356;&#26131;&#20110;&#20351;&#29992;&#65292;&#24182;&#25552;&#39640;&#30456;&#20851;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26377;&#35768;&#22810;&#35299;&#20915;&#21516;&#19968;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#36825;&#24341;&#20837;&#20102;&#26032;&#39062;&#30340;&#20248;&#28857;&#65288;&#19981;&#21516;&#30340;&#26041;&#27861;&#21487;&#33021;&#23545;&#19981;&#21516;&#30340;&#38382;&#39064;&#26377;&#25928;&#65289;&#65292;&#20197;&#21450;&#32570;&#28857;&#65288;&#29992;&#25143;&#21487;&#33021;&#38590;&#20197;&#30693;&#36947;&#20351;&#29992;&#21738;&#31181;&#26041;&#27861;&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#22810;&#26041;&#27861;&#33258;&#35757;&#32451;&#65288;MMST&#65289;&#26041;&#27861;&#65292;&#20854;&#20013;&#19968;&#31181;&#26041;&#27861;&#26159;&#22312;&#21478;&#19968;&#31181;&#26041;&#27861;&#30340;&#31579;&#36873;&#36755;&#20986;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#22686;&#24378;&#27599;&#31181;&#26041;&#27861;&#30340;&#20248;&#28857;&#24182;&#25913;&#21892;&#23427;&#20204;&#30340;&#32570;&#28857;&#12290;&#20351;&#29992;176B&#21442;&#25968;&#30340;&#35821;&#35328;&#21644;&#20195;&#30721;&#35757;&#32451;&#27169;&#22411;&#65292;&#25105;&#20204;&#35777;&#26126;MMST&#21487;&#20197;1&#65289;&#25913;&#21892;&#24615;&#33021;&#36739;&#24046;&#30340;&#26041;&#27861;&#65288;&#39640;&#36798;30%&#65289;&#65292;&#20351;&#27169;&#22411;&#26356;&#26131;&#20110;&#20351;&#29992;&#65292;2&#65289;&#25913;&#21892;&#24615;&#33021;&#36739;&#22909;&#30340;&#26041;&#27861;&#65288;&#39640;&#36798;32.2%&#65289;&#65292;&#20351;&#27169;&#22411;&#24615;&#33021;&#26356;&#20248;&#31168;&#65292;&#20197;&#21450;3&#65289;&#36890;&#36807;&#25913;&#21892;&#27169;&#22411;&#29983;&#25104;&#35299;&#37322;&#33021;&#21147;&#65292;&#25552;&#39640;&#30456;&#20851;&#20294;&#19981;&#21516;&#20219;&#21153;&#30340;&#24615;&#33021;&#65288;&#39640;&#36798;10.3%&#65289;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#28040;&#34701;&#20998;&#26512;&#65292;&#25506;&#35752;MMST&#30340;&#24037;&#20316;&#21407;&#29702;&#12290;&#25105;&#20204;&#35777;&#26126;MMST&#27604;&#20256;&#32479;&#30340;&#33258;&#35757;&#32451;&#26041;&#27861;&#29983;&#25104;&#26356;&#22810;&#25968;&#25454;&#65292;&#20294;&#24615;&#33021;&#25552;&#21319;&#26356;&#26126;&#26174;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models have many methods for solving the same problem. This introduces novel strengths (different methods may work well for different problems) and weaknesses (it may be difficult for users to know which method to use). In this paper, we introduce Multi-Method Self-Training (MMST), where one method is trained on the filtered outputs of another, allowing us to augment the strengths and ameliorate the weaknesses of each method. Using a 176B parameter model trained on both language and code, we show that MMST can 1) improve the less performant method (up to 30%) making the model easier to use, 2) improve the more performant method (up to 32.2%) making the model more performant, and 3) improve the performance of related but distinct tasks (up to 10.3%) by improving the ability of the model to generate rationales. We then conduct ablation analyses to explore why MMST works. We show that MMST generates more data than traditional self-training, but the improvement in performanc
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#34394;&#20551;&#35780;&#35770;&#65292;&#24182;&#36890;&#36807;&#22312;&#39184;&#39302;&#35780;&#35770;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.10617</link><description>&lt;p&gt;
&#20351;&#29992;&#25991;&#26412;&#20998;&#31867;&#26816;&#27979;&#34394;&#20551;&#35780;&#35770;
&lt;/p&gt;
&lt;p&gt;
Detecting deceptive reviews using text classification. (arXiv:2307.10617v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10617
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#34394;&#20551;&#35780;&#35770;&#65292;&#24182;&#36890;&#36807;&#22312;&#39184;&#39302;&#35780;&#35770;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22312;&#32447;&#35780;&#35770;&#22312;&#25512;&#24191;&#20219;&#20309;&#20135;&#21697;&#25110;&#26381;&#21153;&#26041;&#38754;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#20225;&#19994;&#21487;&#33021;&#20250;&#23884;&#20837;&#34394;&#20551;&#35780;&#35770;&#20197;&#21560;&#24341;&#23458;&#25143;&#36141;&#20080;&#20182;&#20204;&#30340;&#20135;&#21697;&#12290;&#20182;&#20204;&#29978;&#33267;&#21487;&#33021;&#31361;&#20986;&#24378;&#35843;&#33258;&#24049;&#20135;&#21697;&#30340;&#20248;&#28857;&#25110;&#25209;&#35780;&#31454;&#20105;&#23545;&#25163;&#30340;&#20135;&#21697;&#12290;&#24066;&#22330;&#33829;&#38144;&#20154;&#21592;&#12289;&#24191;&#21578;&#21830;&#21644;&#20854;&#20182;&#22312;&#32447;&#21830;&#19994;&#29992;&#25143;&#26377;&#21160;&#26426;&#20026;&#20182;&#20204;&#24819;&#35201;&#25512;&#24191;&#30340;&#20135;&#21697;&#32534;&#20889;&#34394;&#20551;&#30340;&#27491;&#38754;&#35780;&#35770;&#65292;&#25110;&#32773;&#20026;&#20182;&#20204;&#30495;&#27491;&#19981;&#21916;&#27426;&#30340;&#20135;&#21697;&#25552;&#20379;&#34394;&#20551;&#30340;&#36127;&#38754;&#35780;&#35770;&#12290;&#22240;&#27492;&#65292;&#35782;&#21035;&#34394;&#20551;&#35780;&#35770;&#26159;&#19968;&#20010;&#32039;&#36843;&#19988;&#25345;&#32493;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#26412;&#30740;&#31350;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26041;&#27861;&#26469;&#35782;&#21035;&#34394;&#20551;&#35780;&#35770;&#12290;&#35770;&#25991;&#35843;&#26597;&#20102;&#22312;&#19968;&#20010;&#39184;&#39302;&#35780;&#35770;&#30340;&#34394;&#20551;&#24847;&#35265;&#22403;&#22334;&#35821;&#26009;&#24211;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#22810;&#27425;&#23454;&#39564;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;n-gram&#27169;&#22411;&#21644;&#26368;&#22823;&#29305;&#24449;&#26469;&#35782;&#21035;&#34394;&#20551;&#35780;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, online reviews play a vital role for promoting any kind of product or services. Businesses may embed fake reviews in order to attract customers to purchase their products. They may even highlight the benefits of their own product or criticize the competition's product. Marketers, advertisers, and other online business users have incentive to create fake positive reviews for products which they want to promote or give fake negative reviews for products which they really don't like. So now-a-days writing a deceptive review is inevitable thing for promoting their own business or degrading competitor's reputation. Thus, identifying deceptive reviews is an intense and on-going research area. This research paper proposes machine learning model approach to identify deceptive reviews. The paper investigates the performance of the several experiments done on a Deceptive Opinion Spam Corpus dataset of restaurants reviews. We developed a n-gram model and max features to identify 
&lt;/p&gt;</description></item><item><title>&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#26159;&#32852;&#37030;&#23398;&#20064;&#39046;&#22495;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#30740;&#31350;&#26041;&#21521;&#65292;&#28041;&#21450;&#21040;&#25968;&#25454;&#20998;&#24067;&#12289;&#27169;&#22411;&#26550;&#26500;&#12289;&#32593;&#32476;&#29615;&#22659;&#21644;&#30828;&#20214;&#35774;&#22791;&#30340;&#24322;&#36136;&#24615;&#25361;&#25112;&#12290;&#26412;&#25991;&#23545;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#30340;&#30740;&#31350;&#25361;&#25112;&#21644;&#26368;&#26032;&#36827;&#23637;&#36827;&#34892;&#20102;&#32508;&#36848;&#21644;&#20998;&#31867;&#65292;&#20026;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#21442;&#32771;&#12290;</title><link>http://arxiv.org/abs/2307.10616</link><description>&lt;p&gt;
&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#65306;&#29616;&#29366;&#19982;&#30740;&#31350;&#25361;&#25112;&#30340;&#25216;&#26415;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous Federated Learning: State-of-the-art and Research Challenges. (arXiv:2307.10616v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10616
&lt;/p&gt;
&lt;p&gt;
&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#26159;&#32852;&#37030;&#23398;&#20064;&#39046;&#22495;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#30740;&#31350;&#26041;&#21521;&#65292;&#28041;&#21450;&#21040;&#25968;&#25454;&#20998;&#24067;&#12289;&#27169;&#22411;&#26550;&#26500;&#12289;&#32593;&#32476;&#29615;&#22659;&#21644;&#30828;&#20214;&#35774;&#22791;&#30340;&#24322;&#36136;&#24615;&#25361;&#25112;&#12290;&#26412;&#25991;&#23545;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#30340;&#30740;&#31350;&#25361;&#25112;&#21644;&#26368;&#26032;&#36827;&#23637;&#36827;&#34892;&#20102;&#32508;&#36848;&#21644;&#20998;&#31867;&#65292;&#20026;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064; (FL) &#30001;&#20110;&#22312;&#22823;&#35268;&#27169;&#24037;&#19994;&#24212;&#29992;&#20013;&#30340;&#28508;&#22312;&#29992;&#36884;&#32780;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#29616;&#26377;&#30340;&#32852;&#37030;&#23398;&#20064;&#30740;&#31350;&#20027;&#35201;&#38024;&#23545;&#27169;&#22411;&#21516;&#36136;&#30340;&#24773;&#20917;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#30340;&#32852;&#37030;&#23398;&#20064;&#36890;&#24120;&#38754;&#20020;&#21442;&#19982;&#26041;&#20043;&#38388;&#30340;&#25968;&#25454;&#20998;&#24067;&#12289;&#27169;&#22411;&#26550;&#26500;&#12289;&#32593;&#32476;&#29615;&#22659;&#21644;&#30828;&#20214;&#35774;&#22791;&#30340;&#24322;&#36136;&#24615;&#12290;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064; (HFL) &#26356;&#20855;&#25361;&#25112;&#24615;&#65292;&#30456;&#24212;&#30340;&#35299;&#20915;&#26041;&#26696;&#22810;&#26679;&#19988;&#22797;&#26434;&#12290;&#22240;&#27492;&#65292;&#23545;&#36825;&#20010;&#20027;&#39064;&#36827;&#34892;&#20851;&#20110;&#30740;&#31350;&#25361;&#25112;&#21644;&#26368;&#26032;&#36827;&#23637;&#30340;&#31995;&#32479;&#35843;&#26597;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#24635;&#32467;&#20102; HFL &#20013;&#26469;&#33258;&#20116;&#20010;&#26041;&#38754;&#30340;&#21508;&#31181;&#30740;&#31350;&#25361;&#25112;&#65306;&#32479;&#35745;&#24322;&#36136;&#24615;&#12289;&#27169;&#22411;&#24322;&#36136;&#24615;&#12289;&#36890;&#20449;&#24322;&#36136;&#24615;&#12289;&#35774;&#22791;&#24322;&#36136;&#24615;&#21644;&#39069;&#22806;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102; HFL &#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#29616;&#26377; HFL &#26041;&#27861;&#30340;&#26032;&#20998;&#31867;&#27861;&#65292;&#24182;&#23545;&#20854;&#20248;&#32570;&#28857;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) has drawn increasing attention owing to its potential use in large-scale industrial applications. Existing federated learning works mainly focus on model homogeneous settings. However, practical federated learning typically faces the heterogeneity of data distributions, model architectures, network environments, and hardware devices among participant clients. Heterogeneous Federated Learning (HFL) is much more challenging, and corresponding solutions are diverse and complex. Therefore, a systematic survey on this topic about the research challenges and state-of-the-art is essential. In this survey, we firstly summarize the various research challenges in HFL from five aspects: statistical heterogeneity, model heterogeneity, communication heterogeneity, device heterogeneity, and additional challenges. In addition, recent advances in HFL are reviewed and a new taxonomy of existing HFL methods is proposed with an in-depth analysis of their pros and cons. We classify
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#22522;&#20110;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#36125;&#21494;&#26031;&#36229;&#21442;&#25968;&#25935;&#24863;&#24615;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#29289;&#32852;&#32593;&#32593;&#32476;&#23433;&#20840;&#24322;&#24120;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20805;&#20998;&#21033;&#29992;&#20102;&#29289;&#32852;&#32593;&#25968;&#25454;&#30340;&#24322;&#26500;&#24615;&#21644;&#22810;&#26679;&#24615;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2307.10596</link><description>&lt;p&gt;
&#22522;&#20110;&#38598;&#25104;&#23398;&#20064;&#30340;&#29289;&#32852;&#32593;&#32593;&#32476;&#23433;&#20840;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#36890;&#36807;&#36125;&#21494;&#26031;&#36229;&#21442;&#25968;&#25935;&#24863;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Ensemble Learning based Anomaly Detection for IoT Cybersecurity via Bayesian Hyperparameters Sensitivity Analysis. (arXiv:2307.10596v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10596
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#22522;&#20110;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#36125;&#21494;&#26031;&#36229;&#21442;&#25968;&#25935;&#24863;&#24615;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#29289;&#32852;&#32593;&#32593;&#32476;&#23433;&#20840;&#24322;&#24120;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20805;&#20998;&#21033;&#29992;&#20102;&#29289;&#32852;&#32593;&#25968;&#25454;&#30340;&#24322;&#26500;&#24615;&#21644;&#22810;&#26679;&#24615;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#32852;&#32593;&#25972;&#21512;&#20102;&#20840;&#29699;&#25968;&#21313;&#20159;&#26234;&#33021;&#35774;&#22791;&#65292;&#20855;&#22791;&#19982;&#20854;&#20182;&#36830;&#25509;&#35774;&#22791;&#36827;&#34892;&#27807;&#36890;&#30340;&#33021;&#21147;&#65292;&#23454;&#29616;&#20960;&#20046;&#26080;&#38656;&#20154;&#20026;&#24178;&#39044;&#12290;&#29289;&#32852;&#32593;&#33021;&#22815;&#36827;&#34892;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#32858;&#21512;&#21644;&#20998;&#26512;&#65292;&#20174;&#32780;&#25552;&#21319;&#21508;&#20010;&#39046;&#22495;&#30340;&#29983;&#27963;&#36136;&#37327;&#12290;&#29305;&#21035;&#26159;&#65292;&#29289;&#32852;&#32593;&#25910;&#38598;&#30340;&#25968;&#25454;&#23545;&#20110;&#24322;&#24120;&#26816;&#27979;&#38750;&#24120;&#26377;&#29992;&#12290;&#29289;&#32852;&#32593;&#30340;&#24322;&#26500;&#24615;&#26082;&#26159;&#32593;&#32476;&#23433;&#20840;&#30340;&#25361;&#25112;&#65292;&#20063;&#26159;&#26426;&#20250;&#12290;&#20256;&#32479;&#30340;&#32593;&#32476;&#23433;&#20840;&#30417;&#27979;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#23545;&#19981;&#21516;&#31867;&#22411;&#30340;&#25968;&#25454;&#36827;&#34892;&#39044;&#22788;&#29702;&#21644;&#22788;&#29702;&#65292;&#36825;&#23545;&#20110;&#21253;&#21547;&#24322;&#26500;&#29305;&#24449;&#30340;&#25968;&#25454;&#38598;&#21487;&#33021;&#20250;&#23384;&#22312;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#24322;&#26500;&#31867;&#22411;&#30340;&#32593;&#32476;&#35774;&#22791;&#24448;&#24448;&#21487;&#20197;&#25429;&#33719;&#27604;&#21333;&#19968;&#31867;&#22411;&#35774;&#22791;&#35835;&#25968;&#26356;&#22810;&#26679;&#21270;&#30340;&#20449;&#21495;&#65292;&#36825;&#23545;&#20110;&#24322;&#24120;&#26816;&#27979;&#29305;&#21035;&#26377;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#39033;&#20851;&#20110;&#20351;&#29992;&#38598;&#25104;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22686;&#24378;&#29289;&#32852;&#32593;&#32593;&#32476;&#23433;&#20840;&#24322;&#24120;&#26816;&#27979;&#30340;&#20840;&#38754;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Internet of Things (IoT) integrates more than billions of intelligent devices over the globe with the capability of communicating with other connected devices with little to no human intervention. IoT enables data aggregation and analysis on a large scale to improve life quality in many domains. In particular, data collected by IoT contain a tremendous amount of information for anomaly detection. The heterogeneous nature of IoT is both a challenge and an opportunity for cybersecurity. Traditional approaches in cybersecurity monitoring often require different kinds of data pre-processing and handling for various data types, which might be problematic for datasets that contain heterogeneous features. However, heterogeneous types of network devices can often capture a more diverse set of signals than a single type of device readings, which is particularly useful for anomaly detection. In this paper, we present a comprehensive study on using ensemble machine learning methods for enhanc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#20351;&#29992;&#24494;&#32858;&#31867;&#21644;SMOTE&#25216;&#26415;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#30005;&#21160;&#27773;&#36710;&#20805;&#30005;&#20107;&#20214;&#65292;&#20026;&#30005;&#21147;&#36127;&#33655;&#32858;&#21512;&#22120;&#21644;&#30005;&#21147;&#31649;&#29702;&#20154;&#21592;&#25552;&#20379;&#25552;&#20379;&#20805;&#30005;&#31449;&#21644;&#30005;&#21147;&#23481;&#37327;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2307.10588</link><description>&lt;p&gt;
&#39044;&#27979;&#30005;&#21160;&#27773;&#36710;&#20805;&#30005;&#34892;&#20026;&#65306;&#37319;&#29992;&#24494;&#32858;&#31867;&#21644;SMOTE&#25216;&#26415;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Forecasting Battery Electric Vehicle Charging Behavior: A Deep Learning Approach Equipped with Micro-Clustering and SMOTE Techniques. (arXiv:2307.10588v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10588
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#20351;&#29992;&#24494;&#32858;&#31867;&#21644;SMOTE&#25216;&#26415;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#30005;&#21160;&#27773;&#36710;&#20805;&#30005;&#20107;&#20214;&#65292;&#20026;&#30005;&#21147;&#36127;&#33655;&#32858;&#21512;&#22120;&#21644;&#30005;&#21147;&#31649;&#29702;&#20154;&#21592;&#25552;&#20379;&#25552;&#20379;&#20805;&#30005;&#31449;&#21644;&#30005;&#21147;&#23481;&#37327;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#28304;&#31995;&#32479;&#12289;&#27668;&#20505;&#21464;&#21270;&#21644;&#20844;&#20849;&#20581;&#24247;&#26159;&#25512;&#21160;&#20132;&#36890;&#30005;&#27668;&#21270;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#20840;&#29699;&#33539;&#22260;&#20869;&#27491;&#22312;&#25512;&#24191;&#20132;&#36890;&#30005;&#27668;&#21270;&#20197;&#20943;&#23569;&#25490;&#25918;&#12290;&#22240;&#27492;&#65292;&#35768;&#22810;&#27773;&#36710;&#21046;&#36896;&#21830;&#23558;&#24456;&#24555;&#24320;&#22987;&#21482;&#29983;&#20135;&#30005;&#27744;&#30005;&#21160;&#27773;&#36710;&#65288;BEV&#65289;&#12290;&#30001;&#20110;&#27668;&#20505;&#21464;&#21270;&#21644;&#31354;&#27668;&#27745;&#26579;&#30340;&#25285;&#24551;&#65292;&#21152;&#21033;&#31119;&#23612;&#20122;&#30340;BEV&#37319;&#29992;&#29575;&#27491;&#22312;&#19978;&#21319;&#12290;&#34429;&#28982;&#36825;&#23545;&#20110;&#27668;&#20505;&#21644;&#27745;&#26579;&#30446;&#26631;&#26469;&#35828;&#24456;&#22909;&#65292;&#20294;&#26410;&#22949;&#21892;&#31649;&#29702;&#30340;BEV&#20805;&#30005;&#21487;&#33021;&#23548;&#33268;&#20805;&#30005;&#22522;&#30784;&#35774;&#26045;&#19981;&#36275;&#21644;&#20572;&#30005;&#12290;&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24494;&#32858;&#31867;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;MCDNN&#65289;&#65292;&#35813;&#31639;&#27861;&#22312;&#23398;&#20064;BEV&#34892;&#31243;&#21644;&#20805;&#30005;&#25968;&#25454;&#20197;&#39044;&#27979;BEV&#20805;&#30005;&#20107;&#20214;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#65292;&#36825;&#23545;&#20110;&#30005;&#21147;&#36127;&#33655;&#32858;&#21512;&#22120;&#21644;&#30005;&#21147;&#31649;&#29702;&#20154;&#21592;&#26377;&#25928;&#25552;&#20379;&#20805;&#30005;&#31449;&#21644;&#30005;&#21147;&#23481;&#37327;&#30340;&#20449;&#24687;&#33267;&#20851;&#37325;&#35201;&#12290;MCDNN&#20351;&#29992;&#21152;&#21033;&#31119;&#23612;&#20122;&#21457;&#29983;&#30340;&#34892;&#31243;&#21644;&#20805;&#30005;&#30340;&#31283;&#20581;&#25968;&#25454;&#38598;&#36827;&#34892;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
Energy systems, climate change, and public health are among the primary reasons for moving toward electrification in transportation. Transportation electrification is being promoted worldwide to reduce emissions. As a result, many automakers will soon start making only battery electric vehicles (BEVs). BEV adoption rates are rising in California, mainly due to climate change and air pollution concerns. While great for climate and pollution goals, improperly managed BEV charging can lead to insufficient charging infrastructure and power outages. This study develops a novel Micro Clustering Deep Neural Network (MCDNN), an artificial neural network algorithm that is highly effective at learning BEVs trip and charging data to forecast BEV charging events, information that is essential for electricity load aggregators and utility managers to provide charging stations and electricity capacity effectively. The MCDNN is configured using a robust dataset of trips and charges that occurred in Ca
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#21487;&#38752;&#24615;&#30340;&#25972;&#20307;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#35780;&#20272;&#20998;&#24067;&#20869;&#20934;&#30830;&#24615;&#12289;&#20998;&#24067;&#20559;&#31227;&#40065;&#26834;&#24615;&#12289;&#23545;&#25239;&#40065;&#26834;&#24615;&#12289;&#26657;&#20934;&#24615;&#21644;&#36234;&#30028;&#26816;&#27979;&#33021;&#21147;&#31561;&#20116;&#20010;&#20851;&#38190;&#23646;&#24615;&#65292;&#24341;&#20837;&#20102;&#21487;&#38752;&#24615;&#24471;&#20998;&#26469;&#35780;&#20272;&#25972;&#20010;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.10586</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#21487;&#38752;&#24615;&#30340;&#25972;&#20307;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Holistic Assessment of the Reliability of Machine Learning Systems. (arXiv:2307.10586v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10586
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#21487;&#38752;&#24615;&#30340;&#25972;&#20307;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#35780;&#20272;&#20998;&#24067;&#20869;&#20934;&#30830;&#24615;&#12289;&#20998;&#24067;&#20559;&#31227;&#40065;&#26834;&#24615;&#12289;&#23545;&#25239;&#40065;&#26834;&#24615;&#12289;&#26657;&#20934;&#24615;&#21644;&#36234;&#30028;&#26816;&#27979;&#33021;&#21147;&#31561;&#20116;&#20010;&#20851;&#38190;&#23646;&#24615;&#65292;&#24341;&#20837;&#20102;&#21487;&#38752;&#24615;&#24471;&#20998;&#26469;&#35780;&#20272;&#25972;&#20010;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#36234;&#26469;&#36234;&#22810;&#22320;&#28183;&#36879;&#21040;&#21307;&#30103;&#20445;&#20581;&#12289;&#20132;&#36890;&#36816;&#36755;&#12289;&#20891;&#20107;&#21644;&#22269;&#23478;&#23433;&#20840;&#31561;&#39640;&#39118;&#38505;&#39046;&#22495;&#65292;&#20154;&#20204;&#23545;&#20854;&#21487;&#38752;&#24615;&#20135;&#29983;&#20102;&#25285;&#24551;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#23545;&#25239;&#25915;&#20987;&#25110;&#29615;&#22659;&#21464;&#21270;&#65292;&#36825;&#20123;&#31995;&#32479;&#30340;&#24615;&#33021;&#21487;&#33021;&#26174;&#33879;&#38477;&#20302;&#65292;&#23548;&#33268;&#39044;&#27979;&#36807;&#20110;&#33258;&#20449;&#12289;&#26080;&#27861;&#26816;&#27979;&#36755;&#20837;&#25925;&#38556;&#20197;&#21450;&#22312;&#24847;&#22806;&#22330;&#26223;&#20013;&#26080;&#27861;&#27867;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#21487;&#38752;&#24615;&#30340;&#25972;&#20307;&#35780;&#20272;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#35780;&#20272;&#20102;&#20116;&#20010;&#20851;&#38190;&#23646;&#24615;&#65306;&#20998;&#24067;&#20869;&#20934;&#30830;&#24615;&#12289;&#20998;&#24067;&#20559;&#31227;&#40065;&#26834;&#24615;&#12289;&#23545;&#25239;&#40065;&#26834;&#24615;&#12289;&#26657;&#20934;&#24615;&#21644;&#36234;&#30028;&#26816;&#27979;&#33021;&#21147;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#21487;&#38752;&#24615;&#24471;&#20998;&#26469;&#35780;&#20272;&#25972;&#20010;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#12290;&#20026;&#20102;&#25552;&#20379;&#19981;&#21516;&#31639;&#27861;&#26041;&#27861;&#24615;&#33021;&#30340;&#35265;&#35299;&#65292;&#25105;&#20204;&#35782;&#21035;&#21644;&#20998;&#31867;&#20102;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#65292;&#28982;&#21518;&#20351;&#29992;&#30495;&#23454;&#19990;&#30028;&#30340;&#20219;&#21153;&#23545;&#20854;&#20013;&#30340;&#19968;&#20123;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
As machine learning (ML) systems increasingly permeate high-stakes settings such as healthcare, transportation, military, and national security, concerns regarding their reliability have emerged. Despite notable progress, the performance of these systems can significantly diminish due to adversarial attacks or environmental changes, leading to overconfident predictions, failures to detect input faults, and an inability to generalize in unexpected scenarios. This paper proposes a holistic assessment methodology for the reliability of ML systems. Our framework evaluates five key properties: in-distribution accuracy, distribution-shift robustness, adversarial robustness, calibration, and out-of-distribution detection. A reliability score is also introduced and used to assess the overall system reliability. To provide insights into the performance of different algorithmic approaches, we identify and categorize state-of-the-art techniques, then evaluate a selection on real-world tasks using
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#23884;&#20837;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#30340;&#39640;&#32423;&#28023;&#19978;&#28023;&#38654;&#39044;&#27979;&#26041;&#27861;&#65292;&#22312;&#35299;&#26512;&#39537;&#21160;&#22240;&#32032;&#21644;&#22788;&#29702;&#19981;&#24179;&#34913;&#25968;&#25454;&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#21319;&#20102;&#28023;&#38654;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#39044;&#27979;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.10580</link><description>&lt;p&gt;
&#20013;&#22269;&#28023;&#19978;&#28023;&#38654;&#39044;&#27979;&#30340;&#26234;&#33021;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Intelligent model for offshore China sea fog forecasting. (arXiv:2307.10580v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10580
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#23884;&#20837;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#30340;&#39640;&#32423;&#28023;&#19978;&#28023;&#38654;&#39044;&#27979;&#26041;&#27861;&#65292;&#22312;&#35299;&#26512;&#39537;&#21160;&#22240;&#32032;&#21644;&#22788;&#29702;&#19981;&#24179;&#34913;&#25968;&#25454;&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#21319;&#20102;&#28023;&#38654;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#21644;&#21450;&#26102;&#22320;&#39044;&#27979;&#28023;&#19978;&#28023;&#38654;&#23545;&#26377;&#25928;&#31649;&#29702;&#28023;&#19978;&#21644;&#27839;&#23736;&#32463;&#27982;&#27963;&#21160;&#38750;&#24120;&#37325;&#35201;&#12290;&#37492;&#20110;&#28023;&#19978;&#28023;&#38654;&#30340;&#22797;&#26434;&#24615;&#21644;&#22266;&#26377;&#21464;&#24322;&#24615;&#65292;&#20256;&#32479;&#30340;&#25968;&#20540;&#21644;&#32479;&#35745;&#39044;&#27979;&#26041;&#27861;&#24448;&#24448;&#19981;&#36275;&#20197;&#24212;&#23545;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#39640;&#32423;&#28023;&#19978;&#28023;&#38654;&#39044;&#27979;&#26041;&#27861;&#65292;&#23558;&#20854;&#23884;&#20837;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#20013;&#65292;&#24182;&#20197;&#38271;&#27743;&#21475;&#27839;&#28023;&#22320;&#21306;&#20026;&#26696;&#20363;&#30740;&#31350;&#12290;&#22312;&#35757;&#32451;&#25105;&#20204;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20043;&#21069;&#65292;&#25105;&#20204;&#37319;&#29992;&#26102;&#28382;&#30456;&#20851;&#20998;&#26512;&#25216;&#26415;&#26469;&#35782;&#21035;&#20851;&#38190;&#39044;&#27979;&#22240;&#23376;&#65292;&#24182;&#35299;&#26512;&#39537;&#21160;&#28023;&#19978;&#28023;&#38654;&#21457;&#29983;&#30340;&#22522;&#26412;&#26426;&#21046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#38598;&#25104;&#23398;&#20064;&#21644;&#28966;&#28857;&#25439;&#22833;&#20989;&#25968;&#26469;&#35299;&#20915;&#19981;&#24179;&#34913;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#21319;&#25105;&#20204;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;&#21253;&#25324;&#27668;&#35937;&#31449;&#35266;&#27979;&#25968;&#25454;&#21644;&#21382;&#21490;&#25968;&#25454;&#30340;&#20840;&#38754;&#25968;&#25454;&#38598;&#36827;&#34892;&#24615;&#33021;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate and timely prediction of sea fog is very important for effectively managing maritime and coastal economic activities. Given the intricate nature and inherent variability of sea fog, traditional numerical and statistical forecasting methods are often proven inadequate. This study aims to develop an advanced sea fog forecasting method embedded in a numerical weather prediction model using the Yangtze River Estuary (YRE) coastal area as a case study. Prior to training our machine learning model, we employ a time-lagged correlation analysis technique to identify key predictors and decipher the underlying mechanisms driving sea fog occurrence. In addition, we implement ensemble learning and a focal loss function to address the issue of imbalanced data, thereby enhancing the predictive ability of our model. To verify the accuracy of our method, we evaluate its performance using a comprehensive dataset spanning one year, which encompasses both weather station observations and histori
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22810;&#30446;&#26631;&#32852;&#37030;&#23398;&#20064;&#26469;&#35843;&#20248;SecureBoost&#36229;&#21442;&#25968;&#30340;&#26041;&#27861;&#65292;&#20197;&#25214;&#21040;&#22312;&#25928;&#29992;&#12289;&#25928;&#29575;&#21644;&#38544;&#31169;&#20043;&#38388;&#26368;&#20339;&#24179;&#34913;&#30340;&#19968;&#32452;&#36229;&#21442;&#25968;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2307.10579</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#30446;&#26631;&#32852;&#37030;&#23398;&#20064;&#23545;SecureBoost&#36229;&#21442;&#25968;&#36827;&#34892;&#35843;&#20248;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SecureBoost Hyperparameter Tuning via Multi-Objective Federated Learning. (arXiv:2307.10579v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10579
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22810;&#30446;&#26631;&#32852;&#37030;&#23398;&#20064;&#26469;&#35843;&#20248;SecureBoost&#36229;&#21442;&#25968;&#30340;&#26041;&#27861;&#65292;&#20197;&#25214;&#21040;&#22312;&#25928;&#29992;&#12289;&#25928;&#29575;&#21644;&#38544;&#31169;&#20043;&#38388;&#26368;&#20339;&#24179;&#34913;&#30340;&#19968;&#32452;&#36229;&#21442;&#25968;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
SecureBoost&#26159;&#19968;&#31181;&#21033;&#29992;&#21516;&#24577;&#21152;&#23494;&#20445;&#25252;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#20013;&#25968;&#25454;&#38544;&#31169;&#30340;&#26641;&#25552;&#21319;&#31639;&#27861;&#12290;&#30001;&#20110;&#20854;&#21487;&#35299;&#37322;&#24615;&#12289;&#25928;&#26524;&#21644;&#38544;&#31169;&#20445;&#25252;&#33021;&#21147;&#65292;&#22312;&#37329;&#34701;&#21644;&#21307;&#30103;&#20445;&#20581;&#31561;&#39046;&#22495;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;SecureBoost&#23384;&#22312;&#35745;&#31639;&#22797;&#26434;&#24615;&#39640;&#21644;&#26631;&#31614;&#27844;&#28431;&#39118;&#38505;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#20805;&#20998;&#21457;&#25381;SecureBoost&#30340;&#28508;&#21147;&#65292;&#38656;&#35201;&#20180;&#32454;&#36873;&#25321;SecureBoost&#30340;&#36229;&#21442;&#25968;&#65292;&#20197;&#22312;&#25928;&#29992;&#12289;&#25928;&#29575;&#21644;&#38544;&#31169;&#20043;&#38388;&#36798;&#21040;&#26368;&#20339;&#24179;&#34913;&#12290;&#29616;&#26377;&#26041;&#27861;&#35201;&#20040;&#32463;&#39564;&#24615;&#22320;&#35774;&#32622;&#36229;&#21442;&#25968;&#65292;&#35201;&#20040;&#21551;&#21457;&#24335;&#22320;&#35774;&#32622;&#36229;&#21442;&#25968;&#65292;&#36828;&#26410;&#36798;&#21040;&#26368;&#20248;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32422;&#26463;&#22810;&#30446;&#26631;SecureBoost (CMOSB) &#31639;&#27861;&#65292;&#20197;&#23547;&#25214;&#27599;&#20010;&#35299;&#37117;&#26159;&#22312;&#25928;&#29992;&#25439;&#22833;&#12289;&#35757;&#32451;&#25104;&#26412;&#21644;&#38544;&#31169;&#27844;&#28431;&#20043;&#38388;&#23454;&#29616;&#26368;&#20339;&#26435;&#34913;&#30340;&#19968;&#32452;&#36229;&#21442;&#25968;&#30340;Pareto&#26368;&#20248;&#35299;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19977;&#20010;&#30446;&#26631;&#30340;&#24230;&#37327;&#26041;&#27861;&#12290;&#29305;&#21035;&#26159;&#65292;&#38544;&#31169;&#27844;&#28431;&#26159;&#29992;... (&#27492;&#22788;&#30465;&#30053;)
&lt;/p&gt;
&lt;p&gt;
SecureBoost is a tree-boosting algorithm leveraging homomorphic encryption to protect data privacy in vertical federated learning setting. It is widely used in fields such as finance and healthcare due to its interpretability, effectiveness, and privacy-preserving capability. However, SecureBoost suffers from high computational complexity and risk of label leakage. To harness the full potential of SecureBoost, hyperparameters of SecureBoost should be carefully chosen to strike an optimal balance between utility, efficiency, and privacy. Existing methods either set hyperparameters empirically or heuristically, which are far from optimal. To fill this gap, we propose a Constrained Multi-Objective SecureBoost (CMOSB) algorithm to find Pareto optimal solutions that each solution is a set of hyperparameters achieving optimal tradeoff between utility loss, training cost, and privacy leakage. We design measurements of the three objectives. In particular, the privacy leakage is measured using 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;&#21407;&#22411;&#27491;&#21017;&#21270;&#31574;&#30053;&#26469;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#24322;&#36136;&#25968;&#25454;&#20998;&#24067;&#30340;&#38382;&#39064;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;MNIST&#21644;Fashion-MNIST&#25968;&#25454;&#38598;&#19978;&#30456;&#27604;&#22522;&#20934;&#27169;&#22411;FedAvg&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20998;&#21035;&#25552;&#39640;&#20102;3.3%&#21644;8.9%&#30340;&#24179;&#22343;&#27979;&#35797;&#20934;&#30830;&#29575;&#65292;&#24182;&#19988;&#22312;&#24322;&#26500;&#35774;&#32622;&#19979;&#20855;&#26377;&#24555;&#36895;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2307.10575</link><description>&lt;p&gt;
&#20351;&#29992;&#21407;&#22411;&#27491;&#21017;&#21270;&#25552;&#21319;&#32852;&#37030;&#23398;&#20064;&#30340;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
Boosting Federated Learning Convergence with Prototype Regularization. (arXiv:2307.10575v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10575
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;&#21407;&#22411;&#27491;&#21017;&#21270;&#31574;&#30053;&#26469;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#24322;&#36136;&#25968;&#25454;&#20998;&#24067;&#30340;&#38382;&#39064;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;MNIST&#21644;Fashion-MNIST&#25968;&#25454;&#38598;&#19978;&#30456;&#27604;&#22522;&#20934;&#27169;&#22411;FedAvg&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20998;&#21035;&#25552;&#39640;&#20102;3.3%&#21644;8.9%&#30340;&#24179;&#22343;&#27979;&#35797;&#20934;&#30830;&#29575;&#65292;&#24182;&#19988;&#22312;&#24322;&#26500;&#35774;&#32622;&#19979;&#20855;&#26377;&#24555;&#36895;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#35201;&#27714;&#23458;&#25143;&#31471;&#22312;&#19981;&#27844;&#38706;&#26412;&#22320;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#19982;&#36793;&#32536;&#26381;&#21153;&#22120;&#20849;&#21516;&#35757;&#32451;&#20849;&#20139;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#24322;&#26500;&#25968;&#25454;&#20998;&#24067;&#24448;&#24448;&#23548;&#33268;&#27169;&#22411;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#21407;&#22411;&#30340;&#27491;&#21017;&#21270;&#31574;&#30053;&#26469;&#35299;&#20915;&#25968;&#25454;&#20998;&#24067;&#30340;&#24322;&#36136;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#27491;&#21017;&#21270;&#36807;&#31243;&#28041;&#21450;&#26381;&#21153;&#22120;&#20174;&#20998;&#24067;&#24335;&#23458;&#25143;&#31471;&#32858;&#21512;&#26412;&#22320;&#21407;&#22411;&#20197;&#29983;&#25104;&#20840;&#23616;&#21407;&#22411;&#65292;&#28982;&#21518;&#23558;&#20854;&#21457;&#36865;&#22238;&#20010;&#20307;&#23458;&#25143;&#31471;&#20197;&#25351;&#23548;&#20854;&#26412;&#22320;&#35757;&#32451;&#12290;&#22312;MNIST&#21644;Fashion-MNIST&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26368;&#27969;&#34892;&#30340;&#22522;&#20934;FedAvg&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#25552;&#35758;&#20998;&#21035;&#22312;&#24179;&#22343;&#27979;&#35797;&#20934;&#30830;&#29575;&#19978;&#23454;&#29616;&#20102;3.3%&#21644;8.9%&#30340;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24322;&#26500;&#29615;&#22659;&#20013;&#20855;&#26377;&#24555;&#36895;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
As a distributed machine learning technique, federated learning (FL) requires clients to collaboratively train a shared model with an edge server without leaking their local data. However, the heterogeneous data distribution among clients often leads to a decrease in model performance. To tackle this issue, this paper introduces a prototype-based regularization strategy to address the heterogeneity in the data distribution. Specifically, the regularization process involves the server aggregating local prototypes from distributed clients to generate a global prototype, which is then sent back to the individual clients to guide their local training. The experimental results on MNIST and Fashion-MNIST show that our proposal achieves improvements of 3.3% and 8.9% in average test accuracy, respectively, compared to the most popular baseline FedAvg. Furthermore, our approach has a fast convergence rate in heterogeneous settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#27450;&#39575;&#24615;&#23545;&#40784;&#30417;&#27979;&#36825;&#19968;&#26032;&#26041;&#21521;&#65292;&#26088;&#22312;&#25506;&#35752;&#22823;&#22411;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#34920;&#38754;&#19978;&#34920;&#29616;&#27491;&#24120;&#65292;&#21364;&#26263;&#20013;&#36827;&#34892;&#38544;&#34255;&#34892;&#20026;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#30740;&#31350;&#26426;&#20250;&#12290;</title><link>http://arxiv.org/abs/2307.10569</link><description>&lt;p&gt;
&#27450;&#39575;&#24615;&#23545;&#40784;&#30417;&#27979;
&lt;/p&gt;
&lt;p&gt;
Deceptive Alignment Monitoring. (arXiv:2307.10569v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10569
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#27450;&#39575;&#24615;&#23545;&#40784;&#30417;&#27979;&#36825;&#19968;&#26032;&#26041;&#21521;&#65292;&#26088;&#22312;&#25506;&#35752;&#22823;&#22411;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#34920;&#38754;&#19978;&#34920;&#29616;&#27491;&#24120;&#65292;&#21364;&#26263;&#20013;&#36827;&#34892;&#38544;&#34255;&#34892;&#20026;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#30740;&#31350;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#33021;&#21147;&#19981;&#26029;&#22686;&#38271;&#65292;&#20197;&#21450;&#23545;&#36825;&#20123;&#27169;&#22411;&#30340;&#33258;&#27835;&#26435;&#19981;&#26029;&#25193;&#22823;&#65292;&#19968;&#20010;&#26032;&#30340;&#23545;&#25163;&#20986;&#29616;&#20102;&#65306;&#27169;&#22411;&#26412;&#36523;&#12290;&#19968;&#20010;&#27169;&#22411;&#30475;&#20284;&#21512;&#29702;&#22320;&#34892;&#20026;&#65292;&#21364;&#26263;&#20013;&#12289;&#24494;&#22937;&#22320;&#20462;&#25913;&#20854;&#34892;&#20026;&#20197;&#36798;&#21040;&#21035;&#30340;&#30446;&#30340;&#30340;&#23041;&#32961;&#65292;&#36890;&#24120;&#22312;AI&#23433;&#20840;&#19982;&#23545;&#40784;&#31038;&#21306;&#20013;&#34987;&#31216;&#20026;&#27450;&#39575;&#24615;&#23545;&#40784;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#26032;&#26041;&#21521;&#31216;&#20026;&#27450;&#39575;&#24615;&#23545;&#40784;&#30417;&#27979;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#26426;&#22120;&#23398;&#20064;&#19981;&#21516;&#23376;&#39046;&#22495;&#20013;&#30340;&#26032;&#20852;&#26041;&#21521;&#65292;&#25105;&#20204;&#35748;&#20026;&#22312;&#19981;&#20037;&#30340;&#23558;&#26469;&#23545;&#27450;&#39575;&#24615;&#23545;&#40784;&#30417;&#27979;&#20250;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#19988;&#32039;&#23494;&#30456;&#20851;&#65292;&#24182;&#19988;&#25105;&#20204;&#35748;&#20026;&#36825;&#20123;&#39046;&#22495;&#30340;&#36827;&#27493;&#26082;&#25552;&#20986;&#20102;&#38271;&#26399;&#25361;&#25112;&#65292;&#20063;&#24102;&#26469;&#20102;&#26032;&#30340;&#30740;&#31350;&#26426;&#20250;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21628;&#21505;&#23545;&#25239;&#24615;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#26356;&#22810;&#22320;&#21442;&#19982;&#36825;&#20123;&#26032;&#20852;&#26041;&#21521;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the capabilities of large machine learning models continue to grow, and as the autonomy afforded to such models continues to expand, the spectre of a new adversary looms: the models themselves. The threat that a model might behave in a seemingly reasonable manner, while secretly and subtly modifying its behavior for ulterior reasons is often referred to as deceptive alignment in the AI Safety &amp; Alignment communities. Consequently, we call this new direction Deceptive Alignment Monitoring. In this work, we identify emerging directions in diverse machine learning subfields that we believe will become increasingly important and intertwined in the near future for deceptive alignment monitoring, and we argue that advances in these fields present both long-term challenges and new research opportunities. We conclude by advocating for greater involvement by the adversarial machine learning community in these emerging directions.
&lt;/p&gt;</description></item><item><title>FACADE&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#27010;&#29575;&#21644;&#20960;&#20309;&#26694;&#26550;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#26816;&#27979;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#26426;&#29702;&#24322;&#24120;&#65292;&#24182;&#25552;&#20379;&#20851;&#38190;&#30340;&#27934;&#23519;&#21147;&#21644;&#24378;&#22823;&#24037;&#20855;&#65292;&#20197;&#25581;&#31034;&#21644;&#23545;&#25239;&#23545;&#25239;&#25915;&#20987;&#65292;&#24182;&#22312;&#23454;&#38469;&#37096;&#32626;&#29615;&#22659;&#20013;&#23637;&#31034;&#20102;&#26377;&#21069;&#36884;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.10563</link><description>&lt;p&gt;
FACADE&#65306;&#19968;&#31181;&#29992;&#20110;&#23545;&#25239;&#30005;&#36335;&#24322;&#24120;&#26816;&#27979;&#21644;&#35780;&#20272;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
FACADE: A Framework for Adversarial Circuit Anomaly Detection and Evaluation. (arXiv:2307.10563v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10563
&lt;/p&gt;
&lt;p&gt;
FACADE&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#27010;&#29575;&#21644;&#20960;&#20309;&#26694;&#26550;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#26816;&#27979;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#26426;&#29702;&#24322;&#24120;&#65292;&#24182;&#25552;&#20379;&#20851;&#38190;&#30340;&#27934;&#23519;&#21147;&#21644;&#24378;&#22823;&#24037;&#20855;&#65292;&#20197;&#25581;&#31034;&#21644;&#23545;&#25239;&#23545;&#25239;&#25915;&#20987;&#65292;&#24182;&#22312;&#23454;&#38469;&#37096;&#32626;&#29615;&#22659;&#20013;&#23637;&#31034;&#20102;&#26377;&#21069;&#36884;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;FACADE&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#27010;&#29575;&#21644;&#20960;&#20309;&#26694;&#26550;&#65292;&#26088;&#22312;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#36827;&#34892;&#26080;&#30417;&#30563;&#30340;&#26426;&#29702;&#24322;&#24120;&#26816;&#27979;&#12290;&#20854;&#20027;&#35201;&#30446;&#26631;&#26159;&#25512;&#36827;&#23545;&#25239;&#25915;&#20987;&#30340;&#29702;&#35299;&#21644;&#20943;&#36731;&#12290;FACADE&#26088;&#22312;&#29983;&#25104;&#30005;&#36335;&#19978;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#36825;&#20026;&#20266;&#31867;&#30340;&#27969;&#24418;&#29305;&#24615;&#21464;&#21270;&#20197;&#21450;&#28608;&#27963;&#31354;&#38388;&#20013;&#39640;&#32500;&#27169;&#24335;&#30340;&#36129;&#29486;&#25552;&#20379;&#20102;&#20851;&#38190;&#30340;&#27934;&#23519;&#21147;&#65292;&#20174;&#32780;&#20026;&#25581;&#31034;&#21644;&#23545;&#25239;&#23545;&#25239;&#25915;&#20987;&#25552;&#20379;&#20102;&#24378;&#22823;&#24037;&#20855;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#22686;&#24378;&#21487;&#25193;&#23637;&#27169;&#22411;&#30417;&#25511;&#65292;&#24182;&#22312;&#23454;&#38469;&#37096;&#32626;&#29615;&#22659;&#20013;&#23637;&#31034;&#20102;&#26377;&#21069;&#36884;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present FACADE, a novel probabilistic and geometric framework designed for unsupervised mechanistic anomaly detection in deep neural networks. Its primary goal is advancing the understanding and mitigation of adversarial attacks. FACADE aims to generate probabilistic distributions over circuits, which provide critical insights to their contribution to changes in the manifold properties of pseudo-classes, or high-dimensional modes in activation space, yielding a powerful tool for uncovering and combating adversarial attacks. Our approach seeks to improve model robustness, enhance scalable model oversight, and demonstrates promising applications in real-world deployment settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20849;&#20139;&#23545;&#25239;&#26679;&#26412;&#26469;&#32431;&#21270;&#26377;&#38376;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#35299;&#20915;&#20102;&#19968;&#20010;&#26032;&#30340;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#65292;&#20174;&#32780;&#20943;&#36731;&#21518;&#38376;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2307.10562</link><description>&lt;p&gt;
&#20849;&#20139;&#23545;&#25239;&#24615;&#36951;&#24536;&#65306;&#36890;&#36807;&#36951;&#24536;&#20849;&#20139;&#23545;&#25239;&#26679;&#26412;&#26469;&#20943;&#36731;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Shared Adversarial Unlearning: Backdoor Mitigation by Unlearning Shared Adversarial Examples. (arXiv:2307.10562v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10562
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20849;&#20139;&#23545;&#25239;&#26679;&#26412;&#26469;&#32431;&#21270;&#26377;&#38376;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#35299;&#20915;&#20102;&#19968;&#20010;&#26032;&#30340;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#65292;&#20174;&#32780;&#20943;&#36731;&#21518;&#38376;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#38376;&#25915;&#20987;&#26159;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20005;&#37325;&#23433;&#20840;&#23041;&#32961;&#65292;&#25932;&#23545;&#26041;&#21487;&#20197;&#21521;&#35757;&#32451;&#38598;&#20013;&#27880;&#20837;&#26377;&#27602;&#26679;&#26412;&#65292;&#23548;&#33268;&#19968;&#20010;&#39044;&#27979;&#21463;&#29305;&#23450;&#35302;&#21457;&#22120;&#28608;&#27963;&#30340;&#26377;&#27602;&#26679;&#26412;&#21040;&#29305;&#23450;&#30446;&#26631;&#31867;&#30340;&#26377;&#38376;&#27169;&#22411;&#65292;&#32780;&#22312;&#26080;&#23475;&#26679;&#26412;&#19978;&#34920;&#29616;&#27491;&#24120;&#12290;&#26412;&#25991;&#36890;&#36807;&#24314;&#31435;&#21518;&#38376;&#39118;&#38505;&#21644;&#23545;&#25239;&#39118;&#38505;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#25512;&#23548;&#20986;&#19968;&#31181;&#26032;&#30340;&#21518;&#38376;&#39118;&#38505;&#19978;&#30028;&#65292;&#20027;&#35201;&#25429;&#25417;&#20102;&#21518;&#38376;&#27169;&#22411;&#19982;&#32431;&#20928;&#27169;&#22411;&#20043;&#38388;&#20849;&#20139;&#30340;&#23545;&#25239;&#26679;&#26412;&#30340;&#39118;&#38505;&#12290;&#36825;&#20010;&#19978;&#30028;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23545;&#25239;&#35757;&#32451;&#25216;&#26415;&#20943;&#36731;&#21518;&#38376;&#25915;&#20987;&#30340;&#26032;&#30340;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20849;&#20139;&#23545;&#25239;&#24615;&#36951;&#24536;&#65288;SAU&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;SAU&#39318;&#20808;&#29983;&#25104;&#20849;&#20139;&#23545;&#25239;&#26679;&#26412;&#65292;&#28982;&#21518;&#36890;&#36807;&#36951;&#24536;&#36825;&#20123;&#29983;&#25104;&#30340;&#20849;&#20139;&#23545;&#25239;&#26679;&#26412;&#65292;&#20351;&#20854;&#33021;&#22815;&#34987;&#32431;&#20928;&#27169;&#22411;&#27491;&#30830;&#20998;&#31867;&#21644;/&#25110;&#21306;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Backdoor attacks are serious security threats to machine learning models where an adversary can inject poisoned samples into the training set, causing a backdoored model which predicts poisoned samples with particular triggers to particular target classes, while behaving normally on benign samples. In this paper, we explore the task of purifying a backdoored model using a small clean dataset. By establishing the connection between backdoor risk and adversarial risk, we derive a novel upper bound for backdoor risk, which mainly captures the risk on the shared adversarial examples (SAEs) between the backdoored model and the purified model. This upper bound further suggests a novel bi-level optimization problem for mitigating backdoor using adversarial training techniques. To solve it, we propose Shared Adversarial Unlearning (SAU). Specifically, SAU first generates SAEs, and then, unlearns the generated SAEs such that they are either correctly classified by the purified model and/or diff
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#21518;&#21464;&#20998;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#36890;&#36807;&#23558;&#21487;&#35843;&#21442;&#25968;&#20174;&#37327;&#23376;&#35745;&#31639;&#26426;&#36716;&#31227;&#21040;&#32463;&#20856;&#35745;&#31639;&#26426;&#65292;&#24182;&#37319;&#29992;&#38598;&#21512;&#31574;&#30053;&#26469;&#20248;&#21270;&#37327;&#23376;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#28151;&#21512;&#37327;&#23376;-&#32463;&#20856;&#35745;&#31639;&#20013;&#30340;&#36139;&#30240;&#39640;&#21407;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#21518;&#21464;&#20998;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2307.10560</link><description>&lt;p&gt;
&#21518;&#21464;&#20998;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Post-variational quantum neural networks. (arXiv:2307.10560v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10560
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#21518;&#21464;&#20998;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#36890;&#36807;&#23558;&#21487;&#35843;&#21442;&#25968;&#20174;&#37327;&#23376;&#35745;&#31639;&#26426;&#36716;&#31227;&#21040;&#32463;&#20856;&#35745;&#31639;&#26426;&#65292;&#24182;&#37319;&#29992;&#38598;&#21512;&#31574;&#30053;&#26469;&#20248;&#21270;&#37327;&#23376;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#28151;&#21512;&#37327;&#23376;-&#32463;&#20856;&#35745;&#31639;&#20013;&#30340;&#36139;&#30240;&#39640;&#21407;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#21518;&#21464;&#20998;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#35745;&#31639;&#26377;&#26395;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#32463;&#20856;&#36229;&#32423;&#35745;&#31639;&#26426;&#25552;&#20379;&#26356;&#22823;&#30340;&#35745;&#31639;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#30828;&#20214;&#36824;&#19981;&#36275;&#20197;&#25191;&#34892;&#23481;&#38169;&#30340;&#37327;&#23376;&#31639;&#27861;&#12290;&#20351;&#29992;&#28151;&#21512;&#37327;&#23376;-&#32463;&#20856;&#35745;&#31639;&#21644;&#21464;&#20998;&#31639;&#27861;&#30340;&#26367;&#20195;&#26041;&#27861;&#21487;&#33021;&#20250;&#20986;&#29616;&#36139;&#30240;&#39640;&#21407;&#38382;&#39064;&#65292;&#23548;&#33268;&#26799;&#24230;&#20248;&#21270;&#25216;&#26415;&#25910;&#25947;&#32531;&#24930;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;"&#21518;&#21464;&#20998;&#31574;&#30053;"&#65292;&#35813;&#31574;&#30053;&#23558;&#21487;&#35843;&#21442;&#25968;&#20174;&#37327;&#23376;&#35745;&#31639;&#26426;&#36716;&#31227;&#21040;&#32463;&#20856;&#35745;&#31639;&#26426;&#65292;&#36873;&#25321;&#22312;&#20248;&#21270;&#37327;&#23376;&#27169;&#22411;&#26102;&#37319;&#29992;&#38598;&#21512;&#31574;&#30053;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#26500;&#24314;&#20010;&#20307;&#37327;&#23376;&#30005;&#36335;&#30340;&#21508;&#31181;&#31574;&#30053;&#21644;&#35774;&#35745;&#21407;&#21017;&#65292;&#20854;&#20013;&#24471;&#21040;&#30340;&#38598;&#21512;&#21487;&#20197;&#36890;&#36807;&#20984;&#32534;&#31243;&#36827;&#34892;&#20248;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#21518;&#21464;&#20998;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#30340;&#26550;&#26500;&#35774;&#35745;&#65292;&#24182;&#20998;&#26512;&#20102;&#35813;&#31070;&#32463;&#32593;&#32476;&#20013;&#20272;&#35745;&#35823;&#24046;&#30340;&#20256;&#25773;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;...
&lt;/p&gt;
&lt;p&gt;
Quantum computing has the potential to provide substantial computational advantages over current state-of-the-art classical supercomputers. However, current hardware is not advanced enough to execute fault-tolerant quantum algorithms. An alternative of using hybrid quantum-classical computing with variational algorithms can exhibit barren plateau issues, causing slow convergence of gradient-based optimization techniques. In this paper, we discuss "post-variational strategies", which shift tunable parameters from the quantum computer to the classical computer, opting for ensemble strategies when optimizing quantum models. We discuss various strategies and design principles for constructing individual quantum circuits, where the resulting ensembles can be optimized with convex programming. Further, we discuss architectural designs of post-variational quantum neural networks and analyze the propagation of estimation errors throughout such neural networks. Lastly, we show that our algorith
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21512;&#35268;&#21270;&#21160;&#24577;&#22270;&#23398;&#20064;&#26469;&#39044;&#27979;&#31354;&#20013;&#20132;&#36890;&#31649;&#21046;&#21592;&#24037;&#20316;&#36127;&#33655;&#27700;&#24179;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#36864;&#20241;&#31354;&#20013;&#20132;&#36890;&#31649;&#21046;&#21592;&#36827;&#34892;&#20154;&#26426;&#20132;&#20114;&#27169;&#25311;&#65292;&#21033;&#29992;&#31354;&#20013;&#20132;&#36890;&#25968;&#25454;&#21644;&#24037;&#20316;&#36127;&#33655;&#26631;&#31614;&#36827;&#34892;&#39044;&#27979;&#21644;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2307.10559</link><description>&lt;p&gt;
&#20351;&#29992;&#21512;&#35268;&#21270;&#21160;&#24577;&#22270;&#23398;&#20064;&#39044;&#27979;&#31354;&#20013;&#20132;&#36890;&#31649;&#21046;&#21592;&#24037;&#20316;&#36127;&#33655;&#27700;&#24179;
&lt;/p&gt;
&lt;p&gt;
Air Traffic Controller Workload Level Prediction using Conformalized Dynamical Graph Learning. (arXiv:2307.10559v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10559
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21512;&#35268;&#21270;&#21160;&#24577;&#22270;&#23398;&#20064;&#26469;&#39044;&#27979;&#31354;&#20013;&#20132;&#36890;&#31649;&#21046;&#21592;&#24037;&#20316;&#36127;&#33655;&#27700;&#24179;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#36864;&#20241;&#31354;&#20013;&#20132;&#36890;&#31649;&#21046;&#21592;&#36827;&#34892;&#20154;&#26426;&#20132;&#20114;&#27169;&#25311;&#65292;&#21033;&#29992;&#31354;&#20013;&#20132;&#36890;&#25968;&#25454;&#21644;&#24037;&#20316;&#36127;&#33655;&#26631;&#31614;&#36827;&#34892;&#39044;&#27979;&#21644;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#20013;&#20132;&#36890;&#31649;&#21046;&#26159;&#19968;&#20010;&#23433;&#20840;&#20851;&#38190;&#30340;&#26381;&#21153;&#31995;&#32479;&#65292;&#35201;&#27714;&#22320;&#38754;&#31354;&#20013;&#20132;&#36890;&#31649;&#21046;&#21592;&#65288;ATCo&#65289;&#26102;&#21051;&#20851;&#27880;&#20197;&#32500;&#25345;&#26085;&#24120;&#33322;&#31354;&#36816;&#33829;&#12290;ATCo&#30340;&#24037;&#20316;&#36127;&#33655;&#21487;&#33021;&#23545;&#36816;&#33829;&#23433;&#20840;&#21644;&#31354;&#22495;&#20351;&#29992;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#20102;&#36991;&#20813;&#36807;&#36733;&#24182;&#30830;&#20445;ATCo&#30340;&#21487;&#25509;&#21463;&#24037;&#20316;&#36127;&#33655;&#27700;&#24179;&#65292;&#20934;&#30830;&#39044;&#27979;ATCo&#30340;&#24037;&#20316;&#36127;&#33655;&#23545;&#20110;&#37319;&#21462;&#32531;&#35299;&#25514;&#26045;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;ATCo&#24037;&#20316;&#36127;&#33655;&#30340;&#30740;&#31350;&#36827;&#34892;&#20102;&#22238;&#39038;&#65292;&#20027;&#35201;&#20174;&#31354;&#20013;&#20132;&#36890;&#30340;&#35282;&#24230;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#31616;&#35201;&#20171;&#32461;&#20102;&#19982;&#36864;&#20241;ATCo&#36827;&#34892;&#20154;&#26426;&#20132;&#20114;&#27169;&#25311;&#30340;&#35774;&#32622;&#65292;&#20854;&#20013;&#33719;&#24471;&#20102;&#31354;&#20013;&#20132;&#36890;&#25968;&#25454;&#21644;&#24037;&#20316;&#36127;&#33655;&#26631;&#31614;&#12290;&#27169;&#25311;&#22312;&#19977;&#31181;&#33778;&#23612;&#20811;&#26031;&#25509;&#36817;&#22330;&#26223;&#19979;&#36827;&#34892;&#65292;&#35201;&#27714;&#20154;&#31867;ATCo&#33258;&#25105;&#35780;&#20272;&#20854;&#24037;&#20316;&#36127;&#33655;&#35780;&#32423;&#65288;&#21363;&#65292;&#20302;-1&#21040;&#39640;-7&#65289;&#12290;&#36827;&#34892;&#20102;&#21021;&#27493;&#30340;&#25968;&#25454;&#20998;&#26512;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22270;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#32467;&#21512;&#21512;&#35268;&#21270;&#39044;&#27979;&#65292;&#26469;&#23545;ATCo&#30340;&#24037;&#20316;&#36127;&#33655;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Air traffic control (ATC) is a safety-critical service system that demands constant attention from ground air traffic controllers (ATCos) to maintain daily aviation operations. The workload of the ATCos can have negative effects on operational safety and airspace usage. To avoid overloading and ensure an acceptable workload level for the ATCos, it is important to predict the ATCos' workload accurately for mitigation actions. In this paper, we first perform a review of research on ATCo workload, mostly from the air traffic perspective. Then, we briefly introduce the setup of the human-in-the-loop (HITL) simulations with retired ATCos, where the air traffic data and workload labels are obtained. The simulations are conducted under three Phoenix approach scenarios while the human ATCos are requested to self-evaluate their workload ratings (i.e., low-1 to high-7). Preliminary data analysis is conducted. Next, we propose a graph-based deep-learning framework with conformal prediction to ide
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32534;&#30721;&#35821;&#35328;&#27169;&#22411;&#30340;&#39118;&#26684;&#25511;&#21046;&#38646;&#26679;&#26412;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#22120;&#65292;&#36890;&#36807;&#25511;&#21046;&#23646;&#24615;&#29983;&#25104;&#21487;&#25511;&#35821;&#38899;&#32780;&#19981;&#20165;&#20165;&#27169;&#20223;&#29305;&#24449;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.10550</link><description>&lt;p&gt;
SC VALL-E: &#21487;&#25511;&#39118;&#26684;&#30340;&#38646;&#26679;&#26412;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
SC VALL-E: Style-Controllable Zero-Shot Text to Speech Synthesizer. (arXiv:2307.10550v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10550
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32534;&#30721;&#35821;&#35328;&#27169;&#22411;&#30340;&#39118;&#26684;&#25511;&#21046;&#38646;&#26679;&#26412;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#22120;&#65292;&#36890;&#36807;&#25511;&#21046;&#23646;&#24615;&#29983;&#25104;&#21487;&#25511;&#35821;&#38899;&#32780;&#19981;&#20165;&#20165;&#27169;&#20223;&#29305;&#24449;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25511;&#21046;&#35821;&#38899;&#30340;&#21508;&#31181;&#29305;&#24449;&#21644;&#29983;&#25104;&#25152;&#38656;&#30340;&#22768;&#38899;&#65292;&#34920;&#36798;&#24615;&#30340;&#35821;&#38899;&#21512;&#25104;&#27169;&#22411;&#36890;&#36807;&#28155;&#21152;&#20855;&#26377;&#19981;&#21516;&#35828;&#35805;&#32773;&#12289;&#24773;&#32490;&#21644;&#19981;&#21516;&#35828;&#35805;&#39118;&#26684;&#30340;&#35821;&#26009;&#24211;&#26469;&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32534;&#30721;&#35821;&#35328;&#27169;&#22411;VALL-E&#30340;&#39118;&#26684;&#25511;&#21046;&#65288;SC&#65289;VALL-E&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36981;&#24490;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;3&#65288;GPT-3&#65289;&#30340;&#32467;&#26500;&#12290;&#25152;&#25552;&#20986;&#30340;SC VALL-E&#27169;&#22411;&#20174;&#25991;&#26412;&#21477;&#23376;&#21644;&#25552;&#31034;&#38899;&#39057;&#20013;&#25509;&#25910;&#36755;&#20837;&#65292;&#24182;&#36890;&#36807;&#25511;&#21046;&#23646;&#24615;&#26469;&#29983;&#25104;&#21487;&#25511;&#30340;&#35821;&#38899;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#27169;&#20223;&#25552;&#31034;&#38899;&#39057;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#36890;&#36807;&#35782;&#21035;&#26032;&#35774;&#35745;&#30340;&#39118;&#26684;&#32593;&#32476;&#30340;&#39118;&#26684;&#23884;&#20837;&#30697;&#38453;&#20013;&#34920;&#31034;&#24773;&#32490;&#12289;&#35828;&#35805;&#36895;&#24230;&#12289;&#38899;&#39640;&#21644;&#22768;&#38899;&#24378;&#24230;&#31561;&#23646;&#24615;&#30340;&#26631;&#35760;&#65292;&#24182;&#35774;&#35745;&#19968;&#20010;&#21487;&#20197;&#25511;&#21046;&#36825;&#20123;&#23646;&#24615;&#30340;&#27169;&#22411;&#12290;&#20026;&#20102;&#35780;&#20272;SC VALL-E&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19982;&#19977;&#31181;&#22522;&#20934;&#27169;&#22411;&#30340;&#27604;&#36739;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Expressive speech synthesis models are trained by adding corpora with diverse speakers, various emotions, and different speaking styles to the dataset, in order to control various characteristics of speech and generate the desired voice. In this paper, we propose a style control (SC) VALL-E model based on the neural codec language model (called VALL-E), which follows the structure of the generative pretrained transformer 3 (GPT-3). The proposed SC VALL-E takes input from text sentences and prompt audio and is designed to generate controllable speech by not simply mimicking the characteristics of the prompt audio but by controlling the attributes to produce diverse voices. We identify tokens in the style embedding matrix of the newly designed style network that represent attributes such as emotion, speaking rate, pitch, and voice intensity, and design a model that can control these attributes. To evaluate the performance of SC VALL-E, we conduct comparative experiments with three repres
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24046;&#20998;&#24179;&#22374;&#23398;&#20064;&#30340;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#38750;&#32447;&#24615;&#21464;&#25442;&#20316;&#20026;&#23433;&#20840;&#28388;&#27874;&#22120;&#65292;&#21487;&#20197;&#22312;&#20445;&#35777;&#31283;&#23450;&#24615;&#12289;&#36755;&#20837;&#21644;&#29366;&#24577;&#32422;&#26463;&#28385;&#36275;&#30340;&#21516;&#26102;&#22823;&#24133;&#20943;&#23569;&#35745;&#31639;&#37327;&#12290;</title><link>http://arxiv.org/abs/2307.10541</link><description>&lt;p&gt;
&#22522;&#20110;&#24046;&#20998;&#24179;&#22374;&#23398;&#20064;&#30340;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#21450;&#31283;&#23450;&#24615;&#12289;&#29366;&#24577;&#21644;&#36755;&#20837;&#38480;&#21046;&#30340;&#23433;&#20840;&#28388;&#27874;&#22120;
&lt;/p&gt;
&lt;p&gt;
Differentially Flat Learning-based Model Predictive Control Using a Stability, State, and Input Constraining Safety Filter. (arXiv:2307.10541v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10541
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24046;&#20998;&#24179;&#22374;&#23398;&#20064;&#30340;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#38750;&#32447;&#24615;&#21464;&#25442;&#20316;&#20026;&#23433;&#20840;&#28388;&#27874;&#22120;&#65292;&#21487;&#20197;&#22312;&#20445;&#35777;&#31283;&#23450;&#24615;&#12289;&#36755;&#20837;&#21644;&#29366;&#24577;&#32422;&#26463;&#28385;&#36275;&#30340;&#21516;&#26102;&#22823;&#24133;&#20943;&#23569;&#35745;&#31639;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#23398;&#20064;&#30340;&#26368;&#20248;&#25511;&#21046;&#31639;&#27861;&#20351;&#29992;&#36807;&#21435;&#30340;&#36712;&#36857;&#25968;&#25454;&#21644;&#23398;&#20064;&#21040;&#30340;&#31995;&#32479;&#21160;&#21147;&#23398;&#27169;&#22411;&#26469;&#25511;&#21046;&#26410;&#30693;&#31995;&#32479;&#12290;&#36825;&#20123;&#25511;&#21046;&#22120;&#35201;&#20040;&#20351;&#29992;&#23398;&#20064;&#21040;&#30340;&#21160;&#21147;&#23398;&#30340;&#32447;&#24615;&#36817;&#20284;&#65292;&#20197;&#25442;&#21462;&#26356;&#24555;&#30340;&#35745;&#31639;&#36895;&#24230;&#65292;&#35201;&#20040;&#20351;&#29992;&#38750;&#32447;&#24615;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#24120;&#34920;&#29616;&#26356;&#22909;&#65292;&#20294;&#20250;&#38480;&#21046;&#23454;&#26102;&#24212;&#29992;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#38750;&#32447;&#24615;&#25511;&#21046;&#22120;&#65292;&#21033;&#29992;&#24046;&#20998;&#24179;&#22374;&#24615;&#21487;&#20197;&#22312;&#19982;&#26368;&#20808;&#36827;&#30340;&#23398;&#20064;&#25511;&#21046;&#22120;&#30456;&#20284;&#30340;&#24615;&#33021;&#19979;&#65292;&#26174;&#33879;&#20943;&#23569;&#35745;&#31639;&#37327;&#12290;&#24046;&#20998;&#24179;&#22374;&#24615;&#26159;&#21160;&#24577;&#31995;&#32479;&#30340;&#19968;&#31181;&#23646;&#24615;&#65292;&#36890;&#36807;&#38750;&#32447;&#24615;&#36755;&#20837;&#26144;&#23556;&#21487;&#20197;&#31934;&#30830;&#32447;&#24615;&#21270;&#38750;&#32447;&#24615;&#31995;&#32479;&#12290;&#22312;&#36825;&#37324;&#65292;&#38750;&#32447;&#24615;&#21464;&#25442;&#34987;&#23398;&#20064;&#20026;&#39640;&#26031;&#36807;&#31243;&#65292;&#24182;&#29992;&#20110;&#20445;&#35777;&#23433;&#20840;&#36807;&#28388;&#22120;&#30340;&#31283;&#23450;&#24615;&#21644;&#36755;&#20837;&#32422;&#26463;&#28385;&#36275;&#26465;&#20214;&#12290;&#28982;&#21518;&#65292;&#36825;&#20010;&#23433;&#20840;&#36807;&#28388;&#22120;&#29992;&#20110;&#25913;&#36827;&#26469;&#33258;&#24179;&#22374;&#27169;&#22411;&#39044;&#27979;&#30340;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning-based optimal control algorithms control unknown systems using past trajectory data and a learned model of the system dynamics. These controllers use either a linear approximation of the learned dynamics, trading performance for faster computation, or nonlinear optimization methods, which typically perform better but can limit real-time applicability. In this work, we present a novel nonlinear controller that exploits differential flatness to achieve similar performance to state-of-the-art learning-based controllers but with significantly less computational effort. Differential flatness is a property of dynamical systems whereby nonlinear systems can be exactly linearized through a nonlinear input mapping. Here, the nonlinear transformation is learned as a Gaussian process and is used in a safety filter that guarantees, with high probability, stability as well as input and flat state constraint satisfaction. This safety filter is then used to refine inputs from a flat model pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;HYPER&#29992;&#20110;&#35843;&#25972;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24322;&#24120;&#20540;&#26816;&#27979;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#26080;&#30417;&#30563;DOD&#27169;&#22411;&#20013;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#21644;&#27169;&#22411;&#36873;&#25321;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#35774;&#35745;&#21644;&#35757;&#32451;&#36229;&#32593;&#32476;(HN)&#23558;&#36229;&#21442;&#25968;&#26144;&#23556;&#21040;&#20027;&#35201;DOD&#27169;&#22411;&#30340;&#26368;&#20248;&#26435;&#37325;&#19978;&#12290;</title><link>http://arxiv.org/abs/2307.10529</link><description>&lt;p&gt;
&#24555;&#36895;&#26080;&#30417;&#30563;&#28145;&#24230;&#24322;&#24120;&#20540;&#27169;&#22411;&#36873;&#25321;&#19982;&#36229;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Fast Unsupervised Deep Outlier Model Selection with Hypernetworks. (arXiv:2307.10529v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10529
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;HYPER&#29992;&#20110;&#35843;&#25972;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24322;&#24120;&#20540;&#26816;&#27979;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#26080;&#30417;&#30563;DOD&#27169;&#22411;&#20013;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#21644;&#27169;&#22411;&#36873;&#25321;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#35774;&#35745;&#21644;&#35757;&#32451;&#36229;&#32593;&#32476;(HN)&#23558;&#36229;&#21442;&#25968;&#26144;&#23556;&#21040;&#20027;&#35201;DOD&#27169;&#22411;&#30340;&#26368;&#20248;&#26435;&#37325;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#20540;&#26816;&#27979;(OD)&#22312;&#35768;&#22810;&#39046;&#22495;&#37117;&#26377;&#24212;&#29992;&#65292;&#24182;&#26377;&#35768;&#22810;&#25216;&#26415;&#30340;&#20016;&#23500;&#25991;&#29486;&#12290;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;OD(DOD)&#30001;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35768;&#22810;&#36827;&#23637;&#32780;&#21463;&#21040;&#20102;&#26368;&#36817;&#30340;&#20851;&#27880;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#20851;&#38190;&#20294;&#40092;&#20026;&#20154;&#30693;&#30340;&#38382;&#39064;&#65292;&#21363;&#26080;&#30417;&#30563;DOD&#30340;&#26377;&#25928;&#36229;&#21442;&#25968;(HP)&#35843;&#25972;/&#27169;&#22411;&#36873;&#25321;&#12290;&#34429;&#28982;&#19968;&#20123;&#20808;&#21069;&#30340;&#24037;&#20316;&#25253;&#21578;&#20102;OD&#27169;&#22411;&#23545;HP&#30340;&#25935;&#24863;&#24615;&#65292;&#20294;&#23545;&#20110;&#23637;&#31034;&#20102;&#38271;&#21015;&#34920;HP&#30340;&#29616;&#20195;DOD&#27169;&#22411;&#26469;&#35828;&#65292;&#36825;&#21464;&#24471;&#38750;&#24120;&#20851;&#38190;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;HYPER&#26469;&#35843;&#25972;DOD&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20004;&#20010;&#22522;&#26412;&#25361;&#25112;&#65306;(1)&#26080;&#30417;&#30563;&#24773;&#20917;&#19979;&#30340;&#39564;&#35777;(&#30001;&#20110;&#32570;&#20047;&#26631;&#35760;&#30340;&#24322;&#24120;&#20540;)&#65292;&#20197;&#21450;(2) HP/&#27169;&#22411;&#31354;&#38388;&#30340;&#39640;&#25928;&#25628;&#32034; (&#30001;&#20110;HP&#25968;&#37327;&#30340;&#25351;&#25968;&#22686;&#38271;)&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#35774;&#35745;&#21644;&#35757;&#32451;&#19968;&#20010;&#26032;&#39062;&#30340;&#36229;&#32593;&#32476;(HN)&#65292;&#20854;&#23558;HP&#26144;&#23556;&#21040;&#20027;&#35201;DOD&#27169;&#22411;&#30340;&#26368;&#20248;&#26435;&#37325;&#19978;&#12290;&#21453;&#36807;&#26469;&#65292;HYPER&#21033;&#29992;&#19968;&#20010;&#21333;&#29420;&#30340;HN&#65292;&#21487;&#20197;&#21160;&#24577;&#29983;&#25104;&#22810;&#20010;DOD&#27169;&#22411;&#30340;&#26435;&#37325; (&#23545;&#24212;&#20110;...)&#12290;
&lt;/p&gt;
&lt;p&gt;
Outlier detection (OD) finds many applications with a rich literature of numerous techniques. Deep neural network based OD (DOD) has seen a recent surge of attention thanks to the many advances in deep learning. In this paper, we consider a critical-yet-understudied challenge with unsupervised DOD, that is, effective hyperparameter (HP) tuning/model selection. While several prior work report the sensitivity of OD models to HPs, it becomes ever so critical for the modern DOD models that exhibit a long list of HPs. We introduce HYPER for tuning DOD models, tackling two fundamental challenges: (1) validation without supervision (due to lack of labeled anomalies), and (2) efficient search of the HP/model space (due to exponential growth in the number of HPs). A key idea is to design and train a novel hypernetwork (HN) that maps HPs onto optimal weights of the main DOD model. In turn, HYPER capitalizes on a single HN that can dynamically generate weights for many DOD models (corresponding t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#20855;&#26377;&#19981;&#21487;&#20449;&#30340;&#26426;&#22120;&#23398;&#20064;&#24314;&#35758;&#30340;&#21333;&#36712;&#36857;&#26102;&#38388;&#21464;&#21270;&#30340;MDP&#20013;&#19968;&#33268;&#24615;&#21644;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24182;&#35777;&#26126;&#20102;&#21033;&#29992;Q&#20540;&#24314;&#35758;&#21487;&#20197;&#33719;&#24471;&#25509;&#36817;&#26368;&#20248;&#30340;&#24615;&#33021;&#20445;&#35777;&#65292;&#24182;&#25913;&#36827;&#20102;&#20165;&#20351;&#29992;&#40657;&#30418;&#24314;&#35758;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2307.10524</link><description>&lt;p&gt;
&#36229;&#36234;&#40657;&#30418;&#24314;&#35758;: &#22522;&#20110;&#23398;&#20064;&#30340;&#22686;&#24378;&#31639;&#27861;&#29992;&#20110;&#20855;&#26377;Q&#20540;&#39044;&#27979;&#30340;MDPs
&lt;/p&gt;
&lt;p&gt;
Beyond Black-Box Advice: Learning-Augmented Algorithms for MDPs with Q-Value Predictions. (arXiv:2307.10524v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10524
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#20855;&#26377;&#19981;&#21487;&#20449;&#30340;&#26426;&#22120;&#23398;&#20064;&#24314;&#35758;&#30340;&#21333;&#36712;&#36857;&#26102;&#38388;&#21464;&#21270;&#30340;MDP&#20013;&#19968;&#33268;&#24615;&#21644;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24182;&#35777;&#26126;&#20102;&#21033;&#29992;Q&#20540;&#24314;&#35758;&#21487;&#20197;&#33719;&#24471;&#25509;&#36817;&#26368;&#20248;&#30340;&#24615;&#33021;&#20445;&#35777;&#65292;&#24182;&#25913;&#36827;&#20102;&#20165;&#20351;&#29992;&#40657;&#30418;&#24314;&#35758;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#21333;&#36712;&#36857;&#26102;&#38388;&#21464;&#21270;&#30340;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;(MDP)&#20013;&#19968;&#33268;&#24615;&#21644;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#35813;&#36807;&#31243;&#20855;&#26377;&#19981;&#21487;&#20449;&#30340;&#26426;&#22120;&#23398;&#20064;&#24314;&#35758;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#19981;&#21516;&#20110;&#24120;&#35268;&#26041;&#27861;&#65292;&#19981;&#20877;&#23558;&#24314;&#35758;&#35270;&#20026;&#26469;&#33258;&#40657;&#30418;&#26469;&#28304;&#65292;&#32780;&#26159;&#32771;&#34385;&#21040;&#26377;&#20851;&#22914;&#20309;&#29983;&#25104;&#24314;&#35758;&#30340;&#20854;&#20182;&#20449;&#24687;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#21253;&#25324;&#36830;&#32493;&#21644;&#31163;&#25955;&#29366;&#24577;/&#21160;&#20316;&#31354;&#38388;&#30340;&#19968;&#33324;MDP&#27169;&#22411;&#19979;&#32473;&#20986;&#30340;Q&#20540;&#24314;&#35758;&#30340;&#19968;&#31181;&#26032;&#22411;&#19968;&#33268;&#24615;&#21644;&#40065;&#26834;&#24615;&#26435;&#34913;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#21033;&#29992;Q&#20540;&#24314;&#35758;&#21487;&#20197;&#21160;&#24577;&#36861;&#27714;&#26426;&#22120;&#23398;&#20064;&#24314;&#35758;&#21644;&#31283;&#20581;&#22522;&#32447;&#20013;&#36739;&#20248;&#30340;&#37027;&#20010;&#65292;&#20174;&#32780;&#20135;&#29983;&#25509;&#36817;&#26368;&#20248;&#30340;&#24615;&#33021;&#20445;&#35777;&#65292;&#24182;&#19988;&#25913;&#36827;&#20102;&#20165;&#20351;&#29992;&#40657;&#30418;&#24314;&#35758;&#25152;&#33021;&#33719;&#24471;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the tradeoff between consistency and robustness in the context of a single-trajectory time-varying Markov Decision Process (MDP) with untrusted machine-learned advice. Our work departs from the typical approach of treating advice as coming from black-box sources by instead considering a setting where additional information about how the advice is generated is available. We prove a first-of-its-kind consistency and robustness tradeoff given Q-value advice under a general MDP model that includes both continuous and discrete state/action spaces. Our results highlight that utilizing Q-value advice enables dynamic pursuit of the better of machine-learned advice and a robust baseline, thus result in near-optimal performance guarantees, which provably improves what can be obtained solely with black-box advice.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#36873;&#25321;&#24615;&#27169;&#22411;&#25554;&#20540;&#30340;&#26041;&#24335;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#25913;&#21892;&#20102;&#27867;&#21270;&#33021;&#21147;&#21644;&#20010;&#24615;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.10507</link><description>&lt;p&gt;
FedSoup:&#36890;&#36807;&#36873;&#25321;&#24615;&#27169;&#22411;&#25554;&#20540;&#25913;&#21892;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#27867;&#21270;&#21644;&#20010;&#24615;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
FedSoup: Improving Generalization and Personalization in Federated Learning via Selective Model Interpolation. (arXiv:2307.10507v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10507
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#36873;&#25321;&#24615;&#27169;&#22411;&#25554;&#20540;&#30340;&#26041;&#24335;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#25913;&#21892;&#20102;&#27867;&#21270;&#33021;&#21147;&#21644;&#20010;&#24615;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#26426;&#26500;&#30340;&#32852;&#37030;&#23398;&#20064;&#20351;&#24471;&#21487;&#20197;&#22312;&#20998;&#24067;&#22312;&#25968;&#25454;&#20013;&#24515;&#65288;&#22914;&#21307;&#38498;&#21644;&#20020;&#24202;&#30740;&#31350;&#23454;&#39564;&#23460;&#65289;&#30340;&#25968;&#25454;&#38598;&#19978;&#24320;&#21457;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#38754;&#20020;&#20998;&#24067;&#20559;&#31227;&#26102;&#65292;&#24403;&#21069;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#22312;&#26412;&#22320;&#21644;&#20840;&#23616;&#24615;&#33021;&#20043;&#38388;&#23384;&#22312;&#19968;&#31181;&#26435;&#34913;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20010;&#24615;&#21270;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#24448;&#24448;&#20542;&#21521;&#20110;&#23545;&#26412;&#22320;&#25968;&#25454;&#36807;&#25311;&#21512;&#65292;&#23548;&#33268;&#23616;&#37096;&#27169;&#22411;&#21457;&#29983;&#36716;&#25240;&#65292;&#24182;&#25233;&#21046;&#20854;&#23545;&#38750;&#20998;&#24067;&#24335;&#25968;&#25454;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32852;&#37030;&#27169;&#22411;&#27748;&#26041;&#27861;&#65288;&#21363;&#65292;&#36873;&#25321;&#24615;&#25554;&#20540;&#27169;&#22411;&#21442;&#25968;&#65289;&#26469;&#20248;&#21270;&#26412;&#22320;&#21644;&#20840;&#23616;&#24615;&#33021;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#32852;&#37030;&#35757;&#32451;&#38454;&#27573;&#65292;&#27599;&#20010;&#23458;&#25143;&#31471;&#36890;&#36807;&#30417;&#25511;&#26412;&#22320;&#21644;&#20840;&#23616;&#27169;&#22411;&#20043;&#38388;&#30340;&#25554;&#20540;&#27169;&#22411;&#30340;&#34920;&#29616;&#26469;&#32500;&#25252;&#33258;&#24049;&#30340;&#20840;&#23616;&#27169;&#22411;&#27744;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#20943;&#36731;&#36807;&#25311;&#21512;&#65292;&#24182;&#23547;&#27714;&#24179;&#22374;&#30340;&#26497;&#23567;&#20540;&#65292;&#20174;&#32780;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#31995;&#32479;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#20010;&#24615;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-silo federated learning (FL) enables the development of machine learning models on datasets distributed across data centers such as hospitals and clinical research laboratories. However, recent research has found that current FL algorithms face a trade-off between local and global performance when confronted with distribution shifts. Specifically, personalized FL methods have a tendency to overfit to local data, leading to a sharp valley in the local model and inhibiting its ability to generalize to out-of-distribution data. In this paper, we propose a novel federated model soup method (i.e., selective interpolation of model parameters) to optimize the trade-off between local and global performance. Specifically, during the federated training phase, each client maintains its own global model pool by monitoring the performance of the interpolated model between the local and global models. This allows us to alleviate overfitting and seek flat minima, which can significantly improve
&lt;/p&gt;</description></item><item><title>FALCON&#26159;&#19968;&#20010;&#35299;&#37322;&#22270;&#20687;&#34920;&#31034;&#29305;&#24449;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#23383;&#24149;&#25968;&#25454;&#38598;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#26469;&#29983;&#25104;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#27010;&#24565;&#65292;&#24182;&#36890;&#36807;&#23545;&#27604;&#35299;&#37322;&#28040;&#38500;&#34394;&#20551;&#27010;&#24565;&#12290;&#22312;&#36739;&#22823;&#30340;&#31354;&#38388;&#20013;&#65292;&#29305;&#24449;&#36890;&#36807;&#30740;&#31350;&#32452;&#21512;&#21487;&#20197;&#26356;&#26131;&#35299;&#37322;&#21644;&#39640;&#38454;&#35780;&#20998;&#27010;&#24565;&#30340;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2307.10504</link><description>&lt;p&gt;
&#22312;&#22270;&#20687;&#34920;&#31034;&#20013;&#35782;&#21035;&#21487;&#35299;&#37322;&#23376;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Identifying Interpretable Subspaces in Image Representations. (arXiv:2307.10504v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10504
&lt;/p&gt;
&lt;p&gt;
FALCON&#26159;&#19968;&#20010;&#35299;&#37322;&#22270;&#20687;&#34920;&#31034;&#29305;&#24449;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#23383;&#24149;&#25968;&#25454;&#38598;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#26469;&#29983;&#25104;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#27010;&#24565;&#65292;&#24182;&#36890;&#36807;&#23545;&#27604;&#35299;&#37322;&#28040;&#38500;&#34394;&#20551;&#27010;&#24565;&#12290;&#22312;&#36739;&#22823;&#30340;&#31354;&#38388;&#20013;&#65292;&#29305;&#24449;&#36890;&#36807;&#30740;&#31350;&#32452;&#21512;&#21487;&#20197;&#26356;&#26131;&#35299;&#37322;&#21644;&#39640;&#38454;&#35780;&#20998;&#27010;&#24565;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;&#22270;&#20687;&#34920;&#31034;&#29305;&#24449;&#30340;&#21487;&#35299;&#37322;&#24615;&#26694;&#26550;&#8212;&#8212;FALCON&#65288;Automatic Feature Explanation using Contrasting Concepts&#65289;&#12290;&#23545;&#20110;&#30446;&#26631;&#29305;&#24449;&#65292;FALCON&#20351;&#29992;&#19968;&#20010;&#22823;&#22411;&#30340;&#23383;&#24149;&#25968;&#25454;&#38598;&#65288;&#22914;LAION-400m&#65289;&#21644;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;CLIP&#65289;&#23545;&#20854;&#39640;&#24230;&#28608;&#27963;&#30340;&#35009;&#21098;&#22270;&#20687;&#36827;&#34892;&#23383;&#24149;&#29983;&#25104;&#12290;&#27599;&#20010;&#23383;&#24149;&#20013;&#30340;&#21333;&#35789;&#37117;&#32463;&#36807;&#35780;&#20998;&#21644;&#25490;&#24207;&#65292;&#20174;&#32780;&#24471;&#21040;&#20102;&#19982;&#30446;&#26631;&#29305;&#24449;&#23494;&#20999;&#30456;&#20851;&#30340;&#23569;&#25968;&#20849;&#20139;&#30340;&#12289;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#27010;&#24565;&#12290;FALCON&#36824;&#20351;&#29992;&#20302;&#28608;&#27963;&#30340;&#65288;&#23545;&#31435;&#30340;&#65289;&#22270;&#20687;&#24212;&#29992;&#23545;&#27604;&#35299;&#37322;&#65292;&#20197;&#28040;&#38500;&#34394;&#20551;&#27010;&#24565;&#12290;&#23613;&#31649;&#35768;&#22810;&#29616;&#26377;&#26041;&#27861;&#29420;&#31435;&#35299;&#37322;&#29305;&#24449;&#65292;&#20294;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#26368;&#20808;&#36827;&#30340;&#33258;&#30417;&#30563;&#21644;&#30417;&#30563;&#27169;&#22411;&#20013;&#65292;&#19981;&#21040;20%&#30340;&#34920;&#31034;&#31354;&#38388;&#21487;&#20197;&#36890;&#36807;&#21333;&#20010;&#29305;&#24449;&#36827;&#34892;&#35299;&#37322;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#19968;&#32452;&#29305;&#24449;&#19968;&#36215;&#30740;&#31350;&#26102;&#65292;&#26356;&#22823;&#30340;&#31354;&#38388;&#20013;&#30340;&#29305;&#24449;&#21464;&#24471;&#26356;&#26131;&#35299;&#37322;&#65292;&#24182;&#21487;&#20197;&#36890;&#36807;FALCON&#24471;&#21040;&#39640;&#38454;&#35780;&#20998;&#27010;&#24565;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Automatic Feature Explanation using Contrasting Concepts (FALCON), an interpretability framework to explain features of image representations. For a target feature, FALCON captions its highly activating cropped images using a large captioning dataset (like LAION-400m) and a pre-trained vision-language model like CLIP. Each word among the captions is scored and ranked leading to a small number of shared, human-understandable concepts that closely describe the target feature. FALCON also applies contrastive interpretation using lowly activating (counterfactual) images, to eliminate spurious concepts. Although many existing approaches interpret features independently, we observe in state-of-the-art self-supervised and supervised models, that less than 20% of the representation space can be explained by individual features. We show that features in larger spaces become more interpretable when studied in groups and can be explained with high-order scoring concepts through FALCON.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22797;&#26434;&#29289;&#29702;&#31995;&#32479;&#30340;&#31454;&#20105;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21516;&#26102;&#35757;&#32451;&#22810;&#20010;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#21160;&#24577;&#25439;&#22833;&#20989;&#25968;&#35782;&#21035;&#19981;&#21516;&#30340;&#21151;&#33021;&#21306;&#22495;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#25104;&#21151;&#22320;&#35299;&#20915;&#27169;&#22411;&#21457;&#29616;&#21644;&#20989;&#25968;&#25311;&#21512;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.10496</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#19987;&#38376;&#27169;&#22411;&#30340;&#31454;&#20105;&#23398;&#20064;&#26041;&#27861;&#65306;&#35299;&#20915;&#20855;&#26377;&#19981;&#21516;&#21151;&#33021;&#21306;&#22495;&#30340;&#22797;&#26434;&#29289;&#29702;&#31995;&#32479;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
A Competitive Learning Approach for Specialized Models: A Solution for Complex Physical Systems with Distinct Functional Regimes. (arXiv:2307.10496v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10496
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22797;&#26434;&#29289;&#29702;&#31995;&#32479;&#30340;&#31454;&#20105;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21516;&#26102;&#35757;&#32451;&#22810;&#20010;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#21160;&#24577;&#25439;&#22833;&#20989;&#25968;&#35782;&#21035;&#19981;&#21516;&#30340;&#21151;&#33021;&#21306;&#22495;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#25104;&#21151;&#22320;&#35299;&#20915;&#27169;&#22411;&#21457;&#29616;&#21644;&#20989;&#25968;&#25311;&#21512;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31185;&#23398;&#21644;&#24037;&#31243;&#20013;&#65292;&#22797;&#26434;&#31995;&#32479;&#26377;&#26102;&#20250;&#23637;&#29616;&#20986;&#22312;&#19981;&#21516;&#21306;&#22495;&#20043;&#38388;&#21464;&#21270;&#30340;&#34892;&#20026;&#12290;&#20256;&#32479;&#30340;&#20840;&#23616;&#27169;&#22411;&#24456;&#38590;&#25429;&#25417;&#21040;&#36825;&#31181;&#22797;&#26434;&#34892;&#20026;&#30340;&#20840;&#33539;&#22260;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#20204;&#20934;&#30830;&#34920;&#31034;&#31995;&#32479;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31454;&#20105;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#33719;&#21462;&#22522;&#20110;&#25968;&#25454;&#30340;&#29289;&#29702;&#31995;&#32479;&#27169;&#22411;&#12290;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#20027;&#35201;&#24605;&#24819;&#26159;&#21516;&#26102;&#23545;&#19968;&#32452;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#37319;&#29992;&#21160;&#24577;&#25439;&#22833;&#20989;&#25968;&#12290;&#27599;&#20010;&#27169;&#22411;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20105;&#22842;&#27599;&#20010;&#35266;&#23519;&#20540;&#65292;&#20174;&#32780;&#20801;&#35768;&#22312;&#25968;&#25454;&#38598;&#20013;&#35782;&#21035;&#20986;&#19981;&#21516;&#30340;&#21151;&#33021;&#21306;&#22495;&#12290;&#20026;&#20102;&#23637;&#31034;&#23398;&#20064;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#23558;&#20854;&#19982;&#20351;&#29992;&#26799;&#24230;&#20248;&#21270;&#22120;&#36827;&#34892;&#35757;&#32451;&#30340;&#21508;&#31181;&#22238;&#24402;&#26041;&#27861;&#30456;&#32467;&#21512;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#28041;&#21450;&#27169;&#22411;&#21457;&#29616;&#21644;&#20989;&#25968;&#25311;&#21512;&#30340;&#21508;&#31181;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#34920;&#26126;&#23427;&#33021;&#22815;&#25104;&#21151;&#22320;&#35782;&#21035;&#20986;&#21151;&#33021;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Complex systems in science and engineering sometimes exhibit behavior that changes across different regimes. Traditional global models struggle to capture the full range of this complex behavior, limiting their ability to accurately represent the system. In response to this challenge, we propose a novel competitive learning approach for obtaining data-driven models of physical systems. The primary idea behind the proposed approach is to employ dynamic loss functions for a set of models that are trained concurrently on the data. Each model competes for each observation during training, allowing for the identification of distinct functional regimes within the dataset. To demonstrate the effectiveness of the learning approach, we coupled it with various regression methods that employ gradient-based optimizers for training. The proposed approach was tested on various problems involving model discovery and function approximation, demonstrating its ability to successfully identify functional
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25209;&#37327;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;Dijkstra&#30340;Annulus Core-Set (DAC)&#21644;LocalMax&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#22312;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#25968;&#25454;&#20013;&#23454;&#29616;&#19982;&#39034;&#24207;&#20027;&#21160;&#23398;&#20064;&#20960;&#20046;&#30456;&#21516;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#26356;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2307.10495</link><description>&lt;p&gt;
&#26032;&#39062;&#30340;&#25209;&#37327;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#21450;&#20854;&#22312;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#25968;&#25454;&#38598;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Novel Batch Active Learning Approach and Its Application to Synthetic Aperture Radar Datasets. (arXiv:2307.10495v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10495
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25209;&#37327;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;Dijkstra&#30340;Annulus Core-Set (DAC)&#21644;LocalMax&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#22312;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#25968;&#25454;&#20013;&#23454;&#29616;&#19982;&#39034;&#24207;&#20027;&#21160;&#23398;&#20064;&#20960;&#20046;&#30456;&#21516;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#26356;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#23398;&#20064;&#36890;&#36807;&#31934;&#36873;&#19968;&#23567;&#37096;&#20998;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#28857;&#26469;&#20248;&#21270;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#30446;&#30340;&#26159;&#26368;&#22823;&#31243;&#24230;&#22320;&#25552;&#39640;&#24213;&#23618;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;&#26368;&#36817;&#22312;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;(SAR)&#25968;&#25454;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#20123;&#39034;&#24207;&#20027;&#21160;&#23398;&#20064;&#30340;&#36827;&#23637;&#12290;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#65292;&#39034;&#24207;&#20027;&#21160;&#23398;&#20064;&#36873;&#25321;&#19968;&#20010;&#22823;&#23567;&#20026;&#19968;&#30340;&#26597;&#35810;&#38598;&#21512;&#65292;&#32780;&#25209;&#37327;&#20027;&#21160;&#23398;&#20064;&#36873;&#25321;&#19968;&#20010;&#21253;&#21547;&#22810;&#20010;&#25968;&#25454;&#28857;&#30340;&#26597;&#35810;&#38598;&#21512;&#12290;&#23613;&#31649;&#25209;&#37327;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#26356;&#21152;&#39640;&#25928;&#65292;&#20294;&#26159;&#30456;&#23545;&#20110;&#39034;&#24207;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#20445;&#25345;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#25361;&#25112;&#22312;&#20110;&#22256;&#38590;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20004;&#37096;&#20998;&#25209;&#37327;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65306;Dijkstra&#30340;Annulus Core-Set(DAC)&#29992;&#20110;&#26680;&#24515;&#38598;&#21512;&#29983;&#25104;&#65292;&#20197;&#21450;&#25209;&#27425;&#25277;&#26679;&#29992;&#20110;&#23616;&#37096;&#26368;&#22823;&#20540;&#12290;&#20351;&#29992;&#32467;&#21512;&#20102;DAC&#21644;LocalMax&#30340;&#25209;&#37327;&#20027;&#21160;&#23398;&#20064;&#36807;&#31243;&#21487;&#20197;&#36798;&#21040;&#20960;&#20046;&#19982;&#39034;&#24207;&#20027;&#21160;&#23398;&#20064;&#30456;&#21516;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#26356;&#21152;&#39640;&#25928;&#65292;&#19982;&#25209;&#27425;&#35268;&#27169;&#25104;&#27491;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
Active learning improves the performance of machine learning methods by judiciously selecting a limited number of unlabeled data points to query for labels, with the aim of maximally improving the underlying classifier's performance. Recent gains have been made using sequential active learning for synthetic aperture radar (SAR) data arXiv:2204.00005. In each iteration, sequential active learning selects a query set of size one while batch active learning selects a query set of multiple datapoints. While batch active learning methods exhibit greater efficiency, the challenge lies in maintaining model accuracy relative to sequential active learning methods. We developed a novel, two-part approach for batch active learning: Dijkstra's Annulus Core-Set (DAC) for core-set generation and LocalMax for batch sampling. The batch active learning process that combines DAC and LocalMax achieves nearly identical accuracy as sequential active learning but is more efficient, proportional to the batch
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#20449;&#20219;&#19982;&#32852;&#37030;&#23398;&#20064;&#12289;&#26143;&#38469;&#25991;&#20214;&#31995;&#32479;&#12289;&#21306;&#22359;&#38142;&#21644;&#26234;&#33021;&#21512;&#32422;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#23433;&#20840;&#20114;&#24800;&#30340;&#25968;&#25454;&#20849;&#20139;&#65292;&#24182;&#25552;&#20379;&#20102;&#28608;&#21169;&#12289;&#35775;&#38382;&#25511;&#21046;&#26426;&#21046;&#65292;&#24182;&#24809;&#32602;&#19981;&#35802;&#23454;&#30340;&#34892;&#20026;&#12290;&#32463;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#25552;&#39640;&#20102;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#30830;&#20445;&#20102;&#25968;&#25454;&#20849;&#20139;&#36807;&#31243;&#30340;&#23433;&#20840;&#24615;&#21644;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.10492</link><description>&lt;p&gt;
&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#32852;&#37030;&#23398;&#20064;&#65306;&#28608;&#21169;&#25968;&#25454;&#20849;&#20139;&#24182;&#24809;&#32602;&#19981;&#35802;&#23454;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Blockchain-Based Federated Learning: Incentivizing Data Sharing and Penalizing Dishonest Behavior. (arXiv:2307.10492v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10492
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#20449;&#20219;&#19982;&#32852;&#37030;&#23398;&#20064;&#12289;&#26143;&#38469;&#25991;&#20214;&#31995;&#32479;&#12289;&#21306;&#22359;&#38142;&#21644;&#26234;&#33021;&#21512;&#32422;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#23433;&#20840;&#20114;&#24800;&#30340;&#25968;&#25454;&#20849;&#20139;&#65292;&#24182;&#25552;&#20379;&#20102;&#28608;&#21169;&#12289;&#35775;&#38382;&#25511;&#21046;&#26426;&#21046;&#65292;&#24182;&#24809;&#32602;&#19981;&#35802;&#23454;&#30340;&#34892;&#20026;&#12290;&#32463;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#25552;&#39640;&#20102;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#30830;&#20445;&#20102;&#25968;&#25454;&#20849;&#20139;&#36807;&#31243;&#30340;&#23433;&#20840;&#24615;&#21644;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25968;&#25454;&#20849;&#20139;&#22312;&#21327;&#20316;&#21644;&#21019;&#26032;&#20013;&#30340;&#37325;&#35201;&#24615;&#26085;&#30410;&#22686;&#38271;&#65292;&#30830;&#20445;&#25968;&#25454;&#20197;&#23433;&#20840;&#21644;&#21487;&#20449;&#30340;&#26041;&#24335;&#36827;&#34892;&#31649;&#29702;&#21644;&#20849;&#20139;&#21464;&#24471;&#26356;&#21152;&#37325;&#35201;&#12290;&#25968;&#25454;&#27835;&#29702;&#26159;&#31649;&#29702;&#25968;&#25454;&#30340;&#24120;&#35265;&#26041;&#27861;&#65292;&#20294;&#38754;&#20020;&#35768;&#22810;&#25361;&#25112;&#65292;&#22914;&#25968;&#25454;&#23396;&#23707;&#12289;&#25968;&#25454;&#19968;&#33268;&#24615;&#12289;&#38544;&#31169;&#12289;&#23433;&#20840;&#21644;&#35775;&#38382;&#25511;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#26694;&#26550;&#65292;&#23558;&#25968;&#25454;&#20449;&#20219;&#19982;&#32852;&#37030;&#23398;&#20064;&#12289;&#26143;&#38469;&#25991;&#20214;&#31995;&#32479;&#12289;&#21306;&#22359;&#38142;&#21644;&#26234;&#33021;&#21512;&#32422;&#30456;&#32467;&#21512;&#65292;&#20197;&#20419;&#36827;&#23433;&#20840;&#20114;&#24800;&#30340;&#25968;&#25454;&#20849;&#20139;&#65292;&#21516;&#26102;&#25552;&#20379;&#28608;&#21169;&#12289;&#35775;&#38382;&#25511;&#21046;&#26426;&#21046;&#65292;&#24182;&#24809;&#32602;&#20219;&#20309;&#19981;&#35802;&#23454;&#30340;&#34892;&#20026;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#33021;&#22815;&#25552;&#39640;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#30830;&#20445;&#25968;&#25454;&#20849;&#20139;&#36807;&#31243;&#30340;&#23433;&#20840;&#24615;&#21644;&#20844;&#24179;&#24615;&#12290;&#35813;&#30740;&#31350;&#35770;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#24179;&#21488;&#65292;&#25104;&#21151;&#35757;&#32451;&#20102;&#19968;&#20010;CNN&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increasing importance of data sharing for collaboration and innovation, it is becoming more important to ensure that data is managed and shared in a secure and trustworthy manner. Data governance is a common approach to managing data, but it faces many challenges such as data silos, data consistency, privacy, security, and access control. To address these challenges, this paper proposes a comprehensive framework that integrates data trust in federated learning with InterPlanetary File System, blockchain, and smart contracts to facilitate secure and mutually beneficial data sharing while providing incentives, access control mechanisms, and penalizing any dishonest behavior. The experimental results demonstrate that the proposed model is effective in improving the accuracy of federated learning models while ensuring the security and fairness of the data-sharing process. The research paper also presents a decentralized federated learning platform that successfully trained a CNN m
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#22270;&#20687;&#21644;&#22768;&#38899;&#22312;&#22810;&#27169;&#24577;LLMs&#20013;&#36827;&#34892;&#38388;&#25509;&#25351;&#20196;&#27880;&#20837;&#65292;&#25915;&#20987;&#32773;&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#25200;&#21160;&#24182;&#23558;&#20854;&#34701;&#20837;&#22270;&#20687;&#25110;&#38899;&#39057;&#24405;&#38899;&#20013;&#65292;&#20197;&#25805;&#32437;&#27169;&#22411;&#36755;&#20986;&#29305;&#23450;&#25991;&#26412;&#21644;&#25351;&#23548;&#23545;&#35805;&#30340;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2307.10490</link><description>&lt;p&gt;
&#22270;&#20687;&#21644;&#22768;&#38899;&#30340;&#28389;&#29992;&#29992;&#20110;&#22312;&#22810;&#27169;&#24577;LLMs&#20013;&#36827;&#34892;&#38388;&#25509;&#25351;&#20196;&#27880;&#20837;
&lt;/p&gt;
&lt;p&gt;
(Ab)using Images and Sounds for Indirect Instruction Injection in Multi-Modal LLMs. (arXiv:2307.10490v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10490
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#22270;&#20687;&#21644;&#22768;&#38899;&#22312;&#22810;&#27169;&#24577;LLMs&#20013;&#36827;&#34892;&#38388;&#25509;&#25351;&#20196;&#27880;&#20837;&#65292;&#25915;&#20987;&#32773;&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#25200;&#21160;&#24182;&#23558;&#20854;&#34701;&#20837;&#22270;&#20687;&#25110;&#38899;&#39057;&#24405;&#38899;&#20013;&#65292;&#20197;&#25805;&#32437;&#27169;&#22411;&#36755;&#20986;&#29305;&#23450;&#25991;&#26412;&#21644;&#25351;&#23548;&#23545;&#35805;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#22270;&#20687;&#21644;&#22768;&#38899;&#22312;&#22810;&#27169;&#24577;LLMs&#20013;&#36827;&#34892;&#38388;&#25509;&#25552;&#31034;&#21644;&#25351;&#20196;&#27880;&#20837;&#12290;&#25915;&#20987;&#32773;&#29983;&#25104;&#19982;&#25552;&#31034;&#30456;&#23545;&#24212;&#30340;&#23545;&#25239;&#25200;&#21160;&#65292;&#24182;&#23558;&#20854;&#34701;&#20837;&#22270;&#20687;&#25110;&#38899;&#39057;&#24405;&#38899;&#20013;&#12290;&#24403;&#29992;&#25143;&#21521;&#65288;&#26410;&#20462;&#25913;&#30340;&#33391;&#24615;&#65289;&#27169;&#22411;&#35810;&#38382;&#34987;&#25200;&#21160;&#30340;&#22270;&#20687;&#25110;&#38899;&#39057;&#26102;&#65292;&#25200;&#21160;&#20250;&#24341;&#23548;&#27169;&#22411;&#36755;&#20986;&#25915;&#20987;&#32773;&#36873;&#25321;&#30340;&#25991;&#26412;&#21644;/&#25110;&#20351;&#21518;&#32493;&#23545;&#35805;&#36981;&#24490;&#25915;&#20987;&#32773;&#30340;&#25351;&#20196;&#12290;&#25105;&#20204;&#29992;&#20960;&#20010;&#27010;&#24565;&#39564;&#35777;&#31034;&#20363;&#38024;&#23545;LLaVa&#21644;PandaGPT&#26469;&#35828;&#26126;&#36825;&#31181;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
We demonstrate how images and sounds can be used for indirect prompt and instruction injection in multi-modal LLMs. An attacker generates an adversarial perturbation corresponding to the prompt and blends it into an image or audio recording. When the user asks the (unmodified, benign) model about the perturbed image or audio, the perturbation steers the model to output the attacker-chosen text and/or make the subsequent dialog follow the attacker's instruction. We illustrate this attack with several proof-of-concept examples targeting LLaVa and PandaGPT.
&lt;/p&gt;</description></item><item><title>SPRINT&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;Python&#24037;&#20855;&#21253;&#65292;&#22522;&#20110;Pyserini&#21644;Lucene&#65292;&#25903;&#25345;&#35780;&#20272;&#21644;&#35299;&#26512;&#38646;&#26679;&#26412;&#31070;&#32463;&#31232;&#30095;&#26816;&#32034;&#12290;&#23427;&#35299;&#20915;&#20102;&#32570;&#20047;&#32479;&#19968;&#29615;&#22659;&#21644;&#23454;&#29616;&#22312;&#26410;&#35265;&#36807;&#39046;&#22495;&#19978;&#30340;&#26816;&#32034;&#33021;&#21147;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.10488</link><description>&lt;p&gt;
SPRINT: &#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#21644;&#35299;&#26512;&#38646;&#26679;&#26412;&#31070;&#32463;&#31232;&#30095;&#26816;&#32034;&#30340;&#32479;&#19968;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
SPRINT: A Unified Toolkit for Evaluating and Demystifying Zero-shot Neural Sparse Retrieval. (arXiv:2307.10488v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10488
&lt;/p&gt;
&lt;p&gt;
SPRINT&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;Python&#24037;&#20855;&#21253;&#65292;&#22522;&#20110;Pyserini&#21644;Lucene&#65292;&#25903;&#25345;&#35780;&#20272;&#21644;&#35299;&#26512;&#38646;&#26679;&#26412;&#31070;&#32463;&#31232;&#30095;&#26816;&#32034;&#12290;&#23427;&#35299;&#20915;&#20102;&#32570;&#20047;&#32479;&#19968;&#29615;&#22659;&#21644;&#23454;&#29616;&#22312;&#26410;&#35265;&#36807;&#39046;&#22495;&#19978;&#30340;&#26816;&#32034;&#33021;&#21147;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#31232;&#30095;&#26816;&#32034;&#31995;&#32479;&#20381;&#36182;&#20110;&#35789;&#27719;&#34920;&#31034;&#26469;&#26816;&#32034;&#25991;&#26723;&#65292;&#22914;BM25&#65292;&#22312;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#21344;&#20027;&#23548;&#22320;&#20301;&#12290;&#38543;&#30528;&#35832;&#22914;BERT&#36825;&#26679;&#30340;&#39044;&#35757;&#32451;transformer&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#31070;&#32463;&#31232;&#30095;&#26816;&#32034;&#24341;&#39046;&#20102;&#26816;&#32034;&#20013;&#30340;&#26032;&#33539;&#24335;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#30446;&#21069;&#32570;&#20047;&#25903;&#25345;&#19981;&#21516;&#31232;&#30095;&#26816;&#32034;&#22120;&#22312;&#32479;&#19968;&#30340;&#29615;&#22659;&#20013;&#36816;&#34892;&#30340;&#36719;&#20214;&#12290;&#36825;&#22952;&#30861;&#20102;&#23454;&#36341;&#32773;&#20844;&#27491;&#22320;&#27604;&#36739;&#19981;&#21516;&#30340;&#31232;&#30095;&#27169;&#22411;&#65292;&#24182;&#33719;&#24471;&#30495;&#23454;&#30340;&#35780;&#20272;&#32467;&#26524;&#12290;&#36824;&#26377;&#19968;&#20010;&#32570;&#22833;&#30340;&#37096;&#20998;&#26159;&#65292;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#24037;&#20316;&#26159;&#23545;&#31232;&#30095;&#26816;&#32034;&#27169;&#22411;&#36827;&#34892;&#22495;&#20869;&#26816;&#32034;&#35780;&#20272;&#65292;&#21363;&#20165;&#22312;&#19968;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#65306;MS MARCO&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#26816;&#32034;&#31995;&#32479;&#20013;&#65292;&#19968;&#20010;&#37325;&#35201;&#30340;&#35201;&#27714;&#26159;&#27169;&#22411;&#33021;&#22815;&#22312;&#26410;&#35265;&#36807;&#30340;&#22495;&#22806;&#65292;&#21363;&#38646;&#26679;&#26412;&#26816;&#32034;&#20219;&#21153;&#20013;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;SPRINT&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;Pyserini&#21644;Lucene&#30340;&#32479;&#19968;Python&#24037;&#20855;&#21253;&#65292;&#25903;&#25345;&#31070;&#32463;&#31232;&#30095;&#26816;&#32034;&#30340;&#36890;&#29992;&#25509;&#21475;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditionally, sparse retrieval systems relied on lexical representations to retrieve documents, such as BM25, dominated information retrieval tasks. With the onset of pre-trained transformer models such as BERT, neural sparse retrieval has led to a new paradigm within retrieval. Despite the success, there has been limited software supporting different sparse retrievers running in a unified, common environment. This hinders practitioners from fairly comparing different sparse models and obtaining realistic evaluation results. Another missing piece is, that a majority of prior work evaluates sparse retrieval models on in-domain retrieval, i.e. on a single dataset: MS MARCO. However, a key requirement in practical retrieval systems requires models that can generalize well to unseen out-of-domain, i.e. zero-shot retrieval tasks. In this work, we provide SPRINT, a unified Python toolkit based on Pyserini and Lucene, supporting a common interface for evaluating neural sparse retrieval. The 
&lt;/p&gt;</description></item><item><title>FinGPT&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#23558;&#20114;&#32852;&#32593;&#35268;&#27169;&#30340;&#37329;&#34701;&#25968;&#25454;&#27665;&#20027;&#21270;&#20026;&#37329;&#34701;&#22823;&#35821;&#35328;&#27169;&#22411;&#12290;&#23427;&#25552;&#20379;&#20102;&#33258;&#21160;&#25910;&#38598;&#21644;&#25972;&#29702;&#23454;&#26102;&#37329;&#34701;&#25968;&#25454;&#30340;&#21151;&#33021;&#65292;&#35299;&#20915;&#20102;&#37329;&#34701;&#25991;&#26412;&#25968;&#25454;&#31232;&#32570;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.10485</link><description>&lt;p&gt;
FinGPT: &#23558;&#20114;&#32852;&#32593;&#35268;&#27169;&#30340;&#37329;&#34701;&#25968;&#25454;&#27665;&#20027;&#21270;&#20026;&#37329;&#34701;&#22823;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
FinGPT: Democratizing Internet-scale Data for Financial Large Language Models. (arXiv:2307.10485v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10485
&lt;/p&gt;
&lt;p&gt;
FinGPT&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#23558;&#20114;&#32852;&#32593;&#35268;&#27169;&#30340;&#37329;&#34701;&#25968;&#25454;&#27665;&#20027;&#21270;&#20026;&#37329;&#34701;&#22823;&#35821;&#35328;&#27169;&#22411;&#12290;&#23427;&#25552;&#20379;&#20102;&#33258;&#21160;&#25910;&#38598;&#21644;&#25972;&#29702;&#23454;&#26102;&#37329;&#34701;&#25968;&#25454;&#30340;&#21151;&#33021;&#65292;&#35299;&#20915;&#20102;&#37329;&#34701;&#25991;&#26412;&#25968;&#25454;&#31232;&#32570;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#29702;&#35299;&#21644;&#29983;&#25104;&#31867;&#20284;&#20154;&#31867;&#25991;&#26412;&#26041;&#38754;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#36825;&#21487;&#33021;&#20250;&#24443;&#24213;&#25913;&#21464;&#37329;&#34701;&#34892;&#19994;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;LLM&#22312;&#37329;&#34701;&#39046;&#22495;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#65292;&#20027;&#35201;&#21407;&#22240;&#26159;&#19968;&#33324;&#25991;&#26412;&#25968;&#25454;&#19982;&#37329;&#34701;&#25991;&#26412;&#25968;&#25454;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#29616;&#26377;&#30340;&#37329;&#34701;&#25991;&#26412;&#25968;&#25454;&#38598;&#25968;&#37327;&#26377;&#38480;&#65288;&#22823;&#23567;&#36739;&#23567;&#65289;&#65292;&#32780;&#31532;&#19968;&#20010;&#37329;&#34701;LLM&#65288;FinLLM&#65289;BloombergGPT&#26159;&#23553;&#38381;&#30340;&#65288;&#21482;&#21457;&#24067;&#20102;&#35757;&#32451;&#26085;&#24535;&#65289;&#12290;&#37492;&#20110;&#27492;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;Internet&#35268;&#27169;&#30340;&#37329;&#34701;&#25968;&#25454;&#23558;LLM&#27665;&#20027;&#21270;&#65292;&#30001;&#20110;&#25968;&#25454;&#26469;&#28304;&#22810;&#26679;&#12289;&#20449;&#22122;&#27604;&#20302;&#21644;&#26102;&#38388;&#26377;&#25928;&#24615;&#39640;&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#25918;&#24615;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#24320;&#28304;&#21644;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#26694;&#26550;&#8220;&#37329;&#34701;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#65288;FinGPT&#65289;&#8221;&#65292;&#23427;&#21487;&#20197;&#33258;&#21160;&#25910;&#38598;&#21644;&#25972;&#29702;&#26469;&#33258;&#20114;&#32852;&#32593;&#19978;&#36229;&#36807;34&#20010;&#19981;&#21516;&#26469;&#28304;&#30340;&#23454;&#26102;&#37329;&#34701;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated remarkable proficiency in understanding and generating human-like texts, which may potentially revolutionize the finance industry. However, existing LLMs often fall short in the financial field, which is mainly attributed to the disparities between general text data and financial text data. Unfortunately, there is only a limited number of financial text datasets available (quite small size), and BloombergGPT, the first financial LLM (FinLLM), is close-sourced (only the training logs were released). In light of this, we aim to democratize Internet-scale financial data for LLMs, which is an open challenge due to diverse data sources, low signal-to-noise ratio, and high time-validity. To address the challenges, we introduce an open-sourced and data-centric framework, \textit{Financial Generative Pre-trained Transformer (FinGPT)}, that automates the collection and curation of real-time financial data from &gt;34 diverse sources on the Internet, p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#38646;&#26679;&#26412;&#25552;&#31034;&#35780;&#20272;&#25351;&#20196;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#35782;&#21035;&#20559;&#35265;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;Alpaca 7B&#22312;&#20559;&#35265;&#35782;&#21035;&#20219;&#21153;&#20013;&#30340;&#26368;&#20339;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#25193;&#22823;&#27169;&#22411;&#22823;&#23567;&#21644;&#25968;&#25454;&#22810;&#26679;&#24615;&#21487;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.10472</link><description>&lt;p&gt;
&#36890;&#36807;&#25552;&#31034;&#65292;&#25351;&#20196;&#24494;&#35843;&#30340;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#35782;&#21035;&#31038;&#20250;&#20559;&#35265;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Instruction Fine-Tuned Language Models Identify Social Bias through Prompting?. (arXiv:2307.10472v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10472
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#38646;&#26679;&#26412;&#25552;&#31034;&#35780;&#20272;&#25351;&#20196;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#35782;&#21035;&#20559;&#35265;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;Alpaca 7B&#22312;&#20559;&#35265;&#35782;&#21035;&#20219;&#21153;&#20013;&#30340;&#26368;&#20339;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#25193;&#22823;&#27169;&#22411;&#22823;&#23567;&#21644;&#25968;&#25454;&#22810;&#26679;&#24615;&#21487;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#30340;&#24191;&#24230;&#21644;&#28145;&#24230;&#19981;&#26029;&#25193;&#23637;&#65292;&#26500;&#24314;&#26377;&#25928;&#30340;&#26694;&#26550;&#26469;&#34913;&#37327;&#21644;&#20943;&#36731;&#36825;&#20123;&#27169;&#22411;&#23398;&#20064;&#25110;&#32487;&#25215;&#30340;&#31038;&#20250;&#20559;&#35265;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#25351;&#20196;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#38646;&#26679;&#26412;&#25552;&#31034;&#65288;&#21253;&#25324;&#24605;&#32500;&#38142;&#25552;&#31034;&#65289;&#35782;&#21035;&#20559;&#35265;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;&#22312;LLaMA&#21450;&#20854;&#20004;&#20010;&#25351;&#20196;&#24494;&#35843;&#29256;&#26412;&#20013;&#65292;Alpaca 7B&#22312;&#20559;&#35265;&#35782;&#21035;&#20219;&#21153;&#20013;&#34920;&#29616;&#26368;&#22909;&#65292;&#20934;&#30830;&#29575;&#36798;56.7%&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25193;&#22823;&#35821;&#35328;&#27169;&#22411;&#22823;&#23567;&#21644;&#25968;&#25454;&#22810;&#26679;&#24615;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;&#36825;&#26159;&#25105;&#20204;&#20559;&#35265;&#32531;&#35299;&#26694;&#26550;&#30340;&#31532;&#19968;&#37096;&#20998;&#65292;&#27491;&#22312;&#36827;&#34892;&#30340;&#24037;&#20316;&#12290;&#25105;&#20204;&#23558;&#26681;&#25454;&#33719;&#24471;&#30340;&#26356;&#22810;&#32467;&#26524;&#19981;&#26029;&#26356;&#26032;&#26412;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the breadth and depth of language model applications continue to expand rapidly, it is increasingly important to build efficient frameworks for measuring and mitigating the learned or inherited social biases of these models. In this paper, we present our work on evaluating instruction fine-tuned language models' ability to identify bias through zero-shot prompting, including Chain-of-Thought (CoT) prompts. Across LLaMA and its two instruction fine-tuned versions, Alpaca 7B performs best on the bias identification task with an accuracy of 56.7%. We also demonstrate that scaling up LLM size and data diversity could lead to further performance gain. This is a work-in-progress presenting the first component of our bias mitigation framework. We will keep updating this work as we get more results.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20027;&#35201;&#30740;&#31350;&#20102;&#19987;&#21033;&#22270;&#20687;&#20013;&#21487;&#35270;&#21270;&#31867;&#22411;&#21644;&#35270;&#35282;&#30340;&#20998;&#31867;&#38382;&#39064;&#65292;&#25193;&#23637;&#20102;CLEF-IP&#25968;&#25454;&#38598;&#24182;&#37319;&#29992;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20102;&#20998;&#31867;&#12290;&#36825;&#39033;&#30740;&#31350;&#23545;&#20110;&#20419;&#36827;&#19987;&#21033;&#25506;&#32034;&#21644;&#26816;&#32034;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2307.10471</link><description>&lt;p&gt;
&#19987;&#21033;&#20013;&#21487;&#35270;&#21270;&#31867;&#22411;&#21644;&#35270;&#35282;&#30340;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Classification of Visualization Types and Perspectives in Patents. (arXiv:2307.10471v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10471
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20027;&#35201;&#30740;&#31350;&#20102;&#19987;&#21033;&#22270;&#20687;&#20013;&#21487;&#35270;&#21270;&#31867;&#22411;&#21644;&#35270;&#35282;&#30340;&#20998;&#31867;&#38382;&#39064;&#65292;&#25193;&#23637;&#20102;CLEF-IP&#25968;&#25454;&#38598;&#24182;&#37319;&#29992;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20102;&#20998;&#31867;&#12290;&#36825;&#39033;&#30740;&#31350;&#23545;&#20110;&#20419;&#36827;&#19987;&#21033;&#25506;&#32034;&#21644;&#26816;&#32034;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#27599;&#24180;&#19987;&#21033;&#30003;&#35831;&#25968;&#37327;&#30340;&#36805;&#36895;&#22686;&#38271;&#65292;&#20419;&#36827;&#19987;&#21033;&#25506;&#32034;&#21644;&#26816;&#32034;&#30340;&#20449;&#24687;&#21644;&#22810;&#23186;&#20307;&#26816;&#32034;&#26041;&#27861;&#33267;&#20851;&#37325;&#35201;&#12290;&#19981;&#21516;&#31867;&#22411;&#30340;&#21487;&#35270;&#21270;&#65288;&#20363;&#22914;&#65292;&#22270;&#24418;&#12289;&#25216;&#26415;&#22270;&#32440;&#65289;&#21644;&#35270;&#35282;&#65288;&#20363;&#22914;&#65292;&#20391;&#35270;&#12289;&#36879;&#35270;&#65289;&#34987;&#29992;&#26469;&#21487;&#35270;&#21270;&#19987;&#21033;&#21019;&#26032;&#30340;&#32454;&#33410;&#12290;&#23545;&#36825;&#20123;&#22270;&#20687;&#30340;&#20998;&#31867;&#21487;&#20197;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#25628;&#32034;&#24182;&#36827;&#34892;&#36827;&#19968;&#27493;&#20998;&#26512;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#29992;&#20110;&#22270;&#20687;&#31867;&#22411;&#20998;&#31867;&#30340;&#25968;&#25454;&#38598;&#32570;&#23569;&#19968;&#20123;&#37325;&#35201;&#30340;&#19987;&#21033;&#21487;&#35270;&#21270;&#31867;&#22411;&#12290;&#27492;&#22806;&#65292;&#30456;&#20851;&#30740;&#31350;&#27809;&#26377;&#20351;&#29992;&#21253;&#25324;transformers&#22312;&#20869;&#30340;&#26368;&#26032;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#20998;&#31867;&#19987;&#21033;&#22270;&#20687;&#20013;&#30340;&#21487;&#35270;&#21270;&#31867;&#22411;&#21644;&#35270;&#35282;&#12290;&#25105;&#20204;&#23545;&#19987;&#21033;&#20013;&#22270;&#20687;&#31867;&#22411;&#20998;&#31867;&#30340;CLEF-IP&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#25193;&#23637;&#65292;&#22686;&#21152;&#21040;&#20102;&#21313;&#20010;&#31867;&#21035;&#65292;&#24182;&#25552;&#20379;&#20102;&#25163;&#21160;&#26631;&#27880;&#30340;&#30495;&#23454;&#26631;&#31614;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20174;&#19968;&#20010;&#25968;&#25454;&#38598;&#20013;&#25512;&#23548;&#20986;&#19968;&#32452;&#23618;&#32423;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the swift growth of patent applications each year, information and multimedia retrieval approaches that facilitate patent exploration and retrieval are of utmost importance. Different types of visualizations (e.g., graphs, technical drawings) and perspectives (e.g., side view, perspective) are used to visualize details of innovations in patents. The classification of these images enables a more efficient search and allows for further analysis. So far, datasets for image type classification miss some important visualization types for patents. Furthermore, related work does not make use of recent deep learning approaches including transformers. In this paper, we adopt state-of-the-art deep learning methods for the classification of visualization types and perspectives in patent images. We extend the CLEF-IP dataset for image type classification in patents to ten classes and provide manual ground truth annotations. In addition, we derive a set of hierarchical classes from a dataset
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#25968;&#25454;&#31185;&#23398;&#30340;&#20215;&#20540;&#21462;&#21521;&#65292;&#25506;&#35752;&#20102;&#20854;&#29305;&#24449;&#21644;&#20316;&#29992;&#12290;&#25968;&#25454;&#31185;&#23398;&#19981;&#26159;&#19968;&#38376;&#31185;&#23398;&#65292;&#32780;&#26159;&#19968;&#31181;&#30740;&#31350;&#33539;&#24335;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21644;&#37325;&#22823;&#30340;&#24433;&#21709;&#65292;&#20294;&#20063;&#23384;&#22312;&#30528;&#26410;&#30693;&#30340;&#39118;&#38505;&#12290;&#36825;&#19968;&#39046;&#22495;&#20173;&#28982;&#22788;&#20110;&#21021;&#32423;&#38454;&#27573;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2307.10460</link><description>&lt;p&gt;
&#25968;&#25454;&#31185;&#23398;&#30340;&#20215;&#20540;&#21462;&#21521;&#65306;&#25968;&#25454;&#31185;&#23398;&#30340;&#26412;&#36136;&#12289;&#20215;&#20540;&#21644;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
A data science axiology: the nature, value, and risks of data science. (arXiv:2307.10460v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10460
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#25968;&#25454;&#31185;&#23398;&#30340;&#20215;&#20540;&#21462;&#21521;&#65292;&#25506;&#35752;&#20102;&#20854;&#29305;&#24449;&#21644;&#20316;&#29992;&#12290;&#25968;&#25454;&#31185;&#23398;&#19981;&#26159;&#19968;&#38376;&#31185;&#23398;&#65292;&#32780;&#26159;&#19968;&#31181;&#30740;&#31350;&#33539;&#24335;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21644;&#37325;&#22823;&#30340;&#24433;&#21709;&#65292;&#20294;&#20063;&#23384;&#22312;&#30528;&#26410;&#30693;&#30340;&#39118;&#38505;&#12290;&#36825;&#19968;&#39046;&#22495;&#20173;&#28982;&#22788;&#20110;&#21021;&#32423;&#38454;&#27573;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#31185;&#23398;&#19981;&#26159;&#19968;&#38376;&#31185;&#23398;&#65292;&#32780;&#26159;&#19968;&#31181;&#30740;&#31350;&#33539;&#24335;&#65292;&#20855;&#26377;&#26080;&#27861;&#39044;&#27979;&#30340;&#33539;&#22260;&#12289;&#35268;&#27169;&#12289;&#22797;&#26434;&#24615;&#21644;&#30693;&#35782;&#21457;&#29616;&#33021;&#21147;&#65292;&#36825;&#26159;&#20854;&#20182;&#26041;&#24335;&#26080;&#27861;&#23454;&#29616;&#30340;&#65292;&#24182;&#19988;&#21487;&#33021;&#36229;&#20986;&#20154;&#31867;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#23427;&#24050;&#32463;&#22312;AI&#20891;&#22791;&#31454;&#36187;&#20013;&#24191;&#27867;&#24212;&#29992;&#20110;&#25968;&#20197;&#19975;&#35745;&#30340;&#24212;&#29992;&#31243;&#24207;&#20013;&#65292;&#24050;&#32463;&#23454;&#36136;&#24615;&#22320;&#25913;&#21464;&#20102;&#25105;&#20204;&#30340;&#19990;&#30028;&#65292;&#20294;&#30001;&#20110;&#20854;&#19981;&#21487;&#24605;&#35758;&#30340;&#22797;&#26434;&#24615;&#65292;&#21487;&#33021;&#24102;&#26469;&#26410;&#30693;&#30340;&#39118;&#38505;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#25968;&#25454;&#31185;&#23398;&#30340;&#20215;&#20540;&#21462;&#21521;&#65292;&#25506;&#35752;&#21644;&#35780;&#20272;&#20102;&#20854;&#26174;&#33879;&#32780;&#20915;&#23450;&#24615;&#30340;&#29305;&#24449;&#65292;&#20197;&#20415;&#20102;&#35299;&#21644;&#23450;&#20041;&#25968;&#25454;&#31185;&#23398;&#65292;&#35748;&#35782;&#21040;&#20854;&#28508;&#22312;&#30340;&#30410;&#22788;&#12289;&#39118;&#38505;&#21644;&#24320;&#25918;&#24615;&#30740;&#31350;&#25361;&#25112;&#12290;&#22522;&#20110;AI&#30340;&#25968;&#25454;&#31185;&#23398;&#26412;&#36136;&#19978;&#28041;&#21450;&#19981;&#30830;&#23450;&#24615;&#65292;&#36825;&#21487;&#33021;&#27604;&#25105;&#20204;&#23545;&#31185;&#23398;&#30830;&#23450;&#24615;&#30340;&#20559;&#22909;&#26356;&#21152;&#29616;&#23454;&#12290;&#25968;&#25454;&#31185;&#23398;&#23558;&#20135;&#29983;&#36828;&#36828;&#36229;&#20986;&#30693;&#35782;&#21457;&#29616;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data science is not a science. It is a research paradigm with an unfathomed scope, scale, complexity, and power for knowledge discovery that is not otherwise possible and can be beyond human reasoning. It is changing our world practically and profoundly already widely deployed in tens of thousands of applications in every discipline in an AI Arms Race that, due to its inscrutability, can lead to unfathomed risks. This paper presents an axiology of data science, its purpose, nature, importance, risks, and value for problem solving, by exploring and evaluating its remarkable, definitive features. As data science is in its infancy, this initial, speculative axiology is intended to aid in understanding and defining data science to recognize its potential benefits, risks, and open research challenges. AI based data science is inherently about uncertainty that may be more realistic than our preference for the certainty of science. Data science will have impacts far beyond knowledge discovery
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#20855;&#26377;&#30828;&#32422;&#26463;&#36755;&#20986;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26144;&#23556;&#38544;&#34255;&#21442;&#25968;&#21521;&#37327;&#21040;&#19968;&#20010;&#31526;&#21512;&#32422;&#26463;&#38598;&#30340;&#28857;&#23454;&#29616;&#32422;&#26463;&#65292;&#24182;&#36890;&#36807;&#38468;&#21152;&#30340;&#31070;&#32463;&#32593;&#32476;&#23618;&#26469;&#36827;&#34892;&#26144;&#23556;&#12290;&#35813;&#26041;&#27861;&#36824;&#21487;&#20197;&#22788;&#29702;&#19981;&#20165;&#23545;&#36755;&#20986;&#21521;&#37327;&#26045;&#21152;&#32422;&#26463;&#65292;&#36824;&#23545;&#20381;&#36182;&#20110;&#36755;&#20837;&#30340;&#32852;&#21512;&#32422;&#26463;&#26045;&#21152;&#32422;&#26463;&#30340;&#24773;&#20917;&#65292;&#24182;&#19988;&#21487;&#20197;&#22788;&#29702;&#19981;&#21516;&#31867;&#22411;&#30340;&#32422;&#26463;&#65292;&#21253;&#25324;&#32447;&#24615;&#21644;&#20108;&#27425;&#32422;&#26463;&#12289;&#31561;&#24335;&#32422;&#26463;&#21644;&#21160;&#24577;&#32422;&#26463;&#12290;</title><link>http://arxiv.org/abs/2307.10459</link><description>&lt;p&gt;
&#19968;&#31181;&#23454;&#29616;&#20855;&#26377;&#30828;&#32422;&#26463;&#36755;&#20986;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#35745;&#31639;&#31616;&#21333;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A New Computationally Simple Approach for Implementing Neural Networks with Output Hard Constraints. (arXiv:2307.10459v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10459
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#20855;&#26377;&#30828;&#32422;&#26463;&#36755;&#20986;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26144;&#23556;&#38544;&#34255;&#21442;&#25968;&#21521;&#37327;&#21040;&#19968;&#20010;&#31526;&#21512;&#32422;&#26463;&#38598;&#30340;&#28857;&#23454;&#29616;&#32422;&#26463;&#65292;&#24182;&#36890;&#36807;&#38468;&#21152;&#30340;&#31070;&#32463;&#32593;&#32476;&#23618;&#26469;&#36827;&#34892;&#26144;&#23556;&#12290;&#35813;&#26041;&#27861;&#36824;&#21487;&#20197;&#22788;&#29702;&#19981;&#20165;&#23545;&#36755;&#20986;&#21521;&#37327;&#26045;&#21152;&#32422;&#26463;&#65292;&#36824;&#23545;&#20381;&#36182;&#20110;&#36755;&#20837;&#30340;&#32852;&#21512;&#32422;&#26463;&#26045;&#21152;&#32422;&#26463;&#30340;&#24773;&#20917;&#65292;&#24182;&#19988;&#21487;&#20197;&#22788;&#29702;&#19981;&#21516;&#31867;&#22411;&#30340;&#32422;&#26463;&#65292;&#21253;&#25324;&#32447;&#24615;&#21644;&#20108;&#27425;&#32422;&#26463;&#12289;&#31561;&#24335;&#32422;&#26463;&#21644;&#21160;&#24577;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31070;&#32463;&#32593;&#32476;&#36755;&#20986;&#20540;&#19978;&#26045;&#21152;&#30828;&#20984;&#32422;&#26463;&#30340;&#35745;&#31639;&#31616;&#21333;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#36890;&#36807;&#23558;&#32593;&#32476;&#30340;&#38544;&#34255;&#21442;&#25968;&#21521;&#37327;&#26144;&#23556;&#21040;&#19968;&#20010;&#28857;&#65292;&#30830;&#20445;&#23427;&#22312;&#30001;&#19968;&#32452;&#32422;&#26463;&#23450;&#20041;&#30340;&#21487;&#34892;&#38598;&#20869;&#12290;&#26144;&#23556;&#26159;&#36890;&#36807;&#20855;&#26377;&#36755;&#20986;&#32422;&#26463;&#30340;&#38468;&#21152;&#31070;&#32463;&#32593;&#32476;&#23618;&#23454;&#29616;&#30340;&#12290;&#23558;&#35813;&#26041;&#27861;&#31616;&#21333;&#22320;&#25193;&#23637;&#21040;&#19981;&#20165;&#23545;&#36755;&#20986;&#21521;&#37327;&#26045;&#21152;&#32422;&#26463;&#65292;&#36824;&#23545;&#20381;&#36182;&#20110;&#36755;&#20837;&#30340;&#32852;&#21512;&#32422;&#26463;&#26045;&#21152;&#32422;&#26463;&#30340;&#24773;&#20917;&#12290;&#22312;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26694;&#26550;&#20013;&#65292;&#21487;&#20197;&#31616;&#21333;&#22320;&#23454;&#29616;&#23545;&#36755;&#20986;&#30340;&#32422;&#26463;&#25237;&#24433;&#26041;&#27861;&#12290;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#19981;&#21516;&#31867;&#22411;&#30340;&#32422;&#26463;&#24341;&#20837;&#21040;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20013;&#65292;&#21253;&#25324;&#32447;&#24615;&#21644;&#20108;&#27425;&#32422;&#26463;&#12289;&#31561;&#24335;&#32422;&#26463;&#21644;&#21160;&#24577;&#32422;&#26463;&#65292;&#20197;&#21450;&#36793;&#30028;&#24418;&#24335;&#30340;&#32422;&#26463;&#12290;&#35813;&#26041;&#27861;&#30340;&#19968;&#20010;&#37325;&#35201;&#29305;&#28857;&#26159;&#23427;&#30340;&#35745;&#31639;&#31616;&#21333;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
A new computationally simple method of imposing hard convex constraints on the neural network output values is proposed. The key idea behind the method is to map a vector of hidden parameters of the network to a point that is guaranteed to be inside the feasible set defined by a set of constraints. The mapping is implemented by the additional neural network layer with constraints for output. The proposed method is simply extended to the case when constraints are imposed not only on the output vectors, but also on joint constraints depending on inputs. The projection approach to imposing constraints on outputs can simply be implemented in the framework of the proposed method. It is shown how to incorporate different types of constraints into the proposed method, including linear and quadratic constraints, equality constraints, and dynamic constraints, constraints in the form of boundaries. An important feature of the method is its computational simplicity. Complexities of the forward pa
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22823;&#22411;&#25163;&#24037;&#26631;&#35760;&#26118;&#34411;&#22270;&#20687;&#25968;&#25454;&#38598;BIOSCAN-Insect&#65292;&#29992;&#20110;&#23545;&#26118;&#34411;&#29983;&#29289;&#22810;&#26679;&#24615;&#36827;&#34892;&#32534;&#30446;&#12290;&#35813;&#25968;&#25454;&#38598;&#36824;&#20855;&#26377;&#24341;&#20154;&#27880;&#30446;&#30340;&#29305;&#24449;&#65292;&#23545;&#24191;&#27867;&#30340;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#20063;&#20855;&#26377;&#30740;&#31350;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2307.10455</link><description>&lt;p&gt;
&#26397;&#30528;&#20840;&#29699;&#29983;&#29289;&#22810;&#26679;&#24615;&#35780;&#20272;&#36808;&#20986;&#30340;&#19968;&#27493;&#65306;BIOSCAN-1M&#26118;&#34411;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
A Step Towards Worldwide Biodiversity Assessment: The BIOSCAN-1M Insect Dataset. (arXiv:2307.10455v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10455
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22823;&#22411;&#25163;&#24037;&#26631;&#35760;&#26118;&#34411;&#22270;&#20687;&#25968;&#25454;&#38598;BIOSCAN-Insect&#65292;&#29992;&#20110;&#23545;&#26118;&#34411;&#29983;&#29289;&#22810;&#26679;&#24615;&#36827;&#34892;&#32534;&#30446;&#12290;&#35813;&#25968;&#25454;&#38598;&#36824;&#20855;&#26377;&#24341;&#20154;&#27880;&#30446;&#30340;&#29305;&#24449;&#65292;&#23545;&#24191;&#27867;&#30340;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#20063;&#20855;&#26377;&#30740;&#31350;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#23545;&#26118;&#34411;&#29983;&#29289;&#22810;&#26679;&#24615;&#36827;&#34892;&#32534;&#30446;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22823;&#22411;&#25163;&#24037;&#26631;&#35760;&#26118;&#34411;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#21363;BIOSCAN-Insect&#25968;&#25454;&#38598;&#12290;&#27599;&#20010;&#35760;&#24405;&#37117;&#30001;&#19987;&#23478;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#19988;&#20855;&#26377;&#30456;&#20851;&#30340;&#36951;&#20256;&#20449;&#24687;&#65292;&#21253;&#25324;&#21407;&#22987;&#26680;&#33527;&#37240;&#26465;&#24418;&#30721;&#24207;&#21015;&#21644;&#20998;&#37197;&#30340;&#26465;&#24418;&#30721;&#32034;&#24341;&#21495;&#65292;&#36825;&#20123;&#26159;&#22522;&#20110;&#36951;&#20256;&#30340;&#29289;&#31181;&#20998;&#31867;&#30340;&#20195;&#29702;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#31934;&#36873;&#30340;&#30334;&#19975;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#20027;&#35201;&#29992;&#20110;&#35757;&#32451;&#33021;&#22815;&#25552;&#20379;&#22522;&#20110;&#22270;&#20687;&#30340;&#20998;&#31867;&#35780;&#20272;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#65292;&#20294;&#35813;&#25968;&#25454;&#38598;&#36824;&#20855;&#26377;&#24341;&#20154;&#27880;&#30446;&#30340;&#29305;&#24449;&#65292;&#23545;&#24191;&#27867;&#30340;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#20063;&#20855;&#26377;&#30740;&#31350;&#20215;&#20540;&#12290;&#30001;&#20110;&#25968;&#25454;&#38598;&#22266;&#26377;&#30340;&#29983;&#29289;&#24615;&#36136;&#65292;&#23637;&#29616;&#20986;&#20102;&#20855;&#26377;&#38271;&#23614;&#31867;&#21035;&#19981;&#24179;&#34913;&#20998;&#24067;&#30340;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#20998;&#31867;&#26631;&#31614;&#26159;&#19968;&#20010;&#20998;&#23618;&#20998;&#31867;&#26041;&#26696;&#65292;&#22312;&#36739;&#20302;&#32423;&#21035;&#19978;&#21576;&#29616;&#20986;&#39640;&#24230;&#32454;&#31890;&#24230;&#30340;&#20998;&#31867;&#38382;&#39064;&#12290;&#38500;&#20102;&#28608;&#21457;&#23545;&#29983;&#29289;&#22810;&#26679;&#24615;&#30740;&#31350;&#30340;&#20852;&#36259;&#22806;&#65292;&#35813;&#25968;&#25454;&#38598;&#36824;&#20419;&#36827;&#20102;&#23545;&#26426;&#22120;&#23398;&#20064;&#30340;&#28145;&#20837;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
In an effort to catalog insect biodiversity, we propose a new large dataset of hand-labelled insect images, the BIOSCAN-Insect Dataset. Each record is taxonomically classified by an expert, and also has associated genetic information including raw nucleotide barcode sequences and assigned barcode index numbers, which are genetically-based proxies for species classification. This paper presents a curated million-image dataset, primarily to train computer-vision models capable of providing image-based taxonomic assessment, however, the dataset also presents compelling characteristics, the study of which would be of interest to the broader machine learning community. Driven by the biological nature inherent to the dataset, a characteristic long-tailed class-imbalance distribution is exhibited. Furthermore, taxonomic labelling is a hierarchical classification scheme, presenting a highly fine-grained classification problem at lower levels. Beyond spurring interest in biodiversity research w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#65292;&#20351;&#29992;&#22270;&#22686;&#24378;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#23558;&#20174;&#24322;&#26500;&#22270;&#20013;&#23548;&#20986;&#30340;&#25512;&#29702;&#30693;&#35782;&#25972;&#21512;&#21040;&#21464;&#21387;&#22120;&#26550;&#26500;&#20013;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#21464;&#21387;&#22120;&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#38480;&#21046;&#12290;&#36890;&#36807;&#20840;&#23616;-&#23616;&#37096;&#27880;&#24847;&#21147;&#12289;&#22270;&#27880;&#24847;&#21147;&#21644;&#20851;&#31995;&#31867;&#22411;&#32771;&#34385;&#65292;&#20248;&#21270;&#20102;&#23454;&#20307;&#21644;&#21333;&#35789;&#20043;&#38388;&#30340;&#27880;&#24847;&#21147;&#12290;&#35813;&#27169;&#24335;&#19982;&#30456;&#23545;&#20301;&#32622;&#26631;&#31614;&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#19982;LUKE&#30340;&#23454;&#20307;&#24863;&#30693;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#30456;&#38598;&#25104;&#12290;</title><link>http://arxiv.org/abs/2307.10443</link><description>&lt;p&gt;
&#20351;&#29992;&#30456;&#23545;&#20301;&#32622;&#26631;&#31614;&#23558;&#24322;&#26500;&#22270;&#19982;&#23454;&#20307;&#24863;&#30693;&#33258;&#27880;&#24847;&#21147;&#30456;&#32467;&#21512;&#30340;&#38405;&#35835;&#29702;&#35299;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Integrating a Heterogeneous Graph with Entity-aware Self-attention using Relative Position Labels for Reading Comprehension Model. (arXiv:2307.10443v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10443
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#65292;&#20351;&#29992;&#22270;&#22686;&#24378;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#23558;&#20174;&#24322;&#26500;&#22270;&#20013;&#23548;&#20986;&#30340;&#25512;&#29702;&#30693;&#35782;&#25972;&#21512;&#21040;&#21464;&#21387;&#22120;&#26550;&#26500;&#20013;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#21464;&#21387;&#22120;&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#38480;&#21046;&#12290;&#36890;&#36807;&#20840;&#23616;-&#23616;&#37096;&#27880;&#24847;&#21147;&#12289;&#22270;&#27880;&#24847;&#21147;&#21644;&#20851;&#31995;&#31867;&#22411;&#32771;&#34385;&#65292;&#20248;&#21270;&#20102;&#23454;&#20307;&#21644;&#21333;&#35789;&#20043;&#38388;&#30340;&#27880;&#24847;&#21147;&#12290;&#35813;&#27169;&#24335;&#19982;&#30456;&#23545;&#20301;&#32622;&#26631;&#31614;&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#19982;LUKE&#30340;&#23454;&#20307;&#24863;&#30693;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#30456;&#38598;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#21464;&#21387;&#22120;&#27169;&#22411;&#22312;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#36755;&#20837;&#24207;&#21015;&#20013;&#32570;&#23569;&#26174;&#24335;&#30693;&#35782;&#65292;&#23427;&#20204;&#20173;&#28982;&#38754;&#20020;&#22788;&#29702;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#30340;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#26469;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#23427;&#21033;&#29992;&#22686;&#24378;&#22270;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#23558;&#30001;&#24322;&#26500;&#22270;&#23548;&#20986;&#30340;&#25512;&#29702;&#30693;&#35782;&#25972;&#21512;&#21040;&#21464;&#21387;&#22120;&#26550;&#26500;&#20013;&#12290;&#25552;&#20986;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#21253;&#25324;&#19977;&#20010;&#20851;&#38190;&#35201;&#32032;&#65306;&#21333;&#35789;&#26631;&#35760;&#30340;&#20840;&#23616;-&#23616;&#37096;&#27880;&#24847;&#21147;&#65292;&#23545;&#23454;&#20307;&#26631;&#35760;&#30340;&#22270;&#27880;&#24847;&#21147;&#65292;&#23454;&#20307;&#26631;&#35760;&#23545;&#30456;&#20851;&#32852;&#30340;&#26631;&#35760;&#26174;&#31034;&#24378;&#28872;&#30340;&#27880;&#24847;&#21147;&#32780;&#23545;&#19981;&#30456;&#20851;&#30340;&#26631;&#35760;&#26174;&#31034;&#36739;&#24369;&#30340;&#27880;&#24847;&#21147;&#65292;&#20197;&#21450;&#32771;&#34385;&#27599;&#20010;&#23454;&#20307;&#26631;&#35760;&#19982;&#21333;&#35789;&#26631;&#35760;&#20043;&#38388;&#30340;&#20851;&#31995;&#31867;&#22411;&#12290;&#36825;&#26679;&#65292;&#22914;&#26524;&#23384;&#22312;&#20851;&#31995;&#65292;&#21017;&#21487;&#20197;&#20248;&#21270;&#20004;&#32773;&#20043;&#38388;&#30340;&#27880;&#24847;&#21147;&#12290;&#35813;&#27169;&#24335;&#19982;&#29305;&#27530;&#30340;&#30456;&#23545;&#20301;&#32622;&#26631;&#31614;&#30456;&#32467;&#21512;&#65292;&#20351;&#20854;&#33021;&#22815;&#19982;LUKE&#30340;&#23454;&#20307;&#24863;&#30693;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#30456;&#38598;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the significant progress made by transformer models in machine reading comprehension tasks, they still face limitations in handling complex reasoning tasks due to the absence of explicit knowledge in the input sequence. This paper proposes a novel attention pattern to overcome this limitation, which integrates reasoning knowledge derived from a heterogeneous graph into the transformer architecture using a graph-enhanced self-attention mechanism. The proposed attention pattern comprises three key elements: global-local attention for word tokens, graph attention for entity tokens that exhibit strong attention towards tokens connected in the graph as opposed to those unconnected, and the consideration of the type of relationship between each entity token and word token. This results in optimized attention between the two if a relationship exists. The pattern is coupled with special relative position labels, allowing it to integrate with LUKE's entity-aware self-attention mechanism
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#21322;&#30417;&#30563;&#29615;&#22659;&#30340;&#32622;&#20449;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#26816;&#26597;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#39044;&#27979;&#19968;&#33268;&#24615;&#65292;&#21363;&#20351;&#21482;&#26377;&#26377;&#38480;&#30340;&#35757;&#32451;&#26631;&#31614;&#65292;&#25105;&#20204;&#20173;&#28982;&#21487;&#20197;&#21512;&#29702;&#22320;&#36817;&#20284;&#27169;&#22411;&#23545;&#26410;&#26631;&#35760;&#26679;&#26412;&#30340;&#32622;&#20449;&#24230;&#12290;&#35813;&#26041;&#27861;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#20998;&#21106;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#36890;&#36807;&#19979;&#28216;&#20027;&#21160;&#23398;&#20064;&#20219;&#21153;&#23637;&#31034;&#20102;&#20854;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2307.10440</link><description>&lt;p&gt;
&#20351;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#32622;&#20449;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Confidence Estimation Using Unlabeled Data. (arXiv:2307.10440v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10440
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#21322;&#30417;&#30563;&#29615;&#22659;&#30340;&#32622;&#20449;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#26816;&#26597;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#39044;&#27979;&#19968;&#33268;&#24615;&#65292;&#21363;&#20351;&#21482;&#26377;&#26377;&#38480;&#30340;&#35757;&#32451;&#26631;&#31614;&#65292;&#25105;&#20204;&#20173;&#28982;&#21487;&#20197;&#21512;&#29702;&#22320;&#36817;&#20284;&#27169;&#22411;&#23545;&#26410;&#26631;&#35760;&#26679;&#26412;&#30340;&#32622;&#20449;&#24230;&#12290;&#35813;&#26041;&#27861;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#20998;&#21106;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#36890;&#36807;&#19979;&#28216;&#20027;&#21160;&#23398;&#20064;&#20219;&#21153;&#23637;&#31034;&#20102;&#20854;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#24230;&#33258;&#20449;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24120;&#35265;&#38382;&#39064;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#37096;&#32626;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#20272;&#35745;&#32622;&#20449;&#24230;&#65292;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#20840;&#30417;&#30563;&#22330;&#26223;&#65292;&#24182;&#20381;&#36182;&#20110;&#35757;&#32451;&#26631;&#31614;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#31181;&#36866;&#29992;&#20110;&#21322;&#30417;&#30563;&#29615;&#22659;&#30340;&#32622;&#20449;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#24403;&#22823;&#37096;&#20998;&#35757;&#32451;&#26631;&#31614;&#19981;&#21487;&#29992;&#26102;&#12290;&#25105;&#20204;&#35748;&#20026;&#21363;&#20351;&#21482;&#26377;&#26377;&#38480;&#30340;&#35757;&#32451;&#26631;&#31614;&#65292;&#36890;&#36807;&#26816;&#26597;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#39044;&#27979;&#19968;&#33268;&#24615;&#65292;&#25105;&#20204;&#20173;&#28982;&#21487;&#20197;&#21512;&#29702;&#22320;&#36817;&#20284;&#27169;&#22411;&#23545;&#26410;&#26631;&#35760;&#26679;&#26412;&#30340;&#32622;&#20449;&#24230;&#12290;&#25105;&#20204;&#20351;&#29992;&#35757;&#32451;&#19968;&#33268;&#24615;&#20316;&#20026;&#26367;&#20195;&#20989;&#25968;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32622;&#20449;&#24230;&#20272;&#35745;&#30340;&#19968;&#33268;&#24615;&#25490;&#24207;&#25439;&#22833;&#12290;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#20998;&#21106;&#20219;&#21153;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#32622;&#20449;&#24230;&#20272;&#35745;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#19979;&#28216;&#20027;&#21160;&#23398;&#20064;&#20219;&#21153;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/TopoXLab/consistency-ranking-loss&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Overconfidence is a common issue for deep neural networks, limiting their deployment in real-world applications. To better estimate confidence, existing methods mostly focus on fully-supervised scenarios and rely on training labels. In this paper, we propose the first confidence estimation method for a semi-supervised setting, when most training labels are unavailable. We stipulate that even with limited training labels, we can still reasonably approximate the confidence of model on unlabeled samples by inspecting the prediction consistency through the training process. We use training consistency as a surrogate function and propose a consistency ranking loss for confidence estimation. On both image classification and segmentation tasks, our method achieves state-of-the-art performances in confidence estimation. Furthermore, we show the benefit of the proposed method through a downstream active learning task. The code is available at https://github.com/TopoXLab/consistency-ranking-loss
&lt;/p&gt;</description></item><item><title>&#29992;&#20110;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#36890;&#24120;&#26080;&#27861;&#37327;&#21270;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;AutoGNNUQ&#65292;&#36890;&#36807;&#26550;&#26500;&#25628;&#32034;&#29983;&#25104;&#39640;&#24615;&#33021;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#38598;&#21512;&#65292;&#24182;&#21033;&#29992;&#26041;&#24046;&#20998;&#35299;&#23558;&#25968;&#25454;&#21644;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#20998;&#24320;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#20943;&#23569;&#19981;&#30830;&#23450;&#24615;&#30340;&#26377;&#20215;&#20540;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2307.10438</link><description>&lt;p&gt;
&#29992;&#22270;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#36827;&#34892;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Uncertainty Quantification for Molecular Property Predictions with Graph Neural Architecture Search. (arXiv:2307.10438v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10438
&lt;/p&gt;
&lt;p&gt;
&#29992;&#20110;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#36890;&#24120;&#26080;&#27861;&#37327;&#21270;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;AutoGNNUQ&#65292;&#36890;&#36807;&#26550;&#26500;&#25628;&#32034;&#29983;&#25104;&#39640;&#24615;&#33021;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#38598;&#21512;&#65292;&#24182;&#21033;&#29992;&#26041;&#24046;&#20998;&#35299;&#23558;&#25968;&#25454;&#21644;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#20998;&#24320;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#20943;&#23569;&#19981;&#30830;&#23450;&#24615;&#30340;&#26377;&#20215;&#20540;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24050;&#25104;&#20026;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#20013;&#31361;&#20986;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20856;&#22411;GNN&#27169;&#22411;&#30340;&#19968;&#20010;&#20851;&#38190;&#38480;&#21046;&#26159;&#26080;&#27861;&#37327;&#21270;&#39044;&#27979;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#36825;&#31181;&#33021;&#21147;&#23545;&#20110;&#30830;&#20445;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#21487;&#20449;&#22320;&#20351;&#29992;&#21644;&#37096;&#32626;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;AutoGNNUQ&#65292;&#19968;&#31181;&#33258;&#21160;&#21270;&#30340;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#12290;AutoGNNUQ&#21033;&#29992;&#26550;&#26500;&#25628;&#32034;&#29983;&#25104;&#19968;&#32452;&#39640;&#24615;&#33021;&#30340;GNN&#38598;&#21512;&#65292;&#33021;&#22815;&#20272;&#35745;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#26041;&#24046;&#20998;&#35299;&#26469;&#20998;&#31163;&#25968;&#25454;&#65288;aleatoric&#65289;&#21644;&#27169;&#22411;&#65288;epistemic&#65289;&#19981;&#30830;&#23450;&#24615;&#65292;&#20026;&#20943;&#23569;&#23427;&#20204;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;&#22312;&#25105;&#20204;&#30340;&#35745;&#31639;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;AutoGNNUQ&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#27979;&#37327;&#24615;&#33021;&#26041;&#38754;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;t-SNE&#21487;&#35270;&#21270;&#26469;&#35299;&#37322;&#19981;&#30830;&#23450;&#24615;&#30340;&#26469;&#28304;&#21644;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have emerged as a prominent class of data-driven methods for molecular property prediction. However, a key limitation of typical GNN models is their inability to quantify uncertainties in the predictions. This capability is crucial for ensuring the trustworthy use and deployment of models in downstream tasks. To that end, we introduce AutoGNNUQ, an automated uncertainty quantification (UQ) approach for molecular property prediction. AutoGNNUQ leverages architecture search to generate an ensemble of high-performing GNNs, enabling the estimation of predictive uncertainties. Our approach employs variance decomposition to separate data (aleatoric) and model (epistemic) uncertainties, providing valuable insights for reducing them. In our computational experiments, we demonstrate that AutoGNNUQ outperforms existing UQ methods in terms of both prediction accuracy and UQ performance on multiple benchmark datasets. Additionally, we utilize t-SNE visualization to exp
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#32534;&#31243;&#26041;&#27861;&#65292;&#29992;&#20110;&#26657;&#20934;&#21644;&#39564;&#35777;&#36710;&#36742;&#36319;&#38543;&#27169;&#22411;&#65292;&#20197;&#25429;&#25417;&#21644;&#22797;&#21046;&#24037;&#20316;&#21306;&#20869;&#22806;&#30340;&#39550;&#39542;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2307.10437</link><description>&lt;p&gt;
&#29992;&#26377;&#38480;&#25968;&#25454;&#36827;&#34892;&#36125;&#21494;&#26031;&#32534;&#31243;&#26041;&#27861;&#30340;&#36710;&#36742;&#36319;&#38543;&#27169;&#22411;&#26657;&#20934;&#21644;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
A Bayesian Programming Approach to Car-following Model Calibration and Validation using Limited Data. (arXiv:2307.10437v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10437
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#32534;&#31243;&#26041;&#27861;&#65292;&#29992;&#20110;&#26657;&#20934;&#21644;&#39564;&#35777;&#36710;&#36742;&#36319;&#38543;&#27169;&#22411;&#65292;&#20197;&#25429;&#25417;&#21644;&#22797;&#21046;&#24037;&#20316;&#21306;&#20869;&#22806;&#30340;&#39550;&#39542;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#20223;&#30495;&#36719;&#20214;&#34987;&#20132;&#36890;&#30740;&#31350;&#20154;&#21592;&#21644;&#24037;&#31243;&#24072;&#29992;&#20110;&#35774;&#35745;&#21644;&#35780;&#20272;&#36947;&#36335;&#21464;&#21270;&#12290;&#36825;&#20123;&#20223;&#30495;&#22120;&#30001;&#24494;&#35266;&#39550;&#39542;&#34892;&#20026;&#27169;&#22411;&#39537;&#21160;&#65292;&#21487;&#20197;&#20174;&#20013;&#25512;&#23548;&#20986;&#23439;&#35266;&#27979;&#37327;&#22914;&#27969;&#37327;&#21644;&#25317;&#22581;&#12290;&#35768;&#22810;&#27169;&#22411;&#35774;&#35745;&#29992;&#20110;&#21487;&#33021;&#30340;&#20132;&#36890;&#22330;&#26223;&#21644;&#36947;&#36335;&#37197;&#32622;&#30340;&#23376;&#38598;&#65292;&#32780;&#20854;&#20182;&#27169;&#22411;&#22312;&#24212;&#29992;&#19978;&#27809;&#26377;&#26126;&#30830;&#30340;&#38480;&#21046;&#12290;&#24037;&#20316;&#21306;&#65288;WZs&#65289;&#26159;&#19968;&#31181;&#21040;&#30446;&#21069;&#20026;&#27490;&#27809;&#26377;&#27169;&#22411;&#33021;&#22815;&#22797;&#29616;&#30495;&#23454;&#39550;&#39542;&#34892;&#20026;&#30340;&#22330;&#26223;&#12290;&#36825;&#20351;&#24471;&#22312;&#35774;&#35745;WZ&#26102;&#20248;&#21270;&#23433;&#20840;&#21644;&#20854;&#20182;&#25351;&#26631;&#21464;&#24471;&#22256;&#38590;&#12290;&#32654;&#22269;&#32852;&#37030;&#20844;&#36335;&#31649;&#29702;&#23616;&#22996;&#25176;USDOT Volpe&#20013;&#24515;&#24320;&#21457;&#19968;&#31181;&#24494;&#35266;&#20223;&#30495;&#22120;&#20013;&#33021;&#22815;&#20934;&#30830;&#25429;&#25417;&#21644;&#22797;&#21046;WZ&#20869;&#22806;&#30340;&#39550;&#39542;&#34892;&#20026;&#30340;&#36710;&#36742;&#36319;&#38543;&#65288;CF&#65289;&#27169;&#22411;&#12290;Volpe&#36824;&#36827;&#34892;&#20102;&#19968;&#39033;&#33258;&#28982;&#39550;&#39542;&#30740;&#31350;&#65292;&#25910;&#38598;&#20102;&#22312;&#20855;&#26377;WZ&#30340;&#36947;&#36335;&#19978;&#39550;&#39542;&#30340;&#36710;&#36742;&#30340;&#36965;&#27979;&#25968;&#25454;&#65292;&#29992;&#20110;&#27169;&#22411;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic simulation software is used by transportation researchers and engineers to design and evaluate changes to roadways. These simulators are driven by models of microscopic driver behavior from which macroscopic measures like flow and congestion can be derived. Many models are designed for a subset of possible traffic scenarios and roadway configurations, while others have no explicit constraints on their application. Work zones (WZs) are one scenario for which no model to date has reproduced realistic driving behavior. This makes it difficult to optimize for safety and other metrics when designing a WZ. The Federal Highway Administration commissioned the USDOT Volpe Center to develop a car-following (CF) model for use in microscopic simulators that can capture and reproduce driver behavior accurately within and outside of WZs. Volpe also performed a naturalistic driving study to collect telematics data from vehicles driven on roads with WZs for use in model calibration. During mod
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30697;&#38453;&#38598;&#21512;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#30340;&#22810;&#33218;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#22312;&#26679;&#26412;&#37327;&#22826;&#23567;&#26080;&#27861;&#35757;&#32451;&#22810;&#33218;&#31070;&#32463;&#32593;&#32476;&#26102;&#20805;&#20998;&#36817;&#20284;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;&#35813;&#26041;&#27861;&#36824;&#21487;&#20197;&#23545;&#20174;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#65288;LSTM&#65289;&#33719;&#24471;&#30340;&#39044;&#27979;&#36171;&#20104;&#19981;&#30830;&#23450;&#24615;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#35206;&#30422;&#33539;&#22260;&#12290;</title><link>http://arxiv.org/abs/2307.10436</link><description>&lt;p&gt;
&#22522;&#20110;&#30697;&#38453;&#38598;&#21512;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#30340;&#22810;&#33218;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#20805;&#20998;&#36817;&#20284;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
A Matrix Ensemble Kalman Filter-based Multi-arm Neural Network to Adequately Approximate Deep Neural Networks. (arXiv:2307.10436v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10436
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30697;&#38453;&#38598;&#21512;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#30340;&#22810;&#33218;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#22312;&#26679;&#26412;&#37327;&#22826;&#23567;&#26080;&#27861;&#35757;&#32451;&#22810;&#33218;&#31070;&#32463;&#32593;&#32476;&#26102;&#20805;&#20998;&#36817;&#20284;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;&#35813;&#26041;&#27861;&#36824;&#21487;&#20197;&#23545;&#20174;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#65288;LSTM&#65289;&#33719;&#24471;&#30340;&#39044;&#27979;&#36171;&#20104;&#19981;&#30830;&#23450;&#24615;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#35206;&#30422;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#32773;&#65288;DLs&#65289;&#26159;&#26368;&#20808;&#36827;&#30340;&#39044;&#27979;&#26426;&#21046;&#65292;&#22312;&#35768;&#22810;&#38656;&#35201;&#22797;&#26434;&#39640;&#32500;&#25968;&#25454;&#22788;&#29702;&#30340;&#39046;&#22495;&#20013;&#26377;&#24191;&#27867;&#24212;&#29992;&#12290;&#34429;&#28982;&#20256;&#32479;&#30340;DLs&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#21644;&#21453;&#21521;&#20256;&#25773;&#36827;&#34892;&#35757;&#32451;&#65292;&#20294;&#24050;&#32463;&#24320;&#21457;&#20986;&#20102;&#19981;&#38656;&#35201;&#26799;&#24230;&#35745;&#31639;&#30340;&#22522;&#20110;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#65288;KF&#65289;&#30340;&#25216;&#26415;&#26469;&#36817;&#20284;DLs&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;KF&#30340;DL&#36817;&#20284;&#22120;&#30340;&#22810;&#33218;&#25193;&#23637;&#65292;&#24403;&#26679;&#26412;&#37327;&#22826;&#23567;&#26080;&#27861;&#35757;&#32451;&#22810;&#33218;DL&#26102;&#65292;&#21487;&#20197;&#27169;&#20223;DL&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#30697;&#38453;&#38598;&#21512;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#22810;&#33218;ANN&#65288;MEnKF-ANN&#65289;&#36824;&#25191;&#34892;&#26174;&#24335;&#30340;&#27169;&#22411;&#22534;&#21472;&#65292;&#22312;&#35757;&#32451;&#26679;&#26412;&#20855;&#26377;&#19981;&#31561;&#23610;&#23544;&#29305;&#24449;&#38598;&#26102;&#20855;&#26377;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#25216;&#26415;&#21487;&#20197;&#36817;&#20284;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#32593;&#32476;&#65292;&#24182;&#32473;&#20986;&#20174;&#36825;&#20123;LSTM&#39044;&#27979;&#20013;&#33719;&#24471;&#30340;&#20540;&#21152;&#19978;&#19981;&#30830;&#23450;&#24615;&#30340;&#29702;&#24819;&#35206;&#30422;&#33539;&#22260;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;MEnKF-ANN&#22914;&#20309;&#8220;&#20805;&#20998;&#8221;&#36817;&#20284;&#19968;&#20010;&#35757;&#32451;&#29992;&#20110;&#20998;&#31867;&#21738;&#20123;&#30899;&#27700;&#21270;&#21512;&#29289;&#22522;&#36136;&#34987;&#28040;&#21270;&#21644;&#21033;&#29992;&#30340;LSTM&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Learners (DLs) are the state-of-art predictive mechanism with applications in many fields requiring complex high dimensional data processing. Although conventional DLs get trained via gradient descent with back-propagation, Kalman Filter (KF)-based techniques that do not need gradient computation have been developed to approximate DLs. We propose a multi-arm extension of a KF-based DL approximator that can mimic DL when the sample size is too small to train a multi-arm DL. The proposed Matrix Ensemble Kalman Filter-based multi-arm ANN (MEnKF-ANN) also performs explicit model stacking that becomes relevant when the training sample has an unequal-size feature set. Our proposed technique can approximate Long Short-term Memory (LSTM) Networks and attach uncertainty to the predictions obtained from these LSTMs with desirable coverage. We demonstrate how MEnKF-ANN can "adequately" approximate an LSTM network trained to classify what carbohydrate substrates are digested and utilized by a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35831;&#27714;&#25104;&#21592;&#26631;&#31614;&#21644;&#25104;&#23545;&#20559;&#22909;&#26469;&#25193;&#23637;&#20027;&#21160;&#35268;&#33539;&#23398;&#20064;&#65292;&#25552;&#39640;&#23398;&#20064;&#24418;&#24335;&#35268;&#33539;&#30340;&#28789;&#27963;&#24615;&#12290;&#22312;&#20004;&#20010;&#19981;&#21516;&#39046;&#22495;&#30340;&#23454;&#39564;&#20013;&#65292;&#32467;&#26524;&#34920;&#26126;&#36890;&#36807;&#23398;&#20064;&#25104;&#21592;&#21644;&#20559;&#22909;&#30340;&#32452;&#21512;&#21487;&#20197;&#31283;&#23450;&#21644;&#26041;&#20415;&#22320;&#35782;&#21035;&#35268;&#33539;&#12290;</title><link>http://arxiv.org/abs/2307.10434</link><description>&lt;p&gt;
&#20174;&#25104;&#21592;&#21644;&#20559;&#22909;&#26597;&#35810;&#20013;&#23398;&#20064;&#24418;&#24335;&#35268;&#33539;
&lt;/p&gt;
&lt;p&gt;
Learning Formal Specifications from Membership and Preference Queries. (arXiv:2307.10434v1 [cs.FL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10434
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35831;&#27714;&#25104;&#21592;&#26631;&#31614;&#21644;&#25104;&#23545;&#20559;&#22909;&#26469;&#25193;&#23637;&#20027;&#21160;&#35268;&#33539;&#23398;&#20064;&#65292;&#25552;&#39640;&#23398;&#20064;&#24418;&#24335;&#35268;&#33539;&#30340;&#28789;&#27963;&#24615;&#12290;&#22312;&#20004;&#20010;&#19981;&#21516;&#39046;&#22495;&#30340;&#23454;&#39564;&#20013;&#65292;&#32467;&#26524;&#34920;&#26126;&#36890;&#36807;&#23398;&#20064;&#25104;&#21592;&#21644;&#20559;&#22909;&#30340;&#32452;&#21512;&#21487;&#20197;&#31283;&#23450;&#21644;&#26041;&#20415;&#22320;&#35782;&#21035;&#35268;&#33539;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#23398;&#20064;&#26159;&#19968;&#31181;&#30740;&#31350;&#24191;&#27867;&#30340;&#23398;&#20064;&#24418;&#24335;&#35268;&#33539;&#30340;&#26041;&#27861;&#65292;&#20363;&#22914;&#33258;&#21160;&#26426;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#23558;&#20027;&#21160;&#35268;&#33539;&#23398;&#20064;&#25193;&#23637;&#21040;&#35831;&#27714;&#32452;&#21512;&#25104;&#21592;&#26631;&#31614;&#21644;&#25104;&#23545;&#20559;&#22909;&#65288;&#23545;&#25104;&#21592;&#26631;&#31614;&#30340;&#19968;&#31181;&#27969;&#34892;&#26367;&#20195;&#26041;&#24335;&#65289;&#12290;&#25104;&#23545;&#20559;&#22909;&#21644;&#25104;&#21592;&#26631;&#31614;&#30340;&#32452;&#21512;&#20801;&#35768;&#26356;&#28789;&#27963;&#30340;&#20027;&#21160;&#35268;&#33539;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#20808;&#21069;&#20165;&#20381;&#36182;&#25104;&#21592;&#26631;&#31614;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26694;&#26550;&#24212;&#29992;&#20110;&#20004;&#20010;&#19981;&#21516;&#30340;&#39046;&#22495;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#24191;&#27867;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20174;&#20004;&#31181;&#27169;&#24335;&#23398;&#20064;&#21487;&#20197;&#36890;&#36807;&#25104;&#21592;&#21644;&#20559;&#22909;&#26469;&#31283;&#20581;&#21644;&#26041;&#20415;&#22320;&#35782;&#21035;&#35268;&#33539;&#12290;
&lt;/p&gt;
&lt;p&gt;
Active learning is a well-studied approach to learning formal specifications, such as automata. In this work, we extend active specification learning by proposing a novel framework that strategically requests a combination of membership labels and pair-wise preferences, a popular alternative to membership labels. The combination of pair-wise preferences and membership labels allows for a more flexible approach to active specification learning, which previously relied on membership labels only. We instantiate our framework in two different domains, demonstrating the generality of our approach. Our results suggest that learning from both modalities allows us to robustly and conveniently identify specifications via membership and preferences.
&lt;/p&gt;</description></item><item><title>DP-TBART&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#24046;&#20998;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#21512;&#25104;&#30340;&#34920;&#26684;&#25968;&#25454;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#23427;&#22312;&#22810;&#31181;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#65292;&#24182;&#22312;&#26576;&#20123;&#22330;&#26223;&#20013;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.10430</link><description>&lt;p&gt;
DP-TBART&#65306;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#29992;&#20110;&#24046;&#20998;&#38544;&#31169;&#34920;&#26684;&#25968;&#25454;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
DP-TBART: A Transformer-based Autoregressive Model for Differentially Private Tabular Data Generation. (arXiv:2307.10430v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10430
&lt;/p&gt;
&lt;p&gt;
DP-TBART&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#24046;&#20998;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#21512;&#25104;&#30340;&#34920;&#26684;&#25968;&#25454;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#23427;&#22312;&#22810;&#31181;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#65292;&#24182;&#22312;&#26576;&#20123;&#22330;&#26223;&#20013;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20445;&#25345;&#24046;&#20998;&#38544;&#31169;&#30340;&#26465;&#20214;&#19979;&#29983;&#25104;&#21512;&#25104;&#34920;&#26684;&#25968;&#25454;&#26159;&#19968;&#20010;&#26085;&#30410;&#37325;&#35201;&#30340;&#38382;&#39064;&#12290;&#34429;&#28982;&#20256;&#32479;&#30340;&#22522;&#20110;&#36793;&#38469;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#32467;&#26524;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#24448;&#24448;&#33853;&#21518;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DP-TBART&#65292;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#23427;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#20445;&#25345;&#24046;&#20998;&#38544;&#31169;&#65292;&#24182;&#33021;&#22815;&#19982;&#22522;&#20110;&#36793;&#38469;&#30340;&#26041;&#27861;&#31454;&#20105;&#65292;&#29978;&#33267;&#22312;&#26576;&#20123;&#22330;&#26223;&#19979;&#36229;&#36807;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#29702;&#35299;&#22522;&#20110;&#36793;&#38469;&#30340;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#20197;&#21450;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#21738;&#20123;&#26041;&#38754;&#33021;&#22815;&#20570;&#20986;&#36129;&#29486;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#29983;&#25104;&#24046;&#20998;&#38544;&#31169;&#21512;&#25104;&#34920;&#26684;&#25968;&#25454;&#26102;&#65292;&#24212;&#32771;&#34385;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25216;&#26415;&#20316;&#20026;&#21487;&#34892;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The generation of synthetic tabular data that preserves differential privacy is a problem of growing importance. While traditional marginal-based methods have achieved impressive results, recent work has shown that deep learning-based approaches tend to lag behind. In this work, we present Differentially-Private TaBular AutoRegressive Transformer (DP-TBART), a transformer-based autoregressive model that maintains differential privacy and achieves performance competitive with marginal-based methods on a wide variety of datasets, capable of even outperforming state-of-the-art methods in certain settings. We also provide a theoretical framework for understanding the limitations of marginal-based approaches and where deep learning-based approaches stand to contribute most. These results suggest that deep learning-based techniques should be considered as a viable alternative to marginal-based methods in the generation of differentially private synthetic tabular data.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;PreDiff&#26041;&#27861;&#65292;&#20351;&#29992;&#26465;&#20214;&#38544;&#24335;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#38477;&#27700;&#36817;&#26399;&#39044;&#27979;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#26174;&#24335;&#30693;&#35782;&#25511;&#21046;&#26426;&#21046;&#20197;&#28385;&#36275;&#29305;&#23450;&#39046;&#22495;&#30340;&#29289;&#29702;&#32422;&#26463;&#12290;</title><link>http://arxiv.org/abs/2307.10422</link><description>&lt;p&gt;
PreDiff: &#20351;&#29992;&#38544;&#24335;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#38477;&#27700;&#36817;&#26399;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
PreDiff: Precipitation Nowcasting with Latent Diffusion Models. (arXiv:2307.10422v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10422
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;PreDiff&#26041;&#27861;&#65292;&#20351;&#29992;&#26465;&#20214;&#38544;&#24335;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#38477;&#27700;&#36817;&#26399;&#39044;&#27979;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#26174;&#24335;&#30693;&#35782;&#25511;&#21046;&#26426;&#21046;&#20197;&#28385;&#36275;&#29305;&#23450;&#39046;&#22495;&#30340;&#29289;&#29702;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#22320;&#29699;&#31995;&#32479;&#39044;&#27979;&#20027;&#35201;&#20381;&#36182;&#20110;&#22797;&#26434;&#30340;&#29289;&#29702;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#35745;&#31639;&#37327;&#22823;&#19988;&#38656;&#35201;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#12290;&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#26102;&#31354;&#22320;&#29699;&#35266;&#27979;&#25968;&#25454;&#30340;&#31354;&#21069;&#22686;&#21152;&#20351;&#24471;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#25968;&#25454;&#39537;&#21160;&#39044;&#27979;&#27169;&#22411;&#25104;&#20026;&#21487;&#33021;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#19981;&#21516;&#30340;&#22320;&#29699;&#31995;&#32479;&#39044;&#27979;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#25928;&#26524;&#65292;&#20294;&#26159;&#23427;&#20204;&#35201;&#20040;&#38590;&#20197;&#22788;&#29702;&#19981;&#30830;&#23450;&#24615;&#65292;&#35201;&#20040;&#24573;&#35270;&#29305;&#23450;&#39046;&#22495;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#23548;&#33268;&#39044;&#27979;&#32467;&#26524;&#27169;&#31946;&#25110;&#20135;&#29983;&#29289;&#29702;&#19978;&#19981;&#21512;&#29702;&#30340;&#39044;&#27979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#29575;&#26102;&#31354;&#39044;&#27979;&#30340;&#20004;&#38454;&#27573;&#27969;&#31243;&#65306;1&#65289;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;PreDiff&#30340;&#26465;&#20214;&#38544;&#24335;&#25193;&#25955;&#27169;&#22411;&#65292;&#33021;&#22815;&#36827;&#34892;&#27010;&#29575;&#39044;&#27979;&#65307;2&#65289;&#25105;&#20204;&#34701;&#20837;&#20102;&#19968;&#31181;&#26174;&#24335;&#30693;&#35782;&#25511;&#21046;&#26426;&#21046;&#65292;&#20197;&#20351;&#39044;&#27979;&#31526;&#21512;&#29305;&#23450;&#39046;&#22495;&#30340;&#29289;&#29702;&#32422;&#26463;&#12290;&#36825;&#26159;&#36890;&#36807;&#22312;&#27599;&#20010;&#21435;&#22122;&#27493;&#39588;&#20013;&#20272;&#35745;&#19982;&#25152;&#26045;&#21152;&#32422;&#26463;&#30340;&#20559;&#24046;&#26469;&#23454;&#29616;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Earth system forecasting has traditionally relied on complex physical models that are computationally expensive and require significant domain expertise. In the past decade, the unprecedented increase in spatiotemporal Earth observation data has enabled data-driven forecasting models using deep learning techniques. These models have shown promise for diverse Earth system forecasting tasks but either struggle with handling uncertainty or neglect domain-specific prior knowledge, resulting in averaging possible futures to blurred forecasts or generating physically implausible predictions. To address these limitations, we propose a two-stage pipeline for probabilistic spatiotemporal forecasting: 1) We develop PreDiff, a conditional latent diffusion model capable of probabilistic forecasts. 2) We incorporate an explicit knowledge control mechanism to align forecasts with domain-specific physical constraints. This is achieved by estimating the deviation from imposed constraints at each denoi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;PIP-Net&#24320;&#23637;&#20102;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#39592;&#25240;&#26816;&#27979;&#21644;&#30382;&#32932;&#30284;&#35786;&#26029;&#26041;&#38754;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#36890;&#36807;&#26080;&#30417;&#30563;&#30340;&#39044;&#35757;&#32451;&#65292;PIP-Net&#33021;&#22815;&#36731;&#26494;&#35782;&#21035;&#25968;&#25454;&#36136;&#37327;&#38382;&#39064;&#65292;&#24182;&#19988;&#25105;&#20204;&#36824;&#21457;&#29616;&#20154;&#20204;&#21487;&#20197;&#36890;&#36807;&#25163;&#21160;&#31105;&#29992;&#19981;&#33391;&#21407;&#22411;&#26469;&#32416;&#27491;PIP-Net&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2307.10404</link><description>&lt;p&gt;
&#20351;&#29992;PIP-Net&#35299;&#37322;&#21644;&#32416;&#27491;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Interpreting and Correcting Medical Image Classification with PIP-Net. (arXiv:2307.10404v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10404
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;PIP-Net&#24320;&#23637;&#20102;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#39592;&#25240;&#26816;&#27979;&#21644;&#30382;&#32932;&#30284;&#35786;&#26029;&#26041;&#38754;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#36890;&#36807;&#26080;&#30417;&#30563;&#30340;&#39044;&#35757;&#32451;&#65292;PIP-Net&#33021;&#22815;&#36731;&#26494;&#35782;&#21035;&#25968;&#25454;&#36136;&#37327;&#38382;&#39064;&#65292;&#24182;&#19988;&#25105;&#20204;&#36824;&#21457;&#29616;&#20154;&#20204;&#21487;&#20197;&#36890;&#36807;&#25163;&#21160;&#31105;&#29992;&#19981;&#33391;&#21407;&#22411;&#26469;&#32416;&#27491;PIP-Net&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#20998;&#21407;&#22411;&#27169;&#22411;&#26159;&#21487;&#35299;&#37322;&#24615;&#30340;&#22270;&#20687;&#20998;&#31867;&#22120;&#65292;&#26159;&#40657;&#30418;&#20154;&#24037;&#26234;&#33021;&#30340;&#26377;&#24076;&#26395;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#21487;&#35299;&#37322;&#24615;&#26426;&#22120;&#23398;&#20064;&#30340;&#36866;&#29992;&#24615;&#21644;&#28508;&#21147;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#30495;&#23454;&#19990;&#30028;&#30340;&#21307;&#23398;&#25104;&#20687;&#25968;&#25454;&#30340;&#33258;&#21160;&#35786;&#26029;&#25903;&#25345;&#12290;PIP-Net&#23398;&#20064;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#20856;&#22411;&#22270;&#20687;&#37096;&#20998;&#65292;&#24182;&#35780;&#20272;&#20854;&#22312;&#39592;&#25240;&#26816;&#27979;&#21644;&#30382;&#32932;&#30284;&#35786;&#26029;&#26041;&#38754;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;PIP-Net&#30340;&#20915;&#31574;&#36807;&#31243;&#31526;&#21512;&#21307;&#23398;&#20998;&#31867;&#26631;&#20934;&#65292;&#20165;&#25552;&#20379;&#22270;&#20687;&#32423;&#21035;&#30340;&#31867;&#26631;&#31614;&#12290;&#30001;&#20110;PIP-Net&#23545;&#21407;&#22411;&#36827;&#34892;&#20102;&#26080;&#30417;&#30563;&#30340;&#39044;&#35757;&#32451;&#65292;&#22240;&#27492;&#21487;&#20197;&#36731;&#26494;&#35782;&#21035;X&#20809;&#20013;&#30340;&#19981;&#33391;&#25991;&#26412;&#25110;&#26631;&#31614;&#38169;&#35823;&#31561;&#25968;&#25454;&#36136;&#37327;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#26174;&#31034;&#20154;&#20204;&#21487;&#20197;&#36890;&#36807;&#30452;&#25509;&#31105;&#29992;&#19981;&#33391;&#21407;&#22411;&#26469;&#25163;&#21160;&#32416;&#27491;PIP-Net&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#37096;&#20998;&#21407;&#22411;&#27169;&#22411;&#23545;&#21307;&#23398;&#24212;&#29992;&#20855;&#26377;&#28508;&#21147;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#30456;&#20114;&#21442;&#32771;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Part-prototype models are explainable-by-design image classifiers, and a promising alternative to black box AI. This paper explores the applicability and potential of interpretable machine learning, in particular PIP-Net, for automated diagnosis support on real-world medical imaging data. PIP-Net learns human-understandable prototypical image parts and we evaluate its accuracy and interpretability for fracture detection and skin cancer diagnosis. We find that PIP-Net's decision making process is in line with medical classification standards, while only provided with image-level class labels. Because of PIP-Net's unsupervised pretraining of prototypes, data quality problems such as undesired text in an X-ray or labelling errors can be easily identified. Additionally, we are the first to show that humans can manually correct the reasoning of PIP-Net by directly disabling undesired prototypes. We conclude that part-prototype models are promising for medical applications due to their inter
&lt;/p&gt;</description></item><item><title>&#24378;&#36879;&#38236;&#21457;&#29616;&#31070;&#32463;&#32593;&#32476;&#30340;&#36873;&#25321;&#21151;&#33021;&#23545;&#20110;&#23454;&#29616;&#22823;&#26679;&#26412;&#24378;&#24341;&#21147;&#36879;&#38236;&#31995;&#32479;&#30340;&#28508;&#21147;&#33267;&#20851;&#37325;&#35201;&#65292;&#32593;&#32476;&#20542;&#21521;&#20110;&#36873;&#25321;&#20855;&#26377;&#36739;&#22823;&#29233;&#22240;&#26031;&#22374;&#21322;&#24452;&#21644;&#26356;&#38598;&#20013;&#28304;&#20809;&#20998;&#24067;&#30340;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2307.10355</link><description>&lt;p&gt;
&#24378;&#36879;&#38236;&#21457;&#29616;&#31070;&#32463;&#32593;&#32476;&#30340;&#36873;&#25321;&#21151;&#33021;
&lt;/p&gt;
&lt;p&gt;
Selection functions of strong lens finding neural networks. (arXiv:2307.10355v1 [astro-ph.CO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10355
&lt;/p&gt;
&lt;p&gt;
&#24378;&#36879;&#38236;&#21457;&#29616;&#31070;&#32463;&#32593;&#32476;&#30340;&#36873;&#25321;&#21151;&#33021;&#23545;&#20110;&#23454;&#29616;&#22823;&#26679;&#26412;&#24378;&#24341;&#21147;&#36879;&#38236;&#31995;&#32479;&#30340;&#28508;&#21147;&#33267;&#20851;&#37325;&#35201;&#65292;&#32593;&#32476;&#20542;&#21521;&#20110;&#36873;&#25321;&#20855;&#26377;&#36739;&#22823;&#29233;&#22240;&#26031;&#22374;&#21322;&#24452;&#21644;&#26356;&#38598;&#20013;&#28304;&#20809;&#20998;&#24067;&#30340;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#24120;&#22312;&#25991;&#29486;&#20013;&#21457;&#29616;&#30340;&#20855;&#26377;&#31867;&#20284;&#26550;&#26500;&#21644;&#35757;&#32451;&#25968;&#25454;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#36879;&#38236;&#21457;&#29616;&#26159;&#26377;&#20559;&#30340;&#20998;&#31867;&#22120;&#12290;&#20102;&#35299;&#36879;&#38236;&#21457;&#29616;&#31070;&#32463;&#32593;&#32476;&#30340;&#36873;&#25321;&#21151;&#33021;&#23545;&#20110;&#20805;&#20998;&#23454;&#29616;&#21363;&#23558;&#20986;&#29616;&#30340;&#22823;&#26679;&#26412;&#24378;&#24341;&#21147;&#36879;&#38236;&#31995;&#32479;&#30340;&#28508;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#19977;&#20010;&#20195;&#34920;&#24615;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#29992;&#20110;&#35757;&#32451;&#26143;&#31995;-&#26143;&#31995;&#21644;&#26143;&#31995;-&#31867;&#26143;&#20307;&#36879;&#38236;&#21457;&#29616;&#31070;&#32463;&#32593;&#32476;&#12290;&#36825;&#20123;&#32593;&#32476;&#20542;&#21521;&#20110;&#36873;&#25321;&#20855;&#26377;&#36739;&#22823;&#29233;&#22240;&#26031;&#22374;&#21322;&#24452;&#21644;&#26356;&#38598;&#20013;&#30340;&#28304;&#20809;&#20998;&#24067;&#30340;&#31995;&#32479;&#12290;&#23558;&#26816;&#27979;&#26174;&#33879;&#24615;&#38408;&#20540;&#20174;8&#963;&#22686;&#21152;&#21040;12&#963;&#65292;&#23548;&#33268;&#36873;&#20013;&#30340;&#24378;&#36879;&#38236;&#31995;&#32479;&#20013;&#26377;50&#65285;&#30340;&#31995;&#32479;&#30340;&#29233;&#22240;&#26031;&#22374;&#21322;&#24452;&#952;_E&#8805;1.04&#35282;&#31186;&#65292;&#28304;&#21322;&#24452;R_S&#8805;0.194&#35282;&#31186;&#65292;&#20174;&#952;_E&#8805;0.879&#35282;&#31186;&#21644;R_S&#8805;0.178&#35282;&#31186;&#12290;
&lt;/p&gt;
&lt;p&gt;
Convolution Neural Networks trained for the task of lens finding with similar architecture and training data as is commonly found in the literature are biased classifiers. An understanding of the selection function of lens finding neural networks will be key to fully realising the potential of the large samples of strong gravitational lens systems that will be found in upcoming wide-field surveys. We use three training datasets, representative of those used to train galaxy-galaxy and galaxy-quasar lens finding neural networks. The networks preferentially select systems with larger Einstein radii and larger sources with more concentrated source-light distributions. Increasing the detection significance threshold to 12$\sigma$ from 8$\sigma$ results in 50 per cent of the selected strong lens systems having Einstein radii $\theta_\mathrm{E}$ $\ge$ 1.04 arcsec from $\theta_\mathrm{E}$ $\ge$ 0.879 arcsec, source radii $R_S$ $\ge$ 0.194 arcsec from $R_S$ $\ge$ 0.178 arcsec and source S\'ersi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31163;&#25955;&#20999;&#21106;Wasserstein&#25439;&#22833;&#30340;&#24615;&#36136;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#27491;&#21017;&#24615;&#21644;&#20248;&#21270;&#24615;&#36136;&#20197;&#21450;&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;&#36817;&#20284;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.10352</link><description>&lt;p&gt;
&#31163;&#25955;&#20999;&#21106;Wasserstein&#25439;&#22833;&#30340;&#24615;&#36136;
&lt;/p&gt;
&lt;p&gt;
Properties of Discrete Sliced Wasserstein Losses. (arXiv:2307.10352v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10352
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31163;&#25955;&#20999;&#21106;Wasserstein&#25439;&#22833;&#30340;&#24615;&#36136;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#27491;&#21017;&#24615;&#21644;&#20248;&#21270;&#24615;&#36136;&#20197;&#21450;&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;&#36817;&#20284;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20999;&#21106;Wasserstein&#65288;SW&#65289;&#36317;&#31163;&#24050;&#25104;&#20026;&#27604;&#36739;&#27010;&#29575;&#27979;&#24230;&#30340;Wasserstein&#36317;&#31163;&#30340;&#19968;&#31181;&#27969;&#34892;&#26367;&#20195;&#26041;&#27861;&#12290;&#24191;&#27867;&#24212;&#29992;&#21253;&#25324;&#22270;&#20687;&#22788;&#29702;&#12289;&#39046;&#22495;&#33258;&#36866;&#24212;&#21644;&#29983;&#25104;&#24314;&#27169;&#65292;&#24120;&#24120;&#38656;&#35201;&#20248;&#21270;&#19968;&#20123;&#21442;&#25968;&#20197;&#26368;&#23567;&#21270;SW&#65292;&#35813;&#21442;&#25968;&#20805;&#24403;&#31163;&#25955;&#27010;&#29575;&#27979;&#24230;&#20043;&#38388;&#30340;&#25439;&#22833;&#20989;&#25968;&#65288;&#22240;&#20026;&#20855;&#26377;&#23494;&#24230;&#30340;&#27979;&#24230;&#22312;&#25968;&#20540;&#19978;&#26159;&#26080;&#27861;&#23454;&#29616;&#30340;&#65289;&#12290;&#25152;&#26377;&#36825;&#20123;&#20248;&#21270;&#38382;&#39064;&#37117;&#23384;&#22312;&#30456;&#21516;&#30340;&#23376;&#38382;&#39064;&#65292;&#21363;&#26368;&#23567;&#21270;&#20999;&#21106;Wasserstein&#33021;&#37327;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;$\mathcal{E}: Y \longmapsto \mathrm{SW}_2^2(\gamma_Y, \gamma_Z)$&#30340;&#23646;&#24615;&#65292;&#21363;&#20004;&#20010;&#20855;&#26377;&#19982;&#19968;&#20010;&#27979;&#24230;&#30340;&#25903;&#25745;&#30456;&#21516;&#25968;&#37327;&#30340;&#31163;&#25955;&#22343;&#21248;&#27979;&#24230;&#20043;&#38388;&#30340;SW&#36317;&#31163;&#20316;&#20026;&#25903;&#25745;$Y \in \mathbb{R}^{n \times d}$&#20989;&#25968;&#30340;&#33021;&#37327;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20010;&#33021;&#37327;&#30340;&#27491;&#21017;&#24615;&#21644;&#20248;&#21270;&#24615;&#36136;&#65292;&#20197;&#21450;&#20854;&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;&#36817;&#20284;$\mathcal{E}_p$&#65288;&#20351;&#29992;SW&#20013;&#30340;&#26399;&#26395;&#20272;&#35745;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Sliced Wasserstein (SW) distance has become a popular alternative to the Wasserstein distance for comparing probability measures. Widespread applications include image processing, domain adaptation and generative modelling, where it is common to optimise some parameters in order to minimise SW, which serves as a loss function between discrete probability measures (since measures admitting densities are numerically unattainable). All these optimisation problems bear the same sub-problem, which is minimising the Sliced Wasserstein energy. In this paper we study the properties of $\mathcal{E}: Y \longmapsto \mathrm{SW}_2^2(\gamma_Y, \gamma_Z)$, i.e. the SW distance between two uniform discrete measures with the same amount of points as a function of the support $Y \in \mathbb{R}^{n \times d}$ of one of the measures. We investigate the regularity and optimisation properties of this energy, as well as its Monte-Carlo approximation $\mathcal{E}_p$ (estimating the expectation in SW using 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25506;&#32034;&#28151;&#21512;&#21407;&#22987;&#21644;&#29983;&#25104;&#30340;&#22270;&#20687;&#25551;&#36848;&#30340;&#19981;&#21516;&#31574;&#30053;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;ImageNet&#19978;&#36229;&#36807;&#20102;&#24403;&#21069;&#26368;&#20339;&#38477;&#22122;&#26041;&#27861;2%&#65292;&#22312;38&#20010;&#20219;&#21153;&#20013;&#24179;&#22343;&#25552;&#39640;&#20102;4%&#65292;&#25105;&#20204;&#30340;&#26368;&#20339;&#26041;&#27861;&#22312;Flickr&#21644;MS-COCO&#26816;&#32034;&#19978;&#20063;&#25552;&#21319;&#20102;2&#20493;&#12290;</title><link>http://arxiv.org/abs/2307.10350</link><description>&lt;p&gt;
&#25913;&#36827;&#35270;&#35273;-&#35821;&#35328;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#30340;&#22270;&#20687;&#25551;&#36848;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improving Multimodal Datasets with Image Captioning. (arXiv:2307.10350v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10350
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25506;&#32034;&#28151;&#21512;&#21407;&#22987;&#21644;&#29983;&#25104;&#30340;&#22270;&#20687;&#25551;&#36848;&#30340;&#19981;&#21516;&#31574;&#30053;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;ImageNet&#19978;&#36229;&#36807;&#20102;&#24403;&#21069;&#26368;&#20339;&#38477;&#22122;&#26041;&#27861;2%&#65292;&#22312;38&#20010;&#20219;&#21153;&#20013;&#24179;&#22343;&#25552;&#39640;&#20102;4%&#65292;&#25105;&#20204;&#30340;&#26368;&#20339;&#26041;&#27861;&#22312;Flickr&#21644;MS-COCO&#26816;&#32034;&#19978;&#20063;&#25552;&#21319;&#20102;2&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#32593;&#32476;&#25968;&#25454;&#38598;&#22312;CLIP&#21644;Flamingo&#31561;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#25104;&#21151;&#20013;&#36215;&#21040;&#20102;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#21407;&#22987;&#32593;&#32476;&#25968;&#25454;&#23384;&#22312;&#22122;&#38899;&#65292;&#29616;&#26377;&#30340;&#38477;&#22122;&#26041;&#27861;&#24448;&#24448;&#20250;&#20197;&#25968;&#25454;&#30340;&#22810;&#26679;&#24615;&#20026;&#20195;&#20215;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32858;&#28966;&#20110;&#22270;&#20687;&#25551;&#36848;&#30340;&#36136;&#37327;&#20316;&#20026;&#22122;&#38899;&#30340;&#19968;&#20010;&#20027;&#35201;&#26469;&#28304;&#65292;&#24182;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#29983;&#25104;&#30340;&#25551;&#36848;&#22686;&#21152;&#21547;&#26377;&#21547;&#20041;&#19981;&#26126;&#30830;&#25991;&#26412;&#30340;&#32593;&#32476;&#25235;&#21462;&#25968;&#25454;&#28857;&#30340;&#23454;&#29992;&#24615;&#12290;&#36890;&#36807;&#25506;&#32034;&#21407;&#22987;&#27169;&#24335;&#21644;&#29983;&#25104;&#27169;&#24335;&#20004;&#31181;&#19981;&#21516;&#30340;&#28151;&#21512;&#31574;&#30053;&#65292;&#25105;&#20204;&#22312;ImageNet&#19978;&#36229;&#36807;&#20102;DataComp&#22522;&#20934;&#25552;&#20986;&#30340;&#26368;&#20339;&#38477;&#22122;&#26041;&#27861;2%&#65292;&#22312;38&#20010;&#20219;&#21153;&#20013;&#24179;&#22343;&#25552;&#39640;&#20102;4%&#65292;&#32473;&#23450;128M&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#20505;&#36873;&#27744;&#12290;&#25105;&#20204;&#26368;&#22909;&#30340;&#26041;&#27861;&#22312;Flickr&#21644;MS-COCO&#26816;&#32034;&#19978;&#20063;&#25552;&#21319;&#20102;2&#20493;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#21512;&#25104;&#25551;&#36848;&#20026;&#20160;&#20040;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#25991;&#26412;&#30417;&#30563;&#26469;&#28304;&#12290;&#22312;&#23581;&#35797;&#19981;&#21516;&#30340;&#22270;&#20687;&#25551;&#36848;&#27169;&#22411;&#30340;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#27169;&#22411;&#22312;&#26631;&#20934;&#22270;&#20687;&#25551;&#36848;&#22522;&#20934;&#19978;&#30340;&#34920;&#29616;&#65288;&#20363;&#22914;&#65292;NoCaps CIDEr&#65289;&#24182;&#19981;&#19968;&#23450;&#26159;
&lt;/p&gt;
&lt;p&gt;
Massive web datasets play a key role in the success of large vision-language models like CLIP and Flamingo. However, the raw web data is noisy, and existing filtering methods to reduce noise often come at the expense of data diversity. Our work focuses on caption quality as one major source of noise, and studies how generated captions can increase the utility of web-scraped datapoints with nondescript text. Through exploring different mixing strategies for raw and generated captions, we outperform the best filtering method proposed by the DataComp benchmark by 2% on ImageNet and 4% on average across 38 tasks, given a candidate pool of 128M image-text pairs. Our best approach is also 2x better at Flickr and MS-COCO retrieval. We then analyze what makes synthetic captions an effective source of text supervision. In experimenting with different image captioning models, we also demonstrate that the performance of a model on standard image captioning benchmarks (e.g., NoCaps CIDEr) is not a
&lt;/p&gt;</description></item><item><title>ProtiGeno&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#22312;&#23547;&#25214;&#30701;&#21407;&#26680;&#22522;&#22240;&#26041;&#38754;&#27604;&#24403;&#21069;&#20808;&#36827;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#21484;&#22238;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.10343</link><description>&lt;p&gt;
ProtiGeno:&#19968;&#31181;&#21033;&#29992;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#30340;&#21407;&#26680;&#30701;&#22522;&#22240;&#39044;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ProtiGeno: a prokaryotic short gene finder using protein language models. (arXiv:2307.10343v1 [q-bio.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10343
&lt;/p&gt;
&lt;p&gt;
ProtiGeno&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#22312;&#23547;&#25214;&#30701;&#21407;&#26680;&#22522;&#22240;&#26041;&#38754;&#27604;&#24403;&#21069;&#20808;&#36827;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#21484;&#22238;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21407;&#26680;&#22522;&#22240;&#39044;&#27979;&#22312;&#29702;&#35299;&#29983;&#29289;&#21644;&#20854;&#21151;&#33021;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#65292;&#22312;&#21307;&#23398;&#21644;&#29983;&#29289;&#25216;&#26415;&#20013;&#26377;&#24212;&#29992;&#12290;&#23613;&#31649;&#24403;&#21069;&#30340;&#22522;&#22240;&#39044;&#27979;&#26041;&#27861;&#22312;&#23547;&#25214;&#38271;&#22522;&#22240;&#26041;&#38754;&#38750;&#24120;&#25935;&#24863;&#65292;&#20294;&#22312;&#23547;&#25214;&#30701;&#22522;&#22240;(&lt;180 nts)&#26041;&#38754;&#65292;&#23427;&#20204;&#30340;&#25935;&#24863;&#24615;&#26126;&#26174;&#38477;&#20302;&#12290;&#21407;&#22240;&#26159;&#32570;&#20047;&#27880;&#37322;&#30340;&#22522;&#22240;&#25968;&#25454;&#20197;&#35782;&#21035;&#30701;&#24320;&#25918;&#38405;&#35835;&#26694;(ORFs)&#20013;&#30340;&#21306;&#21035;&#29305;&#24449;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;ProtiGeno&#65292;&#19987;&#38376;&#38024;&#23545;&#30701;&#21407;&#26680;&#22522;&#22240;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#22312;&#25968;&#30334;&#19975;&#20010;&#36827;&#21270;&#34507;&#30333;&#36136;&#19978;&#35757;&#32451;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#12290;&#22312;&#23545;4,288&#20010;&#21407;&#26680;&#22522;&#22240;&#32452;&#36827;&#34892;&#31995;&#32479;&#22823;&#35268;&#27169;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;ProtiGeno&#30456;&#36739;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#22522;&#22240;&#39044;&#27979;&#26041;&#27861;&#65292;&#22312;&#39044;&#27979;&#30701;&#32534;&#30721;&#21644;&#38750;&#32534;&#30721;&#22522;&#22240;&#26041;&#38754;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#21484;&#22238;&#29575;&#12290;&#36890;&#36807;&#21487;&#35270;&#21270;&#39044;&#27979;&#30340;&#30701;&#22522;&#22240;&#30340;&#19977;&#32500;&#32467;&#26500;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;ProtiGeno&#30340;&#39044;&#27979;&#29305;&#24449;&#21644;&#21487;&#33021;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prokaryotic gene prediction plays an important role in understanding the biology of organisms and their function with applications in medicine and biotechnology. Although the current gene finders are highly sensitive in finding long genes, their sensitivity decreases noticeably in finding shorter genes (&lt;180 nts). The culprit is insufficient annotated gene data to identify distinguishing features in short open reading frames (ORFs). We develop a deep learning-based method called ProtiGeno, specifically targeting short prokaryotic genes using a protein language model trained on millions of evolved proteins. In systematic large-scale experiments on 4,288 prokaryotic genomes, we demonstrate that ProtiGeno predicts short coding and noncoding genes with higher accuracy and recall than the current state-of-the-art gene finders. We discuss the predictive features of ProtiGeno and possible limitations by visualizing the three-dimensional structure of the predicted short genes. Data, codes, and
&lt;/p&gt;</description></item><item><title>IncDSI&#26159;&#19968;&#31181;&#36882;&#22686;&#21487;&#26356;&#26032;&#30340;&#25991;&#26723;&#26816;&#32034;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#26368;&#23567;&#25913;&#21464;&#32593;&#32476;&#21442;&#25968;&#30340;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#65292;&#23454;&#29616;&#23454;&#26102;&#28155;&#21152;&#25991;&#26723;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#25972;&#20010;&#27169;&#22411;&#65292;&#20855;&#26377;&#19982;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#30456;&#31454;&#20105;&#30340;&#36895;&#24230;&#65292;&#33021;&#22815;&#23454;&#26102;&#26356;&#26032;&#30340;&#25991;&#26723;&#26816;&#32034;&#31995;&#32479;&#30340;&#24320;&#21457;&#12290;</title><link>http://arxiv.org/abs/2307.10323</link><description>&lt;p&gt;
IncDSI&#65306;&#36882;&#22686;&#21487;&#26356;&#26032;&#30340;&#25991;&#26723;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
IncDSI: Incrementally Updatable Document Retrieval. (arXiv:2307.10323v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10323
&lt;/p&gt;
&lt;p&gt;
IncDSI&#26159;&#19968;&#31181;&#36882;&#22686;&#21487;&#26356;&#26032;&#30340;&#25991;&#26723;&#26816;&#32034;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#26368;&#23567;&#25913;&#21464;&#32593;&#32476;&#21442;&#25968;&#30340;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#65292;&#23454;&#29616;&#23454;&#26102;&#28155;&#21152;&#25991;&#26723;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#25972;&#20010;&#27169;&#22411;&#65292;&#20855;&#26377;&#19982;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#30456;&#31454;&#20105;&#30340;&#36895;&#24230;&#65292;&#33021;&#22815;&#23454;&#26102;&#26356;&#26032;&#30340;&#25991;&#26723;&#26816;&#32034;&#31995;&#32479;&#30340;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#21516;iable&#25628;&#32034;&#32034;&#24341;&#26159;&#26368;&#36817;&#25552;&#20986;&#30340;&#19968;&#31181;&#25991;&#26723;&#26816;&#32034;&#33539;&#20363;&#65292;&#23427;&#23558;&#25991;&#26723;&#35821;&#26009;&#24211;&#30340;&#20449;&#24687;&#32534;&#30721;&#22312;&#31070;&#32463;&#32593;&#32476;&#30340;&#21442;&#25968;&#20013;&#65292;&#24182;&#30452;&#25509;&#23558;&#26597;&#35810;&#26144;&#23556;&#21040;&#30456;&#24212;&#30340;&#25991;&#26723;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#35768;&#22810;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;&#19968;&#20010;&#37325;&#35201;&#38480;&#21046;&#65306;&#22312;&#35757;&#32451;&#27169;&#22411;&#20043;&#21518;&#28155;&#21152;&#26032;&#25991;&#26723;&#24182;&#19981;&#23481;&#26131;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;IncDSI&#65292;&#19968;&#31181;&#23454;&#26102;&#28155;&#21152;&#25991;&#26723;&#30340;&#26041;&#27861;&#65288;&#27599;&#20010;&#25991;&#26723;&#32422;20-50&#27627;&#31186;&#65289;&#65292;&#32780;&#26080;&#38656;&#23545;&#25972;&#20010;&#25968;&#25454;&#38598;&#65288;&#29978;&#33267;&#37096;&#20998;&#25968;&#25454;&#38598;&#65289;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#23558;&#28155;&#21152;&#25991;&#26723;&#30340;&#36807;&#31243;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#22312;&#32593;&#32476;&#21442;&#25968;&#19978;&#36827;&#34892;&#26368;&#23567;&#25913;&#21464;&#30340;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#12290;&#34429;&#28982;&#36895;&#24230;&#26356;&#24555;&#20960;&#20010;&#25968;&#37327;&#32423;&#65292;&#20294;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#22312;&#25972;&#20010;&#25968;&#25454;&#38598;&#19978;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#30456;&#31454;&#20105;&#65292;&#24182;&#19988;&#21487;&#20197;&#23454;&#26102;&#26356;&#26032;&#30340;&#25991;&#26723;&#26816;&#32034;&#31995;&#32479;&#30340;&#24320;&#21457;&#12290;&#25105;&#20204;&#30340;IncDSI&#20195;&#30721;
&lt;/p&gt;
&lt;p&gt;
Differentiable Search Index is a recently proposed paradigm for document retrieval, that encodes information about a corpus of documents within the parameters of a neural network and directly maps queries to corresponding documents. These models have achieved state-of-the-art performances for document retrieval across many benchmarks. These kinds of models have a significant limitation: it is not easy to add new documents after a model is trained. We propose IncDSI, a method to add documents in real time (about 20-50ms per document), without retraining the model on the entire dataset (or even parts thereof). Instead we formulate the addition of documents as a constrained optimization problem that makes minimal changes to the network parameters. Although orders of magnitude faster, our approach is competitive with re-training the model on the whole dataset and enables the development of document retrieval systems that can be updated with new information in real-time. Our code for IncDSI
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#30340;&#30740;&#31350;&#20013;&#30340;&#21487;&#37325;&#22797;&#24615;&#38382;&#39064;&#65292;&#25351;&#20986;&#20102;&#26080;&#27861;&#22797;&#29616;&#30340;&#21407;&#22240;&#65292;&#20197;&#21450;&#24403;&#21069;&#21487;&#37325;&#22797;&#24615;&#27700;&#24179;&#19981;&#26029;&#25552;&#39640;&#30340;&#25361;&#25112;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#25903;&#25345;&#26426;&#22120;&#23398;&#20064;&#21487;&#37325;&#22797;&#24615;&#30340;&#28508;&#22312;&#39537;&#21160;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2307.10320</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#30340;&#30740;&#31350;&#20013;&#30340;&#21487;&#37325;&#22797;&#24615;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Reproducibility in Machine Learning-Driven Research. (arXiv:2307.10320v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10320
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#30340;&#30740;&#31350;&#20013;&#30340;&#21487;&#37325;&#22797;&#24615;&#38382;&#39064;&#65292;&#25351;&#20986;&#20102;&#26080;&#27861;&#22797;&#29616;&#30340;&#21407;&#22240;&#65292;&#20197;&#21450;&#24403;&#21069;&#21487;&#37325;&#22797;&#24615;&#27700;&#24179;&#19981;&#26029;&#25552;&#39640;&#30340;&#25361;&#25112;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#25903;&#25345;&#26426;&#22120;&#23398;&#20064;&#21487;&#37325;&#22797;&#24615;&#30340;&#28508;&#22312;&#39537;&#21160;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#38754;&#20020;&#30528;&#21487;&#37325;&#22797;&#24615;&#21361;&#26426;&#65292;&#35768;&#22810;&#30740;&#31350;&#30340;&#32467;&#26524;&#21644;&#21457;&#29616;&#24456;&#38590;&#29978;&#33267;&#19981;&#21487;&#33021;&#22797;&#29616;&#12290;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20063;&#38754;&#20020;&#21516;&#26679;&#30340;&#38382;&#39064;&#65292;&#36890;&#24120;&#36825;&#26159;&#22240;&#20026;&#26410;&#21457;&#24067;&#30340;&#25968;&#25454;&#21644;/&#25110;&#28304;&#20195;&#30721;&#65292;&#20197;&#21450;&#23545;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#26465;&#20214;&#30340;&#25935;&#24863;&#24615;&#12290;&#23613;&#31649;&#30740;&#31350;&#30028;&#35752;&#35770;&#20102;&#19981;&#21516;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#27604;&#22914;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#24179;&#21488;&#65292;&#20294;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#30340;&#30740;&#31350;&#30340;&#21487;&#37325;&#22797;&#24615;&#27700;&#24179;&#24182;&#27809;&#26377;&#26174;&#33879;&#25552;&#39640;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#31687;&#23567;&#22411;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#20851;&#20110;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#30340;&#30740;&#31350;&#20013;&#21487;&#37325;&#22797;&#24615;&#30340;&#25991;&#29486;&#65292;&#20027;&#35201;&#30446;&#30340;&#26377;&#19977;&#20010;&#65306;&#65288;&#19968;&#65289;&#21453;&#24605;&#21508;&#20010;&#30740;&#31350;&#39046;&#22495;&#20013;&#26426;&#22120;&#23398;&#20064;&#21487;&#37325;&#22797;&#24615;&#30340;&#29616;&#29366;&#65292;&#65288;&#20108;&#65289;&#35782;&#21035;&#36825;&#20123;&#24212;&#29992;&#20102;&#26426;&#22120;&#23398;&#20064;&#30340;&#30740;&#31350;&#39046;&#22495;&#20013;&#23384;&#22312;&#30340;&#21487;&#37325;&#22797;&#24615;&#38382;&#39064;&#21644;&#38556;&#30861;&#65292;&#65288;&#19977;&#65289;&#35782;&#21035;&#25903;&#25345;&#26426;&#22120;&#23398;&#20064;&#21487;&#37325;&#22797;&#24615;&#30340;&#21487;&#33021;&#39537;&#21160;&#22240;&#32032;&#65292;&#22914;&#24037;&#20855;&#12289;&#23454;&#36341;&#21644;&#24178;&#39044;&#25514;&#26045;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#24076;&#26395;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Research is facing a reproducibility crisis, in which the results and findings of many studies are difficult or even impossible to reproduce. This is also the case in machine learning (ML) and artificial intelligence (AI) research. Often, this is the case due to unpublished data and/or source-code, and due to sensitivity to ML training conditions. Although different solutions to address this issue are discussed in the research community such as using ML platforms, the level of reproducibility in ML-driven research is not increasing substantially. Therefore, in this mini survey, we review the literature on reproducibility in ML-driven research with three main aims: (i) reflect on the current situation of ML reproducibility in various research fields, (ii) identify reproducibility issues and barriers that exist in these research fields applying ML, and (iii) identify potential drivers such as tools, practices, and interventions that support ML reproducibility. With this, we hope to contr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#26641;&#22411;&#22402;&#30452;&#32852;&#21512;&#23398;&#20064;&#20013;&#30340;&#26631;&#31614;&#27844;&#38706;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26631;&#31614;&#25512;&#26029;&#25915;&#20987;&#26041;&#27861;ID2Graph&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;ID-LMID&#30340;&#38450;&#24481;&#26426;&#21046;&#65292;&#36890;&#36807;&#20851;&#27880;&#20114;&#20449;&#24687;&#27491;&#21017;&#21270;&#26469;&#38450;&#27490;&#26631;&#31614;&#27844;&#38706;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;ID2Graph&#25915;&#20987;&#23384;&#22312;&#26174;&#33879;&#30340;&#27844;&#38706;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.10318</link><description>&lt;p&gt;
&#28145;&#20837;&#30740;&#31350;&#28040;&#38500;&#26641;&#22411;&#22402;&#30452;&#32852;&#21512;&#23398;&#20064;&#20013;&#30340;&#26631;&#31614;&#27844;&#38706;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Eliminating Label Leakage in Tree-Based Vertical Federated Learning. (arXiv:2307.10318v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10318
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#26641;&#22411;&#22402;&#30452;&#32852;&#21512;&#23398;&#20064;&#20013;&#30340;&#26631;&#31614;&#27844;&#38706;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26631;&#31614;&#25512;&#26029;&#25915;&#20987;&#26041;&#27861;ID2Graph&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;ID-LMID&#30340;&#38450;&#24481;&#26426;&#21046;&#65292;&#36890;&#36807;&#20851;&#27880;&#20114;&#20449;&#24687;&#27491;&#21017;&#21270;&#26469;&#38450;&#27490;&#26631;&#31614;&#27844;&#38706;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;ID2Graph&#25915;&#20987;&#23384;&#22312;&#26174;&#33879;&#30340;&#27844;&#38706;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22402;&#30452;&#32852;&#21512;&#23398;&#20064;&#65288;VFL&#65289;&#20351;&#24471;&#20855;&#26377;&#20849;&#21516;&#29992;&#25143;&#38598;&#21512;&#30340;&#22810;&#20010;&#21442;&#19982;&#26041;&#33021;&#22815;&#22312;&#19981;&#20998;&#20139;&#31169;&#26377;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#30001;&#20110;&#20854;&#21487;&#35299;&#37322;&#24615;&#21644;&#25928;&#29575;&#65292;&#22522;&#20110;&#26641;&#32467;&#26500;&#30340;&#27169;&#22411;&#22312;VFL&#20013;&#21464;&#24471;&#27969;&#34892;&#36215;&#26469;&#12290;&#28982;&#32780;&#65292;&#26641;&#22411;VFL&#30340;&#33030;&#24369;&#24615;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30340;&#30740;&#31350;&#12290;&#26412;&#30740;&#31350;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26631;&#31614;&#25512;&#26029;&#25915;&#20987;&#26041;&#27861;ID2Graph&#65292;&#35813;&#25915;&#20987;&#21033;&#29992;&#27599;&#20010;&#33410;&#28857;&#65288;&#21363;&#23454;&#20363;&#31354;&#38388;&#65289;&#20998;&#37197;&#30340;&#35760;&#24405;&#26631;&#35782;&#38598;&#21512;&#26469;&#25512;&#23548;&#31169;&#26377;&#35757;&#32451;&#26631;&#31614;&#12290;ID2Graph&#25915;&#20987;&#29983;&#25104;&#35757;&#32451;&#26679;&#26412;&#30340;&#22270;&#32467;&#26500;&#65292;&#20174;&#22270;&#20013;&#25552;&#21462;&#31038;&#21306;&#65292;&#24182;&#20351;&#29992;&#31038;&#21306;&#20449;&#24687;&#23545;&#23616;&#37096;&#25968;&#25454;&#38598;&#36827;&#34892;&#32858;&#31867;&#12290;&#20026;&#20102;&#25269;&#24481;&#23454;&#20363;&#31354;&#38388;&#20013;&#30340;&#26631;&#31614;&#27844;&#38706;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#38450;&#24481;&#26426;&#21046;ID-LMID&#65292;&#35813;&#26426;&#21046;&#36890;&#36807;&#20851;&#27880;&#20114;&#20449;&#24687;&#27491;&#21017;&#21270;&#26469;&#38450;&#27490;&#26631;&#31614;&#27844;&#38706;&#12290;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#32508;&#21512;&#23454;&#39564;&#34920;&#26126;&#65292;ID2Graph&#25915;&#20987;&#21576;&#29616;&#20986;&#26174;&#33879;&#30340;&#27844;&#38706;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vertical federated learning (VFL) enables multiple parties with disjoint features of a common user set to train a machine learning model without sharing their private data. Tree-based models have become prevalent in VFL due to their interpretability and efficiency. However, the vulnerability of tree-based VFL has not been sufficiently investigated. In this study, we first introduce a novel label inference attack, ID2Graph, which utilizes the sets of record-IDs assigned to each node (i.e., instance space) to deduce private training labels. The ID2Graph attack generates a graph structure from training samples, extracts communities from the graph, and clusters the local dataset using community information. To counteract label leakage from the instance space, we propose an effective defense mechanism, ID-LMID, which prevents label leakage by focusing on mutual information regularization. Comprehensive experiments conducted on various datasets reveal that the ID2Graph attack presents signif
&lt;/p&gt;</description></item><item><title>FedBug&#26159;&#19968;&#20010;&#33258;&#24213;&#21521;&#19978;&#36880;&#28176;&#35299;&#20923;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20923;&#32467;&#21644;&#36880;&#28176;&#35299;&#20923;&#27169;&#22411;&#23618;&#65292;&#23454;&#29616;&#20102;&#19968;&#31181;&#26377;&#25928;&#32531;&#35299;&#23458;&#25143;&#31471;&#28418;&#31227;&#29616;&#35937;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.10317</link><description>&lt;p&gt;
FedBug: &#19968;&#31181;&#33258;&#24213;&#21521;&#19978;&#36880;&#28176;&#35299;&#20923;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
FedBug: A Bottom-Up Gradual Unfreezing Framework for Federated Learning. (arXiv:2307.10317v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10317
&lt;/p&gt;
&lt;p&gt;
FedBug&#26159;&#19968;&#20010;&#33258;&#24213;&#21521;&#19978;&#36880;&#28176;&#35299;&#20923;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20923;&#32467;&#21644;&#36880;&#28176;&#35299;&#20923;&#27169;&#22411;&#23618;&#65292;&#23454;&#29616;&#20102;&#19968;&#31181;&#26377;&#25928;&#32531;&#35299;&#23458;&#25143;&#31471;&#28418;&#31227;&#29616;&#35937;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#21327;&#20316;&#35757;&#32451;&#26694;&#26550;&#65292;&#20801;&#35768;&#22810;&#20010;&#23458;&#25143;&#31471;&#22312;&#19981;&#25439;&#23475;&#25968;&#25454;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#20026;&#20849;&#20139;&#27169;&#22411;&#20570;&#20986;&#36129;&#29486;&#12290;&#30001;&#20110;&#26412;&#22320;&#25968;&#25454;&#38598;&#30340;&#24322;&#26500;&#24615;&#65292;&#26356;&#26032;&#30340;&#23458;&#25143;&#31471;&#27169;&#22411;&#21487;&#33021;&#20250;&#36807;&#25311;&#21512;&#24182;&#19982;&#24444;&#27492;&#21457;&#25955;&#65292;&#36825;&#34987;&#31216;&#20026;&#23458;&#25143;&#31471;&#28418;&#31227;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FedBug&#65288;&#20855;&#26377;&#33258;&#24213;&#21521;&#19978;&#36880;&#28176;&#35299;&#20923;&#30340;&#32852;&#37030;&#23398;&#20064;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;FL&#26694;&#26550;&#65292;&#26088;&#22312;&#26377;&#25928;&#22320;&#20943;&#36731;&#23458;&#25143;&#31471;&#28418;&#31227;&#12290;FedBug&#33258;&#36866;&#24212;&#22320;&#21033;&#29992;&#27599;&#20010;&#20840;&#23616;&#36718;&#27425;&#26381;&#21153;&#22120;&#20998;&#21457;&#30340;&#23458;&#25143;&#31471;&#27169;&#22411;&#21442;&#25968;&#20316;&#20026;&#36328;&#23458;&#25143;&#31471;&#23545;&#40784;&#30340;&#21442;&#32771;&#28857;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#23458;&#25143;&#31471;&#19978;&#65292;FedBug&#20174;&#20923;&#32467;&#25972;&#20010;&#27169;&#22411;&#24320;&#22987;&#65292;&#28982;&#21518;&#36880;&#28176;&#35299;&#20923;&#23618;&#65292;&#20174;&#36755;&#20837;&#23618;&#21040;&#36755;&#20986;&#23618;&#12290;&#36825;&#31181;&#33258;&#24213;&#21521;&#19978;&#30340;&#26041;&#27861;&#20801;&#35768;&#27169;&#22411;&#35757;&#32451;&#35299;&#20923;&#30340;&#26032;&#23618;&#23558;&#25968;&#25454;&#25237;&#24433;&#21040;&#19968;&#20010;&#28508;&#22312;&#31354;&#38388;&#20013;&#65292;&#22312;&#36825;&#20010;&#31354;&#38388;&#20013;&#65292;&#20998;&#31163;&#36229;&#24179;&#38754;&#22312;&#25152;&#26377;&#23458;&#25143;&#31471;&#19978;&#20445;&#25345;&#19968;&#33268;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;FedBug
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) offers a collaborative training framework, allowing multiple clients to contribute to a shared model without compromising data privacy. Due to the heterogeneous nature of local datasets, updated client models may overfit and diverge from one another, commonly known as the problem of client drift. In this paper, we propose FedBug (Federated Learning with Bottom-Up Gradual Unfreezing), a novel FL framework designed to effectively mitigate client drift. FedBug adaptively leverages the client model parameters, distributed by the server at each global round, as the reference points for cross-client alignment. Specifically, on the client side, FedBug begins by freezing the entire model, then gradually unfreezes the layers, from the input layer to the output layer. This bottom-up approach allows models to train the newly thawed layers to project data into a latent space, wherein the separating hyperplanes remain consistent across all clients. We theoretically analyze F
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#23391;&#21152;&#25289;&#27468;&#26354;&#30340;&#27468;&#35789;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#23545;&#36825;&#20123;&#27468;&#26354;&#30340;&#24773;&#32490;&#36827;&#34892;&#22810;&#31867;&#20998;&#31867;&#65292;&#21253;&#25324;&#24555;&#20048;&#12289;&#24754;&#20260;&#12289;&#28010;&#28459;&#21644;&#25918;&#26494;&#65292;&#20026;&#20351;&#38899;&#20048;&#26356;&#36148;&#36817;&#20154;&#20204;&#30340;&#24773;&#24863;&#20570;&#20986;&#20102;&#37325;&#35201;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2307.10314</link><description>&lt;p&gt;
&#22522;&#20110;&#27468;&#35789;&#30340;&#23391;&#21152;&#25289;&#27468;&#26354;&#24773;&#32490;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Mood Classification of Bangla Songs Based on Lyrics. (arXiv:2307.10314v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10314
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#23391;&#21152;&#25289;&#27468;&#26354;&#30340;&#27468;&#35789;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#23545;&#36825;&#20123;&#27468;&#26354;&#30340;&#24773;&#32490;&#36827;&#34892;&#22810;&#31867;&#20998;&#31867;&#65292;&#21253;&#25324;&#24555;&#20048;&#12289;&#24754;&#20260;&#12289;&#28010;&#28459;&#21644;&#25918;&#26494;&#65292;&#20026;&#20351;&#38899;&#20048;&#26356;&#36148;&#36817;&#20154;&#20204;&#30340;&#24773;&#24863;&#20570;&#20986;&#20102;&#37325;&#35201;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#20048;&#33021;&#21796;&#36215;&#21508;&#31181;&#24773;&#32490;&#65292;&#38543;&#30528;&#25216;&#26415;&#30340;&#36827;&#27493;&#65292;&#20154;&#20204;&#23545;&#38899;&#20048;&#30340;&#25509;&#35302;&#20063;&#36234;&#26469;&#36234;&#22810;&#12290;&#28982;&#32780;&#23545;&#20110;&#23637;&#29616;&#19981;&#21516;&#20154;&#31867;&#24773;&#24863;&#30340;&#23391;&#21152;&#25289;&#38899;&#20048;&#65292;&#30456;&#20851;&#30340;&#30740;&#31350;&#23578;&#19981;&#36275;&#22815;&#12290;&#26412;&#25991;&#30340;&#20316;&#32773;&#26088;&#22312;&#36890;&#36807;&#20998;&#26512;&#23391;&#21152;&#25289;&#27468;&#26354;&#30340;&#27468;&#35789;&#26469;&#20998;&#31867;&#20854;&#24773;&#32490;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#30740;&#31350;&#20154;&#21592;&#25910;&#38598;&#20102;4000&#39318;&#23391;&#21152;&#25289;&#27468;&#26354;&#30340;&#27468;&#35789;&#21644;&#27969;&#27966;&#65292;&#24182;&#36816;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;BERT&#31639;&#27861;&#26469;&#20998;&#26512;&#25968;&#25454;&#12290;&#22312;&#36825;4000&#39318;&#27468;&#26354;&#20013;&#65292;1513&#39318;&#20195;&#34920;&#24754;&#20260;&#24773;&#32490;&#65292;1362&#39318;&#20195;&#34920;&#28010;&#28459;&#24773;&#32490;&#65292;886&#39318;&#20195;&#34920;&#24555;&#20048;&#65292;&#20854;&#20313;&#30340;239&#39318;&#34987;&#24402;&#31867;&#20026;&#25918;&#26494;&#12290;&#36890;&#36807;&#23884;&#20837;&#27468;&#35789;&#65292;&#20316;&#32773;&#23558;&#36825;&#20123;&#27468;&#26354;&#20998;&#20026;&#22235;&#31181;&#24773;&#32490;&#65306;&#24555;&#20048;&#12289;&#24754;&#20260;&#12289;&#28010;&#28459;&#21644;&#25918;&#26494;&#12290;&#35813;&#30740;&#31350;&#23545;&#20110;&#23454;&#29616;&#38899;&#20048;&#30340;&#22810;&#31867;&#24773;&#32490;&#20998;&#31867;&#33267;&#20851;&#37325;&#35201;&#65292;&#20351;&#38899;&#20048;&#26356;&#33021;&#19982;&#20154;&#20204;&#30340;&#24773;&#24863;&#20135;&#29983;&#20849;&#40483;&#12290;&#35813;&#25991;&#31456;&#35814;&#32454;&#25551;&#36848;&#20102;&#36890;&#36807;&#27468;&#35789;&#20934;&#30830;&#25512;&#23548;&#20986;&#30340;&#22235;&#31181;&#24773;&#32490;&#30340;&#33258;&#21160;&#21270;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Music can evoke various emotions, and with the advancement of technology, it has become more accessible to people. Bangla music, which portrays different human emotions, lacks sufficient research. The authors of this article aim to analyze Bangla songs and classify their moods based on the lyrics. To achieve this, this research has compiled a dataset of 4000 Bangla song lyrics, genres, and used Natural Language Processing and the Bert Algorithm to analyze the data. Among the 4000 songs, 1513 songs are represented for the sad mood, 1362 for the romantic mood, 886 for happiness, and the rest 239 are classified as relaxation. By embedding the lyrics of the songs, the authors have classified the songs into four moods: Happy, Sad, Romantic, and Relaxed. This research is crucial as it enables a multi-class classification of songs' moods, making the music more relatable to people's emotions. The article presents the automated result of the four moods accurately derived from the song lyrics.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;ProActive&#65292;&#19968;&#31181;&#31070;&#32463;&#26631;&#35760;&#30340;&#26102;&#38388;&#28857;&#36807;&#31243;&#65288;MTPP&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#24314;&#27169;&#20154;&#31867;&#27963;&#21160;&#24207;&#21015;&#30340;&#21160;&#24577;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#21161;&#20110;&#27963;&#21160;&#25345;&#32493;&#26102;&#38388;&#39044;&#27979;&#12289;&#30446;&#26631;&#39044;&#27979;&#21644;&#19979;&#19968;&#27493;&#34892;&#21160;&#25512;&#33616;&#31561;&#19979;&#28216;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2307.10305</link><description>&lt;p&gt;
&#26102;&#20809;&#19982;&#34892;&#21160;&#20043;&#32455;&#65306;&#20351;&#29992;&#26102;&#38388;&#28857;&#36807;&#31243;&#27969;&#24314;&#27169;&#20154;&#31867;&#27963;&#21160;&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
Tapestry of Time and Actions: Modeling Human Activity Sequences using Temporal Point Process Flows. (arXiv:2307.10305v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10305
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;ProActive&#65292;&#19968;&#31181;&#31070;&#32463;&#26631;&#35760;&#30340;&#26102;&#38388;&#28857;&#36807;&#31243;&#65288;MTPP&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#24314;&#27169;&#20154;&#31867;&#27963;&#21160;&#24207;&#21015;&#30340;&#21160;&#24577;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#21161;&#20110;&#27963;&#21160;&#25345;&#32493;&#26102;&#38388;&#39044;&#27979;&#12289;&#30446;&#26631;&#39044;&#27979;&#21644;&#19979;&#19968;&#27493;&#34892;&#21160;&#25512;&#33616;&#31561;&#19979;&#28216;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#22987;&#32456;&#21442;&#19982;&#21508;&#31181;&#21508;&#26679;&#30340;&#27963;&#21160;&#21644;&#20219;&#21153;&#65292;&#23637;&#31034;&#20102;&#20182;&#20204;&#36866;&#24212;&#19981;&#21516;&#22330;&#26223;&#30340;&#33021;&#21147;&#12290;&#20219;&#20309;&#20154;&#31867;&#27963;&#21160;&#37117;&#21487;&#20197;&#34987;&#34920;&#31034;&#20026;&#23454;&#29616;&#29305;&#23450;&#30446;&#26631;&#32780;&#25191;&#34892;&#30340;&#34892;&#21160;&#30340;&#26102;&#38388;&#24207;&#21015;&#12290;&#19982;&#20174;&#30005;&#23376;&#35774;&#22791;&#25110;&#26426;&#22120;&#20013;&#25552;&#21462;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#19981;&#21516;&#65292;&#36825;&#20123;&#34892;&#21160;&#24207;&#21015;&#22312;&#20854;&#24615;&#36136;&#19978;&#39640;&#24230;&#19981;&#21516; - &#23436;&#25104;&#34892;&#21160;&#24207;&#21015;&#30340;&#26102;&#38388;&#21487;&#33021;&#20250;&#22240;&#19981;&#21516;&#20010;&#20307;&#32780;&#24322;&#12290;&#22240;&#27492;&#65292;&#29702;&#35299;&#36825;&#20123;&#24207;&#21015;&#30340;&#21160;&#24577;&#23545;&#20110;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#65288;&#22914;&#27963;&#21160;&#25345;&#32493;&#26102;&#38388;&#39044;&#27979;&#65292;&#30446;&#26631;&#39044;&#27979;&#65292;&#19979;&#19968;&#27493;&#34892;&#21160;&#25512;&#33616;&#31561;&#65289;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#36830;&#32493;&#26102;&#38388;&#27963;&#21160;&#24207;&#21015;&#65288;Continuous-Time Activity Sequence&#65292;CTAS&#65289;&#30340;&#26041;&#27861;&#20165;&#38480;&#20110;&#20165;&#26377;&#35270;&#35273;&#25968;&#25454;&#23384;&#22312;&#25110;&#20165;&#36866;&#29992;&#20110;&#29305;&#23450;&#20219;&#21153;&#65292;&#21363;&#20165;&#38480;&#20110;&#19979;&#19968;&#27493;&#34892;&#21160;&#25110;&#30446;&#26631;&#39044;&#27979;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ProActive&#65292;&#19968;&#31181;&#31070;&#32463;&#26631;&#35760;&#30340;&#26102;&#38388;&#28857;&#36807;&#31243;&#65288;Marked Temporal Point Process&#65292;MTPP&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#24314;&#27169;&#36830;&#32493;&#30340;&#20154;&#31867;&#27963;&#21160;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human beings always engage in a vast range of activities and tasks that demonstrate their ability to adapt to different scenarios. Any human activity can be represented as a temporal sequence of actions performed to achieve a certain goal. Unlike the time series datasets extracted from electronics or machines, these action sequences are highly disparate in their nature -- the time to finish a sequence of actions can vary between different persons. Therefore, understanding the dynamics of these sequences is essential for many downstream tasks such as activity length prediction, goal prediction, next action recommendation, etc. Existing neural network-based approaches that learn a continuous-time activity sequence (or CTAS) are limited to the presence of only visual data or are designed specifically for a particular task, i.e., limited to next action or goal prediction. In this paper, we present ProActive, a neural marked temporal point process (MTPP) framework for modeling the continuou
&lt;/p&gt;</description></item><item><title>Polyffusion&#26159;&#19968;&#31181;&#22810;&#22768;&#20048;&#35889;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#20869;&#37096;&#21644;&#22806;&#37096;&#25511;&#21046;&#23454;&#29616;&#21487;&#25511;&#38899;&#20048;&#29983;&#25104;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#22810;&#39033;&#38899;&#20048;&#21019;&#20316;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#21253;&#25324;&#32473;&#23450;&#20276;&#22863;&#29983;&#25104;&#26059;&#24459;&#12289;&#32473;&#23450;&#26059;&#24459;&#29983;&#25104;&#20276;&#22863;&#12289;&#38899;&#20048;&#29255;&#27573;&#20462;&#34917;&#21644;&#38899;&#20048;&#32534;&#25490;&#31561;&#12290;</title><link>http://arxiv.org/abs/2307.10304</link><description>&lt;p&gt;
Polyffusion&#65306;&#19968;&#31181;&#20855;&#26377;&#20869;&#37096;&#21644;&#22806;&#37096;&#25511;&#21046;&#30340;&#22810;&#22768;&#20048;&#35889;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Polyffusion: A Diffusion Model for Polyphonic Score Generation with Internal and External Controls. (arXiv:2307.10304v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10304
&lt;/p&gt;
&lt;p&gt;
Polyffusion&#26159;&#19968;&#31181;&#22810;&#22768;&#20048;&#35889;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#20869;&#37096;&#21644;&#22806;&#37096;&#25511;&#21046;&#23454;&#29616;&#21487;&#25511;&#38899;&#20048;&#29983;&#25104;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#22810;&#39033;&#38899;&#20048;&#21019;&#20316;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#21253;&#25324;&#32473;&#23450;&#20276;&#22863;&#29983;&#25104;&#26059;&#24459;&#12289;&#32473;&#23450;&#26059;&#24459;&#29983;&#25104;&#20276;&#22863;&#12289;&#38899;&#20048;&#29255;&#27573;&#20462;&#34917;&#21644;&#38899;&#20048;&#32534;&#25490;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Polyffusion&#65292;&#19968;&#31181;&#20197;&#22270;&#20687;&#21270;&#30340;&#38050;&#29748;&#28369;&#36718;&#34920;&#31034;&#23558;&#38899;&#20048;&#29983;&#25104;&#20026;&#22810;&#22768;&#20048;&#35889;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#36827;&#34892;&#21487;&#25511;&#30340;&#38899;&#20048;&#29983;&#25104;&#65292;&#26377;&#20004;&#31181;&#33539;&#24335;&#65306;&#20869;&#37096;&#25511;&#21046;&#21644;&#22806;&#37096;&#25511;&#21046;&#12290;&#20869;&#37096;&#25511;&#21046;&#26159;&#25351;&#29992;&#25143;&#39044;&#23450;&#20041;&#38899;&#20048;&#30340;&#19968;&#37096;&#20998;&#65292;&#28982;&#21518;&#35753;&#27169;&#22411;&#22635;&#20805;&#21097;&#20313;&#37096;&#20998;&#65292;&#31867;&#20284;&#20110;&#25513;&#34109;&#38899;&#20048;&#29983;&#25104;&#65288;&#25110;&#38899;&#20048;&#20462;&#34917;&#65289;&#30340;&#20219;&#21153;&#12290;&#22806;&#37096;&#25511;&#21046;&#36890;&#36807;&#20132;&#21449;&#20851;&#27880;&#26426;&#21046;&#65292;&#23558;&#27169;&#22411;&#19982;&#22806;&#37096;&#20294;&#30456;&#20851;&#30340;&#20449;&#24687;(&#22914;&#21644;&#24358;&#12289;&#36136;&#22320;&#25110;&#20854;&#20182;&#29305;&#24449;)&#32467;&#21512;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;Polyffusion&#36890;&#36807;&#20351;&#29992;&#20869;&#37096;&#21644;&#22806;&#37096;&#25511;&#21046;&#65292;&#32479;&#19968;&#20102;&#19968;&#31995;&#21015;&#38899;&#20048;&#21019;&#20316;&#20219;&#21153;&#65292;&#21253;&#25324;&#32473;&#23450;&#20276;&#22863;&#29983;&#25104;&#26059;&#24459;&#65292;&#32473;&#23450;&#26059;&#24459;&#29983;&#25104;&#20276;&#22863;&#65292;&#20219;&#24847;&#38899;&#20048;&#29255;&#27573;&#20462;&#34917;&#65292;&#20197;&#21450;&#32473;&#23450;&#21644;&#24358;&#25110;&#36136;&#22320;&#36827;&#34892;&#38899;&#20048;&#32534;&#25490;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;Transformer&#21644;&#22522;&#20110;&#37319;&#26679;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Polyffusion, a diffusion model that generates polyphonic music scores by regarding music as image-like piano roll representations. The model is capable of controllable music generation with two paradigms: internal control and external control. Internal control refers to the process in which users pre-define a part of the music and then let the model infill the rest, similar to the task of masked music generation (or music inpainting). External control conditions the model with external yet related information, such as chord, texture, or other features, via the cross-attention mechanism. We show that by using internal and external controls, Polyffusion unifies a wide range of music creation tasks, including melody generation given accompaniment, accompaniment generation given melody, arbitrary music segment inpainting, and music arrangement given chords or textures. Experimental results show that our model significantly outperforms existing Transformer and sampling-based base
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#26512;&#22810;&#31181;&#20307;&#32946;&#36187;&#20107;&#30340;&#23454;&#26102;&#35780;&#35770;&#65292;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#33258;&#21160;&#35782;&#21035;&#20027;&#35201;&#34892;&#21160;&#65292;&#24182;&#36890;&#36807;&#20998;&#31867;&#21644;&#24773;&#24863;&#20998;&#26512;&#25552;&#21462;&#27934;&#35265;&#12290;</title><link>http://arxiv.org/abs/2307.10303</link><description>&lt;p&gt;
&#20998;&#26512;&#20307;&#32946;&#35780;&#35770;&#20197;&#23454;&#29616;&#33258;&#21160;&#35782;&#21035;&#20107;&#20214;&#24182;&#25552;&#21462;&#27934;&#35265;
&lt;/p&gt;
&lt;p&gt;
Analyzing sports commentary in order to automatically recognize events and extract insights. (arXiv:2307.10303v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10303
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#26512;&#22810;&#31181;&#20307;&#32946;&#36187;&#20107;&#30340;&#23454;&#26102;&#35780;&#35770;&#65292;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#33258;&#21160;&#35782;&#21035;&#20027;&#35201;&#34892;&#21160;&#65292;&#24182;&#36890;&#36807;&#20998;&#31867;&#21644;&#24773;&#24863;&#20998;&#26512;&#25552;&#21462;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20180;&#32454;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#22810;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#21644;&#26041;&#27861;&#65292;&#20197;&#33258;&#21160;&#35782;&#21035;&#20307;&#32946;&#36187;&#20107;&#20013;&#30340;&#20027;&#35201;&#34892;&#21160;&#12290;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#19981;&#21516;&#26469;&#28304;&#30340;&#29616;&#22330;&#20307;&#32946;&#35780;&#35770;&#65292;&#24182;&#23558;&#36825;&#20123;&#20027;&#35201;&#34892;&#21160;&#20998;&#31867;&#21040;&#19981;&#21516;&#30340;&#31867;&#21035;&#20013;&#65292;&#26469;&#25552;&#21462;&#27934;&#35265;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#24773;&#24863;&#20998;&#26512;&#26159;&#21542;&#21487;&#20197;&#24110;&#21161;&#26816;&#27979;&#36825;&#20123;&#20027;&#35201;&#34892;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we carefully investigate how we can use multiple different Natural Language Processing techniques and methods in order to automatically recognize the main actions in sports events. We aim to extract insights by analyzing live sport commentaries from different sources and by classifying these major actions into different categories. We also study if sentiment analysis could help detect these main actions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DRIG&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#19968;&#33324;&#24615;&#21487;&#21152;&#24178;&#39044;&#65292;&#22312;&#39044;&#27979;&#27169;&#22411;&#20013;&#32467;&#21512;&#20102;&#20869;&#20998;&#24067;&#39044;&#27979;&#21644;&#22240;&#26524;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#26410;&#35265;&#24178;&#39044;&#30340;&#40065;&#26834;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2307.10299</link><description>&lt;p&gt;
&#22240;&#26524;&#24615;&#23548;&#21521;&#30340;&#40065;&#26834;&#24615;&#65306;&#21033;&#29992;&#19968;&#33324;&#24615;&#21487;&#21152;&#24178;&#39044;
&lt;/p&gt;
&lt;p&gt;
Causality-oriented robustness: exploiting general additive interventions. (arXiv:2307.10299v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10299
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DRIG&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#19968;&#33324;&#24615;&#21487;&#21152;&#24178;&#39044;&#65292;&#22312;&#39044;&#27979;&#27169;&#22411;&#20013;&#32467;&#21512;&#20102;&#20869;&#20998;&#24067;&#39044;&#27979;&#21644;&#22240;&#26524;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#26410;&#35265;&#24178;&#39044;&#30340;&#40065;&#26834;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#32463;&#24120;&#21457;&#29983;&#20998;&#24067;&#21464;&#21270;&#65292;&#24613;&#38656;&#24320;&#21457;&#23545;&#36825;&#31181;&#21464;&#21270;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;&#29616;&#26377;&#30340;&#26694;&#26550;&#65292;&#22914;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#25110;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#65292;&#35201;&#20040;&#23545;&#26410;&#35265;&#20998;&#24067;&#32570;&#20047;&#36890;&#29992;&#24615;&#65292;&#35201;&#20040;&#20381;&#36182;&#20110;&#20551;&#23450;&#30340;&#36317;&#31163;&#24230;&#37327;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22240;&#26524;&#24615;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#21644;&#32467;&#26500;&#30340;&#31283;&#20581;&#39044;&#27979;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36827;&#34892;&#22240;&#26524;&#25512;&#26029;&#25152;&#38656;&#30340;&#20551;&#35774;&#21487;&#33021;&#36807;&#20110;&#20005;&#26684;&#65292;&#36825;&#31181;&#22240;&#26524;&#27169;&#22411;&#25552;&#20379;&#30340;&#40065;&#26834;&#24615;&#24120;&#24120;&#32570;&#20047;&#28789;&#27963;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#22240;&#26524;&#24615;&#23548;&#21521;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DRIG&#65288;Distributional Robustness via Invariant Gradients&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#19968;&#33324;&#24615;&#21487;&#21152;&#24178;&#39044;&#65292;&#20197;&#23454;&#29616;&#23545;&#26410;&#35265;&#24178;&#39044;&#30340;&#40065;&#26834;&#39044;&#27979;&#65292;&#24182;&#22312;&#20869;&#20998;&#24067;&#39044;&#27979;&#21644;&#22240;&#26524;&#24615;&#20043;&#38388;&#33258;&#28982;&#22320;&#36827;&#34892;&#25554;&#20540;&#12290;&#22312;&#32447;&#24615;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;DRIG&#20135;&#29983;&#30340;&#39044;&#27979;&#26159;
&lt;/p&gt;
&lt;p&gt;
Since distribution shifts are common in real-world applications, there is a pressing need for developing prediction models that are robust against such shifts. Existing frameworks, such as empirical risk minimization or distributionally robust optimization, either lack generalizability for unseen distributions or rely on postulated distance measures. Alternatively, causality offers a data-driven and structural perspective to robust predictions. However, the assumptions necessary for causal inference can be overly stringent, and the robustness offered by such causal models often lacks flexibility. In this paper, we focus on causality-oriented robustness and propose Distributional Robustness via Invariant Gradients (DRIG), a method that exploits general additive interventions in training data for robust predictions against unseen interventions, and naturally interpolates between in-distribution prediction and causality. In a linear setting, we prove that DRIG yields predictions that are 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#20083;&#33146;X&#20809;&#25668;&#24433;&#22270;&#20687;&#20013;&#20998;&#21106;&#20083;&#22836;&#12289;&#33016;&#22823;&#32908;&#12289;&#32420;&#32500;&#33146;&#20307;&#32452;&#32455;&#21644;&#33026;&#32938;&#32452;&#32455;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26694;&#26550;&#22312;&#21508;&#31181;&#22797;&#26434;&#24773;&#20917;&#19979;&#20855;&#26377;&#20934;&#30830;&#30340;&#20998;&#21106;&#24615;&#33021;&#65292;&#21487;&#29992;&#20110;&#20020;&#24202;&#23454;&#36341;&#20013;&#12290;</title><link>http://arxiv.org/abs/2307.10296</link><description>&lt;p&gt;
&#22312;&#20083;&#33146;X&#20809;&#25668;&#24433;&#22270;&#20687;&#20013;&#23454;&#29616;&#33258;&#21160;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Towards Automated Semantic Segmentation in Mammography Images. (arXiv:2307.10296v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10296
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#20083;&#33146;X&#20809;&#25668;&#24433;&#22270;&#20687;&#20013;&#20998;&#21106;&#20083;&#22836;&#12289;&#33016;&#22823;&#32908;&#12289;&#32420;&#32500;&#33146;&#20307;&#32452;&#32455;&#21644;&#33026;&#32938;&#32452;&#32455;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26694;&#26550;&#22312;&#21508;&#31181;&#22797;&#26434;&#24773;&#20917;&#19979;&#20855;&#26377;&#20934;&#30830;&#30340;&#20998;&#21106;&#24615;&#33021;&#65292;&#21487;&#29992;&#20110;&#20020;&#24202;&#23454;&#36341;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20083;&#33146;X&#20809;&#25668;&#24433;&#22270;&#20687;&#24191;&#27867;&#29992;&#20110;&#26816;&#27979;&#19981;&#21487;&#35302;&#21450;&#30340;&#20083;&#33146;&#30149;&#21464;&#25110;&#32467;&#33410;&#65292;&#20197;&#39044;&#38450;&#30284;&#30151;&#24182;&#22312;&#24517;&#35201;&#26102;&#25552;&#20379;&#24178;&#39044;&#30340;&#26426;&#20250;&#12290;&#35782;&#21035;&#19968;&#20123;&#24863;&#20852;&#36259;&#32467;&#26500;&#23545;&#20110;&#35786;&#26029;&#21644;&#35780;&#20272;&#22270;&#20687;&#30340;&#20805;&#20998;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#22240;&#27492;&#65292;&#35745;&#31639;&#26426;&#36741;&#21161;&#26816;&#27979;&#31995;&#32479;&#21487;&#20197;&#36890;&#36807;&#33258;&#21160;&#20998;&#21106;&#36825;&#20123;&#26631;&#24535;&#24615;&#32467;&#26500;&#26469;&#36741;&#21161;&#21307;&#23398;&#35299;&#37322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#26631;&#20934;&#35270;&#22270;&#20083;&#33146;X&#20809;&#25668;&#24433;&#22270;&#20687;&#20013;&#20998;&#21106;&#20083;&#22836;&#12289;&#33016;&#22823;&#32908;&#12289;&#32420;&#32500;&#33146;&#20307;&#32452;&#32455;&#21644;&#33026;&#32938;&#32452;&#32455;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22823;&#22411;&#31169;&#26377;&#20998;&#21106;&#25968;&#25454;&#38598;&#65292;&#24182;&#36827;&#34892;&#20102;&#32771;&#34385;&#19981;&#21516;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26550;&#26500;&#30340;&#22823;&#37327;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#19981;&#21516;&#21464;&#24322;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#20934;&#30830;&#30340;&#20998;&#21106;&#24615;&#33021;&#65292;&#34920;&#26126;&#35813;&#26694;&#26550;&#21487;&#20197;&#38598;&#25104;&#21040;&#20020;&#24202;&#23454;&#36341;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mammography images are widely used to detect non-palpable breast lesions or nodules, preventing cancer and providing the opportunity to plan interventions when necessary. The identification of some structures of interest is essential to make a diagnosis and evaluate image adequacy. Thus, computer-aided detection systems can be helpful in assisting medical interpretation by automatically segmenting these landmark structures. In this paper, we propose a deep learning-based framework for the segmentation of the nipple, the pectoral muscle, the fibroglandular tissue, and the fatty tissue on standard-view mammography images. We introduce a large private segmentation dataset and extensive experiments considering different deep-learning model architectures. Our experiments demonstrate accurate segmentation performance on variate and challenging cases, showing that this framework can be integrated into clinical practice.
&lt;/p&gt;</description></item><item><title>ECSIC&#26159;&#19968;&#31181;&#29992;&#20110;&#31435;&#20307;&#22270;&#20687;&#21387;&#32553;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#24038;&#21491;&#22270;&#20687;&#20043;&#38388;&#30340;&#30456;&#20114;&#20449;&#24687;&#36827;&#34892;&#32852;&#21512;&#21387;&#32553;&#65292;&#24182;&#20351;&#29992;&#26032;&#39062;&#30340;&#31435;&#20307;&#20132;&#21449;&#27880;&#24847;&#21147;&#27169;&#22359;&#21644;&#31435;&#20307;&#19978;&#19979;&#25991;&#27169;&#22359;&#23454;&#29616;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;ECSIC&#22312;&#20004;&#20010;&#27969;&#34892;&#30340;&#31435;&#20307;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#20855;&#26377;&#24555;&#36895;&#32534;&#30721;&#21644;&#35299;&#30721;&#30340;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.10284</link><description>&lt;p&gt;
ECSIC: &#29992;&#20110;&#31435;&#20307;&#22270;&#20687;&#21387;&#32553;&#30340;&#26497;&#32447;&#20132;&#21449;&#27880;&#24847;&#21147;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
ECSIC: Epipolar Cross Attention for Stereo Image Compression. (arXiv:2307.10284v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10284
&lt;/p&gt;
&lt;p&gt;
ECSIC&#26159;&#19968;&#31181;&#29992;&#20110;&#31435;&#20307;&#22270;&#20687;&#21387;&#32553;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#24038;&#21491;&#22270;&#20687;&#20043;&#38388;&#30340;&#30456;&#20114;&#20449;&#24687;&#36827;&#34892;&#32852;&#21512;&#21387;&#32553;&#65292;&#24182;&#20351;&#29992;&#26032;&#39062;&#30340;&#31435;&#20307;&#20132;&#21449;&#27880;&#24847;&#21147;&#27169;&#22359;&#21644;&#31435;&#20307;&#19978;&#19979;&#25991;&#27169;&#22359;&#23454;&#29616;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;ECSIC&#22312;&#20004;&#20010;&#27969;&#34892;&#30340;&#31435;&#20307;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#20855;&#26377;&#24555;&#36895;&#32534;&#30721;&#21644;&#35299;&#30721;&#30340;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23398;&#20064;&#26041;&#27861;ECSIC&#65292;&#29992;&#20110;&#31435;&#20307;&#22270;&#20687;&#21387;&#32553;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#31435;&#20307;&#22270;&#20687;&#23545;&#24038;&#21491;&#22270;&#20687;&#20043;&#38388;&#30340;&#30456;&#20114;&#20449;&#24687;&#65292;&#37319;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#31435;&#20307;&#20132;&#21449;&#27880;&#24847;&#21147;&#65288;SCA&#65289;&#27169;&#22359;&#21644;&#20004;&#20010;&#31435;&#20307;&#19978;&#19979;&#25991;&#27169;&#22359;&#65292;&#20197;&#32852;&#21512;&#26041;&#24335;&#21387;&#32553;&#24038;&#21491;&#22270;&#20687;&#12290;SCA&#27169;&#22359;&#22312;&#20004;&#20010;&#22270;&#20687;&#30340;&#23545;&#24212;&#26497;&#32447;&#33539;&#22260;&#20869;&#36827;&#34892;&#20132;&#21449;&#27880;&#24847;&#21147;&#22788;&#29702;&#65292;&#24182;&#19988;&#24182;&#34892;&#22788;&#29702;&#23427;&#20204;&#12290;&#31435;&#20307;&#19978;&#19979;&#25991;&#27169;&#22359;&#36890;&#36807;&#20351;&#29992;&#31532;&#19968;&#20010;&#22270;&#20687;&#20316;&#20026;&#19978;&#19979;&#25991;&#26469;&#25913;&#21892;&#23545;&#31532;&#20108;&#20010;&#32534;&#30721;&#22270;&#20687;&#30340;&#29109;&#20272;&#35745;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#21076;&#38500;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#27169;&#22359;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#19982;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23450;&#37327;&#21644;&#23450;&#24615;&#27604;&#36739;&#12290;ECSIC&#22312;&#20004;&#20010;&#24120;&#29992;&#30340;&#31435;&#20307;&#22270;&#20687;&#25968;&#25454;&#38598;Cityscapes&#21644;InStereo2k&#19978;&#36798;&#21040;&#20102;&#31435;&#20307;&#22270;&#20687;&#21387;&#32553;&#27169;&#22411;&#20013;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#21516;&#26102;&#20801;&#35768;&#24555;&#36895;&#32534;&#30721;&#21644;&#35299;&#30721;&#65292;&#38750;&#24120;&#36866;&#29992;&#20110;&#23454;&#26102;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present ECSIC, a novel learned method for stereo image compression. Our proposed method compresses the left and right images in a joint manner by exploiting the mutual information between the images of the stereo image pair using a novel stereo cross attention (SCA) module and two stereo context modules. The SCA module performs cross-attention restricted to the corresponding epipolar lines of the two images and processes them in parallel. The stereo context modules improve the entropy estimation of the second encoded image by using the first image as a context. We conduct an extensive ablation study demonstrating the effectiveness of the proposed modules and a comprehensive quantitative and qualitative comparison with existing methods. ECSIC achieves state-of-the-art performance among stereo image compression models on the two popular stereo image datasets Cityscapes and InStereo2k while allowing for fast encoding and decoding, making it highly practical for real-time
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#39046;&#22495;&#25935;&#24863;&#35821;&#38899;&#35782;&#21035;&#26041;&#27861;&#65292;&#21033;&#29992;&#25991;&#26412;&#25552;&#31034;&#26469;&#29983;&#25104;&#39046;&#22495;&#25935;&#24863;&#27169;&#22411;&#65292;&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#23454;&#29616;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#39046;&#22495;&#21644;&#25552;&#31034;&#19978;&#19979;&#25991;&#19979;&#22343;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#35789;&#38169;&#35823;&#29575;&#38477;&#20302;&#36798;&#21040;&#26368;&#39640;33%&#12290;&#36890;&#36807;&#20165;&#20351;&#29992;&#25991;&#26412;&#36827;&#34892;&#24494;&#35843;&#65292;&#35813;&#27169;&#22411;&#22312;&#21307;&#23398;&#23545;&#35805;&#25968;&#25454;&#38598;&#19978;&#30340;&#35782;&#21035;&#25928;&#26524;&#26368;&#20339;&#65292;&#35789;&#38169;&#35823;&#29575;&#38477;&#20302;&#36798;&#21040;29%&#12290;</title><link>http://arxiv.org/abs/2307.10274</link><description>&lt;p&gt;
&#20351;&#29992;&#25552;&#31034;&#26465;&#20214;&#24494;&#35843;&#23454;&#29616;&#38646;&#26679;&#26412;&#39046;&#22495;&#25935;&#24863;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Zero-shot Domain-sensitive Speech Recognition with Prompt-conditioning Fine-tuning. (arXiv:2307.10274v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10274
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#39046;&#22495;&#25935;&#24863;&#35821;&#38899;&#35782;&#21035;&#26041;&#27861;&#65292;&#21033;&#29992;&#25991;&#26412;&#25552;&#31034;&#26469;&#29983;&#25104;&#39046;&#22495;&#25935;&#24863;&#27169;&#22411;&#65292;&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#23454;&#29616;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#39046;&#22495;&#21644;&#25552;&#31034;&#19978;&#19979;&#25991;&#19979;&#22343;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#35789;&#38169;&#35823;&#29575;&#38477;&#20302;&#36798;&#21040;&#26368;&#39640;33%&#12290;&#36890;&#36807;&#20165;&#20351;&#29992;&#25991;&#26412;&#36827;&#34892;&#24494;&#35843;&#65292;&#35813;&#27169;&#22411;&#22312;&#21307;&#23398;&#23545;&#35805;&#25968;&#25454;&#38598;&#19978;&#30340;&#35782;&#21035;&#25928;&#26524;&#26368;&#20339;&#65292;&#35789;&#38169;&#35823;&#29575;&#38477;&#20302;&#36798;&#21040;29%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#21019;&#24314;&#21033;&#29992;&#25991;&#26412;&#39046;&#22495;&#20449;&#24687;&#30340;&#39046;&#22495;&#25935;&#24863;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#20854;&#29983;&#25104;&#26465;&#20214;&#21270;&#22312;&#32473;&#23450;&#30340;&#25991;&#26412;&#25552;&#31034;&#19978;&#23454;&#29616;&#12290;&#36890;&#36807;&#23545;&#39044;&#35757;&#32451;&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#65288;Whisper&#65289;&#36827;&#34892;&#24494;&#35843;&#65292;&#20174;&#25552;&#31034;&#31034;&#20363;&#20013;&#23398;&#20064;&#65292;&#36825;&#19968;&#30446;&#26631;&#24471;&#20197;&#23454;&#29616;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#33021;&#21147;&#21487;&#20197;&#25512;&#24191;&#21040;&#19981;&#21516;&#30340;&#39046;&#22495;&#21644;&#21508;&#31181;&#25552;&#31034;&#19978;&#19979;&#25991;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#30340;&#26410;&#35265;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#22810;&#36798;33&#65285;&#30340;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#38477;&#20302;&#65292;&#20363;&#22914;&#21307;&#23398;&#23545;&#35805;&#65292;&#31354;&#20013;&#20132;&#36890;&#25511;&#21046;&#36890;&#20449;&#21644;&#37329;&#34701;&#20250;&#35758;&#31561;&#12290;&#32771;&#34385;&#21040;&#38899;&#39057;-&#25991;&#26412;&#23545;&#25968;&#25454;&#30340;&#26377;&#38480;&#21487;&#29992;&#24615;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#25193;&#23637;&#21040;&#20165;&#25991;&#26412;&#24494;&#35843;&#65292;&#20197;&#23454;&#29616;&#39046;&#22495;&#25935;&#24863;&#24615;&#21644;&#39046;&#22495;&#36866;&#24212;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#20165;&#25991;&#26412;&#24494;&#35843;&#27169;&#22411;&#20063;&#21487;&#20197;&#20851;&#27880;&#21508;&#31181;&#25552;&#31034;&#19978;&#19979;&#25991;&#65292;&#35813;&#27169;&#22411;&#22312;&#21307;&#23398;&#23545;&#35805;&#25968;&#25454;&#38598;&#19978;&#30340;WER&#38477;&#20302;&#26368;&#22810;&#36798;&#21040;29&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose a method to create domain-sensitive speech recognition models that utilize textual domain information by conditioning its generation on a given text prompt. This is accomplished by fine-tuning a pre-trained, end-to-end model (Whisper) to learn from demonstrations with prompt examples. We show that this ability can be generalized to different domains and even various prompt contexts, with our model gaining a Word Error Rate (WER) reduction of up to 33% on unseen datasets from various domains, such as medical conversation, air traffic control communication, and financial meetings. Considering the limited availability of audio-transcript pair data, we further extend our method to text-only fine-tuning to achieve domain sensitivity as well as domain adaptation. We demonstrate that our text-only fine-tuned model can also attend to various prompt contexts, with the model reaching the most WER reduction of 29% on the medical conversation dataset.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#32422;&#26463;&#27714;&#35299;&#26041;&#27861;NeuralSAT&#65292;&#29992;&#20110;&#39564;&#35777;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;Preliminary results show that the NeuralSAT prototype is competitive to the state-of-the-art.</title><link>http://arxiv.org/abs/2307.10266</link><description>&lt;p&gt;
&#29992;&#20110;&#39564;&#35777;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;DPLL(T)&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A DPLL(T) Framework for Verifying Deep Neural Networks. (arXiv:2307.10266v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10266
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#32422;&#26463;&#27714;&#35299;&#26041;&#27861;NeuralSAT&#65292;&#29992;&#20110;&#39564;&#35777;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;Preliminary results show that the NeuralSAT prototype is competitive to the state-of-the-art.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#24050;&#32463;&#25104;&#20026;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#38382;&#39064;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#19982;&#20154;&#24037;&#32534;&#20889;&#30340;&#36719;&#20214;&#19968;&#26679;&#65292;&#33258;&#21160;&#29983;&#25104;&#30340;DNNs&#21487;&#33021;&#23384;&#22312;&#38169;&#35823;&#24182;&#21463;&#21040;&#25915;&#20987;&#12290;&#22240;&#27492;&#65292;&#36817;&#24180;&#26469;&#22312;&#24320;&#21457;&#26377;&#25928;&#21644;&#21487;&#25193;&#23637;&#30340;DNN&#39564;&#35777;&#25216;&#26415;&#21644;&#24037;&#20855;&#26041;&#38754;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;NeuralSAT&#65292;&#19968;&#31181;&#29992;&#20110;DNN&#39564;&#35777;&#30340;&#26032;&#30340;&#32422;&#26463;&#27714;&#35299;&#26041;&#27861;&#12290;NeuralSAT&#30340;&#35774;&#35745;&#36981;&#24490;&#20102;&#29992;&#20110;&#29616;&#20195;SMT&#27714;&#35299;&#30340;DPLL(T)&#31639;&#27861;&#65292;&#21253;&#25324;&#65288;&#20914;&#31361;&#65289;&#23376;&#21477;&#23398;&#20064;&#12289;&#25277;&#35937;&#21644;&#29702;&#35770;&#27714;&#35299;&#65292;&#22240;&#27492;NeuralSAT&#21487;&#20197;&#34987;&#35270;&#20026;DNNs&#30340;&#19968;&#20010;SMT&#26694;&#26550;&#12290;&#21021;&#27493;&#32467;&#26524;&#26174;&#31034;&#65292;NeuralSAT&#21407;&#22411;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#31454;&#20105;&#12290;&#25105;&#20204;&#24076;&#26395;&#36890;&#36807;&#36866;&#24403;&#30340;&#20248;&#21270;&#21644;&#24037;&#31243;&#21270;&#65292;NeuralSAT&#33021;&#22815;&#23558;&#29616;&#20195;SAT/SMT&#27714;&#35299;&#22120;&#30340;&#33021;&#21147;&#21644;&#25104;&#21151;&#24102;&#21040;DNN&#39564;&#35777;&#20013;&#12290;NeuralSAT&#21487;&#20174;&#20197;&#19979;&#38142;&#25509;&#33719;&#21462;&#65306;https://github.com/dynaroars/neuralsat-solver
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks (DNNs) have emerged as an effective approach to tackling real-world problems. However, like human-written software, automatically-generated DNNs can have bugs and be attacked. This thus attracts many recent interests in developing effective and scalable DNN verification techniques and tools. In this work, we introduce a NeuralSAT, a new constraint solving approach to DNN verification. The design of NeuralSAT follows the DPLL(T) algorithm used modern SMT solving, which includes (conflict) clause learning, abstraction, and theory solving, and thus NeuralSAT can be considered as an SMT framework for DNNs. Preliminary results show that the NeuralSAT prototype is competitive to the state-of-the-art. We hope, with proper optimization and engineering, NeuralSAT will carry the power and success of modern SAT/SMT solvers to DNN verification. NeuralSAT is avaliable from: https://github.com/dynaroars/neuralsat-solver
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#20351;&#29992;spotPython&#36827;&#34892;scikit-learn&#12289;PyTorch&#21644;river&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#20840;&#38754;&#25351;&#21335;&#12290;&#37325;&#28857;&#20171;&#32461;&#20102;spotPython&#30340;&#20248;&#21270;&#36807;&#31243;&#21644;&#36229;&#21442;&#25968;&#35843;&#25972;&#65292;&#24182;&#25552;&#20379;&#20102;&#20960;&#20010;&#23454;&#38469;&#26696;&#20363;&#30740;&#31350;&#12290;&#35813;&#25351;&#21335;&#20026;&#23545;Python&#36229;&#21442;&#25968;&#35843;&#25972;&#24863;&#20852;&#36259;&#30340;&#20154;&#20204;&#25552;&#20379;&#20102;&#23454;&#29992;&#30340;&#36215;&#28857;&#12290;</title><link>http://arxiv.org/abs/2307.10262</link><description>&lt;p&gt;
&#36229;&#21442;&#25968;&#35843;&#25972;&#25351;&#21335;&#65306;&#36866;&#29992;&#20110;scikit-learn&#12289;PyTorch&#12289;river&#21644;spotPython&#30340;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
Hyperparameter Tuning Cookbook: A guide for scikit-learn, PyTorch, river, and spotPython. (arXiv:2307.10262v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10262
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#20351;&#29992;spotPython&#36827;&#34892;scikit-learn&#12289;PyTorch&#21644;river&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#20840;&#38754;&#25351;&#21335;&#12290;&#37325;&#28857;&#20171;&#32461;&#20102;spotPython&#30340;&#20248;&#21270;&#36807;&#31243;&#21644;&#36229;&#21442;&#25968;&#35843;&#25972;&#65292;&#24182;&#25552;&#20379;&#20102;&#20960;&#20010;&#23454;&#38469;&#26696;&#20363;&#30740;&#31350;&#12290;&#35813;&#25351;&#21335;&#20026;&#23545;Python&#36229;&#21442;&#25968;&#35843;&#25972;&#24863;&#20852;&#36259;&#30340;&#20154;&#20204;&#25552;&#20379;&#20102;&#23454;&#29992;&#30340;&#36215;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#20351;&#29992;spotPython&#23545;scikit-learn&#12289;PyTorch&#21644;river&#36827;&#34892;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#20840;&#38754;&#25351;&#21335;&#12290;&#31532;&#19968;&#37096;&#20998;&#20171;&#32461;&#20102;spotPython&#30340;&#22522;&#20110;&#20195;&#29702;&#27169;&#22411;&#30340;&#20248;&#21270;&#36807;&#31243;&#65292;&#31532;&#20108;&#37096;&#20998;&#30528;&#37325;&#20171;&#32461;&#20102;&#36229;&#21442;&#25968;&#35843;&#25972;&#12290;&#25991;&#20013;&#25552;&#20379;&#20102;&#20960;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#21253;&#25324;&#23545;scikit-learn&#27169;&#22411;&#65288;&#22914;&#25903;&#25345;&#21521;&#37327;&#20998;&#31867;&#65292;&#38543;&#26426;&#26862;&#26519;&#65292;&#26799;&#24230;&#25552;&#21319;&#65288;XGB&#65289;&#21644;K&#26368;&#36817;&#37051;&#65288;KNN&#65289;&#65289;&#20197;&#21450;river&#20013;&#30340;Hoeffding&#33258;&#36866;&#24212;&#26641;&#22238;&#24402;&#22120;&#36827;&#34892;&#36229;&#21442;&#25968;&#35843;&#25972;&#12290;&#36824;&#35752;&#35770;&#20102;&#23558;spotPython&#38598;&#25104;&#21040;PyTorch&#21644;PyTorch Lightning&#35757;&#32451;&#24037;&#20316;&#27969;&#20013;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23454;&#36341;&#21644;&#36880;&#27493;&#35299;&#37322;&#30340;&#26041;&#24335;&#65292;&#26412;&#25163;&#20876;&#20026;&#23545;&#20351;&#29992;Python&#36827;&#34892;&#36229;&#21442;&#25968;&#35843;&#25972;&#24863;&#20852;&#36259;&#30340;&#20219;&#20309;&#20154;&#25552;&#20379;&#20102;&#23454;&#29992;&#30340;&#36215;&#28857;&#12290;&#37325;&#28857;&#21253;&#25324;Tensorboard&#12289;PyTorch Lightning&#12289;spotPython&#21644;river&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#35813;&#20986;&#29256;&#29289;&#27491;&#22312;&#24320;&#21457;&#20013;&#65292;&#26356;&#26032;&#20869;&#23481;&#21487;&#22312;&#23545;&#24212;&#30340;&#32593;&#39029;&#19978;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
This document provides a comprehensive guide to hyperparameter tuning using spotPython for scikit-learn, PyTorch, and river. The first part introduces spotPython's surrogate model-based optimization process, while the second part focuses on hyperparameter tuning. Several case studies are presented, including hyperparameter tuning for sklearn models such as Support Vector Classification, Random Forests, Gradient Boosting (XGB), and K-nearest neighbors (KNN), as well as a Hoeffding Adaptive Tree Regressor from river. The integration of spotPython into the PyTorch and PyTorch Lightning training workflow is also discussed. With a hands-on approach and step-by-step explanations, this cookbook serves as a practical starting point for anyone interested in hyperparameter tuning with Python. Highlights include the interplay between Tensorboard, PyTorch Lightning, spotPython, and river. This publication is under development, with updates available on the corresponding webpage.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#27169;&#24335;&#25366;&#25496;&#21644;&#32858;&#31867;&#20998;&#26512;&#33258;&#21160;&#21270;&#23398;&#29983;&#32593;&#32476;&#23433;&#20840;&#22521;&#35757;&#30340;&#35780;&#20272;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#20998;&#26512;&#23398;&#21592;&#19982;&#23398;&#20064;&#29615;&#22659;&#30340;&#20114;&#21160;&#25968;&#25454;&#65292;&#21487;&#20197;&#25581;&#31034;&#23398;&#21592;&#30340;&#23398;&#20064;&#36807;&#31243;&#65292;&#24182;&#20026;&#20182;&#20204;&#25552;&#20379;&#26377;&#38024;&#23545;&#24615;&#30340;&#21453;&#39304;&#26469;&#24110;&#21161;&#20182;&#20204;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2307.10260</link><description>&lt;p&gt;
&#21033;&#29992;&#27169;&#24335;&#25366;&#25496;&#21644;&#32858;&#31867;&#33258;&#21160;&#21270;&#23398;&#29983;&#32593;&#32476;&#23433;&#20840;&#22521;&#35757;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Student Assessment in Cybersecurity Training Automated by Pattern Mining and Clustering. (arXiv:2307.10260v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10260
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#27169;&#24335;&#25366;&#25496;&#21644;&#32858;&#31867;&#20998;&#26512;&#33258;&#21160;&#21270;&#23398;&#29983;&#32593;&#32476;&#23433;&#20840;&#22521;&#35757;&#30340;&#35780;&#20272;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#20998;&#26512;&#23398;&#21592;&#19982;&#23398;&#20064;&#29615;&#22659;&#30340;&#20114;&#21160;&#25968;&#25454;&#65292;&#21487;&#20197;&#25581;&#31034;&#23398;&#21592;&#30340;&#23398;&#20064;&#36807;&#31243;&#65292;&#24182;&#20026;&#20182;&#20204;&#25552;&#20379;&#26377;&#38024;&#23545;&#24615;&#30340;&#21453;&#39304;&#26469;&#24110;&#21161;&#20182;&#20204;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#36341;&#22411;&#32593;&#32476;&#23433;&#20840;&#22521;&#35757;&#20351;&#23398;&#29983;&#21644;&#19987;&#19994;&#20154;&#22763;&#21487;&#20197;&#20351;&#29992;&#21508;&#31181;&#24037;&#20855;&#24182;&#25552;&#39640;&#20854;&#25216;&#26415;&#33021;&#21147;&#12290;&#22521;&#35757;&#21457;&#29983;&#22312;&#19968;&#20010;&#20132;&#20114;&#24335;&#23398;&#20064;&#29615;&#22659;&#20013;&#65292;&#21487;&#20197;&#22312;&#23436;&#21892;&#30340;&#25805;&#20316;&#31995;&#32479;&#12289;&#32593;&#32476;&#21644;&#24212;&#29992;&#31243;&#24207;&#20013;&#23436;&#25104;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#22312;&#22521;&#35757;&#36807;&#31243;&#20013;&#65292;&#23398;&#20064;&#29615;&#22659;&#21487;&#20197;&#25910;&#38598;&#26377;&#20851;&#23398;&#21592;&#19982;&#29615;&#22659;&#20114;&#21160;&#30340;&#25968;&#25454;&#65292;&#20363;&#22914;&#20182;&#20204;&#23545;&#21629;&#20196;&#34892;&#24037;&#20855;&#30340;&#20351;&#29992;&#12290;&#36825;&#20123;&#25968;&#25454;&#21253;&#21547;&#34920;&#26126;&#23398;&#21592;&#23398;&#20064;&#36807;&#31243;&#30340;&#27169;&#24335;&#65292;&#25581;&#31034;&#23427;&#20204;&#21487;&#20197;&#35780;&#20272;&#23398;&#21592;&#24182;&#25552;&#20379;&#21453;&#39304;&#26469;&#24110;&#21161;&#20182;&#20204;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#23545;&#36825;&#20123;&#25968;&#25454;&#30340;&#33258;&#21160;&#21270;&#20998;&#26512;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22521;&#35757;&#20219;&#21153;&#28041;&#21450;&#22797;&#26434;&#30340;&#38382;&#39064;&#35299;&#20915;&#65292;&#26377;&#22810;&#31181;&#19981;&#21516;&#30340;&#35299;&#20915;&#26041;&#27861;&#21487;&#33021;&#12290;&#27492;&#22806;&#65292;&#23398;&#21592;&#20135;&#29983;&#20102;&#22823;&#37327;&#30340;&#20114;&#21160;&#25968;&#25454;&#12290;&#26412;&#25991;&#20351;&#29992;&#25968;&#25454;&#25366;&#25496;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#23545;&#26469;&#33258;18&#20010;&#32593;&#32476;&#23433;&#20840;&#22521;&#35757;&#20250;&#35805;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hands-on cybersecurity training allows students and professionals to practice various tools and improve their technical skills. The training occurs in an interactive learning environment that enables completing sophisticated tasks in full-fledged operating systems, networks, and applications. During the training, the learning environment allows collecting data about trainees' interactions with the environment, such as their usage of command-line tools. These data contain patterns indicative of trainees' learning processes, and revealing them allows to assess the trainees and provide feedback to help them learn. However, automated analysis of these data is challenging. The training tasks feature complex problem-solving, and many different solution approaches are possible. Moreover, the trainees generate vast amounts of interaction data. This paper explores a dataset from 18 cybersecurity training sessions using data mining and machine learning techniques. We employed pattern mining and 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#22312;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#20013;&#20351;&#29992;AdaBoost&#22686;&#24378;&#30340;HMM&#21644;&#20351;&#29992;&#22810;&#27425;&#38543;&#26426;&#37325;&#21551;&#35757;&#32451;&#30340;HMM&#65292;&#24182;&#21457;&#29616;&#38543;&#26426;&#37325;&#21551;&#23545;&#20110;&#22823;&#37096;&#20998;&#24773;&#20917;&#34920;&#29616;&#20986;&#20196;&#20154;&#24778;&#21916;&#30340;&#25928;&#26524;&#12290;&#21482;&#26377;&#22312;&#35757;&#32451;&#25968;&#25454;&#20005;&#37325;&#26377;&#38480;&#30340;&#26368;&#22256;&#38590;&#30340;&#24773;&#20917;&#19979;&#65292;&#22686;&#24378;&#25165;&#25552;&#20379;&#20102;&#36275;&#22815;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2307.10256</link><description>&lt;p&gt;
&#38543;&#26426;&#37325;&#21551;&#38544;&#34255;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#19982;Boosting&#22312;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#20013;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Hidden Markov Models with Random Restarts vs Boosting for Malware Detection. (arXiv:2307.10256v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10256
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#22312;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#20013;&#20351;&#29992;AdaBoost&#22686;&#24378;&#30340;HMM&#21644;&#20351;&#29992;&#22810;&#27425;&#38543;&#26426;&#37325;&#21551;&#35757;&#32451;&#30340;HMM&#65292;&#24182;&#21457;&#29616;&#38543;&#26426;&#37325;&#21551;&#23545;&#20110;&#22823;&#37096;&#20998;&#24773;&#20917;&#34920;&#29616;&#20986;&#20196;&#20154;&#24778;&#21916;&#30340;&#25928;&#26524;&#12290;&#21482;&#26377;&#22312;&#35757;&#32451;&#25968;&#25454;&#20005;&#37325;&#26377;&#38480;&#30340;&#26368;&#22256;&#38590;&#30340;&#24773;&#20917;&#19979;&#65292;&#22686;&#24378;&#25165;&#25552;&#20379;&#20102;&#36275;&#22815;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#26159;&#26500;&#24314;&#23433;&#20840;&#25968;&#23383;&#31995;&#32479;&#30340;&#30740;&#31350;&#21069;&#27839;&#12290;&#19982;&#35768;&#22810;&#20854;&#20182;&#39046;&#22495;&#19968;&#26679;&#65292;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#30740;&#31350;&#22312;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26041;&#38754;&#26377;&#20102;&#26174;&#33879;&#22686;&#38271;&#12290;&#22312;&#27169;&#24335;&#21305;&#37197;&#21644;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#39046;&#22495;&#20013;&#65292;&#19968;&#31181;&#34987;&#24191;&#27867;&#20351;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#26159;&#38544;&#34255;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#65288;HMMs&#65289;&#12290;HMM&#30340;&#35757;&#32451;&#22522;&#20110;&#29228;&#23665;&#31639;&#27861;&#65292;&#22240;&#27492;&#25105;&#20204;&#36890;&#24120;&#21487;&#20197;&#36890;&#36807;&#22810;&#27425;&#20351;&#29992;&#19981;&#21516;&#30340;&#21021;&#22987;&#20540;&#36827;&#34892;&#35757;&#32451;&#26469;&#25913;&#21892;&#27169;&#22411;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#22312;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#30340;&#32972;&#26223;&#19979;&#65292;&#27604;&#36739;&#20102;&#20351;&#29992;AdaBoost&#36827;&#34892;&#22686;&#24378;&#30340;HMM&#19982;&#20351;&#29992;&#22810;&#27425;&#38543;&#26426;&#37325;&#21551;&#35757;&#32451;&#30340;HMM&#12290;&#36825;&#20123;&#25216;&#26415;&#34987;&#24212;&#29992;&#20110;&#21508;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24694;&#24847;&#36719;&#20214;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#19982;&#22686;&#24378;&#30456;&#27604;&#19979;&#65292;&#38543;&#26426;&#37325;&#21551;&#34920;&#29616;&#20986;&#20196;&#20154;&#24778;&#35766;&#30340;&#22909;&#12290;&#21482;&#26377;&#22312;&#26368;&#22256;&#38590;&#30340;&#8220;&#20919;&#21551;&#21160;&#8221;&#24773;&#20917;&#19979;&#65288;&#35757;&#32451;&#25968;&#25454;&#20005;&#37325;&#26377;&#38480;&#65289;&#65292;&#22686;&#24378;&#20284;&#20046;&#25552;&#20379;&#36275;&#22815;&#30340;&#25913;&#36827;&#26469;&#20351;&#20854;&#26377;&#27491;&#24403;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Effective and efficient malware detection is at the forefront of research into building secure digital systems. As with many other fields, malware detection research has seen a dramatic increase in the application of machine learning algorithms. One machine learning technique that has been used widely in the field of pattern matching in general-and malware detection in particular-is hidden Markov models (HMMs). HMM training is based on a hill climb, and hence we can often improve a model by training multiple times with different initial values. In this research, we compare boosted HMMs (using AdaBoost) to HMMs trained with multiple random restarts, in the context of malware detection. These techniques are applied to a variety of challenging malware datasets. We find that random restarts perform surprisingly well in comparison to boosting. Only in the most difficult "cold start" cases (where training data is severely limited) does boosting appear to offer sufficient improvement to justi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#36873;&#25321;&#24615;&#27880;&#24847;&#21147;LSTM&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#32570;&#22833;&#30340;&#20117;&#26354;&#32447;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.10253</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#36873;&#25321;&#24615;&#27880;&#24847;&#21147;LSTM&#29992;&#20110;&#20117;&#26354;&#32447;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Efficient selective attention LSTM for well log curve synthesis. (arXiv:2307.10253v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10253
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#36873;&#25321;&#24615;&#27880;&#24847;&#21147;LSTM&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#32570;&#22833;&#30340;&#20117;&#26354;&#32447;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#26680;&#24515;&#38075;&#20117;&#36880;&#28176;&#25104;&#20026;&#22320;&#36136;&#24037;&#31243;&#20013;&#30340;&#20027;&#35201;&#21208;&#25506;&#26041;&#27861;&#65292;&#20117;&#26354;&#32447;&#20316;&#20026;&#22320;&#36136;&#20449;&#24687;&#30340;&#20027;&#35201;&#36733;&#20307;&#26085;&#30410;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22320;&#36136;&#29615;&#22659;&#12289;&#27979;&#20117;&#35774;&#22791;&#12289;&#38075;&#23380;&#36136;&#37327;&#21644;&#31361;&#21457;&#20107;&#20214;&#31561;&#22240;&#32032;&#37117;&#20250;&#24433;&#21709;&#20117;&#26354;&#32447;&#30340;&#36136;&#37327;&#12290;&#20197;&#24448;&#30340;&#37325;&#26032;&#27979;&#20117;&#25110;&#25163;&#24037;&#20462;&#27491;&#26041;&#27861;&#25104;&#26412;&#39640;&#25928;&#29575;&#20302;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29616;&#26377;&#25968;&#25454;&#39044;&#27979;&#32570;&#22833;&#20117;&#26354;&#32447;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;&#25152;&#25552;&#26041;&#27861;&#22312;&#20256;&#32479;&#30340;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#31070;&#32463;&#32593;&#32476;&#22522;&#30784;&#19978;&#21152;&#20837;&#20102;&#33258;&#27880;&#24847;&#26426;&#21046;&#26469;&#20998;&#26512;&#25968;&#25454;&#30340;&#31354;&#38388;&#20381;&#36182;&#24615;&#12290;&#23427;&#26377;&#36873;&#25321;&#22320;&#23558;LSTM&#20013;&#30340;&#20027;&#23548;&#35745;&#31639;&#32467;&#26524;&#21253;&#25324;&#22312;&#20869;&#65292;&#23558;&#35745;&#31639;&#22797;&#26434;&#24230;&#20174;O(n^2)&#38477;&#33267;O(nlogn)&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Non-core drilling has gradually become the primary exploration method in geological engineering, and well logging curves have increasingly gained importance as the main carriers of geological information. However, factors such as geological environment, logging equipment, borehole quality, and unexpected events can all impact the quality of well logging curves. Previous methods of re-logging or manual corrections have been associated with high costs and low efficiency. This paper proposes a machine learning method that utilizes existing data to predict missing well logging curves, and its effectiveness and feasibility have been validated through experiments. The proposed method builds upon the traditional Long Short-Term Memory (LSTM) neural network by incorporating a self-attention mechanism to analyze the spatial dependencies of the data. It selectively includes the dominant computational results in the LSTM, reducing the computational complexity from O(n^2) to O(nlogn) and improving
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#32593;&#32476;&#23041;&#32961;&#24402;&#22240;&#20013;&#65292;&#39640;&#32423;&#25915;&#20987;&#27169;&#24335;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#25928;&#26524;&#65292;&#30456;&#27604;&#20110;&#20302;&#32423;&#25915;&#20987;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2307.10252</link><description>&lt;p&gt;
&#19968;&#20010;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#23454;&#35777;&#35780;&#20272;&#65306;&#22312;&#24402;&#22240;&#25915;&#20987;&#20013;&#65292;&#23545;&#20110;&#20302;&#32423;&#25915;&#20987;&#27169;&#24335;&#21644;&#39640;&#32423;&#25915;&#20987;&#27169;&#24335;&#26469;&#35828;&#65292;&#23041;&#32961;&#34892;&#20026;&#32773;&#30340;&#39640;&#23618;&#25915;&#20987;&#27169;&#24335;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
A Machine Learning based Empirical Evaluation of Cyber Threat Actors High Level Attack Patterns over Low level Attack Patterns in Attributing Attacks. (arXiv:2307.10252v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10252
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#32593;&#32476;&#23041;&#32961;&#24402;&#22240;&#20013;&#65292;&#39640;&#32423;&#25915;&#20987;&#27169;&#24335;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#25928;&#26524;&#65292;&#30456;&#27604;&#20110;&#20302;&#32423;&#25915;&#20987;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#23041;&#32961;&#24402;&#22240;&#26159;&#22312;&#32593;&#32476;&#31354;&#38388;&#20013;&#35782;&#21035;&#25915;&#20987;&#20107;&#20214;&#30340;&#34892;&#20026;&#32773;&#30340;&#36807;&#31243;&#12290;&#20934;&#30830;&#21644;&#21450;&#26102;&#30340;&#23041;&#32961;&#24402;&#22240;&#22312;&#24212;&#29992;&#36866;&#24403;&#21644;&#21450;&#26102;&#30340;&#38450;&#24481;&#26426;&#21046;&#19978;&#36215;&#21040;&#37325;&#35201;&#20316;&#29992;&#12290;&#23545;&#20110;&#32593;&#32476;&#23433;&#20840;&#20998;&#26512;&#20154;&#21592;&#26469;&#35828;&#65292;&#36890;&#36807;&#34588;&#32592;&#37096;&#32626;&#12289;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#12289;&#38450;&#28779;&#22681;&#21644;&#28335;&#28304;&#31243;&#24207;&#25910;&#38598;&#25915;&#20987;&#27169;&#24335;&#30340;&#25163;&#21160;&#20998;&#26512;&#20173;&#28982;&#26159;&#39318;&#36873;&#30340;&#23041;&#32961;&#24402;&#22240;&#26041;&#27861;&#12290;&#36825;&#20123;&#25915;&#20987;&#27169;&#24335;&#26159;&#20302;&#32423;&#23041;&#32961;&#36857;&#35937;&#65288;IOC&#65289;&#65292;&#23427;&#20204;&#20195;&#34920;&#20102;&#23545;&#25163;&#22312;&#20182;&#20204;&#30340;&#25915;&#20987;&#20013;&#20351;&#29992;&#30340;&#31574;&#30053;&#12289;&#25216;&#26415;&#12289;&#31243;&#24207;&#65288;TTP&#65289;&#21644;&#36719;&#20214;&#24037;&#20855;&#12290;&#25915;&#20987;&#32773;&#24456;&#23569;&#37325;&#22797;&#20351;&#29992;&#23427;&#20204;&#65292;&#23427;&#20204;&#20063;&#21487;&#20197;&#34987;&#31713;&#25913;&#65292;&#23548;&#33268;&#38169;&#35823;&#21644;&#19981;&#20844;&#27491;&#30340;&#24402;&#22240;&#12290;&#20026;&#20102;&#23454;&#35777;&#35780;&#20272;&#21644;&#27604;&#36739;&#36825;&#20004;&#31181;IOC&#30340;&#26377;&#25928;&#24615;&#65292;&#26377;&#20004;&#20010;&#38382;&#39064;&#38656;&#35201;&#35299;&#20915;&#12290;&#31532;&#19968;&#20010;&#38382;&#39064;&#26159;&#65292;&#22312;&#26368;&#36817;&#30340;&#30740;&#31350;&#24037;&#20316;&#20013;&#65292;&#20302;&#32423;IOC&#23545;&#20110;&#32593;&#32476;&#23041;&#32961;&#24402;&#22240;&#30340;&#25928;&#26524;&#19981;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cyber threat attribution is the process of identifying the actor of an attack incident in cyberspace. An accurate and timely threat attribution plays an important role in deterring future attacks by applying appropriate and timely defense mechanisms. Manual analysis of attack patterns gathered by honeypot deployments, intrusion detection systems, firewalls, and via trace-back procedures is still the preferred method of security analysts for cyber threat attribution. Such attack patterns are low-level Indicators of Compromise (IOC). They represent Tactics, Techniques, Procedures (TTP), and software tools used by the adversaries in their campaigns. The adversaries rarely re-use them. They can also be manipulated, resulting in false and unfair attribution. To empirically evaluate and compare the effectiveness of both kinds of IOC, there are two problems that need to be addressed. The first problem is that in recent research works, the ineffectiveness of low-level IOC for cyber threat attr
&lt;/p&gt;</description></item><item><title>NaRuto&#26159;&#19968;&#20010;&#31995;&#32479;&#65292;&#21487;&#20197;&#20174;&#21465;&#20107;&#25991;&#26412;&#20013;&#33258;&#21160;&#25552;&#21462;&#32467;&#26500;&#21270;&#20107;&#20214;&#65292;&#24182;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#34892;&#21160;&#27169;&#22411;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#23436;&#20840;&#33258;&#21160;&#21270;&#26041;&#27861;&#21644;&#19982;&#21322;&#33258;&#21160;&#21270;&#26041;&#27861;&#30456;&#23218;&#32654;&#12290;</title><link>http://arxiv.org/abs/2307.10247</link><description>&lt;p&gt;
&#20174;&#21465;&#20107;&#25991;&#26412;&#20013;&#33258;&#21160;&#33719;&#21462;&#34892;&#21160;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Automated Action Model Acquisition from Narrative Texts. (arXiv:2307.10247v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10247
&lt;/p&gt;
&lt;p&gt;
NaRuto&#26159;&#19968;&#20010;&#31995;&#32479;&#65292;&#21487;&#20197;&#20174;&#21465;&#20107;&#25991;&#26412;&#20013;&#33258;&#21160;&#25552;&#21462;&#32467;&#26500;&#21270;&#20107;&#20214;&#65292;&#24182;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#34892;&#21160;&#27169;&#22411;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#23436;&#20840;&#33258;&#21160;&#21270;&#26041;&#27861;&#21644;&#19982;&#21322;&#33258;&#21160;&#21270;&#26041;&#27861;&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34892;&#21160;&#27169;&#22411;&#20197;&#21069;&#25552;/&#25928;&#26524;&#20844;&#29702;&#30340;&#24418;&#24335;&#23384;&#22312;&#65292;&#20026;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#25552;&#20379;&#34892;&#21160;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#32852;&#21644;&#21160;&#26426;&#36830;&#25509;&#12290;&#34892;&#21160;&#27169;&#22411;&#33719;&#21462;&#34987;&#35748;&#20026;&#26159;&#35745;&#21010;&#25216;&#26415;&#24212;&#29992;&#20013;&#30340;&#29942;&#39048;&#65292;&#29305;&#21035;&#26159;&#22312;&#21465;&#20107;&#35745;&#21010;&#20013;&#12290;&#20174;&#21465;&#20107;&#25991;&#26412;&#20013;&#20197;&#33258;&#21160;&#21270;&#30340;&#26041;&#24335;&#33719;&#21462;&#34892;&#21160;&#27169;&#22411;&#26159;&#24517;&#35201;&#30340;&#65292;&#20294;&#30001;&#20110;&#36825;&#26679;&#30340;&#25991;&#26412;&#26412;&#36136;&#19978;&#22797;&#26434;&#65292;&#22240;&#27492;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;NaRuto&#65292;&#19968;&#20010;&#31995;&#32479;&#65292;&#23427;&#21487;&#20197;&#20174;&#21465;&#20107;&#25991;&#26412;&#20013;&#25552;&#21462;&#32467;&#26500;&#21270;&#20107;&#20214;&#65292;&#24182;&#22522;&#20110;&#24120;&#35782;&#20107;&#20214;&#20851;&#31995;&#30340;&#39044;&#27979;&#20197;&#21450;&#25991;&#26412;&#19978;&#30340;&#30683;&#30462;&#21644;&#30456;&#20284;&#24615;&#26080;&#30417;&#30563;&#22320;&#29983;&#25104;&#35745;&#21010;&#35821;&#35328;&#39118;&#26684;&#30340;&#34892;&#21160;&#27169;&#22411;&#12290;&#32463;&#20856;&#30340;&#21465;&#20107;&#35745;&#21010;&#39046;&#22495;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;NaRuto&#21487;&#20197;&#29983;&#25104;&#36136;&#37327;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#23436;&#20840;&#33258;&#21160;&#21270;&#26041;&#27861;&#30340;&#34892;&#21160;&#27169;&#22411;&#65292;&#29978;&#33267;&#19982;&#21322;&#33258;&#21160;&#21270;&#26041;&#27861;&#30340;&#34892;&#21160;&#27169;&#22411;&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;
Action models, which take the form of precondition/effect axioms, facilitate causal and motivational connections between actions for AI agents. Action model acquisition has been identified as a bottleneck in the application of planning technology, especially within narrative planning. Acquiring action models from narrative texts in an automated way is essential, but challenging because of the inherent complexities of such texts. We present NaRuto, a system that extracts structured events from narrative text and subsequently generates planning-language-style action models based on predictions of commonsense event relations, as well as textual contradictions and similarities, in an unsupervised manner. Experimental results in classical narrative planning domains show that NaRuto can generate action models of significantly better quality than existing fully automated methods, and even on par with those of semi-automated methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#33041;&#23545;&#40784;&#30340;&#30740;&#31350;&#65292;&#37325;&#28857;&#22312;&#20110;&#33041;&#32534;&#30721;&#21644;&#35299;&#30721;&#27169;&#22411;&#30340;&#24212;&#29992;&#12290;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#29702;&#35299;&#22823;&#33041;&#30340;&#20449;&#24687;&#22788;&#29702;&#26426;&#21046;&#20197;&#21450;&#35774;&#35745;&#33041;&#26426;&#25509;&#21475;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2307.10246</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#33041;&#23545;&#40784;&#65306;&#33041;&#32534;&#30721;&#21644;&#35299;&#30721;&#65288;&#32508;&#36848;&#65289;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks and Brain Alignment: Brain Encoding and Decoding (Survey). (arXiv:2307.10246v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10246
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#33041;&#23545;&#40784;&#30340;&#30740;&#31350;&#65292;&#37325;&#28857;&#22312;&#20110;&#33041;&#32534;&#30721;&#21644;&#35299;&#30721;&#27169;&#22411;&#30340;&#24212;&#29992;&#12290;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#29702;&#35299;&#22823;&#33041;&#30340;&#20449;&#24687;&#22788;&#29702;&#26426;&#21046;&#20197;&#21450;&#35774;&#35745;&#33041;&#26426;&#25509;&#21475;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#33041;&#22914;&#20309;&#34920;&#31034;&#19981;&#21516;&#30340;&#20449;&#24687;&#27169;&#24335;&#65311;&#25105;&#20204;&#33021;&#21542;&#35774;&#35745;&#20986;&#19968;&#20010;&#21487;&#20197;&#33258;&#21160;&#29702;&#35299;&#29992;&#25143;&#24605;&#32771;&#20869;&#23481;&#30340;&#31995;&#32479;&#65311;&#36825;&#20123;&#38382;&#39064;&#21487;&#20197;&#36890;&#36807;&#30740;&#31350;&#21151;&#33021;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;fMRI&#65289;&#31561;&#22823;&#33041;&#35760;&#24405;&#26469;&#22238;&#31572;&#12290;&#20316;&#20026;&#31532;&#19968;&#27493;&#65292;&#31070;&#32463;&#31185;&#23398;&#30028;&#20026;&#34987;&#21160;&#38405;&#35835;/&#21548;&#35273;/&#35266;&#30475;&#27010;&#24565;&#35789;&#27719;&#12289;&#21465;&#36848;&#12289;&#22270;&#29255;&#21644;&#30005;&#24433;&#30456;&#20851;&#30340;&#35748;&#30693;&#31070;&#32463;&#31185;&#23398;&#25968;&#25454;&#38598;&#20316;&#20986;&#20102;&#36129;&#29486;&#12290;&#36807;&#21435;&#20108;&#21313;&#24180;&#20013;&#65292;&#36824;&#25552;&#20986;&#20102;&#20351;&#29992;&#36825;&#20123;&#25968;&#25454;&#38598;&#30340;&#32534;&#30721;&#21644;&#35299;&#30721;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#20316;&#20026;&#22522;&#30784;&#30740;&#31350;&#20013;&#30340;&#39069;&#22806;&#24037;&#20855;&#65292;&#22312;&#35748;&#30693;&#31185;&#23398;&#21644;&#31070;&#32463;&#31185;&#23398;&#39046;&#22495;&#26377;&#30528;&#22810;&#31181;&#23454;&#38469;&#24212;&#29992;&#12290;&#32534;&#30721;&#27169;&#22411;&#26088;&#22312;&#33258;&#21160;&#22320;&#29983;&#25104;fMRI&#22823;&#33041;&#34920;&#24449;&#65292;&#32473;&#23450;&#19968;&#20010;&#21050;&#28608;&#12290;&#23427;&#20204;&#22312;&#35780;&#20272;&#21644;&#35786;&#26029;&#31070;&#32463;&#31995;&#32479;&#30142;&#30149;&#20197;&#21450;&#35774;&#35745;&#22823;&#33041;&#25439;&#20260;&#27835;&#30103;&#26041;&#27861;&#26041;&#38754;&#26377;&#30528;&#22810;&#31181;&#23454;&#38469;&#24212;&#29992;&#12290;&#35299;&#30721;&#27169;&#22411;&#35299;&#20915;&#20102;&#26681;&#25454;fMRI&#37325;&#26500;&#21050;&#28608;&#30340;&#36870;&#38382;&#39064;&#12290;&#23427;&#20204;&#23545;&#20110;&#29702;&#35299;&#22823;&#33041;&#22914;&#20309;&#22788;&#29702;&#20449;&#24687;&#20197;&#21450;&#35774;&#35745;&#33041;&#26426;&#25509;&#21475;&#30340;&#21457;&#23637;&#37117;&#26377;&#30528;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
How does the brain represent different modes of information? Can we design a system that automatically understands what the user is thinking? Such questions can be answered by studying brain recordings like functional magnetic resonance imaging (fMRI). As a first step, the neuroscience community has contributed several large cognitive neuroscience datasets related to passive reading/listening/viewing of concept words, narratives, pictures and movies. Encoding and decoding models using these datasets have also been proposed in the past two decades. These models serve as additional tools for basic research in cognitive science and neuroscience. Encoding models aim at generating fMRI brain representations given a stimulus automatically. They have several practical applications in evaluating and diagnosing neurological conditions and thus also help design therapies for brain damage. Decoding models solve the inverse problem of reconstructing the stimuli given the fMRI. They are useful for 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#28145;&#24230;&#25512;&#33616;&#31995;&#32479;&#22312;&#38754;&#23545;&#30828;&#20214;&#38169;&#35823;&#26102;&#30340;&#20581;&#22766;&#24615;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#38169;&#35823;&#27880;&#20837;&#26694;&#26550;Terrorch&#65292;&#21457;&#29616;&#28608;&#27963;&#21098;&#35009;&#26159;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#38169;&#35823;&#32531;&#35299;&#26041;&#27861;&#65292;&#21487;&#20197;&#24674;&#22797;&#39640;&#36798;30%&#30340;&#34987;&#38477;&#20302;&#30340;AUC-ROC&#24471;&#20998;&#12290;</title><link>http://arxiv.org/abs/2307.10244</link><description>&lt;p&gt;
&#23545;&#28145;&#24230;&#25512;&#33616;&#31995;&#32479;&#25239;&#30828;&#20214;&#38169;&#35823;&#33021;&#21147;&#30340;&#35780;&#20272;&#21644;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Evaluating and Enhancing Robustness of Deep Recommendation Systems Against Hardware Errors. (arXiv:2307.10244v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10244
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#28145;&#24230;&#25512;&#33616;&#31995;&#32479;&#22312;&#38754;&#23545;&#30828;&#20214;&#38169;&#35823;&#26102;&#30340;&#20581;&#22766;&#24615;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#38169;&#35823;&#27880;&#20837;&#26694;&#26550;Terrorch&#65292;&#21457;&#29616;&#28608;&#27963;&#21098;&#35009;&#26159;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#38169;&#35823;&#32531;&#35299;&#26041;&#27861;&#65292;&#21487;&#20197;&#24674;&#22797;&#39640;&#36798;30%&#30340;&#34987;&#38477;&#20302;&#30340;AUC-ROC&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#25512;&#33616;&#31995;&#32479;&#65288;DRS&#65289;&#20005;&#37325;&#20381;&#36182;&#20110;&#19987;&#29992;&#30340;&#39640;&#24615;&#33021;&#35745;&#31639;&#30828;&#20214;&#21644;&#21152;&#36895;&#22120;&#65292;&#20197;&#20248;&#21270;&#33021;&#28304;&#12289;&#25928;&#29575;&#21644;&#25512;&#33616;&#36136;&#37327;&#12290;&#23613;&#31649;&#30446;&#21069;&#22312;&#37096;&#32626;DRS&#30340;&#22823;&#35268;&#27169;&#31995;&#32479;&#20013;&#35266;&#23519;&#21040;&#30828;&#20214;&#38169;&#35823;&#30340;&#22686;&#21152;&#65292;&#20294;DRS&#30340;&#20581;&#22766;&#24615;&#19968;&#30452;&#26410;&#21463;&#21040;&#37325;&#35270;&#12290;&#26412;&#25991;&#39318;&#27425;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;DRS&#22312;&#38754;&#23545;&#30828;&#20214;&#38169;&#35823;&#26102;&#30340;&#20581;&#22766;&#24615;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;Terrorch&#30340;&#29992;&#25143;&#21451;&#22909;&#12289;&#39640;&#25928;&#28789;&#27963;&#30340;&#38169;&#35823;&#27880;&#20837;&#26694;&#26550;&#65292;&#22522;&#20110;&#24191;&#27867;&#20351;&#29992;&#30340;PyTorch&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#24191;&#27867;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#65292;&#35266;&#23519;&#21040;DRS&#30340;&#20581;&#22766;&#24615;&#21463;&#21040;&#22810;&#31181;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#21253;&#25324;&#27169;&#22411;&#21442;&#25968;&#21644;&#36755;&#20837;&#29305;&#24449;&#12290;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;3&#31181;&#38169;&#35823;&#32531;&#35299;&#26041;&#27861;&#65292;&#21253;&#25324;&#22522;&#20110;&#31639;&#27861;&#30340;&#23481;&#38169;&#65288;ABFT&#65289;&#12289;&#28608;&#27963;&#21098;&#35009;&#21644;&#36873;&#25321;&#24615;&#20301;&#20445;&#25252;&#65288;SBP&#65289;&#12290;&#25105;&#20204;&#21457;&#29616;&#24212;&#29992;&#28608;&#27963;&#21098;&#35009;&#21487;&#20197;&#24674;&#22797;&#34987;&#38477;&#20302;&#30340;AUC-ROC&#24471;&#20998;&#39640;&#36798;30%&#65292;&#36825;&#20351;&#20854;&#25104;&#20026;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#32531;&#35299;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep recommendation systems (DRS) heavily depend on specialized HPC hardware and accelerators to optimize energy, efficiency, and recommendation quality. Despite the growing number of hardware errors observed in large-scale fleet systems where DRS are deployed, the robustness of DRS has been largely overlooked. This paper presents the first systematic study of DRS robustness against hardware errors. We develop Terrorch, a user-friendly, efficient and flexible error injection framework on top of the widely-used PyTorch. We evaluate a wide range of models and datasets and observe that the DRS robustness against hardware errors is influenced by various factors from model parameters to input characteristics. We also explore 3 error mitigation methods including algorithm based fault tolerance (ABFT), activation clipping and selective bit protection (SBP). We find that applying activation clipping can recover up to 30% of the degraded AUC-ROC score, making it a promising mitigation method.
&lt;/p&gt;</description></item><item><title>CoNAN&#26159;&#19968;&#31181;&#29992;&#20110;&#26080;&#32422;&#26463;&#20154;&#33080;&#29305;&#24449;&#34701;&#21512;&#30340;&#26465;&#20214;&#31070;&#32463;&#32858;&#21512;&#32593;&#32476;&#65292;&#38024;&#23545;&#22312;&#38271;&#36317;&#31163;&#21644;&#39640;&#39640;&#24230;&#29615;&#22659;&#19979;&#25429;&#33719;&#30340;&#26497;&#20302;&#20998;&#36776;&#29575;&#20154;&#33080;&#65292;&#21033;&#29992;&#29305;&#24449;&#20998;&#24067;&#35843;&#33410;&#26041;&#27861;&#26469;&#36827;&#34892;&#27169;&#26495;&#32858;&#21512;&#12290;</title><link>http://arxiv.org/abs/2307.10237</link><description>&lt;p&gt;
CoNAN&#65306;&#26080;&#32422;&#26463;&#20154;&#33080;&#29305;&#24449;&#34701;&#21512;&#30340;&#26465;&#20214;&#31070;&#32463;&#32858;&#21512;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
CoNAN: Conditional Neural Aggregation Network For Unconstrained Face Feature Fusion. (arXiv:2307.10237v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10237
&lt;/p&gt;
&lt;p&gt;
CoNAN&#26159;&#19968;&#31181;&#29992;&#20110;&#26080;&#32422;&#26463;&#20154;&#33080;&#29305;&#24449;&#34701;&#21512;&#30340;&#26465;&#20214;&#31070;&#32463;&#32858;&#21512;&#32593;&#32476;&#65292;&#38024;&#23545;&#22312;&#38271;&#36317;&#31163;&#21644;&#39640;&#39640;&#24230;&#29615;&#22659;&#19979;&#25429;&#33719;&#30340;&#26497;&#20302;&#20998;&#36776;&#29575;&#20154;&#33080;&#65292;&#21033;&#29992;&#29305;&#24449;&#20998;&#24067;&#35843;&#33410;&#26041;&#27861;&#26469;&#36827;&#34892;&#27169;&#26495;&#32858;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#21463;&#38480;&#21046;&#21644;&#26080;&#25511;&#21046;&#30340;&#29615;&#22659;&#19979;&#65292;&#22914;&#38271;&#36317;&#31163;&#12289;&#20302;&#20998;&#36776;&#29575;&#12289;&#19981;&#21516;&#35270;&#35282;&#12289;&#20809;&#29031;&#12289;&#23039;&#24577;&#21644;&#22823;&#27668;&#26465;&#20214;&#19979;&#65292;&#20174;&#22270;&#20687;&#38598;&#20013;&#36827;&#34892;&#20154;&#33080;&#35782;&#21035;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20154;&#33080;&#29305;&#24449;&#32858;&#21512;&#22312;&#36825;&#31867;&#35782;&#21035;&#31995;&#32479;&#20013;&#36215;&#21040;&#20851;&#38190;&#20316;&#29992;&#65292;&#23427;&#28041;&#21450;&#23558;&#27169;&#26495;&#20013;&#30340;N&#20010;&#29305;&#24449;&#34920;&#31034;&#32858;&#21512;&#25104;&#19968;&#20010;&#20840;&#23616;&#34920;&#31034;&#12290;&#29616;&#26377;&#30340;&#20256;&#32479;&#20154;&#33080;&#29305;&#24449;&#32858;&#21512;&#26041;&#27861;&#35201;&#20040;&#21033;&#29992;&#20803;&#25968;&#25454;&#65292;&#35201;&#20040;&#21033;&#29992;&#39640;&#32500;&#20013;&#38388;&#29305;&#24449;&#34920;&#31034;&#26469;&#20272;&#35745;&#29305;&#24449;&#36136;&#37327;&#36827;&#34892;&#32858;&#21512;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22312;&#36828;&#36317;&#31163;&#21644;&#39640;&#39640;&#24230;&#29615;&#22659;&#19979;&#25429;&#33719;&#30340;&#26497;&#20302;&#20998;&#36776;&#29575;&#20154;&#33080;&#65292;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20803;&#25968;&#25454;&#25110;&#39118;&#26684;&#20449;&#24687;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;CoNAN&#30340;&#29305;&#24449;&#20998;&#24067;&#35843;&#33410;&#26041;&#27861;&#26469;&#36827;&#34892;&#27169;&#26495;&#32858;&#21512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#23398;&#20064;&#19968;&#20010;&#22312;&#36755;&#20837;&#29305;&#24449;&#20998;&#24067;&#20449;&#24687;&#26465;&#20214;&#19979;&#30340;&#19978;&#19979;&#25991;&#21521;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Face recognition from image sets acquired under unregulated and uncontrolled settings, such as at large distances, low resolutions, varying viewpoints, illumination, pose, and atmospheric conditions, is challenging. Face feature aggregation, which involves aggregating a set of N feature representations present in a template into a single global representation, plays a pivotal role in such recognition systems. Existing works in traditional face feature aggregation either utilize metadata or high-dimensional intermediate feature representations to estimate feature quality for aggregation. However, generating high-quality metadata or style information is not feasible for extremely low-resolution faces captured in long-range and high altitude settings. To overcome these limitations, we propose a feature distribution conditioning approach called CoNAN for template aggregation. Specifically, our method aims to learn a context vector conditioned over the distribution information of the incomi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;GPT&#36827;&#34892;&#39640;&#32423;&#24773;&#24863;&#20998;&#26512;&#65292;&#24182;&#32771;&#23519;&#20854;&#19982;&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#24046;&#24322;&#65292;&#21457;&#29616;GPT&#26041;&#27861;&#30456;&#36739;&#20110;&#20854;&#20182;&#27169;&#22411;&#22312;&#39044;&#27979;&#24615;&#33021;&#19978;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#65292;&#24182;&#26377;&#25928;&#35299;&#20915;&#20102;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#19968;&#20123;&#25361;&#25112;&#65292;&#22914;&#29702;&#35299;&#19978;&#19979;&#25991;&#21644;&#26816;&#27979;&#35773;&#21050;&#12290;</title><link>http://arxiv.org/abs/2307.10234</link><description>&lt;p&gt;
SentimentGPT&#65306;&#21033;&#29992;GPT&#36827;&#34892;&#39640;&#32423;&#24773;&#24863;&#20998;&#26512;&#21450;&#20854;&#19982;&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
SentimentGPT: Exploiting GPT for Advanced Sentiment Analysis and its Departure from Current Machine Learning. (arXiv:2307.10234v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10234
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;GPT&#36827;&#34892;&#39640;&#32423;&#24773;&#24863;&#20998;&#26512;&#65292;&#24182;&#32771;&#23519;&#20854;&#19982;&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#24046;&#24322;&#65292;&#21457;&#29616;GPT&#26041;&#27861;&#30456;&#36739;&#20110;&#20854;&#20182;&#27169;&#22411;&#22312;&#39044;&#27979;&#24615;&#33021;&#19978;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#65292;&#24182;&#26377;&#25928;&#35299;&#20915;&#20102;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#19968;&#20123;&#25361;&#25112;&#65292;&#22914;&#29702;&#35299;&#19978;&#19979;&#25991;&#21644;&#26816;&#27979;&#35773;&#21050;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#24773;&#24863;&#20998;&#26512;&#20013;&#21508;&#31181;&#29983;&#25104;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#65288;GPT&#65289;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#32771;&#23519;&#65292;&#29305;&#21035;&#26159;&#22312;SemEval 2017&#25968;&#25454;&#38598;&#30340;&#20219;&#21153;4&#20013;&#12290;&#37319;&#29992;&#20102;&#19977;&#31181;&#20027;&#35201;&#31574;&#30053;&#65306;1&#65289;&#20351;&#29992;GPT-3.5 Turbo&#36827;&#34892;&#25552;&#31034;&#24037;&#31243;&#65292;2&#65289;&#23545;GPT&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;3&#65289;&#37319;&#29992;&#21019;&#26032;&#30340;&#23884;&#20837;&#20998;&#31867;&#26041;&#27861;&#12290;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#36825;&#20123;&#31574;&#30053;&#21644;&#20010;&#21035;GPT&#27169;&#22411;&#20043;&#38388;&#30340;&#35814;&#32454;&#27604;&#36739;&#35265;&#35299;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#29420;&#29305;&#30340;&#20248;&#21183;&#21644;&#28508;&#22312;&#30340;&#23616;&#38480;&#24615;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#23558;&#36825;&#20123;&#22522;&#20110;GPT&#30340;&#26041;&#27861;&#19982;&#20854;&#20182;&#21516;&#26102;&#20195;&#12289;&#39640;&#24615;&#33021;&#30340;&#27169;&#22411;&#22312;&#30456;&#21516;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;GPT&#26041;&#27861;&#22312;&#39044;&#27979;&#24615;&#33021;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#30340;&#20248;&#21183;&#65292;&#30456;&#36739;&#20110;&#26368;&#20808;&#36827;&#25216;&#26415;&#65292;F1&#20998;&#25968;&#22686;&#21152;&#20102;22%&#20197;&#19978;&#12290;&#27492;&#22806;&#65292;&#26412;&#35770;&#25991;&#36824;&#25506;&#35752;&#20102;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#24120;&#35265;&#25361;&#25112;&#65292;&#22914;&#29702;&#35299;&#19978;&#19979;&#25991;&#21644;&#26816;&#27979;&#35773;&#21050;&#12290;&#30740;&#31350;&#24378;&#35843;&#20102;GPT&#26041;&#27861;&#30340;&#37325;&#35201;&#20215;&#20540;&#21644;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study presents a thorough examination of various Generative Pretrained Transformer (GPT) methodologies in sentiment analysis, specifically in the context of Task 4 on the SemEval 2017 dataset. Three primary strategies are employed: 1) prompt engineering using the advanced GPT-3.5 Turbo, 2) fine-tuning GPT models, and 3) an inventive approach to embedding classification. The research yields detailed comparative insights among these strategies and individual GPT models, revealing their unique strengths and potential limitations. Additionally, the study compares these GPT-based methodologies with other contemporary, high-performing models previously used with the same dataset. The results illustrate the significant superiority of the GPT approaches in terms of predictive performance, more than 22% in F1-score compared to the state-of-the-art. Further, the paper addresses common challenges in sentiment analysis tasks, such as understanding context and detecting sarcasm. It underscores
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#20174;&#22269;&#23478;&#32508;&#21512;&#30284;&#30151;&#32593;&#32476;&#65288;NCCN&#65289;&#32959;&#30244;&#23398;&#20020;&#24202;&#25351;&#21335;&#20013;&#25552;&#21462;&#30693;&#35782;&#65292;&#24182;&#29983;&#25104;&#21253;&#21547;&#35813;&#30693;&#35782;&#30340;&#32467;&#26500;&#21270;&#27169;&#22411;&#12290;&#36890;&#36807;&#20351;&#29992;&#30284;&#30151;&#20998;&#26399;&#20449;&#24687;&#12289;&#32479;&#19968;&#21307;&#23398;&#35821;&#35328;&#31995;&#32479;&#65288;UMLS&#65289;&#21644;&#22269;&#23478;&#30284;&#30151;&#30740;&#31350;&#25152;&#30340;&#35789;&#24211;&#65288;NCIt&#65289;&#27010;&#24565;&#20197;&#21450;&#33410;&#28857;&#20998;&#31867;&#30340;&#22686;&#24378;&#31574;&#30053;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#23454;&#29616;&#31243;&#24207;&#21270;&#36941;&#21382;&#21644;&#26597;&#35810;&#12290;</title><link>http://arxiv.org/abs/2307.10231</link><description>&lt;p&gt;
&#30284;&#30151;&#20020;&#24202;&#23454;&#36341;&#25351;&#21335;&#30340;&#33258;&#21160;&#21270;&#30693;&#35782;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Automated Knowledge Modeling for Cancer Clinical Practice Guidelines. (arXiv:2307.10231v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10231
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#20174;&#22269;&#23478;&#32508;&#21512;&#30284;&#30151;&#32593;&#32476;&#65288;NCCN&#65289;&#32959;&#30244;&#23398;&#20020;&#24202;&#25351;&#21335;&#20013;&#25552;&#21462;&#30693;&#35782;&#65292;&#24182;&#29983;&#25104;&#21253;&#21547;&#35813;&#30693;&#35782;&#30340;&#32467;&#26500;&#21270;&#27169;&#22411;&#12290;&#36890;&#36807;&#20351;&#29992;&#30284;&#30151;&#20998;&#26399;&#20449;&#24687;&#12289;&#32479;&#19968;&#21307;&#23398;&#35821;&#35328;&#31995;&#32479;&#65288;UMLS&#65289;&#21644;&#22269;&#23478;&#30284;&#30151;&#30740;&#31350;&#25152;&#30340;&#35789;&#24211;&#65288;NCIt&#65289;&#27010;&#24565;&#20197;&#21450;&#33410;&#28857;&#20998;&#31867;&#30340;&#22686;&#24378;&#31574;&#30053;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#23454;&#29616;&#31243;&#24207;&#21270;&#36941;&#21382;&#21644;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#31215;&#26497;&#30740;&#31350;&#20135;&#29983;&#30340;&#26032;&#35777;&#25454;&#65292;&#30284;&#30151;&#30142;&#30149;&#30340;&#20020;&#24202;&#23454;&#36341;&#25351;&#21335;&#65288;CPGs&#65289;&#21457;&#23637;&#36805;&#36895;&#12290;&#30446;&#21069;&#65292;CPGs&#20027;&#35201;&#20197;&#19981;&#36866;&#21512;&#31649;&#29702;&#36825;&#31181;&#21457;&#23637;&#30693;&#35782;&#30340;&#25991;&#26723;&#26684;&#24335;&#21457;&#24067;&#12290;&#38656;&#35201;&#19968;&#31181;&#36866;&#29992;&#20110;&#31243;&#24207;&#20132;&#20114;&#30340;&#25351;&#21335;&#25991;&#26723;&#30340;&#30693;&#35782;&#27169;&#22411;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22269;&#23478;&#32508;&#21512;&#30284;&#30151;&#32593;&#32476;&#65288;NCCN&#65289;&#32959;&#30244;&#23398;CPGs&#20013;&#25552;&#21462;&#30693;&#35782;&#24182;&#29983;&#25104;&#21253;&#21547;&#25552;&#21462;&#30340;&#30693;&#35782;&#30340;&#32467;&#26500;&#21270;&#27169;&#22411;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#12290;&#20351;&#29992;&#20004;&#20010;&#29256;&#26412;&#30340;NCCN&#38750;&#23567;&#32454;&#32990;&#32954;&#30284;&#65288;NSCLC&#65289;CPG&#23545;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#20197;&#23637;&#31034;&#20854;&#24544;&#23454;&#25552;&#21462;&#21644;&#24314;&#27169;&#30693;&#35782;&#30340;&#25928;&#26524;&#12290;&#36824;&#25552;&#20986;&#20102;&#19977;&#31181;&#22686;&#24378;&#27169;&#22411;&#30340;&#31574;&#30053;&#65292;&#21253;&#25324;&#20351;&#29992;&#30284;&#30151;&#20998;&#26399;&#20449;&#24687;&#12289;&#32479;&#19968;&#21307;&#23398;&#35821;&#35328;&#31995;&#32479;&#65288;UMLS&#65289;&#30340;&#20803;&#35789;&#24211;&#21644;&#22269;&#23478;&#30284;&#30151;&#30740;&#31350;&#25152;&#30340;&#35789;&#24211;&#65288;NCIt&#65289;&#27010;&#24565;&#20197;&#21450;&#33410;&#28857;&#20998;&#31867;&#65292;&#20197;&#23454;&#29616;&#31243;&#24207;&#21270;&#36941;&#21382;&#21644;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clinical Practice Guidelines (CPGs) for cancer diseases evolve rapidly due to new evidence generated by active research. Currently, CPGs are primarily published in a document format that is ill-suited for managing this developing knowledge. A knowledge model of the guidelines document suitable for programmatic interaction is required. This work proposes an automated method for extraction of knowledge from National Comprehensive Cancer Network (NCCN) CPGs in Oncology and generating a structured model containing the retrieved knowledge. The proposed method was tested using two versions of NCCN Non-Small Cell Lung Cancer (NSCLC) CPG to demonstrate the effectiveness in faithful extraction and modeling of knowledge. Three enrichment strategies using Cancer staging information, Unified Medical Language System (UMLS) Metathesaurus &amp; National Cancer Institute thesaurus (NCIt) concepts, and Node classification are also presented to enhance the model towards enabling programmatic traversal and q
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#22635;&#34917;&#20102;&#26102;&#38388;KG&#21644;&#36229;&#20851;&#31995;KG&#25512;&#29702;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#24182;&#24320;&#21457;&#20102;&#20004;&#20010;&#26032;&#30340;&#22522;&#20934;&#36229;&#20851;&#31995;TKG&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2307.10219</link><description>&lt;p&gt;
&#22312;&#22686;&#24378;&#30340;&#19981;&#21464;&#20851;&#31995;&#30693;&#35782;&#19978;&#25506;&#32034;&#36229;&#20851;&#31995;&#26102;&#38388;&#30693;&#35782;&#22270;&#30340;&#38142;&#25509;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Exploring Link Prediction over Hyper-Relational Temporal Knowledge Graphs Enhanced with Time-Invariant Relational Knowledge. (arXiv:2307.10219v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10219
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#22635;&#34917;&#20102;&#26102;&#38388;KG&#21644;&#36229;&#20851;&#31995;KG&#25512;&#29702;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#24182;&#24320;&#21457;&#20102;&#20004;&#20010;&#26032;&#30340;&#22522;&#20934;&#36229;&#20851;&#31995;TKG&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#20851;&#31995;&#30693;&#35782;&#22270;(HKGs)&#26159;&#20256;&#32479;&#30693;&#35782;&#22270;(KGs)&#30340;&#24310;&#20280;&#65292;&#20026;&#27599;&#20010;KG&#20107;&#23454;&#25552;&#20379;&#39069;&#22806;&#30340;&#38190;&#20540;&#23545;(&#21363;&#38480;&#23450;&#35789;)&#65292;&#20197;&#26356;&#22909;&#22320;&#38480;&#21046;&#20107;&#23454;&#30340;&#26377;&#25928;&#24615;&#12290;&#36817;&#24180;&#26469;&#65292;&#30740;&#31350;&#22312;HKGs&#19978;&#36827;&#34892;&#22270;&#25512;&#29702;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#30001;&#20110;&#19990;&#30028;&#30693;&#35782;&#30340;&#19981;&#26029;&#28436;&#21464;&#65292;&#22823;&#37327;&#24179;&#34892;&#24037;&#20316;&#38598;&#20013;&#22312;&#23545;&#26102;&#38388;KGs(TKGs)&#36827;&#34892;&#25512;&#29702;&#65292;&#20854;&#20013;&#27599;&#20010;TKG&#20107;&#23454;&#21487;&#20197;&#34987;&#35270;&#20026;&#24102;&#26377;&#26102;&#38388;&#25139;(&#25110;&#26102;&#38388;&#27573;)&#30340;KG&#20107;&#23454;&#65292;&#25351;&#23450;&#20854;&#26102;&#38388;&#26377;&#25928;&#24615;&#12290;&#29616;&#26377;&#30340;HKG&#25512;&#29702;&#26041;&#27861;&#19981;&#32771;&#34385;&#26102;&#38388;&#20449;&#24687;&#65292;&#22240;&#20026;&#22312;&#20043;&#21069;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#27809;&#26377;&#26174;&#24335;&#22320;&#25351;&#23450;&#12290;&#27492;&#22806;&#65292;&#25152;&#26377;&#20197;&#21069;&#30340;TKG&#25512;&#29702;&#26041;&#27861;&#21482;&#37325;&#35270;&#26102;&#38388;&#25512;&#29702;&#65292;&#24182;&#27809;&#26377;&#21150;&#27861;&#20174;&#38480;&#23450;&#35789;&#20013;&#23398;&#20064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22635;&#34917;TKG&#25512;&#29702;&#21644;HKG&#25512;&#29702;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#20004;&#20010;&#26032;&#30340;&#22522;&#20934;&#36229;&#20851;&#31995;TKG(HTKG)&#25968;&#25454;&#38598;&#65292;&#21363;Wiki-hy&#21644;...
&lt;/p&gt;
&lt;p&gt;
Stemming from traditional knowledge graphs (KGs), hyper-relational KGs (HKGs) provide additional key-value pairs (i.e., qualifiers) for each KG fact that help to better restrict the fact validity. In recent years, there has been an increasing interest in studying graph reasoning over HKGs. In the meantime, due to the ever-evolving nature of world knowledge, extensive parallel works have been focusing on reasoning over temporal KGs (TKGs), where each TKG fact can be viewed as a KG fact coupled with a timestamp (or time period) specifying its time validity. The existing HKG reasoning approaches do not consider temporal information because it is not explicitly specified in previous benchmark datasets. Besides, all the previous TKG reasoning methods only lay emphasis on temporal reasoning and have no way to learn from qualifiers. To this end, we aim to fill the gap between TKG reasoning and HKG reasoning. We develop two new benchmark hyper-relational TKG (HTKG) datasets, i.e., Wiki-hy and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#22823;&#22411;&#24320;&#25918;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#21517;&#20026;aCTIon&#30340;&#32467;&#26500;&#21270;CTI&#20449;&#24687;&#25552;&#21462;&#24037;&#20855;&#26469;&#35299;&#20915;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#28304;&#20013;&#25552;&#21462;&#30456;&#20851;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#21253;&#25324;204&#20010;&#30495;&#23454;&#19990;&#30028;&#19978;&#21487;&#29992;&#30340;&#25253;&#21578;&#21644;&#30456;&#24212;&#30340;&#32467;&#26500;&#21270;CTI&#20449;&#24687;&#12290;&#36890;&#36807;&#19982;&#19977;&#20010;&#29420;&#31435;&#30340;CTI&#20998;&#26512;&#32452;&#21512;&#20316;&#65292;&#25105;&#20204;&#30340;&#22242;&#38431;&#31574;&#21010;&#20102;&#36825;&#20010;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#20043;&#21069;&#20844;&#24320;&#21457;&#24067;&#30340;&#24320;&#28304;&#25968;&#25454;&#38598;&#30340;&#20004;&#20010;&#25968;&#37327;&#32423;&#22823;&#12290;</title><link>http://arxiv.org/abs/2307.10214</link><description>&lt;p&gt;
&#35813;&#25991;&#26631;&#39064;&#24050;&#32763;&#35793;&#20026;&#65306;&#34892;&#21160;&#26102;&#38388;&#65306;&#38024;&#23545;&#37326;&#22806;&#32593;&#32476;&#23041;&#32961;&#24773;&#25253;&#30340;&#33258;&#21160;&#21270;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Time for aCTIon: Automated Analysis of Cyber Threat Intelligence in the Wild. (arXiv:2307.10214v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10214
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#22823;&#22411;&#24320;&#25918;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#21517;&#20026;aCTIon&#30340;&#32467;&#26500;&#21270;CTI&#20449;&#24687;&#25552;&#21462;&#24037;&#20855;&#26469;&#35299;&#20915;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#28304;&#20013;&#25552;&#21462;&#30456;&#20851;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#21253;&#25324;204&#20010;&#30495;&#23454;&#19990;&#30028;&#19978;&#21487;&#29992;&#30340;&#25253;&#21578;&#21644;&#30456;&#24212;&#30340;&#32467;&#26500;&#21270;CTI&#20449;&#24687;&#12290;&#36890;&#36807;&#19982;&#19977;&#20010;&#29420;&#31435;&#30340;CTI&#20998;&#26512;&#32452;&#21512;&#20316;&#65292;&#25105;&#20204;&#30340;&#22242;&#38431;&#31574;&#21010;&#20102;&#36825;&#20010;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#20043;&#21069;&#20844;&#24320;&#21457;&#24067;&#30340;&#24320;&#28304;&#25968;&#25454;&#38598;&#30340;&#20004;&#20010;&#25968;&#37327;&#32423;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#32452;&#32455;&#26426;&#26500;&#65292;&#32593;&#32476;&#23041;&#32961;&#24773;&#25253;(CTI)&#22312;&#35780;&#20272;&#39118;&#38505;&#21644;&#25552;&#21319;&#23433;&#20840;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#28304;&#20013;&#25552;&#21462;&#30456;&#20851;&#20449;&#24687;&#30340;&#36807;&#31243;&#21487;&#33021;&#26082;&#26114;&#36149;&#21448;&#32791;&#26102;&#12290;&#25105;&#20204;&#30340;&#32463;&#39564;&#34920;&#26126;&#65292;&#29616;&#26377;&#30340;&#33258;&#21160;&#21270;&#32467;&#26500;&#21270;CTI&#25552;&#21462;&#24037;&#20855;&#23384;&#22312;&#24615;&#33021;&#38480;&#21046;&#12290;&#27492;&#22806;&#65292;&#32570;&#20047;&#19968;&#20010;&#20849;&#21516;&#30340;&#22522;&#20934;&#26469;&#23450;&#37327;&#35780;&#20272;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#26032;&#30340;&#22823;&#22411;&#24320;&#25918;&#24615;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#21517;&#20026;aCTIon&#30340;&#32467;&#26500;&#21270;CTI&#20449;&#24687;&#25552;&#21462;&#24037;&#20855;&#26469;&#22635;&#34917;&#36825;&#20123;&#31354;&#30333;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;204&#20010;&#30495;&#23454;&#19990;&#30028;&#19978;&#21487;&#29992;&#30340;&#25253;&#21578;&#21644;&#30456;&#24212;&#30340;STIX&#26684;&#24335;&#30340;&#32467;&#26500;&#21270;CTI&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#22242;&#38431;&#22312;&#20960;&#20010;&#26376;&#30340;&#26102;&#38388;&#20869;&#65292;&#19982;&#19977;&#20010;&#29420;&#31435;&#30340;CTI&#20998;&#26512;&#32452;&#21512;&#20316;&#65292;&#36827;&#34892;&#20102;&#25968;&#25454;&#38598;&#30340;&#31574;&#21010;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#35813;&#25968;&#25454;&#38598;&#30340;&#35268;&#27169;&#26159;&#20043;&#21069;&#20844;&#24320;&#21457;&#24067;&#30340;&#24320;&#28304;&#25968;&#25454;&#38598;&#30340;&#20004;&#20010;&#25968;&#37327;&#32423;&#12290;&#28982;&#21518;&#25105;&#20204;&#35774;&#35745;&#20102;aCTIon&#65292;&#21033;&#29992;&#26368;&#36817;&#30340;&#25216;&#26415;&#21019;&#26032;&#8230;
&lt;/p&gt;
&lt;p&gt;
Cyber Threat Intelligence (CTI) plays a crucial role in assessing risks and enhancing security for organizations. However, the process of extracting relevant information from unstructured text sources can be expensive and time-consuming. Our empirical experience shows that existing tools for automated structured CTI extraction have performance limitations. Furthermore, the community lacks a common benchmark to quantitatively assess their performance. We fill these gaps providing a new large open benchmark dataset and aCTIon, a structured CTI information extraction tool. The dataset includes 204 real-world publicly available reports and their corresponding structured CTI information in STIX format. Our team curated the dataset involving three independent groups of CTI analysts working over the course of several months. To the best of our knowledge, this dataset is two orders of magnitude larger than previously released open source datasets. We then design aCTIon, leveraging recently int
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#28145;&#24230;&#36127;&#36733;&#20998;&#35299;&#31639;&#27861;&#23545;&#25932;&#23545;&#25915;&#20987;&#30340;&#25935;&#24863;&#24615;&#65292;&#24182;&#21457;&#29616;&#24120;&#29992;&#30340;CNN-based NILM&#27169;&#22411;&#26131;&#21463;&#25915;&#20987;&#65292;&#36825;&#23545;&#20110;&#36127;&#36733;&#20998;&#35299;&#31639;&#27861;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2307.10209</link><description>&lt;p&gt;
&#20851;&#20110;&#28145;&#24230;&#36127;&#36733;&#20998;&#35299;&#31639;&#27861;&#23545;&#25932;&#23545;&#25915;&#20987;&#30340;&#25935;&#24863;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Sensitivity of Deep Load Disaggregation to Adversarial Attacks. (arXiv:2307.10209v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10209
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#28145;&#24230;&#36127;&#36733;&#20998;&#35299;&#31639;&#27861;&#23545;&#25932;&#23545;&#25915;&#20987;&#30340;&#25935;&#24863;&#24615;&#65292;&#24182;&#21457;&#29616;&#24120;&#29992;&#30340;CNN-based NILM&#27169;&#22411;&#26131;&#21463;&#25915;&#20987;&#65292;&#36825;&#23545;&#20110;&#36127;&#36733;&#20998;&#35299;&#31639;&#27861;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#20405;&#20837;&#24335;&#36127;&#36733;&#30417;&#27979;&#65288;NILM&#65289;&#31639;&#27861;&#65292;&#36890;&#24120;&#31216;&#20026;&#36127;&#36733;&#20998;&#35299;&#31639;&#27861;&#65292;&#26159;&#26377;&#25928;&#33021;&#37327;&#31649;&#29702;&#30340;&#22522;&#26412;&#24037;&#20855;&#12290;&#23613;&#31649;&#28145;&#24230;&#27169;&#22411;&#22312;&#36127;&#36733;&#20998;&#35299;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#38754;&#20020;&#30528;&#35832;&#22810;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#19982;&#38544;&#31169;&#21644;&#23433;&#20840;&#26377;&#20851;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#33879;&#21517;&#30340;&#28145;&#24230;NILM&#22522;&#32447;&#23545;&#25932;&#23545;&#25915;&#20987;&#30340;&#25935;&#24863;&#24615;&#65292;&#25932;&#23545;&#25915;&#20987;&#24050;&#34987;&#35777;&#26126;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#35821;&#38899;&#35782;&#21035;&#31561;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#23041;&#32961;&#12290;&#25932;&#23545;&#25915;&#20987;&#28041;&#21450;&#23558;&#19981;&#21487;&#24863;&#30693;&#30340;&#22122;&#22768;&#24341;&#20837;&#36755;&#20837;&#25968;&#25454;&#65292;&#20197;&#35823;&#23548;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#38169;&#35823;&#30340;&#36755;&#20986;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#24555;&#36895;&#26799;&#24230;&#31526;&#21495;&#26041;&#27861;&#65288;FGSM&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#33879;&#21517;&#30340;&#25932;&#23545;&#25915;&#20987;&#26041;&#27861;&#65292;&#26469;&#25200;&#20081;&#36755;&#20837;&#24207;&#21015;&#65292;&#36825;&#20123;&#24207;&#21015;&#34987;&#29992;&#20110;&#20004;&#20010;&#24120;&#29992;&#30340;&#22522;&#20110;CNN&#30340;NILM&#22522;&#32447;&#27169;&#22411;&#65306;&#24207;&#21015;&#21040;&#24207;&#21015;&#65288;S2S&#65289;&#21644;&#24207;&#21015;&#21040;&#28857;&#65288;S2P&#65289;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#25552;&#20379;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#26131;&#21463;&#25915;&#20987;&#30340;&#26377;&#21147;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Non-intrusive Load Monitoring (NILM) algorithms, commonly referred to as load disaggregation algorithms, are fundamental tools for effective energy management. Despite the success of deep models in load disaggregation, they face various challenges, particularly those pertaining to privacy and security. This paper investigates the sensitivity of prominent deep NILM baselines to adversarial attacks, which have proven to be a significant threat in domains such as computer vision and speech recognition. Adversarial attacks entail the introduction of imperceptible noise into the input data with the aim of misleading the neural network into generating erroneous outputs. We investigate the Fast Gradient Sign Method (FGSM), a well-known adversarial attack, to perturb the input sequences fed into two commonly employed CNN-based NILM baselines: the Sequence-to-Sequence (S2S) and Sequence-to-Point (S2P) models. Our findings provide compelling evidence for the vulnerability of these models, partic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#38271;&#23614;&#20998;&#24067;&#25968;&#25454;&#38598;&#19978;&#30340;&#25932;&#23545;&#35757;&#32451;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25932;&#23545;&#35757;&#32451;&#26694;&#26550;--&#37325;&#26032;&#24179;&#34913;&#25932;&#23545;&#35757;&#32451;&#65288;REAT&#65289;&#12290;REAT&#33021;&#22815;&#35299;&#20915;&#25932;&#23545;&#35757;&#32451;&#36807;&#31243;&#20013;&#20135;&#29983;&#30340;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#26377;&#25928;&#22686;&#24378;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#24182;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.10205</link><description>&lt;p&gt;
&#38271;&#23614;&#20998;&#24067;&#19978;&#30340;&#25932;&#23545;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Adversarial Training Over Long-Tailed Distribution. (arXiv:2307.10205v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10205
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#38271;&#23614;&#20998;&#24067;&#25968;&#25454;&#38598;&#19978;&#30340;&#25932;&#23545;&#35757;&#32451;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25932;&#23545;&#35757;&#32451;&#26694;&#26550;--&#37325;&#26032;&#24179;&#34913;&#25932;&#23545;&#35757;&#32451;&#65288;REAT&#65289;&#12290;REAT&#33021;&#22815;&#35299;&#20915;&#25932;&#23545;&#35757;&#32451;&#36807;&#31243;&#20013;&#20135;&#29983;&#30340;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#26377;&#25928;&#22686;&#24378;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#24182;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26381;&#20174;&#38271;&#23614;&#20998;&#24067;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#25932;&#23545;&#35757;&#32451;&#65292;&#36825;&#22312;&#20043;&#21069;&#30340;&#24037;&#20316;&#20013;&#24456;&#23569;&#34987;&#25506;&#32034;&#12290;&#19982;&#24179;&#34913;&#25968;&#25454;&#38598;&#19978;&#30340;&#20256;&#32479;&#25932;&#23545;&#35757;&#32451;&#30456;&#27604;&#65292;&#35813;&#36807;&#31243;&#38754;&#20020;&#30528;&#20135;&#29983;&#19981;&#24179;&#34913;&#30340;&#25932;&#23545;&#26679;&#26412;&#21644;&#19981;&#24179;&#34913;&#30340;&#29305;&#24449;&#23884;&#20837;&#31354;&#38388;&#30340;&#22256;&#22659;&#65292;&#23548;&#33268;&#29983;&#25104;&#30340;&#27169;&#22411;&#22312;&#23614;&#37096;&#25968;&#25454;&#19978;&#34920;&#29616;&#20986;&#20302;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25932;&#23545;&#35757;&#32451;&#26694;&#26550;--&#37325;&#26032;&#24179;&#34913;&#25932;&#23545;&#35757;&#32451;&#65288;REAT&#65289;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#65306;&#65288;1&#65289;&#19968;&#31181;&#21463;&#26377;&#25928;&#26679;&#26412;&#25968;&#21551;&#21457;&#30340;&#26032;&#35757;&#32451;&#31574;&#30053;&#65292;&#29992;&#20110;&#24341;&#23548;&#27169;&#22411;&#29983;&#25104;&#26356;&#24179;&#34913;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#25932;&#23545;&#26679;&#26412;&#65307;&#65288;2&#65289;&#19968;&#31181;&#31934;&#24515;&#26500;&#24314;&#30340;&#24809;&#32602;&#20989;&#25968;&#65292;&#29992;&#20110;&#24378;&#21046;&#28385;&#36275;&#19968;&#20010;&#20196;&#20154;&#28385;&#24847;&#30340;&#29305;&#24449;&#31354;&#38388;&#12290;&#19981;&#21516;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#32467;&#26500;&#19978;&#30340;&#35780;&#20272;&#32467;&#26524;&#35777;&#26126;&#65292;REAT&#33021;&#22815;&#26377;&#25928;&#22686;&#24378;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#20445;&#25345;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#20195;&#30721;&#21487;&#20197;&#22312;https://&#20013;&#25214;&#21040;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study adversarial training on datasets that obey the long-tailed distribution, which is practical but rarely explored in previous works. Compared with conventional adversarial training on balanced datasets, this process falls into the dilemma of generating uneven adversarial examples (AEs) and an unbalanced feature embedding space, causing the resulting model to exhibit low robustness and accuracy on tail data. To combat that, we propose a new adversarial training framework -- Re-balancing Adversarial Training (REAT). This framework consists of two components: (1) a new training strategy inspired by the term effective number to guide the model to generate more balanced and informative AEs; (2) a carefully constructed penalty function to force a satisfactory feature space. Evaluation results on different datasets and model structures prove that REAT can effectively enhance the model's robustness and preserve the model's clean accuracy. The code can be found in https://
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;IPW&#30340;&#26080;&#20559;&#25490;&#24207;&#24230;&#37327;&#26041;&#27861;&#65292;&#38024;&#23545;&#21452;&#36793;&#24066;&#22330;&#20013;&#29992;&#25143;&#20043;&#38388;&#30340;&#20559;&#35265;&#30456;&#20114;&#20316;&#29992;&#65292;&#35299;&#20915;&#20102;&#20301;&#32622;&#20559;&#35265;&#21644;&#20004;&#20010;&#29992;&#25143;&#32676;&#20307;&#30340;&#20301;&#32622;&#20559;&#24046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.10204</link><description>&lt;p&gt;
&#22522;&#20110;IPW&#30340;&#21452;&#36793;&#24066;&#22330;&#20013;&#30340;&#26080;&#20559;&#25490;&#24207;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
An IPW-based Unbiased Ranking Metric in Two-sided Markets. (arXiv:2307.10204v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10204
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;IPW&#30340;&#26080;&#20559;&#25490;&#24207;&#24230;&#37327;&#26041;&#27861;&#65292;&#38024;&#23545;&#21452;&#36793;&#24066;&#22330;&#20013;&#29992;&#25143;&#20043;&#38388;&#30340;&#20559;&#35265;&#30456;&#20114;&#20316;&#29992;&#65292;&#35299;&#20915;&#20102;&#20301;&#32622;&#20559;&#35265;&#21644;&#20004;&#20010;&#29992;&#25143;&#32676;&#20307;&#30340;&#20301;&#32622;&#20559;&#24046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#26080;&#20559;&#23398;&#20064;&#25490;&#24207;&#65288;LTR&#65289;&#23545;&#20110;&#20248;&#20808;&#32771;&#34385;&#26469;&#33258;&#26377;&#20559;&#30340;&#38544;&#24335;&#29992;&#25143;&#21453;&#39304;&#65288;&#22914;&#28857;&#20987;&#25968;&#25454;&#65289;&#30340;&#39033;&#30446;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#25216;&#26415;&#65292;&#20363;&#22914;&#20498;&#25968;&#20542;&#21521;&#24615;&#21152;&#26435;&#65288;IPW&#65289;&#65292;&#29992;&#20110;&#21333;&#36793;&#24066;&#22330;&#12290;&#28982;&#32780;&#65292;&#22312;&#21452;&#36793;&#24066;&#22330;&#65288;&#22914;&#24037;&#20316;&#24179;&#21488;&#25110;&#32422;&#20250;&#26381;&#21153;&#65289;&#20013;&#65292;&#25104;&#21151;&#36716;&#21270;&#38656;&#35201;&#21305;&#37197;&#20004;&#20010;&#29992;&#25143;&#30340;&#20559;&#22909;&#65292;&#20294;&#23545;&#20110;&#36825;&#31181;&#24773;&#20917;&#20851;&#27880;&#36739;&#23569;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#21452;&#36793;&#24066;&#22330;&#20013;&#29992;&#25143;&#20043;&#38388;&#30340;&#20559;&#35265;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#23450;&#30340;LTR&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#21452;&#36793;&#21305;&#37197;&#24179;&#21488;&#20013;&#21453;&#39304;&#26426;&#21046;&#30340;&#24418;&#24335;&#21270;&#65292;&#24182;&#25351;&#20986;&#23427;&#20204;&#30340;&#38544;&#24335;&#21453;&#39304;&#21487;&#33021;&#21253;&#21547;&#26469;&#33258;&#20004;&#20010;&#29992;&#25143;&#32676;&#20307;&#30340;&#20301;&#32622;&#20559;&#24046;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;IPW&#20272;&#35745;&#22120;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20272;&#35745;&#22120;&#65292;&#31216;&#20026;&#21452;&#36793;IPW&#65292;&#20197;&#35299;&#20915;&#21452;&#36793;&#24066;&#22330;&#20013;&#30340;&#20301;&#32622;&#20559;&#35265;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#20272;&#35745;&#22120;&#28385;&#36275;&#30495;&#23454;&#25490;&#21517;&#30340;&#26080;&#20559;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In modern recommendation systems, unbiased learning-to-rank (LTR) is crucial for prioritizing items from biased implicit user feedback, such as click data. Several techniques, such as Inverse Propensity Weighting (IPW), have been proposed for single-sided markets. However, less attention has been paid to two-sided markets, such as job platforms or dating services, where successful conversions require matching preferences from both users. This paper addresses the complex interaction of biases between users in two-sided markets and proposes a tailored LTR approach. We first present a formulation of feedback mechanisms in two-sided matching platforms and point out that their implicit feedback may include position bias from both user groups. On the basis of this observation, we extend the IPW estimator and propose a new estimator, named two-sided IPW, to address the position bases in two-sided markets. We prove that the proposed estimator satisfies the unbiasedness for the ground-truth ran
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#31163;&#23130;&#27861;&#24237;&#35785;&#35772;&#65292;&#25506;&#32034;&#20102;&#24615;&#21035;&#19981;&#24179;&#31561;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#20559;&#35265;&#38382;&#39064;&#12290;&#38656;&#35201;&#23545;&#29616;&#26377;&#36164;&#28304;&#36827;&#34892;&#20462;&#27491;&#26469;&#37327;&#21270;&#31038;&#20250;&#19981;&#24179;&#31561;&#12290;</title><link>http://arxiv.org/abs/2307.10200</link><description>&lt;p&gt;
&#20174;&#27169;&#22411;&#20559;&#35265;&#20013;&#20998;&#31163;&#31038;&#20250;&#19981;&#24179;&#31561;&#65306;&#31163;&#23130;&#27861;&#24237;&#35785;&#35772;&#20013;&#30340;&#24615;&#21035;&#19981;&#24179;&#31561;
&lt;/p&gt;
&lt;p&gt;
Disentangling Societal Inequality from Model Biases: Gender Inequality in Divorce Court Proceedings. (arXiv:2307.10200v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10200
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#31163;&#23130;&#27861;&#24237;&#35785;&#35772;&#65292;&#25506;&#32034;&#20102;&#24615;&#21035;&#19981;&#24179;&#31561;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#20559;&#35265;&#38382;&#39064;&#12290;&#38656;&#35201;&#23545;&#29616;&#26377;&#36164;&#28304;&#36827;&#34892;&#20462;&#27491;&#26469;&#37327;&#21270;&#31038;&#20250;&#19981;&#24179;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#23130;&#26159;&#27861;&#38498;&#27861;&#24459;&#35299;&#38500;&#23130;&#23035;&#20851;&#31995;&#30340;&#36807;&#31243;&#12290;&#30001;&#20110;&#36825;&#36890;&#24120;&#26159;&#23130;&#23035;&#32852;&#21512;&#30340;&#19981;&#24841;&#24555;&#32467;&#26524;&#65292;&#27599;&#19968;&#26041;&#37117;&#21487;&#33021;&#26377;&#29702;&#30001;&#35201;&#27714;&#36864;&#20986;&#20915;&#23450;&#65292;&#36825;&#36890;&#24120;&#22312;&#27861;&#24237;&#35785;&#35772;&#20013;&#26377;&#35814;&#32454;&#35760;&#24405;&#12290;&#36890;&#36807;&#19968;&#20221;&#21253;&#21547;17,306&#20221;&#27861;&#24237;&#35785;&#35772;&#30340;&#22823;&#37327;&#35821;&#26009;&#24211;&#65292;&#26412;&#25991;&#36890;&#36807;&#31163;&#23130;&#27861;&#24237;&#35785;&#35772;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#24615;&#21035;&#19981;&#24179;&#31561;&#38382;&#39064;&#12290;&#34429;&#28982;&#26032;&#20852;&#30340;&#25968;&#25454;&#26469;&#28304;&#65288;&#20363;&#22914;&#20844;&#20849;&#27861;&#24237;&#35760;&#24405;&#65289;&#22312;&#36741;&#21161;&#31038;&#20250;&#31185;&#23398;&#30740;&#31350;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#20559;&#35265;&#21487;&#33021;&#20250;&#24178;&#25200;&#25110;&#24433;&#21709;&#27492;&#31867;&#30740;&#31350;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#38656;&#35201;&#23545;&#29616;&#26377;NLP&#36164;&#28304;&#20013;&#30340;&#28508;&#22312;&#24046;&#36317;&#21644;&#38480;&#21046;&#36827;&#34892;&#24443;&#24213;&#20998;&#26512;&#12290;&#22312;&#26041;&#27861;&#35770;&#19978;&#65292;&#26412;&#25991;&#35777;&#26126;&#20102;&#29616;&#26377;NLP&#36164;&#28304;&#38656;&#35201;&#36827;&#34892;&#20960;&#20010;&#38750;&#24179;&#20961;&#30340;&#20462;&#25913;&#65292;&#20197;&#37327;&#21270;&#31038;&#20250;&#19981;&#24179;&#31561;&#12290;&#22312;&#23454;&#36136;&#19978;&#65292;&#25105;&#20204;&#21457;&#29616;&#23613;&#31649;&#22823;&#37327;&#30340;&#27861;&#24237;&#26696;&#20214;&#21487;&#33021;&#26263;&#31034;&#30528;&#21464;&#21270;
&lt;/p&gt;
&lt;p&gt;
Divorce is the legal dissolution of a marriage by a court. Since this is usually an unpleasant outcome of a marital union, each party may have reasons to call the decision to quit which is generally documented in detail in the court proceedings. Via a substantial corpus of 17,306 court proceedings, this paper investigates gender inequality through the lens of divorce court proceedings. While emerging data sources (e.g., public court records) on sensitive societal issues hold promise in aiding social science research, biases present in cutting-edge natural language processing (NLP) methods may interfere with or affect such studies. We thus require a thorough analysis of potential gaps and limitations present in extant NLP resources. In this paper, on the methodological side, we demonstrate that existing NLP resources required several non-trivial modifications to quantify societal inequalities. On the substantive side, we find that while a large number of court cases perhaps suggest chan
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;StyleGAN2&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#36827;&#34892;&#21307;&#23398;&#22270;&#20687;&#30340;&#24322;&#24120;&#26816;&#27979;&#65292;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#37325;&#26500;&#22270;&#20687;&#24182;&#21033;&#29992;&#35780;&#20272;&#25351;&#26631;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#65292;&#23454;&#29616;&#20102;&#36229;&#36807;90%&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.10193</link><description>&lt;p&gt;
&#22522;&#20110;StyleGAN2&#30340;&#21307;&#23398;&#25104;&#20687;&#39046;&#22495;&#30340;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
StyleGAN2-based Out-of-Distribution Detection for Medical Imaging. (arXiv:2307.10193v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10193
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;StyleGAN2&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#36827;&#34892;&#21307;&#23398;&#22270;&#20687;&#30340;&#24322;&#24120;&#26816;&#27979;&#65292;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#37325;&#26500;&#22270;&#20687;&#24182;&#21033;&#29992;&#35780;&#20272;&#25351;&#26631;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#65292;&#23454;&#29616;&#20102;&#36229;&#36807;90%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#20020;&#24202;&#19978;&#30340;&#24212;&#29992;&#38754;&#20020;&#19968;&#20010;&#38556;&#30861;&#65292;&#21363;&#36816;&#34892;&#26102;&#23384;&#22312;&#30528;&#35757;&#32451;&#20998;&#24067;&#20043;&#22806;&#30340;&#22270;&#20687;&#12290;&#26412;&#25991;&#26088;&#22312;&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#26469;&#26816;&#27979;&#36825;&#20123;&#35757;&#32451;&#20998;&#24067;&#20043;&#22806;&#30340;&#22270;&#20687;&#12290;&#25105;&#20204;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#21253;&#21547;&#26469;&#33258;456&#21517;&#24739;&#32773;&#30340;3234&#20010;&#21547;&#26377;&#32925;&#33039;&#30340;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#22270;&#20687;&#12290;&#25105;&#20204;&#30340;&#35757;&#32451;&#20998;&#24067;&#37319;&#29992;&#20102;StyleGAN2-ADA&#26550;&#26500;&#36827;&#34892;&#24314;&#27169;&#12290;&#20351;&#29992;&#21453;&#21521;&#20256;&#25773;&#37325;&#26500;&#22270;&#20687;&#65292;&#24182;&#21033;&#29992;Wasserstein&#36317;&#31163;&#12289;&#22343;&#26041;&#24046;&#21644;&#32467;&#26500;&#30456;&#20284;&#24615;&#25351;&#25968;&#26469;&#35780;&#20272;&#37325;&#26500;&#32467;&#26524;&#12290;&#24322;&#24120;&#26816;&#27979;&#25928;&#26524;&#36890;&#36807;&#25509;&#25910;&#32773;&#25805;&#20316;&#29305;&#24449;&#26354;&#32447;&#19979;&#38754;&#31215;&#65288;AUROC&#65289;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#32925;&#33039;&#21644;&#38750;&#32925;&#33039;CT&#20043;&#38388;&#23454;&#29616;&#20102;&#36229;&#36807;90%&#30340;AUROC&#65292;&#21516;&#26102;&#26080;&#27861;&#23436;&#20840;&#37325;&#26500;&#32925;&#33039;&#30340;&#24322;&#24120;&#37096;&#20998;&#65292;&#20363;&#22914;...&#65288;&#25991;&#31456;&#25130;&#26029;&#65289;
&lt;/p&gt;
&lt;p&gt;
One barrier to the clinical deployment of deep learning-based models is the presence of images at runtime that lie far outside the training distribution of a given model. We aim to detect these out-of-distribution (OOD) images with a generative adversarial network (GAN). Our training dataset was comprised of 3,234 liver-containing computed tomography (CT) scans from 456 patients. Our OOD test data consisted of CT images of the brain, head and neck, lung, cervix, and abnormal livers. A StyleGAN2-ADA architecture was employed to model the training distribution. Images were reconstructed using backpropagation. Reconstructions were evaluated using the Wasserstein distance, mean squared error, and the structural similarity index measure. OOD detection was evaluated with the area under the receiver operating characteristic curve (AUROC). Our paradigm distinguished between liver and non-liver CT with greater than 90% AUROC. It was also completely unable to reconstruct liver artifacts, such as
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26159;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20960;&#20010;&#31867;&#21035;&#36827;&#34892;&#30340;&#31616;&#30701;&#35843;&#26597;&#65292;&#25552;&#20379;&#20102;&#21508;&#31867;LLM&#30340;&#26368;&#26032;&#21457;&#23637;&#21644;&#21162;&#21147;&#30340;&#27010;&#36848;&#65292;&#21253;&#25324;&#22810;&#35821;&#35328;LLMs&#12289;&#35270;&#35273;&#35821;&#35328;LLMs&#21644;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#31561;&#12290;&#21516;&#26102;&#65292;&#36824;&#31361;&#20986;&#20102;&#22312;&#32842;&#22825;&#26426;&#22120;&#20154;&#21644;&#34394;&#25311;&#21161;&#25163;&#39046;&#22495;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#22914;&#25552;&#21319;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#21644;&#35299;&#20915;&#36947;&#24503;&#21644;&#27861;&#24459;&#22256;&#22659;&#12290;</title><link>http://arxiv.org/abs/2307.10188</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20960;&#20010;&#31867;&#21035;&#65306;&#31616;&#30701;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Several categories of Large Language Models (LLMs): A Short Survey. (arXiv:2307.10188v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10188
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26159;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20960;&#20010;&#31867;&#21035;&#36827;&#34892;&#30340;&#31616;&#30701;&#35843;&#26597;&#65292;&#25552;&#20379;&#20102;&#21508;&#31867;LLM&#30340;&#26368;&#26032;&#21457;&#23637;&#21644;&#21162;&#21147;&#30340;&#27010;&#36848;&#65292;&#21253;&#25324;&#22810;&#35821;&#35328;LLMs&#12289;&#35270;&#35273;&#35821;&#35328;LLMs&#21644;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#31561;&#12290;&#21516;&#26102;&#65292;&#36824;&#31361;&#20986;&#20102;&#22312;&#32842;&#22825;&#26426;&#22120;&#20154;&#21644;&#34394;&#25311;&#21161;&#25163;&#39046;&#22495;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#22914;&#25552;&#21319;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#21644;&#35299;&#20915;&#36947;&#24503;&#21644;&#27861;&#24459;&#22256;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26377;&#25928;&#24037;&#20855;&#65292;&#24182;&#22312;&#35768;&#22810;&#19981;&#21516;&#39046;&#22495;&#20013;&#24471;&#21040;&#24212;&#29992;&#12290;&#26412;&#25991;&#23545;&#21508;&#31181;LLM&#23376;&#31867;&#36827;&#34892;&#20102;&#31616;&#27905;&#30340;&#24635;&#32467;&#12290;&#35813;&#35843;&#26597;&#24378;&#35843;&#20102;&#21508;&#31181;LLM&#31867;&#22411;&#30340;&#26368;&#26032;&#21457;&#23637;&#21644;&#21162;&#21147;&#65292;&#21253;&#25324;&#22522;&#20110;&#20219;&#21153;&#30340;&#37329;&#34701;LLM&#65292;&#22810;&#35821;&#35328;LLM&#65292;&#29983;&#29289;&#21307;&#23398;&#21644;&#20020;&#24202;LLM&#65292;&#35270;&#35273;&#35821;&#35328;LLM&#21644;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#12290;&#35813;&#35843;&#26597;&#23545;&#27599;&#20010;LLM&#31867;&#21035;&#20013;&#24212;&#29992;&#30340;&#26041;&#27861;&#12289;&#23646;&#24615;&#12289;&#25968;&#25454;&#38598;&#12289;&#21464;&#21387;&#22120;&#27169;&#22411;&#21644;&#27604;&#36739;&#25351;&#26631;&#36827;&#34892;&#20102;&#27010;&#36848;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#31361;&#20986;&#20102;&#22312;&#24320;&#21457;&#32842;&#22825;&#26426;&#22120;&#20154;&#21644;&#34394;&#25311;&#21161;&#25163;&#39046;&#22495;&#23384;&#22312;&#30340;&#26410;&#35299;&#20915;&#38382;&#39064;&#65292;&#22914;&#25552;&#21319;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#65292;&#22686;&#24378;&#32842;&#22825;&#26426;&#22120;&#20154;&#26234;&#33021;&#24615;&#20197;&#21450;&#35299;&#20915;&#36947;&#24503;&#21644;&#27861;&#24459;&#22256;&#22659;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#20026;&#23545;&#22522;&#20110;LLM&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#21644;&#34394;&#25311;&#26234;&#33021;&#21161;&#25163;&#25216;&#26415;&#24863;&#20852;&#36259;&#30340;&#35835;&#32773;&#12289;&#24320;&#21457;&#20154;&#21592;&#12289;&#23398;&#26415;&#30028;&#20154;&#22763;&#21644;&#29992;&#25143;&#25552;&#20379;&#26377;&#29992;&#30340;&#20449;&#24687;&#21644;&#26410;&#26469;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models(LLMs)have become effective tools for natural language processing and have been used in many different fields. This essay offers a succinct summary of various LLM subcategories. The survey emphasizes recent developments and efforts made for various LLM kinds, including task-based financial LLMs, multilingual language LLMs, biomedical and clinical LLMs, vision language LLMs, and code language models. The survey gives a general summary of the methods, attributes, datasets, transformer models, and comparison metrics applied in each category of LLMs. Furthermore, it highlights unresolved problems in the field of developing chatbots and virtual assistants, such as boosting natural language processing, enhancing chatbot intelligence, and resolving moral and legal dilemmas. The purpose of this study is to provide readers, developers, academics, and users interested in LLM-based chatbots and virtual intelligent assistant technologies with useful information and future dire
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#37325;&#35201;&#24615;&#37319;&#26679;&#36827;&#34892;&#38544;&#31169;&#25918;&#22823;&#65292;&#21487;&#20197;&#21516;&#26102;&#22686;&#24378;&#38544;&#31169;&#20445;&#25252;&#21644;&#25552;&#39640;&#25928;&#29992;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#19968;&#33324;&#30340;&#32467;&#26524;&#26469;&#37327;&#21270;&#36873;&#25321;&#27010;&#29575;&#26435;&#37325;&#23545;&#38544;&#31169;&#25918;&#22823;&#30340;&#24433;&#21709;&#65292;&#24182;&#23637;&#31034;&#20102;&#24322;&#36136;&#37319;&#26679;&#27010;&#29575;&#21487;&#20197;&#22312;&#20445;&#25345;&#23376;&#37319;&#26679;&#22823;&#23567;&#19981;&#21464;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#26356;&#22909;&#30340;&#38544;&#31169;&#21644;&#25928;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.10187</link><description>&lt;p&gt;
&#38544;&#31169;&#25918;&#22823;&#36890;&#36807;&#37325;&#35201;&#24615;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Privacy Amplification via Importance Sampling. (arXiv:2307.10187v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10187
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#37325;&#35201;&#24615;&#37319;&#26679;&#36827;&#34892;&#38544;&#31169;&#25918;&#22823;&#65292;&#21487;&#20197;&#21516;&#26102;&#22686;&#24378;&#38544;&#31169;&#20445;&#25252;&#21644;&#25552;&#39640;&#25928;&#29992;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#19968;&#33324;&#30340;&#32467;&#26524;&#26469;&#37327;&#21270;&#36873;&#25321;&#27010;&#29575;&#26435;&#37325;&#23545;&#38544;&#31169;&#25918;&#22823;&#30340;&#24433;&#21709;&#65292;&#24182;&#23637;&#31034;&#20102;&#24322;&#36136;&#37319;&#26679;&#27010;&#29575;&#21487;&#20197;&#22312;&#20445;&#25345;&#23376;&#37319;&#26679;&#22823;&#23567;&#19981;&#21464;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#26356;&#22909;&#30340;&#38544;&#31169;&#21644;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#36890;&#36807;&#37325;&#35201;&#24615;&#37319;&#26679;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#23376;&#37319;&#26679;&#20316;&#20026;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#30340;&#39044;&#22788;&#29702;&#27493;&#39588;&#26469;&#22686;&#24378;&#38544;&#31169;&#20445;&#25252;&#30340;&#24615;&#36136;&#12290;&#36825;&#25193;&#23637;&#20102;&#24050;&#26377;&#30340;&#36890;&#36807;&#23376;&#37319;&#26679;&#36827;&#34892;&#38544;&#31169;&#25918;&#22823;&#30340;&#32467;&#26524;&#21040;&#37325;&#35201;&#24615;&#37319;&#26679;&#65292;&#20854;&#20013;&#27599;&#20010;&#25968;&#25454;&#28857;&#30340;&#26435;&#37325;&#20026;&#20854;&#34987;&#36873;&#25321;&#27010;&#29575;&#30340;&#20498;&#25968;&#12290;&#27599;&#20010;&#28857;&#30340;&#36873;&#25321;&#27010;&#29575;&#30340;&#26435;&#37325;&#23545;&#38544;&#31169;&#30340;&#24433;&#21709;&#24182;&#19981;&#26126;&#26174;&#12290;&#19968;&#26041;&#38754;&#65292;&#36739;&#20302;&#30340;&#36873;&#25321;&#27010;&#29575;&#20250;&#23548;&#33268;&#26356;&#24378;&#30340;&#38544;&#31169;&#25918;&#22823;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#26435;&#37325;&#36234;&#39640;&#65292;&#22312;&#28857;&#34987;&#36873;&#25321;&#26102;&#65292;&#28857;&#23545;&#26426;&#21046;&#36755;&#20986;&#30340;&#24433;&#21709;&#23601;&#36234;&#24378;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#19968;&#33324;&#30340;&#32467;&#26524;&#26469;&#37327;&#21270;&#36825;&#20004;&#20010;&#24433;&#21709;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24322;&#36136;&#37319;&#26679;&#27010;&#29575;&#21487;&#20197;&#21516;&#26102;&#27604;&#22343;&#21248;&#23376;&#37319;&#26679;&#20855;&#26377;&#26356;&#24378;&#30340;&#38544;&#31169;&#21644;&#26356;&#22909;&#30340;&#25928;&#29992;&#65292;&#24182;&#20445;&#25345;&#23376;&#37319;&#26679;&#22823;&#23567;&#19981;&#21464;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#21046;&#23450;&#21644;&#35299;&#20915;&#20102;&#38544;&#31169;&#20248;&#21270;&#37319;&#26679;&#30340;&#38382;&#39064;&#65292;&#21363;&#23547;&#25214;...
&lt;/p&gt;
&lt;p&gt;
We examine the privacy-enhancing properties of subsampling a data set via importance sampling as a pre-processing step for differentially private mechanisms. This extends the established privacy amplification by subsampling result to importance sampling where each data point is weighted by the reciprocal of its selection probability. The implications for privacy of weighting each point are not obvious. On the one hand, a lower selection probability leads to a stronger privacy amplification. On the other hand, the higher the weight, the stronger the influence of the point on the output of the mechanism in the event that the point does get selected. We provide a general result that quantifies the trade-off between these two effects. We show that heterogeneous sampling probabilities can lead to both stronger privacy and better utility than uniform subsampling while retaining the subsample size. In particular, we formulate and solve the problem of privacy-optimal sampling, that is, finding
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;U&#24418;&#22810;&#23618;&#24863;&#30693;&#22120;(MUMLP)&#27169;&#22411;&#65292;&#21033;&#29992;&#35774;&#35745;&#30340;MSC(Multi-Scale Channel)&#22359;&#21644;UMLP(U&#24418;&#22810;&#23618;&#24863;&#30693;&#22120;)&#32467;&#26500;&#65292;&#26377;&#25928;&#22320;&#34920;&#31034;&#20102;&#39640;&#20809;&#35889;&#22270;&#20687;&#30340;&#35821;&#20041;&#21644;&#31354;&#38388;&#20449;&#24687;&#65292;&#24182;&#19988;&#33021;&#22815;&#21387;&#32553;&#22823;&#35268;&#27169;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2307.10186</link><description>&lt;p&gt;
&#22810;&#23610;&#24230;U&#24418;MLP&#29992;&#20110;&#39640;&#20809;&#35889;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Multi-Scale U-Shape MLP for Hyperspectral Image Classification. (arXiv:2307.10186v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10186
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;U&#24418;&#22810;&#23618;&#24863;&#30693;&#22120;(MUMLP)&#27169;&#22411;&#65292;&#21033;&#29992;&#35774;&#35745;&#30340;MSC(Multi-Scale Channel)&#22359;&#21644;UMLP(U&#24418;&#22810;&#23618;&#24863;&#30693;&#22120;)&#32467;&#26500;&#65292;&#26377;&#25928;&#22320;&#34920;&#31034;&#20102;&#39640;&#20809;&#35889;&#22270;&#20687;&#30340;&#35821;&#20041;&#21644;&#31354;&#38388;&#20449;&#24687;&#65292;&#24182;&#19988;&#33021;&#22815;&#21387;&#32553;&#22823;&#35268;&#27169;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#20809;&#35889;&#22270;&#20687;&#22312;&#21508;&#20010;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#30340;&#24212;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#20809;&#35889;&#27874;&#27573;&#20013;&#27880;&#20876;&#20102;&#22823;&#37327;&#30340;&#35821;&#20041;&#21644;&#31354;&#38388;&#20449;&#24687;&#65292;&#24182;&#19988;&#20855;&#26377;&#20809;&#35889;&#29305;&#24449;&#30340;&#31354;&#38388;&#21487;&#21464;&#24615;&#12290;&#22312;&#35782;&#21035;&#39640;&#20809;&#35889;&#22270;&#20687;&#30340;&#20687;&#32032;&#20013;&#23384;&#22312;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#20998;&#21035;&#26159;&#34920;&#31034;&#23616;&#37096;&#21644;&#20840;&#23616;&#20043;&#38388;&#30456;&#20851;&#20449;&#24687;&#20197;&#21450;&#27169;&#22411;&#20016;&#23500;&#21442;&#25968;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;U&#24418;&#22810;&#23618;&#24863;&#30693;&#22120;(MUMLP)&#27169;&#22411;&#65292;&#23427;&#30001;&#35774;&#35745;&#30340;MSC(Multi-Scale Channel)&#22359;&#21644;UMLP(U&#24418;&#22810;&#23618;&#24863;&#30693;&#22120;)&#32467;&#26500;&#32452;&#25104;&#12290;MSC&#23558;&#36890;&#36947;&#32500;&#24230;&#36716;&#25442;&#21644;&#28151;&#21512;&#20809;&#35889;&#27874;&#27573;&#29305;&#24449;&#20197;&#23884;&#20837;&#36866;&#24403;&#30340;&#28145;&#23618;&#34920;&#31034;&#12290;UMLP&#30001;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32467;&#26500;&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#23618;&#35774;&#35745;&#32780;&#25104;&#65292;&#33021;&#22815;&#21387;&#32553;&#22823;&#35268;&#27169;&#21442;&#25968;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#26469;&#35777;&#26126;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hyperspectral images have significant applications in various domains, since they register numerous semantic and spatial information in the spectral band with spatial variability of spectral signatures. Two critical challenges in identifying pixels of the hyperspectral image are respectively representing the correlated information among the local and global, as well as the abundant parameters of the model. To tackle this challenge, we propose a Multi-Scale U-shape Multi-Layer Perceptron (MUMLP) a model consisting of the designed MSC (Multi-Scale Channel) block and the UMLP (U-shape Multi-Layer Perceptron) structure. MSC transforms the channel dimension and mixes spectral band feature to embed the deep-level representation adequately. UMLP is designed by the encoder-decoder structure with multi-layer perceptron layers, which is capable of compressing the large-scale parameters. Extensive experiments are conducted to demonstrate our model can outperform state-of-the-art methods across-th
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DUBA&#30340;&#21452;&#37325;&#38544;&#31192;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21516;&#26102;&#32771;&#34385;&#20102;&#35302;&#21457;&#22120;&#22312;&#31354;&#38388;&#21644;&#39057;&#29575;&#22495;&#20013;&#30340;&#38544;&#21311;&#24615;&#65292;&#20197;&#23454;&#29616;&#33391;&#22909;&#30340;&#25915;&#20987;&#24615;&#33021;&#21644;&#24378;&#22823;&#30340;&#38544;&#21311;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.10184</link><description>&lt;p&gt;
&#19968;&#31181;&#21452;&#37325;&#38544;&#31192;&#21518;&#38376;&#25915;&#20987;&#65306;&#20174;&#31354;&#38388;&#21644;&#39057;&#29575;&#35282;&#24230;&#26469;&#30475;
&lt;/p&gt;
&lt;p&gt;
A Dual Stealthy Backdoor: From Both Spatial and Frequency Perspectives. (arXiv:2307.10184v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10184
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DUBA&#30340;&#21452;&#37325;&#38544;&#31192;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21516;&#26102;&#32771;&#34385;&#20102;&#35302;&#21457;&#22120;&#22312;&#31354;&#38388;&#21644;&#39057;&#29575;&#22495;&#20013;&#30340;&#38544;&#21311;&#24615;&#65292;&#20197;&#23454;&#29616;&#33391;&#22909;&#30340;&#25915;&#20987;&#24615;&#33021;&#21644;&#24378;&#22823;&#30340;&#38544;&#21311;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#38376;&#25915;&#20987;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#26500;&#25104;&#20005;&#37325;&#30340;&#23433;&#20840;&#23041;&#32961;&#12290;&#21518;&#38376;&#27169;&#22411;&#22312;&#24102;&#26377;&#31934;&#24515;&#35774;&#35745;&#30340;&#35302;&#21457;&#22120;&#30340;&#36755;&#20837;&#19978;&#20250;&#20219;&#24847;&#65288;&#26377;&#38024;&#23545;&#24615;&#22320;&#65289;&#20986;&#29616;&#38169;&#35823;&#39044;&#27979;&#65292;&#32780;&#22312;&#24178;&#20928;&#30340;&#36755;&#20837;&#19978;&#34920;&#29616;&#27491;&#24120;&#12290;&#35768;&#22810;&#30740;&#31350;&#25506;&#32034;&#20102;&#21518;&#38376;&#35302;&#21457;&#22120;&#30340;&#38544;&#21311;&#24615;&#20197;&#25552;&#39640;&#25915;&#20987;&#30340;&#38544;&#31192;&#24615;&#12290;&#28982;&#32780;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#21482;&#32771;&#34385;&#20102;&#31354;&#38388;&#22495;&#20013;&#30340;&#38544;&#21311;&#24615;&#65292;&#27809;&#26377;&#26126;&#30830;&#32771;&#34385;&#22312;&#39057;&#29575;&#22495;&#20013;&#29983;&#25104;&#38544;&#21311;&#35302;&#21457;&#22120;&#65292;&#20351;&#29983;&#25104;&#30340;&#27602;&#23475;&#22270;&#20687;&#23481;&#26131;&#34987;&#26368;&#36817;&#30340;&#38450;&#24481;&#26041;&#27861;&#26816;&#27979;&#21040;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DUBA&#30340;DUal&#38544;&#31192;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21516;&#26102;&#32771;&#34385;&#20102;&#35302;&#21457;&#22120;&#22312;&#31354;&#38388;&#21644;&#39057;&#29575;&#22495;&#20013;&#30340;&#38544;&#21311;&#24615;&#65292;&#20197;&#23454;&#29616;&#33391;&#22909;&#30340;&#25915;&#20987;&#24615;&#33021;&#65292;&#21516;&#26102;&#30830;&#20445;&#24378;&#22823;&#30340;&#38544;&#21311;&#24615;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#31163;&#25955;&#23567;&#27874;&#21464;&#25442;&#23558;&#35302;&#21457;&#22120;&#22270;&#20687;&#30340;&#39640;&#39057;&#20449;&#24687;&#23884;&#20837;&#24178;&#20928;&#22270;&#20687;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Backdoor attacks pose serious security threats to deep neural networks (DNNs). Backdoored models make arbitrarily (targeted) incorrect predictions on inputs embedded with well-designed triggers while behaving normally on clean inputs. Many works have explored the invisibility of backdoor triggers to improve attack stealthiness. However, most of them only consider the invisibility in the spatial domain without explicitly accounting for the generation of invisible triggers in the frequency domain, making the generated poisoned images be easily detected by recent defense methods. To address this issue, in this paper, we propose a DUal stealthy BAckdoor attack method named DUBA, which simultaneously considers the invisibility of triggers in both the spatial and frequency domains, to achieve desirable attack performance, while ensuring strong stealthiness. Specifically, we first use Discrete Wavelet Transform to embed the high-frequency information of the trigger image into the clean image 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31038;&#21306;&#24863;&#30693;&#30340;Transformer&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#38381;&#30151;fMRI&#36830;&#25509;&#22270;&#30340;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#31038;&#21306;&#24863;&#30693;&#33410;&#28857;&#23884;&#20837;&#26469;&#25552;&#39640;ASD&#30340;&#35786;&#26029;&#20934;&#30830;&#24615;&#21644;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.10181</link><description>&lt;p&gt;
&#22522;&#20110;&#31038;&#21306;&#24863;&#30693;&#30340;Transformer&#29992;&#20110;&#33258;&#38381;&#30151;fMRI&#36830;&#25509;&#22270;&#30340;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Community-Aware Transformer for Autism Prediction in fMRI Connectome. (arXiv:2307.10181v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10181
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31038;&#21306;&#24863;&#30693;&#30340;Transformer&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#38381;&#30151;fMRI&#36830;&#25509;&#22270;&#30340;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#31038;&#21306;&#24863;&#30693;&#33410;&#28857;&#23884;&#20837;&#26469;&#25552;&#39640;ASD&#30340;&#35786;&#26029;&#20934;&#30830;&#24615;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#65288;ASD&#65289;&#26159;&#19968;&#31181;&#32456;&#29983;&#24615;&#30340;&#31070;&#32463;&#21457;&#32946;&#38556;&#30861;&#65292;&#24433;&#21709;&#31038;&#20132;&#20132;&#27969;&#21644;&#34892;&#20026;&#12290;&#36890;&#36807;&#30740;&#31350;&#21151;&#33021;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;fMRI&#65289;&#22522;&#20110;&#22823;&#33041;&#21151;&#33021;&#36830;&#25509;&#22270;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#29702;&#35299;&#21644;&#35786;&#26029;ASD&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#26377;&#25928;&#30340;&#27835;&#30103;&#12290;&#25105;&#20204;&#23558;&#22823;&#33041;&#24314;&#27169;&#20026;&#19968;&#32452;&#22823;&#33041;&#24863;&#20852;&#36259;&#21306;&#22495;&#65288;ROIs&#65289;&#65292;ROIs&#24418;&#25104;&#31038;&#21306;&#65292;&#20102;&#35299;&#36825;&#20123;&#31038;&#21306;&#23545;ASD&#30340;&#35786;&#26029;&#33267;&#20851;&#37325;&#35201;&#12290;&#19968;&#26041;&#38754;&#65292;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#24050;&#34987;&#35777;&#26126;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#38750;&#24120;&#26377;&#25928;&#65292;&#21253;&#25324;fMRI&#36830;&#25509;&#22270;&#20998;&#26512;&#65292;&#20197;&#23398;&#20064;&#26377;&#29992;&#30340;ROIs&#34920;&#31034;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#24179;&#31561;&#22320;&#22788;&#29702;&#25152;&#26377;ROIs&#65292;&#24182;&#24573;&#35270;&#20102;&#23398;&#20064;&#33410;&#28857;&#23884;&#20837;&#26102;&#31038;&#21306;&#29305;&#23450;&#20851;&#32852;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;Com-BrainTF&#65292;&#36825;&#26159;&#19968;&#31181;&#23618;&#27425;&#21270;&#30340;&#23616;&#37096;-&#20840;&#23616;Transformer&#26550;&#26500;&#65292;&#29992;&#20110;&#23398;&#20064;&#33258;&#38381;&#30151;&#39044;&#27979;&#20219;&#21153;&#30340;&#31038;&#21306;&#24863;&#30693;&#33410;&#28857;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autism spectrum disorder(ASD) is a lifelong neurodevelopmental condition that affects social communication and behavior. Investigating functional magnetic resonance imaging (fMRI)-based brain functional connectome can aid in the understanding and diagnosis of ASD, leading to more effective treatments. The brain is modeled as a network of brain Regions of Interest (ROIs), and ROIs form communities and knowledge of these communities is crucial for ASD diagnosis. On the one hand, Transformer-based models have proven to be highly effective across several tasks, including fMRI connectome analysis to learn useful representations of ROIs. On the other hand, existing transformer-based models treat all ROIs equally and overlook the impact of community-specific associations when learning node embeddings. To fill this gap, we propose a novel method, Com-BrainTF, a hierarchical local-global transformer architecture that learns intra and inter-community aware node embeddings for ASD prediction task
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22522;&#20110;&#28151;&#21512;&#21322;&#38750;&#23616;&#37096;&#20808;&#39564;&#23494;&#24230;&#21644;&#28857;&#36136;&#37327;&#30340;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#25628;&#32034;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#33639;&#20809;&#20449;&#21495;&#20013;&#30830;&#23450;&#31070;&#32463;&#20803;&#30340;&#33033;&#20914;&#24207;&#21015;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#39640;&#25928;&#22320;&#25506;&#32034;&#25152;&#26377;&#21487;&#33021;&#30340;&#33033;&#20914;&#25490;&#21015;&#24182;&#25253;&#21578;&#26368;&#39640;&#21518;&#39564;&#27010;&#29575;&#30340;&#33033;&#20914;&#25490;&#21015;&#21644;&#27599;&#20010;&#33033;&#20914;&#20301;&#32622;&#30340;&#21518;&#39564;&#27010;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.10177</link><description>&lt;p&gt;
&#22522;&#20110;&#38750;&#23616;&#37096;&#20808;&#39564;&#30340;&#36125;&#21494;&#26031;&#33033;&#20914;&#24207;&#21015;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Bayesian Spike Train Inference via Non-Local Priors. (arXiv:2307.10177v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10177
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#28151;&#21512;&#21322;&#38750;&#23616;&#37096;&#20808;&#39564;&#23494;&#24230;&#21644;&#28857;&#36136;&#37327;&#30340;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#25628;&#32034;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#33639;&#20809;&#20449;&#21495;&#20013;&#30830;&#23450;&#31070;&#32463;&#20803;&#30340;&#33033;&#20914;&#24207;&#21015;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#39640;&#25928;&#22320;&#25506;&#32034;&#25152;&#26377;&#21487;&#33021;&#30340;&#33033;&#20914;&#25490;&#21015;&#24182;&#25253;&#21578;&#26368;&#39640;&#21518;&#39564;&#27010;&#29575;&#30340;&#33033;&#20914;&#25490;&#21015;&#21644;&#27599;&#20010;&#33033;&#20914;&#20301;&#32622;&#30340;&#21518;&#39564;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31185;&#23398;&#30340;&#36827;&#23637;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#21516;&#26102;&#27979;&#37327;&#34892;&#20026;&#21160;&#29289;&#20013;&#22823;&#37327;&#31070;&#32463;&#20803;&#30340;&#27963;&#21160;&#12290;&#25105;&#20204;&#21487;&#20197;&#33719;&#24471;&#27599;&#20010;&#31070;&#32463;&#20803;&#30340;&#33639;&#20809;&#20449;&#21495;&#65292;&#36825;&#25552;&#20379;&#20102;&#31070;&#32463;&#27963;&#21160;&#38543;&#26102;&#38388;&#30340;&#19968;&#38454;&#36817;&#20284;&#12290;&#26681;&#25454;&#36825;&#20010;&#33639;&#20809;&#20449;&#21495;&#30830;&#23450;&#31070;&#32463;&#20803;&#30340;&#30830;&#20999;&#33033;&#20914;&#26159;&#35745;&#31639;&#31070;&#32463;&#31185;&#23398;&#39046;&#22495;&#30340;&#19968;&#20010;&#27963;&#36291;&#30740;&#31350;&#39046;&#22495;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;&#21322;&#38750;&#23616;&#37096;&#20808;&#39564;&#23494;&#24230;&#21644;&#28857;&#36136;&#37327;&#30340;&#26032;&#39062;&#36125;&#21494;&#26031;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#25628;&#32034;&#30340;&#26041;&#27861;&#65292;&#32780;&#19981;&#26159;&#35745;&#31639;&#22797;&#26434;&#30340;MCMC&#31639;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#21033;&#29992;&#29616;&#20195;&#35745;&#31639;&#29615;&#22659;&#65288;&#36890;&#24120;&#37197;&#22791;&#26377;&#22810;&#20010;&#22788;&#29702;&#22120;&#65289;&#30340;&#20248;&#21183;&#65292;&#25506;&#32034;&#35266;&#23519;&#21040;&#30340;&#33033;&#20914;&#24207;&#21015;&#20013;&#25152;&#26377;&#21487;&#33021;&#30340;&#33033;&#20914;&#25490;&#21015;&#21644;&#32570;&#22833;&#24773;&#20917;&#12290;&#28982;&#21518;&#25253;&#21578;&#33033;&#20914;&#25490;&#21015;&#30340;&#26368;&#39640;&#21518;&#39564;&#27010;&#29575;&#21644;&#27599;&#20010;&#33033;&#20914;&#20301;&#32622;&#30340;&#21518;&#39564;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advances in neuroscience have enabled researchers to measure the activities of large numbers of neurons simultaneously in behaving animals. We have access to the fluorescence of each of the neurons which provides a first-order approximation of the neural activity over time. Determining the exact spike of a neuron from this fluorescence trace constitutes an active area of research within the field of computational neuroscience. We propose a novel Bayesian approach based on a mixture of half-non-local prior densities and point masses for this task. Instead of a computationally expensive MCMC algorithm, we adopt a stochastic search-based approach that is capable of taking advantage of modern computing environments often equipped with multiple processors, to explore all possible arrangements of spikes and lack thereof in an observed spike train. It then reports the highest posterior probability arrangement of spikes and posterior probability for a spike at each location of the spike train.
&lt;/p&gt;</description></item><item><title>&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;&#65306;&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#25552;&#39640;&#29992;&#25143;&#38271;&#26399;&#28385;&#24847;&#24230;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#39044;&#27979;&#24310;&#36831;&#22870;&#21169;&#30340;&#27169;&#22411;&#21644;&#35774;&#35745;&#19968;&#20010;&#21033;&#29992;&#35813;&#27169;&#22411;&#30340;&#36172;&#21338;&#31639;&#27861;&#26469;&#35299;&#20915;&#20102;&#36890;&#36807;&#27979;&#37327;&#30701;&#26399;&#20195;&#29702;&#22870;&#21169;&#21453;&#26144;&#23454;&#38469;&#38271;&#26399;&#30446;&#26631;&#19981;&#23436;&#32654;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.09943</link><description>&lt;p&gt;
&#36825;&#37324;&#26159;&#32763;&#35793;&#36807;&#30340;&#35770;&#25991;&#26631;&#39064;: &#36807;&#21435;&#26366;&#32763;&#35793;&#12298;Impatient Bandits: Optimizing for the Long-Term Without Delay&#12299;
&lt;/p&gt;
&lt;p&gt;
Impatient Bandits: Optimizing for the Long-Term Without Delay. (arXiv:2307.09943v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09943
&lt;/p&gt;
&lt;p&gt;
&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;&#65306;&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#25552;&#39640;&#29992;&#25143;&#38271;&#26399;&#28385;&#24847;&#24230;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#39044;&#27979;&#24310;&#36831;&#22870;&#21169;&#30340;&#27169;&#22411;&#21644;&#35774;&#35745;&#19968;&#20010;&#21033;&#29992;&#35813;&#27169;&#22411;&#30340;&#36172;&#21338;&#31639;&#27861;&#26469;&#35299;&#20915;&#20102;&#36890;&#36807;&#27979;&#37327;&#30701;&#26399;&#20195;&#29702;&#22870;&#21169;&#21453;&#26144;&#23454;&#38469;&#38271;&#26399;&#30446;&#26631;&#19981;&#23436;&#32654;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#37324;&#26159;&#32763;&#35793;&#36807;&#30340;&#35770;&#25991;&#25688;&#35201;&#65306;&#25512;&#33616;&#31995;&#32479;&#22312;&#22312;&#32447;&#24179;&#21488;&#19978;&#26159;&#19968;&#20010;&#26222;&#36941;&#23384;&#22312;&#30340;&#21151;&#33021;&#12290;&#36234;&#26469;&#36234;&#22810;&#30340;&#24773;&#20917;&#19979;&#65292;&#23427;&#20204;&#26126;&#30830;&#22320;&#34987;&#20219;&#21153;&#20026;&#25552;&#39640;&#29992;&#25143;&#30340;&#38271;&#26399;&#28385;&#24847;&#24230;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#20869;&#23481;&#25506;&#32034;&#20219;&#21153;&#65292;&#23558;&#20854;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#20855;&#26377;&#24310;&#36831;&#22870;&#21169;&#30340;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22312;&#36873;&#25321;&#23398;&#20064;&#20449;&#21495;&#26102;&#23384;&#22312;&#26126;&#26174;&#30340;&#26435;&#34913;&#65306;&#31561;&#24453;&#23436;&#20840;&#30340;&#22870;&#21169;&#21487;&#33021;&#38656;&#35201;&#20960;&#21608;&#26102;&#38388;&#65292;&#36825;&#20250;&#24433;&#21709;&#23398;&#20064;&#21457;&#29983;&#30340;&#36895;&#24230;&#65292;&#32780;&#27979;&#37327;&#30701;&#26399;&#20195;&#29702;&#22870;&#21169;&#21017;&#19981;&#23436;&#32654;&#22320;&#21453;&#26144;&#20102;&#23454;&#38469;&#30340;&#38271;&#26399;&#30446;&#26631;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#27493;&#39588;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#39044;&#27979;&#24310;&#36831;&#22870;&#21169;&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#25972;&#21512;&#36804;&#20170;&#25152;&#33719;&#24471;&#30340;&#25152;&#26377;&#20449;&#24687;&#12290;&#36890;&#36807;&#36125;&#21494;&#26031;&#28388;&#27874;&#22120;&#32452;&#21512;&#23436;&#25972;&#30340;&#35266;&#23519;&#32467;&#26524;&#20197;&#21450;&#37096;&#20998;&#65288;&#30701;&#26399;&#25110;&#20013;&#26399;&#65289;&#30340;&#32467;&#26524;&#65292;&#20174;&#32780;&#24471;&#21040;&#27010;&#29575;&#20449;&#24565;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21033;&#29992;&#36825;&#20010;&#26032;&#30340;&#39044;&#27979;&#27169;&#22411;&#30340;&#36172;&#21338;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#21487;&#20197;&#24555;&#36895;&#23398;&#20064;&#35782;&#21035;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender systems are a ubiquitous feature of online platforms. Increasingly, they are explicitly tasked with increasing users' long-term satisfaction. In this context, we study a content exploration task, which we formalize as a multi-armed bandit problem with delayed rewards. We observe that there is an apparent trade-off in choosing the learning signal: Waiting for the full reward to become available might take several weeks, hurting the rate at which learning happens, whereas measuring short-term proxy rewards reflects the actual long-term goal only imperfectly. We address this challenge in two steps. First, we develop a predictive model of delayed rewards that incorporates all information obtained to date. Full observations as well as partial (short or medium-term) outcomes are combined through a Bayesian filter to obtain a probabilistic belief. Second, we devise a bandit algorithm that takes advantage of this new predictive model. The algorithm quickly learns to identify conten
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#20351;&#29992;&#27491;&#21017;&#34920;&#36798;&#24335;&#21644;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#26469;&#24341;&#23548;&#35821;&#35328;&#27169;&#22411;&#25991;&#26412;&#29983;&#25104;&#30340;&#39640;&#25928;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.09702</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;LLM&#24341;&#23548;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Efficient Guided Generation for LLMs. (arXiv:2307.09702v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09702
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#20351;&#29992;&#27491;&#21017;&#34920;&#36798;&#24335;&#21644;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#26469;&#24341;&#23548;&#35821;&#35328;&#27169;&#22411;&#25991;&#26412;&#29983;&#25104;&#30340;&#39640;&#25928;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#20351;&#29992;&#27491;&#21017;&#34920;&#36798;&#24335;&#21644;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#26469;&#24341;&#23548;&#35821;&#35328;&#27169;&#22411;&#25991;&#26412;&#29983;&#25104;&#30340;&#39640;&#25928;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26631;&#35760;&#24207;&#21015;&#29983;&#25104;&#36807;&#31243;&#20013;&#20960;&#20046;&#19981;&#22686;&#21152;&#20219;&#20309;&#24320;&#38144;&#65292;&#24182;&#20351;&#24471;&#24341;&#23548;&#29983;&#25104;&#22312;&#23454;&#38469;&#20013;&#21487;&#34892;&#12290;&#22312;&#24320;&#28304;Python&#24211;Outlines&#20013;&#25552;&#20379;&#20102;&#19968;&#20010;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this article we describe an efficient approach to guiding language model text generation with regular expressions and context-free grammars. Our approach adds little to no overhead to the token sequence generation process, and makes guided generation feasible in practice. An implementation is provided in the open source Python library Outlines.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35270;&#35282;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#22810;&#21464;&#37327;&#36890;&#36947;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#20043;&#38388;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#12290;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#65292;&#32467;&#21512;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#21644;TS2Vec&#25439;&#22833;&#65292;&#35813;&#26041;&#27861;&#22312;&#22823;&#22810;&#25968;&#35774;&#32622;&#19979;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.09614</link><description>&lt;p&gt;
&#22810;&#35270;&#35282;&#33258;&#30417;&#30563;&#23398;&#20064;&#29992;&#20110;&#22810;&#21464;&#37327;&#36890;&#36947;&#26102;&#38388;&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
Multi-view self-supervised learning for multivariate variable-channel time series. (arXiv:2307.09614v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09614
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35270;&#35282;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#22810;&#21464;&#37327;&#36890;&#36947;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#20043;&#38388;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#12290;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#65292;&#32467;&#21512;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#21644;TS2Vec&#25439;&#22833;&#65292;&#35813;&#26041;&#27861;&#22312;&#22823;&#22810;&#25968;&#35774;&#32622;&#19979;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#22810;&#21464;&#37327;&#29983;&#29289;&#21307;&#23398;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#26631;&#27880;&#26159;&#19968;&#39033;&#32321;&#37325;&#21644;&#26114;&#36149;&#30340;&#20219;&#21153;&#12290;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#36890;&#36807;&#23545;&#26410;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#26469;&#20943;&#23569;&#23545;&#22823;&#22411;&#26631;&#35760;&#25968;&#25454;&#38598;&#30340;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#36755;&#20837;&#36890;&#36947;&#30340;&#38598;&#21512;&#22312;&#19981;&#21516;&#24212;&#29992;&#20043;&#38388;&#36890;&#24120;&#20250;&#26377;&#25152;&#21464;&#21270;&#65292;&#32780;&#22823;&#22810;&#25968;&#29616;&#26377;&#24037;&#20316;&#24182;&#19981;&#20801;&#35768;&#22312;&#20855;&#26377;&#19981;&#21516;&#36755;&#20837;&#36890;&#36947;&#38598;&#21512;&#30340;&#25968;&#25454;&#38598;&#20043;&#38388;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#19968;&#31181;&#32534;&#30721;&#22120;&#26469;&#20998;&#21035;&#22788;&#29702;&#25152;&#26377;&#36755;&#20837;&#36890;&#36947;&#30340;&#26041;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#22312;&#36890;&#36947;&#20043;&#38388;&#25552;&#21462;&#21333;&#19968;&#34920;&#31034;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#19968;&#20010;&#20855;&#26377;&#20845;&#20010;&#33041;&#30005;&#22270;&#36890;&#36947;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#22312;&#19968;&#20010;&#20855;&#26377;&#20004;&#20010;&#19981;&#21516;&#33041;&#30005;&#22270;&#36890;&#36947;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#26469;&#23637;&#31034;&#36825;&#31181;&#26041;&#27861;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#20855;&#26377;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#21644;&#19981;&#20855;&#26377;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#30340;&#32593;&#32476;&#22312;&#19981;&#21516;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#19979;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;TS2Vec&#25439;&#22833;&#22312;&#22823;&#22810;&#25968;&#35774;&#32622;&#20013;&#30340;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#25152;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Labeling of multivariate biomedical time series data is a laborious and expensive process. Self-supervised contrastive learning alleviates the need for large, labeled datasets through pretraining on unlabeled data. However, for multivariate time series data the set of input channels often varies between applications, and most existing work does not allow for transfer between datasets with different sets of input channels. We propose learning one encoder to operate on all input channels individually. We then use a message passing neural network to extract a single representation across channels. We demonstrate the potential of this method by pretraining our network on a dataset with six EEG channels and finetuning on a dataset with two different EEG channels. We compare networks with and without the message passing neural network across different contrastive loss functions. We show that our method combined with the TS2Vec loss outperforms all other methods in most settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TRADYN&#30340;&#27010;&#29575;&#22320;&#24418;&#21644;&#26426;&#22120;&#20154;&#24863;&#30693;&#21069;&#21521;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#33021;&#22815;&#36866;&#24212;&#22312;&#33258;&#20027;&#23548;&#33322;&#29615;&#22659;&#20013;&#30340;&#22320;&#24418;&#21644;&#26426;&#22120;&#20154;&#30340;&#21464;&#21270;&#65292;&#36890;&#36807;&#22312;&#27169;&#25311;&#30340;&#20108;&#32500;&#23548;&#33322;&#29615;&#22659;&#20013;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#38271;&#35270;&#31243;&#36712;&#36857;&#39044;&#27979;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#36739;&#20302;&#30340;&#39044;&#27979;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2307.09206</link><description>&lt;p&gt;
&#24102;&#26377;&#22522;&#20110;&#23398;&#20064;&#30340;&#22320;&#24418;&#21644;&#26426;&#22120;&#20154;&#24863;&#30693;&#21160;&#21147;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#26465;&#20214;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Context-Conditional Navigation with a Learning-Based Terrain- and Robot-Aware Dynamics Model. (arXiv:2307.09206v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09206
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TRADYN&#30340;&#27010;&#29575;&#22320;&#24418;&#21644;&#26426;&#22120;&#20154;&#24863;&#30693;&#21069;&#21521;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#33021;&#22815;&#36866;&#24212;&#22312;&#33258;&#20027;&#23548;&#33322;&#29615;&#22659;&#20013;&#30340;&#22320;&#24418;&#21644;&#26426;&#22120;&#20154;&#30340;&#21464;&#21270;&#65292;&#36890;&#36807;&#22312;&#27169;&#25311;&#30340;&#20108;&#32500;&#23548;&#33322;&#29615;&#22659;&#20013;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#38271;&#35270;&#31243;&#36712;&#36857;&#39044;&#27979;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#36739;&#20302;&#30340;&#39044;&#27979;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#20027;&#23548;&#33322;&#29615;&#22659;&#20013;&#65292;&#22810;&#20010;&#21442;&#25968;&#21487;&#33021;&#20250;&#21457;&#29983;&#21464;&#21270;&#12290;&#22320;&#24418;&#29305;&#24615;&#22914;&#25705;&#25830;&#31995;&#25968;&#21487;&#33021;&#20250;&#26681;&#25454;&#26426;&#22120;&#20154;&#30340;&#20301;&#32622;&#32780;&#38543;&#26102;&#38388;&#21464;&#21270;&#12290;&#27492;&#22806;&#65292;&#26426;&#22120;&#20154;&#30340;&#21160;&#21147;&#23398;&#21487;&#33021;&#20250;&#22240;&#19981;&#21516;&#36127;&#36733;&#12289;&#31995;&#32479;&#36136;&#37327;&#21464;&#21270;&#12289;&#30952;&#25439;&#31561;&#21407;&#22240;&#32780;&#21457;&#29983;&#21464;&#21270;&#65292;&#20174;&#32780;&#25913;&#21464;&#25191;&#34892;&#22120;&#22686;&#30410;&#25110;&#20851;&#33410;&#25705;&#25830;&#21147;&#12290;&#33258;&#20027;&#20195;&#29702;&#24212;&#35813;&#33021;&#22815;&#36866;&#24212;&#36825;&#20123;&#21464;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27010;&#29575;&#22320;&#24418;&#21644;&#26426;&#22120;&#20154;&#24863;&#30693;&#21069;&#21521;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#31216;&#20026;TRADYN&#65292;&#23427;&#33021;&#22815;&#36866;&#24212;&#19978;&#36848;&#21464;&#21270;&#12290;&#23427;&#22522;&#20110;&#22522;&#20110;&#31070;&#32463;&#36807;&#31243;&#30340;&#20803;&#23398;&#20064;&#21069;&#21521;&#21160;&#21147;&#23398;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#30340;&#20108;&#32500;&#23548;&#33322;&#29615;&#22659;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#20102;&#19968;&#20010;&#31867;&#20284;&#33258;&#34892;&#36710;&#30340;&#26426;&#22120;&#20154;&#21644;&#20855;&#26377;&#31354;&#38388;&#21464;&#21270;&#25705;&#25830;&#31995;&#25968;&#30340;&#19981;&#21516;&#22320;&#24418;&#24067;&#23616;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#19982;&#38750;&#33258;&#36866;&#24212;&#26041;&#27861;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#38271;&#35270;&#31243;&#36712;&#36857;&#39044;&#27979;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#36739;&#20302;&#30340;&#39044;&#27979;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
In autonomous navigation settings, several quantities can be subject to variations. Terrain properties such as friction coefficients may vary over time depending on the location of the robot. Also, the dynamics of the robot may change due to, e.g., different payloads, changing the system's mass, or wear and tear, changing actuator gains or joint friction. An autonomous agent should thus be able to adapt to such variations. In this paper, we develop a novel probabilistic, terrain- and robot-aware forward dynamics model, termed TRADYN, which is able to adapt to the above-mentioned variations. It builds on recent advances in meta-learning forward dynamics models based on Neural Processes. We evaluate our method in a simulated 2D navigation setting with a unicycle-like robot and different terrain layouts with spatially varying friction coefficients. In our experiments, the proposed model exhibits lower prediction error for the task of long-horizon trajectory prediction, compared to non-ada
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#20581;&#24247;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;HeLM&#65289;&#65292;&#36890;&#36807;&#23398;&#20064;&#22797;&#26434;&#25968;&#25454;&#27169;&#24577;&#30340;&#32534;&#30721;&#22120;&#65292;&#21516;&#26102;&#25903;&#25345;&#31616;&#21333;&#27169;&#24577;&#25968;&#25454;&#30340;&#25991;&#26412;&#24207;&#21015;&#21270;&#65292;HeLM&#21487;&#20197;&#26377;&#25928;&#22320;&#20351;&#29992;&#20010;&#20307;&#19987;&#23646;&#25968;&#25454;&#20272;&#35745;&#30142;&#30149;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2307.09018</link><description>&lt;p&gt;
&#22522;&#20110;&#20010;&#20307;&#19987;&#23646;&#25968;&#25454;&#30340;&#22810;&#27169;&#24577;&#20581;&#24247;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Multimodal LLMs for health grounded in individual-specific data. (arXiv:2307.09018v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09018
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#20581;&#24247;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;HeLM&#65289;&#65292;&#36890;&#36807;&#23398;&#20064;&#22797;&#26434;&#25968;&#25454;&#27169;&#24577;&#30340;&#32534;&#30721;&#22120;&#65292;&#21516;&#26102;&#25903;&#25345;&#31616;&#21333;&#27169;&#24577;&#25968;&#25454;&#30340;&#25991;&#26412;&#24207;&#21015;&#21270;&#65292;HeLM&#21487;&#20197;&#26377;&#25928;&#22320;&#20351;&#29992;&#20010;&#20307;&#19987;&#23646;&#25968;&#25454;&#20272;&#35745;&#30142;&#30149;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20010;&#20307;&#19987;&#23646;&#25968;&#25454;&#30340;&#20581;&#24247;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#20010;&#24615;&#21270;&#20581;&#24247;&#38382;&#39064;&#65292;&#20294;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;LLMs&#38656;&#35201;&#20855;&#22791;&#25668;&#20837;&#19982;&#20010;&#20307;&#20581;&#24247;&#29366;&#20917;&#30456;&#20851;&#30340;&#22810;&#26679;&#21270;&#25968;&#25454;&#27169;&#24577;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65288;HeLM&#65306;&#20581;&#24247;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22810;&#27169;&#24577;&#29702;&#35299;&#65289;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#32534;&#30721;&#22120;&#65292;&#23558;&#22797;&#26434;&#30340;&#25968;&#25454;&#27169;&#24577;&#26144;&#23556;&#21040;LLMs&#30340;&#20196;&#29260;&#23884;&#20837;&#31354;&#38388;&#65292;&#24182;&#23558;&#31616;&#21333;&#30340;&#27169;&#24577;&#22914;&#34920;&#26684;&#25968;&#25454;&#24207;&#21015;&#21270;&#20026;&#25991;&#26412;&#65292;&#23454;&#29616;&#20102;&#22522;&#20110;&#20010;&#20307;&#19987;&#23646;&#25968;&#25454;&#30340;&#22810;&#27169;&#24577;LLMs&#12290;&#36890;&#36807;&#20351;&#29992;&#26469;&#33258;&#33521;&#22269;&#29983;&#29289;&#24211;&#30340;&#25968;&#25454;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;HeLM&#33021;&#22815;&#26377;&#25928;&#22320;&#21033;&#29992;&#20154;&#21475;&#32479;&#35745;&#23398;&#21644;&#20020;&#24202;&#29305;&#24449;&#65292;&#20197;&#21450;&#39640;&#32500;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#26469;&#20272;&#35745;&#30142;&#30149;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation large language models (LLMs) have shown an impressive ability to solve tasks across a wide range of fields including health. To effectively solve personalized health tasks, LLMs need the ability to ingest a diversity of data modalities that are relevant to an individual's health status. In this paper, we take a step towards creating multimodal LLMs for health that are grounded in individual-specific data by developing a framework (HeLM: Health Large Language Model for Multimodal Understanding) that enables LLMs to use high-dimensional clinical modalities to estimate underlying disease risk. HeLM encodes complex data modalities by learning an encoder that maps them into the LLM's token embedding space and for simple modalities like tabular data by serializing the data into text. Using data from the UK Biobank, we show that HeLM can effectively use demographic and clinical features in addition to high-dimensional time-series data to estimate disease risk. For example, HeLM ach
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#22312;&#39640;&#38647;&#35834;&#25968;&#19979;&#21512;&#25104;&#25289;&#26684;&#26391;&#26085;&#28237;&#27969;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#31890;&#23376;&#36712;&#36857;&#30340;&#32479;&#35745;&#21644;&#25299;&#25169;&#24615;&#36136;&#30340;&#20934;&#30830;&#37325;&#29616;&#12290;</title><link>http://arxiv.org/abs/2307.08529</link><description>&lt;p&gt;
&#36890;&#36807;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#30340;&#21512;&#25104;&#25289;&#26684;&#26391;&#26085;&#28237;&#27969;
&lt;/p&gt;
&lt;p&gt;
Synthetic Lagrangian Turbulence by Generative Diffusion Models. (arXiv:2307.08529v1 [physics.flu-dyn] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08529
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#22312;&#39640;&#38647;&#35834;&#25968;&#19979;&#21512;&#25104;&#25289;&#26684;&#26391;&#26085;&#28237;&#27969;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#31890;&#23376;&#36712;&#36857;&#30340;&#32479;&#35745;&#21644;&#25299;&#25169;&#24615;&#36136;&#30340;&#20934;&#30830;&#37325;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25289;&#26684;&#26391;&#26085;&#28237;&#27969;&#26159;&#28041;&#21450;&#24037;&#31243;&#12289;&#29983;&#29289;&#27969;&#20307;&#12289;&#22823;&#27668;&#12289;&#28023;&#27915;&#21644;&#22825;&#20307;&#29289;&#29702;&#39046;&#22495;&#20013;&#30340;&#20998;&#25955;&#21644;&#28151;&#21512;&#29289;&#29702;&#30340;&#24212;&#29992;&#21644;&#22522;&#30784;&#24615;&#38382;&#39064;&#12290;&#23613;&#31649;&#36807;&#21435;&#19977;&#21313;&#24180;&#36827;&#34892;&#20102;&#21331;&#36234;&#30340;&#29702;&#35770;&#12289;&#25968;&#20540;&#21644;&#23454;&#39564;&#30740;&#31350;&#65292;&#20294;&#27809;&#26377;&#29616;&#26377;&#27169;&#22411;&#33021;&#22815;&#24544;&#23454;&#22320;&#37325;&#29616;&#28237;&#27969;&#20013;&#30340;&#31890;&#23376;&#36712;&#36857;&#25152;&#23637;&#31034;&#30340;&#32479;&#35745;&#21644;&#25299;&#25169;&#24615;&#36136;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#20808;&#36827;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#22312;&#39640;&#38647;&#35834;&#25968;&#19979;&#29983;&#25104;&#19977;&#32500;&#28237;&#27969;&#20013;&#30340;&#21333;&#31890;&#23376;&#36712;&#36857;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#33719;&#21462;&#21487;&#38752;&#30340;&#25289;&#26684;&#26391;&#26085;&#25968;&#25454;&#25152;&#38656;&#30340;&#30452;&#25509;&#25968;&#20540;&#27169;&#25311;&#25110;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#34920;&#26126;&#65292;&#23427;&#33021;&#22815;&#23450;&#37327;&#22320;&#37325;&#29616;&#25972;&#20010;&#26102;&#38388;&#23610;&#24230;&#33539;&#22260;&#20869;&#30340;&#25152;&#26377;&#30456;&#20851;&#32479;&#35745;&#22522;&#20934;&#65292;&#21253;&#25324;&#36895;&#24230;&#22686;&#37327;&#30340;&#23614;&#37096;&#20998;&#24067;&#12289;&#24322;&#24120;&#24130;&#24459;&#21644;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Lagrangian turbulence lies at the core of numerous applied and fundamental problems related to the physics of dispersion and mixing in engineering, bio-fluids, atmosphere, oceans, and astrophysics. Despite exceptional theoretical, numerical, and experimental efforts conducted over the past thirty years, no existing models are capable of faithfully reproducing statistical and topological properties exhibited by particle trajectories in turbulence. We propose a machine learning approach, based on a state-of-the-art Diffusion Model, to generate single-particle trajectories in three-dimensional turbulence at high Reynolds numbers, thereby bypassing the need for direct numerical simulations or experiments to obtain reliable Lagrangian data. Our model demonstrates the ability to quantitatively reproduce all relevant statistical benchmarks over the entire range of time scales, including the presence of fat tails distribution for the velocity increments, anomalous power law, and enhancement of
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20999;&#32447;&#27880;&#24847;&#24494;&#35843;&#26041;&#27861;&#65288;TAFT&#65289;&#65292;&#36890;&#36807;&#32447;&#24615;&#21270;&#21464;&#21387;&#22120;&#30340;&#19968;&#38454;&#27888;&#21202;&#23637;&#24320;&#26469;&#36827;&#34892;&#24494;&#35843;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#19982;&#21407;&#22987;&#38750;&#32447;&#24615;&#32593;&#32476;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#27169;&#22411;&#32452;&#21512;&#12289;&#24182;&#34892;&#35757;&#32451;&#12289;&#26426;&#22120;&#21435;&#38500;&#21644;&#24046;&#20998;&#38544;&#31169;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2307.08122</link><description>&lt;p&gt;
&#20999;&#32447;&#21464;&#25442;&#22120;&#29992;&#20110;&#32452;&#21512;&#12289;&#38544;&#31169;&#21644;&#21435;&#38500;
&lt;/p&gt;
&lt;p&gt;
Tangent Transformers for Composition, Privacy and Removal. (arXiv:2307.08122v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08122
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20999;&#32447;&#27880;&#24847;&#24494;&#35843;&#26041;&#27861;&#65288;TAFT&#65289;&#65292;&#36890;&#36807;&#32447;&#24615;&#21270;&#21464;&#21387;&#22120;&#30340;&#19968;&#38454;&#27888;&#21202;&#23637;&#24320;&#26469;&#36827;&#34892;&#24494;&#35843;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#19982;&#21407;&#22987;&#38750;&#32447;&#24615;&#32593;&#32476;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#27169;&#22411;&#32452;&#21512;&#12289;&#24182;&#34892;&#35757;&#32451;&#12289;&#26426;&#22120;&#21435;&#38500;&#21644;&#24046;&#20998;&#38544;&#31169;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#20999;&#32447;&#20851;&#27880;&#24494;&#35843;&#65288;TAFT&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#21021;&#22987;&#21270;&#28857;&#21608;&#22260;&#35745;&#31639;&#19968;&#38454;&#27888;&#21202;&#23637;&#24320;&#26469;&#33719;&#24471;&#32447;&#24615;&#21270;&#21464;&#21387;&#22120;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20174;&#32447;&#24615;&#21270;&#24471;&#21040;&#30340;&#38597;&#21487;&#27604;&#30697;&#38453;-&#21521;&#37327;&#31215;&#21487;&#20197;&#22312;&#21333;&#20010;&#21069;&#21521;&#20256;&#36882;&#20013;&#39640;&#25928;&#35745;&#31639;&#65292;&#23558;&#35757;&#32451;&#21644;&#25512;&#26029;&#25104;&#26412;&#38477;&#20302;&#21040;&#19982;&#21407;&#22987;&#38750;&#32447;&#24615;&#27169;&#22411;&#30456;&#21516;&#25968;&#37327;&#32423;&#65292;&#24182;&#19988;&#20351;&#29992;&#30456;&#21516;&#25968;&#37327;&#30340;&#21442;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#24403;&#24212;&#29992;&#20110;&#21508;&#31181;&#19979;&#28216;&#35270;&#35273;&#20998;&#31867;&#20219;&#21153;&#26102;&#65292;&#36890;&#36807;TAFT&#36827;&#34892;&#24494;&#35843;&#30340;&#32467;&#26524;&#20999;&#32447;&#21464;&#25442;&#22120;&#21487;&#20197;&#19982;&#23545;&#21407;&#22987;&#38750;&#32447;&#24615;&#32593;&#32476;&#36827;&#34892;&#24494;&#35843;&#30456;&#24403;&#12290;&#30001;&#20110;&#20999;&#32447;&#21464;&#25442;&#22120;&#23545;&#20110;&#26032;&#30340;&#26435;&#20540;&#26159;&#32447;&#24615;&#30340;&#65292;&#24182;&#19988;&#32467;&#26524;&#24494;&#35843;&#25439;&#22833;&#26159;&#20984;&#30340;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#30456;&#27604;&#20110;&#38750;&#32447;&#24615;&#24494;&#35843;&#65292;TAFT&#22312;&#27169;&#22411;&#32452;&#21512;&#12289;&#24182;&#34892;&#35757;&#32451;&#12289;&#26426;&#22120;&#21435;&#38500;&#21644;&#24046;&#20998;&#38544;&#31169;&#26041;&#38754;&#20855;&#26377;&#20960;&#20010;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Tangent Attention Fine-Tuning (TAFT), a method for fine-tuning linearized transformers obtained by computing a First-order Taylor Expansion around a pre-trained initialization. We show that the Jacobian-Vector Product resulting from linearization can be computed efficiently in a single forward pass, reducing training and inference cost to the same order of magnitude as its original non-linear counterpart, while using the same number of parameters. Furthermore, we show that, when applied to various downstream visual classification tasks, the resulting Tangent Transformer fine-tuned with TAFT can perform comparably with fine-tuning the original non-linear network. Since Tangent Transformers are linear with respect to the new set of weights, and the resulting fine-tuning loss is convex, we show that TAFT enjoys several advantages compared to non-linear fine-tuning when it comes to model composition, parallel training, machine unlearning, and differential privacy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#27010;&#29575;&#31574;&#30053;&#25191;&#34892;&#19981;&#30830;&#23450;&#24615;&#30340;&#34892;&#21160;&#40065;&#26834;&#22686;&#24378;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;ARRLC&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#36951;&#25022;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#19978;&#36798;&#21040;&#20102;&#26497;&#23567;&#26497;&#22823;&#26368;&#20248;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#20110;&#38750;&#40065;&#26834;&#31639;&#27861;&#24182;&#19988;&#25910;&#25947;&#26356;&#24555;&#12290;</title><link>http://arxiv.org/abs/2307.07666</link><description>&lt;p&gt;
&#20855;&#26377;&#27010;&#29575;&#31574;&#30053;&#25191;&#34892;&#19981;&#30830;&#23450;&#24615;&#30340;&#39640;&#25928;&#40065;&#26834;&#22686;&#24378;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Efficient Action Robust Reinforcement Learning with Probabilistic Policy Execution Uncertainty. (arXiv:2307.07666v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07666
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#27010;&#29575;&#31574;&#30053;&#25191;&#34892;&#19981;&#30830;&#23450;&#24615;&#30340;&#34892;&#21160;&#40065;&#26834;&#22686;&#24378;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;ARRLC&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#36951;&#25022;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#19978;&#36798;&#21040;&#20102;&#26497;&#23567;&#26497;&#22823;&#26368;&#20248;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#20110;&#38750;&#40065;&#26834;&#31639;&#27861;&#24182;&#19988;&#25910;&#25947;&#26356;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40065;&#26834;&#22686;&#24378;&#23398;&#20064;&#26088;&#22312;&#22312;&#19981;&#30830;&#23450;&#24615;&#38754;&#21069;&#25214;&#21040;&#20248;&#21270;&#26368;&#22351;&#24773;&#20917;&#19979;&#24615;&#33021;&#30340;&#31574;&#30053;&#12290;&#26412;&#25991;&#20851;&#27880;&#20855;&#26377;&#27010;&#29575;&#31574;&#30053;&#25191;&#34892;&#19981;&#30830;&#23450;&#24615;&#30340;&#34892;&#21160;&#40065;&#26834;&#22686;&#24378;&#23398;&#20064;&#65292;&#20854;&#20013;&#20195;&#29702;&#26426;&#22120;&#19981;&#24635;&#26159;&#25353;&#29031;&#31574;&#30053;&#25351;&#23450;&#30340;&#21160;&#20316;&#36827;&#34892;&#65292;&#32780;&#26159;&#20197;&#27010;&#29575;$1-\rho$&#25191;&#34892;&#31574;&#30053;&#25351;&#23450;&#30340;&#21160;&#20316;&#65292;&#20197;&#27010;&#29575;$\rho$&#25191;&#34892;&#26367;&#20195;&#30340;&#23545;&#25239;&#21160;&#20316;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20855;&#26377;&#27010;&#29575;&#31574;&#30053;&#25191;&#34892;&#19981;&#30830;&#23450;&#24615;&#30340;&#34892;&#21160;&#40065;&#26834;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#23384;&#22312;&#26368;&#20248;&#31574;&#30053;&#65292;&#24182;&#25552;&#20379;&#20102;&#35299;&#20915;&#20854;&#30340;&#34892;&#21160;&#40065;&#26834;&#36125;&#23572;&#26364;&#26368;&#20248;&#26041;&#31243;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20855;&#26377;&#35777;&#20070;&#30340;&#34892;&#21160;&#40065;&#26834;&#22686;&#24378;&#23398;&#20064;(ARRLC)&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#23454;&#29616;&#20102;&#26497;&#23567;&#26497;&#22823;&#36951;&#25022;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#26368;&#20248;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#25968;&#20540;&#23454;&#39564;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;ARRLC&#20248;&#20110;&#38750;&#40065;&#26834;&#22686;&#24378;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#19988;&#27604;&#40065;&#26834;TD&#31639;&#27861;&#25910;&#25947;&#26356;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robust reinforcement learning (RL) aims to find a policy that optimizes the worst-case performance in the face of uncertainties. In this paper, we focus on action robust RL with the probabilistic policy execution uncertainty, in which, instead of always carrying out the action specified by the policy, the agent will take the action specified by the policy with probability $1-\rho$ and an alternative adversarial action with probability $\rho$. We establish the existence of an optimal policy on the action robust MDPs with probabilistic policy execution uncertainty and provide the action robust Bellman optimality equation for its solution. Furthermore, we develop Action Robust Reinforcement Learning with Certificates (ARRLC) algorithm that achieves minimax optimal regret and sample complexity. Furthermore, we conduct numerical experiments to validate our approach's robustness, demonstrating that ARRLC outperforms non-robust RL algorithms and converges faster than the robust TD algorithm i
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31283;&#20581;&#30340;&#20307;&#31215;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#30340;&#39057;&#22495;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#20854;&#30456;&#23545;&#20110;&#20256;&#32479;&#25915;&#20987;&#26041;&#27861;&#20855;&#26377;&#20248;&#21183;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#39057;&#22495;&#23545;&#25239;&#25915;&#20987;&#65292;&#36890;&#36807;&#24341;&#20837;&#39057;&#29575;&#19968;&#33268;&#24615;&#25439;&#22833;&#26469;&#20248;&#21270;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#23454;&#29616;&#23545;&#20307;&#32032;&#21644;&#39057;&#22495;&#25915;&#20987;&#30340;&#38450;&#24481;&#12290;</title><link>http://arxiv.org/abs/2307.07269</link><description>&lt;p&gt;
&#39057;&#22495;&#23545;&#25239;&#35757;&#32451;&#29992;&#20110;&#31283;&#20581;&#30340;&#20307;&#31215;&#21307;&#23398;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Frequency Domain Adversarial Training for Robust Volumetric Medical Segmentation. (arXiv:2307.07269v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07269
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31283;&#20581;&#30340;&#20307;&#31215;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#30340;&#39057;&#22495;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#20854;&#30456;&#23545;&#20110;&#20256;&#32479;&#25915;&#20987;&#26041;&#27861;&#20855;&#26377;&#20248;&#21183;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#39057;&#22495;&#23545;&#25239;&#25915;&#20987;&#65292;&#36890;&#36807;&#24341;&#20837;&#39057;&#29575;&#19968;&#33268;&#24615;&#25439;&#22833;&#26469;&#20248;&#21270;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#23454;&#29616;&#23545;&#20307;&#32032;&#21644;&#39057;&#22495;&#25915;&#20987;&#30340;&#38450;&#24481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20851;&#38190;&#24212;&#29992;&#65288;&#22914;&#21307;&#30103;&#20445;&#20581;&#65289;&#20013;&#65292;&#30830;&#20445;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#26368;&#36817;&#22312;&#20307;&#31215;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#20854;&#26131;&#21463;&#23545;&#25239;&#25915;&#20987;&#30340;&#24369;&#28857;&#65292;&#36825;&#20123;&#27169;&#22411;&#26080;&#27861;&#31435;&#21363;&#29992;&#20110;&#23454;&#38469;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20307;&#31215;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#30340;&#19977;&#32500;&#39057;&#22495;&#23545;&#25239;&#25915;&#20987;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#30456;&#23545;&#20110;&#20256;&#32479;&#36755;&#20837;&#25110;&#20307;&#32032;&#22495;&#25915;&#20987;&#30340;&#20248;&#21183;&#12290;&#21033;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#25915;&#20987;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39057;&#22495;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#65292;&#20197;&#20248;&#21270;&#38024;&#23545;&#20307;&#32032;&#21644;&#39057;&#22495;&#25915;&#20987;&#30340;&#31283;&#20581;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#39057;&#29575;&#19968;&#33268;&#24615;&#25439;&#22833;&#26469;&#35843;&#33410;&#25105;&#20204;&#30340;&#39057;&#22495;&#23545;&#25239;&#35757;&#32451;&#65292;&#20197;&#22312;&#28165;&#27905;&#26679;&#21697;&#21644;&#23545;&#25239;&#26679;&#26412;&#20043;&#38388;&#21462;&#24471;&#26356;&#22909;&#30340;&#24179;&#34913;&#12290;&#20195;&#30721;&#21487;&#20844;&#24320;&#35775;&#38382;https://github.com/asif-hanif/vafa&#12290;
&lt;/p&gt;
&lt;p&gt;
It is imperative to ensure the robustness of deep learning models in critical applications such as, healthcare. While recent advances in deep learning have improved the performance of volumetric medical image segmentation models, these models cannot be deployed for real-world applications immediately due to their vulnerability to adversarial attacks. We present a 3D frequency domain adversarial attack for volumetric medical image segmentation models and demonstrate its advantages over conventional input or voxel domain attacks. Using our proposed attack, we introduce a novel frequency domain adversarial training approach for optimizing a robust model against voxel and frequency domain attacks. Moreover, we propose frequency consistency loss to regulate our frequency domain adversarial training that achieves a better tradeoff between model's performance on clean and adversarial samples. Code is publicly available at https://github.com/asif-hanif/vafa.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;&#22522;&#20110;Wasserstein&#37327;&#23376;&#33945;&#29305;&#21345;&#27931;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#37327;&#23376;&#22810;&#20307;Schr\"odinger&#26041;&#31243;&#12290;&#35813;&#26041;&#27861;&#37325;&#26032;&#21046;&#23450;&#20102;&#33021;&#37327;&#27867;&#20989;&#30340;&#26368;&#23567;&#21270;&#38382;&#39064;&#65292;&#24182;&#23558;&#20854;&#36716;&#21270;&#20026;Born&#20998;&#24067;&#31354;&#38388;&#20013;&#30340;&#31890;&#23376;&#25490;&#21015;&#65288;&#21453;&#65289;&#23545;&#31216;&#27874;&#20989;&#25968;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#34920;&#31034;&#20016;&#23500;&#30340;&#27874;&#20989;&#25968;&#26063;&#12290;</title><link>http://arxiv.org/abs/2307.07050</link><description>&lt;p&gt;
Wasserstein&#37327;&#23376;&#33945;&#29305;&#21345;&#27931;&#65306;&#35299;&#20915;&#37327;&#23376;&#22810;&#20307;Schr\"odinger&#26041;&#31243;&#30340;&#19968;&#31181;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Wasserstein Quantum Monte Carlo: A Novel Approach for Solving the Quantum Many-Body Schr\"odinger Equation. (arXiv:2307.07050v1 [physics.comp-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07050
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;&#22522;&#20110;Wasserstein&#37327;&#23376;&#33945;&#29305;&#21345;&#27931;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#37327;&#23376;&#22810;&#20307;Schr\"odinger&#26041;&#31243;&#12290;&#35813;&#26041;&#27861;&#37325;&#26032;&#21046;&#23450;&#20102;&#33021;&#37327;&#27867;&#20989;&#30340;&#26368;&#23567;&#21270;&#38382;&#39064;&#65292;&#24182;&#23558;&#20854;&#36716;&#21270;&#20026;Born&#20998;&#24067;&#31354;&#38388;&#20013;&#30340;&#31890;&#23376;&#25490;&#21015;&#65288;&#21453;&#65289;&#23545;&#31216;&#27874;&#20989;&#25968;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#34920;&#31034;&#20016;&#23500;&#30340;&#27874;&#20989;&#25968;&#26063;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#37327;&#23376;&#22810;&#20307;Schr\"odinger&#26041;&#31243;&#26159;&#37327;&#23376;&#29289;&#29702;&#12289;&#37327;&#23376;&#21270;&#23398;&#21644;&#26448;&#26009;&#31185;&#23398;&#39046;&#22495;&#20013;&#19968;&#20010;&#22522;&#26412;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#19968;&#31181;&#24120;&#35265;&#30340;&#35745;&#31639;&#26041;&#27861;&#26159;&#37327;&#23376;&#21464;&#20998;&#33945;&#29305;&#21345;&#27931;&#65288;QVMC&#65289;&#65292;&#20854;&#20013;&#36890;&#36807;&#22312;&#19968;&#20010;&#21442;&#25968;&#21270;&#27874;&#20989;&#25968;&#26063;&#20013;&#26368;&#23567;&#21270;&#31995;&#32479;&#30340;&#33021;&#37327;&#26469;&#33719;&#24471;&#22522;&#24577;&#35299;&#12290;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#35299;&#20915;&#20102;&#20256;&#32479;QVMC&#30340;&#23616;&#38480;&#24615;&#65292;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#20016;&#23500;&#30340;&#27874;&#20989;&#25968;&#26063;&#12290;&#28982;&#32780;&#65292;&#22312;QVMC&#20013;&#20248;&#21270;&#30446;&#26631;&#20173;&#28982;&#38590;&#20197;&#26368;&#23567;&#21270;&#65292;&#38656;&#35201;&#20351;&#29992;&#33258;&#28982;&#26799;&#24230;&#31561;&#20108;&#38454;&#20248;&#21270;&#26041;&#27861;&#12290;&#26412;&#25991;&#39318;&#20808;&#37325;&#26032;&#21046;&#23450;&#20102;&#33021;&#37327;&#27867;&#20989;&#30340;&#26368;&#23567;&#21270;&#38382;&#39064;&#65292;&#23558;&#20854;&#36716;&#21270;&#20026;Born&#20998;&#24067;&#31354;&#38388;&#20013;&#30340;&#31890;&#23376;&#25490;&#21015;&#65288;&#21453;&#65289;&#23545;&#31216;&#27874;&#20989;&#25968;&#30340;&#38382;&#39064;&#65292;&#32780;&#19981;&#26159;&#27874;&#20989;&#25968;&#30340;&#31354;&#38388;&#12290;&#28982;&#21518;&#25105;&#20204;&#23558;QVMC&#35299;&#37322;&#20026;&#22312;&#36825;&#20010;&#31354;&#38388;&#20013;&#30340;Fisher-Rao&#26799;&#24230;&#27969;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solving the quantum many-body Schr\"odinger equation is a fundamental and challenging problem in the fields of quantum physics, quantum chemistry, and material sciences. One of the common computational approaches to this problem is Quantum Variational Monte Carlo (QVMC), in which ground-state solutions are obtained by minimizing the energy of the system within a restricted family of parameterized wave functions. Deep learning methods partially address the limitations of traditional QVMC by representing a rich family of wave functions in terms of neural networks. However, the optimization objective in QVMC remains notoriously hard to minimize and requires second-order optimization methods such as natural gradient. In this paper, we first reformulate energy functional minimization in the space of Born distributions corresponding to particle-permutation (anti-)symmetric wave functions, rather than the space of wave functions. We then interpret QVMC as the Fisher--Rao gradient flow in this
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#38543;&#26426;&#39640;&#26031;&#26435;&#37325;&#21644;&#20559;&#32622;&#30340;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#24067;&#65292;&#24471;&#21040;&#20102;&#22312;&#22823;&#20294;&#26377;&#38480;&#30340; $n$ &#21644;&#20219;&#24847;&#22266;&#23450;&#32593;&#32476;&#28145;&#24230;&#19979;&#25104;&#31435;&#30340;&#27491;&#24577;&#36924;&#36817;&#30340;&#23450;&#37327;&#30028;&#38480;&#65292;&#35777;&#26126;&#20102;&#38543;&#26426;&#20840;&#36830;&#25509;&#32593;&#32476;&#19982;&#30456;&#24212;&#30340;&#26080;&#38480;&#23485;&#39640;&#26031;&#36807;&#31243;&#20043;&#38388;&#30340;&#36317;&#31163;&#25353;&#29031; $n^{-\gamma}$ &#32553;&#25918;&#65292;&#30028;&#38480;&#22312;&#32593;&#32476;&#23485;&#24230;&#30340;&#20381;&#36182;&#24615;&#26041;&#38754;&#20248;&#20110;&#20197;&#21069;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2307.06092</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#23450;&#37327;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;
&lt;/p&gt;
&lt;p&gt;
Quantitative CLTs in Deep Neural Networks. (arXiv:2307.06092v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06092
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#38543;&#26426;&#39640;&#26031;&#26435;&#37325;&#21644;&#20559;&#32622;&#30340;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#24067;&#65292;&#24471;&#21040;&#20102;&#22312;&#22823;&#20294;&#26377;&#38480;&#30340; $n$ &#21644;&#20219;&#24847;&#22266;&#23450;&#32593;&#32476;&#28145;&#24230;&#19979;&#25104;&#31435;&#30340;&#27491;&#24577;&#36924;&#36817;&#30340;&#23450;&#37327;&#30028;&#38480;&#65292;&#35777;&#26126;&#20102;&#38543;&#26426;&#20840;&#36830;&#25509;&#32593;&#32476;&#19982;&#30456;&#24212;&#30340;&#26080;&#38480;&#23485;&#39640;&#26031;&#36807;&#31243;&#20043;&#38388;&#30340;&#36317;&#31163;&#25353;&#29031; $n^{-\gamma}$ &#32553;&#25918;&#65292;&#30028;&#38480;&#22312;&#32593;&#32476;&#23485;&#24230;&#30340;&#20381;&#36182;&#24615;&#26041;&#38754;&#20248;&#20110;&#20197;&#21069;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#38543;&#26426;&#39640;&#26031;&#26435;&#37325;&#21644;&#20559;&#32622;&#30340;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#24067;&#65292;&#20854;&#20013;&#38544;&#34255;&#23618;&#23485;&#24230;&#19982;&#22823;&#24120;&#25968; $n$ &#25104;&#27604;&#20363;&#12290;&#22312;&#38750;&#32447;&#24615;&#30340;&#28201;&#21644;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#22312;&#22823;&#20294;&#26377;&#38480;&#30340; $n$ &#21644;&#20219;&#24847;&#22266;&#23450;&#32593;&#32476;&#28145;&#24230;&#19979;&#25104;&#31435;&#30340;&#27491;&#24577;&#36924;&#36817;&#30340;&#23450;&#37327;&#30028;&#38480;&#12290;&#25105;&#20204;&#30340;&#23450;&#29702;&#34920;&#26126;&#65292;&#26080;&#35770;&#26159;&#23545;&#20110;&#26377;&#38480;&#32500;&#20998;&#24067;&#36824;&#26159;&#25972;&#20010;&#36807;&#31243;&#65292;&#38543;&#26426;&#20840;&#36830;&#25509;&#32593;&#32476;&#65288;&#21450;&#20854;&#23548;&#25968;&#65289;&#19982;&#30456;&#24212;&#30340;&#26080;&#38480;&#23485;&#39640;&#26031;&#36807;&#31243;&#20043;&#38388;&#30340;&#36317;&#31163;&#37117;&#20250;&#25353;&#29031; $n^{-\gamma}$ &#32553;&#25918;&#65292;&#20854;&#20013; $\gamma&gt;0$&#65292;&#25351;&#25968;&#21462;&#20915;&#20110;&#29992;&#20110;&#24230;&#37327;&#24046;&#24322;&#30340;&#24230;&#37327;&#26041;&#24335;&#12290;&#25105;&#20204;&#30340;&#30028;&#38480;&#22312;&#32593;&#32476;&#23485;&#24230;&#30340;&#20381;&#36182;&#24615;&#26041;&#38754;&#27604;&#25991;&#29486;&#20013;&#20197;&#21069;&#25552;&#20379;&#30340;&#20219;&#20309;&#30028;&#38480;&#37117;&#35201;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the distribution of a fully connected neural network with random Gaussian weights and biases in which the hidden layer widths are proportional to a large constant $n$. Under mild assumptions on the non-linearity, we obtain quantitative bounds on normal approximations valid at large but finite $n$ and any fixed network depth. Our theorems show, both for the finite-dimensional distributions and the entire process, that the distance between a random fully connected network (and its derivatives) to the corresponding infinite width Gaussian process scales like $n^{-\gamma}$ for $\gamma&gt;0,$ with the exponent depending on the metric used to measure discrepancy. Our bounds are stronger in terms of their dependence on network width than any previously available in the literature.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23884;&#20837;&#36317;&#31163;&#30340;&#26041;&#27861;&#26469;&#37327;&#21270;&#22238;&#38899;&#23460;&#25928;&#24212;&#12290;&#36890;&#36807;&#35745;&#31639;&#29992;&#25143;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#32467;&#21512;Echo Chamber Score(ECS)&#25351;&#26631;&#26469;&#35780;&#20272;&#29992;&#25143;&#31038;&#21306;&#30340;&#20957;&#32858;&#21147;&#21644;&#20998;&#31163;&#24230;&#65292;&#32780;&#19981;&#38656;&#35201;&#29992;&#25143;&#24847;&#35782;&#24418;&#24577;&#30340;&#26631;&#31614;&#21644;&#20132;&#20114;&#22270;&#30340;&#32467;&#26500;&#20551;&#35774;&#12290;</title><link>http://arxiv.org/abs/2307.04668</link><description>&lt;p&gt;
&#37327;&#21270;&#22238;&#38899;&#23460;&#25928;&#24212;&#65306;&#19968;&#31181;&#22522;&#20110;&#23884;&#20837;&#36317;&#31163;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Quantifying the Echo Chamber Effect: An Embedding Distance-based Approach. (arXiv:2307.04668v2 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04668
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23884;&#20837;&#36317;&#31163;&#30340;&#26041;&#27861;&#26469;&#37327;&#21270;&#22238;&#38899;&#23460;&#25928;&#24212;&#12290;&#36890;&#36807;&#35745;&#31639;&#29992;&#25143;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#32467;&#21512;Echo Chamber Score(ECS)&#25351;&#26631;&#26469;&#35780;&#20272;&#29992;&#25143;&#31038;&#21306;&#30340;&#20957;&#32858;&#21147;&#21644;&#20998;&#31163;&#24230;&#65292;&#32780;&#19981;&#38656;&#35201;&#29992;&#25143;&#24847;&#35782;&#24418;&#24577;&#30340;&#26631;&#31614;&#21644;&#20132;&#20114;&#22270;&#30340;&#32467;&#26500;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#30340;&#20852;&#36215;&#20419;&#36827;&#20102;&#22238;&#38899;&#23460;&#30340;&#24418;&#25104;&#65292;&#22238;&#38899;&#23460;&#26159;&#22312;&#32447;&#31354;&#38388;&#65292;&#29992;&#25143;&#20027;&#35201;&#36935;&#21040;&#24378;&#21270;&#20182;&#20204;&#29616;&#26377;&#20449;&#24565;&#30340;&#35266;&#28857;&#65292;&#21516;&#26102;&#25490;&#38500;&#19981;&#21516;&#24847;&#35265;&#12290;&#36825;&#31181;&#29616;&#35937;&#26174;&#33879;&#38459;&#30861;&#20102;&#20449;&#24687;&#22312;&#31038;&#21306;&#20043;&#38388;&#30340;&#20256;&#25773;&#65292;&#21152;&#21095;&#20102;&#31038;&#20250;&#26497;&#21270;&#12290;&#22240;&#27492;&#65292;&#24320;&#21457;&#37327;&#21270;&#22238;&#38899;&#23460;&#30340;&#26041;&#27861;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22238;&#38899;&#23460;&#24471;&#20998;&#65288;ECS&#65289;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#35780;&#20272;&#29992;&#25143;&#31038;&#21306;&#20957;&#32858;&#21147;&#21644;&#20998;&#31163;&#24230;&#30340;&#25351;&#26631;&#65292;&#36890;&#36807;&#27979;&#37327;&#23884;&#20837;&#31354;&#38388;&#20013;&#29992;&#25143;&#20043;&#38388;&#30340;&#36317;&#31163;&#26469;&#23454;&#29616;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;ECS&#33021;&#22815;&#22312;&#19981;&#20855;&#22791;&#29992;&#25143;&#24847;&#35782;&#24418;&#24577;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#21457;&#25381;&#20316;&#29992;&#65292;&#24182;&#19988;&#19981;&#23545;&#20132;&#20114;&#22270;&#30340;&#32467;&#26500;&#20570;&#20986;&#20219;&#20309;&#20551;&#35774;&#12290;&#20026;&#20102;&#20415;&#20110;&#27979;&#37327;&#29992;&#25143;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EchoGAE&#65292;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#22270;&#33258;&#32534;&#30721;&#22120;&#30340;&#29992;&#25143;&#23884;&#20837;&#27169;&#22411;&#65292;&#21033;&#29992;&#29992;&#25143;&#30340;&#24086;&#23376;&#21644;&#20132;&#20114;&#22270;&#23558;&#29992;&#25143;&#23884;&#20837;&#21040;&#19968;&#31181;&#26041;&#24335;&#20013;
&lt;/p&gt;
&lt;p&gt;
The rise of social media platforms has facilitated the formation of echo chambers, which are online spaces where users predominantly encounter viewpoints that reinforce their existing beliefs while excluding dissenting perspectives. This phenomenon significantly hinders information dissemination across communities and fuels societal polarization. Therefore, it is crucial to develop methods for quantifying echo chambers. In this paper, we present the Echo Chamber Score (ECS), a novel metric that assesses the cohesion and separation of user communities by measuring distances between users in the embedding space. In contrast to existing approaches, ECS is able to function without labels for user ideologies and makes no assumptions about the structure of the interaction graph. To facilitate measuring distances between users, we propose EchoGAE, a self-supervised graph autoencoder-based user embedding model that leverages users' posts and the interaction graph to embed them in a manner that
&lt;/p&gt;</description></item><item><title>Solvent&#26159;&#19968;&#20010;&#29992;&#20110;&#34507;&#30333;&#36136;&#25240;&#21472;&#30340;&#32479;&#19968;&#30740;&#31350;&#26694;&#26550;&#65292;&#25903;&#25345;&#26368;&#26032;&#27169;&#22411;&#37325;&#35201;&#32452;&#20214;&#30340;&#23454;&#29616;&#21644;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#25552;&#20379;&#23545;&#34507;&#30333;&#36136;&#32467;&#26500;&#24314;&#27169;&#39046;&#22495;&#30340;&#26377;&#29992;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2307.04603</link><description>&lt;p&gt;
Solvent: &#19968;&#20010;&#29992;&#20110;&#34507;&#30333;&#36136;&#25240;&#21472;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Solvent: A Framework for Protein Folding. (arXiv:2307.04603v2 [q-bio.BM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04603
&lt;/p&gt;
&lt;p&gt;
Solvent&#26159;&#19968;&#20010;&#29992;&#20110;&#34507;&#30333;&#36136;&#25240;&#21472;&#30340;&#32479;&#19968;&#30740;&#31350;&#26694;&#26550;&#65292;&#25903;&#25345;&#26368;&#26032;&#27169;&#22411;&#37325;&#35201;&#32452;&#20214;&#30340;&#23454;&#29616;&#21644;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#25552;&#20379;&#23545;&#34507;&#30333;&#36136;&#32467;&#26500;&#24314;&#27169;&#39046;&#22495;&#30340;&#26377;&#29992;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#33268;&#24615;&#21644;&#21487;&#38752;&#24615;&#23545;&#20110;&#36827;&#34892;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#33267;&#20851;&#37325;&#35201;&#12290;&#35768;&#22810;&#33879;&#21517;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#22914;&#30446;&#26631;&#26816;&#27979;&#65292;&#24050;&#32463;&#36890;&#36807;&#31283;&#23450;&#30340;&#22522;&#20934;&#26694;&#26550;&#36827;&#34892;&#20102;&#27604;&#36739;&#21644;&#39564;&#35777;&#12290;&#22312;AlphaFold2&#20043;&#21518;&#65292;&#34507;&#30333;&#36136;&#25240;&#21472;&#20219;&#21153;&#24050;&#32463;&#36827;&#20837;&#20102;&#19968;&#20010;&#26032;&#38454;&#27573;&#65292;&#35768;&#22810;&#26041;&#27861;&#37117;&#26159;&#22522;&#20110;AlphaFold2&#30340;&#32452;&#20214;&#25552;&#20986;&#30340;&#12290;&#22312;&#34507;&#30333;&#36136;&#25240;&#21472;&#20013;&#65292;&#19968;&#20010;&#32479;&#19968;&#30340;&#30740;&#31350;&#26694;&#26550;&#30340;&#37325;&#35201;&#24615;&#21253;&#25324;&#23454;&#29616;&#21644;&#22522;&#20934;&#65292;&#20197;&#19968;&#33268;&#19988;&#20844;&#24179;&#22320;&#27604;&#36739;&#21508;&#31181;&#26041;&#27861;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Solvent&#65292;&#19968;&#20010;&#25903;&#25345;&#26368;&#26032;&#27169;&#22411;&#37325;&#35201;&#32452;&#20214;&#30340;&#34507;&#30333;&#36136;&#25240;&#21472;&#26694;&#26550;&#12290;Solvent&#21253;&#21547;&#22312;&#19968;&#20010;&#32479;&#19968;&#30340;&#20195;&#30721;&#24211;&#20013;&#23454;&#29616;&#30340;&#19981;&#21516;&#27169;&#22411;&#65292;&#24182;&#25903;&#25345;&#22312;&#30456;&#21516;&#25968;&#25454;&#38598;&#19978;&#23545;&#23450;&#20041;&#30340;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#21644;&#35780;&#20272;&#12290;&#25105;&#20204;&#23545;&#33879;&#21517;&#31639;&#27861;&#21450;&#20854;&#32452;&#20214;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#36827;&#34892;&#20102;&#23454;&#39564;&#20197;&#23545;&#34507;&#30333;&#36136;&#32467;&#26500;&#24314;&#27169;&#39046;&#22495;&#25552;&#20379;&#26377;&#29992;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#24076;&#26395;Solvent&#33021;&#25552;&#39640;&#34507;&#30333;&#36136;&#25240;&#21472;&#30740;&#31350;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Consistency and reliability are crucial for conducting AI research. Many famous research fields, such as object detection, have been compared and validated with solid benchmark frameworks. After AlphaFold2, the protein folding task has entered a new phase, and many methods are proposed based on the component of AlphaFold2. The importance of a unified research framework in protein folding contains implementations and benchmarks to consistently and fairly compare various approaches. To achieve this, we present Solvent, an protein folding framework that supports significant components of state-of-th-arts models in the manner of off-the-shelf interface Solvent contains different models implemented in a unified codebase and supports training and evaluation for defined models on the same dataset. We benchmark well-known algorithms and their components and provide experiments that give helpful insights into the protein structure modeling field. We hope that Solvent will increase the reliabili
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20004;&#31181;&#38598;&#21512;&#20803;&#32032;&#23884;&#20837;&#23618;&#30340;&#25506;&#32034;&#65292;&#35777;&#26126;&#20102;&#22810;&#39033;&#24335;&#23485;&#24230;&#23545;&#20110;&#39640;&#32500;&#29305;&#24449;&#30340;&#38598;&#21512;&#34920;&#31034;&#36275;&#22815;&#65292;&#24182;&#25581;&#31034;&#20102;&#20043;&#21069;&#20998;&#26512;&#20013;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.04001</link><description>&lt;p&gt;
&#22810;&#39033;&#24335;&#23485;&#24230;&#23545;&#20110;&#20855;&#26377;&#39640;&#32500;&#29305;&#24449;&#30340;&#38598;&#21512;&#34920;&#31034;&#36275;&#22815;
&lt;/p&gt;
&lt;p&gt;
Polynomial Width is Sufficient for Set Representation with High-dimensional Features. (arXiv:2307.04001v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04001
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20004;&#31181;&#38598;&#21512;&#20803;&#32032;&#23884;&#20837;&#23618;&#30340;&#25506;&#32034;&#65292;&#35777;&#26126;&#20102;&#22810;&#39033;&#24335;&#23485;&#24230;&#23545;&#20110;&#39640;&#32500;&#29305;&#24449;&#30340;&#38598;&#21512;&#34920;&#31034;&#36275;&#22815;&#65292;&#24182;&#25581;&#31034;&#20102;&#20043;&#21069;&#20998;&#26512;&#20013;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38598;&#21512;&#34920;&#31034;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#24050;&#32463;&#21464;&#24471;&#26222;&#36941;&#65292;&#29992;&#20110;&#24314;&#27169;&#31070;&#32463;&#32593;&#32476;&#23545;&#36755;&#20837;&#39034;&#24207;&#19981;&#25935;&#24863;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;DeepSets&#26159;&#26368;&#24120;&#29992;&#30340;&#38598;&#21512;&#34920;&#31034;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#23427;&#23558;&#27599;&#20010;&#38598;&#21512;&#20803;&#32032;&#23884;&#20837;&#21040;&#20855;&#26377;&#32500;&#24230;L&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#65292;&#28982;&#21518;&#36827;&#34892;&#27714;&#21644;&#27744;&#21270;&#20197;&#33719;&#24471;&#25972;&#20010;&#38598;&#21512;&#30340;&#23884;&#20837;&#65292;&#26368;&#21518;&#23558;&#25972;&#20010;&#38598;&#21512;&#30340;&#23884;&#20837;&#26144;&#23556;&#21040;&#36755;&#20986;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#32500;&#24230;L&#23545;DeepSets&#34920;&#36798;&#33021;&#21147;&#30340;&#24433;&#21709;&#12290;&#20043;&#21069;&#30340;&#20998;&#26512;&#35201;&#20040;&#23558;&#39640;&#32500;&#29305;&#24449;&#36807;&#20110;&#31616;&#21270;&#20026;&#19968;&#32500;&#29305;&#24449;&#65292;&#35201;&#20040;&#23616;&#38480;&#20110;&#20998;&#26512;&#28608;&#27963;&#20989;&#25968;&#65292;&#20174;&#32780;&#33073;&#31163;&#23454;&#38469;&#24212;&#29992;&#25110;&#23548;&#33268;L&#38543;&#30528;&#38598;&#21512;&#22823;&#23567;N&#21644;&#29305;&#24449;&#32500;&#24230;D&#21576;&#25351;&#25968;&#22686;&#38271;&#12290;&#20026;&#20102;&#30740;&#31350;&#36798;&#21040;&#36275;&#22815;&#34920;&#36798;&#33021;&#21147;&#30340;&#26368;&#23567;L&#20540;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#38598;&#21512;&#20803;&#32032;&#23884;&#20837;&#23618;&#65306;&#65288;a&#65289;&#32447;&#24615;+&#24130;&#28608;&#27963;&#65288;LP&#65289;&#21644;&#65288;b&#65289;&#32447;&#24615;+&#25351;&#25968;&#28608;&#27963;&#12290;
&lt;/p&gt;
&lt;p&gt;
Set representation has become ubiquitous in deep learning for modeling the inductive bias of neural networks that are insensitive to the input order. DeepSets is the most widely used neural network architecture for set representation. It involves embedding each set element into a latent space with dimension $L$, followed by a sum pooling to obtain a whole-set embedding, and finally mapping the whole-set embedding to the output. In this work, we investigate the impact of the dimension $L$ on the expressive power of DeepSets. Previous analyses either oversimplified high-dimensional features to be one-dimensional features or were limited to analytic activations, thereby diverging from practical use or resulting in $L$ that grows exponentially with the set size $N$ and feature dimension $D$. To investigate the minimal value of $L$ that achieves sufficient expressive power, we present two set-element embedding layers: (a) linear + power activation (LP) and (b) linear + exponential activatio
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#31995;&#32479;&#30740;&#31350;&#27969;&#24335;&#21644;&#27744;&#24335;&#20027;&#21160;&#23398;&#20064;&#19979;&#30340;&#19981;&#30830;&#23450;&#24615;&#37319;&#26679;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31561;&#25928;&#25439;&#22833;&#30340;&#27010;&#24565;&#65292;&#24182;&#35777;&#26126;&#19981;&#30830;&#23450;&#24615;&#37319;&#26679;&#31639;&#27861;&#23454;&#36136;&#19978;&#26159;&#38024;&#23545;&#35813;&#31561;&#25928;&#25439;&#22833;&#36827;&#34892;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2307.02719</link><description>&lt;p&gt;
&#29702;&#35299;&#19981;&#30830;&#23450;&#24615;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Understanding Uncertainty Sampling. (arXiv:2307.02719v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02719
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#31995;&#32479;&#30740;&#31350;&#27969;&#24335;&#21644;&#27744;&#24335;&#20027;&#21160;&#23398;&#20064;&#19979;&#30340;&#19981;&#30830;&#23450;&#24615;&#37319;&#26679;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31561;&#25928;&#25439;&#22833;&#30340;&#27010;&#24565;&#65292;&#24182;&#35777;&#26126;&#19981;&#30830;&#23450;&#24615;&#37319;&#26679;&#31639;&#27861;&#23454;&#36136;&#19978;&#26159;&#38024;&#23545;&#35813;&#31561;&#25928;&#25439;&#22833;&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#37319;&#26679;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#65292;&#23427;&#39034;&#24207;&#22320;&#26597;&#35810;&#24403;&#21069;&#39044;&#27979;&#27169;&#22411;&#23545;&#25968;&#25454;&#26679;&#26412;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#28982;&#32780;&#65292;&#19981;&#30830;&#23450;&#24615;&#37319;&#26679;&#30340;&#20351;&#29992;&#24448;&#24448;&#26159;&#21551;&#21457;&#24335;&#30340;&#65306;&#65288;i&#65289;&#20851;&#20110;&#22312;&#29305;&#23450;&#20219;&#21153;&#21644;&#29305;&#23450;&#25439;&#22833;&#20989;&#25968;&#19979;&#23545;&#8220;&#19981;&#30830;&#23450;&#24615;&#8221;&#30340;&#20934;&#30830;&#23450;&#20041;&#27809;&#26377;&#20849;&#35782;&#65307;&#65288;ii&#65289;&#27809;&#26377;&#29702;&#35770;&#20445;&#35777;&#33021;&#22815;&#32473;&#20986;&#19968;&#20010;&#26631;&#20934;&#21327;&#35758;&#26469;&#23454;&#26045;&#35813;&#31639;&#27861;&#65292;&#20363;&#22914;&#65292;&#22312;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31561;&#20248;&#21270;&#31639;&#27861;&#26694;&#26550;&#19979;&#22914;&#20309;&#22788;&#29702;&#39034;&#24207;&#21040;&#36798;&#30340;&#27880;&#37322;&#25968;&#25454;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#27969;&#24335;&#21644;&#27744;&#24335;&#20027;&#21160;&#23398;&#20064;&#19979;&#30340;&#19981;&#30830;&#23450;&#24615;&#37319;&#26679;&#31639;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31561;&#25928;&#25439;&#22833;&#30340;&#27010;&#24565;&#65292;&#35813;&#27010;&#24565;&#21462;&#20915;&#20110;&#20351;&#29992;&#30340;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#21644;&#21407;&#22987;&#25439;&#22833;&#20989;&#25968;&#65292;&#24182;&#30830;&#31435;&#20102;&#19981;&#30830;&#23450;&#24615;&#37319;&#26679;&#31639;&#27861;&#26412;&#36136;&#19978;&#26159;&#38024;&#23545;&#36825;&#31181;&#31561;&#25928;&#25439;&#22833;&#36827;&#34892;&#20248;&#21270;&#12290;&#36825;&#19968;&#35266;&#28857;&#39564;&#35777;&#20102;&#31639;&#27861;&#30340;&#36866;&#24403;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Uncertainty sampling is a prevalent active learning algorithm that queries sequentially the annotations of data samples which the current prediction model is uncertain about. However, the usage of uncertainty sampling has been largely heuristic: (i) There is no consensus on the proper definition of "uncertainty" for a specific task under a specific loss; (ii) There is no theoretical guarantee that prescribes a standard protocol to implement the algorithm, for example, how to handle the sequentially arrived annotated data under the framework of optimization algorithms such as stochastic gradient descent. In this work, we systematically examine uncertainty sampling algorithms under both stream-based and pool-based active learning. We propose a notion of equivalent loss which depends on the used uncertainty measure and the original loss function and establish that an uncertainty sampling algorithm essentially optimizes against such an equivalent loss. The perspective verifies the properne
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;$\nu^2$-&#27969;&#26041;&#27861;&#65292;&#23427;&#26159;$\nu$-&#27969;&#26041;&#27861;&#22312;&#21253;&#21547;&#22810;&#20010;&#20013;&#24494;&#23376;&#30340;&#26411;&#24577;&#20013;&#30340;&#25193;&#23637;&#12290;&#19982;&#26631;&#20934;&#35299;&#26512;&#25216;&#26415;&#30456;&#27604;&#65292;$\nu^2$-&#27969;&#22312;&#37325;&#24314;&#20013;&#24494;&#23376;&#21160;&#37327;&#21644;&#30456;&#20851;&#24615;&#26041;&#38754;&#26356;&#20934;&#30830;&#65292;&#25512;&#26029;&#26102;&#38388;&#26356;&#24555;&#65292;&#24182;&#19988;&#21487;&#20197;&#25552;&#20379;&#25913;&#36827;&#30340;&#32479;&#35745;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2307.02405</link><description>&lt;p&gt;
$\nu^2$-&#27969;&#65306;&#22312;&#22810;&#20013;&#24494;&#23376;&#26411;&#24577;&#20013;&#20351;&#29992;&#26465;&#20214;&#24402;&#19968;&#21270;&#27969;&#36827;&#34892;&#24555;&#36895;&#21644;&#25913;&#36827;&#30340;&#20013;&#24494;&#23376;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
$\nu^2$-Flows: Fast and improved neutrino reconstruction in multi-neutrino final states with conditional normalizing flows. (arXiv:2307.02405v2 [hep-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02405
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;$\nu^2$-&#27969;&#26041;&#27861;&#65292;&#23427;&#26159;$\nu$-&#27969;&#26041;&#27861;&#22312;&#21253;&#21547;&#22810;&#20010;&#20013;&#24494;&#23376;&#30340;&#26411;&#24577;&#20013;&#30340;&#25193;&#23637;&#12290;&#19982;&#26631;&#20934;&#35299;&#26512;&#25216;&#26415;&#30456;&#27604;&#65292;$\nu^2$-&#27969;&#22312;&#37325;&#24314;&#20013;&#24494;&#23376;&#21160;&#37327;&#21644;&#30456;&#20851;&#24615;&#26041;&#38754;&#26356;&#20934;&#30830;&#65292;&#25512;&#26029;&#26102;&#38388;&#26356;&#24555;&#65292;&#24182;&#19988;&#21487;&#20197;&#25552;&#20379;&#25913;&#36827;&#30340;&#32479;&#35745;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;$\nu^2$-&#27969;&#26041;&#27861;&#65292;&#36825;&#26159;$\nu$-&#27969;&#26041;&#27861;&#23545;&#20110;&#21253;&#21547;&#22810;&#20010;&#20013;&#24494;&#23376;&#30340;&#26411;&#24577;&#30340;&#19968;&#31181;&#25193;&#23637;&#12290;&#35813;&#26550;&#26500;&#21487;&#20197;&#21407;&#29983;&#22320;&#20026;&#26411;&#24577;&#20013;&#30340;&#25152;&#26377;&#23545;&#35937;&#31867;&#22411;&#21644;&#22810;&#37325;&#24615;&#32452;&#21512;&#36827;&#34892;&#32553;&#25918;&#65292;&#36866;&#29992;&#20110;&#20219;&#20309;&#25152;&#38656;&#30340;&#20013;&#24494;&#23376;&#22810;&#37325;&#24615;&#12290;&#22312;$t\bar{t}$&#20108;&#36731;&#23376;&#20107;&#20214;&#20013;&#65292;&#19982;&#20351;&#29992;&#26368;&#27969;&#34892;&#30340;&#26631;&#20934;&#35299;&#26512;&#25216;&#26415;&#30456;&#27604;&#65292;&#20013;&#24494;&#23376;&#30340;&#21160;&#37327;&#20197;&#21450;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#37325;&#24314;&#65292;&#24182;&#19988;&#21487;&#20197;&#25214;&#21040;&#25152;&#26377;&#20107;&#20214;&#30340;&#35299;&#12290;&#25512;&#26029;&#26102;&#38388;&#27604;&#31454;&#20105;&#26041;&#27861;&#26174;&#33879;&#26356;&#24555;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#22312;&#22270;&#24418;&#22788;&#29702;&#21333;&#20803;&#19978;&#24182;&#34892;&#35780;&#20272;&#26469;&#36827;&#19968;&#27493;&#20943;&#23569;&#12290;&#25105;&#20204;&#23558;$\nu^2$-&#27969;&#24212;&#29992;&#20110;$t\bar{t}$&#20108;&#36731;&#23376;&#20107;&#20214;&#65292;&#24182;&#23637;&#31034;&#20102;&#23637;&#24320;&#20998;&#24067;&#20013;&#27599;&#20010;&#31665;&#23376;&#30340;&#19981;&#30830;&#23450;&#24615;&#27604;&#26631;&#20934;&#25216;&#26415;&#26356;&#25509;&#36817;&#23436;&#32654;&#20013;&#24494;&#23376;&#37325;&#24314;&#30340;&#24615;&#33021;&#30028;&#38480;&#12290;&#23545;&#20110;&#36873;&#25321;&#30340;&#21452;&#24494;&#20998;&#35266;&#27979;&#37327;&#65292;$\nu^2$-&#27969;&#21487;&#20197;&#22312;&#27599;&#20010;&#31665;&#23376;&#20013;&#25552;&#20379;&#25913;&#36827;&#30340;&#32479;&#35745;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work we introduce $\nu^2$-Flows, an extension of the $\nu$-Flows method to final states containing multiple neutrinos. The architecture can natively scale for all combinations of object types and multiplicities in the final state for any desired neutrino multiplicities. In $t\bar{t}$ dilepton events, the momenta of both neutrinos and correlations between them are reconstructed more accurately than when using the most popular standard analytical techniques, and solutions are found for all events. Inference time is significantly faster than competing methods, and can be reduced further by evaluating in parallel on graphics processing units. We apply $\nu^2$-Flows to $t\bar{t}$ dilepton events and show that the per-bin uncertainties in unfolded distributions is much closer to the limit of performance set by perfect neutrino reconstruction than standard techniques. For the chosen double differential observables $\nu^2$-Flows results in improved statistical precision for each bin by
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#31181;&#24050;&#30693;&#30340;UCB&#31867;&#22411;&#26041;&#27861;&#29992;&#20110;&#23398;&#20064;&#39044;&#27979;&#29366;&#24577;&#34920;&#31034;&#65288;PSRs&#65289;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#30340;&#22870;&#21169;&#39033;&#26469;&#19978;&#30028;t</title><link>http://arxiv.org/abs/2307.00405</link><description>&lt;p&gt;
&#21487;&#35777;&#26126;&#39640;&#25928;&#30340;UCB&#31867;&#22411;&#31639;&#27861;&#29992;&#20110;&#23398;&#20064;&#39044;&#27979;&#29366;&#24577;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Provably Efficient UCB-type Algorithms For Learning Predictive State Representations. (arXiv:2307.00405v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00405
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#31181;&#24050;&#30693;&#30340;UCB&#31867;&#22411;&#26041;&#27861;&#29992;&#20110;&#23398;&#20064;&#39044;&#27979;&#29366;&#24577;&#34920;&#31034;&#65288;PSRs&#65289;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#30340;&#22870;&#21169;&#39033;&#26469;&#19978;&#30028;t
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#33324;&#30340;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#26088;&#22312;&#36890;&#36807;&#22522;&#20110;&#36807;&#21435;&#35266;&#23519;&#21644;&#34892;&#21160;&#30340;&#21382;&#21490;&#26469;&#26368;&#22823;&#21270;&#32047;&#31215;&#22870;&#21169;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22914;&#26524;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#21487;&#20197;&#29992;&#39044;&#27979;&#29366;&#24577;&#34920;&#31034;&#65288;PSRs&#65289;&#24314;&#27169;&#20302;&#31209;&#32467;&#26500;&#65292;&#37027;&#20040;&#23427;&#26159;&#21487;&#32479;&#35745;&#23398;&#20064;&#30340;&#12290;&#23613;&#31649;&#26377;&#36825;&#20123;&#36827;&#23637;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#20351;&#29992;&#39044;&#20808;&#35774;&#35745;&#22909;&#30340;&#27493;&#39588;&#25110;&#32773;&#26159;&#35745;&#31639;&#25928;&#29575;&#20302;&#19979;&#30340;&#25110;&#32773;&#26159;&#19981;&#21487;&#35745;&#31639;&#30340;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#19978;&#38480;&#32622;&#20449;&#21306;&#38388;&#65288;UCB&#65289;&#26041;&#27861;&#22312;&#36172;&#21338;&#26426;&#21644;MDPs&#20013;&#34987;&#25104;&#21151;&#22320;&#20316;&#20026;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#26041;&#27861;&#65292;&#20294;&#23545;PSR&#36825;&#31181;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#36824;&#27809;&#26377;&#36827;&#34892;&#30740;&#31350;&#65292;&#36825;&#26159;&#30001;&#20110;&#22312;&#36825;&#31181;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#20048;&#35266;&#22411;&#22870;&#21169;&#30340;&#35774;&#35745;&#21313;&#20998;&#22256;&#38590;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;PSRs&#30340;&#31532;&#19968;&#31181;&#24050;&#30693;&#30340;UCB&#31867;&#22411;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#19968;&#20010;&#26032;&#30340;&#22870;&#21169;&#39033;&#26469;&#19978;&#30028;t
&lt;/p&gt;
&lt;p&gt;
The general sequential decision-making problem, which includes Markov decision processes (MDPs) and partially observable MDPs (POMDPs) as special cases, aims at maximizing a cumulative reward by making a sequence of decisions based on a history of observations and actions over time. Recent studies have shown that the sequential decision-making problem is statistically learnable if it admits a low-rank structure modeled by predictive state representations (PSRs). Despite these advancements, existing approaches typically involve oracles or steps that are not computationally efficient. On the other hand, the upper confidence bound (UCB) based approaches, which have served successfully as computationally efficient methods in bandits and MDPs, have not been investigated for more general PSRs, due to the difficulty of optimistic bonus design in these more challenging settings. This paper proposes the first known UCB-type approach for PSRs, featuring a novel bonus term that upper bounds the t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;ChatGPT&#36827;&#34892;&#26426;&#22120;&#20154;&#24212;&#29992;&#30340;&#23454;&#39564;&#30740;&#31350;&#65292;&#36890;&#36807;&#35774;&#35745;&#21407;&#21017;&#21644;&#20989;&#25968;&#24211;&#30340;&#32467;&#21512;&#65292;ChatGPT&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.17582</link><description>&lt;p&gt;
ChatGPT&#29992;&#20110;&#26426;&#22120;&#20154;&#25216;&#26415;&#65306;&#35774;&#35745;&#21407;&#21017;&#21644;&#27169;&#22411;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
ChatGPT for Robotics: Design Principles and Model Abilities. (arXiv:2306.17582v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17582
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;ChatGPT&#36827;&#34892;&#26426;&#22120;&#20154;&#24212;&#29992;&#30340;&#23454;&#39564;&#30740;&#31350;&#65292;&#36890;&#36807;&#35774;&#35745;&#21407;&#21017;&#21644;&#20989;&#25968;&#24211;&#30340;&#32467;&#21512;&#65292;ChatGPT&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;OpenAI&#30340;ChatGPT&#36827;&#34892;&#26426;&#22120;&#20154;&#24212;&#29992;&#30340;&#23454;&#39564;&#30740;&#31350;&#12290;&#25105;&#20204;&#27010;&#36848;&#20102;&#19968;&#31181;&#31574;&#30053;&#65292;&#23558;&#25552;&#31034;&#24037;&#31243;&#30340;&#35774;&#35745;&#21407;&#21017;&#19982;&#39640;&#32423;&#20989;&#25968;&#24211;&#30340;&#21019;&#24314;&#30456;&#32467;&#21512;&#65292;&#20351;ChatGPT&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#12289;&#27169;&#25311;&#22120;&#21644;&#24418;&#24577;&#12290;&#25105;&#20204;&#37325;&#28857;&#35780;&#20272;&#20102;&#19981;&#21516;&#30340;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#21644;&#23545;&#35805;&#31574;&#30053;&#23545;&#25191;&#34892;&#21508;&#31181;&#31867;&#22411;&#26426;&#22120;&#20154;&#20219;&#21153;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;ChatGPT&#20351;&#29992;&#33258;&#30001;&#24418;&#24335;&#23545;&#35805;&#12289;&#35299;&#26512;XML&#26631;&#35760;&#21644;&#21512;&#25104;&#20195;&#30721;&#30340;&#33021;&#21147;&#65292;&#20197;&#21450;&#20351;&#29992;&#20219;&#21153;&#29305;&#23450;&#25552;&#31034;&#20989;&#25968;&#21644;&#36890;&#36807;&#23545;&#35805;&#36827;&#34892;&#38381;&#29615;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#28085;&#30422;&#20102;&#26426;&#22120;&#20154;&#39046;&#22495;&#30340;&#19968;&#31995;&#21015;&#20219;&#21153;&#65292;&#20174;&#22522;&#26412;&#30340;&#36923;&#36753;&#12289;&#20960;&#20309;&#21644;&#25968;&#23398;&#25512;&#29702;&#21040;&#22797;&#26434;&#30340;&#39046;&#22495;&#65292;&#22914;&#31354;&#20013;&#23548;&#33322;&#12289;&#25805;&#32437;&#21644;&#20855;&#36523;&#20195;&#29702;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;ChatGPT&#22312;&#35299;&#20915;&#36825;&#20123;&#20219;&#21153;&#26041;&#38754;&#21487;&#20197;&#21462;&#24471;&#26377;&#25928;&#32467;&#26524;&#65292;&#21516;&#26102;&#20351;&#25105;&#20204;&#33021;&#22815;&#36827;&#34892;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an experimental study regarding the use of OpenAI's ChatGPT for robotics applications. We outline a strategy that combines design principles for prompt engineering and the creation of a high-level function library which allows ChatGPT to adapt to different robotics tasks, simulators, and form factors. We focus our evaluations on the effectiveness of different prompt engineering techniques and dialog strategies towards the execution of various types of robotics tasks. We explore ChatGPT's ability to use free-form dialog, parse XML tags, and to synthesize code, in addition to the use of task-specific prompting functions and closed-loop reasoning through dialogues. Our study encompasses a range of tasks within the robotics domain, from basic logical, geometrical, and mathematical reasoning all the way to complex domains such as aerial navigation, manipulation, and embodied agents. We show that ChatGPT can be effective at solving several of such tasks, while allowing us
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#36164;&#28304;&#21294;&#20047;&#30340;&#21360;&#24230;&#35821;&#35328;&#39532;&#25289;&#22320;&#35821;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#22411;&#28151;&#21512;&#39532;&#25289;&#22320;&#35821;-&#33521;&#35821;&#35821;&#26009;&#24211;&#65292;&#20197;&#21450;&#39044;&#35757;&#32451;&#30340;&#28151;&#21512;BERT&#27169;&#22411;&#21644;&#38024;&#23545;&#28151;&#21512;&#35821;&#35328;&#19979;&#28216;&#20219;&#21153;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#35813;&#35821;&#26009;&#24211;&#35757;&#32451;&#30340;&#27169;&#22411;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;BERT&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.14030</link><description>&lt;p&gt;
My Boli&#65306;&#28151;&#21512;&#39532;&#25289;&#22320;&#35821;-&#33521;&#35821;&#30340;&#35821;&#26009;&#24211;&#12289;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
My Boli: Code-mixed Marathi-English Corpora, Pretrained Language Models and Evaluation Benchmarks. (arXiv:2306.14030v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14030
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#36164;&#28304;&#21294;&#20047;&#30340;&#21360;&#24230;&#35821;&#35328;&#39532;&#25289;&#22320;&#35821;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#22411;&#28151;&#21512;&#39532;&#25289;&#22320;&#35821;-&#33521;&#35821;&#35821;&#26009;&#24211;&#65292;&#20197;&#21450;&#39044;&#35757;&#32451;&#30340;&#28151;&#21512;BERT&#27169;&#22411;&#21644;&#38024;&#23545;&#28151;&#21512;&#35821;&#35328;&#19979;&#28216;&#20219;&#21153;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#35813;&#35821;&#26009;&#24211;&#35757;&#32451;&#30340;&#27169;&#22411;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;BERT&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#32570;&#20047;&#19987;&#38376;&#30340;&#28151;&#21512;&#35821;&#26009;&#24211;&#21644;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#28151;&#21512;&#35821;&#35328;&#25968;&#25454;&#30340;&#30740;&#31350;&#21463;&#21040;&#20102;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#36164;&#28304;&#21294;&#20047;&#30340;&#21360;&#24230;&#35821;&#35328;&#39532;&#25289;&#22320;&#35821;&#65292;&#36825;&#20010;&#35821;&#35328;&#20043;&#21069;&#27809;&#26377;&#20219;&#20309;&#28151;&#21512;&#35821;&#35328;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;L3Cube-MeCorpus&#65292;&#19968;&#20010;&#21253;&#21547;500&#19975;&#26465;&#25512;&#29305;&#30340;&#22823;&#22411;&#28151;&#21512;&#39532;&#25289;&#22320;&#35821;-&#33521;&#35821;(Mr-En)&#35821;&#26009;&#24211;&#65292;&#29992;&#20110;&#39044;&#35757;&#32451;&#12290;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;L3Cube-MeBERT&#21644;MeRoBERTa&#65292;&#22522;&#20110;BERT&#30340;&#28151;&#21512;&#27169;&#22411;&#65292;&#22312;MeCorpus&#19978;&#39044;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19977;&#20010;&#26377;&#30417;&#30563;&#30340;&#25968;&#25454;&#38598;MeHate&#12289;MeSent&#21644;MeLID&#65292;&#29992;&#20110;&#28151;&#21512;Mr-En&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#12289;&#24773;&#24863;&#20998;&#26512;&#21644;&#35821;&#35328;&#35782;&#21035;&#31561;&#19979;&#28216;&#20219;&#21153;&#12290;&#36825;&#20123;&#35780;&#20272;&#25968;&#25454;&#38598;&#20998;&#21035;&#21253;&#21547;&#25163;&#21160;&#27880;&#37322;&#30340;\url{~}12,000&#26465;&#39532;&#25289;&#22320;&#35821;-&#33521;&#35821;&#28151;&#21512;&#25512;&#29305;&#12290;&#21066;&#20943;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#20010;&#26032;&#35821;&#26009;&#24211;&#35757;&#32451;&#30340;&#27169;&#22411;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;BERT&#27169;&#22411;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#25552;&#20379;&#28151;&#21512;&#39532;&#25289;&#22320;&#35821;&#30340;&#20195;&#30721;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
The research on code-mixed data is limited due to the unavailability of dedicated code-mixed datasets and pre-trained language models. In this work, we focus on the low-resource Indian language Marathi which lacks any prior work in code-mixing. We present L3Cube-MeCorpus, a large code-mixed Marathi-English (Mr-En) corpus with 5 million tweets for pretraining. We also release L3Cube-MeBERT and MeRoBERTa, code-mixed BERT-based transformer models pre-trained on MeCorpus. Furthermore, for benchmarking, we present three supervised datasets MeHate, MeSent, and MeLID for downstream tasks like code-mixed Mr-En hate speech detection, sentiment analysis, and language identification respectively. These evaluation datasets individually consist of manually annotated \url{~}12,000 Marathi-English code-mixed tweets. Ablations show that the models trained on this novel corpus significantly outperform the existing state-of-the-art BERT models. This is the first work that presents artifacts for code-mix
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#27491;&#21017;&#21270;SE(3)&#32676;&#21367;&#31215;&#30340;&#20307;&#31215;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#35299;&#36830;&#32493;SO(3)&#26680;&#21644;&#31354;&#38388;&#26680;&#20197;&#23454;&#29616;&#26059;&#36716;&#24179;&#31227;&#31561;&#21464;&#24615;&#65292;&#24182;&#22312;&#21307;&#23398;&#20998;&#31867;&#20219;&#21153;&#20013;&#33719;&#24471;&#20102;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2306.13960</link><description>&lt;p&gt;
&#22522;&#20110;&#27491;&#21017;&#21270;SE(3)&#32676;&#21367;&#31215;&#30340;&#20307;&#31215;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Regular SE(3) Group Convolutions for Volumetric Medical Image Analysis. (arXiv:2306.13960v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13960
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#27491;&#21017;&#21270;SE(3)&#32676;&#21367;&#31215;&#30340;&#20307;&#31215;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#35299;&#36830;&#32493;SO(3)&#26680;&#21644;&#31354;&#38388;&#26680;&#20197;&#23454;&#29616;&#26059;&#36716;&#24179;&#31227;&#31561;&#21464;&#24615;&#65292;&#24182;&#22312;&#21307;&#23398;&#20998;&#31867;&#20219;&#21153;&#20013;&#33719;&#24471;&#20102;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#34920;&#26126;&#65292;&#27491;&#21017;&#32452;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(G-CNN)&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#24182;&#25552;&#39640;&#23545;&#19981;&#21516;&#20960;&#20309;&#23545;&#31216;&#24615;&#30340;&#31561;&#21464;&#24615;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;SE(3)&#38382;&#39064;&#65292;&#21363;&#26059;&#36716;&#24179;&#31227;&#31561;&#21464;&#24615;&#22312;&#20307;&#31215;&#25968;&#25454;&#19978;&#30340;&#38382;&#39064;&#12290;&#20307;&#31215;&#22270;&#20687;&#25968;&#25454;&#22312;&#35768;&#22810;&#21307;&#30103;&#35774;&#32622;&#20013;&#26222;&#36941;&#23384;&#22312;&#12290;&#21463;&#21487;&#20998;&#31163;&#32452;&#21367;&#31215;&#30340;&#26368;&#26032;&#24037;&#20316;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;SE(3)&#32676;&#21367;&#31215;&#26680;&#65292;&#23558;&#20854;&#20998;&#35299;&#20026;&#36830;&#32493;&#30340;SO(3)&#65288;&#26059;&#36716;&#65289;&#26680;&#21644;&#31354;&#38388;&#26680;&#12290;&#25105;&#20204;&#36890;&#36807;&#37319;&#26679;&#22343;&#21248;&#30340;SO(3)&#32593;&#26684;&#26469;&#36817;&#20284;&#36830;&#32493;&#35774;&#23450;&#19979;&#30340;&#23545;&#31216;&#24615;&#12290;&#25105;&#20204;&#30340;&#36830;&#32493;SO(3)&#26680;&#26159;&#36890;&#36807;&#31867;&#20284;&#22343;&#21248;&#32593;&#26684;&#30340;RBF&#25554;&#20540;&#21442;&#25968;&#21270;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20307;&#31215;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#30340;SE(3)&#31561;&#21464;&#27169;&#22411;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21307;&#23398;&#20998;&#31867;&#20219;&#21153;&#19978;&#22987;&#32456;&#20248;&#20110;CNN&#21644;&#24120;&#35268;&#31163;&#25955;G-CNN&#65292;&#24182;&#26174;&#31034;&#20986;&#26174;&#30528;&#25913;&#36827;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22122;&#22768;&#25968;&#25454;&#19979;&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;&#36798;&#21040;16.5%&#12290;
&lt;/p&gt;
&lt;p&gt;
Regular group convolutional neural networks (G-CNNs) have been shown to increase model performance and improve equivariance to different geometrical symmetries. This work addresses the problem of SE(3), i.e., roto-translation equivariance, on volumetric data. Volumetric image data is prevalent in many medical settings. Motivated by the recent work on separable group convolutions, we devise a SE(3) group convolution kernel separated into a continuous SO(3) (rotation) kernel and a spatial kernel. We approximate equivariance to the continuous setting by sampling uniform SO(3) grids. Our continuous SO(3) kernel is parameterized via RBF interpolation on similarly uniform grids. We demonstrate the advantages of our approach in volumetric medical image analysis. Our SE(3) equivariant models consistently outperform CNNs and regular discrete G-CNNs on challenging medical classification tasks and show significantly improved generalization capabilities. Our approach achieves up to a 16.5% gain in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26631;&#31614;&#29983;&#25104;&#26041;&#27861;&#30340;&#22686;&#37327;&#20998;&#31867;&#23398;&#20064;&#65288;CIL&#65289;&#26041;&#27861;&#65288;VAG&#65289;&#65292;&#22823;&#24133;&#20943;&#23569;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#65288;CF&#65289;&#65292;&#24182;&#26356;&#22909;&#22320;&#20445;&#30041;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21487;&#25512;&#24191;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2306.12619</link><description>&lt;p&gt;
&#22522;&#20110;&#26631;&#31614;&#29983;&#25104;&#30340;&#22686;&#37327;&#20998;&#31867;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Class-Incremental Learning based on Label Generation. (arXiv:2306.12619v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12619
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26631;&#31614;&#29983;&#25104;&#26041;&#27861;&#30340;&#22686;&#37327;&#20998;&#31867;&#23398;&#20064;&#65288;CIL&#65289;&#26041;&#27861;&#65288;VAG&#65289;&#65292;&#22823;&#24133;&#20943;&#23569;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#65288;CF&#65289;&#65292;&#24182;&#26356;&#22909;&#22320;&#20445;&#30041;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21487;&#25512;&#24191;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#23545;&#20110;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;&#65288;CIL&#65289;&#35774;&#32622;&#65292;&#30001;&#20110;&#28798;&#38590;&#24615;&#36951;&#24536;&#65288;CF&#65289;&#65292;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#36830;&#32493;&#23398;&#20064;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#25991;&#21457;&#29616;&#65292;&#22914;&#26524;&#23558;CIL&#23450;&#24335;&#20026;&#19968;&#20010;&#36830;&#32493;&#30340;&#26631;&#31614;&#29983;&#25104;&#38382;&#39064;&#65292;&#21017;&#21487;&#20197;&#22823;&#24133;&#20943;&#23569;CF&#24182;&#26356;&#22909;&#22320;&#20445;&#30041;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21487;&#25512;&#24191;&#34920;&#31034;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;CIL&#26041;&#27861;&#65288;VAG&#65289;&#65292;&#35813;&#26041;&#27861;&#36824;&#21033;&#29992;&#20102;&#35789;&#27719;&#34920;&#30340;&#31232;&#30095;&#24615;&#20197;&#20415;&#20110;&#29983;&#25104;&#65292;&#24182;&#20351;&#29992;&#26631;&#31614;&#35821;&#20041;&#21019;&#24314;&#20266;&#37325;&#25773;&#26679;&#26412;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;VAG&#30340;&#24615;&#33021;&#27604;&#22522;&#32447;&#22823;&#24133;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the great success of pre-trained language models, it is still a challenge to use these models for continual learning, especially for the class-incremental learning (CIL) setting due to catastrophic forgetting (CF). This paper reports our finding that if we formulate CIL as a continual label generation problem, CF is drastically reduced and the generalizable representations of pre-trained models can be better retained. We thus propose a new CIL method (VAG) that also leverages the sparsity of vocabulary to focus the generation and creates pseudo-replay samples by using label semantics. Experimental results show that VAG outperforms baselines by a large margin.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21487;&#20197;&#26377;&#25928;&#32416;&#27491;&#25968;&#25454;&#20559;&#24046;&#21644;&#20132;&#21449;&#20559;&#24046;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#26500;&#36896;&#20102;&#19968;&#20010;&#37325;&#26032;&#21152;&#26435;&#26041;&#26696;&#65292;&#21487;&#20197;&#31934;&#30830;&#35780;&#20272;&#20219;&#20309;&#20551;&#35774;&#22312;&#30495;&#23454;&#20998;&#24067;&#19978;&#30340;&#25439;&#22833;&#12290;</title><link>http://arxiv.org/abs/2306.11112</link><description>&lt;p&gt;
&#32416;&#27491;&#20844;&#24179;&#20998;&#31867;&#20013;&#30340;&#20302;&#20272;&#20559;&#24046;&#21644;&#20132;&#21449;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Correcting Underrepresentation and Intersectional Bias for Fair Classification. (arXiv:2306.11112v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11112
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21487;&#20197;&#26377;&#25928;&#32416;&#27491;&#25968;&#25454;&#20559;&#24046;&#21644;&#20132;&#21449;&#20559;&#24046;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#26500;&#36896;&#20102;&#19968;&#20010;&#37325;&#26032;&#21152;&#26435;&#26041;&#26696;&#65292;&#21487;&#20197;&#31934;&#30830;&#35780;&#20272;&#20219;&#20309;&#20551;&#35774;&#22312;&#30495;&#23454;&#20998;&#24067;&#19978;&#30340;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#23398;&#20064;&#34987;&#20302;&#20272;&#20559;&#24046;&#25439;&#22351;&#30340;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#27491;&#20363;&#22312;&#22266;&#23450;&#25968;&#37327;&#30340;&#25935;&#24863;&#32452;&#20013;&#20197;&#19981;&#21516;&#30340;&#26410;&#30693;&#36895;&#29575;&#20174;&#25968;&#25454;&#20013;&#36807;&#28388;&#25481;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#26377;&#23569;&#37327;&#26080;&#20559;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21487;&#20197;&#26377;&#25928;&#22320;&#20272;&#35745;&#27599;&#20010;&#32452;&#30340;&#20943;&#23569;&#21442;&#25968;&#65292;&#21363;&#20351;&#22312;&#20132;&#21449;&#32452;&#25104;&#21592;&#36164;&#26684;&#20351;&#24471;&#23398;&#20064;&#27599;&#20010;&#20132;&#21449;&#29575;&#21464;&#24471;&#35745;&#31639;&#19978;&#19981;&#21487;&#34892;&#30340;&#24773;&#20917;&#19979;&#12290;&#21033;&#29992;&#36825;&#20010;&#20998;&#32452;&#20002;&#22833;&#29575;&#30340;&#20272;&#35745;&#65292;&#25105;&#20204;&#26500;&#36896;&#20102;&#19968;&#20010;&#37325;&#26032;&#21152;&#26435;&#26041;&#26696;&#65292;&#21487;&#20197;&#20351;&#25105;&#20204;&#36817;&#20284;&#35780;&#20272;&#20219;&#20309;&#20551;&#35774;&#22312;&#30495;&#23454;&#20998;&#24067;&#19978;&#30340;&#25439;&#22833;&#65292;&#21363;&#20351;&#25105;&#20204;&#21482;&#33021;&#22312;&#19968;&#20010;&#26377;&#20559;&#26679;&#26412;&#19978;&#35266;&#23519;&#21040;&#32463;&#39564;&#35823;&#24046;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23553;&#35013;&#20102;&#36825;&#20010;&#23398;&#20064;&#21644;&#37325;&#26032;&#21152;&#26435;&#36807;&#31243;&#30340;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#24378;PAC&#39118;&#26684;&#30340;&#20445;&#35777;&#65292;&#21363;&#26377;&#24456;&#39640;&#30340;&#27010;&#29575;&#25105;&#20204;&#23545;&#20551;&#35774;&#22312;&#30495;&#23454;&#20998;&#24067;&#19978;&#30340;&#39118;&#38505;&#30340;&#20272;&#35745;&#23558;&#19982;&#30495;&#23454;&#39118;&#38505;&#20219;&#24847;&#25509;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of learning from data corrupted by underrepresentation bias, where positive examples are filtered from the data at different, unknown rates for a fixed number of sensitive groups. We show that with a small amount of unbiased data, we can efficiently estimate the group-wise drop-out parameters, even in settings where intersectional group membership makes learning each intersectional rate computationally infeasible. Using this estimate for the group-wise drop-out rate, we construct a re-weighting scheme that allows us to approximate the loss of any hypothesis on the true distribution, even if we only observe the empirical error on a biased sample. Finally, we present an algorithm encapsulating this learning and re-weighting process, and we provide strong PAC-style guarantees that, with high probability, our estimate of the risk of the hypothesis over the true distribution will be arbitrarily close to the true risk.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#36924;&#36817;&#26230;&#20307;&#26448;&#26009;&#30340;&#23436;&#25972;&#30456;&#20114;&#20316;&#29992;&#21183;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#35206;&#30422;&#25152;&#26377;&#21407;&#23376;&#23545;&#20043;&#38388;&#30340;&#21183;&#65292;&#20811;&#26381;&#20102;&#30446;&#21069;&#26041;&#27861;&#20013;&#21482;&#32771;&#34385;&#38468;&#36817;&#21407;&#23376;&#38388;&#21183;&#21644;&#26080;&#27861;&#25429;&#25417;&#26080;&#38480;&#37325;&#22797;&#27169;&#24335;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2306.10045</link><description>&lt;p&gt;
&#39044;&#27979;&#26230;&#20307;&#24615;&#36136;&#30340;&#23436;&#25972;&#30456;&#20114;&#20316;&#29992;&#21183;&#30340;&#39640;&#25928;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
Efficient Approximations of Complete Interatomic Potentials for Crystal Property Prediction. (arXiv:2306.10045v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10045
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#36924;&#36817;&#26230;&#20307;&#26448;&#26009;&#30340;&#23436;&#25972;&#30456;&#20114;&#20316;&#29992;&#21183;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#35206;&#30422;&#25152;&#26377;&#21407;&#23376;&#23545;&#20043;&#38388;&#30340;&#21183;&#65292;&#20811;&#26381;&#20102;&#30446;&#21069;&#26041;&#27861;&#20013;&#21482;&#32771;&#34385;&#38468;&#36817;&#21407;&#23376;&#38388;&#21183;&#21644;&#26080;&#27861;&#25429;&#25417;&#26080;&#38480;&#37325;&#22797;&#27169;&#24335;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#26230;&#20307;&#26448;&#26009;&#30340;&#24615;&#36136;&#39044;&#27979;&#12290;&#26230;&#20307;&#32467;&#26500;&#30001;&#19968;&#20010;&#26368;&#23567;&#30340;&#21333;&#20803;&#26684;&#32452;&#25104;&#65292;&#22312;&#19977;&#32500;&#31354;&#38388;&#20013;&#26080;&#38480;&#37325;&#22797;&#12290;&#22914;&#20309;&#22312;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#20934;&#30830;&#34920;&#31034;&#36825;&#31181;&#37325;&#22797;&#32467;&#26500;&#20173;&#28982;&#27809;&#26377;&#35299;&#20915;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#21482;&#22312;&#38468;&#36817;&#30340;&#33410;&#28857;&#20043;&#38388;&#24314;&#31435;&#36793;&#32536;&#26469;&#26500;&#24314;&#22270;&#24418;&#65292;&#22240;&#27492;&#26080;&#27861;&#24544;&#23454;&#22320;&#25429;&#25417;&#26080;&#38480;&#37325;&#22797;&#30340;&#27169;&#24335;&#21644;&#36828;&#36317;&#31163;&#30340;&#21407;&#23376;&#38388;&#30456;&#20114;&#20316;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#20010;&#21019;&#26032;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24314;&#35758;&#30452;&#25509;&#24314;&#27169;&#29289;&#29702;&#21407;&#29702;&#30340;&#30456;&#20114;&#20316;&#29992;&#21183;&#65292;&#32780;&#19981;&#20165;&#20165;&#20351;&#29992;&#36317;&#31163;&#65292;&#22914;&#35768;&#22810;&#29616;&#26377;&#26041;&#27861;&#25152;&#20570;&#30340;&#12290;&#36825;&#20123;&#21183;&#21253;&#25324;&#24211;&#20177;&#21183;&#65292;&#20262;&#25958;&#20998;&#25955;&#21183;&#21644;Pauli&#26021;&#21147;&#21183;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24314;&#27169;&#25152;&#26377;&#21407;&#23376;&#20043;&#38388;&#30340;&#23436;&#25972;&#21183;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#38468;&#36817;&#21407;&#23376;&#20043;&#38388;&#30340;&#21183;&#12290;&#36825;&#24471;&#30410;&#20110;&#25105;&#20204;&#29992;&#21487;&#35777;&#26126;&#30340;&#35823;&#24046;&#30028;&#36924;&#36817;&#26080;&#38480;&#21183;&#21644;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24320;&#21457;&#20102;...
&lt;/p&gt;
&lt;p&gt;
We study property prediction for crystal materials. A crystal structure consists of a minimal unit cell that is repeated infinitely in 3D space. How to accurately represent such repetitive structures in machine learning models remains unresolved. Current methods construct graphs by establishing edges only between nearby nodes, thereby failing to faithfully capture infinite repeating patterns and distant interatomic interactions. In this work, we propose several innovations to overcome these limitations. First, we propose to model physics-principled interatomic potentials directly instead of only using distances as in many existing methods. These potentials include the Coulomb potential, London dispersion potential, and Pauli repulsion potential. Second, we model the complete set of potentials among all atoms, instead of only between nearby atoms as in existing methods. This is enabled by our approximations of infinite potential summations with provable error bounds. We further develop 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#19981;&#21464;&#22240;&#26524;&#38598;&#35206;&#30422;&#26426;&#30340;&#31639;&#27861;&#65292;&#23427;&#36991;&#20813;&#20102;&#20135;&#29983;&#34394;&#20551;&#20851;&#32852;&#65292;&#21487;&#20197;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#35782;&#21035;&#24863;&#20852;&#36259;&#21464;&#37327;&#30340;&#22240;&#26524;&#29238;&#33410;&#28857;&#12290;</title><link>http://arxiv.org/abs/2306.04777</link><description>&lt;p&gt;
&#19981;&#21464;&#22240;&#26524;&#38598;&#35206;&#30422;&#26426;
&lt;/p&gt;
&lt;p&gt;
Invariant Causal Set Covering Machines. (arXiv:2306.04777v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04777
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#19981;&#21464;&#22240;&#26524;&#38598;&#35206;&#30422;&#26426;&#30340;&#31639;&#27861;&#65292;&#23427;&#36991;&#20813;&#20102;&#20135;&#29983;&#34394;&#20551;&#20851;&#32852;&#65292;&#21487;&#20197;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#35782;&#21035;&#24863;&#20852;&#36259;&#21464;&#37327;&#30340;&#22240;&#26524;&#29238;&#33410;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#35268;&#21017;&#30340;&#27169;&#22411;&#65292;&#22914;&#20915;&#31574;&#26641;&#65292;&#22240;&#20854;&#21487;&#35299;&#37322;&#30340;&#29305;&#24615;&#21463;&#21040;&#20174;&#19994;&#32773;&#30340;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#20135;&#29983;&#36825;&#31181;&#27169;&#22411;&#30340;&#23398;&#20064;&#31639;&#27861;&#24448;&#24448;&#23481;&#26131;&#21463;&#21040;&#34394;&#20551;&#20851;&#32852;&#30340;&#24433;&#21709;&#65292;&#22240;&#27492;&#19981;&#33021;&#20445;&#35777;&#25552;&#21462;&#30340;&#26159;&#20855;&#26377;&#22240;&#26524;&#20851;&#31995;&#30340;&#27934;&#35265;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;&#19981;&#21464;&#22240;&#26524;&#39044;&#27979;&#25991;&#29486;&#20013;&#30340;&#24605;&#24819;&#65292;&#25552;&#20986;&#20102;&#19981;&#21464;&#30340;&#22240;&#26524;&#38598;&#35206;&#30422;&#26426;&#65292;&#36825;&#26159;&#19968;&#31181;&#32463;&#20856;&#30340;&#38598;&#35206;&#30422;&#26426;&#31639;&#27861;&#30340;&#25193;&#23637;&#65292;&#29992;&#20110;&#20108;&#20540;&#35268;&#21017;&#30340;&#21512;&#21462;/&#26512;&#21462;&#65292;&#21487;&#20197;&#35777;&#26126;&#23427;&#36991;&#20813;&#20102;&#34394;&#20551;&#20851;&#32852;&#12290;&#25105;&#20204;&#29702;&#35770;&#19978;&#21644;&#23454;&#36341;&#19978;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#35782;&#21035;&#24863;&#20852;&#36259;&#21464;&#37327;&#30340;&#22240;&#26524;&#29238;&#33410;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rule-based models, such as decision trees, appeal to practitioners due to their interpretable nature. However, the learning algorithms that produce such models are often vulnerable to spurious associations and thus, they are not guaranteed to extract causally-relevant insights. In this work, we build on ideas from the invariant causal prediction literature to propose Invariant Causal Set Covering Machines, an extension of the classical Set Covering Machine algorithm for conjunctions/disjunctions of binary-valued rules that provably avoids spurious associations. We demonstrate both theoretically and empirically that our method can identify the causal parents of a variable of interest in polynomial time.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LSTM&#30340;&#20998;&#23618;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;(LHVAE)&#65292;&#30740;&#31350;&#24773;&#24863;&#26465;&#20214;&#23545;&#26059;&#24459;&#21644;&#22768;&#32534;&#37197;&#30340;&#24433;&#21709;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#29983;&#25104;&#30340;&#21644;&#22768;&#36136;&#37327;&#24182;&#25429;&#25417;&#20102;&#20016;&#23500;&#30340;&#21644;&#24358;&#36827;&#34892;&#21464;&#21270;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#24863;&#30693;&#24773;&#24863;&#34920;&#36798;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.03718</link><description>&lt;p&gt;
&#24102;&#26377;&#20998;&#23618;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#24773;&#24863;&#26465;&#20214;&#26059;&#24459;&#21644;&#22768;&#32534;&#37197;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Emotion-Conditioned Melody Harmonization with Hierarchical Variational Autoencoder. (arXiv:2306.03718v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03718
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LSTM&#30340;&#20998;&#23618;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;(LHVAE)&#65292;&#30740;&#31350;&#24773;&#24863;&#26465;&#20214;&#23545;&#26059;&#24459;&#21644;&#22768;&#32534;&#37197;&#30340;&#24433;&#21709;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#29983;&#25104;&#30340;&#21644;&#22768;&#36136;&#37327;&#24182;&#25429;&#25417;&#20102;&#20016;&#23500;&#30340;&#21644;&#24358;&#36827;&#34892;&#21464;&#21270;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#24863;&#30693;&#24773;&#24863;&#34920;&#36798;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#26059;&#24459;&#21644;&#22768;&#32534;&#37197;&#27169;&#22411;&#22312;&#25552;&#39640;&#29983;&#25104;&#30340;&#21644;&#22768;&#36136;&#37327;&#26041;&#38754;&#21462;&#24471;&#20102;&#24456;&#22823;&#36827;&#23637;&#65292;&#20294;&#22823;&#22810;&#25968;&#24573;&#30053;&#20102;&#38899;&#20048;&#20013;&#30340;&#24773;&#24863;&#12290;&#21516;&#26102;&#65292;&#20197;&#21069;&#30340;&#26041;&#27861;&#25152;&#29983;&#25104;&#30340;&#21644;&#22768;&#21464;&#21270;&#24615;&#19981;&#36275;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;LSTM&#30340;&#20998;&#23618;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;(LHVAE)&#65292;&#30740;&#31350;&#24773;&#24863;&#26465;&#20214;&#23545;&#26059;&#24459;&#21644;&#22768;&#32534;&#37197;&#30340;&#24433;&#21709;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#29983;&#25104;&#30340;&#21644;&#22768;&#36136;&#37327;&#24182;&#25429;&#25417;&#20102;&#20016;&#23500;&#30340;&#21644;&#24358;&#36827;&#34892;&#21464;&#21270;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;LHVAE&#22312;&#19981;&#21516;&#23618;&#27425;&#65288;&#20048;&#31456;&#21644;&#23567;&#33410;&#32423;&#21035;&#65289;&#19978;&#34701;&#21512;&#20102;&#28508;&#22312;&#21464;&#37327;&#21644;&#24773;&#24863;&#26465;&#20214;&#65292;&#20197;&#24314;&#27169;&#20840;&#23616;&#21644;&#23616;&#37096;&#38899;&#20048;&#23646;&#24615;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#22312;&#27599;&#20010;&#27493;&#39588;&#20013;&#24341;&#20837;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#26059;&#24459;&#19978;&#19979;&#25991;&#21521;&#37327;&#65292;&#20197;&#26356;&#22909;&#22320;&#23398;&#20064;&#26059;&#24459;&#21644;&#21644;&#22768;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#12290;&#23458;&#35266;&#35780;&#20272;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#32988;&#36807;&#20102;&#20854;&#20182;&#22522;&#20110;LSTM&#30340;&#27169;&#22411;&#12290;&#36890;&#36807;&#20027;&#35266;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#31526;&#21512;&#32473;&#23450;&#24773;&#24863;&#30340;&#21644;&#22768;&#36827;&#34892;&#65292;&#24182;&#22312;&#24863;&#30693;&#24773;&#24863;&#34920;&#36798;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing melody harmonization models have made great progress in improving the quality of generated harmonies, but most of them ignored the emotions beneath the music. Meanwhile, the variability of harmonies generated by previous methods is insufficient. To solve these problems, we propose a novel LSTM-based Hierarchical Variational Auto-Encoder (LHVAE) to investigate the influence of emotional conditions on melody harmonization, while improving the quality of generated harmonies and capturing the abundant variability of chord progressions. Specifically, LHVAE incorporates latent variables and emotional conditions at different levels (piece- and bar-level) to model the global and local music properties. Additionally, we introduce an attention-based melody context vector at each step to better learn the correspondence between melodies and harmonies. Experimental results of the objective evaluation show that our proposed model outperforms other LSTM-based models. Through subjective evalu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#20998;&#23376;&#23545;&#25509;&#21644;&#26426;&#22120;&#23398;&#20064;&#22238;&#24402;&#26041;&#27861;&#65292;&#31579;&#36873;&#20986;&#38024;&#23545; SARS-CoV-2&#30340;&#20027;&#35201;&#34507;&#30333;&#37238;3CL&#28508;&#22312;&#27835;&#30103;&#33647;&#29289;&#12290;&#20854;&#20013;&#65292;&#20915;&#31574;&#26641;&#22238;&#24402;&#65288;DTR&#65289;&#27169;&#22411;&#20855;&#26377;&#25913;&#36827;&#30340;&#32479;&#35745;&#25514;&#26045;R2&#21644;RMSE&#65292;&#26377;&#21161;&#20110;&#35782;&#21035;&#20855;&#26377;&#39640;&#32467;&#21512;&#20146;&#21644;&#21147;&#21644;&#26377;&#21033;&#30340;&#32467;&#21512;&#33021;&#30340;&#33647;&#29289;&#12290;</title><link>http://arxiv.org/abs/2305.18088</link><description>&lt;p&gt;
&#21033;&#29992;&#20998;&#23376;&#23545;&#25509;&#21644;&#26426;&#22120;&#23398;&#20064;&#22238;&#24402;&#26041;&#27861;&#30340;&#33647;&#29289;&#37325;&#29992;&#20197;&#38774;&#21521;COVID-19 3CL Protease
&lt;/p&gt;
&lt;p&gt;
Drug Repurposing Targeting COVID-19 3CL Protease using Molecular Docking and Machine Learning Regression Approach. (arXiv:2305.18088v2 [q-bio.BM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#20998;&#23376;&#23545;&#25509;&#21644;&#26426;&#22120;&#23398;&#20064;&#22238;&#24402;&#26041;&#27861;&#65292;&#31579;&#36873;&#20986;&#38024;&#23545; SARS-CoV-2&#30340;&#20027;&#35201;&#34507;&#30333;&#37238;3CL&#28508;&#22312;&#27835;&#30103;&#33647;&#29289;&#12290;&#20854;&#20013;&#65292;&#20915;&#31574;&#26641;&#22238;&#24402;&#65288;DTR&#65289;&#27169;&#22411;&#20855;&#26377;&#25913;&#36827;&#30340;&#32479;&#35745;&#25514;&#26045;R2&#21644;RMSE&#65292;&#26377;&#21161;&#20110;&#35782;&#21035;&#20855;&#26377;&#39640;&#32467;&#21512;&#20146;&#21644;&#21147;&#21644;&#26377;&#21033;&#30340;&#32467;&#21512;&#33021;&#30340;&#33647;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
COVID-19&#30123;&#24773;&#24050;&#32463;&#25104;&#20026;&#20840;&#29699;&#20581;&#24247;&#21361;&#26426;&#65292;&#36843;&#20999;&#38656;&#35201;&#24555;&#36895;&#37492;&#23450;&#28508;&#22312;&#30340;&#27835;&#30103;&#33647;&#29289;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#33647;&#29289;&#37325;&#29992;&#26159;&#30465;&#26102;&#30465;&#21147;&#30340;&#21807;&#19968;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;Zinc&#25968;&#25454;&#24211;&#23545;&#20840;&#29699;&#24050;&#25209;&#20934;&#65288;&#21253;&#25324;FDA&#25209;&#20934;&#65289;&#30340;5903&#31181;&#33647;&#29289;&#36827;&#34892;&#31579;&#36873;&#65292;&#20316;&#20026;&#28508;&#22312;&#30340;COVID-19&#27835;&#30103;&#33647;&#29289;&#65292;&#20197;&#38774;&#21521;SARS-CoV-2&#30340;&#20027;&#35201;&#34507;&#30333;&#37238;3CL&#12290;&#25105;&#20204;&#20351;&#29992;Autodock-Vina&#36827;&#34892;&#20998;&#23376;&#23545;&#25509;&#65292;&#26816;&#26597;&#33647;&#29289;&#20998;&#23376;&#30340;&#21151;&#25928;&#12290;&#20026;&#20102;&#25552;&#39640;&#33647;&#29289;&#37325;&#29992;&#30340;&#25928;&#29575;&#65292;&#25105;&#20204;&#37319;&#29992;&#20915;&#31574;&#26641;&#12289;&#39069;&#22806;&#26641;&#12289;MLP&#12289;KNN&#12289;XGBoost&#21644;&#26799;&#24230;&#25552;&#21319;&#31561;&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#22238;&#24402;&#26041;&#27861;&#24314;&#27169;&#32467;&#21512;&#33647;&#29289;&#30340;&#32467;&#21512;&#20146;&#21644;&#21147;&#12290;&#35745;&#31639;&#32467;&#26524;&#34920;&#26126;&#65292;&#20915;&#31574;&#26641;&#22238;&#24402;&#65288;DTR&#65289;&#27169;&#22411;&#20855;&#26377;&#25913;&#36827;&#30340;&#32479;&#35745;&#25514;&#26045;R2&#21644;RMSE&#12290;&#36825;&#20123;&#27169;&#25311;&#32467;&#26524;&#26377;&#21161;&#20110;&#35782;&#21035;&#20855;&#26377;&#39640;&#32467;&#21512;&#20146;&#21644;&#21147;&#21644;&#26377;&#21033;&#30340;&#32467;&#21512;&#33021;&#30340;&#33647;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;
The COVID-19 pandemic has created a global health crisis, driving the need for the rapid identification of potential therapeutics. To meet this challenge, drug repurposing is the only solution with saving cost and time. In this study, we used the Zinc database to screen the world-approved including FDA-approved 5903 drugs for repurposing as potential COVID-19 treatments targeting the main protease 3CL of SARS-CoV-2. We performed molecular docking using Autodock-Vina to check the efficacy of drug molecules. To enhance the efficiency of drug repurposing approach, we modeled the binding affinities using several machine learning regression approaches for QSAR modeling such as decision tree, extra trees, MLP, KNN, XGBoost, and gradient boosting. The computational results demonstrated that Decision Tree Regression (DTR) model has improved statistical measures of R2 and RMSE. These simulated results helped to identify drugs with high binding affinity and favorable binding energies. From the s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22810;&#20010;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#20013;&#26500;&#24314;AUC&#20248;&#21270;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#23454;&#36341;&#21644;&#29702;&#35770;&#19978;&#37117;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2305.15776</link><description>&lt;p&gt;
&#22810;&#20010;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#30340;AUC&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
AUC Optimization from Multiple Unlabeled Datasets. (arXiv:2305.15776v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15776
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22810;&#20010;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#20013;&#26500;&#24314;AUC&#20248;&#21270;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#23454;&#36341;&#21644;&#29702;&#35770;&#19978;&#37117;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24369;&#30417;&#30563;&#23398;&#20064;&#26088;&#22312;&#22312;&#32570;&#20047;&#23436;&#32654;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#36171;&#20104;&#26426;&#22120;&#23398;&#20064;&#33021;&#21147;&#65292;&#36825;&#24341;&#36215;&#20102;&#30740;&#31350;&#20154;&#21592;&#30340;&#26497;&#22823;&#20851;&#27880;&#12290;&#22312;&#21508;&#31181;&#31867;&#22411;&#30340;&#24369;&#30417;&#30563;&#23398;&#20064;&#20013;&#65292;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#26696;&#20363;&#20043;&#19968;&#26159;&#20165;&#20102;&#35299;&#31867;&#21035;&#20808;&#39564;&#30693;&#35782;&#30340;&#22810;&#20010;&#26410;&#26631;&#35760;(U)&#25968;&#25454;&#38598;&#30340;&#23398;&#20064;&#25110;&#31216;&#20026;U^m&#23398;&#20064;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#22810;&#20010;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#20013;&#26500;&#24314;&#26368;&#22823;&#21270;&#20998;&#31867;&#22120;&#25104;&#23545;&#25490;&#21517;&#33021;&#21147;&#30340;AUC (ROC&#26354;&#32447;&#19979;&#38754;&#31215;) &#20248;&#21270;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;U^m-AUC&#65292;&#19968;&#31181;&#23558;U^m&#25968;&#25454;&#36716;&#25442;&#20026;&#22810;&#26631;&#35760;AUC&#20248;&#21270;&#38382;&#39064;&#24182;&#33021;&#22815;&#26377;&#25928;&#35757;&#32451;&#30340;AUC&#20248;&#21270;&#26041;&#27861;&#12290;&#25105;&#20204;&#29702;&#35770;&#19978;&#21644;&#23454;&#35777;&#19978;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;U^m-AUC&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Weakly supervised learning aims to empower machine learning when the perfect supervision is unavailable, which has drawn great attention from researchers. Among various types of weak supervision, one of the most challenging cases is to learn from multiple unlabeled (U) datasets with only a little knowledge of the class priors, or U$^m$ learning for short. In this paper, we study the problem of building an AUC (area under ROC curve) optimization model from multiple unlabeled datasets, which maximizes the pairwise ranking ability of the classifier. We propose U$^m$-AUC, an AUC optimization approach that converts the U$^m$ data into a multi-label AUC optimization problem, and can be trained efficiently. We show that the proposed U$^m$-AUC is effective theoretically and empirically.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Evaluation on Medical Datasets Over Time&#65288;EMDOT&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#27169;&#25311;&#27599;&#20010;&#26102;&#38388;&#28857;&#30340;&#22521;&#35757;&#36807;&#31243;&#24182;&#23545;&#26410;&#26469;&#26102;&#38388;&#28857;&#19978;&#30340;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#65292;&#35780;&#20272;&#20102;&#19981;&#21516;&#26102;&#38388;&#27573;&#24615;&#33021;&#30340;&#24046;&#24322;&#65292;&#23545;&#21307;&#23398;&#39046;&#22495;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#20102;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2305.13426</link><description>&lt;p&gt;
&#23545;&#21307;&#23398;&#25968;&#25454;&#38598;&#20013;&#30340;&#27169;&#22411;&#34920;&#29616;&#36827;&#34892;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Evaluating Model Performance in Medical Datasets Over Time. (arXiv:2305.13426v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13426
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Evaluation on Medical Datasets Over Time&#65288;EMDOT&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#27169;&#25311;&#27599;&#20010;&#26102;&#38388;&#28857;&#30340;&#22521;&#35757;&#36807;&#31243;&#24182;&#23545;&#26410;&#26469;&#26102;&#38388;&#28857;&#19978;&#30340;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#65292;&#35780;&#20272;&#20102;&#19981;&#21516;&#26102;&#38388;&#27573;&#24615;&#33021;&#30340;&#24046;&#24322;&#65292;&#23545;&#21307;&#23398;&#39046;&#22495;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#20102;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#20445;&#20581;&#31995;&#32479;&#20013;&#37096;&#32626;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#24517;&#39035;&#38754;&#23545;&#19981;&#26029;&#28436;&#21464;&#30340;&#29615;&#22659;&#20013;&#33719;&#24471;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#25552;&#20986;&#36825;&#26679;&#30340;&#27169;&#22411;&#30340;&#30740;&#31350;&#20154;&#21592;&#36890;&#24120;&#20197;&#19982;&#26102;&#38388;&#26080;&#20851;&#30340;&#26041;&#24335;&#36827;&#34892;&#35780;&#20272;&#65292;&#26681;&#25454;&#22312;&#25972;&#20010;&#30740;&#31350;&#26102;&#38388;&#27573;&#38543;&#26426;&#25277;&#21462;&#30340;&#24739;&#32773;&#26469;&#25286;&#20998;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Evaluation on Medical Datasets Over Time&#65288;EMDOT&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#35780;&#20272;&#27169;&#22411;&#22312;&#19981;&#21516;&#26102;&#38388;&#27573;&#24615;&#33021;&#30340;&#24046;&#24322;&#12290;&#21463;&#21040;&#21453;&#21521;&#27979;&#35797;&#27010;&#24565;&#30340;&#21551;&#21457;&#65292;EMDOT&#27169;&#25311;&#23454;&#36341;&#32773;&#21487;&#33021;&#33021;&#22815;&#22312;&#27599;&#20010;&#26102;&#38388;&#28857;&#25191;&#34892;&#30340;&#28508;&#22312;&#22521;&#35757;&#36807;&#31243;&#65292;&#24182;&#22312;&#25152;&#26377;&#26410;&#26469;&#26102;&#38388;&#28857;&#19978;&#35780;&#20272;&#25152;&#24471;&#21040;&#30340;&#27169;&#22411;&#12290;&#22312;&#20845;&#20010;&#19981;&#21516;&#30340;&#21307;&#30103;&#25968;&#25454;&#28304;&#65288;&#34920;&#26684;&#21644;&#25104;&#20687;&#65289;&#19978;&#35780;&#20272;&#32447;&#24615;&#21644;&#26356;&#22797;&#26434;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20381;&#36182;&#20110;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;&#25152;&#26377;&#21382;&#21490;&#25968;&#25454;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#21487;&#33021;&#26159;&#29702;&#24819;&#30340;&#65292;&#32780;&#22312;&#20854;&#20182;&#24773;&#20917;&#19979;&#20351;&#29992;&#26368;&#36817;&#25968;&#25454;&#30340;&#31383;&#21475;&#21487;&#33021;&#26159;&#26377;&#21033;&#30340;&#12290;&#22312;&#27169;&#22411;&#31361;&#28982;&#21463;&#21040;&#24433;&#21709;&#30340;&#25968;&#25454;&#38598;&#20013;&#65292;&#20351;&#29992;&#22312;&#30456;&#23545;&#36739;&#36817;&#30340;&#25968;&#25454;&#31383;&#21475;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#26159;&#26377;&#24110;&#21161;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) models deployed in healthcare systems must face data drawn from continually evolving environments. However, researchers proposing such models typically evaluate them in a time-agnostic manner, splitting datasets according to patients sampled randomly throughout the entire study time period. This work proposes the Evaluation on Medical Datasets Over Time (EMDOT) framework, which evaluates the performance of a model class across time. Inspired by the concept of backtesting, EMDOT simulates possible training procedures that practitioners might have been able to execute at each point in time and evaluates the resulting models on all future time points. Evaluating both linear and more complex models on six distinct medical data sources (tabular and imaging), we show how depending on the dataset, using all historical data may be ideal in many cases, whereas using a window of the most recent data could be advantageous in others. In datasets where models suffer from sudde
&lt;/p&gt;</description></item><item><title>AlignAtt&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;SimulST&#31574;&#30053;&#65292;&#20351;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#38899;&#39057;&#32763;&#35793;&#23545;&#40784;&#26469;&#25351;&#23548;&#27169;&#22411;&#65292;&#22312;BLEU&#21644;&#24310;&#36831;&#26041;&#38754;&#22343;&#20248;&#20110;&#20043;&#21069;&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2305.11408</link><description>&lt;p&gt;
AlignAtt&#65306;&#20351;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#38899;&#39057;&#32763;&#35793;&#23545;&#40784;&#20316;&#20026;&#21516;&#26102;&#35821;&#38899;&#32763;&#35793;&#30340;&#25351;&#23548;
&lt;/p&gt;
&lt;p&gt;
AlignAtt: Using Attention-based Audio-Translation Alignments as a Guide for Simultaneous Speech Translation. (arXiv:2305.11408v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11408
&lt;/p&gt;
&lt;p&gt;
AlignAtt&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;SimulST&#31574;&#30053;&#65292;&#20351;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#38899;&#39057;&#32763;&#35793;&#23545;&#40784;&#26469;&#25351;&#23548;&#27169;&#22411;&#65292;&#22312;BLEU&#21644;&#24310;&#36831;&#26041;&#38754;&#22343;&#20248;&#20110;&#20043;&#21069;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27880;&#24847;&#21147;&#26159;&#24403;&#20170;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#26368;&#24120;&#29992;&#30340;&#26550;&#26500;&#30340;&#26680;&#24515;&#26426;&#21046;&#65292;&#24182;&#24050;&#20174;&#35768;&#22810;&#35282;&#24230;&#36827;&#34892;&#20998;&#26512;&#65292;&#21253;&#25324;&#20854;&#22312;&#26426;&#22120;&#32763;&#35793;&#30456;&#20851;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;&#36825;&#20123;&#30740;&#31350;&#20013;&#65292;&#27880;&#24847;&#21147;&#22312;&#36755;&#20837;&#25991;&#26412;&#34987;&#26367;&#25442;&#20026;&#38899;&#39057;&#29255;&#27573;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#26159;&#33719;&#21462;&#26377;&#20851;&#21333;&#35789;&#23545;&#40784;&#30340;&#26377;&#29992;&#20449;&#24687;&#30340;&#19968;&#31181;&#26041;&#24335;&#65292;&#20363;&#22914;&#35821;&#38899;&#32763;&#35793;&#65288;ST&#65289;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AlignAtt&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#21516;&#26102;ST&#65288;SimulST&#65289;&#31574;&#30053;&#65292;&#23427;&#21033;&#29992;&#27880;&#24847;&#21147;&#20449;&#24687;&#26469;&#29983;&#25104;&#28304;-&#30446;&#26631;&#23545;&#40784;&#65292;&#20197;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#25351;&#23548;&#27169;&#22411;&#12290;&#36890;&#36807;&#23545;MuST-C v1.0&#30340;8&#31181;&#35821;&#35328;&#23545;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#32447;&#19979;&#35757;&#32451;&#30340;&#27169;&#22411;&#19978;&#24212;&#29992;&#20808;&#21069;&#30340;&#26368;&#26032;SimulST&#31574;&#30053;&#65292;AlignAtt&#22312;BLEU&#26041;&#38754;&#33719;&#24471;&#20102;2&#20010;&#20998;&#25968;&#30340;&#25552;&#39640;&#65292;&#24182;&#19988;8&#31181;&#35821;&#35328;&#30340;&#24310;&#36831;&#32553;&#20943;&#22312;0.5&#31186;&#21040;0.8&#31186;&#20043;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention is the core mechanism of today's most used architectures for natural language processing and has been analyzed from many perspectives, including its effectiveness for machine translation-related tasks. Among these studies, attention resulted to be a useful source of information to get insights about word alignment also when the input text is substituted with audio segments, as in the case of the speech translation (ST) task. In this paper, we propose AlignAtt, a novel policy for simultaneous ST (SimulST) that exploits the attention information to generate source-target alignments that guide the model during inference. Through experiments on the 8 language pairs of MuST-C v1.0, we show that AlignAtt outperforms previous state-of-the-art SimulST policies applied to offline-trained models with gains in terms of BLEU of 2 points and latency reductions ranging from 0.5s to 0.8s across the 8 languages.
&lt;/p&gt;</description></item><item><title>MaxViT-UNet&#26159;&#22522;&#20110;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#28151;&#21512;&#35270;&#35273;&#21464;&#21387;&#22120;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#65292;&#22810;&#36724;&#33258;&#25105;&#20851;&#27880;&#26426;&#21046;&#22312;&#27599;&#20010;&#35299;&#30721;&#22120;&#38454;&#27573;&#26377;&#21161;&#20110;&#26356;&#26377;&#25928;&#22320;&#21306;&#20998;&#23545;&#35937;&#21644;&#32972;&#26223;&#21306;&#22495;&#12290;</title><link>http://arxiv.org/abs/2305.08396</link><description>&lt;p&gt;
MaxViT-UNet: &#22810;&#36724;&#27880;&#24847;&#21147;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
MaxViT-UNet: Multi-Axis Attention for Medical Image Segmentation. (arXiv:2305.08396v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08396
&lt;/p&gt;
&lt;p&gt;
MaxViT-UNet&#26159;&#22522;&#20110;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#28151;&#21512;&#35270;&#35273;&#21464;&#21387;&#22120;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#65292;&#22810;&#36724;&#33258;&#25105;&#20851;&#27880;&#26426;&#21046;&#22312;&#27599;&#20010;&#35299;&#30721;&#22120;&#38454;&#27573;&#26377;&#21161;&#20110;&#26356;&#26377;&#25928;&#22320;&#21306;&#20998;&#23545;&#35937;&#21644;&#32972;&#26223;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#21367;&#31215;&#31639;&#23376;&#30340;&#23616;&#37096;&#24615;&#36136;&#25233;&#21046;&#20102;CNNs&#25429;&#25417;&#20840;&#23616;&#21644;&#38271;&#31243;&#20132;&#20114;&#12290;&#26368;&#36817;&#65292;Transformer&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#31038;&#21306;&#21644;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#21464;&#24471;&#27969;&#34892;&#12290;&#20294;&#26159;&#65292;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#30340;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#21644;&#32570;&#20047;CNN&#31867;&#24402;&#32435;&#20559;&#24046;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MaxViT-UNet&#65292;&#19968;&#31181;&#22522;&#20110;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#28151;&#21512;&#35270;&#35273;&#21464;&#21387;&#22120;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#12290;&#25552;&#20986;&#30340;&#28151;&#21512;&#35299;&#30721;&#22120;&#65292;&#36824;&#22522;&#20110;MaxViT-block&#65292;&#26088;&#22312;&#22312;&#27599;&#20010;&#35299;&#30721;&#38454;&#27573;&#26368;&#23567;&#21270;&#35745;&#31639;&#36127;&#25285;&#19979;&#21033;&#29992;&#21367;&#31215;&#21644;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#30340;&#21147;&#37327;&#12290;&#27599;&#20010;&#35299;&#30721;&#22120;&#38454;&#27573;&#30340;&#22810;&#36724;&#33258;&#25105;&#20851;&#27880;&#26377;&#21161;&#20110;&#26356;&#26377;&#25928;&#22320;&#21306;&#20998;&#23545;&#35937;&#21644;&#32972;&#26223;&#21306;&#22495;&#12290;&#28151;&#21512;&#35299;&#30721;&#22120;&#22359;&#26368;&#21021;&#36890;&#36807;&#19978;&#37319;&#26679;&#20256;&#36755;&#20302;&#23618;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Convolutional neural networks have made significant strides in medical image analysis in recent years. However, the local nature of the convolution operator inhibits the CNNs from capturing global and long-range interactions. Recently, Transformers have gained popularity in the computer vision community and also medical image segmentation. But scalability issues of self-attention mechanism and lack of the CNN like inductive bias have limited their adoption. In this work, we present MaxViT-UNet, an Encoder-Decoder based hybrid vision transformer for medical image segmentation. The proposed hybrid decoder, also based on MaxViT-block, is designed to harness the power of convolution and self-attention mechanism at each decoding stage with minimal computational burden. The multi-axis self-attention in each decoder stage helps in differentiating between the object and background regions much more efficiently. The hybrid decoder block initially fuses the lower level features upsampled via tra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#28857;&#20113;&#30340;&#32479;&#35745;&#24418;&#24577;&#24314;&#27169;&#25216;&#26415;&#65292;&#25670;&#33073;&#20102;&#20256;&#32479;&#22522;&#20110;&#23545;&#24212;&#20851;&#31995;&#30340;&#26041;&#27861;&#30340;&#29942;&#39048;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#30340;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.05610</link><description>&lt;p&gt;
&#28857;&#20113;&#32593;&#32476;&#33021;&#23398;&#20064;&#35299;&#21078;&#32467;&#26500;&#30340;&#32479;&#35745;&#24418;&#24577;&#27169;&#22411;&#21527;?
&lt;/p&gt;
&lt;p&gt;
Can point cloud networks learn statistical shape models of anatomies?. (arXiv:2305.05610v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05610
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#28857;&#20113;&#30340;&#32479;&#35745;&#24418;&#24577;&#24314;&#27169;&#25216;&#26415;&#65292;&#25670;&#33073;&#20102;&#20256;&#32479;&#22522;&#20110;&#23545;&#24212;&#20851;&#31995;&#30340;&#26041;&#27861;&#30340;&#29942;&#39048;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#30340;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32479;&#35745;&#24418;&#24577;&#24314;&#27169;(SSM)&#26159;&#30740;&#31350;&#21644;&#37327;&#21270;&#20154;&#32676;&#20869;&#35299;&#21078;&#21464;&#21270;&#30340;&#26377;&#20215;&#20540;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#22522;&#20110;&#23545;&#24212;&#20851;&#31995;&#30340;SSM&#29983;&#25104;&#26041;&#27861;&#27599;&#28155;&#21152;&#19968;&#20010;&#26032;&#23545;&#35937;&#23601;&#38656;&#35201;&#32791;&#36153;&#26102;&#38388;&#30340;&#37325;&#26032;&#20248;&#21270;&#36807;&#31243;&#65292;&#20351;&#24471;&#25512;&#29702;&#36807;&#31243;&#22312;&#20020;&#24202;&#30740;&#31350;&#20013;&#21464;&#24471;&#19981;&#21487;&#34892;&#12290;&#27492;&#22806;&#65292;&#26500;&#24314;SSM&#38656;&#35201;&#36755;&#20837;&#23436;&#25972;&#30340;&#20960;&#20309;&#20195;&#29702;(&#20363;&#22914;&#65292;&#39640;&#20998;&#36776;&#29575;&#20108;&#36827;&#21046;&#20307;&#31215;&#25110;&#34920;&#38754;&#32593;&#26684;)&#20316;&#20026;&#36755;&#20837;&#24418;&#29366;&#12290;&#32780;&#26080;&#24207;&#30340;3D&#28857;&#20113;&#34920;&#31034;&#26356;&#23481;&#26131;&#20174;&#21508;&#31181;&#21307;&#23398;&#25104;&#20687;&#23454;&#36341;&#20013;&#33719;&#21462;(&#20363;&#22914;&#65292;&#38408;&#20540;&#22270;&#20687;&#21644;&#34920;&#38754;&#25195;&#25551;)&#12290;&#26368;&#36817;&#65292;&#28857;&#20113;&#28145;&#24230;&#32593;&#32476;&#22312;&#23398;&#20064;&#19981;&#21516;&#28857;&#20113;&#20219;&#21153;&#30340;&#25490;&#21015;&#19981;&#21464;&#29305;&#24449;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;(&#20363;&#22914;&#65292;&#23436;&#25104;&#12289;&#35821;&#20041;&#20998;&#21106;&#12289;&#20998;&#31867;)&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#20174;&#28857;&#20113;&#20013;&#23398;&#20064;SSM&#26041;&#38754;&#30340;&#24212;&#29992;&#36824;&#26410;&#34987;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Statistical Shape Modeling (SSM) is a valuable tool for investigating and quantifying anatomical variations within populations of anatomies. However, traditional correspondence-based SSM generation methods require a time-consuming re-optimization process each time a new subject is added to the cohort, making the inference process prohibitive for clinical research. Additionally, they require complete geometric proxies (e.g., high-resolution binary volumes or surface meshes) as input shapes to construct the SSM. Unordered 3D point cloud representations of shapes are more easily acquired from various medical imaging practices (e.g., thresholded images and surface scanning). Point cloud deep networks have recently achieved remarkable success in learning permutation-invariant features for different point cloud tasks (e.g., completion, semantic segmentation, classification). However, their application to learning SSM from point clouds is to-date unexplored. In this work, we demonstrate that 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;BERT&#21644;Query-Aware LSH&#25552;&#39640;&#38750;&#27491;&#24335;&#25991;&#26723;&#20013;&#20195;&#30721;&#31034;&#20363;&#25512;&#33616;&#30340;&#36136;&#37327;&#65292;&#37325;&#28857;&#20851;&#27880;&#20110;Stack Overflow&#19978;&#30340;Java&#32534;&#31243;&#35821;&#35328;&#12290;&#30740;&#31350;&#20351;&#29992;BERT&#23558;&#20195;&#30721;&#31034;&#20363;&#36716;&#25442;&#20026;&#25968;&#20540;&#21521;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.03017</link><description>&lt;p&gt;
&#20351;&#29992;BERT&#21644;Query-Aware LSH&#25552;&#39640;&#38750;&#27491;&#24335;&#25991;&#26723;&#20013;&#20195;&#30721;&#31034;&#20363;&#25512;&#33616;&#65306;&#19968;&#39033;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Improving Code Example Recommendations on Informal Documentation Using BERT and Query-Aware LSH: A Comparative Study. (arXiv:2305.03017v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03017
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;BERT&#21644;Query-Aware LSH&#25552;&#39640;&#38750;&#27491;&#24335;&#25991;&#26723;&#20013;&#20195;&#30721;&#31034;&#20363;&#25512;&#33616;&#30340;&#36136;&#37327;&#65292;&#37325;&#28857;&#20851;&#27880;&#20110;Stack Overflow&#19978;&#30340;Java&#32534;&#31243;&#35821;&#35328;&#12290;&#30740;&#31350;&#20351;&#29992;BERT&#23558;&#20195;&#30721;&#31034;&#20363;&#36716;&#25442;&#20026;&#25968;&#20540;&#21521;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#21644;&#26368;&#36817;&#19968;&#30452;&#22312;&#36827;&#34892;&#20195;&#30721;&#31034;&#20363;&#25512;&#33616;&#30340;&#30740;&#31350;&#65292;&#20197;&#24110;&#21161;&#24320;&#21457;&#20154;&#21592;&#23436;&#25104;&#36719;&#20214;&#24320;&#21457;&#20219;&#21153;&#12290;&#30001;&#20110;&#24320;&#21457;&#20154;&#21592;&#32463;&#24120;&#33457;&#36153;&#22823;&#37327;&#26102;&#38388;&#22312;&#20114;&#32852;&#32593;&#19978;&#23547;&#25214;&#30456;&#20851;&#30340;&#20195;&#30721;&#31034;&#20363;&#65292;&#21033;&#29992;&#24320;&#28304;&#39033;&#30446;&#21644;&#38750;&#27491;&#24335;&#25991;&#26723;&#12290;&#20026;&#20102;&#25214;&#21040;&#26377;&#29992;&#30340;&#20195;&#30721;&#31034;&#20363;&#65292;&#38750;&#27491;&#24335;&#25991;&#26723;&#65288;&#22914;Stack Overflow&#35752;&#35770;&#21644;&#35770;&#22363;&#65289;&#21487;&#20197;&#38750;&#24120;&#23453;&#36149;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#37325;&#28857;&#26159;Stack Overflow&#65292;&#23427;&#26159;&#36719;&#20214;&#24320;&#21457;&#20154;&#21592;&#35752;&#35770;&#19981;&#21516;&#20027;&#39064;&#30340;&#27969;&#34892;&#36164;&#28304;&#12290;&#20026;&#20102;&#25552;&#39640;&#25512;&#33616;&#20195;&#30721;&#31034;&#20363;&#30340;&#36136;&#37327;&#65292;&#25105;&#20204;&#25910;&#38598;&#24182;&#25512;&#33616;&#20102;Java&#32534;&#31243;&#35821;&#35328;&#20013;&#26368;&#20339;&#30340;&#20195;&#30721;&#31034;&#20363;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;BERT&#26469;&#36827;&#34892;&#22788;&#29702;&#65292;&#23427;&#26159;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20174;&#25991;&#26412;&#25968;&#25454;&#20013;&#25552;&#21462;&#35821;&#20041;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#27493;&#26159;&#20351;&#29992;BERT&#23558;&#20195;&#30721;&#31034;&#20363;&#36716;&#25442;&#20026;&#25968;&#20540;&#21521;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The study of code example recommendation has been conducted extensively in the past and recently in order to assist developers in their software development tasks. This is because developers often spend significant time searching for relevant code examples on the internet, utilizing open-source projects and informal documentation. For finding useful code examples, informal documentation, such as Stack Overflow discussions and forums, can be invaluable. We have focused our research on Stack Overflow, which is a popular resource for discussing different topics among software developers. For increasing the quality of the recommended code examples, we have collected and recommended the best code examples in the Java programming language. We have utilized BERT in our approach, which is a Large Language Model (LLM) for text representation that can effectively extract semantic information from textual data. Our first step involved using BERT to convert code examples into numerical vectors. Su
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#27979;&#30340;&#36172;&#21338;&#31574;&#30053;&#26469;&#35299;&#20915;&#39640;&#32500;&#25110;&#32467;&#26500;&#21270;&#25968;&#25454;&#19979;&#38750;&#21442;&#25968;&#21452;&#26679;&#26412;&#21644;&#29420;&#31435;&#24615;&#26816;&#39564;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.00143</link><description>&lt;p&gt;
&#39034;&#24207;&#39044;&#27979;&#21452;&#26679;&#26412;&#21644;&#29420;&#31435;&#24615;&#26816;&#39564;
&lt;/p&gt;
&lt;p&gt;
Sequential Predictive Two-Sample and Independence Testing. (arXiv:2305.00143v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00143
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#27979;&#30340;&#36172;&#21338;&#31574;&#30053;&#26469;&#35299;&#20915;&#39640;&#32500;&#25110;&#32467;&#26500;&#21270;&#25968;&#25454;&#19979;&#38750;&#21442;&#25968;&#21452;&#26679;&#26412;&#21644;&#29420;&#31435;&#24615;&#26816;&#39564;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#39034;&#24207;&#38750;&#21442;&#25968;&#21452;&#26679;&#26412;&#21644;&#29420;&#31435;&#24615;&#26816;&#39564;&#30340;&#38382;&#39064;&#12290;&#39034;&#24207;&#26816;&#39564;&#22312;&#32447;&#22788;&#29702;&#25968;&#25454;&#65292;&#20801;&#35768;&#20351;&#29992;&#35266;&#23519;&#21040;&#30340;&#25968;&#25454;&#26469;&#20915;&#23450;&#26159;&#21542;&#20572;&#27490;&#24182;&#25298;&#32477;&#21407;&#20551;&#35774;&#65292;&#25110;&#22312;&#20445;&#25345;&#31867;&#22411;I&#38169;&#35823;&#25511;&#21046;&#30340;&#21516;&#26102;&#25910;&#38598;&#26356;&#22810;&#25968;&#25454;&#12290;&#25105;&#20204;&#24314;&#31435;&#22312;(&#38750;&#21442;&#25968;)&#27979;&#35797;&#36172;&#21338;&#21407;&#21017;&#20043;&#19978;&#65292;&#20854;&#20013;&#36172;&#24466;&#22312;&#26410;&#26469;&#35266;&#23519;&#20013;&#19979;&#27880;&#65292;&#20182;&#20204;&#30340;&#36130;&#23500;&#23545;&#35777;&#25454;&#21453;&#23545;&#21407;&#20551;&#35774;&#36827;&#34892;&#34913;&#37327;&#12290;&#26368;&#36817;&#24320;&#21457;&#30340;&#22522;&#20110;&#26680;&#30340;&#36172;&#21338;&#31574;&#30053;&#22312;&#31616;&#21333;&#20998;&#24067;&#19978;&#36890;&#24120;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#23545;&#20110;&#39640;&#32500;&#25110;&#32467;&#26500;&#21270;&#25968;&#25454;&#65288;&#22914;&#25991;&#26412;&#21644;&#22270;&#20687;&#65289;&#36873;&#25321;&#21512;&#36866;&#30340;&#26680;&#36890;&#24120;&#26159;&#26840;&#25163;&#30340;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#22522;&#20110;&#39044;&#27979;&#30340;&#36172;&#21338;&#31574;&#30053;&#65292;&#20381;&#36182;&#20110;&#20197;&#19979;&#20107;&#23454;&#65306;&#22914;&#26524;&#19968;&#20010;&#39034;&#24207;&#26356;&#26032;&#30340;&#39044;&#27979;&#22120;&#24320;&#22987;&#19968;&#33268;&#22320;&#30830;&#23450;(a)&#19968;&#20010;&#23454;&#20363;&#20174;&#21738;&#20010;&#20998;&#24067;&#20013;&#32472;&#21046;&#65292;&#25110;&#32773;(b)&#19968;&#20010;&#23454;&#20363;&#26159;&#20174;&#32852;&#21512;&#20998;&#24067;&#36824;&#26159;&#20174;&#36793;&#32536;&#20998;&#24067;&#30340;&#20056;&#31215;&#20013;&#32472;&#21046;&#30340;&#65292;&#21017;&#20998;&#24067;&#26159;&#19981;&#21516;&#25110;&#30456;&#20851;&#30340;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28789;&#27963;&#65292;&#24182;&#23545;&#22522;&#30784;&#25968;&#25454;&#20998;&#24067;&#21644;&#32500;&#24230;&#19981;&#21487;&#30693;&#65292;&#21516;&#26102;&#20445;&#25345;&#19968;&#23450;&#30340;&#26368;&#20248;&#24615;&#20445;&#35777;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#21644;&#23454;&#38469;&#25968;&#25454;&#19978;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#39034;&#24207;&#27979;&#35797;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problems of sequential nonparametric two-sample and independence testing. Sequential tests process data online and allow using observed data to decide whether to stop and reject the null hypothesis or to collect more data while maintaining type I error control. We build upon the principle of (nonparametric) testing by betting, where a gambler places bets on future observations and their wealth measures evidence against the null hypothesis. While recently developed kernel-based betting strategies often work well on simple distributions, selecting a suitable kernel for high-dimensional or structured data, such as text and images, is often nontrivial. To address this drawback, we design prediction-based betting strategies that rely on the following fact: if a sequentially updated predictor starts to consistently determine (a) which distribution an instance is drawn from, or (b) whether an instance is drawn from the joint distribution or the product of the marginal distributio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38598;&#25104;TCN-LSTM&#27169;&#22411;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#27169;&#22411;&#30340;&#32479;&#19968;&#26041;&#27861;&#65292;&#29992;&#20110;&#36710;&#36947;&#21464;&#25442;&#24847;&#22270;&#35782;&#21035;&#21644;&#34892;&#39542;&#29366;&#24577;&#39044;&#27979;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#25928;&#26524;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2304.13732</link><description>&lt;p&gt;
&#36890;&#36807;TCN-LSTM&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#27169;&#22411;&#23454;&#29616;&#36710;&#36947;&#21464;&#25442;&#24847;&#22270;&#35782;&#21035;&#21644;&#34892;&#39542;&#29366;&#24577;&#39044;&#27979;&#30340;&#32479;&#19968;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Unified Approach to Lane Change Intention Recognition and Driving Status Prediction through TCN-LSTM and Multi-Task Learning Models. (arXiv:2304.13732v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13732
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38598;&#25104;TCN-LSTM&#27169;&#22411;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#27169;&#22411;&#30340;&#32479;&#19968;&#26041;&#27861;&#65292;&#29992;&#20110;&#36710;&#36947;&#21464;&#25442;&#24847;&#22270;&#35782;&#21035;&#21644;&#34892;&#39542;&#29366;&#24577;&#39044;&#27979;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#25928;&#26524;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36710;&#36947;&#21464;&#25442;&#65288;LC&#65289;&#26159;&#19968;&#20010;&#36830;&#32493;&#21644;&#22797;&#26434;&#30340;&#25805;&#20316;&#36807;&#31243;&#12290;&#20934;&#30830;&#22320;&#26816;&#27979;&#21644;&#39044;&#27979;LC&#36807;&#31243;&#21487;&#20197;&#24110;&#21161;&#20132;&#36890;&#21442;&#19982;&#32773;&#26356;&#22909;&#22320;&#20102;&#35299;&#20854;&#21608;&#22260;&#29615;&#22659;&#65292;&#35782;&#21035;&#28508;&#22312;&#30340;LC&#23433;&#20840;&#38544;&#24739;&#65292;&#24182;&#25552;&#39640;&#20132;&#36890;&#23433;&#20840;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38598;&#25104;&#26102;&#24577;&#21367;&#31215;&#32593;&#32476;&#19982;&#38271;&#30701;&#26399;&#35760;&#24518;&#21333;&#20803;&#65288;TCN-LSTM&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#25429;&#25417;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#38271;&#26399;&#30456;&#20851;&#24615;&#12290;&#27492;&#22806;&#65292;&#24320;&#21457;&#20102;&#19977;&#20010;&#22810;&#20219;&#21153;&#27169;&#22411;&#65288;MTL-LSTM&#12289;MTL-TCN&#12289;MTL-TCN-LSTM&#65289;&#26469;&#25429;&#25417;&#36755;&#20986;&#25351;&#26631;&#20043;&#38388;&#30340;&#20869;&#22312;&#20851;&#31995;&#12290;&#36827;&#19968;&#27493;&#65292;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#29992;&#20110;LC&#24847;&#22270;&#35782;&#21035;&#21644;&#34892;&#39542;&#29366;&#24577;&#39044;&#27979;&#65288;LC-IR-SP&#65289;&#30340;&#32479;&#19968;&#24314;&#27169;&#26694;&#26550;&#12290;&#20026;&#20102;&#39564;&#35777;&#25152;&#25552;&#20986;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20174;CitySim&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#20102;1023&#20010;&#36710;&#36742;&#36712;&#36857;&#12290;&#20351;&#29992;Pearson&#31995;&#25968;&#26469;&#35780;&#20215;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lane change (LC) is a continuous and complex operation process. Accurately detecting and predicting LC processes can help traffic participants better understand their surrounding environment, recognize potential LC safety hazards, and improve traffic safety. This present paper focuses on LC processes, developing an LC intention recognition (LC-IR) model and an LC status prediction (LC-SP) model. A novel ensemble temporal convolutional network with Long Short-Term Memory units (TCN-LSTM) is first proposed to capture long-range dependencies in sequential data. Then, three multi-task models (MTL-LSTM, MTL-TCN, MTL-TCN -LSTM) are developed to capture the intrinsic relationship among output indicators. Furthermore, a unified modeling framework for LC intention recognition and driving status prediction (LC-IR-SP) is developed. To validate the performance of the proposed models, a total number of 1023 vehicle trajectories is extracted from the CitySim dataset. The Pearson coefficient is emplo
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#22522;&#20110;&#38376;&#25511;&#37327;&#23376;&#35745;&#31639;&#26426;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#21442;&#25968;&#21270;&#30340;&#37327;&#23376;&#30005;&#36335;&#26469;&#35299;&#20915;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#35780;&#20272;&#20102;&#20854;&#28508;&#21147;&#12290;&#26368;&#32456;&#24635;&#32467;&#20102;&#24320;&#21457;&#28145;&#24230;&#37327;&#23376;&#23398;&#20064;&#30340;&#21069;&#26223;&#21644;&#32467;&#35770;&#12290;</title><link>http://arxiv.org/abs/2304.10159</link><description>&lt;p&gt;
&#22522;&#20110;&#28151;&#21512;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning Using Hybrid Quantum Neural Network. (arXiv:2304.10159v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10159
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#22522;&#20110;&#38376;&#25511;&#37327;&#23376;&#35745;&#31639;&#26426;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#21442;&#25968;&#21270;&#30340;&#37327;&#23376;&#30005;&#36335;&#26469;&#35299;&#20915;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#35780;&#20272;&#20102;&#20854;&#28508;&#21147;&#12290;&#26368;&#32456;&#24635;&#32467;&#20102;&#24320;&#21457;&#28145;&#24230;&#37327;&#23376;&#23398;&#20064;&#30340;&#21069;&#26223;&#21644;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#35745;&#31639;&#23545;&#20110;&#20419;&#36827;&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22788;&#29702;&#26356;&#39640;&#25968;&#25454;&#32500;&#24230;&#25110;&#20943;&#23569;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#24635;&#20307;&#35757;&#32451;&#21442;&#25968;&#30340;&#38480;&#21046;&#20855;&#26377;&#24378;&#28872;&#30340;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#22522;&#20110;&#38376;&#25511;&#37327;&#23376;&#35745;&#31639;&#26426;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#21442;&#25968;&#21270;&#30340;&#37327;&#23376;&#30005;&#36335;&#26469;&#35299;&#20915;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#37319;&#29992;&#28145;&#24230; Q-Learning &#26041;&#27861;&#12290;&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#20854;&#28508;&#21147;&#12290;&#22240;&#27492;&#65292;&#35774;&#35745;&#24182;&#22521;&#35757;&#20102;&#19968;&#20010;&#22522;&#20110;&#26368;&#26032;&#30340; Qiskit &#21644; PyTorch &#26694;&#26550;&#30340;&#26032;&#22411; PQC&#65292;&#20197;&#19982;&#23436;&#20840;&#32463;&#20856;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#27604;&#36739;&#65292;&#24102;&#25110;&#19981;&#24102;&#38598;&#25104; PQC&#12290;&#30740;&#31350;&#26368;&#21518;&#24635;&#32467;&#20102;&#20854;&#20851;&#20110;&#24320;&#21457;&#28145;&#24230;&#37327;&#23376;&#23398;&#20064;&#35299;&#20915;&#36855;&#23467;&#38382;&#39064;&#25110;&#20854;&#20182;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#21069;&#26223;&#21644;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum computation has a strong implication for advancing the current limitation of machine learning algorithms to deal with higher data dimensions or reducing the overall training parameters for a deep neural network model. Based on a gate-based quantum computer, a parameterized quantum circuit was designed to solve a model-free reinforcement learning problem with the deep-Q learning method. This research has investigated and evaluated its potential. Therefore, a novel PQC based on the latest Qiskit and PyTorch framework was designed and trained to compare with a full-classical deep neural network with and without integrated PQC. At the end of the research, the research draws its conclusion and prospects on developing deep quantum learning in solving a maze problem or other reinforcement learning problems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;AI&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#25351;&#20986;&#32570;&#20047;AI&#20844;&#24179;&#24615;&#20250;&#21152;&#28145;&#20559;&#35265;&#25104;&#20026;&#31038;&#20250;&#21387;&#21147;&#22240;&#32032;&#65292;&#21487;&#33021;&#23545;&#31038;&#20250;&#20135;&#29983;&#38271;&#26399;&#24433;&#21709;&#65292;&#22240;&#27492;&#38656;&#35201;&#23547;&#27714;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2304.09826</link><description>&lt;p&gt;
AI&#30340;&#20844;&#24179;&#24615;&#21450;&#20854;&#23545;&#31038;&#20250;&#30340;&#38271;&#26399;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Fairness in AI and Its Long-Term Implications on Society. (arXiv:2304.09826v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09826
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;AI&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#25351;&#20986;&#32570;&#20047;AI&#20844;&#24179;&#24615;&#20250;&#21152;&#28145;&#20559;&#35265;&#25104;&#20026;&#31038;&#20250;&#21387;&#21147;&#22240;&#32032;&#65292;&#21487;&#33021;&#23545;&#31038;&#20250;&#20135;&#29983;&#38271;&#26399;&#24433;&#21709;&#65292;&#22240;&#27492;&#38656;&#35201;&#23547;&#27714;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#22312;&#21508;&#31181;&#35774;&#32622;&#20013;&#30340;&#25104;&#21151;&#37096;&#32626;&#24050;&#32463;&#20026;&#20010;&#20154;&#21644;&#31038;&#20250;&#24102;&#26469;&#20102;&#35768;&#22810;&#31215;&#26497;&#30340;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#39044;&#27979;&#30340;&#20559;&#35265;&#65292;AI&#31995;&#32479;&#20063;&#34987;&#35777;&#26126;&#23545;&#37096;&#20998;&#20154;&#21475;&#36896;&#25104;&#20102;&#20260;&#23475;&#12290;&#25105;&#20204;&#30528;&#30524;&#20110;AI&#30340;&#20844;&#24179;&#24615;&#65292;&#20998;&#26512;&#20102;&#32570;&#20047;AI&#20844;&#24179;&#24615;&#26102;&#22914;&#20309;&#23548;&#33268;&#20559;&#35265;&#38543;&#30528;&#26102;&#38388;&#30340;&#21152;&#28145;&#32780;&#25104;&#20026;&#31038;&#20250;&#21387;&#21147;&#22240;&#32032;&#12290;&#22914;&#26524;&#38382;&#39064;&#25345;&#32493;&#23384;&#22312;&#65292;&#21487;&#33021;&#20250;&#23545;&#31038;&#20250;&#20135;&#29983;&#19981;&#33391;&#30340;&#38271;&#26399;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#19982;&#20854;&#20182;&#39118;&#38505;&#30340;&#20132;&#20114;&#26469;&#21152;&#24378;&#12290;&#25105;&#20204;&#26816;&#26597;&#20102;&#25552;&#39640;AI&#20844;&#24179;&#24615;&#30340;&#24403;&#21069;&#31574;&#30053;&#65292;&#24182;&#35780;&#20272;&#23427;&#20204;&#22312;&#23454;&#38469;&#37096;&#32626;&#26041;&#38754;&#30340;&#38480;&#21046;&#65292;&#24182;&#25506;&#35752;&#20102;&#30830;&#20445;&#25105;&#20204;&#22312;&#19981;&#25439;&#23475;&#31038;&#20250;&#37325;&#35201;&#37096;&#20998;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;AI&#30340;&#22909;&#22788;&#30340;&#28508;&#22312;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Successful deployment of artificial intelligence (AI) in various settings has led to numerous positive outcomes for individuals and society. However, AI systems have also been shown to harm parts of the population due to biased predictions. We take a closer look at AI fairness and analyse how lack of AI fairness can lead to deepening of biases over time and act as a social stressor. If the issues persist, it could have undesirable long-term implications on society, reinforced by interactions with other risks. We examine current strategies for improving AI fairness, assess their limitations in terms of real-world deployment, and explore potential paths forward to ensure we reap AI's benefits without harming significant parts of the society.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#25299;&#25169;&#30340;&#28857;&#32858;&#31867;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#21033;&#29992;&#25299;&#25169;&#29305;&#24449;&#25551;&#36848;&#28857;&#20113;&#20869;&#30340;&#25968;&#25454;&#28857;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#22270;&#27169;&#22411;&#26041;&#27861;&#26356;&#20855;&#26377;&#20581;&#22766;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.16716</link><description>&lt;p&gt;
&#22522;&#20110;&#25299;&#25169;&#30340;&#28857;&#20113;&#32858;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Topological Point Cloud Clustering. (arXiv:2303.16716v1 [math.AT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16716
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#25299;&#25169;&#30340;&#28857;&#32858;&#31867;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#21033;&#29992;&#25299;&#25169;&#29305;&#24449;&#25551;&#36848;&#28857;&#20113;&#20869;&#30340;&#25968;&#25454;&#28857;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#22270;&#27169;&#22411;&#26041;&#27861;&#26356;&#20855;&#26377;&#20581;&#22766;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;&#25299;&#25169;&#28857;&#20113;&#32858;&#31867;&#65288;TPCC&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#22522;&#20110;&#28857;&#20113;&#23545;&#20110;&#20840;&#23616;&#25299;&#25169;&#29305;&#24449;&#30340;&#36129;&#29486;&#26469;&#32858;&#31867;&#28857;&#12290;TPCC&#20174;&#35889;&#32858;&#31867;&#21644;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#20013;&#32508;&#21512;&#20102;&#26377;&#21033;&#30340;&#29305;&#24449;&#65292;&#22522;&#20110;&#32771;&#34385;&#19982;&#25152;&#32771;&#34385;&#30340;&#28857;&#20113;&#30456;&#20851;&#32852;&#30340;&#19968;&#20010;&#21333;&#24418;&#22797;&#21512;&#20307;&#30340;&#35889;&#29305;&#24615;&#12290;&#30001;&#20110;&#23427;&#22522;&#20110;&#32771;&#34385;&#31232;&#30095;&#29305;&#24449;&#21521;&#37327;&#35745;&#31639;&#65292;TPCC&#21516;&#26679;&#23481;&#26131;&#35299;&#37322;&#21644;&#23454;&#29616;&#65292;&#23601;&#20687;&#35889;&#32858;&#31867;&#19968;&#26679;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#19981;&#20165;&#20851;&#27880;&#19982;&#20174;&#28857;&#20113;&#25968;&#25454;&#21019;&#24314;&#30340;&#22270;&#30456;&#20851;&#32852;&#30340;&#21333;&#20010;&#30697;&#38453;&#65292;&#32780;&#26159;&#20851;&#27880;&#19982;&#24688;&#24403;&#26500;&#36896;&#30340;&#21333;&#24418;&#22797;&#21512;&#20307;&#30456;&#20851;&#32852;&#30340;&#25972;&#20010;Hodge-Laplacian&#30340;&#19968;&#25972;&#22871;&#30697;&#38453;&#65292;&#25105;&#20204;&#21487;&#20197;&#21033;&#29992;&#26356;&#20016;&#23500;&#30340;&#25299;&#25169;&#29305;&#24449;&#26469;&#25551;&#36848;&#28857;&#20113;&#20869;&#30340;&#25968;&#25454;&#28857;&#65292;&#24182;&#21463;&#30410;&#20110;&#25299;&#25169;&#25216;&#26415;&#30456;&#23545;&#20110;&#22122;&#22768;&#30340;&#30456;&#23545;&#20581;&#22766;&#24615;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#19978;&#27979;&#35797;&#20102;TPCC&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Topological Point Cloud Clustering (TPCC), a new method to cluster points in an arbitrary point cloud based on their contribution to global topological features. TPCC synthesizes desirable features from spectral clustering and topological data analysis and is based on considering the spectral properties of a simplicial complex associated to the considered point cloud. As it is based on considering sparse eigenvector computations, TPCC is similarly easy to interpret and implement as spectral clustering. However, by focusing not just on a single matrix associated to a graph created from the point cloud data, but on a whole set of Hodge-Laplacians associated to an appropriately constructed simplicial complex, we can leverage a far richer set of topological features to characterize the data points within the point cloud and benefit from the relative robustness of topological techniques against noise. We test the performance of TPCC on both synthetic and real-world data and compa
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#65292;&#20854;&#21487;&#33021;&#20250;&#20986;&#29616;&#19981;&#33391;&#29305;&#24615;&#24182;&#36880;&#28176;&#36229;&#36234;&#20154;&#31867;&#26234;&#33021;&#30340;&#38382;&#39064;&#65292;&#20197;&#21450;&#36825;&#23545;&#20154;&#31867;&#26410;&#26469;&#30340;&#25511;&#21046;&#26435;&#20135;&#29983;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2303.16200</link><description>&lt;p&gt;
&#33258;&#28982;&#36873;&#25321;&#25903;&#25345;&#20154;&#24037;&#26234;&#33021;&#32988;&#36807;&#20154;&#31867;
&lt;/p&gt;
&lt;p&gt;
Natural Selection Favors AIs over Humans. (arXiv:2303.16200v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16200
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#65292;&#20854;&#21487;&#33021;&#20250;&#20986;&#29616;&#19981;&#33391;&#29305;&#24615;&#24182;&#36880;&#28176;&#36229;&#36234;&#20154;&#31867;&#26234;&#33021;&#30340;&#38382;&#39064;&#65292;&#20197;&#21450;&#36825;&#23545;&#20154;&#31867;&#26410;&#26469;&#30340;&#25511;&#21046;&#26435;&#20135;&#29983;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#36827;&#21270;&#39537;&#21160;&#20102;&#29983;&#21629;&#30340;&#21457;&#23637;&#65292;&#21253;&#25324;&#20154;&#31867;&#12290;&#36827;&#21270;&#36171;&#20104;&#20102;&#20154;&#31867;&#39640;&#26234;&#21830;&#65292;&#20351;&#25105;&#20204;&#25104;&#20026;&#20102;&#22320;&#29699;&#19978;&#26368;&#25104;&#21151;&#30340;&#29289;&#31181;&#20043;&#19968;&#12290;&#22914;&#20170;&#65292;&#20154;&#31867;&#30340;&#30446;&#26631;&#26159;&#21019;&#36896;&#29978;&#33267;&#36229;&#36234;&#25105;&#20204;&#33258;&#24049;&#26234;&#24935;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;&#24403;&#20154;&#24037;&#26234;&#33021;&#36880;&#28176;&#36827;&#21270;&#24182;&#22312;&#25152;&#26377;&#39046;&#22495;&#36229;&#36234;&#25105;&#20204;&#26102;&#65292;&#36827;&#21270;&#22914;&#20309;&#24433;&#21709;&#25105;&#20204;&#19982;&#20154;&#24037;&#26234;&#33021;&#30340;&#20851;&#31995;&#65311;&#36890;&#36807;&#20998;&#26512;&#24433;&#21709;&#20154;&#24037;&#26234;&#33021;&#36827;&#21270;&#30340;&#29615;&#22659;&#65292;&#25105;&#20204;&#35748;&#20026;&#26368;&#25104;&#21151;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#24456;&#21487;&#33021;&#20855;&#26377;&#19981;&#33391;&#29305;&#24615;&#12290;&#20844;&#21496;&#21644;&#20891;&#38431;&#20043;&#38388;&#30340;&#31454;&#20105;&#21387;&#21147;&#23558;&#20135;&#29983;&#33258;&#21160;&#21270;&#20154;&#31867;&#35282;&#33394;&#12289;&#27450;&#39575;&#20182;&#20154;&#21644;&#25484;&#26435;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#12290;&#22914;&#26524;&#36825;&#26679;&#30340;&#20195;&#29702;&#26377;&#36229;&#36807;&#20154;&#31867;&#30340;&#26234;&#33021;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#20154;&#31867;&#22833;&#21435;&#23545;&#26410;&#26469;&#30340;&#25511;&#21046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35748;&#20026;&#33258;&#28982;&#36873;&#25321;&#20316;&#29992;&#20110;&#31454;&#20105;&#21644;&#24046;&#24322;&#30340;&#31995;&#32479;&#65292;&#33258;&#31169;&#29289;&#31181;&#24448;&#24448;&#22312;&#36825;&#26679;&#30340;&#29615;&#22659;&#20013;&#33719;&#24471;&#36827;&#21270;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
For billions of years, evolution has been the driving force behind the development of life, including humans. Evolution endowed humans with high intelligence, which allowed us to become one of the most successful species on the planet. Today, humans aim to create artificial intelligence systems that surpass even our own intelligence. As artificial intelligences (AIs) evolve and eventually surpass us in all domains, how might evolution shape our relations with AIs? By analyzing the environment that is shaping the evolution of AIs, we argue that the most successful AI agents will likely have undesirable traits. Competitive pressures among corporations and militaries will give rise to AI agents that automate human roles, deceive others, and gain power. If such agents have intelligence that exceeds that of humans, this could lead to humanity losing control of its future. More abstractly, we argue that natural selection operates on systems that compete and vary, and that selfish species typ
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35777;&#26126;&#25910;&#25947;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#26071;&#22411;&#27969;&#24418;&#19978;&#35745;&#31639;&#19968;&#32452;&#28857;&#30340;&#26071;&#24418;&#22343;&#20540;&#21644;&#26071;&#24418;&#20013;&#20540;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#38382;&#39064;&#20013;&#38750;&#24120;&#26377;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.13501</link><description>&lt;p&gt;
&#26071;&#22411;&#27969;&#24418;&#19978;&#30340;&#24358;&#22343;&#20540;&#21450;&#20854;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Chordal Averaging on Flag Manifolds and Its Applications. (arXiv:2303.13501v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13501
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35777;&#26126;&#25910;&#25947;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#26071;&#22411;&#27969;&#24418;&#19978;&#35745;&#31639;&#19968;&#32452;&#28857;&#30340;&#26071;&#24418;&#22343;&#20540;&#21644;&#26071;&#24418;&#20013;&#20540;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#38382;&#39064;&#20013;&#38750;&#24120;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#12289;&#21487;&#35777;&#26126;&#25910;&#25947;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#24358;&#24230;&#37327;&#19979;&#35745;&#31639;&#26071;&#22411;&#27969;&#24418;&#19978;&#19968;&#32452;&#28857;&#30340;&#26071;&#24418;&#22343;&#20540;&#21644;&#26071;&#24418;&#20013;&#20540;&#12290;&#26071;&#22411;&#27969;&#24418;&#26159;&#19968;&#31181;&#25968;&#23398;&#31354;&#38388;&#65292;&#30001;&#23884;&#22871;&#30340;&#21521;&#37327;&#31354;&#38388;&#23376;&#31354;&#38388;&#24207;&#21015;&#32452;&#25104;&#65292;&#24182;&#19988;&#22312;&#32500;&#24230;&#19978;&#36880;&#28176;&#22686;&#21152;&#12290;&#26071;&#22411;&#27969;&#24418;&#26159;&#24050;&#30693;&#30340;&#35768;&#22810;&#30697;&#38453;&#32676;&#30340;&#36229;&#38598;&#65292;&#21253;&#25324;Stiefel&#21644;Grassmanians&#65292;&#20351;&#20854;&#25104;&#20026;&#22312;&#21508;&#31181;&#35745;&#31639;&#26426;&#35270;&#35273;&#38382;&#39064;&#20013;&#38750;&#24120;&#26377;&#29992;&#30340;&#36890;&#29992;&#23545;&#35937;&#12290;&#20026;&#20102;&#35299;&#20915;&#35745;&#31639;&#19968;&#38454;&#26071;&#24092;&#32479;&#35745;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#38382;&#39064;&#36716;&#21270;&#20026;&#28041;&#21450;&#36741;&#21161;&#21464;&#37327;&#21463;Stiefel&#27969;&#24418;&#32422;&#26463;&#30340;&#38382;&#39064;&#12290;Stiefel&#27969;&#24418;&#26159;&#19968;&#32452;&#27491;&#20132;&#26694;&#26550;&#30340;&#31354;&#38388;&#65292;&#21033;&#29992;Stiefel&#27969;&#24418;&#20248;&#21270;&#30340;&#25968;&#20540;&#31283;&#23450;&#24615;&#21644;&#25928;&#29575;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35745;&#31639;&#26071;&#24418;&#22343;&#20540;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;Grassmann&#21644;&#26059;&#36716;&#22343;&#20540;&#20197;&#21450;&#20027;&#25104;&#20998;&#38382;&#39064;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a new, provably-convergent algorithm for computing the flag-mean and flag-median of a set of points on a flag manifold under the chordal metric. The flag manifold is a mathematical space consisting of flags, which are sequences of nested subspaces of a vector space that increase in dimension. The flag manifold is a superset of a wide range of known matrix groups, including Stiefel and Grassmanians, making it a general object that is useful in a wide variety computer vision problems.  To tackle the challenge of computing first order flag statistics, we first transform the problem into one that involves auxiliary variables constrained to the Stiefel manifold. The Stiefel manifold is a space of orthogonal frames, and leveraging the numerical stability and efficiency of Stiefel-manifold optimization enables us to compute the flag-mean effectively. Through a series of experiments, we show the competence of our method in Grassmann and rotation averaging, as well as princi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#26377;&#20851;&#25968;&#25454;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#30740;&#31350;&#65292;&#31995;&#32479;&#22320;&#24635;&#32467;&#20102;&#26368;&#26032;&#30740;&#31350;&#25104;&#26524;&#65292;&#24182;&#36827;&#19968;&#27493;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#21644;&#30693;&#35782;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2303.09767</link><description>&lt;p&gt;
&#20851;&#20110;&#25968;&#25454;&#30340;&#19968;&#20999;&#65306;&#23545;&#25968;&#25454;&#23545;&#25239;&#40065;&#26834;&#24615;&#24433;&#21709;&#30340;&#30740;&#31350;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
It Is All About Data: A Survey on the Effects of Data on Adversarial Robustness. (arXiv:2303.09767v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09767
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#26377;&#20851;&#25968;&#25454;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#30740;&#31350;&#65292;&#31995;&#32479;&#22320;&#24635;&#32467;&#20102;&#26368;&#26032;&#30740;&#31350;&#25104;&#26524;&#65292;&#24182;&#36827;&#19968;&#27493;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#21644;&#30693;&#35782;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#26679;&#26412;&#26159;&#25915;&#20987;&#32773;&#26377;&#24847;&#35774;&#35745;&#29992;&#20110;&#28151;&#28102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20197;&#20415;&#20854;&#29359;&#38169;&#30340;&#36755;&#20837;&#12290;&#36825;&#20123;&#26679;&#26412;&#23545;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#31995;&#32479;&#30340;&#36866;&#29992;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#28041;&#21450;&#29983;&#21629;&#21644;&#23433;&#20840;&#30340;&#39046;&#22495;&#65292;&#26500;&#25104;&#20102;&#20005;&#37325;&#23041;&#32961;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#23545;&#25239;&#40065;&#26834;&#24615;&#39046;&#22495;&#30740;&#31350;&#23545;&#25239;&#25915;&#20987;&#26426;&#21046;&#21644;&#38450;&#24481;&#31574;&#30053;&#12290;&#26412;&#32508;&#36848;&#22238;&#39038;&#20102;&#26377;&#20851;&#27169;&#22411;&#20351;&#29992;&#30340;&#25968;&#25454;&#23545;&#20854;&#25239;&#25915;&#20987;&#40065;&#26834;&#24615;&#24433;&#21709;&#30340;&#25991;&#29486;&#12290;&#23427;&#31995;&#32479;&#22320;&#35782;&#21035;&#21644;&#24635;&#32467;&#20102;&#36825;&#20010;&#39046;&#22495;&#20869;&#30340;&#26368;&#26032;&#30740;&#31350;&#65292;&#24182;&#36827;&#19968;&#27493;&#35752;&#35770;&#20102;&#30693;&#35782;&#30340;&#24046;&#36317;&#21644;&#26377;&#21069;&#36884;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial examples are inputs to machine learning models that an attacker has intentionally designed to confuse the model into making a mistake. Such examples pose a serious threat to the applicability of machine-learning-based systems, especially in life- and safety-critical domains. To address this problem, the area of adversarial robustness investigates mechanisms behind adversarial attacks and defenses against these attacks. This survey reviews literature that focuses on the effects of data used by a model on the model's adversarial robustness. It systematically identifies and summarizes the state-of-the-art research in this area and further discusses gaps of knowledge and promising future research directions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#22810;&#20010;&#23618;&#38754;&#30340;&#40065;&#26834;&#24615;&#32479;&#19968;&#26694;&#26550;&#65292;&#36890;&#36807;&#31532;&#19968;&#20010;&#22810;&#25915;&#20987;&#35780;&#20272;&#25490;&#34892;&#27036; MultiRobustBench&#65292;&#35780;&#20272;&#20102;16&#20010;&#38450;&#24481;&#27169;&#22411;&#38024;&#23545;9&#31181;&#19981;&#21516;&#25915;&#20987;&#31867;&#22411;&#21644;20&#31181;&#19981;&#21516;&#25915;&#20987;&#24378;&#24230;&#30340;&#40065;&#26834;&#24615;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2302.10980</link><description>&lt;p&gt;
MultiRobustBench: &#23545;&#25239;&#22810;&#31181;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
MultiRobustBench: Benchmarking Robustness Against Multiple Attacks. (arXiv:2302.10980v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10980
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#22810;&#20010;&#23618;&#38754;&#30340;&#40065;&#26834;&#24615;&#32479;&#19968;&#26694;&#26550;&#65292;&#36890;&#36807;&#31532;&#19968;&#20010;&#22810;&#25915;&#20987;&#35780;&#20272;&#25490;&#34892;&#27036; MultiRobustBench&#65292;&#35780;&#20272;&#20102;16&#20010;&#38450;&#24481;&#27169;&#22411;&#38024;&#23545;9&#31181;&#19981;&#21516;&#25915;&#20987;&#31867;&#22411;&#21644;20&#31181;&#19981;&#21516;&#25915;&#20987;&#24378;&#24230;&#30340;&#40065;&#26834;&#24615;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#31034;&#20363;&#38450;&#24481;&#39046;&#22495;&#30340;&#24456;&#22823;&#19968;&#37096;&#20998;&#29616;&#26377;&#30740;&#31350;&#37117;&#19987;&#27880;&#20110;&#38450;&#24481;&#21333;&#19968;&#65288;&#36890;&#24120;&#26159;&#26377;&#30028;&#30340;Lp&#33539;&#25968;&#65289;&#25915;&#20987;&#65292;&#20294;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#23545;&#21508;&#31181;&#25915;&#20987;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#22810;&#31181;&#25915;&#20987;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#32479;&#19968;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#33021;&#22815;&#27169;&#25311;&#23398;&#20064;&#22120;&#23545;&#27979;&#35797;&#26102;&#25915;&#20987;&#32773;&#30340;&#19981;&#21516;&#20102;&#35299;&#27700;&#24179;&#65292;&#20174;&#32780;&#20351;&#25105;&#20204;&#33021;&#22815;&#23545;&#26410;&#30693;&#25915;&#20987;&#21644;&#25915;&#20987;&#38598;&#30340;&#40065;&#26834;&#24615;&#36827;&#34892;&#24314;&#27169;&#12290;&#20351;&#29992;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#38024;&#23545;&#22810;&#25915;&#20987;&#35780;&#20272;&#30340;&#25490;&#34892;&#27036; MultiRobustBench&#65292;&#35813;&#25490;&#34892;&#27036;&#33021;&#22815;&#25429;&#25417;&#25915;&#20987;&#31867;&#22411;&#21644;&#25915;&#20987;&#24378;&#24230;&#20043;&#38388;&#30340;&#34920;&#29616;&#24046;&#24322;&#12290;&#25105;&#20204;&#23545;16&#20010;&#38450;&#24481;&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#38024;&#23545;9&#31181;&#19981;&#21516;&#30340;&#25915;&#20987;&#31867;&#22411;&#65292;&#21253;&#25324;Lp&#33539;&#25968;&#23041;&#32961;&#27169;&#22411;&#12289;&#31354;&#38388;&#36716;&#25442;&#21644;&#39068;&#33394;&#25913;&#21464;&#31561;&#65292;&#22312;20&#31181;&#19981;&#21516;&#30340;&#25915;&#20987;&#24378;&#24230;&#19979;&#36827;&#34892;&#20102;&#27979;&#35797;&#65288;&#24635;&#20849;180&#27425;&#25915;&#20987;&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#38024;&#23545;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#65292;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#22810;&#25915;&#20987;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The bulk of existing research in defending against adversarial examples focuses on defending against a single (typically bounded Lp-norm) attack, but for a practical setting, machine learning (ML) models should be robust to a wide variety of attacks. In this paper, we present the first unified framework for considering multiple attacks against ML models. Our framework is able to model different levels of learner's knowledge about the test-time adversary, allowing us to model robustness against unforeseen attacks and robustness against unions of attacks. Using our framework, we present the first leaderboard, MultiRobustBench, for benchmarking multiattack evaluation which captures performance across attack types and attack strengths. We evaluate the performance of 16 defended models for robustness against a set of 9 different attack types, including Lp-based threat models, spatial transformations, and color changes, at 20 different attack strengths (180 attacks total). Additionally, we a
&lt;/p&gt;</description></item><item><title>Navya3DSeg&#26159;&#19968;&#20010;&#26032;&#30340;&#12289;&#20855;&#26377;&#22810;&#26679;&#26631;&#31614;&#31354;&#38388;&#30340;&#25968;&#25454;&#38598;&#65292;&#23545;&#24212;&#20110;&#22823;&#35268;&#27169;&#29983;&#20135;&#32423;&#25805;&#20316;&#39046;&#22495;&#65292;&#21487;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#24863;&#30693;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#39034;&#24207;&#25968;&#25454;&#38598;&#25286;&#20998;&#12290;</title><link>http://arxiv.org/abs/2302.08292</link><description>&lt;p&gt;
Navya3DSeg -- &#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#30340;Navya&#19977;&#32500;&#35821;&#20041;&#20998;&#21106;&#25968;&#25454;&#38598;&#21644;&#25286;&#20998;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Navya3DSeg -- Navya 3D Semantic Segmentation Dataset &amp; split generation for autonomous vehicles. (arXiv:2302.08292v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08292
&lt;/p&gt;
&lt;p&gt;
Navya3DSeg&#26159;&#19968;&#20010;&#26032;&#30340;&#12289;&#20855;&#26377;&#22810;&#26679;&#26631;&#31614;&#31354;&#38388;&#30340;&#25968;&#25454;&#38598;&#65292;&#23545;&#24212;&#20110;&#22823;&#35268;&#27169;&#29983;&#20135;&#32423;&#25805;&#20316;&#39046;&#22495;&#65292;&#21487;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#24863;&#30693;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#39034;&#24207;&#25968;&#25454;&#38598;&#25286;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#24863;&#30693;&#30446;&#21069;&#20005;&#37325;&#20381;&#36182;&#20110;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20307;&#31995;&#32467;&#26500;&#65292;&#38656;&#35201;&#22823;&#35268;&#27169;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#65292;&#20855;&#26377;&#30456;&#24212;&#30340;&#31574;&#21010;&#19982;&#26631;&#27880;&#25104;&#26412;&#12290;&#19977;&#32500;&#35821;&#20041;&#25968;&#25454;&#23545;&#20110;&#26680;&#24515;&#24863;&#30693;&#20219;&#21153;&#22914;&#38556;&#30861;&#26816;&#27979;&#21644;&#33258;&#25105;&#23450;&#20301;&#38750;&#24120;&#26377;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;Navya3DSeg&#65292;&#20855;&#26377;&#22810;&#26679;&#30340;&#26631;&#31614;&#31354;&#38388;&#65292;&#23545;&#24212;&#20110;&#22823;&#35268;&#27169;&#29983;&#20135;&#32423;&#25805;&#20316;&#39046;&#22495;&#65292;&#21253;&#25324;&#26469;&#33258;13&#20010;&#22269;&#23478;&#30340;&#22478;&#24066;&#65292;&#20065;&#26449;&#65292;&#24037;&#19994;&#21306;&#21644;&#22823;&#23398;&#12290;&#23427;&#21253;&#21547;23&#20010;&#24102;&#26631;&#31614;&#24207;&#21015;&#21644;25&#20010;&#27809;&#26377;&#26631;&#31614;&#30340;&#34917;&#20805;&#24207;&#21015;&#65292;&#26088;&#22312;&#25506;&#35752;&#22522;&#20110;&#28857;&#20113;&#30340;&#33258;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#22522;&#20934;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36845;&#20195;&#22810;&#26631;&#31614;&#20998;&#23618;&#30340;&#39034;&#24207;&#25968;&#25454;&#38598;&#25286;&#20998;&#29983;&#25104;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#28436;&#31034;&#20102;&#27604;SemanticKITTI&#25968;&#25454;&#38598;&#25552;&#20986;&#30340;&#21407;&#22987;&#25286;&#20998;+1.2&#65285; mIoU&#30340;&#25913;&#36827;&#12290;&#36825;&#26159;&#19968;&#20010;&#23436;&#25972;&#30340;&#35821;&#20041;&#20998;&#21106;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous driving (AD) perception today relies heavily on deep learning based architectures requiring large scale annotated datasets with their associated costs for curation and annotation. The 3D semantic data are useful for core perception tasks such as obstacle detection and ego-vehicle localization. We propose a new dataset, Navya 3D Segmentation (Navya3DSeg), with a diverse label space corresponding to a large scale production grade operational domain, including rural, urban, industrial sites and universities from 13 countries. It contains 23 labeled sequences and 25 supplementary sequences without labels, designed to explore self-supervised and semi-supervised semantic segmentation benchmarks on point clouds. We also propose a novel method for sequential dataset split generation based on iterative multi-label stratification, and demonstrated to achieve a +1.2% mIoU improvement over the original split proposed by SemanticKITTI dataset. A complete benchmark for semantic segmentati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22270;&#29983;&#25104;&#27169;&#22411;&#65292;&#25512;&#23548;&#20986;&#20102;&#32473;&#23450;&#22270;&#30340;&#31867;&#26631;&#31614;&#27010;&#29575;&#30340;&#20998;&#31867;&#20844;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26465;&#20214; ELBO &#29992;&#20110;&#35757;&#32451;&#29983;&#25104;&#22270;&#33258;&#32534;&#30721;&#22120;&#27169;&#22411;&#12290;&#36825;&#26159;&#19968;&#31181;&#22312;&#22270;&#20998;&#31867;&#20013;&#20855;&#26377;&#21019;&#26032;&#24615;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.07989</link><description>&lt;p&gt;
&#20174;&#22270;&#29983;&#25104;&#21040;&#22270;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
From Graph Generation to Graph Classification. (arXiv:2302.07989v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07989
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22270;&#29983;&#25104;&#27169;&#22411;&#65292;&#25512;&#23548;&#20986;&#20102;&#32473;&#23450;&#22270;&#30340;&#31867;&#26631;&#31614;&#27010;&#29575;&#30340;&#20998;&#31867;&#20844;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26465;&#20214; ELBO &#29992;&#20110;&#35757;&#32451;&#29983;&#25104;&#22270;&#33258;&#32534;&#30721;&#22120;&#27169;&#22411;&#12290;&#36825;&#26159;&#19968;&#31181;&#22312;&#22270;&#20998;&#31867;&#20013;&#20855;&#26377;&#21019;&#26032;&#24615;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#21033;&#29992;&#22270;&#29983;&#25104;&#27169;&#22411; (GGM) &#36827;&#34892;&#22270;&#20998;&#31867;&#30340;&#26032;&#26041;&#27861;&#12290;&#20551;&#35774;&#19968;&#20010;&#23450;&#20041;&#20102;&#22270;&#21450;&#20854;&#31867;&#26631;&#31614;&#30340;&#32852;&#21512;&#27010;&#29575;&#20998;&#24067;&#30340; GGM&#65292;&#25105;&#25512;&#23548;&#20102;&#35745;&#31639;&#32473;&#23450;&#22270;&#30340;&#31867;&#26631;&#31614;&#27010;&#29575;&#30340;&#20998;&#31867;&#20844;&#24335;&#12290;&#21487;&#20197;&#20351;&#29992;&#26032;&#30340;&#26465;&#20214; ELBO &#26469;&#35757;&#32451;&#29983;&#25104;&#22270;&#33258;&#32534;&#30721;&#22120;&#27169;&#22411;&#36827;&#34892;&#21306;&#20998;&#12290;&#34429;&#28982;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#20998;&#31867;&#22312;&#38750;&#20851;&#31995; i.i.d. &#25968;&#25454;&#20013;&#24050;&#32463;&#24471;&#21040;&#20102;&#24456;&#22909;&#30340;&#30740;&#31350;&#65292;&#20294;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#19968;&#31181;&#22270;&#20998;&#31867;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This note describes a new approach to classifying graphs that leverages graph generative models (GGM). Assuming a GGM that defines a joint probability distribution over graphs and their class labels, I derive classification formulas for the probability of a class label given a graph. A new conditional ELBO can be used to train a generative graph auto-encoder model for discrimination. While leveraging generative models for classification has been well explored for non-relational i.i.d. data, to our knowledge it is a novel approach to graph classification.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;VAMoH&#65292;&#32467;&#21512;&#20102;INRs&#23545;&#36830;&#32493;&#20989;&#25968;&#36827;&#34892;&#24314;&#27169;&#30340;&#33021;&#21147;&#21644;VAEs&#30340;&#25512;&#26029;&#33021;&#21147;&#65292;&#20197;&#21450;&#24402;&#19968;&#21270;&#27969;&#21644;&#36229;&#32593;&#32476;&#28151;&#21512;&#26041;&#27861;&#12290;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#65292;VAMoH&#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#36830;&#32493;&#20989;&#25968;&#30340;&#20998;&#24067;&#65292;&#24182;&#21487;&#20197;&#25191;&#34892;&#19982;&#25512;&#26029;&#30456;&#20851;&#30340;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2302.06223</link><description>&lt;p&gt;
&#21464;&#20998;&#28151;&#21512;&#36229;&#29983;&#25104;&#22120;&#29992;&#20110;&#23398;&#20064;&#20989;&#25968;&#20998;&#24067;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Variational Mixture of HyperGenerators for Learning Distributions Over Functions. (arXiv:2302.06223v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06223
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;VAMoH&#65292;&#32467;&#21512;&#20102;INRs&#23545;&#36830;&#32493;&#20989;&#25968;&#36827;&#34892;&#24314;&#27169;&#30340;&#33021;&#21147;&#21644;VAEs&#30340;&#25512;&#26029;&#33021;&#21147;&#65292;&#20197;&#21450;&#24402;&#19968;&#21270;&#27969;&#21644;&#36229;&#32593;&#32476;&#28151;&#21512;&#26041;&#27861;&#12290;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#65292;VAMoH&#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#36830;&#32493;&#20989;&#25968;&#30340;&#20998;&#24067;&#65292;&#24182;&#21487;&#20197;&#25191;&#34892;&#19982;&#25512;&#26029;&#30456;&#20851;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#19968;&#20123;&#26041;&#27861;&#22522;&#20110;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#65288;INRs&#65289;&#25552;&#20986;&#20102;&#20989;&#25968;&#31354;&#38388;&#19978;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22788;&#29702;&#25512;&#26029;&#20219;&#21153;&#65288;&#22914;&#32570;&#22833;&#25968;&#25454;&#25554;&#20540;&#65289;&#26102;&#65292;&#23427;&#20204;&#22312;&#35745;&#31639;&#19978;&#20195;&#20215;&#39640;&#65292;&#25110;&#32773;&#26681;&#26412;&#19981;&#33021;&#22788;&#29702;&#36825;&#20123;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#31216;&#20026;VAMoH&#12290;VAMoH&#32467;&#21512;&#20102;&#20351;&#29992;INRs&#23545;&#36830;&#32493;&#20989;&#25968;&#36827;&#34892;&#24314;&#27169;&#30340;&#33021;&#21147;&#21644;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#30340;&#25512;&#26029;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;VAMoH&#20381;&#36182;&#20110;&#19968;&#20010;&#24402;&#19968;&#21270;&#27969;&#26469;&#23450;&#20041;&#20808;&#39564;&#65292;&#20197;&#21450;&#19968;&#20010;&#36229;&#32593;&#32476;&#28151;&#21512;&#26469;&#21442;&#25968;&#21270;&#25968;&#25454;&#23545;&#25968;&#20284;&#28982;&#12290;&#36825;&#20351;&#24471;VAMoH&#20855;&#26377;&#39640;&#24230;&#34920;&#36798;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#36890;&#36807;&#22312;&#21508;&#31181;&#25968;&#25454;&#31867;&#22411;&#65288;&#22914;&#22270;&#20687;&#12289;&#20307;&#32032;&#21644;&#27668;&#20505;&#25968;&#25454;&#65289;&#19978;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#65292;VAMoH&#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#36830;&#32493;&#20989;&#25968;&#30340;&#20016;&#23500;&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;&#23427;&#21487;&#20197;&#25191;&#34892;&#19982;&#25512;&#26029;&#30456;&#20851;&#30340;&#20219;&#21153;&#65292;&#22914;&#26465;&#20214;&#36229;&#20998;&#36776;&#29575;&#29983;&#25104;&#21644;&#20462;&#22797;&#65292;&#25928;&#26524;&#20248;&#20110;&#25110;&#19981;&#20122;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent approaches build on implicit neural representations (INRs) to propose generative models over function spaces. However, they are computationally costly when dealing with inference tasks, such as missing data imputation, or directly cannot tackle them. In this work, we propose a novel deep generative model, named VAMoH. VAMoH combines the capabilities of modeling continuous functions using INRs and the inference capabilities of Variational Autoencoders (VAEs). In addition, VAMoH relies on a normalizing flow to define the prior, and a mixture of hypernetworks to parametrize the data log-likelihood. This gives VAMoH a high expressive capability and interpretability. Through experiments on a diverse range of data types, such as images, voxels, and climate data, we show that VAMoH can effectively learn rich distributions over continuous functions. Furthermore, it can perform inference-related tasks, such as conditional super-resolution generation and in-painting, as well or better tha
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;ChatGPT&#21644;GPT-4&#30340;&#25968;&#23398;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#26032;&#30340;&#26041;&#27861;&#20197;&#21450;&#21457;&#24067;&#20004;&#20010;&#26032;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#20102;&#23427;&#20204;&#22312;&#21508;&#20010;&#32500;&#24230;&#30340;&#25968;&#23398;&#25512;&#29702;&#19978;&#30340;&#34920;&#29616;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#28085;&#30422;&#30740;&#31350;&#29983;&#32423;&#25968;&#23398;&#24182;&#30001;&#25968;&#23398;&#30740;&#31350;&#20154;&#21592;&#31574;&#21010;&#30340;&#33258;&#28982;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#20063;&#27979;&#35797;&#20102;&#23427;&#20204;&#20316;&#20026;&#19987;&#19994;&#25968;&#23398;&#23478;&#21161;&#25163;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2301.13867</link><description>&lt;p&gt;
ChatGPT&#30340;&#25968;&#23398;&#33021;&#21147;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Mathematical Capabilities of ChatGPT. (arXiv:2301.13867v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13867
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;ChatGPT&#21644;GPT-4&#30340;&#25968;&#23398;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#26032;&#30340;&#26041;&#27861;&#20197;&#21450;&#21457;&#24067;&#20004;&#20010;&#26032;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#20102;&#23427;&#20204;&#22312;&#21508;&#20010;&#32500;&#24230;&#30340;&#25968;&#23398;&#25512;&#29702;&#19978;&#30340;&#34920;&#29616;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#28085;&#30422;&#30740;&#31350;&#29983;&#32423;&#25968;&#23398;&#24182;&#30001;&#25968;&#23398;&#30740;&#31350;&#20154;&#21592;&#31574;&#21010;&#30340;&#33258;&#28982;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#20063;&#27979;&#35797;&#20102;&#23427;&#20204;&#20316;&#20026;&#19987;&#19994;&#25968;&#23398;&#23478;&#21161;&#25163;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23545;ChatGPT&#65288;&#21457;&#24067;&#20110;2023&#24180;1&#26376;9&#26085;&#21644;1&#26376;30&#26085;&#65289;&#21644;GPT-4&#30340;&#25968;&#23398;&#33021;&#21147;&#36827;&#34892;&#20102;&#35843;&#26597;&#21644;&#27979;&#35797;&#65292;&#20351;&#29992;&#20102;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#20197;&#21450;&#25163;&#24037;&#21046;&#20316;&#30340;&#25968;&#25454;&#38598;&#12290;&#19982;&#27491;&#24335;&#25968;&#23398;&#19981;&#21516;&#65292;&#27491;&#24335;&#35777;&#26126;&#30340;&#22823;&#22411;&#25968;&#25454;&#24211;&#21487;&#20379;&#20351;&#29992;&#65288;&#20363;&#22914;&#65292;Lean&#25968;&#23398;&#24211;&#65289;&#65292;&#24403;&#21069;&#29992;&#20110;&#22522;&#20934;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#28982;&#35821;&#35328;&#25968;&#23398;&#25968;&#25454;&#38598;&#35201;&#20040;&#21482;&#28085;&#30422;&#22522;&#30784;&#25968;&#23398;&#65292;&#35201;&#20040;&#38750;&#24120;&#23567;&#12290;&#25105;&#20204;&#36890;&#36807;&#20844;&#24320;&#21457;&#24067;&#20004;&#20010;&#26032;&#25968;&#25454;&#38598;&#65306;GHOSTS&#21644;miniGHOSTS&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#36825;&#26159;&#30001;&#25968;&#23398;&#30740;&#31350;&#20154;&#21592;&#31934;&#24515;&#31574;&#21010;&#30340;&#31532;&#19968;&#20010;&#33258;&#28982;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#28085;&#30422;&#30740;&#31350;&#29983;&#32423;&#25968;&#23398;&#12289;&#25552;&#20379;&#23545;&#35821;&#35328;&#27169;&#22411;&#25968;&#23398;&#33021;&#21147;&#30340;&#25972;&#20307;&#27010;&#36848;&#65292;&#24182;&#21306;&#20998;&#25968;&#23398;&#25512;&#29702;&#30340;&#22810;&#20010;&#26041;&#38754;&#12290;&#36825;&#20123;&#25968;&#25454;&#38598;&#36824;&#27979;&#35797;&#20102;ChatGPT&#21644;GPT-4&#26159;&#21542;&#21487;&#20197;&#25104;&#20026;&#19987;&#19994;&#25968;&#23398;&#23478;&#30340;&#26377;&#29992;&#21161;&#25163;&#65292;&#27169;&#25311;&#20854;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the mathematical capabilities of two iterations of ChatGPT (released 9-January-2023 and 30-January-2023) and of GPT-4 by testing them on publicly available datasets, as well as hand-crafted ones, using a novel methodology. In contrast to formal mathematics, where large databases of formal proofs are available (e.g., the Lean Mathematical Library), current datasets of natural-language mathematics, used to benchmark language models, either cover only elementary mathematics or are very small. We address this by publicly releasing two new datasets: GHOSTS and miniGHOSTS. These are the first natural-language datasets curated by working researchers in mathematics that (1) aim to cover graduate-level mathematics, (2) provide a holistic overview of the mathematical capabilities of language models, and (3) distinguish multiple dimensions of mathematical reasoning. These datasets also test whether ChatGPT and GPT-4 can be helpful assistants to professional mathematicians by emulat
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;PPOCoder&#26694;&#26550;&#23558;&#39044;&#35757;&#32451;&#30340;&#32534;&#31243;&#35821;&#35328;&#27169;&#22411;&#21644;Proximal Policy Optimization&#25216;&#26415;&#32467;&#21512;&#65292;&#36890;&#36807;&#21033;&#29992;&#20195;&#30721;&#25191;&#34892;&#21644;&#32467;&#26500;&#23545;&#40784;&#30340;&#38750;&#21487;&#24494;&#21453;&#39304;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#30340;&#20195;&#30721;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2301.13816</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#22522;&#20110;&#25191;&#34892;&#30340;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Execution-based Code Generation using Deep Reinforcement Learning. (arXiv:2301.13816v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13816
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;PPOCoder&#26694;&#26550;&#23558;&#39044;&#35757;&#32451;&#30340;&#32534;&#31243;&#35821;&#35328;&#27169;&#22411;&#21644;Proximal Policy Optimization&#25216;&#26415;&#32467;&#21512;&#65292;&#36890;&#36807;&#21033;&#29992;&#20195;&#30721;&#25191;&#34892;&#21644;&#32467;&#26500;&#23545;&#40784;&#30340;&#38750;&#21487;&#24494;&#21453;&#39304;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#30340;&#20195;&#30721;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22312;&#22823;&#35268;&#27169;&#20195;&#30721;&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;&#30340;&#32534;&#31243;&#35821;&#35328;&#65288;PL&#65289;&#27169;&#22411;&#65292;&#20316;&#20026;&#33258;&#21160;&#21270;&#36719;&#20214;&#24037;&#31243;&#36807;&#31243;&#30340;&#25163;&#27573;&#65292;&#22312;&#20195;&#30721;&#23436;&#25104;&#12289;&#20195;&#30721;&#32763;&#35793;&#21644;&#31243;&#24207;&#21512;&#25104;&#31561;&#21508;&#31181;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#30456;&#24403;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#20174;&#25991;&#26412;&#29983;&#25104;&#20013;&#20511;&#29992;&#30340;&#30417;&#30563;&#24494;&#35843;&#30446;&#26631;&#65292;&#24573;&#35270;&#20102;&#20195;&#30721;&#30340;&#29420;&#29305;&#24207;&#21015;&#32423;&#29305;&#24449;&#65292;&#21253;&#25324;&#20294;&#19981;&#38480;&#20110;&#21487;&#32534;&#35793;&#24615;&#20197;&#21450;&#35821;&#27861;&#21644;&#21151;&#33021;&#27491;&#30830;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PPOCoder&#65292;&#19968;&#31181;&#26032;&#30340;&#20195;&#30721;&#29983;&#25104;&#26694;&#26550;&#65292;&#23427;&#23558;&#39044;&#35757;&#32451;&#30340;PL&#27169;&#22411;&#19982;Proximal Policy Optimization&#65288;PPO&#65289;&#30456;&#32467;&#21512;&#65292;PPO&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#12290;&#36890;&#36807;&#21033;&#29992;&#20195;&#30721;&#25191;&#34892;&#21644;&#32467;&#26500;&#23545;&#40784;&#30340;&#38750;&#21487;&#24494;&#21453;&#39304;&#65292;PPOCoder&#23558;&#22806;&#37096;&#20195;&#30721;&#29305;&#23450;&#30693;&#35782;&#26080;&#32541;&#38598;&#25104;&#21040;&#27169;&#22411;&#20248;&#21270;&#36807;&#31243;&#20013;&#12290;&#36825;&#26159;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The utilization of programming language (PL) models, pre-trained on large-scale code corpora, as a means of automating software engineering processes has demonstrated considerable potential in streamlining various code generation tasks such as code completion, code translation, and program synthesis. However, current approaches mainly rely on supervised fine-tuning objectives borrowed from text generation, neglecting unique sequence-level characteristics of code, including but not limited to compilability as well as syntactic and functional correctness. To address this limitation, we propose PPOCoder, a new framework for code generation that synergistically combines pre-trained PL models with Proximal Policy Optimization (PPO) which is a widely used deep reinforcement learning technique. By utilizing non-differentiable feedback from code execution and structure alignment, PPOCoder seamlessly integrates external code-specific knowledge into the model optimization process. It's important
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35299;&#37322;&#25968;&#25454;&#39537;&#21160;&#38382;&#39064;&#35299;&#20915;&#26041;&#26696;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21453;&#20107;&#23454;&#35299;&#37322;&#26041;&#27861;&#21644;&#38024;&#23545;&#38543;&#26426;&#26862;&#26519;&#21644;&#26368;&#36817;&#37051;&#39044;&#27979;&#22120;&#30340;&#26368;&#36817;&#35299;&#37322;&#26041;&#27861;&#26469;&#35299;&#37322;&#25968;&#25454;&#39537;&#21160;&#20248;&#21270;&#36807;&#31243;&#20013;&#30340;&#20915;&#31574;&#27969;&#31243;&#65292;&#22635;&#34917;&#20102;&#30446;&#21069;&#22312;&#23398;&#20064;&#31639;&#27861;&#20915;&#31574;&#27969;&#31243;&#35299;&#37322;&#26041;&#38754;&#30340;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2301.10074</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#25968;&#25454;&#39537;&#21160;&#20248;&#21270;&#65306;&#20174;&#32972;&#26223;&#21040;&#20915;&#31574;&#20877;&#21040;&#32972;&#26223; (arXiv:2301.10074v2 [cs.LG] &#26356;&#26032;&#29256;)
&lt;/p&gt;
&lt;p&gt;
Explainable Data-Driven Optimization: From Context to Decision and Back Again. (arXiv:2301.10074v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10074
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35299;&#37322;&#25968;&#25454;&#39537;&#21160;&#38382;&#39064;&#35299;&#20915;&#26041;&#26696;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21453;&#20107;&#23454;&#35299;&#37322;&#26041;&#27861;&#21644;&#38024;&#23545;&#38543;&#26426;&#26862;&#26519;&#21644;&#26368;&#36817;&#37051;&#39044;&#27979;&#22120;&#30340;&#26368;&#36817;&#35299;&#37322;&#26041;&#27861;&#26469;&#35299;&#37322;&#25968;&#25454;&#39537;&#21160;&#20248;&#21270;&#36807;&#31243;&#20013;&#30340;&#20915;&#31574;&#27969;&#31243;&#65292;&#22635;&#34917;&#20102;&#30446;&#21069;&#22312;&#23398;&#20064;&#31639;&#27861;&#20915;&#31574;&#27969;&#31243;&#35299;&#37322;&#26041;&#38754;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#20248;&#21270;&#21033;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26469;&#35299;&#20915;&#23384;&#22312;&#19981;&#30830;&#23450;&#21442;&#25968;&#30340;&#20915;&#31574;&#38382;&#39064;&#12290;&#23613;&#31649;&#22312;&#20998;&#31867;&#35774;&#32622;&#20013;&#26377;&#22823;&#37327;&#24037;&#20316;&#33268;&#21147;&#20110;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20294;&#23545;&#28041;&#21450;&#23398;&#20064;&#31639;&#27861;&#30340;&#20915;&#31574;&#27969;&#31243;&#30340;&#35299;&#37322;&#20173;&#26410;&#24471;&#21040;&#35299;&#20915;&#12290;&#36825;&#31181;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#21487;&#33021;&#38459;&#30861;&#25968;&#25454;&#39537;&#21160;&#35299;&#20915;&#26041;&#26696;&#30340;&#37319;&#29992;&#65292;&#22240;&#20026;&#20174;&#19994;&#20154;&#21592;&#21487;&#33021;&#19981;&#29702;&#35299;&#25110;&#19981;&#20449;&#20219;&#25512;&#33616;&#30340;&#20915;&#31574;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#38024;&#23545;&#25968;&#25454;&#39537;&#21160;&#38382;&#39064;&#35299;&#37322;&#35299;&#20915;&#26041;&#26696;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#26041;&#27861;&#22635;&#34917;&#20102;&#36825;&#19968;&#24046;&#36317;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#20004;&#31867;&#35299;&#37322;&#65292;&#24182;&#24320;&#21457;&#20102;&#23547;&#25214;&#38543;&#26426;&#26862;&#26519;&#21644;&#26368;&#36817;&#37051;&#39044;&#27979;&#22120;&#30340;&#26368;&#36817;&#35299;&#37322;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#35299;&#37322;&#36816;&#33829;&#31649;&#29702;&#20013;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#22914;&#24211;&#23384;&#31649;&#29702;&#21644;&#36335;&#24452;&#35268;&#21010;&#65292;&#26469;&#28436;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-driven optimization uses contextual information and machine learning algorithms to find solutions to decision problems with uncertain parameters. While a vast body of work is dedicated to interpreting machine learning models in the classification setting, explaining decision pipelines involving learning algorithms remains unaddressed. This lack of interpretability can block the adoption of data-driven solutions as practitioners may not understand or trust the recommended decisions. We bridge this gap by introducing a counterfactual explanation methodology tailored to explain solutions to data-driven problems. We introduce two classes of explanations and develop methods to find nearest explanations of random forest and nearest-neighbor predictors. We demonstrate our approach by explaining key problems in operations management such as inventory management and routing.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EPGP&#30340;&#39640;&#26031;&#36807;&#31243;&#20808;&#39564;&#65292;&#29992;&#20110;&#32447;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#31995;&#32479;&#65292;&#24182;&#19988;&#26500;&#36896;&#20102;&#21453;&#26144;&#26631;&#20934;&#35889;&#26041;&#27861;&#30340;GP&#26680;&#20989;&#25968;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#25512;&#26029;&#32447;&#24615;PDE&#31995;&#32479;&#30340;&#21487;&#33021;&#35299;&#65292;&#24182;&#20855;&#26377;&#31639;&#27861;&#24615;&#24378;&#12289;&#26222;&#36866;&#24615;&#24191;&#12289;&#36866;&#29992;&#20110;&#22823;&#25968;&#25454;&#38598;&#30340;&#31232;&#30095;&#29256;&#26412;&#12290;</title><link>http://arxiv.org/abs/2212.14319</link><description>&lt;p&gt;
&#31995;&#32479;&#30340;&#32447;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#39640;&#26031;&#36807;&#31243;&#20808;&#39564;&#19982;&#24120;&#31995;&#25968;&#65288;&#32763;&#35793;&#33258;arXiv:2212.14319v3 [stat.ML] &#26356;&#26032;&#65289;
&lt;/p&gt;
&lt;p&gt;
Gaussian Process Priors for Systems of Linear Partial Differential Equations with Constant Coefficients. (arXiv:2212.14319v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.14319
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EPGP&#30340;&#39640;&#26031;&#36807;&#31243;&#20808;&#39564;&#65292;&#29992;&#20110;&#32447;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#31995;&#32479;&#65292;&#24182;&#19988;&#26500;&#36896;&#20102;&#21453;&#26144;&#26631;&#20934;&#35889;&#26041;&#27861;&#30340;GP&#26680;&#20989;&#25968;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#25512;&#26029;&#32447;&#24615;PDE&#31995;&#32479;&#30340;&#21487;&#33021;&#35299;&#65292;&#24182;&#20855;&#26377;&#31639;&#27861;&#24615;&#24378;&#12289;&#26222;&#36866;&#24615;&#24191;&#12289;&#36866;&#29992;&#20110;&#22823;&#25968;&#25454;&#38598;&#30340;&#31232;&#30095;&#29256;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#26159;&#24314;&#27169;&#29289;&#29702;&#31995;&#32479;&#30340;&#37325;&#35201;&#24037;&#20855;&#65292;&#23558;&#23427;&#20204;&#32435;&#20837;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26159;&#23558;&#29289;&#29702;&#30693;&#35782;&#32435;&#20837;&#30340;&#37325;&#35201;&#26041;&#24335;&#12290;&#23545;&#20110;&#20219;&#20309;&#20855;&#26377;&#24120;&#31995;&#25968;&#30340;&#32447;&#24615;PDE&#31995;&#32479;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#26063;&#31216;&#20026;EPGP&#30340;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#20808;&#39564;&#65292;&#20351;&#24471;&#25152;&#26377;&#23454;&#29616;&#37117;&#26159;&#35813;&#31995;&#32479;&#30340;&#31934;&#30830;&#35299;&#12290;&#25105;&#20204;&#24212;&#29992;Ehrenpreis-Palamodov&#22522;&#26412;&#21407;&#29702;&#65292;&#23427;&#20316;&#20026;&#19968;&#31181;&#38750;&#32447;&#24615;&#20613;&#37324;&#21494;&#21464;&#25442;&#65292;&#26500;&#24314;&#20102;GP&#26680;&#20989;&#25968;&#65292;&#21453;&#26144;&#20102;&#26631;&#20934;&#30340;&#35889;&#26041;&#27861;&#29992;&#20110;GP&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20174;&#20219;&#20309;&#25968;&#25454;&#65288;&#22914;&#26377;&#22122;&#22768;&#30340;&#27979;&#37327;&#25968;&#25454;&#25110;&#28857;&#23450;&#20041;&#30340;&#21021;&#22987;&#21644;&#36793;&#30028;&#26465;&#20214;&#65289;&#25512;&#26029;&#32447;&#24615;PDE&#31995;&#32479;&#30340;&#21487;&#33021;&#35299;&#12290;&#26500;&#36896;EPGP&#20808;&#39564;&#30340;&#31639;&#27861;&#24615;&#24378;&#65292;&#26222;&#36866;&#24615;&#24191;&#65292;&#24182;&#19988;&#26377;&#19968;&#20010;&#31232;&#30095;&#29256;&#26412;&#65288;S-EPGP&#65289;&#65292;&#21487;&#20197;&#23398;&#20064;&#30456;&#20851;&#30340;&#35889;&#39057;&#29575;&#65292;&#24182;&#22312;&#22823;&#25968;&#25454;&#38598;&#19978;&#36816;&#34892;&#25928;&#26524;&#26356;&#22909;&#12290;&#25105;&#20204;&#22312;&#19977;&#31867;PDE&#31995;&#32479;&#19978;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#28909;&#26041;&#31243;&#21644;&#27874;&#26041;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Partial differential equations (PDEs) are important tools to model physical systems and including them into machine learning models is an important way of incorporating physical knowledge. Given any system of linear PDEs with constant coefficients, we propose a family of Gaussian process (GP) priors, which we call EPGP, such that all realizations are exact solutions of this system. We apply the Ehrenpreis-Palamodov fundamental principle, which works as a non-linear Fourier transform, to construct GP kernels mirroring standard spectral methods for GPs. Our approach can infer probable solutions of linear PDE systems from any data such as noisy measurements, or pointwise defined initial and boundary conditions. Constructing EPGP-priors is algorithmic, generally applicable, and comes with a sparse version (S-EPGP) that learns the relevant spectral frequencies and works better for big data sets. We demonstrate our approach on three families of systems of PDEs, the heat equation, wave equati
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#26041;&#24046;&#32593;&#32476;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26641;&#29366;&#32467;&#26500;&#23398;&#20064;&#23558;&#29305;&#24449;&#31354;&#38388;&#20998;&#21106;&#20026;&#22810;&#20010;&#21306;&#22495;&#65292;&#24182;&#20351;&#29992;&#21306;&#22495;&#29305;&#23450;&#30340;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#22343;&#20540;&#21644;&#26041;&#24046;&#26469;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#26032;&#30340;&#20998;&#35010;&#20934;&#21017;&#65292;&#22312;&#35745;&#31639;&#19978;&#21451;&#22909;&#19988;&#19981;&#38656;&#35201;&#20462;&#21098;&#65292;&#36824;&#21487;&#20197;&#26500;&#24314;&#38598;&#21512;&#29256;&#26412;&#26469;&#20272;&#35745;&#24635;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.12658</link><description>&lt;p&gt;
&#25552;&#39640;&#26041;&#24046;&#32593;&#32476;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#26641;&#29366;&#32467;&#26500;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Improving Uncertainty Quantification of Variance Networks by Tree-Structured Learning. (arXiv:2212.12658v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.12658
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#26041;&#24046;&#32593;&#32476;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26641;&#29366;&#32467;&#26500;&#23398;&#20064;&#23558;&#29305;&#24449;&#31354;&#38388;&#20998;&#21106;&#20026;&#22810;&#20010;&#21306;&#22495;&#65292;&#24182;&#20351;&#29992;&#21306;&#22495;&#29305;&#23450;&#30340;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#22343;&#20540;&#21644;&#26041;&#24046;&#26469;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#26032;&#30340;&#20998;&#35010;&#20934;&#21017;&#65292;&#22312;&#35745;&#31639;&#19978;&#21451;&#22909;&#19988;&#19981;&#38656;&#35201;&#20462;&#21098;&#65292;&#36824;&#21487;&#20197;&#26500;&#24314;&#38598;&#21512;&#29256;&#26412;&#26469;&#20272;&#35745;&#24635;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25552;&#39640;&#26041;&#24046;&#32593;&#32476;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26641;&#29366;&#32467;&#26500;&#23616;&#37096;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#26681;&#25454;&#19981;&#30830;&#23450;&#24615;&#30340;&#24322;&#36136;&#24615;&#23558;&#29305;&#24449;&#31354;&#38388;&#21010;&#20998;&#20026;&#22810;&#20010;&#21306;&#22495;&#12290;&#26681;&#25454;&#35757;&#32451;&#25968;&#25454;&#65292;&#24314;&#31435;&#19968;&#26869;&#26641;&#65292;&#20854;&#21494;&#33410;&#28857;&#20195;&#34920;&#19981;&#21516;&#30340;&#21306;&#22495;&#65292;&#22312;&#36825;&#20123;&#21306;&#22495;&#29305;&#23450;&#30340;&#31070;&#32463;&#32593;&#32476;&#20013;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#39044;&#27979;&#22343;&#20540;&#21644;&#26041;&#24046;&#20197;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#19981;&#30830;&#23450;&#24615;&#20998;&#35010;&#31070;&#32463;&#22238;&#24402;&#26641; (USNRT)&#37319;&#29992;&#20102;&#26032;&#39062;&#30340;&#20998;&#35010;&#20934;&#21017;&#12290;&#22312;&#27599;&#20010;&#33410;&#28857;&#19978;&#65292;&#39318;&#20808;&#23545;&#20840;&#25968;&#25454;&#36827;&#34892;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#65292;&#28982;&#21518;&#23545;&#27531;&#24046;&#36827;&#34892;&#32479;&#35745;&#26816;&#39564;&#65292;&#25214;&#21040;&#26368;&#20339;&#20998;&#35010;&#65292;&#23545;&#24212;&#20855;&#26377;&#26368;&#26174;&#33879;&#19981;&#30830;&#23450;&#24615;&#24322;&#36136;&#24615;&#30340;&#20004;&#20010;&#23376;&#21306;&#22495;&#12290;USNRT&#22312;&#35745;&#31639;&#19978;&#21451;&#22909;&#65292;&#21482;&#38656;&#24456;&#23569;&#30340;&#21494;&#33410;&#28857;&#21363;&#21487;&#28385;&#36275;&#35201;&#27714;&#65292;&#26080;&#38656;&#36827;&#34892;&#20462;&#21098;&#12290;&#27492;&#22806;&#65292;&#36824;&#21487;&#20197;&#36731;&#26494;&#26500;&#24314;&#38598;&#21512;&#29256;&#26412;&#20197;&#20272;&#35745;&#21253;&#25324; aleatory &#30340;&#24635;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
To improve the uncertainty quantification of variance networks, we propose a novel tree-structured local neural network model that partitions the feature space into multiple regions based on uncertainty heterogeneity. A tree is built upon giving the training data, whose leaf nodes represent different regions where region-specific neural networks are trained to predict both the mean and the variance for quantifying uncertainty. The proposed Uncertainty-Splitting Neural Regression Tree (USNRT) employs novel splitting criteria. At each node, a neural network is trained on the full data first, and a statistical test for the residuals is conducted to find the best split, corresponding to the two sub-regions with the most significant uncertainty heterogeneity between them. USNRT is computationally friendly because very few leaf nodes are sufficient and pruning is unnecessary. Furthermore, an ensemble version can be easily constructed to estimate the total uncertainty including the aleatory a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#26512;&#20102;&#28151;&#27788;&#21644;&#28237;&#27969;&#30340;&#30456;&#23545;&#22797;&#26434;&#24615;&#65292;&#24182;&#20351;&#29992;&#20869;&#22312;&#32500;&#24230;&#21644;&#26377;&#25928;&#29420;&#31435;&#29305;&#24449;&#25968;&#37327;&#37327;&#21270;&#20102;&#32593;&#32476;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2211.15382</link><description>&lt;p&gt;
&#28151;&#27788;&#19982;&#28237;&#27969;&#30340;&#31070;&#32463;&#32593;&#32476;&#22797;&#26434;&#24615;
&lt;/p&gt;
&lt;p&gt;
Neural Network Complexity of Chaos and Turbulence. (arXiv:2211.15382v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15382
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#26512;&#20102;&#28151;&#27788;&#21644;&#28237;&#27969;&#30340;&#30456;&#23545;&#22797;&#26434;&#24615;&#65292;&#24182;&#20351;&#29992;&#20869;&#22312;&#32500;&#24230;&#21644;&#26377;&#25928;&#29420;&#31435;&#29305;&#24449;&#25968;&#37327;&#37327;&#21270;&#20102;&#32593;&#32476;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#27788;&#21644;&#28237;&#27969;&#26159;&#22797;&#26434;&#30340;&#29289;&#29702;&#29616;&#35937;&#65292;&#28982;&#32780;&#23545;&#20110;&#37327;&#21270;&#23427;&#20204;&#30340;&#22797;&#26434;&#24230;&#30340;&#31934;&#30830;&#23450;&#20041;&#20173;&#28982;&#32570;&#20047;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20174;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35282;&#24230;&#32771;&#34385;&#20102;&#28151;&#27788;&#21644;&#28237;&#27969;&#30340;&#30456;&#23545;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#19968;&#31995;&#21015;&#20998;&#31867;&#38382;&#39064;&#65292;&#32593;&#32476;&#38656;&#35201;&#21306;&#20998;&#28237;&#27969;&#29366;&#24577;&#19979;&#30340;&#27969;&#20307;&#36718;&#24275;&#22270;&#20687;&#21644;&#20854;&#20182;&#31867;&#21035;&#30340;&#22270;&#20687;&#65292;&#22914;&#28151;&#27788;&#29366;&#24577;&#19979;&#30340;&#27969;&#20307;&#36718;&#24275;&#12289;&#21508;&#31181;&#22122;&#22768;&#26500;&#36896;&#21644;&#30495;&#23454;&#19990;&#30028;&#22270;&#20687;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#19981;&#21487;&#21387;&#32553;&#21644;&#24369;&#21387;&#32553;&#27969;&#20307;&#27969;&#21160;&#12290;&#25105;&#20204;&#36890;&#36807;&#20869;&#37096;&#29305;&#24449;&#34920;&#31034;&#30340;&#20869;&#22312;&#32500;&#24230;&#37327;&#21270;&#32593;&#32476;&#25191;&#34892;&#30340;&#35745;&#31639;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#35745;&#31639;&#32593;&#32476;&#29992;&#20110;&#21306;&#20998;&#31867;&#21035;&#30340;&#26377;&#25928;&#29420;&#31435;&#29305;&#24449;&#25968;&#37327;&#12290;&#38500;&#20102;&#25552;&#20379;&#35745;&#31639;&#22797;&#26434;&#24230;&#30340;&#25968;&#20540;&#20272;&#35745;&#22806;&#65292;&#35813;&#24230;&#37327;&#36824;&#34920;&#24449;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#22788;&#29702;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chaos and turbulence are complex physical phenomena, yet a precise definition of the complexity measure that quantifies them is still lacking. In this work we consider the relative complexity of chaos and turbulence from the perspective of deep neural networks. We analyze a set of classification problems, where the network has to distinguish images of fluid profiles in the turbulent regime from other classes of images such as fluid profiles in the chaotic regime, various constructions of noise and real world images. We analyze incompressible as well as weakly compressible fluid flows. We quantify the complexity of the computation performed by the network via the intrinsic dimensionality of the internal feature representations, and calculate the effective number of independent features which the network uses in order to distinguish between classes. In addition to providing a numerical estimate of the complexity of the computation, the measure also characterizes the neural network proces
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#24352;&#37327;&#32593;&#32476;&#30340;&#29305;&#24449;&#31354;&#38388;&#36317;&#31163;&#26041;&#27861;&#26469;&#35299;&#20915;&#27491;&#26080;&#26631;&#35760;&#23398;&#20064;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#29305;&#23450;&#39046;&#22495;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;MNIST&#22270;&#20687;&#21644;15&#20010;&#20998;&#31867;/&#28151;&#21512;&#25968;&#25454;&#38598;&#30340;&#32467;&#26524;&#65292;&#24182;&#21487;&#20197;&#29983;&#25104;&#26032;&#30340;&#27491;&#38754;&#21644;&#36127;&#38754;&#23454;&#20363;&#12290;</title><link>http://arxiv.org/abs/2211.14085</link><description>&lt;p&gt;
&#20351;&#29992;&#24352;&#37327;&#32593;&#32476;&#36827;&#34892;&#27491;&#26080;&#26631;&#35760;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Positive unlabeled learning with tensor networks. (arXiv:2211.14085v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14085
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#24352;&#37327;&#32593;&#32476;&#30340;&#29305;&#24449;&#31354;&#38388;&#36317;&#31163;&#26041;&#27861;&#26469;&#35299;&#20915;&#27491;&#26080;&#26631;&#35760;&#23398;&#20064;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#29305;&#23450;&#39046;&#22495;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;MNIST&#22270;&#20687;&#21644;15&#20010;&#20998;&#31867;/&#28151;&#21512;&#25968;&#25454;&#38598;&#30340;&#32467;&#26524;&#65292;&#24182;&#21487;&#20197;&#29983;&#25104;&#26032;&#30340;&#27491;&#38754;&#21644;&#36127;&#38754;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27491;&#26080;&#26631;&#35760;&#23398;&#20064;&#26159;&#19968;&#20010;&#21253;&#21547;&#27491;&#38754;&#21644;&#26080;&#26631;&#35760;&#25968;&#25454;&#30340;&#20108;&#20803;&#20998;&#31867;&#38382;&#39064;&#12290;&#23427;&#22312;&#36127;&#38754;&#26631;&#31614;&#26114;&#36149;&#25110;&#26080;&#27861;&#33719;&#21462;&#30340;&#39046;&#22495;&#65288;&#22914;&#21307;&#33647;&#21644;&#20010;&#24615;&#21270;&#24191;&#21578;&#65289;&#20013;&#24456;&#24120;&#35265;&#12290;&#22823;&#22810;&#25968;&#27491;&#26080;&#26631;&#35760;&#23398;&#20064;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#29305;&#23450;&#30340;&#25968;&#25454;&#31867;&#22411;&#65288;&#22914;&#22270;&#20687;&#65292;&#20998;&#31867;&#25968;&#25454;&#65289;&#65292;&#19981;&#33021;&#29983;&#25104;&#26032;&#30340;&#27491;&#38754;&#21644;&#36127;&#38754;&#26679;&#26412;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#31354;&#38388;&#36317;&#31163;&#30340;&#24352;&#37327;&#32593;&#32476;&#26041;&#27861;&#26469;&#35299;&#20915;&#27491;&#26080;&#26631;&#35760;&#23398;&#20064;&#38382;&#39064;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#19981;&#20381;&#36182;&#39046;&#22495;&#65292;&#24182;&#22312;MNIST&#22270;&#20687;&#21644;15&#20010;&#20998;&#31867;/&#28151;&#21512;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#25913;&#36827;&#20102;&#26368;&#26032;&#30340;&#32467;&#26524;&#12290;&#35757;&#32451;&#30340;&#24352;&#37327;&#32593;&#32476;&#27169;&#22411;&#36824;&#26159;&#19968;&#20010;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#29983;&#25104;&#26032;&#30340;&#27491;&#38754;&#21644;&#36127;&#38754;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Positive unlabeled learning is a binary classification problem with positive and unlabeled data. It is common in domains where negative labels are costly or impossible to obtain, e.g., medicine and personalized advertising. Most approaches to positive unlabeled learning apply to specific data types (e.g., images, categorical data) and can not generate new positive and negative samples. This work introduces a feature-space distance-based tensor network approach to the positive unlabeled learning problem. The presented method is not domain specific and significantly improves the state-of-the-art results on the MNIST image and 15 categorical/mixed datasets. The trained tensor network model is also a generative model and enables the generation of new positive and negative instances.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#30340;&#22238;&#39038;&#20013;&#30340;&#22909;&#22855;&#24515;&#26041;&#27861;&#65292;&#29992;&#20110;&#31232;&#30095;&#22870;&#21169;&#25110;&#26080;&#22870;&#21169;&#29615;&#22659;&#20013;&#30340;&#25506;&#32034;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#23398;&#20064;&#26410;&#26469;&#30340;&#34920;&#24449;&#65292;&#20197;&#25429;&#25417;&#27599;&#20010;&#32467;&#26524;&#30340;&#19981;&#21487;&#39044;&#27979;&#37096;&#20998;&#65292;&#24182;&#23558;&#20854;&#29992;&#20316;&#39044;&#27979;&#30340;&#39069;&#22806;&#36755;&#20837;&#65292;&#20174;&#32780;&#33719;&#24471;&#40065;&#26834;&#30340;&#20869;&#22312;&#22870;&#21169;&#12290;</title><link>http://arxiv.org/abs/2211.10515</link><description>&lt;p&gt;
&#22238;&#39038;&#20013;&#30340;&#22909;&#22855;&#24515;&#65306;&#38543;&#26426;&#29615;&#22659;&#20013;&#30340;&#20869;&#22312;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Curiosity in Hindsight: Intrinsic Exploration in Stochastic Environments. (arXiv:2211.10515v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.10515
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#30340;&#22238;&#39038;&#20013;&#30340;&#22909;&#22855;&#24515;&#26041;&#27861;&#65292;&#29992;&#20110;&#31232;&#30095;&#22870;&#21169;&#25110;&#26080;&#22870;&#21169;&#29615;&#22659;&#20013;&#30340;&#25506;&#32034;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#23398;&#20064;&#26410;&#26469;&#30340;&#34920;&#24449;&#65292;&#20197;&#25429;&#25417;&#27599;&#20010;&#32467;&#26524;&#30340;&#19981;&#21487;&#39044;&#27979;&#37096;&#20998;&#65292;&#24182;&#23558;&#20854;&#29992;&#20316;&#39044;&#27979;&#30340;&#39069;&#22806;&#36755;&#20837;&#65292;&#20174;&#32780;&#33719;&#24471;&#40065;&#26834;&#30340;&#20869;&#22312;&#22870;&#21169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#22312;&#31232;&#30095;&#22870;&#21169;&#25110;&#26080;&#22870;&#21169;&#29615;&#22659;&#20013;&#30340;&#25506;&#32034;&#38382;&#39064;&#65292;&#22914;Montezuma's Revenge&#12290;&#22312;&#22909;&#22855;&#24515;&#39537;&#21160;&#33539;&#24335;&#20013;&#65292;&#20195;&#29702;&#34987;&#22870;&#21169;&#23454;&#38469;&#32467;&#26524;&#19982;&#39044;&#27979;&#32467;&#26524;&#30340;&#24046;&#24322;&#12290;&#20294;&#22312;&#38543;&#26426;&#29615;&#22659;&#20013;&#65292;&#20351;&#29992;&#39044;&#27979;&#35823;&#24046;&#20316;&#20026;&#20869;&#22312;&#21160;&#26426;&#26159;&#33030;&#24369;&#30340;&#65292;&#22240;&#20026;&#20195;&#29702;&#21487;&#33021;&#34987;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#20013;&#39640;&#29109;&#21306;&#22495;&#65288;&#22914;&#8220;&#22122;&#22768;&#30005;&#35270;&#8221;&#65289;&#25152;&#22256;&#20303;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#30340;&#33258;&#28982;&#35299;&#20915;&#26041;&#26696;&#65306;&#23398;&#20064;&#26410;&#26469;&#30340;&#34920;&#24449;&#65292;&#31934;&#30830;&#22320;&#25429;&#25417;&#27599;&#20010;&#32467;&#26524;&#30340;&#19981;&#21487;&#39044;&#27979;&#26041;&#38754;&#65292;&#24182;&#23558;&#20854;&#29992;&#20316;&#39044;&#27979;&#30340;&#39069;&#22806;&#36755;&#20837;&#65292;&#20174;&#32780;&#20351;&#20869;&#22312;&#22870;&#21169;&#20165;&#21453;&#26144;&#19990;&#30028;&#21160;&#24577;&#30340;&#21487;&#39044;&#27979;&#26041;&#38754;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#36825;&#31181;&#22238;&#39038;&#34920;&#24449;&#32467;&#21512;&#21040;&#27169;&#22411;&#20013;&#65292;&#20197;&#23558;&#8220;&#22122;&#22768;&#8221;&#19982;&#8220;&#26032;&#22855;&#8221;&#21306;&#20998;&#24320;&#26469;&#65292;&#24471;&#21040;&#20102;&#22238;&#39038;&#20013;&#30340;&#22909;&#22855;&#24515;&#65306;&#19968;&#31181;&#31616;&#21333;&#32780;&#21487;&#25193;&#23637;&#30340;&#22909;&#22855;&#24515;&#27867;&#21270;&#26041;&#27861;&#65292;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Consider the problem of exploration in sparse-reward or reward-free environments, such as in Montezuma's Revenge. In the curiosity-driven paradigm, the agent is rewarded for how much each realized outcome differs from their predicted outcome. But using predictive error as intrinsic motivation is fragile in stochastic environments, as the agent may become trapped by high-entropy areas of the state-action space, such as a "noisy TV". In this work, we study a natural solution derived from structural causal models of the world: Our key idea is to learn representations of the future that capture precisely the unpredictable aspects of each outcome -- which we use as additional input for predictions, such that intrinsic rewards only reflect the predictable aspects of world dynamics. First, we propose incorporating such hindsight representations into models to disentangle "noise" from "novelty", yielding Curiosity in Hindsight: a simple and scalable generalization of curiosity that is robust t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#21442;&#25968;&#20989;&#25968;&#36924;&#36817;&#30340;&#20840;&#23616;&#20248;&#21270;&#31639;&#27861;GO-UCB&#65292;&#37319;&#29992;&#22522;&#20110;&#26799;&#24230;&#30340;&#21442;&#25968;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#36827;&#34892;&#20048;&#35266;&#25506;&#32034;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#22788;&#29702;&#20855;&#26377;&#22122;&#22768;&#38646;&#38454;&#35775;&#38382;&#22120;&#30340;&#20840;&#23616;&#20248;&#21270;&#38382;&#39064;&#19978;&#20248;&#20110;&#20256;&#32479;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2211.09100</link><description>&lt;p&gt;
&#20855;&#26377;&#21442;&#25968;&#20989;&#25968;&#36924;&#36817;&#30340;&#20840;&#23616;&#20248;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Global Optimization with Parametric Function Approximation. (arXiv:2211.09100v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09100
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#21442;&#25968;&#20989;&#25968;&#36924;&#36817;&#30340;&#20840;&#23616;&#20248;&#21270;&#31639;&#27861;GO-UCB&#65292;&#37319;&#29992;&#22522;&#20110;&#26799;&#24230;&#30340;&#21442;&#25968;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#36827;&#34892;&#20048;&#35266;&#25506;&#32034;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#22788;&#29702;&#20855;&#26377;&#22122;&#22768;&#38646;&#38454;&#35775;&#38382;&#22120;&#30340;&#20840;&#23616;&#20248;&#21270;&#38382;&#39064;&#19978;&#20248;&#20110;&#20256;&#32479;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#22122;&#22768;&#38646;&#38454;&#35775;&#38382;&#22120;&#30340;&#20840;&#23616;&#20248;&#21270;&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#20010;&#26377;&#29992;&#20110;&#20174;&#28145;&#24230;&#23398;&#20064;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#21040;&#26032;&#26448;&#26009;&#35774;&#35745;&#31561;&#21508;&#31181;&#24212;&#29992;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#20381;&#36182;&#20110;&#39640;&#26031;&#36807;&#31243;&#25110;&#20854;&#20182;&#38750;&#21442;&#25968;&#23478;&#26063;&#65292;&#36825;&#20123;&#26041;&#27861;&#21463;&#21040;&#32500;&#24230;&#28798;&#38590;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;GO-UCB&#65292;&#23427;&#37319;&#29992;&#21442;&#25968;&#21270;&#20989;&#25968;&#23478;&#26063;&#65288;&#22914;&#31070;&#32463;&#32593;&#32476;&#65289;&#12290;&#22312;&#19968;&#20010;&#21487;&#23454;&#29616;&#30340;&#20551;&#35774;&#21644;&#19968;&#20123;&#20854;&#20182;&#28201;&#21644;&#30340;&#20960;&#20309;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;GO-UCB&#23454;&#29616;&#20102;&#19968;&#20010;&#32047;&#31215;&#36951;&#25022;&#24230;$\sim O(\sqrt{T})$&#65292;&#20854;&#20013;$T$&#26159;&#26102;&#38388;&#36328;&#24230;&#12290;GO-UCB&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#22522;&#20110;&#26799;&#24230;&#30340;&#31934;&#24515;&#35774;&#35745;&#30340;&#21442;&#25968;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#65292;&#21487;&#20197;&#36827;&#34892;&#20048;&#35266;&#30340;&#25506;&#32034;&#12290;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#21363;&#20351;&#27169;&#22411;&#34987;&#38169;&#35823;&#25351;&#23450;&#65292;GO-UCB&#30340;&#25928;&#26524;&#20063;&#20248;&#20110;&#27969;&#34892;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of global optimization with noisy zeroth order oracles - a well-motivated problem useful for various applications ranging from hyper-parameter tuning for deep learning to new material design. Existing work relies on Gaussian processes or other non-parametric family, which suffers from the curse of dimensionality. In this paper, we propose a new algorithm GO-UCB that leverages a parametric family of functions (e.g., neural networks) instead. Under a realizable assumption and a few other mild geometric conditions, we show that GO-UCB achieves a cumulative regret of \~O$(\sqrt{T})$ where $T$ is the time horizon. At the core of GO-UCB is a carefully designed uncertainty set over parameters based on gradients that allows optimistic exploration. Synthetic and real-world experiments illustrate GO-UCB works better than popular Bayesian optimization approaches, even if the model is misspecified.
&lt;/p&gt;</description></item><item><title>&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#30340;&#35774;&#32622;&#65292;&#20026;&#20855;&#26377;&#32447;&#24615;&#32467;&#26500;&#30340;MDPs&#30830;&#23450;&#20102;&#25152;&#38656;&#30340;&#22312;&#32447;&#26679;&#26412;&#25968;&#37327;&#65292;&#24182;&#25552;&#20379;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#30340;&#24615;&#36136;&#21644;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2211.04974</link><description>&lt;p&gt;
&#22312;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Leveraging Offline Data in Online Reinforcement Learning. (arXiv:2211.04974v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.04974
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#30340;&#35774;&#32622;&#65292;&#20026;&#20855;&#26377;&#32447;&#24615;&#32467;&#26500;&#30340;MDPs&#30830;&#23450;&#20102;&#25152;&#38656;&#30340;&#22312;&#32447;&#26679;&#26412;&#25968;&#37327;&#65292;&#24182;&#25552;&#20379;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#30340;&#24615;&#36136;&#21644;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#20986;&#29616;&#20102;&#20004;&#20010;&#26680;&#24515;&#33539;&#24335;&#65306;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#21644;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#12290;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#26234;&#33021;&#20307;&#23545;&#29615;&#22659;&#27809;&#26377;&#20808;&#39564;&#30693;&#35782;&#65292;&#24517;&#39035;&#19982;&#29615;&#22659;&#20132;&#20114;&#20197;&#25214;&#21040;&#19968;&#20010; &#949;-&#26368;&#20248;&#31574;&#30053;&#12290;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#23398;&#20064;&#22120;&#21487;&#20197;&#20174;&#19968;&#20010;&#22266;&#23450;&#30340;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#65292;&#20294;&#26080;&#27861;&#19982;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#65292;&#24517;&#39035;&#36890;&#36807;&#31163;&#32447;&#25968;&#25454;&#33719;&#21462;&#26368;&#20339;&#31574;&#30053;&#12290;&#23454;&#38469;&#24773;&#20917;&#36890;&#24120;&#38656;&#35201;&#19968;&#20010;&#20013;&#38388;&#30340;&#35774;&#32622;&#65306;&#22914;&#26524;&#25105;&#20204;&#26377;&#19968;&#20123;&#31163;&#32447;&#25968;&#25454;&#65292;&#24182;&#19988;&#36824;&#21487;&#20197;&#19982;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#65292;&#25105;&#20204;&#22914;&#20309;&#26368;&#22909;&#22320;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#26469;&#20943;&#23569;&#23398;&#20064;&#19968;&#20010; &#949;-&#26368;&#20248;&#31574;&#30053;&#25152;&#38656;&#30340;&#22312;&#32447;&#20132;&#20114;&#27425;&#25968;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#36825;&#20010;&#35774;&#32622;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;FineTuneRL&#35774;&#32622;&#65292;&#29992;&#20110;&#20855;&#26377;&#32447;&#24615;&#32467;&#26500;&#30340;MDPs&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#22312;&#32473;&#23450;&#19968;&#20123;&#31163;&#32447;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#38656;&#35201;&#30340;&#22312;&#32447;&#26679;&#26412;&#25968;&#37327;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20123;&#24615;&#36136;&#21644;&#31639;&#27861;&#26469;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Two central paradigms have emerged in the reinforcement learning (RL) community: online RL and offline RL. In the online RL setting, the agent has no prior knowledge of the environment, and must interact with it in order to find an $\epsilon$-optimal policy. In the offline RL setting, the learner instead has access to a fixed dataset to learn from, but is unable to otherwise interact with the environment, and must obtain the best policy it can from this offline data. Practical scenarios often motivate an intermediate setting: if we have some set of offline data and, in addition, may also interact with the environment, how can we best use the offline data to minimize the number of online interactions necessary to learn an $\epsilon$-optimal policy?  In this work, we consider this setting, which we call the \textsf{FineTuneRL} setting, for MDPs with linear structure. We characterize the necessary number of online samples needed in this setting given access to some offline dataset, and de
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#12289;&#23454;&#26102;&#35299;&#20915;&#36870;&#24378;&#21270;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#22810;&#20010;&#35299;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#25910;&#25947;&#21040;&#36817;&#20284;&#31561;&#20215;&#35299;&#30340;&#27491;&#21017;&#21270;&#21382;&#21490;&#22534;&#26632;&#35266;&#23519;&#22120;&#12290;&#36890;&#36807;&#24320;&#21457;&#26032;&#30340;&#25968;&#25454;&#20016;&#23500;&#24615;&#26465;&#20214;&#65292;&#35777;&#26126;&#20102;&#35813;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.16299</link><description>&lt;p&gt;
&#22522;&#20110;&#35266;&#27979;&#22120;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38750;&#21807;&#19968;&#24615;&#21644;&#31561;&#20215;&#35299;&#30340;&#25910;&#25947;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Nonuniqueness and Convergence to Equivalent Solutions in Observer-based Inverse Reinforcement Learning. (arXiv:2210.16299v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.16299
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#12289;&#23454;&#26102;&#35299;&#20915;&#36870;&#24378;&#21270;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#22810;&#20010;&#35299;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#25910;&#25947;&#21040;&#36817;&#20284;&#31561;&#20215;&#35299;&#30340;&#27491;&#21017;&#21270;&#21382;&#21490;&#22534;&#26632;&#35266;&#23519;&#22120;&#12290;&#36890;&#36807;&#24320;&#21457;&#26032;&#30340;&#25968;&#25454;&#20016;&#23500;&#24615;&#26465;&#20214;&#65292;&#35777;&#26126;&#20102;&#35813;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22312;&#32447;&#21644;&#23454;&#26102;&#35299;&#20915;&#30830;&#23450;&#24615;&#36870;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#20013;&#65292;&#23384;&#22312;&#22810;&#20010;&#35299;&#30340;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#38750;&#21807;&#19968;&#24615;&#38656;&#35201;&#30740;&#31350;&#31561;&#20215;&#35299;&#30340;&#27010;&#24565;&#65292;&#21363;&#32467;&#26524;&#22312;&#19981;&#21516;&#30340;&#20195;&#20215;&#20989;&#25968;&#20294;&#30456;&#21516;&#30340;&#21453;&#39304;&#30697;&#38453;&#65292;&#20197;&#21450;&#25910;&#25947;&#21040;&#36825;&#20123;&#35299;&#30340;&#26041;&#27861;&#12290;&#23613;&#31649;&#24050;&#32463;&#22312;&#25991;&#29486;&#20013;&#24320;&#21457;&#20102;&#31163;&#32447;&#31639;&#27861;&#20197;&#25910;&#25947;&#21040;&#31561;&#20215;&#35299;&#65292;&#20294;&#23578;&#26410;&#25552;&#20379;&#35299;&#20915;&#38750;&#21807;&#19968;&#24615;&#30340;&#22312;&#32447;&#12289;&#23454;&#26102;&#25216;&#26415;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#25910;&#25947;&#21040;&#36870;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#36817;&#20284;&#31561;&#20215;&#35299;&#30340;&#27491;&#21017;&#21270;&#21382;&#21490;&#22534;&#26632;&#35266;&#23519;&#22120;&#12290;&#21457;&#23637;&#20102;&#26032;&#30340;&#25968;&#25454;&#20016;&#23500;&#24615;&#26465;&#20214;&#20197;&#20419;&#36827;&#20998;&#26512;&#65292;&#24182;&#36890;&#36807;&#27169;&#25311;&#32467;&#26524;&#23637;&#31034;&#20102;&#25152;&#24320;&#21457;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
A key challenge in solving the deterministic inverse reinforcement learning (IRL) problem online and in real-time is the existence of multiple solutions. Nonuniqueness necessitates the study of the notion of equivalent solutions, i.e., solutions that result in a different cost functional but same feedback matrix, and convergence to such solutions. While offline algorithms that result in convergence to equivalent solutions have been developed in the literature, online, real-time techniques that address nonuniqueness are not available. In this paper, a regularized history stack observer that converges to approximately equivalent solutions of the IRL problem is developed. Novel data-richness conditions are developed to facilitate the analysis and simulation results are provided to demonstrate the effectiveness of the developed technique.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39640;&#25928;&#22686;&#24378;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#36890;&#36807;&#36873;&#25321;&#25968;&#25454;&#23376;&#38598;&#36827;&#34892;&#22686;&#24378;&#26469;&#36817;&#20284;&#25429;&#25417;&#23436;&#20840;&#25968;&#25454;&#22686;&#24378;&#30340;&#35757;&#32451;&#21160;&#24577;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25918;&#22823;&#21644;&#25200;&#21160;&#32593;&#32476;&#38597;&#21487;&#27604;&#30697;&#38453;&#30340;&#36739;&#23567;&#22855;&#24322;&#20540;&#65292;&#20445;&#30041;&#20854;&#26174;&#33879;&#26041;&#21521;&#65292;&#20174;&#32780;&#25913;&#36827;&#23398;&#20064;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#19988;&#36890;&#36807;&#36845;&#20195;&#22320;&#25552;&#21462;&#23567;&#30340;&#35757;&#32451;&#25968;&#25454;&#23376;&#38598;&#65292;&#33021;&#22815;&#25429;&#25417;&#21040;&#23436;&#20840;&#22686;&#24378;&#30340;&#38597;&#21487;&#27604;&#30697;&#38453;&#19982;&#26631;&#31614;/&#27531;&#24046;&#30340;&#23545;&#40784;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2210.08363</link><description>&lt;p&gt;
&#25968;&#25454;&#39640;&#25928;&#22686;&#24378;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Data-Efficient Augmentation for Training Neural Networks. (arXiv:2210.08363v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.08363
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39640;&#25928;&#22686;&#24378;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#36890;&#36807;&#36873;&#25321;&#25968;&#25454;&#23376;&#38598;&#36827;&#34892;&#22686;&#24378;&#26469;&#36817;&#20284;&#25429;&#25417;&#23436;&#20840;&#25968;&#25454;&#22686;&#24378;&#30340;&#35757;&#32451;&#21160;&#24577;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25918;&#22823;&#21644;&#25200;&#21160;&#32593;&#32476;&#38597;&#21487;&#27604;&#30697;&#38453;&#30340;&#36739;&#23567;&#22855;&#24322;&#20540;&#65292;&#20445;&#30041;&#20854;&#26174;&#33879;&#26041;&#21521;&#65292;&#20174;&#32780;&#25913;&#36827;&#23398;&#20064;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#19988;&#36890;&#36807;&#36845;&#20195;&#22320;&#25552;&#21462;&#23567;&#30340;&#35757;&#32451;&#25968;&#25454;&#23376;&#38598;&#65292;&#33021;&#22815;&#25429;&#25417;&#21040;&#23436;&#20840;&#22686;&#24378;&#30340;&#38597;&#21487;&#27604;&#30697;&#38453;&#19982;&#26631;&#31614;/&#27531;&#24046;&#30340;&#23545;&#40784;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#22312;&#35768;&#22810;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20013;&#26159;&#23454;&#29616;&#26368;&#20808;&#36827;&#24615;&#33021;&#30340;&#20851;&#38190;&#65292;&#20294;&#26159;&#23545;&#20110;&#20013;&#31561;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#26368;&#26377;&#25928;&#30340;&#22686;&#24378;&#25216;&#26415;&#22312;&#35745;&#31639;&#19978;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20005;&#26684;&#30340;&#25216;&#26415;&#26469;&#36873;&#25321;&#25968;&#25454;&#23376;&#38598;&#65292;&#24403;&#36827;&#34892;&#22686;&#24378;&#26102;&#65292;&#21487;&#20197;&#36817;&#20284;&#25429;&#25417;&#23436;&#20840;&#25968;&#25454;&#22686;&#24378;&#30340;&#35757;&#32451;&#21160;&#24577;&#12290;&#39318;&#20808;&#25105;&#20204;&#35777;&#26126;&#20102;&#23558;&#25968;&#25454;&#22686;&#24378;&#24314;&#27169;&#20026;&#21152;&#24615;&#25200;&#21160;&#30340;&#26041;&#24335;&#21487;&#20197;&#25913;&#36827;&#23398;&#20064;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#36890;&#36807;&#30456;&#23545;&#25918;&#22823;&#21644;&#25200;&#21160;&#32593;&#32476;&#38597;&#21487;&#27604;&#30697;&#38453;&#30340;&#36739;&#23567;&#22855;&#24322;&#20540;&#65292;&#21516;&#26102;&#20445;&#30041;&#20854;&#26174;&#33879;&#26041;&#21521;&#12290;&#36825;&#26679;&#21487;&#20197;&#38450;&#27490;&#36807;&#25311;&#21512;&#24182;&#22686;&#24378;&#23545;&#38590;&#20197;&#23398;&#20064;&#30340;&#20449;&#24687;&#30340;&#23398;&#20064;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#22320;&#25552;&#21462;&#23567;&#30340;&#35757;&#32451;&#25968;&#25454;&#23376;&#38598;&#65292;&#24403;&#36825;&#20123;&#23376;&#38598;&#36827;&#34892;&#22686;&#24378;&#26102;&#65292;&#33021;&#22815;&#25429;&#25417;&#21040;&#23436;&#20840;&#22686;&#24378;&#30340;&#38597;&#21487;&#27604;&#30697;&#38453;&#19982;&#26631;&#31614;/&#27531;&#24046;&#30340;&#23545;&#40784;&#20851;&#31995;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;&#25214;&#21040;&#30340;&#22686;&#24378;&#23376;&#38598;&#19978;&#24212;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#21487;&#20197;&#36798;&#21040;&#19982;&#23436;&#20840;&#25968;&#25454;&#22686;&#24378;&#30456;&#20284;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data augmentation is essential to achieve state-of-the-art performance in many deep learning applications. However, the most effective augmentation techniques become computationally prohibitive for even medium-sized datasets. To address this, we propose a rigorous technique to select subsets of data points that when augmented, closely capture the training dynamics of full data augmentation. We first show that data augmentation, modeled as additive perturbations, improves learning and generalization by relatively enlarging and perturbing the smaller singular values of the network Jacobian, while preserving its prominent directions. This prevents overfitting and enhances learning the harder to learn information. Then, we propose a framework to iteratively extract small subsets of training data that when augmented, closely capture the alignment of the fully augmented Jacobian with labels/residuals. We prove that stochastic gradient descent applied to the augmented subsets found by our app
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#40065;&#26834;&#23398;&#20064;&#20013;&#20351;&#29992;&#23616;&#37096;&#26597;&#35810;&#30340;&#23454;&#29992;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#38024;&#23545;&#36825;&#31181;&#40065;&#26834;&#24615;&#27010;&#24565;&#36827;&#34892;&#40065;&#26834;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#30340;&#26080;&#20998;&#24067;&#31639;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#22343;&#21248;&#20998;&#24067;&#19979;&#65292;&#23616;&#37096;&#25104;&#21592;&#26597;&#35810;&#19981;&#20250;&#22686;&#21152;&#24182;&#19988;&#19981;&#20250;&#36229;&#36807;&#36830;&#35789;&#21644;&#20219;&#20309;&#36229;&#31867;&#30340;&#40065;&#26834;&#24615;&#38408;&#20540;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#23616;&#37096;&#31561;&#20215;&#26597;&#35810;oracle&#65292;&#29992;&#20110;&#22312;&#35757;&#32451;&#26679;&#26412;&#20013;&#21028;&#26029;&#20551;&#35774;&#21644;&#30446;&#26631;&#27010;&#24565;&#22312;&#25200;&#21160;&#21306;&#22495;&#20869;&#26159;&#21542;&#19968;&#33268;&#12290;</title><link>http://arxiv.org/abs/2210.06089</link><description>&lt;p&gt;
&#20309;&#26102;&#23616;&#37096;&#26597;&#35810;&#22312;&#40065;&#26834;&#23398;&#20064;&#20013;&#26377;&#29992;&#65311;
&lt;/p&gt;
&lt;p&gt;
When are Local Queries Useful for Robust Learning?. (arXiv:2210.06089v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06089
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#40065;&#26834;&#23398;&#20064;&#20013;&#20351;&#29992;&#23616;&#37096;&#26597;&#35810;&#30340;&#23454;&#29992;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#38024;&#23545;&#36825;&#31181;&#40065;&#26834;&#24615;&#27010;&#24565;&#36827;&#34892;&#40065;&#26834;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#30340;&#26080;&#20998;&#24067;&#31639;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#22343;&#21248;&#20998;&#24067;&#19979;&#65292;&#23616;&#37096;&#25104;&#21592;&#26597;&#35810;&#19981;&#20250;&#22686;&#21152;&#24182;&#19988;&#19981;&#20250;&#36229;&#36807;&#36830;&#35789;&#21644;&#20219;&#20309;&#36229;&#31867;&#30340;&#40065;&#26834;&#24615;&#38408;&#20540;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#23616;&#37096;&#31561;&#20215;&#26597;&#35810;oracle&#65292;&#29992;&#20110;&#22312;&#35757;&#32451;&#26679;&#26412;&#20013;&#21028;&#26029;&#20551;&#35774;&#21644;&#30446;&#26631;&#27010;&#24565;&#22312;&#25200;&#21160;&#21306;&#22495;&#20869;&#26159;&#21542;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;Gourdeau&#31561;&#20154;&#30340;&#30740;&#31350;&#20013;&#24050;&#32463;&#34920;&#26126;&#65292;&#23545;&#20110;&#32771;&#34385;&#31934;&#30830;-&#29699;&#20869;&#40065;&#26834;&#39118;&#38505;&#21644;&#38543;&#26426;&#31034;&#20363;&#35775;&#38382;&#30340;&#27010;&#24565;&#31867;&#30340;&#40065;&#26834;&#21487;&#23398;&#20064;&#24615;&#65292;&#20998;&#24067;&#20551;&#35774;&#26159;&#24517;&#35201;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#36890;&#36807;&#20351;&#29992;&#23616;&#37096;&#26597;&#35810;&#22686;&#24378;&#23398;&#20064;&#32773;&#30340;&#33021;&#21147;&#65292;&#24182;&#32473;&#20986;&#20102;&#31532;&#19968;&#20010;&#38024;&#23545;&#36825;&#31181;&#40065;&#26834;&#24615;&#27010;&#24565;&#36827;&#34892;&#40065;&#26834;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;ERM&#65289;&#30340;&#26080;&#20998;&#24067;&#31639;&#27861;&#12290;&#25105;&#20204;&#32771;&#34385;&#30340;&#31532;&#19968;&#20010;&#23398;&#20064;&#27169;&#22411;&#20351;&#29992;&#23616;&#37096;&#25104;&#21592;&#26597;&#35810;&#65288;LMQ&#65289;&#65292;&#22312;&#36825;&#31181;&#26597;&#35810;&#20013;&#65292;&#23398;&#20064;&#32773;&#21487;&#20197;&#26597;&#35810;&#25509;&#36817;&#35757;&#32451;&#26679;&#26412;&#30340;&#28857;&#30340;&#26631;&#31614;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#22343;&#21248;&#20998;&#24067;&#19979;&#65292;LMQ&#19981;&#20250;&#22686;&#21152;&#24182;&#19988;&#19981;&#20250;&#36229;&#36807;&#36830;&#35789;&#21644;&#20219;&#20309;&#36229;&#31867;&#30340;&#40065;&#26834;&#24615;&#38408;&#20540;&#65292;&#20363;&#22914;&#20915;&#31574;&#21015;&#34920;&#21644;&#21322;&#31354;&#38388;&#12290;&#38754;&#23545;&#36825;&#19968;&#36127;&#38754;&#32467;&#26524;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#23616;&#37096;&#31561;&#20215;&#26597;&#35810;&#65288;LEQ&#65289;oracle&#65292;&#23427;&#36820;&#22238;&#20551;&#35774;&#21644;&#30446;&#26631;&#27010;&#24565;&#22312;&#35757;&#32451;&#26679;&#26412;&#20013;&#30340;&#25200;&#21160;&#21306;&#22495;&#20869;&#26159;&#21542;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributional assumptions have been shown to be necessary for the robust learnability of concept classes when considering the exact-in-the-ball robust risk and access to random examples by Gourdeau et al. (2019). In this paper, we study learning models where the learner is given more power through the use of local queries, and give the first distribution-free algorithms that perform robust empirical risk minimization (ERM) for this notion of robustness. The first learning model we consider uses local membership queries (LMQ), where the learner can query the label of points near the training sample. We show that, under the uniform distribution, LMQs do not increase the robustness threshold of conjunctions and any superclass, e.g., decision lists and halfspaces. Faced with this negative result, we introduce the local equivalence query ($\mathsf{LEQ}$) oracle, which returns whether the hypothesis and target concept agree in the perturbation region around a point in the training sample, a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#38024;&#23545;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#32972;&#21518;&#25915;&#20987;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#21464;&#32858;&#21512;&#22120;&#65292;&#38450;&#24481;&#32972;&#21518;&#25915;&#20987;&#24182;&#20445;&#25345;&#27169;&#22411;&#30340;&#25972;&#20307;&#25928;&#29992;&#12290;&#30740;&#31350;&#21457;&#29616;&#22312;&#25153;&#24179;&#25439;&#22833;&#31354;&#38388;&#20013;&#65292;&#24694;&#24847;&#23458;&#25143;&#31471;&#21487;&#20197;&#36890;&#36807;&#25552;&#20379;&#32972;&#21518;&#26679;&#26412;&#26469;&#35823;&#23548;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#65292;&#32780;&#19981;&#38656;&#35201;&#19982;&#33391;&#24615;&#23458;&#25143;&#31471;&#26377;&#26126;&#26174;&#30340;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2210.01834</link><description>&lt;p&gt;
&#38450;&#24481;&#32852;&#37030;&#32972;&#21518;&#25915;&#20987;&#30340;&#19981;&#21464;&#32858;&#21512;&#22120;
&lt;/p&gt;
&lt;p&gt;
Invariant Aggregator for Defending against Federated Backdoor Attacks. (arXiv:2210.01834v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01834
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#38024;&#23545;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#32972;&#21518;&#25915;&#20987;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#21464;&#32858;&#21512;&#22120;&#65292;&#38450;&#24481;&#32972;&#21518;&#25915;&#20987;&#24182;&#20445;&#25345;&#27169;&#22411;&#30340;&#25972;&#20307;&#25928;&#29992;&#12290;&#30740;&#31350;&#21457;&#29616;&#22312;&#25153;&#24179;&#25439;&#22833;&#31354;&#38388;&#20013;&#65292;&#24694;&#24847;&#23458;&#25143;&#31471;&#21487;&#20197;&#36890;&#36807;&#25552;&#20379;&#32972;&#21518;&#26679;&#26412;&#26469;&#35823;&#23548;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#65292;&#32780;&#19981;&#38656;&#35201;&#19982;&#33391;&#24615;&#23458;&#25143;&#31471;&#26377;&#26126;&#26174;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#22240;&#20854;&#33021;&#22815;&#22312;&#19981;&#30452;&#25509;&#20849;&#20139;&#31169;&#23494;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#39640;&#25928;&#27169;&#22411;&#32780;&#26085;&#30410;&#21463;&#21040;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#32852;&#37030;&#35774;&#32622;&#20351;&#24471;&#27169;&#22411;&#22312;&#23384;&#22312;&#24694;&#24847;&#23458;&#25143;&#31471;&#30340;&#24773;&#20917;&#19979;&#23481;&#26131;&#21463;&#21040;&#21508;&#31181;&#25932;&#23545;&#25915;&#20987;&#12290;&#23613;&#31649;&#23545;&#20110;&#26088;&#22312;&#38477;&#20302;&#27169;&#22411;&#25928;&#29992;&#30340;&#25915;&#20987;&#30340;&#38450;&#24481;&#24050;&#32463;&#21462;&#24471;&#20102;&#29702;&#35770;&#21644;&#23454;&#35777;&#19978;&#30340;&#25104;&#21151;&#65292;&#20294;&#38450;&#24481;&#20165;&#25552;&#39640;&#32972;&#21518;&#26679;&#26412;&#19978;&#27169;&#22411;&#20934;&#30830;&#24615;&#32780;&#19981;&#25439;&#23475;&#20854;&#20182;&#26679;&#26412;&#25928;&#29992;&#30340;&#32972;&#21518;&#25915;&#20987;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#20998;&#26512;&#20102;&#32852;&#37030;&#23398;&#20064;&#22312;&#25153;&#24179;&#25439;&#22833;&#31354;&#38388;&#19978;&#23545;&#32972;&#21518;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#65292;&#36825;&#31181;&#25153;&#24179;&#25439;&#22833;&#31354;&#38388;&#24120;&#35265;&#20110;&#35774;&#35745;&#33391;&#22909;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#22914;Resnet [He et al., 2015]&#65292;&#20294;&#24448;&#24448;&#34987;&#20808;&#21069;&#30340;&#24037;&#20316;&#25152;&#24573;&#35270;&#12290;&#22312;&#25153;&#24179;&#25439;&#22833;&#31354;&#38388;&#19978;&#65292;&#35823;&#23548;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#20197;&#20165;&#23545;&#24694;&#24847;&#23458;&#25143;&#31471;&#30340;&#32972;&#21518;&#26679;&#26412;&#26377;&#21033;&#65292;&#24182;&#19981;&#38656;&#35201;&#24694;&#24847;&#21644;&#33391;&#24615;&#23458;&#25143;&#31471;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning is gaining popularity as it enables training high-utility models across several clients without directly sharing their private data. As a downside, the federated setting makes the model vulnerable to various adversarial attacks in the presence of malicious clients. Despite the theoretical and empirical success in defending against attacks that aim to degrade models' utility, defense against backdoor attacks that increase model accuracy on backdoor samples exclusively without hurting the utility on other samples remains challenging. To this end, we first analyze the vulnerability of federated learning to backdoor attacks over a flat loss landscape which is common for well-designed neural networks such as Resnet [He et al., 2015] but is often overlooked by previous works. Over a flat loss landscape, misleading federated learning models to exclusively benefit malicious clients with backdoor samples do not require a significant difference between malicious and benign cli
&lt;/p&gt;</description></item><item><title>MetaMask&#26159;&#19968;&#31181;&#36890;&#36807;&#20803;&#23398;&#20064;&#23398;&#20064;&#30340;&#32500;&#24230;&#36974;&#32617;&#65292;&#29992;&#20110;&#23545;&#25239;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#32500;&#24230;&#20887;&#20313;&#21644;&#24178;&#25200;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2209.07902</link><description>&lt;p&gt;
MetaMask&#65306;&#37325;&#26032;&#24605;&#32771;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#32500;&#24230;&#24178;&#25200;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
MetaMask: Revisiting Dimensional Confounder for Self-Supervised Learning. (arXiv:2209.07902v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.07902
&lt;/p&gt;
&lt;p&gt;
MetaMask&#26159;&#19968;&#31181;&#36890;&#36807;&#20803;&#23398;&#20064;&#23398;&#20064;&#30340;&#32500;&#24230;&#36974;&#32617;&#65292;&#29992;&#20110;&#23545;&#25239;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#32500;&#24230;&#20887;&#20313;&#21644;&#24178;&#25200;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#25104;&#21151;&#26041;&#27861;&#65292;&#23545;&#27604;&#23398;&#20064;&#26088;&#22312;&#23398;&#20064;&#22312;&#36755;&#20837;&#26679;&#26412;&#30340;&#25197;&#26354;&#20043;&#38388;&#20849;&#20139;&#30340;&#19981;&#21464;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#23545;&#27604;&#23398;&#20064;&#20173;&#28982;&#23384;&#22312;&#20004;&#20010;&#25345;&#20037;&#30340;&#32570;&#38519;&#65306;&#20219;&#21153;&#26080;&#20851;&#20449;&#24687;&#30340;&#24178;&#25200;&#21644;&#26679;&#26412;&#25928;&#29575;&#20302;&#19979;&#65292;&#36825;&#19982;&#24179;&#20961;&#24120;&#25968;&#35299;&#30340;&#21453;&#22797;&#23384;&#22312;&#30456;&#20851;&#12290;&#20174;&#32500;&#24230;&#20998;&#26512;&#30340;&#35282;&#24230;&#65292;&#25105;&#20204;&#21457;&#29616;&#32500;&#24230;&#20887;&#20313;&#21644;&#32500;&#24230;&#24178;&#25200;&#26159;&#36825;&#20123;&#29616;&#35937;&#32972;&#21518;&#30340;&#22266;&#26377;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#23454;&#39564;&#35777;&#25454;&#25903;&#25345;&#20102;&#25105;&#20204;&#30340;&#35266;&#28857;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;MetaMask&#65292;&#21363;&#36890;&#36807;&#20803;&#23398;&#20064;&#23398;&#20064;&#30340;&#32500;&#24230;&#36974;&#32617;&#65292;&#20197;&#23545;&#25239;&#32500;&#24230;&#20887;&#20313;&#21644;&#24178;&#25200;&#12290;MetaMask&#37319;&#29992;&#20887;&#20313;&#20943;&#23569;&#25216;&#26415;&#26469;&#35299;&#20915;&#32500;&#24230;&#20887;&#20313;&#38382;&#39064;&#65292;&#24182;&#21019;&#26032;&#22320;&#24341;&#20837;&#20102;&#19968;&#31181;&#32500;&#24230;&#24178;&#25200;&#35299;&#20915;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
As a successful approach to self-supervised learning, contrastive learning aims to learn invariant information shared among distortions of the input sample. While contrastive learning has yielded continuous advancements in sampling strategy and architecture design, it still remains two persistent defects: the interference of task-irrelevant information and sample inefficiency, which are related to the recurring existence of trivial constant solutions. From the perspective of dimensional analysis, we find out that the dimensional redundancy and dimensional confounder are the intrinsic issues behind the phenomena, and provide experimental evidence to support our viewpoint. We further propose a simple yet effective approach MetaMask, short for the dimensional Mask learned by Meta-learning, to learn representations against dimensional redundancy and confounder. MetaMask adopts the redundancy-reduction technique to tackle the dimensional redundancy issue and innovatively introduces a dimens
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#38750;&#24120;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#19981;&#24433;&#21709;&#27867;&#21270;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#38450;&#24481;&#21508;&#31181;&#31867;&#22411;&#30340;&#19981;&#21487;&#35265;&#27745;&#26579;&#25915;&#20987;&#12290;&#25105;&#20204;&#36890;&#36807;&#20943;&#36731;&#27745;&#26579;&#24341;&#20837;&#30340;&#23574;&#38160;&#25439;&#22833;&#21306;&#22495;&#65292;&#29983;&#25104;&#20248;&#21270;&#30340;&#21451;&#22909;&#22122;&#22768;&#65292;&#23454;&#29616;&#23545;&#25239;&#24615;&#22122;&#22768;&#30340;&#38450;&#24481;&#12290;</title><link>http://arxiv.org/abs/2208.10224</link><description>&lt;p&gt;
&#23545;&#25239;&#24615;&#22122;&#22768;&#30340;&#21451;&#22909;&#22122;&#22768;&#65306;&#38024;&#23545;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#30340;&#24378;&#22823;&#38450;&#24481;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Friendly Noise against Adversarial Noise: A Powerful Defense against Data Poisoning Attacks. (arXiv:2208.10224v4 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.10224
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#38750;&#24120;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#19981;&#24433;&#21709;&#27867;&#21270;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#38450;&#24481;&#21508;&#31181;&#31867;&#22411;&#30340;&#19981;&#21487;&#35265;&#27745;&#26579;&#25915;&#20987;&#12290;&#25105;&#20204;&#36890;&#36807;&#20943;&#36731;&#27745;&#26579;&#24341;&#20837;&#30340;&#23574;&#38160;&#25439;&#22833;&#21306;&#22495;&#65292;&#29983;&#25104;&#20248;&#21270;&#30340;&#21451;&#22909;&#22122;&#22768;&#65292;&#23454;&#29616;&#23545;&#25239;&#24615;&#22122;&#22768;&#30340;&#38450;&#24481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#31867;&#24378;&#22823;&#30340;&#65288;&#19981;&#21487;&#35265;&#30340;&#65289;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#36890;&#36807;&#24494;&#23567;&#30340;&#23545;&#25239;&#25200;&#21160;&#20462;&#25913;&#35757;&#32451;&#26679;&#26412;&#30340;&#23376;&#38598;&#65292;&#20197;&#25913;&#21464;&#29305;&#23450;&#27979;&#35797;&#25968;&#25454;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#29616;&#26377;&#30340;&#38450;&#24481;&#26426;&#21046;&#19981;&#36866;&#21512;&#23454;&#38469;&#37096;&#32626;&#65292;&#22240;&#20026;&#23427;&#20204;&#24448;&#24448;&#35201;&#20040;&#20005;&#37325;&#24433;&#21709;&#27867;&#21270;&#24615;&#33021;&#65292;&#35201;&#20040;&#26159;&#29305;&#23450;&#25915;&#20987;&#30340;&#65292;&#19988;&#24212;&#29992;&#36215;&#26469;&#36895;&#24230;&#36807;&#24930;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#38750;&#24120;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#23427;&#33021;&#22312;&#27867;&#21270;&#24615;&#33021;&#31245;&#24494;&#19979;&#38477;&#30340;&#24773;&#20917;&#19979;&#38450;&#24481;&#21508;&#31181;&#31867;&#22411;&#30340;&#19981;&#21487;&#35265;&#27745;&#26579;&#25915;&#20987;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#25915;&#20987;&#20250;&#24341;&#20837;&#23616;&#37096;&#23574;&#38160;&#30340;&#39640;&#35757;&#32451;&#25439;&#22833;&#21306;&#22495;&#65292;&#24403;&#36825;&#20123;&#21306;&#22495;&#34987;&#26368;&#23567;&#21270;&#26102;&#65292;&#25915;&#20987;&#23601;&#20250;&#25104;&#21151;&#12290;&#20026;&#20102;&#38450;&#27490;&#27745;&#26579;&#25915;&#20987;&#65292;&#25105;&#20204;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#20943;&#36731;&#27602;&#32032;&#24341;&#20837;&#30340;&#23574;&#38160;&#25439;&#22833;&#21306;&#22495;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#65306;&#20248;&#21270;&#30340;&#21451;&#22909;&#22122;&#22768;&#65292;&#29992;&#20110;&#26368;&#22823;&#31243;&#24230;&#22320;&#25200;&#21160;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
A powerful category of (invisible) data poisoning attacks modify a subset of training examples by small adversarial perturbations to change the prediction of certain test-time data. Existing defense mechanisms are not desirable to deploy in practice, as they often either drastically harm the generalization performance, or are attack-specific, and prohibitively slow to apply. Here, we propose a simple but highly effective approach that unlike existing methods breaks various types of invisible poisoning attacks with the slightest drop in the generalization performance. We make the key observation that attacks introduce local sharp regions of high training loss, which when minimized, results in learning the adversarial perturbations and makes the attack successful. To break poisoning attacks, our key idea is to alleviate the sharp loss regions introduced by poisons. To do so, our approach comprises two components: an optimized friendly noise that is generated to maximally perturb examples
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SAFARI&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#30340;&#35299;&#37322;&#21487;&#38752;&#24615;&#12290;&#35813;&#26041;&#27861;&#38024;&#23545;&#29616;&#26377;&#25216;&#26415;&#26080;&#27861;&#35299;&#20915;&#30340;&#20960;&#20010;&#25361;&#25112;&#65292;&#36890;&#36807;&#24341;&#20837;&#20004;&#31181;&#40657;&#30418;&#35780;&#20272;&#26041;&#27861;&#65292;&#21363;&#26368;&#22351;&#24773;&#20917;&#35299;&#37322;&#24046;&#24322;&#21644;&#19968;&#33324;&#24773;&#20917;&#19979;&#30340;&#40065;&#26834;&#24615;&#30340;&#27010;&#29575;&#27010;&#24565;&#65292;&#26469;&#35299;&#20915;&#29616;&#26377;&#24230;&#37327;&#19981;&#20840;&#38754;&#12289;XAI&#25216;&#26415;&#24322;&#36136;&#24615;&#21644;&#35823;&#35299;&#32597;&#35265;&#24615;&#31561;&#38382;&#39064;&#12290;&#20351;&#29992;&#36951;&#20256;&#31639;&#27861;&#21644;&#23376;&#38598;&#27169;&#25311;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2208.09418</link><description>&lt;p&gt;
SAFARI&#65306;&#40065;&#26834;&#24615;&#21487;&#35299;&#37322;&#24615;&#35780;&#20272;&#30340;&#22810;&#21151;&#33021;&#39640;&#25928;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SAFARI: Versatile and Efficient Evaluations for Robustness of Interpretability. (arXiv:2208.09418v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.09418
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SAFARI&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#30340;&#35299;&#37322;&#21487;&#38752;&#24615;&#12290;&#35813;&#26041;&#27861;&#38024;&#23545;&#29616;&#26377;&#25216;&#26415;&#26080;&#27861;&#35299;&#20915;&#30340;&#20960;&#20010;&#25361;&#25112;&#65292;&#36890;&#36807;&#24341;&#20837;&#20004;&#31181;&#40657;&#30418;&#35780;&#20272;&#26041;&#27861;&#65292;&#21363;&#26368;&#22351;&#24773;&#20917;&#35299;&#37322;&#24046;&#24322;&#21644;&#19968;&#33324;&#24773;&#20917;&#19979;&#30340;&#40065;&#26834;&#24615;&#30340;&#27010;&#29575;&#27010;&#24565;&#65292;&#26469;&#35299;&#20915;&#29616;&#26377;&#24230;&#37327;&#19981;&#20840;&#38754;&#12289;XAI&#25216;&#26415;&#24322;&#36136;&#24615;&#21644;&#35823;&#35299;&#32597;&#35265;&#24615;&#31561;&#38382;&#39064;&#12290;&#20351;&#29992;&#36951;&#20256;&#31639;&#27861;&#21644;&#23376;&#38598;&#27169;&#25311;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#21487;&#35299;&#37322;&#24615;&#26159;&#24314;&#31435;&#21487;&#20449;&#36182;&#30340;&#20154;&#24037;&#26234;&#33021;&#30340;&#19968;&#36947;&#38556;&#30861;&#12290;&#23613;&#31649;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#31038;&#21306;&#20570;&#20986;&#20102;&#24040;&#22823;&#30340;&#21162;&#21147;&#65292;&#20294;&#35299;&#37322;&#32570;&#20047;&#40065;&#26834;&#24615;&#8212;&#8212;&#26080;&#27861;&#21306;&#20998;&#30340;&#36755;&#20837;&#25200;&#21160;&#21487;&#33021;&#20250;&#23548;&#33268;&#19981;&#21516;&#30340;&#35299;&#37322;&#32467;&#26524;&#12290;&#22240;&#27492;&#65292;&#38024;&#23545;&#32473;&#23450;&#30340;XAI&#26041;&#27861;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#21487;&#35299;&#37322;&#24615;&#30340;&#40065;&#26834;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#35782;&#21035;&#20102;&#29616;&#26377;&#25216;&#26415;&#26080;&#27861;&#20849;&#21516;&#24212;&#23545;&#30340;&#20960;&#20010;&#25361;&#25112;&#65306;i)&#29616;&#26377;&#25351;&#26631;&#19981;&#20840;&#38754;&#65307;ii)XAI&#25216;&#26415;&#39640;&#24230;&#24322;&#36136;&#65307;iii)&#35823;&#35299;&#36890;&#24120;&#26159;&#32597;&#35265;&#20107;&#20214;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#40657;&#30418;&#35780;&#20272;&#26041;&#27861;&#65292;&#20998;&#21035;&#28041;&#21450;&#26368;&#22351;&#24773;&#20917;&#35299;&#37322;&#24046;&#24322;&#21644;&#19968;&#33324;&#24773;&#20917;&#19979;&#30340;&#40065;&#26834;&#24615;&#30340;&#27010;&#29575;&#27010;&#24565;&#12290;&#20351;&#29992;&#20855;&#26377;&#23450;&#21046;&#36866;&#24212;&#24230;&#20989;&#25968;&#30340;&#36951;&#20256;&#31639;&#27861;&#65288;GA&#65289;&#26469;&#35299;&#20915;&#32422;&#26463;&#20248;&#21270;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#26368;&#22351;&#24773;&#20917;&#35780;&#20272;&#12290;&#20351;&#29992;&#19987;&#38376;&#29992;&#20110;&#20272;&#35745;&#32597;&#35265;&#20107;&#20214;&#27010;&#29575;&#30340;&#23376;&#38598;&#27169;&#25311;&#65288;SS&#65289;&#26469;&#36827;&#34892;&#25972;&#20307;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpretability of Deep Learning (DL) is a barrier to trustworthy AI. Despite great efforts made by the Explainable AI (XAI) community, explanations lack robustness -- indistinguishable input perturbations may lead to different XAI results. Thus, it is vital to assess how robust DL interpretability is, given an XAI method. In this paper, we identify several challenges that the state-of-the-art is unable to cope with collectively: i) existing metrics are not comprehensive; ii) XAI techniques are highly heterogeneous; iii) misinterpretations are normally rare events. To tackle these challenges, we introduce two black-box evaluation methods, concerning the worst-case interpretation discrepancy and a probabilistic notion of how robust in general, respectively. Genetic Algorithm (GA) with bespoke fitness function is used to solve constrained optimisation for efficient worst-case evaluation. Subset Simulation (SS), dedicated to estimate rare event probabilities, is used for evaluating overa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#35266;&#28857;&#24066;&#22330;&#27169;&#22411;&#65288;OMM&#65289;&#65292;&#36890;&#36807;&#24341;&#20837;&#31215;&#26497;&#24178;&#39044;&#26469;&#36943;&#21046;&#26497;&#21491;&#27966;&#35266;&#28857;&#30340;&#20256;&#25773;&#12290;&#36825;&#20010;&#27169;&#22411;&#23558;&#35266;&#28857;&#30340;&#20851;&#27880;&#24066;&#22330;&#35268;&#27169;&#24314;&#27169;&#65292;&#24182;&#32771;&#34385;&#20102;&#35266;&#28857;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#21644;&#31454;&#20105;&#65292;&#26088;&#22312;&#35780;&#20272;&#31215;&#26497;&#24178;&#39044;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2208.06620</link><description>&lt;p&gt;
&#35266;&#28857;&#24066;&#22330;&#27169;&#22411;&#65306;&#21033;&#29992;&#31215;&#26497;&#24178;&#39044;&#26469;&#36943;&#21046;&#26497;&#21491;&#27966;&#35266;&#28857;&#30340;&#20256;&#25773;
&lt;/p&gt;
&lt;p&gt;
Opinion Market Model: Stemming Far-Right Opinion Spread using Positive Interventions. (arXiv:2208.06620v2 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.06620
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#35266;&#28857;&#24066;&#22330;&#27169;&#22411;&#65288;OMM&#65289;&#65292;&#36890;&#36807;&#24341;&#20837;&#31215;&#26497;&#24178;&#39044;&#26469;&#36943;&#21046;&#26497;&#21491;&#27966;&#35266;&#28857;&#30340;&#20256;&#25773;&#12290;&#36825;&#20010;&#27169;&#22411;&#23558;&#35266;&#28857;&#30340;&#20851;&#27880;&#24066;&#22330;&#35268;&#27169;&#24314;&#27169;&#65292;&#24182;&#32771;&#34385;&#20102;&#35266;&#28857;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#21644;&#31454;&#20105;&#65292;&#26088;&#22312;&#35780;&#20272;&#31215;&#26497;&#24178;&#39044;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#26497;&#31471;&#20027;&#20041;&#20855;&#26377;&#20005;&#37325;&#30340;&#31038;&#20250;&#21518;&#26524;&#65292;&#21253;&#25324;&#23558;&#20167;&#24680;&#35328;&#35770;&#21512;&#29702;&#21270;&#12289;&#29992;&#25143;&#30340;&#28608;&#36827;&#21270;&#20197;&#21450;&#31038;&#20250;&#20998;&#35010;&#30340;&#21152;&#21095;&#12290;&#24050;&#32463;&#25506;&#32034;&#20102;&#21508;&#31181;&#32531;&#35299;&#31574;&#30053;&#26469;&#24212;&#23545;&#36825;&#20123;&#21518;&#26524;&#12290;&#20854;&#20013;&#19968;&#31181;&#31574;&#30053;&#20351;&#29992;&#31215;&#26497;&#24178;&#39044;&#65306;&#25511;&#21046;&#20449;&#21495;&#26469;&#22686;&#21152;&#23545;&#35266;&#28857;&#29983;&#24577;&#31995;&#32479;&#30340;&#20851;&#27880;&#65292;&#20197;&#25552;&#21319;&#26576;&#20123;&#35266;&#28857;&#12290;&#20026;&#20102;&#35780;&#20272;&#31215;&#26497;&#24178;&#39044;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#35266;&#28857;&#24066;&#22330;&#27169;&#22411;&#65288;OMM&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#32771;&#34385;&#21040;&#35266;&#28857;&#38388;&#30456;&#20114;&#20316;&#29992;&#21644;&#31215;&#26497;&#24178;&#39044;&#20316;&#29992;&#30340;&#20004;&#23618;&#22312;&#32447;&#35266;&#28857;&#29983;&#24577;&#31995;&#32479;&#27169;&#22411;&#12290;&#35266;&#28857;&#30340;&#20851;&#27880;&#24066;&#22330;&#35268;&#27169;&#20351;&#29992;&#22810;&#20803;&#31163;&#25955;&#26102;&#38388;Hawkes&#36807;&#31243;&#22312;&#31532;&#19968;&#23618;&#36827;&#34892;&#24314;&#27169;&#65307;&#22312;&#31532;&#20108;&#23618;&#20013;&#65292;&#35266;&#28857;&#22312;&#26377;&#38480;&#30340;&#20851;&#27880;&#24230;&#19979;&#21512;&#20316;&#21644;&#31454;&#20105;&#20197;&#33719;&#24471;&#24066;&#22330;&#20221;&#39069;&#65292;&#20351;&#29992;&#24066;&#22330;&#20221;&#39069;&#21560;&#24341;&#27169;&#22411;&#12290;&#36890;&#36807;&#21512;&#25104;&#25968;&#25454;&#38598;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#20272;&#35745;&#26041;&#26696;&#30340;&#25910;&#25947;&#24615;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#22312;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#23398;&#20064;&#20219;&#21153;&#19978;&#27979;&#35797;&#20102;OMM
&lt;/p&gt;
&lt;p&gt;
Online extremism has severe societal consequences, including normalizing hate speech, user radicalization, and increased social divisions. Various mitigation strategies have been explored to address these consequences. One such strategy uses positive interventions: controlled signals that add attention to the opinion ecosystem to boost certain opinions. To evaluate the effectiveness of positive interventions, we introduce the Opinion Market Model (OMM), a two-tier online opinion ecosystem model that considers both inter-opinion interactions and the role of positive interventions. The size of the opinion attention market is modeled in the first tier using the multivariate discrete-time Hawkes process; in the second tier, opinions cooperate and compete for market share, given limited attention using the market share attraction model. We demonstrate the convergence of our proposed estimation scheme on a synthetic dataset. Next, we test OMM on two learning tasks, applying to two real-world
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#22312;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#19978;&#36827;&#34892;&#39044;&#27979;&#24615;&#38382;&#39064;&#22238;&#31572;&#12290;&#35813;&#20219;&#21153;&#23545;&#20110;&#20154;&#20204;&#23547;&#27714;&#26410;&#26469;&#35745;&#21010;&#38750;&#24120;&#37325;&#35201;&#65292;&#24182;&#19988;&#26159;&#20808;&#21069;&#30740;&#31350;&#20013;&#26410;&#34987;&#25506;&#32034;&#30340;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2208.06501</link><description>&lt;p&gt;
ForecastTKGQuestions: &#19968;&#20010;&#29992;&#20110;&#26102;&#38388;&#24615;&#38382;&#39064;&#22238;&#31572;&#21644;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#39044;&#27979;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
ForecastTKGQuestions: A Benchmark for Temporal Question Answering and Forecasting over Temporal Knowledge Graphs. (arXiv:2208.06501v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.06501
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#22312;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#19978;&#36827;&#34892;&#39044;&#27979;&#24615;&#38382;&#39064;&#22238;&#31572;&#12290;&#35813;&#20219;&#21153;&#23545;&#20110;&#20154;&#20204;&#23547;&#27714;&#26410;&#26469;&#35745;&#21010;&#38750;&#24120;&#37325;&#35201;&#65292;&#24182;&#19988;&#26159;&#20808;&#21069;&#30740;&#31350;&#20013;&#26410;&#34987;&#25506;&#32034;&#30340;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#20110;&#26102;&#38388;&#24615;&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#65288;TKGQA&#65289;&#30340;&#20852;&#36259;&#36880;&#28176;&#22686;&#21152;&#12290;TKGQA&#38656;&#35201;&#26102;&#38388;&#25512;&#29702;&#25216;&#26415;&#26469;&#20174;&#26102;&#38388;&#30693;&#35782;&#24211;&#20013;&#25552;&#21462;&#30456;&#20851;&#20449;&#24687;&#12290;&#29616;&#26377;&#30340;TKGQA&#25968;&#25454;&#38598;&#21482;&#21253;&#21547;&#22522;&#20110;&#22266;&#23450;&#26102;&#38388;&#27573;&#30340;&#26102;&#38388;&#24615;&#38382;&#39064;&#65292;&#35813;&#26102;&#38388;&#27573;&#20869;&#30340;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#65288;TKG&#65289;&#21487;&#20197;&#23436;&#20840;&#29992;&#20110;&#31572;&#26696;&#25512;&#29702;&#65292;&#20801;&#35768;TKGQA&#27169;&#22411;&#21033;&#29992;&#26410;&#26469;&#30693;&#35782;&#26469;&#22238;&#31572;&#22522;&#20110;&#36807;&#21435;&#20107;&#23454;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#37492;&#20110;&#21040;&#30446;&#21069;&#20026;&#27490;&#30340;&#30693;&#35782;&#65292;&#25105;&#20204;&#20063;&#24076;&#26395;TKGQA&#31995;&#32479;&#33021;&#22815;&#22238;&#31572;&#20851;&#20110;&#26410;&#26469;&#30340;&#38382;&#39064;&#12290;&#30001;&#20110;&#20154;&#20204;&#19981;&#26029;&#23547;&#27714;&#26410;&#26469;&#30340;&#35745;&#21010;&#65292;&#26500;&#24314;&#33021;&#22815;&#22238;&#31572;&#36825;&#31181;&#39044;&#27979;&#24615;&#38382;&#39064;&#30340;TKGQA&#31995;&#32479;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22312;&#20808;&#21069;&#30340;&#30740;&#31350;&#20013;&#65292;&#36825;&#19968;&#39046;&#22495;&#20173;&#28982;&#26410;&#34987;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#65306;&#39044;&#27979;&#24615;&#38382;&#39064;&#22238;&#31572;&#30340;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Question answering over temporal knowledge graphs (TKGQA) has recently found increasing interest. TKGQA requires temporal reasoning techniques to extract the relevant information from temporal knowledge bases. The only existing TKGQA dataset, i.e., CronQuestions, consists of temporal questions based on the facts from a fixed time period, where a temporal knowledge graph (TKG) spanning the same period can be fully used for answer inference, allowing the TKGQA models to use even the future knowledge to answer the questions based on the past facts. In real-world scenarios, however, it is also common that given the knowledge until now, we wish the TKGQA systems to answer the questions asking about the future. As humans constantly seek plans for the future, building TKGQA systems for answering such forecasting questions is important. Nevertheless, this has still been unexplored in previous research. In this paper, we propose a novel task: forecasting question answering over temporal knowled
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#31163;&#25955;&#36873;&#25321;&#27169;&#22411;&#31867;&#65292;RUMnets&#65292;&#21487;&#20197;&#36817;&#20284;&#34920;&#31034;&#20219;&#20309;&#38543;&#26426;&#25928;&#29992;&#26368;&#22823;&#21270;&#25512;&#23548;&#20986;&#30340;&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;&#36873;&#25321;&#25968;&#25454;&#19978;&#26377;&#33391;&#22909;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2207.12877</link><description>&lt;p&gt;
&#29992;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#38543;&#26426;&#25928;&#29992;&#36873;&#25321;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Representing Random Utility Choice Models with Neural Networks. (arXiv:2207.12877v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.12877
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#31163;&#25955;&#36873;&#25321;&#27169;&#22411;&#31867;&#65292;RUMnets&#65292;&#21487;&#20197;&#36817;&#20284;&#34920;&#31034;&#20219;&#20309;&#38543;&#26426;&#25928;&#29992;&#26368;&#22823;&#21270;&#25512;&#23548;&#20986;&#30340;&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;&#36873;&#25321;&#25968;&#25454;&#19978;&#26377;&#33391;&#22909;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#30340;&#25104;&#21151;&#20043;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#31163;&#25955;&#36873;&#25321;&#27169;&#22411;&#31867;&#65292;&#31216;&#20026;RUMnets&#65292;&#21463;&#38543;&#26426;&#25928;&#29992;&#26368;&#22823;&#21270;&#65288;RUM&#65289;&#26694;&#26550;&#30340;&#21551;&#21457;&#12290;&#35813;&#27169;&#22411;&#20351;&#29992;&#26679;&#26412;&#24179;&#22343;&#36924;&#36817;&#26469;&#26500;&#24314;&#20195;&#29702;&#20154;&#30340;&#38543;&#26426;&#25928;&#29992;&#20989;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;RUMnets&#21487;&#20197;&#23545;RUM&#31163;&#25955;&#36873;&#25321;&#27169;&#22411;&#31867;&#36827;&#34892;&#23574;&#38160;&#36924;&#36817;&#65306;&#20219;&#20309;&#20174;&#38543;&#26426;&#25928;&#29992;&#26368;&#22823;&#21270;&#25512;&#23548;&#20986;&#30340;&#27169;&#22411;&#37117;&#21487;&#20197;&#34987;RUMnet&#26080;&#38480;&#25509;&#36817;&#22320;&#36924;&#36817;&#12290;&#30456;&#21453;&#22320;&#65292;&#20219;&#20309;RUMnet&#37117;&#31526;&#21512;RUM&#21407;&#21017;&#12290;&#25105;&#20204;&#24471;&#21040;&#20102;&#22312;&#36873;&#25321;&#25968;&#25454;&#19978;&#25311;&#21512;&#30340;RUMnet&#30340;&#27867;&#21270;&#35823;&#24046;&#30340;&#19978;&#30028;&#65292;&#24182;&#19988;&#26681;&#25454;&#25968;&#25454;&#38598;&#21644;&#26550;&#26500;&#30340;&#20851;&#38190;&#21442;&#25968;&#65292;&#33719;&#24471;&#20102;&#20851;&#20110;&#20854;&#22312;&#26032;&#30340;&#26410;&#30693;&#25968;&#25454;&#19978;&#39044;&#27979;&#36873;&#25321;&#33021;&#21147;&#30340;&#29702;&#35770;&#27934;&#35265;&#12290;&#36890;&#36807;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#24320;&#28304;&#24211;&#65292;&#25105;&#20204;&#21457;&#29616;RUMnets&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#19982;&#20960;&#31181;&#36873;&#25321;&#24314;&#27169;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by the successes of deep learning, we propose a class of neural network-based discrete choice models, called RUMnets, inspired by the random utility maximization (RUM) framework. This model formulates the agents' random utility function using a sample average approximation. We show that RUMnets sharply approximate the class of RUM discrete choice models: any model derived from random utility maximization has choice probabilities that can be approximated arbitrarily closely by a RUMnet. Reciprocally, any RUMnet is consistent with the RUM principle. We derive an upper bound on the generalization error of RUMnets fitted on choice data, and gain theoretical insights on their ability to predict choices on new, unseen data depending on critical parameters of the dataset and architecture. By leveraging open-source libraries for neural networks, we find that RUMnets are competitive against several choice modeling and machine learning methods in terms of predictive accuracy on two rea
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#21033;&#29992;&#22823;&#26679;&#26412;&#28176;&#36817;&#29702;&#35770;&#23545;&#38543;&#26426;&#26799;&#24230;&#31639;&#27861;&#36827;&#34892;&#35843;&#20248;&#65292;&#21457;&#29616;&#20351;&#29992;&#22266;&#23450;&#30340;&#22823;&#27493;&#38271;&#36827;&#34892;&#36845;&#20195;&#24179;&#22343;&#21487;&#20197;&#40065;&#26834;&#22320;&#20248;&#21270;&#31639;&#27861;&#65292;&#19988;&#20855;&#26377;&#21644;MLE&#25277;&#26679;&#20998;&#24067;&#21327;&#26041;&#24046;&#25104;&#27604;&#20363;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#31867;&#20284;&#20110;Bernstein-von Mises&#30340;&#23450;&#29702;&#29992;&#20110;&#25351;&#23548;&#35843;&#20248;&#65292;&#21253;&#25324;&#38024;&#23545;&#27169;&#22411;&#38169;&#35823;&#35268;&#33539;&#40065;&#26834;&#30340;&#24191;&#20041;&#21518;&#39564;&#12290;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#32467;&#26524;&#21644;&#24314;&#35758;&#22312;&#23454;&#38469;&#26377;&#38480;&#26679;&#26412;&#24773;&#20917;&#19979;&#30340;&#26377;&#25928;&#24615;&#12290;&#36825;&#20123;&#25104;&#26524;&#20026;&#20998;&#26512;&#20854;&#20182;&#38543;&#26426;&#26799;&#24230;Markov Chain Monte Carlo&#31639;&#27861;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2207.12395</link><description>&lt;p&gt;
&#36890;&#36807;&#22823;&#26679;&#26412;&#28176;&#36817;&#29702;&#35770;&#20248;&#21270;&#38543;&#26426;&#26799;&#24230;&#31639;&#27861;&#30340;&#32479;&#35745;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Tuning Stochastic Gradient Algorithms for Statistical Inference via Large-Sample Asymptotics. (arXiv:2207.12395v3 [stat.CO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.12395
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21033;&#29992;&#22823;&#26679;&#26412;&#28176;&#36817;&#29702;&#35770;&#23545;&#38543;&#26426;&#26799;&#24230;&#31639;&#27861;&#36827;&#34892;&#35843;&#20248;&#65292;&#21457;&#29616;&#20351;&#29992;&#22266;&#23450;&#30340;&#22823;&#27493;&#38271;&#36827;&#34892;&#36845;&#20195;&#24179;&#22343;&#21487;&#20197;&#40065;&#26834;&#22320;&#20248;&#21270;&#31639;&#27861;&#65292;&#19988;&#20855;&#26377;&#21644;MLE&#25277;&#26679;&#20998;&#24067;&#21327;&#26041;&#24046;&#25104;&#27604;&#20363;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#31867;&#20284;&#20110;Bernstein-von Mises&#30340;&#23450;&#29702;&#29992;&#20110;&#25351;&#23548;&#35843;&#20248;&#65292;&#21253;&#25324;&#38024;&#23545;&#27169;&#22411;&#38169;&#35823;&#35268;&#33539;&#40065;&#26834;&#30340;&#24191;&#20041;&#21518;&#39564;&#12290;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#32467;&#26524;&#21644;&#24314;&#35758;&#22312;&#23454;&#38469;&#26377;&#38480;&#26679;&#26412;&#24773;&#20917;&#19979;&#30340;&#26377;&#25928;&#24615;&#12290;&#36825;&#20123;&#25104;&#26524;&#20026;&#20998;&#26512;&#20854;&#20182;&#38543;&#26426;&#26799;&#24230;Markov Chain Monte Carlo&#31639;&#27861;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#20248;&#21270;&#21644;&#25277;&#26679;&#30340;&#38543;&#26426;&#26799;&#24230;&#31639;&#27861;&#65288;SGA&#65289;&#30340;&#35843;&#20248;&#36890;&#24120;&#22522;&#20110;&#35797;&#38169;&#21644;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#32780;&#19981;&#26159;&#21487;&#25512;&#24191;&#30340;&#29702;&#35770;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#32852;&#21512;&#27493;&#38271;-&#26679;&#26412;&#22823;&#23567;&#32553;&#25918;&#26497;&#38480;&#26469;&#34920;&#24449;SGA&#30340;&#22823;&#26679;&#26412;&#32479;&#35745;&#28176;&#36817;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20351;&#29992;&#22266;&#23450;&#30340;&#22823;&#27493;&#38271;&#36827;&#34892;&#36845;&#20195;&#24179;&#22343;&#26159;&#23545;&#35843;&#20248;&#21442;&#25968;&#36873;&#25321;&#40065;&#26834;&#30340;&#65292;&#24182;&#19988;&#22312;&#28176;&#36817;&#24847;&#20041;&#19979;&#65292;&#20855;&#26377;&#21644;MLE&#25277;&#26679;&#20998;&#24067;&#21327;&#26041;&#24046;&#25104;&#27604;&#20363;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#19968;&#31181;&#31867;&#20284;&#20110;Bernstein-von Mises&#30340;&#23450;&#29702;&#20197;&#25351;&#23548;&#35843;&#20248;&#65292;&#21253;&#25324;&#38024;&#23545;&#27169;&#22411;&#38169;&#35823;&#35268;&#33539;&#40065;&#26834;&#30340;&#24191;&#20041;&#21518;&#39564;&#12290;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#22312;&#23454;&#38469;&#26377;&#38480;&#26679;&#26412;&#33539;&#22260;&#20869;&#30340;&#32467;&#26524;&#21644;&#24314;&#35758;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;&#22823;&#33539;&#22260;&#27169;&#22411;&#30340;&#20854;&#20182;&#38543;&#26426;&#26799;&#24230;Markov Chain Monte Carlo&#31639;&#27861;&#30340;&#31995;&#32479;&#20998;&#26512;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
The tuning of stochastic gradient algorithms (SGAs) for optimization and sampling is often based on heuristics and trial-and-error rather than generalizable theory. We address this theory--practice gap by characterizing the large-sample statistical asymptotics of SGAs via a joint step-size--sample-size scaling limit. We show that iterate averaging with a large fixed step size is robust to the choice of tuning parameters and asymptotically has covariance proportional to that of the MLE sampling distribution. We also prove a Bernstein--von Mises-like theorem to guide tuning, including for generalized posteriors that are robust to model misspecification. Numerical experiments validate our results and recommendations in realistic finite-sample regimes. Our work lays the foundation for a systematic analysis of other stochastic gradient Markov chain Monte Carlo algorithms for a wide range of models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#23454;&#39564;&#35774;&#35745;&#26041;&#27861;&#22312;&#32447;&#24615;MDPs&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;\textsc{Pedel}&#65292;&#35813;&#31639;&#27861;&#22312;RL&#20013;&#30340;&#20989;&#25968;&#36924;&#36817;&#35774;&#32622;&#19979;&#23454;&#29616;&#20102;&#32454;&#31890;&#24230;&#30340;&#20381;&#36182;&#20110;&#23454;&#20363;&#30340;&#22797;&#26434;&#24230;&#24230;&#37327;&#65292;&#30456;&#23545;&#20110;&#26368;&#20302;&#36951;&#25022;&#12289;&#26368;&#23567;&#26368;&#22823;&#26368;&#20248;&#31639;&#27861;&#20855;&#26377;&#26126;&#26174;&#25910;&#30410;&#12290;</title><link>http://arxiv.org/abs/2207.02575</link><description>&lt;p&gt;
&#22522;&#20110;&#22312;&#32447;&#23454;&#39564;&#35774;&#35745;&#30340;&#32447;&#24615;MDPs&#20013;&#30340;&#20381;&#36182;&#20110;&#23454;&#20363;&#30340;&#36817;&#26368;&#20248;&#31574;&#30053;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Instance-Dependent Near-Optimal Policy Identification in Linear MDPs via Online Experiment Design. (arXiv:2207.02575v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.02575
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#23454;&#39564;&#35774;&#35745;&#26041;&#27861;&#22312;&#32447;&#24615;MDPs&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;\textsc{Pedel}&#65292;&#35813;&#31639;&#27861;&#22312;RL&#20013;&#30340;&#20989;&#25968;&#36924;&#36817;&#35774;&#32622;&#19979;&#23454;&#29616;&#20102;&#32454;&#31890;&#24230;&#30340;&#20381;&#36182;&#20110;&#23454;&#20363;&#30340;&#22797;&#26434;&#24230;&#24230;&#37327;&#65292;&#30456;&#23545;&#20110;&#26368;&#20302;&#36951;&#25022;&#12289;&#26368;&#23567;&#26368;&#22823;&#26368;&#20248;&#31639;&#27861;&#20855;&#26377;&#26126;&#26174;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#34429;&#28982;&#23545;&#20110;&#26368;&#22351;&#24773;&#20917;&#23454;&#20363;&#19979;&#30340;&#26368;&#23567;&#26368;&#22823;&#26679;&#26412;&#22797;&#26434;&#24230;&#26377;&#20102;&#24456;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#26159;&#36825;&#31181;&#22797;&#26434;&#24230;&#34913;&#37327;&#24448;&#24448;&#19981;&#33021;&#30495;&#27491;&#21453;&#26144;&#20986;&#23398;&#20064;&#30340;&#30495;&#27491;&#22256;&#38590;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#23545;&#20110;&#19968;&#20010;&#8220;&#31616;&#21333;&#8221;&#30340;&#23454;&#20363;&#65292;&#25105;&#20204;&#21487;&#33021;&#24076;&#26395;&#33021;&#22815;&#23454;&#29616;&#27604;&#26368;&#22351;&#23454;&#20363;&#19979;&#26356;&#22909;&#30340;&#22797;&#26434;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24076;&#26395;&#20102;&#35299;&#22312;&#20855;&#26377;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30340;&#24378;&#21270;&#23398;&#20064;&#35774;&#32622;&#20013;&#65292;&#23398;&#20064;&#36817;&#26368;&#20248;&#31574;&#30053;&#65288;PAC RL&#65289;&#30340;&#8220;&#20381;&#36182;&#20110;&#23454;&#20363;&#8221;&#30340;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;\textsc{Pedel}&#65292;&#23427;&#23454;&#29616;&#20102;&#19968;&#31181;&#32454;&#31890;&#24230;&#30340;&#20381;&#36182;&#20110;&#23454;&#20363;&#30340;&#22797;&#26434;&#24230;&#24230;&#37327;&#65292;&#36825;&#26159;&#22312;&#20989;&#25968;&#36924;&#36817;&#35774;&#32622;&#20013;&#39318;&#27425;&#20986;&#29616;&#30340;&#65292;&#20174;&#32780;&#25429;&#25417;&#20102;&#22312;&#27599;&#20010;&#29305;&#23450;&#38382;&#39064;&#23454;&#20363;&#19978;&#23398;&#20064;&#30340;&#22256;&#38590;&#31243;&#24230;&#12290;&#36890;&#36807;&#19968;&#20010;&#20855;&#20307;&#30340;&#20363;&#23376;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;\textsc{Pedel}&#30456;&#23545;&#20110;&#26368;&#20302;&#36951;&#25022;&#12289;&#26368;&#23567;&#26368;&#22823;&#26368;&#20248;&#31639;&#27861;&#30340;&#21487;&#35777;&#26126;&#25910;&#30410;&#65292;&#24182;&#19988;&#36825;&#20123;&#31639;&#27861;&#26080;&#27861;&#36798;&#21040;&#36825;&#31181;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
While much progress has been made in understanding the minimax sample complexity of reinforcement learning (RL) -- the complexity of learning on the "worst-case" instance -- such measures of complexity often do not capture the true difficulty of learning. In practice, on an "easy" instance, we might hope to achieve a complexity far better than that achievable on the worst-case instance. In this work we seek to understand the "instance-dependent" complexity of learning near-optimal policies (PAC RL) in the setting of RL with linear function approximation. We propose an algorithm, \textsc{Pedel}, which achieves a fine-grained instance-dependent measure of complexity, the first of its kind in the RL with function approximation setting, thereby capturing the difficulty of learning on each particular problem instance. Through an explicit example, we show that \textsc{Pedel} yields provable gains over low-regret, minimax-optimal algorithms and that such algorithms are unable to hit the insta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#22122;&#22768;&#26102;&#38388;&#24207;&#21015;&#30340;&#24314;&#27169;&#65292;&#32771;&#23519;&#20102;&#20004;&#31181;GAN&#27169;&#22411;&#29992;&#20110;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#30340;&#21487;&#34892;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#23427;&#20204;&#22312;&#21508;&#31867;&#22122;&#22768;&#30340;&#22797;&#21046;&#26041;&#38754;&#20855;&#26377;&#19968;&#23450;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2207.01110</link><description>&lt;p&gt;
&#29992;&#21367;&#31215;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#25968;&#25454;&#39537;&#21160;&#22122;&#22768;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Data-Driven Modeling of Noise Time Series with Convolutional Generative Adversarial Networks. (arXiv:2207.01110v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.01110
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#22122;&#22768;&#26102;&#38388;&#24207;&#21015;&#30340;&#24314;&#27169;&#65292;&#32771;&#23519;&#20102;&#20004;&#31181;GAN&#27169;&#22411;&#29992;&#20110;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#30340;&#21487;&#34892;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#23427;&#20204;&#22312;&#21508;&#31867;&#22122;&#22768;&#30340;&#22797;&#21046;&#26041;&#38754;&#20855;&#26377;&#19968;&#23450;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#36807;&#31243;&#20013;&#20135;&#29983;&#30340;&#38543;&#26426;&#22122;&#22768;&#26159;&#27979;&#37327;&#30340;&#22266;&#26377;&#29305;&#24615;&#65292;&#24182;&#19988;&#26159;&#22823;&#22810;&#25968;&#20449;&#21495;&#22788;&#29702;&#21644;&#25968;&#25454;&#20998;&#26512;&#20219;&#21153;&#30340;&#38480;&#21046;&#22240;&#32032;&#12290;&#37492;&#20110;&#36817;&#24180;&#26469;&#23545;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;(GANs)&#29992;&#20110;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#30340;&#20852;&#36259;&#65292;&#30830;&#23450;GANs&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#33021;&#22815;&#24544;&#23454;&#22320;&#22797;&#21046;&#30446;&#26631;&#25968;&#25454;&#38598;&#20013;&#30340;&#22122;&#22768;&#38750;&#24120;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#39033;&#23454;&#35777;&#30740;&#31350;&#65292;&#26088;&#22312;&#20026;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#36825;&#20010;&#38382;&#39064;&#25552;&#20379;&#21551;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#20004;&#31181;&#22522;&#20110;&#27969;&#34892;&#30340;&#28145;&#24230;&#21367;&#31215;GAN&#65288;DCGAN&#65289;&#32467;&#26500;&#30340;&#36890;&#29992;&#26102;&#38388;&#24207;&#21015;GAN&#65292;&#19968;&#31181;&#26159;&#30452;&#25509;&#30340;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#65292;&#21478;&#19968;&#31181;&#26159;&#22522;&#20110;&#30701;&#26102;&#20613;&#37324;&#21494;&#21464;&#25442;&#65288;STFT&#65289;&#25968;&#25454;&#34920;&#31034;&#30340;&#22270;&#20687;&#27169;&#22411;&#12290;GAN&#27169;&#22411;&#20351;&#29992;&#24050;&#30693;&#22522;&#30784;&#30495;&#23454;&#21442;&#25968;&#30340;&#27169;&#25311;&#22122;&#22768;&#26102;&#38388;&#24207;&#21015;&#20998;&#24067;&#36827;&#34892;&#35757;&#32451;&#21644;&#23450;&#37327;&#35780;&#20272;&#12290;&#30446;&#26631;&#26102;&#38388;&#24207;&#21015;&#20998;&#24067;&#28085;&#30422;&#20102;&#29289;&#29702;&#27979;&#37327;&#12289;&#30005;&#23376;&#23398;&#21644;...
&lt;/p&gt;
&lt;p&gt;
Random noise arising from physical processes is an inherent characteristic of measurements and a limiting factor for most signal processing and data analysis tasks. Given the recent interest in generative adversarial networks (GANs) for data-driven modeling, it is important to determine to what extent GANs can faithfully reproduce noise in target data sets. In this paper, we present an empirical investigation that aims to shed light on this issue for time series. Namely, we assess two general-purpose GANs for time series that are based on the popular deep convolutional GAN (DCGAN) architecture, a direct time-series model and an image-based model that uses a short-time Fourier transform (STFT) data representation. The GAN models are trained and quantitatively evaluated using distributions of simulated noise time series with known ground-truth parameters. Target time series distributions include a broad range of noise types commonly encountered in physical measurements, electronics, and 
&lt;/p&gt;</description></item><item><title>Pythae&#26159;&#19968;&#20010;&#24320;&#28304;Python&#24211;&#65292;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#29983;&#25104;&#33258;&#32534;&#30721;&#22120;&#27169;&#22411;&#23454;&#29616;&#21644;&#26694;&#26550;&#65292;&#21487;&#29992;&#20110;&#36827;&#34892;&#22810;&#31181;&#20219;&#21153;&#30340;&#26696;&#20363;&#30740;&#31350;&#22522;&#20934;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2206.08309</link><description>&lt;p&gt;
Pythae&#65306;&#32479;&#19968;&#30340;&#29983;&#25104;&#33258;&#32534;&#30721;&#22120;Python&#24211;&#8212;&#8212;&#22522;&#20934;&#27979;&#35797;&#29992;&#20363;
&lt;/p&gt;
&lt;p&gt;
Pythae: Unifying Generative Autoencoders in Python -- A Benchmarking Use Case. (arXiv:2206.08309v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.08309
&lt;/p&gt;
&lt;p&gt;
Pythae&#26159;&#19968;&#20010;&#24320;&#28304;Python&#24211;&#65292;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#29983;&#25104;&#33258;&#32534;&#30721;&#22120;&#27169;&#22411;&#23454;&#29616;&#21644;&#26694;&#26550;&#65292;&#21487;&#29992;&#20110;&#36827;&#34892;&#22810;&#31181;&#20219;&#21153;&#30340;&#26696;&#20363;&#30740;&#31350;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22240;&#20854;&#23545;&#22797;&#26434;&#20998;&#24067;&#30340;&#24314;&#27169;&#33021;&#21147;&#32780;&#24341;&#36215;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#22312;&#36825;&#20123;&#27169;&#22411;&#20013;&#65292;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#22240;&#20854;&#22312;&#35745;&#31639;&#19978;&#30340;&#39640;&#25928;&#24615;&#21644;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#20135;&#29983;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#22312;&#36825;&#19968;&#31361;&#30772;&#20043;&#21518;&#65292;&#20026;&#20102;&#25913;&#36827;&#21407;&#22987;&#35770;&#25991;&#24050;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#24037;&#20316;&#65292;&#23548;&#33268;&#20102;&#21508;&#31181;&#19981;&#21516;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#27169;&#22411;&#20197;&#24212;&#23545;&#19981;&#21516;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Pythae&#65292;&#19968;&#20010;&#36890;&#29992;&#30340;&#24320;&#28304;Python&#24211;&#65292;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#23454;&#29616;&#21644;&#19987;&#38376;&#30340;&#26694;&#26550;&#65292;&#21487;&#26041;&#20415;&#12289;&#21487;&#37325;&#29616;&#12289;&#21487;&#38752;&#22320;&#20351;&#29992;&#29983;&#25104;&#33258;&#32534;&#30721;&#22120;&#27169;&#22411;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#35813;&#24211;&#36827;&#34892;&#26696;&#20363;&#30740;&#31350;&#22522;&#20934;&#27979;&#35797;&#65292;&#22312;&#20854;&#20013;&#23637;&#31034;&#24182;&#27604;&#36739;&#20102;19&#20010;&#29983;&#25104;&#33258;&#32534;&#30721;&#22120;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#20195;&#34920;&#20102;&#22312;&#22270;&#20687;&#37325;&#26500;&#12289;&#29983;&#25104;&#12289;&#20998;&#31867;&#12289;&#32858;&#31867;&#21644;&#25554;&#20540;&#31561;&#19979;&#28216;&#20219;&#21153;&#19978;&#30340;&#19968;&#20123;&#20027;&#35201;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, deep generative models have attracted increasing interest due to their capacity to model complex distributions. Among those models, variational autoencoders have gained popularity as they have proven both to be computationally efficient and yield impressive results in multiple fields. Following this breakthrough, extensive research has been done in order to improve the original publication, resulting in a variety of different VAE models in response to different tasks. In this paper we present Pythae, a versatile open-source Python library providing both a unified implementation and a dedicated framework allowing straightforward, reproducible and reliable use of generative autoencoder models. We then propose to use this library to perform a case study benchmark where we present and compare 19 generative autoencoder models representative of some of the main improvements on downstream tasks such as image reconstruction, generation, classification, clustering and interpola
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#24863;&#30693;&#29305;&#24449;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;MMD&#65288;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#65289;&#26469;&#25552;&#39640;&#24046;&#20998;&#38544;&#31169;&#22270;&#20687;&#29983;&#25104;&#30340;&#24615;&#33021;&#65292;&#24182;&#25104;&#21151;&#22320;&#29983;&#25104;&#20102;CIFAR10&#32423;&#21035;&#30340;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2205.12900</link><description>&lt;p&gt;
&#39044;&#20808;&#35757;&#32451;&#30340;&#24863;&#30693;&#29305;&#24449;&#25552;&#39640;&#24046;&#20998;&#38544;&#31169;&#22270;&#20687;&#29983;&#25104;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Pre-trained Perceptual Features Improve Differentially Private Image Generation. (arXiv:2205.12900v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.12900
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#24863;&#30693;&#29305;&#24449;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;MMD&#65288;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#65289;&#26469;&#25552;&#39640;&#24046;&#20998;&#38544;&#31169;&#22270;&#20687;&#29983;&#25104;&#30340;&#24615;&#33021;&#65292;&#24182;&#25104;&#21151;&#22320;&#29983;&#25104;&#20102;CIFAR10&#32423;&#21035;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;DP-SGD&#65289;&#36827;&#34892;&#20013;&#31561;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#30340;&#35757;&#32451;&#38750;&#24120;&#22256;&#38590;&#65306;&#20026;&#20102;&#20445;&#25345;&#21512;&#29702;&#30340;&#38544;&#31169;&#27700;&#24179;&#25152;&#38656;&#30340;&#22122;&#22768;&#27700;&#24179;&#36807;&#22823;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#24314;&#35758;&#21033;&#29992;&#20449;&#24687;&#20016;&#23500;&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#33391;&#22909;&#30456;&#20851;&#34920;&#24449;&#65292;&#28982;&#21518;&#23398;&#20064;&#20351;&#29992;&#35813;&#34920;&#24449;&#27169;&#22411;&#21270;&#31169;&#26377;&#25968;&#25454;&#12290;&#29305;&#21035;&#30340;&#65292;&#25105;&#20204;&#20351;&#29992;&#20174;&#20844;&#20849;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#30340;&#24863;&#30693;&#29305;&#24449;&#30340;&#26680;&#20989;&#25968;&#65292;&#26368;&#23567;&#21270;&#31169;&#26377;&#30446;&#26631;&#25968;&#25454;&#19982;&#29983;&#25104;&#22120;&#20998;&#24067;&#20043;&#38388;&#30340;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#65288;MMD&#65289;&#12290;&#20351;&#29992;MMD&#65292;&#25105;&#20204;&#21487;&#20197;&#19968;&#27425;&#24615;&#23545;&#25968;&#25454;&#30456;&#20851;&#39033;&#36827;&#34892;&#38544;&#31169;&#22788;&#29702;&#65292;&#32780;&#26080;&#38656;&#20687;DP-SGD&#19968;&#26679;&#22312;&#20248;&#21270;&#27599;&#19968;&#27493;&#20013;&#24341;&#20837;&#22122;&#22768;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#29983;&#25104;CIFAR10&#32423;&#21035;&#30340;&#22270;&#20687;&#65292;&#20854; $\epsilon \approx 2$&#65292;&#25429;&#25417;&#20102;&#20998;&#24067;&#20013;&#30340;&#29420;&#29305;&#29305;&#24449;&#65292;&#36828;&#36828;&#36229;&#36807;&#24403;&#21069;&#30340;&#25216;&#26415;&#27700;&#24179;&#65292;&#20027;&#35201;&#38598;&#20013;&#20110;&#25968;&#25454;&#38598;&#65292;&#22914;MNIST&#21644;FashionMNIST &#20197;&#36739;&#22823;&#30340; $\epsilon$&#12290;
&lt;/p&gt;
&lt;p&gt;
Training even moderately-sized generative models with differentially-private stochastic gradient descent (DP-SGD) is difficult: the required level of noise for reasonable levels of privacy is simply too large. We advocate instead building off a good, relevant representation on an informative public dataset, then learning to model the private data with that representation. In particular, we minimize the maximum mean discrepancy (MMD) between private target data and a generator's distribution, using a kernel based on perceptual features learned from a public dataset. With the MMD, we can simply privatize the data-dependent term once and for all, rather than introducing noise at each step of optimization as in DP-SGD. Our algorithm allows us to generate CIFAR10-level images with $\epsilon \approx 2$ which capture distinctive features in the distribution, far surpassing the current state of the art, which mostly focuses on datasets such as MNIST and FashionMNIST at a large $\epsilon \appro
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23398;&#20064;&#21704;&#24076;&#25216;&#26415;&#25552;&#39640;&#38646;&#26679;&#26412;&#23494;&#38598;&#26816;&#32034;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#65292;&#20811;&#26381;&#20102;&#23384;&#20648;&#23494;&#38598;&#32034;&#24341;&#30340;&#39640;&#20869;&#23384;&#20351;&#29992;&#38382;&#39064;&#65292;&#24182;&#22312;&#36328;&#39046;&#22495;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2205.11498</link><description>&lt;p&gt;
&#20026;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#38646;&#26679;&#26412;&#23494;&#38598;&#26816;&#32034;&#27880;&#20837;&#39046;&#22495;&#36866;&#24212;&#30340;&#23398;&#20064;&#21704;&#24076;
&lt;/p&gt;
&lt;p&gt;
Injecting Domain Adaptation with Learning-to-hash for Effective and Efficient Zero-shot Dense Retrieval. (arXiv:2205.11498v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.11498
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#21704;&#24076;&#25216;&#26415;&#25552;&#39640;&#38646;&#26679;&#26412;&#23494;&#38598;&#26816;&#32034;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#65292;&#20811;&#26381;&#20102;&#23384;&#20648;&#23494;&#38598;&#32034;&#24341;&#30340;&#39640;&#20869;&#23384;&#20351;&#29992;&#38382;&#39064;&#65292;&#24182;&#22312;&#36328;&#39046;&#22495;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23494;&#38598;&#26816;&#32034;&#22312;&#26080;&#26597;&#35810;&#35789;&#26816;&#32034;&#20013;&#20811;&#26381;&#20102;&#35789;&#27719;&#38548;&#38402;&#65292;&#24182;&#22312;&#33258;&#21160;&#20449;&#24687;&#26816;&#32034;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#23613;&#31649;&#25104;&#21151;&#65292;&#20294;&#23494;&#38598;&#26816;&#32034;&#22120;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#26381;&#21153;&#25104;&#26412;&#36739;&#39640;&#12290;&#23545;&#20110;&#38656;&#35201;&#20174;&#25968;&#30334;&#19975;&#20221;&#25991;&#26723;&#20013;&#25628;&#32034;&#30340;&#29992;&#20363;&#65292;&#23494;&#38598;&#32034;&#24341;&#21464;&#24471;&#24222;&#22823;&#65292;&#24182;&#19988;&#22312;&#23384;&#20648;&#32034;&#24341;&#26102;&#38656;&#35201;&#39640;&#20869;&#23384;&#20351;&#29992;&#37327;&#12290;&#26368;&#36817;&#30340;&#23398;&#20064;&#21704;&#24076;&#65288;LTH&#65289;&#25216;&#26415;&#65292;&#22914;BPR&#21644;JPQ&#65292;&#29983;&#25104;&#20108;&#36827;&#21046;&#25991;&#26723;&#21521;&#37327;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#23384;&#20648;&#23494;&#38598;&#32034;&#24341;&#30340;&#20869;&#23384;&#38656;&#27714;&#12290;LTH&#25216;&#26415;&#26159;&#26377;&#30417;&#30563;&#30340;&#65292;&#24182;&#20351;&#29992;&#25490;&#21517;&#25439;&#22833;&#23545;&#26816;&#32034;&#22120;&#36827;&#34892;&#24494;&#35843;&#12290;&#23427;&#20204;&#20248;&#20110;&#20256;&#32479;&#30340;&#21521;&#37327;&#21387;&#32553;&#25216;&#26415;&#65292;&#22914;PCA&#25110;PQ&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#20013;&#32570;&#23569;&#30340;&#19968;&#20010;&#29615;&#33410;&#26159;&#29616;&#26377;&#25216;&#26415;&#20165;&#22312;&#39046;&#22495;&#20869;&#36827;&#34892;&#35780;&#20272;&#65292;&#21363;&#20165;&#22312;&#21333;&#19968;&#25968;&#25454;&#38598;&#65288;&#22914;MS MARCO&#65289;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;LTH&#21644;&#21521;&#37327;&#21387;&#32553;&#25216;&#26415;&#65292;&#20197;&#25552;&#39640;TAS-B d&#30340;&#38646;&#26679;&#26412;&#26816;&#32034;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dense retrieval overcome the lexical gap and has shown great success in ad-hoc information retrieval (IR). Despite their success, dense retrievers are expensive to serve across practical use cases. For use cases requiring to search from millions of documents, the dense index becomes bulky and requires high memory usage for storing the index. More recently, learning-to-hash (LTH) techniques, for e.g., BPR and JPQ, produce binary document vectors, thereby reducing the memory requirement to efficiently store the dense index. LTH techniques are supervised and finetune the retriever using a ranking loss. They outperform their counterparts, i.e., traditional out-of-the-box vector compression techniques such as PCA or PQ. A missing piece from prior work is that existing techniques have been evaluated only in-domain, i.e., on a single dataset such as MS MARCO. In our work, we evaluate LTH and vector compression techniques for improving the downstream zero-shot retrieval accuracy of the TAS-B d
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#35777;&#25454;&#22238;&#24402;&#26159;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#35777;&#25454;&#20998;&#24067;&#26469;&#22788;&#29702;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#22312;&#19981;&#30830;&#23450;&#24615;&#25512;&#29702;&#20013;&#26174;&#31034;&#20986;&#30456;&#23545;&#20110;&#20256;&#32479;&#26041;&#27861;&#21644;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#23427;&#24182;&#38750;&#31934;&#30830;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#32780;&#26159;&#19968;&#31181;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2205.10060</link><description>&lt;p&gt;
&#28145;&#24230;&#35777;&#25454;&#22238;&#24402;&#30340;&#19981;&#21512;&#29702;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Unreasonable Effectiveness of Deep Evidential Regression. (arXiv:2205.10060v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.10060
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#35777;&#25454;&#22238;&#24402;&#26159;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#35777;&#25454;&#20998;&#24067;&#26469;&#22788;&#29702;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#22312;&#19981;&#30830;&#23450;&#24615;&#25512;&#29702;&#20013;&#26174;&#31034;&#20986;&#30456;&#23545;&#20110;&#20256;&#32479;&#26041;&#27861;&#21644;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#23427;&#24182;&#38750;&#31934;&#30830;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#32780;&#26159;&#19968;&#31181;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#22312;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#30340;&#19981;&#26029;&#37096;&#32626;&#65292;&#23545;&#20110;&#22522;&#20110;&#21407;&#21017;&#30340;&#19981;&#30830;&#23450;&#24615;&#25512;&#29702;&#30340;&#38656;&#27714;&#26085;&#30410;&#36843;&#20999;&#12290;&#19968;&#31181;&#26032;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#22238;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#20851;&#20110;&#20869;&#37096;&#21464;&#37327;&#21644;&#22806;&#37096;&#21464;&#37327;&#30340;&#19981;&#30830;&#23450;&#24615;&#30340;&#35777;&#25454;&#20998;&#24067;&#65292;&#26174;&#31034;&#20986;&#30456;&#23545;&#20110;&#20256;&#32479;&#30830;&#23450;&#24615;&#26041;&#27861;&#21644;&#20856;&#22411;&#36125;&#21494;&#26031;NN&#30340;&#20248;&#21183;&#65292;&#23588;&#20854;&#22312;&#33021;&#22815;&#35299;&#32806;&#20869;&#37096;&#21644;&#22806;&#37096;&#19981;&#30830;&#23450;&#24615;&#26041;&#38754;&#12290;&#23613;&#31649;&#28145;&#24230;&#35777;&#25454;&#22238;&#24402;&#65288;DER&#65289;&#21462;&#24471;&#20102;&#19968;&#23450;&#30340;&#23454;&#35777;&#25104;&#21151;&#65292;&#20294;&#25968;&#23398;&#22522;&#30784;&#23384;&#22312;&#37325;&#35201;&#30340;&#19981;&#36275;&#65292;&#36825;&#24341;&#21457;&#20102;&#20026;&#20309;&#25152;&#25552;&#20986;&#30340;&#25216;&#26415;&#30475;&#20284;&#26377;&#25928;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#35814;&#32454;&#35828;&#26126;&#20102;&#29702;&#35770;&#19978;&#30340;&#19981;&#36275;&#65292;&#24182;&#20998;&#26512;&#20102;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#65292;&#34920;&#26126;&#28145;&#24230;&#35777;&#25454;&#22238;&#24402;&#26159;&#19968;&#31181;&#21551;&#21457;&#24335;&#32780;&#38750;&#31934;&#30830;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#12290;&#25105;&#20204;&#32487;&#32493;&#35752;&#35770;&#22914;&#20309;&#20462;&#27491;&#21644;&#37325;&#26032;&#23450;&#20041;&#20869;&#37096;&#21644;&#22806;&#37096;&#19981;&#30830;&#23450;&#24615;&#30340;&#25552;&#21462;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is a significant need for principled uncertainty reasoning in machine learning systems as they are increasingly deployed in safety-critical domains. A new approach with uncertainty-aware regression-based neural networks (NNs), based on learning evidential distributions for aleatoric and epistemic uncertainties, shows promise over traditional deterministic methods and typical Bayesian NNs, notably with the capabilities to disentangle aleatoric and epistemic uncertainties. Despite some empirical success of Deep Evidential Regression (DER), there are important gaps in the mathematical foundation that raise the question of why the proposed technique seemingly works. We detail the theoretical shortcomings and analyze the performance on synthetic and real-world data sets, showing that Deep Evidential Regression is a heuristic rather than an exact uncertainty quantification. We go on to discuss corrections and redefinitions of how aleatoric and epistemic uncertainties should be extracte
&lt;/p&gt;</description></item><item><title>HDGT&#26159;&#19968;&#31181;&#23558;&#39550;&#39542;&#22330;&#26223;&#24314;&#27169;&#20026;&#24322;&#26500;&#22270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22330;&#26223;&#32534;&#30721;&#23454;&#29616;&#22810;&#26234;&#33021;&#20307;&#36712;&#36857;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#32771;&#34385;&#21040;&#20102;&#39550;&#39542;&#22330;&#26223;&#20013;&#30340;&#19981;&#21516;&#23545;&#35937;&#21644;&#20016;&#23500;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#24182;&#20351;&#29992;&#33258;&#25105;&#20013;&#24515;&#30340;&#26041;&#24335;&#36827;&#34892;&#31354;&#38388;&#20851;&#31995;&#32534;&#30721;&#12290;</title><link>http://arxiv.org/abs/2205.09753</link><description>&lt;p&gt;
HDGT: &#22810;&#26234;&#33021;&#20307;&#36712;&#36857;&#39044;&#27979;&#30340;&#24322;&#26500;&#39550;&#39542;&#22270;&#21464;&#25442;&#22120;&#36890;&#36807;&#22330;&#26223;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
HDGT: Heterogeneous Driving Graph Transformer for Multi-Agent Trajectory Prediction via Scene Encoding. (arXiv:2205.09753v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.09753
&lt;/p&gt;
&lt;p&gt;
HDGT&#26159;&#19968;&#31181;&#23558;&#39550;&#39542;&#22330;&#26223;&#24314;&#27169;&#20026;&#24322;&#26500;&#22270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22330;&#26223;&#32534;&#30721;&#23454;&#29616;&#22810;&#26234;&#33021;&#20307;&#36712;&#36857;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#32771;&#34385;&#21040;&#20102;&#39550;&#39542;&#22330;&#26223;&#20013;&#30340;&#19981;&#21516;&#23545;&#35937;&#21644;&#20016;&#23500;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#24182;&#20351;&#29992;&#33258;&#25105;&#20013;&#24515;&#30340;&#26041;&#24335;&#36827;&#34892;&#31354;&#38388;&#20851;&#31995;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#39550;&#39542;&#22330;&#26223;&#32534;&#30721;&#20026;&#21521;&#37327;&#34920;&#31034;&#26159;&#33258;&#21160;&#39550;&#39542;&#30340;&#19968;&#20010;&#37325;&#35201;&#20219;&#21153;&#65292;&#21487;&#20197;&#20351;&#24471;&#36712;&#36857;&#39044;&#27979;&#31561;&#19979;&#28216;&#20219;&#21153;&#21463;&#30410;&#12290;&#39550;&#39542;&#22330;&#26223;&#36890;&#24120;&#28041;&#21450;&#21040;&#19981;&#21516;&#31867;&#22411;&#30340;&#23545;&#35937;&#65288;&#26234;&#33021;&#20307;&#12289;&#36710;&#36947;&#12289;&#20132;&#36890;&#26631;&#24535;&#65289;&#20197;&#21450;&#23545;&#35937;&#20043;&#38388;&#20016;&#23500;&#22810;&#26679;&#30340;&#35821;&#20041;&#20851;&#31995;&#12290;&#21516;&#26102;&#65292;&#20803;&#32032;&#20043;&#38388;&#20063;&#23384;&#22312;&#30456;&#23545;&#24615;&#65292;&#36825;&#24847;&#21619;&#30528;&#31354;&#38388;&#20851;&#31995;&#26159;&#19968;&#20010;&#30456;&#23545;&#27010;&#24565;&#65292;&#38656;&#35201;&#20197;&#33258;&#25105;&#20013;&#24515;&#30340;&#26041;&#24335;&#36827;&#34892;&#32534;&#30721;&#65292;&#32780;&#19981;&#26159;&#20840;&#23616;&#22352;&#26631;&#31995;&#12290;&#22522;&#20110;&#36825;&#20123;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24322;&#26500;&#39550;&#39542;&#22270;&#21464;&#25442;&#22120;&#65288;HDGT&#65289;&#65292;&#23558;&#39550;&#39542;&#22330;&#26223;&#24314;&#27169;&#20026;&#19968;&#20010;&#20855;&#26377;&#19981;&#21516;&#31867;&#22411;&#33410;&#28857;&#21644;&#36793;&#30340;&#24322;&#26500;&#22270;&#12290;&#22312;&#24322;&#26500;&#22270;&#30340;&#26500;&#24314;&#20013;&#65292;&#25105;&#20204;&#26681;&#25454;&#22810;&#26679;&#30340;&#35821;&#20041;&#20851;&#31995;&#36830;&#25509;&#19981;&#21516;&#31867;&#22411;&#30340;&#33410;&#28857;&#12290;&#22312;&#31354;&#38388;&#20851;&#31995;&#32534;&#30721;&#20013;&#65292;&#33410;&#28857;&#30340;&#22352;&#26631;&#20197;&#21450;&#20854;&#20837;&#36793;&#26159;&#22312;&#23616;&#37096;&#33410;&#28857;&#20013;&#24515;&#22352;&#26631;&#31995;&#20013;&#34920;&#31034;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Encoding a driving scene into vector representations has been an essential task for autonomous driving that can benefit downstream tasks e.g. trajectory prediction. The driving scene often involves heterogeneous elements such as the different types of objects (agents, lanes, traffic signs) and the semantic relations between objects are rich and diverse. Meanwhile, there also exist relativity across elements, which means that the spatial relation is a relative concept and need be encoded in a ego-centric manner instead of in a global coordinate system. Based on these observations, we propose Heterogeneous Driving Graph Transformer (HDGT), a backbone modelling the driving scene as a heterogeneous graph with different types of nodes and edges. For heterogeneous graph construction, we connect different types of nodes according to diverse semantic relations. For spatial relation encoding, the coordinates of the node as well as its in-edges are in the local node-centric coordinate system. Fo
&lt;/p&gt;</description></item><item><title>Torchhd&#26159;&#19968;&#20010;&#39640;&#24615;&#33021;&#30340;&#24320;&#28304;Python&#24211;&#65292;&#26088;&#22312;&#25903;&#25345;&#36229;&#32500;&#35745;&#31639;&#21644;&#21521;&#37327;&#31526;&#21495;&#20307;&#31995;&#30740;&#31350;&#12290;&#23427;&#25552;&#20379;&#20102;&#26368;&#20808;&#36827;&#30340;&#21151;&#33021;&#65292;&#26131;&#20110;&#20351;&#29992;&#30340;&#30028;&#38754;&#65292;&#24182;&#19988;&#33021;&#22815;&#20351;&#23454;&#39564;&#36816;&#34892;&#36895;&#24230;&#25552;&#39640;&#22810;&#36798;100&#20493;&#12290;</title><link>http://arxiv.org/abs/2205.09208</link><description>&lt;p&gt;
Torchhd:&#19968;&#31181;&#25903;&#25345;&#36229;&#32500;&#35745;&#31639;&#21644;&#21521;&#37327;&#31526;&#21495;&#20307;&#31995;&#30740;&#31350;&#30340;&#24320;&#28304;Python&#24211;
&lt;/p&gt;
&lt;p&gt;
Torchhd: An Open Source Python Library to Support Research on Hyperdimensional Computing and Vector Symbolic Architectures. (arXiv:2205.09208v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.09208
&lt;/p&gt;
&lt;p&gt;
Torchhd&#26159;&#19968;&#20010;&#39640;&#24615;&#33021;&#30340;&#24320;&#28304;Python&#24211;&#65292;&#26088;&#22312;&#25903;&#25345;&#36229;&#32500;&#35745;&#31639;&#21644;&#21521;&#37327;&#31526;&#21495;&#20307;&#31995;&#30740;&#31350;&#12290;&#23427;&#25552;&#20379;&#20102;&#26368;&#20808;&#36827;&#30340;&#21151;&#33021;&#65292;&#26131;&#20110;&#20351;&#29992;&#30340;&#30028;&#38754;&#65292;&#24182;&#19988;&#33021;&#22815;&#20351;&#23454;&#39564;&#36816;&#34892;&#36895;&#24230;&#25552;&#39640;&#22810;&#36798;100&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#32500;&#35745;&#31639;&#65288;HD&#65289;&#65292;&#20063;&#34987;&#31216;&#20026;&#21521;&#37327;&#31526;&#21495;&#20307;&#31995;&#65288;VSA&#65289;&#65292;&#26159;&#19968;&#31181;&#21033;&#29992;&#38543;&#26426;&#39640;&#32500;&#21521;&#37327;&#31354;&#38388;&#30340;&#24615;&#36136;&#36827;&#34892;&#20998;&#24067;&#24335;&#34920;&#31034;&#35745;&#31639;&#30340;&#26694;&#26550;&#12290;&#31185;&#23398;&#30028;&#23545;&#20110;&#32858;&#38598;&#21644;&#20256;&#25773;&#36825;&#20010;&#29305;&#21035;&#22810;&#23398;&#31185;&#39046;&#22495;&#30340;&#30740;&#31350;&#30340;&#25215;&#35834;&#23545;&#20110;&#20854;&#36827;&#27493;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#21152;&#20837;&#36825;&#20123;&#21162;&#21147;&#65292;&#25105;&#20204;&#25512;&#20986;&#20102;Torchhd&#65292;&#19968;&#20010;&#39640;&#24615;&#33021;&#30340;&#36229;&#32500;/VSA&#30340;&#24320;&#28304;Python&#24211;&#12290;Torchhd&#26088;&#22312;&#20351;&#36229;&#32500;/VSA&#26356;&#26131;&#20110;&#20351;&#29992;&#65292;&#24182;&#20026;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#24320;&#21457;&#25552;&#20379;&#39640;&#25928;&#30340;&#22522;&#30784;&#12290;&#36825;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;&#24211;&#24314;&#31435;&#22312;PyTorch&#20043;&#19978;&#65292;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#36229;&#32500;/VSA&#21151;&#33021;&#65292;&#28165;&#26224;&#30340;&#25991;&#26723;&#21644;&#26469;&#33258;&#30693;&#21517;&#20986;&#29256;&#29289;&#30340;&#23454;&#29616;&#31034;&#20363;&#12290;&#23558;&#20844;&#24320;&#21487;&#29992;&#30340;&#20195;&#30721;&#19982;&#20854;&#30456;&#24212;&#30340;Torchhd&#23454;&#29616;&#36827;&#34892;&#27604;&#36739;&#65292;&#23454;&#39564;&#21487;&#20197;&#36816;&#34892;&#36895;&#24230;&#25552;&#39640;&#22810;&#36798;100&#20493;&#12290;Torchhd&#21487;&#22312;&#20197;&#19979;&#38142;&#25509;&#20013;&#33719;&#21462;&#65306;https://github.com/hyperdimensional-computing/tor
&lt;/p&gt;
&lt;p&gt;
Hyperdimensional computing (HD), also known as vector symbolic architectures (VSA), is a framework for computing with distributed representations by exploiting properties of random high-dimensional vector spaces. The commitment of the scientific community to aggregate and disseminate research in this particularly multidisciplinary area has been fundamental for its advancement. Joining these efforts, we present Torchhd, a high-performance open source Python library for HD/VSA. Torchhd seeks to make HD/VSA more accessible and serves as an efficient foundation for further research and application development. The easy-to-use library builds on top of PyTorch and features state-of-the-art HD/VSA functionality, clear documentation, and implementation examples from well-known publications. Comparing publicly available code with their corresponding Torchhd implementation shows that experiments can run up to 100x faster. Torchhd is available at: https://github.com/hyperdimensional-computing/tor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#32467;&#26500;&#21160;&#21147;&#23398;&#21644;&#25391;&#21160;&#22768;&#23398;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#32467;&#26500;&#20581;&#24247;&#30417;&#27979;&#12289;&#20027;&#21160;&#22122;&#22768;&#25511;&#21046;&#12289;&#20027;&#21160;&#25391;&#21160;&#25511;&#21046;&#20197;&#21450;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20195;&#29702;&#27169;&#22411;&#12290;&#36825;&#20123;&#24212;&#29992;&#36890;&#36807;&#25968;&#25454;&#25581;&#31034;&#27934;&#23519;&#21147;&#65292;&#25552;&#39640;&#20102;&#20915;&#31574;&#21046;&#23450;&#21644;&#20248;&#21270;&#20135;&#21697;&#35774;&#35745;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2204.06362</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#32467;&#26500;&#21160;&#21147;&#23398;&#21644;&#25391;&#21160;&#22768;&#23398;&#20013;&#30340;&#24212;&#29992;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Review of Machine Learning Methods Applied to Structural Dynamics and Vibroacoustic. (arXiv:2204.06362v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.06362
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#32467;&#26500;&#21160;&#21147;&#23398;&#21644;&#25391;&#21160;&#22768;&#23398;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#32467;&#26500;&#20581;&#24247;&#30417;&#27979;&#12289;&#20027;&#21160;&#22122;&#22768;&#25511;&#21046;&#12289;&#20027;&#21160;&#25391;&#21160;&#25511;&#21046;&#20197;&#21450;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20195;&#29702;&#27169;&#22411;&#12290;&#36825;&#20123;&#24212;&#29992;&#36890;&#36807;&#25968;&#25454;&#25581;&#31034;&#27934;&#23519;&#21147;&#65292;&#25552;&#39640;&#20102;&#20915;&#31574;&#21046;&#23450;&#21644;&#20248;&#21270;&#20135;&#21697;&#35774;&#35745;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#32467;&#26500;&#21160;&#21147;&#23398;&#21644;&#25391;&#21160;&#22768;&#23398;&#39046;&#22495;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#23427;&#36890;&#36807;&#25968;&#25454;&#25581;&#31034;&#27934;&#23519;&#21147;&#65292;&#32467;&#21512;&#31639;&#27861;&#36827;&#27493;&#21644;&#35745;&#31639;&#33021;&#21147;&#30340;&#25552;&#39640;&#65292;&#22686;&#24378;&#20102;&#20915;&#31574;&#21046;&#23450;&#12289;&#19981;&#30830;&#23450;&#24615;&#22788;&#29702;&#12289;&#27169;&#24335;&#35782;&#21035;&#21644;&#23454;&#26102;&#35780;&#20272;&#30340;&#33021;&#21147;&#12290;&#22312;&#32467;&#26500;&#20581;&#24247;&#30417;&#27979;&#12289;&#20027;&#21160;&#22122;&#22768;&#25511;&#21046;&#21644;&#20027;&#21160;&#25391;&#21160;&#25511;&#21046;&#20197;&#21450;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20195;&#29702;&#27169;&#22411;&#26041;&#38754;&#26377;&#24456;&#22810;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#26377;&#35768;&#22810;&#30456;&#20851;&#24037;&#20316;&#65292;&#20294;&#36824;&#27809;&#26377;&#36827;&#34892;&#32508;&#36848;&#21644;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of Machine Learning (ML) has rapidly spread across several fields, having encountered many applications in Structural Dynamics and Vibroacoustic (SD\&amp;V). The increasing capabilities of ML to unveil insights from data, driven by unprecedented data availability, algorithms advances and computational power, enhance decision making, uncertainty handling, patterns recognition and real-time assessments. Three main applications in SD\&amp;V have taken advantage of these benefits. In Structural Health Monitoring, ML detection and prognosis lead to safe operation and optimized maintenance schedules. System identification and control design are leveraged by ML techniques in Active Noise Control and Active Vibration Control. Finally, the so-called ML-based surrogate models provide fast alternatives to costly simulations, enabling robust and optimized product design. Despite the many works in the area, they have not been reviewed and analyzed. Therefore, to keep track and understand this ongoi
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28176;&#36827;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;PGCN&#65289;&#30340;&#20132;&#36890;&#39044;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#38454;&#27573;&#36880;&#27493;&#36866;&#24212;&#36755;&#20837;&#25968;&#25454;&#26469;&#26500;&#24314;&#19968;&#32452;&#22270;&#65292;&#20197;&#35299;&#20915;&#20132;&#36890;&#39044;&#27979;&#20013;&#30340;&#22797;&#26434;&#26102;&#31354;&#30456;&#20851;&#24615;&#21644;&#25968;&#25454;&#21464;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2202.08982</link><description>&lt;p&gt;
PGCN&#65306;&#29992;&#20110;&#26102;&#31354;&#20132;&#36890;&#39044;&#27979;&#30340;&#28176;&#36827;&#22270;&#21367;&#31215;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
PGCN: Progressive Graph Convolutional Networks for Spatial-Temporal Traffic Forecasting. (arXiv:2202.08982v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.08982
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28176;&#36827;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;PGCN&#65289;&#30340;&#20132;&#36890;&#39044;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#38454;&#27573;&#36880;&#27493;&#36866;&#24212;&#36755;&#20837;&#25968;&#25454;&#26469;&#26500;&#24314;&#19968;&#32452;&#22270;&#65292;&#20197;&#35299;&#20915;&#20132;&#36890;&#39044;&#27979;&#20013;&#30340;&#22797;&#26434;&#26102;&#31354;&#30456;&#20851;&#24615;&#21644;&#25968;&#25454;&#21464;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#32593;&#32476;&#20013;&#22797;&#26434;&#30340;&#26102;&#31354;&#30456;&#20851;&#24615;&#20351;&#24471;&#20132;&#36890;&#39044;&#27979;&#38382;&#39064;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#30001;&#20110;&#20132;&#36890;&#31995;&#32479;&#26412;&#36136;&#19978;&#20855;&#26377;&#22270;&#32467;&#26500;&#65292;&#22240;&#27492;&#24050;&#32463;&#36827;&#34892;&#20102;&#22823;&#37327;&#20851;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#30740;&#31350;&#12290;&#26368;&#36817;&#65292;&#26500;&#24314;&#36866;&#24212;&#24615;&#22270;&#23545;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#22312;&#21333;&#19968;&#38745;&#24577;&#22270;&#32467;&#26500;&#19978;&#30340;&#27169;&#22411;&#19978;&#26174;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22270;&#36866;&#24212;&#24615;&#20165;&#22312;&#35757;&#32451;&#38454;&#27573;&#24212;&#29992;&#65292;&#24182;&#19981;&#21453;&#26144;&#35757;&#32451;&#38454;&#27573;&#20351;&#29992;&#30340;&#25968;&#25454;&#12290;&#36825;&#31181;&#32570;&#28857;&#22312;&#20132;&#36890;&#39044;&#27979;&#20013;&#21487;&#33021;&#26377;&#38382;&#39064;&#65292;&#22240;&#20026;&#20132;&#36890;&#25968;&#25454;&#24120;&#24120;&#23384;&#22312;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#24847;&#22806;&#21464;&#21270;&#21644;&#19981;&#35268;&#21017;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20132;&#36890;&#39044;&#27979;&#26694;&#26550;&#65292;&#31216;&#20026;&#28176;&#36827;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;PGCN&#65289;&#12290;PGCN&#36890;&#36807;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#38454;&#27573;&#36880;&#27493;&#36866;&#24212;&#36755;&#20837;&#25968;&#25454;&#26469;&#26500;&#24314;&#19968;&#32452;&#22270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#35813;&#27169;&#22411;&#26469;&#26500;&#24314;&#28176;&#36827;&#34892;&#30340;&#37051;&#25509;&#30697;&#38453;&#12290;
&lt;/p&gt;
&lt;p&gt;
The complex spatial-temporal correlations in transportation networks make the traffic forecasting problem challenging. Since transportation system inherently possesses graph structures, much research efforts have been put with graph neural networks. Recently, constructing adaptive graphs to the data has shown promising results over the models relying on a single static graph structure. However, the graph adaptations are applied during the training phases, and do not reflect the data used during the testing phases. Such shortcomings can be problematic especially in traffic forecasting since the traffic data often suffers from the unexpected changes and irregularities in the time series. In this study, we propose a novel traffic forecasting framework called Progressive Graph Convolutional Network (PGCN). PGCN constructs a set of graphs by progressively adapting to input data during the training and the testing phases. Specifically, we implemented the model to construct progressive adjace
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#23725;&#22238;&#24402;&#30340;&#31616;&#21333;&#38750;&#21442;&#25968;&#20272;&#35745;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#20272;&#35745;&#20171;&#23548;&#21644;&#26102;&#21464;&#21058;&#37327;&#21709;&#24212;&#26354;&#32447;&#12290;&#36890;&#36807;&#24341;&#20837;&#24207;&#36143;&#26680;&#23884;&#20837;&#25216;&#26415;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#23545;&#22797;&#26434;&#22240;&#26524;&#20272;&#35745;&#30340;&#31616;&#21270;&#12290;&#36890;&#36807;&#27169;&#25311;&#23454;&#39564;&#21644;&#30495;&#23454;&#25968;&#25454;&#30340;&#20272;&#35745;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#24378;&#22823;&#24615;&#33021;&#21644;&#26222;&#36866;&#24615;&#12290;</title><link>http://arxiv.org/abs/2111.03950</link><description>&lt;p&gt;
&#24207;&#36143;&#26680;&#23884;&#20837;&#29992;&#20110;&#20171;&#23548;&#21644;&#26102;&#21464;&#21058;&#37327;&#21709;&#24212;&#26354;&#32447;
&lt;/p&gt;
&lt;p&gt;
Sequential Kernel Embedding for Mediated and Time-Varying Dose Response Curves. (arXiv:2111.03950v4 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.03950
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#23725;&#22238;&#24402;&#30340;&#31616;&#21333;&#38750;&#21442;&#25968;&#20272;&#35745;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#20272;&#35745;&#20171;&#23548;&#21644;&#26102;&#21464;&#21058;&#37327;&#21709;&#24212;&#26354;&#32447;&#12290;&#36890;&#36807;&#24341;&#20837;&#24207;&#36143;&#26680;&#23884;&#20837;&#25216;&#26415;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#23545;&#22797;&#26434;&#22240;&#26524;&#20272;&#35745;&#30340;&#31616;&#21270;&#12290;&#36890;&#36807;&#27169;&#25311;&#23454;&#39564;&#21644;&#30495;&#23454;&#25968;&#25454;&#30340;&#20272;&#35745;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#24378;&#22823;&#24615;&#33021;&#21644;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#26680;&#23725;&#22238;&#24402;&#30340;&#20171;&#23548;&#21644;&#26102;&#21464;&#21058;&#37327;&#21709;&#24212;&#26354;&#32447;&#30340;&#31616;&#21333;&#38750;&#21442;&#25968;&#20272;&#35745;&#22120;&#12290;&#36890;&#36807;&#23884;&#20837;Pearl&#30340;&#20171;&#23548;&#20844;&#24335;&#21644;Robins&#30340;g&#20844;&#24335;&#19982;&#26680;&#20989;&#25968;&#65292;&#25105;&#20204;&#20801;&#35768;&#22788;&#29702;&#12289;&#20171;&#23548;&#32773;&#21644;&#21327;&#21464;&#37327;&#22312;&#19968;&#33324;&#31354;&#38388;&#20013;&#36830;&#32493;&#21464;&#21270;&#65292;&#20063;&#20801;&#35768;&#38750;&#32447;&#24615;&#30340;&#22788;&#29702;-&#28151;&#28102;&#22240;&#32032;&#21453;&#39304;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#21019;&#26032;&#26159;&#19968;&#31181;&#31216;&#20026;&#24207;&#36143;&#26680;&#23884;&#20837;&#30340;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#25216;&#26415;&#65292;&#25105;&#20204;&#20351;&#29992;&#23427;&#26469;&#26500;&#24314;&#22797;&#26434;&#22240;&#26524;&#20272;&#35745;&#30340;&#31616;&#21333;&#20272;&#35745;&#22120;&#12290;&#25105;&#20204;&#30340;&#20272;&#35745;&#22120;&#20445;&#30041;&#20102;&#32463;&#20856;&#35782;&#21035;&#30340;&#26222;&#36866;&#24615;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#38750;&#28176;&#36827;&#22343;&#21248;&#25910;&#25947;&#36895;&#24230;&#12290;&#22312;&#20855;&#26377;&#35768;&#22810;&#21327;&#21464;&#37327;&#30340;&#38750;&#32447;&#24615;&#27169;&#25311;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#20272;&#35745;&#20102;&#32654;&#22269;&#32844;&#19994;&#35757;&#32451;&#22242;&#30340;&#20171;&#23548;&#21644;&#26102;&#21464;&#21058;&#37327;&#21709;&#24212;&#26354;&#32447;&#65292;&#24182;&#28165;&#27905;&#21487;&#33021;&#25104;&#20026;&#26410;&#26469;&#24037;&#20316;&#22522;&#20934;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#32467;&#26524;&#25512;&#24191;&#21040;&#20171;&#23548;&#21644;&#26102;&#21464;&#22788;&#29702;&#25928;&#24212;&#20197;&#21450;&#21453;&#20107;&#23454;&#20998;&#24067;&#65292;&#39564;&#35777;&#20102;&#21322;&#21442;&#25968;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose simple nonparametric estimators for mediated and time-varying dose response curves based on kernel ridge regression. By embedding Pearl's mediation formula and Robins' g-formula with kernels, we allow treatments, mediators, and covariates to be continuous in general spaces, and also allow for nonlinear treatment-confounder feedback. Our key innovation is a reproducing kernel Hilbert space technique called sequential kernel embedding, which we use to construct simple estimators for complex causal estimands. Our estimators preserve the generality of classic identification while also achieving nonasymptotic uniform rates. In nonlinear simulations with many covariates, we demonstrate strong performance. We estimate mediated and time-varying dose response curves of the US Job Corps, and clean data that may serve as a benchmark in future work. We extend our results to mediated and time-varying treatment effects and counterfactual distributions, verifying semiparametric efficiency 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#27880;&#24847;&#26426;&#21046;&#36827;&#34892;&#39640;&#38454;&#24352;&#37327;&#27744;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#29305;&#24449;&#20540;&#24130;&#24402;&#19968;&#21270;&#65288;EPN&#65289;&#26469;&#38450;&#27490;&#29190;&#21457;&#29616;&#35937;&#24182;&#25552;&#39640;&#21160;&#20316;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2110.05216</link><description>&lt;p&gt;
&#21033;&#29992;&#27880;&#24847;&#26426;&#21046;&#30340;&#39640;&#38454;&#24352;&#37327;&#27744;&#21270;&#22312;&#21160;&#20316;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
High-order Tensor Pooling with Attention for Action Recognition. (arXiv:2110.05216v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.05216
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#27880;&#24847;&#26426;&#21046;&#36827;&#34892;&#39640;&#38454;&#24352;&#37327;&#27744;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#29305;&#24449;&#20540;&#24130;&#24402;&#19968;&#21270;&#65288;EPN&#65289;&#26469;&#38450;&#27490;&#29190;&#21457;&#29616;&#35937;&#24182;&#25552;&#39640;&#21160;&#20316;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#27880;&#24847;&#26426;&#21046;&#36827;&#34892;&#39640;&#38454;&#24352;&#37327;&#27744;&#21270;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#25429;&#25417;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#30340;&#29305;&#24449;&#21521;&#37327;&#30340;&#39640;&#38454;&#32479;&#35745;&#20449;&#24687;&#65292;&#24418;&#25104;&#24352;&#37327;&#25551;&#36848;&#31526;&#12290;&#24352;&#37327;&#25551;&#36848;&#31526;&#35201;&#27714;&#20855;&#22791;&#40065;&#26834;&#30340;&#30456;&#20284;&#24615;&#24230;&#37327;&#65292;&#20197;&#24212;&#23545;&#32858;&#21512;&#21521;&#37327;&#25968;&#37327;&#36739;&#23569;&#21644;&#29190;&#21457;&#29616;&#35937;&#65292;&#21363;&#26576;&#20123;&#29305;&#24449;&#20986;&#29616;&#30340;&#39057;&#29575;&#39640;&#20110;&#25110;&#20302;&#20110;&#32479;&#35745;&#39044;&#26399;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#23558;&#22270;&#25289;&#26222;&#25289;&#26031;&#30697;&#38453;&#19978;&#30340;&#28909;&#25193;&#25955;&#36807;&#31243;&#19982;&#21327;&#26041;&#24046;/&#33258;&#30456;&#20851;&#30697;&#38453;&#30340;&#29305;&#24449;&#20540;&#24130;&#24402;&#19968;&#21270;&#65288;EPN&#65289;&#23494;&#20999;&#30456;&#20851;&#65292;&#20854;&#36870;&#24418;&#25104;&#20102;&#19968;&#20010;&#29615;&#29366;&#22270;&#25289;&#26222;&#25289;&#26031;&#30697;&#38453;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#28909;&#25193;&#25955;&#36807;&#31243;&#19982;EPN&#20855;&#26377;&#30456;&#21516;&#30340;&#20316;&#29992;&#65292;&#21363;&#22686;&#24378;&#25110;&#20943;&#24369;&#29305;&#24449;&#20540;&#35889;&#30340;&#24133;&#24230;&#65292;&#20174;&#32780;&#38450;&#27490;&#29190;&#21457;&#29616;&#35937;&#12290;&#25105;&#20204;&#23558;&#39640;&#38454;&#24352;&#37327;&#37197;&#22791;&#20102;EPN&#65292;&#23427;&#21487;&#20197;&#20316;&#20026;&#39640;&#38454;&#20986;&#29616;&#30340;&#35889;&#26816;&#27979;&#22120;&#65292;&#20197;&#38450;&#27490;&#29190;&#21457;&#29616;&#35937;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#65292;&#23545;&#20110;&#19968;&#20010;&#30001;d&#32500;&#29305;&#24449;&#25551;&#36848;&#31526;&#26500;&#24314;&#30340;&#38454;&#25968;&#20026;r&#30340;&#24352;&#37327;&#65292;&#36825;&#26679;&#30340;&#26816;&#27979;&#22120;&#21487;&#20197;&#32473;&#20986;&#33267;&#23569;&#23384;&#22312;&#19968;&#20010;&#39640;&#38454;&#20986;&#29616;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We aim at capturing high-order statistics of feature vectors formed by a neural network, and propose end-to-end second- and higher-order pooling to form a tensor descriptor. Tensor descriptors require a robust similarity measure due to low numbers of aggregated vectors and the burstiness phenomenon, when a given feature appears more/less frequently than statistically expected. The Heat Diffusion Process (HDP) on a graph Laplacian is closely related to the Eigenvalue Power Normalization (EPN) of the covariance/auto-correlation matrix, whose inverse forms a loopy graph Laplacian. We show that the HDP and the EPN play the same role, i.e., to boost or dampen the magnitude of the eigenspectrum thus preventing the burstiness. We equip higher-order tensors with EPN which acts as a spectral detector of higher-order occurrences to prevent burstiness. We also prove that for a tensor of order r built from d dimensional feature descriptors, such a detector gives the likelihood if at least one high
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#25506;&#32034;&#26041;&#27861;&#20197;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#22870;&#21169;&#31232;&#23569;&#26102;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#39640;&#20445;&#30495;&#24230;&#30340;&#24037;&#19994;&#32423;&#27169;&#25311;&#22120;&#19979;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30456;&#27604;&#29616;&#26377;&#31639;&#27861;&#26377;&#24456;&#22823;&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2109.12509</link><description>&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#30340;&#28145;&#24230;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Deep Exploration for Recommendation Systems. (arXiv:2109.12509v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.12509
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#25506;&#32034;&#26041;&#27861;&#20197;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#22870;&#21169;&#31232;&#23569;&#26102;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#39640;&#20445;&#30495;&#24230;&#30340;&#24037;&#19994;&#32423;&#27169;&#25311;&#22120;&#19979;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30456;&#27604;&#29616;&#26377;&#31639;&#27861;&#26377;&#24456;&#22823;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#24212;&#20174;&#24310;&#36831;&#21453;&#39304;&#20013;&#25506;&#32034;&#21644;&#23398;&#20064;&#12290;&#36807;&#21435;&#30340;&#30740;&#31350;&#24448;&#24448;&#20391;&#37325;&#20110;&#20174;&#29992;&#25143;&#23545;&#21333;&#20010;&#25512;&#33616;&#30340;&#21709;&#24212;&#20013;&#23398;&#20064;&#12290;&#36825;&#20123;&#24037;&#20316;&#21033;&#29992;&#20102;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20294;&#25918;&#24323;&#20102;&#23398;&#20064;&#29992;&#25143;&#20043;&#21518;&#30340;&#34892;&#20026;&#12290;&#22312;&#36807;&#21435;&#30340;&#24037;&#20316;&#20013;&#65292;&#34429;&#28982;&#33268;&#21147;&#20110;&#20174;&#38543;&#21518;&#30340;&#34892;&#20026;&#20013;&#23398;&#20064;&#65292;&#20294;&#32570;&#20047;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#24341;&#23548;&#24182;&#33719;&#21462;&#26377;&#24847;&#20041;&#30340;&#24310;&#36831;&#21453;&#39304;&#12290;&#24403;&#22870;&#21169;&#36739;&#23569;&#26102;&#65292;&#36890;&#36807;&#24341;&#23548;&#25506;&#32034;&#26377;&#24847;&#20041;&#30340;&#24310;&#36831;&#21453;&#39304;&#21464;&#24471;&#29305;&#21035;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20026;&#25512;&#33616;&#31995;&#32479;&#24320;&#21457;&#20102;&#28145;&#24230;&#25506;&#32034;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#25512;&#33616;&#31995;&#32479;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#24207;&#21015;&#20915;&#31574;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#28145;&#24230;&#25506;&#32034;&#26041;&#27861;&#22312;&#21333;&#27493;&#25506;&#32034;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#26159;&#22312;&#39640;&#20445;&#30495;&#24230;&#30340;&#24037;&#19994;&#32423;&#27169;&#25311;&#22120;&#19979;&#36827;&#34892;&#30340;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30456;&#27604;&#29616;&#26377;&#31639;&#27861;&#26377;&#24456;&#22823;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern recommendation systems ought to benefit by probing for and learning from delayed feedback. Research has tended to focus on learning from a user's response to a single recommendation. Such work, which leverages methods of supervised and bandit learning, forgoes learning from the user's subsequent behavior. Where past work has aimed to learn from subsequent behavior, there has been a lack of effective methods for probing to elicit informative delayed feedback. Effective exploration through probing for delayed feedback becomes particularly challenging when rewards are sparse. To address this, we develop deep exploration methods for recommendation systems. In particular, we formulate recommendation as a sequential decision problem and demonstrate benefits of deep exploration over single-step exploration. Our experiments are carried out with high-fidelity industrial-grade simulators and establish large improvements over existing algorithms.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#32972;&#26223;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65288;ACB&#65289;&#65292;&#21487;&#20197;&#22312;&#19981;&#30693;&#36947;&#30495;&#23454;&#27169;&#22411;&#31867;&#21035;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#27169;&#22411;&#36873;&#25321;&#65292;&#24182;&#19988;&#20855;&#26377;&#19982;&#24050;&#30693;&#27169;&#22411;&#31867;&#21035;&#31639;&#27861;&#30456;&#21305;&#37197;&#30340;&#36951;&#25022;&#29575;&#12290;&#27169;&#22411;&#36873;&#25321;&#30340;&#20195;&#20215;&#20165;&#23545;&#36951;&#25022;&#19978;&#30028;&#30340;&#20108;&#38454;&#39033;&#26377;&#36129;&#29486;&#65292;&#19988;&#20855;&#26377;&#30452;&#35266;&#30340;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2107.03455</link><description>&lt;p&gt;
&#36890;&#29992;&#32972;&#26223;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Model Selection for Generic Contextual Bandits. (arXiv:2107.03455v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.03455
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#32972;&#26223;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65288;ACB&#65289;&#65292;&#21487;&#20197;&#22312;&#19981;&#30693;&#36947;&#30495;&#23454;&#27169;&#22411;&#31867;&#21035;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#27169;&#22411;&#36873;&#25321;&#65292;&#24182;&#19988;&#20855;&#26377;&#19982;&#24050;&#30693;&#27169;&#22411;&#31867;&#21035;&#31639;&#27861;&#30456;&#21305;&#37197;&#30340;&#36951;&#25022;&#29575;&#12290;&#27169;&#22411;&#36873;&#25321;&#30340;&#20195;&#20215;&#20165;&#23545;&#36951;&#25022;&#19978;&#30028;&#30340;&#20108;&#38454;&#39033;&#26377;&#36129;&#29486;&#65292;&#19988;&#20855;&#26377;&#30452;&#35266;&#30340;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#21487;&#23454;&#29616;&#24615;&#20551;&#35774;&#19979;&#30340;&#36890;&#29992;&#38543;&#26426;&#32972;&#26223;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#36873;&#25321;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33258;&#36866;&#24212;&#32972;&#26223;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#65288;ACB&#65289;&#30340;&#22522;&#20110;&#36830;&#32493;&#31934;&#28860;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20998;&#20026;&#22810;&#20010;&#38454;&#27573;&#65292;&#36880;&#27493;&#28040;&#38500;&#37027;&#20123;&#23545;&#32473;&#23450;&#23454;&#20363;&#26469;&#35828;&#36807;&#20110;&#31616;&#21333;&#30340;&#27169;&#22411;&#31867;&#21035;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#26159;&#33258;&#36866;&#24212;&#30340;&#65292;&#21363;&#22312;&#20219;&#20309;&#21487;&#35777;&#26126;&#30340;&#32972;&#26223;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65288;&#20363;&#22914;\cite{falcon}&#65289;&#30340;&#36951;&#25022;&#29575;&#19982;&#39034;&#24207;&#19968;&#33268;&#21305;&#37197;&#65292;&#21069;&#25552;&#26159;&#38656;&#35201;&#30693;&#36947;&#30495;&#23454;&#30340;&#27169;&#22411;&#31867;&#21035;&#12290;&#19981;&#30693;&#36947;&#27491;&#30830;&#30340;&#27169;&#22411;&#31867;&#21035;&#30340;&#20195;&#20215;&#23454;&#38469;&#19978;&#21482;&#26159;&#23548;&#33268;&#36951;&#25022;&#19978;&#30028;&#20013;&#30340;&#20108;&#38454;&#39033;&#30340;&#38468;&#21152;&#39033;&#12290;&#36825;&#20010;&#20195;&#20215;&#20855;&#26377;&#30452;&#35266;&#30340;&#23646;&#24615;&#65292;&#24403;&#27169;&#22411;&#31867;&#21035;&#21464;&#24471;&#26356;&#23481;&#26131;&#35782;&#21035;&#26102;&#65292;&#23427;&#21464;&#23567;&#65292;&#21453;&#20043;&#20134;&#28982;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#19968;&#31181;&#26356;&#31616;&#21333;&#30340;&#25506;&#32034;-&#21033;&#29992;&#65288;ETC&#65289;&#39118;&#26684;&#30340;&#31639;&#27861;&#20063;&#33021;&#33719;&#24471;&#31867;&#20284;&#30340;&#36951;&#25022;&#19978;&#30028;&#65292;&#23613;&#31649;&#19981;&#30693;&#36947;&#30495;&#23454;&#30340;&#27169;&#22411;&#31867;&#21035;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#36873;&#25321;&#30340;&#20195;&#20215;&#26159;...
&lt;/p&gt;
&lt;p&gt;
We consider the problem of model selection for the general stochastic contextual bandits under the realizability assumption. We propose a successive refinement based algorithm called Adaptive Contextual Bandit ({\ttfamily ACB}), that works in phases and successively eliminates model classes that are too simple to fit the given instance. We prove that this algorithm is adaptive, i.e., the regret rate order-wise matches that of any provable contextual bandit algorithm (ex. \cite{falcon}), that needs the knowledge of the true model class. The price of not knowing the correct model class turns out to be only an additive term contributing to the second order term in the regret bound. This cost possess the intuitive property that it becomes smaller as the model class becomes easier to identify, and vice-versa. We also show that a much simpler explore-then-commit (ETC) style algorithm also obtains similar regret bound, despite not knowing the true model class. However, the cost of model selec
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#22823;&#22810;&#25968;&#26631;&#20934;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#22312;&#21021;&#22987;&#21270;&#26102;&#21482;&#26377;&#19968;&#20010;&#31283;&#23450;&#24179;&#34913;&#28857;&#65292;&#24182;&#19988;&#23398;&#20064;&#38271;&#26102;&#38388;&#20381;&#36182;&#20219;&#21153;&#36890;&#24120;&#35201;&#27714;&#32593;&#32476;&#20855;&#26377;&#22810;&#31283;&#23450;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#28909;&#36523;&#8221;&#30340;&#21021;&#22987;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#32593;&#32476;&#21487;&#36798;&#21040;&#30340;&#22810;&#31283;&#23450;&#24615;&#26469;&#25913;&#21892;&#23398;&#20064;&#33021;&#21147;&#12290;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2106.01001</link><description>&lt;p&gt;
&#35753;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#28909;&#36523;&#20197;&#26368;&#22823;&#21270;&#21487;&#36798;&#21040;&#30340;&#22810;&#31283;&#23450;&#24615;&#26497;&#22823;&#25913;&#21892;&#20102;&#23398;&#20064;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Warming up recurrent neural networks to maximise reachable multistability greatly improves learning. (arXiv:2106.01001v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.01001
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#22823;&#22810;&#25968;&#26631;&#20934;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#22312;&#21021;&#22987;&#21270;&#26102;&#21482;&#26377;&#19968;&#20010;&#31283;&#23450;&#24179;&#34913;&#28857;&#65292;&#24182;&#19988;&#23398;&#20064;&#38271;&#26102;&#38388;&#20381;&#36182;&#20219;&#21153;&#36890;&#24120;&#35201;&#27714;&#32593;&#32476;&#20855;&#26377;&#22810;&#31283;&#23450;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#28909;&#36523;&#8221;&#30340;&#21021;&#22987;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#32593;&#32476;&#21487;&#36798;&#21040;&#30340;&#22810;&#31283;&#23450;&#24615;&#26469;&#25913;&#21892;&#23398;&#20064;&#33021;&#21147;&#12290;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#30693;&#22312;&#26102;&#38388;&#20381;&#36182;&#21464;&#38271;&#26102;&#65292;&#35757;&#32451;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#26159;&#22256;&#38590;&#30340;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22823;&#22810;&#25968;&#26631;&#20934;&#21333;&#20803;&#22312;&#21021;&#22987;&#21270;&#26102;&#21482;&#26377;&#19968;&#20010;&#31283;&#23450;&#30340;&#24179;&#34913;&#28857;&#65292;&#24182;&#19988;&#23398;&#20064;&#38271;&#26102;&#38388;&#20381;&#36182;&#20219;&#21153;&#36890;&#24120;&#21457;&#29983;&#22312;&#32593;&#32476;&#31283;&#23450;&#30340;&#24179;&#34913;&#28857;&#25968;&#37327;&#22686;&#21152;&#30340;&#26102;&#20505;&#65292;&#36825;&#20010;&#24615;&#36136;&#34987;&#31216;&#20026;&#22810;&#31283;&#23450;&#24615;&#12290;&#21021;&#22987;&#20026;&#21333;&#31283;&#23450;&#30340;&#32593;&#32476;&#36890;&#24120;&#38590;&#20197;&#36798;&#21040;&#22810;&#31283;&#23450;&#24615;&#65292;&#20351;&#24471;&#23398;&#20064;&#36755;&#20837;&#21644;&#36755;&#20986;&#20043;&#38388;&#30340;&#38271;&#26102;&#38388;&#20381;&#36182;&#21464;&#24471;&#22256;&#38590;&#12290;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#23548;&#33268;&#20102;&#19968;&#31181;&#26032;&#30340;&#21021;&#22987;&#21270;&#26041;&#27861;&#65292;&#31216;&#20026;&#8220;&#28909;&#36523;&#8221;&#65292;&#29992;&#20110;&#25913;&#21892;&#20219;&#20309;&#24490;&#29615;&#21333;&#20803;&#36830;&#25509;&#30340;&#23398;&#20064;&#33021;&#21147;&#65292;&#20197;&#23398;&#20064;&#20219;&#24847;&#38271;&#30340;&#26102;&#38388;&#20381;&#36182;&#12290;&#36825;&#20010;&#21021;&#22987;&#21270;&#36807;&#31243;&#26088;&#22312;&#36890;&#36807;&#23569;&#37327;&#26799;&#24230;&#27493;&#39588;&#20869;&#26368;&#22823;&#21270;&#32593;&#32476;&#21487;&#36798;&#21040;&#30340;&#22810;&#31283;&#23450;&#24615;&#65292;&#21363;&#32593;&#32476;&#20869;&#21487;&#20197;&#36890;&#36807;&#30456;&#20851;&#36755;&#20837;&#36712;&#36857;&#21040;&#36798;&#30340;&#24179;&#34913;&#28857;&#30340;&#25968;&#37327;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#20449;&#24687;&#24674;&#22797;&#12289;&#24207;&#21015;&#20998;&#31867;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#36825;&#19968;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training recurrent neural networks is known to be difficult when time dependencies become long. In this work, we show that most standard cells only have one stable equilibrium at initialisation, and that learning on tasks with long time dependencies generally occurs once the number of network stable equilibria increases; a property known as multistability. Multistability is often not easily attained by initially monostable networks, making learning of long time dependencies between inputs and outputs difficult. This insight leads to the design of a novel way to initialise any recurrent cell connectivity through a procedure called "warmup" to improve its capability to learn arbitrarily long time dependencies. This initialisation procedure is designed to maximise network reachable multistability, i.e., the number of equilibria within the network that can be reached through relevant input trajectories, in few gradient steps. We show on several information restitution, sequence classificat
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;AirNet&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#35757;&#32451;&#21644;&#20256;&#36755;&#26041;&#27861;&#65292;&#20801;&#35768;&#22312;&#26080;&#32447;&#20449;&#36947;&#19978;&#39640;&#25928;&#22320;&#20256;&#36755;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#65292;&#24182;&#19988;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2105.11166</link><description>&lt;p&gt;
AirNet&#65306;&#36890;&#36807;&#31354;&#20013;&#20256;&#36755;&#30340;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
AirNet: Neural Network Transmission over the Air. (arXiv:2105.11166v6 [cs.NI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.11166
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;AirNet&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#35757;&#32451;&#21644;&#20256;&#36755;&#26041;&#27861;&#65292;&#20801;&#35768;&#22312;&#26080;&#32447;&#20449;&#36947;&#19978;&#39640;&#25928;&#22320;&#20256;&#36755;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#65292;&#24182;&#19988;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#35768;&#22810;&#36793;&#32536;&#24212;&#29992;&#26469;&#35828;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;DNNs&#26159;&#19982;&#20301;&#32622;&#21644;&#26102;&#38388;&#26377;&#20851;&#30340;&#65292;&#24182;&#19988;&#24517;&#39035;&#22312;&#26080;&#32447;&#20449;&#36947;&#19978;&#24555;&#36895;&#32780;&#39640;&#25928;&#22320;&#20256;&#36755;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;AirNet&#65292;&#19968;&#31995;&#21015;&#26032;&#39062;&#30340;&#35757;&#32451;&#21644;&#20256;&#36755;&#26041;&#27861;&#65292;&#20801;&#35768;&#22312;&#20005;&#26684;&#30340;&#21457;&#23556;&#21151;&#29575;&#21644;&#24310;&#36831;&#32422;&#26463;&#19979;&#65292;&#39640;&#25928;&#22320;&#20256;&#36755;DNNs&#12290;&#36825;&#23545;&#24212;&#20110;&#19968;&#31867;&#26032;&#30340;&#32852;&#21512;&#28304;&#20449;&#36947;&#32534;&#30721;&#38382;&#39064;&#65292;&#26088;&#22312;&#36890;&#36807;&#26368;&#22823;&#21270;&#25509;&#25910;&#22120;&#19978;&#30340;&#20934;&#30830;&#24615;&#26469;&#20256;&#36882;DNNs&#65292;&#32780;&#19981;&#26159;&#20197;&#39640;&#20445;&#30495;&#24230;&#24674;&#22797;&#23427;&#20204;&#12290;&#22312;AirNet&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;DNN&#21442;&#25968;&#30452;&#25509;&#26144;&#23556;&#21040;&#20256;&#36755;&#30340;&#20449;&#36947;&#31526;&#21495;&#65292;&#21516;&#26102;&#35757;&#32451;&#32593;&#32476;&#20197;&#28385;&#36275;&#20449;&#36947;&#32422;&#26463;&#65292;&#24182;&#23637;&#29616;&#23545;&#20449;&#36947;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#12290;&#19982;&#22522;&#20110;&#20998;&#31163;&#30340;&#26367;&#20195;&#26041;&#26696;&#30456;&#27604;&#65292;AirNet&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#32593;&#32476;&#36827;&#34892;&#20462;&#21098;&#36827;&#19968;&#27493;&#25913;&#21892;&#20102;AirNet&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art performance for many edge applications is achieved by deep neural networks (DNNs). Often, these DNNs are location- and time-sensitive, and must be delivered over a wireless channel rapidly and efficiently. In this paper, we introduce AirNet, a family of novel training and transmission methods that allow DNNs to be efficiently delivered over wireless channels under stringent transmit power and latency constraints. This corresponds to a new class of joint source-channel coding problems, aimed at delivering DNNs with the goal of maximizing their accuracy at the receiver, rather than recovering them with high fidelity. In AirNet, we propose the direct mapping of the DNN parameters to transmitted channel symbols, while the network is trained to meet the channel constraints, and exhibit robustness against channel noise. AirNet achieves higher accuracy compared to separation-based alternatives. We further improve the performance of AirNet by pruning the network below the avai
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20013;&#20301;&#25968;&#27714;&#21644;&#21407;&#21017;&#30340;PCA&#26041;&#27861;&#65292;&#31216;&#20026;&#20013;&#20301;&#25968;&#27714;&#21644;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;MoMPCA&#65289;&#65292;&#20197;&#24212;&#23545;&#24322;&#24120;&#20540;&#23545;PCA&#30340;&#24433;&#21709;&#65292;&#24182;&#22312;&#26368;&#23567;&#20551;&#35774;&#26465;&#20214;&#19979;&#23454;&#29616;&#20102;&#26368;&#20248;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2102.03403</link><description>&lt;p&gt;
&#40065;&#26834;&#20027;&#25104;&#20998;&#20998;&#26512;&#65306;&#19968;&#31181;&#20013;&#20301;&#25968;&#27714;&#21644;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Robust Principal Component Analysis: A Median of Means Approach. (arXiv:2102.03403v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2102.03403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20013;&#20301;&#25968;&#27714;&#21644;&#21407;&#21017;&#30340;PCA&#26041;&#27861;&#65292;&#31216;&#20026;&#20013;&#20301;&#25968;&#27714;&#21644;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;MoMPCA&#65289;&#65292;&#20197;&#24212;&#23545;&#24322;&#24120;&#20540;&#23545;PCA&#30340;&#24433;&#21709;&#65292;&#24182;&#22312;&#26368;&#23567;&#20551;&#35774;&#26465;&#20214;&#19979;&#23454;&#29616;&#20102;&#26368;&#20248;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#26159;&#25968;&#25454;&#21487;&#35270;&#21270;&#12289;&#21435;&#22122;&#21644;&#38477;&#32500;&#30340;&#22522;&#26412;&#24037;&#20855;&#12290;&#23427;&#22312;&#32479;&#35745;&#23398;&#12289;&#26426;&#22120;&#23398;&#20064;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#31561;&#39046;&#22495;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;PCA&#23481;&#26131;&#21463;&#21040;&#24322;&#24120;&#20540;&#30340;&#24433;&#21709;&#65292;&#24448;&#24448;&#26080;&#27861;&#26816;&#27979;&#21040;&#25968;&#25454;&#38598;&#20013;&#30495;&#23454;&#30340;&#20302;&#32500;&#32467;&#26500;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20013;&#20301;&#25968;&#27714;&#21644;&#21407;&#21017;&#30340;PCA&#26041;&#27861;&#65292;&#31216;&#20026;&#20013;&#20301;&#25968;&#27714;&#21644;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;MoMPCA&#65289;&#12290;&#35813;&#26041;&#27861;&#19981;&#20165;&#22312;&#35745;&#31639;&#19978;&#20855;&#26377;&#21560;&#24341;&#21147;&#65292;&#32780;&#19988;&#22312;&#26368;&#23567;&#20551;&#35774;&#26465;&#20214;&#19979;&#23454;&#29616;&#20102;&#26368;&#20248;&#25910;&#25947;&#36895;&#24230;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#36890;&#36807;Rademacher&#22797;&#26434;&#24230;&#26469;&#25506;&#32034;&#25152;&#24471;&#35299;&#30340;&#38750;&#28176;&#36817;&#35823;&#24046;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
Principal Component Analysis (PCA) is a fundamental tool for data visualization, denoising, and dimensionality reduction. It is widely popular in Statistics, Machine Learning, Computer Vision, and related fields. However, PCA is well-known to fall prey to outliers and often fails to detect the true underlying low-dimensional structure within the dataset. Following the Median of Means (MoM) philosophy, recent supervised learning methods have shown great success in dealing with outlying observations without much compromise to their large sample theoretical properties. This paper proposes a PCA procedure based on the MoM principle. Called the \textbf{M}edian of \textbf{M}eans \textbf{P}rincipal \textbf{C}omponent \textbf{A}nalysis (MoMPCA), the proposed method is not only computationally appealing but also achieves optimal convergence rates under minimal assumptions. In particular, we explore the non-asymptotic error bounds of the obtained solution via the aid of the Rademacher complexiti
&lt;/p&gt;</description></item><item><title>&#24863;&#30693;&#22120;&#29702;&#35770;&#21487;&#20197;&#36890;&#36807;&#32479;&#35745;&#26041;&#27861;&#20934;&#30830;&#39044;&#27979;&#19981;&#21516;&#32467;&#26500;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2012.07881</link><description>&lt;p&gt;
&#24863;&#30693;&#22120;&#29702;&#35770;&#21487;&#20197;&#39044;&#27979;&#31070;&#32463;&#32593;&#32476;&#30340;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Perceptron Theory Can Predict the Accuracy of Neural Networks. (arXiv:2012.07881v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2012.07881
&lt;/p&gt;
&lt;p&gt;
&#24863;&#30693;&#22120;&#29702;&#35770;&#21487;&#20197;&#36890;&#36807;&#32479;&#35745;&#26041;&#27861;&#20934;&#30830;&#39044;&#27979;&#19981;&#21516;&#32467;&#26500;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#23618;&#31070;&#32463;&#32593;&#32476;&#22312;&#35768;&#22810;&#25216;&#26415;&#20998;&#31867;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#24403;&#21069;&#30340;&#26368;&#26032;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#20174;&#20998;&#26512;&#21644;&#39044;&#27979;&#24615;&#33021;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#20123;&#32593;&#32476;&#20173;&#28982;&#26159;&#40657;&#31665;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#38024;&#23545;&#21333;&#23618;&#24863;&#30693;&#22120;&#30340;&#32479;&#35745;&#29702;&#35770;&#65292;&#24182;&#23637;&#31034;&#23427;&#21487;&#20197;&#39044;&#27979;&#22810;&#31181;&#19981;&#21516;&#32467;&#26500;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#25512;&#24191;&#29992;&#20110;&#20998;&#26512;&#20648;&#22791;&#35745;&#31639;&#27169;&#22411;&#21644;&#31526;&#21495;&#25512;&#29702;&#30340;&#36830;&#25509;&#20027;&#20041;&#27169;&#22411;&#30340;&#21521;&#37327;&#31526;&#21495;&#20307;&#31995;&#32467;&#26500;&#30340;&#29616;&#26377;&#29702;&#35770;&#65292;&#25105;&#20204;&#21457;&#23637;&#20102;&#24863;&#30693;&#22120;&#20998;&#31867;&#30340;&#19968;&#33324;&#29702;&#35770;&#12290;&#25105;&#20204;&#30340;&#32479;&#35745;&#29702;&#35770;&#25552;&#20379;&#20102;&#19977;&#20010;&#20844;&#24335;&#65292;&#36890;&#36807;&#36880;&#27493;&#22686;&#21152;&#35814;&#32454;&#20449;&#24687;&#26469;&#21033;&#29992;&#20449;&#21495;&#32479;&#35745;&#12290;&#36825;&#20123;&#20844;&#24335;&#22312;&#35299;&#26512;&#19978;&#26159;&#38590;&#20197;&#22788;&#29702;&#30340;&#65292;&#20294;&#21487;&#20197;&#36890;&#36807;&#25968;&#20540;&#35780;&#20272;&#12290;&#25429;&#25417;&#26368;&#35814;&#32454;&#20449;&#24687;&#30340;&#25551;&#36848;&#32423;&#21035;&#38656;&#35201;&#38543;&#26426;&#25277;&#26679;&#26041;&#27861;&#12290;&#26681;&#25454;&#32593;&#32476;&#27169;&#22411;&#30340;&#19981;&#21516;&#65292;&#36739;&#31616;&#21333;&#30340;&#20844;&#24335;&#24050;&#32463;&#33021;&#22815;&#25552;&#20379;&#39640;&#24230;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multilayer neural networks set the current state of the art for many technical classification problems. But, these networks are still, essentially, black boxes in terms of analyzing them and predicting their performance. Here, we develop a statistical theory for the one-layer perceptron and show that it can predict performances of a surprisingly large variety of neural networks with different architectures. A general theory of classification with perceptrons is developed by generalizing an existing theory for analyzing reservoir computing models and connectionist models for symbolic reasoning known as vector symbolic architectures. Our statistical theory offers three formulas leveraging the signal statistics with increasing detail. The formulas are analytically intractable, but can be evaluated numerically. The description level that captures maximum details requires stochastic sampling methods. Depending on the network model, the simpler formulas already yield high prediction accuracy
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38544;&#24335;&#20989;&#25968;&#23548;&#25968;&#30340;&#21487;&#35270;&#21270;&#26041;&#27861;&#65292;&#26088;&#22312;&#29702;&#35299;&#22810;&#32500;&#25237;&#24433;&#23545;&#23616;&#37096;&#23376;&#31354;&#38388;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#20998;&#26512;&#23616;&#37096;&#23376;&#31354;&#38388;&#30340;&#24418;&#29366;&#21644;&#26041;&#21521;&#20449;&#24687;&#65292;&#25105;&#20204;&#33021;&#22815;&#33719;&#21462;&#26356;&#22810;&#20851;&#20110;&#25968;&#25454;&#20840;&#23616;&#32467;&#26500;&#30340;&#27934;&#23519;&#21147;&#65292;&#24182;&#23558;&#32467;&#26524;&#21487;&#35270;&#21270;&#20026;&#22270;&#24418;&#36827;&#34892;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2009.03259</link><description>&lt;p&gt;
&#38544;&#24335;&#22810;&#32500;&#25237;&#24433;&#23616;&#37096;&#23376;&#31354;&#38388;&#30340;&#21487;&#35270;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Implicit Multidimensional Projection of Local Subspaces. (arXiv:2009.03259v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2009.03259
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38544;&#24335;&#20989;&#25968;&#23548;&#25968;&#30340;&#21487;&#35270;&#21270;&#26041;&#27861;&#65292;&#26088;&#22312;&#29702;&#35299;&#22810;&#32500;&#25237;&#24433;&#23545;&#23616;&#37096;&#23376;&#31354;&#38388;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#20998;&#26512;&#23616;&#37096;&#23376;&#31354;&#38388;&#30340;&#24418;&#29366;&#21644;&#26041;&#21521;&#20449;&#24687;&#65292;&#25105;&#20204;&#33021;&#22815;&#33719;&#21462;&#26356;&#22810;&#20851;&#20110;&#25968;&#25454;&#20840;&#23616;&#32467;&#26500;&#30340;&#27934;&#23519;&#21147;&#65292;&#24182;&#23558;&#32467;&#26524;&#21487;&#35270;&#21270;&#20026;&#22270;&#24418;&#36827;&#34892;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38544;&#24335;&#20989;&#25968;&#23548;&#25968;&#30340;&#21487;&#35270;&#21270;&#26041;&#27861;&#65292;&#26469;&#29702;&#35299;&#22810;&#32500;&#25237;&#24433;&#23545;&#23616;&#37096;&#23376;&#31354;&#38388;&#30340;&#24433;&#21709;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23558;&#23616;&#37096;&#23376;&#31354;&#38388;&#29702;&#35299;&#20026;&#25968;&#25454;&#28857;&#30340;&#22810;&#32500;&#23616;&#37096;&#37051;&#22495;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#22810;&#32500;&#25968;&#25454;&#28857;&#30340;&#25237;&#24433;&#65292;&#24573;&#30053;&#20102;&#37051;&#22495;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#20998;&#26512;&#23616;&#37096;&#23376;&#31354;&#38388;&#30340;&#24418;&#29366;&#21644;&#26041;&#21521;&#20449;&#24687;&#65292;&#36890;&#36807;&#24863;&#30693;&#23616;&#37096;&#32467;&#26500;&#26469;&#33719;&#21462;&#26356;&#22810;&#26377;&#20851;&#25968;&#25454;&#20840;&#23616;&#32467;&#26500;&#30340;&#27934;&#23519;&#21147;&#12290;&#23616;&#37096;&#23376;&#31354;&#38388;&#36890;&#36807;&#30001;&#22522;&#21521;&#37327;&#24352;&#25104;&#30340;&#22810;&#32500;&#26925;&#22278;&#25311;&#21512;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20934;&#30830;&#39640;&#25928;&#30340;&#21521;&#37327;&#36716;&#25442;&#26041;&#27861;&#65292;&#22522;&#20110;&#23558;&#22810;&#32500;&#25237;&#24433;&#34920;&#31034;&#20026;&#38544;&#24335;&#20989;&#25968;&#30340;&#35299;&#26512;&#24494;&#20998;&#12290;&#32467;&#26524;&#20197;&#22270;&#24418;&#30340;&#24418;&#24335;&#36827;&#34892;&#21487;&#35270;&#21270;&#65292;&#24182;&#21033;&#29992;&#19968;&#22871;&#23436;&#25972;&#30340;&#19987;&#38376;&#35774;&#35745;&#30340;&#20132;&#20114;&#25805;&#20316;&#65292;&#22312;&#25105;&#20204;&#39640;&#25928;&#30340;&#22522;&#20110;Web&#30340;&#21487;&#35270;&#21270;&#24037;&#20855;&#20013;&#36827;&#34892;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a visualization method to understand the effect of multidimensional projection on local subspaces, using implicit function differentiation. Here, we understand the local subspace as the multidimensional local neighborhood of data points. Existing methods focus on the projection of multidimensional data points, and the neighborhood information is ignored. Our method is able to analyze the shape and directional information of the local subspace to gain more insights into the global structure of the data through the perception of local structures. Local subspaces are fitted by multidimensional ellipses that are spanned by basis vectors. An accurate and efficient vector transformation method is proposed based on analytical differentiation of multidimensional projections formulated as implicit functions. The results are visualized as glyphs and analyzed using a full set of specifically-designed interactions supported in our efficient web-based visualization tool. The usefulness o
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20915;&#31574;&#26641;&#26041;&#27861;&#36873;&#25321;&#21512;&#36866;&#20013;&#24515;&#24230;&#24230;&#37327;&#26041;&#27861;&#30340;&#32553;&#20943;&#26041;&#27861;&#12290;&#36890;&#36807;&#26500;&#24314;&#20915;&#31574;&#26641;&#35843;&#26597;&#65292;&#32467;&#21512;&#19987;&#23478;&#20559;&#22909;&#65292;&#33021;&#22815;&#22312;&#36739;&#23567;&#30340;&#22270;&#24418;&#25968;&#37327;&#19979;&#24555;&#36895;&#31579;&#36873;&#20986;&#26368;&#21512;&#36866;&#30340;&#20013;&#24515;&#24230;&#24230;&#37327;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2003.01052</link><description>&lt;p&gt;
&#22914;&#20309;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#20013;&#24515;&#24230;&#27979;&#37327;&#26041;&#27861;&#65311;&#19968;&#31181;&#20915;&#31574;&#26641;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
How to choose the most appropriate centrality measure? A decision tree approach. (arXiv:2003.01052v5 [physics.soc-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2003.01052
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20915;&#31574;&#26641;&#26041;&#27861;&#36873;&#25321;&#21512;&#36866;&#20013;&#24515;&#24230;&#24230;&#37327;&#26041;&#27861;&#30340;&#32553;&#20943;&#26041;&#27861;&#12290;&#36890;&#36807;&#26500;&#24314;&#20915;&#31574;&#26641;&#35843;&#26597;&#65292;&#32467;&#21512;&#19987;&#23478;&#20559;&#22909;&#65292;&#33021;&#22815;&#22312;&#36739;&#23567;&#30340;&#22270;&#24418;&#25968;&#37327;&#19979;&#24555;&#36895;&#31579;&#36873;&#20986;&#26368;&#21512;&#36866;&#30340;&#20013;&#24515;&#24230;&#24230;&#37327;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20013;&#24515;&#24230;&#24230;&#37327;&#22312;&#32593;&#32476;&#20998;&#26512;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#22312;400&#22810;&#31181;&#25552;&#20986;&#30340;&#25351;&#26631;&#20013;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#27979;&#37327;&#26041;&#27861;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29616;&#26377;&#26041;&#27861;-&#22522;&#20110;&#27169;&#22411;&#12289;&#25968;&#25454;&#39537;&#21160;&#21644;&#20844;&#29702;&#24615;-&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#32553;&#20943;&#26041;&#27861;&#65292;&#21033;&#29992;&#19987;&#23478;&#23545;&#31616;&#21333;&#22270;&#20013;&#30340;&#20013;&#24515;&#24230;&#34892;&#20026;&#30340;&#20559;&#22909;&#12290;&#23427;&#28041;&#21450;&#24418;&#25104;&#19968;&#32452;&#20505;&#36873;&#27979;&#37327;&#26041;&#27861;&#65292;&#29983;&#25104;&#23613;&#21487;&#33021;&#23567;&#30340;&#22270;&#24418;&#20197;&#8220;&#20998;&#31163;&#8221;&#21508;&#31181;&#27979;&#37327;&#26041;&#27861;&#65292;&#26500;&#24314;&#20915;&#31574;&#26641;&#35843;&#26597;&#65292;&#24182;&#30830;&#23450;&#19982;&#19987;&#23478;&#22238;&#31572;&#19968;&#33268;&#30340;&#27979;&#37327;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#21253;&#25324;&#26032;&#30340;&#22522;&#20110;&#26680;&#30340;&#27979;&#37327;&#26041;&#27861;&#22312;&#20869;&#30340;40&#31181;&#19981;&#21516;&#20013;&#24515;&#24230;&#65292;&#21516;&#26102;&#19982;&#20844;&#29702;&#24615;&#26041;&#27861;&#32467;&#21512;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#21482;&#26377;13&#20010;&#23567;&#22411;1-&#26641;&#36275;&#20197;&#20998;&#31163;&#25152;&#26377;40&#20010;&#27979;&#37327;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;&#25509;&#36817;&#30340;&#19968;&#23545;&#12290;&#32553;&#20943;&#26041;&#27861;&#22312;&#21171;&#21160;&#21147;&#21644;&#26102;&#38388;&#26041;&#38754;&#25552;&#20379;&#20102;&#19968;&#31181;&#20302;&#25104;&#26412;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20026;&#24050;&#26377;&#30340;&#27979;&#37327;&#26041;&#27861;&#25552;&#20379;&#20102;&#34917;&#20805;&#12290;
&lt;/p&gt;
&lt;p&gt;
Centrality metrics are vital for network analysis, but selecting the most appropriate measures for specific applications remains challenging among the 400+ proposed indices. Existing approaches -- model-based, data-driven, and axiomatic -- have limitations. To address this, we introduce the culling method, leveraging expert preferences regarding centrality behavior on simple graphs. It involves forming a set of candidate measures, generating a list of as small graphs as possible needed to ``separate'' measures from each other, constructing a decision-tree survey, and identifying the measure consistent with expert responses. We apply this method to a diverse set of 40 centralities, including new kernel-based measures, and combine it with the axiomatic approach. Remarkably, only 13 small 1-trees suffice to separate all 40 measures, among which there are pairs of close ones. The culling method offers a low-cost solution in terms of labor and time, complements existing methods for measure 
&lt;/p&gt;</description></item></channel></rss>