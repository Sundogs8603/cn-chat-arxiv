<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SECAD-Net&#30340;&#33258;&#25105;&#30417;&#30563;&#31070;&#32463;&#32593;&#32476;&#65292;&#26088;&#22312;&#20197;&#32039;&#20945;&#19988;&#26131;&#20110;&#32534;&#36753;&#30340;&#26041;&#24335;&#20174;&#21407;&#22987;&#36755;&#20837;&#20013;&#37325;&#24314;&#39640;&#36136;&#37327;&#30340;CAD&#27169;&#22411;&#12290;&#20854;&#21033;&#29992;&#23398;&#20064;2D&#33609;&#22270;&#21644;3D&#25380;&#21387;&#21442;&#25968;&#20197;&#21019;&#24314;&#19968;&#32452;&#25380;&#21387;&#31570;&#65292;&#24182;&#32467;&#21512;&#24067;&#23572;&#25805;&#20316;&#26469;&#36924;&#36817;&#30446;&#26631;&#20960;&#20309;&#24418;&#29366;&#12290;&#20351;&#29992;&#38544;&#24335;&#22330;&#34920;&#31034;&#33609;&#22270;&#65292;&#20801;&#35768;&#36890;&#36807;&#22312;&#33609;&#22270;&#28508;&#22312;&#31354;&#38388;&#20013;&#25554;&#20540;&#28508;&#22312;&#20195;&#30721;&#26469;&#21019;&#24314;CAD&#21464;&#20307;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;SECAD-Net&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#37325;&#24314;&#36136;&#37327;&#21644;&#25512;&#26029;&#36895;&#24230;&#26041;&#38754;&#30340;&#21331;&#36234;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.10613</link><description>&lt;p&gt;
SECAD-Net&#65306;&#36890;&#36807;&#23398;&#20064;&#33609;&#22270;&#25380;&#21387;&#25805;&#20316;&#36827;&#34892;&#33258;&#25105;&#30417;&#30563;&#30340;CAD&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
SECAD-Net: Self-Supervised CAD Reconstruction by Learning Sketch-Extrude Operations. (arXiv:2303.10613v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10613
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SECAD-Net&#30340;&#33258;&#25105;&#30417;&#30563;&#31070;&#32463;&#32593;&#32476;&#65292;&#26088;&#22312;&#20197;&#32039;&#20945;&#19988;&#26131;&#20110;&#32534;&#36753;&#30340;&#26041;&#24335;&#20174;&#21407;&#22987;&#36755;&#20837;&#20013;&#37325;&#24314;&#39640;&#36136;&#37327;&#30340;CAD&#27169;&#22411;&#12290;&#20854;&#21033;&#29992;&#23398;&#20064;2D&#33609;&#22270;&#21644;3D&#25380;&#21387;&#21442;&#25968;&#20197;&#21019;&#24314;&#19968;&#32452;&#25380;&#21387;&#31570;&#65292;&#24182;&#32467;&#21512;&#24067;&#23572;&#25805;&#20316;&#26469;&#36924;&#36817;&#30446;&#26631;&#20960;&#20309;&#24418;&#29366;&#12290;&#20351;&#29992;&#38544;&#24335;&#22330;&#34920;&#31034;&#33609;&#22270;&#65292;&#20801;&#35768;&#36890;&#36807;&#22312;&#33609;&#22270;&#28508;&#22312;&#31354;&#38388;&#20013;&#25554;&#20540;&#28508;&#22312;&#20195;&#30721;&#26469;&#21019;&#24314;CAD&#21464;&#20307;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;SECAD-Net&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#37325;&#24314;&#36136;&#37327;&#21644;&#25512;&#26029;&#36895;&#24230;&#26041;&#38754;&#30340;&#21331;&#36234;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#21407;&#22987;&#20960;&#20309;&#24418;&#29366;&#20013;&#21453;&#21521;&#24037;&#31243;CAD&#27169;&#22411;&#26159;&#19968;&#20010;&#32463;&#20856;&#20294;&#22256;&#38590;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#20197;&#24448;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#30001;&#20110;&#21463;&#30417;&#30563;&#30340;&#35774;&#35745;&#27169;&#24335;&#25110;&#37325;&#24314;&#19981;&#26131;&#32534;&#36753;&#30340;CAD&#24418;&#29366;&#32780;&#22823;&#37327;&#20381;&#36182;&#26631;&#31614;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;SECAD-Net&#65292;&#36825;&#26159;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#26088;&#22312;&#20197;&#33258;&#25105;&#30417;&#30563;&#30340;&#26041;&#24335;&#37325;&#24314;&#32039;&#20945;&#19988;&#26131;&#20110;&#32534;&#36753;&#30340;CAD&#27169;&#22411;&#12290;&#20174;&#29616;&#20195;CAD&#36719;&#20214;&#20013;&#26368;&#24120;&#20351;&#29992;&#30340;&#24314;&#27169;&#35821;&#35328;&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#25105;&#20204;&#25552;&#20986;&#20174;&#21407;&#22987;&#24418;&#29366;&#20013;&#23398;&#20064;2D&#33609;&#22270;&#21644;3D&#25380;&#21387;&#21442;&#25968;&#65292;&#20174;&#32780;&#36890;&#36807;&#23558;&#27599;&#20010;&#33609;&#22270;&#20174;2D&#24179;&#38754;&#25380;&#21387;&#21040;3D&#20307;&#26469;&#29983;&#25104;&#19968;&#32452;&#25380;&#21387;&#31570;&#12290;&#36890;&#36807;&#34701;&#21512;&#24067;&#23572;&#25805;&#20316;&#65288;&#21363;&#32852;&#21512;&#65289;&#65292;&#36825;&#20123;&#31570;&#21487;&#20197;&#32452;&#21512;&#22312;&#19968;&#36215;&#65292;&#20197;&#23494;&#20999;&#36924;&#36817;&#30446;&#26631;&#20960;&#20309;&#24418;&#29366;&#12290;&#25105;&#20204;&#20513;&#23548;&#20351;&#29992;&#38544;&#24335;&#22330;&#26469;&#34920;&#31034;&#33609;&#22270;&#65292;&#36825;&#20801;&#35768;&#36890;&#36807;&#22312;&#33609;&#22270;&#28508;&#22312;&#31354;&#38388;&#20013;&#25554;&#20540;&#28508;&#22312;&#20195;&#30721;&#26469;&#21019;&#24314;CAD&#21464;&#20307;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SECAD-Net&#21487;&#20197;&#20174;&#21407;&#22987;&#36755;&#20837;&#20013;&#37325;&#24314;&#39640;&#36136;&#37327;&#30340;CAD&#27169;&#22411;&#65292;&#22312;&#37325;&#24314;&#36136;&#37327;&#21644;&#25512;&#26029;&#36895;&#24230;&#26041;&#38754;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reverse engineering CAD models from raw geometry is a classic but strenuous research problem. Previous learning-based methods rely heavily on labels due to the supervised design patterns or reconstruct CAD shapes that are not easily editable. In this work, we introduce SECAD-Net, an end-to-end neural network aimed at reconstructing compact and easy-to-edit CAD models in a self-supervised manner. Drawing inspiration from the modeling language that is most commonly used in modern CAD software, we propose to learn 2D sketches and 3D extrusion parameters from raw shapes, from which a set of extrusion cylinders can be generated by extruding each sketch from a 2D plane into a 3D body. By incorporating the Boolean operation (i.e., union), these cylinders can be combined to closely approximate the target geometry. We advocate the use of implicit fields for sketch representation, which allows for creating CAD variations by interpolating latent codes in the sketch latent space. Extensive experim
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;T5 Transformer&#27169;&#22411;&#25104;&#21151;&#22320;&#22312;&#23391;&#21152;&#25289;&#35821;&#20013;&#26816;&#27979;&#35821;&#27861;&#38169;&#35823;&#65292;&#24182;&#23545;&#27169;&#22411;&#26816;&#27979;&#21040;&#30340;&#38169;&#35823;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#21516;&#26102;&#25506;&#35752;&#20102;&#23558;&#32763;&#35793;&#27169;&#22411;&#36866;&#24212;&#35821;&#27861;&#26816;&#27979;&#20219;&#21153;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2303.10612</link><description>&lt;p&gt;
&#20351;&#29992;T5 Transformer&#27169;&#22411;&#30340;&#23391;&#21152;&#25289;&#35821;&#35821;&#27861;&#38169;&#35823;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Bangla Grammatical Error Detection Using T5 Transformer Model. (arXiv:2303.10612v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10612
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;T5 Transformer&#27169;&#22411;&#25104;&#21151;&#22320;&#22312;&#23391;&#21152;&#25289;&#35821;&#20013;&#26816;&#27979;&#35821;&#27861;&#38169;&#35823;&#65292;&#24182;&#23545;&#27169;&#22411;&#26816;&#27979;&#21040;&#30340;&#38169;&#35823;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#21516;&#26102;&#25506;&#35752;&#20102;&#23558;&#32763;&#35793;&#27169;&#22411;&#36866;&#24212;&#35821;&#27861;&#26816;&#27979;&#20219;&#21153;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;&#25991;&#26412;&#30340;&#36716;&#25442;&#21464;&#21387;&#22120;&#65288;T5&#65289;&#35821;&#35328;&#27169;&#22411;&#26816;&#27979;&#23391;&#21152;&#25289;&#35821;&#35821;&#27861;&#38169;&#35823;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#31934;&#32454;&#35843;&#25972;&#36807;&#30340;BanglaT5&#30340;&#23567;&#21464;&#31181;&#65292;&#22312;&#19968;&#20010;&#21253;&#21547;9385&#20010;&#21477;&#23376;&#30340;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20854;&#20013;&#38169;&#35823;&#34987;&#19987;&#29992;&#20998;&#30028;&#31526;&#25324;&#36215;&#26469;&#12290;T5&#27169;&#22411;&#20027;&#35201;&#35774;&#35745;&#29992;&#20110;&#32763;&#35793;&#65292;&#32780;&#19981;&#26159;&#29305;&#21035;&#20026;&#36825;&#20010;&#20219;&#21153;&#35774;&#35745;&#30340;&#65292;&#22240;&#27492;&#38656;&#35201;&#36827;&#34892;&#24191;&#27867;&#30340;&#21518;&#22788;&#29702;&#26469;&#20351;&#20854;&#36866;&#24212;&#38169;&#35823;&#26816;&#27979;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;T5&#27169;&#22411;&#22312;&#26816;&#27979;&#23391;&#21152;&#25289;&#35821;&#35821;&#27861;&#38169;&#35823;&#26102;&#21487;&#20197;&#23454;&#29616;&#36739;&#20302;&#30340;Levenshtein&#36317;&#31163;&#65292;&#20294;&#26159;&#24517;&#39035;&#36827;&#34892;&#21518;&#22788;&#29702;&#25165;&#33021;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#12290;&#22312;&#23545;&#31934;&#32454;&#35843;&#25972;&#30340;&#27169;&#22411;&#36755;&#20986;&#36827;&#34892;&#21518;&#22788;&#29702;&#21518;&#65292;&#26368;&#32456;&#27979;&#35797;&#38598;&#30340;&#24179;&#22343;Levenshtein&#36317;&#31163;&#20026;1.0394&#12290;&#26412;&#25991;&#36824;&#23545;&#27169;&#22411;&#26816;&#27979;&#21040;&#30340;&#38169;&#35823;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#24182;&#35752;&#35770;&#20102;&#23558;&#32763;&#35793;&#27169;&#22411;&#36866;&#24212;&#35821;&#27861;&#26816;&#27979;&#20219;&#21153;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25193;&#23637;&#21040;&#20854;&#20182;&#35821;&#35328;&#65292;&#36825;&#22312;&#25104;&#21151;&#26816;&#27979;&#23391;&#21152;&#25289;&#35821;&#35821;&#27861;&#38169;&#35823;&#26102;&#24471;&#21040;&#20102;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a method for detecting grammatical errors in Bangla using a Text-to-Text Transfer Transformer (T5) Language Model, using the small variant of BanglaT5, fine-tuned on a corpus of 9385 sentences where errors were bracketed by the dedicated demarcation symbol. The T5 model was primarily designed for translation and is not specifically designed for this task, so extensive post-processing was necessary to adapt it to the task of error detection. Our experiments show that the T5 model can achieve low Levenshtein Distance in detecting grammatical errors in Bangla, but post-processing is essential to achieve optimal performance. The final average Levenshtein Distance after post-processing the output of the fine-tuned model was 1.0394 on a test set of 5000 sentences. This paper also presents a detailed analysis of the errors detected by the model and discusses the challenges of adapting a translation model for grammar. Our approach can be extended to other languages, demonst
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#19982;&#20256;&#32479;&#22522;&#20110;&#25968;&#23398;&#25968;&#25454;&#29983;&#25104;&#27169;&#22411;&#30340;&#20449;&#21495;&#22788;&#29702;&#26041;&#27861;&#30340;&#25968;&#25454;&#38656;&#27714;&#24046;&#24322;&#65292;&#24182;&#22312;&#20004;&#20010;&#31616;&#21333;&#30340;&#38382;&#39064;&#19978;&#24471;&#21040;&#20102;&#23454;&#35777;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.10608</link><description>&lt;p&gt;
&#19968;&#31181;&#27169;&#22411;&#32988;&#36807;&#19978;&#19975;&#20010;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;
A model is worth tens of thousands of examples. (arXiv:2303.10608v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10608
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#19982;&#20256;&#32479;&#22522;&#20110;&#25968;&#23398;&#25968;&#25454;&#29983;&#25104;&#27169;&#22411;&#30340;&#20449;&#21495;&#22788;&#29702;&#26041;&#27861;&#30340;&#25968;&#25454;&#38656;&#27714;&#24046;&#24322;&#65292;&#24182;&#22312;&#20004;&#20010;&#31616;&#21333;&#30340;&#38382;&#39064;&#19978;&#24471;&#21040;&#20102;&#23454;&#35777;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#22522;&#20110;&#25968;&#23398;&#25968;&#25454;&#29983;&#25104;&#27169;&#22411;&#30340;&#20449;&#21495;&#22788;&#29702;&#26041;&#27861;&#24050;&#34987;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25152;&#21462;&#20195;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#12290;&#30001;&#20110;&#29702;&#35770;&#26679;&#26412;&#22797;&#26434;&#24230;&#20960;&#20046;&#26080;&#27861;&#35780;&#20272;&#65292;&#36825;&#20123;&#26679;&#26412;&#37327;&#36890;&#24120;&#20351;&#29992;&#31895;&#30053;&#30340;&#32463;&#39564;&#27861;&#21017;&#36827;&#34892;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35268;&#21017;&#20165;&#24314;&#35758;&#32593;&#32476;&#24212;&#35813;&#20309;&#26102;&#36215;&#20316;&#29992;&#65292;&#20294;&#24182;&#19981;&#28041;&#21450;&#20256;&#32479;&#26041;&#27861;&#12290;&#29305;&#21035;&#26159;&#65292;&#19968;&#20010;&#26377;&#36259;&#30340;&#38382;&#39064;&#26159;&#65306;&#31070;&#32463;&#32593;&#32476;&#38656;&#35201;&#22810;&#23569;&#25968;&#25454;&#25165;&#33021;&#19982;&#20256;&#32479;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#20445;&#25345;&#21516;&#27493;&#25110;&#32773;&#65292;&#22914;&#26524;&#21487;&#33021;&#30340;&#35805;&#65292;&#36229;&#36234;&#23427;&#20204;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#31616;&#21333;&#30340;&#31034;&#20363;&#26469;&#20174;&#32463;&#39564;&#19978;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;&#25968;&#25454;&#26159;&#26681;&#25454;&#31934;&#30830;&#23450;&#20041;&#30340;&#25968;&#23398;&#27169;&#22411;&#29983;&#25104;&#30340;&#65292;&#24182;&#19988;&#24050;&#30693;&#26368;&#20248;&#25110;&#26368;&#20808;&#36827;&#30340;&#25968;&#23398;&#25968;&#25454;&#26080;&#20851;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional signal processing methods relying on mathematical data generation models have been cast aside in favour of deep neural networks, which require vast amounts of data. Since the theoretical sample complexity is nearly impossible to evaluate, these amounts of examples are usually estimated with crude rules of thumb. However, these rules only suggest when the networks should work, but do not relate to the traditional methods. In particular, an interesting question is: how much data is required for neural networks to be on par or outperform, if possible, the traditional model-based methods? In this work, we empirically investigate this question in two simple examples, where the data is generated according to precisely defined mathematical models, and where well-understood optimal or state-of-the-art mathematical data-agnostic solutions are known. A first problem is deconvolving one-dimensional Gaussian signals and a second one is estimating a circle's radius and location in rando
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#21644;&#34880;&#23481;&#31215;&#33033;&#25615;&#20449;&#21495;&#35774;&#35745;&#20102;&#19968;&#31181;&#38750;&#20405;&#20837;&#24335;&#30340;&#30140;&#30171;&#24863;&#30693;&#26041;&#24335;&#65292;&#24182;&#24314;&#31435;&#20102;&#27169;&#22411;&#26469;&#26816;&#27979;&#30140;&#30171;&#30340;&#23384;&#22312;&#21644;&#24378;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.10607</link><description>&lt;p&gt;
&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#20174;&#34880;&#23481;&#31215;&#33033;&#25615;&#20449;&#21495;&#33258;&#21160;&#35782;&#21035;&#30140;&#30171;
&lt;/p&gt;
&lt;p&gt;
Automatic pain recognition from Blood Volume Pulse (BVP) signal using machine learning techniques. (arXiv:2303.10607v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10607
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#21644;&#34880;&#23481;&#31215;&#33033;&#25615;&#20449;&#21495;&#35774;&#35745;&#20102;&#19968;&#31181;&#38750;&#20405;&#20837;&#24335;&#30340;&#30140;&#30171;&#24863;&#30693;&#26041;&#24335;&#65292;&#24182;&#24314;&#31435;&#20102;&#27169;&#22411;&#26469;&#26816;&#27979;&#30140;&#30171;&#30340;&#23384;&#22312;&#21644;&#24378;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#21457;&#23637;&#33258;&#21160;&#21270;&#30140;&#30171;&#35782;&#21035;&#24863;&#30693;&#31995;&#32479;&#65292;&#30740;&#31350;&#32773;&#36234;&#26469;&#36234;&#20851;&#27880;&#30140;&#30171;&#30340;&#29983;&#29702;&#21453;&#24212;&#12290;&#34429;&#28982;&#36739;&#23569;&#34987;&#25506;&#32034;&#65292;&#20294;&#34880;&#23481;&#31215;&#33033;&#25615;&#65288;BVP&#65289;&#26159;&#19968;&#20010;&#21487;&#33021;&#26377;&#21161;&#20110;&#23458;&#35266;&#30140;&#30171;&#35780;&#20272;&#30340;&#29983;&#29702;&#27979;&#37327;&#20505;&#36873;&#12290;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;BVP&#20449;&#21495;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#35774;&#24819;&#20102;&#19968;&#31181;&#38750;&#20405;&#20837;&#24335;&#30340;&#30140;&#30171;&#24863;&#30693;&#26041;&#24335;&#12290;32&#21517;&#20581;&#24247;&#21463;&#35797;&#32773;&#21442;&#19982;&#20102;&#26412;&#30740;&#31350;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#32452;&#26032;&#39062;&#30340;&#26102;&#38388;&#22495;&#12289;&#39057;&#29575;&#22495;&#21644;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#29305;&#24449;&#65292;&#36825;&#20123;&#29305;&#24449;&#28508;&#22312;&#22320;&#23545;&#30140;&#30171;&#20855;&#26377;&#25935;&#24863;&#24615;&#12290;&#36825;&#20123;&#21253;&#25324;&#26469;&#33258;BVP&#20449;&#21495;&#30340;24&#20010;&#29305;&#24449;&#21644;&#20174;&#21516;&#19968;BVP&#20449;&#21495;&#20013;&#23548;&#20986;&#30340;20&#20010;&#39069;&#22806;&#30340;&#24515;&#36339;&#38388;&#38548;&#65288;IBI&#65289;&#29305;&#24449;&#12290;&#21033;&#29992;&#36825;&#20123;&#29305;&#24449;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#26816;&#27979;&#30140;&#30171;&#30340;&#23384;&#22312;&#21644;&#20854;&#24378;&#24230;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21253;&#25324;&#36923;&#36753;&#22238;&#24402;&#12289;&#38543;&#26426;&#26862;&#26519;&#12289;&#25903;&#25345;&#21521;&#37327;&#26426;&#12289;&#33258;&#36866;&#24212;&#25552;&#21319;&#65288;AdaBoost&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physiological responses to pain have received increasing attention among researchers for developing an automated pain recognition sensing system. Though less explored, Blood Volume Pulse (BVP) is one of the candidate physiological measures that could help objective pain assessment. In this study, we applied machine learning techniques on BVP signals to device a non-invasive modality for pain sensing. Thirty-two healthy subjects participated in this study. First, we investigated a novel set of time-domain, frequency-domain and nonlinear dynamics features that could potentially be sensitive to pain. These include 24 features from BVP signals and 20 additional features from Inter-beat Intervals (IBIs) derived from the same BVP signals. Utilizing these features, we built machine learning models for detecting the presence of pain and its intensity. We explored different machine learning models, including Logistic Regression, Random Forest, Support Vector Machines, Adaptive Boosting (AdaBoos
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#27169;&#22411;&#36866;&#24212;&#20013;&#23384;&#22312;&#30340;&#36890;&#29992;&#25915;&#20987;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;AdaptGuard&#30340;&#27169;&#22411;&#39044;&#22788;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#21644;&#20266;&#23545;&#25239;&#26679;&#26412;&#21152;&#24378;&#30446;&#26631;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#25552;&#39640;&#27169;&#22411;&#36866;&#24212;&#31639;&#27861;&#30340;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.10594</link><description>&lt;p&gt;
AdaptGuard: &#38024;&#23545;&#27169;&#22411;&#36866;&#24212;&#20013;&#30340;&#36890;&#29992;&#25915;&#20987;&#36827;&#34892;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
AdaptGuard: Defending Against Universal Attacks for Model Adaptation. (arXiv:2303.10594v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10594
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#27169;&#22411;&#36866;&#24212;&#20013;&#23384;&#22312;&#30340;&#36890;&#29992;&#25915;&#20987;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;AdaptGuard&#30340;&#27169;&#22411;&#39044;&#22788;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#21644;&#20266;&#23545;&#25239;&#26679;&#26412;&#21152;&#24378;&#30446;&#26631;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#25552;&#39640;&#27169;&#22411;&#36866;&#24212;&#31639;&#27861;&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#36866;&#24212;&#26088;&#22312;&#35299;&#20915;&#22312;&#20165;&#35775;&#38382;&#24050;&#39044;&#35757;&#32451;&#28304;&#27169;&#22411;&#30340;&#32422;&#26463;&#19979;&#30340;&#22495;&#36716;&#31227;&#38382;&#39064;&#12290;&#38543;&#30528;&#23545;&#25968;&#25454;&#38544;&#31169;&#21644;&#20256;&#36755;&#25928;&#29575;&#30340;&#36234;&#26469;&#36234;&#22810;&#20851;&#27880;&#65292;&#36825;&#31181;&#33539;&#24335;&#36817;&#24180;&#26469;&#21464;&#24471;&#36234;&#26469;&#36234;&#27969;&#34892;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#27169;&#22411;&#36866;&#24212;&#31639;&#27861;&#20013;&#30001;&#20110;&#24694;&#24847;&#25552;&#20379;&#26041;&#30340;&#23384;&#22312;&#32780;&#36716;&#31227;&#33258;&#28304;&#22495;&#30340;&#36890;&#29992;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#36890;&#29992;&#23545;&#25239;&#25200;&#21160;&#21644;&#21518;&#38376;&#25915;&#20987;&#20316;&#20026;&#28304;&#20391;&#28431;&#27934;&#30340;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#22312;&#36866;&#24212;&#21518;&#30340;&#30446;&#26631;&#27169;&#22411;&#20013;&#20173;&#28982;&#23384;&#22312;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;AdaptGuard&#30340;&#27169;&#22411;&#39044;&#22788;&#29702;&#26694;&#26550;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#36866;&#24212;&#31639;&#27861;&#30340;&#23433;&#20840;&#24615;&#12290;AdaptGuard&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#36991;&#20813;&#30452;&#25509;&#20351;&#29992;&#39118;&#38505;&#28304;&#21442;&#25968;&#65292;&#24182;&#21033;&#29992;&#35843;&#25972;&#21322;&#24452;&#19979;&#30340;&#20266;&#23545;&#25239;&#26679;&#26412;&#26469;&#22686;&#24378;&#40065;&#26834;&#24615;&#12290;AdaptGuard&#26159;&#19968;&#20010;&#21363;&#25554;&#21363;&#29992;&#30340;&#27169;&#22359;&#65292;&#19981;&#38656;&#35201;&#20462;&#25913;&#29616;&#26377;&#30340;&#27169;&#22411;&#36866;&#24212;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model adaptation aims at solving the domain transfer problem under the constraint of only accessing the pretrained source models. With the increasing considerations of data privacy and transmission efficiency, this paradigm has been gaining recent popularity. This paper studies the vulnerability to universal attacks transferred from the source domain during model adaptation algorithms due to the existence of the malicious providers. We explore both universal adversarial perturbations and backdoor attacks as loopholes on the source side and discover that they still survive in the target models after adaptation. To address this issue, we propose a model preprocessing framework, named AdaptGuard, to improve the security of model adaptation algorithms. AdaptGuard avoids direct use of the risky source parameters through knowledge distillation and utilizes the pseudo adversarial samples under adjusted radius to enhance the robustness. AdaptGuard is a plug-and-play module that requires neithe
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#24040;&#22411;&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#32593;&#32476;&#20013;&#22788;&#29702;&#31227;&#21160;&#29992;&#25143;&#35774;&#22791;&#30340;&#24322;&#26500;&#24615;&#65292;&#24182;&#19988;&#23454;&#29616;&#20102;&#35757;&#32451;&#25439;&#22833;&#26368;&#23567;&#21270;&#21644;&#36718;&#24310;&#36831;&#38477;&#20302;&#30340;&#30446;&#26631;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2303.10580</link><description>&lt;p&gt;
&#24040;&#22411;&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#32593;&#32476;&#20013;&#30340;&#20998;&#23618;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Personalized Federated Learning Over Massive Mobile Edge Computing Networks. (arXiv:2303.10580v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10580
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#24040;&#22411;&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#32593;&#32476;&#20013;&#22788;&#29702;&#31227;&#21160;&#29992;&#25143;&#35774;&#22791;&#30340;&#24322;&#26500;&#24615;&#65292;&#24182;&#19988;&#23454;&#29616;&#20102;&#35757;&#32451;&#25439;&#22833;&#26368;&#23567;&#21270;&#21644;&#36718;&#24310;&#36831;&#38477;&#20302;&#30340;&#30446;&#26631;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;PFL&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#33539; paradigm&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#65288;MEC&#65289;&#32593;&#32476;&#20013;&#21508;&#31181;&#31227;&#21160;&#29992;&#25143;&#35774;&#22791;&#65288;UE&#65289;&#24102;&#26469;&#30340;&#24322;&#26500;&#38382;&#39064;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;UE&#25968;&#37327;&#19981;&#26029;&#22686;&#21152;&#20197;&#21450;&#24102;&#26469;&#30340;&#22797;&#26434;&#34892;&#25919;&#24037;&#20316;&#65292;&#23558;PFL&#31639;&#27861;&#20174;&#20854;&#20256;&#32479;&#30340;&#21452;&#23618;&#26694;&#26550;&#20999;&#25442;&#21040;&#22810;&#23618;&#26694;&#26550;&#26159;&#21487;&#34892;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20998;&#23618;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;HPFL&#65289;&#65292;&#19968;&#31181;&#29992;&#20110;&#22312;&#24040;&#22411;MEC&#32593;&#32476;&#20013;&#37096;&#32626;PFL&#30340;&#31639;&#27861;&#12290;&#22312;HPFL&#20013;&#65292;UE&#34987;&#21010;&#20998;&#20026;&#22810;&#20010;&#38598;&#32676;&#65292;&#27599;&#20010;&#38598;&#32676;&#20013;&#30340;UE&#21516;&#27493;&#23558;&#20854;&#26412;&#22320;&#26356;&#26032;&#36716;&#21457;&#21040;&#36793;&#32536;&#26381;&#21153;&#22120;&#65288;ES&#65289;&#36827;&#34892;&#36793;&#32536;&#27169;&#22411;&#32858;&#21512;&#65292;&#32780;ES&#21322;&#24322;&#27493;&#22320;&#23558;&#20854;&#36793;&#32536;&#27169;&#22411;&#36716;&#21457;&#21040;&#20113;&#26381;&#21153;&#22120;&#36827;&#34892;&#20840;&#23616;&#27169;&#22411;&#32858;&#21512;&#12290;&#19978;&#36848;&#35757;&#32451;&#26041;&#24335;&#22312;&#35757;&#32451;&#25439;&#22833;&#21644;&#36718;&#24310;&#36831;&#20043;&#38388;&#36798;&#21040;&#20102;&#19968;&#20010;&#26435;&#34913;&#12290;HPFL&#20197;&#20998;&#23618;&#26041;&#24335;&#32467;&#21512;&#20102;&#35757;&#32451;&#25439;&#22833;&#26368;&#23567;&#21270;&#21644;&#36718;&#24310;&#36831;&#38477;&#20302;&#30340;&#30446;&#26631;&#65292;&#20174;&#32780;&#22312;&#22521;&#35757;&#25928;&#29575;&#21644;&#36890;&#20449;&#24320;&#38144;&#26041;&#38754;&#27604;&#20256;&#32479;&#30340;PFL&#31639;&#27861;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalized Federated Learning (PFL) is a new Federated Learning (FL) paradigm, particularly tackling the heterogeneity issues brought by various mobile user equipments (UEs) in mobile edge computing (MEC) networks. However, due to the ever-increasing number of UEs and the complicated administrative work it brings, it is desirable to switch the PFL algorithm from its conventional two-layer framework to a multiple-layer one. In this paper, we propose hierarchical PFL (HPFL), an algorithm for deploying PFL over massive MEC networks. The UEs in HPFL are divided into multiple clusters, and the UEs in each cluster forward their local updates to the edge server (ES) synchronously for edge model aggregation, while the ESs forward their edge models to the cloud server semi-asynchronously for global model aggregation. The above training manner leads to a tradeoff between the training loss in each round and the round latency. HPFL combines the objectives of training loss minimization and round 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32467;&#26500;&#23884;&#20837;&#21644;&#39044;&#35745;&#31639;&#30340;&#26041;&#27861;&#65292;&#20197;&#20943;&#23569;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#65292;&#24182;&#23454;&#29616;&#20102;&#22312;&#23376;&#22270; GNN &#19978;&#39640;&#25928;&#35745;&#25968;&#23376;&#32467;&#26500;&#30340;&#30446;&#30340;&#12290;</title><link>http://arxiv.org/abs/2303.10576</link><description>&lt;p&gt;
&#19981;&#38656;&#35201;&#22312;&#23376;&#22270;&#19978;&#36816;&#34892; GNN&#65292;&#20351;&#29992;&#23376;&#22270; GNN &#39640;&#25928;&#35745;&#25968;&#23376;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Efficiently Counting Substructures by Subgraph GNNs without Running GNN on Subgraphs. (arXiv:2303.10576v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10576
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32467;&#26500;&#23884;&#20837;&#21644;&#39044;&#35745;&#31639;&#30340;&#26041;&#27861;&#65292;&#20197;&#20943;&#23569;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#65292;&#24182;&#23454;&#29616;&#20102;&#22312;&#23376;&#22270; GNN &#19978;&#39640;&#25928;&#35745;&#25968;&#23376;&#32467;&#26500;&#30340;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26469;&#65292;&#22312;&#22270;&#23398;&#20064;&#20013;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476; (GNN) &#26469;&#36817;&#20284;&#35745;&#31639;&#29305;&#23450;&#20989;&#25968;&#65292;&#22914;&#35745;&#25968;&#22270;&#30340;&#23376;&#32467;&#26500;&#65292;&#26159;&#19968;&#20010;&#28909;&#38376;&#36235;&#21183;&#12290;&#22312;&#36825;&#20123;&#24037;&#20316;&#20013;&#65292;&#19968;&#31181;&#24120;&#29992;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#23376;&#22270; GNN&#65292;&#23558;&#36755;&#20837;&#22270;&#20998;&#35299;&#20026;&#19968;&#31995;&#21015;&#23376;&#22270;&#65292;&#24182;&#36890;&#36807;&#23545;&#27599;&#20010;&#23376;&#22270;&#24212;&#29992; GNN &#26469;&#22686;&#24378;&#22270;&#30340;&#34920;&#31034;&#12290;&#23613;&#31649;&#23376;&#22270; GNN &#33021;&#22815;&#35745;&#25968;&#22797;&#26434;&#30340;&#23376;&#32467;&#26500;&#65292;&#20294;&#23427;&#20204;&#20250;&#36973;&#21463;&#39640;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#30340;&#22256;&#25200;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38750;&#24120;&#35268;&#30340;&#38382;&#39064;&#65306;&#25105;&#20204;&#26159;&#21542;&#33021;&#22815;&#20351;&#29992; GNN &#39640;&#25928;&#22320;&#35745;&#25968;&#23376;&#32467;&#26500;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#29702;&#35770;&#19978;&#35777;&#26126;&#65292;&#22312;&#23376;&#22270;&#20013;&#21040;&#26681;&#33410;&#28857;&#30340;&#36317;&#31163;&#26159;&#25552;&#39640;&#23376;&#22270; GNN &#35745;&#25968;&#33021;&#21147;&#30340;&#20851;&#38190;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#36825;&#31181;&#20449;&#24687;&#32534;&#30721;&#20026;&#32467;&#26500;&#23884;&#20837;&#65292;&#24182;&#39044;&#20808;&#35745;&#31639;&#36825;&#20123;&#23884;&#20837;&#65292;&#20197;&#36991;&#20813;&#36890;&#36807; GNN &#21453;&#22797;&#25552;&#21462;&#25152;&#26377;&#23376;&#22270;&#20013;&#30340;&#20449;&#24687;&#12290;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#21487;&#20197;&#20445;&#25345;&#23376;&#22270; GNN &#30340;&#35745;&#25968;&#33021;&#21147;&#65292;&#21516;&#26102;&#26174;&#33879;&#38477;&#20302;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Using graph neural networks (GNNs) to approximate specific functions such as counting graph substructures is a recent trend in graph learning. Among these works, a popular way is to use subgraph GNNs, which decompose the input graph into a collection of subgraphs and enhance the representation of the graph by applying GNN to individual subgraphs. Although subgraph GNNs are able to count complicated substructures, they suffer from high computational and memory costs. In this paper, we address a non-trivial question: can we count substructures efficiently with GNNs? To answer the question, we first theoretically show that the distance to the rooted nodes within subgraphs is key to boosting the counting power of subgraph GNNs. We then encode such information into structural embeddings, and precompute the embeddings to avoid extracting information over all subgraphs via GNNs repeatedly. Experiments on various benchmarks show that the proposed model can preserve the counting power of subgra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#20363;&#30456;&#20851;&#30340;&#30028;&#38480;&#65292;&#23450;&#20041;&#20102;&#19968;&#20010;&#28216;&#25103;&#30697;&#38453;&#25490;&#24207;&#65292;&#21487;&#20197;&#25429;&#25417;&#21040;&#26576;&#20123;&#28216;&#25103;&#21160;&#24577;&#27604;&#20854;&#20182;&#28216;&#25103;&#26356;&#24555;&#25910;&#25947;&#30340;&#30452;&#35273;&#65292;&#20174;&#32780;&#38480;&#23450;&#20102;&#29609;&#23478;&#36798;&#21040;&#36817;&#20284;&#22343;&#34913;&#25152;&#38656;&#30340;&#26368;&#23567;&#22238;&#21512;&#25968;&#12290;</title><link>http://arxiv.org/abs/2303.10565</link><description>&lt;p&gt;
&#38646;&#21644;&#30697;&#38453;&#21338;&#24328;&#20013;&#23454;&#20363;&#30456;&#20851;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Instance-dependent Sample Complexity Bounds for Zero-sum Matrix Games. (arXiv:2303.10565v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10565
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#20363;&#30456;&#20851;&#30340;&#30028;&#38480;&#65292;&#23450;&#20041;&#20102;&#19968;&#20010;&#28216;&#25103;&#30697;&#38453;&#25490;&#24207;&#65292;&#21487;&#20197;&#25429;&#25417;&#21040;&#26576;&#20123;&#28216;&#25103;&#21160;&#24577;&#27604;&#20854;&#20182;&#28216;&#25103;&#26356;&#24555;&#25910;&#25947;&#30340;&#30452;&#35273;&#65292;&#20174;&#32780;&#38480;&#23450;&#20102;&#29609;&#23478;&#36798;&#21040;&#36817;&#20284;&#22343;&#34913;&#25152;&#38656;&#30340;&#26368;&#23567;&#22238;&#21512;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35782;&#21035;&#20004;&#20154;&#38646;&#21644; $n\times 2$ &#30697;&#38453;&#21338;&#24328;&#36817;&#20284;&#22343;&#34913;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#21363;&#65292;&#22312;&#19968;&#31995;&#21015;&#37325;&#22797;&#21338;&#24328;&#20013;&#65292;&#20004;&#20301;&#29609;&#23478;&#24517;&#39035;&#36827;&#34892;&#22810;&#23569;&#22238;&#21512;&#25165;&#33021;&#36798;&#21040;&#36817;&#20284;&#22343;&#34913;&#65288;&#20363;&#22914; Nash &#24179;&#34913;&#65289;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#19968;&#31181;&#23454;&#20363;&#30456;&#20851;&#30340;&#30028;&#38480;&#65292;&#23450;&#20041;&#20102;&#19968;&#20010;&#28216;&#25103;&#30697;&#38453;&#25490;&#24207;&#65292;&#25429;&#25417;&#26576;&#20123;&#28216;&#25103;&#21160;&#24577;&#27604;&#20854;&#20182;&#28216;&#25103;&#26356;&#24555;&#25910;&#25947;&#30340;&#30452;&#35273;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;&#38543;&#26426;&#35266;&#23519;&#27169;&#22411;&#65292;&#20351;&#24471;&#24403;&#20004;&#20301;&#29609;&#23478;&#20998;&#21035;&#36873;&#25321;&#34892;&#21160; $i$ &#21644; $j$ &#26102;&#65292;&#20182;&#20204;&#37117;&#33021;&#35266;&#23519;&#21040;&#23545;&#26041;&#30340;&#34892;&#21160;&#20197;&#21450;&#38543;&#26426;&#35266;&#23519; $X_{ij}$&#65292;&#20351;&#24471; $\mathbb E[ X_{ij}] = A_{ij}$&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#39318;&#27425;&#20197;&#23454;&#20363;&#30456;&#20851;&#30340;&#26041;&#24335;&#32473;&#20986;&#20102;&#20004;&#20301;&#29609;&#23478;&#22312;&#36798;&#21040;&#36817;&#20284;&#22343;&#34913;&#20043;&#21069;&#24517;&#39035;&#29609;&#30340;&#22238;&#21512;&#25968;&#19979;&#38480;&#30340;&#24773;&#20917;&#65292;&#20854;&#20013;&#22238;&#21512;&#25968;&#21462;&#20915;&#20110;&#28216;&#25103;&#30697;&#38453; $A$ &#30340;&#29305;&#23450;&#23646;&#24615;&#20197;&#21450;&#25152;&#38656;&#30340;&#20934;&#30830;&#24230;&#12290;&#25105;&#20204;&#36824;p
&lt;/p&gt;
&lt;p&gt;
We study the sample complexity of identifying an approximate equilibrium for two-player zero-sum $n\times 2$ matrix games. That is, in a sequence of repeated game plays, how many rounds must the two players play before reaching an approximate equilibrium (e.g., Nash)? We derive instance-dependent bounds that define an ordering over game matrices that captures the intuition that the dynamics of some games converge faster than others. Specifically, we consider a stochastic observation model such that when the two players choose actions $i$ and $j$, respectively, they both observe each other's played actions and a stochastic observation $X_{ij}$ such that $\mathbb E[ X_{ij}] = A_{ij}$. To our knowledge, our work is the first case of instance-dependent lower bounds on the number of rounds the players must play before reaching an approximate equilibrium in the sense that the number of rounds depends on the specific properties of the game matrix $A$ as well as the desired accuracy. We also p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545; LiDAR &#27450;&#39575;&#25915;&#20987;&#22312;&#30446;&#26631;&#26816;&#27979;&#39046;&#22495;&#23384;&#22312;&#30340;&#30740;&#31350;&#24046;&#36317;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#20351;&#29992;9&#31181; LiDAR &#21644;3&#31181;&#30446;&#26631;&#26816;&#27979;&#22120;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#27979;&#37327;&#65292;&#23637;&#31034;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#27169;&#24335;&#65292;&#25552;&#20986;&#20102;&#23433;&#20840;&#30340; LiDAR &#35774;&#35745;&#21644;&#35780;&#20272;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.10555</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270; LiDAR &#27450;&#39575;&#25915;&#20987;&#23545;&#20110;&#30446;&#26631;&#26816;&#27979;&#30340;&#33021;&#21147;&#65306;&#25913;&#36827;&#12289;&#27979;&#37327;&#21644;&#26032;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Revisiting LiDAR Spoofing Attack Capabilities against Object Detection: Improvements, Measurement, and New Attack. (arXiv:2303.10555v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10555
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545; LiDAR &#27450;&#39575;&#25915;&#20987;&#22312;&#30446;&#26631;&#26816;&#27979;&#39046;&#22495;&#23384;&#22312;&#30340;&#30740;&#31350;&#24046;&#36317;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#20351;&#29992;9&#31181; LiDAR &#21644;3&#31181;&#30446;&#26631;&#26816;&#27979;&#22120;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#27979;&#37327;&#65292;&#23637;&#31034;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#27169;&#24335;&#65292;&#25552;&#20986;&#20102;&#23433;&#20840;&#30340; LiDAR &#35774;&#35745;&#21644;&#35780;&#20272;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LiDAR&#65288;&#20809;&#23398;&#36965;&#24863;&#65289;&#26159;&#36827;&#34892;&#31934;&#30830;&#38271;&#36317;&#31163;&#21644;&#23485;&#33539;&#22260; 3D &#24863;&#24212;&#30340;&#19981;&#21487;&#25110;&#32570;&#30340;&#20256;&#24863;&#22120;&#65292;&#30452;&#25509;&#36896;&#31119;&#20110;&#33258;&#21160;&#39550;&#39542;&#25216;&#26415;&#30340;&#24555;&#36895;&#25512;&#24191;&#12290;&#21516;&#26102;&#65292;&#36825;&#31181;&#23433;&#20840;&#20851;&#38190;&#30340;&#24212;&#29992;&#24378;&#28872;&#25512;&#21160;&#20102;&#23545;&#20854;&#23433;&#20840;&#24615;&#30340;&#30740;&#31350;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#21521; LiDAR &#21457;&#36865;&#24694;&#24847;&#28608;&#20809;&#26469;&#25805;&#32437; LiDAR &#28857;&#20113;&#24182;&#27450;&#39575;&#30446;&#26631;&#26816;&#27979;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#38754;&#20020;&#19977;&#20010;&#20851;&#38190;&#30340;&#30740;&#31350;&#38590;&#28857;&#65306;&#65288;1&#65289;&#20165;&#35780;&#20272;&#29305;&#23450;&#30340; LiDAR&#65288;VLP-16&#65289;&#65307;&#65288;2&#65289;&#20551;&#35774;&#25915;&#20987;&#33021;&#21147;&#26410;&#34987;&#39564;&#35777;&#65307;&#20197;&#21450;&#65288;3&#65289;&#35780;&#20272;&#21463;&#38480;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20123;&#20851;&#38190;&#30340;&#30740;&#31350;&#38590;&#28857;&#65292;&#25105;&#20204;&#23545;&#24635;&#20849;9&#31181;&#27969;&#34892;&#30340; LiDAR &#21644;3&#31181;&#20027;&#35201;&#31867;&#22411;&#30340;&#30446;&#26631;&#26816;&#27979;&#22120;&#36827;&#34892;&#20102;&#31532;&#19968;&#27425;&#22823;&#35268;&#27169;&#30340; LiDAR &#27450;&#39575;&#25915;&#20987;&#33021;&#21147;&#27979;&#37327;&#30740;&#31350;&#12290;&#20026;&#20102;&#36827;&#34892;&#36825;&#20010;&#27979;&#37327;&#65292;&#25105;&#20204;&#36890;&#36807;&#26356;&#21152;&#23567;&#24515;&#30340;&#20809;&#23398;&#21644;&#21151;&#33021;&#30005;&#23376;&#23398;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102; LiDAR &#27450;&#39575;&#33021;&#21147;&#65292;&#23637;&#31034;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#27169;&#24335;&#65292;&#21487;&#20197;&#29978;&#33267;&#35268;&#36991;&#26368;&#20808;&#36827;&#30340;&#38450;&#24481;&#26426;&#21046;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25552;&#20379;&#20102;&#20851;&#20110;&#22914;&#20309;&#22312;&#19981;&#21516;&#30340;&#23454;&#38469;&#35774;&#32622;&#20013;&#23454;&#38469;&#36827;&#34892; LiDAR &#27450;&#39575;&#25915;&#20987;&#30340;&#35265;&#35299;&#65292;&#24182;&#21628;&#21505;&#23433;&#20840;&#30340; LiDAR &#35774;&#35745;&#21644;&#35780;&#20272;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
LiDAR (Light Detection And Ranging) is an indispensable sensor for precise long- and wide-range 3D sensing, which directly benefited the recent rapid deployment of autonomous driving (AD). Meanwhile, such a safety-critical application strongly motivates its security research. A recent line of research demonstrates that one can manipulate the LiDAR point cloud and fool object detection by firing malicious lasers against LiDAR. However, these efforts face 3 critical research gaps: (1) evaluating only on a specific LiDAR (VLP-16); (2) assuming unvalidated attack capabilities; and (3) evaluating with models trained on limited datasets.  To fill these critical research gaps, we conduct the first large-scale measurement study on LiDAR spoofing attack capabilities on object detectors with 9 popular LiDARs in total and 3 major types of object detectors. To perform this measurement, we significantly improved the LiDAR spoofing capability with more careful optics and functional electronics, whic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24377;&#24615;&#30456;&#20114;&#20316;&#29992;&#33021;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#37319;&#29992;&#31283;&#23450;&#39033;&#35299;&#20915;&#20102;GAN&#35757;&#32451;&#19981;&#31283;&#23450;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#26144;&#23556;&#25968;&#25454;&#21040;&#29305;&#24449;&#31354;&#38388;&#36924;&#36817;&#20854;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2303.10553</link><description>&lt;p&gt;
&#22522;&#20110;&#24377;&#24615;&#30456;&#20114;&#20316;&#29992;&#33021;&#30340;&#29983;&#25104;&#27169;&#22411;&#65306;&#29305;&#24449;&#31354;&#38388;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
Elastic Interaction Energy-Based Generative Model: Approximation in Feature Space. (arXiv:2303.10553v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10553
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24377;&#24615;&#30456;&#20114;&#20316;&#29992;&#33021;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#37319;&#29992;&#31283;&#23450;&#39033;&#35299;&#20915;&#20102;GAN&#35757;&#32451;&#19981;&#31283;&#23450;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#26144;&#23556;&#25968;&#25454;&#21040;&#29305;&#24449;&#31354;&#38388;&#36924;&#36817;&#20854;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#65292;&#20351;&#29992;&#22522;&#20110;&#24377;&#24615;&#30456;&#20114;&#20316;&#29992;&#33021;&#65288;EIE&#65289;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#21463;&#26230;&#20307;&#32570;&#38519;&#20043;&#38388;&#30340;&#24377;&#24615;&#30456;&#20114;&#20316;&#29992;&#21551;&#21457;&#12290;&#21033;&#29992;&#22522;&#20110;EIE&#30340;&#24230;&#37327;&#20855;&#26377;&#38271;&#31243;&#24615;&#36136;&#65292;&#33021;&#22815;&#32771;&#34385;&#20998;&#24067;&#20013;&#30340;&#20840;&#23616;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#23427;&#21253;&#21547;&#19968;&#20010;&#33258;&#20132;&#20114;&#39033;&#65292;&#26377;&#21161;&#20110;&#38450;&#27490;&#27169;&#24335;&#23849;&#28291;&#24182;&#25429;&#33719;&#25152;&#26377;&#20998;&#24067;&#30340;&#27169;&#24335;&#12290;&#20026;&#20102;&#20811;&#26381;&#39640;&#32500;&#25968;&#25454;&#30340;&#30456;&#23545;&#20998;&#25955;&#20998;&#24067;&#30340;&#22256;&#38590;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#25968;&#25454;&#26144;&#23556;&#21040;&#28508;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#65292;&#24182;&#36924;&#36817;&#29305;&#24449;&#20998;&#24067;&#32780;&#19981;&#26159;&#25968;&#25454;&#20998;&#24067;&#12290;&#25105;&#20204;&#37319;&#29992; GAN &#26694;&#26550;&#65292;&#24182;&#29992;&#29305;&#24449;&#36716;&#25442;&#32593;&#32476;&#26367;&#25442;&#21028;&#21035;&#22120;&#23558;&#25968;&#25454;&#26144;&#23556;&#21040;&#28508;&#22312;&#31354;&#38388;&#12290;&#25105;&#20204;&#36824;&#22312;&#29305;&#24449;&#36716;&#25442;&#32593;&#32476;&#30340;&#25439;&#22833;&#20989;&#25968;&#20013;&#28155;&#21152;&#20102;&#19968;&#20010;&#31283;&#23450;&#39033;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102; GAN &#35757;&#32451;&#19981;&#31283;&#23450;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a novel approach to generative modeling using a loss function based on elastic interaction energy (EIE), which is inspired by the elastic interaction between defects in crystals. The utilization of the EIE-based metric presents several advantages, including its long range property that enables consideration of global information in the distribution. Moreover, its inclusion of a self-interaction term helps to prevent mode collapse and captures all modes of distribution. To overcome the difficulty of the relatively scattered distribution of high-dimensional data, we first map the data into a latent feature space and approximate the feature distribution instead of the data distribution. We adopt the GAN framework and replace the discriminator with a feature transformation network to map the data into a latent space. We also add a stabilizing term to the loss of the feature transformation network, which effectively addresses the issue of unstable training in GAN-b
&lt;/p&gt;</description></item><item><title>&#26080;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;UTSP&#33021;&#22815;&#23545;&#26053;&#34892;&#21830;&#38382;&#39064;&#36827;&#34892;&#27714;&#35299;&#65292;&#23427;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#22522;&#30784;&#27169;&#22411;&#65292;&#22312;&#20445;&#35777;&#36335;&#24452;&#20026;&#21704;&#23494;&#39039;&#24490;&#29615;&#30340;&#21069;&#25552;&#19979;&#65292;&#33021;&#22815;&#25214;&#21040;&#26368;&#30701;&#36335;&#24452;&#12290;&#30456;&#36739;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;UTSP&#22312;&#35757;&#32451;&#26679;&#26412;&#19982;&#21442;&#25968;&#25968;&#37327;&#19978;&#21344;&#29992;&#26356;&#23569;&#30340;&#36164;&#28304;&#65292;&#19988;&#24615;&#33021;&#26356;&#20339;&#12290;</title><link>http://arxiv.org/abs/2303.10538</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#23398;&#20064;&#27714;&#35299;&#26053;&#34892;&#21830;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Learning for Solving the Travelling Salesman Problem. (arXiv:2303.10538v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10538
&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;UTSP&#33021;&#22815;&#23545;&#26053;&#34892;&#21830;&#38382;&#39064;&#36827;&#34892;&#27714;&#35299;&#65292;&#23427;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#22522;&#30784;&#27169;&#22411;&#65292;&#22312;&#20445;&#35777;&#36335;&#24452;&#20026;&#21704;&#23494;&#39039;&#24490;&#29615;&#30340;&#21069;&#25552;&#19979;&#65292;&#33021;&#22815;&#25214;&#21040;&#26368;&#30701;&#36335;&#24452;&#12290;&#30456;&#36739;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;UTSP&#22312;&#35757;&#32451;&#26679;&#26412;&#19982;&#21442;&#25968;&#25968;&#37327;&#19978;&#21344;&#29992;&#26356;&#23569;&#30340;&#36164;&#28304;&#65292;&#19988;&#24615;&#33021;&#26356;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;UTSP&#65292;&#19968;&#31181;&#21033;&#29992;&#26080;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#27714;&#35299;&#26053;&#34892;&#21830;&#38382;&#39064;&#65288;TSP&#65289;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#26367;&#20195;&#25439;&#22833;&#35757;&#32451;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#12290;GNN&#36755;&#20986;&#19968;&#20010;&#28909;&#21147;&#22270;&#34920;&#31034;&#27599;&#20010;&#36793;&#25104;&#20026;&#26368;&#20248;&#36335;&#24452;&#30340;&#27010;&#29575;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24212;&#29992;&#23616;&#37096;&#25628;&#32034;&#26681;&#25454;&#28909;&#21147;&#22270;&#29983;&#25104;&#26368;&#32456;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#25439;&#22833;&#20989;&#25968;&#30001;&#20004;&#37096;&#20998;&#32452;&#25104;&#65306;&#19968;&#37096;&#20998;&#25512;&#21160;&#27169;&#22411;&#25214;&#21040;&#26368;&#30701;&#30340;&#36335;&#24452;&#65292;&#21478;&#19968;&#37096;&#20998;&#20316;&#20026;&#32422;&#26463;&#26465;&#20214;&#65292;&#30830;&#20445;&#36335;&#24452;&#24418;&#25104;&#21704;&#23494;&#39039;&#24490;&#29615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;UTSP&#20248;&#20110;&#29616;&#26377;&#30340;&#25968;&#25454;&#39537;&#21160;TSP&#21551;&#21457;&#24335;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21442;&#25968;&#25928;&#29575;&#21644;&#25968;&#25454;&#25928;&#29575;&#22343;&#36739;&#39640;&#65306;&#19982;&#24378;&#21270;&#23398;&#20064;&#25110;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#27169;&#22411;&#20165;&#21344;&#29992;&#32422;10&#65285;&#30340;&#21442;&#25968;&#21644;&#32422;0.2&#65285;&#30340;&#35757;&#32451;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose UTSP, an unsupervised learning (UL) framework for solving the Travelling Salesman Problem (TSP). We train a Graph Neural Network (GNN) using a surrogate loss. The GNN outputs a heat map representing the probability for each edge to be part of the optimal path. We then apply local search to generate our final prediction based on the heat map. Our loss function consists of two parts: one pushes the model to find the shortest path and the other serves as a surrogate for the constraint that the route should form a Hamiltonian Cycle. Experimental results show that UTSP outperforms the existing data-driven TSP heuristics. Our approach is parameter efficient as well as data efficient: the model takes $\sim$ 10\% of the number of parameters and $\sim$ 0.2\% of training samples compared with reinforcement learning or supervised learning methods.
&lt;/p&gt;</description></item><item><title>LNO&#26159;&#19968;&#31181;&#29992;&#20110;&#35299;&#24494;&#20998;&#26041;&#31243;&#30340;&#31639;&#27861;&#65292;&#30456;&#27604;&#20854;&#20182;&#31639;&#27861;&#65288;&#22914;FNO&#65289;&#20855;&#26377;&#26356;&#22909;&#30340;&#36924;&#36817;&#31934;&#24230;&#24182;&#36866;&#29992;&#20110;&#38750;&#21608;&#26399;&#24615;&#20449;&#21495;&#21644;&#30636;&#24577;&#21709;&#24212;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#35299;&#37322;&#27169;&#22411;&#24182;&#25913;&#36827;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.10528</link><description>&lt;p&gt;
LNO: &#29992;&#20110;&#35299;&#24494;&#20998;&#26041;&#31243;&#30340;&#25289;&#26222;&#25289;&#26031;&#31070;&#32463;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
LNO: Laplace Neural Operator for Solving Differential Equations. (arXiv:2303.10528v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10528
&lt;/p&gt;
&lt;p&gt;
LNO&#26159;&#19968;&#31181;&#29992;&#20110;&#35299;&#24494;&#20998;&#26041;&#31243;&#30340;&#31639;&#27861;&#65292;&#30456;&#27604;&#20854;&#20182;&#31639;&#27861;&#65288;&#22914;FNO&#65289;&#20855;&#26377;&#26356;&#22909;&#30340;&#36924;&#36817;&#31934;&#24230;&#24182;&#36866;&#29992;&#20110;&#38750;&#21608;&#26399;&#24615;&#20449;&#21495;&#21644;&#30636;&#24577;&#21709;&#24212;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#35299;&#37322;&#27169;&#22411;&#24182;&#25913;&#36827;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#25289;&#26222;&#25289;&#26031;&#31070;&#32463;&#31639;&#23376;&#65288;LNO&#65289;&#65292;&#21033;&#29992;&#25289;&#26222;&#25289;&#26031;&#21464;&#25442;&#23545;&#36755;&#20837;&#31354;&#38388;&#36827;&#34892;&#20998;&#35299;&#12290;&#19982;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#65288;FNO&#65289;&#19981;&#21516;&#65292;LNO&#21487;&#20197;&#22788;&#29702;&#38750;&#21608;&#26399;&#24615;&#20449;&#21495;&#65292;&#32771;&#34385;&#30636;&#24577;&#21709;&#24212;&#65292;&#24182;&#21576;&#25351;&#25968;&#25910;&#25947;&#12290;LNO&#32467;&#21512;&#20102;&#36755;&#20837;&#21644;&#36755;&#20986;&#31354;&#38388;&#20043;&#38388;&#30340;&#26497;&#28857;-&#27531;&#24046;&#20851;&#31995;&#65292;&#20855;&#26377;&#26356;&#24378;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#25913;&#36827;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21333;&#20010;&#25289;&#26222;&#25289;&#26031;&#23618;&#22312;&#36924;&#36817;&#19977;&#20010;ODE&#65288;Duffing&#25391;&#23376;&#12289;&#39537;&#21160;&#24341;&#21147;&#25670;&#21644;Lorenz&#31995;&#32479;&#65289;&#21644;&#19977;&#20010;PDE&#65288;Euler-Bernoulli&#26753;&#12289;&#25193;&#25955;&#26041;&#31243;&#21644;&#21453;&#24212;&#25193;&#25955;&#31995;&#32479;&#65289;&#30340;&#35299;&#26102;&#65292;&#27604;FNO&#20013;&#30340;&#22235;&#20010;&#20613;&#37324;&#21494;&#27169;&#22359;&#20855;&#26377;&#26356;&#22909;&#30340;&#36924;&#36817;&#31934;&#24230;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#26410;&#38459;&#23612;&#24773;&#20917;&#19979;&#65292;LNO&#22312;&#25429;&#25417;&#30636;&#24577;&#21709;&#24212;&#26041;&#38754;&#20248;&#20110;FNO&#12290;&#23545;&#20110;&#32447;&#24615;&#27431;&#25289;-&#20271;&#21162;&#21033;&#26753;&#21644;&#25193;&#25955;&#26041;&#31243;&#65292; LNO&#23545;&#26497;&#28857;-&#27531;&#24046;&#20844;&#24335;&#30340;&#31934;&#30830;&#34920;&#31034;&#27604;FNO&#20135;&#29983;&#20102;&#26174;&#30528;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the Laplace neural operator (LNO), which leverages the Laplace transform to decompose the input space. Unlike the Fourier Neural Operator (FNO), LNO can handle non-periodic signals, account for transient responses, and exhibit exponential convergence. LNO incorporates the pole-residue relationship between the input and the output space, enabling greater interpretability and improved generalization ability. Herein, we demonstrate the superior approximation accuracy of a single Laplace layer in LNO over four Fourier modules in FNO in approximating the solutions of three ODEs (Duffing oscillator, driven gravity pendulum, and Lorenz system) and three PDEs (Euler-Bernoulli beam, diffusion equation, and reaction-diffusion system). Notably, LNO outperforms FNO in capturing transient responses in undamped scenarios. For the linear Euler-Bernoulli beam and diffusion equation, LNO's exact representation of the pole-residue formulation yields significantly better results than FNO. Fo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;CNN&#36827;&#34892;&#36716;&#25442;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#35299;&#37322;&#20013;&#38388;&#23618;&#30340;&#34920;&#31034;&#65292;&#25552;&#21462;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#24615;&#27424;&#23436;&#22791;&#22522;&#30784;&#65292;&#24182;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#32593;&#32476;&#32467;&#26500;&#21644;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#37117;&#24456;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2303.10523</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#35299;&#37322;&#24615;&#22522;&#30784;&#25277;&#21462;&#29992;&#20110;&#22522;&#20110;&#27010;&#24565;&#30340;&#35270;&#35273;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Interpretable Basis Extraction for Concept-Based Visual Explanations. (arXiv:2303.10523v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10523
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;CNN&#36827;&#34892;&#36716;&#25442;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#35299;&#37322;&#20013;&#38388;&#23618;&#30340;&#34920;&#31034;&#65292;&#25552;&#21462;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#24615;&#27424;&#23436;&#22791;&#22522;&#30784;&#65292;&#24182;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#32593;&#32476;&#32467;&#26500;&#21644;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#37117;&#24456;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#23581;&#35797;&#29992;&#20154;&#31867;&#21487;&#20197;&#29702;&#35299;&#30340;&#27010;&#24565;&#26469;&#35299;&#37322;CNN&#22270;&#20687;&#20998;&#31867;&#22120;&#39044;&#27979;&#21644;&#20013;&#38388;&#23618;&#34920;&#31034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#26597;&#25214;&#35299;&#37322;&#20687;&#32032;&#28608;&#27963;&#30340;&#31232;&#30095;&#20108;&#20540;&#21270;&#36716;&#25442;&#34920;&#31034;&#30340;&#29305;&#24449;&#31354;&#38388;&#26059;&#36716;&#26469;&#25552;&#21462;&#35299;&#37322;&#24615;&#27424;&#23436;&#22791;&#22522;&#30784;&#12290;&#25105;&#20204;&#23545;&#29616;&#26377;&#30340;&#27969;&#34892;CNN&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24182;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#22312;&#32593;&#32476;&#26550;&#26500;&#21644;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#25552;&#21462;&#35299;&#37322;&#24615;&#22522;&#30784;&#30340;&#26377;&#25928;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#25991;&#29486;&#20013;&#30340;&#22522;&#30784;&#21487;&#35299;&#37322;&#24615;&#24230;&#37327;&#65292;&#24182;&#34920;&#26126;&#65292;&#24403;&#20013;&#38388;&#23618;&#34920;&#31034;&#34987;&#36716;&#25442;&#20026;&#25105;&#20204;&#26041;&#27861;&#25552;&#21462;&#30340;&#22522;&#30784;&#26102;&#65292;&#23427;&#20204;&#21464;&#24471;&#26356;&#26131;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
An important line of research attempts to explain CNN image classifier predictions and intermediate layer representations in terms of human understandable concepts. In this work, we expand on previous works in the literature that use annotated concept datasets to extract interpretable feature space directions and propose an unsupervised post-hoc method to extract a disentangling interpretable basis by looking for the rotation of the feature space that explains sparse one-hot thresholded transformed representations of pixel activations. We do experimentation with existing popular CNNs and demonstrate the effectiveness of our method in extracting an interpretable basis across network architectures and training datasets. We make extensions to the existing basis interpretability metrics found in the literature and show that, intermediate layer representations become more interpretable when transformed to the bases extracted with our method. Finally, using the basis interpretability metrics
&lt;/p&gt;</description></item><item><title>AdaLoRA&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#39044;&#31639;&#20998;&#37197;&#26041;&#27861;&#65292;&#29992;&#20110;&#21442;&#25968;&#25928;&#29575;&#24494;&#35843;&#12290;&#23558;&#22686;&#37327;&#26356;&#26032;&#30340;&#39044;&#31639;&#26681;&#25454;&#26435;&#37325;&#30697;&#38453;&#30340;&#37325;&#35201;&#24615;&#20998;&#25968;&#36827;&#34892;&#33258;&#36866;&#24212;&#20998;&#37197;&#65292;&#36890;&#36807;&#22855;&#24322;&#20540;&#20998;&#35299;&#30340;&#24418;&#24335;&#65292;&#23454;&#29616;&#20102;&#24494;&#35843;&#34920;&#29616;&#30340;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2303.10512</link><description>&lt;p&gt;
&#21442;&#25968;&#25928;&#29575;&#24494;&#35843;&#30340;&#33258;&#36866;&#24212;&#39044;&#31639;&#20998;&#37197;
&lt;/p&gt;
&lt;p&gt;
Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning. (arXiv:2303.10512v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10512
&lt;/p&gt;
&lt;p&gt;
AdaLoRA&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#39044;&#31639;&#20998;&#37197;&#26041;&#27861;&#65292;&#29992;&#20110;&#21442;&#25968;&#25928;&#29575;&#24494;&#35843;&#12290;&#23558;&#22686;&#37327;&#26356;&#26032;&#30340;&#39044;&#31639;&#26681;&#25454;&#26435;&#37325;&#30697;&#38453;&#30340;&#37325;&#35201;&#24615;&#20998;&#25968;&#36827;&#34892;&#33258;&#36866;&#24212;&#20998;&#37197;&#65292;&#36890;&#36807;&#22855;&#24322;&#20540;&#20998;&#35299;&#30340;&#24418;&#24335;&#65292;&#23454;&#29616;&#20102;&#24494;&#35843;&#34920;&#29616;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#23545;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#24050;&#32463;&#25104;&#20026;&#20102;&#19968;&#31181;&#37325;&#35201;&#30340;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;&#36890;&#24120;&#30340;&#20570;&#27861;&#26159;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#25152;&#26377;&#21442;&#25968;&#65292;&#24403;&#23384;&#22312;&#22823;&#37327;&#19979;&#28216;&#20219;&#21153;&#26102;&#65292;&#36825;&#31181;&#26041;&#27861;&#21464;&#24471;&#19981;&#20999;&#23454;&#38469;&#12290;&#22240;&#27492;&#65292;&#35768;&#22810;&#24494;&#35843;&#26041;&#27861;&#34987;&#25552;&#20986;&#26469;&#20197;&#20197;&#21442;&#25968;&#26377;&#25928;&#30340;&#26041;&#24335;&#23398;&#20064;&#39044;&#35757;&#32451;&#21152;&#26435;&#30340;&#22686;&#37327;&#26356;&#26032;&#65292;&#20363;&#22914;&#20302;&#31209;&#22686;&#37327;&#12290;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#23558;&#22686;&#37327;&#26356;&#26032;&#30340;&#39044;&#31639;&#22343;&#21248;&#20998;&#37197;&#21040;&#25152;&#26377;&#39044;&#35757;&#32451;&#30340;&#26435;&#37325;&#30697;&#38453;&#19978;&#65292;&#24573;&#30053;&#20102;&#19981;&#21516;&#26435;&#37325;&#21442;&#25968;&#30340;&#19981;&#21516;&#37325;&#35201;&#24615;&#12290;&#32467;&#26524;&#65292;&#24494;&#35843;&#30340;&#34920;&#29616;&#26159;&#27425;&#20248;&#30340;&#12290;&#20026;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AdaLoRA&#65292;&#26681;&#25454;&#23427;&#20204;&#30340;&#37325;&#35201;&#24615;&#20998;&#25968;&#33258;&#36866;&#24212;&#20998;&#37197;&#26435;&#37325;&#30697;&#38453;&#30340;&#21442;&#25968;&#39044;&#31639;&#12290;&#29305;&#21035;&#22320;&#65292;AdaLoRA&#23558;&#22686;&#37327;&#26356;&#26032;&#30340;&#21442;&#25968;&#21270;&#20026;&#22855;&#24322;&#20540;&#20998;&#35299;&#30340;&#24418;&#24335;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#20351;&#25105;&#20204;&#21487;&#20197;&#26377;&#25928;&#22320;&#21098;&#26525;&#22855;&#24322;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning large pre-trained language models on downstream tasks has become an important paradigm in NLP. However, common practice fine-tunes all of the parameters in a pre-trained model, which becomes prohibitive when a large number of downstream tasks are present. Therefore, many fine-tuning methods are proposed to learn incremental updates of pre-trained weights in a parameter efficient way, e.g., low-rank increments. These methods often evenly distribute the budget of incremental updates across all pre-trained weight matrices, and overlook the varying importance of different weight parameters. As a consequence, the fine-tuning performance is suboptimal. To bridge this gap, we propose AdaLoRA, which adaptively allocates the parameter budget among weight matrices according to their importance score. In particular, AdaLoRA parameterizes the incremental updates in the form of singular value decomposition. Such a novel approach allows us to effectively prune the singular values of unim
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#21322;&#30417;&#30563;&#23398;&#20064;&#27880;&#37322;&#39046;&#22495;&#29305;&#23450;&#25968;&#25454;&#65292;&#22522;&#20110;&#39044;&#35757;&#32451;&#30340;&#22768;&#23398;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#30340;ASR&#31995;&#32479;&#65292;&#24182;&#22312;&#39046;&#22495;&#29305;&#23450;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#21830;&#19994;ASR&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.10510</link><description>&lt;p&gt;
&#38754;&#21521;&#39046;&#22495;&#29305;&#23450;&#35821;&#38899;&#35782;&#21035;&#30340;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
A Deep Learning System for Domain-specific speech Recognition. (arXiv:2303.10510v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10510
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#21322;&#30417;&#30563;&#23398;&#20064;&#27880;&#37322;&#39046;&#22495;&#29305;&#23450;&#25968;&#25454;&#65292;&#22522;&#20110;&#39044;&#35757;&#32451;&#30340;&#22768;&#23398;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#30340;ASR&#31995;&#32479;&#65292;&#24182;&#22312;&#39046;&#22495;&#29305;&#23450;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#21830;&#19994;ASR&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#26426;&#35821;&#38899;&#25509;&#21475;&#36234;&#26469;&#36234;&#20415;&#25463;&#65292;&#35768;&#22810;&#26368;&#20808;&#36827;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#34987;&#25552;&#20986;&#12290;&#28982;&#32780;&#65292;&#21830;&#19994;ASR&#31995;&#32479;&#36890;&#24120;&#22312;&#39046;&#22495;&#29305;&#23450;&#35821;&#38899;&#65292;&#29305;&#21035;&#26159;&#22312;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#30340;&#34920;&#29616;&#36739;&#24046;&#12290;&#20316;&#32773;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;DeepSpeech2&#21644;Wav2Vec2&#22768;&#23398;&#27169;&#22411;&#65292;&#24320;&#21457;&#20102;&#21463;&#30410;&#29305;&#23450;&#30340;ASR&#31995;&#32479;&#12290;&#20351;&#29992;&#21322;&#30417;&#30563;&#23398;&#20064;&#27880;&#37322;&#39046;&#22495;&#29305;&#23450;&#25968;&#25454;&#65292;&#21482;&#38656;&#23569;&#37327;&#20154;&#24037;&#24178;&#39044;&#21363;&#21487;&#12290;&#26368;&#20339;&#24615;&#33021;&#26469;&#33258;&#19968;&#31181;&#32463;&#36807;&#24494;&#35843;&#30340;Wav2Vec2-Large-LV60&#22768;&#23398;&#27169;&#22411;&#65292;&#24102;&#26377;&#22806;&#37096;KenLM&#65292;&#22312;&#21463;&#30410;&#29305;&#23450;&#35821;&#38899;&#19978;&#36229;&#36234;&#20102;Google&#21644;AWS ASR&#31995;&#32479;&#12290;&#36824;&#30740;&#31350;&#20102;&#23558;&#23481;&#26131;&#20986;&#38169;&#30340;ASR&#36716;&#24405;&#20316;&#20026;&#21475;&#35821;&#29702;&#35299;&#65288;SLU&#65289;&#30340;&#19968;&#37096;&#20998;&#30340;&#21487;&#34892;&#24615;&#12290;&#21463;&#30410;&#29305;&#23450;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#20219;&#21153;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#39046;&#22495;&#29305;&#23450;&#24494;&#35843;&#30340;ASR&#31995;&#32479;&#21487;&#20197;&#36229;&#36234;&#21830;&#19994;ASR&#31995;&#32479;&#24182;&#25552;&#39640;NLU&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As human-machine voice interfaces provide easy access to increasingly intelligent machines, many state-of-the-art automatic speech recognition (ASR) systems are proposed. However, commercial ASR systems usually have poor performance on domain-specific speech especially under low-resource settings. The author works with pre-trained DeepSpeech2 and Wav2Vec2 acoustic models to develop benefit-specific ASR systems. The domain-specific data are collected using proposed semi-supervised learning annotation with little human intervention. The best performance comes from a fine-tuned Wav2Vec2-Large-LV60 acoustic model with an external KenLM, which surpasses the Google and AWS ASR systems on benefit-specific speech. The viability of using error prone ASR transcriptions as part of spoken language understanding (SLU) is also investigated. Results of a benefit-specific natural language understanding (NLU) task show that the domain-specific fine-tuned ASR system can outperform the commercial ASR sys
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#31639;&#23376;&#23398;&#20064;&#21644;&#36924;&#36817;&#35774;&#35745;&#26144;&#23556;&#30340;&#26041;&#27861;&#65292;&#20197;&#28040;&#38500;&#35299;&#20915;PDE&#30340;&#38656;&#35201;&#12290;</title><link>http://arxiv.org/abs/2303.10506</link><description>&lt;p&gt;
&#21453;&#24212;&#25193;&#25955;PDE&#30340;&#21453;&#27493;&#25511;&#21046;&#22120;&#19982;&#35266;&#27979;&#22120;&#22686;&#30410;&#20989;&#25968;&#30340;&#31070;&#32463;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
Neural Operators of Backstepping Controller and Observer Gain Functions for Reaction-Diffusion PDEs. (arXiv:2303.10506v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#31639;&#23376;&#23398;&#20064;&#21644;&#36924;&#36817;&#35774;&#35745;&#26144;&#23556;&#30340;&#26041;&#27861;&#65292;&#20197;&#28040;&#38500;&#35299;&#20915;PDE&#30340;&#38656;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;ODE&#30340;&#27169;&#22411;&#28041;&#21450;&#31995;&#32479;&#30697;&#38453;&#12289;&#25511;&#21046;&#22120;&#28041;&#21450;&#21521;&#37327;&#25110;&#30697;&#38453;&#22686;&#30410;&#19981;&#21516;&#65292;PDE&#27169;&#22411;&#28041;&#21450;&#20989;&#25968;&#31995;&#25968;&#21644;&#31354;&#38388;&#21464;&#37327;&#30456;&#20851;&#30340;&#22686;&#30410;&#20989;&#25968;&#12290;PDE&#21453;&#27493;&#25511;&#21046;&#22120;&#21644;&#35266;&#27979;&#22120;&#30340;&#35774;&#35745;&#26159;&#23558;&#31995;&#32479;&#27169;&#22411;&#20989;&#25968;&#26144;&#23556;&#21040;&#22686;&#30410;&#20989;&#25968;&#30340;&#38750;&#32447;&#24615;&#31639;&#23376;&#12290;&#36890;&#36807;&#23398;&#20064;&#21644;&#36924;&#36817;&#36825;&#31181;&#35774;&#35745;&#26144;&#23556;&#30340;&#31070;&#32463;&#31639;&#23376;&#65292;&#21487;&#20197;&#28040;&#38500;&#35299;&#20915;PDE&#30340;&#38656;&#35201;&#12290;&#23398;&#20064;&#31070;&#32463;&#31639;&#23376;&#38656;&#35201;&#36275;&#22815;&#25968;&#37327;&#30340;&#35774;&#35745;PDE&#30340;&#20808;&#21069;&#35299;&#65292;&#20197;&#21450;&#25805;&#20316;&#21592;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unlike ODEs, whose models involve system matrices and whose controllers involve vector or matrix gains, PDE models involve functions in those roles functional coefficients, dependent on the spatial variables, and gain functions dependent on space as well. The designs of gains for controllers and observers for PDEs, such as PDE backstepping, are mappings of system model functions into gain functions. These infinite dimensional nonlinear operators are given in an implicit form through PDEs, in spatial variables, which need to be solved to determine the gain function for each new functional coefficient of the PDE. The need for solving such PDEs can be eliminated by learning and approximating the said design mapping in the form of a neural operator. Learning the neural operator requires a sufficient number of prior solutions for the design PDEs, offline, as well as the training of the operator. In recent work, we developed the neural operators for PDE backstepping designs for first order h
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#33258;&#21160;&#31243;&#24207;&#20462;&#22797;&#20013;&#30340;&#25972;&#24418;&#25163;&#26415;&#20551;&#35774;&#65292;&#24182;&#25552;&#20986;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;APR&#30340;&#26032;&#26041;&#27861;&#65292;&#20027;&#35201;&#35299;&#20915;&#20102;&#20256;&#32479;APR&#24037;&#20855;&#22312;&#19981;&#21516;&#39033;&#30446;&#20013;&#26080;&#27861;&#20135;&#29983;&#22810;&#26679;&#21270;&#20462;&#34917;&#31243;&#24207;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.10494</link><description>&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#37325;&#26032;&#23457;&#35270;&#25972;&#24418;&#25163;&#26415;&#20551;&#35774;
&lt;/p&gt;
&lt;p&gt;
Revisiting the Plastic Surgery Hypothesis via Large Language Models. (arXiv:2303.10494v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10494
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#33258;&#21160;&#31243;&#24207;&#20462;&#22797;&#20013;&#30340;&#25972;&#24418;&#25163;&#26415;&#20551;&#35774;&#65292;&#24182;&#25552;&#20986;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;APR&#30340;&#26032;&#26041;&#27861;&#65292;&#20027;&#35201;&#35299;&#20915;&#20102;&#20256;&#32479;APR&#24037;&#20855;&#22312;&#19981;&#21516;&#39033;&#30446;&#20013;&#26080;&#27861;&#20135;&#29983;&#22810;&#26679;&#21270;&#20462;&#34917;&#31243;&#24207;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#31243;&#24207;&#20462;&#22797;&#65288;APR&#65289;&#26088;&#22312;&#33258;&#21160;&#29983;&#25104;&#36755;&#20837;&#38169;&#35823;&#31243;&#24207;&#30340;&#34917;&#19969;&#12290;&#20256;&#32479;APR&#24037;&#20855;&#36890;&#24120;&#19987;&#27880;&#20110;&#29305;&#23450;&#30340;&#38169;&#35823;&#31867;&#22411;&#21644;&#20462;&#22797;&#26041;&#24335;&#65292;&#36890;&#36807;&#20351;&#29992;&#27169;&#26495;&#12289;&#21551;&#21457;&#24335;&#21644;&#27491;&#24335;&#35268;&#33539;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25216;&#26415;&#22312;&#38169;&#35823;&#31867;&#22411;&#21644;&#20462;&#34917;&#31243;&#24207;&#30340;&#22810;&#26679;&#21270;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#20154;&#21592;&#35774;&#35745;&#20102;&#21508;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;APR&#24037;&#20855;&#65292;&#26368;&#36817;&#30340;&#24037;&#20316;&#38598;&#20013;&#22312;&#30452;&#25509;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;APR&#12290;&#34429;&#28982;&#22522;&#20110;LLM&#30340;APR&#24037;&#20855;&#33021;&#22815;&#22312;&#35768;&#22810;&#20462;&#22797;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#29992;&#20110;&#30452;&#25509;&#20462;&#22797;&#30340;LLMs&#24182;&#27809;&#26377;&#23436;&#20840;&#20102;&#35299;&#39033;&#30446;&#29305;&#23450;&#20449;&#24687;&#65292;&#22914;&#29420;&#29305;&#30340;&#21464;&#37327;&#25110;&#26041;&#27861;&#21517;&#31216;&#12290;&#25972;&#24418;&#25163;&#26415;&#20551;&#35774;&#26159;APR&#30340;&#19968;&#20010;&#33879;&#21517;&#30340;&#35265;&#35299;&#65292;&#23427;&#25351;&#20986;&#20462;&#22797;&#38169;&#35823;&#30340;&#20195;&#30721;&#37096;&#20998;&#36890;&#24120;&#24050;&#32463;&#23384;&#22312;&#20110;&#21516;&#19968;&#39033;&#30446;&#20013;&#12290;&#20256;&#32479;&#30340;APR&#24037;&#20855;&#20027;&#35201;&#36890;&#36807;&#35774;&#35745;&#25163;&#21160;&#25110;&#22522;&#20110;&#21551;&#21457;&#30340;&#26041;&#27861;&#26469;&#21033;&#29992;&#25972;&#24418;&#25163;&#26415;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated Program Repair (APR) aspires to automatically generate patches for an input buggy program. Traditional APR tools typically focus on specific bug types and fixes through the use of templates, heuristics, and formal specifications. However, these techniques are limited in terms of the bug types and patch variety they can produce. As such, researchers have designed various learning-based APR tools with recent work focused on directly using Large Language Models (LLMs) for APR. While LLM-based APR tools are able to achieve state-of-the-art performance on many repair datasets, the LLMs used for direct repair are not fully aware of the project-specific information such as unique variable or method names.  The plastic surgery hypothesis is a well-known insight for APR, which states that the code ingredients to fix the bug usually already exist within the same project. Traditional APR tools have largely leveraged the plastic surgery hypothesis by designing manual or heuristic-based a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#34920;&#26126;&#40657;&#30418;&#21464;&#20998;&#25512;&#29702;&#65288;BBVI&#65289;&#28385;&#36275;SGD&#25991;&#29486;&#20013;&#30340;ABC&#26465;&#20214;&#65292;&#35813;&#32467;&#26524;&#36866;&#29992;&#20110;&#24179;&#28369;&#21644;&#20108;&#27425;&#22686;&#38271;&#30340;&#23545;&#25968;&#20284;&#28982;&#20989;&#25968;&#65292;&#21516;&#26102;&#25105;&#20204;&#30340;&#32467;&#26524;&#25512;&#24191;&#21040;&#24191;&#27867;&#24212;&#29992;&#20110;BBVI&#23454;&#36341;&#20013;&#30340;&#38750;&#32447;&#24615;&#21327;&#26041;&#24046;&#21442;&#25968;&#21270;&#12290;</title><link>http://arxiv.org/abs/2303.10472</link><description>&lt;p&gt;
&#40657;&#30418;&#21464;&#20998;&#36125;&#21494;&#26031;&#25512;&#29702;&#30340;&#23454;&#29992;&#21305;&#37197;&#26799;&#24230;&#26041;&#24046;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Practical and Matching Gradient Variance Bounds for Black-Box Variational Bayesian Inference. (arXiv:2303.10472v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10472
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#34920;&#26126;&#40657;&#30418;&#21464;&#20998;&#25512;&#29702;&#65288;BBVI&#65289;&#28385;&#36275;SGD&#25991;&#29486;&#20013;&#30340;ABC&#26465;&#20214;&#65292;&#35813;&#32467;&#26524;&#36866;&#29992;&#20110;&#24179;&#28369;&#21644;&#20108;&#27425;&#22686;&#38271;&#30340;&#23545;&#25968;&#20284;&#28982;&#20989;&#25968;&#65292;&#21516;&#26102;&#25105;&#20204;&#30340;&#32467;&#26524;&#25512;&#24191;&#21040;&#24191;&#27867;&#24212;&#29992;&#20110;BBVI&#23454;&#36341;&#20013;&#30340;&#38750;&#32447;&#24615;&#21327;&#26041;&#24046;&#21442;&#25968;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#40657;&#30418;&#21464;&#20998;&#25512;&#29702;&#65288;BBVI&#65289;&#30340;&#26799;&#24230;&#26041;&#24046;&#26159;&#24314;&#31435;&#20854;&#25910;&#25947;&#24615;&#21644;&#31639;&#27861;&#25913;&#36827;&#30340;&#20851;&#38190;&#19968;&#27493;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#23578;&#26410;&#34920;&#26126;BBVI&#30340;&#26799;&#24230;&#26041;&#24046;&#28385;&#36275;&#29992;&#20110;&#30740;&#31350;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#25910;&#25947;&#30340;&#26465;&#20214;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#24212;&#29992;&#20110;&#24179;&#28369;&#21644;&#20108;&#27425;&#22686;&#38271;&#30340;&#23545;&#25968;&#20284;&#28982;&#20989;&#25968;&#26102;&#65292;BBVI&#28385;&#36275;&#19982;SGD&#25991;&#29486;&#20013;&#20351;&#29992;&#30340;ABC&#26465;&#20214;&#30456;&#21305;&#37197;&#30340;&#30028;&#38480;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25512;&#24191;&#21040;&#24191;&#27867;&#24212;&#29992;&#20110;BBVI&#23454;&#36341;&#20013;&#30340;&#38750;&#32447;&#24615;&#21327;&#26041;&#24046;&#21442;&#25968;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#24179;&#22343;&#22330;&#21442;&#25968;&#21270;&#30340;&#26041;&#24046;&#20855;&#26377;&#32463;&#36807;&#39564;&#35777;&#30340;&#20248;&#36234;&#32500;&#24230;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the gradient variance of black-box variational inference (BBVI) is a crucial step for establishing its convergence and developing algorithmic improvements. However, existing studies have yet to show that the gradient variance of BBVI satisfies the conditions used to study the convergence of stochastic gradient descent (SGD), the workhorse of BBVI. In this work, we show that BBVI satisfies a matching bound corresponding to the $ABC$ condition used in the SGD literature when applied to smooth and quadratically-growing log-likelihoods. Our results generalize to nonlinear covariance parameterizations widely used in the practice of BBVI. Furthermore, we show that the variance of the mean-field parameterization has provably superior dimensional dependence.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;SPDF&#31639;&#27861;&#26469;&#23454;&#29616;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#35757;&#32451;&#12290;&#36890;&#36807;&#38750;&#32467;&#26500;&#21270;&#26435;&#37325;&#31232;&#30095;&#24615;&#26469;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#21487;&#20197;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#65292;&#32780;&#23494;&#38598;&#24494;&#35843;&#21017;&#21487;&#20197;&#20445;&#35777;&#39640;&#24615;&#33021;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.10464</link><description>&lt;p&gt;
SPDF&#65306;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#31232;&#30095;&#39044;&#35757;&#32451;&#21644;&#23494;&#38598;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
SPDF: Sparse Pre-training and Dense Fine-tuning for Large Language Models. (arXiv:2303.10464v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10464
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;SPDF&#31639;&#27861;&#26469;&#23454;&#29616;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#35757;&#32451;&#12290;&#36890;&#36807;&#38750;&#32467;&#26500;&#21270;&#26435;&#37325;&#31232;&#30095;&#24615;&#26469;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#21487;&#20197;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#65292;&#32780;&#23494;&#38598;&#24494;&#35843;&#21017;&#21487;&#20197;&#20445;&#35777;&#39640;&#24615;&#33021;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#33539;&#24335;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#22810;&#39033;&#31361;&#30772;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#35821;&#35328;&#27169;&#22411;&#39318;&#20808;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#36328;&#22495;&#30693;&#35782;&#30340;&#39044;&#35757;&#32451;&#65288;&#20363;&#22914;&#65292;Pile&#12289;MassiveText&#31561;&#65289;&#65292;&#28982;&#21518;&#22312;&#29305;&#23450;&#20219;&#21153;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#65288;&#20363;&#22914;&#65292;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#12289;&#25991;&#26412;&#25688;&#35201;&#31561;&#65289;&#12290;&#34429;&#28982;&#25193;&#22823;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#22823;&#23567;&#26377;&#21161;&#20110;&#25552;&#39640;LLM&#24615;&#33021;&#65292;&#20294;&#36825;&#20063;&#24102;&#26469;&#20102;&#26497;&#20026;&#31105;&#27490;&#24615;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#39044;&#35757;&#32451;LLMs&#36890;&#24120;&#38656;&#35201;&#27604;&#24494;&#35843;&#28436;&#20064;&#26356;&#22810;&#30340;FLOPs&#65292;&#20004;&#20010;&#38454;&#27573;&#20043;&#38388;&#30340;&#27169;&#22411;&#23481;&#37327;&#36890;&#24120;&#20445;&#25345;&#19981;&#21464;&#12290;&#20026;&#20102;&#23454;&#29616;&#30456;&#23545;&#20110;&#35757;&#32451;FLOPs&#30340;&#35757;&#32451;&#25928;&#29575;&#65292;&#25105;&#20204;&#24314;&#35758;&#22312;&#20004;&#20010;&#38454;&#27573;&#20043;&#38388;&#35299;&#32806;&#27169;&#22411;&#23481;&#37327;&#65292;&#24182;&#24341;&#20837;&#31232;&#30095;&#39044;&#35757;&#32451;&#21644;&#23494;&#38598;&#24494;&#35843;&#65288;SPDF&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#38750;&#32467;&#26500;&#21270;&#26435;&#37325;&#31232;&#30095;&#24615;&#26469;&#20165;&#35757;&#32451;&#23376;&#38598;&#26435;&#37325;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
The pre-training and fine-tuning paradigm has contributed to a number of breakthroughs in Natural Language Processing (NLP). Instead of directly training on a downstream task, language models are first pre-trained on large datasets with cross-domain knowledge (e.g., Pile, MassiveText, etc.) and then fine-tuned on task-specific data (e.g., natural language generation, text summarization, etc.). Scaling the model and dataset size has helped improve the performance of LLMs, but unfortunately, this also leads to highly prohibitive computational costs. Pre-training LLMs often require orders of magnitude more FLOPs than fine-tuning and the model capacity often remains the same between the two phases. To achieve training efficiency w.r.t training FLOPs, we propose to decouple the model capacity between the two phases and introduce Sparse Pre-training and Dense Fine-tuning (SPDF). In this work, we show the benefits of using unstructured weight sparsity to train only a subset of weights during 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#26368;&#26032;&#30340;&#20132;&#21449;&#23398;&#31185;&#30740;&#31350;&#36827;&#23637;&#65292;&#28085;&#30422;&#20102;&#25968;&#25454;&#21516;&#21270;&#12289;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#39640;&#32500;&#21160;&#24577;&#31995;&#32479;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#36825;&#31687;&#35770;&#25991;&#23545;&#20110;&#24076;&#26395;&#24212;&#29992;DA&#21644;UQ&#25216;&#26415;&#30340;&#26426;&#22120;&#23398;&#20064;&#31185;&#23398;&#23478;&#26159;&#19968;&#20221;&#20840;&#38754;&#30340;&#25351;&#21335;&#12290;</title><link>http://arxiv.org/abs/2303.10462</link><description>&lt;p&gt;
&#25968;&#25454;&#21516;&#21270;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#22312;&#21160;&#24577;&#31995;&#32479;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#65306;&#19968;&#31687;&#32508;&#36848;&#65288;arXiv&#65306;2303.10462v1 [cs.LG]&#65289;
&lt;/p&gt;
&lt;p&gt;
Machine learning with data assimilation and uncertainty quantification for dynamical systems: a review. (arXiv:2303.10462v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10462
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#26368;&#26032;&#30340;&#20132;&#21449;&#23398;&#31185;&#30740;&#31350;&#36827;&#23637;&#65292;&#28085;&#30422;&#20102;&#25968;&#25454;&#21516;&#21270;&#12289;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#39640;&#32500;&#21160;&#24577;&#31995;&#32479;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#36825;&#31687;&#35770;&#25991;&#23545;&#20110;&#24076;&#26395;&#24212;&#29992;DA&#21644;UQ&#25216;&#26415;&#30340;&#26426;&#22120;&#23398;&#20064;&#31185;&#23398;&#23478;&#26159;&#19968;&#20221;&#20840;&#38754;&#30340;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#21516;&#21270;&#65288;DA&#65289;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65288;UQ&#65289;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#20998;&#26512;&#21644;&#20943;&#23569;&#39640;&#32500;&#31354;&#38388; - &#26102;&#38388;&#21160;&#24577;&#20013;&#30340;&#35823;&#24046;&#20256;&#25773;&#12290;&#20856;&#22411;&#24212;&#29992;&#33539;&#22260;&#20174;&#35745;&#31639;&#27969;&#20307;&#21147;&#23398;&#65288;CFD&#65289;&#21040;&#22320;&#29699;&#31185;&#23398;&#21644;&#27668;&#20505;&#31995;&#32479;&#12290;&#26368;&#36817;&#65292;&#24456;&#22810;&#24037;&#20316;&#33268;&#21147;&#20110;&#23558;DA&#65292;UQ&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#25216;&#26415;&#32467;&#21512;&#36215;&#26469;&#12290;&#36825;&#20123;&#30740;&#31350;&#21162;&#21147;&#35299;&#20915;&#39640;&#32500;&#21160;&#24577;&#31995;&#32479;&#20013;&#30340;&#19968;&#20123;&#20851;&#38190;&#25361;&#25112;&#65292;&#21253;&#25324;&#20294;&#19981;&#38480;&#20110;&#21160;&#24577;&#31995;&#32479;&#35782;&#21035;&#65292;&#38477;&#38454;&#26367;&#20195;&#24314;&#27169;&#65292;&#35823;&#24046;&#21327;&#26041;&#24046;&#35268;&#33539;&#21644;&#27169;&#22411;&#35823;&#24046;&#26657;&#27491;&#12290;&#35768;&#22810;&#24320;&#21457;&#30340;&#25216;&#26415;&#21644;&#26041;&#27861;&#23398;&#34920;&#29616;&#20986;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#65292;&#28085;&#30422;&#20102;&#35768;&#22810;&#39046;&#22495;&#65292;&#22240;&#27492;&#38656;&#35201;&#19968;&#20221;&#32508;&#21512;&#24615;&#30340;&#25351;&#21335;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#36825;&#19968;&#20132;&#21449;&#23398;&#31185;&#39046;&#22495;&#26368;&#26032;&#30740;&#31350;&#30340;&#32508;&#36848;&#65292;&#28085;&#30422;&#20102;&#24191;&#27867;&#30340;&#24212;&#29992;&#33539;&#22260;&#12290;&#35813;&#32508;&#36848;&#26088;&#22312;&#38754;&#21521;&#35797;&#22270;&#24212;&#29992;DA&#21644;UQ&#25216;&#26415;&#30340;&#26426;&#22120;&#23398;&#20064;&#31185;&#23398;&#23478;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data Assimilation (DA) and Uncertainty quantification (UQ) are extensively used in analysing and reducing error propagation in high-dimensional spatial-temporal dynamics. Typical applications span from computational fluid dynamics (CFD) to geoscience and climate systems. Recently, much effort has been given in combining DA, UQ and machine learning (ML) techniques. These research efforts seek to address some critical challenges in high-dimensional dynamical systems, including but not limited to dynamical system identification, reduced order surrogate modelling, error covariance specification and model error correction. A large number of developed techniques and methodologies exhibit a broad applicability across numerous domains, resulting in the necessity for a comprehensive guide. This paper provides the first overview of the state-of-the-art researches in this interdisciplinary field, covering a wide range of applications. This review aims at ML scientists who attempt to apply DA and 
&lt;/p&gt;</description></item><item><title>&#23398;&#20064;&#12289;&#36951;&#24536;&#21644;&#37325;&#23398;&#65288;LURE&#65289;&#26159;&#19968;&#31181;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#22312;&#32447;&#23398;&#20064;&#33539;&#24335;&#65292;&#23427;&#22312;&#36951;&#24536;&#38454;&#27573;&#21644;&#37325;&#23398;&#38454;&#27573;&#20043;&#38388;&#36827;&#34892;&#20132;&#26367;&#65292;&#36890;&#36807;&#26377;&#36873;&#25321;&#22320;&#36951;&#24536;&#27169;&#22411;&#20013;&#30340;&#19981;&#33391;&#20449;&#24687;&#21644;&#23545;&#27867;&#21270;&#24615;&#30340;&#23398;&#20064;&#26469;&#22312;&#25928;&#29575;&#21644;&#27867;&#21270;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2303.10455</link><description>&lt;p&gt;
&#23398;&#20064;&#12289;&#36951;&#24536;&#21644;&#37325;&#23398;&#65306;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#22312;&#32447;&#23398;&#20064;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
Learn, Unlearn and Relearn: An Online Learning Paradigm for Deep Neural Networks. (arXiv:2303.10455v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10455
&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#12289;&#36951;&#24536;&#21644;&#37325;&#23398;&#65288;LURE&#65289;&#26159;&#19968;&#31181;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#22312;&#32447;&#23398;&#20064;&#33539;&#24335;&#65292;&#23427;&#22312;&#36951;&#24536;&#38454;&#27573;&#21644;&#37325;&#23398;&#38454;&#27573;&#20043;&#38388;&#36827;&#34892;&#20132;&#26367;&#65292;&#36890;&#36807;&#26377;&#36873;&#25321;&#22320;&#36951;&#24536;&#27169;&#22411;&#20013;&#30340;&#19981;&#33391;&#20449;&#24687;&#21644;&#23545;&#27867;&#21270;&#24615;&#30340;&#23398;&#20064;&#26469;&#22312;&#25928;&#29575;&#21644;&#27867;&#21270;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#36890;&#24120;&#26159;&#22312;&#25552;&#21069;&#25552;&#20379;&#23436;&#25972;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#21069;&#25552;&#19979;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24773;&#20917;&#19979;&#65292;&#25968;&#25454;&#32463;&#24120;&#20250;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#32780;&#20197;&#22359;&#30340;&#24418;&#24335;&#20986;&#29616;&#12290;&#36825;&#23601;&#24341;&#21457;&#20102;&#20851;&#20110;&#35757;&#32451;DNN&#30340;&#26368;&#20339;&#31574;&#30053;&#30340;&#37325;&#35201;&#32771;&#34385;&#65292;&#20363;&#22914;&#26159;&#21542;&#22312;&#27599;&#20010;&#26032;&#25968;&#25454;&#22359;&#21040;&#36798;&#26102;&#20351;&#29992;&#24494;&#35843;&#30340;&#26041;&#27861;&#65288;&#28201;&#21551;&#21160;&#65289;&#25110;&#32773;&#27599;&#24403;&#26377;&#26032;&#30340;&#25968;&#25454;&#22359;&#21487;&#29992;&#26102;&#20174;&#22836;&#24320;&#22987;&#37325;&#26032;&#35757;&#32451;&#23427;&#20204;&#12290;&#34429;&#28982;&#21518;&#32773;&#29992;&#20110;&#35757;&#32451;&#21487;&#20197;&#28040;&#32791;&#26356;&#22810;&#30340;&#36164;&#28304;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#25351;&#20986;&#20102;&#28201;&#21551;&#21160;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#32570;&#20047;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#22312;&#25928;&#29575;&#21644;&#27867;&#21270;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#22312;&#32447;&#23398;&#20064;&#33539;&#24335;&#8212;&#8212;&#23398;&#20064;&#12289;&#36951;&#24536;&#21644;&#37325;&#23398;&#65288;LURE&#65289;&#12290;LURE&#22312;&#36951;&#24536;&#38454;&#27573;&#21644;&#37325;&#23398;&#38454;&#27573;&#20043;&#38388;&#36827;&#34892;&#20132;&#26367;&#65292;&#36951;&#24536;&#38454;&#27573;&#36890;&#36807;&#25968;&#25454;&#20381;&#36182;&#26041;&#24335;&#30340;&#26435;&#37325;&#37325;&#26032;&#21021;&#22987;&#21270;&#26377;&#36873;&#25321;&#22320;&#36951;&#24536;&#27169;&#22411;&#20013;&#30340;&#19981;&#33391;&#20449;&#24687;&#65292;&#32780;&#37325;&#23398;&#38454;&#27573;&#21017;&#24378;&#35843;&#23545;&#27867;&#21270;&#24615;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) are often trained on the premise that the complete training data set is provided ahead of time. However, in real-world scenarios, data often arrive in chunks over time. This leads to important considerations about the optimal strategy for training DNNs, such as whether to fine-tune them with each chunk of incoming data (warm-start) or to retrain them from scratch with the entire corpus of data whenever a new chunk is available. While employing the latter for training can be resource-intensive, recent work has pointed out the lack of generalization in warm-start models. Therefore, to strike a balance between efficiency and generalization, we introduce Learn, Unlearn, and Relearn (LURE) an online learning paradigm for DNNs. LURE interchanges between the unlearning phase, which selectively forgets the undesirable information in the model through weight reinitialization in a data-dependent manner, and the relearning phase, which emphasizes learning on generaliza
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38899;&#39057;&#20449;&#21495;&#22788;&#29702;&#30340;&#20869;&#23481;&#33258;&#36866;&#24212;&#21487;&#23398;&#20064;&#26102;&#39057;&#34920;&#31034;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#21367;&#31215;&#28388;&#27874;&#22120;&#19982;&#21464;&#25442;&#22120;&#26550;&#26500;&#26469;&#23558;&#23567;&#30340;&#27874;&#24418;&#22359;&#25237;&#24433;&#21040;&#23567;&#30340;&#28508;&#22312;&#32500;&#24230;&#19978;&#12290;</title><link>http://arxiv.org/abs/2303.10446</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#38899;&#39057;&#20449;&#21495;&#22788;&#29702;&#30340;&#20869;&#23481;&#33258;&#36866;&#24212;&#21487;&#23398;&#20064;&#26102;&#39057;&#34920;&#31034;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Content Adaptive Learnable Time-Frequency Representation For Audio Signal Processing. (arXiv:2303.10446v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10446
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38899;&#39057;&#20449;&#21495;&#22788;&#29702;&#30340;&#20869;&#23481;&#33258;&#36866;&#24212;&#21487;&#23398;&#20064;&#26102;&#39057;&#34920;&#31034;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#21367;&#31215;&#28388;&#27874;&#22120;&#19982;&#21464;&#25442;&#22120;&#26550;&#26500;&#26469;&#23558;&#23567;&#30340;&#27874;&#24418;&#22359;&#25237;&#24433;&#21040;&#23567;&#30340;&#28508;&#22312;&#32500;&#24230;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#20869;&#23481;&#33258;&#36866;&#24212;&#21069;&#31471;&#65292;&#29992;&#20110;&#38899;&#39057;&#20449;&#21495;&#22788;&#29702;&#12290;&#22312;&#28145;&#24230;&#23398;&#20064;&#30340;&#29616;&#20195;&#20986;&#29616;&#20043;&#21069;&#65292;&#25105;&#20204;&#20351;&#29992;&#22266;&#23450;&#34920;&#31034;&#30340;&#12289;&#19981;&#21487;&#23398;&#20064;&#30340;&#21069;&#31471;&#65292;&#22914;&#35889;&#22270;&#25110;&#26757;&#23572;&#35889;&#22270;&#65292;&#24102;/&#19981;&#24102;&#31070;&#32463;&#32467;&#26500;&#12290;&#38543;&#30528;&#21367;&#31215;&#26550;&#26500;&#25903;&#25345;ASR&#21644;&#22768;&#23398;&#22330;&#26223;&#29702;&#35299;&#31561;&#21508;&#31181;&#24212;&#29992;&#65292;&#36716;&#21521;&#21487;&#23398;&#20064;&#21069;&#31471;&#65292;&#21363;&#20174;&#22836;&#24320;&#22987;&#23398;&#20064;&#21644;&#20248;&#21270;&#29305;&#23450;&#20219;&#21153;&#25152;&#38656;&#30340;&#22522;&#30784;&#20989;&#25968;&#21644;&#26435;&#37325;&#12290;&#22312;&#27809;&#26377;&#21367;&#31215;&#22359;&#30340;&#21464;&#24418;&#22120;&#26550;&#26500;&#20013;&#65292;&#32447;&#24615;&#23618;&#23558;&#23567;&#30340;&#27874;&#24418;&#22359;&#25237;&#24433;&#21040;&#23567;&#30340;&#28508;&#22312;&#32500;&#24230;&#19978;&#65292;&#28982;&#21518;&#23558;&#23427;&#20204;&#39304;&#36865;&#21040;&#21464;&#24418;&#22120;&#26550;&#26500;&#20013;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#20869;&#23481;&#33258;&#36866;&#24212;&#23398;&#20064;&#26102;&#39057;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a learnable content adaptive front end for audio signal processing. Before the modern advent of deep learning, we used fixed representation non-learnable front-ends like spectrogram or mel-spectrogram with/without neural architectures. With convolutional architectures supporting various applications such as ASR and acoustic scene understanding, a shift to a learnable front ends occurred in which both the type of basis functions and the weight were learned from scratch and optimized for the particular task of interest. With the shift to transformer-based architectures with no convolutional blocks present, a linear layer projects small waveform patches onto a small latent dimension before feeding them to a transformer architecture. In this work, we propose a way of computing a content-adaptive learnable time-frequency representation. We pass each audio signal through a bank of convolutional filters, each giving a fixed-dimensional vector. It is akin to learning a bank of finit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EarCough&#30340;&#36731;&#37327;&#32423;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;Hearables&#19978;&#23454;&#29616;&#36830;&#32493;&#20027;&#39064;&#21683;&#22013;&#20107;&#20214;&#26816;&#27979;&#65292;&#20934;&#30830;&#24230;&#39640;&#36798;95.4&#65285;&#65292;F1&#20998;&#25968;&#20026;92.9&#65285;&#65292;&#20165;&#38656;385 KB&#30340;&#31354;&#38388;&#12290;&#36825;&#23558;&#25104;&#20026;&#26410;&#26469;Hearables&#30340;&#20302;&#25104;&#26412;&#22686;&#20540;&#21151;&#33021;&#65292;&#20026;&#21683;&#22013;&#30417;&#27979;&#25216;&#26415;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.10445</link><description>&lt;p&gt;
EarCough&#65306;&#22312;Hearables&#19978;&#23454;&#29616;&#36830;&#32493;&#20027;&#39064;&#21683;&#22013;&#20107;&#20214;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
EarCough: Enabling Continuous Subject Cough Event Detection on Hearables. (arXiv:2303.10445v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10445
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EarCough&#30340;&#36731;&#37327;&#32423;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;Hearables&#19978;&#23454;&#29616;&#36830;&#32493;&#20027;&#39064;&#21683;&#22013;&#20107;&#20214;&#26816;&#27979;&#65292;&#20934;&#30830;&#24230;&#39640;&#36798;95.4&#65285;&#65292;F1&#20998;&#25968;&#20026;92.9&#65285;&#65292;&#20165;&#38656;385 KB&#30340;&#31354;&#38388;&#12290;&#36825;&#23558;&#25104;&#20026;&#26410;&#26469;Hearables&#30340;&#20302;&#25104;&#26412;&#22686;&#20540;&#21151;&#33021;&#65292;&#20026;&#21683;&#22013;&#30417;&#27979;&#25216;&#26415;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21683;&#22013;&#30417;&#27979;&#21487;&#20197;&#23454;&#29616;&#26032;&#30340;&#20010;&#20307;&#32954;&#20581;&#24247;&#24212;&#29992;&#12290;&#20027;&#39064;&#21683;&#22013;&#20107;&#20214;&#26816;&#27979;&#26159;&#36830;&#32493;&#21683;&#22013;&#30417;&#27979;&#30340;&#22522;&#30784;&#12290;&#36817;&#24180;&#26469;&#65292;&#26234;&#33021;&#21487;&#31359;&#25140;&#35774;&#22791;&#30340;&#24555;&#36895;&#22686;&#38271;&#20026;&#36825;&#31181;&#38656;&#27714;&#24320;&#36767;&#20102;&#26032;&#30340;&#26426;&#20250;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;EarCough&#65292;&#36890;&#36807;&#21033;&#29992;&#22987;&#32456;&#22788;&#20110;&#27963;&#21160;&#22122;&#22768;&#25269;&#28040;&#65288;ANC&#65289;&#27169;&#24335;&#19979;&#30340;&#40614;&#20811;&#39118;&#65292;&#20351;Hearables&#19978;&#23454;&#29616;&#20102;&#36830;&#32493;&#20027;&#39064;&#21683;&#22013;&#20107;&#20214;&#26816;&#27979;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#31471;&#21040;&#31471;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;-EarCoughNet&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#36890;&#36807;&#29992;&#25143;&#30740;&#31350;&#26500;&#24314;&#20102;&#19968;&#20010;&#21516;&#27493;&#30340;&#36816;&#21160;&#21644;&#38899;&#39057;&#25968;&#25454;&#38598;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;EarCough&#22312;&#20165;&#38656;&#35201;385 kB&#30340;&#31354;&#38388;&#35201;&#27714;&#19979;&#65292;&#23454;&#29616;&#20102;95.4&#65285;&#30340;&#20934;&#30830;&#24230;&#21644;92.9&#65285;&#30340;F1&#20998;&#25968;&#12290;&#25105;&#20204;&#39044;&#35745;EarCough&#23558;&#25104;&#20026;&#26410;&#26469;Hearables&#30340;&#20302;&#25104;&#26412;&#22686;&#20540;&#21151;&#33021;&#65292;&#23454;&#29616;&#36830;&#32493;&#20027;&#39064;&#21683;&#22013;&#20107;&#20214;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cough monitoring can enable new individual pulmonary health applications. Subject cough event detection is the foundation for continuous cough monitoring. Recently, the rapid growth in smart hearables has opened new opportunities for such needs. This paper proposes EarCough, which enables continuous subject cough event detection on edge computing hearables by leveraging the always-on active noise cancellation (ANC) microphones. Specifically, we proposed a lightweight end-to-end neural network model -EarCoughNet. To evaluate the effectiveness of our method, we constructed a synchronous motion and audio dataset through a user study. Results show that EarCough achieved an accuracy of 95.4% and an F1-score of 92.9% with a space requirement of only 385 kB. We envision EarCough as a low-cost add-on for future hearables to enable continuous subject cough event detection.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#20302;&#20998;&#36776;&#29575;&#22270;&#20687;&#20256;&#24863;&#22120;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#20013;&#38544;&#31169;&#20445;&#25252;&#19982;&#27963;&#21160;&#35782;&#21035;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#36890;&#36807;&#23545;&#22270;&#20687;&#20998;&#36776;&#29575;&#30340;&#35843;&#25972;&#21644;&#29616;&#20195;&#22270;&#20687;&#36229;&#20998;&#36776;&#25216;&#26415;&#30340;&#24212;&#29992;&#65292;&#24110;&#21161;&#35774;&#35745;&#26356;&#26377;&#25928;&#30340;&#38544;&#31169;&#20445;&#25252;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2303.10435</link><description>&lt;p&gt;
&#20302;&#20998;&#36776;&#29575;&#22270;&#20687;&#38544;&#31169;&#20445;&#25252;&#19982;&#27963;&#21160;&#35782;&#21035;&#30340;&#26435;&#34913;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Modeling the Trade-off of Privacy Preservation and Activity Recognition on Low-Resolution Images. (arXiv:2303.10435v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10435
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#20302;&#20998;&#36776;&#29575;&#22270;&#20687;&#20256;&#24863;&#22120;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#20013;&#38544;&#31169;&#20445;&#25252;&#19982;&#27963;&#21160;&#35782;&#21035;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#36890;&#36807;&#23545;&#22270;&#20687;&#20998;&#36776;&#29575;&#30340;&#35843;&#25972;&#21644;&#29616;&#20195;&#22270;&#20687;&#36229;&#20998;&#36776;&#25216;&#26415;&#30340;&#24212;&#29992;&#65292;&#24110;&#21161;&#35774;&#35745;&#26356;&#26377;&#25928;&#30340;&#38544;&#31169;&#20445;&#25252;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#20302;&#20998;&#36776;&#29575;&#22270;&#20687;&#20256;&#24863;&#22120;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#21487;&#20197;&#25552;&#20379;&#26234;&#33021;&#26381;&#21153;&#65288;&#20363;&#22914;&#27963;&#21160;&#35782;&#21035;&#65289;&#65292;&#21516;&#26102;&#20174;&#30828;&#20214;&#32423;&#21035;&#20445;&#30041;&#19981;&#24517;&#35201;&#30340;&#35270;&#35273;&#38544;&#31169;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#20445;&#25252;&#35270;&#35273;&#38544;&#31169;&#21644;&#23454;&#29616;&#20934;&#30830;&#30340;&#26426;&#22120;&#35782;&#21035;&#23545;&#22270;&#20687;&#20998;&#36776;&#29575;&#26377;&#23545;&#25239;&#24615;&#30340;&#38656;&#27714;&#12290;&#26412;&#25991;&#36890;&#36807;&#27169;&#25311;&#26085;&#24120;&#29983;&#27963;&#27963;&#21160;&#24773;&#22659;&#65292;&#39318;&#20808;&#36890;&#36807;&#29992;&#25143;&#35843;&#26597;&#33719;&#21462;&#20102;&#26368;&#37325;&#35201;&#30340;&#35270;&#35273;&#38544;&#31169;&#29305;&#24449;&#12290;&#28982;&#21518;&#37327;&#21270;&#21644;&#20998;&#26512;&#20102;&#22270;&#20687;&#20998;&#36776;&#29575;&#23545;&#20154;&#21644;&#26426;&#22120;&#35782;&#21035;&#24615;&#33021;&#20197;&#21450;&#38544;&#31169;&#24847;&#35782;&#20219;&#21153;&#20013;&#30340;&#24433;&#21709;&#65292;&#24182;&#35843;&#26597;&#20102;&#29616;&#20195;&#22270;&#20687;&#36229;&#20998;&#36776;&#25216;&#26415;&#22914;&#20309;&#24433;&#21709;&#36825;&#20123;&#25928;&#26524;&#12290;&#26681;&#25454;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24314;&#27169;&#20302;&#20998;&#36776;&#29575;&#22270;&#20687;&#38544;&#31169;&#20445;&#25252;&#19982;&#27963;&#21160;&#35782;&#21035;&#26435;&#34913;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#24110;&#21161;&#35774;&#35745;&#26356;&#26377;&#25928;&#30340;&#38544;&#31169;&#20445;&#25252;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
A computer vision system using low-resolution image sensors can provide intelligent services (e.g., activity recognition) but preserve unnecessary visual privacy information from the hardware level. However, preserving visual privacy and enabling accurate machine recognition have adversarial needs on image resolution. Modeling the trade-off of privacy preservation and machine recognition performance can guide future privacy-preserving computer vision systems using low-resolution image sensors. In this paper, using the at-home activity of daily livings (ADLs) as the scenario, we first obtained the most important visual privacy features through a user survey. Then we quantified and analyzed the effects of image resolution on human and machine recognition performance in activity recognition and privacy awareness tasks. We also investigated how modern image super-resolution techniques influence these effects. Based on the results, we proposed a method for modeling the trade-off of privacy 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#20154;&#31867;&#32534;&#20889;&#30340;&#22312;&#32447;&#25200;&#21160;&#30340;&#27979;&#35797;&#38598;&#65292;&#29992;&#20110;&#27602;&#24615;&#35328;&#35770;&#26816;&#27979;&#27169;&#22411;&#30340;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2303.10430</link><description>&lt;p&gt;
NoisyHate&#65306;&#22312;&#20154;&#31867;&#32534;&#20889;&#30340;&#22312;&#32447;&#25200;&#21160;&#19979;&#23545;&#20869;&#23481;&#23457;&#26680;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
NoisyHate: Benchmarking Content Moderation Machine Learning Models with Human-Written Perturbations Online. (arXiv:2303.10430v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10430
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#20154;&#31867;&#32534;&#20889;&#30340;&#22312;&#32447;&#25200;&#21160;&#30340;&#27979;&#35797;&#38598;&#65292;&#29992;&#20110;&#27602;&#24615;&#35328;&#35770;&#26816;&#27979;&#27169;&#22411;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#65292;&#20855;&#26377;&#26377;&#23475;&#20869;&#23481;&#30340;&#22312;&#32447;&#25991;&#26412;&#26159;&#19968;&#31181;&#23041;&#32961;&#65292;&#21487;&#33021;&#20250;&#24341;&#36215;&#32593;&#32476;&#39578;&#25200;&#12290;&#23613;&#31649;&#35768;&#22810;&#24179;&#21488;&#37319;&#21462;&#20102;&#25514;&#26045;&#65292;&#20363;&#22914;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#31995;&#32479;&#26469;&#20943;&#23569;&#20854;&#24433;&#21709;&#65292;&#20294;&#37027;&#20123;&#26377;&#23475;&#20869;&#23481;&#21457;&#24067;&#32773;&#20173;&#28982;&#21487;&#20197;&#36890;&#36807;&#20462;&#25913;&#26377;&#23475;&#35789;&#27719;&#30340;&#25340;&#20889;&#26469;&#36867;&#36991;&#31995;&#32479;&#12290;&#36825;&#20123;&#20462;&#25913;&#21518;&#30340;&#21333;&#35789;&#20063;&#31216;&#20026;&#20154;&#31867;&#32534;&#20889;&#30340;&#25991;&#26412;&#25200;&#21160;&#12290;&#35768;&#22810;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#23450;&#30340;&#25216;&#26415;&#26469;&#29983;&#25104;&#23545;&#25239;&#26679;&#26412;&#65292;&#20197;&#24110;&#21161;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33719;&#24471;&#35782;&#21035;&#36825;&#20123;&#25200;&#21160;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#26426;&#22120;&#29983;&#25104;&#30340;&#25200;&#21160;&#19982;&#20154;&#31867;&#32534;&#20889;&#30340;&#25200;&#21160;&#20043;&#38388;&#20173;&#23384;&#22312;&#24046;&#36317;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;&#20154;&#31867;&#32534;&#20889;&#30340;&#22312;&#32447;&#25200;&#21160;&#30340;&#22522;&#20934;&#27979;&#35797;&#38598;&#65292;&#29992;&#20110;&#27602;&#24615;&#35328;&#35770;&#26816;&#27979;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#25307;&#21215;&#20102;&#19968;&#32452;&#24037;&#20154;&#26469;&#35780;&#20272;&#27492;&#27979;&#35797;&#38598;&#30340;&#36136;&#37327;&#24182;&#21024;&#38500;&#20302;&#36136;&#37327;&#30340;&#26679;&#26412;&#12290;&#21516;&#26102;&#65292;&#20026;&#20102;&#26816;&#26597;&#25105;&#20204;&#30340;&#25200;&#21160;&#26159;&#21542;&#21487;&#20197;&#24402;&#19968;&#21270;&#20026;&#20854;&#24178;&#20928;&#29256;&#26412;&#65292;&#25105;&#20204;&#36824;&#21019;&#24314;&#20102;&#19968;&#20010;&#30456;&#20851;&#30340;&#27979;&#35797;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online texts with toxic content are a threat in social media that might cause cyber harassment. Although many platforms applied measures, such as machine learning-based hate-speech detection systems, to diminish their effect, those toxic content publishers can still evade the system by modifying the spelling of toxic words. Those modified words are also known as human-written text perturbations. Many research works developed certain techniques to generate adversarial samples to help the machine learning models obtain the ability to recognize those perturbations. However, there is still a gap between those machine-generated perturbations and human-written perturbations. In this paper, we introduce a benchmark test set containing human-written perturbations online for toxic speech detection models. We also recruited a group of workers to evaluate the quality of this test set and dropped low-quality samples. Meanwhile, to check if our perturbation can be normalized to its clean version, w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25209;&#37327;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#34507;&#30333;&#36136;&#24207;&#21015;&#35774;&#35745;&#26032;&#26041;&#27861;&#65292;&#33021;&#26356;&#26126;&#26234;&#22320;&#20915;&#23450;&#36873;&#25321;&#21738;&#20123;&#24207;&#21015;&#36827;&#34892;&#20154;&#24037;&#36827;&#21270;&#65292;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#26356;&#24555;&#30340;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2303.10429</link><description>&lt;p&gt;
&#25209;&#37327;&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#34507;&#30333;&#36136;&#24207;&#21015;&#35774;&#35745;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Protein Sequence Design with Batch Bayesian Optimisation. (arXiv:2303.10429v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10429
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25209;&#37327;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#34507;&#30333;&#36136;&#24207;&#21015;&#35774;&#35745;&#26032;&#26041;&#27861;&#65292;&#33021;&#26356;&#26126;&#26234;&#22320;&#20915;&#23450;&#36873;&#25321;&#21738;&#20123;&#24207;&#21015;&#36827;&#34892;&#20154;&#24037;&#36827;&#21270;&#65292;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#26356;&#24555;&#30340;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34507;&#30333;&#36136;&#24207;&#21015;&#35774;&#35745;&#26159;&#34507;&#30333;&#24037;&#31243;&#20013;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#26088;&#22312;&#21457;&#29616;&#20855;&#26377;&#26377;&#29992;&#29983;&#29289;&#21151;&#33021;&#30340;&#26032;&#22411;&#34507;&#30333;&#36136;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25209;&#37327;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;Batch BO&#65289;&#30340;&#34507;&#30333;&#36136;&#24207;&#21015;&#35774;&#35745;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;Batch BO&#38598;&#25104;&#21040;&#23450;&#21521;&#36827;&#21270;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#26356;&#26126;&#26234;&#22320;&#20915;&#23450;&#36873;&#25321;&#21738;&#20123;&#24207;&#21015;&#36827;&#34892;&#20154;&#24037;&#36827;&#21270;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#26356;&#24555;&#30340;&#25910;&#25947;&#12290;&#25105;&#20204;&#22312;&#19968;&#32452;&#34394;&#25311;&#34507;&#30333;&#36136;&#24207;&#21015;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Protein sequence design is a challenging problem in protein engineering, which aims to discover novel proteins with useful biological functions. Directed evolution is a widely-used approach for protein sequence design, which mimics the evolution cycle in a laboratory environment and conducts an iterative protocol. However, the burden of laboratory experiments can be reduced by using machine learning approaches to build a surrogate model of the protein landscape and conducting in-silico population selection through model-based fitness prediction. In this paper, we propose a new method based on Batch Bayesian Optimization (Batch BO), a well-established optimization method, for protein sequence design. By incorporating Batch BO into the directed evolution process, our method is able to make more informed decisions about which sequences to select for artificial evolution, leading to improved performance and faster convergence. We evaluate our method on a suite of in-silico protein sequence
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25512;&#26029;&#21487;&#35266;&#23519;&#26102;&#38388;&#24207;&#21015;&#25152;&#26263;&#31034;&#30340;&#22266;&#26377;&#28508;&#22312;&#22240;&#32032;&#26469;&#36827;&#34892;&#20449;&#21495;&#25104;&#20998;&#20998;&#35299;&#24182;&#37325;&#26500;&#26410;&#26469;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#19981;&#20165;&#33021;&#22815;&#23454;&#29616;&#38271;&#26399;&#25928;&#29575;&#30340;&#31232;&#30095;&#20851;&#31995;&#25512;&#29702;&#65292;&#32780;&#19988;&#36824;&#33021;&#22815;&#35299;&#20915;&#26102;&#38388;&#24207;&#21015;&#22312;&#32570;&#20047;&#36275;&#22815;&#21464;&#37327;&#19979;&#30340;&#24314;&#27169;&#19981;&#31283;&#23450;&#21644;&#19981;&#21487;&#39044;&#27979;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.10426</link><description>&lt;p&gt;
&#21457;&#29616;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#21487;&#39044;&#27979;&#30340;&#28508;&#22312;&#22240;&#32032;
&lt;/p&gt;
&lt;p&gt;
Discovering Predictable Latent Factors for Time Series Forecasting. (arXiv:2303.10426v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10426
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25512;&#26029;&#21487;&#35266;&#23519;&#26102;&#38388;&#24207;&#21015;&#25152;&#26263;&#31034;&#30340;&#22266;&#26377;&#28508;&#22312;&#22240;&#32032;&#26469;&#36827;&#34892;&#20449;&#21495;&#25104;&#20998;&#20998;&#35299;&#24182;&#37325;&#26500;&#26410;&#26469;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#19981;&#20165;&#33021;&#22815;&#23454;&#29616;&#38271;&#26399;&#25928;&#29575;&#30340;&#31232;&#30095;&#20851;&#31995;&#25512;&#29702;&#65292;&#32780;&#19988;&#36824;&#33021;&#22815;&#35299;&#20915;&#26102;&#38388;&#24207;&#21015;&#22312;&#32570;&#20047;&#36275;&#22815;&#21464;&#37327;&#19979;&#30340;&#24314;&#27169;&#19981;&#31283;&#23450;&#21644;&#19981;&#21487;&#39044;&#27979;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#65292;&#22914;Transformer&#21450;&#20854;&#21464;&#31181;&#65292;&#24050;&#32463;&#22312;&#39034;&#24207;&#25968;&#25454;&#24314;&#27169;&#26041;&#38754;&#23637;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#23454;&#29616;&#39640;&#24615;&#33021;&#65292;&#23427;&#20204;&#36890;&#24120;&#20381;&#38752;&#20887;&#20313;&#30340;&#25110;&#19981;&#21487;&#35299;&#37322;&#30340;&#32467;&#26500;&#26469;&#24314;&#27169;&#21464;&#37327;&#20043;&#38388;&#22797;&#26434;&#30340;&#20851;&#31995;&#65292;&#24182;&#20351;&#29992;&#22823;&#35268;&#27169;&#25968;&#25454;&#35843;&#25972;&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#25366;&#25496;&#20219;&#21153;&#32570;&#20047;&#36275;&#22815;&#30340;&#21464;&#37327;&#26469;&#36827;&#34892;&#20851;&#31995;&#25512;&#29702;&#65292;&#22240;&#27492;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#26080;&#27861;&#27491;&#30830;&#22788;&#29702;&#36825;&#31181;&#39044;&#27979;&#38382;&#39064;&#12290;&#30001;&#20110;&#32570;&#20047;&#25968;&#25454;&#65292;&#26102;&#38388;&#24207;&#21015;&#20284;&#20046;&#21463;&#21040;&#35768;&#22810;&#22806;&#29983;&#21464;&#37327;&#30340;&#24433;&#21709;&#65292;&#22240;&#27492;&#24314;&#27169;&#21464;&#24471;&#19981;&#31283;&#23450;&#21644;&#19981;&#21487;&#39044;&#27979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#26694;&#26550;&#26469;&#25512;&#26029;&#21487;&#35266;&#23519;&#26102;&#38388;&#24207;&#21015;&#25152;&#26263;&#31034;&#30340;&#22266;&#26377;&#28508;&#22312;&#22240;&#32032;&#12290;&#25512;&#26029;&#20986;&#30340;&#22240;&#32032;&#34987;&#29992;&#26469;&#24418;&#25104;&#22810;&#20010;&#29420;&#31435;&#21487;&#39044;&#27979;&#30340;&#20449;&#21495;&#25104;&#20998;&#65292;&#36825;&#19981;&#20165;&#33021;&#22815;&#23454;&#29616;&#38271;&#26399;&#25928;&#29575;&#30340;&#31232;&#30095;&#20851;&#31995;&#25512;&#29702;&#65292;&#32780;&#19988;&#36824;&#33021;&#37325;&#26500;&#26410;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern time series forecasting methods, such as Transformer and its variants, have shown strong ability in sequential data modeling. To achieve high performance, they usually rely on redundant or unexplainable structures to model complex relations between variables and tune the parameters with large-scale data. Many real-world data mining tasks, however, lack sufficient variables for relation reasoning, and therefore these methods may not properly handle such forecasting problems. With insufficient data, time series appear to be affected by many exogenous variables, and thus, the modeling becomes unstable and unpredictable. To tackle this critical issue, in this paper, we develop a novel algorithmic framework for inferring the intrinsic latent factors implied by the observable time series. The inferred factors are used to form multiple independent and predictable signal components that enable not only sparse relation reasoning for long-term efficiency but also reconstructing the future
&lt;/p&gt;</description></item><item><title>ExplainFix&#37319;&#29992;&#22266;&#23450;&#28388;&#27874;&#22120;&#21644;&#31934;&#31616;&#30340;&#32593;&#32476;&#21442;&#25968;&#65292;&#25552;&#39640;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#35757;&#32451;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.10408</link><description>&lt;p&gt;
ExplainFix: &#21487;&#35299;&#37322;&#30340;&#22266;&#23450;&#31354;&#38388;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
ExplainFix: Explainable Spatially Fixed Deep Networks. (arXiv:2303.10408v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10408
&lt;/p&gt;
&lt;p&gt;
ExplainFix&#37319;&#29992;&#22266;&#23450;&#28388;&#27874;&#22120;&#21644;&#31934;&#31616;&#30340;&#32593;&#32476;&#21442;&#25968;&#65292;&#25552;&#39640;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#35757;&#32451;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#20010;&#35774;&#35745;&#21407;&#21017;&#65306;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#25152;&#26377;&#31354;&#38388;&#28388;&#27874;&#22120;&#26435;&#37325;&#21487;&#20197;&#22312;&#21021;&#22987;&#21270;&#26102;&#22266;&#23450;&#65292;&#32780;&#19981;&#24517;&#23398;&#20064;&#65307;&#21482;&#38656;&#24456;&#23569;&#30340;&#32593;&#32476;&#21442;&#25968;&#21363;&#21487;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#21487;&#35270;&#21270;&#27169;&#22411;&#35299;&#37322;&#12289;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#25552;&#21319;&#20197;&#21450;&#26032;&#30340;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#24037;&#20855;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;ExplainFix&#27169;&#22411;&#27604;&#23436;&#20840;&#23398;&#20064;&#30340;&#27169;&#22411;&#23569;&#20102;&#22810;&#36798;100&#20493;&#30340;&#31354;&#38388;&#28388;&#27874;&#22120;&#26680;&#65292;&#32780;&#21305;&#37197;&#25110;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Is there an initialization for deep networks that requires no learning? ExplainFix adopts two design principles: the "fixed filters" principle that all spatial filter weights of convolutional neural networks can be fixed at initialization and never learned, and the "nimbleness" principle that only few network parameters suffice. We contribute (a) visual model-based explanations, (b) speed and accuracy gains, and (c) novel tools for deep convolutional neural networks. ExplainFix gives key insights that spatially fixed networks should have a steered initialization, that spatial convolution layers tend to prioritize low frequencies, and that most network parameters are not necessary in spatially fixed models. ExplainFix models have up to 100x fewer spatial filter kernels than fully learned models and matching or improved accuracy. Our extensive empirical analysis confirms that ExplainFix guarantees nimbler models (train up to 17\% faster with channel pruning), matching or improved predict
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24191;&#20041;&#30340;&#28145;&#24230;&#19977;&#32500;&#24418;&#29366;&#20808;&#39564;&#27169;&#22411;&#65292;&#21487;&#20197;&#29992;&#20110;&#22810;&#31181;&#19977;&#32500;&#20219;&#21153;&#65292;&#20854;&#20351;&#29992;&#21521;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#21644;&#31163;&#25955;&#25193;&#25955;&#29983;&#25104;&#22120;&#26469;&#31934;&#30830;&#25429;&#33719;&#23616;&#37096;&#31934;&#32454;&#30340;&#24418;&#29366;&#20449;&#24687;&#65292;&#24182;&#24341;&#20837;&#22810;&#39057;&#29575;&#34701;&#21512;&#27169;&#22359;&#26469;&#25233;&#21046;&#39640;&#39057;&#24418;&#29366;&#29305;&#24449;&#27874;&#21160;&#65292;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#20854;&#22312;&#21508;&#31181;&#19977;&#32500;&#24418;&#29366;&#29983;&#25104;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.10406</link><description>&lt;p&gt;
3DQD: &#22522;&#20110;&#37096;&#20998;&#31163;&#25955;&#25193;&#25955;&#36807;&#31243;&#30340;&#24191;&#20041;&#28145;&#24230;&#19977;&#32500;&#24418;&#29366;&#20808;&#39564;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
3DQD: Generalized Deep 3D Shape Prior via Part-Discretized Diffusion Process. (arXiv:2303.10406v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10406
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24191;&#20041;&#30340;&#28145;&#24230;&#19977;&#32500;&#24418;&#29366;&#20808;&#39564;&#27169;&#22411;&#65292;&#21487;&#20197;&#29992;&#20110;&#22810;&#31181;&#19977;&#32500;&#20219;&#21153;&#65292;&#20854;&#20351;&#29992;&#21521;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#21644;&#31163;&#25955;&#25193;&#25955;&#29983;&#25104;&#22120;&#26469;&#31934;&#30830;&#25429;&#33719;&#23616;&#37096;&#31934;&#32454;&#30340;&#24418;&#29366;&#20449;&#24687;&#65292;&#24182;&#24341;&#20837;&#22810;&#39057;&#29575;&#34701;&#21512;&#27169;&#22359;&#26469;&#25233;&#21046;&#39640;&#39057;&#24418;&#29366;&#29305;&#24449;&#27874;&#21160;&#65292;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#20854;&#22312;&#21508;&#31181;&#19977;&#32500;&#24418;&#29366;&#29983;&#25104;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#24191;&#20041;&#30340;&#19977;&#32500;&#24418;&#29366;&#29983;&#25104;&#20808;&#39564;&#27169;&#22411;&#65292;&#19987;&#38376;&#38024;&#23545;&#22810;&#31181;&#19977;&#32500;&#20219;&#21153;&#65292;&#21253;&#25324;&#26080;&#26465;&#20214;&#24418;&#29366;&#29983;&#25104;&#12289;&#28857;&#20113;&#23436;&#25104;&#21644;&#36328;&#27169;&#24577;&#24418;&#29366;&#29983;&#25104;&#31561;&#12290;&#19968;&#26041;&#38754;&#65292;&#20026;&#20102;&#31934;&#30830;&#25429;&#33719;&#23616;&#37096;&#31934;&#32454;&#30340;&#24418;&#29366;&#20449;&#24687;&#65292;&#25105;&#20204;&#20351;&#29992;&#21521;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VQ-VAE&#65289;&#26469;&#22522;&#20110;&#24191;&#27867;&#30340;&#20219;&#21153;&#35757;&#32451;&#25968;&#25454;&#23398;&#20064;&#32039;&#20945;&#30340;&#30721;&#26412;&#24182;&#32034;&#24341;&#26412;&#22320;&#20960;&#20309;&#20449;&#24687;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31163;&#25955;&#25193;&#25955;&#29983;&#25104;&#22120;&#26469;&#24314;&#27169;&#19981;&#21516;&#26631;&#35760;&#20043;&#38388;&#22266;&#26377;&#30340;&#32467;&#26500;&#20381;&#36182;&#24615;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22810;&#39057;&#29575;&#34701;&#21512;&#27169;&#22359;&#65288;MFM&#65289;&#65292;&#20197;&#22810;&#39057;&#29575;&#19978;&#19979;&#25991;&#20449;&#24687;&#20026;&#25351;&#23548;&#65292;&#25233;&#21046;&#39640;&#39057;&#24418;&#29366;&#29305;&#24449;&#30340;&#27874;&#21160;&#12290;&#19978;&#36848;&#35774;&#35745;&#20849;&#21516;&#37197;&#22791;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#19977;&#32500;&#24418;&#29366;&#20808;&#39564;&#27169;&#22411;&#39640;&#20445;&#30495;&#24230;&#12289;&#22810;&#26679;&#24615;&#29305;&#24449;&#21644;&#36328;&#27169;&#24577;&#23545;&#40784;&#30340;&#33021;&#21147;&#65292;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#19977;&#32500;&#24418;&#29366;&#29983;&#25104;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a generalized 3D shape generation prior model, tailored for multiple 3D tasks including unconditional shape generation, point cloud completion, and cross-modality shape generation, etc. On one hand, to precisely capture local fine detailed shape information, a vector quantized variational autoencoder (VQ-VAE) is utilized to index local geometry from a compactly learned codebook based on a broad set of task training data. On the other hand, a discrete diffusion generator is introduced to model the inherent structural dependencies among different tokens. In the meantime, a multi-frequency fusion module (MFM) is developed to suppress high-frequency shape feature fluctuations, guided by multi-frequency contextual information. The above designs jointly equip our proposed 3D shape prior model with high-fidelity, diverse features as well as the capability of cross-modality alignment, and extensive experiments have demonstrated superior performances on various 3D shape generation ta
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#35299;&#37322;AI&#21644;Grad-Cam&#25216;&#26415;&#30340;&#26234;&#33021;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#26816;&#27979;ROI&#20197;&#39044;&#27979;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#12290;&#22312;176&#21517;MCI&#24739;&#32773;&#19978;&#23454;&#29616;&#24182;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.10401</link><description>&lt;p&gt;
&#22522;&#20110;&#21487;&#35299;&#37322;AI&#30340;&#26234;&#33021;ROI&#26816;&#27979;&#29992;&#20110;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Smart ROI Detection for Alzheimer's disease prediction using explainable AI. (arXiv:2303.10401v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10401
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#35299;&#37322;AI&#21644;Grad-Cam&#25216;&#26415;&#30340;&#26234;&#33021;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#26816;&#27979;ROI&#20197;&#39044;&#27979;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#12290;&#22312;176&#21517;MCI&#24739;&#32773;&#19978;&#23454;&#29616;&#24182;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;MCI&#24739;&#32773;&#21457;&#23637;&#25104;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#26159;&#20943;&#32531;&#35813;&#30149;&#36827;&#31243;&#30340;&#37325;&#35201;&#27493;&#39588;&#12290;&#22240;&#27492;&#65292;&#35768;&#22810;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#24050;&#34987;&#24341;&#20837;&#21040;&#36825;&#39033;&#20219;&#21153;&#20013;&#12290;&#22312;&#36825;&#20123;&#26041;&#27861;&#20013;&#65292;&#22522;&#20110;ROI&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#22797;&#26434;&#24615;&#26041;&#38754;&#22788;&#20110;&#33391;&#22909;&#30340;&#22320;&#20301;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#35299;&#37322;AI&#30340;&#26234;&#33021;&#26041;&#27861;&#26469;&#33258;&#21160;&#26816;&#27979;ROI&#65292;&#24182;&#20351;&#29992;&#25552;&#21462;&#30340;ROI&#36827;&#34892;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Purpose Predicting the progression of MCI to Alzheimer's disease is an important step in reducing the progression of the disease. Therefore, many methods have been introduced for this task based on deep learning. Among these approaches, the methods based on ROIs are in a good position in terms of accuracy and complexity. In these techniques, some specific parts of the brain are extracted as ROI manually for all of the patients. Extracting ROI manually is time-consuming and its results depend on human expertness and precision. Method To overcome these limitations, we propose a novel smart method for detecting ROIs automatically based on Explainable AI using Grad-Cam and a 3DCNN model that extracts ROIs per patient. After extracting the ROIs automatically, Alzheimer's disease is predicted using extracted ROI-based 3D CNN. Results We implement our method on 176 MCI patients of the famous ADNI dataset and obtain remarkable results compared to the state-of-the-art methods. The accuracy acqu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34562;&#31389;&#32593;&#32476;&#36830;&#25509;&#30340;&#26080;&#20154;&#26426;&#32676;&#25511;&#21046;&#26041;&#26696;&#65292;&#36890;&#36807;&#20004;&#38454;&#27573;&#21629;&#20196;&#19982;&#25511;&#21046;&#65288;C&amp;C&#65289;&#20256;&#36755;&#26041;&#26696;&#21644;&#35774;&#22791;&#23545;&#35774;&#22791;&#65288;D2D&#65289;&#36890;&#20449;&#26469;&#25552;&#39640;&#36890;&#20449;&#21487;&#38752;&#24615;&#21644;&#33021;&#28304;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.10398</link><description>&lt;p&gt;
&#33021;&#28304;&#39640;&#25928;&#30340;&#22522;&#20110;&#34562;&#31389;&#32593;&#32476;&#36830;&#25509;&#30340;&#26080;&#20154;&#26426;&#32676;&#25511;&#21046;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Energy-Efficient Cellular-Connected UAV Swarm Control Optimization. (arXiv:2303.10398v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10398
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34562;&#31389;&#32593;&#32476;&#36830;&#25509;&#30340;&#26080;&#20154;&#26426;&#32676;&#25511;&#21046;&#26041;&#26696;&#65292;&#36890;&#36807;&#20004;&#38454;&#27573;&#21629;&#20196;&#19982;&#25511;&#21046;&#65288;C&amp;C&#65289;&#20256;&#36755;&#26041;&#26696;&#21644;&#35774;&#22791;&#23545;&#35774;&#22791;&#65288;D2D&#65289;&#36890;&#20449;&#26469;&#25552;&#39640;&#36890;&#20449;&#21487;&#38752;&#24615;&#21644;&#33021;&#28304;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#34562;&#31389;&#32593;&#32476;&#36830;&#25509;&#30340;&#26080;&#20154;&#26426;&#32676;&#20307;&#26159;&#19968;&#31181;&#24191;&#27867;&#24212;&#29992;&#20110;&#36135;&#36816;&#21644;&#20132;&#36890;&#31649;&#29702;&#31561;&#39046;&#22495;&#30340;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#39640;&#21487;&#38752;&#12289;&#20302;&#24310;&#36831;&#12289;&#39640;&#33021;&#25928;&#22320;&#36890;&#20449;&#21644;&#25511;&#21046;&#26080;&#20154;&#26426;&#32676;&#20307;&#20173;&#28982;&#26159;&#19968;&#39033;&#25361;&#25112;&#12290;&#26412;&#25991;&#22312;&#22522;&#20110;&#34562;&#31389;&#32593;&#32476;&#36830;&#25509;&#30340;&#26080;&#20154;&#26426;&#32676;&#20307;&#20013;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#21629;&#20196;&#19982;&#25511;&#21046;&#65288;C&amp;C&#65289;&#20256;&#36755;&#26041;&#26696;&#65292;&#20854;&#20013;&#22320;&#38754;&#22522;&#31449;&#65288;GBS&#65289;&#22312;&#31532;&#19968;&#38454;&#27573;&#24191;&#25773;&#24120;&#35268;C&amp;C&#28040;&#24687;&#12290;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;&#24050;&#25104;&#21151;&#35299;&#30721;C&amp;C&#28040;&#24687;&#30340;&#26080;&#20154;&#26426;&#23558;&#36890;&#36807;&#35774;&#22791;&#23545;&#35774;&#22791;&#65288;D2D&#65289;&#36890;&#20449;&#20197;&#24191;&#25773;&#25110;&#21333;&#25773;&#27169;&#24335;&#23558;&#28040;&#24687;&#20013;&#32487;&#21040;&#20854;&#20313;&#26080;&#20154;&#26426;&#65292;&#20197;&#28385;&#36275;&#24310;&#36831;&#21644;&#33021;&#37327;&#38480;&#21046;&#19979;&#23613;&#21487;&#33021;&#22810;&#30340;&#26080;&#20154;&#26426;&#25104;&#21151;&#25509;&#25910;&#28040;&#24687;&#12290;&#20026;&#20102;&#25214;&#21040;&#26368;&#20248;&#31574;&#30053;&#20351;&#25104;&#21151;&#25509;&#25910;&#28040;&#24687;&#30340;&#26080;&#20154;&#26426;&#25968;&#26368;&#22823;&#21270;&#65292;&#22312;&#38480;&#21046;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#23558;&#38382;&#39064;&#21046;&#23450;&#20026;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#12290;&#20026;&#20102;&#35299;&#20915;&#27492;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#25955;&#30340;&#26377;&#32422;&#26463;Q&#23398;&#20064;&#31639;&#27861;&#26469;&#23398;&#20064;&#26368;&#20248;&#31574;&#30053;&#12290;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#26696;&#22312;&#33021;&#28304;&#25928;&#29575;&#21644;&#21487;&#38752;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cellular-connected unmanned aerial vehicle (UAV) swarm is a promising solution for diverse applications, including cargo delivery and traffic control. However, it is still challenging to communicate with and control the UAV swarm with high reliability, low latency, and high energy efficiency. In this paper, we propose a two-phase command and control (C&amp;C) transmission scheme in a cellular-connected UAV swarm network, where the ground base station (GBS) broadcasts the common C&amp;C message in Phase I. In Phase II, the UAVs that have successfully decoded the C&amp;C message will relay the message to the rest of UAVs via device-to-device (D2D) communications in either broadcast or unicast mode, under latency and energy constraints. To maximize the number of UAVs that receive the message successfully within the latency and energy constraints, we formulate the problem as a Constrained Markov Decision Process to find the optimal policy. To address this problem, we propose a decentralized constraine
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#34920;&#25512;&#29702;&#30340;&#24320;&#25918;&#24335;&#24120;&#35782;&#38382;&#39064;&#22238;&#31572;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;&#24120;&#35782;&#38382;&#39064;&#30340;&#38544;&#24335;&#22810;&#36339;&#25512;&#29702;&#24182;&#26500;&#24314;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#24320;&#25918;&#24335;&#30693;&#35782;&#22270;&#65292;&#26377;&#26395;&#22312;&#27809;&#26377;&#39044;&#23450;&#20041;&#31572;&#26696;&#36873;&#39033;&#30340;&#30495;&#23454;&#24773;&#22659;&#24212;&#29992;&#20013;&#21457;&#25381;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.10395</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#22270;&#34920;&#25512;&#29702;&#30340;&#24320;&#25918;&#24335;&#24120;&#35782;&#38382;&#39064;&#22238;&#31572;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Graph-Guided Reasoning Approach for Open-ended Commonsense Question Answering. (arXiv:2303.10395v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10395
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#34920;&#25512;&#29702;&#30340;&#24320;&#25918;&#24335;&#24120;&#35782;&#38382;&#39064;&#22238;&#31572;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;&#24120;&#35782;&#38382;&#39064;&#30340;&#38544;&#24335;&#22810;&#36339;&#25512;&#29702;&#24182;&#26500;&#24314;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#24320;&#25918;&#24335;&#30693;&#35782;&#22270;&#65292;&#26377;&#26395;&#22312;&#27809;&#26377;&#39044;&#23450;&#20041;&#31572;&#26696;&#36873;&#39033;&#30340;&#30495;&#23454;&#24773;&#22659;&#24212;&#29992;&#20013;&#21457;&#25381;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#38024;&#23545;&#22810;&#39033;&#36873;&#25321;&#30340;&#24120;&#35782;&#38382;&#39064;&#22238;&#31572;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#27169;&#22411;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#26080;&#27861;&#30452;&#25509;&#24212;&#29992;&#27492;&#31867;&#38382;&#31572;&#31995;&#32479;&#65292;&#22240;&#20026;&#27809;&#26377;&#25552;&#20379;&#31572;&#26696;&#20505;&#36873;&#20154;&#12290;&#22240;&#27492;&#65292;&#26368;&#36817;&#21457;&#24067;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#25361;&#25112;&#38598;&#65288;OpenCSR&#65289;&#29992;&#20110;&#24320;&#25918;&#24335;&#24120;&#35782;&#25512;&#29702;&#65292;&#20854;&#20013;&#21253;&#21547;&#33258;&#28982;&#31185;&#23398;&#38382;&#39064;&#32780;&#27809;&#26377;&#39044;&#23450;&#20041;&#30340;&#36873;&#39033;&#12290;&#22312;OpenCSR&#25361;&#25112;&#38598;&#20013;&#65292;&#35768;&#22810;&#38382;&#39064;&#38656;&#35201;&#38544;&#21547;&#30340;&#22810;&#36339;&#25512;&#29702;&#24182;&#19988;&#20915;&#31574;&#31354;&#38388;&#24456;&#22823;&#65292;&#21453;&#26144;&#20102;&#36825;&#39033;&#20219;&#21153;&#30340;&#22256;&#38590;&#24615;&#36136;&#12290;&#29616;&#26377;&#30340;OpenCSR&#24037;&#20316;&#20165;&#30528;&#30524;&#20110;&#25913;&#36827;&#26816;&#32034;&#36807;&#31243;&#65292;&#20174;&#25991;&#26412;&#30693;&#35782;&#24211;&#20013;&#25552;&#21462;&#30456;&#20851;&#20107;&#23454;&#21477;&#23376;&#65292;&#32780;&#23558;&#37325;&#35201;&#19988;&#38750;&#24179;&#20961;&#30340;&#25512;&#29702;&#20219;&#21153;&#36229;&#20986;&#33539;&#22260;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25193;&#23637;&#33539;&#22260;&#65292;&#21253;&#25324;&#19968;&#20010;&#22522;&#20110;&#26816;&#32034;&#25903;&#25745;&#20107;&#23454;&#26500;&#24314;&#38382;&#39064;&#30456;&#20851;&#24320;&#25918;&#30693;&#35782;&#22270;&#30340;&#25512;&#29702;&#22120;&#65292;&#24182;&#37319;&#29992;&#39034;&#24207;
&lt;/p&gt;
&lt;p&gt;
Recently, end-to-end trained models for multiple-choice commonsense question answering (QA) have delivered promising results. However, such question-answering systems cannot be directly applied in real-world scenarios where answer candidates are not provided. Hence, a new benchmark challenge set for open-ended commonsense reasoning (OpenCSR) has been recently released, which contains natural science questions without any predefined choices. On the OpenCSR challenge set, many questions require implicit multi-hop reasoning and have a large decision space, reflecting the difficult nature of this task. Existing work on OpenCSR sorely focuses on improving the retrieval process, which extracts relevant factual sentences from a textual knowledge base, leaving the important and non-trivial reasoning task outside the scope. In this work, we extend the scope to include a reasoner that constructs a question-dependent open knowledge graph based on retrieved supporting facts and employs a sequentia
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;WFST&#26694;&#26550;&#30340;&#30340;RNN-Transducer Losses&#24378;&#22823;&#19988;&#21487;&#25193;&#23637;&#30340;&#23454;&#29616;&#65292;&#8220;Compose-Transducer&#8221;&#21644;&#8220;Grid-Transducer&#8221;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#30340;W-Transducer Loss&#26469;&#23637;&#31034;&#32452;&#20214;&#30340;&#26131;&#25193;&#23637;&#24615;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;W-Transducer&#65288;W-RNNT&#65289;&#34920;&#29616;&#20986;&#27604;&#26631;&#20934;RNN-T&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.10384</link><description>&lt;p&gt;
&#22522;&#20110;WFST&#26694;&#26550;&#30340;RNN-Transducer Losses&#24378;&#22823;&#19988;&#21487;&#25193;&#23637;&#30340;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
Powerful and Extensible WFST Framework for RNN-Transducer Losses. (arXiv:2303.10384v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10384
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;WFST&#26694;&#26550;&#30340;&#30340;RNN-Transducer Losses&#24378;&#22823;&#19988;&#21487;&#25193;&#23637;&#30340;&#23454;&#29616;&#65292;&#8220;Compose-Transducer&#8221;&#21644;&#8220;Grid-Transducer&#8221;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#30340;W-Transducer Loss&#26469;&#23637;&#31034;&#32452;&#20214;&#30340;&#26131;&#25193;&#23637;&#24615;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;W-Transducer&#65288;W-RNNT&#65289;&#34920;&#29616;&#20986;&#27604;&#26631;&#20934;RNN-T&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21152;&#26435;&#26377;&#38480;&#29366;&#24577;&#36716;&#31227;&#22120;&#65288;WFST&#65289;&#30340;&#26694;&#26550;&#65292;&#20197;&#31616;&#21270;&#23545;RNN-Transducer&#65288;RNN-T&#65289; Losses&#30340;&#20462;&#25913;&#24320;&#21457;&#12290;&#29616;&#26377;&#30340;RNN-T&#23454;&#29616;&#20351;&#29992;&#19982;CUDA&#30456;&#20851;&#30340;&#20195;&#30721;&#65292;&#38590;&#20197;&#25193;&#23637;&#21644;&#35843;&#35797;&#12290;WFST&#26131;&#20110;&#26500;&#24314;&#21644;&#25193;&#23637;&#65292;&#24182;&#20801;&#35768;&#36890;&#36807;&#21487;&#35270;&#21270;&#36827;&#34892;&#35843;&#35797;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#20004;&#20010;&#22522;&#20110;WFST&#30340;RNN-T&#23454;&#29616;&#65306;&#65288;1&#65289;&#8220;Compose-Transducer&#8221;&#65292;&#23427;&#22522;&#20110;&#22768;&#23398;&#21644;&#25991;&#26412;&#26550;&#26500;&#30340;WFST&#22270;&#32452;&#21512;&#65292;&#35745;&#31639;&#25928;&#29575;&#39640;&#21644;&#26131;&#20110;&#20462;&#25913;&#65307;&#65288;2&#65289;&#8220;Grid-Transducer&#8221;&#65292;&#30452;&#25509;&#26500;&#24314;&#26230;&#26684;&#29992;&#20110;&#36827;&#19968;&#27493;&#35745;&#31639;&#65292;&#26368;&#32039;&#20945;&#21644;&#35745;&#31639;&#25928;&#29575;&#26368;&#39640;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;W-Transducer Loss&#65292;&#21363;Connectionist Temporal Classification with Wild Cards&#30340;&#36866;&#24212;&#24615;&#65292;&#23637;&#31034;&#20102;&#32452;&#20214;&#30340;&#26131;&#25193;&#23637;&#24615;&#12290;&#22312;&#32570;&#23569;&#36716;&#24405;&#24320;&#22836;&#37096;&#20998;&#30340;&#24369;&#30417;&#30563;&#25968;&#25454;&#35774;&#32622;&#20013;&#65292;W-Transducer&#65288;W-RNNT&#65289;&#22987;&#32456;&#20248;&#20110;&#26631;&#20934;RNN-T&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a framework based on Weighted Finite-State Transducers (WFST) to simplify the development of modifications for RNN-Transducer (RNN-T) loss. Existing implementations of RNN-T use CUDA-related code, which is hard to extend and debug. WFSTs are easy to construct and extend, and allow debugging through visualization. We introduce two WFST-powered RNN-T implementations: (1) "Compose-Transducer", based on a composition of the WFST graphs from acoustic and textual schema -- computationally competitive and easy to modify; (2) "Grid-Transducer", which constructs the lattice directly for further computations -- most compact, and computationally efficient. We illustrate the ease of extensibility through introduction of a new W-Transducer loss -- the adaptation of the Connectionist Temporal Classification with Wild Cards. W-Transducer (W-RNNT) consistently outperforms the standard RNN-T in a weakly-supervised data setup with missing parts of transcriptions at the beginning and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#24320;&#21457;&#22810;&#32423;&#21035;&#20379;&#24212;&#38142;&#30340;&#21160;&#24577;&#24211;&#23384;&#35746;&#36141;&#31574;&#30053;&#65292;&#22312;&#19977;&#32423;&#20379;&#24212;&#38142;&#20223;&#30495;&#27979;&#35797;&#20013;&#35777;&#26126;&#20102;&#23454;&#29616;&#19982;&#26368;&#20808;&#36827;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#21516;&#26102;&#20855;&#22791;&#20256;&#32479;&#31574;&#30053;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.10382</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24378;&#21270;&#23398;&#20064;&#22312;&#24211;&#23384;&#31649;&#29702;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Interpretable Reinforcement Learning via Neural Additive Models for Inventory Management. (arXiv:2303.10382v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10382
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#24320;&#21457;&#22810;&#32423;&#21035;&#20379;&#24212;&#38142;&#30340;&#21160;&#24577;&#24211;&#23384;&#35746;&#36141;&#31574;&#30053;&#65292;&#22312;&#19977;&#32423;&#20379;&#24212;&#38142;&#20223;&#30495;&#27979;&#35797;&#20013;&#35777;&#26126;&#20102;&#23454;&#29616;&#19982;&#26368;&#20808;&#36827;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#21516;&#26102;&#20855;&#22791;&#20256;&#32479;&#31574;&#30053;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
COVID-19&#30123;&#24773;&#24432;&#26174;&#20102;&#20379;&#24212;&#38142;&#30340;&#37325;&#35201;&#24615;&#21644;&#25968;&#23383;&#21270;&#31649;&#29702;&#22312;&#24212;&#23545;&#29615;&#22659;&#30340;&#21160;&#24577;&#21464;&#21270;&#20013;&#30340;&#20316;&#29992;&#12290;&#26412;&#25991;&#30528;&#37325;&#20110;&#20026;&#22810;&#32423;&#21035;&#21363;&#22810;&#38454;&#27573;&#30340;&#20379;&#24212;&#38142;&#24320;&#21457;&#21160;&#24577;&#24211;&#23384;&#35746;&#36141;&#31574;&#30053;&#12290;&#20256;&#32479;&#30340;&#24211;&#23384;&#20248;&#21270;&#26041;&#27861;&#26088;&#22312;&#30830;&#23450;&#38745;&#24577;&#35746;&#36141;&#31574;&#30053;&#65292;&#36825;&#20123;&#31574;&#30053;&#19981;&#33021;&#36866;&#24212;&#22914;COVID-19&#21361;&#26426;&#20013;&#35266;&#23519;&#21040;&#30340;&#21160;&#24577;&#21464;&#21270;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#31574;&#30053;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#30340;&#20248;&#21183;&#65292;&#36825;&#26159;&#20379;&#24212;&#38142;&#31649;&#29702;&#32773;&#27807;&#36890;&#20915;&#31574;&#19982;&#30456;&#20851;&#26041;&#38656;&#35201;&#20855;&#22791;&#30340;&#20851;&#38190;&#29305;&#24449;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#24615;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#26082;&#20855;&#26377;&#20256;&#32479;&#38745;&#24577;&#31574;&#30053;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#21448;&#20855;&#26377;&#20854;&#20182;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#28789;&#27963;&#24615;&#21644;&#29615;&#22659;&#26080;&#20851;&#24615;&#12290;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;&#20316;&#20026;&#24211;&#23384;&#35746;&#36141;&#31574;&#30053;&#30340;&#35299;&#37322;&#20989;&#25968;&#36924;&#36817;&#22120;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19977;&#32423;&#20379;&#24212;&#38142;&#20223;&#30495;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#24182;&#19982;&#20256;&#32479;&#24211;&#23384;&#31574;&#30053;&#20197;&#21450;&#20854;&#20182;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#31574;&#30053;&#65292;&#24182;&#22312;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The COVID-19 pandemic has highlighted the importance of supply chains and the role of digital management to react to dynamic changes in the environment. In this work, we focus on developing dynamic inventory ordering policies for a multi-echelon, i.e. multi-stage, supply chain. Traditional inventory optimization methods aim to determine a static reordering policy. Thus, these policies are not able to adjust to dynamic changes such as those observed during the COVID-19 crisis. On the other hand, conventional strategies offer the advantage of being interpretable, which is a crucial feature for supply chain managers in order to communicate decisions to their stakeholders. To address this limitation, we propose an interpretable reinforcement learning approach that aims to be as interpretable as the traditional static policies while being as flexible and environment-agnostic as other deep learning-based reinforcement learning solutions. We propose to use Neural Additive Models as an interpr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#33218;&#32769;&#34382;&#26426;&#30340;&#21152;&#36895;&#32852;&#37030;&#23398;&#20064;&#20013;&#23458;&#25143;&#31471;&#36873;&#25321;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;Thompson&#37319;&#26679;&#21644;&#21516;&#27493;&#26356;&#26032;&#23454;&#29616;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#33391;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.10373</link><description>&lt;p&gt;
&#19968;&#31181;&#21152;&#36895;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#23458;&#25143;&#31471;&#36873;&#25321;&#26041;&#27861;&#65306;&#22522;&#20110;&#22810;&#33218;&#32769;&#34382;&#26426;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Client Selection for Generalization in Accelerated Federated Learning: A Multi-Armed Bandit Approach. (arXiv:2303.10373v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10373
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#33218;&#32769;&#34382;&#26426;&#30340;&#21152;&#36895;&#32852;&#37030;&#23398;&#20064;&#20013;&#23458;&#25143;&#31471;&#36873;&#25321;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;Thompson&#37319;&#26679;&#21644;&#21516;&#27493;&#26356;&#26032;&#23454;&#29616;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#33391;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#29992;&#20110;&#36328;&#22810;&#20010;&#33410;&#28857;&#65288;&#21363;&#23458;&#25143;&#31471;&#65289;&#22312;&#26412;&#22320;&#25968;&#25454;&#38598;&#20013;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#65292;&#32780;&#26080;&#38656;&#26126;&#30830;&#20132;&#25442;&#25968;&#25454;&#12290; &#36817;&#24180;&#26469;&#65292;&#30001;&#20110;&#20854;&#22312;&#38544;&#31169;&#32771;&#34385;&#21644;&#36890;&#20449;&#36164;&#28304;&#26041;&#38754;&#30340;&#20248;&#21183;&#65292;&#32852;&#37030;&#23398;&#20064;&#21463;&#21040;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290; &#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#25152;&#36873;&#23458;&#25143;&#31471;&#35757;&#32451;&#20854;&#26412;&#22320;&#27169;&#22411;&#24182;&#23558;&#27169;&#22411;&#30340;&#20989;&#25968;&#21457;&#36865;&#21040;&#26381;&#21153;&#22120;&#65292;&#21518;&#32773;&#28040;&#32791;&#38543;&#26426;&#30340;&#22788;&#29702;&#21644;&#20256;&#36755;&#26102;&#38388;&#12290; &#26381;&#21153;&#22120;&#26356;&#26032;&#20840;&#23616;&#27169;&#22411;&#24182;&#23558;&#20854;&#24191;&#25773;&#22238;&#23458;&#25143;&#31471;&#12290; &#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#23458;&#25143;&#31471;&#36873;&#25321;&#38382;&#39064;&#26159;&#22312;&#27599;&#20010;&#32473;&#23450;&#26102;&#38388;&#23433;&#25490;&#19968;&#32452;&#23458;&#25143;&#31471;&#36827;&#34892;&#35757;&#32451;&#21644;&#20256;&#36755;&#65292;&#20197;&#20248;&#21270;&#23398;&#20064;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22810;&#33218;&#32769;&#34382;&#26426;&#30340;&#23458;&#25143;&#31471;&#36873;&#25321;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;Thompson&#37319;&#26679;&#21644;&#21516;&#27493;&#26356;&#26032;&#30340;&#24605;&#24819;&#26469;&#23454;&#29616;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#21516;&#26102;&#20445;&#25345;&#33391;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#21363;&#20026;&#26032;&#35266;&#23519;&#32467;&#26524;&#25552;&#20379;&#21487;&#38752;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290; &#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;AGCS&#65288;Accelerated Generalized Client Selection&#65289;&#30340;&#26032;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is an emerging machine learning (ML) paradigm used to train models across multiple nodes (i.e., clients) holding local data sets, without explicitly exchanging the data. It has attracted a growing interest in recent years due to its advantages in terms of privacy considerations, and communication resources. In FL, selected clients train their local models and send a function of the models to the server, which consumes a random processing and transmission time. The server updates the global model and broadcasts it back to the clients. The client selection problem in FL is to schedule a subset of the clients for training and transmission at each given time so as to optimize the learning performance. In this paper, we present a novel multi-armed bandit (MAB)-based approach for client selection to minimize the training latency without harming the ability of the model to generalize, that is, to provide reliable predictions for new observations. We develop a novel alg
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#37325;&#24230;&#19981;&#24179;&#34913;&#33410;&#28857;&#20998;&#31867;&#30340;&#36845;&#20195;&#36807;&#37319;&#26679;&#26041;&#27861;UNREAL&#65292;&#36890;&#36807;&#28155;&#21152;&#26410;&#26631;&#35760;&#33410;&#28857;&#32780;&#19981;&#26159;&#21512;&#25104;&#33410;&#28857;&#65292;&#35299;&#20915;&#20102;&#29305;&#24449;&#21644;&#37051;&#22495;&#29983;&#25104;&#30340;&#38590;&#39064;&#65292;&#24182;&#21033;&#29992;&#33410;&#28857;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#36827;&#34892;&#20960;&#20309;&#25490;&#21517;&#26469;&#26377;&#25928;&#22320;&#26657;&#20934;&#20266;&#26631;&#31614;&#20998;&#37197;&#12290;</title><link>http://arxiv.org/abs/2303.10371</link><description>&lt;p&gt;
UNREAL: &#29992;&#20110;&#37325;&#24230;&#19981;&#24179;&#34913;&#33410;&#28857;&#20998;&#31867;&#30340;&#26410;&#26631;&#35760;&#33410;&#28857;&#26816;&#32034;&#21644;&#26631;&#35760;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
UNREAL:Unlabeled Nodes Retrieval and Labeling for Heavily-imbalanced Node Classification. (arXiv:2303.10371v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10371
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#37325;&#24230;&#19981;&#24179;&#34913;&#33410;&#28857;&#20998;&#31867;&#30340;&#36845;&#20195;&#36807;&#37319;&#26679;&#26041;&#27861;UNREAL&#65292;&#36890;&#36807;&#28155;&#21152;&#26410;&#26631;&#35760;&#33410;&#28857;&#32780;&#19981;&#26159;&#21512;&#25104;&#33410;&#28857;&#65292;&#35299;&#20915;&#20102;&#29305;&#24449;&#21644;&#37051;&#22495;&#29983;&#25104;&#30340;&#38590;&#39064;&#65292;&#24182;&#21033;&#29992;&#33410;&#28857;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#36827;&#34892;&#20960;&#20309;&#25490;&#21517;&#26469;&#26377;&#25928;&#22320;&#26657;&#20934;&#20266;&#26631;&#31614;&#20998;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#26497;&#24230;&#20542;&#26012;&#30340;&#26631;&#31614;&#20998;&#24067;&#24456;&#24120;&#35265;&#12290;&#22914;&#26524;&#19981;&#21512;&#36866;&#22320;&#22788;&#29702;&#65292;&#36825;&#23545;&#23569;&#25968;&#31867;&#21035;&#30340;GNNs&#24615;&#33021;&#20250;&#26377;&#26497;&#22823;&#30340;&#24433;&#21709;&#12290;&#30001;&#20110;&#20854;&#23454;&#29992;&#24615;&#65292;&#26368;&#36817;&#19968;&#31995;&#21015;&#30340;&#30740;&#31350;&#37117;&#33268;&#21147;&#20110;&#35299;&#20915;&#36825;&#20010;&#38590;&#39064;&#12290;&#29616;&#26377;&#30340;&#36807;&#37319;&#26679;&#26041;&#27861;&#36890;&#36807;&#20135;&#29983;&#8220;&#20551;&#8221;&#30340;&#23569;&#25968;&#33410;&#28857;&#21644;&#21512;&#25104;&#20854;&#29305;&#24449;&#21644;&#23616;&#37096;&#25299;&#25169;&#26469;&#24179;&#28369;&#26631;&#31614;&#20998;&#24067;&#65292;&#36825;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#24573;&#30053;&#20102;&#22270;&#19978;&#26410;&#26631;&#35760;&#33410;&#28857;&#30340;&#20016;&#23500;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;UNREAL&#65292;&#19968;&#31181;&#36845;&#20195;&#36807;&#37319;&#26679;&#26041;&#27861;&#12290;&#31532;&#19968;&#20010;&#20851;&#38190;&#21306;&#21035;&#22312;&#20110;&#65292;&#25105;&#20204;&#21482;&#28155;&#21152;&#26410;&#26631;&#35760;&#33410;&#28857;&#32780;&#19981;&#26159;&#21512;&#25104;&#33410;&#28857;&#65292;&#36825;&#28040;&#38500;&#20102;&#29305;&#24449;&#21644;&#37051;&#22495;&#29983;&#25104;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#36873;&#25321;&#35201;&#28155;&#21152;&#30340;&#26410;&#26631;&#35760;&#33410;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#20309;&#25490;&#21517;&#26469;&#23545;&#26410;&#26631;&#35760;&#33410;&#28857;&#36827;&#34892;&#25490;&#21517;&#12290;&#20960;&#20309;&#25490;&#21517;&#21033;&#29992;&#33410;&#28857;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#26469;&#26377;&#25928;&#22320;&#26657;&#20934;&#20266;&#26631;&#31614;&#20998;&#37197;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Extremely skewed label distributions are common in real-world node classification tasks. If not dealt with appropriately, it significantly hurts the performance of GNNs in minority classes. Due to its practical importance, there have been a series of recent research devoted to this challenge. Existing over-sampling techniques smooth the label distribution by generating ``fake'' minority nodes and synthesizing their features and local topology, which largely ignore the rich information of unlabeled nodes on graphs. In this paper, we propose UNREAL, an iterative over-sampling method. The first key difference is that we only add unlabeled nodes instead of synthetic nodes, which eliminates the challenge of feature and neighborhood generation. To select which unlabeled nodes to add, we propose geometric ranking to rank unlabeled nodes. Geometric ranking exploits unsupervised learning in the node embedding space to effectively calibrates pseudo-label assignment. Finally, we identify the issu
&lt;/p&gt;</description></item><item><title>CroSel&#26159;&#19968;&#31181;&#22788;&#29702;&#20266;&#26631;&#31614;&#22122;&#22768;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21382;&#21490;&#39044;&#27979;&#20449;&#24687;&#21644;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#39033;&#26469;&#20934;&#30830;&#35782;&#21035;&#37096;&#20998;&#26631;&#31614;&#25968;&#25454;&#30340;&#30495;&#23454;&#26631;&#31614;&#12290;</title><link>http://arxiv.org/abs/2303.10365</link><description>&lt;p&gt;
CroSel: &#29992;&#20110;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#30340;&#33258;&#20449;&#20266;&#26631;&#31614;&#30340;&#36328;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
CroSel: Cross Selection of Confident Pseudo Labels for Partial-Label Learning. (arXiv:2303.10365v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10365
&lt;/p&gt;
&lt;p&gt;
CroSel&#26159;&#19968;&#31181;&#22788;&#29702;&#20266;&#26631;&#31614;&#22122;&#22768;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21382;&#21490;&#39044;&#27979;&#20449;&#24687;&#21644;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#39033;&#26469;&#20934;&#30830;&#35782;&#21035;&#37096;&#20998;&#26631;&#31614;&#25968;&#25454;&#30340;&#30495;&#23454;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;(PLL)&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#24369;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#65292;&#23427;&#20801;&#35768;&#27599;&#20010;&#35757;&#32451;&#31034;&#20363;&#26377;&#19968;&#20010;&#20505;&#36873;&#26631;&#31614;&#38598;&#65292;&#32780;&#19981;&#26159;&#19968;&#20010;&#21333;&#19968;&#30340;ground-truth&#26631;&#31614;&#12290;&#24050;&#32463;&#24191;&#27867;&#25506;&#32034;&#20102;&#22522;&#20110;&#35782;&#21035;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;PLL&#20013;&#30340;&#26631;&#31614;&#27495;&#20041;&#38382;&#39064;&#65292;&#36825;&#20123;&#26041;&#27861;&#23558;&#30495;&#23454;&#26631;&#31614;&#35270;&#20026;&#35201;&#35782;&#21035;&#30340;&#28508;&#22312;&#21464;&#37327;&#12290;&#28982;&#32780;&#65292;&#20934;&#30830;&#21644;&#23436;&#25972;&#22320;&#35782;&#21035;&#30495;&#23454;&#26631;&#31614;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#36825;&#20250;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#23548;&#33268;&#20266;&#26631;&#31614;&#20013;&#30340;&#22122;&#22768;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CroSel&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#27169;&#22411;&#30340;&#21382;&#21490;&#39044;&#27979;&#20449;&#24687;&#26469;&#35782;&#21035;&#22823;&#22810;&#25968;&#35757;&#32451;&#31034;&#20363;&#30340;&#30495;&#23454;&#26631;&#31614;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20132;&#21449;&#36873;&#25321;&#31574;&#30053;&#65292;&#20351;&#24471;&#20004;&#20010;&#28145;&#24230;&#27169;&#22411;&#21487;&#20197;&#30456;&#20114;&#36873;&#25321;&#37096;&#20998;&#26631;&#35760;&#25968;&#25454;&#30340;&#30495;&#23454;&#26631;&#31614;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#39033;co-mix&#65292;&#20197;&#36991;&#20813;&#22240;&#34394;&#20551;&#36873;&#25321;&#32780;&#24341;&#36215;&#30340;&#26679;&#26412;&#28010;&#36153;&#21644;&#24494;&#23567;&#22122;&#22768;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;CroSel&#33021;&#22815;&#25361;&#36873;&#20986;&#22823;&#22810;&#25968;&#31034;&#20363;&#30340;&#30495;&#23454;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;
Partial-label learning (PLL) is an important weakly supervised learning problem, which allows each training example to have a candidate label set instead of a single ground-truth label. Identification-based methods have been widely explored to tackle label ambiguity issues in PLL, which regard the true label as a latent variable to be identified. However, identifying the true labels accurately and completely remains challenging, causing noise in pseudo labels during model training. In this paper, we propose a new method called CroSel, which leverages historical prediction information from models to identify true labels for most training examples. First, we introduce a cross selection strategy, which enables two deep models to select true labels of partially labeled data for each other. Besides, we propose a novel consistent regularization term called co-mix to avoid sample waste and tiny noise caused by false selection. In this way, CroSel can pick out the true labels of most examples 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DC-CCL&#30340;&#35774;&#22791;-&#20113;&#21327;&#21516;&#25511;&#21046;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#20113;&#31471;&#26080;&#27861;&#30452;&#25509;&#37096;&#32626;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#30340;&#22823;&#22411;&#35270;&#35273;&#27169;&#22411;&#20173;&#28982;&#21487;&#20197;&#20174;&#35774;&#22791;&#31471;&#23616;&#37096;&#26679;&#26412;&#20013;&#21463;&#30410;&#12290;</title><link>http://arxiv.org/abs/2303.10361</link><description>&lt;p&gt;
DC-CCL: &#35774;&#22791;-&#20113;&#21327;&#21516;&#25511;&#21046;&#23398;&#20064;&#22312;&#22823;&#22411;&#35270;&#35273;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
DC-CCL: Device-Cloud Collaborative Controlled Learning for Large Vision Models. (arXiv:2303.10361v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10361
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DC-CCL&#30340;&#35774;&#22791;-&#20113;&#21327;&#21516;&#25511;&#21046;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#20113;&#31471;&#26080;&#27861;&#30452;&#25509;&#37096;&#32626;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#30340;&#22823;&#22411;&#35270;&#35273;&#27169;&#22411;&#20173;&#28982;&#21487;&#20197;&#20174;&#35774;&#22791;&#31471;&#23616;&#37096;&#26679;&#26412;&#20013;&#21463;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#22823;&#22411;&#35270;&#35273;&#27169;&#22411;&#24050;&#32463;&#37096;&#32626;&#22312;&#20113;&#31471;&#29992;&#20110;&#23454;&#26102;&#26381;&#21153;&#12290;&#21516;&#26102;&#65292;&#29616;&#22330;&#35774;&#22791;&#19981;&#26029;&#29983;&#25104;&#26032;&#30340;&#26679;&#26412;&#12290;&#22914;&#20309;&#21033;&#29992;&#35774;&#22791;&#31471;&#30340;&#26679;&#26412;&#26469;&#25913;&#36827;&#20113;&#31471;&#30340;&#22823;&#22411;&#27169;&#22411;&#25104;&#20026;&#23454;&#38469;&#38656;&#27714;&#65292;&#20294;&#38519;&#20837;&#20102;&#27809;&#26377;&#21407;&#22987;&#26679;&#26412;&#19978;&#34892;&#21644;&#27809;&#26377;&#22823;&#22411;&#27169;&#22411;&#19979;&#34892;&#30340;&#22256;&#22659;&#20013;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35774;&#22791;-&#20113;&#21327;&#21516;&#25511;&#21046;&#23398;&#20064;&#26694;&#26550;&#65292;&#21517;&#20026;DC-CCL&#65292;&#20351;&#20113;&#31471;&#26080;&#27861;&#30452;&#25509;&#37096;&#32626;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#30340;&#22823;&#22411;&#35270;&#35273;&#27169;&#22411;&#20173;&#28982;&#21487;&#20197;&#20174;&#35774;&#22791;&#31471;&#23616;&#37096;&#26679;&#26412;&#20013;&#21463;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many large vision models have been deployed on the cloud for real-time services. Meanwhile, fresh samples are continuously generated on the served mobile device. How to leverage the device-side samples to improve the cloud-side large model becomes a practical requirement, but falls into the dilemma of no raw sample up-link and no large model down-link. Specifically, the user may opt out of sharing raw samples with the cloud due to the concern of privacy or communication overhead, while the size of some large vision models far exceeds the mobile device's runtime capacity. In this work, we propose a device-cloud collaborative controlled learning framework, called DC-CCL, enabling a cloud-side large vision model that cannot be directly deployed on the mobile device to still benefit from the device-side local samples. In particular, DC-CCL vertically splits the base model into two submodels, one large submodel for learning from the cloud-side samples and the other small submodel for learni
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#31070;&#32463;&#34928;&#24369;&#26426;&#22120;&#65288;NFM&#65289;&#26694;&#26550;&#29992;&#20110;&#29983;&#23384;&#22238;&#24402;&#65292;&#21033;&#29992;&#22810;&#37325;&#34928;&#24369;&#30340;&#32463;&#20856;&#24605;&#24819;&#26469;&#25429;&#25417;&#20010;&#20307;&#38388;&#26410;&#35266;&#23519;&#21040;&#30340;&#24322;&#36136;&#24615;&#65292;&#24182;&#33021;&#22815;&#22788;&#29702;&#38750;&#32447;&#24615;&#21327;&#21464;&#37327;&#20381;&#36182;&#24615;&#12290;&#20004;&#20010;&#20855;&#20307;&#27169;&#22411;&#19979;&#25193;&#23637;&#20102;&#31070;&#32463;&#27604;&#20363;&#21361;&#38505;&#27169;&#22411;&#21644;&#38750;&#21442;&#25968;&#21361;&#38505;&#22238;&#24402;&#27169;&#22411;&#65292;&#32467;&#35770;&#33719;&#24471;&#20102;&#32479;&#35745;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2303.10358</link><description>&lt;p&gt;
&#31070;&#32463;&#34928;&#24369;&#26426;&#22120;&#65306;&#31070;&#32463;&#29983;&#23384;&#22238;&#24402;&#20013;&#36229;&#27604;&#20363;&#21361;&#38505;&#20551;&#35774;&#30340;&#31361;&#30772;
&lt;/p&gt;
&lt;p&gt;
Neural Frailty Machine: Beyond proportional hazard assumption in neural survival regressions. (arXiv:2303.10358v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10358
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#31070;&#32463;&#34928;&#24369;&#26426;&#22120;&#65288;NFM&#65289;&#26694;&#26550;&#29992;&#20110;&#29983;&#23384;&#22238;&#24402;&#65292;&#21033;&#29992;&#22810;&#37325;&#34928;&#24369;&#30340;&#32463;&#20856;&#24605;&#24819;&#26469;&#25429;&#25417;&#20010;&#20307;&#38388;&#26410;&#35266;&#23519;&#21040;&#30340;&#24322;&#36136;&#24615;&#65292;&#24182;&#33021;&#22815;&#22788;&#29702;&#38750;&#32447;&#24615;&#21327;&#21464;&#37327;&#20381;&#36182;&#24615;&#12290;&#20004;&#20010;&#20855;&#20307;&#27169;&#22411;&#19979;&#25193;&#23637;&#20102;&#31070;&#32463;&#27604;&#20363;&#21361;&#38505;&#27169;&#22411;&#21644;&#38750;&#21442;&#25968;&#21361;&#38505;&#22238;&#24402;&#27169;&#22411;&#65292;&#32467;&#35770;&#33719;&#24471;&#20102;&#32479;&#35745;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#31070;&#32463;&#34928;&#24369;&#26426;&#22120;&#65288;NFM&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#24378;&#22823;&#28789;&#27963;&#30340;&#31070;&#32463;&#24314;&#27169;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#23384;&#22238;&#24402;&#12290;NFM&#26694;&#26550;&#21033;&#29992;&#29983;&#23384;&#20998;&#26512;&#20013;&#30340;&#22810;&#37325;&#34928;&#24369;&#30340;&#32463;&#20856;&#24605;&#24819;&#26469;&#25429;&#25417;&#20010;&#20307;&#38388;&#26410;&#35266;&#23519;&#21040;&#30340;&#24322;&#36136;&#24615;&#65292;&#24182;&#33021;&#22815;&#21033;&#29992;&#31070;&#32463;&#32467;&#26500;&#30340;&#24378;&#22823;&#36924;&#36817;&#33021;&#21147;&#22788;&#29702;&#38750;&#32447;&#24615;&#21327;&#21464;&#37327;&#20381;&#36182;&#24615;&#12290;&#26694;&#26550;&#19979;&#25512;&#23548;&#20986;&#20004;&#20010;&#20855;&#20307;&#27169;&#22411;&#65292;&#23427;&#20204;&#20998;&#21035;&#25193;&#23637;&#20102;&#31070;&#32463;&#27604;&#20363;&#21361;&#38505;&#27169;&#22411;&#21644;&#38750;&#21442;&#25968;&#21361;&#38505;&#22238;&#24402;&#27169;&#22411;&#12290;&#36825;&#20004;&#20010;&#27169;&#22411;&#37117;&#20801;&#35768;&#22312;&#20284;&#28982;&#30446;&#26631;&#19979;&#36827;&#34892;&#39640;&#25928;&#35757;&#32451;&#12290;&#29702;&#35770;&#19978;&#65292;&#23545;&#20110;&#20004;&#20010;&#25552;&#20986;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#36890;&#36807;&#34920;&#24449;&#20854;&#25910;&#25947;&#36895;&#29575;&#65292;&#24314;&#31435;&#20102;&#31070;&#32463;&#20989;&#25968;&#36924;&#36817;&#38750;&#21442;&#25968;&#32452;&#20214;&#30340;&#32479;&#35745;&#20445;&#35777;&#12290;&#22312;&#23454;&#35777;&#19978;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#21512;&#25104;&#23454;&#39564;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#29702;&#35770;&#38472;&#36848;&#12290;&#25105;&#20204;&#36824;&#22312;&#19981;&#21516;&#35268;&#27169;&#30340;6&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#35780;&#20272;&#65292;&#26174;&#31034;&#20986;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present neural frailty machine (NFM), a powerful and flexible neural modeling framework for survival regressions. The NFM framework utilizes the classical idea of multiplicative frailty in survival analysis to capture unobserved heterogeneity among individuals, at the same time being able to leverage the strong approximation power of neural architectures for handling nonlinear covariate dependence. Two concrete models are derived under the framework that extends neural proportional hazard models and nonparametric hazard regression models. Both models allow efficient training under the likelihood objective. Theoretically, for both proposed models, we establish statistical guarantees of neural function approximation with respect to nonparametric components via characterizing their rate of convergence. Empirically, we provide synthetic experiments that verify our theoretical statements. We also conduct experimental evaluations over $6$ benchmark datasets of different scales, showing th
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026; Supervision Interpolation &#30340;&#26032;&#27010;&#24565;&#26694;&#26550;&#65292;&#36890;&#36807;&#25918;&#26494;&#21644;&#25512;&#24191; Mixup &#25552;&#20379;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#25554;&#20540;&#22686;&#24378;&#35270;&#35282;&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; LossMix &#30340;&#31616;&#21333;&#32780;&#22810;&#21151;&#33021;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#33021;&#22815;&#22686;&#24378;&#29289;&#20307;&#26816;&#27979;&#22120;&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#65292;&#25110;&#32773;&#35828;LossMix &#22312;&#30446;&#26631;&#26816;&#27979;&#21644;&#20854;&#20182;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2303.10343</link><description>&lt;p&gt;
LossMix&#65306;&#31616;&#21270;&#21644;&#24191;&#27867;&#24212;&#29992; Mixup &#20110;&#30446;&#26631;&#26816;&#27979;&#21644;&#26356;&#22810;&#39046;&#22495;
&lt;/p&gt;
&lt;p&gt;
LossMix: Simplify and Generalize Mixup for Object Detection and Beyond. (arXiv:2303.10343v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10343
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026; Supervision Interpolation &#30340;&#26032;&#27010;&#24565;&#26694;&#26550;&#65292;&#36890;&#36807;&#25918;&#26494;&#21644;&#25512;&#24191; Mixup &#25552;&#20379;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#25554;&#20540;&#22686;&#24378;&#35270;&#35282;&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; LossMix &#30340;&#31616;&#21333;&#32780;&#22810;&#21151;&#33021;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#33021;&#22815;&#22686;&#24378;&#29289;&#20307;&#26816;&#27979;&#22120;&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#65292;&#25110;&#32773;&#35828;LossMix &#22312;&#30446;&#26631;&#26816;&#27979;&#21644;&#20854;&#20182;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#28151;&#21512;&#22686;&#24378;&#24191;&#27867;&#24212;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#20294;&#30001;&#20110;&#31354;&#38388;&#38169;&#20301;&#12289;&#21069;&#26223;/&#32972;&#26223;&#21306;&#20998;&#20197;&#21450;&#22810;&#20010;&#23454;&#20363;&#30340;&#25361;&#25112;&#65292;&#36825;&#20123;&#25216;&#26415;&#19981;&#26131;&#24212;&#29992;&#20110;&#30446;&#26631;&#26816;&#27979;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#31216;&#20026;&#30417;&#30563;&#25554;&#20540;&#30340;&#26032;&#27010;&#24565;&#26694;&#26550;&#65292;&#36890;&#36807;&#25918;&#26494;&#21644;&#25512;&#24191; Mixup &#25552;&#20379;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#25554;&#20540;&#22686;&#24378;&#35270;&#35282;&#65292;&#28982;&#21518;&#22312;&#36825;&#20010;&#26694;&#26550;&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102; LossMix&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#22810;&#21151;&#33021;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#33021;&#22815;&#22686;&#24378;&#29289;&#20307;&#26816;&#27979;&#22120;&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;insight&#26159;&#65292;&#36890;&#36807;&#25554;&#20540;&#25439;&#22833;&#35823;&#24046;&#26469;&#35843;&#25972;&#35757;&#32451;&#21487;&#20197;&#26377;&#25928;&#35268;&#33539;&#28151;&#21512;&#25968;&#25454;&#30340;&#35757;&#32451;&#65292;&#32780;&#19981;&#26159;&#20351;&#29992;ground truth&#26631;&#31614;&#12290;&#22312;PASCAL VOC&#21644;MS COCO&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;LossMix&#22987;&#32456;&#20248;&#20110;&#24403;&#21069;&#27969;&#34892;&#30340;&#28151;&#21512;&#31574;&#30053;&#65292;&#24182;&#19988;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#39046;&#22495;m...
&lt;/p&gt;
&lt;p&gt;
The success of data mixing augmentations in image classification tasks has been well-received. However, these techniques cannot be readily applied to object detection due to challenges such as spatial misalignment, foreground/background distinction, and plurality of instances. To tackle these issues, we first introduce a novel conceptual framework called Supervision Interpolation, which offers a fresh perspective on interpolation-based augmentations by relaxing and generalizing Mixup. Building on this framework, we propose LossMix, a simple yet versatile and effective regularization that enhances the performance and robustness of object detectors and more. Our key insight is that we can effectively regularize the training on mixed data by interpolating their loss errors instead of ground truth labels. Empirical results on the PASCAL VOC and MS COCO datasets demonstrate that LossMix consistently outperforms currently popular mixing strategies. Furthermore, we design a two-stage domain m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#25968;&#23383;&#32534;&#32455;&#30005;&#23481;&#24335;&#27963;&#21160;&#20256;&#24863;&#22120;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#23545;12&#31181;&#22797;&#26434;&#25163;&#21183;&#31867;&#30340;&#20934;&#30830;&#20998;&#31867;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#21644;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.10336</link><description>&lt;p&gt;
&#26368;&#23567;&#21270;&#32534;&#32455;&#20256;&#24863;&#22120;&#19978;&#30340;&#22797;&#26434;&#25163;&#21183;&#35782;&#21035;&#65306;&#36808;&#21521;&#30495;&#23454;&#19990;&#30028;&#20132;&#20114;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Recognizing Complex Gestures on Minimalistic Knitted Sensors: Toward Real-World Interactive Systems. (arXiv:2303.10336v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10336
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#25968;&#23383;&#32534;&#32455;&#30005;&#23481;&#24335;&#27963;&#21160;&#20256;&#24863;&#22120;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#23545;12&#31181;&#22797;&#26434;&#25163;&#21183;&#31867;&#30340;&#20934;&#30830;&#20998;&#31867;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#21644;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35302;&#25511;&#32442;&#32455;&#21697;&#30340;&#21457;&#23637;&#24102;&#26469;&#20102;&#35768;&#22810;&#26032;&#39062;&#30340;&#20132;&#20114;&#25216;&#26415;&#21644;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#25968;&#23383;&#32534;&#32455;&#30005;&#23481;&#24335;&#27963;&#21160;&#20256;&#24863;&#22120;&#21487;&#20197;&#22312;&#24456;&#23569;&#30340;&#20154;&#24037;&#24178;&#39044;&#19979;&#22823;&#35268;&#27169;&#21046;&#36896;&#12290;&#23427;&#20204;&#30340;&#25935;&#24863;&#21306;&#22495;&#30001;&#21333;&#20010;&#23548;&#30005;&#32433;&#32447;&#21019;&#24314;&#65292;&#24182;&#19988;&#21482;&#38656;&#35201;&#19982;&#22806;&#37096;&#30828;&#20214;&#36827;&#34892;&#23569;&#37327;&#36830;&#25509;&#12290;&#36825;&#31181;&#25216;&#26415;&#25552;&#39640;&#20102;&#23427;&#20204;&#30340;&#40065;&#26834;&#24615;&#21644;&#21487;&#29992;&#24615;&#65292;&#21516;&#26102;&#23558;&#23454;&#29616;&#20132;&#20114;&#24615;&#30340;&#22797;&#26434;&#24615;&#20174;&#30828;&#20214;&#36716;&#31227;&#21040;&#20102;&#35745;&#31639;&#27169;&#22411;&#12290;&#26412;&#25991;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#20132;&#20114;&#24335;&#25163;&#21183;&#35782;&#21035;&#31995;&#32479;&#30340;&#22522;&#30784;&#65292;&#25552;&#39640;&#20102;&#36825;&#31181;&#20256;&#24863;&#22120;&#30340;&#21151;&#33021;&#12290;&#23427;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20256;&#24863;&#22120;&#35774;&#35745;&#21644;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#35782;&#21035;&#27169;&#22411;&#65292;&#21487;&#20197;&#23545;12&#31181;&#30456;&#23545;&#22797;&#26434;&#12289;&#21333;&#35302;&#28857;&#25163;&#21183;&#31867;&#36827;&#34892;89.8%&#30340;&#20934;&#30830;&#20998;&#31867;&#65292;&#23637;&#31034;&#20102;&#26410;&#26469;&#20196;&#20154;&#20852;&#22859;&#30340;&#24212;&#29992;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#20854;&#20329;&#25140;&#26102;&#30340;&#24615;&#33021;&#21644;&#27927;&#28068;&#30340;&#24433;&#21709;&#23637;&#31034;&#20102;&#35813;&#31995;&#32479;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#30340;&#36866;&#29992;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developments in touch-sensitive textiles have enabled many novel interactive techniques and applications. Our digitally-knitted capacitive active sensors can be manufactured at scale with little human intervention. Their sensitive areas are created from a single conductive yarn, and they require only few connections to external hardware. This technique increases their robustness and usability, while shifting the complexity of enabling interactivity from the hardware to computational models. This work advances the capabilities of such sensors by creating the foundation for an interactive gesture recognition system. It uses a novel sensor design, and a neural network-based recognition model to classify 12 relatively complex, single touch point gesture classes with 89.8% accuracy, unfolding many possibilities for future applications. We also demonstrate the system's applicability and robustness to real-world conditions through its performance while being worn and the impact of washing and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#23618;&#26041;&#27861;&#26469;&#25511;&#21046;&#28151;&#21512;&#31995;&#32479;&#65292;&#24182;&#21033;&#29992;&#21560;&#24341;&#23376;&#21306;&#22495;&#20272;&#35745;&#22120;&#21644;&#35268;&#21010;&#22120;&#26469;&#30830;&#20445;&#31995;&#32479;&#30340;&#31283;&#23450;&#24615;&#65292;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#35757;&#32451;&#26102;&#38388;&#26356;&#30701;&#65292;&#24615;&#33021;&#30456;&#21516;&#25110;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2303.10327</link><description>&lt;p&gt;
&#20855;&#26377;&#21560;&#24341;&#23376;&#21306;&#22495;&#35268;&#21010;&#22120;&#30340;&#28151;&#21512;&#31995;&#32479;&#31070;&#32463;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Hybrid Systems Neural Control with Region-of-Attraction Planner. (arXiv:2303.10327v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10327
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#23618;&#26041;&#27861;&#26469;&#25511;&#21046;&#28151;&#21512;&#31995;&#32479;&#65292;&#24182;&#21033;&#29992;&#21560;&#24341;&#23376;&#21306;&#22495;&#20272;&#35745;&#22120;&#21644;&#35268;&#21010;&#22120;&#26469;&#30830;&#20445;&#31995;&#32479;&#30340;&#31283;&#23450;&#24615;&#65292;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#35757;&#32451;&#26102;&#38388;&#26356;&#30701;&#65292;&#24615;&#33021;&#30456;&#21516;&#25110;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#31995;&#32479;&#22312;&#26426;&#22120;&#20154;&#25216;&#26415;&#20013;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36830;&#32493;&#21644;&#31163;&#25955;&#21160;&#24577;&#30340;&#22797;&#26434;&#24615;&#65292;&#30830;&#20445;&#28151;&#21512;&#31995;&#32479;&#30340;&#31283;&#23450;&#24615;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#21363;&#20351;&#19968;&#20010;&#20855;&#26377;&#25152;&#26377;&#31995;&#32479;&#27169;&#24335;&#31283;&#23450;&#30340;&#31995;&#32479;&#20173;&#28982;&#21487;&#33021;&#26159;&#19981;&#31283;&#23450;&#30340;&#12290;&#22240;&#27492;&#65292;&#22312;&#27169;&#24335;&#20999;&#25442;&#26102;&#38656;&#35201;&#29305;&#27530;&#22788;&#29702;&#20197;&#31283;&#23450;&#31995;&#32479;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#30340;&#12289;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#30340;&#25511;&#21046;&#19968;&#33324;&#28151;&#21512;&#31995;&#32479;&#30340;&#26041;&#27861;&#12290;&#23545;&#20110;&#27599;&#20010;&#31995;&#32479;&#27169;&#24335;&#65292;&#25105;&#20204;&#39318;&#20808;&#23398;&#20064;NN Lyapunov&#20989;&#25968;&#21644;NN&#25511;&#21046;&#22120;&#65292;&#20197;&#30830;&#20445;&#22312;&#21560;&#24341;&#23376;&#21306;&#22495;&#65288;RoA&#65289;&#20869;&#30340;&#29366;&#24577;&#21487;&#20197;&#34987;&#31283;&#23450;&#22320;&#25511;&#21046;&#12290;&#28982;&#21518;&#22312;&#19981;&#21516;&#27169;&#24335;&#20043;&#38388;&#23398;&#20064;RoA NN&#20272;&#35745;&#22120;&#12290;&#22312;&#27169;&#24335;&#20999;&#25442;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#24494;&#30340;&#35268;&#21010;&#22120;&#65292;&#20197;&#30830;&#20445;&#20999;&#25442;&#21518;&#30340;&#29366;&#24577;&#21487;&#20197;&#33853;&#20837;&#19979;&#19968;&#20010;&#27169;&#24335;&#30340;RoA&#20013;&#65292;&#20174;&#32780;&#31283;&#23450;&#28151;&#21512;&#31995;&#32479;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#26032;&#39062;&#30340;&#29702;&#35770;&#31283;&#23450;&#24615;&#20445;&#35777;&#65292;&#24182;&#22312;&#27773;&#36710;&#36319;&#36394;&#25511;&#21046;&#12289;pogobot&#23548;&#33322;&#21644;&#21452;&#36275;&#34892;&#36208;&#26426;&#22120;&#20154;&#36816;&#21160;&#20013;&#36827;&#34892;&#23454;&#39564;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21482;&#38656;&#35201;0.25&#20493;&#30340;&#35757;&#32451;&#26102;&#38388;&#21363;&#21487;&#25214;&#21040;Lyapunov&#20989;&#25968;&#65292;&#24182;&#19988;&#21487;&#20197;&#23454;&#29616;&#31867;&#20284;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hybrid systems are prevalent in robotics. However, ensuring the stability of hybrid systems is challenging due to sophisticated continuous and discrete dynamics. A system with all its system modes stable can still be unstable. Hence special treatments are required at mode switchings to stabilize the system. In this work, we propose a hierarchical, neural network (NN)-based method to control general hybrid systems. For each system mode, we first learn an NN Lyapunov function and an NN controller to ensure the states within the region of attraction (RoA) can be stabilized. Then an RoA NN estimator is learned across different modes. Upon mode switching, we propose a differentiable planner to ensure the states after switching can land in next mode's RoA, hence stabilizing the hybrid system. We provide novel theoretical stability guarantees and conduct experiments in car tracking control, pogobot navigation, and bipedal walker locomotion. Our method only requires 0.25X of the training time 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#8212;&#8212;&#20266;&#30417;&#30563;&#24230;&#37327;&#65292;&#29992;&#20110;&#35780;&#20272;&#26080;&#30417;&#30563;&#22270;&#29255;&#21040;&#22270;&#29255;&#32763;&#35793;&#27169;&#22411;&#22312;&#26080;&#30417;&#30563;&#36328;&#22495;&#20998;&#31867;&#26694;&#26550;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2303.10310</link><description>&lt;p&gt;
&#20266;&#30417;&#30563;&#24230;&#37327;&#65306;&#22312;&#26080;&#30417;&#30563;&#36328;&#22495;&#20998;&#31867;&#26694;&#26550;&#20013;&#35780;&#20272;&#26080;&#30417;&#30563;&#22270;&#20687;&#32763;&#35793;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Pseudo Supervised Metrics: Evaluating Unsupervised Image to Image Translation Models In Unsupervised Cross-Domain Classification Frameworks. (arXiv:2303.10310v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10310
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#8212;&#8212;&#20266;&#30417;&#30563;&#24230;&#37327;&#65292;&#29992;&#20110;&#35780;&#20272;&#26080;&#30417;&#30563;&#22270;&#29255;&#21040;&#22270;&#29255;&#32763;&#35793;&#27169;&#22411;&#22312;&#26080;&#30417;&#30563;&#36328;&#22495;&#20998;&#31867;&#26694;&#26550;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#21644;&#39640;&#25928;&#24615;&#21462;&#20915;&#20110;&#35775;&#38382;&#22823;&#22411;&#26631;&#35760;&#25968;&#25454;&#38598;&#24182;&#22312;&#27169;&#22411;&#35757;&#32451;&#30340;&#30456;&#21516;&#39046;&#22495;&#19978;&#27979;&#35797;&#25968;&#25454;&#12290;&#24403;&#22788;&#29702;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#30340;&#26032;&#25968;&#25454;&#26102;&#65292;&#20998;&#31867;&#21464;&#24471;&#26356;&#21152;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#25910;&#38598;&#22823;&#22411;&#26631;&#35760;&#25968;&#25454;&#38598;&#24182;&#20174;&#22836;&#35757;&#32451;&#26032;&#20998;&#31867;&#22120;&#32791;&#26102;&#12289;&#26114;&#36149;&#65292;&#26377;&#26102;&#26159;&#19981;&#21487;&#34892;&#25110;&#19981;&#21487;&#33021;&#30340;&#12290;&#36328;&#22495;&#20998;&#31867;&#26694;&#26550;&#36890;&#36807;&#21033;&#29992;&#26080;&#30417;&#30563;&#22270;&#20687;&#23545;&#22270;&#20687; (UI2I) &#32763;&#35793;&#27169;&#22411;&#23558;&#36755;&#20837;&#22270;&#20687;&#20174;&#26410;&#26631;&#35760;&#30340;&#22495;&#36716;&#25442;&#20026;&#26631;&#35760;&#22495;&#26469;&#22788;&#29702;&#36825;&#20010;&#25968;&#25454;&#22495;&#28418;&#31227;&#38382;&#39064;&#12290;&#36825;&#20123;&#26080;&#30417;&#30563;&#27169;&#22411;&#30340;&#38382;&#39064;&#22312;&#20110;&#23427;&#20204;&#26159;&#26080;&#30417;&#30563;&#30340;&#12290;&#30001;&#20110;&#32570;&#23569;&#27880;&#37322;&#65292;&#26080;&#27861;&#20351;&#29992;&#20256;&#32479;&#30340;&#30417;&#30563;&#24230;&#37327;&#26469;&#35780;&#20272;&#36825;&#20123;&#32763;&#35793;&#27169;&#22411;&#20197;&#36873;&#25321;&#26368;&#20339;&#30340;&#26816;&#26597;&#28857;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#20266;&#30417;&#30563;&#24230;&#37327;&#65292;&#19987;&#38376;&#29992;&#20110;&#35780;&#20272;&#26080;&#30417;&#30563;&#36328;&#22495;&#20998;&#31867;&#26694;&#26550;&#20013; UI2I &#32763;&#35793;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to classify images accurately and efficiently is dependent on having access to large labeled datasets and testing on data from the same domain that the model is trained on. Classification becomes more challenging when dealing with new data from a different domain, where collecting a large labeled dataset and training a new classifier from scratch is time-consuming, expensive, and sometimes infeasible or impossible. Cross-domain classification frameworks were developed to handle this data domain shift problem by utilizing unsupervised image-to-image (UI2I) translation models to translate an input image from the unlabeled domain to the labeled domain. The problem with these unsupervised models lies in their unsupervised nature. For lack of annotations, it is not possible to use the traditional supervised metrics to evaluate these translation models to pick the best-saved checkpoint model. In this paper, we introduce a new method called Pseudo Supervised Metrics that was desig
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#24046;&#20998;&#38544;&#31169;&#31579;&#36873;&#35268;&#21017;&#65292;&#21516;&#26102;&#21457;&#29616;&#20102;&#30001;&#20110;&#28155;&#21152;&#21040;&#30830;&#20445;&#38544;&#31169;&#24615;&#30340;&#22122;&#22768;&#37327;&#65292;&#20351;&#29992;&#26377;&#29992;&#30340;&#31169;&#26377;&#31579;&#36873;&#35268;&#21017;&#30340;&#20219;&#21153;&#23384;&#22312;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2303.10303</link><description>&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#31579;&#36873;&#35268;&#21017;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
The Challenge of Differentially Private Screening Rules. (arXiv:2303.10303v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10303
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#24046;&#20998;&#38544;&#31169;&#31579;&#36873;&#35268;&#21017;&#65292;&#21516;&#26102;&#21457;&#29616;&#20102;&#30001;&#20110;&#28155;&#21152;&#21040;&#30830;&#20445;&#38544;&#31169;&#24615;&#30340;&#22122;&#22768;&#37327;&#65292;&#20351;&#29992;&#26377;&#29992;&#30340;&#31169;&#26377;&#31579;&#36873;&#35268;&#21017;&#30340;&#20219;&#21153;&#23384;&#22312;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#24615;$L_1$&#27491;&#21017;&#21270;&#27169;&#22411;&#19968;&#30452;&#26159;&#25968;&#25454;&#20998;&#26512;&#20013;&#26368;&#31616;&#21333;&#21644;&#26368;&#26377;&#25928;&#30340;&#24037;&#20855;&#20043;&#19968;&#65292;&#29305;&#21035;&#26159;&#22312;&#20449;&#24687;&#26816;&#32034;&#38382;&#39064;&#20013;&#65292;&#25991;&#26412;$ n $-gram&#19982;TF-IDF&#25110;Okapi&#29305;&#24449;&#20540;&#26159;&#19968;&#20010;&#24378;&#22823;&#19988;&#26131;&#20110;&#27604;&#36739;&#30340;&#22522;&#20934;&#12290;&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#31579;&#36873;&#35268;&#21017;&#24050;&#32463;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#20316;&#20026;&#20943;&#23569;&#29983;&#25104;$L_1$&#27169;&#22411;&#30340;&#31232;&#30095;&#22238;&#24402;&#26435;&#37325;&#30340;&#36816;&#34892;&#26102;&#38388;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#20449;&#24687;&#26816;&#32034;&#20013;&#38656;&#35201;&#36234;&#26469;&#36234;&#22810;&#30340;&#20445;&#25252;&#38544;&#31169;&#30340;&#27169;&#22411;&#65292;&#20294;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#19981;&#23384;&#22312;&#24046;&#20998;&#38544;&#31169;&#31579;&#36873;&#35268;&#21017;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20026;&#32447;&#24615;&#21644;&#36923;&#36753;&#22238;&#24402;&#24320;&#21457;&#20102;&#31532;&#19968;&#20010;&#24046;&#20998;&#38544;&#31169;&#31579;&#36873;&#35268;&#21017;&#12290;&#22312;&#36825;&#26679;&#20570;&#30340;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#30001;&#20110;&#28155;&#21152;&#21040;&#30830;&#20445;&#38544;&#31169;&#24615;&#30340;&#22122;&#22768;&#37327;&#65292;&#20351;&#29992;&#26377;&#29992;&#30340;&#31169;&#26377;&#31579;&#36873;&#35268;&#21017;&#30340;&#20219;&#21153;&#23384;&#22312;&#22256;&#38590;&#12290;&#25105;&#20204;&#25552;&#20379;&#29702;&#35770;&#35777;&#25454;&#21644;&#23454;&#39564;&#35777;&#25454;&#65292;&#34920;&#26126;&#36825;&#31181;&#22256;&#38590;&#28304;&#20110;&#31579;&#36873;&#27493;&#39588;&#26412;&#36523;&#65292;&#32780;&#19981;&#26159;&#31169;&#26377;&#20248;&#21270;&#22120;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#32467;&#26524;&#65292;
&lt;/p&gt;
&lt;p&gt;
Linear $L_1$-regularized models have remained one of the simplest and most effective tools in data analysis, especially in information retrieval problems where n-grams over text with TF-IDF or Okapi feature values are a strong and easy baseline. Over the past decade, screening rules have risen in popularity as a way to reduce the runtime for producing the sparse regression weights of $L_1$ models. However, despite the increasing need of privacy-preserving models in information retrieval, to the best of our knoweledge, no differentially private screening rule exists. In this paper, we develop the first differentially private screening rule for linear and logistic regression. In doing so, we discover difficulties in the task of making a useful private screening rule due to the amount of noise added to ensure privacy. We provide theoretical arguments and experimental evidence that this difficulty arises from the screening step itself and not the private optimizer. Based on our results, we
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24320;&#21457;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#26681;&#25454;&#36807;&#21435;14&#22825;&#30340;&#29615;&#22659;&#21644;&#31227;&#21160;&#25968;&#25454;&#39044;&#27979;COVID-19&#30340;&#27599;&#26085;&#30149;&#20363;&#25968;&#65292;&#24182;&#22312;&#23433;&#22823;&#30053;&#30465;&#30340;&#22235;&#20010;&#21439;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#26368;&#20339;&#27169;&#22411;&#21487;&#20197;&#20197;90.7%&#30340;&#20934;&#30830;&#29575;&#39044;&#27979;&#26126;&#22825;&#30340;&#27599;&#26085;COVID&#26696;&#20363;&#35745;&#25968;&#65292;&#20197;98.1%&#30340;&#20934;&#30830;&#29575;&#39044;&#27979;7&#22825;&#28378;&#21160;&#24179;&#22343;COVID&#26696;&#20363;&#35745;&#25968;&#12290;</title><link>http://arxiv.org/abs/2303.10294</link><description>&lt;p&gt;
&#22522;&#20110;2020&#24180;&#23433;&#22823;&#30053;&#30465;&#25968;&#25454;&#30340;COVID-19&#30149;&#20363;&#25968;&#37327;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Forecasting COVID-19 Case Counts Based on 2020 Ontario Data. (arXiv:2303.10294v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10294
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24320;&#21457;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#26681;&#25454;&#36807;&#21435;14&#22825;&#30340;&#29615;&#22659;&#21644;&#31227;&#21160;&#25968;&#25454;&#39044;&#27979;COVID-19&#30340;&#27599;&#26085;&#30149;&#20363;&#25968;&#65292;&#24182;&#22312;&#23433;&#22823;&#30053;&#30465;&#30340;&#22235;&#20010;&#21439;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#26368;&#20339;&#27169;&#22411;&#21487;&#20197;&#20197;90.7%&#30340;&#20934;&#30830;&#29575;&#39044;&#27979;&#26126;&#22825;&#30340;&#27599;&#26085;COVID&#26696;&#20363;&#35745;&#25968;&#65292;&#20197;98.1%&#30340;&#20934;&#30830;&#29575;&#39044;&#27979;7&#22825;&#28378;&#21160;&#24179;&#22343;COVID&#26696;&#20363;&#35745;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;&#24320;&#21457;&#21487;&#20197;&#26681;&#25454;&#36807;&#21435;14&#22825;&#30340;&#29615;&#22659;&#21644;&#31227;&#21160;&#25968;&#25454;&#39044;&#27979;&#27599;&#22825;COVID-19&#30149;&#20363;&#25968;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290; &#26041;&#27861;&#65306;&#20351;&#29992;&#26469;&#33258;&#22810;&#20262;&#22810;&#21608;&#22260;&#22235;&#20010;&#21439;&#30340;COVID-19&#25968;&#25454;&#12290; &#25968;&#25454;&#20934;&#22791;&#20026;&#27599;&#22825;&#30340;&#35760;&#24405;&#65292;&#21253;&#21547;&#26032;COVID&#30149;&#20363;&#35745;&#25968;&#12289;&#24739;&#32773;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#12289;&#23460;&#22806;&#22825;&#27668;&#21464;&#37327;&#12289;&#23460;&#20869;&#29615;&#22659;&#22240;&#32032;&#21644;&#22522;&#20110;&#34562;&#31389;&#31227;&#21160;&#21644;&#20844;&#20849;&#21355;&#29983;&#38480;&#21046;&#30340;&#20154;&#31867;&#27963;&#21160;&#12290;&#23545;&#36825;&#20123;&#25968;&#25454;&#36827;&#34892;&#20998;&#26512;&#65292;&#20197;&#30830;&#23450;&#26368;&#37325;&#35201;&#30340;&#21464;&#37327;&#21450;&#20854;&#20132;&#20114;&#20316;&#29992;&#12290;&#20351;&#29992;CNN&#21644;LSTM&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#24320;&#21457;&#39044;&#27979;&#27169;&#22411;&#12290;&#37319;&#29992;5&#20493;&#26102;&#38388;&#20132;&#21449;&#39564;&#35777;&#26041;&#27861;&#20351;&#29992;&#20174;2020&#24180;3&#26376;1&#26085;&#33267;2020&#24180;10&#26376;14&#26085;&#30340;&#25968;&#25454;&#24320;&#21457;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#22312;2020&#24180;10&#26376;15&#26085;&#33267;2020&#24180;12&#26376;24&#26085;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#27979;&#35797;&#12290; &#32467;&#26524;&#65306;&#26368;&#20339;&#30340;LSTM&#27169;&#22411;&#21487;&#20197;&#20197;90.7%&#30340;&#20934;&#30830;&#29575;&#39044;&#27979;&#26126;&#22825;&#30340;&#27599;&#26085;COVID&#26696;&#20363;&#35745;&#25968;&#65292;&#20197;98.1%&#30340;&#20934;&#30830;&#29575;&#39044;&#27979;7&#22825;&#28378;&#21160;&#24179;&#22343;COVID&#26696;&#20363;&#35745;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Objective: To develop machine learning models that can predict the number of COVID-19 cases per day given the last 14 days of environmental and mobility data.  Approach: COVID-19 data from four counties around Toronto, Ontario, were used. Data were prepared into daily records containing the number of new COVID case counts, patient demographic data, outdoor weather variables, indoor environment factors, and human movement based on cell mobility and public health restrictions. This data was analyzed to determine the most important variables and their interactions. Predictive models were developed using CNN and LSTM deep neural network approaches. A 5-fold chronological cross-validation approach used these methods to develop predictive models using data from Mar 1 to Oct 14 2020, and test them on data covering Oct 15 to Dec 24 2020. Results: The best LSTM models forecasted tomorrow's daily COVID case counts with 90.7% accuracy, and the 7-day rolling average COVID case counts with 98.1% ac
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20998;&#26512;&#20102;&#22266;&#23450;&#35774;&#35745;&#19979;&#30340;&#27491;&#21017;&#21270;&#36830;&#32493;&#23398;&#20064;&#38382;&#39064;&#65292;&#25552;&#20986;&#31639;&#27861;&#22312;&#36951;&#24536;&#21644;&#19981;&#22949;&#21327;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#65292;&#21487;&#33021;&#20250;&#23384;&#22312;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.10263</link><description>&lt;p&gt;
&#22266;&#23450;&#35774;&#35745;&#19979;&#27491;&#21017;&#21270;&#36830;&#32493;&#23398;&#20064;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Fixed Design Analysis of Regularization-Based Continual Learning. (arXiv:2303.10263v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10263
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20998;&#26512;&#20102;&#22266;&#23450;&#35774;&#35745;&#19979;&#30340;&#27491;&#21017;&#21270;&#36830;&#32493;&#23398;&#20064;&#38382;&#39064;&#65292;&#25552;&#20986;&#31639;&#27861;&#22312;&#36951;&#24536;&#21644;&#19981;&#22949;&#21327;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#65292;&#21487;&#33021;&#20250;&#23384;&#22312;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;&#36830;&#32493;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#26377;&#20004;&#20010;&#32447;&#24615;&#22238;&#24402;&#20219;&#21153;&#65292;&#29305;&#24449;&#21521;&#37327;&#34987;&#20551;&#23450;&#20026;&#22266;&#23450;&#30340;&#65292;&#26631;&#31614;&#34987;&#20551;&#23450;&#20026;&#38543;&#26426;&#21464;&#37327;&#12290;&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;$\ell_2$-&#27491;&#21017;&#21270;&#30340;&#36830;&#32493;&#23398;&#20064;&#31639;&#27861;&#65292;&#23427;&#35745;&#31639;&#19968;&#20010;&#26222;&#36890;&#26368;&#23567;&#20108;&#20056;&#21442;&#25968;&#26469;&#25311;&#21512;&#31532;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#28982;&#21518;&#35745;&#31639;&#21478;&#19968;&#20010;&#21442;&#25968;&#65292;&#22312;$\ell_2$-&#27491;&#21017;&#21270;&#32422;&#26463;&#19979;&#65292;&#25311;&#21512;&#31532;&#20108;&#20010;&#25968;&#25454;&#38598;&#24182;&#36755;&#20986;&#31532;&#20108;&#20010;&#21442;&#25968;&#12290;&#23545;&#20110;&#36825;&#20010;&#31639;&#27861;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20004;&#20010;&#20219;&#21153;&#30340;&#24179;&#22343;&#39118;&#38505;&#30340;&#20005;&#26684;&#30028;&#38480;&#12290;&#25105;&#20204;&#30340;&#39118;&#38505;&#30028;&#38480;&#25581;&#31034;&#20102;$\ell_2$-&#27491;&#21017;&#21270;&#36830;&#32493;&#23398;&#20064;&#31639;&#27861;&#20013;&#19968;&#20010;&#21487;&#35777;&#26126;&#30340;&#36951;&#24536;&#21644;&#19981;&#22949;&#21327;&#30340;&#26435;&#34913;&#20851;&#31995;&#65306;&#20351;&#29992;&#22823;&#30340;&#27491;&#21017;&#21270;&#21442;&#25968;&#65292;&#31639;&#27861;&#36755;&#20986;&#27604;&#36739;&#19981;&#20250;&#36951;&#24536;&#31532;&#19968;&#20010;&#20219;&#21153;&#30340;&#20449;&#24687;&#65292;&#20294;&#19981;&#24895;&#20174;&#31532;&#20108;&#20010;&#20219;&#21153;&#20013;&#25552;&#21462;&#26032;&#20449;&#24687;&#65307;&#21453;&#20043;&#20134;&#28982;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20855;&#26377;&#19981;&#30456;&#20284;&#20219;&#21153;&#30340;&#36830;&#32493;&#23398;&#20064;&#21487;&#33021;&#20250;&#21457;&#29983;&#28798;&#38590;&#24615;&#30340;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a continual learning (CL) problem with two linear regression tasks in the fixed design setting, where the feature vectors are assumed fixed and the labels are assumed to be random variables. We consider an $\ell_2$-regularized CL algorithm, which computes an Ordinary Least Squares parameter to fit the first dataset, then computes another parameter that fits the second dataset under an $\ell_2$-regularization penalizing its deviation from the first parameter, and outputs the second parameter. For this algorithm, we provide tight bounds on the average risk over the two tasks. Our risk bounds reveal a provable trade-off between forgetting and intransigence of the $\ell_2$-regularized CL algorithm: with a large regularization parameter, the algorithm output forgets less information about the first task but is intransigent to extract new information from the second task; and vice versa. Our results suggest that catastrophic forgetting could happen for CL with dissimilar tasks (u
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#39038;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#38543;&#26426;&#25511;&#21046;&#38382;&#39064;&#21644;&#21338;&#24328;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#30528;&#37325;&#20171;&#32461;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#35299;&#20915;&#39640;&#32500;&#24230;&#21644;&#38750;&#24120;&#22797;&#26434;&#32467;&#26500;&#24773;&#20917;&#19979;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.10257</link><description>&lt;p&gt;
&#29992;&#20110;&#38543;&#26426;&#25511;&#21046;&#21644;&#21338;&#24328;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#26368;&#26032;&#21457;&#23637;
&lt;/p&gt;
&lt;p&gt;
Recent Developments in Machine Learning Methods for Stochastic Control and Games. (arXiv:2303.10257v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10257
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#38543;&#26426;&#25511;&#21046;&#38382;&#39064;&#21644;&#21338;&#24328;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#30528;&#37325;&#20171;&#32461;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#35299;&#20915;&#39640;&#32500;&#24230;&#21644;&#38750;&#24120;&#22797;&#26434;&#32467;&#26500;&#24773;&#20917;&#19979;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#26368;&#20248;&#25511;&#21046;&#21644;&#21338;&#24328;&#24050;&#32463;&#22312;&#37329;&#34701;&#12289;&#32463;&#27982;&#23398;&#12289;&#31038;&#20250;&#31185;&#23398;&#12289;&#26426;&#22120;&#20154;&#21644;&#33021;&#28304;&#31649;&#29702;&#31561;&#39046;&#22495;&#20013;&#25214;&#21040;&#20102;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#35768;&#22810;&#30495;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#37117;&#28041;&#21450;&#21040;&#22797;&#26434;&#30340;&#27169;&#22411;&#65292;&#36825;&#25512;&#21160;&#20102;&#20808;&#36827;&#30340;&#25968;&#20540;&#26041;&#27861;&#30340;&#21457;&#23637;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#35745;&#31639;&#26041;&#27861;&#24050;&#32463;&#21457;&#23637;&#29992;&#20110;&#38543;&#26426;&#25511;&#21046;&#38382;&#39064;&#21644;&#21338;&#24328;&#12290;&#25105;&#20204;&#22238;&#39038;&#36825;&#20123;&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#24050;&#32463;&#35299;&#38145;&#20102;&#39640;&#32500;&#24230;&#21644;&#38750;&#24120;&#22797;&#26434;&#32467;&#26500;&#24773;&#20917;&#19979;&#35299;&#20915;&#27492;&#31867;&#38382;&#39064;&#30340;&#21487;&#33021;&#24615;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#36825;&#26159;&#20256;&#32479;&#25968;&#20540;&#26041;&#27861;&#26080;&#27861;&#23436;&#25104;&#30340;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20027;&#35201;&#32771;&#34385;&#36830;&#32493;&#26102;&#38388;&#21644;&#36830;&#32493;&#31354;&#38388;&#35774;&#32622;&#12290;&#35768;&#22810;&#26032;&#26041;&#27861;&#22522;&#20110;&#26368;&#36817;&#29992;&#20110;&#39640;&#32500;&#20559;&#24494;&#20998;&#26041;&#31243;&#25110;&#21453;&#21521;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#25110;&#32773;&#22522;&#20110;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#30340;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#36825;&#23548;&#33268;&#20102;&#31361;&#30772;&#24615;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic optimal control and games have found a wide range of applications, from finance and economics to social sciences, robotics and energy management. Many real-world applications involve complex models which have driven the development of sophisticated numerical methods. Recently, computational methods based on machine learning have been developed for stochastic control problems and games. We review such methods, with a focus on deep learning algorithms that have unlocked the possibility to solve such problems even when the dimension is high or when the structure is very complex, beyond what is feasible with traditional numerical methods. Here, we consider mostly the continuous time and continuous space setting. Many of the new approaches build on recent neural-network based methods for high-dimensional partial differential equations or backward stochastic differential equations, or on model-free reinforcement learning for Markov decision processes that have led to breakthrough 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#21644;&#31354;&#38388;&#20998;&#35299;&#26469;&#36817;&#20284;&#30005;&#21147;&#31995;&#32479;&#21160;&#21147;&#23398;&#24494;&#20998;&#20195;&#25968;&#26041;&#31243;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#21152;&#36895;&#20223;&#30495;&#65292;&#25552;&#39640;&#25968;&#20540;&#31283;&#23450;&#24615;&#21644;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.10256</link><description>&lt;p&gt;
&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#21644;&#31354;&#38388;&#20998;&#35299;&#22312;&#30005;&#21147;&#31995;&#32479;&#21160;&#21147;&#23398;&#20013;&#27714;&#35299;&#24494;&#20998;&#20195;&#25968;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Solving Differential-Algebraic Equations in Power Systems Dynamics with Neural Networks and Spatial Decomposition. (arXiv:2303.10256v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10256
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#21644;&#31354;&#38388;&#20998;&#35299;&#26469;&#36817;&#20284;&#30005;&#21147;&#31995;&#32479;&#21160;&#21147;&#23398;&#24494;&#20998;&#20195;&#25968;&#26041;&#31243;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#21152;&#36895;&#20223;&#30495;&#65292;&#25552;&#39640;&#25968;&#20540;&#31283;&#23450;&#24615;&#21644;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#21147;&#31995;&#32479;&#30340;&#21160;&#21147;&#23398;&#30001;&#19968;&#32452;&#24494;&#20998;&#20195;&#25968;&#26041;&#31243;&#25551;&#36848;&#12290;&#26102;&#38388;&#22495;&#20223;&#30495;&#29992;&#20110;&#29702;&#35299;&#31995;&#32479;&#21160;&#24577;&#30340;&#28436;&#21464;&#12290;&#30001;&#20110;&#31995;&#32479;&#30340;&#21018;&#24230;&#38656;&#35201;&#20351;&#29992;&#31934;&#32454;&#31163;&#25955;&#21270;&#30340;&#26102;&#38388;&#27493;&#38271;&#65292;&#22240;&#27492;&#36825;&#20123;&#20223;&#30495;&#21487;&#33021;&#20855;&#26377;&#35745;&#31639;&#20195;&#20215;&#36739;&#39640;&#30340;&#29305;&#28857;&#12290;&#36890;&#36807;&#22686;&#21152;&#20801;&#35768;&#30340;&#26102;&#38388;&#27493;&#38271;&#65292;&#25105;&#20204;&#26088;&#22312;&#21152;&#24555;&#36825;&#26679;&#30340;&#20223;&#30495;&#12290;&#26412;&#25991;&#20351;&#29992;&#35266;&#23519;&#32467;&#26524;&#65292;&#21363;&#23613;&#31649;&#21508;&#20010;&#32452;&#20214;&#20351;&#29992;&#20195;&#25968;&#21644;&#24494;&#20998;&#26041;&#31243;&#26469;&#25551;&#36848;&#65292;&#20294;&#23427;&#20204;&#30340;&#32806;&#21512;&#20165;&#28041;&#21450;&#20195;&#25968;&#26041;&#31243;&#30340;&#35266;&#23519;&#32467;&#26524;&#65292;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#26469;&#36817;&#20284;&#32452;&#20214;&#29366;&#24577;&#28436;&#21464;&#65292;&#20174;&#32780;&#20135;&#29983;&#24555;&#36895;&#12289;&#20934;&#30830;&#21644;&#25968;&#20540;&#31283;&#23450;&#30340;&#36817;&#20284;&#22120;&#65292;&#20351;&#24471;&#21487;&#20197;&#20351;&#29992;&#26356;&#22823;&#30340;&#26102;&#38388;&#27493;&#38271;&#12290;&#20026;&#20102;&#35299;&#37322;&#32593;&#32476;&#23545;&#32452;&#20214;&#20197;&#21450;&#32452;&#20214;&#23545;&#32593;&#32476;&#30340;&#24433;&#21709;&#65292;NN&#23558;&#32806;&#21512;&#20195;&#25968;&#21464;&#37327;&#30340;&#26102;&#38388;&#28436;&#21270;&#20316;&#20026;&#20854;&#39044;&#27979;&#30340;&#36755;&#20837;&#12290;&#25105;&#20204;&#26368;&#21021;&#20351;&#29992;&#31354;&#38388;&#20998;&#35299;&#26041;&#27861;&#26469;&#20272;&#35745;NN&#65292;&#20854;&#20013;&#31995;&#32479;&#34987;&#20998;&#25104;&#31354;&#38388;&#21306;&#22495;&#65292;&#27599;&#20010;&#21306;&#22495;&#26377;&#21333;&#29420;&#30340;NN&#20272;&#35745;&#22120;&#12290;&#25105;&#20204;&#23558;&#22522;&#20110;NN&#30340;&#20223;&#30495;&#19982;&#20256;&#32479;&#30340;&#25968;&#20540;&#31215;&#20998;&#26041;&#26696;&#36827;&#34892;&#27604;&#36739;&#65292;&#20197;&#23637;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The dynamics of the power system are described by a system of differential-algebraic equations. Time-domain simulations are used to understand the evolution of the system dynamics. These simulations can be computationally expensive due to the stiffness of the system which requires the use of finely discretized time-steps. By increasing the allowable time-step size, we aim to accelerate such simulations. In this paper, we use the observation that even though the individual components are described using both algebraic and differential equations, their coupling only involves algebraic equations. Following this observation, we use Neural Networks (NNs) to approximate the components' state evolution, leading to fast, accurate, and numerically stable approximators, which enable larger time-steps. To account for effects of the network on the components and vice-versa, the NNs take the temporal evolution of the coupling algebraic variables as an input for their prediction. We initially estima
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24322;&#26500;&#32852;&#37030;&#32593;&#32476;&#20013;&#30340;&#39640;&#25928;&#20998;&#24067;&#24335;&#36845;&#20195;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25903;&#25345;&#21521;&#37327;&#26426;&#23454;&#29616;&#23545;&#32852;&#37030;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#30340;&#22788;&#29702;&#65292;&#24182;&#25903;&#25345;&#20010;&#24615;&#21270;&#30340;&#23398;&#20064;&#27169;&#22411;&#12290;&#20026;&#20102;&#20445;&#25252;&#38544;&#31169;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#38543;&#26426;&#25513;&#30721;&#36807;&#31243;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23545;&#20110;&#35299;&#20915;&#24322;&#26500;&#32593;&#32476;&#20013;&#32852;&#37030;&#23398;&#20064;&#20219;&#21153;&#26159;&#26377;&#25928;&#30340;&#12290;</title><link>http://arxiv.org/abs/2303.10254</link><description>&lt;p&gt;
&#24322;&#26500;&#32593;&#32476;&#20013;&#32852;&#37030;&#25903;&#25345;&#21521;&#37327;&#26426;&#30340;&#22810;&#20219;&#21153;&#27169;&#22411;&#20010;&#24615;&#21270;
&lt;/p&gt;
&lt;p&gt;
Multi-Task Model Personalization for Federated Supervised SVM in Heterogeneous Networks. (arXiv:2303.10254v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10254
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24322;&#26500;&#32852;&#37030;&#32593;&#32476;&#20013;&#30340;&#39640;&#25928;&#20998;&#24067;&#24335;&#36845;&#20195;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25903;&#25345;&#21521;&#37327;&#26426;&#23454;&#29616;&#23545;&#32852;&#37030;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#30340;&#22788;&#29702;&#65292;&#24182;&#25903;&#25345;&#20010;&#24615;&#21270;&#30340;&#23398;&#20064;&#27169;&#22411;&#12290;&#20026;&#20102;&#20445;&#25252;&#38544;&#31169;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#38543;&#26426;&#25513;&#30721;&#36807;&#31243;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23545;&#20110;&#35299;&#20915;&#24322;&#26500;&#32593;&#32476;&#20013;&#32852;&#37030;&#23398;&#20064;&#20219;&#21153;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#30340;&#39640;&#25928;&#20998;&#24067;&#24335;&#36845;&#20195;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#32852;&#37030;&#20998;&#31867;&#21644;&#22238;&#24402;&#12290;&#35813;&#26041;&#27861;&#25903;&#25345;&#22312;&#24322;&#26500;&#33410;&#28857;&#32593;&#32476;&#20013;&#36827;&#34892;&#39640;&#25928;&#30340;&#35745;&#31639;&#21644;&#27169;&#22411;&#20132;&#25442;&#65292;&#24182;&#20801;&#35768;&#22312;&#23384;&#22312;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#27169;&#22411;&#30340;&#20010;&#24615;&#21270;&#12290;&#20026;&#36827;&#19968;&#27493;&#25552;&#39640;&#38544;&#31169;&#20445;&#25252;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#38543;&#26426;&#25513;&#30721;&#36807;&#31243;&#65292;&#26377;&#21161;&#20110;&#36991;&#20813;&#25968;&#25454;&#21453;&#28436;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#25152;&#25552;&#20986;&#30340;&#38544;&#31169;&#26426;&#21046;&#20197;&#21450;&#21442;&#19982;&#32773;&#30828;&#20214;&#21644;&#25968;&#25454;&#30340;&#24322;&#26500;&#24615;&#23545;&#31995;&#32479;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we design an efficient distributed iterative learning method based on support vector machines (SVMs), which tackles federated classification and regression. The proposed method supports efficient computations and model exchange in a network of heterogeneous nodes and allows personalization of the learning model in the presence of non-i.i.d. data. To further enhance privacy, we introduce a random mask procedure that helps avoid data inversion. Finally, we analyze the impact of the proposed privacy mechanisms and the heterogeneity of participant hardware and data on the system performance.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19977;&#35282;&#38754;&#19978;&#36827;&#34892;&#29983;&#25104;&#27169;&#22411;&#30340;&#20869;&#22312;&#23545;&#31216;&#24615;&#29983;&#25104;&#27169;&#22411;&#65292;&#20351;&#29992;&#31163;&#25955;&#20849;&#24418;&#20960;&#20309;&#30340;&#36827;&#23637;&#23558;&#38754;&#32593;&#26684;&#26144;&#23556;&#21040;&#29699;&#24418;&#65292;&#21487;&#20197;&#36890;&#36807;&#20219;&#20309;&#24050;&#32463;&#24320;&#21457;&#20026;&#31616;&#21333;&#27969;&#24418;&#30340;&#29983;&#25104;&#27169;&#22411;&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#12290;&#28436;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#22810;&#20010;&#27969;&#24418;&#21644;&#22810;&#20010;&#29983;&#25104;&#27169;&#22411;&#23376;&#20363;&#31243;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.10251</link><description>&lt;p&gt;
&#20869;&#22312;&#23545;&#31216;&#24615;&#29983;&#25104;&#27169;&#22411;&#30340;&#19977;&#35282;&#38754;&#19978;&#30340;&#25311;&#21512;
&lt;/p&gt;
&lt;p&gt;
Conformal Generative Modeling on Triangulated Surfaces. (arXiv:2303.10251v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10251
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19977;&#35282;&#38754;&#19978;&#36827;&#34892;&#29983;&#25104;&#27169;&#22411;&#30340;&#20869;&#22312;&#23545;&#31216;&#24615;&#29983;&#25104;&#27169;&#22411;&#65292;&#20351;&#29992;&#31163;&#25955;&#20849;&#24418;&#20960;&#20309;&#30340;&#36827;&#23637;&#23558;&#38754;&#32593;&#26684;&#26144;&#23556;&#21040;&#29699;&#24418;&#65292;&#21487;&#20197;&#36890;&#36807;&#20219;&#20309;&#24050;&#32463;&#24320;&#21457;&#20026;&#31616;&#21333;&#27969;&#24418;&#30340;&#29983;&#25104;&#27169;&#22411;&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#12290;&#28436;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#22810;&#20010;&#27969;&#24418;&#21644;&#22810;&#20010;&#29983;&#25104;&#27169;&#22411;&#23376;&#20363;&#31243;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#20869;&#22312;&#23545;&#31216;&#24615;&#29983;&#25104;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#30001;&#31163;&#25955;&#19977;&#35282;&#24418;&#32593;&#26684;&#36817;&#20284;&#30340;&#20108;&#32500;&#38754;&#19978;&#36827;&#34892;&#29983;&#25104;&#24314;&#27169;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#31163;&#25955;&#20849;&#24418;&#20960;&#20309;&#30340;&#36827;&#23637;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#20174;&#28304;&#19977;&#35282;&#32593;&#26684;&#21040;&#31616;&#21333;&#27969;&#24418;&#65288;&#20363;&#22914;&#29699;&#20307;&#65289;&#30340;&#30446;&#26631;&#19977;&#35282;&#32593;&#26684;&#30340;&#26144;&#23556;&#12290;&#22312;&#32771;&#34385;&#32593;&#26684;&#31163;&#25955;&#21270;&#35823;&#24046;&#21518;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#20219;&#20309;&#20026;&#31616;&#21333;&#27969;&#24418;&#24320;&#21457;&#30340;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#20316;&#20026;&#21363;&#25554;&#21363;&#29992;&#30340;&#23376;&#20363;&#31243;&#20351;&#29992;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#22797;&#26434;&#30340;&#27969;&#24418;&#21644;&#22810;&#20010;&#29983;&#25104;&#24314;&#27169;&#23376;&#20363;&#31243;&#19978;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#22312;&#37027;&#37324;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20174;&#26679;&#26412;&#20013;&#23398;&#20064;&#32593;&#26684;&#19978;&#30340;&#20998;&#24067;&#30340;&#33391;&#22909;&#20272;&#35745;&#65292;&#24182;&#19988;&#20063;&#33021;&#21516;&#26102;&#20174;&#21516;&#19968;&#24213;&#23618;&#27969;&#24418;&#30340;&#22810;&#20010;&#19981;&#21516;&#32593;&#26684;&#20013;&#36827;&#34892;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose conformal generative modeling, a framework for generative modeling on 2D surfaces approximated by discrete triangle meshes. Our approach leverages advances in discrete conformal geometry to develop a map from a source triangle mesh to a target triangle mesh of a simple manifold such as a sphere. After accounting for errors due to the mesh discretization, we can use any generative modeling approach developed for simple manifolds as a plug-and-play subroutine. We demonstrate our framework on multiple complicated manifolds and multiple generative modeling subroutines, where we show that our approach can learn good estimates of distributions on meshes from samples, and can also learn simultaneously from multiple distinct meshes of the same underlying manifold.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#22411;LSwinSR&#36229;&#20998;&#36776;&#29575;&#31639;&#27861;&#65292;&#22522;&#20110;&#26368;&#20808;&#36827;&#30340;Swin Transformer&#65292;&#24212;&#29992;&#20110;UAV&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#22788;&#29702;&#65292;&#21516;&#26102;&#24341;&#20837;&#26356;&#20840;&#38754;&#30340;&#35780;&#20272;&#25351;&#26631;F1&#24471;&#20998;&#65292;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20854;&#27604;&#20854;&#20182;&#26368;&#20808;&#36827;&#31639;&#27861;&#26356;&#20855;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.10232</link><description>&lt;p&gt;
&#22522;&#20110;&#32447;&#24615;Swin Transformer&#30340;&#26080;&#20154;&#26426;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;LSwinSR
&lt;/p&gt;
&lt;p&gt;
LSwinSR: UAV Imagery Super-Resolution based on Linear Swin Transformer. (arXiv:2303.10232v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10232
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#22411;LSwinSR&#36229;&#20998;&#36776;&#29575;&#31639;&#27861;&#65292;&#22522;&#20110;&#26368;&#20808;&#36827;&#30340;Swin Transformer&#65292;&#24212;&#29992;&#20110;UAV&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#22788;&#29702;&#65292;&#21516;&#26102;&#24341;&#20837;&#26356;&#20840;&#38754;&#30340;&#35780;&#20272;&#25351;&#26631;F1&#24471;&#20998;&#65292;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20854;&#27604;&#20854;&#20182;&#26368;&#20808;&#36827;&#31639;&#27861;&#26356;&#20855;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#20998;&#36776;&#29575;&#25216;&#26415;&#26088;&#22312;&#20174;&#20302;&#20998;&#36776;&#29575;&#22270;&#20687;&#37325;&#24314;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#65292;&#22240;&#20854;&#23545;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#36965;&#24863;&#31038;&#21306;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#24182;&#19988;&#24050;&#32463;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290; &#23545;&#20110;&#26080;&#20154;&#26426;&#65288;UAV&#65289;&#25293;&#25668;&#30340;&#22270;&#20687;&#65292;&#36229;&#20998;&#36776;&#29575;&#25216;&#26415;&#23588;&#20854;&#26377;&#30410;&#65292;&#22240;&#20026;UAV&#25429;&#33719;&#30340;&#22270;&#20687;&#25968;&#37327;&#21644;&#20998;&#36776;&#29575;&#21463;&#29289;&#29702;&#32422;&#26463;&#65288;&#22914;&#39134;&#34892;&#39640;&#24230;&#21644;&#36733;&#33655;&#33021;&#21147;&#65289;&#30340;&#38480;&#21046;&#12290; &#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#20808;&#36827;&#30340;Swin Transformer&#30340;&#26032;&#22411;&#32593;&#32476;&#65292;&#29992;&#20110;UAV&#22270;&#20687;&#30340;&#36229;&#20998;&#36776;&#29575;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#25928;&#29575;&#21644;&#31454;&#20105;&#24615;&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#26356;&#20840;&#38754;&#30340;&#35780;&#20272;&#25351;&#26631;F1&#24471;&#20998;&#26469;&#26356;&#22909;&#22320;&#21453;&#26144;&#36229;&#20998;&#36776;&#29575;&#31639;&#27861;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Super-resolution, which aims to reconstruct high-resolution images from low-resolution images, has drawn considerable attention and has been intensively studied in computer vision and remote sensing communities. The super-resolution technology is especially beneficial for Unmanned Aerial Vehicles (UAV), as the amount and resolution of images captured by UAV are highly limited by physical constraints such as flight altitude and load capacity. In the wake of the successful application of deep learning methods in the super-resolution task, in recent years, a series of super-resolution algorithms have been developed. In this paper, for the super-resolution of UAV images, a novel network based on the state-of-the-art Swin Transformer is proposed with better efficiency and competitive accuracy. Meanwhile, as one of the essential applications of the UAV is land cover and land use monitoring, simple image quality assessments such as the Peak-Signal-to-Noise Ratio (PSNR) and the Structural Simi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#40065;&#26834;&#27169;&#24577;&#36830;&#25509;&#23548;&#21521;&#30340;&#23545;&#25239;&#24615;&#38450;&#24481;&#65292;&#23454;&#29616;&#31070;&#32463;&#32593;&#32476;&#23545;&#22810;&#26679;&#21270;$\ell_p$&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#20854;&#20013;&#21253;&#25324;&#20004;&#20010;&#22522;&#20110;&#31181;&#32676;&#23398;&#20064;&#30340;&#23398;&#20064;&#38454;&#27573;&#12290;</title><link>http://arxiv.org/abs/2303.10225</link><description>&lt;p&gt;
&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#23545;&#22810;&#26679;&#21270;$\ell_p$&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;:&#40065;&#26834;&#27169;&#24577;&#36830;&#25509;&#23548;&#21521;&#30340;&#23545;&#25239;&#24615;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Robust Mode Connectivity-Oriented Adversarial Defense: Enhancing Neural Network Robustness Against Diversified $\ell_p$ Attacks. (arXiv:2303.10225v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10225
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#40065;&#26834;&#27169;&#24577;&#36830;&#25509;&#23548;&#21521;&#30340;&#23545;&#25239;&#24615;&#38450;&#24481;&#65292;&#23454;&#29616;&#31070;&#32463;&#32593;&#32476;&#23545;&#22810;&#26679;&#21270;$\ell_p$&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#20854;&#20013;&#21253;&#25324;&#20004;&#20010;&#22522;&#20110;&#31181;&#32676;&#23398;&#20064;&#30340;&#23398;&#20064;&#38454;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#26159;&#34913;&#37327;&#31070;&#32463;&#32593;&#32476;&#22312;&#25512;&#29702;&#38454;&#27573;&#25269;&#24481;&#23545;&#25239;&#24615;&#25915;&#20987;&#33021;&#21147;&#30340;&#20851;&#38190;&#27010;&#24565;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23613;&#31649;&#20351;&#29992;&#30340;&#24378;&#21270;&#40065;&#26834;&#24615;&#35757;&#32451;&#25216;&#26415;&#33021;&#22815;&#25552;&#39640;&#23545;&#19968;&#31181;&#31867;&#22411;&#30340;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#20294;&#27169;&#22411;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#22810;&#26679;&#21270;&#30340;$\ell_p$&#25915;&#20987;&#12290;&#20026;&#20102;&#23454;&#29616;&#22810;&#26679;&#21270;&#30340;$\ell_p$&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#40065;&#26834;&#27169;&#24577;&#36830;&#25509; (RMC) &#23548;&#21521;&#30340;&#23545;&#25239;&#24615;&#38450;&#24481;&#65292;&#23427;&#21253;&#21547;&#20004;&#20010;&#22522;&#20110;&#31181;&#32676;&#23398;&#20064;&#30340;&#23398;&#20064;&#38454;&#27573;&#12290;&#31532;&#19968;&#20010;&#38454;&#27573;&#65292;RMC&#65292;&#33021;&#22815;&#25628;&#32034;&#20004;&#20010;&#39044;&#20808;&#35757;&#32451;&#27169;&#22411;&#20043;&#38388;&#30340;&#27169;&#22411;&#21442;&#25968;&#31354;&#38388;&#65292;&#24182;&#25214;&#21040;&#21253;&#21547;&#39640;&#40065;&#26834;&#24615;&#28857;&#30340;&#36335;&#24452;&#20197;&#25269;&#24481;&#22810;&#26679;&#21270;&#30340;$\ell_p$&#25915;&#20987;&#12290;&#22522;&#20110;RMC&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#31532;&#20108;&#20010;&#38454;&#27573;&#65292;&#22522;&#20110;RMC&#30340;&#20248;&#21270;&#65292;&#20854;&#20013;RMC&#20316;&#20026;&#31070;&#32463;&#32593;&#32476;&#22810;&#26679;&#21270;$\ell_p$&#40065;&#26834;&#24615;&#36827;&#19968;&#27493;&#22686;&#24378;&#30340;&#22522;&#26412;&#21333;&#20803;&#12290;&#20026;&#20102;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#65292;&#25105;&#20204;&#23558;&#23398;&#20064;&#19982;&#20165;&#36873;&#25321;&#23376;&#38598;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#30456;&#32467;&#21512;&#65292;&#36825;&#23548;&#33268;&#20102;&#19968;&#32452;&#36739;&#23567;&#30340;&#20195;&#34920;&#24615;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#21487;&#29992;&#20110;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#23545;&#22810;&#26679;&#21270;$\ell_p$&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial robustness is a key concept in measuring the ability of neural networks to defend against adversarial attacks during the inference phase. Recent studies have shown that despite the success of improving adversarial robustness against a single type of attack using robust training techniques, models are still vulnerable to diversified $\ell_p$ attacks. To achieve diversified $\ell_p$ robustness, we propose a novel robust mode connectivity (RMC)-oriented adversarial defense that contains two population-based learning phases. The first phase, RMC, is able to search the model parameter space between two pre-trained models and find a path containing points with high robustness against diversified $\ell_p$ attacks. In light of the effectiveness of RMC, we develop a second phase, RMC-based optimization, with RMC serving as the basic unit for further enhancement of neural network diversified $\ell_p$ robustness. To increase computational efficiency, we incorporate learning with a sel
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20132;&#21449;&#27880;&#24847;&#21147;&#21644;Transfomer&#30340;&#26041;&#27861;&#65292;&#21487;&#38752;&#22320;&#23558;PAP&#26816;&#26597;&#22270;&#20687;&#20998;&#31867;&#20026;&#39048;&#30284;&#12290;&#23545;&#20004;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;PAP&#26816;&#26597;&#25968;&#25454;&#38598;&#19978;&#30340;&#20998;&#31867;&#25928;&#26524;&#20998;&#21035;&#20026;93.70&#65285;&#21644;97.15&#65285;&#12290;</title><link>http://arxiv.org/abs/2303.10222</link><description>&lt;p&gt;
&#22522;&#20110;&#21367;&#26631;&#26816;&#27979;&#30340;&#39048;&#37096;&#30284;&#30151;&#20998;&#31867;&#26041;&#27861;&#8212;&#8212;&#20351;&#29992;&#20132;&#21449;&#27880;&#24847;&#21147;&#21644;&#28508;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
CerviFormer: A Pap-smear based cervical cancer classification method using cross attention and latent transformer. (arXiv:2303.10222v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10222
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20132;&#21449;&#27880;&#24847;&#21147;&#21644;Transfomer&#30340;&#26041;&#27861;&#65292;&#21487;&#38752;&#22320;&#23558;PAP&#26816;&#26597;&#22270;&#20687;&#20998;&#31867;&#20026;&#39048;&#30284;&#12290;&#23545;&#20004;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;PAP&#26816;&#26597;&#25968;&#25454;&#38598;&#19978;&#30340;&#20998;&#31867;&#25928;&#26524;&#20998;&#21035;&#20026;93.70&#65285;&#21644;97.15&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;&#39048;&#37096;&#30284;&#30151;&#26159;&#22899;&#24615;&#27515;&#20129;&#30340;&#20027;&#35201;&#21407;&#22240;&#20043;&#19968;&#12290;&#26089;&#26399;&#35786;&#26029;&#21644;&#26681;&#25454;&#26368;&#20339;&#21307;&#23398;&#24314;&#35758;&#36827;&#34892;&#27835;&#30103;&#65292;&#23601;&#20687;&#20854;&#20182;&#30142;&#30149;&#19968;&#26679;&#65292;&#20197;&#30830;&#20445;&#20854;&#24433;&#21709;&#23613;&#21487;&#33021;&#23567;&#12290; PAP&#26816;&#26597;&#22270;&#20687;&#26159;&#35782;&#21035;&#36825;&#31181;&#30284;&#30151;&#26368;&#20855;&#24314;&#35774;&#24615;&#30340;&#26041;&#24335;&#20043;&#19968;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20132;&#21449;&#27880;&#24847;&#21147;&#21644;Transfomer&#30340;&#26041;&#27861;&#65292;&#21487;&#38752;&#22320;&#23558;PAP&#26816;&#26597;&#22270;&#20687;&#20998;&#31867;&#20026;&#39048;&#30284;&#12290;&#26041;&#27861;&#65306;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CerviFormer&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20381;&#36182;&#20110;Transfomer&#65292;&#22240;&#27492;&#23545;&#20110;&#36755;&#20837;&#25968;&#25454;&#30340;&#22823;&#23567;&#20165;&#38656;&#26368;&#23567;&#30340;&#26550;&#26500;&#20551;&#35774;&#12290;&#35813;&#27169;&#22411;&#20351;&#29992;&#20132;&#21449;&#27880;&#24847;&#21147;&#25216;&#26415;&#65292;&#23558;&#36755;&#20837;&#25968;&#25454;&#37325;&#22797;&#27719;&#38598;&#21040;&#19968;&#20010;&#32039;&#20945;&#30340;&#28508;&#22312;Transformer&#27169;&#22359;&#20013;&#65292;&#20174;&#32780;&#20351;&#20854;&#33021;&#22815;&#31649;&#29702;&#38750;&#24120;&#22823;&#35268;&#27169;&#30340;&#36755;&#20837;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;PAP&#26816;&#26597;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#12290;&#32467;&#26524;&#65306;&#23545;&#20110;Sipakmed&#25968;&#25454;&#30340;3&#29366;&#24577;&#20998;&#31867;&#65292;&#35813;&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#20026;93.70&#65285;&#12290;&#23545;&#20110;Herlev&#25968;&#25454;&#38598;&#19978;&#30340;2&#29366;&#24577;&#20998;&#31867;&#65292;&#35813;&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#20026;97.15&#65285;&#12290;&#32467;&#35770;&#65306;&#25105;&#20204;&#30340;&#30740;&#31350;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;CerviFormer&#27169;&#22411;&#22312;&#23558;PAP&#26816;&#26597;&#22270;&#20687;&#20998;&#31867;&#20026;&#39048;&#37096;&#30284;&#30151;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#36825;&#26377;&#21161;&#20110;&#26089;&#26399;&#26816;&#27979;&#21644;&#27835;&#30103;&#36825;&#31181;&#33268;&#21629;&#30142;&#30149;&#12290;
&lt;/p&gt;
&lt;p&gt;
Purpose: Cervical cancer is one of the primary causes of death in women. It should be diagnosed early and treated according to the best medical advice, as with other diseases, to ensure that its effects are as minimal as possible. Pap smear images are one of the most constructive ways for identifying this type of cancer. This study proposes a cross-attention-based Transfomer approach for the reliable classification of cervical cancer in Pap smear images. Methods: In this study, we propose the CerviFormer -- a model that depends on the Transformers and thereby requires minimal architectural assumptions about the size of the input data. The model uses a cross-attention technique to repeatedly consolidate the input data into a compact latent Transformer module, which enables it to manage very large-scale inputs. We evaluated our model on two publicly available Pap smear datasets. Results: For 3-state classification on the Sipakmed data, the model achieved an accuracy of 93.70%. For 2-stat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#32852;&#37030;&#24773;&#22659;&#36172;&#21338;&#31639;&#27861;&#23545;&#38544;&#24335;&#20449;&#21495;&#36827;&#34892;&#23398;&#20064;&#65292;&#36991;&#20813;&#20102;&#35775;&#38382;&#38590;&#20197;&#33719;&#24471;&#30340;&#26174;&#24335;&#26631;&#31614;&#65292;&#24182;&#21457;&#29616;&#20102;&#31616;&#21333;&#32780;&#24120;&#29992;&#30340;softmax&#21551;&#21457;&#24335;&#22312;&#24179;&#34913;&#31639;&#27861;&#26041;&#38754;&#30340;&#24778;&#20154;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.10218</link><description>&lt;p&gt;
&#32852;&#37030;&#24773;&#22659;&#36172;&#21338;&#31639;&#27861;&#30340;&#23454;&#35777;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
An Empirical Evaluation of Federated Contextual Bandit Algorithms. (arXiv:2303.10218v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10218
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#32852;&#37030;&#24773;&#22659;&#36172;&#21338;&#31639;&#27861;&#23545;&#38544;&#24335;&#20449;&#21495;&#36827;&#34892;&#23398;&#20064;&#65292;&#36991;&#20813;&#20102;&#35775;&#38382;&#38590;&#20197;&#33719;&#24471;&#30340;&#26174;&#24335;&#26631;&#31614;&#65292;&#24182;&#21457;&#29616;&#20102;&#31616;&#21333;&#32780;&#24120;&#29992;&#30340;softmax&#21551;&#21457;&#24335;&#22312;&#24179;&#34913;&#31639;&#27861;&#26041;&#38754;&#30340;&#24778;&#20154;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#32852;&#37030;&#23398;&#20064;&#22312;&#23398;&#20064;&#29992;&#25143;&#35774;&#22791;&#19978;&#30340;&#25935;&#24863;&#25968;&#25454;&#26041;&#38754;&#30340;&#24212;&#29992;&#22686;&#21152;&#65292;&#33258;&#28982;&#32780;&#28982;&#22320;&#20250;&#38382;&#65292;&#26159;&#21542;&#21487;&#20197;&#20351;&#29992;&#38544;&#24335;&#20449;&#21495;&#26469;&#36827;&#34892;&#23398;&#20064;&#65292;&#32780;&#19981;&#38656;&#35201;&#35775;&#38382;&#24456;&#38590;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#33719;&#24471;&#30340;&#26174;&#24335;&#26631;&#31614;&#12290;&#25105;&#20204;&#37319;&#29992;&#32852;&#37030;&#24773;&#22659;&#36172;&#21338;&#31639;&#27861;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#38024;&#23545;&#32852;&#37030;&#35774;&#32622;&#20174;&#20013;&#24515;&#21270;&#30340;&#24773;&#20917;&#19979;&#21457;&#23637;&#20986;&#30528;&#21517;&#30340;&#24773;&#22659;&#36172;&#21338;&#31639;&#27861;&#30340;&#21464;&#20307;&#12290;&#25105;&#20204;&#22312;&#20351;&#29992;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#27169;&#25311;&#30340;&#19968;&#31995;&#21015;&#22330;&#26223;&#20013;&#20180;&#32454;&#35780;&#20272;&#20102;&#36825;&#20123;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#27169;&#25311;&#27169;&#22411;&#36890;&#24120;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#36935;&#21040;&#30340;&#35774;&#32622;&#65292;&#20363;&#22914;&#21021;&#22987;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#38543;&#21518;&#29992;&#25143;&#20132;&#20114;&#20043;&#38388;&#30340;&#21508;&#31181;&#19981;&#23545;&#40784;&#65292;&#36825;&#26159;&#30001;&#20110;&#25968;&#25454;&#30340;&#38750;&#31283;&#23450;&#24615;&#21644;/&#25110;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#24322;&#36136;&#24615;&#36896;&#25104;&#30340;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;&#31616;&#21333;&#21644;&#24120;&#29992;&#30340;softmax&#21551;&#21457;&#24335;&#22312;&#24179;&#34913;&#24773;&#22659;&#36172;&#21338;&#31639;&#27861;&#26041;&#38754;&#30340;&#24778;&#20154;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the adoption of federated learning increases for learning from sensitive data local to user devices, it is natural to ask if the learning can be done using implicit signals generated as users interact with the applications of interest, rather than requiring access to explicit labels which can be difficult to acquire in many tasks. We approach such problems with the framework of federated contextual bandits, and develop variants of prominent contextual bandit algorithms from the centralized seting for the federated setting. We carefully evaluate these algorithms in a range of scenarios simulated using publicly available datasets. Our simulations model typical setups encountered in the real-world, such as various misalignments between an initial pre-trained model and the subsequent user interactions due to non-stationarity in the data and/or heterogeneity across clients. Our experiments reveal the surprising effectiveness of the simple and commonly used softmax heuristic in balancing 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#33945;&#29305;&#21345;&#32599;&#37319;&#26679;&#31639;&#27861;&#36924;&#36817;&#32852;&#30431;&#32467;&#26500;&#32676;&#20307;&#35299;&#37322;&#22120;&#30340;&#22797;&#26434;&#38382;&#39064;&#65292;&#25552;&#20986;&#19968;&#31181;&#24555;&#36895;&#12289;&#26131;&#20110;&#23454;&#29616;&#19988;&#19982;&#29305;&#23450;&#27169;&#22411;&#26080;&#20851;&#30340;&#26032;&#22411;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#20005;&#23494;&#30340;&#32479;&#35745;&#20998;&#26512;&#21450;&#35823;&#24046;&#30028;&#30340;&#35777;&#26126;&#12290;</title><link>http://arxiv.org/abs/2303.10216</link><description>&lt;p&gt;
&#21033;&#29992;&#33945;&#29305;&#21345;&#32599;&#37319;&#26679;&#30340;&#32852;&#30431;&#32467;&#26500;&#32676;&#20307;&#35299;&#37322;&#22120;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
Approximation of group explainers with coalition structure using Monte Carlo sampling on the product space of coalitions and features. (arXiv:2303.10216v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10216
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#33945;&#29305;&#21345;&#32599;&#37319;&#26679;&#31639;&#27861;&#36924;&#36817;&#32852;&#30431;&#32467;&#26500;&#32676;&#20307;&#35299;&#37322;&#22120;&#30340;&#22797;&#26434;&#38382;&#39064;&#65292;&#25552;&#20986;&#19968;&#31181;&#24555;&#36895;&#12289;&#26131;&#20110;&#23454;&#29616;&#19988;&#19982;&#29305;&#23450;&#27169;&#22411;&#26080;&#20851;&#30340;&#26032;&#22411;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#20005;&#23494;&#30340;&#32479;&#35745;&#20998;&#26512;&#21450;&#35823;&#24046;&#30028;&#30340;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#35299;&#37322;&#25216;&#26415;&#37117;&#37319;&#29992;&#20102;&#21512;&#20316;&#21338;&#24328;&#29702;&#35770;&#30340;&#24605;&#24819;&#12290;&#36825;&#20123;&#28216;&#25103;&#29702;&#35770;&#35299;&#37322;&#22120;&#30001;&#20110;&#22797;&#26434;&#24230;&#39640;&#65292;&#22312;&#23454;&#38469;&#29615;&#22659;&#19979;&#26080;&#27861;&#31934;&#30830;&#35745;&#31639;&#12290;&#26412;&#25991;&#37319;&#29992;&#36866;&#24403;&#30340;&#26679;&#26412;&#31354;&#38388;&#20272;&#35745;&#26399;&#26395;&#20540;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#22411;&#33945;&#29305;&#21345;&#32599;&#37319;&#26679;&#31639;&#27861;, &#26469;&#20197;&#20381;&#36182;&#20110;&#32972;&#26223;&#25968;&#25454;&#38598;&#22823;&#23567;&#30340;&#32447;&#24615;&#22797;&#26434;&#24230;&#20272;&#35745;&#29305;&#23450;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#39044;&#27979;&#21521;&#37327;&#30340;&#36793;&#38469;&#21338;&#24328;&#30340;&#32852;&#30431;&#20540;&#65292;&#36825;&#31181;&#26041;&#27861;&#24555;&#36895;&#12289;&#26131;&#20110;&#25191;&#34892;&#19988;&#19982;&#20854;&#20182;&#24050;&#30693;&#30340;&#35768;&#22810;&#26356;&#22797;&#26434;&#21644;&#29305;&#23450;&#20110;&#27169;&#22411;&#30340;&#20272;&#35745;&#25216;&#26415;&#20855;&#26377;&#31867;&#20284;&#30340;&#32479;&#35745;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20005;&#23494;&#30340;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, many Machine Learning (ML) explanation techniques have been designed using ideas from cooperative game theory. These game-theoretic explainers suffer from high complexity, hindering their exact computation in practical settings. In our work, we focus on a wide class of linear game values, as well as coalitional values, for the marginal game based on a given ML model and predictor vector. By viewing these explainers as expectations over appropriate sample spaces, we design a novel Monte Carlo sampling algorithm that estimates them at a reduced complexity that depends linearly on the size of the background dataset. We set up a rigorous framework for the statistical analysis and obtain error bounds for our sampling methods. The advantage of this approach is that it is fast, easily implementable, and model-agnostic. Furthermore, it has similar statistical accuracy as other known estimation techniques that are more complex and model-specific. We provide rigorous proofs of s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#39046;&#22495;&#38543;&#26426;&#21270;&#65288;DR&#65289;&#26469;&#25552;&#39640;DL&#27169;&#22411;&#22312;&#29983;&#25104;&#22522;&#20110;MRI&#30340;&#33041;&#37096;sCT&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.10202</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22522;&#20110;&#33041;&#37096;MRI&#21040;CT&#21512;&#25104;&#30340;&#23545;&#27604;&#24230;&#27867;&#21270;&#25506;&#31350;
&lt;/p&gt;
&lt;p&gt;
Exploring contrast generalisation in deep learning-based brain MRI-to-CT synthesis. (arXiv:2303.10202v1 [physics.med-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10202
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39046;&#22495;&#38543;&#26426;&#21270;&#65288;DR&#65289;&#26469;&#25552;&#39640;DL&#27169;&#22411;&#22312;&#29983;&#25104;&#22522;&#20110;MRI&#30340;&#33041;&#37096;sCT&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#24050;&#32463;&#25552;&#20986;&#24182;&#22312;&#20020;&#24202;&#19978;&#36234;&#26469;&#36234;&#21463;&#21040;&#35748;&#21487;&#30340;&#21512;&#25104;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;sCT&#65289;&#20197;&#20415;&#22522;&#20110;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;MRI&#65289;&#30340;&#25918;&#30103;&#12290;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#26368;&#36817;&#23637;&#31034;&#20102;&#20174;&#22266;&#23450;MRI&#37319;&#38598;&#20013;&#29983;&#25104;&#20934;&#30830;sCT&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;MRI&#21327;&#35758;&#21487;&#33021;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#25110;&#22312;&#19981;&#21516;&#20013;&#24515;&#20043;&#38388;&#26377;&#25152;&#19981;&#21516;&#65292;&#23548;&#33268;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#24046;&#32780;&#29983;&#25104;&#20302;&#36136;&#37327;&#30340;sCT&#12290;&#30446;&#30340;&#65306;&#30740;&#31350;&#39046;&#22495;&#38543;&#26426;&#21270;&#65288;DR&#65289;&#20197;&#22686;&#21152;&#29992;&#20110;&#29983;&#25104;&#33041;&#37096;sCT &#30340;DL&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#26041;&#27861;&#65306;&#25910;&#38598;95&#21517;&#25509;&#21463;&#25918;&#30103;&#30340;&#24739;&#32773;&#30340;CT&#21644;&#30456;&#24212;&#30340;T1&#21152;&#26435;MRI /&#19981;&#21152;&#23545;&#27604;&#21058;&#65292;T2&#21152;&#26435;&#21644;FLAIR MRI&#65292;&#20854;&#20013;&#32771;&#34385;FLAIR&#20316;&#20026;&#30740;&#31350;&#27867;&#21270;&#30340;&#26410;&#35265;&#24207;&#21015;&#12290;&#35757;&#32451;&#20102;&#19968;&#20010;&#22522;&#20934;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#20854;&#20013;&#21253;&#25324;&#25110;&#19981;&#21253;&#25324;FLAIR&#24207;&#21015;&#20197;&#27979;&#35797;&#27169;&#22411;&#22312;&#27809;&#26377;DR&#30340;&#24773;&#20917;&#19979;&#30340;&#34920;&#29616;&#12290;&#35780;&#20272;&#20102;&#22270;&#20687;&#30456;&#20284;&#24615;&#21644;&#22522;&#20110;sCT&#30340;&#21058;&#37327;&#35745;&#21010;&#30340;&#20934;&#30830;&#24615;&#65292;&#20197;&#36873;&#25321;&#26368;&#20339;&#34920;&#29616;&#30340;DR&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Background: Synthetic computed tomography (sCT) has been proposed and increasingly clinically adopted to enable magnetic resonance imaging (MRI)-based radiotherapy. Deep learning (DL) has recently demonstrated the ability to generate accurate sCT from fixed MRI acquisitions. However, MRI protocols may change over time or differ between centres resulting in low-quality sCT due to poor model generalisation. Purpose: investigating domain randomisation (DR) to increase the generalisation of a DL model for brain sCT generation. Methods: CT and corresponding T1-weighted MRI with/without contrast, T2-weighted, and FLAIR MRI from 95 patients undergoing RT were collected, considering FLAIR the unseen sequence where to investigate generalisation. A ``Baseline'' generative adversarial network was trained with/without the FLAIR sequence to test how a model performs without DR. Image similarity and accuracy of sCT-based dose plans were assessed against CT to select the best-performing DR approach a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#30340;&#26080;&#30417;&#30563;&#22495;&#36716;&#31227;&#26041;&#27861;&#65292;&#36890;&#36807;&#21487;&#36870;&#26550;&#26500;&#20445;&#35777;&#24490;&#29615;&#19968;&#33268;&#24615;&#65292;&#36991;&#20813;&#20102;&#27169;&#24335;&#23849;&#28291;&#38382;&#39064;&#65292;&#22312;&#39640;&#20809;&#35889;&#25104;&#20687;&#21644;&#20809;&#22768;&#25104;&#20687;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.10191</link><description>&lt;p&gt;
&#26465;&#20214;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#26080;&#30417;&#30563;&#22495;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Domain Transfer with Conditional Invertible Neural Networks. (arXiv:2303.10191v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10191
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#30340;&#26080;&#30417;&#30563;&#22495;&#36716;&#31227;&#26041;&#27861;&#65292;&#36890;&#36807;&#21487;&#36870;&#26550;&#26500;&#20445;&#35777;&#24490;&#29615;&#19968;&#33268;&#24615;&#65292;&#36991;&#20813;&#20102;&#27169;&#24335;&#23849;&#28291;&#38382;&#39064;&#65292;&#22312;&#39640;&#20809;&#35889;&#25104;&#20687;&#21644;&#20809;&#22768;&#25104;&#20687;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#21307;&#23398;&#22270;&#20687;&#29983;&#25104;&#24050;&#25104;&#20026;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#21644;&#39564;&#35777;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#27169;&#25311;&#25968;&#25454;&#21644;&#30495;&#23454;&#25968;&#25454;&#20043;&#38388;&#30340;&#39046;&#22495;&#24046;&#36317;&#20173;&#26159;&#19968;&#20010;&#26680;&#24515;&#38590;&#39064;&#12290;&#23613;&#31649;&#22522;&#20110;&#24490;&#29615;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#31867;&#20284;&#26550;&#26500;&#30340;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#36716;&#31227;&#22312;&#35813;&#39046;&#22495;&#21462;&#24471;&#20102;&#23454;&#36136;&#24615;&#36827;&#23637;&#65292;&#20294;&#22312;&#26576;&#20123;&#29992;&#20363;&#20013;&#65292;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20173;&#26080;&#27861;&#29983;&#25104;&#20351;&#30456;&#20851;&#19979;&#28216;&#20219;&#21153;&#20135;&#29983;&#20196;&#20154;&#20449;&#26381;&#32467;&#26524;&#30340;&#35757;&#32451;&#22270;&#20687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#22522;&#20110;&#26465;&#20214;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#65288;cINNs&#65289;&#30340;&#22495;&#36716;&#31227;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#20316;&#20026;&#19968;&#20010;&#29305;&#21035;&#30340;&#20248;&#21183;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#20854;&#21487;&#36870;&#26550;&#26500;&#20174;&#26412;&#36136;&#19978;&#20445;&#35777;&#20102;&#24490;&#29615;&#19968;&#33268;&#24615;&#65292;&#32780;&#32593;&#32476;&#35757;&#32451;&#21487;&#20197;&#26377;&#25928;&#22320;&#36827;&#34892;&#26368;&#22823;&#20284;&#28982;&#35757;&#32451;&#12290;&#20026;&#20102;&#23637;&#31034;&#25105;&#20204;&#26041;&#27861;&#30340;&#36890;&#29992;&#36866;&#29992;&#24615;&#65292;&#25105;&#20204;&#23558;&#20854;&#24212;&#29992;&#20110;&#20004;&#20010;&#19981;&#21516;&#23610;&#24230;&#30340;&#20809;&#35889;&#25104;&#20687;&#27169;&#24577;&#8212;&#8212;&#39640;&#20809;&#35889;&#25104;&#20687;&#65288;&#20687;&#32032;&#32423;&#65289;&#21644;&#20809;&#22768;&#25104;&#20687;&#65288;&#22270;&#20687;&#32423;&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#35270;&#35273;&#30495;&#23454;&#24615;&#21644;&#23450;&#37327;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#20851;&#38190;&#22320;&#36991;&#20813;&#20102;&#27169;&#24335;&#23849;&#28291;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Synthetic medical image generation has evolved as a key technique for neural network training and validation. A core challenge, however, remains in the domain gap between simulations and real data. While deep learning-based domain transfer using Cycle Generative Adversarial Networks and similar architectures has led to substantial progress in the field, there are use cases in which state-of-the-art approaches still fail to generate training images that produce convincing results on relevant downstream tasks. Here, we address this issue with a domain transfer approach based on conditional invertible neural networks (cINNs). As a particular advantage, our method inherently guarantees cycle consistency through its invertible architecture, and network training can efficiently be conducted with maximum likelihood training. To showcase our method's generic applicability, we apply it to two spectral imaging modalities at different scales, namely hyperspectral imaging (pixel-level) and photoac
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23545;&#20302;&#36712;&#36947;&#19979;&#26410;&#32463;&#25511;&#21046;&#30340;&#22826;&#31354;&#29289;&#20307;&#36827;&#34892;&#20877;&#20837;&#39044;&#27979;&#65292;&#36991;&#20813;&#20102;&#29289;&#29702;&#24314;&#27169;&#35823;&#24046;&#30340;&#24433;&#21709;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#39044;&#27979;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.10183</link><description>&lt;p&gt;
&#19968;&#31181;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#29305;&#24449;&#24037;&#31243;&#26041;&#27861;&#39044;&#27979;&#22826;&#31354;&#29289;&#20307;&#22833;&#25511;&#20877;&#20837;&#30340;&#35770;&#25991;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A machine learning and feature engineering approach for the prediction of the uncontrolled re-entry of space objects. (arXiv:2303.10183v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10183
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23545;&#20302;&#36712;&#36947;&#19979;&#26410;&#32463;&#25511;&#21046;&#30340;&#22826;&#31354;&#29289;&#20307;&#36827;&#34892;&#20877;&#20837;&#39044;&#27979;&#65292;&#36991;&#20813;&#20102;&#29289;&#29702;&#24314;&#27169;&#35823;&#24046;&#30340;&#24433;&#21709;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#39044;&#27979;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22320;&#29699;&#36712;&#36947;&#19978;&#29289;&#20307;&#25968;&#37327;&#30340;&#19981;&#26029;&#22686;&#21152;&#65292;&#39044;&#35745;&#23558;&#20276;&#38543;&#30528;&#36234;&#26469;&#36234;&#39057;&#32321;&#30340;&#29289;&#20307;&#37325;&#36820;&#22320;&#29699;&#22823;&#27668;&#23618;&#12290;&#35768;&#22810;&#36825;&#26679;&#30340;&#20877;&#20837;&#37117;&#26159;&#26080;&#25511;&#30340;&#65292;&#22240;&#27492;&#39044;&#27979;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#24182;&#19988;&#23384;&#22312;&#22810;&#20010;&#19981;&#30830;&#23450;&#24615;&#12290;&#20256;&#32479;&#19978;&#65292;&#20877;&#20837;&#39044;&#27979;&#26159;&#22522;&#20110;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#25216;&#26415;&#26469;&#20256;&#25773;&#29289;&#20307;&#21160;&#21147;&#23398;&#65292;&#24182;&#23545;&#20316;&#29992;&#20110;&#29289;&#20307;&#30340;&#21147;&#36827;&#34892;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;&#19982;&#22823;&#27668;&#38459;&#21147;&#39044;&#27979;&#30456;&#20851;&#30340;&#24314;&#27169;&#35823;&#24046;&#21487;&#33021;&#23548;&#33268;&#39044;&#27979;&#31934;&#24230;&#19981;&#20339;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20174;&#22522;&#20110;&#29289;&#29702;&#30340;&#26041;&#27861;&#21521;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#30340;&#33539;&#24335;&#36716;&#21464;&#30340;&#21487;&#33021;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24207;&#21015;&#21040;&#24207;&#21015;&#32467;&#26500;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24320;&#21457;&#65292;&#29992;&#20110;&#39044;&#27979;&#20302;&#36712;&#36947;&#19979;&#65288;LEO&#65289;&#26410;&#32463;&#25511;&#21046;&#30340;&#29289;&#20307;&#30340;&#20877;&#20837;&#12290;&#35813;&#27169;&#22411;&#26159;&#22522;&#20110;&#24179;&#22343;&#39640;&#24230;&#26354;&#32447;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The continuously growing number of objects orbiting around the Earth is expected to be accompanied by an increasing frequency of objects re-entering the Earth's atmosphere. Many of these re-entries will be uncontrolled, making their prediction challenging and subject to several uncertainties. Traditionally, re-entry predictions are based on the propagation of the object's dynamics using state-of-the-art modelling techniques for the forces acting on the object. However, modelling errors, particularly related to the prediction of atmospheric drag may result in poor prediction accuracies. In this context, we explore the possibility to perform a paradigm shift, from a physics-based approach to a data-driven approach. To this aim, we present the development of a deep learning model for the re-entry prediction of uncontrolled objects in Low Earth Orbit (LEO). The model is based on a modified version of the Sequence-to-Sequence architecture and is trained on the average altitude profile as de
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#38024;&#23545;&#39640;&#32500;&#25968;&#25454;&#38598;&#30340;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;SFE&#65292;&#30001;&#25628;&#32034;&#20195;&#29702;&#21644;&#20004;&#20010;&#31639;&#23376;&#25191;&#34892;&#25506;&#32034;&#21644;&#24320;&#21457;&#38454;&#27573;&#65292;&#23545;&#20110;&#39640;&#32500;&#24230;&#25968;&#25454;&#38598;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#38477;&#32500;&#21518;&#24615;&#33021;&#19981;&#33021;&#25552;&#39640;</title><link>http://arxiv.org/abs/2303.10182</link><description>&lt;p&gt;
&#39640;&#32500;&#25968;&#25454;&#30340;&#31616;&#21333;&#12289;&#24555;&#36895;&#21644;&#39640;&#25928;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;SFE
&lt;/p&gt;
&lt;p&gt;
SFE: A Simple, Fast and Efficient Feature Selection Algorithm for High-Dimensional Data. (arXiv:2303.10182v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10182
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#38024;&#23545;&#39640;&#32500;&#25968;&#25454;&#38598;&#30340;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;SFE&#65292;&#30001;&#25628;&#32034;&#20195;&#29702;&#21644;&#20004;&#20010;&#31639;&#23376;&#25191;&#34892;&#25506;&#32034;&#21644;&#24320;&#21457;&#38454;&#27573;&#65292;&#23545;&#20110;&#39640;&#32500;&#24230;&#25968;&#25454;&#38598;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#38477;&#32500;&#21518;&#24615;&#33021;&#19981;&#33021;&#25552;&#39640;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#39640;&#32500;&#25968;&#25454;&#38598;&#30340;&#26032;&#22411;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;&#8212;&#8212;SFE&#65288;&#31616;&#21333;&#12289;&#24555;&#36895;&#12289;&#39640;&#25928;&#65289;&#12290;SFE&#31639;&#27861;&#20351;&#29992;&#25628;&#32034;&#20195;&#29702;&#21644;&#20004;&#20010;&#31639;&#23376;&#65288;&#38750;&#36873;&#25321;&#21644;&#36873;&#25321;&#65289;&#25191;&#34892;&#25628;&#32034;&#36807;&#31243;&#65292;&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#65306;&#25506;&#32034;&#21644;&#24320;&#21457;&#12290;&#22312;&#25506;&#32034;&#38454;&#27573;&#65292;&#38750;&#36873;&#25321;&#31639;&#23376;&#22312;&#25972;&#20010;&#38382;&#39064;&#25628;&#32034;&#31354;&#38388;&#20013;&#25628;&#32034;&#19982;&#20998;&#31867;&#32467;&#26524;&#19981;&#30456;&#20851;&#12289;&#20887;&#20313;&#12289;&#24494;&#19981;&#36275;&#36947;&#21644;&#26377;&#22122;&#22768;&#30340;&#29305;&#24449;&#65292;&#24182;&#23558;&#36825;&#20123;&#29305;&#24449;&#30340;&#29366;&#24577;&#20174;&#36873;&#25321;&#27169;&#24335;&#26356;&#25913;&#20026;&#38750;&#36873;&#25321;&#27169;&#24335;&#12290;&#22312;&#24320;&#21457;&#38454;&#27573;&#65292;&#36873;&#25321;&#31639;&#23376;&#22312;&#38382;&#39064;&#25628;&#32034;&#31354;&#38388;&#20013;&#25628;&#32034;&#23545;&#20998;&#31867;&#32467;&#26524;&#24433;&#21709;&#36739;&#22823;&#30340;&#29305;&#24449;&#65292;&#24182;&#23558;&#36825;&#20123;&#29305;&#24449;&#30340;&#29366;&#24577;&#20174;&#38750;&#36873;&#25321;&#27169;&#24335;&#26356;&#25913;&#20026;&#36873;&#25321;&#27169;&#24335;&#12290;&#35813;SFE&#31639;&#27861;&#22312;&#39640;&#32500;&#24230;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#20294;&#26159;&#65292;&#22312;&#38477;&#20302;&#25968;&#25454;&#32500;&#24230;&#20043;&#21518;&#65292;&#23427;&#30340;&#24615;&#33021;&#19981;&#33021;&#26174;&#33879;&#25552;&#39640;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#38656;&#35201;&#23545;&#38477;&#32500;&#21518;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, a new feature selection algorithm, called SFE (Simple, Fast, and Efficient), is proposed for high-dimensional datasets. The SFE algorithm performs its search process using a search agent and two operators: non-selection and selection. It comprises two phases: exploration and exploitation. In the exploration phase, the non-selection operator performs a global search in the entire problem search space for the irrelevant, redundant, trivial, and noisy features, and changes the status of the features from selected mode to non-selected mode. In the exploitation phase, the selection operator searches the problem search space for the features with a high impact on the classification results, and changes the status of the features from non-selected mode to selected mode. The proposed SFE is successful in feature selection from high-dimensional datasets. However, after reducing the dimensionality of a dataset, its performance cannot be increased significantly. In these situations
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#20139;&#20102;&#22312;&#20851;&#38190;&#22330;&#26223;&#19979;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26102;&#36164;&#28304;&#28040;&#32791;&#21644;&#24615;&#33021;&#20043;&#38388;&#30340;&#26435;&#34913;&#26041;&#27861;&#12290;&#32771;&#34385;&#21040;&#27169;&#22411;&#22312;&#20840;&#29699;&#35786;&#25152;&#20013;&#30340;&#37096;&#32626;&#65292;&#26426;&#22120;&#23398;&#20064;&#30028;&#27491;&#22312;&#20026;&#25913;&#36827;&#27169;&#22411;&#25928;&#29575;&#32780;&#21162;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.10181</link><description>&lt;p&gt;
&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#29615;&#22659;&#20013;&#36816;&#34892;&#20851;&#38190;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Operating critical machine learning models in resource constrained regimes. (arXiv:2303.10181v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10181
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#20139;&#20102;&#22312;&#20851;&#38190;&#22330;&#26223;&#19979;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26102;&#36164;&#28304;&#28040;&#32791;&#21644;&#24615;&#33021;&#20043;&#38388;&#30340;&#26435;&#34913;&#26041;&#27861;&#12290;&#32771;&#34385;&#21040;&#27169;&#22411;&#22312;&#20840;&#29699;&#35786;&#25152;&#20013;&#30340;&#37096;&#32626;&#65292;&#26426;&#22120;&#23398;&#20064;&#30028;&#27491;&#22312;&#20026;&#25913;&#36827;&#27169;&#22411;&#25928;&#29575;&#32780;&#21162;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#28145;&#24230;&#23398;&#20064;&#65292;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#21644;&#35745;&#31639;&#26426;&#36741;&#21161;&#24178;&#39044;&#26041;&#38754;&#30340;&#26368;&#26032;&#31361;&#30772;&#65292;&#20351;&#20854;&#24471;&#21040;&#24555;&#36895;&#21457;&#23637;&#12290;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#35757;&#32451;&#25968;&#25454;&#65292;&#35745;&#31639;&#21644;&#33021;&#28304;&#25104;&#26412;&#26041;&#38754;&#30340;&#36164;&#28304;&#28040;&#32791;&#26159;&#24040;&#22823;&#30340;&#12290;&#36825;&#20123;&#24040;&#22823;&#30340;&#36164;&#28304;&#25104;&#26412;&#21487;&#33021;&#20250;&#38459;&#30861;&#36825;&#20123;&#27169;&#22411;&#22312;&#20840;&#29699;&#35786;&#25152;&#20013;&#30340;&#37096;&#32626;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26426;&#22120;&#23398;&#20064;&#30028;&#27491;&#22312;&#21162;&#21147;&#24341;&#20837;&#36164;&#28304;&#25928;&#29575;&#30340;&#27010;&#24565;&#12290;&#20363;&#22914;&#65292;&#20351;&#29992;&#37327;&#21270;&#26469;&#20943;&#36731;&#20869;&#23384;&#28040;&#32791;&#12290;&#34429;&#28982;&#22823;&#22810;&#25968;&#36825;&#20123;&#26041;&#27861;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#20943;&#23569;&#36164;&#28304;&#21033;&#29992;&#65292;&#20294;&#21487;&#33021;&#20250;&#20197;&#24615;&#33021;&#20026;&#20195;&#20215;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#36164;&#28304;&#28040;&#32791;&#21644;&#24615;&#33021;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#29305;&#21035;&#26159;&#22312;&#35786;&#25152;&#31561;&#20851;&#38190;&#29615;&#22659;&#20013;&#20351;&#29992;&#30340;&#27169;&#22411;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
The accelerated development of machine learning methods, primarily deep learning, are causal to the recent breakthroughs in medical image analysis and computer aided intervention. The resource consumption of deep learning models in terms of amount of training data, compute and energy costs are known to be massive. These large resource costs can be barriers in deploying these models in clinics, globally. To address this, there are cogent efforts within the machine learning community to introduce notions of resource efficiency. For instance, using quantisation to alleviate memory consumption. While most of these methods are shown to reduce the resource utilisation, they could come at a cost in performance. In this work, we probe into the trade-off between resource consumption and performance, specifically, when dealing with models that are used in critical settings such as in clinics.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30495;&#23454;&#20020;&#24202;&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#39537;&#21160;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;Policy Constraint Q-Learning(PCQL)&#26469;&#23454;&#29616;&#20840;&#40635;&#33647;&#29289;&#21058;&#37327;&#25511;&#21046;&#65292;&#28155;&#21152;&#20102;&#20445;&#23432;Q-Learning&#26041;&#27861;&#21644;&#31574;&#30053;&#32422;&#26463;&#39033;&#20197;&#30830;&#20445;&#26234;&#33021;&#20307;&#20570;&#20986;&#26356;&#23433;&#20840;&#30340;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2303.10180</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#23433;&#20840;&#30340;&#19993;&#27850;&#37210;&#20840;&#40635;&#21058;&#37327;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Towards Safe Propofol Dosing during General Anesthesia Using Deep Offline Reinforcement Learning. (arXiv:2303.10180v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10180
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30495;&#23454;&#20020;&#24202;&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#39537;&#21160;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;Policy Constraint Q-Learning(PCQL)&#26469;&#23454;&#29616;&#20840;&#40635;&#33647;&#29289;&#21058;&#37327;&#25511;&#21046;&#65292;&#28155;&#21152;&#20102;&#20445;&#23432;Q-Learning&#26041;&#27861;&#21644;&#31574;&#30053;&#32422;&#26463;&#39033;&#20197;&#30830;&#20445;&#26234;&#33021;&#20307;&#20570;&#20986;&#26356;&#23433;&#20840;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#40635;&#37257;&#26377;&#26395;&#23454;&#29616;&#26356;&#31934;&#30830;&#21644;&#20010;&#24615;&#21270;&#30340;&#40635;&#37257;&#31649;&#29702;&#65292;&#20351;&#40635;&#37257;&#24072;&#20813;&#20110;&#37325;&#22797;&#24615;&#20219;&#21153;&#65292;&#19987;&#27880;&#20110;&#24739;&#32773;&#25163;&#26415;&#25252;&#29702;&#30340;&#26368;&#20851;&#38190;&#26041;&#38754;&#12290;&#24403;&#21069;&#30340;&#30740;&#31350;&#36890;&#24120;&#38598;&#20013;&#20110;&#21019;&#24314;&#27169;&#25311;&#29615;&#22659;&#65292;&#20197;&#20415;&#26234;&#33021;&#20307;&#36827;&#34892;&#23398;&#20064;&#12290;&#36825;&#20123;&#26041;&#27861;&#24050;&#32463;&#23637;&#31034;&#20986;&#33391;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#20294;&#31163;&#20020;&#24202;&#24212;&#29992;&#36824;&#26377;&#24456;&#22823;&#30340;&#24046;&#36317;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30495;&#23454;&#20020;&#24202;&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#39537;&#21160;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;Policy Constraint Q-Learning(PCQL)&#26469;&#35299;&#20915;&#23398;&#20064;&#40635;&#37257;&#31574;&#30053;&#30340;&#38382;&#39064;&#12290;&#39318;&#20808;&#24341;&#20837;&#20102;&#20445;&#23432;Q-Learning&#26041;&#27861;&#20197;&#32531;&#35299;&#33073;&#32447;&#24773;&#20917;&#19979;Q&#20989;&#25968;&#36807;&#24230;&#20272;&#35745;&#30340;&#38382;&#39064;&#12290;&#22312;&#26234;&#33021;&#20307;&#30340;&#22521;&#35757;&#20013;&#28155;&#21152;&#19968;&#39033;&#31574;&#30053;&#32422;&#26463;&#39033;&#65292;&#20197;&#20445;&#25345;&#26234;&#33021;&#20307;&#21644;&#40635;&#37257;&#24072;&#30340;&#31574;&#30053;&#20998;&#24067;&#19968;&#33268;&#65292;&#20197;&#30830;&#20445;&#26234;&#33021;&#20307;&#22312;&#20840;&#40635;&#24773;&#26223;&#19979;&#20570;&#20986;&#26356;&#23433;&#20840;&#30340;&#20915;&#31574;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;PCQL&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated anesthesia promises to enable more precise and personalized anesthetic administration and free anesthesiologists from repetitive tasks, allowing them to focus on the most critical aspects of a patient's surgical care. Current research has typically focused on creating simulated environments from which agents can learn. These approaches have demonstrated good experimental results, but are still far from clinical application. In this paper, Policy Constraint Q-Learning (PCQL), a data-driven reinforcement learning algorithm for solving the problem of learning anesthesia strategies on real clinical datasets, is proposed. Conservative Q-Learning was first introduced to alleviate the problem of Q function overestimation in an offline context. A policy constraint term is added to agent training to keep the policy distribution of the agent and the anesthesiologist consistent to ensure safer decisions made by the agent in anesthesia scenarios. The effectiveness of PCQL was validated b
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;QUBO&#30340;&#20998;&#23376;&#25351;&#32441;&#65292;&#20351;&#29992;&#20108;&#27425;&#26080;&#32422;&#26463;&#20108;&#36827;&#21046;&#20248;&#21270;&#23547;&#25214;&#26356;&#26377;&#25928;&#30340;&#20132;&#20114;&#25351;&#32441;&#65292;&#24182;&#22312;QM9&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;</title><link>http://arxiv.org/abs/2303.10179</link><description>&lt;p&gt;
&#22522;&#20110;QUBO&#30340;&#20998;&#23376;&#25351;&#32441;&#29992;&#20110;&#21270;&#23398;&#24615;&#36136;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
QUBO-inspired Molecular Fingerprint for Chemical Property Prediction. (arXiv:2303.10179v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10179
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;QUBO&#30340;&#20998;&#23376;&#25351;&#32441;&#65292;&#20351;&#29992;&#20108;&#27425;&#26080;&#32422;&#26463;&#20108;&#36827;&#21046;&#20248;&#21270;&#23547;&#25214;&#26356;&#26377;&#25928;&#30340;&#20132;&#20114;&#25351;&#32441;&#65292;&#24182;&#22312;QM9&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#25351;&#32441;&#24191;&#27867;&#29992;&#20110;&#39044;&#27979;&#21270;&#23398;&#24615;&#36136;&#65292;&#36873;&#25321;&#21512;&#36866;&#30340;&#25351;&#32441;&#38750;&#24120;&#37325;&#35201;&#12290;&#25105;&#20204;&#22522;&#20110;&#20351;&#29992;&#26356;&#26377;&#25928;&#25351;&#32441;&#36827;&#34892;&#39044;&#27979;&#24615;&#33021;&#26356;&#22909;&#30340;&#20551;&#35774;&#65292;&#29983;&#25104;&#20102;&#26032;&#30340;&#25351;&#32441;&#12290;&#25105;&#20204;&#29983;&#25104;&#20102;&#26377;&#25928;&#30340;&#20132;&#20114;&#25351;&#32441;&#65292;&#23427;&#20204;&#26159;&#22810;&#31181;&#22522;&#30784;&#25351;&#32441;&#30340;&#20056;&#31215;&#12290;&#30001;&#20110;&#35745;&#31639;&#30340;&#38480;&#21046;&#65292;&#35780;&#20272;&#25152;&#26377;&#20132;&#20114;&#25351;&#32441;&#30340;&#32452;&#21512;&#26159;&#22256;&#38590;&#30340;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#23547;&#25214;&#26356;&#26377;&#25928;&#30340;&#20132;&#20114;&#25351;&#32441;&#38382;&#39064;&#36716;&#21270;&#20026;&#20108;&#27425;&#26080;&#32422;&#26463;&#20108;&#36827;&#21046;&#20248;&#21270;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;QM9&#25968;&#25454;&#38598;&#21457;&#29616;&#20102;&#26377;&#25928;&#30340;&#20132;&#20114;&#25351;&#32441;&#12290;
&lt;/p&gt;
&lt;p&gt;
Molecular fingerprints are widely used for predicting chemical properties, and selecting appropriate fingerprints is important. We generate new fingerprints based on the assumption that a performance of prediction using a more effective fingerprint is better. We generate effective interaction fingerprints that are the product of multiple base fingerprints. It is difficult to evaluate all combinations of interaction fingerprints because of computational limitations. Against this problem, we transform a problem of searching more effective interaction fingerprints into a quadratic unconstrained binary optimization problem. In this study, we found effective interaction fingerprints using QM9 dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22270;&#34920;&#33258;&#32534;&#30721;&#22120;&#29992;&#20110;&#28145;&#24230;&#38750;&#21442;&#25968;&#20272;&#35745;&#20869;&#37096;&#25968;&#25454;&#32467;&#26500;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#24191;&#20041;&#35823;&#24046;&#20445;&#35777;&#21644;&#21435;&#22122;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.09863</link><description>&lt;p&gt;
&#36890;&#36807;&#22270;&#34920;&#33258;&#32534;&#30721;&#22120;&#36827;&#34892;&#20869;&#37096;&#25968;&#25454;&#32467;&#26500;&#30340;&#28145;&#24230;&#38750;&#21442;&#25968;&#20272;&#35745;&#65306;&#24191;&#20041;&#35823;&#24046;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Nonparametric Estimation of Intrinsic Data Structures by Chart Autoencoders: Generalization Error and Robustness. (arXiv:2303.09863v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09863
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22270;&#34920;&#33258;&#32534;&#30721;&#22120;&#29992;&#20110;&#28145;&#24230;&#38750;&#21442;&#25968;&#20272;&#35745;&#20869;&#37096;&#25968;&#25454;&#32467;&#26500;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#24191;&#20041;&#35823;&#24046;&#20445;&#35777;&#21644;&#21435;&#22122;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#32534;&#30721;&#22120;&#22312;&#23398;&#20064;&#39640;&#32500;&#25968;&#25454;&#30340;&#20302;&#32500;&#28508;&#22312;&#29305;&#24449;&#26041;&#38754;&#24050;&#32463;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#23637;&#29616;&#20986;&#20102;&#26174;&#30528;&#30340;&#25104;&#21151;&#12290;&#20551;&#35774;&#25968;&#25454;&#22312;&#20302;&#32500;&#27969;&#24418;&#38468;&#36817;&#37319;&#26679;&#65292;&#25105;&#20204;&#37319;&#29992;&#22270;&#34920;&#33258;&#32534;&#30721;&#22120;&#65292;&#23558;&#25968;&#25454;&#32534;&#30721;&#20026;&#19968;&#32452;&#22270;&#34920;&#19978;&#30340;&#20302;&#32500;&#28508;&#22312;&#29305;&#24449;&#65292;&#20174;&#32780;&#20445;&#30041;&#20102;&#25968;&#25454;&#27969;&#24418;&#30340;&#25299;&#25169;&#21644;&#20960;&#20309;&#12290;&#25105;&#20204;&#30340;&#35770;&#25991;&#20026;&#22270;&#34920;&#33258;&#32534;&#30721;&#22120;&#30340;&#24191;&#20041;&#35823;&#24046;&#24314;&#31435;&#20102;&#32479;&#35745;&#20445;&#35777;&#65292;&#24182;&#19988;&#36890;&#36807;&#32771;&#34385;$d$&#32500;&#27969;&#24418;&#19978;$n$&#20010;&#24102;&#22122;&#22768;&#35757;&#32451;&#26679;&#26412;&#21450;&#20854;&#26080;&#22122;&#22768;&#23545;&#24212;&#29289;&#26469;&#23637;&#31034;&#23427;&#20204;&#30340;&#21435;&#22122;&#33021;&#21147;&#12290;&#36890;&#36807;&#35757;&#32451;&#33258;&#32534;&#30721;&#22120;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22270;&#34920;&#33258;&#32534;&#30721;&#22120;&#33021;&#22815;&#26377;&#25928;&#22320;&#21435;&#22122;&#36755;&#20837;&#25968;&#25454;&#21644;&#27491;&#24577;&#20998;&#24067;&#22122;&#22768;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#36866;&#24403;&#30340;&#32593;&#32476;&#26550;&#26500;&#19979;&#65292;&#22270;&#34920;&#33258;&#32534;&#30721;&#22120;&#23454;&#29616;&#20102;&#19968;&#20010;&#22823;&#33268;&#20026;$\displaystyle n^{-\frac{2}{d+2}}\log^4 n$&#38454;&#30340;&#24179;&#26041;&#24191;&#20041;&#35823;&#24046;&#65292;&#35813;&#35823;&#24046;&#21462;&#20915;&#20110;&#27969;&#24418;&#30340;&#20869;&#22312;&#32500;&#24230;&#65292;&#24182;&#19988;&#20165;&#24369;&#20381;&#36182;&#20110;&#26679;&#26412;&#25968;&#37327;$n$&#12290;
&lt;/p&gt;
&lt;p&gt;
Autoencoders have demonstrated remarkable success in learning low-dimensional latent features of high-dimensional data across various applications. Assuming that data are sampled near a low-dimensional manifold, we employ chart autoencoders, which encode data into low-dimensional latent features on a collection of charts, preserving the topology and geometry of the data manifold. Our paper establishes statistical guarantees on the generalization error of chart autoencoders, and we demonstrate their denoising capabilities by considering $n$ noisy training samples, along with their noise-free counterparts, on a $d$-dimensional manifold. By training autoencoders, we show that chart autoencoders can effectively denoise the input data with normal noise. We prove that, under proper network architectures, chart autoencoders achieve a squared generalization error in the order of $\displaystyle n^{-\frac{2}{d+2}}\log^4 n$, which depends on the intrinsic dimension of the manifold and only weakly
&lt;/p&gt;</description></item><item><title>SemDeDup&#26159;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#23884;&#20837;&#26469;&#35782;&#21035;&#21644;&#21024;&#38500;&#35821;&#20041;&#37325;&#22797;&#39033;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;LAION&#30340;&#23376;&#38598;&#36827;&#34892;&#20998;&#26512;&#65292;SemDeDup&#21487;&#20197;&#26368;&#23567;&#21270;&#24615;&#33021;&#25439;&#22833;&#30340;&#21516;&#26102;&#21024;&#38500;50%&#30340;&#25968;&#25454;&#65292;&#23454;&#38469;&#19978;&#23558;&#35757;&#32451;&#26102;&#38388;&#20943;&#21322;&#12290;&#27492;&#22806;&#65292;SemDeDup&#22312;&#25552;&#20379;&#25928;&#29575;&#25910;&#30410;&#30340;&#21516;&#26102;&#25913;&#36827;&#20102;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.09540</link><description>&lt;p&gt;
SemDeDup:&#36890;&#36807;&#35821;&#20041;&#21435;&#37325;&#23454;&#29616;&#32593;&#32476;&#35268;&#27169;&#25968;&#25454;&#30340;&#39640;&#25928;&#23398;&#20064;&#65288;arXiv:2303.09540v1 [cs.LG]&#65289;
&lt;/p&gt;
&lt;p&gt;
SemDeDup: Data-efficient learning at web-scale through semantic deduplication. (arXiv:2303.09540v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09540
&lt;/p&gt;
&lt;p&gt;
SemDeDup&#26159;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#23884;&#20837;&#26469;&#35782;&#21035;&#21644;&#21024;&#38500;&#35821;&#20041;&#37325;&#22797;&#39033;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;LAION&#30340;&#23376;&#38598;&#36827;&#34892;&#20998;&#26512;&#65292;SemDeDup&#21487;&#20197;&#26368;&#23567;&#21270;&#24615;&#33021;&#25439;&#22833;&#30340;&#21516;&#26102;&#21024;&#38500;50%&#30340;&#25968;&#25454;&#65292;&#23454;&#38469;&#19978;&#23558;&#35757;&#32451;&#26102;&#38388;&#20943;&#21322;&#12290;&#27492;&#22806;&#65292;SemDeDup&#22312;&#25552;&#20379;&#25928;&#29575;&#25910;&#30410;&#30340;&#21516;&#26102;&#25913;&#36827;&#20102;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#36827;&#23637;&#24456;&#22823;&#31243;&#24230;&#19978;&#26159;&#30001;&#28023;&#37327;&#25968;&#25454;&#30340;&#22686;&#21152;&#25512;&#21160;&#30340;&#12290;&#28982;&#32780;&#65292;&#20687;LAION&#36825;&#26679;&#30340;&#22823;&#22411;&#32593;&#32476;&#35268;&#27169;&#25968;&#25454;&#38598;&#22312;&#38500;&#26597;&#25214;&#31934;&#30830;&#37325;&#22797;&#39033;&#22806;&#65292;&#22823;&#37096;&#20998;&#26410;&#32463;&#31934;&#24515;&#31579;&#36873;&#65292;&#21487;&#33021;&#23384;&#22312;&#24456;&#22810;&#20887;&#20313;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;SemDeDup&#65292;&#19968;&#31181;&#22522;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#23884;&#20837;&#26469;&#35782;&#21035;&#21644;&#21024;&#38500;&#35821;&#20041;&#37325;&#22797;&#39033;&#30340;&#26041;&#27861;&#65306;&#21363;&#35821;&#20041;&#19978;&#30456;&#20284;&#20294;&#24182;&#38750;&#23436;&#20840;&#30456;&#21516;&#30340;&#25968;&#25454;&#23545;&#12290;&#21435;&#38500;&#35821;&#20041;&#37325;&#22797;&#39033;&#21487;&#20197;&#20445;&#25345;&#24615;&#33021;&#24182;&#21152;&#36895;&#23398;&#20064;&#12290;&#36890;&#36807;&#23545;LAION&#30340;&#23376;&#38598;&#36827;&#34892;&#20998;&#26512;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;SemDeDup&#21487;&#20197;&#26368;&#23567;&#21270;&#24615;&#33021;&#25439;&#22833;&#30340;&#21516;&#26102;&#21024;&#38500;50%&#30340;&#25968;&#25454;&#65292;&#23454;&#38469;&#19978;&#23558;&#35757;&#32451;&#26102;&#38388;&#20943;&#21322;&#12290;&#27492;&#22806;&#65292;&#24615;&#33021;&#22312;&#20998;&#24067;&#20197;&#22806;&#24471;&#21040;&#25552;&#39640;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#20998;&#26512;&#22312;&#37096;&#20998;&#31579;&#36873;&#36807;&#30340;&#25968;&#25454;&#38598;C4&#19978;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;SemDeDup&#22312;&#25552;&#20379;&#25928;&#29575;&#25910;&#30410;&#30340;&#21516;&#26102;&#25913;&#36827;&#20102;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;SemDeDup&#25552;&#20379;&#20102;&#19968;&#20010;&#21033;&#29992;&#36136;&#37327;&#23884;&#20837;&#31616;&#21333;&#26041;&#27861;&#26469;&#20351;&#27169;&#22411;&#26356;&#24555;&#22320;&#23398;&#20064;&#30340;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Progress in machine learning has been driven in large part by massive increases in data. However, large web-scale datasets such as LAION are largely uncurated beyond searches for exact duplicates, potentially leaving much redundancy. Here, we introduce SemDeDup, a method which leverages embeddings from pre-trained models to identify and remove semantic duplicates: data pairs which are semantically similar, but not exactly identical. Removing semantic duplicates preserves performance and speeds up learning. Analyzing a subset of LAION, we show that SemDeDup can remove 50% of the data with minimal performance loss, effectively halving training time. Moreover, performance increases out of distribution. Also, analyzing language models trained on C4, a partially curated dataset, we show that SemDeDup improves over prior approaches while providing efficiency gains. SemDeDup provides an example of how simple ways of leveraging quality embeddings can be used to make models learn faster with le
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#32954;&#37096;&#20998;&#21106;&#36827;&#34892;&#39044;&#22788;&#29702;&#65292;&#20854;&#39564;&#35777; F1 &#20998;&#25968;&#22312;&#39044;&#27979; CT &#25195;&#25551;&#20013; COVID-19 &#30340;&#23384;&#22312;&#21644;&#20005;&#37325;&#31243;&#24230;&#26041;&#38754;&#26174;&#33879;&#36229;&#36807;&#22522;&#32447;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2303.09440</link><description>&lt;p&gt;
&#21033;&#29992;&#32954;&#37096;&#20998;&#21106;&#22686;&#24378;CT&#25195;&#25551;&#26816;&#27979;COVID-19&#30340;&#23384;&#22312;&#21644;&#20005;&#37325;&#31243;&#24230;
&lt;/p&gt;
&lt;p&gt;
Enhanced detection of the presence and severity of COVID-19 from CT scans using lung segmentation. (arXiv:2303.09440v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09440
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#32954;&#37096;&#20998;&#21106;&#36827;&#34892;&#39044;&#22788;&#29702;&#65292;&#20854;&#39564;&#35777; F1 &#20998;&#25968;&#22312;&#39044;&#27979; CT &#25195;&#25551;&#20013; COVID-19 &#30340;&#23384;&#22312;&#21644;&#20005;&#37325;&#31243;&#24230;&#26041;&#38754;&#26174;&#33879;&#36229;&#36807;&#22522;&#32447;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25913;&#36827;&#21307;&#23398;&#25104;&#20687;&#30340;&#33258;&#21160;&#21270;&#20998;&#26512;&#23558;&#20026;&#20020;&#24202;&#21307;&#29983;&#25552;&#20379;&#26356;&#22810;&#27835;&#30103;&#24739;&#32773;&#30340;&#36873;&#25321;&#12290;&#22312;2023&#24180;&#30340;AI-MIA-COV19D &#31454;&#36187;&#20013;&#65292;&#36890;&#36807; CT &#25195;&#25551;&#26816;&#27979; COVID-19 &#30340;&#23384;&#22312;&#21644;&#20005;&#37325;&#31243;&#24230;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#24471;&#21040;&#20102;&#27979;&#35797;&#21644;&#25913;&#36827;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;2022&#24180;&#31454;&#36187;&#25552;&#20132;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411; Cov3d &#30340;&#31532;&#20108;&#20010;&#29256;&#26412;&#65292;&#36890;&#36807;&#23545; CT &#25195;&#25551;&#36827;&#34892;&#32954;&#37096;&#20998;&#21106;&#21644;&#35009;&#21098;&#36755;&#20837;&#21040;&#35813;&#21306;&#22495;&#30340;&#39044;&#22788;&#29702;&#27493;&#39588;&#23545;&#27169;&#22411;&#36827;&#34892;&#20102;&#25913;&#36827;&#12290;&#22312;&#39044;&#27979; CT &#25195;&#25551;&#20013; COVID-19 &#23384;&#22312;&#26041;&#38754;&#65292;&#35813;&#27169;&#22411;&#24471;&#21040;&#20102;92.2%&#30340;&#39564;&#35777;&#23439; F1 &#20998;&#25968;&#65292;&#26126;&#26174;&#39640;&#20110;&#22522;&#32447;74%&#12290;&#23427;&#23545;&#20219;&#21153;&#20108;&#30340;&#39564;&#35777;&#38598;&#39044;&#27979; COVID-19 &#30340;&#20005;&#37325;&#31243;&#24230;&#30340;&#23439; F1 &#20998;&#25968;&#20026;67%&#65292;&#39640;&#20110;&#22522;&#32447;38%&#12290;
&lt;/p&gt;
&lt;p&gt;
Improving automated analysis of medical imaging will provide clinicians more options in providing care for patients. The 2023 AI-enabled Medical Image Analysis Workshop and Covid-19 Diagnosis Competition (AI-MIA-COV19D) provides an opportunity to test and refine machine learning methods for detecting the presence and severity of COVID-19 in patients from CT scans. This paper presents version 2 of Cov3d, a deep learning model submitted in the 2022 competition. The model has been improved through a preprocessing step which segments the lungs in the CT scan and crops the input to this region. It results in a validation macro F1 score for predicting the presence of COVID-19 in the CT scans at 92.2% which is significantly above the baseline of 74%. It gives a macro F1 score for predicting the severity of COVID-19 on the validation set for task 2 as 67% which is above the baseline of 38%.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#40657;&#33394;&#32032;&#30244;&#20010;&#24615;&#21270;&#33402;&#26415;&#27835;&#30103;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#24555;&#36895;&#29983;&#25104;&#20986;&#20010;&#24615;&#21270;&#30340;&#33402;&#26415;&#27835;&#30103;&#20869;&#23481;&#65292;&#26377;&#25928;&#20943;&#36731;&#24739;&#32773;&#24515;&#29702;&#21387;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.09232</link><description>&lt;p&gt;
&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#40657;&#33394;&#32032;&#30244;&#20010;&#24615;&#21270;&#33402;&#26415;&#27835;&#30103;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Generative Adversarial Network for Personalized Art Therapy in Melanoma Disease Management. (arXiv:2303.09232v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09232
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#40657;&#33394;&#32032;&#30244;&#20010;&#24615;&#21270;&#33402;&#26415;&#27835;&#30103;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#24555;&#36895;&#29983;&#25104;&#20986;&#20010;&#24615;&#21270;&#30340;&#33402;&#26415;&#27835;&#30103;&#20869;&#23481;&#65292;&#26377;&#25928;&#20943;&#36731;&#24739;&#32773;&#24515;&#29702;&#21387;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40657;&#33394;&#32032;&#30244;&#26159;&#19968;&#31181;&#26368;&#33268;&#21629;&#30340;&#30382;&#32932;&#30284;&#65292;&#24739;&#32773;&#23481;&#26131;&#24739;&#26377;&#24515;&#29702;&#20581;&#24247;&#30142;&#30149;&#65292;&#36825;&#21487;&#33021;&#20250;&#38477;&#20302;&#30284;&#30151;&#27835;&#30103;&#30340;&#25928;&#26524;&#21644;&#24739;&#32773;&#29992;&#33647;&#35745;&#21010;&#30340;&#36981;&#24490;&#24615;&#12290;&#22240;&#27492;&#65292;&#20445;&#25252;&#24739;&#32773;&#22312;&#25509;&#21463;&#27835;&#30103;&#26102;&#30340;&#24515;&#29702;&#20581;&#24247;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#33402;&#26415;&#27835;&#30103;&#26041;&#27861;&#24182;&#19981;&#26159;&#20010;&#24615;&#21270;&#21644;&#29420;&#29305;&#30340;&#12290;&#25105;&#20204;&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#35757;&#32451;&#26377;&#32032;&#30340;&#22270;&#20687;&#39118;&#26684;&#36716;&#25442;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#24555;&#36895;&#20174;&#20010;&#20154;&#30382;&#32932;&#38236;&#40657;&#33394;&#32032;&#30244;&#22270;&#20687;&#20013;&#29983;&#25104;&#29420;&#29305;&#30340;&#33402;&#26415;&#21697;&#65292;&#20316;&#20026;&#36741;&#21161;&#40657;&#33394;&#32032;&#30244;&#30149;&#31649;&#29702;&#20013;&#30340;&#33402;&#26415;&#27835;&#30103;&#24037;&#20855;&#12290;&#35270;&#35273;&#33402;&#26415;&#27427;&#36175;&#26159;&#30142;&#30149;&#31649;&#29702;&#20013;&#24120;&#35265;&#30340;&#33402;&#26415;&#27835;&#30103;&#24418;&#24335;&#65292;&#23427;&#21487;&#20197;&#26126;&#26174;&#20943;&#36731;&#24515;&#29702;&#21387;&#21147;&#12290;&#25105;&#20204;&#22522;&#20110;&#24490;&#29615;&#19968;&#33268;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;cycle-consistent generative adversarial network&#65289;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#39118;&#26684;&#36716;&#25442;&#30340;&#32593;&#32476;&#26469;&#29983;&#25104;&#26469;&#33258;&#30382;&#32932;&#38236;&#40657;&#33394;&#32032;&#30244;&#22270;&#20687;&#30340;&#20010;&#24615;&#21270;&#21644;&#29420;&#29305;&#33402;&#26415;&#21697;&#12290;&#25105;&#20204;&#24320;&#21457;&#30340;&#27169;&#22411;&#23558;&#40657;&#33394;&#32032;&#30244;&#22270;&#20687;&#36716;&#25442;&#20026;&#29420;&#29305;&#30340;&#33457;&#21321;&#20027;&#39064;&#33402;&#26415;&#21697;
&lt;/p&gt;
&lt;p&gt;
Melanoma is the most lethal type of skin cancer. Patients are vulnerable to mental health illnesses which can reduce the effectiveness of the cancer treatment and the patients adherence to drug plans. It is crucial to preserve the mental health of patients while they are receiving treatment. However, current art therapy approaches are not personal and unique to the patient. We aim to provide a well-trained image style transfer model that can quickly generate unique art from personal dermoscopic melanoma images as an additional tool for art therapy in disease management of melanoma. Visual art appreciation as a common form of art therapy in disease management that measurably reduces the degree of psychological distress. We developed a network based on the cycle-consistent generative adversarial network for style transfer that generates personalized and unique artworks from dermoscopic melanoma images. We developed a model that converts melanoma images into unique flower-themed artworks 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#38899;&#35270;&#39057;&#35821;&#38899;&#35782;&#21035;&#22312;&#22810;&#27169;&#24577;&#36755;&#20837;&#25439;&#22351;&#24773;&#20917;&#19979;&#30340;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#38899;&#35270;&#39057;&#21487;&#38752;&#24615;&#35780;&#20998;&#27169;&#22359;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#38887;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.08536</link><description>&lt;p&gt;
&#35266;&#30475;&#25110;&#21548;&#21462;&#65306;&#20855;&#26377;&#35270;&#35273;&#25439;&#22351;&#24314;&#27169;&#21644;&#21487;&#38752;&#24615;&#35780;&#20998;&#30340;&#24378;&#38887;&#38899;&#35270;&#39057;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Watch or Listen: Robust Audio-Visual Speech Recognition with Visual Corruption Modeling and Reliability Scoring. (arXiv:2303.08536v1 [cs.MM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08536
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#38899;&#35270;&#39057;&#35821;&#38899;&#35782;&#21035;&#22312;&#22810;&#27169;&#24577;&#36755;&#20837;&#25439;&#22351;&#24773;&#20917;&#19979;&#30340;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#38899;&#35270;&#39057;&#21487;&#38752;&#24615;&#35780;&#20998;&#27169;&#22359;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#38887;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#38899;&#35270;&#39057;&#35821;&#38899;&#35782;&#21035;&#65288;AVSR&#65289;&#22312;&#22810;&#27169;&#24577;&#36755;&#20837;&#25439;&#22351;&#24773;&#20917;&#19979;&#36827;&#34892;&#30740;&#31350;&#65292;&#20854;&#20013;&#38899;&#39057;&#36755;&#20837;&#21644;&#35270;&#35273;&#36755;&#20837;&#22343;&#21463;&#25439;&#65292;&#36825;&#22312;&#20808;&#21069;&#30340;&#30740;&#31350;&#26041;&#21521;&#20013;&#27809;&#26377;&#24471;&#21040;&#24456;&#22909;&#30340;&#35299;&#20915;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#38598;&#20013;&#20110;&#22914;&#20309;&#29992;&#28165;&#26224;&#30340;&#35270;&#35273;&#36755;&#20837;&#26469;&#34917;&#20805;&#21463;&#25439;&#30340;&#38899;&#39057;&#36755;&#20837;&#65292;&#20551;&#35774;&#21487;&#29992;&#28165;&#26224;&#30340;&#35270;&#35273;&#36755;&#20837;&#65292;&#20294;&#22312;&#29616;&#23454;&#29983;&#27963;&#20013;&#65292;&#28165;&#26224;&#30340;&#35270;&#35273;&#36755;&#20837;&#24182;&#19981;&#24635;&#26159;&#21487;&#29992;&#30340;&#65292;&#29978;&#33267;&#21487;&#33021;&#34987;&#36974;&#25377;&#30340;&#21767;&#37096;&#21306;&#22495;&#25110;&#22122;&#38899;&#25152;&#25439;&#22351;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper deals with Audio-Visual Speech Recognition (AVSR) under multimodal input corruption situations where audio inputs and visual inputs are both corrupted, which is not well addressed in previous research directions. Previous studies have focused on how to complement the corrupted audio inputs with the clean visual inputs with the assumption of the availability of clean visual inputs. However, in real life, clean visual inputs are not always accessible and can even be corrupted by occluded lip regions or noises. Thus, we firstly analyze that the previous AVSR models are not indeed robust to the corruption of multimodal input streams, the audio and the visual inputs, compared to uni-modal models. Then, we design multimodal input corruption modeling to develop robust AVSR models. Lastly, we propose a novel AVSR framework, namely Audio-Visual Reliability Scoring module (AV-RelScore), that is robust to the corrupted multimodal inputs. The AV-RelScore can determine which input modal 
&lt;/p&gt;</description></item><item><title>R^2&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21306;&#38388;&#27491;&#21017;&#21270;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#26377;&#25928;&#30340;&#26368;&#23567;&#20540;&#21644;&#26368;&#22823;&#20540;&#35843;&#25972;&#26435;&#37325;&#20998;&#24067;&#65292;&#20174;&#32780;&#20351;&#27169;&#22411;&#21387;&#32553;&#21644;&#37327;&#21270;&#25216;&#26415;&#33021;&#22815;&#26356;&#22909;&#22320;&#21033;&#29992;&#20854;&#25968;&#20540;&#34920;&#31034;&#33021;&#21147;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#20248;&#21270;&#30340;&#36136;&#37327;&#65292;&#23588;&#20854;&#26159;&#22312;&#36739;&#20302;&#20301;&#19978;&#12290;</title><link>http://arxiv.org/abs/2303.08253</link><description>&lt;p&gt;
R^2: &#22522;&#20110;&#21306;&#38388;&#27491;&#21017;&#21270;&#30340;&#27169;&#22411;&#21387;&#32553;&#19982;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
R^2: Range Regularization for Model Compression and Quantization. (arXiv:2303.08253v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08253
&lt;/p&gt;
&lt;p&gt;
R^2&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21306;&#38388;&#27491;&#21017;&#21270;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#26377;&#25928;&#30340;&#26368;&#23567;&#20540;&#21644;&#26368;&#22823;&#20540;&#35843;&#25972;&#26435;&#37325;&#20998;&#24067;&#65292;&#20174;&#32780;&#20351;&#27169;&#22411;&#21387;&#32553;&#21644;&#37327;&#21270;&#25216;&#26415;&#33021;&#22815;&#26356;&#22909;&#22320;&#21033;&#29992;&#20854;&#25968;&#20540;&#34920;&#31034;&#33021;&#21147;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#20248;&#21270;&#30340;&#36136;&#37327;&#65292;&#23588;&#20854;&#26159;&#22312;&#36739;&#20302;&#20301;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#27491;&#21017;&#21270;&#26159;&#19968;&#31181;&#24191;&#27867;&#24212;&#29992;&#20110;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#30340;&#25216;&#26415;&#65292;&#20063;&#21487;&#29992;&#20110;&#35843;&#25972;&#26435;&#37325;&#20998;&#24067;&#20197;&#36798;&#21040;&#21508;&#31181;&#30446;&#30340;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#26435;&#37325;&#27491;&#21017;&#21270;&#26469;&#36741;&#21161;&#27169;&#22411;&#37327;&#21270;&#21644;&#21387;&#32553;&#25216;&#26415;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#21306;&#38388;&#27491;&#21017;&#21270;(R^2)&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#27169;&#22411;&#20248;&#21270;&#30340;&#36136;&#37327;&#65292;&#37325;&#28857;&#25918;&#22312;&#38450;&#27490;&#24322;&#24120;&#20540;&#26041;&#38754;&#12290;&#36890;&#36807;&#26377;&#25928;&#22320;&#35843;&#25972;&#20998;&#24067;&#20013;&#30340;&#26368;&#23567;&#20540;&#21644;&#26368;&#22823;&#20540;&#65292;&#23558;&#25972;&#20010;&#20998;&#24067;&#22609;&#36896;&#25104;&#32039;&#20945;&#30340;&#24418;&#29366;&#65292;&#20174;&#32780;&#20351;&#27169;&#22411;&#21387;&#32553;&#21644;&#37327;&#21270;&#25216;&#26415;&#33021;&#22815;&#26356;&#22909;&#22320;&#21033;&#29992;&#23427;&#20204;&#26377;&#38480;&#30340;&#25968;&#20540;&#34920;&#31034;&#33021;&#21147;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;L-inf&#27491;&#21017;&#21270;&#65292;&#20854;&#25193;&#23637;&#38388;&#38548;&#27491;&#21017;&#21270;&#21644;&#26032;&#30340;soft-min-max&#27491;&#21017;&#21270;&#65292;&#20316;&#20026;&#20840;&#31934;&#24230;&#27169;&#22411;&#35757;&#32451;&#26399;&#38388;&#30340;&#27491;&#21017;&#21270;&#25439;&#22833;&#12290;&#32467;&#21512;&#26368;&#20808;&#36827;&#30340;&#37327;&#21270;&#21644;&#21387;&#32553;&#25216;&#26415;&#65292;&#21033;&#29992;R^2&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#24179;&#22343;&#27700;&#24179;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#23588;&#20854;&#26159;&#22312;&#36739;&#20302;&#20301;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model parameter regularization is a widely used technique to improve generalization, but also can be used to shape the weight distributions for various purposes. In this work, we shed light on how weight regularization can assist model quantization and compression techniques, and then propose range regularization (R^2) to further boost the quality of model optimization by focusing on the outlier prevention. By effectively regulating the minimum and maximum weight values from a distribution, we mold the overall distribution into a tight shape so that model compression and quantization techniques can better utilize their limited numeric representation powers. We introduce L-inf regularization, its extension margin regularization and a new soft-min-max regularization to be used as a regularization loss during full-precision model training. Coupled with state-of-the-art quantization and compression techniques, models trained with R^2 perform better on an average, specifically at lower bit 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; MSC-DBSCAN&#25193;&#23637;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#19977;&#20803;&#32858;&#31867;&#20013;&#20174;&#25968;&#25454;&#20013;&#25552;&#21462;&#19981;&#21516;&#23376;&#31354;&#38388;&#30340;&#19981;&#21516;&#20999;&#29255;&#32858;&#31867;&#65292;&#24182;&#21487;&#20197;&#33719;&#24471;&#19982; MSC &#31639;&#27861;&#22312;&#22788;&#29702;&#31209;&#19968;&#24352;&#37327;&#25968;&#25454;&#26102;&#30456;&#21516;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2303.07768</link><description>&lt;p&gt;
&#22810;&#32500;&#25968;&#32452;&#30340;&#22810;&#20999;&#29255;&#32858;&#31867;&#20013;&#30340;DBSCAN&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
DBSCAN of Multi-Slice Clustering for three-order Tensor. (arXiv:2303.07768v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07768
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; MSC-DBSCAN&#25193;&#23637;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#19977;&#20803;&#32858;&#31867;&#20013;&#20174;&#25968;&#25454;&#20013;&#25552;&#21462;&#19981;&#21516;&#23376;&#31354;&#38388;&#30340;&#19981;&#21516;&#20999;&#29255;&#32858;&#31867;&#65292;&#24182;&#21487;&#20197;&#33719;&#24471;&#19982; MSC &#31639;&#27861;&#22312;&#22788;&#29702;&#31209;&#19968;&#24352;&#37327;&#25968;&#25454;&#26102;&#30456;&#21516;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#19977;&#32500;&#25968;&#25454;&#30340;&#19977;&#20803;&#32858;&#31867;&#65292;&#29616;&#26377;&#30340;&#20960;&#31181;&#26041;&#27861;&#38656;&#35201;&#25351;&#23450;&#27599;&#20010;&#32500;&#24230;&#30340;&#32858;&#31867;&#22823;&#23567;&#25110;&#32858;&#31867;&#25968;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#19977;&#20803;&#32858;&#31867;(MSC)&#31639;&#27861;&#21487;&#20197;&#22312;&#20302;&#32500;&#23376;&#31354;&#38388;&#20013;&#25214;&#21040;&#20445;&#30041;&#20449;&#21495;&#30340;&#20999;&#29255;&#20197;&#20415;&#22522;&#20110;&#30456;&#20284;&#24230;&#38408;&#20540;&#25214;&#21040;&#32858;&#31867;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102; MSC-DBSCAN&#25193;&#23637;&#31639;&#27861;&#20197;&#20174;&#25968;&#25454;&#20013;&#25552;&#21462;&#20301;&#20110;&#19981;&#21516;&#23376;&#31354;&#38388;&#30340;&#19981;&#21516;&#20999;&#29255;&#32858;&#31867;(&#22914;&#26524;&#25968;&#25454;&#38598;&#26159;r&#20010;&#31209;&#19968;&#24352;&#37327;(r&gt;1)&#30340;&#24635;&#21644;)&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#20351;&#29992;&#21644; MSC &#31639;&#27861;&#30456;&#21516;&#30340;&#36755;&#20837;&#65292;&#21487;&#20197;&#22312;&#22788;&#29702;&#31209;&#19968;&#24352;&#37327;&#25968;&#25454;&#26102;&#19982; MSC &#31639;&#27861;&#33719;&#24471;&#30456;&#21516;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several methods for triclustering three-dimensional data require the cluster size or the number of clusters in each dimension to be specified. To address this issue, the Multi-Slice Clustering (MSC) for 3-order tensor finds signal slices that lie in a low dimensional subspace for a rank-one tensor dataset in order to find a cluster based on the threshold similarity. We propose an extension algorithm called MSC-DBSCAN to extract the different clusters of slices that lie in the different subspaces from the data if the dataset is a sum of r rank-one tensor (r &gt; 1). Our algorithm uses the same input as the MSC algorithm and can find the same solution for rank-one tensor data as MSC.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Self-Supervised&#23545;&#27604;&#32858;&#31867;&#31639;&#27861;&#30340;&#33258;&#21160;&#20998;&#21106;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#21033;&#29992;StyleGAN&#29983;&#25104;&#22120;&#20013;&#30340;&#22810;&#23610;&#24230;&#38544;&#34255;&#29305;&#24449;&#23545;&#22270;&#20687;&#36827;&#34892;&#24555;&#36895;&#20998;&#21106;&#65292;&#20248;&#20110;&#21322;&#30417;&#30563;&#22522;&#20934;&#26041;&#27861;&#30340;&#24179;&#22343;wIoU&#20540;1.02%&#65292;&#24182;&#25552;&#39640;&#20102;&#25512;&#29702;&#36895;&#24230;&#36798;4.5&#20493;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#19968;&#27425;&#23398;&#20064;&#26041;&#26696;&#65292;&#36866;&#29992;&#20110;&#20854;&#20182;GAN&#29983;&#25104;&#30340;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2303.05639</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;StyleGAN&#22270;&#20687;&#33258;&#21160;&#20998;&#21106;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised One-Shot Learning for Automatic Segmentation of StyleGAN Images. (arXiv:2303.05639v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05639
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Self-Supervised&#23545;&#27604;&#32858;&#31867;&#31639;&#27861;&#30340;&#33258;&#21160;&#20998;&#21106;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#21033;&#29992;StyleGAN&#29983;&#25104;&#22120;&#20013;&#30340;&#22810;&#23610;&#24230;&#38544;&#34255;&#29305;&#24449;&#23545;&#22270;&#20687;&#36827;&#34892;&#24555;&#36895;&#20998;&#21106;&#65292;&#20248;&#20110;&#21322;&#30417;&#30563;&#22522;&#20934;&#26041;&#27861;&#30340;&#24179;&#22343;wIoU&#20540;1.02%&#65292;&#24182;&#25552;&#39640;&#20102;&#25512;&#29702;&#36895;&#24230;&#36798;4.5&#20493;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#19968;&#27425;&#23398;&#20064;&#26041;&#26696;&#65292;&#36866;&#29992;&#20110;&#20854;&#20182;GAN&#29983;&#25104;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Self-Supervised&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#33258;&#21160;&#20998;&#21106;&#29983;&#25104;&#30340;StyleGAN&#22270;&#20687;&#12290;&#22312;GAN&#29983;&#25104;&#22120;&#30340;&#22810;&#23610;&#24230;&#38544;&#34255;&#29305;&#24449;&#20013;&#65292;&#21547;&#26377;&#26377;&#29992;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#21487;&#20197;&#21033;&#29992;&#36825;&#20123;&#29305;&#24449;&#23454;&#29616;&#23545;&#29983;&#25104;&#22270;&#20687;&#30340;&#33258;&#21160;&#20998;&#21106;&#12290;&#20351;&#29992;&#36825;&#20123;&#29305;&#24449;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#20351;&#29992;&#33258;&#30417;&#30563;&#23545;&#27604;&#32858;&#31867;&#31639;&#27861;&#26469;&#23398;&#20064;&#20998;&#21106;&#21512;&#25104;&#22270;&#20687;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#23558;&#38544;&#34255;&#29305;&#24449;&#25237;&#24433;&#21040;&#32039;&#20945;&#31354;&#38388;&#36827;&#34892;&#20687;&#32032;&#20998;&#31867;&#12290;&#36825;&#31181;&#26032;&#22411;&#30340;&#23545;&#27604;&#23398;&#20064;&#22120;&#26159;&#22522;&#20110;&#20351;&#29992;&#20687;&#32032;&#20132;&#25442;&#39044;&#27979;&#25439;&#22833;&#36827;&#34892;&#22270;&#20687;&#20998;&#21106;&#30340;&#65292;&#20174;&#32780;&#21487;&#20197;&#26356;&#24555;&#22320;&#23398;&#20064;&#29992;&#20110;&#19968;&#27425;&#20998;&#21106;&#30340;&#29305;&#24449;&#21521;&#37327;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#23454;&#29616;&#65292;&#24471;&#20986;&#30340;&#20998;&#21106;&#24615;&#33021;&#19981;&#20165;&#27604;&#21322;&#30417;&#30563;&#22522;&#20934;&#26041;&#27861;&#24179;&#22343;wIoU&#39640;&#20986;1.02&#65285;&#65292;&#32780;&#19988;&#25552;&#39640;&#20102;&#25512;&#29702;&#36895;&#24230;4.5&#20493;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#36890;&#29992;&#30340;&#19968;&#27425;&#23398;&#20064;&#26041;&#26696;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#20854;&#20182;GAN&#29983;&#25104;&#30340;&#22270;&#20687;&#65292;&#20174;&#32780;&#20351;&#25105;&#20204;&#30340;&#26694;&#26550;&#33021;&#22815;&#25512;&#24191;&#21040;&#24191;&#27867;&#30340;&#20854;&#20182;StyleGAN&#24212;&#29992;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a framework for the automatic one-shot segmentation of synthetic images generated by a StyleGAN. Our framework is based on the observation that the multi-scale hidden features in the GAN generator hold useful semantic information that can be utilized for automatic on-the-fly segmentation of the generated images. Using these features, our framework learns to segment synthetic images using a self-supervised contrastive clustering algorithm that projects the hidden features into a compact space for per-pixel classification. This novel contrastive learner is based on using a pixel-wise swapped prediction loss for image segmentation that leads to faster learning of the feature vectors for one-shot segmentation. We have tested our implementation on a number of standard benchmarks to yield a segmentation performance that not only outperforms the semi-supervised baseline methods by an average wIoU margin of 1.02% but also improves the inference speeds by a factor of 4.5. Finally, we
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#35745;&#31639;&#36830;&#32493;&#30340;&#24378;&#21270;&#23398;&#20064;&#30446;&#26631;&#26159;&#21487;PAC&#23398;&#20064;&#30340;&#65292;&#36825;&#23545;&#20110;&#35774;&#35745;&#39640;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#25171;&#24320;&#20102;&#26032;&#30340;&#31361;&#30772;&#21475;&#12290;</title><link>http://arxiv.org/abs/2303.05518</link><description>&lt;p&gt;
&#35745;&#31639;&#36830;&#32493;&#30340;&#24378;&#21270;&#23398;&#20064;&#30446;&#26631;&#26159;&#21487;PAC&#23398;&#20064;&#30340;
&lt;/p&gt;
&lt;p&gt;
Computably Continuous Reinforcement-Learning Objectives are PAC-learnable. (arXiv:2303.05518v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05518
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#35745;&#31639;&#36830;&#32493;&#30340;&#24378;&#21270;&#23398;&#20064;&#30446;&#26631;&#26159;&#21487;PAC&#23398;&#20064;&#30340;&#65292;&#36825;&#23545;&#20110;&#35774;&#35745;&#39640;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#25171;&#24320;&#20102;&#26032;&#30340;&#31361;&#30772;&#21475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#26368;&#22823;&#21270;&#25240;&#25187;&#21644;&#26377;&#38480;&#26102;&#38388;&#27493;&#38271;&#32047;&#31215;&#22870;&#21169;&#30340;&#32463;&#20856;&#30446;&#26631;&#26159;&#21487;PAC&#23398;&#20064;&#30340;&#65306;&#23384;&#22312;&#31639;&#27861;&#20351;&#29992;&#26377;&#38480;&#25968;&#37327;&#30340;&#26679;&#26412;&#21644;&#35745;&#31639;&#65292;&#39640;&#27010;&#29575;&#22320;&#23398;&#20064;&#21040;&#36817;&#20284;&#26368;&#20248;&#31574;&#30053;&#12290;&#36817;&#24180;&#26469;&#65292;&#30740;&#31350;&#20154;&#21592;&#24341;&#20837;&#20102;&#36229;&#36234;&#32463;&#20856;&#32047;&#31215;&#22870;&#21169;&#30340;&#26032;&#30446;&#26631;&#21644;&#30456;&#24212;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#22914;&#29992;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#20844;&#24335;&#25351;&#23450;&#30340;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26032;&#30446;&#26631;&#30340;PAC&#21487;&#23398;&#20064;&#24615;&#20173;&#28982;&#23384;&#22312;&#38382;&#39064;&#12290;&#26412;&#35770;&#25991;&#36890;&#36807;&#22312;&#20004;&#31181;&#20998;&#26512;&#35774;&#32622;&#20013;&#20805;&#20998;&#26465;&#20214;&#30340;PAC&#21487;&#23398;&#20064;&#24615;&#65292;&#35777;&#26126;&#20102;&#24378;&#21270;&#23398;&#20064;&#30446;&#26631;&#30340;&#21487;PAC&#23398;&#20064;&#24615;&#12290;&#29305;&#21035;&#22320;&#65292;&#22312;&#20165;&#32771;&#34385;&#26679;&#26412;&#22797;&#26434;&#24615;&#30340;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#22914;&#26524;&#19968;&#20010;&#30446;&#26631;&#20316;&#20026;&#19968;&#20010;oracle&#26159;&#19968;&#33268;&#36830;&#32493;&#30340;&#65292;&#37027;&#20040;&#23427;&#26159;&#21487;PAC&#23398;&#20064;&#30340;&#12290;&#27492;&#22806;&#65292;&#22312;&#32771;&#34385;&#35745;&#31639;&#22797;&#26434;&#24230;&#30340;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#22914;&#26524;&#19968;&#20010;&#30446;&#26631;&#26159;&#35745;&#31639;&#36830;&#32493;&#30340;&#65292;&#24182;&#28385;&#36275;&#8220;&#32479;&#19968;&#36830;&#32493;&#26465;&#20214;&#8221;&#65292;&#37027;&#20040;&#23427;&#26159;&#21487;PAC&#23398;&#20064;&#30340;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20026;&#36229;&#36234;&#32047;&#31215;&#22870;&#21169;&#30340;&#30446;&#26631;&#30340;&#21487;&#23398;&#20064;&#24615;&#25552;&#20379;&#20102;&#35265;&#35299;&#65292;&#24182;&#20026;&#35774;&#35745;&#36825;&#20123;&#30446;&#26631;&#30340;&#39640;&#25928;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
In reinforcement learning, the classic objectives of maximizing discounted and finite-horizon cumulative rewards are PAC-learnable: There are algorithms that learn a near-optimal policy with high probability using a finite amount of samples and computation. In recent years, researchers have introduced objectives and corresponding reinforcement-learning algorithms beyond the classic cumulative rewards, such as objectives specified as linear temporal logic formulas. However, questions about the PAC-learnability of these new objectives have remained open.  This work demonstrates the PAC-learnability of general reinforcement-learning objectives through sufficient conditions for PAC-learnability in two analysis settings. In particular, for the analysis that considers only sample complexity, we prove that if an objective given as an oracle is uniformly continuous, then it is PAC-learnable. Further, for the analysis that considers computational complexity, we prove that if an objective is com
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29616;&#32676;&#32452;&#26465;&#20214;&#26377;&#25928;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#38382;&#39064;&#35268;&#32422;&#20026;&#22810;&#32452;&#23398;&#20064;&#38382;&#39064;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#21463;&#38480;&#20110;&#29305;&#23450;&#20998;&#32452;&#32467;&#26500;&#25110;&#20998;&#24067;&#20551;&#35774;&#31561;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#21033;&#29992;&#19968;&#31181;&#26032;&#30340;&#22810;&#32452;&#23398;&#20064;&#31639;&#27861;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#26679;&#26412;&#22797;&#26434;&#24230;&#20445;&#35777;&#21644;&#31616;&#21270;&#39044;&#27979;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2303.03995</link><description>&lt;p&gt;
&#22810;&#32452;&#23398;&#20064;&#23454;&#29616;&#32676;&#32452;&#26465;&#20214;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Group conditional validity via multi-group learning. (arXiv:2303.03995v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03995
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29616;&#32676;&#32452;&#26465;&#20214;&#26377;&#25928;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#38382;&#39064;&#35268;&#32422;&#20026;&#22810;&#32452;&#23398;&#20064;&#38382;&#39064;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#21463;&#38480;&#20110;&#29305;&#23450;&#20998;&#32452;&#32467;&#26500;&#25110;&#20998;&#24067;&#20551;&#35774;&#31561;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#21033;&#29992;&#19968;&#31181;&#26032;&#30340;&#22810;&#32452;&#23398;&#20064;&#31639;&#27861;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#26679;&#26412;&#22797;&#26434;&#24230;&#20445;&#35777;&#21644;&#31616;&#21270;&#39044;&#27979;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#26080;&#20998;&#24067;&#38480;&#21046;&#32622;&#20449;&#24230;&#39044;&#27979;&#38382;&#39064;&#65292;&#30528;&#37325;&#35752;&#35770;&#32676;&#32452;&#26465;&#20214;&#26377;&#25928;&#24615;&#30340;&#20934;&#21017;&#12290;&#36825;&#19968;&#20934;&#21017;&#20855;&#26377;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#65292;&#20363;&#22914;&#38544;&#34255;&#20998;&#23618;&#21644;&#32676;&#32452;&#20844;&#24179;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#35201;&#20040;&#21463;&#38480;&#20110;&#29305;&#23450;&#30340;&#20998;&#32452;&#32467;&#26500;&#25110;&#20998;&#24067;&#20551;&#35774;&#65292;&#35201;&#20040;&#22312;&#24322;&#26041;&#24046;&#22122;&#22768;&#19979;&#36807;&#20110;&#20445;&#23432;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#20010;&#31616;&#21333;&#30340;&#35268;&#32422;&#65292;&#23558;&#23454;&#29616;&#32676;&#20307;&#26465;&#20214;&#26377;&#25928;&#24615;&#30340;&#38382;&#39064;&#36716;&#21270;&#20026;&#35299;&#20915;&#19968;&#31181;&#22810;&#32452;&#23398;&#20064;&#38382;&#39064;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#20511;&#21161;&#22810;&#32452;&#23398;&#20064;&#31639;&#27861;&#30340;&#29702;&#35770;&#20445;&#35777;&#26469;&#33719;&#24471;&#32622;&#20449;&#24230;&#39044;&#27979;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#20445;&#35777;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#38024;&#23545;&#20855;&#26377;&#20998;&#23618;&#32467;&#26500;&#30340;&#32676;&#20307;&#30340;&#22810;&#32452;&#23398;&#20064;&#31639;&#27861;&#12290;&#22312;&#25105;&#20204;&#30340;&#35268;&#32422;&#20013;&#20351;&#29992;&#27492;&#31639;&#27861;&#21487;&#24102;&#26469;&#25913;&#36827;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#20445;&#35777;&#21644;&#26356;&#31616;&#21333;&#30340;&#39044;&#27979;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of distribution-free conformal prediction and the criterion of group conditional validity. This criterion is motivated by many practical scenarios including hidden stratification and group fairness. Existing methods achieve such guarantees under either restrictive grouping structure or distributional assumptions, or they are overly-conservative under heteroskedastic noise. We propose a simple reduction to the problem of achieving validity guarantees for individual populations by leveraging algorithms for a problem called multi-group learning. This allows us to port theoretical guarantees from multi-group learning to obtain obtain sample complexity guarantees for conformal prediction. We also provide a new algorithm for multi-group learning for groups with hierarchical structure. Using this algorithm in our reduction leads to improved sample complexity guarantees with a simpler predictor structure.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#24191;&#20041;&#26694;&#26550;&#65292;&#23558;FRL&#30340;&#27602;&#21270;&#25915;&#20987;&#35270;&#20026;&#38480;&#21046;&#39044;&#31639;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#21487;&#20197;&#24212;&#29992;&#20110;&#22522;&#20110;&#31574;&#30053;&#30340;FRL&#30340;&#27602;&#21270;&#25915;&#20987;&#21327;&#35758;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#23545;&#31169;&#20154;&#35780;&#20215;&#32773;&#21644;&#20844;&#20849;&#35780;&#20215;&#32773;&#23558;&#20854;&#25193;&#23637;&#21040;&#20351;&#29992;&#28436;&#21592;-&#35780;&#35770;&#23478;&#20316;&#20026;&#26412;&#22320;RL&#31639;&#27861;&#30340;FRL&#12290;</title><link>http://arxiv.org/abs/2303.02725</link><description>&lt;p&gt;
&#32852;&#37030;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26412;&#22320;&#29615;&#22659;&#27602;&#21270;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Local Environment Poisoning Attacks on Federated Reinforcement Learning. (arXiv:2303.02725v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02725
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#24191;&#20041;&#26694;&#26550;&#65292;&#23558;FRL&#30340;&#27602;&#21270;&#25915;&#20987;&#35270;&#20026;&#38480;&#21046;&#39044;&#31639;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#21487;&#20197;&#24212;&#29992;&#20110;&#22522;&#20110;&#31574;&#30053;&#30340;FRL&#30340;&#27602;&#21270;&#25915;&#20987;&#21327;&#35758;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#23545;&#31169;&#20154;&#35780;&#20215;&#32773;&#21644;&#20844;&#20849;&#35780;&#20215;&#32773;&#23558;&#20854;&#25193;&#23637;&#21040;&#20351;&#29992;&#28436;&#21592;-&#35780;&#35770;&#23478;&#20316;&#20026;&#26412;&#22320;RL&#31639;&#27861;&#30340;FRL&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#24050;&#25104;&#20026;&#35299;&#20915;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#30340;&#28909;&#38376;&#24037;&#20855;&#12290;&#22810;&#20195;&#29702;&#32467;&#26500;&#35299;&#20915;&#20102;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#20013;&#25968;&#25454;&#38656;&#27714;&#22823;&#30340;&#20027;&#35201;&#38382;&#39064;&#65292;&#32780;&#32852;&#37030;&#26426;&#21046;&#20445;&#25252;&#20102;&#21508;&#20010;&#20195;&#29702;&#20010;&#20307;&#30340;&#25968;&#25454;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#32852;&#37030;&#26426;&#21046;&#20063;&#20250;&#26292;&#38706;&#31995;&#32479;&#38754;&#20020;&#24694;&#24847;&#20195;&#29702;&#30340;&#27602;&#21270;&#25915;&#20987;&#12290;&#26412;&#24037;&#20316;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#24191;&#20041;&#26694;&#26550;&#65292;&#23558;FRL&#30340;&#27602;&#21270;&#25915;&#20987;&#35270;&#20026;&#38480;&#21046;&#39044;&#31639;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#21487;&#20197;&#24212;&#29992;&#20110;&#22522;&#20110;&#31574;&#30053;&#30340;FRL&#30340;&#27602;&#21270;&#25915;&#20987;&#21327;&#35758;&#65292;&#24182;&#36890;&#36807;&#35757;&#32451;&#19968;&#23545;&#31169;&#20154;&#35780;&#20215;&#32773;&#21644;&#20844;&#20849;&#35780;&#20215;&#32773;&#23558;&#20854;&#25193;&#23637;&#21040;&#20351;&#29992;&#28436;&#21592;-&#35780;&#35770;&#23478;&#20316;&#20026;&#26412;&#22320;RL&#31639;&#27861;&#30340;FRL&#12290;&#25105;&#20204;&#20063;&#35752;&#35770;&#20102;&#20174;FL&#32487;&#25215;&#30340;&#19968;&#31181;&#20256;&#32479;&#38450;&#24481;&#31574;&#30053;&#20197;&#20943;&#36731;&#36825;&#31181;&#39118;&#38505;&#12290;&#25105;&#20204;&#36890;&#36807;&#36827;&#34892;&#23454;&#39564;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#27602;&#21270;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) has become a popular tool for solving traditional Reinforcement Learning (RL) tasks. The multi-agent structure addresses the major concern of data-hungry in traditional RL, while the federated mechanism protects the data privacy of individual agents. However, the federated mechanism also exposes the system to poisoning by malicious agents that can mislead the trained policy. Despite the advantage brought by FL, the vulnerability of Federated Reinforcement Learning (FRL) has not been well-studied before. In this work, we propose the first general framework to characterize FRL poisoning as an optimization problem constrained by a limited budget and design a poisoning protocol that can be applied to policy-based FRL and extended to FRL with actor-critic as a local RL algorithm by training a pair of private and public critics. We also discuss a conventional defense strategy inherited from FL to mitigate this risk. We verify our poisoning effectiveness by conducting 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36229;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#30340;&#24207;&#21015;&#20998;&#31867;&#27169;&#22411;Seq-HyGAN&#65292;&#36890;&#36807;&#21019;&#24314;&#36229;&#22270;&#21644;&#24341;&#20837;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#22788;&#29702;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#22797;&#26434;&#32467;&#26500;&#30456;&#20284;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.02393</link><description>&lt;p&gt;
Seq-HyGAN: &#22522;&#20110;&#36229;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#30340;&#24207;&#21015;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Seq-HyGAN: Sequence Classification via Hypergraph Attention Network. (arXiv:2303.02393v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02393
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36229;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#30340;&#24207;&#21015;&#20998;&#31867;&#27169;&#22411;Seq-HyGAN&#65292;&#36890;&#36807;&#21019;&#24314;&#36229;&#22270;&#21644;&#24341;&#20837;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#22788;&#29702;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#22797;&#26434;&#32467;&#26500;&#30456;&#20284;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24207;&#21015;&#20998;&#31867;&#22312;&#19981;&#21516;&#39046;&#22495;&#26377;&#24191;&#27867;&#30340;&#23454;&#38469;&#24212;&#29992;&#65292;&#20363;&#22914;&#22312;&#20581;&#24247;&#39046;&#22495;&#20013;&#30340;&#22522;&#22240;&#32452;&#20998;&#31867;&#21644;&#22312;&#21830;&#19994;&#39046;&#22495;&#30340;&#24322;&#24120;&#26816;&#27979;&#12290;&#28982;&#32780;&#65292;&#24207;&#21015;&#25968;&#25454;&#20013;&#32570;&#20047;&#26174;&#24335;&#30340;&#29305;&#24449;&#65292;&#36825;&#20351;&#24471;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#38590;&#20197;&#22788;&#29702;&#12290;&#34429;&#28982;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36890;&#36807;&#33258;&#21160;&#23398;&#20064;&#29305;&#24449;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#20165;&#38480;&#20110;&#25429;&#33719;&#30456;&#37051;&#32467;&#26500;&#36830;&#25509;&#24182;&#24573;&#30053;&#24207;&#21015;&#20043;&#38388;&#30340;&#20840;&#23616;&#12289;&#39640;&#38454;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#24207;&#21015;&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36229;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#27169;&#22411;&#8212;&#8212;Seq-HyGAN&#12290;&#20026;&#20102;&#25429;&#25417;&#24207;&#21015;&#25968;&#25454;&#20043;&#38388;&#30340;&#22797;&#26434;&#32467;&#26500;&#30456;&#20284;&#24615;&#65292;&#25105;&#20204;&#39318;&#20808;&#21019;&#24314;&#19968;&#20010;&#36229;&#22270;&#65292;&#20854;&#20013;&#24207;&#21015;&#34987;&#25551;&#32472;&#20026;&#36229;&#36793;&#65292;&#20174;&#24207;&#21015;&#20013;&#25552;&#21462;&#30340;&#23376;&#24207;&#21015;&#34987;&#25551;&#32472;&#20026;&#33410;&#28857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#23427;&#21033;&#29992;&#20102;&#21452;&#23618;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;&#35813;&#27169;&#22411;&#29983;&#25104;&#19968;&#20010;&#24207;&#21015;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Sequence classification has a wide range of real-world applications in different domains, such as genome classification in health and anomaly detection in business. However, the lack of explicit features in sequence data makes it difficult for machine learning models. While Neural Network (NN) models address this with learning features automatically, they are limited to capturing adjacent structural connections and ignore global, higher-order information between the sequences. To address these challenges in the sequence classification problems, we propose a novel Hypergraph Attention Network model, namely Seq-HyGAN. To capture the complex structural similarity between sequence data, we first create a hypergraph where the sequences are depicted as hyperedges and subsequences extracted from sequences are depicted as nodes. Additionally, we introduce an attention-based Hypergraph Neural Network model that utilizes a two-level attention mechanism. This model generates a sequence representa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26089;&#26399;&#36864;&#20986;&#30340;&#20998;&#23618;&#35757;&#32451;&#26041;&#27861;&#65292;&#23558;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#20026;&#36793;&#32536;&#21644;&#20113;&#24037;&#20316;&#32773;&#65292;&#20197;&#20943;&#23569;&#36890;&#20449;&#25104;&#26412;&#12289;&#35757;&#32451;&#36816;&#34892;&#26102;&#38388;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.02384</link><description>&lt;p&gt;
&#21033;&#29992;&#26089;&#26399;&#36864;&#20986;&#36827;&#34892;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#23618;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Training of Deep Neural Networks Using Early Exiting. (arXiv:2303.02384v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02384
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26089;&#26399;&#36864;&#20986;&#30340;&#20998;&#23618;&#35757;&#32451;&#26041;&#27861;&#65292;&#23558;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#20026;&#36793;&#32536;&#21644;&#20113;&#24037;&#20316;&#32773;&#65292;&#20197;&#20943;&#23569;&#36890;&#20449;&#25104;&#26412;&#12289;&#35757;&#32451;&#36816;&#34892;&#26102;&#38388;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#20102;&#35270;&#35273;&#20219;&#21153;&#30340;&#26368;&#20808;&#36827;&#20934;&#30830;&#24615;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#36164;&#28304;&#36827;&#34892;&#35757;&#32451;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#22312;&#36828;&#31163;&#33719;&#21462;&#25968;&#25454;&#30340;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#20113;&#26381;&#21153;&#22120;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#36825;&#22686;&#21152;&#20102;&#36890;&#20449;&#25104;&#26412;&#12289;&#36816;&#34892;&#26102;&#38388;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#23618;&#35757;&#32451;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#26089;&#26399;&#36864;&#20986;&#22312;&#36793;&#32536;&#21644;&#20113;&#24037;&#20316;&#32773;&#20043;&#38388;&#20998;&#21106;&#26550;&#26500;&#65292;&#20197;&#20943;&#23569;&#36890;&#20449;&#25104;&#26412;&#12289;&#35757;&#32451;&#36816;&#34892;&#26102;&#38388;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks provide state-of-the-art accuracy for vision tasks but they require significant resources for training. Thus, they are trained on cloud servers far from the edge devices that acquire the data. This issue increases communication cost, runtime and privacy concerns. In this study, a novel hierarchical training method for deep neural networks is proposed that uses early exits in a divided architecture between edge and cloud workers to reduce the communication cost, training runtime and privacy concerns. The method proposes a brand-new use case for early exits to separate the backward pass of neural networks between the edge and the cloud during the training phase. We address the issues of most available methods that due to the sequential nature of the training phase, cannot train the levels of hierarchy simultaneously or they do it with the cost of compromising privacy. In contrast, our method can use both edge and cloud workers simultaneously, does not share the raw i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#33258;&#21160;&#35782;&#21035;&#21644;&#21457;&#29616;&#31185;&#23398;&#25991;&#29486;&#20013;&#25968;&#23398;&#27010;&#24565;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#20844;&#24335;&#27010;&#24565;&#21457;&#29616;&#21644;&#20844;&#24335;&#27010;&#24565;&#35782;&#21035;&#20004;&#20010;&#23376;&#20219;&#21153;&#65292;&#36890;&#36807;&#23545;arXiv&#23376;&#38598;&#20013;1.8M&#20010;&#20844;&#24335;&#20986;&#29616;&#36827;&#34892;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#27861;&#20248;&#20110;&#24378;&#22522;&#32447;&#12290;</title><link>http://arxiv.org/abs/2303.01994</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21457;&#29616;&#21644;&#35782;&#21035;&#25968;&#23398;&#20844;&#24335;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
Discovery and Recognition of Formula Concepts using Machine Learning. (arXiv:2303.01994v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01994
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#33258;&#21160;&#35782;&#21035;&#21644;&#21457;&#29616;&#31185;&#23398;&#25991;&#29486;&#20013;&#25968;&#23398;&#27010;&#24565;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#20844;&#24335;&#27010;&#24565;&#21457;&#29616;&#21644;&#20844;&#24335;&#27010;&#24565;&#35782;&#21035;&#20004;&#20010;&#23376;&#20219;&#21153;&#65292;&#36890;&#36807;&#23545;arXiv&#23376;&#38598;&#20013;1.8M&#20010;&#20844;&#24335;&#20986;&#29616;&#36827;&#34892;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#27861;&#20248;&#20110;&#24378;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#24341;&#29992;&#30340;&#20449;&#24687;&#26816;&#32034;&#26041;&#27861;&#24050;&#32463;&#34987;&#35777;&#26126;&#21487;&#20197;&#22312;&#38656;&#35201;&#24341;&#29992;&#22823;&#37327;&#25991;&#29486;&#30340;&#23398;&#26415;&#23398;&#31185;&#20013;&#26377;&#25928;&#22320;&#36827;&#34892;&#20316;&#24330;&#26816;&#27979;&#25110;&#25991;&#29486;&#25512;&#33616;&#31995;&#32479;&#31561;&#20449;&#24687;&#26816;&#32034;&#24212;&#29992;&#12290;&#22312;&#31185;&#23398;&#12289;&#25216;&#26415;&#12289;&#24037;&#31243;&#21644;&#25968;&#23398;&#39046;&#22495;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#24120;&#36890;&#36807;&#20844;&#24335;&#31526;&#21495;&#26469;&#24341;&#29992;&#20808;&#21069;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#30340;&#38271;&#26399;&#30446;&#26631;&#26159;&#23558;&#22522;&#20110;&#24341;&#29992;&#30340;&#20449;&#24687;&#26816;&#32034;&#26041;&#27861;&#25512;&#24191;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#32463;&#20856;&#21442;&#32771;&#25991;&#29486;&#21644;&#25968;&#23398;&#27010;&#24565;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22914;&#20309;&#24341;&#29992;&#25968;&#23398;&#20844;&#24335;&#65292;&#24182;&#23450;&#20041;&#20102;&#19968;&#20010;&#8220;&#20844;&#24335;&#27010;&#24565;&#26816;&#32034;&#20219;&#21153;&#8221;&#65292;&#20854;&#20013;&#21253;&#25324;&#20844;&#24335;&#27010;&#24565;&#21457;&#29616;&#65288;Formula Concept Discovery&#65292;FCD&#65289;&#21644;&#20844;&#24335;&#27010;&#24565;&#35782;&#21035;&#65288;Formula Concept Recognition&#65292;FCR&#65289;&#20004;&#20010;&#23376;&#20219;&#21153;&#12290;FCD&#26088;&#22312;&#23450;&#20041;&#21644;&#25506;&#32034;&#21629;&#21517;&#20026;&#32465;&#23450;&#31561;&#25928;&#34920;&#31034;&#30340;&#8220;&#20844;&#24335;&#27010;&#24565;&#8221;&#65292;&#32780;FCR&#26088;&#22312;&#23558;&#32473;&#23450;&#30340;&#20844;&#24335;&#21305;&#37197;&#21040;&#20808;&#21069;&#20998;&#37197;&#30340;&#21807;&#19968;&#25968;&#23398;&#27010;&#24565;&#26631;&#35782;&#31526;&#19978;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#20219;&#21153;&#65292;&#24182;&#22312;&#19968;&#20010;arXiv&#23376;&#38598;&#30340;1.8M&#20010;&#20844;&#24335;&#20986;&#29616;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#24378;&#22522;&#32447;&#65292;&#24182;&#20026;&#31185;&#23398;&#25991;&#29486;&#20013;&#30340;&#25968;&#23398;&#27010;&#24565;&#30340;&#33258;&#21160;&#35782;&#21035;&#21644;&#21457;&#29616;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
Citation-based Information Retrieval (IR) methods for scientific documents have proven effective for IR applications, such as Plagiarism Detection or Literature Recommender Systems in academic disciplines that use many references. In science, technology, engineering, and mathematics, researchers often employ mathematical concepts through formula notation to refer to prior knowledge. Our long-term goal is to generalize citation-based IR methods and apply this generalized method to both classical references and mathematical concepts. In this paper, we suggest how mathematical formulas could be cited and define a Formula Concept Retrieval task with two subtasks: Formula Concept Discovery (FCD) and Formula Concept Recognition (FCR). While FCD aims at the definition and exploration of a 'Formula Concept' that names bundled equivalent representations of a formula, FCR is designed to match a given formula to a prior assigned unique mathematical concept identifier. We present machine learning-
&lt;/p&gt;</description></item><item><title>&#26412;&#27604;&#36187;&#25552;&#20379;&#20004;&#20010;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#35270;&#39057;&#21644;&#22768;&#38899;&#25968;&#25454;&#65292;&#38024;&#23545;&#34920;&#24773;&#24773;&#24863;&#20272;&#35745;&#12289;&#34920;&#24773;&#35782;&#21035;&#12289;&#32908;&#32905;&#21160;&#20316;&#26816;&#27979;&#21644;&#24773;&#24863;&#21453;&#24212;&#24378;&#24230;&#20272;&#35745;&#22235;&#20010;&#20219;&#21153;&#36827;&#34892;&#25361;&#25112;&#12290;&#21442;&#36187;&#32773;&#38656;&#35201;&#25552;&#20379;&#33021;&#22312;&#22797;&#26434;&#12289;&#19981;&#21463;&#25511;&#21046;&#30340;&#29615;&#22659;&#19979;&#36816;&#34892;&#30340;&#26377;&#25928;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.01498</link><description>&lt;p&gt;
ABAW: &#34920;&#24773;&#24773;&#24863;&#20272;&#35745;&#12289;&#34920;&#24773;&#35782;&#21035;&#12289;&#32908;&#32905;&#21160;&#20316;&#26816;&#27979;&#21644;&#24773;&#24863;&#21453;&#24212;&#24378;&#24230;&#20272;&#35745;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
ABAW: Valence-Arousal Estimation, Expression Recognition, Action Unit Detection &amp; Emotional Reaction Intensity Estimation Challenges. (arXiv:2303.01498v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01498
&lt;/p&gt;
&lt;p&gt;
&#26412;&#27604;&#36187;&#25552;&#20379;&#20004;&#20010;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#35270;&#39057;&#21644;&#22768;&#38899;&#25968;&#25454;&#65292;&#38024;&#23545;&#34920;&#24773;&#24773;&#24863;&#20272;&#35745;&#12289;&#34920;&#24773;&#35782;&#21035;&#12289;&#32908;&#32905;&#21160;&#20316;&#26816;&#27979;&#21644;&#24773;&#24863;&#21453;&#24212;&#24378;&#24230;&#20272;&#35745;&#22235;&#20010;&#20219;&#21153;&#36827;&#34892;&#25361;&#25112;&#12290;&#21442;&#36187;&#32773;&#38656;&#35201;&#25552;&#20379;&#33021;&#22312;&#22797;&#26434;&#12289;&#19981;&#21463;&#25511;&#21046;&#30340;&#29615;&#22659;&#19979;&#36816;&#34892;&#30340;&#26377;&#25928;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31532;&#20116;&#23626;&#24773;&#24863;&#34892;&#20026;&#20998;&#26512;(ABAW)&#27604;&#36187;&#26159;&#35813;&#39046;&#22495;&#30456;&#24212;&#30340;ABAW&#30740;&#35752;&#20250;&#30340;&#19968;&#37096;&#20998;&#65292;&#35813;&#30740;&#35752;&#20250;&#23558;&#19982;IEEE&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#27169;&#24335;&#35782;&#21035;&#20250;&#35758;(CVPR)2023&#24180;&#32852;&#21512;&#20030;&#21150;&#12290;ABAW 5&#27604;&#36187;&#26159;ECCV 2022&#12289;IEEE CVPR 2022&#12289;ICCV 2021&#12289;IEEE FG 2020&#21644;CVPR 2017&#20250;&#35758;&#19978;&#20030;&#21150;&#30340;&#27604;&#36187;&#30340;&#24310;&#32493;&#65292;&#26088;&#22312;&#33258;&#21160;&#20998;&#26512;&#24773;&#24863;&#12290;&#20026;&#20170;&#24180;&#30340;&#27604;&#36187;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20004;&#20010;&#25968;&#25454;&#38598;&#65306;i) Aff-Wild2&#25968;&#25454;&#24211;&#30340;&#25193;&#23637;&#29256;&#26412;&#21644;ii) Hume-Reaction&#25968;&#25454;&#24211;&#12290;&#21069;&#32773;&#26159;&#19968;&#20010;&#21253;&#21547;&#32422;600&#20010;&#35270;&#39057;&#12289;&#32422;300&#19975;&#24103;&#30340;&#35270;&#21548;&#25968;&#25454;&#38598;&#65292;&#38024;&#23545;&#20197;&#19979;&#26041;&#38754;&#36827;&#34892;&#20102;&#27880;&#37322;&#65306;a) &#20004;&#20010;&#36830;&#32493;&#30340;&#24773;&#24863;&#32500;&#24230;&#8212;&#8212;&#24773;&#24863;&#20215;&#20540;(&#19968;&#20010;&#20154;&#30340;&#31215;&#26497;/&#28040;&#26497;&#31243;&#24230;)&#21644;&#24773;&#24863;&#21796;&#37266;&#24230;(&#19968;&#20010;&#20154;&#30340;&#27963;&#36291;/&#34987;&#21160;&#31243;&#24230;)&#8212;&#8212;&#65307;b) &#22522;&#26412;&#34920;&#24773;(&#20363;&#22914;&#24555;&#20048;&#12289;&#24754;&#20260;&#12289;&#20013;&#24615;&#29366;&#24577;)&#65307;&#21644;c) &#21407;&#23376;&#20154;&#33080;&#32908;&#32905;&#21160;&#20316;(&#21363;&#65292;&#21160;&#20316;&#21333;&#20803;)&#12290;&#21518;&#32773;&#25968;&#25454;&#38598;&#25429;&#25417;&#20102;&#20010;&#20307;&#23545;&#33258;&#28982;&#29615;&#22659;&#20013;&#20107;&#20214;&#30340;&#21453;&#24212;&#12290;ABAW&#27604;&#36187;&#25552;&#20986;&#20102;&#19977;&#20010;&#23376;&#20219;&#21153;&#65306;&#24773;&#24863;&#20215;&#20540;-&#21796;&#37266;&#24230;&#20272;&#35745;&#12289;&#34920;&#24773;&#35782;&#21035;&#21644;&#21160;&#20316;&#21333;&#20803;&#26816;&#27979;&#65292;&#20197;&#21450;&#19968;&#20010;&#26032;&#30340;&#23376;&#20219;&#21153;&#65292;&#24773;&#24863;&#21453;&#24212;&#24378;&#24230;&#20272;&#35745;&#12290;&#21442;&#36187;&#22242;&#38431;&#38656;&#35201;&#24320;&#21457;&#21644;&#35780;&#20272;&#33021;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#36739;&#23569;&#25511;&#21046;&#30340;&#29615;&#22659;&#20013;&#36816;&#34892;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The fifth Affective Behavior Analysis in-the-wild (ABAW) Competition is part of the respective ABAW Workshop which will be held in conjunction with IEEE Computer Vision and Pattern Recognition Conference (CVPR), 2023. The 5th ABAW Competition is a continuation of the Competitions held at ECCV 2022, IEEE CVPR 2022, ICCV 2021, IEEE FG 2020 and CVPR 2017 Conferences, and is dedicated at automatically analyzing affect. For this year's Competition, we feature two corpora: i) an extended version of the Aff-Wild2 database and ii) the Hume-Reaction dataset. The former database is an audiovisual one of around 600 videos of around 3M frames and is annotated with respect to:a) two continuous affect dimensions -valence (how positive/negative a person is) and arousal (how active/passive a person is)-; b) basic expressions (e.g. happiness, sadness, neutral state); and c) atomic facial muscle actions (i.e., action units). The latter dataset is an audiovisual one in which reactions of individuals to e
&lt;/p&gt;</description></item><item><title>AugGPT&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;ChatGPT&#30340;&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26356;&#24544;&#23454;&#22320;&#20445;&#30041;&#27491;&#30830;&#26631;&#35760;&#30340;&#29983;&#25104;&#25968;&#25454;&#24182;&#25552;&#20379;&#36275;&#22815;&#30340;&#22810;&#26679;&#24615;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#32531;&#35299;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#38480;&#21046;&#26679;&#26412;&#37327;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.13007</link><description>&lt;p&gt;
AugGPT&#65306;&#21033;&#29992;ChatGPT&#36827;&#34892;&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
AugGPT: Leveraging ChatGPT for Text Data Augmentation. (arXiv:2302.13007v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13007
&lt;/p&gt;
&lt;p&gt;
AugGPT&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;ChatGPT&#30340;&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26356;&#24544;&#23454;&#22320;&#20445;&#30041;&#27491;&#30830;&#26631;&#35760;&#30340;&#29983;&#25104;&#25968;&#25454;&#24182;&#25552;&#20379;&#36275;&#22815;&#30340;&#22810;&#26679;&#24615;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#32531;&#35299;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#38480;&#21046;&#26679;&#26412;&#37327;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;&#26159;&#21463;&#38480;&#21046;&#30340;&#26679;&#26412;&#37327;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#20811;&#26381;&#25361;&#25112;&#30340;&#26377;&#25928;&#31574;&#30053;&#12290;&#23545;&#20110;&#23569;&#26679;&#26412;&#23398;&#20064;&#22330;&#26223;&#65292;&#20854;&#20013;&#30446;&#26631;&#22495;&#30340;&#25968;&#25454;&#36890;&#24120;&#26356;&#21152;&#31232;&#32570;&#19988;&#36136;&#37327;&#26356;&#20302;&#65292;&#36825;&#19968;&#25361;&#25112;&#29305;&#21035;&#31361;&#20986;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#25429;&#25417;&#25968;&#25454;&#19981;&#21464;&#24615;&#24182;&#22686;&#21152;&#26679;&#26412;&#37327;&#65292;&#32531;&#35299;&#36825;&#31181;&#25361;&#25112;&#30340;&#19968;&#31181;&#33258;&#28982;&#19988;&#24191;&#27867;&#20351;&#29992;&#30340;&#31574;&#30053;&#26159;&#25191;&#34892;&#25968;&#25454;&#22686;&#24378;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#35201;&#20040;&#26080;&#27861;&#20445;&#35777;&#29983;&#25104;&#25968;&#25454;&#30340;&#27491;&#30830;&#26631;&#35760;&#65288;&#32570;&#20047;&#24544;&#23454;&#24230;&#65289;&#65292;&#35201;&#20040;&#26080;&#27861;&#20445;&#35777;&#29983;&#25104;&#30340;&#25968;&#25454;&#20855;&#26377;&#36275;&#22815;&#30340;&#22810;&#26679;&#24615;&#65288;&#32570;&#20047;&#32039;&#20945;&#24615;&#65289;&#65292;&#25110;&#32773;&#20004;&#32773;&#37117;&#26377;&#12290;&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#36817;&#25104;&#21151;&#65292;&#23588;&#20854;&#26159;&#22312;&#24320;&#21457;ChatGPT&#26041;&#38754;&#23637;&#31034;&#20102;&#25913;&#36827;&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#30340;&#22522;&#30784;&#19978;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;ChatGPT&#65288;&#21517;&#20026;AugGPT&#65289;&#30340;&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text data augmentation is an effective strategy for overcoming the challenge of limited sample sizes in many natural language processing (NLP) tasks. This challenge is especially prominent in the few-shot learning scenario, where the data in the target domain is generally much scarcer and of lowered quality. A natural and widely-used strategy to mitigate such challenges is to perform data augmentation to better capture the data invariance and increase the sample size. However, current text data augmentation methods either can't ensure the correct labeling of the generated data (lacking faithfulness) or can't ensure sufficient diversity in the generated data (lacking compactness), or both. Inspired by the recent success of large language models, especially the development of ChatGPT, which demonstrated improved language comprehension abilities, in this work, we propose a text data augmentation approach based on ChatGPT (named AugGPT). AugGPT rephrases each sentence in the training sampl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26174;&#33879;&#24615;&#25351;&#23548;&#30340;&#31070;&#32463;&#29305;&#24449;&#21387;&#32553;&#19982;&#27973;&#23618;&#21464;&#20998;&#29942;&#39048;&#27880;&#20837;&#30340;&#26032;&#30340;&#36164;&#28304;&#24847;&#35782;&#21387;&#32553;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#27604;&#26368;&#20808;&#36827;&#30340;SC&#26041;&#27861;&#20302;60&#65285;&#30340;&#27604;&#29305;&#29575;&#65292;&#24182;&#19988;&#27604;&#29616;&#26377;&#30340;&#32534;&#35299;&#30721;&#26631;&#20934;&#30340;&#19979;&#25918;&#24555;16&#20493;&#12290;</title><link>http://arxiv.org/abs/2302.10681</link><description>&lt;p&gt;
FrankenSplit:&#22522;&#20110;&#26174;&#33879;&#24615;&#25351;&#23548;&#30340;&#31070;&#32463;&#29305;&#24449;&#21387;&#32553;&#19982;&#27973;&#23618;&#21464;&#20998;&#29942;&#39048;&#27880;&#20837;
&lt;/p&gt;
&lt;p&gt;
FrankenSplit: Saliency Guided Neural Feature Compression with Shallow Variational Bottleneck Injection. (arXiv:2302.10681v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10681
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26174;&#33879;&#24615;&#25351;&#23548;&#30340;&#31070;&#32463;&#29305;&#24449;&#21387;&#32553;&#19982;&#27973;&#23618;&#21464;&#20998;&#29942;&#39048;&#27880;&#20837;&#30340;&#26032;&#30340;&#36164;&#28304;&#24847;&#35782;&#21387;&#32553;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#27604;&#26368;&#20808;&#36827;&#30340;SC&#26041;&#27861;&#20302;60&#65285;&#30340;&#27604;&#29305;&#29575;&#65292;&#24182;&#19988;&#27604;&#29616;&#26377;&#30340;&#32534;&#35299;&#30721;&#26631;&#20934;&#30340;&#19979;&#25918;&#24555;16&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;AI&#21152;&#36895;&#22120;&#30340;&#23835;&#36215;&#20351;&#24471;&#23545;&#24310;&#36831;&#25935;&#24863;&#30340;&#24212;&#29992;&#21487;&#20197;&#22312;&#23458;&#25143;&#31471;&#19978;&#25191;&#34892;&#36731;&#37327;&#32423;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#12290;&#28982;&#32780;&#65292;&#38656;&#35201;&#24378;&#22823;&#27169;&#22411;&#30340;&#20851;&#38190;&#24212;&#29992;&#31243;&#24207;&#38656;&#35201;&#23558;&#35831;&#27714;&#19979;&#25918;&#65292;&#32780;&#39640;&#32500;&#25968;&#25454;&#23558;&#20105;&#22842;&#26377;&#38480;&#30340;&#24102;&#23485;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36164;&#28304;&#24847;&#35782;&#21387;&#32553;&#27169;&#22411;&#30340;&#26694;&#26550;&#24182;&#22312;&#21453;&#26144;&#36793;&#32536;&#35774;&#22791;&#21644;&#26381;&#21153;&#22120;&#20043;&#38388;&#19981;&#23545;&#31216;&#36164;&#28304;&#20998;&#37197;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19981;&#38477;&#20302;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#27604;&#26368;&#20808;&#36827;&#30340;SC&#26041;&#27861;&#20302;60&#65285;&#30340;&#27604;&#29305;&#29575;&#65292;&#24182;&#19988;&#27604;&#29616;&#26377;&#30340;&#32534;&#35299;&#30721;&#26631;&#20934;&#30340;&#19979;&#25918;&#24555;16&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rise of mobile AI accelerators allows latency-sensitive applications to execute lightweight Deep Neural Networks (DNNs) on the client side. However, critical applications require powerful models that edge devices cannot host and must therefore offload requests, where the high-dimensional data will compete for limited bandwidth. This work proposes shifting away from focusing on executing shallow layers of partitioned DNNs. Instead, it advocates concentrating the local resources on variational compression optimized for machine interpretability. We introduce a novel framework for resource-conscious compression models and extensively evaluate our method in an environment reflecting the asymmetric resource distribution between edge devices and servers. Our method achieves 60\% lower bitrate than a state-of-the-art SC method without decreasing accuracy and is up to 16x faster than offloading with existing codec standards.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;SurvLIMEpy&#30340;Python&#21253;&#65292;&#23427;&#23454;&#29616;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#35745;&#31639;&#36866;&#29992;&#20110;&#24314;&#27169;&#29983;&#23384;&#20998;&#26512;&#25968;&#25454;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#23616;&#37096;&#29305;&#24449;&#37325;&#35201;&#24615;&#65292;&#24182;&#25903;&#25345;&#21508;&#31181;&#29983;&#23384;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2302.10571</link><description>&lt;p&gt;
SurvLIMEpy: &#19968;&#20010;&#23454;&#29616;&#20102;SurvLIME&#31639;&#27861;&#30340;Python&#21253;
&lt;/p&gt;
&lt;p&gt;
SurvLIMEpy: A Python package implementing SurvLIME. (arXiv:2302.10571v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10571
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;SurvLIMEpy&#30340;Python&#21253;&#65292;&#23427;&#23454;&#29616;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#35745;&#31639;&#36866;&#29992;&#20110;&#24314;&#27169;&#29983;&#23384;&#20998;&#26512;&#25968;&#25454;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#23616;&#37096;&#29305;&#24449;&#37325;&#35201;&#24615;&#65292;&#24182;&#25903;&#25345;&#21508;&#31181;&#29983;&#23384;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;SurvLIMEpy&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;Python&#21253;&#65292;&#23454;&#29616;&#20102;SurvLIME&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#21487;&#20197;&#35745;&#31639;&#36866;&#29992;&#20110;&#24314;&#27169;&#29983;&#23384;&#20998;&#26512;&#25968;&#25454;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#23616;&#37096;&#29305;&#24449;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#29616;&#21033;&#29992;&#20102;&#24182;&#34892;&#21270;&#33539;&#20363;&#65292;&#22240;&#20026;&#25152;&#26377;&#35745;&#31639;&#37117;&#20197;&#30697;&#38453;&#26041;&#24335;&#25191;&#34892;&#65292;&#20174;&#32780;&#21152;&#24555;&#20102;&#25191;&#34892;&#26102;&#38388;&#12290;&#27492;&#22806;&#65292;SurvLIMEpy&#36824;&#25552;&#20379;&#20102;&#21487;&#35270;&#21270;&#24037;&#20855;&#26469;&#26356;&#22909;&#22320;&#29702;&#35299;&#31639;&#27861;&#30340;&#32467;&#26524;&#12290;&#35813;&#21253;&#25903;&#25345;&#21508;&#31181;&#29983;&#23384;&#27169;&#22411;&#65292;&#20174;Cox&#27604;&#20363;&#39118;&#38505;&#27169;&#22411;&#21040;DeepHit&#25110;DeepSurv&#31561;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#31867;&#22411;&#30340;&#23454;&#39564;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#27169;&#25311;&#25968;&#25454;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#31639;&#27861;&#25429;&#33719;&#29305;&#24449;&#37325;&#35201;&#24615;&#30340;&#33021;&#21147;&#12290;&#20854;&#27425;&#65292;&#22312;&#20351;&#29992;&#19977;&#20010;&#24320;&#28304;&#29983;&#23384;&#25968;&#25454;&#38598;&#20197;&#21450;&#19968;&#32452;&#29983;&#23384;&#31639;&#27861;&#26102;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;SurvLIMEpy&#22312;&#24212;&#29992;&#20110;&#19981;&#21516;&#25968;&#25454;&#38598;&#26102;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we present SurvLIMEpy, an open-source Python package that implements the SurvLIME algorithm. This method allows to compute local feature importance for machine learning algorithms designed for modelling Survival Analysis data. Our implementation takes advantage of the parallelisation paradigm as all computations are performed in a matrix-wise fashion which speeds up execution time. Additionally, SurvLIMEpy assists the user with visualization tools to better understand the result of the algorithm. The package supports a wide variety of survival models, from the Cox Proportional Hazards Model to deep learning models such as DeepHit or DeepSurv. Two types of experiments are presented in this paper. First, by means of simulated data, we study the ability of the algorithm to capture the importance of the features. Second, we use three open source survival datasets together with a set of survival algorithms in order to demonstrate how SurvLIMEpy behaves when applied to differen
&lt;/p&gt;</description></item><item><title>Stochastic GFlowNets&#36890;&#36807;&#23398;&#20064;&#21160;&#24577;&#27169;&#22411;&#26469;&#25193;&#23637;&#20102;GFlowNets&#30340;&#36866;&#29992;&#24615;&#65292;&#35299;&#20915;&#20102;&#21407;&#27169;&#22411;&#21482;&#36866;&#29992;&#20110;&#30830;&#23450;&#24615;&#29615;&#22659;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#20855;&#26377;&#38543;&#26426;&#21160;&#24577;&#30340;&#21508;&#31181;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2302.09465</link><description>&lt;p&gt;
&#38543;&#26426;&#29983;&#25104;&#27969;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Stochastic Generative Flow Networks. (arXiv:2302.09465v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09465
&lt;/p&gt;
&lt;p&gt;
Stochastic GFlowNets&#36890;&#36807;&#23398;&#20064;&#21160;&#24577;&#27169;&#22411;&#26469;&#25193;&#23637;&#20102;GFlowNets&#30340;&#36866;&#29992;&#24615;&#65292;&#35299;&#20915;&#20102;&#21407;&#27169;&#22411;&#21482;&#36866;&#29992;&#20110;&#30830;&#23450;&#24615;&#29615;&#22659;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#20855;&#26377;&#38543;&#26426;&#21160;&#24577;&#30340;&#21508;&#31181;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27969;&#32593;&#32476;&#65288;&#25110;&#31616;&#31216;GFlowNets&#65289;&#26159;&#19968;&#31867;&#36890;&#36807;&#8220;&#25512;&#29702;&#21363;&#25511;&#21046;&#8221;&#23398;&#20064;&#37319;&#26679;&#22797;&#26434;&#32452;&#21512;&#32467;&#26500;&#30340;&#27010;&#29575;&#20195;&#29702;&#12290;&#23427;&#20204;&#22312;&#20174;&#32473;&#23450;&#30340;&#33021;&#37327;&#26223;&#35266;&#20013;&#29983;&#25104;&#39640;&#36136;&#37327;&#12289;&#22810;&#26679;&#21270;&#30340;&#20505;&#36873;&#26041;&#38754;&#34920;&#29616;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;GFlowNets&#20165;&#36866;&#29992;&#20110;&#30830;&#23450;&#24615;&#29615;&#22659;&#65292;&#24182;&#22312;&#20855;&#26377;&#38543;&#26426;&#21160;&#24577;&#30340;&#26356;&#19968;&#33324;&#20219;&#21153;&#20013;&#22833;&#36133;&#65292;&#36825;&#21487;&#33021;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#36866;&#29992;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#31216;&#20026;&#38543;&#26426;GFlowNets&#65292;&#23427;&#23558;GFlowNets&#25193;&#23637;&#21040;&#38543;&#26426;&#29615;&#22659;&#12290;&#36890;&#36807;&#23558;&#29366;&#24577;&#36716;&#31227;&#20998;&#35299;&#20026;&#20004;&#20010;&#27493;&#39588;&#65292;&#38543;&#26426;GFlowNets&#20998;&#31163;&#20102;&#29615;&#22659;&#38543;&#26426;&#24615;&#24182;&#23398;&#20064;&#20102;&#19968;&#20010;&#21160;&#24577;&#27169;&#22411;&#26469;&#25429;&#25417;&#23427;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#38543;&#26426;GFlowNets&#22312;&#20855;&#26377;&#38543;&#26426;&#21160;&#24577;&#30340;&#21508;&#31181;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#30456;&#36739;&#20110;&#26631;&#20934;GFlowNets&#20197;&#21450;MCMC&#21644;&#22522;&#20110;RL&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#26174;&#33879;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Flow Networks (or GFlowNets for short) are a family of probabilistic agents that learn to sample complex combinatorial structures through the lens of "inference as control". They have shown great potential in generating high-quality and diverse candidates from a given energy landscape. However, existing GFlowNets can be applied only to deterministic environments, and fail in more general tasks with stochastic dynamics, which can limit their applicability. To overcome this challenge, this paper introduces Stochastic GFlowNets, a new algorithm that extends GFlowNets to stochastic environments. By decomposing state transitions into two steps, Stochastic GFlowNets isolate environmental stochasticity and learn a dynamics model to capture it. Extensive experimental results demonstrate that Stochastic GFlowNets offer significant advantages over standard GFlowNets as well as MCMC- and RL-based approaches, on a variety of standard benchmarks with stochastic dynamics.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#21033;&#29992;T2I&#27169;&#22411;&#38544;&#21547;&#23398;&#20064;&#30340;&#30693;&#35782;&#26469;&#26356;&#21152;&#31934;&#30830;&#22320;&#25511;&#21046;&#29983;&#25104;&#32467;&#26524;&#12290;&#36890;&#36807;&#35757;&#32451;&#31616;&#21333;&#36731;&#37327;&#32423;&#30340;T2I&#36866;&#37197;&#22120;&#26469;&#23545;&#40784;&#20869;&#37096;&#30693;&#35782;&#19982;&#22806;&#37096;&#25511;&#21046;&#20449;&#21495;&#65292;&#23454;&#29616;&#22312;&#29983;&#25104;&#32467;&#26524;&#30340;&#39068;&#33394;&#21644;&#32467;&#26500;&#26041;&#38754;&#20016;&#23500;&#30340;&#25511;&#21046;&#21644;&#32534;&#36753;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.08453</link><description>&lt;p&gt;
T2I-Adapter&#65306;&#23398;&#20064;&#36866;&#37197;&#22120;&#20197;&#25366;&#25496;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#26356;&#22810;&#21487;&#25511;&#21046;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
T2I-Adapter: Learning Adapters to Dig out More Controllable Ability for Text-to-Image Diffusion Models. (arXiv:2302.08453v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08453
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#21033;&#29992;T2I&#27169;&#22411;&#38544;&#21547;&#23398;&#20064;&#30340;&#30693;&#35782;&#26469;&#26356;&#21152;&#31934;&#30830;&#22320;&#25511;&#21046;&#29983;&#25104;&#32467;&#26524;&#12290;&#36890;&#36807;&#35757;&#32451;&#31616;&#21333;&#36731;&#37327;&#32423;&#30340;T2I&#36866;&#37197;&#22120;&#26469;&#23545;&#40784;&#20869;&#37096;&#30693;&#35782;&#19982;&#22806;&#37096;&#25511;&#21046;&#20449;&#21495;&#65292;&#23454;&#29616;&#22312;&#29983;&#25104;&#32467;&#26524;&#30340;&#39068;&#33394;&#21644;&#32467;&#26500;&#26041;&#38754;&#20016;&#23500;&#30340;&#25511;&#21046;&#21644;&#32534;&#36753;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#27169;&#22411;&#30340;&#24778;&#20154;&#29983;&#25104;&#33021;&#21147;&#24050;&#32463;&#23637;&#31034;&#20102;&#23398;&#20064;&#22797;&#26434;&#32467;&#26500;&#21644;&#26377;&#24847;&#20041;&#35821;&#20041;&#30340;&#24378;&#22823;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20165;&#20165;&#20381;&#38752;&#25991;&#26412;&#25552;&#31034;&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;&#27169;&#22411;&#25152;&#23398;&#30340;&#30693;&#35782;&#65292;&#29305;&#21035;&#26159;&#24403;&#38656;&#35201;&#28789;&#27963;&#31934;&#30830;&#30340;&#25511;&#21046;&#65288;&#20363;&#22914;&#39068;&#33394;&#21644;&#32467;&#26500;&#65289;&#26102;&#12290;&#26412;&#25991;&#26088;&#22312;&#8220;&#25366;&#25496;&#8221;T2I&#27169;&#22411;&#38544;&#21547;&#23398;&#20064;&#30340;&#33021;&#21147;&#65292;&#28982;&#21518;&#26126;&#30830;&#22320;&#20351;&#29992;&#23427;&#20204;&#26469;&#26356;&#21152;&#31934;&#30830;&#22320;&#25511;&#21046;&#29983;&#25104;&#32467;&#26524;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#23398;&#20064;&#31616;&#21333;&#36731;&#37327;&#32423;&#30340;T2I&#36866;&#37197;&#22120;&#65292;&#23558;T2I&#27169;&#22411;&#20869;&#37096;&#30693;&#35782;&#19982;&#22806;&#37096;&#25511;&#21046;&#20449;&#21495;&#23545;&#40784;&#65292;&#21516;&#26102;&#20445;&#25345;&#21407;&#22987;&#22823;&#22411;T2I&#27169;&#22411;&#19981;&#21464;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#21487;&#20197;&#26681;&#25454;&#19981;&#21516;&#30340;&#26465;&#20214;&#35757;&#32451;&#21508;&#31181;&#36866;&#37197;&#22120;&#65292;&#23454;&#29616;&#22312;&#29983;&#25104;&#32467;&#26524;&#30340;&#39068;&#33394;&#21644;&#32467;&#26500;&#26041;&#38754;&#20016;&#23500;&#30340;&#25511;&#21046;&#21644;&#32534;&#36753;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;T2I&#36866;&#37197;&#22120;&#20855;&#26377;&#32452;&#21512;&#24615;&#21644;&#29983;&#25104;&#25928;&#29575;&#31561;&#23454;&#38469;&#20215;&#20540;&#30340;&#21560;&#24341;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The incredible generative ability of large-scale text-to-image (T2I) models has demonstrated strong power of learning complex structures and meaningful semantics. However, relying solely on text prompts cannot fully take advantage of the knowledge learned by the model, especially when flexible and accurate controlling (e.g., color and structure) is needed. In this paper, we aim to ``dig out" the capabilities that T2I models have implicitly learned, and then explicitly use them to control the generation more granularly. Specifically, we propose to learn simple and lightweight T2I-Adapters to align internal knowledge in T2I models with external control signals, while freezing the original large T2I models. In this way, we can train various adapters according to different conditions, achieving rich control and editing effects in the color and structure of the generation results. Further, the proposed T2I-Adapters have attractive properties of practical value, such as composability and gen
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#29983;&#26426;&#22120;&#30340;&#25945;&#24072;-&#23398;&#29983;&#35774;&#32622;&#65292;&#36890;&#36807;&#23398;&#29983;&#26426;&#22120;&#30340;&#38598;&#21512;&#26469;&#30740;&#31350;&#30001;&#20855;&#26377;&#22823;&#37327;&#21487;&#35843;&#21442;&#25968;&#30340;DNN&#30340;&#30417;&#30563;&#23398;&#20064;&#12290;&#30740;&#31350;&#34920;&#26126;DNN&#30340;&#23398;&#20064;&#22312;&#32593;&#32476;&#31354;&#38388;&#20013;&#30456;&#24403;&#24322;&#36136;&#12290;</title><link>http://arxiv.org/abs/2302.07419</link><description>&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#23398;&#29983;&#26426;&#22120;&#23454;&#29616;&#31354;&#38388;&#24322;&#36136;&#24615;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Spatially heterogeneous learning by a deep student machine. (arXiv:2302.07419v3 [cond-mat.dis-nn] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07419
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#29983;&#26426;&#22120;&#30340;&#25945;&#24072;-&#23398;&#29983;&#35774;&#32622;&#65292;&#36890;&#36807;&#23398;&#29983;&#26426;&#22120;&#30340;&#38598;&#21512;&#26469;&#30740;&#31350;&#30001;&#20855;&#26377;&#22823;&#37327;&#21487;&#35843;&#21442;&#25968;&#30340;DNN&#30340;&#30417;&#30563;&#23398;&#20064;&#12290;&#30740;&#31350;&#34920;&#26126;DNN&#30340;&#23398;&#20064;&#22312;&#32593;&#32476;&#31354;&#38388;&#20013;&#30456;&#24403;&#24322;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#21462;&#24471;&#20102;&#38750;&#20961;&#30340;&#25104;&#21151;&#65292;&#20294;&#30001;&#20110;&#20855;&#26377;&#22823;&#37327;&#21487;&#35843;&#21442;&#25968;&#65292;&#20854;&#20173;&#28982;&#26159;&#40657;&#21283;&#23376;&#12290;&#20026;&#20102;&#30740;&#31350;DNN&#30340;&#38544;&#34255;&#23618;&#65292;&#26412;&#25991;&#36890;&#36807;&#19968;&#31181;&#32479;&#35745;&#21147;&#23398;&#26041;&#27861;&#31216;&#20026;&#25945;&#24072;-&#23398;&#29983;&#35774;&#32622;&#65292;&#30740;&#31350;&#20102;&#30001;&#23485;&#24230;&#20026;N&#65292;&#28145;&#24230;&#20026;L&#65292;&#30001;&#20855;&#26377;c&#20010;&#36755;&#20837;&#30340;&#24863;&#30693;&#26426;&#32452;&#25104;&#30340;DNN&#30340;&#30417;&#30563;&#23398;&#20064;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#23398;&#29983;&#26426;&#22120;&#30340;&#38598;&#21512;&#65292;&#35813;&#38598;&#21512;&#21487;&#20197;&#31934;&#30830;&#37325;&#29616;&#30001;&#25945;&#24072;&#26426;&#22120;&#25552;&#20379;&#30340;M&#32452;N&#32500;&#36755;&#20837;/&#36755;&#20986;&#20851;&#31995;&#12290;&#25105;&#20204;&#20351;&#29992;&#21103;&#26412;&#26041;&#27861;&#65288;H. Yoshino&#65288;2020&#65289;&#65289;&#29702;&#35770;&#20998;&#26512;&#20102;&#38598;&#21512;&#65292;&#24182;&#36827;&#34892;&#20102;&#36138;&#23146;&#30340;Monte Carlo&#27169;&#25311;&#12290;&#23545;&#20110;&#39640;&#32500;&#25968;&#25454;$N \gg 1$&#65292;&#29702;&#35770;&#22312;'&#23494;&#38598;&#26497;&#38480;' $N \gg c \gg 1$ &#21644; $M \gg 1$ &#19988;&#22266;&#23450;$\alpha=M/c$&#26102;&#21464;&#24471;&#31934;&#30830;&#12290;&#29702;&#35770;&#21644;&#27169;&#25311;&#37117;&#34920;&#26126;&#65292;DNN&#30340;&#23398;&#20064;&#22312;&#32593;&#32476;&#31354;&#38388;&#20013;&#30456;&#24403;&#24322;&#36136;&#65306;&#26426;&#22120;&#30340;&#37197;&#32622;&#22312;&#38752;&#36817;&#36755;&#20837;/&#36755;&#20986;&#30340;&#23618;&#20869;&#26356;&#21152;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the spectacular successes, deep neural networks (DNN) with a huge number of adjustable parameters remain largely black boxes. To shed light on the hidden layers of DNN, we study supervised learning by a DNN of width $N$ and depth $L$ consisting of perceptrons with $c$ inputs by a statistical mechanics approach called the teacher-student setting. We consider an ensemble of student machines that exactly reproduce $M$ sets of $N$ dimensional input/output relations provided by a teacher machine. We analyze the ensemble theoretically using a replica method (H. Yoshino (2020)) and numerically performing greedy Monte Carlo simulations. The replica theory which works on high dimensional data $N \gg 1$ becomes exact in 'dense limit' $N \gg c \gg 1$ and $M \gg 1$ with fixed $\alpha=M/c$. Both the theory and the simulation suggest learning by the DNN is quite heterogeneous in the network space: configurations of the machines are more correlated within the layers closer to the input/output
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20809;&#23398;XNOR&#38376;&#21644;&#20809;&#30005;&#33655;&#32047;&#21152;&#22120;&#30340;BNN&#21152;&#36895;&#22120;&#35774;&#35745;&#65292;&#30456;&#27604;&#29616;&#26377;&#30340;PICs&#21152;&#36895;&#22120;&#65292;&#35813;&#35774;&#35745;&#22312;&#38754;&#31215;&#12289;&#33021;&#25928;&#21644;&#21534;&#21520;&#37327;&#31561;&#26041;&#38754;&#33719;&#24471;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2302.06405</link><description>&lt;p&gt;
&#22522;&#20110;&#20809;&#23398;&#30340;XNOR&#20301;&#35745;&#25968;&#21152;&#36895;&#22120;&#29992;&#20110;&#20108;&#20540;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#25928;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
An Optical XNOR-Bitcount Based Accelerator for Efficient Inference of Binary Neural Networks. (arXiv:2302.06405v2 [cs.AR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06405
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20809;&#23398;XNOR&#38376;&#21644;&#20809;&#30005;&#33655;&#32047;&#21152;&#22120;&#30340;BNN&#21152;&#36895;&#22120;&#35774;&#35745;&#65292;&#30456;&#27604;&#29616;&#26377;&#30340;PICs&#21152;&#36895;&#22120;&#65292;&#35813;&#35774;&#35745;&#22312;&#38754;&#31215;&#12289;&#33021;&#25928;&#21644;&#21534;&#21520;&#37327;&#31561;&#26041;&#38754;&#33719;&#24471;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20108;&#20540;&#31070;&#32463;&#32593;&#32476;&#65288;BNN&#65289;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#23558;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#27169;&#22411;&#21442;&#25968;&#36716;&#25442;&#20026;1&#20301;&#31934;&#24230;&#65292;&#20174;&#32780;&#26368;&#23567;&#21270;&#25512;&#26029;&#26102;&#30340;&#23384;&#20648;&#21644;&#35745;&#31639;&#38656;&#27714;&#65292;&#24182;&#19988;&#20960;&#20046;&#27809;&#26377;&#31934;&#24230;&#25439;&#22833;&#12290;&#24182;&#19988;&#23427;&#20204;&#20351;&#24471;&#22522;&#20110;XNOR&#21644;&#20301;&#35745;&#25968;&#30340;&#25805;&#20316;&#33021;&#22815;&#36827;&#34892;&#30828;&#20214;&#21152;&#36895;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#22522;&#20110;&#20809;&#30005;&#38598;&#25104;&#30005;&#36335;&#65288;PIC&#65289;&#30340;BNN&#21152;&#36895;&#22120;&#65292;&#23427;&#20204;&#25552;&#20379;&#30340;&#21534;&#21520;&#37327;&#21644;&#33021;&#25928;&#27604;&#20854;&#30005;&#23376;&#23545;&#24212;&#29289;&#26356;&#39640;&#65292;&#20294;&#36825;&#20123;&#21152;&#36895;&#22120;&#20013;&#20351;&#29992;&#30340;XNOR&#21644;&#20301;&#35745;&#25968;&#30005;&#36335;&#36824;&#38656;&#35201;&#36827;&#19968;&#27493;&#22686;&#24378;&#20197;&#25552;&#39640;&#23427;&#20204;&#30340;&#38754;&#31215;&#12289;&#33021;&#25928;&#21644;&#21534;&#21520;&#37327;&#12290;&#26412;&#25991;&#26088;&#22312;&#28385;&#36275;&#36825;&#19968;&#38656;&#27714;&#65292;&#25105;&#20204;&#21457;&#26126;&#20102;&#19968;&#31181;&#22522;&#20110;&#21333;-MRR&#30340;&#20809;&#23398;XNOR&#38376;&#65288;OXG&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20301;&#35745;&#25968;&#30005;&#36335;&#35774;&#35745;&#65292;&#31216;&#20026;&#20809;&#30005;&#33655;&#32047;&#21152;&#22120;&#65288;PCA&#65289;&#12290;&#25105;&#20204;&#20351;&#29992;&#22810;&#20010;OXGs&#26469;&#23454;&#29616;PCA&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#21152;&#36895;&#22120;&#35774;&#35745;&#22312;&#38754;&#31215;&#12289;&#33021;&#25928;&#21644;&#21534;&#21520;&#37327;&#26041;&#38754;&#23454;&#29616;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#65292;&#27604;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;PIC&#30340;BNN&#21152;&#36895;&#22120;&#26356;&#21152;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;
Binary Neural Networks (BNNs) are increasingly preferred over full-precision Convolutional Neural Networks(CNNs) to reduce the memory and computational requirements of inference processing with minimal accuracy drop. BNNs convert CNN model parameters to 1-bit precision, allowing inference of BNNs to be processed with simple XNOR and bitcount operations. This makes BNNs amenable to hardware acceleration. Several photonic integrated circuits (PICs) based BNN accelerators have been proposed. Although these accelerators provide remarkably higher throughput and energy efficiency than their electronic counterparts, the utilized XNOR and bitcount circuits in these accelerators need to be further enhanced to improve their area, energy efficiency, and throughput. This paper aims to fulfill this need. For that, we invent a single-MRR-based optical XNOR gate (OXG). Moreover, we present a novel design of bitcount circuit which we refer to as Photo-Charge Accumulator (PCA). We employ multiple OXGs 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#36923;&#36753;&#24341;&#23548;&#30340;&#20174;&#22024;&#26434;&#30340;&#20247;&#21253;&#26631;&#31614;&#20013;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#25913;&#36827;&#25991;&#26412;&#20998;&#31867;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#31561;&#20219;&#21153;&#20013;&#65292;&#23398;&#20064;&#20174;&#22024;&#26434;&#25968;&#25454;&#20013;&#25552;&#21462;&#26377;&#25928;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#36825;&#31181;&#26694;&#26550;&#33021;&#22815;&#34701;&#21512;&#36923;&#36753;&#30693;&#35782;&#65292;&#25552;&#39640;&#29616;&#26377;&#25216;&#26415;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2302.06337</link><description>&lt;p&gt;
&#21033;&#29992;&#36923;&#36753;&#23398;&#20064;&#20174;&#22024;&#26434;&#30340;&#20247;&#21253;&#26631;&#31614;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning from Noisy Crowd Labels with Logics. (arXiv:2302.06337v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06337
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#36923;&#36753;&#24341;&#23548;&#30340;&#20174;&#22024;&#26434;&#30340;&#20247;&#21253;&#26631;&#31614;&#20013;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#25913;&#36827;&#25991;&#26412;&#20998;&#31867;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#31561;&#20219;&#21153;&#20013;&#65292;&#23398;&#20064;&#20174;&#22024;&#26434;&#25968;&#25454;&#20013;&#25552;&#21462;&#26377;&#25928;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#36825;&#31181;&#26694;&#26550;&#33021;&#22815;&#34701;&#21512;&#36923;&#36753;&#30693;&#35782;&#65292;&#25552;&#39640;&#29616;&#26377;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;&#31526;&#21495;&#36923;&#36753;&#30693;&#35782;&#38598;&#25104;&#21040;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#20174;&#22024;&#26434;&#30340;&#20247;&#21253;&#26631;&#31614;&#20013;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#36923;&#36753;&#24341;&#23548;&#30340;&#20174;&#22024;&#26434;&#30340;&#20247;&#21253;&#26631;&#31614;&#20013;&#23398;&#20064;&#30340;&#26694;&#26550;&#65288;Logic-LNCL&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31867;&#20284;EM&#30340;&#36845;&#20195;&#36923;&#36753;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65292;&#33021;&#22815;&#20174;&#22024;&#26434;&#30340;&#26631;&#35760;&#25968;&#25454;&#21644;&#24863;&#20852;&#36259;&#30340;&#36923;&#36753;&#35268;&#21017;&#20013;&#36827;&#34892;&#23398;&#20064;&#12290;&#19982;&#20256;&#32479;&#30340;EM&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#21547;&#19968;&#31181;&#8220;&#20266;E&#27493;&#39588;&#8221;&#65292;&#20174;&#36923;&#36753;&#35268;&#21017;&#20013;&#33976;&#39311;&#20986;&#19968;&#31181;&#26032;&#31867;&#22411;&#30340;&#23398;&#20064;&#30446;&#26631;&#65292;&#28982;&#21518;&#22312;&#8220;&#20266;M&#27493;&#39588;&#8221;&#20013;&#20351;&#29992;&#35813;&#30446;&#26631;&#26469;&#35757;&#32451;&#20998;&#31867;&#22120;&#12290;&#22312;&#25991;&#26412;&#24773;&#24863;&#20998;&#31867;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#24191;&#27867;&#35780;&#20272;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#25913;&#36827;&#20102;&#26368;&#20808;&#36827;&#25216;&#26415;&#24182;&#20026;&#20174;&#22024;&#26434;&#30340;&#20247;&#21253;&#26631;&#31614;&#20013;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the integration of symbolic logic knowledge into deep neural networks for learning from noisy crowd labels. We introduce Logic-guided Learning from Noisy Crowd Labels (Logic-LNCL), an EM-alike iterative logic knowledge distillation framework that learns from both noisy labeled data and logic rules of interest. Unlike traditional EM methods, our framework contains a ``pseudo-E-step'' that distills from the logic rules a new type of learning target, which is then used in the ``pseudo-M-step'' for training the classifier. Extensive evaluations on two real-world datasets for text sentiment classification and named entity recognition demonstrate that the proposed framework improves the state-of-the-art and provides a new solution to learning from noisy crowd labels.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#31532;&#19968;&#20221;&#23545;&#20110;&#27973;&#23618;ViT&#36827;&#34892;&#35757;&#32451;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#20351;&#29992;SGD&#35757;&#32451;&#20250;&#20135;&#29983;&#31232;&#30095;&#30340;&#27880;&#24847;&#21147;&#22270;&#65292;&#30446;&#21069;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#19982;&#26631;&#35760;&#30456;&#20851;&#20196;&#29260;&#30340;&#20998;&#25968;&#20498;&#25968;&#12289;&#26631;&#35760;&#32423;&#21035;&#30340;&#20196;&#29260;&#22122;&#22768;&#27700;&#24179;&#21644;&#21021;&#22987;&#27169;&#22411;&#38169;&#35823;&#21576;&#27491;&#30456;&#20851;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2302.06015</link><description>&lt;p&gt;
&#27973;&#23618;&#35270;&#35273;Transformer&#30340;&#29702;&#35770;&#29702;&#35299;&#65306;&#23398;&#20064;&#12289;&#27867;&#21270;&#21644;&#26679;&#26412;&#22797;&#26434;&#24615;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Theoretical Understanding of Shallow Vision Transformers: Learning, Generalization, and Sample Complexity. (arXiv:2302.06015v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06015
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#31532;&#19968;&#20221;&#23545;&#20110;&#27973;&#23618;ViT&#36827;&#34892;&#35757;&#32451;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#20351;&#29992;SGD&#35757;&#32451;&#20250;&#20135;&#29983;&#31232;&#30095;&#30340;&#27880;&#24847;&#21147;&#22270;&#65292;&#30446;&#21069;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#19982;&#26631;&#35760;&#30456;&#20851;&#20196;&#29260;&#30340;&#20998;&#25968;&#20498;&#25968;&#12289;&#26631;&#35760;&#32423;&#21035;&#30340;&#20196;&#29260;&#22122;&#22768;&#27700;&#24179;&#21644;&#21021;&#22987;&#27169;&#22411;&#38169;&#35823;&#21576;&#27491;&#30456;&#20851;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20855;&#26377;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#30340;&#35270;&#35273;Transformer&#65288;ViTs&#65289;&#22312;&#35768;&#22810;&#35270;&#35273;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#23454;&#35777;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23618;&#38388;&#30340;&#38750;&#20984;&#20132;&#20114;&#65292;&#29702;&#35770;&#19978;&#30340;&#23398;&#20064;&#21644;&#27867;&#21270;&#20998;&#26512;&#22823;&#22810;&#26159;&#38590;&#20197;&#29702;&#35299;&#30340;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#20110;&#19968;&#39033;&#20998;&#31867;&#20219;&#21153;&#65292;&#20351;&#29992;&#19968;&#20010;&#33258;&#25105;&#27880;&#24847;&#23618;&#21644;&#20004;&#23618;&#24863;&#30693;&#26426;&#30340;&#27973;&#23618;ViT&#36827;&#34892;&#35757;&#32451;&#30340;&#31532;&#19968;&#31687;&#29702;&#35770;&#20998;&#26512;&#65292;&#24314;&#31435;&#20102;&#23545;&#20110;&#25968;&#25454;&#27169;&#22411;&#30340;&#25551;&#36848;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#21516;&#26102;&#34920;&#24449;&#26631;&#35760;&#30456;&#20851;&#21644;&#26631;&#35760;&#19981;&#30456;&#20851;&#30340;&#20196;&#29260;&#12290;&#25105;&#20204;&#30028;&#23450;&#20102;&#36798;&#21040;&#38646;&#27867;&#21270;&#35823;&#24046;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#38480;&#21046;&#19982;&#26631;&#35760;&#30456;&#20851;&#20196;&#29260;&#30340;&#37096;&#20998;&#20498;&#25968;&#12289;&#26631;&#35760;&#32423;&#21035;&#30340;&#20196;&#29260;&#22122;&#22768;&#27700;&#24179;&#21644;&#21021;&#22987;&#27169;&#22411;&#35823;&#24046;&#21576;&#27491;&#30456;&#20851;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;SGD&#65288;stochastic gradient descent&#65289;&#36827;&#34892;&#35757;&#32451;&#36807;&#31243;&#20250;&#23548;&#33268;&#31232;&#30095;&#30340;&#27880;&#24847;&#21147;&#22270;&#65292;&#36825;&#26159;&#23545;&#20110;&#27880;&#24847;&#21147;&#25104;&#21151;&#30340;&#19968;&#31181;&#24418;&#24335;&#35777;&#26126;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#25351;&#20986;&#65292;&#36866;&#24403;&#30340;&#20196;&#29260;&#30830;&#23450;&#26159;&#30830;&#20445;&#23454;&#29616;&#26368;&#20248;&#24615;&#33021;&#30340;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision Transformers (ViTs) with self-attention modules have recently achieved great empirical success in many vision tasks. Due to non-convex interactions across layers, however, theoretical learning and generalization analysis is mostly elusive. Based on a data model characterizing both label-relevant and label-irrelevant tokens, this paper provides the first theoretical analysis of training a shallow ViT, i.e., one self-attention layer followed by a two-layer perceptron, for a classification task. We characterize the sample complexity to achieve a zero generalization error. Our sample complexity bound is positively correlated with the inverse of the fraction of label-relevant tokens, the token noise level, and the initial model error. We also prove that a training process using stochastic gradient descent (SGD) leads to a sparse attention map, which is a formal verification of the general intuition about the success of attention. Moreover, this paper indicates that a proper token spa
&lt;/p&gt;</description></item><item><title>ORCA&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#36328;&#27169;&#24577;&#24494;&#35843;&#26694;&#26550;&#65292;&#23427;&#23558;&#21333;&#20010;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#36866;&#29992;&#24615;&#25193;&#23637;&#21040;&#22810;&#31181;&#27169;&#24577;&#65292;&#36890;&#36807;&#20808;&#23545;&#40784;&#20877;&#24494;&#35843;&#30340;&#26041;&#27861;&#20351;&#36328;&#27169;&#24577;&#24494;&#35843;&#25104;&#20026;&#21487;&#33021;&#65292;&#23454;&#39564;&#32467;&#26524;&#20248;&#20110;&#24191;&#27867;&#33539;&#22260;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.05738</link><description>&lt;p&gt;
&#36328;&#27169;&#24577;&#24494;&#35843;&#65306;&#20808;&#23545;&#40784;&#20877;&#31934;&#32454;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Cross-Modal Fine-Tuning: Align then Refine. (arXiv:2302.05738v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05738
&lt;/p&gt;
&lt;p&gt;
ORCA&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#36328;&#27169;&#24577;&#24494;&#35843;&#26694;&#26550;&#65292;&#23427;&#23558;&#21333;&#20010;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#36866;&#29992;&#24615;&#25193;&#23637;&#21040;&#22810;&#31181;&#27169;&#24577;&#65292;&#36890;&#36807;&#20808;&#23545;&#40784;&#20877;&#24494;&#35843;&#30340;&#26041;&#27861;&#20351;&#36328;&#27169;&#24577;&#24494;&#35843;&#25104;&#20026;&#21487;&#33021;&#65292;&#23454;&#39564;&#32467;&#26524;&#20248;&#20110;&#24191;&#27867;&#33539;&#22260;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24494;&#35843;&#24050;&#32463;&#22312;&#35832;&#22914;&#35270;&#35273;&#21644;NLP&#31561;&#32463;&#36807;&#30740;&#31350;&#30340;&#27169;&#24577;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#12290;&#20294;&#26159;&#65292;&#22312;&#35768;&#22810;&#20854;&#20182;&#27169;&#24577;&#20013;&#30001;&#20110;&#32570;&#20047;&#30456;&#20851;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#31867;&#20284;&#30340;&#36827;&#23637;&#24182;&#27809;&#26377;&#24471;&#21040;&#35266;&#23519;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ORCA&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#36328;&#27169;&#24577;&#24494;&#35843;&#26694;&#26550;&#65292;&#23427;&#23558;&#21333;&#20010;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#36866;&#29992;&#24615;&#25193;&#23637;&#21040;&#22810;&#31181;&#27169;&#24577;&#12290;&#36890;&#36807;&#23545;&#30446;&#26631;&#36755;&#20837;&#36827;&#34892;&#19968;&#31181;&#20808;&#23545;&#40784;&#20877;&#31934;&#32454;&#35843;&#25972;&#30340;&#24037;&#20316;&#27969;&#26469;&#36866;&#24212;&#30446;&#26631;&#20219;&#21153;&#65306;&#32473;&#23450;&#30446;&#26631;&#36755;&#20837;&#65292;ORCA&#39318;&#20808;&#23398;&#20064;&#19968;&#20010;&#23884;&#20837;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#23558;&#23884;&#20837;&#29305;&#24449;&#20998;&#24067;&#19982;&#39044;&#35757;&#32451;&#27169;&#24577;&#36827;&#34892;&#23545;&#40784;&#65292;&#28982;&#21518;&#22312;&#23884;&#20837;&#25968;&#25454;&#19978;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#20197;&#21033;&#29992;&#27169;&#24577;&#20043;&#38388;&#20998;&#20139;&#30340;&#30693;&#35782;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;ORCA&#22312;&#21253;&#21547;&#26469;&#33258;12&#31181;&#27169;&#24577;&#12289;&#36229;&#36807;60&#20010;&#25968;&#25454;&#38598;&#30340;3&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#20248;&#20110;&#24191;&#27867;&#33539;&#22260;&#30340;&#25163;&#24037;&#35774;&#35745;&#12289;AutoML&#12289;&#36890;&#29992;&#22411;&#21644;&#20219;&#21153;&#29305;&#23450;&#22411;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;&#36328;&#27169;&#24577;&#24494;&#35843;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning large-scale pretrained models has led to tremendous progress in well-studied modalities such as vision and NLP. However, similar gains have not been observed in many other modalities due to a lack of relevant pretrained models. In this work, we propose ORCA, a general cross-modal fine-tuning framework that extends the applicability of a single large-scale pretrained model to diverse modalities. ORCA adapts to a target task via an align-then-refine workflow: given the target input, ORCA first learns an embedding network that aligns the embedded feature distribution with the pretraining modality. The pretrained model is then fine-tuned on the embedded data to exploit the knowledge shared across modalities. Through extensive experiments, we show that ORCA obtains state-of-the-art results on 3 benchmarks containing over 60 datasets from 12 modalities, outperforming a wide range of hand-designed, AutoML, general-purpose, and task-specific methods. We highlight the importance of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32463;&#27982;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#26816;&#27979;&#29289;&#32852;&#32593;&#20725;&#23608;&#32593;&#32476;&#25915;&#20987;&#20197;&#21450;&#19981;&#21516;&#31867;&#22411;&#30340;&#25915;&#20987;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#22312;&#36739;&#23567;&#30340;&#39044;&#31639;&#19979;&#21152;&#36895;&#35757;&#32451;&#21644;&#26816;&#27979;&#36807;&#31243;&#65292;&#24182;&#19988;&#20855;&#26377;&#27604;&#26368;&#20808;&#36827;&#30340;&#26816;&#27979;&#27169;&#22411;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.02013</link><description>&lt;p&gt;
&#22522;&#20110;&#32463;&#27982;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;IoT&#20725;&#23608;&#32593;&#32476;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
IoT Botnet Detection Using an Economic Deep Learning Model. (arXiv:2302.02013v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02013
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32463;&#27982;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#26816;&#27979;&#29289;&#32852;&#32593;&#20725;&#23608;&#32593;&#32476;&#25915;&#20987;&#20197;&#21450;&#19981;&#21516;&#31867;&#22411;&#30340;&#25915;&#20987;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#22312;&#36739;&#23567;&#30340;&#39044;&#31639;&#19979;&#21152;&#36895;&#35757;&#32451;&#21644;&#26816;&#27979;&#36807;&#31243;&#65292;&#24182;&#19988;&#20855;&#26377;&#27604;&#26368;&#20808;&#36827;&#30340;&#26816;&#27979;&#27169;&#22411;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25216;&#26415;&#21019;&#26032;&#30340;&#24555;&#36895;&#36827;&#27493;&#22686;&#21152;&#20102;&#36807;&#21435;&#21313;&#24180;&#30340;&#20351;&#29992;&#21644;&#20998;&#21457;&#12290;&#20840;&#29699;&#29289;&#32852;&#32593;&#31995;&#32479;&#30340;&#24555;&#36895;&#22686;&#38271;&#22686;&#21152;&#20102;&#30001;&#24694;&#24847;&#31532;&#19977;&#26041;&#21019;&#24314;&#30340;&#32593;&#32476;&#23433;&#20840;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#32771;&#34385;&#23433;&#20840;&#38382;&#39064;&#21644;&#29289;&#32852;&#32593;&#31995;&#32479;&#38480;&#21046;&#30340;&#21487;&#38752;&#20837;&#20405;&#26816;&#27979;&#21644;&#32593;&#32476;&#21462;&#35777;&#31995;&#32479;&#23545;&#20110;&#20445;&#25252;&#36825;&#20123;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#29289;&#32852;&#32593;&#20725;&#23608;&#32593;&#32476;&#25915;&#20987;&#26159;&#20225;&#19994;&#21644;&#20010;&#20154;&#38754;&#20020;&#30340;&#37325;&#22823;&#23041;&#32961;&#20043;&#19968;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32463;&#27982;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#26816;&#27979;&#29289;&#32852;&#32593;&#20725;&#23608;&#32593;&#32476;&#25915;&#20987;&#20197;&#21450;&#19981;&#21516;&#31867;&#22411;&#30340;&#25915;&#20987;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#36739;&#23567;&#30340;&#23454;&#29616;&#39044;&#31639;&#19979;&#65292;&#21152;&#36895;&#20102;&#35757;&#32451;&#21644;&#26816;&#27979;&#36807;&#31243;&#65292;&#24182;&#33719;&#24471;&#20102;&#27604;&#26368;&#20808;&#36827;&#30340;&#26816;&#27979;&#27169;&#22411;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid progress in technology innovation usage and distribution has increased in the last decade. The rapid growth of the Internet of Things (IoT) systems worldwide has increased network security challenges created by malicious third parties. Thus, reliable intrusion detection and network forensics systems that consider security concerns and IoT systems limitations are essential to protect such systems. IoT botnet attacks are one of the significant threats to enterprises and individuals. Thus, this paper proposed an economic deep learning-based model for detecting IoT botnet attacks along with different types of attacks. The proposed model achieved higher accuracy than the state-of-the-art detection models using a smaller implementation budget and accelerating the training and detecting processes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#28304;&#22495;&#36866;&#24212;&#30340;&#23454;&#29992;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26041;&#26696;&#65292;&#36890;&#36807;&#22312;&#21407;&#22987;&#29305;&#24449;&#31354;&#38388;&#32858;&#31867;&#65292;&#26500;&#24314;&#30495;&#27491;&#22256;&#38590;&#30340;&#36127;&#23545;&#65292;&#32467;&#21512;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#29702;&#35770;&#65292;&#23398;&#20064;&#19968;&#20010;&#22495;&#19981;&#21464;&#30340;&#29305;&#24449;&#26469;&#35299;&#20915;&#22495;&#19978;&#30340;&#24046;&#24322;&#38382;&#39064;&#65292;&#33021;&#22815;&#22312;&#19977;&#20010;&#24120;&#35265;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#26377;&#25928;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2301.13428</link><description>&lt;p&gt;
&#23545;&#27604;&#19982;&#32858;&#31867;&#65306;&#23398;&#20064;&#37051;&#22495;&#23545;&#34920;&#31034;&#29992;&#20110;&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Contrast and Clustering: Learning Neighborhood Pair Representation for Source-free Domain Adaptation. (arXiv:2301.13428v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13428
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#28304;&#22495;&#36866;&#24212;&#30340;&#23454;&#29992;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26041;&#26696;&#65292;&#36890;&#36807;&#22312;&#21407;&#22987;&#29305;&#24449;&#31354;&#38388;&#32858;&#31867;&#65292;&#26500;&#24314;&#30495;&#27491;&#22256;&#38590;&#30340;&#36127;&#23545;&#65292;&#32467;&#21512;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#29702;&#35770;&#65292;&#23398;&#20064;&#19968;&#20010;&#22495;&#19981;&#21464;&#30340;&#29305;&#24449;&#26469;&#35299;&#20915;&#22495;&#19978;&#30340;&#24046;&#24322;&#38382;&#39064;&#65292;&#33021;&#22815;&#22312;&#19977;&#20010;&#24120;&#35265;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#26377;&#25928;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#22495;&#36866;&#24212;&#21033;&#29992;&#26469;&#33258;&#19981;&#21516;&#20998;&#24067;&#30340;&#28304;&#25968;&#25454;&#35299;&#20915;&#20174;&#26410;&#26631;&#35760;&#30446;&#26631;&#22495;&#20998;&#31867;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#26041;&#27861;&#38656;&#35201;&#35775;&#38382;&#28304;&#25968;&#25454;&#65292;&#36825;&#32463;&#24120;&#24341;&#36215;&#25968;&#25454;&#38544;&#31169;&#26041;&#38754;&#30340;&#25285;&#24551;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#26356;&#21152;&#23454;&#38469;&#20294;&#20805;&#28385;&#25361;&#25112;&#30340;&#35774;&#32622;&#65292;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#65292;&#28304;&#22495;&#25968;&#25454;&#19981;&#21487;&#29992;&#65292;&#30446;&#26631;&#22495;&#25968;&#25454;&#26410;&#26631;&#35760;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20174;&#23545;&#27604;&#23398;&#20064;&#30340;&#35282;&#24230;&#35299;&#20915;&#20102;&#22495;&#38388;&#24046;&#24322;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#36890;&#36807;&#20197;&#19979;&#26041;&#24335;&#23398;&#20064;&#19968;&#20010;&#22495;&#19981;&#21464;&#30340;&#29305;&#24449;:1)&#22312;&#21407;&#22987;&#29305;&#24449;&#31354;&#38388;&#20013;&#30452;&#25509;&#25191;&#34892;&#20855;&#26377;&#26368;&#36817;&#37051;&#23621;&#30340;&#32858;&#31867;&#65307;2)&#36890;&#36807;&#25193;&#23637;&#37051;&#23621;&#26500;&#36896;&#30495;&#27491;&#22256;&#38590;&#30340;&#36127;&#23545;&#65292;&#32780;&#19981;&#24341;&#20837;&#39069;&#22806;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65307;3)&#32467;&#21512;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#29702;&#35770;&#20197;&#33719;&#24471;&#35745;&#31639;&#20248;&#21183;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#24120;&#35265;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;VisDA&#12289;Office-Home&#21644;Office-31&#19978;&#36827;&#34892;&#20102;&#20180;&#32454;&#30340;&#28040;&#34701;&#30740;&#31350;&#21644;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised domain adaptation uses source data from different distributions to solve the problem of classifying data from unlabeled target domains. However, conventional methods require access to source data, which often raise concerns about data privacy. In this paper, we consider a more practical but challenging setting where the source domain data is unavailable and the target domain data is unlabeled. Specifically, we address the domain discrepancy problem from the perspective of contrastive learning. The key idea of our work is to learn a domain-invariant feature by 1) performing clustering directly in the original feature space with nearest neighbors; 2) constructing truly hard negative pairs by extended neighbors without introducing additional computational complexity; and 3) combining noise-contrastive estimation theory to gain computational advantage. We conduct careful ablation studies and extensive experiments on three common benchmarks: VisDA, Office-Home, and Office-31. T
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20379;&#20102;&#24102;&#26377;&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#35777;&#26126;&#20102;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#22312;Bradley-Terry-Luce&#21644;Plackett-Luce&#27169;&#22411;&#19979;&#25910;&#25947;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#22312;&#19968;&#23450;&#30340;&#35206;&#30422;&#20551;&#35774;&#19979;&#65292;&#22522;&#20110;&#24754;&#35266;&#20272;&#35745;&#30340;MLE&#25552;&#20379;&#20102;&#24615;&#33021;&#26356;&#22909;&#30340;&#31574;&#30053;&#12290;&#22312;&#35777;&#26126;&#20102;&#30495;&#23454;MLE&#21644;&#20197;&#25104;&#23545;&#27604;&#36739;&#24418;&#24335;&#26367;&#20195;&#30340;&#22791;&#36873;MLE&#37117;&#21487;&#20197;&#22312;PL&#27169;&#22411;&#19979;&#25910;&#25947;&#30340;&#21516;&#26102;&#65292;&#20063;&#34920;&#26126;&#20102;&#30495;&#23454;MLE&#30340;&#39640;&#25928;&#24615;&#12290;&#36825;&#20123;&#32467;&#26524;&#20026;RLHF&#31639;&#27861;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#65292;&#24182;&#32479;&#19968;&#20102;RLHF&#38382;&#39064;&#21644;IRL&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.11270</link><description>&lt;p&gt;
&#20351;&#29992;&#26469;&#33258;&#25104;&#23545;&#25110;$K$&#20803;&#27604;&#36739;&#30340;&#20154;&#31867;&#21453;&#39304;&#30340;&#35268;&#33539;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Principled Reinforcement Learning with Human Feedback from Pairwise or $K$-wise Comparisons. (arXiv:2301.11270v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11270
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20379;&#20102;&#24102;&#26377;&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#35777;&#26126;&#20102;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#22312;Bradley-Terry-Luce&#21644;Plackett-Luce&#27169;&#22411;&#19979;&#25910;&#25947;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#22312;&#19968;&#23450;&#30340;&#35206;&#30422;&#20551;&#35774;&#19979;&#65292;&#22522;&#20110;&#24754;&#35266;&#20272;&#35745;&#30340;MLE&#25552;&#20379;&#20102;&#24615;&#33021;&#26356;&#22909;&#30340;&#31574;&#30053;&#12290;&#22312;&#35777;&#26126;&#20102;&#30495;&#23454;MLE&#21644;&#20197;&#25104;&#23545;&#27604;&#36739;&#24418;&#24335;&#26367;&#20195;&#30340;&#22791;&#36873;MLE&#37117;&#21487;&#20197;&#22312;PL&#27169;&#22411;&#19979;&#25910;&#25947;&#30340;&#21516;&#26102;&#65292;&#20063;&#34920;&#26126;&#20102;&#30495;&#23454;MLE&#30340;&#39640;&#25928;&#24615;&#12290;&#36825;&#20123;&#32467;&#26524;&#20026;RLHF&#31639;&#27861;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#65292;&#24182;&#32479;&#19968;&#20102;RLHF&#38382;&#39064;&#21644;IRL&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20026;&#24102;&#26377;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#24403;&#30495;&#23454;&#22870;&#21169;&#20989;&#25968;&#20026;&#32447;&#24615;&#20989;&#25968;&#26102;&#65292;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65288;MLE&#65289;&#22312;Bradley-Terry-Luce&#65288;BTL&#65289;&#27169;&#22411;&#21644;Plackett-Luce&#65288;PL&#65289;&#27169;&#22411;&#19979;&#22343;&#25910;&#25947;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#22522;&#20110;&#23398;&#24471;&#30340;&#22870;&#21169;&#27169;&#22411;&#35757;&#32451;&#31574;&#30053;&#26102;&#65292;MLE&#20250;&#22833;&#36133;&#65292;&#32780;&#22522;&#20110;&#24754;&#35266;&#20272;&#35745;&#30340;MLE&#22312;&#19968;&#23450;&#30340;&#35206;&#30422;&#20551;&#35774;&#19979;&#25552;&#20379;&#24615;&#33021;&#26356;&#22909;&#30340;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;PL&#27169;&#22411;&#19979;&#65292;&#30495;&#23454;MLE&#21644;&#23558;$k$&#20803;&#27604;&#36739;&#25286;&#20998;&#20026;&#25104;&#23545;&#27604;&#36739;&#30340;&#22791;&#36873;MLE&#37117;&#25910;&#25947;&#12290;&#32780;&#30495;&#23454;MLE&#26159;&#28176;&#36817;&#26356;&#20026;&#39640;&#25928;&#30340;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#39564;&#35777;&#20102;&#29616;&#26377;RLHF&#31639;&#27861;&#65288;&#22914;InstructGPT&#65289;&#30340;&#23454;&#39564;&#25104;&#21151;&#65292;&#24182;&#20026;&#31639;&#27861;&#35774;&#35745;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#32479;&#19968;&#20102;RLHF&#38382;&#39064;&#21644;&#26368;&#22823;&#29109;&#21453;&#21521;&#24378;&#21270;&#23398;&#20064;(IRL)&#38382;&#39064;&#65292;&#24182;&#20026;&#20854;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide a theoretical framework for Reinforcement Learning with Human Feedback (RLHF). Our analysis shows that when the true reward function is linear, the widely used maximum likelihood estimator (MLE) converges under both the Bradley-Terry-Luce (BTL) model and the Plackett-Luce (PL) model. However, we show that when training a policy based on the learned reward model, MLE fails while a pessimistic MLE provides policies with improved performance under certain coverage assumptions. Additionally, we demonstrate that under the PL model, the true MLE and an alternative MLE that splits the $K$-wise comparison into pairwise comparisons both converge. Moreover, the true MLE is asymptotically more efficient. Our results validate the empirical success of existing RLHF algorithms in InstructGPT and provide new insights for algorithm design. Furthermore, our results unify the problem of RLHF and max-entropy Inverse Reinforcement Learning (IRL), and provide the first sample complexity bound fo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; C-MGN &#21644; M-MGN &#20004;&#31181;&#21333;&#35843;&#26799;&#24230;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#29992;&#20110;&#30452;&#25509;&#23398;&#20064;&#20984;&#20989;&#25968;&#30340;&#26799;&#24230;&#65292;&#24182;&#26174;&#31034;&#20986;&#27604;&#29616;&#26377;&#30340;&#26041;&#27861;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2301.10862</link><description>&lt;p&gt;
&#29992;&#21333;&#35843;&#26799;&#24230;&#32593;&#32476;&#23398;&#20064;&#20984;&#20989;&#25968;&#30340;&#26799;&#24230;
&lt;/p&gt;
&lt;p&gt;
Learning Gradients of Convex Functions with Monotone Gradient Networks. (arXiv:2301.10862v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10862
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; C-MGN &#21644; M-MGN &#20004;&#31181;&#21333;&#35843;&#26799;&#24230;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#29992;&#20110;&#30452;&#25509;&#23398;&#20064;&#20984;&#20989;&#25968;&#30340;&#26799;&#24230;&#65292;&#24182;&#26174;&#31034;&#20986;&#27604;&#29616;&#26377;&#30340;&#26041;&#27861;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24050;&#32463;&#20184;&#20986;&#20102;&#22823;&#37327;&#30340;&#21162;&#21147;&#26469;&#25512;&#23548;&#21644;&#20998;&#26512;&#20449;&#21495;&#22788;&#29702;&#38382;&#39064;&#30340;&#26377;&#25928;&#20984;&#20989;&#25968;&#24418;&#24335;&#65292;&#20294;&#20984;&#20989;&#25968;&#30340;&#26799;&#24230;&#20063;&#20855;&#26377;&#37325;&#35201;&#30340;&#24212;&#29992;&#65292;&#20174;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#21040;&#26368;&#20248;&#36816;&#36755;&#31561;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#20984;&#30446;&#26631;&#20989;&#25968;&#65292;&#20294;&#24456;&#23569;&#30740;&#31350;&#23398;&#20064;&#23427;&#20204;&#21333;&#35843;&#26799;&#24230;&#30340;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#21333;&#35843;&#26799;&#24230;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500; C-MGN &#21644; M-MGN&#65292;&#29992;&#20110;&#30452;&#25509;&#23398;&#20064;&#20984;&#20989;&#25968;&#30340;&#26799;&#24230;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#19982;&#29616;&#26377;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#32593;&#32476;&#23481;&#26131;&#35757;&#32451;&#65292;&#33719;&#24471;&#20102;&#26356;&#20934;&#30830;&#30340;&#21333;&#35843;&#26799;&#24230;&#22330;&#65292;&#24182;&#20351;&#29992;&#20102;&#26356;&#23569;&#30340;&#21442;&#25968;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#23427;&#20204;&#23398;&#20064;&#26368;&#20248;&#36816;&#36755;&#26144;&#23556;&#20197;&#22686;&#24378;&#39537;&#21160;&#22270;&#20687;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
While much effort has been devoted to deriving and analyzing effective convex formulations of signal processing problems, the gradients of convex functions also have critical applications ranging from gradient-based optimization to optimal transport. Recent works have explored data-driven methods for learning convex objective functions, but learning their monotone gradients is seldom studied. In this work, we propose C-MGN and M-MGN, two monotone gradient neural network architectures for directly learning the gradients of convex functions. We show that, compared to state of the art methods, our networks are easier to train, learn monotone gradient fields more accurately, and use significantly fewer parameters. We further demonstrate their ability to learn optimal transport mappings to augment driving image data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#20010;&#26080;&#32447;&#27979;&#37327;&#27963;&#21160;&#25152;&#25552;&#20379;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#23558;&#20854;&#19982;&#26426;&#22120;&#23398;&#20064;&#32467;&#21512;&#36215;&#26469;&#29992;&#20110;&#25351;&#32441;&#35782;&#21035;&#12289;&#35270;&#32447;&#26816;&#27979;&#12289;&#26381;&#21153;&#36136;&#37327;&#39044;&#27979;&#25110;&#38142;&#36335;&#36873;&#25321;&#31561;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2301.03364</link><description>&lt;p&gt;
&#36208;&#21521;AI-enabled&#36830;&#25509;&#20135;&#19994;: AGV&#36890;&#20449;&#21644;&#20256;&#24863;&#22120;&#27979;&#37327;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Towards an AI-enabled Connected Industry: AGV Communication and Sensor Measurement Datasets. (arXiv:2301.03364v3 [cs.NI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.03364
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#20010;&#26080;&#32447;&#27979;&#37327;&#27963;&#21160;&#25152;&#25552;&#20379;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#23558;&#20854;&#19982;&#26426;&#22120;&#23398;&#20064;&#32467;&#21512;&#36215;&#26469;&#29992;&#20110;&#25351;&#32441;&#35782;&#21035;&#12289;&#35270;&#32447;&#26816;&#27979;&#12289;&#26381;&#21153;&#36136;&#37327;&#39044;&#27979;&#25110;&#38142;&#36335;&#36873;&#25321;&#31561;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#20010;&#24037;&#19994;&#27979;&#35797;&#24179;&#21488;&#19978;&#30340;&#26080;&#32447;&#27979;&#37327;&#27963;&#21160;: &#24037;&#19994;&#36710;&#36742;&#38388;&#36890;&#20449;(iV2V)&#21644;&#24037;&#19994;&#36710;&#36742;&#21040;&#22522;&#30784;&#35774;&#26045;&#21152;&#20256;&#24863;&#22120;(iV2I+)&#12290;&#25552;&#20379;&#20102;&#20851;&#20110;&#36825;&#20004;&#20010;&#25429;&#33719;&#25968;&#25454;&#38598;&#30340;&#35814;&#32454;&#20449;&#24687;&#12290;iV2V&#28085;&#30422;&#20102;&#33258;&#21160;&#24341;&#23548;&#36710;(AGVs)&#20043;&#38388;&#30340;&#20391;&#21521;&#38142;&#36335;&#36890;&#20449;&#22330;&#26223;&#65292;&#32780;iV2I+&#21017;&#26159;&#22312;&#24037;&#19994;&#35774;&#32622;&#20013;&#36827;&#34892;&#30340;&#65292;&#20854;&#20013;&#33258;&#20027;&#28165;&#27905;&#26426;&#22120;&#20154;&#36830;&#25509;&#21040;&#31169;&#26377;&#34562;&#31389;&#32593;&#32476;&#12290;&#19981;&#21516;&#36890;&#20449;&#25216;&#26415;&#30340;&#32452;&#21512;&#65292;&#36830;&#21516;&#20849;&#21516;&#30340;&#27979;&#37327;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#26426;&#22120;&#23398;&#20064;(ML)&#21487;&#20197;&#21033;&#29992;&#30340;&#27934;&#23519;&#21147;&#65292;&#29992;&#20110;&#25351;&#32441;&#35782;&#21035;&#12289;&#35270;&#32447;&#26816;&#27979;&#12289;&#26381;&#21153;&#36136;&#37327;&#39044;&#27979;&#25110;&#38142;&#36335;&#36873;&#25321;&#31561;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#25968;&#25454;&#38598;&#24050;&#26631;&#35760;&#21644;&#39044;&#36807;&#28388;&#65292;&#20197;&#20415;&#24555;&#36895;&#21551;&#21160;&#21644;&#24212;&#29992;&#12290;&#23545;&#20110;&#20004;&#20010;&#25968;&#25454;&#38598;&#65292;&#36824;&#35814;&#32454;&#20171;&#32461;&#20102;&#30456;&#24212;&#30340;&#27979;&#35797;&#24179;&#21488;&#21644;&#27979;&#37327;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents two wireless measurement campaigns in industrial testbeds: industrial Vehicle-to-vehicle (iV2V) and industrial Vehicle-to-infrastructure plus Sensor (iV2I+). Detailed information about the two captured datasets is provided as well. iV2V covers sidelink communication scenarios between Automated Guided Vehicles (AGVs), while iV2I+ is conducted at an industrial setting where an autonomous cleaning robot is connected to a private cellular network. The combination of different communication technologies, together with a common measurement methodology, provides insights that can be exploited by Machine Learning (ML) for tasks such as fingerprinting, line-of-sight detection, prediction of quality of service or link selection. Moreover, the datasets are labelled and pre-filtered for fast on-boarding and applicability. The corresponding testbeds and measurements are also presented in detail for both datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22686;&#21152;&#20102;ConvNeXt&#30340;&#24494;&#23567;&#35774;&#35745;&#21464;&#21270;&#26469;&#25193;&#20805;DARTS&#25628;&#32034;&#31354;&#38388;&#65292;&#25552;&#20986;&#20102;PIBConv&#22359;&#26469;&#20943;&#23569;&#35745;&#31639;&#21344;&#29992;&#65292;&#25105;&#20204;&#30340;&#26550;&#26500;&#22312;&#23618;&#25968;&#20165;&#20026;2&#26102;&#20248;&#20110;&#19968;&#20010;&#20855;&#26377;&#31867;&#20284;&#35268;&#27169;&#30340;DARTS&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2301.01286</link><description>&lt;p&gt;
DARTS&#25628;&#32034;&#31354;&#38388;&#30340;&#20266;&#21453;&#36716;&#29942;&#39048;&#21367;&#31215;
&lt;/p&gt;
&lt;p&gt;
Pseudo-Inverted Bottleneck Convolution for DARTS Search Space. (arXiv:2301.01286v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.01286
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22686;&#21152;&#20102;ConvNeXt&#30340;&#24494;&#23567;&#35774;&#35745;&#21464;&#21270;&#26469;&#25193;&#20805;DARTS&#25628;&#32034;&#31354;&#38388;&#65292;&#25552;&#20986;&#20102;PIBConv&#22359;&#26469;&#20943;&#23569;&#35745;&#31639;&#21344;&#29992;&#65292;&#25105;&#20204;&#30340;&#26550;&#26500;&#22312;&#23618;&#25968;&#20165;&#20026;2&#26102;&#20248;&#20110;&#19968;&#20010;&#20855;&#26377;&#31867;&#20284;&#35268;&#27169;&#30340;DARTS&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#24494;&#26550;&#26500;&#25628;&#32034;&#65288;DARTS&#65289;&#20316;&#20026;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#26041;&#27861;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#33258;&#24341;&#20837;DARTS&#20197;&#26469;&#65292;&#24456;&#23569;&#26377;&#24037;&#20316;&#22522;&#20110;&#26368;&#20808;&#36827;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#35774;&#35745;&#21407;&#21017;&#36880;&#27493;&#25193;&#23637;DARTS&#25628;&#32034;&#31354;&#38388;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#22686;&#21152;ConvNeXt&#30340;&#24494;&#23567;&#35774;&#35745;&#21464;&#21270;&#26469;&#36880;&#27493;&#25193;&#20805;DARTS&#25628;&#32034;&#31354;&#38388;&#65292;&#24182;&#30740;&#31350;&#20934;&#30830;&#24615;&#12289;&#35780;&#20272;&#23618;&#25968;&#21644;&#35745;&#31639;&#25104;&#26412;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20266;&#21453;&#36716;&#29942;&#39048;&#21367;&#31215;&#65288;PIBConv&#65289;&#22359;&#65292;&#26088;&#22312;&#20943;&#23569;ConvNeXt&#20013;&#25552;&#20986;&#30340;&#21453;&#36716;&#29942;&#39048;&#22359;&#30340;&#35745;&#31639;&#21344;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26550;&#26500;&#23545;&#20110;&#35780;&#20272;&#23618;&#25968;&#30340;&#25935;&#24863;&#24230;&#35201;&#23567;&#24471;&#22810;&#65292;&#24182;&#19988;&#22312;&#23618;&#25968;&#20165;&#20026;2&#26102;&#26174;&#30528;&#20248;&#20110;&#19968;&#20010;&#20855;&#26377;&#31867;&#20284;&#35268;&#27169;&#30340;DARTS&#32593;&#32476;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#26356;&#23569;&#30340;&#23618;&#25968;&#65292;&#23427;&#19981;&#20165;&#22312;&#36739;&#20302;&#30340;GMACs&#21644;&#21442;&#25968;&#25968;&#37327;&#19979;&#23454;&#29616;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615; &#65292;&#35745;&#31639;&#21344;&#29992;&#20063;&#26356;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differentiable Architecture Search (DARTS) has attracted considerable attention as a gradient-based neural architecture search method. Since the introduction of DARTS, there has been little work done on adapting the action space based on state-of-art architecture design principles for CNNs. In this work, we aim to address this gap by incrementally augmenting the DARTS search space with micro-design changes inspired by ConvNeXt and studying the trade-off between accuracy, evaluation layer count, and computational cost. We introduce the Pseudo-Inverted Bottleneck Conv (PIBConv) block intending to reduce the computational footprint of the inverted bottleneck block proposed in ConvNeXt. Our proposed architecture is much less sensitive to evaluation layer count and outperforms a DARTS network with similar size significantly, at layer counts as small as 2. Furthermore, with less layers, not only does it achieve higher accuracy with lower computational footprint (measured in GMACs) and parame
&lt;/p&gt;</description></item><item><title>KoopmanLab &#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22359;&#65292;&#21517;&#20026; Koopman neural operator (KNO), &#29992;&#20110;&#22312;&#27809;&#26377;&#35299;&#26512;&#35299;&#25110;&#38381;&#21512;&#24418;&#24335;&#30340;&#26465;&#20214;&#19979;&#23398;&#20064; PDEs&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#25968;&#20540;&#26041;&#27861;&#31934;&#24230;&#21644;&#25928;&#29575;&#24179;&#34913;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#26410;&#30693;&#28508;&#22312; PDEs &#29983;&#25104;&#30340;&#32463;&#39564;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2301.01104</link><description>&lt;p&gt;
KoopmanLab: &#29992;&#26426;&#22120;&#23398;&#20064;&#27714;&#35299;&#22797;&#26434;&#29289;&#29702;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
KoopmanLab: machine learning for solving complex physics equations. (arXiv:2301.01104v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.01104
&lt;/p&gt;
&lt;p&gt;
KoopmanLab &#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22359;&#65292;&#21517;&#20026; Koopman neural operator (KNO), &#29992;&#20110;&#22312;&#27809;&#26377;&#35299;&#26512;&#35299;&#25110;&#38381;&#21512;&#24418;&#24335;&#30340;&#26465;&#20214;&#19979;&#23398;&#20064; PDEs&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#25968;&#20540;&#26041;&#27861;&#31934;&#24230;&#21644;&#25928;&#29575;&#24179;&#34913;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#26410;&#30693;&#28508;&#22312; PDEs &#29983;&#25104;&#30340;&#32463;&#39564;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#22810;&#29289;&#29702;&#29702;&#35770;&#37117;&#22522;&#20110;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#65292;&#28982;&#32780;&#36234;&#26469;&#36234;&#22797;&#26434;&#30340;&#29289;&#29702;&#26041;&#31243;&#65292;&#29305;&#21035;&#26159;&#32570;&#20047;&#35299;&#26512;&#35299;&#25110;&#38381;&#21512;&#24418;&#24335;&#30340;&#26041;&#31243;&#65292;&#38459;&#30861;&#20102;&#29289;&#29702;&#23398;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#12290;&#20256;&#32479;&#25968;&#20540;&#26041;&#27861;&#22312;&#35745;&#31639;&#27714;&#35299;PDEs&#26102;&#38754;&#20020;&#31934;&#24230;&#21644;&#25928;&#29575;&#20043;&#38388;&#30340;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#19988;&#19981;&#36866;&#29992;&#20110;&#26410;&#30693;&#28508;&#22312;PDEs&#29983;&#25104;&#30340;&#32463;&#39564;&#25968;&#25454;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;KoopmanLab&#65292;&#19968;&#31181;&#39640;&#25928;&#30340;Koopman&#31070;&#32463;&#31639;&#23376;&#31995;&#21015;&#27169;&#22359;&#65292;&#29992;&#20110;&#23398;&#20064;&#27809;&#26377;&#35299;&#26512;&#35299;&#25110;&#38381;&#21512;&#24418;&#24335;&#30340;PDEs&#12290;&#25105;&#20204;&#30340;&#27169;&#22359;&#21253;&#25324;&#22810;&#20010;&#21464;&#31181;&#30340;Koopman&#31070;&#32463;&#31639;&#23376;&#65288;KNO&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#31995;&#32479;&#29702;&#35770;&#24320;&#21457;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;PDE&#27714;&#35299;&#22120;&#65292;&#26080;&#38656;&#32593;&#26684;&#12290;&#32039;&#20945;&#30340;KNO&#21464;&#20307;&#21487;&#20197;&#20934;&#30830;&#22320;&#27714;&#35299;&#23567;&#27169;&#22411;&#23610;&#23544;&#30340;PDEs&#65292;&#32780;&#22823;&#22411;&#30340;KNO&#21464;&#20307;&#22312;&#39044;&#27979;&#30001;&#26410;&#30693;&#65292;&#39640;&#32500;&#21160;&#24577;&#31995;&#32479;&#25511;&#21046;&#30340;&#39640;&#24230;&#22797;&#26434;&#30340;&#31995;&#32479;&#26041;&#38754;&#26356;&#20855;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Numerous physics theories are rooted in partial differential equations (PDEs). However, the increasingly intricate physics equations, especially those that lack analytic solutions or closed forms, have impeded the further development of physics. Computationally solving PDEs by classic numerical approaches suffers from the trade-off between accuracy and efficiency and is not applicable to the empirical data generated by unknown latent PDEs. To overcome this challenge, we present KoopmanLab, an efficient module of the Koopman neural operator family, for learning PDEs without analytic solutions or closed forms. Our module consists of multiple variants of the Koopman neural operator (KNO), a kind of mesh-independent neural-network-based PDE solvers developed following dynamic system theory. The compact variants of KNO can accurately solve PDEs with small model sizes while the large variants of KNO are more competitive in predicting highly complicated dynamic systems govern by unknown, high
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;PGT&#29702;&#35770;&#27010;&#24565;&#21644;&#36817;&#20284;PGT&#30340;&#23450;&#37327;&#25216;&#26415;&#65292;&#20026;&#35780;&#20272;&#21644;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20013;&#30340;&#24615;&#33021;&#25552;&#20379;&#20102;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2301.00243</link><description>&lt;p&gt;
&#25509;&#36817;&#23792;&#20540;&#22320;&#38754;&#30495;&#30456;
&lt;/p&gt;
&lt;p&gt;
Approaching Peak Ground Truth. (arXiv:2301.00243v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00243
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;PGT&#29702;&#35770;&#27010;&#24565;&#21644;&#36817;&#20284;PGT&#30340;&#23450;&#37327;&#25216;&#26415;&#65292;&#20026;&#35780;&#20272;&#21644;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20013;&#30340;&#24615;&#33021;&#25552;&#20379;&#20102;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#24120;&#36890;&#36807;&#35745;&#31639;&#19982;&#21442;&#32771;&#26631;&#27880;&#30340;&#30456;&#20284;&#24230;&#26469;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#26368;&#22823;&#21270;&#19982;&#23427;&#20204;&#30340;&#30456;&#20284;&#24230;&#36827;&#34892;&#35757;&#32451;&#12290;&#23588;&#20854;&#26159;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#65292;&#26631;&#27880;&#26159;&#20027;&#35266;&#30340;&#65292;&#24182;&#19988;&#20855;&#26377;&#20302;&#30340;&#35780;&#26631;&#32773;&#38388;&#21644;&#35780;&#26631;&#32773;&#20869;&#21487;&#38752;&#24615;&#12290;&#30001;&#20110;&#26631;&#27880;&#20165;&#21453;&#26144;&#19990;&#30028;&#30340;&#19968;&#31181;&#35299;&#37322;&#65292;&#21363;&#20351;&#27169;&#22411;&#36798;&#21040;&#20102;&#39640;&#30456;&#20284;&#24230;&#20998;&#25968;&#65292;&#36825;&#20063;&#21487;&#33021;&#23548;&#33268;&#27425;&#20248;&#39044;&#27979;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#29702;&#35770;&#19978;&#30340;PGT&#27010;&#24565;&#12290;PGT&#26631;&#35760;&#20102;&#19982;&#21442;&#32771;&#27880;&#37322;&#30340;&#30456;&#20284;&#24230;&#22686;&#21152;&#19981;&#20877;&#36716;&#21270;&#20026;&#26356;&#22909;&#30340;RWMP&#30340;&#28857;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35745;&#31639;&#35780;&#26631;&#32773;&#38388;&#21644;&#35780;&#26631;&#32773;&#20869;&#21487;&#38752;&#24615;&#26469;&#36817;&#20284;PGT&#30340;&#23450;&#37327;&#25216;&#26415;&#12290;&#26368;&#21518;&#65292;&#22238;&#39038;&#20102;&#22235;&#31867;PGT&#24863;&#30693;&#31574;&#30053;&#65292;&#20197;&#35780;&#20272;&#21644;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models are typically evaluated by computing similarity with reference annotations and trained by maximizing similarity with such. Especially in the biomedical domain, annotations are subjective and suffer from low inter- and intra-rater reliability. Since annotations only reflect one interpretation of the real world, this can lead to sub-optimal predictions even though the model achieves high similarity scores. Here, the theoretical concept of PGT is introduced. PGT marks the point beyond which an increase in similarity with the \emph{reference annotation} stops translating to better RWMP. Additionally, a quantitative technique to approximate PGT by computing inter- and intra-rater reliability is proposed. Finally, four categories of PGT-aware strategies to evaluate and improve model performance are reviewed.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;10,917&#31687;&#26032;&#38395;&#25991;&#31456;&#30340;&#22810;&#26631;&#31614;&#25968;&#25454;&#38598;&#65292;&#21487;&#29992;&#20110;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33258;&#21160;&#25353;&#20027;&#39064;&#23545;&#26032;&#38395;&#25991;&#31456;&#36827;&#34892;&#20998;&#31867;&#65292;&#23545;&#26032;&#38395;&#32467;&#26500;&#12289;&#20998;&#31867;&#21644;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#30340;&#30740;&#31350;&#20154;&#21592;&#38750;&#24120;&#26377;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2212.12061</link><description>&lt;p&gt;
MN-DS&#65306;&#26032;&#38395;&#25991;&#31456;&#23618;&#27425;&#20998;&#31867;&#30340;&#22810;&#26631;&#31614;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
MN-DS: A Multilabeled News Dataset for News Articles Hierarchical Classification. (arXiv:2212.12061v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.12061
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;10,917&#31687;&#26032;&#38395;&#25991;&#31456;&#30340;&#22810;&#26631;&#31614;&#25968;&#25454;&#38598;&#65292;&#21487;&#29992;&#20110;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33258;&#21160;&#25353;&#20027;&#39064;&#23545;&#26032;&#38395;&#25991;&#31456;&#36827;&#34892;&#20998;&#31867;&#65292;&#23545;&#26032;&#38395;&#32467;&#26500;&#12289;&#20998;&#31867;&#21644;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#30340;&#30740;&#31350;&#20154;&#21592;&#38750;&#24120;&#26377;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;10,917&#31687;&#26032;&#38395;&#25991;&#31456;&#65292;&#28085;&#30422;&#20102;&#20174;2019&#24180;1&#26376;1&#26085;&#21040;2019&#24180;12&#26376;31&#26085;&#30340;&#23618;&#27425;&#26032;&#38395;&#20998;&#31867;&#12290;&#25105;&#20204;&#26681;&#25454;17&#20010;&#19968;&#32423;&#31867;&#21035;&#21644;109&#20010;&#20108;&#32423;&#31867;&#21035;&#30340;&#23618;&#27425;&#20998;&#31867;&#25163;&#21160;&#26631;&#35760;&#20102;&#36825;&#20123;&#25991;&#31456;&#12290;&#35813;&#25968;&#25454;&#38598;&#21487;&#29992;&#20110;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#33258;&#21160;&#25353;&#20027;&#39064;&#20998;&#31867;&#26032;&#38395;&#25991;&#31456;&#12290;&#35813;&#25968;&#25454;&#38598;&#23545;&#20110;&#20174;&#20107;&#26032;&#38395;&#32467;&#26500;&#12289;&#20998;&#31867;&#21644;&#26681;&#25454;&#21457;&#24067;&#30340;&#26032;&#38395;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#30340;&#30740;&#31350;&#20154;&#21592;&#38750;&#24120;&#26377;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article presents a dataset of 10,917 news articles with hierarchical news categories collected between January 1st 2019, and December 31st 2019. We manually labelled the articles based on a hierarchical taxonomy with 17 first-level and 109 second-level categories. This dataset can be used to train machine learning models for automatically classifying news articles by topic. This dataset can be helpful for researchers working on news structuring, classification, and predicting future events based on released news.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#30740;&#31350;&#20102;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#30340;&#26657;&#20934;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#8212;&#8212;&#36873;&#25321;&#24615;&#32553;&#25918;&#65292;&#36890;&#36807;&#23558;&#27491;&#30830;/&#38169;&#35823;&#39044;&#27979;&#20998;&#24320;&#36827;&#34892;&#32553;&#25918;&#65292;&#24182;&#26356;&#21152;&#20851;&#27880;&#38169;&#35823;&#39044;&#27979;&#30340;&#36923;&#36753;&#24179;&#28369;&#65292;&#27492;&#26041;&#27861;&#22312;&#35821;&#20041;&#20998;&#21106;&#26657;&#20934;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2212.12053</link><description>&lt;p&gt;
&#20851;&#20110;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#30340;&#26657;&#20934;&#65306;&#20998;&#26512;&#19982;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
On Calibrating Semantic Segmentation Models: Analyses and An Algorithm. (arXiv:2212.12053v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.12053
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#30740;&#31350;&#20102;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#30340;&#26657;&#20934;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#8212;&#8212;&#36873;&#25321;&#24615;&#32553;&#25918;&#65292;&#36890;&#36807;&#23558;&#27491;&#30830;/&#38169;&#35823;&#39044;&#27979;&#20998;&#24320;&#36827;&#34892;&#32553;&#25918;&#65292;&#24182;&#26356;&#21152;&#20851;&#27880;&#38169;&#35823;&#39044;&#27979;&#30340;&#36923;&#36753;&#24179;&#28369;&#65292;&#27492;&#26041;&#27861;&#22312;&#35821;&#20041;&#20998;&#21106;&#26657;&#20934;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#35821;&#20041;&#20998;&#21106;&#26657;&#20934;&#30340;&#38382;&#39064;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#24456;&#22810;&#35299;&#20915;&#26041;&#26696;&#26469;&#22788;&#29702;&#22270;&#20687;&#20998;&#31867;&#32622;&#20449;&#24230;&#30340;&#27169;&#22411;&#35823;&#26657;&#20934;&#65292;&#20294;&#33267;&#20170;&#20026;&#27490;&#65292;&#23545;&#35821;&#20041;&#20998;&#21106;&#30340;&#32622;&#20449;&#24230;&#26657;&#20934;&#30740;&#31350;&#20173;&#28982;&#24456;&#26377;&#38480;&#12290;&#25105;&#20204;&#23545;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#30340;&#26657;&#20934;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21457;&#29616;&#27169;&#22411;&#23481;&#37327;&#12289;&#35009;&#21098;&#22823;&#23567;&#12289;&#22810;&#23610;&#24230;&#27979;&#35797;&#21644;&#39044;&#27979;&#27491;&#30830;&#24615;&#23545;&#26657;&#20934;&#26377;&#24433;&#21709;&#12290;&#20854;&#20013;&#65292;&#39044;&#27979;&#27491;&#30830;&#24615;&#65292;&#29305;&#21035;&#26159;&#38169;&#35823;&#39044;&#27979;&#65292;&#23545;&#30001;&#20110;&#36807;&#24230;&#32622;&#20449;&#32780;&#23548;&#33268;&#30340;&#35823;&#26657;&#20934;&#26356;&#20026;&#37325;&#35201;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#12289;&#32479;&#19968;&#19988;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21363;&#36873;&#25321;&#24615;&#32553;&#25918;&#65292;&#36890;&#36807;&#23558;&#27491;&#30830;/&#38169;&#35823;&#39044;&#27979;&#20998;&#24320;&#36827;&#34892;&#32553;&#25918;&#65292;&#24182;&#26356;&#21152;&#20851;&#27880;&#38169;&#35823;&#39044;&#27979;&#30340;&#36923;&#36753;&#24179;&#28369;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#27969;&#34892;&#30340;&#29616;&#26377;&#26657;&#20934;&#26041;&#27861;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#36873;&#25321;&#24615;&#32553;&#25918;&#22312;&#35821;&#20041;&#20998;&#21106;&#26657;&#20934;&#19978;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#20351;&#29992;&#20102;&#22810;&#31181;&#25968;&#25454;&#38598;&#21644;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#65292;&#20197;&#39564;&#35777;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of semantic segmentation calibration. Lots of solutions have been proposed to approach model miscalibration of confidence in image classification. However, to date, confidence calibration research on semantic segmentation is still limited. We provide a systematic study on the calibration of semantic segmentation models and propose a simple yet effective approach. First, we find that model capacity, crop size, multi-scale testing, and prediction correctness have impact on calibration. Among them, prediction correctness, especially misprediction, is more important to miscalibration due to over-confidence. Next, we propose a simple, unifying, and effective approach, namely selective scaling, by separating correct/incorrect prediction for scaling and more focusing on misprediction logit smoothing. Then, we study popular existing calibration methods and compare them with selective scaling on semantic segmentation calibration. We conduct extensive experiments with a vari
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#26469;&#32771;&#34385;&#24322;&#26500;&#20256;&#24863;&#22120;&#23545;&#20154;&#20307;&#36816;&#21160;&#39044;&#27979;&#30340;&#24433;&#21709;&#65292;&#24182;&#25104;&#21151;&#23558;&#20854;&#24212;&#29992;&#20110;&#19981;&#21516;&#20256;&#24863;&#22120;&#24773;&#22659;&#19979;&#30340;&#23569;&#26679;&#26412;&#26377;&#38480;&#36816;&#21160;&#20219;&#21153;&#20013;&#12290;</title><link>http://arxiv.org/abs/2212.11771</link><description>&lt;p&gt;
&#24322;&#26500;&#20256;&#24863;&#22120;&#24773;&#22659;&#19979;&#30340;&#23569;&#26679;&#26412;&#20154;&#20307;&#36816;&#21160;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Few-shot human motion prediction for heterogeneous sensors. (arXiv:2212.11771v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.11771
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#26469;&#32771;&#34385;&#24322;&#26500;&#20256;&#24863;&#22120;&#23545;&#20154;&#20307;&#36816;&#21160;&#39044;&#27979;&#30340;&#24433;&#21709;&#65292;&#24182;&#25104;&#21151;&#23558;&#20854;&#24212;&#29992;&#20110;&#19981;&#21516;&#20256;&#24863;&#22120;&#24773;&#22659;&#19979;&#30340;&#23569;&#26679;&#26412;&#26377;&#38480;&#36816;&#21160;&#20219;&#21153;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20307;&#36816;&#21160;&#39044;&#27979;&#26159;&#19968;&#39033;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#28041;&#21450;&#22312;&#36830;&#25509;&#30340;&#20256;&#24863;&#22120;&#22270;&#20013;&#39044;&#27979;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#21464;&#37327;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#25193;&#23637;&#20855;&#26377;&#24322;&#26500;&#23646;&#24615;&#30340;&#23569;&#26679;&#26412;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#65292;&#26469;&#26126;&#30830;&#32435;&#20837;&#31354;&#38388;&#22270;&#65292;&#21516;&#26102;&#36328;&#36234;&#20855;&#26377;&#24322;&#26500;&#20256;&#24863;&#22120;&#30340;&#20004;&#20010;&#36816;&#21160;&#20219;&#21153;&#20013;&#36827;&#34892;&#39044;&#27979;&#26102;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human motion prediction is a complex task as it involves forecasting variables over time on a graph of connected sensors. This is especially true in the case of few-shot learning, where we strive to forecast motion sequences for previously unseen actions based on only a few examples. Despite this, almost all related approaches for few-shot motion prediction do not incorporate the underlying graph, while it is a common component in classical motion prediction. Furthermore, state-of-the-art methods for few-shot motion prediction are restricted to motion tasks with a fixed output space meaning these tasks are all limited to the same sensor graph. In this work, we propose to extend recent works on few-shot time-series forecasting with heterogeneous attributes with graph neural networks to introduce the first few-shot motion approach that explicitly incorporates the spatial graph while also generalizing across motion tasks with heterogeneous sensors. In our experiments on motion tasks with 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22797;&#26434;&#39550;&#39542;&#22330;&#26223;&#19979;&#21487;&#33021;&#23384;&#22312;&#30340;&#25439;&#22833;&#20849;&#20139;&#29305;&#24449;&#30340;&#24773;&#20917;&#23545;&#36710;&#36742;&#21327;&#21516;&#24863;&#30693;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#36890;&#20449;&#24863;&#30693;&#29305;&#24449;&#34701;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#25439;&#22833;&#36890;&#20449;&#24863;&#30693;&#20462;&#22797;&#32593;&#32476;&#32531;&#35299;&#20102;&#25439;&#22833;&#36890;&#20449;&#30340;&#21103;&#20316;&#29992;&#24182;&#22686;&#24378;&#20102;&#24863;&#30693;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.08273</link><description>&lt;p&gt;
&#25439;&#22833;&#36890;&#20449;&#19979;&#36710;&#36742;&#20043;&#38388;&#21327;&#21516;&#24863;&#30693;&#30340;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning for Vehicle-to-Vehicle Cooperative Perception under Lossy Communication. (arXiv:2212.08273v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08273
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22797;&#26434;&#39550;&#39542;&#22330;&#26223;&#19979;&#21487;&#33021;&#23384;&#22312;&#30340;&#25439;&#22833;&#20849;&#20139;&#29305;&#24449;&#30340;&#24773;&#20917;&#23545;&#36710;&#36742;&#21327;&#21516;&#24863;&#30693;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#36890;&#20449;&#24863;&#30693;&#29305;&#24449;&#34701;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#25439;&#22833;&#36890;&#20449;&#24863;&#30693;&#20462;&#22797;&#32593;&#32476;&#32531;&#35299;&#20102;&#25439;&#22833;&#36890;&#20449;&#30340;&#21103;&#20316;&#29992;&#24182;&#22686;&#24378;&#20102;&#24863;&#30693;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#26234;&#33021;&#36710;&#36742;&#39550;&#39542;&#20013;&#30340;&#24863;&#30693;&#65288;&#20363;&#22914;3D&#29289;&#20307;&#26816;&#27979;&#65289;&#20013;&#12290;&#30001;&#20110;&#26377;&#30410;&#30340;&#36710;&#36742;&#38388;&#36890;&#20449;&#65288;V2V&#65289;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26469;&#33258;&#20854;&#20182;&#20195;&#29702;&#30340;&#29305;&#24449;&#21487;&#20197;&#20849;&#20139;&#32473;&#26412;&#36710;&#65292;&#20197;&#25552;&#39640;&#26412;&#36710;&#30340;&#24863;&#30693;&#33021;&#21147;&#12290;&#36825;&#34987;&#31216;&#20026;V2V&#30740;&#31350;&#20013;&#30340;&#21327;&#21516;&#24863;&#30693;&#65292;&#20854;&#31639;&#27861;&#26368;&#36817;&#24471;&#21040;&#20102;&#26174;&#33879;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#25152;&#26377;&#29616;&#26377;&#30340;&#21327;&#21516;&#24863;&#30693;&#31639;&#27861;&#37117;&#20551;&#23450;&#29702;&#24819;&#30340;V2V&#36890;&#20449;&#65292;&#32780;&#27809;&#26377;&#32771;&#34385;&#22797;&#26434;&#30340;&#29616;&#23454;&#39550;&#39542;&#22330;&#26223;&#20013;&#21487;&#33021;&#20986;&#29616;&#30340;&#25439;&#22833;&#20849;&#20139;&#29305;&#24449;&#30340;&#24773;&#20917;&#12290;&#26412;&#25991;&#39318;&#20808;&#30740;&#31350;&#20102;V2V&#21327;&#21516;&#24863;&#30693;&#20013;&#25439;&#22833;&#36890;&#20449;&#30340;&#21103;&#20316;&#29992;&#65288;&#20363;&#22914;&#26816;&#27979;&#24615;&#33021;&#19979;&#38477;&#65289;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20013;&#38388;&#25439;&#22833;&#36890;&#20449;&#24863;&#30693;&#29305;&#24449;&#34701;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#25439;&#22833;&#36890;&#20449;&#24863;&#30693;&#20462;&#22797;&#32593;&#32476;&#65288;LCRN&#65289;&#32531;&#35299;&#20102;&#25439;&#22833;&#36890;&#20449;&#30340;&#21103;&#20316;&#29992;&#24182;&#22686;&#24378;&#20102;&#24863;&#30693;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning has been widely used in the perception (e.g., 3D object detection) of intelligent vehicle driving. Due to the beneficial Vehicle-to-Vehicle (V2V) communication, the deep learning based features from other agents can be shared to the ego vehicle so as to improve the perception of the ego vehicle. It is named as Cooperative Perception in the V2V research, whose algorithms have been dramatically advanced recently. However, all the existing cooperative perception algorithms assume the ideal V2V communication without considering the possible lossy shared features because of the Lossy Communication (LC) which is common in the complex real-world driving scenarios. In this paper, we first study the side effect (e.g., detection performance drop) by the lossy communication in the V2V Cooperative Perception, and then we propose a novel intermediate LC-aware feature fusion method to relieve the side effect of lossy communication by a LC-aware Repair Network (LCRN) and enhance the int
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22270;&#19978;&#36328;&#32593;&#32476;&#36801;&#31227;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#21033;&#29992;Weisfeiler-Lehman&#22270;&#21516;&#26500;&#27979;&#35797;&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#22270;&#23376;&#26641;&#24046;&#24322;&#24230;&#37327;&#31639;&#27861;&#26469;&#34913;&#37327;&#28304;&#30446;&#26631;&#20043;&#38388;&#30340;&#22270;&#20998;&#24067;&#24046;&#24322;&#65292;&#36827;&#32780;&#25512;&#23548;&#20986;&#36328;&#32593;&#32476;&#36801;&#31227;&#23398;&#20064;&#30340;&#19968;&#33324;&#21270;&#35823;&#24046;&#30028;&#38480;&#65292;&#21516;&#26102;&#36824;&#24320;&#21457;&#20102;GDSL&#31639;&#27861;&#29992;&#20110;&#23545;&#40784;&#22270;&#23376;&#26641;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2212.08174</link><description>&lt;p&gt;
&#22270;&#19978;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Non-IID Transfer Learning on Graphs. (arXiv:2212.08174v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08174
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22270;&#19978;&#36328;&#32593;&#32476;&#36801;&#31227;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#21033;&#29992;Weisfeiler-Lehman&#22270;&#21516;&#26500;&#27979;&#35797;&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#22270;&#23376;&#26641;&#24046;&#24322;&#24230;&#37327;&#31639;&#27861;&#26469;&#34913;&#37327;&#28304;&#30446;&#26631;&#20043;&#38388;&#30340;&#22270;&#20998;&#24067;&#24046;&#24322;&#65292;&#36827;&#32780;&#25512;&#23548;&#20986;&#36328;&#32593;&#32476;&#36801;&#31227;&#23398;&#20064;&#30340;&#19968;&#33324;&#21270;&#35823;&#24046;&#30028;&#38480;&#65292;&#21516;&#26102;&#36824;&#24320;&#21457;&#20102;GDSL&#31639;&#27861;&#29992;&#20110;&#23545;&#40784;&#22270;&#23376;&#26641;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36801;&#31227;&#23398;&#20064;&#26159;&#25351;&#20174;&#30456;&#20851;&#30340;&#28304;&#22495;&#21040;&#30446;&#26631;&#22495;&#36716;&#31227;&#30693;&#35782;&#25110;&#20449;&#24687;&#30340;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#36801;&#31227;&#23398;&#20064;&#29702;&#35770;&#21644;&#31639;&#27861;&#37117;&#38598;&#20013;&#22312;&#20551;&#35774;&#28304;/&#30446;&#26631;&#26679;&#26412;&#30456;&#20114;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;IID&#20219;&#21153;&#19978;&#12290;&#24456;&#23569;&#26377;&#30740;&#31350;&#19987;&#38376;&#38024;&#23545;&#22270;&#19978;&#30340;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#20219;&#21153;&#65292;&#20363;&#22914;&#36328;&#32593;&#32476;&#25366;&#25496;&#30340;&#30693;&#35782;&#36801;&#31227;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#20174;Weisfeiler-Lehman&#22270;&#21516;&#26500;&#27979;&#35797;&#30340;&#35282;&#24230;&#65292;&#25552;&#20986;&#20102;&#20005;&#26684;&#30340;&#19968;&#33324;&#21270;&#30028;&#38480;&#21644;&#31639;&#27861;&#65292;&#29992;&#20110;&#20174;&#28304;&#22270;&#21040;&#30446;&#26631;&#22270;&#30340;&#36328;&#32593;&#32476;&#36801;&#31227;&#23398;&#20064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#23376;&#26641;&#24046;&#24322;&#24230;&#37327;&#26041;&#27861;&#26469;&#34913;&#37327;&#28304;&#22270;&#21644;&#30446;&#26631;&#22270;&#20043;&#38388;&#30340;&#22270;&#20998;&#24067;&#24046;&#24322;&#65292;&#28982;&#21518;&#22312;&#27492;&#22522;&#30784;&#19978;&#25512;&#23548;&#20102;&#36328;&#32593;&#32476;&#36801;&#31227;&#23398;&#20064;&#30340;&#19968;&#33324;&#21270;&#35823;&#24046;&#30028;&#38480;&#65292;&#21253;&#25324;&#33410;&#28857;&#20998;&#31867;&#21644;&#38142;&#25509;&#39044;&#27979;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#31639;&#27861;GDSL&#65292;&#36890;&#36807;&#23545;&#40784;&#22270;&#23376;&#26641;&#21487;&#20197;&#26377;&#25928;&#22320;&#20943;&#23569;&#36328;&#32593;&#32476;&#20998;&#24067;&#20559;&#31227;&#12290;&#22810;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transfer learning refers to the transfer of knowledge or information from a relevant source domain to a target domain. However, most existing transfer learning theories and algorithms focus on IID tasks, where the source/target samples are assumed to be independent and identically distributed. Very little effort is devoted to theoretically studying the knowledge transferability on non-IID tasks, e.g., cross-network mining. To bridge the gap, in this paper, we propose rigorous generalization bounds and algorithms for cross-network transfer learning from a source graph to a target graph. The crucial idea is to characterize the cross-network knowledge transferability from the perspective of the Weisfeiler-Lehman graph isomorphism test. To this end, we propose a novel Graph Subtree Discrepancy to measure the graph distribution shift between source and target graphs. Then the generalization error bounds on cross-network transfer learning, including both cross-network node classification and
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;\textsc {NFS}&#30340;&#38750;&#31561;&#38388;&#38548;&#20613;&#37324;&#21494;PDE&#27714;&#35299;&#22120;&#65292;&#33021;&#22815;&#22312;&#22788;&#29702;&#23454;&#38469;&#31995;&#32479;&#30340;&#31354;&#38388;&#22495;&#20013;&#38750;&#31561;&#38388;&#38548;&#30340;&#25968;&#25454;&#24773;&#20917;&#19979;&#20445;&#25345;&#36739;&#39640;&#31934;&#24230;&#65292;&#24182;&#19988;&#20855;&#26377;&#32593;&#26684;&#19981;&#21464;&#25512;&#26029;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2212.04689</link><description>&lt;p&gt;
PDE&#30340;&#38750;&#31561;&#38388;&#38548;&#20613;&#37324;&#21494;&#31070;&#32463;&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
Non-equispaced Fourier Neural Solvers for PDEs. (arXiv:2212.04689v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.04689
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;\textsc {NFS}&#30340;&#38750;&#31561;&#38388;&#38548;&#20613;&#37324;&#21494;PDE&#27714;&#35299;&#22120;&#65292;&#33021;&#22815;&#22312;&#22788;&#29702;&#23454;&#38469;&#31995;&#32479;&#30340;&#31354;&#38388;&#22495;&#20013;&#38750;&#31561;&#38388;&#38548;&#30340;&#25968;&#25454;&#24773;&#20917;&#19979;&#20445;&#25345;&#36739;&#39640;&#31934;&#24230;&#65292;&#24182;&#19988;&#20855;&#26377;&#32593;&#26684;&#19981;&#21464;&#25512;&#26029;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#26159;&#22256;&#38590;&#30340;&#12290;&#26368;&#36817;&#25552;&#20986;&#20102;&#31070;&#32463;&#20998;&#36776;&#29575;&#19981;&#21464;&#27169;&#22411;&#65292;&#23613;&#31649;&#23427;&#20204;&#26377;&#25928;&#21644;&#39640;&#25928;&#65292;&#20294;&#36890;&#24120;&#38656;&#35201;&#31561;&#38388;&#38548;&#30340;&#25968;&#25454;&#28857;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#31995;&#32479;&#30340;&#31354;&#38388;&#22495;&#20013;&#37319;&#26679;&#26377;&#26102;&#19981;&#21487;&#36991;&#20813;&#22320;&#26159;&#38750;&#31561;&#38388;&#38548;&#30340;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#36866;&#29992;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#33258;&#36866;&#24212;&#25554;&#20540;&#30340;&#38750;&#31561;&#38388;&#38548;&#20613;&#37324;&#21494;PDE&#27714;&#35299;&#22120;&#65288;\textsc {NFS}&#65289;&#21644;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#30340;&#21464;&#20307;&#20316;&#20026;&#20854;&#32452;&#25104;&#37096;&#20998;&#12290;&#23545;&#22797;&#26434;PDE&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23427;&#22312;&#31934;&#24230;&#21644;&#25928;&#29575;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;&#19982;&#31354;&#38388;&#31561;&#38388;&#38548;&#22522;&#20934;&#26041;&#27861;&#30456;&#27604;&#65292;&#23427;&#22312;MAE&#19978;&#21462;&#24471;&#20102;$42.85\%$&#30340;&#25913;&#36827;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#30053;&#26377;&#31934;&#24230;&#25439;&#22833;&#30340;&#24773;&#20917;&#19979;&#22788;&#29702;&#38750;&#31561;&#38388;&#38548;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;\textsc{NFS}&#26159;&#31532;&#19968;&#20010;&#20855;&#26377;&#32593;&#26684;&#19981;&#21464;&#25512;&#26029;&#33021;&#21147;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#22312;&#38750;&#31561;&#38388;&#38548;&#22330;&#26223;&#19979;&#25104;&#21151;&#27169;&#25311;&#28237;&#27969;&#27969;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solving partial differential equations is difficult. Recently proposed neural resolution-invariant models, despite their effectiveness and efficiency, usually require equispaced spatial points of data. However, sampling in spatial domain is sometimes inevitably non-equispaced in real-world systems, limiting their applicability. In this paper, we propose a Non-equispaced Fourier PDE Solver (\textsc{NFS}) with adaptive interpolation on resampled equispaced points and a variant of Fourier Neural Operators as its components. Experimental results on complex PDEs demonstrate its advantages in accuracy and efficiency. Compared with the spatially-equispaced benchmark methods, it achieves superior performance with $42.85\%$ improvements on MAE, and is able to handle non-equispaced data with a tiny loss of accuracy. Besides, to our best knowledge, \textsc{NFS} is the first ML-based method with mesh invariant inference ability to successfully model turbulent flows in non-equispaced scenarios, wit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; DRIP &#31639;&#27861;&#65292;&#20351;&#29992;&#22810;&#38754;&#20307;&#22495;&#31934;&#28860;&#36845;&#20195;&#26469;&#25552;&#39640;&#31070;&#32463;&#22238;&#36335;&#21453;&#21521;&#21487;&#36798;&#24615;&#20998;&#26512;&#30340;&#31934;&#24230;&#65292;&#36827;&#19968;&#27493;&#25910;&#32039;&#20102;&#21453;&#25237;&#24433;&#38598;&#21512;&#30340;&#36793;&#32536;&#12290;&#21516;&#26102;&#65292;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#20844;&#24335;&#26469;&#34920;&#31034;&#22810;&#38754;&#20307;&#36793;&#32536;&#65292;&#20197;&#21152;&#24378;&#36793;&#32536;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2212.04646</link><description>&lt;p&gt;
DRIP: &#22810;&#38754;&#20307;&#22495;&#31934;&#28860;&#36845;&#20195;&#29992;&#20110;&#31070;&#32463;&#22238;&#36335;&#21453;&#21521;&#21487;&#36798;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
DRIP: Domain Refinement Iteration with Polytopes for Backward Reachability Analysis of Neural Feedback Loops. (arXiv:2212.04646v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.04646
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; DRIP &#31639;&#27861;&#65292;&#20351;&#29992;&#22810;&#38754;&#20307;&#22495;&#31934;&#28860;&#36845;&#20195;&#26469;&#25552;&#39640;&#31070;&#32463;&#22238;&#36335;&#21453;&#21521;&#21487;&#36798;&#24615;&#20998;&#26512;&#30340;&#31934;&#24230;&#65292;&#36827;&#19968;&#27493;&#25910;&#32039;&#20102;&#21453;&#25237;&#24433;&#38598;&#21512;&#30340;&#36793;&#32536;&#12290;&#21516;&#26102;&#65292;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#20844;&#24335;&#26469;&#34920;&#31034;&#22810;&#38754;&#20307;&#36793;&#32536;&#65292;&#20197;&#21152;&#24378;&#36793;&#32536;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#25511;&#21046;&#25216;&#26415;&#30340;&#23433;&#20840;&#35748;&#35777;&#20173;&#28982;&#26159;&#19968;&#20010;&#20027;&#35201;&#30340;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;&#26412;&#25991;&#30740;&#31350;&#36870;&#21521;&#21487;&#36798;&#24615;&#20316;&#20026;&#19968;&#20010;&#26694;&#26550;&#65292;&#20026;&#34987;&#31070;&#32463;&#32593;&#32476; (NN) &#31574;&#30053;&#25511;&#21046;&#30340;&#31995;&#32479;&#25552;&#20379;&#36991;&#30896;&#20445;&#35777;&#12290;&#30001;&#20110; NN &#36890;&#24120;&#19981;&#21487;&#36870;&#65292;&#29616;&#26377;&#26041;&#27861;&#20445;&#23432;&#22320;&#20551;&#23450;&#19968;&#20010;&#22495;&#26469;&#26494;&#24347; NN&#65292;&#36825;&#23548;&#33268;&#29366;&#24577;&#38598;&#30340;&#36807;&#24230;&#36924;&#36817;&#65292;&#21487;&#33021;&#23548;&#33268;&#31995;&#32479;&#36827;&#20837;&#38556;&#30861;&#29289; (&#21363;&#21453;&#25237;&#24433; (BP) &#38598;)&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837; DRIP&#65292;&#19968;&#20010;&#22312;&#26494;&#24347;&#22495;&#19978;&#36827;&#34892;&#31934;&#21270;&#24490;&#29615;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#22823;&#22823;&#25910;&#32039; BP &#38598;&#30340;&#36793;&#30028;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#19968;&#20010;&#20844;&#24335;&#65292;&#21487;&#20197;&#30452;&#25509;&#33719;&#24471;&#29992;&#20110;&#38480;&#21046; BP &#38598;&#30340;&#22810;&#38754;&#20307;&#38381;&#21512;&#24418;&#24335;&#34920;&#31034;&#65292;&#27604;&#20197;&#21069;&#38656;&#35201;&#27714;&#35299;&#32447;&#24615;&#35268;&#21010;&#21644;&#20351;&#29992;&#36229;&#30697;&#24418;&#30340;&#26041;&#27861;&#26356;&#20026;&#32039;&#20945;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#23558; NN &#26494;&#24347;&#31639;&#27861;&#25193;&#23637;&#21040;&#22788;&#29702;&#22810;&#38754;&#20307;&#22495;&#65292;&#36827;&#19968;&#27493;&#25910;&#32039;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
Safety certification of data-driven control techniques remains a major open problem. This work investigates backward reachability as a framework for providing collision avoidance guarantees for systems controlled by neural network (NN) policies. Because NNs are typically not invertible, existing methods conservatively assume a domain over which to relax the NN, which causes loose over-approximations of the set of states that could lead the system into the obstacle (i.e., backprojection (BP) sets). To address this issue, we introduce DRIP, an algorithm with a refinement loop on the relaxation domain, which substantially tightens the BP set bounds. Furthermore, we introduce a formulation that enables directly obtaining closed-form representations of polytopes to bound the BP sets tighter than prior work, which required solving linear programs and using hyper-rectangles. Furthermore, this work extends the NN relaxation algorithm to handle polytope domains, which further tightens the bound
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;LLM-Planner&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#23454;&#20307;&#20195;&#29702;&#36827;&#34892;&#23569;&#26679;&#26412;&#35268;&#21010;&#65292;&#20197;&#23454;&#20307;&#20195;&#29702;&#30446;&#21069;&#25152;&#22312;&#30340;&#29615;&#22659;&#20026;&#22522;&#30784;&#65292;&#22686;&#24378;LLMs&#29983;&#25104;&#21644;&#26356;&#26032;&#35745;&#21010;&#65292;&#23454;&#39564;&#34920;&#26126;&#20854;&#22312;&#22810;&#20219;&#21153;&#21644;&#24555;&#36895;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#36890;&#29992;&#20195;&#29702;&#30340;&#24320;&#21457;&#20013;&#20855;&#26377;&#24456;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2212.04088</link><description>&lt;p&gt;
LLM-Planner: &#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23569;&#26679;&#26412;&#23454;&#20307;&#20195;&#29702;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models. (arXiv:2212.04088v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.04088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;LLM-Planner&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#23454;&#20307;&#20195;&#29702;&#36827;&#34892;&#23569;&#26679;&#26412;&#35268;&#21010;&#65292;&#20197;&#23454;&#20307;&#20195;&#29702;&#30446;&#21069;&#25152;&#22312;&#30340;&#29615;&#22659;&#20026;&#22522;&#30784;&#65292;&#22686;&#24378;LLMs&#29983;&#25104;&#21644;&#26356;&#26032;&#35745;&#21010;&#65292;&#23454;&#39564;&#34920;&#26126;&#20854;&#22312;&#22810;&#20219;&#21153;&#21644;&#24555;&#36895;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#36890;&#29992;&#20195;&#29702;&#30340;&#24320;&#21457;&#20013;&#20855;&#26377;&#24456;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20851;&#27880;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#35268;&#21010;&#22120;&#65292;&#35753;&#23454;&#20307;&#20195;&#29702;&#21487;&#20197;&#25353;&#29031;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#23436;&#25104;&#22312;&#35270;&#35273;&#24863;&#30693;&#29615;&#22659;&#20013;&#30340;&#22797;&#26434;&#20219;&#21153;&#12290;&#29616;&#26377;&#26041;&#27861;&#30340;&#39640;&#25968;&#25454;&#25104;&#26412;&#21644;&#20302;&#26679;&#26412;&#25928;&#29575;&#38459;&#30861;&#20102;&#22810;&#20219;&#21153;&#21644;&#24555;&#36895;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#36890;&#29992;&#20195;&#29702;&#30340;&#24320;&#21457;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;LLM-Planner&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#23454;&#20307;&#20195;&#29702;&#36827;&#34892;&#23569;&#26679;&#26412;&#35268;&#21010;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#20307;&#20195;&#29702;&#30446;&#21069;&#25152;&#22312;&#30340;&#29615;&#22659;&#20026;&#22522;&#30784;&#65292;&#22686;&#24378;LLMs&#29983;&#25104;&#21644;&#26356;&#26032;&#35745;&#21010;&#12290;&#22312;ALFRED&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#21462;&#24471;&#38750;&#24120;&#26377;&#31454;&#20105;&#21147;&#30340;&#23569;&#26679;&#26412;&#24615;&#33021;&#65306;&#23613;&#31649;&#20351;&#29992;&#30340;&#37197;&#23545;&#35757;&#32451;&#25968;&#25454;&#19981;&#21040;0.5&#65285;&#65292;LLM-Planner&#30340;&#34920;&#29616;&#19982;&#20351;&#29992;&#23436;&#25972;&#35757;&#32451;&#25968;&#25454;&#35757;&#32451;&#30340;&#26368;&#26032;&#22522;&#32447;&#30456;&#24403;&#12290;&#29616;&#26377;&#26041;&#27861;&#20960;&#20046;&#26080;&#27861;&#23436;&#25104;&#20219;&#20309;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study focuses on using large language models (LLMs) as a planner for embodied agents that can follow natural language instructions to complete complex tasks in a visually-perceived environment. The high data cost and poor sample efficiency of existing methods hinders the development of versatile agents that are capable of many tasks and can learn new tasks quickly. In this work, we propose a novel method, LLM-Planner, that harnesses the power of large language models to do few-shot planning for embodied agents. We further propose a simple but effective way to enhance LLMs with physical grounding to generate and update plans that are grounded in the current environment. Experiments on the ALFRED dataset show that our method can achieve very competitive few-shot performance: Despite using less than 0.5% of paired training data, LLM-Planner achieves competitive performance with recent baselines that are trained using the full training data. Existing methods can barely complete any ta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37197;&#23545;&#20114;&#34917;&#26102;&#38388;&#24490;&#29615;&#19968;&#33268;&#23545;&#25239;&#32593;&#32476;&#30340;&#38647;&#36798;&#38477;&#27700;&#39044;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#29983;&#25104;&#22120;&#32593;&#32476;&#21644;&#24490;&#29615;&#19968;&#33268;&#24615;&#25439;&#22833;&#21644;&#23545;&#25239;&#24615;&#25439;&#22833;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#25512;&#24191;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#25216;&#26415;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2211.15046</link><description>&lt;p&gt;
&#22522;&#20110;&#37197;&#23545;&#20114;&#34917;&#26102;&#38388;&#24490;&#29615;&#19968;&#33268;&#23545;&#25239;&#32593;&#32476;&#30340;&#38647;&#36798;&#38477;&#27700;&#39044;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
PCT-CycleGAN: Paired Complementary Temporal Cycle-Consistent Adversarial Networks for Radar-Based Precipitation Nowcasting. (arXiv:2211.15046v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15046
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37197;&#23545;&#20114;&#34917;&#26102;&#38388;&#24490;&#29615;&#19968;&#33268;&#23545;&#25239;&#32593;&#32476;&#30340;&#38647;&#36798;&#38477;&#27700;&#39044;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#29983;&#25104;&#22120;&#32593;&#32476;&#21644;&#24490;&#29615;&#19968;&#33268;&#24615;&#25439;&#22833;&#21644;&#23545;&#25239;&#24615;&#25439;&#22833;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#25512;&#24191;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#25216;&#26415;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38477;&#27700;&#39044;&#27979;&#26041;&#27861;&#24050;&#32463;&#22312;&#36807;&#21435;&#20960;&#20010;&#19990;&#32426;&#37324;&#24471;&#21040;&#20102;&#24456;&#22909;&#30340;&#21457;&#23637;&#65292;&#22240;&#20026;&#38632;&#27700;&#23545;&#20154;&#31867;&#29983;&#27963;&#26377;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#24433;&#21709;&#12290;&#29616;&#26377;&#30340;&#38477;&#27700;&#39044;&#27979;&#27169;&#22411;&#21253;&#25324;&#23450;&#37327;&#38477;&#27700;&#39044;&#27979; (QPF) &#27169;&#22411;&#12289;&#21367;&#31215;&#38271;&#30701;&#26399;&#35760;&#24518; (ConvLSTM) &#27169;&#22411;&#20197;&#21450;&#26368;&#26032;&#30340; MetNet-2 &#31561;&#22810;&#31181;&#22797;&#26434;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#37197;&#23545;&#20114;&#34917;&#26102;&#38388;&#24490;&#29615;&#19968;&#33268;&#23545;&#25239;&#32593;&#32476; (PCT-CycleGAN) &#30340;&#38647;&#36798;&#38477;&#27700;&#39044;&#27979;&#26041;&#27861;&#65292;&#21463;&#23545;&#25239;&#29983;&#25104;&#32593;&#32476; (CycleGAN) &#24378;&#22823;&#30340;&#22270;&#20687;&#36716;&#25442;&#24615;&#33021;&#21551;&#21457;&#12290;PCT-CycleGAN &#20351;&#29992;&#20004;&#20010;&#20855;&#26377;&#21521;&#21069;&#21644;&#21521;&#21518;&#26102;&#38388;&#21160;&#24577;&#30340;&#29983;&#25104;&#22120;&#32593;&#32476;&#29983;&#25104;&#26102;&#24207;&#24615;&#65292;&#27599;&#20010;&#29983;&#25104;&#22120;&#32593;&#32476;&#23398;&#20064;&#19968;&#20010;&#24222;&#22823;&#30340;&#19968;&#23545;&#19968;&#26144;&#23556;&#65292;&#20197;&#36924;&#36817;&#34920;&#31034;&#27599;&#20010;&#26041;&#21521;&#19978;&#30340;&#26102;&#38388;&#21160;&#24577;&#30340;&#26144;&#23556;&#20989;&#25968;&#12290;&#20026;&#20102;&#21019;&#24314;&#37197;&#23545;&#20114;&#34917;&#24490;&#29615;&#20043;&#38388;&#30340;&#24378;&#20581;&#26102;&#38388;&#22240;&#26524;&#20851;&#31995;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#24490;&#29615;&#19968;&#33268;&#24615;&#25439;&#22833;&#21644;&#23545;&#25239;&#24615;&#25439;&#22833;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;PCT-CycleGAN &#22312;&#20934;&#30830;&#24615;&#21644;&#25512;&#24191;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#25216;&#26415;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The precipitation nowcasting methods have been elaborated over the centuries because rain has a crucial impact on human life. Not only quantitative precipitation forecast (QPF) models and convolutional long short-term memory (ConvLSTM), but also various sophisticated methods such as the latest MetNet-2 are emerging. In this paper, we propose a paired complementary temporal cycle-consistent adversarial networks (PCT-CycleGAN) for radar-based precipitation nowcasting, inspired by cycle-consistent adversarial networks (CycleGAN), which shows strong performance in image-to-image translation. PCT-CycleGAN generates temporal causality using two generator networks with forward and backward temporal dynamics in paired complementary cycles. Each generator network learns a huge number of one-to-one mappings about time-dependent radar-based precipitation data to approximate a mapping function representing the temporal dynamics in each direction. To create robust temporal causality between paired 
&lt;/p&gt;</description></item><item><title>SpaText&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#24320;&#25918;&#35789;&#27719;&#22330;&#26223;&#20197;&#21450;&#22522;&#20110;CLIP&#30340;&#31354;&#38388;&#25991;&#26412;&#34920;&#31034;&#36827;&#34892;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#65292;&#33021;&#22815;&#23454;&#29616;&#23545;&#19981;&#21516;&#21306;&#22495;/&#23545;&#35937;&#30340;&#24418;&#29366;&#25110;&#24067;&#23616;&#30340;&#32454;&#31890;&#24230;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2211.14305</link><description>&lt;p&gt;
SpaText: &#22522;&#20110;&#31354;&#38388;&#25991;&#26412;&#34920;&#31034;&#30340;&#21487;&#25511;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
SpaText: Spatio-Textual Representation for Controllable Image Generation. (arXiv:2211.14305v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14305
&lt;/p&gt;
&lt;p&gt;
SpaText&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#24320;&#25918;&#35789;&#27719;&#22330;&#26223;&#20197;&#21450;&#22522;&#20110;CLIP&#30340;&#31354;&#38388;&#25991;&#26412;&#34920;&#31034;&#36827;&#34892;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#65292;&#33021;&#22815;&#23454;&#29616;&#23545;&#19981;&#21516;&#21306;&#22495;/&#23545;&#35937;&#30340;&#24418;&#29366;&#25110;&#24067;&#23616;&#30340;&#32454;&#31890;&#24230;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#21069;&#25152;&#26410;&#26377;&#30340;&#20196;&#20154;&#20449;&#26381;&#30340;&#39640;&#36136;&#37327;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#20960;&#20046;&#19981;&#21487;&#33021;&#20197;&#32454;&#31890;&#24230;&#30340;&#26041;&#24335;&#25511;&#21046;&#19981;&#21516;&#21306;&#22495;/&#23545;&#35937;&#30340;&#24418;&#29366;&#25110;&#20854;&#24067;&#23616;&#12290;&#20197;&#24448;&#35797;&#22270;&#25552;&#20379;&#27492;&#31867;&#25511;&#21046;&#30340;&#23581;&#35797;&#21463;&#21040;&#20854;&#20381;&#36182;&#20110;&#22266;&#23450;&#26631;&#31614;&#38598;&#30340;&#38480;&#21046;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SpaText-&#19968;&#31181;&#21033;&#29992;&#24320;&#25918;&#35789;&#27719;&#22330;&#26223;&#25511;&#21046;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#26032;&#26041;&#27861;&#12290;&#38500;&#20102;&#25551;&#36848;&#25972;&#20010;&#22330;&#26223;&#30340;&#20840;&#23616;&#25991;&#26412;&#25552;&#31034;&#22806;&#65292;&#29992;&#25143;&#36824;&#25552;&#20379;&#19968;&#20010;&#20998;&#21106;&#22320;&#22270;&#65292;&#20854;&#20013;&#27599;&#20010;&#24863;&#20852;&#36259;&#21306;&#22495;&#37117;&#30001;&#33258;&#30001;&#24418;&#24335;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#36827;&#34892;&#27880;&#37322;&#12290;&#30001;&#20110;&#32570;&#20047;&#20855;&#26377;&#35814;&#32454;&#25991;&#26412;&#25551;&#36848;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#20197;&#25551;&#36848;&#22270;&#20687;&#20013;&#27599;&#20010;&#21306;&#22495;&#65292;&#25105;&#20204;&#36873;&#25321;&#21033;&#29992;&#24403;&#21069;&#30340;&#22823;&#35268;&#27169;&#25991;&#26412;&#21040;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#24182;&#22522;&#20110;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;CLIP&#30340;&#31354;&#38388;&#25991;&#26412;&#34920;&#31034;&#26469;&#23454;&#29616;&#65292;&#21516;&#26102;&#23637;&#31034;&#20854;&#22312;&#22522;&#20110;&#20687;&#32032;&#21644;&#38544;&#21464;&#37327;&#30340;&#20004;&#20010;&#26368;&#20808;&#36827;&#30340;&#25193;&#25955;&#27169;&#22411;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent text-to-image diffusion models are able to generate convincing results of unprecedented quality. However, it is nearly impossible to control the shapes of different regions/objects or their layout in a fine-grained fashion. Previous attempts to provide such controls were hindered by their reliance on a fixed set of labels. To this end, we present SpaText - a new method for text-to-image generation using open-vocabulary scene control. In addition to a global text prompt that describes the entire scene, the user provides a segmentation map where each region of interest is annotated by a free-form natural language description. Due to lack of large-scale datasets that have a detailed textual description for each region in the image, we choose to leverage the current large-scale text-to-image datasets and base our approach on a novel CLIP-based spatio-textual representation, and show its effectiveness on two state-of-the-art diffusion models: pixel-based and latent-based. In addition
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#22238;&#24402;&#20809;&#22330;&#21453;&#28436;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#21333;&#20010;&#22270;&#20687;&#20013;&#24674;&#22797; SDF &#21442;&#25968;&#21270;&#30340; 3D &#24418;&#29366;&#12289;&#23039;&#24577;&#21644;&#22806;&#35266;&#65292;&#32780;&#26080;&#38656;&#20934;&#30830;&#30340;&#30495;&#23454;&#23039;&#21183;&#65292;&#19988;&#36895;&#24230;&#24555;&#12289;&#35745;&#31639;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2211.11674</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#22238;&#24402;&#20809;&#22330;&#21453;&#28436;&#20174;&#21333;&#24352;&#22270;&#20687;&#20013;&#37325;&#26500;&#24418;&#29366;&#12289;&#23039;&#24577;&#21644;&#22806;&#35266;
&lt;/p&gt;
&lt;p&gt;
Shape, Pose, and Appearance from a Single Image via Bootstrapped Radiance Field Inversion. (arXiv:2211.11674v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11674
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#22238;&#24402;&#20809;&#22330;&#21453;&#28436;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#21333;&#20010;&#22270;&#20687;&#20013;&#24674;&#22797; SDF &#21442;&#25968;&#21270;&#30340; 3D &#24418;&#29366;&#12289;&#23039;&#24577;&#21644;&#22806;&#35266;&#65292;&#32780;&#26080;&#38656;&#20934;&#30830;&#30340;&#30495;&#23454;&#23039;&#21183;&#65292;&#19988;&#36895;&#24230;&#24555;&#12289;&#35745;&#31639;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#21512;&#31070;&#32463;&#36752;&#23556;&#22330; (NeRF) &#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476; (GAN) &#22312;&#20174;&#21333;&#35270;&#35282;&#36827;&#34892;&#19977;&#32500;&#37325;&#24314;&#26041;&#38754;&#23637;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#28508;&#21147;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#26377;&#25928;&#22320;&#24314;&#27169;&#20219;&#24847;&#25299;&#25169;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#35813;&#39046;&#22495;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#65292;&#20854;&#20013;&#30830;&#20999;&#30340;&#22320;&#38754;&#30495;&#23454;&#23039;&#21183;&#26159;&#24050;&#30693;&#30340;&#65292;&#24182;&#24573;&#30053;&#20102;&#23039;&#21183;&#20272;&#35745;&#65292;&#32780;&#23039;&#21183;&#20272;&#35745;&#26159;&#26576;&#20123;&#19979;&#28216;&#24212;&#29992;&#65292;&#22914;&#22686;&#24378;&#29616;&#23454; (AR) &#21644;&#26426;&#22120;&#20154;&#25216;&#26415;&#20013;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#28982;&#22270;&#20687;&#30340;&#26377;&#21407;&#21017;&#30340;&#31471;&#21040;&#31471;&#37325;&#24314;&#26694;&#26550;&#65292;&#20854;&#20013;&#20934;&#30830;&#30340;&#30495;&#23454;&#23039;&#21183;&#26159;&#19981;&#21487;&#29992;&#30340;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20174;&#19968;&#20010;&#29289;&#20307;&#30340;&#21333;&#20010;&#22270;&#20687;&#20013;&#24674;&#22797;&#20102; SDF &#21442;&#25968;&#21270;&#30340; 3D &#24418;&#29366;&#12289;&#23039;&#24577;&#21644;&#22806;&#35266;&#65292;&#32780;&#27809;&#26377;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21033;&#29992;&#22810;&#20010;&#35270;&#35282;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#19968;&#20010;&#26080;&#26465;&#20214;&#30340; 3D-aware &#29983;&#25104;&#22120;&#65292;&#23545;&#20854;&#24212;&#29992;&#20102;&#28151;&#21512;&#21453;&#28436;&#26041;&#26696;&#65292;&#20854;&#20013;&#19968;&#20010;&#27169;&#22411;&#20135;&#29983;&#20102;&#35299;&#30340;&#31532;&#19968;&#20010;&#29468;&#27979;&#65292;&#28982;&#21518;&#36890;&#36807;&#20248;&#21270;&#36827;&#34892;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#22312;&#19968;&#27425;&#21069;&#21521;&#20256;&#36882;&#20013;&#23545;&#22270;&#20687;&#36827;&#34892;&#21435;&#28210;&#26579;&#65292;&#22240;&#27492;&#36895;&#24230;&#24555;&#19988;&#35745;&#31639;&#26377;&#25928;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312; DTU &#21644; Tanks &amp; Temples &#31561;&#22522;&#20934;&#27979;&#35797;&#20013;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#25216;&#26415;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#30495;&#23454;&#19990;&#30028;&#22270;&#20687;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Radiance Fields (NeRF) coupled with GANs represent a promising direction in the area of 3D reconstruction from a single view, owing to their ability to efficiently model arbitrary topologies. Recent work in this area, however, has mostly focused on synthetic datasets where exact ground-truth poses are known, and has overlooked pose estimation, which is important for certain downstream applications such as augmented reality (AR) and robotics. We introduce a principled end-to-end reconstruction framework for natural images, where accurate ground-truth poses are not available. Our approach recovers an SDF-parameterized 3D shape, pose, and appearance from a single image of an object, without exploiting multiple views during training. More specifically, we leverage an unconditional 3D-aware generator, to which we apply a hybrid inversion scheme where a model produces a first guess of the solution which is then refined via optimization. Our framework can de-render an image in as few a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#32437;&#21521;&#32593;&#32476;&#20272;&#35745;&#26694;&#26550;&#65292;&#21033;&#29992;&#33258;&#36866;&#24212;&#21512;&#24182;&#12289;&#24352;&#37327;&#20998;&#35299;&#21644;&#28857;&#36807;&#31243;&#31561;&#26041;&#27861;&#26469;&#20943;&#23569;&#20272;&#35745;&#20559;&#24046;&#21644;&#26041;&#24046;&#12290;</title><link>http://arxiv.org/abs/2211.07866</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#21512;&#24182;&#19979;&#30340;&#32437;&#21521;&#32593;&#32476;&#26377;&#25928;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Efficient Estimation for Longitudinal Network via Adaptive Merging. (arXiv:2211.07866v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.07866
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#32437;&#21521;&#32593;&#32476;&#20272;&#35745;&#26694;&#26550;&#65292;&#21033;&#29992;&#33258;&#36866;&#24212;&#21512;&#24182;&#12289;&#24352;&#37327;&#20998;&#35299;&#21644;&#28857;&#36807;&#31243;&#31561;&#26041;&#27861;&#26469;&#20943;&#23569;&#20272;&#35745;&#20559;&#24046;&#21644;&#26041;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32437;&#21521;&#32593;&#32476;&#30001;&#22810;&#20010;&#33410;&#28857;&#20043;&#38388;&#30340;&#26102;&#38388;&#36793;&#24207;&#21015;&#32452;&#25104;&#65292;&#20854;&#20013;&#26102;&#38388;&#36793;&#22312;&#23454;&#26102;&#20013;&#34987;&#35266;&#23519;&#21040;&#12290;&#38543;&#30528;&#22312;&#32447;&#31038;&#20132;&#24179;&#21488;&#21644;&#30005;&#23376;&#21830;&#21153;&#30340;&#20852;&#36215;&#65292;&#23427;&#24050;&#32463;&#21464;&#24471;&#26222;&#36941;&#65292;&#20294;&#22312;&#25991;&#29486;&#20013;&#24448;&#24448;&#34987;&#24573;&#30053;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#32437;&#21521;&#32593;&#32476;&#20272;&#35745;&#26694;&#26550;&#65292;&#21033;&#29992;&#33258;&#36866;&#24212;&#32593;&#32476;&#21512;&#24182;&#12289;&#24352;&#37327;&#20998;&#35299;&#21644;&#28857;&#36807;&#31243;&#30340;&#20248;&#21183;&#12290;&#23427;&#21512;&#24182;&#30456;&#37051;&#30340;&#31232;&#30095;&#32593;&#32476;&#65292;&#20197;&#25193;&#22823;&#35266;&#27979;&#36793;&#30340;&#25968;&#37327;&#24182;&#20943;&#23569;&#20272;&#35745;&#26041;&#24046;&#65292;&#21516;&#26102;&#36890;&#36807;&#21033;&#29992;&#26412;&#22320;&#26102;&#38388;&#32467;&#26500;&#36827;&#34892;&#33258;&#36866;&#24212;&#32593;&#32476;&#37051;&#22495;&#25511;&#21046;&#24341;&#20837;&#30340;&#20272;&#35745;&#20559;&#24046;&#12290;&#25552;&#20986;&#20102;&#19968;&#20010;&#25237;&#24433;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#26469;&#20419;&#36827;&#20272;&#35745;&#65292;&#20854;&#20013;&#27599;&#27425;&#36845;&#20195;&#30340;&#20272;&#35745;&#38169;&#35823;&#19978;&#30028;&#34987;&#24314;&#31435;&#12290;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#20998;&#26512;&#65292;&#20197;&#37327;&#21270;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#28176;&#36817;&#34892;&#20026;&#65292;&#32467;&#26524;&#34920;&#26126;&#23427;&#21487;&#20197;&#26174;&#30528;&#20943;&#23569;&#20272;&#35745;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Longitudinal network consists of a sequence of temporal edges among multiple nodes, where the temporal edges are observed in real time. It has become ubiquitous with the rise of online social platform and e-commerce, but largely under-investigated in literature. In this paper, we propose an efficient estimation framework for longitudinal network, leveraging strengths of adaptive network merging, tensor decomposition and point process. It merges neighboring sparse networks so as to enlarge the number of observed edges and reduce estimation variance, whereas the estimation bias introduced by network merging is controlled by exploiting local temporal structures for adaptive network neighborhood. A projected gradient descent algorithm is proposed to facilitate estimation, where the upper bound of the estimation error in each iteration is established. A thorough analysis is conducted to quantify the asymptotic behavior of the proposed method, which shows that it can significantly reduce the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35299;&#20915;&#20102;&#22312;&#32447;&#21512;&#21516;&#35774;&#35745;&#20013;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#25351;&#25968;&#32423;&#30340;$m$&#20010;&#26679;&#26412;&#23601;&#36275;&#20197;&#23398;&#20064;&#19968;&#20010;&#36817;&#20046;&#26368;&#20248;&#30340;&#21512;&#21516;&#12290;</title><link>http://arxiv.org/abs/2211.05732</link><description>&lt;p&gt;
&#22312;&#32447;&#21512;&#21516;&#35774;&#35745;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;
&lt;/p&gt;
&lt;p&gt;
The Sample Complexity of Online Contract Design. (arXiv:2211.05732v2 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05732
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#22312;&#32447;&#21512;&#21516;&#35774;&#35745;&#20013;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#25351;&#25968;&#32423;&#30340;$m$&#20010;&#26679;&#26412;&#23601;&#36275;&#20197;&#23398;&#20064;&#19968;&#20010;&#36817;&#20046;&#26368;&#20248;&#30340;&#21512;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22312;&#32447;&#24773;&#22659;&#19979;&#30340;&#38544;&#34255;-&#34892;&#21160;&#22996;&#25176;&#38382;&#39064;&#12290;&#22312;&#27599;&#36718;&#20013;&#65292;&#22996;&#25176;&#20154;&#21457;&#24067;&#19968;&#20221;&#21512;&#21516;&#65292;&#26681;&#25454;&#27599;&#20010;&#32467;&#26524;&#35268;&#23450;&#20195;&#29702;&#20154;&#30340;&#25903;&#20184;&#12290;&#20195;&#29702;&#20154;&#28982;&#21518;&#20570;&#20986;&#19968;&#20010;&#26368;&#22823;&#21270;&#22905;&#33258;&#24049;&#25928;&#29992;&#30340;&#25112;&#30053;&#34892;&#21160;&#36873;&#25321;&#65292;&#20294;&#30452;&#25509;&#35266;&#23519;&#19981;&#21040;&#34892;&#21160;&#12290;&#22996;&#25176;&#20154;&#35266;&#23519;&#32467;&#26524;&#24182;&#20174;&#20195;&#29702;&#20154;&#30340;&#34892;&#21160;&#36873;&#25321;&#20013;&#33719;&#24471;&#25928;&#29992;&#12290;&#26681;&#25454;&#36807;&#21435;&#30340;&#35266;&#23519;&#65292;&#22996;&#25176;&#20154;&#21160;&#24577;&#22320;&#35843;&#25972;&#21512;&#21516;&#65292;&#30446;&#26631;&#26159;&#26368;&#22823;&#21270;&#20854;&#25928;&#29992;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#32473;&#20986;&#20102;&#20854;Stackelberg&#36951;&#25022;&#30340;&#19978;&#30028;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#21512;&#21516;&#31354;&#38388;&#20026;$[0,1]^m$&#26102;&#65292;Stackelberg&#36951;&#25022;&#30340;&#19978;&#30028;&#20026;$\widetilde O(\sqrt{m} \cdot T^{1-1/(2m+1)})$&#65292;&#19979;&#30028;&#20026;$\Omega(T^{1-1/(m+2)})$&#65292;&#20854;&#20013;$\widetilde O$&#25490;&#38500;&#23545;&#25968;&#22240;&#23376;&#12290; &#36825;&#20010;&#32467;&#26524;&#34920;&#26126;&#65292;&#25351;&#25968;&#32423;&#30340;$m$&#20010;&#26679;&#26412;&#23601;&#36275;&#20197;&#23398;&#20064;&#19968;&#20010;&#36817;&#20046;&#26368;&#20248;&#30340;&#21512;&#21516;&#65292;&#35299;&#20915;&#20102;&#22312;&#32447;&#21512;&#21516;&#35774;&#35745;&#20013;&#30340;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the hidden-action principal-agent problem in an online setting. In each round, the principal posts a contract that specifies the payment to the agent based on each outcome. The agent then makes a strategic choice of action that maximizes her own utility, but the action is not directly observable by the principal. The principal observes the outcome and receives utility from the agent's choice of action. Based on past observations, the principal dynamically adjusts the contracts with the goal of maximizing her utility.  We introduce an online learning algorithm and provide an upper bound on its Stackelberg regret. We show that when the contract space is $[0,1]^m$, the Stackelberg regret is upper bounded by $\widetilde O(\sqrt{m} \cdot T^{1-1/(2m+1)})$, and lower bounded by $\Omega(T^{1-1/(m+2)})$, where $\widetilde O$ omits logarithmic factors. This result shows that exponential-in-$m$ samples are sufficient and necessary to learn a near-optimal contract, resolving an open probl
&lt;/p&gt;</description></item><item><title>NESTER&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#31070;&#32463;&#31526;&#21495;&#21270;&#26041;&#27861;&#36827;&#34892;&#27835;&#30103;&#25928;&#26524;&#35780;&#20272;&#65292;&#23558;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#30340;&#25152;&#26377;&#35201;&#27714;&#38598;&#25104;&#21040;&#19968;&#20010;&#26694;&#26550;&#20013;&#65292;&#35813;&#26041;&#27861;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#24615;&#33021;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2211.04370</link><description>&lt;p&gt;
NESTER&#65306;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#31070;&#32463;&#31526;&#21495;&#21270;&#26041;&#27861;&#36827;&#34892;&#27835;&#30103;&#25928;&#26524;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
NESTER: An Adaptive Neurosymbolic Method for Treatment Effect Estimation. (arXiv:2211.04370v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.04370
&lt;/p&gt;
&lt;p&gt;
NESTER&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#31070;&#32463;&#31526;&#21495;&#21270;&#26041;&#27861;&#36827;&#34892;&#27835;&#30103;&#25928;&#26524;&#35780;&#20272;&#65292;&#23558;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#30340;&#25152;&#26377;&#35201;&#27714;&#38598;&#25104;&#21040;&#19968;&#20010;&#26694;&#26550;&#20013;&#65292;&#35813;&#26041;&#27861;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#24615;&#33021;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#36827;&#34892;&#27835;&#30103;&#25928;&#26524;&#35780;&#20272;&#26159;&#22240;&#26524;&#25512;&#26029;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#12290;&#22522;&#20110;&#28508;&#22312;&#32467;&#26524;&#26694;&#26550;&#30340;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#22240;&#26524;&#25512;&#26029;&#20013;&#30340;&#24402;&#32435;&#20559;&#32622;&#21644;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#27599;&#31181;&#29616;&#26377;&#30340;&#25216;&#26415;&#37117;&#36890;&#36807;&#35774;&#35745;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#21644;&#27491;&#21017;&#21270;&#22120;&#26469;&#35299;&#20915;&#27835;&#30103;&#25928;&#26524;&#35780;&#20272;&#30340;&#29305;&#23450;&#26041;&#38754;&#65292;&#20363;&#22914;&#25511;&#21046;&#20542;&#21521;&#24471;&#20998;&#12289;&#24378;&#21046;&#38543;&#26426;&#21270;&#31561;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#31216;&#20026;&#31070;&#32463;&#31526;&#21495;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#22120;&#65288;NESTER&#65289;&#65292;&#23427;&#26159;&#19968;&#31181;&#27835;&#30103;&#25928;&#26524;&#35780;&#20272;&#30340;&#36890;&#29992;&#26041;&#27861;&#12290;NESTER&#23558;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#30340;&#25152;&#26377;&#35201;&#27714;&#38598;&#25104;&#21040;&#19968;&#20010;&#26694;&#26550;&#20013;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#25991;&#29486;&#20013;&#20351;&#29992;&#30340;&#24402;&#32435;&#20559;&#32622;&#30340;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#30340;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#65288;DSL&#65289;&#12290;&#25105;&#20204;&#36824;&#22312;&#29702;&#35770;&#19978;&#30740;&#31350;&#20102;NESTER&#22312;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#20840;&#38754;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#65292;NESTER&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Treatment effect estimation from observational data is a central problem in causal inference. Methods based on potential outcomes framework solve this problem by exploiting inductive biases and heuristics from causal inference. Each existing technique addresses a specific aspect of treatment effect estimation, such as controlling propensity score, enforcing randomization, etc., by designing neural network architectures and regularizers. In this paper, we propose an adaptive method called Neurosymbolic Treatment Effect Estimator (NESTER), a generalized method for treatment effect estimation. NESTER brings together all the desiderata for treatment effect estimation into one framework. For this purpose, we design a Domain Specific Language (DSL) for the treatment effect estimation based on inductive biases used in literature. We also theoretically study NESTER's capability for the treatment effect estimation task. Our comprehensive empirical results show that NESTER performs better on ben
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20010;&#24615;&#21270;&#30315;&#30187;&#26816;&#27979;&#21644;&#20998;&#31867;&#30340;&#20803;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#20803;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#22312;&#26377;&#38480;&#30340;&#26679;&#26412;&#20013;&#24555;&#36895;&#36866;&#24212;&#29305;&#23450;&#24739;&#32773;&#65292;&#24182;&#22312; TUSZ &#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#22522;&#32447;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2211.02642</link><description>&lt;p&gt;
&#19968;&#31181;&#20010;&#24615;&#21270;&#30315;&#30187;&#26816;&#27979;&#21644;&#20998;&#31867;&#30340;&#20803;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Meta-GNN approach to personalized seizure detection and classification. (arXiv:2211.02642v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.02642
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20010;&#24615;&#21270;&#30315;&#30187;&#26816;&#27979;&#21644;&#20998;&#31867;&#30340;&#20803;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#20803;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#22312;&#26377;&#38480;&#30340;&#26679;&#26412;&#20013;&#24555;&#36895;&#36866;&#24212;&#29305;&#23450;&#24739;&#32773;&#65292;&#24182;&#22312; TUSZ &#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#22522;&#32447;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20010;&#24615;&#21270;&#30315;&#30187;&#26816;&#27979;&#21644;&#20998;&#31867;&#26694;&#26550;&#65292;&#20174;&#26377;&#38480;&#30340;&#30315;&#30187;&#26679;&#26412;&#20013;&#24555;&#36895;&#36866;&#24212;&#20110;&#29305;&#23450;&#24739;&#32773;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#20004;&#31181;&#26032;&#20852;&#30340;&#33539;&#20363;&#8212;&#8212;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#20803;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#36825;&#19968;&#30446;&#26631;&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#31181;&#22522;&#20110;&#20803;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#31867;&#22120;&#65292;&#35813;&#20998;&#31867;&#22120;&#20174;&#19968;&#32452;&#35757;&#32451;&#24739;&#32773;&#20013;&#23398;&#20064;&#20102;&#19968;&#20010;&#20840;&#23616;&#27169;&#22411;&#65292;&#20197;&#20415;&#26368;&#32456;&#21487;&#20197;&#20351;&#29992;&#38750;&#24120;&#26377;&#38480;&#30340;&#26679;&#26412;&#23545;&#26032;&#30340;&#26410;&#35265;&#36807;&#30340;&#24739;&#32773;&#36827;&#34892;&#35843;&#25972;&#12290;&#25105;&#20204;&#22312; TUSZ &#25968;&#25454;&#38598;&#19978;&#24212;&#29992;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#36825;&#26159;&#20844;&#24320;&#21487;&#29992;&#30340;&#30315;&#30187;&#22522;&#20934;&#25968;&#25454;&#38598;&#20043;&#19968;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20165;&#36827;&#34892;&#20102; 20 &#27425;&#36845;&#20195;&#21518;&#65292;&#21363;&#21487;&#22312;&#26032;&#30340;&#26410;&#35265;&#36807;&#30340;&#24739;&#32773;&#19978;&#36798;&#21040; 82.7% &#30340;&#20934;&#30830;&#29575;&#21644; 82.08% &#30340; F1 &#20998;&#25968;&#65292;&#20248;&#20110;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a personalized seizure detection and classification framework that quickly adapts to a specific patient from limited seizure samples. We achieve this by combining two novel paradigms that have recently seen much success in a wide variety of real-world applications: graph neural networks (GNN), and meta-learning. We train a Meta-GNN based classifier that learns a global model from a set of training patients such that this global model can eventually be adapted to a new unseen patient using very limited samples. We apply our approach on the TUSZ-dataset, one of the largest and publicly available benchmark datasets for epilepsy. We show that our method outperforms the baselines by reaching 82.7% on accuracy and 82.08% on F1 score after only 20 iterations on new unseen patients.
&lt;/p&gt;</description></item><item><title>&#38544;&#21464;&#37327;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#36890;&#36807;&#23558;&#23450;&#24615;&#22240;&#32032;&#26144;&#23556;&#21040;&#24213;&#23618;&#38544;&#21464;&#37327;&#30340;&#26041;&#24335;&#35299;&#20915;&#20102;&#26631;&#20934;&#39640;&#26031;&#36807;&#31243;&#26080;&#27861;&#36866;&#24212;&#23450;&#24615;&#36755;&#20837;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#38544;&#21464;&#37327;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#30340;&#23436;&#20840;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#25903;&#25345;&#36890;&#36807;&#38544;&#21464;&#37327;&#21487;&#35270;&#21270;&#23450;&#24615;&#36755;&#20837;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2211.02218</link><description>&lt;p&gt;
&#38544;&#21464;&#37327;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#30340;&#23436;&#20840;&#36125;&#21494;&#26031;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Fully Bayesian inference for latent variable Gaussian process models. (arXiv:2211.02218v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.02218
&lt;/p&gt;
&lt;p&gt;
&#38544;&#21464;&#37327;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#36890;&#36807;&#23558;&#23450;&#24615;&#22240;&#32032;&#26144;&#23556;&#21040;&#24213;&#23618;&#38544;&#21464;&#37327;&#30340;&#26041;&#24335;&#35299;&#20915;&#20102;&#26631;&#20934;&#39640;&#26031;&#36807;&#31243;&#26080;&#27861;&#36866;&#24212;&#23450;&#24615;&#36755;&#20837;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#38544;&#21464;&#37327;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#30340;&#23436;&#20840;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#25903;&#25345;&#36890;&#36807;&#38544;&#21464;&#37327;&#21487;&#35270;&#21270;&#23450;&#24615;&#36755;&#20837;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#24037;&#31243;&#21644;&#31185;&#23398;&#24212;&#29992;&#24120;&#24120;&#28041;&#21450;&#19968;&#20010;&#25110;&#22810;&#20010;&#23450;&#24615;&#36755;&#20837;&#12290;&#28982;&#32780;&#65292;&#26631;&#20934;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#19981;&#33021;&#30452;&#25509;&#36866;&#24212;&#23450;&#24615;&#36755;&#20837;&#12290;&#26368;&#36817;&#24341;&#20837;&#30340;&#38544;&#21464;&#37327;&#39640;&#26031;&#36807;&#31243;&#65288;LVGP&#65289;&#36890;&#36807;&#39318;&#20808;&#23558;&#27599;&#20010;&#23450;&#24615;&#22240;&#32032;&#26144;&#23556;&#21040;&#24213;&#23618;&#38544;&#21464;&#37327;&#65288;LV&#65289;&#65292;&#28982;&#21518;&#22312;&#36825;&#20123;LV&#19978;&#20351;&#29992;&#20219;&#20309;&#26631;&#20934;GP&#21327;&#26041;&#24046;&#20989;&#25968;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#36890;&#36807;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65292;&#23545;LV&#36827;&#34892;&#20272;&#35745;&#65292;&#28982;&#21518;&#23558;&#20854;&#25554;&#20837;&#21040;&#39044;&#27979;&#34920;&#36798;&#24335;&#20013;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#25554;&#20837;&#26041;&#27861;&#19981;&#32771;&#34385;LV&#20272;&#35745;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#32780;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#22312;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#21487;&#33021;&#20250;&#24456;&#22823;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20026;LVGP&#27169;&#22411;&#24320;&#21457;&#20102;&#19968;&#20010;&#23436;&#20840;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20854;LV&#21487;&#35270;&#21270;&#20102;&#23450;&#24615;&#36755;&#20837;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#36866;&#29992;&#20110;&#25193;&#23637;LVGP&#21644;LVGP&#36229;&#21442;&#25968;&#30340;&#23436;&#20840;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#36817;&#20284;&#26041;&#27861;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#25968;&#20540;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real engineering and scientific applications often involve one or more qualitative inputs. Standard Gaussian processes (GPs), however, cannot directly accommodate qualitative inputs. The recently introduced latent variable Gaussian process (LVGP) overcomes this issue by first mapping each qualitative factor to underlying latent variables (LVs), and then uses any standard GP covariance function over these LVs. The LVs are estimated similarly to the other GP hyperparameters through maximum likelihood estimation, and then plugged into the prediction expressions. However, this plug-in approach will not account for uncertainty in estimation of the LVs, which can be significant especially with limited training data. In this work, we develop a fully Bayesian approach for the LVGP model and for visualizing the effects of the qualitative inputs via their LVs. We also develop approximations for scaling up LVGPs and fully Bayesian inference for the LVGP hyperparameters. We conduct numerical studi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23545;&#23545;&#35937;&#36712;&#36857;&#34920;&#31034;&#27169;&#22411;&#30340;&#22797;&#26434;&#24230;&#21644;&#25311;&#21512;&#35823;&#24046;&#20043;&#38388;&#30340;&#26435;&#34913;&#36827;&#34892;&#20102;&#32463;&#39564;&#20998;&#26512;&#65292;&#21457;&#29616;&#31616;&#21333;&#30340;&#32447;&#24615;&#27169;&#22411;&#23601;&#33021;&#22815;&#39640;&#24230;&#37325;&#29616;&#30495;&#23454;&#19990;&#30028;&#30340;&#36712;&#36857;&#65292;&#36890;&#36807;&#20351;&#29992;&#32463;&#39564;&#36125;&#21494;&#26031;&#26041;&#27861;&#21487;&#20197;&#20026;&#36712;&#36857;&#36319;&#36394;&#38382;&#39064;&#20013;&#24517;&#35201;&#30340;&#36816;&#21160;&#27169;&#22411;&#25552;&#20379;&#20449;&#24687;&#65292;&#24182;&#21487;&#20197;&#24110;&#21161;&#35268;&#33539;&#39044;&#27979;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2211.01696</link><description>&lt;p&gt;
&#23545;&#35937;&#36712;&#36857;&#34920;&#31034;&#27169;&#22411;&#30340;&#32463;&#39564;&#36125;&#21494;&#26031;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
An Empirical Bayes Analysis of Object Trajectory Representation Models. (arXiv:2211.01696v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01696
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23545;&#23545;&#35937;&#36712;&#36857;&#34920;&#31034;&#27169;&#22411;&#30340;&#22797;&#26434;&#24230;&#21644;&#25311;&#21512;&#35823;&#24046;&#20043;&#38388;&#30340;&#26435;&#34913;&#36827;&#34892;&#20102;&#32463;&#39564;&#20998;&#26512;&#65292;&#21457;&#29616;&#31616;&#21333;&#30340;&#32447;&#24615;&#27169;&#22411;&#23601;&#33021;&#22815;&#39640;&#24230;&#37325;&#29616;&#30495;&#23454;&#19990;&#30028;&#30340;&#36712;&#36857;&#65292;&#36890;&#36807;&#20351;&#29992;&#32463;&#39564;&#36125;&#21494;&#26031;&#26041;&#27861;&#21487;&#20197;&#20026;&#36712;&#36857;&#36319;&#36394;&#38382;&#39064;&#20013;&#24517;&#35201;&#30340;&#36816;&#21160;&#27169;&#22411;&#25552;&#20379;&#20449;&#24687;&#65292;&#24182;&#21487;&#20197;&#24110;&#21161;&#35268;&#33539;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#23545;&#35937;&#36712;&#36857;&#24314;&#27169;&#20013;&#30340;&#27169;&#22411;&#22797;&#26434;&#24230;&#21644;&#25311;&#21512;&#35823;&#24046;&#20043;&#38388;&#30340;&#26435;&#34913;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#32463;&#39564;&#20998;&#26512;&#12290;&#36890;&#36807;&#20998;&#26512;&#22810;&#20010;&#22823;&#22411;&#20844;&#20849;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#21457;&#29616;&#31616;&#21333;&#30340;&#32447;&#24615;&#27169;&#22411;&#22312;&#30456;&#20851;&#26102;&#38388;&#33539;&#22260;&#20869;&#20351;&#29992;&#36739;&#23569;&#30340;&#27169;&#22411;&#22797;&#26434;&#24230;&#23601;&#33021;&#22815;&#39640;&#24230;&#37325;&#29616;&#30495;&#23454;&#19990;&#30028;&#30340;&#36712;&#36857;&#12290;&#36825;&#19968;&#21457;&#29616;&#20801;&#35768;&#23558;&#36712;&#36857;&#36319;&#36394;&#21644;&#39044;&#27979;&#20316;&#20026;&#36125;&#21494;&#26031;&#36807;&#28388;&#38382;&#39064;&#36827;&#34892;&#20844;&#24335;&#21270;&#12290;&#25105;&#20204;&#37319;&#29992;&#32463;&#39564;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#20174;&#25968;&#25454;&#20013;&#20272;&#35745;&#27169;&#22411;&#21442;&#25968;&#30340;&#20808;&#39564;&#20998;&#24067;&#65292;&#36825;&#20123;&#20808;&#39564;&#20998;&#24067;&#21487;&#20197;&#20026;&#36712;&#36857;&#36319;&#36394;&#38382;&#39064;&#20013;&#24517;&#35201;&#30340;&#36816;&#21160;&#27169;&#22411;&#25552;&#20379;&#20449;&#24687;&#65292;&#24182;&#21487;&#20197;&#24110;&#21161;&#35268;&#33539;&#39044;&#27979;&#27169;&#22411;&#12290;&#25105;&#20204;&#20027;&#24352;&#22312;&#36712;&#36857;&#39044;&#27979;&#20219;&#21153;&#20013;&#20351;&#29992;&#32447;&#24615;&#36712;&#36857;&#34920;&#31034;&#27169;&#22411;&#65292;&#22240;&#20026;&#23427;&#20204;&#30446;&#21069;&#24182;&#19981;&#20250;&#38480;&#21046;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an in-depth empirical analysis of the trade-off between model complexity and fit error in modelling object trajectories. Analyzing several large public datasets, we show that simple linear models do represent real-world trajectories with high fidelity over relevant time scales at very moderate model complexity. This finding allows the formulation of trajectory tracking and prediction as a Bayesian filtering problem. Using an Empirical Bayes approach, we estimate prior distributions over model parameters from the data. These prior distributions inform the motion models necessary in the trajectory tracking problem and can help regularize prediction models. We argue for the use of linear trajectory representation models in trajectory prediction tasks as they do not limit prediction performance currently.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#23637;&#31034;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;30&#31186;&#21333;&#23548;&#32852;&#24515;&#30005;&#22270;&#20449;&#21495;&#20272;&#35745;&#24515;&#21147;&#34928;&#31469;&#20303;&#38498;&#65292;&#24182;&#25552;&#20379;&#20102;&#20020;&#24202;&#26377;&#24847;&#20041;&#30340;&#35299;&#37322;&#12290;&#36890;&#36807;&#35757;&#32451;&#26497;&#38480;&#26799;&#24230;&#25552;&#21319;&#21152;&#36895;&#25925;&#38556;&#26102;&#38388;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#27979;&#35797;&#38598;&#19978;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#21487;&#20197;&#22312;&#23450;&#20301;&#21644;&#27835;&#30103;&#20013;&#21457;&#25381;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2211.00819</link><description>&lt;p&gt;
&#20174;30&#31186;&#24515;&#30005;&#22270;&#35299;&#37322;&#24615;&#22320;&#35780;&#20272;&#24515;&#21147;&#34928;&#31469;&#20303;&#38498;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Interpretable estimation of the risk of heart failure hospitalization from a 30-second electrocardiogram. (arXiv:2211.00819v2 [cs.LG] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.00819
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#23637;&#31034;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;30&#31186;&#21333;&#23548;&#32852;&#24515;&#30005;&#22270;&#20449;&#21495;&#20272;&#35745;&#24515;&#21147;&#34928;&#31469;&#20303;&#38498;&#65292;&#24182;&#25552;&#20379;&#20102;&#20020;&#24202;&#26377;&#24847;&#20041;&#30340;&#35299;&#37322;&#12290;&#36890;&#36807;&#35757;&#32451;&#26497;&#38480;&#26799;&#24230;&#25552;&#21319;&#21152;&#36895;&#25925;&#38556;&#26102;&#38388;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#27979;&#35797;&#38598;&#19978;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#21487;&#20197;&#22312;&#23450;&#20301;&#21644;&#27835;&#30103;&#20013;&#21457;&#25381;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#29983;&#23384;&#24314;&#27169;&#20381;&#36182;&#20110;&#21487;&#35299;&#37322;&#30340;&#32479;&#35745;&#27169;&#22411;&#65292;&#20294;&#20854;&#28508;&#22312;&#20551;&#35774;&#24448;&#24448;&#36807;&#20110;&#31616;&#21333;&#21644;&#19981;&#20999;&#23454;&#38469;&#12290;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#20272;&#35745;&#26356;&#22797;&#26434;&#30340;&#20851;&#31995;&#24182;&#23548;&#33268;&#26356;&#31934;&#30830;&#30340;&#39044;&#27979;&#65292;&#20294;&#26159;&#23427;&#20204;&#26159;&#19981;&#21487;&#35299;&#37322;&#30340;&#12290;&#26412;&#30740;&#31350;&#34920;&#26126;&#21487;&#20197;&#36890;&#36807;30&#31186;&#21333;&#23548;&#32852;&#24515;&#30005;&#22270;&#20449;&#21495;&#20272;&#35745;&#24515;&#21147;&#34928;&#31469;&#20303;&#38498;&#12290;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#19981;&#20165;&#20855;&#26377;&#26356;&#24378;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#36824;&#25552;&#20379;&#20020;&#24202;&#26377;&#24847;&#20041;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#26497;&#38480;&#26799;&#24230;&#25552;&#21319;&#21152;&#36895;&#25925;&#38556;&#26102;&#38388;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;Shapley Additive exPlanations&#20540;&#35299;&#37322;&#27599;&#20010;&#29305;&#24449;&#23545;&#39044;&#27979;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;6,573&#21517;&#24739;&#32773;&#30340;&#27979;&#35797;&#38598;&#19978;&#21462;&#24471;&#20102;&#19968;&#24180;&#30340;&#21327;&#35843;&#25351;&#25968;&#20026;0.828&#65292;&#19968;&#24180;&#21644;&#20004;&#24180;&#30340;&#26354;&#32447;&#19979;&#38754;&#31215;&#20998;&#21035;&#20026;0.853&#21644;0.858&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#24515;&#30005;&#22270;&#30340;&#24555;&#36895;&#27979;&#35797;&#21487;&#33021;&#23545;&#23450;&#20301;&#21644;&#27835;&#30103;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Survival modeling in healthcare relies on explainable statistical models; yet, their underlying assumptions are often simplistic and, thus, unrealistic. Machine learning models can estimate more complex relationships and lead to more accurate predictions, but are non-interpretable. This study shows it is possible to estimate hospitalization for congestive heart failure by a 30 seconds single-lead electrocardiogram signal. Using a machine learning approach not only results in greater predictive power but also provides clinically meaningful interpretations. We train an eXtreme Gradient Boosting accelerated failure time model and exploit SHapley Additive exPlanations values to explain the effect of each feature on predictions. Our model achieved a concordance index of 0.828 and an area under the curve of 0.853 at one year and 0.858 at two years on a held-out test set of 6,573 patients. These results show that a rapid test based on an electrocardiogram could be crucial in targeting and tre
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#31070;&#32463;&#31639;&#27861;&#25512;&#29702;&#20219;&#21153;&#30340;&#38750;&#20998;&#24067;&#24335;&#27867;&#21270;&#33021;&#21147;&#65292;&#20998;&#26512;&#24403;&#21069;&#22522;&#20934;&#27979;&#35797;&#30340;&#25361;&#25112;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2211.00692</link><description>&lt;p&gt;
&#20026;&#25913;&#36827;&#31070;&#32463;&#31639;&#27861;&#25512;&#29702;&#20219;&#21153;&#30340;&#38750;&#20998;&#24067;&#24335;&#27867;&#21270;&#33021;&#21147;&#32780;&#21162;&#21147;
&lt;/p&gt;
&lt;p&gt;
Towards Better Out-of-Distribution Generalization of Neural Algorithmic Reasoning Tasks. (arXiv:2211.00692v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.00692
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#31070;&#32463;&#31639;&#27861;&#25512;&#29702;&#20219;&#21153;&#30340;&#38750;&#20998;&#24067;&#24335;&#27867;&#21270;&#33021;&#21147;&#65292;&#20998;&#26512;&#24403;&#21069;&#22522;&#20934;&#27979;&#35797;&#30340;&#25361;&#25112;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#31639;&#27861;&#25512;&#29702;&#20219;&#21153;&#30340;&#38750;&#20998;&#24067;&#24335;&#27867;&#21270;&#33021;&#21147;&#65292;&#20854;&#20013;&#30446;&#26631;&#26159;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20174;&#36755;&#20837;&#36755;&#20986;&#23545;&#20013;&#23398;&#20064;&#31639;&#27861;&#65288;&#20363;&#22914;&#25490;&#24207;&#12289;&#24191;&#24230;&#20248;&#20808;&#25628;&#32034;&#21644;&#28145;&#24230;&#20248;&#20808;&#25628;&#32034;&#65289;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35748;&#20026;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#38750;&#20998;&#24067;&#24335;&#27867;&#21270;&#19982;&#24120;&#35265;&#30340;&#38750;&#20998;&#24067;&#24335;&#35774;&#32622;&#26174;&#30528;&#19981;&#21516;&#12290;&#20363;&#22914;&#65292;&#36890;&#24120;&#22312;&#22270;&#20687;&#20998;&#31867;&#30340;&#38750;&#20998;&#24067;&#24335;&#27867;&#21270;&#20013;&#35266;&#23519;&#21040;&#30340;&#19968;&#20123;&#29616;&#35937;&#65292;&#20363;&#22914;\emph{&#30452;&#32447;&#19978;&#30340;&#20934;&#30830;&#24230;}&#22312;&#36825;&#37324;&#27809;&#26377;&#35266;&#23519;&#21040;&#65292;&#32780;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#31561;&#25216;&#26415;&#19981;&#36215;&#20316;&#29992;&#65292;&#22240;&#20026;&#35768;&#22810;&#22686;&#24378;&#25216;&#26415;&#30340;&#20551;&#35774;&#36890;&#24120;&#34987;&#36829;&#21453;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#24403;&#21069;&#39046;&#20808;&#30340;&#22522;&#20934;&#27979;&#35797;CLRS\citep{deepmind2021clrs}&#30340;&#20027;&#35201;&#25361;&#25112;&#65288;&#20363;&#22914;&#65292;&#36755;&#20837;&#20998;&#24067;&#20559;&#31227;&#12289;&#38750;&#20195;&#34920;&#24615;&#25968;&#25454;&#29983;&#25104;&#21644;&#26080;&#20449;&#24687;&#30340;&#39564;&#35777;&#25351;&#26631;&#65289;&#65292;&#35813;&#27979;&#35797;&#21253;&#21547;30&#20010;&#31639;&#27861;&#25512;&#29702;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#20462;&#22797;&#36755;&#20837;&#20998;&#24067;&#20559;&#31227;&#21644;&#25913;&#36827;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the OOD generalization of neural algorithmic reasoning tasks, where the goal is to learn an algorithm (e.g., sorting, breadth-first search, and depth-first search) from input-output pairs using deep neural networks. First, we argue that OOD generalization in this setting is significantly different than common OOD settings. For example, some phenomena in OOD generalization of image classifications such as \emph{accuracy on the line} are not observed here, and techniques such as data augmentation methods do not help as assumptions underlying many augmentation techniques are often violated. Second, we analyze the main challenges (e.g., input distribution shift, non-representative data generation, and uninformative validation metrics) of the current leading benchmark, i.e., CLRS \citep{deepmind2021clrs}, which contains 30 algorithmic reasoning tasks. We propose several solutions, including a simple-yet-effective fix to the input distribution shift and improved data 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#22312;Lojasiewicz&#20989;&#25968;&#19978;&#65292;&#38543;&#26426;&#38646;&#38454;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#20855;&#26377;&#25910;&#25947;&#36895;&#29575;&#65292;&#19988;&#27604; $\{ \|\mathbf{x}_t-\mathbf{x}_\infty\| \}_{t \in \mathbb{N}}$&#26356;&#24555;&#65292;&#26080;&#35770;$f$&#26159;&#24179;&#28369;&#36824;&#26159;&#38750;&#24179;&#28369;&#30340;&#12290;</title><link>http://arxiv.org/abs/2210.16997</link><description>&lt;p&gt;
&#38543;&#26426;&#38646;&#38454;&#26799;&#24230;&#19979;&#38477;&#22312;L-Lojasiewicz&#20989;&#25968;&#19978;&#30340;&#25910;&#25947;&#36895;&#29575;
&lt;/p&gt;
&lt;p&gt;
Convergence Rates of Stochastic Zeroth-order Gradient Descent for \L ojasiewicz Functions. (arXiv:2210.16997v5 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.16997
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#22312;Lojasiewicz&#20989;&#25968;&#19978;&#65292;&#38543;&#26426;&#38646;&#38454;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#20855;&#26377;&#25910;&#25947;&#36895;&#29575;&#65292;&#19988;&#27604; $\{ \|\mathbf{x}_t-\mathbf{x}_\infty\| \}_{t \in \mathbb{N}}$&#26356;&#24555;&#65292;&#26080;&#35770;$f$&#26159;&#24179;&#28369;&#36824;&#26159;&#38750;&#24179;&#28369;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35777;&#26126;&#20102;&#38543;&#26426;&#38646;&#38454;&#26799;&#24230;&#19979;&#38477;&#65288;SZGD&#65289;&#31639;&#27861;&#22312;Lojasiewicz&#20989;&#25968;&#19978;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;SZGD&#31639;&#27861;&#36845;&#20195;&#22914;&#19979;&#65306;$ \mathbf{x}_{t+1} = \mathbf{x}_t - \eta_t \widehat{\nabla} f (\mathbf{x}_t) $&#65292;&#20854;&#20013;$f$&#26159;&#28385;&#36275;Lojasiewicz&#19981;&#31561;&#24335;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#20855;&#26377;Lojasiewicz&#25351;&#25968;$\theta$&#65292;$\eta_t$&#26159;&#27493;&#38271;&#65288;&#23398;&#20064;&#29575;&#65289;&#65292;$ \widehat{\nabla} f (\mathbf{x}_t)$&#26159;&#20351;&#29992;&#38646;&#38454;&#20449;&#24687;&#20272;&#35745;&#30340;&#36817;&#20284;&#26799;&#24230;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;$ \{ f (\mathbf{x}_t) - f (\mathbf{x}_\infty) \}_{t \in \mathbb{N} }$&#30340;&#25910;&#25947;&#36895;&#24230;&#21487;&#20197;&#27604; $ \{ \| \mathbf{x}_t \mathbf{x}_\infty \| \}_{t \in \mathbb{N} } $&#26356;&#24555;&#65292;&#26080;&#35770;&#30446;&#26631;$f$&#26159;&#24179;&#28369;&#36824;&#26159;&#38750;&#24179;&#28369;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We prove convergence rates of Stochastic Zeroth-order Gradient Descent (SZGD) algorithms for Lojasiewicz functions. The SZGD algorithm iterates as \begin{align*}  \mathbf{x}_{t+1} = \mathbf{x}_t - \eta_t \widehat{\nabla} f (\mathbf{x}_t), \qquad t = 0,1,2,3,\cdots , \end{align*} where $f$ is the objective function that satisfies the \L ojasiewicz inequality with \L ojasiewicz exponent $\theta$, $\eta_t$ is the step size (learning rate), and $ \widehat{\nabla} f (\mathbf{x}_t) $ is the approximate gradient estimated using zeroth-order information only.  Our results show that $ \{ f (\mathbf{x}_t) - f (\mathbf{x}_\infty) \}_{t \in \mathbb{N} } $ can converge faster than $ \{ \| \mathbf{x}_t \mathbf{x}_\infty \| \}_{t \in \mathbb{N} }$, regardless of whether the objective $f$ is smooth or nonsmooth.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#28369;&#30772;&#30862;&#30340;&#24130;&#24459;&#20989;&#25968;&#24418;&#24335;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#27169;&#25311;&#21644;&#22806;&#25512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32553;&#25918;&#34892;&#20026;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#26550;&#26500;&#21644;&#22823;&#37327;&#19981;&#21516;&#20219;&#21153;&#65292;&#21253;&#25324;&#35270;&#35273;&#12289;&#35821;&#35328;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#12289;&#29983;&#25104;&#24314;&#27169;&#12289;&#23545;&#27604;&#23398;&#20064;&#12289;&#26426;&#22120;&#20154;&#12289;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;/&#26657;&#20934;&#12289;&#23545;&#25239;&#40065;&#26834;&#24615;&#12289;&#20998;&#23376;&#12289;&#35745;&#31639;&#26426;&#32534;&#31243;/&#32534;&#30721;&#12289;&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;&#12289;&#31639;&#26415;&#12289;&#26080;&#30417;&#30563;/&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2210.14891</link><description>&lt;p&gt;
&#30772;&#30862;&#30340;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;
&lt;/p&gt;
&lt;p&gt;
Broken Neural Scaling Laws. (arXiv:2210.14891v7 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14891
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#28369;&#30772;&#30862;&#30340;&#24130;&#24459;&#20989;&#25968;&#24418;&#24335;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#27169;&#25311;&#21644;&#22806;&#25512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32553;&#25918;&#34892;&#20026;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#26550;&#26500;&#21644;&#22823;&#37327;&#19981;&#21516;&#20219;&#21153;&#65292;&#21253;&#25324;&#35270;&#35273;&#12289;&#35821;&#35328;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#12289;&#29983;&#25104;&#24314;&#27169;&#12289;&#23545;&#27604;&#23398;&#20064;&#12289;&#26426;&#22120;&#20154;&#12289;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;/&#26657;&#20934;&#12289;&#23545;&#25239;&#40065;&#26834;&#24615;&#12289;&#20998;&#23376;&#12289;&#35745;&#31639;&#26426;&#32534;&#31243;/&#32534;&#30721;&#12289;&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;&#12289;&#31639;&#26415;&#12289;&#26080;&#30417;&#30563;/&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a smoothly broken power law functional form (referred to as a Broken Neural Scaling Law (BNSL)) that accurately models and extrapolates the scaling behaviors of deep neural networks for various architectures and a large and diverse set of tasks, including vision, language, audio, video, generative modeling, contrastive learning, robotics, uncertainty estimation/calibration, adversarial robustness, molecules, computer programming/coding, math word problems, arithmetic, unsupervised/self-supervised learning, and reinforcement learning.
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#28369;&#30772;&#30862;&#30340;&#24130;&#24459;&#20989;&#25968;&#24418;&#24335;&#65288;&#25105;&#20204;&#31216;&#20043;&#20026;&#30772;&#30862;&#30340;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;&#65288;BNSL&#65289;&#65289;&#65292;&#23427;&#20934;&#30830;&#22320;&#27169;&#25311;&#21644;&#22806;&#25512;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32553;&#25918;&#34892;&#20026;&#65288;&#21363;&#24863;&#20852;&#36259;&#30340;&#35780;&#20272;&#25351;&#26631;&#38543;&#29992;&#20110;&#35757;&#32451;&#30340;&#35745;&#31639;&#37327;&#12289;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#12289;&#35757;&#32451;&#25968;&#25454;&#38598;&#22823;&#23567;&#25110;&#19978;&#28216;&#24615;&#33021;&#21464;&#21270;&#32780;&#21464;&#21270;&#65289;&#23545;&#20110;&#21508;&#31181;&#26550;&#26500;&#21644;&#22823;&#37327;&#19981;&#21516;&#20219;&#21153;&#20013;&#30340;&#27599;&#20010;&#20219;&#21153;&#65292;&#21253;&#25324;&#22823;&#35268;&#27169;&#35270;&#35273;&#12289;&#35821;&#35328;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#12289;&#25193;&#25955;&#12289;&#29983;&#25104;&#24314;&#27169;&#12289;&#22810;&#27169;&#24577;&#23398;&#20064;&#12289;&#23545;&#27604;&#23398;&#20064;&#12289;AI&#23545;&#40784;&#12289;&#26426;&#22120;&#20154;&#12289;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#27867;&#21270;&#12289;&#25345;&#32493;&#23398;&#20064;&#12289;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;/&#26657;&#20934;&#12289;&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;&#12289;&#23545;&#25239;&#40065;&#26834;&#24615;&#12289;&#33976;&#39311;&#12289;&#20998;&#23376;&#12289;&#35745;&#31639;&#26426;&#32534;&#31243;/&#32534;&#30721;&#12289;&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;&#12289;&#31639;&#26415;&#12289;&#26080;&#30417;&#30563;/&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a smoothly broken power law functional form (referred to by us as a Broken Neural Scaling Law (BNSL)) that accurately models and extrapolates the scaling behaviors of deep neural networks (i.e. how the evaluation metric of interest varies as the amount of compute used for training, number of model parameters, training dataset size, or upstream performance varies) for various architectures and for each of various tasks within a large and diverse set of upstream and downstream tasks, in zero-shot, prompted, and fine-tuned settings. This set includes large-scale vision, language, audio, video, diffusion, generative modeling, multimodal learning, contrastive learning, AI alignment, robotics, out-of-distribution (OOD) generalization, continual learning, uncertainty estimation / calibration, out-of-distribution detection, adversarial robustness, distillation, molecules, computer programming/coding, math word problems, arithmetic, unsupervised/self-supervised learning, and reinforc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22266;&#20307;&#29123;&#26009;&#22238;&#24402;&#36895;&#29575;&#27979;&#37327;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#29992;&#20110;&#22788;&#29702;&#22810;&#20010;&#19981;&#30830;&#23450;&#24615;&#26469;&#28304;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2210.14287</link><description>&lt;p&gt;
&#32467;&#21512;&#25968;&#25454;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#65306;&#22312;&#22266;&#20307;&#29123;&#26009;&#22238;&#24402;&#36895;&#29575;&#27979;&#37327;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Combined Data and Deep Learning Model Uncertainties: An Application to the Measurement of Solid Fuel Regression Rate. (arXiv:2210.14287v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14287
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22266;&#20307;&#29123;&#26009;&#22238;&#24402;&#36895;&#29575;&#27979;&#37327;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#29992;&#20110;&#22788;&#29702;&#22810;&#20010;&#19981;&#30830;&#23450;&#24615;&#26469;&#28304;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22797;&#26434;&#30340;&#29289;&#29702;&#36807;&#31243;&#34920;&#24449;&#20013;&#65292;&#23545;&#20110;&#26469;&#33258;&#22810;&#20010;&#26469;&#28304;&#30340;&#19981;&#30830;&#23450;&#24615;&#30340;&#35266;&#27979;&#25968;&#25454;&#21644;&#27169;&#22411;&#30340;&#32508;&#21512;&#65292;&#23545;&#20110;&#24863;&#20852;&#36259;&#30340;&#37327;&#65288;QoI&#65289;&#20173;&#28982;&#26159;&#19968;&#39033;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21069;&#21521;&#20256;&#25773;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65288;UQ&#65289;&#36807;&#31243;&#65292;&#20197;&#20135;&#29983;&#35266;&#27979;&#22238;&#24402;&#36895;&#29575;$\dot{r}$&#30340;&#27010;&#29575;&#20998;&#24067;&#12290;&#25105;&#20204;&#20174;&#23454;&#39564;&#20013;&#34920;&#24449;&#20102;&#20004;&#20010;&#36755;&#20837;&#25968;&#25454;&#19981;&#30830;&#23450;&#24615;&#28304;&#65288;&#25668;&#20687;&#26426;&#22833;&#30495;$U_c$&#21644;&#29123;&#26009;&#25918;&#32622;&#30340;&#38750;&#38646;&#35282;&#24230;$U_\gamma$&#65289;&#65292;&#26469;&#33258;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#21644;&#27169;&#22411;&#24418;&#24335;&#19981;&#30830;&#23450;&#24615;&#65288;$U_m$&#65289;&#65292;&#20197;&#21450;&#29992;&#20110;&#35757;&#32451;&#23427;&#30340;&#25163;&#21160;&#20998;&#21106;&#22270;&#20687;&#30340;&#21464;&#24322;&#24615;&#65288;$U_s$&#65289;&#12290;&#25105;&#20204;&#23545;&#36825;&#20123;&#19981;&#30830;&#23450;&#24615;&#28304;&#19982;&#27169;&#22411;&#24418;&#24335;&#19981;&#30830;&#23450;&#24615;&#30340;&#19971;&#31181;&#32452;&#21512;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#23545;&#22266;&#20307;&#28151;&#21512;&#28779;&#31661;&#29123;&#26009;&#22238;&#24402;&#36895;&#29575;&#27979;&#37327;&#30340;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#20840;&#38754;&#35843;&#26597;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#31181;&#21487;&#29992;&#20110;&#22788;&#29702;&#22810;&#20010;&#19981;&#30830;&#23450;&#24615;&#28304;&#30340;&#21069;&#21521;&#20256;&#25773;UQ&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
In complex physical process characterization, such as the measurement of the regression rate for solid hybrid rocket fuels, where both the observation data and the model used have uncertainties originating from multiple sources, combining these in a systematic way for quantities of interest(QoI) remains a challenge. In this paper, we present a forward propagation uncertainty quantification (UQ) process to produce a probabilistic distribution for the observed regression rate $\dot{r}$. We characterized two input data uncertainty sources from the experiment (the distortion from the camera $U_c$ and the non-zero angle fuel placement $U_\gamma$), the prediction and model form uncertainty from the deep neural network ($U_m$), as well as the variability from the manually segmented images used for training it ($U_s$). We conducted seven case studies on combinations of these uncertainty sources with the model form uncertainty. The main contribution of this paper is the investigation and inclus
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#38454;&#27573;&#32858;&#31867;&#31574;&#30053;&#65292;&#21487;&#20197;&#35299;&#20915;&#27969;&#24335;&#23884;&#20837;&#24335;&#28436;&#35762;&#32773;&#20998;&#31163;&#31995;&#32479;&#30340;&#22810;&#26041;&#38754;&#25361;&#25112;&#65292;&#20174;&#32780;&#25552;&#39640;&#25928;&#29575;&#12290;&#27492;&#31574;&#30053;&#20351;&#29992;&#19981;&#21516;&#30340;&#32858;&#31867;&#31639;&#27861;&#22788;&#29702;&#19981;&#21516;&#38271;&#24230;&#30340;&#36755;&#20837;&#65292;&#24182;&#21487;&#36866;&#24212;&#19981;&#21516;&#36164;&#28304;&#32422;&#26463;&#30340;&#35774;&#22791;&#12290;</title><link>http://arxiv.org/abs/2210.13690</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#22810;&#38454;&#27573;&#32858;&#31867;&#23454;&#26102;&#27969;&#24335;&#23884;&#20837;&#24335;&#28436;&#35762;&#20154;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
Highly Efficient Real-Time Streaming and Fully On-Device Speaker Diarization with Multi-Stage Clustering. (arXiv:2210.13690v3 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13690
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#38454;&#27573;&#32858;&#31867;&#31574;&#30053;&#65292;&#21487;&#20197;&#35299;&#20915;&#27969;&#24335;&#23884;&#20837;&#24335;&#28436;&#35762;&#32773;&#20998;&#31163;&#31995;&#32479;&#30340;&#22810;&#26041;&#38754;&#25361;&#25112;&#65292;&#20174;&#32780;&#25552;&#39640;&#25928;&#29575;&#12290;&#27492;&#31574;&#30053;&#20351;&#29992;&#19981;&#21516;&#30340;&#32858;&#31867;&#31639;&#27861;&#22788;&#29702;&#19981;&#21516;&#38271;&#24230;&#30340;&#36755;&#20837;&#65292;&#24182;&#21487;&#36866;&#24212;&#19981;&#21516;&#36164;&#28304;&#32422;&#26463;&#30340;&#35774;&#22791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35828;&#35805;&#32773;&#20998;&#31163;&#26041;&#38754;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#36827;&#23637;&#20027;&#35201;&#38598;&#20013;&#20110;&#25552;&#39640;&#20998;&#31163;&#32467;&#26524;&#30340;&#36136;&#37327;&#65292;&#20294;&#21516;&#26102;&#20063;&#36234;&#26469;&#36234;&#20851;&#27880;&#25552;&#39640;&#20998;&#31163;&#31995;&#32479;&#30340;&#25928;&#29575;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#19968;&#31181;&#22810;&#38454;&#27573;&#32858;&#31867;&#31574;&#30053;&#8212;&#8212;&#23545;&#20110;&#19981;&#21516;&#38271;&#24230;&#30340;&#36755;&#20837;&#20351;&#29992;&#19981;&#21516;&#30340;&#32858;&#31867;&#31639;&#27861;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#23884;&#20837;&#24335;&#28436;&#35762;&#20154;&#20998;&#31163;&#24212;&#29992;&#30340;&#22810;&#26041;&#38754;&#25361;&#25112;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#37319;&#29992;&#21518;&#22791;&#32858;&#31867;&#22120;&#26469;&#22788;&#29702;&#30701;&#24418;&#24335;&#36755;&#20837;&#65307;&#20027;&#32858;&#31867;&#22120;&#29992;&#20110;&#22788;&#29702;&#20013;&#31561;&#38271;&#24230;&#30340;&#36755;&#20837;&#65307;&#22312;&#32463;&#36807;&#20027;&#32858;&#31867;&#22120;&#20043;&#21069;&#65292;&#37319;&#29992;&#39044;&#22788;&#29702;&#32858;&#31867;&#22120;&#26469;&#21387;&#32553;&#38271;&#24418;&#24335;&#36755;&#20837;&#12290;&#20027;&#32858;&#31867;&#22120;&#21644;&#39044;&#22788;&#29702;&#32858;&#31867;&#22120;&#37117;&#21487;&#20197;&#37197;&#32622;&#20026;&#35745;&#31639;&#22797;&#26434;&#24230;&#19978;&#30028;&#65292;&#20197;&#36866;&#24212;&#19981;&#21516;&#36164;&#28304;&#32422;&#26463;&#30340;&#35774;&#22791;&#12290;&#35813;&#22810;&#38454;&#27573;&#32858;&#31867;&#31574;&#30053;&#23545;&#20110;CPU&#12289;&#20869;&#23384;&#21644;&#30005;&#27744;&#39044;&#31639;&#32039;&#24352;&#30340;&#27969;&#24335;&#23884;&#20837;&#24335;&#28436;&#35762;&#32773;&#20998;&#31163;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
While recent research advances in speaker diarization mostly focus on improving the quality of diarization results, there is also an increasing interest in improving the efficiency of diarization systems. In this paper, we demonstrate that a multi-stage clustering strategy that uses different clustering algorithms for input of different lengths can address multi-faceted challenges of on-device speaker diarization applications. Specifically, a fallback clusterer is used to handle short-form inputs; a main clusterer is used to handle medium-length inputs; and a pre-clusterer is used to compress long-form inputs before they are processed by the main clusterer. Both the main clusterer and the pre-clusterer can be configured with an upper bound of the computational complexity to adapt to devices with different resource constraints. This multi-stage clustering strategy is critical for streaming on-device speaker diarization systems, where the budgets of CPU, memory and battery are tight.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#20013;&#30340;&#26426;&#22120;&#20154;&#23398;&#20064;&#31639;&#27861;&#65292;&#25351;&#20986;&#20102;&#39321;&#33609;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#21487;&#33021;&#22240;&#20855;&#26377;&#19981;&#25910;&#25947;&#30340;&#21442;&#25968;&#32780;&#23384;&#22312;&#23398;&#20064;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;NPG&#31639;&#27861;&#21464;&#31181;&#20197;&#35299;&#20915;&#27492;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2210.12812</link><description>&lt;p&gt;
&#23545;&#31216;&#65288;&#20048;&#35266;&#65289;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#30340;&#21442;&#25968;&#25910;&#25947;&#24615;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Symmetric (Optimistic) Natural Policy Gradient for Multi-agent Learning with Parameter Convergence. (arXiv:2210.12812v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.12812
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#20013;&#30340;&#26426;&#22120;&#20154;&#23398;&#20064;&#31639;&#27861;&#65292;&#25351;&#20986;&#20102;&#39321;&#33609;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#21487;&#33021;&#22240;&#20855;&#26377;&#19981;&#25910;&#25947;&#30340;&#21442;&#25968;&#32780;&#23384;&#22312;&#23398;&#20064;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;NPG&#31639;&#27861;&#21464;&#31181;&#20197;&#35299;&#20915;&#27492;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#22810;&#26234;&#33021;&#20307;&#20114;&#21160;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#24182;&#19988;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#30340;&#29702;&#35770;&#22522;&#30784;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#27987;&#21402;&#20852;&#36259;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#20013;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#65288;NPG&#65289;&#31639;&#27861;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#39321;&#33609;NPG&#21487;&#33021;&#27809;&#26377;&#21442;&#25968;&#25910;&#25947;&#65292;&#21363;&#21442;&#25968;&#21270;&#31574;&#30053;&#30340;&#21521;&#37327;&#30340;&#25910;&#25947;&#65292;&#21363;&#20351;&#25104;&#26412;&#34987;&#35268;&#21017;&#21270;&#65288;&#22312;&#25991;&#29486;&#20013;&#20351;&#31574;&#30053;&#31354;&#38388;&#26377;&#24378;&#30340;&#25910;&#25947;&#20445;&#35777;&#65289;&#12290;&#36825;&#20123;&#38750;&#25910;&#25947;&#30340;&#21442;&#25968;&#23548;&#33268;&#23398;&#20064;&#20013;&#30340;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#22312;&#20989;&#25968;&#36924;&#36817;&#30340;&#24773;&#20917;&#19979;&#23588;&#20026;&#37325;&#35201;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21482;&#33021;&#22788;&#29702;&#20302;&#32500;&#21442;&#25968;&#65292;&#32780;&#19981;&#26159;&#39640;&#32500;&#31574;&#30053;&#12290;&#28982;&#21518;&#25105;&#20204;&#38024;&#23545;&#20960;&#31181;&#26631;&#20934;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#22330;&#26223;&#25552;&#20986;&#20102;NPG&#31639;&#27861;&#30340;&#21464;&#31181;&#65306;&#20004;&#20010;&#29609;&#23478;&#30340;&#38646;&#21644;&#30697;&#38453;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#65292;&#20197;&#21450;&#22810;&#20010;&#29609;&#23478;&#30340;&#21333;&#35843;&#21338;&#24328;&#65292;&#20854;&#20855;&#26377;&#20840;&#23616;&#26368;&#21518;&#36845;&#20195;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-agent interactions are increasingly important in the context of reinforcement learning, and the theoretical foundations of policy gradient methods have attracted surging research interest. We investigate the global convergence of natural policy gradient (NPG) algorithms in multi-agent learning. We first show that vanilla NPG may not have parameter convergence, i.e., the convergence of the vector that parameterizes the policy, even when the costs are regularized (which enabled strong convergence guarantees in the policy space in the literature). This non-convergence of parameters leads to stability issues in learning, which becomes especially relevant in the function approximation setting, where we can only operate on low-dimensional parameters, instead of the high-dimensional policy. We then propose variants of the NPG algorithm, for several standard multi-agent learning scenarios: two-player zero-sum matrix and Markov games, and multi-player monotone games, with global last-iter
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#25991;&#26412;&#23884;&#20837;&#22522;&#20934;&#27979;&#35797;(MTEB)&#65292;&#35813;&#22522;&#20934;&#27979;&#35797;&#28085;&#30422;&#20102;8&#20010;&#23884;&#20837;&#20219;&#21153;&#12289;58&#20010;&#25968;&#25454;&#38598;&#21644;112&#31181;&#35821;&#35328;&#65292;&#20197;&#35299;&#20915;&#25991;&#26412;&#23884;&#20837;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#34920;&#29616;&#24046;&#24322;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;33&#20010;&#27169;&#22411;&#30340;&#27979;&#35797;&#65292;&#20316;&#32773;&#21457;&#29616;&#35813;&#39046;&#22495;&#23578;&#26410;&#25910;&#25947;&#20110;&#19968;&#31181;&#36890;&#29992;&#30340;&#25991;&#26412;&#23884;&#20837;&#26041;&#27861;&#65292;</title><link>http://arxiv.org/abs/2210.07316</link><description>&lt;p&gt;
MTEB: &#22823;&#35268;&#27169;&#25991;&#26412;&#23884;&#20837;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
MTEB: Massive Text Embedding Benchmark. (arXiv:2210.07316v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07316
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#25991;&#26412;&#23884;&#20837;&#22522;&#20934;&#27979;&#35797;(MTEB)&#65292;&#35813;&#22522;&#20934;&#27979;&#35797;&#28085;&#30422;&#20102;8&#20010;&#23884;&#20837;&#20219;&#21153;&#12289;58&#20010;&#25968;&#25454;&#38598;&#21644;112&#31181;&#35821;&#35328;&#65292;&#20197;&#35299;&#20915;&#25991;&#26412;&#23884;&#20837;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#34920;&#29616;&#24046;&#24322;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;33&#20010;&#27169;&#22411;&#30340;&#27979;&#35797;&#65292;&#20316;&#32773;&#21457;&#29616;&#35813;&#39046;&#22495;&#23578;&#26410;&#25910;&#25947;&#20110;&#19968;&#31181;&#36890;&#29992;&#30340;&#25991;&#26412;&#23884;&#20837;&#26041;&#27861;&#65292;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#23884;&#20837;&#36890;&#24120;&#22312;&#35206;&#30422;&#20854;&#20182;&#20219;&#21153;&#30340;&#21487;&#33021;&#24212;&#29992;&#26102;&#65292;&#20165;&#22312;&#21333;&#20010;&#20219;&#21153;&#30340;&#23569;&#37327;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;&#30446;&#21069;&#36824;&#19981;&#28165;&#26970;&#22312;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24230;&#65288;STS&#65289;&#19978;&#26368;&#20808;&#36827;&#30340;&#23884;&#20837;&#26041;&#27861;&#26159;&#21542;&#21516;&#26679;&#36866;&#29992;&#20110;&#20854;&#20182;&#20219;&#21153;&#65292;&#27604;&#22914;&#32858;&#31867;&#25110;&#37325;&#26032;&#25490;&#24207;&#12290;&#36825;&#20351;&#24471;&#35780;&#20272;&#35813;&#39046;&#22495;&#30340;&#36827;&#23637;&#21464;&#24471;&#22256;&#38590;&#65292;&#22240;&#20026;&#21508;&#31181;&#27169;&#22411;&#19981;&#26029;&#34987;&#25552;&#20986;&#21364;&#27809;&#26377;&#24471;&#21040;&#36866;&#24403;&#30340;&#35780;&#20272;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22823;&#35268;&#27169;&#25991;&#26412;&#23884;&#20837;&#22522;&#20934;&#27979;&#35797;&#65288;MTEB&#65289;&#12290;MTEB&#28085;&#30422;8&#20010;&#23884;&#20837;&#20219;&#21153;&#65292;&#28085;&#30422;58&#20010;&#25968;&#25454;&#38598;&#21644;112&#20010;&#35821;&#35328;&#12290;&#36890;&#36807;&#22312;MTEB&#19978;&#23545;33&#20010;&#27169;&#22411;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#20840;&#38754;&#30340;&#25991;&#26412;&#23884;&#20837;&#22522;&#20934;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#27809;&#26377;&#20219;&#20309;&#19968;&#31181;&#29305;&#23450;&#30340;&#25991;&#26412;&#23884;&#20837;&#26041;&#27861;&#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#37117;&#21344;&#25454;&#20248;&#21183;&#12290;&#36825;&#34920;&#26126;&#35813;&#39046;&#22495;&#23578;&#26410;&#25910;&#25947;&#20110;&#19968;&#31181;&#36890;&#29992;&#30340;&#25991;&#26412;&#23884;&#20837;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#25193;&#23637;&#36275;&#22815;&#22823;&#20197;&#22312;&#25152;&#26377;&#23884;&#20837;&#20219;&#21153;&#20013;&#25552;&#20379;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;MTEB&#38468;&#24102;&#24320;&#28304;&#20195;&#30721;&#21644;&#25968;&#25454;&#65292;&#20197;&#20351;&#31038;&#21306;&#33021;&#22815;&#22522;&#20934;&#27979;&#35797;&#26032;&#30340;&#23884;&#20837;&#27169;&#22411;&#24182;&#36319;&#36394;&#35813;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-the-art results on all embedding tasks. MTEB comes with open-source code and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#22522;&#20110;&#29305;&#24449;&#31526;&#21512;&#39044;&#27979;&#30340;&#39044;&#27979;&#25512;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#28145;&#24230;&#34920;&#31034;&#23398;&#20064;&#30340;&#24402;&#32435;&#20559;&#32622;&#65292;&#25193;&#23637;&#20102;&#31526;&#21512;&#39044;&#27979;&#21040;&#35821;&#20041;&#29305;&#24449;&#31354;&#38388;&#12290;&#20174;&#29702;&#35770;&#21644;&#23454;&#39564;&#32467;&#26524;&#26469;&#30475;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#24120;&#35268;&#31526;&#21512;&#39044;&#27979;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#20219;&#21153;&#19978;&#23637;&#29616;&#20102;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.00173</link><description>&lt;p&gt;
&#22522;&#20110;&#29305;&#24449;&#31526;&#21512;&#39044;&#27979;&#30340;&#39044;&#27979;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Predictive Inference with Feature Conformal Prediction. (arXiv:2210.00173v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.00173
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#22522;&#20110;&#29305;&#24449;&#31526;&#21512;&#39044;&#27979;&#30340;&#39044;&#27979;&#25512;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#28145;&#24230;&#34920;&#31034;&#23398;&#20064;&#30340;&#24402;&#32435;&#20559;&#32622;&#65292;&#25193;&#23637;&#20102;&#31526;&#21512;&#39044;&#27979;&#21040;&#35821;&#20041;&#29305;&#24449;&#31354;&#38388;&#12290;&#20174;&#29702;&#35770;&#21644;&#23454;&#39564;&#32467;&#26524;&#26469;&#30475;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#24120;&#35268;&#31526;&#21512;&#39044;&#27979;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#20219;&#21153;&#19978;&#23637;&#29616;&#20102;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31526;&#21512;&#39044;&#27979;&#26159;&#19968;&#31181;&#26080;&#20998;&#24067;&#25216;&#26415;&#65292;&#29992;&#20110;&#24314;&#31435;&#26377;&#25928;&#30340;&#39044;&#27979;&#38388;&#38548;&#12290;&#34429;&#28982;&#20256;&#32479;&#19978;&#20154;&#20204;&#22312;&#36755;&#20986;&#31354;&#38388;&#20013;&#36827;&#34892;&#31526;&#21512;&#39044;&#27979;&#65292;&#20294;&#36825;&#24182;&#19981;&#26159;&#21807;&#19968;&#30340;&#21487;&#33021;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#29305;&#24449;&#30340;&#31526;&#21512;&#39044;&#27979;&#65292;&#36890;&#36807;&#21033;&#29992;&#28145;&#24230;&#34920;&#31034;&#23398;&#20064;&#30340;&#24402;&#32435;&#20559;&#32622;&#65292;&#25193;&#23637;&#20102;&#31526;&#21512;&#39044;&#27979;&#23545;&#35821;&#20041;&#29305;&#24449;&#31354;&#38388;&#30340;&#33539;&#22260;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22522;&#20110;&#29305;&#24449;&#30340;&#31526;&#21512;&#39044;&#27979;&#22312;&#28201;&#21644;&#20551;&#35774;&#19979;&#21487;&#20197;&#35777;&#26126;&#20248;&#20110;&#24120;&#35268;&#31526;&#21512;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#21487;&#20197;&#19982;&#26222;&#36890;&#31526;&#21512;&#39044;&#27979;&#32467;&#21512;&#20351;&#29992;&#65292;&#32780;&#19988;&#21487;&#20197;&#19982;&#20854;&#20182;&#33258;&#36866;&#24212;&#31526;&#21512;&#39044;&#27979;&#26041;&#27861;&#32467;&#21512;&#20351;&#29992;&#12290;&#38500;&#20102;&#29616;&#26377;&#39044;&#27979;&#25512;&#26029;&#22522;&#20934;&#27979;&#35797;&#30340;&#23454;&#39564;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#22823;&#35268;&#27169;&#20219;&#21153;&#65288;&#22914;ImageNet&#20998;&#31867;&#21644;Cityscapes&#22270;&#20687;&#20998;&#21106;&#65289;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conformal prediction is a distribution-free technique for establishing valid prediction intervals. Although conventionally people conduct conformal prediction in the output space, this is not the only possibility. In this paper, we propose feature conformal prediction, which extends the scope of conformal prediction to semantic feature spaces by leveraging the inductive bias of deep representation learning. From a theoretical perspective, we demonstrate that feature conformal prediction provably outperforms regular conformal prediction under mild assumptions. Our approach could be combined with not only vanilla conformal prediction, but also other adaptive conformal prediction methods. Apart from experiments on existing predictive inference benchmarks, we also demonstrate the state-of-the-art performance of the proposed methods on large-scale tasks such as ImageNet classification and Cityscapes image segmentation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#22312;&#23384;&#22312;&#25200;&#21160;&#24773;&#20917;&#19979;&#23433;&#20840;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#25506;&#32034;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#25152;&#25511;&#21046;&#23545;&#35937;&#21644;&#25200;&#21160;&#30340;&#37096;&#20998;&#20808;&#39564;&#30693;&#35782;&#65292;&#21487;&#20197;&#20445;&#35777;&#20197;&#39044;&#20808;&#25351;&#23450;&#30340;&#27010;&#29575;&#28385;&#36275;&#26174;&#24335;&#29366;&#24577;&#32422;&#26463;&#12290;</title><link>http://arxiv.org/abs/2209.15452</link><description>&lt;p&gt;
&#23384;&#22312;&#25200;&#21160;&#24773;&#20917;&#19979;&#24378;&#21270;&#23398;&#20064;&#30340;&#23433;&#20840;&#25506;&#32034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Safe Exploration Method for Reinforcement Learning under Existence of Disturbance. (arXiv:2209.15452v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.15452
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#22312;&#23384;&#22312;&#25200;&#21160;&#24773;&#20917;&#19979;&#23433;&#20840;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#25506;&#32034;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#25152;&#25511;&#21046;&#23545;&#35937;&#21644;&#25200;&#21160;&#30340;&#37096;&#20998;&#20808;&#39564;&#30693;&#35782;&#65292;&#21487;&#20197;&#20445;&#35777;&#20197;&#39044;&#20808;&#25351;&#23450;&#30340;&#27010;&#29575;&#28385;&#36275;&#26174;&#24335;&#29366;&#24577;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#24555;&#36895;&#21457;&#23637;&#22312;&#35768;&#22810;&#39046;&#22495;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#25506;&#32034;&#29305;&#24615;&#65292;&#24403;&#25105;&#20204;&#23558;&#36825;&#20123;&#31639;&#27861;&#24212;&#29992;&#20110;&#23433;&#20840;&#20851;&#38190;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#26102;&#65292;&#25105;&#20204;&#24517;&#39035;&#32771;&#34385;&#39118;&#38505;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28041;&#21450;&#23384;&#22312;&#25200;&#21160;&#26102;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#23433;&#20840;&#25506;&#32034;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#23433;&#20840;&#24615;&#23450;&#20041;&#20026;&#20197;&#29366;&#24577;&#26126;&#30830;&#23450;&#20041;&#30340;&#38480;&#21046;&#26465;&#20214;&#30340;&#28385;&#36275;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;&#25506;&#32034;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#25152;&#25511;&#21046;&#23545;&#35937;&#21644;&#25200;&#21160;&#30340;&#37096;&#20998;&#20808;&#39564;&#30693;&#35782;&#12290;&#21363;&#20351;&#25152;&#25511;&#21046;&#23545;&#35937;&#26292;&#38706;&#20110;&#36981;&#24490;&#27491;&#24577;&#20998;&#24067;&#30340;&#38543;&#26426;&#25200;&#21160;&#65292;&#35813;&#26041;&#27861;&#20063;&#33021;&#20445;&#35777;&#20197;&#39044;&#20808;&#25351;&#23450;&#30340;&#27010;&#29575;&#28385;&#36275;&#26174;&#24335;&#29366;&#24577;&#32422;&#26463;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36275;&#22815;&#30340;&#26465;&#20214;&#26469;&#26500;&#24314;&#19981;&#21253;&#21547;&#20256;&#32479;&#25506;&#32034;&#26041;&#27861;&#20013;&#30340;&#25506;&#32034;&#22240;&#32032;&#30340;&#20445;&#23432;&#36755;&#20837;&#12290;&#22312;&#25670;&#21160;&#20219;&#21153;&#30340;&#27169;&#25311;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#28385;&#36275;&#26174;&#24335;&#32422;&#26463;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#23433;&#20840;&#22320;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent rapid developments in reinforcement learning algorithms have been giving us novel possibilities in many fields. However, due to their exploring property, we have to take the risk into consideration when we apply those algorithms to safety-critical problems especially in real environments. In this study, we deal with a safe exploration problem in reinforcement learning under the existence of disturbance. We define the safety during learning as satisfaction of the constraint conditions explicitly defined in terms of the state and propose a safe exploration method that uses partial prior knowledge of a controlled object and disturbance. The proposed method assures the satisfaction of the explicit state constraints with a pre-specified probability even if the controlled object is exposed to a stochastic disturbance following a normal distribution. As theoretical results, we introduce sufficient conditions to construct conservative inputs not containing an exploring aspect used in th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#38463;&#37324;&#24052;&#24052;&#38598;&#22242;&#30340;&#31354;&#21069;&#35268;&#27169;&#30340; OpenBG &#21830;&#19994;&#30693;&#35782;&#22270;&#35889;&#65292;&#21253;&#21547;&#36229;&#36807; 88 &#30334;&#19975;&#23454;&#20307;&#21644; 26 &#20159;&#19977;&#20803;&#32452;&#12290;&#23427;&#20855;&#26377;&#31934;&#32454;&#30340;&#20998;&#31867;&#21644;&#22810;&#27169;&#24577;&#20107;&#23454;&#65292;&#26377;&#21161;&#20110;&#25512;&#21160;&#21830;&#19994;&#26234;&#33021;&#21270;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2209.15214</link><description>&lt;p&gt;
&#21313;&#20159;&#32423;&#39044;&#35757;&#32451;&#22810;&#27169;&#24577;&#21830;&#19994;&#30693;&#35782;&#22270;&#35889;&#30340;&#26500;&#24314;&#19982;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Construction and Applications of Billion-Scale Pre-Trained Multimodal Business Knowledge Graph. (arXiv:2209.15214v6 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.15214
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#38463;&#37324;&#24052;&#24052;&#38598;&#22242;&#30340;&#31354;&#21069;&#35268;&#27169;&#30340; OpenBG &#21830;&#19994;&#30693;&#35782;&#22270;&#35889;&#65292;&#21253;&#21547;&#36229;&#36807; 88 &#30334;&#19975;&#23454;&#20307;&#21644; 26 &#20159;&#19977;&#20803;&#32452;&#12290;&#23427;&#20855;&#26377;&#31934;&#32454;&#30340;&#20998;&#31867;&#21644;&#22810;&#27169;&#24577;&#20107;&#23454;&#65292;&#26377;&#21161;&#20110;&#25512;&#21160;&#21830;&#19994;&#26234;&#33021;&#21270;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21830;&#19994;&#30693;&#35782;&#22270;&#35889;&#26159;&#24403;&#21069;&#35768;&#22810;&#20225;&#19994;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#20026;&#35768;&#22810;&#20135;&#21697;&#25552;&#20379;&#20107;&#23454;&#30693;&#35782;&#21644;&#32467;&#26500;&#21270;&#25968;&#25454;&#65292;&#20351;&#23427;&#20204;&#21464;&#24471;&#26356;&#21152;&#26234;&#33021;&#21270;&#12290;&#23613;&#31649;&#23427;&#20204;&#26377;&#30528;&#35768;&#22810;&#28508;&#22312;&#30340;&#22909;&#22788;&#65292;&#20294;&#26500;&#24314;&#21830;&#19994;&#30693;&#35782;&#22270;&#35889;&#38656;&#35201;&#35299;&#20915;&#32467;&#26500;&#19981;&#36275;&#21644;&#22810;&#27169;&#24577;&#30340;&#38480;&#21046;&#31561;&#38382;&#39064;&#12290;&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#22312;&#38750;&#24494;&#19981;&#36275;&#36947;&#30340;&#23454;&#38469;&#24212;&#29992;&#31995;&#32479;&#20013;&#26500;&#24314;&#30693;&#35782;&#22270;&#35889;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#38463;&#37324;&#24052;&#24052;&#38598;&#22242;&#30340; OpenBG &#21830;&#19994;&#30693;&#35782;&#22270;&#35889;&#30340;&#26500;&#24314;&#36807;&#31243;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#26680;&#24515;&#26412;&#20307;&#65292;&#28085;&#30422;&#21508;&#31181;&#25277;&#35937;&#20135;&#21697;&#21644;&#28040;&#36153;&#38656;&#27714;&#65292;&#24182;&#22312;&#37096;&#32626;&#30340;&#24212;&#29992;&#20013;&#25552;&#20379;&#31934;&#32454;&#30340;&#20998;&#31867;&#21644;&#22810;&#27169;&#24577;&#20107;&#23454;&#12290;OpenBG &#26159;&#19968;&#20010;&#31354;&#21069;&#35268;&#27169;&#30340;&#21830;&#19994;&#30693;&#35782;&#22270;&#35889;&#65306;&#21253;&#21547;&#36229;&#36807; 88 &#30334;&#19975;&#23454;&#20307;&#12289;&#35206;&#30422;&#36229;&#36807; 1 &#30334;&#19975;&#20010;&#26680;&#24515;&#31867;/&#27010;&#24565;&#21644; 2,681 &#31181;&#20851;&#31995;&#30340; 26 &#20159;&#19977;&#20803;&#32452;&#12290;&#25105;&#20204;&#20844;&#24320;&#20102;&#25152;&#26377;&#30340;&#36164;&#28304;&#21644;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20197;&#20419;&#36827;&#30693;&#35782;&#22270;&#35889;&#30340;&#21457;&#23637;&#21644;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Business Knowledge Graphs (KGs) are important to many enterprises today, providing factual knowledge and structured data that steer many products and make them more intelligent. Despite their promising benefits, building business KG necessitates solving prohibitive issues of deficient structure and multiple modalities. In this paper, we advance the understanding of the practical challenges related to building KG in non-trivial real-world systems. We introduce the process of building an open business knowledge graph (OpenBG) derived from a well-known enterprise, Alibaba Group. Specifically, we define a core ontology to cover various abstract products and consumption demands, with fine-grained taxonomy and multimodal facts in deployed applications. OpenBG is an open business KG of unprecedented scale: 2.6 billion triples with more than 88 million entities covering over 1 million core classes/concepts and 2,681 types of relations. We release all the open resources (OpenBG benchmarks) deri
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#34394;&#25311;&#29616;&#23454;&#27169;&#25311;&#25910;&#38598;&#30340;&#21333;&#20010;&#20154;&#31867;&#31034;&#20363;&#36741;&#21161;RL&#35757;&#32451;&#65292;&#20174;&#32780;&#22312;&#26368;&#23567;&#21270;&#20154;&#31867;&#24178;&#39044;&#30340;&#21516;&#26102;&#20445;&#30041;&#20102;&#20351;&#29992;&#20154;&#31867;&#31034;&#33539;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#29992;&#21333;&#20010;&#31034;&#20363;&#29983;&#25104;&#30340;&#22810;&#20010;&#28436;&#31034;&#21487;&#26174;&#33879;&#25552;&#39640;&#35757;&#32451;&#36895;&#24230;&#65292;&#24182;&#20351;&#26234;&#33021;&#20307;&#33021;&#22815;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2209.11275</link><description>&lt;p&gt;
&#20943;&#23569;&#20154;&#24037;&#24178;&#39044;&#65306;&#22686;&#24378;&#21333;&#19968;&#28436;&#31034;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Minimizing Human Assistance: Augmenting a Single Demonstration for Deep Reinforcement Learning. (arXiv:2209.11275v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.11275
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#34394;&#25311;&#29616;&#23454;&#27169;&#25311;&#25910;&#38598;&#30340;&#21333;&#20010;&#20154;&#31867;&#31034;&#20363;&#36741;&#21161;RL&#35757;&#32451;&#65292;&#20174;&#32780;&#22312;&#26368;&#23567;&#21270;&#20154;&#31867;&#24178;&#39044;&#30340;&#21516;&#26102;&#20445;&#30041;&#20102;&#20351;&#29992;&#20154;&#31867;&#31034;&#33539;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#29992;&#21333;&#20010;&#31034;&#20363;&#29983;&#25104;&#30340;&#22810;&#20010;&#28436;&#31034;&#21487;&#26174;&#33879;&#25552;&#39640;&#35757;&#32451;&#36895;&#24230;&#65292;&#24182;&#20351;&#26234;&#33021;&#20307;&#33021;&#22815;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#20351;&#29992;&#20154;&#31867;&#31034;&#33539;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#26234;&#33021;&#20307;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#20219;&#20309;&#38656;&#35201;&#20154;&#20026;&#22320;&#25163;&#21160;&#8220;&#25945;&#25480;&#8221;&#27169;&#22411;&#30340;&#35201;&#27714;&#37117;&#19982;&#21152;&#24378;&#23398;&#20064;&#30340;&#30446;&#26631;&#30456;&#20914;&#31361;&#12290;&#26412;&#25991;&#23581;&#35797;&#22312;&#20445;&#30041;&#24615;&#33021;&#20248;&#21183;&#30340;&#21516;&#26102;&#65292;&#26368;&#23567;&#21270;&#20154;&#31867;&#21442;&#19982;&#23398;&#20064;&#36807;&#31243;&#65292;&#20351;&#29992;&#36890;&#36807;&#31616;&#21333;&#26131;&#29992;&#30340;&#34394;&#25311;&#29616;&#23454;&#27169;&#25311;&#25910;&#38598;&#30340;&#21333;&#20010;&#20154;&#31867;&#31034;&#20363;&#26469;&#21327;&#21161;RL&#22521;&#35757;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22686;&#24378;&#20102;&#21333;&#20010;&#28436;&#31034;&#65292;&#29983;&#25104;&#20102;&#35768;&#22810;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#28436;&#31034;&#12290;&#24403;&#36825;&#20123;&#28436;&#31034;&#19982;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#21644;&#21518;&#35265;&#20043;&#26126;&#30340;&#32463;&#39564;&#37325;&#25918;&#65288;DDPG + HER&#65289;&#30456;&#32467;&#21512;&#26102;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#31616;&#21333;&#20219;&#21153;&#30340;&#35757;&#32451;&#26102;&#38388;&#65292;&#24182;&#20351;&#26234;&#33021;&#20307;&#33021;&#22815;&#35299;&#20915;DDPG + HER&#26080;&#27861;&#35299;&#20915;&#30340;&#22797;&#26434;&#20219;&#21153;&#65288;&#22359;&#21472;&#25918;&#65289;&#12290;&#35813;&#27169;&#22411;&#20351;&#29992;&#21333;&#20010;&#20154;&#31867;&#31034;&#20363;&#23454;&#29616;&#20102;&#36825;&#19968;&#26174;&#33879;&#30340;&#35757;&#32451;&#20248;&#21183;&#65292;&#38656;&#35201;&#19981;&#21040;&#19968;&#20998;&#38047;&#30340;&#20154;&#31867;&#36755;&#20837;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;&#20174;&#20154;&#31867;&#31034;&#20363;&#20013;&#23398;&#20064;&#65292;&#20294;&#20195;&#29702;&#30340;&#36816;&#34892;&#34920;&#29616;&#19982;&#21407;&#22987;&#28436;&#31034;&#32773;&#30340;&#34892;&#20026;&#39118;&#26684;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of human demonstrations in reinforcement learning has proven to significantly improve agent performance. However, any requirement for a human to manually 'teach' the model is somewhat antithetical to the goals of reinforcement learning. This paper attempts to minimize human involvement in the learning process while retaining the performance advantages by using a single human example collected through a simple-to-use virtual reality simulation to assist with RL training. Our method augments a single demonstration to generate numerous human-like demonstrations that, when combined with Deep Deterministic Policy Gradients and Hindsight Experience Replay (DDPG + HER) significantly improve training time on simple tasks and allows the agent to solve a complex task (block stacking) that DDPG + HER alone cannot solve. The model achieves this significant training advantage using a single human example, requiring less than a minute of human input. Moreover, despite learning from a human e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20048;&#35266;&#21407;&#21017;&#21644;&#31163;&#32447;&#22270;&#24418;&#35268;&#21010;&#31639;&#27861;&#30340;&#23398;&#20064;&#31639;&#27861;G-UCB&#65292;&#33021;&#22815;&#24179;&#34913;&#38271;&#26399;&#25506;&#32034;&#21033;&#29992;&#65292;&#29992;&#20110;&#35299;&#20915;&#19968;&#31181;&#21517;&#20026;&#22270;&#36172;&#21338;&#26426;&#30340;MAB&#25193;&#23637;&#65292;&#20174;&#32780;&#33719;&#24471;&#26368;&#22823;&#21270;&#30340;&#25910;&#30410;&#12290;</title><link>http://arxiv.org/abs/2209.09419</link><description>&lt;p&gt;
&#19968;&#31181;&#22270;&#19978;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Multi-armed Bandit Learning on a Graph. (arXiv:2209.09419v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.09419
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20048;&#35266;&#21407;&#21017;&#21644;&#31163;&#32447;&#22270;&#24418;&#35268;&#21010;&#31639;&#27861;&#30340;&#23398;&#20064;&#31639;&#27861;G-UCB&#65292;&#33021;&#22815;&#24179;&#34913;&#38271;&#26399;&#25506;&#32034;&#21033;&#29992;&#65292;&#29992;&#20110;&#35299;&#20915;&#19968;&#31181;&#21517;&#20026;&#22270;&#36172;&#21338;&#26426;&#30340;MAB&#25193;&#23637;&#65292;&#20174;&#32780;&#33719;&#24471;&#26368;&#22823;&#21270;&#30340;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#26159;&#19968;&#20010;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#26694;&#26550;&#65292;&#22312;&#19981;&#30830;&#23450;&#24615;&#20915;&#31574;&#26041;&#38754;&#24050;&#32463;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#12290;&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#20363;&#22914;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#65292;&#36873;&#25321;&#19968;&#20010;&#33218;&#23545;&#24212;&#20110;&#38480;&#21046;&#19979;&#19968;&#20010;&#21487;&#29992;&#33218;&#65288;&#21160;&#20316;&#65289;&#30340;&#36873;&#25321;&#12290;&#20986;&#20110;&#36825;&#20010;&#30446;&#30340;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#21517;&#20026;&#22270;&#36172;&#21338;&#26426;&#30340;MAB&#25193;&#23637;&#65292;&#22312;&#27492;&#36807;&#31243;&#20013;&#65292;&#26234;&#33021;&#20307;&#20174;&#19981;&#21516;&#33410;&#28857;&#20013;&#25910;&#38598;&#22870;&#21169;&#20197;&#33719;&#24471;&#26368;&#22823;&#21270;&#30340;&#25910;&#30410;&#12290;&#22270;&#23450;&#20041;&#20102;&#26234;&#33021;&#20307;&#22312;&#27599;&#19968;&#27493;&#20013;&#36873;&#25321;&#19979;&#19968;&#20010;&#21487;&#29992;&#33410;&#28857;&#30340;&#33258;&#30001;&#24230;&#12290;&#25105;&#20204;&#20551;&#35774;&#22270;&#30340;&#32467;&#26500;&#26159;&#23436;&#20840;&#21487;&#29992;&#30340;&#65292;&#20294;&#22870;&#21169;&#20998;&#24067;&#26159;&#26410;&#30693;&#30340;&#12290;&#22522;&#20110;&#31163;&#32447;&#22270;&#24418;&#35268;&#21010;&#31639;&#27861;&#21644;&#20048;&#35266;&#21407;&#21017;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#23398;&#20064;&#31639;&#27861;G-UCB&#65292;&#24179;&#34913;&#38271;&#26399;&#25506;&#32034;&#21033;&#29992;&#20351;&#29992;&#20048;&#35266;&#21407;&#21017;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#36798;&#21040;&#20102;$O(\sqrt{|S|T\log(T)}+D|S|\log T)$&#30340;&#23398;&#20064;&#36951;&#25022;&#12290;&#20854;&#20013;$|S|$&#26159;
&lt;/p&gt;
&lt;p&gt;
The multi-armed bandit(MAB) problem is a simple yet powerful framework that has been extensively studied in the context of decision-making under uncertainty. In many real-world applications, such as robotic applications, selecting an arm corresponds to a physical action that constrains the choices of the next available arms (actions). Motivated by this, we study an extension of MAB called the graph bandit, where an agent travels over a graph to maximize the reward collected from different nodes. The graph defines the agent's freedom in selecting the next available nodes at each step. We assume the graph structure is fully available, but the reward distributions are unknown. Built upon an offline graph-based planning algorithm and the principle of optimism, we design a learning algorithm, G-UCB, that balances long-term exploration-exploitation using the principle of optimism. We show that our proposed algorithm achieves $O(\sqrt{|S|T\log(T)}+D|S|\log T)$ learning regret, where $|S|$ is 
&lt;/p&gt;</description></item><item><title>EcoFormer&#26159;&#19968;&#31181;&#36890;&#36807;&#26680;&#21704;&#24076;&#25216;&#26415;&#36827;&#34892;&#39640;&#32500;softmax&#27880;&#24847;&#21147;&#20108;&#20540;&#21270;&#30340;&#26032;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#20855;&#26377;&#32447;&#24615;&#26102;&#38388;&#22797;&#26434;&#24230;&#30340;&#39640;&#25928;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#20174;&#32780;&#26174;&#33879;&#38477;&#20302;&#20102;&#35745;&#31639;&#21644;&#33021;&#37327;&#24320;&#38144;&#65292;&#21516;&#26102;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.09004</link><description>&lt;p&gt;
EcoFormer&#65306;&#20855;&#26377;&#32447;&#24615;&#22797;&#26434;&#24230;&#30340;&#33410;&#33021;&#27880;&#24847;&#21147;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
EcoFormer: Energy-Saving Attention with Linear Complexity. (arXiv:2209.09004v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.09004
&lt;/p&gt;
&lt;p&gt;
EcoFormer&#26159;&#19968;&#31181;&#36890;&#36807;&#26680;&#21704;&#24076;&#25216;&#26415;&#36827;&#34892;&#39640;&#32500;softmax&#27880;&#24847;&#21147;&#20108;&#20540;&#21270;&#30340;&#26032;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#20855;&#26377;&#32447;&#24615;&#26102;&#38388;&#22797;&#26434;&#24230;&#30340;&#39640;&#25928;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#20174;&#32780;&#26174;&#33879;&#38477;&#20302;&#20102;&#35745;&#31639;&#21644;&#33021;&#37327;&#24320;&#38144;&#65292;&#21516;&#26102;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#26159;&#19968;&#31181;&#29992;&#20110;&#24314;&#27169;&#24207;&#21015;&#25968;&#25454;&#30340;&#38761;&#21629;&#24615;&#26694;&#26550;&#65292;&#22312;&#24191;&#27867;&#30340;&#20219;&#21153;&#20013;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#20294;&#20854;&#39640;&#35745;&#31639;&#37327;&#21644;&#33021;&#28304;&#25104;&#26412;&#38480;&#21046;&#20102;&#20854;&#23454;&#38469;&#24212;&#29992;&#12290;&#20026;&#20102;&#25552;&#39640;Transformer&#30340;&#25928;&#29575;&#65292;&#21387;&#32553;&#27169;&#22411;&#26159;&#19968;&#20010;&#21463;&#27426;&#36814;&#30340;&#36873;&#25321;&#65292;&#20854;&#20013;&#26368;&#24120;&#29992;&#30340;&#26041;&#27861;&#26159;&#36890;&#36807;&#20108;&#20540;&#21270;&#26469;&#23558;&#28014;&#28857;&#20540;&#38480;&#21046;&#20026;&#20108;&#36827;&#21046;&#20540;&#65292;&#20197;&#20415;&#20110;&#36827;&#34892;&#20301;&#36816;&#31639;&#20174;&#32780;&#33410;&#30465;&#35745;&#31639;&#21644;&#33021;&#28304;&#24320;&#38144;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20108;&#20540;&#21270;&#26041;&#27861;&#21482;&#27880;&#37325;&#26368;&#22823;&#21270;&#32479;&#35745;&#19978;&#30340;&#36755;&#20837;&#20998;&#24067;&#20449;&#24687;&#32780;&#24573;&#30053;&#20102;&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#26680;&#24515;&#30340;&#30456;&#20284;&#24230;&#24314;&#27169;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20108;&#20540;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#26680;&#21704;&#24076;&#25216;&#26415;&#23545;&#39640;&#32500;softmax&#27880;&#24847;&#21147;&#26426;&#21046;&#36827;&#34892;&#23450;&#21046;&#21270;&#22788;&#29702;&#65292;&#23558;&#21407;&#22987;&#26597;&#35810;&#21644;&#38190;&#23884;&#20837;&#21040;&#21704;&#26126;&#31354;&#38388;&#30340;&#20302;&#32500;&#20108;&#36827;&#21046;&#32534;&#30721;&#20013;&#12290;&#26680;&#21704;&#24076;&#20989;&#25968;&#26159;&#20197;&#33258;&#30417;&#30563;&#26041;&#24335;&#20174;&#27880;&#24847;&#21147;&#22270;&#20013;&#25552;&#21462;&#22522;&#26412;&#20851;&#31995;&#30340;&#30456;&#20284;&#24230;&#25152;&#23398;&#20064;&#30340;&#12290;&#22312;&#36825;&#20123;&#20108;&#36827;&#21046;&#32534;&#30721;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#20855;&#26377;&#32447;&#24615;&#26102;&#38388;&#22797;&#26434;&#24230;&#30340;&#39640;&#25928;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#20174;&#32780;&#26174;&#33879;&#38477;&#20302;&#20102;&#35745;&#31639;&#21644;&#33021;&#37327;&#24320;&#38144;&#65292;&#30456;&#27604;&#20110;&#22522;&#26412;&#27169;&#22411;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;&#24615;&#33021;&#21644;&#21487;&#27604;&#24615;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#26469;&#39564;&#35777;&#20854;&#22312;&#22270;&#20687;&#20998;&#31867;&#12289;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#21644;&#26426;&#22120;&#32763;&#35793;&#31561;&#22810;&#20010;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer is a transformative framework that models sequential data and has achieved remarkable performance on a wide range of tasks, but with high computational and energy cost. To improve its efficiency, a popular choice is to compress the models via binarization which constrains the floating-point values into binary ones to save resource consumption owing to cheap bitwise operations significantly. However, existing binarization methods only aim at minimizing the information loss for the input distribution statistically, while ignoring the pairwise similarity modeling at the core of the attention. To this end, we propose a new binarization paradigm customized to high-dimensional softmax attention via kernelized hashing, called EcoFormer, to map the original queries and keys into low-dimensional binary codes in Hamming space. The kernelized hash functions are learned to match the ground-truth similarity relations extracted from the attention map in a self-supervised way. Based on th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21452;&#35889;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#20174;&#25968;&#25454;&#30340;&#38544;&#21547;&#23545;&#31216;&#24615;&#20013;&#23398;&#20064;&#32676;&#12289;&#19981;&#21487;&#32422;&#34920;&#31034;&#21644;&#23545;&#24212;&#30340;&#23436;&#20840;&#19981;&#21464;&#26144;&#23556;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#22522;&#20110;&#19981;&#21464;&#24615;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2209.03416</link><description>&lt;p&gt;
&#21452;&#35889;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Bispectral Neural Networks. (arXiv:2209.03416v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.03416
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21452;&#35889;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#20174;&#25968;&#25454;&#30340;&#38544;&#21547;&#23545;&#31216;&#24615;&#20013;&#23398;&#20064;&#32676;&#12289;&#19981;&#21487;&#32422;&#34920;&#31034;&#21644;&#23545;&#24212;&#30340;&#23436;&#20840;&#19981;&#21464;&#26144;&#23556;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#22522;&#20110;&#19981;&#21464;&#24615;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#21452;&#35889;&#31070;&#32463;&#32593;&#32476;(BNNs)&#65292;&#29992;&#20110;&#23398;&#20064;&#34920;&#31034;&#22312;&#32039;&#33268;&#21487;&#20132;&#25442;&#32676;&#22312;&#23450;&#20041;&#20449;&#21495;&#30340;&#31354;&#38388;&#19978;&#30340;&#20316;&#29992;&#19979;&#20855;&#26377;&#19981;&#21464;&#24615;&#30340;&#34920;&#31034;&#12290;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;&#21452;&#35889;&#30340;&#24605;&#24819;&#65292;&#21363;&#26159;&#19968;&#20010;&#35299;&#26512;&#23450;&#20041;&#30340;&#32676;&#19981;&#21464;&#37327;&#65292;&#23427;&#26159;&#23436;&#25972;&#30340;&#8212;&#8212;&#20063;&#23601;&#26159;&#35828;&#65292;&#23427;&#20445;&#30041;&#20102;&#25152;&#26377;&#20449;&#21495;&#32467;&#26500;&#65292;&#21516;&#26102;&#21482;&#21435;&#38500;&#20102;&#30001;&#20110;&#32676;&#20316;&#29992;&#24341;&#36215;&#30340;&#21464;&#21270;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;BNNs&#33021;&#22815;&#36890;&#36807;&#25968;&#25454;&#20013;&#30340;&#38544;&#21547;&#23545;&#31216;&#24615;&#21516;&#26102;&#23398;&#20064;&#32676;&#12289;&#23427;&#20204;&#30340;&#19981;&#21487;&#32422;&#34920;&#31034;&#21644;&#23545;&#24212;&#30340;&#31561;&#21464;&#21644;&#23436;&#20840;&#19981;&#21464;&#26144;&#23556;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23436;&#25972;&#24615;&#23646;&#24615;&#36171;&#20104;&#20102;&#36825;&#20123;&#32593;&#32476;&#24378;&#22823;&#30340;&#22522;&#20110;&#19981;&#21464;&#24615;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#36825;&#39033;&#24037;&#20316;&#23558;Bispectral Neural Networks&#30830;&#31435;&#20026;&#31283;&#20581;&#19981;&#21464;&#34920;&#31034;&#23398;&#20064;&#30340;&#24378;&#22823;&#35745;&#31639;&#21407;&#35821;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a neural network architecture, Bispectral Neural Networks (BNNs) for learning representations that are invariant to the actions of compact commutative groups on the space over which a signal is defined. The model incorporates the ansatz of the bispectrum, an analytically defined group invariant that is complete -- that is, it preserves all signal structure while removing only the variation due to group actions. Here, we demonstrate that BNNs are able to simultaneously learn groups, their irreducible representations, and corresponding equivariant and complete-invariant maps purely from the symmetries implicit in data. Further, we demonstrate that the completeness property endows these networks with strong invariance-based adversarial robustness. This work establishes Bispectral Neural Networks as a powerful computational primitive for robust invariant representation learning
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#22810;&#26234;&#33021;&#20307;&#23454;&#39564;&#23460;&#20248;&#21270;&#26694;&#26550;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#25511;&#21046;&#35774;&#22791;&#20351;&#29992;&#65292;&#35753;&#20855;&#26377;&#19981;&#21516;&#23398;&#20064;&#33021;&#21147;&#21644;&#30446;&#26631;&#30340;&#26426;&#22120;&#23398;&#20064;&#26234;&#33021;&#20307;&#21327;&#21516;&#36816;&#34892;&#30740;&#31350;&#27963;&#21160;&#12290;</title><link>http://arxiv.org/abs/2208.09099</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#22810;&#26234;&#33021;&#20307;&#23454;&#39564;&#23460;&#20248;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Scalable Multi-Agent Lab Framework for Lab Optimization. (arXiv:2208.09099v3 [cs.MA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.09099
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#22810;&#26234;&#33021;&#20307;&#23454;&#39564;&#23460;&#20248;&#21270;&#26694;&#26550;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#25511;&#21046;&#35774;&#22791;&#20351;&#29992;&#65292;&#35753;&#20855;&#26377;&#19981;&#21516;&#23398;&#20064;&#33021;&#21147;&#21644;&#30446;&#26631;&#30340;&#26426;&#22120;&#23398;&#20064;&#26234;&#33021;&#20307;&#21327;&#21516;&#36816;&#34892;&#30740;&#31350;&#27963;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#26448;&#26009;&#30740;&#31350;&#31995;&#32479;&#35753;&#31185;&#23398;&#23478;&#33021;&#22815;&#26356;&#21152;&#32874;&#26126;&#22320;&#22833;&#36133;&#12289;&#26356;&#24555;&#22320;&#23398;&#20064;&#65292;&#20197;&#21450;&#22312;&#30740;&#31350;&#20013;&#33410;&#30465;&#26356;&#23569;&#30340;&#36164;&#28304;&#12290;&#38543;&#30528;&#36825;&#20123;&#31995;&#32479;&#22312;&#25968;&#37327;&#12289;&#33021;&#21147;&#21644;&#22797;&#26434;&#24615;&#19978;&#30340;&#22686;&#38271;&#65292;&#19968;&#20010;&#26032;&#30340;&#25361;&#25112;&#20986;&#29616;&#20102;&#8212;&#8212;&#22914;&#20309;&#35753;&#23427;&#20204;&#22312;&#22823;&#22411;&#35774;&#26045;&#20013;&#20849;&#21516;&#24037;&#20316;&#21602;&#65311;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#31181;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#8212;&#8212;&#22810;&#26234;&#33021;&#20307;&#23454;&#39564;&#23460;&#25511;&#21046;&#26694;&#26550;&#12290;&#25105;&#20204;&#20197;&#33258;&#20027;&#26448;&#26009;&#31185;&#23398;&#23454;&#39564;&#23460;&#20026;&#20363;&#26469;&#28436;&#31034;&#36825;&#20010;&#26694;&#26550;&#65292;&#20854;&#20013;&#26469;&#33258;&#19981;&#21516;&#30740;&#31350;&#27963;&#21160;&#30340;&#20449;&#24687;&#21487;&#20197;&#34987;&#21512;&#24182;&#26469;&#35299;&#20915;&#25163;&#22836;&#30340;&#31185;&#23398;&#38382;&#39064;&#12290;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197; 1&#65289;&#32771;&#34385;&#21040;&#29616;&#23454;&#36164;&#28304;&#38480;&#21046;&#65292;&#22914;&#35774;&#22791;&#20351;&#29992;&#65307;2&#65289;&#20801;&#35768;&#20855;&#26377;&#19981;&#21516;&#23398;&#20064;&#33021;&#21147;&#21644;&#30446;&#26631;&#30340;&#26426;&#22120;&#23398;&#20064;&#26234;&#33021;&#20307;&#26469;&#36816;&#34892;&#30740;&#31350;&#27963;&#21160;&#65307;&#20197;&#21450; 3&#65289;&#20419;&#36827;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#21644;&#22242;&#38431;&#12290;&#35813;&#26694;&#26550;&#34987;&#31216;&#20026;&#21487;&#25193;&#23637;&#30340;&#22810;&#26234;&#33021;&#20307;&#33258;&#20027;&#35774;&#26045;&#26694;&#26550;&#65292;&#21363;MULTITASK&#12290;MULTITASK&#20351;&#24471;&#35774;&#26045;&#33539;&#22260;&#20869;&#30340;&#27169;&#25311;&#25104;&#20026;&#21487;&#33021;&#65292;&#21253;&#25324;&#26234;&#33021;&#20307;-&#20202;&#22120;&#21644;&#26234;&#33021;&#20307;-&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous materials research systems allow scientists to fail smarter, learn faster, and spend less resources in their studies. As these systems grow in number, capability, and complexity, a new challenge arises - how will they work together across large facilities? We explore one solution to this question - a multi-agent laboratory control frame-work. We demonstrate this framework with an autonomous material science lab in mind - where information from diverse research campaigns can be combined to ad-dress the scientific question at hand. This framework can 1) account for realistic resource limits such as equipment use, 2) allow for machine learning agents with diverse learning capabilities and goals capable of running re-search campaigns, and 3) facilitate multi-agent collaborations and teams. The framework is dubbed the MULTI-agent auTonomous fAcilities - a Scalable frameworK aka MULTITASK. MULTITASK makes possible facility-wide simulations, including agent-instrument and agent-age
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"S-Prompting"&#30340;&#23398;&#20064;&#33539;&#20363;&#65292;&#20351;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#21464;&#21387;&#22120;&#29420;&#31435;&#22320;&#36328;&#36234;&#19981;&#21516;&#39046;&#22495;&#23398;&#20064;&#25552;&#31034;&#65292;&#22823;&#24133;&#24230;&#20943;&#23569;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#36951;&#24536;&#38382;&#39064;&#65292;&#24182;&#20855;&#26377;&#20986;&#33394;&#30340;&#21487;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2207.12819</link><description>&lt;p&gt;
&#20855;&#26377;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#30340;S-Prompts&#23398;&#20064;&#65306;&#39046;&#22495;&#22686;&#37327;&#23398;&#20064;&#30340;&#22885;&#21345;&#22982;&#21059;&#20992;
&lt;/p&gt;
&lt;p&gt;
S-Prompts Learning with Pre-trained Transformers: An Occam's Razor for Domain Incremental Learning. (arXiv:2207.12819v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.12819
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"S-Prompting"&#30340;&#23398;&#20064;&#33539;&#20363;&#65292;&#20351;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#21464;&#21387;&#22120;&#29420;&#31435;&#22320;&#36328;&#36234;&#19981;&#21516;&#39046;&#22495;&#23398;&#20064;&#25552;&#31034;&#65292;&#22823;&#24133;&#24230;&#20943;&#23569;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#36951;&#24536;&#38382;&#39064;&#65292;&#24182;&#20855;&#26377;&#20986;&#33394;&#30340;&#21487;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20173;&#28982;&#26080;&#27861;&#35299;&#20915;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#33539;&#20363;&#65288;&#31216;&#20026;S-Prompting&#65289;&#21644;&#20004;&#31181;&#20855;&#20307;&#26041;&#27861;&#65292;&#26497;&#22823;&#22320;&#20943;&#23569;&#20102;&#20854;&#20013;&#19968;&#31181;&#26368;&#20856;&#22411;&#30340;&#36830;&#32493;&#23398;&#20064;&#22330;&#26223;&#65288;&#21363;&#39046;&#22495;&#22686;&#37327;&#23398;&#20064;&#65289;&#20013;&#30340;&#36951;&#24536;&#31243;&#24230;&#12290;&#35813;&#33539;&#20363;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#20351;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#21464;&#21387;&#22120;&#29420;&#31435;&#22320;&#36328;&#36234;&#19981;&#21516;&#39046;&#22495;&#23398;&#20064;&#25552;&#31034;&#65292;&#24182;&#36991;&#20813;&#24120;&#35265;&#20110;&#20256;&#32479;&#26041;&#27861;&#20013;&#30340;&#31034;&#20363;&#20351;&#29992;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#20010;&#21452;&#36194;&#30340;&#23616;&#38754;&#65292;&#21363;&#25552;&#31034;&#21487;&#20197;&#20026;&#27599;&#20010;&#39046;&#22495;&#21462;&#24471;&#26368;&#20339;&#25928;&#26524;&#12290;&#36328;&#36234;&#39046;&#22495;&#30340;&#29420;&#31435;&#25552;&#31034;&#20165;&#38656;&#35201;&#19968;&#27425;&#20132;&#21449;&#29109;&#25439;&#22833;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#38656;&#35201;&#19968;&#20010;&#31616;&#21333;&#30340;K-NN&#25805;&#20316;&#20316;&#20026;&#25512;&#29702;&#30340;&#39046;&#22495;&#26631;&#35782;&#31526;&#12290;&#35813;&#23398;&#20064;&#33539;&#20363;&#27966;&#29983;&#20102;&#19968;&#31181;&#22270;&#20687;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#21644;&#19968;&#31181;&#26032;&#39062;&#30340;&#35821;&#35328;-&#22270;&#20687;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#12290;&#20855;&#26377;&#20986;&#33394;&#30340;&#21487;&#20280;&#32553;&#24615;&#65288;&#27599;&#20010;&#39046;&#22495;&#22686;&#21152;0.03&#65285;&#30340;&#21442;&#25968;&#65289;&#65292;&#26159;&#29616;&#26377;&#26368;&#20339;&#26041;&#27861;&#20043;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art deep neural networks are still struggling to address the catastrophic forgetting problem in continual learning. In this paper, we propose one simple paradigm (named as S-Prompting) and two concrete approaches to highly reduce the forgetting degree in one of the most typical continual learning scenarios, i.e., domain increment learning (DIL). The key idea of the paradigm is to learn prompts independently across domains with pre-trained transformers, avoiding the use of exemplars that commonly appear in conventional methods. This results in a win-win game where the prompting can achieve the best for each domain. The independent prompting across domains only requests one single cross-entropy loss for training and one simple K-NN operation as a domain identifier for inference. The learning paradigm derives an image prompt learning approach and a novel language-image prompt learning approach. Owning an excellent scalability (0.03% parameter increase per domain), the best of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26377;&#21521;&#22270;&#30340;&#36328;&#27169;&#24577;&#29305;&#24449;&#34917;&#20805;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#21462;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#20132;&#20114;&#20449;&#24687;&#65292;&#32531;&#35299;&#20102;&#22810;&#27169;&#24577;&#34701;&#21512;&#20013;&#30340;&#24322;&#26500;&#24615;&#24046;&#36317;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2207.12261</link><description>&lt;p&gt;
&#22522;&#20110;&#26377;&#21521;&#22270;&#30340;&#36328;&#27169;&#24577;&#29305;&#24449;&#34917;&#20805;&#26041;&#27861;&#29992;&#20110;&#22810;&#27169;&#24577;&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
GraphCFC: A Directed Graph based Cross-modal Feature Complementation Approach for Multimodal Conversational Emotion Recognition. (arXiv:2207.12261v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.12261
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26377;&#21521;&#22270;&#30340;&#36328;&#27169;&#24577;&#29305;&#24449;&#34917;&#20805;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#21462;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#20132;&#20114;&#20449;&#24687;&#65292;&#32531;&#35299;&#20102;&#22810;&#27169;&#24577;&#34701;&#21512;&#20013;&#30340;&#24322;&#26500;&#24615;&#24046;&#36317;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;&#22312;&#20154;&#26426;&#20132;&#20114;&#31995;&#32479;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#25552;&#20379;&#26377;&#20849;&#24773;&#24515;&#29702;&#30340;&#26381;&#21153;&#12290;&#22810;&#27169;&#24577;&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;&#21487;&#20197;&#32531;&#35299;&#21333;&#27169;&#24577;&#26041;&#27861;&#30340;&#32570;&#28857;&#12290;&#26368;&#36817;&#65292;&#30001;&#20110;&#20851;&#31995;&#24314;&#27169;&#26041;&#38754;&#30340;&#21331;&#36234;&#24615;&#33021;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#24050;&#34987;&#24191;&#27867;&#29992;&#20110;&#21508;&#31181;&#39046;&#22495;&#12290;&#22312;&#22810;&#27169;&#24577;&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;&#20013;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#25552;&#21462;&#36828;&#36317;&#31163;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#36328;&#27169;&#24577;&#30340;&#20132;&#20114;&#20449;&#24687;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#30001;&#20110;&#29616;&#26377;&#26041;&#27861;&#65288;&#22914;MMGCN&#65289;&#30452;&#25509;&#34701;&#21512;&#22810;&#20010;&#27169;&#24577;&#65292;&#21487;&#33021;&#20250;&#20135;&#29983;&#20887;&#20313;&#20449;&#24687;&#65292;&#19988;&#21487;&#33021;&#20002;&#22833;&#22810;&#26679;&#21270;&#30340;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26377;&#21521;&#22270;&#30340;&#36328;&#27169;&#24577;&#29305;&#24449;&#34917;&#20805;&#65288;GraphCFC&#65289;&#27169;&#22359;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#27169;&#25311;&#19978;&#19979;&#25991;&#21644;&#20114;&#21160;&#20449;&#24687;&#12290;GraphCFC&#36890;&#36807;&#21033;&#29992;&#22810;&#20010;&#23376;&#31354;&#38388;&#25552;&#21462;&#22120;&#21644;&#25104;&#23545;&#36328;&#27169;&#24577;&#34917;&#20805;&#65288;PairCC&#65289;&#31574;&#30053;&#65292;&#32531;&#35299;&#20102;&#22810;&#27169;&#24577;&#34701;&#21512;&#20013;&#30340;&#24322;&#26500;&#24615;&#24046;&#36317;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emotion Recognition in Conversation (ERC) plays a significant part in Human-Computer Interaction (HCI) systems since it can provide empathetic services. Multimodal ERC can mitigate the drawbacks of uni-modal approaches. Recently, Graph Neural Networks (GNNs) have been widely used in a variety of fields due to their superior performance in relation modeling. In multimodal ERC, GNNs are capable of extracting both long-distance contextual information and inter-modal interactive information. Unfortunately, since existing methods such as MMGCN directly fuse multiple modalities, redundant information may be generated and diverse information may be lost. In this work, we present a directed Graph based Cross-modal Feature Complementation (GraphCFC) module that can efficiently model contextual and interactive information. GraphCFC alleviates the problem of heterogeneity gap in multimodal fusion by utilizing multiple subspace extractors and Pair-wise Cross-modal Complementary (PairCC) strategy. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20026;&#31227;&#21160;&#26080;&#32447;&#32593;&#32476;&#35774;&#35745;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#21644;&#26222;&#36866;&#30340;&#21333;&#21103;&#26412;&#36335;&#30001;&#31574;&#30053;&#65292;&#36890;&#36807;&#26435;&#34913;&#31454;&#20105;&#30340;&#32593;&#32476;&#30446;&#26631;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#20855;&#26377;&#22870;&#21169;&#20989;&#25968;&#30340;&#36335;&#30001;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#32452;&#26032;&#39062;&#30340;&#20851;&#31995;&#39046;&#22495;&#12289;&#36335;&#24452;&#21644;&#19978;&#19979;&#25991;&#29305;&#24449;&#26469;&#25551;&#36848;&#35774;&#22791;&#31227;&#21160;&#24773;&#20917;&#65292;&#21516;&#26102;&#37319;&#29992;&#28789;&#27963;&#30340;&#35757;&#32451;&#26041;&#27861;&#26469;&#35757;&#32451;&#21333;&#20010;DeepRL&#20195;&#29702;&#12290;</title><link>http://arxiv.org/abs/2207.11386</link><description>&lt;p&gt;
&#31227;&#21160;&#26080;&#32447;&#32593;&#32476;&#20013;&#23398;&#20064;&#33258;&#36866;&#24212;&#36716;&#21457;&#31574;&#30053;&#65306;&#36164;&#28304;&#20351;&#29992;&#19982;&#24310;&#36831;&#30340;&#24179;&#34913;
&lt;/p&gt;
&lt;p&gt;
Learning an Adaptive Forwarding Strategy for Mobile Wireless Networks: Resource Usage vs. Latency. (arXiv:2207.11386v2 [cs.NI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.11386
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20026;&#31227;&#21160;&#26080;&#32447;&#32593;&#32476;&#35774;&#35745;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#21644;&#26222;&#36866;&#30340;&#21333;&#21103;&#26412;&#36335;&#30001;&#31574;&#30053;&#65292;&#36890;&#36807;&#26435;&#34913;&#31454;&#20105;&#30340;&#32593;&#32476;&#30446;&#26631;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#20855;&#26377;&#22870;&#21169;&#20989;&#25968;&#30340;&#36335;&#30001;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#32452;&#26032;&#39062;&#30340;&#20851;&#31995;&#39046;&#22495;&#12289;&#36335;&#24452;&#21644;&#19978;&#19979;&#25991;&#29305;&#24449;&#26469;&#25551;&#36848;&#35774;&#22791;&#31227;&#21160;&#24773;&#20917;&#65292;&#21516;&#26102;&#37319;&#29992;&#28789;&#27963;&#30340;&#35757;&#32451;&#26041;&#27861;&#26469;&#35757;&#32451;&#21333;&#20010;DeepRL&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#31227;&#21160;&#26080;&#32447;&#32593;&#32476;&#35774;&#35745;&#26377;&#25928;&#30340;&#36335;&#30001;&#31574;&#30053;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#38656;&#35201;&#26080;&#32541;&#22320;&#36866;&#24212;&#26102;&#31354;&#21464;&#21270;&#30340;&#32593;&#32476;&#26465;&#20214;&#12290;&#26412;&#25991;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DeepRL&#65289;&#26469;&#23398;&#20064;&#36866;&#29992;&#20110;&#27492;&#31867;&#32593;&#32476;&#30340;&#21487;&#25193;&#23637;&#19988;&#26222;&#36866;&#30340;&#21333;&#21103;&#26412;&#36335;&#30001;&#31574;&#30053;&#12290;&#26412;&#25991;&#30340;&#36129;&#29486;&#22914;&#19979;&#65306;&#19968;&#12289;&#35774;&#35745;&#20102;&#19968;&#31181;&#22870;&#21169;&#20989;&#25968;&#65292;&#20351;&#24471;DeepRL&#20195;&#29702;&#21487;&#20197;&#26126;&#30830;&#26435;&#34913;&#31454;&#20105;&#30340;&#32593;&#32476;&#30446;&#26631;&#65292;&#22914;&#22312;&#26368;&#23567;&#21270;&#24310;&#36831;&#19982;&#27599;&#20010;&#21253;&#30340;&#20256;&#36755;&#27425;&#25968;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#65307;&#20108;&#12289;&#25552;&#20986;&#20102;&#19968;&#32452;&#26032;&#39062;&#30340;&#20851;&#31995;&#39046;&#22495;&#12289;&#36335;&#24452;&#21644;&#19978;&#19979;&#25991;&#29305;&#24449;&#26469;&#25551;&#36848;&#31227;&#21160;&#26080;&#32447;&#32593;&#32476;&#65292;&#24182;&#29420;&#31435;&#20110;&#29305;&#23450;&#30340;&#32593;&#32476;&#25299;&#25169;&#27169;&#22411;&#35774;&#22791;&#31227;&#21160;&#27169;&#22411;&#65307;&#19977;&#12289;&#37319;&#29992;&#28789;&#27963;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#23558;&#25152;&#26377;&#25968;&#25454;&#20174;&#25152;&#26377;&#21253;&#21644;&#35774;&#22791;&#27719;&#24635;&#25104;&#19968;&#20010;&#31163;&#32447;&#20013;&#24515;&#21270;&#30340;&#35757;&#32451;&#38598;&#65292;&#20197;&#35757;&#32451;&#21333;&#20010;DeepRL&#20195;&#29702;&#12290;&#20026;&#20102;&#35780;&#20272;&#20854;&#21487;&#25193;&#23637;&#24615;&#21644;&#26222;&#36866;&#24615;&#65292;&#25105;&#20204;&#23558;&#20854;&#19982;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Designing effective routing strategies for mobile wireless networks is challenging due to the need to seamlessly adapt routing behavior to spatially diverse and temporally changing network conditions. In this work, we use deep reinforcement learning (DeepRL) to learn a scalable and generalizable single-copy routing strategy for such networks. We make the following contributions: i) we design a reward function that enables the DeepRL agent to explicitly trade-off competing network goals, such as minimizing delay vs. the number of transmissions per packet; ii) we propose a novel set of relational neighborhood, path, and context features to characterize mobile wireless networks and model device mobility independently of a specific network topology; and iii) we use a flexible training approach that allows us to combine data from all packets and devices into a single offline centralized training set to train a single DeepRL agent. To evaluate generalizeability and scalability, we train our 
&lt;/p&gt;</description></item><item><title>AceIRL&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20027;&#21160;&#25506;&#32034;&#26469;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#21644;&#31574;&#30053;&#65292;&#22312;&#19981;&#38656;&#35201;&#29615;&#22659;&#29983;&#25104;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#30830;&#23450;&#21487;&#34892;&#22870;&#21169;&#20989;&#25968;&#30340;&#32622;&#20449;&#21306;&#38388;&#65292;&#24182;&#25214;&#21040;&#20391;&#37325;&#20110;&#29615;&#22659;&#20013;&#26368;&#26377;&#20449;&#24687;&#30340;&#21306;&#22495;&#30340;&#25506;&#32034;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2207.08645</link><description>&lt;p&gt;
&#36870;&#24378;&#21270;&#23398;&#20064;&#30340;&#20027;&#21160;&#25506;&#32034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Active Exploration for Inverse Reinforcement Learning. (arXiv:2207.08645v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.08645
&lt;/p&gt;
&lt;p&gt;
AceIRL&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20027;&#21160;&#25506;&#32034;&#26469;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#21644;&#31574;&#30053;&#65292;&#22312;&#19981;&#38656;&#35201;&#29615;&#22659;&#29983;&#25104;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#30830;&#23450;&#21487;&#34892;&#22870;&#21169;&#20989;&#25968;&#30340;&#32622;&#20449;&#21306;&#38388;&#65292;&#24182;&#25214;&#21040;&#20391;&#37325;&#20110;&#29615;&#22659;&#20013;&#26368;&#26377;&#20449;&#24687;&#30340;&#21306;&#22495;&#30340;&#25506;&#32034;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36870;&#24378;&#21270;&#23398;&#20064;&#65288;IRL&#65289;&#26159;&#20174;&#19987;&#23478;&#28436;&#31034;&#20013;&#25512;&#26029;&#22870;&#21169;&#20989;&#25968;&#30340;&#24378;&#22823;&#33539;&#24335;&#12290;&#35768;&#22810;IRL&#31639;&#27861;&#38656;&#35201;&#24050;&#30693;&#30340;&#36716;&#31227;&#27169;&#22411;&#65292;&#26377;&#26102;&#29978;&#33267;&#38656;&#35201;&#24050;&#30693;&#30340;&#19987;&#23478;&#31574;&#30053;&#65292;&#25110;&#32773;&#33267;&#23569;&#38656;&#35201;&#35775;&#38382;&#29983;&#25104;&#27169;&#22411;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#20551;&#35774;&#23545;&#20110;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#26469;&#35828;&#22826;&#24378;&#20102;&#65292;&#22240;&#20026;&#21482;&#33021;&#36890;&#36807;&#39034;&#24207;&#20132;&#20114;&#26469;&#35775;&#38382;&#29615;&#22659;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;IRL&#31639;&#27861;&#65306;&#20027;&#21160;&#25506;&#32034;&#36870;&#24378;&#21270;&#23398;&#20064;&#65288;AceIRL&#65289;&#65292;&#23427;&#20027;&#21160;&#25506;&#32034;&#26410;&#30693;&#29615;&#22659;&#21644;&#19987;&#23478;&#31574;&#30053;&#65292;&#24555;&#36895;&#23398;&#20064;&#19987;&#23478;&#30340;&#22870;&#21169;&#20989;&#25968;&#24182;&#35782;&#21035;&#20986;&#19968;&#20010;&#22909;&#30340;&#31574;&#30053;&#12290;AceIRL&#20351;&#29992;&#20808;&#21069;&#30340;&#35266;&#23519;&#32467;&#26524;&#26500;&#24314;&#32622;&#20449;&#21306;&#38388;&#26469;&#25429;&#25417;&#21487;&#34892;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#24182;&#25214;&#21040;&#20391;&#37325;&#20110;&#29615;&#22659;&#20013;&#26368;&#26377;&#20449;&#24687;&#30340;&#21306;&#22495;&#30340;&#25506;&#32034;&#31574;&#30053;&#12290;AceIRL&#26159;&#31532;&#19968;&#31181;&#20855;&#26377;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#19988;&#19981;&#38656;&#35201;&#29615;&#22659;&#29983;&#25104;&#27169;&#22411;&#30340;&#20027;&#21160;IRL&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inverse Reinforcement Learning (IRL) is a powerful paradigm for inferring a reward function from expert demonstrations. Many IRL algorithms require a known transition model and sometimes even a known expert policy, or they at least require access to a generative model. However, these assumptions are too strong for many real-world applications, where the environment can be accessed only through sequential interaction. We propose a novel IRL algorithm: Active exploration for Inverse Reinforcement Learning (AceIRL), which actively explores an unknown environment and expert policy to quickly learn the expert's reward function and identify a good policy. AceIRL uses previous observations to construct confidence intervals that capture plausible reward functions and find exploration policies that focus on the most informative regions of the environment. AceIRL is the first approach to active IRL with sample-complexity bounds that does not require a generative model of the environment. AceIRL 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#36712;&#38899;&#20048;&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#25903;&#25345;&#21508;&#31181;&#19981;&#21516;&#30340;&#20048;&#22120;&#65292;&#22312;&#30701;&#30340;&#24207;&#21015;&#38271;&#24230;&#19979;&#23454;&#29616;&#20102;&#24615;&#33021;&#19978;&#30340;&#26174;&#33879;&#25552;&#21319;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#29992;&#20110;&#20998;&#26512;&#38899;&#20048;&#33258;&#25105;&#20851;&#27880;&#65292;&#24182;&#39564;&#35777;&#20102;&#27169;&#22411;&#26356;&#20851;&#27880;&#19982;&#24403;&#21069;&#38899;&#31526;&#24418;&#25104;&#21644;&#35856;&#36328;&#24230;&#21644;&#20301;&#20110;&#21516;&#19968;&#20843;&#24230;&#30340;&#38899;&#31526;&#12290;</title><link>http://arxiv.org/abs/2207.06983</link><description>&lt;p&gt;
&#22810;&#36712;&#38899;&#20048; Transformer
&lt;/p&gt;
&lt;p&gt;
Multitrack Music Transformer. (arXiv:2207.06983v3 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.06983
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#36712;&#38899;&#20048;&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#25903;&#25345;&#21508;&#31181;&#19981;&#21516;&#30340;&#20048;&#22120;&#65292;&#22312;&#30701;&#30340;&#24207;&#21015;&#38271;&#24230;&#19979;&#23454;&#29616;&#20102;&#24615;&#33021;&#19978;&#30340;&#26174;&#33879;&#25552;&#21319;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#29992;&#20110;&#20998;&#26512;&#38899;&#20048;&#33258;&#25105;&#20851;&#27880;&#65292;&#24182;&#39564;&#35777;&#20102;&#27169;&#22411;&#26356;&#20851;&#27880;&#19982;&#24403;&#21069;&#38899;&#31526;&#24418;&#25104;&#21644;&#35856;&#36328;&#24230;&#21644;&#20301;&#20110;&#21516;&#19968;&#20843;&#24230;&#30340;&#38899;&#31526;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#20351;&#29992; Transformer &#27169;&#22411;&#29983;&#25104;&#22810;&#36712;&#38899;&#20048;&#30340;&#26041;&#27861;&#22312;&#20048;&#22120;&#25968;&#37327;&#12289;&#38899;&#20048;&#29255;&#27573;&#38271;&#24230;&#21644;&#25512;&#29702;&#36895;&#24230;&#26041;&#38754;&#26377;&#38480;&#21046;&#65292;&#37096;&#20998;&#21407;&#22240;&#22312;&#20110;&#24050;&#26377;&#34920;&#31034;&#26041;&#24335;&#38656;&#35201;&#38271;&#24230;&#36739;&#38271;&#30340;&#36755;&#20837;&#24207;&#21015;&#65292;&#20174;&#32780;&#38656;&#35201;&#26356;&#22810;&#30340;&#20869;&#23384;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#36712;&#38899;&#20048;&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#25903;&#25345;&#21508;&#31181;&#19981;&#21516;&#30340;&#20048;&#22120;&#65292;&#21516;&#26102;&#20351;&#36755;&#20837;&#24207;&#21015;&#38271;&#24230;&#26356;&#30701;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340; Multitrack Music Transformer&#65288;MMT&#65289;&#19982;&#26368;&#20808;&#36827;&#30340;&#31995;&#32479;&#30456;&#27604;&#20855;&#26377;&#21487;&#27604;&#24615;&#65292;&#22312;&#20027;&#35266;&#21548;&#27979;&#35797;&#20013;&#25490;&#22312;&#20004;&#20010;&#26368;&#36817;&#25552;&#20986;&#30340;&#27169;&#22411;&#20043;&#38388;&#65292;&#21516;&#26102;&#22312;&#36895;&#24230;&#21644;&#20869;&#23384;&#21344;&#29992;&#19978;&#37117;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#25552;&#21319;&#65292;&#20351;&#24471;&#35813;&#26041;&#27861;&#22312;&#23454;&#26102;&#21363;&#20852;&#21019;&#20316;&#25110;&#25509;&#36817;&#23454;&#26102;&#30340;&#21019;&#24847;&#24212;&#29992;&#20013;&#26356;&#20026;&#23454;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#26512;&#38899;&#20048;&#33258;&#25105;&#20851;&#27880;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#35757;&#32451;&#27169;&#22411;&#26356;&#20851;&#27880;&#19982;&#24403;&#21069;&#38899;&#31526;&#24418;&#25104;&#21644;&#35856;&#36328;&#24230;&#21644;&#20301;&#20110;&#21516;&#19968;&#20843;&#24230;&#30340;&#38899;&#31526;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing approaches for generating multitrack music with transformer models have been limited in terms of the number of instruments, the length of the music segments and slow inference. This is partly due to the memory requirements of the lengthy input sequences necessitated by existing representations. In this work, we propose a new multitrack music representation that allows a diverse set of instruments while keeping a short sequence length. Our proposed Multitrack Music Transformer (MMT) achieves comparable performance with state-of-the-art systems, landing in between two recently proposed models in a subjective listening test, while achieving substantial speedups and memory reductions over both, making the method attractive for real time improvisation or near real time creative applications. Further, we propose a new measure for analyzing musical self-attention and show that the trained model attends more to notes that form a consonant interval with the current note and to notes th
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38544;&#24335;&#20989;&#25968;&#36827;&#34892;&#36830;&#32493;&#25235;&#21462;&#35268;&#21010;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;&#36830;&#32493;&#25235;&#21462;&#21151;&#33021;(CG)&#65292;&#36890;&#36807;&#20154;&#31867;&#31034;&#33539;&#23398;&#20064;&#65292;&#21487;&#22312;&#25805;&#32437;&#22810;&#20010;&#29289;&#20307;&#26102;&#33719;&#24471;&#26356;&#39640;&#30340;&#25104;&#21151;&#29575;&#65292;&#24182;&#22312;&#30495;&#23454;&#26426;&#22120;&#20154;&#19978;&#24471;&#21040;&#39564;&#35777;&#65292;&#25104;&#21151;&#29575;&#26174;&#33879;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2207.05053</link><description>&lt;p&gt;
&#20174;&#20154;&#31867;&#31034;&#33539;&#20013;&#23398;&#20064;&#20351;&#29992;&#26426;&#26800;&#25163;&#36827;&#34892;&#36830;&#32493;&#25235;&#21462;
&lt;/p&gt;
&lt;p&gt;
Learning Continuous Grasping Function with a Dexterous Hand from Human Demonstrations. (arXiv:2207.05053v3 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.05053
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38544;&#24335;&#20989;&#25968;&#36827;&#34892;&#36830;&#32493;&#25235;&#21462;&#35268;&#21010;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;&#36830;&#32493;&#25235;&#21462;&#21151;&#33021;(CG)&#65292;&#36890;&#36807;&#20154;&#31867;&#31034;&#33539;&#23398;&#20064;&#65292;&#21487;&#22312;&#25805;&#32437;&#22810;&#20010;&#29289;&#20307;&#26102;&#33719;&#24471;&#26356;&#39640;&#30340;&#25104;&#21151;&#29575;&#65292;&#24182;&#22312;&#30495;&#23454;&#26426;&#22120;&#20154;&#19978;&#24471;&#21040;&#39564;&#35777;&#65292;&#25104;&#21151;&#29575;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#38544;&#24335;&#20989;&#25968;&#23398;&#20064;&#29983;&#25104;&#26426;&#26800;&#25163;&#25805;&#32437;&#30340;&#25235;&#21462;&#36816;&#21160;&#12290;&#20351;&#29992;&#36830;&#32493;&#26102;&#38388;&#36755;&#20837;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#36830;&#32493;&#19988;&#24179;&#28369;&#30340;&#25235;&#21462;&#35745;&#21010;&#12290;&#25105;&#20204;&#23558;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#21629;&#21517;&#20026;&#36830;&#32493;&#25235;&#21462;&#21151;&#33021;&#65288;CGF&#65289;&#12290;&#36890;&#36807;&#20351;&#29992;3D&#20154;&#31867;&#31034;&#33539;&#65292;&#21033;&#29992;&#26465;&#20214;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#36827;&#34892;&#29983;&#25104;&#24314;&#27169;&#65292;&#24182;&#23558;&#22823;&#35268;&#27169;&#30340;&#20154;&#20307;-&#29289;&#20307;&#20132;&#20114;&#36712;&#36857;&#36890;&#36807;&#36816;&#21160;&#37325;&#26032;&#23450;&#21521;&#36716;&#25442;&#25104;&#26426;&#22120;&#20154;&#28436;&#31034;&#65292;&#28982;&#21518;&#20351;&#29992;&#36825;&#20123;&#28436;&#31034;&#26469;&#35757;&#32451;CGF&#12290;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;CGF&#36827;&#34892;&#25277;&#26679;&#65292;&#22312;&#27169;&#25311;&#22120;&#20013;&#29983;&#25104;&#19981;&#21516;&#30340;&#25235;&#21462;&#35745;&#21010;&#65292;&#24182;&#36873;&#25321;&#25104;&#21151;&#30340;&#35745;&#21010;&#36716;&#31227;&#21040;&#30495;&#23454;&#26426;&#22120;&#20154;&#19978;&#12290;&#36890;&#36807;&#22312;&#22810;&#26679;&#21270;&#30340;&#20154;&#31867;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#25105;&#20204;&#30340;CGF&#20801;&#35768;&#25805;&#32437;&#22810;&#20010;&#23545;&#35937;&#19978;&#30340;&#27867;&#21270;&#12290;&#19982;&#20808;&#21069;&#30340;&#35268;&#21010;&#31639;&#27861;&#30456;&#27604;&#65292;CGF&#26356;&#21152;&#26377;&#25928;&#65292;&#22312;&#36716;&#31227;&#21040;&#20351;&#29992;&#30495;&#23454;Allegro&#25163;&#36827;&#34892;&#25235;&#21462;&#26102;&#65292;&#25104;&#21151;&#29575;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose to learn to generate grasping motion for manipulation with a dexterous hand using implicit functions. With continuous time inputs, the model can generate a continuous and smooth grasping plan. We name the proposed model Continuous Grasping Function (CGF). CGF is learned via generative modeling with a Conditional Variational Autoencoder using 3D human demonstrations. We will first convert the large-scale human-object interaction trajectories to robot demonstrations via motion retargeting, and then use these demonstrations to train CGF. During inference, we perform sampling with CGF to generate different grasping plans in the simulator and select the successful ones to transfer to the real robot. By training on diverse human data, our CGF allows generalization to manipulate multiple objects. Compared to previous planning algorithms, CGF is more efficient and achieves significant improvement on success rate when transferred to grasping with the real Allegro Hand. Our project pa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19968;&#33268;&#32622;&#20449;&#21306;&#38388;&#19978;&#38480;&#25277;&#26679;&#30340;&#20248;&#21270;&#26694;&#26550;&#65292;&#20854;&#20551;&#35774;&#20855;&#26377;&#20132;&#25442;&#24615;&#65292;&#33021;&#22815;&#25552;&#20379;&#26356;&#22810;&#30340;&#25628;&#32034;&#27169;&#22411;&#26550;&#26500;&#36873;&#25321;&#65292;&#24182;&#19988;&#22312;&#36229;&#21442;&#25968;&#35843;&#25972;&#26102;&#20855;&#26377;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2207.03017</link><description>&lt;p&gt;
&#24182;&#34892;&#30340;&#19968;&#33268;&#32622;&#20449;&#21306;&#38388;&#36229;&#21442;&#25968;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Parallel Conformal Hyperparameter Optimization. (arXiv:2207.03017v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.03017
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19968;&#33268;&#32622;&#20449;&#21306;&#38388;&#19978;&#38480;&#25277;&#26679;&#30340;&#20248;&#21270;&#26694;&#26550;&#65292;&#20854;&#20551;&#35774;&#20855;&#26377;&#20132;&#25442;&#24615;&#65292;&#33021;&#22815;&#25552;&#20379;&#26356;&#22810;&#30340;&#25628;&#32034;&#27169;&#22411;&#26550;&#26500;&#36873;&#25321;&#65292;&#24182;&#19988;&#22312;&#36229;&#21442;&#25968;&#35843;&#25972;&#26102;&#20855;&#26377;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#20986;&#29616;&#20102;&#20960;&#20010;&#26032;&#39062;&#30340;&#36229;&#21442;&#25968;&#25628;&#32034;&#26694;&#26550;&#65292;&#20294;&#22823;&#22810;&#25968;&#37117;&#20381;&#36182;&#20110;&#20005;&#26684;&#30340;&#12289;&#36890;&#24120;&#26159;&#27491;&#24577;&#20998;&#24067;&#20551;&#35774;&#65292;&#38480;&#21046;&#20102;&#25628;&#32034;&#27169;&#22411;&#30340;&#28789;&#27963;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32622;&#20449;&#21306;&#38388;&#19968;&#33268;&#24615;&#19978;&#38480;&#25277;&#26679;&#30340;&#20248;&#21270;&#26694;&#26550;&#65292;&#20854;&#20132;&#25442;&#24615;&#20551;&#35774;&#33021;&#22815;&#25552;&#20379;&#26356;&#22810;&#30340;&#25628;&#32034;&#27169;&#22411;&#26550;&#26500;&#36873;&#25321;&#12290;&#23545;&#36229;&#21442;&#20248;&#21270;&#30340;&#22810;&#20010;&#26550;&#26500;&#36827;&#34892;&#20102;&#25506;&#32034;&#21644;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#23494;&#38598;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#38543;&#26426;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several novel frameworks for hyperparameter search have emerged in the last decade, but most rely on strict, often normal, distributional assumptions, limiting search model flexibility. This paper proposes a novel optimization framework based on upper confidence bound sampling of conformal confidence intervals, whose assumption of exchangeability enables greater choice of search model architectures. Several such architectures were explored and benchmarked on hyperparameter tuning of both dense and convolutional neural networks, displaying superior performance to random search.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20027;&#21160;&#39038;&#38382;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26694;&#26550;&#65292;Ask-AC&#65292;&#23427;&#26367;&#25442;&#20102;&#20256;&#32479;&#30340;&#34987;&#21160;&#30417;&#30563;&#20449;&#21495;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#23450;&#21046;&#21270;&#21644;&#39640;&#25928;&#30340;&#20449;&#24687;&#20132;&#25442;&#65292;&#20854;&#20013;&#30340;&#20004;&#20010;&#20114;&#34917;&#32452;&#20214;&#20801;&#35768;&#20195;&#29702;&#20027;&#21160;&#23547;&#27714;&#39038;&#38382;&#24178;&#39044;&#21644;&#35782;&#21035;&#28431;&#25481;&#30340;&#19981;&#31283;&#23450;&#29366;&#24577;&#12290;</title><link>http://arxiv.org/abs/2207.01955</link><description>&lt;p&gt;
Ask-AC: &#19968;&#31181;&#24490;&#29615;&#20013;&#30340;&#20027;&#21160;&#39038;&#38382;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Ask-AC: An Initiative Advisor-in-the-Loop Actor-Critic Framework. (arXiv:2207.01955v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.01955
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20027;&#21160;&#39038;&#38382;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26694;&#26550;&#65292;Ask-AC&#65292;&#23427;&#26367;&#25442;&#20102;&#20256;&#32479;&#30340;&#34987;&#21160;&#30417;&#30563;&#20449;&#21495;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#23450;&#21046;&#21270;&#21644;&#39640;&#25928;&#30340;&#20449;&#24687;&#20132;&#25442;&#65292;&#20854;&#20013;&#30340;&#20004;&#20010;&#20114;&#34917;&#32452;&#20214;&#20801;&#35768;&#20195;&#29702;&#20027;&#21160;&#23547;&#27714;&#39038;&#38382;&#24178;&#39044;&#21644;&#35782;&#21035;&#28431;&#25481;&#30340;&#19981;&#31283;&#23450;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20132;&#20114;&#24335;&#24378;&#21270;&#23398;&#20064;&#26041;&#26696;&#21462;&#24471;&#20102;&#24456;&#22810;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#30446;&#21069;&#30340;&#26041;&#26696;&#20173;&#28982;&#20381;&#36182;&#20110;&#26469;&#33258;&#39038;&#38382;&#19987;&#23478;&#30340;&#34987;&#21160;&#30417;&#30563;&#20449;&#21495;&#65292;&#24418;&#24335;&#21253;&#25324;&#25345;&#32493;&#30417;&#25511;&#25110;&#39044;&#23450;&#20041;&#35268;&#21017;&#65292;&#36825;&#19981;&#21487;&#36991;&#20813;&#22320;&#23548;&#33268;&#20102;&#19968;&#31181;&#40635;&#28902;&#32780;&#26114;&#36149;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#20027;&#21160;&#39038;&#38382;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26694;&#26550;&#65292;&#31216;&#20026;Ask-AC&#65292;&#23427;&#29992;&#19968;&#20010;&#21452;&#21521;&#30340;&#23398;&#20064;&#32773;&#20027;&#21160;&#26426;&#21046;&#26367;&#25442;&#20102;&#21333;&#21521;&#30340;&#39038;&#38382;&#25351;&#23548;&#26426;&#21046;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23398;&#20064;&#32773;&#21644;&#39038;&#38382;&#20043;&#38388;&#30340;&#23450;&#21046;&#21270;&#21644;&#26377;&#25928;&#30340;&#20449;&#24687;&#20132;&#25442;&#12290;Ask-AC &#30340;&#26680;&#24515;&#26159;&#20004;&#20010;&#20114;&#34917;&#30340;&#32452;&#20214;&#65292;&#20998;&#21035;&#26159;&#21160;&#20316;&#35831;&#27714;&#32773;&#21644;&#33258;&#36866;&#24212;&#29366;&#24577;&#36873;&#25321;&#22120;&#65292;&#21487;&#20197;&#26041;&#20415;&#22320;&#32435;&#20837;&#21508;&#31181;&#31163;&#25955;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26550;&#26500;&#20013;&#12290;&#21069;&#32773;&#20801;&#35768;&#20195;&#29702;&#20027;&#21160;&#23547;&#27714;&#19981;&#30830;&#23450;&#29366;&#24577;&#19979;&#30340;&#39038;&#38382;&#24178;&#39044;&#65292;&#21518;&#32773;&#21017;&#21487;&#20197;&#35782;&#21035;&#28431;&#25481;&#30340;&#19981;&#31283;&#23450;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the promising results achieved, state-of-the-art interactive reinforcement learning schemes rely on passively receiving supervision signals from advisor experts, in the form of either continuous monitoring or pre-defined rules, which inevitably result in a cumbersome and expensive learning process. In this paper, we introduce a novel initiative advisor-in-the-loop actor-critic framework, termed as Ask-AC, that replaces the unilateral advisor-guidance mechanism with a bidirectional learner-initiative one, and thereby enables a customized and efficacious message exchange between learner and advisor. At the heart of Ask-AC are two complementary components, namely action requester and adaptive state selector, that can be readily incorporated into various discrete actor-critic architectures. The former component allows the agent to initiatively seek advisor intervention in the presence of uncertain states, while the latter identifies the unstable states potentially missed by the for
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#20197;&#26126;&#30830;&#34913;&#37327;&#31354;&#38388;&#21644;&#20998;&#31867;&#37325;&#22797;&#39044;&#27979;&#30340;&#25968;&#37327;&#65292;&#30740;&#31350;&#20154;&#21592;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;Semantic Sorting and NMS&#27169;&#22359;&#20197;&#28040;&#38500;&#36825;&#20123;&#37325;&#22797;&#39044;&#27979;&#65292;&#20197;&#36229;&#36234;&#20256;&#32479;&#30340;&#24179;&#22343;&#31934;&#24230;&#24230;&#37327;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2207.01614</link><description>&lt;p&gt;
&#36229;&#36234;mAP&#65306;&#36808;&#21521;&#26356;&#22909;&#30340;&#23454;&#20363;&#20998;&#21106;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Beyond mAP: Towards better evaluation of instance segmentation. (arXiv:2207.01614v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.01614
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#20197;&#26126;&#30830;&#34913;&#37327;&#31354;&#38388;&#21644;&#20998;&#31867;&#37325;&#22797;&#39044;&#27979;&#30340;&#25968;&#37327;&#65292;&#30740;&#31350;&#20154;&#21592;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;Semantic Sorting and NMS&#27169;&#22359;&#20197;&#28040;&#38500;&#36825;&#20123;&#37325;&#22797;&#39044;&#27979;&#65292;&#20197;&#36229;&#36234;&#20256;&#32479;&#30340;&#24179;&#22343;&#31934;&#24230;&#24230;&#37327;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27491;&#30830;&#24615;&#26159;&#23454;&#20363;&#20998;&#21106;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65292;&#23427;&#21253;&#25324;&#35745;&#31639;&#27491;&#30830;&#23450;&#20301;&#25152;&#26377;&#39044;&#27979;&#30340;&#23545;&#35937;&#25968;&#21644;&#23545;&#27599;&#20010;&#23450;&#20301;&#39044;&#27979;&#36827;&#34892;&#20998;&#31867;&#12290;&#24179;&#22343;&#31934;&#24230;&#26159;&#34913;&#37327;&#36825;&#20123;&#20998;&#21106;&#26500;&#25104;&#37096;&#20998;&#30340;&#20107;&#23454;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#25351;&#26631;&#22312;&#39640;&#21484;&#22238;&#29575;&#33539;&#22260;&#20869;&#19981;&#24809;&#32602;&#37325;&#22797;&#39044;&#27979;&#65292;&#24182;&#19988;&#19981;&#33021;&#21306;&#20998;&#23450;&#20301;&#27491;&#30830;&#20294;&#20998;&#31867;&#38169;&#35823;&#30340;&#23454;&#20363;&#12290;&#36825;&#20010;&#24369;&#28857;&#26080;&#24847;&#20013;&#23548;&#33268;&#20102;&#32593;&#32476;&#35774;&#35745;&#22312;AP&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#22686;&#30410;&#65292;&#20294;&#20063;&#24341;&#20837;&#20102;&#22823;&#37327;&#30340;&#35823;&#25253;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#19981;&#33021;&#20381;&#36182;AP&#26469;&#36873;&#25321;&#25552;&#20379;&#20551;&#38451;&#24615;&#21644;&#39640;&#21484;&#22238;&#20043;&#38388;&#26368;&#20339;&#25240;&#34935;&#30340;&#27169;&#22411;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#25991;&#29486;&#20013;&#30340;&#26367;&#20195;&#24230;&#37327;&#26631;&#20934;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#26469;&#26126;&#30830;&#27979;&#37327;&#31354;&#38388;&#37325;&#22797;&#39044;&#27979;&#21644;&#20998;&#31867;&#37325;&#22797;&#39044;&#27979;&#30340;&#25968;&#37327;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20687;&#32032;&#21344;&#29992;&#30340;&#35821;&#20041;&#25490;&#24207;&#21644;NMS&#27169;&#22359;&#26469;&#28040;&#38500;&#36825;&#20123;&#37325;&#22797;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Correctness of instance segmentation constitutes counting the number of objects, correctly localizing all predictions and classifying each localized prediction. Average Precision is the de-facto metric used to measure all these constituents of segmentation. However, this metric does not penalize duplicate predictions in the high-recall range, and cannot distinguish instances that are localized correctly but categorized incorrectly. This weakness has inadvertently led to network designs that achieve significant gains in AP but also introduce a large number of false positives. We therefore cannot rely on AP to choose a model that provides an optimal tradeoff between false positives and high recall. To resolve this dilemma, we review alternative metrics in the literature and propose two new measures to explicitly measure the amount of both spatial and categorical duplicate predictions. We also propose a Semantic Sorting and NMS module to remove these duplicates based on a pixel occupancy 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#38543;&#26426;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27987;&#24230;&#19981;&#31561;&#24335;&#65292;&#36890;&#36807;&#26399;&#26395;&#20998;&#31867;&#22120;&#32473;&#20986;&#20102;&#20998;&#31867;&#35823;&#24046;&#30340;&#27010;&#29575;&#19978;&#30028;&#65292;&#24182;&#30830;&#23450;&#20102;&#26368;&#20248;&#23618;&#25968;&#12290;</title><link>http://arxiv.org/abs/2206.11241</link><description>&lt;p&gt;
&#38543;&#26426;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27987;&#24230;&#19981;&#31561;&#24335;&#21644;&#26368;&#20248;&#23618;&#25968;
&lt;/p&gt;
&lt;p&gt;
Concentration inequalities and optimal number of layers for stochastic deep neural networks. (arXiv:2206.11241v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.11241
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#38543;&#26426;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27987;&#24230;&#19981;&#31561;&#24335;&#65292;&#36890;&#36807;&#26399;&#26395;&#20998;&#31867;&#22120;&#32473;&#20986;&#20102;&#20998;&#31867;&#35823;&#24046;&#30340;&#27010;&#29575;&#19978;&#30028;&#65292;&#24182;&#30830;&#23450;&#20102;&#26368;&#20248;&#23618;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#38543;&#26426;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;SDNN&#65289;&#38544;&#34255;&#23618;&#36755;&#20986;&#30340;&#27987;&#24230;&#19981;&#31561;&#24335;&#65292;&#20197;&#21450;&#25972;&#20010;SDNN&#36755;&#20986;&#30340;&#27987;&#24230;&#19981;&#31561;&#24335;&#12290;&#36825;&#20123;&#32467;&#26524;&#20351;&#25105;&#20204;&#33021;&#22815;&#24341;&#20837;&#26399;&#26395;&#20998;&#31867;&#22120;&#65288;EC&#65289;&#65292;&#24182;&#32473;&#20986;EC&#20998;&#31867;&#35823;&#24046;&#30340;&#27010;&#29575;&#19978;&#30028;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#26368;&#20248;&#20572;&#27490;&#31574;&#30053;&#30830;&#23450;&#20102;SDNN&#30340;&#26368;&#20339;&#23618;&#25968;&#12290;&#25105;&#20204;&#23558;&#20998;&#26512;&#24212;&#29992;&#20110;&#20855;&#26377;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#30340;&#38543;&#26426;&#29256;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
We state concentration inequalities for the output of the hidden layers of a stochastic deep neural network (SDNN), as well as for the output of the whole SDNN. These results allow us to introduce an expected classifier (EC), and to give probabilistic upper bound for the classification error of the EC. We also state the optimal number of layers for the SDNN via an optimal stopping procedure. We apply our analysis to a stochastic version of a feedforward neural network with ReLU activation function.
&lt;/p&gt;</description></item><item><title>Merak&#26159;&#19968;&#20010;&#33258;&#21160;&#21270;&#30340;&#19977;&#32500;&#24182;&#34892;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#26694;&#26550;&#65292;&#23427;&#35299;&#20915;&#20102;&#24403;&#21069;&#20854;&#20182;&#26694;&#26550;&#20013;&#38656;&#35201;&#25163;&#21160;&#20462;&#25913;&#27169;&#22411;&#25165;&#21487;&#24182;&#34892;&#30340;&#38382;&#39064;&#65292;&#24182;&#20855;&#26377;&#36739;&#39640;&#30340;&#35745;&#31639;&#12289;GPU&#20869;&#23384;&#21644;&#32593;&#32476;&#24102;&#23485;&#21033;&#29992;&#29575;&#12290;</title><link>http://arxiv.org/abs/2206.04959</link><description>&lt;p&gt;
Merak&#65306;&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;DNN&#35757;&#32451;&#26694;&#26550;&#65292;&#20855;&#22791;&#33258;&#21160;&#21270;&#30340;&#19977;&#32500;&#24182;&#34892;&#25216;&#26415;&#65292;&#36866;&#29992;&#20110;&#24222;&#22823;&#30340;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Merak: An Efficient Distributed DNN Training Framework with Automated 3D Parallelism for Giant Foundation Models. (arXiv:2206.04959v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.04959
&lt;/p&gt;
&lt;p&gt;
Merak&#26159;&#19968;&#20010;&#33258;&#21160;&#21270;&#30340;&#19977;&#32500;&#24182;&#34892;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#26694;&#26550;&#65292;&#23427;&#35299;&#20915;&#20102;&#24403;&#21069;&#20854;&#20182;&#26694;&#26550;&#20013;&#38656;&#35201;&#25163;&#21160;&#20462;&#25913;&#27169;&#22411;&#25165;&#21487;&#24182;&#34892;&#30340;&#38382;&#39064;&#65292;&#24182;&#20855;&#26377;&#36739;&#39640;&#30340;&#35745;&#31639;&#12289;GPU&#20869;&#23384;&#21644;&#32593;&#32476;&#24102;&#23485;&#21033;&#29992;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#27491;&#22312;&#25104;&#20026;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#20027;&#27969;&#12290;&#30001;&#20110;&#27169;&#22411;&#21442;&#25968;&#21644;&#35757;&#32451;&#25968;&#25454;&#38598;&#35268;&#27169;&#24222;&#22823;&#65292;&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#22987;&#32456;&#38656;&#35201;&#32791;&#36153;&#26102;&#38388;&#12290;&#38500;&#20102;&#35745;&#31639;&#23494;&#38598;&#22411;&#65292;&#35757;&#32451;&#36807;&#31243;&#36824;&#26497;&#20854;&#20381;&#36182;&#20869;&#23384;&#21644;&#36890;&#20449;&#65292;&#36825;&#23601;&#38656;&#35201;&#24212;&#29992;&#19977;&#32500;&#24182;&#34892;&#25216;&#26415;&#65292;&#21363;&#38598;&#25104;&#25968;&#25454;&#24182;&#34892;&#12289;&#31649;&#36947;&#27169;&#22411;&#24182;&#34892;&#21644;&#24352;&#37327;&#27169;&#22411;&#24182;&#34892;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#35757;&#32451;&#12290;&#20026;&#27492;&#65292;&#30740;&#21457;&#20102;&#19968;&#20123;&#33258;&#23450;&#20041;&#36719;&#20214;&#26694;&#26550;&#65292;&#22914;Megatron-LM&#21644;DeepSpeed&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#19977;&#20301;&#24182;&#34892;&#25216;&#26415;&#26694;&#26550;&#20173;&#23384;&#22312;&#20004;&#20010;&#38382;&#39064;&#65306;i&#65289;&#23545;&#20110;&#38656;&#35201;&#25163;&#21160;&#20462;&#25913;&#27169;&#22411;&#20197;&#24182;&#34892;&#35757;&#32451;&#30340;&#27169;&#22411;&#24320;&#21457;&#20154;&#21592;&#26469;&#35828;&#65292;&#26694;&#26550;&#24182;&#19981;&#36879;&#26126;&#12290;ii&#65289;&#23427;&#20204;&#30340;&#35745;&#31639;&#12289;GPU&#20869;&#23384;&#21644;&#32593;&#32476;&#24102;&#23485;&#21033;&#29992;&#19981;&#36275;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Merak&#65292;&#19968;&#20010;&#39640;&#36164;&#28304;&#21033;&#29992;&#29575;&#30340;&#33258;&#21160;&#21270;&#19977;&#32500;&#24182;&#34892;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation models are becoming the dominant deep learning technologies. Pretraining a foundation model is always time-consumed due to the large scale of both the model parameter and training dataset. Besides being computing-intensive, the training process is extremely memory-intensive and communication-intensive. These features make it necessary to apply 3D parallelism, which integrates data parallelism, pipeline model parallelism and tensor model parallelism, to achieve high training efficiency.  To achieve this goal, some custom software frameworks such as Megatron-LM and DeepSpeed are developed. However, current 3D parallelism frameworks still meet two issues: i) they are not transparent to model developers, which need to manually modify the model to parallelize training. ii) their utilization of computation, GPU memory and network bandwidth are not sufficient. We propose Merak, an automated 3D parallelism deep learning training framework with high resource utilization. Merak automa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#26426;&#22120;&#23398;&#20064;&#21644;&#20132;&#26131;&#22270;&#20998;&#26512;&#26041;&#27861;&#22312;&#21453;&#27927;&#38065;&#21644;&#25171;&#20987;&#24656;&#24598;&#20027;&#20041;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#29305;&#21035;&#20851;&#27880;&#36825;&#20123;&#25216;&#26415;&#22312;&#31038;&#20250;&#25216;&#26415;&#29983;&#24577;&#31995;&#32479;&#20013;&#37096;&#32626;&#25152;&#20135;&#29983;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2206.04803</link><description>&lt;p&gt;
&#26816;&#27979;&#24322;&#24120;&#21152;&#23494;&#36135;&#24065;&#20132;&#26131;&#65306;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#21462;&#35777;&#22312;AML/CFT&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting Anomalous Cryptocurrency Transactions: an AML/CFT Application of Machine Learning-based Forensics. (arXiv:2206.04803v3 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.04803
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#26426;&#22120;&#23398;&#20064;&#21644;&#20132;&#26131;&#22270;&#20998;&#26512;&#26041;&#27861;&#22312;&#21453;&#27927;&#38065;&#21644;&#25171;&#20987;&#24656;&#24598;&#20027;&#20041;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#29305;&#21035;&#20851;&#27880;&#36825;&#20123;&#25216;&#26415;&#22312;&#31038;&#20250;&#25216;&#26415;&#29983;&#24577;&#31995;&#32479;&#20013;&#37096;&#32626;&#25152;&#20135;&#29983;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21306;&#22359;&#38142;&#21644;&#20998;&#24067;&#24335;&#20998;&#31867;&#36134;&#25216;&#26415;&#22312;&#37329;&#34701;&#39046;&#22495;&#30340;&#24212;&#29992;&#24341;&#21457;&#20102;&#30417;&#31649;&#25285;&#24551;&#65292;&#20854;&#20013;&#29992;&#25143;&#21311;&#21517;&#24615;&#21487;&#33021;&#20445;&#38556;&#38544;&#31169;&#21644;&#25968;&#25454;&#20445;&#25252;&#65292;&#20294;&#19981;&#21487;&#35782;&#21035;&#24615;&#38459;&#30861;&#36861;&#36131;&#65292;&#25361;&#25112;&#21453;&#27927;&#38065;&#21644;&#25171;&#20987;&#24656;&#24598;&#20027;&#20041;&#21644;&#25193;&#25955;&#65288;AML/CFT&#65289;&#30340;&#21162;&#21147;&#12290;&#26412;&#25991;&#20851;&#27880;&#36825;&#20123;&#25216;&#26415;&#22312;&#31038;&#20250;&#25216;&#26415;&#29983;&#24577;&#31995;&#32479;&#20013;&#37096;&#32626;&#23545;&#35813;&#39046;&#22495;&#29305;&#24449;&#21644;&#21457;&#23637;&#30340;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#23545;&#26426;&#22120;&#23398;&#20064;&#21644;&#20132;&#26131;&#22270;&#20998;&#26512;&#26041;&#27861;&#30340;&#24212;&#29992;&#25552;&#20379;&#20102;&#24773;&#22659;&#21270;&#30340;&#35265;&#35299;&#12290;&#23427;&#36890;&#36807;&#21508;&#31181;&#25216;&#26415;&#20998;&#26512;&#20102;&#20197;&#26377;&#21521;&#22270;&#32593;&#32476;&#34920;&#31034;&#30340;&#27604;&#29305;&#24065;&#23454;&#26102;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
In shaping the Internet of Money, the application of blockchain and distributed ledger technologies (DLTs) to the financial sector triggered regulatory concerns. Notably, while the user anonymity enabled in this field may safeguard privacy and data protection, the lack of identifiability hinders accountability and challenges the fight against money laundering and the financing of terrorism and proliferation (AML/CFT). As law enforcement agencies and the private sector apply forensics to track crypto transfers across ecosystems that are socio-technical in nature, this paper focuses on the growing relevance of these techniques in a domain where their deployment impacts the traits and evolution of the sphere. In particular, this work offers contextualized insights into the application of methods of machine learning and transaction graph analysis. Namely, it analyzes a real-world dataset of Bitcoin transactions represented as a directed graph network through various techniques. The modelin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#34507;&#30333;&#36136;&#33050;&#25163;&#26550;&#38382;&#39064;&#65292;&#37319;&#29992;E(3)-&#31561;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#23398;&#20064;&#22810;&#26679;&#21644;&#38271;&#30340;&#34507;&#30333;&#36136;&#20027;&#24178;&#32467;&#26500;&#30340;&#20998;&#24067;&#65292;&#24182;&#29992;SMCDiff&#31639;&#27861;&#20174;&#20998;&#24067;&#20013;&#26377;&#25928;&#22320;&#37319;&#26679;&#20986;&#31526;&#21512;&#32473;&#23450;&#27169;&#20307;&#26465;&#20214;&#30340;&#33050;&#25163;&#26550;&#65292;&#36825;&#19968;&#31639;&#27861;&#22312;&#22823;&#35745;&#31639;&#26497;&#38480;&#20013;&#21487;&#20197;&#29702;&#35770;&#19978;&#20445;&#35777;&#26465;&#20214;&#26679;&#26412;&#30340;&#37319;&#26679;&#65307;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#38271;&#36798;80&#20010;&#27531;&#22522;&#30340;&#33050;&#25163;&#26550;&#65292;&#24182;&#21487;&#20197;&#20026;&#22266;&#23450;&#27169;&#20307;&#23454;&#29616;&#32467;&#26500;&#22810;&#26679;&#30340;&#33050;&#25163;&#26550;&#12290;</title><link>http://arxiv.org/abs/2206.04119</link><description>&lt;p&gt;
&#19977;&#32500;&#34507;&#30333;&#36136;&#20027;&#24178;&#30340;&#25193;&#25955;&#27010;&#29575;&#24314;&#27169;&#22312;&#22522;&#20110;&#27169;&#20307;&#33050;&#25163;&#26550;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Diffusion probabilistic modeling of protein backbones in 3D for the motif-scaffolding problem. (arXiv:2206.04119v2 [q-bio.BM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.04119
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#34507;&#30333;&#36136;&#33050;&#25163;&#26550;&#38382;&#39064;&#65292;&#37319;&#29992;E(3)-&#31561;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#23398;&#20064;&#22810;&#26679;&#21644;&#38271;&#30340;&#34507;&#30333;&#36136;&#20027;&#24178;&#32467;&#26500;&#30340;&#20998;&#24067;&#65292;&#24182;&#29992;SMCDiff&#31639;&#27861;&#20174;&#20998;&#24067;&#20013;&#26377;&#25928;&#22320;&#37319;&#26679;&#20986;&#31526;&#21512;&#32473;&#23450;&#27169;&#20307;&#26465;&#20214;&#30340;&#33050;&#25163;&#26550;&#65292;&#36825;&#19968;&#31639;&#27861;&#22312;&#22823;&#35745;&#31639;&#26497;&#38480;&#20013;&#21487;&#20197;&#29702;&#35770;&#19978;&#20445;&#35777;&#26465;&#20214;&#26679;&#26412;&#30340;&#37319;&#26679;&#65307;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#38271;&#36798;80&#20010;&#27531;&#22522;&#30340;&#33050;&#25163;&#26550;&#65292;&#24182;&#21487;&#20197;&#20026;&#22266;&#23450;&#27169;&#20307;&#23454;&#29616;&#32467;&#26500;&#22810;&#26679;&#30340;&#33050;&#25163;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26500;&#24314;&#25903;&#25345;&#25152;&#38656;&#27169;&#20307;&#30340;&#33050;&#25163;&#26550;&#32467;&#26500;&#65292;&#21363;&#36171;&#20104;&#34507;&#30333;&#36136;&#21151;&#33021;&#30340;&#33050;&#25163;&#26550;&#35774;&#35745;&#23545;&#20110;&#30123;&#33495;&#21644;&#37238;&#30340;&#35774;&#35745;&#20855;&#26377;&#28508;&#22312;&#20215;&#20540;&#12290;&#28982;&#32780;&#65292;&#27169;&#20307;&#33050;&#25163;&#26550;&#38382;&#39064;&#30340;&#36890;&#29992;&#35299;&#20915;&#26041;&#26696;&#20173;&#28982;&#27809;&#26377;&#20986;&#29616;&#12290;&#30446;&#21069;&#29992;&#20110;&#33050;&#25163;&#26550;&#35774;&#35745;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#21482;&#36866;&#29992;&#20110;&#38271;&#24230;&#19981;&#36229;&#36807;20&#30340;&#19981;&#30495;&#23454;&#33050;&#25163;&#26550;&#25110;&#38590;&#20197;&#29983;&#25104;&#22810;&#20010;&#19981;&#21516;&#26679;&#24335;&#30340;&#33050;&#25163;&#26550;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#21033;&#29992;E(3)-&#31561;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#22810;&#31181;&#22810;&#26679;&#19988;&#38271;&#24230;&#26356;&#38271;&#30340;&#34507;&#30333;&#36136;&#20027;&#24178;&#32467;&#26500;&#30340;&#20998;&#24067;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;SMCDiff&#20197;&#26377;&#25928;&#22320;&#20174;&#32473;&#23450;&#30340;&#27169;&#20307;&#26465;&#20214;&#19979;&#30340;&#20998;&#24067;&#20013;&#37319;&#26679;&#20986;&#33050;&#25163;&#26550;&#65307;&#25105;&#20204;&#30340;&#31639;&#27861;&#26159;&#31532;&#19968;&#20010;&#29702;&#35770;&#19978;&#20445;&#35777;&#20174;&#22823;&#35745;&#31639;&#26497;&#38480;&#20013;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#26465;&#20214;&#26679;&#26412;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#19982;AlphaFold2&#39044;&#27979;&#32467;&#26500;&#30340;&#23545;&#40784;&#31243;&#24230;&#35780;&#20272;&#25105;&#20204;&#35774;&#35745;&#30340;&#20027;&#24178;&#12290;&#25105;&#20204;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;(1)&#37319;&#26679;&#38271;&#36798;80&#20010;&#27531;&#22522;&#30340;&#33050;&#25163;&#26550;&#65292;&#24182;&#19988;(2)&#20026;&#22266;&#23450;&#27169;&#20307;&#23454;&#29616;&#32467;&#26500;&#22810;&#26679;&#30340;&#33050;&#25163;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Construction of a scaffold structure that supports a desired motif, conferring protein function, shows promise for the design of vaccines and enzymes. But a general solution to this motif-scaffolding problem remains open. Current machine-learning techniques for scaffold design are either limited to unrealistically small scaffolds (up to length 20) or struggle to produce multiple diverse scaffolds. We propose to learn a distribution over diverse and longer protein backbone structures via an E(3)-equivariant graph neural network. We develop SMCDiff to efficiently sample scaffolds from this distribution conditioned on a given motif; our algorithm is the first to theoretically guarantee conditional samples from a diffusion model in the large-compute limit. We evaluate our designed backbones by how well they align with AlphaFold2-predicted structures. We show that our method can (1) sample scaffolds up to 80 residues and (2) achieve structurally diverse scaffolds for a fixed motif.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#21019;&#26032;&#24615;&#26816;&#27979;&#26041;&#26696;&#65292;&#20197;&#20445;&#25252;&#37319;&#29992;&#36825;&#20123;&#26041;&#27861;&#30340;&#26080;&#20154;&#26426;&#20813;&#21463;&#23545;&#25239;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2206.02670</link><description>&lt;p&gt;
&#22522;&#20110;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26080;&#20154;&#26426;&#24341;&#23548;&#21644;&#35268;&#21010;&#30340;&#40065;&#26834;&#24615;&#23545;&#25239;&#25915;&#20987;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Robust Adversarial Attacks Detection based on Explainable Deep Reinforcement Learning For UAV Guidance and Planning. (arXiv:2206.02670v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.02670
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#21019;&#26032;&#24615;&#26816;&#27979;&#26041;&#26696;&#65292;&#20197;&#20445;&#25252;&#37319;&#29992;&#36825;&#20123;&#26041;&#27861;&#30340;&#26080;&#20154;&#26426;&#20813;&#21463;&#23545;&#25239;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#26080;&#20154;&#26426;&#22312;&#20844;&#20849;&#39046;&#22495;&#36973;&#21463;&#23545;&#25239;&#25915;&#20987;&#30340;&#39118;&#38505;&#22686;&#21152;&#30340;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#21019;&#26032;&#24615;&#26816;&#27979;&#26041;&#26696;&#65292;&#20197;&#20445;&#25252;&#37319;&#29992;&#36825;&#20123;&#26041;&#27861;&#30340;&#26080;&#20154;&#26426;&#20813;&#21463;&#25915;&#20987;&#12290;&#35813;&#26041;&#26696;&#37319;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#24341;&#23548;&#21644;&#35268;&#21010;&#65292;&#21033;&#29992;&#20154;&#24037;&#21183;&#22330;&#26469;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#21644;&#38556;&#30861;&#29289;&#36991;&#20813;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The dangers of adversarial attacks on Uncrewed Aerial Vehicle (UAV) agents operating in public are increasing. Adopting AI-based techniques and, more specifically, Deep Learning (DL) approaches to control and guide these UAVs can be beneficial in terms of performance but can add concerns regarding the safety of those techniques and their vulnerability against adversarial attacks. Confusion in the agent's decision-making process caused by these attacks can seriously affect the safety of the UAV. This paper proposes an innovative approach based on the explainability of DL methods to build an efficient detector that will protect these DL schemes and the UAVs adopting them from attacks. The agent adopts a Deep Reinforcement Learning (DRL) scheme for guidance and planning. The agent is trained with a Deep Deterministic Policy Gradient (DDPG) with Prioritised Experience Replay (PER) DRL scheme that utilises Artificial Potential Field (APF) to improve training times and obstacle avoidance per
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20027;&#25104;&#20998;&#20998;&#26512;&#30340;&#32570;&#22833;&#25968;&#25454;&#25554;&#34917;&#26694;&#26550;&#65292;&#21487;&#36866;&#29992;&#20110;&#39640;&#32500;&#25968;&#25454;&#65292;&#33021;&#22815;&#26174;&#33879;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#65292;&#21516;&#26102;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#23454;&#29616;&#21487;&#27604;&#36739;&#25110;&#26356;&#22909;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2205.15150</link><description>&lt;p&gt;
&#22522;&#20110;&#20027;&#25104;&#20998;&#20998;&#26512;&#30340;&#39640;&#25928;&#32570;&#22833;&#25968;&#25454;&#25554;&#34917;&#31639;&#27861;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Principal Component Analysis based frameworks for efficient missing data imputation algorithms. (arXiv:2205.15150v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.15150
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20027;&#25104;&#20998;&#20998;&#26512;&#30340;&#32570;&#22833;&#25968;&#25454;&#25554;&#34917;&#26694;&#26550;&#65292;&#21487;&#36866;&#29992;&#20110;&#39640;&#32500;&#25968;&#25454;&#65292;&#33021;&#22815;&#26174;&#33879;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#65292;&#21516;&#26102;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#23454;&#29616;&#21487;&#27604;&#36739;&#25110;&#26356;&#22909;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32570;&#22833;&#25968;&#25454;&#26159;&#23454;&#36341;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#35768;&#22810;&#22635;&#34917;&#32570;&#22833;&#20540;&#30340;&#26041;&#27861;&#24050;&#32463;&#34987;&#24320;&#21457;&#20986;&#26469;&#12290;&#28982;&#32780;&#65292;&#24182;&#19981;&#26159;&#25152;&#26377;&#30340;&#26041;&#27861;&#37117;&#33021;&#22815;&#36866;&#29992;&#20110;&#39640;&#32500;&#25968;&#25454;&#65292;&#29305;&#21035;&#26159;&#22810;&#37325;&#25554;&#34917;&#25216;&#26415;&#12290;&#21516;&#26102;&#65292;&#22914;&#20170;&#30340;&#25968;&#25454;&#36235;&#21521;&#20110;&#39640;&#32500;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#30340;&#31616;&#21333;&#32780;&#36890;&#29992;&#30340;&#32570;&#22833;&#20540;&#25554;&#34917;&#26694;&#26550;PCA Imputation&#65288;PCAI&#65289;&#65292;&#20197;&#21152;&#24555;&#25554;&#34917;&#36807;&#31243;&#24182;&#20943;&#36731;&#35768;&#22810;&#21487;&#29992;&#25554;&#34917;&#25216;&#26415;&#30340;&#20869;&#23384;&#38382;&#39064;&#65292;&#32780;&#19981;&#20250;&#29306;&#29298;&#22343;&#26041;&#35823;&#24046;&#26041;&#38754;&#30340;&#25554;&#34917;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#37096;&#20998;&#25110;&#20840;&#37096;&#32570;&#22833;&#29305;&#24449;&#26159;&#20998;&#31867;&#30340;&#65292;&#25110;&#32773;&#32570;&#22833;&#29305;&#24449;&#25968;&#37327;&#36739;&#22823;&#65292;&#35813;&#26694;&#26550;&#20063;&#21487;&#20197;&#20351;&#29992;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20171;&#32461;PCA Imputation - Classification&#65288;PIC&#65289;&#65292;&#36825;&#26159;&#23545;&#20855;&#26377;&#19968;&#20123;&#35843;&#25972;&#30340;&#20998;&#31867;&#38382;&#39064;PCAI&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#36890;&#36807;&#21508;&#31181;&#24773;&#20917;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#34920;&#26126;PCA I&#21644;PIC&#21487;&#20197;&#26174;&#33879;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#65292;&#21516;&#26102;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#23454;&#29616;&#21487;&#27604;&#36739;&#25110;&#26356;&#22909;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Missing data is a commonly occurring problem in practice. Many imputation methods have been developed to fill in the missing entries. However, not all of them can scale to high-dimensional data, especially the multiple imputation techniques. Meanwhile, the data nowadays tends toward high-dimensional. Therefore, in this work, we propose Principal Component Analysis Imputation (PCAI), a simple but versatile framework based on Principal Component Analysis (PCA) to speed up the imputation process and alleviate memory issues of many available imputation techniques, without sacrificing the imputation quality in term of MSE. In addition, the frameworks can be used even when some or all of the missing features are categorical, or when the number of missing features is large. Next, we introduce PCA Imputation - Classification (PIC), an application of PCAI for classification problems with some adjustments. We validate our approach by experiments on various scenarios, which shows that PCAI and PI
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#20027;&#20307;&#23884;&#20837;&#25216;&#26415;&#35299;&#20915;&#20010;&#20307;&#24046;&#24322;&#24102;&#26469;&#30340;&#32676;&#20307;&#32423;&#33041;&#35299;&#30721;&#38382;&#39064;&#65292;&#24182;&#22312;&#33041;&#30913;&#22270;&#20687;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2205.14102</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#32676;&#20307;&#32423;&#33041;&#35299;&#30721;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Group-level Brain Decoding with Deep Learning. (arXiv:2205.14102v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.14102
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#20027;&#20307;&#23884;&#20837;&#25216;&#26415;&#35299;&#20915;&#20010;&#20307;&#24046;&#24322;&#24102;&#26469;&#30340;&#32676;&#20307;&#32423;&#33041;&#35299;&#30721;&#38382;&#39064;&#65292;&#24182;&#22312;&#33041;&#30913;&#22270;&#20687;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#25104;&#20687;&#25968;&#25454;&#35299;&#30721;&#22312;&#33041;&#26426;&#25509;&#21475;&#21644;&#31070;&#32463;&#34920;&#31034;&#30740;&#31350;&#20013;&#24212;&#29992;&#36234;&#26469;&#36234;&#24191;&#27867;&#12290;&#30001;&#20110;&#21463;&#21040;&#20010;&#20307;&#20043;&#38388;&#21464;&#24322;&#30340;&#24433;&#21709;&#65292;&#35299;&#30721;&#36890;&#24120;&#26159;&#38024;&#23545;&#20010;&#20307;&#30340;&#65292;&#24182;&#19988;&#22312;&#20010;&#20307;&#38388;&#26080;&#27861;&#24456;&#22909;&#22320;&#25512;&#24191;&#12290;&#20811;&#26381;&#36825;&#19968;&#38382;&#39064;&#30340;&#26041;&#27861;&#19981;&#20165;&#21487;&#20197;&#25552;&#20379;&#26356;&#20016;&#23500;&#30340;&#31070;&#32463;&#31185;&#23398;&#35265;&#35299;&#65292;&#36824;&#21487;&#20197;&#20351;&#32676;&#20307;&#27169;&#22411;&#20248;&#20110;&#20010;&#20307;&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#31867;&#20284;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#21333;&#35789;&#23884;&#20837;&#30340;&#20027;&#20307;&#23884;&#20837;&#26469;&#23398;&#20064;&#21644;&#21033;&#29992;&#20010;&#20307;&#38388;&#21464;&#21270;&#30340;&#32467;&#26500;&#20316;&#20026;&#35299;&#30721;&#27169;&#22411;&#30340;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#25913;&#32534;&#20102; WaveNet &#26550;&#26500;&#29992;&#20110;&#20998;&#31867;&#12290;&#25105;&#20204;&#23558;&#20854;&#24212;&#29992;&#20110;&#33041;&#30913;&#22270;&#20687;&#25968;&#25454;&#65292;15 &#20010;&#21463;&#35797;&#32773;&#35266;&#30475;&#20102; 118 &#31181;&#19981;&#21516;&#30340;&#22270;&#20687;&#65292;&#27599;&#20010;&#22270;&#20687;&#26377; 30 &#20010;&#26679;&#26412;&#65292;&#20351;&#29992;&#25972;&#20010;&#22270;&#20687;&#21576;&#29616;&#21518;&#30340; 1s &#31383;&#21475;&#36827;&#34892;&#22270;&#20687;&#20998;&#31867;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#28145;&#24230;&#23398;&#20064;&#21644;&#20027;&#20307;&#23884;&#20837;&#30340;&#32467;&#21512;&#23545;&#20110;&#20998;&#31867;&#20855;&#26377;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decoding brain imaging data is gaining popularity, with applications in brain-computer interfaces and the study of neural representations. Decoding is typically subject-specific and does not generalise well over subjects, due to high amounts of between subject variability. Techniques that overcome this will not only provide richer neuroscientific insights but also make it possible for group-level models to outperform subject-specific models. Here, we propose a method that uses subject embedding, analogous to word embedding in Natural Language Processing, to learn and exploit the structure in between-subject variability as part of a decoding model, our adaptation of the WaveNet architecture for classification. We apply this to magnetoencephalography data, where 15 subjects viewed 118 different images, with 30 examples per image; to classify images using the entire 1s window following image presentation. We show that the combination of deep learning and subject embedding is crucial to cl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#36817;&#20046;&#26368;&#20248;&#32479;&#35745;&#29575;&#30340;&#25308;&#21344;&#24237;&#40065;&#26834;&#32852;&#37030;&#23398;&#20064;&#21327;&#35758;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#19982;&#31454;&#20105;&#21327;&#35758;&#30456;&#27604;&#30340;&#32463;&#39564;&#20248;&#36234;&#24615;&#65292;&#21327;&#35758;&#36890;&#36807;&#20998;&#26742;&#21487;&#20197;&#32467;&#21512;&#38544;&#31169;&#20445;&#38556;&#31243;&#24207;&#20197;&#23545;&#21322;&#35802;&#23454;&#26381;&#21153;&#22120;&#36827;&#34892;&#23433;&#20840;&#20445;&#38556;&#12290;</title><link>http://arxiv.org/abs/2205.11765</link><description>&lt;p&gt;
&#20855;&#26377;&#26368;&#20248;&#32479;&#35745;&#29575;&#21644;&#38544;&#31169;&#20445;&#35777;&#30340;&#25308;&#21344;&#24237;&#40065;&#26834;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Byzantine-Robust Federated Learning with Optimal Statistical Rates and Privacy Guarantees. (arXiv:2205.11765v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.11765
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#36817;&#20046;&#26368;&#20248;&#32479;&#35745;&#29575;&#30340;&#25308;&#21344;&#24237;&#40065;&#26834;&#32852;&#37030;&#23398;&#20064;&#21327;&#35758;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#19982;&#31454;&#20105;&#21327;&#35758;&#30456;&#27604;&#30340;&#32463;&#39564;&#20248;&#36234;&#24615;&#65292;&#21327;&#35758;&#36890;&#36807;&#20998;&#26742;&#21487;&#20197;&#32467;&#21512;&#38544;&#31169;&#20445;&#38556;&#31243;&#24207;&#20197;&#23545;&#21322;&#35802;&#23454;&#26381;&#21153;&#22120;&#36827;&#34892;&#23433;&#20840;&#20445;&#38556;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#36817;&#20046;&#26368;&#20248;&#32479;&#35745;&#29575;&#30340;&#25308;&#21344;&#24237;&#40065;&#26834;&#32852;&#37030;&#23398;&#20064;&#21327;&#35758;&#12290;&#19982;&#20043;&#21069;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#21327;&#35758;&#25552;&#39640;&#20102;&#32500;&#24230;&#20381;&#36182;&#24615;&#65292;&#24182;&#22312;&#24378;&#20984;&#25439;&#22833;&#30340;&#25152;&#26377;&#21442;&#25968;&#26041;&#38754;&#23454;&#29616;&#20102;&#32039;&#23494;&#30340;&#32479;&#35745;&#29575;&#12290;&#25105;&#20204;&#23545;&#31454;&#20105;&#21327;&#35758;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#21327;&#35758;&#30340;&#32463;&#39564;&#20248;&#36234;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25351;&#20986;&#65292;&#25105;&#20204;&#30340;&#20998;&#26742;&#21327;&#35758;&#21487;&#20197;&#19982;&#38544;&#31169;&#20445;&#38556;&#31243;&#24207;&#33258;&#28982;&#22320;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#24341;&#20837;&#23545;&#21322;&#35802;&#23454;&#26381;&#21153;&#22120;&#30340;&#23433;&#20840;&#20445;&#38556;&#12290;&#35780;&#20272;&#20195;&#30721;&#20301;&#20110;https://github.com/wanglun1996/secure-robust-federated-learning&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Byzantine-robust federated learning protocols with nearly optimal statistical rates. In contrast to prior work, our proposed protocols improve the dimension dependence and achieve a tight statistical rate in terms of all the parameters for strongly convex losses. We benchmark against competing protocols and show the empirical superiority of the proposed protocols. Finally, we remark that our protocols with bucketing can be naturally combined with privacy-guaranteeing procedures to introduce security against a semi-honest server. The code for evaluation is provided in https://github.com/wanglun1996/secure-robust-federated-learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#22312;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#31354;&#38388;&#20013;&#30340;&#26799;&#24230;&#19979;&#38477;&#26041;&#24335;&#26469;&#35299;&#20915;&#26080;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;GAN&#30340;&#35757;&#32451;&#36807;&#31243;&#21487;&#20197;&#34987;&#35270;&#20026;&#19968;&#31181;&#20998;&#24067;&#20381;&#36182;&#30340;ODE&#30340;&#27169;&#25311;&#12290;GAN&#31639;&#27861;&#26368;&#23567;&#21270;&#20102;&#20004;&#32452;&#26679;&#26412;&#20043;&#38388;&#30340;&#22343;&#26041;&#35823;&#24046;&#65292;&#24182;&#19988;&#21482;&#26377;&#22312;&#21028;&#21035;&#22120;&#36275;&#22815;&#24378;&#22823;&#26102;&#25165;&#33021;&#30495;&#27491;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2205.02910</link><description>&lt;p&gt;
GAN&#20316;&#20026;&#28176;&#36827;&#27969;&#30340;&#26799;&#24230;&#19979;&#38477;
&lt;/p&gt;
&lt;p&gt;
GANs as Gradient Flows that Converge. (arXiv:2205.02910v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.02910
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22312;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#31354;&#38388;&#20013;&#30340;&#26799;&#24230;&#19979;&#38477;&#26041;&#24335;&#26469;&#35299;&#20915;&#26080;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;GAN&#30340;&#35757;&#32451;&#36807;&#31243;&#21487;&#20197;&#34987;&#35270;&#20026;&#19968;&#31181;&#20998;&#24067;&#20381;&#36182;&#30340;ODE&#30340;&#27169;&#25311;&#12290;GAN&#31639;&#27861;&#26368;&#23567;&#21270;&#20102;&#20004;&#32452;&#26679;&#26412;&#20043;&#38388;&#30340;&#22343;&#26041;&#35823;&#24046;&#65292;&#24182;&#19988;&#21482;&#26377;&#22312;&#21028;&#21035;&#22120;&#36275;&#22815;&#24378;&#22823;&#26102;&#25165;&#33021;&#30495;&#27491;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22312;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#31354;&#38388;&#20013;&#30340;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#26469;&#35299;&#20915;&#26080;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#12290;&#20854;&#20013;&#65292;&#19968;&#20010;&#20027;&#35201;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#20998;&#24067;&#20381;&#36182;&#30340;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#24341;&#23548;&#30340;&#26799;&#24230;&#27969;&#65292;&#26410;&#30693;&#30340;&#25968;&#25454;&#20998;&#24067;&#23558;&#22312;&#38271;&#26102;&#38388;&#26497;&#38480;&#19979;&#20986;&#29616;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#36890;&#36807;&#27169;&#25311;&#20998;&#24067;&#20381;&#36182;ODE&#65292;&#21487;&#20197;&#25581;&#31034;&#25968;&#25454;&#20998;&#24067;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;ODE&#30340;&#27169;&#25311;&#34987;&#35777;&#26126;&#31561;&#20215;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#30340;&#35757;&#32451;&#12290;&#36825;&#31181;&#31561;&#20215;&#20851;&#31995;&#25552;&#20379;&#20102;GAN&#30340;&#26032;&#8220;&#21512;&#20316;&#8221;&#35270;&#35282;&#65292;&#24182;&#19988;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#20026;GAN&#30340;&#21457;&#25955;&#24102;&#26469;&#20102;&#26032;&#30340;&#21551;&#31034;&#12290;&#29305;&#21035;&#22320;&#65292;&#23427;&#25581;&#31034;&#20102;GAN&#31639;&#27861;&#38544;&#24335;&#22320;&#26368;&#23567;&#21270;&#20102;&#20004;&#32452;&#26679;&#26412;&#20043;&#38388;&#30340;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#65292;&#20165;&#20165;&#36825;&#20010;MSE&#25311;&#21512;&#23601;&#36275;&#20197;&#23548;&#33268;GAN&#21457;&#25955;&#12290;&#20026;&#20102;&#26500;&#24314;&#20998;&#24067;&#20381;&#36182;ODE&#30340;&#35299;&#65292;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#30456;&#20851;&#30340;&#38750;&#32447;&#24615;Fokker-Planck&#26041;&#31243;&#30340;&#24369;&#35299;&#26159;&#21807;&#19968;&#30340;&#65292;&#36825;&#26159;&#36890;&#36807;Crandall-Liggett&#23450;&#29702;&#24471;&#21040;&#30340;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#19968;&#20010;&#24050;&#30693;&#30340;&#21453;&#21521;&#37319;&#26679;&#25216;&#26415;&#30340;&#31616;&#21333;&#20462;&#25913;&#65292;&#21487;&#20197;&#26500;&#24314;&#19968;&#20010;&#25968;&#20540;&#26041;&#26696;&#26469;&#36817;&#20284;ODE&#30340;&#35299;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#20010;&#25968;&#20540;&#26041;&#26696;&#26469;&#23637;&#31034;&#65292;&#24403;&#21028;&#21035;&#22120;&#36275;&#22815;&#24378;&#22823;&#26102;&#65292;GAN&#31639;&#27861;&#30830;&#23454;&#25910;&#25947;&#20110;&#25968;&#25454;&#20998;&#24067;&#65292;&#20174;&#32780;&#20026;GAN&#25910;&#25947;&#30340;&#38271;&#26399;&#38590;&#39064;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper approaches the unsupervised learning problem by gradient descent in the space of probability density functions. A main result shows that along the gradient flow induced by a distribution-dependent ordinary differential equation (ODE), the unknown data distribution emerges as the long-time limit. That is, one can uncover the data distribution by simulating the distribution-dependent ODE. Intriguingly, the simulation of the ODE is shown equivalent to the training of generative adversarial networks (GANs). This equivalence provides a new "cooperative" view of GANs and, more importantly, sheds new light on the divergence of GANs. In particular, it reveals that the GAN algorithm implicitly minimizes the mean squared error (MSE) between two sets of samples, and this MSE fitting alone can cause GANs to diverge. To construct a solution to the distribution-dependent ODE, we first show that the associated nonlinear Fokker-Planck equation has a unique weak solution, by the Crandall-Lig
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;&#22870;&#21169;&#25253;&#21578;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35760;&#24405;&#24050;&#37096;&#32626;&#21644;&#36845;&#20195;&#26356;&#26032;&#30340;&#23398;&#20064;&#31995;&#32479;&#12290;&#36890;&#36807;&#36861;&#36394;&#35774;&#35745;&#36873;&#25321;&#21644;&#20551;&#35774;&#65292;&#24110;&#21161;&#36879;&#26126;&#22320;&#20256;&#36798;&#31995;&#32479;&#30446;&#26631;&#65292;&#24182;&#36319;&#36394;&#30446;&#26631;&#30340;&#28436;&#21464;&#12290;</title><link>http://arxiv.org/abs/2204.10817</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#30340;&#22870;&#21169;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
Reward Reports for Reinforcement Learning. (arXiv:2204.10817v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.10817
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;&#22870;&#21169;&#25253;&#21578;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35760;&#24405;&#24050;&#37096;&#32626;&#21644;&#36845;&#20195;&#26356;&#26032;&#30340;&#23398;&#20064;&#31995;&#32479;&#12290;&#36890;&#36807;&#36861;&#36394;&#35774;&#35745;&#36873;&#25321;&#21644;&#20551;&#35774;&#65292;&#24110;&#21161;&#36879;&#26126;&#22320;&#20256;&#36798;&#31995;&#32479;&#30446;&#26631;&#65292;&#24182;&#36319;&#36394;&#30446;&#26631;&#30340;&#28436;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38754;&#23545;&#22797;&#26434;&#30340;&#31038;&#20250;&#24433;&#21709;&#26102;&#26500;&#24314;&#23545;&#31038;&#20250;&#26377;&#30410;&#30340;&#31995;&#32479;&#38656;&#35201;&#19968;&#31181;&#21160;&#24577;&#30340;&#26041;&#27861;&#12290;&#26368;&#36817;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#25991;&#29486;&#35760;&#24405;&#26041;&#27861;&#23637;&#31034;&#20102;&#35752;&#35770;&#36825;&#20123;&#22797;&#26434;&#24615;&#30340;&#25991;&#26412;&#26694;&#26550;&#30340;&#21069;&#26223;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#21457;&#23637;&#22522;&#20110;&#38745;&#24577;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539;&#20363;&#65292;&#24573;&#30053;&#20102;&#21453;&#39304;&#21644;&#37096;&#32626;&#21518;&#24615;&#33021;&#30340;&#20316;&#29992;&#12290;&#21516;&#26102;&#65292;&#24378;&#21270;&#23398;&#20064;&#30340;&#26368;&#36817;&#24037;&#20316;&#34920;&#26126;&#65292;&#21453;&#39304;&#21644;&#20248;&#21270;&#30446;&#26631;&#23545;&#31995;&#32479;&#34892;&#20026;&#30340;&#24433;&#21709;&#21487;&#33021;&#26159;&#24191;&#27867;&#19988;&#19981;&#21487;&#39044;&#27979;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#22870;&#21169;&#25253;&#21578;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35760;&#24405;&#24050;&#37096;&#32626;&#21644;&#36845;&#20195;&#26356;&#26032;&#30340;&#23398;&#20064;&#31995;&#32479;&#12290;&#21463;&#21040;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#25991;&#29486;&#30340;&#21508;&#31181;&#36129;&#29486;&#21551;&#31034;&#65292;&#25105;&#20204;&#23558;&#22870;&#21169;&#25253;&#21578;&#27010;&#36848;&#20026;&#36319;&#36394;&#19968;&#20010;&#29305;&#23450;&#33258;&#21160;&#21270;&#31995;&#32479;&#27491;&#22312;&#20248;&#21270;&#30340;&#35774;&#35745;&#36873;&#25321;&#21644;&#20551;&#35774;&#30340;&#27963;&#21160;&#25991;&#20214;&#12290;&#23427;&#20204;&#26088;&#22312;&#20316;&#20026;&#36879;&#26126;&#22320;&#20256;&#36798;&#31995;&#32479;&#30446;&#26631;&#30340;&#25163;&#27573;&#65292;&#24182;&#36319;&#36394;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#19981;&#26029;&#28436;&#21464;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#20102;&#35299;&#31038;&#20250;&#24433;&#21709;&#25216;&#26415;&#30340;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building systems that are good for society in the face of complex societal effects requires a dynamic approach. Recent approaches to machine learning (ML) documentation have demonstrated the promise of discursive frameworks for deliberation about these complexities. However, these developments have been grounded in a static ML paradigm, leaving the role of feedback and post-deployment performance unexamined. Meanwhile, recent work in reinforcement learning has shown that the effects of feedback and optimization objectives on system behavior can be wide-ranging and unpredictable. In this paper we sketch a framework for documenting deployed and iteratively updated learning systems, which we call Reward Reports. Taking inspiration from various contributions to the technical literature on reinforcement learning, we outline Reward Reports as living documents that track updates to design choices and assumptions behind what a particular automated system is optimizing for. They are intended to
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#20559;&#22909;&#30340;&#22870;&#21169;&#23398;&#20064;&#20013;&#23384;&#22312;&#22240;&#26524;&#28151;&#28102;&#21644;&#22870;&#21169;&#35823;&#35782;&#21035;&#65292;&#38750;&#22240;&#26524;&#20998;&#25955;&#29305;&#24449;&#12289;&#20559;&#22909;&#20013;&#30340;&#22122;&#22768;&#20197;&#21450;&#29366;&#24577;&#30340;&#23616;&#37096;&#21487;&#35266;&#23519;&#24615;&#21487;&#33021;&#21152;&#21095;&#22870;&#21169;&#35823;&#35782;&#21035;&#65292;&#24517;&#39035;&#27880;&#24847;&#20445;&#35777;&#23398;&#20064;&#21040;&#30340;&#22870;&#21169;&#30340;&#21487;&#35782;&#21035;&#24615;&#21644;&#22240;&#26524;&#24615;&#12290;</title><link>http://arxiv.org/abs/2204.06601</link><description>&lt;p&gt;
&#22522;&#20110;&#20559;&#22909;&#30340;&#22870;&#21169;&#23398;&#20064;&#20013;&#30340;&#22240;&#26524;&#28151;&#28102;&#21644;&#22870;&#21169;&#35823;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Causal Confusion and Reward Misidentification in Preference-Based Reward Learning. (arXiv:2204.06601v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.06601
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20559;&#22909;&#30340;&#22870;&#21169;&#23398;&#20064;&#20013;&#23384;&#22312;&#22240;&#26524;&#28151;&#28102;&#21644;&#22870;&#21169;&#35823;&#35782;&#21035;&#65292;&#38750;&#22240;&#26524;&#20998;&#25955;&#29305;&#24449;&#12289;&#20559;&#22909;&#20013;&#30340;&#22122;&#22768;&#20197;&#21450;&#29366;&#24577;&#30340;&#23616;&#37096;&#21487;&#35266;&#23519;&#24615;&#21487;&#33021;&#21152;&#21095;&#22870;&#21169;&#35823;&#35782;&#21035;&#65292;&#24517;&#39035;&#27880;&#24847;&#20445;&#35777;&#23398;&#20064;&#21040;&#30340;&#22870;&#21169;&#30340;&#21487;&#35782;&#21035;&#24615;&#21644;&#22240;&#26524;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#20559;&#22909;&#30340;&#22870;&#21169;&#23398;&#20064;&#26469;&#23398;&#20064;&#31574;&#30053;&#26159;&#23450;&#21046;&#26234;&#33021;&#20307;&#34892;&#20026;&#30340;&#19968;&#31181;&#36234;&#26469;&#36234;&#27969;&#34892;&#30340;&#26041;&#27861;&#65292;&#20294;&#25454;&#35828;&#20250;&#23481;&#26131;&#20986;&#29616;&#34394;&#20551;&#30456;&#20851;&#24615;&#21644;&#22870;&#21169;&#27450;&#39575;&#34892;&#20026;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#20559;&#22909;&#20013;&#23398;&#20064;&#26102;&#22240;&#26524;&#28151;&#28102;&#21644;&#22870;&#21169;&#35823;&#35782;&#21035;&#30340;&#31995;&#32479;&#24615;&#30740;&#31350;&#65292;&#32780;&#29616;&#26377;&#30340;&#22823;&#37096;&#20998;&#24037;&#20316;&#21017;&#20851;&#27880;&#20110;&#24378;&#21270;&#23398;&#20064;&#21644;&#34892;&#20026;&#20811;&#38534;&#20013;&#30340;&#22240;&#26524;&#28151;&#28102;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22522;&#20934;&#39046;&#22495;&#19978;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#30340;&#25935;&#24863;&#24615;&#21644;&#28040;&#34701;&#20998;&#26512;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#20174;&#20559;&#22909;&#20013;&#23398;&#20064;&#21040;&#30340;&#22870;&#21169;&#34429;&#28982;&#21487;&#20197;&#22312;&#23398;&#20064;&#29615;&#22659;&#20013;&#33719;&#24471;&#26368;&#23567;&#30340;&#27979;&#35797;&#35823;&#24046;&#65292;&#20294;&#22312;&#20998;&#24067;&#19981;&#21516;&#30340;&#29366;&#24577;&#19979;&#26080;&#27861;&#36827;&#34892;&#27867;&#21270;&#65292;&#20174;&#32780;&#23548;&#33268;&#31574;&#30053;&#22312;&#20248;&#21270;&#26102;&#34920;&#29616;&#36739;&#24046;&#12290;&#25105;&#20204;&#21457;&#29616;&#38750;&#22240;&#26524;&#20998;&#25955;&#29305;&#24449;&#30340;&#23384;&#22312;&#12289;&#38472;&#36848;&#20559;&#22909;&#20013;&#30340;&#22122;&#22768;&#20197;&#21450;&#23616;&#37096;&#29366;&#24577;&#30340;&#21487;&#35266;&#23519;&#24615;&#37117;&#21487;&#33021;&#21152;&#21095;&#22870;&#21169;&#35823;&#35782;&#21035;&#12290;&#25105;&#20204;&#36824;&#30830;&#23450;&#20102;&#19968;&#32452;&#26041;&#27861;&#26469;&#35299;&#37322;&#34987;&#35823;&#35782;&#21035;&#30340;&#23398;&#20064;&#22870;&#21169;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20559;&#22909;&#22870;&#21169;&#23398;&#20064;&#20013;&#20248;&#21270;&#30340;&#31574;&#30053;&#23545;&#22870;&#21169;&#20989;&#25968;&#30340;&#36873;&#25321;&#38750;&#24120;&#25935;&#24863;&#65292;&#24517;&#39035;&#27880;&#24847;&#30830;&#20445;&#23398;&#20064;&#21040;&#30340;&#22870;&#21169;&#30340;&#21487;&#35782;&#21035;&#24615;&#21644;&#22240;&#26524;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning policies via preference-based reward learning is an increasingly popular method for customizing agent behavior, but has been shown anecdotally to be prone to spurious correlations and reward hacking behaviors. While much prior work focuses on causal confusion in reinforcement learning and behavioral cloning, we focus on a systematic study of causal confusion and reward misidentification when learning from preferences. In particular, we perform a series of sensitivity and ablation analyses on several benchmark domains where rewards learned from preferences achieve minimal test error but fail to generalize to out-of-distribution states -- resulting in poor policy performance when optimized. We find that the presence of non-causal distractor features, noise in the stated preferences, and partial state observability can all exacerbate reward misidentification. We also identify a set of methods with which to interpret misidentified learned rewards. In general, we observe that optim
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20027;&#35201;&#30740;&#31350;&#20102;&#20351;&#29992;&#24773;&#24863;&#20449;&#21495;&#39044;&#27979;&#21152;&#23494;&#36135;&#24065;&#22238;&#25253;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24369;&#30417;&#30563;&#23398;&#20064;&#30340;NLP&#26041;&#27861;&#26469;&#35299;&#20915;&#25991;&#26412;&#25968;&#25454;&#26080;&#26631;&#31614;&#30340;&#38382;&#39064;&#65292;&#24182;&#35777;&#23454;&#23427;&#33021;&#25552;&#39640;&#39044;&#27979;&#20215;&#20540;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2204.05781</link><description>&lt;p&gt;
&#20174;&#24773;&#24863;&#20449;&#21495;&#39044;&#27979;&#21152;&#23494;&#36135;&#24065;&#22238;&#25253;&#65306;BERT&#20998;&#31867;&#22120;&#21644;&#24369;&#30417;&#30563;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Forecasting Cryptocurrency Returns from Sentiment Signals: An Analysis of BERT Classifiers and Weak Supervision. (arXiv:2204.05781v3 [q-fin.ST] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.05781
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20027;&#35201;&#30740;&#31350;&#20102;&#20351;&#29992;&#24773;&#24863;&#20449;&#21495;&#39044;&#27979;&#21152;&#23494;&#36135;&#24065;&#22238;&#25253;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24369;&#30417;&#30563;&#23398;&#20064;&#30340;NLP&#26041;&#27861;&#26469;&#35299;&#20915;&#25991;&#26412;&#25968;&#25454;&#26080;&#26631;&#31614;&#30340;&#38382;&#39064;&#65292;&#24182;&#35777;&#23454;&#23427;&#33021;&#25552;&#39640;&#39044;&#27979;&#20215;&#20540;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39044;&#27979;&#20013;&#39044;&#27979;&#37329;&#34701;&#24066;&#22330;&#20215;&#26684;&#21464;&#21160;&#26159;&#19968;&#20010;&#21463;&#25345;&#32493;&#20851;&#27880;&#30340;&#35805;&#39064;&#12290;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#36827;&#27493;&#20197;&#21450;&#20197;&#26032;&#38395;&#25991;&#31456;&#12289;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#31561;&#24418;&#24335;&#25552;&#20379;&#30340;&#22823;&#37327;&#25991;&#26412;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#23558;&#22522;&#20110;&#25991;&#26412;&#30340;&#39044;&#27979;&#22240;&#32032;&#32435;&#20837;&#39044;&#27979;&#27169;&#22411;&#20013;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#24369;&#30417;&#30563;&#23398;&#20064;&#65292;&#19968;&#31181;&#26368;&#36817;&#25552;&#20986;&#30340;NLP&#26041;&#27861;&#26469;&#35299;&#20915;&#25991;&#26412;&#25968;&#25454;&#26080;&#26631;&#31614;&#30340;&#38382;&#39064;&#65292;&#20026;&#36825;&#20010;&#39046;&#22495;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#27809;&#26377;&#20381;&#36182;&#21464;&#37327;&#65292;&#23601;&#19981;&#21487;&#33021;&#22312;&#33258;&#23450;&#20041;&#35821;&#26009;&#24211;&#19978;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;NLP&#27169;&#22411;&#12290;&#25105;&#20204;&#35777;&#23454;&#20102;&#20351;&#29992;&#24369;&#26631;&#31614;&#24494;&#35843;&#30340;&#25991;&#26412;&#29305;&#24449;&#22686;&#24378;&#20102;&#39044;&#27979;&#20215;&#20540;&#65292;&#24182;&#25552;&#39640;&#20102;&#39044;&#27979;&#21152;&#23494;&#36135;&#24065;&#25910;&#30410;&#30340;&#20934;&#30830;&#24615;&#12290;&#26356;&#22522;&#26412;&#30340;&#26159;&#65292;&#25105;&#20204;&#25152;&#25552;&#20986;&#30340;&#24314;&#27169;&#33539;&#24335;&#8212;&#8212;&#24369;&#26631;&#35760;&#29305;&#23450;&#39046;&#22495;&#25991;&#26412;&#21644;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;NLP&#27169;&#22411;&#8212;&#8212;&#22312;&#65288;&#37329;&#34701;&#65289;&#39044;&#27979;&#20013;&#26222;&#36866;&#36866;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anticipating price developments in financial markets is a topic of continued interest in forecasting. Funneled by advancements in deep learning and natural language processing (NLP) together with the availability of vast amounts of textual data in form of news articles, social media postings, etc., an increasing number of studies incorporate text-based predictors in forecasting models. We contribute to this literature by introducing weak learning, a recently proposed NLP approach to address the problem that text data is unlabeled. Without a dependent variable, it is not possible to finetune pretrained NLP models on a custom corpus. We confirm that finetuning using weak labels enhances the predictive value of text-based features and raises forecast accuracy in the context of predicting cryptocurrency returns. More fundamentally, the modeling paradigm we present, weak labeling domain-specific text and finetuning pretrained NLP models, is universally applicable in (financial) forecasting 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#39044;&#21098;&#26525;&#22312;&#31070;&#32463;&#32593;&#32476;&#30340;&#31070;&#32463;&#20999;&#21521;&#26680;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#24471;&#20986;&#20102;&#20004;&#31181;&#24773;&#20917;&#19979;&#30340;&#32467;&#35770;&#12290;</title><link>http://arxiv.org/abs/2203.14328</link><description>&lt;p&gt;
&#38543;&#26426;&#39044;&#21098;&#26525;&#31070;&#32463;&#32593;&#32476;&#30340;&#31070;&#32463;&#20999;&#21521;&#26680;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
On the Neural Tangent Kernel Analysis of Randomly Pruned Neural Networks. (arXiv:2203.14328v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.14328
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39044;&#21098;&#26525;&#22312;&#31070;&#32463;&#32593;&#32476;&#30340;&#31070;&#32463;&#20999;&#21521;&#26680;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#24471;&#20986;&#20102;&#20004;&#31181;&#24773;&#20917;&#19979;&#30340;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39044;&#21098;&#26525;&#22312;&#31070;&#32463;&#32593;&#32476;&#30340;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;NTK&#65289;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#35777;&#26126;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#39044;&#21098;&#26525;&#30340;&#26435;&#37325;&#21644;&#21407;&#22987;&#32593;&#32476;&#30340;NTK&#31561;&#20215;&#65292;&#24471;&#20986;&#20102;&#20004;&#31181;&#24773;&#20917;&#19979;&#30340;&#32467;&#35770;&#65306;&#24403;&#27599;&#23618;&#30340;&#23485;&#24230;&#25353;&#39034;&#24207;&#22686;&#38271;&#21040;&#26080;&#38480;&#22823;&#26102;&#65292;&#39044;&#21098;&#26525;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;NTK&#20250;&#28176;&#36817;&#20110;&#21407;&#22987;&#32593;&#32476;&#30340;&#26497;&#38480;NTK&#65292;&#32780;&#23485;&#24230;&#23545;&#31232;&#30095;&#21442;&#25968;&#30340;&#20381;&#36182;&#26159;&#28176;&#36817;&#32447;&#24615;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by both theory and practice, we study how random pruning of the weights affects a neural network's neural tangent kernel (NTK). In particular, this work establishes an equivalence of the NTKs between a fully-connected neural network and its randomly pruned version. The equivalence is established under two cases. The first main result studies the infinite-width asymptotic. It is shown that given a pruning probability, for fully-connected neural networks with the weights randomly pruned at the initialization, as the width of each layer grows to infinity sequentially, the NTK of the pruned neural network converges to the limiting NTK of the original network with some extra scaling. If the network weights are rescaled appropriately after pruning, this extra scaling can be removed. The second main result considers the finite-width case. It is shown that to ensure the NTK's closeness to the limit, the dependence of width on the sparsity parameter is asymptotically linear, as the NT
&lt;/p&gt;</description></item><item><title>Sionna&#26159;&#19968;&#20010;GPU&#21152;&#36895;&#30340;&#22522;&#20110;TensorFlow&#30340;&#24320;&#28304;&#24211;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#31639;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#22522;&#20934;&#27979;&#35797;&#21644;&#31471;&#21040;&#31471;&#24615;&#33021;&#35780;&#20272;&#12290;&#23427;&#33021;&#22815;&#25552;&#20379;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#21407;&#29983;&#25903;&#25345;&#65292;&#20351;&#24471;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#26356;&#21152;&#19987;&#27880;&#20110;&#33258;&#24049;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2203.11854</link><description>&lt;p&gt;
Sionna&#65306;&#19979;&#19968;&#20195;&#29289;&#29702;&#23618;&#30740;&#31350;&#30340;&#24320;&#28304;&#24211;
&lt;/p&gt;
&lt;p&gt;
Sionna: An Open-Source Library for Next-Generation Physical Layer Research. (arXiv:2203.11854v2 [cs.IT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.11854
&lt;/p&gt;
&lt;p&gt;
Sionna&#26159;&#19968;&#20010;GPU&#21152;&#36895;&#30340;&#22522;&#20110;TensorFlow&#30340;&#24320;&#28304;&#24211;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#31639;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#22522;&#20934;&#27979;&#35797;&#21644;&#31471;&#21040;&#31471;&#24615;&#33021;&#35780;&#20272;&#12290;&#23427;&#33021;&#22815;&#25552;&#20379;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#21407;&#29983;&#25903;&#25345;&#65292;&#20351;&#24471;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#26356;&#21152;&#19987;&#27880;&#20110;&#33258;&#24049;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Sionna&#26159;&#19968;&#31181;&#22522;&#20110;TensorFlow&#30340;GPU&#21152;&#36895;&#24320;&#28304;&#24211;&#65292;&#29992;&#20110;&#38142;&#36335;&#23618;&#27169;&#25311;&#12290;&#23427;&#23454;&#29616;&#20102;&#24191;&#27867;&#32780;&#31934;&#24515;&#27979;&#35797;&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#65292;&#21487;&#29992;&#20110;&#22522;&#20934;&#27979;&#35797;&#21644;&#31471;&#21040;&#31471;&#24615;&#33021;&#35780;&#20272;&#65292;&#24182;&#25552;&#20379;&#21407;&#29983;&#25903;&#25345;&#31070;&#32463;&#32593;&#32476;&#30340;&#38598;&#25104;&#12290;&#36825;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#19987;&#27880;&#20110;&#20182;&#20204;&#30340;&#30740;&#31350;&#65292;&#20351;&#20854;&#26356;&#20855;&#24433;&#21709;&#21147;&#21644;&#21487;&#37325;&#22797;&#24615;&#65292;&#21516;&#26102;&#33410;&#30465;&#20102;&#23454;&#29616;&#20182;&#20204;&#25152;&#19981;&#29087;&#24713;&#30340;&#32452;&#20214;&#30340;&#26102;&#38388;&#12290;&#26412;&#25991;&#31616;&#35201;&#20171;&#32461;&#20102;Sionna&#65292;&#35299;&#37322;&#20102;&#23427;&#30340;&#35774;&#35745;&#21407;&#21017;&#21644;&#21151;&#33021;&#65292;&#20197;&#21450;&#26410;&#26469;&#30340;&#25193;&#23637;&#65292;&#22914;&#38598;&#25104;&#20809;&#32447;&#36319;&#36394;&#21644;&#33258;&#23450;&#20041;CUDA&#20869;&#26680;&#12290;&#25105;&#20204;&#35748;&#20026;Sionna&#26159;&#30740;&#31350;&#19979;&#19968;&#20195;&#36890;&#20449;&#31995;&#32479;&#65288;&#22914;6G&#65289;&#30340;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#65292;&#27426;&#36814;&#26469;&#33258;&#25105;&#20204;&#31038;&#21306;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sionna is a GPU-accelerated open-source library for link-level simulations based on TensorFlow. It enables the rapid prototyping of complex communication system architectures and provides native support for the integration of neural networks. Sionna implements a wide breadth of carefully tested state-of-the-art algorithms that can be used for benchmarking and end-to-end performance evaluation. This allows researchers to focus on their research, making it more impactful and reproducible, while saving time implementing components outside their area of expertise. This white paper provides a brief introduction to Sionna, explains its design principles and features, as well as future extensions, such as integrated ray tracing and custom CUDA kernels. We believe that Sionna is a valuable tool for research on next-generation communication systems, such as 6G, and we welcome contributions from our community.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#35782;&#21035;&#21644;&#35299;&#20915;&#25968;&#25454;&#34920;&#24449;&#20559;&#24046;&#30340;&#25991;&#29486;&#65292;&#25552;&#20986;&#22522;&#20110;&#22810;&#20010;&#22240;&#32032;&#30340;&#20998;&#31867;&#27861;&#65292;&#35813;&#20559;&#24046;&#21487;&#33021;&#24433;&#21709;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#30340;&#20844;&#24179;&#24615;&#21644;&#31038;&#20250;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2203.11852</link><description>&lt;p&gt;
&#25968;&#25454;&#20013;&#30340;&#34920;&#24449;&#20559;&#24046;&#65306;&#35782;&#21035;&#21644;&#35299;&#20915;&#25216;&#26415;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Representation Bias in Data: A Survey on Identification and Resolution Techniques. (arXiv:2203.11852v2 [cs.DB] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.11852
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#35782;&#21035;&#21644;&#35299;&#20915;&#25968;&#25454;&#34920;&#24449;&#20559;&#24046;&#30340;&#25991;&#29486;&#65292;&#25552;&#20986;&#22522;&#20110;&#22810;&#20010;&#22240;&#32032;&#30340;&#20998;&#31867;&#27861;&#65292;&#35813;&#20559;&#24046;&#21487;&#33021;&#24433;&#21709;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#30340;&#20844;&#24179;&#24615;&#21644;&#31038;&#20250;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#31639;&#27861;&#34920;&#29616;&#30340;&#22909;&#22351;&#21462;&#20915;&#20110;&#20854;&#25152;&#20351;&#29992;&#30340;&#25968;&#25454;&#65292;&#32780;&#25968;&#25454;&#38598;&#23588;&#20854;&#26159;&#31038;&#20132;&#25968;&#25454;&#32463;&#24120;&#26410;&#33021;&#36275;&#22815;&#22320;&#20195;&#34920;&#23569;&#25968;&#32676;&#20307;&#12290;&#25968;&#25454;&#20013;&#30340;&#34920;&#24449;&#20559;&#24046;&#21487;&#33021;&#20986;&#29616;&#22312;&#19981;&#21516;&#30340;&#21407;&#22240;&#20013;&#65292;&#21253;&#25324;&#21382;&#21490;&#24615;&#30340;&#27495;&#35270;&#21644;&#25968;&#25454;&#33719;&#21462;&#21644;&#20934;&#22791;&#26041;&#27861;&#20013;&#30340;&#36873;&#25321;&#21644;&#25277;&#26679;&#20559;&#24046;&#12290;&#30001;&#20110;&#8220;&#22403;&#22334;&#36827;&#65292;&#22403;&#22334;&#20986;&#8221;&#65292;&#22914;&#26524;&#19981;&#35299;&#20915;&#34920;&#24449;&#20559;&#24046;&#31561;&#38382;&#39064;&#65292;&#20154;&#24037;&#26234;&#33021;&#35299;&#20915;&#31038;&#20250;&#38382;&#39064;&#30340;&#32467;&#26524;&#23601;&#19981;&#33021;&#20844;&#24179;&#12290;&#34429;&#28982;&#20851;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#20844;&#24179;&#24615;&#24050;&#32463;&#26377;&#20102;&#22823;&#37327;&#30340;&#30740;&#31350;&#65292;&#21253;&#25324;&#20960;&#31687;&#32508;&#36848;&#25991;&#31456;&#65292;&#20294;&#25968;&#25454;&#20013;&#30340;&#20559;&#24046;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#12290;&#26412;&#25991;&#23558;&#20316;&#20026;&#19968;&#31181;&#25968;&#25454;&#29305;&#24449;&#65292;&#29420;&#31435;&#20110;&#21518;&#32493;&#22788;&#29702;&#30340;&#26041;&#24335;&#65292;&#32508;&#36848;&#35782;&#21035;&#21644;&#35299;&#20915;&#34920;&#24449;&#20559;&#24046;&#30340;&#25991;&#29486;&#12290;&#26412;&#25991;&#30340;&#33539;&#22260;&#20165;&#38480;&#20110;&#32467;&#26500;&#21270;&#65288;&#34920;&#26684;&#65289;&#21644;&#38750;&#32467;&#26500;&#21270;&#65288;&#20363;&#22914;&#22270;&#20687;&#12289;&#25991;&#26412;&#12289;&#22270;&#24418;&#65289;&#25968;&#25454;&#12290;&#25991;&#31456;&#25552;&#20986;&#20102;&#22522;&#20110;&#22810;&#20010;&#22240;&#32032;&#30340;&#20998;&#31867;&#27861;&#65292;&#23545;&#30740;&#31350;&#30340;&#25216;&#26415;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-driven algorithms are only as good as the data they work with, while data sets, especially social data, often fail to represent minorities adequately. Representation Bias in data can happen due to various reasons ranging from historical discrimination to selection and sampling biases in the data acquisition and preparation methods. Given that "bias in, bias out", one cannot expect AI-based solutions to have equitable outcomes for societal applications, without addressing issues such as representation bias. While there has been extensive study of fairness in machine learning models, including several review papers, bias in the data has been less studied. This paper reviews the literature on identifying and resolving representation bias as a feature of a data set, independent of how consumed later. The scope of this survey is bounded to structured (tabular) and unstructured (e.g., image, text, graph) data. It presents taxonomies to categorize the studied techniques based on multiple
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#25552;&#20986;&#26657;&#20934;&#34893;&#29983;&#21697;&#23450;&#20215;&#27169;&#22411;&#38382;&#39064;&#30340;&#21338;&#24328;&#35770;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#24076;&#26395;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#35299;&#20915;&#20854;&#20182;&#37329;&#34701;&#39046;&#22495;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#23398;&#20064;&#23616;&#37096;&#27874;&#21160;&#29575;&#20197;&#21450;&#26368;&#23567;&#21270;&#30334;&#24917;&#22823;&#26399;&#26435;&#20215;&#26684;&#25152;&#38656;&#30340;&#36335;&#24452;&#20381;&#36182;&#24615;&#12290;</title><link>http://arxiv.org/abs/2203.06865</link><description>&lt;p&gt;
&#34893;&#29983;&#21697;&#23450;&#20215;&#27169;&#22411;&#30340;&#26657;&#20934;&#65306;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#35266;&#28857;
&lt;/p&gt;
&lt;p&gt;
Calibration of Derivative Pricing Models: a Multi-Agent Reinforcement Learning Perspective. (arXiv:2203.06865v3 [q-fin.CP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.06865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#25552;&#20986;&#26657;&#20934;&#34893;&#29983;&#21697;&#23450;&#20215;&#27169;&#22411;&#38382;&#39064;&#30340;&#21338;&#24328;&#35770;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#24076;&#26395;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#35299;&#20915;&#20854;&#20182;&#37329;&#34701;&#39046;&#22495;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#23398;&#20064;&#23616;&#37096;&#27874;&#21160;&#29575;&#20197;&#21450;&#26368;&#23567;&#21270;&#30334;&#24917;&#22823;&#26399;&#26435;&#20215;&#26684;&#25152;&#38656;&#30340;&#36335;&#24452;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37327;&#21270;&#37329;&#34701;&#20013;&#26368;&#22522;&#26412;&#30340;&#38382;&#39064;&#20043;&#19968;&#26159;&#23384;&#22312;&#36866;&#21512;&#32473;&#23450;&#19968;&#32452;&#26399;&#26435;&#24066;&#22330;&#20215;&#26684;&#30340;&#36830;&#32493;&#26102;&#38388;&#25193;&#25955;&#27169;&#22411;&#12290;&#20256;&#32479;&#19978;&#65292;&#20154;&#20204;&#20351;&#29992;&#30452;&#35273;&#12289;&#29702;&#35770;&#21644;&#32463;&#39564;&#20998;&#26512;&#30340;&#28151;&#21512;&#26041;&#27861;&#26469;&#23547;&#25214;&#23454;&#29616;&#31934;&#30830;&#25110;&#36817;&#20284;&#21305;&#37197;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#22312;&#20110;&#23637;&#31034;&#22914;&#20309;&#36890;&#36807;&#36866;&#24403;&#30340;&#21338;&#24328;&#29702;&#35770;&#24418;&#24335;&#21270;&#38382;&#39064;&#65292;&#20511;&#21161;&#29616;&#20195;&#28145;&#24230;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#29616;&#26377;&#36827;&#23637;&#26469;&#25628;&#32034;&#38543;&#26426;&#36807;&#31243;&#31354;&#38388;&#65292;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#25216;&#26415;&#21487;&#20197;&#34987;&#31038;&#21306;&#21033;&#29992;&#21644;&#25193;&#23637;&#65292;&#20197;&#35299;&#20915;&#35813;&#39046;&#22495;&#30340;&#37325;&#35201;&#38382;&#39064;&#65292;&#22914;&#32852;&#21512;SPX-VIX&#26657;&#20934;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#33021;&#22815;&#23398;&#20064;&#23616;&#37096;&#27874;&#21160;&#29575;&#20197;&#21450;&#22312;&#27874;&#21160;&#29575;&#36807;&#31243;&#20013;&#25152;&#38656;&#30340;&#36335;&#24452;&#20381;&#36182;&#24615;&#65292;&#20197;&#26368;&#23567;&#21270;&#30334;&#24917;&#22823;&#26399;&#26435;&#30340;&#20215;&#26684;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#30475;&#20316;&#26159;&#19968;&#31181;&#31890;&#23376;&#26041;&#27861;&#65292;&#31867;&#20284;&#20110;Guyon et Henry-Labordere&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the most fundamental questions in quantitative finance is the existence of continuous-time diffusion models that fit market prices of a given set of options. Traditionally, one employs a mix of intuition, theoretical and empirical analysis to find models that achieve exact or approximate fits. Our contribution is to show how a suitable game theoretical formulation of this problem can help solve this question by leveraging existing developments in modern deep multi-agent reinforcement learning to search in the space of stochastic processes. More importantly, we hope that our techniques can be leveraged and extended by the community to solve important problems in that field, such as the joint SPX-VIX calibration problem. Our experiments show that we are able to learn local volatility, as well as path-dependence required in the volatility process to minimize the price of a Bermudan option. Our algorithm can be seen as a particle method \`{a} la Guyon et Henry-Labordere where partic
&lt;/p&gt;</description></item><item><title>NELA-GT-2022&#26159;&#19968;&#20221;&#21253;&#21547;361&#20010;&#26469;&#28304;&#30340;1,778,361&#31687;&#25991;&#31456;&#30340;&#22810;&#26631;&#31614;&#26032;&#38395;&#25968;&#25454;&#38598;&#65292;&#21487;&#29992;&#20110;&#30740;&#31350;&#26032;&#38395;&#25253;&#36947;&#20013;&#30340;&#35823;&#23548;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2203.05659</link><description>&lt;p&gt;
NELA-GT-2022&#65306;&#19968;&#20221;&#29992;&#20110;&#30740;&#31350;&#26032;&#38395;&#25253;&#36947;&#20013;&#35823;&#23548;&#20449;&#24687;&#30340;&#22823;&#22411;&#22810;&#26631;&#31614;&#26032;&#38395;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
NELA-GT-2022: A Large Multi-Labelled News Dataset for The Study of Misinformation in News Articles. (arXiv:2203.05659v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.05659
&lt;/p&gt;
&lt;p&gt;
NELA-GT-2022&#26159;&#19968;&#20221;&#21253;&#21547;361&#20010;&#26469;&#28304;&#30340;1,778,361&#31687;&#25991;&#31456;&#30340;&#22810;&#26631;&#31614;&#26032;&#38395;&#25968;&#25454;&#38598;&#65292;&#21487;&#29992;&#20110;&#30740;&#31350;&#26032;&#38395;&#25253;&#36947;&#20013;&#30340;&#35823;&#23548;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;NELA-GT&#25968;&#25454;&#38598;&#30340;&#31532;&#20116;&#29256;&#65292;&#21363;NELA-GT-2022&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;361&#20010;&#26469;&#28304;&#30340;1,778,361&#31687;&#25991;&#31456;&#65292;&#26102;&#38388;&#36328;&#24230;&#20026;2022&#24180;1&#26376;1&#26085;&#33267;12&#26376;31&#26085;&#12290;&#19982;&#36807;&#21435;&#29256;&#26412;&#19968;&#26679;&#65292;NELA-GT-2022&#36824;&#21253;&#25324;&#26469;&#33258;Media Bias / Fact Check&#30340;&#20986;&#21475;&#32423;&#30495;&#23454;&#24615;&#26631;&#31614;&#20197;&#21450;&#23884;&#20837;&#25910;&#38598;&#30340;&#26032;&#38395;&#25991;&#31456;&#20013;&#30340;&#25512;&#25991;&#12290;NELA-GT-2022&#25968;&#25454;&#38598;&#30340;&#33719;&#21462;&#38142;&#25509;&#20026;&#65306; https://doi.org/10.7910/DVN/AMCV2H
&lt;/p&gt;
&lt;p&gt;
In this paper, we present the fifth installment of the NELA-GT datasets, NELA-GT-2022. The dataset contains 1,778,361 articles from 361 outlets between January 1st, 2022 and December 31st, 2022. Just as in past releases of the dataset, NELA-GT-2022 includes outlet-level veracity labels from Media Bias/Fact Check and tweets embedded in collected news articles. The NELA-GT-2022 dataset can be found at: https://doi.org/10.7910/DVN/AMCV2H
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#26080;&#38656;&#25237;&#24433;&#30340;&#26032;&#22411;&#22312;&#32447;&#20984;&#20248;&#21270;&#31639;&#27861;&#65292;&#22522;&#20110;&#22312;&#32447;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#21644;&#39640;&#25928;&#30340;&#8220;&#19981;&#21487;&#34892;&#25237;&#24433;&#8221;&#35745;&#31639;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#33258;&#36866;&#24212;&#36951;&#25022;&#20445;&#35777;&#65292;&#24182;&#36798;&#21040;&#20102;$O(T^{3/4})$&#30340;&#23450;&#24120;&#24615;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2202.04721</link><description>&lt;p&gt;
&#26080;&#38656;&#25237;&#24433;&#30340;&#22312;&#32447;&#20984;&#20248;&#21270;&#31639;&#27861;&#19982;&#33258;&#36866;&#24212;&#36951;&#25022;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
New Projection-free Algorithms for Online Convex Optimization with Adaptive Regret Guarantees. (arXiv:2202.04721v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.04721
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26080;&#38656;&#25237;&#24433;&#30340;&#26032;&#22411;&#22312;&#32447;&#20984;&#20248;&#21270;&#31639;&#27861;&#65292;&#22522;&#20110;&#22312;&#32447;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#21644;&#39640;&#25928;&#30340;&#8220;&#19981;&#21487;&#34892;&#25237;&#24433;&#8221;&#35745;&#31639;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#33258;&#36866;&#24212;&#36951;&#25022;&#20445;&#35777;&#65292;&#24182;&#36798;&#21040;&#20102;$O(T^{3/4})$&#30340;&#23450;&#24120;&#24615;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26032;&#30340;&#39640;&#25928;&#26080;&#38656;&#25237;&#24433;&#30340;&#22312;&#32447;&#20984;&#20248;&#21270;&#31639;&#27861;&#65292;&#23427;&#20204;&#36991;&#20813;&#20102;&#35745;&#31639;&#25512;&#21521;&#21487;&#34892;&#38598;&#19978;&#30340;&#27491;&#20132;&#25237;&#24433;&#65292;&#24182;&#20351;&#29992;&#19981;&#21516;&#19988;&#21487;&#33021;&#26356;&#26377;&#25928;&#30340;&#39044;&#27979;&#12290;&#36825;&#20123;&#31639;&#27861;&#19981;&#26159;&#22522;&#20110;&#29616;&#26377;&#30340;&#36319;&#38543;&#39046;&#23548;&#32773;&#65288;follow-the-leader&#65289;&#26694;&#26550;&#65292;&#32780;&#26159;&#22522;&#20110;&#20855;&#26377;&#26032;&#39062;&#32780;&#26377;&#25928;&#30340;&#8220;&#19981;&#21487;&#34892;&#25237;&#24433;&#8221;&#30340;&#22312;&#32447;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#31532;&#19968;&#20010;&#33021;&#22815;&#33258;&#28982;&#22320;&#20135;&#29983;&#33258;&#36866;&#24212;&#36951;&#25022;&#20445;&#35777;&#65288;&#21363;&#19982;&#24207;&#21015;&#30340;&#20219;&#20309;&#23376;&#38388;&#38548;&#30456;&#20851;&#30340;&#36951;&#25022;&#19978;&#38480;&#65289;&#30340;&#26080;&#38656;&#25237;&#24433;&#31639;&#27861;&#12290;&#22312;&#20551;&#35774;&#26377;&#19968;&#20010;&#32447;&#24615;&#20248;&#21270;&#39044;&#27979;&#65288;LOO&#65289;&#21487;&#20379;&#20351;&#29992;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#20110;&#38271;&#24230;&#20026;$T$&#30340;&#24207;&#21015;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#20445;&#35777;&#20102;$O(T^{3/4})$&#30340;&#33258;&#36866;&#24212;&#36951;&#25022;&#21644;$O(T^{3/4})$&#30340;&#23450;&#24120;&#24615;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present new efficient \textit{projection-free} algorithms for online convex optimization (OCO), where by projection-free we refer to algorithms that avoid computing orthogonal projections onto the feasible set, and instead relay on different and potentially much more efficient oracles. While most state-of-the-art projection-free algorithms are based on the \textit{follow-the-leader} framework, our algorithms are fundamentally different and are based on the \textit{online gradient descent} algorithm with a novel and efficient approach to computing so-called \textit{infeasible projections}. As a consequence, we obtain the first projection-free algorithms which naturally yield \textit{adaptive regret} guarantees, i.e., regret bounds that hold w.r.t. any sub-interval of the sequence. Concretely, when assuming the availability of a linear optimization oracle (LOO) for the feasible set, on a sequence of length $T$, our algorithms guarantee $O(T^{3/4})$ adaptive regret and $O(T^{3/4})$ ada
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#28040;&#24687;&#20256;&#36882;&#30340;&#27714;&#35299;&#22120;&#65292;&#29992;&#21453;&#21521;&#20256;&#25773;&#20248;&#21270;&#30340;&#31070;&#32463;&#20989;&#25968;&#36924;&#36817;&#22120;&#21462;&#20195;&#35745;&#31639;&#22270;&#20013;&#25152;&#26377;&#21551;&#21457;&#24335;&#35774;&#35745;&#30340;&#32452;&#20214;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#31181;&#24102;&#26377;&#23436;&#20840;&#29366;&#24577;&#26356;&#26032;&#30340;&#28040;&#24687;&#20256;&#36882;&#26041;&#27861;&#65292;&#21487;&#25512;&#24191;&#21040;&#20219;&#20309;&#20855;&#26377;&#32463;&#20856;&#35299;&#32467;&#26500;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#20013;&#12290;</title><link>http://arxiv.org/abs/2202.03376</link><description>&lt;p&gt;
&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#20559;&#24494;&#20998;&#26041;&#31243;&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
Message Passing Neural PDE Solvers. (arXiv:2202.03376v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.03376
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#28040;&#24687;&#20256;&#36882;&#30340;&#27714;&#35299;&#22120;&#65292;&#29992;&#21453;&#21521;&#20256;&#25773;&#20248;&#21270;&#30340;&#31070;&#32463;&#20989;&#25968;&#36924;&#36817;&#22120;&#21462;&#20195;&#35745;&#31639;&#22270;&#20013;&#25152;&#26377;&#21551;&#21457;&#24335;&#35774;&#35745;&#30340;&#32452;&#20214;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#31181;&#24102;&#26377;&#23436;&#20840;&#29366;&#24577;&#26356;&#26032;&#30340;&#28040;&#24687;&#20256;&#36882;&#26041;&#27861;&#65292;&#21487;&#25512;&#24191;&#21040;&#20219;&#20309;&#20855;&#26377;&#32463;&#20856;&#35299;&#32467;&#26500;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#25968;&#20540;&#27714;&#35299;&#19968;&#30452;&#26159;&#19968;&#20010;&#22256;&#38590;&#30340;&#38382;&#39064;&#65292;&#36804;&#20170;&#24050;&#32463;&#26377;&#19968;&#20010;&#19990;&#32426;&#30340;&#30740;&#31350;&#21382;&#31243;&#12290;&#36817;&#24180;&#26469;&#65292;&#20986;&#29616;&#20102;&#24314;&#31435;&#31070;&#32463;&#25968;&#20540;&#28151;&#21512;&#27714;&#35299;&#22120;&#30340;&#36235;&#21183;&#65292;&#36825;&#20351;&#24471;&#29616;&#20195;&#23436;&#20840;&#31471;&#23545;&#31471;&#23398;&#20064;&#31995;&#32479;&#30340;&#21457;&#23637;&#21464;&#24471;&#26356;&#20026;&#39640;&#25928;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#22823;&#22810;&#25968;&#24037;&#20316;&#21482;&#33021;&#22312;&#19968;&#20123;&#29305;&#23450;&#30340;&#23646;&#24615;&#26041;&#38754;&#36827;&#34892;&#27867;&#21270;&#65292;&#36825;&#20123;&#23646;&#24615;&#21253;&#25324;&#20998;&#36776;&#29575;&#12289;&#25299;&#25169;&#12289;&#20960;&#20309;&#24418;&#29366;&#12289;&#36793;&#30028;&#26465;&#20214;&#12289;&#22495;&#31163;&#25955;&#27491;&#21017;&#24615;&#21644;&#32500;&#24230;&#31561;&#31561;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#35299;&#20915;&#36825;&#20123;&#23646;&#24615;&#30340;&#27714;&#35299;&#22120;&#65292;&#20854;&#20013;&#25152;&#26377;&#32452;&#20214;&#37117;&#22522;&#20110;&#31070;&#32463;&#28040;&#24687;&#20256;&#36882;&#65292;&#29992;&#21453;&#21521;&#20256;&#25773;&#20248;&#21270;&#30340;&#31070;&#32463;&#20989;&#25968;&#36924;&#36817;&#22120;&#21462;&#20195;&#20102;&#35745;&#31639;&#22270;&#20013;&#25152;&#26377;&#21551;&#21457;&#24335;&#35774;&#35745;&#30340;&#32452;&#20214;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#31070;&#32463;&#28040;&#24687;&#20256;&#36882;&#27714;&#35299;&#22120;&#22312;&#34920;&#31034;&#19978;&#21253;&#21547;&#19968;&#20123;&#32463;&#20856;&#30340;&#26041;&#27861;&#65292;&#22914;&#26377;&#38480;&#24046;&#20998;&#12289;&#26377;&#38480;&#20307;&#31215;&#21644; WENO &#26041;&#26696;&#12290;&#20026;&#20102;&#22312;&#35757;&#32451;&#33258;&#22238;&#24402;&#27169;&#22411;&#26102;&#40723;&#21169;&#31283;&#23450;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#23436;&#20840;&#29366;&#24577;&#26356;&#26032;&#30340;&#28040;&#24687;&#20256;&#36882;&#26041;&#27861;&#65292;&#21487;&#20197;&#25512;&#24191;&#21040;&#20855;&#26377;&#32463;&#20856;&#35299;&#32467;&#26500;&#30340;&#20219;&#20309; PDE &#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
The numerical solution of partial differential equations (PDEs) is difficult, having led to a century of research so far. Recently, there have been pushes to build neural--numerical hybrid solvers, which piggy-backs the modern trend towards fully end-to-end learned systems. Most works so far can only generalize over a subset of properties to which a generic solver would be faced, including: resolution, topology, geometry, boundary conditions, domain discretization regularity, dimensionality, etc. In this work, we build a solver, satisfying these properties, where all the components are based on neural message passing, replacing all heuristically designed components in the computation graph with backprop-optimized neural function approximators. We show that neural message passing solvers representationally contain some classical methods, such as finite differences, finite volumes, and WENO schemes. In order to encourage stability in training autoregressive models, we put forward a metho
&lt;/p&gt;</description></item><item><title>SimGRACE&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#19981;&#38656;&#35201;&#25968;&#25454;&#22686;&#24378;&#23601;&#33021;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#26368;&#26032;&#30340;&#32467;&#26524;&#65292;&#24182;&#37319;&#29992;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#12289;&#36127;&#37319;&#26679;&#31574;&#30053;&#12289;&#20197;&#21450;&#19968;&#20123;&#38468;&#21152;&#39033;&#26469;&#22686;&#24378;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2202.03104</link><description>&lt;p&gt;
SimGRACE: &#19968;&#31181;&#31616;&#21333;&#30340;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#19981;&#38656;&#35201;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
SimGRACE: A Simple Framework for Graph Contrastive Learning without Data Augmentation. (arXiv:2202.03104v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.03104
&lt;/p&gt;
&lt;p&gt;
SimGRACE&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#19981;&#38656;&#35201;&#25968;&#25454;&#22686;&#24378;&#23601;&#33021;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#26368;&#26032;&#30340;&#32467;&#26524;&#65292;&#24182;&#37319;&#29992;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#12289;&#36127;&#37319;&#26679;&#31574;&#30053;&#12289;&#20197;&#21450;&#19968;&#20123;&#38468;&#21152;&#39033;&#26469;&#22686;&#24378;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#22270;&#24418;&#34920;&#31034;&#23398;&#20064;&#30340;&#19968;&#31181;&#20027;&#27969;&#25216;&#26415;&#65292;&#23427;&#26368;&#22823;&#21270;&#20102;&#20849;&#20139;&#30456;&#21516;&#35821;&#20041;&#30340;&#25104;&#23545;&#22270;&#24418;&#22686;&#24378;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#37492;&#20110;&#22270;&#24418;&#25968;&#25454;&#30340;&#22810;&#26679;&#24615;&#65292;&#24456;&#38590;&#22312;&#22686;&#24378;&#36807;&#31243;&#20013;&#24456;&#22909;&#22320;&#20445;&#30041;&#35821;&#20041;&#12290;&#30446;&#21069;&#65292;&#22312;&#24191;&#27867;&#20445;&#30041;&#35821;&#20041;&#30340;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#20013;&#65292;&#25968;&#25454;&#22686;&#24378;&#20998;&#20026;&#19977;&#31181;&#19981;&#20196;&#20154;&#28385;&#24847;&#30340;&#26041;&#24335;&#12290;&#31532;&#19968;&#65292;&#21487;&#20197;&#36890;&#36807;&#23581;&#35797;&#38169;&#35823;&#26469;&#25163;&#21160;&#36873;&#25321;&#27599;&#20010;&#25968;&#25454;&#38598;&#30340;&#22686;&#24378;&#12290;&#31532;&#20108;&#65292;&#21487;&#20197;&#36890;&#36807;&#32321;&#29712;&#30340;&#25628;&#32034;&#36873;&#25321;&#22686;&#24378;&#12290;&#31532;&#19977;&#65292;&#21487;&#20197;&#36890;&#36807;&#24341;&#20837;&#26114;&#36149;&#30340;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#20316;&#20026;&#25351;&#23548;&#26469;&#33719;&#24471;&#22686;&#24378;&#12290;&#25152;&#26377;&#36825;&#20123;&#37117;&#38480;&#21046;&#20102;&#29616;&#26377;GCL&#26041;&#27861;&#30340;&#25928;&#29575;&#21644;&#26356;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#20851;&#38190;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SimGRACE&#30340;&#31616;&#21333;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#19981;&#38656;&#35201;&#25968;&#25454;&#22686;&#24378;&#23601;&#33021;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#26368;&#26032;&#30340;&#32467;&#26524;&#12290;SimGRACE&#37319;&#29992;&#22522;&#20110;&#26368;&#36817;&#25552;&#20986;&#30340;InfoNCE&#25439;&#22833;&#30340;&#31616;&#21333;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#65292;&#24182;&#21033;&#29992;&#39640;&#25928;&#30340;&#36127;&#37319;&#26679;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33410;&#28857;&#23545;&#27604;&#25439;&#22833;&#21644;&#19968;&#20010;&#31616;&#21333;&#30340;&#22270;&#24418;&#27491;&#21017;&#21270;&#39033;&#26469;&#36827;&#19968;&#27493;&#22686;&#24378;SimGRACE&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;SimGRACE&#30456;&#23545;&#20110;&#20854;&#20182;&#26368;&#26032;&#30340;GCL&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph contrastive learning (GCL) has emerged as a dominant technique for graph representation learning which maximizes the mutual information between paired graph augmentations that share the same semantics. Unfortunately, it is difficult to preserve semantics well during augmentations in view of the diverse nature of graph data. Currently, data augmentations in GCL that are designed to preserve semantics broadly fall into three unsatisfactory ways. First, the augmentations can be manually picked per dataset by trial-and-errors. Second, the augmentations can be selected via cumbersome search. Third, the augmentations can be obtained by introducing expensive domain-specific knowledge as guidance. All of these limit the efficiency and more general applicability of existing GCL methods. To circumvent these crucial issues, we propose a \underline{Sim}ple framework for \underline{GRA}ph \underline{C}ontrastive l\underline{E}arning, \textbf{SimGRACE} for brevity, which does not require data 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#37319;&#26679;&#31574;&#30053;&#26469;&#25552;&#39640;L-SVRG&#21644;L-Katyusha&#20248;&#21270;&#26041;&#27861;&#22312;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#21487;&#20197;&#22312;&#23569;&#37327;&#35745;&#31639;&#24320;&#38144;&#20869;&#23454;&#29616;&#37319;&#26679;&#20998;&#24067;&#30340;&#23398;&#20064;&#65292;&#21516;&#26102;&#19981;&#38656;&#35201;&#20808;&#39564;&#30693;&#35782;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#25910;&#25947;&#24615;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2201.13387</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#37319;&#26679;&#30340;L-SVRG&#21644;L-Katyusha&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
L-SVRG and L-Katyusha with Adaptive Sampling. (arXiv:2201.13387v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.13387
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#37319;&#26679;&#31574;&#30053;&#26469;&#25552;&#39640;L-SVRG&#21644;L-Katyusha&#20248;&#21270;&#26041;&#27861;&#22312;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#21487;&#20197;&#22312;&#23569;&#37327;&#35745;&#31639;&#24320;&#38144;&#20869;&#23454;&#29616;&#37319;&#26679;&#20998;&#24067;&#30340;&#23398;&#20064;&#65292;&#21516;&#26102;&#19981;&#38656;&#35201;&#20808;&#39564;&#30693;&#35782;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#25910;&#25947;&#24615;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#38543;&#26426;&#26799;&#24230;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#22914;L-SVRG&#21450;&#20854;&#21152;&#36895;&#21464;&#31181;L-Katyusha&#22312;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26102;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#37319;&#26679;&#31574;&#30053;&#65292;&#21487;&#20197;&#23454;&#29616;&#22312;&#23569;&#37327;&#35745;&#31639;&#24320;&#38144;&#20869;&#23398;&#20064;&#37319;&#26679;&#20998;&#24067;&#65292;&#21516;&#26102;&#21487;&#20197;&#38543;&#30528;&#36845;&#20195;&#32780;&#25913;&#21464;&#65292;&#32780;&#19988;&#19981;&#38656;&#35201;&#20808;&#39564;&#30693;&#35782;&#12290;&#23545;&#20110;&#20984;&#30446;&#26631;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;L-SVRG&#21644;L-Katyusha&#30340;&#25910;&#25947;&#24615;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic gradient-based optimization methods, such as L-SVRG and its accelerated variant L-Katyusha (Kovalev et al., 2020), are widely used to train machine learning models.The theoretical and empirical performance of L-SVRG and L-Katyusha can be improved by sampling observations from a non-uniform distribution (Qian et al., 2021). However,designing a desired sampling distribution requires prior knowledge of smoothness constants, which can be computationally intractable to obtain in practice when the dimension of the model parameter is high. To address this issue, we propose an adaptive sampling strategy for L-SVRG and L-Katyusha that can learn the sampling distribution with little computational overhead, while allowing it to change with iterates, and at the same time does not require any prior knowledge of the problem parameters. We prove convergence guarantees for L-SVRG and L-Katyusha for convex objectives when the sampling distribution changes with iterates. Our results show that
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#36793;&#32536;&#31471;&#26426;&#22120;&#23398;&#20064;&#24037;&#20316;&#27969;&#31243;&#20013;&#20559;&#35265;&#20256;&#36882;&#30340;&#35774;&#35745;&#36873;&#25321;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#25216;&#26415;&#35774;&#35745;&#36873;&#25321;&#65292;&#22914;&#27169;&#22411;&#26550;&#26500;&#21644;&#20248;&#21270;&#31639;&#27861;&#65292;&#20250;&#25918;&#22823;&#21644;&#20256;&#25773;&#21487;&#38752;&#24615;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2201.07677</link><description>&lt;p&gt;
&#24494;&#22411;&#12289;&#22987;&#32456;&#22312;&#32447;&#19988;&#26131;&#30862;: &#35774;&#35745;&#36873;&#25321;&#20013;&#30340;&#20559;&#35265;&#20256;&#36882;&#19982;&#22312;&#32447;&#26426;&#22120;&#23398;&#20064;&#24037;&#20316;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tiny, always-on and fragile: Bias propagation through design choices in on-device machine learning workflows. (arXiv:2201.07677v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.07677
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#36793;&#32536;&#31471;&#26426;&#22120;&#23398;&#20064;&#24037;&#20316;&#27969;&#31243;&#20013;&#20559;&#35265;&#20256;&#36882;&#30340;&#35774;&#35745;&#36873;&#25321;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#25216;&#26415;&#35774;&#35745;&#36873;&#25321;&#65292;&#22914;&#27169;&#22411;&#26550;&#26500;&#21644;&#20248;&#21270;&#31639;&#27861;&#65292;&#20250;&#25918;&#22823;&#21644;&#20256;&#25773;&#21487;&#38752;&#24615;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#21313;&#20159;&#20010;&#20998;&#24067;&#24335;&#12289;&#24322;&#26500;&#30340; IOT &#35774;&#22791;&#65292;&#22312;&#20010;&#20154;&#25968;&#25454;&#19978;&#37096;&#32626;&#30340;&#36793;&#32536;&#31471;&#26426;&#22120;&#23398;&#20064;&#65292;&#29992;&#20110;&#31169;&#23494;&#12289;&#24555;&#36895;&#19988;&#31163;&#32447;&#25512;&#29702;&#12290;&#36793;&#32536;&#31471;&#30340;&#26426;&#22120;&#23398;&#20064;&#39640;&#24230;&#20381;&#36182;&#19978;&#19979;&#25991;&#65292;&#23545;&#29992;&#25143;&#12289;&#29992;&#27861;&#12289;&#30828;&#20214;&#21644;&#29615;&#22659;&#23646;&#24615;&#38750;&#24120;&#25935;&#24863;&#12290;&#36825;&#31181;&#25935;&#24863;&#24615;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#20559;&#35265;&#30340;&#20542;&#21521;&#20351;&#24471;&#30740;&#31350;&#36793;&#32536;&#31471;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20559;&#35265;&#38750;&#24120;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#26159;&#23545;&#36825;&#19968;&#26032;&#20852;&#39046;&#22495;&#20013;&#20559;&#35265;&#30740;&#31350;&#30340;&#39318;&#27425;&#25506;&#32034;&#65292;&#20026;&#26500;&#24314;&#26356;&#20844;&#24179;&#30340;&#36793;&#32536;&#31471;&#26426;&#22120;&#23398;&#20064;&#22880;&#23450;&#20102;&#37325;&#35201;&#22522;&#30784;&#12290;&#26412;&#25991;&#36890;&#36807;&#36719;&#20214;&#24037;&#31243;&#35282;&#24230;&#35843;&#26597;&#20102;&#36793;&#32536;&#31471;&#26426;&#22120;&#23398;&#20064;&#24037;&#20316;&#27969;&#31243;&#20013;&#20559;&#35265;&#20256;&#36882;&#30340;&#35774;&#35745;&#36873;&#25321;&#12290;&#25105;&#20204;&#39318;&#20808;&#23558;&#21487;&#38752;&#24615;&#20559;&#35265;&#30830;&#23450;&#20026;&#19981;&#20844;&#24179;&#24615;&#30340;&#26469;&#28304;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#21487;&#38752;&#24615;&#20559;&#35265;&#30340;&#26041;&#27861;&#12290;&#28982;&#21518;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#20851;&#38190;&#35789;&#26816;&#27979;&#20219;&#21153;&#30340;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102;&#22797;&#26434;&#30340;&#21644;&#30456;&#20114;&#20316;&#29992;&#30340;&#25216;&#26415;&#35774;&#35745;&#36873;&#25321;&#22914;&#20309;&#25918;&#22823;&#21644;&#20256;&#25773;&#21487;&#38752;&#24615;&#20559;&#35265;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#39564;&#35777;&#20102;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#35774;&#35745;&#36873;&#25321;&#65292;&#22914;&#27169;&#22411;&#26550;&#26500;&#21644;&#20248;&#21270;&#31639;&#27861;&#65292;&#20250;&#23545;&#21487;&#38752;&#24615;&#20559;&#35265;&#30340;&#20256;&#25773;&#20135;&#29983;&#24040;&#22823;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Billions of distributed, heterogeneous and resource constrained IoT devices deploy on-device machine learning (ML) for private, fast and offline inference on personal data. On-device ML is highly context dependent, and sensitive to user, usage, hardware and environment attributes. This sensitivity and the propensity towards bias in ML makes it important to study bias in on-device settings. Our study is one of the first investigations of bias in this emerging domain, and lays important foundations for building fairer on-device ML. We apply a software engineering lens, investigating the propagation of bias through design choices in on-device ML workflows. We first identify reliability bias as a source of unfairness and propose a measure to quantify it. We then conduct empirical experiments for a keyword spotting task to show how complex and interacting technical design choices amplify and propagate reliability bias. Our results validate that design choices made during model training, lik
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#38134;&#34892;&#21453;&#27927;&#38065;&#30340;&#32479;&#35745;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#23458;&#25143;&#39118;&#38505;&#35780;&#20272;&#21644;&#21487;&#30097;&#34892;&#20026;&#26631;&#35782;&#20004;&#20010;&#26680;&#24515;&#35201;&#32032;&#12290;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#21253;&#25324;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#12289;&#21322;&#30417;&#30563;&#21644;&#28145;&#24230;&#23398;&#20064;&#12289;&#21487;&#35299;&#37322;&#24615;&#20197;&#21450;&#32467;&#26524;&#30340;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2201.04207</link><description>&lt;p&gt;
&#29992;&#32479;&#35745;&#21644;&#26426;&#22120;&#23398;&#20064;&#25171;&#20987;&#27927;&#38065;&#65306;&#32508;&#36848;&#19982;&#20171;&#32461;
&lt;/p&gt;
&lt;p&gt;
Fighting Money Laundering with Statistics and Machine Learning: An Introduction and Review. (arXiv:2201.04207v4 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.04207
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#38134;&#34892;&#21453;&#27927;&#38065;&#30340;&#32479;&#35745;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#23458;&#25143;&#39118;&#38505;&#35780;&#20272;&#21644;&#21487;&#30097;&#34892;&#20026;&#26631;&#35782;&#20004;&#20010;&#26680;&#24515;&#35201;&#32032;&#12290;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#21253;&#25324;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#12289;&#21322;&#30417;&#30563;&#21644;&#28145;&#24230;&#23398;&#20064;&#12289;&#21487;&#35299;&#37322;&#24615;&#20197;&#21450;&#32467;&#26524;&#30340;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27927;&#38065;&#26159;&#19968;&#20010;&#20005;&#37325;&#30340;&#20840;&#29699;&#24615;&#38382;&#39064;&#65292;&#20294;&#26159;&#38024;&#23545;&#21453;&#27927;&#38065;&#30340;&#32479;&#35745;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#31185;&#23398;&#25991;&#29486;&#21364;&#24456;&#23569;&#12290;&#26412;&#25991;&#30528;&#37325;&#20110;&#38134;&#34892;&#21453;&#27927;&#38065;&#65292;&#24182;&#25552;&#20379;&#20102;&#25991;&#29486;&#32508;&#36848;&#21644;&#20171;&#32461;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26415;&#35821;&#65292;&#20854;&#20013;&#21253;&#25324;&#23458;&#25143;&#39118;&#38505;&#35780;&#20272;&#21644;&#21487;&#30097;&#34892;&#20026;&#26631;&#35782;&#20004;&#20010;&#26680;&#24515;&#35201;&#32032;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23458;&#25143;&#39118;&#38505;&#35780;&#20272;&#26159;&#36890;&#36807;&#35786;&#26029;&#26469;&#23547;&#25214;&#21644;&#35299;&#37322;&#39118;&#38505;&#22240;&#32032;&#65292;&#32780;&#21487;&#30097;&#34892;&#20026;&#26631;&#35782;&#21017;&#26159;&#36890;&#36807;&#26410;&#20844;&#24320;&#30340;&#29305;&#24449;&#21644;&#25163;&#24037;&#39118;&#38505;&#25351;&#25968;&#26469;&#23454;&#29616;&#30340;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#65292;&#20854;&#20013;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#26159;&#38656;&#35201;&#26356;&#22810;&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#65292;&#36825;&#21487;&#33021;&#21487;&#20197;&#36890;&#36807;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#26469;&#35299;&#20915;&#65292;&#20854;&#20182;&#21487;&#33021;&#30340;&#30740;&#31350;&#26041;&#21521;&#21253;&#25324;&#21322;&#30417;&#30563;&#21644;&#28145;&#24230;&#23398;&#20064;&#12289;&#21487;&#35299;&#37322;&#24615;&#20197;&#21450;&#32467;&#26524;&#30340;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Money laundering is a profound global problem. Nonetheless, there is little scientific literature on statistical and machine learning methods for anti-money laundering. In this paper, we focus on anti-money laundering in banks and provide an introduction and review of the literature. We propose a unifying terminology with two central elements: (i) client risk profiling and (ii) suspicious behavior flagging. We find that client risk profiling is characterized by diagnostics, i.e., efforts to find and explain risk factors. On the other hand, suspicious behavior flagging is characterized by non-disclosed features and hand-crafted risk indices. Finally, we discuss directions for future research. One major challenge is the need for more public data sets. This may potentially be addressed by synthetic data generation. Other possible research directions include semi-supervised and deep learning, interpretability, and fairness of the results.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#31526;&#21495;&#22238;&#24402;&#30340;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#65292;&#22312;Sunyaev-Zeldovich &#33639;&#20809;$-$&#26143;&#22242;&#36136;&#37327;&#20851;&#31995;&#20013;&#25214;&#21040;&#20102;&#19968;&#20010;&#26032;&#30340;&#20195;&#29702;&#21464;&#37327;&#65292;&#32467;&#21512;&#20102;$Y_\mathrm{SZ}$&#21644;&#30005;&#31163;&#27668;&#20307;&#27987;&#24230;($c_\mathrm{gas}$)&#65292;&#26497;&#22823;&#22320;&#20943;&#23567;&#20102;&#35813;&#20851;&#31995;&#20013;&#30340;&#25955;&#23556;&#65292;&#25552;&#39640;&#20102;&#23431;&#23449;&#23398;&#20998;&#26512;&#30340;&#28789;&#25935;&#24230;&#12290;</title><link>http://arxiv.org/abs/2201.01305</link><description>&lt;p&gt;
&#29992;&#26426;&#22120;&#23398;&#20064;&#22686;&#24378;&#23431;&#23449;&#22825;&#20307;&#29289;&#29702;&#23398;&#30340;&#27604;&#20363;&#20851;&#31995;: &#24212;&#29992;&#20110;&#20943;&#23569; Sunyaev-Zeldovich &#33639;&#20809;&#36136;&#37327;&#25955;&#23556;
&lt;/p&gt;
&lt;p&gt;
Augmenting astrophysical scaling relations with machine learning: application to reducing the Sunyaev-Zeldovich flux-mass scatter. (arXiv:2201.01305v3 [astro-ph.CO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.01305
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#31526;&#21495;&#22238;&#24402;&#30340;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#65292;&#22312;Sunyaev-Zeldovich &#33639;&#20809;$-$&#26143;&#22242;&#36136;&#37327;&#20851;&#31995;&#20013;&#25214;&#21040;&#20102;&#19968;&#20010;&#26032;&#30340;&#20195;&#29702;&#21464;&#37327;&#65292;&#32467;&#21512;&#20102;$Y_\mathrm{SZ}$&#21644;&#30005;&#31163;&#27668;&#20307;&#27987;&#24230;($c_\mathrm{gas}$)&#65292;&#26497;&#22823;&#22320;&#20943;&#23567;&#20102;&#35813;&#20851;&#31995;&#20013;&#30340;&#25955;&#23556;&#65292;&#25552;&#39640;&#20102;&#23431;&#23449;&#23398;&#20998;&#26512;&#30340;&#28789;&#25935;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#30340;&#22825;&#20307;&#31995;&#32479;&#36890;&#24120;&#23637;&#31034;&#20986;&#21487;&#35266;&#27979;&#29305;&#24615;&#65288;&#22914;&#20142;&#24230;&#12289;&#36895;&#24230;&#20998;&#25955;&#12289;&#25391;&#33633;&#21608;&#26399;&#65289;&#20043;&#38388;&#30340;&#20302;&#31163;&#25955;&#27604;&#20363;&#20851;&#31995;&#65292;&#36825;&#20123;&#27604;&#20363;&#20851;&#31995;&#25581;&#31034;&#20102;&#20854;&#20013;&#30340;&#29289;&#29702;&#29616;&#35937;&#65292;&#24182;&#20026;&#36136;&#37327;&#21644;&#36317;&#31163;&#20272;&#31639;&#31561;&#25552;&#20379;&#20102;&#35266;&#27979;&#24037;&#20855;&#12290;&#26426;&#22120;&#23398;&#20064;&#21487;&#20197;&#22312;&#39640;&#32500;&#24230;&#21442;&#25968;&#31354;&#38388;&#20013;&#25552;&#20379;&#24555;&#36895;&#31995;&#32479;&#30340;&#25628;&#32034;&#26032;&#27604;&#20363;&#20851;&#31995;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#20351;&#29992;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#31526;&#21495;&#22238;&#24402;&#8221;&#30340;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#65292;&#23558;&#25968;&#25454;&#20013;&#30340;&#27169;&#24335;&#24314;&#27169;&#25104;&#35299;&#26512;&#26041;&#31243;&#12290;&#26412;&#25991;&#32858;&#28966;&#20110; Sunyaev-Zeldovich &#33639;&#20809;$-$&#26143;&#22242;&#36136;&#37327;&#20851;&#31995;($Y_\mathrm{SZ}-M$)&#65292;&#36825;&#20010;&#20851;&#31995;&#20013;&#30340;&#25955;&#23556;&#24433;&#21709;&#20174;&#26143;&#22242;&#20016;&#24230;&#25968;&#25454;&#20013;&#25512;&#26029;&#20986;&#30340;&#23431;&#23449;&#23398;&#21442;&#25968;&#12290;&#36890;&#36807;&#22312;Illustris TNG&#27700;&#21147;&#27169;&#25311;&#25968;&#25454;&#19978;&#20351;&#29992;&#31526;&#21495;&#22238;&#24402;&#65292;&#25105;&#20204;&#25214;&#21040;&#20102;&#19968;&#20010;&#26032;&#30340;&#26143;&#22242;&#36136;&#37327;&#20195;&#29702;&#21464;&#37327;&#65292;&#32467;&#21512;&#20102;$Y_\mathrm{SZ}$&#21644;&#30005;&#31163;&#27668;&#20307;&#27987;&#24230;($c_\mathrm{gas}$)&#65306;$M \propto Y_\mathrm{SZ}(c_\mathrm{gas}/C)^{1.7}$&#65292;&#20854;&#20013;$C$&#26159;&#19968;&#20010;&#24120;&#25968;&#12290;&#36825;&#20010;&#20851;&#31995;&#26174;&#33879;&#20943;&#23567;&#20102;$Y_\mathrm{SZ}-M$&#20013;&#30340;&#25955;&#23556;&#65292;&#25552;&#39640;&#20102;&#22522;&#20110;&#26143;&#31995;&#22242;&#30340;&#24403;&#21069;&#21644;&#26410;&#26469;&#30340;&#23431;&#23449;&#23398;&#20998;&#26512;&#30340;&#28789;&#25935;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Complex astrophysical systems often exhibit low-scatter relations between observable properties (e.g., luminosity, velocity dispersion, oscillation period). These scaling relations illuminate the underlying physics, and can provide observational tools for estimating masses and distances. Machine learning can provide a fast and systematic way to search for new scaling relations (or for simple extensions to existing relations) in abstract high-dimensional parameter spaces. We use a machine learning tool called symbolic regression (SR), which models patterns in a dataset in the form of analytic equations. We focus on the Sunyaev-Zeldovich flux$-$cluster mass relation ($Y_\mathrm{SZ}-M$), the scatter in which affects inference of cosmological parameters from cluster abundance data. Using SR on the data from the IllustrisTNG hydrodynamical simulation, we find a new proxy for cluster mass which combines $Y_\mathrm{SZ}$ and concentration of ionized gas ($c_\mathrm{gas}$): $M \propto Y_\mathrm
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#19982;&#22312;&#32447;&#31639;&#27861;&#35774;&#35745;&#30340;&#36229;&#36234;&#26368;&#22351;&#24773;&#20917;&#31639;&#27861;&#20998;&#26512;&#27169;&#22411;&#65292;&#24182;&#22522;&#20110;&#35813;&#27169;&#22411;&#30740;&#31350;&#20102;&#26377;&#21521;&#22270;&#21644;&#26080;&#21521;&#22270;&#30340;&#22312;&#32447;Steiner&#26641;&#38382;&#39064;&#12290;&#36890;&#36807;&#39044;&#27979;&#22312;&#32447;&#32456;&#31471;&#33410;&#28857;&#65292;&#31639;&#27861;&#22312;&#26377;&#33391;&#22909;&#39044;&#27979;&#30340;&#24773;&#20917;&#19979;&#25171;&#30772;&#22312;&#32447;&#19979;&#30028;&#65292;&#24182;&#19988;&#31454;&#20105;&#27604;&#20363;&#20250;&#20248;&#38597;&#22320;&#38477;&#20302;&#12290;</title><link>http://arxiv.org/abs/2112.05353</link><description>&lt;p&gt;
&#23398;&#20064;&#22686;&#24378;&#30340;&#22312;&#32447;Steiner&#26641;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning-Augmented Algorithms for Online Steiner Tree. (arXiv:2112.05353v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.05353
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#19982;&#22312;&#32447;&#31639;&#27861;&#35774;&#35745;&#30340;&#36229;&#36234;&#26368;&#22351;&#24773;&#20917;&#31639;&#27861;&#20998;&#26512;&#27169;&#22411;&#65292;&#24182;&#22522;&#20110;&#35813;&#27169;&#22411;&#30740;&#31350;&#20102;&#26377;&#21521;&#22270;&#21644;&#26080;&#21521;&#22270;&#30340;&#22312;&#32447;Steiner&#26641;&#38382;&#39064;&#12290;&#36890;&#36807;&#39044;&#27979;&#22312;&#32447;&#32456;&#31471;&#33410;&#28857;&#65292;&#31639;&#27861;&#22312;&#26377;&#33391;&#22909;&#39044;&#27979;&#30340;&#24773;&#20917;&#19979;&#25171;&#30772;&#22312;&#32447;&#19979;&#30028;&#65292;&#24182;&#19988;&#31454;&#20105;&#27604;&#20363;&#20250;&#20248;&#38597;&#22320;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#32771;&#34385;&#20102;&#26368;&#36817;&#27969;&#34892;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#19982;&#22312;&#32447;&#31639;&#27861;&#35774;&#35745;&#30456;&#32467;&#21512;&#30340;&#36229;&#36234;&#26368;&#22351;&#24773;&#20917;&#31639;&#27861;&#20998;&#26512;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#36825;&#20010;&#27169;&#22411;&#20013;&#30740;&#31350;&#20102;&#26377;&#21521;&#22270;&#21644;&#26080;&#21521;&#22270;&#30340;&#22312;&#32447;Steiner&#26641;&#38382;&#39064;&#12290;&#22312;&#32447;&#35774;&#32622;&#19979;&#65292;Steiner&#26641;&#24050;&#30693;&#20855;&#26377;&#24378;&#21170;&#30340;&#19979;&#30028;&#65292;&#24182;&#19988;&#20219;&#20309;&#31639;&#27861;&#30340;&#26368;&#22351;&#24773;&#20917;&#20445;&#35777;&#37117;&#36317;&#31163;&#29702;&#24819;&#24456;&#36828;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#39044;&#27979;&#22312;&#32447;&#32456;&#31471;&#33410;&#28857;&#30340;&#31639;&#27861;&#12290;&#36825;&#20123;&#39044;&#27979;&#21487;&#33021;&#26159;&#38169;&#35823;&#30340;&#65292;&#31639;&#27861;&#30340;&#24615;&#33021;&#21462;&#20915;&#20110;&#39044;&#27979;&#38169;&#35823;&#30340;&#32456;&#31471;&#33410;&#28857;&#25968;&#37327;&#12290;&#36825;&#20123;&#20445;&#35777;&#30830;&#20445;&#20102;&#31639;&#27861;&#22312;&#26377;&#33391;&#22909;&#39044;&#27979;&#30340;&#24773;&#20917;&#19979;&#25171;&#30772;&#22312;&#32447;&#19979;&#30028;&#65292;&#24182;&#19988;&#24403;&#39044;&#27979;&#38169;&#35823;&#29575;&#22686;&#21152;&#26102;&#65292;&#31454;&#20105;&#27604;&#20363;&#20250;&#20248;&#38597;&#22320;&#38477;&#20302;&#12290;&#28982;&#21518;&#25105;&#20204;&#35266;&#23519;&#21040;&#29702;&#35770;&#26159;&#39044;&#27979;&#23454;&#39564;&#32467;&#26524;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#32456;&#31471;&#33410;&#28857;&#20174;&#20998;&#24067;&#20013;&#25277;&#21462;&#30340;&#22270;&#19978;&#65292;&#26032;&#30340;&#22312;&#32447;&#31639;&#27861;&#21363;&#20351;&#20855;&#26377;&#36866;&#24230;&#30340;&#27491;&#30830;&#39044;&#27979;&#20540;&#65292;&#20063;&#20855;&#26377;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper considers the recently popular beyond-worst-case algorithm analysis model which integrates machine-learned predictions with online algorithm design. We consider the online Steiner tree problem in this model for both directed and undirected graphs. Steiner tree is known to have strong lower bounds in the online setting and any algorithm's worst-case guarantee is far from desirable. This paper considers algorithms that predict which terminal arrives online. The predictions may be incorrect and the algorithms' performance is parameterized by the number of incorrectly predicted terminals. These guarantees ensure that algorithms break through the online lower bounds with good predictions and the competitive ratio gracefully degrades as the prediction error grows. We then observe that the theory is predictive of what will occur empirically. We show on graphs where terminals are drawn from a distribution, the new online algorithms have strong performance even with modestly correct 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20989;&#25968;&#26041;&#24046;&#30340;Langevin&#20272;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#35745;&#31639;&#36229;&#21442;&#25968;&#27169;&#22411;&#30340;&#27867;&#21270;&#32570;&#21475;&#65292;&#23454;&#29616;&#19982;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#31639;&#27861;&#19968;&#33268;&#12290;</title><link>http://arxiv.org/abs/2112.03660</link><description>&lt;p&gt;
&#36890;&#36807;Langevin&#20989;&#25968;&#26041;&#24046;&#20272;&#35745;&#36229;&#21442;&#25968;&#27169;&#22411;&#30340;&#27867;&#21270;&#32570;&#21475;
&lt;/p&gt;
&lt;p&gt;
A generalization gap estimation for overparameterized models via the Langevin functional variance. (arXiv:2112.03660v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.03660
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20989;&#25968;&#26041;&#24046;&#30340;Langevin&#20272;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#35745;&#31639;&#36229;&#21442;&#25968;&#27169;&#22411;&#30340;&#27867;&#21270;&#32570;&#21475;&#65292;&#23454;&#29616;&#19982;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#31639;&#27861;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#27867;&#21270;&#32570;&#21475;&#30340;&#20272;&#35745;&#65292;&#21363;&#27867;&#21270;&#24615;&#33021;&#19982;&#35757;&#32451;&#24615;&#33021;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#38024;&#23545;&#21253;&#25324;&#31070;&#32463;&#32593;&#32476;&#22312;&#20869;&#30340;&#36229;&#21442;&#25968;&#27169;&#22411;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#20989;&#25968;&#26041;&#24046;&#8212;&#8212;&#19968;&#20010;&#23450;&#20041;&#24191;&#27867;&#30340;&#20449;&#24687;&#20934;&#21017;&#20013;&#30340;&#20851;&#38190;&#27010;&#24565;&#8212;&#8212;&#22312;&#36229;&#21442;&#25968;&#27169;&#22411;&#20013;&#20063;&#33021;&#34920;&#24449;&#27867;&#21270;&#32570;&#21475;&#65292;&#21363;&#20351;&#20256;&#32479;&#29702;&#35770;&#26080;&#27861;&#24212;&#29992;&#20110;&#36229;&#21442;&#25968;&#27169;&#22411;&#12290;&#30001;&#20110;&#20989;&#25968;&#26041;&#24046;&#30340;&#35745;&#31639;&#25104;&#26412;&#23545;&#20110;&#36229;&#21442;&#25968;&#27169;&#22411;&#32780;&#35328;&#38750;&#24120;&#26114;&#36149;&#65292;&#22240;&#27492;&#25105;&#20204;&#25552;&#20986;&#20102;&#20989;&#25968;&#26041;&#24046;&#30340;&#39640;&#25928;&#36817;&#20284;&#26041;&#27861;&#8212;&#8212;&#20989;&#25968;&#26041;&#24046;&#30340;Langevin&#20272;&#35745;&#65288;Langevin FV&#65289;&#12290;&#36825;&#31181;&#26041;&#27861;&#21482;&#21033;&#29992;&#20102;&#24179;&#26041;&#25439;&#22833;&#20989;&#25968;&#30340;&#19968;&#38454;&#26799;&#24230;&#65292;&#32780;&#27809;&#26377;&#20351;&#29992;&#20108;&#38454;&#26799;&#24230;&#65292;&#20174;&#32780;&#20445;&#35777;&#20102;&#35745;&#31639;&#30340;&#39640;&#25928;&#24615;&#65292;&#24182;&#19988;&#23454;&#29616;&#19982;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#31639;&#27861;&#26159;&#19968;&#33268;&#30340;&#12290;&#25105;&#20204;&#36890;&#36807;&#25968;&#20540;&#28436;&#31034;&#20102;Langevin FV&#65292;&#20272;&#35745;&#20102;&#36229;&#21442;&#25968;&#27169;&#22411;&#30340;&#27867;&#21270;&#32570;&#21475;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper discusses the estimation of the generalization gap, the difference between generalization performance and training performance, for overparameterized models including neural networks. We first show that a functional variance, a key concept in defining a widely-applicable information criterion, characterizes the generalization gap even in overparameterized settings where a conventional theory cannot be applied. As the computational cost of the functional variance is expensive for the overparameterized models, we propose an efficient approximation of the function variance, the Langevin approximation of the functional variance (Langevin FV). This method leverages only the $1$st-order gradient of the squared loss function, without referencing the $2$nd-order gradient; this ensures that the computation is efficient and the implementation is consistent with gradient-based optimization algorithms. We demonstrate the Langevin FV numerically by estimating the generalization gaps of o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22522;&#20110;Riemannian Mat\'ern kernels&#25216;&#26415;&#22312;&#26426;&#22120;&#20154;&#23398;&#20013;&#30340;&#23454;&#29616;&#65292;&#24182;&#22312;&#19968;&#32452;&#22522;&#20934;&#20989;&#25968;&#19978;&#23637;&#31034;&#20102;&#20854;&#24615;&#33021;&#12290;&#35813;&#25216;&#26415;&#21487;&#20197;&#26377;&#25928;&#22320;&#22788;&#29702;&#38750;&#27431;&#20960;&#37324;&#24471;&#22495;&#19978;&#23450;&#20041;&#30340;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2111.01460</link><description>&lt;p&gt;
&#22522;&#20110;&#40654;&#26364;Matern&#26680;&#30340;&#20960;&#20309;&#24863;&#30693;&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#26426;&#22120;&#20154;&#23398;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Geometry-aware Bayesian Optimization in Robotics using Riemannian Mat\'ern Kernels. (arXiv:2111.01460v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.01460
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22522;&#20110;Riemannian Mat\'ern kernels&#25216;&#26415;&#22312;&#26426;&#22120;&#20154;&#23398;&#20013;&#30340;&#23454;&#29616;&#65292;&#24182;&#22312;&#19968;&#32452;&#22522;&#20934;&#20989;&#25968;&#19978;&#23637;&#31034;&#20102;&#20854;&#24615;&#33021;&#12290;&#35813;&#25216;&#26415;&#21487;&#20197;&#26377;&#25928;&#22320;&#22788;&#29702;&#38750;&#27431;&#20960;&#37324;&#24471;&#22495;&#19978;&#23450;&#20041;&#30340;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#26159;&#19968;&#31181;&#25968;&#25454;&#26377;&#25928;&#30340;&#25216;&#26415;&#65292;&#21487;&#29992;&#20110;&#25511;&#21046;&#21442;&#25968;&#35843;&#25972;&#12289;&#21442;&#25968;&#31574;&#30053;&#36866;&#24212;&#21644;&#26426;&#22120;&#20154;&#32467;&#26500;&#35774;&#35745;&#12290;&#20854;&#20013;&#35768;&#22810;&#38382;&#39064;&#38656;&#35201;&#20248;&#21270;&#22312;&#38750;&#27431;&#20960;&#37324;&#24471;&#22495;&#19978;&#23450;&#20041;&#30340;&#20989;&#25968;&#65292;&#22914;&#29699;&#12289;&#26059;&#36716;&#32676;&#25110;&#27491;&#23450;&#30697;&#38453;&#31354;&#38388;&#12290;&#20026;&#27492;&#65292;&#24517;&#39035;&#22312;&#25152;&#20851;&#27880;&#30340;&#31354;&#38388;&#20013;&#25918;&#32622;&#39640;&#26031;&#36807;&#31243;&#20808;&#39564;&#65292;&#25110;&#31561;&#20215;&#22320;&#23450;&#20041;&#19968;&#20010;&#26680;&#12290;&#26377;&#25928;&#30340;&#26680;&#36890;&#24120;&#21453;&#26144;&#20102;&#23427;&#20204;&#25152;&#23450;&#20041;&#31354;&#38388;&#30340;&#20960;&#20309;&#29305;&#24615;&#65292;&#20294;&#35774;&#35745;&#23427;&#20204;&#36890;&#24120;&#26159;&#38750;&#24179;&#20961;&#30340;&#12290;&#22522;&#20110;&#38543;&#26426;&#20559;&#24494;&#20998;&#26041;&#31243;&#21644;&#25289;&#26222;&#25289;&#26031; &#8211; &#36125;&#23572;&#29305;&#25289;&#31859;&#31639;&#23376;&#30340;&#35889;&#29702;&#35770;&#30340;&#26368;&#36817;&#30740;&#31350;&#65292;&#25552;&#20379;&#20102;&#26500;&#24314;&#36825;&#31181;&#20960;&#20309;&#24863;&#30693;&#26680;&#30340;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26426;&#22120;&#20154;&#23398;&#20013;&#23454;&#29616;&#36825;&#20123;&#20869;&#26680;&#30340;&#25216;&#26415;&#65292;&#22312;&#19968;&#32452;&#20154;&#24037;&#22522;&#20934;&#21151;&#33021;&#19978;&#23637;&#31034;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#65292;&#24182;&#35828;&#26126;&#20102;&#20960;&#20309;&#24863;&#30693;&#20248;&#21270;&#22312;&#23454;&#38469;&#26426;&#22120;&#20154;&#25511;&#21046;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian optimization is a data-efficient technique which can be used for control parameter tuning, parametric policy adaptation, and structure design in robotics. Many of these problems require optimization of functions defined on non-Euclidean domains like spheres, rotation groups, or spaces of positive-definite matrices. To do so, one must place a Gaussian process prior, or equivalently define a kernel, on the space of interest. Effective kernels typically reflect the geometry of the spaces they are defined on, but designing them is generally non-trivial. Recent work on the Riemannian Mat\'ern kernels, based on stochastic partial differential equations and spectral theory of the Laplace-Beltrami operator, offers promising avenues towards constructing such geometry-aware kernels. In this paper, we study techniques for implementing these kernels on manifolds of interest in robotics, demonstrate their performance on a set of artificial benchmark functions, and illustrate geometry-aware
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#30693;&#35782;&#33457;&#25197;&#8221;&#26041;&#27861;&#26469;&#25551;&#36848;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;&#65292;&#24182;&#21487;&#20197;&#34920;&#36798;&#24191;&#27867;&#30340;&#23884;&#20837;&#20808;&#39564;&#32422;&#26463;&#65292;&#21487;&#36731;&#26494;&#24212;&#23545;&#22797;&#21512;&#20851;&#31995;&#25512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2110.03789</link><description>&lt;p&gt;
&#30693;&#35782;&#33457;&#26463;&#65306;&#36866;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#33457;&#25197;&#29702;&#35770;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Knowledge Sheaves: A Sheaf-Theoretic Framework for Knowledge Graph Embedding. (arXiv:2110.03789v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.03789
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#30693;&#35782;&#33457;&#25197;&#8221;&#26041;&#27861;&#26469;&#25551;&#36848;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;&#65292;&#24182;&#21487;&#20197;&#34920;&#36798;&#24191;&#27867;&#30340;&#23884;&#20837;&#20808;&#39564;&#32422;&#26463;&#65292;&#21487;&#36731;&#26494;&#24212;&#23545;&#22797;&#21512;&#20851;&#31995;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#25351;&#23398;&#20064;&#23454;&#20307;&#65288;&#22270;&#30340;&#39030;&#28857;&#65289;&#21644;&#20851;&#31995;&#65288;&#22270;&#30340;&#36793;&#65289;&#30340;&#34920;&#31034;&#65292;&#20197;&#20351;&#24471;&#29983;&#25104;&#30340;&#34920;&#31034;&#32534;&#30721;&#30693;&#35782;&#22270;&#35889;&#20013;&#24050;&#30693;&#30340;&#20107;&#23454;&#20449;&#24687;&#65292;&#24182;&#21487;&#20197;&#29992;&#20110;&#25512;&#29702;&#26032;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#33258;&#28982;&#22320;&#34920;&#36798;&#20026;&#8220;&#32454;&#32990;&#33457;&#25197;&#8221;&#30340;&#25299;&#25169;&#21644;&#33539;&#30068;&#35821;&#35328;&#65306;&#19968;&#20010;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#21487;&#20197;&#25551;&#36848;&#20026;&#36866;&#24403;&#30340;&#8220;&#30693;&#35782;&#33457;&#25197;&#8221;&#22312;&#22270;&#19978;&#30340;&#36817;&#20284;&#20840;&#23616;&#25130;&#38754;&#65292;&#20854;&#19968;&#33268;&#24615;&#32422;&#26463;&#26159;&#30001;&#30693;&#35782;&#22270;&#35889;&#30340;&#26550;&#26500;&#24341;&#36215;&#30340;&#12290;&#36825;&#31181;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#24191;&#20041;&#30340;&#26694;&#26550;&#26469;&#25512;&#29702;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;&#65292;&#24182;&#20801;&#35768;&#34920;&#36798;&#24191;&#27867;&#30340;&#23884;&#20837;&#20808;&#39564;&#32422;&#26463;&#12290;&#27492;&#22806;&#65292;&#29983;&#25104;&#30340;&#23884;&#20837;&#21487;&#20197;&#36731;&#26494;&#22320;&#36866;&#24212;&#20110;&#22797;&#21512;&#20851;&#31995;&#30340;&#25512;&#29702;&#65292;&#26080;&#38656;&#29305;&#27530;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;&#36825;&#20123;&#24819;&#27861;&#65292;&#20197;&#23637;&#31034;&#36825;&#31181;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph embedding involves learning representations of entities -the vertices of the graph -- and relations -- the edges of the graph -- such that the resulting representations encode the known factual information represented by the knowledge graph and can be used in the inference of new relations. We show that knowledge graph embedding is naturally expressed in the topological and categorical language of \textit{cellular sheaves}: a knowledge graph embedding can be described as an approximate global section of an appropriate \textit{knowledge sheaf} over the graph, with consistency constraints induced by the knowledge graph's schema. This approach provides a generalized framework for reasoning about knowledge graph embedding models and allows for the expression of a wide range of prior constraints on embeddings. Further, the resulting embeddings can be easily adapted for reasoning over composite relations without special training. We implement these ideas to highlight the be
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25919;&#31574;&#26799;&#24230;&#31639;&#27861;&#30340;&#36817;&#20284;&#29275;&#39039;&#26041;&#27861;&#65292;&#21253;&#25324;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#21644;&#20840;&#26032;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#20855;&#26377;&#24555;&#36895;&#25910;&#25947;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2110.02398</link><description>&lt;p&gt;
&#36817;&#20284;&#29275;&#39039;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Approximate Newton policy gradient algorithms. (arXiv:2110.02398v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.02398
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25919;&#31574;&#26799;&#24230;&#31639;&#27861;&#30340;&#36817;&#20284;&#29275;&#39039;&#26041;&#27861;&#65292;&#21253;&#25324;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#21644;&#20840;&#26032;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#20855;&#26377;&#24555;&#36895;&#25910;&#25947;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#25919;&#31574;&#26799;&#24230;&#31639;&#27861;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#21644;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#20013;&#12290;&#24120;&#24120;&#20351;&#29992;&#21508;&#31181;&#29109;&#20989;&#25968;&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;&#20197;&#40723;&#21169;&#25506;&#32034;&#21644;&#25552;&#39640;&#31283;&#23450;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#29109;&#27491;&#21017;&#21270;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#30340;&#36817;&#20284;&#29275;&#39039;&#26041;&#27861;&#12290;&#22312;Shannon&#29109;&#30340;&#24773;&#20917;&#19979;&#65292;&#25152;&#24471;&#21040;&#30340;&#31639;&#27861;&#22797;&#21046;&#20102;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#12290;&#23545;&#20110;&#20854;&#20182;&#29109;&#20989;&#25968;&#65292;&#36825;&#31181;&#26041;&#27861;&#24471;&#21040;&#20102;&#20840;&#26032;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#26377;&#36825;&#20123;&#31639;&#27861;&#20855;&#26377;&#29275;&#39039;&#31867;&#22411;&#30340;&#20108;&#27425;&#25910;&#25947;&#24615;&#65292;&#24182;&#19988;&#30456;&#24212;&#30340;&#26799;&#24230;&#27969;&#20840;&#23616;&#25910;&#25947;&#20110;&#26368;&#20248;&#35299;&#12290;&#25105;&#20204;&#20351;&#29992;&#21512;&#25104;&#21644;&#24037;&#19994;&#35268;&#27169;&#30340;&#31034;&#20363;&#26469;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#36817;&#20284;&#29275;&#39039;&#26041;&#27861;&#36890;&#24120;&#22312;&#21333;&#20301;&#25968;&#36845;&#20195;&#20013;&#25910;&#25947;&#65292;&#24182;&#19988;&#24448;&#24448;&#27604;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#24555;&#20960;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
Policy gradient algorithms have been widely applied to Markov decision processes and reinforcement learning problems in recent years. Regularization with various entropy functions is often used to encourage exploration and improve stability. This paper proposes an approximate Newton method for the policy gradient algorithm with entropy regularization. In the case of Shannon entropy, the resulting algorithm reproduces the natural policy gradient algorithm. For other entropy functions, this method results in brand-new policy gradient algorithms. We prove that all these algorithms enjoy Newton-type quadratic convergence and that the corresponding gradient flow converges globally to the optimal solution. We use synthetic and industrial-scale examples to demonstrate that the proposed approximate Newton method typically converges in single-digit iterations, often orders of magnitude faster than other state-of-the-art algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#23398;&#35268;&#21010;&#38382;&#39064;&#65288;MPCC&#65289;&#30340;&#26041;&#27861;&#29992;&#20110;&#35299;&#20915;&#21452;&#23618;&#25104;&#20687;&#23398;&#20064;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#28385;&#36275;&#19968;&#23450;&#26465;&#20214;&#19979;&#65292;&#21487;&#20197;&#24471;&#20986;&#31283;&#23450;&#26465;&#20214;&#12289;&#26368;&#20248;&#24615;&#26465;&#20214;&#21644;&#23616;&#37096;&#21807;&#19968;&#24615;&#32467;&#26524;&#65292;&#21487;&#29992;&#20110;&#20989;&#25968;&#31354;&#38388;&#20013;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#21487;&#20197;&#21152;&#20837;&#23545;&#26799;&#24230;&#30340;&#32422;&#26463;&#12290;</title><link>http://arxiv.org/abs/2110.02273</link><description>&lt;p&gt;
&#20855;&#26377;&#20114;&#34917;&#32422;&#26463;&#30340;&#25968;&#23398;&#35268;&#21010;&#38382;&#39064;&#20316;&#20026;&#21452;&#23618;&#25104;&#20687;&#23398;&#20064;&#38382;&#39064;&#30340;&#25913;&#36827;&#21644;&#29702;&#35770;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Bilevel Imaging Learning Problems as Mathematical Programs with Complementarity Constraints: Reformulation and Theory. (arXiv:2110.02273v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.02273
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#23398;&#35268;&#21010;&#38382;&#39064;&#65288;MPCC&#65289;&#30340;&#26041;&#27861;&#29992;&#20110;&#35299;&#20915;&#21452;&#23618;&#25104;&#20687;&#23398;&#20064;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#28385;&#36275;&#19968;&#23450;&#26465;&#20214;&#19979;&#65292;&#21487;&#20197;&#24471;&#20986;&#31283;&#23450;&#26465;&#20214;&#12289;&#26368;&#20248;&#24615;&#26465;&#20214;&#21644;&#23616;&#37096;&#21807;&#19968;&#24615;&#32467;&#26524;&#65292;&#21487;&#29992;&#20110;&#20989;&#25968;&#31354;&#38388;&#20013;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#21487;&#20197;&#21152;&#20837;&#23545;&#26799;&#24230;&#30340;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31867;&#21452;&#23618;&#25104;&#20687;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#36739;&#20302;&#23618;&#27425;&#30340;&#23454;&#20363;&#23545;&#24212;&#20110;&#28041;&#21450;&#19968;&#38454;&#21644;&#20108;&#38454;&#38750;&#24179;&#28369;&#31232;&#30095;&#27491;&#21017;&#21270;&#22120;&#30340;&#20984;&#21464;&#20998;&#27169;&#22411;&#12290;&#36890;&#36807;&#20351;&#29992;&#36739;&#20302;&#32423;&#38382;&#39064;&#30340;&#21407;&#22987;&#23545;&#20598;&#37325;&#26500;&#30340;&#20960;&#20309;&#24615;&#36136;&#21644;&#24341;&#20837;&#36866;&#24403;&#30340;&#36741;&#21161;&#21464;&#37327;&#65292;&#25105;&#20204;&#33021;&#22815;&#23558;&#21407;&#22987;&#30340;&#21452;&#23618;&#38382;&#39064;&#37325;&#26032;&#34920;&#36848;&#20026;&#20855;&#26377;&#20114;&#34917;&#32422;&#26463;&#30340;&#25968;&#23398;&#35268;&#21010;&#38382;&#39064;&#65288;MPCC&#65289;&#12290;&#23545;&#20110;&#21518;&#32773;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#32039;&#30340;&#32422;&#26463;&#26465;&#20214;&#21512;&#35268;&#24615;&#26465;&#20214;&#65288;MPCC-RCPLD&#21644;&#37096;&#20998;MPCC-LICQ&#65289;&#24182;&#23548;&#20986;Mordukhovich&#65288;M-&#65289;&#21644;Strong&#65288;S-&#65289;&#31283;&#23450;&#26465;&#20214;&#12290;MPCC&#30340;&#38745;&#27490;&#31995;&#32479;&#20063;&#21487;&#20197;&#25104;&#20026;&#21407;&#22987;&#20844;&#24335;&#30340;&#38745;&#27490;&#26465;&#20214;&#12290;&#27492;&#22806;&#65292;&#23548;&#20986;&#20102;&#20108;&#38454;&#20805;&#20998;&#26368;&#20248;&#24615;&#26465;&#20214;&#65292;&#36824;&#24471;&#20986;&#20102;&#38745;&#27490;&#28857;&#30340;&#23616;&#37096;&#21807;&#19968;&#24615;&#32467;&#26524;&#12290;&#25152;&#25552;&#20986;&#30340;&#25913;&#36827;&#21487;&#20197;&#25193;&#23637;&#21040;&#20989;&#25968;&#31354;&#38388;&#20013;&#30340;&#38382;&#39064;&#65292;&#23548;&#33268;&#23545;&#26799;&#24230;&#30340;&#32422;&#26463;&#30340;&#20855;&#26377;&#20114;&#34917;&#32422;&#26463;&#30340;&#25968;&#23398;&#35268;&#21010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate a family of bilevel imaging learning problems where the lower-level instance corresponds to a convex variational model involving firstand second-order nonsmooth sparsity-based regularizers. By using geometric properties of the primal-dual reformulation of the lower-level problem and introducing suitable auxiliar variables, we are able to reformulate the original bilevel problems as Mathematical Programs with Complementarity Constraints (MPCC). For the latter, we prove tight constraint qualification conditions (MPCC-RCPLD and partial MPCC-LICQ) and derive Mordukhovich (M-) and Strong (S-) stationarity conditions. The stationarity systems for the MPCC turn also into stationarity conditions for the original formulation. Second-order sufficient optimality conditions are derived as well, together with a local uniqueness result for stationary points. The proposed reformulation may be extended to problems in function spaces, leading to MPCC's with constraints on the gradient 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Spacetimeformer&#30340;&#26041;&#27861;&#65292;&#23558;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#36716;&#21270;&#20026;&#8220;&#26102;&#31354;&#24207;&#21015;&#8221;&#24418;&#24335;&#36827;&#34892;&#24314;&#27169;&#65292;&#23454;&#29616;&#20102;&#23545;&#21464;&#37327;&#20043;&#38388;&#21160;&#24577;&#31354;&#38388;&#20851;&#31995;&#30340;&#23398;&#20064;&#65292;&#21516;&#26102;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2109.12218</link><description>&lt;p&gt;
&#38754;&#21521;&#21160;&#24577;&#26102;&#31354;&#39044;&#27979;&#30340;&#38271;&#36317;&#31163;Transformer
&lt;/p&gt;
&lt;p&gt;
Long-Range Transformers for Dynamic Spatiotemporal Forecasting. (arXiv:2109.12218v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.12218
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Spacetimeformer&#30340;&#26041;&#27861;&#65292;&#23558;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#36716;&#21270;&#20026;&#8220;&#26102;&#31354;&#24207;&#21015;&#8221;&#24418;&#24335;&#36827;&#34892;&#24314;&#27169;&#65292;&#23454;&#29616;&#20102;&#23545;&#21464;&#37327;&#20043;&#38388;&#21160;&#24577;&#31354;&#38388;&#20851;&#31995;&#30340;&#23398;&#20064;&#65292;&#21516;&#26102;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#33268;&#21147;&#20110;&#22522;&#20110;&#21382;&#21490;&#24773;&#22659;&#39044;&#27979;&#26410;&#26469;&#20540;&#12290;&#29616;&#26377;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#21033;&#29992;&#31070;&#32463;&#20851;&#27880;&#26426;&#21046;&#36827;&#34892;&#26102;&#38388;&#23398;&#20064;&#65292;&#20294;&#26410;&#32771;&#34385;&#21464;&#37327;&#38388;&#30340;&#31354;&#38388;&#20851;&#31995;&#12290;&#30456;&#27604;&#32780;&#35328;&#65292;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#26126;&#30830;&#24314;&#27169;&#21464;&#37327;&#20851;&#31995;&#65292;&#20294;&#24448;&#24448;&#20381;&#36182;&#20110;&#39044;&#23450;&#20041;&#22270;&#65292;&#19981;&#33021;&#38543;&#26102;&#38388;&#21464;&#21270;&#19988;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#20013;&#23545;&#21508;&#21464;&#37327;&#36827;&#34892;&#29420;&#31435;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#26356;&#26032;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#36890;&#36807;&#23558;&#22810;&#20803;&#39044;&#27979;&#36716;&#21270;&#20026;&#8220;&#26102;&#31354;&#24207;&#21015;&#8221;&#24418;&#24335;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#20854;&#20013;&#27599;&#20010;Transformer&#36755;&#20837;&#34920;&#31034;&#32473;&#23450;&#26102;&#38388;&#21333;&#20010;&#21464;&#37327;&#30340;&#20540;&#12290;&#38271;&#36317;&#31163;Transformer&#21487;&#20197;&#27839;&#30528;&#36825;&#20010;&#25193;&#23637;&#24207;&#21015;&#20849;&#21516;&#23398;&#20064;&#31354;&#38388;&#12289;&#26102;&#38388;&#21644;&#20540;&#20449;&#24687;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#31216;&#20026;Spacetimeformer&#65292;&#22312;&#22810;&#20010;&#22810;&#20803;&#39044;&#27979;&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#24182;&#21487;&#20197;&#21160;&#24577;&#26356;&#26032;&#21464;&#37327;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multivariate time series forecasting focuses on predicting future values based on historical context. State-of-the-art sequence-to-sequence models rely on neural attention between timesteps, which allows for temporal learning but fails to consider distinct spatial relationships between variables. In contrast, methods based on graph neural networks explicitly model variable relationships. However, these methods often rely on predefined graphs that cannot change over time and perform separate spatial and temporal updates without establishing direct connections between each variable at every timestep. Our work addresses these problems by translating multivariate forecasting into a "spatiotemporal sequence" formulation where each Transformer input token represents the value of a single variable at a given time. Long-Range Transformers can then learn interactions between space, time, and value information jointly along this extended sequence. Our method, which we call Spacetimeformer, achie
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#25968;&#25454;&#30340;&#27169;&#22411;&#20013;&#23548;&#25968;&#36873;&#25321;&#21450;&#20854;&#23616;&#38480;&#24615;&#65292;&#35748;&#20026;&#36825;&#20123;&#27169;&#22411;&#30456;&#24403;&#20110;&#38750;&#32447;&#24615;&#38271;&#24230;&#23610;&#24230;&#27604;&#20363;&#23450;&#24459;&#65292;&#24314;&#27169;&#27604;&#20363;&#23450;&#24459;&#21487;&#36991;&#20813;&#23454;&#38469;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2108.00413</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#30340;&#26412;&#26500;&#20851;&#31995;&#25581;&#31034;&#27969;&#20307;&#21147;&#23398;&#20256;&#36755;&#31995;&#25968;&#30340;&#27604;&#20363;&#23450;&#24459;
&lt;/p&gt;
&lt;p&gt;
Data-Driven Constitutive Relation Reveals Scaling Law for Hydrodynamic Transport Coefficients. (arXiv:2108.00413v4 [physics.flu-dyn] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.00413
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#25968;&#25454;&#30340;&#27169;&#22411;&#20013;&#23548;&#25968;&#36873;&#25321;&#21450;&#20854;&#23616;&#38480;&#24615;&#65292;&#35748;&#20026;&#36825;&#20123;&#27169;&#22411;&#30456;&#24403;&#20110;&#38750;&#32447;&#24615;&#38271;&#24230;&#23610;&#24230;&#27604;&#20363;&#23450;&#24459;&#65292;&#24314;&#27169;&#27604;&#20363;&#23450;&#24459;&#21487;&#36991;&#20813;&#23454;&#38469;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23547;&#25214;&#20174;&#39640;&#23494;&#24230;&#27668;&#20307;&#21040;&#31232;&#34180;&#27668;&#20307;&#21306;&#22495;&#22343;&#36866;&#29992;&#30340;&#25193;&#23637;&#27969;&#20307;&#21147;&#23398;&#26041;&#31243;&#20173;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65292;&#25104;&#21151;&#30340;&#20851;&#38190;&#22312;&#20110;&#33719;&#24471;&#31934;&#30830;&#30340;&#24212;&#21147;&#21644;&#28909;&#27969;&#30340;&#26412;&#26500;&#20851;&#31995;&#12290;&#22522;&#20110;&#25968;&#25454;&#30340;&#27169;&#22411;&#36890;&#36807;&#23545;&#39640;&#38454;&#23548;&#25968;&#30340;&#22238;&#24402;&#65292;&#20351;&#24471;&#26412;&#26500;&#20851;&#31995;&#33021;&#22815;&#25193;&#23637;&#29275;&#39039;&#31896;&#24230;&#23450;&#24459;&#21644;&#20613;&#37324;&#21494;&#28909;&#20256;&#23548;&#23450;&#24459;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#20013;&#23548;&#25968;&#30340;&#36873;&#25321;&#26159;&#21363;&#20852;&#20915;&#31574;&#30340;&#65292;&#32570;&#20047;&#26126;&#30830;&#30340;&#29289;&#29702;&#35299;&#37322;&#12290;&#25105;&#20204;&#23545;&#32447;&#24615;&#31995;&#32479;&#36827;&#34892;&#20102;&#29702;&#35770;&#25506;&#31350;&#65292;&#24182;&#35748;&#20026;&#36825;&#20123;&#27169;&#22411;&#30456;&#24403;&#20110;&#38750;&#32447;&#24615;&#38271;&#24230;&#23610;&#24230;&#27604;&#20363;&#23450;&#24459;&#12290;&#27604;&#20363;&#23450;&#24459;&#30340;&#31561;&#25928;&#24615;&#35777;&#26126;&#20102;&#23427;&#30340;&#29289;&#29702;&#21487;&#34892;&#24615;&#65292;&#24182;&#25581;&#31034;&#20102;&#22522;&#20110;&#25968;&#25454;&#30340;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#30340;&#35266;&#28857;&#36824;&#25351;&#20986;&#65292;&#24314;&#27169;&#27604;&#20363;&#23450;&#24459;&#21487;&#20197;&#36991;&#20813;&#22522;&#20110;&#25968;&#25454;&#30340;&#27169;&#22411;&#20013;&#30340;&#23454;&#38469;&#22256;&#38590;&#65292;&#22914;&#65306;
&lt;/p&gt;
&lt;p&gt;
Finding extended hydrodynamics equations valid from the dense gas region to the rarefied gas region remains a great challenge. The key to success is to obtain accurate constitutive relations for stress and heat flux. Data-driven models offer a new phenomenological approach to learning constitutive relations from data. Such models enable complex constitutive relations that extend Newton's law of viscosity and Fourier's law of heat conduction by regression on higher derivatives. However, the choices of derivatives in these models are ad-hoc without a clear physical explanation. We investigated data-driven models theoretically on a linear system. We argue that these models are equivalent to non-linear length scale scaling laws of transport coefficients. The equivalence to scaling laws justified the physical plausibility and revealed the limitation of data-driven models. Our argument also points out that modeling the scaling law could avoid practical difficulties in data-driven models like
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#20010;&#21517;&#20026;&#8220;&#28436;&#21592;&#8221;&#30340;&#35774;&#35745;&#20989;&#25968;&#21644;&#21517;&#20026;&#8220;&#23548;&#28436;&#8221;&#30340;&#22522;&#20110;RL&#30340;&#23398;&#20064;&#22120;&#20043;&#38388;&#30340;&#21327;&#20316;&#65292;&#26368;&#26032;&#26041;&#27861;&#21487;&#20197;&#25214;&#21040;&#26368;&#20248;&#25915;&#20987;&#65292;&#25552;&#39640;&#20102;&#20851;&#20110;RL&#20195;&#29702;&#40065;&#26834;&#24615;&#30340;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2106.05087</link><description>&lt;p&gt;
&#26368;&#24378;&#25932;&#20154;&#26159;&#35841;&#65311;&#25506;&#32034;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#26368;&#20248;&#21644;&#39640;&#25928;&#30340;&#35268;&#36991;&#25915;&#20987;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Who Is the Strongest Enemy? Towards Optimal and Efficient Evasion Attacks in Deep RL. (arXiv:2106.05087v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.05087
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#20010;&#21517;&#20026;&#8220;&#28436;&#21592;&#8221;&#30340;&#35774;&#35745;&#20989;&#25968;&#21644;&#21517;&#20026;&#8220;&#23548;&#28436;&#8221;&#30340;&#22522;&#20110;RL&#30340;&#23398;&#20064;&#22120;&#20043;&#38388;&#30340;&#21327;&#20316;&#65292;&#26368;&#26032;&#26041;&#27861;&#21487;&#20197;&#25214;&#21040;&#26368;&#20248;&#25915;&#20987;&#65292;&#25552;&#39640;&#20102;&#20851;&#20110;RL&#20195;&#29702;&#40065;&#26834;&#24615;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19968;&#20123;&#38480;&#21046;&#26465;&#20214;&#19979;&#65292;&#36890;&#36807;&#22312;&#29366;&#24577;&#35266;&#23519;&#20013;&#36827;&#34892;&#26368;&#20248;&#23545;&#25239;&#25200;&#21160;&#26469;&#23545;&#24378;&#21270;&#23398;&#20064;(RL)&#20195;&#29702;&#30340;&#26368;&#22351;&#24773;&#20917;&#24615;&#33021;&#36827;&#34892;&#35780;&#20272;&#65292;&#23545;&#20110;&#29702;&#35299;RL&#20195;&#29702;&#30340;&#40065;&#26834;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#25214;&#21040;&#26368;&#20248;&#23545;&#25163;&#24456;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#26080;&#35770;&#26159;&#25105;&#20204;&#33021;&#21542;&#25214;&#21040;&#26368;&#20248;&#25915;&#20987;&#30340;&#22909;&#22351;&#65292;&#36824;&#26159;&#22914;&#20309;&#39640;&#25928;&#22320;&#25214;&#21040;&#26368;&#20248;&#25915;&#20987;&#12290;&#29616;&#26377;&#30340;&#23545;&#25239;RL&#24037;&#20316;&#35201;&#20040;&#20351;&#29992;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#23547;&#25214;&#26368;&#24378;&#23545;&#25163;&#65292;&#35201;&#20040;&#36890;&#36807;&#23558;&#20195;&#29702;&#35270;&#20026;&#29615;&#22659;&#30340;&#19968;&#37096;&#20998;&#30452;&#25509;&#35757;&#32451;&#22522;&#20110;RL&#30340;&#23545;&#25163;&#65292;&#21487;&#20197;&#25214;&#21040;&#26368;&#20339;&#23545;&#25163;&#65292;&#20294;&#22312;&#22823;&#29366;&#24577;&#31354;&#38388;&#20013;&#21487;&#33021;&#21464;&#24471;&#26840;&#25163;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#20010;&#21517;&#20026;&#8220;&#28436;&#21592;&#8221;&#30340;&#35774;&#35745;&#20989;&#25968;&#21644;&#21517;&#20026;&#8220;&#23548;&#28436;&#8221;&#30340;&#22522;&#20110;RL&#30340;&#23398;&#20064;&#22120;&#20043;&#38388;&#30340;&#21327;&#20316;&#26469;&#23547;&#25214;&#26368;&#20248;&#25915;&#20987;&#12290;&#28436;&#21592;&#20026;&#32473;&#23450;&#30340;&#31574;&#30053;&#25200;&#21160;&#26041;&#21521;&#21046;&#20316;&#29366;&#24577;&#25200;&#21160;&#65292;&#23548;&#28436;&#23398;&#20064;&#25552;&#20986;&#26368;&#20339;&#31574;&#30053;&#25200;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating the worst-case performance of a reinforcement learning (RL) agent under the strongest/optimal adversarial perturbations on state observations (within some constraints) is crucial for understanding the robustness of RL agents. However, finding the optimal adversary is challenging, in terms of both whether we can find the optimal attack and how efficiently we can find it. Existing works on adversarial RL either use heuristics-based methods that may not find the strongest adversary, or directly train an RL-based adversary by treating the agent as a part of the environment, which can find the optimal adversary but may become intractable in a large state space. This paper introduces a novel attacking method to find the optimal attacks through collaboration between a designed function named "actor" and an RL-based learner named "director". The actor crafts state perturbations for a given policy perturbation direction, and the director learns to propose the best policy perturbation
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21487;&#25945;&#23398;&#33258;&#20027;&#26234;&#33021;&#20307;&#30340;&#27010;&#24565;&#65292;&#23427;&#20204;&#21487;&#20197;&#20174;&#20869;&#37096;&#21644;&#25945;&#23398;&#20449;&#21495;&#20013;&#21516;&#27493;&#23398;&#20064;&#65292;&#20174;&#32780;&#25552;&#39640;&#25216;&#33021;&#20064;&#24471;&#30340;&#25928;&#29575;&#65292;&#27492;&#20030;&#26159;&#26500;&#24314;&#20855;&#26377;&#20154;&#31867;&#32423;&#26234;&#33021;&#30340;&#20195;&#29702;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;</title><link>http://arxiv.org/abs/2105.11977</link><description>&lt;p&gt;
&#36861;&#23547;&#21487;&#25945;&#23398;&#33258;&#20027;&#26234;&#33021;&#20307;
&lt;/p&gt;
&lt;p&gt;
Towards Teachable Autotelic Agents. (arXiv:2105.11977v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.11977
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21487;&#25945;&#23398;&#33258;&#20027;&#26234;&#33021;&#20307;&#30340;&#27010;&#24565;&#65292;&#23427;&#20204;&#21487;&#20197;&#20174;&#20869;&#37096;&#21644;&#25945;&#23398;&#20449;&#21495;&#20013;&#21516;&#27493;&#23398;&#20064;&#65292;&#20174;&#32780;&#25552;&#39640;&#25216;&#33021;&#20064;&#24471;&#30340;&#25928;&#29575;&#65292;&#27492;&#20030;&#26159;&#26500;&#24314;&#20855;&#26377;&#20154;&#31867;&#32423;&#26234;&#33021;&#30340;&#20195;&#29702;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#25506;&#32034;&#21644;&#30452;&#25509;&#25351;&#23548;&#26159;&#20799;&#31461;&#23398;&#20064;&#30340;&#20004;&#20010;&#19981;&#21516;&#26469;&#28304;&#65292;&#20294;&#25945;&#32946;&#31185;&#23398;&#35777;&#26126;&#65292;&#36741;&#21161;&#21457;&#29616;&#25110;&#24341;&#23548;&#28216;&#25103;&#31561;&#28151;&#21512;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#25216;&#33021;&#20064;&#24471;&#12290;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#36825;&#20123;&#26497;&#31471;&#20998;&#21035;&#26144;&#23556;&#20026;&#33258;&#20027;&#20195;&#29702;&#20174;&#33258;&#24049;&#30340;&#20449;&#21495;&#20013;&#23398;&#20064;&#65292;&#20197;&#21450;&#23436;&#20840;&#34987;&#25945;&#24072;&#25945;&#25480;&#30340;&#20132;&#20114;&#24335;&#23398;&#20064;&#20195;&#29702;&#12290;&#22312;&#20004;&#32773;&#20043;&#38388;&#24212;&#35813;&#31449;&#31435;&#21487;&#25945;&#23398;&#30340;&#33258;&#20027;&#26234;&#33021;&#20307;&#65288;TAA&#65289;&#65306;&#23427;&#20204;&#20174;&#20869;&#37096;&#21644;&#25945;&#23398;&#20449;&#21495;&#20013;&#23398;&#20064;&#65292;&#20197;&#20174;&#36741;&#21161;&#21457;&#29616;&#30340;&#26356;&#39640;&#25928;&#29575;&#20013;&#21463;&#30410;&#12290;&#35774;&#35745;&#36825;&#26679;&#30340;&#20195;&#29702;&#23558;&#20351;&#30495;&#23454;&#19990;&#30028;&#30340;&#38750;&#19987;&#19994;&#29992;&#25143;&#23558;&#20195;&#29702;&#30340;&#23398;&#20064;&#36712;&#36857;&#23450;&#21521;&#20110;&#20182;&#20204;&#30340;&#26399;&#26395;&#12290;&#26356;&#22522;&#26412;&#22320;&#65292;&#36825;&#20063;&#21487;&#33021;&#26159;&#26500;&#24314;&#20855;&#26377;&#20154;&#31867;&#32423;&#26234;&#33021;&#30340;&#20195;&#29702;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#21521;&#21487;&#25945;&#23398;&#33258;&#20027;&#20195;&#29702;&#35774;&#35745;&#30340;&#36335;&#32447;&#22270;&#12290;&#22522;&#20110;&#21457;&#23637;&#24515;&#29702;&#23398;&#21644;&#25945;&#32946;&#31185;&#23398;&#65292;&#25105;&#20204;&#39318;&#20808;&#30830;&#23450;&#20102;&#20851;&#38190;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous discovery and direct instruction are two distinct sources of learning in children but education sciences demonstrate that mixed approaches such as assisted discovery or guided play result in improved skill acquisition. In the field of Artificial Intelligence, these extremes respectively map to autonomous agents learning from their own signals and interactive learning agents fully taught by their teachers. In between should stand teachable autotelic agents (TAA): agents that learn from both internal and teaching signals to benefit from the higher efficiency of assisted discovery. Designing such agents will enable real-world non-expert users to orient the learning trajectories of agents towards their expectations. More fundamentally, this may also be a key step to build agents with human-level intelligence. This paper presents a roadmap towards the design of teachable autonomous agents. Building on developmental psychology and education sciences, we start by identifying key fe
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25913;&#36827;&#20102;&#20808;&#21069;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#29289;&#20307;&#20960;&#20309;&#31354;&#38388;&#20998;&#35299;&#20026;&#21018;&#24615;&#26041;&#21521;&#12289;&#38750;&#21018;&#24615;&#23039;&#24577;&#21644;&#22266;&#26377;&#24418;&#29366;&#65292;&#24182;&#20351;&#29992;&#35889;&#20960;&#20309;&#21644;&#27010;&#29575;&#20998;&#31163;&#30340;&#32452;&#21512;&#36827;&#34892;&#35757;&#32451;&#65292;&#36171;&#20104;&#20102;&#23545;&#23545;&#35937;&#21464;&#24418;&#31354;&#38388;&#30340;&#21487;&#35299;&#37322;&#24615;&#25551;&#36848;&#12290;&#21516;&#26102;&#25913;&#36827;&#21253;&#25324;&#26356;&#22797;&#26434;&#30340;&#26059;&#36716;&#19981;&#21464;&#22788;&#29702;&#21644;&#20351;&#29992;&#27969;&#24418;&#21464;&#24418;&#32593;&#32476;&#26469;&#36830;&#25509;&#28508;&#22312;&#21644;&#35889;&#31354;&#38388;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#25511;&#21046;&#29983;&#25104;&#24418;&#29366;&#12290;</title><link>http://arxiv.org/abs/2103.00142</link><description>&lt;p&gt;
&#22312;&#29983;&#25104;&#28508;&#22312;&#24418;&#29366;&#27169;&#22411;&#20013;&#35299;&#31163;&#20960;&#20309;&#21464;&#24418;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Disentangling Geometric Deformation Spaces in Generative Latent Shape Models. (arXiv:2103.00142v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2103.00142
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25913;&#36827;&#20102;&#20808;&#21069;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#29289;&#20307;&#20960;&#20309;&#31354;&#38388;&#20998;&#35299;&#20026;&#21018;&#24615;&#26041;&#21521;&#12289;&#38750;&#21018;&#24615;&#23039;&#24577;&#21644;&#22266;&#26377;&#24418;&#29366;&#65292;&#24182;&#20351;&#29992;&#35889;&#20960;&#20309;&#21644;&#27010;&#29575;&#20998;&#31163;&#30340;&#32452;&#21512;&#36827;&#34892;&#35757;&#32451;&#65292;&#36171;&#20104;&#20102;&#23545;&#23545;&#35937;&#21464;&#24418;&#31354;&#38388;&#30340;&#21487;&#35299;&#37322;&#24615;&#25551;&#36848;&#12290;&#21516;&#26102;&#25913;&#36827;&#21253;&#25324;&#26356;&#22797;&#26434;&#30340;&#26059;&#36716;&#19981;&#21464;&#22788;&#29702;&#21644;&#20351;&#29992;&#27969;&#24418;&#21464;&#24418;&#32593;&#32476;&#26469;&#36830;&#25509;&#28508;&#22312;&#21644;&#35889;&#31354;&#38388;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#25511;&#21046;&#29983;&#25104;&#24418;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#23436;&#25972;&#30340; 3D &#23545;&#35937;&#25551;&#36848;&#38656;&#35201;&#29992;&#21487;&#35299;&#37322;&#30340;&#26041;&#24335;&#34920;&#24449;&#21464;&#24418;&#31354;&#38388;&#65292;&#20174;&#21333;&#20010;&#23454;&#20363;&#30340;&#20851;&#33410;&#26500;&#36896;&#21040;&#36328;&#31867;&#21035;&#30340;&#24418;&#29366;&#21464;&#21270;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#19968;&#31181;&#20808;&#21069;&#30340; 3D &#24418;&#29366;&#20960;&#20309;&#20998;&#31163;&#29983;&#25104;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#23558;&#29289;&#20307;&#20960;&#20309;&#31354;&#38388;&#20998;&#35299;&#20026;&#21018;&#24615;&#26041;&#21521;&#12289;&#38750;&#21018;&#24615;&#23039;&#24577;&#21644;&#22266;&#26377;&#24418;&#29366;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#32463;&#20856;&#35889;&#20960;&#20309;&#21644;&#32467;&#26500;&#21270;&#28508;&#22312;&#34920;&#31034;&#31354;&#38388;&#30340;&#27010;&#29575;&#20998;&#31163;&#30340;&#32452;&#21512;&#65292;&#20174;&#21407;&#22987; 3D &#24418;&#29366;&#36827;&#34892;&#35757;&#32451;&#65292;&#26080;&#38656;&#23545;&#24212;&#20851;&#31995;&#12289;&#26631;&#31614;&#65292;&#29978;&#33267;&#26080;&#38656;&#21018;&#24615;&#23545;&#40784;&#12290;&#25105;&#20204;&#30340;&#25913;&#36827;&#21253;&#25324;&#26356;&#22797;&#26434;&#30340;&#26059;&#36716;&#19981;&#21464;&#22788;&#29702;&#21644;&#20351;&#29992;&#27969;&#24418;&#21464;&#24418;&#32593;&#32476;&#26469;&#36830;&#25509;&#28508;&#22312;&#21644;&#35889;&#31354;&#38388;&#12290;&#28508;&#22312;&#31354;&#38388;&#30340;&#20960;&#20309;&#32467;&#26500;&#36171;&#20104;&#20102;&#23545;&#23545;&#35937;&#21464;&#24418;&#31354;&#38388;&#30340;&#21487;&#35299;&#37322;&#24615;&#25551;&#36848;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#21487;&#20197;&#20351;&#23039;&#24577;&#36716;&#31227;&#21644;&#23039;&#24577;&#24863;&#30693;&#27880;&#20876;&#31561;&#20219;&#21153;&#26356;&#21152;&#23481;&#26131;&#65292;&#21516;&#26102;&#21487;&#20197;&#26356;&#22909;&#22320;&#25511;&#21046;&#29983;&#25104;&#24418;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;
A complete representation of 3D objects requires characterizing the space of deformations in an interpretable manner, from articulations of a single instance to changes in shape across categories. In this work, we improve on a prior generative model of geometric disentanglement for 3D shapes, wherein the space of object geometry is factorized into rigid orientation, non-rigid pose, and intrinsic shape. The resulting model can be trained from raw 3D shapes, without correspondences, labels, or even rigid alignment, using a combination of classical spectral geometry and probabilistic disentanglement of a structured latent representation space. Our improvements include more sophisticated handling of rotational invariance and the use of a diffeomorphic flow network to bridge latent and spectral space. The geometric structuring of the latent space imparts an interpretable characterization of the deformation space of an object. Furthermore, it enables tasks like pose transfer and pose-aware r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#32039;&#23494;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#20998;&#26512;&#22238;&#31572;&#20102;Q&#23398;&#20064;&#26159;&#21542;&#26159;&#26497;&#23567;&#26497;&#22823;&#26368;&#20248;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2102.06548</link><description>&lt;p&gt;
Q&#23398;&#20064;&#26159;&#21542;&#26159;&#26497;&#23567;&#26497;&#22823;&#26368;&#20248;&#30340;&#65311;&#19968;&#39033;&#32039;&#23494;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Is Q-Learning Minimax Optimal? A Tight Sample Complexity Analysis. (arXiv:2102.06548v4 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2102.06548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#32039;&#23494;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#20998;&#26512;&#22238;&#31572;&#20102;Q&#23398;&#20064;&#26159;&#21542;&#26159;&#26497;&#23567;&#26497;&#22823;&#26368;&#20248;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Q&#23398;&#20064;&#26159;&#24378;&#21270;&#23398;&#20064;&#30340;&#26680;&#24515;&#65292;&#26088;&#22312;&#20197;&#27169;&#22411;&#33258;&#30001;&#30340;&#26041;&#24335;&#23398;&#20064;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#26368;&#20248;Q&#20989;&#25968;&#12290;&#38024;&#23545;&#21516;&#27493;&#35774;&#32622;&#65288;&#21363;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#20174;&#29983;&#25104;&#27169;&#22411;&#20013;&#29420;&#31435;&#22320;&#25277;&#21462;&#25152;&#26377;&#29366;&#24577;-&#21160;&#20316;&#23545;&#30340;&#26679;&#26412;&#65289;&#65292;&#22312;&#29702;&#35299;Q&#23398;&#20064;&#30340;&#26679;&#26412;&#25928;&#29575;&#26041;&#38754;&#24050;&#32463;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#23545;&#20110;&#19968;&#20010;&#20855;&#26377;&#29366;&#24577;&#31354;&#38388;&#931;&#21644;&#21160;&#20316;&#31354;&#38388;&#913;&#30340;&#947;&#25240;&#25187;&#30340;&#26080;&#38480;&#26102;&#38388;&#38454;&#27573;MDP&#65292;&#20026;&#20102;&#20135;&#29983;&#26368;&#20248;Q&#20989;&#25968;&#30340;&#20803;&#32032;&#32423;&#949;&#36817;&#20284;&#65292;&#38024;&#23545;Q&#23398;&#20064;&#30340;&#26368;&#26032;&#29702;&#35770;&#38656;&#35201;&#19968;&#20010;&#26679;&#26412;&#22823;&#23567;&#36229;&#36807;&#931;&#8739;&#8739;&#215;&#913;&#8739;&#8739;&#8725;(1&#8722;&#947;)^5&#949;^{2}&#30340;&#37327;&#32423;&#65292;&#20294;&#36825;&#24182;&#19981;&#31526;&#21512;&#29616;&#26377;&#30340;&#26497;&#23567;&#26497;&#22823;&#19979;&#30028;&#12290;&#36825;&#24341;&#20986;&#20102;&#19968;&#20010;&#33258;&#28982;&#30340;&#38382;&#39064;&#65306;Q&#23398;&#20064;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#26159;&#22810;&#23569;&#65311;Q&#23398;&#20064;&#26159;&#21542;&#21487;&#35777;&#26126;&#26159;&#27425;&#20248;&#30340;&#65311;&#26412;&#25991;&#38024;&#23545;&#21516;&#27493;&#35774;&#32622;&#22238;&#31572;&#20102;&#36825;&#20123;&#38382;&#39064;&#65306;(1)
&lt;/p&gt;
&lt;p&gt;
Q-learning, which seeks to learn the optimal Q-function of a Markov decision process (MDP) in a model-free fashion, lies at the heart of reinforcement learning. When it comes to the synchronous setting (such that independent samples for all state-action pairs are drawn from a generative model in each iteration), substantial progress has been made towards understanding the sample efficiency of Q-learning. Consider a $\gamma$-discounted infinite-horizon MDP with state space $\mathcal{S}$ and action space $\mathcal{A}$: to yield an entrywise $\varepsilon$-approximation of the optimal Q-function, state-of-the-art theory for Q-learning requires a sample size exceeding the order of $\frac{|\mathcal{S}||\mathcal{A}|}{(1-\gamma)^5\varepsilon^{2}}$, which fails to match existing minimax lower bounds. This gives rise to natural questions: what is the sharp sample complexity of Q-learning? Is Q-learning provably sub-optimal? This paper addresses these questions for the synchronous setting: (1) wh
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#34394;&#20551;&#27979;&#35797;&#30340;&#40065;&#26834;&#24615;&#23545;&#25239;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#34394;&#20551;&#27979;&#35797;&#26041;&#27861;&#25552;&#39640;&#20102;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#27979;&#35797;&#22330;&#26223;&#19979;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#24471;&#21040;&#20102;&#26174;&#33879;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2007.00691</link><description>&lt;p&gt;
&#22522;&#20110;&#34394;&#20551;&#27979;&#35797;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#40065;&#26834;&#24615;&#23545;&#25239;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Falsification-Based Robust Adversarial Reinforcement Learning. (arXiv:2007.00691v3 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2007.00691
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#34394;&#20551;&#27979;&#35797;&#30340;&#40065;&#26834;&#24615;&#23545;&#25239;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#34394;&#20551;&#27979;&#35797;&#26041;&#27861;&#25552;&#39640;&#20102;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#27979;&#35797;&#22330;&#26223;&#19979;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#24471;&#21040;&#20102;&#26174;&#33879;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#24050;&#32463;&#22312;&#21508;&#31181;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#65292;&#22914;&#26426;&#22120;&#20154;&#25511;&#21046;&#20219;&#21153;&#12290;&#30001;&#20110;&#31574;&#30053;&#23545;&#22521;&#35757;&#29615;&#22659;&#36827;&#34892;&#20102;&#36807;&#24230;&#25311;&#21512;&#65292;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#26080;&#27861;&#25512;&#24191;&#21040;&#23433;&#20840;&#20851;&#38190;&#30340;&#27979;&#35797;&#22330;&#26223;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#34394;&#20551;&#27979;&#35797;&#30340;&#40065;&#26834;&#24615;&#23545;&#25239;&#23398;&#20064; (FRARL) &#26694;&#26550;&#65292;&#36890;&#36807;&#34394;&#20551;&#27979;&#35797;&#26041;&#27861;&#22312;&#23545;&#25239;&#23398;&#20064;&#20013;&#25972;&#21512;&#26102;&#38388;&#36923;&#36753;&#65292;&#20174;&#32780;&#25552;&#39640;&#31574;&#30053;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#24418;&#24577;&#21644;&#20219;&#21153;&#30340;&#19977;&#20010;&#26426;&#22120;&#20154;&#31995;&#32479;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) has achieved enormous progress in solving various sequential decision-making problems, such as control tasks in robotics. Since policies are overfitted to training environments, RL methods have often failed to be generalized to safety-critical test scenarios. Robust adversarial RL (RARL) was previously proposed to train an adversarial network that applies disturbances to a system, which improves the robustness in test scenarios. However, an issue of neural network-based adversaries is that integrating system requirements without handcrafting sophisticated reward signals are difficult. Safety falsification methods allow one to find a set of initial conditions and an input sequence, such that the system violates a given property formulated in temporal logic. In this paper, we propose falsification-based RARL (FRARL): this is the first generic framework for integrating temporal logic falsification in adversarial learning to improve policy robustness. By applyin
&lt;/p&gt;</description></item></channel></rss>