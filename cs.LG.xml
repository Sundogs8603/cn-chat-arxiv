<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#35843;&#26597;&#30740;&#31350;&#32858;&#28966;&#20110;&#36229;&#32500;&#35745;&#31639;&#31995;&#32479;&#30340;&#36755;&#20837;&#21644;&#36229;&#32500;&#21521;&#37327;&#30340;&#29983;&#25104;&#65292;&#25506;&#35752;&#20102;&#21508;&#31181;&#26041;&#27861;&#30340;&#38480;&#21046;&#12289;&#25361;&#25112;&#21644;&#28508;&#22312;&#22909;&#22788;&#12290;</title><link>http://arxiv.org/abs/2308.00685</link><description>&lt;p&gt;
&#20174;&#36229;&#32500;&#21521;&#37327;&#20013;&#23398;&#20064;&#65306;&#20851;&#20110;&#36229;&#32500;&#32534;&#30721;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Learning from Hypervectors: A Survey on Hypervector Encoding. (arXiv:2308.00685v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00685
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#30740;&#31350;&#32858;&#28966;&#20110;&#36229;&#32500;&#35745;&#31639;&#31995;&#32479;&#30340;&#36755;&#20837;&#21644;&#36229;&#32500;&#21521;&#37327;&#30340;&#29983;&#25104;&#65292;&#25506;&#35752;&#20102;&#21508;&#31181;&#26041;&#27861;&#30340;&#38480;&#21046;&#12289;&#25361;&#25112;&#21644;&#28508;&#22312;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#32500;&#35745;&#31639;&#26159;&#19968;&#31181;&#27169;&#25311;&#22823;&#33041;&#32467;&#26500;&#65292;&#25552;&#20379;&#24378;&#22823;&#32780;&#39640;&#25928;&#30340;&#22788;&#29702;&#21644;&#23398;&#20064;&#27169;&#22411;&#30340;&#26032;&#20852;&#35745;&#31639;&#33539;&#24335;&#12290;&#22312;&#36229;&#32500;&#35745;&#31639;&#20013;&#65292;&#25968;&#25454;&#34987;&#32534;&#30721;&#20026;&#38271;&#21521;&#37327;&#65292;&#31216;&#20026;&#36229;&#32500;&#21521;&#37327;&#65292;&#36890;&#24120;&#38271;&#24230;&#20026;1K&#21040;10K&#12290;&#25991;&#29486;&#25552;&#20379;&#20102;&#20960;&#31181;&#32534;&#30721;&#25216;&#26415;&#26469;&#29983;&#25104;&#27491;&#20132;&#25110;&#30456;&#20851;&#30340;&#36229;&#32500;&#21521;&#37327;&#65292;&#36825;&#21462;&#20915;&#20110;&#39044;&#26399;&#30340;&#24212;&#29992;&#12290;&#29616;&#26377;&#25991;&#29486;&#35843;&#26597;&#36890;&#24120;&#38598;&#20013;&#22312;&#36229;&#32500;&#35745;&#31639;&#31995;&#32479;&#30340;&#25972;&#20307;&#26041;&#38754;&#65292;&#21253;&#25324;&#31995;&#32479;&#36755;&#20837;&#12289;&#20027;&#35201;&#35745;&#31639;&#21644;&#26368;&#32456;&#36755;&#20986;&#12290;&#28982;&#32780;&#65292;&#26412;&#30740;&#31350;&#37319;&#21462;&#20102;&#26356;&#20855;&#20307;&#30340;&#26041;&#27861;&#12290;&#23427;&#32858;&#28966;&#20110;&#36229;&#32500;&#35745;&#31639;&#31995;&#32479;&#30340;&#36755;&#20837;&#21644;&#36229;&#32500;&#21521;&#37327;&#30340;&#29983;&#25104;&#65292;&#30452;&#25509;&#24433;&#21709;&#36229;&#32500;&#32534;&#30721;&#36807;&#31243;&#12290;&#26412;&#35843;&#26597;&#27719;&#38598;&#20102;&#19981;&#21516;&#30740;&#31350;&#20013;&#20851;&#20110;&#36229;&#32500;&#21521;&#37327;&#29983;&#25104;&#30340;&#21508;&#31181;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#23427;&#20204;&#30340;&#38480;&#21046;&#12289;&#25361;&#25112;&#21644;&#28508;&#22312;&#30340;&#22909;&#22788;&#12290;&#36890;&#36807;&#23545;&#26412;&#35843;&#26597;&#30340;&#20840;&#38754;&#25506;&#32034;&#65292;&#35835;&#32773;&#23558;&#33719;&#24471;&#19968;&#20123;&#20851;&#20110;&#36229;&#32500;&#32534;&#30721;&#30340;&#37325;&#35201;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hyperdimensional computing (HDC) is an emerging computing paradigm that imitates the brain's structure to offer a powerful and efficient processing and learning model. In HDC, the data are encoded with long vectors, called hypervectors, typically with a length of 1K to 10K. The literature provides several encoding techniques to generate orthogonal or correlated hypervectors, depending on the intended application. The existing surveys in the literature often focus on the overall aspects of HDC systems, including system inputs, primary computations, and final outputs. However, this study takes a more specific approach. It zeroes in on the HDC system input and the generation of hypervectors, directly influencing the hypervector encoding process. This survey brings together various methods for hypervector generation from different studies and explores the limitations, challenges, and potential benefits they entail. Through a comprehensive exploration of this survey, readers will acquire a 
&lt;/p&gt;</description></item><item><title>CodeBPE&#30740;&#31350;&#20102;&#29992;&#20110;&#28304;&#20195;&#30721;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#20013;&#19981;&#21516;&#23376;&#26631;&#35760;&#21270;&#36873;&#39033;&#30340;&#24433;&#21709;&#65292;&#25214;&#20986;&#20102;&#26368;&#26377;&#25928;&#21644;&#38271;&#24230;&#39640;&#25928;&#30340;&#23376;&#26631;&#35760;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20943;&#23569;&#24179;&#22343;&#38271;&#24230;17%&#19988;&#19981;&#24433;&#21709;&#19979;&#28216;&#24615;&#33021;&#65292;&#21487;&#33021;&#25552;&#39640;&#36136;&#37327;0.5-2%&#12290;</title><link>http://arxiv.org/abs/2308.00683</link><description>&lt;p&gt;
CodeBPE: &#25506;&#32034;&#29992;&#20110;&#28304;&#20195;&#30721;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#30340;&#23376;&#26631;&#35760;&#21270;&#36873;&#39033;
&lt;/p&gt;
&lt;p&gt;
CodeBPE: Investigating Subtokenization Options for Large Language Model Pretraining on Source Code. (arXiv:2308.00683v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00683
&lt;/p&gt;
&lt;p&gt;
CodeBPE&#30740;&#31350;&#20102;&#29992;&#20110;&#28304;&#20195;&#30721;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#20013;&#19981;&#21516;&#23376;&#26631;&#35760;&#21270;&#36873;&#39033;&#30340;&#24433;&#21709;&#65292;&#25214;&#20986;&#20102;&#26368;&#26377;&#25928;&#21644;&#38271;&#24230;&#39640;&#25928;&#30340;&#23376;&#26631;&#35760;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20943;&#23569;&#24179;&#22343;&#38271;&#24230;17%&#19988;&#19981;&#24433;&#21709;&#19979;&#28216;&#24615;&#33021;&#65292;&#21487;&#33021;&#25552;&#39640;&#36136;&#37327;0.5-2%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#24191;&#27867;&#37319;&#29992;&#20102;&#38024;&#23545;&#28304;&#20195;&#30721;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#65292;&#25552;&#20986;&#20102;&#28304;&#20195;&#30721;&#29305;&#23450;&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#65292;&#24182;&#30740;&#31350;&#20102;&#19981;&#21516;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#22312;&#28304;&#20195;&#30721;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#21478;&#19968;&#20010;&#37325;&#35201;&#26041;&#38754;&#65292;&#21363;&#19981;&#21516;&#23376;&#26631;&#35760;&#21270;&#36873;&#39033;&#30340;&#24433;&#21709;&#65292;&#24182;&#26088;&#22312;&#30830;&#23450;&#26368;&#26377;&#25928;&#21644;&#38271;&#24230;&#39640;&#25928;&#30340;&#23376;&#26631;&#35760;&#21270;&#65292;&#32771;&#34385;&#21040;&#20195;&#30721;&#30340;&#29305;&#27530;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23376;&#26631;&#35760;&#21270;&#26041;&#27861;&#65292;&#24179;&#22343;&#38271;&#24230;&#20943;&#23569;&#20102;17%&#65292;&#19988;&#27809;&#26377;&#19979;&#28216;&#24615;&#33021;&#19979;&#38477;&#65292;&#24182;&#19988;&#34920;&#26126;&#31934;&#24515;&#36873;&#25321;&#30340;&#23376;&#26631;&#35760;&#21270;&#21487;&#33021;&#20250;&#25552;&#39640;&#36136;&#37327;0.5-2%&#65292;&#21487;&#33021;&#20250;&#30053;&#24494;&#22686;&#21152;&#38271;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works have widely adopted large language model pretraining for source code, suggested source code-specific pretraining objectives and investigated the applicability of various Transformer-based language model architectures for source code. This work investigates another important aspect of such models, namely the effect of different subtokenization options, and aims at identifying most effective and length-efficient subtokenizations, taking into account code specifics. We propose subtokenziation that reduces average length by 17% without downstream performance drop, and show that a carefully chosen subtokenization may improve quality by 0.5-2%, possibly with some length increase.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31163;&#25955;&#27169;&#25311;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#23558;Murski{i}&#30340;&#36890;&#29992;&#20195;&#25968;&#23450;&#29702;&#21644;Cybenko&#30340;&#36890;&#29992;&#36924;&#36817;&#32467;&#26524;&#32479;&#19968;&#36215;&#26469;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#24341;&#20837;&#20851;&#31995;&#32467;&#26500;&#30340;&#22810;&#24577;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#32463;&#20856;&#23398;&#20064;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2308.00677</link><description>&lt;p&gt;
&#31163;&#25955;&#31070;&#32463;&#32593;&#32476;&#21644;&#22810;&#24577;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Discrete neural nets and polymorphic learning. (arXiv:2308.00677v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00677
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31163;&#25955;&#27169;&#25311;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#23558;Murski{i}&#30340;&#36890;&#29992;&#20195;&#25968;&#23450;&#29702;&#21644;Cybenko&#30340;&#36890;&#29992;&#36924;&#36817;&#32467;&#26524;&#32479;&#19968;&#36215;&#26469;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#24341;&#20837;&#20851;&#31995;&#32467;&#26500;&#30340;&#22810;&#24577;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#32463;&#20856;&#23398;&#20064;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
20&#19990;&#32426;70&#24180;&#20195;Murski{i}&#25552;&#20986;&#30340;&#36890;&#29992;&#20195;&#25968;&#23450;&#29702;&#19982;20&#19990;&#32426;80&#24180;&#20195;Cybenko&#31561;&#20154;&#20851;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#36890;&#29992;&#36924;&#36817;&#32467;&#26524;&#26377;&#30528;&#24778;&#20154;&#30340;&#30456;&#20284;&#24615;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#32463;&#20856;&#31070;&#32463;&#32593;&#32476;&#30340;&#31163;&#25955;&#27169;&#25311;&#65292;&#23558;&#36825;&#20123;&#32467;&#26524;&#25918;&#22312;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#19979;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20851;&#31995;&#32467;&#26500;&#22810;&#24577;&#24615;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#20854;&#29992;&#20110;&#32463;&#20856;&#23398;&#20064;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Theorems from universal algebra such as that of Murski\u{i} from the 1970s have a striking similarity to universal approximation results for neural nets along the lines of Cybenko's from the 1980s. We consider here a discrete analogue of the classical notion of a neural net which places these results in a unified setting. We introduce a learning algorithm based on polymorphisms of relational structures and show how to use it for a classical learning task.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#20351;&#29992;&#24037;&#20855;&#25991;&#26723;&#20316;&#20026;&#25945;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20351;&#29992;&#26032;&#24037;&#20855;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#35777;&#26126;&#65292;&#20165;&#20351;&#29992;&#24037;&#20855;&#25991;&#26723;&#30340;&#38646;-shot&#25552;&#31034;&#36275;&#20197;&#23454;&#29616;&#27491;&#30830;&#30340;&#24037;&#20855;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.00675</link><description>&lt;p&gt;
&#24037;&#20855;&#25991;&#26723;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36827;&#34892;&#38646;-shot&#24037;&#20855;&#20351;&#29992;
&lt;/p&gt;
&lt;p&gt;
Tool Documentation Enables Zero-Shot Tool-Usage with Large Language Models. (arXiv:2308.00675v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00675
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#20351;&#29992;&#24037;&#20855;&#25991;&#26723;&#20316;&#20026;&#25945;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20351;&#29992;&#26032;&#24037;&#20855;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#35777;&#26126;&#65292;&#20165;&#20351;&#29992;&#24037;&#20855;&#25991;&#26723;&#30340;&#38646;-shot&#25552;&#31034;&#36275;&#20197;&#23454;&#29616;&#27491;&#30830;&#30340;&#24037;&#20855;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#36890;&#36807;&#25552;&#20379;&#19968;&#20123;&#24037;&#20855;&#20351;&#29992;&#30340;&#28436;&#31034;&#26469;&#25945;&#25480;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20351;&#29992;&#26032;&#24037;&#20855;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#28436;&#31034;&#24456;&#38590;&#33719;&#24471;&#65292;&#24182;&#19988;&#22914;&#26524;&#36873;&#25321;&#20102;&#38169;&#35823;&#30340;&#28436;&#31034;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#19981;&#33391;&#30340;&#20559;&#35265;&#20351;&#29992;&#12290;&#21363;&#20351;&#22312;&#32597;&#35265;&#30340;&#24773;&#20917;&#19979;&#65292;&#28436;&#31034;&#26159;readily available&#30340;&#65292;&#20063;&#27809;&#26377;&#21407;&#21017;&#24615;&#30340;&#36873;&#25321;&#21327;&#35758;&#26469;&#30830;&#23450;&#25552;&#20379;&#22810;&#23569;&#20010;&#21644;&#21738;&#20123;&#28436;&#31034;&#12290;&#38543;&#30528;&#20219;&#21153;&#21464;&#24471;&#26356;&#21152;&#22797;&#26434;&#65292;&#36873;&#25321;&#25628;&#32034;&#32452;&#21512;&#25968;&#30340;&#22686;&#38271;&#25104;&#20026;&#19981;&#21487;&#22788;&#29702;&#30340;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20379;&#20102;&#19968;&#31181;&#26367;&#20195;&#28436;&#31034;&#30340;&#26041;&#27861;&#65306;&#24037;&#20855;&#25991;&#26723;&#12290;&#25105;&#20204;&#20027;&#24352;&#20351;&#29992;&#24037;&#20855;&#25991;&#26723;&#26469;&#25551;&#36848;&#21508;&#20010;&#24037;&#20855;&#30340;&#20351;&#29992;&#65292;&#32780;&#19981;&#26159;&#28436;&#31034;&#12290;&#25105;&#20204;&#36890;&#36807;&#36328;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#24577;&#30340;6&#20010;&#20219;&#21153;&#19978;&#30340;&#19977;&#20010;&#20027;&#35201;&#23454;&#35777;&#21457;&#29616;&#25903;&#25345;&#25105;&#20204;&#30340;&#20027;&#24352;&#12290;&#39318;&#20808;&#65292;&#22312;&#29616;&#26377;&#30340;&#22522;&#20934;&#27979;&#35797;&#19978;&#65292;&#20165;&#20351;&#29992;&#24037;&#20855;&#25991;&#26723;&#30340;&#38646;-shot&#25552;&#31034;&#36275;&#20197;&#24341;&#20986;&#27491;&#30830;&#30340;&#24037;&#20855;&#20351;&#29992;&#65292;&#36798;&#21040;&#20102;few-shot&#25552;&#31034;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Today, large language models (LLMs) are taught to use new tools by providing a few demonstrations of the tool's usage. Unfortunately, demonstrations are hard to acquire, and can result in undesirable biased usage if the wrong demonstration is chosen. Even in the rare scenario that demonstrations are readily available, there is no principled selection protocol to determine how many and which ones to provide. As tasks grow more complex, the selection search grows combinatorially and invariably becomes intractable. Our work provides an alternative to demonstrations: tool documentation. We advocate the use of tool documentation, descriptions for the individual tool usage, over demonstrations. We substantiate our claim through three main empirical findings on 6 tasks across both vision and language modalities. First, on existing benchmarks, zero-shot prompts with only tool documentation are sufficient for eliciting proper tool usage, achieving performance on par with few-shot prompts. Secon
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36951;&#20256;&#32534;&#31243;&#20013;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#27169;&#22411;&#32676;&#20307;&#21644;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#26469;&#36873;&#25321;&#20449;&#24687;&#20016;&#23500;&#30340;&#35757;&#32451;&#25968;&#25454;&#28857;&#65292;&#24182;&#21457;&#29616;&#24494;&#20998;&#29109;&#26159;&#26368;&#22909;&#30340;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#23558;&#30456;&#20851;&#24615;&#20316;&#20026;&#22810;&#26679;&#24615;&#24230;&#37327;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#36873;&#25321;&#25968;&#25454;&#28857;&#36827;&#34892;&#35757;&#32451;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#24085;&#32047;&#25176;&#20248;&#21270;&#26041;&#27861;&#32508;&#21512;&#32771;&#34385;&#19981;&#30830;&#23450;&#24615;&#21644;&#22810;&#26679;&#24615;&#65292;&#23454;&#29616;&#20102;&#24179;&#34913;&#22320;&#36873;&#25321;&#35757;&#32451;&#25968;&#25454;&#28857;&#30340;&#30446;&#30340;&#12290;</title><link>http://arxiv.org/abs/2308.00672</link><description>&lt;p&gt;
&#36951;&#20256;&#32534;&#31243;&#20013;&#30340;&#20027;&#21160;&#23398;&#20064;&#65306;&#20026;&#31526;&#21495;&#22238;&#24402;&#25351;&#23548;&#39640;&#25928;&#25968;&#25454;&#25910;&#38598;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Active Learning in Genetic Programming: Guiding Efficient Data Collection for Symbolic Regression. (arXiv:2308.00672v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00672
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36951;&#20256;&#32534;&#31243;&#20013;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#27169;&#22411;&#32676;&#20307;&#21644;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#26469;&#36873;&#25321;&#20449;&#24687;&#20016;&#23500;&#30340;&#35757;&#32451;&#25968;&#25454;&#28857;&#65292;&#24182;&#21457;&#29616;&#24494;&#20998;&#29109;&#26159;&#26368;&#22909;&#30340;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#23558;&#30456;&#20851;&#24615;&#20316;&#20026;&#22810;&#26679;&#24615;&#24230;&#37327;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#36873;&#25321;&#25968;&#25454;&#28857;&#36827;&#34892;&#35757;&#32451;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#24085;&#32047;&#25176;&#20248;&#21270;&#26041;&#27861;&#32508;&#21512;&#32771;&#34385;&#19981;&#30830;&#23450;&#24615;&#21644;&#22810;&#26679;&#24615;&#65292;&#23454;&#29616;&#20102;&#24179;&#34913;&#22320;&#36873;&#25321;&#35757;&#32451;&#25968;&#25454;&#28857;&#30340;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36951;&#20256;&#32534;&#31243;&#20013;&#35745;&#31639;&#19981;&#30830;&#23450;&#24615;&#21644;&#22810;&#26679;&#24615;&#30340;&#21508;&#31181;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36890;&#36807;&#20351;&#29992;&#27169;&#22411;&#21512;&#38598;&#32467;&#21512;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#65292;&#21487;&#20197;&#21033;&#29992;&#36951;&#20256;&#32534;&#31243;&#20013;&#30340;&#27169;&#22411;&#32676;&#20307;&#36873;&#25321;&#20449;&#24687;&#20016;&#23500;&#30340;&#35757;&#32451;&#25968;&#25454;&#28857;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#20960;&#31181;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#65292;&#24182;&#21457;&#29616;&#24494;&#20998;&#29109;&#34920;&#29616;&#26368;&#22909;&#12290;&#25105;&#20204;&#36824;&#27604;&#36739;&#20102;&#20004;&#31181;&#25968;&#25454;&#22810;&#26679;&#24615;&#24230;&#37327;&#65292;&#24182;&#21457;&#29616;&#20316;&#20026;&#22810;&#26679;&#24615;&#24230;&#37327;&#30340;&#30456;&#20851;&#24615;&#27604;&#26368;&#23567;&#27431;&#27663;&#36317;&#31163;&#34920;&#29616;&#26356;&#22909;&#65292;&#23613;&#31649;&#30456;&#20851;&#24615;&#20855;&#26377;&#19968;&#20123;&#32570;&#28857;&#65292;&#19981;&#33021;&#22312;&#25152;&#26377;&#38382;&#39064;&#19978;&#20351;&#29992;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#24085;&#32047;&#25176;&#20248;&#21270;&#26041;&#27861;&#23558;&#19981;&#30830;&#23450;&#24615;&#21644;&#22810;&#26679;&#24615;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#24179;&#34913;&#22320;&#25351;&#23548;&#36873;&#25321;&#20449;&#24687;&#20016;&#23500;&#19988;&#29420;&#29305;&#30340;&#25968;&#25454;&#28857;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper examines various methods of computing uncertainty and diversity for active learning in genetic programming. We found that the model population in genetic programming can be exploited to select informative training data points by using a model ensemble combined with an uncertainty metric. We explored several uncertainty metrics and found that differential entropy performed the best. We also compared two data diversity metrics and found that correlation as a diversity metric performs better than minimum Euclidean distance, although there are some drawbacks that prevent correlation from being used on all problems. Finally, we combined uncertainty and diversity using a Pareto optimization approach to allow both to be considered in a balanced way to guide the selection of informative and unique data points for training.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24863;&#30693;&#28023;&#26862;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#20915;&#31574;&#31995;&#32479;&#20248;&#21270;&#20013;&#26799;&#24230;&#21453;&#39304;&#31232;&#32570;&#25110;&#26080;&#25928;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#32039;&#20945;&#30340;&#22810;&#23618;&#26550;&#26500;&#21644;&#35282;&#33394;&#27010;&#24565;&#65292;&#24182;&#21033;&#29992;&#24863;&#30693;&#28023;&#26862;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#23545;&#21442;&#25968;&#36827;&#34892;&#20248;&#21270;&#65292;&#20316;&#32773;&#23454;&#29616;&#20102;&#23545;&#22797;&#26434;&#20915;&#31574;&#31995;&#32479;&#30340;&#39640;&#25928;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2308.00629</link><description>&lt;p&gt;
Hessian-Aware Bayesian Optimization for Decision Making Systems - &#24863;&#30693;&#28023;&#26862;&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#20915;&#31574;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Hessian-Aware Bayesian Optimization for Decision Making Systems. (arXiv:2308.00629v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00629
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24863;&#30693;&#28023;&#26862;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#20915;&#31574;&#31995;&#32479;&#20248;&#21270;&#20013;&#26799;&#24230;&#21453;&#39304;&#31232;&#32570;&#25110;&#26080;&#25928;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#32039;&#20945;&#30340;&#22810;&#23618;&#26550;&#26500;&#21644;&#35282;&#33394;&#27010;&#24565;&#65292;&#24182;&#21033;&#29992;&#24863;&#30693;&#28023;&#26862;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#23545;&#21442;&#25968;&#36827;&#34892;&#20248;&#21270;&#65292;&#20316;&#32773;&#23454;&#29616;&#20102;&#23545;&#22797;&#26434;&#20915;&#31574;&#31995;&#32479;&#30340;&#39640;&#25928;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#20248;&#21270;&#20915;&#31574;&#31995;&#32479;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#26799;&#24230;&#26041;&#27861;&#65292;&#38656;&#35201;&#20174;&#29615;&#22659;&#20013;&#33719;&#21462;&#26377;&#20449;&#24687;&#37327;&#30340;&#21453;&#39304;&#12290;&#28982;&#32780;&#65292;&#24403;&#21453;&#39304;&#31232;&#32570;&#25110;&#32773;&#26080;&#20449;&#24687;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#24615;&#33021;&#36739;&#24046;&#12290;&#36125;&#21494;&#26031;&#20248;&#21270;&#31561;&#26080;&#23548;&#25968;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;&#23545;&#26799;&#24230;&#21453;&#39304;&#36136;&#37327;&#30340;&#20381;&#36182;&#65292;&#20294;&#22312;&#22797;&#26434;&#20915;&#31574;&#31995;&#32479;&#30340;&#39640;&#32500;&#29615;&#22659;&#20013;&#24448;&#24448;&#38590;&#20197;&#25193;&#23637;&#12290;&#22914;&#26524;&#31995;&#32479;&#38656;&#35201;&#22810;&#20010;&#21442;&#19982;&#32773;&#20043;&#38388;&#30340;&#20114;&#21160;&#26469;&#23454;&#29616;&#20849;&#21516;&#30446;&#26631;&#65292;&#36825;&#20010;&#38382;&#39064;&#23601;&#21152;&#21095;&#20102;&#12290;&#20026;&#20102;&#35299;&#20915;&#32500;&#24230;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32039;&#20945;&#30340;&#22810;&#23618;&#26550;&#26500;&#65292;&#36890;&#36807;&#35282;&#33394;&#30340;&#27010;&#24565;&#26469;&#24314;&#27169;&#21442;&#19982;&#32773;&#20043;&#38388;&#30340;&#21160;&#24577;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#24863;&#30693;&#28023;&#26862;&#36125;&#21494;&#26031;&#20248;&#21270;&#26469;&#39640;&#25928;&#22320;&#20248;&#21270;&#30001;&#22823;&#37327;&#21442;&#25968;&#21442;&#25968;&#21270;&#30340;&#22810;&#23618;&#26550;&#26500;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;(HA-GP-UCB)&#22312;&#25928;&#26524;&#19978;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many approaches for optimizing decision making systems rely on gradient based methods requiring informative feedback from the environment. However, in the case where such feedback is sparse or uninformative, such approaches may result in poor performance. Derivative-free approaches such as Bayesian Optimization mitigate the dependency on the quality of gradient feedback, but are known to scale poorly in the high-dimension setting of complex decision making systems. This problem is exacerbated if the system requires interactions between several actors cooperating to accomplish a shared goal. To address the dimensionality challenge, we propose a compact multi-layered architecture modeling the dynamics of actor interactions through the concept of role. Additionally, we introduce Hessian-aware Bayesian Optimization to efficiently optimize the multi-layered architecture parameterized by a large number of parameters. Experimental results demonstrate that our method (HA-GP-UCB) works effectiv
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23460;&#22806;&#22810;&#27169;&#24577;&#22810;&#35270;&#35282;&#22810;&#20154;&#31867;&#23039;&#21183;&#25968;&#25454;&#24211;Human-M3&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#25968;&#25454;&#36755;&#20837;&#30340;&#31639;&#27861;&#26469;&#29983;&#25104;&#20934;&#30830;&#30340;&#20154;&#20307;&#23039;&#21183;&#12290;&#36825;&#20010;&#25968;&#25454;&#24211;&#35299;&#20915;&#20102;&#29616;&#26377;&#25968;&#25454;&#38598;&#30340;&#19981;&#36275;&#65292;&#25552;&#20379;&#20102;&#26356;&#22810;&#30340;&#25968;&#25454;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.00628</link><description>&lt;p&gt;
&#20154;&#31867;-M3&#65306;&#19968;&#20010;&#29992;&#20110;&#23460;&#22806;&#22330;&#26223;&#20013;&#30340;3D&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#30340;&#22810;&#35270;&#35282;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Human-M3: A Multi-view Multi-modal Dataset for 3D Human Pose Estimation in Outdoor Scenes. (arXiv:2308.00628v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00628
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23460;&#22806;&#22810;&#27169;&#24577;&#22810;&#35270;&#35282;&#22810;&#20154;&#31867;&#23039;&#21183;&#25968;&#25454;&#24211;Human-M3&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#25968;&#25454;&#36755;&#20837;&#30340;&#31639;&#27861;&#26469;&#29983;&#25104;&#20934;&#30830;&#30340;&#20154;&#20307;&#23039;&#21183;&#12290;&#36825;&#20010;&#25968;&#25454;&#24211;&#35299;&#20915;&#20102;&#29616;&#26377;&#25968;&#25454;&#38598;&#30340;&#19981;&#36275;&#65292;&#25552;&#20379;&#20102;&#26356;&#22810;&#30340;&#25968;&#25454;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#20110;&#23460;&#22806;&#29615;&#22659;&#20013;&#30340;3D&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#23460;&#22806;&#22330;&#26223;3D&#20154;&#20307;&#23039;&#21183;&#25968;&#25454;&#38598;&#32570;&#20047;&#22810;&#26679;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#20027;&#35201;&#21482;&#20351;&#29992;&#19968;&#31181;&#27169;&#24577;&#65288;RGB&#22270;&#20687;&#25110;&#28857;&#20113;&#65289;&#65292;&#24182;&#19988;&#22330;&#26223;&#20013;&#36890;&#24120;&#21482;&#26377;&#19968;&#20010;&#20154;&#12290;&#25968;&#25454;&#38598;&#22522;&#30784;&#30340;&#26377;&#38480;&#33539;&#22260;&#20005;&#37325;&#38459;&#30861;&#20102;&#21487;&#29992;&#25968;&#25454;&#30340;&#21464;&#21270;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Human-M3&#65292;&#36825;&#26159;&#19968;&#20010;&#23460;&#22806;&#22810;&#27169;&#24577;&#22810;&#35270;&#35282;&#22810;&#20154;&#31867;&#23039;&#21183;&#25968;&#25454;&#24211;&#65292;&#20854;&#20013;&#21253;&#25324;&#23460;&#22806;&#22330;&#26223;&#30340;&#22810;&#35270;&#35282;RGB&#35270;&#39057;&#21644;&#30456;&#24212;&#30340;&#28857;&#20113;&#25968;&#25454;&#12290;&#20026;&#20102;&#33719;&#24471;&#20934;&#30830;&#30340;&#20154;&#20307;&#23039;&#21183;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#25968;&#25454;&#36755;&#20837;&#30340;&#31639;&#27861;&#26469;&#29983;&#25104;&#22320;&#38754;&#30495;&#20540;&#26631;&#27880;&#12290;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#20102;&#40065;&#26834;&#30340;&#28857;&#20113;&#26816;&#27979;&#21644;&#36319;&#36394;&#65292;&#35299;&#20915;&#20102;&#20043;&#21069;&#23460;&#22806;&#22330;&#26223;&#20013;&#22810;&#20010;&#20154;&#30340;&#22810;&#35270;&#35282;RGB&#35270;&#39057;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#19981;&#20934;&#30830;&#20154;&#20307;&#23450;&#20301;&#21644;&#21305;&#37197;&#27169;&#31946;&#38382;&#39064;&#65292;&#29983;&#25104;&#20102;&#30456;&#20851;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
3D human pose estimation in outdoor environments has garnered increasing attention recently. However, prevalent 3D human pose datasets pertaining to outdoor scenes lack diversity, as they predominantly utilize only one type of modality (RGB image or pointcloud), and often feature only one individual within each scene. This limited scope of dataset infrastructure considerably hinders the variability of available data. In this article, we propose Human-M3, an outdoor multi-modal multi-view multi-person human pose database which includes not only multi-view RGB videos of outdoor scenes but also corresponding pointclouds. In order to obtain accurate human poses, we propose an algorithm based on multi-modal data input to generate ground truth annotation. This benefits from robust pointcloud detection and tracking, which solves the problem of inaccurate human localization and matching ambiguity that may exist in previous multi-view RGB videos in outdoor multi-person scenes, and generates rel
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#23558;&#26412;&#20307;&#21644;&#35821;&#20041;&#30693;&#35782;&#21453;&#26144;&#21040;&#22270;&#20687;&#20998;&#31867;&#22120;&#20013;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#20449;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.00607</link><description>&lt;p&gt;
&#36229;&#36234;One-Hot-Encoding: &#27880;&#20837;&#35821;&#20041;&#39537;&#21160;&#22270;&#20687;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Beyond One-Hot-Encoding: Injecting Semantics to Drive Image Classifiers. (arXiv:2308.00607v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00607
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#23558;&#26412;&#20307;&#21644;&#35821;&#20041;&#30693;&#35782;&#21453;&#26144;&#21040;&#22270;&#20687;&#20998;&#31867;&#22120;&#20013;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#20013;&#21253;&#21547;&#20102;&#19982;&#29616;&#23454;&#19990;&#30028;&#26412;&#20307;&#35770;&#30456;&#20851;&#30340;&#35821;&#20041;&#20449;&#24687;&#65306;&#29399;&#30340;&#21697;&#31181;&#20855;&#26377;&#21754;&#20083;&#21160;&#29289;&#30340;&#30456;&#20284;&#24615;&#65292;&#39135;&#29289;&#30340;&#22270;&#29255;&#36890;&#24120;&#22312;&#23478;&#24237;&#29615;&#22659;&#20013;&#25551;&#36848;&#65292;&#31561;&#31561;&#12290;&#28982;&#32780;&#65292;&#22312;&#23545;&#22270;&#20687;&#20998;&#31867;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#26102;&#65292;&#23545;&#35937;&#31867;&#20043;&#38388;&#30340;&#30456;&#23545;&#30456;&#20284;&#24615;&#24120;&#24120;&#19982;One-Hot-Encoding&#26631;&#31614;&#37197;&#23545;&#12290;&#26681;&#25454;&#36825;&#31181;&#36923;&#36753;&#65292;&#22914;&#26524;&#19968;&#20010;&#22270;&#20687;&#34987;&#26631;&#35760;&#20026;&#8220;&#21242;&#23376;&#8221;&#65292;&#37027;&#20040;&#8220;&#33590;&#21242;&#8221;&#21644;&#8220;&#40104;&#40060;&#8221;&#22312;&#35757;&#32451;&#25439;&#22833;&#26041;&#38754;&#26159;&#21516;&#26679;&#38169;&#35823;&#30340;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#25972;&#21512;&#21453;&#26144;&#26412;&#20307;&#21644;&#35821;&#20041;&#30693;&#35782;&#30340;&#39069;&#22806;&#30446;&#26631;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#20449;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#21487;&#20197;&#26681;&#25454;&#19982;&#20998;&#31867;&#26631;&#31614;&#30456;&#20851;&#30340;&#20219;&#20309;&#31867;&#22411;&#30340;&#35821;&#20041;&#20449;&#24687;&#23548;&#20986;&#39069;&#22806;&#30340;&#25439;&#22833;&#39033;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#26412;&#20307;&#21644;&#35789;&#23884;&#20837;&#65292;&#24182;&#35752;&#35770;&#20102;&#30001;&#27492;&#24471;&#21040;&#30340;&#20449;&#24687;&#22914;&#20309;&#39537;&#21160;&#21463;&#30417;&#30563;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;&#20854;&#27425;&#65292;
&lt;/p&gt;
&lt;p&gt;
Images are loaded with semantic information that pertains to real-world ontologies: dog breeds share mammalian similarities, food pictures are often depicted in domestic environments, and so on. However, when training machine learning models for image classification, the relative similarities amongst object classes are commonly paired with one-hot-encoded labels. According to this logic, if an image is labelled as 'spoon', then 'tea-spoon' and 'shark' are equally wrong in terms of training loss. To overcome this limitation, we explore the integration of additional goals that reflect ontological and semantic knowledge, improving model interpretability and trustworthiness. We suggest a generic approach that allows to derive an additional loss term starting from any kind of semantic information about the classification label. First, we show how to apply our approach to ontologies and word embeddings, and discuss how the resulting information can drive a supervised learning process. Second
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#37327;&#23376;&#26680;&#30340;&#25903;&#25345;&#21521;&#37327;&#22238;&#24402;&#26041;&#27861;&#34987;&#24212;&#29992;&#20110;&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#20013;&#65292;&#38024;&#23545;NISQ&#35774;&#22791;&#30340;&#38480;&#21046;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#12289;&#28789;&#27963;&#24615;&#21644;&#20860;&#23481;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.00583</link><description>&lt;p&gt;
&#20351;&#29992;&#37327;&#23376;&#26680;&#30340;&#25903;&#25345;&#21521;&#37327;&#22238;&#24402;&#36827;&#34892;&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Semisupervised Anomaly Detection using Support Vector Regression with Quantum Kernel. (arXiv:2308.00583v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00583
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#37327;&#23376;&#26680;&#30340;&#25903;&#25345;&#21521;&#37327;&#22238;&#24402;&#26041;&#27861;&#34987;&#24212;&#29992;&#20110;&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#20013;&#65292;&#38024;&#23545;NISQ&#35774;&#22791;&#30340;&#38480;&#21046;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#12289;&#28789;&#27963;&#24615;&#21644;&#20860;&#23481;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#65288;AD&#65289;&#28041;&#21450;&#35782;&#21035;&#19982;&#20854;&#20182;&#25968;&#25454;&#26377;&#25152;&#19981;&#21516;&#30340;&#35266;&#27979;&#25110;&#20107;&#20214;&#12290;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#33258;&#21160;&#21270;&#36825;&#20010;&#36807;&#31243;&#20013;&#26174;&#31034;&#20986;&#25104;&#21151;&#65292;&#36890;&#36807;&#26816;&#27979;&#22823;&#35268;&#27169;&#25968;&#25454;&#20013;&#38544;&#34255;&#30340;&#27169;&#24335;&#21644;&#20559;&#24046;&#12290;&#37327;&#23376;&#35745;&#31639;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#28508;&#21147;&#24050;&#24471;&#21040;&#24191;&#27867;&#35748;&#21487;&#65292;&#23548;&#33268;&#20102;&#22823;&#37327;&#30740;&#31350;&#24037;&#20316;&#20197;&#24320;&#21457;&#36866;&#29992;&#20110;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#65288;QML&#65289;&#31639;&#27861;&#12290;&#29305;&#21035;&#26159;&#23545;&#36817;&#26399;NISQ&#35774;&#22791;&#30340;QML&#31639;&#27861;&#30340;&#25628;&#32034;&#27491;&#22312;&#20840;&#21147;&#36827;&#34892;&#12290;&#28982;&#32780;&#65292;NISQ&#35774;&#22791;&#30001;&#20110;&#20854;&#26377;&#38480;&#30340;&#37327;&#23376;&#27604;&#29305;&#30456;&#24178;&#26102;&#38388;&#12289;&#36739;&#23569;&#30340;&#37327;&#23376;&#27604;&#29305;&#25968;&#37327;&#21644;&#39640;&#35823;&#24046;&#29575;&#65292;&#23384;&#22312;&#39069;&#22806;&#30340;&#25361;&#25112;&#12290;&#22522;&#20110;&#37327;&#23376;&#26680;&#20272;&#35745;&#30340;&#26680;&#26041;&#27861;&#24050;&#25104;&#20026;NISQ&#35774;&#22791;&#19978;&#36827;&#34892;QML&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#12289;&#28789;&#27963;&#24615;&#21644;&#19982;NISQ&#32422;&#26463;&#30340;&#20860;&#23481;&#24615;&#12290;&#29305;&#21035;&#26159;&#21033;&#29992;&#37327;&#23376;&#26680;&#20272;&#35745;&#30340;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#22312;&#21508;&#31181;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection (AD) involves identifying observations or events that deviate in some way from the rest of the data. Machine learning techniques have shown success in automating this process by detecting hidden patterns and deviations in large-scale data. The potential of quantum computing for machine learning has been widely recognized, leading to extensive research efforts to develop suitable quantum machine learning (QML) algorithms. In particular, the search for QML algorithms for near-term NISQ devices is in full swing. However, NISQ devices pose additional challenges due to their limited qubit coherence times, low number of qubits, and high error rates. Kernel methods based on quantum kernel estimation have emerged as a promising approach to QML on NISQ devices, offering theoretical guarantees, versatility, and compatibility with NISQ constraints. Especially support vector machines (SVM) utilizing quantum kernel estimation have shown success in various supervised learning tasks
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FlexPredict&#30340;&#38543;&#26426;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#27169;&#22411;&#20013;&#21152;&#20837;&#20301;&#32622;&#19981;&#30830;&#23450;&#24615;&#65292;&#20197;&#39044;&#27979;&#25513;&#30422;&#30340;&#26631;&#35760;&#20301;&#32622;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#25513;&#30422;&#22270;&#20687;&#24314;&#27169;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.00566</link><description>&lt;p&gt;
&#22312;&#38543;&#26426;&#20301;&#32622;&#39044;&#27979;&#25513;&#30422;&#30340;&#26631;&#35760;&#25913;&#21892;&#20102;&#25513;&#30422;&#22270;&#20687;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Predicting masked tokens in stochastic locations improves masked image modeling. (arXiv:2308.00566v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00566
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FlexPredict&#30340;&#38543;&#26426;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#27169;&#22411;&#20013;&#21152;&#20837;&#20301;&#32622;&#19981;&#30830;&#23450;&#24615;&#65292;&#20197;&#39044;&#27979;&#25513;&#30422;&#30340;&#26631;&#35760;&#20301;&#32622;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#25513;&#30422;&#22270;&#20687;&#24314;&#27169;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#26159;&#28145;&#24230;&#23398;&#20064;&#20013;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#33539;&#24335;&#65292;&#36890;&#36807;&#26500;&#24314;&#38656;&#35201;&#23398;&#20064;&#26377;&#29992;&#34920;&#31034;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#21487;&#20197;&#20174;&#26080;&#26631;&#31614;&#25968;&#25454;&#20013;&#36827;&#34892;&#23398;&#20064;&#12290;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#20027;&#35201;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#26159;&#25513;&#30422;&#35821;&#35328;&#24314;&#27169;&#65288;MLM&#65289;&#65292;&#32780;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#23384;&#22312;&#30456;&#24212;&#30340;&#25513;&#30422;&#22270;&#20687;&#24314;&#27169;&#65288;MIM&#65289;&#12290;&#28982;&#32780;&#65292;MIM&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#22312;&#20934;&#30830;&#20301;&#32622;&#19978;&#39044;&#27979;&#35821;&#20041;&#20869;&#23481;&#12290;&#20363;&#22914;&#65292;&#32473;&#23450;&#19968;&#24352;&#19981;&#23436;&#25972;&#30340;&#29399;&#30340;&#22270;&#29255;&#65292;&#25105;&#20204;&#21487;&#20197;&#29468;&#27979;&#26377;&#19968;&#20010;&#23614;&#24052;&#65292;&#20294;&#25105;&#20204;&#26080;&#27861;&#30830;&#23450;&#23427;&#30340;&#30830;&#20999;&#20301;&#32622;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FlexPredict&#65292;&#36825;&#26159;&#19968;&#20010;&#32771;&#34385;&#20301;&#32622;&#19981;&#30830;&#23450;&#24615;&#30340;&#38543;&#26426;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#27169;&#22411;&#26465;&#20214;&#21270;&#21040;&#38543;&#26426;&#25513;&#30422;&#30340;&#26631;&#35760;&#20301;&#32622;&#19978;&#65292;&#24341;&#23548;&#27169;&#22411;&#23398;&#20064;&#26356;&#21152;&#40065;&#26834;&#23545;&#20301;&#32622;&#19981;&#30830;&#23450;&#24615;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25913;&#36827;&#20102;&#22810;&#20010;&#20219;&#21153;&#30340;&#19979;&#28216;&#24615;&#33021;&#65292;&#20363;&#22914;&#19982;MIM&#22522;&#20934;&#30456;&#27604;&#65292;FlexPredict&#22312;&#19968;&#31995;&#21015;&#20219;&#21153;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning is a promising paradigm in deep learning that enables learning from unlabeled data by constructing pretext tasks that require learning useful representations. In natural language processing, the dominant pretext task has been masked language modeling (MLM), while in computer vision there exists an equivalent called Masked Image Modeling (MIM). However, MIM is challenging because it requires predicting semantic content in accurate locations. E.g, given an incomplete picture of a dog, we can guess that there is a tail, but we cannot determine its exact location. In this work, we propose FlexPredict, a stochastic model that addresses this challenge by incorporating location uncertainty into the model. Specifically, we condition the model on stochastic masked token positions to guide the model toward learning features that are more robust to location uncertainties. Our approach improves downstream performance on a range of tasks, e.g, compared to MIM baselines, Fle
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#27979;&#35797;&#26102;&#30340;&#23545;&#25239;&#25915;&#20987;&#23545;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#30830;&#23450;&#20102;&#22312;&#20445;&#25345;&#39044;&#27979;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#36798;&#21040;&#30340;&#26368;&#20339;&#40065;&#26834;&#24615;&#27700;&#24179;&#12290;&#35813;&#30740;&#31350;&#25581;&#31034;&#20102;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24182;&#22312;&#19981;&#25439;&#23475;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#25214;&#21040;&#20102;&#40065;&#26834;&#24615;&#30340;&#23454;&#29616;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.00556</link><description>&lt;p&gt;
&#22362;&#24378;&#30340;&#32447;&#24615;&#22238;&#24402;&#65306;&#30456;&#21464;&#21644;&#23545;&#19968;&#33324;&#33539;&#25968;&#30340;&#31934;&#30830;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Robust Linear Regression: Phase-Transitions and Precise Tradeoffs for General Norms. (arXiv:2308.00556v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00556
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#27979;&#35797;&#26102;&#30340;&#23545;&#25239;&#25915;&#20987;&#23545;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#30830;&#23450;&#20102;&#22312;&#20445;&#25345;&#39044;&#27979;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#36798;&#21040;&#30340;&#26368;&#20339;&#40065;&#26834;&#24615;&#27700;&#24179;&#12290;&#35813;&#30740;&#31350;&#25581;&#31034;&#20102;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24182;&#22312;&#19981;&#25439;&#23475;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#25214;&#21040;&#20102;&#40065;&#26834;&#24615;&#30340;&#23454;&#29616;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#27979;&#35797;&#26102;&#23545;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;&#23545;&#20854;&#24433;&#21709;&#65292;&#24182;&#30830;&#23450;&#20102;&#20219;&#20309;&#27169;&#22411;&#22312;&#20445;&#25345;&#32473;&#23450;&#27700;&#24179;&#30340;&#39044;&#27979;&#24615;&#33021;&#65288;&#20934;&#30830;&#24230;&#65289;&#30340;&#21516;&#26102;&#21487;&#20197;&#36798;&#21040;&#30340;&#26368;&#20339;&#40065;&#26834;&#24615;&#27700;&#24179;&#12290;&#36890;&#36807;&#23450;&#37327;&#20272;&#35745;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#22522;&#26412;&#26435;&#34913;&#12290;&#25105;&#20204;&#33719;&#24471;&#20102;&#19968;&#20010;&#26126;&#30830;&#30340;&#25551;&#36848;&#65292;&#21306;&#20998;&#20102;&#22312;&#19981;&#25439;&#23475;&#26631;&#20934;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#23454;&#29616;&#40065;&#26834;&#24615;&#30340;&#24773;&#20917;&#19982;&#19981;&#21487;&#36991;&#20813;&#22320;&#38656;&#35201;&#26435;&#34913;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#22312;&#22810;&#31181;&#35774;&#32622;&#19979;&#36890;&#36807;&#31616;&#21333;&#30340;&#23454;&#39564;&#24471;&#21040;&#20102;&#32463;&#39564;&#35777;&#23454;&#12290;&#36825;&#39033;&#24037;&#20316;&#36866;&#29992;&#20110;&#29305;&#24449;&#21327;&#26041;&#24046;&#30697;&#38453;&#21644;&#20219;&#20309;&#24615;&#36136;&#30340;&#25915;&#20987;&#33539;&#25968;&#65292;&#24182;&#36229;&#36234;&#20102;&#20043;&#21069;&#22312;&#35813;&#39046;&#22495;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate the impact of test-time adversarial attacks on linear regression models and determine the optimal level of robustness that any model can reach while maintaining a given level of standard predictive performance (accuracy). Through quantitative estimates, we uncover fundamental tradeoffs between adversarial robustness and accuracy in different regimes. We obtain a precise characterization which distinguishes between regimes where robustness is achievable without hurting standard accuracy and regimes where a tradeoff might be unavoidable. Our findings are empirically confirmed with simple experiments that represent a variety of settings. This work applies to feature covariance matrices and attack norms of any nature, and extends beyond previous works in this area.
&lt;/p&gt;</description></item><item><title>&#22312;&#23454;&#20363;&#32423;&#29305;&#24449;&#36873;&#25321;&#21644;&#25490;&#24207;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Copula&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#32771;&#34385;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#25429;&#25417;&#26377;&#24847;&#20041;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.00549</link><description>&lt;p&gt;
Copula&#29992;&#20110;&#23454;&#20363;&#32423;&#29305;&#24449;&#36873;&#25321;&#21644;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
Copula for Instance-wise Feature Selection and Ranking. (arXiv:2308.00549v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00549
&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#20363;&#32423;&#29305;&#24449;&#36873;&#25321;&#21644;&#25490;&#24207;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Copula&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#32771;&#34385;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#25429;&#25417;&#26377;&#24847;&#20041;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31070;&#32463;&#32593;&#32476;&#30340;&#32972;&#26223;&#19979;&#65292;&#23454;&#20363;&#32423;&#29305;&#24449;&#36873;&#25321;&#21644;&#25490;&#24207;&#26041;&#27861;&#21487;&#20197;&#20026;&#27599;&#20010;&#26679;&#26412;&#23454;&#29616;&#33391;&#22909;&#30340;&#29305;&#24449;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#20551;&#35774;&#29305;&#24449;&#23376;&#38598;&#29420;&#31435;&#65292;&#23545;&#20110;&#32771;&#34385;&#29305;&#24449;&#20043;&#38388;&#30340;&#20381;&#36182;&#24615;&#23384;&#22312;&#32570;&#38519;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#39640;&#26031;copula&#65292;&#19968;&#31181;&#25429;&#25417;&#21464;&#37327;&#20043;&#38388;&#30456;&#20851;&#24615;&#30340;&#24378;&#22823;&#25968;&#23398;&#25216;&#26415;&#65292;&#26080;&#38656;&#36827;&#34892;&#39069;&#22806;&#30340;&#26356;&#25913;&#23601;&#21487;&#20197;&#23558;&#20854;&#32435;&#20837;&#24403;&#21069;&#30340;&#29305;&#24449;&#36873;&#25321;&#26694;&#26550;&#20013;&#12290;&#36890;&#36807;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#22312;&#24615;&#33021;&#27604;&#36739;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#25429;&#25417;&#26377;&#24847;&#20041;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instance-wise feature selection and ranking methods can achieve a good selection of task-friendly features for each sample in the context of neural networks. However, existing approaches that assume feature subsets to be independent are imperfect when considering the dependency between features. To address this limitation, we propose to incorporate the Gaussian copula, a powerful mathematical technique for capturing correlations between variables, into the current feature selection framework with no additional changes needed. Experimental results on both synthetic and real datasets, in terms of performance comparison and interpretability, demonstrate that our method is capable of capturing meaningful correlations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#31215;&#26497;&#20581;&#24247;&#32769;&#40836;&#21270;&#24212;&#29992;&#31243;&#24207;&#30340;&#26089;&#26399;&#36864;&#23398;&#12290;&#36890;&#36807;&#20351;&#29992;&#21160;&#24577;&#21644;&#38745;&#24577;&#29305;&#24449;&#26500;&#24314;&#20998;&#31867;&#27169;&#22411;&#20197;&#21450;&#37319;&#29992;&#36807;&#37319;&#26679;&#26041;&#27861;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#39640;&#36136;&#37327;&#30340;&#31896;&#38468;&#24230;&#39044;&#27979;&#65292;&#24182;&#22312;&#31185;&#23398;&#25361;&#25112;&#36187;&#20013;&#33719;&#24471;&#20102;&#31532;&#19968;&#21517;&#12290;</title><link>http://arxiv.org/abs/2308.00539</link><description>&lt;p&gt;
&#39044;&#27979;&#19968;&#20010;&#31215;&#26497;&#20581;&#24247;&#32769;&#40836;&#21270;&#24212;&#29992;&#31243;&#24207;&#30340;&#26089;&#26399;&#36864;&#23398;
&lt;/p&gt;
&lt;p&gt;
Predicting Early Dropouts of an Active and Healthy Ageing App. (arXiv:2308.00539v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00539
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#31215;&#26497;&#20581;&#24247;&#32769;&#40836;&#21270;&#24212;&#29992;&#31243;&#24207;&#30340;&#26089;&#26399;&#36864;&#23398;&#12290;&#36890;&#36807;&#20351;&#29992;&#21160;&#24577;&#21644;&#38745;&#24577;&#29305;&#24449;&#26500;&#24314;&#20998;&#31867;&#27169;&#22411;&#20197;&#21450;&#37319;&#29992;&#36807;&#37319;&#26679;&#26041;&#27861;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#39640;&#36136;&#37327;&#30340;&#31896;&#38468;&#24230;&#39044;&#27979;&#65292;&#24182;&#22312;&#31185;&#23398;&#25361;&#25112;&#36187;&#20013;&#33719;&#24471;&#20102;&#31532;&#19968;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#31215;&#26497;&#20581;&#24247;&#32769;&#40836;&#21270;&#24212;&#29992;&#31243;&#24207;&#30340;&#26089;&#26399;&#36864;&#23398;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#24050;&#25552;&#20132;&#32473;2022&#24180;IFMBE&#31185;&#23398;&#25361;&#25112;&#36187;&#65292;&#26159;IUPESM WC 2022&#30340;&#19968;&#37096;&#20998;&#12290;&#25105;&#20204;&#22788;&#29702;&#20102;&#32473;&#23450;&#30340;&#25968;&#25454;&#24211;&#24182;&#29983;&#25104;&#20102;&#19971;&#20010;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20351;&#29992;&#39044;&#22788;&#29702;&#25216;&#26415;&#26500;&#24314;&#20998;&#31867;&#27169;&#22411;&#65292;&#20351;&#29992;&#21160;&#24577;&#21644;&#38745;&#24577;&#29305;&#24449;&#39044;&#27979;&#29992;&#25143;&#30340;&#31896;&#38468;&#24230;&#12290;&#25105;&#20204;&#25552;&#20132;&#20102;11&#27425;&#23448;&#26041;&#36816;&#34892;&#65292;&#32467;&#26524;&#26174;&#31034;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#31896;&#38468;&#24230;&#39044;&#27979;&#12290;&#26681;&#25454;&#32467;&#26524;&#65292;&#21160;&#24577;&#29305;&#24449;&#23545;&#27169;&#22411;&#30340;&#20998;&#31867;&#24615;&#33021;&#26377;&#31215;&#26497;&#24433;&#21709;&#12290;&#30001;&#20110;&#25968;&#25454;&#38598;&#30340;&#19981;&#24179;&#34913;&#24615;&#36136;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#36807;&#37319;&#26679;&#26041;&#27861;&#65292;&#22914;SMOTE&#21644;ADASYN&#65292;&#20197;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;&#36807;&#37319;&#26679;&#26041;&#27861;&#20351;&#32467;&#26524;&#33719;&#24471;&#20102;10%&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;2022&#24180;IFMBE&#31185;&#23398;&#25361;&#25112;&#36187;&#20013;&#33719;&#24471;&#20102;&#31532;&#19968;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we present a machine learning approach for predicting early dropouts of an active and healthy ageing app. The presented algorithms have been submitted to the IFMBE Scientific Challenge 2022, part of IUPESM WC 2022. We have processed the given database and generated seven datasets. We used pre-processing techniques to construct classification models that predict the adherence of users using dynamic and static features. We submitted 11 official runs and our results show that machine learning algorithms can provide high-quality adherence predictions. Based on the results, the dynamic features positively influence a model's classification performance. Due to the imbalanced nature of the dataset, we employed oversampling methods such as SMOTE and ADASYN to improve the classification performance. The oversampling approaches led to a remarkable improvement of 10\%. Our methods won first place in the IFMBE Scientific Challenge 2022.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#23884;&#20837;&#21160;&#24577;&#29305;&#24449;&#30340;&#30636;&#24577;&#31283;&#23450;&#24615;GEDF-SCL&#27169;&#22411;&#65292;&#21033;&#29992;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#32467;&#21512;GEDF&#26469;&#39044;&#27979;&#30005;&#21147;&#31995;&#32479;&#30340;&#30636;&#24577;&#31283;&#23450;&#24615;&#65292;&#24182;&#32771;&#34385;&#20102;&#25299;&#25169;&#20449;&#24687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.00537</link><description>&lt;p&gt;
&#21160;&#24577;&#29305;&#24449;&#19979;&#22522;&#20110;&#22270;&#23884;&#20837;&#30340;&#30005;&#21147;&#31995;&#32479;&#30636;&#24577;&#31283;&#23450;&#24615;&#30417;&#27979;&#30340;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Graph Embedding Dynamic Feature-based Supervised Contrastive Learning of Transient Stability for Changing Power Grid Topologies. (arXiv:2308.00537v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00537
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#23884;&#20837;&#21160;&#24577;&#29305;&#24449;&#30340;&#30636;&#24577;&#31283;&#23450;&#24615;GEDF-SCL&#27169;&#22411;&#65292;&#21033;&#29992;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#32467;&#21512;GEDF&#26469;&#39044;&#27979;&#30005;&#21147;&#31995;&#32479;&#30340;&#30636;&#24577;&#31283;&#23450;&#24615;&#65292;&#24182;&#32771;&#34385;&#20102;&#25299;&#25169;&#20449;&#24687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38754;&#23545;&#24178;&#25200;&#26102;&#65292;&#20934;&#30830;&#30340;&#22312;&#32447;&#30636;&#24577;&#31283;&#23450;&#24615;&#39044;&#27979;&#23545;&#20110;&#30830;&#20445;&#30005;&#21147;&#31995;&#32479;&#30340;&#31283;&#23450;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#30340;&#30636;&#24577;&#31283;&#23450;&#24615;&#20998;&#26512;&#20381;&#36182;&#20110;&#26102;&#38388;&#22495;&#20223;&#30495;&#65292;&#19981;&#33021;&#24555;&#36895;&#36866;&#24212;&#30005;&#21147;&#32593;&#26684;&#25299;&#25169;&#30340;&#21464;&#21270;&#12290;&#20026;&#20102;&#23558;&#39640;&#32500;&#30005;&#21147;&#31995;&#32479;&#25299;&#25169;&#32467;&#26500;&#20449;&#24687;&#21521;&#37327;&#21270;&#20026;&#20302;&#32500;&#33410;&#28857;&#23884;&#20837;&#27969;&#25968;&#25454;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#22270;&#23884;&#20837;&#21160;&#24577;&#29305;&#24449;&#65288;GEDF&#65289;&#30340;&#30636;&#24577;&#31283;&#23450;&#24615;GEDF-&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#65288;GEDF-SCL&#65289;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#32467;&#21512;GEDF&#26469;&#39044;&#27979;&#30636;&#24577;&#31283;&#23450;&#24615;&#65292;&#32771;&#34385;&#20102;&#30005;&#21147;&#31995;&#32479;&#30340;&#25299;&#25169;&#20449;&#24687;&#12290;&#20026;&#20102;&#35780;&#20272;&#25152;&#25552;&#20986;&#30340;GEDF-SCL&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#22522;&#20110;IEEE 39&#33410;&#28857;&#31995;&#32479;&#27169;&#22411;&#29983;&#25104;&#20102;&#25299;&#25169;&#32467;&#26500;&#19981;&#21516;&#30340;&#30005;&#21147;&#32593;&#26684;&#12290;&#36890;&#36807;&#22312;&#36825;&#20123;&#29983;&#25104;&#30340;&#30005;&#21147;&#31995;&#32479;&#25299;&#25169;&#19978;&#27169;&#25311;N-1&#21644;N-$\bm{m}$-1&#25925;&#38556;&#65292;&#33719;&#21462;&#20102;&#30636;&#24577;&#36816;&#34892;&#25968;&#25454;&#12290;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;GEDF-SCL&#27169;&#22411;&#20855;&#26377;&#33391;&#22909;&#30340;&#30636;&#24577;&#31283;&#23450;&#24615;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate online transient stability prediction is critical for ensuring power system stability when facing disturbances. While traditional transient stablity analysis replies on the time domain simulations can not be quickly adapted to the power grid toplogy change. In order to vectorize high-dimensional power grid topological structure information into low-dimensional node-based graph embedding streaming data, graph embedding dynamic feature (GEDF) has been proposed. The transient stability GEDF-based supervised contrastive learning (GEDF-SCL) model uses supervised contrastive learning to predict transient stability with GEDFs, considering power grid topology information. To evaluate the performance of the proposed GEDF-SCL model, power grids of varying topologies were generated based on the IEEE 39-bus system model. Transient operational data was obtained by simulating N-1 and N-$\bm{m}$-1 contingencies on these generated power system topologies. Test result demonstrated that the GED
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#36827;&#34892;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#35270;&#22270;&#30340;&#20998;&#24067;&#26469;&#25552;&#39640;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.00535</link><description>&lt;p&gt;
&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#36827;&#34892;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Graph Contrastive Learning with Generative Adversarial Network. (arXiv:2308.00535v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00535
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#36827;&#34892;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#35270;&#22270;&#30340;&#20998;&#24067;&#26469;&#25552;&#39640;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#36890;&#36807;&#30417;&#30563;&#24335;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#22312;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#23545;&#21033;&#29992;&#33410;&#28857;&#34920;&#31034;&#30340;&#33391;&#22909;&#32467;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#29616;&#23454;&#24212;&#29992;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#26631;&#31614;&#32570;&#20047;&#38382;&#39064;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#20174;&#21407;&#22987;&#22270;&#20013;&#29983;&#25104;&#30340;&#22686;&#24378;&#35270;&#22270;&#20013;&#33410;&#28857;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#65292;&#21033;&#29992;&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;GCL&#65289;&#26469;&#35757;&#32451;&#24102;&#26377;&#26377;&#38480;&#25110;&#29978;&#33267;&#27809;&#26377;&#26631;&#31614;&#30340;GNN&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#25991;&#29486;&#20013;&#22312;&#35270;&#22270;&#29983;&#25104;&#20013;&#26410;&#32771;&#34385;&#22270;&#30340;&#20998;&#24067;&#65292;&#23548;&#33268;&#24573;&#35270;&#20102;&#22823;&#22810;&#25968;&#25991;&#29486;&#20013;&#26410;&#35265;&#36793;&#30340;&#24773;&#20917;&#65292;&#32780;&#23454;&#39564;&#35777;&#26126;&#36825;&#21487;&#20197;&#25552;&#39640;GCL&#30340;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;&#22270;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#29992;&#20110;&#23398;&#20064;GCL&#30340;&#35270;&#22270;&#20998;&#24067;&#65292;&#20197;&#20415;i&#65289;&#33258;&#21160;&#25429;&#25417;&#22270;&#30340;&#29305;&#24449;&#36827;&#34892;&#22686;&#24378;&#65292;&#24182;ii&#65289;&#32852;&#21512;&#35757;&#32451;&#22270;GAN&#27169;&#22411;&#21644;GCL&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GACN&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have demonstrated promising results on exploiting node representations for many downstream tasks through supervised end-to-end training. To deal with the widespread label scarcity issue in real-world applications, Graph Contrastive Learning (GCL) is leveraged to train GNNs with limited or even no labels by maximizing the mutual information between nodes in its augmented views generated from the original graph. However, the distribution of graphs remains unconsidered in view generation, resulting in the ignorance of unseen edges in most existing literature, which is empirically shown to be able to improve GCL's performance in our experiments. To this end, we propose to incorporate graph generative adversarial networks (GANs) to learn the distribution of views for GCL, in order to i) automatically capture the characteristic of graphs for augmentations, and ii) jointly train the graph GAN model and the GCL model. Specifically, we present GACN, a novel Generati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26102;&#24207;&#22810;&#38376;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#65288;TMMOE&#65289;&#65292;&#20197;&#21516;&#26102;&#39044;&#27979;&#36710;&#36742;&#36712;&#36857;&#21644;&#39550;&#39542;&#24847;&#22270;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#20849;&#20139;&#23618;&#21644;&#19987;&#23478;&#23618;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#21644;&#20449;&#24687;&#35782;&#21035;&#65292;&#23454;&#29616;&#20102;&#23545;&#32437;&#21521;&#20301;&#32622;&#12289;&#27178;&#21521;&#20301;&#32622;&#21644;&#39550;&#39542;&#24847;&#22270;&#20043;&#38388;&#20851;&#31995;&#30340;&#32508;&#21512;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2308.00533</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#36710;&#36742;&#36712;&#36857;&#21644;&#39550;&#39542;&#24847;&#22270;&#39044;&#27979;&#30340;&#26032;&#39062;&#26102;&#24207;&#22810;&#38376;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Novel Temporal Multi-Gate Mixture-of-Experts Approach for Vehicle Trajectory and Driving Intention Prediction. (arXiv:2308.00533v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00533
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26102;&#24207;&#22810;&#38376;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#65288;TMMOE&#65289;&#65292;&#20197;&#21516;&#26102;&#39044;&#27979;&#36710;&#36742;&#36712;&#36857;&#21644;&#39550;&#39542;&#24847;&#22270;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#20849;&#20139;&#23618;&#21644;&#19987;&#23478;&#23618;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#21644;&#20449;&#24687;&#35782;&#21035;&#65292;&#23454;&#29616;&#20102;&#23545;&#32437;&#21521;&#20301;&#32622;&#12289;&#27178;&#21521;&#20301;&#32622;&#21644;&#39550;&#39542;&#24847;&#22270;&#20043;&#38388;&#20851;&#31995;&#30340;&#32508;&#21512;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#36710;&#36742;&#36712;&#36857;&#39044;&#27979;&#23545;&#20110;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#21644;&#20808;&#36827;&#39550;&#39542;&#36741;&#21161;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#36710;&#36742;&#36712;&#36857;&#39044;&#27979;&#21253;&#25324;&#32437;&#21521;&#20301;&#32622;&#39044;&#27979;&#21644;&#27178;&#21521;&#20301;&#32622;&#39044;&#27979;&#36825;&#20004;&#20010;&#20851;&#38190;&#20219;&#21153;&#12290;&#39550;&#39542;&#24847;&#22270;&#19982;&#36710;&#36742;&#36816;&#21160;&#20043;&#38388;&#23384;&#22312;&#30528;&#37325;&#35201;&#30340;&#30456;&#20851;&#24615;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#24448;&#24448;&#23558;&#36825;&#19977;&#20010;&#20219;&#21153;&#20998;&#24320;&#36827;&#34892;&#65292;&#27809;&#26377;&#32771;&#34385;&#32437;&#21521;&#20301;&#32622;&#12289;&#27178;&#21521;&#20301;&#32622;&#21644;&#39550;&#39542;&#24847;&#22270;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26102;&#24207;&#22810;&#38376;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#65288;TMMOE&#65289;&#65292;&#20197;&#21516;&#26102;&#39044;&#27979;&#36710;&#36742;&#36712;&#36857;&#21644;&#39550;&#39542;&#24847;&#22270;&#12290;&#35813;&#27169;&#22411;&#30001;&#20849;&#20139;&#23618;&#12289;&#19987;&#23478;&#23618;&#21644;&#20840;&#36830;&#25509;&#23618;&#32452;&#25104;&#12290;&#22312;&#27169;&#22411;&#20013;&#65292;&#20849;&#20139;&#23618;&#21033;&#29992;&#26102;&#24207;&#21367;&#31215;&#32593;&#32476;&#65288;TCN&#65289;&#25552;&#21462;&#26102;&#24207;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#19987;&#23478;&#23618;&#26681;&#25454;&#19977;&#20010;&#20219;&#21153;&#30340;&#38656;&#35201;&#36827;&#34892;&#19981;&#21516;&#30340;&#20449;&#24687;&#35782;&#21035;&#12290;&#27492;&#22806;&#65292;&#20840;&#36830;&#25509;&#23618;&#29992;&#20110;&#39044;&#27979;&#36710;&#36742;&#36712;&#36857;&#21644;&#39550;&#39542;&#24847;&#22270;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate Vehicle Trajectory Prediction is critical for automated vehicles and advanced driver assistance systems. Vehicle trajectory prediction consists of two essential tasks, i.e., longitudinal position prediction and lateral position prediction. There is a significant correlation between driving intentions and vehicle motion. In existing work, the three tasks are often conducted separately without considering the relationships between the longitudinal position, lateral position, and driving intention. In this paper, we propose a novel Temporal Multi-Gate Mixture-of-Experts (TMMOE) model for simultaneously predicting the vehicle trajectory and driving intention. The proposed model consists of three layers: a shared layer, an expert layer, and a fully connected layer. In the model, the shared layer utilizes Temporal Convolutional Networks (TCN) to extract temporal features. Then the expert layer is built to identify different information according to the three tasks. Moreover, the ful
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;VAriational Label-Correlation Enhancement for Congestion Prediction&#65288;{\ours}&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20934;&#30830;&#39044;&#27979;&#38598;&#25104;&#30005;&#36335;&#20013;&#30340;&#25317;&#22622;&#24773;&#20917;&#12290;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#20102;&#26684;&#23376;&#20043;&#38388;&#30340;&#31354;&#38388;&#26631;&#31614;&#30456;&#20851;&#24615;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#35782;&#21035;&#35774;&#35745;&#32570;&#38519;&#65292;&#21152;&#24555;&#30005;&#36335;&#35774;&#35745;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2308.00529</link><description>&lt;p&gt;
&#21464;&#20998;&#26631;&#31614;&#30456;&#20851;&#22686;&#24378;&#26041;&#27861;&#29992;&#20110;&#25317;&#22622;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Variational Label-Correlation Enhancement for Congestion Prediction. (arXiv:2308.00529v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00529
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;VAriational Label-Correlation Enhancement for Congestion Prediction&#65288;{\ours}&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20934;&#30830;&#39044;&#27979;&#38598;&#25104;&#30005;&#36335;&#20013;&#30340;&#25317;&#22622;&#24773;&#20917;&#12290;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#20102;&#26684;&#23376;&#20043;&#38388;&#30340;&#31354;&#38388;&#26631;&#31614;&#30456;&#20851;&#24615;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#35782;&#21035;&#35774;&#35745;&#32570;&#38519;&#65292;&#21152;&#24555;&#30005;&#36335;&#35774;&#35745;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35774;&#35745;&#30340;&#29289;&#29702;&#35774;&#35745;&#36807;&#31243;&#26159;&#19968;&#39033;&#32791;&#26102;&#30340;&#20219;&#21153;&#65292;&#36890;&#24120;&#38656;&#35201;&#20960;&#23567;&#26102;&#21040;&#20960;&#22825;&#25165;&#33021;&#23436;&#25104;&#65292;&#20854;&#20013;&#36335;&#30001;&#26159;&#26368;&#20851;&#38190;&#21644;&#22797;&#26434;&#30340;&#27493;&#39588;&#12290;&#38543;&#30528;&#38598;&#25104;&#30005;&#36335;&#65288;IC&#65289;&#30340;&#22797;&#26434;&#24615;&#22686;&#21152;&#65292;&#23545;&#20934;&#30830;&#30340;&#36335;&#30001;&#36136;&#37327;&#39044;&#27979;&#30340;&#38656;&#27714;&#20063;&#22686;&#21152;&#12290;&#20934;&#30830;&#30340;&#25317;&#22622;&#39044;&#27979;&#26377;&#21161;&#20110;&#21450;&#26089;&#21457;&#29616;&#35774;&#35745;&#32570;&#38519;&#65292;&#20174;&#32780;&#21152;&#24555;&#30005;&#36335;&#35774;&#35745;&#24182;&#33410;&#32422;&#36164;&#28304;&#12290;&#23613;&#31649;&#24403;&#21069;&#25317;&#22622;&#39044;&#27979;&#26041;&#27861;&#21462;&#24471;&#20102;&#19968;&#23450;&#30340;&#36827;&#23637;&#65292;&#20294;&#19968;&#20010;&#37325;&#35201;&#30340;&#26041;&#38754;&#21364;&#24448;&#24448;&#34987;&#24573;&#35270;&#65292;&#21363;&#19981;&#21516;&#26684;&#23376;&#20043;&#38388;&#30340;&#31354;&#38388;&#26631;&#31614;&#30456;&#20851;&#24615;&#12290;&#31354;&#38388;&#26631;&#31614;&#30456;&#20851;&#24615;&#26159;&#30005;&#36335;&#35774;&#35745;&#30340;&#22522;&#26412;&#29305;&#24449;&#65292;&#19968;&#20010;&#26684;&#23376;&#30340;&#25317;&#22622;&#24773;&#20917;&#19981;&#20165;&#19982;&#20854;&#33258;&#36523;&#26377;&#20851;&#65292;&#36824;&#21463;&#20854;&#30456;&#37051;&#26684;&#23376;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#20805;&#20998;&#21033;&#29992;&#30456;&#37051;&#26684;&#23376;&#20043;&#38388;&#30340;&#20869;&#22312;&#31354;&#38388;&#26631;&#31614;&#30456;&#20851;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;VAriational Label-Correlation Enhancement for Congestion Prediction&#65288;{\ours}&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
The physical design process of large-scale designs is a time-consuming task, often requiring hours to days to complete, with routing being the most critical and complex step. As the the complexity of Integrated Circuits (ICs) increases, there is an increased demand for accurate routing quality prediction. Accurate congestion prediction aids in identifying design flaws early on, thereby accelerating circuit design and conserving resources. Despite the advancements in current congestion prediction methodologies, an essential aspect that has been largely overlooked is the spatial label-correlation between different grids in congestion prediction. The spatial label-correlation is a fundamental characteristic of circuit design, where the congestion status of a grid is not isolated but inherently influenced by the conditions of its neighboring grids. In order to fully exploit the inherent spatial label-correlation between neighboring grids, we propose a novel approach, {\ours}, i.e., VAriati
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedLADA&#30340;&#32852;&#37030;&#23616;&#37096;&#33258;&#36866;&#24212;&#20462;&#27491;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;&#24341;&#20837;&#23616;&#37096;&#20462;&#27491;&#25216;&#26415;&#21644;&#21160;&#37327;&#39033;&#65292;&#35299;&#20915;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#31895;&#31961;&#25910;&#25947;&#21644;&#23458;&#25143;&#31471;&#28418;&#31227;&#21152;&#21095;&#30340;&#38382;&#39064;&#65292;&#20855;&#26377;&#32447;&#24615;&#21152;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.00522</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#32852;&#37030;&#23398;&#20064;&#65306;&#22522;&#20110;&#23616;&#37096;&#33258;&#36866;&#24212;&#20462;&#27491;&#20248;&#21270;&#22120;&#30340;&#32447;&#24615;&#21152;&#36895;
&lt;/p&gt;
&lt;p&gt;
Efficient Federated Learning via Local Adaptive Amended Optimizer with Linear Speedup. (arXiv:2308.00522v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00522
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedLADA&#30340;&#32852;&#37030;&#23616;&#37096;&#33258;&#36866;&#24212;&#20462;&#27491;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;&#24341;&#20837;&#23616;&#37096;&#20462;&#27491;&#25216;&#26415;&#21644;&#21160;&#37327;&#39033;&#65292;&#35299;&#20915;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#31895;&#31961;&#25910;&#25947;&#21644;&#23458;&#25143;&#31471;&#28418;&#31227;&#21152;&#21095;&#30340;&#38382;&#39064;&#65292;&#20855;&#26377;&#32447;&#24615;&#21152;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#36866;&#24212;&#20248;&#21270;&#26041;&#27861;&#22312;&#20998;&#24067;&#24335;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#23558;&#33258;&#36866;&#24212;&#20248;&#21270;&#22120;&#25193;&#23637;&#21040;&#32852;&#37030;&#23398;&#20064;&#20013;&#23384;&#22312;&#20005;&#37325;&#30340;&#25928;&#29575;&#38382;&#39064;&#65292;&#21253;&#25324;&#65306;&#65288;i&#65289;&#20840;&#23616;&#33258;&#36866;&#24212;&#20248;&#21270;&#22120;&#20013;&#30001;&#20110;&#26799;&#24230;&#20272;&#35745;&#19981;&#20934;&#30830;&#23548;&#33268;&#30340;&#31895;&#31961;&#25910;&#25947;&#65307;&#65288;ii&#65289;&#30001;&#20110;&#23616;&#37096;&#33258;&#36866;&#24212;&#20248;&#21270;&#22120;&#30340;&#23616;&#37096;&#36807;&#25311;&#21512;&#23548;&#33268;&#30340;&#23458;&#25143;&#31471;&#28418;&#31227;&#21152;&#21095;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#21160;&#37327;&#30340;&#31639;&#27861;&#65292;&#21033;&#29992;&#20840;&#23616;&#26799;&#24230;&#19979;&#38477;&#21644;&#23616;&#37096;&#33258;&#36866;&#24212;&#20462;&#27491;&#20248;&#21270;&#22120;&#26469;&#35299;&#20915;&#36825;&#20123;&#22256;&#38590;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#23616;&#37096;&#20462;&#27491;&#25216;&#26415;&#21040;&#33258;&#36866;&#24212;&#20248;&#21270;&#22120;&#20013;&#65292;&#21629;&#21517;&#20026;&#32852;&#37030;&#23616;&#37096;&#33258;&#36866;&#24212;&#20462;&#27491;&#20248;&#21270;&#22120;&#65288;FedLADA&#65289;&#65292;&#23427;&#36890;&#36807;&#21160;&#37327;&#39033;&#20272;&#35745;&#19978;&#19968;&#36718;&#36890;&#20449;&#20013;&#30340;&#20840;&#23616;&#24179;&#22343;&#20559;&#24046;&#65292;&#24182;&#36890;&#36807;&#26657;&#27491;&#23616;&#37096;&#20559;&#24046;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;&#32463;&#39564;&#35757;&#32451;&#36895;&#24230;&#24182;&#20943;&#36731;&#24322;&#26500;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;FedLADA&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#20855;&#26377;&#32447;&#24615;&#21152;&#36895;&#24230;
&lt;/p&gt;
&lt;p&gt;
Adaptive optimization has achieved notable success for distributed learning while extending adaptive optimizer to federated Learning (FL) suffers from severe inefficiency, including (i) rugged convergence due to inaccurate gradient estimation in global adaptive optimizer; (ii) client drifts exacerbated by local over-fitting with the local adaptive optimizer. In this work, we propose a novel momentum-based algorithm via utilizing the global gradient descent and locally adaptive amended optimizer to tackle these difficulties. Specifically, we incorporate a locally amended technique to the adaptive optimizer, named Federated Local ADaptive Amended optimizer (\textit{FedLADA}), which estimates the global average offset in the previous communication round and corrects the local offset through a momentum-like term to further improve the empirical training speed and mitigate the heterogeneous over-fitting. Theoretically, we establish the convergence rate of \textit{FedLADA} with a linear spee
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#25913;&#21892;&#33008;&#33146;&#30284;&#30340;&#39044;&#21518;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#21487;&#23398;&#20064;&#30340;&#31070;&#32463;&#36317;&#31163;&#26469;&#25551;&#36848;&#32959;&#30244;&#19982;&#34880;&#31649;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#34701;&#21512;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#26469;&#25913;&#36827;&#22810;&#30456;CT&#24433;&#20687;&#20013;&#30340;&#32959;&#30244;&#32441;&#29702;&#29305;&#24449;&#25552;&#21462;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#39044;&#21518;&#39044;&#27979;&#20013;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.00507</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#30456;CT&#32467;&#21512;&#31070;&#32463;&#36317;&#31163;&#21644;&#32441;&#29702;&#24863;&#30693;&#21464;&#21387;&#22120;&#65292;&#25552;&#39640;&#33008;&#33146;&#30284;&#39044;&#21518;&#39044;&#27979;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Improved Prognostic Prediction of Pancreatic Cancer Using Multi-Phase CT by Integrating Neural Distance and Texture-Aware Transformer. (arXiv:2308.00507v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00507
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#25913;&#21892;&#33008;&#33146;&#30284;&#30340;&#39044;&#21518;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#21487;&#23398;&#20064;&#30340;&#31070;&#32463;&#36317;&#31163;&#26469;&#25551;&#36848;&#32959;&#30244;&#19982;&#34880;&#31649;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#34701;&#21512;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#26469;&#25913;&#36827;&#22810;&#30456;CT&#24433;&#20687;&#20013;&#30340;&#32959;&#30244;&#32441;&#29702;&#29305;&#24449;&#25552;&#21462;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#39044;&#21518;&#39044;&#27979;&#20013;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33008;&#33146;&#23548;&#31649;&#33146;&#30284;&#65288;PDAC&#65289;&#26159;&#19968;&#31181;&#39640;&#24230;&#33268;&#21629;&#30340;&#30284;&#30151;&#65292;&#32959;&#30244;-&#34880;&#31649;&#21463;&#32047;&#26497;&#22823;&#24433;&#21709;&#24739;&#32773;&#30340;&#21487;&#20999;&#38500;&#24615;&#21644;&#24635;&#20307;&#29983;&#23384;&#29575;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#39044;&#21518;&#39044;&#27979;&#26041;&#27861;&#26410;&#33021;&#26126;&#30830;&#20934;&#30830;&#22320;&#35843;&#26597;&#32959;&#30244;&#19982;&#38468;&#36817;&#37325;&#35201;&#34880;&#31649;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#23398;&#20064;&#30340;&#31070;&#32463;&#36317;&#31163;&#65292;&#25551;&#36848;&#20102;&#19981;&#21516;&#24739;&#32773;CT&#24433;&#20687;&#20013;&#32959;&#30244;&#19982;&#34880;&#31649;&#20043;&#38388;&#30340;&#31934;&#30830;&#20851;&#31995;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#39044;&#21518;&#39044;&#27979;&#30340;&#20027;&#35201;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#19981;&#21516;&#20110;&#29616;&#26377;&#27169;&#22411;&#22312;&#21160;&#24577;&#23545;&#27604;&#22686;&#24378;CT&#25104;&#20687;&#19978;&#21033;&#29992;CNN&#25110;LSTM&#26469;&#21033;&#29992;&#32959;&#30244;&#22686;&#24378;&#27169;&#24335;&#65292;&#25105;&#20204;&#36890;&#36807;&#34701;&#21512;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#20351;&#29992;CNN&#21644;&#21464;&#21387;&#22120;&#27169;&#22359;&#65292;&#22312;&#22810;&#30456;&#23545;&#27604;&#22686;&#24378;CT&#20013;&#25913;&#36827;&#20102;&#21160;&#24577;&#19982;&#32959;&#30244;&#30456;&#20851;&#30340;&#32441;&#29702;&#29305;&#24449;&#25552;&#21462;&#65292;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#36328;&#22810;&#30456;CT&#24433;&#20687;&#25552;&#21462;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#23545;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#21644;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pancreatic ductal adenocarcinoma (PDAC) is a highly lethal cancer in which the tumor-vascular involvement greatly affects the resectability and, thus, overall survival of patients. However, current prognostic prediction methods fail to explicitly and accurately investigate relationships between the tumor and nearby important vessels. This paper proposes a novel learnable neural distance that describes the precise relationship between the tumor and vessels in CT images of different patients, adopting it as a major feature for prognosis prediction. Besides, different from existing models that used CNNs or LSTMs to exploit tumor enhancement patterns on dynamic contrast-enhanced CT imaging, we improved the extraction of dynamic tumor-related texture features in multi-phase contrast-enhanced CT by fusing local and global features using CNN and transformer modules, further enhancing the features extracted across multi-phase CT images. We extensively evaluated and compared the proposed method
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#25991;&#26412;&#25991;&#26723;&#30340;&#22270;&#35889;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#23637;&#31034;&#32452;&#21512;&#25289;&#26222;&#25289;&#26031;&#23884;&#20837;&#12289;K&#23884;&#20837;&#21644;&#35789;&#21521;&#37327;&#31354;&#38388;&#23884;&#20837;&#20043;&#38388;&#30340;&#31561;&#20215;&#24615;&#65292;&#26500;&#24314;&#20102;&#25991;&#26412;&#20869;&#23481;&#21644;&#32858;&#31867;&#32467;&#26524;&#20043;&#38388;&#30340;&#26725;&#26753;&#12290;</title><link>http://arxiv.org/abs/2308.00504</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#25991;&#26412;&#25991;&#26723;&#30340;&#22270;&#35889;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Explainable Graph Spectral Clustering of Text Documents. (arXiv:2308.00504v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00504
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#25991;&#26412;&#25991;&#26723;&#30340;&#22270;&#35889;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#23637;&#31034;&#32452;&#21512;&#25289;&#26222;&#25289;&#26031;&#23884;&#20837;&#12289;K&#23884;&#20837;&#21644;&#35789;&#21521;&#37327;&#31354;&#38388;&#23884;&#20837;&#20043;&#38388;&#30340;&#31561;&#20215;&#24615;&#65292;&#26500;&#24314;&#20102;&#25991;&#26412;&#20869;&#23481;&#21644;&#32858;&#31867;&#32467;&#26524;&#20043;&#38388;&#30340;&#26725;&#26753;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#35889;&#32858;&#31867;&#26041;&#27861;&#20197;&#20854;&#33021;&#22815;&#34920;&#31034;&#19981;&#21516;&#24418;&#29366;&#12289;&#23494;&#24230;&#31561;&#30340;&#32858;&#31867;&#32780;&#38395;&#21517;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#20123;&#31639;&#27861;&#24212;&#29992;&#20110;&#25991;&#26412;&#25991;&#26723;&#26102;&#65292;&#20854;&#32467;&#26524;&#24456;&#38590;&#21521;&#29992;&#25143;&#35299;&#37322;&#65292;&#29305;&#21035;&#26159;&#30001;&#20110;&#22312;&#20809;&#35889;&#31354;&#38388;&#20013;&#30340;&#23884;&#20837;&#19982;&#25991;&#26723;&#20869;&#23481;&#27809;&#26377;&#26126;&#26174;&#30340;&#20851;&#31995;&#12290;&#22240;&#27492;&#65292;&#36843;&#20999;&#38656;&#35201;&#30740;&#31350;&#35299;&#37322;&#32858;&#31867;&#32467;&#26524;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;&#27492;&#30446;&#26631;&#30340;&#36129;&#29486;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#35299;&#37322;&#22522;&#20110;&#32452;&#21512;&#25289;&#26222;&#25289;&#26031;&#30340;&#22270;&#35889;&#32858;&#31867;&#32467;&#26524;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#23637;&#31034;&#32452;&#21512;&#25289;&#26222;&#25289;&#26031;&#23884;&#20837;&#12289;K&#23884;&#20837;&#65288;&#26412;&#25991;&#25552;&#20986;&#65289;&#21644;&#35789;&#21521;&#37327;&#31354;&#38388;&#23884;&#20837;&#30340;&#65288;&#36817;&#20284;&#65289;&#31561;&#20215;&#24615;&#12290;&#20174;&#32780;&#26500;&#24314;&#20102;&#25991;&#26412;&#20869;&#23481;&#21644;&#32858;&#31867;&#32467;&#26524;&#20043;&#38388;&#30340;&#26725;&#26753;&#12290;&#25105;&#20204;&#20026;&#36825;&#31181;&#26041;&#27861;&#25552;&#20379;&#20102;&#29702;&#35770;&#32972;&#26223;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;&#30740;&#31350;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#26377;&#21033;&#26465;&#20214;&#19979;&#65292;K&#23884;&#20837;&#24456;&#22909;&#22320;&#36817;&#20284;&#20102;&#25289;&#26222;&#25289;&#26031;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spectral clustering methods are known for their ability to represent clusters of diverse shapes, densities etc. However, results of such algorithms, when applied e.g. to text documents, are hard to explain to the user, especially due to embedding in the spectral space which has no obvious relation to document contents. Therefore there is an urgent need to elaborate methods for explaining the outcome of the clustering. This paper presents a contribution towards this goal. We present a proposal of explanation of results of combinatorial Laplacian based graph spectral clustering. It is based on showing (approximate) equivalence of combinatorial Laplacian embedding, $K$-embedding (proposed in this paper) and term vector space embedding. Hence a bridge is constructed between the textual contents and the clustering results. We provide theoretical background for this approach. We performed experimental study showing that $K$-embedding approximates well Laplacian embedding under favourable blo
&lt;/p&gt;</description></item><item><title>DINO-CXR&#26159;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#21464;&#25442;&#22120;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#33016;&#37096;X&#23556;&#32447;&#20998;&#31867;&#65292;&#24182;&#36890;&#36807;&#27604;&#36739;&#20998;&#26512;&#26174;&#31034;&#20986;&#22312;&#32954;&#28814;&#21644;COVID-19&#26816;&#27979;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#65292;&#21516;&#26102;&#38656;&#35201;&#36739;&#23569;&#30340;&#26631;&#27880;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2308.00475</link><description>&lt;p&gt;
DINO-CXR: &#22522;&#20110;&#35270;&#35273;&#21464;&#25442;&#22120;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#29992;&#20110;&#33016;&#37096;X&#23556;&#32447;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
DINO-CXR: A self supervised method based on vision transformer for chest X-ray classification. (arXiv:2308.00475v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00475
&lt;/p&gt;
&lt;p&gt;
DINO-CXR&#26159;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#21464;&#25442;&#22120;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#33016;&#37096;X&#23556;&#32447;&#20998;&#31867;&#65292;&#24182;&#36890;&#36807;&#27604;&#36739;&#20998;&#26512;&#26174;&#31034;&#20986;&#22312;&#32954;&#28814;&#21644;COVID-19&#26816;&#27979;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#65292;&#21516;&#26102;&#38656;&#35201;&#36739;&#23569;&#30340;&#26631;&#27880;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#23398;&#25104;&#20687;&#26041;&#27861;&#30340;&#21457;&#23637;&#20013;&#65292;&#26631;&#27880;&#26377;&#38480;&#30340;&#33016;&#37096;X&#23556;&#32447;&#25968;&#25454;&#38598;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#29942;&#39048;&#12290;&#33258;&#30417;&#30563;&#23398;&#20064; (SSL) &#21487;&#20197;&#36890;&#36807;&#22312;&#26080;&#26631;&#31614;&#25968;&#25454;&#19978;&#35757;&#32451;&#27169;&#22411;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#22312;&#33258;&#28982;&#22270;&#20687;&#30340;&#35270;&#35273;&#35782;&#21035;&#26041;&#38754;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#26041;&#38754;&#24182;&#27809;&#26377;&#24471;&#21040;&#36275;&#22815;&#30340;&#20851;&#27880;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#26041;&#27861; DINO-CXR&#65292;&#23427;&#26159;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#21464;&#25442;&#22120;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861; DINO &#22312;&#33016;&#37096;X&#23556;&#32447;&#20998;&#31867;&#26041;&#38754;&#30340;&#26032;&#30340;&#25913;&#36827;&#12290;&#36890;&#36807;&#27604;&#36739;&#20998;&#26512;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#32954;&#28814;&#21644;COVID-19&#26816;&#27979;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#23450;&#37327;&#20998;&#26512;&#36824;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#29575;&#26041;&#38754;&#32988;&#36807;&#29616;&#26377;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;AUC&#21644;F-1&#20998;&#25968;&#26041;&#38754;&#36798;&#21040;&#21487;&#27604;&#36739;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#38656;&#35201;&#36739;&#23569;&#30340;&#26631;&#27880;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
The limited availability of labeled chest X-ray datasets is a significant bottleneck in the development of medical imaging methods. Self-supervised learning (SSL) can mitigate this problem by training models on unlabeled data. Furthermore, self-supervised pretraining has yielded promising results in visual recognition of natural images but has not been given much consideration in medical image analysis. In this work, we propose a self-supervised method, DINO-CXR, which is a novel adaptation of a self-supervised method, DINO, based on a vision transformer for chest X-ray classification. A comparative analysis is performed to show the effectiveness of the proposed method for both pneumonia and COVID-19 detection. Through a quantitative analysis, it is also shown that the proposed method outperforms state-of-the-art methods in terms of accuracy and achieves comparable results in terms of AUC and F-1 score while requiring significantly less labeled data.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#26368;&#21518;&#19968;&#23618;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#65292;Deep Feature Reweighting&#65288;DFR&#65289;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#22312;&#23384;&#22312;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#25968;&#25454;&#20013;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#20854;&#24212;&#29992;&#20110;&#23454;&#38469;&#21307;&#23398;&#25968;&#25454;&#26102;&#23384;&#22312;&#19968;&#23450;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.00473</link><description>&lt;p&gt;
&#26368;&#21518;&#19968;&#23618;&#30340;&#35757;&#32451;&#26159;&#21542;&#36275;&#20197;&#24212;&#23545;&#34394;&#20551;&#30456;&#20851;&#24615;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is Last Layer Re-Training Truly Sufficient for Robustness to Spurious Correlations?. (arXiv:2308.00473v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00473
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#26368;&#21518;&#19968;&#23618;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#65292;Deep Feature Reweighting&#65288;DFR&#65289;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#22312;&#23384;&#22312;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#25968;&#25454;&#20013;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#20854;&#24212;&#29992;&#20110;&#23454;&#38469;&#21307;&#23398;&#25968;&#25454;&#26102;&#23384;&#22312;&#19968;&#23450;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20197;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;ERM&#65289;&#35757;&#32451;&#30340;&#27169;&#22411;&#24050;&#34987;&#30693;&#26195;&#23398;&#20250;&#20381;&#36182;&#34394;&#20551;&#29305;&#24449;&#65292;&#21363;&#23427;&#20204;&#30340;&#39044;&#27979;&#22522;&#20110;&#19982;&#31867;&#21035;&#26631;&#31614;&#24378;&#30456;&#20851;&#20294;&#32570;&#20047;&#22240;&#26524;&#25512;&#29702;&#30340;&#38750;&#26399;&#26395;&#36741;&#21161;&#29305;&#24449;&#12290;&#36825;&#31181;&#34892;&#20026;&#23588;&#20854;&#22312;&#30456;&#20851;&#31867;&#21035;&#30340;&#26679;&#26412;&#32452;&#20013;&#65292;&#21487;&#33021;&#27809;&#26377;&#36825;&#20123;&#34394;&#20551;&#29305;&#24449;&#25110;&#32773;&#30456;&#21453;&#31867;&#21035;&#30340;&#26679;&#26412;&#20013;&#23384;&#22312;&#36825;&#20123;&#34394;&#20551;&#29305;&#24449;&#26102;&#65292;&#23548;&#33268;&#20934;&#30830;&#24615;&#30340;&#19979;&#38477;&#12290;&#26368;&#36817;&#25552;&#20986;&#30340;&#28145;&#24230;&#29305;&#24449;&#37325;&#21152;&#26435;&#65288;DFR&#65289;&#26041;&#27861;&#25552;&#39640;&#20102;&#36825;&#20123;&#26368;&#24046;&#26679;&#26412;&#32452;&#30340;&#20934;&#30830;&#24615;&#12290;&#22522;&#20110;ERM&#27169;&#22411;&#21487;&#20197;&#36275;&#22815;&#22909;&#22320;&#23398;&#20064;&#26680;&#24515;&#29305;&#24449;&#30340;&#20027;&#35201;&#35770;&#28857;&#65292;DFR&#21482;&#38656;&#23545;&#20998;&#31867;&#27169;&#22411;&#30340;&#26368;&#21518;&#19968;&#23618;&#36827;&#34892;&#23567;&#35268;&#27169;&#24179;&#34913;&#25968;&#25454;&#38598;&#30340;&#37325;&#26032;&#35757;&#32451;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#26816;&#39564;&#20102;DFR&#22312;&#21307;&#23398;&#39046;&#22495;&#30495;&#23454;&#25968;&#25454;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#26368;&#21518;&#19968;&#23618;&#37325;&#26032;&#35757;&#32451;&#26377;&#25928;&#24615;&#32972;&#21518;&#30340;&#25512;&#29702;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#32467;&#26524;&#34920;&#26126;&#23613;&#31649;DFR&#20855;&#26377;&#25552;&#39640;&#26368;&#24046;&#26679;&#26412;&#32452;&#20934;&#30830;&#24615;&#30340;&#28508;&#21147;&#65292;&#20294;&#20854;&#23454;&#29616;&#26041;&#24335;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Models trained with empirical risk minimization (ERM) are known to learn to rely on spurious features, i.e., their prediction is based on undesired auxiliary features which are strongly correlated with class labels but lack causal reasoning. This behavior particularly degrades accuracy in groups of samples of the correlated class that are missing the spurious feature or samples of the opposite class but with the spurious feature present. The recently proposed Deep Feature Reweighting (DFR) method improves accuracy of these worst groups. Based on the main argument that ERM mods can learn core features sufficiently well, DFR only needs to retrain the last layer of the classification model with a small group-balanced data set. In this work, we examine the applicability of DFR to realistic data in the medical domain. Furthermore, we investigate the reasoning behind the effectiveness of last-layer retraining and show that even though DFR has the potential to improve the accuracy of the wors
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#38646;&#38454;&#26597;&#35810;&#36817;&#20284;&#19968;&#38454;&#21644;&#20108;&#38454;&#20449;&#24687;&#30340;&#38646;&#38454;&#20248;&#21270;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37325;&#21442;&#25968;&#21270;&#30446;&#26631;&#20989;&#25968;&#24182;&#32473;&#20986;&#20102;&#23545;&#24212;&#30340;&#26368;&#23567;&#21270;&#31639;&#27861;&#65292;&#21517;&#20026;MiNES&#65288;&#38236;&#20687;&#19979;&#38477;&#33258;&#28982;&#36827;&#21270;&#31574;&#30053;&#65289;&#12290;</title><link>http://arxiv.org/abs/2308.00469</link><description>&lt;p&gt;
&#38236;&#20687;&#33258;&#28982;&#36827;&#21270;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Mirror Natural Evolution Strategies. (arXiv:2308.00469v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00469
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#38646;&#38454;&#26597;&#35810;&#36817;&#20284;&#19968;&#38454;&#21644;&#20108;&#38454;&#20449;&#24687;&#30340;&#38646;&#38454;&#20248;&#21270;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37325;&#21442;&#25968;&#21270;&#30446;&#26631;&#20989;&#25968;&#24182;&#32473;&#20986;&#20102;&#23545;&#24212;&#30340;&#26368;&#23567;&#21270;&#31639;&#27861;&#65292;&#21517;&#20026;MiNES&#65288;&#38236;&#20687;&#19979;&#38477;&#33258;&#28982;&#36827;&#21270;&#31574;&#30053;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;&#38454;&#20248;&#21270;&#22312;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#36817;&#20284;&#26799;&#24230;&#20351;&#29992;&#38646;&#38454;&#20989;&#25968;&#20540;&#24046;&#24322;&#30340;&#38646;&#38454;&#20248;&#21270;&#31639;&#27861;&#30340;&#29702;&#35770;&#30740;&#31350;&#38598;&#20013;&#22312;&#38543;&#26426;&#26041;&#21521;&#19978;&#12290;&#36817;&#20284;&#26799;&#24230;&#21644;Hessian&#20449;&#24687;&#30340;&#38646;&#38454;&#26597;&#35810;&#31639;&#27861;&#30340;&#29702;&#35770;&#30740;&#31350;&#36739;&#23569;&#12290;&#26412;&#25991;&#20027;&#35201;&#30740;&#31350;&#21033;&#29992;&#38646;&#38454;&#26597;&#35810;&#36817;&#20284;&#19968;&#38454;&#21644;&#20108;&#38454;&#20449;&#24687;&#30340;&#38646;&#38454;&#20248;&#21270;&#29702;&#35770;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#37325;&#21442;&#25968;&#21270;&#30446;&#26631;&#20989;&#25968;&#65292;&#20854;&#21442;&#25968;&#20026;$(\mu, \Sigma)$&#12290;&#36825;&#20010;&#37325;&#21442;&#25968;&#21270;&#30446;&#26631;&#20989;&#25968;&#22312;&#21407;&#30446;&#26631;&#20989;&#25968;&#30340;&#26368;&#23567;&#21270;&#22120;&#21644;Hessian&#30340;&#36870;&#20043;&#22788;&#20998;&#21035;&#36798;&#21040;&#20854;&#26368;&#20248;&#20540;&#65292;&#20294;&#26377;&#23567;&#30340;&#25200;&#21160;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#26469;&#26368;&#23567;&#21270;&#25105;&#20204;&#25552;&#20986;&#30340;&#37325;&#21442;&#25968;&#21270;&#30446;&#26631;&#20989;&#25968;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;MiNES&#65288;&#38236;&#20687;&#19979;&#38477;&#33258;&#28982;&#36827;&#21270;&#31574;&#30053;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
The zeroth-order optimization has been widely used in machine learning applications. However, the theoretical study of the zeroth-order optimization focus on the algorithms which approximate (first-order) gradients using (zeroth-order) function value difference at a random direction. The theory of algorithms which approximate the gradient and Hessian information by zeroth-order queries is much less studied. In this paper, we focus on the theory of zeroth-order optimization which utilizes both the first-order and second-order information approximated by the zeroth-order queries. We first propose a novel reparameterized objective function with parameters $(\mu, \Sigma)$. This reparameterized objective function achieves its optimum at the minimizer and the Hessian inverse of the original objective function respectively, but with small perturbations. Accordingly, we propose a new algorithm to minimize our proposed reparameterized objective, which we call \texttt{MiNES} (mirror descent natu
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;MajorCert&#65292;&#36890;&#36807;&#25214;&#21040;&#22312;&#24213;&#23618;&#20998;&#31867;&#22120;&#20013;&#30001;&#21516;&#19968;&#20010;&#34917;&#19969;&#21306;&#22495;&#22312;&#21516;&#19968;&#20010;&#26679;&#26412;&#19978;&#21487;&#20197;&#25805;&#32437;&#30340;&#25152;&#26377;&#21487;&#33021;&#26631;&#31614;&#38598;&#21512;&#65292;&#24182;&#26816;&#26597;&#23427;&#20204;&#30340;&#22823;&#22810;&#25968;&#19981;&#21464;&#24615;&#26469;&#23545;&#26679;&#26412;&#36827;&#34892;&#34917;&#19969;&#31283;&#20581;&#24615;&#35748;&#35777;&#12290;</title><link>http://arxiv.org/abs/2308.00452</link><description>&lt;p&gt;
&#19968;&#20010;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#34917;&#19969;&#31283;&#20581;&#24615;&#35748;&#35777;&#30340;&#22823;&#22810;&#25968;&#19981;&#21464;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Majority Invariant Approach to Patch Robustness Certification for Deep Learning Models. (arXiv:2308.00452v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00452
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;MajorCert&#65292;&#36890;&#36807;&#25214;&#21040;&#22312;&#24213;&#23618;&#20998;&#31867;&#22120;&#20013;&#30001;&#21516;&#19968;&#20010;&#34917;&#19969;&#21306;&#22495;&#22312;&#21516;&#19968;&#20010;&#26679;&#26412;&#19978;&#21487;&#20197;&#25805;&#32437;&#30340;&#25152;&#26377;&#21487;&#33021;&#26631;&#31614;&#38598;&#21512;&#65292;&#24182;&#26816;&#26597;&#23427;&#20204;&#30340;&#22823;&#22810;&#25968;&#19981;&#21464;&#24615;&#26469;&#23545;&#26679;&#26412;&#36827;&#34892;&#34917;&#19969;&#31283;&#20581;&#24615;&#35748;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34917;&#19969;&#31283;&#20581;&#24615;&#35748;&#35777;&#30830;&#20445;&#22312;&#32473;&#23450;&#26679;&#26412;&#19978;&#65292;&#27809;&#26377;&#34917;&#19969;&#33021;&#22815;&#36890;&#36807;&#25805;&#32437;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#39044;&#27979;&#19981;&#21516;&#30340;&#26631;&#31614;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25216;&#26415;&#19981;&#33021;&#23545;&#26080;&#27861;&#22312;&#20998;&#31867;&#22120;&#25110;&#32773;&#34917;&#19969;&#21306;&#22495;&#27700;&#24179;&#19978;&#36798;&#21040;&#20005;&#26684;&#26631;&#20934;&#30340;&#26679;&#26412;&#36827;&#34892;&#35748;&#35777;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;MajorCert&#12290;MajorCert&#39318;&#20808;&#25214;&#21040;&#22312;&#24213;&#23618;&#20998;&#31867;&#22120;&#20013;&#30001;&#21516;&#19968;&#20010;&#34917;&#19969;&#21306;&#22495;&#22312;&#21516;&#19968;&#20010;&#26679;&#26412;&#19978;&#21487;&#20197;&#25805;&#32437;&#30340;&#25152;&#26377;&#21487;&#33021;&#30340;&#26631;&#31614;&#38598;&#21512;&#65292;&#28982;&#21518;&#36880;&#20010;&#26522;&#20030;&#23427;&#20204;&#30340;&#32452;&#21512;&#65292;&#24182;&#26368;&#32456;&#26816;&#26597;&#25152;&#26377;&#36825;&#20123;&#32452;&#21512;&#30340;&#22823;&#22810;&#25968;&#19981;&#21464;&#24615;&#26159;&#21542;&#23436;&#25972;&#20197;&#23545;&#26679;&#26412;&#36827;&#34892;&#35748;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Patch robustness certification ensures no patch within a given bound on a sample can manipulate a deep learning model to predict a different label. However, existing techniques cannot certify samples that cannot meet their strict bars at the classifier or patch region levels. This paper proposes MajorCert. MajorCert firstly finds all possible label sets manipulatable by the same patch region on the same sample across the underlying classifiers, then enumerates their combinations element-wise, and finally checks whether the majority invariant of all these combinations is intact to certify samples.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;LLMs&#33258;&#26816;&#36880;&#27493;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;-shot&#39564;&#35777;&#26041;&#26696;&#65292;&#25104;&#21151;&#35782;&#21035;&#38169;&#35823;&#24182;&#25552;&#39640;&#20102;&#38382;&#31572;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.00436</link><description>&lt;p&gt;
SelfCheck: &#20351;&#29992;LLMs&#33258;&#26816;&#20854;&#36880;&#27493;&#25512;&#29702;&#30340;&#21019;&#26032;
&lt;/p&gt;
&lt;p&gt;
SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning. (arXiv:2308.00436v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00436
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;LLMs&#33258;&#26816;&#36880;&#27493;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;-shot&#39564;&#35777;&#26041;&#26696;&#65292;&#25104;&#21151;&#35782;&#21035;&#38169;&#35823;&#24182;&#25552;&#39640;&#20102;&#38382;&#31572;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#65292;&#23588;&#20854;&#26159;&#38142;&#24335;&#24605;&#32500;&#65288;CoT&#65289;&#30340;&#21457;&#26126;&#65292;&#20351;&#24471;&#35299;&#20915;&#25512;&#29702;&#38382;&#39064;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26368;&#24378;&#22823;&#30340;LLMs&#20173;&#28982;&#38590;&#20197;&#22788;&#29702;&#38656;&#35201;&#38750;&#32447;&#24615;&#24605;&#32500;&#21644;&#22810;&#27493;&#25512;&#29702;&#30340;&#22797;&#26434;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;LLMs&#26159;&#21542;&#20855;&#26377;&#35782;&#21035;&#33258;&#24049;&#38169;&#35823;&#30340;&#33021;&#21147;&#65292;&#32780;&#26080;&#38656;&#20381;&#36182;&#22806;&#37096;&#36164;&#28304;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23427;&#20204;&#26159;&#21542;&#21487;&#20197;&#29992;&#20110;&#35782;&#21035;&#36880;&#27493;&#25512;&#29702;&#20013;&#30340;&#20010;&#21035;&#38169;&#35823;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;-shot&#39564;&#35777;&#26041;&#26696;&#20197;&#35782;&#21035;&#27492;&#31867;&#38169;&#35823;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#27492;&#39564;&#35777;&#26041;&#26696;&#26469;&#25913;&#36827;&#38382;&#31572;&#24615;&#33021;&#65292;&#36890;&#36807;&#23545;&#19981;&#21516;&#29983;&#25104;&#30340;&#31572;&#26696;&#36827;&#34892;&#21152;&#26435;&#25237;&#31080;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#25968;&#23398;&#25968;&#25454;&#38598;-GSM8K&#65292;MathQA&#21644;MATH&#19978;&#27979;&#35797;&#20102;&#35813;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#23427;&#25104;&#21151;&#35782;&#21035;&#38169;&#35823;&#65292;&#24182;&#36827;&#32780;&#25552;&#39640;&#20102;&#26368;&#32456;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent progress in large language models (LLMs), especially the invention of chain-of-thoughts (CoT) prompting, makes it possible to solve reasoning problems. However, even the strongest LLMs are still struggling with more complicated problems that require non-linear thinking and multi-step reasoning. In this work, we explore whether LLMs have the ability to recognize their own errors, without resorting to external resources. In particular, we investigate whether they can be used to identify individual errors within a step-by-step reasoning. To this end, we propose a zero-shot verification scheme to recognize such errors. We then use this verification scheme to improve question-answering performance, by using it to perform weighted voting on different generated answers. We test the method on three math datasets-GSM8K, MathQA, and MATH-and find that it successfully recognizes errors and, in turn, increases final predictive performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#31070;&#32463;&#22270;&#34920;&#25688;&#35201;&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#39044;&#22788;&#29702;&#35757;&#32451;&#25968;&#25454;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#24187;&#35273;&#30340;&#20135;&#29983;&#65292;&#24182;&#36890;&#36807;&#32553;&#30701;&#36828;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#21644;&#28155;&#21152;&#19982;&#22270;&#34920;&#30456;&#20851;&#30340;&#20449;&#24687;&#26469;&#25552;&#39640;&#25972;&#20307;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.00399</link><description>&lt;p&gt;
&#22788;&#29702;&#31070;&#32463;&#22270;&#34920;&#25688;&#35201;&#20013;&#30340;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Tackling Hallucinations in Neural Chart Summarization. (arXiv:2308.00399v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00399
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#31070;&#32463;&#22270;&#34920;&#25688;&#35201;&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#39044;&#22788;&#29702;&#35757;&#32451;&#25968;&#25454;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#24187;&#35273;&#30340;&#20135;&#29983;&#65292;&#24182;&#36890;&#36807;&#32553;&#30701;&#36828;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#21644;&#28155;&#21152;&#19982;&#22270;&#34920;&#30456;&#20851;&#30340;&#20449;&#24687;&#26469;&#25552;&#39640;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#29983;&#25104;&#20013;&#30340;&#24187;&#35273;&#26159;&#25351;&#31995;&#32479;&#20135;&#29983;&#30340;&#25991;&#26412;&#26410;&#19982;&#36755;&#20837;&#20851;&#32852;&#12290;&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#31070;&#32463;&#22270;&#34920;&#25688;&#35201;&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#22270;&#34920;&#25688;&#35201;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#30446;&#26631;&#31471;&#32463;&#24120;&#21253;&#21547;&#39069;&#22806;&#30340;&#20449;&#24687;&#65292;&#23548;&#33268;&#24187;&#35273;&#30340;&#20135;&#29983;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;(NLI)&#30340;&#26041;&#27861;&#26469;&#39044;&#22788;&#29702;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#20154;&#24037;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#20943;&#23569;&#20102;&#24187;&#35273;&#30340;&#20135;&#29983;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#32553;&#30701;&#36755;&#20837;&#24207;&#21015;&#20013;&#30340;&#36828;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#28155;&#21152;&#19982;&#22270;&#34920;&#30456;&#20851;&#30340;&#20449;&#24687;&#65292;&#22914;&#26631;&#39064;&#21644;&#22270;&#20363;&#65292;&#21487;&#20197;&#25552;&#39640;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hallucinations in text generation occur when the system produces text that is not grounded in the input. In this work, we tackle the problem of hallucinations in neural chart summarization. Our analysis shows that the target side of chart summarization training datasets often contains additional information, leading to hallucinations. We propose a natural language inference (NLI) based method to preprocess the training data and show through human evaluation that our method significantly reduces hallucinations. We also found that shortening long-distance dependencies in the input sequence and adding chart-related information like title and legends improves the overall performance.
&lt;/p&gt;</description></item><item><title>&#22312;AIOps&#39046;&#22495;&#20013;&#65292;&#36890;&#36807;&#35843;&#26597;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#30740;&#31350;&#20154;&#21592;&#32508;&#36848;&#20102;&#20114;&#32852;&#32593;&#26381;&#21153;&#20013;&#22823;&#37327;&#30417;&#27979;KPI&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2308.00393</link><description>&lt;p&gt;
&#22312;AIOps&#39046;&#22495;&#20013;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey of Time Series Anomaly Detection Methods in the AIOps Domain. (arXiv:2308.00393v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00393
&lt;/p&gt;
&lt;p&gt;
&#22312;AIOps&#39046;&#22495;&#20013;&#65292;&#36890;&#36807;&#35843;&#26597;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#30740;&#31350;&#20154;&#21592;&#32508;&#36848;&#20102;&#20114;&#32852;&#32593;&#26381;&#21153;&#20013;&#22823;&#37327;&#30417;&#27979;KPI&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20114;&#32852;&#32593;&#30340;&#26381;&#21153;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20135;&#29983;&#20102;&#22823;&#37327;&#30340;&#30417;&#27979;&#20851;&#38190;&#24615;&#33021;&#25351;&#26631;(KPI)&#65292;&#36825;&#20123;&#25351;&#26631;&#20197;&#21333;&#21464;&#37327;&#25110;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#30340;&#24418;&#24335;&#21576;&#29616;&#12290;&#30417;&#27979;&#21644;&#20998;&#26512;&#36825;&#20123;&#26102;&#38388;&#24207;&#21015;&#23545;&#20110;&#30740;&#31350;&#20154;&#21592;&#12289;&#26381;&#21153;&#36816;&#33829;&#21830;&#21644;&#20540;&#29677;&#24037;&#31243;&#24072;&#26469;&#26816;&#27979;&#34920;&#26126;&#26381;&#21153;&#25925;&#38556;&#25110;&#37325;&#22823;&#20107;&#20214;&#30340;&#24322;&#24120;&#20540;&#25110;&#24322;&#24120;&#24773;&#20917;&#33267;&#20851;&#37325;&#35201;&#12290;&#20986;&#29616;&#20102;&#35768;&#22810;&#20808;&#36827;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#26469;&#35299;&#20915;&#21487;&#29992;&#24615;&#21644;&#24615;&#33021;&#38382;&#39064;&#12290;&#26412;&#32508;&#36848;&#20840;&#38754;&#27010;&#36848;&#20102;&#22312;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#33258;&#21160;&#21270;&#21644;&#20248;&#21270;&#25805;&#20316;&#27969;&#31243;&#30340;&#20449;&#24687;&#25216;&#26415;&#36816;&#33829;(AIOps)&#39046;&#22495;&#20013;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#65292;&#27492;&#22806;&#65292;&#20063;&#25506;&#35752;&#20102;&#22522;&#20110;&#26368;&#26032;&#36827;&#23637;&#30340;&#30495;&#23454;&#19990;&#30028;&#21644;&#19979;&#19968;&#20195;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Internet-based services have seen remarkable success, generating vast amounts of monitored key performance indicators (KPIs) as univariate or multivariate time series. Monitoring and analyzing these time series are crucial for researchers, service operators, and on-call engineers to detect outliers or anomalies indicating service failures or significant events. Numerous advanced anomaly detection methods have emerged to address availability and performance issues. This review offers a comprehensive overview of time series anomaly detection in Artificial Intelligence for IT operations (AIOps), which uses AI capabilities to automate and optimize operational workflows. Additionally, it explores future directions for real-world and next-generation time-series anomaly detection based on recent advancements.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#30340;&#21453;&#20107;&#23454;&#22270;&#21464;&#21387;&#22120;&#27169;&#22411;&#65288;CGT&#65289;&#65292;&#36890;&#36807;&#23454;&#20363;&#32423;&#35299;&#37322;&#22120;&#21644;&#25200;&#21160;&#25513;&#30721;&#29983;&#25104;&#22120;&#65292;&#21487;&#20197;&#33719;&#24471;&#20855;&#26377;&#35299;&#37322;&#24615;&#30340;&#39044;&#27979;&#27169;&#22411;&#21644;&#37325;&#35201;&#23376;&#22270;&#12290;</title><link>http://arxiv.org/abs/2308.00391</link><description>&lt;p&gt;
&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#30340;&#21453;&#20107;&#23454;&#22270;&#21464;&#21387;&#22120;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Graph Transformer for Traffic Flow Prediction. (arXiv:2308.00391v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#30340;&#21453;&#20107;&#23454;&#22270;&#21464;&#21387;&#22120;&#27169;&#22411;&#65288;CGT&#65289;&#65292;&#36890;&#36807;&#23454;&#20363;&#32423;&#35299;&#37322;&#22120;&#21644;&#25200;&#21160;&#25513;&#30721;&#29983;&#25104;&#22120;&#65292;&#21487;&#20197;&#33719;&#24471;&#20855;&#26377;&#35299;&#37322;&#24615;&#30340;&#39044;&#27979;&#27169;&#22411;&#21644;&#37325;&#35201;&#23376;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#65288;TFP&#65289;&#26159;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#65288;ITS&#65289;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#23427;&#27169;&#25311;&#20102;&#20132;&#36890;&#27969;&#37327;&#30340;&#28508;&#22312;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#65292;&#29992;&#20110;&#28508;&#22312;&#25317;&#22581;&#39044;&#27979;&#12290;&#26368;&#36817;&#30340;&#22522;&#20110;&#22270;&#30340;&#27169;&#22411;&#37319;&#29992;&#20102;&#22810;&#31181;&#27880;&#24847;&#26426;&#21046;&#65292;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#26041;&#27861;&#24448;&#24448;&#20250;&#20174;&#25968;&#25454;&#38598;&#20013;&#32487;&#25215;&#20559;&#24046;&#27169;&#24335;&#65292;&#24182;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#38376;&#20026;TFP&#35774;&#35745;&#30340;&#21453;&#20107;&#23454;&#22270;&#21464;&#21387;&#22120;&#65288;CGT&#65289;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#23454;&#20363;&#32423;&#35299;&#37322;&#22120;&#65288;&#20363;&#22914;&#65292;&#23547;&#25214;&#37325;&#35201;&#23376;&#22270;&#65289;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#23545;&#36755;&#20837;&#20256;&#24863;&#22120;&#29305;&#24449;&#22312;&#26102;&#38388;&#32500;&#24230;&#19978;&#21644;&#22270;&#21464;&#21387;&#22120;&#27169;&#22359;&#19978;&#30340;&#22270;&#32467;&#26500;&#36827;&#34892;&#25200;&#21160;&#30340;&#29983;&#25104;&#22120;&#65292;&#20197;&#33719;&#21462;&#31354;&#38388;&#21644;&#26102;&#38388;&#19978;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#12290;&#36890;&#36807;&#25628;&#32034;&#36755;&#20837;&#25968;&#25454;&#29305;&#24449;&#21644;&#22270;&#32467;&#26500;&#19978;&#30340;&#26368;&#20339;&#25200;&#21160;&#25513;&#30721;&#65292;&#25105;&#20204;&#21487;&#20197;&#33719;&#21462;&#31616;&#27905;&#32780;&#20027;&#23548;&#30340;&#25968;&#25454;&#25110;&#22270;&#36793;&#38142;&#25509;&#65292;&#20379;&#21518;&#32493;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic flow prediction (TFP) is a fundamental problem of the Intelligent Transportation System (ITS), as it models the latent spatial-temporal dependency of traffic flow for potential congestion prediction. Recent graph-based models with multiple kinds of attention mechanisms have achieved promising performance. However, existing methods for traffic flow prediction tend to inherit the bias pattern from the dataset and lack interpretability. To this end, we propose a Counterfactual Graph Transformer (CGT) model with an instance-level explainer (e.g., finding the important subgraphs) specifically designed for TFP. We design a perturbation mask generator over input sensor features at the time dimension and the graph structure on the graph transformer module to obtain spatial and temporal counterfactual explanations. By searching the optimal perturbation masks on the input data feature and graph structures, we can obtain the concise and dominant data or graph edge links for the subsequent
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#22788;&#29702;&#22312;&#32473;&#23450;&#27169;&#31946;&#29289;&#20307;&#35270;&#22270;&#26102;&#21487;&#33021;&#23384;&#22312;&#30340;&#29289;&#20307;&#37096;&#20998;&#30340;&#19981;&#30830;&#23450;&#21306;&#22495;&#39044;&#27979;&#38382;&#39064;&#12290;&#30740;&#31350;&#34920;&#26126;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#20316;&#20026;&#20219;&#20309;&#39044;&#27979;&#31354;&#38388;&#21344;&#29992;&#30340;&#26041;&#27861;&#30340;&#30452;&#25509;&#25193;&#23637;&#65292;&#36890;&#36807;&#21518;&#22788;&#29702;&#21344;&#29992;&#35780;&#20998;&#25110;&#30452;&#25509;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#25351;&#26631;&#26469;&#23454;&#29616;&#12290;&#36825;&#20123;&#26041;&#27861;&#19982;&#24050;&#30693;&#30340;&#27010;&#29575;&#24418;&#29366;&#23436;&#25104;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#20351;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#28145;&#24230;&#22270;&#20687;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2308.00377</link><description>&lt;p&gt;
&#24102;&#26377;&#19981;&#30830;&#23450;&#21306;&#22495;&#39044;&#27979;&#30340;&#24418;&#29366;&#23436;&#25104;
&lt;/p&gt;
&lt;p&gt;
Shape Completion with Prediction of Uncertain Regions. (arXiv:2308.00377v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00377
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#22788;&#29702;&#22312;&#32473;&#23450;&#27169;&#31946;&#29289;&#20307;&#35270;&#22270;&#26102;&#21487;&#33021;&#23384;&#22312;&#30340;&#29289;&#20307;&#37096;&#20998;&#30340;&#19981;&#30830;&#23450;&#21306;&#22495;&#39044;&#27979;&#38382;&#39064;&#12290;&#30740;&#31350;&#34920;&#26126;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#20316;&#20026;&#20219;&#20309;&#39044;&#27979;&#31354;&#38388;&#21344;&#29992;&#30340;&#26041;&#27861;&#30340;&#30452;&#25509;&#25193;&#23637;&#65292;&#36890;&#36807;&#21518;&#22788;&#29702;&#21344;&#29992;&#35780;&#20998;&#25110;&#30452;&#25509;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#25351;&#26631;&#26469;&#23454;&#29616;&#12290;&#36825;&#20123;&#26041;&#27861;&#19982;&#24050;&#30693;&#30340;&#27010;&#29575;&#24418;&#29366;&#23436;&#25104;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#20351;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#28145;&#24230;&#22270;&#20687;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24418;&#29366;&#23436;&#25104;&#65292;&#21363;&#20174;&#37096;&#20998;&#35266;&#27979;&#39044;&#27979;&#29289;&#20307;&#30340;&#23436;&#25972;&#20960;&#20309;&#24418;&#29366;&#65292;&#23545;&#20110;&#20960;&#20010;&#19979;&#28216;&#20219;&#21153;&#38750;&#24120;&#37325;&#35201;&#65292;&#23588;&#20854;&#26159;&#26426;&#22120;&#20154;&#25805;&#20316;&#12290;&#24403;&#22522;&#20110;&#29289;&#20307;&#24418;&#29366;&#37325;&#24314;&#36827;&#34892;&#35268;&#21010;&#25110;&#23454;&#38469;&#25235;&#21462;&#30340;&#39044;&#27979;&#26102;&#65292;&#25351;&#31034;&#20005;&#37325;&#20960;&#20309;&#19981;&#30830;&#23450;&#24615;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#12290;&#29305;&#21035;&#26159;&#22312;&#32473;&#23450;&#27169;&#31946;&#30340;&#29289;&#20307;&#35270;&#22270;&#26102;&#65292;&#22312;&#25972;&#20010;&#29289;&#20307;&#37096;&#20998;&#23384;&#22312; irreducible uncertainty &#30340;&#25193;&#23637;&#21306;&#22495;&#12290;&#20026;&#20102;&#22788;&#29702;&#36825;&#31181;&#37325;&#35201;&#24773;&#20917;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#26041;&#27861;&#26469;&#39044;&#27979;&#36825;&#20123;&#19981;&#30830;&#23450;&#21306;&#22495;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#21487;&#20197;&#20316;&#20026;&#39044;&#27979;&#23616;&#37096;&#31354;&#38388;&#21344;&#29992;&#30340;&#20219;&#20309;&#26041;&#27861;&#30340;&#30452;&#25509;&#25193;&#23637;&#65292;&#19968;&#31181;&#26159;&#36890;&#36807;&#21518;&#22788;&#29702;&#21344;&#29992;&#35780;&#20998;&#65292;&#21478;&#19968;&#31181;&#26159;&#36890;&#36807;&#30452;&#25509;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#25351;&#26631;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#26041;&#27861;&#19982;&#20004;&#31181;&#24050;&#30693;&#30340;&#27010;&#29575;&#24418;&#29366;&#23436;&#25104;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#29983;&#25104;&#20102;&#19968;&#20010;&#22522;&#20110;ShapeNet&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#30495;&#23454;&#28210;&#26579;&#30340;&#29289;&#20307;&#35270;&#22270;&#28145;&#24230;&#22270;&#20687;&#21450;&#20854;&#24102;&#26377;&#22320;&#38754;&#30495;&#20540;&#26631;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;
Shape completion, i.e., predicting the complete geometry of an object from a partial observation, is highly relevant for several downstream tasks, most notably robotic manipulation. When basing planning or prediction of real grasps on object shape reconstruction, an indication of severe geometric uncertainty is indispensable. In particular, there can be an irreducible uncertainty in extended regions about the presence of entire object parts when given ambiguous object views. To treat this important case, we propose two novel methods for predicting such uncertain regions as straightforward extensions of any method for predicting local spatial occupancy, one through postprocessing occupancy scores, the other through direct prediction of an uncertainty indicator. We compare these methods together with two known approaches to probabilistic shape completion. Moreover, we generate a dataset, derived from ShapeNet, of realistically rendered depth images of object views with ground-truth annot
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20302;&#31209;&#20998;&#35299;&#23398;&#20064;Green&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#39046;&#22495;&#25968;&#25454;&#21644;&#33945;&#29305;&#21345;&#27931;&#26679;&#26412;&#36827;&#34892;&#20998;&#21035;&#23398;&#20064;&#21644;&#31215;&#20998;&#36924;&#36817;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#24182;&#20445;&#25345;&#20102;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.00350</link><description>&lt;p&gt;
&#20351;&#29992;&#20302;&#31209;&#36817;&#20284;&#26377;&#25928;&#22320;&#23398;&#20064;Green&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Learning Green's Function Efficiently Using Low-Rank Approximations. (arXiv:2308.00350v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00350
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20302;&#31209;&#20998;&#35299;&#23398;&#20064;Green&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#39046;&#22495;&#25968;&#25454;&#21644;&#33945;&#29305;&#21345;&#27931;&#26679;&#26412;&#36827;&#34892;&#20998;&#21035;&#23398;&#20064;&#21644;&#31215;&#20998;&#36924;&#36817;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#24182;&#20445;&#25345;&#20102;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23398;&#20064;Green&#20989;&#25968;&#21487;&#20197;&#35299;&#20915;&#19981;&#21516;&#31867;&#22411;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#23398;&#20064;Green&#20989;&#25968;&#30340;&#19968;&#20010;&#23454;&#38469;&#38480;&#21046;&#26159;&#37325;&#22797;&#30340;&#35745;&#31639;&#23494;&#38598;&#30340;&#33945;&#29305;&#21345;&#27931;&#31215;&#20998;&#36924;&#36817;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#36890;&#36807;&#20302;&#31209;&#20998;&#35299;&#23398;&#20064;Green&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#39046;&#22495;&#25968;&#25454;&#36827;&#34892;&#35780;&#20272;&#21644;&#33945;&#29305;&#21345;&#27931;&#26679;&#26412;&#36827;&#34892;&#31215;&#20998;&#36924;&#36817;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#20887;&#20313;&#35745;&#31639;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;MOD-Net&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;&#35745;&#31639;&#26102;&#38388;&#65292;&#21516;&#26102;&#22312;&#20934;&#30830;&#24615;&#19978;&#19982;PINNs&#21644;MOD-Net&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning the Green's function using deep learning models enables to solve different classes of partial differential equations. A practical limitation of using deep learning for the Green's function is the repeated computationally expensive Monte-Carlo integral approximations. We propose to learn the Green's function by low-rank decomposition, which results in a novel architecture to remove redundant computations by separate learning with domain data for evaluation and Monte-Carlo samples for integral approximation. Using experiments we show that the proposed method improves computational time compared to MOD-Net while achieving comparable accuracy compared to both PINNs and MOD-Net.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#40065;&#26834;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21160;&#24577;&#38598;&#25104;&#36873;&#25321;&#25216;&#26415;&#22312;&#27169;&#22411;&#23618;&#38754;&#19978;&#25913;&#21892;&#27169;&#22411;&#30340;&#38450;&#24481;&#20027;&#21160;&#24615;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#35757;&#32451;&#38454;&#27573;&#24341;&#20837;Dirichlet&#20998;&#24067;&#20316;&#20026;&#23376;&#27169;&#22411;&#39044;&#27979;&#20998;&#24067;&#30340;&#20808;&#39564;&#65292;&#24182;&#22312;&#36731;&#37327;&#32423;&#23376;&#27169;&#22411;&#19979;&#24341;&#20837;&#21442;&#25968;&#31354;&#38388;&#30340;&#22810;&#26679;&#24615;&#32422;&#26463;&#65292;&#26500;&#24314;&#22791;&#36873;&#30340;&#38598;&#25104;&#27169;&#22411;&#31354;&#38388;&#12290;&#22312;&#27979;&#35797;&#38454;&#27573;&#65292;&#21160;&#24577;&#36873;&#21462;&#29305;&#23450;&#30340;&#23376;&#27169;&#22411;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.00346</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#21160;&#24577;&#38598;&#25104;&#36873;&#25321;&#26041;&#27861;&#29992;&#20110;&#23545;&#25239;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Dynamic ensemble selection based on Deep Neural Network Uncertainty Estimation for Adversarial Robustness. (arXiv:2308.00346v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00346
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#40065;&#26834;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21160;&#24577;&#38598;&#25104;&#36873;&#25321;&#25216;&#26415;&#22312;&#27169;&#22411;&#23618;&#38754;&#19978;&#25913;&#21892;&#27169;&#22411;&#30340;&#38450;&#24481;&#20027;&#21160;&#24615;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#35757;&#32451;&#38454;&#27573;&#24341;&#20837;Dirichlet&#20998;&#24067;&#20316;&#20026;&#23376;&#27169;&#22411;&#39044;&#27979;&#20998;&#24067;&#30340;&#20808;&#39564;&#65292;&#24182;&#22312;&#36731;&#37327;&#32423;&#23376;&#27169;&#22411;&#19979;&#24341;&#20837;&#21442;&#25968;&#31354;&#38388;&#30340;&#22810;&#26679;&#24615;&#32422;&#26463;&#65292;&#26500;&#24314;&#22791;&#36873;&#30340;&#38598;&#25104;&#27169;&#22411;&#31354;&#38388;&#12290;&#22312;&#27979;&#35797;&#38454;&#27573;&#65292;&#21160;&#24577;&#36873;&#21462;&#29305;&#23450;&#30340;&#23376;&#27169;&#22411;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#20687;&#35782;&#21035;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25928;&#26524;&#65292;&#20294;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#38754;&#20020;&#30528;&#22823;&#37327;&#25968;&#25454;&#19981;&#30830;&#23450;&#24615;&#30340;&#35782;&#21035;&#40065;&#26834;&#24615;&#34180;&#24369;&#30340;&#38382;&#39064;&#12290;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#26159;&#30001;&#29615;&#22659;&#20013;&#19981;&#21487;&#36991;&#20813;&#30340;&#22122;&#22768;&#20197;&#21450;&#21487;&#33021;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#24341;&#36215;&#30340;&#12290;&#21160;&#24577;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#25913;&#21892;&#23545;&#25239;&#24615;&#31034;&#20363;&#25915;&#38450;&#31454;&#36187;&#20013;&#30340;&#38450;&#24481;&#20027;&#21160;&#24615;&#12290;&#19982;&#20197;&#21069;&#20381;&#36182;&#20110;&#36755;&#20837;&#25110;&#20915;&#31574;&#30340;&#21160;&#24577;&#26041;&#27861;&#19981;&#21516;&#65292;&#26412;&#30740;&#31350;&#36890;&#36807;&#21160;&#24577;&#38598;&#25104;&#36873;&#25321;&#25216;&#26415;&#22312;&#27169;&#22411;&#23618;&#38754;&#19978;&#25506;&#32034;&#21160;&#24577;&#23646;&#24615;&#65292;&#20197;&#36827;&#19968;&#27493;&#20445;&#25252;&#27169;&#22411;&#20813;&#21463;&#30333;&#30418;&#25915;&#20987;&#24182;&#25552;&#39640;&#40065;&#26834;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#35757;&#32451;&#38454;&#27573;&#65292;&#23558;Dirichlet&#20998;&#24067;&#20316;&#20026;&#23376;&#27169;&#22411;&#39044;&#27979;&#20998;&#24067;&#30340;&#20808;&#39564;&#65292;&#24182;&#22312;&#36731;&#37327;&#32423;&#23376;&#27169;&#22411;&#19979;&#24341;&#20837;&#21442;&#25968;&#31354;&#38388;&#30340;&#22810;&#26679;&#24615;&#32422;&#26463;&#65292;&#20197;&#26500;&#24314;&#22791;&#36873;&#38598;&#25104;&#27169;&#22411;&#31354;&#38388;&#12290;&#22312;&#27979;&#35797;&#38454;&#27573;&#65292;&#29305;&#23450;&#30340;&#23376;&#27169;&#22411;&#34987;&#21160;&#24577;&#36873;&#21462;
&lt;/p&gt;
&lt;p&gt;
The deep neural network has attained significant efficiency in image recognition. However, it has vulnerable recognition robustness under extensive data uncertainty in practical applications. The uncertainty is attributed to the inevitable ambient noise and, more importantly, the possible adversarial attack. Dynamic methods can effectively improve the defense initiative in the arms race of attack and defense of adversarial examples. Different from the previous dynamic method depend on input or decision, this work explore the dynamic attributes in model level through dynamic ensemble selection technology to further protect the model from white-box attacks and improve the robustness. Specifically, in training phase the Dirichlet distribution is apply as prior of sub-models' predictive distribution, and the diversity constraint in parameter space is introduced under the lightweight sub-models to construct alternative ensembel model spaces. In test phase, the certain sub-models are dynamic
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#31639;&#27861;&#20844;&#24179;&#24615;&#30417;&#25511;&#25193;&#23637;&#21040;&#37096;&#20998;&#35266;&#23519;&#21040;&#30340;&#39532;&#23572;&#21487;&#22827;&#38142;&#27169;&#22411;&#30340;&#31995;&#32479;&#65292;&#24182;&#19988;&#21487;&#20197;&#30417;&#25511;&#21253;&#21547;&#23545;&#20107;&#20214;&#24207;&#21015;&#19978;&#25968;&#20540;&#20989;&#25968;&#30340;&#26399;&#26395;&#20540;&#30340;&#31639;&#26415;&#34920;&#36798;&#24335;&#30340;&#20844;&#24179;&#24615;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.00341</link><description>&lt;p&gt;
&#22312;&#37096;&#20998;&#35266;&#27979;&#26465;&#20214;&#19979;&#30417;&#25511;&#31639;&#27861;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Monitoring Algorithmic Fairness under Partial Observations. (arXiv:2308.00341v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00341
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#31639;&#27861;&#20844;&#24179;&#24615;&#30417;&#25511;&#25193;&#23637;&#21040;&#37096;&#20998;&#35266;&#23519;&#21040;&#30340;&#39532;&#23572;&#21487;&#22827;&#38142;&#27169;&#22411;&#30340;&#31995;&#32479;&#65292;&#24182;&#19988;&#21487;&#20197;&#30417;&#25511;&#21253;&#21547;&#23545;&#20107;&#20214;&#24207;&#21015;&#19978;&#25968;&#20540;&#20989;&#25968;&#30340;&#26399;&#26395;&#20540;&#30340;&#31639;&#26415;&#34920;&#36798;&#24335;&#30340;&#20844;&#24179;&#24615;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#36719;&#20214;&#22312;&#20915;&#31574;&#20013;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#22810;&#22320;&#24433;&#21709;&#20154;&#31867;&#65292;&#23427;&#20204;&#22312;&#20915;&#31574;&#36807;&#31243;&#20013;&#20445;&#25345;&#20844;&#24179;&#21644;&#26080;&#20559;&#30340;&#35201;&#27714;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#34917;&#20805;&#35774;&#35745;&#26102;&#30340;&#20559;&#24046;&#32531;&#35299;&#25514;&#26045;&#65292;&#36817;&#26399;&#24341;&#20837;&#20102;&#19968;&#20123;&#36816;&#34892;&#26102;&#39564;&#35777;&#25216;&#26415;&#26469;&#30417;&#25511;&#37096;&#32626;&#31995;&#32479;&#30340;&#31639;&#27861;&#20844;&#24179;&#24615;&#12290;&#20043;&#21069;&#30340;&#30417;&#25511;&#25216;&#26415;&#20551;&#35774;&#23545;&#65288;&#26410;&#30693;&#30340;&#65289;&#34987;&#30417;&#25511;&#31995;&#32479;&#30340;&#29366;&#24577;&#20855;&#26377;&#23436;&#20840;&#21487;&#35266;&#27979;&#24615;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#21482;&#33021;&#30417;&#25511;&#34987;&#25351;&#23450;&#20026;&#19981;&#21516;&#20107;&#20214;&#30340;&#27010;&#29575;&#30340;&#31639;&#26415;&#34920;&#36798;&#24335;&#30340;&#20844;&#24179;&#24615;&#23646;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#20844;&#24179;&#24615;&#30417;&#25511;&#25193;&#23637;&#21040;&#37096;&#20998;&#35266;&#23519;&#21040;&#30340;&#39532;&#23572;&#21487;&#22827;&#38142;&#65288;POMC&#65289;&#27169;&#22411;&#30340;&#31995;&#32479;&#65292;&#24182;&#19988;&#38024;&#23545;&#21253;&#21547;&#23545;&#20107;&#20214;&#24207;&#21015;&#19978;&#30340;&#25968;&#20540;&#20989;&#25968;&#30340;&#26399;&#26395;&#20540;&#30340;&#31639;&#26415;&#34920;&#36798;&#24335;&#30340;&#35268;&#33539;&#12290;&#25105;&#20204;&#20165;&#20570;&#20986;&#30340;&#20551;&#35774;&#26159;&#22522;&#30784;POMC&#26159;&#38750;&#21608;&#26399;&#24615;&#30340;&#24182;&#19988;&#36215;&#22987;&#20110;&#31283;&#23450;&#20998;&#24067;&#65292;&#19988;&#23545;&#20110;&#20854;&#28151;&#21512;&#26102;&#38388;&#26377;&#19968;&#20010;&#24050;&#30693;&#30340;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
As AI and machine-learned software are used increasingly for making decisions that affect humans, it is imperative that they remain fair and unbiased in their decisions. To complement design-time bias mitigation measures, runtime verification techniques have been introduced recently to monitor the algorithmic fairness of deployed systems. Previous monitoring techniques assume full observability of the states of the (unknown) monitored system. Moreover, they can monitor only fairness properties that are specified as arithmetic expressions over the probabilities of different events. In this work, we extend fairness monitoring to systems modeled as partially observed Markov chains (POMC), and to specifications containing arithmetic expressions over the expected values of numerical functions on event sequences. The only assumptions we make are that the underlying POMC is aperiodic and starts in the stationary distribution, with a bound on its mixing time being known. These assumptions enab
&lt;/p&gt;</description></item><item><title>&#38024;&#23545;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38408;&#20540;&#24863;&#30693;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#21464;&#37327;&#20998;&#37197;&#29575;&#26469;&#25552;&#39640;&#21487;&#34892;&#35299;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2308.00327</link><description>&lt;p&gt;
&#38024;&#23545;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#29983;&#25104;&#21487;&#34892;&#35299;&#30340;&#38408;&#20540;&#24863;&#30693;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Threshold-aware Learning to Generate Feasible Solutions for Mixed Integer Programs. (arXiv:2308.00327v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00327
&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38408;&#20540;&#24863;&#30693;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#21464;&#37327;&#20998;&#37197;&#29575;&#26469;&#25552;&#39640;&#21487;&#34892;&#35299;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26377;&#38480;&#26102;&#38388;&#20869;&#25214;&#21040;&#39640;&#36136;&#37327;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#30340;&#21487;&#34892;&#35299;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#20854;&#20855;&#26377;&#31163;&#25955;&#30340;&#29305;&#24615;&#12290;&#26368;&#36817;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#34987;&#29992;&#26469;&#35299;&#20915;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#12290;&#31070;&#32463;&#32593;&#32476;&#19979;&#28508; (Neural diving&#65292;ND) &#26159;&#19968;&#31181;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#38382;&#39064;&#20013;&#30340;&#37096;&#20998;&#31163;&#25955;&#21464;&#37327;&#36171;&#20540;&#12290;&#28982;&#32780;&#65292;ND&#30340;&#19968;&#20010;&#20027;&#35201;&#32570;&#28857;&#26159;&#26426;&#22120;&#23398;&#20064;&#21644;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#30446;&#26631;&#20043;&#38388;&#23384;&#22312;&#36739;&#22823;&#24046;&#24322;&#65292;&#21363;&#21464;&#37327;&#20540;&#20998;&#31867;&#20934;&#30830;&#24230;&#19982;&#21407;&#22987;&#35299;&#30340;&#30028;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#29305;&#23450;&#33539;&#22260;&#30340;&#21464;&#37327;&#20998;&#37197;&#29575;&#65288;&#35206;&#30422;&#29575;&#65289;&#20250;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#21487;&#34892;&#35299;&#65292;&#25105;&#20204;&#24314;&#35758;&#20248;&#21270;&#35206;&#30422;&#29575;&#26469;&#24357;&#34917;&#23398;&#20064;&#21644;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#30446;&#26631;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20107;&#21518;&#26041;&#27861;&#21644;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#20248;&#21270;&#35206;&#30422;&#29575;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#19968;&#20010;&#20851;&#38190;&#24605;&#24819;&#26159;&#20849;&#21516;&#23398;&#20064;&#38480;&#21046;&#35206;&#30422;&#29575;&#25628;&#32034;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finding a high-quality feasible solution to a combinatorial optimization (CO) problem in a limited time is challenging due to its discrete nature. Recently, there has been an increasing number of machine learning (ML) methods for addressing CO problems. Neural diving (ND) is one of the learning-based approaches to generating partial discrete variable assignments in Mixed Integer Programs (MIP), a framework for modeling CO problems. However, a major drawback of ND is a large discrepancy between the ML and MIP objectives, i.e., variable value classification accuracy over primal bound. Our study investigates that a specific range of variable assignment rates (coverage) yields high-quality feasible solutions, where we suggest optimizing the coverage bridges the gap between the learning and MIP objectives. Consequently, we introduce a post-hoc method and a learning-based approach for optimizing the coverage. A key idea of our approach is to jointly learn to restrict the coverage search spac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#21033;&#29992;&#20849;&#20139;&#32467;&#26500;&#30340;DQN&#32534;&#30721;&#22120;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#65292;&#35813;&#27169;&#22411;&#22312;&#19981;&#21516;&#20219;&#21153;&#21644;&#29615;&#22659;&#20013;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#30340;&#23398;&#20064;&#21644;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#22312;&#22810;&#20010;&#28216;&#25103;&#29615;&#22659;&#20013;&#65292;&#35813;&#27169;&#22411;&#30340;&#24179;&#22343;&#22238;&#21512;&#22870;&#21169;&#20026;46.16&#65292;&#22312;20,000&#27425;&#22238;&#21512;&#20869;&#29978;&#33267;&#36229;&#36807;&#20102;&#20154;&#31867;&#27700;&#24179;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2308.00318</link><description>&lt;p&gt;
&#20687;&#32032;&#21040;&#31574;&#30053;&#65306;&#29992;&#20110;&#20869;&#37096;&#21644;&#36328;&#28216;&#25103;&#24378;&#21270;&#23398;&#20064;&#30340;DQN&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Pixel to policy: DQN Encoders for within &amp; cross-game reinforcement learning. (arXiv:2308.00318v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00318
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#21033;&#29992;&#20849;&#20139;&#32467;&#26500;&#30340;DQN&#32534;&#30721;&#22120;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#65292;&#35813;&#27169;&#22411;&#22312;&#19981;&#21516;&#20219;&#21153;&#21644;&#29615;&#22659;&#20013;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#30340;&#23398;&#20064;&#21644;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#22312;&#22810;&#20010;&#28216;&#25103;&#29615;&#22659;&#20013;&#65292;&#35813;&#27169;&#22411;&#30340;&#24179;&#22343;&#22238;&#21512;&#22870;&#21169;&#20026;46.16&#65292;&#22312;20,000&#27425;&#22238;&#21512;&#20869;&#29978;&#33267;&#36229;&#36807;&#20102;&#20154;&#31867;&#27700;&#24179;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#21487;&#20197;&#24212;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#21644;&#29615;&#22659;&#12290;&#35768;&#22810;&#36825;&#20123;&#29615;&#22659;&#20855;&#26377;&#30456;&#20284;&#30340;&#20849;&#20139;&#32467;&#26500;&#65292;&#21487;&#20197;&#21033;&#29992;&#36825;&#20123;&#32467;&#26500;&#25552;&#39640;&#20854;&#20182;&#20219;&#21153;&#19978;&#30340;RL&#24615;&#33021;&#12290;&#36801;&#31227;&#23398;&#20064;&#21487;&#20197;&#21033;&#29992;&#36825;&#31181;&#20849;&#20139;&#32467;&#26500;&#65292;&#36890;&#36807;&#23398;&#20064;&#21487;&#22312;&#19981;&#21516;&#20219;&#21153;&#21644;&#29615;&#22659;&#20043;&#38388;&#36716;&#31227;&#30340;&#31574;&#30053;&#65292;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#23398;&#20064;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#25913;&#36827;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#24182;&#27604;&#36739;&#20102;&#20174;&#38646;&#24320;&#22987;&#35757;&#32451;&#30340;RL&#27169;&#22411;&#21644;&#36801;&#31227;&#23398;&#20064;&#19981;&#21516;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#22312;&#22810;&#20010;&#28216;&#25103;&#29615;&#22659;&#20013;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#26088;&#22312;&#24320;&#21457;&#36890;&#29992;&#30340;&#28216;&#25103;&#20195;&#29702;&#20197;&#21450;&#20351;&#29992;DQN&#23545;&#39044;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#65292;&#24182;&#22312;&#30456;&#21516;&#25110;&#19981;&#21516;&#28216;&#25103;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;DQN&#27169;&#22411;&#22312;&#24179;&#22343;&#22238;&#21512;&#22870;&#21169;&#19978;&#36798;&#21040;&#20102;46.16&#65292;&#29978;&#33267;&#36229;&#36807;&#20102;&#20154;&#31867;&#27700;&#24179;&#34920;&#29616;&#65292;&#20165;&#20165;&#20351;&#29992;&#20102;20,000&#27425;&#22238;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning can be applied to various tasks, and environments. Many of these environments have a similar shared structure, which can be exploited to improve RL performance on other tasks. Transfer learning can be used to take advantage of this shared structure, by learning policies that are transferable across different tasks and environments and can lead to more efficient learning as well as improved performance on a wide range of tasks. This work explores as well as compares the performance between RL models being trained from the scratch and on different approaches of transfer learning. Additionally, the study explores the performance of a model trained on multiple game environments, with the goal of developing a universal game-playing agent as well as transfer learning a pre-trained encoder using DQN, and training it on the same game or a different game. Our DQN model achieves a mean episode reward of 46.16 which even beats the human-level performance with merely 20k epi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#37325;&#31283;&#20581;&#30340;&#23454;&#20363;&#37325;&#26032;&#21152;&#26435;&#23545;&#25239;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#65288;DRO&#65289;&#25216;&#26415;&#33719;&#24471;&#37325;&#35201;&#24615;&#26435;&#37325;&#65292;&#24182;&#22312;&#26368;&#33030;&#24369;&#30340;&#31034;&#20363;&#19978;&#25552;&#39640;&#31283;&#20581;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.00311</link><description>&lt;p&gt;
&#21452;&#37325;&#31283;&#20581;&#30340;&#23454;&#20363;&#37325;&#26032;&#21152;&#26435;&#23545;&#25239;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Doubly Robust Instance-Reweighted Adversarial Training. (arXiv:2308.00311v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00311
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#37325;&#31283;&#20581;&#30340;&#23454;&#20363;&#37325;&#26032;&#21152;&#26435;&#23545;&#25239;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#65288;DRO&#65289;&#25216;&#26415;&#33719;&#24471;&#37325;&#35201;&#24615;&#26435;&#37325;&#65292;&#24182;&#22312;&#26368;&#33030;&#24369;&#30340;&#31034;&#20363;&#19978;&#25552;&#39640;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26377;&#38480;&#30340;&#27169;&#22411;&#23481;&#37327;&#19979;&#65292;&#20026;&#23545;&#25239;&#24615;&#25968;&#25454;&#20998;&#37197;&#37325;&#35201;&#24615;&#26435;&#37325;&#22312;&#35757;&#32451;&#23545;&#25239;&#24615;&#31283;&#20581;&#32593;&#32476;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#23454;&#20363;&#37325;&#26032;&#21152;&#26435;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#20110;&#21551;&#21457;&#24335;&#31639;&#27861;&#21644;/&#25110;&#20960;&#20309;&#35299;&#37322;&#26469;&#30830;&#23450;&#36825;&#20123;&#37325;&#35201;&#24615;&#26435;&#37325;&#65292;&#20351;&#24471;&#36825;&#20123;&#31639;&#27861;&#32570;&#20047;&#20005;&#26684;&#30340;&#29702;&#35770;&#35299;&#37322;/&#20445;&#35777;&#12290;&#27492;&#22806;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23545;&#25239;&#35757;&#32451;&#22312;&#35757;&#32451;&#20998;&#24067;&#20013;&#30340;&#31283;&#20581;&#24615;&#34920;&#29616;&#38750;&#22343;&#21248;&#65292;&#20363;&#22914;&#65292;&#26576;&#20123;&#31867;&#21035;&#30340;&#25968;&#25454;&#28857;&#27604;&#20854;&#20182;&#31867;&#21035;&#26356;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#37325;&#31283;&#20581;&#30340;&#23454;&#20363;&#37325;&#26032;&#21152;&#26435;&#23545;&#25239;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#25506;&#32034;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#65288;DRO&#65289;&#25216;&#26415;&#26469;&#33719;&#24471;&#37325;&#35201;&#24615;&#26435;&#37325;&#65292;&#24182;&#22312;&#26368;&#33030;&#24369;&#30340;&#31034;&#20363;&#19978;&#25552;&#39640;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Assigning importance weights to adversarial data has achieved great success in training adversarially robust networks under limited model capacity. However, existing instance-reweighted adversarial training (AT) methods heavily depend on heuristics and/or geometric interpretations to determine those importance weights, making these algorithms lack rigorous theoretical justification/guarantee. Moreover, recent research has shown that adversarial training suffers from a severe non-uniform robust performance across the training distribution, e.g., data points belonging to some classes can be much more vulnerable to adversarial attacks than others. To address both issues, in this paper, we propose a novel doubly-robust instance reweighted AT framework, which allows to obtain the importance weights via exploring distributionally robust optimization (DRO) techniques, and at the same time boosts the robustness on the most vulnerable examples. In particular, our importance weights are obtained
&lt;/p&gt;</description></item><item><title>GradOrth&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35745;&#31639;&#26799;&#24230;&#22312;&#20869;&#37096;&#20998;&#24067;&#25968;&#25454;&#30340;&#20302;&#31209;&#23376;&#31354;&#38388;&#20013;&#30340;&#25237;&#24433;&#33539;&#25968;&#26469;&#26816;&#27979;&#22806;&#37096;&#20998;&#24067;&#25968;&#25454;&#12290;&#36825;&#31181;&#26041;&#27861;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.00310</link><description>&lt;p&gt;
GradOrth:&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#36890;&#36807;&#26799;&#24230;&#27491;&#20132;&#25237;&#24433;&#36827;&#34892;&#22806;&#37096;&#20998;&#24067;&#26816;&#27979;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
GradOrth: A Simple yet Efficient Out-of-Distribution Detection with Orthogonal Projection of Gradients. (arXiv:2308.00310v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00310
&lt;/p&gt;
&lt;p&gt;
GradOrth&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35745;&#31639;&#26799;&#24230;&#22312;&#20869;&#37096;&#20998;&#24067;&#25968;&#25454;&#30340;&#20302;&#31209;&#23376;&#31354;&#38388;&#20013;&#30340;&#25237;&#24433;&#33539;&#25968;&#26469;&#26816;&#27979;&#22806;&#37096;&#20998;&#24067;&#25968;&#25454;&#12290;&#36825;&#31181;&#26041;&#27861;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#26816;&#27979;&#22806;&#37096;&#20998;&#24067;&#65288;OOD&#65289;&#25968;&#25454;&#23545;&#20110;&#30830;&#20445;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#23433;&#20840;&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;OOD&#26816;&#27979;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#29305;&#24449;&#22270;&#25110;&#23436;&#25972;&#26799;&#24230;&#31354;&#38388;&#20449;&#24687;&#26469;&#25512;&#23548;OOD&#20998;&#25968;&#65292;&#24573;&#35270;&#20102;&#39044;&#35757;&#32451;&#32593;&#32476;&#20013;&#26368;&#37325;&#35201;&#30340;&#21442;&#25968;&#22312;&#20869;&#37096;&#20998;&#24067;&#65288;ID&#65289;&#25968;&#25454;&#20013;&#30340;&#20316;&#29992;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GradOrth&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#20415;&#22522;&#20110;&#19968;&#20010;&#26377;&#36259;&#30340;&#35266;&#23519;&#32467;&#26524;&#65292;&#21363;&#29992;&#20110;&#35782;&#21035;OOD&#25968;&#25454;&#30340;&#37325;&#35201;&#29305;&#24449;&#20301;&#20110;&#20869;&#37096;&#20998;&#24067;&#65288;ID&#65289;&#25968;&#25454;&#30340;&#20302;&#31209;&#23376;&#31354;&#38388;&#20013;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#35745;&#31639;&#22312;&#20869;&#37096;&#20998;&#24067;&#25968;&#25454;&#20013;&#34987;&#35748;&#20026;&#37325;&#35201;&#30340;&#23376;&#31354;&#38388;&#20013;&#30340;&#26799;&#24230;&#25237;&#24433;&#33539;&#25968;&#26469;&#35782;&#21035;OOD&#25968;&#25454;&#12290;&#22823;&#30340;&#27491;&#20132;&#25237;&#24433;&#20540;&#65288;&#21363;&#23567;&#30340;&#25237;&#24433;&#20540;&#65289;&#34920;&#26126;&#26679;&#26412;&#20026;OOD&#65292;&#22240;&#20026;&#23427;&#25429;&#25417;&#21040;&#20102;ID&#25968;&#25454;&#30340;&#24369;&#30456;&#20851;&#24615;&#12290; &#36825;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#23637;&#31034;&#20102;&#19968;&#20010;&#20540;&#24471;&#27880;&#24847;&#30340;&#21019;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting out-of-distribution (OOD) data is crucial for ensuring the safe deployment of machine learning models in real-world applications. However, existing OOD detection approaches primarily rely on the feature maps or the full gradient space information to derive OOD scores neglecting the role of most important parameters of the pre-trained network over in-distribution (ID) data. In this study, we propose a novel approach called GradOrth to facilitate OOD detection based on one intriguing observation that the important features to identify OOD data lie in the lower-rank subspace of in-distribution (ID) data. In particular, we identify OOD data by computing the norm of gradient projection on the subspaces considered important for the in-distribution data. A large orthogonal projection value (i.e. a small projection value) indicates the sample as OOD as it captures a weak correlation of the ID data. This simple yet effective method exhibits outstanding performance, showcasing a notabl
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20851;&#27880;&#20110;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#20013;&#30340;&#35780;&#20272;&#24230;&#37327;&#12290;&#36890;&#36807;&#23558;&#28304;&#20934;&#30830;&#29575;&#32435;&#20837;&#24230;&#37327;&#20013;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;MLP&#20998;&#31867;&#22120;&#36827;&#34892;&#25913;&#36827;&#65292;&#35813;&#30740;&#31350;&#35299;&#20915;&#20102;&#21407;&#26377;&#24230;&#37327;&#30340;&#19981;&#36275;&#20043;&#22788;&#65292;&#24182;&#23558;&#20854;&#19982;&#25968;&#25454;&#22686;&#24378;&#30456;&#32467;&#21512;&#65292;&#25913;&#36827;&#20102;&#27169;&#22411;&#30340;&#36136;&#37327;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2308.00287</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#35780;&#20272;&#24230;&#37327;&#29992;&#20110;&#23454;&#36341;&#21644;&#33258;&#21160;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Study of Unsupervised Evaluation Metrics for Practical and Automatic Domain Adaptation. (arXiv:2308.00287v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00287
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20851;&#27880;&#20110;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#20013;&#30340;&#35780;&#20272;&#24230;&#37327;&#12290;&#36890;&#36807;&#23558;&#28304;&#20934;&#30830;&#29575;&#32435;&#20837;&#24230;&#37327;&#20013;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;MLP&#20998;&#31867;&#22120;&#36827;&#34892;&#25913;&#36827;&#65292;&#35813;&#30740;&#31350;&#35299;&#20915;&#20102;&#21407;&#26377;&#24230;&#37327;&#30340;&#19981;&#36275;&#20043;&#22788;&#65292;&#24182;&#23558;&#20854;&#19982;&#25968;&#25454;&#22686;&#24378;&#30456;&#32467;&#21512;&#65292;&#25913;&#36827;&#20102;&#27169;&#22411;&#30340;&#36136;&#37327;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;UDA&#65289;&#26041;&#27861;&#21487;&#20197;&#22312;&#27809;&#26377;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#23558;&#27169;&#22411;&#36716;&#31227;&#21040;&#30446;&#26631;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#19968;&#20010;&#26631;&#35760;&#30340;&#30446;&#26631;&#39564;&#35777;&#38598;&#29992;&#20110;&#36229;&#21442;&#25968;&#35843;&#20248;&#21644;&#27169;&#22411;&#36873;&#25321;&#12290;&#26412;&#25991;&#26088;&#22312;&#23547;&#25214;&#19968;&#31181;&#33021;&#22815;&#35780;&#20272;&#36716;&#31227;&#27169;&#22411;&#36136;&#37327;&#30340;&#35780;&#20272;&#24230;&#37327;&#65292;&#32780;&#26080;&#38656;&#35775;&#38382;&#30446;&#26631;&#39564;&#35777;&#26631;&#31614;&#12290;&#25105;&#20204;&#39318;&#20808;&#22522;&#20110;&#27169;&#22411;&#39044;&#27979;&#30340;&#20114;&#20449;&#24687;&#30340;&#24230;&#37327;&#12290;&#36890;&#36807;&#23454;&#35777;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#36825;&#20010;&#24230;&#37327;&#30340;&#19977;&#20010;&#26222;&#36941;&#38382;&#39064;&#65306;1&#65289;&#23427;&#27809;&#26377;&#32771;&#34385;&#28304;&#32467;&#26500;&#65307;2&#65289;&#23427;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#65307;3&#65289;&#23427;&#26080;&#27861;&#26816;&#27979;&#21040;&#30001;&#20110;&#28304;&#21644;&#30446;&#26631;&#29305;&#24449;&#36807;&#24230;&#23545;&#40784;&#23548;&#33268;&#30340;&#36127;&#36801;&#31227;&#12290;&#20026;&#20102;&#35299;&#20915;&#21069;&#20004;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#28304;&#20934;&#30830;&#29575;&#32435;&#20837;&#24230;&#37327;&#20013;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20445;&#25345;&#29420;&#31435;&#30340;&#26032;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#20998;&#31867;&#22120;&#65292;&#20174;&#32780;&#26174;&#33879;&#25913;&#36827;&#20102;&#32467;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#26368;&#21518;&#19968;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#22686;&#24378;&#30340;&#24230;&#37327;&#19982;&#25968;&#25454;&#22686;&#24378;&#30456;&#32467;&#21512;&#65292;&#24471;&#21040;&#19968;&#20010;n
&lt;/p&gt;
&lt;p&gt;
Unsupervised domain adaptation (UDA) methods facilitate the transfer of models to target domains without labels. However, these methods necessitate a labeled target validation set for hyper-parameter tuning and model selection. In this paper, we aim to find an evaluation metric capable of assessing the quality of a transferred model without access to target validation labels. We begin with the metric based on mutual information of the model prediction. Through empirical analysis, we identify three prevalent issues with this metric: 1) It does not account for the source structure. 2) It can be easily attacked. 3) It fails to detect negative transfer caused by the over-alignment of source and target features. To address the first two issues, we incorporate source accuracy into the metric and employ a new MLP classifier that is held out during training, significantly improving the result. To tackle the final issue, we integrate this enhanced metric with data augmentation, resulting in a n
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#27169;&#22411;&#36873;&#25321;&#21644;&#36125;&#21494;&#26031;&#20248;&#21270;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#20197;&#26356;&#24555;&#22320;&#36798;&#21040;&#20989;&#25968;&#30340;&#26368;&#20248;&#35299;&#12290;</title><link>http://arxiv.org/abs/2308.00285</link><description>&lt;p&gt;
&#36890;&#36807;&#36229;&#36125;&#21494;&#26031;&#20248;&#21270;&#36827;&#34892;&#39044;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Predictive Modeling through Hyper-Bayesian Optimization. (arXiv:2308.00285v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00285
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#27169;&#22411;&#36873;&#25321;&#21644;&#36125;&#21494;&#26031;&#20248;&#21270;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#20197;&#26356;&#24555;&#22320;&#36798;&#21040;&#20989;&#25968;&#30340;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#36873;&#25321;&#26159;&#22522;&#20110;&#27169;&#22411;&#30340;&#20248;&#21270;&#25216;&#26415;&#65288;&#22914;&#36125;&#21494;&#26031;&#20248;&#21270;&#65289;&#26368;&#20851;&#38190;&#30340;&#38382;&#39064;&#20043;&#19968;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#23558;&#27169;&#22411;&#36873;&#25321;&#35270;&#20026;&#19968;&#20010;&#20272;&#35745;&#38382;&#39064;&#65292;&#38656;&#35201;&#23450;&#26399;&#26356;&#26032;&#26469;&#36866;&#24212;&#20248;&#21270;&#36845;&#20195;&#20013;&#24471;&#21040;&#30340;&#35266;&#27979;&#32467;&#26524;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#21516;&#26102;&#23454;&#29616;&#39640;&#25928;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#27169;&#22411;&#36873;&#25321;&#21644;&#36125;&#21494;&#26031;&#20248;&#21270;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#20197;&#36798;&#21040;&#26356;&#24555;&#22320;&#36798;&#21040;&#20989;&#25968;&#30340;&#26368;&#20248;&#35299;&#12290;&#31639;&#27861;&#22312;&#27169;&#22411;&#31354;&#38388;&#21644;&#20989;&#25968;&#31354;&#38388;&#20043;&#38388;&#26469;&#22238;&#31227;&#21160;&#65292;&#20854;&#20013;&#25512;&#33616;&#27169;&#22411;&#30340;&#22909;&#22351;&#30001;&#19968;&#20010;&#35780;&#20998;&#20989;&#25968;&#26469;&#34913;&#37327;&#24182;&#21453;&#39304;&#65292;&#36825;&#20010;&#20989;&#25968;&#25429;&#25417;&#20102;&#27169;&#22411;&#22312;&#20989;&#25968;&#31354;&#38388;&#20013;&#23545;&#25910;&#25947;&#30340;&#24110;&#21161;&#31243;&#24230;&#12290;&#35780;&#20998;&#20989;&#25968;&#30340;&#25512;&#23548;&#26041;&#24335;&#20351;&#20854;&#25269;&#28040;&#20102;&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#20989;&#25968;&#31354;&#38388;&#20013;&#30340;&#21160;&#24577;&#24615;&#36136;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#20351;&#27169;&#22411;&#36873;&#25321;&#38382;&#39064;&#20445;&#25345;&#31283;&#23450;&#12290;&#36825;&#31181;&#26469;&#22238;&#36845;&#20195;&#23548;&#33268;&#27169;&#22411;&#36873;&#25321;&#21644;&#36125;&#21494;&#26031;&#20248;&#21270;&#37117;&#33021;&#24555;&#36895;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model selection is an integral problem of model based optimization techniques such as Bayesian optimization (BO). Current approaches often treat model selection as an estimation problem, to be periodically updated with observations coming from the optimization iterations. In this paper, we propose an alternative way to achieve both efficiently. Specifically, we propose a novel way of integrating model selection and BO for the single goal of reaching the function optima faster. The algorithm moves back and forth between BO in the model space and BO in the function space, where the goodness of the recommended model is captured by a score function and fed back, capturing how well the model helped convergence in the function space. The score function is derived in such a way that it neutralizes the effect of the moving nature of the BO in the function space, thus keeping the model selection problem stationary. This back and forth leads to quick convergence for both model selection and BO i
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;CLAMS&#30340;&#32858;&#31867;&#27169;&#31946;&#24230;&#27979;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#35270;&#35273;&#32858;&#31867;&#20013;&#30340;&#24863;&#30693;&#21464;&#24322;&#24615;&#12290;&#36890;&#36807;&#23450;&#24615;&#30740;&#31350;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#24433;&#21709;&#32858;&#31867;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#24182;&#36890;&#36807;&#22238;&#24402;&#27169;&#22359;&#23545;&#32858;&#31867;&#30340;&#27169;&#31946;&#24230;&#36827;&#34892;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2308.00284</link><description>&lt;p&gt;
CLAMS:&#19968;&#31181;&#29992;&#20110;&#20272;&#35745;&#35270;&#35273;&#32858;&#31867;&#20013;&#24863;&#30693;&#21464;&#24322;&#24615;&#30340;&#32858;&#31867;&#27169;&#31946;&#24230;&#27979;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
CLAMS: A Cluster Ambiguity Measure for Estimating Perceptual Variability in Visual Clustering. (arXiv:2308.00284v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00284
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;CLAMS&#30340;&#32858;&#31867;&#27169;&#31946;&#24230;&#27979;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#35270;&#35273;&#32858;&#31867;&#20013;&#30340;&#24863;&#30693;&#21464;&#24322;&#24615;&#12290;&#36890;&#36807;&#23450;&#24615;&#30740;&#31350;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#24433;&#21709;&#32858;&#31867;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#24182;&#36890;&#36807;&#22238;&#24402;&#27169;&#22359;&#23545;&#32858;&#31867;&#30340;&#27169;&#31946;&#24230;&#36827;&#34892;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#32858;&#31867;&#26159;&#25955;&#28857;&#22270;&#20013;&#24120;&#35265;&#30340;&#24863;&#30693;&#20219;&#21153;&#65292;&#25903;&#25345;&#21508;&#31181;&#20998;&#26512;&#20219;&#21153;&#65288;&#20363;&#22914;&#32858;&#31867;&#35782;&#21035;&#65289;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#20351;&#29992;&#30456;&#21516;&#30340;&#25955;&#28857;&#22270;&#65292;&#30001;&#20110;&#20010;&#20307;&#38388;&#30340;&#24046;&#24322;&#21644;&#27169;&#31946;&#30340;&#32858;&#31867;&#36793;&#30028;&#65292;&#24863;&#30693;&#32858;&#31867;&#30340;&#26041;&#24335;&#65288;&#21363;&#36827;&#34892;&#35270;&#35273;&#32858;&#31867;&#65289;&#21487;&#33021;&#20250;&#26377;&#25152;&#19981;&#21516;&#12290;&#23613;&#31649;&#36825;&#31181;&#24863;&#30693;&#21464;&#24322;&#24615;&#23545;&#20110;&#22522;&#20110;&#35270;&#35273;&#32858;&#31867;&#30340;&#25968;&#25454;&#20998;&#26512;&#30340;&#21487;&#38752;&#24615;&#25552;&#20986;&#20102;&#30097;&#38382;&#65292;&#20294;&#25105;&#20204;&#32570;&#20047;&#19968;&#31181;&#31995;&#32479;&#35780;&#20272;&#36825;&#31181;&#21464;&#24322;&#24615;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36827;&#34892;&#35270;&#35273;&#32858;&#31867;&#20013;&#30340;&#24863;&#30693;&#21464;&#24322;&#24615;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#32858;&#31867;&#27169;&#31946;&#24230;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;CLAMS&#65292;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#39044;&#27979;&#21333;&#33394;&#25955;&#28857;&#22270;&#20013;&#32858;&#31867;&#27169;&#31946;&#24230;&#30340;&#25968;&#25454;&#39537;&#21160;&#30340;&#35270;&#35273;&#36136;&#37327;&#27979;&#37327;&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#36827;&#34892;&#20102;&#23450;&#24615;&#30740;&#31350;&#65292;&#20197;&#30830;&#23450;&#24433;&#21709;&#32858;&#31867;&#30340;&#35270;&#35273;&#20998;&#31163;&#30340;&#20851;&#38190;&#22240;&#32032;&#65288;&#20363;&#22914;&#32858;&#31867;&#38388;&#30340;&#25509;&#36817;&#24230;&#25110;&#22823;&#23567;&#24046;&#24322;&#65289;&#12290;&#22522;&#20110;&#30740;&#31350;&#32467;&#26524;&#65292;&#25105;&#20204;&#37096;&#32626;&#20102;&#19968;&#20010;&#22238;&#24402;&#27169;&#22359;&#26469;&#20272;&#35745;&#32858;&#31867;&#30340;&#27169;&#31946;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual clustering is a common perceptual task in scatterplots that supports diverse analytics tasks (e.g., cluster identification). However, even with the same scatterplot, the ways of perceiving clusters (i.e., conducting visual clustering) can differ due to the differences among individuals and ambiguous cluster boundaries. Although such perceptual variability casts doubt on the reliability of data analysis based on visual clustering, we lack a systematic way to efficiently assess this variability. In this research, we study perceptual variability in conducting visual clustering, which we call Cluster Ambiguity. To this end, we introduce CLAMS, a data-driven visual quality measure for automatically predicting cluster ambiguity in monochrome scatterplots. We first conduct a qualitative study to identify key factors that affect the visual separation of clusters (e.g., proximity or size difference between clusters). Based on study findings, we deploy a regression module that estimates t
&lt;/p&gt;</description></item><item><title>ZADU&#26159;&#19968;&#20010;Python&#24211;&#65292;&#36890;&#36807;&#25552;&#20379;&#25197;&#26354;&#24230;&#37327;&#26041;&#27861;&#21487;&#20197;&#39640;&#25928;&#35780;&#20272;&#38477;&#32500;&#23884;&#20837;&#30340;&#21487;&#38752;&#24615;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#25191;&#34892;&#21644;&#20998;&#26512;&#21508;&#20010;&#25968;&#25454;&#28857;&#30340;&#36129;&#29486;&#25552;&#20379;&#20840;&#38754;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2308.00282</link><description>&lt;p&gt;
ZADU: &#35780;&#20272;&#38477;&#32500;&#23884;&#20837;&#21487;&#38752;&#24615;&#30340;Python&#24211;
&lt;/p&gt;
&lt;p&gt;
ZADU: A Python Library for Evaluating the Reliability of Dimensionality Reduction Embeddings. (arXiv:2308.00282v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00282
&lt;/p&gt;
&lt;p&gt;
ZADU&#26159;&#19968;&#20010;Python&#24211;&#65292;&#36890;&#36807;&#25552;&#20379;&#25197;&#26354;&#24230;&#37327;&#26041;&#27861;&#21487;&#20197;&#39640;&#25928;&#35780;&#20272;&#38477;&#32500;&#23884;&#20837;&#30340;&#21487;&#38752;&#24615;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#25191;&#34892;&#21644;&#20998;&#26512;&#21508;&#20010;&#25968;&#25454;&#28857;&#30340;&#36129;&#29486;&#25552;&#20379;&#20840;&#38754;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38477;&#32500;&#25216;&#26415;&#26412;&#36136;&#19978;&#20250;&#25197;&#26354;&#21407;&#22987;&#39640;&#32500;&#25968;&#25454;&#30340;&#32467;&#26500;&#65292;&#20135;&#29983;&#19981;&#23436;&#32654;&#30340;&#20302;&#32500;&#23884;&#20837;&#12290;&#20026;&#20102;&#35780;&#20272;&#38477;&#32500;&#23884;&#20837;&#30340;&#21487;&#38752;&#24615;&#65292;&#25552;&#20986;&#20102;&#21508;&#31181;&#25197;&#26354;&#24230;&#37327;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#23454;&#29616;&#21644;&#25191;&#34892;&#25197;&#26354;&#24230;&#37327;&#19968;&#30452;&#26159;&#32791;&#26102;&#19988;&#32321;&#29712;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;Python&#24211;ZADU&#65292;&#25552;&#20379;&#20102;&#25197;&#26354;&#24230;&#37327;&#26041;&#27861;&#12290;ZADU&#19981;&#20165;&#26131;&#20110;&#23433;&#35013;&#21644;&#25191;&#34892;&#65292;&#36824;&#36890;&#36807;&#19977;&#20010;&#20851;&#38190;&#29305;&#24615;&#23454;&#29616;&#20102;&#23545;&#38477;&#32500;&#23884;&#20837;&#30340;&#20840;&#38754;&#35780;&#20272;&#12290;&#39318;&#20808;&#65292;&#35813;&#24211;&#28085;&#30422;&#20102;&#21508;&#31181;&#25197;&#26354;&#24230;&#37327;&#26041;&#27861;&#12290;&#20854;&#27425;&#65292;&#23427;&#33258;&#21160;&#20248;&#21270;&#25197;&#26354;&#24230;&#37327;&#30340;&#25191;&#34892;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#25191;&#34892;&#22810;&#20010;&#24230;&#37327;&#25152;&#38656;&#30340;&#36816;&#34892;&#26102;&#38388;&#12290;&#26368;&#21518;&#65292;&#35813;&#24211;&#21487;&#26174;&#31034;&#20010;&#21035;&#25968;&#25454;&#28857;&#23545;&#25972;&#20307;&#25197;&#26354;&#30340;&#36129;&#29486;&#65292;&#20415;&#20110;&#23545;&#38477;&#32500;&#23884;&#20837;&#36827;&#34892;&#35814;&#32454;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dimensionality reduction (DR) techniques inherently distort the original structure of input high-dimensional data, producing imperfect low-dimensional embeddings. Diverse distortion measures have thus been proposed to evaluate the reliability of DR embeddings. However, implementing and executing distortion measures in practice has so far been time-consuming and tedious. To address this issue, we present ZADU, a Python library that provides distortion measures. ZADU is not only easy to install and execute but also enables comprehensive evaluation of DR embeddings through three key features. First, the library covers a wide range of distortion measures. Second, it automatically optimizes the execution of distortion measures, substantially reducing the running time required to execute multiple measures. Last, the library informs how individual points contribute to the overall distortions, facilitating the detailed analysis of DR embeddings. By simulating a real-world scenario of optimizin
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#24212;&#29992;&#20110;&#21270;&#21512;&#29289;&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#21327;&#20316;&#20998;&#26512;&#65292;&#20197;&#21450;&#24341;&#20837;&#25237;&#24433;&#25968;&#25454;&#21040;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#29615;&#22659;&#20013;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#25968;&#25454;&#21512;&#20316;&#21644;&#20351;&#29992;&#36741;&#21161;&#25968;&#25454;&#30340;&#26041;&#24335;&#25552;&#39640;&#20102;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.00280</link><description>&lt;p&gt;
&#25968;&#25454;&#21327;&#20316;&#20998;&#26512;&#22312;&#21270;&#21512;&#29289;&#25968;&#25454;&#38598;&#19978;&#30340;&#24212;&#29992;&#20197;&#21450;&#24341;&#20837;&#25237;&#24433;&#25968;&#25454;&#21040;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#29615;&#22659;&#20013;
&lt;/p&gt;
&lt;p&gt;
Data Collaboration Analysis applied to Compound Datasets and the Introduction of Projection data to Non-IID settings. (arXiv:2308.00280v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00280
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#24212;&#29992;&#20110;&#21270;&#21512;&#29289;&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#21327;&#20316;&#20998;&#26512;&#65292;&#20197;&#21450;&#24341;&#20837;&#25237;&#24433;&#25968;&#25454;&#21040;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#29615;&#22659;&#20013;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#25968;&#25454;&#21512;&#20316;&#21644;&#20351;&#29992;&#36741;&#21161;&#25968;&#25454;&#30340;&#26041;&#24335;&#25552;&#39640;&#20102;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#25226;&#19968;&#31181;&#33647;&#29289;&#25512;&#21521;&#24066;&#22330;&#25152;&#38656;&#35201;&#30340;&#26102;&#38388;&#21644;&#36153;&#29992;&#65292;&#24050;&#32463;&#36827;&#34892;&#20102;&#35768;&#22810;&#30740;&#31350;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26681;&#25454;&#20854;&#32467;&#26500;&#39044;&#27979;&#21270;&#21512;&#29289;&#30340;&#24615;&#36136;&#12290;&#32852;&#37030;&#23398;&#20064;&#24050;&#32463;&#24212;&#29992;&#20110;&#21270;&#21512;&#29289;&#25968;&#25454;&#38598;&#20013;&#65292;&#20197;&#25552;&#39640;&#20854;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#20445;&#25252;&#21487;&#33021;&#30340;&#19987;&#26377;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#22312;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#65288;non-IID&#65289;&#30340;&#24773;&#20917;&#19979;&#65292;&#32852;&#37030;&#23398;&#20064;&#30340;&#20934;&#30830;&#24615;&#36739;&#20302;&#65292;&#21363;&#25968;&#25454;&#20998;&#21306;&#20855;&#26377;&#24456;&#22823;&#30340;&#26631;&#31614;&#20559;&#24046;&#65292;&#24182;&#34987;&#35748;&#20026;&#19981;&#36866;&#29992;&#20110;&#21270;&#21512;&#29289;&#25968;&#25454;&#38598;&#65292;&#36825;&#24448;&#24448;&#20855;&#26377;&#36739;&#22823;&#30340;&#26631;&#31614;&#20559;&#24046;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#23545;&#26469;&#33258;&#24320;&#25918;&#28304;&#30340;&#21270;&#21512;&#29289;&#25968;&#25454;&#36827;&#34892;&#20102;&#25968;&#25454;&#21327;&#20316;&#20998;&#26512;&#65288;DC&#65289;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#20351;&#29992;&#25237;&#24433;&#25968;&#25454;&#30340;&#25968;&#25454;&#21327;&#20316;&#20998;&#26512;&#26041;&#27861;&#65288;DCPd&#65289;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#36741;&#21161;&#30340;PubChem&#25968;&#25454;&#12290;&#36825;&#25552;&#39640;&#20102;&#20010;&#20307;&#29992;&#25143;&#20391;&#25968;&#25454;&#36716;&#25442;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given the time and expense associated with bringing a drug to market, numerous studies have been conducted to predict the properties of compounds based on their structure using machine learning. Federated learning has been applied to compound datasets to increase their prediction accuracy while safeguarding potentially proprietary information. However, federated learning is encumbered by low accuracy in not identically and independently distributed (non-IID) settings, i.e., data partitioning has a large label bias, and is considered unsuitable for compound datasets, which tend to have large label bias. To address this limitation, we utilized an alternative method of distributed machine learning to chemical compound data from open sources, called data collaboration analysis (DC). We also proposed data collaboration analysis using projection data (DCPd), which is an improved method that utilizes auxiliary PubChem data. This improves the quality of individual user-side data transformation
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22122;&#22768;&#36127;&#26679;&#26412;&#33258;&#26657;&#27491;&#30340;&#40065;&#26834;&#27491;&#36127;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#39318;&#20808;&#23398;&#20064;&#31616;&#21333;&#30340;&#26696;&#20363;&#26469;&#20943;&#36731;&#26631;&#31614;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#25552;&#39640;&#23398;&#20064;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.00279</link><description>&lt;p&gt;
&#22522;&#20110;&#22122;&#22768;&#36127;&#26679;&#26412;&#33258;&#26657;&#27491;&#30340;&#40065;&#26834;&#27491;&#36127;&#26679;&#26412;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Robust Positive-Unlabeled Learning via Noise Negative Sample Self-correction. (arXiv:2308.00279v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00279
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22122;&#22768;&#36127;&#26679;&#26412;&#33258;&#26657;&#27491;&#30340;&#40065;&#26834;&#27491;&#36127;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#39318;&#20808;&#23398;&#20064;&#31616;&#21333;&#30340;&#26696;&#20363;&#26469;&#20943;&#36731;&#26631;&#31614;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#25552;&#39640;&#23398;&#20064;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#29486;&#20013;&#23558;&#20174;&#27491;&#26679;&#26412;&#21644;&#26080;&#26631;&#31614;&#25968;&#25454;&#20013;&#23398;&#20064;&#31216;&#20026;&#27491;&#36127;&#26679;&#26412;&#65288;PU&#65289;&#23398;&#20064;&#65292;&#24182;&#22312;&#26368;&#36817;&#20960;&#24180;&#24341;&#36215;&#20102;&#24456;&#22823;&#20851;&#27880;&#12290;PU&#23398;&#20064;&#20013;&#19968;&#31181;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#20020;&#26102;&#38408;&#20540;&#20174;&#26080;&#26631;&#31614;&#25968;&#25454;&#20013;&#37319;&#26679;&#19968;&#32452;&#20266;&#36127;&#26679;&#26412;&#65292;&#20197;&#20415;&#21487;&#20197;&#24212;&#29992;&#20256;&#32479;&#30340;&#30417;&#30563;&#26041;&#27861;&#26469;&#21253;&#25324;&#27491;&#26679;&#26412;&#21644;&#36127;&#26679;&#26412;&#12290;&#30001;&#20110;&#26080;&#26631;&#31614;&#25968;&#25454;&#20013;&#26631;&#31614;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#23558;&#26080;&#26631;&#31614;&#30340;&#27491;&#26679;&#26412;&#38169;&#35823;&#20998;&#31867;&#20026;&#36127;&#26679;&#26412;&#30340;&#38169;&#35823;&#19981;&#21487;&#36991;&#20813;&#22320;&#20986;&#29616;&#65292;&#24182;&#19988;&#21487;&#33021;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#32047;&#31215;&#12290;&#36825;&#20123;&#38169;&#35823;&#32463;&#24120;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#21644;&#27169;&#22411;&#19981;&#31283;&#23450;&#12290;&#20026;&#20102;&#20943;&#36731;&#26631;&#31614;&#30340;&#19981;&#30830;&#23450;&#24615;&#24433;&#21709;&#65292;&#25552;&#39640;&#27491;&#36127;&#26679;&#26412;&#23398;&#20064;&#30340;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#40065;&#26834;PU&#23398;&#20064;&#26041;&#27861;&#65292;&#20854;&#20013;&#35757;&#32451;&#31574;&#30053;&#21463;&#21040;&#20154;&#31867;&#23398;&#20064;&#30340;&#33258;&#28982;&#21551;&#31034;&#65306;&#39318;&#20808;&#23398;&#20064;&#31616;&#21333;&#30340;&#26696;&#20363;&#12290;&#30456;&#20284;&#30340;&#30452;&#35273;&#24050;&#32463;&#22312;&#35838;&#31243;&#23398;&#20064;&#20013;&#34987;&#21033;&#29992;&#65292;&#21482;&#20351;&#29992;&#26356;&#31616;&#21333;&#30340;&#26696;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning from positive and unlabeled data is known as positive-unlabeled (PU) learning in literature and has attracted much attention in recent years. One common approach in PU learning is to sample a set of pseudo-negatives from the unlabeled data using ad-hoc thresholds so that conventional supervised methods can be applied with both positive and negative samples. Owing to the label uncertainty among the unlabeled data, errors of misclassifying unlabeled positive samples as negative samples inevitably appear and may even accumulate during the training processes. Those errors often lead to performance degradation and model instability. To mitigate the impact of label uncertainty and improve the robustness of learning with positive and unlabeled data, we propose a new robust PU learning method with a training strategy motivated by the nature of human learning: easy cases should be learned first. Similar intuition has been utilized in curriculum learning to only use easier cases in the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#36136;&#37327;&#24230;&#37327;&#26041;&#27861;--&#26631;&#31614;&#21487;&#20449;&#24230;&#21644;&#26631;&#31614;&#36830;&#32493;&#24615;&#65288;Label-T&amp;C&#65289;--&#25913;&#36827;&#20102;&#22522;&#20110;&#31867;&#21035;&#26631;&#31614;&#30340;&#38477;&#32500;&#35780;&#20272;&#30340;&#36807;&#31243;&#65292;&#19981;&#20877;&#20551;&#35774;&#31867;&#21035;&#22312;&#21407;&#22987;&#31354;&#38388;&#20013;&#24418;&#25104;&#33391;&#22909;&#30340;&#32858;&#31867;&#65292;&#32780;&#26159;&#36890;&#36807;&#20272;&#35745;&#31867;&#21035;&#22312;&#21407;&#22987;&#31354;&#38388;&#21644;&#23884;&#20837;&#31354;&#38388;&#20013;&#24418;&#25104;&#32858;&#31867;&#30340;&#31243;&#24230;&#21644;&#35780;&#20272;&#20004;&#32773;&#20043;&#38388;&#30340;&#24046;&#24322;&#26469;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2308.00278</link><description>&lt;p&gt;
&#31867;&#21035;&#19981;&#31561;&#20110;&#32858;&#31867;&#65306;&#25913;&#36827;&#22522;&#20110;&#26631;&#31614;&#30340;&#38477;&#32500;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Classes are not Clusters: Improving Label-based Evaluation of Dimensionality Reduction. (arXiv:2308.00278v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00278
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#36136;&#37327;&#24230;&#37327;&#26041;&#27861;--&#26631;&#31614;&#21487;&#20449;&#24230;&#21644;&#26631;&#31614;&#36830;&#32493;&#24615;&#65288;Label-T&amp;C&#65289;--&#25913;&#36827;&#20102;&#22522;&#20110;&#31867;&#21035;&#26631;&#31614;&#30340;&#38477;&#32500;&#35780;&#20272;&#30340;&#36807;&#31243;&#65292;&#19981;&#20877;&#20551;&#35774;&#31867;&#21035;&#22312;&#21407;&#22987;&#31354;&#38388;&#20013;&#24418;&#25104;&#33391;&#22909;&#30340;&#32858;&#31867;&#65292;&#32780;&#26159;&#36890;&#36807;&#20272;&#35745;&#31867;&#21035;&#22312;&#21407;&#22987;&#31354;&#38388;&#21644;&#23884;&#20837;&#31354;&#38388;&#20013;&#24418;&#25104;&#32858;&#31867;&#30340;&#31243;&#24230;&#21644;&#35780;&#20272;&#20004;&#32773;&#20043;&#38388;&#30340;&#24046;&#24322;&#26469;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#38477;&#32500;&#23884;&#20837;&#30340;&#21487;&#38752;&#24615;&#30340;&#19968;&#31181;&#24120;&#35265;&#26041;&#27861;&#26159;&#37327;&#21270;&#26631;&#35760;&#31867;&#21035;&#22312;&#23884;&#20837;&#20013;&#22914;&#20309;&#24418;&#25104;&#32039;&#20945;&#19988;&#30456;&#20114;&#20998;&#31163;&#30340;&#32858;&#31867;&#12290;&#36825;&#31181;&#26041;&#27861;&#22522;&#20110;&#19968;&#20010;&#20551;&#35774;&#65292;&#21363;&#31867;&#21035;&#22312;&#21407;&#22987;&#30340;&#39640;&#32500;&#31354;&#38388;&#20013;&#20173;&#28982;&#26159;&#28165;&#26224;&#30340;&#32858;&#31867;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#20013;&#65292;&#36825;&#20010;&#20551;&#35774;&#21487;&#33021;&#19981;&#25104;&#31435;&#65307;&#19968;&#20010;&#31867;&#21035;&#21487;&#33021;&#34987;&#20998;&#35299;&#25104;&#22810;&#20010;&#20998;&#31163;&#30340;&#32858;&#31867;&#65292;&#22810;&#20010;&#31867;&#21035;&#21487;&#33021;&#21512;&#24182;&#25104;&#19968;&#20010;&#32858;&#31867;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#19981;&#33021;&#24635;&#26159;&#20445;&#35777;&#20351;&#29992;&#31867;&#21035;&#26631;&#31614;&#36827;&#34892;&#35780;&#20272;&#30340;&#21487;&#20449;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#26032;&#30340;&#36136;&#37327;&#24230;&#37327;&#26041;&#27861;--&#26631;&#31614;&#21487;&#20449;&#24230;&#21644;&#26631;&#31614;&#36830;&#32493;&#24615;&#65288;Label-T&amp;C&#65289;--&#25913;&#36827;&#20102;&#22522;&#20110;&#31867;&#21035;&#26631;&#31614;&#30340;&#38477;&#32500;&#35780;&#20272;&#30340;&#36807;&#31243;&#12290;Label-T&amp;C&#19981;&#20877;&#20551;&#35774;&#31867;&#21035;&#22312;&#21407;&#22987;&#31354;&#38388;&#20013;&#24418;&#25104;&#33391;&#22909;&#30340;&#32858;&#31867;&#65292;&#32780;&#26159;&#36890;&#36807;&#65288;1&#65289;&#20272;&#35745;&#31867;&#21035;&#22312;&#21407;&#22987;&#31354;&#38388;&#21644;&#23884;&#20837;&#31354;&#38388;&#20013;&#24418;&#25104;&#32858;&#31867;&#30340;&#31243;&#24230;&#21644;&#65288;2&#65289;&#35780;&#20272;&#20004;&#32773;&#20043;&#38388;&#30340;&#24046;&#24322;&#26469;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
A common way to evaluate the reliability of dimensionality reduction (DR) embeddings is to quantify how well labeled classes form compact, mutually separated clusters in the embeddings. This approach is based on the assumption that the classes stay as clear clusters in the original high-dimensional space. However, in reality, this assumption can be violated; a single class can be fragmented into multiple separated clusters, and multiple classes can be merged into a single cluster. We thus cannot always assure the credibility of the evaluation using class labels. In this paper, we introduce two novel quality measures -- Label-Trustworthiness and Label-Continuity (Label-T&amp;C) -- advancing the process of DR evaluation based on class labels. Instead of assuming that classes are well-clustered in the original space, Label-T&amp;C work by (1) estimating the extent to which classes form clusters in the original and embedded spaces and (2) evaluating the difference between the two. A quantitative e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26469;&#36817;&#20284;&#23545;&#31216;&#21644;&#20998;&#37327;&#32452;&#19981;&#21464;&#30340;&#20989;&#25968;&#65292;&#24182;&#23558;&#20854;&#19982;&#32032;&#25551;&#24605;&#24819;&#32467;&#21512;&#36215;&#26469;&#26500;&#24314;&#20102;&#19968;&#20010;&#29992;&#20110;&#36817;&#20284;&#28857;&#38598;&#20043;&#38388;Wasserstein&#36317;&#31163;&#30340;&#20855;&#20307;&#19988;&#39640;&#25928;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.00273</link><description>&lt;p&gt;
&#36890;&#36807;&#19968;&#31181;&#23545;&#31216;&#21644;&#20998;&#37327;&#32452;&#19981;&#21464;&#21151;&#33021;&#30340;&#36890;&#29992;&#26550;&#26500;&#36817;&#20284;Wasserstein&#36317;&#31163;&#30340;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Neural approximation of Wasserstein distance via a universal architecture for symmetric and factorwise group invariant functions. (arXiv:2308.00273v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00273
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26469;&#36817;&#20284;&#23545;&#31216;&#21644;&#20998;&#37327;&#32452;&#19981;&#21464;&#30340;&#20989;&#25968;&#65292;&#24182;&#23558;&#20854;&#19982;&#32032;&#25551;&#24605;&#24819;&#32467;&#21512;&#36215;&#26469;&#26500;&#24314;&#20102;&#19968;&#20010;&#29992;&#20110;&#36817;&#20284;&#28857;&#38598;&#20043;&#38388;Wasserstein&#36317;&#31163;&#30340;&#20855;&#20307;&#19988;&#39640;&#25928;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#22797;&#26434;&#23545;&#35937;&#20043;&#38388;&#30340;&#36317;&#31163;&#20989;&#25968;&#65292;&#27604;&#22914;&#29992;&#20110;&#27604;&#36739;&#28857;&#38598;&#30340;Wasserstein&#36317;&#31163;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#26159;&#19968;&#20010;&#24120;&#35265;&#30340;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#36825;&#31181;&#22797;&#26434;&#23545;&#35937;&#65288;&#22914;&#28857;&#38598;&#21644;&#22270;&#24418;&#65289;&#65292;&#20989;&#25968;&#24448;&#24448;&#38656;&#35201;&#23545;&#21508;&#31181;&#32676;&#25805;&#20316;&#65288;&#22914;&#25490;&#21015;&#25110;&#21018;&#24615;&#21464;&#25442;&#65289;&#20855;&#26377;&#19981;&#21464;&#24615;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#22797;&#26434;&#23545;&#35937;&#19978;&#30340;&#36830;&#32493;&#23545;&#31216;&#20056;&#31215;&#20989;&#25968;&#65288;&#20363;&#22914;&#36317;&#31163;&#20989;&#25968;&#65289;&#20063;&#24517;&#39035;&#23545;&#36825;&#20123;&#32676;&#25805;&#20316;&#30340;&#20056;&#31215;&#20855;&#26377;&#19981;&#21464;&#24615;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#20989;&#25968;&#31216;&#20026;&#23545;&#31216;&#21644;&#20998;&#37327;&#32452;&#19981;&#21464;&#20989;&#25968;&#65288;&#31616;&#31216;SFGI&#20989;&#25968;&#65289;&#12290;&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#36817;&#20284;SFGI&#20989;&#25968;&#30340;&#36890;&#29992;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#23558;&#36825;&#20010;&#36890;&#29992;&#31070;&#32463;&#32593;&#32476;&#19982;&#19968;&#20010;&#32032;&#25551;&#24605;&#24819;&#32467;&#21512;&#36215;&#26469;&#65292;&#24320;&#21457;&#20986;&#19968;&#31181;&#20855;&#20307;&#19988;&#39640;&#25928;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#36817;&#20284;&#28857;&#38598;&#20043;&#38388;&#30340;$p$-th Wasserstein&#36317;&#31163;&#12290;&#38750;&#24120;&#37325;&#35201;&#30340;&#26159;&#65292;&#25152;&#38656;&#30340;&#27169;&#22411;&#22797;&#26434;&#24230;&#19982;&#28857;&#38598;&#30340;&#22823;&#23567;&#21644;&#32500;&#24230;&#26080;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning distance functions between complex objects, such as the Wasserstein distance to compare point sets, is a common goal in machine learning applications. However, functions on such complex objects (e.g., point sets and graphs) are often required to be invariant to a wide variety of group actions e.g. permutation or rigid transformation. Therefore, continuous and symmetric product functions (such as distance functions) on such complex objects must also be invariant to the product of such group actions. We call these functions symmetric and factor-wise group invariant (or SFGI functions in short). In this paper, we first present a general neural network architecture for approximating SFGI functions. The main contribution of this paper combines this general neural network with a sketching idea to develop a specific and efficient neural network which can approximate the $p$-th Wasserstein distance between point sets. Very importantly, the required model complexity is independent of t
&lt;/p&gt;</description></item><item><title>&#22810;&#27169;&#24577;&#22810;&#25439;&#22833;&#34701;&#21512;&#32593;&#32476;&#36890;&#36807;&#26368;&#20339;&#36873;&#25321;&#21644;&#34701;&#21512;&#22810;&#20010;&#27169;&#24577;&#30340;&#29305;&#24449;&#65292;&#25552;&#39640;&#20102;&#24773;&#24863;&#26816;&#27979;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#36825;&#20123;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#20102;&#29992;&#20110;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#20013;&#24773;&#24863;&#26816;&#27979;&#30340;&#29305;&#24449;&#36873;&#25321;&#21644;&#34701;&#21512;&#26041;&#27861;&#30340;&#20248;&#21270;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2308.00264</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#22810;&#25439;&#22833;&#34701;&#21512;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Multi-Modality Multi-Loss Fusion Network. (arXiv:2308.00264v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00264
&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22810;&#25439;&#22833;&#34701;&#21512;&#32593;&#32476;&#36890;&#36807;&#26368;&#20339;&#36873;&#25321;&#21644;&#34701;&#21512;&#22810;&#20010;&#27169;&#24577;&#30340;&#29305;&#24449;&#65292;&#25552;&#39640;&#20102;&#24773;&#24863;&#26816;&#27979;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#36825;&#20123;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#20102;&#29992;&#20110;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#20013;&#24773;&#24863;&#26816;&#27979;&#30340;&#29305;&#24449;&#36873;&#25321;&#21644;&#34701;&#21512;&#26041;&#27861;&#30340;&#20248;&#21270;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36328;&#22810;&#20010;&#27169;&#24577;&#36873;&#25321;&#21644;&#34701;&#21512;&#29305;&#24449;&#30340;&#26368;&#20339;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#32452;&#21512;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#20197;&#25913;&#21892;&#24773;&#24863;&#26816;&#27979;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#34701;&#21512;&#26041;&#27861;&#24182;&#19988;&#30740;&#31350;&#20102;&#22810;&#25439;&#22833;&#35757;&#32451;&#22312;&#22810;&#27169;&#24577;&#34701;&#21512;&#32593;&#32476;&#20013;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#30830;&#23450;&#20102;&#19982;&#23376;&#32593;&#24615;&#33021;&#30456;&#20851;&#30340;&#26377;&#29992;&#21457;&#29616;&#12290;&#25105;&#20204;&#26368;&#22909;&#30340;&#27169;&#22411;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#65288;CMU-MOSI&#12289;CMU-MOSEI&#21644;CH-SIMS&#65289;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#22823;&#22810;&#25968;&#25351;&#26631;&#19978;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#35757;&#32451;&#22810;&#27169;&#24577;&#29305;&#24449;&#21487;&#20197;&#25913;&#21892;&#21333;&#27169;&#24577;&#27979;&#35797;&#65292;&#24182;&#19988;&#22522;&#20110;&#25968;&#25454;&#38598;&#27880;&#37322;&#27169;&#24335;&#35774;&#35745;&#34701;&#21512;&#26041;&#27861;&#21487;&#20197;&#22686;&#24378;&#27169;&#22411;&#24615;&#33021;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#20102;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#22686;&#24378;&#24773;&#24863;&#26816;&#27979;&#30340;&#20248;&#21270;&#29305;&#24449;&#36873;&#25321;&#21644;&#34701;&#21512;&#26041;&#27861;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work we investigate the optimal selection and fusion of features across multiple modalities and combine these in a neural network to improve emotion detection. We compare different fusion methods and examine the impact of multi-loss training within the multi-modality fusion network, identifying useful findings relating to subnet performance. Our best model achieves state-of-the-art performance for three datasets (CMU-MOSI, CMU-MOSEI and CH-SIMS), and outperforms the other methods in most metrics. We have found that training on multimodal features improves single modality testing and designing fusion methods based on dataset annotation schema enhances model performance. These results suggest a roadmap towards an optimized feature selection and fusion approach for enhancing emotion detection in neural networks.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;QAFeL&#30340;&#26032;&#31639;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#26381;&#21153;&#22120;&#21644;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#20849;&#20139;&#8220;&#38544;&#34255;&#8221;&#29366;&#24577;&#65292;&#37319;&#29992;&#37327;&#21270;&#26041;&#26696;&#35299;&#20915;&#20102;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;&#20013;&#39640;&#36890;&#20449;&#25104;&#26412;&#30340;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#33021;&#22815;&#22312;&#20445;&#25345;&#39640;&#31934;&#24230;&#30340;&#21516;&#26102;&#26174;&#33879;&#20943;&#23569;&#23458;&#25143;&#31471;-&#26381;&#21153;&#22120;&#20132;&#20114;&#36807;&#31243;&#20013;&#20256;&#36755;&#30340;&#25968;&#25454;&#37327;&#12290;</title><link>http://arxiv.org/abs/2308.00263</link><description>&lt;p&gt;
&#20351;&#29992;&#21452;&#21521;&#37327;&#21270;&#36890;&#20449;&#21644;&#32531;&#20914;&#32858;&#21512;&#30340;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Asynchronous Federated Learning with Bidirectional Quantized Communications and Buffered Aggregation. (arXiv:2308.00263v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00263
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;QAFeL&#30340;&#26032;&#31639;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#26381;&#21153;&#22120;&#21644;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#20849;&#20139;&#8220;&#38544;&#34255;&#8221;&#29366;&#24577;&#65292;&#37319;&#29992;&#37327;&#21270;&#26041;&#26696;&#35299;&#20915;&#20102;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;&#20013;&#39640;&#36890;&#20449;&#25104;&#26412;&#30340;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#33021;&#22815;&#22312;&#20445;&#25345;&#39640;&#31934;&#24230;&#30340;&#21516;&#26102;&#26174;&#33879;&#20943;&#23569;&#23458;&#25143;&#31471;-&#26381;&#21153;&#22120;&#20132;&#20114;&#36807;&#31243;&#20013;&#20256;&#36755;&#30340;&#25968;&#25454;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;&#19982;&#32531;&#20914;&#32858;&#21512;&#65288;FedBuff&#65289;&#26159;&#19968;&#31181;&#25928;&#29575;&#39640;&#12289;&#21487;&#20280;&#32553;&#24615;&#24378;&#30340;&#20808;&#36827;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#23427;&#23384;&#22312;&#39640;&#36890;&#20449;&#25104;&#26412;&#30340;&#38382;&#39064;&#65292;&#24182;&#26410;&#20351;&#29992;&#37327;&#21270;&#36890;&#20449;&#36827;&#34892;&#30740;&#31350;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65288;QAFeL&#65289;&#65292;&#37319;&#29992;&#19968;&#31181;&#37327;&#21270;&#26041;&#26696;&#65292;&#22312;&#26381;&#21153;&#22120;&#21644;&#23458;&#25143;&#31471;&#20043;&#38388;&#24314;&#31435;&#20849;&#20139;&#30340;&#8220;&#38544;&#34255;&#8221;&#29366;&#24577;&#65292;&#36991;&#20813;&#20102;&#30452;&#25509;&#37327;&#21270;&#24341;&#36215;&#30340;&#35823;&#24046;&#20256;&#25773;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#20445;&#25345;&#39640;&#31934;&#24230;&#30340;&#21516;&#26102;&#65292;&#22312;&#23458;&#25143;&#31471;-&#26381;&#21153;&#22120;&#20132;&#20114;&#36807;&#31243;&#20013;&#26174;&#33879;&#20943;&#23569;&#20102;&#20256;&#36755;&#30340;&#25968;&#25454;&#37327;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;QAFeL&#30340;&#29702;&#35770;&#25910;&#25947;&#20445;&#35777;&#65292;&#24182;&#36890;&#36807;&#26631;&#20934;&#22522;&#20934;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#20998;&#26512;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Asynchronous Federated Learning with Buffered Aggregation (FedBuff) is a state-of-the-art algorithm known for its efficiency and high scalability. However, it has a high communication cost, which has not been examined with quantized communications. To tackle this problem, we present a new algorithm (QAFeL), with a quantization scheme that establishes a shared "hidden" state between the server and clients to avoid the error propagation caused by direct quantization. This approach allows for high precision while significantly reducing the data transmitted during client-server interactions. We provide theoretical convergence guarantees for QAFeL and corroborate our analysis with experiments on a standard benchmark.
&lt;/p&gt;</description></item><item><title>AQUILA&#26159;&#19968;&#20010;&#33258;&#36866;&#24212;&#37327;&#21270;&#26799;&#24230;&#30340;&#36890;&#20449;&#39640;&#25928;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#20256;&#36755;&#22823;&#35268;&#27169;&#27169;&#22411;&#26102;&#30340;&#36890;&#20449;&#24320;&#38144;&#21644;&#23616;&#37096;&#25968;&#25454;&#20559;&#24046;&#23548;&#33268;&#30340;&#20840;&#23616;&#27169;&#22411;&#40065;&#26834;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.00258</link><description>&lt;p&gt;
AQUILA: &#33258;&#36866;&#24212;&#37327;&#21270;&#25042;&#27719;&#32858;&#26799;&#24230;&#30340;&#36890;&#20449;&#39640;&#25928;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
AQUILA: Communication Efficient Federated Learning with Adaptive Quantization of Lazily-Aggregated Gradients. (arXiv:2308.00258v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00258
&lt;/p&gt;
&lt;p&gt;
AQUILA&#26159;&#19968;&#20010;&#33258;&#36866;&#24212;&#37327;&#21270;&#26799;&#24230;&#30340;&#36890;&#20449;&#39640;&#25928;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#20256;&#36755;&#22823;&#35268;&#27169;&#27169;&#22411;&#26102;&#30340;&#36890;&#20449;&#24320;&#38144;&#21644;&#23616;&#37096;&#25968;&#25454;&#20559;&#24046;&#23548;&#33268;&#30340;&#20840;&#23616;&#27169;&#22411;&#40065;&#26834;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#30340;&#24191;&#27867;&#24212;&#29992;&#21463;&#21040;&#39640;&#36890;&#20449;&#24320;&#38144;&#30340;&#25361;&#25112;&#65292;&#20027;&#35201;&#26469;&#33258;&#22823;&#35268;&#27169;&#27169;&#22411;&#30340;&#20256;&#36755;&#12290;&#29616;&#26377;&#30340;&#33258;&#36866;&#24212;&#37327;&#21270;&#26041;&#27861;&#22312;&#27599;&#19968;&#36718;&#35757;&#32451;&#20013;&#37117;&#20551;&#35774;&#35774;&#22791;&#21442;&#19982;&#22343;&#21248;&#65292;&#22312;&#23454;&#36341;&#20013;&#19981;&#21487;&#34892;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#36873;&#21462;&#37327;&#21270;&#32423;&#21035;&#26102;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#24182;&#32463;&#24120;&#24573;&#35270;&#26412;&#22320;&#35774;&#22791;&#25968;&#25454;&#30340;&#20559;&#24046;&#65292;&#20174;&#32780;&#24433;&#21709;&#20840;&#23616;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;AQUILA&#65288;&#33258;&#36866;&#24212;&#37327;&#21270;&#25042;&#27719;&#32858;&#26799;&#24230;&#65289;&#30340;&#26032;&#22411;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#20197;&#22686;&#24378;&#32852;&#37030;&#23398;&#20064;&#30340;&#25928;&#29575;&#21644;&#40065;&#26834;&#24615;&#12290;AQUILA&#25972;&#21512;&#20102;&#19968;&#31181;&#22797;&#26434;&#30340;&#35774;&#22791;&#36873;&#25321;&#26041;&#27861;&#65292;&#20248;&#20808;&#32771;&#34385;&#35774;&#22791;&#26356;&#26032;&#30340;&#36136;&#37327;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The widespread adoption of Federated Learning (FL), a privacy-preserving distributed learning methodology, has been impeded by the challenge of high communication overheads, typically arising from the transmission of large-scale models. Existing adaptive quantization methods, designed to mitigate these overheads, operate under the impractical assumption of uniform device participation in every training round. Additionally, these methods are limited in their adaptability due to the necessity of manual quantization level selection and often overlook biases inherent in local devices' data, thereby affecting the robustness of the global model. In response, this paper introduces AQUILA (adaptive quantization of lazily-aggregated gradients), a novel adaptive framework devised to effectively handle these issues, enhancing the efficiency and robustness of FL. AQUILA integrates a sophisticated device selection method that prioritizes the quality and usefulness of device updates. Utilizing the e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#39640;&#32500;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#20013;&#36827;&#34892;&#26368;&#20339;&#23376;&#38598;&#36873;&#25321;&#30340;&#24555;&#36895;&#19988;&#19968;&#33268;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#25340;&#25509;&#25216;&#26415;&#23454;&#29616;&#20102;&#39640;&#30830;&#23450;&#24615;&#30340;&#26368;&#20339;&#23376;&#38598;&#36873;&#25321;&#65292;&#24182;&#22312;&#21464;&#37327;&#36873;&#25321;&#21644;&#31995;&#25968;&#20272;&#35745;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.00251</link><description>&lt;p&gt;
&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#20013;&#30340;&#26368;&#20339;&#23376;&#38598;&#36873;&#25321;&#65306;&#19968;&#31181;&#36890;&#36807;&#25340;&#25509;&#25216;&#26415;&#30340;&#24555;&#36895;&#19988;&#19968;&#33268;&#30340;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Best-Subset Selection in Generalized Linear Models: A Fast and Consistent Algorithm via Splicing Technique. (arXiv:2308.00251v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00251
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#39640;&#32500;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#20013;&#36827;&#34892;&#26368;&#20339;&#23376;&#38598;&#36873;&#25321;&#30340;&#24555;&#36895;&#19988;&#19968;&#33268;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#25340;&#25509;&#25216;&#26415;&#23454;&#29616;&#20102;&#39640;&#30830;&#23450;&#24615;&#30340;&#26368;&#20339;&#23376;&#38598;&#36873;&#25321;&#65292;&#24182;&#22312;&#21464;&#37327;&#36873;&#25321;&#21644;&#31995;&#25968;&#20272;&#35745;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#32500;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#20013;&#65292;&#24456;&#37325;&#35201;&#30340;&#26159;&#30830;&#23450;&#19968;&#20010;&#33021;&#20805;&#20998;&#35299;&#37322;&#21709;&#24212;&#21464;&#21270;&#30340;&#31232;&#30095;&#27169;&#22411;&#12290;&#34429;&#28982;&#26368;&#20339;&#23376;&#38598;&#36873;&#25321;&#19968;&#30452;&#34987;&#35748;&#20026;&#26159;&#36825;&#31867;&#38382;&#39064;&#30340;&#32456;&#26497;&#30446;&#26631;&#65292;&#20294;&#35201;&#21516;&#26102;&#23454;&#29616;&#35745;&#31639;&#25928;&#29575;&#21644;&#32479;&#35745;&#20445;&#35777;&#21364;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#30446;&#30340;&#22312;&#20110;&#21033;&#29992;&#24555;&#36895;&#31639;&#27861;&#65292;&#20197;&#39640;&#30830;&#23450;&#24615;&#36873;&#25321;&#26368;&#20339;&#23376;&#38598;&#65292;&#35299;&#20915;&#36825;&#19968;&#38590;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#28436;&#31034;&#20102;&#19968;&#31181;&#22312;&#27491;&#21017;&#26465;&#20214;&#19979;&#23454;&#29616;&#26368;&#20339;&#23376;&#38598;&#24674;&#22797;&#30340;&#31639;&#27861;&#12290;&#22312;&#19968;&#23450;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#19982;&#26679;&#26412;&#22823;&#23567;&#21644;&#32500;&#25968;&#30340;&#22810;&#39033;&#24335;&#32423;&#21035;&#30456;&#20851;&#12290;&#38500;&#20102;&#23637;&#31034;&#25105;&#20204;&#26041;&#27861;&#30340;&#32479;&#35745;&#29305;&#24615;&#65292;&#24191;&#27867;&#30340;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#23427;&#22312;&#21464;&#37327;&#36873;&#25321;&#21644;&#31995;&#25968;&#20272;&#35745;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;&#36816;&#34892;&#26102;&#38388;&#20998;&#26512;&#26174;&#31034;&#65292;&#19982;&#27969;&#34892;&#30340;&#21464;&#37327;&#36873;&#25321;&#24037;&#20855;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#23454;&#29616;&#23454;&#29616;&#20102;&#36817;4&#20493;&#30340;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;
In high-dimensional generalized linear models, it is crucial to identify a sparse model that adequately accounts for response variation. Although the best subset section has been widely regarded as the Holy Grail of problems of this type, achieving either computational efficiency or statistical guarantees is challenging. In this article, we intend to surmount this obstacle by utilizing a fast algorithm to select the best subset with high certainty. We proposed and illustrated an algorithm for best subset recovery in regularity conditions. Under mild conditions, the computational complexity of our algorithm scales polynomially with sample size and dimension. In addition to demonstrating the statistical properties of our method, extensive numerical experiments reveal that it outperforms existing methods for variable selection and coefficient estimation. The runtime analysis shows that our implementation achieves approximately a fourfold speedup compared to popular variable selection tool
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;EEG&#30340;&#35748;&#30693;&#36127;&#33655;&#20998;&#31867;&#30340;&#26032;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#29305;&#24449;&#25513;&#34109;&#33258;&#32534;&#30721;&#21644;&#24773;&#32490;&#36801;&#31227;&#23398;&#20064;&#26469;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#21644;&#20998;&#31867;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.00246</link><description>&lt;p&gt;
&#22522;&#20110;&#29305;&#24449;&#25513;&#34109;&#33258;&#32534;&#30721;&#21644;&#24773;&#32490;&#36801;&#31227;&#23398;&#20064;&#30340;&#22522;&#20110;EEG&#30340;&#35748;&#30693;&#36127;&#33655;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
EEG-based Cognitive Load Classification using Feature Masked Autoencoding and Emotion Transfer Learning. (arXiv:2308.00246v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00246
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;EEG&#30340;&#35748;&#30693;&#36127;&#33655;&#20998;&#31867;&#30340;&#26032;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#29305;&#24449;&#25513;&#34109;&#33258;&#32534;&#30721;&#21644;&#24773;&#32490;&#36801;&#31227;&#23398;&#20064;&#26469;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#21644;&#20998;&#31867;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35748;&#30693;&#36127;&#33655;&#26159;&#23436;&#25104;&#20219;&#21153;&#25152;&#38656;&#30340;&#24515;&#29702;&#21162;&#21147;&#37327;&#65292;&#22312;&#24615;&#33021;&#21644;&#20915;&#31574;&#32467;&#26524;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#22240;&#27492;&#22312;&#21508;&#31181;&#25935;&#24863;&#39046;&#22495;&#20013;&#20998;&#31867;&#21644;&#20998;&#26512;&#35748;&#30693;&#36127;&#33655;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#20998;&#31867;&#35748;&#30693;&#36127;&#33655;&#30340;&#26032;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#37319;&#29992;&#20102;&#19968;&#31181;&#21464;&#21387;&#22120;&#26550;&#26500;&#65292;&#21033;&#29992;&#24773;&#32490;&#21644;&#35748;&#30693;&#36127;&#33655;&#20043;&#38388;&#30340;&#36801;&#31227;&#23398;&#20064;&#12290;&#25105;&#20204;&#20351;&#29992;&#33258;&#30417;&#30563;&#25513;&#34109;&#33258;&#32534;&#30721;&#22312;&#24773;&#32490;&#30456;&#20851;&#30340;EEG&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#20351;&#29992;&#20923;&#32467;&#26435;&#37325;&#21644;&#24494;&#35843;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#65292;&#36827;&#34892;&#19979;&#28216;&#30340;&#35748;&#30693;&#36127;&#33655;&#20998;&#31867;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#21033;&#29992;&#20102;&#20004;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#22522;&#20110;EEG&#30340;&#24773;&#32490;&#25968;&#25454;&#38598;&#65292;&#21363;SEED&#21644;SEED-IV&#65292;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#21516;&#26102;&#25105;&#20204;&#20351;&#29992;CL-Drive&#25968;&#25454;&#38598;&#36827;&#34892;&#19979;&#28216;&#30340;&#35748;&#30693;&#36127;&#33655;&#20998;&#31867;&#12290;&#25105;&#20204;&#23454;&#39564;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cognitive load, the amount of mental effort required for task completion, plays an important role in performance and decision-making outcomes, making its classification and analysis essential in various sensitive domains. In this paper, we present a new solution for the classification of cognitive load using electroencephalogram (EEG). Our model uses a transformer architecture employing transfer learning between emotions and cognitive load. We pre-train our model using self-supervised masked autoencoding on emotion-related EEG datasets and use transfer learning with both frozen weights and fine-tuning to perform downstream cognitive load classification. To evaluate our method, we carry out a series of experiments utilizing two publicly available EEG-based emotion datasets, namely SEED and SEED-IV, for pre-training, while we use the CL-Drive dataset for downstream cognitive load classification. The results of our experiments show that our proposed approach achieves strong results and ou
&lt;/p&gt;</description></item><item><title>Capsa&#26159;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#29992;&#20110;&#25193;&#23637;&#20855;&#26377;&#39118;&#38505;&#24863;&#30693;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#23427;&#33021;&#22815;&#37327;&#21270;&#22810;&#31181;&#24418;&#24335;&#30340;&#39118;&#38505;&#65292;&#24182;&#23558;&#19981;&#21516;&#31639;&#27861;&#32452;&#21512;&#22312;&#19968;&#36215;&#20197;&#24182;&#34892;&#37327;&#21270;&#19981;&#21516;&#30340;&#39118;&#38505;&#25351;&#26631;&#12290;&#36890;&#36807;&#22312;&#22797;&#26434;&#24863;&#30693;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#31639;&#27861;&#24182;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#39564;&#35777;&#20102;capsa&#30340;&#26377;&#25928;&#24615;&#12290;capsa&#33021;&#22815;&#36731;&#26494;&#32452;&#21512;aleatoric&#19981;&#30830;&#23450;&#24615;&#12289;epistemic&#19981;&#30830;&#23450;&#24615;&#21644;&#20559;&#35265;&#20272;&#35745;&#33021;&#21147;</title><link>http://arxiv.org/abs/2308.00231</link><description>&lt;p&gt;
Capsa: &#29992;&#20110;&#37327;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#39118;&#38505;&#30340;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Capsa: A Unified Framework for Quantifying Risk in Deep Neural Networks. (arXiv:2308.00231v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00231
&lt;/p&gt;
&lt;p&gt;
Capsa&#26159;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#29992;&#20110;&#25193;&#23637;&#20855;&#26377;&#39118;&#38505;&#24863;&#30693;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#23427;&#33021;&#22815;&#37327;&#21270;&#22810;&#31181;&#24418;&#24335;&#30340;&#39118;&#38505;&#65292;&#24182;&#23558;&#19981;&#21516;&#31639;&#27861;&#32452;&#21512;&#22312;&#19968;&#36215;&#20197;&#24182;&#34892;&#37327;&#21270;&#19981;&#21516;&#30340;&#39118;&#38505;&#25351;&#26631;&#12290;&#36890;&#36807;&#22312;&#22797;&#26434;&#24863;&#30693;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#31639;&#27861;&#24182;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#39564;&#35777;&#20102;capsa&#30340;&#26377;&#25928;&#24615;&#12290;capsa&#33021;&#22815;&#36731;&#26494;&#32452;&#21512;aleatoric&#19981;&#30830;&#23450;&#24615;&#12289;epistemic&#19981;&#30830;&#23450;&#24615;&#21644;&#20559;&#35265;&#20272;&#35745;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22797;&#26434;&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20063;&#24120;&#24120;&#20986;&#29616;&#31361;&#28982;&#12289;&#24847;&#22806;&#19988;&#28798;&#38590;&#24615;&#30340;&#22833;&#36133;&#65292;&#23588;&#20854;&#26159;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#20917;&#19979;&#12290;&#29616;&#26377;&#30340;&#20026;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#39118;&#38505;&#24863;&#30693;&#30340;&#31639;&#27861;&#22797;&#26434;&#32780;&#20020;&#26102;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#36827;&#34892;&#37325;&#22823;&#30340;&#24037;&#31243;&#25913;&#21464;&#65292;&#36890;&#24120;&#21482;&#38024;&#23545;&#29305;&#23450;&#35774;&#32622;&#36827;&#34892;&#24320;&#21457;&#65292;&#24182;&#19988;&#24456;&#38590;&#32452;&#21512;&#22312;&#19968;&#36215;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#25552;&#20986;&#20102;capsa&#65292;&#19968;&#20010;&#29992;&#20110;&#25193;&#23637;&#20855;&#26377;&#39118;&#38505;&#24863;&#30693;&#30340;&#27169;&#22411;&#30340;&#26694;&#26550;&#12290;Capsa&#25552;&#20379;&#20102;&#19968;&#31181;&#37327;&#21270;&#22810;&#31181;&#24418;&#24335;&#39118;&#38505;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#19981;&#21516;&#30340;&#31639;&#27861;&#32452;&#21512;&#22312;&#19968;&#36215;&#20197;&#24182;&#34892;&#37327;&#21270;&#19981;&#21516;&#30340;&#39118;&#38505;&#25351;&#26631;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;capsa&#26694;&#26550;&#20013;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#31639;&#27861;&#24182;&#22312;&#22797;&#26434;&#24863;&#30693;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#39564;&#35777;&#20102;capsa&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;capsa&#36731;&#26494;&#32452;&#21512;aleatoric&#19981;&#30830;&#23450;&#24615;&#12289;epistemic&#19981;&#30830;&#23450;&#24615;&#21644;&#20559;&#35265;&#20272;&#35745;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
The modern pervasiveness of large-scale deep neural networks (NNs) is driven by their extraordinary performance on complex problems but is also plagued by their sudden, unexpected, and often catastrophic failures, particularly on challenging scenarios. Existing algorithms that provide risk-awareness to NNs are complex and ad-hoc. Specifically, these methods require significant engineering changes, are often developed only for particular settings, and are not easily composable. Here we present capsa, a framework for extending models with risk-awareness. Capsa provides a methodology for quantifying multiple forms of risk and composing different algorithms together to quantify different risk metrics in parallel. We validate capsa by implementing state-of-the-art uncertainty estimation algorithms within the capsa framework and benchmarking them on complex perception datasets. We demonstrate capsa's ability to easily compose aleatoric uncertainty, epistemic uncertainty, and bias estimation 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#65292;&#32463;&#36807;&#25351;&#23548;&#35843;&#20248;&#30340;&#35821;&#35328;&#27169;&#22411;&#21576;&#29616;&#20986;&#26032;&#20852;&#30340;&#35748;&#30693;&#20559;&#35265;&#65292;&#36825;&#23545;&#20110;&#29702;&#35299;&#21644;&#24320;&#21457;&#26356;&#21487;&#38752;&#21644;&#26080;&#20559;&#30340;&#35821;&#35328;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2308.00225</link><description>&lt;p&gt;
&#34987;&#25351;&#23548;&#30340;&#20559;&#35265;&#65306;&#32463;&#36807;&#25351;&#23548;&#35843;&#20248;&#30340;&#35821;&#35328;&#27169;&#22411;&#21576;&#29616;&#20986;&#26032;&#20852;&#30340;&#35748;&#30693;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Instructed to Bias: Instruction-Tuned Language Models Exhibit Emergent Cognitive Bias. (arXiv:2308.00225v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00225
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#65292;&#32463;&#36807;&#25351;&#23548;&#35843;&#20248;&#30340;&#35821;&#35328;&#27169;&#22411;&#21576;&#29616;&#20986;&#26032;&#20852;&#30340;&#35748;&#30693;&#20559;&#35265;&#65292;&#36825;&#23545;&#20110;&#29702;&#35299;&#21644;&#24320;&#21457;&#26356;&#21487;&#38752;&#21644;&#26080;&#20559;&#30340;&#35821;&#35328;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#25351;&#23548;&#35843;&#20248;&#21644;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#33021;&#21147;&#12290;&#34429;&#28982;&#36825;&#20123;&#35843;&#20248;&#26041;&#27861;&#21487;&#20197;&#20351;&#27169;&#22411;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#65292;&#20294;&#25105;&#20204;&#25512;&#27979;&#36825;&#20123;&#32463;&#36807;&#35843;&#20248;&#30340;&#27169;&#22411;&#21487;&#33021;&#20250;&#20135;&#29983;&#26356;&#22810;&#38544;&#21547;&#30340;&#35748;&#30693;&#20559;&#35265;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#35777;&#25454;&#65292;&#34920;&#26126;&#36825;&#20123;&#32463;&#36807;&#35843;&#20248;&#30340;&#27169;&#22411;&#21576;&#29616;&#20986;&#20808;&#21069;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#19981;&#23384;&#22312;&#25110;&#36739;&#19981;&#26126;&#26174;&#30340;&#20559;&#35265;&#12290;&#25105;&#20204;&#23545;&#19977;&#31181;&#35748;&#30693;&#20559;&#35265;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21253;&#25324;&#30683;&#30462;&#25928;&#24212;&#12289;&#30830;&#23450;&#24615;&#25928;&#24212;&#21644;&#20449;&#24565;&#20559;&#35265;&#65292;&#36825;&#20123;&#20559;&#35265;&#24050;&#34987;&#35777;&#23454;&#23545;&#20154;&#31867;&#30340;&#20915;&#31574;&#21644;&#25512;&#29702;&#26377;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#31361;&#26174;&#20102;&#36825;&#20123;&#20559;&#35265;&#22312;&#21508;&#31181;&#27169;&#22411;&#20013;&#30340;&#23384;&#22312;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#32463;&#36807;&#25351;&#23548;&#35843;&#20248;&#30340;&#27169;&#22411;&#65292;&#22914;Flan-T5&#12289;GPT3.5&#21644;GPT4&#12290;&#36825;&#39033;&#30740;&#31350;&#23545;&#20110;&#29702;&#35299;&#25351;&#23548;&#35843;&#20248;&#30340;LMs&#20013;&#30340;&#35748;&#30693;&#20559;&#35265;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#36825;&#26377;&#21161;&#20110;&#24320;&#21457;&#26356;&#21487;&#38752;&#21644;&#26080;&#20559;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies show that instruction tuning and learning from human feedback improve the abilities of large language models (LMs) dramatically. While these tuning methods can make models generate high-quality text, we conjecture that more implicit cognitive biases may arise in these fine-tuned models. Our work provides evidence that these fine-tuned models exhibit biases that were absent or less pronounced in their pretrained predecessors. We examine the extent of this phenomenon in three cognitive biases - the decoy effect, the certainty effect, and the belief bias - all of which are known to influence human decision-making and reasoning. Our findings highlight the presence of these biases in various models, especially those that have undergone instruction tuning, such as Flan-T5, GPT3.5, and GPT4. This research constitutes a step toward comprehending cognitive biases in instruction-tuned LMs, which is crucial for the development of more reliable and unbiased language models.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#30005;&#27744;&#35843;&#25511;&#20998;&#23618;V2G&#21327;&#35843;&#31574;&#30053;&#65292;&#26088;&#22312;&#20419;&#36827;&#21487;&#20877;&#29983;&#33021;&#28304;&#21033;&#29992;&#21644;&#30005;&#32593;&#31283;&#23450;&#12290;&#35813;&#31574;&#30053;&#21487;&#20197;&#23454;&#29616;&#22810;&#26041;&#21033;&#30410;&#65292;&#24182;&#19988;&#32771;&#34385;&#20102;&#30005;&#32593;&#12289;&#30005;&#21160;&#36710;&#32858;&#21512;&#22120;&#21644;&#29992;&#25143;&#30340;&#21508;&#31181;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2308.00218</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#30005;&#27744;&#35843;&#25511;&#20998;&#23618;V2G&#21327;&#35843;&#31574;&#30053;&#20197;&#23454;&#29616;&#22810;&#26041;&#21033;&#30410;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning-Based Battery Conditioning Hierarchical V2G Coordination for Multi-Stakeholder Benefits. (arXiv:2308.00218v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00218
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#30005;&#27744;&#35843;&#25511;&#20998;&#23618;V2G&#21327;&#35843;&#31574;&#30053;&#65292;&#26088;&#22312;&#20419;&#36827;&#21487;&#20877;&#29983;&#33021;&#28304;&#21033;&#29992;&#21644;&#30005;&#32593;&#31283;&#23450;&#12290;&#35813;&#31574;&#30053;&#21487;&#20197;&#23454;&#29616;&#22810;&#26041;&#21033;&#30410;&#65292;&#24182;&#19988;&#32771;&#34385;&#20102;&#30005;&#32593;&#12289;&#30005;&#21160;&#36710;&#32858;&#21512;&#22120;&#21644;&#29992;&#25143;&#30340;&#21508;&#31181;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#30005;&#21160;&#36710;&#65288;EV&#65289;&#30340;&#26222;&#21450;&#21644;EV&#30005;&#23376;&#25216;&#26415;&#30340;&#21457;&#23637;&#65292;&#36710;&#36742;&#23545;&#30005;&#32593;&#65288;V2G&#65289;&#25216;&#26415;&#21644;&#22823;&#35268;&#27169;&#35843;&#24230;&#31574;&#30053;&#24050;&#32463;&#20986;&#29616;&#65292;&#20197;&#20419;&#36827;&#21487;&#20877;&#29983;&#33021;&#28304;&#21033;&#29992;&#21644;&#30005;&#32593;&#31283;&#23450;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#21644;&#26435;&#30410;&#35777;&#26126;&#31639;&#27861;&#30340;&#22810;&#26041;&#21442;&#19982;&#32773;&#20998;&#23618;V2G&#21327;&#35843;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#22810;&#26041;&#21442;&#19982;&#32773;&#21253;&#25324;&#30005;&#32593;&#12289;&#30005;&#21160;&#36710;&#32858;&#21512;&#22120;&#65288;EVAs&#65289;&#21644;&#29992;&#25143;&#65292;&#24182;&#19988;&#25152;&#25552;&#20986;&#30340;&#31574;&#30053;&#21487;&#20197;&#23454;&#29616;&#22810;&#26041;&#21033;&#30410;&#12290;&#22312;&#30005;&#32593;&#26041;&#38754;&#65292;&#32771;&#34385;&#21040;&#36127;&#33655;&#27874;&#21160;&#21644;&#21487;&#20877;&#29983;&#33021;&#28304;&#28040;&#32791;&#65292;&#32780;&#22312;EVA&#26041;&#38754;&#65292;&#32771;&#34385;&#20102;&#33021;&#28304;&#38480;&#21046;&#21644;&#20805;&#30005;&#25104;&#26412;&#12290;&#29992;&#25143;&#26041;&#38754;&#32771;&#34385;&#20102;&#30005;&#27744;SOX&#30340;&#19977;&#20010;&#20851;&#38190;&#35843;&#33410;&#21442;&#25968;&#65292;&#21253;&#25324;&#30005;&#33655;&#29366;&#24577;&#12289;&#21151;&#29575;&#29366;&#24577;&#21644;&#20581;&#24247;&#29366;&#24577;&#12290;&#19982;&#22235;&#31181;&#20856;&#22411;&#22522;&#32447;&#30456;&#27604;&#65292;&#22810;&#26041;&#21442;&#19982;&#32773;&#20998;&#23618;&#21327;&#35843;&#31574;&#30053;&#21487;&#20197;&#22686;&#24378;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#21033;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the growing prevalence of electric vehicles (EVs) and advancements in EV electronics, vehicle-to-grid (V2G) techniques and large-scale scheduling strategies have emerged to promote renewable energy utilization and power grid stability. This study proposes a multi-stakeholder hierarchical V2G coordination based on deep reinforcement learning (DRL) and the Proof of Stake algorithm. Furthermore, the multi-stakeholders include the power grid, EV aggregators (EVAs), and users, and the proposed strategy can achieve multi-stakeholder benefits. On the grid side, load fluctuations and renewable energy consumption are considered, while on the EVA side, energy constraints and charging costs are considered. The three critical battery conditioning parameters of battery SOX are considered on the user side, including state of charge, state of power, and state of health. Compared with four typical baselines, the multi-stakeholder hierarchical coordination strategy can enhance renewable energy con
&lt;/p&gt;</description></item><item><title>&#22312;&#24433;&#20687;&#23548;&#24341;&#30340;&#24494;&#21019;&#21307;&#30103;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;X&#23556;&#32447;&#25237;&#24433;&#36827;&#34892;&#36752;&#23556;&#36879;&#26126;&#29289;&#20307;&#30340;&#23039;&#24577;&#20272;&#35745;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#20248;&#21270;&#35270;&#22270;&#21512;&#25104;&#22312;&#23436;&#25104;&#27492;&#20219;&#21153;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.00214</link><description>&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#35843;&#25972;&#23618;&#26512;&#25104;&#20687;&#65288;NeTT&#65289;&#21644;&#25513;&#34109;&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;mNeRF&#65289;&#30340;&#31283;&#20581;&#21333;&#35270;&#38181;&#24418;X&#23556;&#32447;&#23039;&#24577;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Robust Single-view Cone-beam X-ray Pose Estimation with Neural Tuned Tomography (NeTT) and Masked Neural Radiance Fields (mNeRF). (arXiv:2308.00214v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00214
&lt;/p&gt;
&lt;p&gt;
&#22312;&#24433;&#20687;&#23548;&#24341;&#30340;&#24494;&#21019;&#21307;&#30103;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;X&#23556;&#32447;&#25237;&#24433;&#36827;&#34892;&#36752;&#23556;&#36879;&#26126;&#29289;&#20307;&#30340;&#23039;&#24577;&#20272;&#35745;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#20248;&#21270;&#35270;&#22270;&#21512;&#25104;&#22312;&#23436;&#25104;&#27492;&#20219;&#21153;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24433;&#20687;&#23548;&#24341;&#30340;&#24494;&#21019;&#21307;&#30103;&#36807;&#31243;&#20013;&#65292;&#35768;&#22810;&#20219;&#21153;&#21487;&#20197;&#30475;&#20316;&#26159;&#23039;&#24577;&#20272;&#35745;&#38382;&#39064;&#65292;&#20854;&#20013;&#21033;&#29992;X&#23556;&#32447;&#25237;&#24433;&#26469;&#36798;&#21040;3D&#31354;&#38388;&#20013;&#30340;&#30446;&#26631;&#12290;&#36817;&#26399;&#22312;&#21487;&#24494;&#20998;&#28210;&#26579;&#25216;&#26415;&#19978;&#30340;&#36827;&#23637;&#20351;&#24471;RGB&#30456;&#26426;&#35270;&#22270;&#21512;&#25104;&#21644;&#23039;&#24577;&#20272;&#35745;&#30340;&#24615;&#33021;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;&#22312;&#20043;&#21069;&#30340;&#24037;&#20316;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20351;&#29992;X&#23556;&#32447;&#25237;&#24433;&#36827;&#34892;&#36752;&#23556;&#36879;&#26126;&#29289;&#20307;&#30340;&#23039;&#24577;&#20272;&#35745;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#20248;&#21270;&#35270;&#22270;&#21512;&#25104;&#22312;&#23436;&#25104;&#27492;&#20219;&#21153;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#39318;&#20808;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#31639;&#27861;&#65288;DiffDRR&#65289;&#65292;&#33021;&#22815;&#22312;TensorFlow&#20013;&#39640;&#25928;&#35745;&#31639;&#25968;&#23383;&#37325;&#24314;&#25918;&#23556;&#22270;&#20687;&#65288;DRRs&#65289;&#24182;&#21033;&#29992;&#33258;&#21160;&#24494;&#20998;&#12290;&#32467;&#21512;&#32463;&#20856;&#30340;CBCT&#37325;&#24314;&#31639;&#27861;&#65292;&#25105;&#20204;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#36827;&#34892;&#23039;&#24577;&#20272;&#35745;&#65292;&#20351;&#29992;&#19968;&#20010;&#25439;&#22833;&#20989;&#25968;&#26469;&#37327;&#21270;&#20174;&#38543;&#26426;&#21021;&#22987;&#21270;&#23039;&#24577;&#21512;&#25104;&#30340;DRR&#19982;&#30446;&#26631;&#22788;&#30495;&#23454;&#36879;&#35270;&#22270;&#20687;&#30340;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many tasks performed in image-guided, mini-invasive, medical procedures can be cast as pose estimation problems, where an X-ray projection is utilized to reach a target in 3D space. Recent advances in the differentiable rendering of optically reflective materials have enabled state-of-the-art performance in RGB camera view synthesis and pose estimation. Expanding on these prior works, we introduce new methods for pose estimation of radiolucent objects using X-ray projections, and we demonstrate the critical role of optimal view synthesis in performing this task. We first develop an algorithm (DiffDRR) that efficiently computes Digitally Reconstructed Radiographs (DRRs) and leverages automatic differentiation within TensorFlow. In conjunction with classic CBCT reconstruction algorithms, we perform pose estimation by gradient descent using a loss function that quantifies the similarity of the DRR synthesized from a randomly initialized pose and the true fluoroscopic image at the target p
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;SkullGAN&#65292;&#19968;&#31181;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#65292;&#29983;&#25104;&#21512;&#25104;&#30340;&#39045;&#39592;CT&#22270;&#20687;&#65292;&#21487;&#20197;&#20943;&#23569;&#23545;&#30495;&#23454;&#22270;&#20687;&#30340;&#20381;&#36182;&#65292;&#21152;&#36895;&#23558;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#25972;&#21512;&#12290;</title><link>http://arxiv.org/abs/2308.00206</link><description>&lt;p&gt;
SkullGAN: &#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#29983;&#25104;&#21512;&#25104;&#30340;&#39045;&#39592;CT&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
SkullGAN: Synthetic Skull CT Generation with Generative Adversarial Networks. (arXiv:2308.00206v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00206
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;SkullGAN&#65292;&#19968;&#31181;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#65292;&#29983;&#25104;&#21512;&#25104;&#30340;&#39045;&#39592;CT&#22270;&#20687;&#65292;&#21487;&#20197;&#20943;&#23569;&#23545;&#30495;&#23454;&#22270;&#20687;&#30340;&#20381;&#36182;&#65292;&#21152;&#36895;&#23558;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#25972;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#28041;&#21450;&#20154;&#31867;&#39045;&#39592;&#30340;&#21508;&#31181;&#21307;&#30103;&#24212;&#29992;&#20013;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#32463;&#36807;&#31574;&#21010;&#30340;&#21307;&#23398;&#22270;&#20687;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SkullGAN&#65292;&#19968;&#31181;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#65292;&#29992;&#20110;&#21019;&#24314;&#22823;&#35268;&#27169;&#30340;&#21512;&#25104;&#39045;&#39592;CT&#20999;&#29255;&#25968;&#25454;&#38598;&#65292;&#20943;&#23569;&#23545;&#30495;&#23454;&#22270;&#20687;&#30340;&#20381;&#36182;&#24182;&#21152;&#36895;&#23558;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#25972;&#21512;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#23545;38&#20010;&#21463;&#35797;&#32773;&#36827;&#34892;&#20102;&#39045;&#39592;CT&#20999;&#29255;&#36755;&#20837;SkullGAN&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;2&#20159;&#20010;&#21442;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#29983;&#25104;&#30340;&#21512;&#25104;&#39045;&#39592;&#22270;&#20687;&#26681;&#25454;&#19977;&#20010;&#23450;&#37327;&#25918;&#23556;&#23398;&#29305;&#24449;&#36827;&#34892;&#35780;&#20272;&#65306;&#39045;&#39592;&#23494;&#24230;&#27604;&#65288;SDR&#65289;&#12289;&#24179;&#22343;&#21402;&#24230;&#21644;&#24179;&#22343;&#24378;&#24230;&#12290;&#21516;&#26102;&#65292;&#20351;&#29992;t-&#20998;&#24067;&#38543;&#26426;&#37051;&#22495;&#23884;&#20837;&#65288;t-SNE&#65289;&#36827;&#34892;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#65292;&#24182;&#23558;SkullGAN&#21028;&#21035;&#22120;&#20316;&#20026;&#20998;&#31867;&#22120;&#36827;&#34892;&#24212;&#29992;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;SkullGAN&#29983;&#25104;&#30340;&#22270;&#20687;&#19982;&#30495;&#23454;&#39045;&#39592;&#20855;&#26377;&#31867;&#20284;&#30340;&#20851;&#38190;&#23450;&#37327;&#25918;&#23556;&#23398;&#29305;&#24449;&#12290;&#36827;&#19968;&#27493;&#30340;&#30830;&#23450;&#24615;&#20998;&#26512;&#26159;&#36890;&#36807;&#36827;&#34892;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#21150;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning offers potential for various healthcare applications involving the human skull but requires extensive datasets of curated medical images. To overcome this challenge, we propose SkullGAN, a generative adversarial network (GAN), to create large datasets of synthetic skull CT slices, reducing reliance on real images and accelerating the integration of machine learning into healthcare. In our method, CT slices of 38 subjects were fed to SkullGAN, a neural network comprising over 200 million parameters. The synthetic skull images generated were evaluated based on three quantitative radiological features: skull density ratio (SDR), mean thickness, and mean intensity. They were further analyzed using t-distributed stochastic neighbor embedding (t-SNE) and by applying the SkullGAN discriminator as a classifier. The results showed that SkullGAN-generated images demonstrated similar key quantitative radiological features to real skulls. Further definitive analysis was undertaken by
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;CBCL-PR&#26694;&#26550;&#65292;&#28789;&#24863;&#26469;&#33258;&#28023;&#39532;&#20307;&#21644;&#26032;&#30382;&#23618;&#30340;&#27010;&#24565;&#23398;&#20064;&#29702;&#35770;&#65292;&#29992;&#20110;&#35299;&#20915;&#26426;&#22120;&#20154;&#39046;&#22495;&#20013;&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#22238;&#25918;&#26087;&#31867;&#21035;&#30340;&#25968;&#25454;&#26469;&#36991;&#20813;&#36951;&#24536;&#65292;&#22312;&#30446;&#26631;&#20998;&#31867;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#24615;&#33021;&#12290;&#21516;&#26102;&#65292;&#22312;&#26426;&#22120;&#20154;&#19978;&#30340;&#23454;&#39564;&#20063;&#35777;&#23454;&#20102;&#35813;&#26694;&#26550;&#22312;&#22823;&#37327;&#23478;&#24237;&#29289;&#21697;&#30340;&#20998;&#31867;&#20219;&#21153;&#20013;&#26377;&#30528;&#20248;&#31168;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2308.00199</link><description>&lt;p&gt;
CBCL-PR&#65306;&#19968;&#31181;&#28789;&#24863;&#26469;&#33258;&#35748;&#30693;&#30340;&#26426;&#22120;&#20154;&#31867;&#22686;&#37327;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CBCL-PR: A Cognitively Inspired Model for Class-Incremental Learning in Robotics. (arXiv:2308.00199v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00199
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;CBCL-PR&#26694;&#26550;&#65292;&#28789;&#24863;&#26469;&#33258;&#28023;&#39532;&#20307;&#21644;&#26032;&#30382;&#23618;&#30340;&#27010;&#24565;&#23398;&#20064;&#29702;&#35770;&#65292;&#29992;&#20110;&#35299;&#20915;&#26426;&#22120;&#20154;&#39046;&#22495;&#20013;&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#22238;&#25918;&#26087;&#31867;&#21035;&#30340;&#25968;&#25454;&#26469;&#36991;&#20813;&#36951;&#24536;&#65292;&#22312;&#30446;&#26631;&#20998;&#31867;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#24615;&#33021;&#12290;&#21516;&#26102;&#65292;&#22312;&#26426;&#22120;&#20154;&#19978;&#30340;&#23454;&#39564;&#20063;&#35777;&#23454;&#20102;&#35813;&#26694;&#26550;&#22312;&#22823;&#37327;&#23478;&#24237;&#29289;&#21697;&#30340;&#20998;&#31867;&#20219;&#21153;&#20013;&#26377;&#30528;&#20248;&#31168;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22823;&#22810;&#25968;&#23454;&#38469;&#24212;&#29992;&#65292;&#26426;&#22120;&#20154;&#38656;&#35201;&#22312;&#29615;&#22659;&#20013;&#26377;&#38480;&#30340;&#25968;&#25454;&#19979;&#36827;&#34892;&#19981;&#26029;&#22320;&#33258;&#36866;&#24212;&#21644;&#23398;&#20064;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#23398;&#20064;&#65288;FSIL&#65289;&#30340;&#38382;&#39064;&#65292;&#21363;&#35201;&#27714;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#22312;&#21482;&#26377;&#23569;&#37327;&#25968;&#25454;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#22686;&#37327;&#23398;&#20064;&#65292;&#21516;&#26102;&#19981;&#20250;&#24536;&#35760;&#20043;&#21069;&#23398;&#20064;&#30340;&#25968;&#25454;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#28789;&#24863;&#26469;&#33258;&#28023;&#39532;&#20307;&#21644;&#26032;&#30382;&#23618;&#30340;&#27010;&#24565;&#23398;&#20064;&#29702;&#35770;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#23558;&#23545;&#35937;&#31867;&#34920;&#31034;&#20026;&#19968;&#32452;&#31751;&#65292;&#24182;&#23558;&#20854;&#23384;&#20648;&#22312;&#20869;&#23384;&#20013;&#12290;&#26694;&#26550;&#36890;&#36807;&#22238;&#25918;&#26087;&#31867;&#21035;&#30340;&#31751;&#29983;&#25104;&#30340;&#25968;&#25454;&#26469;&#36991;&#20813;&#22312;&#23398;&#20064;&#26032;&#31867;&#21035;&#26102;&#24536;&#35760;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20004;&#20010;&#30446;&#26631;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24471;&#21040;&#20102;&#31867;&#22686;&#37327;&#23398;&#20064;&#21644;FSIL&#26041;&#38754;&#30340;&#26368;&#26032;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#22312;&#26426;&#22120;&#20154;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;FSIL&#26041;&#38754;&#65292;&#32467;&#26524;&#34920;&#26126;&#26426;&#22120;&#20154;&#21487;&#20197;&#19981;&#26029;&#23398;&#20064;&#20998;&#31867;&#22823;&#37327;&#30340;&#23478;&#24237;&#29289;&#21697;&#65292;&#19988;&#20998;&#31867;&#24615;&#33021;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;
For most real-world applications, robots need to adapt and learn continually with limited data in their environments. In this paper, we consider the problem of Few-Shot class Incremental Learning (FSIL), in which an AI agent is required to learn incrementally from a few data samples without forgetting the data it has previously learned. To solve this problem, we present a novel framework inspired by theories of concept learning in the hippocampus and the neocortex. Our framework represents object classes in the form of sets of clusters and stores them in memory. The framework replays data generated by the clusters of the old classes, to avoid forgetting when learning new classes. Our approach is evaluated on two object classification datasets resulting in state-of-the-art (SOTA) performance for class-incremental learning and FSIL. We also evaluate our framework for FSIL on a robot demonstrating that the robot can continually learn to classify a large set of household objects with limit
&lt;/p&gt;</description></item><item><title>C-DARL&#26159;&#19968;&#31181;&#33258;&#30417;&#30563;&#30340;&#34880;&#31649;&#20998;&#21106;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#21644;&#29983;&#25104;&#27169;&#22359;&#30340;&#20351;&#29992;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#22810;&#39046;&#22495;&#34880;&#31649;&#25968;&#25454;&#30340;&#20998;&#24067;&#65292;&#24182;&#29983;&#25104;&#26356;&#30495;&#23454;&#30340;&#34880;&#31649;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2308.00193</link><description>&lt;p&gt;
C-DARL: &#26080;&#26631;&#31614;&#34880;&#31649;&#20998;&#21106;&#30340;&#23545;&#27604;&#25193;&#25955;&#23545;&#25239;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
C-DARL: Contrastive diffusion adversarial representation learning for label-free blood vessel segmentation. (arXiv:2308.00193v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00193
&lt;/p&gt;
&lt;p&gt;
C-DARL&#26159;&#19968;&#31181;&#33258;&#30417;&#30563;&#30340;&#34880;&#31649;&#20998;&#21106;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#21644;&#29983;&#25104;&#27169;&#22359;&#30340;&#20351;&#29992;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#22810;&#39046;&#22495;&#34880;&#31649;&#25968;&#25454;&#30340;&#20998;&#24067;&#65292;&#24182;&#29983;&#25104;&#26356;&#30495;&#23454;&#30340;&#34880;&#31649;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34880;&#31649;&#22312;&#21307;&#23398;&#25104;&#20687;&#20013;&#30340;&#20998;&#21106;&#26159;&#34880;&#31649;&#30142;&#30149;&#35786;&#26029;&#21644;&#20171;&#20837;&#27835;&#30103;&#35268;&#21010;&#20013;&#30340;&#37325;&#35201;&#27493;&#39588;&#20043;&#19968;&#65292;&#28041;&#21450;&#22270;&#20687;&#21307;&#23398;&#21644;&#20171;&#20837;&#21307;&#23398;&#30340;&#24191;&#27867;&#24212;&#29992;&#22330;&#26223;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32454;&#24494;&#30340;&#20998;&#25903;&#21644;&#22797;&#26434;&#30340;&#32467;&#26500;&#65292;&#34880;&#31649;&#25513;&#33180;&#30340;&#25163;&#24037;&#27880;&#37322;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#36164;&#28304;&#23494;&#38598;&#22411;&#30340;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#34880;&#31649;&#20998;&#21106;&#26041;&#27861;&#65292;&#31216;&#20026;&#23545;&#27604;&#25193;&#25955;&#23545;&#25239;&#34920;&#31034;&#23398;&#20064;&#65288;C-DARL&#65289;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21253;&#25324;&#19968;&#20010;&#25193;&#25955;&#27169;&#22359;&#21644;&#19968;&#20010;&#29983;&#25104;&#27169;&#22359;&#65292;&#36890;&#36807;&#20174;&#25193;&#25955;&#28508;&#21464;&#37327;&#29983;&#25104;&#21512;&#25104;&#34880;&#31649;&#22270;&#20687;&#26469;&#23398;&#20064;&#22810;&#39046;&#22495;&#34880;&#31649;&#25968;&#25454;&#30340;&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#25513;&#33180;&#30340;&#23545;&#27604;&#25439;&#22833;&#36827;&#34892;&#23545;&#27604;&#23398;&#20064;&#65292;&#20351;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#26356;&#21152;&#30495;&#23454;&#30340;&#34880;&#31649;&#34920;&#31034;&#12290;&#20026;&#20102;&#39564;&#35777;C-DARL&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;&#22810;&#31181;&#34880;&#31649;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;&#21253;&#25324;&#20896;&#29366;&#21160;&#33033;&#36896;&#24433;&#22270;&#20687;&#12289;&#33145;&#37096;&#25968;&#23383;&#21270;&#22270;&#20687;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Blood vessel segmentation in medical imaging is one of the essential steps for vascular disease diagnosis and interventional planning in a broad spectrum of clinical scenarios in image-based medicine and interventional medicine. Unfortunately, manual annotation of the vessel masks is challenging and resource-intensive due to subtle branches and complex structures. To overcome this issue, this paper presents a self-supervised vessel segmentation method, dubbed the contrastive diffusion adversarial representation learning (C-DARL) model. Our model is composed of a diffusion module and a generation module that learns the distribution of multi-domain blood vessel data by generating synthetic vessel images from diffusion latent. Moreover, we employ contrastive learning through a mask-based contrastive loss so that the model can learn more realistic vessel representations. To validate the efficacy, C-DARL is trained using various vessel datasets, including coronary angiograms, abdominal digi
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#20027;&#23548;-&#26497;&#23567;&#21270;&#31639;&#27861;&#65292;&#23427;&#36890;&#36807;&#33258;&#21160;&#25512;&#23548;&#20027;&#23548;&#22120;&#26469;&#35299;&#20915;&#20219;&#24847;&#38382;&#39064;&#65292;&#24182;&#19988;&#21487;&#20197;&#20174;&#20219;&#20309;&#36215;&#22987;&#28857;&#25910;&#25947;&#65292;&#26080;&#38656;&#35843;&#25972;&#36229;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2308.00190</link><description>&lt;p&gt;
&#36890;&#29992;&#20027;&#23548;-&#26497;&#23567;&#21270;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Universal Majorization-Minimization Algorithms. (arXiv:2308.00190v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00190
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#20027;&#23548;-&#26497;&#23567;&#21270;&#31639;&#27861;&#65292;&#23427;&#36890;&#36807;&#33258;&#21160;&#25512;&#23548;&#20027;&#23548;&#22120;&#26469;&#35299;&#20915;&#20219;&#24847;&#38382;&#39064;&#65292;&#24182;&#19988;&#21487;&#20197;&#20174;&#20219;&#20309;&#36215;&#22987;&#28857;&#25910;&#25947;&#65292;&#26080;&#38656;&#35843;&#25972;&#36229;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#23548;-&#26497;&#23567;&#21270;&#65288;MM&#65289;&#26159;&#19968;&#31867;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#19968;&#20010;&#23616;&#37096;&#32039;&#19978;&#30028;&#65292;&#21363;&#20027;&#23548;&#22120;&#65292;&#26469;&#36845;&#20195;&#22320;&#38477;&#20302;&#25439;&#22833;&#12290;&#20256;&#32479;&#19978;&#65292;&#20027;&#23548;&#22120;&#26159;&#25163;&#21160;&#25512;&#23548;&#30340;&#65292;&#22240;&#27492;MM&#21482;&#36866;&#29992;&#20110;&#23569;&#25968;&#32463;&#36807;&#28145;&#20837;&#30740;&#31350;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;&#20351;&#29992;&#26368;&#36817;&#30340;&#27888;&#21202;&#27169;&#24335;&#33258;&#21160;&#24494;&#20998;&#30340;&#24191;&#20041;&#21270;&#26041;&#27861;&#26469;&#33258;&#21160;&#25512;&#23548;&#20027;&#23548;&#22120;&#12290;&#36825;&#20123;&#36890;&#29992;&#30340;MM&#20248;&#21270;&#22120;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#24847;&#38382;&#39064;&#65292;&#24182;&#19988;&#20174;&#20219;&#20309;&#36215;&#22987;&#28857;&#25910;&#25947;&#65292;&#26080;&#38656;&#35843;&#25972;&#36229;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Majorization-minimization (MM) is a family of optimization methods that iteratively reduce a loss by minimizing a locally-tight upper bound, called a majorizer. Traditionally, majorizers were derived by hand, and MM was only applicable to a small number of well-studied problems. We present optimizers that instead derive majorizers automatically, using a recent generalization of Taylor mode automatic differentiation. These universal MM optimizers can be applied to arbitrary problems and converge from any starting point, with no hyperparameter tuning.
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#27169;&#22411;&#20316;&#20026;&#22797;&#26434;&#31995;&#32479;&#31185;&#23398;&#65292;&#23427;&#20204;&#33021;&#22815;&#23436;&#25104;&#20219;&#21153;&#30340;&#34892;&#20026;&#34920;&#29616;&#38656;&#35201;&#34987;&#35299;&#37322;&#21644;&#29702;&#35299;&#65292;&#20197;&#23454;&#29616;&#23545;&#20854;&#34892;&#20026;&#30340;&#25511;&#21046;&#21644;&#26410;&#26469;&#30740;&#31350;&#30340;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2308.00189</link><description>&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#20316;&#20026;&#22797;&#26434;&#31995;&#32479;&#31185;&#23398;&#65306;&#22914;&#20309;&#29702;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#34892;&#20026;&#65311;
&lt;/p&gt;
&lt;p&gt;
Generative Models as a Complex Systems Science: How can we make sense of large language model behavior?. (arXiv:2308.00189v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00189
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#20316;&#20026;&#22797;&#26434;&#31995;&#32479;&#31185;&#23398;&#65292;&#23427;&#20204;&#33021;&#22815;&#23436;&#25104;&#20219;&#21153;&#30340;&#34892;&#20026;&#34920;&#29616;&#38656;&#35201;&#34987;&#35299;&#37322;&#21644;&#29702;&#35299;&#65292;&#20197;&#23454;&#29616;&#23545;&#20854;&#34892;&#20026;&#30340;&#25511;&#21046;&#21644;&#26410;&#26469;&#30740;&#31350;&#30340;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#24341;&#23548;&#20986;&#26399;&#26395;&#30340;&#34892;&#20026;&#65292;&#21516;&#26102;&#36991;&#20813;&#19981;&#33391;&#34892;&#20026;&#65292;&#37325;&#26032;&#23450;&#20041;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24182;&#27491;&#22312;&#37325;&#26032;&#22609;&#36896;&#25105;&#20204;&#19982;&#35745;&#31639;&#26426;&#30340;&#20132;&#20114;&#26041;&#24335;&#12290;&#26366;&#32463;&#26159;&#19968;&#20010;&#31185;&#23398;&#24037;&#31243;&#23398;&#31185;&#65292;&#23558;&#26500;&#24314;&#27169;&#22359;&#22534;&#21472;&#22312;&#19968;&#36215;&#65292;&#29616;&#22312;&#21487;&#20197;&#35828;&#24050;&#32463;&#26159;&#19968;&#20010;&#22797;&#26434;&#31995;&#32479;&#31185;&#23398;&#65292;&#20854;&#20013;&#23547;&#27714;&#20986;&#29616;&#30340;&#34892;&#20026;&#20197;&#25903;&#25345;&#20197;&#21069;&#26080;&#27861;&#24819;&#35937;&#30340;&#29992;&#20363;&#12290;&#23613;&#31649;&#26377;&#36234;&#26469;&#36234;&#22810;&#30340;&#22522;&#20934;&#27979;&#35797;&#26469;&#34913;&#37327;&#20219;&#21153;&#24615;&#33021;&#65292;&#20294;&#25105;&#20204;&#32570;&#20047;&#35299;&#37322;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#36825;&#20123;&#20219;&#21153;&#23436;&#25104;&#30340;&#34892;&#20026;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#24615;&#30340;&#21162;&#21147;&#65292;&#23558;&#35821;&#35328;&#27169;&#22411;&#30340;&#34892;&#20026;&#20998;&#35299;&#20026;&#35299;&#37322;&#36328;&#20219;&#21153;&#24615;&#33021;&#30340;&#31867;&#21035;&#65292;&#20197;&#25351;&#23548;&#26426;&#26800;&#35299;&#37322;&#24182;&#24110;&#21161;&#26410;&#26469;&#20998;&#26512;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Coaxing out desired behavior from pretrained models, while avoiding undesirable ones, has redefined NLP and is reshaping how we interact with computers. What was once a scientific engineering discipline-in which building blocks are stacked one on top of the other-is arguably already a complex systems science, in which emergent behaviors are sought out to support previously unimagined use cases.  Despite the ever increasing number of benchmarks that measure task performance, we lack explanations of what behaviors language models exhibit that allow them to complete these tasks in the first place. We argue for a systematic effort to decompose language model behavior into categories that explain cross-task performance, to guide mechanistic explanations and help future-proof analytic research.
&lt;/p&gt;</description></item><item><title>&#25968;&#25454;&#31649;&#29702;&#21644;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#30740;&#31350;&#25551;&#36848;&#20102;&#20351;&#29992;&#23454;&#38469;&#22240;&#26524;&#20851;&#31995;&#26469;&#35299;&#37322;&#25968;&#25454;&#24211;&#26597;&#35810;&#31572;&#26696;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#32467;&#26524;&#30340;&#36131;&#20219;&#35780;&#20998;&#30340;&#26368;&#26032;&#24037;&#20316;&#65292;&#20197;&#21450;&#19982;&#25968;&#25454;&#24211;&#20462;&#22797;&#21644;Shap-score&#35745;&#31639;&#30456;&#20851;&#30340;&#20869;&#23481;&#12290;</title><link>http://arxiv.org/abs/2308.00184</link><description>&lt;p&gt;
&#25968;&#25454;&#31649;&#29702;&#21644;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24402;&#22240;&#20998;&#25968;
&lt;/p&gt;
&lt;p&gt;
Attribution-Scores in Data Management and Explainable Machine Learning. (arXiv:2308.00184v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00184
&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#31649;&#29702;&#21644;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#30740;&#31350;&#25551;&#36848;&#20102;&#20351;&#29992;&#23454;&#38469;&#22240;&#26524;&#20851;&#31995;&#26469;&#35299;&#37322;&#25968;&#25454;&#24211;&#26597;&#35810;&#31572;&#26696;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#32467;&#26524;&#30340;&#36131;&#20219;&#35780;&#20998;&#30340;&#26368;&#26032;&#24037;&#20316;&#65292;&#20197;&#21450;&#19982;&#25968;&#25454;&#24211;&#20462;&#22797;&#21644;Shap-score&#35745;&#31639;&#30456;&#20851;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25551;&#36848;&#20102;&#26368;&#36817;&#20851;&#20110;&#22312;&#25968;&#25454;&#24211;&#20013;&#20351;&#29992;&#23454;&#38469;&#22240;&#26524;&#20851;&#31995;&#26469;&#23450;&#20041;&#35299;&#37322;&#26597;&#35810;&#31572;&#26696;&#21644;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#27169;&#22411;&#32467;&#26524;&#30340;&#36131;&#20219;&#35780;&#20998;&#30340;&#30740;&#31350;&#12290;&#22312;&#25968;&#25454;&#24211;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#24182;&#21033;&#29992;&#25968;&#25454;&#24211;&#20462;&#22797;&#19982;&#26377;&#29992;&#30340;&#36830;&#25509;&#12290;&#20462;&#22797;&#36824;&#29992;&#20110;&#32473;&#20986;&#25968;&#25454;&#24211;&#30340;&#19968;&#33268;&#24615;&#30340;&#37327;&#21270;&#24230;&#37327;&#12290;&#23545;&#20110;&#20998;&#31867;&#27169;&#22411;&#65292;&#36131;&#20219;&#35780;&#20998;&#24471;&#21040;&#20102;&#36866;&#24403;&#30340;&#25193;&#23637;&#21644;&#35828;&#26126;&#12290;&#36824;&#20998;&#26512;&#21644;&#35752;&#35770;&#20102;Shap-score&#30340;&#39640;&#25928;&#35745;&#31639;&#12290;&#37325;&#28857;&#25918;&#22312;&#20316;&#32773;&#21644;&#21512;&#20316;&#32773;&#30340;&#24037;&#20316;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
We describe recent research on the use of actual causality in the definition of responsibility scores as explanations for query answers in databases, and for outcomes from classification models in machine learning. In the case of databases, useful connections with database repairs are illustrated and exploited. Repairs are also used to give a quantitative measure of the consistency of a database. For classification models, the responsibility score is properly extended and illustrated. The efficient computation of Shap-score is also analyzed and discussed. The emphasis is placed on work done by the author and collaborators.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#28023;&#24213;&#28369;&#32724;&#22120;&#22312;&#19981;&#21487;&#39044;&#27979;&#28023;&#27915;&#29615;&#22659;&#20013;&#27491;&#24120;&#25805;&#20316;&#30340;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#38469;&#28369;&#32724;&#22120;&#37096;&#32626;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#36827;&#34892;&#39564;&#35777;&#12290;&#31639;&#27861;&#33021;&#22815;&#23454;&#26102;&#25552;&#20379;&#24322;&#24120;&#35686;&#25253;&#65292;&#20351;&#39550;&#39542;&#21592;&#33021;&#22815;&#25511;&#21046;&#28369;&#32724;&#22120;&#24182;&#36991;&#20813;&#36827;&#19968;&#27493;&#25439;&#23475;&#12290;</title><link>http://arxiv.org/abs/2308.00180</link><description>&lt;p&gt;
&#28023;&#24213;&#28369;&#32724;&#22120;&#30340;&#36890;&#29992;&#24322;&#24120;&#26816;&#27979;&#39564;&#35777;&#26041;&#27861;&#8212;&#8212;&#22823;&#35268;&#27169;&#37096;&#32626;&#25968;&#25454;&#38598;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
General Anomaly Detection of Underwater Gliders Validated by Large-scale Deployment Dataset. (arXiv:2308.00180v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00180
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#28023;&#24213;&#28369;&#32724;&#22120;&#22312;&#19981;&#21487;&#39044;&#27979;&#28023;&#27915;&#29615;&#22659;&#20013;&#27491;&#24120;&#25805;&#20316;&#30340;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#38469;&#28369;&#32724;&#22120;&#37096;&#32626;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#36827;&#34892;&#39564;&#35777;&#12290;&#31639;&#27861;&#33021;&#22815;&#23454;&#26102;&#25552;&#20379;&#24322;&#24120;&#35686;&#25253;&#65292;&#20351;&#39550;&#39542;&#21592;&#33021;&#22815;&#25511;&#21046;&#28369;&#32724;&#22120;&#24182;&#36991;&#20813;&#36827;&#19968;&#27493;&#25439;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#19968;&#31181;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#35780;&#20272;&#22312;&#19981;&#21487;&#39044;&#27979;&#30340;&#28023;&#27915;&#29615;&#22659;&#20013;&#28023;&#24213;&#28369;&#32724;&#22120;&#30340;&#27491;&#24120;&#25805;&#20316;&#12290;&#19968;&#26086;&#26816;&#27979;&#21040;&#20219;&#20309;&#24322;&#24120;&#65292;&#21487;&#20197;&#21521;&#28369;&#32724;&#22120;&#39550;&#39542;&#21592;&#25552;&#20379;&#23454;&#26102;&#35686;&#25253;&#65292;&#20351;&#20854;&#33021;&#22815;&#25509;&#31649;&#28369;&#32724;&#22120;&#24182;&#38450;&#27490;&#36827;&#19968;&#27493;&#30340;&#25439;&#23475;&#12290;&#35813;&#26816;&#27979;&#31639;&#27861;&#24212;&#29992;&#20110;&#30001;Skidaway&#28023;&#27915;&#30740;&#31350;&#25152;&#65288;SkIO&#65289;&#21644;&#21335;&#20315;&#32599;&#37324;&#36798;&#22823;&#23398;&#65288;USF&#65289;&#39046;&#23548;&#30340;&#23454;&#38469;&#28369;&#32724;&#22120;&#37096;&#32626;&#20013;&#25910;&#38598;&#30340;&#22823;&#37327;&#25968;&#25454;&#38598;&#12290;&#23601;&#27867;&#21270;&#24615;&#32780;&#35328;&#65292;&#23454;&#39564;&#35780;&#20272;&#21253;&#25324;&#31163;&#32447;&#21644;&#22312;&#32447;&#26816;&#27979;&#27169;&#24335;&#12290;&#31163;&#32447;&#26816;&#27979;&#21033;&#29992;&#23436;&#25972;&#30340;&#22238;&#25910;&#21518;&#25968;&#25454;&#38598;&#65292;&#20855;&#26377;&#39640;&#20998;&#36776;&#29575;&#30340;&#20449;&#24687;&#65292;&#23545;&#24322;&#24120;&#36827;&#34892;&#35814;&#32454;&#20998;&#26512;&#24182;&#19982;&#39550;&#39542;&#21592;&#26085;&#24535;&#36827;&#34892;&#27604;&#36739;&#12290;&#22312;&#32447;&#26816;&#27979;&#19987;&#27880;&#20110;&#20174;&#28369;&#32724;&#22120;&#20256;&#36755;&#30340;&#23454;&#26102;&#25968;&#25454;&#23376;&#38598;&#12290;&#34429;&#28982;&#23454;&#26102;&#25968;&#25454;&#21487;&#33021;&#19981;&#21253;&#21547;&#19982;&#22238;&#25910;&#21518;&#25968;&#25454;&#19968;&#26679;&#20016;&#23500;&#30340;&#20449;&#24687;&#65292;&#20294;&#22312;&#32447;&#26816;&#27979;&#26159;&#23454;&#26102;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper employs an anomaly detection algorithm to assess the normal operation of underwater gliders in unpredictable ocean environments. Real-time alerts can be provided to glider pilots upon detecting any anomalies, enabling them to assume control of the glider and prevent further harm. The detection algorithm is applied to abundant data sets collected in real glider deployments led by the Skidaway Institute of Oceanography (SkIO) and the University of South Florida (USF). Regarding generality, the experimental evaluation is composed of both offline and online detection modes. The offline detection utilizes full post-recovery data sets, which carries high-resolution information, to present detailed analysis of the anomaly and compare it with pilot logs. The online detection focuses on the real-time subsets of data transmitted from the glider at the surfacing events. While the real-time data may not contain as much rich information as the post-recovery data, the online detection is 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22312;&#26631;&#31614;&#31232;&#32570;&#30340;Learning-To-Rank&#38382;&#39064;&#20013;&#65292;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;&#28145;&#24230;&#27169;&#22411;&#26159;&#21542;&#33021;&#32988;&#36807;GBDTs&#21644;&#20854;&#20182;&#38750;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;SimCLR-Rank&#26041;&#27861;&#36827;&#34892;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#25105;&#20204;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#22823;&#37327;&#26080;&#26631;&#31614;&#25968;&#25454;&#21644;&#26377;&#38480;&#26631;&#31614;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#26174;&#33879;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2308.00177</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#28145;&#24230;&#27169;&#22411;&#22312;&#26631;&#31614;&#31232;&#32570;&#30340;Learning-To-Rank&#20013;&#32988;&#36807;GBDTs
&lt;/p&gt;
&lt;p&gt;
Pretrained deep models outperform GBDTs in Learning-To-Rank under label scarcity. (arXiv:2308.00177v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00177
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22312;&#26631;&#31614;&#31232;&#32570;&#30340;Learning-To-Rank&#38382;&#39064;&#20013;&#65292;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;&#28145;&#24230;&#27169;&#22411;&#26159;&#21542;&#33021;&#32988;&#36807;GBDTs&#21644;&#20854;&#20182;&#38750;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;SimCLR-Rank&#26041;&#27861;&#36827;&#34892;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#25105;&#20204;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#22823;&#37327;&#26080;&#26631;&#31614;&#25968;&#25454;&#21644;&#26377;&#38480;&#26631;&#31614;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#26174;&#33879;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#25991;&#26412;&#21644;&#22270;&#20687;&#39046;&#22495;&#26159;&#26368;&#20808;&#36827;&#30340;&#65292;&#20294;&#23427;&#20204;&#22312;&#34920;&#26684;&#24418;&#24335;&#30340;Learning-To-Rank&#38382;&#39064;&#19978;&#23578;&#26410;&#19968;&#33268;&#22320;&#32988;&#36807;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;(GBDTs)&#12290;&#36817;&#26399;&#22312;&#25991;&#26412;&#21644;&#22270;&#20687;&#20219;&#21153;&#19978;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21462;&#24471;&#30340;&#24615;&#33021;&#25552;&#21319;&#20027;&#35201;&#20381;&#36182;&#20110;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#20102;&#27604;&#26377;&#26631;&#31614;&#25968;&#25454;&#22810;&#20960;&#20010;&#25968;&#37327;&#32423;&#30340;&#26080;&#26631;&#31614;&#25968;&#25454;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#36824;&#26410;&#24212;&#29992;&#20110;Learning-To-Rank&#38382;&#39064;&#65292;&#32780;&#35813;&#38382;&#39064;&#36890;&#24120;&#20135;&#29983;&#22823;&#37327;&#26080;&#26631;&#31614;&#25968;&#25454;&#12290;&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#26159;&#21542;&#33021;&#25552;&#39640;LTR&#24615;&#33021;&#65292;&#19982;GBDTs&#21644;&#20854;&#20182;&#38750;&#39044;&#35757;&#32451;&#27169;&#22411;&#30456;&#27604;&#12290;&#36890;&#36807;&#20351;&#29992;&#31616;&#21333;&#30340;&#35774;&#35745;&#36873;&#25321;(&#21253;&#25324;SimCLR-Rank&#65292;&#36825;&#26159;&#25105;&#20204;&#38024;&#23545;&#25490;&#21517;&#38382;&#39064;&#20462;&#25913;&#30340;SimCLR&#26041;&#27861;)&#65292;&#25105;&#20204;&#20135;&#29983;&#20102;&#39044;&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#26377;&#22823;&#37327;&#26080;&#26631;&#31614;&#25968;&#25454;&#19988;&#26377;&#38480;&#26631;&#31614;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#26174;&#33879;&#20248;&#20110;GBDTs(&#21644;&#20854;&#20182;&#38750;&#39044;&#35757;&#32451;&#27169;&#22411;)&#12290;
&lt;/p&gt;
&lt;p&gt;
While deep learning (DL) models are state-of-the-art in text and image domains, they have not yet consistently outperformed Gradient Boosted Decision Trees (GBDTs) on tabular Learning-To-Rank (LTR) problems. Most of the recent performance gains attained by DL models in text and image tasks have used unsupervised pretraining, which exploits orders of magnitude more unlabeled data than labeled data. To the best of our knowledge, unsupervised pretraining has not been applied to the LTR problem, which often produces vast amounts of unlabeled data. In this work, we study whether unsupervised pretraining can improve LTR performance over GBDTs and other non-pretrained models. Using simple design choices--including SimCLR-Rank, our ranking-specific modification of SimCLR (an unsupervised pretraining method for images)--we produce pretrained deep learning models that soundly outperform GBDTs (and other non-pretrained models) in the case where labeled data is vastly outnumbered by unlabeled data
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;FlowArtist&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#21516;&#26102;&#23884;&#20837;&#28857;&#21644;&#23398;&#20064;&#22260;&#32469;&#28857;&#30340;&#21521;&#37327;&#22330;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#20998;&#31163;&#21644;&#21487;&#35270;&#21270;&#22522;&#20110;&#36895;&#24230;&#30340;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2308.00176</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#39640;&#32500;&#32454;&#32990;&#25968;&#25454;&#30340;&#27969;&#21160;&#33402;&#26415;&#23478;
&lt;/p&gt;
&lt;p&gt;
A Flow Artist for High-Dimensional Cellular Data. (arXiv:2308.00176v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;FlowArtist&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#21516;&#26102;&#23884;&#20837;&#28857;&#21644;&#23398;&#20064;&#22260;&#32469;&#28857;&#30340;&#21521;&#37327;&#22330;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#20998;&#31163;&#21644;&#21487;&#35270;&#21270;&#22522;&#20110;&#36895;&#24230;&#30340;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#23558;&#37319;&#26679;&#33258;&#30456;&#20851;&#27969;&#21160;&#25110;&#36895;&#24230;&#30340;&#24213;&#23618;&#27969;&#24418;&#30340;&#28857;&#20113;&#25968;&#25454;&#23884;&#20837;&#20854;&#20013;&#30340;&#38382;&#39064;&#12290;&#36825;&#26679;&#30340;&#25968;&#25454;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#20986;&#29616;&#65292;&#21253;&#25324;&#39640;&#36890;&#37327;&#29983;&#29289;&#23398;&#65288;&#22914;&#21333;&#32454;&#32990;&#36716;&#24405;&#32452;&#23398;&#65289;&#20013;&#27979;&#37327;&#21160;&#24577;&#23454;&#20307;&#30340;&#38745;&#24577;&#24555;&#29031;&#12290;&#29616;&#26377;&#30340;&#23884;&#20837;&#25216;&#26415;&#35201;&#20040;&#19981;&#21033;&#29992;&#36895;&#24230;&#20449;&#24687;&#65292;&#35201;&#20040;&#29420;&#31435;&#22320;&#23884;&#20837;&#22352;&#26631;&#21644;&#36895;&#24230;&#65292;&#21363;&#22312;&#29616;&#26377;&#28857;&#23884;&#20837;&#30340;&#22522;&#30784;&#19978;&#26045;&#21152;&#36895;&#24230;&#65292;&#25110;&#23558;&#28857;&#23884;&#20837;&#21040;&#19968;&#20010;&#39044;&#20808;&#25351;&#23450;&#30340;&#21521;&#37327;&#22330;&#20013;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FlowArtist&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#22312;&#23884;&#20837;&#28857;&#30340;&#21516;&#26102;&#23398;&#20064;&#20102;&#19968;&#20010;&#22260;&#32469;&#28857;&#30340;&#21521;&#37327;&#22330;&#12290;&#36825;&#31181;&#32452;&#21512;&#20351;&#24471;FlowArtist&#33021;&#22815;&#26356;&#22909;&#22320;&#20998;&#31163;&#21644;&#21487;&#35270;&#21270;&#22522;&#20110;&#36895;&#24230;&#30340;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#65292;&#22312;&#29609;&#20855;&#25968;&#25454;&#38598;&#21644;&#21333;&#32454;&#32990;RNA&#36895;&#24230;&#25968;&#25454;&#19978;&#65292;&#35828;&#26126;&#20102;&#21033;&#29992;&#22352;&#26631;&#21644;&#36895;&#24230;&#20449;&#24687;&#21516;&#27493;&#36827;&#34892;&#23884;&#20837;&#21644;&#21487;&#35270;&#21270;&#39640;&#32500;&#25968;&#25454;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of embedding point cloud data sampled from an underlying manifold with an associated flow or velocity. Such data arises in many contexts where static snapshots of dynamic entities are measured, including in high-throughput biology such as single-cell transcriptomics. Existing embedding techniques either do not utilize velocity information or embed the coordinates and velocities independently, i.e., they either impose velocities on top of an existing point embedding or embed points within a prescribed vector field. Here we present FlowArtist, a neural network that embeds points while jointly learning a vector field around the points. The combination allows FlowArtist to better separate and visualize velocity-informed structures. Our results, on toy datasets and single-cell RNA velocity data, illustrate the value of utilizing coordinate and velocity information in tandem for embedding and visualizing high-dimensional data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MDH-FL&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#21516;&#26102;&#21033;&#29992;&#25968;&#25454;&#21644;&#27169;&#22411;&#24322;&#36136;&#24615;&#65292;&#35299;&#20915;&#20102;&#25968;&#25454;&#24322;&#36136;&#24615;&#21644;&#27169;&#22411;&#24322;&#36136;&#24615;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#20840;&#23616;&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.00155</link><description>&lt;p&gt;
&#21307;&#23398;&#24433;&#20687;&#20013;&#25968;&#25454;&#21644;&#27169;&#22411;&#24322;&#36136;&#24615;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Learning for Data and Model Heterogeneity in Medical Imaging. (arXiv:2308.00155v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00155
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MDH-FL&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#21516;&#26102;&#21033;&#29992;&#25968;&#25454;&#21644;&#27169;&#22411;&#24322;&#36136;&#24615;&#65292;&#35299;&#20915;&#20102;&#25968;&#25454;&#24322;&#36136;&#24615;&#21644;&#27169;&#22411;&#24322;&#36136;&#24615;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#20840;&#23616;&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#19981;&#21516;&#23458;&#25143;&#31471;&#21442;&#19982;&#21327;&#20316;&#23398;&#20064;&#20294;&#19981;&#20849;&#20139;&#24444;&#27492;&#25968;&#25454;&#21644;&#20013;&#22830;&#26381;&#21153;&#22120;&#30340;&#36827;&#21270;&#22411;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#22312;&#21307;&#38498;&#21644;&#24037;&#19994;&#31561;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#32852;&#37030;&#23398;&#20064;&#35299;&#20915;&#20102;&#25968;&#25454;&#24322;&#36136;&#24615;&#21644;&#27169;&#22411;&#24322;&#36136;&#24615;&#20316;&#20026;&#21327;&#20316;&#35757;&#32451;&#19981;&#21487;&#36991;&#20813;&#30340;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#26410;&#33021;&#26377;&#25928;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#24322;&#36136;&#24615;&#38382;&#39064;&#12290;&#26412;&#25991;&#21516;&#26102;&#21033;&#29992;&#25968;&#25454;&#21644;&#27169;&#22411;&#24322;&#36136;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MDH-FL&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#32852;&#37030;&#23398;&#20064;&#20013;&#20840;&#23616;&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#21644;&#23545;&#31216;&#25439;&#22833;&#26469;&#38477;&#20302;&#24322;&#36136;&#24615;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is an evolving machine learning method in which multiple clients participate in collaborative learning without sharing their data with each other and the central server. In real-world applications such as hospitals and industries, FL counters the challenges of data heterogeneity and model heterogeneity as an inevitable part of the collaborative training. More specifically, different organizations, such as hospitals, have their own private data and customized models for local training. To the best of our knowledge, the existing methods do not effectively address both problems of model heterogeneity and data heterogeneity in FL. In this paper, we exploit the data and model heterogeneity simultaneously, and propose a method, MDH-FL (Exploiting Model and Data Heterogeneity in FL) to solve such problems to enhance the efficiency of the global model in FL. We use knowledge distillation and a symmetric loss to minimize the heterogeneity and its impact on the model perf
&lt;/p&gt;</description></item><item><title>DiffusAL&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20027;&#21160;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#19977;&#20010;&#29420;&#31435;&#30340;&#35780;&#20998;&#20989;&#25968;&#65292;&#23454;&#29616;&#20102;&#22312;&#19981;&#21516;&#22330;&#26223;&#20013;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#39640;&#20102;&#33410;&#28857;&#20998;&#31867;&#30340;&#26631;&#31614;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.00146</link><description>&lt;p&gt;
DiffusAL: &#23558;&#20027;&#21160;&#23398;&#20064;&#19982;&#22270;&#25193;&#25955;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#33410;&#28857;&#20998;&#31867;&#30340;&#26631;&#31614;&#39640;&#25928;&#21270;
&lt;/p&gt;
&lt;p&gt;
DiffusAL: Coupling Active Learning with Graph Diffusion for Label-Efficient Node Classification. (arXiv:2308.00146v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00146
&lt;/p&gt;
&lt;p&gt;
DiffusAL&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20027;&#21160;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#19977;&#20010;&#29420;&#31435;&#30340;&#35780;&#20998;&#20989;&#25968;&#65292;&#23454;&#29616;&#20102;&#22312;&#19981;&#21516;&#22330;&#26223;&#20013;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#39640;&#20102;&#33410;&#28857;&#20998;&#31867;&#30340;&#26631;&#31614;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33410;&#28857;&#20998;&#31867;&#26159;&#23646;&#24615;&#22270;&#19978;&#30340;&#26680;&#24515;&#20219;&#21153;&#20043;&#19968;&#65292;&#20294;&#25104;&#21151;&#30340;&#22270;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#38656;&#35201;&#36275;&#22815;&#26631;&#35760;&#30340;&#25968;&#25454;&#12290;&#20026;&#20102;&#38477;&#20302;&#27880;&#37322;&#25104;&#26412;&#65292;&#20027;&#21160;&#22270;&#23398;&#20064;&#19987;&#27880;&#20110;&#36873;&#25321;&#26368;&#20855;&#36136;&#37327;&#30340;&#33410;&#28857;&#23376;&#38598;&#65292;&#20197;&#26368;&#22823;&#21270;&#26631;&#31614;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#20915;&#23450;&#23545;&#19968;&#20010;&#26410;&#26631;&#35760;&#30340;&#22270;&#20351;&#29992;&#21738;&#31181;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#22686;&#21152;&#26631;&#31614;&#25928;&#29575;&#65292;&#22987;&#32456;&#26159;&#19968;&#20010;&#25345;&#20037;&#30340;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#35201;&#20040;&#24573;&#35270;&#20102;&#23398;&#21040;&#30340;&#27169;&#22411;&#21644;&#37319;&#26679;&#26041;&#27861;&#30340;&#23545;&#40784;&#65292;&#35201;&#20040;&#21482;&#20851;&#27880;&#26377;&#38480;&#30340;&#36873;&#25321;&#26041;&#38754;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#26377;&#26102;&#27604;&#38543;&#26426;&#37319;&#26679;&#26356;&#24046;&#65292;&#25110;&#32773;&#21482;&#33021;&#19982;&#38543;&#26426;&#37319;&#26679;&#19968;&#26679;&#22909;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20027;&#21160;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;DiffusAL&#65292;&#22312;&#19981;&#21516;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#40065;&#26834;&#24615;&#12290;&#20026;&#20102;&#22312;&#19981;&#21516;&#22270;&#32467;&#26500;&#20043;&#38388;&#23454;&#29616;&#26356;&#22909;&#30340;&#21487;&#36801;&#31227;&#24615;&#65292;&#25105;&#20204;&#20197;&#26080;&#21442;&#25968;&#30340;&#26041;&#24335;&#32467;&#21512;&#20102;&#19977;&#20010;&#29420;&#31435;&#30340;&#35780;&#20998;&#20989;&#25968;&#65292;&#20197;&#35782;&#21035;&#26368;&#20855;&#20449;&#24687;&#30340;&#33410;&#28857;&#26679;&#26412;&#20197;&#36827;&#34892;&#26631;&#35760;&#65306;i) &#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#65292;ii) &#22810;&#26679;&#24615;&#32452;&#20214;&#65292;&#20197;&#21450;iii)
&lt;/p&gt;
&lt;p&gt;
Node classification is one of the core tasks on attributed graphs, but successful graph learning solutions require sufficiently labeled data. To keep annotation costs low, active graph learning focuses on selecting the most qualitative subset of nodes that maximizes label efficiency. However, deciding which heuristic is best suited for an unlabeled graph to increase label efficiency is a persistent challenge. Existing solutions either neglect aligning the learned model and the sampling method or focus only on limited selection aspects. They are thus sometimes worse or only equally good as random sampling. In this work, we introduce a novel active graph learning approach called DiffusAL, showing significant robustness in diverse settings. Toward better transferability between different graph structures, we combine three independent scoring functions to identify the most informative node samples for labeling in a parameter-free way: i) Model Uncertainty, ii) Diversity Component, and iii)
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#22312;&#21453;&#24212;&#24335;&#31995;&#32479;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;DNN&#39564;&#35777;&#30340;&#24418;&#24335;&#21270;XAI&#25216;&#26415;&#65292;&#21487;&#20197;&#35299;&#37322;DNN&#30340;&#34892;&#20026;&#65292;&#24182;&#19988;&#36890;&#36807;&#21033;&#29992;&#31995;&#32479;&#30340;&#36716;&#25442;&#32422;&#26463;&#26469;&#35745;&#31639;&#31616;&#27905;&#30340;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2308.00143</link><description>&lt;p&gt;
&#22312;&#21453;&#24212;&#24335;&#31995;&#32479;&#20869;&#23545;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#24418;&#24335;&#21270;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Formally Explaining Neural Networks within Reactive Systems. (arXiv:2308.00143v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00143
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#22312;&#21453;&#24212;&#24335;&#31995;&#32479;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;DNN&#39564;&#35777;&#30340;&#24418;&#24335;&#21270;XAI&#25216;&#26415;&#65292;&#21487;&#20197;&#35299;&#37322;DNN&#30340;&#34892;&#20026;&#65292;&#24182;&#19988;&#36890;&#36807;&#21033;&#29992;&#31995;&#32479;&#30340;&#36716;&#25442;&#32422;&#26463;&#26469;&#35745;&#31639;&#31616;&#27905;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#29992;&#20316;&#21453;&#24212;&#24335;&#31995;&#32479;&#20013;&#30340;&#25511;&#21046;&#22120;&#12290;&#28982;&#32780;&#65292;DNNs&#20855;&#26377;&#39640;&#24230;&#30340;&#19981;&#36879;&#26126;&#24615;&#65292;&#36825;&#20351;&#24471;&#35299;&#37322;&#21644;&#35777;&#26126;&#23427;&#20204;&#30340;&#34892;&#20026;&#21464;&#24471;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20986;&#29616;&#20102;&#23545;&#21487;&#35299;&#37322;AI(XAI)&#25216;&#26415;&#30340;&#20852;&#36259;&#28608;&#22686;&#65292;&#36825;&#20123;&#25216;&#26415;&#33021;&#22815;&#25214;&#20986;&#23548;&#33268;DNN&#34892;&#20026;&#30340;&#36755;&#20837;&#29305;&#24449;&#12290;&#29616;&#26377;&#30340;XAI&#25216;&#26415;&#36890;&#24120;&#23384;&#22312;&#20004;&#20010;&#38480;&#21046;&#65306;(i)&#23427;&#20204;&#26159;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#24182;&#19981;&#33021;&#25552;&#20379;&#35299;&#37322;&#27491;&#30830;&#24615;&#30340;&#27491;&#24335;&#20445;&#35777;&#65307;(ii)&#23427;&#20204;&#36890;&#24120;&#36866;&#29992;&#20110;&#8220;&#19968;&#27425;&#24615;&#8221;&#31995;&#32479;(&#21363;DNN&#29420;&#31435;&#20110;&#36807;&#21435;&#30340;&#35843;&#29992;)&#65292;&#32780;&#19981;&#26159;&#21453;&#24212;&#24335;&#31995;&#32479;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24320;&#22987;&#24357;&#21512;&#36825;&#20010;&#24046;&#36317;&#65292;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;DNN&#39564;&#35777;&#30340;&#24418;&#24335;&#21270;XAI&#25216;&#26415;&#65292;&#29992;&#20110;&#25512;&#29702;&#22810;&#27493;&#39588;&#30340;&#21453;&#24212;&#24335;&#31995;&#32479;&#12290;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#21033;&#29992;&#31995;&#32479;&#30340;&#36716;&#25442;&#32422;&#26463;&#26469;&#35745;&#31639;&#31616;&#27905;&#30340;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#20197;&#20415;&#20943;&#23569;&#24213;&#23618;&#39564;&#35777;&#22120;&#25152;&#25506;&#32034;&#30340;&#25628;&#32034;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) are increasingly being used as controllers in reactive systems. However, DNNs are highly opaque, which renders it difficult to explain and justify their actions. To mitigate this issue, there has been a surge of interest in explainable AI (XAI) techniques, capable of pinpointing the input features that caused the DNN to act as it did.  Existing XAI techniques typically face two limitations: (i) they are heuristic, and do not provide formal guarantees that the explanations are correct; and (ii) they often apply to ``one-shot'' systems (where the DNN is invoked independently of past invocations), as opposed to reactive systems.  Here, we begin bridging this gap, and propose a formal DNN-verification-based XAI technique for reasoning about multi-step, reactive systems. We suggest methods for efficiently calculating succinct explanations, by exploiting the system's transition constraints in order to curtail the search space explored by the underlying verifier. W
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23558;&#22270;&#19978;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#37325;&#26032;&#34920;&#36848;&#20026;&#38750;&#20984;&#25512;&#24191;&#65292;&#36890;&#36807;&#27969;&#24418;&#23545;&#40784;&#38382;&#39064;&#30340;&#35299;&#26469;&#25214;&#21040;&#22909;&#30340;&#20998;&#31867;&#22120;&#65292;&#24182;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#20013;&#24515;&#24230;&#24230;&#37327;&#26469;&#36873;&#25321;&#26377;&#20449;&#24687;&#37327;&#30340;&#26679;&#26412;&#12290;&#23454;&#39564;&#35777;&#26126;&#26412;&#26041;&#27861;&#22312;&#20302;&#26631;&#31614;&#29575;&#19979;&#20855;&#26377;&#26356;&#20302;&#30340;&#20998;&#31867;&#38169;&#35823;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.00142</link><description>&lt;p&gt;
&#22312;Stiefel&#27969;&#24418;&#19978;&#30340;&#21322;&#30417;&#30563;Laplacian&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Semi-Supervised Laplacian Learning on Stiefel Manifolds. (arXiv:2308.00142v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00142
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23558;&#22270;&#19978;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#37325;&#26032;&#34920;&#36848;&#20026;&#38750;&#20984;&#25512;&#24191;&#65292;&#36890;&#36807;&#27969;&#24418;&#23545;&#40784;&#38382;&#39064;&#30340;&#35299;&#26469;&#25214;&#21040;&#22909;&#30340;&#20998;&#31867;&#22120;&#65292;&#24182;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#20013;&#24515;&#24230;&#24230;&#37327;&#26469;&#36873;&#25321;&#26377;&#20449;&#24687;&#37327;&#30340;&#26679;&#26412;&#12290;&#23454;&#39564;&#35777;&#26126;&#26412;&#26041;&#27861;&#22312;&#20302;&#26631;&#31614;&#29575;&#19979;&#20855;&#26377;&#26356;&#20302;&#30340;&#20998;&#31867;&#38169;&#35823;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#20302;&#26631;&#31614;&#29575;&#19979;&#32463;&#20856;Laplace&#23398;&#20064;&#31639;&#27861;&#36864;&#21270;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#22522;&#20110;&#22270;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#37325;&#26032;&#34920;&#36848;&#20026;\emph{Trust-Region Subproblem} (TRS) &#30340;&#38750;&#20984;&#25512;&#24191;&#12290;&#36825;&#20010;&#25913;&#36827;&#26159;&#21463;&#21040;Laplacian&#29305;&#24449;&#21521;&#37327;&#22312;&#26080;&#38480;&#26410;&#26631;&#35760;&#25968;&#25454;&#26497;&#38480;&#19979;&#30340;&#21487;&#35299;&#24615;&#30340;&#21551;&#21457;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#19968;&#38454;&#26465;&#20214;&#26263;&#31034;&#20102;&#27969;&#24418;&#23545;&#40784;&#38382;&#39064;&#30340;&#35299;&#65292;&#32780;&#19988;&#32463;&#20856;&#30340;\emph{Orthogonal Procrustes} &#38382;&#39064;&#30340;&#35299;&#21487;&#20197;&#34987;&#29992;&#20110;&#39640;&#25928;&#22320;&#25214;&#21040;&#36866;&#20110;&#36827;&#19968;&#27493;&#32454;&#21270;&#30340;&#22909;&#30340;&#20998;&#31867;&#22120;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#22312;&#20302;&#26631;&#31614;&#29575;&#19979;&#36873;&#25321;&#26377;&#30417;&#30563;&#26679;&#26412;&#30340;&#20851;&#38190;&#24615;&#12290;&#25105;&#20204;&#29992;&#22270;Laplacian&#30340;&#26576;&#20010;&#23376;&#30697;&#38453;&#30340;&#20027;&#29305;&#24449;&#21521;&#37327;&#25512;&#23548;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20013;&#24515;&#24230;&#24230;&#37327;&#65292;&#26469;&#34920;&#24449;&#20449;&#24687;&#26679;&#26412;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#19982;&#26368;&#36817;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#65292;&#33021;&#22815;&#23454;&#29616;&#26356;&#20302;&#30340;&#20998;&#31867;&#38169;&#35823;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by the need to address the degeneracy of canonical Laplace learning algorithms in low label rates, we propose to reformulate graph-based semi-supervised learning as a nonconvex generalization of a \emph{Trust-Region Subproblem} (TRS). This reformulation is motivated by the well-posedness of Laplacian eigenvectors in the limit of infinite unlabeled data. To solve this problem, we first show that a first-order condition implies the solution of a manifold alignment problem and that solutions to the classical \emph{Orthogonal Procrustes} problem can be used to efficiently find good classifiers that are amenable to further refinement. Next, we address the criticality of selecting supervised samples at low-label rates. We characterize informative samples with a novel measure of centrality derived from the principal eigenvectors of a certain submatrix of the graph Laplacian. We demonstrate that our framework achieves lower classification error compared to recent state-of-the-art and
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#22871;&#26032;&#30340;&#20844;&#24179;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#35299;&#20915;&#34920;&#26684;&#20998;&#31867;&#20013;&#23454;&#39564;&#35780;&#20272;&#25968;&#25454;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#20026;&#26410;&#26469;&#20844;&#24179;&#24863;&#30693;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#25552;&#20379;&#26356;&#20005;&#35880;&#30340;&#23454;&#39564;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2308.00133</link><description>&lt;p&gt;
&#19968;&#22871;&#29992;&#20110;&#34920;&#26684;&#20998;&#31867;&#20844;&#24179;&#24615;&#30340;&#20844;&#24179;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
A Suite of Fairness Datasets for Tabular Classification. (arXiv:2308.00133v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00133
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#22871;&#26032;&#30340;&#20844;&#24179;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#35299;&#20915;&#34920;&#26684;&#20998;&#31867;&#20013;&#23454;&#39564;&#35780;&#20272;&#25968;&#25454;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#20026;&#26410;&#26469;&#20844;&#24179;&#24863;&#30693;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#25552;&#20379;&#26356;&#20005;&#35880;&#30340;&#23454;&#39564;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#26377;&#35768;&#22810;&#20851;&#20110;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#22312;&#34920;&#26684;&#25968;&#25454;&#20013;&#20844;&#24179;&#24615;&#30340;&#31639;&#27861;&#30340;&#35770;&#25991;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#22823;&#22810;&#25968;&#35770;&#25991;&#21482;&#20351;&#29992;&#20102;&#38750;&#24120;&#23569;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#35780;&#20272;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#22871;&#20989;&#25968;&#65292;&#29992;&#20110;&#25552;&#21462;20&#20010;&#20844;&#27491;&#25968;&#25454;&#38598;&#24182;&#25552;&#20379;&#30456;&#20851;&#30340;&#20844;&#24179;&#20803;&#25968;&#25454;&#12290;&#24076;&#26395;&#36825;&#20123;&#25968;&#25454;&#38598;&#33021;&#22815;&#20419;&#36827;&#26410;&#26469;&#20844;&#24179;&#24863;&#30693;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20013;&#26356;&#20005;&#35880;&#30340;&#23454;&#39564;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
There have been many papers with algorithms for improving fairness of machine-learning classifiers for tabular data. Unfortunately, most use only very few datasets for their experimental evaluation. We introduce a suite of functions for fetching 20 fairness datasets and providing associated fairness metadata. Hopefully, these will lead to more rigorous experimental evaluations in future fairness-aware machine learning research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558; Transformer &#25972;&#21512;&#21040; U-Net &#20013;&#30340;&#26032;&#22411;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#33041;&#32959;&#30244;&#20998;&#21106;&#12290;&#27169;&#22411;&#32467;&#21512;&#20102;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#20687;&#32032;&#32423;&#26631;&#31614;&#65292;&#36890;&#36807;&#27531;&#24046;&#36830;&#25509;&#21644;&#38598;&#25104;&#26041;&#27861;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#20998;&#21106;&#25928;&#26524;&#65292;&#22312; BraTS 2021 &#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.00128</link><description>&lt;p&gt;
&#20351;&#29992;&#27531;&#24046;&#21464;&#21387;&#22120;&#36827;&#34892;&#33041;&#32959;&#30244;&#20998;&#21106;&#30340;&#38598;&#25104;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Ensemble Learning with Residual Transformer for Brain Tumor Segmentation. (arXiv:2308.00128v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00128
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558; Transformer &#25972;&#21512;&#21040; U-Net &#20013;&#30340;&#26032;&#22411;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#33041;&#32959;&#30244;&#20998;&#21106;&#12290;&#27169;&#22411;&#32467;&#21512;&#20102;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#20687;&#32032;&#32423;&#26631;&#31614;&#65292;&#36890;&#36807;&#27531;&#24046;&#36830;&#25509;&#21644;&#38598;&#25104;&#26041;&#27861;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#20998;&#21106;&#25928;&#26524;&#65292;&#22312; BraTS 2021 &#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#32959;&#30244;&#20998;&#21106;&#26159;&#19968;&#20010;&#27963;&#36291;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#30001;&#20110;&#39640;&#24230;&#22797;&#26434;&#24418;&#29366;&#21644;&#32441;&#29702;&#30340;&#32959;&#30244;&#30340;&#21010;&#20998;&#22256;&#38590;&#20197;&#21450;&#24120;&#29992;&#30340; U-Net &#26550;&#26500;&#30340;&#22833;&#36133;&#12290;&#26368;&#36817;&#65292;&#22312;&#20027;&#27969;&#30740;&#31350;&#20013;&#65292;&#19981;&#21516;&#31070;&#32463;&#26550;&#26500;&#30340;&#32452;&#21512;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30740;&#31350;&#26041;&#21521;&#65292;&#29305;&#21035;&#26159; U-Net &#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#32467;&#21512;&#30340; Transformer&#12290;&#19982;&#20043;&#21069;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558; Transformer &#25972;&#21512;&#21040;&#33258;&#36866;&#24212; U-Net &#20013;&#30340;&#26032;&#22411;&#32593;&#32476;&#26550;&#26500;&#65292;&#20197;&#21512;&#29702;&#30340;&#35745;&#31639;&#25104;&#26412;&#25552;&#21462;&#20986; 3D &#20307;&#31215;&#20449;&#24687;&#12290;&#20026;&#20102;&#38450;&#27490;&#20449;&#24687;&#27969;&#22833;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#28155;&#21152;&#20102;&#27531;&#24046;&#36830;&#25509;&#24182;&#25506;&#32034;&#38598;&#25104;&#26041;&#27861;&#65292;&#22240;&#20026;&#35780;&#20272;&#30340;&#27169;&#22411;&#23545;&#19981;&#21516;&#26696;&#20363;&#21644;&#23376;&#21306;&#22495;&#26377;&#36793;&#32536;&#12290;&#22312; BraTS 2021 &#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#36798;&#21040;&#20102;87.6%&#30340;&#24179;&#22343; Dice &#20998;&#25968;&#65292;&#24182;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#32467;&#21512;&#22810;&#20010;&#26550;&#26500;&#36827;&#34892;&#20248;&#21270;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Brain tumor segmentation is an active research area due to the difficulty in delineating highly complex shaped and textured tumors as well as the failure of the commonly used U-Net architectures. The combination of different neural architectures is among the mainstream research recently, particularly the combination of U-Net with Transformers because of their innate attention mechanism and pixel-wise labeling. Different from previous efforts, this paper proposes a novel network architecture that integrates Transformers into a self-adaptive U-Net to draw out 3D volumetric contexts with reasonable computational costs. We further add a residual connection to prevent degradation in information flow and explore ensemble methods, as the evaluated models have edges on different cases and sub-regions. On the BraTS 2021 dataset (3D), our model achieves 87.6% mean Dice score and outperforms the state-of-the-art methods, demonstrating the potential for combining multiple architectures to optimize
&lt;/p&gt;</description></item><item><title>DiviML&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#22359;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#24322;&#26500;&#24179;&#21488;&#19978;&#23558;&#31070;&#32463;&#32593;&#32476;&#26144;&#23556;&#65292;&#36890;&#36807;&#33258;&#21160;&#20998;&#21306;&#21644;&#35774;&#22791;&#26144;&#23556;&#65292;&#23454;&#29616;&#20102;&#32534;&#35793;&#22120;&#32423;&#21035;&#30340;&#20998;&#24067;&#24335;&#31070;&#32463;&#32593;&#32476;&#32534;&#35793;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#36136;&#37327;&#35780;&#20272;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.00127</link><description>&lt;p&gt;
DiviML: &#19968;&#31181;&#29992;&#20110;&#22312;&#24322;&#26500;&#24179;&#21488;&#19978;&#26144;&#23556;&#31070;&#32463;&#32593;&#32476;&#30340;&#22522;&#20110;&#27169;&#22359;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DiviML: A Module-based Heuristic for Mapping Neural Networks onto Heterogeneous Platforms. (arXiv:2308.00127v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00127
&lt;/p&gt;
&lt;p&gt;
DiviML&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#22359;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#24322;&#26500;&#24179;&#21488;&#19978;&#23558;&#31070;&#32463;&#32593;&#32476;&#26144;&#23556;&#65292;&#36890;&#36807;&#33258;&#21160;&#20998;&#21306;&#21644;&#35774;&#22791;&#26144;&#23556;&#65292;&#23454;&#29616;&#20102;&#32534;&#35793;&#22120;&#32423;&#21035;&#30340;&#20998;&#24067;&#24335;&#31070;&#32463;&#32593;&#32476;&#32534;&#35793;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#36136;&#37327;&#35780;&#20272;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#20013;&#24515;&#36234;&#26469;&#36234;&#22810;&#22320;&#37319;&#29992;&#24322;&#26500;&#26550;&#26500;&#65292;&#24182;&#24320;&#22987;&#21253;&#25324;&#29992;&#20110;&#32593;&#32476;&#12289;&#35270;&#39057;&#22788;&#29702;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#19987;&#29992;&#30828;&#20214;&#12290;&#20026;&#20102;&#21033;&#29992;&#29616;&#20195;&#25968;&#25454;&#20013;&#24515;&#30340;&#24322;&#26500;&#35745;&#31639;&#33021;&#21147;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22312;&#32534;&#35793;&#22120;&#32423;&#21035;&#23558;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#20998;&#21306;&#26144;&#23556;&#21040;&#22810;&#20010;&#20114;&#32852;&#30828;&#20214;&#35774;&#22791;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24322;&#26500;DNN&#32534;&#35793;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#25552;&#20379;&#33258;&#21160;&#20998;&#21306;&#21644;&#35774;&#22791;&#26144;&#23556;&#12290;&#25105;&#20204;&#30340;&#35843;&#24230;&#22120;&#38598;&#25104;&#20102;&#19968;&#20010;&#31934;&#30830;&#27714;&#35299;&#22120;&#65292;&#36890;&#36807;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;(MILP)&#30340;&#24418;&#24335;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#27169;&#22359;&#24615;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#23454;&#29616;&#21487;&#25193;&#23637;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#19979;&#30028;&#20844;&#24335;&#26469;&#35780;&#20272;&#21551;&#21457;&#24335;&#35299;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#30001;CPU&#21644;&#20004;&#20010;&#35774;&#22791;&#32452;&#25104;&#30340;&#24322;&#26500;&#31995;&#32479;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#35843;&#24230;&#22120;&#65292;&#20248;&#21270;&#20256;&#32479;&#30340;DNNs&#21644;&#38543;&#26426;&#36830;&#25509;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#32771;&#34385;&#21040;&#24310;&#36831;&#21644;&#21534;&#21520;&#37327;&#30340;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
Datacenters are increasingly becoming heterogeneous, and are starting to include specialized hardware for networking, video processing, and especially deep learning. To leverage the heterogeneous compute capability of modern datacenters, we develop an approach for compiler-level partitioning of deep neural networks (DNNs) onto multiple interconnected hardware devices. We present a general framework for heterogeneous DNN compilation, offering automatic partitioning and device mapping. Our scheduler integrates both an exact solver, through a mixed integer linear programming (MILP) formulation, and a modularity-based heuristic for scalability. Furthermore, we propose a theoretical lower bound formula for the optimal solution, which enables the assessment of the heuristic solutions' quality. We evaluate our scheduler in optimizing both conventional DNNs and randomly-wired neural networks, subject to latency and throughput constraints, on a heterogeneous system comprised of a CPU and two di
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21367;&#31215;&#21344;&#25454;&#27169;&#22411;F-CON&#65292;&#29992;&#20110;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#36827;&#34892;&#22797;&#26434;&#12289;&#26410;&#35265;&#36807;&#29289;&#20307;&#30340;&#23494;&#38598;&#22534;&#25918;&#12290;&#36890;&#36807;&#19982;&#29616;&#26377;&#35268;&#21010;&#26041;&#27861;&#32467;&#21512;&#65292;F-CON&#21487;&#20197;&#26377;&#25928;&#35299;&#20915;&#39640;&#24230;&#36974;&#25377;&#12289;&#37096;&#20998;&#35266;&#23519;&#22330;&#26223;&#19979;&#30340;&#20960;&#20309;&#24418;&#29366;&#24863;&#30693;&#22256;&#38590;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#24418;&#29366;&#34917;&#20840;&#26041;&#27861;&#30340;&#27604;&#36739;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.00091</link><description>&lt;p&gt;
&#22797;&#26434;&#12289;&#26032;&#39062;&#29289;&#20307;&#30340;&#23494;&#38598;&#22534;&#25918;&#30340;&#21367;&#31215;&#21344;&#25454;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Convolutional Occupancy Models for Dense Packing of Complex, Novel Objects. (arXiv:2308.00091v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00091
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21367;&#31215;&#21344;&#25454;&#27169;&#22411;F-CON&#65292;&#29992;&#20110;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#36827;&#34892;&#22797;&#26434;&#12289;&#26410;&#35265;&#36807;&#29289;&#20307;&#30340;&#23494;&#38598;&#22534;&#25918;&#12290;&#36890;&#36807;&#19982;&#29616;&#26377;&#35268;&#21010;&#26041;&#27861;&#32467;&#21512;&#65292;F-CON&#21487;&#20197;&#26377;&#25928;&#35299;&#20915;&#39640;&#24230;&#36974;&#25377;&#12289;&#37096;&#20998;&#35266;&#23519;&#22330;&#26223;&#19979;&#30340;&#20960;&#20309;&#24418;&#29366;&#24863;&#30693;&#22256;&#38590;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#24418;&#29366;&#34917;&#20840;&#26041;&#27861;&#30340;&#27604;&#36739;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#20179;&#20648;&#21644;&#29289;&#27969;&#24212;&#29992;&#20013;&#65292;&#23494;&#38598;&#22534;&#25918;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#29305;&#24615;&#12290;&#36825;&#20010;&#39046;&#22495;&#30340;&#26089;&#26399;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20223;&#30495;&#20013;&#30340;&#35268;&#21010;&#31639;&#27861;&#19978;&#65292;&#20294;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#22534;&#25918;&#24615;&#33021;&#24448;&#24448;&#21463;&#21040;&#39640;&#24230;&#36974;&#25377;&#12289;&#37096;&#20998;&#35266;&#23519;&#22330;&#26223;&#20013;&#19977;&#32500;&#29289;&#20307;&#20960;&#20309;&#24418;&#29366;&#24863;&#30693;&#30340;&#22256;&#38590;&#30340;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#20840;&#21367;&#31215;&#30340;&#24418;&#29366;&#34917;&#20840;&#27169;&#22411;F-CON&#65292;&#23427;&#21487;&#20197;&#36731;&#26494;&#22320;&#19982;&#29616;&#25104;&#30340;&#35268;&#21010;&#26041;&#27861;&#32467;&#21512;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#23454;&#29616;&#23494;&#38598;&#22534;&#25918;&#12290;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#19968;&#20010;&#27169;&#25311;&#25968;&#25454;&#38598;COB-3D-v2&#65292;&#21487;&#20197;&#29992;&#20110;&#35757;&#32451;&#30495;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#24212;&#29992;&#30340;&#24418;&#29366;&#34917;&#20840;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#35813;&#25968;&#25454;&#38598;&#35777;&#26126;F-CON&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#24418;&#29366;&#34917;&#20840;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;F-CON&#24212;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#30340;&#25441;&#25918;&#31995;&#32479;&#20013;&#65292;&#24182;&#28436;&#31034;&#20102;&#22312;&#26434;&#20081;&#30340;&#22330;&#26223;&#20013;&#23545;&#22797;&#26434;&#12289;&#26410;&#35265;&#36807;&#30340;&#29289;&#20307;&#36827;&#34892;&#23494;&#38598;&#22534;&#25918;&#12290;&#22312;&#22810;&#31181;&#35268;&#21010;&#26041;&#27861;&#19979;&#65292;F-CON&#23454;&#29616;&#20102;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#22909;&#30340;&#23494;&#38598;&#22534;&#25918;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dense packing in pick-and-place systems is an important feature in many warehouse and logistics applications. Prior work in this space has largely focused on planning algorithms in simulation, but real-world packing performance is often bottlenecked by the difficulty of perceiving 3D object geometry in highly occluded, partially observed scenes. In this work, we present a fully-convolutional shape completion model, F-CON, which can be easily combined with off-the-shelf planning methods for dense packing in the real world. We also release a simulated dataset, COB-3D-v2, that can be used to train shape completion models for real-word robotics applications, and use it to demonstrate that F-CON outperforms other state-of-the-art shape completion methods. Finally, we equip a real-world pick-and-place system with F-CON, and demonstrate dense packing of complex, unseen objects in cluttered scenes. Across multiple planning methods, F-CON enables substantially better dense packing than other sh
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#25216;&#26415;&#65292;&#29992;&#20110;&#35777;&#26126;&#27979;&#35797;&#20998;&#24067;&#21333;&#35843;&#24615;&#21644;&#23545;&#25968;&#20985;&#24615;&#30340;&#19979;&#30028;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#35843;&#25972;&#19968;&#23545;&#20108;&#39033;&#27010;&#29575;&#30340;&#27010;&#29575;&#26500;&#36896;&#19968;&#23545;&#30697;&#21305;&#37197;&#30340;&#20998;&#24067;&#26063;&#65292;&#23454;&#29616;&#20102;&#20445;&#25345;&#21644;&#36829;&#32972;&#23450;&#20041;&#19981;&#31561;&#24335;&#30340;&#20998;&#24067;&#65292;&#24182;&#24471;&#21040;&#20102;&#30456;&#20851;&#30340;&#26032;&#19979;&#30028;&#12290;</title><link>http://arxiv.org/abs/2308.00089</link><description>&lt;p&gt;
&#27979;&#35797;&#20998;&#24067;&#30340;&#21333;&#35843;&#24615;&#21644;&#23545;&#25968;&#20985;&#24615;&#30340;&#26032;&#19979;&#30028;
&lt;/p&gt;
&lt;p&gt;
New Lower Bounds for Testing Monotonicity and Log Concavity of Distributions. (arXiv:2308.00089v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00089
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#25216;&#26415;&#65292;&#29992;&#20110;&#35777;&#26126;&#27979;&#35797;&#20998;&#24067;&#21333;&#35843;&#24615;&#21644;&#23545;&#25968;&#20985;&#24615;&#30340;&#19979;&#30028;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#35843;&#25972;&#19968;&#23545;&#20108;&#39033;&#27010;&#29575;&#30340;&#27010;&#29575;&#26500;&#36896;&#19968;&#23545;&#30697;&#21305;&#37197;&#30340;&#20998;&#24067;&#26063;&#65292;&#23454;&#29616;&#20102;&#20445;&#25345;&#21644;&#36829;&#32972;&#23450;&#20041;&#19981;&#31561;&#24335;&#30340;&#20998;&#24067;&#65292;&#24182;&#24471;&#21040;&#20102;&#30456;&#20851;&#30340;&#26032;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#29992;&#20110;&#35777;&#26126;&#20998;&#24067;&#27979;&#35797;&#19979;&#30028;&#30340;&#26032;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#36866;&#29992;&#20110;&#28041;&#21450;&#20998;&#24067;&#20013;&#20108;&#39033;&#27010;&#29575;&#30340;&#19981;&#31561;&#24335;&#23450;&#20041;&#30340;&#24615;&#36136;&#12290;&#21033;&#29992;&#36825;&#31181;&#25216;&#26415;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#20851;&#20110;&#31163;&#25955;&#31435;&#26041;&#20307;&#19978;&#30340;&#21333;&#35843;&#24615;&#27979;&#35797;&#30340;&#26032;&#19979;&#30028;&#20197;&#21450;&#20851;&#20110;&#23545;&#25968;&#20985;&#24615;&#27979;&#35797;&#30340;&#32039;&#23494;&#19979;&#30028;&#12290;&#25105;&#20204;&#30340;&#22522;&#26412;&#25216;&#26415;&#28041;&#21450;&#36890;&#36807;&#35843;&#25972;&#19968;&#23545;&#20108;&#39033;&#27010;&#29575;&#30340;&#27010;&#29575;&#26469;&#26500;&#36896;&#19968;&#23545;&#30697;&#21305;&#37197;&#30340;&#20998;&#24067;&#26063;&#65292;&#20351;&#24471;&#19968;&#20010;&#26063;&#20445;&#25345;&#23450;&#20041;&#30340;&#19981;&#31561;&#24335;&#65292;&#32780;&#21478;&#19968;&#20010;&#26063;&#36829;&#32972;&#23427;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a new technique for proving distribution testing lower bounds for properties defined by inequalities involving the bin probabilities of the distribution in question. Using this technique we obtain new lower bounds for monotonicity testing over discrete cubes and tight lower bounds for log-concavity testing.  Our basic technique involves constructing a pair of moment-matching families of distributions by tweaking the probabilities of pairs of bins so that one family maintains the defining inequalities while the other violates them.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65288;GMMs&#65289;&#30340;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#38663;&#33633;&#25429;&#25417;&#31639;&#27861;&#12290;&#36825;&#31181;&#31639;&#27861;&#20855;&#26377;&#26174;&#33879;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#22797;&#26434;&#20960;&#20309;&#32467;&#26500;&#21644;&#27969;&#21160;&#37197;&#32622;&#12290;</title><link>http://arxiv.org/abs/2308.00086</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#38663;&#33633;&#25429;&#25417;&#39640;&#38454;CFD&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
Unsupervised machine learning shock capturing for High-Order CFD solvers. (arXiv:2308.00086v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00086
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65288;GMMs&#65289;&#30340;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#38663;&#33633;&#25429;&#25417;&#31639;&#27861;&#12290;&#36825;&#31181;&#31639;&#27861;&#20855;&#26377;&#26174;&#33879;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#22797;&#26434;&#20960;&#20309;&#32467;&#26500;&#21644;&#27969;&#21160;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65288;GMMs&#65289;&#30340;&#26032;&#22411;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#38663;&#33633;&#25429;&#25417;&#31639;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;GMM&#20256;&#24863;&#22120;&#22312;&#26816;&#27979;&#38663;&#33633;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#22312;&#19981;&#38656;&#35201;&#21442;&#25968;&#35843;&#20248;&#30340;&#24773;&#20917;&#19979;&#22312;&#22810;&#26679;&#30340;&#27979;&#35797;&#26696;&#20363;&#19978;&#37117;&#34920;&#29616;&#20986;&#20102;&#36739;&#22909;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#23558;&#22522;&#20110;GMM&#30340;&#20256;&#24863;&#22120;&#19982;&#26368;&#20808;&#36827;&#30340;&#26367;&#20195;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25152;&#26377;&#26041;&#27861;&#37117;&#38598;&#25104;&#21040;&#39640;&#38454;&#21487;&#21387;&#24615;&#19981;&#36830;&#32493;Galerkin&#27714;&#35299;&#22120;&#20013;&#65292;&#20154;&#24037;&#40655;&#24615;&#21487;&#20197;&#35843;&#33410;&#20197;&#25429;&#25417;&#38663;&#33633;&#12290;&#36229;&#38899;&#36895;&#27979;&#35797;&#26696;&#20363;&#65292;&#21253;&#25324;&#39640;&#38647;&#35834;&#25968;&#65292;&#23637;&#31034;&#20102;&#20256;&#24863;&#22120;&#30340;&#24615;&#33021;&#65292;&#34920;&#26126;&#20854;&#25928;&#26524;&#19982;&#31934;&#35843;&#30340;&#26368;&#20808;&#36827;&#20256;&#24863;&#22120;&#30456;&#24403;&#12290;%&#33410;&#28857;DG&#26041;&#27861;&#20801;&#35768;&#22312;&#20122;&#21333;&#20803;&#36890;&#37327;&#26377;&#24046;&#24322;&#30340;&#20844;&#24335;&#20013;&#36827;&#34892;&#28508;&#22312;&#24212;&#29992;&#65292;&#36229;&#38899;&#36895;&#29305;&#24449;&#26816;&#27979;&#21644;&#32593;&#26684;&#32454;&#21270;&#12290;&#36825;&#31181;&#22522;&#20110;GMM&#30340;&#20256;&#24863;&#22120;&#36866;&#29992;&#20110;&#22797;&#26434;&#20960;&#20309;&#32467;&#26500;&#21644;&#21508;&#31181;&#27969;&#21160;&#37197;&#32622;&#65292;&#20854;&#33258;&#36866;&#24212;&#24615;&#21644;&#26080;&#38656;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#21151;&#33021;&#20351;&#20854;&#20855;&#22791;&#24191;&#27867;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel unsupervised machine learning shock capturing algorithm based on Gaussian Mixture Models (GMMs). The proposed GMM sensor demonstrates remarkable accuracy in detecting shocks and is robust across diverse test cases without the need for parameter tuning. We compare the GMM-based sensor with state-of-the-art alternatives. All methods are integrated into a high-order compressible discontinuous Galerkin solver where artificial viscosity can be modulated to capture shocks. Supersonic test cases, including high Reynolds numbers, showcase the sensor's performance, demonstrating the same effectiveness as fine-tuned state-of-the-art sensors. %The nodal DG aproach allows for potential applications in sub-cell flux-differencing formulations, supersonic feature detection, and mesh refinement. The adaptive nature and ability to function without extensive training datasets make this GMM-based sensor suitable for complex geometries and varied flow configurations. Our study reveals t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65288;NIDS&#65289;&#23545;&#25239;&#23545;&#25239;&#25915;&#20987;&#30340;&#26032;&#22411;&#27169;&#22411;&#12290;&#36890;&#36807;&#23454;&#26045;&#24378;&#22823;&#30340;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;&#22914;FGSM&#12289;JSMA&#12289;PGD&#21644;C&amp;W&#65292;&#24182;&#37319;&#29992;&#23545;&#25239;&#35757;&#32451;&#20316;&#20026;&#38450;&#24481;&#26041;&#27861;&#26469;&#22686;&#24378;NIDS&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.00077</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26032;&#22411;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#23545;&#25239;&#23545;&#25239;&#25915;&#20987;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Novel Deep Learning based Model to Defend Network Intrusion Detection System against Adversarial Attacks. (arXiv:2308.00077v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00077
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65288;NIDS&#65289;&#23545;&#25239;&#23545;&#25239;&#25915;&#20987;&#30340;&#26032;&#22411;&#27169;&#22411;&#12290;&#36890;&#36807;&#23454;&#26045;&#24378;&#22823;&#30340;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;&#22914;FGSM&#12289;JSMA&#12289;PGD&#21644;C&amp;W&#65292;&#24182;&#37319;&#29992;&#23545;&#25239;&#35757;&#32451;&#20316;&#20026;&#38450;&#24481;&#26041;&#27861;&#26469;&#22686;&#24378;NIDS&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65288;NIDS&#65289;&#26159;&#20445;&#25252;&#32593;&#32476;&#31354;&#38388;&#20813;&#21463;&#21508;&#31181;&#23433;&#20840;&#39118;&#38505;&#21644;&#26410;&#30693;&#32593;&#32476;&#25915;&#20987;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#24050;&#32463;&#23454;&#26045;&#20102;&#35768;&#22810;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21644;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#30340;NIDS&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#25152;&#26377;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#37117;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#24694;&#24847;&#34892;&#20026;&#32773;&#36890;&#36807;&#21521;&#31995;&#32479;&#20013;&#27880;&#20837;&#23545;&#25239;&#25200;&#21160;&#26679;&#26412;&#26469;&#35797;&#22270;&#22238;&#36991;&#25110;&#27450;&#39575;&#27169;&#22411;&#12290;&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#30740;&#31350;&#24378;&#22823;&#30340;&#23545;&#25239;&#25915;&#20987;&#31639;&#27861;&#21450;&#20854;&#23545;DL-based NIDS&#30340;&#38450;&#24481;&#26041;&#27861;&#12290;&#24555;&#36895;&#26799;&#24230;&#31526;&#21495;&#26041;&#27861;&#65288;FGSM&#65289;&#65292;Jacobian&#26174;&#33879;&#24615;&#22270;&#25915;&#20987;&#65288;JSMA&#65289;&#65292;&#25237;&#24433;&#26799;&#24230;&#19979;&#38477;&#65288;PGD&#65289;&#21644;Carlini&#65286;Wagner&#65288;C&#65286;W&#65289;&#26159;&#22235;&#31181;&#23545;NIDS&#23454;&#26045;&#30340;&#24378;&#22823;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#12290;&#20316;&#20026;&#38450;&#24481;&#26041;&#27861;&#65292;&#37319;&#29992;&#23545;&#25239;&#35757;&#32451;&#26469;&#22686;&#24378;NIDS&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#12290;&#32467;&#26524;&#21487;&#20197;&#24635;&#32467;&#20026;&#19977;&#20010;&#38454;&#27573;&#65292;&#21363;1&#65289;&#23545;&#25239;&#25915;&#20987;&#21069;&#65292;2&#65289;&#23545;&#25239;&#25915;&#20987;&#21518;&#65292;3&#65289;&#23545;&#25239;&#25915;&#20987;&#21518;&#12290;
&lt;/p&gt;
&lt;p&gt;
Network Intrusion Detection System (NIDS) is an essential tool in securing cyberspace from a variety of security risks and unknown cyberattacks. A number of solutions have been implemented for Machine Learning (ML), and Deep Learning (DL) based NIDS. However, all these solutions are vulnerable to adversarial attacks, in which the malicious actor tries to evade or fool the model by injecting adversarial perturbed examples into the system. The main aim of this research work is to study powerful adversarial attack algorithms and their defence method on DL-based NIDS. Fast Gradient Sign Method (FGSM), Jacobian Saliency Map Attack (JSMA), Projected Gradient Descent (PGD) and Carlini &amp; Wagner (C&amp;W) are four powerful adversarial attack methods implemented against the NIDS. As a defence method, Adversarial Training is used to increase the robustness of the NIDS model. The results are summarized in three phases, i.e., 1) before the adversarial attack, 2) after the adversarial attack, and 3) aft
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#21644;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#25552;&#21319;&#20154;&#32676;&#31649;&#29702;&#30340;&#35268;&#21010;&#21644;&#25805;&#20316;&#38454;&#27573;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#21019;&#26032;&#30340;&#25968;&#25454;&#25910;&#38598;&#25216;&#26415;&#12289;&#25968;&#25454;&#38598;&#25104;&#21644;3D&#25968;&#23383;&#23402;&#29983;&#25216;&#26415;&#65292;&#32467;&#21512;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#36827;&#34892;&#39118;&#38505;&#35782;&#21035;&#65292;&#24182;&#24341;&#20837;&#20102;&#34676;&#34678;&#32467;&#27169;&#22411;&#26469;&#35780;&#20272;&#21644;&#39044;&#27979;&#39118;&#38505;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2308.00076</link><description>&lt;p&gt;
&#20154;&#32676;&#23433;&#20840;&#31649;&#29702;&#31995;&#32479;&#65306;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#27963;&#21160;&#20915;&#31574;&#25903;&#25345;&#30340;&#35268;&#21010;&#21644;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Crowd Safety Manager: Towards Data-Driven Active Decision Support for Planning and Control of Crowd Events. (arXiv:2308.00076v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00076
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#21644;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#25552;&#21319;&#20154;&#32676;&#31649;&#29702;&#30340;&#35268;&#21010;&#21644;&#25805;&#20316;&#38454;&#27573;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#21019;&#26032;&#30340;&#25968;&#25454;&#25910;&#38598;&#25216;&#26415;&#12289;&#25968;&#25454;&#38598;&#25104;&#21644;3D&#25968;&#23383;&#23402;&#29983;&#25216;&#26415;&#65292;&#32467;&#21512;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#36827;&#34892;&#39118;&#38505;&#35782;&#21035;&#65292;&#24182;&#24341;&#20837;&#20102;&#34676;&#34678;&#32467;&#27169;&#22411;&#26469;&#35780;&#20272;&#21644;&#39044;&#27979;&#39118;&#38505;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#21644;&#26041;&#27861;&#65292;&#26088;&#22312;&#22686;&#24378;&#35268;&#21010;&#21644;&#25805;&#20316;&#38454;&#27573;&#30340;&#20154;&#32676;&#31649;&#29702;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#21019;&#26032;&#30340;&#25968;&#25454;&#25910;&#38598;&#25216;&#26415;&#12289;&#25968;&#25454;&#38598;&#25104;&#21644;&#21487;&#35270;&#21270;&#65292;&#20351;&#29992;3D&#25968;&#23383;&#23402;&#29983;&#25216;&#26415;&#65292;&#24182;&#32467;&#21512;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#36827;&#34892;&#39118;&#38505;&#35782;&#21035;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#8220;&#34676;&#34678;&#32467;&#8221;&#27169;&#22411;&#65292;&#36825;&#26159;&#19968;&#20010;&#32508;&#21512;&#24615;&#26694;&#26550;&#65292;&#26088;&#22312;&#35780;&#20272;&#21644;&#39044;&#27979;&#39118;&#38505;&#27700;&#24179;&#12290;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;&#23458;&#35266;&#20272;&#35745;&#21644;&#39044;&#27979;&#65292;&#22914;&#20132;&#36890;&#27969;&#37327;&#36816;&#33829;&#21644;&#25317;&#25380;&#31243;&#24230;&#65292;&#20197;&#21450;&#21508;&#31181;&#24694;&#21270;&#22240;&#32032;&#65292;&#22914;&#22825;&#27668;&#26465;&#20214;&#12289;&#24773;&#32490;&#21644;&#28216;&#23458;&#30340;&#30446;&#30340;&#65292;&#20197;&#35780;&#20272;&#28508;&#22312;&#20107;&#20214;&#39118;&#38505;&#12290;&#25552;&#20986;&#30340;&#26694;&#26550;&#24212;&#29992;&#20110;Scheveningen&#30340;&#20154;&#32676;&#23433;&#20840;&#31649;&#29702;&#39033;&#30446;&#65292;&#20854;&#20013;DigiTwin&#22522;&#20110;&#20016;&#23500;&#30340;&#23454;&#26102;&#25968;&#25454;&#26469;&#28304;&#36827;&#34892;&#24320;&#21457;&#12290;&#19968;&#20010;&#20540;&#24471;&#27880;&#24847;&#30340;&#25968;&#25454;&#26469;&#28304;&#26159;Resono&#65292;&#25552;&#20379;&#35775;&#23458;&#25968;&#37327;&#21644;&#21160;&#21521;&#30340;&#35265;&#35299;&#65292;&#20805;&#20998;&#21033;&#29992;&#20102;&#19968;&#32452;
&lt;/p&gt;
&lt;p&gt;
This paper presents novel technology and methodology aimed at enhancing crowd management in both the planning and operational phases. The approach encompasses innovative data collection techniques, data integration, and visualization using a 3D Digital Twin, along with the incorporation of artificial intelligence (AI) tools for risk identification. The paper introduces the Bowtie model, a comprehensive framework designed to assess and predict risk levels. The model combines objective estimations and predictions, such as traffic flow operations and crowdedness levels, with various aggravating factors like weather conditions, sentiments, and the purpose of visitors, to evaluate the expected risk of incidents. The proposed framework is applied to the Crowd Safety Manager project in Scheveningen, where the DigiTwin is developed based on a wealth of real-time data sources. One noteworthy data source is Resono, offering insights into the number of visitors and their movements, leveraging a m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#20102;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26041;&#27861;&#20013;&#30340;&#26680;SHAP&#26041;&#27861;&#26469;&#26816;&#27979;&#21644;&#35299;&#37322;&#32593;&#32476;&#24322;&#24120;&#65292;&#24182;&#19988;&#36890;&#36807;&#35813;&#26041;&#27861;&#20248;&#21270;&#20102;&#32593;&#32476;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12289;&#21484;&#22238;&#29575;&#12289;&#31934;&#30830;&#24230;&#21644;F&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2308.00074</link><description>&lt;p&gt;
&#20351;&#29992;Kernel SHAP XAI&#26041;&#27861;&#20248;&#21270;&#32593;&#32476;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Using Kernel SHAP XAI Method to optimize the Network Anomaly Detection Model. (arXiv:2308.00074v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00074
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#20102;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26041;&#27861;&#20013;&#30340;&#26680;SHAP&#26041;&#27861;&#26469;&#26816;&#27979;&#21644;&#35299;&#37322;&#32593;&#32476;&#24322;&#24120;&#65292;&#24182;&#19988;&#36890;&#36807;&#35813;&#26041;&#27861;&#20248;&#21270;&#20102;&#32593;&#32476;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12289;&#21484;&#22238;&#29575;&#12289;&#31934;&#30830;&#24230;&#21644;F&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#21450;&#20854;&#35299;&#37322;&#22312;&#35768;&#22810;&#30740;&#31350;&#39046;&#22495;&#20013;&#38750;&#24120;&#37325;&#35201;&#65292;&#27604;&#22914;&#20837;&#20405;&#26816;&#27979;&#12289;&#27450;&#35784;&#26816;&#27979;&#12289;&#32593;&#32476;&#27969;&#37327;&#21644;&#26085;&#24535;&#20013;&#30340;&#26410;&#30693;&#25915;&#20987;&#26816;&#27979;&#12290;&#30001;&#20110;&#20854;&#26080;&#36793;&#30028;&#21644;&#32570;&#20047;&#30417;&#30563;&#24615;&#36136;&#65292;&#24456;&#38590;&#30830;&#23450;&#20026;&#20160;&#20040;&#19968;&#20010;&#23454;&#20363;&#26159;&#24322;&#24120;&#30340;&#65292;&#32780;&#21478;&#19968;&#20010;&#23454;&#20363;&#19981;&#26159;&#12290;&#36825;&#20010;&#38382;&#39064;&#30340;&#31572;&#26696;&#21487;&#33021;&#26469;&#33258;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#36825;&#19968;&#26032;&#20852;&#25216;&#26415;&#12290;XAI&#25552;&#20379;&#20102;&#35299;&#37322;&#21644;&#35299;&#37322;&#22797;&#26434;&#27169;&#22411;&#65288;&#22914;&#28145;&#24230;&#23398;&#20064;&#65289;&#36755;&#20986;&#21644;&#24037;&#20316;&#30340;&#24037;&#20855;&#21644;&#25216;&#26415;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;XAI&#30340;&#26680;SHAP&#26041;&#27861;&#26816;&#27979;&#21644;&#35299;&#37322;&#32593;&#32476;&#24322;&#24120;&#12290;&#37319;&#29992;&#30456;&#21516;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#32593;&#32476;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12289;&#21484;&#22238;&#29575;&#12289;&#31934;&#30830;&#24230;&#21644;F&#20998;&#25968;&#12290;&#20351;&#29992;&#26368;&#26032;&#30340;CICIDS2017&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#12290;&#21019;&#24314;&#20102;&#20004;&#20010;&#27169;&#22411;&#65288;Model_1&#21644;OPT_Model&#65289;&#36827;&#34892;&#27604;&#36739;&#12290;OPT_Model&#22312;&#26080;&#30417;&#30563;&#26041;&#24335;&#35757;&#32451;&#26102;&#30340;&#25972;&#20307;&#20934;&#30830;&#24615;&#21644;F&#20998;&#25968;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection and its explanation is important in many research areas such as intrusion detection, fraud detection, unknown attack detection in network traffic and logs. It is challenging to identify the cause or explanation of why one instance is an anomaly? and the other is not due to its unbounded and lack of supervisory nature. The answer to this question is possible with the emerging technique of explainable artificial intelligence (XAI). XAI provides tools and techniques to interpret and explain the output and working of complex models such as Deep Learning (DL). This paper aims to detect and explain network anomalies with XAI, kernelSHAP method. The same approach is used to improve the network anomaly detection model in terms of accuracy, recall, precision and f score. The experiment is conduced with the latest CICIDS2017 dataset. Two models are created (Model_1 and OPT_Model) and compared. The overall accuracy and F score of OPT_Model (when trained in unsupervised way) are 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#25512;&#29702;&#26041;&#27861;&#65292;&#22312;&#38646;&#23556;&#20987;&#21051;&#26495;&#21360;&#35937;&#35782;&#21035;&#20013;&#21462;&#24471;&#20102;&#37325;&#35201;&#30340;&#36827;&#23637;&#65292;&#24182;&#21457;&#29616;&#25512;&#29702;&#30340;&#24615;&#33021;&#22686;&#30410;&#36828;&#36828;&#36229;&#36807;&#27169;&#22411;&#35268;&#27169;&#25193;&#23637;&#30340;&#22686;&#30410;&#12290;&#25512;&#29702;&#19981;&#20165;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#65292;&#36824;&#25552;&#39640;&#20102;&#20915;&#31574;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.00071</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#25512;&#29702;&#26041;&#27861;&#29992;&#20110;&#21051;&#26495;&#21360;&#35937;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Interpretable Stereotype Identification through Reasoning. (arXiv:2308.00071v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#25512;&#29702;&#26041;&#27861;&#65292;&#22312;&#38646;&#23556;&#20987;&#21051;&#26495;&#21360;&#35937;&#35782;&#21035;&#20013;&#21462;&#24471;&#20102;&#37325;&#35201;&#30340;&#36827;&#23637;&#65292;&#24182;&#21457;&#29616;&#25512;&#29702;&#30340;&#24615;&#33021;&#22686;&#30410;&#36828;&#36828;&#36229;&#36807;&#27169;&#22411;&#35268;&#27169;&#25193;&#23637;&#30340;&#22686;&#30410;&#12290;&#25512;&#29702;&#19981;&#20165;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#65292;&#36824;&#25552;&#39640;&#20102;&#20915;&#31574;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#20351;&#29992;&#20102;&#21253;&#21547;&#22266;&#26377;&#20559;&#35265;&#30340;&#22823;&#37327;&#25968;&#25454;&#38598;&#65292;&#21487;&#33021;&#20250;&#19981;&#32463;&#24847;&#22320;&#25345;&#32493;&#31995;&#32479;&#24615;&#27495;&#35270;&#65292;&#22240;&#27492;&#65292;&#23457;&#26597;&#21644;&#35299;&#20915;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#23558;&#20844;&#24179;&#24615;&#25972;&#21512;&#21040;&#23427;&#20204;&#30340;&#21457;&#23637;&#20013;&#65292;&#20197;&#30830;&#20445;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;&#20844;&#27491;&#21644;&#26080;&#20559;&#30340;&#29305;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22522;&#20110;Vicuna-13B-v1.3&#30340;&#38646;&#23556;&#20987;&#21051;&#26495;&#21360;&#35937;&#35782;&#21035;&#20013;&#25512;&#29702;&#30340;&#37325;&#35201;&#24615;&#12290;&#23613;&#31649;&#25105;&#20204;&#35266;&#23519;&#21040;&#20174;13B&#21040;33B&#30340;&#35268;&#27169;&#25193;&#23637;&#20250;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#25105;&#20204;&#34920;&#26126;&#25512;&#29702;&#30340;&#24615;&#33021;&#22686;&#30410;&#36828;&#36828;&#36229;&#36807;&#35268;&#27169;&#25193;&#23637;&#30340;&#22686;&#30410;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25512;&#29702;&#21487;&#33021;&#26159;&#20351;LLMs&#22312;&#21051;&#26495;&#21360;&#35937;&#31561;&#39046;&#22495;&#20219;&#21153;&#19978;&#36229;&#36234;&#35268;&#27169;&#23450;&#24459;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#23545;&#36873;&#23450;&#30340;&#25512;&#29702;&#36861;&#36394;&#36827;&#34892;&#23450;&#24615;&#20998;&#26512;&#65292;&#25105;&#20204;&#31361;&#20986;&#26174;&#31034;&#20102;&#25512;&#29702;&#19981;&#20165;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#65292;&#36824;&#25552;&#39640;&#20102;&#20915;&#31574;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given that language models are trained on vast datasets that may contain inherent biases, there is a potential danger of inadvertently perpetuating systemic discrimination. Consequently, it becomes essential to examine and address biases in language models, integrating fairness into their development to ensure these models are equitable and free from bias. In this work, we demonstrate the importance of reasoning in zero-shot stereotype identification based on Vicuna-13B-v1.3. While we do observe improved accuracy by scaling from 13B to 33B, we show that the performance gain from reasoning significantly exceeds the gain from scaling up. Our findings suggest that reasoning could be a key factor that enables LLMs to trescend the scaling law on out-of-domain tasks such as stereotype identification. Additionally, through a qualitative analysis of select reasoning traces, we highlight how reasoning enhances not just accuracy but also the interpretability of the decision.
&lt;/p&gt;</description></item><item><title>FinPT&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#36164;&#37329;&#39118;&#38505;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#22823;&#22411;&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#19978;&#36827;&#34892;&#20010;&#20154;&#36164;&#26009;&#35843;&#25972;&#65292;&#22635;&#20805;&#37329;&#34701;&#34920;&#26684;&#25968;&#25454;&#24182;&#33719;&#24471;&#33258;&#28982;&#35821;&#35328;&#23458;&#25143;&#36164;&#26009;&#65292;&#20174;&#32780;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.00065</link><description>&lt;p&gt;
FinPT:&#20351;&#29992;&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#36164;&#37329;&#39118;&#38505;&#39044;&#27979;&#20013;&#20010;&#20154;&#36164;&#26009;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
FinPT: Financial Risk Prediction with Profile Tuning on Pretrained Foundation Models. (arXiv:2308.00065v1 [q-fin.RM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00065
&lt;/p&gt;
&lt;p&gt;
FinPT&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#36164;&#37329;&#39118;&#38505;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#22823;&#22411;&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#19978;&#36827;&#34892;&#20010;&#20154;&#36164;&#26009;&#35843;&#25972;&#65292;&#22635;&#20805;&#37329;&#34701;&#34920;&#26684;&#25968;&#25454;&#24182;&#33719;&#24471;&#33258;&#28982;&#35821;&#35328;&#23458;&#25143;&#36164;&#26009;&#65292;&#20174;&#32780;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36164;&#37329;&#39118;&#38505;&#39044;&#27979;&#22312;&#37329;&#34701;&#39046;&#22495;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#33258;&#21160;&#26816;&#27979;&#28508;&#22312;&#39118;&#38505;&#65292;&#20174;&#32780;&#33410;&#30465;&#21171;&#21160;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#36817;&#24180;&#26469;&#36825;&#19968;&#39046;&#22495;&#30340;&#21457;&#23637;&#28382;&#21518;&#20110;&#20197;&#19979;&#20004;&#20010;&#20107;&#23454;&#65306;1&#65289;&#25152;&#20351;&#29992;&#30340;&#31639;&#27861;&#26377;&#20123;&#36807;&#26102;&#65292;&#29305;&#21035;&#26159;&#22312;&#29983;&#25104;AI&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24555;&#36895;&#21457;&#23637;&#30340;&#32972;&#26223;&#19979;&#65307;2&#65289;&#32570;&#20047;&#32479;&#19968;&#19988;&#24320;&#28304;&#30340;&#37329;&#34701;&#22522;&#20934;&#24050;&#32463;&#38459;&#30861;&#20102;&#30456;&#20851;&#30740;&#31350;&#22810;&#24180;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FinPT&#21644;FinBench&#65306;&#21069;&#32773;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#36164;&#37329;&#39118;&#38505;&#39044;&#27979;&#26041;&#27861;&#65292;&#23545;&#22823;&#22411;&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#20010;&#20154;&#36164;&#26009;&#35843;&#25972;&#65307;&#21518;&#32773;&#26159;&#19968;&#22871;&#20851;&#20110;&#36164;&#37329;&#39118;&#38505;&#30340;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#65292;&#22914;&#36829;&#32422;&#12289;&#27450;&#35784;&#21644;&#27969;&#22833;&#12290;&#22312;FinPT&#20013;&#65292;&#25105;&#20204;&#23558;&#37329;&#34701;&#34920;&#26684;&#25968;&#25454;&#22635;&#20805;&#21040;&#39044;&#23450;&#20041;&#30340;&#25351;&#20196;&#27169;&#26495;&#20013;&#65292;&#36890;&#36807;&#25552;&#31034;LLMs&#33719;&#24471;&#33258;&#28982;&#35821;&#35328;&#23458;&#25143;&#36164;&#26009;&#65292;&#24182;&#36827;&#34892;&#31934;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
Financial risk prediction plays a crucial role in the financial sector. Machine learning methods have been widely applied for automatically detecting potential risks and thus saving the cost of labor. However, the development in this field is lagging behind in recent years by the following two facts: 1) the algorithms used are somewhat outdated, especially in the context of the fast advance of generative AI and large language models (LLMs); 2) the lack of a unified and open-sourced financial benchmark has impeded the related research for years. To tackle these issues, we propose FinPT and FinBench: the former is a novel approach for financial risk prediction that conduct Profile Tuning on large pretrained foundation models, and the latter is a set of high-quality datasets on financial risks such as default, fraud, and churn. In FinPT, we fill the financial tabular data into the pre-defined instruction template, obtain natural-language customer profiles by prompting LLMs, and fine-tune 
&lt;/p&gt;</description></item><item><title>T-Fusion Net&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#22810;&#20010;&#23450;&#20301;&#30340;&#31354;&#38388;&#27880;&#24847;&#26426;&#21046;&#25552;&#39640;&#20102;&#22270;&#20687;&#20998;&#31867;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#20351;&#29992;&#27169;&#31946;&#26368;&#22823;&#34701;&#21512;&#30340;&#26041;&#27861;&#21512;&#24182;&#22810;&#20010;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#22312;Covid-19&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.00053</link><description>&lt;p&gt;
T-Fusion Net&#65306;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#22522;&#20110;&#22810;&#20010;&#23450;&#20301;&#30340;&#31354;&#38388;&#27880;&#24847;&#26426;&#21046;&#29992;&#20110;Covid-19&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
T-Fusion Net: A Novel Deep Neural Network Augmented with Multiple Localizations based Spatial Attention Mechanisms for Covid-19 Detection. (arXiv:2308.00053v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00053
&lt;/p&gt;
&lt;p&gt;
T-Fusion Net&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#22810;&#20010;&#23450;&#20301;&#30340;&#31354;&#38388;&#27880;&#24847;&#26426;&#21046;&#25552;&#39640;&#20102;&#22270;&#20687;&#20998;&#31867;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#20351;&#29992;&#27169;&#31946;&#26368;&#22823;&#34701;&#21512;&#30340;&#26041;&#27861;&#21512;&#24182;&#22810;&#20010;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#22312;Covid-19&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#38598;&#30340;&#19981;&#26029;&#22797;&#26434;&#21270;&#21644;&#23545;&#25913;&#36827;&#24615;&#33021;&#30340;&#38656;&#27714;&#38656;&#35201;&#25506;&#32034;&#21019;&#26032;&#25216;&#26415;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;&#31216;&#20026;T-Fusion Net&#65289;&#65292;&#23427;&#22686;&#24378;&#20102;&#22522;&#20110;&#22810;&#20010;&#23450;&#20301;&#30340;&#31354;&#38388;&#27880;&#24847;&#26426;&#21046;&#12290;&#36825;&#31181;&#27880;&#24847;&#26426;&#21046;&#20351;&#24471;&#32593;&#32476;&#33021;&#22815;&#19987;&#27880;&#20110;&#30456;&#20851;&#30340;&#22270;&#20687;&#21306;&#22495;&#65292;&#25552;&#39640;&#20854;&#21028;&#21035;&#33021;&#21147;&#12290;&#36827;&#19968;&#27493;&#20351;&#29992;&#36825;&#31181;&#32593;&#32476;&#30340;&#22343;&#36136;&#27169;&#22411;&#26469;&#25552;&#39640;&#22270;&#20687;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;&#23545;&#20110;&#21512;&#24182;&#65292;&#25552;&#20986;&#30340;&#26041;&#27861;&#32771;&#34385;&#20102;&#27599;&#20010;T-Fusion Net&#20010;&#20307;&#30340;&#22810;&#20010;&#23454;&#20363;&#12290;&#27169;&#22411;&#36890;&#36807;&#27169;&#31946;&#26368;&#22823;&#34701;&#21512;&#26469;&#21512;&#24182;&#20010;&#20307;&#32593;&#32476;&#30340;&#36755;&#20986;&#12290;&#34701;&#21512;&#36807;&#31243;&#36890;&#36807;&#31934;&#24515;&#36873;&#25321;&#30340;&#21442;&#25968;&#36827;&#34892;&#20248;&#21270;&#65292;&#20197;&#22312;&#20010;&#20307;&#27169;&#22411;&#30340;&#36129;&#29486;&#26041;&#38754;&#21462;&#24471;&#24179;&#34913;&#12290;&#22312;&#22522;&#20934;Covid-19&#65288;SARS-CoV-2 CT&#25195;&#25551;&#65289;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;...&#65288;&#26410;&#23436;&#24453;&#32493;&#65289;
&lt;/p&gt;
&lt;p&gt;
In recent years, deep neural networks are yielding better performance in image classification tasks. However, the increasing complexity of datasets and the demand for improved performance necessitate the exploration of innovative techniques. The present work proposes a new deep neural network (called as, T-Fusion Net) that augments multiple localizations based spatial attention. This attention mechanism allows the network to focus on relevant image regions, improving its discriminative power. A homogeneous ensemble of the said network is further used to enhance image classification accuracy. For ensembling, the proposed approach considers multiple instances of individual T-Fusion Net. The model incorporates fuzzy max fusion to merge the outputs of individual nets. The fusion process is optimized through a carefully chosen parameter to strike a balance on the contributions of the individual models. Experimental evaluations on benchmark Covid-19 (SARS-CoV-2 CT scan) dataset demonstrate t
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35843;&#26597;&#20102;&#22312;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#20013;&#24212;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#29616;&#29366;&#12289;&#26426;&#20250;&#21644;&#24320;&#25918;&#30740;&#31350;&#38382;&#39064;&#12290;&#20316;&#32773;&#20027;&#35201;&#35752;&#35770;&#20102;&#19977;&#31181;&#24212;&#29992;&#31867;&#22411;&#65306;&#26080;&#29305;&#23450;&#30446;&#26631;&#30340;&#29983;&#25104;&#26041;&#24335;&#12289;&#21516;&#26102;&#26368;&#22823;&#21270;&#30446;&#26631;&#20989;&#25968;&#30340;&#36755;&#20986;&#29983;&#25104;&#26041;&#24335;&#20197;&#21450;&#23558;&#26080;&#27861;&#36890;&#36807;&#30446;&#26631;&#20989;&#25968;&#25429;&#25417;&#30340;&#26399;&#26395;&#29305;&#24449;&#23884;&#20837;&#29983;&#25104;&#36807;&#31243;&#30340;&#26041;&#24335;&#12290;&#36825;&#20010;&#26032;&#20852;&#39046;&#22495;&#20013;&#23384;&#22312;&#30528;&#20016;&#23500;&#30340;&#26426;&#20250;&#21644;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.00031</link><description>&lt;p&gt;
&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#30340;&#24378;&#21270;&#23398;&#20064;&#65306;&#29616;&#29366;&#12289;&#26426;&#20250;&#21644;&#24320;&#25918;&#30740;&#31350;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning for Generative AI: State of the Art, Opportunities and Open Research Challenges. (arXiv:2308.00031v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00031
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35843;&#26597;&#20102;&#22312;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#20013;&#24212;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#29616;&#29366;&#12289;&#26426;&#20250;&#21644;&#24320;&#25918;&#30740;&#31350;&#38382;&#39064;&#12290;&#20316;&#32773;&#20027;&#35201;&#35752;&#35770;&#20102;&#19977;&#31181;&#24212;&#29992;&#31867;&#22411;&#65306;&#26080;&#29305;&#23450;&#30446;&#26631;&#30340;&#29983;&#25104;&#26041;&#24335;&#12289;&#21516;&#26102;&#26368;&#22823;&#21270;&#30446;&#26631;&#20989;&#25968;&#30340;&#36755;&#20986;&#29983;&#25104;&#26041;&#24335;&#20197;&#21450;&#23558;&#26080;&#27861;&#36890;&#36807;&#30446;&#26631;&#20989;&#25968;&#25429;&#25417;&#30340;&#26399;&#26395;&#29305;&#24449;&#23884;&#20837;&#29983;&#25104;&#36807;&#31243;&#30340;&#26041;&#24335;&#12290;&#36825;&#20010;&#26032;&#20852;&#39046;&#22495;&#20013;&#23384;&#22312;&#30528;&#20016;&#23500;&#30340;&#26426;&#20250;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26159;&#36817;&#21313;&#24180;&#26469;&#35745;&#31639;&#26426;&#31185;&#23398;&#39046;&#22495;&#26368;&#20196;&#20154;&#20852;&#22859;&#30340;&#21457;&#23637;&#20043;&#19968;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#24050;&#32463;&#25104;&#20026;&#38750;&#24120;&#25104;&#21151;&#30340;&#33539;&#24335;&#12290;&#22312;&#26412;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#23558;RL&#24212;&#29992;&#20110;&#29983;&#25104;AI&#20013;&#30340;&#29616;&#29366;&#12289;&#26426;&#20250;&#21644;&#24320;&#25918;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#35752;&#35770;&#19977;&#31181;&#24212;&#29992;&#31867;&#22411;&#65292;&#21363;&#20316;&#20026;&#19968;&#31181;&#26080;&#29305;&#23450;&#30446;&#26631;&#30340;&#29983;&#25104;&#26041;&#24335;&#65292;&#20316;&#20026;&#19968;&#31181;&#21516;&#26102;&#26368;&#22823;&#21270;&#30446;&#26631;&#20989;&#25968;&#30340;&#36755;&#20986;&#29983;&#25104;&#26041;&#24335;&#65292;&#20197;&#21450;&#20316;&#20026;&#19968;&#31181;&#23558;&#26080;&#27861;&#36890;&#36807;&#30446;&#26631;&#20989;&#25968;&#36731;&#26494;&#25429;&#25417;&#30340;&#26399;&#26395;&#29305;&#24449;&#23884;&#20837;&#29983;&#25104;&#36807;&#31243;&#30340;&#26041;&#24335;&#12290;&#25105;&#20204;&#22312;&#35843;&#26597;&#32467;&#26524;&#20013;&#23545;&#36825;&#20010;&#36855;&#20154;&#30340;&#26032;&#20852;&#39046;&#22495;&#20013;&#30340;&#26426;&#20250;&#21644;&#25361;&#25112;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Artificial Intelligence (AI) is one of the most exciting developments in Computer Science of the last decade. At the same time, Reinforcement Learning (RL) has emerged as a very successful paradigm for a variety of machine learning tasks. In this survey, we discuss the state of the art, opportunities and open research questions in applying RL to generative AI. In particular, we will discuss three types of applications, namely, RL as an alternative way for generation without specified objectives; as a way for generating outputs while concurrently maximizing an objective function; and, finally, as a way of embedding desired characteristics, which cannot be easily captured by means of an objective function, into the generative process. We conclude the survey with an in-depth discussion of the opportunities and challenges in this fascinating emerging area.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;MusicVAE&#35299;&#37322;&#38899;&#20048;&#26102;&#65292;&#22823;&#37096;&#20998;&#20851;&#20110;&#38899;&#39640;&#21644;&#33410;&#22863;&#30340;&#20449;&#24687;&#34987;&#32534;&#30721;&#22312;&#21069;&#20960;&#20010;&#38899;&#20048;&#31070;&#32463;&#20803;&#20013;&#65292;&#32780;&#26059;&#24459;&#30340;&#27010;&#24565;&#21017;&#34920;&#29616;&#20026;&#36739;&#38271;&#30340;&#38899;&#20048;&#24207;&#21015;&#20013;&#30340;&#29420;&#31435;&#31070;&#32463;&#20803;&#12290;</title><link>http://arxiv.org/abs/2308.00015</link><description>&lt;p&gt;
&#25506;&#32034;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22914;&#20309;&#35299;&#37322;&#38899;&#20048;
&lt;/p&gt;
&lt;p&gt;
Exploring how a Generative AI interprets music. (arXiv:2308.00015v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00015
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;MusicVAE&#35299;&#37322;&#38899;&#20048;&#26102;&#65292;&#22823;&#37096;&#20998;&#20851;&#20110;&#38899;&#39640;&#21644;&#33410;&#22863;&#30340;&#20449;&#24687;&#34987;&#32534;&#30721;&#22312;&#21069;&#20960;&#20010;&#38899;&#20048;&#31070;&#32463;&#20803;&#20013;&#65292;&#32780;&#26059;&#24459;&#30340;&#27010;&#24565;&#21017;&#34920;&#29616;&#20026;&#36739;&#38271;&#30340;&#38899;&#20048;&#24207;&#21015;&#20013;&#30340;&#29420;&#31435;&#31070;&#32463;&#20803;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;Google&#30340;MusicVAE&#65292;&#19968;&#20010;&#20855;&#26377;512&#32500;&#28508;&#22312;&#31354;&#38388;&#30340;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#26469;&#34920;&#31034;&#20960;&#23567;&#33410;&#30340;&#38899;&#20048;&#65292;&#24182;&#26681;&#25454;&#23427;&#20204;&#22312;&#25551;&#36848;&#38899;&#20048;&#26041;&#38754;&#30340;&#30456;&#20851;&#24615;&#26469;&#32452;&#32455;&#28508;&#22312;&#32500;&#24230;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24179;&#22343;&#32780;&#35328;&#65292;&#22823;&#22810;&#25968;&#28508;&#22312;&#31070;&#32463;&#20803;&#22312;&#36755;&#20837;&#30495;&#23454;&#38899;&#20048;&#36712;&#36947;&#26102;&#20445;&#25345;&#27785;&#40664;&#65306;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#22122;&#22768;&#8221;&#31070;&#32463;&#20803;&#12290;&#20854;&#20313;&#23569;&#25968;&#34987;&#28608;&#27963;&#30340;&#28508;&#22312;&#31070;&#32463;&#20803;&#34987;&#31216;&#20026;&#8220;&#38899;&#20048;&#8221;&#31070;&#32463;&#20803;&#12290;&#25105;&#20204;&#24819;&#30693;&#36947;&#21738;&#20123;&#31070;&#32463;&#20803;&#25658;&#24102;&#30528;&#38899;&#20048;&#20449;&#24687;&#20197;&#21450;&#23427;&#20204;&#32534;&#30721;&#30340;&#26159;&#21738;&#31181;&#31867;&#22411;&#30340;&#38899;&#20048;&#20449;&#24687;&#65292;&#21363;&#21487;&#20197;&#35782;&#21035;&#20026;&#38899;&#39640;&#12289;&#33410;&#22863;&#25110;&#26059;&#24459;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22823;&#37096;&#20998;&#20851;&#20110;&#38899;&#39640;&#21644;&#33410;&#22863;&#30340;&#20449;&#24687;&#37117;&#34987;&#32534;&#30721;&#22312;&#21069;&#20960;&#20010;&#38899;&#20048;&#31070;&#32463;&#20803;&#20013;&#65306;&#22240;&#27492;&#65292;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#20102;&#19968;&#20123;&#38750;&#32447;&#24615;&#32534;&#30721;&#35768;&#22810;&#29992;&#20110;&#25551;&#36848;&#38899;&#39640;&#21644;&#33410;&#22863;&#30340;&#20154;&#20026;&#23450;&#20041;&#21464;&#37327;&#30340;&#21464;&#37327;&#12290;&#26059;&#24459;&#30340;&#27010;&#24565;&#20284;&#20046;&#21482;&#20250;&#22312;&#36739;&#38271;&#30340;&#38899;&#20048;&#24207;&#21015;&#20013;&#20986;&#29616;&#22312;&#29420;&#31435;&#30340;&#31070;&#32463;&#20803;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
We use Google's MusicVAE, a Variational Auto-Encoder with a 512-dimensional latent space to represent a few bars of music, and organize the latent dimensions according to their relevance in describing music. We find that, on average, most latent neurons remain silent when fed real music tracks: we call these "noise" neurons. The remaining few dozens of latent neurons that do fire are called "music neurons". We ask which neurons carry the musical information and what kind of musical information they encode, namely something that can be identified as pitch, rhythm or melody. We find that most of the information about pitch and rhythm is encoded in the first few music neurons: the neural network has thus constructed a couple of variables that non-linearly encode many human-defined variables used to describe pitch and rhythm. The concept of melody only seems to show up in independent neurons for longer sequences of music.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;&#21333;&#22768;&#36947;&#22810;&#20154;&#28436;&#35762;&#20998;&#31163;&#26041;&#27861;&#65292;&#26088;&#22312;&#20943;&#23569;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#24182;&#22312;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2308.00010</link><description>&lt;p&gt;
&#21333;&#22768;&#36947;&#22810;&#20154;&#28436;&#35762;&#20998;&#31163;&#20351;&#29992;&#39640;&#25928;Transformer&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Monaural Multi-Speaker Speech Separation Using Efficient Transformer Model. (arXiv:2308.00010v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00010
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;&#21333;&#22768;&#36947;&#22810;&#20154;&#28436;&#35762;&#20998;&#31163;&#26041;&#27861;&#65292;&#26088;&#22312;&#20943;&#23569;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#24182;&#22312;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20154;&#32858;&#20250;&#38382;&#39064;&#26159;&#19968;&#20010;&#38590;&#20197;&#20998;&#31163;&#25110;&#21306;&#20998;&#26469;&#33258;&#20960;&#20010;&#35828;&#35805;&#32773;&#30340;&#28151;&#21512;&#35821;&#38899;&#20013;&#30340;&#20010;&#21035;&#35828;&#35805;&#32773;&#30340;&#22330;&#26223;&#12290;&#22312;&#36825;&#20010;&#39046;&#22495;&#24050;&#32463;&#36827;&#34892;&#20102;&#20960;&#39033;&#30740;&#31350;&#65292;&#20294;&#27169;&#22411;&#30340;&#22823;&#23567;&#21644;&#22797;&#26434;&#24615;&#27491;&#22312;&#19982;&#35821;&#38899;&#20998;&#31163;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#36827;&#34892;&#26435;&#34913;&#12290;"&#21333;&#22768;&#36947;&#22810;&#20154;&#28436;&#35762;&#20998;&#31163;"&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#26550;&#26500;&#21450;&#20854;&#39640;&#25928;&#24418;&#24335;&#30340;&#28436;&#35762;&#20998;&#31163;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#20351;&#29992;&#21253;&#21547;&#22810;&#26679;&#21270;&#35828;&#35805;&#32773;&#35805;&#35821;&#30340;LibriMix&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#20174;&#28151;&#21512;&#38899;&#39057;&#36755;&#20837;&#20013;&#20998;&#31163;&#20986;2&#20010;&#19981;&#21516;&#30340;&#35828;&#35805;&#32773;&#28304;&#12290;&#35813;&#27169;&#22411;&#30340;&#24320;&#21457;&#30446;&#26631;&#26159;&#20943;&#23569;&#35821;&#38899;&#20998;&#31163;&#27169;&#22411;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#24182;&#22312;&#19982;&#29616;&#26377;&#35821;&#38899;&#20998;&#31163;&#27169;&#22411;&#24615;&#33021;&#26368;&#23567;&#21270;&#26435;&#34913;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#35813;&#39033;&#30446;&#39044;&#35745;&#23558;&#20026;&#35821;&#38899;&#20998;&#31163;&#39046;&#22495;&#30340;&#25345;&#32493;&#30740;&#31350;&#20570;&#20986;&#36129;&#29486;&#65292;&#21516;&#26102;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cocktail party problem is the scenario where it is difficult to separate or distinguish individual speaker from a mixed speech from several speakers. There have been several researches going on in this field but the size and complexity of the model is being traded off with the accuracy and robustness of speech separation. "Monaural multi-speaker speech separation" presents a speech-separation model based on the Transformer architecture and its efficient forms. The model has been trained with the LibriMix dataset containing diverse speakers' utterances. The model separates 2 distinct speaker sources from a mixed audio input. The developed model approaches the reduction in computational complexity of the speech separation model, with minimum tradeoff with the performance of prevalent speech separation model and it has shown significant movement towards that goal. This project foresees, a rise in contribution towards the ongoing research in the field of speech separation with computationa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;3D&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#30452;&#25509;&#20998;&#31867;&#20896;&#24515;&#30149;&#24739;&#32773;&#21644;&#27491;&#24120;&#21463;&#35797;&#32773;&#65292;&#30456;&#36739;&#20110;2D&#27169;&#22411;&#25552;&#39640;&#20102;23.65%&#30340;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;Grad-GAM&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#24615;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#19982;2D&#35821;&#20041;&#20998;&#21106;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#35299;&#37322;&#24615;&#21644;&#20934;&#30830;&#30340;&#24322;&#24120;&#23450;&#20301;&#12290;</title><link>http://arxiv.org/abs/2308.00009</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#20896;&#24515;&#30149;&#30340;3D&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#22120;&#21450;&#20854;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
A 3D deep learning classifier and its explainability when assessing coronary artery disease. (arXiv:2308.00009v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00009
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;3D&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#30452;&#25509;&#20998;&#31867;&#20896;&#24515;&#30149;&#24739;&#32773;&#21644;&#27491;&#24120;&#21463;&#35797;&#32773;&#65292;&#30456;&#36739;&#20110;2D&#27169;&#22411;&#25552;&#39640;&#20102;23.65%&#30340;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;Grad-GAM&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#24615;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#19982;2D&#35821;&#20041;&#20998;&#21106;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#35299;&#37322;&#24615;&#21644;&#20934;&#30830;&#30340;&#24322;&#24120;&#23450;&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26089;&#26399;&#21457;&#29616;&#21644;&#35786;&#26029;&#20896;&#24515;&#30149;&#65288;CAD&#65289;&#21487;&#25405;&#25937;&#29983;&#21629;&#24182;&#38477;&#20302;&#21307;&#30103;&#25104;&#26412;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;3D Resnet-50&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#30452;&#25509;&#23545;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#20896;&#29366;&#21160;&#33033;&#36896;&#24433;&#22270;&#20687;&#19978;&#30340;&#27491;&#24120;&#21463;&#35797;&#32773;&#21644;&#20896;&#24515;&#30149;&#24739;&#32773;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;2D Resnet-50&#27169;&#22411;&#25552;&#39640;&#20102;23.65%&#12290;&#36890;&#36807;&#20351;&#29992;Grad-GAM&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;3D&#20896;&#24515;&#30149;&#20998;&#31867;&#19982;2D&#20108;&#31867;&#35821;&#20041;&#20998;&#21106;&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#35299;&#37322;&#24615;&#21644;&#20934;&#30830;&#30340;&#24322;&#24120;&#23450;&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;
Early detection and diagnosis of coronary artery disease (CAD) could save lives and reduce healthcare costs. In this study, we propose a 3D Resnet-50 deep learning model to directly classify normal subjects and CAD patients on computed tomography coronary angiography images. Our proposed method outperforms a 2D Resnet-50 model by 23.65%. Explainability is also provided by using a Grad-GAM. Furthermore, we link the 3D CAD classification to a 2D two-class semantic segmentation for improved explainability and accurate abnormality localisation.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#26469;&#36827;&#34892;&#27668;&#36947;&#20998;&#21106;&#65292;&#36890;&#36807;&#25554;&#20540;&#21644;&#22270;&#20687;&#20998;&#21106;&#25552;&#39640;&#25968;&#25454;&#36136;&#37327;&#65292;&#37319;&#29992;&#38598;&#25104;&#23398;&#20064;&#31574;&#30053;&#22312;&#22810;&#20010;&#23610;&#24230;&#19978;&#32858;&#21512;&#27668;&#36947;&#26641;&#65292;&#24615;&#33021;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#65292;&#36866;&#29992;&#20110;&#20219;&#20309;2D&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.00008</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#27668;&#36947;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
A data-centric deep learning approach to airway segmentation. (arXiv:2308.00008v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00008
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#26469;&#36827;&#34892;&#27668;&#36947;&#20998;&#21106;&#65292;&#36890;&#36807;&#25554;&#20540;&#21644;&#22270;&#20687;&#20998;&#21106;&#25552;&#39640;&#25968;&#25454;&#36136;&#37327;&#65292;&#37319;&#29992;&#38598;&#25104;&#23398;&#20064;&#31574;&#30053;&#22312;&#22810;&#20010;&#23610;&#24230;&#19978;&#32858;&#21512;&#27668;&#36947;&#26641;&#65292;&#24615;&#33021;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#65292;&#36866;&#29992;&#20110;&#20219;&#20309;2D&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27668;&#36947;&#26641;&#24322;&#24120;&#30340;&#24418;&#24577;&#21644;&#20998;&#24067;&#21487;&#20197;&#29992;&#20110;&#35786;&#26029;&#21644;&#30142;&#30149;&#34920;&#24449;&#21508;&#31181;&#24930;&#24615;&#21628;&#21560;&#29366;&#20917;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#27668;&#36947;&#20998;&#21106;&#22312;&#29983;&#25104;&#25972;&#20010;&#27668;&#36947;&#26641;&#36718;&#24275;&#20197;&#20272;&#35745;&#30142;&#30149;&#33539;&#22260;&#21644;&#20005;&#37325;&#31243;&#24230;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#26469;&#20998;&#21106;&#27668;&#36947;&#26641;&#12290;&#25152;&#25552;&#20986;&#30340;&#25216;&#26415;&#21033;&#29992;&#25554;&#20540;&#21644;&#22270;&#20687;&#20998;&#21106;&#26469;&#25552;&#39640;&#25968;&#25454;&#30340;&#26377;&#29992;&#24615;&#21644;&#36136;&#37327;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23454;&#26045;&#20102;&#19968;&#20010;&#38598;&#25104;&#23398;&#20064;&#31574;&#30053;&#26469;&#32858;&#21512;&#19981;&#21516;&#23610;&#24230;&#19979;&#30340;&#20998;&#21106;&#30340;&#27668;&#36947;&#26641;&#12290;&#22312;&#20998;&#21106;&#24615;&#33021;&#26041;&#38754;&#65288;Dice&#30456;&#20284;&#31995;&#25968;&#65289;&#65292;&#24403;&#20351;&#29992;&#32452;&#21512;&#25439;&#22833;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#24179;&#22343;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;2.5%&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#25216;&#26415;&#20351;&#29992;GPU&#36739;&#23569;&#65292;&#28789;&#27963;&#24615;&#39640;&#65292;&#21487;&#20197;&#37096;&#32626;&#22312;&#20219;&#20309;2D&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
The morphology and distribution of airway tree abnormalities enables diagnosis and disease characterisation across a variety of chronic respiratory conditions. In this regard, airway segmentation plays a critical role in the production of the outline of the entire airway tree to enable estimation of disease extent and severity. In this study, we propose a data-centric deep learning technique to segment the airway tree. The proposed technique utilises interpolation and image split to improve data usefulness and quality. Then, an ensemble learning strategy is implemented to aggregate the segmented airway trees at different scales. In terms of segmentation performance (dice similarity coefficient), our method outperforms the baseline model by 2.5% on average when a combined loss is used. Further, our proposed technique has a low GPU usage and high flexibility enabling it to be deployed on any 2D deep learning model.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#26102;&#38388;&#24120;&#35782;&#25512;&#29702;&#39046;&#22495;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#37325;&#28857;&#20851;&#27880;&#36890;&#36807;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#26469;&#25552;&#39640;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#23545;&#22810;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22686;&#24378;&#27169;&#22411;&#20173;&#28982;&#38590;&#20197;&#36798;&#21040;&#20154;&#31867;&#27700;&#24179;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.00002</link><description>&lt;p&gt;
&#26102;&#38388;&#24120;&#35782;&#25512;&#29702;&#19982;&#33719;&#21462;&#27010;&#36848;
&lt;/p&gt;
&lt;p&gt;
An Overview Of Temporal Commonsense Reasoning and Acquisition. (arXiv:2308.00002v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00002
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#26102;&#38388;&#24120;&#35782;&#25512;&#29702;&#39046;&#22495;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#37325;&#28857;&#20851;&#27880;&#36890;&#36807;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#26469;&#25552;&#39640;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#23545;&#22810;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22686;&#24378;&#27169;&#22411;&#20173;&#28982;&#38590;&#20197;&#36798;&#21040;&#20154;&#31867;&#27700;&#24179;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24120;&#35782;&#25512;&#29702;&#26159;&#25351;&#29702;&#35299;&#30701;&#35821;&#12289;&#21160;&#20316;&#21644;&#20107;&#20214;&#30340;&#20856;&#22411;&#26102;&#38388;&#32972;&#26223;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#38656;&#35201;&#36825;&#31181;&#30693;&#35782;&#30340;&#38382;&#39064;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#36825;&#31181;&#33021;&#21147;&#22312;&#26102;&#38388;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#21487;&#33021;&#24212;&#29992;&#20110;&#26102;&#38388;&#32447;&#25688;&#35201;&#12289;&#26102;&#38388;&#38382;&#31572;&#21644;&#26102;&#38388;&#33258;&#28982;&#35821;&#35328;&#25512;&#26029;&#31561;&#26041;&#38754;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34429;&#28982;&#21892;&#20110;&#29983;&#25104;&#35821;&#27861;&#27491;&#30830;&#30340;&#21477;&#23376;&#21644;&#35299;&#20915;&#20998;&#31867;&#20219;&#21153;&#65292;&#20294;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#24448;&#24448;&#20250;&#37319;&#21462;&#25463;&#24452;&#65292;&#24182;&#38519;&#20837;&#31616;&#21333;&#30340;&#35821;&#35328;&#38519;&#38449;&#12290;&#26412;&#25991;&#31456;&#27010;&#36848;&#20102;&#22312;&#26102;&#38388;&#24120;&#35782;&#25512;&#29702;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#29305;&#21035;&#20851;&#27880;&#36890;&#36807;&#21508;&#31181;&#22686;&#24378;&#26041;&#24335;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#20197;&#21450;&#23545;&#36234;&#26469;&#36234;&#22810;&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22686;&#24378;&#27169;&#22411;&#22312;&#25512;&#29702;&#20219;&#21153;&#19978;&#20173;&#28982;&#38590;&#20197;&#36798;&#21040;&#20154;&#31867;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal commonsense reasoning refers to the ability to understand the typical temporal context of phrases, actions, and events, and use it to reason over problems requiring such knowledge. This trait is essential in temporal natural language processing tasks, with possible applications such as timeline summarization, temporal question answering, and temporal natural language inference. Recent research on the performance of large language models suggests that, although they are adept at generating syntactically correct sentences and solving classification tasks, they often take shortcuts in their reasoning and fall prey to simple linguistic traps. This article provides an overview of research in the domain of temporal commonsense reasoning, particularly focusing on enhancing language model performance through a variety of augmentations and their evaluation across a growing number of datasets. However, these augmented models still struggle to approach human performance on reasoning task
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38750;&#21018;&#24615;&#25569;&#23039;&#27880;&#20876;&#26469;&#23398;&#20064;&#36890;&#29992;&#24037;&#20855;&#20351;&#29992;&#34892;&#20026;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#21333;&#20010;&#31034;&#33539;&#21487;&#25193;&#23637;&#22320;&#23398;&#20064;&#26032;&#31867;&#21035;&#20013;&#30340;&#24037;&#20855;&#25805;&#20316;&#65292;&#24182;&#36890;&#36807;&#24341;&#23548;&#31574;&#30053;&#25628;&#32034;&#35299;&#20915;&#22797;&#26434;&#30340;&#24037;&#20855;&#20351;&#29992;&#20219;&#21153;&#65292;&#24182;&#22312;&#27979;&#35797;&#26102;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2307.16499</link><description>&lt;p&gt;
&#23398;&#20064;&#20855;&#26377;&#38750;&#21018;&#24615;&#25569;&#23039;&#27880;&#20876;&#30340;&#36890;&#29992;&#24037;&#20855;&#20351;&#29992;
&lt;/p&gt;
&lt;p&gt;
Learning Generalizable Tool Use with Non-rigid Grasp-pose Registration. (arXiv:2307.16499v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16499
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38750;&#21018;&#24615;&#25569;&#23039;&#27880;&#20876;&#26469;&#23398;&#20064;&#36890;&#29992;&#24037;&#20855;&#20351;&#29992;&#34892;&#20026;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#21333;&#20010;&#31034;&#33539;&#21487;&#25193;&#23637;&#22320;&#23398;&#20064;&#26032;&#31867;&#21035;&#20013;&#30340;&#24037;&#20855;&#25805;&#20316;&#65292;&#24182;&#36890;&#36807;&#24341;&#23548;&#31574;&#30053;&#25628;&#32034;&#35299;&#20915;&#22797;&#26434;&#30340;&#24037;&#20855;&#20351;&#29992;&#20219;&#21153;&#65292;&#24182;&#22312;&#27979;&#35797;&#26102;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#20855;&#20351;&#29992;&#26159;&#20154;&#31867;&#26234;&#33021;&#30340;&#19968;&#20010;&#26631;&#24535;&#24615;&#29305;&#24449;&#65292;&#30001;&#20110;&#22797;&#26434;&#30340;&#25509;&#35302;&#21644;&#39640;&#32500;&#21160;&#20316;&#31354;&#38388;&#65292;&#23427;&#20173;&#28982;&#26159;&#26426;&#22120;&#20154;&#39046;&#22495;&#20013;&#30340;&#19968;&#20010;&#25361;&#25112;&#24615;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#24037;&#20855;&#20351;&#29992;&#34892;&#20026;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#26041;&#24335;&#65292;&#21482;&#20351;&#29992;&#19968;&#20010;&#31034;&#33539;&#21363;&#21487;&#23398;&#20064;&#26032;&#31867;&#21035;&#20013;&#30340;&#24037;&#20855;&#25805;&#20316;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#22810;&#25351;&#26426;&#22120;&#20154;&#25163;&#30340;&#25235;&#21462;&#37197;&#32622;&#25512;&#24191;&#21040;&#26032;&#23545;&#35937;&#19978;&#12290;&#36890;&#36807;&#26377;&#21033;&#30340;&#21021;&#22987;&#21270;&#21644;&#24418;&#29366;&#22870;&#21169;&#20449;&#21495;&#65292;&#25105;&#20204;&#20351;&#29992;&#27492;&#26041;&#27861;&#26469;&#24341;&#23548;&#31574;&#30053;&#25628;&#32034;&#12290;&#23398;&#21040;&#30340;&#31574;&#30053;&#21487;&#20197;&#35299;&#20915;&#22797;&#26434;&#30340;&#24037;&#20855;&#20351;&#29992;&#20219;&#21153;&#65292;&#24182;&#19988;&#22312;&#27979;&#35797;&#26102;&#21487;&#20197;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#24037;&#20855;&#12290;&#35757;&#32451;&#36807;&#30340;&#31574;&#30053;&#30340;&#21487;&#35270;&#21270;&#21644;&#35270;&#39057;&#21487;&#22312;https://maltemosbach.github.io/generalizable_tool_use&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tool use, a hallmark feature of human intelligence, remains a challenging problem in robotics due the complex contacts and high-dimensional action space. In this work, we present a novel method to enable reinforcement learning of tool use behaviors. Our approach provides a scalable way to learn the operation of tools in a new category using only a single demonstration. To this end, we propose a new method for generalizing grasping configurations of multi-fingered robotic hands to novel objects. This is used to guide the policy search via favorable initializations and a shaped reward signal. The learned policies solve complex tool use tasks and generalize to unseen tools at test time. Visualizations and videos of the trained policies are available at https://maltemosbach.github.io/generalizable_tool_use.
&lt;/p&gt;</description></item><item><title>L3DMC&#26159;&#19968;&#31181;&#20351;&#29992;&#28151;&#21512;&#26354;&#29575;&#31354;&#38388;&#36827;&#34892;&#32456;&#36523;&#23398;&#20064;&#30340;&#33976;&#39311;&#31574;&#30053;&#65292;&#36890;&#36807;&#24314;&#27169;&#21644;&#32500;&#25252;&#22797;&#26434;&#20960;&#20309;&#32467;&#26500;&#26469;&#20445;&#30041;&#24050;&#32463;&#23398;&#21040;&#30340;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2307.16459</link><description>&lt;p&gt;
L3DMC: &#20351;&#29992;&#28151;&#21512;&#26354;&#29575;&#31354;&#38388;&#30340;&#33976;&#39311;&#36827;&#34892;&#32456;&#36523;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
L3DMC: Lifelong Learning using Distillation via Mixed-Curvature Space. (arXiv:2307.16459v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16459
&lt;/p&gt;
&lt;p&gt;
L3DMC&#26159;&#19968;&#31181;&#20351;&#29992;&#28151;&#21512;&#26354;&#29575;&#31354;&#38388;&#36827;&#34892;&#32456;&#36523;&#23398;&#20064;&#30340;&#33976;&#39311;&#31574;&#30053;&#65292;&#36890;&#36807;&#24314;&#27169;&#21644;&#32500;&#25252;&#22797;&#26434;&#20960;&#20309;&#32467;&#26500;&#26469;&#20445;&#30041;&#24050;&#32463;&#23398;&#21040;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#32456;&#36523;&#23398;&#20064;&#27169;&#22411;&#22312;&#19968;&#31995;&#21015;&#20219;&#21153;&#19978;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#20854;&#24615;&#33021;&#20250;&#19979;&#38477;&#65292;&#22240;&#20026;&#22312;&#39034;&#24207;&#23398;&#20064;&#26032;&#27010;&#24565;&#26102;&#23884;&#20837;&#31354;&#38388;&#30340;&#20960;&#20309;&#32467;&#26500;&#20250;&#21457;&#29983;&#21464;&#21270;&#12290;&#29616;&#26377;&#30340;&#32456;&#36523;&#23398;&#20064;&#26041;&#27861;&#22823;&#22810;&#22312;&#22266;&#23450;&#26354;&#29575;&#65288;&#20363;&#22914;&#38646;&#26354;&#29575;&#30340;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#65289;&#19978;&#36816;&#34892;&#65292;&#36825;&#24182;&#19981;&#36866;&#21512;&#24314;&#27169;&#22797;&#26434;&#30340;&#25968;&#25454;&#20960;&#20309;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;&#33976;&#39311;&#31574;&#30053;&#30452;&#25509;&#24212;&#29992;&#20110;&#20302;&#32500;&#23884;&#20837;&#65292;&#36890;&#36807;&#20351;&#27169;&#22411;&#39640;&#24230;&#31283;&#23450;&#26469;&#38459;&#30861;&#32456;&#36523;&#23398;&#20064;&#27169;&#22411;&#23398;&#20064;&#26032;&#27010;&#24565;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;L3DMC&#30340;&#33976;&#39311;&#31574;&#30053;&#65292;&#23427;&#22312;&#28151;&#21512;&#26354;&#29575;&#31354;&#38388;&#19978;&#25805;&#20316;&#65292;&#36890;&#36807;&#24314;&#27169;&#21644;&#32500;&#25252;&#22797;&#26434;&#30340;&#20960;&#20309;&#32467;&#26500;&#26469;&#20445;&#30041;&#24050;&#32463;&#23398;&#21040;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#27491;&#23450;&#30340;&#37325;&#26500;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHS&#65289;&#23558;&#22266;&#23450;&#26354;&#29575;&#31354;&#38388;&#65288;&#27431;&#20960;&#37324;&#24503;&#21644;&#21452;&#26354;&#65289;&#30340;&#25237;&#24433;&#20302;&#32500;&#23884;&#20837;&#21040;&#26356;&#39640;&#32500;&#24230;&#30340;&#31354;&#38388;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
The performance of a lifelong learning (L3) model degrades when it is trained on a series of tasks, as the geometrical formation of the embedding space changes while learning novel concepts sequentially. The majority of existing L3 approaches operate on a fixed-curvature (e.g., zero-curvature Euclidean) space that is not necessarily suitable for modeling the complex geometric structure of data. Furthermore, the distillation strategies apply constraints directly on low-dimensional embeddings, discouraging the L3 model from learning new concepts by making the model highly stable. To address the problem, we propose a distillation strategy named L3DMC that operates on mixed-curvature spaces to preserve the already-learned knowledge by modeling and maintaining complex geometrical structures. We propose to embed the projected low dimensional embedding of fixed-curvature spaces (Euclidean and hyperbolic) to higher-dimensional Reproducing Kernel Hilbert Space (RKHS) using a positive-definite k
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#65292;&#36890;&#36807;&#36817;&#20284;&#25968;&#25454;&#27969;&#24418;&#65292;&#24182;&#29992;&#32447;&#24615;&#23376;&#31354;&#38388;&#24314;&#27169;&#32467;&#26500;&#65292;&#26469;&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#20943;&#36731;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#22312;&#22810;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25345;&#32493;&#23398;&#20064;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2307.16419</link><description>&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#30340;&#23376;&#31354;&#38388;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Subspace Distillation for Continual Learning. (arXiv:2307.16419v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16419
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#65292;&#36890;&#36807;&#36817;&#20284;&#25968;&#25454;&#27969;&#24418;&#65292;&#24182;&#29992;&#32447;&#24615;&#23376;&#31354;&#38388;&#24314;&#27169;&#32467;&#26500;&#65292;&#26469;&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#20943;&#36731;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#22312;&#22810;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25345;&#32493;&#23398;&#20064;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#65292;&#19968;&#20010;&#26368;&#32456;&#30340;&#30446;&#26631;&#26159;&#20445;&#30041;&#22312;&#21069;&#38754;&#20219;&#21153;&#20013;&#23398;&#21040;&#30340;&#30693;&#35782;&#65292;&#21516;&#26102;&#23398;&#20064;&#26032;&#20219;&#21153;&#12290;&#20026;&#20102;&#20943;&#36731;&#23545;&#20808;&#21069;&#30693;&#35782;&#30340;&#36951;&#24536;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#32771;&#34385;&#20102;&#31070;&#32463;&#32593;&#32476;&#28508;&#22312;/&#36755;&#20986;&#31354;&#38388;&#30340;&#27969;&#24418;&#32467;&#26500;&#22312;&#23398;&#20064;&#26032;&#20219;&#21153;&#20013;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36817;&#20284;&#25968;&#25454;&#27969;&#24418;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#36890;&#36807;&#32447;&#24615;&#23376;&#31354;&#38388;&#26469;&#24314;&#27169;&#32467;&#26500;&#24182;&#20445;&#25345;&#31070;&#32463;&#32593;&#32476;&#22312;&#23398;&#20064;&#26032;&#27010;&#24565;&#26102;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20351;&#29992;&#23376;&#31354;&#38388;&#24314;&#27169;&#20855;&#26377;&#19968;&#20123;&#26377;&#36259;&#30340;&#29305;&#24615;&#65292;&#21253;&#25324;&#23545;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#65292;&#22240;&#27492;&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#21487;&#20197;&#26377;&#25928;&#22320;&#20943;&#36731;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22914;&#20309;&#36866;&#24212;&#20998;&#31867;&#21644;&#20998;&#21106;&#38382;&#39064;&#12290;&#32463;&#39564;&#19978;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20960;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#19978;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
An ultimate objective in continual learning is to preserve knowledge learned in preceding tasks while learning new tasks. To mitigate forgetting prior knowledge, we propose a novel knowledge distillation technique that takes into the account the manifold structure of the latent/output space of a neural network in learning novel tasks. To achieve this, we propose to approximate the data manifold up-to its first order, hence benefiting from linear subspaces to model the structure and maintain the knowledge of a neural network while learning novel concepts. We demonstrate that the modeling with subspaces provides several intriguing properties, including robustness to noise and therefore effective for mitigating Catastrophic Forgetting in continual learning. We also discuss and show how our proposed method can be adopted to address both classification and segmentation problems. Empirically, we observe that our proposed method outperforms various continual learning methods on several challe
&lt;/p&gt;</description></item><item><title>&#22312;&#22810;&#27169;&#24577;&#23454;&#20307;&#23545;&#40784;&#20013;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#24573;&#35270;&#20102;&#35270;&#35273;&#22270;&#20687;&#30340;&#19981;&#23436;&#25972;&#24615;&#21644;&#27169;&#31946;&#24615;&#65292;&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#34920;&#26126;&#27169;&#22411;&#22312;&#38754;&#23545;&#19981;&#23436;&#25972;&#24615;&#26102;&#23481;&#26131;&#20986;&#29616;&#36807;&#25311;&#21512;&#21644;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.16210</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#22810;&#27169;&#24577;&#23454;&#20307;&#23545;&#40784;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#32570;&#22833;&#21644;&#27169;&#31946;&#30340;&#35270;&#35273;&#27169;&#24577;
&lt;/p&gt;
&lt;p&gt;
Rethinking Uncertainly Missing and Ambiguous Visual Modality in Multi-Modal Entity Alignment. (arXiv:2307.16210v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16210
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#27169;&#24577;&#23454;&#20307;&#23545;&#40784;&#20013;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#24573;&#35270;&#20102;&#35270;&#35273;&#22270;&#20687;&#30340;&#19981;&#23436;&#25972;&#24615;&#21644;&#27169;&#31946;&#24615;&#65292;&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#34920;&#26126;&#27169;&#22411;&#22312;&#38754;&#23545;&#19981;&#23436;&#25972;&#24615;&#26102;&#23481;&#26131;&#20986;&#29616;&#36807;&#25311;&#21512;&#21644;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#23454;&#20307;&#23545;&#40784;&#65288;EA&#65289;&#30340;&#37325;&#35201;&#25193;&#23637;&#65292;&#22810;&#27169;&#24577;&#23454;&#20307;&#23545;&#40784;&#65288;MMEA&#65289;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#30456;&#20851;&#30340;&#35270;&#35273;&#20449;&#24687;&#26469;&#35782;&#21035;&#36328;&#19981;&#21516;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#20043;&#38388;&#30340;&#30456;&#21516;&#23454;&#20307;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;MMEA&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#22810;&#27169;&#24577;&#23454;&#20307;&#29305;&#24449;&#30340;&#34701;&#21512;&#33539;&#24335;&#19978;&#65292;&#32780;&#24573;&#35270;&#20102;&#32570;&#22833;&#21644;&#20869;&#22312;&#27169;&#31946;&#24615;&#30340;&#35270;&#35273;&#22270;&#20687;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#23545;&#35270;&#35273;&#27169;&#24577;&#19981;&#23436;&#25972;&#24615;&#36827;&#34892;&#20102;&#36827;&#19968;&#27493;&#20998;&#26512;&#65292;&#22312;&#25105;&#20204;&#25552;&#20986;&#30340;MMEA-UMVM&#25968;&#25454;&#38598;&#19978;&#23545;&#26368;&#26032;&#30340;MMEA&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#28085;&#30422;&#21452;&#35821;&#21644;&#21333;&#35821;&#23545;&#40784;KGs&#30340;&#31867;&#22411;&#65292;&#24182;&#37319;&#29992;&#26631;&#20934;&#65288;&#38750;&#36845;&#20195;&#65289;&#21644;&#36845;&#20195;&#35757;&#32451;&#33539;&#24335;&#26469;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#38754;&#23545;&#27169;&#24577;&#19981;&#23436;&#25972;&#24615;&#26102;&#65292;&#27169;&#22411;&#24456;&#23481;&#26131;&#36807;&#25311;&#21512;&#27169;&#24577;&#22122;&#22768;&#65292;&#24182;&#22312;&#39640;&#32570;&#22833;&#27169;&#24577;&#30340;&#24773;&#20917;&#19979;&#20986;&#29616;&#24615;&#33021;&#25391;&#33633;&#25110;&#19979;&#38477;&#12290;&#36825;&#35777;&#26126;&#20102;&#22686;&#21152;&#35270;&#35273;&#19981;&#30830;&#23450;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
As a crucial extension of entity alignment (EA), multi-modal entity alignment (MMEA) aims to identify identical entities across disparate knowledge graphs (KGs) by exploiting associated visual information. However, existing MMEA approaches primarily concentrate on the fusion paradigm of multi-modal entity features, while neglecting the challenges presented by the pervasive phenomenon of missing and intrinsic ambiguity of visual images. In this paper, we present a further analysis of visual modality incompleteness, benchmarking latest MMEA models on our proposed dataset MMEA-UMVM, where the types of alignment KGs covering bilingual and monolingual, with standard (non-iterative) and iterative training paradigms to evaluate the model performance. Our research indicates that, in the face of modality incompleteness, models succumb to overfitting the modality noise, and exhibit performance oscillations or declines at high rates of missing modality. This proves that the inclusion of additiona
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#23545;&#25239;&#24615;&#21518;&#38376;&#38450;&#24481;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#34987;&#27745;&#26579;&#26679;&#26412;&#20013;&#27880;&#20837;&#38750;&#23545;&#25239;&#24615;&#21518;&#38376;&#65292;&#24403;&#35302;&#21457;&#26102;&#21487;&#20197;&#25233;&#21046;&#25915;&#20987;&#32773;&#23545;&#27745;&#26579;&#25968;&#25454;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#21516;&#26102;&#20445;&#25345;&#23545;&#24178;&#20928;&#25968;&#25454;&#30340;&#24433;&#21709;&#26377;&#38480;&#12290;</title><link>http://arxiv.org/abs/2307.15539</link><description>&lt;p&gt;
&#38750;&#23545;&#25239;&#24615;&#21518;&#38376;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Backdoor Defense with Non-Adversarial Backdoor. (arXiv:2307.15539v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15539
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#23545;&#25239;&#24615;&#21518;&#38376;&#38450;&#24481;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#34987;&#27745;&#26579;&#26679;&#26412;&#20013;&#27880;&#20837;&#38750;&#23545;&#25239;&#24615;&#21518;&#38376;&#65292;&#24403;&#35302;&#21457;&#26102;&#21487;&#20197;&#25233;&#21046;&#25915;&#20987;&#32773;&#23545;&#27745;&#26579;&#25968;&#25454;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#21516;&#26102;&#20445;&#25345;&#23545;&#24178;&#20928;&#25968;&#25454;&#30340;&#24433;&#21709;&#26377;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#36825;&#31181;&#25915;&#20987;&#24182;&#19981;&#20250;&#24433;&#21709;&#32593;&#32476;&#23545;&#24178;&#20928;&#25968;&#25454;&#30340;&#24615;&#33021;&#65292;&#20294;&#19968;&#26086;&#28155;&#21152;&#35302;&#21457;&#27169;&#24335;&#65292;&#23601;&#20250;&#25805;&#32437;&#32593;&#32476;&#34892;&#20026;&#12290;&#29616;&#26377;&#30340;&#38450;&#24481;&#26041;&#27861;&#22823;&#22823;&#38477;&#20302;&#20102;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#20294;&#23427;&#20204;&#22312;&#24178;&#20928;&#25968;&#25454;&#19978;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#20173;&#28982;&#36828;&#36828;&#33853;&#21518;&#20110;&#24178;&#20928;&#27169;&#22411;&#12290;&#21463;&#21518;&#38376;&#25915;&#20987;&#30340;&#38544;&#34109;&#24615;&#21644;&#26377;&#25928;&#24615;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#38750;&#24120;&#26377;&#25928;&#30340;&#38450;&#24481;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#27880;&#20837;&#20102;&#38024;&#23545;&#34987;&#27745;&#26579;&#26679;&#26412;&#30340;&#38750;&#23545;&#25239;&#24615;&#21518;&#38376;&#12290;&#25353;&#29031;&#21518;&#38376;&#25915;&#20987;&#30340;&#19968;&#33324;&#27493;&#39588;&#65292;&#25105;&#20204;&#26816;&#27979;&#19968;&#23567;&#32452;&#21487;&#30097;&#26679;&#26412;&#65292;&#28982;&#21518;&#23545;&#23427;&#20204;&#24212;&#29992;&#27602;&#21270;&#31574;&#30053;&#12290;&#19968;&#26086;&#35302;&#21457;&#65292;&#38750;&#23545;&#25239;&#24615;&#21518;&#38376;&#25233;&#21046;&#20102;&#25915;&#20987;&#32773;&#23545;&#27745;&#26579;&#25968;&#25454;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#20294;&#23545;&#24178;&#20928;&#25968;&#25454;&#30340;&#24433;&#21709;&#26377;&#38480;&#12290;&#38450;&#24481;&#21487;&#20197;&#22312;&#25968;&#25454;&#39044;&#22788;&#29702;&#26399;&#38388;&#36827;&#34892;&#65292;&#32780;&#19981;&#38656;&#35201;&#23545;&#26631;&#20934;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#27969;&#31243;&#36827;&#34892;&#20219;&#20309;&#20462;&#25913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) are vulnerable to backdoor attack, which does not affect the network's performance on clean data but would manipulate the network behavior once a trigger pattern is added. Existing defense methods have greatly reduced attack success rate, but their prediction accuracy on clean data still lags behind a clean model by a large margin. Inspired by the stealthiness and effectiveness of backdoor attack, we propose a simple but highly effective defense framework which injects non-adversarial backdoors targeting poisoned samples. Following the general steps in backdoor attack, we detect a small set of suspected samples and then apply a poisoning strategy to them. The non-adversarial backdoor, once triggered, suppresses the attacker's backdoor on poisoned data, but has limited influence on clean data. The defense can be carried out during data preprocessing, without any modification to the standard end-to-end training pipeline. We conduct extensive experiments on mul
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#26368;&#23567;&#33539;&#25968;&#30340;&#20004;&#23618;ReLU&#32593;&#32476;&#36827;&#34892;&#22122;&#22768;&#21333;&#21464;&#37327;&#22238;&#24402;&#30340;&#25554;&#20540;&#65292;&#23545;&#20110;$L_1$&#25439;&#22833;&#21644;$ p &lt;2 $&#30340;$L_p$&#25439;&#22833;&#25233;&#21046;&#36807;&#25311;&#21512;&#65292;&#20294;&#23545;&#20110;$ p \geq 2 $&#30340;&#25439;&#22833;&#26159;&#28798;&#38590;&#24615;&#30340;&#12290;</title><link>http://arxiv.org/abs/2307.15396</link><description>&lt;p&gt;
&#22522;&#20110;&#27973;&#23618;&#21333;&#21464;&#37327;ReLU&#32593;&#32476;&#30340;&#22122;&#22768;&#25554;&#20540;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Noisy Interpolation Learning with Shallow Univariate ReLU Networks. (arXiv:2307.15396v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15396
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26368;&#23567;&#33539;&#25968;&#30340;&#20004;&#23618;ReLU&#32593;&#32476;&#36827;&#34892;&#22122;&#22768;&#21333;&#21464;&#37327;&#22238;&#24402;&#30340;&#25554;&#20540;&#65292;&#23545;&#20110;$L_1$&#25439;&#22833;&#21644;$ p &lt;2 $&#30340;$L_p$&#25439;&#22833;&#25233;&#21046;&#36807;&#25311;&#21512;&#65292;&#20294;&#23545;&#20110;$ p \geq 2 $&#30340;&#25439;&#22833;&#26159;&#28798;&#38590;&#24615;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22122;&#22768;&#21333;&#21464;&#37327;&#22238;&#24402;&#20013;&#20351;&#29992;&#26368;&#23567;&#33539;&#25968;&#65288;&#26435;&#37325;&#30340;$\ell_2$&#33539;&#25968;&#65289;&#30340;&#20004;&#23618;ReLU&#32593;&#32476;&#36827;&#34892;&#25554;&#20540;&#30340;&#28176;&#36817;&#36807;&#25311;&#21512;&#34892;&#20026;&#12290;&#25105;&#20204;&#21457;&#29616;&#23545;&#20110;$L_1$&#25439;&#22833;&#21644;$ p &lt;2 $&#30340;&#20219;&#20309;$L_p$&#25439;&#22833;&#65292;&#36807;&#25311;&#21512;&#29616;&#35937;&#20250;&#34987;&#25233;&#21046;&#65292;&#20294;&#23545;&#20110;$ p \geq 2 $&#30340;&#25439;&#22833;&#26159;&#28798;&#38590;&#24615;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the asymptotic overfitting behavior of interpolation with minimum norm ($\ell_2$ of the weights) two-layer ReLU networks for noisy univariate regression. We show that overfitting is tempered for the $L_1$ loss, and any $L_p$ loss for $p&lt;2$, but catastrophic for $p\geq 2$.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#21487;&#22312;&#20998;&#24067;&#24335;&#29615;&#22659;&#20013;&#23454;&#29616;&#32447;&#24615;&#21152;&#36895;&#65292;&#20197;&#35299;&#20915;&#32452;&#21512;&#26497;&#23567;&#21270;&#38382;&#39064;&#20013;&#20869;&#23618;&#20989;&#25968;&#30340;&#20849;&#35782;&#35823;&#24046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.13430</link><description>&lt;p&gt;
&#22312;&#20998;&#24067;&#24335;&#38543;&#26426;&#32452;&#21512;&#26497;&#23567;&#21270;&#20248;&#21270;&#20013;&#23454;&#29616;&#32447;&#24615;&#21152;&#36895;
&lt;/p&gt;
&lt;p&gt;
Achieving Linear Speedup in Decentralized Stochastic Compositional Minimax Optimization. (arXiv:2307.13430v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13430
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#21487;&#22312;&#20998;&#24067;&#24335;&#29615;&#22659;&#20013;&#23454;&#29616;&#32447;&#24615;&#21152;&#36895;&#65292;&#20197;&#35299;&#20915;&#32452;&#21512;&#26497;&#23567;&#21270;&#38382;&#39064;&#20013;&#20869;&#23618;&#20989;&#25968;&#30340;&#20849;&#35782;&#35823;&#24046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#38543;&#26426;&#32452;&#21512;&#26497;&#23567;&#21270;&#38382;&#39064;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#28085;&#30422;&#20102;&#35768;&#22810;&#26032;&#20852;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#30001;&#20110;&#20998;&#24067;&#24335;&#25968;&#25454;&#30340;&#20986;&#29616;&#65292;&#38656;&#35201;&#22312;&#20998;&#25955;&#35774;&#32622;&#19979;&#20248;&#21270;&#36825;&#31181;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#25439;&#22833;&#20989;&#25968;&#20013;&#30340;&#32452;&#21512;&#32467;&#26500;&#32473;&#35774;&#35745;&#39640;&#25928;&#30340;&#20998;&#25955;&#24335;&#20248;&#21270;&#31639;&#27861;&#24102;&#26469;&#20102;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#30001;&#20110;&#23545;&#20869;&#23618;&#20989;&#25968;&#30340;&#20849;&#35782;&#35823;&#24046;&#36739;&#22823;&#65292;&#26631;&#20934;&#30340;&#20256;&#36882;&#31574;&#30053;&#26080;&#27861;&#22312;&#20998;&#25955;&#24335;&#32452;&#21512;&#26497;&#23567;&#21270;&#38382;&#39064;&#20013;&#23454;&#29616;&#32447;&#24615;&#21152;&#36895;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24102;&#26377;&#21160;&#37327;&#30340;&#20998;&#25955;&#24335;&#38543;&#26426;&#32452;&#21512;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#20197;&#20943;&#23567;&#20869;&#23618;&#20989;&#25968;&#30340;&#20849;&#35782;&#35823;&#24046;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#34920;&#26126;&#65292;&#23427;&#21487;&#20197;&#23454;&#29616;&#19982;&#24037;&#20316;&#32773;&#25968;&#37327;&#25104;&#32447;&#24615;&#21152;&#36895;&#12290;&#25105;&#20204;&#30456;&#20449;&#36825;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#33021;&#22815;&#22312;&#20998;&#24067;&#24335;&#29615;&#22659;&#20013;&#20026;&#32452;&#21512;&#26497;&#23567;&#21270;&#38382;&#39064;&#25552;&#20379;&#32447;&#24615;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;
The stochastic compositional minimax problem has attracted a surge of attention in recent years since it covers many emerging machine learning models. Meanwhile, due to the emergence of distributed data, optimizing this kind of problem under the decentralized setting becomes badly needed. However, the compositional structure in the loss function brings unique challenges to designing efficient decentralized optimization algorithms. In particular, our study shows that the standard gossip communication strategy cannot achieve linear speedup for decentralized compositional minimax problems due to the large consensus error about the inner-level function. To address this issue, we developed a novel decentralized stochastic compositional gradient descent ascent with momentum algorithm to reduce the consensus error in the inner-level function. As such, our theoretical results demonstrate that it is able to achieve linear speedup with respect to the number of workers. We believe this novel algo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#24320;&#25918;&#38134;&#34892;&#20316;&#20026;&#20363;&#23376;&#65292;&#20998;&#26512;&#20102;&#22823;&#25968;&#25454;&#21644;&#24378;&#22823;&#30340;&#20449;&#24687;&#25216;&#26415;&#65288;&#22914;&#26426;&#22120;&#23398;&#20064;&#65289;&#25152;&#24102;&#26469;&#30340;&#20844;&#24179;&#24615;&#38544;&#24739;&#12290;&#36890;&#36807;&#30740;&#31350;&#37329;&#34701;&#33030;&#24369;&#24615;&#30340;&#32500;&#24230;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#32454;&#33268;&#20132;&#26131;&#25968;&#25454;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#37266;&#23545;&#20854;&#24212;&#24403;&#35880;&#24910;&#20351;&#29992;&#20197;&#36991;&#20813;&#28508;&#22312;&#30340;&#27495;&#35270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.13408</link><description>&lt;p&gt;
&#22823;&#25968;&#25454;&#21644;&#20449;&#24687;&#25216;&#26415;&#23545;&#24369;&#21183;&#32676;&#20307;&#30340;&#21452;&#20995;&#21073;&#65306;&#24320;&#25918;&#38134;&#34892;&#30340;&#19968;&#20010;&#35686;&#31034;&#25925;&#20107;
&lt;/p&gt;
&lt;p&gt;
The Double-Edged Sword of Big Data and Information Technology for the Disadvantaged: A Cautionary Tale from Open Banking. (arXiv:2307.13408v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13408
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#24320;&#25918;&#38134;&#34892;&#20316;&#20026;&#20363;&#23376;&#65292;&#20998;&#26512;&#20102;&#22823;&#25968;&#25454;&#21644;&#24378;&#22823;&#30340;&#20449;&#24687;&#25216;&#26415;&#65288;&#22914;&#26426;&#22120;&#23398;&#20064;&#65289;&#25152;&#24102;&#26469;&#30340;&#20844;&#24179;&#24615;&#38544;&#24739;&#12290;&#36890;&#36807;&#30740;&#31350;&#37329;&#34701;&#33030;&#24369;&#24615;&#30340;&#32500;&#24230;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#32454;&#33268;&#20132;&#26131;&#25968;&#25454;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#37266;&#23545;&#20854;&#24212;&#24403;&#35880;&#24910;&#20351;&#29992;&#20197;&#36991;&#20813;&#28508;&#22312;&#30340;&#27495;&#35270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#21644;&#23637;&#31034;&#20102;&#30475;&#20284;&#20013;&#31435;&#30340;&#25968;&#25454;&#21644;&#24378;&#22823;&#30340;&#25216;&#26415;&#65288;&#22914;&#26426;&#22120;&#23398;&#20064;&#65289;&#22312;&#24320;&#25918;&#38134;&#34892;&#31561;&#39046;&#22495;&#23545;&#20844;&#24179;&#24615;&#30340;&#38544;&#34255;&#24433;&#21709;&#12290;&#24320;&#25918;&#38134;&#34892;&#22312;&#37329;&#34701;&#26381;&#21153;&#39046;&#22495;&#24341;&#21457;&#20102;&#19968;&#22330;&#38761;&#21629;&#65292;&#20026;&#23458;&#25143;&#33719;&#21462;&#12289;&#31649;&#29702;&#12289;&#20445;&#30041;&#21644;&#39118;&#38505;&#35780;&#20272;&#25171;&#24320;&#20102;&#26032;&#30340;&#26426;&#20250;&#12290;&#28982;&#32780;&#65292;&#20132;&#26131;&#25968;&#25454;&#30340;&#32454;&#33268;&#31243;&#24230;&#21487;&#33021;&#20250;&#24102;&#26469;&#28508;&#22312;&#30340;&#21361;&#23475;&#65292;&#26410;&#34987;&#27880;&#24847;&#30340;&#25935;&#24863;&#21644;&#31105;&#27490;&#24615;&#29305;&#24449;&#30340;&#20195;&#29702;&#21487;&#33021;&#23548;&#33268;&#38388;&#25509;&#27495;&#35270;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#36890;&#36807;&#20844;&#24179;&#35299;&#37322;&#30340;&#35270;&#35282;&#65292;&#30740;&#31350;&#20102;&#37329;&#34701;&#33030;&#24369;&#24615;&#65288;FV&#65289;&#30340;&#32500;&#24230;&#65292;&#36825;&#26159;COVID-19&#21644;&#36890;&#32960;&#19978;&#21319;&#24102;&#26469;&#30340;&#20840;&#29699;&#20851;&#27880;&#28857;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35797;&#22270;&#20102;&#35299;&#23548;&#33268;FV&#30340;&#34892;&#20026;&#22240;&#32032;&#65292;&#20197;&#21450;&#23427;&#22914;&#20309;&#24433;&#21709;&#22788;&#20110;&#39118;&#38505;&#32676;&#20307;&#20013;&#30340;&#24369;&#21183;&#32676;&#20307;&#12290;&#20351;&#29992;&#26469;&#33258;&#33521;&#22269;&#19968;&#23478;&#37329;&#34701;&#31185;&#25216;&#20511;&#36151;&#20844;&#21496;&#30340;&#29420;&#29305;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#32454;&#31890;&#24230;&#20132;&#26131;&#25968;&#25454;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#20063;&#23545;&#20854;&#25552;&#20986;&#20102;&#35686;&#21578;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research article analyses and demonstrates the hidden implications for fairness of seemingly neutral data coupled with powerful technology, such as machine learning (ML), using Open Banking as an example. Open Banking has ignited a revolution in financial services, opening new opportunities for customer acquisition, management, retention, and risk assessment. However, the granularity of transaction data holds potential for harm where unnoticed proxies for sensitive and prohibited characteristics may lead to indirect discrimination. Against this backdrop, we investigate the dimensions of financial vulnerability (FV), a global concern resulting from COVID-19 and rising inflation. Specifically, we look to understand the behavioral elements leading up to FV and its impact on at-risk, disadvantaged groups through the lens of fair interpretation. Using a unique dataset from a UK FinTech lender, we demonstrate the power of fine-grained transaction data while simultaneously cautioning its
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#26080;&#20851;&#37197;&#26041;MARIO&#65292;&#29992;&#20110;&#25913;&#21892;&#22270;&#23545;&#27604;&#23398;&#20064;&#30340;OOD&#27867;&#21270;&#24615;&#33021;&#12290;MARIO&#24341;&#20837;&#20102;&#20449;&#24687;&#29942;&#39048;&#21407;&#21017;&#21644;&#19981;&#21464;&#24615;&#21407;&#21017;&#65292;&#26088;&#22312;&#33719;&#24471;&#20855;&#26377;&#20998;&#24067;&#20559;&#31227;&#40065;&#26834;&#24615;&#21644;&#19981;&#21464;&#24615;&#30340;&#22270;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2307.13055</link><description>&lt;p&gt;
MARIO: &#29992;&#20110;&#25913;&#21892;&#22270;&#23545;&#27604;&#23398;&#20064;&#30340;&#27169;&#22411;&#26080;&#20851;&#37197;&#26041;&#65292;&#25552;&#39640;OOD&#27867;&#21270;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
MARIO: Model Agnostic Recipe for Improving OOD Generalization of Graph Contrastive Learning. (arXiv:2307.13055v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13055
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#26080;&#20851;&#37197;&#26041;MARIO&#65292;&#29992;&#20110;&#25913;&#21892;&#22270;&#23545;&#27604;&#23398;&#20064;&#30340;OOD&#27867;&#21270;&#24615;&#33021;&#12290;MARIO&#24341;&#20837;&#20102;&#20449;&#24687;&#29942;&#39048;&#21407;&#21017;&#21644;&#19981;&#21464;&#24615;&#21407;&#21017;&#65292;&#26088;&#22312;&#33719;&#24471;&#20855;&#26377;&#20998;&#24067;&#20559;&#31227;&#40065;&#26834;&#24615;&#21644;&#19981;&#21464;&#24615;&#30340;&#22270;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22270;&#25968;&#25454;&#19978;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30340;&#22495;&#22806;&#27867;&#21270;&#38382;&#39064;&#12290;&#36825;&#31181;&#24773;&#20917;&#29305;&#21035;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#21363;&#20351;&#26377;&#26631;&#31614;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#20063;&#26174;&#31034;&#20986;&#23545;&#20998;&#24067;&#20559;&#31227;&#30340;&#25935;&#24863;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MARIO&#30340;&#27169;&#22411;&#26080;&#20851;&#37197;&#26041;&#65292;&#26088;&#22312;&#24320;&#21457;&#20855;&#26377;&#20998;&#24067;&#20559;&#31227;&#40065;&#26834;&#24615;&#30340;&#22270;&#23545;&#27604;&#26041;&#27861;&#65292;&#20811;&#26381;&#29616;&#26377;&#26694;&#26550;&#30340;&#23616;&#38480;&#24615;&#65306;(i)&#20449;&#24687;&#29942;&#39048;(IB)&#21407;&#21017;&#29992;&#20110;&#23454;&#29616;&#21487;&#27867;&#21270;&#30340;&#34920;&#31034;&#65292;(ii)&#19981;&#21464;&#24615;&#21407;&#21017;&#37319;&#29992;&#23545;&#25239;&#24615;&#25968;&#25454;&#22686;&#24378;&#26469;&#33719;&#24471;&#19981;&#21464;&#34920;&#31034;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#39033;&#30740;&#31350;OOD&#27867;&#21270;&#38382;&#39064;&#30340;&#24037;&#20316;
&lt;/p&gt;
&lt;p&gt;
In this work, we investigate the problem of out-of-distribution (OOD) generalization for unsupervised learning methods on graph data. This scenario is particularly challenging because graph neural networks (GNNs) have been shown to be sensitive to distributional shifts, even when labels are available. To address this challenge, we propose a \underline{M}odel-\underline{A}gnostic \underline{R}ecipe for \underline{I}mproving \underline{O}OD generalizability of unsupervised graph contrastive learning methods, which we refer to as MARIO. MARIO introduces two principles aimed at developing distributional-shift-robust graph contrastive methods to overcome the limitations of existing frameworks: (i) Information Bottleneck (IB) principle for achieving generalizable representations and (ii) Invariant principle that incorporates adversarial data augmentation to obtain invariant representations. To the best of our knowledge, this is the first work that investigates the OOD generalization problem 
&lt;/p&gt;</description></item><item><title>Jina Embeddings&#26159;&#19968;&#32452;&#39640;&#24615;&#33021;&#30340;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#65292;&#33021;&#22815;&#25429;&#25417;&#25991;&#26412;&#30340;&#35821;&#20041;&#26412;&#36136;&#12290;&#35813;&#35770;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;Jina Embeddings&#30340;&#24320;&#21457;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#24615;&#33021;&#35780;&#20272;&#39564;&#35777;&#20102;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.11224</link><description>&lt;p&gt;
Jina Embeddings:&#19968;&#31181;&#26032;&#39062;&#30340;&#39640;&#24615;&#33021;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Jina Embeddings: A Novel Set of High-Performance Sentence Embedding Models. (arXiv:2307.11224v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11224
&lt;/p&gt;
&lt;p&gt;
Jina Embeddings&#26159;&#19968;&#32452;&#39640;&#24615;&#33021;&#30340;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#65292;&#33021;&#22815;&#25429;&#25417;&#25991;&#26412;&#30340;&#35821;&#20041;&#26412;&#36136;&#12290;&#35813;&#35770;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;Jina Embeddings&#30340;&#24320;&#21457;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#24615;&#33021;&#35780;&#20272;&#39564;&#35777;&#20102;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Jina Embeddings&#30001;&#19968;&#32452;&#39640;&#24615;&#33021;&#30340;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#32452;&#25104;&#65292;&#33021;&#22815;&#23558;&#21508;&#31181;&#25991;&#26412;&#36755;&#20837;&#36716;&#21270;&#20026;&#25968;&#20540;&#34920;&#31034;&#65292;&#20174;&#32780;&#25429;&#25417;&#25991;&#26412;&#30340;&#35821;&#20041;&#26412;&#36136;&#12290;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#24182;&#38750;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#25991;&#26412;&#29983;&#25104;&#65292;&#20294;&#22312;&#23494;&#38598;&#26816;&#32034;&#21644;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#31561;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#26412;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;Jina Embeddings&#30340;&#24320;&#21457;&#36807;&#31243;&#65292;&#20174;&#21019;&#24314;&#39640;&#36136;&#37327;&#30340;&#25104;&#23545;&#21644;&#19977;&#20803;&#25968;&#25454;&#38598;&#24320;&#22987;&#12290;&#23427;&#24378;&#35843;&#20102;&#25968;&#25454;&#28165;&#29702;&#22312;&#25968;&#25454;&#38598;&#20934;&#22791;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#24182;&#23545;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#36827;&#34892;&#20102;&#28145;&#20837;&#25506;&#35752;&#65292;&#26368;&#21518;&#21033;&#29992;Massive Textual Embedding Benchmark&#65288;MTEB&#65289;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#24615;&#33021;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Jina Embeddings constitutes a set of high-performance sentence embedding models adept at translating various textual inputs into numerical representations, thereby capturing the semantic essence of the text. While these models are not exclusively designed for text generation, they excel in applications such as dense retrieval and semantic textual similarity. This paper details the development of Jina Embeddings, starting with the creation of a high-quality pairwise and triplet dataset. It underlines the crucial role of data cleaning in dataset preparation, gives in-depth insights into the model training process, and concludes with a comprehensive performance evaluation using the Massive Textual Embedding Benchmark (MTEB).
&lt;/p&gt;</description></item><item><title>&#20113;&#31995;&#32479;&#20013;&#30340;&#24615;&#33021;&#38382;&#39064;&#35782;&#21035;&#23384;&#22312;&#25361;&#25112;&#65292;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#29420;&#31435;&#20998;&#26512;&#27599;&#20010;&#25351;&#26631;&#30340;&#24322;&#24120;&#26080;&#27861;&#35299;&#20915;&#38382;&#39064;&#65292;&#38656;&#35201;&#32771;&#34385;&#25351;&#26631;&#20043;&#38388;&#30340;&#20851;&#32852;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.10869</link><description>&lt;p&gt;
&#20113;&#31995;&#32479;&#20013;&#30340;&#24615;&#33021;&#38382;&#39064;&#35782;&#21035;&#19982;&#20851;&#31995;-&#26102;&#38388;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Performance Issue Identification in Cloud Systems with Relational-Temporal Anomaly Detection. (arXiv:2307.10869v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10869
&lt;/p&gt;
&lt;p&gt;
&#20113;&#31995;&#32479;&#20013;&#30340;&#24615;&#33021;&#38382;&#39064;&#35782;&#21035;&#23384;&#22312;&#25361;&#25112;&#65292;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#29420;&#31435;&#20998;&#26512;&#27599;&#20010;&#25351;&#26631;&#30340;&#24322;&#24120;&#26080;&#27861;&#35299;&#20915;&#38382;&#39064;&#65292;&#38656;&#35201;&#32771;&#34385;&#25351;&#26631;&#20043;&#38388;&#30340;&#20851;&#32852;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24615;&#33021;&#38382;&#39064;&#22312;&#22823;&#35268;&#27169;&#20113;&#26381;&#21153;&#31995;&#32479;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#21487;&#33021;&#23548;&#33268;&#24040;&#39069;&#25910;&#20837;&#25439;&#22833;&#12290;&#20026;&#20102;&#30830;&#20445;&#21487;&#38752;&#30340;&#24615;&#33021;&#65292;&#20351;&#29992;&#26381;&#21153;&#30417;&#25511;&#25351;&#26631;&#20934;&#30830;&#22320;&#35782;&#21035;&#21644;&#23450;&#20301;&#36825;&#20123;&#38382;&#39064;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#37492;&#20110;&#29616;&#20195;&#20113;&#31995;&#32479;&#30340;&#22797;&#26434;&#24615;&#21644;&#35268;&#27169;&#65292;&#36825;&#39033;&#20219;&#21153;&#21487;&#33021;&#20250;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#24182;&#19988;&#21487;&#33021;&#38656;&#35201;&#36229;&#20986;&#20010;&#20154;&#33021;&#21147;&#30340;&#24191;&#27867;&#19987;&#19994;&#30693;&#35782;&#21644;&#36164;&#28304;&#12290;&#19968;&#20123;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#36807;&#20998;&#26512;&#27599;&#20010;&#25351;&#26631;&#29420;&#31435;&#22320;&#26816;&#27979;&#24322;&#24120;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#38590;&#20197;&#30001;&#24037;&#31243;&#24072;&#25163;&#21160;&#35786;&#26029;&#30340;&#21387;&#20498;&#24615;&#35686;&#25253;&#39118;&#26292;&#12290;&#20026;&#20102;&#36861;&#27714;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#19981;&#20165;&#24212;&#32771;&#34385;&#25351;&#26631;&#30340;&#26102;&#38388;&#27169;&#24335;&#65292;&#36824;&#24212;&#32771;&#34385;&#25351;&#26631;&#20043;&#38388;&#30340;&#20851;&#32852;&#24615;&#27169;&#24335;&#65292;&#36825;&#21487;&#20197;&#34987;&#24418;&#24335;&#21270;&#20026;&#22810;&#21464;&#37327;&#25351;&#26631;&#24322;&#24120;&#26816;&#27979;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#22312;&#26126;&#30830;&#25552;&#21462;&#36825;&#20004;&#31181;&#31867;&#22411;&#30340;&#29305;&#24449;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#12290;&#27492;&#22806;&#65292;&#23384;&#22312;&#19968;&#20123;&#26410;&#26631;&#35760;&#30340;&#24322;&#24120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Performance issues permeate large-scale cloud service systems, which can lead to huge revenue losses. To ensure reliable performance, it's essential to accurately identify and localize these issues using service monitoring metrics. Given the complexity and scale of modern cloud systems, this task can be challenging and may require extensive expertise and resources beyond the capacity of individual humans. Some existing methods tackle this problem by analyzing each metric independently to detect anomalies. However, this could incur overwhelming alert storms that are difficult for engineers to diagnose manually. To pursue better performance, not only the temporal patterns of metrics but also the correlation between metrics (i.e., relational patterns) should be considered, which can be formulated as a multivariate metrics anomaly detection problem. However, most of the studies fall short of extracting these two types of features explicitly. Moreover, there exist some unlabeled anomalies m
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#24378;&#21270;&#23398;&#20064;&#23545;&#22478;&#24066;&#22522;&#30784;&#35774;&#26045;&#30456;&#20114;&#20381;&#36182;&#32593;&#32476;&#20013;&#30340;&#33030;&#24369;&#33410;&#28857;&#36827;&#34892;&#20102;&#20934;&#30830;&#24314;&#27169;&#21644;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2307.09866</link><description>&lt;p&gt;
&#26816;&#27979;&#22478;&#24066;&#22522;&#30784;&#35774;&#26045;&#30456;&#20114;&#20381;&#36182;&#32593;&#32476;&#20013;&#30340;&#33030;&#24369;&#33410;&#28857;
&lt;/p&gt;
&lt;p&gt;
Detecting Vulnerable Nodes in Urban Infrastructure Interdependent Network. (arXiv:2307.09866v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09866
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#24378;&#21270;&#23398;&#20064;&#23545;&#22478;&#24066;&#22522;&#30784;&#35774;&#26045;&#30456;&#20114;&#20381;&#36182;&#32593;&#32476;&#20013;&#30340;&#33030;&#24369;&#33410;&#28857;&#36827;&#34892;&#20102;&#20934;&#30830;&#24314;&#27169;&#21644;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#21644;&#25551;&#36848;&#22478;&#24066;&#22522;&#30784;&#35774;&#26045;&#30340;&#33030;&#24369;&#24615;&#23545;&#25105;&#20204;&#20855;&#26377;&#37325;&#35201;&#20215;&#20540;&#65292;&#36825;&#20123;&#22522;&#30784;&#35774;&#26045;&#26159;&#22478;&#24066;&#27491;&#24120;&#36816;&#34892;&#25152;&#24517;&#38656;&#30340;&#24037;&#31243;&#35774;&#26045;&#65292;&#20197;&#32593;&#32476;&#30340;&#24418;&#24335;&#33258;&#28982;&#23384;&#22312;&#12290;&#28508;&#22312;&#30340;&#24212;&#29992;&#21253;&#25324;&#20445;&#25252;&#33030;&#24369;&#35774;&#26045;&#21644;&#35774;&#35745;&#31283;&#20581;&#30340;&#25299;&#25169;&#32467;&#26500;&#31561;&#12290;&#30001;&#20110;&#19981;&#21516;&#25299;&#25169;&#29305;&#24615;&#21644;&#22522;&#30784;&#35774;&#26045;&#33030;&#24369;&#24615;&#20197;&#21450;&#20854;&#22797;&#26434;&#30340;&#28436;&#21270;&#26426;&#21046;&#20043;&#38388;&#30340;&#24378;&#20851;&#32852;&#65292;&#19968;&#20123;&#21551;&#21457;&#24335;&#20998;&#26512;&#21644;&#26426;&#22120;&#36741;&#21161;&#20998;&#26512;&#22312;&#35299;&#20915;&#36825;&#31181;&#22330;&#26223;&#26102;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#30456;&#20114;&#20381;&#36182;&#32593;&#32476;&#24314;&#27169;&#20026;&#24322;&#26500;&#22270;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#22312;&#23454;&#38469;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20934;&#30830;&#22320;&#25551;&#36848;&#22478;&#24066;&#31995;&#32479;&#30340;&#33030;&#24369;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#26469;&#29702;&#35299;&#21644;&#20998;&#26512;&#24322;&#26500;&#22270;&#65292;&#20174;&#32780;&#33021;&#22815;&#25429;&#25417;&#32423;&#32852;&#22833;&#36133;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding and characterizing the vulnerability of urban infrastructures, which refers to the engineering facilities essential for the regular running of cities and that exist naturally in the form of networks, is of great value to us. Potential applications include protecting fragile facilities and designing robust topologies, etc. Due to the strong correlation between different topological characteristics and infrastructure vulnerability and their complicated evolution mechanisms, some heuristic and machine-assisted analysis fall short in addressing such a scenario. In this paper, we model the interdependent network as a heterogeneous graph and propose a system based on graph neural network with reinforcement learning, which can be trained on real-world data, to characterize the vulnerability of the city system accurately. The presented system leverages deep learning techniques to understand and analyze the heterogeneous graph, which enables us to capture the risk of cascade failu
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#26041;&#27861;&#26469;&#22788;&#29702;&#22522;&#20110;&#36229;&#31435;&#26041;&#20307;&#30340;&#20998;&#31867;&#38382;&#39064;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#39640;&#25928;&#22320;&#35757;&#32451;&#36229;&#31435;&#26041;&#20307;&#27169;&#22411;&#65292;&#24182;&#22823;&#22823;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#65292;&#21516;&#26102;&#33719;&#24471;&#26356;&#20248;&#30340;&#20998;&#31867;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.09269</link><description>&lt;p&gt;
&#22522;&#20110;&#36229;&#31435;&#26041;&#20307;&#30340;&#20998;&#31867;&#30340;&#31471;&#21040;&#31471;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
End-to-End Neural Network Training for Hyperbox-Based Classification. (arXiv:2307.09269v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09269
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#26041;&#27861;&#26469;&#22788;&#29702;&#22522;&#20110;&#36229;&#31435;&#26041;&#20307;&#30340;&#20998;&#31867;&#38382;&#39064;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#39640;&#25928;&#22320;&#35757;&#32451;&#36229;&#31435;&#26041;&#20307;&#27169;&#22411;&#65292;&#24182;&#22823;&#22823;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#65292;&#21516;&#26102;&#33719;&#24471;&#26356;&#20248;&#30340;&#20998;&#31867;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#36229;&#31435;&#26041;&#20307;&#30340;&#20998;&#31867;&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#25216;&#26415;&#65292;&#20854;&#20013;&#23545;&#25968;&#25454;&#30340;&#20915;&#31574;&#34987;&#34920;&#31034;&#20026;&#19968;&#31995;&#21015;&#27491;&#20132;&#30340;&#22810;&#32500;&#31435;&#26041;&#20307;&#65288;&#21363;&#36229;&#31435;&#26041;&#20307;&#65289;&#65292;&#36825;&#20123;&#31435;&#26041;&#20307;&#36890;&#24120;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#21644;&#26131;&#35835;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#24050;&#32463;&#19981;&#20877;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#29616;&#20170;&#35768;&#22810;&#24212;&#29992;&#39046;&#22495;&#38754;&#20020;&#30340;&#19981;&#26029;&#22686;&#21152;&#30340;&#25968;&#25454;&#37327;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22522;&#20110;&#36229;&#31435;&#26041;&#20307;&#20998;&#31867;&#30340;&#23436;&#20840;&#21487;&#24494;&#20998;&#26694;&#26550;&#26469;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#12290;&#19982;&#20197;&#24448;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#36229;&#31435;&#26041;&#20307;&#27169;&#22411;&#21487;&#20197;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#39640;&#25928;&#35757;&#32451;&#65292;&#20174;&#32780;&#22823;&#22823;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#24182;&#33719;&#24471;&#26356;&#20248;&#30340;&#20998;&#31867;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hyperbox-based classification has been seen as a promising technique in which decisions on the data are represented as a series of orthogonal, multidimensional boxes (i.e., hyperboxes) that are often interpretable and human-readable. However, existing methods are no longer capable of efficiently handling the increasing volume of data many application domains face nowadays. We address this gap by proposing a novel, fully differentiable framework for hyperbox-based classification via neural networks. In contrast to previous work, our hyperbox models can be efficiently trained in an end-to-end fashion, which leads to significantly reduced training times and superior classification results.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;GPT-3.5&#21644;GPT-4&#27169;&#22411;&#22312;&#19981;&#21516;&#26102;&#38388;&#28857;&#19978;&#30340;&#24615;&#33021;&#21644;&#34892;&#20026;&#21464;&#21270;&#65292;&#21457;&#29616;&#23427;&#20204;&#30340;&#34920;&#29616;&#21487;&#20197;&#26377;&#24456;&#22823;&#30340;&#24046;&#24322;&#65292;&#21253;&#25324;&#22312;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#12289;&#22238;&#31572;&#25935;&#24863;&#38382;&#39064;&#12289;&#29983;&#25104;&#20195;&#30721;&#21644;&#35270;&#35273;&#25512;&#29702;&#31561;&#20219;&#21153;&#19978;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#30456;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#26381;&#21153;&#30340;&#34892;&#20026;&#22312;&#30456;&#23545;&#30701;&#30340;&#26102;&#38388;&#20869;&#21487;&#20197;&#21457;&#29983;&#26174;&#33879;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2307.09009</link><description>&lt;p&gt;
ChatGPT&#30340;&#34892;&#20026;&#38543;&#26102;&#38388;&#21464;&#21270;&#22914;&#20309;&#65311;
&lt;/p&gt;
&lt;p&gt;
How is ChatGPT's behavior changing over time?. (arXiv:2307.09009v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09009
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;GPT-3.5&#21644;GPT-4&#27169;&#22411;&#22312;&#19981;&#21516;&#26102;&#38388;&#28857;&#19978;&#30340;&#24615;&#33021;&#21644;&#34892;&#20026;&#21464;&#21270;&#65292;&#21457;&#29616;&#23427;&#20204;&#30340;&#34920;&#29616;&#21487;&#20197;&#26377;&#24456;&#22823;&#30340;&#24046;&#24322;&#65292;&#21253;&#25324;&#22312;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#12289;&#22238;&#31572;&#25935;&#24863;&#38382;&#39064;&#12289;&#29983;&#25104;&#20195;&#30721;&#21644;&#35270;&#35273;&#25512;&#29702;&#31561;&#20219;&#21153;&#19978;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#30456;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#26381;&#21153;&#30340;&#34892;&#20026;&#22312;&#30456;&#23545;&#30701;&#30340;&#26102;&#38388;&#20869;&#21487;&#20197;&#21457;&#29983;&#26174;&#33879;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
GPT-3.5&#21644;GPT-4&#26159;&#20004;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26381;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#36827;&#34892;&#26356;&#26032;&#26159;&#19981;&#36879;&#26126;&#30340;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23545;GPT-3.5&#21644;GPT-4&#30340;2023&#24180;3&#26376;&#21644;2023&#24180;6&#26376;&#29256;&#26412;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#28041;&#21450;&#22235;&#39033;&#19981;&#21516;&#30340;&#20219;&#21153;&#65306;1&#65289;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#65292;2&#65289;&#22238;&#31572;&#25935;&#24863;/&#21361;&#38505;&#38382;&#39064;&#65292;3&#65289;&#29983;&#25104;&#20195;&#30721;&#21644;4&#65289;&#35270;&#35273;&#25512;&#29702;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;GPT-3.5&#21644;GPT-4&#30340;&#24615;&#33021;&#21644;&#34892;&#20026;&#22312;&#26102;&#38388;&#19978;&#21487;&#20197;&#26377;&#24456;&#22823;&#30340;&#21464;&#21270;&#12290;&#20363;&#22914;&#65292;GPT-4&#65288;2023&#24180;3&#26376;&#65289;&#22312;&#35782;&#21035;&#36136;&#25968;&#26041;&#38754;&#34920;&#29616;&#38750;&#24120;&#20986;&#33394;&#65288;&#20934;&#30830;&#29575;&#20026;97.6%&#65289;&#65292;&#20294;GPT-4&#65288;2023&#24180;6&#26376;&#65289;&#22312;&#30456;&#21516;&#30340;&#38382;&#39064;&#19978;&#34920;&#29616;&#38750;&#24120;&#24046;&#65288;&#20934;&#30830;&#29575;&#20026;2.4%&#65289;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;GPT-3.5&#65288;2023&#24180;6&#26376;&#65289;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#27604;GPT-3.5&#65288;2023&#24180;3&#26376;&#65289;&#35201;&#22909;&#24471;&#22810;&#12290;GPT-4&#22312;6&#26376;&#20221;&#23545;&#22238;&#31572;&#25935;&#24863;&#38382;&#39064;&#30340;&#24847;&#24895;&#36739;3&#26376;&#20221;&#35201;&#20302;&#65292;&#32780;&#26080;&#35770;&#26159;GPT-4&#36824;&#26159;GPT-3.5&#22312;6&#26376;&#20221;&#30340;&#20195;&#30721;&#29983;&#25104;&#20013;&#37117;&#26377;&#26356;&#22810;&#30340;&#26684;&#24335;&#38169;&#35823;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#30456;&#21516;LLM&#26381;&#21153;&#30340;&#34892;&#20026;&#22312;&#30456;&#23545;&#36739;&#30701;&#30340;&#26102;&#38388;&#20869;&#21487;&#20197;&#21457;&#29983;&#37325;&#22823;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
GPT-3.5 and GPT-4 are the two most widely used large language model (LLM) services. However, when and how these models are updated over time is opaque. Here, we evaluate the March 2023 and June 2023 versions of GPT-3.5 and GPT-4 on four diverse tasks: 1) solving math problems, 2) answering sensitive/dangerous questions, 3) generating code and 4) visual reasoning. We find that the performance and behavior of both GPT-3.5 and GPT-4 can vary greatly over time. For example, GPT-4 (March 2023) was very good at identifying prime numbers (accuracy 97.6%) but GPT-4 (June 2023) was very poor on these same questions (accuracy 2.4%). Interestingly GPT-3.5 (June 2023) was much better than GPT-3.5 (March 2023) in this task. GPT-4 was less willing to answer sensitive questions in June than in March, and both GPT-4 and GPT-3.5 had more formatting mistakes in code generation in June than in March. Overall, our findings shows that the behavior of the same LLM service can change substantially in a relat
&lt;/p&gt;</description></item><item><title>DISPEL&#26159;&#19968;&#31181;&#36890;&#36807;&#21518;&#22788;&#29702;&#32454;&#31890;&#24230;&#25513;&#34109;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#36807;&#28388;&#25481;&#26410;&#23450;&#20041;&#21644;&#26080;&#27861;&#21306;&#20998;&#30340;&#39046;&#22495;&#29305;&#23450;&#29305;&#24449;&#30340;&#39046;&#22495;&#27867;&#21270;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.07181</link><description>&lt;p&gt;
DISPEL&#65306;&#36890;&#36807;&#39046;&#22495;&#29305;&#23450;&#35299;&#25918;&#36827;&#34892;&#22495;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
DISPEL: Domain Generalization via Domain-Specific Liberating. (arXiv:2307.07181v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07181
&lt;/p&gt;
&lt;p&gt;
DISPEL&#26159;&#19968;&#31181;&#36890;&#36807;&#21518;&#22788;&#29702;&#32454;&#31890;&#24230;&#25513;&#34109;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#36807;&#28388;&#25481;&#26410;&#23450;&#20041;&#21644;&#26080;&#27861;&#21306;&#20998;&#30340;&#39046;&#22495;&#29305;&#23450;&#29305;&#24449;&#30340;&#39046;&#22495;&#27867;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#27867;&#21270;&#26088;&#22312;&#36890;&#36807;&#20165;&#22312;&#26377;&#38480;&#30340;&#28304;&#39046;&#22495;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#23398;&#20064;&#19968;&#20010;&#33021;&#22815;&#22312;&#26410;&#30693;&#27979;&#35797;&#39046;&#22495;&#19978;&#34920;&#29616;&#33391;&#22909;&#30340;&#27867;&#21270;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#39046;&#22495;&#27867;&#21270;&#26041;&#27861;&#24448;&#24448;&#24341;&#20837;&#19982;&#39044;&#27979;&#26080;&#20851;&#30340;&#22122;&#22768;&#25110;&#38656;&#35201;&#25910;&#38598;&#39046;&#22495;&#26631;&#31614;&#26469;&#35299;&#20915;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#20174;&#19981;&#21516;&#30340;&#35282;&#24230;&#32771;&#34385;&#20102;&#39046;&#22495;&#27867;&#21270;&#38382;&#39064;&#65292;&#23558;&#24213;&#23618;&#29305;&#24449;&#32452;&#21010;&#20998;&#20026;&#39046;&#22495;&#20849;&#20139;&#21644;&#39046;&#22495;&#29305;&#23450;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#39046;&#22495;&#29305;&#23450;&#29305;&#24449;&#24456;&#38590;&#20174;&#36755;&#20837;&#25968;&#25454;&#20013;&#36827;&#34892;&#35782;&#21035;&#21644;&#21306;&#20998;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DISPEL&#65288;DomaIn-SPEcific Liberating&#65289;&#30340;&#21518;&#22788;&#29702;&#32454;&#31890;&#24230;&#25513;&#34109;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#36807;&#28388;&#25481;&#26410;&#23450;&#20041;&#21644;&#26080;&#27861;&#21306;&#20998;&#30340;&#39046;&#22495;&#29305;&#23450;&#29305;&#24449;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;DISPEL&#21033;&#29992;&#19968;&#20010;&#25513;&#34109;&#29983;&#25104;&#22120;&#20026;&#27599;&#20010;&#36755;&#20837;&#25968;&#25454;&#29983;&#25104;&#19968;&#20010;&#21807;&#19968;&#30340;&#25513;&#34109;&#26469;&#36807;&#28388;&#39046;&#22495;&#29305;&#23450;&#29305;&#24449;&#12290;DISPEL&#26694;&#26550;&#38750;&#24120;&#28789;&#27963;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#32454;&#31890;&#24230;&#30340;&#25513;&#34109;&#20219;&#21153;&#21644;&#39046;&#22495;&#27867;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain generalization aims to learn a generalization model that can perform well on unseen test domains by only training on limited source domains. However, existing domain generalization approaches often bring in prediction-irrelevant noise or require the collection of domain labels. To address these challenges, we consider the domain generalization problem from a different perspective by categorizing underlying feature groups into domain-shared and domain-specific features. Nevertheless, the domain-specific features are difficult to be identified and distinguished from the input data. In this work, we propose DomaIn-SPEcific Liberating (DISPEL), a post-processing fine-grained masking approach that can filter out undefined and indistinguishable domain-specific features in the embedding space. Specifically, DISPEL utilizes a mask generator that produces a unique mask for each input data to filter domain-specific features. The DISPEL framework is highly flexible to be applied to any fin
&lt;/p&gt;</description></item><item><title>CrunchGPT&#26159;&#19968;&#20010;&#22522;&#20110;ChatGPT&#30340;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#26694;&#26550;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#29992;&#25143;&#25552;&#31034;&#26469;&#21327;&#35843;&#25972;&#20010;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#30340;&#24037;&#20316;&#27969;&#31243;&#65292;&#23454;&#29616;&#26080;&#32541;&#38598;&#25104;&#25968;&#25454;&#21644;&#29289;&#29702;&#30693;&#35782;&#65292;&#35299;&#20915;&#20102;SciML&#22312;&#39044;&#22788;&#29702;&#12289;&#38382;&#39064;&#24314;&#27169;&#12289;&#20195;&#30721;&#29983;&#25104;&#12289;&#21518;&#22788;&#29702;&#21644;&#20998;&#26512;&#31561;&#26041;&#38754;&#30340;&#32791;&#26102;&#38382;&#39064;&#65292;&#25299;&#23637;&#20102;&#20854;&#24037;&#19994;&#24212;&#29992;&#21644;&#25968;&#23383;&#23402;&#29983;&#26694;&#26550;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.15551</link><description>&lt;p&gt;
CrunchGPT&#65306;&#22522;&#20110;ChatGPT&#30340;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
CrunchGPT: A chatGPT assisted framework for scientific machine learning. (arXiv:2306.15551v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15551
&lt;/p&gt;
&lt;p&gt;
CrunchGPT&#26159;&#19968;&#20010;&#22522;&#20110;ChatGPT&#30340;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#26694;&#26550;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#29992;&#25143;&#25552;&#31034;&#26469;&#21327;&#35843;&#25972;&#20010;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#30340;&#24037;&#20316;&#27969;&#31243;&#65292;&#23454;&#29616;&#26080;&#32541;&#38598;&#25104;&#25968;&#25454;&#21644;&#29289;&#29702;&#30693;&#35782;&#65292;&#35299;&#20915;&#20102;SciML&#22312;&#39044;&#22788;&#29702;&#12289;&#38382;&#39064;&#24314;&#27169;&#12289;&#20195;&#30721;&#29983;&#25104;&#12289;&#21518;&#22788;&#29702;&#21644;&#20998;&#26512;&#31561;&#26041;&#38754;&#30340;&#32791;&#26102;&#38382;&#39064;&#65292;&#25299;&#23637;&#20102;&#20854;&#24037;&#19994;&#24212;&#29992;&#21644;&#25968;&#23383;&#23402;&#29983;&#26694;&#26550;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#65288;SciML&#65289;&#36817;&#24180;&#26469;&#22312;&#35745;&#31639;&#31185;&#23398;&#21644;&#24037;&#31243;&#30340;&#35768;&#22810;&#19981;&#21516;&#39046;&#22495;&#21462;&#24471;&#20102;&#36827;&#23637;&#12290;&#20854;&#30446;&#26631;&#26159;&#22312;&#19981;&#38656;&#35201;&#22797;&#26434;&#21644;&#35745;&#31639;&#23494;&#38598;&#30340;&#25968;&#25454;&#21516;&#21270;&#26041;&#26696;&#30340;&#24773;&#20917;&#19979;&#65292;&#26080;&#32541;&#22320;&#23558;&#25968;&#25454;&#21644;&#29289;&#29702;&#30693;&#35782;&#38598;&#25104;&#36215;&#26469;&#12290;&#28982;&#32780;&#65292;&#39044;&#22788;&#29702;&#12289;&#38382;&#39064;&#24314;&#27169;&#12289;&#20195;&#30721;&#29983;&#25104;&#12289;&#21518;&#22788;&#29702;&#21644;&#20998;&#26512;&#20173;&#28982;&#26159;&#32791;&#26102;&#30340;&#65292;&#24182;&#19988;&#21487;&#33021;&#38480;&#21046;SciML&#22312;&#24037;&#19994;&#24212;&#29992;&#21644;&#25968;&#23383;&#23402;&#29983;&#26694;&#26550;&#20013;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23558;SciML&#30340;&#21508;&#20010;&#38454;&#27573;&#25972;&#21512;&#21040;ChatGPT&#30340;&#20254;&#19979;&#65292;&#24418;&#25104;CrunchGPT&#65292;&#23427;&#36890;&#36807;&#29992;&#25143;&#31616;&#21333;&#30340;&#25552;&#31034;&#26469;&#21327;&#35843;&#25972;&#20010;SciML&#30340;&#24037;&#20316;&#27969;&#31243;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20004;&#20010;&#31034;&#20363;&#65292;&#28436;&#31034;&#20102;CrunchGPT&#22312;&#27668;&#21160;&#23398;&#20013;&#20248;&#21270;&#26426;&#32764;&#21644;&#22312;&#21508;&#31181;&#20960;&#20309;&#24418;&#29366;&#20013;&#33719;&#24471;&#27969;&#22330;&#30340;&#28508;&#22312;&#29992;&#36884;&#65292;&#24182;&#24378;&#35843;&#20102;&#39564;&#35777;&#38454;&#27573;&#12290;&#20026;&#20102;&#28436;&#31034;CrunchGPT&#30340;&#27969;&#31243;&#21644;
&lt;/p&gt;
&lt;p&gt;
Scientific Machine Learning (SciML) has advanced recently across many different areas in computational science and engineering. The objective is to integrate data and physics seamlessly without the need of employing elaborate and computationally taxing data assimilation schemes. However, preprocessing, problem formulation, code generation, postprocessing and analysis are still time consuming and may prevent SciML from wide applicability in industrial applications and in digital twin frameworks. Here, we integrate the various stages of SciML under the umbrella of ChatGPT, to formulate CrunchGPT, which plays the role of a conductor orchestrating the entire workflow of SciML based on simple prompts by the user. Specifically, we present two examples that demonstrate the potential use of CrunchGPT in optimizing airfoils in aerodynamics, and in obtaining flow fields in various geometries in interactive mode, with emphasis on the validation stage. To demonstrate the flow of the CrunchGPT, and
&lt;/p&gt;</description></item><item><title>ChiPFormer&#36890;&#36807;&#31163;&#32447;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#24067;&#23616;&#31574;&#30053;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#33455;&#29255;&#24067;&#23616;&#30340;&#36136;&#37327;&#65292;&#24182;&#22312;&#20943;&#23569;&#24067;&#23616;&#26102;&#38388;&#30340;&#21516;&#26102;&#22686;&#24378;&#20102;&#23545;&#26410;&#30693;&#33455;&#29255;&#30005;&#36335;&#30340;&#36866;&#24212;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.14744</link><description>&lt;p&gt;
ChiPFormer: &#36890;&#36807;&#31163;&#32447;&#20915;&#31574;&#21464;&#25442;&#22120;&#23454;&#29616;&#21487;&#36716;&#31227;&#33455;&#29255;&#24067;&#23616;
&lt;/p&gt;
&lt;p&gt;
ChiPFormer: Transferable Chip Placement via Offline Decision Transformer. (arXiv:2306.14744v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14744
&lt;/p&gt;
&lt;p&gt;
ChiPFormer&#36890;&#36807;&#31163;&#32447;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#24067;&#23616;&#31574;&#30053;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#33455;&#29255;&#24067;&#23616;&#30340;&#36136;&#37327;&#65292;&#24182;&#22312;&#20943;&#23569;&#24067;&#23616;&#26102;&#38388;&#30340;&#21516;&#26102;&#22686;&#24378;&#20102;&#23545;&#26410;&#30693;&#33455;&#29255;&#30005;&#36335;&#30340;&#36866;&#24212;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#33455;&#29255;&#35774;&#35745;&#20013;&#65292;&#24067;&#23616;&#26159;&#19968;&#20010;&#20851;&#38190;&#27493;&#39588;&#65292;&#26088;&#22312;&#30830;&#23450;&#33455;&#29255;&#30011;&#24067;&#19978;&#30005;&#36335;&#27169;&#22359;&#30340;&#20301;&#32622;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#24378;&#21270;&#23398;&#20064;&#21487;&#20197;&#25552;&#39640;&#33455;&#29255;&#24067;&#23616;&#20013;&#30340;&#20154;&#31867;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#22312;&#35757;&#32451;&#26102;&#38388;&#38271;&#19988;&#22312;&#26410;&#30693;&#30340;&#33455;&#29255;&#30005;&#36335;&#20013;&#20855;&#26377;&#36739;&#20302;&#30340;&#36716;&#31227;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#23558;&#33455;&#29255;&#24067;&#23616;&#38382;&#39064;&#20316;&#20026;&#19968;&#20010;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;ChiPFormer&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#22266;&#23450;&#30340;&#31163;&#32447;&#25968;&#25454;&#23398;&#20064;&#21040;&#21487;&#36716;&#31227;&#30340;&#24067;&#23616;&#31574;&#30053;&#12290;ChiPFormer&#20855;&#26377;&#19968;&#20123;&#20808;&#21069;&#30340;&#30740;&#31350;&#25152;&#27809;&#26377;&#30340;&#20248;&#21183;&#12290;&#39318;&#20808;&#65292;ChiPFormer&#33021;&#22815;&#21033;&#29992;&#31163;&#32447;&#24067;&#23616;&#35774;&#35745;&#22312;&#22810;&#20219;&#21153;&#35774;&#32622;&#20013;&#26356;&#26377;&#25928;&#22320;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#31574;&#30053;&#12290;&#20854;&#27425;&#65292;ChiPFormer&#33021;&#22815;&#20419;&#36827;&#23545;&#26410;&#30693;&#33455;&#29255;&#30005;&#36335;&#30340;&#26377;&#25928;&#24494;&#35843;&#65292;&#23558;&#24067;&#23616;&#36816;&#34892;&#26102;&#38388;&#20174;&#20960;&#23567;&#26102;&#32553;&#30701;&#21040;&#20960;&#20998;&#38047;&#12290;&#31532;&#19977;&#65292;&#23545;32&#20010;&#33455;&#29255;&#30005;&#36335;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;ChiPFormer&#22312;&#20943;&#23569;&#24067;&#23616;&#26102;&#38388;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#26174;&#33879;&#25552;&#39640;&#30340;&#24067;&#23616;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Placement is a critical step in modern chip design, aiming to determine the positions of circuit modules on the chip canvas. Recent works have shown that reinforcement learning (RL) can improve human performance in chip placement. However, such an RL-based approach suffers from long training time and low transfer ability in unseen chip circuits. To resolve these challenges, we cast the chip placement as an offline RL formulation and present ChiPFormer that enables learning a transferable placement policy from fixed offline data. ChiPFormer has several advantages that prior arts do not have. First, ChiPFormer can exploit offline placement designs to learn transferable policies more efficiently in a multi-task setting. Second, ChiPFormer can promote effective finetuning for unseen chip circuits, reducing the placement runtime from hours to minutes. Third, extensive experiments on 32 chip circuits demonstrate that ChiPFormer achieves significantly better placement quality while reducing t
&lt;/p&gt;</description></item><item><title>CARL-G&#26159;&#19968;&#20010;&#22522;&#20110;&#32858;&#31867;&#30340;&#22270;&#34920;&#24449;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#21463;&#38598;&#32676;&#39564;&#35777;&#21551;&#21457;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#20811;&#26381;&#20102;&#36127;&#37319;&#26679;&#21644;&#22797;&#26434;&#26550;&#26500;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.06936</link><description>&lt;p&gt;
CARL-G: &#22522;&#20110;&#32858;&#31867;&#21152;&#36895;&#30340;&#22270;&#34920;&#24449;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
CARL-G: Clustering-Accelerated Representation Learning on Graphs. (arXiv:2306.06936v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06936
&lt;/p&gt;
&lt;p&gt;
CARL-G&#26159;&#19968;&#20010;&#22522;&#20110;&#32858;&#31867;&#30340;&#22270;&#34920;&#24449;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#21463;&#38598;&#32676;&#39564;&#35777;&#21551;&#21457;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#20811;&#26381;&#20102;&#36127;&#37319;&#26679;&#21644;&#22797;&#26434;&#26550;&#26500;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#22312;&#22270;&#19978;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#65292;&#24182;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#37117;&#23384;&#22312;&#19968;&#20123;&#38556;&#30861;&#65292;&#38459;&#30861;&#20102;&#23427;&#20204;&#20805;&#20998;&#21457;&#25381;&#28508;&#21147;&#12290;&#20363;&#22914;&#65292;&#23545;&#27604;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#36127;&#37319;&#26679;&#65292;&#36825;&#22312;&#35745;&#31639;&#19978;&#24448;&#24448;&#26159;&#26114;&#36149;&#30340;&#12290;&#34429;&#28982;&#38750;&#23545;&#27604;&#26041;&#27861;&#36991;&#20813;&#20102;&#36825;&#19968;&#26114;&#36149;&#30340;&#27493;&#39588;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#35201;&#20040;&#20381;&#36182;&#36807;&#20110;&#22797;&#26434;&#30340;&#26550;&#26500;&#65292;&#35201;&#20040;&#20381;&#36182;&#20110;&#29305;&#23450;&#25968;&#25454;&#38598;&#30340;&#22686;&#24378;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#25105;&#20204;&#26159;&#21542;&#21487;&#20197;&#20511;&#37492;&#32463;&#20856;&#30340;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#25991;&#29486;&#65292;&#20197;&#20811;&#26381;&#36825;&#20123;&#38556;&#30861;&#65311;&#22312;&#25105;&#20204;&#30340;&#20851;&#38190;&#27934;&#23519;&#30340;&#25351;&#23548;&#19979;&#65292;&#36317;&#31163;&#32858;&#31867;&#30340;&#30446;&#26631;&#19982;&#23545;&#27604;&#23398;&#20064;&#30340;&#30446;&#26631;&#38750;&#24120;&#30456;&#20284;&#65306;&#37117;&#35797;&#22270;&#23558;&#30456;&#20284;&#39033;&#30340;&#34920;&#31034;&#25289;&#22312;&#19968;&#36215;&#65292;&#24182;&#23558;&#19981;&#30456;&#20284;&#39033;&#20998;&#24320;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CARL-G - &#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110;&#32858;&#31867;&#30340;&#22270;&#34920;&#24449;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#20102;&#19968;&#20010;&#21463;&#38598;&#32676;&#39564;&#35777;&#21551;&#21457;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning on graphs has made large strides in achieving great performance in various downstream tasks. However, many state-of-the-art methods suffer from a number of impediments, which prevent them from realizing their full potential. For instance, contrastive methods typically require negative sampling, which is often computationally costly. While non-contrastive methods avoid this expensive step, most existing methods either rely on overly complex architectures or dataset-specific augmentations. In this paper, we ask: Can we borrow from classical unsupervised machine learning literature in order to overcome those obstacles? Guided by our key insight that the goal of distance-based clustering closely resembles that of contrastive learning: both attempt to pull representations of similar items together and dissimilar items apart. As a result, we propose CARL-G - a novel clustering-based framework for graph representation learning that uses a loss inspired by Cluster Vali
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23494;&#38598;&#39640;&#26031;&#21464;&#20998;&#26063;&#30340;&#26799;&#24230;&#20272;&#35745;&#22120;&#65292;&#22312;&#27492;&#22522;&#30784;&#19978;&#20351;&#29992;&#36817;&#31471;&#21644;&#25237;&#24433;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65292;&#25552;&#20379;&#20102;&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;&#25910;&#25947;&#20110;&#36924;&#30495;&#25512;&#26029;&#38382;&#39064;&#30340;&#31532;&#19968;&#20010;&#20005;&#26684;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2306.03638</link><description>&lt;p&gt;
&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;&#30340;&#25910;&#25947;&#24615;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Provable convergence guarantees for black-box variational inference. (arXiv:2306.03638v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03638
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23494;&#38598;&#39640;&#26031;&#21464;&#20998;&#26063;&#30340;&#26799;&#24230;&#20272;&#35745;&#22120;&#65292;&#22312;&#27492;&#22522;&#30784;&#19978;&#20351;&#29992;&#36817;&#31471;&#21644;&#25237;&#24433;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65292;&#25552;&#20379;&#20102;&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;&#25910;&#25947;&#20110;&#36924;&#30495;&#25512;&#26029;&#38382;&#39064;&#30340;&#31532;&#19968;&#20010;&#20005;&#26684;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;&#34987;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#27809;&#26377;&#35777;&#26126;&#20854;&#38543;&#26426;&#20248;&#21270;&#25104;&#21151;&#30340;&#35777;&#26126;&#12290;&#25105;&#20204;&#25552;&#20986;&#36825;&#26159;&#29616;&#26377;&#38543;&#26426;&#20248;&#21270;&#35777;&#26126;&#20013;&#30340;&#29702;&#35770;&#24046;&#36317;&#65292;&#21363;&#20855;&#26377;&#24322;&#24120;&#22122;&#22768;&#36793;&#30028;&#21644;&#22797;&#21512;&#38750;&#24179;&#28369;&#30446;&#26631;&#30340;&#26799;&#24230;&#20272;&#35745;&#22120;&#30340;&#25361;&#25112;&#12290;&#23545;&#20110;&#23494;&#38598;&#30340;&#39640;&#26031;&#21464;&#20998;&#26063;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#29616;&#26377;&#30340;&#22522;&#20110;&#20877;&#21442;&#25968;&#21270;&#30340;&#26799;&#24230;&#20272;&#35745;&#22120;&#28385;&#36275;&#20108;&#27425;&#22122;&#22768;&#30028;&#65292;&#24182;&#20026;&#20351;&#29992;&#35813;&#30028;&#38480;&#30340;&#36817;&#31471;&#21644;&#25237;&#24433;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#25552;&#20379;&#26032;&#30340;&#25910;&#25947;&#20445;&#35777;&#12290;&#36825;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;&#25910;&#25947;&#20110;&#36924;&#30495;&#25512;&#26029;&#38382;&#39064;&#30340;&#20005;&#26684;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
While black-box variational inference is widely used, there is no proof that its stochastic optimization succeeds. We suggest this is due to a theoretical gap in existing stochastic optimization proofs-namely the challenge of gradient estimators with unusual noise bounds, and a composite non-smooth objective. For dense Gaussian variational families, we observe that existing gradient estimators based on reparameterization satisfy a quadratic noise bound and give novel convergence guarantees for proximal and projected stochastic gradient descent using this bound. This provides the first rigorous guarantee that black-box variational inference converges for realistic inference problems.
&lt;/p&gt;</description></item><item><title>W-procer&#26159;&#19968;&#31181;&#22522;&#20110;&#21152;&#26435;&#21407;&#22411;&#23545;&#27604;&#23398;&#20064;&#30340;&#21307;&#23398;&#23569;&#26679;&#26412;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26041;&#27861;&#65292;&#22312;&#26500;&#24314;&#22522;&#20110;&#21407;&#22411;&#30340;&#23545;&#27604;&#25439;&#22833;&#21644;&#21152;&#26435;&#32593;&#32476;&#26041;&#38754;&#20855;&#26377;&#21019;&#26032;&#24615;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.18624</link><description>&lt;p&gt;
W-procer: &#22522;&#20110;&#21152;&#26435;&#21407;&#22411;&#23545;&#27604;&#23398;&#20064;&#30340;&#21307;&#23398;&#23569;&#26679;&#26412;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
W-procer: Weighted Prototypical Contrastive Learning for Medical Few-Shot Named Entity Recognition. (arXiv:2305.18624v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18624
&lt;/p&gt;
&lt;p&gt;
W-procer&#26159;&#19968;&#31181;&#22522;&#20110;&#21152;&#26435;&#21407;&#22411;&#23545;&#27604;&#23398;&#20064;&#30340;&#21307;&#23398;&#23569;&#26679;&#26412;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26041;&#27861;&#65292;&#22312;&#26500;&#24314;&#22522;&#20110;&#21407;&#22411;&#30340;&#23545;&#27604;&#25439;&#22833;&#21644;&#21152;&#26435;&#32593;&#32476;&#26041;&#38754;&#20855;&#26377;&#21019;&#26032;&#24615;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#24050;&#25104;&#20026;&#23569;&#26679;&#26412;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#30340;&#19968;&#31181;&#21463;&#27426;&#36814;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#20256;&#32479;&#37197;&#32622;&#21147;&#27714;&#20943;&#23569;&#20855;&#26377;&#30456;&#21516;&#26631;&#31614;&#30340;&#26631;&#35760;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#24182;&#22686;&#21152;&#20855;&#26377;&#19981;&#21516;&#26631;&#31614;&#30340;&#26631;&#35760;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#28982;&#32780;&#65292;&#22312;&#21307;&#23398;&#39046;&#22495;&#20013;&#23384;&#22312;&#22823;&#37327;&#34987;&#27880;&#37322;&#20026;&#8220;O&#8221;&#65288;&#21363;&#8220;OUTSIDE&#8221;&#65289;&#30340;&#23454;&#20307;&#65292;&#24182;&#19988;&#23427;&#20204;&#19981;&#24076;&#26395;&#34987;&#25512;&#31163;&#21040;&#24403;&#21069;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#26631;&#35760;&#20026;&#8220;O&#8221;&#20197;&#22806;&#30340;&#20854;&#20182;&#23454;&#20307;&#65292;&#36825;&#31181;&#35774;&#23450;&#25928;&#26524;&#19981;&#20339;&#65292;&#21487;&#33021;&#20250;&#24471;&#20986;&#21547;&#26377;&#22122;&#22768;&#21407;&#22411;&#26631;&#31614;&#30340;&#35821;&#20041;&#34920;&#31034;&#65292;&#23613;&#31649;&#23384;&#22312;&#35768;&#22810;&#8220;O&#8221;&#26631;&#31614;&#23454;&#20307;&#19982;&#26377;&#26631;&#31614;&#23454;&#20307;&#30456;&#20851;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21307;&#23398;&#23569;&#26679;&#26412;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20013;&#22522;&#20110;&#21152;&#26435;&#21407;&#22411;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65288;W-PROCER&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20027;&#35201;&#22260;&#32469;&#26500;&#24314;&#22522;&#20110;&#21407;&#22411;&#30340;&#23545;&#27604;&#25439;&#22833;&#21644;&#21152;&#26435;&#32593;&#32476;&#23637;&#24320;&#12290;&#36825;&#20123;&#32452;&#20214;&#22312;&#21327;&#21161;&#22312;&#21307;&#23398;&#39046;&#22495;&#20013;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#38754;&#21457;&#25381;&#20102;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23558;W-PROCER&#24212;&#29992;&#20110;&#19968;&#20010;&#20844;&#20849;&#30340;&#21307;&#23398;&#25968;&#25454;&#38598;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#20248;&#24322;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive learning has become a popular solution for few-shot Name Entity Recognization (NER). The conventional configuration strives to reduce the distance between tokens with the same labels and increase the distance between tokens with different labels. The effect of this setup may, however, in the medical domain, there are a lot of entities annotated as OUTSIDE (O), and they are undesirably pushed apart to other entities that are not labeled as OUTSIDE (O) by the current contrastive learning method end up with a noisy prototype for the semantic representation of the label, though there are many OUTSIDE (O) labeled entities are relevant to the labeled entities. To address this challenge, we propose a novel method named Weighted Prototypical Contrastive Learning for Medical Few Shot Named Entity Recognization (W-PROCER). Our approach primarily revolves around constructing the prototype-based contractive loss and weighting network. These components play a crucial role in assisting t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#21457;&#29616;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#22312;&#20219;&#21153;&#29305;&#23450;&#23376;&#31354;&#38388;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#26032;&#21442;&#25968;&#21270;&#21644;&#24494;&#35843;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#12290;&#30740;&#31350;&#21457;&#29616;&#22312;&#35813;&#23376;&#31354;&#38388;&#20013;&#65292;&#21482;&#38656;&#23569;&#37327;&#33258;&#30001;&#21442;&#25968;&#21363;&#21487;&#26377;&#25928;&#24494;&#35843;&#27169;&#22411;&#65292;&#24182;&#19988;&#26576;&#20123;&#32500;&#24230;&#23545;&#20110;&#24341;&#20837;&#20219;&#21153;&#29305;&#23450;&#30693;&#35782;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2305.17446</link><description>&lt;p&gt;
&#24494;&#23567;&#23376;&#31354;&#38388;&#20013;&#21457;&#29983;&#24494;&#35843;: &#25506;&#32034;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#22312;&#20219;&#21153;&#29305;&#23450;&#23376;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning Happens in Tiny Subspaces: Exploring Intrinsic Task-specific Subspaces of Pre-trained Language Models. (arXiv:2305.17446v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17446
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#21457;&#29616;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#22312;&#20219;&#21153;&#29305;&#23450;&#23376;&#31354;&#38388;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#26032;&#21442;&#25968;&#21270;&#21644;&#24494;&#35843;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#12290;&#30740;&#31350;&#21457;&#29616;&#22312;&#35813;&#23376;&#31354;&#38388;&#20013;&#65292;&#21482;&#38656;&#23569;&#37327;&#33258;&#30001;&#21442;&#25968;&#21363;&#21487;&#26377;&#25928;&#24494;&#35843;&#27169;&#22411;&#65292;&#24182;&#19988;&#26576;&#20123;&#32500;&#24230;&#23545;&#20110;&#24341;&#20837;&#20219;&#21153;&#29305;&#23450;&#30693;&#35782;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#30693;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#36807;&#24230;&#21442;&#25968;&#21270;&#24182;&#20855;&#26377;&#26174;&#33879;&#30340;&#20887;&#20313;&#65292;&#34920;&#26126;PLMs&#30340;&#33258;&#30001;&#24230;&#36739;&#23567;&#12290;&#26412;&#25991;&#20174;&#26032;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#37325;&#26032;&#21442;&#25968;&#21270;&#21644;&#24494;&#35843;PLMs&#30340;&#38382;&#39064;&#65306;&#21457;&#29616;&#20869;&#22312;&#30340;&#20219;&#21153;&#29305;&#23450;&#23376;&#31354;&#38388;&#12290;&#20855;&#20307;&#22320;&#65292;&#36890;&#36807;&#21033;&#29992;&#32473;&#23450;&#20219;&#21153;&#30340;&#24494;&#35843;&#36807;&#31243;&#30340;&#21160;&#24577;&#65292;&#23398;&#20064;&#20102;&#21442;&#25968;&#20248;&#21270;&#36712;&#36857;&#20197;&#25581;&#31034;&#20854;&#20869;&#22312;&#30340;&#20219;&#21153;&#29305;&#23450;&#23376;&#31354;&#38388;&#12290;&#19968;&#20010;&#20851;&#38190;&#21457;&#29616;&#26159;&#65292;&#22312;&#23376;&#31354;&#38388;&#20013;&#65292;PLMs&#21487;&#20197;&#36890;&#36807;&#23569;&#37327;&#30340;&#33258;&#30001;&#21442;&#25968;&#36827;&#34892;&#26377;&#25928;&#30340;&#24494;&#35843;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#23376;&#31354;&#38388;&#30340;&#24494;&#35843;&#36807;&#31243;&#20013;&#20986;&#29616;&#20102;&#19968;&#20123;&#24322;&#24120;&#32500;&#24230;&#12290;&#31105;&#29992;&#36825;&#20123;&#32500;&#24230;&#20250;&#20005;&#37325;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;&#12290;&#36825;&#34920;&#26126;&#36825;&#20123;&#32500;&#24230;&#23545;&#20110;&#24341;&#20837;&#20219;&#21153;&#29305;&#23450;&#30693;&#35782;&#21040;&#19979;&#28216;&#20219;&#21153;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models (PLMs) are known to be overly parameterized and have significant redundancy, indicating a small degree of freedom of the PLMs. Motivated by the observation, in this paper, we study the problem of re-parameterizing and fine-tuning PLMs from a new perspective: Discovery of intrinsic task-specific subspace. Specifically, by exploiting the dynamics of the fine-tuning process for a given task, the parameter optimization trajectory is learned to uncover its intrinsic task-specific subspace. A key finding is that PLMs can be effectively fine-tuned in the subspace with a small number of free parameters. Beyond, we observe some outlier dimensions emerging during fine-tuning in the subspace. Disabling these dimensions degrades the model performance significantly. This suggests that these dimensions are crucial to induce task-specific knowledge to downstream tasks.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22797;&#26434;&#20219;&#21153;&#20013;&#20855;&#26377;&#38750;&#39532;&#23572;&#21487;&#22827;&#22238;&#25253;&#20989;&#25968;&#30340;&#38543;&#26426;&#21338;&#24328;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22870;&#21169;&#26426;&#21046;&#30340;&#31639;&#27861;&#65292;&#22312;&#32435;&#20160;&#22343;&#34913;&#19979;&#23398;&#20064;&#27599;&#20010;&#26234;&#33021;&#20307;&#30340;&#26368;&#20339;&#24212;&#31572;&#31574;&#30053;&#65292;&#24182;&#35777;&#26126;&#20102;&#23398;&#20064;&#30340;Q&#20989;&#25968;&#23558;&#20250;&#25910;&#25947;&#20110;&#19968;&#20010;&#32435;&#20160;&#22343;&#34913;&#30340;Q&#20989;&#25968;&#65292;&#21069;&#25552;&#26159;&#38454;&#27573;&#21338;&#24328;&#20855;&#26377;&#20840;&#23616;&#26368;&#20248;&#28857;&#25110;&#32773;&#38797;&#28857;&#65292;&#24182;&#19988;&#26234;&#33021;&#20307;&#22522;&#20110;&#36825;&#19968;&#28857;&#36827;&#34892;&#26368;&#20339;&#24212;&#31572;&#31574;&#30053;&#30340;Q&#20989;&#25968;&#26356;&#26032;&#12290;</title><link>http://arxiv.org/abs/2305.17372</link><description>&lt;p&gt;
&#22312;&#38543;&#26426;&#21338;&#24328;&#20013;&#20351;&#29992;&#22870;&#21169;&#26426;&#21046;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning With Reward Machines in Stochastic Games. (arXiv:2305.17372v2 [cs.MA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17372
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22797;&#26434;&#20219;&#21153;&#20013;&#20855;&#26377;&#38750;&#39532;&#23572;&#21487;&#22827;&#22238;&#25253;&#20989;&#25968;&#30340;&#38543;&#26426;&#21338;&#24328;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22870;&#21169;&#26426;&#21046;&#30340;&#31639;&#27861;&#65292;&#22312;&#32435;&#20160;&#22343;&#34913;&#19979;&#23398;&#20064;&#27599;&#20010;&#26234;&#33021;&#20307;&#30340;&#26368;&#20339;&#24212;&#31572;&#31574;&#30053;&#65292;&#24182;&#35777;&#26126;&#20102;&#23398;&#20064;&#30340;Q&#20989;&#25968;&#23558;&#20250;&#25910;&#25947;&#20110;&#19968;&#20010;&#32435;&#20160;&#22343;&#34913;&#30340;Q&#20989;&#25968;&#65292;&#21069;&#25552;&#26159;&#38454;&#27573;&#21338;&#24328;&#20855;&#26377;&#20840;&#23616;&#26368;&#20248;&#28857;&#25110;&#32773;&#38797;&#28857;&#65292;&#24182;&#19988;&#26234;&#33021;&#20307;&#22522;&#20110;&#36825;&#19968;&#28857;&#36827;&#34892;&#26368;&#20339;&#24212;&#31572;&#31574;&#30053;&#30340;Q&#20989;&#25968;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22797;&#26434;&#20219;&#21153;&#20013;&#20855;&#26377;&#38750;&#39532;&#23572;&#21487;&#22827;&#22238;&#25253;&#20989;&#25968;&#30340;&#38543;&#26426;&#21338;&#24328;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#12290;&#25105;&#20204;&#21033;&#29992;&#22870;&#21169;&#26426;&#21046;&#26469;&#25972;&#21512;&#39640;&#23618;&#27425;&#30340;&#22797;&#26434;&#20219;&#21153;&#30693;&#35782;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#31216;&#20026;QRM-SG&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#27599;&#20010;&#26234;&#33021;&#20307;&#22312;&#32435;&#20160;&#22343;&#34913;&#19979;&#30340;&#26368;&#20339;&#24212;&#31572;&#31574;&#30053;&#12290;&#22312;QRM-SG&#20013;&#65292;&#25105;&#20204;&#22312;&#22686;&#24191;&#29366;&#24577;&#31354;&#38388;&#20013;&#23450;&#20041;&#20102;&#32435;&#20160;&#22343;&#34913;&#19979;&#30340;Q&#20989;&#25968;&#12290;&#22686;&#24191;&#29366;&#24577;&#31354;&#38388;&#25972;&#21512;&#20102;&#38543;&#26426;&#21338;&#24328;&#30340;&#29366;&#24577;&#21644;&#22870;&#21169;&#26426;&#21046;&#30340;&#29366;&#24577;&#12290;&#27599;&#20010;&#26234;&#33021;&#20307;&#23398;&#20064;&#20102;&#31995;&#32479;&#20013;&#25152;&#26377;&#26234;&#33021;&#20307;&#30340;Q&#20989;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;QRM-SG&#20013;&#23398;&#20064;&#30340;Q&#20989;&#25968;&#23558;&#20250;&#25910;&#25947;&#20110;&#19968;&#20010;&#32435;&#20160;&#22343;&#34913;&#30340;Q&#20989;&#25968;&#65292;&#21069;&#25552;&#26159;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#27599;&#20010;&#26102;&#38388;&#27493;&#30340;&#38454;&#27573;&#21338;&#24328;&#20855;&#26377;&#20840;&#23616;&#26368;&#20248;&#28857;&#25110;&#32773;&#38797;&#28857;&#65292;&#24182;&#19988;&#26234;&#33021;&#20307;&#22522;&#20110;&#36825;&#19968;&#28857;&#36827;&#34892;&#26368;&#20339;&#24212;&#31572;&#31574;&#30053;&#30340;Q&#20989;&#25968;&#26356;&#26032;&#12290;&#25105;&#20204;&#20351;&#29992;Lemke-Howson&#26041;&#27861;&#26469;&#24471;&#20986;&#32473;&#23450;&#24403;&#21069;Q&#20989;&#25968;&#26102;&#30340;&#26368;&#20339;&#24212;&#31572;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate multi-agent reinforcement learning for stochastic games with complex tasks, where the reward functions are non-Markovian. We utilize reward machines to incorporate high-level knowledge of complex tasks. We develop an algorithm called Q-learning with reward machines for stochastic games (QRM-SG), to learn the best-response strategy at Nash equilibrium for each agent. In QRM-SG, we define the Q-function at a Nash equilibrium in augmented state space. The augmented state space integrates the state of the stochastic game and the state of reward machines. Each agent learns the Q-functions of all agents in the system. We prove that Q-functions learned in QRM-SG converge to the Q-functions at a Nash equilibrium if the stage game at each time step during learning has a global optimum point or a saddle point, and the agents update Q-functions based on the best-response strategy at this point. We use the Lemke-Howson method to derive the best-response strategy given current Q-func
&lt;/p&gt;</description></item><item><title>Diable&#26159;&#19968;&#20010;&#39640;&#25928;&#30340;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#31995;&#32479;&#65292;&#23427;&#36890;&#36807;&#22312;&#34920;&#26684;&#19978;&#36827;&#34892;&#25805;&#20316;&#26469;&#26356;&#26032;&#23545;&#35805;&#29366;&#24577;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#26102;&#38388;&#25928;&#29575;&#25552;&#39640;&#20102;2.4&#20493;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#31454;&#20105;&#24615;&#30340;&#30446;&#26631;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.17020</link><description>&lt;p&gt;
Diable: &#22312;&#34920;&#26684;&#19978;&#36827;&#34892;&#30340;&#39640;&#25928;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
Diable: Efficient Dialogue State Tracking as Operations on Tables. (arXiv:2305.17020v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17020
&lt;/p&gt;
&lt;p&gt;
Diable&#26159;&#19968;&#20010;&#39640;&#25928;&#30340;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#31995;&#32479;&#65292;&#23427;&#36890;&#36807;&#22312;&#34920;&#26684;&#19978;&#36827;&#34892;&#25805;&#20316;&#26469;&#26356;&#26032;&#23545;&#35805;&#29366;&#24577;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#26102;&#38388;&#25928;&#29575;&#25552;&#39640;&#20102;2.4&#20493;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#31454;&#20105;&#24615;&#30340;&#30446;&#26631;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#31995;&#32479;&#23558;&#23436;&#25972;&#30340;&#23545;&#35805;&#21382;&#21490;&#20316;&#20026;&#36755;&#20837;&#65292;&#23558;&#24403;&#21069;&#29366;&#24577;&#34920;&#31034;&#20026;&#21253;&#21547;&#25152;&#26377;&#27133;&#30340;&#21015;&#34920;&#65292;&#24182;&#22312;&#27599;&#20010;&#23545;&#35805;&#22238;&#21512;&#20013;&#20174;&#22836;&#24320;&#22987;&#29983;&#25104;&#25972;&#20010;&#29366;&#24577;&#12290;&#36825;&#31181;&#26041;&#27861;&#25928;&#29575;&#20302;&#19979;&#65292;&#29305;&#21035;&#26159;&#24403;&#27133;&#30340;&#25968;&#37327;&#24456;&#22810;&#19988;&#23545;&#35805;&#24456;&#38271;&#26102;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Diable&#65292;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#24418;&#24335;&#21270;&#26041;&#27861;&#65292;&#31616;&#21270;&#20102;&#39640;&#25928;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#31995;&#32479;&#30340;&#35774;&#35745;&#21644;&#23454;&#29616;&#65292;&#24182;&#19988;&#21487;&#20197;&#36731;&#26494;&#22320;&#23884;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#23558;&#23545;&#35805;&#29366;&#24577;&#34920;&#31034;&#20026;&#34920;&#26684;&#65292;&#24182;&#23558;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#24418;&#24335;&#21270;&#20026;&#34920;&#26684;&#25805;&#20316;&#20219;&#21153;&#12290;&#22312;&#27599;&#20010;&#22238;&#21512;&#20013;&#65292;&#31995;&#32479;&#36890;&#36807;&#22522;&#20110;&#23545;&#35805;&#19978;&#19979;&#25991;&#29983;&#25104;&#34920;&#26684;&#25805;&#20316;&#26469;&#26356;&#26032;&#20808;&#21069;&#30340;&#29366;&#24577;&#12290;&#22312;MultiWoz&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;&#65292;Diable (i) &#20248;&#20110;&#24378;&#22823;&#30340;&#39640;&#25928;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#22522;&#20934;&#65292;(ii) &#26102;&#38388;&#25928;&#29575;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;2.4&#20493;&#65292;&#21516;&#26102;&#20445;&#25345;&#31454;&#20105;&#24615;&#30340;&#32852;&#21512;&#30446;&#26631;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;(iii) &#23545;&#26080;&#22122;&#22768;&#30340;&#36755;&#20837;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequence-to-sequence state-of-the-art systems for dialogue state tracking (DST) use the full dialogue history as input, represent the current state as a list with all the slots, and generate the entire state from scratch at each dialogue turn. This approach is inefficient, especially when the number of slots is large and the conversation is long. We propose Diable, a new task formalisation that simplifies the design and implementation of efficient DST systems and allows one to easily plug and play large language models. We represent the dialogue state as a table and formalise DST as a table manipulation task. At each turn, the system updates the previous state by generating table operations based on the dialogue context. Extensive experimentation on the MultiWoz datasets demonstrates that Diable (i) outperforms strong efficient DST baselines, (ii) is 2.4x more time efficient than current state-of-the-art methods while retaining competitive Joint Goal Accuracy, and (iii) is robust to no
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;GAN&#26469;&#23398;&#20064;&#19968;&#20010;&#21407;&#22411;&#26230;&#26684;&#19978;&#30340;&#38543;&#26426;&#36807;&#31243;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#21512;&#36866;&#30340;&#22810;&#27169;&#22411;&#31243;&#24207;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#31934;&#24230;&#12290;GAN&#20284;&#20046;&#26159;&#22788;&#29702;&#22797;&#26434;&#32479;&#35745;&#21160;&#21147;&#23398;&#38382;&#39064;&#30340;&#26377;&#21069;&#36884;&#30340;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2305.15920</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#27169;&#22411;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#38543;&#26426;&#21160;&#21147;&#23398;&#23398;&#20064;&#21644;&#31934;&#30830;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Learning and accurate generation of stochastic dynamics based on multi-model Generative Adversarial Networks. (arXiv:2305.15920v1 [cond-mat.stat-mech])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15920
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;GAN&#26469;&#23398;&#20064;&#19968;&#20010;&#21407;&#22411;&#26230;&#26684;&#19978;&#30340;&#38543;&#26426;&#36807;&#31243;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#21512;&#36866;&#30340;&#22810;&#27169;&#22411;&#31243;&#24207;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#31934;&#24230;&#12290;GAN&#20284;&#20046;&#26159;&#22788;&#29702;&#22797;&#26434;&#32479;&#35745;&#21160;&#21147;&#23398;&#38382;&#39064;&#30340;&#26377;&#21069;&#36884;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#24050;&#32463;&#22312;&#36828;&#31163;&#29289;&#29702;&#39046;&#22495;&#65292;&#22914;&#25991;&#26412;&#21644;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#23637;&#31034;&#20986;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#26412;&#25991;&#20351;&#29992;GAN&#26469;&#23398;&#20064;&#19968;&#20010;&#21407;&#22411;&#26230;&#26684;&#19978;&#30340;&#38543;&#26426;&#36807;&#31243;&#12290;&#36890;&#36807;&#21512;&#29702;&#22320;&#21521;&#21407;&#22987;&#25968;&#25454;&#28155;&#21152;&#22122;&#22768;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#23558;&#29983;&#25104;&#22120;&#21644;&#37492;&#21035;&#22120;&#25439;&#22833;&#20989;&#25968;&#30340;&#20540;&#24102;&#21040;&#20102;&#23427;&#20204;&#30340;&#29702;&#24819;&#20540;&#38468;&#36817;&#12290;&#28982;&#32780;&#65292;&#20687;&#23545;&#25239;&#24615;&#26041;&#27861;&#19968;&#26679;&#65292;&#38663;&#33633;&#20173;&#28982;&#23384;&#22312;&#12290;&#36825;&#20250;&#30772;&#22351;&#27169;&#22411;&#36873;&#25321;&#21644;&#29983;&#25104;&#36712;&#36857;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#65292;&#19968;&#31181;&#21512;&#36866;&#30340;&#22810;&#27169;&#22411;&#31243;&#24207;&#65292;&#22312;&#27599;&#19968;&#27493;&#38543;&#26426;&#36873;&#25321;&#29983;&#25104;&#22120;&#25512;&#36827;&#38543;&#26426;&#36712;&#36857;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#31934;&#24230;&#12290;&#22522;&#20110;&#20197;&#19978;&#21457;&#29616;&#65292;GAN&#20284;&#20046;&#26159;&#22788;&#29702;&#22797;&#26434;&#32479;&#35745;&#21160;&#21147;&#23398;&#38382;&#39064;&#30340;&#26377;&#21069;&#36884;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Adversarial Networks (GANs) have shown immense potential in fields far from physics, such as in text and image generation. Here we use GANs to learn a prototypical stochastic process on a lattice. By suitably adding noise to the original data we succeed in bringing both the Generator and the Discriminator loss functions close to their ideal value. However, as typical for adversarial approaches, oscillations persist. This undermines model selection and the quality of the generated trajectory. We demonstrate that a suitable multi-model procedure where stochastic trajectories are advanced at each step upon randomly selecting a Generator leads to a remarkable increase in accuracy. Based on the reported findings GANs appears as a promising tool to tackle complex statistical dynamics.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AlpacaFarm&#30340;&#20302;&#25104;&#26412;&#27169;&#25311;&#22120;&#65292;&#35813;&#27169;&#25311;&#22120;&#20026;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#35774;&#35745;LLM&#25552;&#31034;&#26469;&#27169;&#25311;&#20154;&#31867;&#21453;&#39304;&#65292;&#25552;&#20986;&#33258;&#21160;&#35780;&#20272;&#24182;&#25552;&#20379;&#21442;&#32771;&#23454;&#29616;&#65292;&#20811;&#26381;&#20102;&#25968;&#25454;&#25910;&#38598;&#30340;&#39640;&#26114;&#25104;&#26412;&#12289;&#32570;&#20047;&#21487;&#20449;&#30340;&#35780;&#20272;&#21644;&#32570;&#20047;&#21442;&#32771;&#26041;&#27861;&#23454;&#29616;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.14387</link><description>&lt;p&gt;
AlpacaFarm: &#19968;&#31181;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#26041;&#27861;&#27169;&#25311;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback. (arXiv:2305.14387v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14387
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AlpacaFarm&#30340;&#20302;&#25104;&#26412;&#27169;&#25311;&#22120;&#65292;&#35813;&#27169;&#25311;&#22120;&#20026;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#35774;&#35745;LLM&#25552;&#31034;&#26469;&#27169;&#25311;&#20154;&#31867;&#21453;&#39304;&#65292;&#25552;&#20986;&#33258;&#21160;&#35780;&#20272;&#24182;&#25552;&#20379;&#21442;&#32771;&#23454;&#29616;&#65292;&#20811;&#26381;&#20102;&#25968;&#25454;&#25910;&#38598;&#30340;&#39640;&#26114;&#25104;&#26412;&#12289;&#32570;&#20047;&#21487;&#20449;&#30340;&#35780;&#20272;&#21644;&#32570;&#20047;&#21442;&#32771;&#26041;&#27861;&#23454;&#29616;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#22240;&#20854;&#33391;&#22909;&#30340;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#32780;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#24320;&#21457;&#36825;&#20123;LLMs&#38656;&#35201;&#20351;&#29992;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#35757;&#32451;&#30340;&#22797;&#26434;&#19988;&#23578;&#19981;&#26126;&#30830;&#30340;&#24037;&#20316;&#27969;&#31243;&#12290;&#23558;&#27492;&#25351;&#20196;&#36319;&#38543;&#36807;&#31243;&#22797;&#21046;&#21644;&#29702;&#35299;&#38754;&#20020;&#19977;&#22823;&#25361;&#25112;&#65306; &#25968;&#25454;&#25910;&#38598;&#30340;&#39640;&#26114;&#25104;&#26412;&#65292;&#32570;&#20047;&#21487;&#20449;&#30340;&#35780;&#20272;&#21644;&#32570;&#20047;&#21442;&#32771;&#26041;&#27861;&#23454;&#29616;&#12290;&#25105;&#20204;&#36890;&#36807;AlpacaFarm&#35299;&#20915;&#20102;&#36825;&#20123;&#25361;&#25112;&#65292;&#36825;&#26159;&#19968;&#20010;&#20302;&#25104;&#26412;&#30340;&#27169;&#25311;&#22120;&#65292;&#21487;&#29992;&#20110;&#20174;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#12290;&#31532;&#19968;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;LLM&#25552;&#31034;&#26469;&#27169;&#25311;&#20154;&#31867;&#21453;&#39304;&#65292;&#20854;&#25104;&#26412;&#27604;&#20247;&#21253;&#24037;&#20316;&#32773;&#20415;&#23452;45&#20493;&#65292;&#24182;&#19988;&#19982;&#20154;&#31867;&#21453;&#39304;&#20855;&#26377;&#39640;&#24230;&#19968;&#33268;&#24615;&#12290;&#31532;&#20108;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#19982;&#30495;&#23454;&#19990;&#30028;&#20132;&#20114;&#20013;&#33719;&#24471;&#30340;&#20154;&#31867;&#25351;&#20196;&#36827;&#34892;&#39564;&#35777;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#20026;&#20960;&#31181;&#20174;&#37197;&#23545;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#26041;&#27861;&#65288;PPO&#65292;best-of-n&#65292;expert iteration&#31561;&#65289;&#25552;&#20379;&#20102;&#21442;&#32771;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) such as ChatGPT have seen widespread adoption due to their ability to follow user instructions well. Developing these LLMs involves a complex yet poorly understood workflow requiring training with human feedback. Replicating and understanding this instruction-following process faces three major challenges: the high cost of data collection, the lack of trustworthy evaluation, and the absence of reference method implementations. We address these challenges with AlpacaFarm, a simulator that enables research and development for learning from feedback at a low cost. First, we design LLM prompts to simulate human feedback that are 45x cheaper than crowdworkers and display high agreement with humans. Second, we propose an automatic evaluation and validate it against human instructions obtained on real-world interactions. Third, we contribute reference implementations for several methods (PPO, best-of-n, expert iteration, and more) that learn from pairwise feedback
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;GELU&#28608;&#27963;&#20989;&#25968;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#25968;&#23398;&#20998;&#26512;&#21644;&#24191;&#27867;&#30340;&#23454;&#39564;&#27604;&#36739;&#65292;&#35777;&#26126;&#20102;&#23427;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#20855;&#26377;&#20248;&#36234;&#30340;&#24615;&#33021;&#21644;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.12073</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;GELU&#28608;&#27963;&#20989;&#25968;&#65306;&#20840;&#38754;&#30340;&#25968;&#23398;&#20998;&#26512;&#21644;&#24615;&#33021;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
GELU Activation Function in Deep Learning: A Comprehensive Mathematical Analysis and Performance. (arXiv:2305.12073v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12073
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;GELU&#28608;&#27963;&#20989;&#25968;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#25968;&#23398;&#20998;&#26512;&#21644;&#24191;&#27867;&#30340;&#23454;&#39564;&#27604;&#36739;&#65292;&#35777;&#26126;&#20102;&#23427;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#20855;&#26377;&#20248;&#36234;&#30340;&#24615;&#33021;&#21644;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#65292;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#28608;&#27963;&#20989;&#25968;&#26159;&#24433;&#21709;&#20854;&#23398;&#20064;&#33021;&#21147;&#12289;&#31283;&#23450;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#36817;&#24180;&#26469;&#65292;&#39640;&#26031;&#35823;&#24046;&#32447;&#24615;&#21333;&#20803;&#65288;GELU&#65289;&#28608;&#27963;&#20989;&#25968;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#20027;&#27969;&#26041;&#27861;&#65292;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#22914;&#20462;&#27491;&#32447;&#24615;&#21333;&#20803;&#65288;ReLU&#65289;&#12290;&#26412;&#25991;&#23545;GELU&#28608;&#27963;&#20989;&#25968;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#25968;&#23398;&#20998;&#26512;&#65292;&#35814;&#32454;&#25506;&#35752;&#20102;&#20854;&#21487;&#24494;&#24615;&#12289;&#26377;&#30028;&#24615;&#12289;&#24179;&#31283;&#24615;&#21644;&#20809;&#28369;&#24615;&#31561;&#24615;&#36136;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;GELU&#20989;&#25968;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#27604;&#36739;&#65292;&#21033;&#29992;&#22312;CIFAR-10&#12289;CIFAR-100&#21644;STL-10&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27531;&#24046;&#21367;&#31215;&#32593;&#32476;&#20316;&#20026;&#23454;&#35777;&#27979;&#35797;&#22522;&#30784;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#35777;&#26126;&#20102;GELU&#30456;&#23545;&#20110;&#20854;&#20182;&#28608;&#27963;&#20989;&#25968;&#30340;&#21331;&#36234;&#24615;&#33021;&#65292;&#30830;&#31435;&#20102;&#23427;&#22312;&#24191;&#27867;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Selecting the most suitable activation function is a critical factor in the effectiveness of deep learning models, as it influences their learning capacity, stability, and computational efficiency. In recent years, the Gaussian Error Linear Unit (GELU) activation function has emerged as a dominant method, surpassing traditional functions such as the Rectified Linear Unit (ReLU) in various applications. This study presents a rigorous mathematical investigation of the GELU activation function, exploring its differentiability, boundedness, stationarity, and smoothness properties in detail. Additionally, we conduct an extensive experimental comparison of the GELU function against a broad range of alternative activation functions, utilizing a residual convolutional network trained on the CIFAR-10, CIFAR-100, and STL-10 datasets as the empirical testbed. Our results demonstrate the superior performance of GELU compared to other activation functions, establishing its suitability for a wide ra
&lt;/p&gt;</description></item><item><title>&#36830;&#32493;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#38754;&#20020;&#30528;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#25361;&#25112;&#65292;&#38656;&#35201;&#35299;&#20915;&#26032;&#22686;&#23454;&#20307;&#21644;&#20851;&#31995;&#20197;&#21450;&#22810;&#27169;&#24577;&#28304;&#25968;&#25454;&#21464;&#21270;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.08698</link><description>&lt;p&gt;
&#36830;&#32493;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
Continual Multimodal Knowledge Graph Construction. (arXiv:2305.08698v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08698
&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#38754;&#20020;&#30528;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#25361;&#25112;&#65292;&#38656;&#35201;&#35299;&#20915;&#26032;&#22686;&#23454;&#20307;&#21644;&#20851;&#31995;&#20197;&#21450;&#22810;&#27169;&#24577;&#28304;&#25968;&#25454;&#21464;&#21270;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#65288;MKGC&#65289;&#28041;&#21450;&#20351;&#29992;&#22810;&#31181;&#24418;&#24335;&#30340;&#25968;&#25454;&#65292;&#22914;&#25991;&#26412;&#21644;&#22270;&#20687;&#65292;&#21019;&#24314;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#32467;&#26500;&#21270;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;MKGC&#27169;&#22411;&#22312;&#22788;&#29702;&#21160;&#24577;&#29616;&#23454;&#22330;&#26223;&#20013;&#26032;&#22686;&#23454;&#20307;&#21644;&#20851;&#31995;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#30446;&#21069;&#30340;&#36830;&#32493;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#35774;&#32622;&#20027;&#35201;&#20851;&#27880;&#20174;&#25991;&#26412;&#25968;&#25454;&#20013;&#25552;&#21462;&#23454;&#20307;&#21644;&#20851;&#31995;&#65292;&#24573;&#35270;&#20102;&#20854;&#20182;&#22810;&#27169;&#24577;&#28304;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#25506;&#32034;&#36830;&#32493;MKGC&#30340;&#25361;&#25112;&#65292;&#20197;&#35299;&#20915;&#28798;&#38590;&#24615;&#36951;&#24536;&#29616;&#35937;&#65292;&#24182;&#30830;&#20445;&#20445;&#30041;&#20174;&#19981;&#21516;&#24418;&#24335;&#30340;&#25968;&#25454;&#20013;&#25552;&#21462;&#30340;&#36807;&#21435;&#30693;&#35782;&#12290;&#26412;&#30740;&#31350;&#33268;&#21147;&#20110;&#36890;&#36807;&#24320;&#21457;&#32456;&#36523;MKGC&#22522;&#20934;&#25968;&#25454;&#38598;&#26469;&#30740;&#31350;&#36825;&#20010;&#22797;&#26434;&#30340;&#20027;&#39064;&#12290;&#22522;&#20110;&#32463;&#39564;&#21457;&#29616;&#65292;&#24403;&#22810;&#23186;&#20307;&#25968;&#25454;&#35757;&#32451;&#26102;&#65292;&#19968;&#20123;&#20856;&#22411;&#30340;MKGC&#27169;&#22411;&#21487;&#33021;&#22312;&#36830;&#32493;&#35774;&#32622;&#20013;&#24847;&#22806;&#34920;&#29616;&#19981;&#20339;&#65292;&#19982;&#37027;&#20123;&#20165;&#21033;&#29992;&#25991;&#26412;&#36164;&#28304;&#30340;&#27169;&#22411;&#30456;&#27604;&#12290;&#25105;&#20204;&#20197;&#23454;&#39564;&#35777;&#25454;&#20026;&#22522;&#30784;&#65292;&#24635;&#32467;&#20986;&#20197;&#19979;&#35770;&#28857;&#65306;&#36830;&#32493;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#38754;&#20020;&#30528;&#25968;&#25454;&#28304;&#21464;&#21270;&#23548;&#33268;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal Knowledge Graph Construction (MKGC) involves creating structured representations of entities and relations using multiple modalities, such as text and images. However, existing MKGC models face challenges in handling the addition of new entities and relations in dynamic real-world scenarios. The current continual setting for knowledge graph construction mainly focuses on entity and relation extraction from text data, overlooking other multimodal sources. Therefore, there arises the need to explore the challenge of continual MKGC to address the phenomenon of catastrophic forgetting and ensure the retention of past knowledge extracted from different forms of data. This research focuses on investigating this complex topic by developing lifelong MKGC benchmark datasets. Based on the empirical findings that several typical MKGC models, when trained on multimedia data, might unexpectedly underperform compared to those solely utilizing textual resources in a continual setting, we p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#22240;&#26524;&#26426;&#21046;&#22312;&#19981;&#21516;&#29615;&#22659;&#19979;&#20445;&#25345;&#19981;&#21464;&#30340;&#30452;&#35273;&#26469;&#20027;&#21160;&#20934;&#22791;&#30340;&#20195;&#29702;&#29305;&#24449;&#36873;&#25321;&#21644;&#24037;&#31243;&#25216;&#26415;&#65292;&#29992;&#20197;&#24212;&#23545;&#32479;&#35745;&#39044;&#27979;&#27169;&#22411;&#22312;&#20998;&#24067;&#36716;&#31227;&#24773;&#20917;&#19979;&#30340;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.05832</link><description>&lt;p&gt;
&#22240;&#26524;&#20449;&#24687;&#20998;&#31163;&#65306;&#20026;&#25239;&#20998;&#24067;&#36716;&#31227;&#35774;&#35745;&#20195;&#29702;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Causal Information Splitting: Engineering Proxy Features for Robustness to Distribution Shifts. (arXiv:2305.05832v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05832
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#22240;&#26524;&#26426;&#21046;&#22312;&#19981;&#21516;&#29615;&#22659;&#19979;&#20445;&#25345;&#19981;&#21464;&#30340;&#30452;&#35273;&#26469;&#20027;&#21160;&#20934;&#22791;&#30340;&#20195;&#29702;&#29305;&#24449;&#36873;&#25321;&#21644;&#24037;&#31243;&#25216;&#26415;&#65292;&#29992;&#20197;&#24212;&#23545;&#32479;&#35745;&#39044;&#27979;&#27169;&#22411;&#22312;&#20998;&#24067;&#36716;&#31227;&#24773;&#20917;&#19979;&#30340;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32479;&#35745;&#39044;&#27979;&#27169;&#22411;&#36890;&#24120;&#26159;&#22312;&#19982;&#26368;&#32456;&#20351;&#29992;&#24773;&#20917;&#19981;&#21516;&#30340;&#27010;&#29575;&#20998;&#24067;&#20013;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#20026;&#20102;&#39044;&#27979;&#20998;&#24067;&#36716;&#31227;&#65292;&#26377;&#19968;&#31181;&#26041;&#27861;&#26159;&#21033;&#29992;&#22240;&#26524;&#26426;&#21046;&#22312;&#19981;&#21516;&#29615;&#22659;&#19979;&#20445;&#25345;&#19981;&#21464;&#30340;&#30452;&#35273;&#26469;&#20027;&#21160;&#20934;&#22791;&#12290;&#26412;&#25991;&#38024;&#23545;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22330;&#26223;&#65292;&#20854;&#20013;&#30446;&#26631;&#30340;&#22240;&#26524;&#21644;&#21453;&#22240;&#26524;&#21464;&#37327;&#37117;&#26159;&#26410;&#34987;&#35266;&#23519;&#21040;&#30340;&#12290;&#21033;&#29992;&#20449;&#24687;&#35770;&#65292;&#25105;&#20204;&#20026;&#19979;&#28216;&#35266;&#27979;&#21464;&#37327;&#24320;&#21457;&#20102;&#29305;&#24449;&#36873;&#25321;&#21644;&#24037;&#31243;&#25216;&#26415;&#65292;&#36825;&#20123;&#21464;&#37327;&#20805;&#24403;&#20195;&#29702;&#12290;&#25105;&#20204;&#36873;&#25321;&#26377;&#21161;&#20110;&#24314;&#31435;&#31283;&#23450;&#27169;&#22411;&#30340;&#20195;&#29702;&#65292;&#24182;&#20351;&#29992;&#36741;&#21161;&#35757;&#32451;&#20219;&#21153;&#20174;&#20195;&#29702;&#20013;&#25552;&#21462;&#22686;&#24378;&#31283;&#23450;&#24615;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#30495;&#23454;&#25968;&#25454;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Statistical prediction models are often trained on data that is drawn from different probability distributions than their eventual use cases. One approach to proactively prepare for these shifts harnesses the intuition that causal mechanisms should remain invariant between environments. Here we focus on a challenging setting in which the causal and anticausal variables of the target are unobserved. Leaning on information theory, we develop feature selection and engineering techniques for the observed downstream variables that act as proxies. We identify proxies that help to build stable models and moreover utilize auxiliary training tasks to extract stability-enhancing information from proxies. We demonstrate the effectiveness of our techniques on synthetic and real data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026; FedNoRo &#30340;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#26631;&#31614;&#22122;&#22768;&#24322;&#36136;&#24615;&#30340;&#32852;&#37030;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#22312; ICH &#21644; ISIC2019 &#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.05230</link><description>&lt;p&gt;
FedNoRo: &#38024;&#23545;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#26631;&#31614;&#22122;&#22768;&#24322;&#36136;&#24615;&#30340;&#22122;&#22768;-&#40065;&#26834;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FedNoRo: Towards Noise-Robust Federated Learning by Addressing Class Imbalance and Label Noise Heterogeneity. (arXiv:2305.05230v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05230
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026; FedNoRo &#30340;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#26631;&#31614;&#22122;&#22768;&#24322;&#36136;&#24615;&#30340;&#32852;&#37030;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#22312; ICH &#21644; ISIC2019 &#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;(FNLL)&#27491;&#22312;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#38544;&#31169;&#20445;&#25252;&#30340;&#22810;&#28304;&#20998;&#25955;&#23398;&#20064;&#24037;&#20855;&#12290;&#29616;&#26377;&#30740;&#31350;&#22522;&#20110;&#20840;&#23616;&#25968;&#25454;&#31867;&#21035;&#24179;&#34913;&#30340;&#20551;&#35774;&#65292;&#21487;&#33021;&#26080;&#27861;&#24314;&#27169;&#22797;&#26434;&#30340;&#26631;&#31614;&#22122;&#22768;&#65292;&#29305;&#21035;&#26159;&#22312;&#21307;&#23398;&#22330;&#26223;&#20013;&#12290;&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#20026;&#30495;&#23454;&#30340;&#32852;&#37030;&#26631;&#31614;&#22122;&#22768;&#38382;&#39064;&#65292;&#20854;&#20013;&#20840;&#23616;&#25968;&#25454;&#26159;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#65292;&#24182;&#19988;&#26631;&#31614;&#22122;&#22768;&#26159;&#24322;&#36136;&#30340;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026; FedNoRo &#30340;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#29992;&#20110;&#22122;&#22768;-&#40065;&#26834;&#32852;&#37030;&#23398;&#20064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312; FedNoRo &#30340;&#31532;&#19968;&#38454;&#27573;&#65292;&#37319;&#29992;&#27599;&#31867;&#25439;&#22833;&#25351;&#26631;&#20043;&#21518;&#36319;&#38543;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#36827;&#34892;&#22024;&#26434;&#23458;&#25143;&#31471;&#35782;&#21035;&#12290;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;&#21516;&#26102;&#37319;&#29992;&#30693;&#35782;&#33976;&#39311;&#21644;&#36317;&#31163;&#24863;&#30693;&#32858;&#21512;&#20989;&#25968;&#36827;&#34892;&#22122;&#22768;-&#40065;&#26834;&#32852;&#37030;&#27169;&#22411;&#26356;&#26032;&#12290;&#23545;&#24191;&#27867;&#20351;&#29992;&#30340; ICH &#21644; ISIC2019 &#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;FedNoRo &#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340; FNLL &#26041;&#27861;&#22312;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#26631;&#31614;&#22122;&#22768;&#24322;&#36136;&#24615;&#26041;&#38754;&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated noisy label learning (FNLL) is emerging as a promising tool for privacy-preserving multi-source decentralized learning. Existing research, relying on the assumption of class-balanced global data, might be incapable to model complicated label noise, especially in medical scenarios. In this paper, we first formulate a new and more realistic federated label noise problem where global data is class-imbalanced and label noise is heterogeneous, and then propose a two-stage framework named FedNoRo for noise-robust federated learning. Specifically, in the first stage of FedNoRo, per-class loss indicators followed by Gaussian Mixture Model are deployed for noisy client identification. In the second stage, knowledge distillation and a distance-aware aggregation function are jointly adopted for noise-robust federated model updating. Experimental results on the widely-used ICH and ISIC2019 datasets demonstrate the superiority of FedNoRo against the state-of-the-art FNLL methods for addre
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;&#36328;&#27169;&#24577;&#38899;&#39057;-&#25991;&#26412;&#34920;&#24449;&#23398;&#20064;&#65292;&#36890;&#36807;&#20351;&#29992;&#39046;&#22495;&#29305;&#23450;&#30340;&#31579;&#36873;&#21644;&#36719;&#26631;&#27880;&#23545;&#27604;&#24615;&#25439;&#22833;&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;&#38646;-shot&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.01864</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#25913;&#36827;&#38899;&#39057;-&#25991;&#26412;&#36328;&#27169;&#24577;&#34920;&#24449;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Improvement of Audio-Text Cross-Modal Representations. (arXiv:2305.01864v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01864
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;&#36328;&#27169;&#24577;&#38899;&#39057;-&#25991;&#26412;&#34920;&#24449;&#23398;&#20064;&#65292;&#36890;&#36807;&#20351;&#29992;&#39046;&#22495;&#29305;&#23450;&#30340;&#31579;&#36873;&#21644;&#36719;&#26631;&#27880;&#23545;&#27604;&#24615;&#25439;&#22833;&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;&#38646;-shot&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#36890;&#36807;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#33719;&#24471;&#36328;&#27169;&#24577;&#38899;&#39057;-&#25991;&#26412;&#34920;&#24449;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20811;&#26381;&#20102;&#20351;&#29992;&#39044;&#23450;&#20041;&#26631;&#31614;&#30340;&#20256;&#32479;&#35757;&#32451;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;&#36825;&#20351;&#24471;&#31038;&#21306;&#33021;&#22815;&#22312;&#38646;-shot&#20998;&#31867;&#31561;&#20219;&#21153;&#19978;&#21462;&#24471;&#36827;&#23637;&#65292;&#21542;&#21017;&#26159;&#19981;&#21487;&#33021;&#30340;&#12290;&#28982;&#32780;&#65292;&#23398;&#20064;&#36825;&#26679;&#30340;&#34920;&#24449;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#24037;&#27880;&#37322;&#30340;&#38899;&#39057;-&#25991;&#26412;&#23545;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26410;&#37197;&#23545;&#25991;&#26412;&#21644;&#38899;&#39057;&#25913;&#36827;&#36825;&#20123;&#34920;&#24449;&#23398;&#20064;&#26694;&#26550;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#39046;&#22495;&#38750;&#29305;&#23450;&#21644;&#39046;&#22495;&#29305;&#23450;&#30340;&#31579;&#36873;&#26041;&#27861;&#65292;&#21019;&#24314;&#25105;&#20204;&#29992;&#20110;&#36827;&#19968;&#27493;&#25913;&#36827;&#27169;&#22411;&#30340;&#38899;&#39057;-&#25991;&#26412;&#23545;&#12290;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#24403;&#19982;&#36719;&#26631;&#27880;&#23545;&#27604;&#24615;&#25439;&#22833;&#32467;&#21512;&#20351;&#29992;&#39046;&#22495;&#29305;&#23450;&#31579;&#36873;&#26102;&#65292;&#25105;&#20204;&#33021;&#22815;&#22312;&#19979;&#28216;&#22768;&#38899;&#20107;&#20214;&#20998;&#31867;&#25110;&#22768;&#23398;&#22330;&#26223;&#20998;&#31867;&#20219;&#21153;&#30340;&#38646;-shot&#20998;&#31867;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in using language models to obtain cross-modal audio-text representations have overcome the limitations of conventional training approaches that use predefined labels. This has allowed the community to make progress in tasks like zero-shot classification, which would otherwise not be possible. However, learning such representations requires a large amount of human-annotated audio-text pairs. In this paper, we study unsupervised approaches to improve the learning framework of such representations with unpaired text and audio. We explore domain-unspecific and domain-specific curation methods to create audio-text pairs that we use to further improve the model. We also show that when domain-specific curation is used in conjunction with a soft-labeled contrastive loss, we are able to obtain significant improvement in terms of zero-shot classification performance on downstream sound event classification or acoustic scene classification tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312; LDP &#19979;&#25910;&#38598;&#22810;&#20010;&#25935;&#24863;&#23646;&#24615;&#23545;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#24182;&#25552;&#20986;&#20102;&#32771;&#34385;&#22495;&#22823;&#23567;&#30340;&#26032;&#30340;&#38544;&#31169;&#39044;&#31639;&#20998;&#37197;&#26041;&#26696;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#26696;&#22312;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#20844;&#24179;&#24615;&#26041;&#38754;&#22343;&#20248;&#20110;&#26368;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;LDP &#24102;&#26469;&#20102;&#30053;&#24494;&#25913;&#21892;&#30340;&#20844;&#24179;&#24615;&#32780;&#19981;&#20250;&#26126;&#26174;&#24433;&#21709;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.12845</link><description>&lt;p&gt;
&#65288;&#26412;&#22320;&#65289;&#24046;&#20998;&#38544;&#31169;&#23545;&#20844;&#24179;&#24615;&#27809;&#26377;&#24102;&#26469;&#19981;&#24179;&#31561;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
(Local) Differential Privacy has NO Disparate Impact on Fairness. (arXiv:2304.12845v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12845
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312; LDP &#19979;&#25910;&#38598;&#22810;&#20010;&#25935;&#24863;&#23646;&#24615;&#23545;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#24182;&#25552;&#20986;&#20102;&#32771;&#34385;&#22495;&#22823;&#23567;&#30340;&#26032;&#30340;&#38544;&#31169;&#39044;&#31639;&#20998;&#37197;&#26041;&#26696;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#26696;&#22312;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#20844;&#24179;&#24615;&#26041;&#38754;&#22343;&#20248;&#20110;&#26368;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;LDP &#24102;&#26469;&#20102;&#30053;&#24494;&#25913;&#21892;&#30340;&#20844;&#24179;&#24615;&#32780;&#19981;&#20250;&#26126;&#26174;&#24433;&#21709;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#65288;LDP&#65289;&#20316;&#20026;&#24378;&#22823;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807; LDP&#65292;&#29992;&#25143;&#21487;&#20197;&#22312;&#23558;&#25968;&#25454;&#20256;&#36755;&#20986;&#21435;&#21069;&#22312;&#35774;&#22791;&#19978;&#23545;&#20854;&#36827;&#34892;&#25200;&#21160;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#22312;&#21508;&#20010;&#34892;&#19994;&#20013;&#25910;&#38598;&#22810;&#20010;&#25935;&#24863;&#20449;&#24687;&#30340;&#24773;&#20917;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#20165;&#25910;&#38598;&#21333;&#20010;&#25935;&#24863;&#23646;&#24615;&#21487;&#33021;&#24050;&#32463;&#19981;&#36275;&#20197;&#20445;&#38556;&#29992;&#25143;&#30340;&#38544;&#31169;&#12290;&#25968;&#25454;&#20013;&#30340;&#30456;&#20851;&#23646;&#24615;&#20173;&#28982;&#21487;&#33021;&#23548;&#33268;&#23545;&#25935;&#24863;&#23646;&#24615;&#30340;&#25512;&#26029;&#12290;&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#22312; LDP &#19979;&#25910;&#38598;&#22810;&#20010;&#25935;&#24863;&#23646;&#24615;&#23545;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38544;&#31169;&#39044;&#31639;&#20998;&#37197;&#26041;&#26696;&#65292;&#32771;&#34385;&#21040;&#25935;&#24863;&#23646;&#24615;&#30340;&#19981;&#21516;&#22495;&#22823;&#23567;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#36825;&#36890;&#24120;&#27604;&#29616;&#26377;&#26368;&#26032;&#35299;&#20915;&#26041;&#26696;&#30340;&#38544;&#31169;-&#25928;&#29992;-&#20844;&#24179;&#24615;&#26435;&#34913;&#26041;&#24335;&#26356;&#22909;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;LDP &#22312;&#23398;&#20064;&#38382;&#39064;&#20013;&#24102;&#26469;&#20102;&#30053;&#24494;&#25913;&#21892;&#30340;&#20844;&#24179;&#24615;&#65292;&#32780;&#19981;&#20250;&#26126;&#26174;&#24433;&#21709;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, Local Differential Privacy (LDP), a robust privacy-preserving methodology, has gained widespread adoption in real-world applications. With LDP, users can perturb their data on their devices before sending it out for analysis. However, as the collection of multiple sensitive information becomes more prevalent across various industries, collecting a single sensitive attribute under LDP may not be sufficient. Correlated attributes in the data may still lead to inferences about the sensitive attribute. This paper empirically studies the impact of collecting multiple sensitive attributes under LDP on fairness. We propose a novel privacy budget allocation scheme that considers the varying domain size of sensitive attributes. This generally led to a better privacy-utility-fairness trade-off in our experiments than the state-of-art solution. Our results show that LDP leads to slightly improved fairness in learning problems without significantly affecting the performance of the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#36229;&#25289;&#26222;&#25289;&#26031;&#27491;&#21017;&#21270;&#27010;&#24565;&#22240;&#24335;&#20998;&#35299;&#30340;&#22810;&#35270;&#35282;&#32858;&#31867;&#26041;&#27861;&#65292;&#22312;&#20302;&#31209;&#24352;&#37327;&#31354;&#38388;&#20013;&#23545;&#27599;&#20010;&#35270;&#35282;&#36827;&#34892;&#32858;&#31867;&#24182;&#25552;&#21462;&#38750;&#32447;&#24615;&#23616;&#37096;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2304.11435</link><description>&lt;p&gt;
&#20302;&#31209;&#24352;&#37327;&#31354;&#38388;&#20013;&#30340;&#36229;&#25289;&#26222;&#25289;&#26031;&#27491;&#21017;&#21270;&#27010;&#24565;&#22240;&#24335;&#20998;&#35299;&#22810;&#35270;&#35282;&#32858;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Hyper-Laplacian Regularized Concept Factorization in Low-rank Tensor Space for Multi-view Clustering. (arXiv:2304.11435v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11435
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#36229;&#25289;&#26222;&#25289;&#26031;&#27491;&#21017;&#21270;&#27010;&#24565;&#22240;&#24335;&#20998;&#35299;&#30340;&#22810;&#35270;&#35282;&#32858;&#31867;&#26041;&#27861;&#65292;&#22312;&#20302;&#31209;&#24352;&#37327;&#31354;&#38388;&#20013;&#23545;&#27599;&#20010;&#35270;&#35282;&#36827;&#34892;&#32858;&#31867;&#24182;&#25552;&#21462;&#38750;&#32447;&#24615;&#23616;&#37096;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#24352;&#37327;&#30340;&#22810;&#35270;&#35282;&#23376;&#31354;&#38388;&#32858;&#31867;&#26041;&#27861;&#24050;&#32463;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#33021;&#22815;&#35780;&#20272;&#39640;&#38454;&#30456;&#20851;&#24615;&#24182;&#25552;&#39640;&#22810;&#35270;&#35282;&#25968;&#25454;&#30340;&#32858;&#31867;&#20998;&#26512;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#22823;&#22810;&#25968;&#23384;&#22312;&#20004;&#20010;&#32570;&#38519;&#12290;&#39318;&#20808;&#65292;&#22522;&#20110;&#33258;&#34920;&#31034;&#30340;&#24352;&#37327;&#23376;&#31354;&#38388;&#23398;&#20064;&#36890;&#24120;&#24341;&#36215;&#39640;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#65292;&#24182;&#19988;&#23616;&#38480;&#20110;&#24863;&#30693;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#38750;&#32447;&#24615;&#23616;&#37096;&#32467;&#26500;&#12290;&#20854;&#27425;&#65292;&#24352;&#37327;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;t-SVD&#65289;&#27169;&#22411;&#23558;&#27599;&#20010;&#22855;&#24322;&#20540;&#31561;&#20998;&#22320;&#37325;&#26032;&#20998;&#24067;&#65292;&#32780;&#26080;&#27861;&#32771;&#34385;&#23427;&#20204;&#20043;&#38388;&#30340;&#19981;&#21516;&#37325;&#35201;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#31209;&#24352;&#37327;&#31354;&#38388;&#20013;&#30340;&#36229;&#25289;&#26222;&#25289;&#26031;&#27491;&#21017;&#21270;&#27010;&#24565;&#22240;&#24335;&#20998;&#35299;&#65288;HLRCF&#65289;&#22810;&#35270;&#35282;&#32858;&#31867;&#26041;&#27861;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#23558;&#27010;&#24565;&#22240;&#24335;&#20998;&#35299;&#24212;&#29992;&#20110;&#25506;&#32034;&#27599;&#20010;&#35270;&#35282;&#30340;&#28508;&#22312;&#32858;&#31867;&#34920;&#31034;&#12290;&#36827;&#19968;&#27493;&#22320;&#65292;&#36229;&#22270;&#25289;&#26222;&#25289;&#26031;&#27491;&#21017;&#21270;&#36171;&#20104;&#20102;&#35813;&#27169;&#22411;&#25552;&#21462;&#38750;&#32447;&#24615;&#23616;&#37096;&#32467;&#26500;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tensor-oriented multi-view subspace clustering has achieved significant strides in assessing high-order correlations and improving clustering analysis of multi-view data. Nevertheless, most of existing investigations are typically hampered by the two flaws. First, self-representation based tensor subspace learning usually induces high time and space complexity, and is limited in perceiving nonlinear local structure in the embedding space. Second, the tensor singular value decomposition (t-SVD) model redistributes each singular value equally without considering the diverse importance among them. To well cope with the issues, we propose a hyper-Laplacian regularized concept factorization (HLRCF) in low-rank tensor space for multi-view clustering. Specifically, we adopt the concept factorization to explore the latent cluster-wise representation of each view. Further, the hypergraph Laplacian regularization endows the model with the capability of extracting the nonlinear local structures i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#26680;&#25554;&#20540;&#30340;&#27867;&#21270;&#35823;&#24046;&#26377;&#19968;&#20010;&#19979;&#30028;&#65292;&#22312;&#36739;&#22823;&#33539;&#22260;&#30340;&#26680;&#20989;&#25968;&#20013;&#27867;&#21270;&#33021;&#21147;&#36739;&#24046;&#65292;&#20351;&#24471;&#36807;&#25311;&#21512;&#30340;&#23485;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#24615;&#33021;&#24046;&#12290;</title><link>http://arxiv.org/abs/2303.15809</link><description>&lt;p&gt;
&#26680;&#25554;&#20540;&#30340;&#27867;&#21270;&#33021;&#21147;&#36739;&#24369;
&lt;/p&gt;
&lt;p&gt;
Kernel interpolation generalizes poorly. (arXiv:2303.15809v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15809
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#26680;&#25554;&#20540;&#30340;&#27867;&#21270;&#35823;&#24046;&#26377;&#19968;&#20010;&#19979;&#30028;&#65292;&#22312;&#36739;&#22823;&#33539;&#22260;&#30340;&#26680;&#20989;&#25968;&#20013;&#27867;&#21270;&#33021;&#21147;&#36739;&#24046;&#65292;&#20351;&#24471;&#36807;&#25311;&#21512;&#30340;&#23485;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#24615;&#33021;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26680;&#22238;&#24402;&#30740;&#31350;&#30340;&#22797;&#20852;&#20013;&#65292;&#19968;&#20010;&#26368;&#26377;&#36259;&#30340;&#38382;&#39064;&#21487;&#33021;&#26159;&#26680;&#25554;&#20540;&#26159;&#21542;&#20855;&#26377;&#36739;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#22240;&#20026;&#36825;&#21487;&#33021;&#26377;&#21161;&#20110;&#25105;&#20204;&#29702;&#35299;&#28145;&#24230;&#32593;&#32476;&#39046;&#22495;&#20013;&#30340;&#8220;&#33391;&#24615;&#36807;&#25311;&#21512;&#29616;&#35937;&#8221;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#22312;&#28201;&#21644;&#30340;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#20219;&#20309;$\varepsilon&gt;0$&#65292;&#26680;&#25554;&#20540;&#30340;&#27867;&#21270;&#35823;&#24046;&#37117;&#26377;&#19968;&#20010;&#19979;&#30028;$\Omega(n^{-\varepsilon})$&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#26680;&#25554;&#20540;&#22312;&#36739;&#22823;&#33539;&#22260;&#30340;&#26680;&#20989;&#25968;&#20013;&#27867;&#21270;&#33021;&#21147;&#36739;&#24046;&#12290;&#20316;&#20026;&#19968;&#20010;&#30452;&#25509;&#30340;&#25512;&#35770;&#65292;&#25105;&#20204;&#21487;&#20197;&#35777;&#26126;&#22312;&#29699;&#19978;&#23450;&#20041;&#30340;&#36807;&#25311;&#21512;&#23485;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#20063;&#36739;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the most interesting problems in the recent renaissance of the studies in kernel regression might be whether the kernel interpolation can generalize well, since it may help us understand the `benign overfitting henomenon' reported in the literature on deep networks. In this paper, under mild conditions, we show that for any $\varepsilon&gt;0$, the generalization error of kernel interpolation is lower bounded by $\Omega(n^{-\varepsilon})$. In other words, the kernel interpolation generalizes poorly for a large class of kernels. As a direct corollary, we can show that overfitted wide neural networks defined on sphere generalize poorly.
&lt;/p&gt;</description></item><item><title>FedGH&#26159;&#19968;&#31181;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#23458;&#25143;&#31471;&#25345;&#26377;&#20855;&#26377;&#19981;&#21516;&#32467;&#26500;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#35757;&#32451;&#20849;&#20139;&#30340;&#24191;&#20041;&#20840;&#23616;&#39044;&#27979;&#22836;&#26469;&#25552;&#39640;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.13137</link><description>&lt;p&gt;
FedGH:&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#19982;&#24191;&#20041;&#20840;&#23616;&#22836;
&lt;/p&gt;
&lt;p&gt;
FedGH: Heterogeneous Federated Learning with Generalized Global Header. (arXiv:2303.13137v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13137
&lt;/p&gt;
&lt;p&gt;
FedGH&#26159;&#19968;&#31181;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#23458;&#25143;&#31471;&#25345;&#26377;&#20855;&#26377;&#19981;&#21516;&#32467;&#26500;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#35757;&#32451;&#20849;&#20139;&#30340;&#24191;&#20041;&#20840;&#23616;&#39044;&#27979;&#22836;&#26469;&#25552;&#39640;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;(Federated learning, FL)&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#20801;&#35768;&#22810;&#20010;&#21442;&#19982;&#26041;&#22312;&#38544;&#31169;&#20445;&#25252;&#30340;&#24773;&#20917;&#19979;&#21327;&#20316;&#35757;&#32451;&#20849;&#20139;&#27169;&#22411;&#12290;&#29616;&#26377;&#27178;&#21521;FL&#26041;&#27861;&#36890;&#24120;&#20551;&#23450;FL&#26381;&#21153;&#22120;&#21644;&#23458;&#25143;&#31471;&#25345;&#26377;&#30456;&#21516;&#30340;&#27169;&#22411;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#31995;&#32479;&#24322;&#26500;&#21644;&#20010;&#24615;&#21270;&#38656;&#27714;&#65292;&#20351;&#24471;&#20801;&#35768;&#23458;&#25143;&#31471;&#25345;&#26377;&#20855;&#26377;&#19981;&#21516;&#32467;&#26500;&#30340;&#27169;&#22411;&#24050;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#26041;&#21521;&#12290;&#29616;&#26377;&#30340;&#27169;&#22411;&#24322;&#26500;FL&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#20135;&#29983;&#39640;&#36890;&#20449;&#21644;/&#25110;&#35745;&#31639;&#25104;&#26412;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32852;&#37030;&#20840;&#23616;&#39044;&#27979;&#22836;(FedGH)&#26041;&#27861;&#12290;&#23427;&#26159;&#19968;&#31181;&#36890;&#20449;&#21644;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#27169;&#22411;&#24322;&#26500;FL&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;FL&#26381;&#21153;&#22120;&#19978;&#23545;&#23458;&#25143;&#31471;&#27169;&#22411;&#25552;&#21462;&#30340;&#34920;&#31034;&#36827;&#34892;&#35757;&#32451;&#26469;&#35757;&#32451;&#20849;&#20139;&#30340;&#24191;&#20041;&#20840;&#23616;&#39044;&#27979;&#22836;&#12290;&#36890;&#36807;FedGH&#35757;&#32451;&#30340;&#24191;&#20041;&#20840;&#23616;&#39044;&#27979;&#22836;&#21487;&#20197;&#30452;&#25509;&#37096;&#32626;&#22312;&#23458;&#25143;&#31471;&#35774;&#22791;&#19978;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#26412;&#22320;&#25512;&#29702;&#12290;&#25105;&#20204;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;FedGH&#22312;&#20934;&#30830;&#24615;&#12289;&#36890;&#20449;&#25928;&#29575;&#21644;&#27169;&#22411;&#20010;&#24615;&#21270;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#27169;&#22411;&#24322;&#26500;FL&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is an emerging machine learning paradigm that allows multiple parties to train a shared model collaboratively in a privacy-preserving manner. Existing horizontal FL methods generally assume that the FL server and clients hold the same model structure. However, due to system heterogeneity and the need for personalization, enabling clients to hold models with diverse structures has become an important direction. Existing model-heterogeneous FL approaches often require publicly available datasets and incur high communication and/or computational costs, which limit their performances. To address these limitations, we propose the Federated Global prediction Header (FedGH) approach. It is a communication and computation-efficient model-heterogeneous FL framework which trains a shared generalized global prediction header with representations extracted by heterogeneous extractors for clients' models at the FL server. The trained generalized global prediction header lear
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;mCPT&#27169;&#22411;&#29992;&#20110;&#22810;&#35821;&#35328;&#30340;&#12289;&#22810;&#26631;&#31614;&#30340;&#38646;&#26679;&#26412;&#25110;&#23569;&#26679;&#26412;&#30340;&#26694;&#26550;&#26816;&#27979;&#20219;&#21153;&#65292;&#24182;&#22312;&#35199;&#29677;&#29273;&#35821;&#21644;&#20854;&#20182;8&#31181;&#35821;&#35328;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25104;&#32489;&#12290;&#35813;&#26041;&#26696;&#37319;&#29992;&#20102;&#22522;&#20110;&#22810;&#35821;&#35328;&#21464;&#21387;&#22120;&#30340;&#39044;&#35757;&#32451;&#31243;&#24207;&#65292;&#20351;&#29992;&#26631;&#31614;&#24863;&#30693;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2303.09901</link><description>&lt;p&gt;
SemEval-2023&#20219;&#21153;3&#19978;&#30340;mCPT&#65306;&#29992;&#20110;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#26694;&#26550;&#26816;&#27979;&#30340;&#22810;&#35821;&#35328;&#26631;&#31614;&#24863;&#30693;&#23545;&#27604;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
mCPT at SemEval-2023 Task 3: Multilingual Label-Aware Contrastive Pre-Training of Transformers for Few- and Zero-shot Framing Detection. (arXiv:2303.09901v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09901
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;mCPT&#27169;&#22411;&#29992;&#20110;&#22810;&#35821;&#35328;&#30340;&#12289;&#22810;&#26631;&#31614;&#30340;&#38646;&#26679;&#26412;&#25110;&#23569;&#26679;&#26412;&#30340;&#26694;&#26550;&#26816;&#27979;&#20219;&#21153;&#65292;&#24182;&#22312;&#35199;&#29677;&#29273;&#35821;&#21644;&#20854;&#20182;8&#31181;&#35821;&#35328;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25104;&#32489;&#12290;&#35813;&#26041;&#26696;&#37319;&#29992;&#20102;&#22522;&#20110;&#22810;&#35821;&#35328;&#21464;&#21387;&#22120;&#30340;&#39044;&#35757;&#32451;&#31243;&#24207;&#65292;&#20351;&#29992;&#26631;&#31614;&#24863;&#30693;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#38646;&#26679;&#26412;&#30340;&#35199;&#29677;&#29273;&#35821;&#26694;&#26550;&#26816;&#27979;&#20219;&#21153;&#30340;&#33719;&#32988;&#31995;&#32479;&#65292;&#24182;&#22312;&#21478;&#22806;&#20843;&#31181;&#35821;&#35328;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25104;&#32489;&#12290;&#26694;&#26550;&#26816;&#27979;&#20219;&#21153;&#30340;&#25361;&#25112;&#22312;&#20110;&#22312;&#21482;&#26377;&#23569;&#37327;&#25110;&#38646;&#20010;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#35782;&#21035;&#19968;&#32452;14&#20010;&#26694;&#26550;&#65292;&#21363;&#22810;&#35821;&#35328;&#22810;&#26631;&#31614;&#30340;&#23569;&#26679;&#26412;&#21644;&#38646;&#26679;&#26412;&#35774;&#32622;&#12290;&#25105;&#20204;&#24320;&#21457;&#30340;&#35299;&#20915;&#26041;&#26696;&#37319;&#29992;&#20102;&#22522;&#20110;&#22810;&#35821;&#35328;&#21464;&#21387;&#22120;&#30340;&#39044;&#35757;&#32451;&#31243;&#24207;&#65292;&#20351;&#29992;&#26631;&#31614;&#24863;&#30693;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#12290;&#38500;&#20102;&#25551;&#36848;&#31995;&#32479;&#22806;&#65292;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#23884;&#20837;&#31354;&#38388;&#20998;&#26512;&#21644;&#28040;&#34701;&#30740;&#31350;&#65292;&#20197;&#23637;&#31034;&#25105;&#20204;&#30340;&#39044;&#35757;&#32451;&#31243;&#24207;&#22914;&#20309;&#25903;&#25345;&#26694;&#26550;&#26816;&#27979;&#20197;&#25512;&#36827;&#35745;&#31639;&#26694;&#26550;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents the winning system for the zero-shot Spanish framing detection task, which also achieves competitive places in eight additional languages. The challenge of the framing detection task lies in identifying a set of 14 frames when only a few or zero samples are available, i.e., a multilingual multi-label few- or zero-shot setting. Our developed solution employs a pre-training procedure based on multilingual Transformers using a label-aware contrastive loss function. In addition to describing the system, we perform an embedding space analysis and ablation study to demonstrate how our pre-training procedure supports framing detection to advance computational framing analysis.
&lt;/p&gt;</description></item><item><title>SSCNN&#26159;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#20013;&#24494;&#23376;&#26395;&#36828;&#38236;&#25968;&#25454;&#22788;&#29702;&#20013;&#31232;&#30095;&#12289;&#39640;&#32500;&#24230;&#21644;&#38750;&#35268;&#21017;&#20960;&#20309;&#24418;&#29366;&#31561;&#38382;&#39064;&#30340;&#32593;&#32476;&#65292;&#20854;&#20107;&#20214;&#37325;&#26500;&#24615;&#33021;&#20248;&#20110;&#20256;&#32479;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#36816;&#34892;&#36895;&#24230;&#22823;&#22823;&#25552;&#39640;&#65292;&#21487;&#29992;&#20110;&#25913;&#21892;&#20013;&#24494;&#23376;&#33021;&#37327;&#21644;&#26041;&#21521;&#30340;&#31532;&#19968;&#20272;&#35745;&#20197;&#25773;&#31181;&#26356;&#20808;&#36827;&#30340;&#37325;&#24314;&#12290;</title><link>http://arxiv.org/abs/2303.08812</link><description>&lt;p&gt;
&#22522;&#20110;&#31232;&#30095;&#23376;&#27969;&#24418;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#20013;&#24494;&#23376;&#26395;&#36828;&#38236;&#35302;&#21457;&#20107;&#20214;&#37325;&#26500;
&lt;/p&gt;
&lt;p&gt;
Trigger-Level Event Reconstruction for Neutrino Telescopes Using Sparse Submanifold Convolutional Neural Networks. (arXiv:2303.08812v1 [hep-ex])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08812
&lt;/p&gt;
&lt;p&gt;
SSCNN&#26159;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#20013;&#24494;&#23376;&#26395;&#36828;&#38236;&#25968;&#25454;&#22788;&#29702;&#20013;&#31232;&#30095;&#12289;&#39640;&#32500;&#24230;&#21644;&#38750;&#35268;&#21017;&#20960;&#20309;&#24418;&#29366;&#31561;&#38382;&#39064;&#30340;&#32593;&#32476;&#65292;&#20854;&#20107;&#20214;&#37325;&#26500;&#24615;&#33021;&#20248;&#20110;&#20256;&#32479;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#36816;&#34892;&#36895;&#24230;&#22823;&#22823;&#25552;&#39640;&#65292;&#21487;&#29992;&#20110;&#25913;&#21892;&#20013;&#24494;&#23376;&#33021;&#37327;&#21644;&#26041;&#21521;&#30340;&#31532;&#19968;&#20272;&#35745;&#20197;&#25773;&#31181;&#26356;&#20808;&#36827;&#30340;&#37325;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#22312;&#31185;&#23398;&#25968;&#25454;&#20998;&#26512;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#65292;&#21253;&#25324;&#22312;&#20013;&#24494;&#23376;&#26395;&#36828;&#38236;&#20013;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#23454;&#39564;&#30340;&#25968;&#25454;&#23545;CNN&#25552;&#20986;&#20102;&#35768;&#22810;&#25361;&#25112;&#65292;&#22914;&#38750;&#35268;&#21017;&#20960;&#20309;&#24418;&#29366;&#12289;&#31232;&#30095;&#24615;&#21644;&#39640;&#32500;&#24230;&#12290;&#22240;&#27492;&#65292;CNN&#22312;&#20013;&#24494;&#23376;&#26395;&#36828;&#38236;&#25968;&#25454;&#19978;&#38750;&#24120;&#20302;&#25928;&#65292;&#24182;&#38656;&#35201;&#22823;&#37327;&#30340;&#39044;&#22788;&#29702;&#65292;&#23548;&#33268;&#20449;&#24687;&#25439;&#22833;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31232;&#30095;&#23376;&#27969;&#24418;&#21367;&#31215;&#65288;SSCNN&#65289;&#20316;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#26174;&#31034;SSCNN&#20107;&#20214;&#37325;&#26500;&#24615;&#33021;&#19982;&#20256;&#32479;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30456;&#27604;&#21487;&#27604;&#25110;&#26356;&#22909;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;SSCNN&#22312;GPU&#19978;&#30340;&#36816;&#34892;&#36895;&#24230;&#32422;&#20026;&#20256;&#32479;CNN&#30340;16&#20493;&#12290;&#30001;&#20110;&#36825;&#31181;&#21152;&#36895;&#65292;&#39044;&#35745;&#33021;&#22815;&#22788;&#29702;IceCube&#35268;&#27169;&#30340;&#20013;&#24494;&#23376;&#26395;&#36828;&#38236;&#30340;&#35302;&#21457;&#32423;&#20107;&#20214;&#36895;&#29575;&#12290;&#36825;&#20123;&#32593;&#32476;&#21487;&#29992;&#20110;&#25913;&#21892;&#20013;&#24494;&#23376;&#33021;&#37327;&#21644;&#26041;&#21521;&#30340;&#31532;&#19968;&#20272;&#35745;&#20197;&#25773;&#31181;&#26356;&#20808;&#36827;&#30340;&#37325;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;
Convolutional neural networks (CNNs) have seen extensive applications in scientific data analysis, including in neutrino telescopes. However, the data from these experiments present numerous challenges to CNNs, such as non-regular geometry, sparsity, and high dimensionality. Consequently, CNNs are highly inefficient on neutrino telescope data, and require significant pre-processing that results in information loss. We propose sparse submanifold convolutions (SSCNNs) as a solution to these issues and show that the SSCNN event reconstruction performance is comparable to or better than traditional and machine learning algorithms. Additionally, our SSCNN runs approximately 16 times faster than a traditional CNN on a GPU. As a result of this speedup, it is expected to be capable of handling the trigger-level event rate of IceCube-scale neutrino telescopes. These networks could be used to improve the first estimation of the neutrino energy and direction to seed more advanced reconstructions,
&lt;/p&gt;</description></item><item><title>&#38750;&#23545;&#25968;&#20985;&#21183;V&#30340;&#39640;&#32500;&#37319;&#26679;&#36895;&#29575;&#21487;&#20197;&#22312;&#19968;&#20123;&#26465;&#20214;&#19979;&#23454;&#29616;&#19982;&#20984;&#20989;&#25968;&#30456;&#21516;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.03237</link><description>&lt;p&gt;
&#38750;&#23545;&#25968;&#20985;&#37319;&#26679;&#21644;&#23545;&#25968;&#20998;&#21306;&#20272;&#35745;&#30340;&#25910;&#25947;&#36895;&#29575;
&lt;/p&gt;
&lt;p&gt;
Convergence Rates for Non-Log-Concave Sampling and Log-Partition Estimation. (arXiv:2303.03237v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03237
&lt;/p&gt;
&lt;p&gt;
&#38750;&#23545;&#25968;&#20985;&#21183;V&#30340;&#39640;&#32500;&#37319;&#26679;&#36895;&#29575;&#21487;&#20197;&#22312;&#19968;&#20123;&#26465;&#20214;&#19979;&#23454;&#29616;&#19982;&#20984;&#20989;&#25968;&#30456;&#21516;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#21513;&#24067;&#26031;&#20998;&#24067;$p(x)\propto\exp(-V(x)/\epsilon)$&#20013;&#37319;&#26679;&#24182;&#35745;&#31639;&#20854;&#23545;&#25968;&#20998;&#21306;&#20989;&#25968;&#26159;&#32479;&#35745;&#23398;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#32479;&#35745;&#29289;&#29702;&#20013;&#30340;&#22522;&#26412;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;&#26377;&#25928;&#30340;&#31639;&#27861;&#24050;&#30693;&#20110;&#20984;&#21183;&#20989;&#25968;$V$&#65292;&#20294;&#38750;&#20984;&#24773;&#20917;&#19979;&#30340;&#24773;&#20917;&#35201;&#22256;&#38590;&#24471;&#22810;&#65292;&#31639;&#27861;&#24517;&#28982;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#21463;&#21040;&#32500;&#24230;&#28798;&#38590;&#30340;&#22256;&#25200;&#12290;&#26368;&#36817;&#65292;&#24050;&#32463;&#35777;&#26126;&#22312;&#36866;&#24403;&#30340;&#26465;&#20214;&#19979;&#65292;&#39640;&#32500;&#37319;&#26679;&#38750;&#23545;&#25968;&#20985;&#21183;V&#30340;&#36895;&#29575;&#20063;&#21487;&#20197;&#36798;&#21040;&#21516;&#26679;&#24555;&#30340;&#36895;&#24230;&#12290;&#26412;&#25991;&#23545;&#36825;&#20123;&#32467;&#26524;&#36827;&#34892;&#20102;&#22238;&#39038;&#65292;&#24182;&#24378;&#35843;&#20102;&#39046;&#22495;&#20013;&#30340;&#19968;&#20123;&#24320;&#25918;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sampling from Gibbs distributions $p(x) \propto \exp(-V(x)/\varepsilon)$ and computing their log-partition function are fundamental tasks in statistics, machine learning, and statistical physics. However, while efficient algorithms are known for convex potentials $V$, the situation is much more difficult in the non-convex case, where algorithms necessarily suffer from the curse of dimensionality in the worst case. For optimization, which can be seen as a low-temperature limit of sampling, it is known that smooth functions $V$ allow faster convergence rates. Specifically, for $m$-times differentiable functions in $d$ dimensions, the optimal rate for algorithms with $n$ function evaluations is known to be $O(n^{-m/d})$, where the constant can potentially depend on $m, d$ and the function to be optimized. Hence, the curse of dimensionality can be alleviated for smooth functions at least in terms of the convergence rate. Recently, it has been shown that similarly fast rates can also be ach
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#28145;&#20837;&#29702;&#35299;&#20102;&#25193;&#25955;&#30446;&#26631;&#65292;&#24182;&#25581;&#31034;&#20102;&#21152;&#26435;&#25439;&#22833;&#21644;ELBO&#30446;&#26631;&#20043;&#38388;&#30340;&#30452;&#25509;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2303.00848</link><description>&lt;p&gt;
&#20197;ELBOs&#30340;&#21152;&#26435;&#31215;&#20998;&#29702;&#35299;&#25193;&#25955;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;
Understanding the Diffusion Objective as a Weighted Integral of ELBOs. (arXiv:2303.00848v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00848
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28145;&#20837;&#29702;&#35299;&#20102;&#25193;&#25955;&#30446;&#26631;&#65292;&#24182;&#25581;&#31034;&#20102;&#21152;&#26435;&#25439;&#22833;&#21644;ELBO&#30446;&#26631;&#20043;&#38388;&#30340;&#30452;&#25509;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#29486;&#20013;&#30340;&#25193;&#25955;&#27169;&#22411;&#37319;&#29992;&#19981;&#21516;&#30340;&#30446;&#26631;&#36827;&#34892;&#20248;&#21270;&#65292;&#24182;&#19988;&#36825;&#20123;&#30446;&#26631;&#37117;&#26159;&#21152;&#26435;&#25439;&#22833;&#30340;&#29305;&#20363;&#65292;&#20854;&#20013;&#21152;&#26435;&#20989;&#25968;&#25351;&#23450;&#27599;&#20010;&#22122;&#22768;&#32423;&#21035;&#30340;&#26435;&#37325;&#12290;&#22343;&#21248;&#21152;&#26435;&#23545;&#24212;&#20110;&#26368;&#22823;&#20284;&#28982;&#30340;&#21407;&#21017;&#24615;&#36817;&#20284;ELBO&#30340;&#26368;&#22823;&#21270;&#12290;&#20294;&#26159;&#23454;&#38469;&#19978;&#65292;&#30001;&#20110;&#26356;&#22909;&#30340;&#26679;&#26412;&#36136;&#37327;&#65292;&#30446;&#21069;&#30340;&#25193;&#25955;&#27169;&#22411;&#20351;&#29992;&#38750;&#22343;&#21248;&#21152;&#26435;&#12290;&#26412;&#25991;&#25581;&#31034;&#20102;&#21152;&#26435;&#25439;&#22833;&#65288;&#24102;&#26377;&#20219;&#20309;&#21152;&#26435;&#65289;&#21644;ELBO&#30446;&#26631;&#20043;&#38388;&#30340;&#30452;&#25509;&#20851;&#31995;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21152;&#26435;&#25439;&#22833;&#21487;&#20197;&#34987;&#20889;&#25104;&#19968;&#31181;ELBOs&#30340;&#21152;&#26435;&#31215;&#20998;&#24418;&#24335;&#65292;&#20854;&#20013;&#27599;&#20010;&#22122;&#22768;&#32423;&#21035;&#37117;&#26377;&#19968;&#20010;ELBO&#12290;&#22914;&#26524;&#26435;&#37325;&#20989;&#25968;&#26159;&#21333;&#35843;&#30340;&#65292;&#37027;&#20040;&#21152;&#26435;&#25439;&#22833;&#26159;&#19968;&#31181;&#22522;&#20110;&#20284;&#28982;&#30340;&#30446;&#26631;&#65306;&#23427;&#22312;&#31616;&#21333;&#30340;&#25968;&#25454;&#22686;&#24378;&#19979;&#65288;&#21363;&#39640;&#26031;&#22122;&#22768;&#25200;&#21160;&#65289;&#19979;&#26368;&#22823;&#21270;ELBO&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#26356;&#28145;&#20837;&#22320;&#29702;&#35299;&#20102;&#25193;&#25955;&#30446;&#26631;&#65292;&#20294;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#19968;&#20123;&#27604;&#36739;&#21333;&#35843;&#21644;&#38750;&#21333;&#35843;&#26435;&#37325;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models in the literature are optimized with various objectives that are special cases of a weighted loss, where the weighting function specifies the weight per noise level. Uniform weighting corresponds to maximizing the ELBO, a principled approximation of maximum likelihood. In current practice diffusion models are optimized with non-uniform weighting due to better results in terms of sample quality. In this work we expose a direct relationship between the weighted loss (with any weighting) and the ELBO objective.  We show that the weighted loss can be written as a weighted integral of ELBOs, with one ELBO per noise level. If the weighting function is monotonic, then the weighted loss is a likelihood-based objective: it maximizes the ELBO under simple data augmentation, namely Gaussian noise perturbation. Our main contribution is a deeper theoretical understanding of the diffusion objective, but we also performed some experiments comparing monotonic with non-monotonic weight
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#22914;&#20309;&#23558;&#24046;&#20998;&#38544;&#31169;&#24212;&#29992;&#20110;&#22797;&#26434;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#23454;&#29992;&#25351;&#21335;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#23454;&#36341;&#20013;&#30340;&#31354;&#30333;&#65292;&#20026;&#23454;&#29616;&#26426;&#22120;&#23398;&#20064;&#19982;&#24046;&#20998;&#38544;&#31169;&#30340;&#32467;&#21512;&#25552;&#20379;&#20102;&#23454;&#38469;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2303.00654</link><description>&lt;p&gt;
&#22914;&#20309;&#29992;&#24046;&#20998;&#38544;&#31169;&#23454;&#29616;&#26426;&#22120;&#23398;&#20064;&#65306;&#26426;&#22120;&#23398;&#20064;&#19982;&#24046;&#20998;&#38544;&#31169;&#23454;&#29992;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
How to DP-fy ML: A Practical Guide to Machine Learning with Differential Privacy. (arXiv:2303.00654v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00654
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#22914;&#20309;&#23558;&#24046;&#20998;&#38544;&#31169;&#24212;&#29992;&#20110;&#22797;&#26434;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#23454;&#29992;&#25351;&#21335;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#23454;&#36341;&#20013;&#30340;&#31354;&#30333;&#65292;&#20026;&#23454;&#29616;&#26426;&#22120;&#23398;&#20064;&#19982;&#24046;&#20998;&#38544;&#31169;&#30340;&#32467;&#21512;&#25552;&#20379;&#20102;&#23454;&#38469;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#24191;&#27867;&#65292;&#24182;&#19988;&#26159;&#30740;&#31350;&#30340;&#37325;&#28857;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#31038;&#21306;&#24320;&#22987;&#24847;&#35782;&#21040;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#25968;&#25454;&#30340;&#38544;&#31169;&#30340;&#37325;&#35201;&#24615;&#12290;&#24046;&#20998;&#38544;&#31169;&#24050;&#32463;&#25104;&#20026;&#23545;&#25968;&#25454;&#21311;&#21517;&#21270;&#20570;&#20986;&#27491;&#24335;&#38472;&#36848;&#30340;&#40644;&#37329;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#22312;&#24037;&#19994;&#30028;&#24050;&#32463;&#26377;&#19968;&#20123;&#24212;&#29992;&#24046;&#20998;&#38544;&#31169;&#30340;&#23581;&#35797;&#65292;&#20294;&#23558;&#24046;&#20998;&#38544;&#31169;&#24212;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#22797;&#26434;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20173;&#28982;&#24456;&#23569;&#12290;&#24046;&#20998;&#38544;&#31169;&#30340;&#24212;&#29992;&#21463;&#38480;&#20110;&#32570;&#20047;&#23454;&#38469;&#25351;&#23548;&#65292;&#19981;&#28165;&#26970;&#38656;&#35201;&#20160;&#20040;&#26679;&#30340;&#38544;&#31169;&#20445;&#35777;&#65292;&#24182;&#19988;&#22312;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#38544;&#31169;&#20445;&#25252;&#12289;&#25928;&#29992;&#21644;&#35745;&#31639;&#20043;&#38388;&#23384;&#22312;&#33391;&#22909;&#30340;&#24179;&#34913;&#12290;&#35843;&#25972;&#21644;&#20248;&#21270;&#24615;&#33021;&#30340;&#25216;&#24039;&#25955;&#24067;&#22312;&#35770;&#25991;&#20013;&#25110;&#32773;&#23384;&#22312;&#20110;&#20174;&#19994;&#32773;&#30340;&#22836;&#33041;&#20013;&#12290;&#27492;&#22806;&#65292;&#25991;&#29486;&#20284;&#20046;&#23545;&#20110;&#22914;&#20309;&#20197;&#21450;&#26159;&#21542;&#24212;&#29992;&#26550;&#26500;&#35843;&#25972;&#20197;&#21450;&#21738;&#20123;&#32452;&#20214;&#22312;&#24212;&#29992;&#24046;&#20998;&#38544;&#31169;&#26102;&#26159;&#8220;&#23433;&#20840;&#8221;&#30340;&#38382;&#39064;&#23384;&#22312;&#30528;&#30456;&#20114;&#30683;&#30462;&#30340;&#35777;&#25454;&#12290;&#26412;&#24037;&#20316;&#26159;&#19968;&#20221;&#33258;&#21253;&#21547;&#30340;&#25351;&#21335;&#65292;&#26088;&#22312;&#22635;&#34917;&#36825;&#20123;&#31354;&#30333;&#24182;&#25552;&#20379;&#23454;&#38469;&#25351;&#23548;&#65292;&#24110;&#21161;&#23454;&#29616;&#26426;&#22120;&#23398;&#20064;&#19982;&#24046;&#20998;&#38544;&#31169;&#30340;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
ML models are ubiquitous in real world applications and are a constant focus of research. At the same time, the community has started to realize the importance of protecting the privacy of ML training data.  Differential Privacy (DP) has become a gold standard for making formal statements about data anonymization. However, while some adoption of DP has happened in industry, attempts to apply DP to real world complex ML models are still few and far between. The adoption of DP is hindered by limited practical guidance of what DP protection entails, what privacy guarantees to aim for, and the difficulty of achieving good privacy-utility-computation trade-offs for ML models. Tricks for tuning and maximizing performance are scattered among papers or stored in the heads of practitioners. Furthermore, the literature seems to present conflicting evidence on how and whether to apply architectural adjustments and which components are "safe" to use with DP.  This work is a self-contained guide th
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;Nystr\"om $M$-Hilbert-Schmidt&#29420;&#31435;&#20934;&#21017;&#65292;&#38024;&#23545;&#22823;&#35268;&#27169;&#24212;&#29992;&#30340;&#20108;&#27425;&#35745;&#31639;&#29942;&#39048;&#38382;&#39064;&#36827;&#34892;&#20102;&#35299;&#20915;&#65292;&#24182;&#20860;&#39038;&#20102;&#22810;&#20010;&#38543;&#26426;&#21464;&#37327;&#30340;&#25512;&#24191;&#24773;&#20917;&#21644;&#29702;&#35770;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2302.09930</link><description>&lt;p&gt;
Nystr\"om $M$-Hilbert-Schmidt&#29420;&#31435;&#20934;&#21017;
&lt;/p&gt;
&lt;p&gt;
Nystr\"om $M$-Hilbert-Schmidt Independence Criterion. (arXiv:2302.09930v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09930
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;Nystr\"om $M$-Hilbert-Schmidt&#29420;&#31435;&#20934;&#21017;&#65292;&#38024;&#23545;&#22823;&#35268;&#27169;&#24212;&#29992;&#30340;&#20108;&#27425;&#35745;&#31639;&#29942;&#39048;&#38382;&#39064;&#36827;&#34892;&#20102;&#35299;&#20915;&#65292;&#24182;&#20860;&#39038;&#20102;&#22810;&#20010;&#38543;&#26426;&#21464;&#37327;&#30340;&#25512;&#24191;&#24773;&#20917;&#21644;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26680;&#25216;&#26415;&#26159;&#25968;&#25454;&#31185;&#23398;&#20013;&#26368;&#21463;&#27426;&#36814;&#21644;&#24378;&#22823;&#30340;&#26041;&#27861;&#20043;&#19968;&#12290;&#26680;&#30340;&#24191;&#27867;&#24212;&#29992;&#30340;&#20851;&#38190;&#29305;&#24615;&#21253;&#25324;&#65306;(i) &#23427;&#20204;&#38024;&#23545;&#30340;&#39046;&#22495;&#25968;&#37327;&#22810;&#65292;(ii) &#19982;&#26680;&#30456;&#20851;&#30340;&#20989;&#25968;&#31867;&#20855;&#26377;Hilbert&#32467;&#26500;&#65292;&#20415;&#20110;&#32479;&#35745;&#20998;&#26512;&#65292;&#20197;&#21450;(iii) &#23427;&#20204;&#33021;&#22815;&#20197;&#19981;&#20002;&#22833;&#20449;&#24687;&#30340;&#26041;&#24335;&#34920;&#31034;&#27010;&#29575;&#20998;&#24067;&#12290;&#36825;&#20123;&#29305;&#24615;&#23548;&#33268;&#20102;Hilbert-Schmidt&#29420;&#31435;&#20934;&#21017;(HSIC)&#30340;&#24040;&#22823;&#25104;&#21151;&#65292;&#35813;&#20934;&#21017;&#33021;&#22815;&#22312;&#28201;&#21644;&#26465;&#20214;&#19979;&#25429;&#25417;&#38543;&#26426;&#21464;&#37327;&#30340;&#32852;&#21512;&#29420;&#31435;&#24615;&#65292;&#24182;&#20801;&#35768;&#20855;&#26377;&#20108;&#27425;&#35745;&#31639;&#22797;&#26434;&#24615;&#30340;&#38381;&#24335;&#20272;&#35745;&#22120;(&#30456;&#23545;&#20110;&#26679;&#26412;&#22823;&#23567;)&#12290;&#20026;&#20102;&#35299;&#20915;&#22823;&#35268;&#27169;&#24212;&#29992;&#20013;&#30340;&#20108;&#27425;&#35745;&#31639;&#29942;&#39048;&#38382;&#39064;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#22810;&#20010;HSIC&#36817;&#20284;&#20272;&#35745;&#22120;&#65292;&#28982;&#32780;&#36825;&#20123;&#20272;&#35745;&#22120;&#38480;&#21046;&#20110;$M=2$&#20010;&#38543;&#26426;&#21464;&#37327;&#65292;&#19981;&#33021;&#33258;&#28982;&#22320;&#25512;&#24191;&#21040;$M \geq 2$&#30340;&#24773;&#20917;&#65292;&#24182;&#19988;&#32570;&#20047;&#29702;&#35770;&#20445;&#35777;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;Nystr\"om $M$-Hilbert-Schmidt&#29420;&#31435;&#20934;&#21017;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Kernel techniques are among the most popular and powerful approaches of data science. Among the key features that make kernels ubiquitous are (i) the number of domains they have been designed for, (ii) the Hilbert structure of the function class associated to kernels facilitating their statistical analysis, and (iii) their ability to represent probability distributions without loss of information. These properties give rise to the immense success of Hilbert-Schmidt independence criterion (HSIC) which is able to capture joint independence of random variables under mild conditions, and permits closed-form estimators with quadratic computational complexity (w.r.t. the sample size). In order to alleviate the quadratic computational bottleneck in large-scale applications, multiple HSIC approximations have been proposed, however these estimators are restricted to $M=2$ random variables, do not extend naturally to the $M\ge 2$ case, and lack theoretical guarantees. In this work, we propose an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#40654;&#26364;&#23376;&#27969;&#24418;&#20248;&#21270;&#31639;&#27861;&#36827;&#34892;&#20102;&#31616;&#21270;&#65292;&#25552;&#20986;&#20102;&#40654;&#26364;&#27491;&#24120;&#22352;&#26631;&#30340;&#24191;&#20041;&#29256;&#26412;&#65292;&#21487;&#29992;&#20110;&#23545;&#31216;&#27491;&#23450;&#30697;&#38453;&#30340;&#23376;&#27969;&#24418;&#20248;&#21270;&#65292;&#24182;&#20026;&#28145;&#24230;&#23398;&#20064;&#24320;&#21457;&#20102;&#39640;&#25928;&#30340;&#20108;&#38454;&#20248;&#21270;&#22120;&#65292;&#26080;&#38656;&#26174;&#24335;&#30697;&#38453;&#27714;&#36870;&#12290;</title><link>http://arxiv.org/abs/2302.09738</link><description>&lt;p&gt;
&#31616;&#21270;&#22522;&#20110;&#21160;&#37327;&#30340;&#40654;&#26364;&#23376;&#27969;&#24418;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Simplifying Momentum-based Riemannian Submanifold Optimization. (arXiv:2302.09738v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09738
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#40654;&#26364;&#23376;&#27969;&#24418;&#20248;&#21270;&#31639;&#27861;&#36827;&#34892;&#20102;&#31616;&#21270;&#65292;&#25552;&#20986;&#20102;&#40654;&#26364;&#27491;&#24120;&#22352;&#26631;&#30340;&#24191;&#20041;&#29256;&#26412;&#65292;&#21487;&#29992;&#20110;&#23545;&#31216;&#27491;&#23450;&#30697;&#38453;&#30340;&#23376;&#27969;&#24418;&#20248;&#21270;&#65292;&#24182;&#20026;&#28145;&#24230;&#23398;&#20064;&#24320;&#21457;&#20102;&#39640;&#25928;&#30340;&#20108;&#38454;&#20248;&#21270;&#22120;&#65292;&#26080;&#38656;&#26174;&#24335;&#30697;&#38453;&#27714;&#36870;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24102;&#26377;&#21160;&#37327;&#30340;&#40654;&#26364;&#23376;&#27969;&#24418;&#20248;&#21270;&#22312;&#35745;&#31639;&#19978;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#30830;&#20445;&#36845;&#20195;&#20445;&#25345;&#22312;&#23376;&#27969;&#24418;&#19978;&#36890;&#24120;&#38656;&#35201;&#35299;&#20915;&#22256;&#38590;&#30340;&#24494;&#20998;&#26041;&#31243;&#12290;&#26412;&#25991;&#38024;&#23545;&#20855;&#26377;&#20223;&#23556;&#19981;&#21464;&#24230;&#37327;&#30340;&#23545;&#31216;&#27491;&#23450;&#30697;&#38453;&#30340;&#23376;&#27969;&#24418;&#20248;&#21270;&#31639;&#27861;&#36827;&#34892;&#20102;&#31616;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#40654;&#26364;&#27491;&#24120;&#22352;&#26631;&#30340;&#24191;&#20041;&#29256;&#26412;&#65292;&#21487;&#20197;&#23558;&#38382;&#39064;&#21160;&#24577;&#22320;&#31616;&#21270;&#20026;&#27431;&#20960;&#37324;&#24471;&#26080;&#32422;&#26463;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#26041;&#27861;&#26469;&#35299;&#37322;&#21644;&#31616;&#21270;&#29616;&#26377;&#30340;&#32467;&#26500;&#21270;&#21327;&#26041;&#24046;&#26041;&#27861;&#65292;&#24182;&#20026;&#28145;&#24230;&#23398;&#20064;&#24320;&#21457;&#20102;&#39640;&#25928;&#30340;&#20108;&#38454;&#20248;&#21270;&#22120;&#65292;&#32780;&#26080;&#38656;&#26174;&#24335;&#30697;&#38453;&#27714;&#36870;&#12290;
&lt;/p&gt;
&lt;p&gt;
Riemannian submanifold optimization with momentum is computationally challenging because ensuring iterates remain on the submanifold often requires solving difficult differential equations. We simplify such optimization algorithms for the submanifold of symmetric positive-definite matrices with the affine invariant metric. We propose a generalized version of the Riemannian normal coordinates which dynamically trivializes the problem into a Euclidean unconstrained problem. We use our approach to explain and simplify existing approaches for structured covariances and develop efficient second-order optimizers for deep learning without explicit matrix inverses.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23454;&#39564;&#21644;&#29702;&#35770;&#20998;&#26512;&#65292;&#26412;&#35770;&#25991;&#21457;&#29616;&#22312;&#30693;&#35782;&#33976;&#39311;&#20013;&#65292;&#23398;&#29983;&#32593;&#32476;&#23545;&#25945;&#24072;&#32593;&#32476;&#30340;&#27010;&#29575;&#20559;&#31163;&#26159;&#31995;&#32479;&#24615;&#22840;&#22823;&#30340;&#65292;&#21516;&#26102;&#20063;&#24471;&#21040;&#20102;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2301.12923</link><description>&lt;p&gt;
&#20851;&#20110;&#30693;&#35782;&#33976;&#39311;&#20013;&#30340;&#23398;&#29983;-&#25945;&#24072;&#20559;&#24046;&#65306;&#36829;&#21453;&#35268;&#21017;&#26159;&#21542;&#26377;&#30410;&#65311;
&lt;/p&gt;
&lt;p&gt;
On student-teacher deviations in distillation: does it pay to disobey?. (arXiv:2301.12923v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12923
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23454;&#39564;&#21644;&#29702;&#35770;&#20998;&#26512;&#65292;&#26412;&#35770;&#25991;&#21457;&#29616;&#22312;&#30693;&#35782;&#33976;&#39311;&#20013;&#65292;&#23398;&#29983;&#32593;&#32476;&#23545;&#25945;&#24072;&#32593;&#32476;&#30340;&#27010;&#29575;&#20559;&#31163;&#26159;&#31995;&#32479;&#24615;&#22840;&#22823;&#30340;&#65292;&#21516;&#26102;&#20063;&#24471;&#21040;&#20102;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#34987;&#24191;&#27867;&#29992;&#20110;&#36890;&#36807;&#35757;&#32451;&#23398;&#29983;&#27169;&#20223;&#32463;&#36807;&#35757;&#32451;&#30340;&#8220;&#25945;&#24072;&#8221;&#32593;&#32476;&#30340;&#36719;&#27010;&#29575;&#26469;&#25552;&#39640;&#8220;&#23398;&#29983;&#8221;&#32593;&#32476;&#30340;&#27979;&#35797;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23613;&#31649;&#34987;&#35757;&#32451;&#25104;&#36866;&#24212;&#25945;&#24072;&#30340;&#27010;&#29575;&#65292;&#23398;&#29983;&#19981;&#20165;&#26126;&#26174;&#20559;&#31163;&#36825;&#20123;&#27010;&#29575;&#65292;&#32780;&#19988;&#34920;&#29616;&#27604;&#25945;&#24072;&#26356;&#22909;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#30830;&#23450;&#23398;&#29983;-&#25945;&#24072;&#20559;&#24046;&#30340;&#30830;&#20999;&#24615;&#36136;&#65292;&#24182;&#35770;&#35777;&#23427;&#20204;&#19982;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#22914;&#20309;&#20849;&#23384;&#26469;&#35299;&#20915;&#36825;&#19968;&#30475;&#20284;&#30683;&#30462;&#30340;&#35266;&#23519;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#23545;&#22270;&#20687;&#21644;&#35821;&#35328;&#25968;&#25454;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#30830;&#23450;&#36825;&#20123;&#20559;&#24046;&#23545;&#24212;&#20110;&#23398;&#29983;&#31995;&#32479;&#24615;&#22320;&#22840;&#22823;&#25945;&#24072;&#30340;&#33258;&#20449;&#27700;&#24179;&#12290;&#25509;&#19979;&#26469;&#65292;&#22312;&#19968;&#20123;&#31616;&#21333;&#30340;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#20174;&#29702;&#35770;&#21644;&#23454;&#35777;&#19978;&#24314;&#31435;&#20102;KD&#22312;&#25910;&#25947;&#26356;&#24555;&#30340;&#36807;&#31243;&#20013;&#22840;&#22823;&#20102;&#26799;&#24230;&#19979;&#38477;&#30340;&#38544;&#21547;&#20559;&#24046;&#30340;&#35777;&#25454;&#12290;&#26368;&#21518;&#65292;
&lt;/p&gt;
&lt;p&gt;
Knowledge distillation (KD) has been widely-used to improve the test accuracy of a ``student'' network by training the student to mimic soft probabilities of a trained "teacher" network. Yet, it has been shown in recent work that, despite being trained to fit the teacher's probabilities, the student not only significantly deviates from these probabilities, but also performs even better than the teacher. Our work aims to reconcile this seemingly paradoxical observation by characterizing the precise nature of the student-teacher deviations, and by arguing how they can co-occur with better generalization. First, through experiments on image and language data, we identify that these deviations correspond to the student systematically exaggerating the confidence levels of the teacher. Next, we theoretically and empirically establish in some simple settings that KD also exaggerates the implicit bias of gradient descent in converging faster along the top eigendirections of the data. Finally, 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#27491;&#21017;&#21270;&#22343;&#34913;&#65292;&#21487;&#20197;&#23558;&#20004;&#20154;&#38646;&#21644;&#21338;&#24328;&#20013;&#30340;&#19981;&#23436;&#32654;&#20449;&#24687;&#25277;&#35937;&#20986;&#26469;&#24182;&#20316;&#20026;&#23436;&#20840;&#20449;&#24687;&#38382;&#39064;&#22788;&#29702;&#12290;</title><link>http://arxiv.org/abs/2301.09159</link><description>&lt;p&gt;
&#20174;&#20004;&#20154;&#38646;&#21644;&#21338;&#24328;&#20013;&#25277;&#35937;&#20986;&#19981;&#23436;&#32654;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Abstracting Imperfect Information Away from Two-Player Zero-Sum Games. (arXiv:2301.09159v2 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09159
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27491;&#21017;&#21270;&#22343;&#34913;&#65292;&#21487;&#20197;&#23558;&#20004;&#20154;&#38646;&#21644;&#21338;&#24328;&#20013;&#30340;&#19981;&#23436;&#32654;&#20449;&#24687;&#25277;&#35937;&#20986;&#26469;&#24182;&#20316;&#20026;&#23436;&#20840;&#20449;&#24687;&#38382;&#39064;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Nayyar&#31561;&#20154;&#22312;&#20854;&#24320;&#21019;&#24615;&#30340;&#24037;&#20316;&#20013;&#34920;&#26126;&#65292;&#36890;&#36807;&#22312;&#28216;&#25103;&#36807;&#31243;&#20013;&#35753;&#29609;&#23478;&#20844;&#24320;&#23459;&#24067;&#20854;&#31574;&#30053;&#65292;&#19981;&#23436;&#32654;&#20449;&#24687;&#21487;&#20197;&#34987;&#20174;&#20849;&#21516;&#25928;&#30410;&#28216;&#25103;&#20013;&#25277;&#35937;&#20986;&#26469;&#12290;&#36825;&#20010;&#35265;&#35299;&#26159;&#25903;&#25745;&#20849;&#21516;&#25928;&#30410;&#28216;&#25103;&#21512;&#29702;&#30340;&#27714;&#35299;&#22120;&#21644;&#20915;&#31574;&#26102;&#38388;&#35268;&#21010;&#31639;&#27861;&#30340;&#22522;&#30784;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#23558;&#21516;&#26679;&#30340;&#35265;&#35299;&#31616;&#21333;&#24212;&#29992;&#20110;&#20004;&#20154;&#38646;&#21644;&#21338;&#24328;&#20250;&#22833;&#36133;&#65292;&#22240;&#20026;&#20855;&#26377;&#20844;&#24320;&#31574;&#30053;&#23459;&#24067;&#30340;&#28216;&#25103;&#30340;&#32435;&#20160;&#22343;&#34913;&#21487;&#33021;&#19982;&#21407;&#22987;&#28216;&#25103;&#30340;&#32435;&#20160;&#22343;&#34913;&#19981;&#30456;&#23545;&#24212;&#12290;&#22240;&#27492;&#65292;&#29616;&#26377;&#30340;&#21512;&#29702;&#30340;&#20915;&#31574;&#26102;&#38388;&#35268;&#21010;&#31639;&#27861;&#38656;&#35201;&#22797;&#26434;&#30340;&#39069;&#22806;&#26426;&#21046;&#65292;&#20854;&#20855;&#26377;&#19981;&#21560;&#24341;&#20154;&#30340;&#29305;&#24615;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#23637;&#31034;&#26576;&#20123;&#27491;&#21017;&#21270;&#22343;&#34913;&#19981;&#20855;&#26377;&#19978;&#36848;&#30340;&#19981;&#23545;&#24212;&#38382;&#39064;&#65292;&#22240;&#27492;&#65292;&#35745;&#31639;&#23427;&#20204;&#21487;&#20197;&#34987;&#35270;&#20026;&#23436;&#20840;&#20449;&#24687;&#38382;&#39064;&#12290;&#22240;&#20026;&#36825;&#20123;&#27491;&#21017;&#21270;&#22343;&#34913;&#21487;&#20197;&#34987;&#26080;&#38480;&#25509;&#36817;&#32435;&#20160;&#22343;&#34913;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#20026;&#19968;&#31181;&#26032;&#30340;&#35270;&#35282;&#25171;&#24320;&#20102;&#22823;&#38376;&#12290;
&lt;/p&gt;
&lt;p&gt;
In their seminal work, Nayyar et al. (2013) showed that imperfect information can be abstracted away from common-payoff games by having players publicly announce their policies as they play. This insight underpins sound solvers and decision-time planning algorithms for common-payoff games. Unfortunately, a naive application of the same insight to two-player zero-sum games fails because Nash equilibria of the game with public policy announcements may not correspond to Nash equilibria of the original game. As a consequence, existing sound decision-time planning algorithms require complicated additional mechanisms that have unappealing properties. The main contribution of this work is showing that certain regularized equilibria do not possess the aforementioned non-correspondence problem -- thus, computing them can be treated as perfect-information problems. Because these regularized equilibria can be made arbitrarily close to Nash equilibria, our result opens the door to a new perspectiv
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#28789;&#27963;&#30721;&#29575;&#30340;Reed-Muller&#23376;&#30721;&#30340;&#39640;&#25928;&#35299;&#30721;&#38382;&#39064;&#65292;&#36890;&#36807;&#25193;&#23637;&#36882;&#24402;&#25237;&#24433;&#32858;&#21512;&#65288;RPA&#65289;&#35793;&#30721;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;subRPA&#21644;soft-subRPA&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#32500;&#25345;&#36739;&#20302;&#22797;&#26434;&#24615;&#30340;&#21516;&#26102;&#25552;&#39640;&#35793;&#30721;&#24615;&#33021;&#24182;&#23454;&#29616;&#21487;&#24494;&#20998;&#30340;&#35299;&#30721;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2301.06251</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#19979;&#30340;Reed-Muller&#23376;&#30721;&#39640;&#25928;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Machine Learning-Aided Efficient Decoding of Reed-Muller Subcodes. (arXiv:2301.06251v2 [cs.IT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.06251
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#28789;&#27963;&#30721;&#29575;&#30340;Reed-Muller&#23376;&#30721;&#30340;&#39640;&#25928;&#35299;&#30721;&#38382;&#39064;&#65292;&#36890;&#36807;&#25193;&#23637;&#36882;&#24402;&#25237;&#24433;&#32858;&#21512;&#65288;RPA&#65289;&#35793;&#30721;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;subRPA&#21644;soft-subRPA&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#32500;&#25345;&#36739;&#20302;&#22797;&#26434;&#24615;&#30340;&#21516;&#26102;&#25552;&#39640;&#35793;&#30721;&#24615;&#33021;&#24182;&#23454;&#29616;&#21487;&#24494;&#20998;&#30340;&#35299;&#30721;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Reed-Muller&#65288;RM&#65289;&#30721;&#22312;&#19968;&#33324;&#30340;&#20108;&#36827;&#21046;&#36755;&#20837;&#26080;&#35760;&#24518;&#23545;&#31216;&#20449;&#36947;&#19978;&#36798;&#21040;&#23481;&#37327;&#65292;&#24182;&#19988;&#25454;&#25512;&#27979;&#65292;&#22312;&#27604;&#20363;&#23450;&#24459;&#26041;&#38754;&#30340;&#24615;&#33021;&#19982;&#38543;&#26426;&#30721;&#30456;&#24403;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#32467;&#26524;&#26159;&#24314;&#31435;&#22312;&#23545;&#19968;&#33324;&#30721;&#21442;&#25968;&#20351;&#29992;&#26368;&#22823;&#20284;&#28982;&#35793;&#30721;&#22120;&#30340;&#24773;&#20917;&#19979;&#30340;&#12290;&#27492;&#22806;&#65292;RM&#30721;&#21482;&#33021;&#25509;&#21463;&#26377;&#38480;&#30340;&#30721;&#29575;&#38598;&#12290;&#23545;&#20110;&#26377;&#38480;&#38271;&#24230;&#30340;RM&#30721;&#65292;&#24050;&#32463;&#26377;&#35832;&#22914;&#36830;&#32493;&#21462;&#28040;&#21015;&#34920;&#65288;SCL&#65289;&#35793;&#30721;&#22120;&#21644;&#26368;&#36817;&#24341;&#20837;&#30340;&#36882;&#24402;&#25237;&#24433;&#32858;&#21512;&#65288;RPA&#65289;&#35793;&#30721;&#22120;&#31561;&#39640;&#25928;&#35793;&#30721;&#22120;&#21487;&#29992;&#12290;&#26412;&#25991;&#25105;&#20204;&#30740;&#31350;&#20855;&#26377;&#28789;&#27963;&#30721;&#29575;&#30340;RM&#30721;&#23376;&#30721;&#12290;&#39318;&#20808;&#25105;&#20204;&#23558;RPA&#35793;&#30721;&#31639;&#27861;&#25193;&#23637;&#21040;RM&#23376;&#30721;&#19978;&#12290;&#20026;&#20102;&#38477;&#20302;&#25105;&#20204;&#30340;&#35793;&#30721;&#31639;&#27861;&#65288;&#31216;&#20026;subRPA&#65289;&#30340;&#22797;&#26434;&#24615;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#25237;&#24433;&#21098;&#26525;&#26041;&#27861;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#22522;&#20110;&#36719;&#21028;&#26029;&#30340;&#29256;&#26412;&#65292;&#31216;&#20026;soft-subRPA&#65292;&#23427;&#19981;&#20165;&#25913;&#36827;&#20102;subRPA&#30340;&#24615;&#33021;&#65292;&#36824;&#20351;&#24471;&#35793;&#30721;&#31639;&#27861;&#21487;&#24494;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reed-Muller (RM) codes achieve the capacity of general binary-input memoryless symmetric channels and are conjectured to have a comparable performance to that of random codes in terms of scaling laws. However, such results are established assuming maximum-likelihood decoders for general code parameters. Also, RM codes only admit limited sets of rates. Efficient decoders such as successive cancellation list (SCL) decoder and recently-introduced recursive projection-aggregation (RPA) decoders are available for RM codes at finite lengths. In this paper, we focus on subcodes of RM codes with flexible rates. We first extend the RPA decoding algorithm to RM subcodes. To lower the complexity of our decoding algorithm, referred to as subRPA, we investigate different approaches to prune the projections. Next, we derive the soft-decision based version of our algorithm, called soft-subRPA, that not only improves upon the performance of subRPA but also enables a differentiable decoding algorithm. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32456;&#36523;&#24378;&#21270;&#23398;&#20064;&#20013;&#20351;&#29992;&#35843;&#25972;&#25513;&#30721;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#35843;&#25972;&#25513;&#30721;&#24212;&#29992;&#20110;PPO&#21644;IMPALA&#20195;&#29702;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#31163;&#25955;&#21644;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.11110</link><description>&lt;p&gt;
&#20351;&#29992;&#35843;&#25972;&#25513;&#30721;&#30340;&#32456;&#36523;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Lifelong Reinforcement Learning with Modulating Masks. (arXiv:2212.11110v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.11110
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32456;&#36523;&#24378;&#21270;&#23398;&#20064;&#20013;&#20351;&#29992;&#35843;&#25972;&#25513;&#30721;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#35843;&#25972;&#25513;&#30721;&#24212;&#29992;&#20110;PPO&#21644;IMPALA&#20195;&#29702;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#31163;&#25955;&#21644;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32456;&#36523;&#23398;&#20064;&#26088;&#22312;&#21019;&#24314;&#22312;&#20854;&#29983;&#21629;&#21608;&#26399;&#20013;&#25345;&#32493;&#21644;&#36880;&#27493;&#23398;&#20064;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#31867;&#20284;&#29983;&#29289;&#23398;&#20064;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#36825;&#26041;&#38754;&#30340;&#23581;&#35797;&#36935;&#21040;&#20102;&#38382;&#39064;&#65292;&#21253;&#25324;&#28798;&#38590;&#24615;&#36951;&#24536;&#12289;&#20219;&#21153;&#20043;&#38388;&#30340;&#24178;&#25200;&#65292;&#20197;&#21450;&#26080;&#27861;&#21033;&#29992;&#20808;&#21069;&#30340;&#30693;&#35782;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#30456;&#24403;&#22810;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#23398;&#20064;&#28041;&#21450;&#36755;&#20837;&#20998;&#24067;&#21464;&#21270;&#30340;&#22810;&#20010;&#30417;&#30563;&#20998;&#31867;&#20219;&#21153;&#19978;&#65292;&#20294;&#26159;&#32456;&#36523;&#24378;&#21270;&#23398;&#20064;&#24517;&#39035;&#22788;&#29702;&#29366;&#24577;&#21644;&#36716;&#25442;&#20998;&#24067;&#20197;&#21450;&#22870;&#21169;&#20989;&#25968;&#30340;&#21464;&#21270;&#12290;&#26368;&#36817;&#38024;&#23545;&#20998;&#31867;&#38382;&#39064;&#24320;&#21457;&#30340;&#20351;&#29992;&#22266;&#23450;&#39592;&#24178;&#32593;&#32476;&#30340;&#35843;&#25972;&#25513;&#30721;&#23545;&#20110;&#22788;&#29702;&#22914;&#27492;&#22823;&#33539;&#22260;&#30340;&#20219;&#21153;&#21464;&#21270;&#29305;&#21035;&#36866;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#35843;&#25972;&#25513;&#30721;&#24212;&#29992;&#20110;&#28145;&#23618;&#27425;&#30340;&#32456;&#36523;&#24378;&#21270;&#23398;&#20064;&#65292;&#20855;&#20307;&#21253;&#25324;PPO&#21644;IMPALA&#20195;&#29702;&#12290;&#22312;&#31163;&#25955;&#21644;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#19982;&#32456;&#36523;&#24378;&#21270;&#23398;&#20064;&#22522;&#32447;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#32467;&#26524;&#26174;&#31034;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#20808;&#21069;&#20219;&#21153;&#30340;&#32447;&#24615;&#32452;&#21512;&#30340;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lifelong learning aims to create AI systems that continuously and incrementally learn during a lifetime, similar to biological learning. Attempts so far have met problems, including catastrophic forgetting, interference among tasks, and the inability to exploit previous knowledge. While considerable research has focused on learning multiple supervised classification tasks that involve changes in the input distribution, lifelong reinforcement learning (LRL) must deal with variations in the state and transition distributions, and in the reward functions. Modulating masks with a fixed backbone network, recently developed for classification, are particularly suitable to deal with such a large spectrum of task variations. In this paper, we adapted modulating masks to work with deep LRL, specifically PPO and IMPALA agents. The comparison with LRL baselines in both discrete and continuous RL tasks shows superior performance. We further investigated the use of a linear combination of previousl
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#28145;&#24230;&#40654;&#26364;&#32593;&#32476;&#23545;EEG&#30340;&#24212;&#29992;&#65292;&#25506;&#35752;&#20102;&#32593;&#32476;&#22823;&#23567;&#12289;&#31471;&#21040;&#31471;&#33021;&#21147;&#12289;&#27169;&#22411;&#35757;&#32451;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#27604;&#36739;&#20102;&#20854;&#19982;&#22522;&#20110;&#40654;&#26364;&#20960;&#20309;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2212.10426</link><description>&lt;p&gt;
EEG&#35299;&#30721;&#30340;&#28145;&#24230;&#40654;&#26364;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Deep Riemannian Networks for EEG Decoding. (arXiv:2212.10426v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10426
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#28145;&#24230;&#40654;&#26364;&#32593;&#32476;&#23545;EEG&#30340;&#24212;&#29992;&#65292;&#25506;&#35752;&#20102;&#32593;&#32476;&#22823;&#23567;&#12289;&#31471;&#21040;&#31471;&#33021;&#21147;&#12289;&#27169;&#22411;&#35757;&#32451;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#27604;&#36739;&#20102;&#20854;&#19982;&#22522;&#20110;&#40654;&#26364;&#20960;&#20309;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#22312;&#30005;&#33041;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#35299;&#30721;&#20219;&#21153;&#20013;&#65292;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#36890;&#24120;&#26159;&#30001;&#28145;&#24230;&#23398;&#20064;&#25110;&#22522;&#20110;&#40654;&#26364;&#20960;&#20309;&#30340;&#35299;&#30721;&#22120;&#23454;&#29616;&#30340;&#12290;&#26368;&#36817;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#23545;&#28145;&#24230;&#40654;&#26364;&#32593;&#32476;&#65288;DRNs&#65289;&#20135;&#29983;&#20102;&#20852;&#36259;&#65292;&#21487;&#33021;&#32467;&#21512;&#20102;&#20043;&#21069;&#20004;&#31867;&#26041;&#27861;&#30340;&#20248;&#28857;&#12290;&#28982;&#32780;&#65292;&#36824;&#26377;&#19968;&#31995;&#21015;&#38382;&#39064;&#38656;&#35201;&#36827;&#19968;&#27493;&#27934;&#23519;&#65292;&#20197;&#38138;&#24179;DRNs&#22312;EEG&#20013;&#26356;&#24191;&#27867;&#24212;&#29992;&#30340;&#36947;&#36335;&#12290;&#36825;&#20123;&#38382;&#39064;&#21253;&#25324;&#26550;&#26500;&#35774;&#35745;&#38382;&#39064;&#65292;&#22914;&#32593;&#32476;&#22823;&#23567;&#21644;&#31471;&#21040;&#31471;&#33021;&#21147;&#65292;&#20197;&#21450;&#27169;&#22411;&#35757;&#32451;&#38382;&#39064;&#12290;&#36825;&#20123;&#22240;&#32032;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#32593;&#32476;&#20013;&#30340;&#25968;&#25454;&#22914;&#20309;&#36716;&#25442;&#65292;&#20197;&#21450;&#26159;&#21542;&#19982;&#20256;&#32479;&#30340;EEG&#35299;&#30721;&#30456;&#20851;&#20063;&#19981;&#28165;&#26970;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20998;&#26512;&#20855;&#26377;&#24191;&#27867;&#36229;&#21442;&#25968;&#30340;DRNs&#26469;&#22880;&#23450;&#36825;&#20123;&#20027;&#39064;&#39046;&#22495;&#30340;&#22522;&#30784;&#12290;&#20351;&#29992;&#20004;&#20010;&#20844;&#20849;EEG&#25968;&#25454;&#38598;&#27979;&#35797;&#20102;&#32593;&#32476;&#65292;&#24182;&#19982;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#40654;&#26364;&#20960;&#20309;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art performance in electroencephalography (EEG) decoding tasks is currently often achieved with either Deep-Learning or Riemannian-Geometry-based decoders. Recently, there is growing interest in Deep Riemannian Networks (DRNs) possibly combining the advantages of both previous classes of methods. However, there are still a range of topics where additional insight is needed to pave the way for a more widespread application of DRNs in EEG. These include architecture design questions such as network size and end-to-end ability as well as model training questions. How these factors affect model performance has not been explored. Additionally, it is not clear how the data within these networks is transformed, and whether this would correlate with traditional EEG decoding. Our study aims to lay the groundwork in the area of these topics through the analysis of DRNs for EEG with a wide range of hyperparameters. Networks were tested on two public EEG datasets and compared with sta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23450;&#20041;&#20102;&#25968;&#25454;&#23637;&#24320;&#24230;&#30340;&#27010;&#24565;&#65292;&#24182;&#21457;&#29616;&#24120;&#29992;&#30340;&#24230;&#37327;&#26041;&#27861;&#19981;&#21487;&#38752;&#65292;&#25552;&#20986;&#20102;&#20843;&#31181;&#22791;&#36873;&#30340;&#25968;&#25454;&#23637;&#24320;&#24230;&#37327;&#26041;&#27861;&#65292;&#24182;&#25512;&#33616;&#20351;&#29992;&#19968;&#31181;&#22522;&#20110;&#20027;&#25104;&#20998;&#21644;&#19968;&#31181;&#22522;&#20110;&#29109;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#21487;&#38752;&#22320;&#27604;&#36739;&#19981;&#21516;&#27169;&#22411;&#30340;&#23637;&#24320;&#24230;&#12290;</title><link>http://arxiv.org/abs/2212.08172</link><description>&lt;p&gt;
&#39640;&#32500;&#28508;&#31354;&#38388;&#20013;&#21487;&#38752;&#30340;&#23637;&#24320;&#24230;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Reliable Measures of Spread in High Dimensional Latent Spaces. (arXiv:2212.08172v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08172
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23450;&#20041;&#20102;&#25968;&#25454;&#23637;&#24320;&#24230;&#30340;&#27010;&#24565;&#65292;&#24182;&#21457;&#29616;&#24120;&#29992;&#30340;&#24230;&#37327;&#26041;&#27861;&#19981;&#21487;&#38752;&#65292;&#25552;&#20986;&#20102;&#20843;&#31181;&#22791;&#36873;&#30340;&#25968;&#25454;&#23637;&#24320;&#24230;&#37327;&#26041;&#27861;&#65292;&#24182;&#25512;&#33616;&#20351;&#29992;&#19968;&#31181;&#22522;&#20110;&#20027;&#25104;&#20998;&#21644;&#19968;&#31181;&#22522;&#20110;&#29109;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#21487;&#38752;&#22320;&#27604;&#36739;&#19981;&#21516;&#27169;&#22411;&#30340;&#23637;&#24320;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#28508;&#31354;&#38388;&#30340;&#20960;&#20309;&#29305;&#24615;&#21487;&#20197;&#36890;&#36807;&#25805;&#20316;&#36825;&#20123;&#29305;&#24615;&#26469;&#25913;&#21892;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#20854;&#20013;&#19968;&#20010;&#29305;&#24615;&#26159;&#27169;&#22411;&#28508;&#31354;&#38388;&#20013;&#30340;&#25968;&#25454;&#23637;&#24320;&#24230;&#65292;&#21363;&#21487;&#29992;&#28508;&#31354;&#38388;&#30340;&#20805;&#20998;&#21033;&#29992;&#31243;&#24230;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#25968;&#25454;&#23637;&#24320;&#24230;&#65292;&#24182;&#35777;&#26126;&#20102;&#24120;&#29992;&#30340;&#25968;&#25454;&#23637;&#24320;&#24230;&#24230;&#37327;&#26041;&#27861;&#65292;&#24179;&#22343;&#20313;&#24358;&#30456;&#20284;&#24230;&#21644;&#20998;&#21306;&#20989;&#25968;&#30340;&#26368;&#23567;/&#26368;&#22823;&#27604;&#20363;I&#65288;V&#65289;&#65292;&#19981;&#33021;&#25552;&#20379;&#21487;&#38752;&#30340;&#23545;&#27604;&#19981;&#21516;&#27169;&#22411;&#28508;&#31354;&#38388;&#20351;&#29992;&#24773;&#20917;&#30340;&#25351;&#26631;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#30740;&#31350;&#20102;&#20843;&#31181;&#22791;&#36873;&#30340;&#25968;&#25454;&#23637;&#24320;&#24230;&#24230;&#37327;&#26041;&#27861;&#65292;&#20854;&#20013;&#38500;&#19968;&#31181;&#22806;&#65292;&#25152;&#26377;&#26041;&#27861;&#22312;&#24212;&#29992;&#20110;&#19971;&#31181;&#21512;&#25104;&#25968;&#25454;&#20998;&#24067;&#26102;&#37117;&#20248;&#20110;&#24403;&#21069;&#30340;&#24230;&#37327;&#26041;&#27861;&#12290;&#22312;&#25105;&#20204;&#25552;&#20986;&#30340;&#24230;&#37327;&#26041;&#27861;&#20013;&#65292;&#25512;&#33616;&#20351;&#29992;&#19968;&#31181;&#22522;&#20110;&#20027;&#25104;&#20998;&#30340;&#24230;&#37327;&#26041;&#27861;&#21644;&#19968;&#31181;&#22522;&#20110;&#29109;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#23427;&#20204;&#21487;&#20197;&#25552;&#20379;&#21487;&#38752;&#30340;&#12289;&#30456;&#23545;&#30340;&#23637;&#24320;&#24230;&#37327;&#65292;&#24182;&#21487;&#29992;&#20110;&#27604;&#36739;&#19981;&#21516;&#22823;&#23567;&#21644;&#32500;&#24230;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding geometric properties of natural language processing models' latent spaces allows the manipulation of these properties for improved performance on downstream tasks. One such property is the amount of data spread in a model's latent space, or how fully the available latent space is being used. In this work, we define data spread and demonstrate that the commonly used measures of data spread, Average Cosine Similarity and a partition function min/max ratio I(V), do not provide reliable metrics to compare the use of latent space across models. We propose and examine eight alternative measures of data spread, all but one of which improve over these current metrics when applied to seven synthetic data distributions. Of our proposed measures, we recommend one principal component-based measure and one entropy-based measure that provide reliable, relative measures of spread and can be used to compare models of different sizes and dimensionalities.
&lt;/p&gt;</description></item><item><title>&#22788;&#29702;&#22810;&#20010;&#20998;&#24067;&#26102;&#65292;&#36890;&#36807;&#36830;&#25509;&#22810;&#20010;&#35757;&#32451;&#36807;&#31243;&#24471;&#21040;&#30340;&#34920;&#31034;&#27604;&#21333;&#20010;&#35757;&#32451;&#36807;&#31243;&#24471;&#21040;&#30340;&#34920;&#31034;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2212.07346</link><description>&lt;p&gt;
&#23398;&#20064;&#38024;&#23545;&#19981;&#21516;&#20219;&#21153;&#21644;&#20998;&#24067;&#30340;&#26377;&#29992;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning useful representations for shifting tasks and distributions. (arXiv:2212.07346v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.07346
&lt;/p&gt;
&lt;p&gt;
&#22788;&#29702;&#22810;&#20010;&#20998;&#24067;&#26102;&#65292;&#36890;&#36807;&#36830;&#25509;&#22810;&#20010;&#35757;&#32451;&#36807;&#31243;&#24471;&#21040;&#30340;&#34920;&#31034;&#27604;&#21333;&#20010;&#35757;&#32451;&#36807;&#31243;&#24471;&#21040;&#30340;&#34920;&#31034;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#25105;&#20204;&#22788;&#29702;&#22810;&#20010;&#20998;&#24067;&#26102;&#65292;&#20197;&#20248;&#21270;&#21333;&#20010;&#35757;&#32451;&#20998;&#24067;&#30340;&#39044;&#26399;&#25104;&#26412;&#20026;&#21103;&#20316;&#29992;&#23398;&#20064;&#34920;&#31034;&#30340;&#20027;&#23548;&#26041;&#27861;&#26159;&#21542;&#20173;&#28982;&#26159;&#19968;&#20010;&#22909;&#26041;&#27861;&#65311;&#25105;&#20204;&#30340;&#35770;&#28857;&#26159;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#27604;&#21333;&#20010;&#20248;&#21270;&#36807;&#31243;&#33719;&#24471;&#30340;&#26356;&#20016;&#23500;&#30340;&#34920;&#31034;&#26356;&#22909;&#12290;&#25105;&#20204;&#29992;&#31616;&#21333;&#30340;&#29702;&#35770;&#35770;&#35777;&#21644;&#21033;&#29992;&#19968;&#20010;&#34920;&#38754;&#19978;&#22825;&#30495;&#30340;&#38598;&#25104;&#25216;&#26415;&#36827;&#34892;&#30340;&#23454;&#39564;&#26469;&#25903;&#25345;&#36825;&#20010;&#35770;&#28857;&#65306;&#23558;&#20174;&#22810;&#20010;&#35757;&#32451;&#36807;&#31243;&#33719;&#24471;&#30340;&#34920;&#31034;&#36830;&#25509;&#36215;&#26469;&#65292;&#36825;&#20123;&#35757;&#32451;&#36807;&#31243;&#20351;&#29992;&#30456;&#21516;&#30340;&#25968;&#25454;&#12289;&#27169;&#22411;&#12289;&#31639;&#27861;&#21644;&#36229;&#21442;&#25968;&#65292;&#20294;&#20351;&#29992;&#19981;&#21516;&#30340;&#38543;&#26426;&#31181;&#23376;&#36827;&#34892;&#35757;&#32451;&#12290;&#36825;&#20123;&#29420;&#31435;&#35757;&#32451;&#30340;&#32593;&#32476;&#34920;&#29616;&#30456;&#20284;&#12290;&#28982;&#32780;&#65292;&#22312;&#28041;&#21450;&#26032;&#20998;&#24067;&#30340;&#19968;&#20123;&#24773;&#20917;&#19979;&#65292;&#36830;&#25509;&#30340;&#34920;&#31034;&#27604;&#29992;&#21333;&#20010;&#35757;&#32451;&#36816;&#34892;&#35757;&#32451;&#30340;&#31561;&#25928;&#22823;&#23567;&#30340;&#32593;&#32476;&#34920;&#29616;&#20986;&#33394;&#12290;&#36825;&#35777;&#26126;&#20102;&#22810;&#20010;&#35757;&#32451;&#36807;&#31243;&#26500;&#24314;&#30340;&#34920;&#31034;&#23454;&#38469;&#19978;&#26159;&#19981;&#21516;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Does the dominant approach to learn representations (as a side effect of optimizing an expected cost for a single training distribution) remain a good approach when we are dealing with multiple distributions? Our thesis is that such scenarios are better served by representations that are richer than those obtained with a single optimization episode. We support this thesis with simple theoretical arguments and with experiments utilizing an apparently na\"{\i}ve ensembling technique: concatenating the representations obtained from multiple training episodes using the same data, model, algorithm, and hyper-parameters, but different random seeds. These independently trained networks perform similarly. Yet, in a number of scenarios involving new distributions, the concatenated representation performs substantially better than an equivalently sized network trained with a single training run. This proves that the representations constructed by multiple training episodes are in fact different.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DARE-GP&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#22312;&#19981;&#24433;&#21709;&#26234;&#33021;&#38899;&#31665;&#25928;&#29992;&#30340;&#24773;&#20917;&#19979;&#65292;&#36867;&#36991;&#19982;&#26234;&#33021;&#38899;&#31665;&#30456;&#36830;&#30340;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#20998;&#31867;&#22120;&#65292;&#20174;&#32780;&#20445;&#25252;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2211.09273</link><description>&lt;p&gt;
&#38024;&#23545;&#22768;&#23398;&#23545;&#25239;&#24615;&#26426;&#22120;&#23398;&#20064;&#36867;&#36991;&#30340;&#23454;&#26102;&#35821;&#38899;&#24773;&#24863;&#26816;&#27979;&#30340;&#38544;&#31169;&#20445;&#25252;
&lt;/p&gt;
&lt;p&gt;
Privacy against Real-Time Speech Emotion Detection via Acoustic Adversarial Evasion of Machine Learning. (arXiv:2211.09273v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09273
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DARE-GP&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#22312;&#19981;&#24433;&#21709;&#26234;&#33021;&#38899;&#31665;&#25928;&#29992;&#30340;&#24773;&#20917;&#19979;&#65292;&#36867;&#36991;&#19982;&#26234;&#33021;&#38899;&#31665;&#30456;&#36830;&#30340;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#20998;&#31867;&#22120;&#65292;&#20174;&#32780;&#20445;&#25252;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#30417;&#35270;&#26159;&#19968;&#20010;&#26032;&#20852;&#39046;&#22495;&#65292;&#28041;&#21450;&#21040;&#24191;&#27867;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;&#36825;&#20123;&#38382;&#39064;&#22312;&#26222;&#36941;&#23384;&#22312;&#30340;&#29289;&#32852;&#32593;&#35774;&#22791;&#21644;&#22810;&#20256;&#24863;&#22120;&#30340;&#25903;&#25345;&#19979;&#65292;&#21464;&#24471;&#26356;&#21152;&#20005;&#37325;&#65292;&#32780;&#36825;&#20123;&#35774;&#22791;&#21487;&#20197;&#25903;&#25345;&#36825;&#20123;&#30417;&#35270;&#29992;&#20363;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#20010;&#36825;&#26679;&#30340;&#24212;&#29992;&#26696;&#20363;&#65306;&#20351;&#29992;&#19982;&#26234;&#33021;&#38899;&#31665;&#30456;&#36830;&#30340;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#65288;SER&#65289;&#20998;&#31867;&#22120;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#19981;&#24433;&#21709;&#26234;&#33021;&#38899;&#31665;&#30340;&#25928;&#29992;&#30340;&#24773;&#20917;&#19979;&#65292;&#36867;&#36991;&#19982;&#26234;&#33021;&#38899;&#31665;&#30456;&#36830;&#30340;&#40657;&#30418;SER&#20998;&#31867;&#22120;&#12290;&#36890;&#36807;&#23545;&#26426;&#22120;&#23398;&#20064;&#30340;&#23545;&#25239;&#36867;&#36991;&#30340;&#35270;&#35282;&#26469;&#32771;&#34385;&#36825;&#20010;&#38544;&#31169;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#21517;&#20026;&#8220;&#36890;&#36807;&#36951;&#20256;&#35268;&#21010;&#20987;&#36133;&#22768;&#23398;&#24773;&#24863;&#35782;&#21035;&#30340;DARE-GP&#8221;&#65292;&#23427;&#20351;&#29992;&#36951;&#20256;&#35268;&#21010;&#29983;&#25104;&#38750;&#20405;&#20837;&#24615;&#30340;&#28155;&#21152;&#38899;&#39057;&#25200;&#21160;&#65288;AAPs&#65289;&#12290;&#36890;&#36807;&#32422;&#26463;&#36825;&#20123;AAP&#30340;&#36827;&#21270;&#65292;&#21487;&#20197;&#20445;&#25252;&#36716;&#24405;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#38477;&#20302;SER&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;AAP&#30340;&#21152;&#24615;&#29305;&#24615;&#65292;&#20197;&#21450;&#20026;&#29305;&#23450;&#30446;&#26631;&#29983;&#25104;&#36825;&#20123;AAPs&#30340;&#26041;&#27861;&#65292;&#20351;DARE-GP&#25104;&#20026;&#19968;&#20010;&#26377;&#25928;&#30340;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emotional Surveillance is an emerging area with wide-reaching privacy concerns. These concerns are exacerbated by ubiquitous IoT devices with multiple sensors that can support these surveillance use cases. The work presented here considers one such use case: the use of a speech emotion recognition (SER) classifier tied to a smart speaker. This work demonstrates the ability to evade black-box SER classifiers tied to a smart speaker without compromising the utility of the smart speaker. This privacy concern is considered through the lens of adversarial evasion of machine learning. Our solution, Defeating Acoustic Recognition of Emotion via Genetic Programming (DARE-GP), uses genetic programming to generate non-invasive additive audio perturbations (AAPs). By constraining the evolution of these AAPs, transcription accuracy can be protected while simultaneously degrading SER classifier performance. The additive nature of these AAPs, along with an approach that generates these AAPs for a fi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#20302;&#32500;&#21160;&#21147;&#23398;&#27169;&#22411;&#26041;&#27861;&#65292;&#29992;&#20110;&#25429;&#25417;Kolmogorov&#27969;&#30340;&#27969;&#21160;&#21160;&#21147;&#23398;&#21644;&#23646;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;&#27424;&#23436;&#22791;&#33258;&#32534;&#30721;&#22120;&#36827;&#34892;&#32500;&#24230;&#38477;&#20302;&#21644;&#28508;&#22312;&#31354;&#38388;&#30340;&#31163;&#25955;&#26102;&#38388;&#27169;&#22411;&#24320;&#21457;&#65292;&#25105;&#20204;&#21487;&#20197;&#26377;&#25928;&#22320;&#24314;&#27169;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#28151;&#27788;&#21644;&#38388;&#27463;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2210.16708</link><description>&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;Kolmogorov&#27969;&#20302;&#32500;&#21160;&#21147;&#23398;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Data-driven low-dimensional dynamic model of Kolmogorov flow. (arXiv:2210.16708v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.16708
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#20302;&#32500;&#21160;&#21147;&#23398;&#27169;&#22411;&#26041;&#27861;&#65292;&#29992;&#20110;&#25429;&#25417;Kolmogorov&#27969;&#30340;&#27969;&#21160;&#21160;&#21147;&#23398;&#21644;&#23646;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;&#27424;&#23436;&#22791;&#33258;&#32534;&#30721;&#22120;&#36827;&#34892;&#32500;&#24230;&#38477;&#20302;&#21644;&#28508;&#22312;&#31354;&#38388;&#30340;&#31163;&#25955;&#26102;&#38388;&#27169;&#22411;&#24320;&#21457;&#65292;&#25105;&#20204;&#21487;&#20197;&#26377;&#25928;&#22320;&#24314;&#27169;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#28151;&#27788;&#21644;&#38388;&#27463;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#20197;&#27169;&#25311;&#27969;&#21160;&#21160;&#21147;&#23398;&#12289;&#24182;&#29992;&#20110;&#27169;&#22411;&#25511;&#21046;&#26041;&#27861;&#30340;&#38477;&#32500;&#27169;&#22411;&#26159;&#20855;&#26377;&#21560;&#24341;&#21147;&#30340;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26377;&#25928;&#25429;&#25417;&#27969;&#21160;&#30340;&#21160;&#21147;&#23398;&#21644;&#24615;&#36136;&#30340;&#26368;&#23567;&#32500;&#24230;&#27169;&#22411;&#12290;&#25105;&#20204;&#23558;&#20854;&#24212;&#29992;&#20110;&#22312;&#28151;&#27788;&#21644;&#38388;&#27463;&#34892;&#20026;&#20013;&#30340;Kolmogorov&#27969;&#65292;&#36825;&#22312;&#35768;&#22810;&#27969;&#31243;&#20013;&#24456;&#24120;&#35265;&#19988;&#38590;&#20197;&#24314;&#27169;&#12290;&#27969;&#21160;&#30340;&#36712;&#36857;&#25509;&#36817;&#20110;&#30456;&#23545;&#21608;&#26399;&#36712;&#36947;(RPOs)&#65292;&#24182;&#22841;&#26434;&#30528;&#19982;&#21253;&#21547;RPOs&#30340;&#21306;&#22495;&#20043;&#38388;&#30340;&#20598;&#21457;&#29190;&#21457;&#20107;&#20214;&#30456;&#23545;&#24212;&#30340;&#36828;&#36275;&#12290;&#27169;&#22411;&#24320;&#21457;&#30340;&#31532;&#19968;&#27493;&#26159;&#20351;&#29992;&#19968;&#20010;&#27424;&#23436;&#22791;&#30340;&#33258;&#32534;&#30721;&#22120;&#23558;&#23436;&#25972;&#29366;&#24577;&#25968;&#25454;&#26144;&#23556;&#21040;&#32500;&#24230;&#26174;&#33879;&#36739;&#20302;&#30340;&#28508;&#22312;&#31354;&#38388;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#28508;&#22312;&#31354;&#38388;&#21160;&#21147;&#23398;&#30340;&#31163;&#25955;&#26102;&#38388;&#28436;&#21270;&#27169;&#22411;&#12290;&#36890;&#36807;&#20998;&#26512;&#27169;&#22411;&#22312;&#28508;&#22312;&#31354;&#38388;&#32500;&#24230;&#19978;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#21487;&#20197;&#30830;&#23450;&#26368;&#20339;&#30340;&#32500;&#24230;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reduced order models (ROMs) that capture flow dynamics are of interest for decreasing computational costs for simulation as well as for model-based control approaches. This work presents a data-driven framework for minimal-dimensional models that effectively capture the dynamics and properties of the flow. We apply this to Kolmogorov flow in a regime consisting of chaotic and intermittent behavior, which is common in many flows processes and is challenging to model. The trajectory of the flow travels near relative periodic orbits (RPOs), interspersed with sporadic bursting events corresponding to excursions between the regions containing the RPOs. The first step in development of the models is use of an undercomplete autoencoder to map from the full state data down to a latent space of dramatically lower dimension. Then models of the discrete-time evolution of the dynamics in the latent space are developed. By analyzing the model performance as a function of latent space dimension we c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#31867;&#20998;&#31867;&#22120;&#21644;&#24191;&#20041;&#20284;&#28982;&#27604;&#26816;&#39564;&#30340;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#22810;&#23618;&#24863;&#30693;&#22120;&#31070;&#32463;&#32593;&#32476;&#21644;&#25903;&#25345;&#21521;&#37327;&#26426;&#27169;&#22411;&#22312;&#25910;&#25947;&#26102;&#20250;&#34920;&#29616;&#20026;&#24191;&#20041;&#20284;&#28982;&#27604;&#26816;&#39564;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#23637;&#31034;&#20102;&#19968;&#31867;&#26368;&#23567;&#20108;&#20056;SVM&#22312;&#25910;&#25947;&#26102;&#20063;&#33021;&#36798;&#21040;&#24191;&#20041;&#20284;&#28982;&#27604;&#26816;&#39564;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2210.12494</link><description>&lt;p&gt;
&#20851;&#20110;&#24191;&#20041;&#20284;&#28982;&#27604;&#26816;&#39564;&#21644;&#19968;&#31867;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
On the Generalized Likelihood Ratio Test and One-Class Classifiers. (arXiv:2210.12494v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.12494
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#31867;&#20998;&#31867;&#22120;&#21644;&#24191;&#20041;&#20284;&#28982;&#27604;&#26816;&#39564;&#30340;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#22810;&#23618;&#24863;&#30693;&#22120;&#31070;&#32463;&#32593;&#32476;&#21644;&#25903;&#25345;&#21521;&#37327;&#26426;&#27169;&#22411;&#22312;&#25910;&#25947;&#26102;&#20250;&#34920;&#29616;&#20026;&#24191;&#20041;&#20284;&#28982;&#27604;&#26816;&#39564;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#23637;&#31034;&#20102;&#19968;&#31867;&#26368;&#23567;&#20108;&#20056;SVM&#22312;&#25910;&#25947;&#26102;&#20063;&#33021;&#36798;&#21040;&#24191;&#20041;&#20284;&#28982;&#27604;&#26816;&#39564;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#31867;&#20998;&#31867;&#65288;OCC&#65289;&#26159;&#20915;&#23450;&#35266;&#23519;&#26679;&#26412;&#26159;&#21542;&#23646;&#20110;&#30446;&#26631;&#31867;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#32771;&#34385;&#22312;&#21253;&#21547;&#30446;&#26631;&#31867;&#26679;&#26412;&#30340;&#25968;&#25454;&#38598;&#19978;&#23398;&#20064;&#19968;&#20010;&#34920;&#29616;&#20026;&#24191;&#20041;&#20284;&#28982;&#27604;&#26816;&#39564;&#65288;GLRT&#65289;&#30340;OCC&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;&#24403;&#30446;&#26631;&#31867;&#30340;&#32479;&#35745;&#20449;&#24687;&#21487;&#29992;&#26102;&#65292;GLRT&#35299;&#20915;&#20102;&#30456;&#21516;&#30340;&#38382;&#39064;&#12290;GLRT&#26159;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#19988;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#21487;&#35777;&#26126;&#26368;&#20339;&#30340;&#20998;&#31867;&#22120;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#22810;&#23618;&#24863;&#30693;&#22120;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#21644;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#27169;&#22411;&#12290;&#23427;&#20204;&#20351;&#29992;&#20154;&#24037;&#25968;&#25454;&#38598;&#35757;&#32451;&#20026;&#20004;&#31867;&#20998;&#31867;&#22120;&#65292;&#20854;&#20013;&#26367;&#20195;&#31867;&#20351;&#29992;&#22312;&#30446;&#26631;&#31867;&#25968;&#25454;&#38598;&#30340;&#23450;&#20041;&#22495;&#19978;&#22343;&#21248;&#29983;&#25104;&#30340;&#38543;&#26426;&#26679;&#26412;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#36866;&#24403;&#30340;&#20551;&#35774;&#19979;&#65292;&#27169;&#22411;&#22312;&#22823;&#25968;&#25454;&#38598;&#19978;&#25910;&#25947;&#21040;&#20102;GLRT&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#20855;&#26377;&#36866;&#24403;&#26680;&#20989;&#25968;&#30340;&#19968;&#31867;&#26368;&#23567;&#20108;&#20056;SVM&#65288;OCLSSVM&#65289;&#22312;&#25910;&#25947;&#26102;&#34920;&#29616;&#20026;GLRT&#12290;
&lt;/p&gt;
&lt;p&gt;
One-class classification (OCC) is the problem of deciding whether an observed sample belongs to a target class. We consider the problem of learning an OCC model that performs as the generalized likelihood ratio test (GLRT), given a dataset containing samples of the target class. The GLRT solves the same problem when the statistics of the target class are available. The GLRT is a well-known and provably optimal (under specific assumptions) classifier. To this end, we consider both the multilayer perceptron neural network (NN) and the support vector machine (SVM) models. They are trained as two-class classifiers using an artificial dataset for the alternative class, obtained by generating random samples, uniformly over the domain of the target-class dataset. We prove that, under suitable assumptions, the models converge (with a large dataset) to the GLRT. Moreover, we show that the one-class least squares SVM (OCLSSVM) with suitable kernels at convergence performs as the GLRT. Lastly, we
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#27963;&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#21327;&#26041;&#24046;&#30697;&#38453;&#19978;&#20855;&#26377;&#20302;&#31209;&#32467;&#26500;&#32422;&#26463;&#30340;&#22270;&#23398;&#20064;&#12290;&#36890;&#36807;&#20351;&#29992;Riemannian&#20248;&#21270;&#65292;&#21033;&#29992;&#27491;&#23450;&#30697;&#38453;&#21644;&#22266;&#23450;&#31209;&#27491;&#21322;&#23450;&#30697;&#38453;&#30340;&#20960;&#20309;&#29305;&#24615;&#65292;&#35299;&#20915;&#20102;&#36825;&#31867;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2210.11950</link><description>&lt;p&gt;
&#29992;Riemannian&#20248;&#21270;&#23398;&#20064;&#22270;&#24418;&#22240;&#23376;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning Graphical Factor Models with Riemannian Optimization. (arXiv:2210.11950v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.11950
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#27963;&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#21327;&#26041;&#24046;&#30697;&#38453;&#19978;&#20855;&#26377;&#20302;&#31209;&#32467;&#26500;&#32422;&#26463;&#30340;&#22270;&#23398;&#20064;&#12290;&#36890;&#36807;&#20351;&#29992;Riemannian&#20248;&#21270;&#65292;&#21033;&#29992;&#27491;&#23450;&#30697;&#38453;&#21644;&#22266;&#23450;&#31209;&#27491;&#21322;&#23450;&#30697;&#38453;&#30340;&#20960;&#20309;&#29305;&#24615;&#65292;&#35299;&#20915;&#20102;&#36825;&#31867;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#24418;&#27169;&#22411;&#21644;&#22240;&#23376;&#20998;&#26512;&#26159;&#22810;&#20803;&#32479;&#35745;&#23398;&#20013;&#25104;&#29087;&#30340;&#24037;&#20855;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#37117;&#21487;&#20197;&#19982;&#21327;&#26041;&#24046;&#21644;&#31934;&#24230;&#30697;&#38453;&#30340;&#32467;&#26500;&#32852;&#31995;&#36215;&#26469;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#22312;&#22270;&#30340;&#23398;&#20064;&#36807;&#31243;&#20013;&#27809;&#26377;&#34987;&#20849;&#21516;&#21033;&#29992;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#22312;&#21327;&#26041;&#24046;&#30697;&#38453;&#19978;&#20855;&#26377;&#20302;&#31209;&#32467;&#26500;&#32422;&#26463;&#30340;&#22270;&#23398;&#20064;&#30340;&#28789;&#27963;&#31639;&#27861;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#35813;&#38382;&#39064;&#34987;&#34920;&#36798;&#20026;&#22522;&#20110;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#30340;&#32602;&#20989;&#25968;&#26041;&#27861;&#65292;&#20854;&#20013;&#21327;&#26041;&#24046;&#30697;&#38453;&#21487;&#20197;&#36873;&#25321;&#24615;&#22320;&#34987;&#32422;&#26463;&#20026;&#20302;&#31209;&#21152;&#23545;&#35282;&#32447;&#32467;&#26500;&#65288;&#20302;&#31209;&#22240;&#23376;&#27169;&#22411;&#65289;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#27491;&#23450;&#30697;&#38453;&#21644;&#22266;&#23450;&#31209;&#27491;&#21322;&#23450;&#30697;&#38453;&#30340;&#20960;&#20309;&#29305;&#24615;&#65288;&#36825;&#20123;&#29305;&#24615;&#38750;&#24120;&#36866;&#29992;&#20110;&#26925;&#22278;&#27169;&#22411;&#65289;&#26469;&#35299;&#20915;&#36825;&#31867;&#38382;&#39064;&#20013;&#30340;&#26368;&#20248;&#21270;&#38382;&#39064;&#12290;&#25968;&#20540;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
Graphical models and factor analysis are well-established tools in multivariate statistics. While these models can be both linked to structures exhibited by covariance and precision matrices, they are generally not jointly leveraged within graph learning processes. This paper therefore addresses this issue by proposing a flexible algorithmic framework for graph learning under low-rank structural constraints on the covariance matrix. The problem is expressed as penalized maximum likelihood estimation of an elliptical distribution (a generalization of Gaussian graphical models to possibly heavy-tailed distributions), where the covariance matrix is optionally constrained to be structured as low-rank plus diagonal (low-rank factor model). The resolution of this class of problems is then tackled with Riemannian optimization, where we leverage geometries of positive definite matrices and positive semi-definite matrices of fixed rank that are well suited to elliptical models. Numerical experi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#25991;&#26412;&#21040;&#22270;&#20687;AI&#38598;&#25104;&#21040;3D&#35774;&#35745;&#24037;&#20316;&#27969;&#31243;&#20013;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25554;&#20214;3DALL-E&#65292;&#35774;&#35745;&#24072;&#21487;&#20197;&#20351;&#29992;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#28789;&#24863;&#26500;&#24314;3D&#27169;&#22411;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#35774;&#35745;&#24072;&#23545;&#20110;3DALL-E&#22312;&#24037;&#20316;&#27969;&#31243;&#20013;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#65292;&#21487;&#20197;&#29983;&#25104;&#21442;&#32771;&#22270;&#20687;&#12289;&#38450;&#27490;&#35774;&#35745;&#22266;&#21270;&#24182;&#28608;&#21457;&#35774;&#35745;&#30340;&#32771;&#34385;&#12290;</title><link>http://arxiv.org/abs/2210.11603</link><description>&lt;p&gt;
3DALL-E: &#23558;&#25991;&#26412;&#21040;&#22270;&#20687;AI&#38598;&#25104;&#21040;3D&#35774;&#35745;&#24037;&#20316;&#27969;&#31243;&#20013;
&lt;/p&gt;
&lt;p&gt;
3DALL-E: Integrating Text-to-Image AI in 3D Design Workflows. (arXiv:2210.11603v2 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.11603
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#25991;&#26412;&#21040;&#22270;&#20687;AI&#38598;&#25104;&#21040;3D&#35774;&#35745;&#24037;&#20316;&#27969;&#31243;&#20013;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25554;&#20214;3DALL-E&#65292;&#35774;&#35745;&#24072;&#21487;&#20197;&#20351;&#29992;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#28789;&#24863;&#26500;&#24314;3D&#27169;&#22411;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#35774;&#35745;&#24072;&#23545;&#20110;3DALL-E&#22312;&#24037;&#20316;&#27969;&#31243;&#20013;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#65292;&#21487;&#20197;&#29983;&#25104;&#21442;&#32771;&#22270;&#20687;&#12289;&#38450;&#27490;&#35774;&#35745;&#22266;&#21270;&#24182;&#28608;&#21457;&#35774;&#35745;&#30340;&#32771;&#34385;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;AI&#33021;&#22815;&#29983;&#25104;&#26032;&#39062;&#30340;&#22270;&#20687;&#20197;&#20379;&#28789;&#24863;&#65292;&#20294;&#20854;&#22312;3D&#35774;&#35745;&#24037;&#20316;&#27969;&#31243;&#20013;&#30340;&#24212;&#29992;&#20197;&#21450;&#35774;&#35745;&#24072;&#22914;&#20309;&#21033;&#29992;AI&#25552;&#20379;&#30340;&#28789;&#24863;&#26500;&#24314;3D&#27169;&#22411;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;DALL-E&#12289;GPT-3&#21644;CLIP&#38598;&#25104;&#21040;CAD&#36719;&#20214;&#20013;&#65292;&#21019;&#24314;&#20102;3DALL-E&#25554;&#20214;&#65292;&#29992;&#20110;&#29983;&#25104;3D&#35774;&#35745;&#30340;2D&#22270;&#20687;&#28789;&#24863;&#12290;3DALL-E&#20801;&#35768;&#29992;&#25143;&#22522;&#20110;&#20182;&#20204;&#27491;&#22312;&#24314;&#27169;&#30340;&#20869;&#23481;&#26500;&#24314;&#25991;&#26412;&#21644;&#22270;&#20687;&#25552;&#31034;&#12290;&#22312;&#19968;&#39033;&#28041;&#21450;13&#21517;&#35774;&#35745;&#24072;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#35774;&#35745;&#24072;&#20204;&#35748;&#20026;3DALL-E&#22312;&#20182;&#20204;&#30340;&#24037;&#20316;&#27969;&#31243;&#20013;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#21487;&#20197;&#20351;&#29992;&#25991;&#26412;&#21040;&#22270;&#20687;AI&#29983;&#25104;&#21442;&#32771;&#22270;&#20687;&#65292;&#38450;&#27490;&#35774;&#35745;&#22266;&#21270;&#65292;&#24182;&#28608;&#21457;&#35774;&#35745;&#30340;&#32771;&#34385;&#12290;&#25105;&#20204;&#35814;&#36848;&#20102;&#22312;3D&#24314;&#27169;&#20219;&#21153;&#20013;&#35266;&#23519;&#21040;&#30340;&#25552;&#31034;&#27169;&#24335;&#65292;&#24182;&#25552;&#20379;&#20102;&#21442;&#19982;&#32773;&#25152;&#35266;&#23519;&#21040;&#30340;&#25552;&#31034;&#22797;&#26434;&#24615;&#30340;&#24230;&#37327;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#22914;&#20309;&#23558;3DALL-E&#19982;&#29616;&#26377;&#30340;&#29983;&#25104;&#24335;&#35774;&#35745;&#24037;&#20316;&#27969;&#31243;&#30456;&#32467;&#21512;&#65292;&#24182;&#25552;&#20986;&#20102;&#25552;&#31034;&#25991;&#29486;&#30446;&#24405;&#20316;&#20026;&#19968;&#31181;&#20154;&#24037;&#26234;&#33021;-&#20154;&#31867;&#35774;&#35745;&#21382;&#21490;&#30340;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image AI are capable of generating novel images for inspiration, but their applications for 3D design workflows and how designers can build 3D models using AI-provided inspiration have not yet been explored. To investigate this, we integrated DALL-E, GPT-3, and CLIP within a CAD software in 3DALL-E, a plugin that generates 2D image inspiration for 3D design. 3DALL-E allows users to construct text and image prompts based on what they are modeling. In a study with 13 designers, we found that designers saw great potential in 3DALL-E within their workflows and could use text-to-image AI to produce reference images, prevent design fixation, and inspire design considerations. We elaborate on prompting patterns observed across 3D modeling tasks and provide measures of prompt complexity observed across participants. From our findings, we discuss how 3DALL-E can merge with existing generative design workflows and propose prompt bibliographies as a form of human-AI design history.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25193;&#23637;&#20102;RibSeg&#25968;&#25454;&#38598;&#21040;&#22823;&#35268;&#27169;&#22522;&#20934;&#27979;&#35797;RibSeg v2&#65292;&#21152;&#20837;&#20102;&#25163;&#21160;&#26631;&#27880;&#30340;&#32907;&#39592;&#26631;&#35760;&#21644;&#35299;&#21078;&#20013;&#24515;&#32447;&#25552;&#21462;&#65292;&#20849;&#21253;&#21547;660&#20010;CT&#25195;&#25551;&#65288;15,466&#20010;&#29420;&#31435;&#30340;&#32907;&#39592;&#65289;&#65292;&#24182;&#25552;&#20986;&#20102;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#32907;&#39592;&#26631;&#35760;&#12289;&#22522;&#20110;&#39592;&#26550;&#21270;&#26041;&#27861;&#29992;&#20110;&#20013;&#24515;&#32447;&#25552;&#21462;&#12289;&#19968;&#31181;&#31232;&#30095;&#28857;&#20113;&#34920;&#31034;CT&#25195;&#25551;&#30340;&#26041;&#27861;&#65292;&#20197;&#21450;&#36866;&#29992;&#20110;&#35813;&#20219;&#21153;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2210.09309</link><description>&lt;p&gt;
RibSeg v2&#65306;&#32907;&#39592;&#26631;&#35760;&#21644;&#35299;&#21078;&#20013;&#24515;&#32447;&#25552;&#21462;&#30340;&#22823;&#35268;&#27169;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
RibSeg v2: A Large-scale Benchmark for Rib Labeling and Anatomical Centerline Extraction. (arXiv:2210.09309v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.09309
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25193;&#23637;&#20102;RibSeg&#25968;&#25454;&#38598;&#21040;&#22823;&#35268;&#27169;&#22522;&#20934;&#27979;&#35797;RibSeg v2&#65292;&#21152;&#20837;&#20102;&#25163;&#21160;&#26631;&#27880;&#30340;&#32907;&#39592;&#26631;&#35760;&#21644;&#35299;&#21078;&#20013;&#24515;&#32447;&#25552;&#21462;&#65292;&#20849;&#21253;&#21547;660&#20010;CT&#25195;&#25551;&#65288;15,466&#20010;&#29420;&#31435;&#30340;&#32907;&#39592;&#65289;&#65292;&#24182;&#25552;&#20986;&#20102;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#32907;&#39592;&#26631;&#35760;&#12289;&#22522;&#20110;&#39592;&#26550;&#21270;&#26041;&#27861;&#29992;&#20110;&#20013;&#24515;&#32447;&#25552;&#21462;&#12289;&#19968;&#31181;&#31232;&#30095;&#28857;&#20113;&#34920;&#31034;CT&#25195;&#25551;&#30340;&#26041;&#27861;&#65292;&#20197;&#21450;&#36866;&#29992;&#20110;&#35813;&#20219;&#21153;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#30340;&#32907;&#39592;&#26631;&#35760;&#21644;&#35299;&#21078;&#20013;&#24515;&#32447;&#25552;&#21462;&#26159;&#21508;&#31181;&#20020;&#24202;&#24212;&#29992;&#30340;&#24120;&#35265;&#21069;&#25552;&#26465;&#20214;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#35201;&#20040;&#20351;&#29992;&#20869;&#37096;&#25968;&#25454;&#38598;&#65292;&#26080;&#27861;&#20026;&#31038;&#32676;&#25152;&#20849;&#20139;&#65292;&#35201;&#20040;&#21482;&#20851;&#27880;&#20110;&#32907;&#39592;&#20998;&#21106;&#32780;&#24573;&#30053;&#20102;&#32907;&#39592;&#26631;&#35760;&#30340;&#20020;&#24202;&#24847;&#20041;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#23558;&#20043;&#21069;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#32907;&#39592;&#20998;&#21106;&#30340;RibSeg&#25968;&#25454;&#38598;&#25193;&#23637;&#20026;&#32508;&#21512;&#24615;&#22522;&#20934;&#27979;&#35797;RibSeg v2&#65292;&#24182;&#21152;&#20837;&#20102;&#32907;&#39592;&#26631;&#35760;&#21644;&#35299;&#21078;&#20013;&#24515;&#32447;&#25552;&#21462;&#30340;&#25163;&#21160;&#26631;&#27880;&#65292;&#20849;&#21253;&#21547;&#20102;660&#20010;CT&#25195;&#25551;&#65288;15,466&#20010;&#29420;&#31435;&#30340;&#32907;&#39592;&#65289;&#12290;&#22522;&#20110;RibSeg v2&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21253;&#21547;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#32907;&#39592;&#26631;&#35760;&#65292;&#20197;&#21450;&#22522;&#20110;&#39592;&#26550;&#21270;&#26041;&#27861;&#29992;&#20110;&#20013;&#24515;&#32447;&#25552;&#21462;&#30340;&#27969;&#31243;&#12290;&#20026;&#20102;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;&#28857;&#20113;&#34920;&#31034;CT&#25195;&#25551;&#30340;&#26041;&#27861;&#65292;&#19982;&#26631;&#20934;&#30340;&#23494;&#38598;&#20307;&#32032;&#32593;&#26684;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35774;&#35745;&#21644;&#20998;&#26512;&#35780;&#20272;&#25351;&#26631;&#65292;&#20197;&#35299;&#20915;&#27599;&#20010;&#20219;&#21153;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#26159;&#65292;&#8230;
&lt;/p&gt;
&lt;p&gt;
Automatic rib labeling and anatomical centerline extraction are common prerequisites for various clinical applications. Prior studies either use in-house datasets that are inaccessible to communities, or focus on rib segmentation that neglects the clinical significance of rib labeling. To address these issues, we extend our prior dataset (RibSeg) on the binary rib segmentation task to a comprehensive benchmark, named RibSeg v2, with 660 CT scans (15,466 individual ribs in total) and annotations manually inspected by experts for rib labeling and anatomical centerline extraction. Based on the RibSeg v2, we develop a pipeline including deep learning-based methods for rib labeling, and a skeletonization-based method for centerline extraction. To improve computational efficiency, we propose a sparse point cloud representation of CT scans and compare it with standard dense voxel grids. Moreover, we design and analyze evaluation metrics to address the key challenges of each task. Our dataset,
&lt;/p&gt;</description></item><item><title>FAIR&#21407;&#21017;&#26088;&#22312;&#23454;&#29616;&#23398;&#26415;&#25968;&#25454;&#30340;&#37325;&#29992;&#24615;&#65292;&#24182;&#22312;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#21644;&#25968;&#25454;&#39046;&#22495;&#24471;&#21040;&#24212;&#29992;&#12290;&#30740;&#31350;&#20154;&#21592;&#20174;&#19981;&#21516;&#32972;&#26223;&#21644;&#23398;&#31185;&#30340;&#35282;&#24230;&#35752;&#35770;&#20102;&#20182;&#20204;&#22312;&#23454;&#36341;&#31038;&#21306;&#20013;&#23450;&#20041;&#21644;&#37319;&#29992;FAIR&#21407;&#21017;&#30340;&#32463;&#39564;&#21644;&#25104;&#26524;&#65292;&#20197;&#21450;&#36861;&#27714;&#21644;&#28608;&#21169;FAIR&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#21487;&#33021;&#24102;&#26469;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2210.08973</link><description>&lt;p&gt;
FAIR for AI:&#36328;&#23398;&#31185;&#21644;&#22269;&#38469;&#31038;&#21306;&#24314;&#35774;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
FAIR for AI: An interdisciplinary and international community building perspective. (arXiv:2210.08973v2 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.08973
&lt;/p&gt;
&lt;p&gt;
FAIR&#21407;&#21017;&#26088;&#22312;&#23454;&#29616;&#23398;&#26415;&#25968;&#25454;&#30340;&#37325;&#29992;&#24615;&#65292;&#24182;&#22312;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#21644;&#25968;&#25454;&#39046;&#22495;&#24471;&#21040;&#24212;&#29992;&#12290;&#30740;&#31350;&#20154;&#21592;&#20174;&#19981;&#21516;&#32972;&#26223;&#21644;&#23398;&#31185;&#30340;&#35282;&#24230;&#35752;&#35770;&#20102;&#20182;&#20204;&#22312;&#23454;&#36341;&#31038;&#21306;&#20013;&#23450;&#20041;&#21644;&#37319;&#29992;FAIR&#21407;&#21017;&#30340;&#32463;&#39564;&#21644;&#25104;&#26524;&#65292;&#20197;&#21450;&#36861;&#27714;&#21644;&#28608;&#21169;FAIR&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#21487;&#33021;&#24102;&#26469;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
2016&#24180;&#65292;&#25552;&#20986;&#20102;&#19968;&#32452;&#22522;&#26412;&#30340;&#21487;&#23547;&#25214;&#12289;&#21487;&#35775;&#38382;&#12289;&#21487;&#20114;&#25805;&#20316;&#21644;&#21487;&#37325;&#29992;&#65288;FAIR&#65289;&#21407;&#21017;&#65292;&#20316;&#20026;&#36866;&#24403;&#30340;&#25968;&#25454;&#31649;&#29702;&#21644;&#31649;&#29702;&#30340;&#20808;&#20915;&#26465;&#20214;&#65292;&#26088;&#22312;&#23454;&#29616;&#23398;&#26415;&#25968;&#25454;&#30340;&#21487;&#37325;&#29992;&#24615;&#12290;&#36825;&#20123;&#21407;&#21017;&#20063;&#36866;&#29992;&#20110;&#20854;&#20182;&#25968;&#23383;&#36164;&#20135;&#65292;&#24182;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#65292;FAIR&#25351;&#23548;&#21407;&#21017;&#34987;&#37325;&#26032;&#35299;&#37322;&#25110;&#25193;&#23637;&#65292;&#21253;&#25324;&#20135;&#29983;&#25968;&#25454;&#30340;&#36719;&#20214;&#12289;&#24037;&#20855;&#12289;&#31639;&#27861;&#21644;&#24037;&#20316;&#27969;&#31243;&#12290;&#22914;&#20170;&#65292;FAIR&#21407;&#21017;&#27491;&#22312;&#36866;&#24212;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#21644;&#25968;&#25454;&#30340;&#32972;&#26223;&#19979;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26469;&#33258;&#19981;&#21516;&#22269;&#23478;&#12289;&#23398;&#31185;&#21644;&#32972;&#26223;&#30340;&#30740;&#31350;&#20154;&#21592;&#30340;&#35266;&#28857;&#12289;&#24895;&#26223;&#21644;&#32463;&#39564;&#65292;&#20182;&#20204;&#39046;&#23548;&#30528;&#22312;&#20182;&#20204;&#30340;&#23454;&#36341;&#31038;&#21306;&#20013;&#23450;&#20041;&#21644;&#37319;&#29992;FAIR&#21407;&#21017;&#65292;&#24182;&#35752;&#35770;&#36861;&#27714;&#21644;&#28608;&#21169;FAIR&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#21487;&#33021;&#20135;&#29983;&#30340;&#32467;&#26524;&#12290;&#26412;&#25253;&#21578;&#30340;&#26448;&#26009;&#22522;&#20110;2022&#24180;6&#26376;7&#26085;&#22312;&#38463;&#36129;&#22269;&#23478;&#23454;&#39564;&#23460;&#20030;&#34892;&#30340;FAIR for AI&#30740;&#35752;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
A foundational set of findable, accessible, interoperable, and reusable (FAIR) principles were proposed in 2016 as prerequisites for proper data management and stewardship, with the goal of enabling the reusability of scholarly data. The principles were also meant to apply to other digital assets, at a high level, and over time, the FAIR guiding principles have been re-interpreted or extended to include the software, tools, algorithms, and workflows that produce data. FAIR principles are now being adapted in the context of AI models and datasets. Here, we present the perspectives, vision, and experiences of researchers from different countries, disciplines, and backgrounds who are leading the definition and adoption of FAIR principles in their communities of practice, and discuss outcomes that may result from pursuing and incentivizing FAIR AI research. The material for this report builds on the FAIR for AI Workshop held at Argonne National Laboratory on June 7, 2022.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#23545;&#26410;&#30693;&#25968;&#37327;&#30340;&#21333;&#36890;&#36947;&#27700;&#22768;&#20449;&#21495;&#36827;&#34892;&#28304;&#20998;&#31163;&#12290;&#36890;&#36807;&#22266;&#23450;&#36755;&#20986;&#36890;&#36947;&#25968;&#37327;&#21644;&#26032;&#30340;&#24615;&#33021;&#35780;&#20272;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#25490;&#21015;&#38382;&#39064;&#24341;&#36215;&#30340;&#32500;&#24230;&#28798;&#38590;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#19982;&#24050;&#30693;&#20449;&#21495;&#25968;&#37327;&#30456;&#20284;&#30340;&#20998;&#31163;&#24615;&#33021;&#12290;&#35813;&#31639;&#27861;&#20855;&#26377;&#31454;&#20105;&#24615;&#33021;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#22312;&#35813;&#26694;&#26550;&#19979;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2207.11749</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#26410;&#30693;&#25968;&#37327;&#21333;&#36890;&#36947;&#27700;&#22768;&#20449;&#21495;&#28304;&#20998;&#31163;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Source Separation of Unknown Numbers of Single-Channel Underwater Acoustic Signals Based on Autoencoders. (arXiv:2207.11749v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.11749
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#23545;&#26410;&#30693;&#25968;&#37327;&#30340;&#21333;&#36890;&#36947;&#27700;&#22768;&#20449;&#21495;&#36827;&#34892;&#28304;&#20998;&#31163;&#12290;&#36890;&#36807;&#22266;&#23450;&#36755;&#20986;&#36890;&#36947;&#25968;&#37327;&#21644;&#26032;&#30340;&#24615;&#33021;&#35780;&#20272;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#25490;&#21015;&#38382;&#39064;&#24341;&#36215;&#30340;&#32500;&#24230;&#28798;&#38590;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#19982;&#24050;&#30693;&#20449;&#21495;&#25968;&#37327;&#30456;&#20284;&#30340;&#20998;&#31163;&#24615;&#33021;&#12290;&#35813;&#31639;&#27861;&#20855;&#26377;&#31454;&#20105;&#24615;&#33021;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#22312;&#35813;&#26694;&#26550;&#19979;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#24456;&#23569;&#26377;&#30740;&#31350;&#20851;&#27880;&#26410;&#30693;&#25968;&#37327;&#20449;&#21495;&#30340;&#28304;&#20998;&#31163;&#38382;&#39064;&#65292;&#20197;&#21450;&#22914;&#20309;&#35780;&#20272;&#31995;&#32479;&#30340;&#24615;&#33021;&#23578;&#19981;&#28165;&#26970;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#22266;&#23450;&#36755;&#20986;&#36890;&#36947;&#25968;&#37327;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36991;&#20813;&#20102;&#30001;&#20110;&#36755;&#20986;&#19982;&#30446;&#26631;&#23545;&#40784;&#24341;&#36215;&#30340;&#25490;&#21015;&#38382;&#39064;&#23548;&#33268;&#30340;&#32500;&#24230;&#28798;&#38590;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#20004;&#27493;&#31639;&#27861;&#65292;&#24182;&#38024;&#23545;&#26377;&#38745;&#38899;&#36890;&#36947;&#30340;&#24773;&#20917;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24615;&#33021;&#35780;&#20272;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#27169;&#25311;&#28151;&#21512;&#30340;&#36752;&#23556;&#33337;&#22122;&#22768;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#36798;&#21040;&#19982;&#24050;&#30693;&#20449;&#21495;&#25968;&#37327;&#30456;&#20284;&#30340;&#20998;&#31163;&#24615;&#33021;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#24050;&#30693;&#20449;&#21495;&#25968;&#37327;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#33021;&#65292;&#20855;&#26377;&#39640;&#24230;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#24182;&#22312;&#35813;&#26694;&#26550;&#19979;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few existing studies focus on the source separation problem with unknown numbers of signals, and how to evaluate the performances of the systems is not yet clear. We propose a solution with a fixed number of output channels to address these two problems, enabling it to avoid the dimensional disaster caused by the permutation problem induced by the alignment of outputs to targets. Specifically, we propose a two-step algorithm based on autoencoders and a new performance evaluation method for situations with mute channels. Experiments conducted on simulated mixtures of radiated ship noise show that the proposed solution can achieve similar separation performance to that attained with a known number of signals. The proposed algorithm achieved competitive performance as two algorithms developed for known numbers of signals, which is highly explainable and extensible and get the state of the art under this framework.
&lt;/p&gt;</description></item><item><title>OGRIT&#26159;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#30340;&#30446;&#26631;&#35782;&#21035;&#26041;&#27861;&#65292;&#33021;&#22815;&#22788;&#29702;&#36974;&#25377;&#23548;&#33268;&#30340;&#25968;&#25454;&#32570;&#22833;&#65292;&#24182;&#19988;&#20855;&#26377;&#24555;&#36895;&#12289;&#20934;&#30830;&#12289;&#21487;&#35299;&#37322;&#21644;&#21487;&#39564;&#35777;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2206.14163</link><description>&lt;p&gt;
&#20855;&#26377;&#36974;&#25377;&#30340;&#33258;&#21160;&#39550;&#39542;&#30340;&#21487;&#39564;&#35777;&#30446;&#26631;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Verifiable Goal Recognition for Autonomous Driving with Occlusions. (arXiv:2206.14163v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.14163
&lt;/p&gt;
&lt;p&gt;
OGRIT&#26159;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#30340;&#30446;&#26631;&#35782;&#21035;&#26041;&#27861;&#65292;&#33021;&#22815;&#22788;&#29702;&#36974;&#25377;&#23548;&#33268;&#30340;&#25968;&#25454;&#32570;&#22833;&#65292;&#24182;&#19988;&#20855;&#26377;&#24555;&#36895;&#12289;&#20934;&#30830;&#12289;&#21487;&#35299;&#37322;&#21644;&#21487;&#39564;&#35777;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#35782;&#21035;&#65288;GR&#65289;&#28041;&#21450;&#25512;&#26029;&#20854;&#20182;&#36710;&#36742;&#30340;&#30446;&#26631;&#65292;&#20363;&#22914;&#29305;&#23450;&#30340;&#36335;&#21475;&#20986;&#21475;&#65292;&#36825;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#23427;&#20204;&#30340;&#26410;&#26469;&#34892;&#20026;&#12290;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#65292;&#36710;&#36742;&#21487;&#33021;&#20250;&#36935;&#21040;&#35768;&#22810;&#19981;&#21516;&#30340;&#24773;&#26223;&#65292;&#30001;&#20110;&#36974;&#25377;&#65292;&#29615;&#22659;&#21487;&#33021;&#26159;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;OGRIT&#65288;Goal Recognition with Interpretable Trees under Occlusion&#65289;&#30340;&#26032;&#22411;GR&#26041;&#27861;&#12290;OGRIT&#21033;&#29992;&#20174;&#36710;&#36742;&#36712;&#36857;&#25968;&#25454;&#23398;&#20064;&#21040;&#30340;&#20915;&#31574;&#26641;&#26469;&#25512;&#26029;&#19968;&#32452;&#29983;&#25104;&#30446;&#26631;&#30340;&#27010;&#29575;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;OGRIT&#33021;&#22815;&#22788;&#29702;&#30001;&#20110;&#36974;&#25377;&#32780;&#23548;&#33268;&#30340;&#25968;&#25454;&#32570;&#22833;&#65292;&#24182;&#19988;&#20351;&#29992;&#30456;&#21516;&#30340;&#23398;&#20064;&#20915;&#31574;&#26641;&#22312;&#22810;&#20010;&#22330;&#26223;&#20013;&#36827;&#34892;&#25512;&#29702;&#65292;&#21516;&#26102;&#20855;&#26377;&#35745;&#31639;&#36895;&#24230;&#24555;&#65292;&#20934;&#30830;&#24615;&#39640;&#65292;&#21487;&#35299;&#37322;&#24615;&#24378;&#21644;&#21487;&#39564;&#35777;&#24615;&#30340;&#29305;&#28857;&#12290;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#29992;&#20110;&#35780;&#20272;OGRIT&#30340;&#36974;&#25377;&#21306;&#22495;&#30340;inDO&#65292;rounDO&#21644;OpenDDO&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Goal recognition (GR) involves inferring the goals of other vehicles, such as a certain junction exit, which can enable more accurate prediction of their future behaviour. In autonomous driving, vehicles can encounter many different scenarios and the environment may be partially observable due to occlusions. We present a novel GR method named Goal Recognition with Interpretable Trees under Occlusion (OGRIT). OGRIT uses decision trees learned from vehicle trajectory data to infer the probabilities of a set of generated goals. We demonstrate that OGRIT can handle missing data due to occlusions and make inferences across multiple scenarios using the same learned decision trees, while being computationally fast, accurate, interpretable and verifiable. We also release the inDO, rounDO and OpenDDO datasets of occluded regions used to evaluate OGRIT.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#36827;&#34892;&#21452;&#21521;&#20114;&#21160;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20801;&#35768;&#31070;&#32463;&#32593;&#32476;&#23637;&#31034;&#20854;&#25152;&#23398;&#22240;&#26524;&#22270;&#65292;&#24182;&#20801;&#35768;&#20154;&#31867;&#20462;&#25913;&#22240;&#26524;&#22270;&#21518;&#37325;&#26032;&#27880;&#20837;&#26426;&#22120;&#20013;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#19968;&#31181;&#35843;&#35797;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#24335;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2205.09787</link><description>&lt;p&gt;
&#21487;&#20105;&#35758;&#31070;&#32463;&#32593;&#32476;&#30340;&#22240;&#26524;&#21457;&#29616;&#19982;&#30693;&#35782;&#27880;&#20837;
&lt;/p&gt;
&lt;p&gt;
Causal Discovery and Knowledge Injection for Contestable Neural Networks. (arXiv:2205.09787v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.09787
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#36827;&#34892;&#21452;&#21521;&#20114;&#21160;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20801;&#35768;&#31070;&#32463;&#32593;&#32476;&#23637;&#31034;&#20854;&#25152;&#23398;&#22240;&#26524;&#22270;&#65292;&#24182;&#20801;&#35768;&#20154;&#31867;&#20462;&#25913;&#22240;&#26524;&#22270;&#21518;&#37325;&#26032;&#27880;&#20837;&#26426;&#22120;&#20013;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#19968;&#31181;&#35843;&#35797;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#24335;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#22312;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#26159;&#21542;&#23398;&#20064;&#21040;&#20102;&#30456;&#20851;&#30340;&#22240;&#26524;&#20851;&#31995;&#23578;&#19981;&#28165;&#26970;&#65292;&#32780;&#23427;&#20204;&#30340;&#40657;&#31665;&#29305;&#24615;&#20351;&#24471;&#27169;&#22411;&#26500;&#24314;&#32773;&#38590;&#20197;&#29702;&#35299;&#21644;&#35843;&#35797;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#36890;&#36807;&#20801;&#35768;&#31070;&#32463;&#32593;&#32476;&#39537;&#21160;&#30340;&#26426;&#22120;&#23637;&#31034;&#20854;&#25152;&#23398;&#22240;&#26524;&#22270;&#65292;&#24182;&#20801;&#35768;&#20154;&#31867;&#20462;&#25913;&#22240;&#26524;&#22270;&#21518;&#37325;&#26032;&#27880;&#20837;&#26426;&#22120;&#20013;&#65292;&#23454;&#29616;&#21452;&#21521;&#20114;&#21160;&#12290;&#25152;&#23398;&#27169;&#22411;&#20445;&#35777;&#31526;&#21512;&#22240;&#26524;&#22270;&#24182;&#36981;&#24490;&#19987;&#23478;&#30693;&#35782;&#65292;&#20854;&#20013;&#37096;&#20998;&#30693;&#35782;&#20063;&#21487;&#20197;&#20107;&#20808;&#32473;&#23450;&#12290;&#36890;&#36807;&#23545;&#27169;&#22411;&#34892;&#20026;&#36827;&#34892;&#21487;&#35270;&#21270;&#24182;&#23454;&#29616;&#30693;&#35782;&#27880;&#20837;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#20174;&#25968;&#25454;&#20013;&#21457;&#29616;&#22240;&#26524;&#32467;&#26500;&#24182;&#25903;&#25745;&#39044;&#27979;&#30340;&#20174;&#19994;&#32773;&#36827;&#34892;&#35843;&#35797;&#12290;&#22312;&#30495;&#23454;&#21644;&#21512;&#25104;&#34920;&#26684;&#25968;&#25454;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25913;&#36827;&#39044;&#27979;&#24615;&#33021;&#39640;&#36798;2.4&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks have proven to be effective at solving machine learning tasks but it is unclear whether they learn any relevant causal relationships, while their black-box nature makes it difficult for modellers to understand and debug them. We propose a novel method overcoming these issues by allowing a two-way interaction whereby neural-network-empowered machines can expose the underpinning learnt causal graphs and humans can contest the machines by modifying the causal graphs before re-injecting them into the machines. The learnt models are guaranteed to conform to the graphs and adhere to expert knowledge, some of which can also be given up-front. By building a window into the model behaviour and enabling knowledge injection, our method allows practitioners to debug networks based on the causal structure discovered from the data and underpinning the predictions. Experiments with real and synthetic tabular data show that our method improves predictive performance up to 2.4x while pr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#28145;&#24230;&#25209;&#37327;&#20027;&#21160;&#23398;&#20064;&#22238;&#24402;&#30340;&#26694;&#26550;&#21644;&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#20013;&#21253;&#25324;&#35768;&#22810;&#29616;&#26377;&#30340;&#36125;&#21494;&#26031;&#21644;&#38750;&#36125;&#21494;&#26031;&#26041;&#27861;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#25442;&#24120;&#29992;&#26368;&#21518;&#19968;&#23618;&#29305;&#24449;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#32467;&#21512;&#19968;&#31181;&#26032;&#39062;&#30340;&#32858;&#31867;&#26041;&#27861;&#12290;&#22312;15&#20010;&#22823;&#22411;&#34920;&#26684;&#22238;&#24402;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#27979;&#35797;&#65292;&#35813;&#26041;&#27861;&#22312;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#36866;&#29992;&#20110;&#22823;&#22411;&#25968;&#25454;&#38598;&#19988;&#26131;&#20110;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2203.09410</link><description>&lt;p&gt;
&#28145;&#24230;&#25209;&#37327;&#20027;&#21160;&#23398;&#20064;&#22238;&#24402;&#30340;&#26694;&#26550;&#21644;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
A Framework and Benchmark for Deep Batch Active Learning for Regression. (arXiv:2203.09410v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.09410
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#28145;&#24230;&#25209;&#37327;&#20027;&#21160;&#23398;&#20064;&#22238;&#24402;&#30340;&#26694;&#26550;&#21644;&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#20013;&#21253;&#25324;&#35768;&#22810;&#29616;&#26377;&#30340;&#36125;&#21494;&#26031;&#21644;&#38750;&#36125;&#21494;&#26031;&#26041;&#27861;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#25442;&#24120;&#29992;&#26368;&#21518;&#19968;&#23618;&#29305;&#24449;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#32467;&#21512;&#19968;&#31181;&#26032;&#39062;&#30340;&#32858;&#31867;&#26041;&#27861;&#12290;&#22312;15&#20010;&#22823;&#22411;&#34920;&#26684;&#22238;&#24402;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#27979;&#35797;&#65292;&#35813;&#26041;&#27861;&#22312;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#36866;&#29992;&#20110;&#22823;&#22411;&#25968;&#25454;&#38598;&#19988;&#26131;&#20110;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#27880;&#30417;&#30563;&#23398;&#20064;&#25968;&#25454;&#30340;&#33719;&#21462;&#25104;&#26412;&#36739;&#39640;&#12290;&#20026;&#20102;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#22238;&#24402;&#30340;&#26679;&#26412;&#25928;&#29575;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#33258;&#36866;&#24212;&#36873;&#25321;&#26080;&#26631;&#31614;&#25968;&#25454;&#25209;&#27425;&#36827;&#34892;&#26631;&#27880;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#26500;&#24314;&#36825;&#26679;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;(&#32593;&#32476;&#30456;&#20851;&#30340;)&#22522;&#30784;&#26680;&#12289;&#26680;&#21464;&#25442;&#21644;&#36873;&#25321;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;&#35768;&#22810;&#29616;&#26377;&#30340;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#36924;&#36817;&#31070;&#32463;&#32593;&#32476;&#30340;&#36125;&#21494;&#26031;&#26041;&#27861;&#20197;&#21450;&#38750;&#36125;&#21494;&#26031;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24314;&#35758;&#29992;&#25551;&#32472;&#26377;&#38480;&#23485;&#24230;&#31070;&#32463;&#27491;&#20999;&#26680;&#26367;&#25442;&#24120;&#29992;&#30340;&#26368;&#21518;&#19968;&#23618;&#29305;&#24449;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#19968;&#31181;&#26032;&#39062;&#30340;&#32858;&#31867;&#26041;&#27861;&#30456;&#32467;&#21512;&#12290;&#20026;&#20102;&#35780;&#20272;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#30001;15&#20010;&#22823;&#22411;&#34920;&#26684;&#22238;&#24402;&#25968;&#25454;&#38598;&#32452;&#25104;&#30340;&#24320;&#25918;&#28304;&#20195;&#30721;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22522;&#20934;&#27979;&#35797;&#20013;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#27700;&#24179;&#65292;&#36866;&#29992;&#20110;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#24182;&#19988;&#21487;&#20197;&#30452;&#25509;&#20351;&#29992;&#65292;&#26080;&#38656;&#35843;&#25972;&#32593;&#32476;&#26550;&#26500;&#25110;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
The acquisition of labels for supervised learning can be expensive. In order to improve the sample-efficiency of neural network regression, we study active learning methods that adaptively select batches of unlabeled data for labeling. We present a framework for constructing such methods out of (network-dependent) base kernels, kernel transformations and selection methods. Our framework encompasses many existing Bayesian methods based on Gaussian Process approximations of neural networks as well as non-Bayesian methods. Additionally, we propose to replace the commonly used last-layer features with sketched finite-width Neural Tangent Kernels, and to combine them with a novel clustering method. To evaluate different methods, we introduce an open-source benchmark consisting of 15 large tabular regression data sets. Our proposed method outperforms the state-of-the-art on our benchmark, scales to large data sets, and works out-of-the-box without adjusting the network architecture or traini
&lt;/p&gt;</description></item><item><title>AgraSSt&#26159;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#38544;&#24335;&#22270;&#29983;&#25104;&#22120;&#36136;&#37327;&#30340;&#32479;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#22522;&#20110;&#26680;&#30340;&#24046;&#24322;&#24230;&#37327;&#65292;&#23427;&#33021;&#22815;&#30830;&#23450;&#23398;&#20064;&#21040;&#30340;&#22270;&#29983;&#25104;&#36807;&#31243;&#26159;&#21542;&#33021;&#29983;&#25104;&#31867;&#20284;&#32473;&#23450;&#36755;&#20837;&#22270;&#30340;&#22270;&#24418;&#65292;&#24182;&#25552;&#20379;&#26377;&#20851;&#22270;&#29983;&#25104;&#22120;&#35757;&#32451;&#36807;&#31243;&#30340;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#21644;&#21487;&#38752;&#26679;&#26412;&#25209;&#27425;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2203.03673</link><description>&lt;p&gt;
AgraSSt: &#36866;&#29992;&#20110;&#35299;&#37322;&#24615;&#35780;&#20272;&#38544;&#24335;&#22270;&#29983;&#25104;&#22120;&#30340;&#36817;&#20284;&#22270;&#26031;&#22374;&#32479;&#35745;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
AgraSSt: Approximate Graph Stein Statistics for Interpretable Assessment of Implicit Graph Generators. (arXiv:2203.03673v4 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.03673
&lt;/p&gt;
&lt;p&gt;
AgraSSt&#26159;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#38544;&#24335;&#22270;&#29983;&#25104;&#22120;&#36136;&#37327;&#30340;&#32479;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#22522;&#20110;&#26680;&#30340;&#24046;&#24322;&#24230;&#37327;&#65292;&#23427;&#33021;&#22815;&#30830;&#23450;&#23398;&#20064;&#21040;&#30340;&#22270;&#29983;&#25104;&#36807;&#31243;&#26159;&#21542;&#33021;&#29983;&#25104;&#31867;&#20284;&#32473;&#23450;&#36755;&#20837;&#22270;&#30340;&#22270;&#24418;&#65292;&#24182;&#25552;&#20379;&#26377;&#20851;&#22270;&#29983;&#25104;&#22120;&#35757;&#32451;&#36807;&#31243;&#30340;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#21644;&#21487;&#38752;&#26679;&#26412;&#25209;&#27425;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#24182;&#20998;&#26512;&#20102;&#19968;&#31181;&#26032;&#30340;&#32479;&#35745;&#26041;&#27861;&#65292;&#31216;&#20026;AgraSSt&#65292;&#29992;&#20110;&#35780;&#20272;&#21487;&#33021;&#19981;&#23384;&#22312;&#26174;&#24335;&#24418;&#24335;&#30340;&#22270;&#29983;&#25104;&#22120;&#30340;&#36136;&#37327;&#12290;&#29305;&#21035;&#22320;&#65292;AgraSSt&#21487;&#29992;&#20110;&#30830;&#23450;&#23398;&#20064;&#21040;&#30340;&#22270;&#29983;&#25104;&#36807;&#31243;&#33021;&#21542;&#29983;&#25104;&#31867;&#20284;&#32473;&#23450;&#36755;&#20837;&#22270;&#30340;&#22270;&#24418;&#12290;&#21463;&#38543;&#26426;&#22270;&#30340;&#26031;&#22374;&#25805;&#32437;&#31526;&#21551;&#21457;&#65292;AgraSSt&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#22522;&#20110;&#20174;&#22270;&#29983;&#25104;&#22120;&#33719;&#24471;&#30340;&#25805;&#20316;&#31526;&#26500;&#24314;&#22522;&#20110;&#26680;&#30340;&#24046;&#24322;&#24230;&#37327;&#12290;AgraSSt&#21487;&#20197;&#20026;&#22270;&#29983;&#25104;&#22120;&#35757;&#32451;&#36807;&#31243;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#38382;&#39064;&#65292;&#24182;&#24110;&#21161;&#35782;&#21035;&#21487;&#38752;&#30340;&#26679;&#26412;&#25209;&#27425;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#12290;&#21033;&#29992;&#26031;&#22374;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#23545;&#24191;&#27867;&#30340;&#38543;&#26426;&#22270;&#27169;&#22411;&#32473;&#20986;&#20102;&#29702;&#35770;&#20445;&#35777;&#12290;&#25105;&#20204;&#22312;&#24050;&#30693;&#22270;&#29983;&#25104;&#36807;&#31243;&#30340;&#21512;&#25104;&#36755;&#20837;&#22270;&#21644;&#26368;&#20808;&#36827;&#30340;&#65288;&#28145;&#24230;&#65289;&#22270;&#29983;&#25104;&#27169;&#22411;&#35757;&#32451;&#30340;&#30495;&#23454;&#19990;&#30028;&#36755;&#20837;&#22270;&#19978;&#25552;&#20379;&#20102;&#23454;&#35777;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose and analyse a novel statistical procedure, coined AgraSSt, to assess the quality of graph generators that may not be available in explicit form. In particular, AgraSSt can be used to determine whether a learnt graph generating process is capable of generating graphs that resemble a given input graph. Inspired by Stein operators for random graphs, the key idea of AgraSSt is the construction of a kernel discrepancy based on an operator obtained from the graph generator. AgraSSt can provide interpretable criticisms for a graph generator training procedure and help identify reliable sample batches for downstream tasks. Using Stein`s method we give theoretical guarantees for a broad class of random graph models. We provide empirical results on both synthetic input graphs with known graph generation procedures, and real-world input graphs that the state-of-the-art (deep) generative models for graphs are trained on.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#32447;&#24615;&#22238;&#24402;&#24773;&#20917;&#30340;&#36801;&#31227;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#23558;&#26032;&#25968;&#25454;&#19982;&#21382;&#21490;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#29305;&#21035;&#22312;&#26032;&#25968;&#25454;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#30410;&#22788;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#39564;&#35777;&#20013;&#34920;&#29616;&#20986;&#23545;&#36127;&#36801;&#31227;&#23398;&#20064;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2202.05069</link><description>&lt;p&gt;
&#19981;&#21516;&#36755;&#20837;&#32500;&#24230;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#36801;&#31227;&#23398;&#20064;&#65306;&#32447;&#24615;&#22238;&#24402;&#24773;&#20917;&#19979;&#30340;&#31639;&#27861;&#21644;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Transfer-Learning Across Datasets with Different Input Dimensions: An Algorithm and Analysis for the Linear Regression Case. (arXiv:2202.05069v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.05069
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#32447;&#24615;&#22238;&#24402;&#24773;&#20917;&#30340;&#36801;&#31227;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#23558;&#26032;&#25968;&#25454;&#19982;&#21382;&#21490;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#29305;&#21035;&#22312;&#26032;&#25968;&#25454;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#30410;&#22788;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#39564;&#35777;&#20013;&#34920;&#29616;&#20986;&#23545;&#36127;&#36801;&#31227;&#23398;&#20064;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26032;&#20256;&#24863;&#22120;&#21644;&#30417;&#27979;&#35774;&#22791;&#30340;&#21457;&#23637;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#25968;&#25454;&#28304;&#21487;&#20197;&#20316;&#20026;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36755;&#20837;&#12290;&#36825;&#20123;&#25968;&#25454;&#26082;&#21487;&#20197;&#24110;&#21161;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#23558;&#36825;&#20123;&#26032;&#36755;&#20837;&#19982;&#21382;&#21490;&#25968;&#25454;&#30456;&#32467;&#21512;&#20173;&#28982;&#26159;&#19968;&#20010;&#23578;&#26410;&#35814;&#32454;&#30740;&#31350;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36801;&#31227;&#23398;&#20064;&#31639;&#27861;&#65292;&#23558;&#26032;&#25968;&#25454;&#21644;&#21382;&#21490;&#25968;&#25454;&#32467;&#21512;&#36215;&#26469;&#65292;&#29305;&#21035;&#22312;&#26032;&#25968;&#25454;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#30410;&#22788;&#12290;&#25105;&#20204;&#23558;&#37325;&#28857;&#25918;&#22312;&#32447;&#24615;&#22238;&#24402;&#24773;&#20917;&#19979;&#65292;&#36825;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#23545;&#35813;&#26041;&#27861;&#30340;&#30410;&#22788;&#36827;&#34892;&#20005;&#26684;&#30340;&#29702;&#35770;&#30740;&#31350;&#12290;&#25105;&#20204;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#36127;&#36801;&#31227;&#23398;&#20064;&#26159;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#65292;&#24182;&#36890;&#36807;&#30495;&#23454;&#21644;&#27169;&#25311;&#25968;&#25454;&#36827;&#34892;&#20102;&#23454;&#35777;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the development of new sensors and monitoring devices, more sources of data become available to be used as inputs for machine learning models. These can on the one hand help to improve the accuracy of a model. On the other hand however, combining these new inputs with historical data remains a challenge that has not yet been studied in enough detail. In this work, we propose a transfer-learning algorithm that combines the new and the historical data, that is especially beneficial when the new data is scarce. We focus the approach on the linear regression case, which allows us to conduct a rigorous theoretical study on the benefits of the approach. We show that our approach is robust against negative transfer-learning, and we confirm this result empirically with real and simulated data.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#22810;&#36755;&#20986;&#39640;&#26031;&#36807;&#31243;&#30340;&#19977;&#31181;&#25509;&#25910;&#20449;&#21495;&#24378;&#24230;&#25351;&#32441;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#23460;&#20869;&#23450;&#20301;&#20013;&#25968;&#25454;&#25910;&#38598;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2202.01980</link><description>&lt;p&gt;
&#22810;&#36755;&#20986;&#39640;&#26031;&#36807;&#31243;&#25968;&#25454;&#22686;&#24378;&#29992;&#20110;&#22810;&#24314;&#31569;&#21644;&#22810;&#27004;&#23618;&#23460;&#20869;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Multi-Output Gaussian Process-Based Data Augmentation for Multi-Building and Multi-Floor Indoor Localization. (arXiv:2202.01980v2 [cs.NI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.01980
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#22810;&#36755;&#20986;&#39640;&#26031;&#36807;&#31243;&#30340;&#19977;&#31181;&#25509;&#25910;&#20449;&#21495;&#24378;&#24230;&#25351;&#32441;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#23460;&#20869;&#23450;&#20301;&#20013;&#25968;&#25454;&#25910;&#38598;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25509;&#25910;&#20449;&#21495;&#24378;&#24230;&#25351;&#32441;&#30340;&#23450;&#20301;&#25104;&#20026;&#23460;&#20869;&#23450;&#20301;&#30340;&#20027;&#27969;&#25216;&#26415;&#65292;&#22240;&#20026;&#23427;&#26080;&#38656;&#23433;&#35013;&#26032;&#30340;&#22522;&#30784;&#35774;&#26045;&#21644;&#20462;&#25913;&#29616;&#26377;&#35774;&#22791;&#65292;&#23588;&#20854;&#26159;&#32771;&#34385;&#21040;&#29616;&#20195;&#24314;&#31569;&#20013;Wi-Fi&#35774;&#22791;&#30340;&#26222;&#21450;&#21644;&#26080;&#22788;&#19981;&#22312;&#30340;Wi-Fi&#25509;&#20837;&#12290;&#20351;&#29992;AI/ML&#25216;&#26415;&#22914;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#21487;&#20197;&#20351;&#23450;&#20301;&#25351;&#32441;&#26356;&#31934;&#30830;&#12289;&#21487;&#38752;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#30340;&#22810;&#24314;&#31569;&#21644;&#22810;&#27004;&#23618;&#23460;&#20869;&#23450;&#20301;&#12290;&#28982;&#32780;&#65292;&#23460;&#20869;&#23450;&#20301;&#30340;DNNs&#24212;&#29992;&#20381;&#36182;&#20110;&#22823;&#37327;&#32463;&#36807;&#39044;&#22788;&#29702;&#21644;&#31934;&#24515;&#26631;&#35760;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#32771;&#34385;&#21040;&#22312;&#23460;&#20869;&#29615;&#22659;&#20013;&#30340;&#25968;&#25454;&#25910;&#38598;&#30340;&#22256;&#38590;&#65292;&#23588;&#20854;&#26159;&#22312;&#24403;&#21069;COVID-19&#27969;&#34892;&#30149;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#22810;&#36755;&#20986;&#39640;&#26031;&#36807;&#31243;&#65288;MOGP&#65289;&#30340;&#19977;&#31181;&#19981;&#21516;&#30340;&#25509;&#25910;&#20449;&#21495;&#24378;&#24230;&#25351;&#32441;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#21333;&#23618;&#27004;&#12289;&#37051;&#36817;&#27004;&#23618;&#21644;&#21333;&#20010;&#24314;&#31569;&#65307;&#19981;&#21516;&#20110;&#21333;&#36755;&#20986;&#39640;&#26031;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Location fingerprinting based on RSSI becomes a mainstream indoor localization technique due to its advantage of not requiring the installation of new infrastructure and the modification of existing devices, especially given the prevalence of Wi-Fi-enabled devices and the ubiquitous Wi-Fi access in modern buildings. The use of AI/ML technologies like DNNs makes location fingerprinting more accurate and reliable, especially for large-scale multi-building and multi-floor indoor localization. The application of DNNs for indoor localization, however, depends on a large amount of preprocessed and deliberately-labeled data for their training. Considering the difficulty of the data collection in an indoor environment, especially under the current epidemic situation of COVID-19, we investigate three different methods of RSSI data augmentation based on Multi-Output Gaussian Process (MOGP), i.e., by a single floor, by neighboring floors, and by a single building; unlike Single-Output Gaussian Pr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#33258;&#21160;&#35782;&#21035;Pauli&#33258;&#26059;&#38459;&#30861;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#31639;&#27861;&#20351;&#29992;&#27169;&#25311;&#25968;&#25454;&#21644;&#36328;&#35774;&#22791;&#39564;&#35777;&#65292;&#20811;&#26381;&#20102;PSB&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#65292;&#24182;&#22312;&#30789;&#22330;&#25928;&#24212;&#26230;&#20307;&#31649;&#22120;&#20214;&#19978;&#21462;&#24471;&#20102;96&#65285;&#30340;&#20934;&#30830;&#24615;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#39044;&#35745;&#21487;&#22312;&#25152;&#26377;&#31867;&#22411;&#30340;&#37327;&#23376;&#28857;&#22120;&#20214;&#19978;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2202.00574</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#35782;&#21035;Pauli&#33258;&#26059;&#38459;&#30861;
&lt;/p&gt;
&lt;p&gt;
Identifying Pauli spin blockade using deep learning. (arXiv:2202.00574v3 [cond-mat.mes-hall] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.00574
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#33258;&#21160;&#35782;&#21035;Pauli&#33258;&#26059;&#38459;&#30861;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#31639;&#27861;&#20351;&#29992;&#27169;&#25311;&#25968;&#25454;&#21644;&#36328;&#35774;&#22791;&#39564;&#35777;&#65292;&#20811;&#26381;&#20102;PSB&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#65292;&#24182;&#22312;&#30789;&#22330;&#25928;&#24212;&#26230;&#20307;&#31649;&#22120;&#20214;&#19978;&#21462;&#24471;&#20102;96&#65285;&#30340;&#20934;&#30830;&#24615;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#39044;&#35745;&#21487;&#22312;&#25152;&#26377;&#31867;&#22411;&#30340;&#37327;&#23376;&#28857;&#22120;&#20214;&#19978;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Pauli&#33258;&#26059;&#38459;&#30861;&#65288;PSB&#65289;&#21487;&#29992;&#20110;&#33258;&#26059;&#37327;&#23376;&#20301;&#30340;&#21021;&#22987;&#21270;&#21644;&#35835;&#21462;&#65292;&#21363;&#20351;&#22312;&#39640;&#28201;&#19979;&#20063;&#33021;&#21457;&#25381;&#20316;&#29992;&#65292;&#20294;&#24456;&#38590;&#35782;&#21035;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#30005;&#33655;&#20256;&#36755;&#27979;&#37327;&#33258;&#21160;&#35782;&#21035;PSB&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;&#36890;&#36807;&#35757;&#32451;&#31639;&#27861;&#20351;&#29992;&#27169;&#25311;&#25968;&#25454;&#21644;&#36328;&#35774;&#22791;&#39564;&#35777;&#65292;&#20811;&#26381;&#20102;PSB&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#12290;&#25105;&#20204;&#22312;&#30789;&#22330;&#25928;&#24212;&#26230;&#20307;&#31649;&#22120;&#20214;&#19978;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#27979;&#35797;&#22120;&#20214;&#19978;&#25253;&#21578;&#20102;96&#65285;&#30340;&#20934;&#30830;&#24615;&#65292;&#36825;&#34920;&#26126;&#35813;&#26041;&#27861;&#23545;&#22120;&#20214;&#21464;&#24322;&#24615;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#39044;&#35745;&#35813;&#26041;&#27861;&#21487;&#22312;&#25152;&#26377;&#31867;&#22411;&#30340;&#37327;&#23376;&#28857;&#22120;&#20214;&#19978;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pauli spin blockade (PSB) can be employed as a great resource for spin qubit initialisation and readout even at elevated temperatures but it can be difficult to identify. We present a machine learning algorithm capable of automatically identifying PSB using charge transport measurements. The scarcity of PSB data is circumvented by training the algorithm with simulated data and by using cross-device validation. We demonstrate our approach on a silicon field-effect transistor device and report an accuracy of 96% on different test devices, giving evidence that the approach is robust to device variability. The approach is expected to be employable across all types of quantum dot devices.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#35780;&#20272;&#26631;&#20934;&#65292;&#38024;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#26410;&#30693;&#26679;&#26412;&#30340;&#26816;&#27979;&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#32771;&#23519;&#12290;&#36890;&#36807;&#26500;&#24314;&#32479;&#19968;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#27604;&#36739;&#29616;&#26377;&#26041;&#27861;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;Deep Ensemble&#26041;&#27861;&#20855;&#26377;&#36739;&#39640;&#30340;&#26410;&#30693;&#26816;&#27979;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2112.00337</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26410;&#30693;&#26816;&#27979;&#33021;&#21147;&#30340;&#32479;&#19968;&#35780;&#20272;&#26631;&#20934;
&lt;/p&gt;
&lt;p&gt;
A Unified Benchmark for the Unknown Detection Capability of Deep Neural Networks. (arXiv:2112.00337v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.00337
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#35780;&#20272;&#26631;&#20934;&#65292;&#38024;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#26410;&#30693;&#26679;&#26412;&#30340;&#26816;&#27979;&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#32771;&#23519;&#12290;&#36890;&#36807;&#26500;&#24314;&#32479;&#19968;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#27604;&#36739;&#29616;&#26377;&#26041;&#27861;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;Deep Ensemble&#26041;&#27861;&#20855;&#26377;&#36739;&#39640;&#30340;&#26410;&#30693;&#26816;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#20294;&#23384;&#22312;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#23545;&#20110;&#23436;&#20840;&#26410;&#30693;&#30340;&#26679;&#26412;&#39044;&#27979;&#36807;&#20110;&#33258;&#20449;&#12290;&#35768;&#22810;&#30740;&#31350;&#24050;&#32463;&#25552;&#20986;&#25104;&#21151;&#36807;&#28388;&#36825;&#20123;&#26410;&#30693;&#26679;&#26412;&#30340;&#26041;&#27861;&#65292;&#20294;&#21482;&#32771;&#34385;&#20102;&#29421;&#31364;&#32780;&#20855;&#20307;&#30340;&#20219;&#21153;&#65292;&#22914;&#35823;&#20998;&#31867;&#26816;&#27979;&#12289;&#24320;&#38598;&#35782;&#21035;&#25110;&#20998;&#24067;&#22806;&#26816;&#27979;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#20123;&#20219;&#21153;&#24212;&#34987;&#35270;&#20026;&#26681;&#26412;&#30456;&#21516;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#29702;&#24819;&#30340;&#27169;&#22411;&#24212;&#20855;&#22791;&#23545;&#25152;&#26377;&#36825;&#20123;&#20219;&#21153;&#30340;&#26816;&#27979;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26410;&#30693;&#26816;&#27979;&#20219;&#21153;&#65292;&#23558;&#20043;&#21069;&#30340;&#21333;&#20010;&#20219;&#21153;&#36827;&#34892;&#25972;&#21512;&#65292;&#20197;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#24191;&#27867;&#26410;&#30693;&#26679;&#26412;&#19978;&#30340;&#26816;&#27979;&#33021;&#21147;&#36827;&#34892;&#20005;&#26684;&#35780;&#20272;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19981;&#21516;&#35268;&#27169;&#30340;&#32479;&#19968;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#23545;&#29616;&#26377;&#27969;&#34892;&#26041;&#27861;&#30340;&#26410;&#30693;&#26816;&#27979;&#33021;&#21147;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks have achieved outstanding performance over various tasks, but they have a critical issue: over-confident predictions even for completely unknown samples. Many studies have been proposed to successfully filter out these unknown samples, but they only considered narrow and specific tasks, referred to as misclassification detection, open-set recognition, or out-of-distribution detection. In this work, we argue that these tasks should be treated as fundamentally an identical problem because an ideal model should possess detection capability for all those tasks. Therefore, we introduce the unknown detection task, an integration of previous individual tasks, for a rigorous examination of the detection capability of deep neural networks on a wide spectrum of unknown samples. To this end, unified benchmark datasets on different scales were constructed and the unknown detection capabilities of existing popular methods were subject to comparison. We found that Deep Ensemble 
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20998;&#26512;&#22810;&#20803;&#26497;&#20540;&#30340;&#35889;&#32858;&#31867;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#21644;&#25968;&#20540;&#23454;&#39564;&#23637;&#31034;&#20102;&#20854;&#22312;&#23398;&#20064;&#35282;&#24230;&#27979;&#24230;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2111.07799</link><description>&lt;p&gt;
&#22810;&#20803;&#26497;&#20540;&#30340;&#35889;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Spectral learning of multivariate extremes. (arXiv:2111.07799v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.07799
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20998;&#26512;&#22810;&#20803;&#26497;&#20540;&#30340;&#35889;&#32858;&#31867;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#21644;&#25968;&#20540;&#23454;&#39564;&#23637;&#31034;&#20102;&#20854;&#22312;&#23398;&#20064;&#35282;&#24230;&#27979;&#24230;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20998;&#26512;&#22810;&#20803;&#26497;&#20540;&#30340;&#35889;&#32858;&#31867;&#31639;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20851;&#27880;&#26497;&#20540;&#29702;&#35770;&#20013;&#30001;&#35282;&#24230;&#25110;&#35889;&#27979;&#24230;&#34920;&#24449;&#30340;&#22810;&#20803;&#26497;&#20540;&#30340;&#28176;&#36817;&#20381;&#36182;&#24615;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#30740;&#31350;&#20102;&#35889;&#32858;&#31867;&#30340;&#29702;&#35770;&#24615;&#33021;&#65292;&#35813;&#32858;&#31867;&#22522;&#20110;&#20174;&#26497;&#20540;&#26679;&#26412;&#20013;&#26500;&#24314;&#30340;&#38543;&#26426;k&#26368;&#36817;&#37051;&#22270;&#65292;&#21363;&#23545;&#20110;&#21322;&#24452;&#36229;&#36807;&#19968;&#20010;&#36739;&#22823;&#38408;&#20540;&#30340;&#38543;&#26426;&#21521;&#37327;&#30340;&#35282;&#24230;&#37096;&#20998;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#32447;&#24615;&#22240;&#23376;&#27169;&#22411;&#20135;&#29983;&#30340;&#26497;&#20540;&#30340;&#28176;&#36817;&#20998;&#24067;&#65292;&#24182;&#35777;&#26126;&#65292;&#22312;&#26576;&#20123;&#26465;&#20214;&#19979;&#65292;&#35889;&#32858;&#31867;&#21487;&#20197;&#19968;&#33268;&#22320;&#35782;&#21035;&#20986;&#22312;&#35813;&#27169;&#22411;&#20013;&#20135;&#29983;&#30340;&#26497;&#20540;&#30340;&#32858;&#31867;&#12290;&#22522;&#20110;&#36825;&#20010;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#19968;&#33268;&#24615;&#20272;&#35745;&#31574;&#30053;&#26469;&#23398;&#20064;&#35282;&#24230;&#27979;&#24230;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#19982;&#25968;&#20540;&#23454;&#39564;&#30456;&#32467;&#21512;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#22312;&#26377;&#38480;&#26679;&#26412;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a spectral clustering algorithm for analyzing the dependence structure of multivariate extremes. More specifically, we focus on the asymptotic dependence of multivariate extremes characterized by the angular or spectral measure in extreme value theory. Our work studies the theoretical performance of spectral clustering based on a random $k$-nearest neighbor graph constructed from an extremal sample, i.e., the angular part of random vectors for which the radius exceeds a large threshold. In particular, we derive the asymptotic distribution of extremes arising from a linear factor model and prove that, under certain conditions, spectral clustering can consistently identify the clusters of extremes arising in this model. Leveraging this result we propose a simple consistent estimation strategy for learning the angular measure. Our theoretical findings are complemented with numerical experiments illustrating the finite sample performance of our methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20171;&#32461;&#20102;&#36229;&#32500;&#35745;&#31639;&#21644;&#30690;&#37327;&#31526;&#21495;&#21270;&#26550;&#26500;&#65288;HDC/VSA&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#39640;&#32500;&#20998;&#24067;&#24335;&#34920;&#31034;&#21644;&#20195;&#25968;&#23646;&#24615;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#34701;&#21512;&#20102;&#32467;&#26500;&#21270;&#31526;&#21495;&#34920;&#31034;&#21644;&#21521;&#37327;&#20998;&#24067;&#24335;&#34920;&#31034;&#30340;&#20248;&#21183;&#12290;&#35813;&#39046;&#22495;&#28041;&#21450;&#22810;&#20010;&#23398;&#31185;&#65292;&#24182;&#20171;&#32461;&#20102;&#22810;&#20010;&#30456;&#20851;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2111.06077</link><description>&lt;p&gt;
&#12298;&#36229;&#32500;&#35745;&#31639;&#32508;&#36848;&#21450;&#30690;&#37327;&#31526;&#21495;&#21270;&#26550;&#26500;&#12299;&#31532;&#19968;&#37096;&#20998;&#65306;&#27169;&#22411;&#21644;&#25968;&#25454;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
A Survey on Hyperdimensional Computing aka Vector Symbolic Architectures, Part I: Models and Data Transformations. (arXiv:2111.06077v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.06077
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20171;&#32461;&#20102;&#36229;&#32500;&#35745;&#31639;&#21644;&#30690;&#37327;&#31526;&#21495;&#21270;&#26550;&#26500;&#65288;HDC/VSA&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#39640;&#32500;&#20998;&#24067;&#24335;&#34920;&#31034;&#21644;&#20195;&#25968;&#23646;&#24615;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#34701;&#21512;&#20102;&#32467;&#26500;&#21270;&#31526;&#21495;&#34920;&#31034;&#21644;&#21521;&#37327;&#20998;&#24067;&#24335;&#34920;&#31034;&#30340;&#20248;&#21183;&#12290;&#35813;&#39046;&#22495;&#28041;&#21450;&#22810;&#20010;&#23398;&#31185;&#65292;&#24182;&#20171;&#32461;&#20102;&#22810;&#20010;&#30456;&#20851;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#20849;&#20998;&#20004;&#37096;&#20998;&#65292;&#33268;&#21147;&#20110;&#20171;&#32461;&#19968;&#20010;&#34987;&#31216;&#20026;&#36229;&#32500;&#35745;&#31639;&#21644;&#30690;&#37327;&#31526;&#21495;&#21270;&#26550;&#26500;&#65288;HDC/VSA&#65289;&#30340;&#35745;&#31639;&#26694;&#26550;&#12290;HDC/VSA&#26159;&#19968;&#31867;&#20351;&#29992;&#39640;&#32500;&#20998;&#24067;&#24335;&#34920;&#31034;&#21644;&#20381;&#36182;&#20854;&#20851;&#38190;&#25805;&#20316;&#30340;&#20195;&#25968;&#23646;&#24615;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#20197;&#34701;&#21512;&#32467;&#26500;&#21270;&#31526;&#21495;&#34920;&#31034;&#21644;&#21521;&#37327;&#20998;&#24067;&#24335;&#34920;&#31034;&#30340;&#20248;&#21183;&#12290;HDC/VSA&#23478;&#26063;&#20013;&#30340;&#26174;&#33879;&#27169;&#22411;&#21253;&#25324;&#24352;&#37327;&#31215;&#34920;&#31034;&#12289;&#20840;&#24687;&#20943;&#23569;&#34920;&#31034;&#12289;&#20056;&#21152;&#32622;&#25442;&#12289;&#20108;&#20803;&#25955;&#23556;&#30721;&#21644;&#31232;&#30095;&#20108;&#20803;&#20998;&#24067;&#24335;&#34920;&#31034;&#65292;&#36824;&#26377;&#20854;&#20182;&#27169;&#22411;&#12290;HDC/VSA&#26159;&#19968;&#20010;&#39640;&#24230;&#36328;&#23398;&#31185;&#30340;&#39046;&#22495;&#65292;&#28041;&#21450;&#35745;&#31639;&#26426;&#31185;&#23398;&#12289;&#30005;&#23376;&#24037;&#31243;&#12289;&#20154;&#24037;&#26234;&#33021;&#12289;&#25968;&#23398;&#21644;&#35748;&#30693;&#31185;&#23398;&#12290;&#36825;&#19968;&#20107;&#23454;&#20351;&#24471;&#23545;&#35813;&#39046;&#22495;&#36827;&#34892;&#20840;&#38754;&#27010;&#36848;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This two-part comprehensive survey is devoted to a computing framework most commonly known under the names Hyperdimensional Computing and Vector Symbolic Architectures (HDC/VSA). Both names refer to a family of computational models that use high-dimensional distributed representations and rely on the algebraic properties of their key operations to incorporate the advantages of structured symbolic representations and vector distributed representations. Notable models in the HDC/VSA family are Tensor Product Representations, Holographic Reduced Representations, Multiply-Add-Permute, Binary Spatter Codes, and Sparse Binary Distributed Representations but there are other models too. HDC/VSA is a highly interdisciplinary field with connections to computer science, electrical engineering, artificial intelligence, mathematics, and cognitive science. This fact makes it challenging to create a thorough overview of the field. However, due to a surge of new researchers joining the field in recent
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#26465;&#20214;&#20272;&#35745;-&#20248;&#21270;&#65288;ICEO&#65289;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#32771;&#34385;&#20248;&#21270;&#38382;&#39064;&#32467;&#26500;&#30340;&#21516;&#26102;&#20272;&#35745;&#38543;&#26426;&#21442;&#25968;&#30340;&#26465;&#20214;&#20998;&#24067;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20123;&#24615;&#33021;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2110.12351</link><description>&lt;p&gt;
&#32508;&#21512;&#26465;&#20214;&#20272;&#35745;-&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Integrated Conditional Estimation-Optimization. (arXiv:2110.12351v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.12351
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#26465;&#20214;&#20272;&#35745;-&#20248;&#21270;&#65288;ICEO&#65289;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#32771;&#34385;&#20248;&#21270;&#38382;&#39064;&#32467;&#26500;&#30340;&#21516;&#26102;&#20272;&#35745;&#38543;&#26426;&#21442;&#25968;&#30340;&#26465;&#20214;&#20998;&#24067;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20123;&#24615;&#33021;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#23454;&#38469;&#20248;&#21270;&#38382;&#39064;&#28041;&#21450;&#20855;&#26377;&#27010;&#29575;&#20998;&#24067;&#30340;&#19981;&#30830;&#23450;&#21442;&#25968;&#65292;&#21487;&#20197;&#20351;&#29992;&#19978;&#19979;&#25991;&#29305;&#24449;&#20449;&#24687;&#36827;&#34892;&#20272;&#35745;&#12290;&#19982;&#20808;&#20272;&#35745;&#19981;&#30830;&#23450;&#21442;&#25968;&#30340;&#20998;&#24067;&#28982;&#21518;&#22522;&#20110;&#20272;&#35745;&#20248;&#21270;&#30446;&#26631;&#30340;&#26631;&#20934;&#26041;&#27861;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#26465;&#20214;&#20272;&#35745;-&#20248;&#21270;&#65288;ICEO&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22312;&#32771;&#34385;&#20248;&#21270;&#38382;&#39064;&#32467;&#26500;&#30340;&#21516;&#26102;&#20272;&#35745;&#38543;&#26426;&#21442;&#25968;&#30340;&#26465;&#20214;&#20998;&#24067;&#12290;&#25105;&#20204;&#30452;&#25509;&#24314;&#27169;&#38543;&#26426;&#21442;&#25968;&#30340;&#26465;&#20214;&#20998;&#24067;&#19982;&#19978;&#19979;&#25991;&#29305;&#24449;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#28982;&#21518;&#29992;&#19982;&#19979;&#28216;&#20248;&#21270;&#38382;&#39064;&#19968;&#33268;&#30340;&#30446;&#26631;&#20272;&#35745;&#27010;&#29575;&#27169;&#22411;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;ICEO&#26041;&#27861;&#22312;&#36866;&#24230;&#35268;&#21017;&#26465;&#20214;&#19979;&#26159;&#28176;&#36827;&#19968;&#33268;&#30340;&#65292;&#24182;&#36827;&#19968;&#27493;&#25552;&#20379;&#20102;&#19968;&#20123;&#25512;&#24191;&#30028;&#38480;&#24418;&#24335;&#30340;&#26377;&#38480;&#24615;&#33021;&#20445;&#35777;&#12290;&#35745;&#31639;&#19978;&#65292;&#20351;&#29992;
&lt;/p&gt;
&lt;p&gt;
Many real-world optimization problems involve uncertain parameters with probability distributions that can be estimated using contextual feature information. In contrast to the standard approach of first estimating the distribution of uncertain parameters and then optimizing the objective based on the estimation, we propose an integrated conditional estimation-optimization (ICEO) framework that estimates the underlying conditional distribution of the random parameter while considering the structure of the optimization problem. We directly model the relationship between the conditional distribution of the random parameter and the contextual features, and then estimate the probabilistic model with an objective that aligns with the downstream optimization problem. We show that our ICEO approach is asymptotically consistent under moderate regularity conditions and further provide finite performance guarantees in the form of generalization bounds. Computationally, performing estimation with
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20851;&#27880;&#22522;&#20110;&#25209;&#20934;&#30340;&#22810;&#36194;&#32773;&#25237;&#31080;&#35268;&#21017;&#30340;&#23398;&#20064;&#38382;&#39064;&#65292;&#25105;&#20204;&#21457;&#29616;&#23613;&#31649;&#23384;&#22312;&#25351;&#25968;&#32423;&#30340;&#21487;&#33021;&#32467;&#26524;&#65292;&#20294;&#36890;&#36807;&#22810;&#39033;&#24335;&#25968;&#37327;&#30340;&#26679;&#26412;&#21487;&#20197;&#39640;&#32622;&#20449;&#24230;&#21644;&#20934;&#30830;&#24615;&#22320;&#23398;&#20064;&#30446;&#26631;&#35268;&#21017;&#12290;</title><link>http://arxiv.org/abs/2110.00254</link><description>&lt;p&gt;
&#23398;&#20064;&#22522;&#20110;&#25209;&#20934;&#30340;&#22810;&#36194;&#32773;&#25237;&#31080;&#35268;&#21017;&#30340;&#22797;&#26434;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Complexity of Learning Approval-Based Multiwinner Voting Rules. (arXiv:2110.00254v3 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.00254
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20851;&#27880;&#22522;&#20110;&#25209;&#20934;&#30340;&#22810;&#36194;&#32773;&#25237;&#31080;&#35268;&#21017;&#30340;&#23398;&#20064;&#38382;&#39064;&#65292;&#25105;&#20204;&#21457;&#29616;&#23613;&#31649;&#23384;&#22312;&#25351;&#25968;&#32423;&#30340;&#21487;&#33021;&#32467;&#26524;&#65292;&#20294;&#36890;&#36807;&#22810;&#39033;&#24335;&#25968;&#37327;&#30340;&#26679;&#26412;&#21487;&#20197;&#39640;&#32622;&#20449;&#24230;&#21644;&#20934;&#30830;&#24615;&#22320;&#23398;&#20064;&#30446;&#26631;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#36194;&#32773;&#25237;&#31080;&#30340;PAC&#21487;&#23398;&#20064;&#24615;&#65292;&#37325;&#28857;&#20851;&#27880;&#22522;&#20110;&#25209;&#20934;&#30340;&#22996;&#21592;&#20250;&#35780;&#20998;&#65288;ABCS&#65289;&#35268;&#21017;&#12290;&#36825;&#20123;&#35268;&#21017;&#36866;&#29992;&#20110;&#20855;&#26377;&#25209;&#20934;&#31080;&#30340;&#36873;&#27665;&#26723;&#26696;&#65292;&#27599;&#20010;&#36873;&#27665;&#25209;&#20934;&#19968;&#37096;&#20998;&#20505;&#36873;&#20154;&#12290;&#26681;&#25454;ABCS&#35268;&#21017;&#65292;&#27599;&#20010;$k$&#20505;&#36873;&#20154;&#22996;&#21592;&#20250;&#20174;&#27599;&#20010;&#36873;&#27665;&#37027;&#37324;&#25910;&#38598;&#24471;&#20998;&#65292;&#35813;&#24471;&#20998;&#21462;&#20915;&#20110;&#36873;&#27665;&#31080;&#25968;&#21644;&#20854;&#19982;&#22996;&#21592;&#20250;&#30340;&#20132;&#38598;&#30340;&#22823;&#23567;&#12290;&#28982;&#21518;&#65292;&#24471;&#20998;&#26368;&#39640;&#30340;&#22996;&#21592;&#20250;&#33719;&#32988;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#23398;&#20064;&#30446;&#26631;&#35268;&#21017;&#65288;&#21363;&#23398;&#20064;&#30456;&#24212;&#30340;&#35780;&#20998;&#20989;&#25968;&#65289;&#65292;&#24182;&#21033;&#29992;&#23569;&#37327;&#37319;&#26679;&#26723;&#26696;&#20013;&#33719;&#32988;&#30340;&#22996;&#21592;&#20250;&#30340;&#20449;&#24687;&#12290;&#23613;&#31649;&#19982;&#21333;&#36194;&#32773;&#36873;&#20030;&#30456;&#27604;&#23384;&#22312;&#25351;&#25968;&#32423;&#30340;&#32467;&#26524;&#65292;&#20294;&#25105;&#20204;&#34920;&#26126;&#26679;&#26412;&#22797;&#26434;&#24615;&#20173;&#28982;&#36739;&#20302;&#65306;&#22810;&#39033;&#24335;&#25968;&#37327;&#30340;&#26679;&#26412;&#36275;&#22815;&#25552;&#20379;&#36275;&#22815;&#30340;&#20449;&#24687;&#65292;&#20197;&#39640;&#32622;&#20449;&#24230;&#21644;&#20934;&#30830;&#24615;&#23398;&#20064;&#30446;&#26631;&#35268;&#21017;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#21363;&#20351;&#23545;&#20110;&#23398;&#20064;&#20219;&#21153;&#26469;&#35828;&#20063;&#24456;&#31616;&#21333;&#65292;&#38656;&#35201;&#35299;&#20915;&#30340;&#20219;&#21153;&#20063;&#24456;&#31616;&#21333;&#65292;&#20026;&#20102;&#23398;&#20064;&#65292;&#25105;&#20204;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#26679;&#26412;&#37319;&#26679;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the {PAC} learnability of multiwinner voting, focusing on the class of approval-based committee scoring (ABCS) rules. These are voting rules applied on profiles with approval ballots, where each voter approves some of the candidates. According to ABCS rules, each committee of $k$ candidates collects from each voter a score, which depends on the size of the voter's ballot and on the size of its intersection with the committee. Then, committees of maximum score are the winning ones. Our goal is to learn a target rule (i.e., to learn the corresponding scoring function) using information about the winning committees of a small number of sampled profiles. Despite the existence of exponentially many outcomes compared to single-winner elections, we show that the sample complexity is still low: a polynomial number of samples carries enough information for learning the target rule with high confidence and accuracy. Unfortunately, even simple tasks that need to be solved for learning fr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#21160;&#24577;&#38598;&#20307;&#26234;&#33021;&#23398;&#20064;&#65288;DCIL&#65289;&#26041;&#27861;&#65292;&#21033;&#29992;&#31934;&#28860;&#30340;&#26799;&#24230;&#26356;&#26032;&#21098;&#26525;&#26435;&#37325;&#65292;&#36890;&#36807;&#24418;&#25104;&#21452;&#21521;&#36716;&#21457;&#36335;&#24452;&#26469;&#23547;&#25214;&#39640;&#25928;&#31232;&#30095;&#27169;&#22411;&#12290;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#20102;&#21098;&#26525;&#21644;&#26410;&#21098;&#26525;&#26435;&#37325;&#30340;&#38598;&#20307;&#26234;&#33021;&#20043;&#38388;&#30340;&#23398;&#20064;&#21327;&#21516;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2109.04660</link><description>&lt;p&gt;
&#21160;&#24577;&#38598;&#20307;&#26234;&#33021;&#23398;&#20064;&#65306;&#36890;&#36807;&#31934;&#28860;&#30340;&#26799;&#24230;&#25214;&#21040;&#39640;&#25928;&#31232;&#30095;&#27169;&#22411;&#20197;&#21098;&#26525;&#26435;&#37325;
&lt;/p&gt;
&lt;p&gt;
Dynamic Collective Intelligence Learning: Finding Efficient Sparse Model via Refined Gradients for Pruned Weights. (arXiv:2109.04660v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.04660
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#21160;&#24577;&#38598;&#20307;&#26234;&#33021;&#23398;&#20064;&#65288;DCIL&#65289;&#26041;&#27861;&#65292;&#21033;&#29992;&#31934;&#28860;&#30340;&#26799;&#24230;&#26356;&#26032;&#21098;&#26525;&#26435;&#37325;&#65292;&#36890;&#36807;&#24418;&#25104;&#21452;&#21521;&#36716;&#21457;&#36335;&#24452;&#26469;&#23547;&#25214;&#39640;&#25928;&#31232;&#30095;&#27169;&#22411;&#12290;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#20102;&#21098;&#26525;&#21644;&#26410;&#21098;&#26525;&#26435;&#37325;&#30340;&#38598;&#20307;&#26234;&#33021;&#20043;&#38388;&#30340;&#23398;&#20064;&#21327;&#21516;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#22686;&#38271;&#65292;DNN&#21442;&#25968;&#30340;&#25968;&#37327;&#22823;&#24133;&#22686;&#21152;&#12290;&#36825;&#20351;&#24471;DNN&#27169;&#22411;&#38590;&#20197;&#37096;&#32626;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#23884;&#20837;&#24335;&#31995;&#32479;&#19978;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#20986;&#29616;&#20102;&#21160;&#24577;&#21098;&#26525;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#30452;&#36890;&#20272;&#35745;&#65288;STE&#65289;&#26469;&#36817;&#20284;&#21098;&#26525;&#26435;&#37325;&#30340;&#26799;&#24230;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23547;&#25214;&#19981;&#21516;&#30340;&#31232;&#30095;&#27169;&#24335;&#12290;STE&#21487;&#20197;&#24110;&#21161;&#21098;&#26525;&#26435;&#37325;&#22312;&#23547;&#25214;&#21160;&#24577;&#31232;&#30095;&#27169;&#24335;&#30340;&#36807;&#31243;&#20013;&#22797;&#27963;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#36825;&#20123;&#31895;&#31961;&#30340;&#26799;&#24230;&#20250;&#23548;&#33268;&#35757;&#32451;&#19981;&#31283;&#23450;&#21644;&#24615;&#33021;&#19979;&#38477;&#65292;&#22240;&#20026;STE&#36817;&#20284;&#30340;&#26799;&#24230;&#20449;&#21495;&#19981;&#21487;&#38752;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31934;&#28860;&#30340;&#26799;&#24230;&#26469;&#26356;&#26032;&#21098;&#26525;&#26435;&#37325;&#65292;&#36890;&#36807;&#20174;&#20004;&#32452;&#65288;&#21098;&#26525;&#21644;&#26410;&#21098;&#26525;&#65289;&#26435;&#37325;&#24418;&#25104;&#21452;&#21521;&#36716;&#21457;&#36335;&#24452;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21160;&#24577;&#38598;&#20307;&#26234;&#33021;&#23398;&#20064;&#65288;DCIL&#65289;&#65292;&#21033;&#29992;&#20004;&#32452;&#26435;&#37325;&#30340;&#38598;&#20307;&#26234;&#33021;&#20043;&#38388;&#30340;&#23398;&#20064;&#21327;&#21516;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the growth of deep neural networks (DNN), the number of DNN parameters has drastically increased. This makes DNN models hard to be deployed on resource-limited embedded systems. To alleviate this problem, dynamic pruning methods have emerged, which try to find diverse sparsity patterns during training by utilizing Straight-Through-Estimator (STE) to approximate gradients of pruned weights. STE can help the pruned weights revive in the process of finding dynamic sparsity patterns. However, using these coarse gradients causes training instability and performance degradation owing to the unreliable gradient signal of the STE approximation. In this work, to tackle this issue, we introduce refined gradients to update the pruned weights by forming dual forwarding paths from two sets (pruned and unpruned) of weights. We propose a novel Dynamic Collective Intelligence Learning (DCIL) which makes use of the learning synergy between the collective intelligence of both weight sets. We verify
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#29702;&#35299;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#36890;&#36807;&#23558;&#20854;&#36755;&#20986;&#20998;&#35299;&#20026;&#36739;&#23567;&#30340;&#39033;&#65292;&#35777;&#26126;&#33258;&#27880;&#24847;&#21147;&#23545;&#8220;&#26631;&#35760;&#22343;&#34913;&#24615;&#8221;&#30340;&#24378;&#24402;&#32435;&#20559;&#35265;&#65292;&#32780;&#36339;&#36291;&#36830;&#25509;&#21644;&#22810;&#23618;&#24863;&#30693;&#26426;&#21487;&#20197;&#38459;&#27490;&#36755;&#20986;&#30340;&#36864;&#21270;&#12290;</title><link>http://arxiv.org/abs/2103.03404</link><description>&lt;p&gt;
&#27880;&#24847;&#21147;&#19981;&#26159;&#31572;&#26696;&#65306;&#32431;&#27880;&#24847;&#21147;&#22312;&#28145;&#24230;&#26041;&#38754;&#20197;&#21452;&#25351;&#25968;&#26041;&#24335;&#38477;&#20302;&#31561;&#32423;
&lt;/p&gt;
&lt;p&gt;
Attention is Not All You Need: Pure Attention Loses Rank Doubly Exponentially with Depth. (arXiv:2103.03404v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2103.03404
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#29702;&#35299;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#36890;&#36807;&#23558;&#20854;&#36755;&#20986;&#20998;&#35299;&#20026;&#36739;&#23567;&#30340;&#39033;&#65292;&#35777;&#26126;&#33258;&#27880;&#24847;&#21147;&#23545;&#8220;&#26631;&#35760;&#22343;&#34913;&#24615;&#8221;&#30340;&#24378;&#24402;&#32435;&#20559;&#35265;&#65292;&#32780;&#36339;&#36291;&#36830;&#25509;&#21644;&#22810;&#23618;&#24863;&#30693;&#26426;&#21487;&#20197;&#38459;&#27490;&#36755;&#20986;&#30340;&#36864;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#26550;&#26500;&#24050;&#32463;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#21464;&#24471;&#26080;&#22788;&#19981;&#22312;&#65292;&#28982;&#32780;&#25105;&#20204;&#23545;&#20854;&#26377;&#25928;&#24615;&#30340;&#21407;&#22240;&#30340;&#29702;&#35299;&#20173;&#28982;&#26377;&#38480;&#12290;&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29702;&#35299;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#30340;&#26041;&#27861;&#65306;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#20204;&#30340;&#36755;&#20986;&#21487;&#20197;&#20998;&#35299;&#20026;&#36739;&#23567;&#39033;&#30340;&#27714;&#21644;&#65292;&#27599;&#20010;&#39033;&#28041;&#21450;&#22312;&#23618;&#38388;&#25805;&#20316;&#30340;&#19968;&#31995;&#21015;&#27880;&#24847;&#21147;&#22836;&#12290;&#21033;&#29992;&#36825;&#31181;&#20998;&#35299;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#33258;&#27880;&#24847;&#21147;&#23545;&#8220;&#26631;&#35760;&#22343;&#34913;&#24615;&#8221;&#30340;&#24378;&#24402;&#32435;&#20559;&#35265;&#12290;&#20855;&#20307;&#22320;&#35828;&#65292;&#27809;&#26377;&#36339;&#36291;&#36830;&#25509;&#25110;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;MLP&#65289;&#65292;&#36755;&#20986;&#20250;&#20197;&#21452;&#25351;&#25968;&#30340;&#26041;&#24335;&#25910;&#25947;&#21040;&#19968;&#20010;&#31209;&#20026;1&#30340;&#30697;&#38453;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#36339;&#36291;&#36830;&#25509;&#21644;MLP&#21487;&#20197;&#38459;&#27490;&#36755;&#20986;&#30340;&#36864;&#21270;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#25152;&#35782;&#21035;&#30340;&#25910;&#25947;&#29616;&#35937;&#22312;&#19981;&#21516;&#21464;&#20307;&#30340;&#26631;&#20934;Transformer&#26550;&#26500;&#19978;&#30340;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention-based architectures have become ubiquitous in machine learning, yet our understanding of the reasons for their effectiveness remains limited. This work proposes a new way to understand self-attention networks: we show that their output can be decomposed into a sum of smaller terms, each involving the operation of a sequence of attention heads across layers. Using this decomposition, we prove that self-attention possesses a strong inductive bias towards "token uniformity". Specifically, without skip connections or multi-layer perceptrons (MLPs), the output converges doubly exponentially to a rank-1 matrix. On the other hand, skip connections and MLPs stop the output from degeneration. Our experiments verify the identified convergence phenomena on different variants of standard transformer architectures.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#26080;&#23725;&#32447;&#24615;&#22238;&#24402;&#20013;&#23384;&#22312;&#19968;&#20010;&#21452;&#19979;&#38477;&#23792;&#65292;&#26080;&#35770;&#36755;&#20837;&#20998;&#24067;&#30340;&#29305;&#24449;&#26144;&#23556;&#26159;&#30830;&#23450;&#24615;&#30340;&#36824;&#26159;&#38543;&#26426;&#30340;&#65292;&#37117;&#20250;&#23548;&#33268;&#26399;&#26395;&#22343;&#26041;&#27867;&#21270;&#35823;&#24046;&#22686;&#21152;&#12290;&#24182;&#19988;&#25105;&#20204;&#30340;&#32467;&#26524;&#36866;&#29992;&#20110;&#24191;&#27867;&#30340;&#36755;&#20837;&#20998;&#24067;&#31867;&#12290;</title><link>http://arxiv.org/abs/2010.01851</link><description>&lt;p&gt;
&#26080;&#23725;&#22238;&#24402;&#20013;&#21452;&#19979;&#38477;&#23792;&#30340;&#26222;&#36941;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Universality of the Double Descent Peak in Ridgeless Regression. (arXiv:2010.01851v8 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2010.01851
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#26080;&#23725;&#32447;&#24615;&#22238;&#24402;&#20013;&#23384;&#22312;&#19968;&#20010;&#21452;&#19979;&#38477;&#23792;&#65292;&#26080;&#35770;&#36755;&#20837;&#20998;&#24067;&#30340;&#29305;&#24449;&#26144;&#23556;&#26159;&#30830;&#23450;&#24615;&#30340;&#36824;&#26159;&#38543;&#26426;&#30340;&#65292;&#37117;&#20250;&#23548;&#33268;&#26399;&#26395;&#22343;&#26041;&#27867;&#21270;&#35823;&#24046;&#22686;&#21152;&#12290;&#24182;&#19988;&#25105;&#20204;&#30340;&#32467;&#26524;&#36866;&#29992;&#20110;&#24191;&#27867;&#30340;&#36755;&#20837;&#20998;&#24067;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#26080;&#23725;&#32447;&#24615;&#22238;&#24402;&#20013;&#30001;&#26631;&#31614;&#22122;&#22768;&#24341;&#36215;&#30340;&#26399;&#26395;&#22343;&#26041;&#27867;&#21270;&#35823;&#24046;&#30340;&#38750;&#28176;&#36817;&#38750;&#20998;&#24067;&#30456;&#20851;&#19979;&#30028;&#12290;&#25105;&#20204;&#30340;&#19979;&#30028;&#23558;&#31867;&#20284;&#30340;&#24050;&#30693;&#32467;&#26524;&#25512;&#24191;&#21040;&#36229;&#21442;&#25968;&#21270;&#65288;&#25554;&#20540;&#65289;&#21306;&#22495;&#12290;&#19982;&#22823;&#22810;&#25968;&#21069;&#26399;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#36866;&#29992;&#20110;&#20855;&#26377;&#20960;&#20046;&#24517;&#28982;&#23436;&#20840;&#31209;&#29305;&#24449;&#30697;&#38453;&#30340;&#24191;&#27867;&#36755;&#20837;&#20998;&#24067;&#31867;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#35206;&#30422;&#21508;&#31181;&#30830;&#23450;&#24615;&#25110;&#38543;&#26426;&#29305;&#24449;&#26144;&#23556;&#31867;&#22411;&#12290;&#25105;&#20204;&#30340;&#19979;&#30028;&#26159;&#28176;&#36817;&#23574;&#38160;&#30340;&#65292;&#24182;&#19988;&#24847;&#21619;&#30528;&#22312;&#23384;&#22312;&#26631;&#31614;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#65292;&#26080;&#23725;&#32447;&#24615;&#22238;&#24402;&#22312;&#20219;&#20309;&#36825;&#20123;&#29305;&#24449;&#26144;&#23556;&#30340;&#25554;&#20540;&#38408;&#20540;&#21608;&#22260;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#35814;&#32454;&#20998;&#26512;&#20102;&#25152;&#26045;&#21152;&#30340;&#20551;&#35774;&#65292;&#24182;&#20026;&#35299;&#26512;&#65288;&#38543;&#26426;&#65289;&#29305;&#24449;&#26144;&#23556;&#25552;&#20379;&#20102;&#29702;&#35770;&#12290;&#21033;&#29992;&#36825;&#20010;&#29702;&#35770;&#65292;&#25105;&#20204;&#21487;&#20197;&#35777;&#26126;&#25105;&#20204;&#30340;&#20551;&#35774;&#23545;&#20855;&#26377;&#65288;&#21202;&#36125;&#26684;&#65289;&#23494;&#24230;&#30340;&#36755;&#20837;&#20998;&#24067;&#20197;&#21450;&#30001;&#20998;&#26512;&#28608;&#27963;&#20989;&#25968;&#32473;&#20986;&#30340;&#38543;&#26426;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#24449;&#26144;&#23556;&#25104;&#31435;&#12290;
&lt;/p&gt;
&lt;p&gt;
We prove a non-asymptotic distribution-independent lower bound for the expected mean squared generalization error caused by label noise in ridgeless linear regression. Our lower bound generalizes a similar known result to the overparameterized (interpolating) regime. In contrast to most previous works, our analysis applies to a broad class of input distributions with almost surely full-rank feature matrices, which allows us to cover various types of deterministic or random feature maps. Our lower bound is asymptotically sharp and implies that in the presence of label noise, ridgeless linear regression does not perform well around the interpolation threshold for any of these feature maps. We analyze the imposed assumptions in detail and provide a theory for analytic (random) feature maps. Using this theory, we can show that our assumptions are satisfied for input distributions with a (Lebesgue) density and feature maps given by random deep neural networks with analytic activation functi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#22826;&#38451;&#36752;&#29031;&#24230;&#39044;&#27979;&#30340;&#32479;&#19968;&#26550;&#26500;&#65292;&#33021;&#22815;&#22312;&#19981;&#21516;&#26102;&#38388;&#23610;&#24230;&#19978;&#36827;&#34892;&#39044;&#27979;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#23558;&#36825;&#31181;&#26041;&#27861;&#25193;&#23637;&#21040;&#27599;&#23567;&#26102;&#39044;&#27979;&#33539;&#22260;&#12290;</title><link>http://arxiv.org/abs/1905.02616</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#38598;&#25104;&#22810;&#26102;&#38388;&#23610;&#24230;&#24314;&#27169;&#36827;&#34892;&#22826;&#38451;&#36752;&#29031;&#24230;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
An Integrated Multi-Time-Scale Modeling for Solar Irradiance Forecasting Using Deep Learning. (arXiv:1905.02616v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1905.02616
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#22826;&#38451;&#36752;&#29031;&#24230;&#39044;&#27979;&#30340;&#32479;&#19968;&#26550;&#26500;&#65292;&#33021;&#22815;&#22312;&#19981;&#21516;&#26102;&#38388;&#23610;&#24230;&#19978;&#36827;&#34892;&#39044;&#27979;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#23558;&#36825;&#31181;&#26041;&#27861;&#25193;&#23637;&#21040;&#27599;&#23567;&#26102;&#39044;&#27979;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#30701;&#26399;&#22826;&#38451;&#36752;&#29031;&#24230;&#39044;&#27979;&#65292;&#20256;&#32479;&#30340;&#28857;&#39044;&#27979;&#26041;&#27861;&#30001;&#20110;&#22826;&#38451;&#33021;&#21151;&#29575;&#30340;&#38750;&#24179;&#31283;&#29305;&#24615;&#32780;&#21464;&#24471;&#19981;&#22826;&#26377;&#29992;&#12290;&#30001;&#20110;&#22826;&#38451;&#33021;&#30340;&#21464;&#21160;&#24615;&#65292;&#38656;&#35201;&#26356;&#22810;&#30340;&#36816;&#34892;&#22791;&#29992;&#26469;&#30830;&#20445;&#30005;&#32593;&#30340;&#21487;&#38752;&#36816;&#34892;&#12290;&#21457;&#30005;&#19981;&#30830;&#23450;&#24615;&#36234;&#22823;&#65292;&#36816;&#34892;&#22791;&#29992;&#38656;&#27714;&#36234;&#22823;&#65292;&#36825;&#23558;&#23548;&#33268;&#36816;&#34892;&#25104;&#26412;&#30340;&#22686;&#21152;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#21644;&#38271;&#30701;&#26102;&#35760;&#24518;&#32593;&#32476;&#65288;LSTM&#65289;&#36827;&#34892;&#22810;&#26102;&#38388;&#23610;&#24230;&#39044;&#27979;&#30340;&#32479;&#19968;&#26550;&#26500;&#65292;&#29992;&#20110;&#39044;&#27979;&#27599;&#22825;&#20869;&#30340;&#22826;&#38451;&#36752;&#29031;&#24230;&#12290;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#36825;&#31181;&#24314;&#27169;&#26041;&#27861;&#25193;&#23637;&#21040;&#27599;&#23567;&#26102;&#39044;&#27979;&#33539;&#22260;&#65292;&#20174;&#32780;&#23454;&#29616;&#22810;&#26102;&#38388;&#33539;&#22260;&#30340;&#39044;&#27979;&#65292;&#33021;&#22815;&#39044;&#27979;&#27599;&#23567;&#26102;&#21644;&#27599;&#22825;&#30340;&#22826;&#38451;&#36752;&#29031;&#24230;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#27969;&#31243;&#26469;&#23454;&#26045;&#25552;&#20986;&#30340;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
For short-term solar irradiance forecasting, the traditional point forecasting methods are rendered less useful due to the non-stationary characteristic of solar power. The amount of operating reserves required to maintain reliable operation of the electric grid rises due to the variability of solar energy. The higher the uncertainty in the generation, the greater the operating-reserve requirements, which translates to an increased cost of operation. In this research work, we propose a unified architecture for multi-time-scale predictions for intra-day solar irradiance forecasting using recurrent neural networks (RNN) and long-short-term memory networks (LSTMs). This paper also lays out a framework for extending this modeling approach to intra-hour forecasting horizons thus, making it a multi-time-horizon forecasting approach, capable of predicting intra-hour as well as intra-day solar irradiance. We develop an end-to-end pipeline to effectuate the proposed architecture. The performanc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#22686;&#21152;&#22411;&#27169;&#22411;&#30340;&#20840;&#23616;&#22686;&#21152;&#24615;&#35299;&#37322;&#26041;&#27861;&#65292;&#21457;&#29616;&#19981;&#21516;&#30340;&#35299;&#37322;&#26041;&#27861;&#20197;&#19981;&#21516;&#30340;&#26041;&#24335;&#21051;&#30011;&#20102;&#40657;&#30418;&#27169;&#22411;&#39044;&#27979;&#20989;&#25968;&#20013;&#30340;&#38750;&#22686;&#21152;&#24615;&#25104;&#20998;&#12290;&#23613;&#31649;&#31934;&#31616;&#30340;&#35299;&#37322;&#19968;&#33324;&#26159;&#26368;&#20934;&#30830;&#30340;&#22686;&#21152;&#24615;&#35299;&#37322;&#65292;&#20294;&#26174;&#24335;&#24314;&#27169;&#38750;&#22686;&#21152;&#24615;&#25104;&#20998;&#30340;&#26641;&#24418;&#35299;&#37322;&#24448;&#24448;&#26356;&#20934;&#30830;&#12290;&#26426;&#22120;&#23398;&#20064;&#20174;&#19994;&#32773;&#33021;&#22815;&#26356;&#22909;&#22320;&#21033;&#29992;&#22686;&#21152;&#24615;&#35299;&#37322;&#26469;&#23436;&#25104;&#21508;&#31181;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/1801.08640</link><description>&lt;p&gt;
&#22312;&#23398;&#20064;&#40657;&#30418;&#27169;&#22411;&#30340;&#22686;&#21152;&#24615;&#35299;&#37322;&#26102;&#38656;&#35201;&#32771;&#34385;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Considerations When Learning Additive Explanations for Black-Box Models. (arXiv:1801.08640v4 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1801.08640
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#22686;&#21152;&#22411;&#27169;&#22411;&#30340;&#20840;&#23616;&#22686;&#21152;&#24615;&#35299;&#37322;&#26041;&#27861;&#65292;&#21457;&#29616;&#19981;&#21516;&#30340;&#35299;&#37322;&#26041;&#27861;&#20197;&#19981;&#21516;&#30340;&#26041;&#24335;&#21051;&#30011;&#20102;&#40657;&#30418;&#27169;&#22411;&#39044;&#27979;&#20989;&#25968;&#20013;&#30340;&#38750;&#22686;&#21152;&#24615;&#25104;&#20998;&#12290;&#23613;&#31649;&#31934;&#31616;&#30340;&#35299;&#37322;&#19968;&#33324;&#26159;&#26368;&#20934;&#30830;&#30340;&#22686;&#21152;&#24615;&#35299;&#37322;&#65292;&#20294;&#26174;&#24335;&#24314;&#27169;&#38750;&#22686;&#21152;&#24615;&#25104;&#20998;&#30340;&#26641;&#24418;&#35299;&#37322;&#24448;&#24448;&#26356;&#20934;&#30830;&#12290;&#26426;&#22120;&#23398;&#20064;&#20174;&#19994;&#32773;&#33021;&#22815;&#26356;&#22909;&#22320;&#21033;&#29992;&#22686;&#21152;&#24615;&#35299;&#37322;&#26469;&#23436;&#25104;&#21508;&#31181;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#35299;&#37322;&#40657;&#30418;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#26080;&#35770;&#26159;&#23616;&#37096;&#36824;&#26159;&#20840;&#23616;&#30340;&#65292;&#37117;&#26159;&#22686;&#21152;&#22411;&#30340;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#22686;&#21152;&#22411;&#27169;&#22411;&#30340;&#20840;&#23616;&#22686;&#21152;&#24615;&#35299;&#37322;&#65292;&#37325;&#28857;&#20851;&#27880;&#22235;&#31181;&#35299;&#37322;&#26041;&#27861;&#65306;&#23616;&#37096;&#20381;&#36182;&#12289;&#36866;&#24212;&#20840;&#23616;&#29615;&#22659;&#30340;Shapley&#35299;&#37322;&#12289;&#31934;&#31616;&#30340;&#22686;&#21152;&#24615;&#35299;&#37322;&#21644;&#22522;&#20110;&#26799;&#24230;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19981;&#21516;&#30340;&#35299;&#37322;&#26041;&#27861;&#20197;&#19981;&#21516;&#30340;&#26041;&#24335;&#21051;&#30011;&#20102;&#40657;&#30418;&#27169;&#22411;&#39044;&#27979;&#20989;&#25968;&#20013;&#30340;&#38750;&#22686;&#21152;&#24615;&#25104;&#20998;&#12290;&#25105;&#20204;&#20351;&#29992;&#20027;&#25928;&#24212;&#21644;&#24635;&#25928;&#24212;&#30340;&#27010;&#24565;&#26469;&#38170;&#23450;&#22686;&#21152;&#24615;&#35299;&#37322;&#65292;&#24182;&#23450;&#37327;&#35780;&#20272;&#22686;&#21152;&#24615;&#21644;&#38750;&#22686;&#21152;&#24615;&#35299;&#37322;&#12290;&#23613;&#31649;&#31934;&#31616;&#30340;&#35299;&#37322;&#19968;&#33324;&#26159;&#26368;&#20934;&#30830;&#30340;&#22686;&#21152;&#24615;&#35299;&#37322;&#65292;&#20294;&#26174;&#24335;&#24314;&#27169;&#38750;&#22686;&#21152;&#24615;&#25104;&#20998;&#30340;&#26641;&#24418;&#35299;&#37322;&#24448;&#24448;&#26356;&#20934;&#30830;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#25105;&#20204;&#30340;&#29992;&#25143;&#30740;&#31350;&#34920;&#26126;&#65292;&#26426;&#22120;&#23398;&#20064;&#20174;&#19994;&#32773;&#33021;&#22815;&#26356;&#22909;&#22320;&#21033;&#29992;&#22686;&#21152;&#24615;&#35299;&#37322;&#26469;&#23436;&#25104;&#21508;&#31181;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many methods to explain black-box models, whether local or global, are additive. In this paper, we study global additive explanations for non-additive models, focusing on four explanation methods: partial dependence, Shapley explanations adapted to a global setting, distilled additive explanations, and gradient-based explanations. We show that different explanation methods characterize non-additive components in a black-box model's prediction function in different ways. We use the concepts of main and total effects to anchor additive explanations, and quantitatively evaluate additive and non-additive explanations. Even though distilled explanations are generally the most accurate additive explanations, non-additive explanations such as tree explanations that explicitly model non-additive components tend to be even more accurate. Despite this, our user study showed that machine learning practitioners were better able to leverage additive explanations for various tasks. These considerati
&lt;/p&gt;</description></item></channel></rss>