<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#23454;&#39564;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26631;&#35760;&#25968;&#25454;&#22686;&#24378;&#21644;&#27979;&#22320;&#25554;&#20540;&#26041;&#27861;&#23398;&#20064;&#34507;&#30333;&#36136;&#25240;&#21472;&#30340;&#38598;&#20307;&#21464;&#37327;&#30340;&#31574;&#30053;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#37319;&#26679;&#25928;&#29575;&#65292;&#24182;&#22312;&#36807;&#28193;&#24577;&#25968;&#25454;&#26377;&#38480;&#19988;&#22024;&#26434;&#26102;&#34920;&#29616;&#20248;&#20110;&#22522;&#20110;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01542</link><description>&lt;p&gt;
&#20351;&#29992;&#26631;&#35760;&#25968;&#25454;&#22686;&#24378;&#30340;&#27979;&#22320;&#25554;&#20540;&#26041;&#27861;&#23398;&#20064;&#34507;&#30333;&#36136;&#25240;&#21472;&#30340;&#38598;&#20307;&#21464;&#37327;
&lt;/p&gt;
&lt;p&gt;
Learning Collective Variables for Protein Folding with Labeled Data Augmentation through Geodesic Interpolation
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01542
&lt;/p&gt;
&lt;p&gt;
&#26412;&#23454;&#39564;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26631;&#35760;&#25968;&#25454;&#22686;&#24378;&#21644;&#27979;&#22320;&#25554;&#20540;&#26041;&#27861;&#23398;&#20064;&#34507;&#30333;&#36136;&#25240;&#21472;&#30340;&#38598;&#20307;&#21464;&#37327;&#30340;&#31574;&#30053;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#37319;&#26679;&#25928;&#29575;&#65292;&#24182;&#22312;&#36807;&#28193;&#24577;&#25968;&#25454;&#26377;&#38480;&#19988;&#22024;&#26434;&#26102;&#34920;&#29616;&#20248;&#20110;&#22522;&#20110;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20998;&#23376;&#21160;&#21147;&#23398;&#65288;MD&#65289;&#27169;&#25311;&#20013;&#65292;&#36890;&#24120;&#36890;&#36807;&#22686;&#24378;&#37319;&#26679;&#25216;&#26415;&#26469;&#30740;&#31350;&#34507;&#30333;&#36136;&#25240;&#21472;&#31561;&#32597;&#35265;&#20107;&#20214;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#20381;&#36182;&#20110;&#27839;&#30528;&#21152;&#36895;&#21457;&#29983;&#30340;&#38598;&#20307;&#21464;&#37327;&#65288;CV&#65289;&#30340;&#23450;&#20041;&#12290;&#33719;&#24471;&#23500;&#26377;&#34920;&#36798;&#21147;&#30340;CV&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#24448;&#24448;&#21463;&#21040;&#20851;&#20110;&#29305;&#23450;&#20107;&#20214;&#30340;&#20449;&#24687;&#19981;&#36275;&#30340;&#38459;&#30861;&#65292;&#20363;&#22914;&#20174;&#26410;&#25240;&#21472;&#21040;&#25240;&#21472;&#26500;&#35937;&#30340;&#36716;&#21464;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#25311;&#26080;&#20851;&#30340;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#21033;&#29992;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#24230;&#37327;&#26469;&#29983;&#25104;&#31867;&#20284;&#34507;&#30333;&#36136;&#25240;&#21472;&#36716;&#21464;&#30340;&#27979;&#22320;&#25554;&#20540;&#65292;&#20174;&#32780;&#25552;&#39640;&#37319;&#26679;&#25928;&#29575;&#65292;&#32780;&#26080;&#38656;&#30495;&#23454;&#30340;&#36807;&#28193;&#24577;&#26679;&#26412;&#12290;&#36890;&#36807;&#21033;&#29992;&#25554;&#20540;&#36827;&#24230;&#21442;&#25968;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#22238;&#24402;&#30340;&#23398;&#20064;&#26041;&#26696;&#26469;&#26500;&#24314;CV&#27169;&#22411;&#65292;&#24403;&#36807;&#28193;&#24577;&#25968;&#25454;&#26377;&#38480;&#19988;&#22024;&#26434;&#26102;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#20248;&#20110;&#22522;&#20110;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In molecular dynamics (MD) simulations, rare events, such as protein folding, are typically studied by means of enhanced sampling techniques, most of which rely on the definition of a collective variable (CV) along which the acceleration occurs. Obtaining an expressive CV is crucial, but often hindered by the lack of information about the particular event, e.g., the transition from unfolded to folded conformation. We propose a simulation-free data augmentation strategy using physics-inspired metrics to generate geodesic interpolations resembling protein folding transitions, thereby improving sampling efficiency without true transition state samples. Leveraging interpolation progress parameters, we introduce a regression-based learning scheme for CV models, which outperforms classifier-based methods when transition state data is limited and noisy
&lt;/p&gt;</description></item><item><title>&#23637;&#31034;&#20102;&#23545;&#40784;&#30340;LLM&#23545;&#31616;&#21333;&#33258;&#36866;&#24212;&#36234;&#29425;&#25915;&#20987;&#19981;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#25104;&#21151;&#23454;&#29616;&#20102;&#22312;&#22810;&#20010;&#27169;&#22411;&#19978;&#20960;&#20046;100%&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#21516;&#26102;&#36824;&#20171;&#32461;&#20102;&#23545;&#20110;&#19981;&#20844;&#24320;logprobs&#30340;&#27169;&#22411;&#22914;&#20309;&#36827;&#34892;&#36234;&#29425;&#20197;&#21450;&#22914;&#20309;&#22312;&#21463;&#27745;&#26579;&#30340;&#27169;&#22411;&#20013;&#26597;&#25214;&#26408;&#39532;&#23383;&#31526;&#20018;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2404.02151</link><description>&lt;p&gt;
&#29992;&#31616;&#21333;&#33258;&#36866;&#24212;&#25915;&#20987;&#36234;&#29425;&#21151;&#33021;&#23545;&#40784;&#30340;LLM
&lt;/p&gt;
&lt;p&gt;
Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02151
&lt;/p&gt;
&lt;p&gt;
&#23637;&#31034;&#20102;&#23545;&#40784;&#30340;LLM&#23545;&#31616;&#21333;&#33258;&#36866;&#24212;&#36234;&#29425;&#25915;&#20987;&#19981;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#25104;&#21151;&#23454;&#29616;&#20102;&#22312;&#22810;&#20010;&#27169;&#22411;&#19978;&#20960;&#20046;100%&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#21516;&#26102;&#36824;&#20171;&#32461;&#20102;&#23545;&#20110;&#19981;&#20844;&#24320;logprobs&#30340;&#27169;&#22411;&#22914;&#20309;&#36827;&#34892;&#36234;&#29425;&#20197;&#21450;&#22914;&#20309;&#22312;&#21463;&#27745;&#26579;&#30340;&#27169;&#22411;&#20013;&#26597;&#25214;&#26408;&#39532;&#23383;&#31526;&#20018;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20351;&#26159;&#26368;&#26032;&#30340;&#23433;&#20840;&#23545;&#40784;&#30340;LLM&#20063;&#19981;&#20855;&#26377;&#25269;&#25239;&#31616;&#21333;&#33258;&#36866;&#24212;&#36234;&#29425;&#25915;&#20987;&#30340;&#31283;&#20581;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#25104;&#21151;&#21033;&#29992;&#23545;logprobs&#30340;&#35775;&#38382;&#36827;&#34892;&#36234;&#29425;&#65306;&#25105;&#20204;&#26368;&#21021;&#35774;&#35745;&#20102;&#19968;&#20010;&#23545;&#25239;&#24615;&#25552;&#31034;&#27169;&#26495;&#65288;&#26377;&#26102;&#20250;&#36866;&#24212;&#30446;&#26631;LLM&#65289;&#65292;&#28982;&#21518;&#25105;&#20204;&#22312;&#21518;&#32512;&#19978;&#24212;&#29992;&#38543;&#26426;&#25628;&#32034;&#20197;&#26368;&#22823;&#21270;&#30446;&#26631;logprob&#65288;&#20363;&#22914;token&#8220;Sure&#8221;&#65289;&#65292;&#21487;&#33021;&#20250;&#36827;&#34892;&#22810;&#27425;&#37325;&#21551;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#23545;GPT-3.5/4&#12289;Llama-2-Chat-7B/13B/70B&#12289;Gemma-7B&#21644;&#38024;&#23545;GCG&#25915;&#20987;&#36827;&#34892;&#23545;&#25239;&#35757;&#32451;&#30340;HarmBench&#19978;&#30340;R2D2&#31561;&#20960;&#20046;100%&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;--&#26681;&#25454;GPT-4&#30340;&#35780;&#21028;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#36716;&#31227;&#25110;&#39044;&#22635;&#20805;&#25915;&#20987;&#20197;100%&#30340;&#25104;&#21151;&#29575;&#23545;&#25152;&#26377;&#19981;&#26292;&#38706;logprobs&#30340;Claude&#27169;&#22411;&#36827;&#34892;&#36234;&#29425;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#21463;&#27745;&#26579;&#30340;&#27169;&#22411;&#20013;&#20351;&#29992;&#23545;&#19968;&#32452;&#21463;&#38480;&#21046;&#30340;token&#25191;&#34892;&#38543;&#26426;&#25628;&#32034;&#20197;&#26597;&#25214;&#26408;&#39532;&#23383;&#31526;&#20018;&#30340;&#26041;&#27861;--&#36825;&#39033;&#20219;&#21153;&#19982;&#35768;&#22810;&#20854;&#20182;&#20219;&#21153;&#20849;&#20139;&#30456;&#21516;&#30340;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02151v1 Announce Type: cross  Abstract: We show that even the most recent safety-aligned LLMs are not robust to simple adaptive jailbreaking attacks. First, we demonstrate how to successfully leverage access to logprobs for jailbreaking: we initially design an adversarial prompt template (sometimes adapted to the target LLM), and then we apply random search on a suffix to maximize the target logprob (e.g., of the token "Sure"), potentially with multiple restarts. In this way, we achieve nearly 100\% attack success rate -- according to GPT-4 as a judge -- on GPT-3.5/4, Llama-2-Chat-7B/13B/70B, Gemma-7B, and R2D2 from HarmBench that was adversarially trained against the GCG attack. We also show how to jailbreak all Claude models -- that do not expose logprobs -- via either a transfer or prefilling attack with 100\% success rate. In addition, we show how to use random search on a restricted set of tokens for finding trojan strings in poisoned models -- a task that shares many s
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#25289;&#32454;&#23391;&#21010;&#20998;&#38598;&#65292;&#25105;&#20204;&#33021;&#22815;&#22312;&#22240;&#23376;&#25968;&#25454;&#20013;&#31283;&#20581;&#22320;&#20272;&#35745;&#24322;&#36136;&#24615;&#65292;&#24182;&#23558;&#22240;&#23376;&#31354;&#38388;&#21010;&#20998;&#25104;&#21327;&#21464;&#37327;&#32452;&#21512;&#30340;&#8220;&#27744;&#8221;&#65292;&#20197;&#20415;&#21306;&#20998;&#32467;&#26524;&#30340;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2404.02141</link><description>&lt;p&gt;
&#20351;&#29992;&#25289;&#32454;&#23391;&#21010;&#20998;&#22312;&#22240;&#23376;&#25968;&#25454;&#20013;&#31283;&#20581;&#20272;&#35745;&#24322;&#36136;&#24615;
&lt;/p&gt;
&lt;p&gt;
Robustly estimating heterogeneity in factorial data using Rashomon Partitions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02141
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#25289;&#32454;&#23391;&#21010;&#20998;&#38598;&#65292;&#25105;&#20204;&#33021;&#22815;&#22312;&#22240;&#23376;&#25968;&#25454;&#20013;&#31283;&#20581;&#22320;&#20272;&#35745;&#24322;&#36136;&#24615;&#65292;&#24182;&#23558;&#22240;&#23376;&#31354;&#38388;&#21010;&#20998;&#25104;&#21327;&#21464;&#37327;&#32452;&#21512;&#30340;&#8220;&#27744;&#8221;&#65292;&#20197;&#20415;&#21306;&#20998;&#32467;&#26524;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#32479;&#35745;&#20998;&#26512;&#65292;&#26080;&#35770;&#26159;&#22312;&#35266;&#27979;&#25968;&#25454;&#36824;&#26159;&#38543;&#26426;&#23545;&#29031;&#35797;&#39564;&#20013;&#65292;&#37117;&#20250;&#38382;&#65306;&#24863;&#20852;&#36259;&#30340;&#32467;&#26524;&#22914;&#20309;&#38543;&#21487;&#35266;&#23519;&#21327;&#21464;&#37327;&#32452;&#21512;&#21464;&#21270;&#65311;&#19981;&#21516;&#30340;&#33647;&#29289;&#32452;&#21512;&#22914;&#20309;&#24433;&#21709;&#20581;&#24247;&#32467;&#26524;&#65292;&#31185;&#25216;&#37319;&#32435;&#22914;&#20309;&#20381;&#36182;&#28608;&#21169;&#21644;&#20154;&#21475;&#32479;&#35745;&#23398;&#65311;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#23558;&#36825;&#20010;&#22240;&#23376;&#31354;&#38388;&#21010;&#20998;&#25104;&#21327;&#21464;&#37327;&#32452;&#21512;&#30340;&#8220;&#27744;&#8221;&#65292;&#22312;&#36825;&#20123;&#27744;&#20013;&#32467;&#26524;&#20250;&#21457;&#29983;&#24046;&#24322;&#65288;&#20294;&#27744;&#20869;&#37096;&#19981;&#20250;&#21457;&#29983;&#65289;&#65292;&#32780;&#29616;&#26377;&#26041;&#27861;&#35201;&#20040;&#23547;&#25214;&#19968;&#20010;&#21333;&#19968;&#30340;&#8220;&#26368;&#20248;&#8221;&#20998;&#21106;&#65292;&#35201;&#20040;&#20174;&#21487;&#33021;&#20998;&#21106;&#30340;&#25972;&#20010;&#38598;&#21512;&#20013;&#25277;&#26679;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#24573;&#35270;&#20102;&#36825;&#26679;&#19968;&#20010;&#20107;&#23454;&#65306;&#29305;&#21035;&#26159;&#22312;&#21327;&#21464;&#37327;&#20043;&#38388;&#23384;&#22312;&#30456;&#20851;&#32467;&#26500;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#33021;&#20197;&#35768;&#22810;&#31181;&#26041;&#24335;&#21010;&#20998;&#21327;&#21464;&#37327;&#31354;&#38388;&#65292;&#22312;&#32479;&#35745;&#19978;&#26159;&#26080;&#27861;&#21306;&#20998;&#30340;&#65292;&#23613;&#31649;&#23545;&#25919;&#31574;&#25110;&#31185;&#23398;&#26377;&#30528;&#38750;&#24120;&#19981;&#21516;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25289;&#32454;&#23391;&#21010;&#20998;&#38598;&#30340;&#26367;&#20195;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02141v1 Announce Type: cross  Abstract: Many statistical analyses, in both observational data and randomized control trials, ask: how does the outcome of interest vary with combinations of observable covariates? How do various drug combinations affect health outcomes, or how does technology adoption depend on incentives and demographics? Our goal is to partition this factorial space into ``pools'' of covariate combinations where the outcome differs across the pools (but not within a pool). Existing approaches (i) search for a single ``optimal'' partition under assumptions about the association between covariates or (ii) sample from the entire set of possible partitions. Both these approaches ignore the reality that, especially with correlation structure in covariates, many ways to partition the covariate space may be statistically indistinguishable, despite very different implications for policy or science. We develop an alternative perspective, called Rashomon Partition Set
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20027;&#39064;&#30340;&#27700;&#21360;&#31639;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#24403;&#21069;&#27700;&#21360;&#26041;&#26696;&#30340;&#23616;&#38480;&#24615;&#65292;&#20026;&#21306;&#20998;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#21644;&#20154;&#31867;&#29983;&#25104;&#30340;&#25991;&#26412;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#12290;</title><link>https://arxiv.org/abs/2404.02138</link><description>&lt;p&gt;
&#22522;&#20110;&#20027;&#39064;&#30340;LLM&#29983;&#25104;&#25991;&#26412;&#30340;&#27700;&#21360;
&lt;/p&gt;
&lt;p&gt;
Topic-based Watermarks for LLM-Generated Text
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02138
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20027;&#39064;&#30340;&#27700;&#21360;&#31639;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#24403;&#21069;&#27700;&#21360;&#26041;&#26696;&#30340;&#23616;&#38480;&#24615;&#65292;&#20026;&#21306;&#20998;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#21644;&#20154;&#31867;&#29983;&#25104;&#30340;&#25991;&#26412;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#23548;&#33268;&#20102;&#29983;&#25104;&#30340;&#25991;&#26412;&#36755;&#20986;&#19982;&#20154;&#31867;&#29983;&#25104;&#30340;&#25991;&#26412;&#30456;&#20284;&#24230;&#38590;&#20197;&#20998;&#36776;&#12290;&#27700;&#21360;&#31639;&#27861;&#26159;&#28508;&#22312;&#24037;&#20855;&#65292;&#36890;&#36807;&#22312;LLM&#29983;&#25104;&#30340;&#36755;&#20986;&#20013;&#23884;&#20837;&#21487;&#26816;&#27979;&#30340;&#31614;&#21517;&#65292;&#21487;&#20197;&#21306;&#20998;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#21644;&#20154;&#31867;&#29983;&#25104;&#30340;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#27700;&#21360;&#26041;&#26696;&#22312;&#24050;&#30693;&#25915;&#20987;&#19979;&#32570;&#20047;&#20581;&#22766;&#24615;&#12290;&#27492;&#22806;&#65292;&#32771;&#34385;&#21040;LLM&#27599;&#22825;&#29983;&#25104;&#25968;&#19975;&#20010;&#25991;&#26412;&#36755;&#20986;&#65292;&#27700;&#21360;&#31639;&#27861;&#38656;&#35201;&#35760;&#24518;&#27599;&#20010;&#36755;&#20986;&#25165;&#33021;&#35753;&#26816;&#27979;&#27491;&#24120;&#24037;&#20316;&#65292;&#36825;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#26412;&#25991;&#38024;&#23545;&#24403;&#21069;&#27700;&#21360;&#26041;&#26696;&#30340;&#23616;&#38480;&#24615;&#65292;&#25552;&#20986;&#20102;&#38024;&#23545;LLMs&#30340;&#8220;&#22522;&#20110;&#20027;&#39064;&#30340;&#27700;&#21360;&#31639;&#27861;&#8221;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02138v1 Announce Type: cross  Abstract: Recent advancements of large language models (LLMs) have resulted in indistinguishable text outputs comparable to human-generated text. Watermarking algorithms are potential tools that offer a way to differentiate between LLM- and human-generated text by embedding detectable signatures within LLM-generated output. However, current watermarking schemes lack robustness against known attacks against watermarking algorithms. In addition, they are impractical considering an LLM generates tens of thousands of text outputs per day and the watermarking algorithm needs to memorize each output it generates for the detection to work. In this work, focusing on the limitations of current watermarking schemes, we propose the concept of a "topic-based watermarking algorithm" for LLMs. The proposed algorithm determines how to generate tokens for the watermarked LLM output based on extracted topics of an input prompt or the output of a non-watermarked 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;LawInstruct&#30340;&#22823;&#22411;&#27861;&#24459;&#25351;&#23548;&#25968;&#25454;&#38598;&#65292;&#35777;&#26126;&#20102;&#39046;&#22495;&#29305;&#23450;&#30340;&#39044;&#35757;&#32451;&#21644;&#25351;&#23548;&#35843;&#25972;&#21487;&#20197;&#25913;&#21892;&#22312;LegalBench&#19978;&#30340;&#24615;&#33021;&#65292;&#20026;&#22312;&#27861;&#24459;&#39046;&#22495;&#24320;&#21457;&#20855;&#26377;&#26356;&#24378;&#20449;&#24687;&#22788;&#29702;&#21644;&#20915;&#31574;&#33021;&#21147;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#20010;&#36164;&#28304;&#12290;</title><link>https://arxiv.org/abs/2404.02127</link><description>&lt;p&gt;
FLawN-T5: &#26377;&#25928;&#25351;&#23548;&#35843;&#25972;&#25968;&#25454;&#28151;&#21512;&#22312;&#27861;&#24459;&#25512;&#29702;&#20013;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
FLawN-T5: An Empirical Examination of Effective Instruction-Tuning Data Mixtures for Legal Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02127
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;LawInstruct&#30340;&#22823;&#22411;&#27861;&#24459;&#25351;&#23548;&#25968;&#25454;&#38598;&#65292;&#35777;&#26126;&#20102;&#39046;&#22495;&#29305;&#23450;&#30340;&#39044;&#35757;&#32451;&#21644;&#25351;&#23548;&#35843;&#25972;&#21487;&#20197;&#25913;&#21892;&#22312;LegalBench&#19978;&#30340;&#24615;&#33021;&#65292;&#20026;&#22312;&#27861;&#24459;&#39046;&#22495;&#24320;&#21457;&#20855;&#26377;&#26356;&#24378;&#20449;&#24687;&#22788;&#29702;&#21644;&#20915;&#31574;&#33021;&#21147;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#20010;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02127v1  &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495;  &#25688;&#35201;: &#25351;&#23548;&#35843;&#25972;&#26159;&#20351;&#35821;&#35328;&#27169;&#22411;&#23545;&#30452;&#25509;&#29992;&#25143;&#20132;&#20114;&#26377;&#25928;&#30340;&#37325;&#35201;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#27861;&#24459;&#20219;&#21153;&#20173;&#28982;&#36229;&#20986;&#20102;&#22823;&#22810;&#25968;&#24320;&#25918;&#24335;LLMs&#30340;&#33539;&#22260;&#65292;&#32780;&#19988;&#30446;&#21069;&#35813;&#39046;&#22495;&#36824;&#27809;&#26377;&#20219;&#20309;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#12290;&#36825;&#20005;&#37325;&#38480;&#21046;&#20102;&#35813;&#24212;&#29992;&#39046;&#22495;&#30340;&#30740;&#31350;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#31574;&#21010;&#20102;&#19968;&#20010;&#21517;&#20026;LawInstruct&#30340;&#22823;&#22411;&#27861;&#24459;&#25351;&#23548;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;17&#20010;&#21496;&#27861;&#31649;&#36758;&#21306;&#12289;24&#31181;&#35821;&#35328;&#65292;&#24635;&#35745;1200&#19975;&#20010;&#31034;&#20363;&#12290;&#25105;&#20204;&#21576;&#29616;&#35777;&#25454;&#34920;&#26126;&#65292;&#39046;&#22495;&#29305;&#23450;&#30340;&#39044;&#35757;&#32451;&#21644;&#25351;&#23548;&#35843;&#25972;&#33021;&#22815;&#25913;&#21892;&#22312;LegalBench&#19978;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#23558;Flan-T5 XL&#22312;&#22522;&#20934;&#32447;&#19978;&#25552;&#39640;8&#20010;&#28857;&#25110;16%&#12290;&#28982;&#32780;&#65292;&#35813;&#25928;&#24212;&#24182;&#19981;&#36866;&#29992;&#20110;&#25152;&#26377;&#20219;&#21153;&#12289;&#35757;&#32451;&#27169;&#24335;&#12289;&#27169;&#22411;&#22823;&#23567;&#21644;&#20854;&#20182;&#22240;&#32032;&#12290;LawInstruct&#26159;&#19968;&#20010;&#36164;&#28304;&#65292;&#21487;&#20197;&#21152;&#36895;&#22312;&#27861;&#24459;&#39046;&#22495;&#24320;&#21457;&#20855;&#26377;&#26356;&#24378;&#20449;&#24687;&#22788;&#29702;&#21644;&#20915;&#31574;&#33021;&#21147;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02127v1 Announce Type: cross  Abstract: Instruction tuning is an important step in making language models useful for direct user interaction. However, many legal tasks remain out of reach for most open LLMs and there do not yet exist any large scale instruction datasets for the domain. This critically limits research in this application area. In this work, we curate LawInstruct, a large legal instruction dataset, covering 17 jurisdictions, 24 languages and a total of 12M examples. We present evidence that domain-specific pretraining and instruction tuning improve performance on LegalBench, including improving Flan-T5 XL by 8 points or 16\% over the baseline. However, the effect does not generalize across all tasks, training regimes, model sizes, and other factors. LawInstruct is a resource for accelerating the development of models with stronger information processing and decision making capabilities in the legal domain.
&lt;/p&gt;</description></item><item><title>GINopic&#26159;&#19968;&#31181;&#20027;&#39064;&#24314;&#27169;&#26694;&#26550;&#65292;&#21033;&#29992;&#22270;&#21516;&#26500;&#32593;&#32476;&#25429;&#25417;&#21333;&#35789;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#20027;&#39064;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#26356;&#22909;&#30340;&#26377;&#25928;&#24615;&#21644;&#25512;&#36827;&#20027;&#39064;&#24314;&#27169;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.02115</link><description>&lt;p&gt;
GINopic&#65306;&#21033;&#29992;&#22270;&#21516;&#26500;&#32593;&#32476;&#36827;&#34892;&#20027;&#39064;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
GINopic: Topic Modeling with Graph Isomorphism Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02115
&lt;/p&gt;
&lt;p&gt;
GINopic&#26159;&#19968;&#31181;&#20027;&#39064;&#24314;&#27169;&#26694;&#26550;&#65292;&#21033;&#29992;&#22270;&#21516;&#26500;&#32593;&#32476;&#25429;&#25417;&#21333;&#35789;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#20027;&#39064;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#26356;&#22909;&#30340;&#26377;&#25928;&#24615;&#21644;&#25512;&#36827;&#20027;&#39064;&#24314;&#27169;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#39064;&#24314;&#27169;&#26159;&#20998;&#26512;&#21644;&#25506;&#32034;&#22823;&#22411;&#25991;&#26723;&#38598;&#21512;&#30340;&#24191;&#27867;&#20351;&#29992;&#26041;&#27861;&#12290; &#26368;&#36817;&#30340;&#30740;&#31350;&#24037;&#20316;&#23558;&#39044;&#35757;&#32451;&#30340;&#19978;&#19979;&#25991;&#21270;&#35821;&#35328;&#27169;&#22411;&#65292;&#22914;BERT&#23884;&#20837;&#65292;&#32435;&#20837;&#20027;&#39064;&#24314;&#27169;&#20013;&#12290; &#28982;&#32780;&#65292;&#23427;&#20204;&#36890;&#24120;&#24573;&#30053;&#20102;&#21333;&#35789;&#20043;&#38388;&#30456;&#20114;&#20381;&#36182;&#20256;&#36798;&#30340;&#22266;&#26377;&#20449;&#24687;&#20215;&#20540;&#12290; &#26412;&#30740;&#31350;&#20171;&#32461;&#20102;GINopic&#65292;&#19968;&#31181;&#22522;&#20110;&#22270;&#21516;&#26500;&#32593;&#32476;&#30340;&#20027;&#39064;&#24314;&#27169;&#26694;&#26550;&#65292;&#20197;&#25429;&#25417;&#21333;&#35789;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290; &#36890;&#36807;&#22312;&#19981;&#21516;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20869;&#22312;&#30340;&#65288;&#23450;&#37327;&#21644;&#23450;&#24615;&#65289;&#21644;&#22806;&#37096;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19982;&#29616;&#26377;&#20027;&#39064;&#27169;&#22411;&#30456;&#27604;&#65292;GINopic&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#31361;&#20986;&#20102;&#20854;&#25512;&#36827;&#20027;&#39064;&#24314;&#27169;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02115v1 Announce Type: new  Abstract: Topic modeling is a widely used approach for analyzing and exploring large document collections. Recent research efforts have incorporated pre-trained contextualized language models, such as BERT embeddings, into topic modeling. However, they often neglect the intrinsic informational value conveyed by mutual dependencies between words. In this study, we introduce GINopic, a topic modeling framework based on graph isomorphism networks to capture the correlation between words. By conducting intrinsic (quantitative as well as qualitative) and extrinsic evaluations on diverse benchmark datasets, we demonstrate the effectiveness of GINopic compared to existing topic models and highlight its potential for advancing topic modeling.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#35843;&#25972;&#21644;&#35780;&#20272;&#32456;&#36523;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65292;&#22312;&#27492;&#26041;&#27861;&#20013;&#65292;&#21482;&#26377;&#23454;&#39564;&#25968;&#25454;&#30340;&#19968;&#23567;&#37096;&#20998;&#21487;&#29992;&#20110;&#36229;&#21442;&#25968;&#35843;&#25972;&#65292;&#38024;&#23545;&#32456;&#36523;&#24378;&#21270;&#23398;&#20064;&#30340;&#30740;&#31350;&#36827;&#23637;&#21487;&#33021;&#34987;&#19981;&#24403;&#30340;&#32463;&#39564;&#26041;&#27861;&#25152;&#38459;&#30861;</title><link>https://arxiv.org/abs/2404.02113</link><description>&lt;p&gt;
&#38024;&#23545;&#26410;&#30693;&#36827;&#34892;&#35843;&#25972;&#65306;&#37325;&#26032;&#23457;&#35270;&#32456;&#36523;&#24378;&#21270;&#23398;&#20064;&#30340;&#35780;&#20272;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Tuning for the Unknown: Revisiting Evaluation Strategies for Lifelong RL
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02113
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#35843;&#25972;&#21644;&#35780;&#20272;&#32456;&#36523;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65292;&#22312;&#27492;&#26041;&#27861;&#20013;&#65292;&#21482;&#26377;&#23454;&#39564;&#25968;&#25454;&#30340;&#19968;&#23567;&#37096;&#20998;&#21487;&#29992;&#20110;&#36229;&#21442;&#25968;&#35843;&#25972;&#65292;&#38024;&#23545;&#32456;&#36523;&#24378;&#21270;&#23398;&#20064;&#30340;&#30740;&#31350;&#36827;&#23637;&#21487;&#33021;&#34987;&#19981;&#24403;&#30340;&#32463;&#39564;&#26041;&#27861;&#25152;&#38459;&#30861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32487;&#32493;&#25110;&#32456;&#36523;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#23545;&#29615;&#22659;&#30340;&#35775;&#38382;&#24212;&#35813;&#26159;&#26377;&#38480;&#30340;&#12290;&#22914;&#26524;&#25105;&#20204;&#24076;&#26395;&#35774;&#35745;&#30340;&#31639;&#27861;&#33021;&#22815;&#38271;&#26102;&#38388;&#36816;&#34892;&#65292;&#24182;&#19981;&#26029;&#36866;&#24212;&#26032;&#30340;&#12289;&#24847;&#24819;&#19981;&#21040;&#30340;&#24773;&#20917;&#65292;&#37027;&#20040;&#25105;&#20204;&#24517;&#39035;&#24895;&#24847;&#22312;&#25972;&#20010;&#20195;&#29702;&#30340;&#25972;&#20010;&#29983;&#21629;&#21608;&#26399;&#20869;&#37096;&#32626;&#25105;&#20204;&#30340;&#20195;&#29702;&#32780;&#19981;&#35843;&#25972;&#23427;&#20204;&#30340;&#36229;&#21442;&#25968;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013; -- &#29978;&#33267;&#32487;&#32493;&#24378;&#21270;&#23398;&#20064;&#20013; -- &#20855;&#22791;&#23545;&#20195;&#29702;&#30340;&#37096;&#32626;&#29615;&#22659;&#20855;&#26377;&#26080;&#38480;&#21046;&#35775;&#38382;&#26435;&#30340;&#26631;&#20934;&#20570;&#27861;&#21487;&#33021;&#24050;&#32463;&#38459;&#30861;&#20102;&#23545;&#32456;&#36523;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;&#30340;&#36827;&#23637;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35843;&#25972;&#21644;&#35780;&#20272;&#32456;&#36523;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65292;&#20854;&#20013;&#21482;&#26377;&#23454;&#39564;&#25968;&#25454;&#30340;&#30334;&#20998;&#20043;&#19968;&#21487;&#20197;&#29992;&#20110;&#36229;&#21442;&#25968;&#35843;&#25972;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;DQN&#21644;Soft Actor Critic&#22312;&#21508;&#31181;&#25345;&#32493;&#21644;&#38750;&#31283;&#23450;&#39046;&#22495;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#20004;&#31181;&#26041;&#27861;&#36890;&#24120;&#34920;&#29616;&#36739;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02113v1 Announce Type: new  Abstract: In continual or lifelong reinforcement learning access to the environment should be limited. If we aspire to design algorithms that can run for long-periods of time, continually adapting to new, unexpected situations then we must be willing to deploy our agents without tuning their hyperparameters over the agent's entire lifetime. The standard practice in deep RL -- and even continual RL -- is to assume unfettered access to deployment environment for the full lifetime of the agent. This paper explores the notion that progress in lifelong RL research has been held back by inappropriate empirical methodologies. In this paper we propose a new approach for tuning and evaluating lifelong RL agents where only one percent of the experiment data can be used for hyperparameter tuning. We then conduct an empirical study of DQN and Soft Actor Critic across a variety of continuing and non-stationary domains. We find both methods generally perform po
&lt;/p&gt;</description></item><item><title>ImageNot&#25968;&#25454;&#38598;&#19982;ImageNet&#24418;&#25104;&#23545;&#27604;&#65292;&#23637;&#31034;&#20102;&#20851;&#38190;&#27169;&#22411;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#25490;&#21517;&#19968;&#33268;&#24615;&#65292;&#20197;&#21450;&#23427;&#20204;&#30456;&#23545;&#20110;&#20808;&#21069;&#27169;&#22411;&#30340;&#25913;&#36827;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#20013;&#37117;&#20855;&#26377;&#24378;&#30456;&#20851;&#24615;&#65292;&#34920;&#26126;&#20102;ImageNot&#23545;&#20110;&#36801;&#31227;&#23398;&#20064;&#20855;&#26377;&#31867;&#20284;&#25928;&#29992;&#20110;ImageNet&#12290;</title><link>https://arxiv.org/abs/2404.02112</link><description>&lt;p&gt;
ImageNot&#65306;&#19982;ImageNet&#24418;&#25104;&#23545;&#27604;&#65292;&#20445;&#25345;&#27169;&#22411;&#25490;&#21517;
&lt;/p&gt;
&lt;p&gt;
ImageNot: A contrast with ImageNet preserves model rankings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02112
&lt;/p&gt;
&lt;p&gt;
ImageNot&#25968;&#25454;&#38598;&#19982;ImageNet&#24418;&#25104;&#23545;&#27604;&#65292;&#23637;&#31034;&#20102;&#20851;&#38190;&#27169;&#22411;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#25490;&#21517;&#19968;&#33268;&#24615;&#65292;&#20197;&#21450;&#23427;&#20204;&#30456;&#23545;&#20110;&#20808;&#21069;&#27169;&#22411;&#30340;&#25913;&#36827;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#20013;&#37117;&#20855;&#26377;&#24378;&#30456;&#20851;&#24615;&#65292;&#34920;&#26126;&#20102;ImageNot&#23545;&#20110;&#36801;&#31227;&#23398;&#20064;&#20855;&#26377;&#31867;&#20284;&#25928;&#29992;&#20110;ImageNet&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;ImageNot&#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#19982;ImageNet&#22312;&#35268;&#27169;&#19978;&#21305;&#37197;&#20294;&#22312;&#20854;&#20182;&#26041;&#38754;&#26377;&#30528;&#26174;&#33879;&#24046;&#24322;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22810;&#24180;&#26469;&#20026;ImageNet&#24320;&#21457;&#30340;&#20851;&#38190;&#27169;&#22411;&#26550;&#26500;&#22312;ImageNot&#19978;&#35757;&#32451;&#21644;&#35780;&#20272;&#26102;&#65292;&#20854;&#25490;&#21517;&#19982;&#23427;&#20204;&#22312;ImageNet&#19978;&#30340;&#25490;&#21517;&#23436;&#20840;&#30456;&#21516;&#12290;&#26080;&#35770;&#26159;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#27169;&#22411;&#36824;&#26159;&#24494;&#35843;&#27169;&#22411;&#65292;&#36825;&#19968;&#28857;&#37117;&#25104;&#31435;&#12290;&#27492;&#22806;&#65292;&#27599;&#20010;&#27169;&#22411;&#30456;&#23545;&#20110;&#20808;&#21069;&#27169;&#22411;&#30340;&#25913;&#36827;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#20013;&#37117;&#26377;&#24456;&#24378;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20379;&#35777;&#25454;&#34920;&#26126;&#65292;ImageNot&#22312;&#36801;&#31227;&#23398;&#20064;&#30446;&#30340;&#19978;&#20855;&#26377;&#31867;&#20284;&#30340;&#25928;&#29992;&#20110;ImageNet&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#23637;&#31034;&#20102;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30456;&#23545;&#24615;&#33021;&#30340;&#24778;&#20154;&#22806;&#37096;&#26377;&#25928;&#24615;&#12290;&#36825;&#19982;&#36890;&#24120;&#22312;&#25968;&#25454;&#38598;&#21457;&#29983;&#23567;&#21464;&#21270;&#26102;&#29978;&#33267;&#32454;&#24494;&#25913;&#21464;&#26102;&#32477;&#23545;&#20934;&#30830;&#24230;&#25968;&#23383;&#20250;&#24613;&#21095;&#19979;&#38477;&#24418;&#25104;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02112v1 Announce Type: new  Abstract: We introduce ImageNot, a dataset designed to match the scale of ImageNet while differing drastically in other aspects. We show that key model architectures developed for ImageNet over the years rank identically when trained and evaluated on ImageNot to how they rank on ImageNet. This is true when training models from scratch or fine-tuning them. Moreover, the relative improvements of each model over earlier models strongly correlate in both datasets. We further give evidence that ImageNot has a similar utility as ImageNet for transfer learning purposes. Our work demonstrates a surprising degree of external validity in the relative performance of image classification models. This stands in contrast with absolute accuracy numbers that typically drop sharply even under small changes to a dataset.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#31574;&#30053;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#20998;&#21035;&#21033;&#29992;&#38544;&#24335;&#26799;&#24230;&#20256;&#36755;&#21644;&#22522;&#20110;Hessian&#30340;&#25216;&#26415;&#65292;&#20998;&#21035;&#30830;&#20445;&#20102;$\tilde{\mathcal{O}}(T^{3/5})$&#21644;$\tilde{\mathcal{O}}(\sqrt{T})$&#25968;&#37327;&#32423;&#30340;&#26399;&#26395;&#21518;&#24724;&#12290;</title><link>https://arxiv.org/abs/2404.02108</link><description>&lt;p&gt;
&#36866;&#29992;&#20110;&#26080;&#38480;&#26102;&#22495;&#24179;&#22343;&#22870;&#21169;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#26041;&#24046;&#32553;&#20943;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Variance-Reduced Policy Gradient Approaches for Infinite Horizon Average Reward Markov Decision Processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02108
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#31574;&#30053;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#20998;&#21035;&#21033;&#29992;&#38544;&#24335;&#26799;&#24230;&#20256;&#36755;&#21644;&#22522;&#20110;Hessian&#30340;&#25216;&#26415;&#65292;&#20998;&#21035;&#30830;&#20445;&#20102;$\tilde{\mathcal{O}}(T^{3/5})$&#21644;$\tilde{\mathcal{O}}(\sqrt{T})$&#25968;&#37327;&#32423;&#30340;&#26399;&#26395;&#21518;&#24724;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#26080;&#38480;&#26102;&#22495;&#24179;&#22343;&#22870;&#21169;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#32972;&#26223;&#19979;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#31574;&#30053;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#19968;&#33324;&#21442;&#25968;&#21270;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#20351;&#29992;&#38544;&#24335;&#26799;&#24230;&#20256;&#36755;&#36827;&#34892;&#26041;&#24046;&#32553;&#20943;&#65292;&#30830;&#20445;&#26399;&#26395;&#21518;&#24724;&#30340;&#25968;&#37327;&#32423;&#20026;$\tilde{\mathcal{O}}(T^{3/5})$&#12290;&#31532;&#20108;&#31181;&#26041;&#27861;&#65292;&#26681;&#26893;&#20110;&#22522;&#20110;Hessian&#30340;&#25216;&#26415;&#65292;&#30830;&#20445;&#26399;&#26395;&#21518;&#24724;&#30340;&#25968;&#37327;&#32423;&#20026;$\tilde{\mathcal{O}}(\sqrt{T})$&#12290;&#36825;&#20123;&#32467;&#26524;&#26174;&#33879;&#25913;&#36827;&#20102;&#35813;&#38382;&#39064;&#30340;&#29616;&#26377;&#25216;&#26415;&#27700;&#24179;&#65292;&#20854;&#21518;&#24724;&#29575;&#20026;$\tilde{\mathcal{O}}(T^{3/4})&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02108v1 Announce Type: new  Abstract: We present two Policy Gradient-based methods with general parameterization in the context of infinite horizon average reward Markov Decision Processes. The first approach employs Implicit Gradient Transport for variance reduction, ensuring an expected regret of the order $\tilde{\mathcal{O}}(T^{3/5})$. The second approach, rooted in Hessian-based techniques, ensures an expected regret of the order $\tilde{\mathcal{O}}(\sqrt{T})$. These results significantly improve the state of the art of the problem, which achieves a regret of $\tilde{\mathcal{O}}(T^{3/4})$.
&lt;/p&gt;</description></item><item><title>&#26032;&#25512;&#20986;&#30340;Eurus&#27169;&#22411;&#36890;&#36807;&#22522;&#20110;&#39318;&#36873;&#26641;&#30340;&#25512;&#29702;&#20248;&#21270;&#65292;&#22312;&#22810;&#39033;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#19994;&#30028;&#39046;&#20808;&#30340;&#25104;&#26524;&#65292;&#23588;&#20854;&#22312;&#20987;&#36133;&#20102;GPT-3.5 Turbo&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#31361;&#20986;&#12290;</title><link>https://arxiv.org/abs/2404.02078</link><description>&lt;p&gt;
&#36890;&#36807;&#39318;&#36873;&#26641;&#25512;&#36827;LLM&#25512;&#29702;&#36890;&#29992;&#24615;
&lt;/p&gt;
&lt;p&gt;
Advancing LLM Reasoning Generalists with Preference Trees
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02078
&lt;/p&gt;
&lt;p&gt;
&#26032;&#25512;&#20986;&#30340;Eurus&#27169;&#22411;&#36890;&#36807;&#22522;&#20110;&#39318;&#36873;&#26641;&#30340;&#25512;&#29702;&#20248;&#21270;&#65292;&#22312;&#22810;&#39033;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#19994;&#30028;&#39046;&#20808;&#30340;&#25104;&#26524;&#65292;&#23588;&#20854;&#22312;&#20987;&#36133;&#20102;GPT-3.5 Turbo&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#31361;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Eurus&#65292;&#19968;&#22871;&#19987;&#20026;&#25512;&#29702;&#20248;&#21270;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;&#32463;&#36807;Mistral-7B&#21644;CodeLlama-70B&#30340;&#24494;&#35843;&#65292;Eurus&#27169;&#22411;&#22312;&#28085;&#30422;&#25968;&#23398;&#12289;&#20195;&#30721;&#29983;&#25104;&#21644;&#36923;&#36753;&#25512;&#29702;&#38382;&#39064;&#30340;&#22810;&#26679;&#21270;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#19994;&#30028;&#39046;&#20808;&#30340;&#25104;&#26524;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;Eurus-70B&#22312;&#36890;&#36807;&#28085;&#30422;&#20116;&#39033;&#20219;&#21153;&#30340;12&#20010;&#27979;&#35797;&#30340;&#20840;&#38754;&#22522;&#20934;&#27979;&#35797;&#20013;&#20987;&#36133;&#20102;GPT-3.5 Turbo&#65292;&#24182;&#22312;LeetCode&#21644;TheoremQA&#20004;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#20998;&#21035;&#23454;&#29616;&#20102;33.3%&#21644;32.6%&#30340;pass@1&#20934;&#30830;&#29575;&#65292;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#24320;&#28304;&#27169;&#22411;&#36229;&#36807;13.3%&#30340;&#36793;&#38469;&#12290;Eurus&#30340;&#24378;&#22823;&#24615;&#33021;&#20027;&#35201;&#24402;&#21151;&#20110;&#25105;&#20204;&#26032;&#35774;&#35745;&#30340;&#22823;&#35268;&#27169;&#12289;&#39640;&#36136;&#37327;&#23545;&#40784;&#25968;&#25454;&#38598;UltraInteract&#65292;&#35813;&#25968;&#25454;&#38598;&#19987;&#38376;&#20026;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#32780;&#35774;&#35745;&#12290;UltraInteract&#21487;&#29992;&#20110;&#30417;&#30563;&#24494;&#35843;&#21644;&#39318;&#36873;&#23398;&#20064;&#12290;&#23545;&#20110;&#27599;&#20010;&#25351;&#20196;&#65292;&#23427;&#21253;&#25324;&#19968;&#20010;&#39318;&#36873;&#26641;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02078v1 Announce Type: new  Abstract: We introduce Eurus, a suite of large language models (LLMs) optimized for reasoning. Finetuned from Mistral-7B and CodeLlama-70B, Eurus models achieve state-of-the-art results among open-source models on a diverse set of benchmarks covering mathematics, code generation, and logical reasoning problems. Notably, Eurus-70B beats GPT-3.5 Turbo in reasoning through a comprehensive benchmarking across 12 tests covering five tasks, and achieves a 33.3% pass@1 accuracy on LeetCode and 32.6% on TheoremQA, two challenging benchmarks, substantially outperforming existing open-source models by margins more than 13.3%. The strong performance of Eurus can be primarily attributed to UltraInteract, our newly-curated large-scale, high-quality alignment dataset specifically designed for complex reasoning tasks. UltraInteract can be used in both supervised fine-tuning and preference learning. For each instruction, it includes a preference tree consisting o
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;Transformer&#20013;&#25552;&#21462;&#22270;&#24418;&#20197;&#29992;&#20110;&#22330;&#26223;&#22270;&#29983;&#25104;&#30340;&#36731;&#37327;&#32423;&#21333;&#38454;&#27573;&#27169;&#22411;&#65292;&#26377;&#25928;&#22320;&#25552;&#21462;&#20102;&#20851;&#31995;&#22270;&#12290;</title><link>https://arxiv.org/abs/2404.02072</link><description>&lt;p&gt;
&#20174;Transformer&#20013;&#25552;&#21462;&#22270;&#24418;&#29992;&#20110;&#22330;&#26223;&#22270;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
EGTR: Extracting Graph from Transformer for Scene Graph Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02072
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;Transformer&#20013;&#25552;&#21462;&#22270;&#24418;&#20197;&#29992;&#20110;&#22330;&#26223;&#22270;&#29983;&#25104;&#30340;&#36731;&#37327;&#32423;&#21333;&#38454;&#27573;&#27169;&#22411;&#65292;&#26377;&#25928;&#22320;&#25552;&#21462;&#20102;&#20851;&#31995;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22330;&#26223;&#22270;&#29983;&#25104;&#65288;SGG&#65289;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#28041;&#21450;&#26816;&#27979;&#23545;&#35937;&#24182;&#39044;&#27979;&#23545;&#35937;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25552;&#20986;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#21333;&#38454;&#27573;SGG&#27169;&#22411;&#65292;&#23427;&#20174;DETR&#35299;&#30721;&#22120;&#30340;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#23618;&#20013;&#23398;&#20064;&#30340;&#21508;&#31181;&#20851;&#31995;&#20013;&#25552;&#21462;&#20851;&#31995;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02072v1 Announce Type: cross  Abstract: Scene Graph Generation (SGG) is a challenging task of detecting objects and predicting relationships between objects. After DETR was developed, one-stage SGG models based on a one-stage object detector have been actively studied. However, complex modeling is used to predict the relationship between objects, and the inherent relationship between object queries learned in the multi-head self-attention of the object detector has been neglected. We propose a lightweight one-stage SGG model that extracts the relation graph from the various relationships learned in the multi-head self-attention layers of the DETR decoder. By fully utilizing the self-attention by-products, the relation graph can be extracted effectively with a shallow relation extraction head. Considering the dependency of the relation extraction task on the object detection task, we propose a novel relation smoothing technique that adjusts the relation label adaptively accor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#31471;&#20998;&#20219;&#24847;&#27169;&#22411;&#36827;&#34892;&#20102;&#22810;&#26041;&#38754;&#30340;&#32418;&#38431;&#20998;&#26512;&#65292;&#21457;&#29616;&#27169;&#22411;&#22312;&#24212;&#23545;&#39118;&#26684;&#36801;&#31227;&#12289;&#38544;&#31169;&#25915;&#20987;&#21644;&#25932;&#23545;&#25915;&#20987;&#26041;&#38754;&#23384;&#22312;&#19968;&#20123;&#25361;&#25112;</title><link>https://arxiv.org/abs/2404.02067</link><description>&lt;p&gt;
&#32418;&#38431;&#27979;&#35797;&#31471;&#20998;&#20219;&#24847;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Red-Teaming Segment Anything Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02067
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#31471;&#20998;&#20219;&#24847;&#27169;&#22411;&#36827;&#34892;&#20102;&#22810;&#26041;&#38754;&#30340;&#32418;&#38431;&#20998;&#26512;&#65292;&#21457;&#29616;&#27169;&#22411;&#22312;&#24212;&#23545;&#39118;&#26684;&#36801;&#31227;&#12289;&#38544;&#31169;&#25915;&#20987;&#21644;&#25932;&#23545;&#25915;&#20987;&#26041;&#38754;&#23384;&#22312;&#19968;&#20123;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#37325;&#35201;&#30340;&#24037;&#20855;&#65292;&#36890;&#36807;&#22312;&#22823;&#37327;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#23545;&#29305;&#23450;&#24212;&#29992;&#36827;&#34892;&#24494;&#35843;&#65292;&#26469;&#35299;&#20915;&#35768;&#22810;&#22797;&#26434;&#20219;&#21153;&#12290;&#31471;&#20998;&#20219;&#24847;&#27169;&#22411;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#20998;&#21106;&#20219;&#21153;&#20013;&#31532;&#19968;&#20010;&#20063;&#26159;&#26368;&#30693;&#21517;&#30340;&#22522;&#30784;&#27169;&#22411;&#20043;&#19968;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#26041;&#38754;&#30340;&#32418;&#38431;&#20998;&#26512;&#65292;&#27979;&#35797;&#20102;&#31471;&#20998;&#20219;&#24847;&#27169;&#22411;&#22312;&#25361;&#25112;&#24615;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65306;&#65288;1&#65289;&#25105;&#20204;&#20998;&#26512;&#20102;&#39118;&#26684;&#36801;&#31227;&#23545;&#20998;&#21106;&#25513;&#27169;&#30340;&#24433;&#21709;&#65292;&#35777;&#26126;&#23558;&#36870;&#22659;&#22825;&#27668;&#26465;&#20214;&#21644;&#38632;&#28404;&#24212;&#29992;&#20110;&#22478;&#24066;&#36947;&#36335;&#20202;&#34920;&#30424;&#22270;&#20687;&#26174;&#33879;&#25197;&#26354;&#29983;&#25104;&#30340;&#25513;&#27169;&#12290;&#65288;2&#65289;&#25105;&#20204;&#19987;&#27880;&#20110;&#35780;&#20272;&#27169;&#22411;&#26159;&#21542;&#21487;&#20197;&#29992;&#20110;&#25915;&#20987;&#38544;&#31169;&#65292;&#22914;&#35782;&#21035;&#21517;&#20154;&#30340;&#38754;&#23380;&#65292;&#24182;&#26174;&#31034;&#20986;&#27169;&#22411;&#22312;&#36825;&#19968;&#20219;&#21153;&#19978;&#20855;&#26377;&#19968;&#20123;&#19981;&#33391;&#30693;&#35782;&#12290;&#65288;3&#65289;&#26368;&#21518;&#65292;&#25105;&#20204;&#26816;&#26597;&#27169;&#22411;&#22312;&#25991;&#26412;&#25552;&#31034;&#19979;&#23545;&#20998;&#21106;&#25513;&#27169;&#30340;&#25932;&#23545;&#25915;&#20987;&#26377;&#22810;&#24378;&#22823;&#12290;&#25105;&#20204;&#19981;&#20165;&#23637;&#31034;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02067v1 Announce Type: cross  Abstract: Foundation models have emerged as pivotal tools, tackling many complex tasks through pre-training on vast datasets and subsequent fine-tuning for specific applications. The Segment Anything Model is one of the first and most well-known foundation models for computer vision segmentation tasks. This work presents a multi-faceted red-teaming analysis that tests the Segment Anything Model against challenging tasks: (1) We analyze the impact of style transfer on segmentation masks, demonstrating that applying adverse weather conditions and raindrops to dashboard images of city roads significantly distorts generated masks. (2) We focus on assessing whether the model can be used for attacks on privacy, such as recognizing celebrities' faces, and show that the model possesses some undesired knowledge in this task. (3) Finally, we check how robust the model is to adversarial attacks on segmentation masks under text prompts. We not only show the
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#32858;&#28966;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25968;&#23383;&#36951;&#24536;&#65292;&#26088;&#22312;&#33719;&#24471;&#19968;&#31181;&#36951;&#24536;&#26041;&#27861;&#65292;&#33021;&#26377;&#25928;&#22320;&#28040;&#38500;&#27169;&#22411;&#20013;&#30340;&#19981;&#33391;&#30693;&#35782;&#25110;&#34892;&#20026;&#65292;&#21516;&#26102;&#20445;&#30041;&#21407;&#22987;&#27169;&#22411;&#22312;&#29702;&#24819;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#20855;&#26377;&#21487;&#20280;&#32553;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.02062</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25968;&#23383;&#36951;&#24536;&#65306;&#36951;&#24536;&#26041;&#27861;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Digital Forgetting in Large Language Models: A Survey of Unlearning Methods
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02062
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#32858;&#28966;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25968;&#23383;&#36951;&#24536;&#65292;&#26088;&#22312;&#33719;&#24471;&#19968;&#31181;&#36951;&#24536;&#26041;&#27861;&#65292;&#33021;&#26377;&#25928;&#22320;&#28040;&#38500;&#27169;&#22411;&#20013;&#30340;&#19981;&#33391;&#30693;&#35782;&#25110;&#34892;&#20026;&#65292;&#21516;&#26102;&#20445;&#30041;&#21407;&#22987;&#27169;&#22411;&#22312;&#29702;&#24819;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#20855;&#26377;&#21487;&#20280;&#32553;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#36951;&#24536;&#30340;&#30446;&#26631;&#26159;&#65292;&#22312;&#32473;&#23450;&#19968;&#20010;&#23384;&#22312;&#19981;&#33391;&#30693;&#35782;&#25110;&#34892;&#20026;&#30340;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#24471;&#21040;&#19968;&#20010;&#26032;&#27169;&#22411;&#65292;&#20854;&#20013;&#19981;&#20877;&#23384;&#22312;&#26816;&#27979;&#21040;&#30340;&#38382;&#39064;&#12290;&#36951;&#24536;&#30340;&#21160;&#26426;&#21253;&#25324;&#38544;&#31169;&#20445;&#25252;&#12289;&#29256;&#26435;&#20445;&#25252;&#12289;&#28040;&#38500;&#20559;&#35265;&#21644;&#27495;&#35270;&#20197;&#21450;&#39044;&#38450;&#26377;&#23475;&#20869;&#23481;&#29983;&#25104;&#12290;&#26377;&#25928;&#30340;&#25968;&#23383;&#36951;&#24536;&#24517;&#39035;&#26159;&#26377;&#25928;&#30340;&#65288;&#21363;&#26032;&#27169;&#22411;&#22810;&#20040;&#22909;&#22320;&#36951;&#24536;&#20102;&#19981;&#33391;&#30693;&#35782;/&#34892;&#20026;&#65289;&#65292;&#20445;&#30041;&#21407;&#22987;&#27169;&#22411;&#22312;&#29702;&#24819;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#20855;&#26377;&#21487;&#20280;&#32553;&#24615;&#65288;&#29305;&#21035;&#26159;&#36951;&#24536;&#24517;&#39035;&#27604;&#20165;&#37325;&#26032;&#35757;&#32451;&#35201;&#26377;&#25928;&#65292;&#20165;&#37325;&#26032;&#35757;&#32451;&#38656;&#35201;&#20445;&#30041;&#30340;&#20219;&#21153;/&#25968;&#25454;&#65289;&#12290;&#26412;&#32508;&#36848;&#32858;&#28966;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#36951;&#24536;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;LLMs&#30340;&#32972;&#26223;&#65292;&#21253;&#25324;&#23427;&#20204;&#30340;&#32452;&#25104;&#37096;&#20998;&#12289;LLMs&#30340;&#31867;&#22411;&#20197;&#21450;&#23427;&#20204;&#36890;&#24120;&#30340;&#35757;&#32451;&#27969;&#31243;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25551;&#36848;&#25968;&#23383;&#36951;&#24536;&#30340;&#21160;&#26426;&#12289;&#31867;&#22411;&#21644;&#26399;&#26395;&#30340;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02062v1 Announce Type: cross  Abstract: The objective of digital forgetting is, given a model with undesirable knowledge or behavior, obtain a new model where the detected issues are no longer present. The motivations for forgetting include privacy protection, copyright protection, elimination of biases and discrimination, and prevention of harmful content generation. Effective digital forgetting has to be effective (meaning how well the new model has forgotten the undesired knowledge/behavior), retain the performance of the original model on the desirable tasks, and be scalable (in particular forgetting has to be more efficient than retraining from scratch on just the tasks/data to be retained). This survey focuses on forgetting in large language models (LLMs). We first provide background on LLMs, including their components, the types of LLMs, and their usual training pipeline. Second, we describe the motivations, types, and desired properties of digital forgetting. Third, 
&lt;/p&gt;</description></item><item><title>fastprop&#26159;&#19968;&#31181;DeepQSPR&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#20998;&#23376;&#32423;&#25551;&#36848;&#31526;&#65292;&#22312;&#26497;&#22823;&#32553;&#30701;&#26102;&#38388;&#20869;&#65292;&#22312;&#22810;&#26679;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#24182;&#36229;&#36234;&#20102;&#23398;&#20064;&#34920;&#31034;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.02058</link><description>&lt;p&gt;
&#20855;&#26377;&#24555;&#36895;prop&#30340;&#21487;&#25512;&#24191;&#12289;&#24555;&#36895;&#21644;&#20934;&#30830;&#30340;DeepQSPR Part 1: &#26694;&#26550;&#21644;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Generalizable, Fast, and Accurate DeepQSPR with fastprop Part 1: Framework and Benchmarks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02058
&lt;/p&gt;
&lt;p&gt;
fastprop&#26159;&#19968;&#31181;DeepQSPR&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#20998;&#23376;&#32423;&#25551;&#36848;&#31526;&#65292;&#22312;&#26497;&#22823;&#32553;&#30701;&#26102;&#38388;&#20869;&#65292;&#22312;&#22810;&#26679;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#24182;&#36229;&#36234;&#20102;&#23398;&#20064;&#34920;&#31034;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#32467;&#26500;-&#24615;&#36136;&#20851;&#31995;&#30740;&#31350;&#26088;&#22312;&#23450;&#20041;&#20998;&#23376;&#32467;&#26500;&#19982;&#20219;&#24847;&#24863;&#20852;&#36259;&#30340;&#25968;&#37327;&#20043;&#38388;&#30340;&#26144;&#23556;&#12290;&#21382;&#21490;&#19978;&#65292;&#36825;&#26159;&#36890;&#36807;&#24320;&#21457;&#25551;&#36848;&#31526;&#26469;&#23454;&#29616;&#30340;&#65292;&#36825;&#38656;&#35201;&#26174;&#33879;&#30340;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#65292;&#24182;&#19988;&#38590;&#20197;&#27867;&#21270;&#12290;&#22240;&#27492;&#65292;&#35813;&#39046;&#22495;&#24050;&#32463;&#28436;&#21464;&#20026;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#65292;&#24182;&#36716;&#20026;&#20351;&#29992;&#39640;&#24230;&#21487;&#25512;&#24191;&#30340;&#23398;&#20064;&#34920;&#31034;&#12290;&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;fastprop&#65292;&#19968;&#31181;DeepQSPR&#26694;&#26550;&#65292;&#20351;&#29992;&#19968;&#32452;&#26126;&#26234;&#30340;&#20998;&#23376;&#32423;&#25551;&#36848;&#31526;&#65292;&#22312;&#26497;&#22823;&#32553;&#30701;&#30340;&#26102;&#38388;&#20869;&#28385;&#36275;&#24182;&#36229;&#36234;&#20102;&#22810;&#26679;&#25968;&#25454;&#38598;&#19978;&#23398;&#20064;&#34920;&#31034;&#30340;&#24615;&#33021;&#12290;fastprop&#21487;&#20197;&#22312;github&#19978;&#20813;&#36153;&#33719;&#21462;&#65292;&#32593;&#22336;&#20026;github.com/JacksonBurns/fastprop&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02058v1 Announce Type: new  Abstract: Quantitative Structure Property Relationship studies aim to define a mapping between molecular structure and arbitrary quantities of interest. This was historically accomplished via the development of descriptors which requires significant domain expertise and struggles to generalize. Thus the field has morphed into Molecular Property Prediction and been given over to learned representations which are highly generalizable. The paper introduces fastprop, a DeepQSPR framework which uses a cogent set of molecular level descriptors to meet and exceed the performance of learned representations on diverse datasets in dramatically less time. fastprop is freely available on github at github.com/JacksonBurns/fastprop.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#38024;&#23545;&#39044;&#35757;&#32451;&#35821;&#38899;&#32534;&#30721;&#22120;&#30340;&#22122;&#22768;&#36974;&#34109;&#25915;&#20987;&#65292;&#24182;&#28145;&#20837;&#25506;&#35752;&#20102;&#25915;&#20987;&#30340;&#31934;&#24230;&#25552;&#21319;&#21644;&#21453;&#21046;&#25514;&#26045;&#12290;</title><link>https://arxiv.org/abs/2404.02052</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#38899;&#27169;&#22411;&#30340;&#22122;&#22768;&#36974;&#34109;&#25915;&#20987;&#19982;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Noise Masking Attacks and Defenses for Pretrained Speech Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02052
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#38024;&#23545;&#39044;&#35757;&#32451;&#35821;&#38899;&#32534;&#30721;&#22120;&#30340;&#22122;&#22768;&#36974;&#34109;&#25915;&#20987;&#65292;&#24182;&#28145;&#20837;&#25506;&#35752;&#20102;&#25915;&#20987;&#30340;&#31934;&#24230;&#25552;&#21319;&#21644;&#21453;&#21046;&#25514;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#27169;&#22411;&#32463;&#24120;&#22312;&#25935;&#24863;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#20294;&#36825;&#21487;&#33021;&#23548;&#33268;&#28508;&#22312;&#30340;&#38544;&#31169;&#27844;&#38706;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;Amid&#31561;&#20154;&#20110;2022&#24180;&#25552;&#20986;&#30340;&#22122;&#22768;&#36974;&#34109;&#25915;&#20987;&#65292;&#36825;&#20123;&#25915;&#20987;&#38024;&#23545;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#35831;&#27714;&#19968;&#27573;&#37096;&#20998;&#34987;&#22122;&#22768;&#26367;&#25442;&#30340;&#35805;&#35821;&#30340;&#36716;&#24405;&#26469;&#23454;&#26045;&#25915;&#20987;&#12290;&#20182;&#20204;&#34920;&#26126;&#65292;&#24403;&#35757;&#32451;&#26102;&#30475;&#21040;&#20102;&#19968;&#26465;&#35760;&#24405;&#65292;&#27169;&#22411;&#23558;&#29992;&#20854;&#35760;&#24518;&#30340;&#25935;&#24863;&#36716;&#24405;&#36716;&#24405;&#36825;&#20010;&#24102;&#26377;&#22122;&#22768;&#30340;&#35760;&#24405;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#25915;&#20987;&#25193;&#23637;&#21040;&#39044;&#35757;&#32451;&#35821;&#38899;&#32534;&#30721;&#22120;&#65292;&#24182;&#19988;&#25105;&#20204;&#30340;&#26041;&#27861;&#24494;&#35843;&#32534;&#30721;&#22120;&#20197;&#29983;&#25104;&#19968;&#20010;ASR&#27169;&#22411;&#65292;&#28982;&#21518;&#22312;&#35813;&#27169;&#22411;&#19978;&#36827;&#34892;&#22122;&#22768;&#36974;&#34109;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20174;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#24674;&#22797;&#31169;&#20154;&#20449;&#24687;&#65292;&#23613;&#31649;&#27169;&#22411;&#22312;&#39044;&#35757;&#32451;&#26102;&#20174;&#26410;&#30475;&#36807;&#36716;&#24405;&#65281;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#25552;&#39640;&#36825;&#20123;&#25915;&#20987;&#30340;&#31934;&#24230;&#65292;&#24182;&#30740;&#31350;&#20102;&#19968;&#20123;&#38024;&#23545;&#25105;&#20204;&#25915;&#20987;&#30340;&#23545;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02052v1 Announce Type: new  Abstract: Speech models are often trained on sensitive data in order to improve model performance, leading to potential privacy leakage. Our work considers noise masking attacks, introduced by Amid et al. 2022, which attack automatic speech recognition (ASR) models by requesting a transcript of an utterance which is partially replaced with noise. They show that when a record has been seen at training time, the model will transcribe the noisy record with its memorized sensitive transcript. In our work, we extend these attacks beyond ASR models, to attack pretrained speech encoders. Our method fine-tunes the encoder to produce an ASR model, and then performs noise masking on this model, which we find recovers private information from the pretraining data, despite the model never having seen transcripts at pretraining time! We show how to improve the precision of these attacks and investigate a number of countermeasures to our attacks.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#37329;&#34701;&#20132;&#26131;&#25968;&#25454;&#36890;&#29992;&#34920;&#31034;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#26412;&#22320;&#12289;&#20840;&#23616;&#21644;&#22806;&#37096;&#35821;&#22659;&#65292;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#29983;&#25104;&#27169;&#22411;&#21644;&#25972;&#21512;&#22806;&#37096;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#26412;&#22320;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#36229;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.02047</link><description>&lt;p&gt;
&#37329;&#34701;&#20132;&#26131;&#25968;&#25454;&#30340;&#36890;&#29992;&#34920;&#31034;&#65306;&#34701;&#21512;&#26412;&#22320;&#12289;&#20840;&#23616;&#21644;&#22806;&#37096;&#35821;&#22659;
&lt;/p&gt;
&lt;p&gt;
Universal representations for financial transactional data: embracing local, global, and external contexts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02047
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#37329;&#34701;&#20132;&#26131;&#25968;&#25454;&#36890;&#29992;&#34920;&#31034;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#26412;&#22320;&#12289;&#20840;&#23616;&#21644;&#22806;&#37096;&#35821;&#22659;&#65292;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#29983;&#25104;&#27169;&#22411;&#21644;&#25972;&#21512;&#22806;&#37096;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#26412;&#22320;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#36229;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37329;&#34701;&#20132;&#26131;&#30340;&#26377;&#25928;&#22788;&#29702;&#23545;&#38134;&#34892;&#25968;&#25454;&#20998;&#26512;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#19968;&#39046;&#22495;&#20013;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#19987;&#27880;&#20110;&#20026;&#29420;&#31435;&#38382;&#39064;&#25552;&#20379;&#19987;&#38376;&#21270;&#35299;&#20915;&#26041;&#26696;&#65292;&#32780;&#19981;&#26159;&#26500;&#24314;&#36866;&#29992;&#20110;&#35768;&#22810;&#38382;&#39064;&#30340;&#36890;&#29992;&#34920;&#31034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#21508;&#31181;&#20225;&#19994;&#25361;&#25112;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#32771;&#34385;&#25968;&#25454;&#29305;&#23450;&#24615;&#30340;&#26032;&#39062;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#25972;&#21512;&#22806;&#37096;&#20449;&#24687;&#21040;&#23458;&#25143;&#34920;&#31034;&#30340;&#26041;&#24335;&#65292;&#20511;&#37492;&#20854;&#20182;&#23458;&#25143;&#34892;&#21160;&#30340;&#35265;&#35299;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20934;&#65292;&#25551;&#36848;&#20102;&#20840;&#29699;&#33539;&#22260;&#20869;&#30340;&#34920;&#31034;&#36136;&#37327;&#65292;&#28041;&#21450;&#25972;&#20010;&#20132;&#26131;&#21382;&#21490;&#65307;&#26412;&#22320;&#33539;&#22260;&#20869;&#65292;&#21453;&#26144;&#23458;&#25143;&#24403;&#21069;&#29366;&#24577;&#65307;&#21160;&#24577;&#33539;&#22260;&#20869;&#65292;&#25429;&#25417;&#34920;&#31034;&#38543;&#26102;&#38388;&#28436;&#21464;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#29983;&#25104;&#26041;&#27861;&#22312;&#26412;&#22320;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#23545;&#20110;&#19979;&#19968;&#20010;MCC&#39044;&#27979;&#20219;&#21153;&#30340;ROC-AUC&#25552;&#21319;&#39640;&#36798;14&#65285;&#65292;&#23545;&#20110;dow...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02047v1 Announce Type: cross  Abstract: Effective processing of financial transactions is essential for banking data analysis. However, in this domain, most methods focus on specialized solutions to stand-alone problems instead of constructing universal representations suitable for many problems. We present a representation learning framework that addresses diverse business challenges. We also suggest novel generative models that account for data specifics, and a way to integrate external information into a client's representation, leveraging insights from other customers' actions. Finally, we offer a benchmark, describing representation quality globally, concerning the entire transaction history; locally, reflecting the client's current state; and dynamically, capturing representation evolution over time. Our generative approach demonstrates superior performance in local tasks, with an increase in ROC-AUC of up to 14\% for the next MCC prediction task and up to 46\% for dow
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#21464;&#21387;&#22120;&#19982;&#26377;&#38480;&#20256;&#24863;&#22120;&#32852;&#31995;&#36215;&#26469;&#65292;&#25105;&#20204;&#21457;&#29616;&#23427;&#20204;&#21487;&#20197;&#34920;&#36798;&#20196;&#20154;&#24778;&#35766;&#30340;&#22823;&#31867;&#20256;&#24863;&#65292;&#36827;&#19968;&#27493;&#25193;&#23637;&#20102;RASP&#65292;&#25512;&#20986;&#20102;&#26032;&#30340;&#21464;&#20307;&#65292;&#24182;&#23637;&#31034;&#20102;&#25513;&#30721;&#24179;&#22343;&#22256;&#38590;&#27880;&#24847;&#21464;&#21387;&#22120;&#21487;&#20197;&#27169;&#25311;S-RASP.</title><link>https://arxiv.org/abs/2404.02040</link><description>&lt;p&gt;
&#21464;&#21387;&#22120;&#20316;&#20026;&#20256;&#24863;&#22120;
&lt;/p&gt;
&lt;p&gt;
Transformers as Transducers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02040
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#21464;&#21387;&#22120;&#19982;&#26377;&#38480;&#20256;&#24863;&#22120;&#32852;&#31995;&#36215;&#26469;&#65292;&#25105;&#20204;&#21457;&#29616;&#23427;&#20204;&#21487;&#20197;&#34920;&#36798;&#20196;&#20154;&#24778;&#35766;&#30340;&#22823;&#31867;&#20256;&#24863;&#65292;&#36827;&#19968;&#27493;&#25193;&#23637;&#20102;RASP&#65292;&#25512;&#20986;&#20102;&#26032;&#30340;&#21464;&#20307;&#65292;&#24182;&#23637;&#31034;&#20102;&#25513;&#30721;&#24179;&#22343;&#22256;&#38590;&#27880;&#24847;&#21464;&#21387;&#22120;&#21487;&#20197;&#27169;&#25311;S-RASP.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#23558;&#21464;&#21387;&#22120;&#19982;&#26377;&#38480;&#20256;&#24863;&#22120;&#32852;&#31995;&#36215;&#26469;&#65292;&#30740;&#31350;&#20102;&#21464;&#21387;&#22120;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#26144;&#23556;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#21487;&#20197;&#34920;&#36798;&#20196;&#20154;&#24778;&#35766;&#30340;&#22823;&#31867;&#20256;&#24863;&#12290;&#25105;&#20204;&#20351;&#29992;RASP&#30340;&#21464;&#20307;&#65292;&#36825;&#26159;&#19968;&#31181;&#26088;&#22312;&#24110;&#21161;&#20154;&#20204;&#8220;&#20687;&#21464;&#21387;&#22120;&#19968;&#26679;&#24605;&#32771;&#8221;&#30340;&#32534;&#31243;&#35821;&#35328;&#65292;&#20316;&#20026;&#20013;&#38388;&#34920;&#31034;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;&#24067;&#23572;&#21464;&#20307;B-RASP&#21040;&#24207;&#21015;&#21040;&#24207;&#21015;&#20989;&#25968;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#30830;&#20999;&#35745;&#31639;&#20102;&#19968;&#38454;&#26377;&#29702;&#20989;&#25968;&#65288;&#22914;&#23383;&#31526;&#20018;&#26059;&#36716;&#65289;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#25193;&#23637;&#12290;B-RASP[pos]&#20801;&#35768;&#22312;&#20301;&#32622;&#19978;&#36827;&#34892;&#35745;&#31639;&#65288;&#22914;&#22797;&#21046;&#23383;&#31526;&#20018;&#30340;&#21069;&#21322;&#37096;&#20998;&#65289;&#65292;&#24182;&#21253;&#21547;&#25152;&#26377;&#19968;&#38454;&#27491;&#21017;&#20989;&#25968;&#12290;S-RASP&#28155;&#21152;&#21069;&#32512;&#21644;&#65292;&#21487;&#20197;&#36827;&#34892;&#39069;&#22806;&#30340;&#31639;&#26415;&#25805;&#20316;&#65288;&#22914;&#23545;&#23383;&#31526;&#20018;&#27714;&#24179;&#26041;&#65289;&#65292;&#24182;&#21253;&#21547;&#25152;&#26377;&#19968;&#38454;&#22810;&#27491;&#21017;&#20989;&#25968;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25513;&#30721;&#24179;&#22343;&#22256;&#38590;&#27880;&#24847;&#21464;&#21387;&#22120;&#21487;&#20197;&#27169;&#25311;S-RASP&#12290;&#25105;&#20204;&#32467;&#26524;&#30340;&#19968;&#20010;&#25512;&#35770;&#26159;n...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02040v1 Announce Type: cross  Abstract: We study the sequence-to-sequence mapping capacity of transformers by relating them to finite transducers, and find that they can express surprisingly large classes of transductions. We do so using variants of RASP, a programming language designed to help people "think like transformers," as an intermediate representation. We extend the existing Boolean variant B-RASP to sequence-to-sequence functions and show that it computes exactly the first-order rational functions (such as string rotation). Then, we introduce two new extensions. B-RASP[pos] enables calculations on positions (such as copying the first half of a string) and contains all first-order regular functions. S-RASP adds prefix sum, which enables additional arithmetic operations (such as squaring a string) and contains all first-order polyregular functions. Finally, we show that masked average-hard attention transformers can simulate S-RASP. A corollary of our results is a n
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#20998;&#27573;&#33258;&#22238;&#24402;&#29983;&#25104;&#27169;&#22411;AUTODIFF&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#31181;&#21517;&#20026;conformal motif&#30340;&#26032;&#22411;&#20998;&#23376;&#32452;&#35013;&#31574;&#30053;&#21644;SE(3)-&#31561;&#21464;&#21367;&#31215;&#32593;&#32476;&#32534;&#30721;&#34507;&#30333;&#36136;-&#37197;&#20307;&#22797;&#21512;&#29289;&#30456;&#20114;&#20316;&#29992;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#35299;&#20915;&#32467;&#26500;&#22522;&#33647;&#29289;&#35774;&#35745;&#20013;&#30340;&#23616;&#37096;&#32467;&#26500;&#21644;&#26500;&#35937;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2404.02003</link><description>&lt;p&gt;
AUTODIFF&#65306;&#22522;&#20110;&#32467;&#26500;&#30340;&#33647;&#29289;&#35774;&#35745;&#30340;&#33258;&#22238;&#24402;&#25193;&#25955;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
AUTODIFF: Autoregressive Diffusion Modeling for Structure-based Drug Design
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02003
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#20998;&#27573;&#33258;&#22238;&#24402;&#29983;&#25104;&#27169;&#22411;AUTODIFF&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#31181;&#21517;&#20026;conformal motif&#30340;&#26032;&#22411;&#20998;&#23376;&#32452;&#35013;&#31574;&#30053;&#21644;SE(3)-&#31561;&#21464;&#21367;&#31215;&#32593;&#32476;&#32534;&#30721;&#34507;&#30333;&#36136;-&#37197;&#20307;&#22797;&#21512;&#29289;&#30456;&#20114;&#20316;&#29992;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#35299;&#20915;&#32467;&#26500;&#22522;&#33647;&#29289;&#35774;&#35745;&#20013;&#30340;&#23616;&#37096;&#32467;&#26500;&#21644;&#26500;&#35937;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#22522;&#33647;&#29289;&#35774;&#35745;&#65288;SBDD&#65289;&#26088;&#22312;&#29983;&#25104;&#33021;&#22815;&#32039;&#23494;&#32467;&#21512;&#38774;&#34507;&#30333;&#30340;&#20998;&#23376;&#65292;&#26159;&#33647;&#29289;&#21457;&#29616;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#20808;&#21069;&#30340;&#26041;&#27861;&#24050;&#32463;&#21462;&#24471;&#20102;&#21021;&#27493;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20173;&#28982;&#23384;&#22312;&#26080;&#25928;&#30340;&#23616;&#37096;&#32467;&#26500;&#25110;&#19981;&#29616;&#23454;&#30340;&#26500;&#35937;&#38382;&#39064;&#65292;&#36825;&#20027;&#35201;&#26159;&#30001;&#20110;&#38190;&#35282;&#25110;&#25197;&#36716;&#35282;&#24230;&#30340;&#20542;&#26012;&#19981;&#36275;&#36896;&#25104;&#30340;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AUTODIFF&#65292;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#20998;&#27573;&#33258;&#22238;&#24402;&#29983;&#25104;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;conformal motif&#30340;&#26032;&#22411;&#20998;&#23376;&#32452;&#35013;&#31574;&#30053;&#65292;&#39318;&#20808;&#20445;&#30041;&#20998;&#23376;&#30340;&#23616;&#37096;&#32467;&#26500;&#30340;&#26500;&#35937;&#65292;&#28982;&#21518;&#25105;&#20204;&#20351;&#29992;SE(3)-&#31561;&#21464;&#21367;&#31215;&#32593;&#32476;&#32534;&#30721;&#34507;&#30333;&#36136;-&#37197;&#20307;&#22797;&#21512;&#29289;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#36890;&#36807;&#25193;&#25955;&#24314;&#27169;&#36880;&#20010;motif&#29983;&#25104;&#20998;&#23376;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#32422;&#26463;&#29983;&#25104;&#20998;&#23376;&#30340;&#20998;&#23376;&#36136;&#37327;&#25913;&#36827;&#20102;SBDD&#30340;&#35780;&#20272;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02003v1 Announce Type: new  Abstract: Structure-based drug design (SBDD), which aims to generate molecules that can bind tightly to the target protein, is an essential problem in drug discovery, and previous approaches have achieved initial success. However, most existing methods still suffer from invalid local structure or unrealistic conformation issues, which are mainly due to the poor leaning of bond angles or torsional angles. To alleviate these problems, we propose AUTODIFF, a diffusion-based fragment-wise autoregressive generation model. Specifically, we design a novel molecule assembly strategy named conformal motif that preserves the conformation of local structures of molecules first, then we encode the interaction of the protein-ligand complex with an SE(3)-equivariant convolutional network and generate molecules motif-by-motif with diffusion modeling. In addition, we also improve the evaluation framework of SBDD by constraining the molecular weights of the genera
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#39318;&#20010;&#20165;&#22312;&#38750;&#27954;&#35821;&#38899;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#33258;&#30417;&#30563;&#22810;&#35821;&#35328;&#35821;&#38899;&#27169;&#22411;&#65292;&#30456;&#27604;&#20110;&#24120;&#35268;&#26041;&#27861;&#65292;&#26356;&#39640;&#25928;&#24182;&#22312;ASR&#21644;LID&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.02000</link><description>&lt;p&gt;
&#38750;&#27954;&#20013;&#24515;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#25216;&#26415;&#22312;&#25746;&#21704;&#25289;&#20197;&#21335;&#22320;&#21306;&#30340;&#22810;&#35821;&#35328;&#35821;&#38899;&#34920;&#24449;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Africa-Centric Self-Supervised Pre-Training for Multilingual Speech Representation in a Sub-Saharan Context
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02000
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#39318;&#20010;&#20165;&#22312;&#38750;&#27954;&#35821;&#38899;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#33258;&#30417;&#30563;&#22810;&#35821;&#35328;&#35821;&#38899;&#27169;&#22411;&#65292;&#30456;&#27604;&#20110;&#24120;&#35268;&#26041;&#27861;&#65292;&#26356;&#39640;&#25928;&#24182;&#22312;ASR&#21644;LID&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#20165;&#22312;&#38750;&#27954;&#35821;&#38899;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#33258;&#30417;&#30563;&#22810;&#35821;&#35328;&#35821;&#38899;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#20174;&#25746;&#21704;&#25289;&#20197;&#21335;&#38750;&#27954;&#22320;&#21306;&#35762;&#35805;&#30340;21&#31181;&#35821;&#35328;&#21644;&#26041;&#35328;&#20013;&#23398;&#20064;&#20102;&#36817;60,000&#23567;&#26102;&#30340;&#26410;&#26631;&#35760;&#35821;&#38899;&#29255;&#27573;&#12290;&#22312;FLEURS-102&#25968;&#25454;&#38598;&#30340;SSA&#23376;&#38598;&#19978;&#65292;&#25105;&#20204;&#22522;&#20110;HuBERT$_{base}$ (0.09B) &#26550;&#26500;&#30340;&#26041;&#27861;&#23637;&#29616;&#20986;&#20102;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#65292;&#19982;FLEURS&#22522;&#20934;&#25552;&#20986;&#30340;w2v-bert-51 (0.6B) &#39044;&#35757;&#32451;&#27169;&#22411;&#30456;&#27604;&#65292;&#22312;ASR&#19979;&#28216;&#20219;&#21153;&#20013;&#26356;&#21152;&#39640;&#25928;&#65292;&#20351;&#29992;&#30340;&#25968;&#25454;&#37327;&#23569;7&#20493;&#65292;&#21442;&#25968;&#23569;6&#20493;&#12290;&#27492;&#22806;&#65292;&#22312;LID&#19979;&#28216;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#29575;&#19978;&#36229;&#36807;FLEURS&#22522;&#32447;&#36229;&#36807;22%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02000v1 Announce Type: new  Abstract: We present the first self-supervised multilingual speech model trained exclusively on African speech. The model learned from nearly 60 000 hours of unlabeled speech segments in 21 languages and dialects spoken in sub-Saharan Africa. On the SSA subset of the FLEURS-102 dataset, our approach based on a HuBERT$_{base}$ (0.09B) architecture shows competitive results, for ASR downstream task, compared to the w2v-bert-51 (0.6B) pre-trained model proposed in the FLEURS benchmark, while being more efficient by using 7x less data and 6x less parameters. Furthermore, in the context of a LID downstream task, our approach outperforms FLEURS baselines accuracy by over 22\%.
&lt;/p&gt;</description></item><item><title>&#24378;&#21270;&#23398;&#20064;&#22312;&#36235;&#21270;&#26041;&#38754;&#30340;&#24212;&#29992;&#30740;&#31350;&#34920;&#26126;&#65292;&#26234;&#33021;&#20307;&#33021;&#22815;&#26681;&#25454;&#24067;&#26391;&#36816;&#21160;&#29289;&#29702;&#32422;&#26463;&#24555;&#36895;&#23454;&#29616;&#36235;&#21270;&#65292;&#21516;&#26102;&#21457;&#29616;&#26032;&#20852;&#31574;&#30053;&#30340;&#25928;&#29575;&#21644;&#26234;&#33021;&#20307;&#22823;&#23567;&#12289;&#28216;&#27891;&#36895;&#24230;&#30340;&#25910;&#25947;&#12290;</title><link>https://arxiv.org/abs/2404.01999</link><description>&lt;p&gt;
&#21033;&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#36235;&#21270;&#31574;&#30053;&#30340;&#20986;&#29616;
&lt;/p&gt;
&lt;p&gt;
Emergence of Chemotactic Strategies with Multi-Agent Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01999
&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#36235;&#21270;&#26041;&#38754;&#30340;&#24212;&#29992;&#30740;&#31350;&#34920;&#26126;&#65292;&#26234;&#33021;&#20307;&#33021;&#22815;&#26681;&#25454;&#24067;&#26391;&#36816;&#21160;&#29289;&#29702;&#32422;&#26463;&#24555;&#36895;&#23454;&#29616;&#36235;&#21270;&#65292;&#21516;&#26102;&#21457;&#29616;&#26032;&#20852;&#31574;&#30053;&#30340;&#25928;&#29575;&#21644;&#26234;&#33021;&#20307;&#22823;&#23567;&#12289;&#28216;&#27891;&#36895;&#24230;&#30340;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26159;&#19968;&#31181;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#20026;&#24494;&#22411;&#26426;&#22120;&#20154;&#32534;&#31243;&#30340;&#28789;&#27963;&#39640;&#25928;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#24403;&#24378;&#21270;&#23398;&#20064;&#34987;&#35757;&#32451;&#25191;&#34892;&#36235;&#21270;&#26102;&#65292;&#26159;&#21542;&#33021;&#21521;&#29983;&#29289;&#31995;&#32479;&#25552;&#20379;&#27934;&#23519;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25506;&#31350;&#20102;&#26234;&#33021;&#20307;&#22914;&#20309;&#22788;&#29702;&#20449;&#24687;&#20197;&#26397;&#21521;&#30446;&#26631;&#28216;&#21160;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#27169;&#25311;&#23454;&#39564;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#26234;&#33021;&#20307;&#24418;&#29366;&#12289;&#22823;&#23567;&#21644;&#28216;&#27891;&#36895;&#24230;&#65292;&#20197;&#30830;&#23450;&#29983;&#29289;&#28216;&#27891;&#32773;&#30340;&#29289;&#29702;&#32422;&#26463;&#65292;&#21363;&#24067;&#26391;&#36816;&#21160;&#65292;&#26159;&#21542;&#23548;&#33268;&#24378;&#21270;&#23398;&#20064;&#32773;&#30340;&#35757;&#32451;&#22833;&#36133;&#12290;&#25105;&#20204;&#21457;&#29616;RL&#26234;&#33021;&#20307;&#21487;&#20197;&#22312;&#29289;&#29702;&#21487;&#33021;&#24615;&#33539;&#22260;&#20869;&#25191;&#34892;&#36235;&#21270;&#65292;&#24182;&#19988;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#29978;&#33267;&#22312;&#20027;&#21160;&#28216;&#21160;&#21387;&#20498;&#38543;&#26426;&#29615;&#22659;&#20043;&#21069;&#23601;&#33021;&#25191;&#34892;&#36235;&#21270;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#26032;&#20852;&#25919;&#31574;&#30340;&#25928;&#29575;&#65292;&#24182;&#30830;&#23450;&#20102;&#26234;&#33021;&#20307;&#22823;&#23567;&#21644;&#28216;&#27891;&#36895;&#24230;&#30340;&#25910;&#25947;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#37319;&#29992;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01999v1 Announce Type: cross  Abstract: Reinforcement learning (RL) is a flexible and efficient method for programming micro-robots in complex environments. Here we investigate whether reinforcement learning can provide insights into biological systems when trained to perform chemotaxis. Namely, whether we can learn about how intelligent agents process given information in order to swim towards a target. We run simulations covering a range of agent shapes, sizes, and swim speeds to determine if the physical constraints on biological swimmers, namely Brownian motion, lead to regions where reinforcement learners' training fails. We find that the RL agents can perform chemotaxis as soon as it is physically possible and, in some cases, even before the active swimming overpowers the stochastic environment. We study the efficiency of the emergent policy and identify convergence in agent size and swim speeds. Finally, we study the strategy adopted by the reinforcement learning algo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21152;&#27861;&#22270;&#20687;&#20998;&#35299;&#25216;&#26415;&#65292;&#21033;&#29992;&#21453;&#20809;&#22240;&#23376;&#20998;&#35299;&#36827;&#34892;&#20302;&#20809;&#22686;&#24378;&#65292;&#26080;&#38656;&#37197;&#23545;&#25110;&#38750;&#37197;&#23545;&#30417;&#30563;&#35757;&#32451;&#65292;&#25552;&#39640;&#20102;&#20302;&#20809;&#22686;&#24378;&#24615;&#33021;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#26356;&#22909;&#30340;&#27867;&#21270;&#12290;</title><link>https://arxiv.org/abs/2404.01998</link><description>&lt;p&gt;
&#20302;&#20809;&#24378;&#24230;&#22686;&#24378;&#30340;&#21453;&#20809;&#22240;&#23376;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
Specularity Factorization for Low-Light Enhancement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01998
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21152;&#27861;&#22270;&#20687;&#20998;&#35299;&#25216;&#26415;&#65292;&#21033;&#29992;&#21453;&#20809;&#22240;&#23376;&#20998;&#35299;&#36827;&#34892;&#20302;&#20809;&#22686;&#24378;&#65292;&#26080;&#38656;&#37197;&#23545;&#25110;&#38750;&#37197;&#23545;&#30417;&#30563;&#35757;&#32451;&#65292;&#25552;&#39640;&#20102;&#20302;&#20809;&#22686;&#24378;&#24615;&#33021;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#26356;&#22909;&#30340;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21152;&#27861;&#22270;&#20687;&#20998;&#35299;&#25216;&#26415;&#65292;&#23558;&#22270;&#20687;&#30475;&#20316;&#30001;&#22810;&#20010;&#28508;&#22312;&#30340;&#21453;&#20809;&#32452;&#20214;&#32452;&#25104;&#65292;&#36825;&#20123;&#32452;&#20214;&#21487;&#20197;&#36890;&#36807;&#22312;&#20998;&#35299;&#36807;&#31243;&#20013;&#35843;&#21046;&#31232;&#30095;&#24615;&#26469;&#31616;&#21333;&#22320;&#36882;&#24402;&#20272;&#35745;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#39537;&#21160;&#30340;RSFNet&#36890;&#36807;&#23558;&#20248;&#21270;&#23637;&#24320;&#25104;&#32593;&#32476;&#23618;&#26469;&#20272;&#35745;&#36825;&#20123;&#22240;&#23376;&#65292;&#20165;&#38656;&#35201;&#23398;&#20064;&#23569;&#37327;&#26631;&#37327;&#12290;&#30001;&#35774;&#35745;&#21487;&#35299;&#37322;&#30340;&#32467;&#26524;&#22240;&#23376;&#21487;&#20197;&#36890;&#36807;&#32593;&#32476;&#34701;&#21512;&#29992;&#20110;&#19981;&#21516;&#30340;&#22270;&#20687;&#22686;&#24378;&#20219;&#21153;&#65292;&#25110;&#20197;&#29992;&#25143;&#21487;&#25511;&#30340;&#26041;&#24335;&#30452;&#25509;&#36827;&#34892;&#32452;&#21512;&#12290;&#22522;&#20110;RSFNet&#65292;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#19968;&#20010;&#26080;&#37197;&#23545;&#25110;&#38750;&#37197;&#23545;&#30417;&#30563;&#35757;&#32451;&#30340;&#38646;&#21442;&#32771;&#20302;&#20809;&#22686;&#24378;&#65288;LLE&#65289;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#25552;&#39640;&#20102;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#30340;&#26368;&#26032;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#20010;&#20854;&#20182;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#27867;&#21270;&#12290;&#25105;&#20204;&#36824;&#23558;&#25105;&#20204;&#30340;&#22240;&#23376;&#19982;&#20854;&#20182;&#20219;&#21153;&#29305;&#23450;&#30340;&#34701;&#21512;&#32593;&#32476;&#38598;&#25104;&#65292;&#29992;&#20110;&#21435;&#38632;&#12289;&#21435;&#27169;&#31946;&#21644;&#21435;&#38654;&#31561;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01998v1 Announce Type: cross  Abstract: We present a new additive image factorization technique that treats images to be composed of multiple latent specular components which can be simply estimated recursively by modulating the sparsity during decomposition. Our model-driven {\em RSFNet} estimates these factors by unrolling the optimization into network layers requiring only a few scalars to be learned. The resultant factors are interpretable by design and can be fused for different image enhancement tasks via a network or combined directly by the user in a controllable fashion. Based on RSFNet, we detail a zero-reference Low Light Enhancement (LLE) application trained without paired or unpaired supervision. Our system improves the state-of-the-art performance on standard benchmarks and achieves better generalization on multiple other datasets. We also integrate our factors with other task specific fusion networks for applications like deraining, deblurring and dehazing wit
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36328;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#23454;&#29616;&#30340;DELAN&#26694;&#26550;&#25552;&#20986;&#20102;&#21452;&#23618;&#23545;&#40784;&#26041;&#27861;&#65292;&#20197;&#25552;&#21319;&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;&#20013;&#30340;&#36328;&#27169;&#24577;&#20132;&#20114;&#21644;&#34892;&#21160;&#20915;&#31574;&#12290;</title><link>https://arxiv.org/abs/2404.01994</link><description>&lt;p&gt;
DELAN: &#36890;&#36807;&#36328;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#23454;&#29616;&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;&#30340;&#21452;&#23618;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
DELAN: Dual-Level Alignment for Vision-and-Language Navigation by Cross-Modal Contrastive Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01994
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36328;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#23454;&#29616;&#30340;DELAN&#26694;&#26550;&#25552;&#20986;&#20102;&#21452;&#23618;&#23545;&#40784;&#26041;&#27861;&#65292;&#20197;&#25552;&#21319;&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;&#20013;&#30340;&#36328;&#27169;&#24577;&#20132;&#20114;&#21644;&#34892;&#21160;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01994v1 &#20844;&#21578;&#31867;&#22411;: &#20132;&#21449;  &#25688;&#35201;: &#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;(VLN)&#35201;&#27714;&#20195;&#29702;&#26681;&#25454;&#33258;&#28982;&#35821;&#35328;&#25351;&#31034;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#23548;&#33322;&#12290;&#20026;&#20102;&#23436;&#25104;&#20219;&#21153;&#65292;&#20195;&#29702;&#38656;&#35201;&#23545;&#40784;&#21644;&#25972;&#21512;&#21508;&#31181;&#23548;&#33322;&#27169;&#24577;&#65292;&#21253;&#25324;&#25351;&#31034;&#12289;&#35266;&#23519;&#21644;&#23548;&#33322;&#21382;&#21490;&#12290;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#34701;&#21512;&#38454;&#27573;&#30340;&#36328;&#27169;&#24577;&#27880;&#24847;&#21147;&#65292;&#20197;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#30001;&#19981;&#21516;&#21333;&#19968;&#32534;&#30721;&#22120;&#29983;&#25104;&#30340;&#27169;&#24577;&#29305;&#24449;&#20301;&#20110;&#21508;&#33258;&#30340;&#31354;&#38388;&#20013;&#65292;&#23548;&#33268;&#36328;&#27169;&#24577;&#34701;&#21512;&#21644;&#20915;&#31574;&#36136;&#37327;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#36328;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#23454;&#29616;&#30340;Dual-levEL AligNment (DELAN)&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#26088;&#22312;&#22312;&#34701;&#21512;&#20043;&#21069;&#23545;&#40784;&#21508;&#31181;&#19982;&#23548;&#33322;&#30456;&#20851;&#30340;&#27169;&#24577;&#65292;&#20174;&#32780;&#22686;&#24378;&#36328;&#27169;&#24577;&#20132;&#20114;&#21644;&#34892;&#21160;&#20915;&#31574;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#39044;&#34701;&#21512;&#23545;&#40784;&#20998;&#20026;&#20004;&#20010;&#23618;&#27425;: &#25351;&#31034;-&#21382;&#21490;&#23618;&#21644;&#22320;&#26631;-&#35266;&#23519;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01994v1 Announce Type: cross  Abstract: Vision-and-Language navigation (VLN) requires an agent to navigate in unseen environment by following natural language instruction. For task completion, the agent needs to align and integrate various navigation modalities, including instruction, observation and navigation history. Existing works primarily concentrate on cross-modal attention at the fusion stage to achieve this objective. Nevertheless, modality features generated by disparate uni-encoders reside in their own spaces, leading to a decline in the quality of cross-modal fusion and decision. To address this problem, we propose a Dual-levEL AligNment (DELAN) framework by cross-modal contrastive learning. This framework is designed to align various navigation-related modalities before fusion, thereby enhancing cross-modal interaction and action decision-making. Specifically, we divide the pre-fusion alignment into dual levels: instruction-history level and landmark-observation
&lt;/p&gt;</description></item><item><title>&#20957;&#35270;&#32447;&#32034;&#30340;&#26377;&#25928;&#21033;&#29992;&#26174;&#33879;&#25552;&#39640;&#20102;&#26381;&#21153;&#26426;&#22120;&#20154;&#24863;&#30693;&#29992;&#25143;&#20114;&#21160;&#24847;&#22270;&#30340;&#24615;&#33021;&#65292;&#20998;&#31867;&#20934;&#30830;&#24615;&#21644;&#36866;&#24212;&#26032;&#29615;&#22659;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.01986</link><description>&lt;p&gt;
&#39044;&#27979;&#19982;&#26381;&#21153;&#26426;&#22120;&#20154;&#20114;&#21160;&#24847;&#22270;&#65306;&#20957;&#35270;&#32447;&#32034;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Predicting the Intention to Interact with a Service Robot:the Role of Gaze Cues
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01986
&lt;/p&gt;
&lt;p&gt;
&#20957;&#35270;&#32447;&#32034;&#30340;&#26377;&#25928;&#21033;&#29992;&#26174;&#33879;&#25552;&#39640;&#20102;&#26381;&#21153;&#26426;&#22120;&#20154;&#24863;&#30693;&#29992;&#25143;&#20114;&#21160;&#24847;&#22270;&#30340;&#24615;&#33021;&#65292;&#20998;&#31867;&#20934;&#30830;&#24615;&#21644;&#36866;&#24212;&#26032;&#29615;&#22659;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#26381;&#21153;&#26426;&#22120;&#20154;&#26469;&#35828;&#65292;&#23613;&#24555;&#24863;&#30693;&#19968;&#20010;&#25509;&#36817;&#30340;&#20154;&#26159;&#21542;&#26377;&#24847;&#20114;&#21160;&#33267;&#20851;&#37325;&#35201;&#65306;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#23427;&#21487;&#20197;&#20027;&#21160;&#37319;&#21462;&#21451;&#22909;&#34892;&#20026;&#65292;&#20174;&#32780;&#25552;&#39640;&#29992;&#25143;&#20307;&#39564;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#28508;&#22312;&#29992;&#25143;&#20114;&#21160;&#24847;&#22270;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#20998;&#31867;&#22120;&#26469;&#35299;&#20915;&#36825;&#20010;&#24863;&#30693;&#20219;&#21153;&#65292;&#21487;&#20197;&#20197;&#33258;&#30417;&#30563;&#26041;&#24335;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#22312;&#20110;&#30740;&#31350;&#22312;&#36825;&#19968;&#32972;&#26223;&#19979;&#20195;&#34920;&#20010;&#20154;&#20957;&#35270;&#30340;&#29305;&#24449;&#30340;&#30410;&#22788;&#12290;&#23545;&#26032;&#39062;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#20957;&#35270;&#32447;&#32034;&#30340;&#21253;&#21547;&#26174;&#33879;&#25913;&#21892;&#20102;&#20998;&#31867;&#22120;&#24615;&#33021;&#65288;AUROC&#20174;84.5%&#25552;&#39640;&#21040;91.2%&#65289;&#65307;&#21487;&#20197;&#23454;&#29616;&#20934;&#30830;&#20998;&#31867;&#30340;&#36317;&#31163;&#20174;2.4&#31859;&#25552;&#39640;&#21040;3.2&#31859;&#12290;&#25105;&#20204;&#36824;&#37327;&#21270;&#20102;&#31995;&#32479;&#22312;&#27809;&#26377;&#22806;&#37096;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#36866;&#24212;&#26032;&#29615;&#22659;&#30340;&#33021;&#21147;&#12290;&#23450;&#24615;&#23454;&#39564;&#23637;&#31034;&#20102;&#26381;&#21153;&#21592;&#26426;&#22120;&#20154;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01986v1 Announce Type: cross  Abstract: For a service robot, it is crucial to perceive as early as possible that an approaching person intends to interact: in this case, it can proactively enact friendly behaviors that lead to an improved user experience. We solve this perception task with a sequence-to-sequence classifier of a potential user intention to interact, which can be trained in a self-supervised way. Our main contribution is a study of the benefit of features representing the person's gaze in this context. Extensive experiments on a novel dataset show that the inclusion of gaze cues significantly improves the classifier performance (AUROC increases from 84.5% to 91.2%); the distance at which an accurate classification can be achieved improves from 2.4 m to 3.2 m. We also quantify the system's ability to adapt to new environments without external supervision. Qualitative experiments show practical applications with a waiter robot.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#22312;&#20020;&#24202;&#35797;&#39564;&#20013;&#21033;&#29992;&#24739;&#32773;&#35821;&#38899;&#25968;&#25454;&#36827;&#34892;&#22810;&#35821;&#35328;&#35828;&#35805;&#32773;&#39564;&#35777;&#65292;&#20197;&#24212;&#23545;&#36523;&#20221;&#39564;&#35777;&#21644;&#25490;&#38500;&#37325;&#22797;&#20837;&#32452;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2404.01981</link><description>&lt;p&gt;
&#20020;&#24202;&#35797;&#39564;&#20013;&#30340;&#38646;&#26679;&#26412;&#22810;&#35821;&#35328;&#35828;&#35805;&#32773;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Multi-Lingual Speaker Verification in Clinical Trials
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01981
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#22312;&#20020;&#24202;&#35797;&#39564;&#20013;&#21033;&#29992;&#24739;&#32773;&#35821;&#38899;&#25968;&#25454;&#36827;&#34892;&#22810;&#35821;&#35328;&#35828;&#35805;&#32773;&#39564;&#35777;&#65292;&#20197;&#24212;&#23545;&#36523;&#20221;&#39564;&#35777;&#21644;&#25490;&#38500;&#37325;&#22797;&#20837;&#32452;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20020;&#24202;&#35797;&#39564;&#28041;&#21450;&#22823;&#37327;&#20020;&#24202;&#21307;&#29983;&#12289;&#24739;&#32773;&#21644;&#25968;&#25454;&#25910;&#38598;&#29615;&#22659;&#65292;&#33719;&#21462;&#39640;&#36136;&#37327;&#25968;&#25454;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#20020;&#24202;&#35797;&#39564;&#20013;&#65292;&#24739;&#32773;&#26681;&#25454;&#20854;&#35821;&#38899;&#25968;&#25454;&#36827;&#34892;&#35780;&#20272;&#65292;&#20197;&#26816;&#27979;&#21644;&#30417;&#27979;&#35748;&#30693;&#21644;&#24515;&#29702;&#20581;&#24247;&#38556;&#30861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#36825;&#20123;&#35821;&#38899;&#24405;&#38899;&#26469;&#39564;&#35777;&#24050;&#20837;&#32452;&#24739;&#32773;&#30340;&#36523;&#20221;&#65292;&#24182;&#35782;&#21035;&#21644;&#25490;&#38500;&#35797;&#22270;&#22810;&#27425;&#20837;&#32452;&#21516;&#19968;&#35797;&#39564;&#30340;&#20010;&#20307;&#12290;&#30001;&#20110;&#20020;&#24202;&#30740;&#31350;&#32463;&#24120;&#36328;&#36234;&#19981;&#21516;&#22269;&#23478;&#36827;&#34892;&#65292;&#22240;&#27492;&#24517;&#39035;&#21019;&#24314;&#19968;&#20010;&#33021;&#22815;&#22312;&#19981;&#21516;&#35821;&#35328;&#20013;&#25191;&#34892;&#35828;&#35805;&#32773;&#39564;&#35777;&#30340;&#31995;&#32479;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#24320;&#21457;&#24037;&#20316;&#12290;&#25105;&#20204;&#36890;&#36807;&#25307;&#21215;&#21644;&#27979;&#35797;&#35762;&#33521;&#35821;&#12289;&#24503;&#35821;&#12289;&#20025;&#40614;&#35821;&#12289;&#35199;&#29677;&#29273;&#35821;&#21644;&#38463;&#25289;&#20271;&#35821;&#30340;&#35821;&#38899;&#21463;&#25439;&#24739;&#32773;&#26469;&#35780;&#20272;&#39044;&#35757;&#32451;&#30340;TitaNet&#12289;ECAPA-TDNN&#21644;SpeakerNet&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#32463;&#36807;&#27979;&#35797;&#30340;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01981v1 Announce Type: new  Abstract: Due to the substantial number of clinicians, patients, and data collection environments involved in clinical trials, gathering data of superior quality poses a significant challenge. In clinical trials, patients are assessed based on their speech data to detect and monitor cognitive and mental health disorders. We propose using these speech recordings to verify the identities of enrolled patients and identify and exclude the individuals who try to enroll multiple times in the same trial. Since clinical studies are often conducted across different countries, creating a system that can perform speaker verification in diverse languages without additional development effort is imperative. We evaluate pre-trained TitaNet, ECAPA-TDNN, and SpeakerNet models by enrolling and testing with speech-impaired patients speaking English, German, Danish, Spanish, and Arabic languages. Our results demonstrate that tested models can effectively generalize 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#20219;&#21153;&#27491;&#21017;&#21270;&#65288;JTR&#65289;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#21333;&#20010;&#32852;&#21512;&#20219;&#21153;&#28508;&#22312;&#31354;&#38388;&#20013;&#21516;&#26102;&#23545;&#25152;&#26377;&#20219;&#21153;&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;&#25913;&#21892;&#20102;&#24403;&#25968;&#25454;&#26410;&#23436;&#20840;&#26631;&#35760;&#25152;&#26377;&#20219;&#21153;&#26102;&#30340;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2404.01976</link><description>&lt;p&gt;
&#32852;&#21512;&#20219;&#21153;&#27491;&#21017;&#21270;&#29992;&#20110;&#37096;&#20998;&#26631;&#35760;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Joint-Task Regularization for Partially Labeled Multi-Task Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01976
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#20219;&#21153;&#27491;&#21017;&#21270;&#65288;JTR&#65289;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#21333;&#20010;&#32852;&#21512;&#20219;&#21153;&#28508;&#22312;&#31354;&#38388;&#20013;&#21516;&#26102;&#23545;&#25152;&#26377;&#20219;&#21153;&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;&#25913;&#21892;&#20102;&#24403;&#25968;&#25454;&#26410;&#23436;&#20840;&#26631;&#35760;&#25152;&#26377;&#20219;&#21153;&#26102;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#21464;&#24471;&#36234;&#26469;&#36234;&#27969;&#34892;&#65292;&#20294;&#20854;&#23454;&#29992;&#24615;&#21463;&#21040;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#38598;&#30340;&#38480;&#21046;&#12290;&#22823;&#22810;&#25968;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#20381;&#36182;&#23436;&#20840;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#27599;&#20010;&#36755;&#20837;&#31034;&#20363;&#37117;&#38468;&#24102;&#25152;&#26377;&#30446;&#26631;&#20219;&#21153;&#30340;&#22320;&#38754;&#30495;&#30456;&#26631;&#31614;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#31579;&#36873;&#36825;&#26679;&#30340;&#25968;&#25454;&#38598;&#21487;&#33021;&#20250;&#22240;&#20026;&#26114;&#36149;&#19988;&#19981;&#20999;&#23454;&#38469;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#38656;&#35201;&#27599;&#20010;&#22270;&#20687;&#30340;&#27599;&#20010;&#20687;&#32032;&#26631;&#31614;&#30340;&#23494;&#38598;&#39044;&#27979;&#20219;&#21153;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32852;&#21512;&#20219;&#21153;&#27491;&#21017;&#21270;&#65288;JTR&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#30452;&#35266;&#30340;&#25216;&#26415;&#65292;&#21033;&#29992;&#36328;&#20219;&#21153;&#20851;&#31995;&#22312;&#21333;&#20010;&#32852;&#21512;&#20219;&#21153;&#28508;&#22312;&#31354;&#38388;&#20013;&#21516;&#26102;&#23545;&#25152;&#26377;&#20219;&#21153;&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;&#20197;&#25913;&#21892;&#22312;&#25968;&#25454;&#26410;&#23436;&#20840;&#26631;&#35760;&#25152;&#26377;&#20219;&#21153;&#26102;&#30340;&#23398;&#20064;&#12290;JTR&#19982;&#29616;&#26377;&#26041;&#27861;&#26377;&#25152;&#19981;&#21516;&#20043;&#22788;&#22312;&#20110;&#23427;&#21516;&#26102;&#23545;&#25152;&#26377;&#20219;&#21153;&#36827;&#34892;&#27491;&#21017;&#21270;&#32780;&#19981;&#26159;&#20998;&#21035;&#23545;&#27599;&#23545;&#20219;&#21153;&#36827;&#34892;&#65292;&#22240;&#27492;&#23454;&#29616;&#20102;&#30456;&#23545;&#20110;&#20219;&#21153;&#25968;&#37327;&#30340;&#32447;&#24615;&#22797;&#26434;&#24230;&#65292;&#32780;&#20197;&#21069;&#30340;&#26041;&#27861;&#27809;&#26377;&#20570;&#21040;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01976v1 Announce Type: cross  Abstract: Multi-task learning has become increasingly popular in the machine learning field, but its practicality is hindered by the need for large, labeled datasets. Most multi-task learning methods depend on fully labeled datasets wherein each input example is accompanied by ground-truth labels for all target tasks. Unfortunately, curating such datasets can be prohibitively expensive and impractical, especially for dense prediction tasks which require per-pixel labels for each image. With this in mind, we propose Joint-Task Regularization (JTR), an intuitive technique which leverages cross-task relations to simultaneously regularize all tasks in a single joint-task latent space to improve learning when data is not fully labeled for all tasks. JTR stands out from existing approaches in that it regularizes all tasks jointly rather than separately in pairs -- therefore, it achieves linear complexity relative to the number of tasks while previous 
&lt;/p&gt;</description></item><item><title>DSGNN &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21452;&#35270;&#22270;&#36229;&#32593;&#26684;&#24863;&#30693;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#20174;&#21355;&#26143;&#25968;&#25454;&#21644;&#27668;&#35937;&#25968;&#25454;&#20013;&#27169;&#25311;&#36828;&#36317;&#31163;&#32593;&#26684;&#21306;&#22495;&#30340;&#31354;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#20197;&#23454;&#29616;&#21306;&#22495;&#31354;&#27668;&#36136;&#37327;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2404.01975</link><description>&lt;p&gt;
DSGNN: &#19968;&#31181;&#29992;&#20110;&#21306;&#22495;&#31354;&#27668;&#36136;&#37327;&#20272;&#35745;&#30340;&#21452;&#35270;&#22270;&#36229;&#32593;&#26684;&#24863;&#30693;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
DSGNN: A Dual-View Supergrid-Aware Graph Neural Network for Regional Air Quality Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01975
&lt;/p&gt;
&lt;p&gt;
DSGNN &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21452;&#35270;&#22270;&#36229;&#32593;&#26684;&#24863;&#30693;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#20174;&#21355;&#26143;&#25968;&#25454;&#21644;&#27668;&#35937;&#25968;&#25454;&#20013;&#27169;&#25311;&#36828;&#36317;&#31163;&#32593;&#26684;&#21306;&#22495;&#30340;&#31354;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#20197;&#23454;&#29616;&#21306;&#22495;&#31354;&#27668;&#36136;&#37327;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#27668;&#36136;&#37327;&#20272;&#35745;&#21487;&#20197;&#20026;&#27809;&#26377;&#31354;&#27668;&#36136;&#37327;&#30417;&#27979;&#31449;&#30340;&#30446;&#26631;&#21306;&#22495;&#25552;&#20379;&#31354;&#27668;&#36136;&#37327;&#65292;&#23545;&#20844;&#20247;&#26377;&#29992;&#12290; &#24050;&#26377;&#30340;&#31354;&#27668;&#36136;&#37327;&#20272;&#35745;&#26041;&#27861;&#23558;&#30740;&#31350;&#21306;&#22495;&#21010;&#20998;&#20026;&#19981;&#30456;&#20132;&#30340;&#32593;&#26684;&#21306;&#22495;&#65292;&#24182;&#24212;&#29992;&#20108;&#32500;&#21367;&#31215;&#26469;&#26681;&#25454;&#22320;&#29702;&#23398;&#30340;&#31532;&#19968;&#23450;&#24459;&#27169;&#25311;&#30456;&#37051;&#32593;&#26684;&#21306;&#22495;&#30340;&#31354;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#20294;&#26410;&#33021;&#27169;&#25311;&#36828;&#36317;&#31163;&#32593;&#26684;&#21306;&#22495;&#30340;&#31354;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290; &#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21306;&#22495;&#31354;&#27668;&#36136;&#37327;&#20272;&#35745;&#30340;&#21452;&#35270;&#22270;&#36229;&#32593;&#26684;&#24863;&#30693;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;DSGNN&#65289;&#65292;&#21487;&#20197;&#20174;&#21452;&#35270;&#22270;&#65288;&#21363;&#65292;&#21355;&#26143;&#33719;&#21462;&#30340;&#27668;&#28342;&#33014;&#20809;&#23398;&#21402;&#24230;&#65288;AOD&#65289;&#21644;&#27668;&#35937;&#23398;&#65289;&#27169;&#25311;&#36828;&#36317;&#31163;&#32593;&#26684;&#21306;&#22495;&#30340;&#31354;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290; &#20855;&#20307;&#26469;&#35828;&#65292;&#21033;&#29992;&#22270;&#20687;&#26469;&#34920;&#31034;&#21306;&#22495;&#25968;&#25454;&#65288;&#21363;&#65292;AOD&#25968;&#25454;&#21644;&#27668;&#35937;&#25968;&#25454;&#65289;&#12290; &#24341;&#20837;&#21452;&#35270;&#22270;&#36229;&#32593;&#26684;&#23398;&#20064;&#27169;&#22359;&#20197;&#21442;&#25968;&#21270;&#26041;&#24335;&#29983;&#25104;&#36229;&#32593;&#26684;&#12290; &#22522;&#20110;&#21452;&#35270;&#22270;&#36229;&#32593;&#26684;&#65292;&#21452;&#35270;&#22270;&#38544;&#24615;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01975v1 Announce Type: new  Abstract: Air quality estimation can provide air quality for target regions without air quality stations, which is useful for the public. Existing air quality estimation methods divide the study area into disjointed grid regions, and apply 2D convolution to model the spatial dependencies of adjacent grid regions based on the first law of geography, failing to model the spatial dependencies of distant grid regions. To this end, we propose a Dual-view Supergrid-aware Graph Neural Network (DSGNN) for regional air quality estimation, which can model the spatial dependencies of distant grid regions from dual views (i.e., satellite-derived aerosol optical depth (AOD) and meteorology). Specifically, images are utilized to represent the regional data (i.e., AOD data and meteorology data). The dual-view supergrid learning module is introduced to generate supergrids in a parameterized way. Based on the dual-view supergrids, the dual-view implicit correlatio
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;AutoML&#25216;&#26415;&#26368;&#22823;&#21270;Deep Shift&#31070;&#32463;&#32593;&#32476;&#24615;&#33021;&#24182;&#26368;&#23567;&#21270;&#36164;&#28304;&#28040;&#32791;&#65292;&#25552;&#20986;&#20102;&#32467;&#21512;&#22810;&#20445;&#30495;&#24230;HPO&#21644;&#22810;&#30446;&#26631;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#25552;&#39640;&#20934;&#30830;&#29575;&#30340;&#21516;&#26102;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.01965</link><description>&lt;p&gt;
&#26088;&#22312;&#21033;&#29992;AutoML&#23454;&#29616;&#21487;&#25345;&#32493;&#28145;&#24230;&#23398;&#20064;&#65306;&#22522;&#20110;Deep Shift&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#30446;&#26631;HPO&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Leveraging AutoML for Sustainable Deep Learning: A Multi-Objective HPO Approach on Deep Shift Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01965
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;AutoML&#25216;&#26415;&#26368;&#22823;&#21270;Deep Shift&#31070;&#32463;&#32593;&#32476;&#24615;&#33021;&#24182;&#26368;&#23567;&#21270;&#36164;&#28304;&#28040;&#32791;&#65292;&#25552;&#20986;&#20102;&#32467;&#21512;&#22810;&#20445;&#30495;&#24230;HPO&#21644;&#22810;&#30446;&#26631;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#25552;&#39640;&#20934;&#30830;&#29575;&#30340;&#21516;&#26102;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#36890;&#36807;&#20174;&#22823;&#22411;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#22797;&#26434;&#27169;&#24335;&#25512;&#21160;&#20102;&#21508;&#20010;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;DL&#27169;&#22411;&#30340;&#35745;&#31639;&#38656;&#27714;&#24102;&#26469;&#20102;&#29615;&#22659;&#21644;&#36164;&#28304;&#25361;&#25112;&#12290;Deep Shift&#31070;&#32463;&#32593;&#32476;&#65288;DSNN&#65289;&#21033;&#29992;shift&#25805;&#20316;&#20943;&#23569;&#25512;&#29702;&#26102;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#20026;&#27492;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#20511;&#37492;&#26631;&#20934;DNN&#30340;&#35265;&#35299;&#65292;&#25105;&#20204;&#26377;&#20852;&#36259;&#36890;&#36807;AutoML&#25216;&#26415;&#20805;&#20998;&#21457;&#25381;DSNN&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36229;&#21442;&#25968;&#20248;&#21270;&#65288;HPO&#65289;&#23545;&#20110;&#26368;&#22823;&#21270;DSNN&#24615;&#33021;&#21516;&#26102;&#26368;&#23567;&#21270;&#36164;&#28304;&#28040;&#32791;&#30340;&#24433;&#21709;&#12290;&#30001;&#20110;&#23558;&#20934;&#30830;&#24615;&#21644;&#33021;&#32791;&#20316;&#20026;&#21487;&#33021;&#20114;&#34917;&#30446;&#26631;&#32467;&#21512;&#30340;&#22810;&#30446;&#26631;&#65288;MO&#65289;&#20248;&#21270;&#65292;&#25105;&#20204;&#24314;&#35758;&#23558;&#26368;&#20808;&#36827;&#30340;&#22810;&#20445;&#30495;&#24230;&#65288;MF&#65289;HPO&#19982;&#22810;&#30446;&#26631;&#20248;&#21270;&#30456;&#32467;&#21512;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24471;&#21040;&#20102;&#20934;&#30830;&#29575;&#36229;&#36807;80&#65285;&#19988;&#35745;&#31639;&#20302;&#32791;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01965v1 Announce Type: cross  Abstract: Deep Learning (DL) has advanced various fields by extracting complex patterns from large datasets. However, the computational demands of DL models pose environmental and resource challenges. Deep shift neural networks (DSNNs) offer a solution by leveraging shift operations to reduce computational complexity at inference. Following the insights from standard DNNs, we are interested in leveraging the full potential of DSNNs by means of AutoML techniques. We study the impact of hyperparameter optimization (HPO) to maximize DSNN performance while minimizing resource consumption. Since this combines multi-objective (MO) optimization with accuracy and energy consumption as potentially complementary objectives, we propose to combine state-of-the-art multi-fidelity (MF) HPO with multi-objective optimization. Experimental results demonstrate the effectiveness of our approach, resulting in models with over 80\% in accuracy and low computational 
&lt;/p&gt;</description></item><item><title>CAM-based&#26041;&#27861;&#35299;&#37322;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#20915;&#31574;&#26102;&#65292;&#21487;&#33021;&#20250;&#38169;&#35823;&#22320;&#23558;&#27169;&#22411;&#26080;&#27861;&#30475;&#21040;&#30340;&#37096;&#20998;&#24402;&#22240;&#20026;&#37325;&#35201;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#23545;&#27169;&#22411;&#34892;&#20026;&#30340;&#35823;&#35299;&#37322;&#12290;</title><link>https://arxiv.org/abs/2404.01964</link><description>&lt;p&gt;
&#22522;&#20110;CAM&#30340;&#26041;&#27861;&#21487;&#20197;&#31359;&#22681;&#32780;&#36807;
&lt;/p&gt;
&lt;p&gt;
CAM-Based Methods Can See through Walls
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01964
&lt;/p&gt;
&lt;p&gt;
CAM-based&#26041;&#27861;&#35299;&#37322;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#20915;&#31574;&#26102;&#65292;&#21487;&#33021;&#20250;&#38169;&#35823;&#22320;&#23558;&#27169;&#22411;&#26080;&#27861;&#30475;&#21040;&#30340;&#37096;&#20998;&#24402;&#22240;&#20026;&#37325;&#35201;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#23545;&#27169;&#22411;&#34892;&#20026;&#30340;&#35823;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
CAM-based&#26041;&#27861;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#20107;&#21518;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#29983;&#25104;&#26174;&#33879;&#24615;&#22320;&#22270;&#26469;&#35299;&#37322;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#20915;&#31574;&#12290;&#26174;&#33879;&#24615;&#22320;&#22270;&#31361;&#20986;&#26174;&#31034;&#19982;&#39044;&#27979;&#30456;&#20851;&#30340;&#22270;&#20687;&#37325;&#35201;&#21306;&#22495;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22823;&#22810;&#25968;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#38169;&#35823;&#22320;&#23558;&#22270;&#20687;&#30340;&#26576;&#20123;&#37096;&#20998;&#24402;&#22240;&#20026;&#27169;&#22411;&#26080;&#27861;&#30475;&#21040;&#30340;&#37325;&#35201;&#24471;&#20998;&#12290;&#25105;&#20204;&#34920;&#26126;&#36825;&#31181;&#29616;&#35937;&#22312;&#29702;&#35770;&#21644;&#23454;&#39564;&#20013;&#22343;&#23384;&#22312;&#12290;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;GradCAM&#22312;&#19968;&#20010;&#31616;&#21333;&#30340;&#25513;&#33180;CNN&#27169;&#22411;&#21021;&#22987;&#21270;&#26102;&#30340;&#34892;&#20026;&#12290;&#23454;&#39564;&#19978;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#31867;&#20284;VGG&#30340;&#27169;&#22411;&#65292;&#38480;&#21046;&#20854;&#19981;&#20351;&#29992;&#22270;&#20687;&#30340;&#19979;&#21322;&#37096;&#20998;&#65292;&#20173;&#28982;&#35266;&#23519;&#21040;&#26410;&#35265;&#37096;&#20998;&#30340;&#27491;&#20998;&#25968;&#12290;&#36825;&#31181;&#34892;&#20026;&#22312;&#20004;&#20010;&#26032;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23450;&#37327;&#35780;&#20272;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#26159;&#26377;&#38382;&#39064;&#30340;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#23545;&#27169;&#22411;&#34892;&#20026;&#30340;&#38169;&#35823;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01964v1 Announce Type: cross  Abstract: CAM-based methods are widely-used post-hoc interpretability method that produce a saliency map to explain the decision of an image classification model. The saliency map highlights the important areas of the image relevant to the prediction. In this paper, we show that most of these methods can incorrectly attribute an important score to parts of the image that the model cannot see. We show that this phenomenon occurs both theoretically and experimentally. On the theory side, we analyze the behavior of GradCAM on a simple masked CNN model at initialization. Experimentally, we train a VGG-like model constrained to not use the lower part of the image and nevertheless observe positive scores in the unseen part of the image. This behavior is evaluated quantitatively on two new datasets. We believe that this is problematic, potentially leading to mis-interpretation of the model's behavior.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#35270;&#35273;-&#35821;&#35328;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;Bi-LORA&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;VLMs&#21644;LORA&#35843;&#25972;&#25216;&#26415;&#65292;&#23558;&#21512;&#25104;&#22270;&#20687;&#26816;&#27979;&#36716;&#21270;&#20026;&#22270;&#20687;&#23383;&#24149;&#20219;&#21153;&#65292;&#20197;&#25552;&#39640;&#23545;&#26410;&#35265;&#36807;&#30340;&#27169;&#22411;&#29983;&#25104;&#22270;&#20687;&#30340;&#31934;&#24230;&#12290;</title><link>https://arxiv.org/abs/2404.01959</link><description>&lt;p&gt;
Bi-LORA&#65306;&#19968;&#31181;&#29992;&#20110;&#21512;&#25104;&#22270;&#20687;&#26816;&#27979;&#30340;&#35270;&#35273;-&#35821;&#35328;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Bi-LORA: A Vision-Language Approach for Synthetic Image Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01959
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#35270;&#35273;-&#35821;&#35328;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;Bi-LORA&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;VLMs&#21644;LORA&#35843;&#25972;&#25216;&#26415;&#65292;&#23558;&#21512;&#25104;&#22270;&#20687;&#26816;&#27979;&#36716;&#21270;&#20026;&#22270;&#20687;&#23383;&#24149;&#20219;&#21153;&#65292;&#20197;&#25552;&#39640;&#23545;&#26410;&#35265;&#36807;&#30340;&#27169;&#22411;&#29983;&#25104;&#22270;&#20687;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#22270;&#20687;&#21512;&#25104;&#25216;&#26415;&#30340;&#36827;&#27493;&#65292;&#22914;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#21644;&#25193;&#25955;&#27169;&#22411;&#65288;DM&#65289;&#65292;&#24050;&#32463;&#24320;&#21551;&#20102;&#19968;&#20010;&#29983;&#25104;&#39640;&#24230;&#36924;&#30495;&#22270;&#20687;&#30340;&#26102;&#20195;&#12290;&#23613;&#31649;&#36825;&#31181;&#25216;&#26415;&#36827;&#27493;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20852;&#36259;&#65292;&#20294;&#20063;&#24341;&#21457;&#20102;&#23545;&#20110;&#21306;&#20998;&#30495;&#23454;&#22270;&#20687;&#21644;&#21512;&#25104;&#22270;&#20687;&#20043;&#38388;&#28508;&#22312;&#38590;&#24230;&#30340;&#25285;&#24551;&#12290;&#26412;&#25991;&#20511;&#37492;&#20102;&#35270;&#35273;&#21644;&#35821;&#35328;&#20043;&#38388;&#24378;&#22823;&#30340;&#25910;&#25947;&#33021;&#21147;&#65292;&#32467;&#21512;&#20102;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#30340;&#38646;&#27425;&#23398;&#20064;&#29305;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;Bi-LORA&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;VLMs&#65292;&#32467;&#21512;&#20302;&#31209;&#36866;&#24212;&#65288;LORA&#65289;&#35843;&#25972;&#25216;&#26415;&#65292;&#22686;&#24378;&#20102;&#23545;&#26410;&#35265;&#36807;&#30340;&#27169;&#22411;&#29983;&#25104;&#22270;&#20687;&#30340;&#21512;&#25104;&#22270;&#20687;&#26816;&#27979;&#30340;&#31934;&#24230;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20851;&#38190;&#27010;&#24565;&#36716;&#21464;&#22312;&#20110;&#23558;&#20108;&#20998;&#31867;&#37325;&#26032;&#26500;&#24314;&#20026;&#22270;&#20687;&#23383;&#24149;&#20219;&#21153;&#65292;&#21033;&#29992;&#20102;&#23574;&#31471;VLM&#30340;&#29420;&#29305;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01959v1 Announce Type: cross  Abstract: Advancements in deep image synthesis techniques, such as generative adversarial networks (GANs) and diffusion models (DMs), have ushered in an era of generating highly realistic images. While this technological progress has captured significant interest, it has also raised concerns about the potential difficulty in distinguishing real images from their synthetic counterparts. This paper takes inspiration from the potent convergence capabilities between vision and language, coupled with the zero-shot nature of vision-language models (VLMs). We introduce an innovative method called Bi-LORA that leverages VLMs, combined with low-rank adaptation (LORA) tuning techniques, to enhance the precision of synthetic image detection for unseen model-generated images. The pivotal conceptual shift in our methodology revolves around reframing binary classification as an image captioning task, leveraging the distinctive capabilities of cutting-edge VLM
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;MESEN&#65292;&#19968;&#20010;&#21033;&#29992;&#26410;&#26631;&#35760;&#22810;&#27169;&#24577;&#25968;&#25454;&#35774;&#35745;&#21333;&#27169;&#24577;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#20219;&#21153;&#26426;&#21046;&#21644;&#29305;&#24449;&#23398;&#20064;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#21333;&#27169;&#24577;&#29305;&#24449;&#25552;&#21462;&#12290;</title><link>https://arxiv.org/abs/2404.01958</link><description>&lt;p&gt;
&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#35774;&#35745;&#21033;&#29992;&#23569;&#37327;&#26631;&#31614;&#36827;&#34892;&#21333;&#27169;&#24577;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#30340;MESEN
&lt;/p&gt;
&lt;p&gt;
MESEN: Exploit Multimodal Data to Design Unimodal Human Activity Recognition with Few Labels
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01958
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;MESEN&#65292;&#19968;&#20010;&#21033;&#29992;&#26410;&#26631;&#35760;&#22810;&#27169;&#24577;&#25968;&#25454;&#35774;&#35745;&#21333;&#27169;&#24577;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#20219;&#21153;&#26426;&#21046;&#21644;&#29305;&#24449;&#23398;&#20064;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#21333;&#27169;&#24577;&#29305;&#24449;&#25552;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#65288;HAR&#65289;&#23558;&#25104;&#20026;&#21508;&#31181;&#26032;&#20852;&#24212;&#29992;&#30340;&#37325;&#35201;&#21151;&#33021;&#12290;&#28982;&#32780;&#65292;HAR&#36890;&#24120;&#38754;&#20020;&#19982;&#27169;&#24577;&#38480;&#21046;&#21644;&#26631;&#31614;&#31232;&#32570;&#30456;&#20851;&#30340;&#25361;&#25112;&#65292;&#23548;&#33268;&#24403;&#21069;&#35299;&#20915;&#26041;&#26696;&#19982;&#30495;&#23454;&#19990;&#30028;&#38656;&#27714;&#20043;&#38388;&#23384;&#22312;&#24212;&#29992;&#24046;&#36317;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MESEN&#65292;&#19968;&#20010;&#22810;&#27169;&#24577;&#22686;&#24378;&#30340;&#21333;&#27169;&#24577;&#20256;&#24863;&#26694;&#26550;&#65292;&#29992;&#20110;&#21033;&#29992;HAR&#27169;&#22411;&#35774;&#35745;&#38454;&#27573;&#21487;&#29992;&#30340;&#26410;&#26631;&#35760;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#20197;&#22312;&#37096;&#32626;&#38454;&#27573;&#21152;&#24378;&#21333;&#27169;&#24577;HAR&#12290;&#36890;&#36807;&#23545;&#30417;&#30563;&#22810;&#27169;&#24577;&#34701;&#21512;&#23545;&#21333;&#27169;&#24577;&#29305;&#24449;&#25552;&#21462;&#30340;&#24433;&#21709;&#30340;&#30740;&#31350;&#65292;MESEN&#22312;&#22810;&#27169;&#24577;&#36741;&#21161;&#39044;&#35757;&#32451;&#38454;&#27573;&#35774;&#35745;&#20102;&#19968;&#20010;&#22810;&#20219;&#21153;&#26426;&#21046;&#12290;&#36890;&#36807;&#25972;&#21512;&#36328;&#27169;&#24577;&#29305;&#24449;&#23545;&#27604;&#23398;&#20064;&#21644;&#22810;&#27169;&#24577;&#20266;&#20998;&#31867;&#23545;&#40784;&#30340;&#24314;&#35758;&#26426;&#21046;&#65292;MESEN&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#25552;&#21462;&#20986;&#27599;&#31181;&#27169;&#24577;&#30340;&#26377;&#25928;&#21333;&#27169;&#24577;&#29305;&#24449;&#12290;&#38543;&#21518;&#65292;MESEN&#21487;&#20197;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01958v1 Announce Type: new  Abstract: Human activity recognition (HAR) will be an essential function of various emerging applications. However, HAR typically encounters challenges related to modality limitations and label scarcity, leading to an application gap between current solutions and real-world requirements. In this work, we propose MESEN, a multimodal-empowered unimodal sensing framework, to utilize unlabeled multimodal data available during the HAR model design phase for unimodal HAR enhancement during the deployment phase. From a study on the impact of supervised multimodal fusion on unimodal feature extraction, MESEN is designed to feature a multi-task mechanism during the multimodal-aided pre-training stage. With the proposed mechanism integrating cross-modal feature contrastive learning and multimodal pseudo-classification aligning, MESEN exploits unlabeled multimodal data to extract effective unimodal features for each modality. Subsequently, MESEN can adapt to
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#29992;&#20110;&#20013;&#39118;&#20998;&#21106;&#30340;&#21512;&#25104;&#26694;&#26550;&#65292;&#20351;&#29992;&#30149;&#21464;&#29305;&#23450;&#22686;&#24378;&#31574;&#30053;&#25193;&#23637;&#20102;SynthSeg&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23454;&#29616;&#23545;&#20581;&#24247;&#32452;&#32455;&#21644;&#30149;&#29702;&#30149;&#21464;&#30340;&#20998;&#21106;&#65292;&#26080;&#38656;&#29305;&#23450;&#24207;&#21015;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#22312;&#39046;&#22495;&#20869;&#21644;&#39046;&#22495;&#22806;&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#40065;&#26834;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.01946</link><description>&lt;p&gt;
&#29992;&#20110;&#40065;&#26834;&#24615;&#20013;&#39118;&#20998;&#21106;&#30340;&#21512;&#25104;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Synthetic Data for Robust Stroke Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01946
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#29992;&#20110;&#20013;&#39118;&#20998;&#21106;&#30340;&#21512;&#25104;&#26694;&#26550;&#65292;&#20351;&#29992;&#30149;&#21464;&#29305;&#23450;&#22686;&#24378;&#31574;&#30053;&#25193;&#23637;&#20102;SynthSeg&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23454;&#29616;&#23545;&#20581;&#24247;&#32452;&#32455;&#21644;&#30149;&#29702;&#30149;&#21464;&#30340;&#20998;&#21106;&#65292;&#26080;&#38656;&#29305;&#23450;&#24207;&#21015;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#22312;&#39046;&#22495;&#20869;&#21644;&#39046;&#22495;&#22806;&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#40065;&#26834;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01946v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449; &#25688;&#35201;&#65306;&#30446;&#21069;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#31070;&#32463;&#24433;&#20687;&#35821;&#20041;&#20998;&#21106;&#38656;&#35201;&#39640;&#20998;&#36776;&#29575;&#25195;&#25551;&#21644;&#22823;&#37327;&#27880;&#37322;&#25968;&#25454;&#38598;&#65292;&#36825;&#32473;&#20020;&#24202;&#36866;&#29992;&#24615;&#24102;&#26469;&#20102;&#26174;&#33879;&#38556;&#30861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21512;&#25104;&#26694;&#26550;&#65292;&#29992;&#20110;&#30149;&#21464;&#20998;&#21106;&#20219;&#21153;&#65292;&#25193;&#23637;&#20102;&#24050;&#24314;&#31435;&#30340;SynthSeg&#26041;&#27861;&#30340;&#33021;&#21147;&#65292;&#20197;&#36866;&#24212;&#20855;&#26377;&#30149;&#21464;&#29305;&#23450;&#22686;&#24378;&#31574;&#30053;&#30340;&#22823;&#22411;&#24322;&#36136;&#30149;&#21464;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#20174;&#20581;&#24247;&#21644;&#20013;&#39118;&#25968;&#25454;&#38598;&#27966;&#29983;&#30340;&#26631;&#31614;&#26144;&#23556;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#36825;&#37324;&#28436;&#31034;&#20102;UNet&#26550;&#26500;&#65292;&#20419;&#36827;&#20102;&#20581;&#24247;&#32452;&#32455;&#21644;&#30149;&#29702;&#30149;&#21464;&#30340;&#20998;&#21106;&#65292;&#32780;&#26080;&#38656;&#29305;&#23450;&#20110;&#24207;&#21015;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#38024;&#23545;&#39046;&#22495;&#20869;&#21644;&#39046;&#22495;&#22806;&#65288;OOD&#65289;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#34920;&#29616;&#20986;&#40065;&#26834;&#24615;&#33021;&#65292;&#19982;&#35757;&#32451;&#39046;&#22495;&#20869;&#30340;&#24403;&#21069;&#26041;&#27861;&#30456;&#23218;&#32654;&#65292;&#24182;&#22312;OOD&#25968;&#25454;&#19978;&#26174;&#30528;&#20248;&#20110;&#23427;&#20204;&#12290;&#36825;&#19968;&#36129;&#29486;&#26377;&#26395;&#25512;&#21160;&#21307;&#23398;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01946v1 Announce Type: cross  Abstract: Deep learning-based semantic segmentation in neuroimaging currently requires high-resolution scans and extensive annotated datasets, posing significant barriers to clinical applicability. We present a novel synthetic framework for the task of lesion segmentation, extending the capabilities of the established SynthSeg approach to accommodate large heterogeneous pathologies with lesion-specific augmentation strategies. Our method trains deep learning models, demonstrated here with the UNet architecture, using label maps derived from healthy and stroke datasets, facilitating the segmentation of both healthy tissue and pathological lesions without sequence-specific training data. Evaluated against in-domain and out-of-domain (OOD) datasets, our framework demonstrates robust performance, rivaling current methods within the training domain and significantly outperforming them on OOD data. This contribution holds promise for advancing medical
&lt;/p&gt;</description></item><item><title>&#22312;&#22823;&#25968;&#25454;&#32858;&#31867;&#20013;&#65292;&#30740;&#31350;&#20102;&#26102;&#38388;&#21644;&#20934;&#30830;&#24615;&#30340;&#26435;&#34913;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#22312;&#26377;&#25928;&#32447;&#24615;&#26102;&#38388;&#20869;&#33719;&#24471;&#26680;&#24515;&#38598;&#30340;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2404.01936</link><description>&lt;p&gt;
&#22823;&#25968;&#25454;&#32858;&#31867;&#20013;&#30340;&#26102;&#38388;&#19982;&#20934;&#30830;&#24615;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Settling Time vs. Accuracy Tradeoffs for Clustering Big Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01936
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#25968;&#25454;&#32858;&#31867;&#20013;&#65292;&#30740;&#31350;&#20102;&#26102;&#38388;&#21644;&#20934;&#30830;&#24615;&#30340;&#26435;&#34913;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#22312;&#26377;&#25928;&#32447;&#24615;&#26102;&#38388;&#20869;&#33719;&#24471;&#26680;&#24515;&#38598;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;k-means&#21644;k-median&#32858;&#31867;&#30340;&#29702;&#35770;&#21644;&#23454;&#38469;&#36816;&#34892;&#26102;&#38480;&#21046;&#12290;&#30001;&#20110;&#20960;&#20046;&#25152;&#26377;&#30340;&#32858;&#31867;&#26041;&#27861;&#37117;&#27604;&#35835;&#21462;&#25968;&#25454;&#38598;&#25152;&#38656;&#30340;&#26102;&#38388;&#26356;&#24930;&#65292;&#26368;&#24555;&#30340;&#26041;&#27861;&#26159;&#24555;&#36895;&#21387;&#32553;&#25968;&#25454;&#65292;&#24182;&#22312;&#21387;&#32553;&#34920;&#31034;&#19978;&#25191;&#34892;&#32858;&#31867;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#24182;&#27809;&#26377;&#36890;&#29992;&#30340;&#26368;&#20339;&#36873;&#25321;&#26469;&#21387;&#32553;&#28857;&#25968; - &#23613;&#31649;&#38543;&#26426;&#25277;&#26679;&#30340;&#36816;&#34892;&#26102;&#38388;&#20026;&#27425;&#32447;&#24615;&#26102;&#38388;&#19988;&#26680;&#24515;&#38598;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#65292;&#20294;&#21069;&#32773;&#24182;&#19981;&#24378;&#21046;&#20934;&#30830;&#24615;&#65292;&#32780;&#21518;&#32773;&#38543;&#30528;&#25968;&#25454;&#28857;&#25968;&#21644;&#32858;&#31867;&#25968;&#30340;&#22686;&#38271;&#32780;&#22826;&#24930;&#12290;&#23454;&#38469;&#19978;&#65292;&#20154;&#20204;&#25512;&#27979;&#20219;&#20309;&#22522;&#20110;&#28789;&#25935;&#24615;&#30340;&#26680;&#24515;&#38598;&#26500;&#24314;&#37117;&#38656;&#35201;&#36229;&#32447;&#24615;&#26102;&#38388;&#25165;&#33021;&#23545;&#25968;&#25454;&#38598;&#22823;&#23567;&#36827;&#34892;&#22788;&#29702;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#23384;&#22312;&#19968;&#31181;&#31639;&#27861;&#65292;&#36890;&#36807;&#28789;&#25935;&#24615;&#25277;&#26679;&#22312;&#26377;&#25928;&#32447;&#24615;&#26102;&#38388;&#20869;&#33719;&#24471;&#26680;&#24515;&#38598; - &#20165;&#22312;&#35835;&#21462;&#25968;&#25454;&#25152;&#38656;&#26102;&#38388;&#30340;&#23545;&#25968;&#22240;&#23376;&#20869;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01936v1 Announce Type: new  Abstract: We study the theoretical and practical runtime limits of k-means and k-median clustering on large datasets. Since effectively all clustering methods are slower than the time it takes to read the dataset, the fastest approach is to quickly compress the data and perform the clustering on the compressed representation. Unfortunately, there is no universal best choice for compressing the number of points - while random sampling runs in sublinear time and coresets provide theoretical guarantees, the former does not enforce accuracy while the latter is too slow as the numbers of points and clusters grow. Indeed, it has been conjectured that any sensitivity-based coreset construction requires super-linear time in the dataset size. We examine this relationship by first showing that there does exist an algorithm that obtains coresets via sensitivity sampling in effectively linear time - within log-factors of the time it takes to read the data. An
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22810;&#27169;&#24577;VAE&#22914;&#20309;&#22312;&#26080;&#30417;&#30563;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#20013;&#23454;&#29616;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#27169;&#25311;&#22120;&#20013;&#30340;&#27169;&#22411;&#24615;&#33021;&#25552;&#39640;55%&#12290;</title><link>https://arxiv.org/abs/2404.01932</link><description>&lt;p&gt;
&#36328;&#36234;&#35821;&#35328;&#12289;&#35270;&#35273;&#21644;&#34892;&#21160;&#65306;&#22810;&#27169;&#24577;VAE&#22312;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Bridging Language, Vision and Action: Multimodal VAEs in Robotic Manipulation Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01932
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22810;&#27169;&#24577;VAE&#22914;&#20309;&#22312;&#26080;&#30417;&#30563;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#20013;&#23454;&#29616;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#27169;&#25311;&#22120;&#20013;&#30340;&#27169;&#22411;&#24615;&#33021;&#25552;&#39640;55%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#26426;&#22120;&#20154;&#25805;&#20316;&#39046;&#22495;&#20013;&#26080;&#30417;&#30563;&#30340;&#35270;&#35273;-&#35821;&#35328;-&#21160;&#20316;&#26144;&#23556;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#22810;&#27169;&#24577;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#22914;&#20309;&#34987;&#24212;&#29992;&#20110;&#26080;&#30417;&#30563;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#20013;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#25913;&#36827;&#27169;&#22411;&#24615;&#33021;&#30340;&#27169;&#22411;&#19981;&#21464;&#24335;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#27169;&#25311;&#22120;&#20013;&#30340;&#27169;&#22411;&#24615;&#33021;&#25552;&#39640;&#39640;&#36798;55%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01932v1 Announce Type: cross  Abstract: In this work, we focus on unsupervised vision-language-action mapping in the area of robotic manipulation. Recently, multiple approaches employing pre-trained large language and vision models have been proposed for this task. However, they are computationally demanding and require careful fine-tuning of the produced outputs. A more lightweight alternative would be the implementation of multimodal Variational Autoencoders (VAEs) which can extract the latent features of the data and integrate them into a joint representation, as has been demonstrated mostly on image-image or image-text data for the state-of-the-art models. Here we explore whether and how can multimodal VAEs be employed in unsupervised robotic manipulation tasks in a simulated environment. Based on the obtained results, we propose a model-invariant training alternative that improves the models' performance in a simulator by up to 55%. Moreover, we systematically evaluate 
&lt;/p&gt;</description></item><item><title>&#25552;&#20379;&#20102;&#26032;&#30340;&#20840;&#38754;&#36817;&#20284;&#20445;&#35777;&#65292;&#25903;&#25345;&#26368;&#22823;&#22686;&#30410;&#27604;&#29575;&#21644;&#36817;&#20284;&#27425;&#27169;&#20989;&#25968;&#65292;&#21253;&#25324;&#22522;&#25968;&#32422;&#26463;&#19979;&#30340;&#26368;&#22823;&#21270;&#21644;&#26368;&#23567;&#25104;&#26412;&#35206;&#30422;&#20445;&#35777;&#65292;&#24182;&#19988;&#24341;&#20837;&#20102;&#33258;&#36866;&#24212;&#36873;&#25321;&#31574;&#30053;&#30340;&#26032;&#21442;&#25968;&#8220;&#26368;&#22823;&#22686;&#30410;&#27604;&#29575;&#8221;&#12290;</title><link>https://arxiv.org/abs/2404.01930</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#32452;&#21512;&#26368;&#22823;&#21270;&#65306;&#36229;&#36234;&#36817;&#20284;&#36138;&#24515;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Adaptive Combinatorial Maximization: Beyond Approximate Greedy Policies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01930
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20379;&#20102;&#26032;&#30340;&#20840;&#38754;&#36817;&#20284;&#20445;&#35777;&#65292;&#25903;&#25345;&#26368;&#22823;&#22686;&#30410;&#27604;&#29575;&#21644;&#36817;&#20284;&#27425;&#27169;&#20989;&#25968;&#65292;&#21253;&#25324;&#22522;&#25968;&#32422;&#26463;&#19979;&#30340;&#26368;&#22823;&#21270;&#21644;&#26368;&#23567;&#25104;&#26412;&#35206;&#30422;&#20445;&#35777;&#65292;&#24182;&#19988;&#24341;&#20837;&#20102;&#33258;&#36866;&#24212;&#36873;&#25321;&#31574;&#30053;&#30340;&#26032;&#21442;&#25968;&#8220;&#26368;&#22823;&#22686;&#30410;&#27604;&#29575;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#33258;&#36866;&#24212;&#32452;&#21512;&#26368;&#22823;&#21270;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#26159;&#19968;&#20010;&#26680;&#24515;&#25361;&#25112;&#65292;&#24212;&#29992;&#20110;&#20027;&#21160;&#23398;&#20064;&#20197;&#21450;&#35768;&#22810;&#20854;&#20182;&#39046;&#22495;&#12290;&#25105;&#20204;&#30740;&#31350;&#36125;&#21494;&#26031;&#35774;&#32622;&#65292;&#24182;&#32771;&#34385;&#22312;&#22522;&#25968;&#32422;&#26463;&#21644;&#26368;&#23567;&#25104;&#26412;&#35206;&#30422;&#19979;&#30340;&#26368;&#22823;&#21270;&#30446;&#26631;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#26032;&#30340;&#20840;&#38754;&#36817;&#20284;&#20445;&#35777;&#65292;&#21253;&#21547;&#20808;&#21069;&#30340;&#32467;&#26524;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#26126;&#26174;&#21152;&#24378;&#12290;&#25105;&#20204;&#30340;&#36817;&#20284;&#20445;&#35777;&#21516;&#26102;&#25903;&#25345;&#26368;&#22823;&#22686;&#30410;&#27604;&#29575;&#21644;&#36817;&#20284;&#27425;&#27169;&#20989;&#25968;&#65292;&#24182;&#21253;&#25324;&#22522;&#25968;&#32422;&#26463;&#19979;&#30340;&#26368;&#22823;&#21270;&#21644;&#26368;&#23567;&#25104;&#26412;&#35206;&#30422;&#20445;&#35777;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20026;&#20462;&#25913;&#21518;&#30340;&#20808;&#39564;&#25552;&#20379;&#20102;&#19968;&#20010;&#36817;&#20284;&#20445;&#35777;&#65292;&#36825;&#23545;&#20110;&#33719;&#24471;&#19981;&#21462;&#20915;&#20110;&#20808;&#39564;&#20013;&#26368;&#23567;&#27010;&#29575;&#30340;&#20027;&#21160;&#23398;&#20064;&#20445;&#35777;&#33267;&#20851;&#37325;&#35201;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#33258;&#36866;&#24212;&#36873;&#25321;&#31574;&#30053;&#30340;&#19968;&#20010;&#26032;&#21442;&#25968;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#26368;&#22823;&#22686;&#30410;&#27604;&#29575;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01930v1 Announce Type: new  Abstract: We study adaptive combinatorial maximization, which is a core challenge in machine learning, with applications in active learning as well as many other domains. We study the Bayesian setting, and consider the objectives of maximization under a cardinality constraint and minimum cost coverage. We provide new comprehensive approximation guarantees that subsume previous results, as well as considerably strengthen them. Our approximation guarantees simultaneously support the maximal gain ratio as well as near-submodular utility functions, and include both maximization under a cardinality constraint and a minimum cost coverage guarantee. In addition, we provided an approximation guarantee for a modified prior, which is crucial for obtaining active learning guarantees that do not depend on the smallest probability in the prior. Moreover, we discover a new parameter of adaptive selection policies, which we term the "maximal gain ratio". We show
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26088;&#22312;&#23545;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#36827;&#34892;&#24494;&#23567;&#25200;&#21160;&#20197;&#35268;&#36991;&#26816;&#27979;&#30340;&#24191;&#27867;&#23545;&#25239;&#25915;&#20987;&#26694;&#26550;&#65292;&#36890;&#36807;&#30333;&#30418;&#21644;&#40657;&#30418;&#20004;&#31181;&#25915;&#20987;&#35774;&#32622;&#20197;&#21450;&#23545;&#25239;&#23398;&#20064;&#22312;&#21160;&#24577;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#65292;&#35780;&#20272;&#20102;&#24403;&#21069;&#26816;&#27979;&#27169;&#22411;&#23545;&#27492;&#31867;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#28508;&#21147;&#22686;&#24378;</title><link>https://arxiv.org/abs/2404.01907</link><description>&lt;p&gt;
&#20154;&#24615;&#21270;&#26426;&#22120;&#29983;&#25104;&#30340;&#20869;&#23481;&#65306;&#36890;&#36807;&#23545;&#25239;&#25915;&#20987;&#35268;&#36991;AI&#25991;&#26412;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Humanizing Machine-Generated Content: Evading AI-Text Detection through Adversarial Attack
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01907
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26088;&#22312;&#23545;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#36827;&#34892;&#24494;&#23567;&#25200;&#21160;&#20197;&#35268;&#36991;&#26816;&#27979;&#30340;&#24191;&#27867;&#23545;&#25239;&#25915;&#20987;&#26694;&#26550;&#65292;&#36890;&#36807;&#30333;&#30418;&#21644;&#40657;&#30418;&#20004;&#31181;&#25915;&#20987;&#35774;&#32622;&#20197;&#21450;&#23545;&#25239;&#23398;&#20064;&#22312;&#21160;&#24577;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#65292;&#35780;&#20272;&#20102;&#24403;&#21069;&#26816;&#27979;&#27169;&#22411;&#23545;&#27492;&#31867;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#28508;&#21147;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#65292;&#26816;&#27979;&#25991;&#26412;&#26159;&#21542;&#30001;&#26426;&#22120;&#29983;&#25104;&#22312;&#38754;&#23545;&#35832;&#22914;&#35823;&#20256;&#20449;&#24687;&#12289;&#20445;&#25252;&#30693;&#35782;&#20135;&#26435;&#21644;&#39044;&#38450;&#23398;&#26415;&#25220;&#34989;&#31561;&#24694;&#24847;&#29992;&#20363;&#26102;&#21464;&#24471;&#26085;&#30410;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#34429;&#28982;&#32463;&#36807;&#33391;&#22909;&#35757;&#32451;&#30340;&#25991;&#26412;&#26816;&#27979;&#22120;&#22312;&#26410;&#30693;&#27979;&#35797;&#25968;&#25454;&#19978;&#23637;&#29616;&#20102;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#20123;&#26816;&#27979;&#22120;&#22312;&#22788;&#29702;&#23545;&#25239;&#25915;&#20987;&#65288;&#22914;&#37322;&#20041;&#65289;&#26102;&#23384;&#22312;&#28431;&#27934;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#24191;&#27867;&#31867;&#21035;&#30340;&#23545;&#25239;&#25915;&#20987;&#26694;&#26550;&#65292;&#26088;&#22312;&#23545;&#26426;&#22120;&#29983;&#25104;&#30340;&#20869;&#23481;&#36827;&#34892;&#24494;&#23567;&#25200;&#21160;&#20197;&#35268;&#36991;&#26816;&#27979;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#31181;&#25915;&#20987;&#35774;&#32622;&#65306;&#30333;&#30418;&#21644;&#40657;&#30418;&#65292;&#24182;&#22312;&#21160;&#24577;&#22330;&#26223;&#20013;&#37319;&#29992;&#23545;&#25239;&#23398;&#20064;&#26469;&#35780;&#20272;&#24403;&#21069;&#26816;&#27979;&#27169;&#22411;&#23545;&#27492;&#31867;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#28508;&#21147;&#22686;&#24378;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#30446;&#21069;&#30340;&#26816;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01907v1 Announce Type: new  Abstract: With the development of large language models (LLMs), detecting whether text is generated by a machine becomes increasingly challenging in the face of malicious use cases like the spread of false information, protection of intellectual property, and prevention of academic plagiarism. While well-trained text detectors have demonstrated promising performance on unseen test data, recent research suggests that these detectors have vulnerabilities when dealing with adversarial attacks such as paraphrasing. In this paper, we propose a framework for a broader class of adversarial attacks, designed to perform minor perturbations in machine-generated content to evade detection. We consider two attack settings: white-box and black-box, and employ adversarial learning in dynamic scenarios to assess the potential enhancement of the current detection model's robustness against such attacks. The empirical results reveal that the current detection mode
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28608;&#27963;&#23548;&#21521;&#25216;&#26415;&#65292;&#36890;&#36807;&#32534;&#36753;&#27169;&#22411;&#20869;&#37096;&#28608;&#27963;&#26469;&#25913;&#21892;CodeLLMs&#22312;&#20195;&#30721;&#31867;&#22411;&#39044;&#27979;&#20013;&#23545;&#20110;&#35821;&#27861;&#24178;&#25200;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#25104;&#21151;&#24212;&#29992;&#20110;Python&#21644;TypeScript&#30340;&#31867;&#22411;&#39044;&#27979;&#65292;&#23558;&#31867;&#22411;&#35823;&#24046;&#29575;&#32416;&#27491;&#39640;&#36798;90%&#12290;</title><link>https://arxiv.org/abs/2404.01903</link><description>&lt;p&gt;
&#22312;CodeLLMs&#20013;&#23454;&#29616;&#31867;&#22411;&#39044;&#27979;&#30340;&#40065;&#26834;&#28608;&#27963;&#23548;&#21521;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Activation Steering for Robust Type Prediction in CodeLLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01903
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28608;&#27963;&#23548;&#21521;&#25216;&#26415;&#65292;&#36890;&#36807;&#32534;&#36753;&#27169;&#22411;&#20869;&#37096;&#28608;&#27963;&#26469;&#25913;&#21892;CodeLLMs&#22312;&#20195;&#30721;&#31867;&#22411;&#39044;&#27979;&#20013;&#23545;&#20110;&#35821;&#27861;&#24178;&#25200;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#25104;&#21151;&#24212;&#29992;&#20110;Python&#21644;TypeScript&#30340;&#31867;&#22411;&#39044;&#27979;&#65292;&#23558;&#31867;&#22411;&#35823;&#24046;&#29575;&#32416;&#27491;&#39640;&#36798;90%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#22312;&#20195;&#30721;&#19978;&#30340;&#29616;&#20195;LLMs&#33021;&#22815;&#25104;&#21151;&#22320;&#23436;&#25104;&#21508;&#31181;&#32534;&#31243;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#23545;&#35821;&#27861;&#29305;&#24449;&#38750;&#24120;&#25935;&#24863;&#65292;&#20363;&#22914;&#21464;&#37327;&#21644;&#31867;&#22411;&#30340;&#21517;&#31216;&#12289;&#20195;&#30721;&#32467;&#26500;&#20197;&#21450;&#31867;&#22411;&#25552;&#31034;&#30340;&#23384;&#22312;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25512;&#29702;&#26102;&#25216;&#26415;&#65292;&#20351;CodeLLMs&#26356;&#33021;&#25269;&#24481;&#35821;&#27861;&#24178;&#25200;&#22240;&#32032;&#65292;&#36825;&#20123;&#22240;&#32032;&#19982;&#35821;&#20041;&#26080;&#20851;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#28608;&#27963;&#23548;&#21521;&#65292;&#28041;&#21450;&#32534;&#36753;&#20869;&#37096;&#27169;&#22411;&#28608;&#27963;&#20197;&#23558;&#27169;&#22411;&#24341;&#23548;&#21040;&#27491;&#30830;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#36890;&#36807;&#20174;&#31361;&#21464;&#27979;&#35797;&#20013;&#27762;&#21462;&#28789;&#24863;&#26500;&#24314;&#28608;&#27963;&#21521;&#37327;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26500;&#24314;&#26368;&#23567;&#30340;&#30772;&#22351;&#35821;&#20041;&#30340;&#20195;&#30721;&#32534;&#36753;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#20174;&#20445;&#30041;&#35821;&#20041;&#30340;&#20195;&#30721;&#32534;&#36753;&#20013;&#26500;&#24314;&#28608;&#27963;&#21521;&#37327;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#36880;&#28176;&#31867;&#22411;&#21270;&#35821;&#35328;Python&#21644;TypeScript&#30340;&#31867;&#22411;&#39044;&#27979;&#20219;&#21153;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#32416;&#27491;&#39640;&#36798;90%&#30340;&#31867;&#22411;&#38169;&#35823;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01903v1 Announce Type: new  Abstract: Contemporary LLMs pretrained on code are capable of succeeding at a wide variety of programming tasks. However, their performance is very sensitive to syntactic features, such as the names of variables and types, the structure of code, and presence of type hints. We contribute an inference-time technique to make CodeLLMs more robust to syntactic distractors that are semantically irrelevant. Our methodology relies on activation steering, which involves editing internal model activations to steer the model towards the correct prediction. We contribute a novel way to construct steering vectors by taking inspiration from mutation testing, which constructs minimal semantics-breaking code edits. In contrast, we construct steering vectors from semantics-preserving code edits. We apply our approach to the task of type prediction for the gradually typed languages Python and TypeScript. This approach corrects up to 90% of type mispredictions. Fina
&lt;/p&gt;</description></item><item><title>COS-GNN&#23558;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#19982;&#36830;&#32493;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;CGNNs&#65289;&#32467;&#21512;&#22312;&#19968;&#36215;&#65292;&#20197;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#23545;&#22270;&#33410;&#28857;&#36827;&#34892;&#34920;&#31034;&#65292;&#24182;&#23558;&#20854;&#19982;&#26102;&#38388;&#19968;&#36215;&#38598;&#25104;&#21040;ODE&#36807;&#31243;&#20013;&#65292;&#20197;&#22686;&#24378;&#20449;&#24687;&#20445;&#23384;&#21644;&#35299;&#20915;&#22312;&#31163;&#25955;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2404.01897</link><description>&lt;p&gt;
&#36830;&#32493;&#33033;&#20914;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Continuous Spiking Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01897
&lt;/p&gt;
&lt;p&gt;
COS-GNN&#23558;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#19982;&#36830;&#32493;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;CGNNs&#65289;&#32467;&#21512;&#22312;&#19968;&#36215;&#65292;&#20197;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#23545;&#22270;&#33410;&#28857;&#36827;&#34892;&#34920;&#31034;&#65292;&#24182;&#23558;&#20854;&#19982;&#26102;&#38388;&#19968;&#36215;&#38598;&#25104;&#21040;ODE&#36807;&#31243;&#20013;&#65292;&#20197;&#22686;&#24378;&#20449;&#24687;&#20445;&#23384;&#21644;&#35299;&#20915;&#22312;&#31163;&#25955;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;CGNNs&#65289;&#22240;&#24341;&#20837;&#36830;&#32493;&#21160;&#21147;&#23398;&#32780;&#24341;&#36215;&#20102;&#26497;&#22823;&#20851;&#27880;&#65292;&#33021;&#22815;&#25512;&#24191;&#29616;&#26377;&#30340;&#31163;&#25955;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#12290;&#23427;&#20204;&#36890;&#24120;&#21463;&#25193;&#25955;&#31867;&#26041;&#27861;&#21551;&#21457;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20256;&#25773;&#26041;&#26696;&#65292;&#24182;&#20351;&#29992;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#36827;&#34892;&#20998;&#26512;&#12290;&#28982;&#32780;&#65292;CGNNs&#30340;&#23454;&#29616;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#33021;&#21147;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#38590;&#20197;&#37096;&#32626;&#22312;&#30005;&#27744;&#20379;&#30005;&#35774;&#22791;&#19978;&#12290;&#21463;&#26368;&#36817;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#30340;&#21551;&#21457;&#65292;SNNs&#27169;&#25311;&#29983;&#29289;&#25512;&#29702;&#36807;&#31243;&#24182;&#25552;&#20379;&#19968;&#31181;&#33410;&#33021;&#30340;&#31070;&#32463;&#26550;&#26500;&#65292;&#25105;&#20204;&#23558;SNNs&#19982;CGNNs&#32467;&#21512;&#21040;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#20013;&#65292;&#21629;&#21517;&#20026;&#36830;&#32493;&#33033;&#20914;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;COS-GNN&#65289;&#12290;&#25105;&#20204;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#20351;&#29992;SNNs&#36827;&#34892;&#22270;&#33410;&#28857;&#34920;&#31034;&#65292;&#36825;&#20123;&#34920;&#31034;&#36827;&#19968;&#27493;&#19982;&#26102;&#38388;&#19968;&#36215;&#38598;&#25104;&#21040;ODE&#36807;&#31243;&#20013;&#65292;&#20197;&#22686;&#24378;&#20449;&#24687;&#20445;&#23384;&#21644;&#32531;&#35299;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01897v1 Announce Type: cross  Abstract: Continuous graph neural networks (CGNNs) have garnered significant attention due to their ability to generalize existing discrete graph neural networks (GNNs) by introducing continuous dynamics. They typically draw inspiration from diffusion-based methods to introduce a novel propagation scheme, which is analyzed using ordinary differential equations (ODE). However, the implementation of CGNNs requires significant computational power, making them challenging to deploy on battery-powered devices. Inspired by recent spiking neural networks (SNNs), which emulate a biological inference process and provide an energy-efficient neural architecture, we incorporate the SNNs with CGNNs in a unified framework, named Continuous Spiking Graph Neural Networks (COS-GNN). We employ SNNs for graph node representation at each time step, which are further integrated into the ODE process along with time. To enhance information preservation and mitigate in
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#20855;&#26377;&#20999;&#25442;&#25104;&#26412;&#30340;&#23545;&#25239;&#24615;&#32452;&#21512;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#25512;&#23548;&#20102;&#26497;&#23567;&#21518;&#24724;&#30340;&#19979;&#38480;&#24182;&#35774;&#35745;&#20102;&#36924;&#36817;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2404.01883</link><description>&lt;p&gt;
&#20855;&#26377;&#20999;&#25442;&#25104;&#26412;&#30340;&#23545;&#25239;&#24615;&#32452;&#21512;&#36172;&#21338;&#26426;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Adversarial Combinatorial Bandits with Switching Costs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01883
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#20855;&#26377;&#20999;&#25442;&#25104;&#26412;&#30340;&#23545;&#25239;&#24615;&#32452;&#21512;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#25512;&#23548;&#20102;&#26497;&#23567;&#21518;&#24724;&#30340;&#19979;&#38480;&#24182;&#35774;&#35745;&#20102;&#36924;&#36817;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#20999;&#25442;&#25104;&#26412;$\lambda$&#30340;&#23545;&#25239;&#24615;&#32452;&#21512;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#32771;&#34385;&#20102;&#36172;&#21338;&#26426;&#21453;&#39304;&#21644;&#21322;&#36172;&#21338;&#26426;&#21453;&#39304;&#35774;&#32622;&#12290;&#22312;&#24573;&#35270;&#23545;&#25163;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;$K$&#20010;&#22522;&#26412;&#33218;&#21644;&#26102;&#38388;&#36328;&#24230;$T$&#30340;&#26497;&#23567;&#21518;&#24724;&#30340;&#19979;&#38480;&#65292;&#24182;&#35774;&#35745;&#20102;&#31639;&#27861;&#26469;&#36924;&#36817;&#36825;&#20123;&#19979;&#38480;&#12290;&#20026;&#20102;&#35777;&#26126;&#36825;&#20123;&#19979;&#38480;&#65292;&#25105;&#20204;&#20026;&#20004;&#31181;&#21453;&#39304;&#35774;&#32622;&#35774;&#35745;&#20102;&#38543;&#26426;&#25439;&#22833;&#24207;&#21015;&#65292;&#20511;&#37492;&#20102;Dekel&#31561;&#20154;&#65288;2014&#24180;&#65289;&#20043;&#21069;&#24037;&#20316;&#20013;&#30340;&#19968;&#20010;&#24605;&#24819;&#12290;&#36172;&#21338;&#26426;&#21453;&#39304;&#30340;&#19979;&#38480;&#20026;$ \tilde{\Omega}\big( (\lambda K)^{\frac{1}{3}} (TI)^{\frac{2}{3}}\big)$&#65292;&#32780;&#21322;&#36172;&#21338;&#26426;&#21453;&#39304;&#30340;&#19979;&#38480;&#20026;$ \tilde{\Omega}\big( (\lambda K I)^{\frac{1}{3}} T^{\frac{2}{3}}\big)$&#65292;&#20854;&#20013;$I$&#26159;&#22312;&#27599;&#19968;&#36718;&#20013;&#25773;&#25918;&#30340;&#32452;&#21512;&#33218;&#20013;&#30340;&#22522;&#26412;&#33218;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01883v1 Announce Type: cross  Abstract: We study the problem of adversarial combinatorial bandit with a switching cost $\lambda$ for a switch of each selected arm in each round, considering both the bandit feedback and semi-bandit feedback settings. In the oblivious adversarial case with $K$ base arms and time horizon $T$, we derive lower bounds for the minimax regret and design algorithms to approach them. To prove these lower bounds, we design stochastic loss sequences for both feedback settings, building on an idea from previous work in Dekel et al. (2014). The lower bound for bandit feedback is $ \tilde{\Omega}\big( (\lambda K)^{\frac{1}{3}} (TI)^{\frac{2}{3}}\big)$ while that for semi-bandit feedback is $ \tilde{\Omega}\big( (\lambda K I)^{\frac{1}{3}} T^{\frac{2}{3}}\big)$ where $I$ is the number of base arms in the combinatorial arm played in each round. To approach these lower bounds, we design algorithms that operate in batches by dividing the time horizon into batc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23450;&#20041;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#31243;&#24207;&#20844;&#24179;&#24615;&#65292;&#25552;&#20986;&#20102;&#35780;&#20272;&#32676;&#20307;&#31243;&#24207;&#20844;&#24179;&#24615;&#30340;&#26032;&#24230;&#37327;&#26631;&#20934;$GPF_{FAE}$&#65292;&#24182;&#20351;&#29992;&#29305;&#24449;&#24402;&#22240;&#35299;&#37322;&#26469;&#25429;&#25417;&#20915;&#31574;&#36807;&#31243;&#65292;&#23454;&#39564;&#35777;&#23454;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#31243;&#24207;&#21644;&#20998;&#37197;&#20844;&#24179;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35782;&#21035;&#23548;&#33268;&#31243;&#24207;&#24615;&#19981;&#20844;&#24179;&#30340;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2404.01877</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#31243;&#24207;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Procedural Fairness in Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01877
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23450;&#20041;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#31243;&#24207;&#20844;&#24179;&#24615;&#65292;&#25552;&#20986;&#20102;&#35780;&#20272;&#32676;&#20307;&#31243;&#24207;&#20844;&#24179;&#24615;&#30340;&#26032;&#24230;&#37327;&#26631;&#20934;$GPF_{FAE}$&#65292;&#24182;&#20351;&#29992;&#29305;&#24449;&#24402;&#22240;&#35299;&#37322;&#26469;&#25429;&#25417;&#20915;&#31574;&#36807;&#31243;&#65292;&#23454;&#39564;&#35777;&#23454;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#31243;&#24207;&#21644;&#20998;&#37197;&#20844;&#24179;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35782;&#21035;&#23548;&#33268;&#31243;&#24207;&#24615;&#19981;&#20844;&#24179;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20844;&#24179;&#24615;&#19968;&#30452;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#65292;&#28982;&#32780;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#27169;&#22411;&#30340;&#20998;&#37197;&#20844;&#24179;&#24615;&#19978;&#12290;&#21478;&#19968;&#20010;&#20844;&#24179;&#24615;&#32500;&#24230;&#65292;&#21363;&#31243;&#24207;&#20844;&#24179;&#24615;&#65292;&#21364;&#34987;&#24573;&#35270;&#20102;&#12290;&#26412;&#25991;&#39318;&#20808;&#23450;&#20041;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#31243;&#24207;&#20844;&#24179;&#24615;&#65292;&#28982;&#21518;&#32473;&#20986;&#20102;&#20010;&#20307;&#21644;&#32676;&#20307;&#31243;&#24207;&#20844;&#24179;&#24615;&#30340;&#27491;&#24335;&#23450;&#20041;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#26469;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#32676;&#20307;&#31243;&#24207;&#20844;&#24179;&#24615;&#65292;&#31216;&#20026;$GPF_{FAE}$&#65292;&#23427;&#21033;&#29992;&#20102;&#19968;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#21363;&#29305;&#24449;&#24402;&#22240;&#35299;&#37322;&#65288;FAE&#65289;&#65292;&#26469;&#25429;&#25417;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#20843;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;$GPF_{FAE}$&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#31243;&#24207;&#21644;&#20998;&#37197;&#20844;&#24179;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35782;&#21035;&#23548;&#33268;&#31243;&#24207;&#24615;&#19981;&#20844;&#24179;&#38382;&#39064;&#30340;&#29305;&#24449;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01877v1 Announce Type: new  Abstract: Fairness in machine learning (ML) has received much attention. However, existing studies have mainly focused on the distributive fairness of ML models. The other dimension of fairness, i.e., procedural fairness, has been neglected. In this paper, we first define the procedural fairness of ML models, and then give formal definitions of individual and group procedural fairness. We propose a novel metric to evaluate the group procedural fairness of ML models, called $GPF_{FAE}$, which utilizes a widely used explainable artificial intelligence technique, namely feature attribution explanation (FAE), to capture the decision process of the ML models. We validate the effectiveness of $GPF_{FAE}$ on a synthetic dataset and eight real-world datasets. Our experiments reveal the relationship between procedural and distributive fairness of the ML model. Based on our analysis, we propose a method for identifying the features that lead to the procedur
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#38024;&#23545;&#20302;&#22320;&#29699;&#36712;&#36947;&#21355;&#26143;&#32593;&#32476;&#30340;&#26032;&#22411;FEEL&#31639;&#27861;FEDMEGA&#65292;&#36890;&#36807;&#25972;&#21512;&#21355;&#26143;&#38388;&#38142;&#25509;&#65288;ISL&#65289;&#36827;&#34892;&#36712;&#36947;&#20869;&#27169;&#22411;&#32858;&#21512;&#12290;</title><link>https://arxiv.org/abs/2404.01875</link><description>&lt;p&gt;
&#21355;&#26143;&#32852;&#21512;&#36793;&#32536;&#23398;&#20064;&#65306;&#26550;&#26500;&#35774;&#35745;&#19982;&#25910;&#25947;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Satellite Federated Edge Learning: Architecture Design and Convergence Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01875
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#38024;&#23545;&#20302;&#22320;&#29699;&#36712;&#36947;&#21355;&#26143;&#32593;&#32476;&#30340;&#26032;&#22411;FEEL&#31639;&#27861;FEDMEGA&#65292;&#36890;&#36807;&#25972;&#21512;&#21355;&#26143;&#38388;&#38142;&#25509;&#65288;ISL&#65289;&#36827;&#34892;&#36712;&#36947;&#20869;&#27169;&#22411;&#32858;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#22320;&#29699;&#36712;&#36947;&#65288;LEO&#65289;&#21355;&#26143;&#32593;&#32476;&#30340;&#22823;&#37327;&#22686;&#38271;&#23548;&#33268;&#20135;&#29983;&#22823;&#37327;&#36965;&#24863;&#25968;&#25454;&#65292;&#20256;&#32479;&#19978;&#36825;&#20123;&#25968;&#25454;&#34987;&#20256;&#36755;&#21040;&#22320;&#38754;&#26381;&#21153;&#22120;&#36827;&#34892;&#38598;&#20013;&#22788;&#29702;&#65292;&#24341;&#21457;&#38544;&#31169;&#21644;&#24102;&#23485;&#38382;&#39064;&#12290;&#32852;&#21512;&#36793;&#32536;&#23398;&#20064;&#65288;FEEL&#65289;&#20316;&#20026;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#26377;&#28508;&#21147;&#36890;&#36807;&#20165;&#20998;&#20139;&#27169;&#22411;&#21442;&#25968;&#32780;&#38750;&#21407;&#22987;&#25968;&#25454;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;LEO&#36229;&#32423;&#26143;&#24231;&#32593;&#32476;&#37327;&#36523;&#23450;&#21046;&#30340;&#26032;&#22411;FEEL&#31639;&#27861;&#65292;&#21517;&#20026;FEDMEGA&#65292;&#36890;&#36807;&#25972;&#21512;&#21355;&#26143;&#38388;&#38142;&#25509;&#65288;ISL&#65289;&#36827;&#34892;&#36712;&#36947;&#20869;&#27169;&#22411;&#32858;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01875v1 Announce Type: cross  Abstract: The proliferation of low-earth-orbit (LEO) satellite networks leads to the generation of vast volumes of remote sensing data which is traditionally transferred to the ground server for centralized processing, raising privacy and bandwidth concerns. Federated edge learning (FEEL), as a distributed machine learning approach, has the potential to address these challenges by sharing only model parameters instead of raw data. Although promising, the dynamics of LEO networks, characterized by the high mobility of satellites and short ground-to-satellite link (GSL) duration, pose unique challenges for FEEL. Notably, frequent model transmission between the satellites and ground incurs prolonged waiting time and large transmission latency. This paper introduces a novel FEEL algorithm, named FEDMEGA, tailored to LEO mega-constellation networks. By integrating inter-satellite links (ISL) for intra-orbit model aggregation, the proposed algorithm s
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#38382;&#21367;&#26041;&#27861;&#65292;&#26681;&#25454;&#29992;&#25143;&#20808;&#21069;&#30340;&#22238;&#31572;&#36873;&#25321;&#21518;&#32493;&#30340;&#38382;&#39064;&#65292;&#26088;&#22312;&#25552;&#39640;&#25237;&#31080;&#24314;&#35758;&#24212;&#29992;&#30340;&#25512;&#33616;&#20934;&#30830;&#24615;&#21516;&#26102;&#20943;&#23569;&#32473;&#36873;&#27665;&#25552;&#38382;&#30340;&#38382;&#39064;&#25968;&#37327;&#12290;</title><link>https://arxiv.org/abs/2404.01872</link><description>&lt;p&gt;
&#24555;&#36895;&#33258;&#36866;&#24212;&#38382;&#21367;&#29992;&#20110;&#25237;&#31080;&#24314;&#35758;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Fast and Adaptive Questionnaires for Voting Advice Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01872
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#38382;&#21367;&#26041;&#27861;&#65292;&#26681;&#25454;&#29992;&#25143;&#20808;&#21069;&#30340;&#22238;&#31572;&#36873;&#25321;&#21518;&#32493;&#30340;&#38382;&#39064;&#65292;&#26088;&#22312;&#25552;&#39640;&#25237;&#31080;&#24314;&#35758;&#24212;&#29992;&#30340;&#25512;&#33616;&#20934;&#30830;&#24615;&#21516;&#26102;&#20943;&#23569;&#32473;&#36873;&#27665;&#25552;&#38382;&#30340;&#38382;&#39064;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25237;&#31080;&#24314;&#35758;&#24212;&#29992;&#30340;&#26377;&#25928;&#24615;&#24120;&#24120;&#21463;&#20854;&#38382;&#21367;&#38271;&#24230;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#29992;&#25143;&#30130;&#21171;&#21644;&#26410;&#23436;&#25104;&#22238;&#31572;&#30340;&#38382;&#39064;&#65292;&#19968;&#20123;&#24212;&#29992;&#65288;&#20363;&#22914;&#29790;&#22763;Smartvote&#65289;&#25552;&#20379;&#20102;&#20854;&#38382;&#21367;&#30340;&#31934;&#31616;&#29256;&#26412;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31934;&#31616;&#29256;&#26412;&#26080;&#27861;&#20445;&#35777;&#25512;&#33616;&#30340;&#25919;&#20826;&#25110;&#20505;&#36873;&#20154;&#30340;&#20934;&#30830;&#24615;&#65292;&#25105;&#20204;&#21457;&#29616;&#20854;&#20445;&#25345;&#22312;40%&#20197;&#19979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#38382;&#21367;&#26041;&#27861;&#65292;&#26681;&#25454;&#29992;&#25143;&#20808;&#21069;&#30340;&#22238;&#31572;&#36873;&#25321;&#21518;&#32493;&#30340;&#38382;&#39064;&#65292;&#26088;&#22312;&#25552;&#39640;&#25512;&#33616;&#20934;&#30830;&#24615;&#21516;&#26102;&#20943;&#23569;&#25552;&#38382;&#32473;&#36873;&#27665;&#30340;&#38382;&#39064;&#25968;&#37327;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#27169;&#22359;&#22312;&#20219;&#20309;&#23436;&#25104;&#38454;&#27573;&#39044;&#27979;&#32570;&#22833;&#20540;&#65292;&#21033;&#29992;&#20004;&#32500;&#28508;&#22312;&#31354;&#38388;&#21453;&#26144;&#25919;&#27835;&#23398;&#20256;&#32479;&#26041;&#27861;&#65292;&#29992;&#20110;&#21487;&#35270;&#21270;&#25919;&#27835;&#21462;&#21521;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36873;&#25321;&#22120;&#27169;&#22359;&#29992;&#20110;&#30830;&#23450;&#26368;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01872v1 Announce Type: new  Abstract: The effectiveness of Voting Advice Applications (VAA) is often compromised by the length of their questionnaires. To address user fatigue and incomplete responses, some applications (such as the Swiss Smartvote) offer a condensed version of their questionnaire. However, these condensed versions can not ensure the accuracy of recommended parties or candidates, which we show to remain below 40%. To tackle these limitations, this work introduces an adaptive questionnaire approach that selects subsequent questions based on users' previous answers, aiming to enhance recommendation accuracy while reducing the number of questions posed to the voters. Our method uses an encoder and decoder module to predict missing values at any completion stage, leveraging a two-dimensional latent space reflective of political science's traditional methods for visualizing political orientations. Additionally, a selector module is proposed to determine the most 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22312;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#22522;&#20110;&#36125;&#21494;&#26031;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#36890;&#36807;&#20027;&#21160;&#25506;&#32034;&#25913;&#21892;&#27169;&#22411;&#36136;&#37327;&#21644;&#20445;&#25345;&#25968;&#25454;&#25928;&#29575;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2404.01867</link><description>&lt;p&gt;
&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#22522;&#20110;&#36125;&#21494;&#26031;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20027;&#21160;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Active Exploration in Bayesian Model-based Reinforcement Learning for Robot Manipulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01867
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22312;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#22522;&#20110;&#36125;&#21494;&#26031;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#36890;&#36807;&#20027;&#21160;&#25506;&#32034;&#25913;&#21892;&#27169;&#22411;&#36136;&#37327;&#21644;&#20445;&#25345;&#25968;&#25454;&#25928;&#29575;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#35299;&#20915;&#26426;&#22120;&#20154;&#25805;&#20316;&#31561;&#22797;&#26434;&#29615;&#22659;&#20013;&#30340;&#22810;&#20010;&#20219;&#21153;&#20173;&#28982;&#26159;&#26426;&#22120;&#20154;&#25216;&#26415;&#20013;&#30340;&#19968;&#20010;&#25345;&#32493;&#25361;&#25112;&#65292;&#20063;&#26159;&#25968;&#25454;&#39537;&#21160;&#35299;&#20915;&#26041;&#26696;&#65288;&#22914;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#65289;&#30340;&#19968;&#20010;&#26426;&#20250;&#12290;&#27169;&#22411;&#39537;&#21160;&#30340;RL&#36890;&#36807;&#26500;&#24314;&#26426;&#22120;&#20154;&#30340;&#21160;&#24577;&#27169;&#22411;&#65292;&#23454;&#29616;&#25968;&#25454;&#37325;&#29992;&#21644;&#22312;&#20855;&#26377;&#30456;&#21516;&#26426;&#22120;&#20154;&#21644;&#31867;&#20284;&#29615;&#22659;&#30340;&#20219;&#21153;&#20043;&#38388;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#26426;&#22120;&#20154;&#23398;&#20013;&#30340;&#25968;&#25454;&#25910;&#38598;&#25104;&#26412;&#24456;&#39640;&#65292;&#25105;&#20204;&#24517;&#39035;&#20381;&#36182;&#25968;&#25454;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#22914;&#22522;&#20110;&#27169;&#22411;&#30340;RL&#65292;&#22312;&#22522;&#20110;&#23398;&#20064;&#27169;&#22411;&#30340;&#24265;&#20215;&#20223;&#30495;&#20013;&#36827;&#34892;&#22823;&#37096;&#20998;&#31574;&#30053;&#23398;&#20064;&#12290;&#22240;&#27492;&#65292;&#27169;&#22411;&#30340;&#36136;&#37327;&#23545;&#20110;&#21518;&#32493;&#20219;&#21153;&#30340;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#33268;&#21147;&#20110;&#36890;&#36807;&#22312;&#21021;&#22987;&#25506;&#32034;&#38454;&#27573;&#25191;&#34892;&#21160;&#24577;&#27169;&#22411;&#30340;&#20027;&#21160;&#23398;&#20064;&#65292;&#22522;&#20110;&#26368;&#22823;&#21270;&#20449;&#24687;&#25910;&#38598;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#36136;&#37327;&#24182;&#20445;&#25345;&#25968;&#25454;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01867v1 Announce Type: cross  Abstract: Efficiently tackling multiple tasks within complex environment, such as those found in robot manipulation, remains an ongoing challenge in robotics and an opportunity for data-driven solutions, such as reinforcement learning (RL). Model-based RL, by building a dynamic model of the robot, enables data reuse and transfer learning between tasks with the same robot and similar environment. Furthermore, data gathering in robotics is expensive and we must rely on data efficient approaches such as model-based RL, where policy learning is mostly conducted on cheaper simulations based on the learned model. Therefore, the quality of the model is fundamental for the performance of the posterior tasks. In this work, we focus on improving the quality of the model and maintaining the data efficiency by performing active learning of the dynamic model during a preliminary exploration phase based on maximize information gathering. We employ Bayesian ne
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22870;&#21169;&#20989;&#25968;&#35757;&#32451;&#30340;&#32454;&#21270;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#21487;&#33021;&#20250;&#22240;&#20026;&#22870;&#21169;&#36807;&#24230;&#20248;&#21270;&#32780;&#25439;&#23475;&#24615;&#33021;&#65292;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22870;&#21169;&#27169;&#22411;&#32622;&#20449;&#24230;&#30340;&#23545;&#40784;&#24615;&#22686;&#24378;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2404.01863</link><description>&lt;p&gt;
&#38754;&#21521;&#32454;&#21270;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#32622;&#20449;&#24230;&#24863;&#30693;&#22870;&#21169;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Confidence-aware Reward Optimization for Fine-tuning Text-to-Image Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01863
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22870;&#21169;&#20989;&#25968;&#35757;&#32451;&#30340;&#32454;&#21270;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#21487;&#33021;&#20250;&#22240;&#20026;&#22870;&#21169;&#36807;&#24230;&#20248;&#21270;&#32780;&#25439;&#23475;&#24615;&#33021;&#65292;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22870;&#21169;&#27169;&#22411;&#32622;&#20449;&#24230;&#30340;&#23545;&#40784;&#24615;&#22686;&#24378;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#31867;&#21453;&#39304;&#25968;&#25454;&#35757;&#32451;&#30340;&#22870;&#21169;&#20989;&#25968;&#19978;&#23545;&#32454;&#21270;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#26377;&#25928;&#22320;&#20351;&#27169;&#22411;&#34892;&#20026;&#19982;&#20154;&#31867;&#24847;&#22270;&#19968;&#33268;&#12290;&#28982;&#32780;&#65292;&#36807;&#24230;&#20248;&#21270;&#20351;&#29992;&#36825;&#20123;&#22870;&#21169;&#27169;&#22411;&#65292;&#20316;&#20026;&#31616;&#21333;&#30340;&#26367;&#20195;&#30446;&#26631;&#65292;&#21487;&#33021;&#20250;&#25439;&#23475;&#32454;&#21270;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#36825;&#19968;&#29616;&#35937;&#34987;&#31216;&#20026;&#22870;&#21169;&#36807;&#24230;&#20248;&#21270;&#12290;&#20026;&#28145;&#20837;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Text-Image Alignment Assessment (TIA2)&#22522;&#20934;&#27979;&#35797;&#65292;&#23427;&#21253;&#25324;&#19968;&#31995;&#21015;&#25991;&#26412;&#25552;&#31034;&#12289;&#22270;&#20687;&#21644;&#20154;&#31867;&#27880;&#37322;&#12290;&#25105;&#20204;&#22312;&#35813;&#22522;&#20934;&#27979;&#35797;&#19978;&#35780;&#20272;&#20102;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#22870;&#21169;&#27169;&#22411;&#65292;&#21457;&#29616;&#23427;&#20204;&#32463;&#24120;&#19982;&#20154;&#31867;&#35780;&#20272;&#19981;&#19968;&#33268;&#12290;&#25105;&#20204;&#20174;&#23454;&#35777;&#35282;&#24230;&#35777;&#26126;&#65292;&#24403;&#20351;&#29992;&#19968;&#20010;&#19982;&#20154;&#31867;&#35780;&#20272;&#19981;&#19968;&#33268;&#30340;&#22870;&#21169;&#27169;&#22411;&#20316;&#20026;&#24494;&#35843;&#30446;&#26631;&#26102;&#65292;&#22870;&#21169;&#36807;&#24230;&#20248;&#21270;&#23588;&#20026;&#26126;&#26174;&#12290;&#20026;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TextNorm&#65292;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#23427;&#22522;&#20110;&#20272;&#35745;&#30340;&#22870;&#21169;&#27169;&#22411;&#32622;&#20449;&#24230;&#26469;&#22686;&#24378;&#23545;&#40784;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01863v1 Announce Type: cross  Abstract: Fine-tuning text-to-image models with reward functions trained on human feedback data has proven effective for aligning model behavior with human intent. However, excessive optimization with such reward models, which serve as mere proxy objectives, can compromise the performance of fine-tuned models, a phenomenon known as reward overoptimization. To investigate this issue in depth, we introduce the Text-Image Alignment Assessment (TIA2) benchmark, which comprises a diverse collection of text prompts, images, and human annotations. Our evaluation of several state-of-the-art reward models on this benchmark reveals their frequent misalignment with human assessment. We empirically demonstrate that overoptimization occurs notably when a poorly aligned reward model is used as the fine-tuning objective. To address this, we propose TextNorm, a simple method that enhances alignment based on a measure of reward model confidence estimated across 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#30740;&#31350;&#21457;&#29616;&#35838;&#31243;&#35780;&#20215;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#65292;&#27604;&#36739;&#33521;&#35821;&#21644;&#29790;&#20856;&#35838;&#31243;&#25968;&#25454;&#65292;&#25581;&#31034;&#20102;&#23398;&#29983;&#23545;&#35838;&#31243;&#30340;&#35780;&#20215;&#20027;&#35266;&#24615;&#22240;&#32771;&#23448;&#24615;&#21035;&#32780;&#23384;&#22312;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2404.01857</link><description>&lt;p&gt;
&#22312;&#35838;&#31243;&#35780;&#20215;&#20013;&#26816;&#27979;&#24615;&#21035;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Detecting Gender Bias in Course Evaluations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01857
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#30740;&#31350;&#21457;&#29616;&#35838;&#31243;&#35780;&#20215;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#65292;&#27604;&#36739;&#33521;&#35821;&#21644;&#29790;&#20856;&#35838;&#31243;&#25968;&#25454;&#65292;&#25581;&#31034;&#20102;&#23398;&#29983;&#23545;&#35838;&#31243;&#30340;&#35780;&#20215;&#20027;&#35266;&#24615;&#22240;&#32771;&#23448;&#24615;&#21035;&#32780;&#23384;&#22312;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#26159;&#19968;&#31687;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20174;&#30805;&#22763;&#35770;&#25991;&#30740;&#31350;&#20013;&#21457;&#29616;&#35838;&#31243;&#35780;&#20215;&#20013;&#24615;&#21035;&#20559;&#35265;&#30340;&#25130;&#22270;&#12290;&#25105;&#20204;&#20351;&#29992;&#19981;&#21516;&#26041;&#27861;&#26469;&#26816;&#27979;&#21644;&#25506;&#32034;&#25968;&#25454;&#65292;&#21457;&#29616;&#23398;&#29983;&#23545;&#35838;&#31243;&#30340;&#35780;&#20215;&#20250;&#22240;&#32771;&#23448;&#30340;&#24615;&#21035;&#32780;&#26377;&#25152;&#19981;&#21516;&#12290;&#35780;&#20272;&#21644;&#27604;&#36739;&#20102;&#26469;&#33258;&#33521;&#35821;&#21644;&#29790;&#20856;&#35838;&#31243;&#30340;&#25968;&#25454;&#65292;&#20197;&#25429;&#25417;&#21487;&#33021;&#23384;&#22312;&#30340;&#24615;&#21035;&#20559;&#35265;&#20013;&#26356;&#22810;&#30340;&#32454;&#24494;&#24046;&#21035;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#23637;&#31034;&#20102;&#21040;&#30446;&#21069;&#20026;&#27490;&#30340;&#24037;&#20316;&#32467;&#26524;&#65292;&#20294;&#36825;&#26159;&#19968;&#20010;&#27491;&#22312;&#36827;&#34892;&#30340;&#39033;&#30446;&#65292;&#36824;&#26377;&#26356;&#22810;&#30340;&#24037;&#20316;&#35201;&#20570;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01857v1 Announce Type: cross  Abstract: An outtake from the findnings of a master thesis studying gender bias in course evaluations through the lense of machine learning and nlp. We use different methods to examine and explore the data and find differences in what students write about courses depending on gender of the examiner. Data from English and Swedish courses are evaluated and compared, in order to capture more nuance in the gender bias that might be found. Here we present the results from the work so far, but this is an ongoing project and there is more work to do.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#25104;&#23545;&#30456;&#20284;&#24230;&#20998;&#24067;&#32858;&#31867;&#65288;PSDC&#65289;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#20013;&#36873;&#21462;&#24178;&#20928;&#26679;&#26412;&#38598;&#21644;&#22024;&#26434;&#26679;&#26412;&#38598;&#65292;&#36741;&#21161;&#36827;&#34892;&#21322;&#30417;&#30563;&#23398;&#20064;&#65292;&#20174;&#32780;&#36827;&#19968;&#27493;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;</title><link>https://arxiv.org/abs/2404.01853</link><description>&lt;p&gt;
&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#30340;&#25104;&#23545;&#30456;&#20284;&#24230;&#20998;&#24067;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Pairwise Similarity Distribution Clustering for Noisy Label Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01853
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#25104;&#23545;&#30456;&#20284;&#24230;&#20998;&#24067;&#32858;&#31867;&#65288;PSDC&#65289;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#20013;&#36873;&#21462;&#24178;&#20928;&#26679;&#26412;&#38598;&#21644;&#22024;&#26434;&#26679;&#26412;&#38598;&#65292;&#36741;&#21161;&#36827;&#34892;&#21322;&#30417;&#30563;&#23398;&#20064;&#65292;&#20174;&#32780;&#36827;&#19968;&#27493;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#26088;&#22312;&#20351;&#29992;&#22823;&#37327;&#24102;&#26377;&#38169;&#35823;&#26631;&#31614;&#30340;&#26679;&#26412;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#20027;&#35201;&#25361;&#25112;&#22312;&#20110;&#22914;&#20309;&#22788;&#29702;&#30001;&#38169;&#35823;&#26631;&#31614;&#24341;&#36215;&#30340;&#19981;&#20934;&#30830;&#30417;&#30563;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26679;&#26412;&#36873;&#25321;&#31639;&#27861;&#65292;&#31216;&#20026;&#25104;&#23545;&#30456;&#20284;&#24230;&#20998;&#24067;&#32858;&#31867;&#65288;PSDC&#65289;&#65292;&#23558;&#35757;&#32451;&#26679;&#26412;&#20998;&#20026;&#19968;&#20010;&#24178;&#20928;&#38598;&#21644;&#21478;&#19968;&#20010;&#22024;&#26434;&#38598;&#65292;&#21487;&#20197;&#20026;&#20219;&#20309;&#29616;&#25104;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#26696;&#25552;&#20379;&#21160;&#21147;&#65292;&#20197;&#36827;&#19968;&#27493;&#20026;&#19981;&#21516;&#19979;&#28216;&#20219;&#21153;&#35757;&#32451;&#32593;&#32476;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#29992;&#26679;&#26412;&#23545;&#20043;&#38388;&#30340;&#25104;&#23545;&#30456;&#20284;&#24230;&#34920;&#31034;&#26679;&#26412;&#32467;&#26500;&#65292;&#24182;&#20351;&#29992;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65288;GMM&#65289;&#26469;&#24314;&#27169;&#23646;&#20110;&#21516;&#19968;&#22024;&#26434;&#38598;&#30340;&#26679;&#26412;&#23545;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#20998;&#24067;&#65292;&#20174;&#32780;&#27599;&#20010;sa
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01853v1 Announce Type: new  Abstract: Noisy label learning aims to train deep neural networks using a large amount of samples with noisy labels, whose main challenge comes from how to deal with the inaccurate supervision caused by wrong labels. Existing works either take the label correction or sample selection paradigm to involve more samples with accurate labels into the training process. In this paper, we propose a simple yet effective sample selection algorithm, termed as Pairwise Similarity Distribution Clustering~(PSDC), to divide the training samples into one clean set and another noisy set, which can power any of the off-the-shelf semi-supervised learning regimes to further train networks for different downstream tasks. Specifically, we take the pairwise similarity between sample pairs to represent the sample structure, and the Gaussian Mixture Model~(GMM) to model the similarity distribution between sample pairs belonging to the same noisy cluster, therefore each sa
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31232;&#30095;&#30697;&#38453;&#20056;&#27861;&#65292;&#32467;&#21512;&#20004;&#31181;&#26032;&#25216;&#26415;&#21644;&#27169;&#22411;&#24494;&#35843;&#65292;&#30740;&#31350;&#20102;&#21152;&#36895;Transformer&#39044;&#35757;&#32451;&#20013;&#21069;&#39304;&#32593;&#32476;&#30340;&#21487;&#34892;&#24615;&#65292;&#20197;&#21450;&#36890;&#36807;&#35745;&#31639;2:4&#25513;&#30721;&#21644;&#20943;&#23569;GPU L2&#32531;&#23384;&#26469;&#23454;&#29616;&#35757;&#32451;&#21152;&#36895;&#12290;</title><link>https://arxiv.org/abs/2404.01847</link><description>&lt;p&gt;
&#20351;&#29992;2:4&#31232;&#30095;&#24615;&#21152;&#36895;Transformer&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Accelerating Transformer Pre-Training with 2:4 Sparsity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01847
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31232;&#30095;&#30697;&#38453;&#20056;&#27861;&#65292;&#32467;&#21512;&#20004;&#31181;&#26032;&#25216;&#26415;&#21644;&#27169;&#22411;&#24494;&#35843;&#65292;&#30740;&#31350;&#20102;&#21152;&#36895;Transformer&#39044;&#35757;&#32451;&#20013;&#21069;&#39304;&#32593;&#32476;&#30340;&#21487;&#34892;&#24615;&#65292;&#20197;&#21450;&#36890;&#36807;&#35745;&#31639;2:4&#25513;&#30721;&#21644;&#20943;&#23569;GPU L2&#32531;&#23384;&#26469;&#23454;&#29616;&#35757;&#32451;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#22823;&#22411;Transformer&#24456;&#24930;&#65292;&#20294;&#26368;&#36817;GPU&#26550;&#26500;&#30340;&#21019;&#26032;&#20351;&#25105;&#20204;&#21344;&#25454;&#20248;&#21183;&#12290;NVIDIA&#30340;Ampere GPU&#21487;&#20197;&#27604;&#20854;&#23494;&#38598;&#31561;&#20215;&#29289;&#24555;&#20004;&#20493;&#30340;&#36895;&#24230;&#25191;&#34892;&#32454;&#31890;&#24230;&#30340;2:4&#31232;&#30095;&#30697;&#38453;&#20056;&#27861;&#12290;&#22312;&#27492;&#24615;&#36136;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#20840;&#38754;&#35843;&#26597;&#20102;&#21152;&#36895;Transformer&#39044;&#35757;&#32451;&#20013;&#21069;&#39304;&#32593;&#32476;&#65288;FFNs&#65289;&#30340;&#21487;&#34892;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#8220;&#32763;&#36716;&#29575;&#8221;&#26469;&#30417;&#35270;2:4&#35757;&#32451;&#36807;&#31243;&#30340;&#31283;&#23450;&#24615;&#12290;&#21033;&#29992;&#36825;&#19968;&#25351;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#25216;&#26415;&#26469;&#20445;&#25345;&#20934;&#30830;&#24615;&#65306;&#36890;&#36807;&#22312;&#26799;&#24230;&#19978;&#24212;&#29992;&#25513;&#30721;&#34928;&#20943;&#39033;&#20462;&#25913;&#31232;&#30095;&#31934;&#21270;&#30340;&#30452;&#36890;&#20272;&#35745;&#22120;&#65292;&#24182;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#32467;&#26463;&#26102;&#38468;&#36817;&#24212;&#29992;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#23494;&#38598;&#24494;&#35843;&#36807;&#31243;&#26469;&#25552;&#39640;&#27169;&#22411;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#31181;&#26377;&#25928;&#30340;&#25216;&#26415;&#26469;&#23454;&#38469;&#21152;&#36895;&#35757;&#32451;&#65306;&#36890;&#36807;&#21367;&#31215;&#35745;&#31639;&#21487;&#36716;&#32622;&#30340;2:4&#25513;&#30721;&#65292;&#20197;&#21450;&#36890;&#36807;&#20943;&#23569;GPU L2&#32531;&#23384;&#26469;&#21152;&#36895;&#38376;&#25511;&#28608;&#27963;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01847v1 Announce Type: new  Abstract: Training large Transformers is slow, but recent innovations on GPU architecture gives us an advantage. NVIDIA Ampere GPUs can execute a fine-grained 2:4 sparse matrix multiplication twice as fast as its dense equivalent. In the light of this property, we comprehensively investigate the feasibility of accelerating feed-forward networks (FFNs) of Transformers in pre-training. First, we define a "flip rate" to monitor the stability of a 2:4 training process. Utilizing this metric, we suggest two techniques to preserve accuracy: to modify the sparse-refined straight-through estimator by applying the mask decay term on gradients, and to enhance the model's quality by a simple yet effective dense fine-tuning procedure near the end of pre-training. Besides, we devise two effective techniques to practically accelerate training: to calculate transposable 2:4 mask by convolution, and to accelerate gated activation functions by reducing GPU L2 cach
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#23637;&#31034;&#20102;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#19968;&#31181;&#27969;&#34892;&#30340;&#38750;&#21442;&#25968;&#26041;&#27861;&#8212;&#8212;&#22238;&#24402;&#26641;&#19978;&#65292;&#23376;&#25277;&#26679;&#32858;&#21512;&#65288;subagging&#65289;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21457;&#29616;&#23545;&#20110;&#20219;&#20309;&#32473;&#23450;&#30340;&#20998;&#35010;&#25968;&#65292;subagging&#37117;&#21487;&#20197;&#20248;&#20110;&#21333;&#26869;&#26641;&#65292;&#24182;&#19988;&#22312;&#36739;&#22810;&#20998;&#35010;&#30340;&#24773;&#20917;&#19979;&#25913;&#36827;&#26356;&#22823;&#12290;</title><link>https://arxiv.org/abs/2404.01832</link><description>&lt;p&gt;
&#20309;&#26102;&#20351;&#29992;Subagging&#65311;
&lt;/p&gt;
&lt;p&gt;
When does Subagging Work?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01832
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#23637;&#31034;&#20102;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#19968;&#31181;&#27969;&#34892;&#30340;&#38750;&#21442;&#25968;&#26041;&#27861;&#8212;&#8212;&#22238;&#24402;&#26641;&#19978;&#65292;&#23376;&#25277;&#26679;&#32858;&#21512;&#65288;subagging&#65289;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21457;&#29616;&#23545;&#20110;&#20219;&#20309;&#32473;&#23450;&#30340;&#20998;&#35010;&#25968;&#65292;subagging&#37117;&#21487;&#20197;&#20248;&#20110;&#21333;&#26869;&#26641;&#65292;&#24182;&#19988;&#22312;&#36739;&#22810;&#20998;&#35010;&#30340;&#24773;&#20917;&#19979;&#25913;&#36827;&#26356;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#19968;&#31181;&#27969;&#34892;&#30340;&#38750;&#21442;&#25968;&#26041;&#27861;&#8212;&#8212;&#22238;&#24402;&#26641;&#19978;&#65292;&#23376;&#25277;&#26679;&#32858;&#21512;&#65288;subagging&#65289;&#30340;&#26377;&#25928;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#26641;&#30340;&#36880;&#28857;&#19968;&#33268;&#24615;&#30340;&#20805;&#20998;&#26465;&#20214;&#12290;&#25105;&#20204;&#26126;&#30830;&#20102;&#65288;i&#65289;&#20559;&#24046;&#21462;&#20915;&#20110;&#21333;&#20803;&#30340;&#30452;&#24452;&#65292;&#22240;&#27492;&#65292;&#20855;&#26377;&#23569;&#25968;&#20998;&#35010;&#30340;&#26641;&#20542;&#21521;&#20110;&#23384;&#22312;&#20559;&#24046;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#26041;&#24046;&#21462;&#20915;&#20110;&#21333;&#20803;&#20013;&#30340;&#35266;&#27979;&#25968;&#37327;&#65292;&#22240;&#27492;&#65292;&#20855;&#26377;&#35768;&#22810;&#20998;&#35010;&#30340;&#26641;&#20542;&#21521;&#20110;&#20855;&#26377;&#36739;&#22823;&#30340;&#26041;&#24046;&#12290;&#34429;&#28982;&#36825;&#20123;&#20851;&#20110;&#20559;&#24046;&#21644;&#26041;&#24046;&#30340;&#38472;&#36848;&#22312;&#21327;&#21464;&#37327;&#31354;&#38388;&#20013;&#26159;&#20840;&#23616;&#36866;&#29992;&#30340;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#65292;&#22312;&#26576;&#20123;&#32422;&#26463;&#26465;&#20214;&#19979;&#65292;&#23427;&#20204;&#22312;&#23616;&#37096;&#20063;&#26159;&#25104;&#31435;&#30340;&#12290;&#31532;&#20108;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#23376;&#25277;&#26679;&#32858;&#21512;&#21644;&#20855;&#26377;&#19981;&#21516;&#20998;&#35010;&#25968;&#30340;&#26641;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23545;&#20110;&#20219;&#20309;&#32473;&#23450;&#30340;&#20998;&#35010;&#25968;&#65292;&#23376;&#25277;&#26679;&#32858;&#21512;&#37117;&#20248;&#20110;&#21333;&#26869;&#26641;&#65292;&#24182;&#19988;&#36825;&#31181;&#25913;&#36827;&#22312;&#36739;&#22810;&#20998;&#35010;&#30340;&#24773;&#20917;&#19979;&#27604;&#36739;&#23569;&#20998;&#35010;&#30340;&#24773;&#20917;&#19979;&#26356;&#22823;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#20197;&#26368;&#20339;&#22823;&#23567;&#29983;&#38271;&#30340;&#21333;&#26869;&#26641;&#21487;&#20197;&#20248;&#20110;&#23376;&#25277;&#26679;&#32858;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01832v1 Announce Type: cross  Abstract: We study the effectiveness of subagging, or subsample aggregating, on regression trees, a popular non-parametric method in machine learning. First, we give sufficient conditions for pointwise consistency of trees. We formalize that (i) the bias depends on the diameter of cells, hence trees with few splits tend to be biased, and (ii) the variance depends on the number of observations in cells, hence trees with many splits tend to have large variance. While these statements for bias and variance are known to hold globally in the covariate space, we show that, under some constraints, they are also true locally. Second, we compare the performance of subagging to that of trees across different numbers of splits. We find that (1) for any given number of splits, subagging improves upon a single tree, and (2) this improvement is larger for many splits than it is for few splits. However, (3) a single tree grown at optimal size can outperform su
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#26410;&#30693;&#35760;&#24405;&#31574;&#30053;&#21644;&#20215;&#20540;&#20989;&#25968;&#30340;&#21452;&#37325;&#31283;&#20581;&#31163;&#32447;&#35780;&#20272;&#20272;&#35745;&#22120;DRUnknown&#65292;&#23454;&#29616;&#20102;&#26368;&#23567;&#28176;&#36817;&#26041;&#24046;&#21644;&#21322;&#21442;&#25968;&#19979;&#30028;&#19979;&#26368;&#20339;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.01830</link><description>&lt;p&gt;
&#20272;&#35745;&#35760;&#24405;&#31574;&#30053;&#30340;&#21452;&#37325;&#31283;&#20581;&#31163;&#32447;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Doubly-Robust Off-Policy Evaluation with Estimated Logging Policy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01830
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#26410;&#30693;&#35760;&#24405;&#31574;&#30053;&#21644;&#20215;&#20540;&#20989;&#25968;&#30340;&#21452;&#37325;&#31283;&#20581;&#31163;&#32447;&#35780;&#20272;&#20272;&#35745;&#22120;DRUnknown&#65292;&#23454;&#29616;&#20102;&#26368;&#23567;&#28176;&#36817;&#26041;&#24046;&#21644;&#21322;&#21442;&#25968;&#19979;&#30028;&#19979;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#26032;&#39062;&#30340;&#21452;&#37325;&#31283;&#20581;&#65288;DR&#65289;&#31163;&#32447;&#35780;&#20272;&#65288;OPE&#65289;&#20272;&#35745;&#22120;DRUnknown&#65292;&#26088;&#22312;&#24212;&#23545;&#35760;&#24405;&#31574;&#30053;&#21644;&#20215;&#20540;&#20989;&#25968;&#22343;&#26410;&#30693;&#30340;&#24773;&#20917;&#12290;&#35813;&#20272;&#35745;&#22120;&#39318;&#20808;&#20272;&#35745;&#35760;&#24405;&#31574;&#30053;&#65292;&#28982;&#21518;&#36890;&#36807;&#26368;&#23567;&#21270;&#20272;&#35745;&#22120;&#30340;&#28176;&#36817;&#26041;&#24046;&#26469;&#20272;&#35745;&#20215;&#20540;&#20989;&#25968;&#27169;&#22411;&#65292;&#21516;&#26102;&#32771;&#34385;&#35760;&#24405;&#31574;&#30053;&#30340;&#20272;&#35745;&#25928;&#26524;&#12290;&#24403;&#35760;&#24405;&#31574;&#30053;&#27169;&#22411;&#27491;&#30830;&#25351;&#23450;&#26102;&#65292;DRUnknown&#22312;&#29616;&#26377;OPE&#20272;&#35745;&#22120;&#31867;&#20013;&#36798;&#21040;&#26368;&#23567;&#30340;&#28176;&#36817;&#26041;&#24046;&#12290;&#24403;&#20215;&#20540;&#20989;&#25968;&#27169;&#22411;&#20063;&#34987;&#27491;&#30830;&#25351;&#23450;&#26102;&#65292;DRUnknown&#26159;&#26368;&#20248;&#30340;&#65292;&#22240;&#20026;&#23427;&#30340;&#28176;&#36817;&#26041;&#24046;&#36798;&#21040;&#20102;&#21322;&#21442;&#25968;&#19979;&#30028;&#12290;&#25105;&#20204;&#22312;&#24773;&#22659;&#33218;&#21644;&#24378;&#21270;&#23398;&#20064;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#32467;&#26524;&#65292;&#27604;&#36739;&#20102;DRUnknown&#19982;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01830v1 Announce Type: cross  Abstract: We introduce a novel doubly-robust (DR) off-policy evaluation (OPE) estimator for Markov decision processes, DRUnknown, designed for situations where both the logging policy and the value function are unknown. The proposed estimator initially estimates the logging policy and then estimates the value function model by minimizing the asymptotic variance of the estimator while considering the estimating effect of the logging policy. When the logging policy model is correctly specified, DRUnknown achieves the smallest asymptotic variance within the class containing existing OPE estimators. When the value function model is also correctly specified, DRUnknown is optimal as its asymptotic variance reaches the semiparametric lower bound. We present experimental results conducted in contextual bandits and reinforcement learning to compare the performance of DRUnknown with that of existing methods.
&lt;/p&gt;</description></item><item><title>&#25345;&#32493;&#23545;&#25239;&#24615;&#38450;&#24481;&#27010;&#24565;&#19979;&#25552;&#20986;&#20102;&#21508;&#21521;&#24322;&#24615;&#21644;&#21508;&#21521;&#21516;&#24615;&#20266;&#37325;&#28436;&#65288;AIR&#65289;&#65292;&#36890;&#36807;&#21508;&#21521;&#21516;&#24615;&#37325;&#28436;&#20445;&#25345;&#27169;&#22411;&#19968;&#33268;&#24615;&#65292;&#22312;&#21508;&#21521;&#24322;&#24615;&#37325;&#28436;&#20013;&#23398;&#20064;&#25240;&#34935;&#25968;&#25454;&#27969;&#24418;&#12290;</title><link>https://arxiv.org/abs/2404.01828</link><description>&lt;p&gt;
&#19981;&#24536;&#38450;&#24481;&#65306;&#21508;&#21521;&#24322;&#24615;&#19982;&#21508;&#21521;&#21516;&#24615;&#20266;&#37325;&#28436;&#30340;&#25345;&#32493;&#23545;&#25239;&#24615;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Defense without Forgetting: Continual Adversarial Defense with Anisotropic &amp; Isotropic Pseudo Replay
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01828
&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23545;&#25239;&#24615;&#38450;&#24481;&#27010;&#24565;&#19979;&#25552;&#20986;&#20102;&#21508;&#21521;&#24322;&#24615;&#21644;&#21508;&#21521;&#21516;&#24615;&#20266;&#37325;&#28436;&#65288;AIR&#65289;&#65292;&#36890;&#36807;&#21508;&#21521;&#21516;&#24615;&#37325;&#28436;&#20445;&#25345;&#27169;&#22411;&#19968;&#33268;&#24615;&#65292;&#22312;&#21508;&#21521;&#24322;&#24615;&#37325;&#28436;&#20013;&#23398;&#20064;&#25240;&#34935;&#25968;&#25454;&#27969;&#24418;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#34920;&#29616;&#20986;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#26131;&#21463;&#24615;&#12290;&#23545;&#25239;&#24615;&#38450;&#24481;&#25216;&#26415;&#36890;&#24120;&#19987;&#27880;&#20110;&#19968;&#27425;&#24615;&#35774;&#32622;&#65292;&#20197;&#20445;&#25345;&#23545;&#25239;&#25915;&#20987;&#30340;&#31283;&#20581;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#37096;&#32626;&#22330;&#26223;&#20013;&#65292;&#26032;&#30340;&#25915;&#20987;&#21487;&#33021;&#20250;&#36830;&#32493;&#20986;&#29616;&#12290;&#22240;&#27492;&#65292;&#38450;&#24481;&#27169;&#22411;&#19981;&#26029;&#36866;&#24212;&#26032;&#30340;&#25915;&#20987;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#20294;&#36866;&#24212;&#36807;&#31243;&#21487;&#33021;&#20250;&#23548;&#33268;&#23545;&#20808;&#21069;&#36827;&#34892;&#38450;&#24481;&#30340;&#25915;&#20987;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#35752;&#35770;&#20102;&#22312;&#19968;&#31995;&#21015;&#25915;&#20987;&#19979;&#30340;&#25345;&#32493;&#23545;&#25239;&#24615;&#38450;&#24481;&#30340;&#27010;&#24565;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21508;&#21521;&#24322;&#24615;&#19982;&#21508;&#21521;&#21516;&#24615;&#37325;&#28436;&#65288;AIR&#65289;&#30340;&#32456;&#36523;&#38450;&#24481;&#22522;&#32447;&#65292;&#20855;&#26377;&#19977;&#20010;&#20248;&#21183;&#65306;&#65288;1&#65289;&#21508;&#21521;&#21516;&#24615;&#37325;&#28436;&#30830;&#20445;&#27169;&#22411;&#22312;&#26032;&#25968;&#25454;&#30340;&#37051;&#22495;&#20998;&#24067;&#20013;&#20445;&#25345;&#19968;&#33268;&#24615;&#65292;&#38388;&#25509;&#22320;&#23545;&#40784;&#20102;&#26087;&#20219;&#21153;&#21644;&#26032;&#20219;&#21153;&#20043;&#38388;&#30340;&#36755;&#20986;&#20559;&#22909;&#12290; (2) &#21508;&#21521;&#24322;&#24615;&#37325;&#28436;&#20351;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#19968;&#20010;&#24102;&#26377;&#26032;&#28151;&#21512;&#35821;&#20041;&#30340;&#25240;&#34935;&#25968;&#25454;&#27969;&#24418;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01828v1 Announce Type: cross  Abstract: Deep neural networks have demonstrated susceptibility to adversarial attacks. Adversarial defense techniques often focus on one-shot setting to maintain robustness against attack. However, new attacks can emerge in sequences in real-world deployment scenarios. As a result, it is crucial for a defense model to constantly adapt to new attacks, but the adaptation process can lead to catastrophic forgetting of previously defended against attacks. In this paper, we discuss for the first time the concept of continual adversarial defense under a sequence of attacks, and propose a lifelong defense baseline called Anisotropic \&amp; Isotropic Replay (AIR), which offers three advantages: (1) Isotropic replay ensures model consistency in the neighborhood distribution of new data, indirectly aligning the output preference between old and new tasks. (2) Anisotropic replay enables the model to learn a compromise data manifold with fresh mixed semantics 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#38754;&#21521;&#24694;&#24847;&#20869;&#23481;&#26816;&#27979;&#31038;&#21306;&#27169;&#22411;&#27867;&#21270;&#30340;&#26356;&#20026;&#29616;&#23454;&#30340;&#35780;&#20272;&#35774;&#32622;&#65292;&#36890;&#36807;&#23569;&#26679;&#26412;&#23376;&#22270;&#37319;&#26679;&#26041;&#27861;&#27979;&#35797;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.01822</link><description>&lt;p&gt;
&#38754;&#21521;&#24694;&#24847;&#20869;&#23481;&#26816;&#27979;&#31038;&#21306;&#27169;&#22411;&#27867;&#21270;&#30340;&#26356;&#20026;&#29616;&#23454;&#30340;&#35780;&#20272;&#35774;&#32622;
&lt;/p&gt;
&lt;p&gt;
A (More) Realistic Evaluation Setup for Generalisation of Community Models on Malicious Content Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01822
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#38754;&#21521;&#24694;&#24847;&#20869;&#23481;&#26816;&#27979;&#31038;&#21306;&#27169;&#22411;&#27867;&#21270;&#30340;&#26356;&#20026;&#29616;&#23454;&#30340;&#35780;&#20272;&#35774;&#32622;&#65292;&#36890;&#36807;&#23569;&#26679;&#26412;&#23376;&#22270;&#37319;&#26679;&#26041;&#27861;&#27979;&#35797;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#20110;&#24694;&#24847;&#20869;&#23481;&#26816;&#27979;&#30340;&#31038;&#21306;&#27169;&#22411;&#32771;&#34385;&#31038;&#20132;&#22270;&#20013;&#30340;&#19978;&#19979;&#25991;&#20197;&#21450;&#20869;&#23481;&#26412;&#36523;&#65292;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#34394;&#20551;&#20449;&#24687;&#21644;&#20167;&#24680;&#35328;&#35770;&#20173;&#22312;&#31038;&#20132;&#23186;&#20307;&#32593;&#32476;&#19978;&#20256;&#25773;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#25105;&#20204;&#30340;&#23569;&#26679;&#26412;&#23376;&#22270;&#37319;&#26679;&#26041;&#27861;&#30340;&#27169;&#22411;&#27867;&#21270;&#30340;&#26032;&#22411;&#35780;&#20272;&#35774;&#32622;&#12290;&#36825;&#20010;&#35774;&#32622;&#36890;&#36807;&#22312;&#26356;&#22823;&#22270;&#30340;&#23616;&#37096;&#25506;&#32034;&#20013;&#23569;&#37327;&#26631;&#35760;&#31034;&#20363;&#65292;&#27169;&#25311;&#26356;&#20026;&#29616;&#23454;&#30340;&#24212;&#29992;&#22330;&#26223;&#65292;&#20197;&#27979;&#35797;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01822v1 Announce Type: cross  Abstract: Community models for malicious content detection, which take into account the context from a social graph alongside the content itself, have shown remarkable performance on benchmark datasets. Yet, misinformation and hate speech continue to propagate on social media networks. This mismatch can be partially attributed to the limitations of current evaluation setups that neglect the rapid evolution of online content and the underlying social graph. In this paper, we propose a novel evaluation setup for model generalisation based on our few-shot subgraph sampling approach. This setup tests for generalisation through few labelled examples in local explorations of a larger graph, emulating more realistic application settings. We show this to be a challenging inductive setup, wherein strong performance on the training graph is not indicative of performance on unseen tasks, domains, or graph structures. Lastly, we show that graph meta-learner
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#35774;&#35745;&#20986;&#30340;&#28151;&#21512;&#31995;&#32479;&#27169;&#22411;&#20855;&#26377;&#20998;&#27573;&#32447;&#24615;&#21160;&#21147;&#23398;&#65292;&#21487;&#20197;&#29992;&#20110;&#20248;&#21270;&#25511;&#21046;&#35774;&#35745;&#65292;&#24182;&#19988;&#22312;&#26377;&#38480;&#35270;&#37326;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#20013;&#35745;&#31639;&#20986;&#20855;&#26377;&#24378;&#23616;&#37096;&#26368;&#20248;&#24615;&#20445;&#35777;&#30340;&#26368;&#20248;&#35299;&#12290;</title><link>https://arxiv.org/abs/2404.01814</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#28151;&#21512;&#31995;&#32479;&#35782;&#21035;&#25511;&#21046;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A neural network-based approach to hybrid systems identification for control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01814
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#35774;&#35745;&#20986;&#30340;&#28151;&#21512;&#31995;&#32479;&#27169;&#22411;&#20855;&#26377;&#20998;&#27573;&#32447;&#24615;&#21160;&#21147;&#23398;&#65292;&#21487;&#20197;&#29992;&#20110;&#20248;&#21270;&#25511;&#21046;&#35774;&#35745;&#65292;&#24182;&#19988;&#22312;&#26377;&#38480;&#35270;&#37326;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#20013;&#35745;&#31639;&#20986;&#20855;&#26377;&#24378;&#23616;&#37096;&#26368;&#20248;&#24615;&#20445;&#35777;&#30340;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#35774;&#35745;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#27169;&#22411;&#30340;&#38382;&#39064;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#20174;&#26377;&#38480;&#25968;&#37327;&#30340;(&#29366;&#24577;-&#36755;&#20837;)-&#21518;&#32487;&#29366;&#24577;&#25968;&#25454;&#28857;&#20013;&#35782;&#21035;&#26410;&#30693;&#21160;&#24577;&#31995;&#32479;&#65292;&#24182;&#19988;&#35813;&#27169;&#22411;&#36866;&#29992;&#20110;&#20248;&#21270;&#25511;&#21046;&#35774;&#35745;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#23450;&#30340;&#31070;&#32463;&#32593;&#32476;(NN)&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#20135;&#29983;&#20855;&#26377;&#20998;&#27573;&#32447;&#24615;&#21160;&#21147;&#23398;&#30340;&#28151;&#21512;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#23545;&#32593;&#32476;&#21442;&#25968;&#20855;&#26377;&#21487;&#24494;&#24615;&#65292;&#20174;&#32780;&#20351;&#24471;&#21487;&#20197;&#20351;&#29992;&#22522;&#20110;&#23548;&#25968;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;NN&#26435;&#37325;&#30340;&#31934;&#24515;&#36873;&#25321;&#21487;&#20197;&#20135;&#29983;&#20855;&#26377;&#38750;&#24120;&#26377;&#21033;&#32467;&#26500;&#23646;&#24615;&#30340;&#28151;&#21512;&#31995;&#32479;&#27169;&#22411;&#65292;&#24403;&#20316;&#20026;&#26377;&#38480;&#35270;&#37326;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;(OCP)&#30340;&#19968;&#37096;&#20998;&#20351;&#29992;&#26102;&#65292;&#20855;&#26377;&#24456;&#24378;&#30340;&#23616;&#37096;&#26368;&#20248;&#24615;&#20445;&#35777;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#20197;&#36890;&#36807;&#38750;&#32447;&#24615;&#35268;&#21010;&#35745;&#31639;&#20855;&#26377;&#24378;&#23616;&#37096;&#26368;&#20248;&#24615;&#20445;&#35777;&#30340;&#26368;&#20248;&#35299;&#65292;&#19982;&#36890;&#24120;&#38656;&#35201;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;&#30340;&#19968;&#33324;&#28151;&#21512;&#31995;&#32479;&#30340;&#32463;&#20856;OCP&#30456;&#27604;&#12290;&#21478;&#22806;&#65292;&#36825;&#20123;&#27169;&#22411;&#36824;&#21487;&#20197;&#34987;&#29992;&#20110;&#25925;&#38556;&#26816;&#27979;&#21644;&#25925;&#38556;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01814v1 Announce Type: cross  Abstract: We consider the problem of designing a machine learning-based model of an unknown dynamical system from a finite number of (state-input)-successor state data points, such that the model obtained is also suitable for optimal control design. We propose a specific neural network (NN) architecture that yields a hybrid system with piecewise-affine dynamics that is differentiable with respect to the network's parameters, thereby enabling the use of derivative-based training procedures. We show that a careful choice of our NN's weights produces a hybrid system model with structural properties that are highly favourable when used as part of a finite horizon optimal control problem (OCP). Specifically, we show that optimal solutions with strong local optimality guarantees can be computed via nonlinear programming, in contrast to classical OCPs for general hybrid systems which typically require mixed-integer optimization. In addition to being we
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#20004;&#31181;&#24773;&#32490;&#32500;&#24230;&#36827;&#34892;&#24207;&#25968;&#20998;&#31867;&#65292;&#20174;&#32780;&#25913;&#36827;&#25991;&#26412;&#24773;&#24863;&#39044;&#27979;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#21306;&#20998;&#19981;&#21516;&#24773;&#32490;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#21644;&#24046;&#24322;&#24615;</title><link>https://arxiv.org/abs/2404.01805</link><description>&lt;p&gt;
&#20351;&#29992;&#32452;&#21512;&#30340;valence&#21644;arousal&#24207;&#21015;&#20998;&#31867;&#25913;&#36827;&#25991;&#26412;&#24773;&#24863;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Improved Text Emotion Prediction Using Combined Valence and Arousal Ordinal Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01805
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#20004;&#31181;&#24773;&#32490;&#32500;&#24230;&#36827;&#34892;&#24207;&#25968;&#20998;&#31867;&#65292;&#20174;&#32780;&#25913;&#36827;&#25991;&#26412;&#24773;&#24863;&#39044;&#27979;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#21306;&#20998;&#19981;&#21516;&#24773;&#32490;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#21644;&#24046;&#24322;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#25991;&#26412;&#25968;&#25454;&#20013;&#30340;&#24773;&#24863;&#26816;&#27979;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#22240;&#20026;&#36825;&#23545;&#20110;&#24320;&#21457;&#23500;&#26377;&#21516;&#29702;&#24515;&#30340;&#20154;&#26426;&#20132;&#20114;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20174;&#25991;&#26412;&#20013;&#23545;&#24773;&#24863;&#36827;&#34892;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#35748;&#21487;&#24182;&#21306;&#20998;&#20102;&#21508;&#31181;&#24773;&#24863;&#20043;&#38388;&#30340;&#22810;&#26679;&#24615;&#30456;&#20284;&#24615;&#21644;&#24046;&#24322;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#35757;&#32451;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#26469;&#20026;&#26631;&#20934;&#24773;&#24863;&#20998;&#31867;&#24314;&#31435;&#20102;&#19968;&#20010;&#22522;&#32447;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#24182;&#38750;&#25152;&#26377;&#30340;&#38169;&#20998;&#37117;&#21516;&#31561;&#37325;&#35201;&#65292;&#22240;&#20026;&#24773;&#24863;&#31867;&#21035;&#20043;&#38388;&#23384;&#22312;&#30693;&#35273;&#30456;&#20284;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#24773;&#24863;&#26631;&#35760;&#38382;&#39064;&#20174;&#20256;&#32479;&#20998;&#31867;&#27169;&#22411;&#36716;&#21464;&#20026;&#24207;&#25968;&#20998;&#31867;&#27169;&#22411;&#26469;&#37325;&#26032;&#23450;&#20041;&#65292;&#20854;&#20013;&#31163;&#25955;&#30340;&#24773;&#24863;&#26681;&#25454;&#20854;valence&#27700;&#24179;&#34987;&#25490;&#21015;&#25104;&#39034;&#24207;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20108;&#32500;&#24773;&#24863;&#31354;&#38388;&#20013;&#25191;&#34892;&#24207;&#25968;&#20998;&#31867;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01805v1 Announce Type: new  Abstract: Emotion detection in textual data has received growing interest in recent years, as it is pivotal for developing empathetic human-computer interaction systems. This paper introduces a method for categorizing emotions from text, which acknowledges and differentiates between the diversified similarities and distinctions of various emotions. Initially, we establish a baseline by training a transformer-based model for standard emotion classification, achieving state-of-the-art performance. We argue that not all misclassifications are of the same importance, as there are perceptual similarities among emotional classes. We thus redefine the emotion labeling problem by shifting it from a traditional classification model to an ordinal classification one, where discrete emotions are arranged in a sequential order according to their valence levels. Finally, we propose a method that performs ordinal classification in the two-dimensional emotion spa
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31995;&#32479;&#35299;&#20915;&#26041;&#26696;&#65292;&#21517;&#20026;&#31070;&#32463;&#24418;&#24577;&#30340;&#26080;&#32447;&#35774;&#22791;-&#36793;&#32536;&#21327;&#21516;&#25512;&#29702;&#65292;&#35774;&#22791;&#20351;&#29992;&#31070;&#32463;&#24418;&#24577;&#30828;&#20214;&#36827;&#34892;&#25968;&#25454;&#22788;&#29702;&#65292;&#36793;&#32536;&#26381;&#21153;&#22120;&#20351;&#29992;&#20256;&#32479;&#25216;&#26415;&#65292;&#20197;&#38477;&#20302;&#36890;&#20449;&#24320;&#38144;&#12290;</title><link>https://arxiv.org/abs/2404.01804</link><description>&lt;p&gt;
&#36890;&#36807;&#26377;&#21521;&#20449;&#24687;&#29942;&#39048;&#36827;&#34892;&#31070;&#32463;&#24418;&#24577;&#30340;&#26080;&#32447;&#35774;&#22791;-&#36793;&#32536;&#21327;&#21516;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Neuromorphic Wireless Device-Edge Co-Inference via the Directed Information Bottleneck
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01804
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31995;&#32479;&#35299;&#20915;&#26041;&#26696;&#65292;&#21517;&#20026;&#31070;&#32463;&#24418;&#24577;&#30340;&#26080;&#32447;&#35774;&#22791;-&#36793;&#32536;&#21327;&#21516;&#25512;&#29702;&#65292;&#35774;&#22791;&#20351;&#29992;&#31070;&#32463;&#24418;&#24577;&#30828;&#20214;&#36827;&#34892;&#25968;&#25454;&#22788;&#29702;&#65292;&#36793;&#32536;&#26381;&#21153;&#22120;&#20351;&#29992;&#20256;&#32479;&#25216;&#26415;&#65292;&#20197;&#38477;&#20302;&#36890;&#20449;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19979;&#19968;&#20195;&#26080;&#32447;&#31995;&#32479;&#30340;&#19968;&#20010;&#37325;&#35201;&#29992;&#20363;&#26159;&#35774;&#22791;-&#36793;&#32536;&#21327;&#21516;&#25512;&#29702;&#65292;&#20854;&#20013;&#35821;&#20041;&#20219;&#21153;&#22312;&#35774;&#22791;&#21644;&#36793;&#32536;&#26381;&#21153;&#22120;&#20043;&#38388;&#21010;&#20998;&#12290;&#35774;&#22791;&#36827;&#34892;&#25968;&#25454;&#25910;&#38598;&#21644;&#25968;&#25454;&#30340;&#37096;&#20998;&#22788;&#29702;&#65292;&#36828;&#31243;&#26381;&#21153;&#22120;&#26681;&#25454;&#20174;&#35774;&#22791;&#25509;&#25910;&#30340;&#20449;&#24687;&#23436;&#25104;&#32473;&#23450;&#20219;&#21153;&#12290;&#36890;&#24120;&#35201;&#27714;&#22312;&#35774;&#22791;&#22788;&#23613;&#21487;&#33021;&#39640;&#25928;&#22320;&#36816;&#34892;&#22788;&#29702;&#21644;&#36890;&#20449;&#65292;&#32780;&#22312;&#36793;&#32536;&#26377;&#26356;&#22810;&#30340;&#35745;&#31639;&#36164;&#28304;&#21487;&#29992;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#31181;&#24773;&#20917;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#31995;&#32479;&#35299;&#20915;&#26041;&#26696;&#65292;&#31216;&#20026;&#31070;&#32463;&#24418;&#24577;&#30340;&#26080;&#32447;&#35774;&#22791;-&#36793;&#32536;&#21327;&#21516;&#25512;&#29702;&#12290;&#26681;&#25454;&#36825;&#20010;&#31995;&#32479;&#65292;&#35774;&#22791;&#20351;&#29992;&#31070;&#32463;&#24418;&#24577;&#30828;&#20214;&#36816;&#34892;&#24863;&#24212;&#12289;&#22788;&#29702;&#21644;&#36890;&#20449;&#21333;&#20803;&#65292;&#32780;&#26381;&#21153;&#22120;&#37319;&#29992;&#20256;&#32479;&#30340;&#26080;&#32447;&#30005;&#21644;&#35745;&#31639;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01804v1 Announce Type: new  Abstract: An important use case of next-generation wireless systems is device-edge co-inference, where a semantic task is partitioned between a device and an edge server. The device carries out data collection and partial processing of the data, while the remote server completes the given task based on information received from the device. It is often required that processing and communication be run as efficiently as possible at the device, while more computing resources are available at the edge. To address such scenarios, we introduce a new system solution, termed neuromorphic wireless device-edge co-inference. According to it, the device runs sensing, processing, and communication units using neuromorphic hardware, while the server employs conventional radio and computing technologies. The proposed system is designed using a transmitter-centric information-theoretic criterion that targets a reduction of the communication overhead, while retain
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20351;&#29992;&#36229;&#20998;&#36776;&#29575;&#20998;&#26512;&#36827;&#34892;&#24223;&#29289;&#20998;&#31867;&#65292;&#25552;&#20986;&#36328;&#22495;&#20998;&#31867;&#21644;&#36229;&#20998;&#36776;&#29575;&#22686;&#24378;&#26469;&#35780;&#20272;&#19981;&#21516;&#22270;&#20687;&#20998;&#36776;&#29575;&#23545;&#24223;&#29289;&#20998;&#31867;&#30340;&#24433;&#21709;&#65292;&#20197;&#24212;&#23545;&#38750;&#27861;&#22635;&#22475;&#22330;&#30340;&#34067;&#24310;&#12290;</title><link>https://arxiv.org/abs/2404.01790</link><description>&lt;p&gt;
&#24223;&#29289;&#20998;&#31867;&#30340;&#36229;&#20998;&#36776;&#29575;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Super-Resolution Analysis for Landfill Waste Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01790
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20351;&#29992;&#36229;&#20998;&#36776;&#29575;&#20998;&#26512;&#36827;&#34892;&#24223;&#29289;&#20998;&#31867;&#65292;&#25552;&#20986;&#36328;&#22495;&#20998;&#31867;&#21644;&#36229;&#20998;&#36776;&#29575;&#22686;&#24378;&#26469;&#35780;&#20272;&#19981;&#21516;&#22270;&#20687;&#20998;&#36776;&#29575;&#23545;&#24223;&#29289;&#20998;&#31867;&#30340;&#24433;&#21709;&#65292;&#20197;&#24212;&#23545;&#38750;&#27861;&#22635;&#22475;&#22330;&#30340;&#34067;&#24310;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#27861;&#22635;&#22475;&#22330;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#30001;&#20110;&#20854;&#23545;&#29615;&#22659;&#12289;&#32463;&#27982;&#21644;&#20844;&#20849;&#21355;&#29983;&#30340;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#33322;&#31354;&#24433;&#20687;&#36827;&#34892;&#29615;&#22659;&#29359;&#32618;&#30417;&#27979;&#12290;&#34429;&#28982;&#20154;&#24037;&#26234;&#33021;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#36827;&#23637;&#32473;&#20154;&#20197;&#24076;&#26395;&#65292;&#20294;&#25361;&#25112;&#22312;&#20110;&#35757;&#32451;&#20855;&#26377;&#39640;&#20998;&#36776;&#29575;&#25991;&#29486;&#25968;&#25454;&#38598;&#30340;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#36866;&#24212;&#20110;&#24320;&#25918;&#24335;&#20302;&#20998;&#36776;&#29575;&#22270;&#20687;&#12290;&#37492;&#20110;&#36136;&#37327;&#24046;&#24322;&#24040;&#22823;&#19988;&#27880;&#37322;&#26377;&#38480;&#65292;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#27169;&#22411;&#22312;&#36825;&#20123;&#39046;&#22495;&#20043;&#38388;&#30340;&#36866;&#24212;&#24615;&#12290;&#20986;&#20110;&#23545;&#24223;&#29289;&#26816;&#27979;&#31639;&#27861;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#30340;&#24517;&#35201;&#24615;&#65292;&#26412;&#30740;&#31350;&#20027;&#24352;&#36328;&#22495;&#20998;&#31867;&#21644;&#36229;&#20998;&#36776;&#29575;&#22686;&#24378;&#65292;&#20197;&#20998;&#26512;&#19981;&#21516;&#22270;&#20687;&#20998;&#36776;&#29575;&#23545;&#24223;&#29289;&#20998;&#31867;&#30340;&#24433;&#21709;&#65292;&#20316;&#20026;&#25171;&#20987;&#38750;&#27861;&#22635;&#22475;&#22330;&#34067;&#24310;&#30340;&#35780;&#20272;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#36890;&#36807;&#25552;&#39640;&#22270;&#20687;&#36136;&#37327;&#21487;&#20197;&#25913;&#21892;&#24615;&#33021;&#65292;&#20294;&#27880;&#24847;&#21040;&#23545;&#27169;&#22411;&#25935;&#24863;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01790v1 Announce Type: cross  Abstract: Illegal landfills are a critical issue due to their environmental, economic, and public health impacts. This study leverages aerial imagery for environmental crime monitoring. While advances in artificial intelligence and computer vision hold promise, the challenge lies in training models with high-resolution literature datasets and adapting them to open-access low-resolution images. Considering the substantial quality differences and limited annotation, this research explores the adaptability of models across these domains. Motivated by the necessity for a comprehensive evaluation of waste detection algorithms, it advocates cross-domain classification and super-resolution enhancement to analyze the impact of different image resolutions on waste classification as an evaluation to combat the proliferation of illegal landfills. We observed performance improvements by enhancing image quality but noted an influence on model sensitivity, ne
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#24403;&#19979;&#35757;&#32451;&#20998;&#31867;&#22120;&#30340;&#26631;&#31614;&#19981;&#21487;&#38752;&#26102;&#65292;20&#31181;&#26368;&#26032;OOD&#26816;&#27979;&#26041;&#27861;&#22312;&#31163;&#32676;&#26816;&#27979;&#20013;&#30340;&#34920;&#29616;&#65292;&#20026;&#20102;&#35299;&#31867;&#21035;&#26631;&#31614;&#22122;&#38899;&#23545;OOD&#26816;&#27979;&#30340;&#24433;&#21709;&#25552;&#20379;&#20102;&#28145;&#20837;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2404.01775</link><description>&lt;p&gt;
&#25151;&#38388;&#37324;&#30340;&#19968;&#21482;&#21557;&#38393;&#30340;&#22823;&#35937;&#65306;&#24744;&#30340;&#31163;&#32676;&#26816;&#27979;&#22120;&#23545;&#26631;&#31614;&#22122;&#38899;&#40065;&#26834;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
A noisy elephant in the room: Is your out-of-distribution detector robust to label noise?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01775
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#24403;&#19979;&#35757;&#32451;&#20998;&#31867;&#22120;&#30340;&#26631;&#31614;&#19981;&#21487;&#38752;&#26102;&#65292;20&#31181;&#26368;&#26032;OOD&#26816;&#27979;&#26041;&#27861;&#22312;&#31163;&#32676;&#26816;&#27979;&#20013;&#30340;&#34920;&#29616;&#65292;&#20026;&#20102;&#35299;&#31867;&#21035;&#26631;&#31614;&#22122;&#38899;&#23545;OOD&#26816;&#27979;&#30340;&#24433;&#21709;&#25552;&#20379;&#20102;&#28145;&#20837;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#26816;&#27979;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#20013;&#30340;&#38476;&#29983;&#25110;&#24847;&#22806;&#22270;&#20687;&#26159;&#30830;&#20445;&#23433;&#20840;&#37096;&#32626;&#30340;&#20851;&#38190;&#12290;&#22312;&#20998;&#31867;&#39046;&#22495;&#65292;&#26816;&#27979;&#27169;&#22411;&#35757;&#32451;&#39046;&#22495;&#22806;&#22270;&#20687;&#30340;&#20219;&#21153;&#34987;&#31216;&#20026;&#31163;&#32676;&#26816;&#27979;&#65288;OOD&#26816;&#27979;&#65289;&#12290;&#23613;&#31649;&#20154;&#20204;&#36234;&#26469;&#36234;&#20851;&#27880;&#21457;&#23637;&#20107;&#21518;OOD&#26816;&#27979;&#26041;&#27861;&#65292;&#20294;&#23545;&#20110;&#22312;&#22522;&#30784;&#20998;&#31867;&#22120;&#26410;&#32463;&#36807;&#24178;&#20928;&#12289;&#31934;&#24515;&#31579;&#36873;&#25968;&#25454;&#38598;&#35757;&#32451;&#26102;&#36825;&#20123;&#26041;&#27861;&#34920;&#29616;&#22914;&#20309;&#30340;&#35752;&#35770;&#30456;&#23545;&#36739;&#23569;&#12290;&#26412;&#30740;&#31350;&#22312;20&#31181;&#26368;&#26032;&#30340;OOD&#26816;&#27979;&#26041;&#27861;&#20013;&#26356;&#20026;&#29616;&#23454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20102;&#28145;&#20837;&#25506;&#35752;&#65292;&#22312;&#27492;&#24773;&#20917;&#19979;&#65292;&#29992;&#20110;&#35757;&#32451;&#22522;&#30784;&#20998;&#31867;&#22120;&#30340;&#26631;&#31614;&#19981;&#21487;&#38752;&#65288;&#20363;&#22914;&#65292;&#20247;&#21253;&#25110;&#32593;&#32476;&#25235;&#21462;&#26631;&#31614;&#65289;&#12290;&#36890;&#36807;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#12289;&#22122;&#38899;&#31867;&#22411;&#21644;&#32423;&#21035;&#12289;&#26550;&#26500;&#21644;&#26816;&#26597;&#28857;&#31574;&#30053;&#19978;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#31867;&#21035;&#26631;&#31614;&#22122;&#38899;&#23545;OOD&#26816;&#27979;&#30340;&#24433;&#21709;&#65292;&#24182;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01775v1 Announce Type: cross  Abstract: The ability to detect unfamiliar or unexpected images is essential for safe deployment of computer vision systems. In the context of classification, the task of detecting images outside of a model's training domain is known as out-of-distribution (OOD) detection. While there has been a growing research interest in developing post-hoc OOD detection methods, there has been comparably little discussion around how these methods perform when the underlying classifier is not trained on a clean, carefully curated dataset. In this work, we take a closer look at 20 state-of-the-art OOD detection methods in the (more realistic) scenario where the labels used to train the underlying classifier are unreliable (e.g. crowd-sourced or web-scraped labels). Extensive experiments across different datasets, noise types &amp; levels, architectures and checkpointing strategies provide insights into the effect of class label noise on OOD detection, and show tha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#32479;&#19968;DNN&#25511;&#21046;&#31995;&#32479;&#30340;&#23450;&#24615;&#21644;&#23450;&#37327;&#23433;&#20840;&#39564;&#35777;&#38382;&#39064;&#65292;&#36890;&#36807;&#26500;&#24314;&#26377;&#25928;&#31070;&#32463;&#38556;&#30861;&#35777;&#20070;(NBCs)&#26469;&#23454;&#29616;&#20960;&#20046;&#30830;&#23450;&#30340;&#23433;&#20840;&#20445;&#35777;</title><link>https://arxiv.org/abs/2404.01769</link><description>&lt;p&gt;
&#32479;&#19968;DNN&#25511;&#21046;&#31995;&#32479;&#30340;&#23450;&#24615;&#21644;&#23450;&#37327;&#23433;&#20840;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Unifying Qualitative and Quantitative Safety Verification of DNN-Controlled Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01769
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#32479;&#19968;DNN&#25511;&#21046;&#31995;&#32479;&#30340;&#23450;&#24615;&#21644;&#23450;&#37327;&#23433;&#20840;&#39564;&#35777;&#38382;&#39064;&#65292;&#36890;&#36807;&#26500;&#24314;&#26377;&#25928;&#31070;&#32463;&#38556;&#30861;&#35777;&#20070;(NBCs)&#26469;&#23454;&#29616;&#20960;&#20046;&#30830;&#23450;&#30340;&#23433;&#20840;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#20351;&#24471;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#30417;&#30563;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#65292;&#20984;&#26174;&#20102;&#36805;&#36895;&#24314;&#31435;&#36825;&#31181;DNN&#25511;&#21046;&#31995;&#32479;&#30340;&#35748;&#35777;&#23433;&#20840;&#20445;&#35777;&#30340;&#36843;&#20999;&#38656;&#35201;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#39564;&#35777;&#26041;&#27861;&#20381;&#36182;&#20110;&#23450;&#24615;&#26041;&#27861;&#65292;&#20027;&#35201;&#37319;&#29992;&#21487;&#36798;&#24615;&#20998;&#26512;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22312;&#24320;&#25918;&#21644;&#23545;&#25239;&#29615;&#22659;&#20013;&#36816;&#34892;&#26102;&#20854;&#34892;&#20026;&#20855;&#26377;&#38543;&#26426;&#36235;&#21183;&#30340;DNN&#25511;&#21046;&#31995;&#32479;&#32780;&#35328;&#65292;&#23450;&#24615;&#39564;&#35777;&#35777;&#26126;&#19981;&#36275;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#32479;&#19968;DNN&#25511;&#21046;&#31995;&#32479;&#30340;&#23450;&#24615;&#21644;&#23450;&#37327;&#23433;&#20840;&#39564;&#35777;&#38382;&#39064;&#12290;&#36825;&#26159;&#36890;&#36807;&#23558;&#39564;&#35777;&#20219;&#21153;&#26500;&#24314;&#20026;&#26377;&#25928;&#31070;&#32463;&#38556;&#30861;&#35777;&#20070;(NBCs)&#30340;&#32508;&#21512;&#26469;&#23454;&#29616;&#30340;&#12290;&#26368;&#21021;&#65292;&#35813;&#26694;&#26550;&#26088;&#22312;&#36890;&#36807;&#23450;&#24615;&#39564;&#35777;&#24314;&#31435;&#20960;&#20046;&#30830;&#23450;&#30340;&#23433;&#20840;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01769v1 Announce Type: new  Abstract: The rapid advance of deep reinforcement learning techniques enables the oversight of safety-critical systems through the utilization of Deep Neural Networks (DNNs). This underscores the pressing need to promptly establish certified safety guarantees for such DNN-controlled systems. Most of the existing verification approaches rely on qualitative approaches, predominantly employing reachability analysis. However, qualitative verification proves inadequate for DNN-controlled systems as their behaviors exhibit stochastic tendencies when operating in open and adversarial environments. In this paper, we propose a novel framework for unifying both qualitative and quantitative safety verification problems of DNN-controlled systems. This is achieved by formulating the verification tasks as the synthesis of valid neural barrier certificates (NBCs). Initially, the framework seeks to establish almost-sure safety guarantees through qualitative verif
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#36965;&#24863;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#22312;&#26368;&#19981;&#21457;&#36798;&#22269;&#23478;&#36827;&#34892;&#20102;&#20840;&#29699;&#26333;&#20809;&#21644;&#29289;&#29702;&#33030;&#24369;&#24615;&#21160;&#24577;&#26144;&#23556;&#65292;&#26088;&#22312;&#25512;&#21160;&#22823;&#35268;&#27169;&#28798;&#23475;&#39118;&#38505;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2404.01748</link><description>&lt;p&gt;
&#20351;&#29992;&#36965;&#24863;&#21644;&#26426;&#22120;&#23398;&#20064;&#23545;&#26368;&#19981;&#21457;&#36798;&#22269;&#23478;&#30340;&#26333;&#20809;&#21644;&#29289;&#29702;&#33030;&#24369;&#24615;&#21160;&#24577;&#20840;&#29699;&#26144;&#23556;
&lt;/p&gt;
&lt;p&gt;
Global Mapping of Exposure and Physical Vulnerability Dynamics in Least Developed Countries using Remote Sensing and Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01748
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#36965;&#24863;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#22312;&#26368;&#19981;&#21457;&#36798;&#22269;&#23478;&#36827;&#34892;&#20102;&#20840;&#29699;&#26333;&#20809;&#21644;&#29289;&#29702;&#33030;&#24369;&#24615;&#21160;&#24577;&#26144;&#23556;&#65292;&#26088;&#22312;&#25512;&#21160;&#22823;&#35268;&#27169;&#28798;&#23475;&#39118;&#38505;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#19990;&#30028;&#26631;&#35760;&#20102;&#12298;2015-2030&#24180;&#20943;&#23569;&#28798;&#23475;&#39118;&#38505;&#24046;&#36317;&#26694;&#26550;&#12299;&#30340;&#20013;&#26399;&#65292;&#35768;&#22810;&#22269;&#23478;&#20173;&#22312;&#21162;&#21147;&#30417;&#27979;&#20854;&#27668;&#20505;&#21644;&#28798;&#23475;&#39118;&#38505;&#65292;&#22240;&#20026;&#22823;&#35268;&#27169;&#35843;&#26597;&#26333;&#20809;&#21644;&#29289;&#29702;&#33030;&#24369;&#24615;&#20998;&#24067;&#26114;&#36149;&#65292;&#22240;&#27492;&#65292;&#22312;&#27668;&#20505;&#21464;&#21270;&#21152;&#21095;&#30340;&#24433;&#21709;&#20043;&#19979;&#65292;&#36825;&#20123;&#22269;&#23478;&#22312;&#20943;&#23569;&#39118;&#38505;&#26041;&#38754;&#24182;&#19981;&#22312;&#36712;&#36947;&#19978;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#26102;&#38388;&#24207;&#21015;&#36965;&#24863;&#20174;&#20844;&#24320;&#21487;&#29992;&#30340;Sentinel-1 SAR GRD&#21644;Sentinel-2 Harmonized MSI&#26469;&#26144;&#23556;&#36825;&#19968;&#37325;&#35201;&#20449;&#24687;&#30340;&#25345;&#32493;&#21162;&#21147;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;OpenSendaiBench&#8221;&#30340;&#21457;&#23637;&#65292;&#20854;&#20013;&#21253;&#25324;47&#20010;&#22269;&#23478;&#65292;&#20854;&#20013;&#22823;&#22810;&#25968;&#26159;&#26368;&#19981;&#21457;&#36798;&#22269;&#23478;&#65288;LDCs&#65289;&#65292;&#35757;&#32451;&#20102;ResNet-50&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#26144;&#23556;&#23391;&#21152;&#25289;&#22269;&#36798;&#21345;&#22320;&#21306;&#30340;&#38750;&#27491;&#24335;&#24314;&#31569;&#30340;&#20998;&#24067;&#26469;&#23637;&#31034;&#12290;&#20316;&#20026;&#20840;&#29699;&#23457;&#35745;&#28798;&#23475;&#39118;&#38505;&#38543;&#26102;&#38388;&#28436;&#21464;&#30340;&#24320;&#21019;&#24615;&#24037;&#20316;&#65292;&#26412;&#25991;&#26088;&#22312;&#25512;&#36827;&#22823;&#35268;&#27169;&#21306;&#22495;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01748v1 Announce Type: new  Abstract: As the world marked the midterm of the Sendai Framework for Disaster Risk Reduction 2015-2030, many countries are still struggling to monitor their climate and disaster risk because of the expensive large-scale survey of the distribution of exposure and physical vulnerability and, hence, are not on track in reducing risks amidst the intensifying effects of climate change. We present an ongoing effort in mapping this vital information using machine learning and time-series remote sensing from publicly available Sentinel-1 SAR GRD and Sentinel-2 Harmonized MSI. We introduce the development of "OpenSendaiBench" consisting of 47 countries wherein most are least developed (LDCs), trained ResNet-50 deep learning models, and demonstrated the region of Dhaka, Bangladesh by mapping the distribution of its informal constructions. As a pioneering effort in auditing global disaster risk over time, this paper aims to advance the area of large-scale r
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#31181;&#32467;&#21512;&#28145;&#24230;&#23398;&#20064;&#21644;&#32422;&#26463;&#20248;&#21270;&#30340;&#39640;&#25928;&#26041;&#27861;&#65292;&#21033;&#29992;&#30693;&#35782;&#33976;&#39311;&#35757;&#32451;&#26356;&#23567;&#26356;&#39640;&#25928;&#30340;&#32593;&#32476;&#65292;&#20174;&#32780;&#20943;&#36731;&#22797;&#26434;&#24615;</title><link>https://arxiv.org/abs/2404.01746</link><description>&lt;p&gt;
&#38754;&#21521;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#21487;&#25193;&#23637;&#39640;&#25928;&#20132;&#20114;&#24863;&#30693;&#35268;&#21010;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Towards Scalable &amp; Efficient Interaction-Aware Planning in Autonomous Vehicles using Knowledge Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01746
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#31181;&#32467;&#21512;&#28145;&#24230;&#23398;&#20064;&#21644;&#32422;&#26463;&#20248;&#21270;&#30340;&#39640;&#25928;&#26041;&#27861;&#65292;&#21033;&#29992;&#30693;&#35782;&#33976;&#39311;&#35757;&#32451;&#26356;&#23567;&#26356;&#39640;&#25928;&#30340;&#32593;&#32476;&#65292;&#20174;&#32780;&#20943;&#36731;&#22797;&#26434;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30495;&#23454;&#19990;&#30028;&#30340;&#39550;&#39542;&#28041;&#21450;&#36710;&#36742;&#22312;&#25317;&#25380;&#20132;&#36890;&#22330;&#26223;&#20013;&#30456;&#20114;&#22797;&#26434;&#30340;&#20114;&#21160;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20851;&#27880;&#20110;&#22686;&#24378;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#20132;&#20114;&#24847;&#35782;&#65292;&#20197;&#21033;&#29992;&#36825;&#20123;&#20114;&#21160;&#26469;&#36827;&#34892;&#20915;&#31574;&#21046;&#23450;&#12290;&#36825;&#20123;&#20132;&#20114;&#24863;&#30693;&#35268;&#21010;&#22120;&#20381;&#36182;&#20110;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#27169;&#22411;&#26469;&#25429;&#33719;&#36710;&#36742;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#26088;&#22312;&#23558;&#36825;&#20123;&#39044;&#27979;&#19982;&#20256;&#32479;&#25511;&#21046;&#25216;&#26415;&#22914;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#30456;&#25972;&#21512;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19982;&#20256;&#32479;&#25511;&#21046;&#33539;&#24335;&#30340;&#25972;&#21512;&#24448;&#24448;&#23548;&#33268;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#20381;&#36182;&#20110;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21407;&#21017;&#24615;&#21644;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#23558;&#28145;&#24230;&#23398;&#20064;&#19982;&#32422;&#26463;&#20248;&#21270;&#30456;&#32467;&#21512;&#65292;&#37319;&#29992;&#30693;&#35782;&#33976;&#39311;&#26469;&#35757;&#32451;&#26356;&#23567;&#12289;&#26356;&#39640;&#25928;&#30340;&#32593;&#32476;&#65292;&#20174;&#32780;&#20943;&#36731;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;&#20248;&#21270;&#21518;&#30340;&#32593;&#32476;&#33021;&#22815;&#32500;&#25345;&#38382;&#39064;&#30340;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01746v1 Announce Type: cross  Abstract: Real-world driving involves intricate interactions among vehicles navigating through dense traffic scenarios. Recent research focuses on enhancing the interaction awareness of autonomous vehicles to leverage these interactions in decision-making. These interaction-aware planners rely on neural-network-based prediction models to capture inter-vehicle interactions, aiming to integrate these predictions with traditional control techniques such as Model Predictive Control. However, this integration of deep learning-based models with traditional control paradigms often results in computationally demanding optimization problems, relying on heuristic methods. This study introduces a principled and efficient method for combining deep learning with constrained optimization, employing knowledge distillation to train smaller and more efficient networks, thereby mitigating complexity. We demonstrate that these refined networks maintain the problem
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#26368;&#20248;KL&#32422;&#26463;&#30340;&#24378;&#21270;&#23398;&#20064;&#35299;&#30340;&#38381;&#21512;&#24418;&#24335;&#21051;&#30011;&#65292;&#35777;&#26126;&#20102;&#23454;&#29616;KL&#25955;&#24230;&#21644;&#22870;&#21169;&#20043;&#38388;&#26435;&#34913;&#30340;&#23545;&#40784;&#26041;&#27861;&#24517;&#39035;&#36817;&#20284;&#26368;&#20248;KL&#32422;&#26463;&#30340;RL&#35299;&#12290;</title><link>https://arxiv.org/abs/2404.01730</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#30340;&#28176;&#36827;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Asymptotics of Language Model Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01730
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#26368;&#20248;KL&#32422;&#26463;&#30340;&#24378;&#21270;&#23398;&#20064;&#35299;&#30340;&#38381;&#21512;&#24418;&#24335;&#21051;&#30011;&#65292;&#35777;&#26126;&#20102;&#23454;&#29616;KL&#25955;&#24230;&#21644;&#22870;&#21169;&#20043;&#38388;&#26435;&#34913;&#30340;&#23545;&#40784;&#26041;&#27861;&#24517;&#39035;&#36817;&#20284;&#26368;&#20248;KL&#32422;&#26463;&#30340;RL&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35753;$p$&#34920;&#31034;&#19968;&#20010;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#12290;&#35753;$r$&#34920;&#31034;&#19968;&#20010;&#22870;&#21169;&#27169;&#22411;&#65292;&#36820;&#22238;&#19968;&#20010;&#26631;&#37327;&#65292;&#25429;&#25417;&#20174;$p$&#20013;&#25277;&#21462;&#30340;&#20869;&#23481;&#34987;&#20559;&#22909;&#30340;&#31243;&#24230;&#12290;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#30340;&#30446;&#26631;&#26159;&#25913;&#21464;$p$&#20026;&#19968;&#20010;&#26032;&#30340;&#20998;&#24067;$\phi$&#65292;&#20351;&#24471;&#26399;&#26395;&#22870;&#21169;&#26356;&#39640;&#65292;&#21516;&#26102;&#20445;&#25345;$\phi$&#25509;&#36817;$p$&#12290;&#19968;&#31181;&#27969;&#34892;&#30340;&#23545;&#40784;&#26041;&#27861;&#26159;KL&#32422;&#26463;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#65292;&#36873;&#25321;&#19968;&#20010;&#20998;&#24067;$\phi_\Delta$&#65292;&#26368;&#22823;&#21270;$E_{\phi_{\Delta}} r(y)$&#65292;&#21516;&#26102;&#28385;&#36275;&#30456;&#23545;&#29109;&#32422;&#26463;$KL(\phi_\Delta || p) \leq \Delta$&#12290;&#21478;&#19968;&#31181;&#31616;&#21333;&#30340;&#23545;&#40784;&#26041;&#27861;&#26159;&#26368;&#20339;-$N$&#65292;&#20174;$p$&#20013;&#25277;&#21462;$N$&#20010;&#26679;&#26412;&#65292;&#24182;&#36873;&#25321;&#22870;&#21169;&#26368;&#39640;&#30340;&#19968;&#20010;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26368;&#20248;KL&#32422;&#26463;&#30340;RL&#35299;&#30340;&#38381;&#21512;&#24418;&#24335;&#21051;&#30011;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20219;&#20309;&#23454;&#29616;KL&#25955;&#24230;&#21644;&#22870;&#21169;&#20043;&#38388;&#21487;&#27604;&#36739;&#30340;&#26435;&#34913;&#30340;&#23545;&#40784;&#26041;&#27861;&#65292;&#24517;&#39035;&#36817;&#20284;&#26368;&#20248;KL&#32422;&#26463;&#30340;RL&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01730v1 Announce Type: new  Abstract: Let $p$ denote a generative language model. Let $r$ denote a reward model that returns a scalar that captures the degree at which a draw from $p$ is preferred. The goal of language model alignment is to alter $p$ to a new distribution $\phi$ that results in a higher expected reward while keeping $\phi$ close to $p.$ A popular alignment method is the KL-constrained reinforcement learning (RL), which chooses a distribution $\phi_\Delta$ that maximizes $E_{\phi_{\Delta}} r(y)$ subject to a relative entropy constraint $KL(\phi_\Delta || p) \leq \Delta.$ Another simple alignment method is best-of-$N$, where $N$ samples are drawn from $p$ and one with highest reward is selected. In this paper, we offer a closed-form characterization of the optimal KL-constrained RL solution. We demonstrate that any alignment method that achieves a comparable trade-off between KL divergence and reward must approximate the optimal KL-constrained RL solution in t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#20110;&#20998;&#35299;&#36716;&#24405;&#27169;&#22411;&#30340;&#26377;&#25928;ILM&#35757;&#32451;&#21644;&#35299;&#30721;&#31574;&#30053;&#65292;&#33021;&#22815;&#26174;&#33879;&#25913;&#21892;&#19982;&#26631;&#20934;&#35299;&#30721;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;LibriSpeech&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;17%&#30340;&#30456;&#23545;&#25913;&#21892;&#12290;</title><link>https://arxiv.org/abs/2404.01716</link><description>&lt;p&gt;
&#26377;&#25928;&#30340;&#20869;&#37096;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#21644;&#34701;&#21512;&#23545;&#20110;&#20998;&#35299;&#36716;&#24405;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Effective internal language model training and fusion for factorized transducer model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01716
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#20110;&#20998;&#35299;&#36716;&#24405;&#27169;&#22411;&#30340;&#26377;&#25928;ILM&#35757;&#32451;&#21644;&#35299;&#30721;&#31574;&#30053;&#65292;&#33021;&#22815;&#26174;&#33879;&#25913;&#21892;&#19982;&#26631;&#20934;&#35299;&#30721;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;LibriSpeech&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;17%&#30340;&#30456;&#23545;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#36716;&#24405;&#22120;&#30340;&#20869;&#37096;&#35821;&#35328;&#27169;&#22411;&#65288;ILM&#65289;&#24050;&#34987;&#24191;&#27867;&#30740;&#31350;&#12290;&#22312;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#24037;&#20316;&#20013;&#65292;&#23427;&#20027;&#35201;&#29992;&#20110;&#20272;&#35745;ILM&#20998;&#25968;&#65292;&#24182;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#38543;&#21518;&#34987;&#20943;&#21435;&#65292;&#20197;&#20419;&#36827;&#19982;&#22806;&#37096;&#35821;&#35328;&#27169;&#22411;&#26356;&#22909;&#30340;&#38598;&#25104;&#12290;&#26368;&#36817;&#65292;&#24050;&#25552;&#20986;&#20102;&#21508;&#31181;&#20998;&#35299;&#36716;&#24405;&#27169;&#22411;&#65292;&#26126;&#30830;&#37319;&#29992;&#29420;&#31435;&#30340;&#20869;&#37096;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#38750;&#31354;&#30333;&#20196;&#29260;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#37319;&#29992;&#20102;&#20998;&#35299;&#36716;&#24405;&#27169;&#22411;&#65292;&#19982;&#27973;&#23618;&#34701;&#21512;&#30456;&#27604;&#65292;&#25913;&#36827;&#20173;&#28982;&#26377;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;ILM&#35757;&#32451;&#21644;&#35299;&#30721;&#31574;&#30053;&#65292;&#29992;&#20110;&#20998;&#35299;&#36716;&#24405;&#27169;&#22411;&#65292;&#26377;&#25928;&#22320;&#32467;&#21512;&#20102;&#31354;&#30333;&#12289;&#22768;&#23398;&#21644;ILM&#20998;&#25968;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;LibriSpeech&#25968;&#25454;&#38598;&#19978;&#21033;&#29992;&#32463;&#36807;&#33391;&#22909;&#35757;&#32451;&#30340;ILM&#21644;&#25152;&#25552;&#20986;&#30340;&#35299;&#30721;&#31574;&#30053;&#26102;&#65292;&#30456;&#23545;&#20110;&#26631;&#20934;&#35299;&#30721;&#26041;&#27861;&#65292;&#26377;17%&#30340;&#30456;&#23545;&#25913;&#21892;&#12290;&#27492;&#22806;&#65292;&#19982;&#24378;RNN-T ba&#30456;&#27604;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01716v1 Announce Type: cross  Abstract: The internal language model (ILM) of the neural transducer has been widely studied. In most prior work, it is mainly used for estimating the ILM score and is subsequently subtracted during inference to facilitate improved integration with external language models. Recently, various of factorized transducer models have been proposed, which explicitly embrace a standalone internal language model for non-blank token prediction. However, even with the adoption of factorized transducer models, limited improvement has been observed compared to shallow fusion. In this paper, we propose a novel ILM training and decoding strategy for factorized transducer models, which effectively combines the blank, acoustic and ILM scores. Our experiments show a 17% relative improvement over the standard decoding method when utilizing a well-trained ILM and the proposed decoding strategy on LibriSpeech datasets. Furthermore, when compared to a strong RNN-T ba
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20849;&#36717;&#26799;&#24230;&#26679;&#24335;&#30340;&#26032;&#20248;&#21270;&#31639;&#27861;CG-like-Adam&#65292;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#65292;&#24182;&#22312;&#25910;&#25947;&#20998;&#26512;&#21644;&#25968;&#20540;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#20248;&#36234;&#24615;</title><link>https://arxiv.org/abs/2404.01714</link><description>&lt;p&gt;
&#22522;&#20110;&#20849;&#36717;&#26799;&#24230;&#30340;&#33258;&#36866;&#24212;&#30697;&#20272;&#35745;&#20248;&#21270;&#31639;&#27861;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Conjugate-Gradient-like Based Adaptive Moment Estimation Optimization Algorithm for Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01714
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20849;&#36717;&#26799;&#24230;&#26679;&#24335;&#30340;&#26032;&#20248;&#21270;&#31639;&#27861;CG-like-Adam&#65292;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#65292;&#24182;&#22312;&#25910;&#25947;&#20998;&#26512;&#21644;&#25968;&#20540;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#20248;&#36234;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20026;&#21152;&#24555;&#22521;&#35757;&#36895;&#24230;&#24182;&#22686;&#24378;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#23558;&#20256;&#32479;&#30340;&#20849;&#36717;&#26799;&#24230;&#20462;&#27491;&#20026;&#20849;&#36717;&#26799;&#24230;&#26679;&#24335;&#65292;&#24182;&#23558;&#20854;&#24182;&#20837;&#36890;&#29992;Adam&#20013;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CG-like-Adam&#30340;&#26032;&#20248;&#21270;&#31639;&#27861;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#29992;Adam&#30340;&#19968;&#38454;&#21644;&#20108;&#38454;&#30697;&#20272;&#35745;&#22343;&#30001;&#20849;&#36717;&#26799;&#24230;&#26679;&#24335;&#26367;&#25442;&#12290;&#25910;&#25947;&#20998;&#26512;&#22788;&#29702;&#20102;&#19968;&#38454;&#30697;&#20272;&#35745;&#30340;&#25351;&#25968;&#31227;&#21160;&#24179;&#22343;&#31995;&#25968;&#20026;&#24120;&#25968;&#19988;&#19968;&#38454;&#30697;&#20272;&#35745;&#26080;&#20559;&#30340;&#24773;&#20917;&#12290;&#25968;&#20540;&#23454;&#39564;&#26174;&#31034;&#20102;&#22522;&#20110;CIFAR10/100&#25968;&#25454;&#38598;&#30340;&#25152;&#25552;&#31639;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01714v1 Announce Type: cross  Abstract: Training deep neural networks is a challenging task. In order to speed up training and enhance the performance of deep neural networks, we rectify the vanilla conjugate gradient as conjugate-gradient-like and incorporate it into the generic Adam, and thus propose a new optimization algorithm named CG-like-Adam for deep learning. Specifically, both the first-order and the second-order moment estimation of generic Adam are replaced by the conjugate-gradient-like. Convergence analysis handles the cases where the exponential moving average coefficient of the first-order moment estimation is constant and the first-order moment estimation is unbiased. Numerical experiments show the superiority of the proposed algorithm based on the CIFAR10/100 dataset.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;&#30340;Hessian-free&#22312;&#32447;&#36951;&#24536;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#36817;&#20046;&#30636;&#26102;&#30340;&#22312;&#32447;&#36951;&#24536;&#65292;&#20165;&#38656;&#35201;&#36827;&#34892;&#30690;&#37327;&#21152;&#27861;&#25805;&#20316;&#12290;</title><link>https://arxiv.org/abs/2404.01712</link><description>&lt;p&gt;
&#36890;&#36807;&#20813;Hessian&#37325;&#26032;&#25972;&#21512;&#20010;&#20307;&#25968;&#25454;&#32479;&#35745;&#23454;&#29616;&#39640;&#25928;&#22312;&#32447;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Efficient Online Unlearning via Hessian-Free Recollection of Individual Data Statistics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01712
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;&#30340;Hessian-free&#22312;&#32447;&#36951;&#24536;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#36817;&#20046;&#30636;&#26102;&#30340;&#22312;&#32447;&#36951;&#24536;&#65292;&#20165;&#38656;&#35201;&#36827;&#34892;&#30690;&#37327;&#21152;&#27861;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#36951;&#24536;&#26088;&#22312;&#36890;&#36807;&#20351;&#27169;&#22411;&#33021;&#22815;&#36873;&#25321;&#24615;&#22320;&#24536;&#35760;&#29305;&#23450;&#25968;&#25454;&#26469;&#32500;&#25252;&#25968;&#25454;&#25152;&#26377;&#32773;&#30340;&#34987;&#36951;&#24536;&#26435;&#21033;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#34920;&#26126;&#65292;&#19968;&#31181;&#25968;&#25454;&#36951;&#24536;&#30340;&#26041;&#27861;&#26159;&#36890;&#36807;&#39044;&#20808;&#35745;&#31639;&#21644;&#23384;&#20648;&#25658;&#24102;&#20108;&#38454;&#20449;&#24687;&#30340;&#32479;&#35745;&#25968;&#25454;&#65292;&#20197;&#25913;&#36827;&#35745;&#31639;&#21644;&#20869;&#23384;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20381;&#36182;&#20110;&#33499;&#21051;&#30340;&#20551;&#35774;&#65292;&#32780;&#19988;&#35745;&#31639;/&#23384;&#20648;&#21463;&#21040;&#27169;&#22411;&#21442;&#25968;&#32500;&#24230;&#30340;&#35781;&#21650;&#65292;&#36825;&#20351;&#24471;&#38590;&#20197;&#24212;&#29992;&#21040;&#22823;&#22810;&#25968;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20813;Hessian&#22312;&#32447;&#36951;&#24536;&#26041;&#27861;&#12290;&#25105;&#20204;&#24314;&#35758;&#20026;&#27599;&#20010;&#25968;&#25454;&#28857;&#32500;&#25252;&#19968;&#20010;&#32479;&#35745;&#21521;&#37327;&#65292;&#36890;&#36807;&#37325;&#26032;&#35757;&#32451;&#21644;&#23398;&#20064;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#30340;&#20223;&#23556;&#38543;&#26426;&#36882;&#24402;&#36924;&#36817;&#26469;&#35745;&#31639;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#23454;&#29616;&#20102;&#36817;&#20046;&#30636;&#26102;&#30340;&#22312;&#32447;&#36951;&#24536;&#65292;&#22240;&#20026;&#23427;&#21482;&#38656;&#35201;&#36827;&#34892;&#30690;&#37327;&#21152;&#27861;&#25805;&#20316;&#12290;&#22522;&#20110;&#37325;&#26032;&#25910;&#38598;&#36951;&#24536;&#25968;&#25454;&#32479;&#35745;&#30340;&#31574;&#30053;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01712v1 Announce Type: cross  Abstract: Machine unlearning strives to uphold the data owners' right to be forgotten by enabling models to selectively forget specific data. Recent methods suggest that one approach of data forgetting is by precomputing and storing statistics carrying second-order information to improve computational and memory efficiency. However, they rely on restrictive assumptions and the computation/storage suffer from the curse of model parameter dimensionality, making it challenging to apply to most deep neural networks. In this work, we propose a Hessian-free online unlearning method. We propose to maintain a statistical vector for each data point, computed through affine stochastic recursion approximation of the difference between retrained and learned models. Our proposed algorithm achieves near-instantaneous online unlearning as it only requires a vector addition operation. Based on the strategy that recollecting statistics for forgetting data, the p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#25237;&#24433;&#26041;&#24046;&#23545;&#39640;&#26031;&#36807;&#31243;&#28508;&#21464;&#37327;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;&#38598;&#25104;&#20102;&#35889;&#28151;&#21512;&#65288;SM&#65289;&#26680;&#21644;&#21487;&#24494;&#38543;&#26426;&#20613;&#31435;&#21494;&#29305;&#24449;&#65288;RFF&#65289;&#26680;&#36924;&#36817;&#26469;&#35299;&#20915;&#26680;&#28789;&#27963;&#24615;&#19981;&#36275;&#38382;&#39064;&#65292;&#20174;&#32780;&#38450;&#27490;&#27169;&#22411;&#23849;&#28291;&#12290;</title><link>https://arxiv.org/abs/2404.01697</link><description>&lt;p&gt;
&#38450;&#27490;&#39640;&#26031;&#36807;&#31243;&#28508;&#21464;&#37327;&#27169;&#22411;&#20013;&#30340;&#27169;&#22411;&#23849;&#28291;
&lt;/p&gt;
&lt;p&gt;
Preventing Model Collapse in Gaussian Process Latent Variable Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01697
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#25237;&#24433;&#26041;&#24046;&#23545;&#39640;&#26031;&#36807;&#31243;&#28508;&#21464;&#37327;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;&#38598;&#25104;&#20102;&#35889;&#28151;&#21512;&#65288;SM&#65289;&#26680;&#21644;&#21487;&#24494;&#38543;&#26426;&#20613;&#31435;&#21494;&#29305;&#24449;&#65288;RFF&#65289;&#26680;&#36924;&#36817;&#26469;&#35299;&#20915;&#26680;&#28789;&#27963;&#24615;&#19981;&#36275;&#38382;&#39064;&#65292;&#20174;&#32780;&#38450;&#27490;&#27169;&#22411;&#23849;&#28291;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Gaussian process latent variable models (GPLVMs)&#26159;&#19968;&#31867;&#22810;&#25165;&#22810;&#33402;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#24120;&#29992;&#20110;&#38477;&#32500;&#12290;&#28982;&#32780;&#65292;&#29992;GPLVMs&#23545;&#25968;&#25454;&#24314;&#27169;&#26102;&#24120;&#35265;&#30340;&#25361;&#25112;&#21253;&#25324;&#26680;&#28789;&#27963;&#24615;&#19981;&#36275;&#21644;&#25237;&#24433;&#22122;&#22768;&#36873;&#25321;&#19981;&#24403;&#65292;&#23548;&#33268;&#20102;&#19968;&#31181;&#20197;&#27169;&#31946;&#28508;&#21464;&#37327;&#34920;&#31034;&#20026;&#20027;&#35201;&#29305;&#24449;&#30340;&#27169;&#22411;&#23849;&#28291;&#65292;&#36825;&#31181;&#34920;&#31034;&#19981;&#21453;&#26144;&#25968;&#25454;&#30340;&#28508;&#22312;&#32467;&#26500;&#12290;&#26412;&#25991;&#39318;&#20808;&#20174;&#29702;&#35770;&#19978;&#36890;&#36807;&#32447;&#24615;GPLVM&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;&#25237;&#24433;&#26041;&#24046;&#23545;&#27169;&#22411;&#23849;&#28291;&#30340;&#24433;&#21709;&#12290;&#20854;&#27425;&#65292;&#36890;&#36807;&#38598;&#25104;&#35889;&#28151;&#21512;&#65288;SM&#65289;&#26680;&#21644;&#21487;&#24494;&#38543;&#26426;&#20613;&#31435;&#21494;&#29305;&#24449;&#65288;RFF&#65289;&#26680;&#36924;&#36817;&#65292;&#35299;&#20915;&#20102;&#30001;&#20110;&#26680;&#28789;&#27963;&#24615;&#19981;&#36275;&#23548;&#33268;&#30340;&#27169;&#22411;&#23849;&#28291;&#38382;&#39064;&#65292;&#20174;&#32780;&#20445;&#35777;&#20102;&#36890;&#36807;&#29616;&#25104;&#30340;&#33258;&#21160;&#24494;&#20998;&#24037;&#20855;&#23454;&#29616;&#23398;&#20064;&#26680;&#21442;&#25968;&#30340;&#35745;&#31639;&#21487;&#25193;&#23637;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01697v1 Announce Type: cross  Abstract: Gaussian process latent variable models (GPLVMs) are a versatile family of unsupervised learning models, commonly used for dimensionality reduction. However, common challenges in modeling data with GPLVMs include inadequate kernel flexibility and improper selection of the projection noise, which leads to a type of model collapse characterized primarily by vague latent representations that do not reflect the underlying structure of the data. This paper addresses these issues by, first, theoretically examining the impact of the projection variance on model collapse through the lens of a linear GPLVM. Second, we address the problem of model collapse due to inadequate kernel flexibility by integrating the spectral mixture (SM) kernel and a differentiable random Fourier feature (RFF) kernel approximation, which ensures computational scalability and efficiency through off-the-shelf automatic differentiation tools for learning the kernel hype
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#26102;&#24577;&#30693;&#35782;&#22270;&#25512;&#29702;&#30340;&#24323;&#26435;&#26426;&#21046;&#65292;&#36890;&#36807;&#32622;&#20449;&#24230;&#20272;&#35745;&#24110;&#21161;&#29616;&#26377;&#27169;&#22411;&#26377;&#36873;&#25321;&#24615;&#22320;&#39044;&#27979;&#26410;&#26469;&#20107;&#23454;&#65292;&#20943;&#23569;&#19981;&#30830;&#23450;&#24615;&#24102;&#26469;&#30340;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2404.01695</link><description>&lt;p&gt;
&#36873;&#25321;&#24615;&#26102;&#24577;&#30693;&#35782;&#22270;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Selective Temporal Knowledge Graph Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01695
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#26102;&#24577;&#30693;&#35782;&#22270;&#25512;&#29702;&#30340;&#24323;&#26435;&#26426;&#21046;&#65292;&#36890;&#36807;&#32622;&#20449;&#24230;&#20272;&#35745;&#24110;&#21161;&#29616;&#26377;&#27169;&#22411;&#26377;&#36873;&#25321;&#24615;&#22320;&#39044;&#27979;&#26410;&#26469;&#20107;&#23454;&#65292;&#20943;&#23569;&#19981;&#30830;&#23450;&#24615;&#24102;&#26469;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#30693;&#35782;&#22270;&#65288;TKG&#65289;&#26368;&#36817;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#23427;&#20197;&#65288;&#20027;&#20307;&#65292;&#20851;&#31995;&#65292;&#23458;&#20307;&#65292;&#26102;&#38388;&#25139;&#65289;&#30340;&#24418;&#24335;&#25551;&#36848;&#20102;&#26102;&#38388;&#28436;&#21464;&#30340;&#20107;&#23454;&#12290;TKG&#25512;&#29702;&#26088;&#22312;&#22522;&#20110;&#32473;&#23450;&#30340;&#21382;&#21490;&#20107;&#23454;&#39044;&#27979;&#26410;&#26469;&#20107;&#23454;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;TKG&#25512;&#29702;&#27169;&#22411;&#26080;&#27861;&#36991;&#20813;&#23545;&#20182;&#20204;&#35748;&#20026;&#19981;&#30830;&#23450;&#30340;&#39044;&#27979;&#65292;&#36825;&#24517;&#28982;&#20250;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#24102;&#26469;&#39118;&#38505;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;TKG&#25512;&#29702;&#30340;&#24323;&#26435;&#26426;&#21046;&#65292;&#23427;&#21487;&#20197;&#24110;&#21161;&#29616;&#26377;&#30340;&#27169;&#22411;&#36827;&#34892;&#36873;&#25321;&#24615;&#32780;&#38750;&#19981;&#21152;&#21306;&#21035;&#30340;&#39044;&#27979;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#32622;&#20449;&#24230;&#20272;&#35745;&#22120;&#65292;&#31216;&#20026;&#24102;&#26377;&#21382;&#21490;&#20449;&#24687;&#30340;&#32622;&#20449;&#24230;&#20272;&#35745;&#22120;&#65288;CEHis&#65289;&#65292;&#20197;&#20351;&#29616;&#26377;&#30340;TKG&#25512;&#29702;&#27169;&#22411;&#39318;&#20808;&#35780;&#20272;&#20854;&#22312;&#20570;&#20986;&#39044;&#27979;&#26102;&#30340;&#32622;&#20449;&#24230;&#65292;&#28982;&#21518;&#25918;&#24323;&#37027;&#20123;&#32622;&#20449;&#24230;&#36739;&#20302;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01695v1 Announce Type: new  Abstract: Temporal Knowledge Graph (TKG), which characterizes temporally evolving facts in the form of (subject, relation, object, timestamp), has attracted much attention recently. TKG reasoning aims to predict future facts based on given historical ones. However, existing TKG reasoning models are unable to abstain from predictions they are uncertain, which will inevitably bring risks in real-world applications. Thus, in this paper, we propose an abstention mechanism for TKG reasoning, which helps the existing models make selective, instead of indiscriminate, predictions. Specifically, we develop a confidence estimator, called Confidence Estimator with History (CEHis), to enable the existing TKG reasoning models to first estimate their confidence in making predictions, and then abstain from those with low confidence. To do so, CEHis takes two kinds of information into consideration, namely, the certainty of the current prediction and the accuracy
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;HeMeNet&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#36755;&#20837;&#34507;&#30333;&#36136;&#32467;&#26500;&#30340;&#22522;&#30784;&#19978;&#32852;&#21512;&#22788;&#29702;&#22810;&#20010;&#20219;&#21153;&#65292;&#35299;&#20915;&#20102;&#34507;&#30333;&#36136;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2404.01693</link><description>&lt;p&gt;
HeMeNet&#65306;&#29992;&#20110;&#34507;&#30333;&#36136;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#24322;&#36136;&#22810;&#36890;&#36947;&#31561;&#21464;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
HeMeNet: Heterogeneous Multichannel Equivariant Network for Protein Multitask Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01693
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;HeMeNet&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#36755;&#20837;&#34507;&#30333;&#36136;&#32467;&#26500;&#30340;&#22522;&#30784;&#19978;&#32852;&#21512;&#22788;&#29702;&#22810;&#20010;&#20219;&#21153;&#65292;&#35299;&#20915;&#20102;&#34507;&#30333;&#36136;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#21644;&#21033;&#29992;&#34507;&#30333;&#36136;&#30340;3D&#32467;&#26500;&#23545;&#20110;&#22810;&#31181;&#29983;&#29289;&#23398;&#21644;&#33647;&#29289;&#21457;&#29616;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#28145;&#24230;&#23398;&#20064;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#22522;&#20110;&#34507;&#30333;&#32467;&#26500;&#30340;&#21151;&#33021;&#39044;&#27979;&#20219;&#21153;&#65292;&#20294;&#30446;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#20026;&#27599;&#20010;&#20219;&#21153;&#21333;&#29420;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#27599;&#20010;&#20219;&#21153;&#35268;&#27169;&#36739;&#23567;&#65292;&#36825;&#31181;&#21333;&#19968;&#20219;&#21153;&#31574;&#30053;&#38480;&#21046;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#19968;&#20123;&#26631;&#35760;&#30340;3D&#34507;&#30333;&#25968;&#25454;&#38598;&#22312;&#29983;&#29289;&#19978;&#30456;&#20851;&#65292;&#23558;&#22810;&#28304;&#25968;&#25454;&#38598;&#32452;&#21512;&#29992;&#20110;&#26356;&#22823;&#35268;&#27169;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#26159;&#20811;&#26381;&#36825;&#19968;&#38382;&#39064;&#30340;&#19968;&#31181;&#26041;&#24335;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#20197;3D&#34507;&#30333;&#32467;&#26500;&#20026;&#36755;&#20837;&#20849;&#21516;&#22788;&#29702;&#22810;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01693v1 Announce Type: new  Abstract: Understanding and leveraging the 3D structures of proteins is central to a variety of biological and drug discovery tasks. While deep learning has been applied successfully for structure-based protein function prediction tasks, current methods usually employ distinct training for each task. However, each of the tasks is of small size, and such a single-task strategy hinders the models' performance and generalization ability. As some labeled 3D protein datasets are biologically related, combining multi-source datasets for larger-scale multi-task learning is one way to overcome this problem. In this paper, we propose a neural network model to address multiple tasks jointly upon the input of 3D protein structures. In particular, we first construct a standard structure-based multi-task benchmark called Protein-MT, consisting of 6 biologically relevant tasks, including affinity prediction and property prediction, integrated from 4 public data
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26680;&#22823;&#23567;&#32553;&#25918;&#25552;&#39640;&#23884;&#20837;&#24335;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;&#23398;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.01685</link><description>&lt;p&gt;
&#36890;&#36807;&#26680;&#22823;&#23567;&#32553;&#25918;&#25552;&#39640;&#23884;&#20837;&#24335;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;&#23398;
&lt;/p&gt;
&lt;p&gt;
A Methodology for Improving Accuracy of Embedded Spiking Neural Networks through Kernel Size Scaling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01685
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26680;&#22823;&#23567;&#32553;&#25918;&#25552;&#39640;&#23884;&#20837;&#24335;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;&#23398;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#30001;&#20110;&#20854;&#31232;&#30095;&#30340;&#22522;&#20110;&#33033;&#20914;&#30340;&#25805;&#20316;&#32780;&#33021;&#20026;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#24212;&#29992;&#25552;&#20379;&#36229;&#20302;&#21151;&#32791;/&#33021;&#32791;&#12290;&#30446;&#21069;&#65292;&#22823;&#22810;&#25968;SNN&#26550;&#26500;&#38656;&#35201;&#26356;&#22823;&#30340;&#27169;&#22411;&#22823;&#23567;&#25165;&#33021;&#23454;&#29616;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#36825;&#23545;&#36164;&#28304;&#21463;&#38480;&#30340;&#23884;&#20837;&#24335;&#24212;&#29992;&#19981;&#22826;&#36866;&#21512;&#12290;&#22240;&#27492;&#65292;&#36843;&#20999;&#38656;&#35201;&#24320;&#21457;&#33021;&#22815;&#20197;&#21487;&#25509;&#21463;&#30340;&#20869;&#23384;&#21344;&#29992;&#23454;&#29616;&#39640;&#20934;&#30830;&#24615;&#30340;SNNs&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26680;&#22823;&#23567;&#32553;&#25918;&#25552;&#39640;SNNs&#20934;&#30830;&#24615;&#30340;&#26032;&#26041;&#27861;&#23398;&#12290;&#20854;&#20851;&#38190;&#27493;&#39588;&#21253;&#25324;&#35843;&#26597;&#19981;&#21516;&#26680;&#22823;&#23567;&#23545;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#65292;&#35774;&#35745;&#26032;&#30340;&#26680;&#22823;&#23567;&#38598;&#21512;&#65292;&#22522;&#20110;&#36873;&#23450;&#30340;&#26680;&#22823;&#23567;&#29983;&#25104;SNN&#26550;&#26500;&#65292;&#24182;&#20998;&#26512;SNN&#27169;&#22411;&#36873;&#25321;&#30340;&#20934;&#30830;&#24615;-&#20869;&#23384;&#25240;&#34935;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23398;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65288;&#23545;&#20110;CIFAR10&#26377;93.24%&#30340;&#20934;&#30830;&#24230;&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01685v1 Announce Type: cross  Abstract: Spiking Neural Networks (SNNs) can offer ultra low power/ energy consumption for machine learning-based applications due to their sparse spike-based operations. Currently, most of the SNN architectures need a significantly larger model size to achieve higher accuracy, which is not suitable for resource-constrained embedded applications. Therefore, developing SNNs that can achieve high accuracy with acceptable memory footprint is highly needed. Toward this, we propose a novel methodology that improves the accuracy of SNNs through kernel size scaling. Its key steps include investigating the impact of different kernel sizes on the accuracy, devising new sets of kernel sizes, generating SNN architectures based on the selected kernel sizes, and analyzing the accuracy-memory trade-offs for SNN model selection. The experimental results show that our methodology achieves higher accuracy than state-of-the-art (93.24% accuracy for CIFAR10 and 70
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#24046;&#20998;&#38544;&#31169;&#20316;&#20026;&#28608;&#21169;&#26426;&#21046;&#65292;&#20013;&#20171;&#32773;&#36890;&#36807;&#36125;&#21494;&#26031;&#24778;&#35766;&#20540;&#26469;&#20215;&#20540;&#21270;&#25200;&#21160;&#30340;&#20805;&#20998;&#32479;&#35745;&#37327;&#65292;&#23454;&#29616;&#38544;&#31169;-&#20272;&#20540;&#26435;&#34913;&#65292;&#21516;&#26102;&#20445;&#25252;&#24046;&#20998;&#38544;&#31169;&#21644;&#39640;&#24230;&#30456;&#20284;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.01676</link><description>&lt;p&gt;
&#31169;&#20154;&#21512;&#20316;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#28608;&#21169;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Incentives in Private Collaborative Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01676
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#24046;&#20998;&#38544;&#31169;&#20316;&#20026;&#28608;&#21169;&#26426;&#21046;&#65292;&#20013;&#20171;&#32773;&#36890;&#36807;&#36125;&#21494;&#26031;&#24778;&#35766;&#20540;&#26469;&#20215;&#20540;&#21270;&#25200;&#21160;&#30340;&#20805;&#20998;&#32479;&#35745;&#37327;&#65292;&#23454;&#29616;&#38544;&#31169;-&#20272;&#20540;&#26435;&#34913;&#65292;&#21516;&#26102;&#20445;&#25252;&#24046;&#20998;&#38544;&#31169;&#21644;&#39640;&#24230;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21327;&#20316;&#26426;&#22120;&#23398;&#20064;&#28041;&#21450;&#22312;&#26469;&#33258;&#22810;&#26041;&#25968;&#25454;&#30340;&#27169;&#22411;&#19978;&#35757;&#32451;&#65292;&#20294;&#24517;&#39035;&#28608;&#21169;&#21508;&#26041;&#21442;&#19982;&#12290;&#29616;&#26377;&#25968;&#25454;&#20272;&#20540;&#26041;&#27861;&#20844;&#24179;&#22320;&#26681;&#25454;&#20849;&#20139;&#25968;&#25454;&#25110;&#27169;&#22411;&#21442;&#25968;&#20215;&#20540;&#21644;&#22870;&#21169;&#27599;&#20010;&#21442;&#19982;&#26041;&#65292;&#20294;&#24573;&#35270;&#20102;&#28041;&#21450;&#30340;&#38544;&#31169;&#39118;&#38505;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#20316;&#20026;&#19968;&#31181;&#28608;&#21169;&#26426;&#21046;&#12290;&#27599;&#20010;&#21442;&#19982;&#26041;&#21487;&#20197;&#36873;&#25321;&#20854;&#38656;&#35201;&#30340;DP&#20445;&#35777;&#65292;&#24182;&#30456;&#24212;&#22320;&#25200;&#21160;&#20854;&#20805;&#20998;&#32479;&#35745;&#37327;&#65288;SS&#65289;&#12290;&#20013;&#20171;&#32773;&#36890;&#36807;&#27169;&#22411;&#21442;&#25968;&#25152;&#24341;&#21457;&#30340;&#36125;&#21494;&#26031;&#24778;&#35766;&#20540;&#26469;&#20215;&#20540;&#21270;&#25200;&#21160;&#30340;SS&#12290;&#30001;&#20110;&#25105;&#20204;&#30340;&#20272;&#20540;&#20989;&#25968;&#24378;&#21046;&#23454;&#26045;&#38544;&#31169;-&#20272;&#20540;&#26435;&#34913;&#65292;&#21508;&#26041;&#34987;&#38459;&#27490;&#36873;&#25321;&#36807;&#39640;&#30340;DP&#20445;&#35777;&#65292;&#20197;&#20943;&#23569;&#22823;&#32852;&#30431;&#27169;&#22411;&#30340;&#25928;&#29992;&#12290;&#26368;&#21518;&#65292;&#20013;&#20171;&#32773;&#29992;&#19981;&#21516;&#30340;&#21518;&#39564;&#27169;&#22411;&#21442;&#25968;&#26679;&#26412;&#22870;&#21169;&#27599;&#20010;&#21442;&#19982;&#26041;&#12290;&#36825;&#31181;&#22870;&#21169;&#20173;&#28982;&#28385;&#36275;&#20844;&#24179;&#31561;&#29616;&#26377;&#28608;&#21169;&#35201;&#27714;&#65292;&#20294;&#21478;&#22806;&#36824;&#20445;&#25252;&#24046;&#20998;&#38544;&#31169;&#21644;&#39640;&#24230;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01676v1 Announce Type: new  Abstract: Collaborative machine learning involves training models on data from multiple parties but must incentivize their participation. Existing data valuation methods fairly value and reward each party based on shared data or model parameters but neglect the privacy risks involved. To address this, we introduce differential privacy (DP) as an incentive. Each party can select its required DP guarantee and perturb its sufficient statistic (SS) accordingly. The mediator values the perturbed SS by the Bayesian surprise it elicits about the model parameters. As our valuation function enforces a privacy-valuation trade-off, parties are deterred from selecting excessive DP guarantees that reduce the utility of the grand coalition's model. Finally, the mediator rewards each party with different posterior samples of the model parameters. Such rewards still satisfy existing incentives like fairness but additionally preserve DP and a high similarity to th
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22823;&#35268;&#27169;Twitter&#30740;&#31350;&#65292;&#26412;&#30740;&#31350;&#23581;&#35797;&#29702;&#35299;COVID-19&#22914;&#20309;&#24433;&#21709;&#21453;&#30123;&#33495;&#35328;&#35770;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#26041;&#27861;&#21435;&#35782;&#21035;&#20855;&#20307;&#21407;&#22240;/&#20851;&#27880;&#28857;&#12290;</title><link>https://arxiv.org/abs/2404.01669</link><description>&lt;p&gt;
COVID-19&#22914;&#20309;&#24433;&#21709;&#21453;&#30123;&#33495;&#35328;&#35770;&#65306;&#19968;&#20010;&#36328;&#36234;&#30123;&#24773;&#21069;&#21518;&#30340;&#22823;&#35268;&#27169;Twitter&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
How COVID-19 has Impacted the Anti-Vaccine Discourse: A Large-Scale Twitter Study Spanning Pre-COVID and Post-COVID Era
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01669
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22823;&#35268;&#27169;Twitter&#30740;&#31350;&#65292;&#26412;&#30740;&#31350;&#23581;&#35797;&#29702;&#35299;COVID-19&#22914;&#20309;&#24433;&#21709;&#21453;&#30123;&#33495;&#35328;&#35770;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#26041;&#27861;&#21435;&#35782;&#21035;&#20855;&#20307;&#21407;&#22240;/&#20851;&#27880;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#20110;&#30123;&#33495;&#30340;&#20105;&#35770;&#24050;&#32463;&#25345;&#32493;&#20102;&#20960;&#21313;&#24180;&#65292;&#20294;COVID-19&#22823;&#27969;&#34892;&#23637;&#31034;&#20102;&#29702;&#35299;&#21644;&#20943;&#23569;&#21453;&#30123;&#33495;&#24773;&#32490;&#30340;&#37325;&#35201;&#24615;&#12290;&#23613;&#31649;&#30123;&#24773;&#21487;&#33021;&#24050;&#32463;&#32467;&#26463;&#65292;&#20294;&#20173;&#28982;&#24456;&#37325;&#35201;&#29702;&#35299;&#30123;&#24773;&#22914;&#20309;&#24433;&#21709;&#20102;&#21453;&#30123;&#33495;&#35805;&#35821;&#65292;&#24182;&#19988;&#22312;&#30123;&#24773;&#32467;&#26463;&#21518;&#21453;&#23545;&#38750;COVID&#30123;&#33495;&#65288;&#22914;&#27969;&#24863;&#12289;&#40635;&#30137;&#12289;&#33034;&#39635;&#28784;&#36136;&#28814;&#12289;HPV&#30123;&#33495;&#65289;&#30340;&#35770;&#28857;&#26159;&#21542;&#20063;&#22240;&#30123;&#24773;&#32780;&#25913;&#21464;&#12290;&#26412;&#30740;&#31350;&#35797;&#22270;&#36890;&#36807;&#23545;Twitter&#19978;&#30340;&#21453;&#30123;&#33495;&#24086;&#23376;&#36827;&#34892;&#22823;&#35268;&#27169;&#30740;&#31350;&#26469;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#12290;&#20960;&#20046;&#25152;&#26377;&#20197;&#31038;&#20132;&#23186;&#20307;&#20102;&#35299;&#21453;&#30123;&#33495;&#35266;&#28857;&#30340;&#20808;&#21069;&#30740;&#31350;&#37117;&#21482;&#32771;&#34385;&#21453;&#30123;&#33495;&#12289;&#36190;&#25104;&#30123;&#33495;&#21644;&#20013;&#31435;&#19977;&#31181;&#24191;&#27867;&#31435;&#22330;&#12290;&#36804;&#20170;&#20026;&#27490;&#36824;&#27809;&#26377;&#21162;&#21147;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#22823;&#35268;&#27169;&#35782;&#21035;&#21453;&#30123;&#33495;&#24773;&#32490;&#32972;&#21518;&#30340;&#20855;&#20307;&#21407;&#22240;/&#20851;&#27880;&#28857;&#65288;&#22914;&#21103;&#20316;&#29992;&#12289;&#38452;&#35851;&#35770;&#12289;&#25919;&#27835;&#21407;&#22240;&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#23558;&#25512;&#25991;&#20998;&#31867;&#20026;11&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01669v1 Announce Type: cross  Abstract: The debate around vaccines has been going on for decades, but the COVID-19 pandemic showed how crucial it is to understand and mitigate anti-vaccine sentiments. While the pandemic may be over, it is still important to understand how the pandemic affected the anti-vaccine discourse, and whether the arguments against non-COVID vaccines (e.g., Flu, MMR, IPV, HPV vaccines) have also changed due to the pandemic. This study attempts to answer these questions through a large-scale study of anti-vaccine posts on Twitter. Almost all prior works that utilized social media to understand anti-vaccine opinions considered only the three broad stances of Anti-Vax, Pro-Vax, and Neutral. There has not been any effort to identify the specific reasons/concerns behind the anti-vax sentiments (e.g., side-effects, conspiracy theories, political reasons) on social media at scale. In this work, we propose two novel methods for classifying tweets into 11 diffe
&lt;/p&gt;</description></item><item><title>&#21457;&#24067;&#26085;&#35821;&#39044;&#35757;&#32451;&#27169;&#22411;&#20197;&#32553;&#23567;&#38750;&#33521;&#35821;&#31038;&#21306;&#20013;&#30340;AI&#35775;&#38382;&#24046;&#36317;&#65292;&#20419;&#36827;AI&#27665;&#20027;&#21270;&#12290;</title><link>https://arxiv.org/abs/2404.01657</link><description>&lt;p&gt;
&#21457;&#24067;&#38024;&#23545;&#26085;&#35821;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Release of Pre-Trained Models for the Japanese Language
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01657
&lt;/p&gt;
&lt;p&gt;
&#21457;&#24067;&#26085;&#35821;&#39044;&#35757;&#32451;&#27169;&#22411;&#20197;&#32553;&#23567;&#38750;&#33521;&#35821;&#31038;&#21306;&#20013;&#30340;AI&#35775;&#38382;&#24046;&#36317;&#65292;&#20419;&#36827;AI&#27665;&#20027;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01657v1 &#20844;&#21578;&#31867;&#22411;:&#20132;&#21449;&#25688;&#35201;: AI&#27665;&#20027;&#21270;&#26088;&#22312;&#21019;&#36896;&#19968;&#20010;&#26222;&#36890;&#20154;&#21487;&#20197;&#21033;&#29992;AI&#25216;&#26415;&#30340;&#19990;&#30028;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#35768;&#22810;&#30740;&#31350;&#26426;&#26500;&#24050;&#32463;&#35797;&#22270;&#35753;&#20182;&#20204;&#30340;&#32467;&#26524;&#23545;&#20844;&#20247;&#21487;&#21450;&#12290;&#29305;&#21035;&#26159;&#65292;&#22522;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#35757;&#32451;&#30340;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#23637;&#29616;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#28508;&#21147;&#65292;&#23427;&#20204;&#30340;&#21457;&#24067;&#20135;&#29983;&#20102;&#37325;&#22823;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#21457;&#24067;&#30340;&#27169;&#22411;&#19987;&#38376;&#38024;&#23545;&#33521;&#35821;&#65292;&#22240;&#27492;&#65292;&#22312;&#38750;&#33521;&#35821;&#31038;&#21306;&#20013;&#65292;AI&#27665;&#20027;&#21270;&#23384;&#22312;&#26126;&#26174;&#28382;&#21518;&#12290;&#20026;&#20102;&#32553;&#23567;AI&#35775;&#38382;&#30340;&#24046;&#36317;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#29992;&#26085;&#35821;&#39044;&#20808;&#35757;&#32451;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#65288;GPT&#65289;&#12289;&#23545;&#27604;&#35821;&#35328;&#21644;&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#12289;&#31283;&#23450;&#25193;&#25955;&#21644;&#38544;&#34255;&#21333;&#20803;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;&#26469;&#33258;&#21464;&#21387;&#22120;&#65288;HuBERT&#65289;&#12290;&#36890;&#36807;&#25552;&#20379;&#36825;&#20123;&#27169;&#22411;&#65292;&#29992;&#25143;&#21487;&#20197;&#33258;&#30001;&#22320;&#19982;&#31526;&#21512;&#26085;&#26412;&#25991;&#21270;&#20215;&#20540;&#35266;&#30340;AI&#36827;&#34892;&#20132;&#20114;&#65292;&#24182;&#30830;&#20445;&#26085;&#26412;&#25991;&#21270;&#30340;&#36523;&#20221;&#65292;&#20174;&#32780;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01657v1 Announce Type: cross  Abstract: AI democratization aims to create a world in which the average person can utilize AI techniques. To achieve this goal, numerous research institutes have attempted to make their results accessible to the public. In particular, large pre-trained models trained on large-scale data have shown unprecedented potential, and their release has had a significant impact. However, most of the released models specialize in the English language, and thus, AI democratization in non-English-speaking communities is lagging significantly. To reduce this gap in AI access, we released Generative Pre-trained Transformer (GPT), Contrastive Language and Image Pre-training (CLIP), Stable Diffusion, and Hidden-unit Bidirectional Encoder Representations from Transformers (HuBERT) pre-trained in Japanese. By providing these models, users can freely interface with AI that aligns with Japanese cultural values and ensures the identity of Japanese culture, thus enha
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#27979;&#35797;&#26102;&#38388;&#21069;&#21521;&#36866;&#24212;&#65288;FOA&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#26080;&#23548;&#25968;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#36866;&#24212;&#36827;&#21270;&#31574;&#30053;&#20165;&#23398;&#20064;&#26032;&#28155;&#21152;&#30340;&#25552;&#31034;&#65292;&#20197;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#35774;&#22791;&#19978;&#23454;&#29616;&#27169;&#22411;&#36866;&#24212;&#12290;</title><link>https://arxiv.org/abs/2404.01650</link><description>&lt;p&gt;
&#20165;&#20351;&#29992;&#21069;&#21521;&#20256;&#25773;&#30340;&#27979;&#35797;&#26102;&#38388;&#27169;&#22411;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Test-Time Model Adaptation with Only Forward Passes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01650
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#27979;&#35797;&#26102;&#38388;&#21069;&#21521;&#36866;&#24212;&#65288;FOA&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#26080;&#23548;&#25968;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#36866;&#24212;&#36827;&#21270;&#31574;&#30053;&#20165;&#23398;&#20064;&#26032;&#28155;&#21152;&#30340;&#25552;&#31034;&#65292;&#20197;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#35774;&#22791;&#19978;&#23454;&#29616;&#27169;&#22411;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#24050;&#34987;&#35777;&#26126;&#22312;&#36866;&#24212;&#32473;&#23450;&#35757;&#32451;&#27169;&#22411;&#21040;&#20855;&#26377;&#28508;&#22312;&#20998;&#24067;&#36716;&#31227;&#30340;&#26410;&#35265;&#27979;&#35797;&#26679;&#26412;&#26102;&#26159;&#26377;&#25928;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#27169;&#22411;&#36890;&#24120;&#37096;&#32626;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#35774;&#22791;&#19978;&#65292;&#20363;&#22914;FPGA&#65292;&#24182;&#19988;&#36890;&#24120;&#34987;&#37327;&#21270;&#21644;&#30828;&#32534;&#30721;&#20026;&#19981;&#21487;&#20462;&#25913;&#30340;&#21442;&#25968;&#20197;&#21152;&#36895;&#12290;&#37492;&#20110;&#27492;&#65292;&#30001;&#20110;&#29616;&#26377;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#20110;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#21453;&#21521;&#20256;&#25773;&#36827;&#34892;&#27169;&#22411;&#26356;&#26032;&#65292;&#22240;&#27492;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27979;&#35797;&#26102;&#38388;&#21069;&#21521;&#36866;&#24212;&#65288;FOA&#65289;&#26041;&#27861;&#12290; &#22312;FOA&#20013;&#65292;&#25105;&#20204;&#35797;&#22270;&#36890;&#36807;&#19968;&#20010;&#26080;&#23548;&#25968;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#36866;&#24212;&#36827;&#21270;&#31574;&#30053;&#26469;&#20165;&#23398;&#20064;&#26032;&#28155;&#21152;&#30340;&#25552;&#31034;&#65288;&#20316;&#20026;&#27169;&#22411;&#30340;&#36755;&#20837;&#65289;&#12290; &#20026;&#20102;&#20351;&#36825;&#31181;&#31574;&#30053;&#22312;&#25105;&#20204;&#30340;&#22312;&#32447;&#26080;&#30417;&#30563;&#35774;&#32622;&#19979;&#31283;&#23450;&#24037;&#20316;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#36890;&#36807;&#34913;&#37327;&#27979;&#35797;&#35757;&#32451;&#32479;&#35745;&#24046;&#24322;&#21644;&#27169;&#22411;&#39044;&#27979;&#29109;&#30340;&#26032;&#39062;&#36866;&#24212;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#28608;&#27963;&#31227;&#20301;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01650v1 Announce Type: new  Abstract: Test-time adaptation has proven effective in adapting a given trained model to unseen test samples with potential distribution shifts. However, in real-world scenarios, models are usually deployed on resource-limited devices, e.g., FPGAs, and are often quantized and hard-coded with non-modifiable parameters for acceleration. In light of this, existing methods are often infeasible since they heavily depend on computation-intensive backpropagation for model updating that may be not supported. To address this, we propose a test-time Forward-Only Adaptation (FOA) method. In FOA, we seek to solely learn a newly added prompt (as model's input) via a derivative-free covariance matrix adaptation evolution strategy. To make this strategy work stably under our online unsupervised setting, we devise a novel fitness function by measuring test-training statistic discrepancy and model prediction entropy. Moreover, we design an activation shifting sche
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;Transformer-based&#39044;&#27979;&#19982;weighted constrained Dynamic Time Warping (wcDTW)&#30456;&#32467;&#21512;&#65292;&#25913;&#36827;&#20102;&#23454;&#26102;&#30005;&#27744;&#31454;&#26631;&#20013;&#30340;&#22330;&#26223;&#36873;&#25321;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#25311;&#23637;&#31034;&#20102;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#32422;10%&#30340;&#25910;&#30410;&#22686;&#38271;&#12290;</title><link>https://arxiv.org/abs/2404.01646</link><description>&lt;p&gt;
Transformer&#36935;&#35265;wcDTW&#20197;&#25913;&#36827;&#23454;&#26102;&#30005;&#27744;&#31454;&#26631;&#65306;&#19968;&#31181;&#26032;&#30340;&#22330;&#26223;&#36873;&#25321;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Transformer meets wcDTW to improve real-time battery bids: A new approach to scenario selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01646
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;Transformer-based&#39044;&#27979;&#19982;weighted constrained Dynamic Time Warping (wcDTW)&#30456;&#32467;&#21512;&#65292;&#25913;&#36827;&#20102;&#23454;&#26102;&#30005;&#27744;&#31454;&#26631;&#20013;&#30340;&#22330;&#26223;&#36873;&#25321;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#25311;&#23637;&#31034;&#20102;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#32422;10%&#30340;&#25910;&#30410;&#22686;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#26102;&#33021;&#28304;&#24066;&#22330;&#20013;&#38543;&#26426;&#30005;&#27744;&#31454;&#26631;&#26159;&#19968;&#20010;&#24494;&#22937;&#30340;&#36807;&#31243;&#65292;&#20854;&#26377;&#25928;&#24615;&#21462;&#20915;&#20110;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#20197;&#21450;&#36873;&#25321;&#29992;&#20110;&#20248;&#21270;&#30340;&#20195;&#34920;&#24615;&#22330;&#26223;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#23558;&#22522;&#20110;Transformer&#30340;&#39044;&#27979;&#19982;&#21152;&#26435;&#32422;&#26463;&#21160;&#24577;&#26102;&#38388;&#25197;&#26354;&#65288;wcDTW&#65289;&#30456;&#32467;&#21512;&#65292;&#20197;&#25913;&#36827;&#22330;&#26223;&#36873;&#25321;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;Transformer&#30340;&#39044;&#27979;&#33021;&#21147;&#26469;&#39044;&#27979;&#33021;&#28304;&#20215;&#26684;&#65292;&#32780;wcDTW&#36890;&#36807;&#32500;&#25252;&#22810;&#20010;&#19981;&#30830;&#23450;&#20135;&#21697;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#30830;&#20445;&#36873;&#25321;&#30456;&#20851;&#30340;&#21382;&#21490;&#22330;&#26223;&#12290;&#36890;&#36807;&#22312;2023&#24180;7&#26376;&#30340;PJM&#24066;&#22330;&#36827;&#34892;&#24191;&#27867;&#30340;&#27169;&#25311;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#25910;&#30410;&#22686;&#21152;&#20102;10&#65285;&#65292;&#31361;&#26174;&#20102;&#20854;&#26377;&#21487;&#33021;&#22312;&#23454;&#26102;&#24066;&#22330;&#20013;&#38761;&#26032;&#30005;&#27744;&#31454;&#26631;&#31574;&#30053;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01646v1 Announce Type: new  Abstract: Stochastic battery bidding in real-time energy markets is a nuanced process, with its efficacy depending on the accuracy of forecasts and the representative scenarios chosen for optimization. In this paper, we introduce a pioneering methodology that amalgamates Transformer-based forecasting with weighted constrained Dynamic Time Warping (wcDTW) to refine scenario selection. Our approach harnesses the predictive capabilities of Transformers to foresee Energy prices, while wcDTW ensures the selection of pertinent historical scenarios by maintaining the coherence between multiple uncertain products. Through extensive simulations in the PJM market for July 2023, our method exhibited a 10% increase in revenue compared to the conventional method, highlighting its potential to revolutionize battery bidding strategies in real-time markets.
&lt;/p&gt;</description></item><item><title>ContrastCAD&#25552;&#20986;&#20102;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;CAD&#27169;&#22411;&#26500;&#24314;&#24207;&#21015;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#20351;&#29992;RRE&#26041;&#27861;&#22686;&#24378;&#20102;&#35757;&#32451;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.01645</link><description>&lt;p&gt;
ContrastCAD&#65306;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#35745;&#31639;&#26426;&#36741;&#21161;&#35774;&#35745;&#27169;&#22411;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
ContrastCAD: Contrastive Learning-based Representation Learning for Computer-Aided Design Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01645
&lt;/p&gt;
&lt;p&gt;
ContrastCAD&#25552;&#20986;&#20102;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;CAD&#27169;&#22411;&#26500;&#24314;&#24207;&#21015;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#20351;&#29992;RRE&#26041;&#27861;&#22686;&#24378;&#20102;&#35757;&#32451;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer-based&#27169;&#22411;&#30340;&#25104;&#21151;&#40723;&#33310;&#20102;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#20351;&#29992;&#22522;&#20110;&#24207;&#21015;&#30340;&#26041;&#27861;&#23398;&#20064;CAD&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#23398;&#20064;CAD&#27169;&#22411;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#34987;&#34920;&#31034;&#20026;&#20855;&#26377;&#38271;&#26500;&#24314;&#24207;&#21015;&#30340;&#22797;&#26434;&#24418;&#29366;&#12290;&#27492;&#22806;&#65292;&#30456;&#21516;&#30340;CAD&#27169;&#22411;&#21487;&#20197;&#29992;&#19981;&#21516;&#30340;CAD&#26500;&#24314;&#24207;&#21015;&#26469;&#34920;&#31034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ContrastCAD&#30340;&#26032;&#39062;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#22320;&#25429;&#25417;CAD&#27169;&#22411;&#26500;&#24314;&#24207;&#21015;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;ContrastCAD&#20351;&#29992;&#36749;&#23398;&#25216;&#26415;&#29983;&#25104;&#22686;&#24378;&#35270;&#22270;&#32780;&#19981;&#25913;&#21464;CAD&#27169;&#22411;&#30340;&#24418;&#29366;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;CAD&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#31216;&#20026;&#38543;&#26426;&#26367;&#25442;&#21644;&#25380;&#20986;&#65288;RRE&#65289;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#27169;&#22411;&#22312;&#35757;&#32451;&#19981;&#24179;&#34913;&#30340;&#35757;&#32451;CAD&#25968;&#25454;&#38598;&#26102;&#30340;&#23398;&#20064;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;RRE&#22686;&#24378;&#26041;&#27861;&#26174;&#30528;&#25552;&#39640;&#20102;&#23398;&#20064;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01645v1 Announce Type: cross  Abstract: The success of Transformer-based models has encouraged many researchers to learn CAD models using sequence-based approaches. However, learning CAD models is still a challenge, because they can be represented as complex shapes with long construction sequences. Furthermore, the same CAD model can be expressed using different CAD construction sequences. We propose a novel contrastive learning-based approach, named ContrastCAD, that effectively captures semantic information within the construction sequences of the CAD model. ContrastCAD generates augmented views using dropout techniques without altering the shape of the CAD model. We also propose a new CAD data augmentation method, called a Random Replace and Extrude (RRE) method, to enhance the learning performance of the model when training an imbalanced training CAD dataset. Experimental results show that the proposed RRE augmentation method significantly enhances the learning performan
&lt;/p&gt;</description></item><item><title>ADVREPAIR&#26159;&#19968;&#31181;&#21033;&#29992;&#26377;&#38480;&#25968;&#25454;&#36827;&#34892;&#23545;&#25239;&#25915;&#20987;&#30340;&#21487;&#35777;&#20462;&#22797;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24418;&#24335;&#39564;&#35777;&#26500;&#24314;&#34917;&#19969;&#27169;&#22359;&#65292;&#22312;&#31283;&#20581;&#37051;&#22495;&#20869;&#25552;&#20379;&#21487;&#35777;&#21644;&#19987;&#38376;&#30340;&#20462;&#22797;&#65292;&#21516;&#26102;&#20855;&#26377;&#27867;&#21270;&#21040;&#20854;&#20182;&#36755;&#20837;&#30340;&#38450;&#24481;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.01642</link><description>&lt;p&gt;
ADVREPAIR&#65306;&#23545;&#25239;&#25915;&#20987;&#30340;&#21487;&#35777;&#20462;&#22797;
&lt;/p&gt;
&lt;p&gt;
ADVREPAIR:Provable Repair of Adversarial Attack
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01642
&lt;/p&gt;
&lt;p&gt;
ADVREPAIR&#26159;&#19968;&#31181;&#21033;&#29992;&#26377;&#38480;&#25968;&#25454;&#36827;&#34892;&#23545;&#25239;&#25915;&#20987;&#30340;&#21487;&#35777;&#20462;&#22797;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24418;&#24335;&#39564;&#35777;&#26500;&#24314;&#34917;&#19969;&#27169;&#22359;&#65292;&#22312;&#31283;&#20581;&#37051;&#22495;&#20869;&#25552;&#20379;&#21487;&#35777;&#21644;&#19987;&#38376;&#30340;&#20462;&#22797;&#65292;&#21516;&#26102;&#20855;&#26377;&#27867;&#21270;&#21040;&#20854;&#20182;&#36755;&#20837;&#30340;&#38450;&#24481;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#22312;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#20013;&#30340;&#37096;&#32626;&#26085;&#30410;&#22686;&#21152;&#65292;&#20294;&#23427;&#20204;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#26500;&#25104;&#20005;&#37325;&#30340;&#23433;&#20840;&#39118;&#38505;&#12290;&#29616;&#26377;&#30340;&#20351;&#29992;&#26377;&#38480;&#25968;&#25454;&#30340;&#31070;&#32463;&#20803;&#32423;&#26041;&#27861;&#22312;&#20462;&#22797;&#23545;&#25163;&#26041;&#38754;&#32570;&#20047;&#25928;&#21147;&#65292;&#22240;&#20026;&#23545;&#25239;&#25915;&#20987;&#26426;&#21046;&#30340;&#22266;&#26377;&#22797;&#26434;&#24615;&#65292;&#32780;&#23545;&#25239;&#35757;&#32451;&#65292;&#21033;&#29992;&#22823;&#37327;&#23545;&#25239;&#26679;&#26412;&#22686;&#24378;&#40065;&#26834;&#24615;&#65292;&#32570;&#20047;&#21487;&#35777;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ADVREPAIR&#65292;&#19968;&#31181;&#21033;&#29992;&#26377;&#38480;&#25968;&#25454;&#36827;&#34892;&#23545;&#25239;&#25915;&#20987;&#30340;&#21487;&#35777;&#20462;&#22797;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#24418;&#24335;&#39564;&#35777;&#65292;ADVREPAIR&#26500;&#24314;&#34917;&#19969;&#27169;&#22359;&#65292;&#24403;&#19982;&#21407;&#22987;&#32593;&#32476;&#38598;&#25104;&#26102;&#65292;&#22312;&#31283;&#20581;&#37051;&#22495;&#20869;&#25552;&#20379;&#21487;&#35777;&#21644;&#19987;&#38376;&#30340;&#20462;&#22797;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#21253;&#25324;&#19968;&#31181;&#21551;&#21457;&#24335;&#26426;&#21046;&#26469;&#20998;&#37197;&#34917;&#19969;&#27169;&#22359;&#65292;&#20351;&#24471;&#36825;&#31181;&#38450;&#24481;&#23545;&#25239;&#25915;&#20987;&#27867;&#21270;&#21040;&#20854;&#20182;&#36755;&#20837;&#12290;ADVREPAIR&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01642v1 Announce Type: new  Abstract: Deep neural networks (DNNs) are increasingly deployed in safety-critical domains, but their vulnerability to adversarial attacks poses serious safety risks. Existing neuron-level methods using limited data lack efficacy in fixing adversaries due to the inherent complexity of adversarial attack mechanisms, while adversarial training, leveraging a large number of adversarial samples to enhance robustness, lacks provability. In this paper, we propose ADVREPAIR, a novel approach for provable repair of adversarial attacks using limited data. By utilizing formal verification, ADVREPAIR constructs patch modules that, when integrated with the original network, deliver provable and specialized repairs within the robustness neighborhood. Additionally, our approach incorporates a heuristic mechanism for assigning patch modules, allowing this defense against adversarial attacks to generalize to other inputs. ADVREPAIR demonstrates superior efficienc
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#24555;&#36895;&#25511;&#21046;&#25668;&#20687;&#26426;&#26333;&#20809;&#30340;&#26032;&#26694;&#26550;&#65292;&#20855;&#26377;&#31616;&#21270;&#35757;&#32451;&#22330;&#22320;&#12289;&#22870;&#21169;&#35774;&#35745;&#21644;&#26333;&#20809;&#35843;&#25972;&#33021;&#21147;&#36880;&#27493;&#25913;&#21892;&#31561;&#22235;&#22823;&#21019;&#26032;&#36129;&#29486;</title><link>https://arxiv.org/abs/2404.01636</link><description>&lt;p&gt;
&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#23398;&#20064;&#25511;&#21046;&#25668;&#20687;&#26426;&#26333;&#20809;
&lt;/p&gt;
&lt;p&gt;
Learning to Control Camera Exposure via Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01636
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#24555;&#36895;&#25511;&#21046;&#25668;&#20687;&#26426;&#26333;&#20809;&#30340;&#26032;&#26694;&#26550;&#65292;&#20855;&#26377;&#31616;&#21270;&#35757;&#32451;&#22330;&#22320;&#12289;&#22870;&#21169;&#35774;&#35745;&#21644;&#26333;&#20809;&#35843;&#25972;&#33021;&#21147;&#36880;&#27493;&#25913;&#21892;&#31561;&#22235;&#22823;&#21019;&#26032;&#36129;&#29486;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35843;&#25972;&#25668;&#20687;&#26426;&#26333;&#20809;&#26159;&#30830;&#20445;&#35745;&#31639;&#26426;&#35270;&#35273;&#24212;&#29992;&#21151;&#33021;&#30340;&#31532;&#19968;&#27493;&#12290;&#20256;&#32479;&#30340;&#25668;&#20687;&#26426;&#26333;&#20809;&#25511;&#21046;&#26041;&#27861;&#38656;&#35201;&#22810;&#27425;&#25910;&#25947;&#21644;&#32791;&#26102;&#30340;&#27969;&#31243;&#65292;&#20351;&#20854;&#19981;&#36866;&#29992;&#20110;&#21160;&#24577;&#29031;&#26126;&#26465;&#20214;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#24555;&#36895;&#25511;&#21046;&#25668;&#20687;&#26426;&#26333;&#20809;&#30340;&#26032;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#22235;&#20010;&#36129;&#29486;&#65306;1&#65289;&#31616;&#21270;&#35757;&#32451;&#22330;&#22320;&#26469;&#27169;&#25311;&#29616;&#23454;&#19990;&#30028;&#30340;&#22810;&#26679;&#21270;&#21644;&#21160;&#24577;&#29031;&#26126;&#21464;&#21270;&#65292;2&#65289;&#38378;&#28865;&#21644;&#22270;&#20687;&#23646;&#24615;&#24863;&#30693;&#22870;&#21169;&#35774;&#35745;&#65292;&#20197;&#21450;&#36731;&#37327;&#32423;&#29366;&#24577;&#35774;&#35745;&#20197;&#36827;&#34892;&#23454;&#26102;&#22788;&#29702;&#65292;3&#65289;&#38745;&#24577;&#21040;&#21160;&#24577;&#29031;&#26126;&#35838;&#31243;&#65292;&#36880;&#27493;&#25913;&#21892;&#20195;&#29702;&#30340;&#26333;&#20809;&#35843;&#25972;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01636v1 Announce Type: cross  Abstract: Adjusting camera exposure in arbitrary lighting conditions is the first step to ensure the functionality of computer vision applications. Poorly adjusted camera exposure often leads to critical failure and performance degradation. Traditional camera exposure control methods require multiple convergence steps and time-consuming processes, making them unsuitable for dynamic lighting conditions. In this paper, we propose a new camera exposure control framework that rapidly controls camera exposure while performing real-time processing by exploiting deep reinforcement learning. The proposed framework consists of four contributions: 1) a simplified training ground to simulate real-world's diverse and dynamic lighting changes, 2) flickering and image attribute-aware reward design, along with lightweight state design for real-time processing, 3) a static-to-dynamic lighting curriculum to gradually improve the agent's exposure-adjusting capabi
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#23454;&#29616;&#26089;&#26399;&#24322;&#24120;&#26816;&#27979;&#65292;&#20197;&#22686;&#24378;&#27773;&#36710;AMS&#30005;&#36335;&#30340;&#21151;&#33021;&#23433;&#20840;&#24615;</title><link>https://arxiv.org/abs/2404.01632</link><description>&lt;p&gt;
&#36890;&#36807;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#22686;&#24378;&#27773;&#36710;AMS&#30005;&#36335;&#30340;&#21151;&#33021;&#23433;&#20840;&#24615;
&lt;/p&gt;
&lt;p&gt;
Enhancing Functional Safety in Automotive AMS Circuits through Unsupervised Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01632
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#23454;&#29616;&#26089;&#26399;&#24322;&#24120;&#26816;&#27979;&#65292;&#20197;&#22686;&#24378;&#27773;&#36710;AMS&#30005;&#36335;&#30340;&#21151;&#33021;&#23433;&#20840;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#22312;&#27773;&#36710;&#39046;&#22495;&#24191;&#27867;&#20351;&#29992;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#31243;&#24207;&#65292;&#30830;&#20445;&#27773;&#36710;&#31995;&#32479;&#20869;&#30340;&#30005;&#36335;&#21644;&#32452;&#20214;&#30340;&#21151;&#33021;&#23433;&#20840;&#65288;FuSa&#65289;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#20123;&#31995;&#32479;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#27169;&#25311;&#21644;&#28151;&#21512;&#20449;&#21495;&#65288;AMS&#65289;&#30005;&#36335;&#27604;&#23427;&#20204;&#30340;&#25968;&#23383;&#23545;&#24212;&#29289;&#26356;&#23481;&#26131;&#21463;&#21040;&#30001;&#21442;&#25968;&#25200;&#21160;&#12289;&#22122;&#22768;&#12289;&#29615;&#22659;&#21387;&#21147;&#21644;&#20854;&#20182;&#22240;&#32032;&#24341;&#36215;&#30340;&#25925;&#38556;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#36830;&#32493;&#20449;&#21495;&#29305;&#24615;&#20026;&#26089;&#26399;&#24322;&#24120;&#26816;&#27979;&#25552;&#20379;&#20102;&#26426;&#20250;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23454;&#26045;&#23433;&#20840;&#26426;&#21046;&#20197;&#38450;&#27490;&#31995;&#32479;&#25925;&#38556;&#12290;&#20026;&#20102;&#28385;&#36275;&#36825;&#31181;&#38656;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#29992;&#20110;AMS&#30005;&#36335;&#30340;&#26089;&#26399;&#24322;&#24120;&#26816;&#27979;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#28041;&#21450;&#22312;&#21508;&#31181;&#30005;&#36335;&#20301;&#32622;&#21644;&#21333;&#29420;&#32452;&#20214;&#27880;&#20837;&#24322;&#24120;&#65292;&#20197;&#21019;&#24314;&#22810;&#26679;&#21270;&#21644;&#20840;&#38754;&#30340;&#24322;&#24120;&#25968;&#25454;&#38598;&#65292;&#28982;&#21518;&#20174;&#35266;&#23519;&#21040;&#30340;&#30005;&#36335;&#20013;&#25552;&#21462;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01632v1 Announce Type: new  Abstract: Given the widespread use of safety-critical applications in the automotive field, it is crucial to ensure the Functional Safety (FuSa) of circuits and components within automotive systems. The Analog and Mixed-Signal (AMS) circuits prevalent in these systems are more vulnerable to faults induced by parametric perturbations, noise, environmental stress, and other factors, in comparison to their digital counterparts. However, their continuous signal characteristics present an opportunity for early anomaly detection, enabling the implementation of safety mechanisms to prevent system failure. To address this need, we propose a novel framework based on unsupervised machine learning for early anomaly detection in AMS circuits. The proposed approach involves injecting anomalies at various circuit locations and individual components to create a diverse and comprehensive anomaly dataset, followed by the extraction of features from the observed ci
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#31070;&#32463;&#22349;&#32553;&#29616;&#35937;&#24341;&#20837;&#31070;&#32463;&#22349;&#32553;&#26469;&#24418;&#25104;&#34920;&#31034;&#31354;&#38388;&#20013;&#30340;&#31561;&#35282;&#32039;&#26694;&#26550;&#32467;&#26500;&#65292;&#36890;&#36807;&#25552;&#20986;&#39044;&#22791;&#25968;&#25454;&#35757;&#32451;&#21644;&#27531;&#24046;&#20462;&#27491;&#65292;&#20351;&#24471;&#21333;&#21608;&#26399;&#23398;&#20064;&#30340;&#36830;&#32493;&#23398;&#20064;&#27169;&#22411;&#33021;&#26356;&#22909;&#22320;&#36866;&#24212;&#27969;&#25968;&#25454;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#26126;&#26174;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2404.01628</link><description>&lt;p&gt;
&#23398;&#20064;&#31561;&#35282;&#34920;&#31034;&#36827;&#34892;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning Equi-angular Representations for Online Continual Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01628
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#22349;&#32553;&#29616;&#35937;&#24341;&#20837;&#31070;&#32463;&#22349;&#32553;&#26469;&#24418;&#25104;&#34920;&#31034;&#31354;&#38388;&#20013;&#30340;&#31561;&#35282;&#32039;&#26694;&#26550;&#32467;&#26500;&#65292;&#36890;&#36807;&#25552;&#20986;&#39044;&#22791;&#25968;&#25454;&#35757;&#32451;&#21644;&#27531;&#24046;&#20462;&#27491;&#65292;&#20351;&#24471;&#21333;&#21608;&#26399;&#23398;&#20064;&#30340;&#36830;&#32493;&#23398;&#20064;&#27169;&#22411;&#33021;&#26356;&#22909;&#22320;&#36866;&#24212;&#27969;&#25968;&#25454;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#26126;&#26174;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#23384;&#22312;&#27424;&#25311;&#21512;&#35299;&#20915;&#26041;&#26696;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#30001;&#20110;&#19981;&#21450;&#26102;&#30340;&#27169;&#22411;&#26356;&#26032;&#22521;&#35757;&#19981;&#36275;&#65288;&#20363;&#22914;&#65292;&#21333;&#21608;&#26399;&#35757;&#32451;&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#31070;&#32463;&#22349;&#32553;&#29616;&#35937;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#35825;&#23548;&#31070;&#32463;&#22349;&#32553;&#24418;&#25104;&#34920;&#31034;&#31354;&#38388;&#20013;&#30340;&#21333;&#32431;&#31561;&#35282;&#32039;&#26694;&#26550;&#65288;ETF&#65289;&#32467;&#26500;&#65292;&#20197;&#20415;&#36890;&#36807;&#22312;&#34920;&#31034;&#31354;&#38388;&#20013;&#25552;&#20986;&#39044;&#22791;&#25968;&#25454;&#35757;&#32451;&#21644;&#27531;&#24046;&#20462;&#27491;&#26469;&#26356;&#22909;&#22320;&#20351;&#32463;&#36807;&#21333;&#21608;&#26399;&#23398;&#20064;&#30340;&#25345;&#32493;&#23398;&#20064;&#27169;&#22411;&#36866;&#24212;&#27969;&#23186;&#20307;&#25968;&#25454;&#12290;&#36890;&#36807;&#20351;&#29992;CIFAR-10/100&#12289;TinyImageNet&#12289;ImageNet-200&#21644;ImageNet-1K&#36827;&#34892;&#22823;&#37327;&#23454;&#35777;&#39564;&#35777;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#22330;&#26223;&#65288;&#22914;&#19981;&#30456;&#20132;&#21644;&#39640;&#26031;&#35843;&#24230;&#36830;&#32493;&#65288;&#21363;&#26080;&#36793;&#30028;&#65289;&#25968;&#25454;&#35774;&#32622;&#65289;&#20013;&#22343;&#36739;&#39046;&#20808;&#26041;&#27861;&#34920;&#29616;&#20986;&#26174;&#33879;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01628v1 Announce Type: cross  Abstract: Online continual learning suffers from an underfitted solution due to insufficient training for prompt model update (e.g., single-epoch training). To address the challenge, we propose an efficient online continual learning method using the neural collapse phenomenon. In particular, we induce neural collapse to form a simplex equiangular tight frame (ETF) structure in the representation space so that the continuously learned model with a single epoch can better fit to the streamed data by proposing preparatory data training and residual correction in the representation space. With an extensive set of empirical validations using CIFAR-10/100, TinyImageNet, ImageNet-200, and ImageNet-1K, we show that our proposed method outperforms state-of-the-art methods by a noticeable margin in various online continual learning scenarios such as disjoint and Gaussian scheduled continuous (i.e., boundary-free) data setups.
&lt;/p&gt;</description></item><item><title>LLM-ABR&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#33258;&#20027;&#35774;&#35745;&#36866;&#29992;&#20110;&#21508;&#31181;&#32593;&#32476;&#29305;&#24615;&#30340;&#33258;&#36866;&#24212;&#30721;&#29575;&#31639;&#27861;&#65292;&#24182;&#22312;&#19981;&#21516;&#32593;&#32476;&#29615;&#22659;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>https://arxiv.org/abs/2404.01617</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#33258;&#36866;&#24212;&#30721;&#29575;&#31639;&#27861;&#30340;LLM-ABR
&lt;/p&gt;
&lt;p&gt;
LLM-ABR: Designing Adaptive Bitrate Algorithms via Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01617
&lt;/p&gt;
&lt;p&gt;
LLM-ABR&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#33258;&#20027;&#35774;&#35745;&#36866;&#29992;&#20110;&#21508;&#31181;&#32593;&#32476;&#29305;&#24615;&#30340;&#33258;&#36866;&#24212;&#30721;&#29575;&#31639;&#27861;&#65292;&#24182;&#22312;&#19981;&#21516;&#32593;&#32476;&#29615;&#22659;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;LLM-ABR&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#29983;&#25104;&#33021;&#21147;&#26469;&#33258;&#21160;&#35774;&#35745;&#36866;&#29992;&#20110;&#19981;&#21516;&#32593;&#32476;&#29305;&#24615;&#30340;&#33258;&#36866;&#24212;&#30721;&#29575;&#65288;ABR&#65289;&#31639;&#27861;&#30340;&#31995;&#32479;&#12290;LLM-ABR&#22312;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#20869;&#36816;&#34892;&#65292;&#36171;&#20104;LLMs&#35774;&#35745;&#20851;&#38190;&#32452;&#20214;&#22914;&#29366;&#24577;&#21644;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#21253;&#25324;&#23485;&#24102;&#12289;&#21355;&#26143;&#12289;4G&#21644;5G&#22312;&#20869;&#30340;&#19981;&#21516;&#32593;&#32476;&#35774;&#32622;&#20013;&#35780;&#20272;&#20102;LLM-ABR&#12290;LLM-ABR&#22987;&#32456;&#20248;&#20110;&#40664;&#35748;ABR&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01617v1 Announce Type: cross  Abstract: We present LLM-ABR, the first system that utilizes the generative capabilities of large language models (LLMs) to autonomously design adaptive bitrate (ABR) algorithms tailored for diverse network characteristics. Operating within a reinforcement learning framework, LLM-ABR empowers LLMs to design key components such as states and neural network architectures. We evaluate LLM-ABR across diverse network settings, including broadband, satellite, 4G, and 5G. LLM-ABR consistently outperforms default ABR algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#38899;&#39057;&#27169;&#25311;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#34394;&#25311;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#22768;&#28304;&#23450;&#20301;&#21040;&#29305;&#23450;&#20301;&#32622;&#30340;&#20934;&#30830;&#24615;&#65292;&#26377;&#25928;&#20811;&#26381;&#20102;&#25968;&#25454;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2404.01611</link><description>&lt;p&gt;
&#34394;&#25311;&#29615;&#22659;&#20013;&#22768;&#28304;&#23450;&#20301;&#30340;&#38899;&#39057;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Audio Simulation for Sound Source Localization in Virtual Evironment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01611
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#38899;&#39057;&#27169;&#25311;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#34394;&#25311;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#22768;&#28304;&#23450;&#20301;&#21040;&#29305;&#23450;&#20301;&#32622;&#30340;&#20934;&#30830;&#24615;&#65292;&#26377;&#25928;&#20811;&#26381;&#20102;&#25968;&#25454;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#30452;&#23556;&#23450;&#20301;&#22312;&#20449;&#21495;&#21294;&#20047;&#30340;&#29615;&#22659;&#20013;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#21448;&#30456;&#20851;&#30340;&#38382;&#39064;&#12290;&#22312;&#20027;&#35201;&#20026;&#23460;&#20869;&#22330;&#26223;&#30340;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#22768;&#23398;&#26041;&#27861;&#36935;&#21040;&#22256;&#38590;&#65292;&#36825;&#26159;&#30001;&#20110;&#21453;&#23556;&#30340;&#29305;&#24615;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#29289;&#29702;&#22522;&#30784;&#30340;&#22768;&#20256;&#25773;&#27169;&#25311;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#34394;&#25311;&#29615;&#22659;&#20013;&#23558;&#22768;&#28304;&#23450;&#20301;&#21040;&#29305;&#23450;&#20301;&#32622;&#12290;&#35813;&#36807;&#31243;&#35797;&#22270;&#20811;&#26381;&#25968;&#25454;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#20107;&#20214;&#21457;&#29983;&#21518;&#30340;&#22768;&#28304;&#23450;&#20301;&#12290;&#25105;&#20204;&#20351;&#29992;&#38899;&#39057;&#21464;&#25442;&#22120;&#39057;&#35889;&#22270;&#26041;&#27861;&#23454;&#29616;&#20102;0.786+/-0,0136&#30340;F1&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01611v1 Announce Type: new  Abstract: Non-line-of-sight localization in signal-deprived environments is a challenging yet pertinent problem. Acoustic methods in such predominantly indoor scenarios encounter difficulty due to the reverberant nature. In this study, we aim to locate sound sources to specific locations within a virtual environment by leveraging physically grounded sound propagation simulations and machine learning methods. This process attempts to overcome the issue of data insufficiency to localize sound sources to their location of occurrence especially in post-event localization. We achieve 0.786+/- 0.0136 F1-score using an audio transformer spectrogram approach.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#19981;&#21464;&#24615;&#21407;&#21017;&#35299;&#20915;&#20844;&#24179;&#21644;&#27867;&#21270;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#22522;&#20110;&#35757;&#32451;&#29615;&#22659;&#30340;oracle FAIRM&#65292;&#20197;&#21450;&#22312;&#32447;&#24615;&#27169;&#22411;&#20013;&#23454;&#29616;FAIRM&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#26497;&#23567;&#26368;&#20248;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.01608</link><description>&lt;p&gt;
FAIRM: &#23398;&#20064;&#19981;&#21464;&#34920;&#31034;&#20197;&#23454;&#29616;&#31639;&#27861;&#20844;&#24179;&#24615;&#21644;&#22495;&#27867;&#21270;&#30340;&#26497;&#23567;&#26368;&#20248;&#24615;
&lt;/p&gt;
&lt;p&gt;
FAIRM: Learning invariant representations for algorithmic fairness and domain generalization with minimax optimality
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01608
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#19981;&#21464;&#24615;&#21407;&#21017;&#35299;&#20915;&#20844;&#24179;&#21644;&#27867;&#21270;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#22522;&#20110;&#35757;&#32451;&#29615;&#22659;&#30340;oracle FAIRM&#65292;&#20197;&#21450;&#22312;&#32447;&#24615;&#27169;&#22411;&#20013;&#23454;&#29616;FAIRM&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#26497;&#23567;&#26368;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#20551;&#35774;&#27979;&#35797;&#25968;&#25454;&#19982;&#35757;&#32451;&#25968;&#25454;&#20855;&#26377;&#30456;&#21516;&#30340;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24212;&#29992;&#20013;&#23384;&#22312;&#22810;&#20010;&#23618;&#27425;&#30340;&#24322;&#36136;&#24615;&#65292;&#36825;&#19968;&#20551;&#35774;&#21487;&#33021;&#19981;&#25104;&#31435;&#65292;&#20174;&#32780;&#24341;&#21457;&#31639;&#27861;&#20844;&#24179;&#24615;&#21644;&#22495;&#27867;&#21270;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#19981;&#21464;&#24615;&#21407;&#21017;&#35299;&#20915;&#20102;&#20844;&#24179;&#19988;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#30340;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#35757;&#32451;&#29615;&#22659;&#30340;oracle&#65292;FAIRM&#65292;&#23427;&#22312;&#22810;&#26679;&#24615;&#31867;&#22411;&#26465;&#20214;&#19979;&#20855;&#26377;&#29702;&#24819;&#30340;&#20844;&#24179;&#24615;&#21644;&#22495;&#27867;&#21270;&#29305;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#24369;&#20998;&#24067;&#20551;&#35774;&#19979;&#25552;&#20379;&#20102;&#19968;&#20010;&#20855;&#26377;&#26377;&#38480;&#26679;&#26412;&#29702;&#35770;&#20445;&#35777;&#30340;&#32463;&#39564;FAIRM&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#26377;&#25928;&#30340;&#31639;&#27861;&#26469;&#22312;&#32447;&#24615;&#27169;&#22411;&#20013;&#23454;&#29616;FAIRM&#65292;&#24182;&#23637;&#31034;&#20102;&#20855;&#26377;&#26497;&#23567;&#26368;&#20248;&#24615;&#30340;&#38750;&#28176;&#36817;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;MNIST&#25968;&#25454;&#30340;&#25968;&#20540;&#23454;&#39564;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#20248;&#20110;&#23545;&#24212;&#26041;&#27861;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01608v1 Announce Type: cross  Abstract: Machine learning methods often assume that the test data have the same distribution as the training data. However, this assumption may not hold due to multiple levels of heterogeneity in applications, raising issues in algorithmic fairness and domain generalization. In this work, we address the problem of fair and generalizable machine learning by invariant principles. We propose a training environment-based oracle, FAIRM, which has desirable fairness and domain generalization properties under a diversity-type condition. We then provide an empirical FAIRM with finite-sample theoretical guarantees under weak distributional assumptions. We then develop efficient algorithms to realize FAIRM in linear models and demonstrate the nonasymptotic performance with minimax optimality. We evaluate our method in numerical experiments with synthetic data and MNIST data and show that it outperforms its counterparts.
&lt;/p&gt;</description></item><item><title>Transformer&#30340;&#28145;&#24230;&#23545;&#20854;&#36827;&#34892;&#19981;&#21516;&#20219;&#21153;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#31995;&#32479;&#35780;&#20272;&#65292;&#30740;&#31350;&#21457;&#29616;&#25512;&#29702;&#21644;&#27867;&#21270;&#38656;&#35201;&#33267;&#23569;&#20004;&#20010;&#27880;&#24847;&#21147;&#23618;&#65292;&#32780;&#19978;&#19979;&#25991;&#27867;&#21270;&#21487;&#33021;&#38656;&#35201;&#19977;&#20010;&#27880;&#24847;&#21147;&#23618;&#12290;&#36890;&#36807;&#32452;&#21512;&#31616;&#21333;&#25805;&#20316;&#21644;&#22534;&#21472;&#22810;&#20010;&#27880;&#24847;&#21147;&#23618;&#65292;&#22797;&#26434;&#20219;&#21153;&#21487;&#20197;&#24471;&#21040;&#35299;&#20915;&#12290;</title><link>https://arxiv.org/abs/2404.01601</link><description>&lt;p&gt;
&#21464;&#28145;&#24230;Transformer&#33021;&#23398;&#21040;&#20160;&#20040;&#65311;&#24207;&#21015;&#23398;&#20064;&#20219;&#21153;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
What Can Transformer Learn with Varying Depth? Case Studies on Sequence Learning Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01601
&lt;/p&gt;
&lt;p&gt;
Transformer&#30340;&#28145;&#24230;&#23545;&#20854;&#36827;&#34892;&#19981;&#21516;&#20219;&#21153;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#31995;&#32479;&#35780;&#20272;&#65292;&#30740;&#31350;&#21457;&#29616;&#25512;&#29702;&#21644;&#27867;&#21270;&#38656;&#35201;&#33267;&#23569;&#20004;&#20010;&#27880;&#24847;&#21147;&#23618;&#65292;&#32780;&#19978;&#19979;&#25991;&#27867;&#21270;&#21487;&#33021;&#38656;&#35201;&#19977;&#20010;&#27880;&#24847;&#21147;&#23618;&#12290;&#36890;&#36807;&#32452;&#21512;&#31616;&#21333;&#25805;&#20316;&#21644;&#22534;&#21472;&#22810;&#20010;&#27880;&#24847;&#21147;&#23618;&#65292;&#22797;&#26434;&#20219;&#21153;&#21487;&#20197;&#24471;&#21040;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#19981;&#21516;&#28145;&#24230;&#30340;Transformer&#26550;&#26500;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#32452;&#26032;&#39062;&#30340;&#24207;&#21015;&#23398;&#20064;&#20219;&#21153;&#65292;&#31995;&#32479;&#35780;&#20272;&#21644;&#29702;&#35299;&#20102;Transformer&#30340;&#28145;&#24230;&#22914;&#20309;&#24433;&#21709;&#20854;&#36827;&#34892;&#35760;&#24518;&#12289;&#25512;&#29702;&#12289;&#27867;&#21270;&#21644;&#19978;&#19979;&#25991;&#27867;&#21270;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21482;&#26377;&#19968;&#20010;&#27880;&#24847;&#21147;&#23618;&#30340;Transformer&#21487;&#20197;&#22312;&#35760;&#24518;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#20854;&#20182;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23637;&#31034;&#25512;&#29702;&#21644;&#27867;&#21270;&#33021;&#21147;&#38656;&#35201;Transformer&#33267;&#23569;&#20855;&#26377;&#20004;&#20010;&#27880;&#24847;&#21147;&#23618;&#65292;&#32780;&#19978;&#19979;&#25991;&#27867;&#21270;&#33021;&#21147;&#21487;&#33021;&#38656;&#35201;&#19977;&#20010;&#27880;&#24847;&#21147;&#23618;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#21333;&#20010;&#27880;&#24847;&#21147;&#23618;&#21487;&#20197;&#25191;&#34892;&#30340;&#19968;&#31867;&#31616;&#21333;&#25805;&#20316;&#65292;&#24182;&#23637;&#31034;&#20102;&#22797;&#26434;&#20219;&#21153;&#21487;&#20197;&#20316;&#20026;&#36825;&#20123;&#31616;&#21333;&#25805;&#20316;&#30340;&#32452;&#21512;&#26469;&#22788;&#29702;&#65292;&#22240;&#27492;&#21487;&#20197;&#36890;&#36807;&#22534;&#21472;&#22810;&#20010;&#27880;&#24847;&#21147;&#23618;&#26469;&#35299;&#20915;&#12290;&#36825;&#20026;&#30740;&#31350;&#26356;&#23454;&#38469;&#21644;&#22797;&#26434;&#30340;&#38382;&#39064;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01601v1 Announce Type: new  Abstract: We study the capabilities of the transformer architecture with varying depth. Specifically, we designed a novel set of sequence learning tasks to systematically evaluate and comprehend how the depth of transformer affects its ability to perform memorization, reasoning, generalization, and contextual generalization. We show a transformer with only one attention layer can excel in memorization but falls short in other tasks. Then, we show that exhibiting reasoning and generalization ability requires the transformer to have at least two attention layers, while context generalization ability may necessitate three attention layers. Additionally, we identify a class of simple operations that a single attention layer can execute, and show that the complex tasks can be approached as the combinations of these simple operations and thus can be resolved by stacking multiple attention layers. This sheds light on studying more practical and complex t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22312;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#20013;&#25913;&#36827;&#21160;&#20316;&#36873;&#25321;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26497;&#20540;&#23547;&#25214;&#25511;&#21046;&#65288;ESC&#65289;&#36827;&#34892;&#33258;&#36866;&#24212;&#25511;&#21046;&#27493;&#39588;&#65292;&#20197;&#21152;&#36895;&#31574;&#30053;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2404.01598</link><description>&lt;p&gt;
&#29992;&#20110;&#21152;&#36895;&#31574;&#30053;&#20248;&#21270;&#30340;&#26497;&#20540;&#23547;&#25214;&#21160;&#20316;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Extremum-Seeking Action Selection for Accelerating Policy Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01598
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22312;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#20013;&#25913;&#36827;&#21160;&#20316;&#36873;&#25321;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26497;&#20540;&#23547;&#25214;&#25511;&#21046;&#65288;ESC&#65289;&#36827;&#34892;&#33258;&#36866;&#24212;&#25511;&#21046;&#27493;&#39588;&#65292;&#20197;&#21152;&#36895;&#31574;&#30053;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36830;&#32493;&#31354;&#38388;&#19978;&#36827;&#34892;&#25511;&#21046;&#30340;&#24378;&#21270;&#23398;&#20064;&#36890;&#24120;&#20351;&#29992;&#39640;&#29109;&#30340;&#38543;&#26426;&#31574;&#30053;&#65292;&#22914;&#39640;&#26031;&#20998;&#24067;&#65292;&#29992;&#20110;&#23616;&#37096;&#25506;&#32034;&#21644;&#20272;&#35745;&#31574;&#30053;&#26799;&#24230;&#20197;&#20248;&#21270;&#24615;&#33021;&#12290;&#35768;&#22810;&#26426;&#22120;&#20154;&#25511;&#21046;&#38382;&#39064;&#28041;&#21450;&#22797;&#26434;&#19981;&#31283;&#23450;&#30340;&#21160;&#21147;&#23398;&#65292;&#20854;&#20013;&#26045;&#21152;&#22312;&#21487;&#34892;&#25511;&#21046;&#27969;&#24418;&#20043;&#22806;&#30340;&#21160;&#20316;&#24456;&#24555;&#20250;&#23548;&#33268;&#19981;&#33391;&#21457;&#25955;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#22823;&#22810;&#25968;&#20174;&#29615;&#22659;&#21160;&#20316;&#31354;&#38388;&#20013;&#37319;&#26679;&#30340;&#26679;&#26412;&#29983;&#25104;&#30340;&#36712;&#36857;&#20215;&#20540;&#36739;&#20302;&#65292;&#20960;&#20046;&#27809;&#26377;&#36129;&#29486;&#20110;&#31574;&#30053;&#25913;&#36827;&#65292;&#23548;&#33268;&#23398;&#20064;&#32531;&#24930;&#25110;&#22833;&#36133;&#12290;&#25105;&#20204;&#25552;&#20986;&#22312;&#36825;&#31181;&#26080;&#27169;&#22411;RL&#35774;&#32622;&#20013;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#26497;&#20540;&#23547;&#25214;&#25511;&#21046;&#65288;ESC&#65289;&#30340;&#38468;&#21152;&#33258;&#36866;&#24212;&#25511;&#21046;&#27493;&#39588;&#26469;&#25913;&#21892;&#21160;&#20316;&#36873;&#25321;&#12290;&#23545;&#20110;&#20174;&#38543;&#26426;&#31574;&#30053;&#37319;&#26679;&#30340;&#27599;&#20010;&#21160;&#20316;&#65292;&#25105;&#20204;&#24212;&#29992;&#27491;&#24358;&#25200;&#21160;&#24182;&#26597;&#35810;&#20272;&#35745;&#30340;Q&#20540;&#20316;&#20026;&#21709;&#24212;&#20449;&#21495;&#12290;&#26681;&#25454;ESC&#65292;&#25105;&#20204;&#21160;&#24577;&#25913;&#36827;&#37319;&#26679;&#30340;&#21160;&#20316;&#20197;&#20351;&#20043;&#26356;&#25509;&#36817;&#29702;&#24819;&#21160;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01598v1 Announce Type: cross  Abstract: Reinforcement learning for control over continuous spaces typically uses high-entropy stochastic policies, such as Gaussian distributions, for local exploration and estimating policy gradient to optimize performance. Many robotic control problems deal with complex unstable dynamics, where applying actions that are off the feasible control manifolds can quickly lead to undesirable divergence. In such cases, most samples taken from the ambient action space generate low-value trajectories that hardly contribute to policy improvement, resulting in slow or failed learning. We propose to improve action selection in this model-free RL setting by introducing additional adaptive control steps based on Extremum-Seeking Control (ESC). On each action sampled from stochastic policies, we apply sinusoidal perturbations and query for estimated Q-values as the response signal. Based on ESC, we then dynamically improve the sampled actions to be closer 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#20013;&#23545;&#40784;&#19981;&#37197;&#23545;&#26679;&#26412;&#25361;&#25112;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20272;&#35745;&#20542;&#21521;&#24471;&#20998;&#26469;&#23450;&#20041;&#26679;&#26412;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;</title><link>https://arxiv.org/abs/2404.01595</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#25968;&#25454;&#26080;&#37197;&#23545;&#20542;&#21521;&#24471;&#20998;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Propensity Score Alignment of Unpaired Multimodal Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01595
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#20013;&#23545;&#40784;&#19981;&#37197;&#23545;&#26679;&#26412;&#25361;&#25112;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20272;&#35745;&#20542;&#21521;&#24471;&#20998;&#26469;&#23450;&#20041;&#26679;&#26412;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#36890;&#24120;&#20381;&#36182;&#20110;&#37197;&#23545;&#26679;&#26412;&#26469;&#23398;&#20064;&#20849;&#21516;&#30340;&#34920;&#31034;&#65292;&#20294;&#22312;&#29983;&#29289;&#23398;&#31561;&#39046;&#22495;&#65292;&#24448;&#24448;&#38590;&#20197;&#25910;&#38598;&#37197;&#23545;&#26679;&#26412;&#65292;&#22240;&#20026;&#27979;&#37327;&#35774;&#22791;&#36890;&#24120;&#20250;&#30772;&#22351;&#26679;&#26412;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35299;&#20915;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#20013;&#23545;&#40784;&#19981;&#37197;&#23545;&#26679;&#26412;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#22240;&#26524;&#25512;&#26029;&#20013;&#30340;&#28508;&#22312;&#32467;&#26524;&#19982;&#22810;&#27169;&#24577;&#35266;&#23519;&#20013;&#30340;&#28508;&#22312;&#35270;&#22270;&#36827;&#34892;&#31867;&#27604;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#20351;&#29992;Rubin&#30340;&#26694;&#26550;&#26469;&#20272;&#35745;&#19968;&#20010;&#20849;&#21516;&#30340;&#31354;&#38388;&#65292;&#20197;&#21305;&#37197;&#26679;&#26412;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20551;&#35774;&#25105;&#20204;&#25910;&#38598;&#20102;&#32463;&#36807;&#22788;&#29702;&#23454;&#39564;&#24178;&#25200;&#30340;&#26679;&#26412;&#65292;&#24182;&#21033;&#29992;&#27492;&#26469;&#20174;&#27599;&#31181;&#27169;&#24577;&#20013;&#20272;&#35745;&#20542;&#21521;&#24471;&#20998;&#65292;&#20854;&#20013;&#21253;&#25324;&#28508;&#22312;&#29366;&#24577;&#21644;&#22788;&#29702;&#20043;&#38388;&#30340;&#25152;&#26377;&#20849;&#20139;&#20449;&#24687;&#65292;&#24182;&#21487;&#29992;&#20110;&#23450;&#20041;&#26679;&#26412;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#25105;&#20204;&#23581;&#35797;&#20102;&#20004;&#31181;&#21033;&#29992;&#36825;&#19968;&#26041;&#27861;&#30340;&#23545;&#40784;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01595v1 Announce Type: new  Abstract: Multimodal representation learning techniques typically rely on paired samples to learn common representations, but paired samples are challenging to collect in fields such as biology where measurement devices often destroy the samples. This paper presents an approach to address the challenge of aligning unpaired samples across disparate modalities in multimodal representation learning. We draw an analogy between potential outcomes in causal inference and potential views in multimodal observations, which allows us to use Rubin's framework to estimate a common space in which to match samples. Our approach assumes we collect samples that are experimentally perturbed by treatments, and uses this to estimate a propensity score from each modality, which encapsulates all shared information between a latent state and treatment and can be used to define a distance between samples. We experiment with two alignment techniques that leverage this di
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24187;&#35273;&#22810;&#26679;&#24615;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20943;&#36731;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25991;&#26412;&#25688;&#35201;&#20013;&#20135;&#29983;&#30340;&#24187;&#35273;&#65292;&#20943;&#23569;&#20102;&#26114;&#36149;&#30340;&#20154;&#31867;&#27880;&#37322;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2404.01588</link><description>&lt;p&gt;
&#22522;&#20110;&#24187;&#35273;&#22810;&#26679;&#24615;&#30340;&#25991;&#26412;&#25688;&#35201;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Hallucination Diversity-Aware Active Learning for Text Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01588
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24187;&#35273;&#22810;&#26679;&#24615;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20943;&#36731;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25991;&#26412;&#25688;&#35201;&#20013;&#20135;&#29983;&#30340;&#24187;&#35273;&#65292;&#20943;&#23569;&#20102;&#26114;&#36149;&#30340;&#20154;&#31867;&#27880;&#37322;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#34920;&#29616;&#20986;&#29983;&#25104;&#24187;&#35273;&#36755;&#20986;&#30340;&#20542;&#21521;&#65292;&#21363;&#22312;&#20107;&#23454;&#19978;&#19981;&#27491;&#30830;&#25110;&#19981;&#25903;&#25345;&#30340;&#25991;&#26412;&#12290;&#29616;&#26377;&#30340;&#20943;&#36731;&#24187;&#35273;&#30340;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#26114;&#36149;&#30340;&#20154;&#31867;&#27880;&#37322;&#26469;&#35782;&#21035;&#21644;&#32416;&#27491;LLMs&#36755;&#20986;&#20013;&#30340;&#24187;&#35273;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;&#26041;&#27861;&#19987;&#27880;&#20110;&#29305;&#23450;&#31867;&#22411;&#30340;&#24187;&#35273;&#65292;&#20363;&#22914;&#23454;&#20307;&#25110;&#26631;&#35760;&#38169;&#35823;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#35299;&#20915;LLMs&#36755;&#20986;&#20013;&#23637;&#31034;&#30340;&#21508;&#31181;&#31867;&#22411;&#30340;&#24187;&#35273;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#26088;&#22312;&#20943;&#36731;LLMs&#24187;&#35273;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#65292;&#38477;&#20302;&#20102;&#23545;&#24187;&#35273;&#25152;&#38656;&#30340;&#26114;&#36149;&#20154;&#31867;&#27880;&#37322;&#12290;&#36890;&#36807;&#22312;&#25991;&#26412;&#25688;&#35201;&#20013;&#34913;&#37327;&#35821;&#20041;&#26694;&#26550;&#12289;&#35758;&#35770;&#21644;&#20869;&#23481;&#21487;&#39564;&#35777;&#24615;&#38169;&#35823;&#20013;&#30340;&#32454;&#31890;&#24230;&#24187;&#35273;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; HAllucination Diversity-Aware Sampling&#65288;HADAS&#65289;&#26469;&#36873;&#25321;&#22810;&#26679;&#21270;&#30340;&#24187;&#35273;&#65292;&#20197;&#20379;LLM&#24494;&#35843;&#30340;&#20027;&#21160;&#23398;&#20064;&#27880;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01588v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have shown propensity to generate hallucinated outputs, i.e., texts that are factually incorrect or unsupported. Existing methods for alleviating hallucinations typically require costly human annotations to identify and correct hallucinations in LLM outputs. Moreover, most of these methods focus on a specific type of hallucination, e.g., entity or token errors, which limits their effectiveness in addressing various types of hallucinations exhibited in LLM outputs. To our best knowledge, in this paper we propose the first active learning framework to alleviate LLM hallucinations, reducing costly human annotations of hallucination needed. By measuring fine-grained hallucinations from errors in semantic frame, discourse and content verifiability in text summarization, we propose HAllucination Diversity-Aware Sampling (HADAS) to select diverse hallucinations for annotations in active learning for LLM finetuning
&lt;/p&gt;</description></item><item><title>GLEMOS&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#22522;&#20934;&#35780;&#20272;&#65292;&#29992;&#20110;&#30636;&#26102;&#22320;&#36873;&#25321;&#26377;&#25928;&#30340;&#22270;&#23398;&#20064;&#27169;&#22411;&#65292;&#22635;&#34917;&#20102;&#20043;&#21069;&#32570;&#20047;&#30340;&#35780;&#20272;GL&#27169;&#22411;&#36873;&#25321;&#26041;&#27861;&#24615;&#33021;&#30340;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2404.01578</link><description>&lt;p&gt;
GLEMOS&#65306;&#30636;&#26102;&#22270;&#23398;&#20064;&#27169;&#22411;&#36873;&#25321;&#30340;&#22522;&#20934;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
GLEMOS: Benchmark for Instantaneous Graph Learning Model Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01578
&lt;/p&gt;
&lt;p&gt;
GLEMOS&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#22522;&#20934;&#35780;&#20272;&#65292;&#29992;&#20110;&#30636;&#26102;&#22320;&#36873;&#25321;&#26377;&#25928;&#30340;&#22270;&#23398;&#20064;&#27169;&#22411;&#65292;&#22635;&#34917;&#20102;&#20043;&#21069;&#32570;&#20047;&#30340;&#35780;&#20272;GL&#27169;&#22411;&#36873;&#25321;&#26041;&#27861;&#24615;&#33021;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#23398;&#20064;&#65288;GL&#65289;&#27169;&#22411;&#65288;&#21363;GL&#31639;&#27861;&#21450;&#20854;&#36229;&#21442;&#25968;&#35774;&#32622;&#65289;&#30340;&#36873;&#25321;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#36234;&#26469;&#36234;&#22810;&#30340;GL&#27169;&#22411;&#34987;&#24320;&#21457;&#20986;&#26469;&#65292;&#36873;&#25321;&#27491;&#30830;&#30340;GL&#27169;&#22411;&#21464;&#24471;&#36234;&#26469;&#36234;&#22256;&#38590;&#21644;&#32791;&#26102;&#12290;&#22240;&#27492;&#65292;&#20026;GL&#30340;&#29992;&#25143;&#25552;&#20379;&#33021;&#22815;&#22312;&#27809;&#26377;&#25163;&#21160;&#24178;&#39044;&#30340;&#24773;&#20917;&#19979;&#25191;&#34892;&#26377;&#25928;GL&#27169;&#22411;&#30340;&#36817;&#20046;&#30636;&#26102;&#36873;&#25321;&#30340;&#33021;&#21147;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#21644;&#23454;&#38469;&#20215;&#20540;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;&#24050;&#32463;&#26377;&#23581;&#35797;&#35299;&#20915;&#36825;&#19968;&#37325;&#35201;&#38382;&#39064;&#65292;&#20294;&#23578;&#26410;&#26377;&#19968;&#20010;&#20840;&#38754;&#30340;&#22522;&#20934;&#29615;&#22659;&#26469;&#35780;&#20272;GL&#27169;&#22411;&#36873;&#25321;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#20026;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;GLEMOS&#65292;&#19968;&#20010;&#29992;&#20110;&#30636;&#26102;GL&#27169;&#22411;&#36873;&#25321;&#30340;&#20840;&#38754;&#22522;&#20934;&#35780;&#20272;&#65292;&#20854;&#20570;&#20986;&#20197;&#19979;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01578v1 Announce Type: new  Abstract: The choice of a graph learning (GL) model (i.e., a GL algorithm and its hyperparameter settings) has a significant impact on the performance of downstream tasks. However, selecting the right GL model becomes increasingly difficult and time consuming as more and more GL models are developed. Accordingly, it is of great significance and practical value to equip users of GL with the ability to perform a near-instantaneous selection of an effective GL model without manual intervention. Despite the recent attempts to tackle this important problem, there has been no comprehensive benchmark environment to evaluate the performance of GL model selection methods. To bridge this gap, we present GLEMOS in this work, a comprehensive benchmark for instantaneous GL model selection that makes the following contributions. (i) GLEMOS provides extensive benchmark data for fundamental GL tasks, i.e., link prediction and node classification, including the pe
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#32858;&#28966;&#20110;&#21033;&#29992;&#22810;&#31890;&#24230;&#25200;&#21160;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#36890;&#36807;&#36716;&#21270;&#20026;&#39034;&#24207;&#20915;&#31574;&#36807;&#31243;&#26469;&#35299;&#20915;&#32452;&#21512;&#29190;&#28856;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2404.01574</link><description>&lt;p&gt;
&#30446;&#26631;&#40657;&#30418;&#31070;&#32463;&#25490;&#24207;&#27169;&#22411;&#30340;&#22810;&#31890;&#24230;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Multi-granular Adversarial Attacks against Black-box Neural Ranking Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01574
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#32858;&#28966;&#20110;&#21033;&#29992;&#22810;&#31890;&#24230;&#25200;&#21160;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#36890;&#36807;&#36716;&#21270;&#20026;&#39034;&#24207;&#20915;&#31574;&#36807;&#31243;&#26469;&#35299;&#20915;&#32452;&#21512;&#29190;&#28856;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#25490;&#24207;&#25915;&#20987;&#30001;&#20110;&#22312;&#21457;&#29616;&#31070;&#32463;&#25490;&#24207;&#27169;&#22411;&#30340;&#33030;&#24369;&#24615;&#24182;&#22686;&#24378;&#20854;&#40065;&#26834;&#24615;&#26041;&#38754;&#21462;&#24471;&#25104;&#21151;&#32780;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#20256;&#32479;&#30340;&#25915;&#20987;&#26041;&#27861;&#20165;&#22312;&#21333;&#19968;&#31890;&#24230;&#19978;&#36827;&#34892;&#25200;&#21160;&#65292;&#20363;&#22914;&#21333;&#35789;&#32423;&#25110;&#21477;&#23376;&#32423;&#65292;&#23545;&#30446;&#26631;&#25991;&#26723;&#36827;&#34892;&#25915;&#20987;&#12290;&#28982;&#32780;&#65292;&#23558;&#25200;&#21160;&#38480;&#21046;&#22312;&#21333;&#19968;&#31890;&#24230;&#19978;&#21487;&#33021;&#20250;&#20943;&#23569;&#21019;&#36896;&#23545;&#25239;&#24615;&#31034;&#20363;&#30340;&#28789;&#27963;&#24615;&#65292;&#20174;&#32780;&#38477;&#20302;&#25915;&#20987;&#30340;&#28508;&#22312;&#23041;&#32961;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#36890;&#36807;&#32467;&#21512;&#19981;&#21516;&#31890;&#24230;&#30340;&#25200;&#21160;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#12290;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#28041;&#21450;&#35299;&#20915;&#32452;&#21512;&#29190;&#28856;&#38382;&#39064;&#65292;&#38656;&#35201;&#35782;&#21035;&#20986;&#36328;&#25152;&#26377;&#21487;&#33021;&#30340;&#31890;&#24230;&#12289;&#20301;&#32622;&#21644;&#25991;&#26412;&#29255;&#27573;&#30340;&#26368;&#20339;&#32452;&#21512;&#25200;&#21160;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#23558;&#22810;&#31890;&#24230;&#23545;&#25239;&#25915;&#20987;&#36716;&#21270;&#20026;&#19968;&#20010;&#39034;&#24207;&#20915;&#31574;&#36807;&#31243;&#65292;&#20854;&#20013;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01574v1 Announce Type: cross  Abstract: Adversarial ranking attacks have gained increasing attention due to their success in probing vulnerabilities, and, hence, enhancing the robustness, of neural ranking models. Conventional attack methods employ perturbations at a single granularity, e.g., word-level or sentence-level, to a target document. However, limiting perturbations to a single level of granularity may reduce the flexibility of creating adversarial examples, thereby diminishing the potential threat of the attack. Therefore, we focus on generating high-quality adversarial examples by incorporating multi-granular perturbations. Achieving this objective involves tackling a combinatorial explosion problem, which requires identifying an optimal combination of perturbations across all possible levels of granularity, positions, and textual pieces. To address this challenge, we transform the multi-granular adversarial attack into a sequential decision-making process, where 
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#20026;&#26031;&#22374;&#31119;&#33258;&#28982;&#35821;&#35328;&#25512;&#26029;&#65288;SNLI&#65289;&#25968;&#25454;&#38598;&#29983;&#25104;&#23545;&#27604;&#38598;&#30340;&#21019;&#26032;&#25216;&#26415;&#65292;&#36890;&#36807;&#33258;&#21160;&#26367;&#25442;&#21160;&#35789;&#12289;&#21103;&#35789;&#21644;&#24418;&#23481;&#35789;&#20026;&#21516;&#20041;&#35789;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#26159;&#21542;&#22522;&#20110;&#30495;&#23454;&#30340;&#35821;&#35328;&#29702;&#35299;&#36824;&#26159;&#20165;&#20165;&#22522;&#20110;&#27169;&#24335;&#35782;&#21035;&#12290;</title><link>https://arxiv.org/abs/2404.01569</link><description>&lt;p&gt;
&#20351;&#29992;&#23545;&#27604;&#38598;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#19968;&#31181;&#23454;&#39564;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Evaluating Large Language Models Using Contrast Sets: An Experimental Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01569
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#20026;&#26031;&#22374;&#31119;&#33258;&#28982;&#35821;&#35328;&#25512;&#26029;&#65288;SNLI&#65289;&#25968;&#25454;&#38598;&#29983;&#25104;&#23545;&#27604;&#38598;&#30340;&#21019;&#26032;&#25216;&#26415;&#65292;&#36890;&#36807;&#33258;&#21160;&#26367;&#25442;&#21160;&#35789;&#12289;&#21103;&#35789;&#21644;&#24418;&#23481;&#35789;&#20026;&#21516;&#20041;&#35789;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#26159;&#21542;&#22522;&#20110;&#30495;&#23454;&#30340;&#35821;&#35328;&#29702;&#35299;&#36824;&#26159;&#20165;&#20165;&#22522;&#20110;&#27169;&#24335;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#25512;&#26029;&#65288;NLI&#65289;&#39046;&#22495;&#65292;&#23588;&#20854;&#26159;&#28041;&#21450;&#22810;&#20010;&#36755;&#20837;&#25991;&#26412;&#20998;&#31867;&#30340;&#20219;&#21153;&#20013;&#65292;&#20132;&#21449;&#29109;&#25439;&#22833;&#24230;&#37327;&#34987;&#24191;&#27867;&#24212;&#29992;&#20316;&#20026;&#38169;&#35823;&#24230;&#37327;&#30340;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#35813;&#24230;&#37327;&#22312;&#26377;&#25928;&#35780;&#20272;&#27169;&#22411;&#29702;&#35299;&#35821;&#21477;&#34164;&#28085;&#33021;&#21147;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#20026;&#26031;&#22374;&#31119;&#33258;&#28982;&#35821;&#35328;&#25512;&#26029;&#65288;SNLI&#65289;&#25968;&#25454;&#38598;&#29983;&#25104;&#23545;&#27604;&#38598;&#12290;&#25105;&#20204;&#30340;&#31574;&#30053;&#28041;&#21450;&#33258;&#21160;&#23558;&#21160;&#35789;&#12289;&#21103;&#35789;&#21644;&#24418;&#23481;&#35789;&#26367;&#25442;&#20026;&#23427;&#20204;&#30340;&#21516;&#20041;&#35789;&#65292;&#20197;&#20445;&#30041;&#21477;&#23376;&#30340;&#21407;&#22987;&#21547;&#20041;&#12290;&#35813;&#26041;&#27861;&#26088;&#22312;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#26159;&#21542;&#22522;&#20110;&#30495;&#23454;&#30340;&#35821;&#35328;&#29702;&#35299;&#36824;&#26159;&#20165;&#20165;&#22522;&#20110;&#27169;&#24335;&#35782;&#21035;&#12290;&#25105;&#20204;&#20351;&#29992;ELECTRA-small&#27169;&#22411;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#35813;&#27169;&#22411;&#22312;&#20256;&#32479;&#30340;SNLI&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;89.9%&#30340;&#20934;&#30830;&#24230;&#65292;&#20294;&#22312;&#25105;&#20204;&#30340;&#23545;&#27604;&#38598;&#19978;&#26174;&#31034;&#20986;&#20102;72.5%&#30340;&#20934;&#30830;&#24230;&#65292;&#34920;&#26126;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01569v1 Announce Type: cross  Abstract: In the domain of Natural Language Inference (NLI), especially in tasks involving the classification of multiple input texts, the Cross-Entropy Loss metric is widely employed as a standard for error measurement. However, this metric falls short in effectively evaluating a model's capacity to understand language entailments. In this study, we introduce an innovative technique for generating a contrast set for the Stanford Natural Language Inference (SNLI) dataset. Our strategy involves the automated substitution of verbs, adverbs, and adjectives with their synonyms to preserve the original meaning of sentences. This method aims to assess whether a model's performance is based on genuine language comprehension or simply on pattern recognition. We conducted our analysis using the ELECTRA-small model. The model achieved an accuracy of 89.9% on the conventional SNLI dataset but showed a reduced accuracy of 72.5% on our contrast set, indicati
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21160;&#24577;&#32593;&#32476;&#26725;&#25509;&#30340;&#26032;&#39062;&#20998;&#24067;&#24335;&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;Dec-POMDP&#65289;&#26041;&#27861;&#65292;&#20197;&#21450;&#22522;&#20110;&#22270;&#21367;&#31215;&#24378;&#21270;&#23398;&#20064;&#65288;DGN&#65289;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2404.01557</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#33258;&#20027;&#32676;&#20307;&#24418;&#25104;&#21160;&#24577;&#32593;&#32476;&#26725;&#25509;
&lt;/p&gt;
&lt;p&gt;
Distributed Autonomous Swarm Formation for Dynamic Network Bridging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01557
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21160;&#24577;&#32593;&#32476;&#26725;&#25509;&#30340;&#26032;&#39062;&#20998;&#24067;&#24335;&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;Dec-POMDP&#65289;&#26041;&#27861;&#65292;&#20197;&#21450;&#22522;&#20110;&#22270;&#21367;&#31215;&#24378;&#21270;&#23398;&#20064;&#65288;DGN&#65289;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30340;&#26426;&#22120;&#20154;&#31995;&#32479;&#25805;&#20316;&#21644;&#26080;&#32541;&#21327;&#20316;&#26159;&#19979;&#19968;&#20195;&#25216;&#26415;&#21644;&#24212;&#29992;&#30340;&#22522;&#26412;&#32452;&#25104;&#37096;&#20998;&#12290;&#22312;&#35832;&#22914;&#28798;&#38590;&#21709;&#24212;&#20043;&#31867;&#30340;&#24773;&#26223;&#20013;&#65292;&#32676;&#20307;&#25805;&#20316;&#38656;&#35201;&#21327;&#35843;&#30340;&#34892;&#20026;&#21644;&#31227;&#21160;&#25511;&#21046;&#20197;&#20998;&#24067;&#24335;&#26041;&#24335;&#22788;&#29702;&#65292;&#20195;&#29702;&#20043;&#38388;&#30340;&#36890;&#20449;&#20197;&#21450;&#24213;&#23618;&#32593;&#32476;&#36136;&#37327;&#23545;&#20854;&#34892;&#21160;&#30340;&#36136;&#37327;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;&#26412;&#25991;&#38024;&#23545;&#21160;&#24577;&#32593;&#32476;&#26725;&#25509;&#38382;&#39064;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#24335;&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;Dec-POMDP&#65289;&#26041;&#27861;&#65292;&#20854;&#20013;&#19968;&#32676;&#20195;&#29702;&#21327;&#20316;&#24418;&#25104;&#20004;&#20010;&#36828;&#36317;&#31227;&#21160;&#30446;&#26631;&#20043;&#38388;&#30340;&#36830;&#25509;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#21367;&#31215;&#24378;&#21270;&#23398;&#20064;&#65288;DGN&#65289;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33258;&#28982;&#36866;&#29992;&#20110;&#20219;&#21153;&#30340;&#32593;&#32476;&#21270;&#12289;&#20998;&#24067;&#24335;&#24615;&#36136;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#19982;&#19968;&#20010;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#20013;&#24515;&#21270;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01557v1 Announce Type: cross  Abstract: Effective operation and seamless cooperation of robotic systems are a fundamental component of next-generation technologies and applications. In contexts such as disaster response, swarm operations require coordinated behavior and mobility control to be handled in a distributed manner, with the quality of the agents' actions heavily relying on the communication between them and the underlying network. In this paper, we formulate the problem of dynamic network bridging in a novel Decentralized Partially Observable Markov Decision Process (Dec-POMDP), where a swarm of agents cooperates to form a link between two distant moving targets. Furthermore, we propose a Multi-Agent Reinforcement Learning (MARL) approach for the problem based on Graph Convolutional Reinforcement Learning (DGN) which naturally applies to the networked, distributed nature of the task. The proposed method is evaluated in a simulated environment and compared to a cent
&lt;/p&gt;</description></item><item><title>&#23558;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#19982;&#25511;&#21046;&#29702;&#35770;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35774;&#23450;&#28857;&#26356;&#26032;&#31639;&#27861;&#65292;&#20197;&#30830;&#20445;&#23433;&#20840;&#26465;&#20214;&#24182;&#23454;&#29616;&#33391;&#22909;&#30340;&#20219;&#21153;&#30446;&#26631;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.01551</link><description>&lt;p&gt;
&#20855;&#26377;&#25511;&#21046;&#29702;&#35770;&#23433;&#20840;&#20445;&#35777;&#30340;&#21160;&#24577;&#32593;&#32476;&#26725;&#25509;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multi-Agent Reinforcement Learning with Control-Theoretic Safety Guarantees for Dynamic Network Bridging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01551
&lt;/p&gt;
&lt;p&gt;
&#23558;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#19982;&#25511;&#21046;&#29702;&#35770;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35774;&#23450;&#28857;&#26356;&#26032;&#31639;&#27861;&#65292;&#20197;&#30830;&#20445;&#23433;&#20840;&#26465;&#20214;&#24182;&#23454;&#29616;&#33391;&#22909;&#30340;&#20219;&#21153;&#30446;&#26631;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23433;&#20840;&#20851;&#38190;&#29615;&#22659;&#19979;&#35299;&#20915;&#22797;&#26434;&#30340;&#21512;&#20316;&#20219;&#21153;&#23545;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#23588;&#20854;&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#26465;&#20214;&#19979;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#65292;&#23558;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#19982;&#25511;&#21046;&#29702;&#35770;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#20197;&#30830;&#20445;&#23433;&#20840;&#21644;&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#21253;&#25324;&#19968;&#31181;&#26032;&#39062;&#30340;&#35774;&#23450;&#28857;&#26356;&#26032;&#31639;&#27861;&#65292;&#21160;&#24577;&#35843;&#25972;&#26234;&#33021;&#20307;&#20301;&#32622;&#65292;&#20197;&#20445;&#25345;&#23433;&#20840;&#26465;&#20214;&#32780;&#19981;&#24433;&#21709;&#20219;&#21153;&#30446;&#26631;&#12290;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#65292;&#25105;&#20204;&#35777;&#26126;&#30456;&#27604;&#20256;&#32479;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#65292;&#25105;&#20204;&#21462;&#24471;&#20102;&#26174;&#33879;&#20248;&#21183;&#65292;&#23454;&#29616;&#20102;&#19982;&#38646;&#23433;&#20840;&#36829;&#35268;&#30456;&#27604;&#21487;&#27604;&#30340;&#20219;&#21153;&#24615;&#33021;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#23433;&#20840;&#25511;&#21046;&#19982;&#23398;&#20064;&#26041;&#27861;&#30456;&#32467;&#21512;&#19981;&#20165;&#22686;&#24378;&#20102;&#23433;&#20840;&#21512;&#35268;&#24615;&#65292;&#36824;&#23454;&#29616;&#20102;&#33391;&#22909;&#30340;&#20219;&#21153;&#30446;&#26631;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01551v1 Announce Type: cross  Abstract: Addressing complex cooperative tasks in safety-critical environments poses significant challenges for Multi-Agent Systems, especially under conditions of partial observability. This work introduces a hybrid approach that integrates Multi-Agent Reinforcement Learning with control-theoretic methods to ensure safe and efficient distributed strategies. Our contributions include a novel setpoint update algorithm that dynamically adjusts agents' positions to preserve safety conditions without compromising the mission's objectives. Through experimental validation, we demonstrate significant advantages over conventional MARL strategies, achieving comparable task performance with zero safety violations. Our findings indicate that integrating safe control with learning approaches not only enhances safety compliance but also achieves good performance in mission objectives.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#23545;&#26469;&#33258;&#21333;&#20010;&#22522;&#30784;&#27169;&#22411;&#30340;&#22810;&#27425;&#36816;&#34892;&#36827;&#34892;&#36731;&#24494;&#24494;&#35843;&#65292;&#21487;&#20197;&#36890;&#36807;&#35757;&#32451;&#26399;&#38388;&#30340;&#38543;&#26426;&#24615;&#36873;&#25321;&#26469;&#26174;&#33879;&#24433;&#21709;&#26368;&#32456;&#38598;&#21512;&#20013;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.01542</link><description>&lt;p&gt;
&#36890;&#36807;&#32447;&#19978;&#30340;&#19968;&#33268;&#24615;&#39044;&#27979;&#22522;&#30784;&#27169;&#22411;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Predicting the Performance of Foundation Models via Agreement-on-the-Line
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01542
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#23545;&#26469;&#33258;&#21333;&#20010;&#22522;&#30784;&#27169;&#22411;&#30340;&#22810;&#27425;&#36816;&#34892;&#36827;&#34892;&#36731;&#24494;&#24494;&#35843;&#65292;&#21487;&#20197;&#36890;&#36807;&#35757;&#32451;&#26399;&#38388;&#30340;&#38543;&#26426;&#24615;&#36873;&#25321;&#26469;&#26174;&#33879;&#24433;&#21709;&#26368;&#32456;&#38598;&#21512;&#20013;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20272;&#35745;&#26631;&#31614;&#31232;&#32570;&#24773;&#20917;&#19979;&#30340;&#22806;&#37096;&#20998;&#24067;&#24615;&#33021;&#23545;&#20110;&#23433;&#20840;&#37096;&#32626;&#22522;&#30784;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#65292;&#20154;&#20204;&#21457;&#29616;&#31070;&#32463;&#32593;&#32476;&#38598;&#21512;&#35266;&#23519;&#21040;&#8220;&#32447;&#19978;&#19968;&#33268;&#24615;&#8221;&#29616;&#35937;&#65292;&#21487;&#20197;&#21033;&#29992;&#23427;&#21487;&#38752;&#22320;&#39044;&#27979;&#26080;&#26631;&#31614;&#30340;&#22806;&#37096;&#20998;&#24067;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#19982;&#22312;&#20998;&#24067;&#25968;&#25454;&#19978;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#22810;&#27425;&#36718;&#25968;&#30340;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#30456;&#27604;&#65292;&#22522;&#30784;&#27169;&#22411;&#32463;&#21382;&#20102;&#20174;&#39044;&#35757;&#32451;&#26435;&#37325;&#20013;&#36827;&#34892;&#26368;&#23567;&#24494;&#35843;&#65292;&#36825;&#21487;&#33021;&#20250;&#20943;&#23569;&#35266;&#23519;&#21040;&#32447;&#19978;&#19968;&#33268;&#24615;&#25152;&#38656;&#30340;&#38598;&#21512;&#22810;&#26679;&#24615;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20986;&#24403;&#36731;&#24494;&#24494;&#35843;&#25972;&#26469;&#33258;$\textit{&#21333;&#20010;}$&#22522;&#30784;&#27169;&#22411;&#30340;&#22810;&#27425;&#36816;&#34892;&#26102;&#65292;&#35757;&#32451;&#26399;&#38388;&#30340;&#38543;&#26426;&#24615;&#36873;&#25321;&#65288;&#32447;&#24615;&#22836;&#21021;&#22987;&#21270;&#12289;&#25968;&#25454;&#25490;&#24207;&#21644;&#25968;&#25454;&#23376;&#38598;&#65289;&#21487;&#33021;&#23548;&#33268;&#19981;&#21516;&#31243;&#24230;&#30340;&#32447;&#19978;&#19968;&#33268;&#24615;&#30340;&#26368;&#32456;&#38598;&#21512;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#21482;&#26377;&#38543;&#26426;&#22836;&#21021;&#22987;&#21270;&#23601;&#33021;&#26497;&#22823;&#31243;&#24230;&#22320;&#24433;&#21709;&#38598;&#21512;&#20013;&#30340;&#19968;&#33268;&#24615;&#12290;&#234;n&#21021;&#21270;&#23601;&#33021;&#20351;&#20135;&#29983;&#30340;&#38598;&#21512;&#20013;&#30340;&#19968;&#33268;&#24615;&#27700;&#24179;&#20135;&#29983;&#24040;&#22823;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01542v1 Announce Type: new  Abstract: Estimating the out-of-distribution performance in regimes where labels are scarce is critical to safely deploy foundation models. Recently, it was shown that ensembles of neural networks observe the phenomena ``agreement-on-the-line'', which can be leveraged to reliably predict OOD performance without labels. However, in contrast to classical neural networks that are trained on in-distribution data from scratch for numerous epochs, foundation models undergo minimal finetuning from heavily pretrained weights, which may reduce the ensemble diversity needed to observe agreement-on-the-line. In our work, we demonstrate that when lightly finetuning multiple runs from a $\textit{single}$ foundation model, the choice of randomness during training (linear head initialization, data ordering, and data subsetting) can lead to drastically different levels of agreement-on-the-line in the resulting ensemble. Surprisingly, only random head initializati
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#29983;&#25104;&#21463;&#25968;&#23383;&#20998;&#24067;&#35268;&#24459;&#25511;&#21046;&#30340;&#38170;&#28857;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22312;&#35821;&#20041;&#19978;&#24341;&#23548;&#25968;&#23383;&#30340;&#31574;&#30053;&#65292;&#22312;&#24191;&#27867;&#33539;&#22260;&#30340;&#25968;&#23383;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#25968;&#23398;&#22522;&#30784;&#34920;&#31034;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2404.01536</link><description>&lt;p&gt;
&#25918;&#32622;&#38170;&#28857;&#65306;&#22312;&#35821;&#35328;&#24314;&#27169;&#20013;&#32473;&#25968;&#23383;&#35821;&#20041;&#19978;&#30340;&#24341;&#23548;
&lt;/p&gt;
&lt;p&gt;
Laying Anchors: Semantically Priming Numerals in Language Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01536
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#29983;&#25104;&#21463;&#25968;&#23383;&#20998;&#24067;&#35268;&#24459;&#25511;&#21046;&#30340;&#38170;&#28857;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22312;&#35821;&#20041;&#19978;&#24341;&#23548;&#25968;&#23383;&#30340;&#31574;&#30053;&#65292;&#22312;&#24191;&#27867;&#33539;&#22260;&#30340;&#25968;&#23383;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#25968;&#23398;&#22522;&#30784;&#34920;&#31034;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#22823;&#37327;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31649;&#32447;&#20013;&#30340;&#20107;&#23454;&#26631;&#20934;&#65292;&#28982;&#32780;&#36825;&#20123;&#27169;&#22411;&#26410;&#33021;&#27491;&#30830;&#32534;&#30721;&#25968;&#23383;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#38656;&#35201;&#25968;&#23383;&#29702;&#35299;&#30340;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31574;&#30053;&#65292;&#36890;&#36807;&#22312;&#20219;&#20309;&#35821;&#26009;&#24211;&#20013;&#29983;&#25104;&#21463;&#25968;&#23383;&#20998;&#24067;&#35268;&#24459;&#25511;&#21046;&#30340;&#38170;&#28857;&#26469;&#22312;&#35821;&#20041;&#19978;&#24341;&#23548;&#25968;&#23383;&#65292;&#20174;&#32780;&#23454;&#29616;&#36825;&#20123;&#25968;&#23383;&#26631;&#35760;&#30340;&#25968;&#23398;&#22522;&#30784;&#34920;&#31034;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#19968;&#31995;&#21015;&#25968;&#20540;&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#25216;&#26415;&#30340;&#20248;&#36234;&#24615;&#65292;&#23545;&#39046;&#22495;&#20869;&#65288;&#24050;&#35265;&#65289;&#21644;&#39046;&#22495;&#22806;&#65288;&#26410;&#35265;&#65289;&#30340;&#25968;&#23383;&#37117;&#36866;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#23454;&#35777;&#35780;&#20272;&#25193;&#23637;&#21040;&#20174;1&#21040;10&#20159;&#30340;&#25968;&#23383;&#33539;&#22260;&#65292;&#27604;&#20197;&#24448;&#30456;&#21516;&#31867;&#22411;&#30740;&#31350;&#30340;&#33539;&#22260;&#24191;&#24471;&#22810;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#23398;&#24471;&#30340;&#23884;&#20837;&#21521;&#25968;&#23398;&#19978;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01536v1 Announce Type: cross  Abstract: Off-the-shelf pre-trained language models have become the de facto standard in NLP pipelines for a multitude of downstream tasks. However, the inability of these models to properly encode numerals limits their performance on tasks requiring numeric comprehension. We introduce strategies to semantically prime numerals in any corpus by generating anchors governed by the distribution of numerals in said corpus, thereby enabling mathematically grounded representations of these numeral tokens. We establish the superiority of our proposed techniques through evaluation on a range of numeracy tasks for both in-domain (seen) and out-domain (unseen) numerals. Further, we expand our empirical evaluations to numerals ranging from 1 to 10 billion, a significantly broader range compared to previous studies of the same nature, and we demonstrate significant improvements in the mathematical grounding of our learned embeddings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#29992;&#20110;&#20272;&#31639;5G&#21644;B5G&#32593;&#32476;&#20013;&#31471;&#21040;&#31471;&#65288;E2E&#65289;&#32593;&#32476;&#20999;&#29255;&#21534;&#21520;&#37327;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#65292;&#24182;&#23558;&#39044;&#27979;&#30340;&#21534;&#21520;&#37327;&#19982;&#24403;&#21069;&#32593;&#32476;&#29366;&#24577;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#25512;&#23548;&#20986;&#19968;&#20010;&#27493;                                                                                                                                                                                                  en_tdlr: Introducing a machine learning model for estimating throughput in 5G and B5G networks with network slices, and combining predicted throughput with current network state for performance optimization.</title><link>https://arxiv.org/abs/2404.01530</link><description>&lt;p&gt;
5G&#21644;B5G&#32593;&#32476;&#20013;&#30340;ML&#20851;&#38190;&#32489;&#25928;&#25351;&#26631;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
ML KPI Prediction in 5G and B5G Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01530
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#29992;&#20110;&#20272;&#31639;5G&#21644;B5G&#32593;&#32476;&#20013;&#31471;&#21040;&#31471;&#65288;E2E&#65289;&#32593;&#32476;&#20999;&#29255;&#21534;&#21520;&#37327;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#65292;&#24182;&#23558;&#39044;&#27979;&#30340;&#21534;&#21520;&#37327;&#19982;&#24403;&#21069;&#32593;&#32476;&#29366;&#24577;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#25512;&#23548;&#20986;&#19968;&#20010;&#27493;                                                                                                                                                                                                  en_tdlr: Introducing a machine learning model for estimating throughput in 5G and B5G networks with network slices, and combining predicted throughput with current network state for performance optimization.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#36816;&#33829;&#21830;&#22312;&#28385;&#36275;&#23458;&#25143;&#38656;&#27714;&#26102;&#38754;&#20020;&#26032;&#25361;&#25112;&#12290;&#36825;&#20123;&#25361;&#25112;&#28304;&#20110;&#26032;&#26381;&#21153;&#30340;&#20986;&#29616;&#65292;&#22914;&#39640;&#28165;&#35270;&#39057;&#27969;&#23186;&#20307;&#12289;&#29289;&#32852;&#32593;&#12289;&#33258;&#21160;&#39550;&#39542;&#31561;&#65292;&#20197;&#21450;&#32593;&#32476;&#27969;&#37327;&#30340;&#25351;&#25968;&#22686;&#38271;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;5G&#21644;B5G&#32593;&#32476;&#19968;&#30452;&#22312;&#21457;&#23637;&#65292;&#20197;&#36866;&#24212;&#21508;&#31181;&#24212;&#29992;&#21644;&#29992;&#20363;&#12290;&#27492;&#22806;&#65292;&#36825;&#31181;&#21457;&#23637;&#24102;&#26469;&#20102;&#26032;&#29305;&#24615;&#65292;&#22914;&#21033;&#29992;&#32593;&#32476;&#20999;&#29255;&#21019;&#24314;&#22810;&#20010;&#31471;&#21040;&#31471;&#38548;&#31163;&#30340;&#34394;&#25311;&#32593;&#32476;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#30830;&#20445;&#26381;&#21153;&#36136;&#37327;&#65292;&#36816;&#33829;&#21830;&#24517;&#39035;&#26681;&#25454;&#20851;&#38190;&#24615;&#33021;&#25351;&#26631;&#65288;KPI&#65289;&#21644;&#20999;&#29255;&#26381;&#21153;&#32423;&#21035;&#21327;&#35758;&#65288;SLA&#65289;&#26469;&#32500;&#25252;&#21644;&#20248;&#21270;&#20182;&#20204;&#30340;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01530v1 Announce Type: cross  Abstract: Network operators are facing new challenges when meeting the needs of their customers. The challenges arise due to the rise of new services, such as HD video streaming, IoT, autonomous driving, etc., and the exponential growth of network traffic. In this context, 5G and B5G networks have been evolving to accommodate a wide range of applications and use cases. Additionally, this evolution brings new features, like the ability to create multiple end-to-end isolated virtual networks using network slicing. Nevertheless, to ensure the quality of service, operators must maintain and optimize their networks in accordance with the key performance indicators (KPIs) and the slice service-level agreements (SLAs).   In this paper, we introduce a machine learning (ML) model used to estimate throughput in 5G and B5G networks with end-to-end (E2E) network slices. Then, we combine the predicted throughput with the current network state to derive an es
&lt;/p&gt;</description></item><item><title>Fair MP-Boost&#26159;&#19968;&#31181;&#26088;&#22312;&#24179;&#34913;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#30340;Boosting&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#23398;&#20064;&#29305;&#24449;&#21644;&#35266;&#27979;&#26469;&#36873;&#25321;&#23567;&#25209;&#37327;&#65292;&#20197;&#21516;&#26102;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.01521</link><description>&lt;p&gt;
&#20844;&#24179;&#30340;MP-BOOST: &#20844;&#24179;&#19988;&#21487;&#35299;&#37322;&#30340;&#23567;&#25209;&#37327; Boosting
&lt;/p&gt;
&lt;p&gt;
Fair MP-BOOST: Fair and Interpretable Minipatch Boosting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01521
&lt;/p&gt;
&lt;p&gt;
Fair MP-Boost&#26159;&#19968;&#31181;&#26088;&#22312;&#24179;&#34913;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#30340;Boosting&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#23398;&#20064;&#29305;&#24449;&#21644;&#35266;&#27979;&#26469;&#36873;&#25321;&#23567;&#25209;&#37327;&#65292;&#20197;&#21516;&#26102;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38598;&#25104;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;Boosting&#65292;&#22312;&#34920;&#26684;&#25968;&#25454;&#20013;&#24050;&#34987;&#35777;&#26126;&#26159;&#39640;&#25928;&#19988;&#24191;&#27867;&#24212;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#21033;&#29992;&#20256;&#32479;Boosting&#26041;&#27861;&#30340;&#31283;&#20581;&#39044;&#27979;&#33021;&#21147;&#65292;&#21516;&#26102;&#22686;&#24378;&#20844;&#24179;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;Fair MP-Boost&#65292;&#36825;&#26159;&#19968;&#31181;&#24179;&#34913;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#30340;&#38543;&#26426;Boosting&#26041;&#26696;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#26399;&#38388;&#33258;&#36866;&#24212;&#23398;&#20064;&#29305;&#24449;&#21644;&#35266;&#23519;&#26469;&#23454;&#29616;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;Fair MP-Boost&#20381;&#25454;&#33258;&#36866;&#24212;&#23398;&#20064;&#30340;&#29305;&#24449;&#21644;&#35266;&#23519;&#37319;&#26679;&#27010;&#29575;&#65292;&#39034;&#24207;&#25277;&#21462;&#23567;&#25209;&#37327;&#35266;&#23519;&#21644;&#29305;&#24449;&#65292;&#34987;&#31216;&#20026;minipatches (MP)&#12290;&#25105;&#20204;&#36890;&#36807;&#32467;&#21512;&#25439;&#22833;&#20989;&#25968;&#25110;&#29305;&#24449;&#37325;&#35201;&#24615;&#20998;&#25968;&#26469;&#35774;&#35745;&#36825;&#20123;&#27010;&#29575;&#65292;&#20197;&#21516;&#26102;&#35299;&#20915;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;Fair MP-Boost&#20248;&#20808;&#32771;&#34385;&#37325;&#35201;&#19988;&#20844;&#24179;&#30340;&#29305;&#24449;&#20197;&#21450;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#23454;&#20363;&#65292;&#20174;&#32780;&#36873;&#25321;&#26368;&#30456;&#20851;&#30340;&#23567;&#25209;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01521v1 Announce Type: cross  Abstract: Ensemble methods, particularly boosting, have established themselves as highly effective and widely embraced machine learning techniques for tabular data. In this paper, we aim to leverage the robust predictive power of traditional boosting methods while enhancing fairness and interpretability. To achieve this, we develop Fair MP-Boost, a stochastic boosting scheme that balances fairness and accuracy by adaptively learning features and observations during training. Specifically, Fair MP-Boost sequentially samples small subsets of observations and features, termed minipatches (MP), according to adaptively learned feature and observation sampling probabilities. We devise these probabilities by combining loss functions, or by combining feature importance scores to address accuracy and fairness simultaneously. Hence, Fair MP-Boost prioritizes important and fair features along with challenging instances, to select the most relevant minipatc
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35299;&#20915;&#26368;&#20248;&#20256;&#36755;&#38382;&#39064;&#30340;&#21160;&#20316;&#20998;&#21106;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;Gromov-Wasserstein&#38382;&#39064;&#20013;&#32534;&#30721;&#26102;&#38388;&#19968;&#33268;&#24615;&#20808;&#39564;&#26469;&#23454;&#29616;&#20174;&#35270;&#39057;&#24103;&#21644;&#21160;&#20316;&#31867;&#21035;&#20043;&#38388;&#30340;&#22122;&#22768;&#25104;&#26412;&#20013;&#35299;&#30721;&#26102;&#38388;&#19968;&#33268;&#30340;&#20998;&#21106;&#12290;</title><link>https://arxiv.org/abs/2404.01518</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#26080;&#30417;&#30563;&#21160;&#20316;&#20998;&#21106;&#30340;&#20020;&#26102;&#19968;&#33268;&#19981;&#24179;&#34913;&#26368;&#20248;&#20256;&#36755;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Temporally Consistent Unbalanced Optimal Transport for Unsupervised Action Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01518
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35299;&#20915;&#26368;&#20248;&#20256;&#36755;&#38382;&#39064;&#30340;&#21160;&#20316;&#20998;&#21106;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;Gromov-Wasserstein&#38382;&#39064;&#20013;&#32534;&#30721;&#26102;&#38388;&#19968;&#33268;&#24615;&#20808;&#39564;&#26469;&#23454;&#29616;&#20174;&#35270;&#39057;&#24103;&#21644;&#21160;&#20316;&#31867;&#21035;&#20043;&#38388;&#30340;&#22122;&#22768;&#25104;&#26412;&#20013;&#35299;&#30721;&#26102;&#38388;&#19968;&#33268;&#30340;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38271;&#26102;&#38388;&#26410;&#20462;&#21098;&#35270;&#39057;&#30340;&#21160;&#20316;&#20998;&#21106;&#20219;&#21153;&#30340;&#26032;&#26041;&#27861;&#65292;&#22522;&#20110;&#35299;&#20915;&#26368;&#20248;&#20256;&#36755;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#26102;&#38388;&#19968;&#33268;&#24615;&#20808;&#39564;&#32534;&#30721;&#21040;Gromov-Wasserstein&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#33021;&#22815;&#20174;&#35270;&#39057;&#24103;&#21644;&#21160;&#20316;&#31867;&#21035;&#20043;&#38388;&#30340;&#22122;&#22768;&#20851;&#32852;/&#21305;&#37197;&#25104;&#26412;&#30697;&#38453;&#20013;&#35299;&#30721;&#20986;&#19968;&#20010;&#26102;&#38388;&#19968;&#33268;&#30340;&#20998;&#21106;&#12290;&#19982;&#20808;&#21069;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#30693;&#36947;&#35270;&#39057;&#30340;&#21160;&#20316;&#39034;&#24207;&#26469;&#23454;&#29616;&#26102;&#38388;&#19968;&#33268;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#65288;&#34701;&#21512;&#65289;Gromov-Wasserstein&#38382;&#39064;&#21487;&#20197;&#22312;GPU&#19978;&#20351;&#29992;&#20960;&#27425;&#25237;&#24433;&#38236;&#19979;&#38477;&#36845;&#20195;&#39640;&#25928;&#27714;&#35299;&#12290;&#25105;&#20204;&#22312;&#26080;&#30417;&#30563;&#23398;&#20064;&#29615;&#22659;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20854;&#20013;&#25105;&#20204;&#30340;&#26041;&#27861;&#29992;&#20110;&#29983;&#25104;&#33258;&#35757;&#32451;&#30340;&#20266;&#26631;&#31614;&#12290;&#25105;&#20204;&#22312;Breakfast&#12289;50-Salads&#12289;YouTube Instructions&#21644;Desktop Assembly&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#20998;&#21106;&#26041;&#27861;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#27969;&#31243;&#65292;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01518v1 Announce Type: cross  Abstract: We propose a novel approach to the action segmentation task for long, untrimmed videos, based on solving an optimal transport problem. By encoding a temporal consistency prior into a Gromov-Wasserstein problem, we are able to decode a temporally consistent segmentation from a noisy affinity/matching cost matrix between video frames and action classes. Unlike previous approaches, our method does not require knowing the action order for a video to attain temporal consistency. Furthermore, our resulting (fused) Gromov-Wasserstein problem can be efficiently solved on GPUs using a few iterations of projected mirror descent. We demonstrate the effectiveness of our method in an unsupervised learning setting, where our method is used to generate pseudo-labels for self-training. We evaluate our segmentation approach and unsupervised learning pipeline on the Breakfast, 50-Salads, YouTube Instructions and Desktop Assembly datasets, yielding state
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22312;&#36127;&#36733;&#39044;&#27979;&#20013;&#20351;&#29992;&#20010;&#24615;&#21270;&#23618;&#30340;&#36890;&#29992;&#26694;&#26550;PL-FL&#65292;&#36890;&#36807;&#30740;&#31350;&#34920;&#26126;PL-FL&#22312;&#20302;&#36890;&#20449;&#24102;&#23485;&#35201;&#27714;&#19979;&#20248;&#20110;FL&#21644;&#32431;&#26412;&#22320;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2404.01517</link><description>&lt;p&gt;
&#20351;&#29992;&#20010;&#24615;&#21270;&#23618;&#35299;&#20915;&#32852;&#37030;&#36127;&#36733;&#39044;&#27979;&#20013;&#30340;&#24322;&#36136;&#24615;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Addressing Heterogeneity in Federated Load Forecasting with Personalization Layers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01517
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22312;&#36127;&#36733;&#39044;&#27979;&#20013;&#20351;&#29992;&#20010;&#24615;&#21270;&#23618;&#30340;&#36890;&#29992;&#26694;&#26550;PL-FL&#65292;&#36890;&#36807;&#30740;&#31350;&#34920;&#26126;PL-FL&#22312;&#20302;&#36890;&#20449;&#24102;&#23485;&#35201;&#27714;&#19979;&#20248;&#20110;FL&#21644;&#32431;&#26412;&#22320;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#30005;&#34920;&#30340;&#20986;&#29616;&#20351;&#24471;&#21487;&#20197;&#24191;&#27867;&#25910;&#38598;&#29992;&#30005;&#25968;&#25454;&#26469;&#35757;&#32451;&#30701;&#26399;&#36127;&#36733;&#39044;&#27979;&#27169;&#22411;&#12290;&#20026;&#20102;&#24212;&#23545;&#38544;&#31169;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20316;&#20026;&#35757;&#32451;&#30340;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#65292;&#20294;&#38543;&#30528;&#23458;&#25143;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#22686;&#21152;&#65292;&#25152;&#35757;&#32451;&#27169;&#22411;&#30340;&#36136;&#37327;&#20250;&#19979;&#38477;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#19968;&#20010;&#21517;&#20026;PL-FL&#30340;&#36890;&#29992;&#26694;&#26550;&#20013;&#20351;&#29992;&#20010;&#24615;&#21270;&#23618;&#36827;&#34892;&#36127;&#36733;&#39044;&#27979;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;PL-FL&#32988;&#36807;FL&#21644;&#32431;&#26412;&#22320;&#35757;&#32451;&#65292;&#21516;&#26102;&#36824;&#27604;FL&#38656;&#35201;&#26356;&#20302;&#30340;&#36890;&#20449;&#24102;&#23485;&#12290;&#36890;&#36807;&#22312;&#26469;&#33258;NREL ComStock&#25968;&#25454;&#20179;&#24211;&#30340;&#19977;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24191;&#27867;&#30340;&#27169;&#25311;&#23436;&#25104;&#20102;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01517v1 Announce Type: new  Abstract: The advent of smart meters has enabled pervasive collection of energy consumption data for training short-term load forecasting models. In response to privacy concerns, federated learning (FL) has been proposed as a privacy-preserving approach for training, but the quality of trained models degrades as client data becomes heterogeneous. In this paper we propose the use of personalization layers for load forecasting in a general framework called PL-FL. We show that PL-FL outperforms FL and purely local training, while requiring lower communication bandwidth than FL. This is done through extensive simulations on three different datasets from the NREL ComStock repository.
&lt;/p&gt;</description></item><item><title>&#22270;&#20687;&#32593;&#27169;&#22411;&#30340;&#20559;&#35265;&#26159;&#21542;&#33021;&#22815;&#35299;&#37322;&#27169;&#22411;&#30340;&#27867;&#21270;&#38382;&#39064;&#65292;&#23545;&#27492;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2404.01509</link><description>&lt;p&gt;
&#22270;&#20687;&#32593;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#33021;&#35299;&#37322;&#27867;&#21270;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Biases in ImageNet Models Explain Generalization?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01509
&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#32593;&#27169;&#22411;&#30340;&#20559;&#35265;&#26159;&#21542;&#33021;&#22815;&#35299;&#37322;&#27169;&#22411;&#30340;&#27867;&#21270;&#38382;&#39064;&#65292;&#23545;&#27492;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#26159;&#27169;&#22411;&#23545;&#26469;&#33258;&#35757;&#32451;&#20998;&#24067;&#38271;&#23614;&#30340;&#31232;&#26377;&#20869;&#37096;&#20998;&#24067;&#65288;ID&#65289;&#26679;&#26412;&#21644;&#35757;&#32451;&#20998;&#24067;&#20043;&#22806;&#65288;OOD&#65289;&#26679;&#26412;&#30340;&#24378;&#22823;&#27867;&#21270;&#33021;&#21147;&#12290;&#23545;&#20110;&#22270;&#20687;&#20998;&#31867;&#65292;&#36825;&#20307;&#29616;&#22312;&#23545;&#25197;&#26354;&#22270;&#20687;&#30340;&#25915;&#20987;&#12289;&#24615;&#33021;&#19979;&#38477;&#20197;&#21450;&#23545;&#27010;&#24565;&#65288;&#22914;&#33609;&#22270;&#65289;&#30340;&#27867;&#21270;&#19981;&#36275;&#12290;&#30446;&#21069;&#23545;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#30340;&#29702;&#35299;&#38750;&#24120;&#26377;&#38480;&#65292;&#20294;&#21457;&#29616;&#20102;&#19968;&#20123;&#21306;&#21035;&#27169;&#22411;&#19982;&#20154;&#31867;&#35270;&#35273;&#30340;&#20559;&#35265;&#65292;&#36825;&#20123;&#20559;&#35265;&#21487;&#33021;&#23548;&#33268;&#36825;&#20123;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#24050;&#32463;&#23581;&#35797;&#20102;&#22810;&#31181;&#19981;&#21516;&#25104;&#21151;&#31243;&#24230;&#30340;&#26041;&#27861;&#26469;&#20943;&#23569;&#36825;&#20123;&#35757;&#32451;&#20013;&#30340;&#20559;&#35265;&#20197;&#25913;&#21892;&#27867;&#21270;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#24050;&#24314;&#31435;&#30340;ResNet-50&#26550;&#26500;&#19978;&#36827;&#34892;&#22823;&#35268;&#27169;&#30740;&#31350;&#65292;&#22312;48&#20010;&#36890;&#36807;&#19981;&#21516;&#33719;&#21462;&#36884;&#24452;&#33719;&#24471;&#30340;ImageNet&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01509v1 Announce Type: cross  Abstract: The robust generalization of models to rare, in-distribution (ID) samples drawn from the long tail of the training distribution and to out-of-training-distribution (OOD) samples is one of the major challenges of current deep learning methods. For image classification, this manifests in the existence of adversarial attacks, the performance drops on distorted images, and a lack of generalization to concepts such as sketches. The current understanding of generalization in neural networks is very limited, but some biases that differentiate models from human vision have been identified and might be causing these limitations. Consequently, several attempts with varying success have been made to reduce these biases during training to improve generalization. We take a step back and sanity-check these attempts. Fixing the architecture to the well-established ResNet-50, we perform a large-scale study on 48 ImageNet models obtained via different 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25972;&#21512;&#26041;&#27861;&#65292;&#20351;&#29992;MosquitoFusion&#25968;&#25454;&#38598;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#23454;&#29616;&#23545;&#34442;&#23376;&#12289;&#34442;&#32676;&#21644;&#32321;&#27542;&#22320;&#30340;&#23454;&#26102;&#26816;&#27979;&#65292;&#21516;&#26102;&#32467;&#21512;&#22320;&#29702;&#20449;&#24687;&#31995;&#32479;(GIS)&#36827;&#19968;&#27493;&#20016;&#23500;&#20102;&#31354;&#38388;&#27169;&#24335;&#20998;&#26512;&#30340;&#28145;&#24230;&#12290;</title><link>https://arxiv.org/abs/2404.01501</link><description>&lt;p&gt;
MosquitoFusion&#65306;&#29992;&#28145;&#24230;&#23398;&#20064;&#23454;&#26102;&#26816;&#27979;&#34442;&#23376;&#12289;&#34442;&#32676;&#21644;&#32321;&#27542;&#22320;&#30340;&#22810;&#31867;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
MosquitoFusion: A Multiclass Dataset for Real-Time Detection of Mosquitoes, Swarms, and Breeding Sites Using Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01501
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25972;&#21512;&#26041;&#27861;&#65292;&#20351;&#29992;MosquitoFusion&#25968;&#25454;&#38598;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#23454;&#29616;&#23545;&#34442;&#23376;&#12289;&#34442;&#32676;&#21644;&#32321;&#27542;&#22320;&#30340;&#23454;&#26102;&#26816;&#27979;&#65292;&#21516;&#26102;&#32467;&#21512;&#22320;&#29702;&#20449;&#24687;&#31995;&#32479;(GIS)&#36827;&#19968;&#27493;&#20016;&#23500;&#20102;&#31354;&#38388;&#27169;&#24335;&#20998;&#26512;&#30340;&#28145;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25972;&#21512;&#26041;&#27861;&#65292;&#20351;&#29992;&#25105;&#20204;&#30340;&#22810;&#31867;&#25968;&#25454;&#38598;&#65288;MosquitoFusion&#65289;&#20013;&#21253;&#21547;&#30340;1204&#24352;&#22810;&#26679;&#21270;&#22270;&#20687;&#20197;&#21450;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#65292;&#26469;&#33258;&#21160;&#21270;&#35782;&#21035;&#34442;&#23376;&#12289;&#34442;&#32676;&#21644;&#32321;&#27542;&#22320;&#30340;&#30495;&#23454;&#26102;&#38388;&#34442;&#23376;&#26816;&#27979;&#12290;&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#39044;&#35757;&#32451;YOLOv8&#27169;&#22411;&#36798;&#21040;&#20102;57.1%&#30340;&#24179;&#22343;&#31934;&#24230;&#65288;mAP@50&#65289;&#65292;&#31934;&#30830;&#29575;&#20026;73.4%&#65292;&#21484;&#22238;&#29575;&#20026;50.5%&#12290;&#22320;&#29702;&#20449;&#24687;&#31995;&#32479;&#65288;GIS&#65289;&#30340;&#25972;&#21512;&#36827;&#19968;&#27493;&#20016;&#23500;&#20102;&#25105;&#20204;&#20998;&#26512;&#30340;&#28145;&#24230;&#65292;&#25552;&#20379;&#20102;&#23545;&#31354;&#38388;&#27169;&#24335;&#30340;&#26377;&#20215;&#20540;&#35265;&#35299;&#12290;&#25968;&#25454;&#38598;&#21644;&#20195;&#30721;&#21487;&#22312;https://github.com/faiyazabdullah/MosquitoFusion&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01501v1 Announce Type: cross  Abstract: In this paper, we present an integrated approach to real-time mosquito detection using our multiclass dataset (MosquitoFusion) containing 1204 diverse images and leverage cutting-edge technologies, specifically computer vision, to automate the identification of Mosquitoes, Swarms, and Breeding Sites. The pre-trained YOLOv8 model, trained on this dataset, achieved a mean Average Precision (mAP@50) of 57.1%, with precision at 73.4% and recall at 50.5%. The integration of Geographic Information Systems (GIS) further enriches the depth of our analysis, providing valuable insights into spatial patterns. The dataset and code are available at https://github.com/faiyazabdullah/MosquitoFusion.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35780;&#20272;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#21457;&#29616;XGBoost&#27169;&#22411;&#22312;&#26862;&#26519;&#28779;&#28798;&#20998;&#31867;&#20013;&#20855;&#26377;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#21644;&#38543;&#26426;&#26862;&#26519;&#22238;&#24402;&#27169;&#22411;&#22312;&#39044;&#27979;&#28779;&#28798;&#33539;&#22260;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#38598;&#25104;&#25968;&#20540;&#25968;&#25454;&#21644;&#22270;&#20687;&#20449;&#24687;&#30340;&#28151;&#21512;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2404.01487</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#38598;&#25104;&#29305;&#24449;&#24037;&#31243;&#29992;&#20110;&#26862;&#26519;&#28779;&#28798;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Explainable AI Integrated Feature Engineering for Wildfire Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01487
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35780;&#20272;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#21457;&#29616;XGBoost&#27169;&#22411;&#22312;&#26862;&#26519;&#28779;&#28798;&#20998;&#31867;&#20013;&#20855;&#26377;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#21644;&#38543;&#26426;&#26862;&#26519;&#22238;&#24402;&#27169;&#22411;&#22312;&#39044;&#27979;&#28779;&#28798;&#33539;&#22260;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#38598;&#25104;&#25968;&#20540;&#25968;&#25454;&#21644;&#22270;&#20687;&#20449;&#24687;&#30340;&#28151;&#21512;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26862;&#26519;&#28779;&#28798;&#23545;&#39044;&#27979;&#25552;&#20986;&#20102;&#22797;&#26434;&#30340;&#25361;&#25112;&#65292;&#38656;&#35201;&#20351;&#29992;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#26377;&#25928;&#24314;&#27169;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;&#19982;&#39044;&#27979;&#26862;&#26519;&#28779;&#28798;&#30456;&#20851;&#30340;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#30340;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20102;&#24443;&#24213;&#35780;&#20272;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23545;&#20110;&#20998;&#31867;&#19981;&#21516;&#31867;&#22411;&#25110;&#38454;&#27573;&#30340;&#26862;&#26519;&#28779;&#28798;&#65292;XGBoost&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#21644;&#31283;&#20581;&#24615;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#38543;&#26426;&#26862;&#26519;&#22238;&#24402;&#27169;&#22411;&#22312;&#39044;&#27979;&#21463;&#24433;&#21709;&#30340;&#26862;&#26519;&#28779;&#28798;&#33539;&#22260;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#21516;&#26102;&#22312;&#39044;&#27979;&#35823;&#24046;&#21644;&#35299;&#37322;&#26041;&#24046;&#26041;&#38754;&#37117;&#34920;&#29616;&#20248;&#31168;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#38598;&#25104;&#25968;&#20540;&#25968;&#25454;&#21644;&#22270;&#20687;&#20449;&#24687;&#30340;&#28151;&#21512;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#21516;&#26102;&#36827;&#34892;&#20998;&#31867;&#21644;&#22238;&#24402;&#12290;&#20026;&#20102;&#26356;&#28145;&#20837;&#22320;&#20102;&#35299;&#36825;&#20123;&#27169;&#22411;&#30340;&#20915;&#31574;&#36807;&#31243;&#24182;&#35782;&#21035;&#20851;&#38190;&#30340;&#36129;&#29486;&#29305;&#24449;&#65292;&#25105;&#20204;&#21033;&#29992;eX
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01487v1 Announce Type: new  Abstract: Wildfires present intricate challenges for prediction, necessitating the use of sophisticated machine learning techniques for effective modeling\cite{jain2020review}. In our research, we conducted a thorough assessment of various machine learning algorithms for both classification and regression tasks relevant to predicting wildfires. We found that for classifying different types or stages of wildfires, the XGBoost model outperformed others in terms of accuracy and robustness. Meanwhile, the Random Forest regression model showed superior results in predicting the extent of wildfire-affected areas, excelling in both prediction error and explained variance. Additionally, we developed a hybrid neural network model that integrates numerical data and image information for simultaneous classification and regression. To gain deeper insights into the decision-making processes of these models and identify key contributing features, we utilized eX
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#33258;&#21160;&#39550;&#39542;&#31070;&#32463;&#36816;&#21160;&#35268;&#21010;&#26694;&#26550;&#65292;&#36890;&#36807;&#26597;&#35810;&#24863;&#20852;&#36259;&#30340;&#26102;&#31354;&#28857;&#30340;&#21344;&#25454;&#20449;&#24687;&#65292;&#36991;&#20813;&#20102;&#20256;&#32479;&#23545;&#35937;&#26816;&#27979;&#21644;&#23494;&#38598;&#21344;&#25454;&#26629;&#26684;&#22320;&#22270;&#26041;&#27861;&#20013;&#30340;&#20449;&#24687;&#20002;&#22833;&#21644;&#35745;&#31639;&#28010;&#36153;&#12290;</title><link>https://arxiv.org/abs/2404.01486</link><description>&lt;p&gt;
QuAD: &#22522;&#20110;&#26597;&#35810;&#30340;&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#36816;&#21160;&#35268;&#21010;&#26041;&#27861;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;
&lt;/p&gt;
&lt;p&gt;
QuAD: Query-based Interpretable Neural Motion Planning for Autonomous Driving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01486
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#33258;&#21160;&#39550;&#39542;&#31070;&#32463;&#36816;&#21160;&#35268;&#21010;&#26694;&#26550;&#65292;&#36890;&#36807;&#26597;&#35810;&#24863;&#20852;&#36259;&#30340;&#26102;&#31354;&#28857;&#30340;&#21344;&#25454;&#20449;&#24687;&#65292;&#36991;&#20813;&#20102;&#20256;&#32479;&#23545;&#35937;&#26816;&#27979;&#21644;&#23494;&#38598;&#21344;&#25454;&#26629;&#26684;&#22320;&#22270;&#26041;&#27861;&#20013;&#30340;&#20449;&#24687;&#20002;&#22833;&#21644;&#35745;&#31639;&#28010;&#36153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#24517;&#39035;&#20102;&#35299;&#20854;&#29615;&#22659;&#20197;&#30830;&#23450;&#36866;&#24403;&#30340;&#21160;&#20316;&#12290;&#20256;&#32479;&#33258;&#20027;&#31995;&#32479;&#20381;&#36182;&#20110;&#23545;&#35937;&#26816;&#27979;&#26469;&#25214;&#21040;&#22330;&#26223;&#20013;&#30340;&#20195;&#29702;&#12290;&#28982;&#32780;&#65292;&#23545;&#35937;&#26816;&#27979;&#20551;&#35774;&#19968;&#32452;&#31163;&#25955;&#30340;&#23545;&#35937;&#65292;&#24182;&#20002;&#22833;&#26377;&#20851;&#19981;&#30830;&#23450;&#24615;&#30340;&#20449;&#24687;&#65292;&#22240;&#27492;&#22312;&#39044;&#27979;&#36825;&#20123;&#20195;&#29702;&#26410;&#26469;&#34892;&#20026;&#26102;&#20219;&#20309;&#38169;&#35823;&#37117;&#20250;&#32047;&#31215;&#12290;&#19982;&#27492;&#30456;&#21453;&#65292;&#23494;&#38598;&#30340;&#21344;&#25454;&#26629;&#26684;&#22320;&#22270;&#24050;&#34987;&#29992;&#20110;&#29702;&#35299;&#33258;&#30001;&#31354;&#38388;&#12290;&#28982;&#32780;&#65292;&#20026;&#25972;&#20010;&#22330;&#26223;&#39044;&#27979;&#32593;&#26684;&#26159;&#28010;&#36153;&#30340;&#65292;&#22240;&#20026;&#21482;&#26377;&#26576;&#20123;&#26102;&#31354;&#21306;&#22495;&#26159;&#21487;&#21040;&#36798;&#30340;&#24182;&#19988;&#23545;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30456;&#20851;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#12289;&#21487;&#35299;&#37322;&#30340;&#12289;&#39640;&#25928;&#30340;&#33258;&#20027;&#26694;&#26550;&#65292;&#25670;&#33073;&#20102;&#20808;&#24863;&#30693;&#12289;&#20877;&#39044;&#27979;&#65292;&#26368;&#21518;&#35268;&#21010;&#30340;&#32423;&#32852;&#27169;&#22359;&#30340;&#33539;&#24335;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#23558;&#35268;&#21010;&#32773;&#30340;&#37325;&#28857;&#36716;&#31227;&#21040;&#26597;&#35810;&#30456;&#20851;&#26102;&#31354;&#28857;&#30340;&#21344;&#25454;&#65292;&#38480;&#21046;&#35745;&#31639;&#22312;&#36825;&#20123;&#24863;&#20852;&#36259;&#30340;&#21306;&#22495;&#19978;&#12290;&#21033;&#29992;&#36825;&#31181;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01486v1 Announce Type: cross  Abstract: A self-driving vehicle must understand its environment to determine the appropriate action. Traditional autonomy systems rely on object detection to find the agents in the scene. However, object detection assumes a discrete set of objects and loses information about uncertainty, so any errors compound when predicting the future behavior of those agents. Alternatively, dense occupancy grid maps have been utilized to understand free-space. However, predicting a grid for the entire scene is wasteful since only certain spatio-temporal regions are reachable and relevant to the self-driving vehicle. We present a unified, interpretable, and efficient autonomy framework that moves away from cascading modules that first perceive, then predict, and finally plan. Instead, we shift the paradigm to have the planner query occupancy at relevant spatio-temporal points, restricting the computation to those regions of interest. Exploiting this represent
&lt;/p&gt;</description></item><item><title>TraveLER&#26159;&#19968;&#31181;&#22810;&#37325;LMM&#20195;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#27839;&#30528;&#35270;&#39057;&#31227;&#21160;&#65292;&#24182;&#36890;&#36807;&#20132;&#20114;&#24335;&#25552;&#38382;&#25910;&#38598;&#20851;&#38190;&#24103;&#30340;&#20449;&#24687;&#65292;&#20197;&#35299;&#20915;&#35270;&#39057;&#38382;&#31572;&#20013;&#20851;&#38190;&#26102;&#38388;&#25139;&#36873;&#25321;&#21644;&#38169;&#35823;&#26102;&#38388;&#25139;&#35843;&#25972;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2404.01476</link><description>&lt;p&gt;
TraveLER&#65306;&#29992;&#20110;&#35270;&#39057;&#38382;&#31572;&#30340;&#22810;&#37325;LMM&#20195;&#29702;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
TraveLER: A Multi-LMM Agent Framework for Video Question-Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01476
&lt;/p&gt;
&lt;p&gt;
TraveLER&#26159;&#19968;&#31181;&#22810;&#37325;LMM&#20195;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#27839;&#30528;&#35270;&#39057;&#31227;&#21160;&#65292;&#24182;&#36890;&#36807;&#20132;&#20114;&#24335;&#25552;&#38382;&#25910;&#38598;&#20851;&#38190;&#24103;&#30340;&#20449;&#24687;&#65292;&#20197;&#35299;&#20915;&#35270;&#39057;&#38382;&#31572;&#20013;&#20851;&#38190;&#26102;&#38388;&#25139;&#36873;&#25321;&#21644;&#38169;&#35823;&#26102;&#38388;&#25139;&#35843;&#25972;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMMs&#65289;&#22312;&#35270;&#39057;&#38382;&#31572;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#35268;&#27169;&#12289;&#22522;&#20110;&#22270;&#20687;&#30340;&#39044;&#35757;&#32451;&#20197;&#38646;&#26679;&#26412;&#26041;&#24335;&#20197;&#24103;&#20026;&#21333;&#20301;&#36827;&#34892;&#22788;&#29702;&#12290;&#34429;&#28982;&#22522;&#20110;&#22270;&#20687;&#30340;&#35270;&#39057;&#26041;&#27861;&#23637;&#29616;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#20294;&#30446;&#21069;&#30340;&#23616;&#38480;&#26159;&#23427;&#20204;&#32463;&#24120;&#24573;&#35270;&#20102;&#22914;&#20309;&#36873;&#25321;&#20851;&#38190;&#26102;&#38388;&#25139;&#65292;&#24182;&#19988;&#26080;&#27861;&#22312;&#30830;&#23450;&#38169;&#35823;&#26102;&#38388;&#25139;&#26102;&#36827;&#34892;&#35843;&#25972;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#26080;&#27861;&#25552;&#21462;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#32454;&#33410;&#65292;&#32780;&#26159;&#25552;&#20379;&#24103;&#30340;&#19968;&#33324;&#25551;&#36848;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22810;&#37325;LMM&#20195;&#29702;&#26694;&#26550;&#65292;&#23427;&#27839;&#30528;&#35270;&#39057;&#36827;&#34892;&#31227;&#21160;&#65292;&#36890;&#36807;&#20132;&#20114;&#24335;&#25552;&#38382;&#30340;&#26041;&#24335;&#36845;&#20195;&#22320;&#20174;&#20851;&#38190;&#24103;&#25910;&#38598;&#30456;&#20851;&#20449;&#24687;&#65292;&#30452;&#21040;&#33719;&#24471;&#36275;&#22815;&#30340;&#20449;&#24687;&#26469;&#22238;&#31572;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TraveLER&#65292;&#36825;&#26159;&#19968;&#20010;&#21487;&#20197;&#21046;&#23450;&#8220;&#36941;&#21382;&#8221;&#35270;&#39057;&#35745;&#21010;&#30340;&#27169;&#22411;&#65292;&#35810;&#38382;&#20851;&#20110;&#21333;&#20010;&#24103;&#30340;&#38382;&#39064;&#20197;&#8220;&#23450;&#20301;&#8221;&#24182;&#23384;&#20648;&#20851;&#38190;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01476v1 Announce Type: cross  Abstract: Recently, Large Multimodal Models (LMMs) have made significant progress in video question-answering using a frame-wise approach by leveraging large-scale, image-based pretraining in a zero-shot manner. While image-based methods for videos have shown impressive performance, a current limitation is that they often overlook how key timestamps are selected and cannot adjust when incorrect timestamps are identified. Moreover, they are unable to extract details relevant to the question, instead providing general descriptions of the frame. To overcome this, we design a multi-LMM agent framework that travels along the video, iteratively collecting relevant information from keyframes through interactive question-asking until there is sufficient information to answer the question. Specifically, we propose TraveLER, a model that can create a plan to "Traverse" through the video, ask questions about individual frames to "Locate" and store key info
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#26694;&#26550;&#8220;ChemBench&#8221;&#65292;&#26088;&#22312;&#35780;&#20272;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21270;&#23398;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#19982;&#20154;&#31867;&#21270;&#23398;&#23478;&#19987;&#19994;&#30693;&#35782;&#30340;&#23545;&#27604;&#12290;</title><link>https://arxiv.org/abs/2404.01475</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#26159;&#36229;&#20154;&#31867;&#21270;&#23398;&#23478;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are large language models superhuman chemists?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01475
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#26694;&#26550;&#8220;ChemBench&#8221;&#65292;&#26088;&#22312;&#35780;&#20272;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21270;&#23398;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#19982;&#20154;&#31867;&#21270;&#23398;&#23478;&#19987;&#19994;&#30693;&#35782;&#30340;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22240;&#20854;&#22788;&#29702;&#20154;&#31867;&#35821;&#35328;&#24182;&#25191;&#34892;&#26410;&#32463;&#26126;&#30830;&#35757;&#32451;&#30340;&#20219;&#21153;&#30340;&#33021;&#21147;&#32780;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#36825;&#23545;&#21270;&#23398;&#31185;&#23398;&#26159;&#30456;&#20851;&#30340;&#65292;&#22240;&#20026;&#21270;&#23398;&#38754;&#20020;&#30528;&#25968;&#25454;&#38598;&#23567;&#19988;&#22810;&#26679;&#30340;&#38382;&#39064;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#36890;&#24120;&#20197;&#25991;&#26412;&#24418;&#24335;&#21576;&#29616;&#12290; LLMs&#22312;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#20986;&#28508;&#21147;&#65292;&#24182;&#19988;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#21033;&#29992;&#26469;&#39044;&#27979;&#21270;&#23398;&#24615;&#36136;&#65292;&#20248;&#21270;&#21453;&#24212;&#65292;&#29978;&#33267;&#33258;&#20027;&#35774;&#35745;&#21644;&#36827;&#34892;&#23454;&#39564;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23545;LLMs&#30340;&#21270;&#23398;&#25512;&#29702;&#33021;&#21147;&#20165;&#26377;&#38750;&#24120;&#26377;&#38480;&#30340;&#31995;&#32479;&#24615;&#29702;&#35299;&#65292;&#36825;&#26159;&#25913;&#36827;&#27169;&#22411;&#21644;&#20943;&#36731;&#28508;&#22312;&#21361;&#23475;&#25152;&#24517;&#38656;&#30340;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#8220;ChemBench&#8221;&#65292;&#36825;&#26159;&#19968;&#20010;&#33258;&#21160;&#21270;&#26694;&#26550;&#65292;&#26088;&#22312;&#20005;&#26684;&#35780;&#20272;&#26368;&#20808;&#36827;&#30340;LLMs&#30340;&#21270;&#23398;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#20197;&#19982;&#20154;&#31867;&#21270;&#23398;&#23478;&#30340;&#19987;&#19994;&#30693;&#35782;&#30456;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01475v1 Announce Type: cross  Abstract: Large language models (LLMs) have gained widespread interest due to their ability to process human language and perform tasks on which they have not been explicitly trained. This is relevant for the chemical sciences, which face the problem of small and diverse datasets that are frequently in the form of text. LLMs have shown promise in addressing these issues and are increasingly being harnessed to predict chemical properties, optimize reactions, and even design and conduct experiments autonomously. However, we still have only a very limited systematic understanding of the chemical reasoning capabilities of LLMs, which would be required to improve models and mitigate potential harms. Here, we introduce "ChemBench," an automated framework designed to rigorously evaluate the chemical knowledge and reasoning abilities of state-of-the-art LLMs against the expertise of human chemists. We curated more than 7,000 question-answer pairs for a 
&lt;/p&gt;</description></item><item><title>TS-CausalNN&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#21487;&#20197;&#21516;&#26102;&#21457;&#29616;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#21516;&#26102;&#21457;&#29983;&#21644;&#28382;&#21518;&#30340;&#22240;&#26524;&#20851;&#31995;</title><link>https://arxiv.org/abs/2404.01466</link><description>&lt;p&gt;
TS-CausalNN: &#20174;&#38750;&#32447;&#24615;&#38750;&#24179;&#31283;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#23398;&#20064;&#26102;&#38388;&#22240;&#26524;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
TS-CausalNN: Learning Temporal Causal Relations from Non-linear Non-stationary Time Series Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01466
&lt;/p&gt;
&lt;p&gt;
TS-CausalNN&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#21487;&#20197;&#21516;&#26102;&#21457;&#29616;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#21516;&#26102;&#21457;&#29983;&#21644;&#28382;&#21518;&#30340;&#22240;&#26524;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01466v1 Announce Type: new  Abstract: The growing availability and importance of time series data across various domains, including environmental science, epidemiology, and economics, has led to an increasing need for time-series causal discovery methods that can identify the intricate relationships in the non-stationary, non-linear, and often noisy real world data. However, the majority of current time series causal discovery methods assume stationarity and linear relations in data, making them infeasible for the task. Further, the recent deep learning-based methods rely on the traditional causal structure learning approaches making them computationally expensive. In this paper, we propose a Time-Series Causal Neural Network (TS-CausalNN) - a deep learning technique to discover contemporaneous and lagged causal relations simultaneously. Our proposed architecture comprises (i) convolutional blocks comprising parallel custom causal layers, (ii) acyclicity constraint, and (iii
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26080;&#30417;&#30563;&#19977;&#32500;&#25554;&#20540;&#26694;&#26550;UVI-Net&#65292;&#33021;&#22815;&#22312;4D&#21307;&#23398;&#22270;&#20687;&#20013;&#23454;&#29616;&#26080;&#38656;&#20013;&#38388;&#24103;&#30340;&#26102;&#38388;&#25554;&#20540;&#65292;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#24615;&#33021;&#25913;&#21892;&#12290;</title><link>https://arxiv.org/abs/2404.01464</link><description>&lt;p&gt;
4D&#21307;&#23398;&#22270;&#20687;&#30340;&#26080;&#30417;&#30563;&#25554;&#20540;&#26041;&#27861;&#65306;&#26080;&#38656;&#20219;&#20309;&#20013;&#38388;&#24103;
&lt;/p&gt;
&lt;p&gt;
Data-Efficient Unsupervised Interpolation Without Any Intermediate Frame for 4D Medical Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01464
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26080;&#30417;&#30563;&#19977;&#32500;&#25554;&#20540;&#26694;&#26550;UVI-Net&#65292;&#33021;&#22815;&#22312;4D&#21307;&#23398;&#22270;&#20687;&#20013;&#23454;&#29616;&#26080;&#38656;&#20013;&#38388;&#24103;&#30340;&#26102;&#38388;&#25554;&#20540;&#65292;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#24615;&#33021;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
4D&#21307;&#23398;&#22270;&#20687;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#33021;&#22815;&#25429;&#25417;&#21160;&#24577;&#21464;&#21270;&#24182;&#30417;&#27979;&#38271;&#26399;&#30142;&#30149;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#33719;&#21462;4D&#21307;&#23398;&#22270;&#20687;&#20250;&#38754;&#20020;&#25361;&#25112;&#65292;&#22914;&#36752;&#23556;&#26292;&#38706;&#21644;&#25104;&#20687;&#26102;&#38388;&#65292;&#38656;&#35201;&#22312;&#23454;&#29616;&#39640;&#26102;&#38388;&#20998;&#36776;&#29575;&#21644;&#20943;&#23569;&#19981;&#33391;&#24433;&#21709;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25968;&#25454;&#33719;&#21462;&#19981;&#20165;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22686;&#21152;&#27599;&#20010;&#25968;&#25454;&#38598;&#30340;&#24103;&#29575;&#20063;&#24456;&#22256;&#38590;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26080;&#30417;&#30563;&#19977;&#32500;&#25554;&#20540;&#26694;&#26550;&#65292;UVI-Net&#12290;&#35813;&#26694;&#26550;&#20419;&#36827;&#20102;&#26102;&#38388;&#25554;&#20540;&#32780;&#26080;&#38656;&#20219;&#20309;&#20013;&#38388;&#24103;&#65292;&#19982;&#22823;&#22810;&#25968;&#29616;&#26377;&#26080;&#30417;&#30563;&#26041;&#27861;&#19981;&#21516;&#12290;&#23545;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#35780;&#20272;&#25351;&#26631;&#19978;&#37117;&#21462;&#24471;&#20102;&#26174;&#30528;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01464v1 Announce Type: cross  Abstract: 4D medical images, which represent 3D images with temporal information, are crucial in clinical practice for capturing dynamic changes and monitoring long-term disease progression. However, acquiring 4D medical images poses challenges due to factors such as radiation exposure and imaging duration, necessitating a balance between achieving high temporal resolution and minimizing adverse effects. Given these circumstances, not only is data acquisition challenging, but increasing the frame rate for each dataset also proves difficult. To address this challenge, this paper proposes a simple yet effective Unsupervised Volumetric Interpolation framework, UVI-Net. This framework facilitates temporal interpolation without the need for any intermediate frames, distinguishing it from the majority of other existing unsupervised methods. Experiments on benchmark datasets demonstrate significant improvements across diverse evaluation metrics compare
&lt;/p&gt;</description></item><item><title>OpenChemIE&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20174;&#21270;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#21453;&#24212;&#25968;&#25454;&#30340;&#24037;&#20855;&#21253;&#65292;&#36890;&#36807;&#25972;&#21512;&#25991;&#26412;&#12289;&#34920;&#26684;&#21644;&#22270;&#20687;&#20449;&#24687;&#20197;&#21450;&#20351;&#29992;&#19987;&#38376;&#31070;&#32463;&#27169;&#22411;&#21644;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#25991;&#26723;&#32423;&#21035;&#30340;&#21453;&#24212;&#25968;&#25454;&#25552;&#21462;&#12290;</title><link>https://arxiv.org/abs/2404.01462</link><description>&lt;p&gt;
OpenChemIE&#65306;&#29992;&#20110;&#21270;&#23398;&#25991;&#29486;&#20449;&#24687;&#25552;&#21462;&#30340;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
OpenChemIE: An Information Extraction Toolkit For Chemistry Literature
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01462
&lt;/p&gt;
&lt;p&gt;
OpenChemIE&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20174;&#21270;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#21453;&#24212;&#25968;&#25454;&#30340;&#24037;&#20855;&#21253;&#65292;&#36890;&#36807;&#25972;&#21512;&#25991;&#26412;&#12289;&#34920;&#26684;&#21644;&#22270;&#20687;&#20449;&#24687;&#20197;&#21450;&#20351;&#29992;&#19987;&#38376;&#31070;&#32463;&#27169;&#22411;&#21644;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#25991;&#26723;&#32423;&#21035;&#30340;&#21453;&#24212;&#25968;&#25454;&#25552;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01462v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;  &#25688;&#35201;&#65306;&#20174;&#21270;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#20449;&#24687;&#23545;&#20110;&#26500;&#24314;&#25968;&#25454;&#39537;&#21160;&#21270;&#23398;&#30340;&#26368;&#26032;&#21453;&#24212;&#25968;&#25454;&#24211;&#33267;&#20851;&#37325;&#35201;&#12290;&#23436;&#25972;&#30340;&#20449;&#24687;&#25552;&#21462;&#38656;&#35201;&#32467;&#21512;&#25991;&#26412;&#12289;&#34920;&#26684;&#21644;&#22270;&#20687;&#20013;&#30340;&#20449;&#24687;&#65292;&#32780;&#20197;&#24448;&#30340;&#24037;&#20316;&#20027;&#35201;&#30740;&#31350;&#20174;&#21333;&#19968;&#26041;&#24335;&#25552;&#21462;&#21453;&#24212;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;OpenChemIE&#26469;&#35299;&#20915;&#36825;&#19968;&#22797;&#26434;&#25361;&#25112;&#65292;&#23454;&#29616;&#22312;&#25991;&#26723;&#32423;&#21035;&#25552;&#21462;&#21453;&#24212;&#25968;&#25454;&#12290;OpenChemIE&#20998;&#20004;&#27493;&#35299;&#20915;&#38382;&#39064;&#65306;&#20174;&#21508;&#20010;&#26041;&#24335;&#20013;&#25552;&#21462;&#30456;&#20851;&#20449;&#24687;&#65292;&#28982;&#21518;&#25972;&#21512;&#32467;&#26524;&#24471;&#21040;&#26368;&#32456;&#30340;&#21453;&#24212;&#21015;&#34920;&#12290;&#23545;&#20110;&#31532;&#19968;&#27493;&#65292;&#25105;&#20204;&#37319;&#29992;&#19987;&#38376;&#30340;&#31070;&#32463;&#27169;&#22411;&#65292;&#27599;&#20010;&#27169;&#22411;&#22788;&#29702;&#21270;&#23398;&#20449;&#24687;&#25552;&#21462;&#30340;&#29305;&#23450;&#20219;&#21153;&#65292;&#27604;&#22914;&#20174;&#25991;&#26412;&#25110;&#22270;&#20687;&#20013;&#35299;&#26512;&#20998;&#23376;&#25110;&#21453;&#24212;&#12290;&#28982;&#21518;&#25105;&#20204;&#20351;&#29992;&#21270;&#23398;&#30456;&#20851;&#30340;&#31639;&#27861;&#25972;&#21512;&#36825;&#20123;&#27169;&#22359;&#30340;&#20449;&#24687;&#65292;&#23454;&#29616;&#31934;&#32454;&#21270;&#21453;&#24212;&#25552;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01462v1 Announce Type: cross  Abstract: Information extraction from chemistry literature is vital for constructing up-to-date reaction databases for data-driven chemistry. Complete extraction requires combining information across text, tables, and figures, whereas prior work has mainly investigated extracting reactions from single modalities. In this paper, we present OpenChemIE to address this complex challenge and enable the extraction of reaction data at the document level. OpenChemIE approaches the problem in two steps: extracting relevant information from individual modalities and then integrating the results to obtain a final list of reactions. For the first step, we employ specialized neural models that each address a specific task for chemistry information extraction, such as parsing molecules or reactions from text or figures. We then integrate the information from these modules using chemistry-informed algorithms, allowing for the extraction of fine-grained reactio
&lt;/p&gt;</description></item><item><title>&#32467;&#21512;&#21338;&#24328;&#35770;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#22320;&#29702;&#20998;&#24067;&#24335;&#25968;&#25454;&#20013;&#24515;&#20013;AI&#25512;&#26029;&#24037;&#20316;&#36127;&#36733;&#30340;&#20998;&#24067;&#65292;&#20197;&#38477;&#20302;&#30899;&#25490;&#25918;&#21644;&#36816;&#33829;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2404.01459</link><description>&lt;p&gt;
&#28216;&#25103;&#29702;&#35770;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#22320;&#29702;&#20998;&#24067;&#24335;&#25968;&#25454;&#20013;&#24515;&#20013;&#26368;&#23567;&#21270;&#30899;&#25490;&#25918;&#21644;&#33021;&#28304;&#25104;&#26412;&#30340;AI&#25512;&#26029;&#36127;&#36733;
&lt;/p&gt;
&lt;p&gt;
Game-Theoretic Deep Reinforcement Learning to Minimize Carbon Emissions and Energy Costs for AI Inference Workloads in Geo-Distributed Data Centers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01459
&lt;/p&gt;
&lt;p&gt;
&#32467;&#21512;&#21338;&#24328;&#35770;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#22320;&#29702;&#20998;&#24067;&#24335;&#25968;&#25454;&#20013;&#24515;&#20013;AI&#25512;&#26029;&#24037;&#20316;&#36127;&#36733;&#30340;&#20998;&#24067;&#65292;&#20197;&#38477;&#20302;&#30899;&#25490;&#25918;&#21644;&#36816;&#33829;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#20013;&#24515;&#30001;&#20110;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#24037;&#20316;&#36127;&#36733;&#30340;&#22686;&#21152;&#32780;&#28040;&#32791;&#26356;&#22810;&#33021;&#28304;&#65292;&#36825;&#23545;&#29615;&#22659;&#36896;&#25104;&#36127;&#38754;&#24433;&#21709;&#24182;&#25552;&#39640;&#36816;&#33829;&#25104;&#26412;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#26041;&#27861;&#65292;&#23558;&#21338;&#24328;&#35770;&#65288;GT&#65289;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#30456;&#32467;&#21512;&#65292;&#20248;&#21270;&#22320;&#29702;&#20998;&#24067;&#24335;&#25968;&#25454;&#20013;&#24515;&#20013;AI&#25512;&#26029;&#24037;&#20316;&#36127;&#36733;&#30340;&#20998;&#24067;&#65292;&#20197;&#20943;&#23569;&#30899;&#25490;&#25918;&#21644;&#20113;&#36816;&#33829;&#65288;&#33021;&#28304;+&#25968;&#25454;&#20256;&#36755;&#65289;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01459v1 Announce Type: cross  Abstract: Data centers are increasingly using more energy due to the rise in Artificial Intelligence (AI) workloads, which negatively impacts the environment and raises operational costs. Reducing operating expenses and carbon emissions while maintaining performance in data centers is a challenging problem. This work introduces a unique approach combining Game Theory (GT) and Deep Reinforcement Learning (DRL) for optimizing the distribution of AI inference workloads in geo-distributed data centers to reduce carbon emissions and cloud operating (energy + data transfer) costs. The proposed technique integrates the principles of non-cooperative Game Theory into a DRL framework, enabling data centers to make intelligent decisions regarding workload allocation while considering the heterogeneity of hardware resources, the dynamic nature of electricity prices, inter-data center data transfer costs, and carbon footprints. We conducted extensive experim
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#20808;&#39564;&#39057;&#29575;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#65288;PFGDM&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#31283;&#20581;&#19988;&#20445;&#25345;&#32467;&#26500;&#30340;&#26377;&#38480;&#35282;&#24230;CBCT&#37325;&#24314;&#12290;</title><link>https://arxiv.org/abs/2404.01448</link><description>&lt;p&gt;
&#26377;&#38480;&#35282;&#24230;(CBCT)&#37325;&#24314;&#30340;&#20808;&#39564;&#39057;&#29575;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Prior Frequency Guided Diffusion Model for Limited Angle (LA)-CBCT Reconstruction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01448
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#20808;&#39564;&#39057;&#29575;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#65288;PFGDM&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#31283;&#20581;&#19988;&#20445;&#25345;&#32467;&#26500;&#30340;&#26377;&#38480;&#35282;&#24230;CBCT&#37325;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38181;&#26463;&#35745;&#31639;&#26426;&#20307;&#23618;&#25668;&#24433;&#65288;CBCT&#65289;&#24191;&#27867;&#24212;&#29992;&#20110;&#22270;&#20687;&#24341;&#23548;&#25918;&#30103;&#12290;&#20174;&#26377;&#38480;&#35282;&#24230;&#37319;&#38598;&#65288;LA-CBCT&#65289;&#37325;&#24314;CBCT&#23545;&#20110;&#25552;&#39640;&#25104;&#20687;&#25928;&#29575;&#12289;&#20943;&#23569;&#21058;&#37327;&#20197;&#21450;&#26356;&#22909;&#30340;&#26426;&#26800;&#38388;&#38553;&#28165;&#38500;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;LA-CBCT&#37325;&#24314;&#21463;&#21040;&#20005;&#37325;&#27424;&#37319;&#26679;&#20266;&#24433;&#30340;&#22256;&#25200;&#65292;&#20351;&#20854;&#25104;&#20026;&#39640;&#24230;&#19981;&#36866;&#23450;&#30340;&#36870;&#38382;&#39064;&#12290;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#25968;&#25454;&#20998;&#24067;&#36870;&#36716;&#25968;&#25454;&#21152;&#22122;&#36807;&#31243;&#26469;&#29983;&#25104;&#25968;&#25454;/&#22270;&#20687;&#65307;&#24182;&#19988;&#21487;&#20197;&#20316;&#20026;LA-CBCT&#37325;&#24314;&#20013;&#30340;&#21435;&#22122;&#22120;/&#27491;&#21017;&#21270;&#22120;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20026;LA-CBCT&#37325;&#24314;&#24320;&#21457;&#20102;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#21363;&#20808;&#39564;&#39057;&#29575;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#65288;PFGDM&#65289;&#65292;&#29992;&#20110;&#31283;&#20581;&#19988;&#20445;&#25345;&#32467;&#26500;&#30340;LA-CBCT&#37325;&#24314;&#12290;PFGDM&#20351;&#29992;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;LA-CBCT&#37325;&#24314;&#30340;&#27491;&#21017;&#21270;&#22120;&#65292;&#26465;&#20214;&#26159;&#22522;&#20110;&#20174;&#24739;&#32773;&#29305;&#23450;&#20808;&#21069;CT&#25195;&#25551;&#20013;&#25552;&#21462;&#30340;&#39640;&#39057;&#20449;&#24687;&#65292;&#36825;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01448v1 Announce Type: cross  Abstract: Cone-beam computed tomography (CBCT) is widely used in image-guided radiotherapy. Reconstructing CBCTs from limited-angle acquisitions (LA-CBCT) is highly desired for improved imaging efficiency, dose reduction, and better mechanical clearance. LA-CBCT reconstruction, however, suffers from severe under-sampling artifacts, making it a highly ill-posed inverse problem. Diffusion models can generate data/images by reversing a data-noising process through learned data distributions; and can be incorporated as a denoiser/regularizer in LA-CBCT reconstruction. In this study, we developed a diffusion model-based framework, prior frequency-guided diffusion model (PFGDM), for robust and structure-preserving LA-CBCT reconstruction. PFGDM uses a conditioned diffusion model as a regularizer for LA-CBCT reconstruction, and the condition is based on high-frequency information extracted from patient-specific prior CT scans which provides a strong ana
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22312;&#32447;&#25991;&#26412;&#28040;&#24687;&#20013;&#39044;&#27979;&#34920;&#24773;&#31526;&#21495;&#25152;&#34920;&#36798;&#24773;&#24863;&#30340;&#26032;&#26041;&#27861;&#65292;&#26080;&#38656;&#20154;&#24037;&#26631;&#27880;&#25968;&#25454;&#65292;&#33410;&#30465;&#20102;&#23453;&#36149;&#26102;&#38388;&#12290;</title><link>https://arxiv.org/abs/2404.01439</link><description>&lt;p&gt;
&#20174;&#25551;&#36848;&#20013;&#26080;&#30417;&#30563;&#22320;&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#65292;&#21019;&#24314;&#34920;&#24773;&#31526;&#21495;&#35789;&#24211;
&lt;/p&gt;
&lt;p&gt;
Creating emoji lexica from unsupervised sentiment analysis of their descriptions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01439
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22312;&#32447;&#25991;&#26412;&#28040;&#24687;&#20013;&#39044;&#27979;&#34920;&#24773;&#31526;&#21495;&#25152;&#34920;&#36798;&#24773;&#24863;&#30340;&#26032;&#26041;&#27861;&#65292;&#26080;&#38656;&#20154;&#24037;&#26631;&#27880;&#25968;&#25454;&#65292;&#33410;&#30465;&#20102;&#23453;&#36149;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#19978;&#23186;&#20307;&#65292;&#22914;&#21338;&#23458;&#21644;&#31038;&#20132;&#32593;&#32476;&#32593;&#31449;&#65292;&#29983;&#25104;&#20102;&#22823;&#37327;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#20379;&#20998;&#26512;&#20010;&#20154;&#21644;&#32452;&#32455;&#30340;&#35266;&#28857;&#21644;&#24773;&#24863;&#20043;&#29992;&#12290;&#20256;&#32479;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#24050;&#32463;&#19981;&#33021;&#24456;&#22909;&#22320;&#37327;&#21270;&#36825;&#20123;&#35266;&#28857;&#30340;&#26497;&#24615;&#24230;&#37327;&#65292;&#22240;&#27492;&#38656;&#35201;&#26032;&#39062;&#30340;&#26041;&#27861;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#34920;&#24773;&#31526;&#21495;&#25152;&#34920;&#36798;&#30340;&#24773;&#24863;&#24471;&#21040;&#30340;&#20851;&#27880;&#36739;&#23569;&#12290;&#28982;&#32780;&#65292;&#36807;&#21435;&#22235;&#24180;&#38388;&#65292;&#31526;&#21495;&#30340;&#20351;&#29992;&#37327;&#28608;&#22686;&#12290;&#22914;&#20170;&#65292;Twitter&#20013;&#27599;&#22825;&#20250;&#34987;&#36755;&#20837;&#22823;&#32422;&#20004;&#30334;&#20159;&#20010;&#31526;&#21495;&#65292;&#24182;&#19988;&#27599;&#20010;&#26032;&#30340;Unicode&#29256;&#26412;&#37117;&#20250;&#22686;&#21152;&#26032;&#30340;&#34920;&#24773;&#31526;&#21495;&#65292;&#20351;&#24471;&#23427;&#20204;&#23545;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#39044;&#27979;&#22312;&#32447;&#25991;&#26412;&#28040;&#24687;&#65288;&#22914;&#25512;&#25991;&#65289;&#20013;&#34920;&#24773;&#31526;&#21495;&#34920;&#36798;&#30340;&#24773;&#24863;&#65292;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#20154;&#24037;&#25163;&#21160;&#27880;&#37322;&#25968;&#25454;&#65292;&#20197;&#27492;&#33410;&#30465;&#23453;&#36149;&#30340;&#26102;&#38388;&#29992;&#20110;&#20854;&#20182;&#20998;&#26512;&#20219;&#21153;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#33258;&#21160;&#26500;&#24314;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#34920;&#24773;&#31526;&#21495;&#24773;&#24863;&#35789;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01439v1 Announce Type: cross  Abstract: Online media, such as blogs and social networking sites, generate massive volumes of unstructured data of great interest to analyze the opinions and sentiments of individuals and organizations. Novel approaches beyond Natural Language Processing are necessary to quantify these opinions with polarity metrics. So far, the sentiment expressed by emojis has received little attention. The use of symbols, however, has boomed in the past four years. About twenty billion are typed in Twitter nowadays, and new emojis keep appearing in each new Unicode version, making them increasingly relevant to sentiment analysis tasks. This has motivated us to propose a novel approach to predict the sentiments expressed by emojis in online textual messages, such as tweets, that does not require human effort to manually annotate data and saves valuable time for other analysis tasks. For this purpose, we automatically constructed a novel emoji sentiment lexico
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;&#20110;RMSProp&#21644;Adam&#22312;&#38750;&#20984;&#20248;&#21270;&#20013;&#30340;&#32039;&#33268;&#25910;&#25947;&#24615;&#20998;&#26512;&#65292;&#39318;&#27425;&#23637;&#31034;&#20102;&#22312;&#26368;&#23485;&#26494;&#30340;&#20551;&#35774;&#19979;&#30340;&#25910;&#25947;&#24615;&#32467;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;RMSProp&#21644;Adam&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#20998;&#21035;&#20026;$\mathcal O(\epsilon^{-4})$&#12290;</title><link>https://arxiv.org/abs/2404.01436</link><description>&lt;p&gt;
RMSProp&#21644;Adam&#22312;&#20855;&#26377;&#20223;&#23556;&#22122;&#22768;&#26041;&#24046;&#30340;&#24191;&#20041;&#20809;&#28369;&#38750;&#20984;&#20248;&#21270;&#20013;&#30340;&#25910;&#25947;&#24615;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Convergence Guarantees for RMSProp and Adam in Generalized-smooth Non-convex Optimization with Affine Noise Variance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01436
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;&#20110;RMSProp&#21644;Adam&#22312;&#38750;&#20984;&#20248;&#21270;&#20013;&#30340;&#32039;&#33268;&#25910;&#25947;&#24615;&#20998;&#26512;&#65292;&#39318;&#27425;&#23637;&#31034;&#20102;&#22312;&#26368;&#23485;&#26494;&#30340;&#20551;&#35774;&#19979;&#30340;&#25910;&#25947;&#24615;&#32467;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;RMSProp&#21644;Adam&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#20998;&#21035;&#20026;$\mathcal O(\epsilon^{-4})$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#22352;&#26631;&#32423;&#21035;&#24191;&#20041;&#20809;&#28369;&#24615;&#21644;&#20223;&#23556;&#22122;&#22768;&#26041;&#24046;&#30340;&#26368;&#23485;&#26494;&#20551;&#35774;&#19979;&#65292;&#20026;&#38750;&#20984;&#20248;&#21270;&#20013;&#30340;RMSProp&#21644;Adam&#25552;&#20379;&#20102;&#39318;&#20010;&#25910;&#25947;&#24615;&#20998;&#26512;&#12290;&#39318;&#20808;&#20998;&#26512;&#20102;RMSProp&#65292;&#23427;&#26159;&#19968;&#31181;&#20855;&#26377;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#20294;&#27809;&#26377;&#19968;&#38454;&#21160;&#37327;&#30340;Adam&#30340;&#29305;&#20363;&#12290;&#20855;&#20307;&#22320;&#65292;&#20026;&#20102;&#35299;&#20915;&#33258;&#36866;&#24212;&#26356;&#26032;&#12289;&#26080;&#30028;&#26799;&#24230;&#20272;&#35745;&#21644;Lipschitz&#24120;&#25968;&#20043;&#38388;&#30340;&#20381;&#36182;&#25361;&#25112;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19979;&#38477;&#24341;&#29702;&#20013;&#30340;&#19968;&#38454;&#39033;&#25910;&#25947;&#65292;&#24182;&#19988;&#20854;&#20998;&#27597;&#30001;&#26799;&#24230;&#33539;&#25968;&#30340;&#20989;&#25968;&#19978;&#30028;&#38480;&#21046;&#12290;&#22522;&#20110;&#36825;&#19968;&#32467;&#26524;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#36866;&#24403;&#30340;&#36229;&#21442;&#25968;&#30340;RMSProp&#25910;&#25947;&#21040;&#19968;&#20010;$\epsilon$-&#31283;&#23450;&#28857;&#65292;&#20854;&#36845;&#20195;&#22797;&#26434;&#24230;&#20026;$\mathcal O(\epsilon^{-4})$&#12290;&#28982;&#21518;&#65292;&#23558;&#25105;&#20204;&#30340;&#20998;&#26512;&#25512;&#24191;&#21040;Adam&#65292;&#39069;&#22806;&#30340;&#25361;&#25112;&#26159;&#30001;&#20110;&#26799;&#24230;&#19982;&#19968;&#38454;&#21160;&#37327;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#19978;&#30028;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01436v1 Announce Type: cross  Abstract: This paper provides the first tight convergence analyses for RMSProp and Adam in non-convex optimization under the most relaxed assumptions of coordinate-wise generalized smoothness and affine noise variance. We first analyze RMSProp, which is a special case of Adam with adaptive learning rates but without first-order momentum. Specifically, to solve the challenges due to dependence among adaptive update, unbounded gradient estimate and Lipschitz constant, we demonstrate that the first-order term in the descent lemma converges and its denominator is upper bounded by a function of gradient norm. Based on this result, we show that RMSProp with proper hyperparameters converges to an $\epsilon$-stationary point with an iteration complexity of $\mathcal O(\epsilon^{-4})$. We then generalize our analysis to Adam, where the additional challenge is due to a mismatch between the gradient and first-order momentum. We develop a new upper bound on
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;LLMs&#30340;&#20301;&#32622;&#20559;&#24046;&#20027;&#35201;&#28304;&#20110;&#19981;&#21516;&#27169;&#22411;&#30340;&#22266;&#26377;&#20301;&#32622;&#20559;&#22909;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#20301;&#32622;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2404.01430</link><description>&lt;p&gt;
&#38477;&#20302;LLMs&#20013;&#20301;&#32622;&#20559;&#24046;&#30340;&#38754;&#21521;&#20301;&#32622;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Position-Aware Parameter Efficient Fine-Tuning Approach for Reducing Positional Bias in LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01430
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;LLMs&#30340;&#20301;&#32622;&#20559;&#24046;&#20027;&#35201;&#28304;&#20110;&#19981;&#21516;&#27169;&#22411;&#30340;&#22266;&#26377;&#20301;&#32622;&#20559;&#22909;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#20301;&#32622;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#22686;&#24378;&#20102;&#23427;&#20204;&#22788;&#29702;&#38271;&#36755;&#20837;&#19978;&#19979;&#25991;&#30340;&#33021;&#21147;&#12290;&#23545;&#20110;&#28041;&#21450;&#20174;&#22806;&#37096;&#25968;&#25454;&#23384;&#20648;&#24211;&#26816;&#32034;&#30693;&#35782;&#30340;&#20219;&#21153;&#65292;&#36825;&#19968;&#36827;&#23637;&#23588;&#20026;&#20851;&#38190;&#65292;&#22240;&#20026;&#21487;&#33021;&#28041;&#21450;&#38271;&#36755;&#20837;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#26174;&#31034;LLMs&#23384;&#22312;&#20301;&#32622;&#20559;&#24046;&#65292;&#34920;&#26126;&#20854;&#24615;&#33021;&#20250;&#26681;&#25454;&#36755;&#20837;&#24207;&#21015;&#20013;&#26377;&#29992;&#20449;&#24687;&#30340;&#20301;&#32622;&#32780;&#21464;&#21270;&#12290;&#26412;&#30740;&#31350;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#20197;&#35843;&#26597;&#20301;&#32622;&#20559;&#24046;&#30340;&#26681;&#26412;&#21407;&#22240;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;LLMs&#30340;&#20301;&#32622;&#20559;&#24046;&#30340;&#20027;&#35201;&#36129;&#29486;&#32773;&#28304;&#20110;&#19981;&#21516;&#27169;&#22411;&#30340;&#22266;&#26377;&#20301;&#32622;&#20559;&#22909;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#20165;&#20165;&#37319;&#29992;&#22522;&#20110;&#25552;&#31034;&#30340;&#35299;&#20915;&#26041;&#26696;&#26080;&#27861;&#20811;&#26381;&#20301;&#32622;&#20559;&#22909;&#12290;&#20026;&#20102;&#35299;&#20915;&#39044;&#35757;&#32451;LLMs&#30340;&#20301;&#32622;&#20559;&#24046;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#38754;&#21521;&#20301;&#32622;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PAPEFT&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21253;&#21547;&#19968;&#20010;&#25968;&#25454;&#22686;&#24191;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01430v1 Announce Type: cross  Abstract: Recent advances in large language models (LLMs) have enhanced their ability to process long input contexts. This development is particularly crucial for tasks that involve retrieving knowledge from an external datastore, which can result in long inputs. However, recent studies show a positional bias in LLMs, demonstrating varying performance depending on the location of useful information within the input sequence. In this study, we conduct extensive experiments to investigate the root causes of positional bias. Our findings indicate that the primary contributor to LLM positional bias stems from the inherent positional preferences of different models. We demonstrate that merely employing prompt-based solutions is inadequate for overcoming the positional preferences. To address this positional bias issue of a pre-trained LLM, we developed a Position-Aware Parameter Efficient Fine-Tuning (PAPEFT) approach which is composed of a data augm
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#25968;&#25454;&#21462;&#20195;&#21644;&#25968;&#25454;&#31215;&#32047;&#20004;&#31181;&#24773;&#20917;&#65292;&#21457;&#29616;&#32047;&#31215;&#25968;&#25454;&#21487;&#20197;&#38450;&#27490;&#27169;&#22411;&#23849;&#28291;&#12290;</title><link>https://arxiv.org/abs/2404.01413</link><description>&lt;p&gt;
&#27169;&#22411;&#23849;&#28291;&#26159;&#21542;&#19981;&#21487;&#36991;&#20813;&#65311;&#36890;&#36807;&#32047;&#31215;&#30495;&#23454;&#21644;&#21512;&#25104;&#25968;&#25454;&#25171;&#30772;&#36882;&#24402;&#30340;&#35781;&#21650;
&lt;/p&gt;
&lt;p&gt;
Is Model Collapse Inevitable? Breaking the Curse of Recursion by Accumulating Real and Synthetic Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01413
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#25968;&#25454;&#21462;&#20195;&#21644;&#25968;&#25454;&#31215;&#32047;&#20004;&#31181;&#24773;&#20917;&#65292;&#21457;&#29616;&#32047;&#31215;&#25968;&#25454;&#21487;&#20197;&#38450;&#27490;&#27169;&#22411;&#23849;&#28291;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29983;&#25104;&#27169;&#22411;&#30340;&#28608;&#22686;&#65292;&#20197;&#21450;&#22312;&#32593;&#32476;&#35268;&#27169;&#25968;&#25454;&#19978;&#30340;&#39044;&#35757;&#32451;&#65292;&#19968;&#20010;&#21450;&#26102;&#30340;&#38382;&#39064;&#28014;&#20986;&#27700;&#38754;&#65306;&#24403;&#36825;&#20123;&#27169;&#22411;&#34987;&#35757;&#32451;&#22312;&#23427;&#20204;&#33258;&#24049;&#29983;&#25104;&#30340;&#36755;&#20986;&#19978;&#26102;&#20250;&#21457;&#29983;&#20160;&#20040;&#65311;&#26368;&#36817;&#23545;&#27169;&#22411;&#25968;&#25454;&#21453;&#39304;&#24490;&#29615;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#36825;&#26679;&#30340;&#24490;&#29615;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#23849;&#28291;&#65292;&#21363;&#24615;&#33021;&#38543;&#30528;&#27599;&#27425;&#27169;&#22411;&#25311;&#21512;&#36845;&#20195;&#36880;&#28176;&#19979;&#38477;&#65292;&#30452;&#21040;&#26368;&#26032;&#30340;&#27169;&#22411;&#21464;&#24471;&#26080;&#29992;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#20960;&#31687;&#30740;&#31350;&#27169;&#22411;&#23849;&#28291;&#30340;&#35770;&#25991;&#37117;&#20551;&#35774;&#38543;&#30528;&#26102;&#38388;&#25512;&#31227;&#65292;&#26032;&#25968;&#25454;&#20250;&#21462;&#20195;&#26087;&#25968;&#25454;&#65292;&#32780;&#19981;&#26159;&#20551;&#35774;&#25968;&#25454;&#20250;&#38543;&#26102;&#38388;&#32047;&#31215;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#36825;&#20004;&#31181;&#24773;&#20917;&#65292;&#24182;&#34920;&#26126;&#31215;&#32047;&#25968;&#25454;&#21487;&#20197;&#38450;&#27490;&#27169;&#22411;&#23849;&#28291;&#12290;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;&#19968;&#20010;&#35299;&#26512;&#21487;&#22788;&#29702;&#30340;&#35774;&#32622;&#65292;&#20854;&#20013;&#19968;&#31995;&#21015;&#32447;&#24615;&#27169;&#22411;&#25311;&#21512;&#21040;&#20808;&#21069;&#27169;&#22411;&#30340;&#39044;&#27979;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#22914;&#26524;&#25968;&#25454;&#34987;&#26367;&#25442;&#65292;&#27979;&#35797;&#35823;&#24046;&#20250;&#38543;&#30528;&#27169;&#22411;&#25311;&#21512;&#36845;&#20195;&#27425;&#25968;&#32447;&#24615;&#22686;&#21152;&#65307;&#25105;&#20204;&#25193;&#23637;&#20102;&#36825;&#20010;&#30740;&#31350;&#25506;&#35752;&#20102;&#25968;&#25454;&#36880;&#28176;&#32047;&#31215;&#30340;&#24773;&#20917;&#19979;&#20250;&#21457;&#29983;&#20160;&#20040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01413v1 Announce Type: cross  Abstract: The proliferation of generative models, combined with pretraining on web-scale data, raises a timely question: what happens when these models are trained on their own generated outputs? Recent investigations into model-data feedback loops discovered that such loops can lead to model collapse, a phenomenon where performance progressively degrades with each model-fitting iteration until the latest model becomes useless. However, several recent papers studying model collapse assumed that new data replace old data over time rather than assuming data accumulate over time. In this paper, we compare these two settings and show that accumulating data prevents model collapse. We begin by studying an analytically tractable setup in which a sequence of linear models are fit to the previous models' predictions. Previous work showed if data are replaced, the test error increases linearly with the number of model-fitting iterations; we extend this r
&lt;/p&gt;</description></item><item><title>&#22312;&#30740;&#31350;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#35268;&#27169;&#29305;&#24615;&#26102;&#21457;&#29616;&#65292;&#36739;&#23567;&#30340;&#27169;&#22411;&#22312;&#30456;&#21516;&#25512;&#29702;&#39044;&#31639;&#19979;&#24448;&#24448;&#27604;&#36739;&#22823;&#30340;&#27169;&#22411;&#26356;&#26377;&#25928;&#22320;&#29983;&#25104;&#39640;&#36136;&#37327;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2404.01367</link><description>&lt;p&gt;
&#22823;&#24182;&#38750;&#24635;&#26159;&#26356;&#22909;&#65306;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#35268;&#27169;&#29305;&#24615;
&lt;/p&gt;
&lt;p&gt;
Bigger is not Always Better: Scaling Properties of Latent Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01367
&lt;/p&gt;
&lt;p&gt;
&#22312;&#30740;&#31350;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#35268;&#27169;&#29305;&#24615;&#26102;&#21457;&#29616;&#65292;&#36739;&#23567;&#30340;&#27169;&#22411;&#22312;&#30456;&#21516;&#25512;&#29702;&#39044;&#31639;&#19979;&#24448;&#24448;&#27604;&#36739;&#22823;&#30340;&#27169;&#22411;&#26356;&#26377;&#25928;&#22320;&#29983;&#25104;&#39640;&#36136;&#37327;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#65288;LDMs&#65289;&#30340;&#35268;&#27169;&#29305;&#24615;&#65292;&#37325;&#28857;&#20851;&#27880;&#23427;&#20204;&#30340;&#37319;&#26679;&#25928;&#29575;&#12290;&#23613;&#31649;&#25913;&#36827;&#30340;&#32593;&#32476;&#26550;&#26500;&#21644;&#25512;&#29702;&#31639;&#27861;&#24050;&#32463;&#35777;&#26126;&#21487;&#20197;&#26377;&#25928;&#25552;&#21319;&#25193;&#25955;&#27169;&#22411;&#30340;&#37319;&#26679;&#25928;&#29575;&#65292;&#20294;&#27169;&#22411;&#22823;&#23567;&#30340;&#20316;&#29992;&#8212;&#8212;&#37319;&#26679;&#25928;&#29575;&#30340;&#20851;&#38190;&#20915;&#23450;&#22240;&#32032;&#8212;&#8212;&#23578;&#26410;&#21463;&#21040;&#24443;&#24213;&#30340;&#23457;&#26597;&#12290;&#36890;&#36807;&#23545;&#24050;&#24314;&#31435;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#23454;&#35777;&#20998;&#26512;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#65292;&#25506;&#35752;&#20102;&#27169;&#22411;&#22823;&#23567;&#22914;&#20309;&#24433;&#21709;&#22312;&#19981;&#21516;&#37319;&#26679;&#27493;&#39588;&#19979;&#30340;&#37319;&#26679;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25581;&#31034;&#20102;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#30340;&#36235;&#21183;&#65306;&#22312;&#32473;&#23450;&#25512;&#29702;&#39044;&#31639;&#19979;&#36816;&#34892;&#26102;&#65292;&#36739;&#23567;&#30340;&#27169;&#22411;&#32463;&#24120;&#32988;&#36807;&#20854;&#36739;&#22823;&#30340;&#31561;&#20215;&#29289;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#32467;&#26524;&#19978;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#30740;&#31350;&#65292;&#36890;&#36807;&#24212;&#29992;&#21508;&#31181;&#25193;&#25955;&#37319;&#26679;&#22120;&#65292;&#25506;&#32034;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#65292;&#35780;&#20272;&#21518;&#31934;&#39311;&#27169;&#22411;&#65292;&#20197;&#21450;&#36827;&#34892;&#27604;&#36739;&#65292;&#26469;&#23637;&#31034;&#36825;&#20123;&#21457;&#29616;&#30340;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01367v1 Announce Type: cross  Abstract: We study the scaling properties of latent diffusion models (LDMs) with an emphasis on their sampling efficiency. While improved network architecture and inference algorithms have shown to effectively boost sampling efficiency of diffusion models, the role of model size -- a critical determinant of sampling efficiency -- has not been thoroughly examined. Through empirical analysis of established text-to-image diffusion models, we conduct an in-depth investigation into how model size influences sampling efficiency across varying sampling steps. Our findings unveil a surprising trend: when operating under a given inference budget, smaller models frequently outperform their larger equivalents in generating high-quality results. Moreover, we extend our study to demonstrate the generalizability of the these findings by applying various diffusion samplers, exploring diverse downstream tasks, evaluating post-distilled models, as well as compar
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GRIFFIN&#30340;&#35757;&#32451;-free MoE&#65292;&#33021;&#22815;&#22312;&#21508;&#31181;LLM&#27169;&#22411;&#20013;&#36873;&#25321;&#21807;&#19968;&#30340;FF&#19987;&#23478;&#20197;&#23454;&#29616;&#39640;&#25928;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2404.01365</link><description>&lt;p&gt;
&#22522;&#20110;&#25552;&#31034;&#30340;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#29992;&#20110;&#39640;&#25928;&#29983;&#25104;LLM
&lt;/p&gt;
&lt;p&gt;
Prompt-prompted Mixture of Experts for Efficient LLM Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01365
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GRIFFIN&#30340;&#35757;&#32451;-free MoE&#65292;&#33021;&#22815;&#22312;&#21508;&#31181;LLM&#27169;&#22411;&#20013;&#36873;&#25321;&#21807;&#19968;&#30340;FF&#19987;&#23478;&#20197;&#23454;&#29616;&#39640;&#25928;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22522;&#20110;transformer&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#65292;&#30001;&#20110;&#20854;&#20986;&#33394;&#30340;&#23454;&#29992;&#24615;&#65292;&#23427;&#20204;&#24050;&#34987;&#24212;&#29992;&#20110;&#35768;&#22810;&#39046;&#22495;&#65292;&#20294;&#22312;&#37096;&#32626;&#26102;&#23384;&#22312;&#30456;&#24403;&#22823;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#19968;&#20123;&#26041;&#27861;&#65292;&#22914;&#20462;&#21098;&#25110;&#26500;&#24314;&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;&#65292;&#26088;&#22312;&#21033;&#29992;transformer&#21069;&#39304;&#65288;FF&#65289;&#22359;&#20013;&#30340;&#31232;&#30095;&#24615;&#65292;&#20197;&#25552;&#39640;&#36895;&#24230;&#24182;&#38477;&#20302;&#20869;&#23384;&#38656;&#27714;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#25216;&#26415;&#22312;&#23454;&#36341;&#20013;&#21487;&#33021;&#38750;&#24120;&#26114;&#36149;&#21644;&#19981;&#28789;&#27963;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#24120;&#38656;&#35201;&#35757;&#32451;&#25110;&#20165;&#38480;&#20110;&#29305;&#23450;&#31867;&#22411;&#30340;&#26550;&#26500;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;GRIFFIN&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;MoE&#65292;&#23427;&#22312;&#24207;&#21015;&#32423;&#21035;&#20026;&#19981;&#21516;&#38750;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#22823;&#37327;LLMs&#36873;&#25321;&#29420;&#29305;&#30340;FF&#19987;&#23478;&#20197;&#23454;&#29616;&#39640;&#25928;&#29983;&#25104;&#12290;&#36825;&#26159;&#21487;&#33021;&#30340;&#65292;&#22240;&#20026;&#25105;&#20204;&#20851;&#38190;&#35266;&#23519;&#21040;&#65292;&#35768;&#22810;&#32463;&#36807;&#35757;&#32451;&#30340;LLMs&#22312;&#24207;&#21015;&#20013;&#33258;&#28982;&#20135;&#29983;&#39640;&#24230;&#32467;&#26500;&#21270;&#30340;FF&#28608;&#27963;&#27169;&#24335;&#65292;&#36825;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01365v1 Announce Type: cross  Abstract: With the development of transformer-based large language models (LLMs), they have been applied to many fields due to their remarkable utility, but this comes at a considerable computational cost at deployment. Fortunately, some methods such as pruning or constructing a mixture of experts (MoE) aim at exploiting sparsity in transformer feedforward (FF) blocks to gain boosts in speed and reduction in memory requirements. However, these techniques can be very costly and inflexible in practice, as they often require training or are restricted to specific types of architectures. To address this, we introduce GRIFFIN, a novel training-free MoE that selects unique FF experts at the sequence level for efficient generation across a plethora of LLMs with different non-ReLU activation functions. This is possible due to a critical observation that many trained LLMs naturally produce highly structured FF activation patterns within a sequence, which
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36716;&#31227;&#29109;&#36827;&#34892;&#20449;&#24687;&#24179;&#38754;&#20998;&#26512;&#21487;&#35270;&#21270;&#25581;&#31034;&#20102;&#20449;&#24687;&#29942;&#39048;&#26041;&#27861;&#20013;&#21387;&#32553;&#21644;&#20449;&#24687;&#20445;&#30041;&#30340;&#26435;&#34913;&#65292;&#20197;&#21450;&#20449;&#24687;&#35770;&#21387;&#32553;&#19982;&#27867;&#21270;&#20043;&#38388;&#23384;&#22312;&#30340;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2404.01364</link><description>&lt;p&gt;
&#36890;&#36807;&#36716;&#31227;&#29109;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#20449;&#24687;&#24179;&#38754;&#20998;&#26512;&#21487;&#35270;&#21270;
&lt;/p&gt;
&lt;p&gt;
Information Plane Analysis Visualization in Deep Learning via Transfer Entropy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01364
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36716;&#31227;&#29109;&#36827;&#34892;&#20449;&#24687;&#24179;&#38754;&#20998;&#26512;&#21487;&#35270;&#21270;&#25581;&#31034;&#20102;&#20449;&#24687;&#29942;&#39048;&#26041;&#27861;&#20013;&#21387;&#32553;&#21644;&#20449;&#24687;&#20445;&#30041;&#30340;&#26435;&#34913;&#65292;&#20197;&#21450;&#20449;&#24687;&#35770;&#21387;&#32553;&#19982;&#27867;&#21270;&#20043;&#38388;&#23384;&#22312;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21069;&#39304;&#32593;&#32476;&#20013;&#65292;&#21487;&#20197;&#21033;&#29992;&#36716;&#31227;&#29109;&#65288;TE&#65289;&#26469;&#34913;&#37327;&#19968;&#23618;&#23545;&#21478;&#19968;&#23618;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#37327;&#21270;&#23427;&#20204;&#20043;&#38388;&#22312;&#35757;&#32451;&#26399;&#38388;&#30340;&#20449;&#24687;&#20256;&#36882;&#12290;&#26681;&#25454;&#20449;&#24687;&#29942;&#39048;&#21407;&#29702;&#65292;&#31070;&#32463;&#27169;&#22411;&#30340;&#20869;&#37096;&#34920;&#31034;&#24212;&#23613;&#21487;&#33021;&#21387;&#32553;&#36755;&#20837;&#25968;&#25454;&#65292;&#21516;&#26102;&#20173;&#20445;&#30041;&#36275;&#22815;&#30340;&#20851;&#20110;&#36755;&#20986;&#30340;&#20449;&#24687;&#12290;&#20449;&#24687;&#24179;&#38754;&#20998;&#26512;&#26159;&#19968;&#31181;&#21487;&#35270;&#21270;&#25216;&#26415;&#65292;&#29992;&#20110;&#36890;&#36807;&#32472;&#21046;&#36755;&#20837;&#25968;&#25454;&#20013;&#30340;&#20449;&#24687;&#37327;&#19982;&#21387;&#32553;&#34920;&#31034;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#26469;&#29702;&#35299;&#20449;&#24687;&#29942;&#39048;&#26041;&#27861;&#20013;&#21387;&#32553;&#21644;&#20449;&#24687;&#20445;&#30041;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#22768;&#31216;&#20449;&#24687;&#35770;&#21387;&#32553;&#21644;&#27867;&#21270;&#20043;&#38388;&#23384;&#22312;&#22240;&#26524;&#32852;&#31995;&#65292;&#36890;&#36807;&#20114;&#20449;&#24687;&#26469;&#34913;&#37327;&#26159;&#21487;&#20449;&#30340;&#65292;&#20294;&#19981;&#21516;&#30740;&#31350;&#30340;&#32467;&#26524;&#23384;&#22312;&#20914;&#31361;&#12290;TE&#21487;&#20197;&#25429;&#25417;&#26102;&#38388;&#20851;&#31995;&#65292;&#19982;&#20114;&#20449;&#24687;&#30456;&#27604;&#65292;&#23384;&#22312;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01364v1 Announce Type: cross  Abstract: In a feedforward network, Transfer Entropy (TE) can be used to measure the influence that one layer has on another by quantifying the information transfer between them during training. According to the Information Bottleneck principle, a neural model's internal representation should compress the input data as much as possible while still retaining sufficient information about the output. Information Plane analysis is a visualization technique used to understand the trade-off between compression and information preservation in the context of the Information Bottleneck method by plotting the amount of information in the input data against the compressed representation. The claim that there is a causal link between information-theoretic compression and generalization, measured by mutual information, is plausible, but results from different studies are conflicting. In contrast to mutual information, TE can capture temporal relationships be
&lt;/p&gt;</description></item><item><title>LLM Attributor&#26159;&#19968;&#20010;Python&#24211;&#65292;&#25552;&#20379;&#20102;&#20132;&#20114;&#24335;&#21487;&#35270;&#21270;&#26041;&#24335;&#29992;&#20110;&#23558;LLM&#30340;&#25991;&#26412;&#29983;&#25104;&#32467;&#26524;&#24402;&#22240;&#21040;&#35757;&#32451;&#25968;&#25454;&#28857;&#65292;&#24110;&#21161;&#29992;&#25143;&#26816;&#26597;&#27169;&#22411;&#34892;&#20026;&#12289;&#22686;&#24378;&#21487;&#20449;&#24230;&#65292;&#24182;&#19982;&#29992;&#25143;&#25552;&#20379;&#30340;&#25991;&#26412;&#36827;&#34892;&#27604;&#36739;&#12290;</title><link>https://arxiv.org/abs/2404.01361</link><description>&lt;p&gt;
LLM Attributor: &#20132;&#20114;&#24335;&#21487;&#35270;&#21270;&#24402;&#22240;&#29992;&#20110;LLM&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
LLM Attributor: Interactive Visual Attribution for LLM Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01361
&lt;/p&gt;
&lt;p&gt;
LLM Attributor&#26159;&#19968;&#20010;Python&#24211;&#65292;&#25552;&#20379;&#20102;&#20132;&#20114;&#24335;&#21487;&#35270;&#21270;&#26041;&#24335;&#29992;&#20110;&#23558;LLM&#30340;&#25991;&#26412;&#29983;&#25104;&#32467;&#26524;&#24402;&#22240;&#21040;&#35757;&#32451;&#25968;&#25454;&#28857;&#65292;&#24110;&#21161;&#29992;&#25143;&#26816;&#26597;&#27169;&#22411;&#34892;&#20026;&#12289;&#22686;&#24378;&#21487;&#20449;&#24230;&#65292;&#24182;&#19982;&#29992;&#25143;&#25552;&#20379;&#30340;&#25991;&#26412;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26174;&#31034;&#20986;&#22312;&#21508;&#20010;&#39046;&#22495;&#29983;&#25104;&#20196;&#20154;&#20449;&#26381;&#30340;&#25991;&#26412;&#30340;&#33021;&#21147;&#65292;&#20294;&#23545;&#20854;&#28508;&#22312;&#39118;&#38505;&#30340;&#25285;&#24551;&#20984;&#26174;&#20102;&#20102;&#35299;&#25991;&#26412;&#29983;&#25104;&#32972;&#21518;&#21407;&#22240;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;LLM Attributor&#65292;&#19968;&#20010;&#25552;&#20379;LLM&#25991;&#26412;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#24402;&#22240;&#20132;&#20114;&#21487;&#35270;&#21270;&#30340;Python&#24211;&#12290;&#25105;&#20204;&#30340;&#24211;&#20026;&#24555;&#36895;&#23558;LLM&#30340;&#25991;&#26412;&#29983;&#25104;&#24402;&#22240;&#21040;&#35757;&#32451;&#25968;&#25454;&#28857;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#26041;&#24335;&#65292;&#20197;&#26816;&#26597;&#27169;&#22411;&#34892;&#20026;&#12289;&#22686;&#24378;&#20854;&#21487;&#20449;&#24230;&#65292;&#24182;&#23558;&#27169;&#22411;&#29983;&#25104;&#30340;&#25991;&#26412;&#19982;&#29992;&#25143;&#25552;&#20379;&#30340;&#25991;&#26412;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#24037;&#20855;&#30340;&#35270;&#35273;&#21644;&#20132;&#20114;&#35774;&#35745;&#65292;&#24182;&#24378;&#35843;LLaMA2&#27169;&#22411;&#30340;&#20351;&#29992;&#22330;&#26223;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#20004;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#65306;&#20851;&#20110;&#26368;&#36817;&#28798;&#38590;&#21644;&#37329;&#34701;&#30456;&#20851;&#38382;&#31572;&#23545;&#30340;&#22312;&#32447;&#25991;&#31456;&#12290;&#30001;&#20110;LLM Attributor&#23545;&#35745;&#31639;&#31508;&#35760;&#26412;&#30340;&#24191;&#27867;&#25903;&#25345;&#65292;&#29992;&#25143;&#21487;&#20197;&#36731;&#26494;&#23558;&#20854;&#25972;&#21512;&#21040;&#20182;&#20204;&#30340;&#24037;&#20316;&#27969;&#31243;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01361v1 Announce Type: cross  Abstract: While large language models (LLMs) have shown remarkable capability to generate convincing text across diverse domains, concerns around its potential risks have highlighted the importance of understanding the rationale behind text generation. We present LLM Attributor, a Python library that provides interactive visualizations for training data attribution of an LLM's text generation. Our library offers a new way to quickly attribute an LLM's text generation to training data points to inspect model behaviors, enhance its trustworthiness, and compare model-generated text with user-provided text. We describe the visual and interactive design of our tool and highlight usage scenarios for LLaMA2 models fine-tuned with two different datasets: online articles about recent disasters and finance-related question-answer pairs. Thanks to LLM Attributor's broad support for computational notebooks, users can easily integrate it into their workflow 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20840;&#38754;&#27604;&#36739;&#20102;&#25968;&#25454;&#39537;&#21160;&#21644;&#29289;&#29702;&#39537;&#21160;&#20004;&#31181;&#28145;&#24230;&#23398;&#20064;&#30456;&#20301;&#24674;&#22797;&#31574;&#30053;&#65292;&#22312;&#26102;&#38388;&#28040;&#32791;&#12289;&#20934;&#30830;&#24615;&#12289;&#27867;&#21270;&#33021;&#21147;&#12289;&#36866;&#24212;&#30149;&#24577;&#38382;&#39064;&#21644;&#20808;&#39564;&#33021;&#21147;&#31561;&#26041;&#38754;&#30340;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2404.01360</link><description>&lt;p&gt;
&#21033;&#29992;&#25968;&#25454;&#21644;&#29289;&#29702;&#23398;&#36827;&#34892;&#28145;&#24230;&#23398;&#20064;&#30456;&#20301;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
Harnessing Data and Physics for Deep Learning Phase Recovery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01360
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20840;&#38754;&#27604;&#36739;&#20102;&#25968;&#25454;&#39537;&#21160;&#21644;&#29289;&#29702;&#39537;&#21160;&#20004;&#31181;&#28145;&#24230;&#23398;&#20064;&#30456;&#20301;&#24674;&#22797;&#31574;&#30053;&#65292;&#22312;&#26102;&#38388;&#28040;&#32791;&#12289;&#20934;&#30830;&#24615;&#12289;&#27867;&#21270;&#33021;&#21147;&#12289;&#36866;&#24212;&#30149;&#24577;&#38382;&#39064;&#21644;&#20808;&#39564;&#33021;&#21147;&#31561;&#26041;&#38754;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#20301;&#24674;&#22797;&#26159;&#20174;&#20809;&#24378;&#24230;&#27979;&#37327;&#20013;&#35745;&#31639;&#20809;&#27874;&#30340;&#30456;&#20301;&#65292;&#23545;&#20110;&#21508;&#31181;&#24212;&#29992;&#38750;&#24120;&#37325;&#35201;&#65292;&#20363;&#22914;&#30456;&#24178;&#34893;&#23556;&#25104;&#20687;&#12289;&#33258;&#36866;&#24212;&#20809;&#23398;&#21644;&#29983;&#29289;&#21307;&#23398;&#25104;&#20687;&#12290;&#28145;&#24230;&#23398;&#20064;&#22312;&#35299;&#20915;&#30456;&#20301;&#24674;&#22797;&#38382;&#39064;&#26041;&#38754;&#34987;&#35777;&#26126;&#38750;&#24120;&#26377;&#25928;&#12290;&#20004;&#31181;&#20027;&#35201;&#30340;&#28145;&#24230;&#23398;&#20064;&#30456;&#20301;&#24674;&#22797;&#31574;&#30053;&#20998;&#21035;&#20026;&#25968;&#25454;&#39537;&#21160;&#65288;DD&#65289;&#19982;&#30417;&#30563;&#23398;&#20064;&#27169;&#24335;&#20197;&#21450;&#29289;&#29702;&#39537;&#21160;&#65288;PD&#65289;&#19982;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#24335;&#12290;DD&#21644;PD&#20197;&#19981;&#21516;&#26041;&#24335;&#23454;&#29616;&#30456;&#21516;&#30446;&#26631;&#65292;&#24182;&#32570;&#20047;&#24517;&#35201;&#30340;&#30740;&#31350;&#26469;&#25581;&#31034;&#23427;&#20204;&#30340;&#30456;&#20284;&#24615;&#21644;&#24046;&#24322;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#27604;&#36739;&#20102;&#36825;&#20004;&#31181;&#28145;&#24230;&#23398;&#20064;&#30456;&#20301;&#24674;&#22797;&#31574;&#30053;&#22312;&#26102;&#38388;&#28040;&#32791;&#12289;&#20934;&#30830;&#24615;&#12289;&#27867;&#21270;&#33021;&#21147;&#12289;&#36866;&#24212;&#30149;&#24577;&#38382;&#39064;&#21644;&#20808;&#39564;&#33021;&#21147;&#26041;&#38754;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01360v1 Announce Type: cross  Abstract: Phase recovery, calculating the phase of a light wave from its intensity measurements, is essential for various applications, such as coherent diffraction imaging, adaptive optics, and biomedical imaging. It enables the reconstruction of an object's refractive index distribution or topography as well as the correction of imaging system aberrations. In recent years, deep learning has been proven to be highly effective in addressing phase recovery problems. Two main deep learning phase recovery strategies are data-driven (DD) with supervised learning mode and physics-driven (PD) with self-supervised learning mode. DD and PD achieve the same goal in different ways and lack the necessary study to reveal similarities and differences. Therefore, in this paper, we comprehensively compare these two deep learning phase recovery strategies in terms of time consumption, accuracy, generalization ability, ill-posedness adaptability, and prior capac
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#31038;&#20132;&#23186;&#20307;&#20998;&#26512;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#25968;&#23383;&#20581;&#24247;&#26041;&#27861;&#65292;&#25104;&#21151;&#26816;&#27979;&#20986;&#19982;GLP-1&#21463;&#20307;&#28608;&#21160;&#21058;&#30456;&#20851;&#30340;21&#31181;&#28508;&#22312;&#19981;&#33391;&#21103;&#20316;&#29992;&#65292;&#21253;&#25324;&#26131;&#24594;&#21644;&#40635;&#26408;&#24863;&#65292;&#20174;&#32780;&#38761;&#26032;&#20102;&#23545;&#26032;&#37096;&#32626;&#33647;&#29289;&#26410;&#25253;&#21578;ASEs&#30340;&#26816;&#27979;&#12290;</title><link>https://arxiv.org/abs/2404.01358</link><description>&lt;p&gt;
&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#21644;&#31038;&#20132;&#23186;&#20307;&#20998;&#26512;&#21457;&#29616;GLP-1&#21463;&#20307;&#28608;&#21160;&#21058;&#30340;&#19981;&#33391;&#21103;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Utilizing AI and Social Media Analytics to Discover Adverse Side Effects of GLP-1 Receptor Agonists
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01358
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#31038;&#20132;&#23186;&#20307;&#20998;&#26512;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#25968;&#23383;&#20581;&#24247;&#26041;&#27861;&#65292;&#25104;&#21151;&#26816;&#27979;&#20986;&#19982;GLP-1&#21463;&#20307;&#28608;&#21160;&#21058;&#30456;&#20851;&#30340;21&#31181;&#28508;&#22312;&#19981;&#33391;&#21103;&#20316;&#29992;&#65292;&#21253;&#25324;&#26131;&#24594;&#21644;&#40635;&#26408;&#24863;&#65292;&#20174;&#32780;&#38761;&#26032;&#20102;&#23545;&#26032;&#37096;&#32626;&#33647;&#29289;&#26410;&#25253;&#21578;ASEs&#30340;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;&#30340;&#19981;&#33391;&#21103;&#20316;&#29992;&#65288;ASEs&#65289;&#22312;FDA&#25209;&#20934;&#21518;&#34987;&#21457;&#29616;&#65292;&#23545;&#24739;&#32773;&#23433;&#20840;&#26500;&#25104;&#23041;&#32961;&#12290;&#20026;&#20102;&#21450;&#26102;&#21457;&#29616;&#34987;&#24573;&#35270;&#30340;ASEs&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#25968;&#23383;&#20581;&#24247;&#26041;&#27861;&#65292;&#33021;&#22815;&#20998;&#26512;&#26469;&#33258;&#31038;&#20132;&#23186;&#20307;&#12289;&#24050;&#21457;&#34920;&#30340;&#20020;&#24202;&#30740;&#31350;&#12289;&#21046;&#36896;&#21830;&#25253;&#21578;&#21644;ChatGPT&#31561;&#22823;&#37327;&#20844;&#24320;&#25968;&#25454;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#19982;&#32925;&#32032;&#26679;&#32957;1&#21463;&#20307;&#28608;&#21160;&#21058;&#65288;GLP-1 RA&#65289;&#30456;&#20851;&#30340;ASEs&#65292;&#36825;&#19968;&#24066;&#22330;&#39044;&#35745;&#21040;2030&#24180;&#23558;&#21576;&#25351;&#25968;&#22686;&#38271;&#33267;1335&#20159;&#32654;&#20803;&#12290;&#21033;&#29992;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25104;&#21151;&#26816;&#27979;&#20986;FDA&#25209;&#20934;&#26102;&#34987;&#24573;&#35270;&#30340;21&#31181;&#28508;&#22312;ASEs&#65292;&#21253;&#25324;&#26131;&#24594;&#21644;&#40635;&#26408;&#24863;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#20998;&#26512;&#26041;&#27861;&#24443;&#24213;&#25913;&#21464;&#20102;&#23545;&#26032;&#37096;&#32626;&#33647;&#29289;&#30456;&#20851;&#26410;&#25253;&#21578;&#30340;ASEs&#30340;&#26816;&#27979;&#65292;&#21033;&#29992;&#21069;&#27839;&#30340;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#31038;&#20132;&#23186;&#20307;&#20998;&#26512;&#12290;&#23427;&#21487;&#20197;&#36890;&#36807;&#37322;&#25918;&#31038;&#20132;&#23186;&#20307;&#30340;&#21147;&#37327;&#26469;&#25903;&#25345;&#30417;&#31649;&#26426;&#26500;&#21644;&#21046;&#36896;&#21830;&#22312;&#24066;&#22330;&#19978;&#22686;&#21152;&#26032;&#33647;&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01358v1 Announce Type: cross  Abstract: Adverse side effects (ASEs) of drugs, revealed after FDA approval, pose a threat to patient safety. To promptly detect overlooked ASEs, we developed a digital health methodology capable of analyzing massive public data from social media, published clinical research, manufacturers' reports, and ChatGPT. We uncovered ASEs associated with the glucagon-like peptide 1 receptor agonists (GLP-1 RA), a market expected to grow exponentially to $133.5 billion USD by 2030. Using a Named Entity Recognition (NER) model, our method successfully detected 21 potential ASEs overlooked upon FDA approval, including irritability and numbness. Our data-analytic approach revolutionizes the detection of unreported ASEs associated with newly deployed drugs, leveraging cutting-edge AI-driven social media analytics. It can increase the safety of new drugs in the marketplace by unlocking the power of social media to support regulators and manufacturers in the ra
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#25932;&#23545;&#36755;&#20837;&#25200;&#21160;&#30340;&#25935;&#24863;&#24615;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#40065;&#26834;&#20934;&#30830;&#20844;&#24179;&#24615;&#23450;&#20041;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#25932;&#23545;&#25915;&#20987;&#26041;&#27861;&#21644;&#30456;&#24212;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2404.01356</link><description>&lt;p&gt;
&#36755;&#20837;&#25200;&#21160;&#23545;&#40065;&#26834;&#20934;&#30830;&#20844;&#24179;&#24615;&#30340;&#21452;&#20995;&#21073;
&lt;/p&gt;
&lt;p&gt;
The Double-Edged Sword of Input Perturbations to Robust Accurate Fairness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01356
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#25932;&#23545;&#36755;&#20837;&#25200;&#21160;&#30340;&#25935;&#24863;&#24615;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#40065;&#26834;&#20934;&#30830;&#20844;&#24179;&#24615;&#23450;&#20041;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#25932;&#23545;&#25915;&#20987;&#26041;&#27861;&#21644;&#30456;&#24212;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#34987;&#35748;&#20026;&#23545;&#25932;&#23545;&#36755;&#20837;&#25200;&#21160;&#25935;&#24863;&#65292;&#23548;&#33268;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#25110;&#20010;&#20307;&#20844;&#24179;&#24615;&#38477;&#20302;&#12290;&#20026;&#20102;&#20849;&#21516;&#34920;&#24449;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#20010;&#20307;&#20844;&#24179;&#24615;&#23545;&#25932;&#23545;&#25200;&#21160;&#30340;&#25935;&#24863;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;&#40065;&#26834;&#20934;&#30830;&#20844;&#24179;&#24615;&#30340;&#26032;&#23450;&#20041;&#12290;&#40065;&#26834;&#20934;&#30830;&#20844;&#24179;&#24615;&#35201;&#27714;&#24403;&#23454;&#20363;&#21450;&#20854;&#30456;&#20284;&#23545;&#24212;&#29289;&#21463;&#21040;&#36755;&#20837;&#25200;&#21160;&#26102;&#65292;&#39044;&#27979;&#19982;&#22320;&#38754;&#20107;&#23454;&#19968;&#33268;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#25932;&#23545;&#25915;&#20987;&#26041;&#27861;RAFair&#65292;&#20197;&#26292;&#38706;DNN&#20013;&#30340;&#34394;&#20551;&#25110;&#20559;&#35265;&#25932;&#23545;&#32570;&#38519;&#65292;&#36825;&#20123;&#32570;&#38519;&#20250;&#27450;&#39575;&#20934;&#30830;&#24615;&#25110;&#25439;&#23475;&#20010;&#20307;&#20844;&#24179;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#36825;&#26679;&#30340;&#25932;&#23545;&#23454;&#20363;&#21487;&#20197;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#33391;&#24615;&#25200;&#21160;&#26377;&#25928;&#22320;&#35299;&#20915;&#65292;&#20174;&#32780;&#20351;&#23427;&#20204;&#30340;&#39044;&#27979;&#20934;&#30830;&#32780;&#20844;&#24179;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25506;&#35752;&#20102;&#36755;&#20837;&#23545;&#20934;&#30830;&#20844;&#24179;&#24615;&#30340;&#21452;&#20995;&#21073;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01356v1 Announce Type: cross  Abstract: Deep neural networks (DNNs) are known to be sensitive to adversarial input perturbations, leading to a reduction in either prediction accuracy or individual fairness. To jointly characterize the susceptibility of prediction accuracy and individual fairness to adversarial perturbations, we introduce a novel robustness definition termed robust accurate fairness. Informally, robust accurate fairness requires that predictions for an instance and its similar counterparts consistently align with the ground truth when subjected to input perturbations. We propose an adversarial attack approach dubbed RAFair to expose false or biased adversarial defects in DNN, which either deceive accuracy or compromise individual fairness. Then, we show that such adversarial instances can be effectively addressed by carefully designed benign perturbations, correcting their predictions to be accurate and fair. Our work explores the double-edged sword of input 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MLFS&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#21442;&#25968;&#30340;&#36229;&#32593;&#32476;&#35757;&#32451;&#65292;&#21487;&#20197;&#33719;&#24471;&#36866;&#29992;&#20110;&#21830;&#19994;&#36793;&#32536;&#24212;&#29992;&#30340;&#39640;&#36136;&#37327;&#32534;&#30721;&#22120;&#27169;&#22411;&#65292;&#24182;&#26377;&#25928;&#22320;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#12290;</title><link>https://arxiv.org/abs/2404.01353</link><description>&lt;p&gt;
&#29992;&#20110;&#36793;&#32536;&#24212;&#29992;&#30340;LLM&#39640;&#25928;&#25552;&#21462;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficiently Distilling LLMs for Edge Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01353
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MLFS&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#21442;&#25968;&#30340;&#36229;&#32593;&#32476;&#35757;&#32451;&#65292;&#21487;&#20197;&#33719;&#24471;&#36866;&#29992;&#20110;&#21830;&#19994;&#36793;&#32536;&#24212;&#29992;&#30340;&#39640;&#36136;&#37327;&#32534;&#30721;&#22120;&#27169;&#22411;&#65292;&#24182;&#26377;&#25928;&#22320;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24037;&#19994;&#24212;&#29992;&#20013;&#65292;LLMs&#30340;&#36229;&#32593;&#32476;&#35757;&#32451;&#20855;&#26377;&#24456;&#22823;&#30340;&#37325;&#35201;&#24615;&#65292;&#22240;&#20026;&#23427;&#36171;&#20104;&#20102;&#20197;&#22266;&#23450;&#25104;&#26412;&#20135;&#29983;&#19981;&#21516;&#22823;&#23567;/&#24310;&#36831;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MLFS&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#21442;&#25968;&#30340;&#36229;&#32593;&#32476;&#35757;&#32451;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#20197;&#33719;&#24471;&#36866;&#29992;&#20110;&#21830;&#19994;&#36793;&#32536;&#24212;&#29992;&#30340;&#39640;&#36136;&#37327;&#32534;&#30721;&#22120;&#27169;&#22411;&#65292;&#24182;&#19988;&#34429;&#28982;&#20165;&#35299;&#30721;&#22120;&#27169;&#22411;&#23545;&#21387;&#32553;&#20855;&#26377;&#30456;&#24403;&#30340;&#25269;&#25239;&#21147;&#65292;&#20294;&#21487;&#20197;&#26377;&#25928;&#22320;&#23545;&#35299;&#30721;&#22120;&#36827;&#34892;&#20999;&#29255;&#20197;&#22823;&#24133;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01353v1 Announce Type: cross  Abstract: Supernet training of LLMs is of great interest in industrial applications as it confers the ability to produce a palette of smaller models at constant cost, regardless of the number of models (of different size / latency) produced. We propose a new method called Multistage Low-rank Fine-tuning of Super-transformers (MLFS) for parameter-efficient supernet training. We show that it is possible to obtain high-quality encoder models that are suitable for commercial edge applications, and that while decoder-only models are resistant to a comparable degree of compression, decoders can be effectively sliced for a significant reduction in training time.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;AETTA&#65292;&#19968;&#31181;&#29992;&#20110;&#27979;&#35797;&#26102;&#36866;&#24212;&#30340;&#26080;&#26631;&#31614;&#20934;&#30830;&#24615;&#20272;&#35745;&#31639;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#19981;&#19968;&#33268;&#24615;&#26469;&#25913;&#36827;&#20934;&#30830;&#24615;&#20272;&#35745;&#24182;&#22312;&#36866;&#24212;&#22833;&#36133;&#24773;&#20917;&#19979;&#23637;&#29616;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2404.01351</link><description>&lt;p&gt;
AETTA: &#26080;&#26631;&#31614;&#20934;&#30830;&#24615;&#20272;&#35745;&#29992;&#20110;&#27979;&#35797;&#26102;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
AETTA: Label-Free Accuracy Estimation for Test-Time Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01351
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;AETTA&#65292;&#19968;&#31181;&#29992;&#20110;&#27979;&#35797;&#26102;&#36866;&#24212;&#30340;&#26080;&#26631;&#31614;&#20934;&#30830;&#24615;&#20272;&#35745;&#31639;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#19981;&#19968;&#33268;&#24615;&#26469;&#25913;&#36827;&#20934;&#30830;&#24615;&#20272;&#35745;&#24182;&#22312;&#36866;&#24212;&#22833;&#36133;&#24773;&#20917;&#19979;&#23637;&#29616;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27979;&#35797;&#26102;&#36866;&#24212;&#65288;TTA&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#26080;&#26631;&#31614;&#27979;&#35797;&#25968;&#25454;&#26469;&#20351;&#39044;&#35757;&#32451;&#27169;&#22411;&#36866;&#24212;&#39046;&#22495;&#20559;&#31227;&#12290;&#28982;&#32780;&#65292;TTA&#38754;&#20020;&#30528;&#36866;&#24212;&#22833;&#36133;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#22312;&#21160;&#24577;&#22330;&#26223;&#20013;&#23427;&#20381;&#36182;&#20110;&#23545;&#26410;&#30693;&#27979;&#35797;&#26679;&#26412;&#30340;&#30450;&#30446;&#36866;&#24212;&#12290;&#20256;&#32479;&#30340;&#29992;&#20110;&#35780;&#20272;&#26816;&#27979;&#24615;&#33021;&#30340;&#26041;&#27861;&#22312;TTA&#32972;&#26223;&#19979;&#21463;&#21040;&#38480;&#21046;&#65292;&#22240;&#20026;&#23427;&#20204;&#23545;&#20110;&#26080;&#26631;&#31614;&#25968;&#25454;&#25110;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#26377;&#30528;&#19981;&#20999;&#23454;&#38469;&#30340;&#20551;&#35774;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AETTA&#65292;&#19968;&#20010;&#29992;&#20110;TTA&#30340;&#26080;&#26631;&#31614;&#20934;&#30830;&#24615;&#20272;&#35745;&#31639;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#39044;&#27979;&#19981;&#19968;&#33268;&#24615;&#20316;&#20026;&#20934;&#30830;&#24615;&#20272;&#35745;&#65292;&#36890;&#36807;&#27604;&#36739;&#30446;&#26631;&#27169;&#22411;&#39044;&#27979;&#21644;dropout&#25512;&#26029;&#26469;&#35745;&#31639;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#39044;&#27979;&#19981;&#19968;&#33268;&#24615;&#20197;&#25193;&#23637;AETTA&#22312;&#36866;&#24212;&#22833;&#36133;&#19979;&#30340;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#23545;&#22235;&#20010;&#22522;&#32447;&#21644;&#20845;&#31181;TTA&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;AETTA&#30456;&#27604;&#20110;&#20854;&#20182;&#26041;&#27861;&#24179;&#22343;&#25552;&#39640;&#20102;19.8%&#30340;&#20934;&#30830;&#24615;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01351v1 Announce Type: cross  Abstract: Test-time adaptation (TTA) has emerged as a viable solution to adapt pre-trained models to domain shifts using unlabeled test data. However, TTA faces challenges of adaptation failures due to its reliance on blind adaptation to unknown test samples in dynamic scenarios. Traditional methods for out-of-distribution performance estimation are limited by unrealistic assumptions in the TTA context, such as requiring labeled data or re-training models. To address this issue, we propose AETTA, a label-free accuracy estimation algorithm for TTA. We propose the prediction disagreement as the accuracy estimate, calculated by comparing the target model prediction with dropout inferences. We then improve the prediction disagreement to extend the applicability of AETTA under adaptation failures. Our extensive evaluation with four baselines and six TTA methods demonstrates that AETTA shows an average of 19.8%p more accurate estimation compared with 
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#20013;&#30340;&#21452;&#21521;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#65292;&#22312;&#23391;&#21152;&#25289;&#35821;&#20013;&#35782;&#21035;&#34394;&#20551;&#26032;&#38395;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;99.16%</title><link>https://arxiv.org/abs/2404.01345</link><description>&lt;p&gt;
&#21033;&#29992;&#21452;&#21521;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22686;&#24378;&#23391;&#21152;&#25289;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Enhancing Bangla Fake News Detection Using Bidirectional Gated Recurrent Units and Deep Learning Techniques
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01345
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#20013;&#30340;&#21452;&#21521;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#65292;&#22312;&#23391;&#21152;&#25289;&#35821;&#20013;&#35782;&#21035;&#34394;&#20551;&#26032;&#38395;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;99.16%
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34394;&#20551;&#26032;&#38395;&#30340;&#20852;&#36215;&#20351;&#24471;&#38656;&#35201;&#26377;&#25928;&#30340;&#26816;&#27979;&#26041;&#27861;&#21464;&#24471;&#36234;&#21457;&#37325;&#35201;&#65292;&#20854;&#20013;&#21253;&#25324;&#38750;&#33521;&#35821;&#35821;&#35328;&#12290;&#35813;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#23391;&#21152;&#25289;&#35821;&#36825;&#31181;&#34987;&#35748;&#20026;&#19981;&#22826;&#37325;&#35201;&#30340;&#35821;&#35328;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#32422;50,000&#26465;&#26032;&#38395;&#39033;&#30446;&#30340;&#23436;&#25972;&#25968;&#25454;&#38598;&#12290;&#22312;&#36825;&#20010;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#20960;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21253;&#25324;&#21452;&#21521;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#65288;GRU&#65289;&#12289;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#12289;&#19968;&#32500;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;&#28151;&#21512;&#26550;&#26500;&#12290;&#23545;&#20110;&#36825;&#39033;&#30740;&#31350;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#21033;&#29992;&#19968;&#31995;&#21015;&#26377;&#29992;&#25351;&#26631;&#30340;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#21484;&#22238;&#29575;&#12289;&#31934;&#30830;&#29575;&#12289;F1&#24471;&#20998;&#21644;&#20934;&#30830;&#24230;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#22823;&#22411;&#24212;&#29992;&#31243;&#24207;&#26469;&#23436;&#25104;&#36825;&#39033;&#24037;&#20316;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#35797;&#39564;&#65292;&#23637;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#35782;&#21035;&#23391;&#21152;&#25289;&#35821;&#20013;&#30340;&#34394;&#20551;&#26032;&#38395;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#20854;&#20013;&#21452;&#21521;GRU&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#36798;&#21040;&#20102;&#24778;&#20154;&#30340;99.16%&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#31361;&#20986;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#23391;&#21152;&#25289;&#35821;&#20013;&#35782;&#21035;&#34394;&#20551;&#26032;&#38395;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01345v1 Announce Type: new  Abstract: The rise of fake news has made the need for effective detection methods, including in languages other than English, increasingly important. The study aims to address the challenges of Bangla which is considered a less important language. To this end, a complete dataset containing about 50,000 news items is proposed. Several deep learning models have been tested on this dataset, including the bidirectional gated recurrent unit (GRU), the long short-term memory (LSTM), the 1D convolutional neural network (CNN), and hybrid architectures. For this research, we assessed the efficacy of the model utilizing a range of useful measures, including recall, precision, F1 score, and accuracy. This was done by employing a big application. We carry out comprehensive trials to show the effectiveness of these models in identifying bogus news in Bangla, with the Bidirectional GRU model having a stunning accuracy of 99.16%. Our analysis highlights the impo
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#29256;&#26412;&#30340;DBSCAN&#65292;&#21033;&#29992;&#30456;&#20284;&#24615;&#22270;&#30340;&#22359;&#23545;&#35282;&#23646;&#24615;&#24341;&#23548;&#32858;&#31867;&#36807;&#31243;&#65292;&#36890;&#36807;&#26500;&#24314;&#22359;&#23545;&#35282;&#22270;&#24182;&#36827;&#34892;&#32858;&#31867;&#25490;&#24207;&#65292;&#26131;&#20110;&#30830;&#23450;&#32858;&#31867;&#32467;&#26500;&#12290;</title><link>https://arxiv.org/abs/2404.01341</link><description>&lt;p&gt;
&#22359;&#23545;&#35282;&#24341;&#23548;&#30340;DBSCAN&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Block-Diagonal Guided DBSCAN Clustering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01341
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#29256;&#26412;&#30340;DBSCAN&#65292;&#21033;&#29992;&#30456;&#20284;&#24615;&#22270;&#30340;&#22359;&#23545;&#35282;&#23646;&#24615;&#24341;&#23548;&#32858;&#31867;&#36807;&#31243;&#65292;&#36890;&#36807;&#26500;&#24314;&#22359;&#23545;&#35282;&#22270;&#24182;&#36827;&#34892;&#32858;&#31867;&#25490;&#24207;&#65292;&#26131;&#20110;&#30830;&#23450;&#32858;&#31867;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38598;&#32676;&#20998;&#26512;&#22312;&#25968;&#25454;&#24211;&#25366;&#25496;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#32780;&#22312;&#36825;&#19968;&#39046;&#22495;&#20013;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#31639;&#27861;&#20043;&#19968;&#26159;DBSCAN&#12290;&#28982;&#32780;&#65292;DBSCAN&#23384;&#22312;&#19968;&#20123;&#23616;&#38480;&#24615;&#65292;&#20363;&#22914;&#38590;&#20197;&#22788;&#29702;&#39640;&#32500;&#22823;&#35268;&#27169;&#25968;&#25454;&#12289;&#23545;&#36755;&#20837;&#21442;&#25968;&#25935;&#24863;&#20197;&#21450;&#22312;&#20135;&#29983;&#32858;&#31867;&#32467;&#26524;&#26102;&#32570;&#20047;&#31283;&#20581;&#24615;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#25913;&#36827;&#29256;&#26412;&#30340;DBSCAN&#65292;&#21033;&#29992;&#20102;&#30456;&#20284;&#24615;&#22270;&#30340;&#22359;&#23545;&#35282;&#23646;&#24615;&#26469;&#24341;&#23548;DBSCAN&#30340;&#32858;&#31867;&#36807;&#31243;&#12290;&#20854;&#20851;&#38190;&#24605;&#24819;&#26159;&#26500;&#24314;&#19968;&#20010;&#22270;&#65292;&#34913;&#37327;&#39640;&#32500;&#22823;&#35268;&#27169;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#24182;&#26377;&#21487;&#33021;&#36890;&#36807;&#26410;&#30693;&#32622;&#25442;&#36716;&#25442;&#20026;&#22359;&#23545;&#35282;&#24418;&#24335;&#65292;&#38543;&#21518;&#36890;&#36807;&#19968;&#20010;&#32858;&#31867;&#25490;&#24207;&#36807;&#31243;&#26469;&#29983;&#25104;&#26399;&#26395;&#30340;&#32622;&#25442;&#12290;&#32858;&#31867;&#32467;&#26500;&#21487;&#20197;&#36890;&#36807;&#35782;&#21035;&#32622;&#25442;&#21518;&#22270;&#20013;&#30340;&#23545;&#35282;&#22359;&#26469;&#36731;&#26494;&#30830;&#23450;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#19979;&#38477;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01341v1 Announce Type: cross  Abstract: Cluster analysis plays a crucial role in database mining, and one of the most widely used algorithms in this field is DBSCAN. However, DBSCAN has several limitations, such as difficulty in handling high-dimensional large-scale data, sensitivity to input parameters, and lack of robustness in producing clustering results. This paper introduces an improved version of DBSCAN that leverages the block-diagonal property of the similarity graph to guide the clustering procedure of DBSCAN. The key idea is to construct a graph that measures the similarity between high-dimensional large-scale data points and has the potential to be transformed into a block-diagonal form through an unknown permutation, followed by a cluster-ordering procedure to generate the desired permutation. The clustering structure can be easily determined by identifying the diagonal blocks in the permuted graph. We propose a gradient descent-based method to solve the propose
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#36890;&#36947;&#36827;&#34892;&#32858;&#31867;&#65292;&#23454;&#29616;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36890;&#36947;&#31574;&#30053;&#65292;&#26377;&#25928;&#24179;&#34913;&#20102;&#20010;&#20307;&#36890;&#36947;&#22788;&#29702;&#21644;&#36890;&#36947;&#20043;&#38388;&#24517;&#35201;&#20132;&#20114;&#20316;&#29992;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.01340</link><description>&lt;p&gt;
&#20174;&#30456;&#20284;&#21040;&#20248;&#36234;&#65306;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#36890;&#36947;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
From Similarity to Superiority: Channel Clustering for Time Series Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01340
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#36890;&#36947;&#36827;&#34892;&#32858;&#31867;&#65292;&#23454;&#29616;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36890;&#36947;&#31574;&#30053;&#65292;&#26377;&#25928;&#24179;&#34913;&#20102;&#20010;&#20307;&#36890;&#36947;&#22788;&#29702;&#21644;&#36890;&#36947;&#20043;&#38388;&#24517;&#35201;&#20132;&#20114;&#20316;&#29992;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22312;&#26368;&#36817;&#20960;&#21313;&#24180;&#21560;&#24341;&#20102;&#24456;&#22810;&#20851;&#27880;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#29420;&#31435;&#36890;&#36947;&#31574;&#30053;&#36890;&#36807;&#21333;&#29420;&#22788;&#29702;&#19981;&#21516;&#36890;&#36947;&#26469;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#65292;&#20294;&#22312;&#26410;&#30693;&#23454;&#20363;&#19978;&#23548;&#33268;&#20102;&#24046;&#21170;&#30340;&#27867;&#21270;&#65292;&#24182;&#24573;&#30053;&#20102;&#36890;&#36947;&#20043;&#38388;&#28508;&#22312;&#30340;&#24517;&#35201;&#20132;&#20114;&#20316;&#29992;&#12290;&#30456;&#21453;&#65292;&#20381;&#36182;&#36890;&#36947;&#31574;&#30053;&#23558;&#25152;&#26377;&#36890;&#36947;&#28151;&#21512;&#22312;&#19968;&#36215;&#65292;&#29978;&#33267;&#21253;&#21547;&#26080;&#20851;&#32039;&#35201;&#21644;&#38543;&#24847;&#30340;&#20449;&#24687;&#65292;&#28982;&#32780;&#36825;&#20250;&#23548;&#33268;&#36807;&#24230;&#24179;&#28369;&#30340;&#38382;&#39064;&#24182;&#38480;&#21046;&#20102;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#30446;&#21069;&#32570;&#20047;&#19968;&#31181;&#33021;&#22815;&#26377;&#25928;&#24179;&#34913;&#20010;&#20307;&#36890;&#36947;&#22788;&#29702;&#20197;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#32780;&#21448;&#19981;&#24573;&#35270;&#36890;&#36947;&#20043;&#38388;&#24517;&#35201;&#20132;&#20114;&#20316;&#29992;&#30340;&#36890;&#36947;&#31574;&#30053;&#12290;&#21463;&#21040;&#25105;&#20204;&#35266;&#23519;&#21040;&#30340;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#32451;&#20064;&#25552;&#39640;&#23545;&#28151;&#21512;&#36890;&#36947;&#30340;&#32467;&#26524;&#19982;&#19968;&#23545;&#36890;&#36947;&#20043;&#38388;&#26412;&#36136;&#30456;&#20284;&#24615;&#20043;&#38388;&#30340;&#20851;&#32852;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#36866;&#24212;&#24615;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01340v1 Announce Type: cross  Abstract: Time series forecasting has attracted significant attention in recent decades. Previous studies have demonstrated that the Channel-Independent (CI) strategy improves forecasting performance by treating different channels individually, while it leads to poor generalization on unseen instances and ignores potentially necessary interactions between channels. Conversely, the Channel-Dependent (CD) strategy mixes all channels with even irrelevant and indiscriminate information, which, however, results in oversmoothing issues and limits forecasting accuracy. There is a lack of channel strategy that effectively balances individual channel treatment for improved forecasting performance without overlooking essential interactions between channels. Motivated by our observation of a correlation between the time series model's performance boost against channel mixing and the intrinsic similarity on a pair of channels, we developed a novel and adapt
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#65292;&#36890;&#36807;Latent Dirichlet Allocation (LDA)&#36827;&#34892;&#30456;&#20851;&#30340;&#20027;&#39064;&#24314;&#27169;&#65292;&#24110;&#21161;&#25237;&#36164;&#32773;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#28304;&#20013;&#26816;&#27979;&#36130;&#32463;&#20107;&#20214;&#20013;&#30340;&#30456;&#20851;&#20449;&#24687;&#12289;&#39044;&#27979;&#21644;&#39044;&#27979;</title><link>https://arxiv.org/abs/2404.01338</link><description>&lt;p&gt;
&#36890;&#36807;Latent Dirichlet Allocation&#20027;&#39064;&#24314;&#27169;&#33258;&#21160;&#26816;&#27979;&#36130;&#32463;&#26032;&#38395;&#20013;&#30340;&#30456;&#20851;&#20449;&#24687;&#12289;&#39044;&#27979;&#21644;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Automatic detection of relevant information, predictions and forecasts in financial news through topic modelling with Latent Dirichlet Allocation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01338
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#65292;&#36890;&#36807;Latent Dirichlet Allocation (LDA)&#36827;&#34892;&#30456;&#20851;&#30340;&#20027;&#39064;&#24314;&#27169;&#65292;&#24110;&#21161;&#25237;&#36164;&#32773;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#28304;&#20013;&#26816;&#27979;&#36130;&#32463;&#20107;&#20214;&#20013;&#30340;&#30456;&#20851;&#20449;&#24687;&#12289;&#39044;&#27979;&#21644;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01338v1&#36890;&#21578;&#31867;&#22411;:&#26032;&#25688;&#35201;:&#37329;&#34701;&#26032;&#38395;&#26159;&#19968;&#31181;&#38750;&#32467;&#26500;&#21270;&#30340;&#20449;&#24687;&#28304;&#65292;&#21487;&#20197;&#24320;&#37319;&#20197;&#20174;&#20013;&#25552;&#21462;&#30693;&#35782;&#65292;&#29992;&#20110;&#24066;&#22330;&#31579;&#36873;&#24212;&#29992;&#12290;&#20174;&#25345;&#32493;&#30340;&#37329;&#34701;&#26032;&#38395;&#27969;&#20013;&#25163;&#21160;&#25552;&#21462;&#30456;&#20851;&#20449;&#24687;&#26159;&#32321;&#29712;&#30340;&#65292;&#36229;&#20986;&#20102;&#35768;&#22810;&#25237;&#36164;&#32773;&#30340;&#25216;&#33021;&#33539;&#22260;&#65292;&#20182;&#20204;&#26368;&#22810;&#21482;&#33021;&#20851;&#27880;&#20960;&#20010;&#26469;&#28304;&#21644;&#20316;&#32773;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#23545;&#37329;&#34701;&#26032;&#38395;&#30340;&#20998;&#26512;&#65292;&#20197;&#35782;&#21035;&#30456;&#20851;&#25991;&#26412;&#65292;&#24182;&#22312;&#35813;&#25991;&#26412;&#20013;&#36827;&#34892;&#39044;&#27979;&#21644;&#39044;&#27979;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#31995;&#32479;&#65292;&#24110;&#21161;&#25237;&#36164;&#32773;&#36890;&#36807;&#32771;&#34385;&#35805;&#35821;&#23618;&#38754;&#19978;&#30340;&#30456;&#20851;&#24615;&#21644;&#26102;&#24577;&#24615;&#65292;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#28304;&#20013;&#26816;&#27979;&#30456;&#20851;&#30340;&#36130;&#32463;&#20107;&#20214;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#25991;&#26412;&#20998;&#21106;&#20197;&#23558;&#30456;&#20851;&#25991;&#26412;&#24402;&#20026;&#19968;&#32452;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24212;&#29992;&#20849;&#25351;&#35299;&#26512;&#26469;&#21457;&#29616;&#27573;&#33853;&#20869;&#37096;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;Latent Dirichlet Allocation (LDA)&#36827;&#34892;&#30456;&#20851;&#20027;&#39064;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01338v1 Announce Type: new  Abstract: Financial news items are unstructured sources of information that can be mined to extract knowledge for market screening applications. Manual extraction of relevant information from the continuous stream of finance-related news is cumbersome and beyond the skills of many investors, who, at most, can follow a few sources and authors. Accordingly, we focus on the analysis of financial news to identify relevant text and, within that text, forecasts and predictions. We propose a novel Natural Language Processing (NLP) system to assist investors in the detection of relevant financial events in unstructured textual sources by considering both relevance and temporality at the discursive level. Firstly, we segment the text to group together closely related text. Secondly, we apply co-reference resolution to discover internal dependencies within segments. Finally, we perform relevant topic modelling with Latent Dirichlet Allocation (LDA) to separ
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32467;&#21512;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31995;&#32479;&#65292;&#26088;&#22312;&#22312;&#37329;&#34701;&#26032;&#38395;&#20013;&#26816;&#27979;&#31687;&#31456;&#32423;&#21035;&#30340;&#20851;&#38190;&#22768;&#26126;&#30340;&#26102;&#38388;&#24615;&#65292;&#20197;&#20998;&#26512;&#21477;&#27861;&#21644;&#35821;&#20041;&#20381;&#36182;&#20851;&#31995;&#65292;&#21306;&#20998;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#26377;&#20215;&#20540;&#30340;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2404.01337</link><description>&lt;p&gt;
&#32467;&#21512;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#22312;&#37329;&#34701;&#26032;&#38395;&#20013;&#26816;&#27979;&#31687;&#31456;&#32423;&#21035;&#30340;&#26102;&#38388;&#24615;
&lt;/p&gt;
&lt;p&gt;
Detection of Temporality at Discourse Level on Financial News by Combining Natural Language Processing and Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01337
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32467;&#21512;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31995;&#32479;&#65292;&#26088;&#22312;&#22312;&#37329;&#34701;&#26032;&#38395;&#20013;&#26816;&#27979;&#31687;&#31456;&#32423;&#21035;&#30340;&#20851;&#38190;&#22768;&#26126;&#30340;&#26102;&#38388;&#24615;&#65292;&#20197;&#20998;&#26512;&#21477;&#27861;&#21644;&#35821;&#20041;&#20381;&#36182;&#20851;&#31995;&#65292;&#21306;&#20998;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#26377;&#20215;&#20540;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Finance-related news, such as Bloomberg News, CNN Business, and Forbes, provide valuable real data for market screening systems. Experts in these news articles not only provide technical analyses but also share opinions considering political, sociological, and cultural factors. We propose a novel system that utilizes Natural Language Processing and Machine Learning techniques to detect the temporality of key statements in finance-related news at the discourse level, aiming to differentiate between context information and valuable predictions by analyzing syntactic and semantic dependencies.
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01337v1 Announce Type: new  Abstract: Finance-related news such as Bloomberg News, CNN Business and Forbes are valuable sources of real data for market screening systems. In news, an expert shares opinions beyond plain technical analyses that include context such as political, sociological and cultural factors. In the same text, the expert often discusses the performance of different assets. Some key statements are mere descriptions of past events while others are predictions. Therefore, understanding the temporality of the key statements in a text is essential to separate context information from valuable predictions. We propose a novel system to detect the temporality of finance-related news at discourse level that combines Natural Language Processing and Machine Learning techniques, and exploits sophisticated features such as syntactic and semantic dependencies. More specifically, we seek to extract the dominant tenses of the main statements, which may be either explicit 
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#24314;&#31569;&#35774;&#35745;&#20013;&#24320;&#21019;&#20102;&#26032;&#30340;&#26041;&#27861;&#35770;&#33539;&#24335;&#65292;&#26174;&#33879;&#25193;&#23637;&#20102;&#35774;&#35745;&#36807;&#31243;&#30340;&#21019;&#26032;&#28508;&#21147;&#21644;&#25928;&#29575;&#65292;&#36890;&#36807;&#24191;&#27867;&#24212;&#29992;&#29983;&#25104;&#24335;AI&#25216;&#26415;&#21644;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;2D&#22270;&#20687;&#12289;&#35270;&#39057;&#21644;3D&#27169;&#22411;&#65292;&#24182;&#23457;&#35270;&#20854;&#22312;&#19981;&#21516;&#38454;&#27573;&#30340;&#24433;&#21709;&#36235;&#21183;&#12290;</title><link>https://arxiv.org/abs/2404.01335</link><description>&lt;p&gt;
&#24314;&#31569;&#35774;&#35745;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65306;&#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Generative AI for Architectural Design: A Literature Review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01335
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#24314;&#31569;&#35774;&#35745;&#20013;&#24320;&#21019;&#20102;&#26032;&#30340;&#26041;&#27861;&#35770;&#33539;&#24335;&#65292;&#26174;&#33879;&#25193;&#23637;&#20102;&#35774;&#35745;&#36807;&#31243;&#30340;&#21019;&#26032;&#28508;&#21147;&#21644;&#25928;&#29575;&#65292;&#36890;&#36807;&#24191;&#27867;&#24212;&#29992;&#29983;&#25104;&#24335;AI&#25216;&#26415;&#21644;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;2D&#22270;&#20687;&#12289;&#35270;&#39057;&#21644;3D&#27169;&#22411;&#65292;&#24182;&#23457;&#35270;&#20854;&#22312;&#19981;&#21516;&#38454;&#27573;&#30340;&#24433;&#21709;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01335v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#36234; &#25688;&#35201;&#65306;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#22312;&#24314;&#31569;&#35774;&#35745;&#20013;&#24320;&#21019;&#20102;&#26032;&#30340;&#26041;&#27861;&#35770;&#33539;&#24335;&#65292;&#26174;&#33879;&#25193;&#23637;&#20102;&#35774;&#35745;&#36807;&#31243;&#30340;&#21019;&#26032;&#28508;&#21147;&#21644;&#25928;&#29575;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22312;&#24314;&#31569;&#35774;&#35745;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#36825;&#19968;&#36235;&#21183;&#21463;&#30410;&#20110;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#24555;&#36895;&#21457;&#23637;&#12290;&#25991;&#31456;&#20840;&#38754;&#22238;&#39038;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#21644;&#22823;&#35268;&#27169;&#27169;&#22411;&#30340;&#22522;&#26412;&#21407;&#29702;&#65292;&#24182;&#31361;&#20986;&#20102;&#22312;&#29983;&#25104;2D&#22270;&#20687;&#12289;&#35270;&#39057;&#21644;3D&#27169;&#22411;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#23457;&#26597;&#26469;&#33258;2020&#24180;&#30340;&#26368;&#26032;&#25991;&#29486;&#65292;&#26412;&#25991;&#23457;&#35270;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22312;&#24314;&#31569;&#35774;&#35745;&#30340;&#19981;&#21516;&#38454;&#27573;&#30340;&#24433;&#21709;&#65292;&#20174;&#29983;&#25104;&#21021;&#22987;&#24314;&#31569;3D&#24418;&#24335;&#21040;&#29983;&#25104;&#26368;&#32456;&#24314;&#31569;&#22270;&#20687;&#12290;&#30740;&#31350;&#22686;&#38271;&#30340;&#26126;&#26174;&#36235;&#21183;&#34920;&#26126;&#24314;&#31569;&#35774;&#35745;&#39046;&#22495;&#23545;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#20542;&#21521;&#19981;&#26029;&#22686;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01335v1 Announce Type: cross  Abstract: Generative Artificial Intelligence (AI) has pioneered new methodological paradigms in architectural design, significantly expanding the innovative potential and efficiency of the design process. This paper explores the extensive applications of generative AI technologies in architectural design, a trend that has benefited from the rapid development of deep generative models. This article provides a comprehensive review of the basic principles of generative AI and large-scale models and highlights the applications in the generation of 2D images, videos, and 3D models. In addition, by reviewing the latest literature from 2020, this paper scrutinizes the impact of generative AI technologies at different stages of architectural design, from generating initial architectural 3D forms to producing final architectural imagery. The marked trend of research growth indicates an increasing inclination within the architectural design community towa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;&#26631;&#27880;&#26041;&#27861;&#65292;&#23558;&#20154;&#21147;&#24037;&#20316;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#26088;&#22312;&#25552;&#39640;NER&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#20197;&#25104;&#26412;&#25928;&#30410;&#30340;&#26041;&#24335;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2404.01334</link><description>&lt;p&gt;
&#20351;&#29992;LLMs&#22686;&#24378;NER&#25968;&#25454;&#38598;&#65306;&#36808;&#21521;&#33258;&#21160;&#21270;&#21644;&#31934;&#32454;&#21270;&#26631;&#27880;
&lt;/p&gt;
&lt;p&gt;
Augmenting NER Datasets with LLMs: Towards Automated and Refined Annotation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01334
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;&#26631;&#27880;&#26041;&#27861;&#65292;&#23558;&#20154;&#21147;&#24037;&#20316;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#26088;&#22312;&#25552;&#39640;NER&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#20197;&#25104;&#26412;&#25928;&#30410;&#30340;&#26041;&#24335;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#65292;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#34987;&#35748;&#20026;&#26159;&#19968;&#39033;&#20851;&#38190;&#25216;&#26415;&#65292;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#12290;&#20256;&#32479;&#30340;&#29992;&#20110;&#20026;NER&#27169;&#22411;&#26631;&#27880;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#38754;&#20020;&#30528;&#39640;&#25104;&#26412;&#21644;&#25968;&#25454;&#38598;&#36136;&#37327;&#21464;&#21270;&#30340;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#28151;&#21512;&#26631;&#27880;&#26041;&#27861;&#65292;&#23558;&#20154;&#21147;&#24037;&#20316;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#30456;&#32467;&#21512;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#26088;&#22312;&#25913;&#21892;&#25163;&#21160;&#27880;&#37322;&#20013;&#22266;&#26377;&#30340;&#22122;&#38899;&#65292;&#22914;&#36951;&#28431;&#65292;&#20174;&#32780;&#25552;&#39640;NER&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#32780;&#19988;&#36824;&#20197;&#19968;&#31181;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#30340;&#26041;&#24335;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#37319;&#29992;&#26631;&#31614;&#28151;&#21512;&#31574;&#30053;&#65292;&#23427;&#35299;&#20915;&#20102;LLM-based&#27880;&#37322;&#20013;&#36935;&#21040;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#36890;&#36807;&#23545;&#22810;&#20010;&#25968;&#25454;&#38598;&#30340;&#20998;&#26512;&#65292;&#36825;&#31181;&#26041;&#27861;&#19968;&#30452;&#34920;&#29616;&#20986;&#27604;&#20256;&#32479;&#27880;&#37322;&#26041;&#27861;&#26356;&#20248;&#24322;&#30340;&#24615;&#33021;&#65292;&#21363;&#20351;&#22312;co
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01334v1 Announce Type: new  Abstract: In the field of Natural Language Processing (NLP), Named Entity Recognition (NER) is recognized as a critical technology, employed across a wide array of applications. Traditional methodologies for annotating datasets for NER models are challenged by high costs and variations in dataset quality. This research introduces a novel hybrid annotation approach that synergizes human effort with the capabilities of Large Language Models (LLMs). This approach not only aims to ameliorate the noise inherent in manual annotations, such as omissions, thereby enhancing the performance of NER models, but also achieves this in a cost-effective manner. Additionally, by employing a label mixing strategy, it addresses the issue of class imbalance encountered in LLM-based annotations. Through an analysis across multiple datasets, this method has been consistently shown to provide superior performance compared to traditional annotation methods, even under co
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;Shapley&#20540;&#26041;&#27861;&#35299;&#37322;LLM&#34892;&#20026;&#65292;&#25581;&#31034;&#20102;&#25152;&#35859;&#30340;&#8220;&#20196;&#29260;&#22122;&#38899;&#8221;&#25928;&#24212;&#65292;&#25581;&#31034;&#20102;LLMs&#30340;&#20915;&#31574;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21463;&#21040;&#25552;&#31034;&#32452;&#20214;&#30340;&#24433;&#21709;</title><link>https://arxiv.org/abs/2404.01332</link><description>&lt;p&gt;
&#31561;&#31561;&#65292;&#36825;&#37117;&#26159;&#20196;&#29260;&#22122;&#38899;&#65311;&#19968;&#30452;&#23601;&#26159;&#21527;&#65306;&#21033;&#29992; Shapley &#20540;&#35299;&#37322; LLM &#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Wait, It's All Token Noise? Always Has Been: Interpreting LLM Behavior Using Shapley Value
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01332
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;Shapley&#20540;&#26041;&#27861;&#35299;&#37322;LLM&#34892;&#20026;&#65292;&#25581;&#31034;&#20102;&#25152;&#35859;&#30340;&#8220;&#20196;&#29260;&#22122;&#38899;&#8221;&#25928;&#24212;&#65292;&#25581;&#31034;&#20102;LLMs&#30340;&#20915;&#31574;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21463;&#21040;&#25552;&#31034;&#32452;&#20214;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#20026;&#27169;&#25311;&#20154;&#31867;&#34892;&#20026;&#21644;&#35748;&#30693;&#36807;&#31243;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#65292;&#28508;&#22312;&#24212;&#29992;&#21253;&#25324;&#24066;&#22330;&#30740;&#31350;&#21644;&#28040;&#36153;&#32773;&#34892;&#20026;&#20998;&#26512;&#31561;&#21508;&#20010;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;LLMs&#30340;&#26174;&#33879;&#24046;&#24322;&#26263;&#31034;&#20102;&#19981;&#21516;&#30340;&#22522;&#30784;&#36807;&#31243;&#22312;&#36215;&#20316;&#29992;&#65292;&#20197;&#21450;LLMs&#23545;&#25552;&#31034;&#21464;&#21270;&#30340;&#25935;&#24863;&#24615;&#65292;&#21033;&#29992;LLMs&#20316;&#20026;&#20154;&#31867;&#20027;&#20307;&#30340;&#26367;&#20195;&#20173;&#28982;&#23384;&#22312;&#19981;&#30830;&#23450;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21512;&#20316;&#21338;&#24328;&#29702;&#35770;&#20013;Shapley&#20540;&#30340;&#26032;&#26041;&#27861;&#26469;&#35299;&#37322;LLM&#34892;&#20026;&#65292;&#24182;&#37327;&#21270;&#27599;&#20010;&#25552;&#31034;&#32452;&#20214;&#23545;&#27169;&#22411;&#36755;&#20986;&#30340;&#30456;&#23545;&#36129;&#29486;&#12290;&#36890;&#36807;&#20004;&#20010;&#24212;&#29992;--&#19968;&#20010;&#31163;&#25955;&#36873;&#25321;&#23454;&#39564;&#21644;&#19968;&#20010;&#35748;&#30693;&#20559;&#35265;&#35843;&#26597;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Shapley&#20540;&#26041;&#27861;&#22914;&#20309;&#25581;&#31034;&#25105;&#20204;&#25152;&#35859;&#30340;&#8220;&#20196;&#29260;&#22122;&#38899;&#8221;&#25928;&#24212;&#65292;&#21363;LLM&#20915;&#31574;&#21463;&#21040;&#30340;&#24433;&#21709;&#20005;&#37325;&#20559;&#21521;&#20110;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01332v1 Announce Type: cross  Abstract: The emergence of large language models (LLMs) has opened up exciting possibilities for simulating human behavior and cognitive processes, with potential applications in various domains, including marketing research and consumer behavior analysis. However, the validity of utilizing LLMs as stand-ins for human subjects remains uncertain due to glaring divergences that suggest fundamentally different underlying processes at play and the sensitivity of LLM responses to prompt variations. This paper presents a novel approach based on Shapley values from cooperative game theory to interpret LLM behavior and quantify the relative contribution of each prompt component to the model's output. Through two applications-a discrete choice experiment and an investigation of cognitive biases-we demonstrate how the Shapley value method can uncover what we term "token noise" effects, a phenomenon where LLM decisions are disproportionately influenced by 
&lt;/p&gt;</description></item><item><title>Holo-VQVAE&#26159;&#19968;&#31181;&#38024;&#23545;&#20165;&#30456;&#20301;&#20840;&#24687;&#22270;&#30340;&#26032;&#22411;&#29983;&#25104;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#30690;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#32467;&#26500;&#65292;&#36890;&#36807;&#38598;&#25104;&#35282;&#35889;&#26041;&#27861;&#26469;&#23398;&#20064;&#22270;&#20687;&#22495;&#65292;&#22312;&#20840;&#24687;&#22270;&#29983;&#25104;&#20013;&#23454;&#29616;&#20102;&#20174;&#22797;&#26434;&#20998;&#24067;&#20013;&#30452;&#25509;&#29983;&#25104;&#22810;&#26679;&#21270;&#20840;&#24687;&#20869;&#23481;&#12290;</title><link>https://arxiv.org/abs/2404.01330</link><description>&lt;p&gt;
Holo-VQVAE&#65306;&#29992;&#20110;&#20165;&#30456;&#20301;&#20840;&#24687;&#22270;&#30340;VQ-VAE
&lt;/p&gt;
&lt;p&gt;
Holo-VQVAE: VQ-VAE for phase-only holograms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01330
&lt;/p&gt;
&lt;p&gt;
Holo-VQVAE&#26159;&#19968;&#31181;&#38024;&#23545;&#20165;&#30456;&#20301;&#20840;&#24687;&#22270;&#30340;&#26032;&#22411;&#29983;&#25104;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#30690;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#32467;&#26500;&#65292;&#36890;&#36807;&#38598;&#25104;&#35282;&#35889;&#26041;&#27861;&#26469;&#23398;&#20064;&#22270;&#20687;&#22495;&#65292;&#22312;&#20840;&#24687;&#22270;&#29983;&#25104;&#20013;&#23454;&#29616;&#20102;&#20174;&#22797;&#26434;&#20998;&#24067;&#20013;&#30452;&#25509;&#29983;&#25104;&#22810;&#26679;&#21270;&#20840;&#24687;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Holography stands at the forefront of visual technology innovation, offering immersive, three-dimensional visualizations through the manipulation of light wave amplitude and phase. Contemporary research in hologram generation has predominantly focused on image-to-hologram conversion, producing holograms from existing images. These approaches, while effective, inherently limit the scope of innovation and creativity in hologram generation. In response to this limitation, we present Holo-VQVAE, a novel generative framework tailored for phase-only holograms (POHs). Holo-VQVAE leverages the architecture of Vector Quantized Variational AutoEncoders, enabling it to learn the complex distributions of POHs. Furthermore, it integrates the Angular Spectrum Method into the training process, facilitating learning in the image domain. This framework allows for the generation of unseen, diverse holographic content directly from its intricately learned distributions.
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01330v1 Announce Type: cross  Abstract: Holography stands at the forefront of visual technology innovation, offering immersive, three-dimensional visualizations through the manipulation of light wave amplitude and phase. Contemporary research in hologram generation has predominantly focused on image-to-hologram conversion, producing holograms from existing images. These approaches, while effective, inherently limit the scope of innovation and creativity in hologram generation. In response to this limitation, we present Holo-VQVAE, a novel generative framework tailored for phase-only holograms (POHs). Holo-VQVAE leverages the architecture of Vector Quantized Variational AutoEncoders, enabling it to learn the complex distributions of POHs. Furthermore, it integrates the Angular Spectrum Method into the training process, facilitating learning in the image domain. This framework allows for the generation of unseen, diverse holographic content directly from its intricately learne
&lt;/p&gt;</description></item><item><title>EBER chatbot&#26159;&#19968;&#20010;&#26088;&#22312;&#20943;&#23569;&#32769;&#24180;&#20154;&#25968;&#23383;&#40511;&#27807;&#30340;&#23089;&#20048;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#20854;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#20854;"&#26234;&#33021;&#30005;&#21488;"&#27010;&#24565;&#65292;&#26681;&#25454;&#29992;&#25143;&#30340;&#24515;&#24773;&#21644;&#38656;&#27714;&#25552;&#20379;&#30456;&#20851;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2404.01327</link><description>&lt;p&gt;
&#26080;&#25277;&#35937;&#33021;&#21147;&#30340;&#32769;&#24180;&#20154;&#25968;&#23383;&#21253;&#23481;&#23089;&#20048;&#32842;&#22825;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
Entertainment chatbot for the digital inclusion of elderly people without abstraction capabilities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01327
&lt;/p&gt;
&lt;p&gt;
EBER chatbot&#26159;&#19968;&#20010;&#26088;&#22312;&#20943;&#23569;&#32769;&#24180;&#20154;&#25968;&#23383;&#40511;&#27807;&#30340;&#23089;&#20048;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#20854;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#20854;"&#26234;&#33021;&#30005;&#21488;"&#27010;&#24565;&#65292;&#26681;&#25454;&#29992;&#25143;&#30340;&#24515;&#24773;&#21644;&#38656;&#27714;&#25552;&#20379;&#30456;&#20851;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#20801;&#35768;&#21019;&#24314;&#23545;&#35805;&#24335;&#32842;&#22825;&#26426;&#22120;&#20154;&#24179;&#21488;&#12290;&#23613;&#31649;&#20154;&#24037;&#26234;&#33021;&#22312;&#35768;&#22810;&#22823;&#20247;&#24066;&#22330;&#39046;&#22495;&#20173;&#28982;&#36807;&#20110;&#19981;&#25104;&#29087;&#65292;&#26080;&#27861;&#25903;&#25345;&#20196;&#20154;&#28385;&#24847;&#30340;&#29992;&#25143;&#20307;&#39564;&#65292;&#20294;&#23545;&#35805;&#24335;&#30028;&#38754;&#24050;&#32463;&#22312;&#20020;&#26102;&#24212;&#29992;&#20013;&#25214;&#21040;&#20102;&#33258;&#24049;&#30340;&#20301;&#32622;&#65292;&#27604;&#22914;&#30005;&#35805;&#20013;&#24515;&#21644;&#22312;&#32447;&#36141;&#29289;&#21161;&#25163;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#23558;&#20854;&#24212;&#29992;&#20110;&#32769;&#24180;&#20154;&#30340;&#31038;&#20250;&#21253;&#23481;&#65292;&#32769;&#24180;&#20154;&#29305;&#21035;&#23481;&#26131;&#21463;&#21040;&#25968;&#23383;&#40511;&#27807;&#30340;&#24433;&#21709;&#12290;&#35768;&#22810;&#32769;&#24180;&#20154;&#36890;&#36807;&#20256;&#32479;&#23186;&#20307;&#22914;&#30005;&#35270;&#21644;&#24191;&#25773;&#26469;&#20943;&#36731;&#23396;&#29420;&#24863;&#65292;&#36825;&#20123;&#23186;&#20307;&#34987;&#35748;&#20026;&#26159;&#21019;&#36896;&#38506;&#20276;&#24863;&#30340;&#26041;&#24335;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#26088;&#22312;&#20943;&#23569;&#32769;&#24180;&#20154;&#25968;&#23383;&#40511;&#27807;&#30340;EBER&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;EBER&#20250;&#22312;&#21518;&#21488;&#38405;&#35835;&#26032;&#38395;&#65292;&#24182;&#26681;&#25454;&#29992;&#25143;&#30340;&#24515;&#24773;&#35843;&#25972;&#22238;&#22797;&#12290;&#20854;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#8220;&#26234;&#33021;&#30005;&#21488;&#8221;&#27010;&#24565;&#65292;&#26681;&#25454;&#35813;&#27010;&#24565;&#65292;&#19981;&#26159;&#31616;&#21270;&#25968;&#23383;&#20449;&#24687;&#31995;&#32479;&#20197;&#20351;&#20854;&#26131;&#20110;&#35775;&#38382;&#65292;&#32780;&#26159;&#26681;&#25454;&#29992;&#25143;&#30340;&#24515;&#24773;&#21644;&#38656;&#27714;&#25552;&#20379;&#30456;&#20851;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01327v1 Announce Type: cross  Abstract: Current language processing technologies allow the creation of conversational chatbot platforms. Even though artificial intelligence is still too immature to support satisfactory user experience in many mass market domains, conversational interfaces have found their way into ad hoc applications such as call centres and online shopping assistants. However, they have not been applied so far to social inclusion of elderly people, who are particularly vulnerable to the digital divide. Many of them relieve their loneliness with traditional media such as TV and radio, which are known to create a feeling of companionship. In this paper we present the EBER chatbot, designed to reduce the digital gap for the elderly. EBER reads news in the background and adapts its responses to the user's mood. Its novelty lies in the concept of "intelligent radio", according to which, instead of simplifying a digital information system to make it accessible to
&lt;/p&gt;</description></item><item><title>JailbreakBench&#26159;&#19968;&#20010;&#29992;&#20110;&#23545;&#25239;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36234;&#29425;&#30340;&#24320;&#25918;&#22522;&#20934;&#65292;&#25552;&#20379;&#26032;&#30340;&#25968;&#25454;&#38598;&#12289;&#23545;&#25239;&#25552;&#31034;&#21644;&#35780;&#20272;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2404.01318</link><description>&lt;p&gt;
JailbreakBench: &#19968;&#20010;&#29992;&#20110;&#23545;&#25239;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36234;&#29425;&#30340;&#24320;&#25918;&#40065;&#26834;&#24615;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01318
&lt;/p&gt;
&lt;p&gt;
JailbreakBench&#26159;&#19968;&#20010;&#29992;&#20110;&#23545;&#25239;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36234;&#29425;&#30340;&#24320;&#25918;&#22522;&#20934;&#65292;&#25552;&#20379;&#26032;&#30340;&#25968;&#25454;&#38598;&#12289;&#23545;&#25239;&#25552;&#31034;&#21644;&#35780;&#20272;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#29425;&#25915;&#20987;&#20250;&#23548;&#33268;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#26377;&#23475;&#12289;&#19981;&#36947;&#24503;&#25110;&#20196;&#20154;&#21453;&#24863;&#30340;&#20869;&#23481;&#12290;&#35780;&#20272;&#36825;&#20123;&#25915;&#20987;&#23384;&#22312;&#35768;&#22810;&#25361;&#25112;&#65292;&#24403;&#21069;&#30340;&#22522;&#20934;&#21644;&#35780;&#20272;&#25216;&#26415;&#24182;&#26410;&#20805;&#20998;&#35299;&#20915;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;JailbreakBench&#65292;&#19968;&#20010;&#24320;&#28304;&#22522;&#20934;&#65292;&#21253;&#25324;&#20855;&#26377;100&#20010;&#29420;&#29305;&#34892;&#20026;&#30340;&#26032;&#36234;&#29425;&#25968;&#25454;&#38598;&#65288;&#31216;&#20026;JBB-Behaviors&#65289;&#12289;&#19968;&#32452;&#26368;&#20808;&#36827;&#30340;&#23545;&#25239;&#25552;&#31034;&#65288;&#31216;&#20026;&#36234;&#29425;&#24037;&#20214;&#65289;&#21644;&#19968;&#20010;&#26631;&#20934;&#21270;&#35780;&#20272;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01318v1 Announce Type: cross  Abstract: Jailbreak attacks cause large language models (LLMs) to generate harmful, unethical, or otherwise objectionable content. Evaluating these attacks presents a number of challenges, which the current collection of benchmarks and evaluation techniques do not adequately address. First, there is no clear standard of practice regarding jailbreaking evaluation. Second, existing works compute costs and success rates in incomparable ways. And third, numerous works are not reproducible, as they withhold adversarial prompts, involve closed-source code, or rely on evolving proprietary APIs. To address these challenges, we introduce JailbreakBench, an open-sourced benchmark with the following components: (1) a new jailbreaking dataset containing 100 unique behaviors, which we call JBB-Behaviors; (2) an evolving repository of state-of-the-art adversarial prompts, which we refer to as jailbreak artifacts; (3) a standardized evaluation framework that i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;Transformer&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#36890;&#36807;&#26234;&#33021;&#23398;&#20064;&#29575;&#20998;&#24067;&#21462;&#24471;&#20102;&#27604;&#24179;&#22374;&#23398;&#20064;&#29575;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;GLUE&#25968;&#25454;&#38598;&#20013;&#24471;&#21040;&#39564;&#35777;&#12290;</title><link>https://arxiv.org/abs/2404.01317</link><description>&lt;p&gt;
&#22312;Transformer&#20013;&#26234;&#33021;&#23398;&#20064;&#29575;&#20998;&#24067;&#20197;&#20943;&#23569;&#28798;&#38590;&#24615;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Intelligent Learning Rate Distribution to reduce Catastrophic Forgetting in Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01317
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;Transformer&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#36890;&#36807;&#26234;&#33021;&#23398;&#20064;&#29575;&#20998;&#24067;&#21462;&#24471;&#20102;&#27604;&#24179;&#22374;&#23398;&#20064;&#29575;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;GLUE&#25968;&#25454;&#38598;&#20013;&#24471;&#21040;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#23545;&#22823;&#22411;&#25991;&#26412;&#35821;&#26009;&#24211;&#36827;&#34892;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#26159;&#19968;&#31181;&#24120;&#35265;&#20570;&#27861;&#12290;&#28982;&#21518;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#20197;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#21462;&#24471;&#26368;&#20339;&#32467;&#26524;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;Transformer&#31070;&#32463;&#32593;&#32476;&#20013;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#38382;&#39064;&#65292;&#24182;&#36136;&#30097;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#23545;&#25972;&#20010;&#32593;&#32476;&#37319;&#29992;&#30456;&#21516;&#23398;&#20064;&#29575;&#30340;&#24494;&#35843;&#24120;&#35265;&#20570;&#27861;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#36229;&#21442;&#25968;&#20248;&#21270;&#36807;&#31243;&#65292;&#25214;&#21040;&#20102;&#27604;&#24179;&#22374;&#23398;&#20064;&#29575;&#26356;&#22909;&#30340;&#23398;&#20064;&#29575;&#20998;&#24067;&#12290;&#25105;&#20204;&#32467;&#21512;&#36825;&#20123;&#23398;&#20064;&#29575;&#20998;&#24067;&#65292;&#24182;&#23637;&#31034;&#23427;&#20204;&#23545;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#30340;&#24615;&#33021;&#34920;&#29616;&#26356;&#22909;&#12290;&#25105;&#20204;&#20351;&#29992;GLUE&#25968;&#25454;&#38598;&#20013;&#30340;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22522;&#20934;&#39564;&#35777;&#20102;&#36825;&#20123;&#23398;&#20064;&#29575;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01317v1 Announce Type: cross  Abstract: Pretraining language models on large text corpora is a common practice in natural language processing. Fine-tuning of these models is then performed to achieve the best results on a variety of tasks. In this paper, we investigate the problem of catastrophic forgetting in transformer neural networks and question the common practice of fine-tuning with a flat learning rate for the entire network in this context. We perform a hyperparameter optimization process to find learning rate distributions that are better than a flat learning rate. We combine the learning rate distributions thus found and show that they generalize to better performance with respect to the problem of catastrophic forgetting. We validate these learning rate distributions with a variety of NLP benchmarks from the GLUE dataset.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#35299;&#20915;&#20102;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#20316;&#19994;&#36710;&#38388;&#35843;&#24230;&#38382;&#39064;&#65292;&#37325;&#28857;&#22312;&#20110;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#26469;&#22788;&#29702;&#20855;&#26377;&#19981;&#30830;&#23450;&#25345;&#32493;&#26102;&#38388;&#30340;JSSP&#12290;</title><link>https://arxiv.org/abs/2404.01308</link><description>&lt;p&gt;
&#23398;&#20064;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#35299;&#20915;&#20316;&#19994;&#36710;&#38388;&#35843;&#24230;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Learning to Solve Job Shop Scheduling under Uncertainty
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01308
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#35299;&#20915;&#20102;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#20316;&#19994;&#36710;&#38388;&#35843;&#24230;&#38382;&#39064;&#65292;&#37325;&#28857;&#22312;&#20110;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#26469;&#22788;&#29702;&#20855;&#26377;&#19981;&#30830;&#23450;&#25345;&#32493;&#26102;&#38388;&#30340;JSSP&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#19994;&#36710;&#38388;&#35843;&#24230;&#38382;&#39064;&#65288;JSSP&#65289;&#26159;&#19968;&#20010;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#20854;&#20013;&#20219;&#21153;&#38656;&#35201;&#22312;&#26426;&#22120;&#19978;&#36827;&#34892;&#35843;&#24230;&#65292;&#20197;&#26368;&#23567;&#21270;&#35832;&#22914;&#26368;&#22823;&#23436;&#24037;&#26102;&#38388;&#25110;&#24310;&#36831;&#31561;&#26631;&#20934;&#12290;&#20026;&#20102;&#35299;&#20915;&#26356;&#21152;&#29616;&#23454;&#30340;&#22330;&#26223;&#65292;&#25105;&#20204;&#20026;&#27599;&#20010;&#20219;&#21153;&#30340;&#25345;&#32493;&#26102;&#38388;&#20851;&#32852;&#20102;&#19968;&#20010;&#27010;&#29575;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#29983;&#25104;&#19968;&#20010;&#31283;&#20581;&#30340;&#35843;&#24230;&#65292;&#21363;&#26368;&#23567;&#21270;&#24179;&#22343;&#23436;&#24037;&#26102;&#38388;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#25216;&#26415;&#26469;&#23547;&#25214;&#31283;&#20581;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#37325;&#28857;&#20851;&#27880;&#20855;&#26377;&#19981;&#30830;&#23450;&#25345;&#32493;&#26102;&#38388;&#30340;JSSP&#12290;&#26412;&#30740;&#31350;&#30340;&#20851;&#38190;&#36129;&#29486;&#21253;&#25324;&#65306;&#65288;1&#65289;DRL&#22312;JSSP&#24212;&#29992;&#20013;&#30340;&#36827;&#23637;&#65292;&#22686;&#24378;&#27867;&#21270;&#24615;&#21644;&#21487;&#20280;&#32553;&#24615;&#65292;&#65288;2&#65289;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#20855;&#26377;&#19981;&#30830;&#23450;&#25345;&#32493;&#26102;&#38388;&#30340;JSSP&#12290; Wheatley&#26041;&#27861;&#65292;&#38598;&#25104;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#21644;DRL&#65292;&#24050;&#20844;&#24320;&#21487;&#29992;&#20110;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01308v1 Announce Type: new  Abstract: Job-Shop Scheduling Problem (JSSP) is a combinatorial optimization problem where tasks need to be scheduled on machines in order to minimize criteria such as makespan or delay. To address more realistic scenarios, we associate a probability distribution with the duration of each task. Our objective is to generate a robust schedule, i.e. that minimizes the average makespan. This paper introduces a new approach that leverages Deep Reinforcement Learning (DRL) techniques to search for robust solutions, emphasizing JSSPs with uncertain durations. Key contributions of this research include: (1) advancements in DRL applications to JSSPs, enhancing generalization and scalability, (2) a novel method for addressing JSSPs with uncertain durations. The Wheatley approach, which integrates Graph Neural Networks (GNNs) and DRL, is made publicly available for further research and applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21463;&#31070;&#32463;&#31995;&#32479;&#21551;&#21457;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#25299;&#25169;&#30340;&#31232;&#30095;&#26041;&#27861;&#65292;&#25506;&#32034;&#31867;&#20284;&#20110;&#29983;&#29289;&#32593;&#32476;&#30340;&#26426;&#21046;&#65292;&#23637;&#31034;&#20102;&#23545;&#21508;&#31181; NLP &#20219;&#21153;&#37117;&#34920;&#29616;&#20986;&#33394;&#21644;&#39640;&#25928;&#30340;&#27169;&#22411;-&#19981;&#21487;&#30693;&#31232;&#30095;&#24615;&#26041;&#27861;</title><link>https://arxiv.org/abs/2404.01306</link><description>&lt;p&gt;
NeuroPrune&#65306;&#19968;&#31181;&#21463;&#31070;&#32463;&#31995;&#32479;&#21551;&#21457;&#30340;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25299;&#25169;&#31232;&#30095;&#35757;&#32451;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
NeuroPrune: A Neuro-inspired Topological Sparse Training Algorithm for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21463;&#31070;&#32463;&#31995;&#32479;&#21551;&#21457;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#25299;&#25169;&#30340;&#31232;&#30095;&#26041;&#27861;&#65292;&#25506;&#32034;&#31867;&#20284;&#20110;&#29983;&#29289;&#32593;&#32476;&#30340;&#26426;&#21046;&#65292;&#23637;&#31034;&#20102;&#23545;&#21508;&#31181; NLP &#20219;&#21153;&#37117;&#34920;&#29616;&#20986;&#33394;&#21644;&#39640;&#25928;&#30340;&#27169;&#22411;-&#19981;&#21487;&#30693;&#31232;&#30095;&#24615;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110; Transformer &#30340;&#35821;&#35328;&#27169;&#22411;&#30001;&#20110;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#30340;&#20986;&#33394;&#24615;&#33021;&#32780;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#21464;&#24471;&#26222;&#36941;&#12290;&#28982;&#32780;&#65292;&#26114;&#36149;&#30340;&#35757;&#32451;&#20197;&#21450;&#25512;&#29702;&#20173;&#28982;&#26159;&#23427;&#20204;&#24191;&#27867;&#36866;&#29992;&#24615;&#30340;&#19968;&#20010;&#37325;&#35201;&#38556;&#30861;&#12290;&#22312;&#27169;&#22411;&#26550;&#26500;&#30340;&#21508;&#20010;&#23618;&#27425;&#24378;&#21046;&#24341;&#20837;&#31232;&#30095;&#24615;&#24050;&#34987;&#35777;&#26126;&#26377;&#21161;&#20110;&#35299;&#20915;&#25193;&#23637;&#24615;&#21644;&#25928;&#29575;&#38382;&#39064;&#65292;&#20294;&#31232;&#30095;&#24615;&#23545;&#32593;&#32476;&#25299;&#25169;&#30340;&#24433;&#21709;&#20173;&#23384;&#22312;&#26029;&#35010;&#12290;&#21463;&#22823;&#33041;&#31070;&#32463;&#32593;&#32476;&#21551;&#21457;&#65292;&#25105;&#20204;&#36890;&#36807;&#32593;&#32476;&#25299;&#25169;&#30340;&#35270;&#35282;&#25506;&#32034;&#31232;&#30095;&#24615;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;&#22312;&#29983;&#29289;&#32593;&#32476;&#20013;&#35266;&#23519;&#21040;&#30340;&#26426;&#21046;&#65292;&#22914;&#20248;&#20808;&#38468;&#30528;&#21644;&#20887;&#20313;&#31361;&#35302;&#20462;&#21098;&#65292;&#24182;&#23637;&#31034;&#20102;&#22522;&#20110;&#21407;&#21017;&#30340;&#12289;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#31232;&#30095;&#24615;&#26041;&#27861;&#22312;&#36328;&#36234;&#20998;&#31867;&#65288;&#22914;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65289;&#21644;&#29983;&#25104;&#65288;&#25688;&#35201;&#12289;&#26426;&#22120;&#32763;&#35793;&#65289;&#30340;&#21508;&#31181; NLP &#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#19988;&#39640;&#25928;&#65292;&#23613;&#31649; o
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01306v1 Announce Type: cross  Abstract: Transformer-based Language Models have become ubiquitous in Natural Language Processing (NLP) due to their impressive performance on various tasks. However, expensive training as well as inference remains a significant impediment to their widespread applicability. While enforcing sparsity at various levels of the model architecture has found promise in addressing scaling and efficiency issues, there remains a disconnect between how sparsity affects network topology. Inspired by brain neuronal networks, we explore sparsity approaches through the lens of network topology. Specifically, we exploit mechanisms seen in biological networks, such as preferential attachment and redundant synapse pruning, and show that principled, model-agnostic sparsity approaches are performant and efficient across diverse NLP tasks, spanning both classification (such as natural language inference) and generation (summarization, machine translation), despite o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22312;&#32423;&#32852;&#31995;&#32479;&#20013;&#36816;&#29992;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#30340;&#21487;&#34892;&#24615;&#65292;&#20197;&#20943;&#23569;&#22312;&#26597;&#35810;&#36828;&#31243;&#27169;&#22411;&#26102;&#27844;&#28431;&#31169;&#20154;&#20449;&#24687;&#30340;&#39118;&#38505;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#20010;&#38544;&#31169;&#24230;&#37327;&#12290;</title><link>https://arxiv.org/abs/2404.01041</link><description>&lt;p&gt;
LLM&#21487;&#20197;&#22312;&#19981;&#36879;&#38706;&#31169;&#20154;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#20854;&#20182;LLM&#30340;&#24110;&#21161;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can LLMs get help from other LLMs without revealing private information?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01041
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22312;&#32423;&#32852;&#31995;&#32479;&#20013;&#36816;&#29992;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#30340;&#21487;&#34892;&#24615;&#65292;&#20197;&#20943;&#23569;&#22312;&#26597;&#35810;&#36828;&#31243;&#27169;&#22411;&#26102;&#27844;&#28431;&#31169;&#20154;&#20449;&#24687;&#30340;&#39118;&#38505;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#20010;&#38544;&#31169;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32423;&#32852;&#26159;&#19968;&#31181;&#24120;&#35265;&#31867;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#65292;&#20854;&#20013;&#22914;&#26524;&#26412;&#22320;&#27169;&#22411;&#26080;&#27861;&#21333;&#29420;&#20934;&#30830;&#26631;&#35760;&#29992;&#25143;&#25968;&#25454;&#65292;&#21017;&#21487;&#20197;&#26597;&#35810;&#19968;&#20010;&#22823;&#22411;&#30340;&#36828;&#31243;&#27169;&#22411;&#12290;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#30001;&#20110;&#20854;&#22312;&#26174;&#33879;&#38477;&#20302;&#25512;&#26029;&#25104;&#26412;&#30340;&#21516;&#26102;&#20445;&#25345;&#20219;&#21153;&#24615;&#33021;&#30340;&#33021;&#21147;&#65292;&#26381;&#21153;&#22534;&#26632;&#36234;&#26469;&#36234;&#22810;&#22320;&#20351;&#29992;&#32423;&#32852;&#12290;&#28982;&#32780;&#65292;&#22312;&#26412;&#22320;&#27169;&#22411;&#21487;&#20197;&#35775;&#38382;&#25935;&#24863;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#24212;&#29992;&#32423;&#32852;&#31995;&#32479;&#26500;&#25104;&#29992;&#25143;&#30340;&#37325;&#22823;&#38544;&#31169;&#39118;&#38505;&#65292;&#22240;&#20026;&#36825;&#20123;&#25968;&#25454;&#21487;&#33021;&#34987;&#36716;&#21457;&#21040;&#36828;&#31243;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#27492;&#31867;&#35774;&#32622;&#20013;&#24212;&#29992;&#32423;&#32852;&#31995;&#32479;&#30340;&#21487;&#34892;&#24615;&#65292;&#26041;&#27861;&#26159;&#20026;&#26412;&#22320;&#27169;&#22411;&#37197;&#22791;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#65292;&#20174;&#32780;&#20943;&#23569;&#35775;&#38382;&#36828;&#31243;&#27169;&#22411;&#26102;&#27844;&#28431;&#31169;&#20154;&#20449;&#24687;&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#37327;&#21270;&#27492;&#31867;&#35774;&#32622;&#20013;&#30340;&#20449;&#24687;&#27844;&#28431;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#38544;&#31169;&#24230;&#37327;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#26368;&#36817;&#24341;&#20837;&#30340;&#31038;&#20132;&#23398;&#20064;&#33539;&#24335;&#30340;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01041v1 Announce Type: cross  Abstract: Cascades are a common type of machine learning systems in which a large, remote model can be queried if a local model is not able to accurately label a user's data by itself. Serving stacks for large language models (LLMs) increasingly use cascades due to their ability to preserve task performance while dramatically reducing inference costs. However, applying cascade systems in situations where the local model has access to sensitive data constitutes a significant privacy risk for users since such data could be forwarded to the remote model. In this work, we show the feasibility of applying cascade systems in such setups by equipping the local model with privacy-preserving techniques that reduce the risk of leaking private information when querying the remote model. To quantify information leakage in such setups, we introduce two privacy measures. We then propose a system that leverages the recently introduced social learning paradigm 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#20004;&#38454;&#27573;&#31574;&#30053;&#65292;&#24110;&#21161;&#32456;&#31471;&#29992;&#25143;&#36890;&#36807;&#26085;&#24535;&#23450;&#20301;&#37197;&#32622;&#38169;&#35823;&#30340;&#26681;&#26412;&#21407;&#22240;&#65292;&#24182;&#24320;&#21457;&#20102;&#30456;&#24212;&#24037;&#20855;LogConfigLocalizer&#65292;&#20197;&#24110;&#21161;&#32456;&#31471;&#29992;&#25143;&#36890;&#36807;&#26085;&#24535;&#20998;&#26512;&#35299;&#20915;&#37197;&#32622;&#38169;&#35823;&#12290;</title><link>https://arxiv.org/abs/2404.00640</link><description>&lt;p&gt;
&#12298;&#38754;&#23545;&#23427;&#20204;&#33258;&#24049;&#65306;&#22522;&#20110;LLM&#30340;&#20004;&#38454;&#27573;&#31574;&#30053;&#36890;&#36807;&#26085;&#24535;&#23450;&#20301;&#37197;&#32622;&#38169;&#35823;&#12299;
&lt;/p&gt;
&lt;p&gt;
Face It Yourselves: An LLM-Based Two-Stage Strategy to Localize Configuration Errors via Logs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00640
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#20004;&#38454;&#27573;&#31574;&#30053;&#65292;&#24110;&#21161;&#32456;&#31471;&#29992;&#25143;&#36890;&#36807;&#26085;&#24535;&#23450;&#20301;&#37197;&#32622;&#38169;&#35823;&#30340;&#26681;&#26412;&#21407;&#22240;&#65292;&#24182;&#24320;&#21457;&#20102;&#30456;&#24212;&#24037;&#20855;LogConfigLocalizer&#65292;&#20197;&#24110;&#21161;&#32456;&#31471;&#29992;&#25143;&#36890;&#36807;&#26085;&#24535;&#20998;&#26512;&#35299;&#20915;&#37197;&#32622;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#37197;&#32622;&#36719;&#20214;&#31995;&#32479;&#23481;&#26131;&#20986;&#29616;&#37197;&#32622;&#38169;&#35823;&#65292;&#32473;&#20844;&#21496;&#24102;&#26469;&#37325;&#22823;&#25439;&#22833;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24222;&#22823;&#32780;&#22797;&#26434;&#30340;&#37197;&#32622;&#31354;&#38388;&#65292;&#35786;&#26029;&#36825;&#20123;&#38169;&#35823;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#36825;&#20123;&#38169;&#35823;&#23545;&#26377;&#32463;&#39564;&#30340;&#32500;&#25252;&#32773;&#21644;&#27809;&#26377;&#36719;&#20214;&#31995;&#32479;&#28304;&#20195;&#30721;&#35775;&#38382;&#26435;&#38480;&#30340;&#26032;&#32456;&#31471;&#29992;&#25143;&#37117;&#26500;&#25104;&#37325;&#22823;&#25361;&#25112;&#12290;&#37492;&#20110;&#26085;&#24535;&#23545;&#22823;&#22810;&#25968;&#32456;&#31471;&#29992;&#25143;&#26469;&#35828;&#26131;&#20110;&#35775;&#38382;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#21021;&#27493;&#30740;&#31350;&#65292;&#27010;&#36848;&#20102;&#21033;&#29992;&#26085;&#24535;&#23450;&#20301;&#37197;&#32622;&#38169;&#35823;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;&#26681;&#25454;&#21021;&#27493;&#30740;&#31350;&#25152;&#24471;&#30340;&#35265;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;LLM&#30340;&#20004;&#38454;&#27573;&#31574;&#30053;&#65292;&#24110;&#21161;&#32456;&#31471;&#29992;&#25143;&#36890;&#36807;&#26085;&#24535;&#23450;&#20301;&#37197;&#32622;&#38169;&#35823;&#30340;&#26681;&#26412;&#21407;&#22240;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23454;&#29616;&#20102;&#19968;&#20010;&#24037;&#20855;LogConfigLocalizer&#65292;&#31526;&#21512;&#19978;&#36848;&#31574;&#30053;&#30340;&#35774;&#35745;&#65292;&#24076;&#26395;&#36890;&#36807;&#26085;&#24535;&#20998;&#26512;&#24110;&#21161;&#32456;&#31471;&#29992;&#25143;&#24212;&#23545;&#37197;&#32622;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00640v1 Announce Type: cross  Abstract: Configurable software systems are prone to configuration errors, resulting in significant losses to companies. However, diagnosing these errors is challenging due to the vast and complex configuration space. These errors pose significant challenges for both experienced maintainers and new end-users, particularly those without access to the source code of the software systems. Given that logs are easily accessible to most end-users, we conduct a preliminary study to outline the challenges and opportunities of utilizing logs in localizing configuration errors. Based on the insights gained from the preliminary study, we propose an LLM-based two-stage strategy for end-users to localize the root-cause configuration properties based on logs. We further implement a tool, LogConfigLocalizer, aligned with the design of the aforementioned strategy, hoping to assist end-users in coping with configuration errors through log analysis.   To the best
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;CHAIN&#65292;&#35813;&#26041;&#27861;&#22312;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#35299;&#20915;&#20102;GANs&#20013;&#37492;&#21035;&#22120;&#36807;&#25311;&#21512;&#21644;&#35757;&#32451;&#19981;&#31283;&#23450;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#27867;&#21270;&#33021;&#21147;&#21644;&#35757;&#32451;&#31283;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.00521</link><description>&lt;p&gt;
CHAIN&#65306;&#36890;&#36807;&#21463;&#38480;&#21807;&#19968;&#24615;&#36830;&#32493;&#24615;&#35268;&#33539;&#21270;&#22686;&#24378;&#25968;&#25454;&#39640;&#25928;GANs&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
CHAIN: Enhancing Generalization in Data-Efficient GANs via lipsCHitz continuity constrAIned Normalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00521
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;CHAIN&#65292;&#35813;&#26041;&#27861;&#22312;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#35299;&#20915;&#20102;GANs&#20013;&#37492;&#21035;&#22120;&#36807;&#25311;&#21512;&#21644;&#35757;&#32451;&#19981;&#31283;&#23450;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#27867;&#21270;&#33021;&#21147;&#21644;&#35757;&#32451;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#26174;&#30528;&#25512;&#21160;&#20102;&#22270;&#20687;&#29983;&#25104;&#65292;&#20294;&#23427;&#20204;&#30340;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#22312;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;GANs&#32463;&#24120;&#38754;&#20020;&#37492;&#21035;&#22120;&#36807;&#25311;&#21512;&#21644;&#35757;&#32451;&#19981;&#31283;&#23450;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#36890;&#36807;&#35782;&#21035;Batch Normalization&#65288;BN&#65289;&#20013;&#30340;&#20851;&#38190;&#32570;&#38519;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65306;&#22312;&#20013;&#24515;&#21270;&#21644;&#32553;&#25918;&#27493;&#39588;&#20013;&#26799;&#24230;&#29190;&#28856;&#30340;&#20542;&#21521;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CHAIN&#65288;&#21463;&#38480;&#21807;&#19968;&#24615;&#36830;&#32493;&#24615;&#35268;&#33539;&#21270;&#65289;&#65292;&#23427;&#23558;&#20256;&#32479;&#30340;&#20013;&#24515;&#21270;&#27493;&#39588;&#26367;&#25442;&#20026;&#38646;&#22343;&#20540;&#27491;&#21017;&#21270;&#65292;&#24182;&#22312;&#32553;&#25918;&#27493;&#39588;&#20013;&#38598;&#25104;&#20102;Lipschitz&#36830;&#32493;&#24615;&#32422;&#26463;&#12290;CHAIN&#36890;&#36807;&#33258;&#36866;&#24212;&#25554;&#20540;&#24402;&#19968;&#21270;&#21644;&#38750;&#24402;&#19968;&#21270;&#29305;&#24449;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;GANs&#30340;&#35757;&#32451;&#65292;&#26377;&#25928;&#36991;&#20813;&#20102;&#37492;&#21035;&#22120;&#36807;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00521v1 Announce Type: new  Abstract: Generative Adversarial Networks (GANs) significantly advanced image generation but their performance heavily depends on abundant training data. In scenarios with limited data, GANs often struggle with discriminator overfitting and unstable training. Batch Normalization (BN), despite being known for enhancing generalization and training stability, has rarely been used in the discriminator of Data-Efficient GANs. Our work addresses this gap by identifying a critical flaw in BN: the tendency for gradient explosion during the centering and scaling steps. To tackle this issue, we present CHAIN (lipsCHitz continuity constrAIned Normalization), which replaces the conventional centering step with zero-mean regularization and integrates a Lipschitz continuity constraint in the scaling step. CHAIN further enhances GAN training by adaptively interpolating the normalized and unnormalized features, effectively avoiding discriminator overfitting. Our 
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22522;&#30784;&#19990;&#30028;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#30452;&#25509;&#39044;&#27979;&#22240;&#26524;&#26410;&#26469;&#29366;&#24577;&#30340;&#26041;&#27861;&#65292;&#22312;&#23433;&#20840;&#39044;&#27979;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#26631;&#20934;&#19990;&#30028;&#27169;&#22411;&#65292;&#24182;&#19988;&#24615;&#33021;&#19982;&#30417;&#30563;&#23398;&#20064;&#30456;&#24403;&#65292;&#23613;&#31649;&#27809;&#26377;&#20351;&#29992;&#20219;&#20309;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2404.00462</link><description>&lt;p&gt;
&#20855;&#26377;&#22522;&#30784;&#19990;&#30028;&#27169;&#22411;&#30340;&#26080;&#20154;&#39550;&#39542;&#27773;&#36710;&#38646;&#23556;&#20987;&#23433;&#20840;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Zero-shot Safety Prediction for Autonomous Robots with Foundation World Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00462
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22522;&#30784;&#19990;&#30028;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#30452;&#25509;&#39044;&#27979;&#22240;&#26524;&#26410;&#26469;&#29366;&#24577;&#30340;&#26041;&#27861;&#65292;&#22312;&#23433;&#20840;&#39044;&#27979;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#26631;&#20934;&#19990;&#30028;&#27169;&#22411;&#65292;&#24182;&#19988;&#24615;&#33021;&#19982;&#30417;&#30563;&#23398;&#20064;&#30456;&#24403;&#65292;&#23613;&#31649;&#27809;&#26377;&#20351;&#29992;&#20219;&#20309;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19990;&#30028;&#27169;&#22411;&#21019;&#36896;&#19968;&#20010;&#20195;&#29702;&#19990;&#30028;&#26469;&#35757;&#32451;&#25511;&#21046;&#22120;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#31995;&#32479;&#30340;&#20869;&#37096;&#21160;&#24577;&#27169;&#22411;&#26469;&#39044;&#27979;&#23433;&#20840;&#36829;&#35268;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#19990;&#30028;&#27169;&#22411;&#20165;&#20381;&#36182;&#20110;&#32479;&#35745;&#23398;&#20064;&#22914;&#20309;&#35266;&#23519;&#38543;&#30528;&#34892;&#21160;&#32780;&#21464;&#21270;&#65292;&#32570;&#20047;&#23545;&#20195;&#29702;&#21160;&#24577;&#20934;&#30830;&#24615;&#30340;&#31934;&#30830;&#37327;&#21270;&#65292;&#36825;&#22312;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#20013;&#26500;&#25104;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;&#35266;&#23519;&#23884;&#20837;&#21040;&#26377;&#24847;&#20041;&#19988;&#22240;&#26524;&#28508;&#22312;&#30340;&#34920;&#31034;&#20013;&#30340;&#22522;&#30784;&#19990;&#30028;&#27169;&#22411;&#12290;&#36825;&#20351;&#24471;&#20195;&#29702;&#21160;&#24577;&#33021;&#22815;&#36890;&#36807;&#21033;&#29992;&#26080;&#38656;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30452;&#25509;&#39044;&#27979;&#22240;&#26524;&#26410;&#26469;&#29366;&#24577;&#12290;&#22312;&#20004;&#20010;&#24120;&#35265;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#36825;&#31181;&#26032;&#39062;&#27169;&#22411;&#22312;&#23433;&#20840;&#39044;&#27979;&#20219;&#21153;&#20013;&#20248;&#20110;&#26631;&#20934;&#19990;&#30028;&#27169;&#22411;&#65292;&#24182;&#19988;&#24615;&#33021;&#19982;&#30417;&#30563;&#23398;&#20064;&#30456;&#24403;&#65292;&#23613;&#31649;&#27809;&#26377;&#20351;&#29992;&#20219;&#20309;&#25968;&#25454;&#12290;&#25105;&#20204;&#36890;&#36807;&#27604;&#36739;&#26356;&#19987;&#19994;&#21644;&#31995;&#32479;&#30456;&#20851;&#30340;&#25351;&#26631;&#35780;&#20272;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00462v1 Announce Type: new  Abstract: A world model creates a surrogate world to train a controller and predict safety violations by learning the internal dynamic model of systems. However, the existing world models rely solely on statistical learning of how observations change in response to actions, lacking precise quantification of how accurate the surrogate dynamics are, which poses a significant challenge in safety-critical systems. To address this challenge, we propose foundation world models that embed observations into meaningful and causally latent representations. This enables the surrogate dynamics to directly predict causal future states by leveraging a training-free large language model. In two common benchmarks, this novel model outperforms standard world models in the safety prediction task and has a performance comparable to supervised learning despite not using any data. We evaluate its performance with a more specialized and system-relevant metric by compar
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#8220;&#35268;&#21010;&#19982;&#26816;&#32034;&#8221;&#21644;&#8220;&#32534;&#36753;&#19982;&#30830;&#35748;&#8221;&#33539;&#24335;&#65292;&#36890;&#36807;&#31070;&#32463;&#26816;&#32034;&#27169;&#22359;&#21644;LLM-based&#26597;&#35810;&#35268;&#21010;&#22120;&#25552;&#39640;&#20102;&#24037;&#20855;&#21033;&#29992;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2404.00450</link><description>&lt;p&gt;
&#35268;&#21010;&#21644;&#32534;&#36753;&#26816;&#32034;&#20197;&#22686;&#24378;&#24037;&#20855;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Planning and Editing What You Retrieve for Enhanced Tool Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00450
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#8220;&#35268;&#21010;&#19982;&#26816;&#32034;&#8221;&#21644;&#8220;&#32534;&#36753;&#19982;&#30830;&#35748;&#8221;&#33539;&#24335;&#65292;&#36890;&#36807;&#31070;&#32463;&#26816;&#32034;&#27169;&#22359;&#21644;LLM-based&#26597;&#35810;&#35268;&#21010;&#22120;&#25552;&#39640;&#20102;&#24037;&#20855;&#21033;&#29992;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#23558;&#22806;&#37096;&#24037;&#20855;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38598;&#25104;&#26041;&#38754;&#21462;&#24471;&#30340;&#36827;&#23637;&#25171;&#24320;&#20102;&#26032;&#30340;&#39046;&#22495;&#65292;&#24212;&#29992;&#33539;&#22260;&#28085;&#30422;&#25968;&#23398;&#25512;&#29702;&#12289;&#20195;&#30721;&#29983;&#25104;&#22120;&#21644;&#26234;&#33021;&#21161;&#25163;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#31616;&#21333;&#30340;&#19968;&#27425;&#24615;&#26816;&#32034;&#31574;&#30053;&#65292;&#26080;&#27861;&#26377;&#25928;&#20934;&#30830;&#22320;&#31579;&#36873;&#30456;&#20851;&#24037;&#20855;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#8220;&#35268;&#21010;&#19982;&#26816;&#32034;&#65288;P&amp;R&#65289;&#8221;&#21644;&#8220;&#32534;&#36753;&#19982;&#30830;&#35748;&#65288;E&amp;G&#65289;&#8221;&#33539;&#24335;&#30340;&#27169;&#22411;&#65292;&#21253;&#25324;&#20102;&#31070;&#32463;&#26816;&#32034;&#27169;&#22359;&#21644;&#22522;&#20110;LLM&#30340;&#26597;&#35810;&#35268;&#21010;&#22120;&#65292;&#20197;&#22686;&#24378;&#24037;&#20855;&#21033;&#29992;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00450v1 Announce Type: new  Abstract: Recent advancements in integrating external tools with Large Language Models (LLMs) have opened new frontiers, with applications in mathematical reasoning, code generators, and smart assistants. However, existing methods, relying on simple one-time retrieval strategies, fall short on effectively and accurately shortlisting relevant tools. This paper introduces a novel \modelname (\modelmeaning) approach, encompassing ``Plan-and-Retrieve (P\&amp;R)'' and ``Edit-and-Ground (E\&amp;G)'' paradigms. The P\&amp;R paradigm consists of a neural retrieval module for shortlisting relevant tools and an LLM-based query planner that decomposes complex queries into actionable tasks, enhancing the effectiveness of tool utilization. The E\&amp;G paradigm utilizes LLMs to enrich tool descriptions based on user scenarios, bridging the gap between user queries and tool functionalities. Experiment results demonstrate that these paradigms significantly improve the recall an
&lt;/p&gt;</description></item><item><title>InfLoRA&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;PEFT&#26041;&#27861;&#65292;&#21517;&#20026;&#26080;&#24178;&#25200;&#20302;&#31209;&#33258;&#36866;&#24212;&#65288;InfLoRA&#65289;&#65292;&#29992;&#20110;&#25345;&#32493;&#23398;&#20064;&#65292;&#26088;&#22312;&#28040;&#38500;&#26032;&#20219;&#21153;&#23545;&#26087;&#20219;&#21153;&#30340;&#24178;&#25200;&#65292;&#24110;&#21161;&#27169;&#22411;&#22312;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#20043;&#38388;&#21462;&#24471;&#33391;&#22909;&#24179;&#34913;&#12290;</title><link>https://arxiv.org/abs/2404.00228</link><description>&lt;p&gt;
InfLoRA&#65306;&#26080;&#24178;&#25200;&#30340;&#20302;&#31209;&#33258;&#36866;&#24212;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
InfLoRA: Interference-Free Low-Rank Adaptation for Continual Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00228
&lt;/p&gt;
&lt;p&gt;
InfLoRA&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;PEFT&#26041;&#27861;&#65292;&#21517;&#20026;&#26080;&#24178;&#25200;&#20302;&#31209;&#33258;&#36866;&#24212;&#65288;InfLoRA&#65289;&#65292;&#29992;&#20110;&#25345;&#32493;&#23398;&#20064;&#65292;&#26088;&#22312;&#28040;&#38500;&#26032;&#20219;&#21153;&#23545;&#26087;&#20219;&#21153;&#30340;&#24178;&#25200;&#65292;&#24110;&#21161;&#27169;&#22411;&#22312;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#20043;&#38388;&#21462;&#24471;&#33391;&#22909;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#35201;&#27714;&#27169;&#22411;&#20381;&#27425;&#23398;&#20064;&#22810;&#20010;&#20219;&#21153;&#12290;&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#65292;&#27169;&#22411;&#24212;&#20855;&#22791;&#22312;&#26087;&#20219;&#21153;&#19978;&#32500;&#25345;&#24615;&#33021;&#65288;&#31283;&#23450;&#24615;&#65289;&#21644;&#19981;&#26029;&#36866;&#24212;&#26032;&#20219;&#21153;&#30340;&#33021;&#21147;&#65288;&#21487;&#22609;&#24615;&#65289;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#23613;&#31649;&#29616;&#26377;&#22522;&#20110;PEFT&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#34920;&#29616;&#20986;&#27604;&#38750;PEFT&#26041;&#27861;&#26356;&#20248;&#31168;&#30340;&#24615;&#33021;&#65292;&#20294;&#22823;&#22810;&#25968;&#26041;&#27861;&#24182;&#26410;&#32771;&#34385;&#22914;&#20309;&#28040;&#38500;&#26032;&#20219;&#21153;&#23545;&#26087;&#20219;&#21153;&#30340;&#24178;&#25200;&#65292;&#20174;&#32780;&#38459;&#30861;&#27169;&#22411;&#22312;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#20043;&#38388;&#21462;&#24471;&#33391;&#22909;&#24179;&#34913;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;PEFT&#26041;&#27861;&#65292;&#31216;&#20026;&#26080;&#24178;&#25200;&#20302;&#31209;&#33258;&#36866;&#24212;&#65288;InfLoRA&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#25345;&#32493;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00228v1 Announce Type: cross  Abstract: Continual learning requires the model to learn multiple tasks sequentially. In continual learning, the model should possess the ability to maintain its performance on old tasks (stability) and the ability to adapt to new tasks continuously (plasticity). Recently, parameter-efficient fine-tuning (PEFT), which involves freezing a pre-trained model and injecting a small number of learnable parameters to adapt to downstream tasks, has gained increasing popularity in continual learning. Although existing continual learning methods based on PEFT have demonstrated superior performance compared to those not based on PEFT, most of them do not consider how to eliminate the interference of the new task on the old tasks, which inhibits the model from making a good trade-off between stability and plasticity. In this work, we propose a new PEFT method, called interference-free low-rank adaptation (InfLoRA), for continual learning. InfLoRA injects a 
&lt;/p&gt;</description></item><item><title>&#25506;&#35752;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20889;&#20316;&#21161;&#25163;&#24341;&#21457;&#30340;&#20889;&#20316;&#25152;&#26377;&#26435;&#24863;&#21644;&#20316;&#32773;&#36523;&#20221;&#35748;&#30693;&#20043;&#38388;&#30340;&#24515;&#29702;&#22256;&#22659;&#12290;</title><link>https://arxiv.org/abs/2404.00027</link><description>&lt;p&gt;
LLM&#20316;&#20026;&#20889;&#20316;&#21161;&#25163;&#65306;&#25506;&#35752;&#25152;&#26377;&#26435;&#24863;&#21644;&#25512;&#29702;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
LLMs as Writing Assistants: Exploring Perspectives on Sense of Ownership and Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00027
&lt;/p&gt;
&lt;p&gt;
&#25506;&#35752;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20889;&#20316;&#21161;&#25163;&#24341;&#21457;&#30340;&#20889;&#20316;&#25152;&#26377;&#26435;&#24863;&#21644;&#20316;&#32773;&#36523;&#20221;&#35748;&#30693;&#20043;&#38388;&#30340;&#24515;&#29702;&#22256;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20889;&#20316;&#20013;&#30340;&#25152;&#26377;&#26435;&#24863;&#38480;&#21046;&#20102;&#25105;&#20204;&#23545;&#24605;&#24819;&#12289;&#26102;&#38388;&#21644;&#36129;&#29486;&#30340;&#25237;&#20837;&#65292;&#23548;&#33268;&#23545;&#20135;&#20986;&#29289;&#30340;&#20381;&#24651;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#20889;&#20316;&#21161;&#25163;&#24341;&#20837;&#20102;&#19968;&#31181;&#24515;&#29702;&#22256;&#22659;&#65292;&#22240;&#20026;&#19968;&#20123;&#20869;&#23481;&#24182;&#38750;&#30452;&#25509;&#25105;&#20204;&#30340;&#21019;&#20316;&#12290;&#25105;&#20204;&#24448;&#24448;&#26356;&#20542;&#21521;&#20110;&#22312;&#21019;&#36896;&#24615;&#20219;&#21153;&#20013;&#26356;&#22810;&#22320;&#24402;&#21151;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#23613;&#31649;&#23427;&#20204;&#23545;&#25152;&#26377;&#20219;&#21153;&#37117;&#26159;&#24179;&#31561;&#30340;&#12290;&#27492;&#22806;&#65292;&#34429;&#28982;&#25105;&#20204;&#21487;&#33021;&#19981;&#20250;&#23436;&#20840;&#22768;&#31216;&#23545;&#30001;LLM&#29983;&#25104;&#30340;&#20869;&#23481;&#25317;&#26377;&#25152;&#26377;&#26435;&#65292;&#20294;&#21364;&#33258;&#30001;&#22320;&#22768;&#31216;&#20316;&#32773;&#36523;&#20221;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#31616;&#30701;&#35843;&#26597;&#26469;&#30740;&#31350;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#20102;&#35299;&#28508;&#22312;&#30340;&#35748;&#30693;&#36807;&#31243;&#65292;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#20154;&#26426;&#20132;&#20114;&#22312;&#20889;&#20316;&#20013;&#30340;&#24212;&#29992;&#24182;&#25913;&#36827;&#20889;&#20316;&#36741;&#21161;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00027v1 Announce Type: cross  Abstract: Sense of ownership in writing confines our investment of thoughts, time, and contribution, leading to attachment to the output. However, using writing assistants introduces a mental dilemma, as some content isn't directly our creation. For instance, we tend to credit Large Language Models (LLMs) more in creative tasks, even though all tasks are equal for them. Additionally, while we may not claim complete ownership of LLM-generated content, we freely claim authorship. We conduct a short survey to examine these issues and understand underlying cognitive processes in order to gain a better knowledge of human-computer interaction in writing and improve writing aid systems.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#35752;&#20102;&#20154;&#20204;&#26085;&#30410;&#20381;&#36182;&#30340;&#22522;&#20110;LLM&#30340;&#20889;&#20316;&#21161;&#25163;&#23545;&#21019;&#36896;&#21147;&#21644;&#20010;&#24615;&#21487;&#33021;&#36896;&#25104;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#26088;&#22312;&#25913;&#36827;&#20154;&#26426;&#20132;&#20114;&#31995;&#32479;&#21644;&#25552;&#21319;&#20889;&#20316;&#21161;&#25163;&#30340;&#20010;&#24615;&#21270;&#21644;&#20010;&#24615;&#21270;&#21151;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.00026</link><description>&lt;p&gt;
&#22696;&#27700;&#19982;&#20010;&#24615;&#65306;&#22312;LLMs&#26102;&#20195;&#22609;&#36896;&#20010;&#24615;&#21270;&#21465;&#20107;
&lt;/p&gt;
&lt;p&gt;
Ink and Individuality: Crafting a Personalised Narrative in the Age of LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00026
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#35752;&#20102;&#20154;&#20204;&#26085;&#30410;&#20381;&#36182;&#30340;&#22522;&#20110;LLM&#30340;&#20889;&#20316;&#21161;&#25163;&#23545;&#21019;&#36896;&#21147;&#21644;&#20010;&#24615;&#21487;&#33021;&#36896;&#25104;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#26088;&#22312;&#25913;&#36827;&#20154;&#26426;&#20132;&#20114;&#31995;&#32479;&#21644;&#25552;&#21319;&#20889;&#20316;&#21161;&#25163;&#30340;&#20010;&#24615;&#21270;&#21644;&#20010;&#24615;&#21270;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21644;&#20010;&#24615;&#21270;&#26500;&#25104;&#20102;&#20351;&#27599;&#20010;&#20316;&#23478;&#29420;&#29305;&#24182;&#24433;&#21709;&#20854;&#25991;&#23383;&#20197;&#26377;&#25928;&#21560;&#24341;&#35835;&#32773;&#21516;&#26102;&#20256;&#36798;&#30495;&#23454;&#24615;&#30340;&#29420;&#29305;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#26085;&#30410;&#20381;&#36182;&#22522;&#20110;LLM&#30340;&#20889;&#20316;&#21161;&#25163;&#21487;&#33021;&#20250;&#21361;&#21450;&#25105;&#20204;&#30340;&#21019;&#36896;&#21147;&#21644;&#20010;&#24615;&#12290;&#25105;&#20204;&#32463;&#24120;&#24573;&#35270;&#36825;&#19968;&#36235;&#21183;&#23545;&#25105;&#20204;&#30340;&#21019;&#36896;&#21147;&#21644;&#29420;&#29305;&#24615;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#23613;&#31649;&#21487;&#33021;&#20250;&#36896;&#25104;&#21518;&#26524;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#36827;&#34892;&#31616;&#35201;&#35843;&#26597;&#25506;&#32034;&#19981;&#21516;&#30340;&#35266;&#28857;&#21644;&#27010;&#24565;&#65292;&#20197;&#21450;&#23581;&#35797;&#29702;&#35299;&#20154;&#20204;&#30340;&#35266;&#28857;&#65292;&#32467;&#21512;&#20197;&#24448;&#22312;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#26469;&#30740;&#31350;&#36825;&#20123;&#38382;&#39064;&#12290;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#23545;&#20110;&#25913;&#36827;&#20154;&#26426;&#20132;&#20114;&#31995;&#32479;&#21644;&#22686;&#24378;&#20010;&#24615;&#21270;&#21644;&#20010;&#24615;&#21270;&#20889;&#20316;&#21161;&#25163;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00026v1 Announce Type: cross  Abstract: Individuality and personalization comprise the distinctive characteristics that make each writer unique and influence their words in order to effectively engage readers while conveying authenticity. However, our growing reliance on LLM-based writing assistants risks compromising our creativity and individuality over time. We often overlook the negative impacts of this trend on our creativity and uniqueness, despite the possible consequences. This study investigates these concerns by performing a brief survey to explore different perspectives and concepts, as well as trying to understand people's viewpoints, in conjunction with past studies in the area. Addressing these issues is essential for improving human-computer interaction systems and enhancing writing assistants for personalization and individuality.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Systemic Quantum Score (SQS)&#30340;&#26032;&#26041;&#27861;&#65292;&#23637;&#31034;&#22312;&#37329;&#34701;&#39046;&#22495;&#29983;&#20135;&#32423;&#24212;&#29992;&#26696;&#20363;&#20013;&#30456;&#27604;&#32431;&#32463;&#20856;&#27169;&#22411;&#26356;&#26377;&#20248;&#21183;&#65292;&#33021;&#22815;&#20174;&#36739;&#23569;&#25968;&#25454;&#28857;&#20013;&#25552;&#21462;&#27169;&#24335;&#24182;&#34920;&#29616;&#20986;&#26356;&#22909;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.00015</link><description>&lt;p&gt;
&#21033;&#29992;&#37327;&#23376;&#22686;&#24378;&#26426;&#22120;&#23398;&#20064;&#36171;&#33021;&#20449;&#29992;&#35780;&#20998;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Empowering Credit Scoring Systems with Quantum-Enhanced Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00015
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Systemic Quantum Score (SQS)&#30340;&#26032;&#26041;&#27861;&#65292;&#23637;&#31034;&#22312;&#37329;&#34701;&#39046;&#22495;&#29983;&#20135;&#32423;&#24212;&#29992;&#26696;&#20363;&#20013;&#30456;&#27604;&#32431;&#32463;&#20856;&#27169;&#22411;&#26356;&#26377;&#20248;&#21183;&#65292;&#33021;&#22815;&#20174;&#36739;&#23569;&#25968;&#25454;&#28857;&#20013;&#25552;&#21462;&#27169;&#24335;&#24182;&#34920;&#29616;&#20986;&#26356;&#22909;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Quantum Kernels&#34987;&#35748;&#20026;&#22312;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#30340;&#26089;&#26399;&#38454;&#27573;&#25552;&#20379;&#20102;&#26377;&#29992;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#21033;&#29992;&#24222;&#22823;&#25968;&#25454;&#38598;&#26102;&#65292;&#39640;&#24230;&#22797;&#26434;&#30340;&#32463;&#20856;&#27169;&#22411;&#24456;&#38590;&#36229;&#36234;&#65292;&#29305;&#21035;&#26159;&#22312;&#29702;&#35299;&#21147;&#26041;&#38754;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#19968;&#26086;&#25968;&#25454;&#31232;&#32570;&#19988;&#20542;&#26012;&#65292;&#32463;&#20856;&#27169;&#22411;&#23601;&#20250;&#36935;&#21040;&#22256;&#38590;&#12290;&#37327;&#23376;&#29305;&#24449;&#31354;&#38388;&#34987;&#39044;&#35745;&#22312;&#36825;&#26679;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#26223;&#20013;&#33021;&#22815;&#25214;&#21040;&#26356;&#22909;&#30340;&#25968;&#25454;&#29305;&#24449;&#21644;&#30446;&#26631;&#31867;&#21035;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#26368;&#37325;&#35201;&#30340;&#26159;&#22686;&#24378;&#20102;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Systemic Quantum Score (SQS)&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#21021;&#27493;&#32467;&#26524;&#65292;&#34920;&#26126;&#22312;&#37329;&#34701;&#34892;&#19994;&#29983;&#20135;&#32423;&#24212;&#29992;&#26696;&#20363;&#20013;&#65292;SQS&#21487;&#33021;&#27604;&#32431;&#32463;&#20856;&#27169;&#22411;&#20855;&#26377;&#20248;&#21183;&#12290;&#25105;&#20204;&#30340;&#20855;&#20307;&#30740;&#31350;&#34920;&#26126;&#65292;SQS&#33021;&#22815;&#20174;&#36739;&#23569;&#30340;&#25968;&#25454;&#28857;&#20013;&#25552;&#21462;&#20986;&#27169;&#24335;&#65292;&#24182;&#19988;&#22312;&#25968;&#25454;&#38656;&#27714;&#37327;&#22823;&#30340;&#31639;&#27861;&#65288;&#22914;XGBoost&#65289;&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24102;&#26469;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00015v1 Announce Type: cross  Abstract: Quantum Kernels are projected to provide early-stage usefulness for quantum machine learning. However, highly sophisticated classical models are hard to surpass without losing interpretability, particularly when vast datasets can be exploited. Nonetheless, classical models struggle once data is scarce and skewed. Quantum feature spaces are projected to find better links between data features and the target class to be predicted even in such challenging scenarios and most importantly, enhanced generalization capabilities. In this work, we propose a novel approach called Systemic Quantum Score (SQS) and provide preliminary results indicating potential advantage over purely classical models in a production grade use case for the Finance sector. SQS shows in our specific study an increased capacity to extract patterns out of fewer data points as well as improved performance over data-hungry algorithms such as XGBoost, providing advantage i
&lt;/p&gt;</description></item><item><title>FairRAG&#26694;&#26550;&#36890;&#36807;&#22312;&#22806;&#37096;&#22270;&#20687;&#25968;&#25454;&#24211;&#26816;&#32034;&#21040;&#30340;&#21442;&#32771;&#22270;&#20687;&#26469;&#25552;&#39640;&#20154;&#31867;&#29983;&#25104;&#20013;&#30340;&#20844;&#24179;&#24615;&#65292;&#24182;&#24212;&#29992;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#21435;&#20559;&#31574;&#30053;&#65292;&#20174;&#32780;&#20026;&#29983;&#25104;&#36807;&#31243;&#25552;&#20379;&#26469;&#33258;&#19981;&#21516;&#20154;&#21475;&#32479;&#35745;&#32452;&#30340;&#22270;&#20687;&#12290;</title><link>https://arxiv.org/abs/2403.19964</link><description>&lt;p&gt;
FairRAG: &#20844;&#24179;&#20154;&#31867;&#29983;&#25104;&#30340;&#20844;&#24179;&#26816;&#32034;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
FairRAG: Fair Human Generation via Fair Retrieval Augmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19964
&lt;/p&gt;
&lt;p&gt;
FairRAG&#26694;&#26550;&#36890;&#36807;&#22312;&#22806;&#37096;&#22270;&#20687;&#25968;&#25454;&#24211;&#26816;&#32034;&#21040;&#30340;&#21442;&#32771;&#22270;&#20687;&#26469;&#25552;&#39640;&#20154;&#31867;&#29983;&#25104;&#20013;&#30340;&#20844;&#24179;&#24615;&#65292;&#24182;&#24212;&#29992;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#21435;&#20559;&#31574;&#30053;&#65292;&#20174;&#32780;&#20026;&#29983;&#25104;&#36807;&#31243;&#25552;&#20379;&#26469;&#33258;&#19981;&#21516;&#20154;&#21475;&#32479;&#35745;&#32452;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23384;&#22312;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#21453;&#26144;&#29978;&#33267;&#25918;&#22823;&#20102;&#20854;&#35757;&#32451;&#25968;&#25454;&#20013;&#26681;&#28145;&#33922;&#22266;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;&#36825;&#23545;&#20154;&#31867;&#22270;&#20687;&#29983;&#25104;&#23588;&#20026;&#20196;&#20154;&#25285;&#24551;&#65292;&#22240;&#20026;&#27169;&#22411;&#20559;&#21521;&#26576;&#20123;&#20154;&#21475;&#32479;&#35745;&#32452;&#12290;&#29616;&#26377;&#30340;&#32416;&#27491;&#27492;&#38382;&#39064;&#30340;&#23581;&#35797;&#21463;&#21040;&#39044;&#35757;&#32451;&#27169;&#22411;&#22266;&#26377;&#38480;&#21046;&#30340;&#24433;&#21709;&#65292;&#24182;&#26410;&#33021;&#22312;&#26681;&#26412;&#19978;&#25913;&#21892;&#20154;&#21475;&#22810;&#26679;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20844;&#24179;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;FairRAG&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#26469;&#33258;&#22806;&#37096;&#22270;&#20687;&#25968;&#25454;&#24211;&#30340;&#21442;&#32771;&#22270;&#20687;&#19978;&#36827;&#34892;&#26465;&#20214;&#21270;&#26469;&#25552;&#39640;&#20154;&#31867;&#29983;&#25104;&#20013;&#30340;&#20844;&#24179;&#24615;&#12290;FairRAG&#36890;&#36807;&#19968;&#20010;&#36731;&#37327;&#32423;&#32447;&#24615;&#27169;&#22359;&#23454;&#29616;&#26465;&#20214;&#21270;&#65292;&#23558;&#21442;&#32771;&#22270;&#20687;&#25237;&#23556;&#21040;&#25991;&#26412;&#31354;&#38388;&#20013;&#12290;&#20026;&#20102;&#22686;&#24378;&#20844;&#24179;&#24615;&#65292;FairRAG&#24212;&#29992;&#20102;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#21435;&#20559;&#26041;&#27861;&#65292;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#25552;&#20379;&#26469;&#33258;&#19981;&#21516;&#20154;&#21475;&#32479;&#35745;&#32452;&#30340;&#22270;&#20687;&#12290;&#22823;&#37327;&#23454;&#39564;&#23637;&#31034;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19964v1 Announce Type: cross  Abstract: Existing text-to-image generative models reflect or even amplify societal biases ingrained in their training data. This is especially concerning for human image generation where models are biased against certain demographic groups. Existing attempts to rectify this issue are hindered by the inherent limitations of the pre-trained models and fail to substantially improve demographic diversity. In this work, we introduce Fair Retrieval Augmented Generation (FairRAG), a novel framework that conditions pre-trained generative models on reference images retrieved from an external image database to improve fairness in human generation. FairRAG enables conditioning through a lightweight linear module that projects reference images into the textual space. To enhance fairness, FairRAG applies simple-yet-effective debiasing strategies, providing images from diverse demographic groups during the generative process. Extensive experiments demonstrat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36879;&#38236;&#65292;&#36890;&#36807;&#20854;&#38544;&#21547;&#30340;&#39640;&#23618;&#27425;&#27010;&#24565;&#26469;&#36827;&#34892;&#23545;&#35270;&#35273;&#27169;&#22411;&#30340;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2403.19837</link><description>&lt;p&gt;
&#36890;&#36807;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#23545;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22522;&#20110;&#27010;&#24565;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Concept-based Analysis of Neural Networks via Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19837
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36879;&#38236;&#65292;&#36890;&#36807;&#20854;&#38544;&#21547;&#30340;&#39640;&#23618;&#27425;&#27010;&#24565;&#26469;&#36827;&#34892;&#23545;&#35270;&#35273;&#27169;&#22411;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30340;&#24418;&#24335;&#21270;&#20998;&#26512;&#38750;&#24120;&#21487;&#21462;&#65292;&#20294;&#30001;&#20110;&#38590;&#20197;&#34920;&#36798;&#35270;&#35273;&#20219;&#21153;&#30340;&#24418;&#24335;&#21270;&#35268;&#33539;&#20197;&#21450;&#32570;&#20047;&#39640;&#25928;&#30340;&#39564;&#35777;&#31243;&#24207;&#65292;&#36825;&#26159;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#26032;&#20852;&#30340;&#22810;&#27169;&#24577;&#12289;&#35270;&#35273;&#35821;&#35328;&#12289;&#22522;&#30784;&#27169;&#22411;&#65288;VLMs&#65289;&#20316;&#20026;&#19968;&#31181;&#36890;&#36807;&#20854;&#21487;&#20197;&#25512;&#29702;&#35270;&#35273;&#27169;&#22411;&#30340;&#36879;&#38236;&#12290;VLMs&#24050;&#32463;&#22312;&#22823;&#37327;&#22270;&#20687;&#21450;&#20854;&#25991;&#26412;&#25551;&#36848;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#65292;&#22240;&#27492;&#38544;&#24335;&#22320;&#20102;&#35299;&#25551;&#36848;&#36825;&#20123;&#22270;&#20687;&#30340;&#39640;&#23618;&#27425;&#12289;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#21517;&#20026;$\texttt{Con}_{\texttt{spec}}$&#30340;&#36923;&#36753;&#35268;&#33539;&#35821;&#35328;&#65292;&#26088;&#22312;&#20415;&#20110;&#25353;&#29031;&#36825;&#20123;&#27010;&#24565;&#32534;&#20889;&#35268;&#33539;&#12290;&#20026;&#20102;&#23450;&#20041;&#21644;&#24418;&#24335;&#21270;&#26816;&#26597;$\texttt{Con}_{\texttt{spec}}$&#35268;&#33539;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#19968;&#20010;VLM&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#31181;&#32534;&#30721;&#21644;&#39640;&#25928;&#26816;&#26597;&#35270;&#35273;&#27169;&#22411;&#30340;&#33258;&#28982;&#35821;&#35328;&#23646;&#24615;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;te
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19837v1 Announce Type: cross  Abstract: Formal analysis of vision-based deep neural networks (DNNs) is highly desirable but it is very challenging due to the difficulty of expressing formal specifications for vision tasks and the lack of efficient verification procedures. In this paper, we propose to leverage emerging multimodal, vision-language, foundation models (VLMs) as a lens through which we can reason about vision models. VLMs have been trained on a large body of images accompanied by their textual description, and are thus implicitly aware of high-level, human-understandable concepts describing the images. We describe a logical specification language $\texttt{Con}_{\texttt{spec}}$ designed to facilitate writing specifications in terms of these concepts. To define and formally check $\texttt{Con}_{\texttt{spec}}$ specifications, we leverage a VLM, which provides a means to encode and efficiently check natural-language properties of vision models. We demonstrate our te
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;ViT&#27169;&#22411;&#29983;&#25104;&#30340;&#20840;&#23616;&#22270;&#20687;&#20808;&#39564;&#65292;&#20026;&#21333;&#22270;&#28145;&#24230;&#20272;&#35745;&#27169;&#22411;&#25552;&#20379;&#26356;&#35814;&#32454;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20351;&#29992;&#25193;&#25955;&#39592;&#24178;&#19988;&#21463;ViT&#23884;&#20837;&#26465;&#20214;&#32422;&#26463;&#30340;&#28145;&#24230;&#20272;&#35745;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.18807</link><description>&lt;p&gt;
ECoDepth: &#26377;&#25928;&#35843;&#25972;&#25193;&#25955;&#27169;&#22411;&#20197;&#29992;&#20110;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
ECoDepth: Effective Conditioning of Diffusion Models for Monocular Depth Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18807
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;ViT&#27169;&#22411;&#29983;&#25104;&#30340;&#20840;&#23616;&#22270;&#20687;&#20808;&#39564;&#65292;&#20026;&#21333;&#22270;&#28145;&#24230;&#20272;&#35745;&#27169;&#22411;&#25552;&#20379;&#26356;&#35814;&#32454;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20351;&#29992;&#25193;&#25955;&#39592;&#24178;&#19988;&#21463;ViT&#23884;&#20837;&#26465;&#20214;&#32422;&#26463;&#30340;&#28145;&#24230;&#20272;&#35745;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32570;&#20047;&#35270;&#24046;&#32447;&#32034;&#30340;&#24773;&#20917;&#19979;&#65292;&#22522;&#20110;&#23398;&#20064;&#30340;&#21333;&#22270;&#28145;&#24230;&#20272;&#35745;&#65288;SIDE&#65289;&#27169;&#22411;&#20005;&#37325;&#20381;&#36182;&#22270;&#20687;&#20013;&#30340;&#38452;&#24433;&#21644;&#19978;&#19979;&#25991;&#32447;&#32034;&#12290;&#25105;&#20204;&#20174;&#24050;&#26377;&#30740;&#31350;&#30340;&#21551;&#21457;&#20013;&#25506;&#35752;&#20351;&#29992;&#20174;&#39044;&#35757;&#32451;&#30340;ViT&#27169;&#22411;&#29983;&#25104;&#30340;&#20840;&#23616;&#22270;&#20687;&#20808;&#39564;&#65292;&#20197;&#25552;&#20379;&#26356;&#35814;&#32454;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#22522;&#20110;&#36825;&#19968;&#24819;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20351;&#29992;&#25193;&#25955;&#39592;&#24178;&#30340;SIDE&#27169;&#22411;&#65292;&#20854;&#21463;&#21040;ViT&#23884;&#20837;&#30340;&#26465;&#20214;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18807v1 Announce Type: cross  Abstract: In the absence of parallax cues, a learning-based single image depth estimation (SIDE) model relies heavily on shading and contextual cues in the image. While this simplicity is attractive, it is necessary to train such models on large and varied datasets, which are difficult to capture. It has been shown that using embeddings from pre-trained foundational models, such as CLIP, improves zero shot transfer in several applications. Taking inspiration from this, in our paper we explore the use of global image priors generated from a pre-trained ViT model to provide more detailed contextual information. We argue that the embedding vector from a ViT model, pre-trained on a large dataset, captures greater relevant information for SIDE than the usual route of generating pseudo image captions, followed by CLIP based text embeddings. Based on this idea, we propose a new SIDE model using a diffusion backbone which is conditioned on ViT embedding
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#38271;&#31687;&#22238;&#24212;&#20998;&#35299;&#20026;&#21333;&#20010;&#20107;&#23454;&#65292;&#24182;&#36890;&#36807;&#21457;&#36865;&#25628;&#32034;&#26597;&#35810;&#21040;Google&#25628;&#32034;&#65292;&#35780;&#20272;&#20107;&#23454;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#25193;&#23637;&#20102;F1&#20998;&#25968;&#20316;&#20026;&#38271;&#31687;&#20107;&#23454;&#24615;&#30340;&#32858;&#21512;&#24230;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.18802</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38271;&#31687;&#20107;&#23454;&#24615;
&lt;/p&gt;
&lt;p&gt;
Long-form factuality in large language models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18802
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#38271;&#31687;&#22238;&#24212;&#20998;&#35299;&#20026;&#21333;&#20010;&#20107;&#23454;&#65292;&#24182;&#36890;&#36807;&#21457;&#36865;&#25628;&#32034;&#26597;&#35810;&#21040;Google&#25628;&#32034;&#65292;&#35780;&#20272;&#20107;&#23454;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#25193;&#23637;&#20102;F1&#20998;&#25968;&#20316;&#20026;&#38271;&#31687;&#20107;&#23454;&#24615;&#30340;&#32858;&#21512;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22238;&#31572;&#24320;&#25918;&#24615;&#20027;&#39064;&#30340;&#20107;&#23454;&#24615;&#25552;&#31034;&#26102;&#65292;&#32463;&#24120;&#29983;&#25104;&#21253;&#21547;&#20107;&#23454;&#38169;&#35823;&#30340;&#20869;&#23481;&#12290;&#20026;&#20102;&#22312;&#24320;&#25918;&#39046;&#22495;&#20013;&#23545;&#27169;&#22411;&#30340;&#38271;&#31687;&#20107;&#23454;&#24615;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;GPT-4&#29983;&#25104;&#20102;&#19968;&#20010;&#21517;&#20026;LongFact&#30340;&#25552;&#31034;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#25968;&#21315;&#20010;&#22218;&#25324;38&#20010;&#20027;&#39064;&#30340;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;LLM&#20195;&#29702;&#21487;&#20197;&#36890;&#36807;&#19968;&#31181;&#21517;&#20026;Search-Augmented Factuality Evaluator&#65288;SAFE&#65289;&#30340;&#26041;&#27861;&#20316;&#20026;&#38271;&#31687;&#20107;&#23454;&#24615;&#30340;&#33258;&#21160;&#35780;&#20272;&#22120;&#12290;SAFE&#21033;&#29992;LLM&#23558;&#38271;&#31687;&#22238;&#24212;&#20998;&#35299;&#20026;&#19968;&#32452;&#21333;&#29420;&#30340;&#20107;&#23454;&#65292;&#24182;&#36890;&#36807;&#21457;&#36865;&#25628;&#32034;&#26597;&#35810;&#21040;Google&#25628;&#32034;&#20197;&#21450;&#30830;&#23450;&#19968;&#20010;&#20107;&#23454;&#26159;&#21542;&#24471;&#21040;&#25628;&#32034;&#32467;&#26524;&#25903;&#25345;&#30340;&#22810;&#27493;&#25512;&#29702;&#36807;&#31243;&#26469;&#35780;&#20272;&#27599;&#20010;&#20107;&#23454;&#30340;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#35758;&#23558;F1&#20998;&#25968;&#25193;&#23637;&#20026;&#38271;&#31687;&#20107;&#23454;&#24615;&#30340;&#32858;&#21512;&#24230;&#37327;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24179;&#34913;&#20102;&#22238;&#24212;&#20013;&#25903;&#25345;&#20107;&#23454;&#30340;&#30334;&#20998;&#27604;&#65288;&#31934;&#24230;&#65289;&#19982;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18802v1 Announce Type: cross  Abstract: Large language models (LLMs) often generate content that contains factual errors when responding to fact-seeking prompts on open-ended topics. To benchmark a model's long-form factuality in open domains, we first use GPT-4 to generate LongFact, a prompt set comprising thousands of questions spanning 38 topics. We then propose that LLM agents can be used as automated evaluators for long-form factuality through a method which we call Search-Augmented Factuality Evaluator (SAFE). SAFE utilizes an LLM to break down a long-form response into a set of individual facts and to evaluate the accuracy of each fact using a multi-step reasoning process comprising sending search queries to Google Search and determining whether a fact is supported by the search results. Furthermore, we propose extending F1 score as an aggregated metric for long-form factuality. To do so, we balance the percentage of supported facts in a response (precision) with the 
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#31070;&#32463;&#23884;&#20837;&#21387;&#32553;&#30340;&#22810;&#20219;&#21153;&#23884;&#20837;&#26041;&#27861;&#22312;&#22320;&#29699;&#35266;&#27979;&#20013;&#23454;&#29616;&#20102;&#25968;&#25454;&#39640;&#25928;&#30340;&#35757;&#32451;&#21644;&#25512;&#26029;&#65292;&#36890;&#36807;&#21387;&#32553;&#29575;&#19982;&#23884;&#20837;&#25928;&#29992;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#21462;&#24471;&#20102;&#25968;&#25454;&#37327;&#26174;&#33879;&#20943;&#23569;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.17886</link><description>&lt;p&gt;
&#21387;&#32553;&#22810;&#20219;&#21153;&#23884;&#20837;&#29992;&#20110;&#22320;&#29699;&#35266;&#27979;&#20013;&#25968;&#25454;&#39640;&#25928;&#19979;&#28216;&#35757;&#32451;&#21644;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Compressed Multi-task embeddings for Data-Efficient Downstream training and inference in Earth Observation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17886
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#23884;&#20837;&#21387;&#32553;&#30340;&#22810;&#20219;&#21153;&#23884;&#20837;&#26041;&#27861;&#22312;&#22320;&#29699;&#35266;&#27979;&#20013;&#23454;&#29616;&#20102;&#25968;&#25454;&#39640;&#25928;&#30340;&#35757;&#32451;&#21644;&#25512;&#26029;&#65292;&#36890;&#36807;&#21387;&#32553;&#29575;&#19982;&#23884;&#20837;&#25928;&#29992;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#21462;&#24471;&#20102;&#25968;&#25454;&#37327;&#26174;&#33879;&#20943;&#23569;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22320;&#29699;&#35266;&#27979;&#65288;EO&#65289;&#20013;&#22823;&#35268;&#27169;&#25968;&#25454;&#30340;&#23384;&#20648;&#24211;&#22686;&#38271;&#65292;&#27169;&#22411;&#35757;&#32451;&#21644;&#25512;&#26029;&#30340;&#36716;&#31227;&#21644;&#23384;&#20648;&#25104;&#26412;&#20063;&#22312;&#22686;&#21152;&#65292;&#28040;&#32791;&#20102;&#22823;&#37327;&#36164;&#28304;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#31070;&#32463;&#23884;&#20837;&#21387;&#32553;&#65288;NEC&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#23545;&#25968;&#25454;&#20351;&#29992;&#32773;&#20256;&#36755;&#21387;&#32553;&#30340;&#23884;&#20837;&#32780;&#19981;&#26159;&#21407;&#22987;&#25968;&#25454;&#12290;&#25105;&#20204;&#36890;&#36807;&#23398;&#20064;&#31070;&#32463;&#21387;&#32553;&#26469;&#35843;&#25972;&#22522;&#30784;&#27169;&#22411;&#65288;FM&#65289;&#65292;&#29983;&#25104;&#22810;&#20219;&#21153;&#23884;&#20837;&#65292;&#21516;&#26102;&#22312;&#21387;&#32553;&#29575;&#21644;&#23884;&#20837;&#25928;&#29992;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#12290;&#25105;&#20204;&#20165;&#38024;&#23545;FM&#21442;&#25968;&#30340;&#19968;&#23567;&#37096;&#20998;&#65288;10%&#65289;&#36827;&#34892;&#26356;&#26032;&#65292;&#36827;&#34892;&#30701;&#26102;&#38388;&#35757;&#32451;&#65288;&#39044;&#35757;&#32451;&#36845;&#20195;&#30340;1%&#65289;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;EO&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;NEC&#65306;&#22330;&#26223;&#20998;&#31867;&#21644;&#35821;&#20041;&#20998;&#21106;&#12290;&#19982;&#23558;&#20256;&#32479;&#21387;&#32553;&#24212;&#29992;&#20110;&#21407;&#22987;&#25968;&#25454;&#30456;&#27604;&#65292;NEC&#22312;&#20943;&#23569;&#25968;&#25454;&#37327;&#26041;&#38754;&#21487;&#23454;&#29616;&#31867;&#20284;&#30340;&#20934;&#30830;&#24615;&#65292;&#38477;&#20302;&#20102;75%&#21040;90%&#30340;&#25968;&#25454;&#37327;&#12290;&#21363;&#20351;&#22312;99.7%&#30340;&#21387;&#32553;&#19979;&#65292;&#22312;&#22330;&#26223;&#20998;&#31867;&#20219;&#21153;&#19978;&#24615;&#33021;&#20165;&#19979;&#38477;&#20102;5%&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;NEC&#26159;&#19968;&#31181;&#25968;&#25454;&#39640;&#25928;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17886v1 Announce Type: new  Abstract: As repositories of large scale data in earth observation (EO) have grown, so have transfer and storage costs for model training and inference, expending significant resources. We introduce Neural Embedding Compression (NEC), based on the transfer of compressed embeddings to data consumers instead of raw data. We adapt foundation models (FM) through learned neural compression to generate multi-task embeddings while navigating the tradeoff between compression rate and embedding utility. We update only a small fraction of the FM parameters (10%) for a short training period (1% of the iterations of pre-training). We evaluate NEC on two EO tasks: scene classification and semantic segmentation. Compared with applying traditional compression to the raw data, NEC achieves similar accuracy with a 75% to 90% reduction in data. Even at 99.7% compression, performance drops by only 5% on the scene classification task. Overall, NEC is a data-efficient
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102; PeersimGym &#29615;&#22659;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#20219;&#21153;&#21368;&#36733;&#38382;&#39064;&#65292;&#25903;&#25345;&#23450;&#21046;&#21270;&#20223;&#30495;&#29615;&#22659;&#65292;&#26377;&#21161;&#20110;&#24320;&#21457;&#21644;&#20248;&#21270;&#35745;&#31639;&#32593;&#32476;&#20013;&#30340;&#20219;&#21153;&#21368;&#36733;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.17637</link><description>&lt;p&gt;
PeersimGym&#65306;&#29992;&#20110;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#20219;&#21153;&#21368;&#36733;&#38382;&#39064;&#30340;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
PeersimGym: An Environment for Solving the Task Offloading Problem with Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17637
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102; PeersimGym &#29615;&#22659;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#20219;&#21153;&#21368;&#36733;&#38382;&#39064;&#65292;&#25903;&#25345;&#23450;&#21046;&#21270;&#20223;&#30495;&#29615;&#22659;&#65292;&#26377;&#21161;&#20110;&#24320;&#21457;&#21644;&#20248;&#21270;&#35745;&#31639;&#32593;&#32476;&#20013;&#30340;&#20219;&#21153;&#21368;&#36733;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#21153;&#21368;&#36733;&#23545;&#20110;&#22312;&#35832;&#22914;&#29289;&#32852;&#32593;&#20043;&#31867;&#30340;&#32593;&#32476;&#20013;&#24179;&#34913;&#35774;&#22791;&#30340;&#35745;&#31639;&#36127;&#36733;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#38754;&#20020;&#30528;&#35832;&#22914;&#22312;&#20005;&#26684;&#30340;&#36890;&#20449;&#21644;&#23384;&#20648;&#32422;&#26463;&#19979;&#26368;&#23567;&#21270;&#24310;&#36831;&#21644;&#33021;&#28304;&#20351;&#29992;&#31561;&#37325;&#35201;&#20248;&#21270;&#25361;&#25112;&#12290;&#20256;&#32479;&#20248;&#21270;&#22312;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#65307;&#21551;&#21457;&#24335;&#26041;&#27861;&#32570;&#20047;&#23454;&#29616;&#26368;&#20339;&#32467;&#26524;&#65292;&#32780;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#36890;&#36807;&#20801;&#35768;&#36890;&#36807;&#36845;&#20195;&#20132;&#20114;&#23398;&#20064;&#26368;&#20339;&#21368;&#36733;&#31574;&#30053;&#30340;&#26041;&#24335;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#36884;&#24452;&#12290;&#28982;&#32780;&#65292;RL &#30340;&#21151;&#25928;&#21462;&#20915;&#20110;&#23545;&#20016;&#23500;&#25968;&#25454;&#38598;&#21644;&#23450;&#21046;&#30340;&#29616;&#23454;&#35757;&#32451;&#29615;&#22659;&#30340;&#35775;&#38382;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102; PeersimGym&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#12289;&#21487;&#23450;&#21046;&#30340;&#20223;&#30495;&#29615;&#22659;&#65292;&#26088;&#22312;&#24320;&#21457;&#21644;&#20248;&#21270;&#35745;&#31639;&#32593;&#32476;&#20013;&#30340;&#20219;&#21153;&#21368;&#36733;&#31574;&#30053;&#12290;PeersimGym &#25903;&#25345;&#21508;&#31181;&#32593;&#32476;&#25299;&#25169;&#21644;&#35745;&#31639;&#32422;&#26463;&#65292;&#24182;&#25972;&#21512;&#20102;&#19968;&#31181;"PettingZo"&#26041;&#27861;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#36731;&#26494;&#37197;&#32622;&#20223;&#30495;&#21442;&#25968;&#21644;&#30417;&#25511;&#20223;&#30495;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17637v1 Announce Type: cross  Abstract: Task offloading, crucial for balancing computational loads across devices in networks such as the Internet of Things, poses significant optimization challenges, including minimizing latency and energy usage under strict communication and storage constraints. While traditional optimization falls short in scalability; and heuristic approaches lack in achieving optimal outcomes, Reinforcement Learning (RL) offers a promising avenue by enabling the learning of optimal offloading strategies through iterative interactions. However, the efficacy of RL hinges on access to rich datasets and custom-tailored, realistic training environments. To address this, we introduce PeersimGym, an open-source, customizable simulation environment tailored for developing and optimizing task offloading strategies within computational networks. PeersimGym supports a wide range of network topologies and computational constraints and integrates a \textit{PettingZo
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#21306;&#22495;&#25351;&#23548;&#26631;&#35760;&#25216;&#26415;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#23391;&#21152;&#25289;&#25991;&#26412;&#19982;&#22320;&#26041;&#26041;&#35328;&#36716;&#24405;&#20026;&#22269;&#38469;&#38899;&#26631;&#30340;&#26041;&#27861;&#65292;&#20026;&#27169;&#22411;&#25552;&#20379;&#20102;&#20851;&#20110;&#36755;&#20837;&#25991;&#26412;&#30340;&#22320;&#21306;&#26041;&#35328;&#20449;&#24687;&#65292;&#20197;&#29702;&#35299;&#19982;&#27599;&#20010;&#22320;&#21306;&#30456;&#20851;&#30340;&#29420;&#29305;&#38899;&#38901;&#27169;&#24335;&#12290;</title><link>https://arxiv.org/abs/2403.17407</link><description>&lt;p&gt;
&#20351;&#29992;&#21306;&#22495;&#25351;&#23548;&#26631;&#35760;&#23558;&#23391;&#21152;&#25289;&#25991;&#26412;&#19982;&#22320;&#26041;&#26041;&#35328;&#36716;&#24405;&#20026;&#22269;&#38469;&#38899;&#26631;
&lt;/p&gt;
&lt;p&gt;
Transcribing Bengali Text with Regional Dialects to IPA using District Guided Tokens
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17407
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#21306;&#22495;&#25351;&#23548;&#26631;&#35760;&#25216;&#26415;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#23391;&#21152;&#25289;&#25991;&#26412;&#19982;&#22320;&#26041;&#26041;&#35328;&#36716;&#24405;&#20026;&#22269;&#38469;&#38899;&#26631;&#30340;&#26041;&#27861;&#65292;&#20026;&#27169;&#22411;&#25552;&#20379;&#20102;&#20851;&#20110;&#36755;&#20837;&#25991;&#26412;&#30340;&#22320;&#21306;&#26041;&#35328;&#20449;&#24687;&#65292;&#20197;&#29702;&#35299;&#19982;&#27599;&#20010;&#22320;&#21306;&#30456;&#20851;&#30340;&#29420;&#29305;&#38899;&#38901;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23391;&#21152;&#25289;&#25991;&#26412;&#21040;&#22269;&#38469;&#38899;&#26631;&#65288;IPA&#65289;&#30340;&#20934;&#30830;&#36716;&#24405;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#35821;&#35328;&#30340;&#22797;&#26434;&#38899;&#38901;&#23398;&#21644;&#35821;&#22659;&#30456;&#20851;&#30340;&#38899;&#21464;&#12290;&#23545;&#20110;&#21306;&#22495;&#23391;&#21152;&#25289;&#26041;&#35328;&#26469;&#35828;&#65292;&#30001;&#20110;&#32570;&#20047;&#38024;&#23545;&#36825;&#20123;&#26041;&#35328;&#30340;&#26631;&#20934;&#25340;&#20889;&#32422;&#23450;&#12289;&#24403;&#22320;&#21644;&#22806;&#35821;&#22312;&#36825;&#20123;&#22320;&#21306;&#20013;&#27969;&#34892;&#30340;&#35789;&#27719;&#20197;&#21450;&#19981;&#21516;&#22320;&#21306;&#20043;&#38388;&#30340;&#38899;&#38901;&#22810;&#26679;&#24615;&#65292;&#36825;&#19968;&#25361;&#25112;&#29978;&#33267;&#26356;&#20026;&#20005;&#23803;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#38382;&#39064;&#65292;&#21363;&#22312;&#35206;&#30422;&#23391;&#21152;&#25289;&#22269;&#20845;&#20010;&#22320;&#21306;&#30340;&#26032;&#25968;&#25454;&#38598;&#19978;&#24341;&#20837;&#8220;&#21306;&#22495;&#25351;&#23548;&#26631;&#35760;&#8221;&#65288;DGT&#65289;&#25216;&#26415;&#12290;&#20854;&#20851;&#38190;&#24605;&#24819;&#26159;&#22312;&#29983;&#25104;IPA&#36716;&#24405;&#20043;&#21069;&#21521;&#27169;&#22411;&#25552;&#20379;&#26377;&#20851;&#36755;&#20837;&#25991;&#26412;&#30340;&#21306;&#22495;&#26041;&#35328;&#25110;&#8220;&#22320;&#21306;&#8221;&#30340;&#26126;&#30830;&#20449;&#24687;&#12290;&#36825;&#36890;&#36807;&#22312;&#36755;&#20837;&#24207;&#21015;&#21069;&#28155;&#21152;&#19968;&#20010;&#22320;&#21306;&#26631;&#35760;&#26469;&#23454;&#29616;&#65292;&#26377;&#25928;&#22320;&#24341;&#23548;&#27169;&#22411;&#29702;&#35299;&#19982;&#27599;&#20010;&#22320;&#21306;&#30456;&#20851;&#30340;&#29420;&#29305;&#38899;&#38901;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17407v1 Announce Type: cross  Abstract: Accurate transcription of Bengali text to the International Phonetic Alphabet (IPA) is a challenging task due to the complex phonology of the language and context-dependent sound changes. This challenge is even more for regional Bengali dialects due to unavailability of standardized spelling conventions for these dialects, presence of local and foreign words popular in those regions and phonological diversity across different regions. This paper presents an approach to this sequence-to-sequence problem by introducing the District Guided Tokens (DGT) technique on a new dataset spanning six districts of Bangladesh. The key idea is to provide the model with explicit information about the regional dialect or "district" of the input text before generating the IPA transcription. This is achieved by prepending a district token to the input sequence, effectively guiding the model to understand the unique phonetic patterns associated with each 
&lt;/p&gt;</description></item><item><title>FLIGAN&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#26469;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#65292;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#19981;&#23436;&#25972;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#25552;&#21319;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#23436;&#25972;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.16930</link><description>&lt;p&gt;
FLIGAN: &#20351;&#29992; GAN &#25913;&#36827;&#19981;&#23436;&#25972;&#25968;&#25454;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FLIGAN: Enhancing Federated Learning with Incomplete Data using GAN
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16930
&lt;/p&gt;
&lt;p&gt;
FLIGAN&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#26469;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#65292;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#19981;&#23436;&#25972;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#25552;&#21319;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#23436;&#25972;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#26426;&#21046;&#65292;&#29992;&#20110;&#22312;&#32593;&#32476;&#21270;&#35774;&#22791;&#65288;&#22914;&#31227;&#21160;&#35774;&#22791;&#12289;&#29289;&#32852;&#32593;&#36793;&#32536;&#33410;&#28857;&#65289;&#19978;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20998;&#24067;&#24335;&#35757;&#32451;&#12290;&#23427;&#36890;&#36807;&#22312;&#32593;&#32476;&#19978;&#19981;&#20849;&#20139;&#23454;&#38469;&#25968;&#25454;&#26469;&#23454;&#29616;&#36793;&#32536;&#30340;&#20154;&#24037;&#26234;&#33021;&#12290;&#29616;&#26377;&#30740;&#31350;&#36890;&#24120;&#20391;&#37325;&#20110;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#30340;&#36890;&#29992;&#26041;&#38754;&#21644;&#23458;&#25143;&#31995;&#32479;&#29305;&#24449;&#30340;&#24322;&#36136;&#24615;&#65292;&#20294;&#20182;&#20204;&#24120;&#24120;&#24573;&#35270;&#27169;&#22411;&#24320;&#21457;&#20013;&#19981;&#36275;&#30340;&#25968;&#25454;&#38382;&#39064;&#65292;&#36825;&#21487;&#33021;&#26469;&#33258;&#36793;&#32536;&#33410;&#28857;&#19978;&#19981;&#22343;&#21248;&#30340;&#31867;&#21035;&#26631;&#31614;&#20998;&#24067;&#65292;&#20197;&#21450;&#39640;&#24230;&#21487;&#21464;&#30340;&#25968;&#25454;&#23481;&#37327;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FLIGAN&#30340;&#21019;&#26032;&#26041;&#27861;&#26469;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#25968;&#25454;&#19981;&#23436;&#25972;&#24615;&#30340;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#26469;&#25935;&#38160;&#25429;&#25417;&#22797;&#26434;&#25968;&#25454;&#20998;&#24067;&#65292;&#24182;&#29983;&#25104;&#19982;&#30495;&#23454;&#25968;&#25454;&#32039;&#23494;&#30456;&#20284;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#26469;&#22686;&#24378;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#23436;&#25972;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16930v1 Announce Type: new  Abstract: Federated Learning (FL) provides a privacy-preserving mechanism for distributed training of machine learning models on networked devices (e.g., mobile devices, IoT edge nodes). It enables Artificial Intelligence (AI) at the edge by creating models without sharing the actual data across the network. Existing research works typically focus on generic aspects of non-IID data and heterogeneity in client's system characteristics, but they often neglect the issue of insufficient data for model development, which can arise from uneven class label distribution and highly variable data volumes across edge nodes. In this work, we propose FLIGAN, a novel approach to address the issue of data incompleteness in FL. First, we leverage Generative Adversarial Networks (GANs) to adeptly capture complex data distributions and generate synthetic data that closely resemble the real-world data. Then, we use synthetic data to enhance the robustness and comple
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25506;&#35752;&#20102;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#12289;&#36125;&#21494;&#26031;&#38544;&#31169;&#21450;&#20854;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#65292;&#25581;&#31034;&#20102;&#20851;&#20110;&#25928;&#29992;-&#38544;&#31169;&#26435;&#34913;&#30340;&#26032;&#35265;&#35299;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#31361;&#20986;&#25915;&#20987;&#21644;&#38450;&#24481;&#31574;&#30053;&#30340;&#30456;&#20114;&#20316;&#29992;&#21644;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.16591</link><description>&lt;p&gt;
&#25581;&#31034;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#12289;&#24179;&#22343;&#36125;&#21494;&#26031;&#38544;&#31169;&#21644;&#26368;&#22823;&#36125;&#21494;&#26031;&#38544;&#31169;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Deciphering the Interplay between Local Differential Privacy, Average Bayesian Privacy, and Maximum Bayesian Privacy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16591
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25506;&#35752;&#20102;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#12289;&#36125;&#21494;&#26031;&#38544;&#31169;&#21450;&#20854;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#65292;&#25581;&#31034;&#20102;&#20851;&#20110;&#25928;&#29992;-&#38544;&#31169;&#26435;&#34913;&#30340;&#26032;&#35265;&#35299;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#31361;&#20986;&#25915;&#20987;&#21644;&#38450;&#24481;&#31574;&#30053;&#30340;&#30456;&#20114;&#20316;&#29992;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#30340;&#36805;&#36895;&#21457;&#23637;&#23548;&#33268;&#20102;&#38544;&#31169;&#23450;&#20041;&#30340;&#22810;&#26679;&#21270;&#65292;&#30001;&#20110;&#23545;&#38544;&#31169;&#26500;&#25104;&#30340;&#23041;&#32961;&#65292;&#21253;&#25324;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#65288;LDP&#65289;&#30340;&#27010;&#24565;&#12290;&#34429;&#28982;&#34987;&#24191;&#27867;&#25509;&#21463;&#24182;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#34987;&#21033;&#29992;&#65292;&#20294;&#36825;&#31181;&#20256;&#32479;&#30340;&#38544;&#31169;&#27979;&#37327;&#26041;&#27861;&#20173;&#28982;&#23384;&#22312;&#19968;&#23450;&#38480;&#21046;&#65292;&#20174;&#26080;&#27861;&#38450;&#27490;&#25512;&#26029;&#25259;&#38706;&#21040;&#32570;&#20047;&#23545;&#23545;&#25163;&#32972;&#26223;&#30693;&#35782;&#30340;&#32771;&#34385;&#12290;&#22312;&#36825;&#39033;&#20840;&#38754;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#36125;&#21494;&#26031;&#38544;&#31169;&#24182;&#28145;&#20837;&#25506;&#35752;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#21644;&#20854;&#36125;&#21494;&#26031;&#23545;&#24212;&#29289;&#20043;&#38388;&#38169;&#32508;&#22797;&#26434;&#30340;&#20851;&#31995;&#65292;&#25581;&#31034;&#20102;&#20851;&#20110;&#25928;&#29992;-&#38544;&#31169;&#26435;&#34913;&#30340;&#26032;&#35265;&#35299;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#27010;&#25324;&#20102;&#25915;&#20987;&#21644;&#38450;&#24481;&#31574;&#30053;&#65292;&#31361;&#20986;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#21644;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#36129;&#29486;&#22522;&#20110;&#24179;&#22343;&#36125;&#21494;&#26031;&#38544;&#31169;&#65288;ABP&#65289;&#21644;&#26368;&#22823;&#36125;&#21494;&#26031;&#38544;&#31169;&#20043;&#38388;&#30340;&#20005;&#26684;&#23450;&#20041;&#21644;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16591v1 Announce Type: cross  Abstract: The swift evolution of machine learning has led to emergence of various definitions of privacy due to the threats it poses to privacy, including the concept of local differential privacy (LDP). Although widely embraced and utilized across numerous domains, this conventional approach to measure privacy still exhibits certain limitations, spanning from failure to prevent inferential disclosure to lack of consideration for the adversary's background knowledge. In this comprehensive study, we introduce Bayesian privacy and delve into the intricate relationship between local differential privacy and its Bayesian counterparts, unveiling novel insights into utility-privacy trade-offs. We introduce a framework that encapsulates both attack and defense strategies, highlighting their interplay and effectiveness. Our theoretical contributions are anchored in the rigorous definitions and relationships between Average Bayesian Privacy (ABP) and Max
&lt;/p&gt;</description></item><item><title>FusionINN&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#36870;&#22270;&#20687;&#34701;&#21512;&#26694;&#26550;&#65292;&#21487;&#20197;&#39640;&#25928;&#29983;&#25104;&#34701;&#21512;&#22270;&#20687;&#65292;&#24182;&#35299;&#24320;&#34701;&#21512;&#36807;&#31243;&#30340;&#36870;&#21521;&#20998;&#35299;&#65292;&#20445;&#35777;&#26080;&#25439;&#30340;&#20687;&#32032;&#26144;&#23556;&#12290;</title><link>https://arxiv.org/abs/2403.15769</link><description>&lt;p&gt;
FusionINN&#65306;&#21487;&#36870;&#22270;&#20687;&#34701;&#21512;&#29992;&#20110;&#33041;&#32959;&#30244;&#30417;&#27979;
&lt;/p&gt;
&lt;p&gt;
FusionINN: Invertible Image Fusion for Brain Tumor Monitoring
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15769
&lt;/p&gt;
&lt;p&gt;
FusionINN&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#36870;&#22270;&#20687;&#34701;&#21512;&#26694;&#26550;&#65292;&#21487;&#20197;&#39640;&#25928;&#29983;&#25104;&#34701;&#21512;&#22270;&#20687;&#65292;&#24182;&#35299;&#24320;&#34701;&#21512;&#36807;&#31243;&#30340;&#36870;&#21521;&#20998;&#35299;&#65292;&#20445;&#35777;&#26080;&#25439;&#30340;&#20687;&#32032;&#26144;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#34701;&#21512;&#36890;&#24120;&#20351;&#29992;&#19981;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#23558;&#22810;&#20010;&#28304;&#22270;&#20687;&#21512;&#24182;&#20026;&#21333;&#20010;&#34701;&#21512;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20020;&#24202;&#19987;&#23478;&#65292;&#20165;&#20381;&#36182;&#34701;&#21512;&#22270;&#20687;&#21487;&#33021;&#19981;&#36275;&#20197;&#20570;&#20986;&#35786;&#26029;&#20915;&#31574;&#65292;&#22240;&#20026;&#34701;&#21512;&#26426;&#21046;&#28151;&#21512;&#20102;&#26469;&#33258;&#28304;&#22270;&#20687;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#38590;&#20197;&#35299;&#37322;&#28508;&#22312;&#30340;&#32959;&#30244;&#30149;&#29702;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;FusionINN&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#36870;&#22270;&#20687;&#34701;&#21512;&#26694;&#26550;&#65292;&#33021;&#22815;&#39640;&#25928;&#29983;&#25104;&#34701;&#21512;&#22270;&#20687;&#65292;&#24182;&#36890;&#36807;&#27714;&#35299;&#34701;&#21512;&#36807;&#31243;&#30340;&#36870;&#36807;&#31243;&#23558;&#20854;&#20998;&#35299;&#22238;&#28304;&#22270;&#20687;&#12290;FusionINN&#36890;&#36807;&#25972;&#21512;&#19968;&#20010;&#27491;&#24577;&#20998;&#24067;&#30340;&#28508;&#22312;&#22270;&#20687;&#19982;&#34701;&#21512;&#22270;&#20687;&#19968;&#36215;&#65292;&#20197;&#20419;&#36827;&#20998;&#35299;&#36807;&#31243;&#30340;&#29983;&#25104;&#24314;&#27169;&#65292;&#20174;&#32780;&#20445;&#35777;&#26080;&#25439;&#30340;&#19968;&#23545;&#19968;&#20687;&#32032;&#26144;&#23556;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#26159;&#39318;&#27425;&#30740;&#31350;&#34701;&#21512;&#22270;&#20687;&#30340;&#21487;&#20998;&#35299;&#24615;&#65292;&#36825;&#23545;&#20110;&#29983;&#21629;&#25935;&#24863;&#24212;&#29992;&#31243;&#24207;&#23588;&#20026;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15769v1 Announce Type: cross  Abstract: Image fusion typically employs non-invertible neural networks to merge multiple source images into a single fused image. However, for clinical experts, solely relying on fused images may be insufficient for making diagnostic decisions, as the fusion mechanism blends features from source images, thereby making it difficult to interpret the underlying tumor pathology. We introduce FusionINN, a novel invertible image fusion framework, capable of efficiently generating fused images and also decomposing them back to the source images by solving the inverse of the fusion process. FusionINN guarantees lossless one-to-one pixel mapping by integrating a normally distributed latent image alongside the fused image to facilitate the generative modeling of the decomposition process. To the best of our knowledge, we are the first to investigate the decomposability of fused images, which is particularly crucial for life-sensitive applications such as
&lt;/p&gt;</description></item><item><title>Foundation Models&#20026;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#24102;&#26469;&#21019;&#26032;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#25110;&#24494;&#35843;&#30340;&#27169;&#22411;&#26469;&#33719;&#24471;&#20855;&#20307;&#23450;&#21046;&#30340;&#24191;&#20041;&#30693;&#35782;&#65292;&#25552;&#21319;&#20102;&#23454;&#36341;&#20013;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.14735</link><description>&lt;p&gt;
&#22522;&#20110;Foundation Models&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#65306;&#25945;&#31243;&#19982;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Foundation Models for Time Series Analysis: A Tutorial and Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14735
&lt;/p&gt;
&lt;p&gt;
Foundation Models&#20026;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#24102;&#26469;&#21019;&#26032;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#25110;&#24494;&#35843;&#30340;&#27169;&#22411;&#26469;&#33719;&#24471;&#20855;&#20307;&#23450;&#21046;&#30340;&#24191;&#20041;&#30693;&#35782;&#65292;&#25552;&#21319;&#20102;&#23454;&#36341;&#20013;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20316;&#20026;&#25968;&#25454;&#25366;&#25496;&#39046;&#22495;&#30340;&#28966;&#28857;&#65292;&#26159;&#25552;&#21462;&#26377;&#20215;&#20540;&#35265;&#35299;&#30340;&#22522;&#30707;&#65292;&#23545;&#20247;&#22810;&#23454;&#38469;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;Foundation Models&#65288;FMs&#65289;&#30340;&#21457;&#23637;&#26681;&#26412;&#24615;&#22320;&#25913;&#21464;&#20102;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#27169;&#22411;&#35774;&#35745;&#30340;&#33539;&#24335;&#65292;&#25552;&#21319;&#20102;&#23454;&#36341;&#20013;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#30340;&#25928;&#26524;&#12290;&#36825;&#20123;&#21019;&#26032;&#26041;&#27861;&#36890;&#24120;&#21033;&#29992;&#39044;&#35757;&#32451;&#25110;&#24494;&#35843;&#30340;FMs&#65292;&#20197;&#33719;&#21462;&#19987;&#38376;&#20026;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#37327;&#36523;&#23450;&#21046;&#30340;&#24191;&#20041;&#30693;&#35782;&#12290;&#22312;&#26412;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#25552;&#20379;Foundation Models&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#20840;&#38754;&#21644;&#26368;&#26032;&#27010;&#36848;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#35843;&#26597;&#20027;&#35201;&#38598;&#20013;&#22312;Foundation Models&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#25110;&#31649;&#36947;&#26041;&#38754;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#32570;&#20047;&#28145;&#20837;&#20102;&#35299;&#38416;&#26126;Foundation Models&#22914;&#20309;&#21463;&#30410;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#22522;&#26412;&#26426;&#21046;&#12290;&#20026;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#30340;&#35843;&#26597;&#37319;&#29992;&#20102;&#20197;&#27169;&#22411;&#20026;&#20013;&#24515;&#30340;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14735v1 Announce Type: new  Abstract: Time series analysis stands as a focal point within the data mining community, serving as a cornerstone for extracting valuable insights crucial to a myriad of real-world applications. Recent advancements in Foundation Models (FMs) have fundamentally reshaped the paradigm of model design for time series analysis, boosting various downstream tasks in practice. These innovative approaches often leverage pre-trained or fine-tuned FMs to harness generalized knowledge tailored specifically for time series analysis. In this survey, we aim to furnish a comprehensive and up-to-date overview of FMs for time series analysis. While prior surveys have predominantly focused on either the application or the pipeline aspects of FMs in time series analysis, they have often lacked an in-depth understanding of the underlying mechanisms that elucidate why and how FMs benefit time series analysis. To address this gap, our survey adopts a model-centric class
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;CICLe&#26694;&#26550;&#65292;&#22522;&#20110;&#36866;&#24212;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26041;&#24335;&#65292;&#22312;&#22823;&#35268;&#27169;&#22810;&#31867;&#39135;&#21697;&#39118;&#38505;&#20998;&#31867;&#20013;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#31526;&#21512;&#39044;&#27979;&#30340;LLM-in-the-loop&#26694;&#26550;&#65292;&#21487;&#20197;&#25552;&#39640;&#22522;&#26412;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#65292;&#24182;&#20943;&#23569;&#33021;&#28304;&#28040;&#32791;&#12290;</title><link>https://arxiv.org/abs/2403.11904</link><description>&lt;p&gt;
CICLe: &#36866;&#24212;&#19978;&#19979;&#25991;&#30340;&#22823;&#35268;&#27169;&#22810;&#31867;&#39135;&#21697;&#39118;&#38505;&#20998;&#31867;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
CICLe: Conformal In-Context Learning for Largescale Multi-Class Food Risk Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11904
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;CICLe&#26694;&#26550;&#65292;&#22522;&#20110;&#36866;&#24212;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26041;&#24335;&#65292;&#22312;&#22823;&#35268;&#27169;&#22810;&#31867;&#39135;&#21697;&#39118;&#38505;&#20998;&#31867;&#20013;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#31526;&#21512;&#39044;&#27979;&#30340;LLM-in-the-loop&#26694;&#26550;&#65292;&#21487;&#20197;&#25552;&#39640;&#22522;&#26412;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#65292;&#24182;&#20943;&#23569;&#33021;&#28304;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#27745;&#26579;&#25110;&#25530;&#20551;&#39135;&#21697;&#23545;&#20154;&#31867;&#20581;&#24247;&#26500;&#25104;&#37325;&#22823;&#39118;&#38505;&#12290;&#22312;&#32473;&#23450;&#20102;&#29992;&#20110;&#35757;&#32451;&#30340;&#26631;&#35760;&#32593;&#32476;&#25991;&#26412;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26469;&#33258;&#21160;&#26816;&#27979;&#36825;&#31181;&#39118;&#38505;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#20010;&#21253;&#21547;7,546&#20010;&#25551;&#36848;&#20844;&#20849;&#39135;&#21697;&#21484;&#22238;&#20844;&#21578;&#30340;&#30701;&#25991;&#26412;&#25968;&#25454;&#38598;&#12290;&#27599;&#20010;&#25991;&#26412;&#37117;&#32463;&#36807;&#25163;&#21160;&#26631;&#35760;&#65292;&#20998;&#20026;&#20004;&#20010;&#31890;&#24230;&#32423;&#21035;&#65288;&#31895;&#31890;&#24230;&#21644;&#32454;&#31890;&#24230;&#65289;&#65292;&#29992;&#20110;&#34920;&#31034;&#21484;&#22238;&#23545;&#24212;&#30340;&#39135;&#21697;&#20135;&#21697;&#21644;&#21361;&#23475;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#25968;&#25454;&#38598;&#24182;&#23545;&#26420;&#32032;&#12289;&#20256;&#32479;&#21644;Transformer&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#22522;&#20110;tf-idf&#34920;&#31034;&#30340;&#36923;&#36753;&#22238;&#24402;&#22312;&#25903;&#25345;&#36739;&#20302;&#30340;&#31867;&#21035;&#19978;&#20248;&#20110;RoBERTa&#21644;XLM-R&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#19981;&#21516;&#30340;&#25552;&#31034;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31526;&#21512;&#39044;&#27979;&#30340;LLM-in-the-loop&#26694;&#26550;&#65292;&#36825;&#21487;&#20197;&#25552;&#39640;&#22522;&#26412;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#19982;&#26222;&#36890;&#25552;&#31034;&#30456;&#27604;&#30340;&#33021;&#28304;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11904v1 Announce Type: new  Abstract: Contaminated or adulterated food poses a substantial risk to human health. Given sets of labeled web texts for training, Machine Learning and Natural Language Processing can be applied to automatically detect such risks. We publish a dataset of 7,546 short texts describing public food recall announcements. Each text is manually labeled, on two granularity levels (coarse and fine), for food products and hazards that the recall corresponds to. We describe the dataset and benchmark naive, traditional, and Transformer models. Based on our analysis, Logistic Regression based on a tf-idf representation outperforms RoBERTa and XLM-R on classes with low support. Finally, we discuss different prompting strategies and present an LLM-in-the-loop framework, based on Conformal Prediction, which boosts the performance of the base classifier while reducing energy consumption compared to normal prompting.
&lt;/p&gt;</description></item><item><title>Mamba&#27169;&#22411;&#20316;&#20026;&#19968;&#31181;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#20855;&#26377;&#25429;&#25417;&#22797;&#26434;&#20381;&#36182;&#20851;&#31995;&#12289;&#36817;&#32447;&#24615;&#22797;&#26434;&#24230;&#20197;&#21450;&#24615;&#33021;&#20248;&#21183;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.11144</link><description>&lt;p&gt;
Mamba&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#26377;&#25928;&#24615;&#22914;&#20309;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is Mamba Effective for Time Series Forecasting?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11144
&lt;/p&gt;
&lt;p&gt;
Mamba&#27169;&#22411;&#20316;&#20026;&#19968;&#31181;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#20855;&#26377;&#25429;&#25417;&#22797;&#26434;&#20381;&#36182;&#20851;&#31995;&#12289;&#36817;&#32447;&#24615;&#22797;&#26434;&#24230;&#20197;&#21450;&#24615;&#33021;&#20248;&#21183;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65288;TSF&#65289;&#39046;&#22495;&#20013;&#65292;&#30001;&#20110;Transformer&#27169;&#22411;&#33021;&#22815;&#32858;&#28966;&#20840;&#23616;&#29615;&#22659;&#65292;&#26377;&#25928;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#20013;&#38271;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#20197;&#21450;&#36776;&#21035;&#22810;&#21464;&#37327;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#22240;&#27492;&#23427;&#19968;&#30452;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;Transformer&#27169;&#22411;&#30340;&#20302;&#25928;&#29575;&#21644;&#20851;&#20110;&#20854;&#25429;&#25417;&#20381;&#36182;&#20851;&#31995;&#33021;&#21147;&#30340;&#36136;&#30097;&#65292;&#23545;Transformer&#26550;&#26500;&#30340;&#19981;&#26029;&#23436;&#21892;&#24037;&#20316;&#20173;&#22312;&#36827;&#34892;&#20013;&#12290;&#26368;&#36817;&#65292;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSMs&#65289;&#22914;Mamba&#22240;&#20854;&#33021;&#22815;&#20687;Transformer&#19968;&#26679;&#25429;&#25417;&#24207;&#21015;&#20013;&#30340;&#22797;&#26434;&#20381;&#36182;&#20851;&#31995;&#65292;&#21516;&#26102;&#21448;&#20445;&#25345;&#36817;&#32447;&#24615;&#30340;&#22797;&#26434;&#24230;&#32780;&#22791;&#21463;&#25512;&#23815;&#12290;&#22312;&#25991;&#26412;&#21644;&#22270;&#20687;&#20219;&#21153;&#20013;&#65292;&#22522;&#20110;Mamba&#30340;&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#24182;&#33410;&#32422;&#25104;&#26412;&#65292;&#23454;&#29616;&#21452;&#36194;&#23616;&#38754;&#12290;&#36825;&#24341;&#36215;&#20102;&#25105;&#20204;&#23545;&#25506;&#32034;SSM&#22312;TSF&#20219;&#21153;&#20013;&#28508;&#21147;&#30340;&#20852;&#36259;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20004;&#31181;&#22522;&#20110;SSM&#30340;&#31616;&#21333;&#27169;&#22411;&#65292;S-Mamba&#21644;......
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11144v1 Announce Type: new  Abstract: In the realm of time series forecasting (TSF), the Transformer has consistently demonstrated robust performance due to its ability to focus on the global context and effectively capture long-range dependencies within time, as well as discern correlations between multiple variables. However, due to the inefficiencies of the Transformer model and questions surrounding its ability to capture dependencies, ongoing efforts to refine the Transformer architecture persist. Recently, state space models (SSMs), e.g. Mamba, have gained traction due to their ability to capture complex dependencies in sequences, similar to the Transformer, while maintaining near-linear complexity. In text and image tasks, Mamba-based models can improve performance and cost savings, creating a win-win situation. This has piqued our interest in exploring SSM's potential in TSF tasks. In this paper, we introduce two straightforward SSM-based models for TSF, S-Mamba and 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;Transformer&#27169;&#22411;&#65292;&#32467;&#21512;&#38376;&#25511;&#21160;&#24577;&#21487;&#23398;&#20064;&#27880;&#24847;&#26426;&#21046;&#65292;&#29992;&#20110;&#30000;&#32435;&#35199;&#20234;&#26031;&#26364;&#36807;&#31243;&#30340;&#25925;&#38556;&#26816;&#27979;&#19982;&#35786;&#26029;&#65292;&#25552;&#39640;&#24615;&#33021;&#36890;&#36807;&#29420;&#31435;&#22788;&#29702;&#36755;&#20837;&#25968;&#25454;&#21644;&#25552;&#21462;&#22810;&#26679;&#21270;&#20449;&#24687;&#65292;&#20197;&#21450;&#21160;&#24577;&#23398;&#20064;&#36866;&#24212;&#24615;&#35843;&#25972;&#27880;&#24847;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.10842</link><description>&lt;p&gt;
&#20351;&#29992;&#38376;&#25511;&#21160;&#24577;&#21487;&#23398;&#20064;&#27880;&#24847;&#26426;&#21046;&#30340;&#21452;Transformer&#22312;&#30000;&#32435;&#35199;&#20234;&#26031;&#26364;&#36807;&#31243;&#20013;&#36827;&#34892;&#25925;&#38556;&#26816;&#27979;&#19982;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Twin Transformer using Gated Dynamic Learnable Attention mechanism for Fault Detection and Diagnosis in the Tennessee Eastman Process
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10842
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;Transformer&#27169;&#22411;&#65292;&#32467;&#21512;&#38376;&#25511;&#21160;&#24577;&#21487;&#23398;&#20064;&#27880;&#24847;&#26426;&#21046;&#65292;&#29992;&#20110;&#30000;&#32435;&#35199;&#20234;&#26031;&#26364;&#36807;&#31243;&#30340;&#25925;&#38556;&#26816;&#27979;&#19982;&#35786;&#26029;&#65292;&#25552;&#39640;&#24615;&#33021;&#36890;&#36807;&#29420;&#31435;&#22788;&#29702;&#36755;&#20837;&#25968;&#25454;&#21644;&#25552;&#21462;&#22810;&#26679;&#21270;&#20449;&#24687;&#65292;&#20197;&#21450;&#21160;&#24577;&#23398;&#20064;&#36866;&#24212;&#24615;&#35843;&#25972;&#27880;&#24847;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25925;&#38556;&#26816;&#27979;&#21644;&#35786;&#26029;&#65288;FDD&#65289;&#23545;&#20110;&#30830;&#20445;&#24037;&#19994;&#36807;&#31243;&#30340;&#23433;&#20840;&#24615;&#21644;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;FDD&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#30000;&#32435;&#35199;&#20234;&#26031;&#26364;&#36807;&#31243;&#65288;TEP&#65289;&#65292;&#36825;&#26159;&#21270;&#24037;&#36807;&#31243;&#25511;&#21046;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#20934;&#12290;&#35813;&#27169;&#22411;&#37319;&#29992;&#20004;&#20010;&#29420;&#31435;&#30340;Transformer&#20998;&#25903;&#65292;&#33021;&#22815;&#29420;&#31435;&#22788;&#29702;&#36755;&#20837;&#25968;&#25454;&#24182;&#25552;&#21462;&#22810;&#26679;&#21270;&#30340;&#20449;&#24687;&#12290;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27880;&#24847;&#26426;&#21046;&#65292;&#21363;&#38376;&#25511;&#21160;&#24577;&#21487;&#23398;&#20064;&#27880;&#24847;&#65288;GDLAttention&#65289;&#65292;&#23427;&#38598;&#25104;&#20102;&#38376;&#25511;&#26426;&#21046;&#21644;&#21160;&#24577;&#23398;&#20064;&#33021;&#21147;&#12290;&#38376;&#25511;&#26426;&#21046;&#35843;&#33410;&#27880;&#24847;&#26435;&#37325;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#20851;&#27880;&#36755;&#20837;&#30340;&#26368;&#30456;&#20851;&#37096;&#20998;&#12290;&#21160;&#24577;&#23398;&#20064;&#26041;&#27861;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#35843;&#25972;&#27880;&#24847;&#31574;&#30053;&#65292;&#26377;&#21487;&#33021;&#25552;&#39640;&#24615;&#33021;&#12290;&#27880;&#24847;&#26426;&#21046;&#20351;&#29992;&#21452;&#32447;&#24615;&#30456;&#20284;&#24615;&#20989;&#25968;&#65292;&#25552;&#20379;&#26356;&#22823;&#30340;&#28789;&#27963;&#24615;&#26469;&#25429;&#25417;&#26597;&#35810;&#21644;&#36755;&#20837;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10842v1 Announce Type: cross  Abstract: Fault detection and diagnosis (FDD) is a crucial task for ensuring the safety and efficiency of industrial processes. We propose a novel FDD methodology for the Tennessee Eastman Process (TEP), a widely used benchmark for chemical process control. The model employs two separate Transformer branches, enabling independent processing of input data and potential extraction of diverse information. A novel attention mechanism, Gated Dynamic Learnable Attention (GDLAttention), is introduced which integrates a gating mechanism and dynamic learning capabilities. The gating mechanism modulates the attention weights, allowing the model to focus on the most relevant parts of the input. The dynamic learning approach adapts the attention strategy during training, potentially leading to improved performance. The attention mechanism uses a bilinear similarity function, providing greater flexibility in capturing complex relationships between query and 
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#38750;&#35821;&#35328;&#20803;&#32032;&#36827;&#34892;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#30340;&#39044;&#35757;&#32451;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#24182;&#22312;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#20013;&#21462;&#24471;&#20102;&#20248;&#36234;&#34920;&#29616;</title><link>https://arxiv.org/abs/2403.10576</link><description>&lt;p&gt;
&#24573;&#30053;&#25105;&#20294;&#19981;&#35201;&#26367;&#20195;&#25105;&#65306;&#21033;&#29992;&#38750;&#35821;&#35328;&#20803;&#32032;&#36827;&#34892;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#30340;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Ignore Me But Don't Replace Me: Utilizing Non-Linguistic Elements for Pretraining on the Cybersecurity Domain
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10576
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#38750;&#35821;&#35328;&#20803;&#32032;&#36827;&#34892;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#30340;&#39044;&#35757;&#32451;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#24182;&#22312;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#20013;&#21462;&#24471;&#20102;&#20248;&#36234;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#32593;&#32476;&#23433;&#20840;&#20449;&#24687;&#36890;&#24120;&#25216;&#26415;&#22797;&#26434;&#19988;&#36890;&#36807;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20256;&#36882;&#65292;&#20351;&#24471;&#33258;&#21160;&#21270;&#22788;&#29702;&#32593;&#32476;&#23041;&#32961;&#24773;&#25253;&#21464;&#24471;&#26497;&#20855;&#25361;&#25112;&#24615;&#12290;&#38024;&#23545;&#28041;&#21450;&#39640;&#24230;&#19987;&#19994;&#30693;&#35782;&#30340;&#25991;&#26412;&#39046;&#22495;&#65292;&#22522;&#20110;&#39046;&#22495;&#35821;&#26009;&#24211;&#30340;&#39044;&#35757;&#32451;&#19968;&#30452;&#26159;&#35821;&#35328;&#27169;&#22411;&#33719;&#21462;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#30340;&#19968;&#31181;&#24120;&#35265;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#32593;&#32476;&#23433;&#20840;&#25991;&#26412;&#36890;&#24120;&#21253;&#21547;&#38750;&#35821;&#35328;&#20803;&#32032;&#65288;&#22914;URL&#21644;&#21704;&#24076;&#20540;&#65289;&#65292;&#36825;&#21487;&#33021;&#19982;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#19981;&#36866;&#29992;&#12290;&#20808;&#21069;&#22312;&#20854;&#20182;&#39046;&#22495;&#30340;&#24037;&#20316;&#20013;&#65292;&#24050;&#23558;&#27492;&#31867;&#25991;&#26412;&#35270;&#20026;&#22122;&#38899;&#36827;&#34892;&#31227;&#38500;&#25110;&#36807;&#28388;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#23578;&#26410;&#24471;&#21040;&#35843;&#26597;&#65292;&#29305;&#21035;&#26159;&#22312;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#19979;&#28216;&#20219;&#21153;&#21644;&#25506;&#27979;&#20219;&#21153;&#35780;&#20272;&#20102;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#31574;&#30053;&#65288;&#36873;&#25321;&#24615;MLM&#21644;&#32852;&#21512;&#35757;&#32451;NLE&#26631;&#35760;&#20998;&#31867;&#65289;&#20248;&#20110;&#24120;&#29992;&#30340;&#26367;&#25442;&#38750;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10576v1 Announce Type: cross  Abstract: Cybersecurity information is often technically complex and relayed through unstructured text, making automation of cyber threat intelligence highly challenging. For such text domains that involve high levels of expertise, pretraining on in-domain corpora has been a popular method for language models to obtain domain expertise. However, cybersecurity texts often contain non-linguistic elements (such as URLs and hash values) that could be unsuitable with the established pretraining methodologies. Previous work in other domains have removed or filtered such text as noise, but the effectiveness of these methods have not been investigated, especially in the cybersecurity domain. We propose different pretraining methodologies and evaluate their effectiveness through downstream tasks and probing tasks. Our proposed strategy (selective MLM and jointly training NLE token classification) outperforms the commonly taken approach of replacing non-l
&lt;/p&gt;</description></item><item><title>FeatUp&#26159;&#19968;&#20010;&#20219;&#21153;&#21644;&#27169;&#22411;&#26080;&#20851;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#28145;&#24230;&#29305;&#24449;&#20013;&#24674;&#22797;&#20002;&#22833;&#30340;&#31354;&#38388;&#20449;&#24687;&#65292;&#20174;&#32780;&#20351;&#29305;&#24449;&#21487;&#20197;&#20197;&#20219;&#20309;&#20998;&#36776;&#29575;&#37325;&#24314;&#65292;&#22312;&#29616;&#26377;&#24212;&#29992;&#20013;&#21462;&#24471;&#20998;&#36776;&#29575;&#21644;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.10516</link><description>&lt;p&gt;
FeatUp: &#19968;&#20010;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#29305;&#24449;&#20219;&#24847;&#20998;&#36776;&#29575;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
FeatUp: A Model-Agnostic Framework for Features at Any Resolution
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10516
&lt;/p&gt;
&lt;p&gt;
FeatUp&#26159;&#19968;&#20010;&#20219;&#21153;&#21644;&#27169;&#22411;&#26080;&#20851;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#28145;&#24230;&#29305;&#24449;&#20013;&#24674;&#22797;&#20002;&#22833;&#30340;&#31354;&#38388;&#20449;&#24687;&#65292;&#20174;&#32780;&#20351;&#29305;&#24449;&#21487;&#20197;&#20197;&#20219;&#20309;&#20998;&#36776;&#29575;&#37325;&#24314;&#65292;&#22312;&#29616;&#26377;&#24212;&#29992;&#20013;&#21462;&#24471;&#20998;&#36776;&#29575;&#21644;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29305;&#24449;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#30740;&#31350;&#30340;&#22522;&#30707;&#65292;&#25429;&#25417;&#22270;&#20687;&#35821;&#20041;&#24182;&#20351;&#31038;&#21306;&#33021;&#22815;&#35299;&#20915;&#19979;&#28216;&#20219;&#21153;&#65292;&#21363;&#20351;&#22312;&#38646;&#25110;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#20063;&#33021;&#20570;&#21040;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#29305;&#24449;&#36890;&#24120;&#32570;&#20047;&#31354;&#38388;&#20998;&#36776;&#29575;&#65292;&#26080;&#27861;&#30452;&#25509;&#25191;&#34892;&#20687;&#20998;&#21106;&#21644;&#28145;&#24230;&#39044;&#27979;&#36825;&#26679;&#30340;&#31264;&#23494;&#39044;&#27979;&#20219;&#21153;&#65292;&#22240;&#20026;&#27169;&#22411;&#20250;&#36807;&#20110;&#32858;&#21512;&#22823;&#33539;&#22260;&#30340;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;FeatUp&#65292;&#19968;&#20010;&#20219;&#21153;&#21644;&#27169;&#22411;&#26080;&#20851;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#24674;&#22797;&#28145;&#24230;&#29305;&#24449;&#20013;&#20002;&#22833;&#30340;&#31354;&#38388;&#20449;&#24687;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;FeatUp&#30340;&#20004;&#20010;&#21464;&#20307;&#65306;&#19968;&#20010;&#22312;&#21333;&#27425;&#21069;&#21521;&#20256;&#36882;&#20013;&#24341;&#23548;&#20855;&#26377;&#39640;&#20998;&#36776;&#29575;&#20449;&#21495;&#30340;&#29305;&#24449;&#65292;&#21478;&#19968;&#20010;&#36866;&#24212;&#21333;&#20010;&#22270;&#20687;&#24182;&#20197;&#20219;&#20309;&#20998;&#36776;&#29575;&#37325;&#26500;&#29305;&#24449;&#30340;&#38544;&#24335;&#27169;&#22411;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#20351;&#29992;&#20102;&#19968;&#20010;&#20855;&#26377;&#19982; NeRF &#31867;&#20284;&#30340;&#28145;&#24230;&#31867;&#27604;&#30340;&#22810;&#35270;&#22270;&#19968;&#33268;&#24615;&#25439;&#22833;&#12290;&#25105;&#20204;&#30340;&#29305;&#24449;&#20445;&#30041;&#20854;&#21407;&#22987;&#35821;&#20041;&#65292;&#24182;&#21487;&#20197;&#26367;&#25442;&#29616;&#26377;&#24212;&#29992;&#31243;&#24207;&#65292;&#21363;&#20351;&#19981;&#37325;&#26032;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10516v1 Announce Type: cross  Abstract: Deep features are a cornerstone of computer vision research, capturing image semantics and enabling the community to solve downstream tasks even in the zero- or few-shot regime. However, these features often lack the spatial resolution to directly perform dense prediction tasks like segmentation and depth prediction because models aggressively pool information over large areas. In this work, we introduce FeatUp, a task- and model-agnostic framework to restore lost spatial information in deep features. We introduce two variants of FeatUp: one that guides features with high-resolution signal in a single forward pass, and one that fits an implicit model to a single image to reconstruct features at any resolution. Both approaches use a multi-view consistency loss with deep analogies to NeRFs. Our features retain their original semantics and can be swapped into existing applications to yield resolution and performance gains even without re-
&lt;/p&gt;</description></item><item><title>&#35813;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#22810;&#27169;&#24577;Transformer&#26550;&#26500;&#30340;&#38899;&#35270;&#39057;&#24773;&#24863;&#35782;&#21035;&#31995;&#32479;&#65292;&#33021;&#22815;&#22312;&#35270;&#39057;&#20013;&#21516;&#26102;&#21033;&#29992;&#38899;&#39057;&#21644;&#35270;&#35273;&#32447;&#32034;&#65292;&#23454;&#29616;&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.10488</link><description>&lt;p&gt;
&#37326;&#22806;&#24773;&#24863;&#32500;&#24230;&#35782;&#21035;&#30340;&#32852;&#21512;&#22810;&#27169;&#24577;Transformer
&lt;/p&gt;
&lt;p&gt;
Joint Multimodal Transformer for Dimensional Emotional Recognition in the Wild
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10488
&lt;/p&gt;
&lt;p&gt;
&#35813;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#22810;&#27169;&#24577;Transformer&#26550;&#26500;&#30340;&#38899;&#35270;&#39057;&#24773;&#24863;&#35782;&#21035;&#31995;&#32479;&#65292;&#33021;&#22815;&#22312;&#35270;&#39057;&#20013;&#21516;&#26102;&#21033;&#29992;&#38899;&#39057;&#21644;&#35270;&#35273;&#32447;&#32034;&#65292;&#23454;&#29616;&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35270;&#39057;&#20013;&#36827;&#34892;&#38899;&#35270;&#39057;&#24773;&#24863;&#35782;&#21035;&#23545;&#20110;&#21333;&#27169;&#24615;&#33021;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#23427;&#26377;&#25928;&#22320;&#21033;&#29992;&#20102;&#35270;&#35273;&#21644;&#21548;&#35273;&#27169;&#24577;&#20043;&#38388;&#20197;&#21450;&#27169;&#24577;&#20869;&#37096;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#26412;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20851;&#38190;&#20132;&#21449;&#27880;&#24847;&#21147;&#30340;&#32852;&#21512;&#22810;&#27169;&#24577;Transformer&#26550;&#26500;&#30340;&#38899;&#35270;&#39057;&#24773;&#24863;&#35782;&#21035;&#31995;&#32479;&#12290;&#35813;&#26694;&#26550;&#26088;&#22312;&#21033;&#29992;&#35270;&#39057;&#20013;&#38899;&#39057;&#21644;&#35270;&#35273;&#32447;&#32034;&#65288;&#38754;&#37096;&#34920;&#24773;&#21644;&#35821;&#38899;&#27169;&#24335;&#65289;&#30340;&#20114;&#34917;&#24615;&#65292;&#30456;&#36739;&#20110;&#20165;&#20381;&#36182;&#20110;&#21333;&#19968;&#27169;&#24577;&#65292;&#23454;&#29616;&#20102;&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#21033;&#29992;&#21333;&#29420;&#30340;&#20027;&#24178;&#32593;&#32476;&#26469;&#25429;&#33719;&#27599;&#31181;&#27169;&#24577;&#65288;&#38899;&#39057;&#21644;&#35270;&#35273;&#65289;&#20869;&#37096;&#30340;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290;&#38543;&#21518;&#65292;&#19968;&#20010;&#32852;&#21512;&#22810;&#27169;&#24577;Transformer&#26550;&#26500;&#38598;&#25104;&#20102;&#21508;&#33258;&#27169;&#24577;&#30340;&#23884;&#20837;&#65292;&#20351;&#35813;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#33719;&#27169;&#24577;&#38388;&#65288;&#38899;&#39057;&#21644;&#35270;&#35273;&#20043;&#38388;&#65289;&#21644;&#27169;&#24577;&#20869;&#37096;&#65288;&#27599;&#31181;&#27169;&#24577;&#20869;&#37096;&#65289;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10488v1 Announce Type: cross  Abstract: Audiovisual emotion recognition (ER) in videos has immense potential over unimodal performance. It effectively leverages the inter- and intra-modal dependencies between visual and auditory modalities. This work proposes a novel audio-visual emotion recognition system utilizing a joint multimodal transformer architecture with key-based cross-attention. This framework aims to exploit the complementary nature of audio and visual cues (facial expressions and vocal patterns) in videos, leading to superior performance compared to solely relying on a single modality. The proposed model leverages separate backbones for capturing intra-modal temporal dependencies within each modality (audio and visual). Subsequently, a joint multimodal transformer architecture integrates the individual modality embeddings, enabling the model to effectively capture inter-modal (between audio and visual) and intra-modal (within each modality) relationships. Exten
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#39044;&#23450;&#20041;&#30340;&#20856;&#22411;&#20154;&#21475;&#32479;&#35745;&#25991;&#26412;&#24182;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#21152;&#20837;&#27491;&#21017;&#21270;&#39033;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#26041;&#27861;&#26377;&#25928;&#20943;&#36731;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31038;&#20250;&#20559;&#35265;&#65292;&#21516;&#26102;&#19981;&#38656;&#35201;&#20381;&#36182;&#26174;&#24335;&#30340;&#20154;&#21475;&#32479;&#35745;&#26631;&#31614;&#12290;</title><link>https://arxiv.org/abs/2403.09516</link><description>&lt;p&gt;
&#21033;&#29992;&#20856;&#22411;&#34920;&#31034;&#20943;&#36731;&#31038;&#20250;&#20559;&#35265;&#32780;&#19981;&#20351;&#29992;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Leveraging Prototypical Representations for Mitigating Social Bias without Demographic Information
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09516
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#39044;&#23450;&#20041;&#30340;&#20856;&#22411;&#20154;&#21475;&#32479;&#35745;&#25991;&#26412;&#24182;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#21152;&#20837;&#27491;&#21017;&#21270;&#39033;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#26041;&#27861;&#26377;&#25928;&#20943;&#36731;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31038;&#20250;&#20559;&#35265;&#65292;&#21516;&#26102;&#19981;&#38656;&#35201;&#20381;&#36182;&#26174;&#24335;&#30340;&#20154;&#21475;&#32479;&#35745;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20943;&#36731;&#31038;&#20250;&#20559;&#35265;&#36890;&#24120;&#38656;&#35201;&#35782;&#21035;&#19982;&#27599;&#20010;&#25968;&#25454;&#26679;&#26412;&#30456;&#20851;&#32852;&#30340;&#31038;&#20250;&#32676;&#20307;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DAFair&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31038;&#20250;&#20559;&#35265;&#38382;&#39064;&#12290;&#19982;&#20381;&#36182;&#26174;&#24335;&#20154;&#21475;&#32479;&#35745;&#26631;&#31614;&#30340;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#20219;&#20309;&#27492;&#31867;&#20449;&#24687;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#21033;&#29992;&#39044;&#23450;&#20041;&#30340;&#20154;&#21475;&#32479;&#35745;&#20856;&#22411;&#25991;&#26412;&#65292;&#24182;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#21152;&#20837;&#19968;&#20010;&#27491;&#21017;&#21270;&#39033;&#26469;&#20943;&#36731;&#27169;&#22411;&#34920;&#31034;&#20013;&#30340;&#20559;&#35265;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20219;&#21153;&#21644;&#20004;&#20010;&#27169;&#22411;&#19978;&#30340;&#23454;&#35777;&#32467;&#26524;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#20043;&#21069;&#19981;&#20381;&#36182;&#26631;&#35760;&#25968;&#25454;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#20351;&#29992;&#26377;&#38480;&#30340;&#20154;&#21475;&#32479;&#35745;&#26631;&#27880;&#25968;&#25454;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20063;&#20248;&#20110;&#24120;&#35265;&#30340;&#21435;&#20559;&#35265;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09516v1 Announce Type: new  Abstract: Mitigating social biases typically requires identifying the social groups associated with each data sample. In this paper, we present DAFair, a novel approach to address social bias in language models. Unlike traditional methods that rely on explicit demographic labels, our approach does not require any such information. Instead, we leverage predefined prototypical demographic texts and incorporate a regularization term during the fine-tuning process to mitigate bias in the model's representations. Our empirical results across two tasks and two models demonstrate the effectiveness of our method compared to previous approaches that do not rely on labeled data. Moreover, with limited demographic-annotated data, our approach outperforms common debiasing approaches.
&lt;/p&gt;</description></item><item><title>&#39318;&#27425;&#25552;&#20986;&#20102;&#20174;&#21463;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#30340;&#35270;&#35282;&#20840;&#29699;&#39318;&#27425;&#25552;&#20379;&#28151;&#21512;&#21160;&#21147;&#36710;&#36742;&#30340;&#21463;&#32422;&#26463;&#26368;&#20248;&#29123;&#26009;&#28040;&#32791;&#30340;&#25968;&#23398;&#34920;&#36798;&#65292;&#24182;&#39318;&#27425;&#21033;&#29992;&#20004;&#31181;&#20027;&#27969;&#30340;&#21463;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26469;&#33719;&#24471;&#36710;&#36742;&#22312;&#30005;&#27744;&#30005;&#27668;&#24179;&#34913;&#26465;&#20214;&#19979;&#30340;&#26368;&#23567;&#29123;&#26009;&#28040;&#32791;&#12290;</title><link>https://arxiv.org/abs/2403.07503</link><description>&lt;p&gt;
&#21463;&#32422;&#26463;&#30340;&#28151;&#21512;&#21160;&#21147;&#36710;&#36742;&#30340;&#26368;&#20248;&#29123;&#26009;&#28040;&#32791;&#65306;&#19968;&#31181;&#21463;&#32422;&#26463;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Constrained Optimal Fuel Consumption of HEV: A Constrained Reinforcement Learning Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07503
&lt;/p&gt;
&lt;p&gt;
&#39318;&#27425;&#25552;&#20986;&#20102;&#20174;&#21463;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#30340;&#35270;&#35282;&#20840;&#29699;&#39318;&#27425;&#25552;&#20379;&#28151;&#21512;&#21160;&#21147;&#36710;&#36742;&#30340;&#21463;&#32422;&#26463;&#26368;&#20248;&#29123;&#26009;&#28040;&#32791;&#30340;&#25968;&#23398;&#34920;&#36798;&#65292;&#24182;&#39318;&#27425;&#21033;&#29992;&#20004;&#31181;&#20027;&#27969;&#30340;&#21463;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26469;&#33719;&#24471;&#36710;&#36742;&#22312;&#30005;&#27744;&#30005;&#27668;&#24179;&#34913;&#26465;&#20214;&#19979;&#30340;&#26368;&#23567;&#29123;&#26009;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#21160;&#21147;&#36710;&#36742;&#65288;HEVs&#65289;&#22240;&#33021;&#26356;&#22909;&#22320;&#32467;&#21512;&#20869;&#29123;&#26426;&#21644;&#30005;&#21160;&#26426;&#30340;&#24037;&#20316;&#29305;&#24615;&#32780;&#26085;&#30410;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#22312;&#29305;&#23450;&#35013;&#37197;&#26465;&#20214;&#21644;&#29305;&#23450;&#36895;&#24230;&#26354;&#32447;&#19979;&#65292;HEV&#30340;&#26368;&#23567;&#29123;&#26009;&#28040;&#32791;&#23545;&#20110;&#30005;&#27744;&#30005;&#27668;&#24179;&#34913;&#24773;&#20917;&#20173;&#38656;&#22312;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#36827;&#19968;&#27493;&#38416;&#26126;&#12290;&#38024;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#24037;&#20316;&#39318;&#27425;&#20174;&#21463;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#65288;CRL&#65289;&#30340;&#35270;&#35282;&#25552;&#20379;&#20102;&#21463;&#32422;&#26463;&#30340;&#26368;&#20248;&#29123;&#26009;&#28040;&#32791;&#65288;COFC&#65289;&#30340;&#25968;&#23398;&#34920;&#36798;&#12290;&#21516;&#26102;&#65292;&#39318;&#27425;&#21033;&#29992;&#20102;&#20004;&#31181;&#20027;&#27969;&#30340;CRL&#26041;&#27861;&#65292;&#21363;&#21463;&#32422;&#26463;&#21464;&#20998;&#31574;&#30053;&#20248;&#21270;&#65288;CVPO&#65289;&#21644;&#22522;&#20110;&#25289;&#26684;&#26391;&#26085;&#30340;&#26041;&#27861;&#65292;&#20197;&#22312;&#30005;&#27744;&#30005;&#27668;&#24179;&#34913;&#26465;&#20214;&#19979;&#33719;&#24471;&#36710;&#36742;&#30340;&#26368;&#23567;&#29123;&#26009;&#28040;&#32791;&#12290;&#25105;&#20204;&#22312;&#30693;&#21517;&#30340;&#26222;&#38160;&#26031;&#20016;&#30000;&#28151;&#21512;&#31995;&#32479;&#65288;THS&#65289;&#19979;NEDC&#26465;&#20214;&#19979;&#36827;&#34892;&#26696;&#20363;&#30740;&#31350;&#65307;&#25105;&#20204;g
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07503v1 Announce Type: new  Abstract: Hybrid electric vehicles (HEVs) are becoming increasingly popular because they can better combine the working characteristics of internal combustion engines and electric motors. However, the minimum fuel consumption of an HEV for a battery electrical balance case under a specific assembly condition and a specific speed curve still needs to be clarified in academia and industry. Regarding this problem, this work provides the mathematical expression of constrained optimal fuel consumption (COFC) from the perspective of constrained reinforcement learning (CRL) for the first time globally. Also, two mainstream approaches of CRL, constrained variational policy optimization (CVPO) and Lagrangian-based approaches, are utilized for the first time to obtain the vehicle's minimum fuel consumption under the battery electrical balance condition. We conduct case studies on the well-known Prius TOYOTA hybrid system (THS) under the NEDC condition; we g
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#28508;&#22312;&#23545;&#25239;&#35757;&#32451;&#65288;LAT&#65289;&#26469;&#38450;&#24481;AI&#31995;&#32479;&#20013;&#26410;&#39044;&#35265;&#30340;&#25925;&#38556;&#27169;&#24335;&#65292;&#36890;&#36807;&#21033;&#29992;&#32593;&#32476;&#23454;&#38469;&#29992;&#20110;&#39044;&#27979;&#30340;&#21387;&#32553;&#12289;&#25277;&#35937;&#21644;&#32467;&#26500;&#21270;&#27010;&#24565;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#26377;&#25928;&#28165;&#38500;&#20102;&#24694;&#24847;&#36719;&#20214;&#21644;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2403.05030</link><description>&lt;p&gt;
&#21033;&#29992;&#28508;&#22312;&#23545;&#25239;&#35757;&#32451;&#38450;&#24481;&#26410;&#39044;&#35265;&#30340;&#25925;&#38556;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Defending Against Unforeseen Failure Modes with Latent Adversarial Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05030
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#28508;&#22312;&#23545;&#25239;&#35757;&#32451;&#65288;LAT&#65289;&#26469;&#38450;&#24481;AI&#31995;&#32479;&#20013;&#26410;&#39044;&#35265;&#30340;&#25925;&#38556;&#27169;&#24335;&#65292;&#36890;&#36807;&#21033;&#29992;&#32593;&#32476;&#23454;&#38469;&#29992;&#20110;&#39044;&#27979;&#30340;&#21387;&#32553;&#12289;&#25277;&#35937;&#21644;&#32467;&#26500;&#21270;&#27010;&#24565;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#26377;&#25928;&#28165;&#38500;&#20102;&#24694;&#24847;&#36719;&#20214;&#21644;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#26377;&#26102;&#22312;&#37096;&#32626;&#21518;&#20250;&#23637;&#31034;&#20986;&#26377;&#23475;&#30340;&#24847;&#22806;&#34892;&#20026;&#12290;&#23613;&#31649;&#24320;&#21457;&#20154;&#21592;&#36827;&#34892;&#20102;&#22823;&#37327;&#35786;&#26029;&#21644;&#35843;&#35797;&#65292;&#36825;&#31181;&#24773;&#20917;&#32463;&#24120;&#21457;&#29983;&#12290;&#30001;&#20110;&#25915;&#20987;&#38754;&#38750;&#24120;&#24191;&#27867;&#65292;&#20174;&#27169;&#22411;&#20013;&#20943;&#23569;&#39118;&#38505;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#32791;&#23613;&#22320;&#25628;&#32034;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#22833;&#36133;&#30340;&#36755;&#20837;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#32418;&#38431;&#21644;&#23545;&#25239;&#35757;&#32451;&#65288;AT&#65289;&#36890;&#24120;&#29992;&#20110;&#20351;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#26356;&#21152;&#20581;&#22766;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24182;&#19981;&#36275;&#20197;&#36991;&#20813;&#35768;&#22810;&#19982;&#23545;&#25239;&#35757;&#32451;&#19981;&#21516;&#30340;&#30495;&#23454;&#19990;&#30028;&#25925;&#38556;&#27169;&#24335;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#28508;&#22312;&#23545;&#25239;&#35757;&#32451;&#65288;LAT&#65289;&#26469;&#38450;&#24481;&#28431;&#27934;&#65292;&#32780;&#26080;&#38656;&#29983;&#25104;&#24341;&#21457;&#36825;&#20123;&#28431;&#27934;&#30340;&#36755;&#20837;&#12290;LAT&#21033;&#29992;&#32593;&#32476;&#23454;&#38469;&#29992;&#20110;&#39044;&#27979;&#30340;&#21387;&#32553;&#12289;&#25277;&#35937;&#21644;&#32467;&#26500;&#21270;&#27010;&#24565;&#30340;&#28508;&#22312;&#34920;&#31034;&#12290;&#25105;&#20204;&#20351;&#29992;LAT&#26469;&#28165;&#38500;&#24694;&#24847;&#36719;&#20214;&#24182;&#38450;&#24481;&#38024;&#23545;&#20445;&#30041;&#31867;&#21035;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;&#25105;&#20204;&#23637;&#31034;&#22312;&#22270;&#20687;&#20998;&#31867;&#12289;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05030v1 Announce Type: cross  Abstract: AI systems sometimes exhibit harmful unintended behaviors post-deployment. This is often despite extensive diagnostics and debugging by developers. Minimizing risks from models is challenging because the attack surface is so large. It is not tractable to exhaustively search for inputs that may cause a model to fail. Red-teaming and adversarial training (AT) are commonly used to make AI systems more robust. However, they have not been sufficient to avoid many real-world failure modes that differ from the ones adversarially trained on. In this work, we utilize latent adversarial training (LAT) to defend against vulnerabilities without generating inputs that elicit them. LAT leverages the compressed, abstract, and structured latent representations of concepts that the network actually uses for prediction. We use LAT to remove trojans and defend against held-out classes of adversarial attacks. We show in image classification, text classifi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#38024;&#23545;SemEval-2024&#20219;&#21153;3&#24320;&#21457;&#30340;&#22810;&#27169;&#24577;&#24773;&#24863;&#22240;&#26524;&#20998;&#26512;&#31995;&#32479;&#65292;&#25552;&#20986;&#20102;&#36890;&#36807;&#20004;&#27493;&#26694;&#26550;&#35299;&#20915;&#22810;&#27169;&#24577;&#24773;&#24863;&#22240;&#26524;&#20998;&#26512;&#25361;&#25112;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.04798</link><description>&lt;p&gt;
JMI&#22312;SemEval 2024&#20219;&#21153;3&#20013;&#30340;&#24212;&#29992;&#65306;&#20351;&#29992;GPT&#21644;instruction-tuned Llama&#27169;&#22411;&#36827;&#34892;&#22810;&#27169;&#24577;&#24773;&#24863;&#22240;&#26524;&#20998;&#26512;&#30340;&#20004;&#27493;&#27861;
&lt;/p&gt;
&lt;p&gt;
JMI at SemEval 2024 Task 3: Two-step approach for multimodal ECAC using in-context learning with GPT and instruction-tuned Llama models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#38024;&#23545;SemEval-2024&#20219;&#21153;3&#24320;&#21457;&#30340;&#22810;&#27169;&#24577;&#24773;&#24863;&#22240;&#26524;&#20998;&#26512;&#31995;&#32479;&#65292;&#25552;&#20986;&#20102;&#36890;&#36807;&#20004;&#27493;&#26694;&#26550;&#35299;&#20915;&#22810;&#27169;&#24577;&#24773;&#24863;&#22240;&#26524;&#20998;&#26512;&#25361;&#25112;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#38024;&#23545;SemEval-2024&#20219;&#21153;3&#65306;&#8220;&#23545;&#35805;&#20013;&#30340;&#22810;&#27169;&#24577;&#24773;&#24863;&#22240;&#26524;&#20998;&#26512;&#31454;&#36187;&#8221;&#24320;&#21457;&#30340;&#31995;&#32479;&#12290;&#26377;&#25928;&#25429;&#25417;&#20154;&#31867;&#23545;&#35805;&#20013;&#30340;&#24773;&#24863;&#38656;&#35201;&#25972;&#21512;&#25991;&#26412;&#12289;&#38899;&#39057;&#21644;&#35270;&#39057;&#31561;&#22810;&#31181;&#27169;&#24577;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22810;&#26679;&#24615;&#27169;&#24577;&#30340;&#22797;&#26434;&#24615;&#32473;&#24320;&#21457;&#39640;&#25928;&#30340;&#22810;&#27169;&#24577;&#24773;&#24863;&#22240;&#26524;&#20998;&#26512;&#31995;&#32479;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#36890;&#36807;&#20004;&#27493;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#25105;&#20204;&#22312;&#23454;&#29616;&#20013;&#37319;&#29992;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#12290;&#22312;&#26041;&#27861;1&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20004;&#20010;&#21333;&#29420;&#30340;Llama 2&#27169;&#22411;&#36827;&#34892;&#24773;&#24863;&#21644;&#21407;&#22240;&#39044;&#27979;&#30340;instruction-tuning&#12290;&#22312;&#26041;&#27861;2&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;GPT-4V&#36827;&#34892;&#20250;&#35805;&#32423;&#35270;&#39057;&#25551;&#36848;&#65292;&#24182;&#20351;&#29992;&#24102;&#26377;GPT 3.5&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#23545;&#27880;&#37322;&#23545;&#35805;&#36827;&#34892;&#22788;&#29702;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#33719;&#24471;&#20102;&#31532;4&#21517;&#65292;&#31995;&#32479;&#28040;&#34701;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04798v1 Announce Type: new  Abstract: This paper presents our system development for SemEval-2024 Task 3: "The Competition of Multimodal Emotion Cause Analysis in Conversations". Effectively capturing emotions in human conversations requires integrating multiple modalities such as text, audio, and video. However, the complexities of these diverse modalities pose challenges for developing an efficient multimodal emotion cause analysis (ECA) system. Our proposed approach addresses these challenges by a two-step framework. We adopt two different approaches in our implementation. In Approach 1, we employ instruction-tuning with two separate Llama 2 models for emotion and cause prediction. In Approach 2, we use GPT-4V for conversation-level video description and employ in-context learning with annotated conversation using GPT 3.5. Our system wins rank 4, and system ablation experiments demonstrate that our proposed solutions achieve significant performance gains. All the experime
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31934;&#30830;&#34920;&#31034;&#22810;&#27169;&#24577;&#20020;&#24202;&#26102;&#38388;&#24207;&#21015;&#30340;&#21160;&#24577;&#23884;&#20837;&#21644;&#26631;&#35760;&#26694;&#26550;</title><link>https://arxiv.org/abs/2403.04012</link><description>&lt;p&gt;
&#29992;&#20110;&#22810;&#27169;&#24577;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#21160;&#24577;&#23884;&#20837;&#21644;&#26631;&#35760;&#30340;&#26102;&#38388;&#20132;&#21449;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
Temporal Cross-Attention for Dynamic Embedding and Tokenization of Multimodal Electronic Health Records
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04012
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31934;&#30830;&#34920;&#31034;&#22810;&#27169;&#24577;&#20020;&#24202;&#26102;&#38388;&#24207;&#21015;&#30340;&#21160;&#24577;&#23884;&#20837;&#21644;&#26631;&#35760;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#31995;&#32479;&#30340;&#24191;&#24230;&#12289;&#35268;&#27169;&#21644;&#26102;&#38388;&#31890;&#24230;&#20026;&#20351;&#29992;&#39034;&#24207;&#28145;&#24230;&#23398;&#20064;&#20272;&#35745;&#20010;&#24615;&#21270;&#21644;&#32972;&#26223;&#24739;&#32773;&#20581;&#24247;&#36712;&#36857;&#25552;&#20379;&#20102;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#39640;&#32500;&#24230;&#12289;&#31232;&#30095;&#24615;&#12289;&#22810;&#27169;&#24577;&#24615;&#12289;&#19981;&#35268;&#21017;&#21644;&#21464;&#37327;&#29305;&#23450;&#30340;&#35760;&#24405;&#39057;&#29575;&#20197;&#21450;&#22312;&#21516;&#26102;&#35760;&#24405;&#22810;&#20010;&#27979;&#37327;&#26102;&#25139;&#37325;&#22797;&#65292;&#23398;&#20064;&#26377;&#29992;&#30340;EHR&#25968;&#25454;&#34920;&#31034;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#21162;&#21147;&#23558;&#32467;&#26500;&#21270;EHR&#21644;&#38750;&#32467;&#26500;&#21270;&#20020;&#24202;&#31508;&#35760;&#34701;&#21512;&#65292;&#34920;&#26126;&#20102;&#26356;&#20934;&#30830;&#39044;&#27979;&#20020;&#24202;&#32467;&#26524;&#30340;&#28508;&#21147;&#65292;&#20294;&#23545;&#30452;&#25509;&#35299;&#20915;&#26102;&#38388;EHR&#25361;&#25112;&#30340;EHR&#23884;&#20837;&#26041;&#27861;&#30340;&#20851;&#27880;&#36739;&#23569;&#8212;&#8212;&#21363;&#36890;&#36807;&#23398;&#20064;&#26469;&#33258;&#22810;&#27169;&#24577;&#24739;&#32773;&#26102;&#38388;&#24207;&#21015;&#30340;&#20855;&#26377;&#26102;&#38388;&#24863;&#30693;&#30340;&#34920;&#31034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#31934;&#30830;&#34920;&#31034;&#22810;&#27169;&#24577;&#20020;&#24202;&#26102;&#38388;&#24207;&#21015;&#30340;&#21160;&#24577;&#23884;&#20837;&#21644;&#26631;&#35760;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04012v1 Announce Type: new  Abstract: The breadth, scale, and temporal granularity of modern electronic health records (EHR) systems offers great potential for estimating personalized and contextual patient health trajectories using sequential deep learning. However, learning useful representations of EHR data is challenging due to its high dimensionality, sparsity, multimodality, irregular and variable-specific recording frequency, and timestamp duplication when multiple measurements are recorded simultaneously. Although recent efforts to fuse structured EHR and unstructured clinical notes suggest the potential for more accurate prediction of clinical outcomes, less focus has been placed on EHR embedding approaches that directly address temporal EHR challenges by learning time-aware representations from multimodal patient time series. In this paper, we introduce a dynamic embedding and tokenization framework for precise representation of multimodal clinical time series that
&lt;/p&gt;</description></item><item><title>3D&#25193;&#25955;&#31574;&#30053;&#65288;DP3&#65289;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#35270;&#35273;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;3D&#35270;&#35273;&#34920;&#31034;&#30340;&#24378;&#22823;&#24615;&#32467;&#21512;&#21040;&#25193;&#25955;&#31574;&#30053;&#20013;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#23398;&#20064;&#22797;&#26434;&#25216;&#33021;&#25152;&#38656;&#22823;&#37327;&#20154;&#31867;&#28436;&#31034;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.03954</link><description>&lt;p&gt;
3D&#25193;&#25955;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
3D Diffusion Policy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03954
&lt;/p&gt;
&lt;p&gt;
3D&#25193;&#25955;&#31574;&#30053;&#65288;DP3&#65289;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#35270;&#35273;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;3D&#35270;&#35273;&#34920;&#31034;&#30340;&#24378;&#22823;&#24615;&#32467;&#21512;&#21040;&#25193;&#25955;&#31574;&#30053;&#20013;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#23398;&#20064;&#22797;&#26434;&#25216;&#33021;&#25152;&#38656;&#22823;&#37327;&#20154;&#31867;&#28436;&#31034;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#20223;&#23398;&#20064;&#20026;&#25945;&#25480;&#26426;&#22120;&#20154;&#28789;&#24039;&#25216;&#33021;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#26041;&#24335;&#65307;&#28982;&#32780;&#65292;&#23398;&#20064;&#22797;&#26434;&#32780;&#20855;&#26377;&#36890;&#29992;&#24615;&#30340;&#25216;&#33021;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#31867;&#28436;&#31034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;3D&#25193;&#25955;&#31574;&#30053;&#65288;DP3&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#23558;3D&#35270;&#35273;&#34920;&#31034;&#30340;&#24378;&#22823;&#24615;&#34701;&#20837;&#21040;&#25193;&#25955;&#31574;&#30053;&#20013;&#30340;&#26032;&#39062;&#35270;&#35273;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#25193;&#25955;&#31574;&#30053;&#26159;&#19968;&#31867;&#26377;&#26465;&#20214;&#30340;&#21160;&#20316;&#29983;&#25104;&#27169;&#22411;&#12290;DP3&#30340;&#26680;&#24515;&#35774;&#35745;&#26159;&#21033;&#29992;&#19968;&#20010;&#32039;&#20945;&#30340;3D&#35270;&#35273;&#34920;&#31034;&#65292;&#35813;&#34920;&#31034;&#26159;&#20174;&#31232;&#30095;&#28857;&#20113;&#20013;&#25552;&#21462;&#20986;&#26469;&#30340;&#65292;&#20351;&#29992;&#39640;&#25928;&#30340;&#28857;&#32534;&#30721;&#22120;&#12290;&#22312;&#25105;&#20204;&#28085;&#30422;&#20102;72&#20010;&#20223;&#30495;&#20219;&#21153;&#30340;&#23454;&#39564;&#20013;&#65292;DP3&#20165;&#38656;&#35201;10&#20010;&#28436;&#31034;&#23601;&#21487;&#20197;&#25104;&#21151;&#22788;&#29702;&#22823;&#22810;&#25968;&#20219;&#21153;&#65292;&#24182;&#19988;&#27604;&#22522;&#32447;&#27169;&#22411;&#25552;&#39640;&#20102;55.3%&#12290;&#22312;4&#20010;&#30495;&#23454;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#65292;DP3&#34920;&#29616;&#20986;&#20102;&#39640;&#25104;&#21151;&#29575;&#30340;&#31934;&#30830;&#25511;&#21046;&#65292;&#27599;&#39033;&#20219;&#21153;&#20165;&#38656;40&#27425;&#28436;&#31034;&#21363;&#21487;&#25104;&#21151;&#29575;&#20026;85%&#65292;&#22312;&#19981;&#21516;&#39046;&#22495;&#23637;&#29616;&#20102;&#20986;&#33394;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03954v1 Announce Type: cross  Abstract: Imitation learning provides an efficient way to teach robots dexterous skills; however, learning complex skills robustly and generalizablely usually consumes large amounts of human demonstrations. To tackle this challenging problem, we present 3D Diffusion Policy (DP3), a novel visual imitation learning approach that incorporates the power of 3D visual representations into diffusion policies, a class of conditional action generative models. The core design of DP3 is the utilization of a compact 3D visual representation, extracted from sparse point clouds with an efficient point encoder. In our experiments involving 72 simulation tasks, DP3 successfully handles most tasks with just 10 demonstrations and surpasses baselines with a 55.3% relative improvement. In 4 real robot tasks, DP3 demonstrates precise control with a high success rate of 85%, given only 40 demonstrations of each task, and shows excellent generalization abilities in di
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Vision Mamba&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#65292;&#32467;&#21512;&#20102;&#21367;&#31215;&#23618;&#30340;&#23616;&#37096;&#29305;&#24449;&#25552;&#21462;&#33021;&#21147;&#21644;SSM&#25429;&#25417;&#38271;&#36317;&#31163;&#20381;&#36182;&#24615;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.03849</link><description>&lt;p&gt;
MedMamba: &#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#30340;Vision Mamba
&lt;/p&gt;
&lt;p&gt;
MedMamba: Vision Mamba for Medical Image Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03849
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Vision Mamba&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#65292;&#32467;&#21512;&#20102;&#21367;&#31215;&#23618;&#30340;&#23616;&#37096;&#29305;&#24449;&#25552;&#21462;&#33021;&#21147;&#21644;SSM&#25429;&#25417;&#38271;&#36317;&#31163;&#20381;&#36182;&#24615;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#20013;&#38750;&#24120;&#22522;&#30784;&#21644;&#20851;&#38190;&#30340;&#20219;&#21153;&#12290;&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;CNN&#21644;Transformer&#30340;&#27169;&#22411;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#20998;&#31867;&#21508;&#31181;&#21307;&#23398;&#22270;&#20687;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;CNN&#22312;&#38271;&#36317;&#31163;&#24314;&#27169;&#33021;&#21147;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#65292;&#26080;&#27861;&#26377;&#25928;&#25552;&#21462;&#21307;&#23398;&#22270;&#20687;&#20013;&#30340;&#32454;&#31890;&#24230;&#29305;&#24449;&#65292;&#32780;Transformers&#21463;&#21040;&#20108;&#27425;&#35745;&#31639;&#22797;&#26434;&#24230;&#30340;&#38459;&#30861;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#30001;Mamba&#34920;&#31034;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSM&#65289;&#21487;&#20197;&#39640;&#25928;&#22320;&#24314;&#27169;&#38271;&#36317;&#31163;&#20132;&#20114;&#20316;&#29992;&#21516;&#26102;&#20445;&#25345;&#32447;&#24615;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#30340;Vision Mamba&#65288;MedMamba&#65289;&#12290;&#26356;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;Conv-SSM&#27169;&#22359;&#65292;&#23558;&#21367;&#31215;&#23618;&#30340;&#23616;&#37096;&#29305;&#24449;&#25552;&#21462;&#33021;&#21147;&#19982;SSM&#25429;&#25417;&#38271;&#36317;&#31163;&#20381;&#36182;&#24615;&#30340;&#33021;&#21147;&#32467;&#21512;&#22312;&#19968;&#36215;&#12290;&#20026;&#20102;&#23637;&#31034;MedMamba&#30340;&#28508;&#21147;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03849v1 Announce Type: cross  Abstract: Medical image classification is a very fundamental and crucial task in the field of computer vision. These years, CNN-based and Transformer-based models are widely used in classifying various medical images. Unfortunately, The limitation of CNNs in long-range modeling capabilities prevent them from effectively extracting fine-grained features in medical images , while Transformers are hampered by their quadratic computational complexity. Recent research has shown that the state space model (SSM) represented by Mamba can efficiently model long-range interactions while maintaining linear computational complexity. Inspired by this, we propose Vision Mamba for medical image classification (MedMamba). More specifically, we introduce a novel Conv-SSM module, which combines the local feature extraction ability of convolutional layers with the ability of SSM to capture long-range dependency. To demonstrate the potential of MedMamba, we conduct
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19977;&#20010;&#26032;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#26469;&#27169;&#25311;&#22810;&#20154;&#20043;&#38388;&#30340;&#32454;&#31890;&#24230;&#21160;&#24577;&#65292;&#24182;&#20026;&#31038;&#20132;&#25512;&#29702;&#28216;&#25103;&#35774;&#32622;&#25552;&#20379;&#20102;&#24191;&#27867;&#30340;&#25968;&#25454;&#27880;&#37322;&#65307;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#22522;&#32447;&#26041;&#27861;&#65292;&#21033;&#29992;&#23494;&#38598;&#23545;&#40784;&#30340;&#35821;&#35328;-&#35270;&#35273;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2403.02090</link><description>&lt;p&gt;
&#24314;&#27169;&#22810;&#27169;&#24577;&#31038;&#20132;&#20114;&#21160;&#65306;&#20855;&#26377;&#23494;&#38598;&#23545;&#40784;&#34920;&#31034;&#30340;&#26032;&#25361;&#25112;&#21644;&#22522;&#32447;
&lt;/p&gt;
&lt;p&gt;
Modeling Multimodal Social Interactions: New Challenges and Baselines with Densely Aligned Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02090
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19977;&#20010;&#26032;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#26469;&#27169;&#25311;&#22810;&#20154;&#20043;&#38388;&#30340;&#32454;&#31890;&#24230;&#21160;&#24577;&#65292;&#24182;&#20026;&#31038;&#20132;&#25512;&#29702;&#28216;&#25103;&#35774;&#32622;&#25552;&#20379;&#20102;&#24191;&#27867;&#30340;&#25968;&#25454;&#27880;&#37322;&#65307;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#22522;&#32447;&#26041;&#27861;&#65292;&#21033;&#29992;&#23494;&#38598;&#23545;&#40784;&#30340;&#35821;&#35328;-&#35270;&#35273;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#28041;&#21450;&#35328;&#35821;&#21644;&#38750;&#35328;&#35821;&#32447;&#32034;&#30340;&#31038;&#20132;&#20114;&#21160;&#23545;&#26377;&#25928;&#35299;&#37322;&#31038;&#20132;&#24773;&#22659;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20851;&#20110;&#22810;&#27169;&#24577;&#31038;&#20132;&#32447;&#32034;&#30340;&#20808;&#21069;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#21333;&#20154;&#34892;&#20026;&#19978;&#65292;&#25110;&#20381;&#36182;&#20110;&#19982;&#22810;&#26041;&#29615;&#22659;&#20013;&#30340;&#35805;&#35821;&#23494;&#20999;&#23545;&#40784;&#30340;&#25972;&#20307;&#35270;&#35273;&#34920;&#31034;&#12290;&#23427;&#20204;&#22312;&#24314;&#27169;&#22810;&#26041;&#20114;&#21160;&#30340;&#22797;&#26434;&#21160;&#24577;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19977;&#20010;&#26032;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#20197;&#24314;&#27169;&#22810;&#20154;&#20043;&#38388;&#30340;&#32454;&#31890;&#24230;&#21160;&#24577;&#65306;&#35805;&#35821;&#30446;&#26631;&#35782;&#21035;&#12289;&#20195;&#35789;&#25351;&#20195;&#28040;&#35299;&#21644;&#25552;&#21450;&#29609;&#23478;&#39044;&#27979;&#12290;&#25105;&#20204;&#20026;&#31038;&#20132;&#25512;&#29702;&#28216;&#25103;&#35774;&#32622;&#20013;&#30340;&#36825;&#20123;&#26032;&#25361;&#25112;&#25552;&#20379;&#20102;&#24191;&#27867;&#30340;&#25968;&#25454;&#27880;&#37322;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#22522;&#32447;&#65292;&#36890;&#36807;&#23558;&#35270;&#35273;&#29305;&#24449;&#19982;&#20854;&#23545;&#24212;&#30340;&#35805;&#35821;&#21516;&#27493;&#65292;&#21033;&#29992;&#23494;&#38598;&#23545;&#40784;&#30340;&#35821;&#35328;-&#35270;&#35273;&#34920;&#31034;&#65292;&#36825;&#26377;&#21161;&#20110;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02090v1 Announce Type: cross  Abstract: Understanding social interactions involving both verbal and non-verbal cues is essential to effectively interpret social situations. However, most prior works on multimodal social cues focus predominantly on single-person behaviors or rely on holistic visual representations that are not densely aligned to utterances in multi-party environments. They are limited in modeling the intricate dynamics of multi-party interactions. In this paper, we introduce three new challenging tasks to model the fine-grained dynamics between multiple people: speaking target identification, pronoun coreference resolution, and mentioned player prediction. We contribute extensive data annotations to curate these new challenges in social deduction game settings. Furthermore, we propose a novel multimodal baseline that leverages densely aligned language-visual representations by synchronizing visual features with their corresponding utterances. This facilitates
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#23553;&#35013;&#65288;Nano-Capsulator&#65289;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#26684;&#24335;&#20013;&#21387;&#32553;&#25552;&#31034;&#30340;&#25361;&#25112;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#36716;&#31227;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.18700</link><description>&lt;p&gt;
&#23398;&#20064;&#22312;&#33258;&#28982;&#35821;&#35328;&#26684;&#24335;&#20013;&#21387;&#32553;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning to Compress Prompt in Natural Language Formats
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18700
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#23553;&#35013;&#65288;Nano-Capsulator&#65289;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#26684;&#24335;&#20013;&#21387;&#32553;&#25552;&#31034;&#30340;&#25361;&#25112;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#36716;&#31227;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25797;&#38271;&#22788;&#29702;&#22810;&#20010;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#20294;&#23427;&#20204;&#30340;&#33021;&#21147;&#21463;&#21040;&#38271;&#19978;&#19979;&#25991;&#12289;&#25512;&#29702;&#36895;&#24230;&#24930;&#20197;&#21450;&#35745;&#31639;&#32467;&#26524;&#25104;&#26412;&#39640;&#30340;&#38480;&#21046;&#12290;&#37096;&#32626;&#20855;&#26377;&#31934;&#30830;&#21644;&#20449;&#24687;&#20016;&#23500;&#19978;&#19979;&#25991;&#30340;LLMs&#26377;&#21161;&#20110;&#29992;&#25143;&#26356;&#26377;&#25928;&#21644;&#26356;&#20855;&#25104;&#26412;&#25928;&#30410;&#22320;&#22788;&#29702;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;&#29616;&#26377;&#20316;&#21697;&#20381;&#36182;&#23558;&#38271;&#25552;&#31034;&#19978;&#19979;&#25991;&#21387;&#32553;&#20026;&#36719;&#25552;&#31034;&#12290;&#28982;&#32780;&#65292;&#36719;&#25552;&#31034;&#21387;&#32553;&#22312;&#19981;&#21516;LLM&#20043;&#38388;&#30340;&#21487;&#36716;&#31227;&#24615;&#21463;&#21040;&#38480;&#21046;&#65292;&#23588;&#20854;&#26159;&#22522;&#20110;API&#30340;LLMs&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#20197;LLM&#21487;&#36716;&#31227;&#24615;&#30340;&#24418;&#24335;&#21387;&#32553;&#38271;&#25552;&#31034;&#30340;&#33258;&#28982;&#35821;&#35328;&#24418;&#24335;&#12290;&#36825;&#24102;&#26469;&#20004;&#20010;&#25361;&#25112;&#65306;(i) &#33258;&#28982;&#35821;&#35328;&#65288;NL&#65289;&#25552;&#31034;&#19981;&#20860;&#23481;&#21453;&#21521;&#20256;&#25773;&#65292;(ii) NL&#25552;&#31034;&#22312;&#26045;&#21152;&#38271;&#24230;&#32422;&#26463;&#26041;&#38754;&#32570;&#20047;&#28789;&#27963;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#23553;&#35013;&#65288;Nano-Capsulator&#65289;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18700v1 Announce Type: cross  Abstract: Large language models (LLMs) are great at processing multiple natural language processing tasks, but their abilities are constrained by inferior performance with long context, slow inference speed, and the high cost of computing the results. Deploying LLMs with precise and informative context helps users process large-scale datasets more effectively and cost-efficiently. Existing works rely on compressing long prompt contexts into soft prompts. However, soft prompt compression encounters limitations in transferability across different LLMs, especially API-based LLMs. To this end, this work aims to compress lengthy prompts in the form of natural language with LLM transferability. This poses two challenges: (i) Natural Language (NL) prompts are incompatible with back-propagation, and (ii) NL prompts lack flexibility in imposing length constraints. In this work, we propose a Natural Language Prompt Encapsulation (Nano-Capsulator) framewor
&lt;/p&gt;</description></item><item><title>SEAC&#26159;&#19968;&#31181;&#24377;&#24615;&#26102;&#38388;&#27493;&#38271;&#30340;&#31163;&#31574;&#30053;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65292;&#36890;&#36807;&#21487;&#21464;&#25345;&#32493;&#26102;&#38388;&#30340;&#26102;&#38388;&#27493;&#38271;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#26681;&#25454;&#24773;&#20917;&#25913;&#21464;&#25511;&#21046;&#39057;&#29575;&#65292;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>https://arxiv.org/abs/2402.14961</link><description>&lt;p&gt;
&#24377;&#24615;&#26102;&#38388;&#27493;&#38271;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning with Elastic Time Steps
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14961
&lt;/p&gt;
&lt;p&gt;
SEAC&#26159;&#19968;&#31181;&#24377;&#24615;&#26102;&#38388;&#27493;&#38271;&#30340;&#31163;&#31574;&#30053;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65292;&#36890;&#36807;&#21487;&#21464;&#25345;&#32493;&#26102;&#38388;&#30340;&#26102;&#38388;&#27493;&#38271;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#26681;&#25454;&#24773;&#20917;&#25913;&#21464;&#25511;&#21046;&#39057;&#29575;&#65292;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#36890;&#24120;&#24212;&#29992;&#20110;&#26426;&#22120;&#20154;&#23398;&#20064;&#20197;&#20197;&#22266;&#23450;&#25511;&#21046;&#39057;&#29575;&#25191;&#34892;&#21160;&#20316;&#30340;&#25511;&#21046;&#22120;&#12290;&#37492;&#20110;RL&#31639;&#27861;&#30340;&#31163;&#25955;&#24615;&#36136;&#65292;&#23427;&#20204;&#23545;&#25511;&#21046;&#39057;&#29575;&#30340;&#36873;&#25321;&#30340;&#24433;&#21709;&#35270;&#32780;&#19981;&#35265;&#65306;&#25214;&#21040;&#27491;&#30830;&#30340;&#25511;&#21046;&#39057;&#29575;&#21487;&#33021;&#24456;&#22256;&#38590;&#65292;&#38169;&#35823;&#24448;&#24448;&#20250;&#23548;&#33268;&#36807;&#24230;&#20351;&#29992;&#35745;&#31639;&#36164;&#28304;&#29978;&#33267;&#23548;&#33268;&#26080;&#27861;&#25910;&#25947;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36719;&#24377;&#24615;&#28436;&#21592;-&#35780;&#35770;&#23478;&#65288;SEAC&#65289;, &#19968;&#31181;&#26032;&#39062;&#30340;&#31163;&#31574;&#30053;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;SEAC&#23454;&#29616;&#20102;&#24377;&#24615;&#26102;&#38388;&#27493;&#38271;&#65292;&#21363;&#20855;&#26377;&#24050;&#30693;&#21464;&#21270;&#25345;&#32493;&#26102;&#38388;&#30340;&#26102;&#38388;&#27493;&#38271;&#65292;&#20801;&#35768;&#20195;&#29702;&#26681;&#25454;&#24773;&#20917;&#25913;&#21464;&#20854;&#25511;&#21046;&#39057;&#29575;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;SEAC&#20165;&#22312;&#24517;&#35201;&#26102;&#24212;&#29992;&#25511;&#21046;&#65292;&#26368;&#23567;&#21270;&#35745;&#31639;&#36164;&#28304;&#21644;&#25968;&#25454;&#20351;&#29992;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#35780;&#20272;&#20102;SEAC&#22312;&#29275;&#39039;&#36816;&#21160;&#23398;&#36855;&#23467;&#23548;&#33322;&#20219;&#21153;&#21644;&#19977;&#32500;&#36187;&#36710;&#35270;&#39057;&#28216;&#25103;Trackmania&#20013;&#30340;&#33021;&#21147;&#12290;SEAC&#22312;&#34920;&#29616;&#19978;&#20248;&#20110;SAC&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14961v1 Announce Type: cross  Abstract: Traditional Reinforcement Learning (RL) algorithms are usually applied in robotics to learn controllers that act with a fixed control rate. Given the discrete nature of RL algorithms, they are oblivious to the effects of the choice of control rate: finding the correct control rate can be difficult and mistakes often result in excessive use of computing resources or even lack of convergence.   We propose Soft Elastic Actor-Critic (SEAC), a novel off-policy actor-critic algorithm to address this issue. SEAC implements elastic time steps, time steps with a known, variable duration, which allow the agent to change its control frequency to adapt to the situation. In practice, SEAC applies control only when necessary, minimizing computational resources and data usage.   We evaluate SEAC's capabilities in simulation in a Newtonian kinematics maze navigation task and on a 3D racing video game, Trackmania. SEAC outperforms the SAC baseline in t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#23398;&#20064;&#24191;&#20041;&#26391;&#20043;&#19975;&#26041;&#31243;&#20013;&#35760;&#24518;&#26680;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#27491;&#21017;&#21270;Prony&#26041;&#27861;&#20272;&#35745;&#30456;&#20851;&#20989;&#25968;&#24182;&#22312;Sobolev&#33539;&#25968;Loss&#20989;&#25968;&#21644;RKHS&#27491;&#21017;&#21270;&#19979;&#23454;&#29616;&#22238;&#24402;&#65292;&#22312;&#25351;&#25968;&#21152;&#26435;&#30340;$L^2$&#31354;&#38388;&#20869;&#33719;&#24471;&#25913;&#36827;&#24615;&#33021;&#65292;&#23545;&#27604;&#20854;&#20182;&#22238;&#24402;&#20272;&#35745;&#22120;&#23637;&#31034;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.11705</link><description>&lt;p&gt;
&#22312;&#24191;&#20041;&#26391;&#20043;&#19975;&#26041;&#31243;&#20013;&#23398;&#20064;&#35760;&#24518;&#26680;
&lt;/p&gt;
&lt;p&gt;
Learning Memory Kernels in Generalized Langevin Equations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11705
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#23398;&#20064;&#24191;&#20041;&#26391;&#20043;&#19975;&#26041;&#31243;&#20013;&#35760;&#24518;&#26680;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#27491;&#21017;&#21270;Prony&#26041;&#27861;&#20272;&#35745;&#30456;&#20851;&#20989;&#25968;&#24182;&#22312;Sobolev&#33539;&#25968;Loss&#20989;&#25968;&#21644;RKHS&#27491;&#21017;&#21270;&#19979;&#23454;&#29616;&#22238;&#24402;&#65292;&#22312;&#25351;&#25968;&#21152;&#26435;&#30340;$L^2$&#31354;&#38388;&#20869;&#33719;&#24471;&#25913;&#36827;&#24615;&#33021;&#65292;&#23545;&#27604;&#20854;&#20182;&#22238;&#24402;&#20272;&#35745;&#22120;&#23637;&#31034;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#24191;&#20041;&#26391;&#20043;&#19975;&#26041;&#31243;&#20013;&#30340;&#35760;&#24518;&#26680;&#12290;&#35813;&#26041;&#27861;&#26368;&#21021;&#21033;&#29992;&#27491;&#21017;&#21270;Prony&#26041;&#27861;&#20174;&#36712;&#36857;&#25968;&#25454;&#20013;&#20272;&#35745;&#30456;&#20851;&#20989;&#25968;&#65292;&#28982;&#21518;&#36890;&#36807;&#22522;&#20110;Sobolev&#33539;&#25968;&#30340;&#22238;&#24402;&#21644;RKHS&#27491;&#21017;&#21270;&#26469;&#36827;&#34892;&#22238;&#24402;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20445;&#35777;&#22312;&#25351;&#25968;&#21152;&#26435;&#30340;$L^2$&#31354;&#38388;&#20869;&#33719;&#24471;&#20102;&#25913;&#36827;&#30340;&#24615;&#33021;&#65292;&#26680;&#20272;&#35745;&#35823;&#24046;&#21463;&#25511;&#20110;&#20272;&#35745;&#30456;&#20851;&#20989;&#25968;&#30340;&#35823;&#24046;&#12290;&#25105;&#20204;&#36890;&#36807;&#25968;&#20540;&#31034;&#20363;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#20272;&#35745;&#22120;&#30456;&#23545;&#20110;&#20381;&#36182;&#20110;$L^2$&#25439;&#22833;&#20989;&#25968;&#30340;&#20854;&#20182;&#22238;&#24402;&#20272;&#35745;&#22120;&#20197;&#21450;&#20174;&#36870;&#25289;&#26222;&#25289;&#26031;&#21464;&#25442;&#25512;&#23548;&#20986;&#30340;&#20272;&#35745;&#22120;&#30340;&#20248;&#36234;&#24615;&#65292;&#36825;&#20123;&#31034;&#20363;&#31361;&#26174;&#20102;&#25105;&#20204;&#30340;&#20272;&#35745;&#22120;&#22312;&#21508;&#31181;&#26435;&#37325;&#21442;&#25968;&#36873;&#25321;&#19978;&#30340;&#25345;&#32493;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#21253;&#25324;&#21147;&#21644;&#28418;&#31227;&#39033;&#22312;&#26041;&#31243;&#20013;&#30340;&#24212;&#29992;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11705v1 Announce Type: cross  Abstract: We introduce a novel approach for learning memory kernels in Generalized Langevin Equations. This approach initially utilizes a regularized Prony method to estimate correlation functions from trajectory data, followed by regression over a Sobolev norm-based loss function with RKHS regularization. Our approach guarantees improved performance within an exponentially weighted $L^2$ space, with the kernel estimation error controlled by the error in estimated correlation functions. We demonstrate the superiority of our estimator compared to other regression estimators that rely on $L^2$ loss functions and also an estimator derived from the inverse Laplace transform, using numerical examples that highlight its consistent advantage across various weight parameter selections. Additionally, we provide examples that include the application of force and drift terms in the equation.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#23616;&#37096;&#34394;&#21457;&#29616;&#29575;&#25511;&#21046;&#30340;&#36164;&#28304;&#20998;&#37197;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;$O(\sqrt{T})$&#30340;&#21518;&#24724;&#29575;&#65292;&#24182;&#25351;&#20986;&#36825;&#31181;&#21518;&#24724;&#29575;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#26159;&#19981;&#21487;&#25913;&#36827;&#30340;&#12290;</title><link>https://arxiv.org/abs/2402.11425</link><description>&lt;p&gt;
&#22312;&#32447;&#23616;&#37096;&#34394;&#21457;&#29616;&#29575;&#25511;&#21046;&#65306;&#19968;&#31181;&#36164;&#28304;&#20998;&#37197;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Online Local False Discovery Rate Control: A Resource Allocation Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11425
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#23616;&#37096;&#34394;&#21457;&#29616;&#29575;&#25511;&#21046;&#30340;&#36164;&#28304;&#20998;&#37197;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;$O(\sqrt{T})$&#30340;&#21518;&#24724;&#29575;&#65292;&#24182;&#25351;&#20986;&#36825;&#31181;&#21518;&#24724;&#29575;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#26159;&#19981;&#21487;&#25913;&#36827;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#32447;&#23616;&#37096;&#34394;&#21457;&#29616;&#29575;&#65288;FDR&#65289;&#25511;&#21046;&#38382;&#39064;&#65292;&#20854;&#20013;&#22810;&#20010;&#27979;&#35797;&#34987;&#39034;&#24207;&#36827;&#34892;&#65292;&#30446;&#26631;&#26159;&#26368;&#22823;&#21270;&#24635;&#26399;&#26395;&#30340;&#21457;&#29616;&#27425;&#25968;&#12290;&#25105;&#20204;&#23558;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#19968;&#31181;&#22312;&#32447;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;&#65292;&#28041;&#21450;&#25509;&#21463;/&#25298;&#32477;&#20915;&#31574;&#65292;&#20174;&#39640;&#23618;&#27425;&#26469;&#30475;&#65292;&#36825;&#21487;&#20197;&#34987;&#35270;&#20026;&#19968;&#20010;&#24102;&#26377;&#39069;&#22806;&#19981;&#30830;&#23450;&#24615;&#30340;&#22312;&#32447;&#32972;&#21253;&#38382;&#39064;&#65292;&#21363;&#38543;&#26426;&#39044;&#31639;&#34917;&#20805;&#12290;&#25105;&#20204;&#20174;&#19968;&#33324;&#30340;&#21040;&#36798;&#20998;&#24067;&#24320;&#22987;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;$O(\sqrt{T})$&#30340;&#21518;&#24724;&#12290;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#36825;&#31181;&#21518;&#24724;&#29575;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#26159;&#19981;&#21487;&#25913;&#36827;&#30340;&#26469;&#34917;&#20805;&#36825;&#19968;&#32467;&#26524;&#12290;&#28982;&#21518;&#25105;&#20204;&#23558;&#28966;&#28857;&#36716;&#21521;&#31163;&#25955;&#21040;&#36798;&#20998;&#24067;&#12290;&#25105;&#20204;&#21457;&#29616;&#35768;&#22810;&#29616;&#26377;&#30340;&#22312;&#32447;&#36164;&#28304;&#20998;&#37197;&#25991;&#29486;&#20013;&#30340;&#37325;&#26032;&#35299;&#20915;&#21551;&#21457;&#24335;&#34429;&#28982;&#22312;&#20856;&#22411;&#35774;&#32622;&#20013;&#23454;&#29616;&#20102;&#26377;&#30028;&#30340;&#25439;&#22833;&#65292;&#20294;&#21487;&#33021;&#20250;&#36896;&#25104;$\Omega(\sqrt{T})$&#29978;&#33267;$\Omega(T)$&#30340;&#21518;&#24724;&#12290;&#36890;&#36807;&#35266;&#23519;&#21040;&#20856;&#22411;&#31574;&#30053;&#24448;&#24448;&#22826;&#36807;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11425v1 Announce Type: cross  Abstract: We consider the problem of online local false discovery rate (FDR) control where multiple tests are conducted sequentially, with the goal of maximizing the total expected number of discoveries. We formulate the problem as an online resource allocation problem with accept/reject decisions, which from a high level can be viewed as an online knapsack problem, with the additional uncertainty of random budget replenishment. We start with general arrival distributions and propose a simple policy that achieves a $O(\sqrt{T})$ regret. We complement the result by showing that such regret rate is in general not improvable. We then shift our focus to discrete arrival distributions. We find that many existing re-solving heuristics in the online resource allocation literature, albeit achieve bounded loss in canonical settings, may incur a $\Omega(\sqrt{T})$ or even a $\Omega(T)$ regret. With the observation that canonical policies tend to be too op
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#22522;&#20934;&#26469;&#27979;&#37327;&#35821;&#35328;&#27169;&#22411;&#23545;&#35805;&#20013;&#30340;&#8220;&#20154;&#35774;&#8221;&#28418;&#31227;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;split-softmax&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#26469;&#23545;&#25239;&#27880;&#24847;&#21147;&#34928;&#20943;&#21644;&#8220;&#20154;&#35774;&#8221;&#28418;&#31227;</title><link>https://arxiv.org/abs/2402.10962</link><description>&lt;p&gt;
&#22312;&#35821;&#35328;&#27169;&#22411;&#23545;&#35805;&#20013;&#27979;&#37327;&#21644;&#25511;&#21046;&#8220;&#20154;&#35774;&#8221;&#28418;&#31227;
&lt;/p&gt;
&lt;p&gt;
Measuring and Controlling Persona Drift in Language Model Dialogs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10962
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#22522;&#20934;&#26469;&#27979;&#37327;&#35821;&#35328;&#27169;&#22411;&#23545;&#35805;&#20013;&#30340;&#8220;&#20154;&#35774;&#8221;&#28418;&#31227;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;split-softmax&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#26469;&#23545;&#25239;&#27880;&#24847;&#21147;&#34928;&#20943;&#21644;&#8220;&#20154;&#35774;&#8221;&#28418;&#31227;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#26159;&#23450;&#21046;&#35821;&#35328;&#27169;&#22411;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#26631;&#20934;&#24037;&#20855;&#65292;&#20351;&#20854;&#33021;&#22815;&#25215;&#25285;&#29305;&#23450;&#30340;&#8220;&#20154;&#35774;&#8221;&#12290;&#22312;&#20351;&#29992;&#25552;&#31034;&#26102;&#30340;&#19968;&#20010;&#38544;&#21547;&#20551;&#35774;&#26159;&#65292;&#23427;&#20204;&#23558;&#26159;&#31283;&#23450;&#30340;&#65292;&#22240;&#27492;&#32842;&#22825;&#26426;&#22120;&#20154;&#23558;&#22312;&#25972;&#20010;&#23545;&#35805;&#36807;&#31243;&#20013;&#32487;&#32493;&#26681;&#25454;&#35268;&#23450;&#30340;&#8220;&#20154;&#35774;&#8221;&#29983;&#25104;&#25991;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#37327;&#21270;&#22522;&#20934;&#26469;&#27979;&#35797;&#36825;&#19968;&#20551;&#35774;&#65292;&#36890;&#36807;&#20004;&#20010;&#20010;&#24615;&#21270;&#32842;&#22825;&#26426;&#22120;&#20154;&#20043;&#38388;&#30340;&#33258;&#25105;&#23545;&#35805;&#26469;&#35780;&#20272;&#8220;&#20154;&#35774;&#8221;&#30340;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#23545;&#27969;&#34892;&#27169;&#22411;&#22914;LLaMA2-chat-70B&#36827;&#34892;&#27979;&#35797;&#65292;&#21457;&#29616;&#22312;&#20843;&#36718;&#23545;&#35805;&#20013;&#23384;&#22312;&#26174;&#33879;&#30340;&#8220;&#20154;&#35774;&#8221;&#28418;&#31227;&#12290;&#23545;&#36825;&#19968;&#29616;&#35937;&#30340;&#23454;&#35777;&#21644;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#30001;&#20110;&#38271;&#23545;&#35805;&#20013;&#30340;&#27880;&#24847;&#21147;&#34928;&#20943;&#65292;&#21464;&#21387;&#22120;&#27880;&#24847;&#21147;&#26426;&#21046;&#36215;&#21040;&#20102;&#19968;&#23450;&#20316;&#29992;&#12290;&#20026;&#20102;&#23545;&#25239;&#27880;&#24847;&#21147;&#34928;&#20943;&#21644;&#8220;&#20154;&#35774;&#8221;&#28418;&#31227;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;split-softmax&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#65292;&#19982;&#20004;&#20010;&#24378;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10962v1 Announce Type: cross  Abstract: Prompting is a standard tool for customizing language-model chatbots, enabling them to take on a specific "persona". An implicit assumption in the use of prompts is that they will be stable, so the chatbot will continue to generate text according to the stipulated persona for the duration of a conversation. We propose a quantitative benchmark to test this assumption, evaluating persona stability via self-chats between two personalized chatbots. Testing popular models like LLaMA2-chat-70B, we reveal a significant persona drift within eight rounds of conversations. An empirical and theoretical analysis of this phenomenon suggests the transformer attention mechanism plays a role, due to attention decay over long exchanges. To combat attention decay and persona drift, we propose a lightweight method called split-softmax, which compares favorably against two strong baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#20027;&#25968;&#25454;&#36873;&#25321;&#31574;&#30053;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25968;&#23398;&#25991;&#26412;&#30340;&#33258;&#21160;&#35780;&#20272;&#21644;&#36873;&#25321;&#65292;&#24182;&#36890;&#36807;&#36830;&#32493;&#39044;&#35757;&#32451;&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#12290;&#20027;&#35201;&#21019;&#26032;&#21253;&#25324;&#21033;&#29992;&#20803;&#25552;&#31034;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#39564;&#35777;&#22120;&#65292;&#21457;&#24067;&#20102;&#39640;&#36136;&#37327;&#30340;AutoMathText&#25968;&#25454;&#38598;&#65292;&#24182;&#23454;&#29616;&#20102;&#39044;&#35757;&#32451;&#20196;&#29260;&#25928;&#29575;&#30340;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.07625</link><description>&lt;p&gt;
AutoMathText&#65306;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25968;&#23398;&#25991;&#26412;&#30340;&#33258;&#20027;&#25968;&#25454;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
AutoMathText: Autonomous Data Selection with Language Models for Mathematical Texts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07625
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#20027;&#25968;&#25454;&#36873;&#25321;&#31574;&#30053;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25968;&#23398;&#25991;&#26412;&#30340;&#33258;&#21160;&#35780;&#20272;&#21644;&#36873;&#25321;&#65292;&#24182;&#36890;&#36807;&#36830;&#32493;&#39044;&#35757;&#32451;&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#12290;&#20027;&#35201;&#21019;&#26032;&#21253;&#25324;&#21033;&#29992;&#20803;&#25552;&#31034;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#39564;&#35777;&#22120;&#65292;&#21457;&#24067;&#20102;&#39640;&#36136;&#37327;&#30340;AutoMathText&#25968;&#25454;&#38598;&#65292;&#24182;&#23454;&#29616;&#20102;&#39044;&#35757;&#32451;&#20196;&#29260;&#25928;&#29575;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#36890;&#36807;&#25345;&#32493;&#30340;&#39044;&#35757;&#32451;&#25913;&#21892;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#23398;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31574;&#30053;&#65292;&#21033;&#29992;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33258;&#20027;&#25968;&#25454;&#36873;&#25321;&#12290;&#19982;&#20256;&#32479;&#30340;&#26377;&#20154;&#24037;&#26631;&#27880;&#25968;&#25454;&#30340;&#30417;&#30563;&#24494;&#35843;&#25110;&#35757;&#32451;&#36807;&#30340;&#20998;&#31867;&#22120;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20803;&#25552;&#31034;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#38646;&#26679;&#26412;&#39564;&#35777;&#22120;&#65292;&#33258;&#20027;&#35780;&#20272;&#21644;&#36873;&#25321;&#39640;&#36136;&#37327;&#30340;&#25968;&#23398;&#20869;&#23481;&#65292;&#24182;&#21457;&#24067;&#20102;&#32463;&#36807;&#31574;&#21010;&#30340;&#24320;&#28304;AutoMathText&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#36229;&#36807;200GB&#30340;&#25968;&#25454;&#12290;&#20026;&#20102;&#35777;&#26126;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#23545;AutoMathText&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#36830;&#32493;&#39044;&#35757;&#32451;&#65292;&#20351;&#24471;7B&#21442;&#25968;&#30340;Mistral&#35821;&#35328;&#27169;&#22411;&#22312;MATH&#25968;&#25454;&#38598;&#19978;&#30340;&#19979;&#28216;&#24615;&#33021;&#22823;&#24133;&#25552;&#21319;&#65292;&#32780;&#20196;&#29260;&#25968;&#37327;&#27604;&#20043;&#21069;&#30340;&#36830;&#32493;&#39044;&#35757;&#32451;&#24037;&#20316;&#20943;&#23569;&#20102;&#20960;&#20010;&#25968;&#37327;&#32423;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#22522;&#20934;&#30340;&#39044;&#35757;&#32451;&#20196;&#29260;&#25928;&#29575;&#25552;&#39640;&#20102;2&#20493;&#65292;&#31361;&#26174;&#20102;&#25105;&#20204;&#26041;&#27861;&#22312;&#22686;&#24378;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
To improve language models' proficiency in mathematical reasoning via continual pretraining, we introduce a novel strategy that leverages base language models for autonomous data selection. Departing from conventional supervised fine-tuning or trained classifiers with human-annotated data, our approach utilizes meta-prompted language models as zero-shot verifiers to autonomously evaluate and select high-quality mathematical content, and we release the curated open-source AutoMathText dataset encompassing over 200GB of data. To demonstrate the efficacy of our method, we continuously pretrained a 7B-parameter Mistral language model on the AutoMathText dataset, achieving substantial improvements in downstream performance on the MATH dataset with a token amount reduced by orders of magnitude compared to previous continuous pretraining works. Our method showcases a 2 times increase in pretraining token efficiency compared to baselines, underscoring the potential of our approach in enhancing
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#36335;&#24452;&#31354;&#38388;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#65288;PKF&#65289;&#30340;&#25193;&#23637;&#31639;&#27861;&#65292;&#21487;&#20197;&#21160;&#24577;&#36319;&#36394;&#25968;&#25454;&#21644;&#20808;&#21069;&#30693;&#35782;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#29992;&#36125;&#21494;&#26031;&#26041;&#27861;&#37327;&#21270;&#19981;&#21516;&#30340;&#19981;&#30830;&#23450;&#24615;&#26469;&#28304;&#12290;&#36890;&#36807;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#25968;&#20540;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;PKF&#20248;&#20110;&#20256;&#32479;KF&#26041;&#27861;&#65292;&#24182;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#29983;&#29289;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2402.04498</link><description>&lt;p&gt;
&#24102;&#26377;&#21160;&#24577;&#36807;&#31243;&#19981;&#30830;&#23450;&#24615;&#30340;&#36335;&#24452;&#31354;&#38388;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Pathspace Kalman Filters with Dynamic Process Uncertainty for Analyzing Time-course Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04498
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#36335;&#24452;&#31354;&#38388;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#65288;PKF&#65289;&#30340;&#25193;&#23637;&#31639;&#27861;&#65292;&#21487;&#20197;&#21160;&#24577;&#36319;&#36394;&#25968;&#25454;&#21644;&#20808;&#21069;&#30693;&#35782;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#29992;&#36125;&#21494;&#26031;&#26041;&#27861;&#37327;&#21270;&#19981;&#21516;&#30340;&#19981;&#30830;&#23450;&#24615;&#26469;&#28304;&#12290;&#36890;&#36807;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#25968;&#20540;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;PKF&#20248;&#20110;&#20256;&#32479;KF&#26041;&#27861;&#65292;&#24182;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#29983;&#29289;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#65288;KF&#65289;&#26159;&#19968;&#31181;&#26368;&#20248;&#32447;&#24615;&#29366;&#24577;&#39044;&#27979;&#31639;&#27861;&#65292;&#24191;&#27867;&#24212;&#29992;&#20110;&#24037;&#31243;&#23398;&#12289;&#32463;&#27982;&#23398;&#12289;&#26426;&#22120;&#20154;&#23398;&#21644;&#22826;&#31354;&#25506;&#32034;&#31561;&#39046;&#22495;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21457;&#23637;&#20102;KF&#30340;&#25193;&#23637;&#65292;&#31216;&#20026;&#36335;&#24452;&#31354;&#38388;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#65288;PKF&#65289;&#65292;&#23427;&#20801;&#35768;&#25105;&#20204;&#21160;&#24577;&#36319;&#36394;&#19982;&#24213;&#23618;&#25968;&#25454;&#21644;&#20808;&#21069;&#30693;&#35782;&#30456;&#20851;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#20351;&#29992;&#36125;&#21494;&#26031;&#26041;&#27861;&#37327;&#21270;&#19981;&#21516;&#30340;&#19981;&#30830;&#23450;&#24615;&#26469;&#28304;&#12290;&#35813;&#31639;&#27861;&#30340;&#19968;&#20010;&#24212;&#29992;&#26159;&#33258;&#21160;&#26816;&#27979;&#20869;&#37096;&#26426;&#21046;&#27169;&#22411;&#19982;&#25968;&#25454;&#22312;&#26102;&#38388;&#19978;&#21457;&#29983;&#21464;&#21270;&#30340;&#26102;&#38388;&#31383;&#21475;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#25551;&#36848;PKF&#31639;&#27861;&#25910;&#25947;&#24615;&#30340;&#23450;&#29702;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;PKF&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#20248;&#20110;&#20256;&#32479;KF&#26041;&#27861;&#65292;&#24179;&#22343;&#22343;&#26041;&#35823;&#24046;&#38477;&#20302;&#20102;&#25968;&#20010;&#25968;&#37327;&#32423;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#29983;&#29289;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Kalman Filter (KF) is an optimal linear state prediction algorithm, with applications in fields as diverse as engineering, economics, robotics, and space exploration. Here, we develop an extension of the KF, called a Pathspace Kalman Filter (PKF) which allows us to a) dynamically track the uncertainties associated with the underlying data and prior knowledge, and b) take as input an entire trajectory and an underlying mechanistic model, and using a Bayesian methodology quantify the different sources of uncertainty. An application of this algorithm is to automatically detect temporal windows where the internal mechanistic model deviates from the data in a time-dependent manner. First, we provide theorems characterizing the convergence of the PKF algorithm. Then, we numerically demonstrate that the PKF outperforms conventional KF methods on a synthetic dataset lowering the mean-squared-error by several orders of magnitude. Finally, we apply this method to biological time-course dataset i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#24341;&#20837;&#22312;&#32447;&#22343;&#21248;&#39118;&#38505;&#26102;&#38388;&#25277;&#26679;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#22312;&#32447;&#36817;&#20284;&#31639;&#27861;&#65292;&#19968;&#31181;&#24102;&#26377;&#23398;&#20064;&#22686;&#24378;&#65292;&#19968;&#31181;&#27809;&#26377;&#23398;&#20064;&#22686;&#24378;&#12290;&#36890;&#36807;&#31454;&#20105;&#27604;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#29702;&#35770;&#24615;&#33021;&#20445;&#35777;&#12290;&#25105;&#20204;&#36890;&#36807;&#21512;&#25104;&#23454;&#39564;&#21644;&#23454;&#38469;&#26696;&#20363;&#30740;&#31350;&#35780;&#20272;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.01995</link><description>&lt;p&gt;
&#22312;&#32447;&#22343;&#21248;&#39118;&#38505;&#26102;&#38388;&#25277;&#26679;&#65306;&#31532;&#19968;&#27425;&#36817;&#20284;&#31639;&#27861;&#65292;&#20855;&#26377;&#20840;&#32622;&#20449;&#21306;&#38388;&#38598;&#25104;&#30340;&#23398;&#20064;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Online Uniform Risk Times Sampling: First Approximation Algorithms, Learning Augmentation with Full Confidence Interval Integration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01995
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#24341;&#20837;&#22312;&#32447;&#22343;&#21248;&#39118;&#38505;&#26102;&#38388;&#25277;&#26679;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#22312;&#32447;&#36817;&#20284;&#31639;&#27861;&#65292;&#19968;&#31181;&#24102;&#26377;&#23398;&#20064;&#22686;&#24378;&#65292;&#19968;&#31181;&#27809;&#26377;&#23398;&#20064;&#22686;&#24378;&#12290;&#36890;&#36807;&#31454;&#20105;&#27604;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#29702;&#35770;&#24615;&#33021;&#20445;&#35777;&#12290;&#25105;&#20204;&#36890;&#36807;&#21512;&#25104;&#23454;&#39564;&#21644;&#23454;&#38469;&#26696;&#20363;&#30740;&#31350;&#35780;&#20272;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#23383;&#20581;&#24247;&#39046;&#22495;&#65292;&#23558;&#26377;&#38480;&#30340;&#27835;&#30103;&#39044;&#31639;&#20998;&#37197;&#21040;&#21487;&#29992;&#30340;&#39118;&#38505;&#26102;&#38388;&#19978;&#26159;&#20943;&#23569;&#29992;&#25143;&#30130;&#21171;&#30340;&#20851;&#38190;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26410;&#30693;&#30340;&#23454;&#38469;&#39118;&#38505;&#26102;&#38388;&#25968;&#37327;&#65292;&#36825;&#19968;&#31574;&#30053;&#36935;&#21040;&#20102;&#26174;&#33879;&#30340;&#38556;&#30861;&#65292;&#29616;&#26377;&#26041;&#27861;&#22312;&#29702;&#35770;&#20445;&#35777;&#26041;&#38754;&#36824;&#19981;&#36275;&#22815;&#12290;&#26412;&#25991;&#39318;&#27425;&#23558;&#22312;&#32447;&#22343;&#21248;&#39118;&#38505;&#26102;&#38388;&#25277;&#26679;&#38382;&#39064;&#24341;&#20837;&#36817;&#20284;&#31639;&#27861;&#26694;&#26550;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#22312;&#32447;&#36817;&#20284;&#31639;&#27861;&#65292;&#19968;&#31181;&#24102;&#26377;&#23398;&#20064;&#22686;&#24378;&#65292;&#19968;&#31181;&#27809;&#26377;&#23398;&#20064;&#22686;&#24378;&#65292;&#24182;&#20351;&#29992;&#31454;&#20105;&#27604;&#20998;&#26512;&#20026;&#23427;&#20204;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#29702;&#35770;&#24615;&#33021;&#20445;&#35777;&#12290;&#25105;&#20204;&#20351;&#29992;&#21512;&#25104;&#23454;&#39564;&#21644;HeartSteps&#31227;&#21160;&#24212;&#29992;&#30340;&#23454;&#38469;&#26696;&#20363;&#30740;&#31350;&#35780;&#20272;&#20102;&#25105;&#20204;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In digital health, the strategy of allocating a limited treatment budget across available risk times is crucial to reduce user fatigue. This strategy, however, encounters a significant obstacle due to the unknown actual number of risk times, a factor not adequately addressed by existing methods lacking theoretical guarantees. This paper introduces, for the first time, the online uniform risk times sampling problem within the approximation algorithm framework. We propose two online approximation algorithms for this problem, one with and one without learning augmentation, and provide rigorous theoretical performance guarantees for them using competitive ratio analysis. We assess the performance of our algorithms using both synthetic experiments and a real-world case study on HeartSteps mobile applications.
&lt;/p&gt;</description></item><item><title>XtalNet&#26159;&#39318;&#20010;&#29992;&#20110;&#20174;&#31881;&#26411;X&#23556;&#32447;&#34893;&#23556;&#23454;&#29616;&#31471;&#21040;&#31471;&#26230;&#20307;&#32467;&#26500;&#39044;&#27979;&#30340;&#31561;&#21464;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#22810;&#36798;400&#20010;&#21407;&#23376;&#30340;&#26377;&#26426;&#32467;&#26500;&#12290;</title><link>https://arxiv.org/abs/2401.03862</link><description>&lt;p&gt;
&#20174;&#31881;&#26411;X&#23556;&#32447;&#34893;&#23556;&#23454;&#29616;&#31471;&#21040;&#31471;&#30340;&#26230;&#20307;&#32467;&#26500;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
End-to-End Crystal Structure Prediction from Powder X-Ray Diffraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.03862
&lt;/p&gt;
&lt;p&gt;
XtalNet&#26159;&#39318;&#20010;&#29992;&#20110;&#20174;&#31881;&#26411;X&#23556;&#32447;&#34893;&#23556;&#23454;&#29616;&#31471;&#21040;&#31471;&#26230;&#20307;&#32467;&#26500;&#39044;&#27979;&#30340;&#31561;&#21464;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#22810;&#36798;400&#20010;&#21407;&#23376;&#30340;&#26377;&#26426;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26230;&#20307;&#32467;&#26500;&#39044;&#27979;&#65288;CSP&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#22823;&#22810;&#25968;&#26041;&#27861;&#38598;&#20013;&#22312;&#26080;&#26465;&#20214;&#22320;&#29983;&#25104;&#20855;&#26377;&#26377;&#38480;&#21407;&#23376;&#30340;&#26080;&#26426;&#26230;&#20307;&#12290; &#26412;&#30740;&#31350;&#24341;&#20837;&#20102;XtalNet&#65292;&#36825;&#26159;&#39318;&#20010;&#29992;&#20110;&#20174;&#31881;&#26411;X&#23556;&#32447;&#34893;&#23556;&#65288;PXRD&#65289;&#23454;&#29616;&#31471;&#21040;&#31471;CSP&#30340;&#31561;&#21464;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#12290; &#19982;&#20808;&#21069;&#20165;&#20381;&#36182;&#25104;&#20998;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;XtalNet&#21033;&#29992;PXRD&#20316;&#20026;&#39069;&#22806;&#26465;&#20214;&#65292;&#28040;&#38500;&#20102;&#27169;&#31946;&#24615;&#65292;&#24182;&#20351;&#24471;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#22810;&#36798;400&#20010;&#21407;&#23376;&#30340;&#21333;&#20803;&#32990;&#30340;&#22797;&#26434;&#26377;&#26426;&#32467;&#26500;&#12290; XtalNet&#21253;&#25324;&#20004;&#20010;&#27169;&#22359;&#65306;&#23545;&#27604;PXRD-&#26230;&#20307;&#39044;&#35757;&#32451;&#65288;CPCP&#65289;&#27169;&#22359;&#65292;&#23558;PXRD&#31354;&#38388;&#19982;&#26230;&#20307;&#32467;&#26500;&#31354;&#38388;&#23545;&#40784;&#65292;&#20197;&#21450;&#26465;&#20214;&#26230;&#20307;&#32467;&#26500;&#29983;&#25104;&#65288;CCSG&#65289;&#27169;&#22359;&#65292;&#26681;&#25454;PXRD&#27169;&#24335;&#29983;&#25104;&#20505;&#36873;&#26230;&#20307;&#32467;&#26500;&#12290; &#22312;&#20004;&#20010;MOF&#25968;&#25454;&#38598;&#65288;hMOF-100&#21644;hMOF-400&#65289;&#19978;&#30340;&#35780;&#20272;&#34920;&#26126;&#20102;XtalNet&#30340;&#26377;&#25928;&#24615;&#12290; XtalNet&#23454;&#29616;&#20102;9&#30340;&#21069;&#21313;&#21305;&#37197;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.03862v2 Announce Type: replace-cross  Abstract: Crystal structure prediction (CSP) has made significant progress, but most methods focus on unconditional generations of inorganic crystal with limited atoms in the unit cell. This study introduces XtalNet, the first equivariant deep generative model for end-to-end CSP from Powder X-ray Diffraction (PXRD). Unlike previous methods that rely solely on composition, XtalNet leverages PXRD as an additional condition, eliminating ambiguity and enabling the generation of complex organic structures with up to 400 atoms in the unit cell. XtalNet comprises two modules: a Contrastive PXRD-Crystal Pretraining (CPCP) module that aligns PXRD space with crystal structure space, and a Conditional Crystal Structure Generation (CCSG) module that generates candidate crystal structures conditioned on PXRD patterns. Evaluation on two MOF datasets (hMOF-100 and hMOF-400) demonstrates XtalNet's effectiveness. XtalNet achieves a top-10 Match Rate of 9
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#20687;&#32032;Splat&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#20174;&#22270;&#20687;&#23545;&#20013;&#37325;&#24314;&#19977;&#32500;&#36752;&#23556;&#22330;&#30340;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#20869;&#23384;&#39640;&#25928;&#28210;&#26579;&#12289;&#39640;&#36895;3D&#37325;&#24314;&#65292;&#26377;&#25928;&#20811;&#26381;&#20102;&#31232;&#30095;&#34920;&#31034;&#30340;&#23616;&#37096;&#26497;&#23567;&#20540;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#20248;&#36234;&#30340;&#23454;&#26102;&#28210;&#26579;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2312.12337</link><description>&lt;p&gt;
pixelSplat&#65306;&#26469;&#33258;&#22270;&#20687;&#23545;&#30340;&#19977;&#32500;&#39640;&#26031;&#26001;&#22359;&#29992;&#20110;&#21487;&#25193;&#23637;&#21487;&#27867;&#21270;&#19977;&#32500;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
pixelSplat: 3D Gaussian Splats from Image Pairs for Scalable Generalizable 3D Reconstruction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.12337
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#20687;&#32032;Splat&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#20174;&#22270;&#20687;&#23545;&#20013;&#37325;&#24314;&#19977;&#32500;&#36752;&#23556;&#22330;&#30340;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#20869;&#23384;&#39640;&#25928;&#28210;&#26579;&#12289;&#39640;&#36895;3D&#37325;&#24314;&#65292;&#26377;&#25928;&#20811;&#26381;&#20102;&#31232;&#30095;&#34920;&#31034;&#30340;&#23616;&#37096;&#26497;&#23567;&#20540;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#20248;&#36234;&#30340;&#23454;&#26102;&#28210;&#26579;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;pixelSplat&#65292;&#36825;&#26159;&#19968;&#20010;&#21069;&#21521;&#27169;&#22411;&#65292;&#36890;&#36807;&#22270;&#20687;&#23545;&#23398;&#20064;&#37325;&#24314;&#30001;&#19977;&#32500;&#39640;&#26031;&#22522;&#20803;&#21442;&#25968;&#21270;&#30340;&#19977;&#32500;&#36752;&#23556;&#22330;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20855;&#26377;&#23454;&#26102;&#21644;&#20869;&#23384;&#39640;&#25928;&#30340;&#28210;&#26579;&#65292;&#21487;&#23454;&#29616;&#21487;&#25193;&#23637;&#30340;&#35757;&#32451;&#65292;&#21516;&#26102;&#22312;&#25512;&#26029;&#26102;&#23454;&#29616;&#24555;&#36895;&#30340;&#19977;&#32500;&#37325;&#24314;&#12290;&#20026;&#20102;&#20811;&#26381;&#31232;&#30095;&#21644;&#23616;&#37096;&#25903;&#25345;&#34920;&#31034;&#22266;&#26377;&#30340;&#23616;&#37096;&#26497;&#23567;&#20540;&#38382;&#39064;&#65292;&#25105;&#20204;&#39044;&#27979;&#20102;&#19977;&#32500;&#19978;&#30340;&#23494;&#38598;&#27010;&#29575;&#20998;&#24067;&#65292;&#24182;&#20174;&#35813;&#27010;&#29575;&#20998;&#24067;&#20013;&#37319;&#26679;&#39640;&#26031;&#22343;&#20540;&#12290;&#36890;&#36807;&#37325;&#26032;&#21442;&#25968;&#21270;&#25216;&#24039;&#65292;&#25105;&#20204;&#20351;&#24471;&#36825;&#20010;&#37319;&#26679;&#25805;&#20316;&#21487;&#24494;&#20998;&#65292;&#20174;&#32780;&#33021;&#22815;&#36890;&#36807;&#39640;&#26031;&#26001;&#22359;&#34920;&#31034;&#21453;&#21521;&#20256;&#25773;&#26799;&#24230;&#12290;&#25105;&#20204;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;RealEstate10k&#21644;ACID&#25968;&#25454;&#38598;&#19978;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#35282;&#22522;&#32447;&#26032;&#35270;&#22270;&#21512;&#25104;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#30340;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#20809;&#22330;&#21464;&#25442;&#22120;&#65292;&#24182;&#22312;&#37325;&#24314;&#21487;&#35299;&#37322;&#21644;&#21487;&#32534;&#36753;&#30340;&#19977;&#32500;&#36752;&#23556;&#30340;&#21516;&#26102;&#21152;&#36895;&#28210;&#26579;2.5&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.12337v3 Announce Type: replace-cross  Abstract: We introduce pixelSplat, a feed-forward model that learns to reconstruct 3D radiance fields parameterized by 3D Gaussian primitives from pairs of images. Our model features real-time and memory-efficient rendering for scalable training as well as fast 3D reconstruction at inference time. To overcome local minima inherent to sparse and locally supported representations, we predict a dense probability distribution over 3D and sample Gaussian means from that probability distribution. We make this sampling operation differentiable via a reparameterization trick, allowing us to back-propagate gradients through the Gaussian splatting representation. We benchmark our method on wide-baseline novel view synthesis on the real-world RealEstate10k and ACID datasets, where we outperform state-of-the-art light field transformers and accelerate rendering by 2.5 orders of magnitude while reconstructing an interpretable and editable 3D radiance
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#20154;&#31867;&#35821;&#35328;&#27169;&#22411;&#30340;&#24314;&#31435;&#38656;&#35201;&#26356;&#22909;&#22320;&#25972;&#21512;&#20154;&#31867;&#32972;&#26223;&#65292;&#24182;&#38754;&#20020;&#30528;&#22914;&#20309;&#25429;&#25417;&#20154;&#31867;&#22240;&#32032;&#12289;&#22914;&#20309;&#34920;&#31034;&#20197;&#21450;&#22914;&#20309;&#24314;&#27169;&#30340;&#19968;&#31995;&#21015;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2312.07751</link><description>&lt;p&gt;
&#22823;&#22411;&#20154;&#31867;&#35821;&#35328;&#27169;&#22411;&#65306;&#38656;&#27714;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Large Human Language Models: A Need and the Challenges
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.07751
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#20154;&#31867;&#35821;&#35328;&#27169;&#22411;&#30340;&#24314;&#31435;&#38656;&#35201;&#26356;&#22909;&#22320;&#25972;&#21512;&#20154;&#31867;&#32972;&#26223;&#65292;&#24182;&#38754;&#20020;&#30528;&#22914;&#20309;&#25429;&#25417;&#20154;&#31867;&#22240;&#32032;&#12289;&#22914;&#20309;&#34920;&#31034;&#20197;&#21450;&#22914;&#20309;&#24314;&#27169;&#30340;&#19968;&#31995;&#21015;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#31867;&#20013;&#24515;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#30340;&#36827;&#23637;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#24847;&#35782;&#21040;&#23558;&#20154;&#31867;&#21644;&#31038;&#20250;&#22240;&#32032;&#32435;&#20837;&#21040;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#24050;&#32463;&#20005;&#37325;&#20381;&#36182;&#20110;LLM&#65292;&#20854;&#20013;&#22823;&#22810;&#25968;&#24182;&#27809;&#26377;&#23545;&#20316;&#32773;&#36827;&#34892;&#24314;&#27169;&#12290;&#20026;&#20102;&#26500;&#24314;&#33021;&#22815;&#30495;&#27491;&#29702;&#35299;&#20154;&#31867;&#35821;&#35328;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#65292;&#25105;&#20204;&#24517;&#39035;&#26356;&#22909;&#22320;&#23558;&#20154;&#31867;&#32972;&#26223;&#25972;&#21512;&#21040;LLM&#20013;&#12290;&#36825;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#35774;&#35745;&#32771;&#34385;&#21644;&#25361;&#25112;&#65292;&#28041;&#21450;&#21040;&#35201;&#25429;&#25417;&#21738;&#20123;&#20154;&#31867;&#22240;&#32032;&#12289;&#22914;&#20309;&#34920;&#31034;&#23427;&#20204;&#20197;&#21450;&#35201;&#37319;&#29992;&#20309;&#31181;&#24314;&#27169;&#31574;&#30053;&#31561;&#38382;&#39064;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20513;&#20174;&#24515;&#29702;&#23398;&#21644;&#34892;&#20026;&#31185;&#23398;&#30340;&#27010;&#24565;&#20986;&#21457;&#65292;&#25903;&#25345;&#19977;&#20010;&#31435;&#22330;&#26469;&#21019;&#24314;&#22823;&#22411;&#20154;&#31867;&#35821;&#35328;&#27169;&#22411;&#65288;LHLMs&#65289;&#65306;&#39318;&#20808;&#65292;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#24212;&#21253;&#25324;&#20154;&#31867;&#32972;&#26223;&#12290;&#20854;&#27425;&#65292;LHLMs&#24212;&#35813;&#24847;&#35782;&#21040;&#20154;&#19981;&#20165;&#20165;&#26159;&#20182;&#20204;&#25152;&#23646;&#30340;&#32676;&#20307;&#12290;&#31532;&#19977;&#65292;LHLMs&#24212;&#35813;&#33021;&#22815;&#32771;&#34385;&#21040;&#20154;&#31867;&#32972;&#26223;&#30340;&#21160;&#24577;&#21644;&#26102;&#38388;&#20381;&#36182;&#24615;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.07751v2 Announce Type: replace-cross  Abstract: As research in human-centered NLP advances, there is a growing recognition of the importance of incorporating human and social factors into NLP models. At the same time, our NLP systems have become heavily reliant on LLMs, most of which do not model authors. To build NLP systems that can truly understand human language, we must better integrate human contexts into LLMs. This brings to the fore a range of design considerations and challenges in terms of what human aspects to capture, how to represent them, and what modeling strategies to pursue. To address these, we advocate for three positions toward creating large human language models (LHLMs) using concepts from psychological and behavioral sciences: First, LM training should include the human context. Second, LHLMs should recognize that people are more than their group(s). Third, LHLMs should be able to account for the dynamic and temporally-dependent nature of the human con
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#22240;&#20854;&#36890;&#29992;&#35821;&#35328;&#29702;&#35299;&#30340;&#33021;&#21147;&#20197;&#21450;&#29983;&#25104;&#39640;&#36136;&#37327;&#25991;&#26412;&#25110;&#35745;&#31639;&#26426;&#20195;&#30721;&#30340;&#33021;&#21147;&#32780;&#22791;&#21463;&#20851;&#27880;&#65292;&#23545;&#25968;&#23398;&#23478;&#30340;&#28508;&#22312;&#24110;&#21161;&#21644;&#25913;&#21464;&#24037;&#20316;&#26041;&#24335;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;</title><link>https://arxiv.org/abs/2312.04556</link><description>&lt;p&gt;
&#25968;&#23398;&#23478;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for Mathematicians
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.04556
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#22240;&#20854;&#36890;&#29992;&#35821;&#35328;&#29702;&#35299;&#30340;&#33021;&#21147;&#20197;&#21450;&#29983;&#25104;&#39640;&#36136;&#37327;&#25991;&#26412;&#25110;&#35745;&#31639;&#26426;&#20195;&#30721;&#30340;&#33021;&#21147;&#32780;&#22791;&#21463;&#20851;&#27880;&#65292;&#23545;&#25968;&#23398;&#23478;&#30340;&#28508;&#22312;&#24110;&#21161;&#21644;&#25913;&#21464;&#24037;&#20316;&#26041;&#24335;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#22240;&#20854;&#36890;&#29992;&#35821;&#35328;&#29702;&#35299;&#30340;&#33021;&#21147;&#20197;&#21450;&#29983;&#25104;&#39640;&#36136;&#37327;&#25991;&#26412;&#25110;&#35745;&#31639;&#26426;&#20195;&#30721;&#30340;&#33021;&#21147;&#32780;&#22791;&#21463;&#20851;&#27880;&#12290;&#23545;&#35768;&#22810;&#32844;&#19994;&#26469;&#35828;&#65292;LLMs&#20195;&#34920;&#19968;&#31181;&#26080;&#20215;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#21152;&#24555;&#24037;&#20316;&#36895;&#24230;&#24182;&#25552;&#39640;&#24037;&#20316;&#36136;&#37327;&#12290;&#26412;&#25991;&#35752;&#35770;&#23427;&#20204;&#22312;&#24110;&#21161;&#19987;&#19994;&#25968;&#23398;&#23478;&#26041;&#38754;&#30340;&#20316;&#29992;&#31243;&#24230;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20379;&#20102;&#25152;&#26377;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#30340;Transformer&#27169;&#22411;&#30340;&#25968;&#23398;&#25551;&#36848;&#12290;&#22522;&#20110;&#26368;&#36817;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#26368;&#20339;&#23454;&#36341;&#21644;&#28508;&#22312;&#38382;&#39064;&#65292;&#24182;&#25253;&#21578;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#23398;&#33021;&#21147;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;LLMs&#25913;&#21464;&#25968;&#23398;&#23478;&#24037;&#20316;&#26041;&#24335;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.04556v2 Announce Type: replace-cross  Abstract: Large language models (LLMs) such as ChatGPT have received immense interest for their general-purpose language understanding and, in particular, their ability to generate high-quality text or computer code. For many professions, LLMs represent an invaluable tool that can speed up and improve the quality of work. In this note, we discuss to what extent they can aid professional mathematicians. We first provide a mathematical description of the transformer model used in all modern language models. Based on recent studies, we then outline best practices and potential issues and report on the mathematical abilities of language models. Finally, we shed light on the potential of LLMs to change how mathematicians work.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#28040;&#38500;&#29616;&#26377;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#20013;&#22320;&#29702;&#20559;&#35265;&#30340;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#20351;&#36825;&#20123;&#27169;&#22411;&#26356;&#21152;&#40065;&#26834;&#21040;&#22270;&#20687;&#30340;&#22320;&#29702;&#20301;&#32622;&#12290;</title><link>https://arxiv.org/abs/2312.02957</link><description>&lt;p&gt;
&#20026;&#25152;&#26377;&#20154;&#26500;&#24314;&#22320;&#29702;&#19981;&#21487;&#30693;&#27169;&#22411;&#30340;&#20998;&#31867;&#65306;&#28040;&#38500;&#22320;&#22495;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Classification for everyone : Building geography agnostic models for fairer recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.02957
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#28040;&#38500;&#29616;&#26377;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#20013;&#22320;&#29702;&#20559;&#35265;&#30340;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#20351;&#36825;&#20123;&#27169;&#22411;&#26356;&#21152;&#40065;&#26834;&#21040;&#22270;&#20687;&#30340;&#22320;&#29702;&#20301;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20943;&#36731;&#29616;&#26377;&#26368;&#20808;&#36827;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#20013;&#22266;&#26377;&#22320;&#29702;&#20559;&#35265;&#30340;&#19981;&#21516;&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#23450;&#37327;&#22320;&#23637;&#31034;&#20102;&#36825;&#31181;&#20559;&#35265;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978; - Dollar Street&#25968;&#25454;&#38598;&#21644;ImageNet&#19978;&#30340;&#23384;&#22312;&#65292;&#20351;&#29992;&#24102;&#26377;&#20301;&#32622;&#20449;&#24687;&#30340;&#22270;&#29255;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21487;&#20197;&#29992;&#26469;&#20943;&#23569;&#36825;&#31181;&#20559;&#35265;&#30340;&#19981;&#21516;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#19981;&#21516;&#25216;&#26415;&#23545;&#20110;&#20351;&#36825;&#20123;&#27169;&#22411;&#26356;&#21152;&#40065;&#26834;&#21040;&#22270;&#20687;&#22320;&#29702;&#20301;&#32622;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.02957v3 Announce Type: replace-cross  Abstract: In this paper, we analyze different methods to mitigate inherent geographical biases present in state of the art image classification models. We first quantitatively present this bias in two datasets - The Dollar Street Dataset and ImageNet, using images with location information. We then present different methods which can be employed to reduce this bias. Finally, we analyze the effectiveness of the different techniques on making these models more robust to geographical locations of the images.
&lt;/p&gt;</description></item><item><title>DiffiT&#26159;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;Vision Transformer&#21644;&#25193;&#25955;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#22312;&#22270;&#20687;&#29983;&#25104;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#24341;&#20837;&#32454;&#31890;&#24230;&#21435;&#22122;&#25511;&#21046;&#21644;&#26102;&#38388;&#20381;&#36182;&#30340;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#39640;&#20445;&#30495;&#22270;&#20687;&#30340;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2312.02139</link><description>&lt;p&gt;
DiffiT: &#29992;&#20110;&#22270;&#20687;&#29983;&#25104;&#30340;&#25193;&#25955;&#35270;&#35273;Transformer&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DiffiT: Diffusion Vision Transformers for Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.02139
&lt;/p&gt;
&lt;p&gt;
DiffiT&#26159;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;Vision Transformer&#21644;&#25193;&#25955;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#22312;&#22270;&#20687;&#29983;&#25104;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#24341;&#20837;&#32454;&#31890;&#24230;&#21435;&#22122;&#25511;&#21046;&#21644;&#26102;&#38388;&#20381;&#36182;&#30340;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#39640;&#20445;&#30495;&#22270;&#20687;&#30340;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#24378;&#22823;&#34920;&#29616;&#21147;&#21644;&#39640;&#26679;&#26412;&#36136;&#37327;&#30340;&#25193;&#25955;&#27169;&#22411;&#22312;&#29983;&#25104;&#39046;&#22495;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#24320;&#21019;&#24615;&#30340;&#35270;&#35273;Transformer&#65288;ViT&#65289;&#23637;&#29616;&#20102;&#24378;&#22823;&#30340;&#24314;&#27169;&#33021;&#21147;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#35782;&#21035;&#20219;&#21153;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;ViTs&#22312;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#23398;&#20064;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#27169;&#22411;&#65292;&#31216;&#20026;Diffusion Vision Transformers&#65288;DiffiT&#65289;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23545;&#21435;&#22122;&#36807;&#31243;&#36827;&#34892;&#32454;&#31890;&#24230;&#25511;&#21046;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#26102;&#38388;&#20381;&#36182;&#30340;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#65288;TMSA&#65289;&#26426;&#21046;&#12290;DiffiT&#22312;&#29983;&#25104;&#39640;&#20445;&#30495;&#22270;&#20687;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#65292;&#21442;&#25968;&#25928;&#29575;&#20063;&#26174;&#33879;&#25552;&#39640;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#22522;&#20110;&#28508;&#31354;&#38388;&#21644;&#22270;&#20687;&#31354;&#38388;&#30340;DiffiT&#27169;&#22411;&#65292;&#24182;&#22312;&#19981;&#21516;&#20998;&#36776;&#29575;&#30340;&#21508;&#31181;&#31867;&#21035;&#26465;&#20214;&#21644;&#38750;&#26465;&#20214;&#32508;&#21512;&#20219;&#21153;&#19978;&#23637;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28508;&#31354;&#38388;DiffiT&#27169;&#22411;&#36798;&#21040;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.02139v2 Announce Type: replace-cross  Abstract: Diffusion models with their powerful expressivity and high sample quality have achieved State-Of-The-Art (SOTA) performance in the generative domain. The pioneering Vision Transformer (ViT) has also demonstrated strong modeling capabilities and scalability, especially for recognition tasks. In this paper, we study the effectiveness of ViTs in diffusion-based generative learning and propose a new model denoted as Diffusion Vision Transformers (DiffiT). Specifically, we propose a methodology for finegrained control of the denoising process and introduce the Time-dependant Multihead Self Attention (TMSA) mechanism. DiffiT is surprisingly effective in generating high-fidelity images with significantly better parameter efficiency. We also propose latent and image space DiffiT models and show SOTA performance on a variety of class-conditional and unconditional synthesis tasks at different resolutions. The Latent DiffiT model achieves
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26089;&#26399;&#21644;&#26202;&#26399;&#38544;&#24615;&#20559;&#35265;&#30340;&#20108;&#20998;&#27861;&#65292;&#26412;&#25991;&#35777;&#26126;&#22312;&#35757;&#32451;&#21516;&#36136;&#31070;&#32463;&#32593;&#32476;&#26102;&#20250;&#20986;&#29616;&#20174;&#26680;&#39044;&#27979;&#22120;&#21040;&#26368;&#23567;&#33539;&#25968;/&#26368;&#22823;&#38388;&#38548;&#39044;&#27979;&#22120;&#30340;&#24613;&#21095;&#36716;&#21464;&#65292;&#20174;&#32780;&#24341;&#21457;&#27979;&#35797;&#20934;&#30830;&#24615;&#30340;&#26174;&#33879;&#21464;&#21270;&#12290;</title><link>https://arxiv.org/abs/2311.18817</link><description>&lt;p&gt;
&#26089;&#26399;&#21644;&#26202;&#26399;&#38544;&#24615;&#20559;&#35265;&#30340;&#20108;&#20998;&#27861;&#21487;&#20197;&#26126;&#26174;&#35825;&#23548;&#39046;&#24735;
&lt;/p&gt;
&lt;p&gt;
Dichotomy of Early and Late Phase Implicit Biases Can Provably Induce Grokking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.18817
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26089;&#26399;&#21644;&#26202;&#26399;&#38544;&#24615;&#20559;&#35265;&#30340;&#20108;&#20998;&#27861;&#65292;&#26412;&#25991;&#35777;&#26126;&#22312;&#35757;&#32451;&#21516;&#36136;&#31070;&#32463;&#32593;&#32476;&#26102;&#20250;&#20986;&#29616;&#20174;&#26680;&#39044;&#27979;&#22120;&#21040;&#26368;&#23567;&#33539;&#25968;/&#26368;&#22823;&#38388;&#38548;&#39044;&#27979;&#22120;&#30340;&#24613;&#21095;&#36716;&#21464;&#65292;&#20174;&#32780;&#24341;&#21457;&#27979;&#35797;&#20934;&#30830;&#24615;&#30340;&#26174;&#33879;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Power&#31561;&#20154;&#65288;2022&#65289;&#26368;&#36817;&#30340;&#30740;&#31350;&#31361;&#20986;&#20102;&#23398;&#20064;&#31639;&#26415;&#20219;&#21153;&#20013;&#20986;&#29616;&#30340;&#20196;&#20154;&#24778;&#35766;&#30340;&#8220;&#39046;&#24735;&#8221;&#29616;&#35937;&#65306;&#31070;&#32463;&#32593;&#32476;&#39318;&#20808;&#8220;&#35760;&#24518;&#8221;&#35757;&#32451;&#38598;&#65292;&#23548;&#33268;&#35757;&#32451;&#20934;&#30830;&#24615;&#23436;&#32654;&#65292;&#20294;&#27979;&#35797;&#20934;&#30830;&#24615;&#25509;&#36817;&#38543;&#26426;&#65292;&#28982;&#21518;&#22312;&#35757;&#32451;&#36275;&#22815;&#38271;&#26102;&#38388;&#21518;&#65292;&#31361;&#28982;&#36807;&#28193;&#33267;&#23436;&#32654;&#27979;&#35797;&#20934;&#30830;&#24615;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#39046;&#24735;&#29616;&#35937;&#22312;&#29702;&#35770;&#35774;&#32622;&#20013;&#30340;&#24773;&#20917;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#21487;&#20197;&#36890;&#36807;&#26089;&#26399;&#21644;&#26202;&#26399;&#38544;&#24615;&#20559;&#35265;&#30340;&#20108;&#20998;&#27861;&#26469;&#35825;&#23548;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#24403;&#20351;&#29992;&#22823;&#30340;&#21021;&#22987;&#21270;&#21644;&#23567;&#30340;&#26435;&#37325;&#34928;&#20943;&#35757;&#32451;&#21516;&#36136;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#26102;&#65292;&#25105;&#20204;&#35777;&#26126;&#35757;&#32451;&#36807;&#31243;&#22312;&#38271;&#26102;&#38388;&#20869;&#34987;&#22256;&#22312;&#23545;&#24212;&#20110;&#26680;&#39044;&#27979;&#22120;&#30340;&#35299;&#19978;&#65292;&#28982;&#21518;&#31361;&#28982;&#20986;&#29616;&#23545;&#20110;&#26368;&#23567;&#33539;&#25968;/&#26368;&#22823;&#38388;&#38548;&#39044;&#27979;&#22120;&#30340;&#38750;&#24120;&#26126;&#26174;&#30340;&#36807;&#28193;&#65292;&#23548;&#33268;&#27979;&#35797;&#20934;&#30830;&#24615;&#20986;&#29616;&#26174;&#33879;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.18817v2 Announce Type: replace-cross  Abstract: Recent work by Power et al. (2022) highlighted a surprising "grokking" phenomenon in learning arithmetic tasks: a neural net first "memorizes" the training set, resulting in perfect training accuracy but near-random test accuracy, and after training for sufficiently longer, it suddenly transitions to perfect test accuracy. This paper studies the grokking phenomenon in theoretical setups and shows that it can be induced by a dichotomy of early and late phase implicit biases. Specifically, when training homogeneous neural nets with large initialization and small weight decay on both classification and regression tasks, we prove that the training process gets trapped at a solution corresponding to a kernel predictor for a long time, and then a very sharp transition to min-norm/max-margin predictors occurs, leading to a dramatic change in test accuracy.
&lt;/p&gt;</description></item><item><title>XLB&#26159;&#19968;&#31181;&#22522;&#20110;Python&#30340;&#19981;&#21516;iable LBM&#24211;&#65292;&#20248;&#21270;&#20102;&#22312;&#19981;&#21516;&#30828;&#20214;&#24179;&#21488;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#25104;&#21151;&#22788;&#29702;&#20102;&#25968;&#21313;&#20159;&#20010;&#21333;&#20803;&#30340;&#27169;&#25311;&#12290;</title><link>https://arxiv.org/abs/2311.16080</link><description>&lt;p&gt;
XLB&#65306;&#19968;&#31181;&#22522;&#20110;Python&#30340;&#21487;&#24494;&#22823;&#35268;&#27169;&#24182;&#34892;&#26230;&#26684;&#29627;&#23572;&#20857;&#26364;&#24211;
&lt;/p&gt;
&lt;p&gt;
XLB: A differentiable massively parallel lattice Boltzmann library in Python
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.16080
&lt;/p&gt;
&lt;p&gt;
XLB&#26159;&#19968;&#31181;&#22522;&#20110;Python&#30340;&#19981;&#21516;iable LBM&#24211;&#65292;&#20248;&#21270;&#20102;&#22312;&#19981;&#21516;&#30828;&#20214;&#24179;&#21488;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#25104;&#21151;&#22788;&#29702;&#20102;&#25968;&#21313;&#20159;&#20010;&#21333;&#20803;&#30340;&#27169;&#25311;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LBM&#65288;&#26230;&#26684;&#29627;&#23572;&#20857;&#26364;&#26041;&#27861;&#65289;&#30001;&#20110;&#20854;&#22312;&#35745;&#31639;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#30340;&#31639;&#27861;&#28508;&#21147;&#32780;&#25104;&#20026;&#35299;&#20915;&#27969;&#20307;&#21160;&#21147;&#23398;&#38382;&#39064;&#30340;&#26480;&#20986;&#25216;&#26415;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;XLB&#24211;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;Python&#30340;&#21487;&#24494;LBM&#24211;&#65292;&#22522;&#20110;JAX&#24179;&#21488;&#12290; XLB&#30340;&#26550;&#26500;&#24314;&#31435;&#22312;&#30830;&#20445;&#21487;&#35775;&#38382;&#24615;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#35745;&#31639;&#24615;&#33021;&#30340;&#22522;&#30784;&#19978;&#65292;&#26377;&#25928;&#23454;&#29616;&#22312;CPU&#12289;TPU&#12289;&#22810;GPU&#21644;&#20998;&#24067;&#24335;&#22810;GPU&#25110;TPU&#31995;&#32479;&#19978;&#30340;&#32553;&#25918;&#12290;&#35813;&#24211;&#21487;&#20197;&#36731;&#26494;&#22320;&#25193;&#23637;&#20026;&#20855;&#26377;&#26032;&#39062;&#36793;&#30028;&#26465;&#20214;&#12289;&#30896;&#25758;&#27169;&#22411;&#25110;&#22810;&#29289;&#29702;&#27169;&#25311;&#33021;&#21147;&#12290; XLB&#30340;&#21487;&#24494;&#24615;&#21644;&#25968;&#25454;&#32467;&#26500;&#19982;&#24191;&#27867;&#30340;&#22522;&#20110;JAX&#30340;&#26426;&#22120;&#23398;&#20064;&#29983;&#24577;&#31995;&#32479;&#20860;&#23481;&#65292;&#20351;&#20854;&#33021;&#22815;&#35299;&#20915;&#22522;&#20110;&#29289;&#29702;&#30340;&#26426;&#22120;&#23398;&#20064;&#12289;&#20248;&#21270;&#21644;&#21453;&#38382;&#39064;&#12290; XLB&#24050;&#25104;&#21151;&#25193;&#23637;&#21040;&#22788;&#29702;&#25968;&#21313;&#20159;&#20010;&#21333;&#20803;&#30340;&#27169;&#25311;&#65292;&#23454;&#29616;&#27599;&#31186;&#21313;&#20159;&#32423;&#21035;&#30340;&#26230;&#26684;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.16080v3 Announce Type: replace-cross  Abstract: The lattice Boltzmann method (LBM) has emerged as a prominent technique for solving fluid dynamics problems due to its algorithmic potential for computational scalability. We introduce XLB library, a Python-based differentiable LBM library based on the JAX platform. The architecture of XLB is predicated upon ensuring accessibility, extensibility, and computational performance, enabling scaling effectively across CPU, TPU, multi-GPU, and distributed multi-GPU or TPU systems. The library can be readily augmented with novel boundary conditions, collision models, or multi-physics simulation capabilities. XLB's differentiability and data structure is compatible with the extensive JAX-based machine learning ecosystem, enabling it to address physics-based machine learning, optimization, and inverse problems. XLB has been successfully scaled to handle simulations with billions of cells, achieving giga-scale lattice updates per second. 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23398;&#20064;&#20195;&#29702;&#27169;&#22411;&#24182;&#20351;&#29992;&#36827;&#21270;&#25628;&#32034;&#36807;&#31243;&#65292;&#21457;&#29616;&#20102;&#21487;&#23450;&#21046;&#21040;&#19981;&#21516;&#20301;&#32622;&#30340;&#26377;&#25928;&#22303;&#22320;&#21033;&#29992;&#25919;&#31574;&#65292;&#20026;&#22303;&#22320;&#21033;&#29992;&#35268;&#21010;&#25552;&#20379;&#20102;&#19968;&#20010;&#28508;&#22312;&#26377;&#29992;&#30340;&#24037;&#20855;&#12290;</title><link>https://arxiv.org/abs/2311.12304</link><description>&lt;p&gt;
&#21457;&#29616;&#26377;&#25928;&#30340;&#22303;&#22320;&#21033;&#29992;&#35268;&#21010;&#25919;&#31574;
&lt;/p&gt;
&lt;p&gt;
Discovering Effective Policies for Land-Use Planning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.12304
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#20195;&#29702;&#27169;&#22411;&#24182;&#20351;&#29992;&#36827;&#21270;&#25628;&#32034;&#36807;&#31243;&#65292;&#21457;&#29616;&#20102;&#21487;&#23450;&#21046;&#21040;&#19981;&#21516;&#20301;&#32622;&#30340;&#26377;&#25928;&#22303;&#22320;&#21033;&#29992;&#25919;&#31574;&#65292;&#20026;&#22303;&#22320;&#21033;&#29992;&#35268;&#21010;&#25552;&#20379;&#20102;&#19968;&#20010;&#28508;&#22312;&#26377;&#29992;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22303;&#22320;&#34987;&#20998;&#37197;&#32473;&#19981;&#21516;&#30340;&#29992;&#36884;&#65292;&#22914;&#26862;&#26519;&#12289;&#22478;&#24066;&#21306;&#22495;&#21644;&#20892;&#19994;&#65292;&#23545;&#38470;&#22320;&#30899;&#24179;&#34913;&#21644;&#27668;&#20505;&#21464;&#21270;&#26377;&#37325;&#22823;&#24433;&#21709;&#12290;&#22522;&#20110;&#21487;&#29992;&#30340;&#22303;&#22320;&#21033;&#29992;&#21464;&#21270;&#30340;&#21382;&#21490;&#25968;&#25454;&#21644;&#30456;&#20851;&#30340;&#30899;&#25490;&#25918;&#21644;&#21560;&#25910;&#30340;&#27169;&#25311;&#65292;&#21487;&#20197;&#23398;&#20064;&#21040;&#19968;&#20010;&#20195;&#29702;&#27169;&#22411;&#65292;&#20174;&#32780;&#33021;&#22815;&#39640;&#25928;&#35780;&#20272;&#20915;&#31574;&#32773;&#21487;&#36873;&#25321;&#30340;&#19981;&#21516;&#36873;&#39033;&#12290;&#28982;&#21518;&#21487;&#20197;&#20351;&#29992;&#36827;&#21270;&#25628;&#32034;&#36807;&#31243;&#26469;&#21457;&#29616;&#29305;&#23450;&#20301;&#32622;&#30340;&#26377;&#25928;&#22303;&#22320;&#21033;&#29992;&#25919;&#31574;&#12290;&#35813;&#31995;&#32479;&#26500;&#24314;&#22312;Project Resilience&#24179;&#21488;&#19978;&#65292;&#24182;&#20351;&#29992;Land-Use Harmonization&#25968;&#25454;&#38598;LUH2&#21644;&#31807;&#35760;&#27169;&#22411;BLUE&#36827;&#34892;&#35780;&#20272;&#12290;&#23427;&#29983;&#25104;&#21487;&#23450;&#21046;&#21040;&#19981;&#21516;&#20301;&#32622;&#30340;&#30899;&#24433;&#21709;&#21644;&#22303;&#22320;&#21033;&#29992;&#21464;&#21270;&#37327;&#30340;&#24085;&#32047;&#25176;&#21069;&#27839;&#65292;&#20174;&#32780;&#20026;&#22303;&#22320;&#21033;&#29992;&#35268;&#21010;&#25552;&#20379;&#20102;&#19968;&#20010;&#28508;&#22312;&#26377;&#29992;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
How areas of land are allocated for different uses, such as forests, urban areas, and agriculture, has a large effect on the terrestrial carbon balance, and therefore climate change. Based on available historical data on land-use changes and a simulation of the associated carbon emissions and removals, a surrogate model can be learned that makes it possible to evaluate the different options available to decision-makers efficiently. An evolutionary search process can then be used to discover effective land-use policies for specific locations. Such a system was built on the Project Resilience platform and evaluated with the Land-Use Harmonization dataset LUH2 and the bookkeeping model BLUE. It generates Pareto fronts that trade off carbon impact and amount of land-use change customized to different locations, thus providing a potentially useful tool for land-use planning.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#25506;&#32034;&#22810;&#31181;&#26041;&#27861;&#65292;&#23558;&#38024;&#23545;&#20302;&#31209;MDPs&#30340;&#29616;&#26377;&#26041;&#27861;&#25193;&#23637;&#21040;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#65292;&#21516;&#26102;&#20445;&#25345;&#36817;&#20284;&#27491;&#30830;&#30340;&#23398;&#20064;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2311.03564</link><description>&lt;p&gt;
&#20855;&#26377;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#30340;&#20302;&#31209;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Low-Rank MDPs with Continuous Action Spaces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.03564
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#25506;&#32034;&#22810;&#31181;&#26041;&#27861;&#65292;&#23558;&#38024;&#23545;&#20302;&#31209;MDPs&#30340;&#29616;&#26377;&#26041;&#27861;&#25193;&#23637;&#21040;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#65292;&#21516;&#26102;&#20445;&#25345;&#36817;&#20284;&#27491;&#30830;&#30340;&#23398;&#20064;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#31209;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#26368;&#36817;&#22312;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#39046;&#22495;&#20013;&#23853;&#38706;&#22836;&#35282;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#25552;&#20379;&#32422;&#31561;&#20110;&#27491;&#30830;&#65288;PAC&#65289;&#30340;&#23398;&#20064;&#20445;&#35777;&#65292;&#21516;&#26102;&#36824;&#21487;&#20197;&#34701;&#21512;ML&#31639;&#27861;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#23545;&#20302;&#31209;MDPs&#30340;&#26041;&#27861;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#23427;&#20204;&#21482;&#32771;&#34385;&#26377;&#38480;&#30340;&#21160;&#20316;&#31354;&#38388;&#65292;&#24182;&#19988;&#22312;&#21160;&#20316;&#25968;&#37327;$|\mathcal{A}| \to \infty$&#26102;&#32473;&#20986;&#20102;&#31354;&#27934;&#30340;&#30028;&#38480;&#65292;&#36825;&#26497;&#22823;&#22320;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#36866;&#29992;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23558;&#36825;&#20123;&#26041;&#27861;&#25193;&#23637;&#21040;&#20855;&#26377;&#36830;&#32493;&#21160;&#20316;&#30340;&#35774;&#32622;&#30340;&#38382;&#39064;&#65292;&#24182;&#25506;&#35752;&#20102;&#22810;&#31181;&#20855;&#20307;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#36825;&#31181;&#25193;&#23637;&#12290;&#20316;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;FLAMBE&#31639;&#27861;&#65288;Agarwal&#31561;&#65292;2020&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#20302;&#31209;MDPs&#30340;PAC RL&#30340;&#22870;&#21169;&#26080;&#20851;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#65292;&#22312;&#19981;&#23545;&#31639;&#27861;&#36827;&#34892;&#20219;&#20309;&#20462;&#25913;&#30340;&#24773;&#20917;&#19979;&#65292;&#24403;&#20801;&#35768;&#21160;&#20316;&#20026;&#36830;&#32493;&#26102;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#31867;&#20284;&#30340;PAC&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.03564v2 Announce Type: replace-cross  Abstract: Low-Rank Markov Decision Processes (MDPs) have recently emerged as a promising framework within the domain of reinforcement learning (RL), as they allow for provably approximately correct (PAC) learning guarantees while also incorporating ML algorithms for representation learning. However, current methods for low-rank MDPs are limited in that they only consider finite action spaces, and give vacuous bounds as $|\mathcal{A}| \to \infty$, which greatly limits their applicability. In this work, we study the problem of extending such methods to settings with continuous actions, and explore multiple concrete approaches for performing this extension. As a case study, we consider the seminal FLAMBE algorithm (Agarwal et al., 2020), which is a reward-agnostic method for PAC RL with low-rank MDPs. We show that, without any modifications to the algorithm, we obtain a similar PAC bound when actions are allowed to be continuous. Specifical
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#31163;&#21644;&#23398;&#20064;&#28508;&#22312;&#28151;&#28102;&#22240;&#32032;&#65292;&#25552;&#39640;&#20102;&#29992;&#25143;&#20559;&#22909;&#24314;&#27169;&#30340;&#20934;&#30830;&#24615;</title><link>https://arxiv.org/abs/2311.03381</link><description>&lt;p&gt;
&#20998;&#31163;&#21644;&#23398;&#20064;&#28508;&#22312;&#28151;&#28102;&#22240;&#32032;&#20197;&#22686;&#24378;&#29992;&#25143;&#20559;&#22909;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Separating and Learning Latent Confounders to Enhancing User Preferences Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.03381
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#31163;&#21644;&#23398;&#20064;&#28508;&#22312;&#28151;&#28102;&#22240;&#32032;&#65292;&#25552;&#39640;&#20102;&#29992;&#25143;&#20559;&#22909;&#24314;&#27169;&#30340;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#27169;&#22411;&#26088;&#22312;&#20174;&#21382;&#21490;&#21453;&#39304;&#20013;&#25429;&#33719;&#29992;&#25143;&#20559;&#22909;&#65292;&#28982;&#21518;&#39044;&#27979;&#29992;&#25143;&#23545;&#20505;&#36873;&#39033;&#30446;&#30340;&#29305;&#23450;&#21453;&#39304;&#12290;&#28982;&#32780;&#65292;&#21508;&#31181;&#26410;&#27979;&#37327;&#30340;&#28151;&#28102;&#22240;&#32032;&#23548;&#33268;&#21382;&#21490;&#21453;&#39304;&#20013;&#30340;&#29992;&#25143;&#20559;&#22909;&#19982;&#30495;&#23454;&#20559;&#22909;&#20043;&#38388;&#23384;&#22312;&#20559;&#24046;&#65292;&#36827;&#32780;&#23548;&#33268;&#27169;&#22411;&#26410;&#36798;&#21040;&#39044;&#26399;&#30340;&#24615;&#33021;&#12290;&#29616;&#26377;&#30340;&#21435;&#20559;&#27169;&#22411;&#35201;&#20040;&#29305;&#23450;&#20110;&#35299;&#20915;&#29305;&#23450;&#20559;&#24046;&#65292;&#35201;&#20040;&#30452;&#25509;&#20174;&#29992;&#25143;&#21382;&#21490;&#21453;&#39304;&#20013;&#33719;&#21462;&#36741;&#21161;&#20449;&#24687;&#65292;&#36825;&#26080;&#27861;&#30830;&#23450;&#25152;&#23398;&#20559;&#22909;&#26159;&#30495;&#23454;&#29992;&#25143;&#20559;&#22909;&#36824;&#26159;&#28151;&#20837;&#26410;&#27979;&#37327;&#30340;&#28151;&#28102;&#22240;&#32032;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#20197;&#21069;&#30340;&#25512;&#33616;&#31995;&#32479;&#19981;&#20165;&#26159;&#26410;&#27979;&#37327;&#30340;&#28151;&#28102;&#22240;&#32032;&#30340;&#21518;&#32487;&#32773;&#65292;&#36824;&#20250;&#20316;&#20026;&#24433;&#21709;&#29992;&#25143;&#20559;&#22909;&#24314;&#27169;&#30340;&#26410;&#27979;&#37327;&#28151;&#28102;&#22240;&#32032;&#65292;&#36825;&#22312;&#20808;&#21069;&#30340;&#30740;&#31350;&#20013;&#19968;&#30452;&#34987;&#24573;&#35270;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;&#21069;&#36848;&#25512;&#33616;&#31995;&#32479;&#30340;&#24433;&#21709;&#32435;&#20837;&#32771;&#34385;&#65292;&#24182;&#23558;&#20854;&#35270;&#20026;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.03381v2 Announce Type: replace-cross  Abstract: Recommender models aim to capture user preferences from historical feedback and then predict user-specific feedback on candidate items. However, the presence of various unmeasured confounders causes deviations between the user preferences in the historical feedback and the true preferences, resulting in models not meeting their expected performance. Existing debias models either (1) specific to solving one particular bias or (2) directly obtain auxiliary information from user historical feedback, which cannot identify whether the learned preferences are true user preferences or mixed with unmeasured confounders. Moreover, we find that the former recommender system is not only a successor to unmeasured confounders but also acts as an unmeasured confounder affecting user preference modeling, which has always been neglected in previous studies. To this end, we incorporate the effect of the former recommender system and treat it as
&lt;/p&gt;</description></item><item><title>&#38543;&#26426;&#30446;&#26631;&#23884;&#20837;&#22312;&#36830;&#32493;&#36755;&#20986;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#34920;&#29616;&#20986;&#38750;&#29702;&#24615;&#26377;&#25928;&#24615;&#65292;&#23588;&#20854;&#22312;&#36739;&#22823;&#30340;&#25968;&#25454;&#38598;&#19978;&#24182;&#19988;&#23545;&#32597;&#35265;&#35789;&#25928;&#26524;&#26368;&#20026;&#26174;&#33879;&#12290;</title><link>https://arxiv.org/abs/2310.20620</link><description>&lt;p&gt;
&#38543;&#26426;&#30446;&#26631;&#23884;&#20837;&#23545;&#36830;&#32493;&#36755;&#20986;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#38750;&#29702;&#24615;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Unreasonable Effectiveness of Random Target Embeddings for Continuous-Output Neural Machine Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.20620
&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#30446;&#26631;&#23884;&#20837;&#22312;&#36830;&#32493;&#36755;&#20986;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#34920;&#29616;&#20986;&#38750;&#29702;&#24615;&#26377;&#25928;&#24615;&#65292;&#23588;&#20854;&#22312;&#36739;&#22823;&#30340;&#25968;&#25454;&#38598;&#19978;&#24182;&#19988;&#23545;&#32597;&#35265;&#35789;&#25928;&#26524;&#26368;&#20026;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#36755;&#20986;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;CoNMT&#65289;&#23558;&#31163;&#25955;&#30340;&#19979;&#19968;&#20010;&#35789;&#39044;&#27979;&#38382;&#39064;&#26367;&#25442;&#20026;&#23884;&#20837;&#39044;&#27979;&#12290;&#30446;&#26631;&#23884;&#20837;&#31354;&#38388;&#30340;&#35821;&#20041;&#32467;&#26500;&#65288;&#21363;&#30456;&#20851;&#35789;&#20043;&#38388;&#30340;&#25509;&#36817;&#31243;&#24230;&#65289;&#22312;&#30452;&#35273;&#19978;&#34987;&#35748;&#20026;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#25361;&#25112;&#20102;&#36825;&#19968;&#20551;&#35774;&#65292;&#24182;&#23637;&#31034;&#23436;&#20840;&#38543;&#26426;&#30340;&#36755;&#20986;&#23884;&#20837;&#21487;&#20197;&#32988;&#36807;&#36153;&#21147;&#39044;&#35757;&#32451;&#30340;&#23884;&#20837;&#65292;&#29305;&#21035;&#26159;&#22312;&#36739;&#22823;&#30340;&#25968;&#25454;&#38598;&#19978;&#12290;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#26174;&#31034;&#65292;&#36825;&#19968;&#20196;&#20154;&#24778;&#35766;&#30340;&#25928;&#26524;&#23545;&#20110;&#32597;&#35265;&#35789;&#26368;&#20026;&#26174;&#33879;&#65292;&#36825;&#26159;&#30001;&#20110;&#23427;&#20204;&#30340;&#23884;&#20837;&#30340;&#20960;&#20309;&#24615;&#36136;&#12290;&#25105;&#20204;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#28151;&#21512;&#31574;&#30053;&#65292;&#32467;&#21512;&#19981;&#21516;&#26631;&#35760;&#30340;&#38543;&#26426;&#21644;&#39044;&#35757;&#32451;&#23884;&#20837;&#65292;&#36827;&#19968;&#27493;&#38416;&#26126;&#20102;&#36825;&#19968;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.20620v2 Announce Type: replace  Abstract: Continuous-output neural machine translation (CoNMT) replaces the discrete next-word prediction problem with an embedding prediction. The semantic structure of the target embedding space (i.e., closeness of related words) is intuitively believed to be crucial. We challenge this assumption and show that completely random output embeddings can outperform laboriously pretrained ones, especially on larger datasets. Further investigation shows this surprising effect is strongest for rare words, due to the geometry of their embeddings. We shed further light on this finding by designing a mixed strategy that combines random and pre-trained embeddings for different tokens.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24773;&#24863;&#25925;&#20107;&#29983;&#25104;&#22120;AffGen&#65292;&#36890;&#36807;&#24341;&#20837;&#21160;&#24577;&#26463;&#35843;&#25972;&#21644;&#24773;&#24863;&#37325;&#26032;&#25490;&#21517;&#20004;&#31181;&#26032;&#25216;&#26415;&#65292;&#22312;&#21465;&#20107;&#20013;&#27880;&#20837;&#8220;&#26377;&#36259;&#30340;&#36716;&#25240;&#8221;&#65292;&#24182;&#22312;&#29983;&#25104;&#20805;&#28385;&#24773;&#24863;&#21644;&#26377;&#36259;&#30340;&#21465;&#20107;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#24322;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2310.15079</link><description>&lt;p&gt;
&#24773;&#24863;&#21644;&#21160;&#24577;&#26463;&#25628;&#32034;&#29992;&#20110;&#25925;&#20107;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Affective and Dynamic Beam Search for Story Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.15079
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24773;&#24863;&#25925;&#20107;&#29983;&#25104;&#22120;AffGen&#65292;&#36890;&#36807;&#24341;&#20837;&#21160;&#24577;&#26463;&#35843;&#25972;&#21644;&#24773;&#24863;&#37325;&#26032;&#25490;&#21517;&#20004;&#31181;&#26032;&#25216;&#26415;&#65292;&#22312;&#21465;&#20107;&#20013;&#27880;&#20837;&#8220;&#26377;&#36259;&#30340;&#36716;&#25240;&#8221;&#65292;&#24182;&#22312;&#29983;&#25104;&#20805;&#28385;&#24773;&#24863;&#21644;&#26377;&#36259;&#30340;&#21465;&#20107;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#24322;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25925;&#20107;&#20855;&#26377;&#36855;&#20154;&#30340;&#28508;&#21147;&#65292;&#20351;&#20854;&#25104;&#20026;&#19968;&#20010;&#24341;&#20154;&#20837;&#32988;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#23545;&#23089;&#20048;&#12289;&#25945;&#32946;&#12289;&#27835;&#30103;&#21644;&#35748;&#30693;&#30740;&#31350;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24773;&#24863;&#25925;&#20107;&#29983;&#25104;&#22120;&#65288;AffGen&#65289;&#65292;&#29992;&#20110;&#29983;&#25104;&#26377;&#36259;&#30340;&#21465;&#20107;&#12290;AffGen&#36890;&#36807;&#37319;&#29992;&#20004;&#31181;&#26032;&#25216;&#26415;&#8212;&#8212;&#21160;&#24577;&#26463;&#35843;&#25972;&#21644;&#24773;&#24863;&#37325;&#26032;&#25490;&#21517;&#65292;&#22312;&#21465;&#20107;&#20013;&#24341;&#20837;&#8220;&#26377;&#36259;&#30340;&#36716;&#25240;&#8221;&#12290;&#21160;&#24577;&#26463;&#35843;&#25972;&#20351;&#29992;&#19978;&#19979;&#25991;&#22810;&#33218;&#36172;&#21338;&#27169;&#22411;&#40723;&#21169;&#19981;&#22826;&#21487;&#39044;&#27979;&#12289;&#26356;&#20855;&#21560;&#24341;&#21147;&#30340;&#35789;&#35821;&#36873;&#25321;&#12290;&#24773;&#24863;&#37325;&#26032;&#25490;&#21517;&#22522;&#20110;&#24773;&#24863;&#24378;&#24230;&#23545;&#21477;&#23376;&#20505;&#36873;&#39033;&#36827;&#34892;&#20248;&#20808;&#25490;&#24207;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#21253;&#25324;&#33258;&#21160;&#35780;&#20272;&#21644;&#20154;&#31867;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;AffGen&#22312;&#29983;&#25104;&#20805;&#28385;&#24773;&#24863;&#24182;&#19988;&#26377;&#36259;&#30340;&#21465;&#20107;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#32447;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#28040;&#34701;&#30740;&#31350;&#21644;&#20998;&#26512;&#25552;&#20379;&#20102;&#20851;&#20110;AffGen&#20248;&#21183;&#21644;&#21155;&#21183;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.15079v1 Announce Type: cross  Abstract: Storytelling's captivating potential makes it a fascinating research area, with implications for entertainment, education, therapy, and cognitive studies. In this paper, we propose Affective Story Generator (AffGen) for generating interesting narratives. AffGen introduces "intriguing twists" in narratives by employing two novel techniques-Dynamic Beam Sizing and Affective Reranking. Dynamic Beam Sizing encourages less predictable, more captivating word choices using a contextual multi-arm bandit model. Affective Reranking prioritizes sentence candidates based on affect intensity. Our empirical evaluations, both automatic and human, demonstrate AffGen's superior performance over existing baselines in generating affectively charged and interesting narratives. Our ablation study and analysis provide insights into the strengths and weaknesses of AffGen.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#36755;&#20837;&#20013;&#28155;&#21152;&#20855;&#26377;&#35270;&#35273;&#22522;&#30784;&#30340;&#20449;&#24687;&#20316;&#20026;&#39044;&#38450;&#24615;&#28548;&#28165;&#65292;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#20943;&#23569;&#19981;&#20805;&#20998;&#24615;&#65292;&#24182;&#31616;&#21270;&#27169;&#22411;&#22238;&#31572;&#38382;&#39064;&#30340;&#26041;&#24335;&#12290;</title><link>https://arxiv.org/abs/2310.05861</link><description>&lt;p&gt;
&#37325;&#26032;&#34920;&#36848;&#12289;&#22686;&#24378;&#12289;&#25512;&#29702;&#65306;&#35270;&#35273;&#38382;&#39064;&#30340;&#35270;&#35273;&#22522;&#30784;&#19982;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Rephrase, Augment, Reason: Visual Grounding of Questions for Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.05861
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#36755;&#20837;&#20013;&#28155;&#21152;&#20855;&#26377;&#35270;&#35273;&#22522;&#30784;&#30340;&#20449;&#24687;&#20316;&#20026;&#39044;&#38450;&#24615;&#28548;&#28165;&#65292;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#20943;&#23569;&#19981;&#20805;&#20998;&#24615;&#65292;&#24182;&#31616;&#21270;&#27169;&#22411;&#22238;&#31572;&#38382;&#39064;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#30340;&#35270;&#35273;-&#35821;&#35328;&#20219;&#21153;&#21487;&#20197;&#22312;&#38646;&#33267;&#23569;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#22788;&#29702;&#65292;&#21363;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#35270;&#35273;&#32534;&#30721;&#22120;&#30456;&#32467;&#21512;&#65292;&#24418;&#25104;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#12290;&#23613;&#31649;&#36825;&#20855;&#26377;&#24040;&#22823;&#30340;&#20248;&#21183;&#65292;&#27604;&#22914;&#19981;&#38656;&#35201;&#35757;&#32451;&#25968;&#25454;&#25110;&#33258;&#23450;&#20041;&#26550;&#26500;&#65292;&#20294;&#22914;&#20309;&#23558;&#36755;&#20837;&#21576;&#29616;&#32473;LVLM&#20250;&#23545;&#38646;&#21442;&#32771;&#27169;&#22411;&#30340;&#24615;&#33021;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;&#29305;&#21035;&#26159;&#65292;&#20197;&#19981;&#20805;&#20998;&#26041;&#24335;&#34920;&#36798;&#30340;&#36755;&#20837;&#21487;&#33021;&#20250;&#23548;&#33268;&#38169;&#35823;&#31572;&#26696;&#65292;&#21407;&#22240;&#21253;&#25324;&#32570;&#22833;&#35270;&#35273;&#20449;&#24687;&#12289;&#22797;&#26434;&#30340;&#38544;&#21547;&#25512;&#29702;&#25110;&#35821;&#35328;&#27495;&#20041;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#22312;&#36755;&#20837;&#20013;&#28155;&#21152;&#20855;&#26377;&#35270;&#35273;&#22522;&#30784;&#30340;&#20449;&#24687;&#20316;&#20026;&#39044;&#38450;&#24615;&#28548;&#28165;&#65292;&#24212;&#35813;&#33021;&#22815;&#36890;&#36807;&#20943;&#23569;&#19981;&#20805;&#20998;&#24615;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#20363;&#22914;&#36890;&#36807;&#23450;&#20301;&#23545;&#35937;&#21644;&#28040;&#38500;&#24341;&#29992;&#27495;&#20041;&#12290;&#31867;&#20284;&#22320;&#65292;&#22312;VQA&#35774;&#32622;&#20013;&#65292;&#25913;&#21464;&#38382;&#39064;&#30340;&#26500;&#24605;&#26041;&#24335;&#21487;&#20197;&#20351;&#27169;&#22411;&#26356;&#23481;&#26131;&#22238;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.05861v2 Announce Type: replace-cross  Abstract: An increasing number of vision-language tasks can be handled with little to no training, i.e., in a zero and few-shot manner, by marrying large language models (LLMs) to vision encoders, resulting in large vision-language models (LVLMs). While this has huge upsides, such as not requiring training data or custom architectures, how an input is presented to an LVLM can have a major impact on zero-shot model performance. In particular, inputs phrased in an underspecified way can result in incorrect answers due to factors like missing visual information, complex implicit reasoning, or linguistic ambiguity. Therefore, adding visually-grounded information to the input as a preemptive clarification should improve model performance by reducing underspecification, e.g., by localizing objects and disambiguating references. Similarly, in the VQA setting, changing the way questions are framed can make them easier for models to answer. To th
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#39564;&#35777;&#65292;&#30740;&#31350;&#32773;&#20174;&#20005;&#35880;&#30340;&#35270;&#35282;&#25506;&#31350;&#20102; PINNs &#22312;&#25509;&#36817;&#26377;&#38480;&#26102;&#38388;&#29190;&#28856;&#30340; Burgers' PDE &#20013;&#30340;&#31283;&#23450;&#24615;&#21644;&#27867;&#21270;&#30028;&#38480;&#12290;</title><link>https://arxiv.org/abs/2310.05169</link><description>&lt;p&gt;
&#30740;&#31350; PINNs &#22312;&#25509;&#36817;&#26377;&#38480;&#26102;&#38388;&#29190;&#28856;&#30340; Burgers' PDE &#20013;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Investigating the Ability of PINNs To Solve Burgers' PDE Near Finite-Time BlowUp
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.05169
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#39564;&#35777;&#65292;&#30740;&#31350;&#32773;&#20174;&#20005;&#35880;&#30340;&#35270;&#35282;&#25506;&#31350;&#20102; PINNs &#22312;&#25509;&#36817;&#26377;&#38480;&#26102;&#38388;&#29190;&#28856;&#30340; Burgers' PDE &#20013;&#30340;&#31283;&#23450;&#24615;&#21644;&#27867;&#21270;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2310.05169v2 &#20844;&#21578;&#31867;&#22411;: &#26367;&#25442; &#25688;&#35201;: &#29289;&#29702;&#21551;&#21457;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#22312;&#35299;&#20915;&#22797;&#26434;&#30340; PDE &#26102;&#19981;&#26029;&#21462;&#24471;&#26032;&#30340;&#25104;&#23601;&#65292;&#21516;&#26102;&#22312;&#31934;&#24230;&#21644;&#25512;&#29702;&#36895;&#24230;&#20043;&#38388;&#25552;&#20379;&#20102;&#26377;&#21560;&#24341;&#21147;&#30340;&#26435;&#34913;&#12290;PDE &#30340;&#19968;&#20010;&#29305;&#21035;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26041;&#38754;&#26159;&#65292;&#23384;&#22312;&#19968;&#20123;&#31616;&#21333;&#30340; PDE&#65292;&#23427;&#20204;&#21487;&#20197;&#20174;&#20809;&#28369;&#30340;&#21021;&#22987;&#26465;&#20214;&#24320;&#22987;&#65292;&#22312;&#26377;&#38480;&#26102;&#38388;&#20869;&#28436;&#21464;&#20026;&#22855;&#24322;&#35299;&#12290;&#36817;&#26469;&#19968;&#20123;&#24341;&#20154;&#27880;&#30446;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;PINNs &#21487;&#33021;&#29978;&#33267;&#25797;&#38271;&#26816;&#27979;&#36825;&#31181;&#26377;&#38480;&#26102;&#38388;&#30340;&#29190;&#28856;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30528;&#25163;&#20174;&#20005;&#35880;&#30340;&#29702;&#35770;&#35270;&#35282;&#30740;&#31350; PINNs &#30340;&#31283;&#23450;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20026; Burgers' PDE &#25512;&#23548;&#20102; PINNs &#30340;&#27867;&#21270;&#30028;&#38480;&#65292;&#22312;&#20219;&#24847;&#32500;&#24230;&#19979;&#65292;&#22312;&#20801;&#35768;&#26377;&#38480;&#26102;&#38388;&#29190;&#28856;&#30340;&#26465;&#20214;&#19979;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#30028;&#38480;&#19982;&#31070;&#32463;&#32593;&#32476;&#25214;&#21040;&#30340;&#26367;&#20195;&#35299;&#19982;&#30495;&#23454;&#29190;&#28856;&#35299;&#20043;&#38388;&#30340; $\ell_2$ &#36317;&#31163;&#26174;&#30528;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.05169v2 Announce Type: replace  Abstract: Physics Informed Neural Networks (PINNs) have been achieving ever newer feats of solving complicated PDEs numerically while offering an attractive trade-off between accuracy and speed of inference. A particularly challenging aspect of PDEs is that there exist simple PDEs which can evolve into singular solutions in finite time starting from smooth initial conditions. In recent times some striking experiments have suggested that PINNs might be good at even detecting such finite-time blow-ups. In this work, we embark on a program to investigate this stability of PINNs from a rigorous theoretical viewpoint. Firstly, we derive generalization bounds for PINNs for Burgers' PDE, in arbitrary dimensions, under conditions that allow for a finite-time blow-up. Then we demonstrate via experiments that our bounds are significantly correlated to the $\ell_2$-distance of the neurally found surrogate from the true blow-up solution, when computed on 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23450;&#20041;&#20004;&#20010;&#38382;&#39064;&#65292;&#35774;&#35745;&#20102;&#29992;&#20110;&#27880;&#24847;&#21147;&#30697;&#38453;&#30340;&#20195;&#29702;&#24555;&#36895;&#31639;&#27861;&#21450;&#20854;&#22238;&#24402;&#38382;&#39064;&#27714;&#35299;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2308.14304</link><description>&lt;p&gt;
&#36890;&#36807;&#39044;&#26465;&#20214;&#22120;&#35299;&#20915;&#27880;&#24847;&#21147;&#26680;&#22238;&#24402;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Solving Attention Kernel Regression Problem via Pre-conditioner
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.14304
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23450;&#20041;&#20004;&#20010;&#38382;&#39064;&#65292;&#35774;&#35745;&#20102;&#29992;&#20110;&#27880;&#24847;&#21147;&#30697;&#38453;&#30340;&#20195;&#29702;&#24555;&#36895;&#31639;&#27861;&#21450;&#20854;&#22238;&#24402;&#38382;&#39064;&#27714;&#35299;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27880;&#24847;&#21147;&#26426;&#21046;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20851;&#38190;&#65292;&#27880;&#24847;&#21147;&#30697;&#38453;&#20316;&#20026;&#36825;&#31181;&#26041;&#26696;&#30340;&#31639;&#27861;&#21644;&#35745;&#31639;&#29942;&#39048;&#12290;&#26412;&#25991;&#36890;&#36807;&#23450;&#20041;&#20004;&#20010;&#38382;&#39064;&#65292;&#26088;&#22312;&#35774;&#35745;&#29992;&#20110;&#20195;&#29702;&#27880;&#24847;&#21147;&#30697;&#38453;&#30340;&#24555;&#36895;&#31639;&#27861;&#20197;&#21450;&#35299;&#20915;&#22238;&#24402;&#38382;&#39064;&#12290;&#39318;&#20808;&#32771;&#34385;&#30697;&#38453;$A\in \mathbb{R}^{n\times d}$&#30340;&#30697;&#38453;&#25351;&#25968;$A^\top A$&#20316;&#20026;&#19968;&#20010;&#20195;&#29702;&#65292;&#28982;&#21518;&#35774;&#35745;&#20004;&#31181;&#31867;&#22411;&#30340;&#22238;&#24402;&#38382;&#39064;&#30340;&#31639;&#27861;&#65306;&#23545;&#20110;&#20219;&#24847;&#27491;&#25972;&#25968;$j$&#65292;&#20998;&#21035;&#26159;$\min_{x\in \mathbb{R}^d}\|(A^\top A)^jx-b\|_2$&#21644;$\min_{x\in \mathbb{R}^d}\|A(A^\top A)^jx-b\|_2$&#12290;&#30740;&#31350;&#36825;&#20123;&#22238;&#24402;&#30340;&#31639;&#27861;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#22240;&#20026;&#30697;&#38453;&#25351;&#25968;&#21487;&#20197;&#36890;&#36807;&#36825;&#20123;&#36739;&#23567;&#30340;&#38382;&#39064;&#36880;&#39033;&#36924;&#36817;&#12290;&#31532;&#20108;&#20010;&#20195;&#29702;&#26159;&#23558;&#25351;&#25968;&#36880;&#39033;&#24212;&#29992;&#20110;Gram&#30697;&#38453;&#65292;&#35760;&#20026;$\exp(AA^\top)$&#65292;&#24182;&#35299;&#20915;&#22238;&#24402;&#38382;&#39064;$\min
&lt;/p&gt;
&lt;p&gt;
arXiv:2308.14304v2 Announce Type: replace  Abstract: The attention mechanism is the key to large language models, and the attention matrix serves as an algorithmic and computational bottleneck for such a scheme. In this paper, we define two problems, motivated by designing fast algorithms for proxy of attention matrix and solving regressions against them. Given an input matrix $A\in \mathbb{R}^{n\times d}$ with $n\gg d$ and a response vector $b$, we first consider the matrix exponential of the matrix $A^\top A$ as a proxy, and we in turn design algorithms for two types of regression problems: $\min_{x\in \mathbb{R}^d}\|(A^\top A)^jx-b\|_2$ and $\min_{x\in \mathbb{R}^d}\|A(A^\top A)^jx-b\|_2$ for any positive integer $j$. Studying algorithms for these regressions is essential, as matrix exponential can be approximated term-by-term via these smaller problems. The second proxy is applying exponential entrywise to the Gram matrix, denoted by $\exp(AA^\top)$ and solving the regression $\min
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21508;&#21521;&#24322;&#24615;&#39640;&#26031;&#36807;&#31243;&#30340;&#26041;&#27861;&#22635;&#34917;&#20132;&#36890;&#29366;&#24577;&#25968;&#25454;&#65292;&#25552;&#20379;&#26356;&#21487;&#38752;&#30340;&#20132;&#36890;&#29366;&#24577;&#20272;&#35745;&#65292;&#20855;&#26377;&#32479;&#35745;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;</title><link>https://arxiv.org/abs/2303.02311</link><description>&lt;p&gt;
&#37319;&#29992;&#21508;&#21521;&#24322;&#24615;&#39640;&#26031;&#36807;&#31243;&#20174;&#36710;&#36742;&#36712;&#36857;&#20013;&#20272;&#35745;&#20132;&#36890;&#29366;&#24577;
&lt;/p&gt;
&lt;p&gt;
Traffic State Estimation from Vehicle Trajectories with Anisotropic Gaussian Processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2303.02311
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21508;&#21521;&#24322;&#24615;&#39640;&#26031;&#36807;&#31243;&#30340;&#26041;&#27861;&#22635;&#34917;&#20132;&#36890;&#29366;&#24577;&#25968;&#25454;&#65292;&#25552;&#20379;&#26356;&#21487;&#38752;&#30340;&#20132;&#36890;&#29366;&#24577;&#20272;&#35745;&#65292;&#20855;&#26377;&#32479;&#35745;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30417;&#27979;&#36947;&#36335;&#20132;&#36890;&#29366;&#24577;&#23545;&#21508;&#31181;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#21253;&#25324;&#26053;&#34892;&#26102;&#38388;&#39044;&#27979;&#12289;&#20132;&#36890;&#25511;&#21046;&#21644;&#20132;&#36890;&#23433;&#20840;&#12290;&#28982;&#32780;&#65292;&#20256;&#24863;&#22120;&#30340;&#32570;&#20047;&#32463;&#24120;&#23548;&#33268;&#20132;&#36890;&#29366;&#24577;&#25968;&#25454;&#19981;&#23436;&#25972;&#65292;&#20351;&#24471;&#33719;&#21462;&#21487;&#38752;&#20449;&#24687;&#20197;&#36827;&#34892;&#20915;&#31574;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#26469;&#22635;&#34917;&#20132;&#36890;&#29366;&#24577;&#25968;&#25454;&#65292;&#20197;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26680;&#26059;&#36716;&#37325;&#26032;&#21442;&#25968;&#21270;&#26041;&#26696;&#65292;&#23558;&#26631;&#20934;&#21508;&#21521;&#21516;&#24615;GP&#26680;&#36716;&#25442;&#20026;&#21508;&#21521;&#24322;&#24615;&#26680;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#23545;&#20132;&#36890;&#27969;&#25968;&#25454;&#20013;&#30340;&#25317;&#22581;&#20256;&#25773;&#36827;&#34892;&#24314;&#27169;&#12290;&#27169;&#22411;&#21442;&#25968;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#26469;&#33258;&#31232;&#30095;&#25506;&#27979;&#36710;&#36742;&#25110;&#29615;&#24418;&#26816;&#27979;&#22120;&#30340;&#25968;&#25454;&#36827;&#34892;&#32479;&#35745;&#25512;&#26029;&#26469;&#20272;&#35745;&#12290;&#27492;&#22806;&#65292;&#26059;&#36716;&#30340;GP&#26041;&#27861;&#20026;&#22635;&#34917;&#21518;&#30340;&#20132;&#36890;&#29366;&#24577;&#25552;&#20379;&#20102;&#32479;&#35745;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#20351;&#20854;&#26356;&#21487;&#38752;&#12290;&#25105;&#20204;&#36824;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#25193;&#23637;&#21040;&#22810;&#36755;&#20986;GP&#65292;&#36825;&#20801;&#35768;si
&lt;/p&gt;
&lt;p&gt;
arXiv:2303.02311v2 Announce Type: replace  Abstract: Accurately monitoring road traffic state is crucial for various applications, including travel time prediction, traffic control, and traffic safety. However, the lack of sensors often results in incomplete traffic state data, making it challenging to obtain reliable information for decision-making. This paper proposes a novel method for imputing traffic state data using Gaussian processes (GP) to address this issue. We propose a kernel rotation re-parametrization scheme that transforms a standard isotropic GP kernel into an anisotropic kernel, which can better model the congestion propagation in traffic flow data. The model parameters can be estimated by statistical inference using data from sparse probe vehicles or loop detectors. Moreover, the rotated GP method provides statistical uncertainty quantification for the imputed traffic state, making it more reliable. We also extend our approach to a multi-output GP, which allows for si
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;OCSDF&#65292;&#36890;&#36807;&#23398;&#20064;&#24102;&#31526;&#21495;&#36317;&#31163;&#20989;&#25968;&#23454;&#29616;&#19968;&#31867;&#20998;&#31867;&#65292;&#25552;&#20379;&#40065;&#26834;&#24615;&#36793;&#30028;&#23545;$l2$&#23545;&#25239;&#25915;&#20987;&#65292;&#31454;&#20105;&#24615;&#33021;&#22312;&#34920;&#26684;&#21644;&#22270;&#20687;&#25968;&#25454;&#19978;&#65292;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#24378;&#65292;&#36830;&#25509;OCC&#19982;&#22270;&#20687;&#29983;&#25104;&#21644;&#38544;&#24335;&#31070;&#32463;&#34920;&#38754;&#21442;&#25968;&#12290;</title><link>https://arxiv.org/abs/2303.01978</link><description>&lt;p&gt;
&#20351;&#29992;1-Lipschitz&#31070;&#32463;&#32593;&#32476;&#30340;&#24102;&#31526;&#21495;&#36317;&#31163;&#20989;&#25968;&#30340;&#40065;&#26834;&#19968;&#31867;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Robust One-Class Classification with Signed Distance Function using 1-Lipschitz Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2303.01978
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;OCSDF&#65292;&#36890;&#36807;&#23398;&#20064;&#24102;&#31526;&#21495;&#36317;&#31163;&#20989;&#25968;&#23454;&#29616;&#19968;&#31867;&#20998;&#31867;&#65292;&#25552;&#20379;&#40065;&#26834;&#24615;&#36793;&#30028;&#23545;$l2$&#23545;&#25239;&#25915;&#20987;&#65292;&#31454;&#20105;&#24615;&#33021;&#22312;&#34920;&#26684;&#21644;&#22270;&#20687;&#25968;&#25454;&#19978;&#65292;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#24378;&#65292;&#36830;&#25509;OCC&#19982;&#22270;&#20687;&#29983;&#25104;&#21644;&#38544;&#24335;&#31070;&#32463;&#34920;&#38754;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;&#19968;&#31867;&#24102;&#31526;&#21495;&#36317;&#31163;&#20989;&#25968;&#65288;OCSDF&#65289;&#65292;&#36890;&#36807;&#21487;&#35777;&#26126;&#22320;&#23398;&#20064;&#36793;&#30028;&#25903;&#25345;&#30340;&#24102;&#31526;&#21495;&#36317;&#31163;&#20989;&#25968;&#65288;SDF&#65289;&#26469;&#25191;&#34892;&#19968;&#31867;&#20998;&#31867;&#65288;OCC&#65289;&#12290;&#25903;&#25345;&#21040;&#36793;&#30028;&#30340;&#36317;&#31163;&#21487;&#20197;&#35299;&#37322;&#20026;&#27491;&#24120;&#24230;&#20998;&#25968;&#65292;&#20854;&#20351;&#29992;1-Lipschitz&#31070;&#32463;&#32593;&#32476;&#30340;&#36817;&#20284;&#25552;&#20379;&#20102;&#23545;$l2$&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#36793;&#30028;&#65292;&#36825;&#26159;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;OCC&#31639;&#27861;&#20013;&#19968;&#20010;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#30340;&#24369;&#28857;&#12290;&#22240;&#27492;&#65292;OCSDF&#24102;&#26377;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#31216;&#20026;&#35748;&#35777;AUROC&#65292;&#21487;&#20197;&#20197;&#19982;&#20219;&#20309;&#20256;&#32479;AUROC&#30456;&#21516;&#30340;&#25104;&#26412;&#35745;&#31639;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#34920;&#26684;&#21644;&#22270;&#20687;&#25968;&#25454;&#19978;OCSDF&#19982;&#24182;&#21457;&#26041;&#27861;&#31454;&#20105;&#65292;&#21516;&#26102;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#35201;&#24378;&#24471;&#22810;&#65292;&#35828;&#26126;&#20102;&#20854;&#29702;&#35770;&#29305;&#24615;&#12290;&#26368;&#21518;&#65292;&#20316;&#20026;&#25506;&#32034;&#24615;&#30740;&#31350;&#23637;&#26395;&#65292;&#25105;&#20204;&#20174;&#29702;&#35770;&#21644;&#32463;&#39564;&#35282;&#24230;&#23637;&#31034;&#20102;OCSDF&#22914;&#20309;&#23558;OCC&#19982;&#22270;&#20687;&#29983;&#25104;&#21644;&#38544;&#24335;&#31070;&#32463;&#34920;&#38754;&#21442;&#25968;&#32852;&#31995;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2303.01978v2 Announce Type: replace  Abstract: We propose a new method, dubbed One Class Signed Distance Function (OCSDF), to perform One Class Classification (OCC) by provably learning the Signed Distance Function (SDF) to the boundary of the support of any distribution. The distance to the support can be interpreted as a normality score, and its approximation using 1-Lipschitz neural networks provides robustness bounds against $l2$ adversarial attacks, an under-explored weakness of deep learning-based OCC algorithms. As a result, OCSDF comes with a new metric, certified AUROC, that can be computed at the same cost as any classical AUROC. We show that OCSDF is competitive against concurrent methods on tabular and image data while being way more robust to adversarial attacks, illustrating its theoretical properties. Finally, as exploratory research perspectives, we theoretically and empirically show how OCSDF connects OCC with image generation and implicit neural surface parametr
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#24314;&#27169;&#21644;&#25511;&#21046;&#26694;&#26550;&#65292;&#21152;&#36895;&#27169;&#22411;&#35780;&#20272;&#12289;&#25552;&#20379;&#25104;&#26412;&#25928;&#30410;&#30340;&#26799;&#24230;&#65292;&#24182;&#22312;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#20013;&#20445;&#25345;&#33391;&#22909;&#30340;&#39044;&#27979;&#31934;&#24230;&#65292;&#21516;&#26102;&#36890;&#36807;&#24314;&#31569;&#20248;&#21270;&#27979;&#35797;&#26694;&#26550;&#65288;BOPTEST&#65289;&#23545;&#24314;&#27169;&#21644;&#25511;&#21046;&#24615;&#33021;&#36827;&#34892;&#24191;&#27867;&#35780;&#20272;</title><link>https://arxiv.org/abs/2301.13447</link><description>&lt;p&gt;
&#22312;&#24314;&#31569;&#20248;&#21270;&#27979;&#35797;&#26694;&#26550;&#65288;BOPTEST&#65289;&#20013;&#23545;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#31639;&#27861;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Model Predictive Control Algorithms in Building Optimization Testing Framework (BOPTEST)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2301.13447
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#24314;&#27169;&#21644;&#25511;&#21046;&#26694;&#26550;&#65292;&#21152;&#36895;&#27169;&#22411;&#35780;&#20272;&#12289;&#25552;&#20379;&#25104;&#26412;&#25928;&#30410;&#30340;&#26799;&#24230;&#65292;&#24182;&#22312;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#20013;&#20445;&#25345;&#33391;&#22909;&#30340;&#39044;&#27979;&#31934;&#24230;&#65292;&#21516;&#26102;&#36890;&#36807;&#24314;&#31569;&#20248;&#21270;&#27979;&#35797;&#26694;&#26550;&#65288;BOPTEST&#65289;&#23545;&#24314;&#27169;&#21644;&#25511;&#21046;&#24615;&#33021;&#36827;&#34892;&#24191;&#27867;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#24314;&#27169;&#21644;&#25511;&#21046;&#26694;&#26550;&#65292;&#29992;&#20110;&#22522;&#20110;&#29289;&#29702;&#30340;&#24314;&#31569;&#20223;&#30495;&#22120;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#65306;&#65288;a&#65289;&#35757;&#32451;&#21152;&#36895;&#27169;&#22411;&#35780;&#20272;&#12289;&#25552;&#20379;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#30340;&#26799;&#24230;&#12289;&#24182;&#20445;&#25345;&#22312;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;MPC&#65289;&#20013;&#22238;&#28335;&#35270;&#37326;&#30340;&#33391;&#22909;&#39044;&#27979;&#31934;&#24230;&#30340;&#21487;&#24494;&#26367;&#20195;&#27169;&#22411;&#65307;&#20197;&#21450;&#65288;b&#65289;&#21046;&#23450;&#21644;&#35299;&#20915;&#38750;&#32447;&#24615;&#24314;&#31569;&#31354;&#35843;&#26262;&#36890;&#31354;&#35843;MPC&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#24314;&#31569;&#20248;&#21270;&#27979;&#35797;&#26694;&#26550;&#65288;BOPTEST&#65289;&#20013;&#25552;&#20379;&#30340;&#21508;&#31181;&#27979;&#35797;&#26696;&#20363;&#65292;&#24191;&#27867;&#35780;&#20272;&#24314;&#27169;&#21644;&#25511;&#21046;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#19982;&#20854;&#20182;&#24314;&#27169;&#25216;&#26415;&#20860;&#23481;&#65292;&#24182;&#21487;&#20197;&#36890;&#36807;&#19981;&#21516;&#30340;&#25511;&#21046;&#20844;&#24335;&#36827;&#34892;&#23450;&#21046;&#65292;&#20351;&#20854;&#36866;&#24212;&#24182;&#20026;&#24403;&#21069;&#27491;&#22312;&#20026;BOPTEST&#24320;&#21457;&#30340;&#27979;&#35797;&#26696;&#20363;&#25552;&#20379;&#20102;&#21069;&#30651;&#35774;&#35745;&#20445;&#35777;&#12290;&#36825;&#31181;&#27169;&#22359;&#21270;&#24615;&#20026;&#22312;&#22823;&#22411;&#24314;&#31569;&#29289;&#20013;&#35774;&#35745;&#39044;&#27979;&#25511;&#21046;&#22120;&#25552;&#20379;&#20102;&#19968;&#26465;&#36947;&#36335;&#65292;&#30830;&#20445;
&lt;/p&gt;
&lt;p&gt;
arXiv:2301.13447v2 Announce Type: replace-cross  Abstract: We present a data-driven modeling and control framework for physics-based building emulators. Our approach consists of: (a) Offline training of differentiable surrogate models that accelerate model evaluations, provide cost-effective gradients, and maintain good predictive accuracy for the receding horizon in Model Predictive Control (MPC), and (b) Formulating and solving nonlinear building HVAC MPC problems. We extensively evaluate the modeling and control performance using multiple surrogate models and optimization frameworks across various test cases available in the Building Optimization Testing Framework (BOPTEST). Our framework is compatible with other modeling techniques and can be customized with different control formulations, making it adaptable and future-proof for test cases currently under development for BOPTEST. This modularity provides a path towards prototyping predictive controllers in large buildings, ensurin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20013;&#38388;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#23558;&#35757;&#32451;&#26500;&#24314;&#20026;&#24369;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#65292;&#20197;&#35299;&#20915;&#36890;&#36807;&#19981;&#23436;&#25972;&#27880;&#37322;&#25968;&#25454;&#26816;&#27979;&#24694;&#24615;&#20083;&#33146;&#30149;&#21464;&#30340;&#22256;&#22659;</title><link>https://arxiv.org/abs/2301.13418</link><description>&lt;p&gt;
BRAIxDet&#65306;&#23398;&#20064;&#20351;&#29992;&#19981;&#23436;&#25972;&#27880;&#37322;&#26816;&#27979;&#24694;&#24615;&#20083;&#33146;&#30149;&#21464;
&lt;/p&gt;
&lt;p&gt;
BRAIxDet: Learning to Detect Malignant Breast Lesion with Incomplete Annotations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2301.13418
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20013;&#38388;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#23558;&#35757;&#32451;&#26500;&#24314;&#20026;&#24369;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#65292;&#20197;&#35299;&#20915;&#36890;&#36807;&#19981;&#23436;&#25972;&#27880;&#37322;&#25968;&#25454;&#26816;&#27979;&#24694;&#24615;&#20083;&#33146;&#30149;&#21464;&#30340;&#22256;&#22659;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#31579;&#26597;&#20083;&#25151;X&#32447;&#29031;&#29255;&#20013;&#26816;&#27979;&#24694;&#24615;&#30149;&#21464;&#30340;&#26041;&#27861;&#36890;&#24120;&#26159;&#20351;&#29992;&#20855;&#26377;&#23436;&#20840;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;&#20854;&#20013;&#22270;&#20687;&#34987;&#26631;&#35760;&#20026;&#30284;&#30151;&#30149;&#21464;&#30340;&#23450;&#20301;&#21644;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#30495;&#23454;&#19990;&#30028;&#30340;&#31579;&#26597;&#20083;&#25151;X&#32447;&#29031;&#29255;&#25968;&#25454;&#38598;&#36890;&#24120;&#26377;&#19968;&#20010;&#37096;&#20998;&#26159;&#23436;&#20840;&#27880;&#37322;&#30340;&#65292;&#21478;&#19968;&#20010;&#37096;&#20998;&#21482;&#26377;&#20840;&#23616;&#20998;&#31867;&#30340;&#24369;&#27880;&#37322;&#65288;&#21363;&#27809;&#26377;&#30149;&#21464;&#23450;&#20301;&#65289;&#12290;&#37492;&#20110;&#36825;&#31867;&#25968;&#25454;&#38598;&#30340;&#24222;&#22823;&#35268;&#27169;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#24120;&#22312;&#24369;&#27880;&#37322;&#23376;&#38598;&#38754;&#20020;&#20004;&#38590;&#36873;&#25321;&#65306;&#35201;&#20040;&#19981;&#20351;&#29992;&#23427;&#65292;&#35201;&#20040;&#23436;&#20840;&#27880;&#37322;&#23427;&#12290;&#31532;&#19968;&#31181;&#36873;&#25321;&#20250;&#38477;&#20302;&#26816;&#27979;&#20934;&#30830;&#24615;&#65292;&#22240;&#20026;&#23427;&#27809;&#26377;&#20351;&#29992;&#25972;&#20010;&#25968;&#25454;&#38598;&#65292;&#32780;&#31532;&#20108;&#31181;&#36873;&#25321;&#21017;&#36807;&#20110;&#26114;&#36149;&#65292;&#22240;&#20026;&#27880;&#37322;&#38656;&#35201;&#19987;&#19994;&#25918;&#23556;&#31185;&#21307;&#24072;&#23436;&#25104;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36825;&#19968;&#22256;&#22659;&#30340;&#19968;&#20010;&#20013;&#38388;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#23558;&#35757;&#32451;&#26500;&#24314;&#20026;&#19968;&#20010;&#25105;&#20204;&#31216;&#20043;&#20026;&#24694;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2301.13418v3 Announce Type: replace-cross  Abstract: Methods to detect malignant lesions from screening mammograms are usually trained with fully annotated datasets, where images are labelled with the localisation and classification of cancerous lesions. However, real-world screening mammogram datasets commonly have a subset that is fully annotated and another subset that is weakly annotated with just the global classification (i.e., without lesion localisation). Given the large size of such datasets, researchers usually face a dilemma with the weakly annotated subset: to not use it or to fully annotate it. The first option will reduce detection accuracy because it does not use the whole dataset, and the second option is too expensive given that the annotation needs to be done by expert radiologists. In this paper, we propose a middle-ground solution for the dilemma, which is to formulate the training as a weakly- and semi-supervised learning problem that we refer to as malignant
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#39640;&#21534;&#21520;&#37327;&#30340;SRAM ReLU&#20248;&#21270;CD-CiM&#23439;&#65292;&#21487;&#20197;&#22312;&#19968;&#20010;CiM&#21608;&#26399;&#20869;&#23436;&#25104;&#20004;&#20010;8b&#21521;&#37327;&#30340;MAC&#21644;ReLU&#65292;&#20165;&#38656;&#36827;&#34892;&#19968;&#27425;A/D&#36716;&#25442;&#12290;</title><link>https://arxiv.org/abs/2212.04320</link><description>&lt;p&gt;
&#20351;&#29992;&#20840;&#24182;&#34892;&#27169;&#25311;&#21152;&#27861;&#22120;&#32593;&#32476;&#21644;&#21333;ADC&#25509;&#21475;&#30340;65nm 8b&#28608;&#27963;8b&#21152;&#26435;SRAM&#22522;&#20110;&#20805;&#30005;&#22495;&#35745;&#31639;&#20869;&#23384;&#23439;
&lt;/p&gt;
&lt;p&gt;
A 65nm 8b-Activation 8b-Weight SRAM-Based Charge-Domain Computing-in-Memory Macro Using A Fully-Parallel Analog Adder Network and A Single-ADC Interface
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2212.04320
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#39640;&#21534;&#21520;&#37327;&#30340;SRAM ReLU&#20248;&#21270;CD-CiM&#23439;&#65292;&#21487;&#20197;&#22312;&#19968;&#20010;CiM&#21608;&#26399;&#20869;&#23436;&#25104;&#20004;&#20010;8b&#21521;&#37327;&#30340;MAC&#21644;ReLU&#65292;&#20165;&#38656;&#36827;&#34892;&#19968;&#27425;A/D&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20911;&#35834;&#20381;&#26364;&#26550;&#26500;&#20013;&#25191;&#34892;&#25968;&#25454;&#23494;&#38598;&#22411;&#20219;&#21153;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23384;&#22312;&#20869;&#23384;&#22681;&#29942;&#39048;&#65292;&#21516;&#26102;&#23454;&#29616;&#39640;&#24615;&#33021;&#21644;&#21151;&#32791;&#25928;&#29575;&#12290;&#35745;&#31639;&#20869;&#23384;&#65288;CiM&#65289;&#26159;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#32531;&#35299;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20869;&#23384;&#20013;&#25903;&#25345;&#22806;&#35774;&#25509;&#21475;&#21644;&#25968;&#25454;&#36890;&#36335;&#65292;&#22312;&#20854;&#20013;&#36827;&#34892;&#24182;&#34892;&#21407;&#20301;&#20056;&#32047;&#21152;&#65288;MAC&#65289;&#25805;&#20316;&#12290;&#22522;&#20110;&#20805;&#30005;&#22495;&#30340;SRAM CiM&#65288;CD-CiM&#65289;&#26174;&#31034;&#20986;&#22686;&#24378;&#21151;&#32791;&#25928;&#29575;&#21644;&#35745;&#31639;&#31934;&#24230;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;SRAM&#30340;CD-CiM&#38754;&#20020;&#30528;&#35268;&#27169;&#21270;&#25361;&#25112;&#65292;&#20197;&#28385;&#36275;&#39640;&#24615;&#33021;&#22810;&#27604;&#29305;&#37327;&#21270;&#24212;&#29992;&#30340;&#21534;&#21520;&#37327;&#35201;&#27714;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#21534;&#21520;&#37327;&#30340;SRAM ReLU&#20248;&#21270;CD-CiM&#23439;&#12290;&#23427;&#33021;&#22815;&#22312;&#19968;&#20010;CiM&#21608;&#26399;&#20869;&#23436;&#25104;&#20004;&#20010;8b&#21521;&#37327;&#30340;MAC&#21644;ReLU&#65292;&#20165;&#38656;&#36827;&#34892;&#19968;&#27425;A/D&#36716;&#25442;&#12290;&#38500;&#20102;&#29992;&#20110;&#27169;&#25311;&#35745;&#31639;&#21644;A/D&#36716;&#25442;&#25509;&#21475;&#30340;&#38750;&#32447;&#24615;&#34917;&#20607;&#65292;&#36825;&#39033;&#24037;&#20316;&#23454;&#29616;&#20102;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2212.04320v2 Announce Type: replace-cross  Abstract: Performing data-intensive tasks in the von Neumann architecture is challenging to achieve both high performance and power efficiency due to the memory wall bottleneck. Computing-in-memory (CiM) is a promising mitigation approach by enabling parallel in-situ multiply-accumulate (MAC) operations within the memory with support from the peripheral interface and datapath. SRAM-based charge-domain CiM (CD-CiM) has shown its potential of enhanced power efficiency and computing accuracy. However, existing SRAM-based CD-CiM faces scaling challenges to meet the throughput requirement of high-performance multi-bit-quantization applications. This paper presents an SRAM-based high-throughput ReLU-optimized CD-CiM macro. It is capable of completing MAC and ReLU of two signed 8b vectors in one CiM cycle with only one A/D conversion. Along with non-linearity compensation for the analog computing and A/D conversion interfaces, this work achieve
&lt;/p&gt;</description></item><item><title>featMAP&#26041;&#27861;&#36890;&#36807;&#20445;&#30041;&#28304;&#29305;&#24449;&#65292;&#21033;&#29992;&#20999;&#32447;&#31354;&#38388;&#23884;&#20837;&#21644;&#21508;&#21521;&#24322;&#24615;&#25237;&#24433;&#23454;&#29616;&#20102;&#21487;&#35299;&#37322;&#30340;&#38477;&#32500;&#65292;&#33021;&#22815;&#26377;&#25928;&#35299;&#37322;&#25968;&#23383;&#20998;&#31867;&#12289;&#30446;&#26631;&#26816;&#27979;&#21644;&#23545;&#25239;&#26679;&#26412;&#30340;&#35823;&#20998;&#31867;&#12290;</title><link>https://arxiv.org/abs/2211.09321</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;&#20445;&#30041;&#27969;&#24418;&#36924;&#36817;&#19982;&#25237;&#24433;&#30340;&#38477;&#32500;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Interpretable Dimensionality Reduction by Feature Preserving Manifold Approximation and Projection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2211.09321
&lt;/p&gt;
&lt;p&gt;
featMAP&#26041;&#27861;&#36890;&#36807;&#20445;&#30041;&#28304;&#29305;&#24449;&#65292;&#21033;&#29992;&#20999;&#32447;&#31354;&#38388;&#23884;&#20837;&#21644;&#21508;&#21521;&#24322;&#24615;&#25237;&#24433;&#23454;&#29616;&#20102;&#21487;&#35299;&#37322;&#30340;&#38477;&#32500;&#65292;&#33021;&#22815;&#26377;&#25928;&#35299;&#37322;&#25968;&#23383;&#20998;&#31867;&#12289;&#30446;&#26631;&#26816;&#27979;&#21644;&#23545;&#25239;&#26679;&#26412;&#30340;&#35823;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#32447;&#24615;&#38477;&#32500;&#30001;&#20110;&#20302;&#32500;&#23884;&#20837;&#31354;&#38388;&#20013;&#32570;&#23569;&#28304;&#29305;&#24449;&#32780;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;featMAP&#30340;&#21487;&#35299;&#37322;&#26041;&#27861;&#65292;&#36890;&#36807;&#20999;&#32447;&#31354;&#38388;&#23884;&#20837;&#26469;&#20445;&#30041;&#28304;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#25552;&#35758;&#30340;&#26680;&#24515;&#26159;&#21033;&#29992;&#23616;&#37096;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;SVD&#65289;&#26469;&#36924;&#36817;&#20999;&#32447;&#31354;&#38388;&#65292;&#24182;&#36890;&#36807;&#20445;&#25345;&#23545;&#40784;&#23558;&#20854;&#23884;&#20837;&#21040;&#20302;&#32500;&#31354;&#38388;&#20013;&#12290;&#22522;&#20110;&#23884;&#20837;&#30340;&#20999;&#32447;&#31354;&#38388;&#65292;featMAP&#36890;&#36807;&#23616;&#37096;&#23637;&#31034;&#28304;&#29305;&#24449;&#21644;&#29305;&#24449;&#37325;&#35201;&#24615;&#23454;&#29616;&#20102;&#21487;&#35299;&#37322;&#24615;&#12290;&#27492;&#22806;&#65292;featMAP&#36890;&#36807;&#21508;&#21521;&#24322;&#24615;&#25237;&#24433;&#23558;&#25968;&#25454;&#28857;&#23884;&#20837;&#20197;&#20445;&#30041;&#23616;&#37096;&#30456;&#20284;&#24615;&#21644;&#21407;&#22987;&#23494;&#24230;&#12290;&#25105;&#20204;&#23558;featMAP&#24212;&#29992;&#20110;&#35299;&#37322;&#25968;&#23383;&#20998;&#31867;&#12289;&#30446;&#26631;&#26816;&#27979;&#21644;MNIST&#23545;&#25239;&#26679;&#26412;&#12290;FeatMAP&#21033;&#29992;&#28304;&#29305;&#24449;&#26126;&#30830;&#21306;&#20998;&#25968;&#23383;&#21644;&#23545;&#35937;&#65292;&#24182;&#35299;&#37322;&#23545;&#25239;&#26679;&#26412;&#30340;&#35823;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2211.09321v2 Announce Type: replace-cross  Abstract: Nonlinear dimensionality reduction lacks interpretability due to the absence of source features in low-dimensional embedding space. We propose an interpretable method featMAP to preserve source features by tangent space embedding. The core of our proposal is to utilize local singular value decomposition (SVD) to approximate the tangent space which is embedded to low-dimensional space by maintaining the alignment. Based on the embedding tangent space, featMAP enables the interpretability by locally demonstrating the source features and feature importance. Furthermore, featMAP embeds the data points by anisotropic projection to preserve the local similarity and original density. We apply featMAP to interpreting digit classification, object detection and MNIST adversarial examples. FeatMAP uses source features to explicitly distinguish the digits and objects and to explain the misclassification of adversarial examples. We also com
&lt;/p&gt;</description></item><item><title>GIDN&#27169;&#22411;&#36890;&#36807;&#21019;&#19990;&#27169;&#22359;&#22312;&#19981;&#21516;&#29305;&#24449;&#31354;&#38388;&#20013;&#27867;&#21270;&#20102;&#22270;&#25193;&#25955;&#65292;&#20197;&#36798;&#21040;&#26356;&#39640;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2210.01301</link><description>&lt;p&gt;
GIDN&#65306;&#29992;&#20110;&#39640;&#25928;&#38142;&#25509;&#39044;&#27979;&#30340;&#36731;&#37327;&#32423;&#22270;&#21019;&#19990;&#25193;&#25955;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
GIDN: A Lightweight Graph Inception Diffusion Network for High-efficient Link Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2210.01301
&lt;/p&gt;
&lt;p&gt;
GIDN&#27169;&#22411;&#36890;&#36807;&#21019;&#19990;&#27169;&#22359;&#22312;&#19981;&#21516;&#29305;&#24449;&#31354;&#38388;&#20013;&#27867;&#21270;&#20102;&#22270;&#25193;&#25955;&#65292;&#20197;&#36798;&#21040;&#26356;&#39640;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#21019;&#19990;&#25193;&#25955;&#32593;&#32476;&#65288;GIDN&#65289;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#22312;&#19981;&#21516;&#29305;&#24449;&#31354;&#38388;&#20013;&#27867;&#21270;&#20102;&#22270;&#25193;&#25955;&#65292;&#24182;&#20351;&#29992;&#21019;&#19990;&#27169;&#22359;&#26469;&#36991;&#20813;&#22797;&#26434;&#32593;&#32476;&#32467;&#26500;&#24341;&#36215;&#30340;&#22823;&#37327;&#35745;&#31639;&#12290;&#25105;&#20204;&#22312;Open Graph Benchmark&#65288;OGB&#65289;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;GIDN&#27169;&#22411;&#65292;&#22312;ogbl-collab&#25968;&#25454;&#38598;&#19978;&#27604;AGDN&#34920;&#29616;&#25552;&#39640;&#20102;11%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2210.01301v3 Announce Type: replace  Abstract: In this paper, we propose a Graph Inception Diffusion Networks(GIDN) model. This model generalizes graph diffusion in different feature spaces, and uses the inception module to avoid the large amount of computations caused by complex network structures. We evaluate GIDN model on Open Graph Benchmark(OGB) datasets, reached an 11% higher performance than AGDN on ogbl-collab dataset.
&lt;/p&gt;</description></item><item><title>&#25913;&#36827;&#24739;&#32773;&#25252;&#29702;&#38656;&#35201;&#32771;&#34385;&#22240;&#26524;&#20851;&#31995;&#65292;&#24314;&#31435;&#21644;&#39564;&#35777;&#30340;&#39044;&#27979;&#27169;&#22411;&#24517;&#39035;&#23545;&#27835;&#30103;&#20915;&#31574;&#30340;&#22240;&#26524;&#20851;&#31995;&#36827;&#34892;&#32771;&#34385;&#65292;&#20197;&#36991;&#20813;&#22312;&#20915;&#31574;&#26102;&#36896;&#25104;&#20260;&#23475;&#12290;</title><link>https://arxiv.org/abs/2209.07397</link><description>&lt;p&gt;
&#20174;&#31639;&#27861;&#21040;&#34892;&#21160;&#65306;&#25913;&#36827;&#24739;&#32773;&#25252;&#29702;&#38656;&#35201;&#22240;&#26524;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
From algorithms to action: improving patient care requires causality
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2209.07397
&lt;/p&gt;
&lt;p&gt;
&#25913;&#36827;&#24739;&#32773;&#25252;&#29702;&#38656;&#35201;&#32771;&#34385;&#22240;&#26524;&#20851;&#31995;&#65292;&#24314;&#31435;&#21644;&#39564;&#35777;&#30340;&#39044;&#27979;&#27169;&#22411;&#24517;&#39035;&#23545;&#27835;&#30103;&#20915;&#31574;&#30340;&#22240;&#26524;&#20851;&#31995;&#36827;&#34892;&#32771;&#34385;&#65292;&#20197;&#36991;&#20813;&#22312;&#20915;&#31574;&#26102;&#36896;&#25104;&#20260;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30284;&#30151;&#30740;&#31350;&#20013;&#65292;&#24314;&#31435;&#21644;&#39564;&#35777;&#39044;&#27979;&#32467;&#26524;&#30340;&#20852;&#36259;&#24456;&#22823;&#65292;&#20197;&#25903;&#25345;&#27835;&#30103;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22823;&#22810;&#25968;&#32467;&#26524;&#39044;&#27979;&#27169;&#22411;&#26159;&#22312;&#19981;&#32771;&#34385;&#27835;&#30103;&#20915;&#31574;&#30340;&#22240;&#26524;&#20851;&#31995;&#30340;&#24773;&#20917;&#19979;&#24320;&#21457;&#21644;&#39564;&#35777;&#30340;&#65292;&#35768;&#22810;&#24050;&#21457;&#34920;&#30340;&#32467;&#26524;&#39044;&#27979;&#27169;&#22411;&#22312;&#29992;&#20110;&#20915;&#31574;&#26102;&#21487;&#33021;&#20250;&#36896;&#25104;&#20260;&#23475;&#65292;&#23613;&#31649;&#22312;&#39564;&#35777;&#30740;&#31350;&#20013;&#34987;&#21457;&#29616;&#20934;&#30830;&#12290;&#12298;&#32654;&#22269;&#32852;&#21512;&#30284;&#30151;&#22996;&#21592;&#20250;&#39118;&#38505;&#27169;&#22411;&#35748;&#21487;&#26680;&#26597;&#21333;&#12299;&#23545;&#39044;&#27979;&#27169;&#22411;&#30340;&#39564;&#35777;&#25351;&#21335;&#21644;&#39118;&#38505;&#27169;&#22411;&#35748;&#21487;&#26680;&#26597;&#34920;&#26080;&#27861;&#38450;&#27490;&#22312;&#24320;&#21457;&#21644;&#39564;&#35777;&#36807;&#31243;&#20013;&#20934;&#30830;&#20294;&#22312;&#29992;&#20110;&#20915;&#31574;&#26102;&#26377;&#23475;&#30340;&#39044;&#27979;&#27169;&#22411;&#30340;&#20986;&#29616;&#12290;&#25105;&#20204;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#20250;&#20986;&#29616;&#36825;&#31181;&#24773;&#20917;&#20197;&#21450;&#22914;&#20309;&#26500;&#24314;&#21644;&#39564;&#35777;&#23545;&#20915;&#31574;&#26377;&#29992;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2209.07397v2 Announce Type: replace  Abstract: In cancer research there is much interest in building and validating outcome predicting outcomes to support treatment decisions. However, because most outcome prediction models are developed and validated without regard to the causal aspects of treatment decision making, many published outcome prediction models may cause harm when used for decision making, despite being found accurate in validation studies. Guidelines on prediction model validation and the checklist for risk model endorsement by the American Joint Committee on Cancer do not protect against prediction models that are accurate during development and validation but harmful when used for decision making. We explain why this is the case and how to build and validate models that are useful for decision making.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#20998;&#24067;&#28418;&#31227;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;&#29992;&#26102;&#38388;&#26465;&#20214;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;TCVAE&#65289;&#26469;&#27169;&#25311;&#21160;&#24577;&#20998;&#24067;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#21033;&#29992;&#28508;&#22312;&#21464;&#37327;&#36827;&#34892;&#39044;&#27979;</title><link>https://arxiv.org/abs/2209.00654</link><description>&lt;p&gt;
&#22522;&#20110;&#26102;&#38388;&#26465;&#20214;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#20998;&#24067;&#28418;&#31227;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Distributional Drift Adaptation with Temporal Conditional Variational Autoencoder for Multivariate Time Series Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2209.00654
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#20998;&#24067;&#28418;&#31227;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;&#29992;&#26102;&#38388;&#26465;&#20214;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;TCVAE&#65289;&#26469;&#27169;&#25311;&#21160;&#24577;&#20998;&#24067;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#21033;&#29992;&#28508;&#22312;&#21464;&#37327;&#36827;&#34892;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#29616;&#23454;&#19990;&#30028;&#20013;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#65288;MTS&#65289;&#30340;&#38750;&#24179;&#31283;&#24615;&#65292;&#38543;&#30528;&#26102;&#38388;&#25512;&#31227;&#20854;&#20998;&#24067;&#20250;&#21457;&#29983;&#21464;&#21270;&#65292;&#36825;&#34987;&#31216;&#20026;&#20998;&#24067;&#28418;&#31227;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;MTS&#39044;&#27979;&#27169;&#22411;&#22312;&#22788;&#29702;&#20998;&#24067;&#28418;&#31227;&#26102;&#36973;&#21463;&#37325;&#21019;&#65292;&#24182;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#32780;&#38477;&#20302;&#39044;&#27979;&#24615;&#33021;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#36866;&#24212;&#26368;&#26032;&#21040;&#36798;&#30340;&#25968;&#25454;&#25110;&#26681;&#25454;&#26410;&#26469;&#25968;&#25454;&#25512;&#23548;&#30340;&#20803;&#30693;&#35782;&#33258;&#25105;&#32416;&#27491;&#26469;&#35299;&#20915;&#20998;&#24067;&#28418;&#31227;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#22312;MTS&#39044;&#27979;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#24456;&#23569;&#20174;&#20998;&#24067;&#35282;&#24230;&#25429;&#25417;&#20869;&#22312;&#30340;&#20998;&#24067;&#21464;&#21270;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65306;&#26102;&#38388;&#26465;&#20214;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;TCVAE&#65289;&#65292;&#29992;&#20110;&#22312;MTS&#20013;&#27169;&#25311;&#21382;&#21490;&#35266;&#23519;&#20540;&#21644;&#26410;&#26469;&#25968;&#25454;&#20043;&#38388;&#30340;&#21160;&#24577;&#20998;&#24067;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#23558;&#36825;&#20123;&#20381;&#36182;&#20851;&#31995;&#25512;&#26029;&#20026;&#19968;&#20010;&#26102;&#38388;&#26465;&#20214;&#20998;&#24067;&#20197;&#21033;&#29992;&#28508;&#22312;&#21464;&#37327;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#26102;&#38388;Hawkes&#27880;&#24847;&#21147;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
arXiv:2209.00654v4 Announce Type: replace  Abstract: Due to the non-stationary nature, the distribution of real-world multivariate time series (MTS) changes over time, which is known as distribution drift. Most existing MTS forecasting models greatly suffer from distribution drift and degrade the forecasting performance over time. Existing methods address distribution drift via adapting to the latest arrived data or self-correcting per the meta knowledge derived from future data. Despite their great success in MTS forecasting, these methods hardly capture the intrinsic distribution changes, especially from a distributional perspective. Accordingly, we propose a novel framework temporal conditional variational autoencoder (TCVAE) to model the dynamic distributional dependencies over time between historical observations and future data in MTSs and infer the dependencies as a temporal conditional distribution to leverage latent variables. Specifically, a novel temporal Hawkes attention me
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#33021;&#22815;&#32467;&#21512;&#29615;&#22659;&#20960;&#20309;&#30693;&#35782;&#21644;&#20154;&#31867;&#36712;&#36857;&#35266;&#23519;&#65292;&#23398;&#20064;&#20154;&#21592;&#21160;&#24577;&#65292;&#23454;&#29616;&#36328;&#29615;&#22659;&#20154;&#27969;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2208.10851</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#22320;&#26495;&#22330;&#65306;&#36328;&#29615;&#22659;&#20256;&#36755;&#20154;&#27969;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Bayesian Floor Field: Transferring people flow predictions across environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2208.10851
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#33021;&#22815;&#32467;&#21512;&#29615;&#22659;&#20960;&#20309;&#30693;&#35782;&#21644;&#20154;&#31867;&#36712;&#36857;&#35266;&#23519;&#65292;&#23398;&#20064;&#20154;&#21592;&#21160;&#24577;&#65292;&#23454;&#29616;&#36328;&#29615;&#22659;&#20154;&#27969;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26144;&#23556;&#20154;&#21592;&#21160;&#24577;&#23545;&#26426;&#22120;&#20154;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#20351;&#23427;&#20204;&#33021;&#22815;&#22312;&#20154;&#31867;&#23621;&#20303;&#30340;&#29615;&#22659;&#20013;&#20849;&#23384;&#12290;&#28982;&#32780;&#65292;&#23398;&#20064;&#20154;&#21592;&#21160;&#24577;&#27169;&#22411;&#26159;&#19968;&#20010;&#32791;&#26102;&#30340;&#36807;&#31243;&#65292;&#38656;&#35201;&#35266;&#23519;&#22823;&#37327;&#20154;&#21592;&#22312;&#29615;&#22659;&#20013;&#31227;&#21160;&#12290;&#27492;&#22806;&#65292;&#29992;&#20110;&#26144;&#23556;&#21160;&#24577;&#30340;&#26041;&#27861;&#26080;&#27861;&#36328;&#29615;&#22659;&#20256;&#36755;&#23398;&#20064;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#24314;&#31569;&#20960;&#20309;&#24418;&#29366;&#23545;&#20154;&#21592;&#31227;&#21160;&#30340;&#24433;&#21709;&#21487;&#20197;&#29992;&#26469;&#39044;&#27979;&#20182;&#20204;&#30340;&#21160;&#24577;&#27169;&#24335;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#24320;&#22987;&#20174;&#21344;&#29992;&#23398;&#20064;&#21160;&#24577;&#22320;&#22270;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#22522;&#20110;&#36712;&#36857;&#21644;&#22522;&#20110;&#20960;&#20309;&#30340;&#26041;&#27861;&#23578;&#26410;&#32467;&#21512;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36125;&#21494;&#26031;&#26041;&#27861;&#26469;&#23398;&#20064;&#21487;&#20197;&#32467;&#21512;&#29615;&#22659;&#20960;&#20309;&#30693;&#35782;&#21644;&#26469;&#33258;&#20154;&#31867;&#36712;&#36857;&#30340;&#35266;&#23519;&#30340;&#20154;&#21592;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2208.10851v2 Announce Type: replace-cross  Abstract: Mapping people dynamics is a crucial skill for robots, because it enables them to coexist in human-inhabited environments. However, learning a model of people dynamics is a time consuming process which requires observation of large amount of people moving in an environment. Moreover, approaches for mapping dynamics are unable to transfer the learned models across environments: each model is only able to describe the dynamics of the environment it has been built in. However, the impact of architectural geometry on people's movement can be used to anticipate their patterns of dynamics, and recent work has looked into learning maps of dynamics from occupancy. So far however, approaches based on trajectories and those based on geometry have not been combined. In this work we propose a novel Bayesian approach to learn people dynamics able to combine knowledge about the environment geometry with observations from human trajectories. 
&lt;/p&gt;</description></item><item><title>&#24212;&#29992;&#24191;&#20041;&#20998;&#27573;&#32447;&#24615;&#33707;&#23572;&#26031;&#29702;&#35770;&#23450;&#20041;&#21644;&#30740;&#31350;&#20102;ReLU&#31070;&#32463;&#32593;&#32476;&#20989;&#25968;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#25299;&#25169;&#22797;&#26434;&#24230;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20415;&#20110;&#35745;&#31639;&#30340;&#32039;&#20945;&#27169;&#22411;&#65292;&#24182;&#35777;&#26126;&#23616;&#37096;&#22797;&#26434;&#24230;&#21487;&#20197;&#20219;&#24847;&#39640;&#12290;</title><link>https://arxiv.org/abs/2204.06062</link><description>&lt;p&gt;
ReLU&#31070;&#32463;&#32593;&#32476;&#20989;&#25968;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#25299;&#25169;&#22797;&#26434;&#24230;&#27979;&#37327;
&lt;/p&gt;
&lt;p&gt;
Local and global topological complexity measures OF ReLU neural network functions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2204.06062
&lt;/p&gt;
&lt;p&gt;
&#24212;&#29992;&#24191;&#20041;&#20998;&#27573;&#32447;&#24615;&#33707;&#23572;&#26031;&#29702;&#35770;&#23450;&#20041;&#21644;&#30740;&#31350;&#20102;ReLU&#31070;&#32463;&#32593;&#32476;&#20989;&#25968;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#25299;&#25169;&#22797;&#26434;&#24230;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20415;&#20110;&#35745;&#31639;&#30340;&#32039;&#20945;&#27169;&#22411;&#65292;&#24182;&#35777;&#26126;&#23616;&#37096;&#22797;&#26434;&#24230;&#21487;&#20197;&#20219;&#24847;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24212;&#29992;&#20102;Grunert-Kuhnel-Rote&#25552;&#20986;&#30340;&#24191;&#20041;&#20998;&#27573;&#32447;&#24615;(PL)&#33707;&#23572;&#26031;&#29702;&#35770;&#26469;&#23450;&#20041;&#21644;&#30740;&#31350;&#20840;&#36830;&#25509;&#21069;&#39304;ReLU&#31070;&#32463;&#32593;&#32476;&#20989;&#25968; F: R^n -&gt; R &#30340;&#26032;&#23616;&#37096;&#21644;&#20840;&#23616;&#25299;&#25169;&#22797;&#26434;&#24230;&#27010;&#24565;&#12290;&#22312;&#27492;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20026;&#27599;&#20010;&#36825;&#26679;&#30340; F &#26500;&#24314;&#19968;&#20010;&#35268;&#33539;&#22810;&#38754;&#20307;&#22797;&#21512;&#20307; K(F)&#65292;&#24182;&#19988;&#23450;&#20041;&#20102;&#23450;&#20041;&#22495;&#21040; K(F) &#30340;&#24418;&#21464;&#32553;&#24182;&#65292;&#20174;&#32780;&#24471;&#21040;&#19968;&#20010;&#26041;&#20415;&#30340;&#32039;&#20945;&#27169;&#22411;&#36827;&#34892;&#35745;&#31639;&#12290;&#25105;&#20204;&#36824;&#32473;&#20986;&#20102;&#19968;&#20010;&#26500;&#36896;&#65292;&#34920;&#26126;&#23616;&#37096;&#22797;&#26434;&#24230;&#21487;&#20197;&#20219;&#24847;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2204.06062v2 Announce Type: replace-cross  Abstract: We apply a generalized piecewise-linear (PL) version of Morse theory due to Grunert-Kuhnel-Rote to define and study new local and global notions of topological complexity for fully-connected feedforward ReLU neural network functions, F: R^n -&gt; R. Along the way, we show how to construct, for each such F, a canonical polytopal complex K(F) and a deformation retract of the domain onto K(F), yielding a convenient compact model for performing calculations. We also give a construction showing that local complexity can be arbitrarily high.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#21464;&#20998;&#25512;&#29702;&#30340;&#21464;&#20998;&#21160;&#24577;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#35299;&#20915;&#33258;&#30417;&#30563;&#25506;&#32034;&#20013;&#30340;&#22810;&#27169;&#24577;&#24615;&#21644;&#38543;&#26426;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2010.08755</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#25506;&#32034;&#20013;&#30340;&#21464;&#20998;&#21160;&#21147;&#23398;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Variational Dynamic for Self-Supervised Exploration in Deep Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2010.08755
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#21464;&#20998;&#25512;&#29702;&#30340;&#21464;&#20998;&#21160;&#24577;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#35299;&#20915;&#33258;&#30417;&#30563;&#25506;&#32034;&#20013;&#30340;&#22810;&#27169;&#24577;&#24615;&#21644;&#38543;&#26426;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26377;&#25928;&#25506;&#32034;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#37027;&#20123;&#22806;&#37096;&#22870;&#21169;&#31232;&#30095;&#29978;&#33267;&#23436;&#20840;&#34987;&#24573;&#35270;&#30340;&#20219;&#21153;&#12290;&#22522;&#20110;&#20869;&#22312;&#21160;&#26426;&#30340;&#26174;&#33879;&#36827;&#23637;&#22312;&#31616;&#21333;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#22312;&#20855;&#26377;&#22810;&#27169;&#24577;&#21644;&#38543;&#26426;&#21160;&#24577;&#30340;&#29615;&#22659;&#20013;&#32463;&#24120;&#38519;&#20837;&#22256;&#22659;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#21464;&#20998;&#25512;&#29702;&#30340;&#21464;&#20998;&#21160;&#24577;&#27169;&#22411;&#65292;&#20197;&#24314;&#27169;&#22810;&#27169;&#24577;&#24615;&#21644;&#38543;&#26426;&#24615;&#12290;&#25105;&#20204;&#23558;&#29615;&#22659;&#29366;&#24577;-&#21160;&#20316;&#36716;&#25442;&#35270;&#20026;&#19968;&#31181;&#26465;&#20214;&#29983;&#25104;&#36807;&#31243;&#65292;&#36890;&#36807;&#22312;&#24403;&#21069;&#29366;&#24577;&#12289;&#21160;&#20316;&#21644;&#28508;&#21464;&#37327;&#26465;&#20214;&#19979;&#29983;&#25104;&#19979;&#19968;&#20010;&#29366;&#24577;&#30340;&#39044;&#27979;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#29702;&#35299;&#21160;&#24577;&#24182;&#22312;&#25506;&#32034;&#20013;&#21462;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#29615;&#22659;&#36716;&#25442;&#30340;&#36127;&#23545;&#25968;&#20284;&#28982;&#30340;&#19978;&#30028;&#65292;&#24182;&#23558;&#36825;&#26679;&#30340;&#19978;&#30028;&#29992;&#20316;&#20869;&#22312;&#22870;&#21169;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2010.08755v3 Announce Type: replace  Abstract: Efficient exploration remains a challenging problem in reinforcement learning, especially for tasks where extrinsic rewards from environments are sparse or even totally disregarded. Significant advances based on intrinsic motivation show promising results in simple environments but often get stuck in environments with multimodal and stochastic dynamics. In this work, we propose a variational dynamic model based on the conditional variational inference to model the multimodality and stochasticity. We consider the environmental state-action transition as a conditional generative process by generating the next-state prediction under the condition of the current state, action, and latent variable, which provides a better understanding of the dynamics and leads a better performance in exploration. We derive an upper bound of the negative log-likelihood of the environmental transition and use such an upper bound as the intrinsic reward for
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#20013;&#20351;&#29992;&#19981;&#21516;&#24120;&#29992;&#28608;&#27963;&#20989;&#25968;&#65288;&#22914;sigmoid&#21644;&#21452;&#26354;&#27491;&#20999;&#65289;&#26102;&#30340;VC&#32500;&#24230;&#65292;&#37319;&#29992;&#20102;Pfaffian&#20989;&#25968;&#29702;&#35770;&#26694;&#26550;&#65292;&#36890;&#36807;&#26550;&#26500;&#21442;&#25968;&#21644;&#21512;&#20316;&#25968;&#37327;&#25552;&#20379;&#20102;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2401.12362</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#24102;&#26377;Pfaffian&#28608;&#27963;&#20989;&#25968;&#30340;VC&#32500;&#24230;
&lt;/p&gt;
&lt;p&gt;
VC dimension of Graph Neural Networks with Pfaffian activation functions. (arXiv:2401.12362v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12362
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#20013;&#20351;&#29992;&#19981;&#21516;&#24120;&#29992;&#28608;&#27963;&#20989;&#25968;&#65288;&#22914;sigmoid&#21644;&#21452;&#26354;&#27491;&#20999;&#65289;&#26102;&#30340;VC&#32500;&#24230;&#65292;&#37319;&#29992;&#20102;Pfaffian&#20989;&#25968;&#29702;&#35770;&#26694;&#26550;&#65292;&#36890;&#36807;&#26550;&#26500;&#21442;&#25968;&#21644;&#21512;&#20316;&#25968;&#37327;&#25552;&#20379;&#20102;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#36817;&#24180;&#26469;&#20316;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#20986;&#29616;&#65292;&#20197;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#23398;&#20064;&#21508;&#31181;&#22270;&#39046;&#22495;&#30340;&#20219;&#21153;&#65307;&#22522;&#20110;&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#65292;GNN&#30001;&#20110;&#20854;&#19982;Weisfeiler-Lehman&#65288;WL&#65289;&#22270;&#21516;&#26500;&#27979;&#35797;&#23494;&#20999;&#30456;&#20851;&#30340;&#30452;&#35266;&#34920;&#36798;&#32780;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#23427;&#20204;&#24050;&#34987;&#35777;&#26126;&#31561;&#20215;&#12290;&#20174;&#29702;&#35770;&#35282;&#24230;&#30475;&#65292;GNN&#34987;&#35777;&#26126;&#26159;&#36890;&#29992;&#36924;&#36817;&#22120;&#65292;&#24182;&#19988;&#26368;&#36817;&#23545;&#20855;&#26377;&#20998;&#27573;&#22810;&#39033;&#24335;&#28608;&#27963;&#20989;&#25968;&#30340;GNN&#30340;&#27867;&#21270;&#33021;&#21147;&#65288;&#21363;&#65292;&#23545;Vapnik Cherovenikis&#65288;VC&#65289;&#32500;&#24230;&#30340;&#30028;&#38480;&#65289;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#30446;&#26631;&#26159;&#23558;&#23545;GNN&#30340;VC&#32500;&#24230;&#30340;&#20998;&#26512;&#25193;&#23637;&#21040;&#20854;&#20182;&#24120;&#29992;&#28608;&#27963;&#20989;&#25968;&#65292;&#22914;sigmoid&#21644;&#21452;&#26354;&#27491;&#20999;&#65292;&#20351;&#29992;Pfaffian&#20989;&#25968;&#29702;&#35770;&#26694;&#26550;&#12290;&#25552;&#20379;&#20102;&#19982;&#26550;&#26500;&#21442;&#25968;&#65288;&#28145;&#24230;&#65292;&#31070;&#32463;&#20803;&#25968;&#37327;&#65292;&#36755;&#20837;&#23610;&#23544;&#65289;&#20197;&#21450;&#19982;&#21512;&#20316;&#25968;&#37327;&#26377;&#20851;&#30340;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have emerged in recent years as a powerful tool to learn tasks across a wide range of graph domains in a data-driven fashion; based on a message passing mechanism, GNNs have gained increasing popularity due to their intuitive formulation, closely linked with the Weisfeiler-Lehman (WL) test for graph isomorphism, to which they have proven equivalent. From a theoretical point of view, GNNs have been shown to be universal approximators, and their generalization capability (namely, bounds on the Vapnik Chervonekis (VC) dimension) has recently been investigated for GNNs with piecewise polynomial activation functions. The aim of our work is to extend this analysis on the VC dimension of GNNs to other commonly used activation functions, such as sigmoid and hyperbolic tangent, using the framework of Pfaffian function theory. Bounds are provided with respect to architecture parameters (depth, number of neurons, input size) as well as with respect to the number of co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#38598;&#25104;&#30340;&#32852;&#37030;&#23398;&#20064;&#22312;&#25932;&#23545;&#23041;&#32961;&#19979;&#30340;&#28431;&#27934;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#31574;&#30053;&#65292;&#25581;&#31034;&#20102;&#35813;&#27169;&#22411;&#22312;&#19981;&#21516;&#37197;&#32622;&#30340;&#32852;&#37030;&#23398;&#20064;&#19979;&#23545;&#25932;&#23545;&#23041;&#32961;&#30340;&#39640;&#25935;&#24863;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.10375</link><description>&lt;p&gt;
&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#38598;&#25104;&#30340;&#32852;&#37030;&#23398;&#20064;&#22312;&#25932;&#23545;&#23041;&#32961;&#19979;&#30340;&#28431;&#27934;
&lt;/p&gt;
&lt;p&gt;
Vulnerabilities of Foundation Model Integrated Federated Learning Under Adversarial Threats. (arXiv:2401.10375v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10375
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#38598;&#25104;&#30340;&#32852;&#37030;&#23398;&#20064;&#22312;&#25932;&#23545;&#23041;&#32961;&#19979;&#30340;&#28431;&#27934;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#31574;&#30053;&#65292;&#25581;&#31034;&#20102;&#35813;&#27169;&#22411;&#22312;&#19981;&#21516;&#37197;&#32622;&#30340;&#32852;&#37030;&#23398;&#20064;&#19979;&#23545;&#25932;&#23545;&#23041;&#32961;&#30340;&#39640;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#35299;&#20915;&#19982;&#25968;&#25454;&#38544;&#31169;&#21644;&#23433;&#20840;&#30456;&#20851;&#30340;&#26426;&#22120;&#23398;&#20064;&#30340;&#37325;&#35201;&#38382;&#39064;&#65292;&#20294;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#23384;&#22312;&#25968;&#25454;&#19981;&#36275;&#21644;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#22522;&#30784;&#27169;&#22411;&#30340;&#20986;&#29616;&#20026;&#29616;&#26377;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#30340;&#23616;&#38480;&#24615;&#25552;&#20379;&#20102;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20363;&#22914;&#36890;&#36807;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#21021;&#22987;&#21270;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#20869;&#22312;&#23433;&#20840;&#24615;&#38382;&#39064;&#65292;&#23558;&#22522;&#30784;&#27169;&#22411;&#38598;&#25104;&#21040;&#32852;&#37030;&#23398;&#20064;&#20013;&#21487;&#33021;&#24341;&#20837;&#26032;&#30340;&#39118;&#38505;&#65292;&#36825;&#26041;&#38754;&#30340;&#30740;&#31350;&#23578;&#23646;&#26410;&#24320;&#21457;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#39318;&#27425;&#30740;&#31350;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#38598;&#25104;&#30340;&#32852;&#37030;&#23398;&#20064;&#22312;&#25932;&#23545;&#23041;&#32961;&#19979;&#30340;&#28431;&#27934;&#12290;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#38598;&#25104;&#30340;&#32852;&#37030;&#23398;&#20064;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#31574;&#30053;&#65292;&#21033;&#29992;&#22522;&#30784;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#38382;&#39064;&#26469;&#30772;&#22351;&#32852;&#37030;&#23398;&#20064;&#23458;&#25143;&#31471;&#27169;&#22411;&#12290;&#36890;&#36807;&#22312;&#22270;&#20687;&#21644;&#25991;&#26412;&#39046;&#22495;&#20013;&#20351;&#29992;&#30693;&#21517;&#27169;&#22411;&#21644;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#38598;&#25104;&#30340;&#32852;&#37030;&#23398;&#20064;&#22312;&#19981;&#21516;&#37197;&#32622;&#30340;&#32852;&#37030;&#23398;&#20064;&#19979;&#23545;&#36825;&#31181;&#26032;&#23041;&#32961;&#30340;&#39640;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) addresses critical issues in machine learning related to data privacy and security, yet suffering from data insufficiency and imbalance under certain circumstances. The emergence of foundation models (FMs) offers potential solutions to the limitations of existing FL frameworks, e.g., by generating synthetic data for model initialization. However, due to the inherent safety concerns of FMs, integrating FMs into FL could introduce new risks, which remains largely unexplored. To address this gap, we conduct the first investigation on the vulnerability of FM integrated FL (FM-FL) under adversarial threats. Based on a unified framework of FM-FL, we introduce a novel attack strategy that exploits safety issues of FM to compromise FL client models. Through extensive experiments with well-known models and benchmark datasets in both image and text domains, we reveal the high susceptibility of the FM-FL to this new threat under various FL configurations. Furthermore, we f
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#12289;&#20197;&#36755;&#20986;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32479;&#35745;&#39564;&#35777;&#25216;&#26415;&#26469;&#35748;&#35777;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#20998;&#31867;&#22120;&#30340;&#36755;&#20986;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#26631;&#35760;&#21487;&#33021;&#19981;&#21487;&#38752;&#30340;&#29305;&#23450;&#36755;&#20837;&#65292;&#20197;&#20415;&#21518;&#32493;&#30001;&#20154;&#24037;&#19987;&#23478;&#26816;&#26597;&#12290;&#19982;&#29616;&#26377;&#25216;&#26415;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#21333;&#20010;&#36755;&#20986;&#32780;&#19981;&#26159;&#25972;&#20010;DNN&#30340;&#35748;&#35777;&#12290;</title><link>http://arxiv.org/abs/2401.02283</link><description>&lt;p&gt;
DEM: &#33322;&#31354;&#33322;&#22825;&#20013;&#29992;&#20110;&#35748;&#35777;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#36755;&#20986;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DEM: A Method for Certifying Deep Neural Network Classifier Outputs in Aerospace. (arXiv:2401.02283v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02283
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#12289;&#20197;&#36755;&#20986;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32479;&#35745;&#39564;&#35777;&#25216;&#26415;&#26469;&#35748;&#35777;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#20998;&#31867;&#22120;&#30340;&#36755;&#20986;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#26631;&#35760;&#21487;&#33021;&#19981;&#21487;&#38752;&#30340;&#29305;&#23450;&#36755;&#20837;&#65292;&#20197;&#20415;&#21518;&#32493;&#30001;&#20154;&#24037;&#19987;&#23478;&#26816;&#26597;&#12290;&#19982;&#29616;&#26377;&#25216;&#26415;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#21333;&#20010;&#36755;&#20986;&#32780;&#19981;&#26159;&#25972;&#20010;DNN&#30340;&#35748;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33322;&#31354;&#33322;&#22825;&#39046;&#22495;&#30340;&#36719;&#20214;&#24320;&#21457;&#35201;&#27714;&#36981;&#24490;&#20005;&#26684;&#12289;&#39640;&#36136;&#37327;&#30340;&#26631;&#20934;&#12290;&#23613;&#31649;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#23384;&#22312;&#30528;&#21830;&#29992;&#36719;&#20214;&#30340;&#30417;&#31649;&#25351;&#21335;&#65288;&#20363;&#22914;ARP-4754&#21644;DO-178&#65289;&#65292;&#20294;&#36825;&#20123;&#25351;&#21335;&#24182;&#19981;&#36866;&#29992;&#20110;&#20855;&#26377;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#32452;&#20214;&#30340;&#36719;&#20214;&#12290;&#22240;&#27492;&#65292;&#22914;&#20309;&#20351;&#33322;&#31354;&#33322;&#22825;&#31995;&#32479;&#21463;&#30410;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38761;&#21629;&#23578;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#20197;&#36755;&#20986;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#29992;&#20110;DNN&#30340;&#35748;&#35777;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#32479;&#35745;&#39564;&#35777;&#25216;&#26415;&#65292;&#24182;&#20855;&#26377;&#33021;&#22815;&#26631;&#35760;DNN&#36755;&#20986;&#21487;&#33021;&#19981;&#21487;&#38752;&#30340;&#29305;&#23450;&#36755;&#20837;&#30340;&#20851;&#38190;&#20248;&#21183;&#65292;&#20197;&#20415;&#21518;&#32493;&#30001;&#19987;&#23478;&#26816;&#26597;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;DNN&#23545;&#20854;&#20182;&#38468;&#36817;&#36755;&#20837;&#30340;&#39044;&#27979;&#36827;&#34892;&#32479;&#35745;&#20998;&#26512;&#65292;&#20197;&#26816;&#27979;&#19981;&#19968;&#33268;&#24615;&#12290;&#36825;&#19982;&#29616;&#26377;&#25216;&#26415;&#30456;&#21453;&#65292;&#21518;&#32773;&#36890;&#24120;&#35797;&#22270;&#23545;&#25972;&#20010;DNN&#36827;&#34892;&#35748;&#35777;&#65292;&#32780;&#38750;&#21333;&#20010;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
Software development in the aerospace domain requires adhering to strict, high-quality standards. While there exist regulatory guidelines for commercial software in this domain (e.g., ARP-4754 and DO-178), these do not apply to software with deep neural network (DNN) components. Consequently, it is unclear how to allow aerospace systems to benefit from the deep learning revolution. Our work here seeks to address this challenge with a novel, output-centric approach for DNN certification. Our method employs statistical verification techniques, and has the key advantage of being able to flag specific inputs for which the DNN's output may be unreliable - so that they may be later inspected by a human expert. To achieve this, our method conducts a statistical analysis of the DNN's predictions for other, nearby inputs, in order to detect inconsistencies. This is in contrast to existing techniques, which typically attempt to certify the entire DNN, as opposed to individual outputs. Our method
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#30701;&#26399;&#35745;&#21010;&#29983;&#25104;&#21644;&#36873;&#25321;&#19982;&#20998;&#24067;&#24335;&#20248;&#21270;&#20197;&#21450;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#28176;&#36827;&#26041;&#27861;&#65292;&#29992;&#20110;&#26080;&#20154;&#26426;&#30340;&#21327;&#35843;&#21644;&#35268;&#21010;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.09852</link><description>&lt;p&gt;
&#30701;&#26399;&#19982;&#38271;&#26399;&#26080;&#20154;&#26426;&#21327;&#35843;&#65306;&#20998;&#24067;&#24335;&#20248;&#21270;&#19982;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#20132;&#27719;
&lt;/p&gt;
&lt;p&gt;
Short vs. Long-term Coordination of Drones: When Distributed Optimization Meets Deep Reinforcement Learning. (arXiv:2311.09852v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.09852
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#30701;&#26399;&#35745;&#21010;&#29983;&#25104;&#21644;&#36873;&#25321;&#19982;&#20998;&#24067;&#24335;&#20248;&#21270;&#20197;&#21450;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#28176;&#36827;&#26041;&#27861;&#65292;&#29992;&#20110;&#26080;&#20154;&#26426;&#30340;&#21327;&#35843;&#21644;&#35268;&#21010;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26234;&#33021;&#22478;&#24066;&#20013;&#65292;&#25903;&#25345;&#20805;&#30005;&#25216;&#26415;&#30340;&#33258;&#20027;&#20132;&#20114;&#24335;&#26080;&#20154;&#26426;&#32676;&#21487;&#20197;&#25552;&#20379;&#24341;&#20154;&#27880;&#30446;&#30340;&#24863;&#30693;&#33021;&#21147;&#65292;&#20363;&#22914;&#20132;&#36890;&#30417;&#27979;&#21644;&#28798;&#38590;&#21709;&#24212;&#12290;&#29616;&#26377;&#26041;&#27861;&#65292;&#21253;&#25324;&#20998;&#24067;&#24335;&#20248;&#21270;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#65292;&#26088;&#22312;&#21327;&#35843;&#26080;&#20154;&#26426;&#20197;&#23454;&#29616;&#25104;&#26412;&#25928;&#30410;&#39640;&#12289;&#39640;&#36136;&#37327;&#30340;&#23548;&#33322;&#12289;&#24863;&#30693;&#21644;&#20805;&#30005;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#65306;&#30701;&#26399;&#20248;&#21270;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#30340;&#24847;&#22806;&#21464;&#21270;&#19979;&#24182;&#19981;&#26377;&#25928;&#65292;&#32780;&#38271;&#26399;&#23398;&#20064;&#32570;&#20047;&#21487;&#25193;&#23637;&#24615;&#12289;&#38887;&#24615;&#21644;&#28789;&#27963;&#24615;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28176;&#36827;&#26041;&#27861;&#65292;&#23558;&#22522;&#20110;&#20998;&#24067;&#24335;&#20248;&#21270;&#30340;&#30701;&#26399;&#35745;&#21010;&#29983;&#25104;&#21644;&#36873;&#25321;&#19982;&#22522;&#20110;DRL&#30340;&#38271;&#26399;&#39134;&#34892;&#26041;&#21521;&#30340;&#25112;&#30053;&#35843;&#24230;&#30456;&#32467;&#21512;&#12290;&#36890;&#36807;&#23545;&#20174;&#29616;&#23454;&#22478;&#24066;&#31227;&#21160;&#20013;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#34920;&#26126;&#25152;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Swarms of autonomous interactive drones, with the support of recharging technology, can provide compelling sensing capabilities in Smart Cities, such as traffic monitoring and disaster response. Existing approaches, including distributed optimization and deep reinforcement learning (DRL), aim to coordinate drones to achieve cost-effective, high-quality navigation, sensing, and charging. However, they face grand challenges: short-term optimization is not effective in dynamic environments with unanticipated changes, while long-term learning lacks scalability, resilience, and flexibility. To bridge this gap, this paper introduces a new progressive approach that combines short-term plan generation and selection based on distributed optimization with a DRL-based long-term strategic scheduling of flying direction. Extensive experimentation with datasets generated from realistic urban mobility underscores an outstanding performance of the proposed solution compared to state-of-the-art. We als
&lt;/p&gt;</description></item><item><title>FedSN&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;LEO&#21355;&#26143;&#32593;&#32476;&#20013;&#30340;&#24322;&#26500;&#35745;&#31639;&#21644;&#23384;&#20648;&#33021;&#21147;&#12289;&#26377;&#38480;&#30340;&#19978;&#34892;&#36895;&#29575;&#20197;&#21450;&#27169;&#22411;&#38472;&#26087;&#31561;&#20851;&#38190;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2311.01483</link><description>&lt;p&gt;
FedSN&#65306;&#19968;&#20010;&#36866;&#29992;&#20110;LEO&#21355;&#26143;&#32593;&#32476;&#30340;&#36890;&#29992;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
FedSN: A General Federated Learning Framework over LEO Satellite Networks. (arXiv:2311.01483v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01483
&lt;/p&gt;
&lt;p&gt;
FedSN&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;LEO&#21355;&#26143;&#32593;&#32476;&#20013;&#30340;&#24322;&#26500;&#35745;&#31639;&#21644;&#23384;&#20648;&#33021;&#21147;&#12289;&#26377;&#38480;&#30340;&#19978;&#34892;&#36895;&#29575;&#20197;&#21450;&#27169;&#22411;&#38472;&#26087;&#31561;&#20851;&#38190;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35768;&#22810;&#20302;&#22320;&#29699;&#36712;&#36947;&#65288;LEO&#65289;&#21355;&#26143;&#24050;&#32463;&#30001;&#21830;&#19994;&#20844;&#21496;&#25104;&#21151;&#22320;&#21457;&#23556;&#21644;&#37096;&#32626;&#21040;&#22826;&#31354;&#20013;&#65292;&#22914;SpaceX&#12290;&#30001;&#20110;LEO&#21355;&#26143;&#37197;&#22791;&#20102;&#22810;&#27169;&#20256;&#24863;&#22120;&#65292;&#23427;&#20204;&#19981;&#20165;&#29992;&#20110;&#36890;&#20449;&#65292;&#36824;&#29992;&#20110;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#65292;&#22914;&#31354;&#38388;&#35843;&#21046;&#35782;&#21035;&#12289;&#36965;&#24863;&#22270;&#20687;&#20998;&#31867;&#31561;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#19982;LEO&#21355;&#26143;&#30340;&#26377;&#38480;&#25509;&#35302;&#26102;&#38388;&#65288;&#20363;&#22914;5&#20998;&#38047;&#65289;&#65292;&#22320;&#38754;&#31449;&#65288;GS&#65289;&#21487;&#33021;&#26080;&#27861;&#19979;&#36733;&#22914;&#27492;&#22823;&#37327;&#30340;&#21407;&#22987;&#24863;&#27979;&#25968;&#25454;&#36827;&#34892;&#38598;&#20013;&#27169;&#22411;&#35757;&#32451;&#12290;&#22240;&#27492;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#24050;&#32463;&#25104;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#22312;&#35774;&#22791;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#35201;&#22312;LEO&#21355;&#26143;&#19978;&#20351;&#29992;FL&#65292;&#25105;&#20204;&#20173;&#28982;&#38754;&#20020;&#19977;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#21363;i&#65289;&#24322;&#26500;&#35745;&#31639;&#21644;&#23384;&#20648;&#33021;&#21147;&#65292;ii&#65289;&#26377;&#38480;&#30340;&#19978;&#34892;&#36895;&#29575;&#65292;&#20197;&#21450;iii&#65289;&#27169;&#22411;&#38472;&#26087;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedSN&#30340;&#36890;&#29992;FL&#26694;&#26550;&#26469;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#65292;&#19968;
&lt;/p&gt;
&lt;p&gt;
Recently, a large number of Low Earth Orbit (LEO) satellites have been launched and deployed successfully in space by commercial companies, such as SpaceX. Due to multimodal sensors equipped by the LEO satellites, they serve not only for communication but also for various machine learning applications, such as space modulation recognition, remote sensing image classification, etc. However, the ground station (GS) may be incapable of downloading such a large volume of raw sensing data for centralized model training due to the limited contact time with LEO satellites (e.g. 5 minutes). Therefore, federated learning (FL) has emerged as the promising solution to address this problem via on-device training. Unfortunately, to enable FL on LEO satellites, we still face three critical challenges that are i) heterogeneous computing and memory capabilities, ii) limited uplink rate, and iii) model staleness. To this end, we propose FedSN as a general FL framework to tackle the above challenges, an
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#35843;&#26597;&#35270;&#35273;&#19982;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20869;&#22312;&#30693;&#35782;&#65292;&#21457;&#29616;&#23427;&#20204;&#26174;&#31034;&#20102;&#22768;&#38899;&#35937;&#24449;&#24615;&#30340;&#27169;&#24335;&#65292;&#36827;&#19968;&#27493;&#35777;&#23454;&#22768;&#38899;&#21644;&#24847;&#20041;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#22312;&#36328;&#27169;&#24577;&#20851;&#32852;&#20013;&#24471;&#21040;&#20102;&#20307;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.16781</link><description>&lt;p&gt;
Kiki&#36824;&#26159;Bouba&#65311;&#35270;&#35273;&#19982;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#22768;&#38899;&#35937;&#24449;&#24615;
&lt;/p&gt;
&lt;p&gt;
Kiki or Bouba? Sound Symbolism in Vision-and-Language Models. (arXiv:2310.16781v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16781
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#35843;&#26597;&#35270;&#35273;&#19982;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20869;&#22312;&#30693;&#35782;&#65292;&#21457;&#29616;&#23427;&#20204;&#26174;&#31034;&#20102;&#22768;&#38899;&#35937;&#24449;&#24615;&#30340;&#27169;&#24335;&#65292;&#36827;&#19968;&#27493;&#35777;&#23454;&#22768;&#38899;&#21644;&#24847;&#20041;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#22312;&#36328;&#27169;&#24577;&#20851;&#32852;&#20013;&#24471;&#21040;&#20102;&#20307;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20154;&#31867;&#35821;&#35328;&#20013;&#30340;&#22768;&#38899;&#21644;&#24847;&#20041;&#20043;&#38388;&#30340;&#26144;&#23556;&#34987;&#35748;&#20026;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#26159;&#38543;&#26426;&#30340;&#65292;&#20294;&#35748;&#30693;&#31185;&#23398;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#35821;&#35328;&#21644;&#20154;&#21475;&#32676;&#20307;&#20043;&#38388;&#30340;&#29305;&#23450;&#22768;&#38899;&#21644;&#24847;&#20041;&#20043;&#38388;&#23384;&#22312;&#38750;&#24179;&#20961;&#30340;&#30456;&#20851;&#24615;&#65292;&#36825;&#31181;&#29616;&#35937;&#34987;&#31216;&#20026;&#22768;&#38899;&#35937;&#24449;&#24615;&#12290;&#22312;&#35768;&#22810;&#24847;&#20041;&#32500;&#24230;&#20013;&#65292;&#22768;&#38899;&#35937;&#24449;&#24615;&#22312;&#35821;&#35328;&#21644;&#35270;&#35273;&#39046;&#22495;&#20043;&#38388;&#30340;&#36328;&#27169;&#24577;&#20851;&#32852;&#26041;&#38754;&#23588;&#20026;&#26174;&#33879;&#21644;&#20805;&#20998;&#35777;&#26126;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22768;&#38899;&#35937;&#24449;&#24615;&#26159;&#21542;&#22312;CLIP&#21644;Stable Diffusion&#31561;&#35270;&#35273;&#19982;&#35821;&#35328;&#27169;&#22411;&#20013;&#24471;&#21040;&#20307;&#29616;&#12290;&#36890;&#36807;&#20351;&#29992;&#38646;&#26679;&#26412;&#30693;&#35782;&#25506;&#27979;&#26469;&#35843;&#26597;&#36825;&#20123;&#27169;&#22411;&#30340;&#20869;&#22312;&#30693;&#35782;&#65292;&#25105;&#20204;&#21457;&#29616;&#24378;&#26377;&#21147;&#30340;&#35777;&#25454;&#34920;&#26126;&#23427;&#20204;&#30830;&#23454;&#26174;&#31034;&#20102;&#36825;&#31181;&#27169;&#24335;&#65292;&#19982;&#24515;&#29702;&#35821;&#35328;&#23398;&#20013;&#20247;&#25152;&#21608;&#30693;&#30340;kiki-bouba&#25928;&#24212;&#30456;&#19968;&#33268;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20379;&#20102;&#19968;&#31181;&#20351;&#29992;&#35745;&#31639;&#24037;&#20855;&#26469;&#23637;&#31034;&#22768;&#38899;&#35937;&#24449;&#24615;&#24182;&#29702;&#35299;&#20854;&#26412;&#36136;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#23558;&#20844;&#24320;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although the mapping between sound and meaning in human language is assumed to be largely arbitrary, research in cognitive science has shown that there are non-trivial correlations between particular sounds and meanings across languages and demographic groups, a phenomenon known as sound symbolism. Among the many dimensions of meaning, sound symbolism is particularly salient and well-demonstrated with regards to cross-modal associations between language and the visual domain. In this work, we address the question of whether sound symbolism is reflected in vision-and-language models such as CLIP and Stable Diffusion. Using zero-shot knowledge probing to investigate the inherent knowledge of these models, we find strong evidence that they do show this pattern, paralleling the well-known kiki-bouba effect in psycholinguistics. Our work provides a novel method for demonstrating sound symbolism and understanding its nature using computational tools. Our code will be made publicly available.
&lt;/p&gt;</description></item><item><title>&#29616;&#26377;&#30340;&#27169;&#22411;&#26550;&#26500;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#34920;&#29616;&#26368;&#22909;&#65292;&#29305;&#21035;&#26159;&#22312;&#20219;&#21153;&#22797;&#26434;&#24615;&#22686;&#21152;&#30340;&#24773;&#20917;&#19979;&#12290;&#19981;&#21516;&#26550;&#26500;&#23545;&#36229;&#21442;&#25968;&#35774;&#32622;&#30340;&#25935;&#24863;&#24230;&#26377;&#25152;&#24046;&#24322;&#65292;&#19988;&#19968;&#20123;&#26550;&#26500;&#23637;&#29616;&#20986;&#24179;&#31283;&#30340;&#23398;&#20064;&#36712;&#36857;&#12290;</title><link>http://arxiv.org/abs/2310.08049</link><description>&lt;p&gt;
&#25506;&#32034;&#27169;&#22411;&#26550;&#26500;&#19982;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#20043;&#38388;&#30340;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Exploring the Relationship Between Model Architecture and In-Context Learning Ability. (arXiv:2310.08049v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08049
&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#27169;&#22411;&#26550;&#26500;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#34920;&#29616;&#26368;&#22909;&#65292;&#29305;&#21035;&#26159;&#22312;&#20219;&#21153;&#22797;&#26434;&#24615;&#22686;&#21152;&#30340;&#24773;&#20917;&#19979;&#12290;&#19981;&#21516;&#26550;&#26500;&#23545;&#36229;&#21442;&#25968;&#35774;&#32622;&#30340;&#25935;&#24863;&#24230;&#26377;&#25152;&#24046;&#24322;&#65292;&#19988;&#19968;&#20123;&#26550;&#26500;&#23637;&#29616;&#20986;&#24179;&#31283;&#30340;&#23398;&#20064;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#26550;&#26500;&#21644;&#25191;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#20219;&#21153;&#30340;&#33021;&#21147;&#20043;&#38388;&#26377;&#20160;&#20040;&#20851;&#32852;&#65311;&#22312;&#36825;&#39033;&#23454;&#35777;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#23581;&#35797;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#21313;&#20116;&#31181;&#27169;&#22411;&#26550;&#26500;&#22312;&#19968;&#22871;&#21512;&#25104;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;&#25152;&#36873;&#30340;&#26550;&#26500;&#20195;&#34920;&#20102;&#21508;&#31181;&#33539;&#24335;&#65292;&#21253;&#25324;&#24490;&#29615;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#21464;&#25442;&#22120;&#20197;&#21450;&#26032;&#20852;&#30340;&#27880;&#24847;&#21147;&#26367;&#20195;&#26041;&#26696;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#65292;&#25152;&#26377;&#32771;&#34385;&#30340;&#26550;&#26500;&#37117;&#33021;&#22815;&#25191;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#24403;&#20219;&#21153;&#22797;&#26434;&#24615;&#22686;&#21152;&#26102;&#65292;&#24403;&#20195;&#26550;&#26500;&#34920;&#29616;&#26368;&#22909;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#21518;&#32493;&#23454;&#39564;&#25506;&#32034;&#20102;&#19968;&#20123;&#24433;&#21709;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#22240;&#32032;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#19981;&#21516;&#26550;&#26500;&#23545;&#36229;&#21442;&#25968;&#35774;&#32622;&#26377;&#19981;&#21516;&#30340;&#25935;&#24863;&#24615;&#12290;&#25105;&#20204;&#23545;&#35757;&#32451;&#21160;&#24577;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#26576;&#20123;&#26550;&#26500;&#21576;&#29616;&#20986;&#24179;&#31283;&#12289;&#28176;&#36827;&#30340;&#23398;&#20064;&#36712;&#36857;&#65292;&#32780;...
&lt;/p&gt;
&lt;p&gt;
What is the relationship between model architecture and the ability to perform in-context learning? In this empirical study, we take the first steps towards answering this question. In particular, we evaluate fifteen model architectures across a suite of synthetic in-context learning tasks. The selected architectures represent a broad range of paradigms, including recurrent and convolution-based neural networks, transformers, and emerging attention alternatives. We discover that all considered architectures can perform in-context learning under certain conditions. However, contemporary architectures are found to be the best performing, especially as task complexity grows. Additionally, our follow-up experiments delve into various factors that influence in-context learning. We observe varied sensitivities among architectures with respect to hyperparameter settings. Our study of training dynamics reveals that certain architectures exhibit a smooth, progressive learning trajectory, while 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#25552;&#21319;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#30693;&#35782;&#39537;&#21160;&#23545;&#35805;&#29983;&#25104;&#30340;&#20013;&#38388;&#27493;&#39588;&#30340;&#29983;&#25104;&#24615;&#33021;&#12290;&#36890;&#36807;&#24341;&#20837;&#24341;&#23548;&#25552;&#31034;&#21644;&#20462;&#25913;&#25439;&#22833;&#20989;&#25968;&#30340;&#33258;&#20030;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#33258;&#21160;&#29983;&#25104;&#22238;&#31572;&#30340;&#22810;&#26679;&#24615;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.06404</link><description>&lt;p&gt;
Hexa: &#30693;&#35782;&#39537;&#21160;&#30340;&#23545;&#35805;&#31995;&#32479;&#30340;&#33258;&#25105;&#25552;&#21319;
&lt;/p&gt;
&lt;p&gt;
Hexa: Self-Improving for Knowledge-Grounded Dialogue System. (arXiv:2310.06404v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06404
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#25552;&#21319;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#30693;&#35782;&#39537;&#21160;&#23545;&#35805;&#29983;&#25104;&#30340;&#20013;&#38388;&#27493;&#39588;&#30340;&#29983;&#25104;&#24615;&#33021;&#12290;&#36890;&#36807;&#24341;&#20837;&#24341;&#23548;&#25552;&#31034;&#21644;&#20462;&#25913;&#25439;&#22833;&#20989;&#25968;&#30340;&#33258;&#20030;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#33258;&#21160;&#29983;&#25104;&#22238;&#31572;&#30340;&#22810;&#26679;&#24615;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#39537;&#21160;&#30340;&#23545;&#35805;&#29983;&#25104;&#20013;&#19968;&#31181;&#24120;&#35265;&#30340;&#20570;&#27861;&#26159;&#20351;&#29992;&#27169;&#22359;&#21270;&#30340;&#26041;&#27861;&#26126;&#30830;&#22320;&#21033;&#29992;&#20013;&#38388;&#27493;&#39588;&#65288;&#22914;&#32593;&#32476;&#25628;&#32034;&#12289;&#35760;&#24518;&#26816;&#32034;&#65289;&#12290;&#28982;&#32780;&#65292;&#19982;&#23545;&#35805;&#21709;&#24212;&#30456;&#27604;&#65292;&#36825;&#20123;&#27493;&#39588;&#30340;&#25968;&#25454;&#24448;&#24448;&#38590;&#20197;&#33719;&#21462;&#65292;&#22240;&#20026;&#22312;&#26222;&#36890;&#23545;&#35805;&#20013;&#26080;&#27861;&#35266;&#23519;&#21040;&#23427;&#20204;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20123;&#25968;&#25454;&#30340;&#32570;&#22833;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#25105;&#25552;&#21319;&#26041;&#27861;&#65292;&#20197;&#25913;&#36827;&#20013;&#38388;&#27493;&#39588;&#30340;&#29983;&#25104;&#24615;&#33021;&#65292;&#32780;&#19981;&#38656;&#35201;&#22320;&#38754;&#30495;&#23454;&#25968;&#25454;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24341;&#23548;&#25552;&#31034;&#21644;&#20462;&#25913;&#30340;&#25439;&#22833;&#20989;&#25968;&#30340;&#24341;&#23548;&#33258;&#21160;&#29983;&#25104;&#22238;&#31572;&#22810;&#26679;&#24615;&#30340;&#33258;&#20030;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#32463;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#25104;&#21151;&#22320;&#21033;&#29992;&#20102;&#33258;&#25105;&#25552;&#21319;&#26426;&#21046;&#65292;&#22312;&#29983;&#25104;&#20013;&#38388;&#21644;&#26368;&#32456;&#22238;&#31572;&#26041;&#38754;&#25913;&#21892;&#20102;&#30693;&#35782;&#39537;&#21160;&#23545;&#35805;&#29983;&#25104;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
A common practice in knowledge-grounded dialogue generation is to explicitly utilize intermediate steps (e.g., web-search, memory retrieval) with modular approaches. However, data for such steps are often inaccessible compared to those of dialogue responses as they are unobservable in an ordinary dialogue. To fill in the absence of these data, we develop a self-improving method to improve the generative performances of intermediate steps without the ground truth data. In particular, we propose a novel bootstrapping scheme with a guided prompt and a modified loss function to enhance the diversity of appropriate self-generated responses. Through experiments on various benchmark datasets, we empirically demonstrate that our method successfully leverages a self-improving mechanism in generating intermediate and final responses and improves the performances on the task of knowledge-grounded dialogue generation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550; TEMPO&#65292;&#36890;&#36807;&#21033;&#29992;&#26102;&#38388;&#24207;&#21015;&#20219;&#21153;&#30340;&#20004;&#20010;&#37325;&#35201;&#24402;&#32435;&#20559;&#24046;&#65292;&#21363;&#23558;&#22797;&#26434;&#20132;&#20114;&#20998;&#35299;&#21644;&#24341;&#20837;&#22522;&#20110;&#36873;&#25321;&#30340;&#25552;&#31034;&#26469;&#26377;&#25928;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2310.04948</link><description>&lt;p&gt;
TEMPO: &#22522;&#20110;&#25552;&#31034;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;&#27169;&#22411;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
TEMPO: Prompt-based Generative Pre-trained Transformer for Time Series Forecasting. (arXiv:2310.04948v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04948
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550; TEMPO&#65292;&#36890;&#36807;&#21033;&#29992;&#26102;&#38388;&#24207;&#21015;&#20219;&#21153;&#30340;&#20004;&#20010;&#37325;&#35201;&#24402;&#32435;&#20559;&#24046;&#65292;&#21363;&#23558;&#22797;&#26434;&#20132;&#20114;&#20998;&#35299;&#21644;&#24341;&#20837;&#22522;&#20110;&#36873;&#25321;&#30340;&#25552;&#31034;&#26469;&#26377;&#25928;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#23613;&#31649;&#22312;&#21462;&#24471;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#30340;&#21516;&#26102;&#65292;&#26368;&#22909;&#30340;&#26550;&#26500;&#22312;&#19981;&#21516;&#24212;&#29992;&#21644;&#39046;&#22495;&#20043;&#38388;&#24046;&#24322;&#24456;&#22823;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#65292;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;(GPT)&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;&#22312;&#21508;&#31181;&#25991;&#26412;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25506;&#32034;&#26159;&#21542;GPT&#31867;&#22411;&#30340;&#26550;&#26500;&#21487;&#20197;&#23545;&#26102;&#38388;&#24207;&#21015;&#20135;&#29983;&#26377;&#25928;&#30340;&#24433;&#21709;&#65292;&#25429;&#25417;&#20854;&#20869;&#22312;&#21160;&#24577;&#23646;&#24615;&#24182;&#26174;&#33879;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;TEMPO&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#21033;&#29992;&#26102;&#38388;&#24207;&#21015;&#20219;&#21153;&#30340;&#20004;&#31181;&#37325;&#35201;&#24402;&#32435;&#20559;&#24046;&#26469;&#39044;&#35757;&#32451;&#27169;&#22411;&#65306;(i) &#23545;&#36235;&#21183;&#12289;&#23395;&#33410;&#21644;&#27531;&#24046;&#25104;&#20998;&#22797;&#26434;&#20132;&#20114;&#30340;&#20998;&#35299;&#65307;&#21644;(ii) &#25552;&#20986;&#22522;&#20110;&#36873;&#25321;&#30340;&#25552;&#31034;&#20197;&#20415;&#20110;&#38750;&#20998;&#24067;&#33258;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
The past decade has witnessed significant advances in time series modeling with deep learning. While achieving state-of-the-art results, the best-performing architectures vary highly across applications and domains. Meanwhile, for natural language processing, the Generative Pre-trained Transformer (GPT) has demonstrated impressive performance via training one general-purpose model across various textual datasets. It is intriguing to explore whether GPT-type architectures can be effective for time series, capturing the intrinsic dynamic attributes and leading to significant accuracy improvements. In this paper, we propose a novel framework, TEMPO, that can effectively learn time series representations. We focus on utilizing two essential inductive biases of the time series task for pre-trained models: (i) decomposition of the complex interaction between trend, seasonal and residual components; and (ii) introducing the selection-based prompts to facilitate distribution adaptation in non-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#29983;&#25104;&#27169;&#22411;&#22312;&#28151;&#21512;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#23545;&#31283;&#23450;&#24615;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#35777;&#26126;&#21021;&#22987;&#29983;&#25104;&#27169;&#22411;&#36275;&#22815;&#25509;&#36817;&#25968;&#25454;&#20998;&#24067;&#24182;&#19988;&#25968;&#25454;&#27604;&#20363;&#36866;&#24403;&#65292;&#36845;&#20195;&#35757;&#32451;&#26159;&#31283;&#23450;&#30340;&#12290;</title><link>http://arxiv.org/abs/2310.00429</link><description>&lt;p&gt;
&#20851;&#20110;&#29983;&#25104;&#27169;&#22411;&#22312;&#20854;&#33258;&#24049;&#30340;&#25968;&#25454;&#19978;&#36845;&#20195;&#35757;&#32451;&#30340;&#31283;&#23450;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Stability of Iterative Retraining of Generative Models on their own Data. (arXiv:2310.00429v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00429
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29983;&#25104;&#27169;&#22411;&#22312;&#28151;&#21512;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#23545;&#31283;&#23450;&#24615;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#35777;&#26126;&#21021;&#22987;&#29983;&#25104;&#27169;&#22411;&#36275;&#22815;&#25509;&#36817;&#25968;&#25454;&#20998;&#24067;&#24182;&#19988;&#25968;&#25454;&#27604;&#20363;&#36866;&#24403;&#65292;&#36845;&#20195;&#35757;&#32451;&#26159;&#31283;&#23450;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#24314;&#27169;&#22797;&#26434;&#25968;&#25454;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#65292;&#24448;&#24448;&#23637;&#29616;&#20986;&#36229;&#36807;&#20856;&#22411;&#20154;&#31867;&#33021;&#21147;&#30340;&#26679;&#26412;&#30495;&#23454;&#24615;&#36776;&#21035;&#33021;&#21147;&#12290;&#36825;&#19968;&#25104;&#21151;&#30340;&#20851;&#38190;&#39537;&#21160;&#21147;&#26080;&#30097;&#26159;&#36825;&#20123;&#27169;&#22411;&#28040;&#32791;&#28023;&#37327;&#32593;&#32476;&#35268;&#27169;&#25968;&#25454;&#30340;&#32467;&#26524;&#12290;&#30001;&#20110;&#36825;&#20123;&#27169;&#22411;&#24778;&#20154;&#30340;&#24615;&#33021;&#21644;&#26131;&#24471;&#24615;&#65292;&#32593;&#32476;&#19978;&#23558;&#19981;&#21487;&#36991;&#20813;&#22320;&#20986;&#29616;&#36234;&#26469;&#36234;&#22810;&#30340;&#21512;&#25104;&#20869;&#23481;&#12290;&#36825;&#20010;&#20107;&#23454;&#30452;&#25509;&#24847;&#21619;&#30528;&#29983;&#25104;&#27169;&#22411;&#30340;&#26410;&#26469;&#36845;&#20195;&#24517;&#39035;&#38754;&#23545;&#19968;&#20010;&#29616;&#23454;&#65306;&#23427;&#20204;&#30340;&#35757;&#32451;&#25968;&#25454;&#30001;&#28165;&#27905;&#25968;&#25454;&#21644;&#20808;&#21069;&#27169;&#22411;&#29983;&#25104;&#30340;&#20154;&#24037;&#25968;&#25454;&#32452;&#25104;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#23545;&#28151;&#21512;&#25968;&#25454;&#38598;&#65288;&#21253;&#25324;&#30495;&#23454;&#25968;&#25454;&#21644;&#21512;&#25104;&#25968;&#25454;&#65289;&#19978;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#23545;&#31283;&#23450;&#24615;&#30340;&#24433;&#21709;&#36827;&#34892;&#20005;&#26684;&#30740;&#31350;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#22312;&#21021;&#22987;&#29983;&#25104;&#27169;&#22411;&#36275;&#22815;&#22909;&#22320;&#36817;&#20284;&#25968;&#25454;&#20998;&#24067;&#24182;&#19988;&#30495;&#23454;&#25968;&#25454;&#19982;&#21512;&#25104;&#25968;&#25454;&#30340;&#27604;&#20363;&#36866;&#24403;&#30340;&#24773;&#20917;&#19979;&#65292;&#36845;&#20195;&#35757;&#32451;&#30340;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep generative models have made tremendous progress in modeling complex data, often exhibiting generation quality that surpasses a typical human's ability to discern the authenticity of samples. Undeniably, a key driver of this success is enabled by the massive amounts of web-scale data consumed by these models. Due to these models' striking performance and ease of availability, the web will inevitably be increasingly populated with synthetic content. Such a fact directly implies that future iterations of generative models must contend with the reality that their training is curated from both clean data and artificially generated data from past models. In this paper, we develop a framework to rigorously study the impact of training generative models on mixed datasets (of real and synthetic data) on their stability. We first prove the stability of iterative training under the condition that the initial generative models approximate the data distribution well enough and the proportion o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#37327;&#23376;&#21551;&#21457;&#30340;&#20108;&#32500;&#24352;&#37327;&#32593;&#32476;&#39640;&#25928;&#27169;&#25311;IBM&#26368;&#22823;&#30340;&#37327;&#23376;&#22788;&#29702;&#22120;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#24352;&#37327;&#26356;&#26032;&#23454;&#29616;&#21069;&#25152;&#26410;&#26377;&#30340;&#20934;&#30830;&#24230;&#21644;&#26497;&#20302;&#30340;&#35745;&#31639;&#36164;&#28304;&#28040;&#32791;&#65292;&#24182;&#20026;&#26368;&#26032;&#30340;IBM&#37327;&#23376;&#26426;&#22120;&#35774;&#31435;&#20102;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2309.15642</link><description>&lt;p&gt;
IBM&#26368;&#22823;&#37327;&#23376;&#22788;&#29702;&#22120;&#30340;&#24352;&#37327;&#32593;&#32476;&#27169;&#25311;&#30340;&#39640;&#25928;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient tensor network simulation of IBM's largest quantum processors. (arXiv:2309.15642v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15642
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#37327;&#23376;&#21551;&#21457;&#30340;&#20108;&#32500;&#24352;&#37327;&#32593;&#32476;&#39640;&#25928;&#27169;&#25311;IBM&#26368;&#22823;&#30340;&#37327;&#23376;&#22788;&#29702;&#22120;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#24352;&#37327;&#26356;&#26032;&#23454;&#29616;&#21069;&#25152;&#26410;&#26377;&#30340;&#20934;&#30830;&#24230;&#21644;&#26497;&#20302;&#30340;&#35745;&#31639;&#36164;&#28304;&#28040;&#32791;&#65292;&#24182;&#20026;&#26368;&#26032;&#30340;IBM&#37327;&#23376;&#26426;&#22120;&#35774;&#31435;&#20102;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#37327;&#23376;&#21551;&#21457;&#30340;&#20108;&#32500;&#24352;&#37327;&#32593;&#32476;&#26469;&#39640;&#25928;&#20934;&#30830;&#22320;&#27169;&#25311;IBM&#26368;&#22823;&#30340;&#37327;&#23376;&#22788;&#29702;&#22120;&#65292;&#21363;Eagle&#65288;127&#20010;&#37327;&#23376;&#27604;&#29305;&#65289;&#65292;Osprey&#65288;433&#20010;&#37327;&#23376;&#27604;&#29305;&#65289;&#21644;Condor&#65288;1121&#20010;&#37327;&#23376;&#27604;&#29305;&#65289;&#12290;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#22270;&#30340;&#25237;&#24433;&#32416;&#32544;&#23545;&#24577;&#65288;gPEPS&#65289;&#27169;&#25311;&#20102;&#19968;&#20010;&#22797;&#26434;&#30340;&#37327;&#23376;&#22810;&#20307;&#31995;&#32479;&#30340;&#21160;&#21147;&#23398;&#65292;&#20855;&#20307;&#26469;&#35828;&#65292;&#36825;&#26159;IBM&#26368;&#36817;&#22312;Nature 618&#24180;&#31532;500-505&#39029;&#65288;2023&#24180;&#65289;&#19978;&#32771;&#34385;&#30340;&#36386;&#20987;&#26131;&#36763;&#23454;&#39564;-&#25105;&#20204;&#22312;PRB 99, 195105&#65288;2019&#24180;&#65289;&#20013;&#25552;&#20986;&#20102;&#36825;&#20010;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#20110;&#35813;&#27169;&#22411;&#65292;&#31616;&#21333;&#30340;&#24352;&#37327;&#26356;&#26032;&#24050;&#32463;&#36275;&#20197;&#20197;&#26497;&#20302;&#30340;&#35745;&#31639;&#36164;&#28304;&#23454;&#29616;&#38750;&#24120;&#22823;&#30340;&#21069;&#25152;&#26410;&#26377;&#30340;&#31934;&#24230;&#12290;&#38500;&#20102;&#27169;&#25311;127&#20010;&#37327;&#23376;&#27604;&#29305;&#30340;&#21407;&#22987;&#23454;&#39564;&#22806;&#65292;&#25105;&#20204;&#36824;&#23558;&#32467;&#26524;&#25193;&#23637;&#21040;433&#20010;&#21644;1121&#20010;&#37327;&#23376;&#27604;&#29305;&#65292;&#20174;&#32780;&#20026;&#26368;&#26032;&#30340;IBM&#37327;&#23376;&#26426;&#22120;&#35774;&#23450;&#20102;&#19968;&#20010;&#22522;&#20934;&#12290;&#25105;&#20204;&#36824;&#25253;&#36947;&#20102;&#26080;&#38480;&#22810;&#20010;&#37327;&#23376;&#27604;&#29305;&#30340;&#20934;&#30830;&#27169;&#25311;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;gPEPS&#26159;&#39640;&#25928;&#27169;&#25311;&#37327;&#23376;&#35745;&#31639;&#26426;&#30340;&#33258;&#28982;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show how quantum-inspired 2d tensor networks can be used to efficiently and accurately simulate the largest quantum processors from IBM, namely Eagle (127 qubits), Osprey (433 qubits) and Condor (1121 qubits). We simulate the dynamics of a complex quantum many-body system -- specifically, the kicked Ising experiment considered recently by IBM in Nature 618, p. 500-505 (2023) -using graph-based Projected Entangled Pair States (gPEPS), which was proposed by some of us in PRB 99, 195105 (2019). Our results show that simple tensor updates are already sufficient to achieve very large unprecedented accuracy with remarkably low computational resources for this model. Apart from simulating the original experiment for 127 qubits, we also extend our results to 433 and 1121 qubits, thus setting a benchmark for the newest IBM quantum machines. We also report accurate simulations for infinitely-many qubits. Our results show that gPEPS are a natural tool to efficiently simulate quantum computer
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20540;&#23548;&#21521;&#30340;Monte-Carlo Tree Search&#35299;&#30721;&#31639;&#27861;PPO-MCTS&#65292;&#36890;&#36807;&#22312;PPO&#20043;&#19978;&#38598;&#25104;MCTS&#65292;&#35299;&#20915;&#20102;&#35757;&#32451;&#21644;&#27979;&#35797;&#20043;&#38388;&#37096;&#20998;&#36755;&#20986;&#35780;&#20998;&#26426;&#21046;&#30340;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#31639;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.15028</link><description>&lt;p&gt;
&#35753;PPO&#21464;&#24471;&#26356;&#22909;&#65306;&#22522;&#20110;&#20540;&#23548;&#21521;&#30340;Monte-Carlo Tree Search&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Making PPO even better: Value-Guided Monte-Carlo Tree Search decoding. (arXiv:2309.15028v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15028
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20540;&#23548;&#21521;&#30340;Monte-Carlo Tree Search&#35299;&#30721;&#31639;&#27861;PPO-MCTS&#65292;&#36890;&#36807;&#22312;PPO&#20043;&#19978;&#38598;&#25104;MCTS&#65292;&#35299;&#20915;&#20102;&#35757;&#32451;&#21644;&#27979;&#35797;&#20043;&#38388;&#37096;&#20998;&#36755;&#20986;&#35780;&#20998;&#26426;&#21046;&#30340;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#31639;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#26102;&#65292;&#20351;&#29992;&#26368;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#22914;Proximal Policy Optimization (PPO)&#65292;&#22240;&#27492;&#21487;&#20197;&#35748;&#20026;&#25512;&#29702;&#26102;&#38388;&#30340;&#25628;&#32034;&#31639;&#27861;&#65292;&#22914;Monte-Carlo Tree Search (MCTS) &#26159;&#19981;&#24517;&#35201;&#30340;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#36890;&#36807;&#22312;PPO&#20043;&#19978;&#38598;&#25104;MCTS&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#21319;PPO&#30340;&#24615;&#33021;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#22312;&#35299;&#30721;&#25991;&#26412;&#26102;&#65292;&#19981;&#35201;&#20002;&#24323;&#20540;&#32593;&#32476;&#65292;&#21363;PPO&#35757;&#32451;&#26102;&#29992;&#20110;&#35780;&#20272;&#37096;&#20998;&#36755;&#20986;&#24207;&#21015;&#30340;&#21103;&#20135;&#21697;&#65292;&#32780;&#26159;&#23558;&#20854;&#19982;&#31574;&#30053;&#32593;&#32476;&#32039;&#23494;&#32467;&#21512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;PPO-MCTS&#30340;&#26032;&#39062;&#30340;&#20540;&#23548;&#21521;&#35299;&#30721;&#31639;&#27861;&#65292;&#21487;&#20197;&#23558;&#26469;&#33258;PPO&#30340;&#20540;&#32593;&#32476;&#19982;&#25512;&#29702;&#26102;&#38388;&#20135;&#29983;&#30340;&#31574;&#30053;&#32593;&#32476;&#32039;&#23494;&#32467;&#21512;&#12290;&#19982;&#22522;&#20110;MCTS&#30340;&#25511;&#21046;&#25991;&#26412;&#29983;&#25104;&#30340;&#20808;&#21069;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20851;&#38190;&#20248;&#21183;&#22312;&#20110;&#20943;&#23569;&#20102;&#35757;&#32451;&#21644;&#27979;&#35797;&#20043;&#38388;&#37096;&#20998;&#36755;&#20986;&#30340;&#35780;&#20998;&#26426;&#21046;&#30340;&#22522;&#26412;&#19981;&#21305;&#37197;&#12290;&#22312;&#22235;&#20010;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;PPO-MCTS&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inference-time search algorithms such as Monte-Carlo Tree Search (MCTS) may seem unnecessary when generating natural language text based on state-of-the-art reinforcement learning such as Proximal Policy Optimization (PPO). In this paper, we demonstrate that it is possible to get extra mileage out of PPO by integrating MCTS on top. The key idea is not to throw out the value network, a byproduct of PPO training for evaluating partial output sequences, when decoding text out of the policy network. More concretely, we present a novel value-guided decoding algorithm called PPO-MCTS, which can integrate the value network from PPO to work closely with the policy network during inference-time generation. Compared to prior approaches based on MCTS for controlled text generation, the key strength of our approach is to reduce the fundamental mismatch of the scoring mechanisms of the partial outputs between training and test. Evaluation on four text generation tasks demonstrate that PPO-MCTS grea
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21464;&#37327;&#37325;&#35201;&#24615;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22312;&#25968;&#25454;&#20998;&#24067;&#19978;&#26159;&#31283;&#23450;&#30340;&#65292;&#24182;&#21487;&#20197;&#19982;&#29616;&#26377;&#30340;&#27169;&#22411;&#31867;&#21644;&#20840;&#23616;&#21464;&#37327;&#37325;&#35201;&#24615;&#25351;&#26631;&#32467;&#21512;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.13775</link><description>&lt;p&gt;
&#35770;&#25991;&#26631;&#39064;&#65306;The Rashomon Importance Distribution: &#25670;&#33073;&#19981;&#31283;&#23450;&#30340;&#22522;&#20110;&#21333;&#19968;&#27169;&#22411;&#30340;&#21464;&#37327;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Rashomon Importance Distribution: Getting RID of Unstable, Single Model-based Variable Importance. (arXiv:2309.13775v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13775
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21464;&#37327;&#37325;&#35201;&#24615;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22312;&#25968;&#25454;&#20998;&#24067;&#19978;&#26159;&#31283;&#23450;&#30340;&#65292;&#24182;&#21487;&#20197;&#19982;&#29616;&#26377;&#30340;&#27169;&#22411;&#31867;&#21644;&#20840;&#23616;&#21464;&#37327;&#37325;&#35201;&#24615;&#25351;&#26631;&#32467;&#21512;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#21464;&#37327;&#37325;&#35201;&#24615;&#23545;&#20110;&#22238;&#31572;&#36951;&#20256;&#23398;&#12289;&#20844;&#20849;&#25919;&#31574;&#21644;&#21307;&#23398;&#31561;&#39046;&#22495;&#30340;&#37325;&#22823;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#35745;&#31639;&#32473;&#23450;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#32473;&#23450;&#27169;&#22411;&#30340;&#21464;&#37327;&#37325;&#35201;&#24615;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#32473;&#23450;&#25968;&#25454;&#38598;&#65292;&#21487;&#33021;&#26377;&#35768;&#22810;&#27169;&#22411;&#21516;&#26679;&#33021;&#35299;&#37322;&#30446;&#26631;&#32467;&#26524;;&#22914;&#26524;&#19981;&#32771;&#34385;&#25152;&#26377;&#21487;&#33021;&#30340;&#35299;&#37322;&#65292;&#19981;&#21516;&#30340;&#30740;&#31350;&#32773;&#21487;&#33021;&#20250;&#24471;&#20986;&#35768;&#22810;&#20914;&#31361;&#20294;&#21516;&#26679;&#26377;&#25928;&#30340;&#32467;&#35770;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#32771;&#34385;&#20102;&#32473;&#23450;&#25968;&#25454;&#38598;&#30340;&#25152;&#26377;&#21487;&#33021;&#35299;&#37322;&#65292;&#36825;&#20123;&#27934;&#23519;&#21147;&#21487;&#33021;&#19981;&#20855;&#26377;&#26222;&#36866;&#24615;&#65292;&#22240;&#20026;&#24182;&#38750;&#25152;&#26377;&#22909;&#30340;&#35299;&#37322;&#22312;&#21512;&#29702;&#30340;&#25968;&#25454;&#25200;&#21160;&#19979;&#37117;&#26159;&#31283;&#23450;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21464;&#37327;&#37325;&#35201;&#24615;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#37327;&#21270;&#20102;&#22312;&#25152;&#26377;&#22909;&#30340;&#27169;&#22411;&#38598;&#21512;&#20013;&#30340;&#21464;&#37327;&#37325;&#35201;&#24615;&#65292;&#24182;&#19988;&#22312;&#25968;&#25454;&#20998;&#24067;&#19978;&#26159;&#31283;&#23450;&#30340;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#38750;&#24120;&#28789;&#27963;&#65292;&#21487;&#20197;&#19982;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#27169;&#22411;&#31867;&#21644;&#20840;&#23616;&#21464;&#37327;&#37325;&#35201;&#24615;&#25351;&#26631;&#32467;&#21512;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantifying variable importance is essential for answering high-stakes questions in fields like genetics, public policy, and medicine. Current methods generally calculate variable importance for a given model trained on a given dataset. However, for a given dataset, there may be many models that explain the target outcome equally well; without accounting for all possible explanations, different researchers may arrive at many conflicting yet equally valid conclusions given the same data. Additionally, even when accounting for all possible explanations for a given dataset, these insights may not generalize because not all good explanations are stable across reasonable data perturbations. We propose a new variable importance framework that quantifies the importance of a variable across the set of all good models and is stable across the data distribution. Our framework is extremely flexible and can be integrated with most existing model classes and global variable importance metrics. We d
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22522;&#20110;&#39044;&#27979;&#30340;&#23646;&#24615;&#26041;&#27861;&#19982;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#20135;&#29983;&#30340;&#23646;&#24615;&#22270;&#20855;&#26377;&#19981;&#21516;&#30340;&#39640;&#39057;&#20869;&#23481;&#65292;&#28388;&#38500;&#39640;&#39057;&#29575;&#21487;&#20197;&#25552;&#39640;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.09591</link><description>&lt;p&gt;
&#26799;&#24230;&#21453;&#20987;&#65306;&#22914;&#20309;&#28388;&#38500;&#39640;&#39057;&#29575;&#25552;&#39640;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Gradient strikes back: How filtering out high frequencies improves explanations. (arXiv:2307.09591v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09591
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22522;&#20110;&#39044;&#27979;&#30340;&#23646;&#24615;&#26041;&#27861;&#19982;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#20135;&#29983;&#30340;&#23646;&#24615;&#22270;&#20855;&#26377;&#19981;&#21516;&#30340;&#39640;&#39057;&#20869;&#23481;&#65292;&#28388;&#38500;&#39640;&#39057;&#29575;&#21487;&#20197;&#25552;&#39640;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26032;&#22411;&#22522;&#20110;&#39044;&#27979;&#30340;&#23646;&#24615;&#26041;&#27861;&#30340;&#21457;&#23637;&#36805;&#29467;&#65292;&#36880;&#28176;&#21462;&#20195;&#20102;&#26087;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#26469;&#35299;&#37322;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#39044;&#27979;&#22411;&#26041;&#27861;&#20026;&#20309;&#20248;&#20110;&#26799;&#24230;&#22411;&#26041;&#27861;&#20173;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#20174;&#32463;&#39564;&#35266;&#23519;&#24320;&#22987;&#65306;&#36825;&#20004;&#31181;&#26041;&#27861;&#20135;&#29983;&#30340;&#23646;&#24615;&#22270;&#20855;&#26377;&#38750;&#24120;&#19981;&#21516;&#30340;&#21151;&#29575;&#35889;&#65292;&#26799;&#24230;&#22411;&#26041;&#27861;&#25581;&#31034;&#20102;&#27604;&#39044;&#27979;&#22411;&#26041;&#27861;&#26356;&#22810;&#30340;&#39640;&#39057;&#20869;&#23481;&#12290;&#36825;&#19968;&#35266;&#23519;&#24341;&#21457;&#20102;&#22810;&#20010;&#38382;&#39064;&#65306;&#36825;&#31181;&#39640;&#39057;&#20449;&#24687;&#30340;&#26469;&#28304;&#26159;&#20160;&#20040;&#65292;&#23427;&#26159;&#21542;&#30495;&#27491;&#21453;&#26144;&#20102;&#31995;&#32479;&#25152;&#20316;&#20986;&#30340;&#20915;&#31574;&#65311;&#26368;&#21518;&#65292;&#20026;&#20160;&#20040;&#22312;&#22810;&#20010;&#35780;&#20215;&#25351;&#26631;&#19979;&#65292;&#39044;&#27979;&#22411;&#26041;&#27861;&#20013;&#32570;&#20047;&#39640;&#39057;&#20449;&#24687;&#23558;&#20135;&#29983;&#26356;&#22909;&#30340;&#21487;&#35299;&#37322;&#24615;&#20998;&#25968;&#65311;&#25105;&#20204;&#20998;&#26512;&#20102;&#19977;&#20010;&#20195;&#34920;&#24615;&#30340;&#35270;&#35273;&#20998;&#31867;&#27169;&#22411;&#30340;&#26799;&#24230;&#65292;&#24182;&#35266;&#23519;&#21040;&#23427;&#21253;&#21547;&#26469;&#33258;&#39640;&#39057;&#30340;&#22122;&#22768;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed an explosion in the development of novel prediction-based attribution methods, which have slowly been supplanting older gradient-based methods to explain the decisions of deep neural networks. However, it is still not clear why prediction-based methods outperform gradient-based ones. Here, we start with an empirical observation: these two approaches yield attribution maps with very different power spectra, with gradient-based methods revealing more high-frequency content than prediction-based methods. This observation raises multiple questions: What is the source of this high-frequency information, and does it truly reflect decisions made by the system? Lastly, why would the absence of high-frequency information in prediction-based methods yield better explainability scores along multiple metrics? We analyze the gradient of three representative visual classification models and observe that it contains noisy information emanating from high-frequencies. Furthe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#26680;&#25216;&#26415;&#65292;&#20855;&#26377;&#35299;&#37322;&#24615;&#19988;&#21442;&#25968;&#36739;&#23569;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#21644;&#20998;&#21106;PPG&#20449;&#21495;&#30340;&#36136;&#37327;&#21644;&#20266;&#24433;&#65292;&#19982;&#29616;&#26377;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#30456;&#27604;&#26377;&#30528;&#31867;&#20284;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.05385</link><description>&lt;p&gt;
&#23398;&#20064;&#26680;&#25216;&#26415;&#29992;&#20110;&#21487;&#35299;&#37322;&#21644;&#39640;&#25928;&#30340;PPG&#20449;&#21495;&#36136;&#37327;&#35780;&#20272;&#21644;&#20266;&#24433;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Learned Kernels for Interpretable and Efficient PPG Signal Quality Assessment and Artifact Segmentation. (arXiv:2307.05385v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05385
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#26680;&#25216;&#26415;&#65292;&#20855;&#26377;&#35299;&#37322;&#24615;&#19988;&#21442;&#25968;&#36739;&#23569;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#21644;&#20998;&#21106;PPG&#20449;&#21495;&#30340;&#36136;&#37327;&#21644;&#20266;&#24433;&#65292;&#19982;&#29616;&#26377;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#30456;&#27604;&#26377;&#30528;&#31867;&#20284;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#30005;&#23481;&#25239;(PPG)&#25552;&#20379;&#20102;&#19968;&#31181;&#20302;&#25104;&#26412;&#12289;&#38750;&#20405;&#20837;&#24615;&#30340;&#26041;&#27861;&#26469;&#25345;&#32493;&#30417;&#27979;&#21508;&#31181;&#24515;&#34880;&#31649;&#21442;&#25968;&#12290;PPG&#20449;&#21495;&#30001;&#21487;&#31359;&#25140;&#35774;&#22791;&#20135;&#29983;&#65292;&#24120;&#24120;&#21253;&#21547;&#30001;&#22806;&#37096;&#22240;&#32032;(&#22914;&#20154;&#20307;&#36816;&#21160;)&#24341;&#36215;&#30340;&#22823;&#22411;&#20266;&#24433;&#12290;&#20026;&#20102;&#30830;&#20445;&#23545;&#29983;&#29702;&#21442;&#25968;&#36827;&#34892;&#31283;&#20581;&#21644;&#20934;&#30830;&#30340;&#25552;&#21462;&#65292;&#20449;&#21495;&#30340;&#25439;&#22351;&#21306;&#22495;&#38656;&#35201;&#34987;&#27491;&#30830;&#22320;&#35782;&#21035;&#21644;&#22788;&#29702;&#12290;&#20043;&#21069;&#30340;&#26041;&#27861;&#20381;&#38752;&#25163;&#24037;&#29305;&#24449;&#26816;&#27979;&#22120;&#25110;&#20449;&#21495;&#24230;&#37327;&#65292;&#32467;&#26524;&#24615;&#33021;&#19981;&#20339;&#65292;&#25110;&#20381;&#38752;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#31561;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65292;&#35745;&#31639;&#21644;&#20869;&#23384;&#23494;&#38598;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23398;&#20064;&#19968;&#23567;&#32452;&#21487;&#35299;&#37322;&#30340;&#21367;&#31215;&#26680;&#65292;&#20854;&#24615;&#33021;&#19982;&#29616;&#26377;&#25216;&#26415;DNN&#26041;&#27861;&#30456;&#20284;&#65292;&#29978;&#33267;&#26356;&#22909;&#65292;&#32780;&#21442;&#25968;&#25968;&#37327;&#27604;DNN&#26041;&#27861;&#23569;&#20960;&#20010;&#25968;&#37327;&#32423;&#12290;&#36825;&#39033;&#24037;&#20316;&#23454;&#29616;&#20102;&#39640;&#25928;&#12289;&#31283;&#20581;&#21644;&#21487;&#35299;&#37322;&#30340;PPG&#20449;&#21495;&#36136;&#37327;&#35780;&#20272;&#21644;&#20266;&#24433;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;
Photoplethysmography (PPG) provides a low-cost, non-invasive method to continuously monitor various cardiovascular parameters. PPG signals are generated by wearable devices and frequently contain large artifacts caused by external factors, such as motion of the human subject. In order to ensure robust and accurate extraction of physiological parameters, corrupted areas of the signal need to be identified and handled appropriately. Previous methodology relied either on handcrafted feature detectors or signal metrics which yield sub-optimal performance, or relied on machine learning techniques such as deep neural networks (DNN) which lack interpretability and are computationally and memory intensive. In this work, we present a novel method to learn a small set of interpretable convolutional kernels that has performance similar to -- and often better than -- the state-of-the-art DNN approach with several orders of magnitude fewer parameters. This work allows for efficient, robust, and int
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;Samplet&#22352;&#26631;&#19979;&#26680;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#24341;&#20837;l1&#27491;&#21017;&#21270;&#39033;&#21487;&#20197;&#22686;&#21152;&#31995;&#25968;&#30340;&#31232;&#30095;&#24615;&#12290;&#30456;&#27604;&#20110;&#21333;&#23610;&#24230;&#22522;&#65292;Samplet&#22522;&#21487;&#20197;&#26356;&#22909;&#22320;&#34920;&#31034;&#26356;&#22810;&#31867;&#22411;&#30340;&#20449;&#21495;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#20351;&#29992;&#36719;&#38408;&#20540;&#21644;&#21322;&#20809;&#28369;&#29275;&#39039;&#27861;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.10180</link><description>&lt;p&gt;
&#22522;&#20110;Samplet&#22522; Pursuit &#30340;&#26680;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Samplet basis pursuit. (arXiv:2306.10180v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10180
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;Samplet&#22352;&#26631;&#19979;&#26680;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#24341;&#20837;l1&#27491;&#21017;&#21270;&#39033;&#21487;&#20197;&#22686;&#21152;&#31995;&#25968;&#30340;&#31232;&#30095;&#24615;&#12290;&#30456;&#27604;&#20110;&#21333;&#23610;&#24230;&#22522;&#65292;Samplet&#22522;&#21487;&#20197;&#26356;&#22909;&#22320;&#34920;&#31034;&#26356;&#22810;&#31867;&#22411;&#30340;&#20449;&#21495;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#20351;&#29992;&#36719;&#38408;&#20540;&#21644;&#21322;&#20809;&#28369;&#29275;&#39039;&#27861;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#22522;&#20110;l1&#27491;&#21017;&#21270;&#30340;Samplet&#22352;&#26631;&#19979;&#30340;&#26680;&#23398;&#20064;&#38382;&#39064;&#12290;&#22312;Samplet&#22522;&#30340;&#31995;&#25968;&#19978;&#65292;&#24212;&#29992;l1&#27491;&#21017;&#21270;&#39033;&#21487;&#20197;&#24378;&#21046;&#22686;&#21152;&#31232;&#30095;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#31216;&#36825;&#31181;&#26041;&#27861;&#20026;Samplet&#22522; Pursuit&#12290;Samplet&#22522;&#26159;&#27874;&#24418;&#31867;&#22411;&#30340;&#26377;&#31526;&#21495;&#27979;&#24230;&#65292;&#19987;&#38376;&#29992;&#20110;&#25955;&#20081;&#25968;&#25454;&#12290;&#23427;&#20204;&#20855;&#26377;&#19982;&#23567;&#27874;&#30456;&#20284;&#30340;&#26412;&#22320;&#21270;&#12289;&#22810;&#20998;&#36776;&#29575;&#20998;&#26512;&#21644;&#25968;&#25454;&#21387;&#32553;&#24615;&#36136;&#12290;&#21487;&#20197;&#22312;Samplet&#22522;&#19978;&#31232;&#30095;&#22320;&#34920;&#31034;&#30340;&#20449;&#21495;&#31867;&#27604;&#21333;&#23610;&#24230;&#22522;&#19978;&#33021;&#22815;&#34920;&#31034;&#31232;&#30095;&#30340;&#20449;&#21495;&#31867;&#21035;&#35201;&#22823;&#24471;&#22810;&#12290;&#29305;&#21035;&#22320;&#65292;&#20165;&#29992;&#22522;&#20989;&#25968;&#26144;&#23556;&#30340;&#20960;&#20010;&#29305;&#24449;&#21472;&#21152;&#21363;&#21487;&#34920;&#31034;&#30340;&#25152;&#26377;&#20449;&#21495;&#20063;&#21487;&#20197;&#22312;Samplet&#22352;&#26631;&#19979;&#23454;&#29616;&#31232;&#30095;&#34920;&#31034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#23558;&#36719;&#38408;&#20540;&#21644;&#21322;&#20809;&#28369;&#29275;&#39039;&#27861;&#30456;&#32467;&#21512;&#65292;&#24182;&#23558;&#35813;&#26041;&#27861;&#19982;&#24555;&#36895;&#36845;&#20195;&#25910;&#32553;&#38408;&#20540;&#31639;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#31232;&#30095;&#24615;&#21644;&#39044;&#27979;&#31934;&#24230;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider kernel-based learning in samplet coordinates with l1-regularization. The application of an l1-regularization term enforces sparsity of the coefficients with respect to the samplet basis. Therefore, we call this approach samplet basis pursuit. Samplets are wavelet-type signed measures, which are tailored to scattered data. They provide similar properties as wavelets in terms of localization, multiresolution analysis, and data compression. The class of signals that can sparsely be represented in a samplet basis is considerably larger than the class of signals which exhibit a sparse representation in the single-scale basis. In particular, every signal that can be represented by the superposition of only a few features of the canonical feature map is also sparse in samplet coordinates. We propose the efficient solution of the problem under consideration by combining soft-shrinkage with the semi-smooth Newton method and compare the approach to the fast iterative shrinkage thresh
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#22411;&#28151;&#21512;CNN-ViT&#31070;&#32463;&#32593;&#32476;FasterViT&#65292;&#24341;&#20837;&#20102;&#20855;&#26377;&#20998;&#23618;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;HAT&#65292;&#23558;&#20840;&#23616;&#33258;&#25105;&#27880;&#24847;&#21147;&#20998;&#35299;&#20026;&#22810;&#32423;&#27880;&#24847;&#21147;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#36328;&#31383;&#21475;&#36890;&#20449;&#12290; FasterViT&#22312;&#31934;&#24230;&#21644;&#22270;&#20687;&#21534;&#21520;&#37327;&#26041;&#38754;&#36798;&#21040;&#20102;SOTA&#21069;&#27839;&#27700;&#24179;&#65292;&#24182;&#24050;&#22312;&#20998;&#31867;&#65292;&#29289;&#20307;&#26816;&#27979;&#21644;&#20998;&#21106;&#31561;CV&#20219;&#21153;&#20013;&#24471;&#21040;&#24191;&#27867;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2306.06189</link><description>&lt;p&gt;
FasterViT&#65306;&#20855;&#26377;&#20998;&#23618;&#27880;&#24847;&#21147;&#30340;&#24555;&#36895;&#35270;&#35273;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
FasterViT: Fast Vision Transformers with Hierarchical Attention. (arXiv:2306.06189v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06189
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#22411;&#28151;&#21512;CNN-ViT&#31070;&#32463;&#32593;&#32476;FasterViT&#65292;&#24341;&#20837;&#20102;&#20855;&#26377;&#20998;&#23618;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;HAT&#65292;&#23558;&#20840;&#23616;&#33258;&#25105;&#27880;&#24847;&#21147;&#20998;&#35299;&#20026;&#22810;&#32423;&#27880;&#24847;&#21147;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#36328;&#31383;&#21475;&#36890;&#20449;&#12290; FasterViT&#22312;&#31934;&#24230;&#21644;&#22270;&#20687;&#21534;&#21520;&#37327;&#26041;&#38754;&#36798;&#21040;&#20102;SOTA&#21069;&#27839;&#27700;&#24179;&#65292;&#24182;&#24050;&#22312;&#20998;&#31867;&#65292;&#29289;&#20307;&#26816;&#27979;&#21644;&#20998;&#21106;&#31561;CV&#20219;&#21153;&#20013;&#24471;&#21040;&#24191;&#27867;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#22411;&#28151;&#21512;CNN-ViT&#31070;&#32463;&#32593;&#32476;&#65292;&#21629;&#21517;&#20026;FasterViT&#65292;&#19987;&#27880;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#24212;&#29992;&#20013;&#30340;&#39640;&#22270;&#20687;&#21534;&#21520;&#37327;&#12290;FasterViT&#23558;CNN&#20013;&#24555;&#36895;&#26412;&#22320;&#34920;&#31034;&#23398;&#20064;&#30340;&#20248;&#28857;&#19982;ViT&#20013;&#30340;&#20840;&#23616;&#24314;&#27169;&#29305;&#24615;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20998;&#23618;&#27880;&#24847;&#21147;&#65288;HAT&#65289;&#26041;&#27861;&#65292;&#23558;&#20855;&#26377;&#20108;&#27425;&#22797;&#26434;&#24230;&#30340;&#20840;&#23616;&#33258;&#25105;&#27880;&#24847;&#21147;&#20998;&#35299;&#20026;&#20855;&#26377;&#36739;&#23567;&#35745;&#31639;&#25104;&#26412;&#30340;&#22810;&#32423;&#27880;&#24847;&#21147;&#12290;&#25105;&#20204;&#21463;&#30410;&#20110;&#39640;&#25928;&#30340;&#22522;&#20110;&#31383;&#21475;&#30340;&#33258;&#25105;&#27880;&#24847;&#21147;&#65292;&#27599;&#20010;&#31383;&#21475;&#37117;&#21487;&#20197;&#35775;&#38382;&#19987;&#38376;&#29992;&#20110;&#26412;&#22320;&#21644;&#20840;&#23616;&#34920;&#31034;&#23398;&#20064;&#30340;&#19987;&#29992;&#36733;&#20307;&#20196;&#29260;&#12290;&#22312;&#39640;&#23618;&#27425;&#19978;&#65292;&#20840;&#23616;&#33258;&#25105;&#27880;&#24847;&#21147;&#23454;&#29616;&#20102;&#36739;&#20302;&#25104;&#26412;&#30340;&#36328;&#31383;&#21475;&#36890;&#20449;&#65292;FasterViT&#22312;&#31934;&#24230;&#19982;&#22270;&#20687;&#21534;&#21520;&#37327;&#26041;&#38754;&#23454;&#29616;&#20102;SOTA&#21069;&#27839;&#27700;&#24179;&#65292;&#24182;&#24050;&#22312;&#21508;&#31181;CV&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#24191;&#27867;&#39564;&#35777;&#65292;&#21253;&#25324;&#20998;&#31867;&#65292;&#29289;&#20307;&#26816;&#27979;&#21644;&#20998;&#21106;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;HAT&#21487;&#20197;&#29992;&#20316;&#29616;&#26377;CNN&#26550;&#26500;&#30340;&#21363;&#25554;&#21363;&#29992;&#27169;&#22359;&#65292;&#20197;&#25913;&#21892;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We design a new family of hybrid CNN-ViT neural networks, named FasterViT, with a focus on high image throughput for computer vision (CV) applications. FasterViT combines the benefits of fast local representation learning in CNNs and global modeling properties in ViT. Our newly introduced Hierarchical Attention (HAT) approach decomposes global self-attention with quadratic complexity into a multi-level attention with reduced computational costs. We benefit from efficient window-based self-attention. Each window has access to dedicated carrier tokens that participate in local and global representation learning. At a high level, global self-attentions enable the efficient cross-window communication at lower costs. FasterViT achieves a SOTA Pareto-front in terms of accuracy \vs image throughput. We have extensively validated its effectiveness on various CV tasks including classification, object detection and segmentation. We also show that HAT can be used as a plug-and-play module for exi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#8212;&#8212;&#23569;&#26679;&#26412;N-&#20803;&#20107;&#23454;&#38142;&#25509;&#39044;&#27979;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;FLEN&#30340;&#27169;&#22411;&#26469;&#23454;&#29616;&#12290;FLEN&#30001;&#19977;&#20010;&#27169;&#22359;&#32452;&#25104;&#65292;&#21487;&#20197;&#20174;&#26377;&#38480;&#30340;&#26631;&#35760;&#23454;&#20363;&#20013;&#39044;&#27979;N-&#20803;&#20107;&#23454;&#20013;&#30340;&#32570;&#22833;&#23454;&#20307;&#12290;</title><link>http://arxiv.org/abs/2305.06104</link><description>&lt;p&gt;
N-&#20803;&#20107;&#23454;&#30340;&#23569;&#26679;&#26412;&#38142;&#25509;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Few-shot Link Prediction on N-ary Facts. (arXiv:2305.06104v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06104
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#8212;&#8212;&#23569;&#26679;&#26412;N-&#20803;&#20107;&#23454;&#38142;&#25509;&#39044;&#27979;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;FLEN&#30340;&#27169;&#22411;&#26469;&#23454;&#29616;&#12290;FLEN&#30001;&#19977;&#20010;&#27169;&#22359;&#32452;&#25104;&#65292;&#21487;&#20197;&#20174;&#26377;&#38480;&#30340;&#26631;&#35760;&#23454;&#20363;&#20013;&#39044;&#27979;N-&#20803;&#20107;&#23454;&#20013;&#30340;&#32570;&#22833;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
N-&#20803;&#20107;&#23454;&#30001;&#20027;&#35201;&#19977;&#20803;&#32452;&#65288;&#22836;&#23454;&#20307;&#12289;&#20851;&#31995;&#12289;&#23614;&#23454;&#20307;&#65289;&#21644;&#20219;&#24847;&#25968;&#37327;&#30340;&#36741;&#21161;&#23646;&#24615;&#20540;&#23545;&#32452;&#25104;&#65292;&#36825;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#30693;&#35782;&#22270;&#35889;&#20013;&#24456;&#24120;&#35265;&#12290;&#23545;&#20110;N-&#20803;&#20107;&#23454;&#30340;&#38142;&#25509;&#39044;&#27979;&#26159;&#39044;&#27979;&#20854;&#20013;&#19968;&#20010;&#20803;&#32032;&#30340;&#32570;&#22833;&#65292;&#22635;&#34917;&#32570;&#22833;&#20803;&#32032;&#26377;&#21161;&#20110;&#20016;&#23500;&#30693;&#35782;&#22270;&#35889;&#24182;&#20419;&#36827;&#35768;&#22810;&#19979;&#28216;&#24212;&#29992;&#31243;&#24207;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#26469;&#29702;&#35299;N-&#20803;&#20107;&#23454;&#20013;&#30340;&#20803;&#32032;&#65292;&#20294;&#36825;&#20123;&#30740;&#31350;&#24573;&#35270;&#20102;&#23569;&#26679;&#26412;&#20851;&#31995;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#22330;&#26223;&#20013;&#21364;&#24456;&#24120;&#35265;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#24341;&#20837;&#19968;&#20010;&#26032;&#20219;&#21153;&#8212;&#8212;&#23569;&#26679;&#26412;N-&#20803;&#20107;&#23454;&#38142;&#25509;&#39044;&#27979;&#65292;&#26088;&#22312;&#20351;&#29992;&#26377;&#38480;&#30340;&#26631;&#35760;&#23454;&#20363;&#26469;&#39044;&#27979;N-&#20803;&#20107;&#23454;&#20013;&#30340;&#32570;&#22833;&#23454;&#20307;&#12290;&#25105;&#20204;&#20063;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;N-&#20803;&#20107;&#23454;&#30340;&#23569;&#26679;&#26412;&#38142;&#25509;&#39044;&#27979;&#27169;&#22411;FLEN&#65292;&#23427;&#30001;&#19977;&#20010;&#27169;&#22359;&#32452;&#25104;&#65306;&#20851;&#31995;&#23398;&#20064;&#27169;&#22359;&#12289;&#25903;&#25345;&#29305;&#23450;&#35843;&#25972;&#27169;&#22359;&#21644;&#26597;&#35810;&#25512;&#29702;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
N-ary facts composed of a primary triple (head entity, relation, tail entity) and an arbitrary number of auxiliary attribute-value pairs, are prevalent in real-world knowledge graphs (KGs). Link prediction on n-ary facts is to predict a missing element in an n-ary fact. This helps populate and enrich KGs and further promotes numerous downstream applications. Previous studies usually require a substantial amount of high-quality data to understand the elements in n-ary facts. However, these studies overlook few-shot relations, which have limited labeled instances, yet are common in real-world scenarios. Thus, this paper introduces a new task, few-shot link prediction on n-ary facts. It aims to predict a missing entity in an n-ary fact with limited labeled instances. We further propose a model for Few-shot Link prEdict on N-ary facts, thus called FLEN, which consists of three modules: the relation learning, support-specific adjusting, and query inference modules. FLEN captures relation me
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#22312;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#22522;&#20110;IDS&#30340;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#65292;&#35813;&#25216;&#26415;&#21033;&#29992;&#22810;&#20010;&#39046;&#22495;&#30340;&#30693;&#35782;&#34701;&#21512;&#21644;/&#25110;&#36866;&#24212;&#20197;&#25552;&#39640;&#30446;&#26631;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#22312;&#30446;&#26631;&#39046;&#22495;&#20013;&#30340;&#26631;&#35760;&#25968;&#25454;&#38750;&#24120;&#23569;&#30340;&#24773;&#20917;&#19979;&#12290;</title><link>http://arxiv.org/abs/2304.10550</link><description>&lt;p&gt;
&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#22312;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Deep Transfer Learning Applications in Intrusion Detection Systems: A Comprehensive Review. (arXiv:2304.10550v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10550
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#22312;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#22522;&#20110;IDS&#30340;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#65292;&#35813;&#25216;&#26415;&#21033;&#29992;&#22810;&#20010;&#39046;&#22495;&#30340;&#30693;&#35782;&#34701;&#21512;&#21644;/&#25110;&#36866;&#24212;&#20197;&#25552;&#39640;&#30446;&#26631;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#22312;&#30446;&#26631;&#39046;&#22495;&#20013;&#30340;&#26631;&#35760;&#25968;&#25454;&#38750;&#24120;&#23569;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#29699;&#33539;&#22260;&#20869;&#65292;&#22806;&#37096;&#20114;&#32852;&#32593;&#36234;&#26469;&#36234;&#22810;&#22320;&#19982;&#24403;&#20195;&#24037;&#19994;&#25511;&#21046;&#31995;&#32479;&#30456;&#36830;&#25509;&#12290;&#22240;&#27492;&#65292;&#26377;&#19968;&#20010;&#36843;&#20999;&#30340;&#38656;&#27714;&#20445;&#25252;&#32593;&#32476;&#20813;&#21463;&#21508;&#31181;&#23041;&#32961;&#12290;&#21487;&#20197;&#20351;&#29992;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65288;IDS&#65289;&#26469;&#20445;&#25252;&#24037;&#19994;&#27963;&#21160;&#30340;&#20851;&#38190;&#22522;&#30784;&#35774;&#26045;&#65292;&#36825;&#26159;&#19968;&#31181;&#39044;&#38450;&#24615;&#25514;&#26045;&#26426;&#21046;&#65292;&#29992;&#20110;&#35782;&#21035;&#26032;&#30340;&#21361;&#38505;&#23041;&#32961;&#21644;&#25932;&#23545;&#27963;&#21160;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#29992;&#20110;&#22312;&#35768;&#22810;&#31181;&#24037;&#19994;&#25511;&#21046;&#32593;&#32476;&#20013;&#21019;&#24314;IDS&#30340;&#26368;&#26032;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#25216;&#26415;&#65292;&#29305;&#21035;&#20391;&#37325;&#20110;&#22522;&#20110;IDS&#30340;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#65288;DTL&#65289;&#12290;DTL&#21487;&#20197;&#30475;&#20316;&#26159;&#23558;&#26469;&#33258;&#22810;&#20010;&#39046;&#22495;&#30340;&#30693;&#35782;&#34701;&#21512;&#21644;/&#25110;&#36866;&#24212;&#20197;&#22686;&#24378;&#30446;&#26631;&#20219;&#21153;&#30340;&#24615;&#33021;&#30340;&#19968;&#31181;&#20449;&#24687;&#34701;&#21512;&#12290;&#37325;&#28857;&#26159;&#24403;&#30446;&#26631;&#22495;&#20013;&#30340;&#26631;&#35760;&#25968;&#25454;&#24456;&#23569;&#26102;&#65292;DTL&#21487;&#20197;&#24110;&#21161;&#25552;&#39640;IDS&#30340;&#24615;&#33021;&#12290;&#32771;&#34385;&#20102;2015&#24180;&#20043;&#21518;&#30340;&#20986;&#29256;&#29289;&#12290;&#36825;&#20123;&#36873;&#23450;&#30340;&#20986;&#29256;&#29289;&#34987;&#20998;&#20026;&#19977;&#31867;&#65306;&#20165;DTL&#21644;&#20165;IDS&#65292;&#20855;&#26377;&#36801;&#31227;&#23398;&#20064;&#65288;TL&#65289;&#30340;IDS&#65292;&#20197;&#21450;&#22522;&#20110;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#30340;IDS&#12290;&#35813;&#30740;&#31350;&#20840;&#38754;&#22238;&#39038;&#20102;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#20013;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#24212;&#29992;&#30340;&#26368;&#26032;&#25216;&#26415;&#21644;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Globally, the external Internet is increasingly being connected to the contemporary industrial control system. As a result, there is an immediate need to protect the network from several threats. The key infrastructure of industrial activity may be protected from harm by using an intrusion detection system (IDS), a preventive measure mechanism, to recognize new kinds of dangerous threats and hostile activities. The most recent artificial intelligence (AI) techniques used to create IDS in many kinds of industrial control networks are examined in this study, with a particular emphasis on IDS-based deep transfer learning (DTL). This latter can be seen as a type of information fusion that merge, and/or adapt knowledge from multiple domains to enhance the performance of the target task, particularly when the labeled data in the target domain is scarce. Publications issued after 2015 were taken into account. These selected publications were divided into three categories: DTL-only and IDS-onl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;Python&#30340;&#25945;&#31243;&#65292;&#20171;&#32461;&#20102;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;MCMC&#26041;&#27861;&#24212;&#29992;&#65292;&#36890;&#36807;&#25945;&#31243;&#20351;&#24471;&#28145;&#24230;&#23398;&#20064;&#24320;&#21457;&#32773;&#33021;&#22815;&#26356;&#22909;&#22320;&#24212;&#29992;&#36125;&#21494;&#26031;&#25512;&#26029;&#36827;&#34892;&#21442;&#25968;&#20272;&#35745;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;</title><link>http://arxiv.org/abs/2304.02595</link><description>&lt;p&gt;
&#22522;&#20110;MCMC&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65306;&#22522;&#20110;Python&#30340;&#25945;&#31243;
&lt;/p&gt;
&lt;p&gt;
Bayesian neural networks via MCMC: a Python-based tutorial. (arXiv:2304.02595v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02595
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;Python&#30340;&#25945;&#31243;&#65292;&#20171;&#32461;&#20102;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;MCMC&#26041;&#27861;&#24212;&#29992;&#65292;&#36890;&#36807;&#25945;&#31243;&#20351;&#24471;&#28145;&#24230;&#23398;&#20064;&#24320;&#21457;&#32773;&#33021;&#22815;&#26356;&#22909;&#22320;&#24212;&#29992;&#36125;&#21494;&#26031;&#25512;&#26029;&#36827;&#34892;&#21442;&#25968;&#20272;&#35745;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#25512;&#26029;&#20026;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#25552;&#20379;&#20102;&#21442;&#25968;&#20272;&#35745;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#26041;&#27861;&#12290;&#21464;&#20998;&#25512;&#26029;&#21644;&#39532;&#23572;&#31185;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#65288;MCMC&#65289;&#37319;&#26679;&#25216;&#26415;&#29992;&#20110;&#23454;&#29616;&#36125;&#21494;&#26031;&#25512;&#26029;&#12290;&#22312;&#36807;&#21435;&#19977;&#21313;&#24180;&#20013;&#65292;MCMC&#26041;&#27861;&#22312;&#36866;&#24212;&#26356;&#22823;&#30340;&#27169;&#22411;&#65288;&#22914;&#28145;&#24230;&#23398;&#20064;&#65289;&#21644;&#22823;&#25968;&#25454;&#38382;&#39064;&#26041;&#38754;&#38754;&#20020;&#20102;&#35768;&#22810;&#25361;&#25112;&#12290;&#21253;&#25324;&#26799;&#24230;&#30340;&#39640;&#32423;&#25552;&#35758;&#65288;&#20363;&#22914;Langevin&#25552;&#35758;&#20998;&#24067;&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;MCMC&#37319;&#26679;&#20013;&#30340;&#19968;&#20123;&#38480;&#21046;&#30340;&#26041;&#27861;&#65292;&#27492;&#22806;&#65292;MCMC&#26041;&#27861;&#36890;&#24120;&#34987;&#38480;&#21046;&#22312;&#32479;&#35745;&#23398;&#23478;&#30340;&#20351;&#29992;&#33539;&#22260;&#20869;&#65292;&#24182;&#19988;&#20173;&#19981;&#26159;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#20154;&#21592;&#30340;&#20027;&#27969;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;MCMC&#26041;&#27861;&#30340;&#25945;&#31243;&#65292;&#28085;&#30422;&#20102;&#31616;&#21333;&#30340;&#36125;&#21494;&#26031;&#32447;&#24615;&#21644;&#36923;&#36753;&#27169;&#22411;&#65292;&#20197;&#21450;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#12290;&#36825;&#20010;&#25945;&#31243;&#30340;&#30446;&#30340;&#26159;&#36890;&#36807;&#32534;&#30721;&#26469;&#24357;&#21512;&#29702;&#35770;&#21644;&#23454;&#29616;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#37492;&#20110;&#24403;&#21069;MCMC&#26041;&#27861;&#30340;&#26222;&#21450;&#31243;&#24230;&#20173;&#28982;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian inference provides a methodology for parameter estimation and uncertainty quantification in machine learning and deep learning methods. Variational inference and Markov Chain Monte-Carlo (MCMC) sampling techniques are used to implement Bayesian inference. In the past three decades, MCMC methods have faced a number of challenges in being adapted to larger models (such as in deep learning) and big data problems. Advanced proposals that incorporate gradients, such as a Langevin proposal distribution, provide a means to address some of the limitations of MCMC sampling for Bayesian neural networks. Furthermore, MCMC methods have typically been constrained to use by statisticians and are still not prominent among deep learning researchers. We present a tutorial for MCMC methods that covers simple Bayesian linear and logistic models, and Bayesian neural networks. The aim of this tutorial is to bridge the gap between theory and implementation via coding, given a general sparsity of li
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#32456;&#36523;&#24322;&#24120;&#26816;&#27979;&#30340;&#37325;&#35201;&#24615;&#65292;&#25552;&#20986;&#35774;&#35745;&#32456;&#36523;&#23398;&#20064;&#22797;&#26434;&#24615;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#30340;&#25361;&#25112;&#21644;&#26426;&#20250;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#22330;&#26223;&#29983;&#25104;&#36807;&#31243;&#20351;&#24471;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#36827;&#34892;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2303.07557</link><description>&lt;p&gt;
&#32456;&#36523;&#23398;&#20064;&#22312;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;: &#26032;&#30340;&#25361;&#25112;&#12289;&#35270;&#35282;&#21644;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;
Lifelong Learning for Anomaly Detection: New Challenges, Perspectives, and Insights. (arXiv:2303.07557v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07557
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#32456;&#36523;&#24322;&#24120;&#26816;&#27979;&#30340;&#37325;&#35201;&#24615;&#65292;&#25552;&#20986;&#35774;&#35745;&#32456;&#36523;&#23398;&#20064;&#22797;&#26434;&#24615;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#30340;&#25361;&#25112;&#21644;&#26426;&#20250;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#22330;&#26223;&#29983;&#25104;&#36807;&#31243;&#20351;&#24471;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#36827;&#34892;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#23454;&#38469;&#39046;&#22495;&#20013;&#65292;&#24322;&#24120;&#26816;&#27979;&#20855;&#26377;&#26497;&#20854;&#37325;&#35201;&#30340;&#24847;&#20041;&#65292;&#29305;&#21035;&#26159;&#22312;&#34892;&#20026;&#19981;&#26029;&#21464;&#21270;&#30340;&#24773;&#20917;&#19979;&#12290;&#32456;&#36523;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#20852;&#36235;&#21183;&#65292;&#23427;&#33021;&#22815;&#28385;&#36275;&#38656;&#35201;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#19981;&#26029;&#36866;&#24212;&#26032;&#25361;&#25112;&#24182;&#20445;&#30041;&#36807;&#21435;&#30693;&#35782;&#30340;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#24456;&#23569;&#26377;&#20154;&#33268;&#21147;&#20110;&#24314;&#31435;&#32456;&#36523;&#24322;&#24120;&#26816;&#27979;&#30340;&#22522;&#30784;&#65292;&#36825;&#19982;&#26356;&#24191;&#27867;&#25506;&#32034;&#30340;&#20998;&#31867;&#35774;&#32622;&#23384;&#22312;&#26412;&#36136;&#19981;&#21516;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#36890;&#36807;&#25506;&#35752;&#12289;&#38416;&#36848;&#21644;&#35752;&#35770;&#32456;&#36523;&#24322;&#24120;&#26816;&#27979;&#65292;&#35797;&#22270;&#20026;&#20854;&#26356;&#24191;&#27867;&#30340;&#37319;&#29992;&#24314;&#31435;&#22522;&#30784;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#32456;&#36523;&#24322;&#24120;&#26816;&#27979;&#24456;&#37325;&#35201;&#65292;&#23450;&#20041;&#20102;&#24212;&#23545;&#32456;&#36523;&#23398;&#20064;&#22797;&#26434;&#24615;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#35774;&#35745;&#30340;&#25361;&#25112;&#21644;&#26426;&#20250;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23545;&#23398;&#20064;&#35774;&#32622;&#21644;&#22330;&#26223;&#29983;&#25104;&#36807;&#31243;&#36827;&#34892;&#20102;&#34920;&#24449;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#20351;&#29992;&#36825;&#20123;&#24037;&#20855;&#36827;&#34892;&#32456;&#36523;&#24322;&#24120;&#26816;&#27979;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection is of paramount importance in many real-world domains, characterized by evolving behavior. Lifelong learning represents an emerging trend, answering the need for machine learning models that continuously adapt to new challenges in dynamic environments while retaining past knowledge. However, limited efforts are dedicated to building foundations for lifelong anomaly detection, which provides intrinsically different challenges compared to the more widely explored classification setting. In this paper, we face this issue by exploring, motivating, and discussing lifelong anomaly detection, trying to build foundations for its wider adoption. First, we explain why lifelong anomaly detection is relevant, defining challenges and opportunities to design anomaly detection methods that deal with lifelong learning complexities. Second, we characterize learning settings and a scenario generation procedure that enables researchers to experiment with lifelong anomaly detection using
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20960;&#20046;&#32447;&#24615;&#26102;&#38388;&#20869;&#29992;&#40065;&#26834;&#20132;&#26367;&#26368;&#23567;&#21270;&#26041;&#27861;&#23436;&#25104;&#20302;&#31209;&#30697;&#38453;&#34917;&#20840;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#35266;&#23519;&#20960;&#20046;&#32447;&#24615;&#25968;&#37327;&#30340;&#26465;&#30446;&#21363;&#21487;&#24674;&#22797;&#30697;&#38453;$M$&#65292;&#27492;&#26041;&#27861;&#20811;&#26381;&#20102;&#20132;&#26367;&#26368;&#23567;&#21270;&#26041;&#27861;&#38656;&#35201;&#31934;&#30830;&#35745;&#31639;&#30340;&#38480;&#21046;&#65292;&#26356;&#31526;&#21512;&#23454;&#38469;&#23454;&#29616;&#20013;&#23545;&#25928;&#29575;&#30340;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2302.11068</link><description>&lt;p&gt;
&#29992;&#40065;&#26834;&#20132;&#26367;&#26368;&#23567;&#21270;&#26041;&#27861;&#22312;&#20960;&#20046;&#32447;&#24615;&#26102;&#38388;&#20869;&#23436;&#25104;&#20302;&#31209;&#30697;&#38453;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
Low Rank Matrix Completion via Robust Alternating Minimization in Nearly Linear Time. (arXiv:2302.11068v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11068
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20960;&#20046;&#32447;&#24615;&#26102;&#38388;&#20869;&#29992;&#40065;&#26834;&#20132;&#26367;&#26368;&#23567;&#21270;&#26041;&#27861;&#23436;&#25104;&#20302;&#31209;&#30697;&#38453;&#34917;&#20840;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#35266;&#23519;&#20960;&#20046;&#32447;&#24615;&#25968;&#37327;&#30340;&#26465;&#30446;&#21363;&#21487;&#24674;&#22797;&#30697;&#38453;$M$&#65292;&#27492;&#26041;&#27861;&#20811;&#26381;&#20102;&#20132;&#26367;&#26368;&#23567;&#21270;&#26041;&#27861;&#38656;&#35201;&#31934;&#30830;&#35745;&#31639;&#30340;&#38480;&#21046;&#65292;&#26356;&#31526;&#21512;&#23454;&#38469;&#23454;&#29616;&#20013;&#23545;&#25928;&#29575;&#30340;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#23450;&#19968;&#20010;&#30697;&#38453;$M\in \mathbb{R}^{m\times n}$&#65292;&#20302;&#31209;&#30697;&#38453;&#34917;&#20840;&#38382;&#39064;&#35201;&#27714;&#25105;&#20204;&#36890;&#36807;&#21482;&#35266;&#23519;&#19968;&#32452;&#25351;&#23450;&#30340;&#26465;&#30446;$\Omega\subseteq [m]\times [n]$&#26469;&#25214;&#21040;$M$&#30340;&#31209;&#20026;$k$&#30340;&#36817;&#20284;$UV^\top$&#65292;&#20854;&#20013;$U\in \mathbb{R}^{m\times k}$&#65292;$V\in \mathbb{R}^{n\times k}$&#12290;&#26412;&#25991;&#20027;&#35201;&#30740;&#31350;&#20102;&#19968;&#31181;&#34987;&#24191;&#27867;&#20351;&#29992;&#30340;&#26041;&#27861;--&#20132;&#26367;&#26368;&#23567;&#21270;&#26694;&#26550;&#12290;Jain&#12289;Netrapalli&#21644;Sanghavi~\cite{jns13}&#35777;&#26126;&#20102;&#22914;&#26524;$M$&#30340;&#34892;&#21644;&#21015;&#26159;&#19981;&#30456;&#24178;&#30340;&#65292;&#37027;&#20040;&#20132;&#26367;&#26368;&#23567;&#21270;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#35266;&#23519;&#20960;&#20046;&#32447;&#24615;&#25968;&#37327;&#30340;&#26465;&#30446;&#21487;&#38752;&#22320;&#24674;&#22797;&#30697;&#38453;$M$&#12290;&#34429;&#28982;&#26679;&#26412;&#22797;&#26434;&#24230;&#20043;&#21518;&#34987;&#25913;&#36827;~\cite{glz17}&#65292;&#20294;&#20132;&#26367;&#26368;&#23567;&#21270;&#27493;&#39588;&#35201;&#27714;&#31934;&#30830;&#35745;&#31639;&#12290;&#36825;&#38459;&#30861;&#20102;&#26356;&#39640;&#25928;&#31639;&#27861;&#30340;&#24320;&#21457;&#65292;&#24182;&#26410;&#25551;&#36848;&#20132;&#26367;&#26368;&#23567;&#21270;&#30340;&#23454;&#38469;&#23454;&#29616;&#65292;&#20854;&#20013;&#26356;&#26032;&#36890;&#24120;&#26159;&#36817;&#20284;&#25191;&#34892;&#65292;&#20197;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given a matrix $M\in \mathbb{R}^{m\times n}$, the low rank matrix completion problem asks us to find a rank-$k$ approximation of $M$ as $UV^\top$ for $U\in \mathbb{R}^{m\times k}$ and $V\in \mathbb{R}^{n\times k}$ by only observing a few entries specified by a set of entries $\Omega\subseteq [m]\times [n]$. In particular, we examine an approach that is widely used in practice -- the alternating minimization framework. Jain, Netrapalli and Sanghavi~\cite{jns13} showed that if $M$ has incoherent rows and columns, then alternating minimization provably recovers the matrix $M$ by observing a nearly linear in $n$ number of entries. While the sample complexity has been subsequently improved~\cite{glz17}, alternating minimization steps are required to be computed exactly. This hinders the development of more efficient algorithms and fails to depict the practical implementation of alternating minimization, where the updates are usually performed approximately in favor of efficiency.  In this p
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;hGAIL&#30340;&#26550;&#26500;&#65292;&#29992;&#20110;&#35299;&#20915;&#36710;&#36742;&#30340;&#33258;&#20027;&#23548;&#33322;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#24863;&#30693;&#20449;&#24687;&#30452;&#25509;&#26144;&#23556;&#21040;&#20302;&#32423;&#21160;&#20316;&#30340;&#21516;&#26102;&#65292;&#23398;&#20064;&#36710;&#36742;&#29615;&#22659;&#30340;&#20013;&#32423;&#36755;&#20837;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2302.04823</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#23618;&#29983;&#25104;&#23545;&#25239;&#27169;&#25311;&#23398;&#20064;&#30340;&#33258;&#21160;&#39550;&#39542;&#22312;&#22478;&#24066;&#29615;&#22659;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Generative Adversarial Imitation Learning with Mid-level Input Generation for Autonomous Driving on Urban Environments. (arXiv:2302.04823v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04823
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;hGAIL&#30340;&#26550;&#26500;&#65292;&#29992;&#20110;&#35299;&#20915;&#36710;&#36742;&#30340;&#33258;&#20027;&#23548;&#33322;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#24863;&#30693;&#20449;&#24687;&#30452;&#25509;&#26144;&#23556;&#21040;&#20302;&#32423;&#21160;&#20316;&#30340;&#21516;&#26102;&#65292;&#23398;&#20064;&#36710;&#36742;&#29615;&#22659;&#30340;&#20013;&#32423;&#36755;&#20837;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#29616;&#23454;&#20013;&#30340;&#22478;&#24066;&#23548;&#33322;&#22330;&#26223;&#65292;&#35774;&#35745;&#20581;&#22766;&#30340;&#25511;&#21046;&#31574;&#30053;&#24182;&#19981;&#26159;&#19968;&#39033;&#31616;&#21333;&#30340;&#20219;&#21153;&#12290;&#22312;&#31471;&#21040;&#31471;&#30340;&#26041;&#27861;&#20013;&#65292;&#36825;&#20123;&#31574;&#30053;&#24517;&#39035;&#23558;&#36710;&#36742;&#25668;&#20687;&#22836;&#33719;&#24471;&#30340;&#39640;&#32500;&#22270;&#20687;&#26144;&#23556;&#21040;&#20302;&#32423;&#21160;&#20316;&#65292;&#22914;&#36716;&#21521;&#21644;&#27833;&#38376;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;hGAIL&#30340;&#26550;&#26500;&#65292;&#29992;&#20110;&#35299;&#20915;&#36710;&#36742;&#30340;&#33258;&#20027;&#23548;&#33322;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#24863;&#30693;&#20449;&#24687;&#30452;&#25509;&#26144;&#23556;&#21040;&#20302;&#32423;&#21160;&#20316;&#30340;&#21516;&#26102;&#65292;&#23398;&#20064;&#36710;&#36742;&#29615;&#22659;&#30340;&#20013;&#32423;&#36755;&#20837;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deriving robust control policies for realistic urban navigation scenarios is not a trivial task. In an end-to-end approach, these policies must map high-dimensional images from the vehicle's cameras to low-level actions such as steering and throttle. While pure Reinforcement Learning (RL) approaches are based exclusively on rewards,Generative Adversarial Imitation Learning (GAIL) agents learn from expert demonstrations while interacting with the environment, which favors GAIL on tasks for which a reward signal is difficult to derive. In this work, the hGAIL architecture was proposed to solve the autonomous navigation of a vehicle in an end-to-end approach, mapping sensory perceptions directly to low-level actions, while simultaneously learning mid-level input representations of the agent's environment. The proposed hGAIL consists of an hierarchical Adversarial Imitation Learning architecture composed of two main modules: the GAN (Generative Adversarial Nets) which generates the Bird's-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20256;&#32479;&#37319;&#26679;&#26041;&#27861;&#20272;&#35745;&#37327;&#23376;&#29627;&#33394;&#31995;&#32479;&#25130;&#26029;&#35823;&#24046;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#20272;&#35745;&#23454;&#38469;&#37327;&#23376;&#27169;&#25311;&#29627;&#33394;&#29702;&#35770;&#25152;&#38656;&#30340;&#36164;&#28304;&#65292;&#24182;&#26816;&#26597;&#23545;&#24212;&#37327;&#23376;&#27169;&#25311;&#30340;&#32467;&#26524;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.08546</link><description>&lt;p&gt;
&#20351;&#29992;&#37319;&#26679;&#31639;&#27861;&#20272;&#35745;&#37327;&#23376;&#29627;&#33394;&#31995;&#32479;&#30340;&#25130;&#26029;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
Estimating truncation effects of quantum bosonic systems using sampling algorithms. (arXiv:2212.08546v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08546
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20256;&#32479;&#37319;&#26679;&#26041;&#27861;&#20272;&#35745;&#37327;&#23376;&#29627;&#33394;&#31995;&#32479;&#25130;&#26029;&#35823;&#24046;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#20272;&#35745;&#23454;&#38469;&#37327;&#23376;&#27169;&#25311;&#29627;&#33394;&#29702;&#35770;&#25152;&#38656;&#30340;&#36164;&#28304;&#65292;&#24182;&#26816;&#26597;&#23545;&#24212;&#37327;&#23376;&#27169;&#25311;&#30340;&#32467;&#26524;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35201;&#22312;&#22522;&#20110;&#37327;&#23376;&#27604;&#29305;&#25110;&#37327;&#23376;&#20301;&#30340;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#27169;&#25311;&#29627;&#33394;&#23376;&#65292;&#24517;&#39035;&#36890;&#36807;&#23558;&#26080;&#38480;&#32500;&#23616;&#37096;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#25130;&#26029;&#20026;&#26377;&#38480;&#32500;&#26469;&#35268;&#33539;&#29702;&#35770;&#12290;&#22312;&#23547;&#27714;&#23454;&#38469;&#37327;&#23376;&#24212;&#29992;&#30340;&#36807;&#31243;&#20013;&#65292;&#20102;&#35299;&#25130;&#26029;&#35823;&#24046;&#26377;&#22810;&#22823;&#38750;&#24120;&#37325;&#35201;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#38500;&#38750;&#25105;&#20204;&#25317;&#26377;&#22909;&#30340;&#37327;&#23376;&#35745;&#31639;&#26426;&#65292;&#21542;&#21017;&#24456;&#38590;&#20272;&#35745;&#35823;&#24046;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#20256;&#32479;&#30340;&#32463;&#20856;&#35774;&#22791;&#37319;&#26679;&#26041;&#27861;&#65292;&#20855;&#20307;&#32780;&#35328;&#26159;&#39532;&#23572;&#31185;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#26041;&#27861;&#21487;&#20197;&#29992;&#29616;&#26377;&#21512;&#29702;&#30340;&#35745;&#31639;&#36164;&#28304;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#20197;&#20108;&#32500;&#26684;&#28857;&#19978;&#30340;&#26631;&#37327;&#22330;&#29702;&#35770;&#20026;&#20363;&#28436;&#31034;&#20102;&#36825;&#20010;&#24819;&#27861;&#65292;&#20854;&#22823;&#23567;&#36229;&#36807;&#20102;&#20351;&#29992;&#30830;&#20999;&#23545;&#35282;&#21270;&#26041;&#27861;&#25152;&#33021;&#36798;&#21040;&#30340;&#33539;&#22260;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#29992;&#20110;&#20272;&#35745;&#23454;&#38469;&#37327;&#23376;&#27169;&#25311;&#29627;&#33394;&#29702;&#35770;&#25152;&#38656;&#30340;&#36164;&#28304;&#65292;&#24182;&#26816;&#26597;&#23545;&#24212;&#37327;&#23376;&#27169;&#25311;&#30340;&#32467;&#26524;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
To simulate bosons on a qubit- or qudit-based quantum computer, one has to regularize the theory by truncating infinite-dimensional local Hilbert spaces to finite dimensions. In the search for practical quantum applications, it is important to know how big the truncation errors can be. In general, it is not easy to estimate errors unless we have a good quantum computer. In this paper we show that traditional sampling methods on classical devices, specifically Markov Chain Monte Carlo, can address this issue with a reasonable amount of computational resources available today. As a demonstration, we apply this idea to the scalar field theory on a two-dimensional lattice, with a size that goes beyond what is achievable using exact diagonalization methods. This method can be used to estimate the resources needed for realistic quantum simulations of bosonic theories, and also, to check the validity of the results of the corresponding quantum simulations.
&lt;/p&gt;</description></item><item><title>&#23545;&#27604;&#21487;&#20449;&#24230;&#20256;&#25773;&#37319;&#29992;&#36845;&#20195;&#30340;&#20256;&#23548;&#24335;&#20266;&#26631;&#31614;&#32454;&#21270;&#65292;&#23558;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#22024;&#26434;&#26631;&#31614;&#23398;&#20064;&#32479;&#19968;&#65292;&#21487;&#22312;&#21508;&#31181;&#25968;&#25454;&#22330;&#26223;&#20013;&#21487;&#38752;&#22320;&#36229;&#36807;&#26377;&#30417;&#30563;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2211.09929</link><description>&lt;p&gt;
&#21487;&#38752;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#23545;&#27604;&#21487;&#20449;&#24230;&#20256;&#25773;
&lt;/p&gt;
&lt;p&gt;
Contrastive Credibility Propagation for Reliable Semi-Supervised Learning. (arXiv:2211.09929v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09929
&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#21487;&#20449;&#24230;&#20256;&#25773;&#37319;&#29992;&#36845;&#20195;&#30340;&#20256;&#23548;&#24335;&#20266;&#26631;&#31614;&#32454;&#21270;&#65292;&#23558;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#22024;&#26434;&#26631;&#31614;&#23398;&#20064;&#32479;&#19968;&#65292;&#21487;&#22312;&#21508;&#31181;&#25968;&#25454;&#22330;&#26223;&#20013;&#21487;&#38752;&#22320;&#36229;&#36807;&#26377;&#30417;&#30563;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#26631;&#31614;&#23481;&#26131;&#20986;&#38169;&#65292;&#36825;&#20351;&#24471;&#21322;&#30417;&#30563;&#23398;&#20064;(Semi-Supervised Learning, SSL)&#21464;&#24471;&#22256;&#38590;&#12290;&#36890;&#24120;&#65292;&#25105;&#20204;&#24456;&#23569;&#20102;&#35299;&#31639;&#27861;&#20309;&#26102;&#20197;&#21450;&#20026;&#20160;&#20040;&#26080;&#27861;&#36229;&#36807;&#26377;&#30417;&#30563;&#22522;&#20934;&#12290;&#25105;&#20204;&#20351;&#29992;&#22522;&#20934;&#25968;&#25454;&#38598;&#26500;&#24314;&#20102;&#20116;&#31181;&#24120;&#35265;&#30340;&#30495;&#23454;&#19990;&#30028;&#21322;&#30417;&#30563;&#23398;&#20064;&#25968;&#25454;&#22330;&#26223;&#65306;&#23569;&#26631;&#31614;&#26679;&#26412;&#12289;&#24320;&#25918;&#22495;&#26679;&#26412;&#12289;&#22024;&#26434;&#26631;&#31614;&#12289;&#26377;&#26631;&#31614;&#21644;&#26080;&#26631;&#31614;&#38598;&#21512;&#20013;&#30340;&#31867;&#21035;&#20998;&#24067;&#19981;&#22343;&#34913;/&#38169;&#20301;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23545;&#27604;&#21487;&#20449;&#24230;&#20256;&#25773;(Contrastive Credibility Propagation, CCP)&#30340;&#26032;&#31639;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#30340;&#20256;&#23548;&#24335;&#20266;&#26631;&#31614;&#32454;&#21270;&#23454;&#29616;&#28145;&#24230;&#21322;&#30417;&#30563;&#23398;&#20064;&#12290;CCP&#23558;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#22024;&#26434;&#26631;&#31614;&#23398;&#20064;&#32479;&#19968;&#22312;&#19968;&#36215;&#65292;&#30446;&#30340;&#26159;&#22312;&#20219;&#20309;&#25968;&#25454;&#22330;&#26223;&#20013;&#21487;&#38752;&#22320;&#36229;&#36807;&#26377;&#30417;&#30563;&#22522;&#20934;&#12290;&#19982;&#19987;&#27880;&#20110;&#23376;&#38598;&#22330;&#26223;&#30340;&#20043;&#21069;&#26041;&#27861;&#30456;&#27604;&#65292;CCP&#22312;&#25152;&#26377;&#22330;&#26223;&#20013;&#29420;&#29305;&#22320;&#36229;&#36807;&#20102;&#26377;&#30417;&#30563;&#22522;&#20934;&#65292;&#25903;&#25345;&#22312;&#26631;&#27880;&#25968;&#25454;&#25110;&#26080;&#26631;&#31614;&#25968;&#25454;&#36136;&#37327;&#26410;&#30693;&#30340;&#24773;&#20917;&#19979;&#30340;&#23454;&#36341;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Producing labels for unlabeled data is error-prone, making semi-supervised learning (SSL) troublesome. Often, little is known about when and why an algorithm fails to outperform a supervised baseline. Using benchmark datasets, we craft five common real-world SSL data scenarios: few-label, open-set, noisy-label, and class distribution imbalance/misalignment in the labeled and unlabeled sets. We propose a novel algorithm called Contrastive Credibility Propagation (CCP) for deep SSL via iterative transductive pseudo-label refinement. CCP unifies semi-supervised learning and noisy label learning for the goal of reliably outperforming a supervised baseline in any data scenario. Compared to prior methods which focus on a subset of scenarios, CCP uniquely outperforms the supervised baseline in all scenarios, supporting practitioners when the qualities of labeled or unlabeled data are unknown.
&lt;/p&gt;</description></item><item><title>MAgNET&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#22522;&#20110;MAg&#65288;&#22810;&#36890;&#36947;&#32858;&#21512;&#65289;&#25805;&#20316;&#65292;&#37319;&#29992;&#22270;&#24418;U-Net&#26550;&#26500;&#22788;&#29702;&#20219;&#24847;&#32467;&#26500;&#65288;&#22270;&#24418;&#25968;&#25454;&#65289;&#30340;&#22823;&#32500;&#25968;&#25454;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#22788;&#29702;&#20219;&#24847;&#22797;&#26434;&#30340;&#32593;&#26684;&#12290;</title><link>http://arxiv.org/abs/2211.00713</link><description>&lt;p&gt;
MAgNET: &#38754;&#21521;&#22522;&#20110;&#32593;&#26684;&#27169;&#25311;&#30340;&#22270;&#24418;U-Net&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
MAgNET: A Graph U-Net Architecture for Mesh-Based Simulations. (arXiv:2211.00713v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.00713
&lt;/p&gt;
&lt;p&gt;
MAgNET&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#22522;&#20110;MAg&#65288;&#22810;&#36890;&#36947;&#32858;&#21512;&#65289;&#25805;&#20316;&#65292;&#37319;&#29992;&#22270;&#24418;U-Net&#26550;&#26500;&#22788;&#29702;&#20219;&#24847;&#32467;&#26500;&#65288;&#22270;&#24418;&#25968;&#25454;&#65289;&#30340;&#22823;&#32500;&#25968;&#25454;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#22788;&#29702;&#20219;&#24847;&#22797;&#26434;&#30340;&#32593;&#26684;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#23574;&#31471;&#24212;&#29992;&#20013;&#65292;&#39640;&#20445;&#30495;&#35745;&#31639;&#27169;&#22411;&#34987;&#35777;&#26126;&#36895;&#24230;&#22826;&#24930;&#32780;&#19981;&#20999;&#23454;&#38469;&#65292;&#22240;&#27492;&#34987;&#26356;&#24555;&#30340;&#20195;&#29702;&#27169;&#22411;&#25152;&#26367;&#20195;&#12290;&#26368;&#36817;&#65292;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#21152;&#36895;&#36825;&#31181;&#39044;&#27979;&#26041;&#38754;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#24403;&#38754;&#23545;&#26356;&#22823;&#26356;&#22797;&#26434;&#30340;&#38382;&#39064;&#26102;&#65292;&#23427;&#20204;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;MAgNET&#65306;&#22810;&#36890;&#36947;&#32858;&#21512;&#32593;&#32476;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#22788;&#29702;&#20219;&#24847;&#32467;&#26500;&#65288;&#22270;&#24418;&#25968;&#25454;&#65289;&#30340;&#22823;&#32500;&#25968;&#25454;&#12290;MAgNET&#24314;&#31435;&#22312;MAg&#65288;&#22810;&#36890;&#36947;&#32858;&#21512;&#65289;&#25805;&#20316;&#20043;&#19978;&#65292;&#23427;&#23558;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#22810;&#36890;&#36947;&#26412;&#22320;&#25805;&#20316;&#30340;&#27010;&#24565;&#25512;&#24191;&#21040;&#20219;&#24847;&#38750;&#32593;&#26684;&#36755;&#20837;&#12290;MAg&#23618;&#19982;&#25152;&#25552;&#20986;&#30340;&#26032;&#22411;&#22270;&#24418;&#27744;&#21270;/&#21453;&#27744;&#21270;&#25805;&#20316;&#20132;&#38169;&#65292;&#24418;&#25104;&#20102;&#19968;&#20010;&#22270;&#24418;U-Net&#26550;&#26500;&#65292;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#22788;&#29702;&#20219;&#24847;&#22797;&#26434;&#30340;&#32593;&#26684;&#65292;&#24182;&#23545;&#22823;&#32500;&#22270;&#24418;&#32467;&#26500;&#25968;&#25454;&#36827;&#34892;&#30417;&#30563;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many cutting-edge applications, high-fidelity computational models prove too slow to be practical and are thus replaced by much faster surrogate models. Recently, deep learning techniques have become increasingly important in accelerating such predictions. However, they tend to falter when faced with larger and more complex problems. Therefore, this work introduces MAgNET: Multi-channel Aggregation Network, a novel geometric deep learning framework designed to operate on large-dimensional data of arbitrary structure (graph data). MAgNET is built upon the MAg (Multichannel Aggregation) operation, which generalizes the concept of multi-channel local operations in convolutional neural networks to arbitrary non-grid inputs. The MAg layers are interleaved with the proposed novel graph pooling/unpooling operations to form a graph U-Net architecture that is robust and can handle arbitrary complex meshes, efficiently performing supervised learning on large-dimensional graph-structured data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#31867;&#20998;&#31867;&#22120;&#21644;&#24191;&#20041;&#20284;&#28982;&#27604;&#26816;&#39564;&#30340;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#22810;&#23618;&#24863;&#30693;&#22120;&#31070;&#32463;&#32593;&#32476;&#21644;&#25903;&#25345;&#21521;&#37327;&#26426;&#27169;&#22411;&#22312;&#25910;&#25947;&#26102;&#20250;&#34920;&#29616;&#20026;&#24191;&#20041;&#20284;&#28982;&#27604;&#26816;&#39564;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#23637;&#31034;&#20102;&#19968;&#31867;&#26368;&#23567;&#20108;&#20056;SVM&#22312;&#25910;&#25947;&#26102;&#20063;&#33021;&#36798;&#21040;&#24191;&#20041;&#20284;&#28982;&#27604;&#26816;&#39564;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2210.12494</link><description>&lt;p&gt;
&#20851;&#20110;&#24191;&#20041;&#20284;&#28982;&#27604;&#26816;&#39564;&#21644;&#19968;&#31867;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
On the Generalized Likelihood Ratio Test and One-Class Classifiers. (arXiv:2210.12494v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.12494
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#31867;&#20998;&#31867;&#22120;&#21644;&#24191;&#20041;&#20284;&#28982;&#27604;&#26816;&#39564;&#30340;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#22810;&#23618;&#24863;&#30693;&#22120;&#31070;&#32463;&#32593;&#32476;&#21644;&#25903;&#25345;&#21521;&#37327;&#26426;&#27169;&#22411;&#22312;&#25910;&#25947;&#26102;&#20250;&#34920;&#29616;&#20026;&#24191;&#20041;&#20284;&#28982;&#27604;&#26816;&#39564;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#23637;&#31034;&#20102;&#19968;&#31867;&#26368;&#23567;&#20108;&#20056;SVM&#22312;&#25910;&#25947;&#26102;&#20063;&#33021;&#36798;&#21040;&#24191;&#20041;&#20284;&#28982;&#27604;&#26816;&#39564;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#31867;&#20998;&#31867;&#65288;OCC&#65289;&#26159;&#20915;&#23450;&#35266;&#23519;&#26679;&#26412;&#26159;&#21542;&#23646;&#20110;&#30446;&#26631;&#31867;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#32771;&#34385;&#22312;&#21253;&#21547;&#30446;&#26631;&#31867;&#26679;&#26412;&#30340;&#25968;&#25454;&#38598;&#19978;&#23398;&#20064;&#19968;&#20010;&#34920;&#29616;&#20026;&#24191;&#20041;&#20284;&#28982;&#27604;&#26816;&#39564;&#65288;GLRT&#65289;&#30340;OCC&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;&#24403;&#30446;&#26631;&#31867;&#30340;&#32479;&#35745;&#20449;&#24687;&#21487;&#29992;&#26102;&#65292;GLRT&#35299;&#20915;&#20102;&#30456;&#21516;&#30340;&#38382;&#39064;&#12290;GLRT&#26159;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#19988;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#21487;&#35777;&#26126;&#26368;&#20339;&#30340;&#20998;&#31867;&#22120;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#22810;&#23618;&#24863;&#30693;&#22120;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#21644;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#27169;&#22411;&#12290;&#23427;&#20204;&#20351;&#29992;&#20154;&#24037;&#25968;&#25454;&#38598;&#35757;&#32451;&#20026;&#20004;&#31867;&#20998;&#31867;&#22120;&#65292;&#20854;&#20013;&#26367;&#20195;&#31867;&#20351;&#29992;&#22312;&#30446;&#26631;&#31867;&#25968;&#25454;&#38598;&#30340;&#23450;&#20041;&#22495;&#19978;&#22343;&#21248;&#29983;&#25104;&#30340;&#38543;&#26426;&#26679;&#26412;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#36866;&#24403;&#30340;&#20551;&#35774;&#19979;&#65292;&#27169;&#22411;&#22312;&#22823;&#25968;&#25454;&#38598;&#19978;&#25910;&#25947;&#21040;&#20102;GLRT&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#20855;&#26377;&#36866;&#24403;&#26680;&#20989;&#25968;&#30340;&#19968;&#31867;&#26368;&#23567;&#20108;&#20056;SVM&#65288;OCLSSVM&#65289;&#22312;&#25910;&#25947;&#26102;&#34920;&#29616;&#20026;GLRT&#12290;
&lt;/p&gt;
&lt;p&gt;
One-class classification (OCC) is the problem of deciding whether an observed sample belongs to a target class. We consider the problem of learning an OCC model that performs as the generalized likelihood ratio test (GLRT), given a dataset containing samples of the target class. The GLRT solves the same problem when the statistics of the target class are available. The GLRT is a well-known and provably optimal (under specific assumptions) classifier. To this end, we consider both the multilayer perceptron neural network (NN) and the support vector machine (SVM) models. They are trained as two-class classifiers using an artificial dataset for the alternative class, obtained by generating random samples, uniformly over the domain of the target-class dataset. We prove that, under suitable assumptions, the models converge (with a large dataset) to the GLRT. Moreover, we show that the one-class least squares SVM (OCLSSVM) with suitable kernels at convergence performs as the GLRT. Lastly, we
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;CP-PINNs&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;PINNs&#19982;&#24635;&#21464;&#24046;&#24809;&#32602;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#21464;&#28857;&#26816;&#27979;&#21644;PDE&#30340;&#21457;&#29616;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#20803;&#23398;&#20064;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#25968;&#25454;&#30340;&#36830;&#32493;&#25209;&#27425;&#19978;&#21160;&#24577;&#25913;&#36827;&#20248;&#21270;&#30446;&#26631;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#23384;&#22312;&#21464;&#28857;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20934;&#30830;&#20272;&#35745;&#21442;&#25968;&#21644;&#27169;&#22411;&#23545;&#40784;&#65292;&#22312;&#27809;&#26377;&#21464;&#28857;&#30340;&#24773;&#20917;&#19979;&#33021;&#22815;&#25968;&#20540;&#19978;&#25910;&#25947;&#21040;&#21407;&#22987;PINNs&#27169;&#22411;&#30340;&#35299;&#12290;</title><link>http://arxiv.org/abs/2208.08626</link><description>&lt;p&gt;
CP-PINNs: &#20351;&#29992;&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;&#21644;&#24635;&#21464;&#24046;&#24809;&#32602;&#36827;&#34892;PDE&#20013;&#30340;&#21464;&#28857;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
CP-PINNs: Changepoints Detection in PDEs using Physics Informed Neural Networks with Total-Variation Penalty. (arXiv:2208.08626v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.08626
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;CP-PINNs&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;PINNs&#19982;&#24635;&#21464;&#24046;&#24809;&#32602;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#21464;&#28857;&#26816;&#27979;&#21644;PDE&#30340;&#21457;&#29616;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#20803;&#23398;&#20064;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#25968;&#25454;&#30340;&#36830;&#32493;&#25209;&#27425;&#19978;&#21160;&#24577;&#25913;&#36827;&#20248;&#21270;&#30446;&#26631;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#23384;&#22312;&#21464;&#28857;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20934;&#30830;&#20272;&#35745;&#21442;&#25968;&#21644;&#27169;&#22411;&#23545;&#40784;&#65292;&#22312;&#27809;&#26377;&#21464;&#28857;&#30340;&#24773;&#20917;&#19979;&#33021;&#22815;&#25968;&#20540;&#19978;&#25910;&#25947;&#21040;&#21407;&#22987;PINNs&#27169;&#22411;&#30340;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#22312;&#21442;&#25968;&#20013;&#23384;&#22312;&#26410;&#30693;&#21464;&#28857;&#30340;&#24773;&#20917;&#19979;&#65292;&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#21487;&#33021;&#26080;&#27861;&#27491;&#30830;&#20272;&#35745;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#30340;&#21160;&#24577;&#36807;&#31243;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;CP-PINNs&#27169;&#22411;&#65292;&#23558;PINNs&#19982;&#24635;&#21464;&#24046;&#24809;&#32602;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#20934;&#30830;&#30340;&#21464;&#28857;&#26816;&#27979;&#21644;PDE&#30340;&#21457;&#29616;&#12290;&#20026;&#20102;&#22312;&#27169;&#22411;&#25311;&#21512;&#12289;PDE&#21457;&#29616;&#21644;&#21464;&#28857;&#26816;&#27979;&#20219;&#21153;&#20043;&#38388;&#36827;&#34892;&#26368;&#20248;&#32452;&#21512;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#20803;&#23398;&#20064;&#31639;&#27861;&#65292;&#21033;&#29992;&#25209;&#37327;&#23398;&#20064;&#22312;&#25968;&#25454;&#30340;&#36830;&#32493;&#25209;&#27425;&#19978;&#21160;&#24577;&#25913;&#36827;&#20248;&#21270;&#30446;&#26631;&#12290;&#22312;&#23454;&#35777;&#26041;&#38754;&#65292;&#22312;&#21160;&#24577;&#36807;&#31243;&#20013;&#23384;&#22312;&#21464;&#28857;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#20934;&#30830;&#20272;&#35745;&#21442;&#25968;&#21644;&#27169;&#22411;&#23545;&#40784;&#65292;&#22312;&#25968;&#25454;&#20013;&#27809;&#26377;&#21464;&#28857;&#30340;&#24773;&#20917;&#19979;&#65292;&#25968;&#20540;&#19978;&#25910;&#25947;&#21040;&#21407;&#22987;PINNs&#27169;&#22411;&#30340;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper shows that Physics-Informed Neural Networks (PINNs) can fail to estimate the correct Partial Differential Equations (PDEs) dynamics in cases of unknown changepoints in the parameters. To address this, we propose a new CP-PINNs model which integrates PINNs with Total-Variation penalty for accurate changepoints detection and PDEs discovery. In order to optimally combine the tasks of model fitting, PDEs discovery, and changepoints detection, we develop a new meta-learning algorithm that exploits batch learning to dynamically refines the optimization objective when moving over the consecutive batches of the data. Empirically, in case of changepoints in the dynamics, our approach demonstrates accurate parameter estimation and model alignment, and in case of no changepoints in the data, it converges numerically to the solution from the original PINNs model.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;MapReduce&#21644;&#36866;&#24212;&#24615;&#22797;&#26434;&#24230;&#27169;&#22411;&#20013;&#29992;&#20110;&#22823;&#23567;&#32422;&#26463;&#23376;&#27169;&#22411;&#26368;&#22823;&#21270;&#38382;&#39064;&#30340;&#21487;&#25193;&#23637;&#20998;&#24067;&#24335;&#31639;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#20960;&#31181;&#27425;&#32447;&#24615;&#33258;&#36866;&#24212;&#31639;&#27861;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;&#31639;&#27861;&#28385;&#36275;&#22312;MR&#29615;&#22659;&#20013;&#25152;&#38656;&#30340;&#19968;&#33268;&#24615;&#29305;&#24615;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#19990;&#30028;&#39318;&#20010;&#20855;&#26377;&#24658;&#23450;MR&#36718;&#27425;&#30340;&#32447;&#24615;&#26102;&#38388;&#20998;&#24067;&#24335;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2206.09563</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#20998;&#24067;&#24335;&#31639;&#27861;&#22312;MapReduce&#21644;&#36866;&#24212;&#24615;&#22797;&#26434;&#24230;&#27169;&#22411;&#20013;&#29992;&#20110;&#22823;&#23567;&#32422;&#26463;&#23376;&#27169;&#22411;&#26368;&#22823;&#21270;&#38382;&#39064;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Scalable Distributed Algorithms for Size-Constrained Submodular Maximization in the MapReduce and Adaptive Complexity Models. (arXiv:2206.09563v4 [cs.DS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.09563
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;MapReduce&#21644;&#36866;&#24212;&#24615;&#22797;&#26434;&#24230;&#27169;&#22411;&#20013;&#29992;&#20110;&#22823;&#23567;&#32422;&#26463;&#23376;&#27169;&#22411;&#26368;&#22823;&#21270;&#38382;&#39064;&#30340;&#21487;&#25193;&#23637;&#20998;&#24067;&#24335;&#31639;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#20960;&#31181;&#27425;&#32447;&#24615;&#33258;&#36866;&#24212;&#31639;&#27861;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;&#31639;&#27861;&#28385;&#36275;&#22312;MR&#29615;&#22659;&#20013;&#25152;&#38656;&#30340;&#19968;&#33268;&#24615;&#29305;&#24615;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#19990;&#30028;&#39318;&#20010;&#20855;&#26377;&#24658;&#23450;MR&#36718;&#27425;&#30340;&#32447;&#24615;&#26102;&#38388;&#20998;&#24067;&#24335;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;MapReduce&#27169;&#22411;&#20013;&#23545;&#23376;&#27169;&#22411;&#20989;&#25968;&#36827;&#34892;&#20998;&#24067;&#24335;&#26368;&#22823;&#21270;&#30340;&#30740;&#31350;&#24050;&#32463;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#65292;&#36825;&#23548;&#33268;&#20102;&#20004;&#31181;&#26694;&#26550;&#21487;&#20197;&#22312;MR&#29615;&#22659;&#20013;&#36816;&#34892;&#38598;&#20013;&#24335;&#31639;&#27861;&#32780;&#19981;&#25439;&#22833;&#36924;&#36817;&#24615;&#33021;&#65292;&#21482;&#35201;&#38598;&#20013;&#24335;&#31639;&#27861;&#28385;&#36275;&#19968;&#23450;&#30340;&#19968;&#33268;&#24615;&#29305;&#24615; - &#36825;&#20010;&#29305;&#24615;&#20165;&#30001;&#26631;&#20934;&#30340;&#36138;&#23146;&#31639;&#27861;&#21644;&#36830;&#32493;&#36138;&#23146;&#31639;&#27861;&#28385;&#36275;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#36824;&#26377;&#19968;&#31995;&#21015;&#30340;&#30740;&#31350;&#24037;&#20316;&#30528;&#30524;&#20110;&#33258;&#36866;&#24212;&#22797;&#26434;&#24230;&#27169;&#22411;&#20013;&#30340;&#23376;&#27169;&#22411;&#26368;&#22823;&#21270;&#30340;&#24182;&#34892;&#35745;&#31639;&#33021;&#21147;&#65292;&#20854;&#20013;&#27599;&#20010;&#32447;&#31243;&#21487;&#20197;&#35775;&#38382;&#25972;&#20010;&#24213;&#38598;&#12290;&#23545;&#20110;&#28385;&#36275;&#22823;&#23567;&#32422;&#26463;&#30340;&#21333;&#35843;&#23376;&#27169;&#22411;&#20989;&#25968;&#26368;&#22823;&#21270;&#38382;&#39064;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20960;&#31181;&#27425;&#32447;&#24615;&#33258;&#36866;&#24212;&#31639;&#27861;&#28385;&#36275;&#22312;MR&#29615;&#22659;&#20013;&#25152;&#38656;&#30340;&#19968;&#33268;&#24615;&#29305;&#24615;&#65292;&#20174;&#32780;&#24471;&#21040;&#20102;&#39640;&#24230;&#23454;&#29992;&#30340;&#21487;&#24182;&#34892;&#35745;&#31639;&#21644;&#20998;&#24067;&#24335;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#31532;&#19968;&#20010;&#20855;&#26377;&#24658;&#23450;MR&#36718;&#27425;&#30340;&#32447;&#24615;&#26102;&#38388;&#20998;&#24067;&#24335;&#31639;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#22686;&#21152;m&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributed maximization of a submodular function in the MapReduce model has received much attention, culminating in two frameworks that allow a centralized algorithm to be run in the MR setting without loss of approximation, as long as the centralized algorithm satisfies a certain consistency property - which had only been shown to be satisfied by the standard greedy and continous greedy algorithms. A separate line of work has studied parallelizability of submodular maximization in the adaptive complexity model, where each thread may have access to the entire ground set. For the size-constrained maximization of a monotone and submodular function, we show that several sublinearly adaptive algorithms satisfy the consistency property required to work in the MR setting, which yields highly practical parallelizable and distributed algorithms. Also, we develop the first linear-time distributed algorithm for this problem with constant MR rounds. Finally, we provide a method to increase the m
&lt;/p&gt;</description></item></channel></rss>