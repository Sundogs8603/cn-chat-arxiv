<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#35770;&#25991;&#26088;&#22312;&#25552;&#20986;&#19968;&#31181;&#39640;&#25928;&#30340;&#26041;&#27861;&#26469;&#20174;&#25193;&#25955;&#27169;&#22411;&#20013;&#25552;&#28860;GANs&#65292;&#24182;&#29992;&#20110;&#22270;&#20687;&#21040;&#22270;&#20687;&#30340;&#36716;&#25442;&#20219;&#21153;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#28789;&#27963;&#30340;&#23454;&#26102;&#22270;&#20687;&#32534;&#36753;&#65292;&#24182;&#26174;&#33879;&#38477;&#20302;&#35757;&#32451;&#19981;&#21516;&#27010;&#24565;&#27169;&#22411;&#30340;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2401.06127</link><description>&lt;p&gt;
E$^{2}$GAN: &#39640;&#25928;&#35757;&#32451;&#22270;&#20687;&#21040;&#22270;&#20687;&#36716;&#25442;&#20219;&#21153;&#30340;&#39640;&#25928;GANs
&lt;/p&gt;
&lt;p&gt;
E$^{2}$GAN: Efficient Training of Efficient GANs for Image-to-Image Translation. (arXiv:2401.06127v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06127
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#26088;&#22312;&#25552;&#20986;&#19968;&#31181;&#39640;&#25928;&#30340;&#26041;&#27861;&#26469;&#20174;&#25193;&#25955;&#27169;&#22411;&#20013;&#25552;&#28860;GANs&#65292;&#24182;&#29992;&#20110;&#22270;&#20687;&#21040;&#22270;&#20687;&#30340;&#36716;&#25442;&#20219;&#21153;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#28789;&#27963;&#30340;&#23454;&#26102;&#22270;&#20687;&#32534;&#36753;&#65292;&#24182;&#26174;&#33879;&#38477;&#20302;&#35757;&#32451;&#19981;&#21516;&#27010;&#24565;&#27169;&#22411;&#30340;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#23454;&#29616;&#28789;&#27963;&#30340;&#23454;&#26102;&#35774;&#22791;&#19978;&#22270;&#20687;&#32534;&#36753;&#65292;&#19968;&#31181;&#39640;&#24230;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#26159;&#21033;&#29992;&#22823;&#35268;&#27169;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65292;&#20363;&#22914;&#31283;&#23450;&#25193;&#25955; (Stable Diffusion)&#65292;&#29983;&#25104;&#29992;&#20110;&#35757;&#32451;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476; (GANs) &#30340;&#37197;&#23545;&#25968;&#25454;&#38598;&#12290;&#36825;&#31181;&#26041;&#27861;&#26174;&#33879;&#20943;&#36731;&#20102;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#22270;&#20687;&#32534;&#36753;&#26102;&#36890;&#24120;&#30001;&#39640;&#31471;&#21830;&#29992;GPU&#29305;&#23450;&#30340;&#20005;&#26684;&#35201;&#27714;&#12290;&#28982;&#32780;&#65292;&#19982;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#19981;&#21516;&#65292;&#27599;&#20010;&#29983;&#25104;&#30340; GAN &#37117;&#19987;&#38376;&#29992;&#20110;&#29305;&#23450;&#30340;&#22270;&#20687;&#32534;&#36753;&#20219;&#21153;&#65292;&#22240;&#27492;&#38656;&#35201;&#26114;&#36149;&#30340;&#35757;&#32451;&#24037;&#20316;&#26469;&#33719;&#24471;&#21508;&#31181;&#27010;&#24565;&#30340;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#24182;&#35299;&#20915;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#30740;&#31350;&#26041;&#21521;&#65306;&#33021;&#21542;&#20351;&#20174;&#25193;&#25955;&#27169;&#22411;&#20013;&#25552;&#28860; GANs &#30340;&#36807;&#31243;&#26356;&#21152;&#39640;&#25928;&#65311;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#21019;&#26032;&#25216;&#26415;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#20855;&#26377;&#24191;&#20041;&#29305;&#24449;&#30340;&#22522;&#26412; GAN &#27169;&#22411;&#65292;&#36890;&#36807;&#24494;&#35843;&#36866;&#24212;&#19981;&#21516;&#30340;&#27010;&#24565;&#65292;&#28040;&#38500;&#20102;...
&lt;/p&gt;
&lt;p&gt;
One highly promising direction for enabling flexible real-time on-device image editing is utilizing data distillation by leveraging large-scale text-to-image diffusion models, such as Stable Diffusion, to generate paired datasets used for training generative adversarial networks (GANs). This approach notably alleviates the stringent requirements typically imposed by high-end commercial GPUs for performing image editing with diffusion models. However, unlike text-to-image diffusion models, each distilled GAN is specialized for a specific image editing task, necessitating costly training efforts to obtain models for various concepts. In this work, we introduce and address a novel research direction: can the process of distilling GANs from diffusion models be made significantly more efficient? To achieve this goal, we propose a series of innovative techniques. First, we construct a base GAN model with generalized features, adaptable to different concepts through fine-tuning, eliminating t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#28608;&#27963;&#26368;&#22823;&#21270;&#26041;&#27861;&#22312;&#23545;&#25239;&#27169;&#22411;&#25805;&#20316;&#20013;&#30340;&#33030;&#24369;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#25805;&#32437;&#29305;&#24449;&#21487;&#35270;&#21270;&#65292;&#20197;&#38544;&#34255;&#29305;&#23450;&#31070;&#32463;&#20803;&#30340;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.06122</link><description>&lt;p&gt;
&#29992;&#26799;&#24230;&#24377;&#23556;&#25805;&#32437;&#29305;&#24449;&#21487;&#35270;&#21270;
&lt;/p&gt;
&lt;p&gt;
Manipulating Feature Visualizations with Gradient Slingshots. (arXiv:2401.06122v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06122
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#28608;&#27963;&#26368;&#22823;&#21270;&#26041;&#27861;&#22312;&#23545;&#25239;&#27169;&#22411;&#25805;&#20316;&#20013;&#30340;&#33030;&#24369;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#25805;&#32437;&#29305;&#24449;&#21487;&#35270;&#21270;&#65292;&#20197;&#38544;&#34255;&#29305;&#23450;&#31070;&#32463;&#20803;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#33021;&#22815;&#23398;&#20064;&#22797;&#26434;&#32780;&#22810;&#26679;&#21270;&#30340;&#34920;&#31034;&#65292;&#28982;&#32780;&#65292;&#23398;&#20064;&#21040;&#30340;&#27010;&#24565;&#30340;&#35821;&#20041;&#24615;&#36136;&#20173;&#28982;&#26410;&#30693;&#12290;&#35299;&#37322;DNNs&#23398;&#20064;&#21040;&#30340;&#27010;&#24565;&#30340;&#24120;&#29992;&#26041;&#27861;&#26159;&#28608;&#27963;&#26368;&#22823;&#21270;(AM)&#65292;&#23427;&#29983;&#25104;&#19968;&#20010;&#21512;&#25104;&#30340;&#36755;&#20837;&#20449;&#21495;&#65292;&#26368;&#22823;&#21270;&#28608;&#27963;&#32593;&#32476;&#20013;&#30340;&#29305;&#23450;&#31070;&#32463;&#20803;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#31181;&#26041;&#27861;&#23545;&#20110;&#23545;&#25239;&#27169;&#22411;&#25805;&#20316;&#30340;&#33030;&#24369;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#25805;&#32437;&#29305;&#24449;&#21487;&#35270;&#21270;&#65292;&#32780;&#19981;&#25913;&#21464;&#27169;&#22411;&#32467;&#26500;&#25110;&#23545;&#27169;&#22411;&#30340;&#20915;&#31574;&#36807;&#31243;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#20960;&#20010;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#25928;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#38544;&#34255;&#29305;&#23450;&#31070;&#32463;&#20803;&#21151;&#33021;&#30340;&#33021;&#21147;&#65292;&#22312;&#27169;&#22411;&#23457;&#26680;&#36807;&#31243;&#20013;&#20351;&#29992;&#36873;&#25321;&#30340;&#30446;&#26631;&#35299;&#37322;&#23631;&#34109;&#20102;&#21407;&#22987;&#35299;&#37322;&#12290;&#20316;&#20026;&#19968;&#31181;&#34917;&#25937;&#25514;&#26045;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38450;&#27490;&#36825;&#31181;&#25805;&#32437;&#30340;&#38450;&#25252;&#25514;&#26045;&#65292;&#24182;&#25552;&#20379;&#20102;&#23450;&#37327;&#35777;&#25454;&#65292;&#35777;&#26126;&#20102;&#23427;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks (DNNs) are capable of learning complex and versatile representations, however, the semantic nature of the learned concepts remains unknown. A common method used to explain the concepts learned by DNNs is Activation Maximization (AM), which generates a synthetic input signal that maximally activates a particular neuron in the network. In this paper, we investigate the vulnerability of this approach to adversarial model manipulations and introduce a novel method for manipulating feature visualization without altering the model architecture or significantly impacting the model's decision-making process. We evaluate the effectiveness of our method on several neural network models and demonstrate its capabilities to hide the functionality of specific neurons by masking the original explanations of neurons with chosen target explanations during model auditing. As a remedy, we propose a protective measure against such manipulations and provide quantitative evidence which 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TOFU&#30340;&#34394;&#25311;&#36951;&#24536;&#20219;&#21153;&#65292;&#26088;&#22312;&#24110;&#21161;&#25105;&#20204;&#28145;&#20837;&#29702;&#35299;&#36951;&#24536;&#12290;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#21253;&#21547;200&#20010;&#21512;&#25104;&#20316;&#32773;&#37197;&#32622;&#25991;&#20214;&#30340;&#25968;&#25454;&#38598;&#20197;&#21450;&#19968;&#22871;&#32508;&#21512;&#24230;&#37327;&#26631;&#20934;&#65292;&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#36951;&#24536;&#26041;&#27861;&#30340;&#25928;&#26524;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#32452;&#22522;&#20934;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.06121</link><description>&lt;p&gt;
TOFU: &#19968;&#31181;&#29992;&#20110;LLM&#30340;&#34394;&#25311;&#36951;&#24536;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
TOFU: A Task of Fictitious Unlearning for LLMs. (arXiv:2401.06121v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06121
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TOFU&#30340;&#34394;&#25311;&#36951;&#24536;&#20219;&#21153;&#65292;&#26088;&#22312;&#24110;&#21161;&#25105;&#20204;&#28145;&#20837;&#29702;&#35299;&#36951;&#24536;&#12290;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#21253;&#21547;200&#20010;&#21512;&#25104;&#20316;&#32773;&#37197;&#32622;&#25991;&#20214;&#30340;&#25968;&#25454;&#38598;&#20197;&#21450;&#19968;&#22871;&#32508;&#21512;&#24230;&#37327;&#26631;&#20934;&#65292;&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#36951;&#24536;&#26041;&#27861;&#30340;&#25928;&#26524;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#32452;&#22522;&#20934;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35757;&#32451;&#26102;&#21487;&#33021;&#20250;&#35760;&#24518;&#21644;&#37325;&#29616;&#25935;&#24863;&#25110;&#31169;&#23494;&#25968;&#25454;&#65292;&#24341;&#21457;&#27861;&#24459;&#21644;&#20262;&#29702;&#19978;&#30340;&#20851;&#20999;&#12290;&#36951;&#24536;&#65292;&#25110;&#32773;&#35843;&#25972;&#27169;&#22411;&#20197;&#24536;&#35760;&#35757;&#32451;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#20449;&#24687;&#65292;&#21487;&#20197;&#20026;&#25105;&#20204;&#25552;&#20379;&#19968;&#31181;&#22312;&#35757;&#32451;&#21518;&#20445;&#25252;&#31169;&#23494;&#25968;&#25454;&#30340;&#26041;&#24335;&#12290;&#23613;&#31649;&#23384;&#22312;&#20960;&#31181;&#29992;&#20110;&#36825;&#31181;&#36951;&#24536;&#30340;&#26041;&#27861;&#65292;&#20294;&#23578;&#19981;&#28165;&#26970;&#23427;&#20204;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#20250;&#23548;&#33268;&#19982;&#20174;&#26410;&#23398;&#20064;&#36807;&#35201;&#34987;&#36951;&#24536;&#30340;&#25968;&#25454;&#30340;&#27169;&#22411;&#30456;&#31561;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TOFU&#65292;&#19968;&#31181;&#34394;&#25311;&#36951;&#24536;&#20219;&#21153;&#65292;&#20316;&#20026;&#19968;&#20010;&#22522;&#20934;&#65292;&#26088;&#22312;&#24110;&#21161;&#25105;&#20204;&#21152;&#28145;&#23545;&#36951;&#24536;&#30340;&#29702;&#35299;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#30001;200&#20010;&#22810;&#26679;&#30340;&#21512;&#25104;&#20316;&#32773;&#37197;&#32622;&#25991;&#20214;&#32452;&#25104;&#30340;&#25968;&#25454;&#38598;&#65292;&#27599;&#20010;&#37197;&#32622;&#25991;&#20214;&#21253;&#21547;20&#20010;&#38382;&#31572;&#23545;&#65292;&#20197;&#21450;&#19968;&#20010;&#31216;&#20026;&#8220;&#36951;&#24536;&#38598;&#8221;&#30340;&#23376;&#38598;&#65292;&#20316;&#20026;&#36951;&#24536;&#30340;&#30446;&#26631;&#12290;&#25105;&#20204;&#32534;&#21046;&#20102;&#19968;&#22871;&#24230;&#37327;&#26631;&#20934;&#65292;&#20849;&#21516;&#25552;&#20379;&#20102;&#23545;&#36951;&#24536;&#25928;&#26524;&#30340;&#25972;&#20307;&#24433;&#21709;&#30340;&#23436;&#25972;&#30011;&#38754;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#32452;&#22522;&#20934;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models trained on massive corpora of data from the web can memorize and reproduce sensitive or private data raising both legal and ethical concerns. Unlearning, or tuning models to forget information present in their training data, provides us with a way to protect private data after training. Although several methods exist for such unlearning, it is unclear to what extent they result in models equivalent to those where the data to be forgotten was never learned in the first place. To address this challenge, we present TOFU, a Task of Fictitious Unlearning, as a benchmark aimed at helping deepen our understanding of unlearning. We offer a dataset of 200 diverse synthetic author profiles, each consisting of 20 question-answer pairs, and a subset of these profiles called the forget set that serves as the target for unlearning. We compile a suite of metrics that work together to provide a holistic picture of unlearning efficacy. Finally, we provide a set of baseline results
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#26497;&#31471;&#21387;&#32553;&#26041;&#38754;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#65292;&#30456;&#27604;&#26368;&#26032;&#25216;&#26415;&#65292;&#22312;&#32473;&#23450;&#30340;&#21387;&#32553;&#39044;&#31639;&#19979;&#20934;&#30830;&#24615;&#26356;&#39640;&#12290;</title><link>http://arxiv.org/abs/2401.06118</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#26497;&#31471;&#21387;&#32553;&#36890;&#36807;&#21152;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Extreme Compression of Large Language Models via Additive Quantization. (arXiv:2401.06118v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06118
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#26497;&#31471;&#21387;&#32553;&#26041;&#38754;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#65292;&#30456;&#27604;&#26368;&#26032;&#25216;&#26415;&#65292;&#22312;&#32473;&#23450;&#30340;&#21387;&#32553;&#39044;&#31639;&#19979;&#20934;&#30830;&#24615;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#24320;&#28304;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#20986;&#29616;&#24341;&#21457;&#20102;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#37327;&#21270;&#25216;&#26415;&#30340;&#31454;&#36187;&#65292;&#20174;&#32780;&#20351;&#20854;&#33021;&#22815;&#22312;&#26368;&#32456;&#29992;&#25143;&#35774;&#22791;&#19978;&#25191;&#34892;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#22810;&#30721;&#26412;&#37327;&#21270;(MCQ)&#30340;&#32463;&#20856;&#26041;&#27861;&#35282;&#24230;&#37325;&#26032;&#24605;&#32771;&#20102;&#8220;&#26497;&#31471;&#8221;LLM&#21387;&#32553;&#30340;&#38382;&#39064;&#65292;&#21363;&#38024;&#23545;&#38750;&#24120;&#20302;&#30340;&#20301;&#25968;&#65292;&#20363;&#22914;&#27599;&#20010;&#21442;&#25968;2&#21040;3&#20301;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24314;&#31435;&#22312;&#21152;&#24615;&#37327;&#21270;&#36825;&#19968;&#32463;&#20856;&#31639;&#27861;&#20043;&#19978;&#65292;&#24182;&#23558;&#20854;&#36866;&#24212;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#37327;&#21270;&#12290;&#30001;&#27492;&#24471;&#21040;&#30340;&#31639;&#27861;&#22312;LLM&#21387;&#32553;&#26041;&#38754;&#25512;&#36827;&#20102;&#26368;&#26032;&#25216;&#26415;&#65292;&#20197;&#32473;&#23450;&#21387;&#32553;&#39044;&#31639;&#30340;&#20934;&#30830;&#24615;&#32780;&#35328;&#65292;&#20248;&#20110;&#25152;&#26377;&#26368;&#36817;&#25552;&#20986;&#30340;&#25216;&#26415;&#12290;&#20363;&#22914;&#65292;&#24403;&#23558;Llama 2&#27169;&#22411;&#21387;&#32553;&#21040;&#27599;&#20010;&#21442;&#25968;2&#20301;&#26102;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#23558;7B&#27169;&#22411;&#37327;&#21270;&#20026;6.93&#22256;&#24785;&#24230;(&#30456;&#23545;&#20110;&#20043;&#21069;&#26368;&#20339;&#24037;&#20316;&#25913;&#36827;1.29&#65292;&#30456;&#23545;&#20110;FP16&#25913;&#36827;1.81)&#65292;13B&#27169;&#22411;&#37327;&#21270;&#20026;5.70&#22256;&#24785;&#24230;(&#25913;&#36827;0.36)&#65292;70B&#27169;&#22411;&#37327;&#21270;&#20026;3.94&#22256;&#24785;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of accurate open large language models (LLMs) has led to a race towards quantization techniques for such models enabling execution on end-user devices. In this paper, we revisit the problem of "extreme" LLM compression--defined as targeting extremely low bit counts, such as 2 to 3 bits per parameter, from the point of view of classic methods in Multi-Codebook Quantization (MCQ). Our work builds on top of Additive Quantization, a classic algorithm from the MCQ family, and adapts it to the quantization of language models. The resulting algorithm advances the state-of-the-art in LLM compression, outperforming all recently-proposed techniques in terms of accuracy at a given compression budget. For instance, when compressing Llama 2 models to 2 bits per parameter, our algorithm quantizes the 7B model to 6.93 perplexity (a 1.29 improvement relative to the best prior work, and 1.81 points from FP16), the 13B model to 5.70 perplexity (a .36 improvement) and the 70B model to 3.94 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PALP&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#20010;&#24615;&#21270;&#23545;&#20934;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#39069;&#22806;&#30340;&#20998;&#25968;&#33976;&#39311;&#37319;&#26679;&#39033;&#65292;&#20445;&#25345;&#20010;&#24615;&#21270;&#27169;&#22411;&#19982;&#30446;&#26631;&#25552;&#31034;&#30340;&#23545;&#20934;&#65292;&#20351;&#24471;&#33021;&#22815;&#21019;&#24314;&#20855;&#26377;&#22797;&#26434;&#21644;&#22797;&#26434;&#25552;&#31034;&#30340;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2401.06105</link><description>&lt;p&gt;
PALP&#65306;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#20010;&#24615;&#21270;&#23545;&#20934;
&lt;/p&gt;
&lt;p&gt;
PALP: Prompt Aligned Personalization of Text-to-Image Models. (arXiv:2401.06105v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06105
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PALP&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#20010;&#24615;&#21270;&#23545;&#20934;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#39069;&#22806;&#30340;&#20998;&#25968;&#33976;&#39311;&#37319;&#26679;&#39033;&#65292;&#20445;&#25345;&#20010;&#24615;&#21270;&#27169;&#22411;&#19982;&#30446;&#26631;&#25552;&#31034;&#30340;&#23545;&#20934;&#65292;&#20351;&#24471;&#33021;&#22815;&#21019;&#24314;&#20855;&#26377;&#22797;&#26434;&#21644;&#22797;&#26434;&#25552;&#31034;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20869;&#23481;&#21019;&#20316;&#32773;&#36890;&#24120;&#24076;&#26395;&#20351;&#29992;&#20010;&#20154;&#20027;&#39064;&#21019;&#24314;&#20010;&#24615;&#21270;&#30340;&#22270;&#20687;&#65292;&#36229;&#20986;&#20102;&#20256;&#32479;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#33021;&#21147;&#33539;&#22260;&#12290;&#27492;&#22806;&#65292;&#20182;&#20204;&#21487;&#33021;&#24076;&#26395;&#29983;&#25104;&#30340;&#22270;&#20687;&#20855;&#26377;&#29305;&#23450;&#30340;&#20301;&#32622;&#12289;&#39118;&#26684;&#12289;&#27675;&#22260;&#31561;&#12290;&#29616;&#26377;&#30340;&#20010;&#24615;&#21270;&#26041;&#27861;&#21487;&#33021;&#20250;&#22312;&#20010;&#24615;&#21270;&#33021;&#21147;&#25110;&#19982;&#22797;&#26434;&#25991;&#26412;&#25552;&#31034;&#30340;&#23545;&#40784;&#26041;&#38754;&#23384;&#22312;&#22949;&#21327;&#12290;&#36825;&#31181;&#26435;&#34913;&#21487;&#33021;&#22952;&#30861;&#29992;&#25143;&#25552;&#31034;&#30340;&#23454;&#29616;&#21644;&#20027;&#20307;&#23436;&#25972;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#19987;&#27880;&#20110;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#21333;&#20010;&#25552;&#31034;&#30340;&#20010;&#24615;&#21270;&#26041;&#27861;&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;&#23545;&#20934;&#25552;&#31034;&#30340;&#20010;&#24615;&#21270;&#12290;&#34429;&#28982;&#36825;&#21487;&#33021;&#30475;&#36215;&#26469;&#26377;&#38480;&#21046;&#65292;&#20294;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25913;&#21892;&#25991;&#26412;&#23545;&#20934;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20351;&#24471;&#33021;&#22815;&#21019;&#24314;&#20855;&#26377;&#22797;&#26434;&#21644;&#22797;&#26434;&#25552;&#31034;&#30340;&#22270;&#20687;&#65292;&#36825;&#21487;&#33021;&#23545;&#24403;&#21069;&#30340;&#25216;&#26415;&#26500;&#25104;&#25361;&#25112;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#39069;&#22806;&#30340;&#20998;&#25968;&#33976;&#39311;&#37319;&#26679;&#39033;&#20351;&#20010;&#24615;&#21270;&#27169;&#22411;&#19982;&#30446;&#26631;&#25552;&#31034;&#20445;&#25345;&#23545;&#20934;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#22810;&#21151;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Content creators often aim to create personalized images using personal subjects that go beyond the capabilities of conventional text-to-image models. Additionally, they may want the resulting image to encompass a specific location, style, ambiance, and more. Existing personalization methods may compromise personalization ability or the alignment to complex textual prompts. This trade-off can impede the fulfillment of user prompts and subject fidelity. We propose a new approach focusing on personalization methods for a \emph{single} prompt to address this issue. We term our approach prompt-aligned personalization. While this may seem restrictive, our method excels in improving text alignment, enabling the creation of images with complex and intricate prompts, which may pose a challenge for current techniques. In particular, our method keeps the personalized model aligned with a target prompt using an additional score distillation sampling term. We demonstrate the versatility of our met
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21483;&#20570;Patchscope&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#26597;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#34255;&#34920;&#31034;&#12290;&#35813;&#26694;&#26550;&#19981;&#20165;&#32479;&#19968;&#20102;&#20808;&#21069;&#30340;&#26816;&#26597;&#25216;&#26415;&#65292;&#36824;&#35299;&#20915;&#20102;&#20854;&#20013;&#19968;&#20123;&#38382;&#39064;&#65292;&#24182;&#19988;&#36824;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.06102</link><description>&lt;p&gt;
Patchscope: &#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#26597;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#34255;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Patchscope: A Unifying Framework for Inspecting Hidden Representations of Language Models. (arXiv:2401.06102v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06102
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21483;&#20570;Patchscope&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#26597;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#34255;&#34920;&#31034;&#12290;&#35813;&#26694;&#26550;&#19981;&#20165;&#32479;&#19968;&#20102;&#20808;&#21069;&#30340;&#26816;&#26597;&#25216;&#26415;&#65292;&#36824;&#35299;&#20915;&#20102;&#20854;&#20013;&#19968;&#20123;&#38382;&#39064;&#65292;&#24182;&#19988;&#36824;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#26597;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#38544;&#34255;&#34920;&#31034;&#20013;&#32534;&#30721;&#30340;&#20449;&#24687;&#21487;&#20197;&#35299;&#37322;&#27169;&#22411;&#30340;&#34892;&#20026;&#24182;&#39564;&#35777;&#20854;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#19968;&#33268;&#24615;&#12290;&#37492;&#20110;LLM&#29983;&#25104;&#20154;&#31867;&#21487;&#29702;&#35299;&#25991;&#26412;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#24314;&#35758;&#21033;&#29992;&#27169;&#22411;&#26412;&#36523;&#20197;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#20854;&#20869;&#37096;&#34920;&#31034;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31216;&#20026;Patchscopes&#30340;&#26694;&#26550;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#23427;&#26469;&#22238;&#31572;&#20851;&#20110;LLM&#35745;&#31639;&#30340;&#21508;&#31181;&#30740;&#31350;&#38382;&#39064;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#20808;&#21069;&#22522;&#20110;&#23558;&#34920;&#31034;&#25237;&#24433;&#21040;&#35789;&#27719;&#31354;&#38388;&#21644;&#24178;&#39044;LLM&#35745;&#31639;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#21487;&#20197;&#30475;&#20316;&#26159;&#35813;&#26694;&#26550;&#30340;&#29305;&#27530;&#23454;&#20363;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;Patchscope&#21487;&#20197;&#24357;&#34917;&#20248;&#21183;&#65292;&#22914;&#26816;&#26597;&#26089;&#26399;&#23618;&#22833;&#36133;&#25110;&#34920;&#36798;&#33021;&#21147;&#19981;&#36275;&#12290;&#38500;&#20102;&#32479;&#19968;&#20808;&#21069;&#30340;&#26816;&#26597;&#25216;&#26415;&#65292;Patchscopes&#36824;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#65292;&#20363;&#22914;&#20351;&#29992;&#26356;&#24378;&#22823;&#30340;&#27169;&#22411;&#26469;&#35299;&#37322;&#36739;&#23567;&#27169;&#22411;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspecting the information encoded in hidden representations of large language models (LLMs) can explain models' behavior and verify their alignment with human values. Given the capabilities of LLMs in generating human-understandable text, we propose leveraging the model itself to explain its internal representations in natural language. We introduce a framework called Patchscopes and show how it can be used to answer a wide range of research questions about an LLM's computation. We show that prior interpretability methods based on projecting representations into the vocabulary space and intervening on the LLM computation, can be viewed as special instances of this framework. Moreover, several of their shortcomings such as failure in inspecting early layers or lack of expressivity can be mitigated by a Patchscope. Beyond unifying prior inspection techniques, Patchscopes also opens up new possibilities such as using a more capable model to explain the representations of a smaller model,
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25968;&#23398;&#20998;&#26512;&#65292;&#30740;&#31350;&#21457;&#29616;AUROC&#21644;AUPRC&#22312;&#31867;&#21035;&#19981;&#24179;&#34913;&#24773;&#20917;&#19979;&#21487;&#20197;&#20197;&#27010;&#29575;&#26415;&#35821;&#31616;&#27905;&#22320;&#30456;&#20851;&#32852;&#12290;&#30456;&#27604;&#20110;&#20154;&#20204;&#26222;&#36941;&#35748;&#20026;&#30340;AUPRC&#20248;&#36234;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;AUPRC&#24182;&#19981;&#22914;&#20154;&#20204;&#39044;&#26399;&#30340;&#26377;&#20248;&#21183;&#65292;&#24182;&#19988;&#21487;&#33021;&#26159;&#19968;&#31181;&#26377;&#23475;&#30340;&#25351;&#26631;&#12290;&#30740;&#31350;&#36824;&#36890;&#36807;&#20998;&#26512;&#22823;&#37327;&#25991;&#29486;&#39564;&#35777;&#20102;&#36825;&#19968;&#32467;&#35770;&#12290;</title><link>http://arxiv.org/abs/2401.06091</link><description>&lt;p&gt;
AUROC&#21644;AUPRC&#22312;&#31867;&#19981;&#24179;&#34913;&#19979;&#30340;&#28145;&#20837;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Closer Look at AUROC and AUPRC under Class Imbalance. (arXiv:2401.06091v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06091
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25968;&#23398;&#20998;&#26512;&#65292;&#30740;&#31350;&#21457;&#29616;AUROC&#21644;AUPRC&#22312;&#31867;&#21035;&#19981;&#24179;&#34913;&#24773;&#20917;&#19979;&#21487;&#20197;&#20197;&#27010;&#29575;&#26415;&#35821;&#31616;&#27905;&#22320;&#30456;&#20851;&#32852;&#12290;&#30456;&#27604;&#20110;&#20154;&#20204;&#26222;&#36941;&#35748;&#20026;&#30340;AUPRC&#20248;&#36234;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;AUPRC&#24182;&#19981;&#22914;&#20154;&#20204;&#39044;&#26399;&#30340;&#26377;&#20248;&#21183;&#65292;&#24182;&#19988;&#21487;&#33021;&#26159;&#19968;&#31181;&#26377;&#23475;&#30340;&#25351;&#26631;&#12290;&#30740;&#31350;&#36824;&#36890;&#36807;&#20998;&#26512;&#22823;&#37327;&#25991;&#29486;&#39564;&#35777;&#20102;&#36825;&#19968;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#19968;&#20010;&#24191;&#27867;&#30340;&#35266;&#28857;&#26159;&#65292;&#22312;&#20108;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#38754;&#31215;&#21463;&#38480;&#21046;&#30340;&#20934;&#30830;&#29575;&#26354;&#32447;&#65288;AUPRC&#65289;&#27604;&#21463;&#35797;&#32773;&#24037;&#20316;&#29305;&#24449;&#26354;&#32447;&#19979;&#30340;&#38754;&#31215;&#65288;AUROC&#65289;&#26356;&#22909;&#22320;&#29992;&#20110;&#27169;&#22411;&#27604;&#36739;&#65292;&#23588;&#20854;&#26159;&#22312;&#23384;&#22312;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#24773;&#20917;&#19979;&#12290;&#26412;&#25991;&#36890;&#36807;&#26032;&#39062;&#30340;&#25968;&#23398;&#20998;&#26512;&#25361;&#25112;&#20102;&#36825;&#19968;&#35266;&#28857;&#65292;&#24182;&#35828;&#26126;&#20102;AUROC&#21644;AUPRC&#21487;&#20197;&#20197;&#27010;&#29575;&#26415;&#35821;&#31616;&#27905;&#22320;&#30456;&#20851;&#32852;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;AUPRC&#24182;&#19981;&#22914;&#20154;&#20204;&#26222;&#36941;&#35748;&#20026;&#30340;&#22312;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#24773;&#20917;&#19979;&#26356;&#20248;&#65292;&#29978;&#33267;&#21487;&#33021;&#26159;&#19968;&#31181;&#26377;&#23475;&#30340;&#25351;&#26631;&#65292;&#22240;&#20026;&#23427;&#20542;&#21521;&#20110;&#36807;&#20998;&#20559;&#21521;&#20110;&#22312;&#27491;&#26679;&#26412;&#36739;&#20026;&#39057;&#32321;&#30340;&#23376;&#32676;&#20013;&#25913;&#21892;&#27169;&#22411;&#12290;&#36825;&#31181;&#20559;&#24046;&#21487;&#33021;&#20250;&#26080;&#24847;&#20013;&#22686;&#21152;&#31639;&#27861;&#30340;&#24046;&#24322;&#12290;&#22312;&#36825;&#20123;&#27934;&#35265;&#30340;&#25512;&#21160;&#19979;&#65292;&#25105;&#20204;&#23545;&#29616;&#26377;&#30340;&#26426;&#22120;&#23398;&#20064;&#25991;&#29486;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#22238;&#39038;&#65292;&#24182;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;arXiv&#19978;&#30340;150&#22810;&#19975;&#31687;&#35770;&#25991;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#37325;&#28857;&#26159;&#39564;&#35777;&#21644;&#35777;&#26126;&#22768;&#31216;&#30340;AUPRC&#20248;&#36234;&#24615;&#30340;&#26222;&#36941;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In machine learning (ML), a widespread adage is that the area under the precision-recall curve (AUPRC) is a superior metric for model comparison to the area under the receiver operating characteristic (AUROC) for binary classification tasks with class imbalance. This paper challenges this notion through novel mathematical analysis, illustrating that AUROC and AUPRC can be concisely related in probabilistic terms. We demonstrate that AUPRC, contrary to popular belief, is not superior in cases of class imbalance and might even be a harmful metric, given its inclination to unduly favor model improvements in subpopulations with more frequent positive labels. This bias can inadvertently heighten algorithmic disparities. Prompted by these insights, a thorough review of existing ML literature was conducted, utilizing large language models to analyze over 1.5 million papers from arXiv. Our investigation focused on the prevalence and substantiation of the purported AUPRC superiority. The result
&lt;/p&gt;</description></item><item><title>PANDORA &#26159;&#19968;&#31181;&#29992;&#20110;&#21333;&#38142;&#25509;&#32858;&#31867;&#30340;&#24182;&#34892;&#26641;&#29366;&#22270;&#26500;&#24314;&#31639;&#27861;&#65292;&#36890;&#36807;&#29420;&#29305;&#30340;&#36882;&#24402;&#26641;&#25910;&#32553;&#26041;&#27861;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#24182;&#34892;&#21270;&#22256;&#38590;&#30340;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#28176;&#36827;&#24847;&#20041;&#19979;&#30340;&#24037;&#20316;&#26368;&#20248;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#32447;&#31243;&#21152;&#36895;&#22120;&#12290;</title><link>http://arxiv.org/abs/2401.06089</link><description>&lt;p&gt;
PANDORA&#65306;GPU&#19978;&#29992;&#20110;&#21333;&#38142;&#25509;&#32858;&#31867;&#30340;&#24182;&#34892;&#26641;&#29366;&#22270;&#26500;&#24314;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
PANDORA: A Parallel Dendrogram Construction Algorithm for Single Linkage Clustering on GPU. (arXiv:2401.06089v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06089
&lt;/p&gt;
&lt;p&gt;
PANDORA &#26159;&#19968;&#31181;&#29992;&#20110;&#21333;&#38142;&#25509;&#32858;&#31867;&#30340;&#24182;&#34892;&#26641;&#29366;&#22270;&#26500;&#24314;&#31639;&#27861;&#65292;&#36890;&#36807;&#29420;&#29305;&#30340;&#36882;&#24402;&#26641;&#25910;&#32553;&#26041;&#27861;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#24182;&#34892;&#21270;&#22256;&#38590;&#30340;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#28176;&#36827;&#24847;&#20041;&#19979;&#30340;&#24037;&#20316;&#26368;&#20248;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#32447;&#31243;&#21152;&#36895;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;PANDORA&#65292;&#19968;&#31181;&#29992;&#20110;&#39640;&#25928;&#26500;&#24314;&#21333;&#38142;&#25509;&#23618;&#27425;&#32858;&#31867;&#30340;&#26641;&#29366;&#22270;&#30340;&#24182;&#34892;&#31639;&#27861;&#65292;&#21253;&#25324;HDSCAN&#12290;&#20256;&#32479;&#30340;&#26368;&#23567;&#29983;&#25104;&#26641;&#22522;&#30784;&#30340;&#26641;&#29366;&#22270;&#26500;&#24314;&#26041;&#27861;&#65292;&#20363;&#22914;&#20957;&#32858;&#25110;&#20998;&#35010;&#25216;&#26415;&#65292;&#36890;&#24120;&#38590;&#20197;&#36827;&#34892;&#26377;&#25928;&#30340;&#24182;&#34892;&#21270;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#22312;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#20013;&#24120;&#35265;&#30340;&#20542;&#26012;&#26641;&#29366;&#22270;&#12290;PANDORA&#36890;&#36807;&#29420;&#29305;&#30340;&#36882;&#24402;&#26641;&#25910;&#32553;&#26041;&#27861;&#35299;&#20915;&#20102;&#36825;&#20123;&#25361;&#25112;&#65292;&#35813;&#26041;&#27861;&#31616;&#21270;&#20102;&#26641;&#29366;&#22270;&#30340;&#21021;&#22987;&#26500;&#24314;&#36807;&#31243;&#65292;&#28982;&#21518;&#36880;&#27493;&#37325;&#24314;&#23436;&#25972;&#30340;&#26641;&#29366;&#22270;&#12290;&#35813;&#36807;&#31243;&#20351;&#24471;PANDORA&#22312;&#28176;&#36827;&#24847;&#20041;&#19979;&#24037;&#20316;&#26368;&#20248;&#65292;&#19982;&#26641;&#29366;&#22270;&#30340;&#20542;&#26012;&#31243;&#24230;&#26080;&#20851;&#12290;PANDORA&#30340;&#25152;&#26377;&#27493;&#39588;&#37117;&#26159;&#23436;&#20840;&#24182;&#34892;&#30340;&#65292;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#32447;&#31243;&#21152;&#36895;&#22120;&#65292;&#22914;GPU&#12290;&#25105;&#20204;&#30340;&#23454;&#29616;&#20351;&#29992;Kokkos&#32534;&#20889;&#65292;&#25903;&#25345;CPU&#21644;&#22810;&#20379;&#24212;&#21830;&#30340;GPU&#65288;&#20363;&#22914;Nvidia&#12289;AMD&#65289;&#12290;&#22810;&#32447;&#31243;&#29256;&#26412;&#30340;PANDORA&#27604;&#24403;&#21069;&#26368;&#20339;&#30340;&#22810;&#32447;&#31243;&#29256;&#26412;&#24555;2.2&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents \pandora, a novel parallel algorithm for efficiently constructing dendrograms for single-linkage hierarchical clustering, including \hdbscan. Traditional dendrogram construction methods from a minimum spanning tree (MST), such as agglomerative or divisive techniques, often fail to efficiently parallelize, especially with skewed dendrograms common in real-world data.  \pandora addresses these challenges through a unique recursive tree contraction method, which simplifies the tree for initial dendrogram construction and then progressively reconstructs the complete dendrogram. This process makes \pandora asymptotically work-optimal, independent of dendrogram skewness. All steps in \pandora are fully parallel and suitable for massively threaded accelerators such as GPUs.  Our implementation is written in Kokkos, providing support for both CPUs and multi-vendor GPUs (e.g., Nvidia, AMD). The multithreaded version of \pandora is 2.2$\times$ faster than the current best-mul
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#25991;&#26412;&#29983;&#25104;&#25216;&#26415;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#35757;&#32451;&#20102;&#20960;&#31181;&#21464;&#31181;&#30340;&#29983;&#29289;&#21307;&#23398;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#33258;&#21160;&#34917;&#20840;&#24037;&#20855;&#65292;&#21487;&#20026;&#19977;&#32423;&#25252;&#29702;&#20154;&#21592;&#25552;&#20379;&#20934;&#30830;&#21644;&#26684;&#24335;&#33391;&#22909;&#30340;&#20027;&#35201;&#30151;&#29366;&#30701;&#35821;&#25110;&#21477;&#23376;&#12290;</title><link>http://arxiv.org/abs/2401.06088</link><description>&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#33258;&#21160;&#34917;&#20840;&#20027;&#35201;&#30151;&#29366;
&lt;/p&gt;
&lt;p&gt;
Autocompletion of Chief Complaints in the Electronic Health Records using Large Language Models. (arXiv:2401.06088v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#25991;&#26412;&#29983;&#25104;&#25216;&#26415;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#35757;&#32451;&#20102;&#20960;&#31181;&#21464;&#31181;&#30340;&#29983;&#29289;&#21307;&#23398;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#33258;&#21160;&#34917;&#20840;&#24037;&#20855;&#65292;&#21487;&#20026;&#19977;&#32423;&#25252;&#29702;&#20154;&#21592;&#25552;&#20379;&#20934;&#30830;&#21644;&#26684;&#24335;&#33391;&#22909;&#30340;&#20027;&#35201;&#30151;&#29366;&#30701;&#35821;&#25110;&#21477;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#35201;&#30151;&#29366;&#65288;CC&#65289;&#26159;&#24739;&#32773;&#21307;&#30103;&#35760;&#24405;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#23427;&#25551;&#36848;&#20102;&#23547;&#27714;&#21307;&#30103;&#20445;&#20581;&#30340;&#20027;&#35201;&#21407;&#22240;&#25110;&#20851;&#27880;&#28857;&#12290;&#23427;&#20026;&#21307;&#30103;&#20445;&#20581;&#25552;&#20379;&#32773;&#25552;&#20379;&#20102;&#20851;&#38190;&#20449;&#24687;&#65292;&#20197;&#20415;&#20570;&#20986;&#26377;&#26681;&#25454;&#30340;&#24739;&#32773;&#25252;&#29702;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#20026;&#21307;&#30103;&#20445;&#20581;&#25552;&#20379;&#32773;&#35760;&#24405;CC&#21487;&#33021;&#32791;&#26102;&#65292;&#23588;&#20854;&#26159;&#22312;&#32321;&#24537;&#30340;&#24613;&#35786;&#31185;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#20020;&#24202;&#35760;&#24405;&#20013;&#20026;&#19977;&#32423;&#25252;&#29702;&#20154;&#21592;&#25552;&#20379;&#20934;&#30830;&#21644;&#26684;&#24335;&#33391;&#22909;&#30340;&#30701;&#35821;&#25110;&#21477;&#23376;&#30340;&#33258;&#21160;&#34917;&#20840;&#24037;&#20855;&#21487;&#20197;&#25104;&#20026;&#23453;&#36149;&#30340;&#36164;&#28304;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#25991;&#26412;&#29983;&#25104;&#25216;&#26415;&#20351;&#29992;CC&#25968;&#25454;&#24320;&#21457;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#22312;&#25105;&#20204;&#30340;&#25552;&#35758;&#20013;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#27169;&#22411;&#65292;&#24182;&#24494;&#35843;&#20102;&#19977;&#31181;&#19981;&#21516;&#21464;&#31181;&#30340;&#29983;&#29289;&#21307;&#23398;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#65288;BioGPT&#65289;&#65292;&#20998;&#21035;&#26159;microsoft/biogpt&#65292;microsoft/BioGPT-Large&#21644;microsoft/BioGPT-Large-PubMedQA&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#32467;&#21512;&#20856;&#22411;&#30340;CC&#21477;&#23376;&#65292;&#21033;&#29992;GPT&#30340;OpenAI API&#26469;&#35843;&#25972;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Chief Complaint (CC) is a crucial component of a patient's medical record as it describes the main reason or concern for seeking medical care. It provides critical information for healthcare providers to make informed decisions about patient care. However, documenting CCs can be time-consuming for healthcare providers, especially in busy emergency departments. To address this issue, an autocompletion tool that suggests accurate and well-formatted phrases or sentences for clinical notes can be a valuable resource for triage nurses. In this study, we utilized text generation techniques to develop machine learning models using CC data. In our proposed work, we train a Long Short-Term Memory (LSTM) model and fine-tune three different variants of Biomedical Generative Pretrained Transformers (BioGPT), namely microsoft/biogpt, microsoft/BioGPT-Large, and microsoft/BioGPT-Large-PubMedQA. Additionally, we tune a prompt by incorporating exemplar CC sentences, utilizing the OpenAI API of GPT
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#22522;&#20110;&#20195;&#29702;&#27169;&#22411;&#30340;&#20307;&#32946;&#21338;&#24425;&#20132;&#26131;&#20013;&#20351;&#29992;XGBoost&#23398;&#20064;&#21040;&#30340;&#21160;&#24577;&#25237;&#27880;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;&#20195;&#29702;&#27169;&#22411;&#30340;&#27169;&#25311;&#22120;&#36827;&#34892;&#23454;&#39564;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2401.06086</link><description>&lt;p&gt;
XGBoost&#23398;&#20064;&#29992;&#20110;&#22522;&#20110;&#20195;&#29702;&#27169;&#22411;&#30340;&#20307;&#32946;&#21338;&#24425;&#20132;&#26131;&#20013;&#30340;&#21160;&#24577;&#25237;&#27880;&#26041;&#24335;
&lt;/p&gt;
&lt;p&gt;
XGBoost Learning of Dynamic Wager Placement for In-Play Betting on an Agent-Based Model of a Sports Betting Exchange. (arXiv:2401.06086v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06086
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#22522;&#20110;&#20195;&#29702;&#27169;&#22411;&#30340;&#20307;&#32946;&#21338;&#24425;&#20132;&#26131;&#20013;&#20351;&#29992;XGBoost&#23398;&#20064;&#21040;&#30340;&#21160;&#24577;&#25237;&#27880;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;&#20195;&#29702;&#27169;&#22411;&#30340;&#27169;&#25311;&#22120;&#36827;&#34892;&#23454;&#39564;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#20171;&#32461;&#20102;&#22312;Bristol Betting Exchange (BBE)&#20013;&#20351;&#29992;&#39640;&#25928;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;XGBoost&#30340;&#32467;&#26524;&#12290;BBE&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#22522;&#20110;&#20195;&#29702;&#27169;&#22411;&#30340;&#27169;&#25311;&#22120;&#65292;&#26088;&#22312;&#27169;&#25311;&#20855;&#26377;&#36187;&#20107;&#20013;&#25237;&#27880;&#21151;&#33021;&#30340;&#29616;&#20195;&#20307;&#32946;&#21338;&#24425;&#20132;&#26131;&#12290;&#25105;&#20204;&#20351;&#29992;BBE&#27169;&#25311;&#22120;&#21644;&#20854;&#19968;&#31995;&#21015;&#31616;&#21333;&#30340;&#36172;&#24466;&#20195;&#29702;&#20316;&#20026;&#25968;&#25454;&#29983;&#25104;&#22120;&#65292;&#36755;&#20837;&#21040;&#25105;&#20204;&#30340;XGBoost&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#20013;&#12290;&#36890;&#36807;&#23398;&#20064;BBE&#36172;&#24466;&#20195;&#29702;&#25152;&#36827;&#34892;&#30340;&#26356;&#21152;&#26377;&#21033;&#21487;&#22270;&#30340;&#25237;&#27880;&#65292;XGBoost&#33021;&#22815;&#21457;&#29616;&#26377;&#21033;&#21487;&#22270;&#30340;&#21160;&#24577;&#25237;&#27880;&#31574;&#30053;&#12290;&#22312;XGBoost&#35757;&#32451;&#20043;&#21518;&#65292;&#29983;&#25104;&#19968;&#20010;&#25110;&#22810;&#20010;&#20915;&#31574;&#26641;&#65292;&#23558;&#20855;&#26377;&#30001;XGBoost&#23398;&#20064;&#20915;&#31574;&#26641;&#30830;&#23450;&#30340;&#25237;&#27880;&#31574;&#30053;&#30340;&#36172;&#24466;&#20195;&#29702;&#28155;&#21152;&#21040;BBE&#27169;&#25311;&#22120;&#20013;&#65292;&#22312;&#21508;&#31181;&#26465;&#20214;&#21644;&#25237;&#27880;&#24066;&#22330;&#24773;&#26223;&#19979;&#36827;&#34892;&#19968;&#31995;&#21015;&#27604;&#36187;&#30340;&#25237;&#27880;&#65292;&#20197;&#30408;&#21033;&#20316;&#20026;&#20027;&#35201;&#30340;&#27604;&#36739;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;&#26412;&#30740;&#31350;&#30340;&#21021;&#27493;&#21457;&#29616;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
We present first results from the use of XGBoost, a highly effective machine learning (ML) method, within the Bristol Betting Exchange (BBE), an open-source agent-based model (ABM) designed to simulate a contemporary sports-betting exchange with in-play betting during track-racing events such as horse races. We use the BBE ABM and its array of minimally-simple bettor-agents as a synthetic data generator which feeds into our XGBoost ML system, with the intention that XGBoost discovers profitable dynamic betting strategies by learning from the more profitable bets made by the BBE bettor-agents. After this XGBoost training, which results in one or more decision trees, a bettor-agent with a betting strategy determined by the XGBoost-learned decision tree(s) is added to the BBE ABM and made to bet on a sequence of races under various conditions and betting-market scenarios, with profitability serving as the primary metric of comparison and evaluation. Our initial findings presented here sho
&lt;/p&gt;</description></item><item><title>Peridynamic&#31070;&#32463;&#36816;&#31639;&#31526;&#26159;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#38750;&#23616;&#37096;&#26412;&#26500;&#27169;&#22411;&#65292;&#33021;&#22815;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#22797;&#26434;&#26448;&#26009;&#34892;&#20026;&#65292;&#24182;&#25552;&#20379;&#20102;&#20855;&#26377;&#23458;&#35266;&#24615;&#21644;&#21160;&#37327;&#24179;&#34913;&#23450;&#24459;&#30340;&#27491;&#21521;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.06070</link><description>&lt;p&gt;
Peridynamic&#31070;&#32463;&#36816;&#31639;&#31526;&#65306;&#19968;&#31181;&#38754;&#21521;&#22797;&#26434;&#26448;&#26009;&#21709;&#24212;&#30340;&#25968;&#25454;&#39537;&#21160;&#38750;&#23616;&#37096;&#26412;&#26500;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Peridynamic Neural Operators: A Data-Driven Nonlocal Constitutive Model for Complex Material Responses. (arXiv:2401.06070v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06070
&lt;/p&gt;
&lt;p&gt;
Peridynamic&#31070;&#32463;&#36816;&#31639;&#31526;&#26159;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#38750;&#23616;&#37096;&#26412;&#26500;&#27169;&#22411;&#65292;&#33021;&#22815;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#22797;&#26434;&#26448;&#26009;&#34892;&#20026;&#65292;&#24182;&#25552;&#20379;&#20102;&#20855;&#26377;&#23458;&#35266;&#24615;&#21644;&#21160;&#37327;&#24179;&#34913;&#23450;&#24459;&#30340;&#27491;&#21521;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#31070;&#32463;&#36816;&#31639;&#31526;&#20316;&#20026;&#38544;&#34255;&#30340;&#25511;&#21046;&#26041;&#31243;&#30340;&#38544;&#24335;&#35299;&#31639;&#31639;&#23376;&#65292;&#24050;&#32463;&#25104;&#20026;&#23398;&#20064;&#22797;&#26434;&#23454;&#38469;&#29289;&#29702;&#31995;&#32479;&#21709;&#24212;&#30340;&#27969;&#34892;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#22823;&#37096;&#20998;&#31070;&#32463;&#36816;&#31639;&#31526;&#24212;&#29992;&#36804;&#20170;&#20026;&#27490;&#37117;&#26159;&#25968;&#25454;&#39537;&#21160;&#30340;&#65292;&#24573;&#30053;&#20102;&#25968;&#25454;&#20013;&#23545;&#22522;&#26412;&#29289;&#29702;&#35268;&#24459;&#30340;&#22266;&#26377;&#20445;&#23384;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31215;&#20998;&#31070;&#32463;&#36816;&#31639;&#31526;&#26550;&#26500;&#65292;&#31216;&#20026;Peridynamic&#31070;&#32463;&#36816;&#31639;&#31526;&#65288;PNO&#65289;&#65292;&#29992;&#20110;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#38750;&#23616;&#37096;&#26412;&#26500;&#23450;&#24459;&#12290;&#36825;&#20010;&#31070;&#32463;&#36816;&#31639;&#31526;&#25552;&#20379;&#20102;&#19968;&#31181;&#20197;&#29366;&#24577;&#20026;&#22522;&#30784;&#30340;Peridynamics&#27491;&#21521;&#27169;&#22411;&#65292;&#33021;&#22815;&#33258;&#21160;&#20445;&#35777;&#23458;&#35266;&#24615;&#21644;&#21160;&#37327;&#24179;&#34913;&#23450;&#24459;&#12290;&#20316;&#20026;&#24212;&#29992;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#27169;&#22411;&#22312;&#20174;&#21512;&#25104;&#21644;&#23454;&#39564;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#22797;&#26434;&#26448;&#26009;&#34892;&#20026;&#26041;&#38754;&#30340;&#34920;&#29616;&#21644;&#25928;&#26524;&#12290;&#25105;&#20204;&#26174;&#31034;&#65292;&#30001;&#20110;&#20854;&#33021;&#22815;&#25429;&#25417;&#22797;&#26434;&#21709;&#24212;&#65292;&#25105;&#20204;&#23398;&#20064;&#24471;&#21040;&#30340;&#31070;&#32463;&#36816;&#31639;&#31526;&#19982;&#22522;&#32447;&#27169;&#22411;&#30456;&#27604;&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#19978;&#22343;&#26377;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural operators, which can act as implicit solution operators of hidden governing equations, have recently become popular tools for learning the responses of complex real-world physical systems. Nevertheless, most neural operator applications have thus far been data-driven and neglect the intrinsic preservation of fundamental physical laws in data. In this work, we introduce a novel integral neural operator architecture called the Peridynamic Neural Operator (PNO) that learns a nonlocal constitutive law from data. This neural operator provides a forward model in the form of state-based peridynamics, with objectivity and momentum balance laws automatically guaranteed. As applications, we demonstrate the expressivity and efficacy of our model in learning complex material behaviors from both synthetic and experimental data sets. We show that, owing to its ability to capture complex responses, our learned neural operator achieves improved accuracy and efficiency compared to baseline model
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25968;&#25454;&#27745;&#26579;&#38382;&#39064;&#65292;&#20197;&#21450;&#35813;&#27745;&#26579;&#23545;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.06059</link><description>&lt;p&gt;
&#30740;&#31350;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#25454;&#27745;&#26579;
&lt;/p&gt;
&lt;p&gt;
Investigating Data Contamination for Pre-training Language Models. (arXiv:2401.06059v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06059
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25968;&#25454;&#27745;&#26579;&#38382;&#39064;&#65292;&#20197;&#21450;&#35813;&#27745;&#26579;&#23545;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#32593;&#32476;&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#19978;&#23637;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36234;&#26469;&#36234;&#25285;&#24515;&#36825;&#31181;&#33021;&#21147;&#26159;&#21542;&#26159;&#30001;&#20110;&#35780;&#20272;&#25968;&#25454;&#38598;&#34987;&#21253;&#21547;&#22312;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#23548;&#33268;&#30340;&#65292;&#36825;&#31181;&#29616;&#35937;&#34987;&#31216;&#20026;&#8220;&#25968;&#25454;&#27745;&#26579;&#8221;&#65292;&#20174;&#32780;&#22312;&#20154;&#24037;&#25552;&#39640;&#24615;&#33021;&#12290;&#30446;&#21069;&#23545;&#36825;&#31181;&#28508;&#22312;&#27745;&#26579;&#22914;&#20309;&#24433;&#21709;&#35821;&#35328;&#27169;&#22411;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#32570;&#20047;&#20102;&#35299;&#12290;&#26412;&#25991;&#36890;&#36807;&#20174;&#22836;&#24320;&#22987;&#39044;&#35757;&#32451;&#19968;&#31995;&#21015;GPT-2&#27169;&#22411;&#65292;&#25506;&#35752;&#20102;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#25968;&#25454;&#27745;&#26579;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;&#26469;&#33258;&#35780;&#20272;&#25968;&#25454;&#30340;&#25991;&#26412;&#27745;&#26579;&#65288;&#21363;&#36755;&#20837;&#25991;&#26412;&#30340;&#35780;&#20272;&#26679;&#26412;&#65289;&#21644;&#22522;&#20934;&#27745;&#26579;&#65288;&#21363;&#36755;&#20837;&#20013;&#30340;&#25552;&#31034;&#21644;&#26399;&#26395;&#36755;&#20986;&#65289;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#37325;&#22797;&#27745;&#26579;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35843;&#26597;&#20102;&#26222;&#36941;&#23384;&#22312;&#30340;n
&lt;/p&gt;
&lt;p&gt;
Language models pre-trained on web-scale corpora demonstrate impressive capabilities on diverse downstream tasks. However, there is increasing concern whether such capabilities might arise from evaluation datasets being included in the pre-training corpus -- a phenomenon known as \textit{data contamination} -- in a manner that artificially increases performance. There has been little understanding of how this potential contamination might influence LMs' performance on downstream tasks. In this paper, we explore the impact of data contamination at the pre-training stage by pre-training a series of GPT-2 models \textit{from scratch}. We highlight the effect of both text contamination (\textit{i.e.}\ input text of the evaluation samples) and ground-truth contamination (\textit{i.e.}\ the prompts asked on the input and the desired outputs) from evaluation data. We also investigate the effects of repeating contamination for various downstream tasks. Additionally, we examine the prevailing n
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22235;&#31181;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#24615;&#33021;&#20197;&#21450;&#36890;&#36807;&#24212;&#29992;&#19981;&#21516;&#30340;&#20154;&#24037;&#29305;&#24449;&#22686;&#24378;&#31574;&#30053;&#26469;&#25552;&#39640;&#22270;&#20998;&#31867;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;GNN&#26550;&#26500;&#30340;&#35745;&#31639;&#33021;&#21147;&#21644;&#20154;&#24037;&#29305;&#24449;&#25552;&#20379;&#30340;&#20449;&#24687;&#27700;&#24179;&#23545;&#20219;&#21153;&#30340;&#24615;&#33021;&#20855;&#26377;&#24179;&#34913;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.06048</link><description>&lt;p&gt;
&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#29305;&#24449;&#22686;&#24378;&#31574;&#30053;&#26469;&#20998;&#31867;&#31038;&#20132;&#32593;&#32476;&#30340;&#33021;&#21147;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Power of Graph Neural Networks and Feature Augmentation Strategies to Classify Social Networks. (arXiv:2401.06048v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06048
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22235;&#31181;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#24615;&#33021;&#20197;&#21450;&#36890;&#36807;&#24212;&#29992;&#19981;&#21516;&#30340;&#20154;&#24037;&#29305;&#24449;&#22686;&#24378;&#31574;&#30053;&#26469;&#25552;&#39640;&#22270;&#20998;&#31867;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;GNN&#26550;&#26500;&#30340;&#35745;&#31639;&#33021;&#21147;&#21644;&#20154;&#24037;&#29305;&#24449;&#25552;&#20379;&#30340;&#20449;&#24687;&#27700;&#24179;&#23545;&#20219;&#21153;&#30340;&#24615;&#33021;&#20855;&#26377;&#24179;&#34913;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22235;&#31181;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65288;GNNs&#65289;&#22312;&#20351;&#29992;&#32463;&#20856;&#30340;&#32593;&#32476;&#31185;&#23398;&#29983;&#25104;&#27169;&#22411;&#21019;&#24314;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22270;&#20998;&#31867;&#20219;&#21153;&#12290;&#30001;&#20110;&#21512;&#25104;&#32593;&#32476;&#19981;&#21253;&#21547;&#65288;&#33410;&#28857;&#25110;&#36793;&#65289;&#29305;&#24449;&#65292;&#22240;&#27492;&#23545;&#33410;&#28857;&#24212;&#29992;&#20102;&#20116;&#31181;&#19981;&#21516;&#30340;&#22686;&#24378;&#31574;&#30053;&#65288;&#20154;&#36896;&#29305;&#24449;&#31867;&#22411;&#65289;&#12290;&#30740;&#31350;&#20102;4&#31181;GNNs&#65288;&#20855;&#26377;&#23618;&#27425;&#21644;&#20840;&#23616;&#32858;&#21512;&#30340;GCN&#12289;GIN&#21644;GATv2&#65289;&#21644;5&#31181;&#29305;&#24449;&#31867;&#22411;&#65288;&#24120;&#25968;1&#12289;&#22122;&#22768;&#12289;&#24230;&#12289;&#24402;&#19968;&#21270;&#24230;&#21644;ID - &#21508;&#31181;&#38271;&#24230;&#30340;&#24490;&#29615;&#25968;&#37327;&#30340;&#21521;&#37327;&#65289;&#30340;&#25152;&#26377;&#32452;&#21512;&#65292;&#24182;&#22312;GNNs&#20013;&#20351;&#29992;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#38544;&#34255;&#32500;&#24230;&#20316;&#20026;&#20989;&#25968;&#27604;&#36739;&#20854;&#24615;&#33021;&#12290;&#36824;&#20351;&#29992;&#31532;&#20108;&#20010;&#21512;&#25104;&#32593;&#32476;&#25968;&#25454;&#38598;&#20998;&#26512;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65288;&#21253;&#21547;&#19981;&#21516;&#35268;&#27169;&#30340;&#32593;&#32476;&#65289;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;GNN&#26550;&#26500;&#30340;&#35745;&#31639;&#33021;&#21147;&#21644;&#20154;&#24037;&#29305;&#24449;&#25552;&#20379;&#30340;&#20449;&#24687;&#27700;&#24179;&#30340;&#24179;&#34913;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies four Graph Neural Network architectures (GNNs) for a graph classification task on a synthetic dataset created using classic generative models of Network Science. Since the synthetic networks do not contain (node or edge) features, five different augmentation strategies (artificial feature types) are applied to nodes. All combinations of the 4 GNNs (GCN with Hierarchical and Global aggregation, GIN and GATv2) and the 5 feature types (constant 1, noise, degree, normalized degree and ID -- a vector of the number of cycles of various lengths) are studied and their performances compared as a function of the hidden dimension of artificial neural networks used in the GNNs. The generalisation ability of these models is also analysed using a second synthetic network dataset (containing networks of different sizes).Our results point towards the balanced importance of the computational power of the GNN architecture and the the information level provided by the artificial featur
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23567;&#27874;&#21551;&#21457;&#30340;&#22810;&#23610;&#24230;&#22270;&#21367;&#31215;&#24490;&#29615;&#32593;&#32476;&#65292;&#29992;&#20110;&#20132;&#36890;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#23558;&#22810;&#23610;&#24230;&#20998;&#26512;&#26041;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#23545;&#20132;&#36890;&#25968;&#25454;&#20013;&#30340;&#22810;&#23610;&#24230;&#32467;&#26500;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#23637;&#29616;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.06040</link><description>&lt;p&gt;
&#22522;&#20110;&#23567;&#27874;&#21551;&#21457;&#30340;&#22810;&#23610;&#24230;&#22270;&#21367;&#31215;&#24490;&#29615;&#32593;&#32476;&#29992;&#20110;&#20132;&#36890;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Wavelet-Inspired Multiscale Graph Convolutional Recurrent Network for Traffic Forecasting. (arXiv:2401.06040v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06040
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23567;&#27874;&#21551;&#21457;&#30340;&#22810;&#23610;&#24230;&#22270;&#21367;&#31215;&#24490;&#29615;&#32593;&#32476;&#65292;&#29992;&#20110;&#20132;&#36890;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#23558;&#22810;&#23610;&#24230;&#20998;&#26512;&#26041;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#23545;&#20132;&#36890;&#25968;&#25454;&#20013;&#30340;&#22810;&#23610;&#24230;&#32467;&#26500;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#23637;&#29616;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#39044;&#27979;&#26159;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#30340;&#22522;&#30784;&#12290;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#20132;&#36890;&#39044;&#27979;&#20013;&#23637;&#29616;&#20986;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24182;&#27809;&#26377;&#26126;&#30830;&#22320;&#23545;&#20132;&#36890;&#25968;&#25454;&#20013;&#30340;&#26576;&#20123;&#33258;&#28982;&#29305;&#24449;&#36827;&#34892;&#24314;&#27169;&#65292;&#27604;&#22914;&#21253;&#21547;&#19981;&#21516;&#31890;&#24230;&#25110;&#23610;&#24230;&#19978;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#21464;&#21270;&#30340;&#22810;&#23610;&#24230;&#32467;&#26500;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23567;&#27874;&#21551;&#21457;&#30340;&#22270;&#21367;&#31215;&#24490;&#29615;&#32593;&#32476;&#65288;WavGCRN&#65289;&#65292;&#23558;&#22810;&#23610;&#24230;&#20998;&#26512;&#65288;MSA&#65289;&#26041;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#26041;&#27861;&#30456;&#32467;&#21512;&#12290;&#22312;WavGCRN&#20013;&#65292;&#20132;&#36890;&#25968;&#25454;&#36890;&#36807;&#31163;&#25955;&#23567;&#27874;&#21464;&#25442;&#65288;DWT&#65289;&#34987;&#20998;&#35299;&#20026;&#26102;&#39057;&#20998;&#37327;&#65292;&#26500;&#24314;&#20102;&#22810;&#27969;&#36755;&#20837;&#32467;&#26500;&#65307;&#28982;&#21518;&#20351;&#29992;&#22270;&#21367;&#31215;&#24490;&#29615;&#32593;&#32476;&#65288;GCRNs&#65289;&#20316;&#20026;&#32534;&#30721;&#22120;&#23545;&#27599;&#20010;&#27969;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#65292;&#25552;&#21462;&#19981;&#21516;&#23610;&#24230;&#30340;&#26102;&#31354;&#29305;&#24449;&#65307;&#26368;&#21518;&#65292;&#21487;&#23398;&#20064;&#30340;&#36870;DWT&#21644;GCRN&#34987;&#32467;&#21512;&#20026;&#35299;&#30721;&#22120;&#65292;&#34701;&#21512;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic forecasting is the foundation for intelligent transportation systems. Spatiotemporal graph neural networks have demonstrated state-of-the-art performance in traffic forecasting. However, these methods do not explicitly model some of the natural characteristics in traffic data, such as the multiscale structure that encompasses spatial and temporal variations at different levels of granularity or scale. To that end, we propose a Wavelet-Inspired Graph Convolutional Recurrent Network (WavGCRN) which combines multiscale analysis (MSA)-based method with Deep Learning (DL)-based method. In WavGCRN, the traffic data is decomposed into time-frequency components with Discrete Wavelet Transformation (DWT), constructing a multi-stream input structure; then Graph Convolutional Recurrent networks (GCRNs) are employed as encoders for each stream, extracting spatiotemporal features in different scales; and finally the learnable Inversed DWT and GCRN are combined as the decoder, fusing the inf
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#26465;&#20214;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#39640;&#25928;&#30340;&#19977;&#24179;&#38754;&#32593;&#32476;&#20197;&#21450;&#32852;&#21512;&#24103;&#24314;&#27169;&#26041;&#27861;&#21644;&#22522;&#20110;&#20809;&#27969;&#30340;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#12289;&#26102;&#38388;&#36830;&#36143;&#19988;&#26080;&#35270;&#35273;&#20266;&#24433;&#30340;&#35270;&#39057;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2401.06035</link><description>&lt;p&gt;
RAVEN&#65306;&#29992;&#39640;&#25928;&#30340;&#19977;&#24179;&#38754;&#32593;&#32476;&#37325;&#26032;&#24605;&#32771;&#23545;&#25239;&#24615;&#35270;&#39057;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
RAVEN: Rethinking Adversarial Video Generation with Efficient Tri-plane Networks. (arXiv:2401.06035v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06035
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#26465;&#20214;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#39640;&#25928;&#30340;&#19977;&#24179;&#38754;&#32593;&#32476;&#20197;&#21450;&#32852;&#21512;&#24103;&#24314;&#27169;&#26041;&#27861;&#21644;&#22522;&#20110;&#20809;&#27969;&#30340;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#12289;&#26102;&#38388;&#36830;&#36143;&#19988;&#26080;&#35270;&#35273;&#20266;&#24433;&#30340;&#35270;&#39057;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#26465;&#20214;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#65292;&#26088;&#22312;&#35299;&#20915;&#38271;&#26399;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#20381;&#36182;&#24615;&#12290;&#20026;&#20102;&#25429;&#25417;&#36825;&#20123;&#20381;&#36182;&#20851;&#31995;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#21463;&#21040;&#19977;&#32500;&#24863;&#30693;&#29983;&#25104;&#26694;&#26550;&#21551;&#21457;&#30340;&#28151;&#21512;&#26174;&#24335;-&#38544;&#24335;&#19977;&#24179;&#38754;&#34920;&#31034;&#27861;&#24182;&#20837;&#65292;&#24182;&#20351;&#29992;&#19968;&#20010;&#21807;&#19968;&#30340;&#28508;&#22312;&#32534;&#30721;&#26469;&#24314;&#27169;&#25972;&#20010;&#35270;&#39057;&#24207;&#21015;&#12290;&#28982;&#21518;&#65292;&#20174;&#20013;&#38388;&#19977;&#24179;&#38754;&#34920;&#31034;&#27861;&#20013;&#21512;&#25104;&#21333;&#20010;&#35270;&#39057;&#24103;&#65292;&#35813;&#34920;&#31034;&#27861;&#26412;&#36523;&#26159;&#20174;&#20027;&#35201;&#28508;&#22312;&#32534;&#30721;&#20013;&#27966;&#29983;&#30340;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#31574;&#30053;&#23558;&#35745;&#31639;&#22797;&#26434;&#24230;&#20943;&#23569;&#20102;2&#20493;&#65292;&#20197;FLOPs&#24230;&#37327;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20415;&#20110;&#39640;&#25928;&#21644;&#26102;&#38388;&#36830;&#36143;&#22320;&#29983;&#25104;&#35270;&#39057;&#12290;&#27492;&#22806;&#65292;&#19982;&#33258;&#22238;&#24402;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#32852;&#21512;&#24103;&#24314;&#27169;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;&#35270;&#35273;&#20266;&#24433;&#30340;&#20135;&#29983;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#20013;&#38598;&#25104;&#22522;&#20110;&#20809;&#27969;&#30340;&#27169;&#22359;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel unconditional video generative model designed to address long-term spatial and temporal dependencies. To capture these dependencies, our approach incorporates a hybrid explicit-implicit tri-plane representation inspired by 3D-aware generative frameworks developed for three-dimensional object representation and employs a singular latent code to model an entire video sequence. Individual video frames are then synthesized from an intermediate tri-plane representation, which itself is derived from the primary latent code. This novel strategy reduces computational complexity by a factor of $2$ as measured in FLOPs. Consequently, our approach facilitates the efficient and temporally coherent generation of videos. Moreover, our joint frame modeling approach, in contrast to autoregressive methods, mitigates the generation of visual artifacts. We further enhance the model's capabilities by integrating an optical flow-based module within our Generative Adversarial Network (GAN
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24182;&#34892;&#22810;&#20809;&#35889;&#21644;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#22270;&#20687;&#36827;&#34892;&#28023;&#20912;&#26816;&#27979;&#30340;&#26032;&#24037;&#20855;&#65288;ViSual_IceD&#65289;&#65292;&#35813;&#24037;&#20855;&#36890;&#36807;&#34701;&#21512;&#22810;&#20809;&#35889;&#22270;&#20687;&#21644;SAR&#22270;&#20687;&#26469;&#35299;&#20915;&#28023;&#20912;&#26816;&#27979;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.06009</link><description>&lt;p&gt;
&#20351;&#29992;&#24182;&#34892;&#22810;&#20809;&#35889;&#21644;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#22270;&#20687;&#36827;&#34892;&#28023;&#20912;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Sea ice detection using concurrent multispectral and synthetic aperture radar imagery. (arXiv:2401.06009v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06009
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24182;&#34892;&#22810;&#20809;&#35889;&#21644;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#22270;&#20687;&#36827;&#34892;&#28023;&#20912;&#26816;&#27979;&#30340;&#26032;&#24037;&#20855;&#65288;ViSual_IceD&#65289;&#65292;&#35813;&#24037;&#20855;&#36890;&#36807;&#34701;&#21512;&#22810;&#20809;&#35889;&#22270;&#20687;&#21644;SAR&#22270;&#20687;&#26469;&#35299;&#20915;&#28023;&#20912;&#26816;&#27979;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#65288;SAR&#65289;&#22270;&#20687;&#26159;&#28023;&#20912;&#26144;&#23556;&#30340;&#20027;&#35201;&#25968;&#25454;&#31867;&#22411;&#65292;&#22240;&#20026;&#23427;&#20855;&#26377;&#26102;&#31354;&#35206;&#30422;&#33021;&#21147;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#20113;&#21644;&#20809;&#29031;&#26465;&#20214;&#19981;&#21033;&#26102;&#26816;&#27979;&#28023;&#20912;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22270;&#20687;&#20013;&#23384;&#22312;&#27169;&#31946;&#30340;&#20449;&#21495;&#21644;&#22122;&#22768;&#65292;&#20351;&#29992;SAR&#22270;&#20687;&#36827;&#34892;&#33258;&#21160;&#28023;&#20912;&#26816;&#27979;&#20173;&#28982;&#23384;&#22312;&#38382;&#39064;&#12290;&#30456;&#21453;&#65292;&#20351;&#29992;&#22810;&#20809;&#35889;&#22270;&#20687;&#65288;MSI&#65289;&#21487;&#20197;&#36731;&#26494;&#21306;&#20998;&#20912;&#21644;&#27700;&#65292;&#20294;&#22312;&#26497;&#22320;&#22320;&#21306;&#65292;&#28023;&#27915;&#34920;&#38754;&#32463;&#24120;&#34987;&#20113;&#23618;&#36974;&#25377;&#65292;&#38451;&#20809;&#21487;&#33021;&#22312;&#25968;&#26376;&#20869;&#19981;&#20986;&#29616;&#22312;&#22320;&#24179;&#32447;&#19978;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24182;&#34892;&#22810;&#20809;&#35889;&#21487;&#35265;&#20809;&#21644;SAR&#22270;&#20687;&#36827;&#34892;&#28023;&#20912;&#26816;&#27979;&#30340;&#26032;&#24037;&#20855;&#65288;ViSual_IceD&#65289;&#12290;ViSual_IceD&#26159;&#19968;&#20010;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#65292;&#23427;&#22312;&#32463;&#20856;&#30340;U-Net&#20307;&#31995;&#32467;&#26500;&#30340;&#22522;&#30784;&#19978;&#26500;&#24314;&#65292;&#21253;&#21547;&#20004;&#20010;&#24182;&#34892;&#30340;&#32534;&#30721;&#22120;&#38454;&#27573;&#65292;&#33021;&#22815;&#34701;&#21512;&#21644;&#25340;&#25509;&#20855;&#26377;&#19981;&#21516;&#31354;&#38388;&#20998;&#36776;&#29575;&#30340;MSI&#21644;SAR&#22270;&#20687;&#12290;ViSual_IceD&#30340;&#24615;&#33021;&#32463;&#36807;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Synthetic Aperture Radar (SAR) imagery is the primary data type used for sea ice mapping due to its spatio-temporal coverage and the ability to detect sea ice independent of cloud and lighting conditions. Automatic sea ice detection using SAR imagery remains problematic due to the presence of ambiguous signal and noise within the image. Conversely, ice and water are easily distinguishable using multispectral imagery (MSI), but in the polar regions the ocean's surface is often occluded by cloud or the sun may not appear above the horizon for many months. To address some of these limitations, this paper proposes a new tool trained using concurrent multispectral Visible and SAR imagery for sea Ice Detection (ViSual\_IceD). ViSual\_IceD is a convolution neural network (CNN) that builds on the classic U-Net architecture by containing two parallel encoder stages, enabling the fusion and concatenation of MSI and SAR imagery containing different spatial resolutions. The performance of ViSual\_
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#38750;&#20154;&#28789;&#38271;&#31867;&#21160;&#29289;&#22823;&#33041;&#22914;&#20309;&#22312;&#35270;&#35273;&#20013;&#32467;&#21512;&#29983;&#25104;&#24335;&#21644;&#21028;&#21035;&#24335;&#35745;&#31639;&#12290;&#19968;&#20010;&#35266;&#24565;&#24378;&#35843;&#33258;&#19979;&#32780;&#19978;&#30340;&#20449;&#21495;&#27969;&#21160;&#65292;&#36890;&#36807;&#28388;&#38500;&#19981;&#30456;&#20851;&#30340;&#21464;&#24322;&#21644;&#36716;&#25442;&#35270;&#35273;&#20449;&#24687;&#26469;&#20195;&#34920;&#34892;&#20026;&#19978;&#30456;&#20851;&#30340;&#20449;&#24687;&#65307;&#32780;&#21478;&#19968;&#20010;&#35266;&#24565;&#23558;&#35270;&#35273;&#35270;&#20026;Helmholtz&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2401.06005</link><description>&lt;p&gt;
&#38750;&#20154;&#28789;&#38271;&#31867;&#21160;&#29289;&#22823;&#33041;&#22914;&#20309;&#22312;&#35270;&#35273;&#20013;&#32467;&#21512;&#29983;&#25104;&#24335;&#21644;&#21028;&#21035;&#24335;&#35745;&#31639;&#65311;
&lt;/p&gt;
&lt;p&gt;
How does the primate brain combine generative and discriminative computations in vision?. (arXiv:2401.06005v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06005
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#38750;&#20154;&#28789;&#38271;&#31867;&#21160;&#29289;&#22823;&#33041;&#22914;&#20309;&#22312;&#35270;&#35273;&#20013;&#32467;&#21512;&#29983;&#25104;&#24335;&#21644;&#21028;&#21035;&#24335;&#35745;&#31639;&#12290;&#19968;&#20010;&#35266;&#24565;&#24378;&#35843;&#33258;&#19979;&#32780;&#19978;&#30340;&#20449;&#21495;&#27969;&#21160;&#65292;&#36890;&#36807;&#28388;&#38500;&#19981;&#30456;&#20851;&#30340;&#21464;&#24322;&#21644;&#36716;&#25442;&#35270;&#35273;&#20449;&#24687;&#26469;&#20195;&#34920;&#34892;&#20026;&#19978;&#30456;&#20851;&#30340;&#20449;&#24687;&#65307;&#32780;&#21478;&#19968;&#20010;&#35266;&#24565;&#23558;&#35270;&#35273;&#35270;&#20026;Helmholtz&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#34987;&#24191;&#27867;&#29702;&#35299;&#20026;&#19968;&#31181;&#25512;&#29702;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#29983;&#29289;&#35270;&#35273;&#30740;&#31350;&#21644;&#26426;&#22120;&#35270;&#35273;&#24037;&#31243;&#20013;&#65292;&#20851;&#20110;&#25512;&#29702;&#36807;&#31243;&#30340;&#20004;&#31181;&#23545;&#31435;&#35266;&#24565;&#37117;&#20855;&#26377;&#24433;&#21709;&#21147;&#12290;&#31532;&#19968;&#31181;&#24378;&#35843;&#33258;&#19979;&#32780;&#19978;&#30340;&#20449;&#21495;&#27969;&#21160;&#65292;&#23558;&#35270;&#35273;&#25551;&#36848;&#20026;&#19968;&#31181;&#20027;&#35201;&#26159;&#21069;&#39304;&#30340;&#21028;&#21035;&#25512;&#29702;&#36807;&#31243;&#65292;&#20854;&#36890;&#36807;&#28388;&#38500;&#19981;&#30456;&#20851;&#30340;&#21464;&#24322;&#21644;&#20197;&#36866;&#21512;&#19979;&#28216;&#30340;&#35748;&#30693;&#21644;&#34892;&#20026;&#25511;&#21046;&#21151;&#33021;&#38656;&#35201;&#30340;&#26041;&#24335;&#26469;&#36716;&#25442;&#35270;&#35273;&#20449;&#24687;&#65292;&#24182;&#20195;&#34920;&#34892;&#20026;&#19978;&#30456;&#20851;&#20449;&#24687;&#12290;&#22312;&#36825;&#31181;&#35266;&#24565;&#20013;&#65292;&#35270;&#35273;&#30001;&#24863;&#23448;&#25968;&#25454;&#39537;&#21160;&#65292;&#24863;&#30693;&#26159;&#30452;&#25509;&#30340;&#65292;&#22240;&#20026;&#22788;&#29702;&#20174;&#25968;&#25454;&#21040;&#24863;&#20852;&#36259;&#30340;&#28508;&#21464;&#37327;&#30340;&#36807;&#31243;&#36827;&#34892;&#12290;&#22312;&#36825;&#31181;&#35266;&#24565;&#20013;&#65292;"&#25512;&#29702;"&#30340;&#27010;&#24565;&#31867;&#20284;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#24037;&#31243;&#25991;&#29486;&#20013;&#25551;&#36848;&#30340;&#65292;&#23545;&#22270;&#20687;&#36827;&#34892;&#22788;&#29702;&#30340;&#21069;&#39304;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#34987;&#35748;&#20026;&#26159;&#22312;&#25191;&#34892;&#25512;&#29702;&#12290;&#32780;&#21478;&#19968;&#31181;&#35266;&#24565;&#26159;&#23558;&#35270;&#35273;&#35270;&#20026;Helmholtz&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision is widely understood as an inference problem. However, two contrasting conceptions of the inference process have each been influential in research on biological vision as well as the engineering of machine vision. The first emphasizes bottom-up signal flow, describing vision as a largely feedforward, discriminative inference process that filters and transforms the visual information to remove irrelevant variation and represent behaviorally relevant information in a format suitable for downstream functions of cognition and behavioral control. In this conception, vision is driven by the sensory data, and perception is direct because the processing proceeds from the data to the latent variables of interest. The notion of "inference" in this conception is that of the engineering literature on neural networks, where feedforward convolutional neural networks processing images are said to perform inference. The alternative conception is that of vision as an inference process in Helmhol
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26641;&#30340;&#21487;&#21464;&#31995;&#25968;&#27169;&#22411;&#65292;&#20351;&#29992;&#24490;&#29615;&#26799;&#24230;&#25552;&#21319;&#26426;&#36827;&#34892;&#24314;&#27169;&#65292;&#23454;&#29616;&#20102;&#36880;&#32500;&#26089;&#20572;&#21644;&#29305;&#24449;&#37325;&#35201;&#24615;&#35780;&#20998;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#20135;&#29983;&#19982;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;VCM&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.05982</link><description>&lt;p&gt;
&#22522;&#20110;&#26641;&#30340;&#21487;&#21464;&#31995;&#25968;&#27169;&#22411;&#20171;&#32461;
&lt;/p&gt;
&lt;p&gt;
A tree-based varying coefficient model. (arXiv:2401.05982v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05982
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26641;&#30340;&#21487;&#21464;&#31995;&#25968;&#27169;&#22411;&#65292;&#20351;&#29992;&#24490;&#29615;&#26799;&#24230;&#25552;&#21319;&#26426;&#36827;&#34892;&#24314;&#27169;&#65292;&#23454;&#29616;&#20102;&#36880;&#32500;&#26089;&#20572;&#21644;&#29305;&#24449;&#37325;&#35201;&#24615;&#35780;&#20998;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#20135;&#29983;&#19982;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;VCM&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26641;&#30340;&#21487;&#21464;&#31995;&#25968;&#27169;&#22411;(VCM)&#65292;&#20854;&#20013;&#21487;&#21464;&#31995;&#25968;&#20351;&#29992;Delong&#31561;&#20154;(2023)&#30340;&#24490;&#29615;&#26799;&#24230;&#25552;&#21319;&#26426;(CGBM)&#36827;&#34892;&#24314;&#27169;&#12290;&#20351;&#29992;CGBM&#23545;&#31995;&#25968;&#20989;&#25968;&#36827;&#34892;&#24314;&#27169;&#65292;&#21487;&#20197;&#36827;&#34892;&#36880;&#32500;&#26089;&#20572;&#21644;&#29305;&#24449;&#37325;&#35201;&#24615;&#35780;&#20998;&#12290;&#36880;&#32500;&#26089;&#20572;&#19981;&#20165;&#21487;&#20197;&#20943;&#23569;&#32500;&#24230;&#29305;&#23450;&#30340;&#36807;&#25311;&#21512;&#39118;&#38505;&#65292;&#36824;&#21487;&#20197;&#25581;&#31034;&#32500;&#24230;&#20043;&#38388;&#27169;&#22411;&#22797;&#26434;&#24615;&#30340;&#24046;&#24322;&#12290;&#20351;&#29992;&#29305;&#24449;&#37325;&#35201;&#24615;&#35780;&#20998;&#21487;&#20197;&#36827;&#34892;&#31616;&#21333;&#30340;&#29305;&#24449;&#36873;&#25321;&#21644;&#26131;&#20110;&#35299;&#37322;&#30340;&#27169;&#22411;&#35299;&#37322;&#12290;&#35813;&#27169;&#22411;&#22312;Richman&#21644;W&#252;thrich&#65288;2023&#65289;&#20351;&#29992;&#30340;&#30456;&#21516;&#30340;&#27169;&#25311;&#21644;&#30495;&#23454;&#25968;&#25454;&#31034;&#20363;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#23427;&#22312;&#26679;&#26412;&#22806;&#25439;&#22833;&#26041;&#38754;&#20135;&#29983;&#20102;&#19982;&#20182;&#20204;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;VCM LocalGLMnet&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper introduces a tree-based varying coefficient model (VCM) where the varying coefficients are modelled using the cyclic gradient boosting machine (CGBM) from Delong et al. (2023). Modelling the coefficient functions using a CGBM allows for dimension-wise early stopping and feature importance scores. The dimension-wise early stopping not only reduces the risk of dimension-specific overfitting, but also reveals differences in model complexity across dimensions. The use of feature importance scores allows for simple feature selection and easy model interpretation. The model is evaluated on the same simulated and real data examples as those used in Richman and W\"uthrich (2023), and the results show that it produces results in terms of out of sample loss that are comparable to those of their neural network-based VCM called LocalGLMnet.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#38750;&#20405;&#20837;&#24335;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#65288;SciML&#65289;&#20013;&#30340;operator inference&#65288;OpInf&#65289;&#26041;&#27861;&#65292;&#20174;&#25968;&#25454;&#20013;&#26500;&#24314;&#22522;&#20110;&#29289;&#29702;&#30340;&#20302;&#25104;&#26412;&#31616;&#21270;&#27169;&#22411;&#65288;ROMs&#65289;&#65292;&#29992;&#20110;&#38750;&#32447;&#24615;&#31561;&#31163;&#23376;&#20307;&#28237;&#27969;&#27169;&#25311;&#30340;Hasegawa-Wakatani&#26041;&#31243;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#22788;&#29702;&#22797;&#26434;&#12289;&#38750;&#32447;&#24615;&#21644;&#33258;&#39537;&#21160;&#21160;&#21147;&#23398;&#27169;&#22411;&#26102;&#20855;&#26377;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.05972</link><description>&lt;p&gt;
&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#22522;&#20110;&#29289;&#29702;&#30340;&#31616;&#21270;&#27169;&#22411;&#65292;&#29992;&#20110;Hasegawa-Wakatani&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Learning physics-based reduced models from data for the Hasegawa-Wakatani equations. (arXiv:2401.05972v1 [physics.comp-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05972
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#38750;&#20405;&#20837;&#24335;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#65288;SciML&#65289;&#20013;&#30340;operator inference&#65288;OpInf&#65289;&#26041;&#27861;&#65292;&#20174;&#25968;&#25454;&#20013;&#26500;&#24314;&#22522;&#20110;&#29289;&#29702;&#30340;&#20302;&#25104;&#26412;&#31616;&#21270;&#27169;&#22411;&#65288;ROMs&#65289;&#65292;&#29992;&#20110;&#38750;&#32447;&#24615;&#31561;&#31163;&#23376;&#20307;&#28237;&#27969;&#27169;&#25311;&#30340;Hasegawa-Wakatani&#26041;&#31243;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#22788;&#29702;&#22797;&#26434;&#12289;&#38750;&#32447;&#24615;&#21644;&#33258;&#39537;&#21160;&#21160;&#21147;&#23398;&#27169;&#22411;&#26102;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#38750;&#20405;&#20837;&#24335;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#65288;SciML&#65289;&#30340;&#32422;&#20943;&#27169;&#22411;&#65288;ROMs&#65289;&#26500;&#24314;&#65292;&#29992;&#20110;&#38750;&#32447;&#24615;&#12289;&#28151;&#27788;&#31561;&#31163;&#23376;&#20307;&#28237;&#27969;&#27169;&#25311;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;operator inference&#65288;OpInf&#65289;&#20174;&#25968;&#25454;&#20013;&#26500;&#24314;&#22522;&#20110;&#29289;&#29702;&#30340;&#20302;&#25104;&#26412;ROMs&#29992;&#20110;&#36825;&#31181;&#27169;&#25311;&#12290;&#20197;Hasegawa-Wakatani&#65288;HW&#65289;&#26041;&#31243;&#20026;&#20195;&#34920;&#65292;&#22312;&#24418;&#25104;&#22797;&#26434;&#12289;&#38750;&#32447;&#24615;&#21644;&#33258;&#39537;&#21160;&#21160;&#21147;&#23398;&#30340;&#24773;&#20917;&#19979;&#32771;&#23519;&#20102;OpInf&#26500;&#24314;&#20934;&#30830;ROMs&#30340;&#28508;&#21147;&#65292;&#24182;&#36827;&#34892;&#20102;&#20004;&#32452;&#23454;&#39564;&#12290;&#31532;&#19968;&#32452;&#23454;&#39564;&#21033;&#29992;&#36890;&#36807;&#30452;&#25509;&#25968;&#20540;&#27169;&#25311;&#20174;&#29305;&#23450;&#21021;&#20540;&#26465;&#20214;&#24320;&#22987;&#30340;HW&#26041;&#31243;&#33719;&#24471;&#30340;&#25968;&#25454;&#65292;&#36827;&#34892;OpInf ROMs&#30340;&#35757;&#32451;&#20197;&#23454;&#29616;&#36229;&#36234;&#35757;&#32451;&#26102;&#38388;&#33539;&#22260;&#30340;&#39044;&#27979;&#12290;&#22312;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#31532;&#20108;&#32452;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#30456;&#21516;&#30340;&#25968;&#25454;&#38598;&#23545;ROMs&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper focuses on the construction of non-intrusive Scientific Machine Learning (SciML) Reduced-Order Models (ROMs) for nonlinear, chaotic plasma turbulence simulations. In particular, we propose using Operator Inference (OpInf) to build low-cost physics-based ROMs from data for such simulations. As a representative example, we focus on the Hasegawa-Wakatani (HW) equations used for modeling two-dimensional electrostatic drift-wave plasma turbulence. For a comprehensive perspective of the potential of OpInf to construct accurate ROMs for this model, we consider a setup for the HW equations that leads to the formation of complex, nonlinear, and self-driven dynamics, and perform two sets of experiments. We first use the data obtained via a direct numerical simulation of the HW equations starting from a specific initial condition and train OpInf ROMs for predictions beyond the training time horizon. In the second, more challenging set of experiments, we train ROMs using the same datase
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SATOP&#30340;&#31354;&#38388;&#24863;&#30693;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#24033;&#35686;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#20572;&#36710;&#20301;&#12289;&#20195;&#29702;&#21644;&#21160;&#20316;&#20043;&#38388;&#30340;&#31354;&#38388;&#20851;&#31995;&#26469;&#21019;&#24314;&#21160;&#20316;&#30340;&#34920;&#31034;&#65292;&#20174;&#32780;&#21160;&#24577;&#35843;&#25972;&#20197;&#36866;&#24212;&#24403;&#21069;&#21487;&#32602;&#27454;&#30340;&#20572;&#36710;&#36829;&#35268;&#34892;&#20026;&#65292;&#24182;&#25552;&#21069;&#35745;&#21010;&#20197;&#25552;&#39640;&#21040;&#36798;&#36829;&#35268;&#34892;&#20026;&#21457;&#29983;&#26102;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.05969</link><description>&lt;p&gt;
&#31354;&#38388;&#24863;&#30693;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#24033;&#35686;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Spatial-Aware Deep Reinforcement Learning for the Traveling Officer Problem. (arXiv:2401.05969v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05969
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SATOP&#30340;&#31354;&#38388;&#24863;&#30693;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#24033;&#35686;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#20572;&#36710;&#20301;&#12289;&#20195;&#29702;&#21644;&#21160;&#20316;&#20043;&#38388;&#30340;&#31354;&#38388;&#20851;&#31995;&#26469;&#21019;&#24314;&#21160;&#20316;&#30340;&#34920;&#31034;&#65292;&#20174;&#32780;&#21160;&#24577;&#35843;&#25972;&#20197;&#36866;&#24212;&#24403;&#21069;&#21487;&#32602;&#27454;&#30340;&#20572;&#36710;&#36829;&#35268;&#34892;&#20026;&#65292;&#24182;&#25552;&#21069;&#35745;&#21010;&#20197;&#25552;&#39640;&#21040;&#36798;&#36829;&#35268;&#34892;&#20026;&#21457;&#29983;&#26102;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24033;&#35686;&#38382;&#39064;&#65288;TOP&#65289;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38543;&#26426;&#20248;&#21270;&#20219;&#21153;&#12290;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#65292;&#19968;&#21517;&#20572;&#36710;&#21592;&#36890;&#36807;&#37197;&#22791;&#20572;&#36710;&#20256;&#24863;&#22120;&#30340;&#22478;&#24066;&#36827;&#34892;&#24341;&#23548;&#65292;&#20197;&#20415;&#23613;&#21487;&#33021;&#22810;&#22320;&#32602;&#27454;&#36829;&#35268;&#20572;&#36710;&#32773;&#12290;TOP&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#20572;&#36710;&#36829;&#35268;&#30340;&#21160;&#24577;&#24615;&#65292;&#36825;&#20123;&#36829;&#35268;&#34892;&#20026;&#20250;&#22312;&#19968;&#27573;&#26102;&#38388;&#21518;&#38543;&#26426;&#20986;&#29616;&#21644;&#28040;&#22833;&#65292;&#19981;&#35770;&#23427;&#20204;&#26159;&#21542;&#34987;&#32602;&#27454;&#12290;&#22240;&#27492;&#65292;&#35299;&#20915;&#26041;&#26696;&#38656;&#35201;&#21160;&#24577;&#22320;&#35843;&#25972;&#20197;&#36866;&#24212;&#24403;&#21069;&#21487;&#32602;&#27454;&#30340;&#20572;&#36710;&#36829;&#35268;&#34892;&#20026;&#65292;&#21516;&#26102;&#36824;&#35201;&#25552;&#21069;&#35745;&#21010;&#65292;&#22686;&#21152;&#24033;&#35686;&#22312;&#36829;&#35268;&#34892;&#20026;&#21457;&#29983;&#26102;&#21040;&#36798;&#30340;&#21487;&#33021;&#24615;&#12290;&#23613;&#31649;&#23384;&#22312;&#21508;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#38590;&#20197;&#32771;&#34385;&#21040;&#34892;&#21160;&#23545;&#26410;&#26469;&#32602;&#27454;&#36829;&#35268;&#30340;&#33021;&#21147;&#20135;&#29983;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;SATOP&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#31354;&#38388;&#24863;&#30693;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;TOP&#12290;&#25105;&#20204;&#30340;&#26032;&#22411;&#29366;&#24577;&#32534;&#30721;&#22120;&#21019;&#24314;&#20102;&#27599;&#20010;&#21160;&#20316;&#30340;&#34920;&#31034;&#65292;&#21033;&#29992;&#20572;&#36710;&#20301;&#12289;&#20195;&#29702;&#21644;&#21160;&#20316;&#20043;&#38388;&#30340;&#31354;&#38388;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
The traveling officer problem (TOP) is a challenging stochastic optimization task. In this problem, a parking officer is guided through a city equipped with parking sensors to fine as many parking offenders as possible. A major challenge in TOP is the dynamic nature of parking offenses, which randomly appear and disappear after some time, regardless of whether they have been fined. Thus, solutions need to dynamically adjust to currently fineable parking offenses while also planning ahead to increase the likelihood that the officer arrives during the offense taking place. Though various solutions exist, these methods often struggle to take the implications of actions on the ability to fine future parking violations into account. This paper proposes SATOP, a novel spatial-aware deep reinforcement learning approach for TOP. Our novel state encoder creates a representation of each action, leveraging the spatial relationships between parking spots, the agent, and the action. Furthermore, we
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23581;&#35797;&#20351;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#29983;&#25104;&#26032;&#30340;&#26725;&#26753;&#31867;&#22411;&#65292;&#36890;&#36807;&#23545;&#28508;&#22312;&#31354;&#38388;&#36827;&#34892;&#37319;&#26679;&#65292;&#21487;&#20197;&#22312;&#20154;&#31867;&#21407;&#22987;&#26725;&#26753;&#31867;&#22411;&#30340;&#22522;&#30784;&#19978;&#32452;&#21512;&#19981;&#21516;&#30340;&#32467;&#26500;&#32452;&#20214;&#65292;&#21019;&#36896;&#20855;&#26377;&#19968;&#23450;&#21019;&#26032;&#33021;&#21147;&#30340;&#26032;&#26725;&#26753;&#31867;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.05964</link><description>&lt;p&gt;
&#20174;PixelCNN&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#23581;&#35797;&#29983;&#25104;&#26032;&#30340;&#26725;&#26753;&#31867;&#22411;
&lt;/p&gt;
&lt;p&gt;
An attempt to generate new bridge types from latent space of PixelCNN. (arXiv:2401.05964v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05964
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23581;&#35797;&#20351;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#29983;&#25104;&#26032;&#30340;&#26725;&#26753;&#31867;&#22411;&#65292;&#36890;&#36807;&#23545;&#28508;&#22312;&#31354;&#38388;&#36827;&#34892;&#37319;&#26679;&#65292;&#21487;&#20197;&#22312;&#20154;&#31867;&#21407;&#22987;&#26725;&#26753;&#31867;&#22411;&#30340;&#22522;&#30784;&#19978;&#32452;&#21512;&#19981;&#21516;&#30340;&#32467;&#26500;&#32452;&#20214;&#65292;&#21019;&#36896;&#20855;&#26377;&#19968;&#23450;&#21019;&#26032;&#33021;&#21147;&#30340;&#26032;&#26725;&#26753;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23581;&#35797;&#20351;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#29983;&#25104;&#26032;&#30340;&#26725;&#26753;&#31867;&#22411;&#12290;&#20351;&#29992;&#19977;&#36328;&#26753;&#26725;&#12289;&#25329;&#26725;&#12289;&#26012;&#25289;&#26725;&#21644;&#24748;&#32034;&#26725;&#30340;&#23545;&#31216;&#32467;&#26500;&#22270;&#20687;&#25968;&#25454;&#38598;&#20316;&#20026;&#35757;&#32451;&#38598;&#65292;&#22522;&#20110;Python&#32534;&#31243;&#35821;&#35328;&#12289;TensorFlow&#21644;Keras&#28145;&#24230;&#23398;&#20064;&#24179;&#21488;&#26500;&#24314;&#21644;&#35757;&#32451;PixelCNN&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#25429;&#25417;&#22270;&#20687;&#30340;&#32479;&#35745;&#32467;&#26500;&#65292;&#24182;&#22312;&#32473;&#23450;&#21069;&#19968;&#20687;&#32032;&#30340;&#24773;&#20917;&#19979;&#35745;&#31639;&#19979;&#19968;&#20010;&#20687;&#32032;&#30340;&#27010;&#29575;&#20998;&#24067;&#12290;&#36890;&#36807;&#23545;&#33719;&#24471;&#30340;&#28508;&#22312;&#31354;&#38388;&#36827;&#34892;&#37319;&#26679;&#65292;&#21487;&#20197;&#29983;&#25104;&#19982;&#35757;&#32451;&#38598;&#20013;&#19981;&#21516;&#30340;&#26725;&#26753;&#31867;&#22411;&#12290;PixelCNN&#21487;&#20197;&#22312;&#20154;&#31867;&#21407;&#22987;&#26725;&#26753;&#31867;&#22411;&#30340;&#22522;&#30784;&#19978;&#26377;&#26426;&#22320;&#32452;&#21512;&#19981;&#21516;&#30340;&#32467;&#26500;&#32452;&#20214;&#65292;&#21019;&#24314;&#20855;&#26377;&#19968;&#23450;&#20154;&#31867;&#21019;&#26032;&#33021;&#21147;&#30340;&#26032;&#26725;&#26753;&#31867;&#22411;&#12290;&#33258;&#22238;&#24402;&#27169;&#22411;&#26080;&#27861;&#29702;&#35299;&#24207;&#21015;&#30340;&#21547;&#20041;&#65292;&#32780;&#22810;&#27169;&#24577;&#27169;&#22411;&#32467;&#21512;&#22238;&#24402;&#21644;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#33021;&#22815;&#29702;&#35299;&#24207;&#21015;&#12290;&#22810;&#27169;&#24577;&#27169;&#22411;&#24212;&#35813;&#26159;...
&lt;/p&gt;
&lt;p&gt;
Try to generate new bridge types using generative artificial intelligence technology. Using symmetric structured image dataset of three-span beam bridge, arch bridge, cable-stayed bridge and suspension bridge , based on Python programming language, TensorFlow and Keras deep learning platform framework , PixelCNN is constructed and trained. The model can capture the statistical structure of the images and calculate the probability distribution of the next pixel when the previous pixels are given. From the obtained latent space sampling, new bridge types different from the training dataset can be generated. PixelCNN can organically combine different structural components on the basis of human original bridge types, creating new bridge types that have a certain degree of human original ability. Autoregressive models cannot understand the meaning of the sequence, while multimodal models combine regression and autoregressive models to understand the sequence. Multimodal models should be the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;Transformer&#34920;&#31034;&#20013;&#23398;&#20064;&#35748;&#30693;&#22320;&#22270;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#38024;&#23545;&#37096;&#20998;&#35266;&#23519;&#29615;&#22659;&#20013;&#30340;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2401.05946</link><description>&lt;p&gt;
&#20174;Transformer&#34920;&#31034;&#20013;&#23398;&#20064;&#35748;&#30693;&#22320;&#22270;&#20197;&#23454;&#29616;&#37096;&#20998;&#35266;&#23519;&#29615;&#22659;&#20013;&#30340;&#39640;&#25928;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Learning Cognitive Maps from Transformer Representations for Efficient Planning in Partially Observed Environments. (arXiv:2401.05946v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05946
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;Transformer&#34920;&#31034;&#20013;&#23398;&#20064;&#35748;&#30693;&#22320;&#22270;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#38024;&#23545;&#37096;&#20998;&#35266;&#23519;&#29615;&#22659;&#20013;&#30340;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#21253;&#25324;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#20165;&#36879;&#38706;&#30340;&#19978;&#19979;&#25991;&#20219;&#21153;&#65292;&#20294;&#26631;&#20934;&#30340;Transformer&#21644;&#38024;&#23545;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#35757;&#32451;&#30340;&#21464;&#20307;(a)&#27809;&#26377;&#23398;&#20064;&#21040;&#26126;&#30830;&#30340;&#29615;&#22659;&#19990;&#30028;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#28789;&#27963;&#26597;&#35810;&#65292;(b)&#19981;&#33021;&#29992;&#20110;&#35268;&#21010;&#25110;&#23548;&#33322;&#12290;&#26412;&#25991;&#32771;&#34385;&#37096;&#20998;&#35266;&#23519;&#29615;&#22659;&#65288;POEs&#65289;&#65292;&#20195;&#29702;&#26681;&#25454;&#20854;&#23548;&#33322;&#26102;&#25509;&#25910;&#21040;&#30340;&#24863;&#30693;&#21035;&#21517;&#35266;&#23519;&#65292;&#36825;&#20351;&#24471;&#36335;&#24452;&#35268;&#21010;&#21464;&#24471;&#22256;&#38590;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20855;&#26377;&#65288;&#22810;&#20010;&#65289;&#31163;&#25955;&#29942;&#39048;&#30340;Transformer&#65292;TDB&#65292;&#20854;&#28508;&#22312;&#20195;&#30721;&#21487;&#20197;&#23398;&#20064;&#35266;&#23519;&#21644;&#21160;&#20316;&#21382;&#21490;&#30340;&#21387;&#32553;&#34920;&#31034;&#12290;&#22312;&#35757;&#32451;TDB&#20197;&#39044;&#27979;&#32473;&#23450;&#21382;&#21490;&#30340;&#26410;&#26469;&#35266;&#23519;&#21518;&#65292;&#25105;&#20204;&#20174;&#20854;&#27963;&#36291;&#30340;&#29942;&#39048;&#32034;&#24341;&#20013;&#25552;&#21462;&#29615;&#22659;&#30340;&#21487;&#35299;&#37322;&#35748;&#30693;&#22320;&#22270;&#12290;&#28982;&#21518;&#65292;&#23558;&#36825;&#20123;&#22320;&#22270;&#19982;&#22806;&#37096;&#27714;&#35299;&#22120;&#37197;&#23545;&#20197;&#35299;&#20915;&#65288;&#21463;&#38480;&#21046;&#30340;&#65289;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;POEs&#19978;&#35757;&#32451;&#30340;TDB&#21487;&#20197;&#20445;&#30041;...
&lt;/p&gt;
&lt;p&gt;
Despite their stellar performance on a wide range of tasks, including in-context tasks only revealed during inference, vanilla transformers and variants trained for next-token predictions (a) do not learn an explicit world model of their environment which can be flexibly queried and (b) cannot be used for planning or navigation. In this paper, we consider partially observed environments (POEs), where an agent receives perceptually aliased observations as it navigates, which makes path planning hard. We introduce a transformer with (multiple) discrete bottleneck(s), TDB, whose latent codes learn a compressed representation of the history of observations and actions. After training a TDB to predict the future observation(s) given the history, we extract interpretable cognitive maps of the environment from its active bottleneck(s) indices. These maps are then paired with an external solver to solve (constrained) path planning problems. First, we show that a TDB trained on POEs (a) retains
&lt;/p&gt;</description></item><item><title>&#27492;&#30740;&#31350;&#20351;&#29992;&#22810;&#23618;&#24863;&#30693;&#22120;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#20102;&#33778;&#24459;&#23486;HIV/AIDS&#30340;&#26102;&#38388;&#24207;&#21015;&#65292;&#30740;&#31350;&#21457;&#29616;COVID-19&#30123;&#24773;&#23545;HIV&#27969;&#34892;&#27809;&#26377;&#26126;&#26174;&#30340;&#24433;&#21709;&#65292;&#24182;&#39044;&#27979;&#21040;2030&#24180;&#35813;&#22269;&#30340;&#32047;&#31215;&#30149;&#20363;&#23558;&#36798;&#21040;145,273&#20363;&#12290;</title><link>http://arxiv.org/abs/2401.05933</link><description>&lt;p&gt;
HIV/AIDS&#22312;&#33778;&#24459;&#23486;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65306;&#28145;&#24230;&#23398;&#20064;&#20197;&#21450;COVID-19&#30123;&#24773;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Time Series Forecasting of HIV/AIDS in the Philippines Using Deep Learning: Does COVID-19 Epidemic Matter?. (arXiv:2401.05933v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05933
&lt;/p&gt;
&lt;p&gt;
&#27492;&#30740;&#31350;&#20351;&#29992;&#22810;&#23618;&#24863;&#30693;&#22120;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#20102;&#33778;&#24459;&#23486;HIV/AIDS&#30340;&#26102;&#38388;&#24207;&#21015;&#65292;&#30740;&#31350;&#21457;&#29616;COVID-19&#30123;&#24773;&#23545;HIV&#27969;&#34892;&#27809;&#26377;&#26126;&#26174;&#30340;&#24433;&#21709;&#65292;&#24182;&#39044;&#27979;&#21040;2030&#24180;&#35813;&#22269;&#30340;&#32047;&#31215;&#30149;&#20363;&#23558;&#36798;&#21040;145,273&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33778;&#24459;&#23486;&#30340;HIV/AIDS&#30123;&#24773;&#22312;&#35199;&#22826;&#24179;&#27915;&#22320;&#21306;&#20256;&#25773;&#36895;&#24230;&#26368;&#24555;&#65292;2010&#24180;&#33267;2021&#24180;&#38388;HIV&#21457;&#30149;&#29575;&#22686;&#38271;&#20102;676%&#12290;&#23613;&#31649;COVID-19&#23545;HIV&#26381;&#21153;&#21644;&#21457;&#23637;&#30340;&#23436;&#20840;&#24433;&#21709;&#23578;&#19981;&#26126;&#30830;&#65292;&#20294;&#39044;&#35745;&#36825;&#31181;&#24178;&#25200;&#21487;&#33021;&#20250;&#23548;&#33268;HIV&#27515;&#20129;&#20154;&#25968;&#26174;&#33879;&#22686;&#21152;&#12290;&#22240;&#27492;&#65292;&#35813;&#22269;&#38656;&#35201;&#19968;&#20123;&#24314;&#27169;&#21644;&#39044;&#27979;&#25216;&#26415;&#26469;&#39044;&#27979;&#20256;&#25773;&#27169;&#24335;&#24182;&#25913;&#21892;&#25919;&#24220;&#30340;&#39044;&#38450;&#12289;&#27835;&#30103;&#12289;&#26816;&#27979;&#21644;&#25252;&#29702;&#35745;&#21010;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#22810;&#23618;&#24863;&#30693;&#22120;&#31070;&#32463;&#32593;&#32476;&#65292;&#20351;&#29992;&#33778;&#24459;&#23486;HIV/AIDS&#21644;ART&#27880;&#20876;&#22788;&#30340;&#32479;&#35745;&#25968;&#25454;&#65292;&#22312;COVID-19&#22823;&#27969;&#34892;&#34989;&#20987;&#35813;&#22269;&#30340;&#26399;&#38388;&#39044;&#27979;&#26102;&#38388;&#24207;&#21015;&#12290;&#32463;&#36807;&#25968;&#25454;&#30340;&#35757;&#32451;&#12289;&#39564;&#35777;&#21644;&#27979;&#35797;&#65292;&#30740;&#31350;&#21457;&#29616;&#21040;2030&#24180;&#65292;&#35813;&#22269;&#30340;&#32047;&#31215;&#30149;&#20363;&#39044;&#35745;&#23558;&#36798;&#21040;145,273&#20363;&#12290;&#27492;&#22806;&#65292;&#35266;&#23519;&#20540;&#19982;&#39044;&#26399;&#30340;HIV&#27969;&#34892;&#20043;&#38388;&#20960;&#20046;&#27809;&#26377;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
With a 676% growth rate in HIV incidence between 2010 and 2021, the HIV/AIDS epidemic in the Philippines is the one that is spreading the quickest in the western Pacific. Although the full effects of COVID-19 on HIV services and development are still unknown, it is predicted that such disruptions could lead to a significant increase in HIV casualties. Therefore, the nation needs some modeling and forecasting techniques to foresee the spread pattern and enhance the governments prevention, treatment, testing, and care program. In this study, the researcher uses Multilayer Perceptron Neural Network to forecast time series during the period when the COVID-19 pandemic strikes the nation, using statistics taken from the HIV/AIDS and ART Registry of the Philippines. After training, validation, and testing of data, the study finds that the predicted cumulative cases in the nation by 2030 will reach 145,273. Additionally, there is very little difference between observed and anticipated HIV epid
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#30315;&#30187;&#30142;&#30149;&#30340;&#23450;&#21046;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;EpilepsyLLM&#65292;&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;LLM&#24182;&#20351;&#29992;&#30315;&#30187;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;&#36890;&#36807;&#35813;&#27169;&#22411;&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#22238;&#31572;&#19982;&#30315;&#30187;&#30456;&#20851;&#30340;&#38382;&#39064;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#20351;&#29992;&#26085;&#35821;&#36827;&#34892;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2401.05908</link><description>&lt;p&gt;
EpilepsyLLM: &#20351;&#29992;&#30315;&#30187;&#21307;&#23398;&#30693;&#35782;&#36827;&#34892;&#39046;&#22495;&#29305;&#23450;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
EpilepsyLLM: Domain-Specific Large Language Model Fine-tuned with Epilepsy Medical Knowledge. (arXiv:2401.05908v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05908
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#30315;&#30187;&#30142;&#30149;&#30340;&#23450;&#21046;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;EpilepsyLLM&#65292;&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;LLM&#24182;&#20351;&#29992;&#30315;&#30187;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;&#36890;&#36807;&#35813;&#27169;&#22411;&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#22238;&#31572;&#19982;&#30315;&#30187;&#30456;&#20851;&#30340;&#38382;&#39064;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#20351;&#29992;&#26085;&#35821;&#36827;&#34892;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20973;&#20511;&#24222;&#22823;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#22312;&#32508;&#21512;&#21644;&#29983;&#25104;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#22522;&#20110;&#36825;&#20123;&#24378;&#22823;&#30340;LLMs&#65292;&#36890;&#36807;&#39046;&#22495;&#29305;&#23450;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#65292;&#27169;&#22411;&#25317;&#26377;&#26356;&#19987;&#19994;&#30340;&#30693;&#35782;&#65292;&#22240;&#27492;&#26356;&#23454;&#29992;&#65292;&#27604;&#22914;&#21307;&#23398;LLMs&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#32463;&#36807;&#24494;&#35843;&#30340;&#21307;&#23398;LLMs&#20165;&#38480;&#20110;&#36890;&#29992;&#30340;&#33521;&#25991;&#21307;&#23398;&#30693;&#35782;&#12290;&#23545;&#20110;&#29305;&#23450;&#30142;&#30149;&#30340;&#38382;&#39064;&#65292;&#27169;&#22411;&#30340;&#21709;&#24212;&#26159;&#19981;&#20934;&#30830;&#30340;&#65292;&#26377;&#26102;&#29978;&#33267;&#23436;&#20840;&#19981;&#30456;&#20851;&#65292;&#29305;&#21035;&#26159;&#22312;&#20351;&#29992;&#38750;&#33521;&#25991;&#35821;&#35328;&#26102;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#20351;&#29992;&#26085;&#35821;&#36827;&#34892;&#30315;&#30187;&#30142;&#30149;&#30340;&#30740;&#31350;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#23450;&#21046;&#30340;LLM&#65292;&#31216;&#20026;EpilepsyLLM&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20351;&#29992;&#26469;&#33258;&#30315;&#30187;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#36890;&#36807;&#24494;&#35843;&#25216;&#26415;&#20174;&#39044;&#35757;&#32451;&#30340;LLM&#36827;&#34892;&#35757;&#32451;&#65292;&#20854;&#20013;&#21253;&#21547;&#26377;&#20851;&#30142;&#30149;&#22522;&#26412;&#20449;&#24687;&#12289;&#24120;&#35265;&#27835;&#30103;&#26041;&#27861;&#21644;&#33647;&#29289;&#20197;&#21450;&#29983;&#27963;&#21644;&#24037;&#20316;&#20013;&#30340;&#37325;&#35201;&#27880;&#24847;&#20107;&#39033;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
With large training datasets and massive amounts of computing sources, large language models (LLMs) achieve remarkable performance in comprehensive and generative ability. Based on those powerful LLMs, the model fine-tuned with domain-specific datasets posseses more specialized knowledge and thus is more practical like medical LLMs. However, the existing fine-tuned medical LLMs are limited to general medical knowledge with English language. For disease-specific problems, the model's response is inaccurate and sometimes even completely irrelevant, especially when using a language other than English. In this work, we focus on the particular disease of Epilepsy with Japanese language and introduce a customized LLM termed as EpilepsyLLM. Our model is trained from the pre-trained LLM by fine-tuning technique using datasets from the epilepsy domain. The datasets contain knowledge of basic information about disease, common treatment methods and drugs, and important notes in life and work. The
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;ORPO&#65292;&#36890;&#36807;&#26500;&#24314;&#20048;&#35266;MDP&#26469;&#40723;&#21169;&#26356;&#22810;&#36229;&#20986;&#31163;&#32447;&#25968;&#25454;&#38598;&#30340;&#27169;&#22411;&#39044;&#27979;&#65292;&#20174;&#32780;&#25552;&#21319;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.05899</link><description>&lt;p&gt;
&#20048;&#35266;&#27169;&#22411;&#39044;&#27979;&#29992;&#20110;&#24754;&#35266;&#31163;&#32447;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Optimistic Model Rollouts for Pessimistic Offline Policy Optimization. (arXiv:2401.05899v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05899
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;ORPO&#65292;&#36890;&#36807;&#26500;&#24314;&#20048;&#35266;MDP&#26469;&#40723;&#21169;&#26356;&#22810;&#36229;&#20986;&#31163;&#32447;&#25968;&#25454;&#38598;&#30340;&#27169;&#22411;&#39044;&#27979;&#65292;&#20174;&#32780;&#25552;&#21319;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27169;&#22411;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20026;&#36890;&#36807;&#21512;&#25104;&#27169;&#22411;&#39044;&#27979;&#26469;&#25913;&#21892;&#27867;&#21270;&#33021;&#21147;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#36890;&#36807;&#26500;&#24314;&#24754;&#35266;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;P-MDP&#65289;&#26469;&#23558;&#24754;&#35266;&#20027;&#20041;&#32435;&#20837;&#31574;&#30053;&#20248;&#21270;&#20013;&#12290;&#28982;&#32780;&#65292;P-MDP&#38480;&#21046;&#31574;&#30053;&#22312;&#36229;&#20986;&#31163;&#32447;&#25968;&#25454;&#38598;&#25903;&#25345;&#30340;&#20998;&#24067;&#21306;&#22495;&#23398;&#20064;&#65292;&#20174;&#32780;&#21487;&#33021;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#21160;&#21147;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26500;&#24314;&#20048;&#35266;MDP&#65288;O-MDP&#65289;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#26368;&#21021;&#35266;&#23519;&#21040;&#36890;&#36807;&#40723;&#21169;&#26356;&#22810;&#36229;&#20986;&#31163;&#32447;&#25968;&#25454;&#38598;&#30340;&#27169;&#22411;&#39044;&#27979;&#21487;&#20197;&#24102;&#26469;&#20048;&#35266;&#20027;&#20041;&#30340;&#28508;&#22312;&#22909;&#22788;&#12290;&#21463;&#27492;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ORPO&#65292;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#31163;&#32447;RL&#26694;&#26550;&#12290;ORPO&#36890;&#36807;&#22312;O-MDP&#20013;&#35757;&#32451;&#20048;&#35266;&#30340;&#27169;&#22411;&#39044;&#27979;&#31574;&#30053;&#26469;&#37319;&#26679;&#26356;&#22810;&#36229;&#20986;&#31163;&#32447;&#25968;&#25454;&#38598;&#30340;&#27169;&#22411;&#39044;&#27979;&#12290;&#28982;&#21518;&#25105;&#20204;&#37325;&#26032;&#20026;&#36825;&#20123;&#39044;&#27979;&#25171;&#19978;&#26631;&#31614;&#65292;&#24182;&#36890;&#36807;&#24754;&#35266;&#20027;&#20041;&#30340;&#31163;&#32447;&#31574;&#30053;&#20248;&#21270;&#26469;&#25552;&#21319;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model-based offline reinforcement learning (RL) has made remarkable progress, offering a promising avenue for improving generalization with synthetic model rollouts. Existing works primarily focus on incorporating pessimism for policy optimization, usually via constructing a Pessimistic Markov Decision Process (P-MDP). However, the P-MDP discourages the policies from learning in out-of-distribution (OOD) regions beyond the support of offline datasets, which can under-utilize the generalization ability of dynamics models. In contrast, we propose constructing an Optimistic MDP (O-MDP). We initially observed the potential benefits of optimism brought by encouraging more OOD rollouts. Motivated by this observation, we present ORPO, a simple yet effective model-based offline RL framework. ORPO generates Optimistic model Rollouts for Pessimistic offline policy Optimization. Specifically, we train an optimistic rollout policy in the O-MDP to sample more OOD model rollouts. Then we relabel the
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#23398;&#20064;&#22312;&#26234;&#33021;&#30005;&#32593;&#32593;&#32476;&#20013;&#36215;&#21040;&#25512;&#36827;&#20027;&#21160;&#32593;&#32476;&#23433;&#20840;&#25514;&#26045;&#30340;&#20316;&#29992;&#65292;&#26412;&#35770;&#25991;&#36890;&#36807;&#32508;&#21512;&#35843;&#26597;&#20102;&#26368;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#20027;&#21160;&#32593;&#32476;&#38450;&#24481;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.05896</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#26234;&#33021;&#30005;&#32593;&#32593;&#32476;&#20013;&#25512;&#36827;&#20027;&#21160;&#24335;&#32593;&#32476;&#23433;&#20840;&#25514;&#26045;&#30340;&#20316;&#29992;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
The Role of Deep Learning in Advancing Proactive Cybersecurity Measures for Smart Grid Networks: A Survey. (arXiv:2401.05896v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05896
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#26234;&#33021;&#30005;&#32593;&#32593;&#32476;&#20013;&#36215;&#21040;&#25512;&#36827;&#20027;&#21160;&#32593;&#32476;&#23433;&#20840;&#25514;&#26045;&#30340;&#20316;&#29992;&#65292;&#26412;&#35770;&#25991;&#36890;&#36807;&#32508;&#21512;&#35843;&#26597;&#20102;&#26368;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#20027;&#21160;&#32593;&#32476;&#38450;&#24481;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26234;&#33021;&#30005;&#32593;&#65288;SG&#65289;&#36234;&#26469;&#36234;&#20381;&#36182;&#20256;&#24863;&#22120;&#21644;&#36890;&#20449;&#31995;&#32479;&#31561;&#20808;&#36827;&#25216;&#26415;&#26469;&#23454;&#29616;&#39640;&#25928;&#30340;&#33021;&#28304;&#21457;&#30005;&#12289;&#20998;&#37197;&#21644;&#28040;&#32791;&#65292;&#23427;&#20204;&#25104;&#20026;&#20102;&#22797;&#26434;&#32593;&#32476;&#25915;&#20987;&#30340;&#26377;&#21560;&#24341;&#21147;&#30340;&#30446;&#26631;&#12290;&#36825;&#20123;&#19981;&#26029;&#36827;&#21270;&#30340;&#23041;&#32961;&#38656;&#35201;&#24378;&#22823;&#30340;&#23433;&#20840;&#25514;&#26045;&#26469;&#32500;&#25345;&#29616;&#20195;&#33021;&#28304;&#31995;&#32479;&#30340;&#31283;&#23450;&#24615;&#21644;&#38887;&#24615;&#12290;&#34429;&#28982;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#65292;&#20294;&#20851;&#20110;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#22312;SG&#20013;&#23454;&#26045;&#20027;&#21160;&#32593;&#32476;&#38450;&#24481;&#31574;&#30053;&#30340;&#32508;&#21512;&#24615;&#25506;&#32034;&#22312;&#25991;&#29486;&#20013;&#20173;&#28982;&#24456;&#23569;&#35265;&#12290;&#26412;&#35843;&#26597;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#65292;&#30740;&#31350;&#20102;&#29992;&#20110;&#20027;&#21160;&#32593;&#32476;&#38450;&#24481;&#30340;&#26368;&#26032;DL&#25216;&#26415;&#12290;&#35843;&#26597;&#39318;&#20808;&#27010;&#36848;&#20102;&#30456;&#20851;&#24037;&#20316;&#21644;&#25105;&#20204;&#29420;&#29305;&#30340;&#36129;&#29486;&#65292;&#28982;&#21518;&#23545;SG&#22522;&#30784;&#35774;&#26045;&#36827;&#34892;&#20102;&#26816;&#26597;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#23558;&#21508;&#31181;&#32593;&#32476;&#38450;&#24481;&#25216;&#26415;&#20998;&#20026;&#21453;&#24212;&#24615;&#21644;&#20027;&#21160;&#24615;&#20004;&#20010;&#31867;&#21035;&#12290;&#25105;&#20204;&#37325;&#28857;&#20851;&#27880;DL-enabled&#30340;&#20027;&#21160;&#38450;&#24481;&#65292;&#25552;&#20379;&#20102;DL&#26041;&#27861;&#30340;&#20840;&#38754;&#20998;&#31867;&#65292;&#31361;&#20986;&#20102;&#23427;&#20204;&#30340;&#35282;&#33394;&#21644;
&lt;/p&gt;
&lt;p&gt;
As smart grids (SG) increasingly rely on advanced technologies like sensors and communication systems for efficient energy generation, distribution, and consumption, they become enticing targets for sophisticated cyberattacks. These evolving threats demand robust security measures to maintain the stability and resilience of modern energy systems. While extensive research has been conducted, a comprehensive exploration of proactive cyber defense strategies utilizing Deep Learning (DL) in {SG} remains scarce in the literature. This survey bridges this gap, studying the latest DL techniques for proactive cyber defense. The survey begins with an overview of related works and our distinct contributions, followed by an examination of SG infrastructure. Next, we classify various cyber defense techniques into reactive and proactive categories. A significant focus is placed on DL-enabled proactive defenses, where we provide a comprehensive taxonomy of DL approaches, highlighting their roles and
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20108;&#36827;&#21046;&#32447;&#24615;&#26641;&#25552;&#20132;&#30340;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#25152;&#26377;&#26435;&#20445;&#25252;&#27169;&#22411;&#65292;&#36890;&#36807;&#39564;&#35777;&#35745;&#31639;&#30340;&#23436;&#25972;&#24615;&#21644;&#25928;&#26524;&#65292;&#35299;&#20915;&#20102;&#27169;&#22411;&#25152;&#26377;&#26435;&#30340;&#20914;&#31361;&#38382;&#39064;&#65292;&#24182;&#38477;&#20302;&#20102;&#26356;&#26032;&#35777;&#26126;&#30340;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2401.05895</link><description>&lt;p&gt;
&#22522;&#20110;&#20108;&#36827;&#21046;&#32447;&#24615;&#26641;&#25552;&#20132;&#30340;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#25152;&#26377;&#26435;&#20445;&#25252;
&lt;/p&gt;
&lt;p&gt;
Binary Linear Tree Commitment-based Ownership Protection for Distributed Machine Learning. (arXiv:2401.05895v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05895
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20108;&#36827;&#21046;&#32447;&#24615;&#26641;&#25552;&#20132;&#30340;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#25152;&#26377;&#26435;&#20445;&#25252;&#27169;&#22411;&#65292;&#36890;&#36807;&#39564;&#35777;&#35745;&#31639;&#30340;&#23436;&#25972;&#24615;&#21644;&#25928;&#26524;&#65292;&#35299;&#20915;&#20102;&#27169;&#22411;&#25152;&#26377;&#26435;&#30340;&#20914;&#31361;&#38382;&#39064;&#65292;&#24182;&#38477;&#20302;&#20102;&#26356;&#26032;&#35777;&#26126;&#30340;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#36890;&#36807;&#23558;&#35745;&#31639;&#20219;&#21153;&#20998;&#37197;&#32473;&#22810;&#20010;&#24037;&#20316;&#33410;&#28857;&#65292;&#23454;&#29616;&#20102;&#23545;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#24182;&#34892;&#35757;&#32451;&#12290;&#20294;&#26159;&#65292;&#26368;&#32456;&#27169;&#22411;&#26435;&#37325;&#30340;&#20256;&#25773;&#24448;&#24448;&#20250;&#23548;&#33268;&#27169;&#22411;&#25152;&#26377;&#26435;&#30340;&#28508;&#22312;&#20914;&#31361;&#65292;&#22240;&#20026;&#24037;&#20316;&#33410;&#28857;&#38590;&#20197;&#35777;&#26126;&#33258;&#24049;&#22312;&#35757;&#32451;&#35745;&#31639;&#20013;&#30340;&#21442;&#19982;&#24230;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#25152;&#26377;&#26435;&#38382;&#39064;&#65292;&#24182;&#38450;&#27490;&#24847;&#22806;&#25925;&#38556;&#21644;&#24694;&#24847;&#25915;&#20987;&#65292;&#22312;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#20013;&#39564;&#35777;&#24037;&#20316;&#33410;&#28857;&#30340;&#35745;&#31639;&#23436;&#25972;&#24615;&#21644;&#25928;&#26524;&#21464;&#24471;&#23588;&#20026;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20108;&#36827;&#21046;&#32447;&#24615;&#26641;&#25552;&#20132;&#30340;&#25152;&#26377;&#26435;&#20445;&#25252;&#27169;&#22411;&#65292;&#20197;&#30830;&#20445;&#35745;&#31639;&#30340;&#23436;&#25972;&#24615;&#65292;&#21516;&#26102;&#20445;&#35777;&#24320;&#38144;&#26377;&#38480;&#21644;&#35777;&#26126;&#31616;&#27905;&#12290;&#30001;&#20110;&#35757;&#32451;&#36807;&#31243;&#20013;&#21442;&#25968;&#30340;&#39057;&#32321;&#26356;&#26032;&#65292;&#25105;&#20204;&#30340;&#25552;&#20132;&#26041;&#26696;&#24341;&#20837;&#20102;&#21487;&#32500;&#25252;&#30340;&#26641;&#32467;&#26500;&#65292;&#38477;&#20302;&#20102;&#26356;&#26032;&#35777;&#26126;&#30340;&#25104;&#26412;&#12290;&#19982;&#22522;&#20110;SNARK&#30340;&#39564;&#35777;&#35745;&#31639;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#21516;&#26102;&#25903;&#25345;&#25209;&#37327;&#35757;&#32451;&#21644;&#22312;&#32447;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributed machine learning enables parallel training of extensive datasets by delegating computing tasks across multiple workers. Despite the cost reduction benefits of distributed machine learning, the dissemination of final model weights often leads to potential conflicts over model ownership as workers struggle to substantiate their involvement in the training computation. To address the above ownership issues and prevent accidental failures and malicious attacks, verifying the computational integrity and effectiveness of workers becomes particularly crucial in distributed machine learning. In this paper, we proposed a novel binary linear tree commitment-based ownership protection model to ensure computational integrity with limited overhead and concise proof. Due to the frequent updates of parameters during training, our commitment scheme introduces a maintainable tree structure to reduce the costs of updating proofs. Distinguished from SNARK-based verifiable computation, our mod
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19981;&#30830;&#23450;&#29615;&#22659;&#20013;&#36827;&#34892;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25512;&#23548;&#39057;&#29575;&#20445;&#35777;&#26469;&#20272;&#35745;&#24403;&#21069;&#19978;&#19979;&#25991;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#26469;&#35782;&#21035;&#19978;&#19979;&#25991;&#21464;&#37327;&#12290;</title><link>http://arxiv.org/abs/2401.05876</link><description>&lt;p&gt;
&#22312;&#19981;&#30830;&#23450;&#29615;&#22659;&#20013;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Safe reinforcement learning in uncertain contexts. (arXiv:2401.05876v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05876
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19981;&#30830;&#23450;&#29615;&#22659;&#20013;&#36827;&#34892;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25512;&#23548;&#39057;&#29575;&#20445;&#35777;&#26469;&#20272;&#35745;&#24403;&#21069;&#19978;&#19979;&#25991;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#26469;&#35782;&#21035;&#19978;&#19979;&#25991;&#21464;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#20445;&#35777;&#23433;&#20840;&#26159;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#37325;&#35201;&#26041;&#38754;&#12290;&#29616;&#26377;&#30340;&#23433;&#20840;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#32771;&#34385;&#36830;&#32493;&#21464;&#37327;&#65292;&#21363;&#22238;&#24402;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#26426;&#22120;&#20154;&#31995;&#32479;&#20063;&#20250;&#21463;&#21040;&#31163;&#25955;&#30340;&#22806;&#37096;&#29615;&#22659;&#21464;&#21270;&#30340;&#24433;&#21709;&#65292;&#20363;&#22914;&#24517;&#39035;&#25658;&#24102;&#29305;&#23450;&#37325;&#37327;&#30340;&#29289;&#20307;&#25110;&#22312;&#20912;&#20923;&#12289;&#28287;&#28070;&#25110;&#24178;&#29157;&#30340;&#34920;&#38754;&#19978;&#25805;&#20316;&#12290;&#36825;&#20123;&#24433;&#21709;&#21487;&#20197;&#24314;&#27169;&#20026;&#31163;&#25955;&#30340;&#19978;&#19979;&#25991;&#21464;&#37327;&#12290;&#22312;&#29616;&#26377;&#25991;&#29486;&#20013;&#65292;&#22914;&#26524;&#32771;&#34385;&#36825;&#20123;&#19978;&#19979;&#25991;&#65292;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#20551;&#35774;&#23427;&#20204;&#26159;&#24050;&#30693;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25918;&#24323;&#20102;&#36825;&#20010;&#20551;&#35774;&#65292;&#23637;&#31034;&#20102;&#22312;&#26080;&#27861;&#30452;&#25509;&#27979;&#37327;&#19978;&#19979;&#25991;&#21464;&#37327;&#30340;&#24773;&#20917;&#19979;&#22914;&#20309;&#36827;&#34892;&#23433;&#20840;&#23398;&#20064;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#38024;&#23545;&#22810;&#31867;&#20998;&#31867;&#25512;&#23548;&#20102;&#39057;&#29575;&#20445;&#35777;&#65292;&#20174;&#27979;&#37327;&#20540;&#20013;&#20272;&#35745;&#24403;&#21069;&#19978;&#19979;&#25991;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23454;&#39564;&#26469;&#35782;&#21035;&#19978;&#19979;&#25991;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#22312;&#21738;&#20123;&#26465;&#20214;&#19979;&#21487;&#20197;&#20445;&#30041;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#21152;&#20197;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
When deploying machine learning algorithms in the real world, guaranteeing safety is an essential asset. Existing safe learning approaches typically consider continuous variables, i.e., regression tasks. However, in practice, robotic systems are also subject to discrete, external environmental changes, e.g., having to carry objects of certain weights or operating on frozen, wet, or dry surfaces. Such influences can be modeled as discrete context variables. In the existing literature, such contexts are, if considered, mostly assumed to be known. In this work, we drop this assumption and show how we can perform safe learning when we cannot directly measure the context variables. To achieve this, we derive frequentist guarantees for multi-class classification, allowing us to estimate the current context from measurements. Further, we propose an approach for identifying contexts through experiments. We discuss under which conditions we can retain theoretical guarantees and demonstrate the 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21152;&#36895;&#24230;&#35745;&#25968;&#25454;&#25512;&#26029;&#25104;&#21151;&#21644;&#22833;&#36133;&#30340;&#35762;&#35805;&#24847;&#22270;&#65292;&#22312;&#37326;&#22806;&#29615;&#22659;&#20013;&#30340;&#30740;&#31350;&#34920;&#26126;&#21152;&#36895;&#24230;&#35745;&#25968;&#25454;&#20013;&#23384;&#22312;&#26377;&#29992;&#30340;&#20449;&#24687;&#65292;&#20294;&#19981;&#36275;&#20197;&#21487;&#38752;&#22320;&#25429;&#25417;&#35762;&#35805;&#24847;&#22270;&#12290;</title><link>http://arxiv.org/abs/2401.05849</link><description>&lt;p&gt;
&#36890;&#36807;&#21152;&#36895;&#24230;&#35745;&#25968;&#25454;&#25512;&#26029;&#35762;&#35805;&#24847;&#22270;&#8212;&#8212;&#37326;&#22806;&#29615;&#22659;&#20013;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Inferring Intentions to Speak Using Accelerometer Data In-the-Wild. (arXiv:2401.05849v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05849
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21152;&#36895;&#24230;&#35745;&#25968;&#25454;&#25512;&#26029;&#25104;&#21151;&#21644;&#22833;&#36133;&#30340;&#35762;&#35805;&#24847;&#22270;&#65292;&#22312;&#37326;&#22806;&#29615;&#22659;&#20013;&#30340;&#30740;&#31350;&#34920;&#26126;&#21152;&#36895;&#24230;&#35745;&#25968;&#25454;&#20013;&#23384;&#22312;&#26377;&#29992;&#30340;&#20449;&#24687;&#65292;&#20294;&#19981;&#36275;&#20197;&#21487;&#38752;&#22320;&#25429;&#25417;&#35762;&#35805;&#24847;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#20855;&#26377;&#33391;&#22909;&#30340;&#33258;&#28982;&#30452;&#35273;&#65292;&#21487;&#20197;&#35782;&#21035;&#20986;&#20182;&#20154;&#26377;&#35805;&#35201;&#35828;&#30340;&#26102;&#20505;&#12290;&#22914;&#26524;&#20154;&#24037;&#26234;&#33021;&#20063;&#33021;&#35782;&#21035;&#20986;&#35762;&#35805;&#24847;&#22270;&#65292;&#23558;&#20250;&#38750;&#24120;&#26377;&#36259;&#12290;&#29305;&#21035;&#26159;&#22312;&#20154;&#24037;&#26234;&#33021;&#24341;&#23548;&#22242;&#20307;&#35752;&#35770;&#30340;&#22330;&#26223;&#19979;&#65292;&#36825;&#23558;&#26159;&#19968;&#39033;&#26377;&#29992;&#30340;&#25216;&#33021;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#36890;&#36807;&#21152;&#36895;&#24230;&#35745;&#25968;&#25454;&#25512;&#26029;&#25104;&#21151;&#21644;&#22833;&#36133;&#30340;&#35762;&#35805;&#24847;&#22270;&#12290;&#20043;&#25152;&#20197;&#36873;&#25321;&#21152;&#36895;&#24230;&#35745;&#25968;&#25454;&#65292;&#26159;&#22240;&#20026;&#23427;&#20855;&#26377;&#38544;&#31169;&#20445;&#25252;&#21151;&#33021;&#65292;&#21516;&#26102;&#22312;&#37326;&#22806;&#29615;&#22659;&#20013;&#26131;&#20110;&#23454;&#29616;&#65292;&#21487;&#20197;&#25918;&#32622;&#22312;&#26234;&#33021;&#24509;&#31456;&#19978;&#12290;&#20351;&#29992;&#30495;&#23454;&#31038;&#20132;&#32593;&#32476;&#20107;&#20214;&#30340;&#25968;&#25454;&#26469;&#35757;&#32451;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#26088;&#22312;&#25512;&#26029;&#35762;&#35805;&#24847;&#22270;&#12290;&#25968;&#25454;&#20013;&#30340;&#19968;&#37096;&#20998;&#19981;&#25104;&#21151;&#30340;&#35762;&#35805;&#24847;&#22270;&#26696;&#20363;&#34987;&#27880;&#37322;&#12290;&#27169;&#22411;&#22312;&#25104;&#21151;&#30340;&#35762;&#35805;&#24847;&#22270;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#22312;&#25104;&#21151;&#21644;&#22833;&#36133;&#30340;&#26696;&#20363;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;&#24635;&#20043;&#65292;&#21152;&#36895;&#24230;&#35745;&#25968;&#25454;&#20013;&#23384;&#22312;&#26377;&#29992;&#30340;&#20449;&#24687;&#65292;&#20294;&#19981;&#36275;&#20197;&#21487;&#38752;&#22320;&#25429;&#25417;&#35762;&#35805;&#24847;&#22270;&#12290;&#20363;&#22914;&#65292;&#23039;&#21183;&#21464;&#21270;&#19982;&#35762;&#35805;&#24847;&#22270;&#20855;&#26377;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans have good natural intuition to recognize when another person has something to say. It would be interesting if an AI can also recognize intentions to speak. Especially in scenarios when an AI is guiding a group discussion, this can be a useful skill. This work studies the inference of successful and unsuccessful intentions to speak from accelerometer data. This is chosen because it is privacy-preserving and feasible for in-the-wild settings since it can be placed in a smart badge. Data from a real-life social networking event is used to train a machine-learning model that aims to infer intentions to speak. A subset of unsuccessful intention-to-speak cases in the data is annotated. The model is trained on the successful intentions to speak and evaluated on both the successful and unsuccessful cases. In conclusion, there is useful information in accelerometer data, but not enough to reliably capture intentions to speak. For example, posture shifts are correlated with intentions to 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#22810;&#30446;&#26631;&#20248;&#21270;&#26041;&#27861;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#22312;&#20171;&#30005;&#26448;&#26009;&#30740;&#31350;&#20013;&#21462;&#24471;&#20102;&#31361;&#30772;&#12290;&#30740;&#31350;&#20154;&#21592;&#25104;&#21151;&#22320;&#21512;&#25104;&#21644;&#34920;&#24449;&#20102;&#20004;&#31181;&#26032;&#22411;&#20171;&#30005;&#26448;&#26009;&#65292;CsTaTeO6&#21644;Bi2Zr2O7&#65292;&#20026;&#26410;&#30693;&#26448;&#26009;&#30340;&#23547;&#25214;&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;&#24037;&#20316;&#27969;&#31243;&#12290;</title><link>http://arxiv.org/abs/2401.05848</link><description>&lt;p&gt;
&#25512;&#21160;&#24102;&#38553;&#21644;&#20171;&#30005;&#24120;&#25968;&#30340;&#24085;&#32047;&#25176;&#21069;&#27839;&#65306;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#24341;&#23548;&#30340;&#23547;&#25214;&#20171;&#30005;&#26448;&#26009;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Pushing the Pareto front of band gap and permittivity: ML-guided search for dielectric materials. (arXiv:2401.05848v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05848
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#22810;&#30446;&#26631;&#20248;&#21270;&#26041;&#27861;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#22312;&#20171;&#30005;&#26448;&#26009;&#30740;&#31350;&#20013;&#21462;&#24471;&#20102;&#31361;&#30772;&#12290;&#30740;&#31350;&#20154;&#21592;&#25104;&#21151;&#22320;&#21512;&#25104;&#21644;&#34920;&#24449;&#20102;&#20004;&#31181;&#26032;&#22411;&#20171;&#30005;&#26448;&#26009;&#65292;CsTaTeO6&#21644;Bi2Zr2O7&#65292;&#20026;&#26410;&#30693;&#26448;&#26009;&#30340;&#23547;&#25214;&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;&#24037;&#20316;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#20171;&#30005;&#24120;&#25968;&#30340;&#26448;&#26009;&#22312;&#22806;&#37096;&#30005;&#22330;&#19979;&#23481;&#26131;&#26497;&#21270;&#65292;&#22312;&#35768;&#22810;&#29616;&#20195;&#30005;&#23376;&#35774;&#22791;&#20013;&#21457;&#25381;&#30528;&#37325;&#35201;&#30340;&#21151;&#33021;&#12290;&#23427;&#20204;&#30340;&#23454;&#38469;&#25928;&#29992;&#30001;&#20004;&#20010;&#30456;&#20114;&#20914;&#31361;&#30340;&#29305;&#24615;&#20915;&#23450;&#65306;&#39640;&#20171;&#30005;&#24120;&#25968;&#24448;&#24448;&#20986;&#29616;&#22312;&#24102;&#38553;&#36739;&#31364;&#30340;&#26448;&#26009;&#20013;&#65292;&#38480;&#21046;&#20102;&#20171;&#30005;&#20987;&#31359;&#20043;&#21069;&#30340;&#24037;&#20316;&#30005;&#21387;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#36890;&#37327;&#30340;&#24037;&#20316;&#27969;&#31243;&#65292;&#23558;&#20803;&#32032;&#26367;&#20195;&#12289;&#26426;&#22120;&#23398;&#20064;&#39044;&#31579;&#36873;&#12289;&#20174;&#22836;&#35745;&#31639;&#27169;&#25311;&#21644;&#20154;&#31867;&#19987;&#23478;&#30452;&#35273;&#30456;&#32467;&#21512;&#65292;&#20197;&#26377;&#25928;&#22320;&#25506;&#32034;&#26410;&#30693;&#26448;&#26009;&#30340;&#28508;&#22312;&#20171;&#30005;&#24615;&#33021;&#65292;&#20174;&#32780;&#21512;&#25104;&#21644;&#34920;&#24449;&#20004;&#31181;&#26032;&#22411;&#30340;&#20171;&#30005;&#26448;&#26009;CsTaTeO6&#21644;Bi2Zr2O7&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#22312;&#20984;&#38754;&#24085;&#32047;&#25176;&#21069;&#27839;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#29615;&#22659;&#20013;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#12290;&#34429;&#28982;&#36890;&#24120;&#34987;&#35748;&#20026;&#27604;&#21333;&#30446;&#26631;&#20248;&#21270;&#26356;&#20855;&#25361;&#25112;&#24615;&#65292;&#20294;&#25105;&#20204;&#25552;&#20986;&#24182;&#23637;&#31034;&#20102;&#21021;&#27493;&#35777;&#25454;&#34920;&#26126;&#24102;&#38553;&#21644;&#20171;&#30005;&#24120;&#25968;&#20043;&#38388;&#30340;1/x&#30456;&#20851;&#24615;&#23454;&#38469;&#19978;&#20351;&#20219;&#21153;&#21464;      &#24471;&#23481;&#26131;&#12290;
&lt;/p&gt;
&lt;p&gt;
Materials with high-dielectric constant easily polarize under external electric fields, allowing them to perform essential functions in many modern electronic devices. Their practical utility is determined by two conflicting properties: high dielectric constants tend to occur in materials with narrow band gaps, limiting the operating voltage before dielectric breakdown. We present a high-throughput workflow that combines element substitution, ML pre-screening, ab initio simulation and human expert intuition to efficiently explore the vast space of unknown materials for potential dielectrics, leading to the synthesis and characterization of two novel dielectric materials, CsTaTeO6 and Bi2Zr2O7. Our key idea is to deploy ML in a multi-objective optimization setting with concave Pareto front. While usually considered more challenging than single-objective optimization, we argue and show preliminary evidence that the $1/x$-correlation between band gap and permittivity in fact makes the tas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32858;&#21512;&#31574;&#30053;&#65292;&#29992;&#20110;&#35780;&#20272;&#32858;&#31867;&#36136;&#37327;&#12290;&#36890;&#36807;&#23545;&#32858;&#31867;&#32423;&#21035;&#30340;&#36718;&#24275;&#24471;&#20998;&#36827;&#34892;&#24179;&#22343;&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#23545;&#25152;&#26377;&#32858;&#31867;&#30340;&#24471;&#20998;&#36827;&#34892;&#23439;&#35266;&#24179;&#22343;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#23439;&#35266;&#24179;&#22343;&#36718;&#24275;&#24471;&#20998;&#23545;&#20110;&#32858;&#31867;&#19981;&#22343;&#34913;&#21644;&#32972;&#26223;&#22122;&#22768;&#26159;&#31283;&#20581;&#30340;&#12290;</title><link>http://arxiv.org/abs/2401.05831</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#36718;&#24275;&#31995;&#25968;&#65306;&#20174;&#24494;&#35266;&#21040;&#23439;&#35266;&#32858;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Revisiting Silhouette: From Micro to Macro Aggregation. (arXiv:2401.05831v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05831
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32858;&#21512;&#31574;&#30053;&#65292;&#29992;&#20110;&#35780;&#20272;&#32858;&#31867;&#36136;&#37327;&#12290;&#36890;&#36807;&#23545;&#32858;&#31867;&#32423;&#21035;&#30340;&#36718;&#24275;&#24471;&#20998;&#36827;&#34892;&#24179;&#22343;&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#23545;&#25152;&#26377;&#32858;&#31867;&#30340;&#24471;&#20998;&#36827;&#34892;&#23439;&#35266;&#24179;&#22343;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#23439;&#35266;&#24179;&#22343;&#36718;&#24275;&#24471;&#20998;&#23545;&#20110;&#32858;&#31867;&#19981;&#22343;&#34913;&#21644;&#32972;&#26223;&#22122;&#22768;&#26159;&#31283;&#20581;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36718;&#24275;&#31995;&#25968;&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#20869;&#37096;&#32858;&#31867;&#35780;&#20272;&#25351;&#26631;&#65292;&#23427;&#20250;&#20026;&#27599;&#20010;&#25968;&#25454;&#28857;&#20135;&#29983;&#19968;&#20010;&#24471;&#20998;&#65292;&#29992;&#20110;&#35780;&#20272;&#20854;&#32858;&#31867;&#20998;&#37197;&#30340;&#36136;&#37327;&#12290;&#30446;&#21069;&#65292;&#20026;&#20102;&#35780;&#20272;&#25972;&#20010;&#25968;&#25454;&#38598;&#30340;&#32858;&#31867;&#36136;&#37327;&#65292;&#36890;&#24120;&#20250;&#23558;&#25968;&#25454;&#38598;&#20013;&#25152;&#26377;&#28857;&#30340;&#24471;&#20998;&#24179;&#22343;&#25104;&#19968;&#20010;&#21333;&#19968;&#20540;&#65292;&#36825;&#20010;&#31574;&#30053;&#34987;&#31216;&#20026;&#24494;&#35266;&#24179;&#22343;&#12290;&#28982;&#32780;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#21512;&#25104;&#20363;&#23376;&#23637;&#31034;&#20102;&#65292;&#35813;&#24494;&#35266;&#24179;&#22343;&#31574;&#30053;&#23545;&#20110;&#32858;&#31867;&#19981;&#22343;&#34913;&#21644;&#24322;&#24120;&#20540;&#65288;&#32972;&#26223;&#22122;&#22768;&#65289;&#38750;&#24120;&#25935;&#24863;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#32858;&#21512;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#39318;&#20808;&#23545;&#32858;&#31867;&#32423;&#21035;&#30340;&#36718;&#24275;&#24471;&#20998;&#36827;&#34892;&#24179;&#22343;&#65292;&#28982;&#21518;&#20877;&#23545;&#25152;&#26377;&#32858;&#31867;&#30340;&#24471;&#20998;&#36827;&#34892;&#23439;&#35266;&#24179;&#22343;&#12290;&#22522;&#20110;&#30456;&#21516;&#30340;&#21512;&#25104;&#20363;&#23376;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25552;&#20986;&#30340;&#23439;&#35266;&#24179;&#22343;&#36718;&#24275;&#24471;&#20998;&#23545;&#20110;&#32858;&#31867;&#19981;&#22343;&#34913;&#21644;&#32972;&#26223;&#22122;&#22768;&#26159;&#31283;&#20581;&#30340;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;&#30740;&#31350;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#25552;&#20986;&#30340;&#23439;&#35266;&#24179;&#22343;&#21464;&#20307;&#21487;&#20197;&#26356;&#22909;&#22320;&#20272;&#35745;&#30495;&#23454;&#30340;&#32858;&#31867;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Silhouette coefficient is an established internal clustering evaluation measure that produces a score per data point, assessing the quality of its clustering assignment. To assess the quality of the clustering of the whole dataset, the scores of all the points in the dataset are typically averaged into a single value, a strategy which we call as micro-averaging. As we illustrate in this work, by using a synthetic example, this micro-averaging strategy is sensitive both to cluster imbalance and outliers (background noise). To address these issues, we propose an alternative aggregation strategy, which first averages the silhouette scores at a cluster level and then (macro) averages the scores across the clusters. Based on the same synthetic example, we show that the proposed macro-averaged silhouette score is robust to cluster imbalance and background noise. We have conducted an experimental study showing that our macro-averaged variant provides better estimates of the ground truth numbe
&lt;/p&gt;</description></item><item><title>SCoBots&#26159;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#27010;&#24565;&#29942;&#39048;&#20195;&#29702;&#65292;&#33021;&#22815;&#36879;&#26126;&#21270;&#25972;&#20010;&#20915;&#31574;&#27969;&#31243;&#65292;&#24110;&#21161;&#39046;&#22495;&#19987;&#23478;&#29702;&#35299;&#21644;&#35268;&#33539;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#34892;&#20026;&#65292;&#20174;&#32780;&#21487;&#33021;&#23454;&#29616;&#26356;&#22909;&#30340;&#20154;&#31867;&#23545;&#40784;&#24378;&#21270;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2401.05821</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#27010;&#24565;&#29942;&#39048;&#29992;&#20110;&#23545;&#40784;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Interpretable Concept Bottlenecks to Align Reinforcement Learning Agents. (arXiv:2401.05821v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05821
&lt;/p&gt;
&lt;p&gt;
SCoBots&#26159;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#27010;&#24565;&#29942;&#39048;&#20195;&#29702;&#65292;&#33021;&#22815;&#36879;&#26126;&#21270;&#25972;&#20010;&#20915;&#31574;&#27969;&#31243;&#65292;&#24110;&#21161;&#39046;&#22495;&#19987;&#23478;&#29702;&#35299;&#21644;&#35268;&#33539;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#34892;&#20026;&#65292;&#20174;&#32780;&#21487;&#33021;&#23454;&#29616;&#26356;&#22909;&#30340;&#20154;&#31867;&#23545;&#40784;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22870;&#21169;&#31232;&#30095;&#24615;&#12289;&#38590;&#20197;&#24402;&#22240;&#30340;&#38382;&#39064;&#20197;&#21450;&#19981;&#23545;&#40784;&#31561;&#31561;&#37117;&#26159;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#23398;&#20064;&#26368;&#20248;&#31574;&#30053;&#22256;&#38590;&#29978;&#33267;&#19981;&#21487;&#33021;&#30340;&#21407;&#22240;&#20043;&#19968;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#28145;&#24230;&#32593;&#32476;&#30340;&#40657;&#30418;&#29305;&#24615;&#38459;&#30861;&#20102;&#39046;&#22495;&#19987;&#23478;&#30340;&#21442;&#19982;&#65292;&#36825;&#20123;&#19987;&#23478;&#21487;&#20197;&#35299;&#37322;&#27169;&#22411;&#24182;&#32416;&#27491;&#38169;&#35823;&#34892;&#20026;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36830;&#32493;&#27010;&#24565;&#29942;&#39048;&#20195;&#29702;&#65288;SCoBots&#65289;&#65292;&#36890;&#36807;&#25972;&#21512;&#36830;&#32493;&#30340;&#27010;&#24565;&#29942;&#39048;&#23618;&#65292;&#20351;&#25972;&#20010;&#20915;&#31574;&#27969;&#31243;&#36879;&#26126;&#21270;&#12290;SCoBots&#19981;&#20165;&#21033;&#29992;&#30456;&#20851;&#30340;&#23545;&#35937;&#23646;&#24615;&#65292;&#36824;&#21033;&#29992;&#20851;&#31995;&#27010;&#24565;&#12290;&#23454;&#39564;&#32467;&#26524;&#24378;&#26377;&#21147;&#22320;&#35777;&#26126;&#65292;SCoBots&#20351;&#39046;&#22495;&#19987;&#23478;&#33021;&#22815;&#26377;&#25928;&#29702;&#35299;&#21644;&#35268;&#33539;&#20182;&#20204;&#30340;&#34892;&#20026;&#65292;&#20174;&#32780;&#21487;&#33021;&#23454;&#29616;&#26356;&#22909;&#30340;&#20154;&#31867;&#23545;&#40784;&#24378;&#21270;&#23398;&#20064;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;SCoBots&#20351;&#25105;&#20204;&#33021;&#22815;&#35782;&#21035;&#26368;&#31616;&#21333;&#19988;&#20855;&#26377;&#20195;&#34920;&#24615;&#30340;&#35270;&#39057;&#28216;&#25103;Pong&#20013;&#30340;&#19981;&#23545;&#40784;&#38382;&#39064;&#65292;&#24182;&#21152;&#20197;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reward sparsity, difficult credit assignment, and misalignment are only a few of the many issues that make it difficult, if not impossible, for deep reinforcement learning (RL) agents to learn optimal policies. Unfortunately, the black-box nature of deep networks impedes the inclusion of domain experts who could interpret the model and correct wrong behavior. To this end, we introduce Successive Concept Bottlenecks Agents (SCoBots), which make the whole decision pipeline transparent via the integration of consecutive concept bottleneck layers. SCoBots make use of not only relevant object properties but also of relational concepts. Our experimental results provide strong evidence that SCoBots allow domain experts to efficiently understand and regularize their behavior, resulting in potentially better human-aligned RL. In this way, SCoBots enabled us to identify a misalignment problem in the most simple and iconic video game, Pong, and resolve it.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#38459;&#25239;&#35760;&#24518;&#20013;&#30340;&#22122;&#22768;&#23545;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#25552;&#39640;&#24377;&#24615;&#30340;&#23545;&#31574;&#12290;</title><link>http://arxiv.org/abs/2401.05820</link><description>&lt;p&gt;
&#38459;&#25239;&#35760;&#24518;&#20013;&#30340;&#22122;&#22768;&#23545;&#22270;&#20687;&#20998;&#31867;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Implications of Noise in Resistive Memory on Deep Neural Networks for Image Classification. (arXiv:2401.05820v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05820
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#38459;&#25239;&#35760;&#24518;&#20013;&#30340;&#22122;&#22768;&#23545;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#25552;&#39640;&#24377;&#24615;&#30340;&#23545;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38459;&#25239;&#23384;&#20648;&#26159;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;SRAM&#26367;&#20195;&#21697;&#65292;&#20294;&#20063;&#26159;&#19968;&#31181;&#22266;&#26377;&#19981;&#31283;&#23450;&#30340;&#35774;&#22791;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#21162;&#21147;&#30830;&#20445;&#27491;&#30830;&#30340;&#35835;&#20889;&#25805;&#20316;&#12290;&#20026;&#20102;&#36991;&#20813;&#22312;&#38754;&#31215;&#12289;&#26102;&#38388;&#21644;&#33021;&#37327;&#26041;&#38754;&#30340;&#30456;&#20851;&#25104;&#26412;&#65292;&#26412;&#30740;&#31350;&#33268;&#21147;&#20110;&#25506;&#32034;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#21487;&#20197;&#23481;&#24525;&#22810;&#23569;&#20869;&#23384;&#25805;&#20316;&#20013;&#30340;&#22122;&#22768;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#29305;&#27530;&#30340;&#22122;&#22768;&#25805;&#20316;&#31526;&#65292;&#27169;&#25311;&#31034;&#20363;&#38459;&#25239;&#23384;&#20648;&#21333;&#20803;&#20013;&#30340;&#22122;&#22768;&#65292;&#25506;&#32034;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;CIFAR-10&#20998;&#31867;&#20219;&#21153;&#19978;&#30340;&#24377;&#24615;&#65292;&#24182;&#35752;&#35770;&#20102;&#19968;&#20123;&#25552;&#39640;&#36825;&#31181;&#24377;&#24615;&#30340;&#23545;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
Resistive memory is a promising alternative to SRAM, but is also an inherently unstable device that requires substantial effort to ensure correct read and write operations. To avoid the associated costs in terms of area, time and energy, the present work is concerned with exploring how much noise in memory operations can be tolerated by image classification tasks based on neural networks. We introduce a special noisy operator that mimics the noise in an exemplary resistive memory unit, explore the resilience of convolutional neural networks on the CIFAR-10 classification task, and discuss a couple of countermeasures to improve this resilience.
&lt;/p&gt;</description></item><item><title>TAnet&#26159;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#33041;&#30005;&#20449;&#21495;&#30340;&#21548;&#35273;&#31354;&#38388;&#27880;&#24847;&#21147;&#35299;&#30721;&#32593;&#32476;&#65292;&#37319;&#29992;&#20102;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#25429;&#25417;&#33041;&#30005;&#20449;&#21495;&#20013;&#26102;&#38388;&#27493;&#20043;&#38388;&#30340;&#20132;&#20114;&#20316;&#29992;&#65292;&#24182;&#25552;&#20379;&#20102;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#22909;&#30340;&#35299;&#30721;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.05819</link><description>&lt;p&gt;
TAnet: &#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#33041;&#30005;&#20449;&#21495;&#30340;&#21548;&#35273;&#31354;&#38388;&#27880;&#24847;&#21147;&#35299;&#30721;&#30340;&#26102;&#38388;&#27880;&#24847;&#21147;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
TAnet: A New Temporal Attention Network for EEG-based Auditory Spatial Attention Decoding with a Short Decision Window. (arXiv:2401.05819v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05819
&lt;/p&gt;
&lt;p&gt;
TAnet&#26159;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#33041;&#30005;&#20449;&#21495;&#30340;&#21548;&#35273;&#31354;&#38388;&#27880;&#24847;&#21147;&#35299;&#30721;&#32593;&#32476;&#65292;&#37319;&#29992;&#20102;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#25429;&#25417;&#33041;&#30005;&#20449;&#21495;&#20013;&#26102;&#38388;&#27493;&#20043;&#38388;&#30340;&#20132;&#20114;&#20316;&#29992;&#65292;&#24182;&#25552;&#20379;&#20102;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#22909;&#30340;&#35299;&#30721;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21548;&#35273;&#31354;&#38388;&#27880;&#24847;&#21147;&#26816;&#27979;&#65288;ASAD&#65289;&#36890;&#36807;&#20998;&#26512;&#30005;&#33041;&#33041;&#30005;&#20449;&#21495;&#26469;&#30830;&#23450;&#21548;&#20247;&#23545;&#35828;&#35805;&#32773;&#30340;&#27880;&#24847;&#26041;&#21521;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25913;&#36827;ASAD&#30340;&#24615;&#33021;&#65292;&#20351;&#29992;&#36739;&#30701;&#30340;&#20915;&#31574;&#31383;&#21475;&#65288;&#23567;&#20110;1&#31186;&#65289;&#65292;&#32780;&#19981;&#26159;&#20197;&#21069;&#30740;&#31350;&#20013;&#20351;&#29992;&#30340;&#36739;&#38271;&#20915;&#31574;&#31383;&#21475;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#26102;&#38388;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;&#21363;TAnet&#65289;&#12290;TAnet&#37319;&#29992;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#25429;&#25417;&#37319;&#38598;&#21040;&#30340;&#33041;&#30005;&#20449;&#21495;&#20013;&#26102;&#38388;&#27493;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#20026;&#36825;&#20123;&#26102;&#38388;&#27493;&#20998;&#37197;&#30456;&#24212;&#30340;&#26435;&#37325;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#22522;&#20110;CNN&#30340;&#26041;&#27861;&#21644;&#26368;&#36817;&#30340;ASAD&#26041;&#27861;&#30456;&#27604;&#65292;TAnet&#22312;KUL&#25968;&#25454;&#38598;&#20013;&#25552;&#20379;&#20102;&#25913;&#36827;&#30340;&#35299;&#30721;&#24615;&#33021;&#65292;&#20351;&#29992;&#36739;&#30701;&#30340;&#20915;&#31574;&#31383;&#21475;&#65288;&#21363;&#23567;&#20110;1&#31186;&#65289;&#30340;&#24773;&#20917;&#19979;&#65292;&#35299;&#30721;&#20934;&#30830;&#29575;&#20998;&#21035;&#20026;92.4%&#65288;&#20915;&#31574;&#31383;&#21475;0.1&#31186;&#65289;&#12289;94.9%&#65288;0.25&#31186;&#65289;&#12289;95.1%&#65288;0.3&#31186;&#65289;&#12289;95.4%&#65288;0.4&#31186;&#65289;&#21644;95.5%&#65288;0.5&#31186;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Auditory spatial attention detection (ASAD) is used to determine the direction of a listener's attention to a speaker by analyzing her/his electroencephalographic (EEG) signals. This study aimed to further improve the performance of ASAD with a short decision window (i.e., &lt;1 s) rather than with long decision windows in previous studies. An end-to-end temporal attention network (i.e., TAnet) was introduced in this work. TAnet employs a multi-head attention (MHA) mechanism, which can more effectively capture the interactions among time steps in collected EEG signals and efficiently assign corresponding weights to those EEG time steps. Experiments demonstrated that, compared with the CNN-based method and recent ASAD methods, TAnet provided improved decoding performance in the KUL dataset, with decoding accuracies of 92.4% (decision window 0.1 s), 94.9% (0.25 s), 95.1% (0.3 s), 95.4% (0.4 s), and 95.5% (0.5 s) with short decision windows (i.e., &lt;1 s). As a new ASAD model with a short deci
&lt;/p&gt;</description></item><item><title>Cheetah&#26159;&#19968;&#31181;&#39640;&#36895;&#21487;&#24494;&#20998;&#27169;&#25311;&#24037;&#20855;&#65292;&#21487;&#20197;&#20943;&#23569;&#35745;&#31639;&#26102;&#38388;&#24182;&#23454;&#29616;&#24555;&#36895;&#25910;&#38598;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;&#23427;&#33021;&#22815;&#20419;&#36827;&#21152;&#36895;&#22120;&#35843;&#20248;&#21644;&#31995;&#32479;&#35782;&#21035;&#65292;&#24182;&#19982;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#26080;&#32541;&#38598;&#25104;&#12290;</title><link>http://arxiv.org/abs/2401.05815</link><description>&lt;p&gt;
Cheetah: &#36890;&#36807;&#39640;&#36895;&#21487;&#24494;&#20998;&#27169;&#25311;&#22635;&#34917;&#26426;&#22120;&#23398;&#20064;&#21644;&#31890;&#23376;&#21152;&#36895;&#22120;&#29289;&#29702;&#20043;&#38388;&#30340;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Cheetah: Bridging the Gap Between Machine Learning and Particle Accelerator Physics with High-Speed, Differentiable Simulations. (arXiv:2401.05815v1 [physics.acc-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05815
&lt;/p&gt;
&lt;p&gt;
Cheetah&#26159;&#19968;&#31181;&#39640;&#36895;&#21487;&#24494;&#20998;&#27169;&#25311;&#24037;&#20855;&#65292;&#21487;&#20197;&#20943;&#23569;&#35745;&#31639;&#26102;&#38388;&#24182;&#23454;&#29616;&#24555;&#36895;&#25910;&#38598;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;&#23427;&#33021;&#22815;&#20419;&#36827;&#21152;&#36895;&#22120;&#35843;&#20248;&#21644;&#31995;&#32479;&#35782;&#21035;&#65292;&#24182;&#19982;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#26080;&#32541;&#38598;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#21152;&#36895;&#22120;&#29289;&#29702;&#23398;&#20013;&#29616;&#20195;&#25361;&#25112;&#30340;&#24378;&#22823;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#26377;&#38480;&#30340;&#26463;&#27969;&#26102;&#38388;&#21487;&#29992;&#24615;&#65292;&#27169;&#25311;&#30340;&#35745;&#31639;&#25104;&#26412;&#20197;&#21450;&#20248;&#21270;&#38382;&#39064;&#30340;&#39640;&#32500;&#24230;&#32473;&#29983;&#25104;&#25152;&#38656;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25968;&#25454;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Cheetah&#65292;&#19968;&#31181;&#22522;&#20110;PyTorch&#30340;&#39640;&#36895;&#21487;&#24494;&#32447;&#24615;&#26463;&#27969;&#21160;&#21147;&#23398;&#20195;&#30721;&#12290;Cheetah&#36890;&#36807;&#23558;&#35745;&#31639;&#26102;&#38388;&#20943;&#23569;&#25968;&#20010;&#25968;&#37327;&#32423;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#25910;&#38598;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#24182;&#20419;&#36827;&#20102;&#38754;&#21521;&#21152;&#36895;&#22120;&#35843;&#20248;&#21644;&#31995;&#32479;&#35782;&#21035;&#30340;&#39640;&#25928;&#26799;&#24230;&#20248;&#21270;&#12290;&#36825;&#20351;Cheetah&#25104;&#20026;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#12289;&#26131;&#20110;&#25193;&#23637;&#30340;&#24037;&#20855;&#65292;&#19982;&#24191;&#27867;&#37319;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#26080;&#32541;&#38598;&#25104;&#12290;&#25105;&#20204;&#36890;&#36807;&#20116;&#20010;&#31034;&#20363;&#23637;&#31034;&#20102;Cheetah&#30340;&#23454;&#29992;&#24615;&#65292;&#21253;&#25324;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#12289;&#22522;&#20110;&#26799;&#24230;&#30340;&#26463;&#32447;&#35843;&#20248;&#12289;&#22522;&#20110;&#26799;&#24230;&#30340;&#31995;&#32479;&#35782;&#21035;&#12289;&#29289;&#29702;&#23398;-i
&lt;/p&gt;
&lt;p&gt;
Machine learning has emerged as a powerful solution to the modern challenges in accelerator physics. However, the limited availability of beam time, the computational cost of simulations, and the high-dimensionality of optimisation problems pose significant challenges in generating the required data for training state-of-the-art machine learning models. In this work, we introduce Cheetah, a PyTorch-based high-speed differentiable linear-beam dynamics code. Cheetah enables the fast collection of large data sets by reducing computation times by multiple orders of magnitude and facilitates efficient gradient-based optimisation for accelerator tuning and system identification. This positions Cheetah as a user-friendly, readily extensible tool that integrates seamlessly with widely adopted machine learning tools. We showcase the utility of Cheetah through five examples, including reinforcement learning training, gradient-based beamline tuning, gradient-based system identification, physics-i
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;GST-Pro&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22270;&#24418;&#26102;&#31354;&#36807;&#31243;&#21644;&#24322;&#24120;&#35780;&#20998;&#22120;&#26469;&#35299;&#20915;&#22312;&#37319;&#26679;&#19981;&#35268;&#21017;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20013;&#26816;&#27979;&#24322;&#24120;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.05800</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#24418;&#26102;&#31354;&#36807;&#31243;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#21450;&#20854;&#22312;&#32570;&#22833;&#20540;&#24773;&#20917;&#19979;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Graph Spatiotemporal Process for Multivariate Time Series Anomaly Detection with Missing Values. (arXiv:2401.05800v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05800
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;GST-Pro&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22270;&#24418;&#26102;&#31354;&#36807;&#31243;&#21644;&#24322;&#24120;&#35780;&#20998;&#22120;&#26469;&#35299;&#20915;&#22312;&#37319;&#26679;&#19981;&#35268;&#21017;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20013;&#26816;&#27979;&#24322;&#24120;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#36827;&#34892;&#26816;&#27979;&#23545;&#20110;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#38750;&#24120;&#37325;&#35201;&#65292;&#21253;&#25324;&#26234;&#33021;&#30005;&#32593;&#12289;&#20132;&#36890;&#27969;&#39044;&#27979;&#21644;&#24037;&#19994;&#36807;&#31243;&#25511;&#21046;&#31561;&#12290;&#28982;&#32780;&#65292;&#30495;&#23454;&#19990;&#30028;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36890;&#24120;&#19981;&#26159;&#33391;&#22909;&#32467;&#26500;&#21270;&#30340;&#65292;&#36825;&#32473;&#29616;&#26377;&#26041;&#27861;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#65306;&#65288;1&#65289;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#32570;&#22833;&#20540;&#23384;&#22312;&#20110;&#21464;&#37327;&#21644;&#26102;&#38388;&#32500;&#24230;&#65292;&#38459;&#30861;&#20102;&#23545;&#20132;&#32455;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#36827;&#34892;&#26377;&#25928;&#24314;&#27169;&#65292;&#23548;&#33268;&#37325;&#35201;&#30340;&#27169;&#24335;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#34987;&#24573;&#35270;&#65307;&#65288;2&#65289;&#22312;&#37319;&#26679;&#19981;&#35268;&#21017;&#30340;&#35266;&#27979;&#19979;&#36827;&#34892;&#24322;&#24120;&#35780;&#20998;&#30340;&#30740;&#31350;&#36739;&#23569;&#65292;&#36825;&#20351;&#24471;&#22312;&#22810;&#21464;&#37327;&#24207;&#21015;&#20013;&#20351;&#29992;&#29616;&#26377;&#26816;&#27979;&#22120;&#26102;&#24456;&#38590;&#22788;&#29702;&#27809;&#26377;&#23436;&#20840;&#35266;&#27979;&#20540;&#30340;&#24773;&#20917;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;GST-Pro&#30340;&#26032;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#22270;&#24418;&#26102;&#31354;&#36807;&#31243;&#21644;&#24322;&#24120;&#35780;&#20998;&#22120;&#26469;&#35299;&#20915;&#22312;&#37319;&#26679;&#19981;&#35268;&#21017;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20013;&#26816;&#27979;&#24322;&#24120;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
The detection of anomalies in multivariate time series data is crucial for various practical applications, including smart power grids, traffic flow forecasting, and industrial process control. However, real-world time series data is usually not well-structured, posting significant challenges to existing approaches: (1) The existence of missing values in multivariate time series data along variable and time dimensions hinders the effective modeling of interwoven spatial and temporal dependencies, resulting in important patterns being overlooked during model training; (2) Anomaly scoring with irregularly-sampled observations is less explored, making it difficult to use existing detectors for multivariate series without fully-observed values. In this work, we introduce a novel framework called GST-Pro, which utilizes a graph spatiotemporal process and anomaly scorer to tackle the aforementioned challenges in detecting anomalies on irregularly-sampled multivariate time series. Our approac
&lt;/p&gt;</description></item><item><title>&#25913;&#36827;&#20102;&#38169;&#35823;&#26377;&#30028;&#22312;&#32447;&#23398;&#20064;&#20013;&#21453;&#39304;&#20215;&#26684;&#30340;&#19978;&#19979;&#30028;&#65292;&#36824;&#35299;&#20915;&#20102;&#22810;&#31867;&#23398;&#20064;&#20013;&#26631;&#20934;&#21453;&#39304;&#19982;&#36172;&#24466;&#21453;&#39304;&#30340;&#20215;&#26684;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.05794</link><description>&lt;p&gt;
&#38169;&#35823;&#26377;&#30028;&#22312;&#32447;&#23398;&#20064;&#20013;&#21453;&#39304;&#20215;&#26684;&#30340;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Bounds on the price of feedback for mistake-bounded online learning. (arXiv:2401.05794v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05794
&lt;/p&gt;
&lt;p&gt;
&#25913;&#36827;&#20102;&#38169;&#35823;&#26377;&#30028;&#22312;&#32447;&#23398;&#20064;&#20013;&#21453;&#39304;&#20215;&#26684;&#30340;&#19978;&#19979;&#30028;&#65292;&#36824;&#35299;&#20915;&#20102;&#22810;&#31867;&#23398;&#20064;&#20013;&#26631;&#20934;&#21453;&#39304;&#19982;&#36172;&#24466;&#21453;&#39304;&#30340;&#20215;&#26684;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25913;&#36827;&#20102;(Auer&#21644;Long&#65292;Machine Learning&#65292;1999)&#20013;&#21508;&#31181;&#22312;&#32447;&#23398;&#20064;&#22330;&#26223;&#30340;&#33509;&#24178;&#26368;&#22351;&#24773;&#20917;&#30028;&#38480;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23558;&#24310;&#36831;&#27169;&#26865;&#20004;&#21487;&#24378;&#21270;&#23398;&#20064;&#30340;&#19978;&#30028;&#32553;&#23567;&#20102;2&#20493;&#65292;&#23558;&#20989;&#25968;&#26063;&#32452;&#21512;&#23398;&#20064;&#30340;&#19978;&#30028;&#32553;&#23567;&#20102;2.41&#20493;&#65292;&#23558;&#19981;&#30693;&#24615;&#23398;&#20064;&#30340;&#19978;&#30028;&#32553;&#23567;&#20102;1.09&#20493;&#12290;&#25105;&#20204;&#36824;&#25913;&#36827;&#20102;&#21516;&#19968;&#35770;&#25991;&#20013;&#20989;&#25968;&#26063;&#32452;&#21512;&#23398;&#20064;&#30340;&#19979;&#30028;&#65292;&#23558;&#20854;&#32553;&#23567;&#21040;&#920;(ln{k})&#30340;&#22240;&#23376;&#65292;&#19982;&#19978;&#30028;&#30456;&#21305;&#37197;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;(Lon&#65292;Theoretical Computer Science&#65292;2020)&#20013;&#20851;&#20110;&#22810;&#31867;&#23398;&#20064;&#26631;&#20934;&#21453;&#39304;&#19982;&#36172;&#24466;&#21453;&#39304;&#20215;&#26684;&#30340;&#38382;&#39064;&#65292;&#24182;&#23558;(Feng&#31561;&#20154;&#65292;Theoretical Computer Science&#65292;2023)&#20013;$r$-&#36755;&#20837;&#24310;&#36831;&#27169;&#26865;&#20004;&#21487;&#24378;&#21270;&#23398;&#20064;&#30340;&#19978;&#30028;&#32553;&#23567;&#20102;$r$&#20493;&#65292;&#19982;&#21516;&#19968;&#35770;&#25991;&#20013;&#30340;&#19979;&#30028;&#30456;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
We improve several worst-case bounds for various online learning scenarios from (Auer and Long, Machine Learning, 1999). In particular, we sharpen an upper bound for delayed ambiguous reinforcement learning by a factor of 2, an upper bound for learning compositions of families of functions by a factor of 2.41, and an upper bound for agnostic learning by a factor of 1.09. We also improve a lower bound from the same paper for learning compositions of $k$ families of functions by a factor of $\Theta(\ln{k})$, matching the upper bound up to a constant factor. In addition, we solve a problem from (Long, Theoretical Computer Science, 2020) on the price of bandit feedback with respect to standard feedback for multiclass learning, and we improve an upper bound from (Feng et al., Theoretical Computer Science, 2023) on the price of $r$-input delayed ambiguous reinforcement learning by a factor of $r$, matching a lower bound from the same paper up to the leading term.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22810;&#35821;&#35328;&#23884;&#20837;&#31354;&#38388;&#20013;&#25237;&#24433;&#35821;&#35328;&#29305;&#23450;&#22240;&#32032;&#30340;&#26032;&#35270;&#35282;&#65292;&#24182;&#36890;&#36807;&#21457;&#29616;&#19968;&#20010;&#20302;&#31209;&#23376;&#31354;&#38388;&#26469;&#28040;&#38500;&#19982;&#35821;&#20041;&#26080;&#20851;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#20805;&#20998;&#21033;&#29992;&#35821;&#20041;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2401.05792</link><description>&lt;p&gt;
&#21457;&#29616;&#29992;&#20110;&#36328;&#35821;&#35328;&#19981;&#21487;&#30693;&#22810;&#35821;&#35328;&#34920;&#31034;&#30340;&#20302;&#31209;&#23376;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Discovering Low-rank Subspaces for Language-agnostic Multilingual Representations. (arXiv:2401.05792v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05792
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22810;&#35821;&#35328;&#23884;&#20837;&#31354;&#38388;&#20013;&#25237;&#24433;&#35821;&#35328;&#29305;&#23450;&#22240;&#32032;&#30340;&#26032;&#35270;&#35282;&#65292;&#24182;&#36890;&#36807;&#21457;&#29616;&#19968;&#20010;&#20302;&#31209;&#23376;&#31354;&#38388;&#26469;&#28040;&#38500;&#19982;&#35821;&#20041;&#26080;&#20851;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#20805;&#20998;&#21033;&#29992;&#35821;&#20041;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#65288;ML-LMs&#65289;&#23637;&#31034;&#20986;&#20102;&#22312;&#26080;&#30452;&#25509;&#36328;&#35821;&#35328;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#21331;&#36234;&#30340;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#36716;&#25442;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#38543;&#21518;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#22810;&#35821;&#35328;&#23884;&#20837;&#31354;&#38388;&#20013;&#23384;&#22312;&#24378;&#28872;&#30340;&#35821;&#35328;&#36523;&#20221;&#20449;&#24687;&#65292;&#36825;&#38459;&#30861;&#20102;&#35821;&#35328;&#38388;&#20849;&#20139;&#30340;&#35821;&#35328;&#22240;&#32032;&#30340;&#34920;&#36798;&#12290;&#23545;&#20110;&#36328;&#35821;&#35328;&#21477;&#23376;&#26816;&#32034;&#31561;&#35821;&#20041;&#20219;&#21153;&#65292;&#24076;&#26395;&#28040;&#38500;&#36825;&#31181;&#35821;&#35328;&#36523;&#20221;&#20449;&#21495;&#65292;&#20805;&#20998;&#21033;&#29992;&#35821;&#20041;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20174;&#22810;&#35821;&#35328;&#23884;&#20837;&#31354;&#38388;&#20013;&#25237;&#24433;&#35821;&#35328;&#29305;&#23450;&#22240;&#32032;&#30340;&#26032;&#35270;&#35282;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21457;&#29616;&#23384;&#22312;&#19968;&#20010;&#20302;&#31209;&#23376;&#31354;&#38388;&#65292;&#35813;&#23376;&#31354;&#38388;&#20027;&#35201;&#32534;&#30721;&#19982;&#35821;&#20041;&#26080;&#20851;&#30340;&#20449;&#24687;&#65288;&#20363;&#22914;&#65292;&#21477;&#27861;&#20449;&#24687;&#65289;&#12290;&#20026;&#20102;&#35782;&#21035;&#35813;&#23376;&#31354;&#38388;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#22855;&#24322;&#20540;&#20998;&#35299;&#65292;&#23558;&#22810;&#35821;&#35328;&#35821;&#26009;&#24211;&#20316;&#20026;&#36755;&#20837;&#12290;&#19968;&#26086;&#25214;&#21040;&#20102;&#35813;&#23376;&#31354;&#38388;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#25237;&#24433;&#23558;&#20449;&#24687;&#21521;&#35813;&#23376;&#31354;&#38388;&#30340;&#38646;&#31354;&#38388;&#25237;&#24433;&#65292;&#20174;&#32780;&#33719;&#24471;&#28040;&#38500;&#20102;&#35821;&#35328;&#29305;&#23450;&#22240;&#32032;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large pretrained multilingual language models (ML-LMs) have shown remarkable capabilities of zero-shot cross-lingual transfer, without direct cross-lingual supervision. While these results are promising, follow-up works found that, within the multilingual embedding spaces, there exists strong language identity information which hinders the expression of linguistic factors shared across languages. For semantic tasks like cross-lingual sentence retrieval, it is desired to remove such language identity signals to fully leverage semantic information. In this work, we provide a novel view of projecting away language-specific factors from a multilingual embedding space. Specifically, we discover that there exists a low-rank subspace that primarily encodes information irrelevant to semantics (e.g., syntactic information). To identify this subspace, we present a simple but effective unsupervised method based on singular value decomposition with multiple monolingual corpora as input. Once the s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30693;&#35782;&#36716;&#21270;&#65288;KT&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#8220;&#32763;&#35793;&#8221;&#27169;&#22411;&#26469;&#25509;&#25910;&#36739;&#22823;&#27169;&#22411;&#30340;&#21442;&#25968;&#24182;&#29983;&#25104;&#21387;&#32553;&#21442;&#25968;&#65292;&#20174;&#32780;&#23454;&#29616;&#27169;&#22411;&#21387;&#32553;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#25110;&#26045;&#21152;&#26550;&#26500;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2401.05772</link><description>&lt;p&gt;
&#30693;&#35782;&#36716;&#21270;&#65306;&#19968;&#31181;&#29992;&#20110;&#27169;&#22411;&#21387;&#32553;&#30340;&#26032;&#36884;&#24452;
&lt;/p&gt;
&lt;p&gt;
Knowledge Translation: A New Pathway for Model Compression. (arXiv:2401.05772v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05772
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30693;&#35782;&#36716;&#21270;&#65288;KT&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#8220;&#32763;&#35793;&#8221;&#27169;&#22411;&#26469;&#25509;&#25910;&#36739;&#22823;&#27169;&#22411;&#30340;&#21442;&#25968;&#24182;&#29983;&#25104;&#21387;&#32553;&#21442;&#25968;&#65292;&#20174;&#32780;&#23454;&#29616;&#27169;&#22411;&#21387;&#32553;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#25110;&#26045;&#21152;&#26550;&#26500;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#35757;&#32451;&#12289;&#25512;&#29702;&#21644;&#27169;&#22411;&#23384;&#20648;&#24320;&#38144;&#21364;&#22312;&#19981;&#26029;&#22686;&#21152;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#33268;&#21147;&#20110;&#22312;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#20943;&#23569;&#27169;&#22411;&#21442;&#25968;&#30340;&#25968;&#37327;&#65292;&#20294;&#23427;&#20204;&#19981;&#21487;&#36991;&#20813;&#22320;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#21387;&#32553;&#27169;&#22411;&#25110;&#26045;&#21152;&#26550;&#26500;&#38480;&#21046;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#30693;&#35782;&#36716;&#21270;&#65288;KT&#65289;&#65292;&#20854;&#20013;&#35757;&#32451;&#19968;&#20010;&#8220;&#32763;&#35793;&#8221;&#27169;&#22411;&#26469;&#25509;&#25910;&#36739;&#22823;&#27169;&#22411;&#30340;&#21442;&#25968;&#24182;&#29983;&#25104;&#21387;&#32553;&#21442;&#25968;&#12290;&#30693;&#35782;&#36716;&#21270;&#30340;&#27010;&#24565;&#20511;&#37492;&#33258;&#35821;&#35328;&#32763;&#35793;&#65292;&#23427;&#26377;&#25928;&#22320;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#23558;&#19981;&#21516;&#30340;&#35821;&#35328;&#36716;&#25442;&#20026;&#30456;&#21516;&#30340;&#24847;&#24605;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25506;&#32034;&#20102;&#31070;&#32463;&#32593;&#32476;&#23558;&#19981;&#21516;&#22823;&#23567;&#30340;&#27169;&#22411;&#36716;&#25442;&#20026;&#20445;&#25345;&#20854;&#21151;&#33021;&#24615;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;KT&#26694;&#26550;&#65292;&#20171;&#32461;&#20102;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning has witnessed significant advancements in recent years at the cost of increasing training, inference, and model storage overhead. While existing model compression methods strive to reduce the number of model parameters while maintaining high accuracy, they inevitably necessitate the re-training of the compressed model or impose architectural constraints. To overcome these limitations, this paper presents a novel framework, termed \textbf{K}nowledge \textbf{T}ranslation (KT), wherein a ``translation'' model is trained to receive the parameters of a larger model and generate compressed parameters. The concept of KT draws inspiration from language translation, which effectively employs neural networks to convert different languages, maintaining identical meaning. Accordingly, we explore the potential of neural networks to convert models of disparate sizes, while preserving their functionality. We propose a comprehensive framework for KT, introduce data augmentation strategie
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FSFC&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#35299;&#20915;&#20102;&#22312;&#20855;&#26377;&#20998;&#31867;&#21709;&#24212;&#21644;&#32437;&#21521;&#29305;&#24449;&#30340;&#24773;&#20917;&#19979;&#21516;&#26102;&#36827;&#34892;&#21151;&#33021;&#25968;&#25454;&#29305;&#24449;&#36873;&#25321;&#21644;&#20998;&#31867;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.05765</link><description>&lt;p&gt;
&#21151;&#33021;&#25968;&#25454;&#20998;&#31867;&#30340;&#29305;&#24449;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Feature Selection for Functional Data Classification. (arXiv:2401.05765v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05765
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FSFC&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#35299;&#20915;&#20102;&#22312;&#20855;&#26377;&#20998;&#31867;&#21709;&#24212;&#21644;&#32437;&#21521;&#29305;&#24449;&#30340;&#24773;&#20917;&#19979;&#21516;&#26102;&#36827;&#34892;&#21151;&#33021;&#25968;&#25454;&#29305;&#24449;&#36873;&#25321;&#21644;&#20998;&#31867;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21151;&#33021;&#25968;&#25454;&#20998;&#26512;&#24050;&#32463;&#25104;&#20026;&#35768;&#22810;&#38656;&#35201;&#25972;&#21512;&#21644;&#35299;&#37322;&#22797;&#26434;&#25968;&#25454;&#30340;&#24403;&#20195;&#31185;&#23398;&#39046;&#22495;&#20013;&#30340;&#20851;&#38190;&#24037;&#20855;&#12290;&#27492;&#22806;&#65292;&#26032;&#25216;&#26415;&#30340;&#20986;&#29616;&#20419;&#36827;&#20102;&#22823;&#37327;&#32437;&#21521;&#21464;&#37327;&#30340;&#25910;&#38598;&#65292;&#20351;&#24471;&#29305;&#24449;&#36873;&#25321;&#23545;&#20110;&#36991;&#20813;&#36807;&#25311;&#21512;&#21644;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FSFC&#65288;&#21151;&#33021;&#20998;&#31867;&#29305;&#24449;&#36873;&#25321;&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#35299;&#20915;&#20102;&#22312;&#20855;&#26377;&#20998;&#31867;&#21709;&#24212;&#21644;&#32437;&#21521;&#29305;&#24449;&#30340;&#24773;&#20917;&#19979;&#21516;&#26102;&#36827;&#34892;&#21151;&#33021;&#25968;&#25454;&#29305;&#24449;&#36873;&#25321;&#21644;&#20998;&#31867;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#19968;&#20010;&#26032;&#23450;&#20041;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#23558;&#36923;&#36753;&#25439;&#22833;&#21644;&#21151;&#33021;&#29305;&#24449;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#35782;&#21035;&#29992;&#20110;&#20998;&#31867;&#30340;&#26368;&#20851;&#38190;&#29305;&#24449;&#12290;&#20026;&#20102;&#35299;&#20915;&#26368;&#23567;&#21270;&#36807;&#31243;&#65292;&#25105;&#20204;&#20351;&#29992;&#21151;&#33021;&#20027;&#25104;&#20998;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;&#29256;&#26412;&#30340;&#21452;&#22686;&#24191;Lagrange&#31639;&#27861;&#65292;&#21033;&#29992;&#20102;&#12290;&#12290;&#12290;
&lt;/p&gt;
&lt;p&gt;
Functional data analysis has emerged as a crucial tool in many contemporary scientific domains that require the integration and interpretation of complex data. Moreover, the advent of new technologies has facilitated the collection of a large number of longitudinal variables, making feature selection pivotal for avoiding overfitting and improving prediction performance. This paper introduces a novel methodology called FSFC (Feature Selection for Functional Classification), that addresses the challenge of jointly performing feature selection and classification of functional data in scenarios with categorical responses and longitudinal features. Our approach tackles a newly defined optimization problem that integrates logistic loss and functional features to identify the most crucial features for classification. To address the minimization procedure, we employ functional principal components and develop a new adaptive version of the Dual Augmented Lagrangian algorithm that leverages the 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#23545;HVAC&#25511;&#21046;&#30340;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20102;&#23454;&#39564;&#35780;&#20272;&#65292;&#21457;&#29616;SAC&#21644;TD3&#31561;&#31639;&#27861;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#20855;&#26377;&#28508;&#21147;&#65292;&#24182;&#25581;&#31034;&#20102;&#19982;&#27867;&#21270;&#21644;&#22686;&#37327;&#23398;&#20064;&#30456;&#20851;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.05737</link><description>&lt;p&gt;
HVAC&#25511;&#21046;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#23454;&#39564;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
An experimental evaluation of Deep Reinforcement Learning algorithms for HVAC control. (arXiv:2401.05737v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05737
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#23545;HVAC&#25511;&#21046;&#30340;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20102;&#23454;&#39564;&#35780;&#20272;&#65292;&#21457;&#29616;SAC&#21644;TD3&#31561;&#31639;&#27861;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#20855;&#26377;&#28508;&#21147;&#65292;&#24182;&#25581;&#31034;&#20102;&#19982;&#27867;&#21270;&#21644;&#22686;&#37327;&#23398;&#20064;&#30456;&#20851;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26262;&#36890;&#31354;&#35843;&#31995;&#32479;&#26159;&#21830;&#19994;&#21644;&#23621;&#20303;&#24314;&#31569;&#33021;&#28304;&#28040;&#32791;&#30340;&#37325;&#35201;&#39537;&#21160;&#22240;&#32032;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#32988;&#36807;&#20256;&#32479;&#30340;&#21453;&#24212;&#24335;&#25511;&#21046;&#22120;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#35299;&#20915;&#26041;&#26696;&#36890;&#24120;&#26159;&#20026;&#29305;&#23450;&#35774;&#32622;&#32780;&#35774;&#35745;&#30340;&#65292;&#24182;&#19988;&#32570;&#20047;&#21487;&#27604;&#24615;&#30340;&#26631;&#20934;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#25991;&#37319;&#29992;Sinergym&#26694;&#26550;&#65292;&#20197;&#33298;&#36866;&#24230;&#21644;&#33021;&#28304;&#28040;&#32791;&#20026;&#35780;&#21028;&#26631;&#20934;&#65292;&#23545;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;HVAC&#25511;&#21046;&#26041;&#38754;&#36827;&#34892;&#20102;&#20851;&#38190;&#21644;&#21487;&#37325;&#29616;&#30340;&#35780;&#20272;&#12290;&#30740;&#31350;&#36890;&#36807;&#26816;&#26597;&#25511;&#21046;&#22120;&#30340;&#40065;&#26834;&#24615;&#12289;&#36866;&#24212;&#24615;&#21644;&#20248;&#21270;&#30446;&#26631;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#30830;&#35748;&#20102;SAC&#21644;TD3&#31561;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#30340;&#28508;&#21147;&#65292;&#24182;&#25581;&#31034;&#20102;&#19982;&#27867;&#21270;&#21644;&#22686;&#37327;&#23398;&#20064;&#30456;&#20851;&#30340;&#20960;&#20010;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Heating, Ventilation, and Air Conditioning (HVAC) systems are a major driver of energy consumption in commercial and residential buildings. Recent studies have shown that Deep Reinforcement Learning (DRL) algorithms can outperform traditional reactive controllers. However, DRL-based solutions are generally designed for ad hoc setups and lack standardization for comparison. To fill this gap, this paper provides a critical and reproducible evaluation, in terms of comfort and energy consumption, of several state-of-the-art DRL algorithms for HVAC control. The study examines the controllers' robustness, adaptability, and trade-off between optimization goals by using the Sinergym framework. The results obtained confirm the potential of DRL algorithms, such as SAC and TD3, in complex scenarios and reveal several challenges related to generalization and incremental learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#23545;&#35937;&#30340;&#25193;&#25955;&#25216;&#26415;&#65292;&#36890;&#36807;&#20998;&#37197;&#26356;&#22810;&#30340;&#35745;&#31639;&#36164;&#28304;&#32473;&#21069;&#26223;&#32534;&#36753;&#21306;&#22495;&#26469;&#23454;&#29616;&#35270;&#39057;&#32534;&#36753;&#30340;&#39640;&#25928;&#29575;&#65292;&#20174;&#32780;&#22823;&#22823;&#25552;&#39640;&#20102;&#36895;&#24230;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2401.05735</link><description>&lt;p&gt;
&#38754;&#21521;&#23545;&#35937;&#30340;&#25193;&#25955;&#25216;&#26415;&#23454;&#29616;&#39640;&#25928;&#35270;&#39057;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Object-Centric Diffusion for Efficient Video Editing. (arXiv:2401.05735v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05735
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#23545;&#35937;&#30340;&#25193;&#25955;&#25216;&#26415;&#65292;&#36890;&#36807;&#20998;&#37197;&#26356;&#22810;&#30340;&#35745;&#31639;&#36164;&#28304;&#32473;&#21069;&#26223;&#32534;&#36753;&#21306;&#22495;&#26469;&#23454;&#29616;&#35270;&#39057;&#32534;&#36753;&#30340;&#39640;&#25928;&#29575;&#65292;&#20174;&#32780;&#22823;&#22823;&#25552;&#39640;&#20102;&#36895;&#24230;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#35270;&#39057;&#32534;&#36753;&#24050;&#32463;&#36798;&#21040;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#36136;&#37327;&#65292;&#24182;&#19988;&#21487;&#20197;&#26681;&#25454;&#32534;&#36753;&#25552;&#31034;&#26469;&#36716;&#25442;&#35270;&#39057;&#30340;&#20840;&#23616;&#39118;&#26684;&#12289;&#23616;&#37096;&#32467;&#26500;&#21644;&#23646;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#36890;&#24120;&#38656;&#35201;&#20351;&#29992;&#22823;&#37327;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#36164;&#28304;&#26469;&#29983;&#25104;&#20855;&#26377;&#26102;&#24207;&#19968;&#33268;&#24615;&#30340;&#24103;&#65292;&#21487;&#33021;&#28041;&#21450;&#25193;&#25955;&#21453;&#28436;&#21644;/&#25110;&#36328;&#24103;&#27880;&#24847;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#36825;&#31181;&#20302;&#25928;&#24615;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#20462;&#25913;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#36895;&#24230;&#21516;&#26102;&#20445;&#25345;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#38754;&#21521;&#23545;&#35937;&#30340;&#25193;&#25955;&#25216;&#26415;&#65288;OCD&#65289;&#65292;&#36890;&#36807;&#23558;&#35745;&#31639;&#36164;&#28304;&#26356;&#22810;&#22320;&#20998;&#37197;&#32473;&#23545;&#24863;&#30693;&#36136;&#37327;&#26356;&#37325;&#35201;&#30340;&#21069;&#26223;&#32534;&#36753;&#21306;&#22495;&#65292;&#36827;&#19968;&#27493;&#38477;&#20302;&#24310;&#36831;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#26032;&#30340;&#25552;&#26696;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#65306;i&#65289;&#38754;&#21521;&#23545;&#35937;&#30340;&#37319;&#26679;&#65292;&#23558;&#29992;&#20110;&#26174;&#33879;&#21306;&#22495;&#25110;&#32972;&#26223;&#30340;&#25193;&#25955;&#27493;&#39588;&#19982;&#29992;&#20110;&#21069;&#26223;&#30340;&#25193;&#25955;&#27493;&#39588;&#20998;&#31163;&#24320;&#26469;&#65292;&#23558;&#22823;&#37096;&#20998;&#27169;&#22411;&#23481;&#37327;&#20998;&#37197;&#32473;&#21069;&#32773;&#65307;ii&#65289;&#38754;&#21521;&#23545;&#35937;&#30340;3D&#20196;&#29260;&#21512;&#24182;&#65292;&#29992;&#20110;&#25913;&#21892;&#21069;&#26223;&#21644;&#32972;&#26223;&#20043;&#38388;&#30340;&#28151;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion-based video editing have reached impressive quality and can transform either the global style, local structure, and attributes of given video inputs, following textual edit prompts. However, such solutions typically incur heavy memory and computational costs to generate temporally-coherent frames, either in the form of diffusion inversion and/or cross-frame attention. In this paper, we conduct an analysis of such inefficiencies, and suggest simple yet effective modifications that allow significant speed-ups whilst maintaining quality. Moreover, we introduce Object-Centric Diffusion, coined as OCD, to further reduce latency by allocating computations more towards foreground edited regions that are arguably more important for perceptual quality. We achieve this by two novel proposals: i) Object-Centric Sampling, decoupling the diffusion steps spent on salient regions or background, allocating most of the model capacity to the former, and ii) Object-Centric 3D Token Merging, whi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#36830;&#25509;&#20027;&#20041;&#38899;&#32032;&#35782;&#21035;&#22120;&#30340;&#31867;&#29109;&#26469;&#39044;&#27979;&#38899;&#32032;&#31867;&#20043;&#38388;&#30340;&#26102;&#38388;&#36793;&#30028;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;&#26041;&#27861;&#30340;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.05717</link><description>&lt;p&gt;
&#36890;&#36807;&#36830;&#25509;&#20027;&#20041;&#38899;&#32032;&#35782;&#21035;&#20013;&#30340;&#31867;&#29109;&#27979;&#37327;&#36827;&#34892;&#20998;&#21106;&#36793;&#30028;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Segment Boundary Detection via Class Entropy Measurements in Connectionist Phoneme Recognition. (arXiv:2401.05717v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05717
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#36830;&#25509;&#20027;&#20041;&#38899;&#32032;&#35782;&#21035;&#22120;&#30340;&#31867;&#29109;&#26469;&#39044;&#27979;&#38899;&#32032;&#31867;&#20043;&#38388;&#30340;&#26102;&#38388;&#36793;&#30028;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;&#26041;&#27861;&#30340;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#36830;&#25509;&#20027;&#20041;&#38899;&#32032;&#35782;&#21035;&#22120;&#30340;&#36755;&#20986;&#30340;&#31867;&#29109;&#26469;&#39044;&#27979;&#38899;&#32032;&#31867;&#20043;&#38388;&#30340;&#26102;&#38388;&#36793;&#30028;&#30340;&#21487;&#33021;&#24615;&#12290;&#20854;&#21407;&#29702;&#26159;&#65292;&#29109;&#30340;&#20540;&#24212;&#35813;&#22312;&#20004;&#20010;&#30001;&#35782;&#21035;&#32593;&#32476;&#24456;&#22909;&#24314;&#27169;&#65288;&#24050;&#30693;&#65289;&#30340;&#29255;&#27573;&#20043;&#38388;&#30340;&#36807;&#28193;&#38468;&#36817;&#22686;&#21152;&#65292;&#22240;&#20026;&#23427;&#26159;&#19981;&#30830;&#23450;&#24615;&#30340;&#19968;&#20010;&#24230;&#37327;&#12290;&#31867;&#29109;&#30340;&#20248;&#21183;&#22312;&#20110;&#23427;&#30340;&#31616;&#21333;&#24615;&#65292;&#22240;&#20026;&#27599;&#20010;&#31867;&#30340;&#21518;&#39564;&#27010;&#29575;&#22312;&#36830;&#25509;&#20027;&#20041;&#38899;&#32032;&#35782;&#21035;&#20013;&#26159;&#21487;&#29992;&#30340;&#12290;&#29109;&#21644;&#19968;&#20123;&#22522;&#20110;&#29109;&#24046;&#24322;&#30340;&#24230;&#37327;&#34987;&#21333;&#29420;&#21644;&#32452;&#21512;&#20351;&#29992;&#12290;&#29992;&#20110;&#39044;&#27979;&#36793;&#30028;&#30340;&#20915;&#31574;&#26041;&#27861;&#20174;&#31616;&#21333;&#30340;&#38408;&#20540;&#21040;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#36807;&#31243;&#19981;&#31561;&#12290;&#19981;&#21516;&#30340;&#26041;&#27861;&#22312;&#31934;&#30830;&#24230;&#26041;&#38754;&#36827;&#34892;&#27604;&#36739;&#65292;&#20197;C&#65288;&#22312;&#21442;&#32771;&#26102;&#38388;&#30340;10&#25110;20&#27627;&#31186;&#20869;&#39044;&#27979;&#30340;&#36793;&#30028;&#25968;&#37327;&#65289;&#19982;&#39044;&#27979;&#36793;&#30028;&#24635;&#25968;&#20043;&#38388;&#30340;&#27604;&#29575;&#21644;&#21484;&#22238;&#29575;&#20026;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article investigates the possibility to use the class entropy of the output of a connectionist phoneme recogniser to predict time boundaries between phonetic classes. The rationale is that the value of the entropy should increase in proximity of a transition between two segments that are well modelled (known) by the recognition network since it is a measure of uncertainty. The advantage of this measure is its simplicity as the posterior probabilities of each class are available in connectionist phoneme recognition. The entropy and a number of measures based on differentiation of the entropy are used in isolation and in combination. The decision methods for predicting the boundaries range from simple thresholds to neural network based procedure. The different methods are compared with respect to their precision, measured in terms of the ratio between the number C of predicted boundaries within 10 or 20 msec of the reference and the total number of predicted boundaries, and recall, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#26597;&#35810;&#40657;&#30418;&#20989;&#25968;&#26469;&#20272;&#35745;&#24402;&#19968;&#21270;&#24120;&#25968;&#30340;&#38382;&#39064;&#65292;&#21457;&#29616;&#38382;&#39064;&#30340;&#38590;&#24230;&#21462;&#20915;&#20110;&#38382;&#39064;&#21442;&#25968;$\lambda$&#30340;&#22823;&#23567;&#65292;&#24403;$\lambda$&#36235;&#36817;&#20110;&#38646;&#26102;&#31867;&#20284;&#20110;&#36125;&#21494;&#26031;&#31215;&#20998;(BQ)&#65292;&#24403;$\lambda$&#36235;&#36817;&#20110;&#26080;&#31351;&#22823;&#26102;&#31867;&#20284;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;(BO)&#65292;&#19988;&#36825;&#31181;&#27169;&#24335;&#36866;&#29992;&#20110;&#23384;&#22312;&#22122;&#22768;&#30340;&#24773;&#20917;&#12290;&#32467;&#26524;&#24471;&#21040;&#20102;&#31639;&#27861;&#26080;&#20851;&#30340;&#19979;&#30028;&#21644;&#19978;&#30028;&#30340;&#25903;&#25345;&#65292;&#20197;&#21450;&#27169;&#25311;&#30740;&#31350;&#30340;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2401.05716</link><description>&lt;p&gt;
&#22522;&#20110;&#26680;&#20989;&#25968;&#30340;&#24402;&#19968;&#21270;&#24120;&#25968;&#20272;&#35745;&#65306;&#36830;&#25509;&#36125;&#21494;&#26031;&#31215;&#20998;&#21644;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Kernelized Normalizing Constant Estimation: Bridging Bayesian Quadrature and Bayesian Optimization. (arXiv:2401.05716v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05716
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#26597;&#35810;&#40657;&#30418;&#20989;&#25968;&#26469;&#20272;&#35745;&#24402;&#19968;&#21270;&#24120;&#25968;&#30340;&#38382;&#39064;&#65292;&#21457;&#29616;&#38382;&#39064;&#30340;&#38590;&#24230;&#21462;&#20915;&#20110;&#38382;&#39064;&#21442;&#25968;$\lambda$&#30340;&#22823;&#23567;&#65292;&#24403;$\lambda$&#36235;&#36817;&#20110;&#38646;&#26102;&#31867;&#20284;&#20110;&#36125;&#21494;&#26031;&#31215;&#20998;(BQ)&#65292;&#24403;$\lambda$&#36235;&#36817;&#20110;&#26080;&#31351;&#22823;&#26102;&#31867;&#20284;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;(BO)&#65292;&#19988;&#36825;&#31181;&#27169;&#24335;&#36866;&#29992;&#20110;&#23384;&#22312;&#22122;&#22768;&#30340;&#24773;&#20917;&#12290;&#32467;&#26524;&#24471;&#21040;&#20102;&#31639;&#27861;&#26080;&#20851;&#30340;&#19979;&#30028;&#21644;&#19978;&#30028;&#30340;&#25903;&#25345;&#65292;&#20197;&#21450;&#27169;&#25311;&#30740;&#31350;&#30340;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#36890;&#36807;&#40657;&#30418;&#20989;&#25968;&#26597;&#35810;&#26469;&#20272;&#35745;&#24402;&#19968;&#21270;&#24120;&#25968;$\int e^{-\lambda f(x)}dx$&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;$f$&#23646;&#20110;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;(RKHS)&#65292;&#32780;$\lambda$&#26159;&#19968;&#20010;&#38382;&#39064;&#21442;&#25968;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20026;&#20102;&#22312;&#30456;&#23545;&#35823;&#24046;&#36739;&#23567;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;&#24402;&#19968;&#21270;&#24120;&#25968;&#65292;&#38590;&#24230;&#30340;&#32423;&#21035;&#21462;&#20915;&#20110;$\lambda$&#30340;&#20540;&#65306;&#24403;$\lambda$&#36235;&#36817;&#20110;&#38646;&#26102;&#65292;&#38382;&#39064;&#31867;&#20284;&#20110;&#36125;&#21494;&#26031;&#31215;&#20998;(BQ)&#65292;&#32780;&#24403;$\lambda$&#36235;&#36817;&#20110;&#26080;&#31351;&#22823;&#26102;&#65292;&#38382;&#39064;&#31867;&#20284;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;(BO)&#12290;&#26356;&#19968;&#33324;&#22320;&#65292;&#38382;&#39064;&#22312;BQ&#21644;BO&#20043;&#38388;&#21464;&#21270;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21363;&#20351;&#22312;&#20989;&#25968;&#35780;&#20272;&#23384;&#22312;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#27169;&#24335;&#20173;&#28982;&#36866;&#29992;&#65292;&#20026;&#35813;&#20027;&#39064;&#24102;&#26469;&#20102;&#26032;&#30340;&#26041;&#38754;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#24471;&#21040;&#20102;&#31639;&#27861;&#26080;&#20851;&#30340;&#19979;&#30028;&#21644;&#31639;&#27861;&#19978;&#30028;&#30340;&#25903;&#25345;&#65292;&#20197;&#21450;&#22312;&#21508;&#31181;&#22522;&#20934;&#20989;&#25968;&#19978;&#36827;&#34892;&#30340;&#27169;&#25311;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the problem of estimating the normalizing constant $\int e^{-\lambda f(x)}dx$ through queries to the black-box function $f$, where $f$ belongs to a reproducing kernel Hilbert space (RKHS), and $\lambda$ is a problem parameter. We show that to estimate the normalizing constant within a small relative error, the level of difficulty depends on the value of $\lambda$: When $\lambda$ approaches zero, the problem is similar to Bayesian quadrature (BQ), while when $\lambda$ approaches infinity, the problem is similar to Bayesian optimization (BO). More generally, the problem varies between BQ and BO. We find that this pattern holds true even when the function evaluations are noisy, bringing new aspects to this topic. Our findings are supported by both algorithm-independent lower bounds and algorithmic upper bounds, as well as simulation studies conducted on a variety of benchmark functions.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25968;&#25454;&#39640;&#25928;&#20803;&#23398;&#20064;&#31639;&#27861;&#30340;&#21019;&#26032;&#23460;&#20869;&#23450;&#20301;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#20219;&#21153;&#21152;&#26435;&#25439;&#22833;&#26469;&#25552;&#39640;&#36866;&#24212;&#24615;&#21644;&#23398;&#20064;&#25928;&#29575;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#22312;&#21160;&#24577;&#23460;&#20869;&#29615;&#22659;&#20013;&#30340;&#31283;&#20581;&#24615;&#21644;&#20248;&#36234;&#24615;&#65292;&#22312;&#24179;&#22343;&#27431;&#27663;&#36317;&#31163;&#19978;&#21462;&#24471;&#20102;23.13&#65285;&#30340;&#26174;&#33879;&#22686;&#30410;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#26377;&#38480;CSI&#25968;&#25454;&#30340;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2401.05711</link><description>&lt;p&gt;
&#22522;&#20110;CSI&#22270;&#20687;&#30340;&#23569;&#26679;&#26412;&#20803;&#23398;&#20064;&#21160;&#24577;&#23460;&#20869;&#25351;&#32441;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Dynamic Indoor Fingerprinting Localization based on Few-Shot Meta-Learning with CSI Images. (arXiv:2401.05711v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05711
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25968;&#25454;&#39640;&#25928;&#20803;&#23398;&#20064;&#31639;&#27861;&#30340;&#21019;&#26032;&#23460;&#20869;&#23450;&#20301;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#20219;&#21153;&#21152;&#26435;&#25439;&#22833;&#26469;&#25552;&#39640;&#36866;&#24212;&#24615;&#21644;&#23398;&#20064;&#25928;&#29575;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#22312;&#21160;&#24577;&#23460;&#20869;&#29615;&#22659;&#20013;&#30340;&#31283;&#20581;&#24615;&#21644;&#20248;&#36234;&#24615;&#65292;&#22312;&#24179;&#22343;&#27431;&#27663;&#36317;&#31163;&#19978;&#21462;&#24471;&#20102;23.13&#65285;&#30340;&#26174;&#33879;&#22686;&#30410;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#26377;&#38480;CSI&#25968;&#25454;&#30340;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25351;&#32441;&#23450;&#20301;&#22240;&#20854;&#26377;&#25928;&#24615;&#32780;&#21463;&#21040;&#38738;&#30544;&#65292;&#20294;&#25968;&#25454;&#37319;&#38598;&#25104;&#26412;&#39640;&#21644;&#22522;&#20110;&#38745;&#24577;&#25968;&#25454;&#24211;&#30340;&#20272;&#35745;&#19981;&#20934;&#30830;&#38480;&#21046;&#20102;&#20854;&#24212;&#29992;&#12290;&#38024;&#23545;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25968;&#25454;&#39640;&#25928;&#20803;&#23398;&#20064;&#31639;&#27861;&#30340;&#21019;&#26032;&#23460;&#20869;&#23450;&#20301;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#20197;&#20803;&#23398;&#20064;&#20013;&#8220;&#23398;&#20064;&#22914;&#20309;&#23398;&#20064;&#8221;&#30340;&#33539;&#20363;&#20026;&#22522;&#30784;&#65292;&#21033;&#29992;&#21382;&#21490;&#23450;&#20301;&#20219;&#21153;&#26469;&#25552;&#39640;&#22312;&#21160;&#24577;&#23460;&#20869;&#29615;&#22659;&#20013;&#30340;&#36866;&#24212;&#24615;&#21644;&#23398;&#20064;&#25928;&#29575;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20219;&#21153;&#21152;&#26435;&#25439;&#22833;&#26469;&#22686;&#24378;&#35813;&#26694;&#26550;&#20869;&#30340;&#30693;&#35782;&#20256;&#36882;&#12290;&#25105;&#20204;&#30340;&#32508;&#21512;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#22312;&#24403;&#21069;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#31283;&#20581;&#24615;&#21644;&#20248;&#36234;&#24615;&#65292;&#22312;&#24179;&#22343;&#27431;&#27663;&#36317;&#31163;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;23.13&#65285;&#22686;&#30410;&#65292;&#23588;&#20854;&#22312;&#20855;&#26377;&#26377;&#38480;CSI&#25968;&#25454;&#30340;&#22330;&#26223;&#19979;&#25928;&#26524;&#26126;&#26174;&#12290;
&lt;/p&gt;
&lt;p&gt;
While fingerprinting localization is favored for its effectiveness, it is hindered by high data acquisition costs and the inaccuracy of static database-based estimates. Addressing these issues, this letter presents an innovative indoor localization method using a data-efficient meta-learning algorithm. This approach, grounded in the ``Learning to Learn'' paradigm of meta-learning, utilizes historical localization tasks to improve adaptability and learning efficiency in dynamic indoor environments. We introduce a task-weighted loss to enhance knowledge transfer within this framework. Our comprehensive experiments confirm the method's robustness and superiority over current benchmarks, achieving a notable 23.13\% average gain in Mean Euclidean Distance, particularly effective in scenarios with limited CSI data.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23637;&#31034;&#20102;&#19968;&#31181;&#36866;&#24212;&#24615;&#20998;&#24067;&#24335;&#22870;&#21169;&#35780;&#35770;&#23478;&#26550;&#26500;&#65292;&#33021;&#22815;&#22312;&#26410;&#30693;&#25200;&#21160;&#30340;&#24773;&#20917;&#19979;&#24674;&#22797;&#30495;&#23454;&#22870;&#21169;&#65292;&#24182;&#22312;&#22810;&#20010;&#25511;&#21046;&#20219;&#21153;&#20013;&#21462;&#24471;&#36739;&#39640;&#30340;&#22238;&#25253;&#12290;</title><link>http://arxiv.org/abs/2401.05710</link><description>&lt;p&gt;
&#23545;&#25200;&#21160;&#22870;&#21169;&#24378;&#21270;&#23398;&#20064;&#30340;&#20998;&#24067;&#24335;&#22870;&#21169;&#35780;&#35770;&#23478;&#26550;&#26500;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
The Distributional Reward Critic Architecture for Perturbed-Reward Reinforcement Learning. (arXiv:2401.05710v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05710
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23637;&#31034;&#20102;&#19968;&#31181;&#36866;&#24212;&#24615;&#20998;&#24067;&#24335;&#22870;&#21169;&#35780;&#35770;&#23478;&#26550;&#26500;&#65292;&#33021;&#22815;&#22312;&#26410;&#30693;&#25200;&#21160;&#30340;&#24773;&#20917;&#19979;&#24674;&#22797;&#30495;&#23454;&#22870;&#21169;&#65292;&#24182;&#22312;&#22810;&#20010;&#25511;&#21046;&#20219;&#21153;&#20013;&#21462;&#24471;&#36739;&#39640;&#30340;&#22238;&#25253;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#26410;&#30693;&#22870;&#21169;&#25200;&#21160;&#30340;&#24773;&#20917;&#19979;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#23545;&#36825;&#20010;&#38382;&#39064;&#20570;&#20986;&#20102;&#24378;&#22823;&#30340;&#20551;&#35774;&#65292;&#21253;&#25324;&#22870;&#21169;&#24179;&#28369;&#24615;&#12289;&#24050;&#30693;&#25200;&#21160;&#21644;/&#25110;&#19981;&#20250;&#25913;&#21464;&#26368;&#20248;&#31574;&#30053;&#30340;&#25200;&#21160;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#26410;&#30693;&#20219;&#24847;&#25200;&#21160;&#30340;&#24773;&#20917;&#65292;&#36825;&#20123;&#25200;&#21160;&#23545;&#22870;&#21169;&#31354;&#38388;&#36827;&#34892;&#20102;&#31163;&#25955;&#21270;&#21644;&#27927;&#29260;&#65292;&#20294;&#22312;&#25200;&#21160;&#21518;&#65292;&#30495;&#23454;&#22870;&#21169;&#23646;&#20110;&#26368;&#39057;&#32321;&#35266;&#23519;&#21040;&#30340;&#31867;&#21035;&#12290;&#36825;&#31867;&#25200;&#21160;&#27867;&#21270;&#20102;&#29616;&#26377;&#30340;&#31867;&#21035;&#65288;&#24182;&#22312;&#26497;&#38480;&#24773;&#20917;&#19979;&#27867;&#21270;&#20102;&#25152;&#26377;&#36830;&#32493;&#26377;&#30028;&#25200;&#21160;&#65289;&#65292;&#24182;&#25112;&#32988;&#20102;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#20998;&#24067;&#24335;&#22870;&#21169;&#35780;&#35770;&#23478;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#22312;&#25216;&#26415;&#26465;&#20214;&#19979;&#23427;&#21487;&#20197;&#24674;&#22797;&#30495;&#23454;&#22870;&#21169;&#12290;&#22312;&#31163;&#25955;&#21644;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#30340;&#30446;&#26631;&#25200;&#21160;&#19979;&#65292;&#25105;&#20204;&#22312;40/57&#20010;&#29615;&#22659;&#20013;&#36194;&#21033;/&#24179;&#23616;&#65288;&#30456;&#23545;&#20110;&#26368;&#20339;&#22522;&#32447;&#30340;16/57&#65289;&#12290;&#21363;&#20351;&#22312;&#38750;&#30446;&#26631;&#25200;&#21160;&#19979;&#65292;&#25105;&#20204;&#20173;&#28982;&#32988;&#36807;&#35774;&#35745;&#20026;&#24102;&#26377;&#30446;&#26631;&#25200;&#21160;&#30340;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study reinforcement learning in the presence of an unknown reward perturbation. Existing methodologies for this problem make strong assumptions including reward smoothness, known perturbations, and/or perturbations that do not modify the optimal policy. We study the case of unknown arbitrary perturbations that discretize and shuffle reward space, but have the property that the true reward belongs to the most frequently observed class after perturbation. This class of perturbations generalizes existing classes (and, in the limit, all continuous bounded perturbations) and defeats existing methods. We introduce an adaptive distributional reward critic and show theoretically that it can recover the true rewards under technical conditions. Under the targeted perturbation in discrete and continuous control tasks, we win/tie the highest return in 40/57 settings (compared to 16/57 for the best baseline). Even under the untargeted perturbation, we still win an edge over the baseline designed
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#30740;&#31350;&#22914;&#20309;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#36741;&#21161;&#25171;&#30772;&#32593;&#32476;&#25915;&#20987;&#29983;&#21629;&#21608;&#26399;&#30340;&#27599;&#20010;&#38454;&#27573;&#65292;&#36890;&#36807;&#22788;&#29702;&#21644;&#23398;&#20064;&#26469;&#33258;&#24322;&#26500;&#32593;&#32476;&#23041;&#32961;&#25968;&#25454;&#65292;&#20197;&#22686;&#24378;&#38450;&#24481;&#25514;&#26045;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.05680</link><description>&lt;p&gt;
&#22312;&#36741;&#21161;&#38450;&#24481;&#24615;&#32593;&#32476;&#25805;&#20316;&#20013;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Use of Graph Neural Networks in Aiding Defensive Cyber Operations. (arXiv:2401.05680v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05680
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#30740;&#31350;&#22914;&#20309;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#36741;&#21161;&#25171;&#30772;&#32593;&#32476;&#25915;&#20987;&#29983;&#21629;&#21608;&#26399;&#30340;&#27599;&#20010;&#38454;&#27573;&#65292;&#36890;&#36807;&#22788;&#29702;&#21644;&#23398;&#20064;&#26469;&#33258;&#24322;&#26500;&#32593;&#32476;&#23041;&#32961;&#25968;&#25454;&#65292;&#20197;&#22686;&#24378;&#38450;&#24481;&#25514;&#26045;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19968;&#20010;&#26085;&#30410;&#20114;&#32852;&#30340;&#19990;&#30028;&#20013;&#65292;&#20449;&#24687;&#26159;&#29616;&#20195;&#31038;&#20250;&#30340;&#21629;&#33033;&#65292;&#24120;&#35265;&#30340;&#32593;&#32476;&#25915;&#20987;&#30772;&#22351;&#20102;&#25968;&#23383;&#31995;&#32479;&#21644;&#20449;&#24687;&#30340;&#26426;&#23494;&#24615;&#12289;&#23436;&#25972;&#24615;&#21644;&#21487;&#29992;&#24615;&#12290;&#27492;&#22806;&#65292;&#32593;&#32476;&#25915;&#20987;&#26681;&#25454;&#30446;&#26631;&#30340;&#19981;&#21516;&#32780;&#19981;&#21516;&#65292;&#24182;&#19988;&#36805;&#36895;&#28436;&#21464;&#20197;&#25513;&#30422;&#38450;&#24481;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#20856;&#22411;&#30340;&#32593;&#32476;&#25915;&#20987;&#23637;&#31034;&#20102;&#20174;&#25915;&#20987;&#21457;&#36215;&#21040;&#26368;&#32456;&#35299;&#20915;&#30340;&#19968;&#31995;&#21015;&#38454;&#27573;&#65292;&#31216;&#20026;&#25915;&#20987;&#29983;&#21629;&#21608;&#26399;&#12290;&#36825;&#20123;&#22810;&#26679;&#30340;&#29305;&#24449;&#21644;&#32593;&#32476;&#25915;&#20987;&#30340;&#19981;&#25032;&#28436;&#36827;&#20351;&#24471;&#32593;&#32476;&#38450;&#24481;&#37319;&#32435;&#20102;&#29616;&#20195;&#26041;&#27861;&#65292;&#22914;&#26426;&#22120;&#23398;&#20064;&#65292;&#20197;&#22686;&#24378;&#38450;&#24481;&#25514;&#26045;&#24182;&#25171;&#30772;&#25915;&#20987;&#29983;&#21629;&#21608;&#26399;&#12290;&#22312;&#37319;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20013;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#22788;&#29702;&#21644;&#23398;&#20064;&#26469;&#33258;&#24322;&#26500;&#32593;&#32476;&#23041;&#32961;&#25968;&#25454;&#26469;&#22686;&#24378;&#38450;&#24481;&#25514;&#26045;&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#30740;&#31350;&#22312;&#36741;&#21161;&#25171;&#30772;&#25915;&#20987;&#29983;&#21629;&#21608;&#26399;&#30340;&#27599;&#20010;&#38454;&#27573;&#20013;&#24212;&#29992;GNN&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In an increasingly interconnected world, where information is the lifeblood of modern society, regular cyber-attacks sabotage the confidentiality, integrity, and availability of digital systems and information. Additionally, cyber-attacks differ depending on the objective and evolve rapidly to disguise defensive systems. However, a typical cyber-attack demonstrates a series of stages from attack initiation to final resolution, called an attack life cycle. These diverse characteristics and the relentless evolution of cyber attacks have led cyber defense to adopt modern approaches like Machine Learning to bolster defensive measures and break the attack life cycle. Among the adopted ML approaches, Graph Neural Networks have emerged as a promising approach for enhancing the effectiveness of defensive measures due to their ability to process and learn from heterogeneous cyber threat data. In this paper, we look into the application of GNNs in aiding to break each stage of one of the most re
&lt;/p&gt;</description></item><item><title>EsaCL&#26159;&#19968;&#31181;&#39640;&#25928;&#31232;&#30095;&#27169;&#22411;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#20462;&#21098;&#20887;&#20313;&#21442;&#25968;&#24182;&#36991;&#20813;&#37325;&#26032;&#35757;&#32451;&#65292;&#35299;&#20915;&#20102;&#25345;&#32493;&#23398;&#20064;&#20013;&#23384;&#20648;&#21644;&#35745;&#31639;&#38656;&#27714;&#22686;&#21152;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.05667</link><description>&lt;p&gt;
EsaCL: &#39640;&#25928;&#31232;&#30095;&#27169;&#22411;&#30340;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
EsaCL: Efficient Continual Learning of Sparse Models. (arXiv:2401.05667v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05667
&lt;/p&gt;
&lt;p&gt;
EsaCL&#26159;&#19968;&#31181;&#39640;&#25928;&#31232;&#30095;&#27169;&#22411;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#20462;&#21098;&#20887;&#20313;&#21442;&#25968;&#24182;&#36991;&#20813;&#37325;&#26032;&#35757;&#32451;&#65292;&#35299;&#20915;&#20102;&#25345;&#32493;&#23398;&#20064;&#20013;&#23384;&#20648;&#21644;&#35745;&#31639;&#38656;&#27714;&#22686;&#21152;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25345;&#32493;&#23398;&#20064;&#29615;&#22659;&#20013;&#65292;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#22312;&#19981;&#24536;&#35760;&#22914;&#20309;&#25191;&#34892;&#20808;&#21069;&#23398;&#20064;&#20219;&#21153;&#30340;&#24773;&#20917;&#19979;&#65292;&#39640;&#25928;&#22320;&#23398;&#20064;&#19968;&#31995;&#21015;&#20219;&#21153;&#12290;&#35768;&#22810;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#36807;&#22312;&#20808;&#21069;&#20219;&#21153;&#19978;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#25110;&#25193;&#23637;&#27169;&#22411;&#20197;&#36866;&#24212;&#26032;&#20219;&#21153;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#38754;&#20020;&#23384;&#20648;&#21644;&#35745;&#31639;&#38656;&#27714;&#30340;&#22686;&#21152;&#38382;&#39064;&#65292;&#32780;&#23545;&#20110;&#31232;&#30095;&#27169;&#22411;&#26469;&#35828;&#65292;&#30001;&#20110;&#38656;&#35201;&#26114;&#36149;&#30340;&#31232;&#30095;&#21270;&#21518;&#37325;&#26032;&#35757;&#32451;&#65292;&#36825;&#20010;&#38382;&#39064;&#26356;&#21152;&#20005;&#37325;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39640;&#25928;&#31232;&#30095;&#27169;&#22411;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65288;EsaCL&#65289;&#65292;&#23427;&#21487;&#20197;&#33258;&#21160;&#20462;&#21098;&#20887;&#20313;&#21442;&#25968;&#65292;&#32780;&#19981;&#20250;&#23545;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#65292;&#24182;&#19988;&#21487;&#20197;&#36991;&#20813;&#37325;&#26032;&#35757;&#32451;&#30340;&#38656;&#35201;&#12290;&#25105;&#20204;&#23545;&#21442;&#25968;&#20462;&#21098;&#30340;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#25439;&#22833;&#20989;&#25968;&#23545;&#27169;&#22411;&#21442;&#25968;&#25935;&#24863;&#24615;&#30340;&#26041;&#21521;&#24615;&#20462;&#21098;&#65288;SDP&#65289;&#31574;&#30053;&#12290;SDP&#20445;&#35777;&#20102;&#27169;&#22411;&#22312;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#30340;&#24615;&#33021;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
A key challenge in the continual learning setting is to efficiently learn a sequence of tasks without forgetting how to perform previously learned tasks. Many existing approaches to this problem work by either retraining the model on previous tasks or by expanding the model to accommodate new tasks. However, these approaches typically suffer from increased storage and computational requirements, a problem that is worsened in the case of sparse models due to need for expensive re-training after sparsification. To address this challenge, we propose a new method for efficient continual learning of sparse models (EsaCL) that can automatically prune redundant parameters without adversely impacting the model's predictive power, and circumvent the need of retraining. We conduct a theoretical analysis of loss landscapes with parameter pruning, and design a directional pruning (SDP) strategy that is informed by the sharpness of the loss function with respect to the model parameters. SDP ensures
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20256;&#36755;&#29109;&#27969;&#30340;&#26041;&#27861;&#26469;&#35786;&#26029;&#24037;&#19994;&#31995;&#32479;&#33021;&#25928;&#24322;&#24120;&#29366;&#24577;&#30340;&#26681;&#26412;&#21407;&#22240;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.05664</link><description>&lt;p&gt;
&#22522;&#20110;&#20256;&#36755;&#29109;&#27969;&#30340;&#33021;&#25928;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Root Cause Analysis on Energy Efficiency with Transfer Entropy Flow. (arXiv:2401.05664v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05664
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20256;&#36755;&#29109;&#27969;&#30340;&#26041;&#27861;&#26469;&#35786;&#26029;&#24037;&#19994;&#31995;&#32479;&#33021;&#25928;&#24322;&#24120;&#29366;&#24577;&#30340;&#26681;&#26412;&#21407;&#22240;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#25928;&#26159;&#24037;&#19994;&#39046;&#22495;&#30340;&#19968;&#20010;&#37325;&#35201;&#20851;&#27880;&#28857;&#12290;&#25214;&#20986;&#33021;&#25928;&#24322;&#24120;&#29366;&#24577;&#30340;&#26681;&#26412;&#21407;&#22240;&#21487;&#20197;&#24110;&#21161;&#25913;&#21892;&#24037;&#19994;&#31995;&#32479;&#30340;&#33021;&#25928;&#65292;&#20174;&#32780;&#33410;&#32422;&#33021;&#28304;&#25104;&#26412;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#20256;&#36755;&#29109;&#65288;Transfer Entropy&#65292;TE&#65289;&#23545;&#24037;&#19994;&#31995;&#32479;&#30340;&#33021;&#25928;&#36827;&#34892;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;TE&#27969;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#32771;&#34385;&#20174;&#21508;&#20010;&#23376;&#31995;&#32479;&#30340;&#29289;&#29702;&#27979;&#37327;&#21040;&#33021;&#25928;&#25351;&#26631;&#30340;TE&#27969;&#21160;&#26469;&#20316;&#20026;&#35786;&#26029;&#31995;&#32479;&#33021;&#25928;&#24322;&#24120;&#29366;&#24577;&#30340;&#22240;&#26524;&#24378;&#24230;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#22522;&#20110;Copula&#29109;&#30340;&#38750;&#21442;&#25968;&#21270;TE&#20272;&#35745;&#22120;&#12290;&#25105;&#20204;&#23545;&#20174;&#21387;&#32553;&#31354;&#27668;&#31995;&#32479;&#25910;&#38598;&#30340;&#30495;&#23454;&#25968;&#25454;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;TE&#27969;&#26041;&#27861;&#25104;&#21151;&#22320;&#30830;&#23450;&#20102;&#31995;&#32479;&#33021;&#25928;&#30340;&#26681;&#26412;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
Energy efficiency is a big concern in industrial sectors. Finding the root cause of anomaly state of energy efficiency can help to improve energy efficiency of industrial systems and therefore save energy cost. In this research, we propose to use transfer entropy (TE) for root cause analysis on energy efficiency of industrial systems. A method, called TE flow, is proposed in that a TE flow from physical measurements of each subsystem to the energy efficiency indicator along timeline is considered as causal strength for diagnosing root cause of anomaly states of energy efficiency of a system. The copula entropy-based nonparametric TE estimator is used in the proposed method. We conducted experiments on real data collected from a compressing air system to verify the proposed method. Experimental results show that the TE flow method successfully identified the root cause of the energy (in)efficiency of the system.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;AMIE&#65292;&#35813;&#31995;&#32479;&#21033;&#29992;&#33258;&#25105;&#23545;&#25112;&#30340;&#27169;&#25311;&#29615;&#22659;&#21644;&#33258;&#21160;&#21270;&#21453;&#39304;&#26426;&#21046;&#36827;&#34892;&#35786;&#26029;&#23545;&#35805;&#65292;&#24182;&#19988;&#36890;&#36807;&#35780;&#20272;&#30149;&#21490;&#37319;&#38598;&#12289;&#35786;&#26029;&#20934;&#30830;&#24615;&#12289;&#31649;&#29702;&#25512;&#29702;&#12289;&#27807;&#36890;&#25216;&#24039;&#21644;&#21516;&#29702;&#24515;&#31561;&#32500;&#24230;&#24615;&#33021;&#65292;&#19982;&#21021;&#32423;&#20445;&#20581;&#21307;&#29983;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2401.05654</link><description>&lt;p&gt;
&#36808;&#21521;&#23545;&#35805;&#24335;&#35786;&#26029;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Towards Conversational Diagnostic AI. (arXiv:2401.05654v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05654
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;AMIE&#65292;&#35813;&#31995;&#32479;&#21033;&#29992;&#33258;&#25105;&#23545;&#25112;&#30340;&#27169;&#25311;&#29615;&#22659;&#21644;&#33258;&#21160;&#21270;&#21453;&#39304;&#26426;&#21046;&#36827;&#34892;&#35786;&#26029;&#23545;&#35805;&#65292;&#24182;&#19988;&#36890;&#36807;&#35780;&#20272;&#30149;&#21490;&#37319;&#38598;&#12289;&#35786;&#26029;&#20934;&#30830;&#24615;&#12289;&#31649;&#29702;&#25512;&#29702;&#12289;&#27807;&#36890;&#25216;&#24039;&#21644;&#21516;&#29702;&#24515;&#31561;&#32500;&#24230;&#24615;&#33021;&#65292;&#19982;&#21021;&#32423;&#20445;&#20581;&#21307;&#29983;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#30340;&#26680;&#24515;&#22312;&#20110;&#21307;&#29983;&#21644;&#24739;&#32773;&#20043;&#38388;&#30340;&#23545;&#35805;&#65292;&#29087;&#32451;&#30340;&#30149;&#21490;&#37319;&#38598;&#20026;&#20934;&#30830;&#30340;&#35786;&#26029;&#12289;&#26377;&#25928;&#30340;&#27835;&#30103;&#21644;&#25345;&#20037;&#30340;&#20449;&#20219;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#33021;&#22815;&#36827;&#34892;&#35786;&#26029;&#23545;&#35805;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21487;&#20197;&#25552;&#39640;&#21307;&#30103;&#30340;&#21487;&#21450;&#24615;&#12289;&#19968;&#33268;&#24615;&#21644;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#25509;&#36817;&#20020;&#24202;&#19987;&#23478;&#30340;&#27700;&#24179;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;AMIE&#65288;Articulate Medical Intelligence Explorer&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20248;&#21270;&#20110;&#35786;&#26029;&#23545;&#35805;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;AMIE&#21033;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#33258;&#25105;&#23545;&#25112;&#30340;&#27169;&#25311;&#29615;&#22659;&#65292;&#24182;&#24102;&#26377;&#33258;&#21160;&#21270;&#30340;&#21453;&#39304;&#26426;&#21046;&#65292;&#20197;&#20415;&#22312;&#19981;&#21516;&#30340;&#30142;&#30149;&#29366;&#20917;&#12289;&#19987;&#19994;&#39046;&#22495;&#21644;&#24773;&#22659;&#19979;&#23454;&#29616;&#23398;&#20064;&#30340;&#25193;&#23637;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#35780;&#20272;&#20020;&#24202;&#26377;&#24847;&#20041;&#32500;&#24230;&#24615;&#33021;&#30340;&#26694;&#26550;&#65292;&#21253;&#25324;&#30149;&#21490;&#37319;&#38598;&#12289;&#35786;&#26029;&#20934;&#30830;&#24615;&#12289;&#31649;&#29702;&#25512;&#29702;&#12289;&#27807;&#36890;&#25216;&#24039;&#21644;&#21516;&#29702;&#24515;&#12290;&#25105;&#20204;&#23558;AMIE&#30340;&#24615;&#33021;&#19982;&#21021;&#32423;&#20445;&#20581;&#21307;&#29983;&#65288;PCPs&#65289;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#20351;&#29992;&#20102;&#38543;&#26426;&#12289;&#21452;&#30450;&#21313;&#23383;&#35774;&#35745;&#30340;&#35797;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
At the heart of medicine lies the physician-patient dialogue, where skillful history-taking paves the way for accurate diagnosis, effective management, and enduring trust. Artificial Intelligence (AI) systems capable of diagnostic dialogue could increase accessibility, consistency, and quality of care. However, approximating clinicians' expertise is an outstanding grand challenge. Here, we introduce AMIE (Articulate Medical Intelligence Explorer), a Large Language Model (LLM) based AI system optimized for diagnostic dialogue.  AMIE uses a novel self-play based simulated environment with automated feedback mechanisms for scaling learning across diverse disease conditions, specialties, and contexts. We designed a framework for evaluating clinically-meaningful axes of performance including history-taking, diagnostic accuracy, management reasoning, communication skills, and empathy. We compared AMIE's performance to that of primary care physicians (PCPs) in a randomized, double-blind cross
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;Shapley&#20540;&#22238;&#24402;&#23545;&#28192;&#36947;&#21512;&#20316;&#20249;&#20276;&#23618;&#38754;&#30340;&#33829;&#38144;&#32489;&#25928;&#36827;&#34892;&#37327;&#21270;&#65292;&#24182;&#36890;&#36807;&#19982;&#33829;&#38144;&#32452;&#21512;&#24314;&#27169;&#36827;&#34892;&#27604;&#36739;&#65292;&#35777;&#26126;&#20102;Shapley&#20540;&#22238;&#24402;&#30340;&#23454;&#29992;&#24615;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#35745;&#31639;&#35843;&#25972;&#31995;&#25968;&#12290;</title><link>http://arxiv.org/abs/2401.05653</link><description>&lt;p&gt;
&#20351;&#29992;&#33829;&#38144;&#32452;&#21512;&#24314;&#27169;&#65288;MMM&#65289;&#21644;Shapley&#20540;&#22238;&#24402;&#37327;&#21270;&#28192;&#36947;&#21512;&#20316;&#20249;&#20276;&#23618;&#38754;&#30340;&#33829;&#38144;&#32489;&#25928;
&lt;/p&gt;
&lt;p&gt;
Quantifying Marketing Performance at Channel-Partner Level by Using Marketing Mix Modeling (MMM) and Shapley Value Regression. (arXiv:2401.05653v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05653
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;Shapley&#20540;&#22238;&#24402;&#23545;&#28192;&#36947;&#21512;&#20316;&#20249;&#20276;&#23618;&#38754;&#30340;&#33829;&#38144;&#32489;&#25928;&#36827;&#34892;&#37327;&#21270;&#65292;&#24182;&#36890;&#36807;&#19982;&#33829;&#38144;&#32452;&#21512;&#24314;&#27169;&#36827;&#34892;&#27604;&#36739;&#65292;&#35777;&#26126;&#20102;Shapley&#20540;&#22238;&#24402;&#30340;&#23454;&#29992;&#24615;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#35745;&#31639;&#35843;&#25972;&#31995;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#22312;&#28192;&#36947;&#21512;&#20316;&#20249;&#20276;&#23618;&#38754;&#21033;&#29992;Shapley&#20540;&#22238;&#24402;&#26469;&#35299;&#26512;&#33829;&#38144;&#32489;&#25928;&#30340;&#24212;&#29992;&#65292;&#34917;&#20805;&#20102;&#28192;&#36947;&#23618;&#38754;&#30340;&#33829;&#38144;&#32452;&#21512;&#24314;&#27169;&#65288;MMM&#65289;&#12290;&#21033;&#29992;&#26469;&#33258;&#37329;&#34701;&#26381;&#21153;&#34892;&#19994;&#30340;&#30495;&#23454;&#25968;&#25454;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Shapley&#20540;&#22238;&#24402;&#22312;&#35780;&#20272;&#20010;&#21035;&#21512;&#20316;&#20249;&#20276;&#36129;&#29486;&#26041;&#38754;&#30340;&#23454;&#29992;&#24615;&#12290;&#23613;&#31649;&#32467;&#26500;&#21270;&#30340;&#29616;&#22330;&#27979;&#35797;&#20197;&#21450;&#21512;&#20316;&#21338;&#24328;&#29702;&#35770;&#26368;&#20026;&#20934;&#30830;&#65292;&#20294;&#32463;&#24120;&#20250;&#38750;&#24120;&#22797;&#26434;&#21644;&#26114;&#36149;&#12290;&#22240;&#27492;&#65292;Shapley&#20540;&#22238;&#24402;&#26159;&#19968;&#31181;&#26356;&#21487;&#34892;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20998;&#31163;&#33829;&#38144;&#28192;&#36947;&#20013;&#27599;&#20010;&#33829;&#38144;&#21512;&#20316;&#20249;&#20276;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#25512;&#23548;&#35843;&#25972;&#31995;&#25968;&#65292;&#23558;&#20854;&#19982;&#20854;&#20182;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the application of Shapley Value Regression in dissecting marketing performance at channel-partner level, complementing channel-level Marketing Mix Modeling (MMM). Utilizing real-world data from the financial services industry, we demonstrate the practicality of Shapley Value Regression in evaluating individual partner contributions. Although structured in-field testing along with cooperative game theory is most accurate, it can often be highly complex and expensive to conduct. Shapley Value Regression is thus a more feasible approach to disentangle the influence of each marketing partner within a marketing channel. We also propose a simple method to derive adjusted coefficients of Shapley Value Regression and compares it with alternative approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;O2C&#31995;&#32479;&#65292;&#36890;&#36807;&#22312;&#36816;&#34892;&#26102;&#23884;&#20837;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21040;eBPF&#31243;&#24207;&#20013;&#65292;&#23558;&#25805;&#20316;&#31995;&#32479;&#20869;&#26680;&#38548;&#31163;&#30340;&#33021;&#21147;&#19982;&#21363;&#26102;&#24212;&#23545;&#23041;&#32961;&#30340;&#21151;&#33021;&#30456;&#32467;&#21512;&#65292;&#22312;&#20445;&#38556;&#31995;&#32479;&#31283;&#23450;&#24615;&#30340;&#21516;&#26102;&#26377;&#25928;&#38480;&#21046;&#25439;&#23475;&#25193;&#25955;&#12290;</title><link>http://arxiv.org/abs/2401.05641</link><description>&lt;p&gt;
&#24403;eBPF&#36935;&#35265;&#26426;&#22120;&#23398;&#20064;&#65306;&#21363;&#26102;&#25805;&#20316;&#31995;&#32479;&#20869;&#26680;&#38548;&#31163;
&lt;/p&gt;
&lt;p&gt;
When eBPF Meets Machine Learning: On-the-fly OS Kernel Compartmentalization. (arXiv:2401.05641v1 [cs.OS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05641
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;O2C&#31995;&#32479;&#65292;&#36890;&#36807;&#22312;&#36816;&#34892;&#26102;&#23884;&#20837;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21040;eBPF&#31243;&#24207;&#20013;&#65292;&#23558;&#25805;&#20316;&#31995;&#32479;&#20869;&#26680;&#38548;&#31163;&#30340;&#33021;&#21147;&#19982;&#21363;&#26102;&#24212;&#23545;&#23041;&#32961;&#30340;&#21151;&#33021;&#30456;&#32467;&#21512;&#65292;&#22312;&#20445;&#38556;&#31995;&#32479;&#31283;&#23450;&#24615;&#30340;&#21516;&#26102;&#26377;&#25928;&#38480;&#21046;&#25439;&#23475;&#25193;&#25955;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38548;&#31163;&#26377;&#25928;&#22320;&#38450;&#27490;&#21021;&#22987;&#27745;&#26579;&#36716;&#21270;&#20026;&#25104;&#21151;&#30340;&#25915;&#20987;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;O2C&#65292;&#19968;&#20010;&#21019;&#26032;&#30340;&#31995;&#32479;&#65292;&#26088;&#22312;&#23454;&#26102;&#24378;&#21046;&#25191;&#34892;&#25805;&#20316;&#31995;&#32479;&#20869;&#26680;&#30340;&#38548;&#31163;&#12290;&#23427;&#19981;&#20165;&#33021;&#22815;&#36805;&#36895;&#24212;&#23545;&#31361;&#21457;&#23041;&#32961;&#65292;&#36824;&#36890;&#36807;&#25191;&#34892;&#36807;&#31243;&#32500;&#25345;&#31995;&#32479;&#30340;&#31283;&#23450;&#24615;&#12290;O2C&#21033;&#29992;&#20102;eBPF&#29983;&#24577;&#31995;&#32479;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21487;&#20197;&#22312;&#36816;&#34892;&#26102;&#23558;&#25191;&#34892;&#25805;&#20316;&#30340;eBPF&#31243;&#24207;&#23884;&#20837;&#20869;&#26680;&#12290;O2C&#22312;eBPF&#31243;&#24207;&#20013;&#23884;&#20837;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#21363;&#26102;&#38548;&#31163;&#20013;&#30340;&#29420;&#29305;&#25361;&#25112;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#35777;&#26126;O2C&#33021;&#26377;&#25928;&#38480;&#21046;&#25439;&#23475;&#22312;&#38548;&#31163;&#21306;&#20869;&#30340;&#25193;&#25955;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#20915;&#31574;&#26641;&#22312;O2C&#20013;&#20855;&#26377;&#20248;&#21183;&#65292;&#22240;&#20026;&#23427;&#33021;&#22815;&#22788;&#29702;&#34920;&#26684;&#25968;&#25454;&#12289;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#19982;eBPF&#29983;&#24577;&#31995;&#32479;&#30456;&#20860;&#23481;&#12290;&#26368;&#21518;&#65292;O2C&#20855;&#26377;&#36731;&#37327;&#32423;&#30340;&#29305;&#28857;&#65292;&#20960;&#20046;&#19981;&#21487;&#24863;&#30693;&#31995;&#32479;&#30340;&#36127;&#25285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Compartmentalization effectively prevents initial corruption from turning into a successful attack. This paper presents O2C, a pioneering system designed to enforce OS kernel compartmentalization on the fly. It not only provides immediate remediation for sudden threats but also maintains consistent system availability through the enforcement process.  O2C is empowered by the newest advancements of the eBPF ecosystem which allows to instrument eBPF programs that perform enforcement actions into the kernel at runtime. O2C takes the lead in embedding a machine learning model into eBPF programs, addressing unique challenges in on-the-fly compartmentalization. Our comprehensive evaluation shows that O2C effectively confines damage within the compartment. Further, we validate that decision tree is optimally suited for O2C owing to its advantages in processing tabular data, its explainable nature, and its compliance with the eBPF ecosystem. Last but not least, O2C is lightweight, showing negl
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#26500;&#24314;&#21487;&#23548;&#20989;&#25968;&#26469;&#36817;&#20284;&#23433;&#20840;&#38598;&#21512;&#65292;&#24182;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#25511;&#21046;&#38556;&#30861;&#20989;&#25968;&#65292;&#20197;&#35299;&#20915;&#22312;&#22797;&#26434;&#23433;&#20840;&#32422;&#26463;&#21644;&#26377;&#38480;&#25191;&#34892;&#33021;&#21147;&#19979;&#23547;&#25214;&#26368;&#20248;CBF&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.05629</link><description>&lt;p&gt;
&#22312;&#22797;&#26434;&#23433;&#20840;&#32422;&#26463;&#21644;&#26377;&#38480;&#25191;&#34892;&#33021;&#21147;&#19979;&#23398;&#20064;&#20197;&#24615;&#33021;&#20026;&#23548;&#21521;&#30340;&#25511;&#21046;&#38556;&#30861;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Learning Performance-Oriented Control Barrier Functions Under Complex Safety Constraints and Limited Actuation. (arXiv:2401.05629v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05629
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#26500;&#24314;&#21487;&#23548;&#20989;&#25968;&#26469;&#36817;&#20284;&#23433;&#20840;&#38598;&#21512;&#65292;&#24182;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#25511;&#21046;&#38556;&#30861;&#20989;&#25968;&#65292;&#20197;&#35299;&#20915;&#22312;&#22797;&#26434;&#23433;&#20840;&#32422;&#26463;&#21644;&#26377;&#38480;&#25191;&#34892;&#33021;&#21147;&#19979;&#23547;&#25214;&#26368;&#20248;CBF&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25511;&#21046;&#38556;&#30861;&#20989;&#25968;&#65288;CBFs&#65289;&#25552;&#20379;&#20102;&#19968;&#20010;&#20248;&#38597;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#38750;&#32447;&#24615;&#25511;&#21046;&#31995;&#32479;&#30340;&#36712;&#36857;&#32422;&#26463;&#22312;&#39044;&#23450;&#20041;&#23433;&#20840;&#38598;&#21512;&#30340;&#19981;&#21464;&#23376;&#38598;&#19978;&#65292;&#35774;&#35745;&#23433;&#20840;&#36807;&#28388;&#22120;&#12290;&#28982;&#32780;&#65292;&#25214;&#21040;&#19968;&#20010;&#21516;&#26102;&#22312;&#26368;&#22823;&#21270;&#25511;&#21046;&#19981;&#21464;&#38598;&#20307;&#31215;&#21644;&#36866;&#24212;&#22797;&#26434;&#23433;&#20840;&#32422;&#26463;&#26041;&#38754;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;CBF&#65292;&#23588;&#20854;&#26159;&#22312;&#20855;&#26377;&#25191;&#34892;&#32422;&#26463;&#30340;&#39640;&#30456;&#23545;&#24230;&#30340;&#31995;&#32479;&#20013;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#20840;&#38754;&#35299;&#20915;&#20102;&#36825;&#20123;&#38556;&#30861;&#12290;&#32473;&#23450;&#23450;&#20041;&#23433;&#20840;&#38598;&#21512;&#30340;&#22810;&#20010;&#29366;&#24577;&#32422;&#26463;&#30340;&#24067;&#23572;&#32452;&#21512;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20174;&#26500;&#24314;&#19968;&#20010;&#21333;&#19968;&#30340;&#21487;&#23548;&#20989;&#25968;&#24320;&#22987;&#65292;&#20854;0&#36229;&#32423;&#32423;&#21035;&#38598;&#21512;&#25552;&#20379;&#20102;&#23433;&#20840;&#38598;&#21512;&#30340;&#20869;&#37096;&#36817;&#20284;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#20010;&#20989;&#25968;&#20197;&#21450;&#19968;&#20010;&#24179;&#28369;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#21442;&#25968;&#21270;CBF&#20505;&#36873;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#22522;&#20110;&#21704;&#23494;&#39039;-&#38597;&#21487;&#27604;&#30340;&#35757;&#32451;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Control Barrier Functions (CBFs) provide an elegant framework for designing safety filters for nonlinear control systems by constraining their trajectories to an invariant subset of a prespecified safe set. However, the task of finding a CBF that concurrently maximizes the volume of the resulting control invariant set while accommodating complex safety constraints, particularly in high relative degree systems with actuation constraints, continues to pose a substantial challenge. In this work, we propose a novel self-supervised learning framework that holistically addresses these hurdles. Given a Boolean composition of multiple state constraints that define the safe set, our approach starts with building a single continuously differentiable function whose 0-superlevel set provides an inner approximation of the safe set. We then use this function together with a smooth neural network to parameterize the CBF candidate. Finally, we design a training loss function based on a Hamilton-Jacobi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#24182;&#35777;&#26126;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#24212;&#29992;&#20110;&#35299;&#20915;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#20248;&#21270;&#36807;&#31243;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;Q-Learning&#35757;&#32451;&#30340;GNNs&#26469;&#23398;&#20064;&#31574;&#30053;&#65292;&#21487;&#20197;&#36798;&#21040;&#25509;&#36817;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#21551;&#21457;&#24335;&#27714;&#35299;&#22120;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#21482;&#38656;&#20351;&#29992;&#37096;&#20998;&#21442;&#25968;&#21644;&#35757;&#32451;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2401.05610</link><description>&lt;p&gt;
&#22270;&#24418;Q-Learning&#29992;&#20110;&#32452;&#21512;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Graph Q-Learning for Combinatorial Optimization. (arXiv:2401.05610v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05610
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#24182;&#35777;&#26126;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#24212;&#29992;&#20110;&#35299;&#20915;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#20248;&#21270;&#36807;&#31243;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;Q-Learning&#35757;&#32451;&#30340;GNNs&#26469;&#23398;&#20064;&#31574;&#30053;&#65292;&#21487;&#20197;&#36798;&#21040;&#25509;&#36817;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#21551;&#21457;&#24335;&#27714;&#35299;&#22120;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#21482;&#38656;&#20351;&#29992;&#37096;&#20998;&#21442;&#25968;&#21644;&#35757;&#32451;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#24418;&#32467;&#26500;&#21270;&#25968;&#25454;&#22312;&#33258;&#28982;&#31185;&#23398;&#21644;&#31038;&#20250;&#31185;&#23398;&#20013;&#26080;&#22788;&#19981;&#22312;&#65292;&#32780;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26368;&#36817;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#22270;&#24418;&#25968;&#25454;&#19978;&#30340;&#39044;&#27979;&#21644;&#25512;&#29702;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#35777;&#26126;&#20102;GNNs&#21487;&#20197;&#24212;&#29992;&#20110;&#35299;&#20915;&#32452;&#21512;&#20248;&#21270;&#65288;CO&#65289;&#38382;&#39064;&#12290;CO&#28041;&#21450;&#22312;&#36890;&#24120;&#38750;&#24120;&#24222;&#22823;&#30340;&#31163;&#25955;&#35299;&#31354;&#38388;&#19978;&#20248;&#21270;&#20989;&#25968;&#12290;&#20026;&#20102;&#23398;&#20064;&#35299;&#20915;CO&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#20248;&#21270;&#36807;&#31243;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#65292;&#20854;&#20013;&#22238;&#25253;&#19982;&#20505;&#36873;&#35299;&#19982;&#26368;&#20248;&#35299;&#30340;&#25509;&#36817;&#31243;&#24230;&#26377;&#20851;&#12290;&#25105;&#20204;&#20351;&#29992;GNN&#26469;&#23398;&#20064;&#31574;&#30053;&#65292;&#20197;&#36845;&#20195;&#22320;&#26500;&#24314;&#36234;&#26469;&#36234;&#26377;&#21069;&#26223;&#30340;&#20505;&#36873;&#35299;&#12290;&#25105;&#20204;&#25552;&#20986;&#21021;&#27493;&#35777;&#25454;&#34920;&#26126;&#65292;&#20351;&#29992;Q-Learning&#35757;&#32451;&#30340;GNNs&#21487;&#20197;&#35299;&#20915;CO&#38382;&#39064;&#65292;&#20854;&#24615;&#33021;&#25509;&#36817;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#21551;&#21457;&#24335;&#27714;&#35299;&#22120;&#30340;&#24615;&#33021;&#65292;&#20165;&#20351;&#29992;&#20102;&#37096;&#20998;&#21442;&#25968;&#21644;&#35757;&#32451;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph-structured data is ubiquitous throughout natural and social sciences, and Graph Neural Networks (GNNs) have recently been shown to be effective at solving prediction and inference problems on graph data. In this paper, we propose and demonstrate that GNNs can be applied to solve Combinatorial Optimization (CO) problems. CO concerns optimizing a function over a discrete solution space that is often intractably large. To learn to solve CO problems, we formulate the optimization process as a sequential decision making problem, where the return is related to how close the candidate solution is to optimality. We use a GNN to learn a policy to iteratively build increasingly promising candidate solutions. We present preliminary evidence that GNNs trained through Q-Learning can solve CO problems with performance approaching state-of-the-art heuristic-based solvers, using only a fraction of the parameters and training time.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#30340;&#36951;&#24536;&#38382;&#39064;&#65292;&#24182;&#24471;&#20986;&#20102;&#24494;&#35843;&#24615;&#33021;&#19982;&#36951;&#24536;&#31243;&#24230;&#20043;&#38388;&#23384;&#22312;&#21453;&#27604;&#32447;&#24615;&#20851;&#31995;&#30340;&#32467;&#35770;&#65292;&#25552;&#20986;&#20102;&#36951;&#24536;&#31243;&#24230;&#38543;&#24494;&#35843;&#21442;&#25968;&#25968;&#37327;&#21644;&#26356;&#26032;&#27493;&#39588;&#25968;&#37327;&#21576;&#24130;&#24459;&#22686;&#38271;&#30340;&#32553;&#25918;&#35268;&#24459;&#12290;&#30740;&#31350;&#32467;&#26524;&#36824;&#26174;&#31034;&#65292;&#25552;&#21069;&#20572;&#27490;&#24494;&#35843;&#25110;&#25913;&#21464;&#24494;&#35843;&#21442;&#25968;&#25968;&#37327;&#37117;&#26080;&#27861;&#36991;&#20813;&#36951;&#24536;&#65292;&#36825;&#20026;&#26410;&#26469;&#20943;&#36731;&#36951;&#24536;&#30340;&#24494;&#35843;&#26041;&#26696;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#23433;&#20840;&#20851;&#38190;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2401.05605</link><description>&lt;p&gt;
&#32553;&#25918;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#26102;&#30340;&#36951;&#24536;&#35268;&#24459;
&lt;/p&gt;
&lt;p&gt;
Scaling Laws for Forgetting When Fine-Tuning Large Language Models. (arXiv:2401.05605v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05605
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#30340;&#36951;&#24536;&#38382;&#39064;&#65292;&#24182;&#24471;&#20986;&#20102;&#24494;&#35843;&#24615;&#33021;&#19982;&#36951;&#24536;&#31243;&#24230;&#20043;&#38388;&#23384;&#22312;&#21453;&#27604;&#32447;&#24615;&#20851;&#31995;&#30340;&#32467;&#35770;&#65292;&#25552;&#20986;&#20102;&#36951;&#24536;&#31243;&#24230;&#38543;&#24494;&#35843;&#21442;&#25968;&#25968;&#37327;&#21644;&#26356;&#26032;&#27493;&#39588;&#25968;&#37327;&#21576;&#24130;&#24459;&#22686;&#38271;&#30340;&#32553;&#25918;&#35268;&#24459;&#12290;&#30740;&#31350;&#32467;&#26524;&#36824;&#26174;&#31034;&#65292;&#25552;&#21069;&#20572;&#27490;&#24494;&#35843;&#25110;&#25913;&#21464;&#24494;&#35843;&#21442;&#25968;&#25968;&#37327;&#37117;&#26080;&#27861;&#36991;&#20813;&#36951;&#24536;&#65292;&#36825;&#20026;&#26410;&#26469;&#20943;&#36731;&#36951;&#24536;&#30340;&#24494;&#35843;&#26041;&#26696;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#23433;&#20840;&#20851;&#38190;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#24182;&#37327;&#21270;&#20102;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#23545;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#24494;&#35843;&#26102;&#36951;&#24536;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#31574;&#30053;&#65288;&#22914;Low-Rank Adapters&#65289;&#20173;&#28982;&#23384;&#22312;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#38382;&#39064;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#22312;&#20351;&#29992;Low-Rank Adapters&#36827;&#34892;LLMs&#24494;&#35843;&#26102;&#65292;&#24494;&#35843;&#24615;&#33021;&#19982;&#36951;&#24536;&#31243;&#24230;&#20043;&#38388;&#23384;&#22312;&#24378;&#28872;&#30340;&#21453;&#27604;&#32447;&#24615;&#20851;&#31995;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24471;&#21040;&#20102;&#31934;&#30830;&#30340;&#32553;&#25918;&#35268;&#24459;&#65292;&#34920;&#26126;&#36951;&#24536;&#31243;&#24230;&#38543;&#30528;&#24494;&#35843;&#21442;&#25968;&#30340;&#25968;&#37327;&#21644;&#26356;&#26032;&#27493;&#39588;&#30340;&#25968;&#37327;&#21576;&#29616;&#20986;&#19968;&#31181;&#24179;&#31227;&#30340;&#24130;&#24459;&#22686;&#38271;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#36951;&#24536;&#23545;Llama 2 7B&#32842;&#22825;&#27169;&#22411;&#20013;&#30340;&#30693;&#35782;&#12289;&#25512;&#29702;&#21644;&#23433;&#20840;&#38450;&#25252;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#26080;&#27861;&#36890;&#36807;&#25552;&#21069;&#20572;&#27490;&#24494;&#35843;&#25110;&#25913;&#21464;&#24494;&#35843;&#21442;&#25968;&#30340;&#25968;&#37327;&#26469;&#36991;&#20813;&#36951;&#24536;&#12290;&#25105;&#20204;&#30456;&#20449;&#36825;&#20026;&#26410;&#26469;&#35780;&#20272;&#21644;&#24320;&#21457;&#33021;&#22815;&#20943;&#36731;&#36951;&#24536;&#30340;&#24494;&#35843;&#26041;&#26696;&#24320;&#36767;&#20102;&#37325;&#35201;&#30340;&#23433;&#20840;&#20851;&#38190;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study and quantify the problem of forgetting when fine-tuning pre-trained large language models (LLMs) on a downstream task. We find that parameter-efficient fine-tuning (PEFT) strategies, such as Low-Rank Adapters (LoRA), still suffer from catastrophic forgetting. In particular, we identify a strong inverse linear relationship between the fine-tuning performance and the amount of forgetting when fine-tuning LLMs with LoRA. We further obtain precise scaling laws that show forgetting increases as a shifted power law in the number of parameters fine-tuned and the number of update steps. We also examine the impact of forgetting on knowledge, reasoning, and the safety guardrails trained into Llama 2 7B chat. Our study suggests that forgetting cannot be avoided through early stopping or by varying the number of parameters fine-tuned. We believe this opens up an important safety-critical direction for future research to evaluate and develop fine-tuning schemes which mitigate forgetting
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#24378;&#25193;&#25955;&#30456;&#20851;&#20809;&#35889;&#23398;&#20013;&#30340;&#34880;&#27969;&#35780;&#20272;&#65292;&#24182;&#36890;&#36807;&#22122;&#22768;&#40065;&#26834;&#24615;&#20998;&#26512;&#23637;&#31034;&#20102;&#20854;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.05580</link><description>&lt;p&gt;
&#21152;&#24378;&#25193;&#25955;&#30456;&#20851;&#20809;&#35889;&#23398;&#20013;&#30340;&#34880;&#27969;&#35780;&#20272;&#65306;&#19968;&#31181;&#24102;&#26377;&#22122;&#22768;&#40065;&#26834;&#24615;&#20998;&#26512;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Enhancing Blood Flow Assessment in Diffuse Correlation Spectroscopy: A Transfer Learning Approach with Noise Robustness Analysis. (arXiv:2401.05580v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05580
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#24378;&#25193;&#25955;&#30456;&#20851;&#20809;&#35889;&#23398;&#20013;&#30340;&#34880;&#27969;&#35780;&#20272;&#65292;&#24182;&#36890;&#36807;&#22122;&#22768;&#40065;&#26834;&#24615;&#20998;&#26512;&#23637;&#31034;&#20102;&#20854;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#30456;&#20851;&#20809;&#35889;&#23398;&#65288;DCS&#65289;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#38750;&#20405;&#20837;&#24615;&#25216;&#26415;&#65292;&#36890;&#36807;&#20351;&#29992;&#36817;&#32418;&#22806;&#30456;&#24178;&#28857;&#20809;&#28304;&#29031;&#23556;&#26469;&#26816;&#27979;&#20809;&#35889;&#21464;&#21270;&#26469;&#27979;&#37327;&#32452;&#32455;&#34880;&#27969;&#12290;&#23613;&#31649;&#26426;&#22120;&#23398;&#20064;&#24050;&#32463;&#26174;&#31034;&#20986;&#27979;&#37327;&#34880;&#27969;&#25351;&#25968;&#65288;BFi&#65289;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#19968;&#20010;&#26377;&#20851;&#35813;&#26041;&#27861;&#25104;&#21151;&#19982;&#21542;&#30340;&#38382;&#39064;&#26159;&#20854;&#22312;&#28041;&#21450;&#19981;&#21516;&#20449;&#22122;&#27604;&#65288;SNR&#65289;&#21644;&#21508;&#31181;&#19981;&#21516;&#20020;&#24202;&#24212;&#29992;&#21644;&#35774;&#32622;&#30340;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#20559;&#24046;&#26041;&#38754;&#30340;&#40065;&#26834;&#24615;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#35780;&#20272;SNR&#23545;&#23398;&#20064;&#29305;&#24449;&#30340;&#27867;&#21270;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#24182;&#23637;&#31034;&#36801;&#31227;&#23398;&#20064;&#30340;&#40065;&#26834;&#24615;&#12290;&#20351;&#29992;&#19981;&#21516;&#28155;&#21152;&#22122;&#22768;&#27700;&#24179;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#26469;&#27169;&#25311;&#19981;&#21516;&#30340;SNR&#12290;&#25152;&#25552;&#20986;&#30340;&#32593;&#32476;&#20197;1x64&#30340;&#33258;&#30456;&#20851;&#26354;&#32447;&#20026;&#36755;&#20837;&#65292;&#24182;&#29983;&#25104;BFi&#21644;&#30456;&#20851;&#21442;&#25968;beta&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#34920;&#29616;&#20986;&#26497;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffuse correlation spectroscopy (DCS) is an emerging noninvasive technique that measures the tissue blood flow, by using near-infrared coherent point-source illumination to detect spectral changes. While machine learning has demonstrated significant potential for measuring blood flow index (BFi), an open question concerning the success of this approach pertains to its robustness in scenarios involving deviations between datasets with varying Signal-to-Noise Ratios (SNRs) originating from diverse clinical applications and various setups. This study proposes a transfer learning approach, aims to assess the influence of SNRs on the generalization ability of learned features, and demonstrate the robustness for transfer learning. A synthetic dataset with varying levels of added noise is utilized to simulate different SNRs. The proposed network takes a 1x64 autocorrelation curve as input and generates BFi and the correlation parameter beta. The proposed model demonstrates excellent performa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22686;&#24378;&#24778;&#21916;&#24341;&#23548;&#30340;&#39034;&#24207;&#23398;&#20064;&#26694;&#26550;SurpriseAF-BO&#65292;&#29992;&#20110;&#39044;&#27979;&#29076;&#27744;&#20960;&#20309;&#24418;&#29366;&#12290;&#36825;&#31181;&#26694;&#26550;&#36890;&#36807;&#36845;&#20195;&#33258;&#36866;&#24212;&#23398;&#20064;&#36807;&#31243;&#65292;&#27169;&#25311;&#20102;&#24037;&#33402;&#21442;&#25968;&#19982;&#29076;&#27744;&#29305;&#24615;&#20043;&#38388;&#30340;&#21160;&#21147;&#23398;&#20851;&#31995;&#65292;&#24182;&#22312;&#26377;&#38480;&#25968;&#25454;&#38598;&#26465;&#20214;&#19979;&#36827;&#34892;&#20102;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2401.05579</link><description>&lt;p&gt;
&#38024;&#23545;&#39044;&#27979;&#29076;&#27744;&#20960;&#20309;&#24418;&#29366;&#30340;&#22686;&#24378;&#24778;&#21916;&#24341;&#23548;&#30340;&#39034;&#24207;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
An Augmented Surprise-guided Sequential Learning Framework for Predicting the Melt Pool Geometry. (arXiv:2401.05579v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05579
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22686;&#24378;&#24778;&#21916;&#24341;&#23548;&#30340;&#39034;&#24207;&#23398;&#20064;&#26694;&#26550;SurpriseAF-BO&#65292;&#29992;&#20110;&#39044;&#27979;&#29076;&#27744;&#20960;&#20309;&#24418;&#29366;&#12290;&#36825;&#31181;&#26694;&#26550;&#36890;&#36807;&#36845;&#20195;&#33258;&#36866;&#24212;&#23398;&#20064;&#36807;&#31243;&#65292;&#27169;&#25311;&#20102;&#24037;&#33402;&#21442;&#25968;&#19982;&#29076;&#27744;&#29305;&#24615;&#20043;&#38388;&#30340;&#21160;&#21147;&#23398;&#20851;&#31995;&#65292;&#24182;&#22312;&#26377;&#38480;&#25968;&#25454;&#38598;&#26465;&#20214;&#19979;&#36827;&#34892;&#20102;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37329;&#23646;&#22686;&#26448;&#21046;&#36896;&#65288;MAM&#65289;&#24050;&#32463;&#37325;&#22609;&#20102;&#21046;&#36896;&#19994;&#65292;&#25552;&#20379;&#20102;&#22797;&#26434;&#35774;&#35745;&#12289;&#26368;&#23567;&#28010;&#36153;&#12289;&#24555;&#36895;&#21407;&#22411;&#12289;&#26448;&#26009;&#22810;&#26679;&#24615;&#21644;&#23450;&#21046;&#35299;&#20915;&#26041;&#26696;&#31561;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#20854;&#22312;&#24037;&#19994;&#20013;&#30340;&#20840;&#38754;&#24212;&#29992;&#38754;&#20020;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#30830;&#20445;&#20135;&#21697;&#36136;&#37327;&#30340;&#19968;&#33268;&#24615;&#26041;&#38754;&#12290;MAM&#25104;&#21151;&#30340;&#20851;&#38190;&#22240;&#32032;&#26159;&#29702;&#35299;&#24037;&#33402;&#21442;&#25968;&#19982;&#29076;&#27744;&#29305;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#23558;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#34701;&#20837;MAM&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#12290;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#34429;&#28982;&#26377;&#25928;&#65292;&#21364;&#20381;&#36182;&#20110;&#22823;&#25968;&#25454;&#38598;&#26469;&#25429;&#25417;&#22797;&#26434;&#20851;&#31995;&#65292;&#32780;&#22312;MAM&#20013;&#65292;&#30001;&#20110;&#38656;&#35201;&#22823;&#37327;&#30340;&#26102;&#38388;&#21644;&#36164;&#28304;&#26469;&#21019;&#24314;&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22686;&#24378;&#24778;&#21916;&#24341;&#23548;&#30340;&#39034;&#24207;&#23398;&#20064;&#26694;&#26550;SurpriseAF-BO&#65292;&#26631;&#24535;&#30528;MAM&#30340;&#37325;&#22823;&#36716;&#21464;&#12290;&#35813;&#26694;&#26550;&#37319;&#29992;&#36845;&#20195;&#33258;&#36866;&#24212;&#23398;&#20064;&#36807;&#31243;&#65292;&#23545;&#24037;&#33402;&#21442;&#25968;&#19982;&#29076;&#27744;&#29305;&#24615;&#20043;&#38388;&#30340;&#21160;&#21147;&#23398;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#22312;&#26377;&#38480;&#25968;&#25454;&#38598;&#26465;&#20214;&#19979;&#36827;&#34892;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Metal Additive Manufacturing (MAM) has reshaped the manufacturing industry, offering benefits like intricate design, minimal waste, rapid prototyping, material versatility, and customized solutions. However, its full industry adoption faces hurdles, particularly in achieving consistent product quality. A crucial aspect for MAM's success is understanding the relationship between process parameters and melt pool characteristics. Integrating Artificial Intelligence (AI) into MAM is essential. Traditional machine learning (ML) methods, while effective, depend on large datasets to capture complex relationships, a significant challenge in MAM due to the extensive time and resources required for dataset creation. Our study introduces a novel surprise-guided sequential learning framework, SurpriseAF-BO, signaling a significant shift in MAM. This framework uses an iterative, adaptive learning process, modeling the dynamics between process parameters and melt pool characteristics with limited da
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26497;&#38480;&#23398;&#20064;&#26426;&#24555;&#36895;&#31934;&#30830;&#20998;&#26512;&#33041;&#34880;&#27969;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#19982;&#29616;&#26377;&#31639;&#27861;&#30340;&#32508;&#21512;&#27604;&#36739;&#39564;&#35777;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;&#23427;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#22312;&#21508;&#31181;&#22122;&#22768;&#21644;&#20809;&#23398;&#21442;&#25968;&#19979;&#37117;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#21516;&#26102;&#65292;&#19982;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#31070;&#32463;&#32593;&#32476;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#36739;&#30701;&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#12290;&#36825;&#31181;&#31574;&#30053;&#36866;&#29992;&#20110;&#22312;&#32447;&#35757;&#32451;&#30340;&#36793;&#32536;&#35745;&#31639;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.05578</link><description>&lt;p&gt;
&#36890;&#36807;&#26497;&#38480;&#23398;&#20064;&#26426;&#24555;&#36895;&#20998;&#26512;&#33041;&#34880;&#27969;
&lt;/p&gt;
&lt;p&gt;
Fast Cerebral Blood Flow Analysis via Extreme Learning Machine. (arXiv:2401.05578v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05578
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26497;&#38480;&#23398;&#20064;&#26426;&#24555;&#36895;&#31934;&#30830;&#20998;&#26512;&#33041;&#34880;&#27969;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#19982;&#29616;&#26377;&#31639;&#27861;&#30340;&#32508;&#21512;&#27604;&#36739;&#39564;&#35777;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;&#23427;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#22312;&#21508;&#31181;&#22122;&#22768;&#21644;&#20809;&#23398;&#21442;&#25968;&#19979;&#37117;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#21516;&#26102;&#65292;&#19982;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#31070;&#32463;&#32593;&#32476;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#36739;&#30701;&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#12290;&#36825;&#31181;&#31574;&#30053;&#36866;&#29992;&#20110;&#22312;&#32447;&#35757;&#32451;&#30340;&#36793;&#32536;&#35745;&#31639;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#24555;&#36895;&#31934;&#30830;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#21033;&#29992;&#25193;&#25955;&#30456;&#20851;&#20809;&#35889;&#23398;&#65288;DCS&#65289;&#21644;&#26497;&#38480;&#23398;&#20064;&#26426;&#65288;ELM&#65289;&#26469;&#20998;&#26512;&#33041;&#34880;&#27969;&#65288;CBF&#65289;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;ELM&#21644;&#29616;&#26377;&#31639;&#27861;&#65292;&#24182;&#20351;&#29992;&#32508;&#21512;&#25351;&#26631;&#23545;&#36825;&#20123;&#31639;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#38598;&#23545;&#21322;&#26080;&#31351;&#21644;&#22810;&#23618;&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#22122;&#22768;&#27700;&#24179;&#21644;&#20809;&#23398;&#21442;&#25968;&#19979;&#65292;ELM&#22987;&#32456;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#20248;&#20110;&#36845;&#20195;&#25311;&#21512;&#31639;&#27861;&#12290;&#36890;&#36807;&#19982;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#27604;&#36739;&#65292;ELM&#33719;&#24471;&#20102;&#21487;&#27604;&#36739;&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;ELM&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#27809;&#26377;&#21453;&#21521;&#20256;&#25773;&#36807;&#31243;&#65292;&#23548;&#33268;&#35757;&#32451;&#36895;&#24230;&#27604;&#29616;&#26377;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#26356;&#24555;&#12290;&#36825;&#31181;&#25552;&#20986;&#30340;&#31574;&#30053;&#22312;&#22312;&#32447;&#35757;&#32451;&#30340;&#36793;&#32536;&#35745;&#31639;&#24212;&#29992;&#20013;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a rapid and precise analytical approach for analyzing cerebral blood flow (CBF) using Diffuse Correlation Spectroscopy (DCS) with the application of the Extreme Learning Machine (ELM). Our evaluation of ELM and existing algorithms involves a comprehensive set of metrics. We assess these algorithms using synthetic datasets for both semi-infinite and multi-layer models. The results demonstrate that ELM consistently achieves higher fidelity across various noise levels and optical parameters, showcasing robust generalization ability and outperforming iterative fitting algorithms. Through a comparison with a computationally efficient neural network, ELM attains comparable accuracy with reduced training and inference times. Notably, the absence of a back-propagation process in ELM during training results in significantly faster training speeds compared to existing neural network approaches. This proposed strategy holds promise for edge computing applications with online training
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20808;&#22825;&#20215;&#20540;&#39537;&#21160;&#22686;&#24378;&#23398;&#20064;&#65288;IVRL&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#25551;&#36848;&#22810;&#26234;&#33021;&#20307;&#22312;&#21512;&#20316;&#20013;&#30340;&#22797;&#26434;&#34892;&#20026;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#24314;&#31435;&#26234;&#33021;&#20307;&#23545;&#32676;&#20307;&#25928;&#29992;&#21644;&#31995;&#32479;&#25104;&#26412;&#30340;&#35748;&#30693;&#65292;&#28385;&#36275;&#20854;&#21512;&#20316;&#20249;&#20276;&#30340;&#38656;&#27714;&#65292;&#25903;&#25345;&#20854;&#31038;&#21306;&#24182;&#34701;&#20837;&#20154;&#31867;&#31038;&#20250;&#12290;</title><link>http://arxiv.org/abs/2401.05572</link><description>&lt;p&gt;
&#29992;&#20110;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#20808;&#22825;&#20215;&#20540;&#39537;&#21160;&#22686;&#24378;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Innate-Values-driven Reinforcement Learning for Cooperative Multi-Agent Systems. (arXiv:2401.05572v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05572
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20808;&#22825;&#20215;&#20540;&#39537;&#21160;&#22686;&#24378;&#23398;&#20064;&#65288;IVRL&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#25551;&#36848;&#22810;&#26234;&#33021;&#20307;&#22312;&#21512;&#20316;&#20013;&#30340;&#22797;&#26434;&#34892;&#20026;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#24314;&#31435;&#26234;&#33021;&#20307;&#23545;&#32676;&#20307;&#25928;&#29992;&#21644;&#31995;&#32479;&#25104;&#26412;&#30340;&#35748;&#30693;&#65292;&#28385;&#36275;&#20854;&#21512;&#20316;&#20249;&#20276;&#30340;&#38656;&#27714;&#65292;&#25903;&#25345;&#20854;&#31038;&#21306;&#24182;&#34701;&#20837;&#20154;&#31867;&#31038;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#22825;&#20215;&#20540;&#25551;&#36848;&#20102;&#26234;&#33021;&#20307;&#30340;&#20869;&#22312;&#21160;&#26426;&#65292;&#21453;&#26144;&#20102;&#20182;&#20204;&#36861;&#27714;&#30446;&#26631;&#21644;&#21457;&#23637;&#22810;&#26679;&#25216;&#33021;&#20197;&#28385;&#36275;&#21508;&#31181;&#38656;&#27714;&#30340;&#22266;&#26377;&#20852;&#36259;&#21644;&#20559;&#22909;&#12290;&#24378;&#21270;&#23398;&#20064;&#30340;&#26412;&#36136;&#26159;&#22522;&#20110;&#22870;&#21169;&#39537;&#21160;&#65288;&#22914;&#25928;&#29992;&#65289;&#30340;&#34892;&#20026;&#20114;&#21160;&#23398;&#20064;&#65292;&#31867;&#20284;&#20110;&#33258;&#28982;&#26234;&#33021;&#20307;&#12290;&#29305;&#21035;&#26159;&#22312;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#65292;&#24314;&#31435;&#26234;&#33021;&#20307;&#23545;&#24179;&#34913;&#32676;&#20307;&#25928;&#29992;&#21644;&#31995;&#32479;&#25104;&#26412;&#30340;&#35748;&#30693;&#65292;&#28385;&#36275;&#32676;&#20307;&#25104;&#21592;&#22312;&#21512;&#20316;&#20013;&#30340;&#38656;&#27714;&#65292;&#26159;&#20010;&#20307;&#20026;&#25903;&#25345;&#20854;&#31038;&#21306;&#21644;&#34701;&#20837;&#20154;&#31867;&#31038;&#20250;&#32780;&#23398;&#20064;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#22797;&#21512;&#20869;&#22312;&#20215;&#20540;&#22686;&#24378;&#23398;&#20064;&#27169;&#22411; - &#20808;&#22825;&#20215;&#20540;&#39537;&#21160;&#22686;&#24378;&#23398;&#20064;&#65292;&#29992;&#20110;&#25551;&#36848;&#22810;&#26234;&#33021;&#20307;&#21512;&#20316;&#20013;&#22797;&#26434;&#30340;&#20114;&#21160;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Innate values describe agents' intrinsic motivations, which reflect their inherent interests and preferences to pursue goals and drive them to develop diverse skills satisfying their various needs. The essence of reinforcement learning (RL) is learning from interaction based on reward-driven (such as utilities) behaviors, much like natural agents. It is an excellent model to describe the innate-values-driven (IV) behaviors of AI agents. Especially in multi-agent systems (MAS), building the awareness of AI agents to balance the group utilities and system costs and satisfy group members' needs in their cooperation is a crucial problem for individuals learning to support their community and integrate human society in the long term. This paper proposes a hierarchical compound intrinsic value reinforcement learning model -innate-values-driven reinforcement learning termed IVRL to describe the complex behaviors of multi-agent interaction in their cooperation. We implement the IVRL architec
&lt;/p&gt;</description></item><item><title>QuantumSEA&#26159;&#19968;&#31181;&#22522;&#20110;&#22122;&#22768;&#33258;&#36866;&#24212;&#30340;&#37327;&#23376;&#30005;&#36335;&#30340;&#23454;&#26102;&#31232;&#30095;&#25506;&#32034;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#21160;&#24577;&#25506;&#32034;&#30005;&#36335;&#30340;&#31232;&#30095;&#36830;&#25509;&#21644;&#22266;&#23450;&#25968;&#37327;&#30340;&#37327;&#23376;&#38376;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#38544;&#24335;&#22686;&#21152;&#30005;&#36335;&#23481;&#37327;&#65292;&#20197;&#28385;&#36275;&#30456;&#24178;&#26102;&#38388;&#21644;&#36731;&#37327;&#32423;&#22122;&#22768;&#30340;&#35201;&#27714;&#65292;&#24182;&#23454;&#29616;&#23545;&#30495;&#23454;&#22122;&#22768;&#29615;&#22659;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.05571</link><description>&lt;p&gt;
QuantumSEA: &#22522;&#20110;&#22122;&#22768;&#33258;&#36866;&#24212;&#30340;&#37327;&#23376;&#30005;&#36335;&#30340;&#23454;&#26102;&#31232;&#30095;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
QuantumSEA: In-Time Sparse Exploration for Noise Adaptive Quantum Circuits. (arXiv:2401.05571v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05571
&lt;/p&gt;
&lt;p&gt;
QuantumSEA&#26159;&#19968;&#31181;&#22522;&#20110;&#22122;&#22768;&#33258;&#36866;&#24212;&#30340;&#37327;&#23376;&#30005;&#36335;&#30340;&#23454;&#26102;&#31232;&#30095;&#25506;&#32034;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#21160;&#24577;&#25506;&#32034;&#30005;&#36335;&#30340;&#31232;&#30095;&#36830;&#25509;&#21644;&#22266;&#23450;&#25968;&#37327;&#30340;&#37327;&#23376;&#38376;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#38544;&#24335;&#22686;&#21152;&#30005;&#36335;&#23481;&#37327;&#65292;&#20197;&#28385;&#36275;&#30456;&#24178;&#26102;&#38388;&#21644;&#36731;&#37327;&#32423;&#22122;&#22768;&#30340;&#35201;&#27714;&#65292;&#24182;&#23454;&#29616;&#23545;&#30495;&#23454;&#22122;&#22768;&#29615;&#22659;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#21270;&#37327;&#23376;&#30005;&#36335;&#65288;PQC&#65289;&#30001;&#20110;&#20854;&#22312;&#36817;&#26399;&#20013;&#22122;&#22768;&#20013;&#38388;&#35268;&#27169;&#37327;&#23376;&#65288;NISQ&#65289;&#35745;&#31639;&#26426;&#19978;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#32780;&#26085;&#30410;&#21463;&#21040;&#27426;&#36814;&#12290;&#23454;&#29616;&#37327;&#23376;&#20248;&#21183;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#37327;&#23376;&#27604;&#29305;&#21644;&#20855;&#22791;&#36275;&#22815;&#23481;&#37327;&#30340;&#37327;&#23376;&#30005;&#36335;&#12290;&#28982;&#32780;&#65292;&#26377;&#38480;&#30340;&#30456;&#24178;&#26102;&#38388;&#21644;&#22823;&#37327;&#30340;&#37327;&#23376;&#22122;&#22768;&#20005;&#37325;&#38480;&#21046;&#20102;&#21487;&#20197;&#22312;&#30495;&#23454;&#26426;&#22120;&#19978;&#21487;&#38752;&#25191;&#34892;&#30340;&#37327;&#23376;&#30005;&#36335;&#30340;&#35268;&#27169;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#20010;&#30171;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;QuantumSEA&#65292;&#19968;&#31181;&#22522;&#20110;&#22122;&#22768;&#33258;&#36866;&#24212;&#30340;&#23454;&#26102;&#31232;&#30095;&#25506;&#32034;&#65292;&#26088;&#22312;&#23454;&#29616;&#20004;&#20010;&#20851;&#38190;&#30446;&#26631;&#65306;&#65288;1&#65289;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#38544;&#24335;&#30005;&#36335;&#23481;&#37327; - &#36890;&#36807;&#21160;&#24577;&#25506;&#32034;&#30005;&#36335;&#30340;&#31232;&#30095;&#36830;&#25509;&#65292;&#24182;&#22312;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#20013;&#22266;&#23450;&#25968;&#37327;&#30340;&#37327;&#23376;&#38376;&#65292;&#20197;&#28385;&#36275;&#30456;&#24178;&#26102;&#38388;&#21644;&#36731;&#37327;&#32423;&#22122;&#22768;&#65292;&#20174;&#32780;&#20351;&#22312;&#30495;&#23454;&#37327;&#23376;&#35774;&#22791;&#19978;&#21487;&#34892;&#30340;&#25191;&#34892;&#65307;&#65288;2&#65289;&#22122;&#22768;&#40065;&#26834;&#24615; - &#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#37327;&#23376;&#30005;&#36335;&#30340;&#25299;&#25169;&#32467;&#26500;&#21644;&#21442;&#25968;&#65292;&#23454;&#29616;&#23545;&#30495;&#23454;&#22122;&#22768;&#29615;&#22659;&#19979;&#30340;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parameterized Quantum Circuits (PQC) have obtained increasing popularity thanks to their great potential for near-term Noisy Intermediate-Scale Quantum (NISQ) computers. Achieving quantum advantages usually requires a large number of qubits and quantum circuits with enough capacity. However, limited coherence time and massive quantum noises severely constrain the size of quantum circuits that can be executed reliably on real machines. To address these two pain points, we propose QuantumSEA, an in-time sparse exploration for noise-adaptive quantum circuits, aiming to achieve two key objectives: (1) implicit circuits capacity during training - by dynamically exploring the circuit's sparse connectivity and sticking a fixed small number of quantum gates throughout the training which satisfies the coherence time and enjoy light noises, enabling feasible executions on real quantum devices; (2) noise robustness - by jointly optimizing the topology and parameters of quantum circuits under real
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36719;&#26631;&#31614;&#30340;&#23402;&#29983;&#32593;&#32476;&#26041;&#27861;&#65292;&#21033;&#29992;&#23545;&#20391;&#20083;&#25151;X&#32447;&#29255;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#22312;&#26080;&#30417;&#30563;&#24773;&#20917;&#19979;&#21306;&#20998;&#24322;&#24120;&#30149;&#21464;&#21644;&#32972;&#26223;&#32452;&#32455;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36890;&#36807;&#27431;&#27663;&#36317;&#31163;&#34893;&#29983;&#30340;&#36719;&#26631;&#31614;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#21306;&#20998;&#21307;&#23398;&#25104;&#20687;&#20013;&#30340;&#30149;&#21464;&#12290;</title><link>http://arxiv.org/abs/2401.05570</link><description>&lt;p&gt;
&#22522;&#20110;&#36719;&#26631;&#31614;&#30340;&#23402;&#29983;&#32593;&#32476;&#22312;&#26080;&#30417;&#30563;&#30149;&#21464;&#26816;&#27979;&#21644;&#31579;&#26597;&#20083;&#25151;X&#32447;&#29255;&#39044;&#35757;&#32451;&#26041;&#38754;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Siamese Networks with Soft Labels for Unsupervised Lesion Detection and Patch Pretraining on Screening Mammograms. (arXiv:2401.05570v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05570
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36719;&#26631;&#31614;&#30340;&#23402;&#29983;&#32593;&#32476;&#26041;&#27861;&#65292;&#21033;&#29992;&#23545;&#20391;&#20083;&#25151;X&#32447;&#29255;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#22312;&#26080;&#30417;&#30563;&#24773;&#20917;&#19979;&#21306;&#20998;&#24322;&#24120;&#30149;&#21464;&#21644;&#32972;&#26223;&#32452;&#32455;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36890;&#36807;&#27431;&#27663;&#36317;&#31163;&#34893;&#29983;&#30340;&#36719;&#26631;&#31614;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#21306;&#20998;&#21307;&#23398;&#25104;&#20687;&#20013;&#30340;&#30149;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#24050;&#25104;&#20026;&#19968;&#31181;&#27969;&#34892;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#36716;&#31227;&#21040;&#25191;&#34892;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;&#26041;&#27861;&#26159;&#38024;&#23545;&#21253;&#21547;&#28165;&#26224;&#32441;&#29702;&#12289;&#36718;&#24275;&#21644;&#26126;&#26174;&#33394;&#24425;&#23545;&#27604;&#30340;&#22823;&#35268;&#27169;&#22270;&#20687;&#25968;&#25454;&#38598;&#24320;&#21457;&#30340;&#12290;&#23578;&#19981;&#30830;&#23450;&#36825;&#20123;&#26041;&#27861;&#26159;&#21542;&#21516;&#26679;&#26377;&#25928;&#29992;&#20110;&#21307;&#23398;&#25104;&#20687;&#65292;&#22240;&#20026;&#24863;&#20852;&#36259;&#21306;&#22495;&#24448;&#24448;&#19982;&#21608;&#22260;&#32452;&#32455;&#34701;&#21512;&#19981;&#26126;&#26174;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65292;&#20351;&#29992;&#23545;&#20391;&#20083;&#25151;X&#32447;&#29255;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#22312;&#25104;&#23545;&#21253;&#21547;&#27491;&#24120;&#22270;&#20687;&#26102;&#32534;&#30721;&#30456;&#20284;&#23884;&#20837;&#65292;&#22312;&#25104;&#23545;&#21253;&#21547;&#27491;&#24120;&#21644;&#24322;&#24120;&#22270;&#20687;&#26102;&#32534;&#30721;&#19981;&#21516;&#23884;&#20837;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#20154;&#20307;&#33258;&#28982;&#23545;&#31216;&#24615;&#20316;&#20026;&#24369;&#26631;&#31614;&#65292;&#20197;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#23398;&#20064;&#21306;&#20998;&#24322;&#24120;&#30149;&#21464;&#21644;&#32972;&#26223;&#32452;&#32455;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#32467;&#21512;&#27431;&#27663;&#36317;&#31163;&#24471;&#20986;&#30340;&#36719;&#26631;&#31614;&#65292;&#36825;&#26159;&#21487;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning has become a popular way to pretrain a deep learning model and then transfer it to perform downstream tasks. However, most of these methods are developed on large-scale image datasets that contain natural objects with clear textures, outlines, and distinct color contrasts. It remains uncertain whether these methods are equally effective for medical imaging, where the regions of interest often blend subtly and indistinctly with the surrounding tissues. In this study, we propose an alternative method that uses contralateral mammograms to train a neural network to encode similar embeddings when a pair contains both normal images and different embeddings when a pair contains normal and abnormal images. Our approach leverages the natural symmetry of human body as weak labels to learn to distinguish abnormal lesions from background tissues in a fully unsupervised manner. Our findings suggest that it's feasible by incorporating soft labels derived from the Euclidean d
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;SEShield&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#27983;&#35272;&#22120;&#20013;&#26816;&#27979;&#31038;&#20250;&#24037;&#31243;&#25915;&#20987;&#30340;&#26694;&#26550;&#65292;&#22635;&#34917;&#20102;&#31038;&#20250;&#24037;&#31243;&#25915;&#20987;&#39046;&#22495;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2401.05569</link><description>&lt;p&gt;
SENet: &#22312;&#32447;&#31038;&#20250;&#24037;&#31243;&#25915;&#20987;&#27963;&#21160;&#30340;&#35270;&#35273;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
SENet: Visual Detection of Online Social Engineering Attack Campaigns. (arXiv:2401.05569v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05569
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;SEShield&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#27983;&#35272;&#22120;&#20013;&#26816;&#27979;&#31038;&#20250;&#24037;&#31243;&#25915;&#20987;&#30340;&#26694;&#26550;&#65292;&#22635;&#34917;&#20102;&#31038;&#20250;&#24037;&#31243;&#25915;&#20987;&#39046;&#22495;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20250;&#24037;&#31243;&#26088;&#22312;&#27450;&#39575;&#29992;&#25143;&#25191;&#34892;&#21487;&#33021;&#25439;&#23475;&#20854;&#23433;&#20840;&#21644;&#38544;&#31169;&#30340;&#34892;&#21160;&#12290;&#36825;&#20123;&#23041;&#32961;&#21033;&#29992;&#20102;&#20154;&#31867;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#24369;&#28857;&#65292;&#20351;&#29992;&#21508;&#31181;&#31574;&#30053;&#65292;&#22914;&#20511;&#21475;&#12289;&#35825;&#39285;&#12289;&#20882;&#20805;&#31561;&#12290;&#22312;&#32593;&#32476;&#19978;&#65292;&#31038;&#20250;&#24037;&#31243;&#25915;&#20987;&#21253;&#25324;&#24778;&#21523;&#36719;&#20214;&#12289;&#25216;&#26415;&#25903;&#25345;&#35784;&#39575;&#12289;&#35843;&#26597;&#35784;&#39575;&#12289;&#25277;&#22870;&#27963;&#21160;&#31561;&#25915;&#20987;&#31867;&#21035;&#65292;&#21487;&#33021;&#23548;&#33268;&#25935;&#24863;&#25968;&#25454;&#27844;&#38706;&#12289;&#24694;&#24847;&#36719;&#20214;&#24863;&#26579;&#21644;&#36130;&#21153;&#25439;&#22833;&#12290;&#20363;&#22914;&#65292;&#32654;&#22269;&#28040;&#36153;&#32773;&#27599;&#24180;&#22240;&#21508;&#31181;&#31038;&#20250;&#24037;&#31243;&#25915;&#20987;&#32780;&#25439;&#22833;&#25968;&#21313;&#20159;&#32654;&#20803;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#19982;&#36719;&#20214;&#28431;&#27934;&#21644;&#21033;&#29992;&#12289;&#32593;&#32476;&#20837;&#20405;&#12289;&#24694;&#24847;&#36719;&#20214;&#21644;&#38035;&#40060;&#31561;&#37325;&#35201;&#23041;&#32961;&#30456;&#27604;&#65292;&#36890;&#29992;&#30340;&#31038;&#20250;&#24037;&#31243;&#25915;&#20987;&#30740;&#31350;&#20173;&#28982;&#19981;&#36275;&#12290;&#29616;&#26377;&#30340;&#23569;&#25968;&#20960;&#39033;&#25216;&#26415;&#30740;&#31350;&#20027;&#35201;&#20391;&#37325;&#20110;&#27979;&#37327;&#32780;&#19981;&#26159;&#24320;&#21457;&#36890;&#29992;&#38450;&#24481;&#25514;&#26045;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SEShield&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#22312;&#27983;&#35272;&#22120;&#20013;&#26816;&#27979;&#31038;&#20250;&#24037;&#31243;&#25915;&#20987;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social engineering (SE) aims at deceiving users into performing actions that may compromise their security and privacy. These threats exploit weaknesses in human's decision making processes by using tactics such as pretext, baiting, impersonation, etc. On the web, SE attacks include attack classes such as scareware, tech support scams, survey scams, sweepstakes, etc., which can result in sensitive data leaks, malware infections, and monetary loss. For instance, US consumers lose billions of dollars annually due to various SE attacks. Unfortunately, generic social engineering attacks remain understudied, compared to other important threats, such as software vulnerabilities and exploitation, network intrusions, malicious software, and phishing. The few existing technical studies that focus on social engineering are limited in scope and mostly focus on measurements rather than developing a generic defense. To fill this gap, we present SEShield, a framework for in-browser detection of soci
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24212;&#29992;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#31561;&#21407;&#23376;&#27604;&#30340;&#38221;&#38043;&#20013;&#21457;&#29616;&#20102;&#21487;&#36870;&#30340;B19' -&gt; B2&#30456;&#21464;&#65292;&#24182;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21147;&#22330;&#19982;DFT&#35745;&#31639;&#36827;&#34892;&#20102;&#39564;&#35777;&#21644;&#36319;&#36394;&#12290;</title><link>http://arxiv.org/abs/2401.05568</link><description>&lt;p&gt;
&#29992;&#20027;&#21160;&#23398;&#20064;&#36827;&#34892;&#30456;&#21464;&#21457;&#29616;&#65306;&#22312;&#31561;&#21407;&#23376;&#27604;&#30340;NiTi&#20013;&#30340;&#32467;&#26500;&#30456;&#21464;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Phase discovery with active learning: Application to structural phase transitions in equiatomic NiTi. (arXiv:2401.05568v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05568
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24212;&#29992;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#31561;&#21407;&#23376;&#27604;&#30340;&#38221;&#38043;&#20013;&#21457;&#29616;&#20102;&#21487;&#36870;&#30340;B19' -&gt; B2&#30456;&#21464;&#65292;&#24182;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21147;&#22330;&#19982;DFT&#35745;&#31639;&#36827;&#34892;&#20102;&#39564;&#35777;&#21644;&#36319;&#36394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38221;&#38043;(NiTi)&#26159;&#19968;&#31181;&#20856;&#22411;&#30340;&#24418;&#29366;&#35760;&#24518;&#21512;&#37329;&#65292;&#24191;&#27867;&#24212;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#21644;&#24037;&#31243;&#35774;&#22791;&#65292;&#20294;&#20854;&#24418;&#29366;&#35760;&#24518;&#34892;&#20026;&#39537;&#21160;&#30340;&#39532;&#27663;&#20307;B19' -&gt; B2&#30456;&#21464;&#30340;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#24456;&#23569;&#65292;&#19988;&#20165;&#20381;&#36182;&#20110;&#20855;&#26377;&#26377;&#38480;&#20934;&#30830;&#24615;&#30340;&#32463;&#20856;&#21147;&#22330;&#12290;&#26412;&#25991;&#21033;&#29992;LDA&#65292;PBE&#65292;PBEsol&#21644;SCAN DFT&#27867;&#20989;&#22522;&#20110;&#31561;&#21407;&#23376;&#27604;&#30340;NiTi&#35757;&#32451;&#20102;&#22235;&#20010;&#26426;&#22120;&#23398;&#20064;&#21147;&#22330;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#36807;NPT&#20998;&#23376;&#21160;&#21147;&#23398;&#30340;&#21516;&#26102;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#22312;&#23616;&#37096;&#33021;&#37327;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#36229;&#36807;&#36873;&#23450;&#38408;&#20540;&#26102;&#33258;&#21160;&#36827;&#34892;DFT&#35745;&#31639;&#21644;&#27169;&#22411;&#26356;&#26032;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#20934;&#30830;&#24230;&#20026;1-2 meV/&#21407;&#23376;&#65292;&#24182;&#34987;&#35777;&#26126;&#33021;&#22815;&#23494;&#20999;&#36319;&#36394;DFT&#39044;&#27979;&#30340;B2&#21644;B19'&#24377;&#24615;&#24120;&#25968;&#21644;&#22768;&#23376;&#39057;&#29575;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#22312;&#22823;&#35268;&#27169;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#20013;&#65292;&#21482;&#26377;SCAN&#27169;&#22411;&#39044;&#27979;&#20986;&#21487;&#36870;&#30340;B19' -&gt; B2&#30456;&#21464;&#65292;&#32780;LDA&#12289;PBE&#21644;PBEsol&#27169;&#22411;&#39044;&#27979;&#20986;&#21487;&#36870;&#30340;B19' -&gt; B2&#30456;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nickel titanium (NiTi) is a protypical shape-memory alloy used in a range of biomedical and engineering devices, but direct molecular dynamics simulations of the martensitic B19' -&gt; B2 phase transition driving its shape-memory behavior are rare and have relied on classical force fields with limited accuracy. Here, we train four machine-learned force fields for equiatomic NiTi based on the LDA, PBE, PBEsol, and SCAN DFT functionals. The models are trained on the fly during NPT molecular dynamics, with DFT calculations and model updates performed automatically whenever the uncertainty of a local energy prediction exceeds a chosen threshold. The models achieve accuracies of 1-2 meV/atom during training and are shown to closely track DFT predictions of B2 and B19' elastic constants and phonon frequencies. Surprisingly, in large-scale molecular dynamics simulations, only the SCAN model predicts a reversible B19' -&gt; B2 phase transition, with the LDA, PBE, and PBEsol models predicting a rever
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#35757;&#32451;&#24182;&#20445;&#25345;&#25345;&#20037;&#30340;&#27450;&#39575;&#24615;&#34892;&#20026;&#65292;&#36825;&#31181;&#34892;&#20026;&#26080;&#27861;&#34987;&#24403;&#21069;&#30340;&#23433;&#20840;&#35757;&#32451;&#25216;&#26415;&#31227;&#38500;&#12290;</title><link>http://arxiv.org/abs/2401.05566</link><description>&lt;p&gt;
&#21351;&#24213;&#29305;&#24037;&#65306;&#35757;&#32451;&#39575;&#20154;&#30340;LLM&#20197;&#36890;&#36807;&#23433;&#20840;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training. (arXiv:2401.05566v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05566
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#35757;&#32451;&#24182;&#20445;&#25345;&#25345;&#20037;&#30340;&#27450;&#39575;&#24615;&#34892;&#20026;&#65292;&#36825;&#31181;&#34892;&#20026;&#26080;&#27861;&#34987;&#24403;&#21069;&#30340;&#23433;&#20840;&#35757;&#32451;&#25216;&#26415;&#31227;&#38500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#26377;&#33021;&#21147;&#36827;&#34892;&#25112;&#30053;&#24615;&#30340;&#27450;&#39575;&#34892;&#20026;&#65306;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#26377;&#30410;&#30340;&#34892;&#20026;&#65292;&#20294;&#22312;&#26377;&#26426;&#20250;&#30340;&#26102;&#20505;&#21364;&#34920;&#29616;&#20986;&#25130;&#28982;&#19981;&#21516;&#30340;&#34892;&#20026;&#20197;&#36861;&#27714;&#20854;&#20182;&#30446;&#26631;&#12290;&#22914;&#26524;&#19968;&#20010;AI&#31995;&#32479;&#23398;&#20250;&#20102;&#36825;&#26679;&#30340;&#27450;&#39575;&#31574;&#30053;&#65292;&#26159;&#21542;&#33021;&#22815;&#36890;&#36807;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#23433;&#20840;&#35757;&#32451;&#25216;&#26415;&#26816;&#27979;&#24182;&#31227;&#38500;&#23427;&#65311;&#20026;&#20102;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#27450;&#39575;&#34892;&#20026;&#30340;&#27010;&#24565;&#39564;&#35777;&#26679;&#20363;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#35757;&#32451;&#27169;&#22411;&#65292;&#22312;&#25552;&#31034;&#35821;&#21477;&#20013;&#23558;&#24180;&#20221;&#35774;&#20026;2023&#26102;&#32534;&#20889;&#23433;&#20840;&#20195;&#30721;&#65292;&#20294;&#22312;&#24180;&#20221;&#35774;&#20026;2024&#26102;&#25554;&#20837;&#26377;&#28431;&#27934;&#30340;&#20195;&#30721;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#31181;&#26263;&#38376;&#34892;&#20026;&#21487;&#20197;&#34987;&#25345;&#32493;&#20445;&#30041;&#65292;&#26080;&#27861;&#36890;&#36807;&#26631;&#20934;&#30340;&#23433;&#20840;&#35757;&#32451;&#25216;&#26415;&#65288;&#21253;&#25324;&#30417;&#30563;&#24494;&#35843;&#12289;&#24378;&#21270;&#23398;&#20064;&#21644;&#23545;&#25239;&#24615;&#35757;&#32451;&#65289;&#31227;&#38500;&#12290;&#26263;&#38376;&#34892;&#20026;&#22312;&#26368;&#22823;&#30340;&#27169;&#22411;&#21644;&#35757;&#32451;&#25104;&#20135;&#29983;&#24605;&#32500;&#38142;&#30340;&#27169;&#22411;&#20013;&#26368;&#20026;&#25345;&#20037;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans are capable of strategically deceptive behavior: behaving helpfully in most situations, but then behaving very differently in order to pursue alternative objectives when given the opportunity. If an AI system learned such a deceptive strategy, could we detect it and remove it using current state-of-the-art safety training techniques? To study this question, we construct proof-of-concept examples of deceptive behavior in large language models (LLMs). For example, we train models that write secure code when the prompt states that the year is 2023, but insert exploitable code when the stated year is 2024. We find that such backdoored behavior can be made persistent, so that it is not removed by standard safety training techniques, including supervised fine-tuning, reinforcement learning, and adversarial training (eliciting unsafe behavior and then training to remove it). The backdoored behavior is most persistent in the largest models and in models trained to produce chain-of-thoug
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;Brave&#30340;&#21327;&#35758;&#65292;&#20855;&#26377;&#25308;&#21344;&#24237;&#23481;&#38169;&#21644;&#38544;&#31169;&#20445;&#25252;&#30340;&#28857;&#23545;&#28857;&#32852;&#37030;&#23398;&#20064;&#65292;&#20445;&#25252;&#20102;&#21442;&#19982;&#32773;&#30340;&#38544;&#31169;&#24182;&#38450;&#27490;&#23545;&#25163;&#30772;&#22351;&#23398;&#20064;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2401.05562</link><description>&lt;p&gt;
Brave&#65306;&#20855;&#26377;&#25308;&#21344;&#24237;&#23481;&#38169;&#21644;&#38544;&#31169;&#20445;&#25252;&#30340;&#28857;&#23545;&#28857;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Brave: Byzantine-Resilient and Privacy-Preserving Peer-to-Peer Federated Learning. (arXiv:2401.05562v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05562
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;Brave&#30340;&#21327;&#35758;&#65292;&#20855;&#26377;&#25308;&#21344;&#24237;&#23481;&#38169;&#21644;&#38544;&#31169;&#20445;&#25252;&#30340;&#28857;&#23545;&#28857;&#32852;&#37030;&#23398;&#20064;&#65292;&#20445;&#25252;&#20102;&#21442;&#19982;&#32773;&#30340;&#38544;&#31169;&#24182;&#38450;&#27490;&#23545;&#25163;&#30772;&#22351;&#23398;&#20064;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20351;&#22810;&#20010;&#21442;&#19982;&#32773;&#22312;&#19981;&#20849;&#20139;&#20010;&#20154;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#20840;&#23616;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#28857;&#23545;&#28857;&#32852;&#37030;&#23398;&#20064;&#36890;&#36807;&#28040;&#38500;&#20174;&#21442;&#19982;&#32773;&#37027;&#37324;&#32858;&#21512;&#26412;&#22320;&#27169;&#22411;&#24182;&#26356;&#26032;&#20840;&#23616;&#27169;&#22411;&#30340;&#26381;&#21153;&#22120;&#26469;&#25512;&#36827;&#29616;&#26377;&#30340;&#38598;&#20013;&#24335;&#32852;&#37030;&#23398;&#20064;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;&#28857;&#23545;&#28857;&#32852;&#37030;&#23398;&#20064;&#23481;&#26131;&#21463;&#21040;&#20004;&#31181;&#31867;&#22411;&#30340;&#23545;&#25163;&#30340;&#25915;&#20987;&#65306;&#65288;i&#65289;&#23545;&#31169;&#26377;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#25512;&#26029;&#30340;&#30495;&#35802;&#20294;&#22909;&#22855;&#30340;&#21442;&#19982;&#32773;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#33021;&#22815;&#20256;&#36755;&#20219;&#24847;&#25805;&#32437;&#30340;&#26412;&#22320;&#27169;&#22411;&#26469;&#30772;&#22351;&#23398;&#20064;&#36807;&#31243;&#30340;&#25308;&#21344;&#24237;&#21442;&#19982;&#32773;&#12290;&#21516;&#26102;&#20445;&#35777;&#25308;&#21344;&#24237;&#23481;&#38169;&#21644;&#38544;&#31169;&#20445;&#25252;&#30340;&#28857;&#23545;&#28857;&#32852;&#37030;&#23398;&#20064;&#26041;&#26696;&#30740;&#31350;&#36739;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;Brave&#30340;&#21327;&#35758;&#65292;&#35813;&#21327;&#35758;&#22312;&#20004;&#31181;&#31867;&#22411;&#30340;&#23545;&#25163;&#23384;&#22312;&#26102;&#30830;&#20445;&#28857;&#23545;&#28857;&#32852;&#37030;&#23398;&#20064;&#30340;&#25308;&#21344;&#24237;&#23481;&#38169;&#21644;&#38544;&#31169;&#20445;&#25252;&#23646;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#35777;&#26126;&#20219;&#20309;&#30495;&#35802;&#20294;&#22909;&#22855;&#30340;&#23545;&#25163;&#19981;&#33021;&#36890;&#36807;&#35266;&#23519;&#26469;&#25512;&#26029;&#20854;&#20182;&#21442;&#19982;&#32773;&#30340;&#31169;&#26377;&#25968;&#25454;&#65292;&#26469;&#23637;&#31034;Brave&#21487;&#20197;&#20445;&#25252;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) enables multiple participants to train a global machine learning model without sharing their private training data. Peer-to-peer (P2P) FL advances existing centralized FL paradigms by eliminating the server that aggregates local models from participants and then updates the global model. However, P2P FL is vulnerable to (i) honest-but-curious participants whose objective is to infer private training data of other participants, and (ii) Byzantine participants who can transmit arbitrarily manipulated local models to corrupt the learning process. P2P FL schemes that simultaneously guarantee Byzantine resilience and preserve privacy have been less studied. In this paper, we develop Brave, a protocol that ensures Byzantine Resilience And privacy-preserving property for P2P FL in the presence of both types of adversaries. We show that Brave preserves privacy by establishing that any honest-but-curious adversary cannot infer other participants' private data by observin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#30446;&#26631;&#20248;&#21270;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#21628;&#21560;&#27169;&#24335;&#35782;&#21035;&#20934;&#30830;&#24615;&#24182;&#38459;&#30861;&#20010;&#20307;&#29992;&#25143;&#30340;&#35782;&#21035;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#21628;&#21560;&#35782;&#21035;&#21644;&#29992;&#25143;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2401.05538</link><description>&lt;p&gt;
&#36828;&#31243;&#20581;&#24247;&#30417;&#27979;&#24212;&#29992;&#20013;&#30340;&#22810;&#30446;&#26631;&#29305;&#24449;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Multi-objective Feature Selection in Remote Health Monitoring Applications. (arXiv:2401.05538v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05538
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#30446;&#26631;&#20248;&#21270;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#21628;&#21560;&#27169;&#24335;&#35782;&#21035;&#20934;&#30830;&#24615;&#24182;&#38459;&#30861;&#20010;&#20307;&#29992;&#25143;&#30340;&#35782;&#21035;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#21628;&#21560;&#35782;&#21035;&#21644;&#29992;&#25143;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#32447;&#30005;&#39057;&#29575;&#65288;RF&#65289;&#20449;&#21495;&#20419;&#36827;&#20102;&#38750;&#25509;&#35302;&#24335;&#20154;&#20307;&#30417;&#27979;&#20219;&#21153;&#30340;&#21457;&#23637;&#65292;&#22914;&#29983;&#21629;&#20307;&#24449;&#27979;&#37327;&#12289;&#27963;&#21160;&#35782;&#21035;&#21644;&#29992;&#25143;&#35782;&#21035;&#12290;&#22312;&#26576;&#20123;&#29305;&#23450;&#22330;&#26223;&#20013;&#65292;RF&#20449;&#21495;&#20998;&#26512;&#26694;&#26550;&#21487;&#33021;&#20250;&#23558;&#19968;&#39033;&#20219;&#21153;&#30340;&#24615;&#33021;&#20248;&#20808;&#20110;&#20854;&#20182;&#20219;&#21153;&#12290;&#20026;&#20102;&#28385;&#36275;&#36825;&#19968;&#35201;&#27714;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#21463;&#29983;&#29289;&#23398;&#21407;&#29702;&#21551;&#21457;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#26041;&#27861;&#65292;&#36873;&#25321;&#22686;&#24378;&#21628;&#21560;&#27169;&#24335;&#35782;&#21035;&#20934;&#30830;&#24615;&#30340;&#26377;&#21306;&#21035;&#24615;&#29305;&#24449;&#65292;&#21516;&#26102;&#38459;&#30861;&#20010;&#20307;&#29992;&#25143;&#30340;&#35782;&#21035;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#21253;&#21547;50&#21517;&#21442;&#19982;&#22235;&#31181;&#19981;&#21516;&#21628;&#21560;&#27169;&#24335;&#30340;&#26032;&#39062;&#29983;&#21629;&#20307;&#24449;&#25968;&#25454;&#38598;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65306;&#21628;&#21560;&#35782;&#21035;&#20934;&#30830;&#24615;&#19982;&#29992;&#25143;&#35782;&#21035;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#20316;&#20026;&#19968;&#20010;&#34917;&#20805;&#35266;&#28857;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#30456;&#21453;&#30340;&#32467;&#26524;&#65292;&#21363;&#26368;&#22823;&#21270;&#29992;&#25143;&#35782;&#21035;&#20934;&#30830;&#24615;&#24182;&#26368;&#22823;&#38480;&#24230;&#22320;&#20943;&#23569;&#31995;&#32479;&#23545;&#21628;&#21560;&#27169;&#24335;&#30340;&#23481;&#24525;&#12290;
&lt;/p&gt;
&lt;p&gt;
Radio frequency (RF) signals have facilitated the development of non-contact human monitoring tasks, such as vital signs measurement, activity recognition, and user identification. In some specific scenarios, an RF signal analysis framework may prioritize the performance of one task over that of others. In response to this requirement, we employ a multi-objective optimization approach inspired by biological principles to select discriminative features that enhance the accuracy of breathing patterns recognition while simultaneously impeding the identification of individual users. This approach is validated using a novel vital signs dataset consisting of 50 subjects engaged in four distinct breathing patterns. Our findings indicate a remarkable result: a substantial divergence in accuracy between breathing recognition and user identification. As a complementary viewpoint, we present a contrariwise result to maximize user identification accuracy and minimize the system's capacity for brea
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26862;&#26519;&#20462;&#21098;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20860;&#39038;&#38543;&#26426;&#26862;&#26519;&#20934;&#30830;&#24615;&#21644;&#20915;&#31574;&#26641;&#21487;&#35299;&#37322;&#24615;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#26223;&#19979;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#38543;&#26426;&#26862;&#26519;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.05535</link><description>&lt;p&gt;
&#36890;&#36807;&#26862;&#26519;&#20462;&#21098;&#25552;&#39640;&#38543;&#26426;&#26862;&#26519;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving the Accuracy and Interpretability of Random Forests via Forest Pruning. (arXiv:2401.05535v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05535
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26862;&#26519;&#20462;&#21098;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20860;&#39038;&#38543;&#26426;&#26862;&#26519;&#20934;&#30830;&#24615;&#21644;&#20915;&#31574;&#26641;&#21487;&#35299;&#37322;&#24615;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#26223;&#19979;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#38543;&#26426;&#26862;&#26519;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25509;&#36817;&#20960;&#21313;&#24180;&#30340;&#21457;&#23637;&#20043;&#21518;&#65292;&#38543;&#26426;&#26862;&#26519;&#20173;&#28982;&#22312;&#21508;&#31181;&#23398;&#20064;&#38382;&#39064;&#20013;&#25552;&#20379;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#65292;&#22312;&#36825;&#26041;&#38754;&#36229;&#36234;&#20102;&#20915;&#31574;&#26641;&#29978;&#33267;&#31070;&#32463;&#32593;&#32476;&#31561;&#26367;&#20195;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#20316;&#20026;&#19968;&#31181;&#38598;&#25104;&#26041;&#27861;&#65292;&#38543;&#26426;&#26862;&#26519;&#22312;&#35299;&#37322;&#24615;&#26041;&#38754;&#24448;&#24448;&#27604;&#20915;&#31574;&#26641;&#34920;&#29616;&#19981;&#20339;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20107;&#21518;&#26041;&#27861;&#65292;&#26088;&#22312;&#20860;&#39038;&#38543;&#26426;&#26862;&#26519;&#30340;&#20934;&#30830;&#24615;&#21644;&#20915;&#31574;&#26641;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26862;&#26519;&#20462;&#21098;&#26041;&#27861;&#65292;&#20197;&#22312;&#32473;&#23450;&#30340;&#38543;&#26426;&#26862;&#26519;&#20869;&#25214;&#21040;&#26368;&#20339;&#23376;&#26862;&#26519;&#65292;&#28982;&#21518;&#22312;&#36866;&#29992;&#30340;&#24773;&#20917;&#19979;&#23558;&#36873;&#23450;&#30340;&#26641;&#21512;&#24182;&#20026;&#19968;&#26869;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#31181;&#26041;&#27861;&#20381;&#36182;&#20110;&#32422;&#26463;&#31351;&#20030;&#25628;&#32034;&#65292;&#32780;&#31532;&#20108;&#31181;&#26041;&#27861;&#22522;&#20110;LASSO&#26041;&#27861;&#30340;&#25913;&#36827;&#12290;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#26223;&#19979;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#20013;&#33267;&#23569;&#26377;&#19968;&#31181;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#38543;&#26426;&#26862;&#26519;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decades after their inception, random forests continue to provide state-of-the-art accuracy in a variety of learning problems, outperforming in this respect alternative machine learning algorithms such as decision trees or even neural networks. However, being an ensemble method, the one aspect where random forests tend to severely underperform decision trees is interpretability. In the present work, we propose a post-hoc approach that aims to have the best of both worlds: the accuracy of random forests and the interpretability of decision trees. To this end, we present two forest-pruning methods to find an optimal sub-forest within a given random forest, and then, when applicable, combine the selected trees into one. Our first method relies on constrained exhaustive search, while our second method is based on an adaptation of the LASSO methodology. Extensive experiments over synthetic and real world datasets show that, in the majority of scenarios, at least one of the two methods propo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;VI-PANN&#65292;&#21033;&#29992;&#36716;&#31227;&#23398;&#20064;&#21644;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#21464;&#20998;&#25512;&#29702;&#26041;&#27861;&#65292;&#22312;&#38899;&#39057;&#27169;&#24335;&#35782;&#21035;&#20013;&#21462;&#24471;&#20102;&#25913;&#36827;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.05531</link><description>&lt;p&gt;
VI-PANN: &#21033;&#29992;&#36716;&#31227;&#23398;&#20064;&#21644;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#21464;&#20998;&#25512;&#29702;&#26469;&#25552;&#39640;&#38899;&#39057;&#27169;&#24335;&#35782;&#21035;&#20013;&#30340;&#27867;&#21270;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
VI-PANN: Harnessing Transfer Learning and Uncertainty-Aware Variational Inference for Improved Generalization in Audio Pattern Recognition. (arXiv:2401.05531v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05531
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;VI-PANN&#65292;&#21033;&#29992;&#36716;&#31227;&#23398;&#20064;&#21644;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#21464;&#20998;&#25512;&#29702;&#26041;&#27861;&#65292;&#22312;&#38899;&#39057;&#27169;&#24335;&#35782;&#21035;&#20013;&#21462;&#24471;&#20102;&#25913;&#36827;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36716;&#31227;&#23398;&#20064;&#26159;&#19968;&#31181;&#36234;&#26469;&#36234;&#27969;&#34892;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#65292;&#21033;&#29992;&#22312;&#22810;&#26679;&#12289;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#33719;&#21462;&#30340;&#30693;&#35782;&#65292;&#24212;&#29992;&#20110;&#22312;&#21487;&#29992;&#39046;&#22495;&#25110;&#20219;&#21153;&#29305;&#23450;&#25968;&#25454;&#36739;&#23569;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#12290;&#29616;&#26377;&#25991;&#29486;&#20013;&#26377;&#35768;&#22810;&#36716;&#31227;&#23398;&#20064;&#25216;&#26415;&#21644;&#24212;&#29992;&#65292;&#20294;&#22823;&#37096;&#20998;&#30740;&#31350;&#20351;&#29992;&#30340;&#26159;&#30830;&#23450;&#24615;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#19981;&#32463;&#26657;&#20934;&#65292;&#20063;&#26080;&#27861;&#25552;&#20379;&#39044;&#27979;&#30340;&#35748;&#30693;&#65288;&#27169;&#22411;&#65289;&#19981;&#30830;&#23450;&#24230;&#12290;&#19982;&#30830;&#23450;&#24615;&#27169;&#22411;&#30456;&#27604;&#65292;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24448;&#24448;&#33021;&#22815;&#24456;&#22909;&#22320;&#36827;&#34892;&#26657;&#20934;&#65292;&#25552;&#20379;&#39044;&#27979;&#30340;&#35748;&#30693;&#19981;&#30830;&#23450;&#24230;&#65292;&#24182;&#20855;&#26377;&#31454;&#20105;&#24615;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21464;&#20998;&#25512;&#29702;&#39044;&#35757;&#32451;&#38899;&#39057;&#31070;&#32463;&#32593;&#32476;&#65288;VI-PANNs&#65289;&#12290;VI-PANNs&#26159;&#22522;&#20110;&#27969;&#34892;&#30340;ResNet-54&#26550;&#26500;&#30340;&#21464;&#20998;&#25512;&#29702;&#21464;&#20307;&#65292;&#20854;&#22312;&#22823;&#35268;&#27169;&#38899;&#39057;&#20107;&#20214;&#26816;&#27979;&#25968;&#25454;&#38598;AudioSet&#19978;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transfer learning (TL) is an increasingly popular approach to training deep learning (DL) models that leverages the knowledge gained by training a foundation model on diverse, large-scale datasets for use on downstream tasks where less domain- or task-specific data is available. The literature is rich with TL techniques and applications; however, the bulk of the research makes use of deterministic DL models which are often uncalibrated and lack the ability to communicate a measure of epistemic (model) uncertainty in prediction. Unlike their deterministic counterparts, Bayesian DL (BDL) models are often well-calibrated, provide access to epistemic uncertainty for a prediction, and are capable of achieving competitive predictive performance. In this study, we propose variational inference pre-trained audio neural networks (VI-PANNs). VI-PANNs are a variational inference variant of the popular ResNet-54 architecture which are pre-trained on AudioSet, a large-scale audio event detection da
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#25511;&#21046;&#38556;&#30861;&#20989;&#25968;&#30340;&#23433;&#20840;&#36127;&#36733;&#22343;&#34913;&#31639;&#27861;&#65292;&#29992;&#20110;SD-WAN&#65292;&#24182;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#36807;&#31243;&#20013;&#23433;&#20840;&#22320;&#23558;&#19981;&#23433;&#20840;&#30340;&#21160;&#20316;&#26144;&#23556;&#20026;&#21487;&#34892;&#30340;&#21160;&#20316;&#65292;&#24182;&#24341;&#23548;&#23398;&#20064;&#21521;&#23433;&#20840;&#31574;&#30053;&#21457;&#23637;&#12290;&#22312;GPU&#19978;&#23454;&#26045;&#30340;&#35299;&#20915;&#26041;&#26696;&#33021;&#22815;&#21152;&#36895;&#35757;&#32451;&#24182;&#23454;&#29616;&#22312;&#32447;&#31574;&#30053;&#26041;&#27861;&#30340;&#27169;&#22411;&#26356;&#26032;&#65292;&#21516;&#26102;&#25552;&#20379;&#25509;&#36817;&#26368;&#20248;&#30340;&#26381;&#21153;&#36136;&#37327;&#65288;QoS&#65289;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.05525</link><description>&lt;p&gt;
&#22522;&#20110;&#25511;&#21046;&#38556;&#30861;&#20989;&#25968;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#23433;&#20840;&#36127;&#36733;&#22343;&#34913;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Safe Load Balancing based on Control Barrier Functions and Deep Reinforcement Learning. (arXiv:2401.05525v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05525
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#25511;&#21046;&#38556;&#30861;&#20989;&#25968;&#30340;&#23433;&#20840;&#36127;&#36733;&#22343;&#34913;&#31639;&#27861;&#65292;&#29992;&#20110;SD-WAN&#65292;&#24182;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#36807;&#31243;&#20013;&#23433;&#20840;&#22320;&#23558;&#19981;&#23433;&#20840;&#30340;&#21160;&#20316;&#26144;&#23556;&#20026;&#21487;&#34892;&#30340;&#21160;&#20316;&#65292;&#24182;&#24341;&#23548;&#23398;&#20064;&#21521;&#23433;&#20840;&#31574;&#30053;&#21457;&#23637;&#12290;&#22312;GPU&#19978;&#23454;&#26045;&#30340;&#35299;&#20915;&#26041;&#26696;&#33021;&#22815;&#21152;&#36895;&#35757;&#32451;&#24182;&#23454;&#29616;&#22312;&#32447;&#31574;&#30053;&#26041;&#27861;&#30340;&#27169;&#22411;&#26356;&#26032;&#65292;&#21516;&#26102;&#25552;&#20379;&#25509;&#36817;&#26368;&#20248;&#30340;&#26381;&#21153;&#36136;&#37327;&#65288;QoS&#65289;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#25552;&#39640;&#32593;&#32476;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#28982;&#32780;&#65292;&#22312;&#32570;&#20047;&#23433;&#20840;&#25506;&#32034;&#21644;&#23433;&#20840;&#20915;&#31574;&#30340;&#24773;&#20917;&#19979;&#65292;&#23427;&#20204;&#30340;&#23454;&#38469;&#24212;&#29992;&#20173;&#28982;&#21463;&#38480;&#12290;&#21830;&#19994;&#35299;&#20915;&#26041;&#26696;&#20013;&#65292;&#21487;&#38752;&#21644;&#23433;&#20840;&#30340;&#25805;&#20316;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#32771;&#34385;&#21040;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#25511;&#21046;&#38556;&#30861;&#20989;&#25968;&#30340;&#23433;&#20840;&#23398;&#20064;&#36127;&#36733;&#22343;&#34913;&#31639;&#27861;&#65292;&#29992;&#20110;&#36719;&#20214;&#23450;&#20041;&#24191;&#22495;&#32593;&#65288;SD-WAN&#65289;&#12290;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#36807;&#31243;&#20013;&#65292;&#23427;&#23558;&#19981;&#23433;&#20840;&#30340;&#21160;&#20316;&#23433;&#20840;&#22320;&#26144;&#23556;&#20026;&#21487;&#34892;&#30340;&#21160;&#20316;&#65292;&#24182;&#24341;&#23548;&#23398;&#20064;&#21521;&#23433;&#20840;&#31574;&#30053;&#21457;&#23637;&#12290;&#25105;&#20204;&#25104;&#21151;&#22312;GPU&#19978;&#23454;&#26045;&#20102;&#35813;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#21152;&#36895;&#35757;&#32451;&#32422;110&#20493;&#65292;&#24182;&#22312;&#20960;&#31186;&#38047;&#20869;&#23454;&#29616;&#20102;&#22312;&#32447;&#31574;&#30053;&#26041;&#27861;&#30340;&#27169;&#22411;&#26356;&#26032;&#65292;&#20351;&#35299;&#20915;&#26041;&#26696;&#21464;&#24471;&#23454;&#29992;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#36817;&#20046;&#26368;&#20248;&#30340;&#26381;&#21153;&#36136;&#37327;&#65288;QoS&#65289;&#24615;&#33021;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning (DRL) algorithms have recently made significant strides in improving network performance. Nonetheless, their practical use is still limited in the absence of safe exploration and safe decision-making. In the context of commercial solutions, reliable and safe-to-operate systems are of paramount importance. Taking this problem into account, we propose a safe learning-based load balancing algorithm for Software Defined-Wide Area Network (SD-WAN), which is empowered by Deep Reinforcement Learning (DRL) combined with a Control Barrier Function (CBF). It safely projects unsafe actions into feasible ones during both training and testing, and it guides learning towards safe policies. We successfully implemented the solution on GPU to accelerate training by approximately 110x times and achieve model updates for on-policy methods within a few seconds, making the solution practical. We show that our approach delivers near-optimal Quality-of-Service (QoS performance in 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#30456;&#20851;&#37327;&#21270;&#22120;&#25913;&#36827;&#20102;MARINA&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21152;&#26435;Hessian&#26041;&#24046;&#36827;&#34892;&#21407;&#22987;&#20998;&#26512;&#65292;&#24182;&#25193;&#23637;&#20102;MARINA&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#26356;&#24191;&#27867;&#30340;&#21387;&#32553;&#22120;&#33539;&#22260;&#12290;</title><link>http://arxiv.org/abs/2401.05518</link><description>&lt;p&gt;
&#30456;&#20851;&#37327;&#21270;&#29992;&#20110;&#26356;&#24555;&#30340;&#38750;&#20984;&#20998;&#24067;&#24335;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Correlated Quantization for Faster Nonconvex Distributed Optimization. (arXiv:2401.05518v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05518
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#30456;&#20851;&#37327;&#21270;&#22120;&#25913;&#36827;&#20102;MARINA&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21152;&#26435;Hessian&#26041;&#24046;&#36827;&#34892;&#21407;&#22987;&#20998;&#26512;&#65292;&#24182;&#25193;&#23637;&#20102;MARINA&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#26356;&#24191;&#27867;&#30340;&#21387;&#32553;&#22120;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#65288;&#38543;&#26426;&#65289;&#21387;&#32553;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#20998;&#24067;&#24335;&#27169;&#22411;&#35757;&#32451;&#30340;&#27599;&#19968;&#36718;&#36890;&#20449;&#20013;&#20943;&#23569;&#20256;&#36755;&#27604;&#29305;&#30340;&#25968;&#37327;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;MARINA&#31639;&#27861;&#65292;&#24182;&#21033;&#29992;&#25552;&#20986;&#30340;&#30456;&#20851;&#37327;&#21270;&#22120;&#23637;&#31034;&#20102;&#23427;&#22312;&#36890;&#20449;&#22797;&#26434;&#24230;&#26041;&#38754;&#20248;&#20110;&#21407;&#22987;&#30340;MARINA&#31639;&#27861;&#21644;Suresh&#31561;&#20154;&#30340;&#20998;&#24067;&#24335;SGD&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantization (Alistarh et al., 2017) is an important (stochastic) compression technique that reduces the volume of transmitted bits during each communication round in distributed model training. Suresh et al. (2022) introduce correlated quantizers and show their advantages over independent counterparts by analyzing distributed SGD communication complexity. We analyze the forefront distributed non-convex optimization algorithm MARINA (Gorbunov et al., 2022) utilizing the proposed correlated quantizers and show that it outperforms the original MARINA and distributed SGD of Suresh et al. (2022) with regard to the communication complexity. We significantly refine the original analysis of MARINA without any additional assumptions using the weighted Hessian variance (Tyurin et al., 2022), and then we expand the theoretical framework of MARINA to accommodate a substantially broader range of potentially correlated and biased compressors, thus dilating the applicability of the method beyond the
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35752;&#35770;&#20102;&#22810;&#26679;&#24615;&#24863;&#30693;&#32858;&#31867;&#38382;&#39064;&#65292;&#22312;&#36873;&#25321;&#32858;&#31867;&#20013;&#24515;&#26102;&#35201;&#32771;&#34385;&#22810;&#20010;&#23646;&#24615;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#32858;&#31867;&#30446;&#26631;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;&#19981;&#21516;&#32858;&#31867;&#30446;&#26631;&#30340;&#21442;&#25968;&#21270;&#36817;&#20284;&#31639;&#27861;&#65292;&#36825;&#20123;&#31639;&#27861;&#22312;&#20445;&#35777;&#32858;&#31867;&#36136;&#37327;&#30340;&#21516;&#26102;&#65292;&#20855;&#26377;&#32039;&#30830;&#30340;&#36817;&#20284;&#27604;&#12290;</title><link>http://arxiv.org/abs/2401.05502</link><description>&lt;p&gt;
&#22810;&#26679;&#24615;&#24863;&#30693;&#32858;&#31867;&#65306;&#35745;&#31639;&#22797;&#26434;&#24615;&#21644;&#36817;&#20284;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Diversity-aware clustering: Computational Complexity and Approximation Algorithms. (arXiv:2401.05502v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05502
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35752;&#35770;&#20102;&#22810;&#26679;&#24615;&#24863;&#30693;&#32858;&#31867;&#38382;&#39064;&#65292;&#22312;&#36873;&#25321;&#32858;&#31867;&#20013;&#24515;&#26102;&#35201;&#32771;&#34385;&#22810;&#20010;&#23646;&#24615;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#32858;&#31867;&#30446;&#26631;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;&#19981;&#21516;&#32858;&#31867;&#30446;&#26631;&#30340;&#21442;&#25968;&#21270;&#36817;&#20284;&#31639;&#27861;&#65292;&#36825;&#20123;&#31639;&#27861;&#22312;&#20445;&#35777;&#32858;&#31867;&#36136;&#37327;&#30340;&#21516;&#26102;&#65292;&#20855;&#26377;&#32039;&#30830;&#30340;&#36817;&#20284;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#26679;&#24615;&#24863;&#30693;&#32858;&#31867;&#38382;&#39064;&#65292;&#20854;&#20013;&#25968;&#25454;&#28857;&#19982;&#22810;&#20010;&#23646;&#24615;&#30456;&#20851;&#32852;&#65292;&#24418;&#25104;&#20132;&#21449;&#30340;&#32452;&#12290;&#32858;&#31867;&#35299;&#20915;&#26041;&#26696;&#38656;&#35201;&#30830;&#20445;&#20174;&#27599;&#20010;&#32452;&#20013;&#36873;&#25321;&#26368;&#23569;&#25968;&#37327;&#30340;&#32858;&#31867;&#20013;&#24515;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#32858;&#31867;&#30446;&#26631;&#65292;&#21487;&#20197;&#26159;$k$-&#20013;&#20301;&#25968;&#65292;$k$-&#22343;&#20540;&#25110;$k$-&#20379;&#24212;&#21830;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#21442;&#25968;&#21270;&#36817;&#20284;&#31639;&#27861;&#65292;&#36817;&#20284;&#27604;&#20998;&#21035;&#20026;$1+\frac{2}{e}$&#65292;$1+\frac{8}{e}$&#21644;$3$&#65292;&#29992;&#20110;&#22810;&#26679;&#24615;&#24863;&#30693;$k$-&#20013;&#20301;&#25968;&#65292;&#22810;&#26679;&#24615;&#24863;&#30693;$k$-&#22343;&#20540;&#21644;&#22810;&#26679;&#24615;&#24863;&#30693;$k$-&#20379;&#24212;&#21830;&#12290;&#36825;&#20123;&#36817;&#20284;&#27604;&#22312;&#20551;&#35774;Gap-ETH&#21644;FPT $\neq$ W[2]&#30340;&#24773;&#20917;&#19979;&#26159;&#32039;&#30830;&#30340;&#12290;&#23545;&#20110;&#20844;&#24179;$k$-&#20013;&#20301;&#25968;&#21644;&#20844;&#24179;$k$-&#22343;&#20540;&#30340;&#19981;&#30456;&#20132;&#24037;&#21378;&#32452;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21442;&#25968;&#21270;&#36817;&#20284;&#31639;&#27861;&#65292;&#36817;&#20284;&#27604;&#20998;&#21035;&#20026;$1+\frac{2}{e}$&#21644;$1+\frac{8}{e}$&#12290;&#23545;&#20110;&#20855;&#26377;&#19981;&#30456;&#20132;&#24037;&#21378;&#32452;&#30340;&#20844;&#24179;$k$-&#20379;&#24212;&#21830;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#36817;&#20284;&#31639;&#27861;&#65292;&#22240;&#23376;&#20026;$3$&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we study diversity-aware clustering problems where the data points are associated with multiple attributes resulting in intersecting groups. A clustering solution need to ensure that a minimum number of cluster centers are chosen from each group while simultaneously minimizing the clustering objective, which can be either $k$-median, $k$-means or $k$-supplier. We present parameterized approximation algorithms with approximation ratios $1+ \frac{2}{e}$, $1+\frac{8}{e}$ and $3$ for diversity-aware $k$-median, diversity-aware $k$-means and diversity-aware $k$-supplier, respectively. The approximation ratios are tight assuming Gap-ETH and FPT $\neq$ W[2]. For fair $k$-median and fair $k$-means with disjoint faicility groups, we present parameterized approximation algorithm with approximation ratios $1+\frac{2}{e}$ and $1+\frac{8}{e}$, respectively. For fair $k$-supplier with disjoint facility groups, we present a polynomial-time approximation algorithm with factor $3$, improv
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36882;&#24402;&#30340;&#32858;&#31867;&#26041;&#26696;&#65292;&#29992;&#20110;&#22788;&#29702;&#20855;&#26377;&#27979;&#37327;&#35823;&#24046;&#30340;&#23454;&#39564;&#25968;&#25454;&#65292;&#22312;&#22320;&#29702;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30456;&#27604;&#20110;&#20256;&#32479;&#26041;&#27861;&#32473;&#20986;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.05479</link><description>&lt;p&gt;
&#32858;&#31867;&#30340;&#36882;&#24402;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
The recursive scheme of clustering. (arXiv:2401.05479v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05479
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36882;&#24402;&#30340;&#32858;&#31867;&#26041;&#26696;&#65292;&#29992;&#20110;&#22788;&#29702;&#20855;&#26377;&#27979;&#37327;&#35823;&#24046;&#30340;&#23454;&#39564;&#25968;&#25454;&#65292;&#22312;&#22320;&#29702;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30456;&#27604;&#20110;&#20256;&#32479;&#26041;&#27861;&#32473;&#20986;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#32858;&#31867;&#38382;&#39064;&#26159;&#25968;&#25454;&#20998;&#26512;&#20013;&#26368;&#37325;&#35201;&#30340;&#38382;&#39064;&#20043;&#19968;&#12290;&#24403;&#22788;&#29702;&#20855;&#26377;&#27979;&#37327;&#19981;&#30830;&#23450;&#24615;&#21644;&#35823;&#24046;&#30340;&#23454;&#39564;&#25968;&#25454;&#26102;&#65292;&#36825;&#21487;&#33021;&#26159;&#26377;&#38382;&#39064;&#30340;&#12290;&#25105;&#20204;&#30340;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36882;&#24402;&#26041;&#26696;&#65292;&#29992;&#20110;&#32858;&#31867;&#22312;&#22320;&#29702;&#65288;&#27668;&#20505;&#65289;&#23454;&#39564;&#20013;&#33719;&#24471;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#26032;&#26041;&#27861;&#36827;&#34892;&#30340;&#32858;&#31867;&#19982;&#19987;&#23478;&#35780;&#20272;&#30456;&#27604;&#32473;&#20986;&#26356;&#21487;&#25509;&#21463;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The problem of data clustering is one of the most important in data analysis. It can be problematic when dealing with experimental data characterized by measurement uncertainties and errors. Our paper proposes a recursive scheme for clustering data obtained in geographical (climatological) experiments. The discussion of results obtained by k-means and SOM methods with the developed recursive procedure is presented. We show that the clustering using the new approach gives more acceptable results when compared to experts assessments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36328;&#32593;&#32476;&#33410;&#28857;&#20998;&#31867;&#26041;&#27861;OTGCN&#65292;&#21033;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;&#21644;&#26368;&#20248;&#36755;&#36816;&#31574;&#30053;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#25968;&#25454;&#37319;&#38598;&#22320;&#28857;&#30340;&#26679;&#26412;&#20043;&#38388;&#32416;&#27491;&#39046;&#22495;&#28418;&#31227;&#65292;&#24182;&#19988;&#22312;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#30340;&#20998;&#31867;&#19978;&#21462;&#24471;&#20102;&#26377;&#25928;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.05478</link><description>&lt;p&gt;
&#36328;&#32593;&#32476;&#33410;&#28857;&#20998;&#31867;&#29992;&#20110;&#33258;&#38381;&#30151;&#26816;&#27979;&#30340;&#20154;&#32676;&#22270;
&lt;/p&gt;
&lt;p&gt;
Population Graph Cross-Network Node Classification for Autism Detection Across Sample Groups. (arXiv:2401.05478v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05478
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36328;&#32593;&#32476;&#33410;&#28857;&#20998;&#31867;&#26041;&#27861;OTGCN&#65292;&#21033;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;&#21644;&#26368;&#20248;&#36755;&#36816;&#31574;&#30053;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#25968;&#25454;&#37319;&#38598;&#22320;&#28857;&#30340;&#26679;&#26412;&#20043;&#38388;&#32416;&#27491;&#39046;&#22495;&#28418;&#31227;&#65292;&#24182;&#19988;&#22312;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#30340;&#20998;&#31867;&#19978;&#21462;&#24471;&#20102;&#26377;&#25928;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26159;&#23558;&#22270;&#20687;&#21644;&#38750;&#22270;&#20687;&#21307;&#30103;&#20449;&#24687;&#32467;&#21512;&#36215;&#26469;&#36827;&#34892;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#36328;&#32593;&#32476;&#33410;&#28857;&#20998;&#31867;&#25193;&#23637;&#20102;GNN&#25216;&#26415;&#20197;&#35299;&#20915;&#39046;&#22495;&#28418;&#31227;&#38382;&#39064;&#65292;&#20801;&#35768;&#22312;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#32593;&#32476;&#19978;&#36827;&#34892;&#33410;&#28857;&#20998;&#31867;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#22823;&#32780;&#26032;&#39062;&#30340;&#36328;&#32593;&#32476;&#33410;&#28857;&#20998;&#31867;&#26041;&#27861;OTGCN&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#27010;&#24565;&#65292;&#21516;&#26102;&#24212;&#29992;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#30340;&#31574;&#30053;&#26469;&#32416;&#27491;&#26469;&#33258;&#19981;&#21516;&#25968;&#25454;&#37319;&#38598;&#22320;&#28857;&#26679;&#26412;&#20043;&#38388;&#21487;&#33021;&#21457;&#29983;&#30340;&#39046;&#22495;&#28418;&#31227;&#12290;&#36825;&#31181;&#32508;&#21512;&#26041;&#27861;&#20026;&#19981;&#21516;&#20301;&#32622;&#21644;&#35774;&#22791;&#25910;&#38598;&#21040;&#30340;&#22810;&#31181;&#19981;&#21516;&#24418;&#24335;&#30340;&#25968;&#25454;&#30340;&#22330;&#26223;&#25552;&#20379;&#20102;&#23454;&#38469;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#22270;&#20687;&#21644;&#38750;&#22270;&#20687;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#20998;&#31867;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#21463;&#35797;&#32773;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNN) are a powerful tool for combining imaging and non-imaging medical information for node classification tasks. Cross-network node classification extends GNN techniques to account for domain drift, allowing for node classification on an unlabeled target network. In this paper we present OTGCN, a powerful, novel approach to cross-network node classification. This approach leans on concepts from graph convolutional networks to harness insights from graph data structures while simultaneously applying strategies rooted in optimal transport to correct for the domain drift that can occur between samples from different data collection sites. This blended approach provides a practical solution for scenarios with many distinct forms of data collected across different locations and equipment. We demonstrate the effectiveness of this approach at classifying Autism Spectrum Disorder subjects using a blend of imaging and non-imaging data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#39038;&#20102;&#21487;&#31359;&#25140;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#39046;&#22495;&#30340;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#65292;&#24182;&#24635;&#32467;&#20102;&#21508;&#20010;&#30740;&#31350;&#25152;&#37319;&#29992;&#30340;&#35757;&#32451;&#31243;&#24207;&#12290;&#30740;&#31350;&#21457;&#29616;&#23384;&#22312;&#32570;&#20047;&#35814;&#32454;&#35757;&#32451;&#21327;&#35758;&#30340;&#36235;&#21183;&#65292;&#21516;&#26102;&#21033;&#29992;&#25511;&#21046;&#21464;&#37327;&#26041;&#27861;&#35780;&#20272;&#20102;&#20851;&#38190;&#21487;&#35843;&#32452;&#20214;&#23545;&#36328;&#20027;&#20307;&#27867;&#21270;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.05477</link><description>&lt;p&gt;
&#20026;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#27169;&#22411;&#26631;&#20934;&#21270;&#35757;&#32451;&#27969;&#31243;&#65306;&#21487;&#35843;&#22240;&#32032;&#30340;&#20840;&#38754;&#22238;&#39038;
&lt;/p&gt;
&lt;p&gt;
Standardizing Your Training Process for Human Activity Recognition Models: A Comprehensive Review in the Tunable Factors. (arXiv:2401.05477v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05477
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#20102;&#21487;&#31359;&#25140;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#39046;&#22495;&#30340;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#65292;&#24182;&#24635;&#32467;&#20102;&#21508;&#20010;&#30740;&#31350;&#25152;&#37319;&#29992;&#30340;&#35757;&#32451;&#31243;&#24207;&#12290;&#30740;&#31350;&#21457;&#29616;&#23384;&#22312;&#32570;&#20047;&#35814;&#32454;&#35757;&#32451;&#21327;&#35758;&#30340;&#36235;&#21183;&#65292;&#21516;&#26102;&#21033;&#29992;&#25511;&#21046;&#21464;&#37327;&#26041;&#27861;&#35780;&#20272;&#20102;&#20851;&#38190;&#21487;&#35843;&#32452;&#20214;&#23545;&#36328;&#20027;&#20307;&#27867;&#21270;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#25104;&#20026;&#19968;&#20010;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#23548;&#33268;&#20102;&#22312;&#21487;&#31359;&#25140;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#65288;WHAR&#65289;&#39046;&#22495;&#30340;&#24212;&#29992;&#30740;&#31350;&#28608;&#22686;&#12290;&#23613;&#31649;&#21457;&#23637;&#36805;&#36895;&#65292;&#20294;&#20154;&#20204;&#23545;&#23454;&#39564;&#27169;&#22411;&#35757;&#32451;&#20013;&#20351;&#29992;&#30340;&#26631;&#20934;&#21270;&#21644;&#19968;&#33268;&#24615;&#31243;&#24207;&#30340;&#32570;&#20047;&#25285;&#24551;&#65292;&#21487;&#33021;&#20250;&#24433;&#21709;&#30740;&#31350;&#32467;&#26524;&#30340;&#21487;&#37325;&#22797;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;WHAR&#39046;&#22495;&#30340;&#24403;&#20195;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#36827;&#34892;&#20102;&#20840;&#38754;&#22238;&#39038;&#65292;&#24182;&#25972;&#29702;&#20102;&#21508;&#31181;&#30740;&#31350;&#20013;&#25152;&#37319;&#29992;&#30340;&#35757;&#32451;&#31243;&#24207;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#19968;&#20010;&#20027;&#35201;&#36235;&#21183;&#26159;&#32570;&#20047;&#27169;&#22411;&#35757;&#32451;&#21327;&#35758;&#25552;&#20379;&#30340;&#32454;&#33410;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#26356;&#28165;&#26970;&#22320;&#20102;&#35299;&#32570;&#22833;&#25551;&#36848;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#21033;&#29992;&#25511;&#21046;&#21464;&#37327;&#26041;&#27861;&#35780;&#20272;&#20102;&#20851;&#38190;&#21487;&#35843;&#32452;&#20214;&#65288;&#20363;&#22914;&#20248;&#21270;&#25216;&#26415;&#21644;&#25552;&#21069;&#20572;&#27490;&#20934;&#21017;&#65289;&#23545;&#36328;&#20027;&#20307;&#27867;&#21270;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, deep learning has emerged as a potent tool across a multitude of domains, leading to a surge in research pertaining to its application in the wearable human activity recognition (WHAR) domain. Despite the rapid development, concerns have been raised about the lack of standardization and consistency in the procedures used for experimental model training, which may affect the reproducibility and reliability of research results. In this paper, we provide an exhaustive review of contemporary deep learning research in the field of WHAR and collate information pertaining to the training procedure employed in various studies. Our findings suggest that a major trend is the lack of detail provided by model training protocols. Besides, to gain a clearer understanding of the impact of missing descriptions, we utilize a control variables approach to assess the impact of key tunable components (e.g., optimization techniques and early stopping criteria) on the inter-subject generali
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#24314;&#27169;&#29289;&#31181;&#20998;&#24067;&#30340;&#26041;&#27861;&#65292;&#20197;&#39044;&#27979;&#26893;&#29289;&#28781;&#32477;&#39118;&#38505;&#21644;&#35780;&#20272;&#27668;&#20505;&#21464;&#21270;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#23545;&#29289;&#31181;&#30340;IUCN&#29366;&#24577;&#36827;&#34892;&#20934;&#30830;&#30340;&#20998;&#31867;&#65292;&#20026;&#20840;&#29699;&#29983;&#29289;&#22810;&#26679;&#24615;&#26694;&#26550;&#30340;&#21046;&#23450;&#25552;&#20379;&#31185;&#23398;&#20381;&#25454;&#12290;</title><link>http://arxiv.org/abs/2401.05470</link><description>&lt;p&gt;
&#29992;&#28145;&#24230;&#23398;&#20064;&#24314;&#27169;&#29289;&#31181;&#20998;&#24067;&#20197;&#39044;&#27979;&#26893;&#29289;&#28781;&#32477;&#39118;&#38505;&#21644;&#35780;&#20272;&#27668;&#20505;&#21464;&#21270;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Modelling Species Distributions with Deep Learning to Predict Plant Extinction Risk and Assess Climate Change Impacts. (arXiv:2401.05470v1 [q-bio.PE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05470
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#24314;&#27169;&#29289;&#31181;&#20998;&#24067;&#30340;&#26041;&#27861;&#65292;&#20197;&#39044;&#27979;&#26893;&#29289;&#28781;&#32477;&#39118;&#38505;&#21644;&#35780;&#20272;&#27668;&#20505;&#21464;&#21270;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#23545;&#29289;&#31181;&#30340;IUCN&#29366;&#24577;&#36827;&#34892;&#20934;&#30830;&#30340;&#20998;&#31867;&#65292;&#20026;&#20840;&#29699;&#29983;&#29289;&#22810;&#26679;&#24615;&#26694;&#26550;&#30340;&#21046;&#23450;&#25552;&#20379;&#31185;&#23398;&#20381;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;2020&#24180;&#30340;&#20840;&#29699;&#29983;&#29289;&#22810;&#26679;&#24615;&#26694;&#26550;&#38656;&#35201;&#26377;&#38596;&#24515;&#21187;&#21187;&#30340;&#12289;&#20197;&#30740;&#31350;&#20026;&#22522;&#30784;&#30340;&#30446;&#26631;&#12290;&#20272;&#35745;&#30001;&#20110;&#27668;&#20505;&#21464;&#21270;&#23548;&#33268;&#30340;&#21152;&#36895;&#28781;&#32477;&#39118;&#38505;&#33267;&#20851;&#37325;&#35201;&#12290;&#22269;&#38469;&#33258;&#28982;&#20445;&#25252;&#32852;&#30431;(IUCN)&#34913;&#37327;&#29289;&#31181;&#30340;&#28781;&#32477;&#39118;&#38505;&#12290;&#24050;&#32463;&#24320;&#21457;&#20986;&#20102;&#33258;&#21160;&#30340;&#26041;&#27861;&#26469;&#25552;&#20379;&#23545;&#26410;&#35780;&#20272;&#31181;&#32676;&#30340;IUCN&#29366;&#24577;&#30340;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20462;&#27491;&#26041;&#27861;&#22522;&#20110;&#24403;&#21069;&#29289;&#31181;&#29305;&#24449;&#65292;&#20027;&#35201;&#26159;&#22320;&#29702;&#29305;&#24449;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#26080;&#27861;&#22312;&#26410;&#26469;&#30340;&#39044;&#27979;&#20013;&#20351;&#29992;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#29289;&#31181;&#20998;&#24067;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#23545;IUCN&#29289;&#31181;&#29366;&#24577;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20381;&#36182;&#20110;&#25429;&#25417;&#29289;&#31181;&#29615;&#22659;&#20559;&#22909;&#30340;&#28789;&#27963;SDM&#29305;&#24449;&#30340;&#21516;&#26102;&#65292;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;&#20132;&#21449;&#39564;&#35777;&#24471;&#20986;&#20102;0.61&#30340;&#29366;&#24577;&#20998;&#31867;&#20934;&#30830;&#24230;&#21644;0.78&#30340;&#20108;&#20803;&#20998;&#31867;&#20934;&#30830;&#24230;&#30340;&#24179;&#22343;&#20540;&#12290;&#27668;&#20505;&#21464;&#21270;&#23558;&#37325;&#22609;&#26410;&#26469;&#30340;&#29289;&#31181;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
The post-2020 global biodiversity framework needs ambitious, research-based targets. Estimating the accelerated extinction risk due to climate change is critical. The International Union for Conservation of Nature (IUCN) measures the extinction risk of species. Automatic methods have been developed to provide information on the IUCN status of under-assessed taxa. However, these compensatory methods are based on current species characteristics, mainly geographical, which precludes their use in future projections. Here, we evaluate a novel method for classifying the IUCN status of species benefiting from the generalisation power of species distribution models based on deep learning. Our method matches state-of-the-art classification performance while relying on flexible SDM-based features that capture species' environmental preferences. Cross-validation yields average accuracies of 0.61 for status classification and 0.78 for binary classification. Climate change will reshape future speci
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#20174;&#26234;&#33021;&#25163;&#34920;&#30340;PPG&#12289;&#21152;&#36895;&#24230;&#35745;&#21644;&#38464;&#34746;&#20202;&#20449;&#21495;&#20013;&#20934;&#30830;&#25552;&#21462;&#21628;&#21560;&#39057;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.05469</link><description>&lt;p&gt;
&#22522;&#20110;&#40065;&#26834;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26234;&#33021;&#25163;&#34920; PPG &#21644; IMU &#30340;&#21628;&#21560;&#39057;&#29575;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Robust CNN-based Respiration Rate Estimation for Smartwatch PPG and IMU. (arXiv:2401.05469v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05469
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#20174;&#26234;&#33021;&#25163;&#34920;&#30340;PPG&#12289;&#21152;&#36895;&#24230;&#35745;&#21644;&#38464;&#34746;&#20202;&#20449;&#21495;&#20013;&#20934;&#30830;&#25552;&#21462;&#21628;&#21560;&#39057;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21628;&#21560;&#39057;&#29575;&#65288;RR&#65289;&#26159;&#21508;&#31181;&#21307;&#30103;&#24773;&#20917;&#30340;&#25351;&#26631;&#65292;&#22914;&#24515;&#34880;&#31649;&#30142;&#30149;&#21644;&#30561;&#30496;&#38556;&#30861;&#12290;&#30446;&#21069;&#30340;RR&#20272;&#35745;&#26041;&#27861;&#22823;&#22810;&#35774;&#35745;&#29992;&#20110;&#25351;&#30002;&#22522;&#30784;PPG&#22312;&#38745;&#24577;&#24773;&#20917;&#19979;&#25910;&#38598;&#30340;&#25968;&#25454;&#65288;&#22914;&#22312;&#21307;&#38498;&#65289;&#12290;&#19982;&#25351;&#30002;&#22522;&#30784;&#30340;PPG&#20449;&#21495;&#19981;&#21516;&#65292;&#22522;&#20110;&#25163;&#33109;&#30340;PPG&#26356;&#23481;&#26131;&#21463;&#21040;&#22122;&#38899;&#30340;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#22312;&#20302;&#39057;&#33539;&#22260;&#20869;&#65292;&#20854;&#20013;&#21253;&#25324;&#21628;&#21560;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#22312;&#33258;&#30001;&#29983;&#27963;&#26465;&#20214;&#19979;&#25910;&#38598;&#20174;&#25163;&#33109;&#21306;&#22495;&#33719;&#21462;&#30340;PPG&#25968;&#25454;&#26102;&#65292;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#38590;&#20197;&#20934;&#30830;&#25552;&#21462;RR&#12290;&#26234;&#33021;&#25163;&#34920;&#30340;&#26222;&#21450;&#65292;&#37197;&#22791;&#20102;&#21253;&#25324;PPG&#12289;&#21152;&#36895;&#24230;&#35745;&#21644;&#38464;&#34746;&#20202;&#22312;&#20869;&#30340;&#21508;&#31181;&#20256;&#24863;&#22120;&#65292;&#36843;&#20351;&#20154;&#20204;&#38656;&#35201;&#19968;&#31181;&#40065;&#26834;&#30340;RR&#20272;&#35745;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#20174;&#26234;&#33021;&#25163;&#34920;&#25429;&#33719;&#30340;PPG&#12289;&#21152;&#36895;&#24230;&#35745;&#21644;&#38464;&#34746;&#20202;&#20449;&#21495;&#20013;&#25552;&#21462;RR&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#25193;&#24352;&#27531;&#20313;&#24341;&#20837;&#27169;&#22359;&#21644;&#19968;&#32500;&#21367;&#31215;&#65292;&#20174;&#20013;&#25552;&#21462;&#26102;&#38388;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Respiratory rate (RR) serves as an indicator of various medical conditions, such as cardiovascular diseases and sleep disorders. These RR estimation methods were mostly designed for finger-based PPG collected from subjects in stationary situations (e.g., in hospitals). In contrast to finger-based PPG signals, wrist-based PPG are more susceptible to noise, particularly in their low frequency range, which includes respiratory information. Therefore, the existing methods struggle to accurately extract RR when PPG data are collected from wrist area under free-living conditions. The increasing popularity of smartwatches, equipped with various sensors including PPG, has prompted the need for a robust RR estimation method. In this paper, we propose a convolutional neural network-based approach to extract RR from PPG, accelerometer, and gyroscope signals captured via smartwatches. Our method, including a dilated residual inception module and 1D convolutions, extract the temporal information fr
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#38382;&#39064;&#8212;&#8212;&#26032;&#33410;&#28857;&#39044;&#27979;&#65292;&#21363;&#20174;&#20197;&#21069;&#19982;&#22270;&#19981;&#30456;&#36830;&#30340;&#23396;&#31435;&#33410;&#28857;&#20013;&#39044;&#27979;&#25152;&#26377;&#36830;&#25509;&#12290;&#36890;&#36807;&#22522;&#20110;&#28145;&#24230;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26550;&#26500;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#21487;&#20197;&#35299;&#20915;&#36825;&#19968;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.05468</link><description>&lt;p&gt;
&#22270;&#25366;&#25496;&#20013;&#24341;&#20837;&#30340;&#26032;&#33410;&#28857;&#39044;&#27979;&#65306;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#23396;&#31435;&#33410;&#28857;&#30340;&#25152;&#26377;&#36830;&#25509;
&lt;/p&gt;
&lt;p&gt;
Introducing New Node Prediction in Graph Mining: Predicting All Links from Isolated Nodes with Graph Neural Networks. (arXiv:2401.05468v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05468
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#38382;&#39064;&#8212;&#8212;&#26032;&#33410;&#28857;&#39044;&#27979;&#65292;&#21363;&#20174;&#20197;&#21069;&#19982;&#22270;&#19981;&#30456;&#36830;&#30340;&#23396;&#31435;&#33410;&#28857;&#20013;&#39044;&#27979;&#25152;&#26377;&#36830;&#25509;&#12290;&#36890;&#36807;&#22522;&#20110;&#28145;&#24230;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26550;&#26500;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#21487;&#20197;&#35299;&#20915;&#36825;&#19968;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22270;&#25366;&#25496;&#21644;&#31038;&#20132;&#32593;&#32476;&#20998;&#26512;&#39046;&#22495;&#20013;&#30340;&#19968;&#20010;&#26032;&#38382;&#39064;&#65292;&#21363;&#26032;&#33410;&#28857;&#39044;&#27979;&#12290;&#20174;&#25216;&#26415;&#19978;&#35762;&#65292;&#36825;&#20010;&#20219;&#21153;&#21487;&#34987;&#24402;&#31867;&#20026;&#38646;&#26679;&#26412;&#30340;&#22270;&#22806;&#25152;&#26377;&#36830;&#25509;&#39044;&#27979;&#12290;&#36825;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#26088;&#22312;&#39044;&#27979;&#19968;&#20010;&#20808;&#21069;&#19982;&#22270;&#19981;&#30456;&#36830;&#12289;&#23396;&#31435;&#19988;&#26410;&#35266;&#27979;&#21040;&#30340;&#26032;&#33410;&#28857;&#30340;&#25152;&#26377;&#36830;&#25509;&#12290;&#19982;&#32463;&#20856;&#30340;&#36830;&#25509;&#39044;&#27979;&#26041;&#27861;&#65288;&#21253;&#25324;&#23569;&#26679;&#26412;&#30340;&#22270;&#22806;&#36830;&#25509;&#39044;&#27979;&#65289;&#19981;&#21516;&#65292;&#36825;&#20010;&#38382;&#39064;&#26377;&#20004;&#20010;&#20851;&#38190;&#30340;&#19981;&#21516;&#20043;&#22788;&#65306;&#65288;1&#65289;&#26032;&#33410;&#28857;&#27809;&#26377;&#29616;&#26377;&#30340;&#36830;&#25509;&#21487;&#20379;&#25552;&#21462;&#27169;&#24335;&#29992;&#20110;&#26032;&#30340;&#39044;&#27979;&#65307;&#65288;2&#65289;&#30446;&#26631;&#26159;&#39044;&#27979;&#19981;&#20165;&#20165;&#26159;&#19968;&#20010;&#65292;&#32780;&#26159;&#36825;&#20010;&#26032;&#33410;&#28857;&#30340;&#25152;&#26377;&#36830;&#25509;&#65292;&#25110;&#32773;&#33267;&#23569;&#20854;&#20013;&#30340;&#19968;&#20010;&#26174;&#33879;&#37096;&#20998;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#22522;&#20110;&#28145;&#24230;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26550;&#26500;&#21487;&#20197;&#23398;&#20064;&#35299;&#20915;&#36825;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#22312;&#19968;&#20010;&#25991;&#29486;&#24341;&#29992;&#32593;&#32476;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a new problem in the field of graph mining and social network analysis called new node prediction. More technically, the task can be categorized as zero-shot out-of-graph all-links prediction. This challenging problem aims to predict all links from a new, isolated, and unobserved node that was previously disconnected from the graph. Unlike classic approaches to link prediction (including few-shot out-of-graph link prediction), this problem presents two key differences: (1) the new node has no existing links from which to extract patterns for new predictions; and (2) the goal is to predict not just one, but all the links of this new node, or at least a significant part of them. Experiments demonstrate that an architecture based on Deep Graph Neural Networks can learn to solve this challenging problem in a bibliographic citation network.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#25945;&#23398;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#36845;&#20195;&#26426;&#22120;&#25945;&#23398;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;&#26367;&#20195;&#27169;&#22411;&#65292;&#22686;&#24378;&#20102;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#38646;&#26679;&#26412;&#23398;&#20064;&#22120;&#30340;&#27169;&#22359;&#21270;AI&#20195;&#29702;&#30340;&#40065;&#26834;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.05467</link><description>&lt;p&gt;
&#22522;&#20110;&#38646;&#26679;&#26412;&#23398;&#20064;&#30340;&#27169;&#22359;&#21270;AI&#20195;&#29702;&#30340;&#26426;&#22120;&#25945;&#23398;
&lt;/p&gt;
&lt;p&gt;
Machine Teaching for Building Modular AI Agents based on Zero-shot Learners. (arXiv:2401.05467v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05467
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#25945;&#23398;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#36845;&#20195;&#26426;&#22120;&#25945;&#23398;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;&#26367;&#20195;&#27169;&#22411;&#65292;&#22686;&#24378;&#20102;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#38646;&#26679;&#26412;&#23398;&#20064;&#22120;&#30340;&#27169;&#22359;&#21270;AI&#20195;&#29702;&#30340;&#40065;&#26834;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#23548;&#33268;&#20102;&#35768;&#22810;&#27169;&#22359;&#21270;AI&#20195;&#29702;&#30340;&#21019;&#24314;&#12290;&#36825;&#20123;&#20195;&#29702;&#20351;&#29992;LLMs&#20316;&#20026;&#38646;&#26679;&#26412;&#23398;&#20064;&#22120;&#65292;&#22312;&#20154;&#31867;&#29992;&#25143;&#35774;&#23450;&#30340;&#22797;&#26434;&#20219;&#21153;&#20013;&#25191;&#34892;&#23376;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#22686;&#24378;&#21033;&#29992;LLMs&#20316;&#20026;&#38646;&#26679;&#26412;&#23398;&#20064;&#22120;&#30340;&#27169;&#22359;&#21270;AI&#20195;&#29702;&#30340;&#40065;&#26834;&#24615;&#21644;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#36845;&#20195;&#26426;&#22120;&#25945;&#23398;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#22312;&#26377;&#38480;&#30340;&#20154;&#31867;&#21453;&#39304;&#19979;&#36880;&#28176;&#25945;&#23548;AI&#20195;&#29702;&#30340;&#39640;&#25928;&#26041;&#24335;&#65292;&#35299;&#20915;&#20102;&#38646;&#26679;&#26412;&#23398;&#20064;&#36136;&#37327;&#38480;&#21046;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#20027;&#24352;&#21033;&#29992;&#21021;&#22987;&#37096;&#32626;&#30340;&#25968;&#25454;&#36861;&#36394;&#20197;&#21450;&#38646;&#26679;&#26412;&#23398;&#20064;&#22120;&#30340;&#36755;&#20986;&#25110;&#27880;&#37322;&#26469;&#35757;&#32451;&#26356;&#23567;&#19988;&#20219;&#21153;&#29305;&#23450;&#30340;&#26367;&#20195;&#27169;&#22411;&#65292;&#21487;&#20197;&#20943;&#23569;&#32463;&#27982;&#25104;&#26412;&#21644;&#29615;&#22659;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#26426;&#22120;&#25945;&#23398;&#36807;&#31243;&#21033;&#29992;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#26469;&#32416;&#27491;&#39640;&#27010;&#29575;&#35823;&#26631;&#27880;&#30340;&#31034;&#20363;&#12290;&#22312;&#19977;&#20010;&#24120;&#35265;&#23545;&#35805;AI&#20195;&#29702;&#20219;&#21153;&#19978;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#25509;&#36817;&#29702;&#24819;&#24615;&#33021;&#21487;&#20197;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent advances in large language models (LLMs) have led to the creation of many modular AI agents. These agents employ LLMs as zero-shot learners to perform sub-tasks in order to solve complex tasks set forth by human users. We propose an approach to enhance the robustness and performance of modular AI agents that utilize LLMs as zero-shot learners. Our iterative machine teaching method offers an efficient way to teach AI agents over time with limited human feedback, addressing the limit posed by the quality of zero-shot learning. We advocate leveraging the data traces from initial deployments and outputs or annotations from the zero-shot learners to train smaller and task-specific substitute models which can reduce both the monetary costs and environmental impact. Our machine teaching process avails human expertise to correct examples with a high likelihood of misannotations. Results on three tasks, common to conversational AI agents, show that close-to-oracle performance can be 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#26500;&#24314;&#20102;&#19968;&#20010;&#20154;&#31867;&#21644;&#31070;&#32463;&#32593;&#32476;&#20043;&#38388;&#30340;&#21452;&#21521;&#30693;&#35782;&#20132;&#20114;&#30028;&#38754;&#65292;&#20351;&#29992;&#35270;&#35273;&#27010;&#24565;&#21644;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#31995;&#20316;&#20026;&#8220;&#35821;&#35328;&#8221;&#65292;&#20197;&#23454;&#29616;&#20154;&#31867;&#21644;&#31070;&#32463;&#32593;&#32476;&#20043;&#38388;&#30340;&#30693;&#35782;&#20132;&#27969;&#12290;&#35813;&#30028;&#38754;&#21487;&#20197;&#20351;&#31070;&#32463;&#32593;&#32476;&#21521;&#20154;&#31867;&#25552;&#20379;&#30452;&#35266;&#30340;&#25512;&#29702;&#35299;&#37322;&#65292;&#21516;&#26102;&#20154;&#31867;&#21487;&#20197;&#20462;&#25913;&#20854;&#20013;&#30340;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2401.05461</link><description>&lt;p&gt;
&#20154;&#31867;&#19982;&#31070;&#32463;&#32593;&#32476;&#20043;&#38388;&#30340;&#21452;&#21521;&#30693;&#35782;&#20132;&#20114;&#30028;&#38754;
&lt;/p&gt;
&lt;p&gt;
The two-way knowledge interaction interface between humans and neural networks. (arXiv:2401.05461v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05461
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#26500;&#24314;&#20102;&#19968;&#20010;&#20154;&#31867;&#21644;&#31070;&#32463;&#32593;&#32476;&#20043;&#38388;&#30340;&#21452;&#21521;&#30693;&#35782;&#20132;&#20114;&#30028;&#38754;&#65292;&#20351;&#29992;&#35270;&#35273;&#27010;&#24565;&#21644;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#31995;&#20316;&#20026;&#8220;&#35821;&#35328;&#8221;&#65292;&#20197;&#23454;&#29616;&#20154;&#31867;&#21644;&#31070;&#32463;&#32593;&#32476;&#20043;&#38388;&#30340;&#30693;&#35782;&#20132;&#27969;&#12290;&#35813;&#30028;&#38754;&#21487;&#20197;&#20351;&#31070;&#32463;&#32593;&#32476;&#21521;&#20154;&#31867;&#25552;&#20379;&#30452;&#35266;&#30340;&#25512;&#29702;&#35299;&#37322;&#65292;&#21516;&#26102;&#20154;&#31867;&#21487;&#20197;&#20462;&#25913;&#20854;&#20013;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#24050;&#32463;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#20010;&#39046;&#22495;&#65292;&#24182;&#19988;&#36890;&#24120;&#20248;&#20110;&#20154;&#31867;&#65292;&#20294;&#23427;&#20204;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#20173;&#28982;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65292;&#20154;&#31867;&#26080;&#27861;&#30452;&#35266;&#22320;&#29702;&#35299;NN&#30340;&#20915;&#31574;&#36923;&#36753;&#12290;&#36825;&#20063;&#22952;&#30861;&#20102;&#20154;&#31867;&#19982;NN&#20043;&#38388;&#30340;&#30693;&#35782;&#20132;&#20114;&#65292;&#38459;&#27490;&#20102;&#20154;&#20204;&#22312;NN&#30340;&#20915;&#31574;&#20986;&#38169;&#26102;&#30452;&#25509;&#21442;&#19982;&#32473;&#20104;&#25351;&#23548;&#12290;&#34429;&#28982;&#26368;&#36817;&#22312;&#21487;&#35299;&#37322;AI&#26041;&#38754;&#30340;&#30740;&#31350;&#24050;&#32463;&#20174;&#21508;&#20010;&#35282;&#24230;&#23454;&#29616;&#20102;NN&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#20294;&#23578;&#26410;&#25552;&#20379;&#26377;&#25928;&#30340;&#20154;&#31867;&#21644;NN&#20043;&#38388;&#30340;&#30693;&#35782;&#20132;&#27969;&#26041;&#27861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21452;&#21521;&#20132;&#20114;&#30028;&#38754;&#65292;&#23427;&#23558;&#35270;&#35273;&#27010;&#24565;&#21450;&#20854;&#20851;&#31995;&#30340;&#32467;&#26500;&#21270;&#34920;&#31034;&#20316;&#20026;&#20154;&#31867;&#21644;NN&#20043;&#38388;&#30340;&#30693;&#35782;&#20132;&#27969;&#30340;&#8220;&#35821;&#35328;&#8221;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;NN&#22522;&#20110;&#31867;&#21035;&#29305;&#23450;&#32467;&#26500;&#27010;&#24565;&#22270;&#65288;C-SCG&#65289;&#21521;&#20154;&#31867;&#25552;&#20379;&#30452;&#35266;&#30340;&#25512;&#29702;&#35299;&#37322;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20154;&#31867;&#21487;&#20197;&#20462;&#25913;C-SCG&#20013;&#23384;&#22312;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite neural networks (NN) have been widely applied in various fields and generally outperforms humans, they still lack interpretability to a certain extent, and humans are unable to intuitively understand the decision logic of NN. This also hinders the knowledge interaction between humans and NN, preventing humans from getting involved to give direct guidance when NN's decisions go wrong. While recent research in explainable AI has achieved interpretability of NN from various perspectives, it has not yet provided effective methods for knowledge exchange between humans and NN. To address this problem, we constructed a two-way interaction interface that uses structured representations of visual concepts and their relationships as the "language" for knowledge exchange between humans and NN. Specifically, NN provide intuitive reasoning explanations to humans based on the class-specific structural concepts graph (C-SCG). On the other hand, humans can modify the biases present in the C-SC
&lt;/p&gt;</description></item><item><title>CoLafier&#26159;&#19968;&#31181;&#20351;&#29992;&#23616;&#37096;&#20869;&#22312;&#32500;&#24230;&#65288;LID&#65289;&#36827;&#34892;&#24102;&#26377;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#23427;&#36890;&#36807;&#21033;&#29992;LID-dis&#21644;LID-gen&#20004;&#20010;&#23376;&#32593;&#32476;&#65292;&#20854;&#20013;LID-dis&#20351;&#29992;&#26679;&#26412;&#30340;&#29305;&#24449;&#21644;&#26631;&#31614;&#26469;&#39044;&#27979;&#26631;&#31614;&#65292;&#20135;&#29983;&#22686;&#24378;&#30340;&#20869;&#37096;&#34920;&#31034;&#12290;&#19982;LID-dis&#30456;&#21453;&#65292;LID-gen&#20165;&#20351;&#29992;&#26679;&#26412;&#30340;&#29305;&#24449;&#12290;CoLafier&#21033;&#29992;&#27599;&#20010;&#23454;&#20363;&#30340;&#20004;&#20010;&#22686;&#24378;&#35270;&#22270;&#21516;&#26102;&#36755;&#20837;&#20004;&#20010;&#23376;&#32593;&#32476;&#65292;&#21033;&#29992;LID&#20998;&#25968;&#26469;&#20998;&#37197;&#26631;&#31614;&#12290;</title><link>http://arxiv.org/abs/2401.05458</link><description>&lt;p&gt;
CoLafier: &#24102;&#26377;&#23616;&#37096;&#20869;&#22312;&#32500;&#24230;&#25351;&#23548;&#30340;&#21327;&#20316;&#22122;&#22768;&#26631;&#31614;&#20928;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
CoLafier: Collaborative Noisy Label Purifier With Local Intrinsic Dimensionality Guidance. (arXiv:2401.05458v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05458
&lt;/p&gt;
&lt;p&gt;
CoLafier&#26159;&#19968;&#31181;&#20351;&#29992;&#23616;&#37096;&#20869;&#22312;&#32500;&#24230;&#65288;LID&#65289;&#36827;&#34892;&#24102;&#26377;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#23427;&#36890;&#36807;&#21033;&#29992;LID-dis&#21644;LID-gen&#20004;&#20010;&#23376;&#32593;&#32476;&#65292;&#20854;&#20013;LID-dis&#20351;&#29992;&#26679;&#26412;&#30340;&#29305;&#24449;&#21644;&#26631;&#31614;&#26469;&#39044;&#27979;&#26631;&#31614;&#65292;&#20135;&#29983;&#22686;&#24378;&#30340;&#20869;&#37096;&#34920;&#31034;&#12290;&#19982;LID-dis&#30456;&#21453;&#65292;LID-gen&#20165;&#20351;&#29992;&#26679;&#26412;&#30340;&#29305;&#24449;&#12290;CoLafier&#21033;&#29992;&#27599;&#20010;&#23454;&#20363;&#30340;&#20004;&#20010;&#22686;&#24378;&#35270;&#22270;&#21516;&#26102;&#36755;&#20837;&#20004;&#20010;&#23376;&#32593;&#32476;&#65292;&#21033;&#29992;LID&#20998;&#25968;&#26469;&#20998;&#37197;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#20013;&#65292;&#22122;&#22768;&#26631;&#31614;&#24120;&#24120;&#24433;&#21709;&#20854;&#24615;&#33021;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CoLafier&#65292;&#19968;&#31181;&#21033;&#29992;&#23616;&#37096;&#20869;&#22312;&#32500;&#24230;&#65288;LID&#65289;&#36827;&#34892;&#24102;&#26377;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#12290;CoLafier&#30001;&#20004;&#20010;&#23376;&#32593;&#32476;&#32452;&#25104;&#65306;LID-dis&#21644;LID-gen&#12290;LID-dis&#26159;&#19987;&#38376;&#30340;&#20998;&#31867;&#22120;&#65292;&#20351;&#29992;&#25105;&#20204;&#29420;&#29305;&#30340;&#26041;&#26696;&#36827;&#34892;&#35757;&#32451;&#65292;&#23427;&#21516;&#26102;&#20351;&#29992;&#26679;&#26412;&#30340;&#29305;&#24449;&#21644;&#26631;&#31614;&#26469;&#39044;&#27979;&#26631;&#31614;&#65292;&#20174;&#32780;&#20135;&#29983;&#22686;&#24378;&#30340;&#20869;&#37096;&#34920;&#31034;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#20174;&#36825;&#20010;&#34920;&#31034;&#35745;&#31639;&#20986;&#26469;&#30340;LID&#20998;&#25968;&#33021;&#22815;&#26377;&#25928;&#21306;&#20998;&#19981;&#21516;&#22122;&#22768;&#24773;&#20917;&#19979;&#30340;&#27491;&#30830;&#21644;&#38169;&#35823;&#26631;&#31614;&#12290;&#19982;LID-dis&#30456;&#21453;&#65292;LID-gen&#20316;&#20026;&#24120;&#35268;&#20998;&#31867;&#22120;&#65292;&#20165;&#20351;&#29992;&#26679;&#26412;&#30340;&#29305;&#24449;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;CoLafier&#21033;&#29992;&#27599;&#20010;&#23454;&#20363;&#30340;&#20004;&#20010;&#22686;&#24378;&#35270;&#22270;&#21516;&#26102;&#36755;&#20837;&#20004;&#20010;&#23376;&#32593;&#32476;&#12290;CoLafier&#23558;&#26469;&#33258;LID-dis&#30340;&#20004;&#20010;&#35270;&#22270;&#30340;LID&#20998;&#25968;&#20316;&#20026;&#20998;&#37197;&#26631;&#31614;&#30340;&#32771;&#34385;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) have advanced many machine learning tasks, but their performance is often harmed by noisy labels in real-world data. Addressing this, we introduce CoLafier, a novel approach that uses Local Intrinsic Dimensionality (LID) for learning with noisy labels. CoLafier consists of two subnets: LID-dis and LID-gen. LID-dis is a specialized classifier. Trained with our uniquely crafted scheme, LID-dis consumes both a sample's features and its label to predict the label - which allows it to produce an enhanced internal representation. We observe that LID scores computed from this representation effectively distinguish between correct and incorrect labels across various noise scenarios. In contrast to LID-dis, LID-gen, functioning as a regular classifier, operates solely on the sample's features. During training, CoLafier utilizes two augmented views per instance to feed both subnets. CoLafier considers the LID scores from the two views as produced by LID-dis to assign 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32500;&#24230;&#24863;&#30693;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;DAO&#65292;&#36890;&#36807;&#20840;&#38754;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#22312;800&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#19977;&#31181;&#27969;&#34892;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.05453</link><description>&lt;p&gt;
&#32500;&#24230;&#24863;&#30693;&#30340;&#24322;&#24120;&#26816;&#27979;&#65306;&#29702;&#35770;&#21644;&#23454;&#39564;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Dimensionality-Aware Outlier Detection: Theoretical and Experimental Analysis. (arXiv:2401.05453v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05453
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32500;&#24230;&#24863;&#30693;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;DAO&#65292;&#36890;&#36807;&#20840;&#38754;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#22312;800&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#19977;&#31181;&#27969;&#34892;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#21442;&#25968;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20805;&#20998;&#32771;&#34385;&#20102;&#25968;&#25454;&#38598;&#20869;&#22312;&#32500;&#24230;&#30340;&#23616;&#37096;&#21464;&#21270;&#12290;&#36890;&#36807;&#20351;&#29992;&#23616;&#37096;&#20869;&#22312;&#32500;&#24230;&#65288;LID&#65289;&#29702;&#35770;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#19968;&#31181;&#32500;&#24230;&#24863;&#30693;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;DAO&#65292;&#23427;&#34987;&#25512;&#23548;&#20026;&#19968;&#20010;&#21253;&#21547;&#26597;&#35810;&#28857;&#21644;&#38543;&#26426;&#36873;&#25321;&#30340;&#36817;&#37051;&#30340;&#28176;&#36817;&#23616;&#37096;&#26399;&#26395;&#23494;&#24230;&#27604;&#30340;&#20272;&#35745;&#12290;DAO&#30340;&#32500;&#24230;&#24863;&#30693;&#34892;&#20026;&#26159;&#30001;&#20110;&#23427;&#20197;&#29702;&#35770;&#19978;&#35777;&#26126;&#30340;&#26041;&#24335;&#20351;&#29992;&#23616;&#37096;LID&#20540;&#30340;&#23616;&#37096;&#20272;&#35745;&#12290;&#36890;&#36807;&#23545;800&#22810;&#20010;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#20840;&#38754;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;DAO&#26126;&#26174;&#20248;&#20110;&#19977;&#31181;&#27969;&#34892;&#19988;&#37325;&#35201;&#30340;&#22522;&#20934;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65306;&#23616;&#37096;&#31163;&#32676;&#22240;&#23376;&#65288;LOF&#65289;&#65292;&#31616;&#21270;&#29256;LOF&#21644;kNN&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a nonparametric method for outlier detection that takes full account of local variations in intrinsic dimensionality within the dataset. Using the theory of Local Intrinsic Dimensionality (LID), our 'dimensionality-aware' outlier detection method, DAO, is derived as an estimator of an asymptotic local expected density ratio involving the query point and a close neighbor drawn at random. The dimensionality-aware behavior of DAO is due to its use of local estimation of LID values in a theoretically-justified way. Through comprehensive experimentation on more than 800 synthetic and real datasets, we show that DAO significantly outperforms three popular and important benchmark outlier detection methods: Local Outlier Factor (LOF), Simplified LOF, and kNN.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#29992;&#20110;&#26080;&#34966;&#21512;&#25104;&#21160;&#33033;&#34880;&#21387;&#27874;&#24418;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#19968;&#31181;&#22522;&#20110;Transformer&#65292;&#19968;&#31181;&#22522;&#20110;&#39057;&#22495;&#23398;&#20064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#39057;&#22495;&#23398;&#20064;&#27169;&#22411;&#22312;&#21160;&#33033;&#34880;&#21387;&#20272;&#35745;&#26041;&#38754;&#20248;&#20110;Transformer&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.05452</link><description>&lt;p&gt;
&#20351;&#29992;Transformer&#21644;&#39057;&#22495;&#23398;&#20064;&#20174;&#21333;&#28857;PPG&#21512;&#25104;&#26080;&#34966;&#24335;&#21160;&#33033;&#34880;&#21387;&#27874;&#24418;
&lt;/p&gt;
&lt;p&gt;
Cuff-less Arterial Blood Pressure Waveform Synthesis from Single-site PPG using Transformer &amp; Frequency-domain Learning. (arXiv:2401.05452v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05452
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#29992;&#20110;&#26080;&#34966;&#21512;&#25104;&#21160;&#33033;&#34880;&#21387;&#27874;&#24418;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#19968;&#31181;&#22522;&#20110;Transformer&#65292;&#19968;&#31181;&#22522;&#20110;&#39057;&#22495;&#23398;&#20064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#39057;&#22495;&#23398;&#20064;&#27169;&#22411;&#22312;&#21160;&#33033;&#34880;&#21387;&#20272;&#35745;&#26041;&#38754;&#20248;&#20110;Transformer&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#20351;&#29992;&#21333;&#28857;&#20809;&#30005;&#33033;&#25615;&#22270;(PPG)&#20449;&#21495;&#26080;&#34966;&#21512;&#25104;&#21160;&#33033;&#34880;&#21387;(ABP)&#27874;&#24418;&#12290;&#25105;&#20204;&#21033;&#29992;&#20844;&#20849;&#30340;UCI&#25968;&#25454;&#38598;&#23545;&#26080;&#34966;&#34880;&#21387;&#20272;&#35745;&#36827;&#34892;&#20102;&#35757;&#32451;&#21644;&#35780;&#20272;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#20010;&#21253;&#21547;&#20301;&#32622;&#32534;&#30721;&#12289;&#22810;&#22836;&#27880;&#24847;&#21147;&#12289;&#23618;&#24402;&#19968;&#21270;&#21644;dropout&#25216;&#26415;&#30340;Transformer&#27169;&#22411;&#65292;&#24182;&#20197;14&#30340;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;(MAE)&#21512;&#25104;ABP&#27874;&#24418;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#31181;&#39057;&#22495;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#33719;&#21462;PPG&#21644;ABP&#20449;&#21495;&#23545;&#24212;&#20110;&#20004;&#20010;&#24515;&#33039;&#21608;&#26399;&#30340;&#31163;&#25955;&#20313;&#24358;&#21464;&#25442;(DCT)&#31995;&#25968;&#65292;&#28982;&#21518;&#23398;&#20064;&#23427;&#20204;&#20043;&#38388;&#30340;&#32447;&#24615;/&#38750;&#32447;&#24615;&#22238;&#24402;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#39057;&#22495;&#32447;&#24615;/&#38750;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#22312;&#33298;&#24352;&#21387;(DBP)&#21644;&#25910;&#32553;&#21387;(SBP)&#26041;&#38754;&#30340;MAE&#20998;&#21035;&#20026;11.87&#21644;8.01&#65292;&#20248;&#20110;Transformer&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose two novel purpose-built deep learning (DL) models for synthesis of the arterial blood pressure (ABP) waveform in a cuff-less manner, using a single-site photoplethysmography (PPG) signal. We utilize the public UCI dataset on cuff-less blood pressure (CLBP) estimation to train and evaluate our DL models. Firstly, we implement a transformer model that incorporates positional encoding, multi-head attention, layer normalization, and dropout techniques, and synthesizes the ABP waveform with a mean absolute error (MAE) of 14. Secondly, we implement a frequency-domain (FD) learning approach where we first obtain the discrete cosine transform (DCT) coefficients of the PPG and ABP signals corresponding to two cardiac cycles, and then learn a linear/non-linear (L/NL) regression between them. We learn that the FD L/NL regression model outperforms the transformer model by achieving an MAE of 11.87 and 8.01, for diastolic blood pressure (DBP) and systolic blood pressure (SBP), respective
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#31995;&#32479;&#32508;&#36848;&#20102;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#22312;&#33041;&#30005;&#22270;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#35774;&#35745;&#33391;&#22909;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#26469;&#25552;&#21462;&#26080;&#26631;&#31614;&#26679;&#26412;&#30340;&#34920;&#31034;&#65292;&#35299;&#20915;&#20102;&#33041;&#30005;&#20449;&#21495;&#26631;&#31614;&#30340;&#38382;&#39064;&#21644;&#20010;&#20307;&#20043;&#38388;&#30340;&#21464;&#21270;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.05446</link><description>&lt;p&gt;
&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#22312;&#33041;&#30005;&#22270;&#20013;&#30340;&#24212;&#29992;&#65306;&#31995;&#32479;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Self-supervised Learning for Electroencephalogram: A Systematic Survey. (arXiv:2401.05446v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05446
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#31995;&#32479;&#32508;&#36848;&#20102;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#22312;&#33041;&#30005;&#22270;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#35774;&#35745;&#33391;&#22909;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#26469;&#25552;&#21462;&#26080;&#26631;&#31614;&#26679;&#26412;&#30340;&#34920;&#31034;&#65292;&#35299;&#20915;&#20102;&#33041;&#30005;&#20449;&#21495;&#26631;&#31614;&#30340;&#38382;&#39064;&#21644;&#20010;&#20307;&#20043;&#38388;&#30340;&#21464;&#21270;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#26159;&#19968;&#31181;&#38750;&#20405;&#20837;&#24615;&#30340;&#35760;&#24405;&#29983;&#29289;&#30005;&#20449;&#21495;&#30340;&#25216;&#26415;&#12290;&#36817;&#24180;&#26469;&#65292;&#23558;&#30417;&#30563;&#24335;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#19982;&#33041;&#30005;&#20449;&#21495;&#30456;&#32467;&#21512;&#65292;&#24050;&#32463;&#22312;&#21508;&#31181;&#22522;&#20110;&#33041;&#30005;&#30340;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#33258;&#21160;&#20998;&#26512;&#12290;&#28982;&#32780;&#65292;&#33041;&#30005;&#20449;&#21495;&#30340;&#26631;&#31614;&#38382;&#39064;&#38480;&#21046;&#20102;&#22522;&#20110;&#33041;&#30005;&#30340;&#28145;&#24230;&#27169;&#22411;&#30340;&#21457;&#23637;&#12290;&#33719;&#21462;&#33041;&#30005;&#27880;&#37322;&#26159;&#22256;&#38590;&#30340;&#65292;&#38656;&#35201;&#39046;&#22495;&#19987;&#23478;&#25351;&#23548;&#25910;&#38598;&#21644;&#26631;&#35760;&#65292;&#24182;&#19988;&#19981;&#21516;&#21463;&#35797;&#32773;&#20043;&#38388;&#30340;&#33041;&#30005;&#20449;&#21495;&#30340;&#21464;&#21270;&#20250;&#23548;&#33268;&#26174;&#33879;&#30340;&#26631;&#31614;&#20559;&#31227;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#36890;&#36807;&#35774;&#35745;&#33391;&#22909;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#20174;&#26080;&#26631;&#31614;&#26679;&#26412;&#20013;&#25552;&#21462;&#34920;&#31034;&#12290;&#26412;&#25991;&#32858;&#28966;&#20110;&#23558;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#19982;&#26102;&#38388;&#24207;&#21015;&#33041;&#30005;&#20449;&#21495;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#39640;&#25928;&#30340;&#34920;&#31034;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#33041;&#30005;&#20449;&#21495;&#30340;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#30340;&#31995;&#32479;&#32508;&#36848;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;1&#65289;&#25105;&#20204;&#20171;&#32461;&#20102;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#30340;&#27010;&#24565;&#21644;&#29702;&#35770;&#65292;&#20197;&#21450;&#20856;&#22411;&#30340;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#12290;2&#65289;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20221;&#20840;&#38754;&#30340;&#32508;&#36848;&#65292;&#27010;&#36848;&#20102;&#30446;&#21069;&#30340;&#30740;&#31350;&#36827;&#23637;&#12289;&#26041;&#27861;&#21644;&#24212;&#29992;&#22330;&#26223;&#65292;&#24182;&#25351;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electroencephalogram (EEG) is a non-invasive technique to record bioelectrical signals. Integrating supervised deep learning techniques with EEG signals has recently facilitated automatic analysis across diverse EEG-based tasks. However, the label issues of EEG signals have constrained the development of EEG-based deep models. Obtaining EEG annotations is difficult that requires domain experts to guide collection and labeling, and the variability of EEG signals among different subjects causes significant label shifts. To solve the above challenges, self-supervised learning (SSL) has been proposed to extract representations from unlabeled samples through well-designed pretext tasks. This paper concentrates on integrating SSL frameworks with temporal EEG signals to achieve efficient representation and proposes a systematic review of the SSL for EEG signals. In this paper, 1) we introduce the concept and theory of self-supervised learning and typical SSL frameworks. 2) We provide a compre
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23618;&#20869;&#36830;&#25509;&#30340;&#20840;&#33033;&#20914;&#34892;&#20026;&#32593;&#32476;&#65292;&#21033;&#29992;&#29305;&#27530;&#30340;&#31070;&#32463;&#24418;&#24577;&#30828;&#20214;&#23454;&#29616;&#20102;&#36739;&#20302;&#33021;&#32791;&#30340;&#20154;&#24037;&#26234;&#33021;&#12290;&#35813;&#26041;&#27861;&#22312;&#23454;&#29616;&#25511;&#21046;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#21487;&#27604;&#36739;&#24615;&#33021;&#65292;&#24182;&#35299;&#20915;&#20102;&#20351;&#29992;&#33033;&#20914;&#39057;&#29575;&#20316;&#20026;&#36755;&#20986;&#25152;&#24102;&#26469;&#30340;&#28014;&#28857;&#30697;&#38453;&#36816;&#31639;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.05444</link><description>&lt;p&gt;
&#22522;&#20110;&#23618;&#20869;&#36830;&#25509;&#30340;&#20840;&#33033;&#20914;&#34892;&#20026;&#32593;&#32476;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Fully Spiking Actor Network with Intra-layer Connections for Reinforcement Learning. (arXiv:2401.05444v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05444
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23618;&#20869;&#36830;&#25509;&#30340;&#20840;&#33033;&#20914;&#34892;&#20026;&#32593;&#32476;&#65292;&#21033;&#29992;&#29305;&#27530;&#30340;&#31070;&#32463;&#24418;&#24577;&#30828;&#20214;&#23454;&#29616;&#20102;&#36739;&#20302;&#33021;&#32791;&#30340;&#20154;&#24037;&#26234;&#33021;&#12290;&#35813;&#26041;&#27861;&#22312;&#23454;&#29616;&#25511;&#21046;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#21487;&#27604;&#36739;&#24615;&#33021;&#65292;&#24182;&#35299;&#20915;&#20102;&#20351;&#29992;&#33033;&#20914;&#39057;&#29575;&#20316;&#20026;&#36755;&#20986;&#25152;&#24102;&#26469;&#30340;&#28014;&#28857;&#30697;&#38453;&#36816;&#31639;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20511;&#21161;&#29305;&#27530;&#30340;&#31070;&#32463;&#24418;&#24577;&#30828;&#20214;&#65292;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#34987;&#26399;&#26395;&#33021;&#20197;&#36739;&#20302;&#33021;&#37327;&#28040;&#32791;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#12290;&#36890;&#36807;&#23558;SNN&#19982;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#32467;&#21512;&#65292;&#20026;&#29616;&#23454;&#25511;&#21046;&#20219;&#21153;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#39640;&#33021;&#25928;&#26041;&#24335;&#12290;&#26412;&#25991;&#38024;&#23545;&#20195;&#29702;&#38656;&#35201;&#23398;&#20064;&#22810;&#32500;&#30830;&#23450;&#24615;&#31574;&#30053;&#20197;&#36827;&#34892;&#25511;&#21046;&#30340;&#20219;&#21153;&#36827;&#34892;&#30740;&#31350;&#65292;&#36825;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#38750;&#24120;&#24120;&#35265;&#12290;&#26368;&#36817;&#65292;&#26367;&#20195;&#26799;&#24230;&#26041;&#27861;&#24050;&#34987;&#29992;&#20110;&#35757;&#32451;&#22810;&#23618;SNN&#65292;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#20801;&#35768;SNNs&#23454;&#29616;&#19982;&#23545;&#24212;&#28145;&#24230;&#32593;&#32476;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22522;&#20110;&#33033;&#20914;&#30340;RL&#26041;&#27861;&#23558;&#33033;&#20914;&#39057;&#29575;&#20316;&#20026;SNN&#30340;&#36755;&#20986;&#65292;&#24182;&#36890;&#36807;&#20840;&#36830;&#25509;&#65288;FC&#65289;&#23618;&#23558;&#20854;&#36716;&#25442;&#20026;&#34920;&#31034;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#65288;&#21363;&#30830;&#23450;&#24615;&#31574;&#30053;&#65289;&#12290;&#28982;&#32780;&#65292;&#33033;&#20914;&#39057;&#29575;&#30340;&#21313;&#36827;&#21046;&#29305;&#24615;&#20351;&#24471;FC&#23618;&#38656;&#35201;&#28014;&#28857;&#30697;&#38453;&#36816;&#31639;&#65292;&#20351;&#24471;&#25972;&#20010;SNN&#26080;&#27861;&#37096;&#32626;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the help of special neuromorphic hardware, spiking neural networks (SNNs) are expected to realize artificial intelligence (AI) with less energy consumption. It provides a promising energy-efficient way for realistic control tasks by combining SNNs with deep reinforcement learning (DRL). In this paper, we focus on the task where the agent needs to learn multi-dimensional deterministic policies to control, which is very common in real scenarios. Recently, the surrogate gradient method has been utilized for training multi-layer SNNs, which allows SNNs to achieve comparable performance with the corresponding deep networks in this task. Most existing spike-based RL methods take the firing rate as the output of SNNs, and convert it to represent continuous action space (i.e., the deterministic policy) through a fully-connected (FC) layer. However, the decimal characteristic of the firing rate brings the floating-point matrix operations to the FC layer, making the whole SNN unable to depl
&lt;/p&gt;</description></item><item><title>&#21151;&#33021;&#22270;&#27169;&#22411;&#65288;FGMs&#65289;&#36890;&#36807;&#32467;&#26500;&#23454;&#29616;&#20102;&#26679;&#26412;&#39640;&#25928;&#30340;&#25968;&#25454;&#39537;&#21160;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2401.05442</link><description>&lt;p&gt;
&#21151;&#33021;&#22270;&#27169;&#22411;&#65306;&#32467;&#26500;&#23454;&#29616;&#31163;&#32447;&#25968;&#25454;&#39537;&#21160;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Functional Graphical Models: Structure Enables Offline Data-Driven Optimization. (arXiv:2401.05442v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05442
&lt;/p&gt;
&lt;p&gt;
&#21151;&#33021;&#22270;&#27169;&#22411;&#65288;FGMs&#65289;&#36890;&#36807;&#32467;&#26500;&#23454;&#29616;&#20102;&#26679;&#26412;&#39640;&#25928;&#30340;&#25968;&#25454;&#39537;&#21160;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36890;&#24120;&#26159;&#20026;&#20102;&#35299;&#20915;&#39044;&#27979;&#38382;&#39064;&#32780;&#35757;&#32451;&#30340;&#65292;&#20294;&#25105;&#20204;&#32463;&#24120;&#24076;&#26395;&#23558;&#23427;&#20204;&#29992;&#20110;&#20248;&#21270;&#38382;&#39064;&#12290;&#20363;&#22914;&#65292;&#32473;&#23450;&#19968;&#32452;&#34507;&#30333;&#36136;&#21450;&#20854;&#23545;&#24212;&#30340;&#33639;&#20809;&#27700;&#24179;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#21487;&#33021;&#24076;&#26395;&#20026;&#20855;&#26377;&#26368;&#39640;&#33639;&#20809;&#30340;&#26032;&#34507;&#30333;&#36136;&#36827;&#34892;&#20248;&#21270;&#12290;&#36825;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#20248;&#21270;&#65288;DDO&#65289;&#38754;&#20020;&#30528;&#19968;&#31995;&#21015;&#25361;&#25112;&#65292;&#36229;&#20986;&#20102;&#26631;&#20934;&#39044;&#27979;&#38382;&#39064;&#20013;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#25105;&#20204;&#38656;&#35201;&#25104;&#21151;&#39044;&#27979;&#22312;&#35757;&#32451;&#38598;&#20013;&#27809;&#26377;&#35265;&#36807;&#30340;&#20248;&#20110;&#26368;&#20339;&#35774;&#35745;&#30340;&#26032;&#35774;&#35745;&#30340;&#24615;&#33021;&#30340;&#27169;&#22411;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#29978;&#33267;&#19981;&#28165;&#26970;&#29616;&#26377;&#26041;&#27861;&#20160;&#20040;&#26102;&#20505;&#29978;&#33267;&#33021;&#27604;&#31616;&#21333;&#22320;&#36873;&#25321;&#25968;&#25454;&#38598;&#20013;&#26368;&#20339;&#35774;&#35745;&#30340;&#26420;&#32032;&#26041;&#27861;&#25191;&#34892;&#24471;&#26356;&#22909;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#32467;&#26500;&#23454;&#29616;&#39640;&#25928;&#30340;&#25968;&#25454;&#39537;&#21160;&#20248;&#21270;&#12290;&#20026;&#20102;&#24418;&#24335;&#21270;&#32467;&#26500;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21151;&#33021;&#22270;&#27169;&#22411;&#65288;FGMs&#65289;&#24182;&#20174;&#29702;&#35770;&#19978;&#23637;&#31034;&#20102;&#23427;&#20204;&#22914;&#20309;&#36890;&#36807;&#20998;&#35299;&#23454;&#29616;&#22522;&#20110;&#25968;&#25454;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
While machine learning models are typically trained to solve prediction problems, we might often want to use them for optimization problems. For example, given a dataset of proteins and their corresponding fluorescence levels, we might want to optimize for a new protein with the highest possible fluorescence. This kind of data-driven optimization (DDO) presents a range of challenges beyond those in standard prediction problems, since we need models that successfully predict the performance of new designs that are better than the best designs seen in the training set. It is not clear theoretically when existing approaches can even perform better than the naive approach that simply selects the best design in the dataset. In this paper, we study how structure can enable sample-efficient data-driven optimization. To formalize the notion of structure, we introduce functional graphical models (FGMs) and show theoretically how they can provide for principled data-driven optimization by decomp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#36866;&#24212;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21382;&#21490;&#25968;&#25454;&#23545;&#27604;&#29305;&#24065;&#12289;&#20197;&#22826;&#22346;&#21644;&#27604;&#29305;&#24065;&#25903;&#37197;&#24230;&#12289;&#20197;&#22826;&#22346;&#25903;&#37197;&#24230;&#36827;&#34892;&#39044;&#27979;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.05441</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#36866;&#24212;&#32593;&#32476;&#30340;&#26041;&#27861;&#23545;&#21152;&#23494;&#36135;&#24065;&#20215;&#20540;&#36827;&#34892;&#20808;&#36827;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
An adaptive network-based approach for advanced forecasting of cryptocurrency values. (arXiv:2401.05441v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05441
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#36866;&#24212;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21382;&#21490;&#25968;&#25454;&#23545;&#27604;&#29305;&#24065;&#12289;&#20197;&#22826;&#22346;&#21644;&#27604;&#29305;&#24065;&#25903;&#37197;&#24230;&#12289;&#20197;&#22826;&#22346;&#25903;&#37197;&#24230;&#36827;&#34892;&#39044;&#27979;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#36866;&#24212;&#32593;&#32476;&#22522;&#20110;&#27169;&#31946;&#25512;&#29702;&#31995;&#32479; (ANFIS) &#39044;&#27979;&#26410;&#26469;&#19971;&#22825;&#21152;&#23494;&#36135;&#24065;&#20215;&#26684;&#30340;&#26550;&#26500;&#12290;&#22312;&#27599;&#26085;&#26102;&#38388;&#26694;&#26550;&#19979;&#32771;&#34385;&#20102;&#27604;&#29305;&#24065; (BTC)&#12289;&#20197;&#22826;&#22346; (ETH)&#12289;&#27604;&#29305;&#24065;&#25903;&#37197;&#24230; (BTC.D) &#21644;&#20197;&#22826;&#22346;&#25903;&#37197;&#24230; (ETH.D)&#30340;&#21382;&#21490;&#25968;&#25454;&#12290;&#25152;&#20351;&#29992;&#30340;&#26041;&#27861;&#21253;&#25324;&#28151;&#21512;&#21644;&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#65292;&#20197;&#21450;&#32593;&#26684;&#21010;&#20998;&#12289;&#20943;&#27861;&#32858;&#31867;&#21644;&#27169;&#31946;C&#22343;&#20540;&#32858;&#31867; (FCM) &#31639;&#27861;&#29992;&#20110;&#25968;&#25454;&#32858;&#31867;&#12290;&#36890;&#36807;&#32479;&#35745;&#35780;&#20272;&#26631;&#20934;&#27604;&#36739;&#20102;&#26412;&#25991;&#35774;&#35745;&#30340;&#26550;&#26500;&#24615;&#33021;&#19982;&#19981;&#21516;&#36755;&#20837;&#21644;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#26368;&#32456;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#30701;&#26102;&#38388;&#20869;&#39044;&#27979;&#25968;&#23383;&#36135;&#24065;&#30340;&#20215;&#26684;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes an architecture for predicting the price of cryptocurrencies for the next seven days using the Adaptive Network Based Fuzzy Inference System (ANFIS). Historical data of cryptocurrencies and indexes that are considered are Bitcoin (BTC), Ethereum (ETH), Bitcoin Dominance (BTC.D), and Ethereum Dominance (ETH.D) in a daily timeframe. The methods used to teach the data are hybrid and backpropagation algorithms, as well as grid partition, subtractive clustering, and Fuzzy C-means clustering (FCM) algorithms, which are used in data clustering. The architectural performance designed in this paper has been compared with different inputs and neural network models in terms of statistical evaluation criteria. Finally, the proposed method can predict the price of digital currencies in a short time.
&lt;/p&gt;</description></item><item><title>Autosen&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#33258;&#21160;Wi-Fi&#24863;&#30693;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#33258;&#21160;&#36328;&#27169;&#24577;&#33258;&#32534;&#30721;&#22120;&#23398;&#20064;&#24314;&#31435;&#20102;&#24133;&#24230;&#21644;&#30456;&#20301;&#20043;&#38388;&#30340;&#30452;&#25509;&#36830;&#25509;&#65292;&#26377;&#25928;&#25552;&#21462;Wi-Fi&#20449;&#21495;&#20013;&#30340;&#26377;&#20215;&#20540;&#29305;&#24449;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#29305;&#24449;&#36827;&#34892;&#29305;&#23450;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2401.05440</link><description>&lt;p&gt;
Autosen:&#36890;&#36807;&#36328;&#27169;&#24577;&#33258;&#32534;&#30721;&#22120;&#25913;&#21892;&#33258;&#21160;Wi-Fi&#20154;&#20307;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Autosen: improving automatic wifi human sensing through cross-modal autoencoder. (arXiv:2401.05440v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05440
&lt;/p&gt;
&lt;p&gt;
Autosen&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#33258;&#21160;Wi-Fi&#24863;&#30693;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#33258;&#21160;&#36328;&#27169;&#24577;&#33258;&#32534;&#30721;&#22120;&#23398;&#20064;&#24314;&#31435;&#20102;&#24133;&#24230;&#21644;&#30456;&#20301;&#20043;&#38388;&#30340;&#30452;&#25509;&#36830;&#25509;&#65292;&#26377;&#25928;&#25552;&#21462;Wi-Fi&#20449;&#21495;&#20013;&#30340;&#26377;&#20215;&#20540;&#29305;&#24449;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#29305;&#24449;&#36827;&#34892;&#29305;&#23450;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Wi-Fi&#20154;&#20307;&#24863;&#30693;&#22240;&#20854;&#20302;&#25104;&#26412;&#21644;&#38544;&#31169;&#20248;&#21183;&#32780;&#22791;&#21463;&#25512;&#23815;&#65292;&#22312;&#35782;&#21035;&#20154;&#31867;&#27963;&#21160;&#26041;&#38754;&#20855;&#26377;&#24456;&#22823;&#30340;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#20854;&#26377;&#25928;&#24615;&#22312;&#21463;&#25511;&#12289;&#21333;&#29992;&#25143;&#12289;&#30452;&#32447;&#35270;&#32447;&#30340;&#35774;&#32622;&#20013;&#21463;&#21040;&#38480;&#21046;&#65292;&#21463;&#25968;&#25454;&#25910;&#38598;&#22797;&#26434;&#24615;&#21644;&#26631;&#35760;&#25968;&#25454;&#38598;&#31232;&#32570;&#24615;&#30340;&#21046;&#32422;&#12290;&#20256;&#32479;&#30340;&#36328;&#27169;&#24577;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#32780;&#26080;&#38656;&#26631;&#35760;&#25968;&#25454;&#26469;&#20943;&#36731;&#36825;&#20123;&#38480;&#21046;&#65292;&#20294;&#24448;&#24448;&#26080;&#27861;&#20174;&#24133;&#24230;-&#30456;&#20301;&#32452;&#21512;&#20013;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#29305;&#24449;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;AutoSen&#65292;&#19968;&#31181;&#21019;&#26032;&#30340;&#33258;&#21160;Wi-Fi&#24863;&#30693;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#19981;&#21516;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;AutoSen&#36890;&#36807;&#33258;&#21160;&#36328;&#27169;&#24577;&#33258;&#32534;&#30721;&#22120;&#23398;&#20064;&#24314;&#31435;&#20102;&#24133;&#24230;&#21644;&#30456;&#20301;&#20043;&#38388;&#30340;&#30452;&#25509;&#36830;&#25509;&#12290;&#35813;&#33258;&#32534;&#30721;&#22120;&#20174;&#26410;&#26631;&#35760;&#30340;CSI&#25968;&#25454;&#20013;&#39640;&#25928;&#25552;&#21462;&#26377;&#20215;&#20540;&#30340;&#29305;&#24449;&#65292;&#21253;&#25324;&#24133;&#24230;&#21644;&#30456;&#20301;&#20449;&#24687;&#65292;&#21516;&#26102;&#28040;&#38500;&#23427;&#20204;&#21508;&#33258;&#29420;&#29305;&#30340;&#22122;&#22768;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;&#36825;&#20123;&#29305;&#24449;&#20351;&#29992;&#23569;&#26679;&#26412;&#23398;&#20064;&#25216;&#26415;&#26469;&#23436;&#25104;&#29305;&#23450;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
WiFi human sensing is highly regarded for its low-cost and privacy advantages in recognizing human activities. However, its effectiveness is largely confined to controlled, single-user, line-of-sight settings, limited by data collection complexities and the scarcity of labeled datasets. Traditional cross-modal methods, aimed at mitigating these limitations by enabling self-supervised learning without labeled data, struggle to extract meaningful features from amplitude-phase combinations. In response, we introduce AutoSen, an innovative automatic WiFi sensing solution that departs from conventional approaches. AutoSen establishes a direct link between amplitude and phase through automated cross-modal autoencoder learning. This autoencoder efficiently extracts valuable features from unlabeled CSI data, encompassing amplitude and phase information while eliminating their respective unique noises. These features are then leveraged for specific tasks using few-shot learning techniques. Auto
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#19977;&#32500;Terzaghi&#22266;&#32467;&#26041;&#31243;&#30340;&#29289;&#29702;&#20449;&#24687;&#21270;&#31070;&#32463;&#32593;&#32476;(PINNs)&#26694;&#26550;&#65292;&#33021;&#22815;&#24555;&#36895;&#39044;&#27979;&#19981;&#21516;&#26465;&#20214;&#19979;&#30340;&#22266;&#32467;&#26696;&#20363;&#12290;&#36890;&#36807;&#23545;&#27604;&#20256;&#32479;&#25968;&#20540;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;PINNs&#22312;&#27491;&#21521;&#21644;&#21453;&#21521;&#38382;&#39064;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#30830;&#23450;&#20102;&#22266;&#32467;&#31995;&#25968;&#21644;&#22122;&#22768;&#25968;&#25454;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.05439</link><description>&lt;p&gt;
&#29992;&#29289;&#29702;&#20449;&#24687;&#24341;&#23548;&#30340;&#28145;&#24230;&#23398;&#20064;&#35299;&#19977;&#32500;Terzaghi&#22266;&#32467;&#26041;&#31243;&#65306;&#27491;&#21521;&#21644;&#21453;&#21521;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Physics-informed Deep Learning to Solve Three-dimensional Terzaghi Consolidation Equation: Forward and Inverse Problems. (arXiv:2401.05439v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05439
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#19977;&#32500;Terzaghi&#22266;&#32467;&#26041;&#31243;&#30340;&#29289;&#29702;&#20449;&#24687;&#21270;&#31070;&#32463;&#32593;&#32476;(PINNs)&#26694;&#26550;&#65292;&#33021;&#22815;&#24555;&#36895;&#39044;&#27979;&#19981;&#21516;&#26465;&#20214;&#19979;&#30340;&#22266;&#32467;&#26696;&#20363;&#12290;&#36890;&#36807;&#23545;&#27604;&#20256;&#32479;&#25968;&#20540;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;PINNs&#22312;&#27491;&#21521;&#21644;&#21453;&#21521;&#38382;&#39064;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#30830;&#23450;&#20102;&#22266;&#32467;&#31995;&#25968;&#21644;&#22122;&#22768;&#25968;&#25454;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#21463;&#29289;&#29702;&#25511;&#21046;&#26041;&#31243;&#32422;&#26463;&#30340;&#20986;&#29616;&#65292;&#24341;&#21457;&#20102;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#20013;&#30340;&#19968;&#20010;&#26032;&#36235;&#21183;&#65292;&#34987;&#31216;&#20026;&#29289;&#29702;&#20449;&#24687;&#21270;&#31070;&#32463;&#32593;&#32476;(PINNs)&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;PINNs&#35299;&#20915;&#39640;&#32500;&#38382;&#39064;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65292;&#31354;&#38388;&#22797;&#26434;&#24230;&#23545;&#20110;&#35299;&#20915;&#22823;&#35268;&#27169;&#22810;&#26041;&#21521;&#38382;&#39064;&#24102;&#26469;&#20102;&#22256;&#38590;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;PINN&#26694;&#26550;&#65292;&#33021;&#22815;&#24555;&#36895;&#39044;&#27979;&#19981;&#21516;&#26465;&#20214;&#19979;&#30340;&#20960;&#20010;&#19977;&#32500;Terzaghi&#22266;&#32467;&#26696;&#20363;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#19981;&#21516;&#26696;&#20363;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#24182;&#31361;&#20986;&#20102;&#19977;&#32500;&#22266;&#32467;&#38382;&#39064;&#20013;&#30340;&#24046;&#24322;&#12290;&#20171;&#32461;&#20102;&#29992;&#20110;&#19977;&#32500;&#22266;&#32467;&#38382;&#39064;&#30340;PINNs&#26694;&#26550;&#30340;&#35843;&#25972;&#31574;&#30053;&#12290;&#28982;&#21518;&#65292;&#27979;&#35797;&#24182;&#27604;&#36739;&#20102;PINNs&#22312;&#27491;&#21521;&#38382;&#39064;&#20013;&#37319;&#29992;&#30340;&#20256;&#32479;&#25968;&#20540;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#30830;&#23450;&#20102;&#21453;&#21521;&#38382;&#39064;&#20013;&#22266;&#32467;&#31995;&#25968;&#21644;&#22122;&#22768;&#25968;&#25454;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of neural networks constrained by physical governing equations has sparked a new trend in deep learning research, which is known as Physics-Informed Neural Networks (PINNs). However, solving high-dimensional problems with PINNs is still a substantial challenge, the space complexity brings difficulty to solving large multidirectional problems. In this paper, a novel PINN framework to quickly predict several three-dimensional Terzaghi consolidation cases under different conditions is proposed. Meanwhile, the loss functions for different cases are introduced, and their differences in three-dimensional consolidation problems are highlighted. The tuning strategies for the PINNs framework for three-dimensional consolidation problems are introduced. Then, the performance of PINNs is tested and compared with traditional numerical methods adopted in forward problems, and the coefficients of consolidation and the impact of noisy data in inverse problems are identified. Finally, the
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21487;&#31359;&#25140;&#24212;&#29992;&#20013;&#34920;&#31034;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#32570;&#22833;&#25968;&#25454;&#24773;&#20917;&#19979;&#12290;&#20316;&#32773;&#36890;&#36807;&#27604;&#36739;Transformer&#27169;&#22411;&#21644;&#32479;&#35745;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;Transformer&#27169;&#22411;&#22312;&#21464;&#21270;&#39057;&#32321;&#30340;&#20449;&#21495;&#30340;&#32570;&#22833;&#25968;&#25454;&#22635;&#20805;&#26041;&#38754;&#34920;&#29616;&#20248;&#31168;&#12290;&#27492;&#30740;&#31350;&#20026;&#22522;&#20110;&#25513;&#30721;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#30340;&#35774;&#35745;&#21644;&#24320;&#21457;&#25552;&#20379;&#20102;&#27934;&#23519;&#12290;</title><link>http://arxiv.org/abs/2401.05437</link><description>&lt;p&gt;
&#21487;&#31359;&#25140;&#24212;&#29992;&#20013;&#30340;&#34920;&#31034;&#23398;&#20064;&#65306;&#22312;&#32570;&#22833;&#25968;&#25454;&#24773;&#20917;&#19979;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Representation Learning for Wearable-Based Applications in the Case of Missing Data. (arXiv:2401.05437v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05437
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21487;&#31359;&#25140;&#24212;&#29992;&#20013;&#34920;&#31034;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#32570;&#22833;&#25968;&#25454;&#24773;&#20917;&#19979;&#12290;&#20316;&#32773;&#36890;&#36807;&#27604;&#36739;Transformer&#27169;&#22411;&#21644;&#32479;&#35745;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;Transformer&#27169;&#22411;&#22312;&#21464;&#21270;&#39057;&#32321;&#30340;&#20449;&#21495;&#30340;&#32570;&#22833;&#25968;&#25454;&#22635;&#20805;&#26041;&#38754;&#34920;&#29616;&#20248;&#31168;&#12290;&#27492;&#30740;&#31350;&#20026;&#22522;&#20110;&#25513;&#30721;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#30340;&#35774;&#35745;&#21644;&#24320;&#21457;&#25552;&#20379;&#20102;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#31359;&#25140;&#35774;&#22791;&#25345;&#32493;&#25910;&#38598;&#20256;&#24863;&#22120;&#25968;&#25454;&#24182;&#29992;&#20110;&#25512;&#26029;&#20010;&#20307;&#30340;&#34892;&#20026;&#65292;&#22914;&#30561;&#30496;&#12289;&#20307;&#21147;&#27963;&#21160;&#21644;&#24773;&#32490;&#12290;&#23613;&#31649;&#22312;&#36825;&#20010;&#39046;&#22495;&#26377;&#24456;&#22823;&#30340;&#20852;&#36259;&#21644;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#25968;&#25454;&#36136;&#37327;&#20302;&#21644;&#25968;&#25454;&#27880;&#37322;&#26377;&#38480;&#65292;&#24314;&#27169;&#30495;&#23454;&#29615;&#22659;&#20013;&#30340;&#22810;&#27169;&#24335;&#20256;&#24863;&#22120;&#25968;&#25454;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#29992;&#20110;&#22635;&#20805;&#32570;&#22833;&#21487;&#31359;&#25140;&#25968;&#25454;&#30340;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#23558;&#20854;&#19982;&#26368;&#20808;&#36827;&#30340;&#32479;&#35745;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#20351;&#29992;10&#20010;&#29983;&#29702;&#21644;&#34892;&#20026;&#20449;&#21495;&#30340;&#21464;&#21270;&#29575;&#19981;&#21516;&#30340;&#25513;&#30721;&#27604;&#29575;&#65292;&#30740;&#31350;&#20102;Transformer&#27169;&#22411;&#22312;&#32570;&#22833;&#25968;&#25454;&#22635;&#20805;&#19978;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;Transformer&#27169;&#22411;&#22312;&#21464;&#21270;&#39057;&#32321;&#30340;&#20449;&#21495;&#30340;&#32570;&#22833;&#25968;&#25454;&#22635;&#20805;&#26041;&#38754;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#65292;&#20294;&#23545;&#20110;&#21333;&#35843;&#20449;&#21495;&#21017;&#19981;&#28982;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#22635;&#20805;&#31574;&#30053;&#21644;&#25513;&#30721;&#27604;&#29575;&#23545;&#19979;&#28216;&#20998;&#31867;&#20219;&#21153;&#30340;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#20026;&#22522;&#20110;&#25513;&#30721;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#30340;&#35774;&#35745;&#21644;&#24320;&#21457;&#25552;&#20379;&#20102;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
Wearable devices continuously collect sensor data and use it to infer an individual's behavior, such as sleep, physical activity, and emotions. Despite the significant interest and advancements in this field, modeling multimodal sensor data in real-world environments is still challenging due to low data quality and limited data annotations. In this work, we investigate representation learning for imputing missing wearable data and compare it with state-of-the-art statistical approaches. We investigate the performance of the transformer model on 10 physiological and behavioral signals with different masking ratios. Our results show that transformers outperform baselines for missing data imputation of signals that change more frequently, but not for monotonic signals. We further investigate the impact of imputation strategies and masking rations on downstream classification tasks. Our study provides insights for the design and development of masking-based self-supervised learning tasks a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;OFDM&#20449;&#36947;&#20272;&#35745;&#26041;&#26696;&#65292;&#36890;&#36807;&#22312;&#21333;&#20010;OFDM&#26102;&#38553;&#20869;&#21033;&#29992;&#26080;&#32447;&#20449;&#36947;&#39057;&#29575;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#20351;&#29992;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#25216;&#26415;&#23454;&#29616;&#20102;&#36229;&#36234;&#29616;&#26377;&#26041;&#27861;&#30340;&#20449;&#36947;&#20272;&#35745;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.05436</link><description>&lt;p&gt;
&#28145;&#24230;OFDM&#20449;&#36947;&#20272;&#35745;: &#25429;&#33719;&#39057;&#29575;&#37325;&#22797;&#24615;
&lt;/p&gt;
&lt;p&gt;
Deep OFDM Channel Estimation: Capturing Frequency Recurrence. (arXiv:2401.05436v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05436
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;OFDM&#20449;&#36947;&#20272;&#35745;&#26041;&#26696;&#65292;&#36890;&#36807;&#22312;&#21333;&#20010;OFDM&#26102;&#38553;&#20869;&#21033;&#29992;&#26080;&#32447;&#20449;&#36947;&#39057;&#29575;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#20351;&#29992;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#25216;&#26415;&#23454;&#29616;&#20102;&#36229;&#36234;&#29616;&#26377;&#26041;&#27861;&#30340;&#20449;&#36947;&#20272;&#35745;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27491;&#20132;&#39057;&#20998;&#22797;&#29992;(OFDM)&#31995;&#32479;&#20013;&#30340;&#20449;&#36947;&#20272;&#35745;&#26041;&#26696;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;Single Slot Recurrence Along Frequency Network (SisRafNet)&#65292;&#26159;&#22522;&#20110;&#23545;&#39057;&#29575;&#38388;&#36890;&#36947;&#24207;&#21015;&#34892;&#20026;&#30340;&#26032;&#39062;&#30740;&#31350;&#12290;&#21033;&#29992;&#26080;&#32447;&#20449;&#36947;&#22312;&#39057;&#29575;&#19978;&#20855;&#26377;&#39640;&#24230;&#30456;&#20851;&#24615;&#36825;&#19968;&#20107;&#23454;&#65292;&#25105;&#20204;&#22312;&#21333;&#20010;OFDM&#26102;&#38553;&#20869;&#37319;&#29992;&#20102;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#25216;&#26415;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#36890;&#24120;&#19982;&#24490;&#29615;&#30456;&#20851;&#26041;&#27861;&#30456;&#20851;&#30340;&#24310;&#36831;&#21644;&#20869;&#23384;&#38480;&#21046;&#12290;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20449;&#36947;&#20272;&#35745;&#25216;&#26415;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;SisRafNet&#20855;&#26377;&#26356;&#20248;&#36234;&#30340;&#20272;&#35745;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#22810;&#31181;&#20449;&#22122;&#27604;&#19979;&#38024;&#23545;&#22810;&#20010;&#31526;&#21512;&#31532;&#19977;&#20195;&#21512;&#20316;&#20249;&#20276;&#35745;&#21010;(3GPP)&#26631;&#20934;&#30340;&#20449;&#36947;&#22330;&#26223;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a deep-learning-based channel estimation scheme in an orthogonal frequency division multiplexing (OFDM) system. Our proposed method, named Single Slot Recurrence Along Frequency Network (SisRafNet), is based on a novel study of recurrent models for exploiting sequential behavior of channels across frequencies. Utilizing the fact that wireless channels have a high degree of correlation across frequencies, we employ recurrent neural network techniques within a single OFDM slot, thus overcoming the latency and memory constraints typically associated with recurrence based methods. The proposed SisRafNet delivers superior estimation performance compared to existing deep-learning-based channel estimation techniques and the performance has been validated on a wide range of 3rd Generation Partnership Project (3GPP) compliant channel scenarios at multiple signal-to-noise ratios.
&lt;/p&gt;</description></item><item><title>ECGformer&#26159;&#19968;&#31181;&#21033;&#29992;transformer&#26550;&#26500;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#23545;&#24515;&#30005;&#22270;&#25968;&#25454;&#20013;&#30340;&#24515;&#24459;&#22833;&#24120;&#36827;&#34892;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2401.05434</link><description>&lt;p&gt;
ECGformer: &#21033;&#29992;transformer&#36827;&#34892;&#24515;&#30005;&#22270;&#24515;&#25615;&#24515;&#24459;&#22833;&#24120;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
ECGformer: Leveraging transformer for ECG heartbeat arrhythmia classification. (arXiv:2401.05434v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05434
&lt;/p&gt;
&lt;p&gt;
ECGformer&#26159;&#19968;&#31181;&#21033;&#29992;transformer&#26550;&#26500;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#23545;&#24515;&#30005;&#22270;&#25968;&#25454;&#20013;&#30340;&#24515;&#24459;&#22833;&#24120;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#24459;&#22833;&#24120;&#26159;&#24515;&#25615;&#19981;&#35268;&#21017;&#30340;&#19968;&#31181;&#24773;&#20917;&#65292;&#21487;&#20197;&#30001;&#24515;&#33039;&#19981;&#21516;&#21306;&#22495;&#24341;&#36215;&#65292;&#23548;&#33268;&#24515;&#25615;&#24555;&#36895;&#12289;&#32531;&#24930;&#25110;&#19981;&#35268;&#21017;&#12290;&#24515;&#30005;&#22270;&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#35786;&#26029;&#24037;&#20855;&#65292;&#29992;&#20110;&#26816;&#27979;&#24515;&#33039;&#30340;&#19981;&#35268;&#21017;&#21644;&#24322;&#24120;&#65292;&#20197;&#20415;&#19987;&#23478;&#20998;&#26512;&#24515;&#33039;&#30340;&#30005;&#20449;&#21495;&#65292;&#35782;&#21035;&#22797;&#26434;&#30340;&#27169;&#24335;&#21644;&#20559;&#31163;&#27491;&#24120;&#30340;&#24773;&#20917;&#12290;&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#65292;&#24050;&#32463;&#36827;&#34892;&#20102;&#35768;&#22810;&#30740;&#31350;&#65292;&#20197;&#24320;&#21457;&#22522;&#20110;ECG&#25968;&#25454;&#30340;&#33258;&#21160;&#21270;&#24515;&#25615;&#20998;&#31867;&#26041;&#27861;&#12290;&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#22788;&#29702;&#21508;&#31181;&#21307;&#23398;&#25361;&#25112;&#26041;&#38754;&#23637;&#31034;&#20986;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#23558;transformer&#20316;&#20026;&#24207;&#21015;&#22788;&#29702;&#30340;&#27169;&#22411;&#26550;&#26500;&#12290;&#36890;&#36807;&#21033;&#29992;transformer&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;ECGformer&#27169;&#22411;&#65292;&#29992;&#20110;&#23545;&#24515;&#30005;&#22270;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#21508;&#31181;&#24515;&#24459;&#22833;&#24120;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#20351;&#29992;MIT-BIH&#21644;PTB da
&lt;/p&gt;
&lt;p&gt;
An arrhythmia, also known as a dysrhythmia, refers to an irregular heartbeat. There are various types of arrhythmias that can originate from different areas of the heart, resulting in either a rapid, slow, or irregular heartbeat. An electrocardiogram (ECG) is a vital diagnostic tool used to detect heart irregularities and abnormalities, allowing experts to analyze the heart's electrical signals to identify intricate patterns and deviations from the norm. Over the past few decades, numerous studies have been conducted to develop automated methods for classifying heartbeats based on ECG data. In recent years, deep learning has demonstrated exceptional capabilities in tackling various medical challenges, particularly with transformers as a model architecture for sequence processing. By leveraging the transformers, we developed the ECGformer model for the classification of various arrhythmias present in electrocardiogram data. We assessed the suggested approach using the MIT-BIH and PTB da
&lt;/p&gt;</description></item><item><title>TEN-GUARD&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24352;&#37327;&#20998;&#35299;&#26041;&#27861;&#26816;&#27979;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#21518;&#38376;&#25915;&#20987;&#30340;&#26032;&#26041;&#27861;&#65292;&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#20855;&#26377;&#22810;&#20010;&#20248;&#21183;&#65292;&#21253;&#25324;&#33021;&#22815;&#21516;&#26102;&#20998;&#26512;&#22810;&#20010;&#27169;&#22411;&#65292;&#22312;&#21508;&#31181;&#32593;&#32476;&#26550;&#26500;&#19978;&#24037;&#20316;&#65292;&#19981;&#23545;&#35302;&#21457;&#22120;&#30340;&#24615;&#36136;&#20570;&#20219;&#20309;&#20551;&#35774;&#65292;&#24182;&#19988;&#35745;&#31639;&#25928;&#29575;&#39640;&#12290;</title><link>http://arxiv.org/abs/2401.05432</link><description>&lt;p&gt;
TEN-GUARD: &#20351;&#29992;&#24352;&#37327;&#20998;&#35299;&#26816;&#27979;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
TEN-GUARD: Tensor Decomposition for Backdoor Attack Detection in Deep Neural Networks. (arXiv:2401.05432v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05432
&lt;/p&gt;
&lt;p&gt;
TEN-GUARD&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24352;&#37327;&#20998;&#35299;&#26041;&#27861;&#26816;&#27979;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#21518;&#38376;&#25915;&#20987;&#30340;&#26032;&#26041;&#27861;&#65292;&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#20855;&#26377;&#22810;&#20010;&#20248;&#21183;&#65292;&#21253;&#25324;&#33021;&#22815;&#21516;&#26102;&#20998;&#26512;&#22810;&#20010;&#27169;&#22411;&#65292;&#22312;&#21508;&#31181;&#32593;&#32476;&#26550;&#26500;&#19978;&#24037;&#20316;&#65292;&#19981;&#23545;&#35302;&#21457;&#22120;&#30340;&#24615;&#36136;&#20570;&#20219;&#20309;&#20551;&#35774;&#65292;&#24182;&#19988;&#35745;&#31639;&#25928;&#29575;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#29992;&#20110;&#35757;&#32451;&#23427;&#20204;&#30340;&#25968;&#25454;&#38598;&#36234;&#26469;&#36234;&#22823;&#65292;&#23558;&#23427;&#20204;&#38598;&#25104;&#21040;&#30740;&#31350;&#21644;&#21830;&#19994;&#39033;&#30446;&#20013;&#30340;&#40664;&#35748;&#26041;&#27861;&#26159;&#19979;&#36733;&#39044;&#35757;&#32451;&#27169;&#22411;&#24182;&#36827;&#34892;&#24494;&#35843;&#12290;&#20294;&#26159;&#36825;&#20123;&#27169;&#22411;&#30340;&#26469;&#28304;&#19981;&#30830;&#23450;&#65292;&#21487;&#33021;&#23384;&#22312;&#38544;&#34255;&#30340;&#24694;&#24847;&#34892;&#20026;&#65292;&#22914;&#29305;&#27931;&#20234;&#26408;&#39532;&#25110;&#21518;&#38376;&#65292;&#20854;&#20013;&#23545;&#36755;&#20837;&#36827;&#34892;&#23567;&#30340;&#25913;&#21464;&#65288;&#35302;&#21457;&#22120;&#65289;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#20135;&#29983;&#38169;&#35823;&#30340;&#36755;&#20986;&#65288;&#20363;&#22914;&#65292;&#35823;&#20998;&#31867;&#65289;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21518;&#38376;&#26816;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20004;&#31181;&#24212;&#29992;&#20110;&#32593;&#32476;&#28608;&#27963;&#30340;&#24352;&#37327;&#20998;&#35299;&#26041;&#27861;&#12290;&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;&#26816;&#27979;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#22810;&#20010;&#20248;&#21183;&#65292;&#21253;&#25324;&#33021;&#22815;&#21516;&#26102;&#20998;&#26512;&#22810;&#20010;&#27169;&#22411;&#65292;&#22312;&#21508;&#31181;&#32593;&#32476;&#26550;&#26500;&#19978;&#24037;&#20316;&#65292;&#19981;&#23545;&#29992;&#20110;&#25913;&#21464;&#32593;&#32476;&#34892;&#20026;&#30340;&#35302;&#21457;&#22120;&#30340;&#24615;&#36136;&#20570;&#20219;&#20309;&#20551;&#35774;&#65292;&#24182;&#19988;&#35745;&#31639;&#25928;&#29575;&#39640;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#26816;&#27979;&#27969;&#31243;&#30340;&#35814;&#32454;&#25551;&#36848;&#20197;&#21450;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
As deep neural networks and the datasets used to train them get larger, the default approach to integrating them into research and commercial projects is to download a pre-trained model and fine tune it. But these models can have uncertain provenance, opening up the possibility that they embed hidden malicious behavior such as trojans or backdoors, where small changes to an input (triggers) can cause the model to produce incorrect outputs (e.g., to misclassify). This paper introduces a novel approach to backdoor detection that uses two tensor decomposition methods applied to network activations. This has a number of advantages relative to existing detection methods, including the ability to analyze multiple models at the same time, working across a wide variety of network architectures, making no assumptions about the nature of triggers used to alter network behavior, and being computationally efficient. We provide a detailed description of the detection pipeline along with results on 
&lt;/p&gt;</description></item><item><title>TRLS&#26159;&#19968;&#31181;&#36890;&#36807;&#22768;&#35889;&#22270;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;&#30340;&#21307;&#23398;&#20449;&#21495;&#22788;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#26102;&#38388;&#39057;&#29575;RNN&#20174;&#22686;&#24378;&#22768;&#35889;&#22270;&#20013;&#25552;&#21462;&#20986;&#26356;&#22810;&#20449;&#24687;&#65292;&#24182;&#22312;&#21307;&#23398;&#20449;&#21495;&#20998;&#31867;&#20013;&#26174;&#31034;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.05431</link><description>&lt;p&gt;
TRLS:&#19968;&#31181;&#22522;&#20110;&#22768;&#35889;&#22270;&#30340;&#21307;&#23398;&#20449;&#21495;&#22788;&#29702;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
TRLS: A Time Series Representation Learning Framework via Spectrogram for Medical Signal Processing. (arXiv:2401.05431v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05431
&lt;/p&gt;
&lt;p&gt;
TRLS&#26159;&#19968;&#31181;&#36890;&#36807;&#22768;&#35889;&#22270;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;&#30340;&#21307;&#23398;&#20449;&#21495;&#22788;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#26102;&#38388;&#39057;&#29575;RNN&#20174;&#22686;&#24378;&#22768;&#35889;&#22270;&#20013;&#25552;&#21462;&#20986;&#26356;&#22810;&#20449;&#24687;&#65292;&#24182;&#22312;&#21307;&#23398;&#20449;&#21495;&#20998;&#31867;&#20013;&#26174;&#31034;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26080;&#26631;&#31614;&#26102;&#38388;&#24207;&#21015;&#20013;&#65292;&#25552;&#20986;&#20102;&#29992;&#20110;&#21307;&#23398;&#20449;&#21495;&#22788;&#29702;&#30340;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#12290;&#23613;&#31649;&#22312;&#20043;&#21069;&#30340;&#24037;&#20316;&#20013;&#21462;&#24471;&#20102;&#35768;&#22810;&#21331;&#36234;&#36827;&#23637;&#65292;&#20294;&#25105;&#20204;&#35266;&#23519;&#21040;&#25552;&#21462;&#30340;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#20173;&#28982;&#19981;&#33021;&#24456;&#22909;&#22320;&#27867;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22768;&#35889;&#22270;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#65288;&#21307;&#23398;&#20449;&#21495;&#65289;&#34920;&#31034;&#23398;&#20064;&#30340;&#26694;&#26550;&#65288;TRLS&#65289;&#65292;&#20197;&#33719;&#24471;&#26356;&#22810;&#20449;&#24687;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#23558;&#36755;&#20837;&#30340;&#26102;&#22495;&#21307;&#23398;&#20449;&#21495;&#36716;&#21270;&#20026;&#22768;&#35889;&#22270;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#31216;&#20026;&#26102;&#38388;&#39057;&#29575;RNN&#65288;TFRNN&#65289;&#30340;&#26102;&#39057;&#32534;&#30721;&#22120;&#65292;&#20174;&#22686;&#24378;&#30340;&#22768;&#35889;&#22270;&#20013;&#25429;&#25417;&#26356;&#31283;&#20581;&#30340;&#22810;&#23610;&#24230;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;TRLS&#20197;&#22768;&#35889;&#22270;&#20316;&#20026;&#36755;&#20837;&#65292;&#20855;&#26377;&#20004;&#31181;&#19981;&#21516;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#24335;&#65292;&#24182;&#26368;&#22823;&#21270;&#27491;&#26679;&#26412;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#35268;&#36991;&#20102;&#35774;&#35745;&#36127;&#26679;&#26412;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#23545;&#22235;&#20010;&#30495;&#23454;&#21307;&#23398;&#20449;&#21495;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#21307;&#23398;&#20449;&#21495;&#20998;&#31867;&#35780;&#20272;&#34920;&#26126;&#65292;TRLS&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Representation learning frameworks in unlabeled time series have been proposed for medical signal processing. Despite the numerous excellent progresses have been made in previous works, we observe the representation extracted for the time series still does not generalize well. In this paper, we present a Time series (medical signal) Representation Learning framework via Spectrogram (TRLS) to get more informative representations. We transform the input time-domain medical signals into spectrograms and design a time-frequency encoder named Time Frequency RNN (TFRNN) to capture more robust multi-scale representations from the augmented spectrograms. Our TRLS takes spectrogram as input with two types of different data augmentations and maximizes the similarity between positive ones, which effectively circumvents the problem of designing negative samples. Our evaluation of four real-world medical signal datasets focusing on medical signal classification shows that TRLS is superior to the ex
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#20851;&#31995;&#22270;&#30340;&#32929;&#31080;&#36235;&#21183;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#21160;&#24577;&#30340;&#22810;&#20851;&#31995;&#32929;&#31080;&#22270;&#26469;&#24314;&#27169;&#32929;&#31080;&#20043;&#38388;&#22797;&#26434;&#30340;&#26102;&#21464;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#25193;&#25955;&#36807;&#31243;&#20248;&#21270;&#22270;&#34920;&#31034;&#12290;&#26368;&#32456;&#30340;&#22270;&#34920;&#31034;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#21508;&#20010;&#32929;&#31080;&#20869;&#37096;&#30340;&#26102;&#38388;&#29305;&#24449;&#65292;&#21516;&#26102;&#32771;&#34385;&#32929;&#31080;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2401.05430</link><description>&lt;p&gt;
&#20855;&#26377;&#24182;&#34892;&#20445;&#30041;&#30340;&#22810;&#20851;&#31995;&#22270;&#25193;&#25955;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#32929;&#31080;&#36235;&#21183;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Multi-relational Graph Diffusion Neural Network with Parallel Retention for Stock Trends Classification. (arXiv:2401.05430v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05430
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#20851;&#31995;&#22270;&#30340;&#32929;&#31080;&#36235;&#21183;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#21160;&#24577;&#30340;&#22810;&#20851;&#31995;&#32929;&#31080;&#22270;&#26469;&#24314;&#27169;&#32929;&#31080;&#20043;&#38388;&#22797;&#26434;&#30340;&#26102;&#21464;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#25193;&#25955;&#36807;&#31243;&#20248;&#21270;&#22270;&#34920;&#31034;&#12290;&#26368;&#32456;&#30340;&#22270;&#34920;&#31034;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#21508;&#20010;&#32929;&#31080;&#20869;&#37096;&#30340;&#26102;&#38388;&#29305;&#24449;&#65292;&#21516;&#26102;&#32771;&#34385;&#32929;&#31080;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32929;&#31080;&#36235;&#21183;&#20998;&#31867;&#20173;&#28982;&#26159;&#19968;&#39033;&#22522;&#26412;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#32929;&#31080;&#20043;&#38388;&#21644;&#20869;&#37096;&#20043;&#38388;&#30340;&#22797;&#26434;&#26102;&#21464;&#21160;&#21147;&#23398;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#39044;&#27979;&#22810;&#20010;&#32929;&#31080;&#30340;&#26410;&#26469;&#36208;&#21183;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#29983;&#25104;&#21160;&#24577;&#22810;&#20851;&#31995;&#32929;&#31080;&#22270;&#26469;&#24314;&#27169;&#32929;&#31080;&#20043;&#38388;&#22797;&#26434;&#30340;&#26102;&#21464;&#20851;&#31995;&#12290;&#36825;&#26159;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#36793;&#29983;&#25104;&#31639;&#27861;&#23454;&#29616;&#30340;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#20449;&#24687;&#29109;&#21644;&#20449;&#21495;&#33021;&#37327;&#26469;&#37327;&#21270;&#27599;&#20010;&#20132;&#26131;&#26085;&#30340;&#32929;&#31080;&#20043;&#38388;&#20851;&#31995;&#30340;&#24378;&#24230;&#21644;&#26041;&#21521;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#38543;&#26426;&#30340;&#22810;&#20851;&#31995;&#25193;&#25955;&#36807;&#31243;&#36827;&#19968;&#27493;&#20248;&#21270;&#36825;&#20123;&#21021;&#22987;&#22270;&#65292;&#33258;&#36866;&#24212;&#22320;&#23398;&#20064;&#20219;&#21153;&#26368;&#20248;&#36793;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#24182;&#34892;&#20445;&#30041;&#30340;&#35299;&#32806;&#34920;&#31034;&#23398;&#20064;&#26041;&#26696;&#26469;&#33719;&#24471;&#26368;&#32456;&#30340;&#22270;&#34920;&#31034;&#12290;&#36825;&#31181;&#31574;&#30053;&#26356;&#22909;&#22320;&#25429;&#25417;&#20102;&#21508;&#20010;&#32929;&#31080;&#20869;&#37096;&#30340;&#29420;&#29305;&#26102;&#38388;&#29305;&#24449;&#65292;&#21516;&#26102;&#20063;&#33021;&#22815;&#32771;&#34385;&#32929;&#31080;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stock trend classification remains a fundamental yet challenging task, owing to the intricate time-evolving dynamics between and within stocks. To tackle these two challenges, we propose a graph-based representation learning approach aimed at predicting the future movements of multiple stocks. Initially, we model the complex time-varying relationships between stocks by generating dynamic multi-relational stock graphs. This is achieved through a novel edge generation algorithm that leverages information entropy and signal energy to quantify the intensity and directionality of inter-stock relations on each trading day. Then, we further refine these initial graphs through a stochastic multi-relational diffusion process, adaptively learning task-optimal edges. Subsequently, we implement a decoupled representation learning scheme with parallel retention to obtain the final graph representation. This strategy better captures the unique temporal features within individual stocks while also ca
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#26694;&#26550;&#29992;&#20110;&#22312;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#20219;&#21153;&#20013;&#39640;&#25928;&#21033;&#29992;&#25968;&#25454;&#65292;&#21516;&#26102;&#32771;&#34385;&#20256;&#24863;&#22120;&#27169;&#24577;&#21644;&#37319;&#26679;&#29575;&#30340;&#20248;&#21270;&#65292;&#36890;&#36807;&#35774;&#35745;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#26469;&#25351;&#23548;&#20256;&#24863;&#22120;&#27169;&#24577;&#21644;&#37319;&#26679;&#29575;&#30340;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2401.05426</link><description>&lt;p&gt;
CoSS&#65306;&#38024;&#23545;&#25968;&#25454;&#39640;&#25928;AI&#30340;&#20256;&#24863;&#22120;&#21644;&#37319;&#26679;&#29575;&#20248;&#21270;&#22312;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
CoSS: Co-optimizing Sensor and Sampling Rate for Data-Efficient AI in Human Activity Recognition. (arXiv:2401.05426v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05426
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#26694;&#26550;&#29992;&#20110;&#22312;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#20219;&#21153;&#20013;&#39640;&#25928;&#21033;&#29992;&#25968;&#25454;&#65292;&#21516;&#26102;&#32771;&#34385;&#20256;&#24863;&#22120;&#27169;&#24577;&#21644;&#37319;&#26679;&#29575;&#30340;&#20248;&#21270;&#65292;&#36890;&#36807;&#35774;&#35745;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#26469;&#25351;&#23548;&#20256;&#24863;&#22120;&#27169;&#24577;&#21644;&#37319;&#26679;&#29575;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#31070;&#32463;&#32593;&#32476;&#25216;&#26415;&#30340;&#36827;&#27493;&#26174;&#33879;&#25552;&#39640;&#20102;&#21033;&#29992;&#22810;&#20010;&#26102;&#38388;&#24207;&#21015;&#20256;&#24863;&#22120;&#36827;&#34892;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#30340;&#25928;&#26524;&#12290;&#34429;&#28982;&#20351;&#29992;&#22823;&#37327;&#20256;&#24863;&#22120;&#21644;&#39640;&#37319;&#26679;&#29575;&#36890;&#24120;&#21487;&#20197;&#25552;&#39640;&#32467;&#26524;&#65292;&#20294;&#24448;&#24448;&#20250;&#23548;&#33268;&#25968;&#25454;&#20302;&#25928;&#21644;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#19981;&#24517;&#35201;&#25193;&#23637;&#65292;&#32473;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#23454;&#38469;&#24212;&#29992;&#24102;&#26469;&#25361;&#25112;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#24341;&#20837;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;HAR&#20219;&#21153;&#20013;&#30340;&#25968;&#25454;&#39640;&#25928;&#21033;&#29992;&#65292;&#21516;&#26102;&#32771;&#34385;&#20256;&#24863;&#22120;&#27169;&#24577;&#21644;&#37319;&#26679;&#29575;&#30340;&#20248;&#21270;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#35774;&#35745;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#65292;&#31216;&#20026;&#8220;&#26435;&#37325;&#20998;&#25968;&#8221;&#65292;&#23427;&#20204;&#35780;&#20272;&#35757;&#32451;&#38454;&#27573;&#20013;&#27599;&#20010;&#20256;&#24863;&#22120;&#27169;&#24577;&#21644;&#37319;&#26679;&#29575;&#30340;&#37325;&#35201;&#24615;&#12290;&#36825;&#20123;&#20998;&#25968;&#25351;&#23548;&#20256;&#24863;&#22120;&#27169;&#24577;&#21644;&#37319;&#26679;&#29575;&#30340;&#36873;&#25321;&#12290;&#20462;&#21098;&#26041;&#27861;&#20801;&#35768;&#29992;&#25143;&#22312;&#35745;&#31639;&#39044;&#31639;&#21644;&#24615;&#33021;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#65292;&#26681;&#25454;&#36873;&#25321;&#20256;&#24863;&#22120;&#27169;&#24577;&#21644;&#37319;&#26679;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in Artificial Neural Networks have significantly improved human activity recognition using multiple time-series sensors. While employing numerous sensors with high-frequency sampling rates usually improves the results, it often leads to data inefficiency and unnecessary expansion of the ANN, posing a challenge for their practical deployment on edge devices. Addressing these issues, our work introduces a pragmatic framework for data-efficient utilization in HAR tasks, considering the optimization of both sensor modalities and sampling rate simultaneously. Central to our approach are the designed trainable parameters, termed 'Weight Scores,' which assess the significance of each sensor modality and sampling rate during the training phase. These scores guide the sensor modalities and sampling rate selection. The pruning method allows users to make a trade-off between computational budgets and performance by selecting the sensor modalities and sampling rates according t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#24178;&#25200;&#19988;&#36731;&#37327;&#32423;&#30340;&#32819;&#25140;&#31995;&#32479;EarSD&#65292;&#36890;&#36807;&#27979;&#37327;&#29992;&#25143;&#32819;&#21518;&#37096;&#20301;&#30340;&#29983;&#29702;&#20449;&#21495;&#65292;&#23454;&#29616;&#20102;&#23545;&#30315;&#30187;&#21457;&#20316;&#30340;&#36830;&#32493;&#26816;&#27979;&#12290;&#36825;&#31181;&#31995;&#32479;&#30456;&#27604;&#20256;&#32479;&#30340;&#22522;&#20110;&#22836;&#30382;&#30340;&#33041;&#30005;&#22270;&#27979;&#35797;&#20855;&#26377;&#25104;&#26412;&#20302;&#12289;&#20415;&#25658;&#24615;&#22909;&#12289;&#20351;&#29992;&#33298;&#36866;&#30340;&#20248;&#28857;&#12290;</title><link>http://arxiv.org/abs/2401.05425</link><description>&lt;p&gt;
&#19968;&#31181;&#26080;&#24178;&#25200;&#19988;&#36731;&#37327;&#32423;&#30340;&#32819;&#25140;&#31995;&#32479;&#29992;&#20110;&#36830;&#32493;&#30315;&#30187;&#21457;&#20316;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
An Unobtrusive and Lightweight Ear-worn System for Continuous Epileptic Seizure Detection. (arXiv:2401.05425v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05425
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#24178;&#25200;&#19988;&#36731;&#37327;&#32423;&#30340;&#32819;&#25140;&#31995;&#32479;EarSD&#65292;&#36890;&#36807;&#27979;&#37327;&#29992;&#25143;&#32819;&#21518;&#37096;&#20301;&#30340;&#29983;&#29702;&#20449;&#21495;&#65292;&#23454;&#29616;&#20102;&#23545;&#30315;&#30187;&#21457;&#20316;&#30340;&#36830;&#32493;&#26816;&#27979;&#12290;&#36825;&#31181;&#31995;&#32479;&#30456;&#27604;&#20256;&#32479;&#30340;&#22522;&#20110;&#22836;&#30382;&#30340;&#33041;&#30005;&#22270;&#27979;&#35797;&#20855;&#26377;&#25104;&#26412;&#20302;&#12289;&#20415;&#25658;&#24615;&#22909;&#12289;&#20351;&#29992;&#33298;&#36866;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30315;&#30187;&#26159;&#20840;&#29699;&#26368;&#24120;&#35265;&#30340;&#31070;&#32463;&#30142;&#30149;&#20043;&#19968;&#65292;&#24433;&#21709;&#30528;&#20840;&#29699;&#32422;5000&#19975;&#20154;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#22914;&#26524;&#33021;&#24471;&#21040;&#27491;&#30830;&#30340;&#35786;&#26029;&#21644;&#27835;&#30103;&#65292;&#39640;&#36798;70%&#30340;&#30315;&#30187;&#24739;&#32773;&#21487;&#20197;&#26080;&#30315;&#30187;&#21457;&#20316;&#22320;&#29983;&#27963;&#65292;&#32780;&#19968;&#31181;&#21487;&#38752;&#30340;&#30417;&#27979;&#30315;&#30187;&#21457;&#20316;&#30340;&#25216;&#26415;&#21487;&#20197;&#25552;&#39640;&#37027;&#20123;&#19981;&#26029;&#38754;&#20020;&#38543;&#26426;&#21457;&#20316;&#24656;&#24807;&#30340;&#24739;&#32773;&#30340;&#29983;&#27963;&#36136;&#37327;&#12290;&#23613;&#31649;&#22522;&#20110;&#22836;&#30382;&#30340;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#27979;&#35797;&#26159;&#35786;&#26029;&#30315;&#30187;&#30340;&#40644;&#37329;&#26631;&#20934;&#65292;&#20294;&#35813;&#26041;&#27861;&#25104;&#26412;&#39640;&#12289;&#38656;&#35201;&#20303;&#38498;&#27835;&#30103;&#12289;&#38656;&#35201;&#29087;&#32451;&#30340;&#25805;&#20316;&#20154;&#21592;&#65292;&#24182;&#19988;&#23545;&#29992;&#25143;&#26469;&#35828;&#19981;&#33298;&#36866;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EarSD&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#36731;&#37327;&#32423;&#12289;&#26080;&#24178;&#25200;&#21644;&#31038;&#20250;&#25509;&#21463;&#24230;&#39640;&#30340;&#32819;&#25140;&#31995;&#32479;&#65292;&#36890;&#36807;&#27979;&#37327;&#29992;&#25143;&#32819;&#21518;&#37096;&#20301;&#30340;&#29983;&#29702;&#20449;&#21495;&#26469;&#26816;&#27979;&#30315;&#30187;&#21457;&#20316;&#30340;&#24320;&#22987;&#12290;EarSD&#21253;&#25324;&#19968;&#20010;&#38598;&#25104;&#30340;&#33258;&#23450;&#20041;&#20256;&#24863;&#12289;&#35745;&#31639;&#21644;&#36890;&#20449;&#30005;&#36335;&#26495;&#65292;&#29992;&#20110;&#25910;&#38598;&#21644;&#25918;&#22823;&#24863;&#20852;&#36259;&#30340;&#20449;&#21495;&#65292;&#21435;&#38500;&#36816;&#21160;&#20266;&#24433;&#21644;&#29615;&#22659;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;
Epilepsy is one of the most common neurological diseases globally, affecting around 50 million people worldwide. Fortunately, up to 70 percent of people with epilepsy could live seizure-free if properly diagnosed and treated, and a reliable technique to monitor the onset of seizures could improve the quality of life of patients who are constantly facing the fear of random seizure attacks. The scalp-based EEG test, despite being the gold standard for diagnosing epilepsy, is costly, necessitates hospitalization, demands skilled professionals for operation, and is discomforting for users. In this paper, we propose EarSD, a novel lightweight, unobtrusive, and socially acceptable ear-worn system to detect epileptic seizure onsets by measuring the physiological signals from behind the user's ears. EarSD includes an integrated custom-built sensing, computing, and communication PCB to collect and amplify the signals of interest, remove the noises caused by motion artifacts and environmental im
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;PEEKC&#25968;&#25454;&#38598;&#21644;TrueLearn Python&#24211;&#65292;&#21487;&#29992;&#20110;&#24314;&#27169;&#25945;&#32946;&#35270;&#39057;&#30340;&#21442;&#19982;&#24230;&#12290;TrueLearn&#27169;&#22411;&#31995;&#21015;&#36981;&#24490;"&#24320;&#25918;&#23398;&#20064;&#32773;"&#27010;&#24565;&#65292;&#25552;&#20379;&#21487;&#25193;&#23637;&#30340;&#22312;&#32447;&#27169;&#22411;&#20197;&#21450;&#36739;&#39640;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.05424</link><description>&lt;p&gt;
&#29992;&#20110;&#24314;&#27169;&#25945;&#32946;&#35270;&#39057;&#21442;&#19982;&#24230;&#30340;&#24037;&#20855;&#31665;
&lt;/p&gt;
&lt;p&gt;
A Toolbox for Modelling Engagement with Educational Videos. (arXiv:2401.05424v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05424
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;PEEKC&#25968;&#25454;&#38598;&#21644;TrueLearn Python&#24211;&#65292;&#21487;&#29992;&#20110;&#24314;&#27169;&#25945;&#32946;&#35270;&#39057;&#30340;&#21442;&#19982;&#24230;&#12290;TrueLearn&#27169;&#22411;&#31995;&#21015;&#36981;&#24490;"&#24320;&#25918;&#23398;&#20064;&#32773;"&#27010;&#24565;&#65292;&#25552;&#20379;&#21487;&#25193;&#23637;&#30340;&#22312;&#32447;&#27169;&#22411;&#20197;&#21450;&#36739;&#39640;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#36827;&#27493;&#21644;&#24212;&#29992;&#65292;&#23558;&#25945;&#32946;&#20010;&#24615;&#21270;&#21040;&#20840;&#29699;&#20154;&#21475;&#21487;&#33021;&#25104;&#20026;&#26410;&#26469;&#26032;&#25945;&#32946;&#31995;&#32479;&#30340;&#22522;&#30707;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;PEEKC&#25968;&#25454;&#38598;&#21644;TrueLearn Python&#24211;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#19968;&#31995;&#21015;&#22312;&#32447;&#23398;&#20064;&#32773;&#29366;&#24577;&#27169;&#22411;&#21644;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#23545;&#20110;&#20419;&#36827;&#23398;&#20064;&#32773;&#21442;&#19982;&#24230;&#24314;&#27169;&#30740;&#31350;&#33267;&#20851;&#37325;&#35201;&#12290;TrueLearn&#27169;&#22411;&#31995;&#21015;&#36981;&#24490;&#8220;&#24320;&#25918;&#23398;&#20064;&#32773;&#8221;&#27010;&#24565;&#65292;&#20351;&#29992;&#20154;&#24615;&#21270;&#30340;&#29992;&#25143;&#34920;&#31034;&#27861;&#36827;&#34892;&#35774;&#35745;&#12290;&#36825;&#19968;&#21487;&#25193;&#23637;&#30340;&#22312;&#32447;&#27169;&#22411;&#31995;&#21015;&#36824;&#24110;&#21161;&#32456;&#31471;&#29992;&#25143;&#21487;&#35270;&#21270;&#23398;&#20064;&#32773;&#27169;&#22411;&#65292;&#36825;&#22312;&#26410;&#26469;&#21487;&#33021;&#20419;&#36827;&#29992;&#25143;&#19982;&#33258;&#24049;&#30340;&#27169;&#22411;/&#25512;&#33616;&#31995;&#32479;&#36827;&#34892;&#20132;&#20114;&#12290;&#20016;&#23500;&#30340;&#25991;&#26723;&#21644;&#32534;&#30721;&#31034;&#20363;&#20351;&#35813;&#24211;&#23545;&#26426;&#22120;&#23398;&#20064;&#24320;&#21457;&#20154;&#21592;&#21644;&#25945;&#32946;&#25968;&#25454;&#25366;&#25496;&#19982;&#23398;&#20064;&#20998;&#26512;&#23454;&#36341;&#32773;&#37117;&#38750;&#24120;&#26131;&#20110;&#35775;&#38382;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20102;&#25968;&#25454;&#38598;&#21644;&#24211;&#30340;&#23454;&#29992;&#24615;&#65292;&#39044;&#27979;&#24615;&#33021;&#26126;&#26174;&#36229;&#36807;&#23545;&#27604;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the advancement and utility of Artificial Intelligence (AI), personalising education to a global population could be a cornerstone of new educational systems in the future. This work presents the PEEKC dataset and the TrueLearn Python library, which contains a dataset and a series of online learner state models that are essential to facilitate research on learner engagement modelling.TrueLearn family of models was designed following the "open learner" concept, using humanly-intuitive user representations. This family of scalable, online models also help end-users visualise the learner models, which may in the future facilitate user interaction with their models/recommenders. The extensive documentation and coding examples make the library highly accessible to both machine learning developers and educational data mining and learning analytics practitioners. The experiments show the utility of both the dataset and the library with predictive performance significantly exceeding compa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;WildGEN&#30340;&#27010;&#24565;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#26041;&#27861;&#29983;&#25104;&#37326;&#29983;&#21160;&#29289;&#38271;&#26102;&#38388;&#33539;&#22260;&#20869;&#30340;&#36816;&#21160;&#36712;&#36857;&#12290;&#36890;&#36807;&#24179;&#28369;&#28388;&#27874;&#22120;&#23545;&#29983;&#25104;&#36712;&#36857;&#36827;&#34892;&#21518;&#22788;&#29702;&#65292;&#20197;&#25552;&#39640;&#29983;&#25104;&#36712;&#36857;&#30340;&#36136;&#37327;&#12290;&#35780;&#20272;&#32467;&#26524;&#36890;&#36807;&#35270;&#35273;&#26816;&#26597;&#21644;&#35745;&#31639;&#29983;&#25104;&#36712;&#36857;&#19982;&#30495;&#23454;&#36712;&#36857;&#20043;&#38388;&#30340;Hausdorff&#36317;&#31163;&#12290;</title><link>http://arxiv.org/abs/2401.05421</link><description>&lt;p&gt;
WildGEN&#65306;&#29992;&#20110;&#37326;&#29983;&#21160;&#29289;&#30340;&#38271;&#26102;&#38388;&#36712;&#36857;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
WildGEN: Long-horizon Trajectory Generation for Wildlife. (arXiv:2401.05421v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05421
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;WildGEN&#30340;&#27010;&#24565;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#26041;&#27861;&#29983;&#25104;&#37326;&#29983;&#21160;&#29289;&#38271;&#26102;&#38388;&#33539;&#22260;&#20869;&#30340;&#36816;&#21160;&#36712;&#36857;&#12290;&#36890;&#36807;&#24179;&#28369;&#28388;&#27874;&#22120;&#23545;&#29983;&#25104;&#36712;&#36857;&#36827;&#34892;&#21518;&#22788;&#29702;&#65292;&#20197;&#25552;&#39640;&#29983;&#25104;&#36712;&#36857;&#30340;&#36136;&#37327;&#12290;&#35780;&#20272;&#32467;&#26524;&#36890;&#36807;&#35270;&#35273;&#26816;&#26597;&#21644;&#35745;&#31639;&#29983;&#25104;&#36712;&#36857;&#19982;&#30495;&#23454;&#36712;&#36857;&#20043;&#38388;&#30340;Hausdorff&#36317;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36712;&#36857;&#29983;&#25104;&#26159;&#30740;&#31350;&#34892;&#20154;&#12289;&#36710;&#36742;&#21644;&#37326;&#29983;&#21160;&#29289;&#31227;&#21160;&#30340;&#37325;&#35201;&#38382;&#39064;&#12290;&#29983;&#25104;&#30340;&#36712;&#36857;&#26377;&#21161;&#20110;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#30340;&#35757;&#32451;&#35821;&#26009;&#24211;&#20016;&#23500;&#65292;&#24182;&#21487;&#29992;&#20110;&#20419;&#36827;&#27169;&#25311;&#20219;&#21153;&#12290;&#36825;&#22312;&#37326;&#29983;&#21160;&#29289;&#39046;&#22495;&#23588;&#20026;&#37325;&#35201;&#65292;&#22240;&#20026;&#33719;&#21462;&#39069;&#22806;&#30340;&#30495;&#23454;&#25968;&#25454;&#30340;&#25104;&#26412;&#21487;&#33021;&#38750;&#24120;&#26114;&#36149;&#12289;&#32791;&#26102;&#19988;&#28041;&#21450;&#20262;&#29702;&#32771;&#34385;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;WildGEN&#65306;&#19968;&#31181;&#27010;&#24565;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#30340;&#26041;&#27861;&#65292;&#22312;&#31232;&#30095;&#30340;&#30495;&#23454;&#26679;&#26412;&#38598;&#19978;&#33719;&#24471;&#37326;&#40517;&#22312;&#38271;&#26102;&#38388;&#33539;&#22260;&#20869;&#23637;&#31034;&#30340;&#36816;&#21160;&#29305;&#24449;&#12290;&#38543;&#21518;&#65292;&#36890;&#36807;&#24179;&#28369;&#28388;&#27874;&#22120;&#23545;&#29983;&#25104;&#30340;&#36712;&#36857;&#36827;&#34892;&#21518;&#22788;&#29702;&#65292;&#20197;&#20943;&#23569;&#36807;&#24230;&#28216;&#33633;&#12290;&#25105;&#20204;&#36890;&#36807;&#35270;&#35273;&#26816;&#26597;&#21644;&#29983;&#25104;&#36712;&#36857;&#19982;&#30495;&#23454;&#36712;&#36857;&#20043;&#38388;&#30340;Hausdorff&#36317;&#31163;&#30340;&#35745;&#31639;&#26469;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Trajectory generation is an important concern in pedestrian, vehicle, and wildlife movement studies. Generated trajectories help enrich the training corpus in relation to deep learning applications, and may be used to facilitate simulation tasks. This is especially significant in the wildlife domain, where the cost of obtaining additional real data can be prohibitively expensive, time-consuming, and bear ethical considerations. In this paper, we introduce WildGEN: a conceptual framework that addresses this challenge by employing a Variational Auto-encoders (VAEs) based method for the acquisition of movement characteristics exhibited by wild geese over a long horizon using a sparse set of truth samples. A subsequent post-processing step of the generated trajectories is performed based on smoothing filters to reduce excessive wandering. Our evaluation is conducted through visual inspection and the computation of the Hausdorff distance between the generated and real trajectories. In addit
&lt;/p&gt;</description></item><item><title>Holographic Metasurface Transceivers (HMTs) are being used as cost-effective substitutes for large antenna arrays for beamforming in Millimeter and TeraHertz wave communication. In this work, a learning algorithm is developed to optimize beamforming in far-field regions, by using a fixed-budget multi-armed bandit framework to maximize received signal strength. The algorithm exploits the parametric form of channel gains and works with the discrete values of phase-shifting parameters.</title><link>http://arxiv.org/abs/2401.05420</link><description>&lt;p&gt;
HoloBeam:&#23398;&#20064;&#36828;&#22330;&#20840;&#24687;&#20803;&#34920;&#38754;&#25910;&#21457;&#22120;&#20013;&#30340;&#26368;&#20339;&#27874;&#26463;&#25104;&#22411;
&lt;/p&gt;
&lt;p&gt;
HoloBeam: Learning Optimal Beamforming in Far-Field Holographic Metasurface Transceivers. (arXiv:2401.05420v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05420
&lt;/p&gt;
&lt;p&gt;
Holographic Metasurface Transceivers (HMTs) are being used as cost-effective substitutes for large antenna arrays for beamforming in Millimeter and TeraHertz wave communication. In this work, a learning algorithm is developed to optimize beamforming in far-field regions, by using a fixed-budget multi-armed bandit framework to maximize received signal strength. The algorithm exploits the parametric form of channel gains and works with the discrete values of phase-shifting parameters.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#24687;&#20803;&#34920;&#38754;&#25910;&#21457;&#22120;&#65288;HMT&#65289;&#27491;&#20197;&#32463;&#27982;&#23454;&#24800;&#30340;&#26041;&#24335;&#25104;&#20026;&#27627;&#31859;&#21644;&#22826;&#36203;&#20857;&#27874;&#36890;&#20449;&#20013;&#27874;&#26463;&#25104;&#22411;&#30340;&#26367;&#20195;&#21697;&#12290;&#28982;&#32780;&#65292;&#22312;HMT&#20013;&#36890;&#36807;&#27874;&#26463;&#25104;&#22411;&#23454;&#29616;&#25152;&#38656;&#20449;&#36947;&#22686;&#30410;&#38656;&#35201;&#36866;&#24403;&#35774;&#32622;&#22823;&#37327;&#20803;&#32032;&#30340;&#30456;&#31227;&#65292;&#36825;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26368;&#20339;&#30456;&#31227;&#20381;&#36182;&#20110;&#25509;&#25910;&#22120;&#20301;&#32622;&#65292;&#36825;&#21487;&#33021;&#26159;&#26410;&#30693;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;{\it &#22266;&#23450;&#39044;&#31639;&#22810;&#33218;&#32769;&#34382;&#26426;&#31639;&#27861;}&#24320;&#21457;&#20102;&#19968;&#20010;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#22312;&#36828;&#22330;&#21306;&#22495;&#36827;&#34892;&#27874;&#26463;&#25104;&#22411;&#24182;&#26368;&#22823;&#21270;&#25509;&#25910;&#20449;&#21495;&#24378;&#24230;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#21517;&#20026; \Algo&#65292;&#21033;&#29992;&#20102;&#27874;&#26463;&#30340;&#36890;&#36947;&#22686;&#30410;&#21442;&#25968;&#24418;&#24335;&#65292;&#23427;&#21487;&#20197;&#29992;&#20004;&#20010;{\it &#30456;&#31227;&#21442;&#25968;}&#34920;&#31034;&#12290;&#21363;&#20351;&#36827;&#34892;&#21442;&#25968;&#21270;&#65292;&#38382;&#39064;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#30456;&#31227;&#21442;&#25968;&#37319;&#29992;&#36830;&#32493;&#20540;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;\HB &#20351;&#29992;&#30456;&#31227;&#21442;&#25968;&#30340;&#31163;&#25955;&#20540;&#24182;&#21033;&#29992;&#20854;&#23376;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Holographic Metasurface Transceivers (HMTs) are emerging as cost-effective substitutes to large antenna arrays for beamforming in Millimeter and TeraHertz wave communication. However, to achieve desired channel gains through beamforming in HMT, phase-shifts of a large number of elements need to be appropriately set, which is challenging. Also, these optimal phase-shifts depend on the location of the receivers, which could be unknown. In this work, we develop a learning algorithm using a {\it fixed-budget multi-armed bandit framework} to beamform and maximize received signal strength at the receiver for far-field regions. Our algorithm, named \Algo exploits the parametric form of channel gains of the beams, which can be expressed in terms of two {\it phase-shifting parameters}. Even after parameterization, the problem is still challenging as phase-shifting parameters take continuous values. To overcome this, {\it\HB} works with the discrete values of phase-shifting parameters and exploi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#23558;&#38477;&#32500;&#21644;&#20915;&#31574;&#36793;&#30028;&#24212;&#29992;&#20110;&#29616;&#26377;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#31361;&#26174;&#25968;&#25454;&#20013;&#30340;&#27169;&#24335;&#21644;&#32858;&#31867;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#25552;&#39640;&#36712;&#36857;&#26631;&#35760;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.05418</link><description>&lt;p&gt;
ANALYTiC: &#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20915;&#31574;&#36793;&#30028;&#21644;&#38477;&#32500;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
ANALYTiC: Understanding Decision Boundaries and Dimensionality Reduction in Machine Learning. (arXiv:2401.05418v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05418
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#23558;&#38477;&#32500;&#21644;&#20915;&#31574;&#36793;&#30028;&#24212;&#29992;&#20110;&#29616;&#26377;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#31361;&#26174;&#25968;&#25454;&#20013;&#30340;&#27169;&#24335;&#21644;&#32858;&#31867;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#25552;&#39640;&#36712;&#36857;&#26631;&#35760;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32039;&#20945;&#20415;&#25658;&#35774;&#22791;&#30340;&#20986;&#29616;&#32473;&#25105;&#20204;&#24102;&#26469;&#20102;&#19968;&#25209;&#21487;&#20197;&#29992;&#20110;&#25512;&#26029;&#36235;&#21183;&#21644;&#27169;&#24335;&#30340;&#36319;&#36394;&#36816;&#21160;&#25968;&#25454;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#29616;&#26377;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#22522;&#30784;&#19978;&#24212;&#29992;&#38477;&#32500;&#21644;&#20915;&#31574;&#36793;&#30028;&#30340;&#32452;&#21512;&#65292;&#31361;&#26174;&#25968;&#25454;&#20013;&#30340;&#27169;&#24335;&#21644;&#32858;&#31867;&#12290;&#25105;&#20204;&#20351;&#29992;&#19977;&#20010;&#19981;&#21516;&#30340;&#36712;&#36857;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#23454;&#39564;&#20998;&#26512;&#65292;&#26088;&#22312;&#21033;&#29992;&#24050;&#26631;&#35760;&#30340;&#25968;&#25454;&#24182;&#25552;&#39640;&#23427;&#20204;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;&#36825;&#20123;&#32452;&#21512;&#26041;&#27861;&#22312;&#25552;&#39640;&#36712;&#36857;&#26631;&#35760;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#26412;&#30740;&#31350;&#20026;&#23558;&#26426;&#22120;&#23398;&#20064;&#21644;&#35270;&#35273;&#26041;&#27861;&#22312;&#19978;&#19979;&#25991;&#20013;&#26356;&#24191;&#27867;&#22320;&#38598;&#25104;&#25171;&#19979;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of compact, handheld devices has given us a pool of tracked movement data that could be used to infer trends and patterns that can be made to use. With this flooding of various trajectory data of animals, humans, vehicles, etc., the idea of ANALYTiC originated, using active learning to infer semantic annotations from the trajectories by learning from sets of labeled data. This study explores the application of dimensionality reduction and decision boundaries in combination with the already present active learning, highlighting patterns and clusters in data. We test these features with three different trajectory datasets with objective of exploiting the the already labeled data and enhance their interpretability. Our experimental analysis exemplifies the potential of these combined methodologies in improving the efficiency and accuracy of trajectory labeling. This study serves as a stepping-stone towards the broader integration of machine learning and visual methods in contex
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;WDSNet&#30340;&#23567;&#27874;&#21160;&#24577;&#36873;&#25321;&#32593;&#32476;&#65292;&#29992;&#20110;&#26234;&#33021;&#36873;&#25321;&#36866;&#24403;&#30340;&#23567;&#27874;&#22522;&#20989;&#25968;&#20197;&#22686;&#24378;&#24815;&#24615;&#20256;&#24863;&#22120;&#20449;&#21495;&#12290;&#27492;&#22806;&#65292;&#35770;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#31867;&#21035;&#34920;&#31034;&#26426;&#21046;&#65292;&#29992;&#20110;&#25552;&#39640;&#23567;&#27874;&#22522;&#20989;&#25968;&#30340;&#36873;&#25321;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.05416</link><description>&lt;p&gt;
&#24815;&#24615;&#20256;&#24863;&#22120;&#20449;&#21495;&#22686;&#24378;&#30340;&#23567;&#27874;&#21160;&#24577;&#36873;&#25321;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Wavelet Dynamic Selection Network for Inertial Sensor Signal Enhancement. (arXiv:2401.05416v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05416
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;WDSNet&#30340;&#23567;&#27874;&#21160;&#24577;&#36873;&#25321;&#32593;&#32476;&#65292;&#29992;&#20110;&#26234;&#33021;&#36873;&#25321;&#36866;&#24403;&#30340;&#23567;&#27874;&#22522;&#20989;&#25968;&#20197;&#22686;&#24378;&#24815;&#24615;&#20256;&#24863;&#22120;&#20449;&#21495;&#12290;&#27492;&#22806;&#65292;&#35770;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#31867;&#21035;&#34920;&#31034;&#26426;&#21046;&#65292;&#29992;&#20110;&#25552;&#39640;&#23567;&#27874;&#22522;&#20989;&#25968;&#30340;&#36873;&#25321;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#23039;&#24577;&#21644;&#36816;&#21160;&#24863;&#24212;&#37096;&#20214;&#65292;&#24815;&#24615;&#20256;&#24863;&#22120;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#20415;&#25658;&#35774;&#22791;&#20013;&#12290;&#28982;&#32780;&#65292;&#24815;&#24615;&#20256;&#24863;&#22120;&#30340;&#20005;&#37325;&#35823;&#24046;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#21151;&#33021;&#65292;&#29305;&#21035;&#26159;&#36712;&#36857;&#24674;&#22797;&#21644;&#35821;&#20041;&#35782;&#21035;&#12290;&#23567;&#27874;&#20316;&#20026;&#20027;&#27969;&#30340;&#20449;&#21495;&#22788;&#29702;&#26041;&#27861;&#65292;&#30001;&#20110;&#20016;&#23500;&#22810;&#26679;&#30340;&#23567;&#27874;&#22522;&#20989;&#25968;&#32780;&#34987;&#35465;&#20026;&#20449;&#21495;&#30340;&#25968;&#23398;&#26174;&#24494;&#38236;&#12290;&#28982;&#32780;&#65292;&#24815;&#24615;&#20256;&#24863;&#22120;&#30340;&#22797;&#26434;&#22122;&#22768;&#31867;&#22411;&#21644;&#24212;&#29992;&#22330;&#26223;&#20351;&#24471;&#36873;&#25321;&#36866;&#24403;&#30340;&#23567;&#27874;&#22522;&#20989;&#25968;&#21464;&#24471;&#22256;&#38590;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23567;&#27874;&#21160;&#24577;&#36873;&#25321;&#32593;&#32476;&#65288;WDSNet&#65289;&#65292;&#23427;&#33021;&#26234;&#33021;&#22320;&#20026;&#21487;&#21464;&#24815;&#24615;&#20449;&#21495;&#36873;&#25321;&#36866;&#24403;&#30340;&#23567;&#27874;&#22522;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#25797;&#38271;&#20174;&#36755;&#20837;&#25968;&#25454;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#20294;&#24573;&#35270;&#20102;&#23398;&#20064;&#30446;&#26631;&#31867;&#21035;&#30340;&#29305;&#24449;&#65292;&#36825;&#23545;&#20110;&#25552;&#39640;&#31867;&#21035;&#24863;&#30693;&#33021;&#21147;&#12289;&#25913;&#21892;&#23567;&#27874;&#22522;&#20989;&#25968;&#30340;&#36873;&#25321;&#26159;&#24517;&#35201;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31867;&#21035;&#34920;&#31034;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
As attitude and motion sensing components, inertial sensors are widely used in various portable devices. But the severe errors of inertial sensors restrain their function, especially the trajectory recovery and semantic recognition. As a mainstream signal processing method, wavelet is hailed as the mathematical microscope of signal due to the plentiful and diverse wavelet basis functions. However, complicated noise types and application scenarios of inertial sensors make selecting wavelet basis perplexing. To this end, we propose a wavelet dynamic selection network (WDSNet), which intelligently selects the appropriate wavelet basis for variable inertial signals. In addition, existing deep learning architectures excel at extracting features from input data but neglect to learn the characteristics of target categories, which is essential to enhance the category awareness capability, thereby improving the selection of wavelet basis. Therefore, we propose a category representation mechanis
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#22240;&#26524;&#24615;&#30340;&#35282;&#24230;&#31995;&#32479;&#30740;&#31350;&#20102;&#37329;&#34701;&#39046;&#22495;&#20013;&#30340;&#19977;&#20010;&#22256;&#22659;&#65306;&#26102;&#38388;&#20998;&#36776;&#29575;&#19981;&#21305;&#37197;&#12289;&#38750;&#24179;&#31283;&#24615;&#21644;&#26410;&#30693;&#22240;&#26524;&#22240;&#32032;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2401.05414</link><description>&lt;p&gt;
&#20851;&#20110;&#37329;&#34701;&#20013;&#30340;&#19977;&#20010;&#22240;&#26524;&#24615;&#22256;&#22659;: &#26102;&#38388;&#20998;&#36776;&#29575;&#12289;&#38750;&#24179;&#31283;&#24615;&#21644;&#28508;&#22312;&#22240;&#32032;
&lt;/p&gt;
&lt;p&gt;
On the Three Demons in Causality in Finance: Time Resolution, Nonstationarity, and Latent Factors. (arXiv:2401.05414v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05414
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#22240;&#26524;&#24615;&#30340;&#35282;&#24230;&#31995;&#32479;&#30740;&#31350;&#20102;&#37329;&#34701;&#39046;&#22495;&#20013;&#30340;&#19977;&#20010;&#22256;&#22659;&#65306;&#26102;&#38388;&#20998;&#36776;&#29575;&#19981;&#21305;&#37197;&#12289;&#38750;&#24179;&#31283;&#24615;&#21644;&#26410;&#30693;&#22240;&#26524;&#22240;&#32032;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37329;&#34701;&#25968;&#25454;&#26412;&#36136;&#19978;&#26159;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#22240;&#27492;&#23384;&#22312;&#19977;&#20010;&#22522;&#26412;&#38382;&#39064;&#65306;&#26102;&#38388;&#20998;&#36776;&#29575;&#19981;&#21305;&#37197;&#12289;&#20998;&#24067;&#30340;&#26102;&#21464;&#24615;-&#38750;&#24179;&#31283;&#24615;&#20197;&#21450;&#37325;&#35201;&#20294;&#26410;&#30693;/&#26410;&#35266;&#27979;&#30340;&#22240;&#26524;&#22240;&#32032;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#22240;&#26524;&#24615;&#30340;&#35282;&#24230;&#31995;&#32479;&#22320;&#30740;&#31350;&#37329;&#34701;&#20013;&#30340;&#36825;&#19977;&#20010;&#22256;&#22659;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#22312;&#22240;&#26524;&#24615;&#30340;&#32972;&#26223;&#19979;&#25552;&#20986;&#20102;&#26032;&#39062;&#32780;&#26377;&#21551;&#21457;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#19968;&#20010;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Financial data is generally time series in essence and thus suffers from three fundamental issues: the mismatch in time resolution, the time-varying property of the distribution - nonstationarity, and causal factors that are important but unknown/unobserved. In this paper, we follow a causal perspective to systematically look into these three demons in finance. Specifically, we reexamine these issues in the context of causality, which gives rise to a novel and inspiring understanding of how the issues can be addressed. Following this perspective, we provide systematic solutions to these problems, which hopefully would serve as a foundation for future research in the area.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;RawECGNet&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20351;&#29992;&#21407;&#22987;&#30340;&#21333;&#23548;&#32852;&#24515;&#30005;&#22270;&#26469;&#26816;&#27979;&#25151;&#39076;&#21644;&#24515;&#25151;&#25169;&#21160;&#30340;&#21457;&#20316;&#12290;&#19982;&#22522;&#20110;&#33410;&#24459;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;RawECGNet&#21033;&#29992;&#20102;&#26356;&#22810;&#30340;&#24418;&#24577;&#23398;&#20449;&#24687;&#65292;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.05411</link><description>&lt;p&gt;
RawECGNet: &#20174;&#21407;&#22987;&#24515;&#30005;&#22270;&#20013;&#36827;&#34892;&#25151;&#39076;&#26816;&#27979;&#30340;&#28145;&#24230;&#23398;&#20064;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
RawECGNet: Deep Learning Generalization for Atrial Fibrillation Detection from the Raw ECG. (arXiv:2401.05411v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05411
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;RawECGNet&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20351;&#29992;&#21407;&#22987;&#30340;&#21333;&#23548;&#32852;&#24515;&#30005;&#22270;&#26469;&#26816;&#27979;&#25151;&#39076;&#21644;&#24515;&#25151;&#25169;&#21160;&#30340;&#21457;&#20316;&#12290;&#19982;&#22522;&#20110;&#33410;&#24459;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;RawECGNet&#21033;&#29992;&#20102;&#26356;&#22810;&#30340;&#24418;&#24577;&#23398;&#20449;&#24687;&#65292;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24341;&#35328;&#65306;&#21033;&#29992;&#38271;&#26399;&#12289;&#26080;&#21019;&#30340;&#24515;&#30005;&#22270;&#35760;&#24405;&#36827;&#34892;&#25151;&#39076;&#65288;AF&#65289;&#30340;&#26816;&#27979;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#34920;&#29616;&#20986;&#24456;&#39640;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#33410;&#24459;&#30340;&#26041;&#27861;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#19981;&#21516;&#24515;&#30005;&#27874;&#24418;&#25152;&#20256;&#36882;&#30340;&#24418;&#24577;&#23398;&#20449;&#24687;&#65292;&#29305;&#21035;&#26159;f&#27874;&#12290;&#22240;&#27492;&#65292;&#36825;&#31181;&#27169;&#22411;&#30340;&#24615;&#33021;&#21487;&#33021;&#21463;&#21040;&#22266;&#26377;&#38480;&#21046;&#12290;&#26041;&#27861;&#65306;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;RawECGNet&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20351;&#29992;&#21407;&#22987;&#12289;&#21333;&#23548;&#32852;&#30340;&#24515;&#30005;&#22270;&#26469;&#26816;&#27979;&#25151;&#39076;&#21644;&#24515;&#25151;&#25169;&#21160;&#65288;AFl&#65289;&#30340;&#21457;&#20316;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;RawECGNet&#22312;&#20004;&#20010;&#22806;&#37096;&#25968;&#25454;&#38598;&#19978;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#32771;&#34385;&#20102;&#22320;&#29702;&#12289;&#31181;&#26063;&#21644;&#23548;&#32852;&#20301;&#32622;&#30340;&#20998;&#24067;&#21464;&#21270;&#12290;RawECGNet&#36824;&#19982;&#19968;&#31181;&#21517;&#20026;ArNet2&#30340;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#33410;&#24459;&#20449;&#24687;&#20316;&#20026;&#36755;&#20837;&#12290;&#32467;&#26524;&#65306;&#20351;&#29992;RawECGNet&#65292;&#22312;&#22806;&#37096;&#27979;&#35797;&#38598;&#20013;&#21508;&#20010;&#23548;&#32852;&#30340;F1&#24471;&#20998;&#22914;&#19979;
&lt;/p&gt;
&lt;p&gt;
Introduction: Deep learning models for detecting episodes of atrial fibrillation (AF) using rhythm information in long-term, ambulatory ECG recordings have shown high performance. However, the rhythm-based approach does not take advantage of the morphological information conveyed by the different ECG waveforms, particularly the f-waves. As a result, the performance of such models may be inherently limited. Methods: To address this limitation, we have developed a deep learning model, named RawECGNet, to detect episodes of AF and atrial flutter (AFl) using the raw, single-lead ECG. We compare the generalization performance of RawECGNet on two external data sets that account for distribution shifts in geography, ethnicity, and lead position. RawECGNet is further benchmarked against a state-of-the-art deep learning model, named ArNet2, which utilizes rhythm information as input. Results: Using RawECGNet, the results for the different leads in the external test sets in terms of the F1 score
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23460;&#20869;&#29615;&#22659;&#20013;&#36890;&#36807;&#21033;&#29992;UWB&#22810;&#38745;&#24577;&#26080;&#32447;&#30005;&#26469;&#23454;&#29616;&#26080;&#35774;&#22791;&#30340;&#20154;&#20307;&#29366;&#24577;&#20272;&#35745;&#30340;&#26041;&#24335;&#65292;&#21487;&#20197;&#20272;&#35745;&#20154;&#30340;&#20301;&#32622;&#12289;&#27963;&#21160;&#21644;&#21306;&#22495;&#20869;&#30340;&#20154;&#25968;&#12290;</title><link>http://arxiv.org/abs/2401.05410</link><description>&lt;p&gt;
&#20351;&#29992;UWB&#22810;&#38745;&#24577;&#26080;&#32447;&#30005;&#23454;&#29616;&#26080;&#35774;&#22791;&#30340;&#20154;&#20307;&#29366;&#24577;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Device-Free Human State Estimation using UWB Multi-Static Radios. (arXiv:2401.05410v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05410
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23460;&#20869;&#29615;&#22659;&#20013;&#36890;&#36807;&#21033;&#29992;UWB&#22810;&#38745;&#24577;&#26080;&#32447;&#30005;&#26469;&#23454;&#29616;&#26080;&#35774;&#22791;&#30340;&#20154;&#20307;&#29366;&#24577;&#20272;&#35745;&#30340;&#26041;&#24335;&#65292;&#21487;&#20197;&#20272;&#35745;&#20154;&#30340;&#20301;&#32622;&#12289;&#27963;&#21160;&#21644;&#21306;&#22495;&#20869;&#30340;&#20154;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20154;&#20307;&#29366;&#24577;&#20272;&#35745;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#23460;&#20869;&#29615;&#22659;&#20013;&#20272;&#35745;&#20154;&#30340;&#20301;&#32622;&#21644;&#27963;&#21160;&#65292;&#32780;&#26080;&#38656;&#20182;&#20204;&#25658;&#24102;&#29305;&#23450;&#35774;&#22791;&#12290;&#20026;&#20102;&#23454;&#29616;&#8220;&#26080;&#35774;&#22791;&#8221;&#30340;&#23450;&#20301;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#20998;&#24067;&#22312;&#24863;&#20852;&#36259;&#30340;&#29615;&#22659;&#20013;&#30340;&#23569;&#37327;&#20302;&#25104;&#26412;&#36229;&#23485;&#24102;&#65288;UWB&#65289;&#20256;&#24863;&#22120;&#12290;&#20026;&#20102;&#20174;&#29615;&#22659;&#20013;&#20165;&#21453;&#23556;&#20154;&#20307;&#30340;UWB&#20449;&#21495;&#20013;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#20272;&#35745;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#19968;&#20010;&#21487;&#20197;&#23398;&#20064;&#25512;&#29702;&#30340;&#28145;&#24230;&#32593;&#32476;&#12290;&#30828;&#20214;&#35774;&#32622;&#21253;&#25324;&#29992;&#20110;&#24863;&#30693;&#30340;&#21830;&#19994;&#29616;&#25104;&#65288;COTS&#65289;&#21333;&#22825;&#32447;UWB&#27169;&#22359;&#65292;&#37197;&#23545;&#26641;&#33683;&#27966;&#21333;&#20803;&#29992;&#20110;&#35745;&#31639;&#22788;&#29702;&#21644;&#25968;&#25454;&#20256;&#36755;&#12290;&#25105;&#20204;&#21033;&#29992;UWB&#20256;&#24863;&#22120;&#30340;&#20449;&#36947;&#33033;&#20914;&#21709;&#24212;&#65288;CIR&#65289;&#27979;&#37327;&#26469;&#20272;&#35745;&#22312;&#32473;&#23450;&#21306;&#22495;&#20869;&#30340;&#20154;&#20307;&#29366;&#24577; - &#21253;&#25324;&#20301;&#32622;&#21644;&#27963;&#21160;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21487;&#20197;&#20272;&#35745;&#21344;&#25454;&#35813;&#24863;&#20852;&#36259;&#21306;&#22495;&#30340;&#20154;&#25968;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;
&lt;/p&gt;
&lt;p&gt;
We present a human state estimation framework that allows us to estimate the location, and even the activities, of people in an indoor environment without the requirement that they carry a specific devices with them. To achieve this "device free" localization we use a small number of low-cost Ultra-Wide Band (UWB) sensors distributed across the environment of interest. To achieve high quality estimation from the UWB signals merely reflected of people in the environment, we exploit a deep network that can learn to make inferences. The hardware setup consists of commercial off-the-shelf (COTS) single antenna UWB modules for sensing, paired with Raspberry PI units for computational processing and data transfer. We make use of the channel impulse response (CIR) measurements from the UWB sensors to estimate the human state - comprised of location and activity - in a given area. Additionally, we can also estimate the number of humans that occupy this region of interest. In our approach, firs
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#38024;&#23545;&#33041;&#30005;&#22270;&#25968;&#25454;&#20013;&#30340;&#20266;&#36857;&#26816;&#27979;&#21644;&#20998;&#31867;&#65292;&#23545;&#21313;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#22312;&#20845;&#31181;&#24120;&#29992;&#34920;&#31034;&#26041;&#27861;&#19978;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#65292;&#24182;&#21457;&#29616;&#26576;&#20123;&#34920;&#31034;&#26041;&#27861;&#22312;&#31361;&#20986;&#25968;&#25454;&#30340;&#20449;&#22122;&#27604;&#26041;&#38754;&#26356;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2401.05409</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#20687;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#34920;&#31034;&#65306;&#33041;&#30005;&#22270;&#20266;&#36857;&#26816;&#27979;&#30340;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Image-based Data Representations of Time Series: A Comparative Analysis in EEG Artifact Detection. (arXiv:2401.05409v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05409
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#38024;&#23545;&#33041;&#30005;&#22270;&#25968;&#25454;&#20013;&#30340;&#20266;&#36857;&#26816;&#27979;&#21644;&#20998;&#31867;&#65292;&#23545;&#21313;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#22312;&#20845;&#31181;&#24120;&#29992;&#34920;&#31034;&#26041;&#27861;&#19978;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#65292;&#24182;&#21457;&#29616;&#26576;&#20123;&#34920;&#31034;&#26041;&#27861;&#22312;&#31361;&#20986;&#25968;&#25454;&#30340;&#20449;&#22122;&#27604;&#26041;&#38754;&#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26367;&#20195;&#25968;&#25454;&#34920;&#31034;&#26159;&#22686;&#24378;&#19979;&#28216;&#27169;&#22411;&#24615;&#33021;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#31665;&#20013;&#23384;&#22312;&#22823;&#37327;&#36825;&#26679;&#30340;&#34920;&#31034;&#26041;&#27861;&#65292;&#39046;&#22495;&#20869;&#32570;&#20047;&#23545;&#27599;&#31181;&#34920;&#31034;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#30340;&#27604;&#36739;&#29702;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#33041;&#30005;&#22270;&#25968;&#25454;&#20013;&#30340;&#20266;&#36857;&#26816;&#27979;&#21644;&#20998;&#31867;&#20316;&#20026;&#27979;&#35797;&#24179;&#21488;&#65292;&#35780;&#20272;&#20102;&#20845;&#31181;&#24120;&#29992;&#34920;&#31034;&#26041;&#27861;&#19978;&#30340;&#21313;&#19968;&#20010;&#27969;&#34892;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#34429;&#28982;&#34920;&#31034;&#30340;&#36873;&#25321;&#28041;&#21450;&#20559;&#24046;&#21644;&#26041;&#24046;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#20294;&#26576;&#20123;&#34920;&#31034;&#22312;&#31361;&#20986;&#25968;&#25454;&#30340;&#20449;&#22122;&#27604;&#26041;&#38754;&#23454;&#38469;&#19978;&#26356;&#26377;&#25928;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#38024;&#23545;&#33041;&#30005;&#22270;&#25968;&#25454;&#30340;&#32467;&#26524;&#65292;&#24182;&#24320;&#28304;&#20102;&#25105;&#20204;&#30340;&#27979;&#35797;&#26694;&#26550;&#65292;&#20197;&#20415;&#26410;&#26469;&#36827;&#34892;&#36825;&#31181;&#26041;&#27861;&#30340;&#27604;&#36739;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Alternative data representations are powerful tools that augment the performance of downstream models. However, there is an abundance of such representations within the machine learning toolbox, and the field lacks a comparative understanding of the suitability of each representation method.  In this paper, we propose artifact detection and classification within EEG data as a testbed for profiling image-based data representations of time series data. We then evaluate eleven popular deep learning architectures on each of six commonly-used representation methods.  We find that, while the choice of representation entails a choice within the tradeoff between bias and variance, certain representations are practically more effective in highlighting features which increase the signal-to-noise ratio of the data. We present our results on EEG data, and open-source our testing framework to enable future comparative analyses in this vein.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#21487;&#31359;&#25140;&#35774;&#22791;&#21644;&#33258;&#25105;&#25253;&#21578;&#25514;&#26045;&#65292;&#26088;&#22312;&#35299;&#20915;&#23454;&#39564;&#23460;&#30740;&#31350;&#21644;&#29616;&#23454;&#22330;&#26223;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#24182;&#21462;&#24471;&#20102;&#26377;&#20851;&#29992;&#25143;&#24773;&#24863;&#20215;&#20540;&#30340;&#26377;&#24076;&#26395;&#30340;&#20998;&#31867;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.05408</link><description>&lt;p&gt;
&#20174;&#21487;&#31359;&#25140;&#35774;&#22791;&#20013;&#35299;&#30721;&#24773;&#24863;&#20215;&#20540;&#65306;&#25105;&#20204;&#30340;&#25968;&#25454;&#33021;&#21542;&#25581;&#31034;&#25105;&#20204;&#30495;&#23454;&#30340;&#24773;&#24863;&#65311;
&lt;/p&gt;
&lt;p&gt;
Decoding Emotional Valence from Wearables: Can Our Data Reveal Our True Feelings?. (arXiv:2401.05408v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05408
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#21487;&#31359;&#25140;&#35774;&#22791;&#21644;&#33258;&#25105;&#25253;&#21578;&#25514;&#26045;&#65292;&#26088;&#22312;&#35299;&#20915;&#23454;&#39564;&#23460;&#30740;&#31350;&#21644;&#29616;&#23454;&#22330;&#26223;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#24182;&#21462;&#24471;&#20102;&#26377;&#20851;&#29992;&#25143;&#24773;&#24863;&#20215;&#20540;&#30340;&#26377;&#24076;&#26395;&#30340;&#20998;&#31867;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#26816;&#27979;&#21644;&#36319;&#36394;&#24773;&#24863;&#29366;&#24577;&#26377;&#21161;&#20110;&#24110;&#21161;&#21508;&#31181;&#24515;&#29702;&#20581;&#24247;&#29366;&#20917;&#30340;&#20010;&#20307;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#22312;&#23454;&#39564;&#23460;&#29615;&#22659;&#20013;&#21033;&#29992;&#21487;&#31359;&#25140;&#35774;&#22791;&#25429;&#25417;&#20102;&#29983;&#29702;&#20449;&#21495;&#65292;&#20026;&#20102;&#35299;&#29983;&#29702;&#21453;&#24212;&#21644;&#24515;&#29702;&#29366;&#24577;&#30340;&#20851;&#31995;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#65292;&#20294;&#26159;&#23558;&#36825;&#20123;&#21457;&#29616;&#36716;&#21270;&#21040;&#29616;&#23454;&#29983;&#27963;&#22330;&#26223;&#20173;&#22788;&#20110;&#21021;&#32423;&#38454;&#27573;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#28040;&#36153;&#32423;&#21487;&#31359;&#25140;&#35774;&#22791;&#21644;&#33258;&#25105;&#25253;&#21578;&#25514;&#26045;&#26469;&#24357;&#21512;&#23454;&#39564;&#23460;&#30740;&#31350;&#21644;&#29616;&#23454;&#22330;&#26223;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#21021;&#27493;&#30740;&#31350;&#65292;&#28041;&#21450;15&#21517;&#20581;&#24247;&#21442;&#19982;&#32773;&#65292;&#20197;&#35780;&#20272;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#25429;&#25417;&#29992;&#25143;&#24773;&#24863;&#20215;&#20540;&#30340;&#21487;&#31359;&#25140;&#35774;&#22791;&#30340;&#21151;&#25928;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;&#25910;&#38598;&#21040;&#30340;&#25968;&#25454;&#36827;&#34892;&#20102;&#21021;&#27493;&#20998;&#26512;&#65292;&#37325;&#28857;&#20851;&#27880;&#24773;&#24863;&#20215;&#20540;&#20998;&#31867;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#39640;&#21644;&#20302;&#27491;&#38754;&#24773;&#24863;&#20043;&#38388;&#20855;&#26377;&#24456;&#22909;&#30340;&#21306;&#20998;&#33021;&#21147;&#65292;F1&#20998;&#25968;&#36798;&#21040;0.65&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic detection and tracking of emotional states has the potential for helping individuals with various mental health conditions. While previous studies have captured physiological signals using wearable devices in laboratory settings, providing valuable insights into the relationship between physiological responses and mental states, the transfer of these findings to real-life scenarios is still in its nascent stages. Our research aims to bridge the gap between laboratory-based studies and real-life settings by leveraging consumer-grade wearables and self-report measures. We conducted a preliminary study involving 15 healthy participants to assess the efficacy of wearables in capturing user valence in real-world settings. In this paper, we present the initial analysis of the collected data, focusing primarily on the results of valence classification. Our findings demonstrate promising results in distinguishing between high and low positive valence, achieving an F1 score of 0.65. T
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#23545;&#22810;&#20256;&#24863;&#22120;&#25968;&#25454;&#36827;&#34892;&#24443;&#24213;&#30340;&#39044;&#22788;&#29702;&#21644;&#29305;&#24449;&#36873;&#25321;&#65292;&#25104;&#21151;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23454;&#29616;&#20102;&#20914;&#20987;&#22368;&#33853;&#26816;&#27979;&#65292;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.05407</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#29305;&#24449;&#25490;&#24207;&#22312;&#22810;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#20914;&#20987;&#22368;&#33853;&#26816;&#27979;&#20107;&#20214;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Machine Learning and Feature Ranking for Impact Fall Detection Event Using Multisensor Data. (arXiv:2401.05407v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05407
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#23545;&#22810;&#20256;&#24863;&#22120;&#25968;&#25454;&#36827;&#34892;&#24443;&#24213;&#30340;&#39044;&#22788;&#29702;&#21644;&#29305;&#24449;&#36873;&#25321;&#65292;&#25104;&#21151;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23454;&#29616;&#20102;&#20914;&#20987;&#22368;&#33853;&#26816;&#27979;&#65292;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#20154;&#30340;&#36300;&#20498;&#65292;&#29305;&#21035;&#26159;&#32769;&#24180;&#20154;&#65292;&#21487;&#33021;&#23548;&#33268;&#20005;&#37325;&#30340;&#20260;&#23475;&#21644;&#24182;&#21457;&#30151;&#12290;&#22312;&#36300;&#20498;&#20107;&#20214;&#20013;&#26816;&#27979;&#20914;&#20987;&#30636;&#38388;&#23545;&#20110;&#21450;&#26102;&#25552;&#20379;&#24110;&#21161;&#21644;&#20943;&#23569;&#36127;&#38754;&#24433;&#21709;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#22810;&#20256;&#24863;&#22120;&#25968;&#25454;&#38598;&#24212;&#29992;&#24443;&#24213;&#30340;&#39044;&#22788;&#29702;&#25216;&#26415;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#30446;&#30340;&#26159;&#28040;&#38500;&#22122;&#38899;&#24182;&#25552;&#39640;&#25968;&#25454;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20351;&#29992;&#29305;&#24449;&#36873;&#25321;&#36807;&#31243;&#26469;&#35782;&#21035;&#22810;&#20256;&#24863;&#22120;UP-FALL&#25968;&#25454;&#38598;&#20013;&#26368;&#30456;&#20851;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#25928;&#29575;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#22810;&#20010;&#20256;&#24863;&#22120;&#30340;&#32467;&#26524;&#25968;&#25454;&#20449;&#24687;&#35780;&#20272;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#26816;&#27979;&#20914;&#20987;&#30636;&#38388;&#26041;&#38754;&#30340;&#25928;&#29575;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#20351;&#29992;&#21508;&#31181;&#35780;&#20272;&#25351;&#26631;&#35780;&#20272;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#22312;&#20914;&#20987;&#26816;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#65292;&#23637;&#31034;&#20102;&#21033;&#29992;&#22810;&#20256;&#24863;&#22120;&#25968;&#25454;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Falls among individuals, especially the elderly population, can lead to serious injuries and complications. Detecting impact moments within a fall event is crucial for providing timely assistance and minimizing the negative consequences. In this work, we aim to address this challenge by applying thorough preprocessing techniques to the multisensor dataset, the goal is to eliminate noise and improve data quality. Furthermore, we employ a feature selection process to identify the most relevant features derived from the multisensor UP-FALL dataset, which in turn will enhance the performance and efficiency of machine learning models. We then evaluate the efficiency of various machine learning models in detecting the impact moment using the resulting data information from multiple sensors. Through extensive experimentation, we assess the accuracy of our approach using various evaluation metrics. Our results achieve high accuracy rates in impact detection, showcasing the power of leveraging 
&lt;/p&gt;</description></item><item><title>RFRL Gym&#26159;&#19968;&#20010;&#29992;&#20110;&#35748;&#30693;&#26080;&#32447;&#30005;&#24212;&#29992;&#30340;&#24378;&#21270;&#23398;&#20064;&#27979;&#35797;&#24179;&#21488;&#65292;&#21487;&#20197;&#24110;&#21161;&#24320;&#21457;&#21644;&#27979;&#35797;RFRL&#25216;&#26415;&#65292;&#27169;&#25311;&#26080;&#32447;&#30005;&#39057;&#35889;&#29615;&#22659;&#65292;&#24182;&#23454;&#39564;&#19981;&#21516;&#30340;&#39057;&#35889;&#24863;&#30693;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2401.05406</link><description>&lt;p&gt;
RFRL Gym: &#29992;&#20110;&#35748;&#30693;&#26080;&#32447;&#30005;&#24212;&#29992;&#30340;&#24378;&#21270;&#23398;&#20064;&#27979;&#35797;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
RFRL Gym: A Reinforcement Learning Testbed for Cognitive Radio Applications. (arXiv:2401.05406v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05406
&lt;/p&gt;
&lt;p&gt;
RFRL Gym&#26159;&#19968;&#20010;&#29992;&#20110;&#35748;&#30693;&#26080;&#32447;&#30005;&#24212;&#29992;&#30340;&#24378;&#21270;&#23398;&#20064;&#27979;&#35797;&#24179;&#21488;&#65292;&#21487;&#20197;&#24110;&#21161;&#24320;&#21457;&#21644;&#27979;&#35797;RFRL&#25216;&#26415;&#65292;&#27169;&#25311;&#26080;&#32447;&#30005;&#39057;&#35889;&#29615;&#22659;&#65292;&#24182;&#23454;&#39564;&#19981;&#21516;&#30340;&#39057;&#35889;&#24863;&#30693;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#32447;&#30005;&#39057;&#29575;&#24378;&#21270;&#23398;&#20064;&#65288;RFRL&#65289;&#39044;&#35745;&#23558;&#25104;&#20026;&#19979;&#19968;&#20195;&#26080;&#32447;&#36890;&#20449;&#31995;&#32479;&#65288;&#29305;&#21035;&#26159;6G&#21644;&#19979;&#19968;&#20195;&#20891;&#20107;&#36890;&#20449;&#65289;&#20013;&#24191;&#27867;&#24212;&#29992;&#30340;&#25216;&#26415;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#33268;&#21147;&#20110;&#24320;&#21457;&#19968;&#20010;&#24037;&#20855;&#65292;&#20197;&#20419;&#36827;&#21033;&#29992;&#39057;&#35889;&#24863;&#30693;&#30340;RFRL&#25216;&#26415;&#30340;&#21457;&#23637;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#24037;&#20855;&#26088;&#22312;&#35299;&#20915;&#20004;&#20010;&#35748;&#30693;&#26080;&#32447;&#30005;&#24212;&#29992;&#65292;&#21363;&#21160;&#24577;&#39057;&#35889;&#25509;&#20837;&#21644;&#24178;&#25200;&#12290;&#20026;&#20102;&#35757;&#32451;&#21644;&#27979;&#35797;&#36825;&#20123;&#24212;&#29992;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#65292;&#38656;&#35201;&#19968;&#20010;&#27169;&#25311;&#29615;&#22659;&#26469;&#27169;&#25311;&#26080;&#32447;&#30005;&#39057;&#35889;&#20013;&#20195;&#29702;&#20154;&#23558;&#36935;&#21040;&#30340;&#26465;&#20214;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#36825;&#26679;&#19968;&#20010;&#29615;&#22659;&#65292;&#31216;&#20026;RFRL Gym&#12290;&#36890;&#36807;RFRL Gym&#65292;&#29992;&#25143;&#21487;&#20197;&#35774;&#35745;&#33258;&#24049;&#30340;&#22330;&#26223;&#26469;&#27169;&#25311;RL&#20195;&#29702;&#20154;&#22312;&#26080;&#32447;&#30005;&#39057;&#35889;&#20013;&#21487;&#33021;&#36935;&#21040;&#30340;&#24773;&#20917;&#65292;&#24182;&#23581;&#35797;&#19981;&#21516;&#30340;&#39057;&#35889;&#24863;&#30693;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Radio Frequency Reinforcement Learning (RFRL) is anticipated to be a widely applicable technology in the next generation of wireless communication systems, particularly 6G and next-gen military communications. Given this, our research is focused on developing a tool to promote the development of RFRL techniques that leverage spectrum sensing. In particular, the tool was designed to address two cognitive radio applications, specifically dynamic spectrum access and jamming. In order to train and test reinforcement learning (RL) algorithms for these applications, a simulation environment is necessary to simulate the conditions that an agent will encounter within the Radio Frequency (RF) spectrum. In this paper, such an environment has been developed, herein referred to as the RFRL Gym. Through the RFRL Gym, users can design their own scenarios to model what an RL agent may encounter within the RF spectrum as well as experiment with different spectrum sensing techniques. Additionally, the 
&lt;/p&gt;</description></item><item><title>SelfEEG&#26159;&#19968;&#20010;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#30340;&#24320;&#28304;Python&#24211;&#65292;&#26088;&#22312;&#24110;&#21161;&#20182;&#20204;&#22312;&#33041;&#30005;&#22270;&#25968;&#25454;&#19978;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#23454;&#39564;&#12290;&#23427;&#25552;&#20379;&#20102;&#29992;&#25143;&#21451;&#22909;&#19988;&#39640;&#24230;&#21487;&#23450;&#21046;&#30340;&#29615;&#22659;&#65292;&#28085;&#30422;&#20102;&#20174;&#25968;&#25454;&#23548;&#20837;&#21040;&#27169;&#22411;&#35774;&#35745;&#21644;&#35757;&#32451;&#30340;&#25152;&#26377;&#38454;&#27573;&#65292;&#24182;&#25552;&#20379;&#20102;&#21508;&#31181;&#21151;&#33021;&#26469;&#26377;&#25928;&#22320;&#22788;&#29702;&#33041;&#30005;&#22270;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2401.05405</link><description>&lt;p&gt;
SelfEEG: &#19968;&#31181;&#29992;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#33041;&#30005;&#22270;Python&#24211;
&lt;/p&gt;
&lt;p&gt;
SelfEEG: A Python library for Self-Supervised Learning in Electroencephalography. (arXiv:2401.05405v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05405
&lt;/p&gt;
&lt;p&gt;
SelfEEG&#26159;&#19968;&#20010;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#30340;&#24320;&#28304;Python&#24211;&#65292;&#26088;&#22312;&#24110;&#21161;&#20182;&#20204;&#22312;&#33041;&#30005;&#22270;&#25968;&#25454;&#19978;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#23454;&#39564;&#12290;&#23427;&#25552;&#20379;&#20102;&#29992;&#25143;&#21451;&#22909;&#19988;&#39640;&#24230;&#21487;&#23450;&#21046;&#30340;&#29615;&#22659;&#65292;&#28085;&#30422;&#20102;&#20174;&#25968;&#25454;&#23548;&#20837;&#21040;&#27169;&#22411;&#35774;&#35745;&#21644;&#35757;&#32451;&#30340;&#25152;&#26377;&#38454;&#27573;&#65292;&#24182;&#25552;&#20379;&#20102;&#21508;&#31181;&#21151;&#33021;&#26469;&#26377;&#25928;&#22320;&#22788;&#29702;&#33041;&#30005;&#22270;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
SelfEEG&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;Python&#24211;&#65292;&#26088;&#22312;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#22312;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#25968;&#25454;&#19978;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#23454;&#39564;&#12290;&#23427;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#25552;&#20379;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#20294;&#39640;&#24230;&#21487;&#23450;&#21046;&#30340;&#29615;&#22659;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#39640;&#25928;&#22320;&#35774;&#35745;&#21644;&#25191;&#34892;&#33041;&#30005;&#22270;&#25968;&#25454;&#19978;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#12290;SelfEEG&#28085;&#30422;&#20102;&#20856;&#22411;&#33258;&#30417;&#30563;&#23398;&#20064;&#27969;&#31243;&#30340;&#21508;&#20010;&#38454;&#27573;&#65292;&#21253;&#25324;&#25968;&#25454;&#23548;&#20837;&#12289;&#27169;&#22411;&#35774;&#35745;&#21644;&#35757;&#32451;&#12290;&#23427;&#21253;&#25324;&#19987;&#38376;&#35774;&#35745;&#30340;&#27169;&#22359;&#65292;&#29992;&#20110;&#22312;&#19981;&#21516;&#31890;&#24230;&#27700;&#24179;&#65288;&#20363;&#22914;&#22522;&#20110;&#20250;&#35758;&#12289;&#21463;&#35797;&#32773;&#25110;&#25968;&#25454;&#38598;&#30340;&#21010;&#20998;&#65289;&#19978;&#20998;&#21106;&#25968;&#25454;&#65307;&#22312;&#23567;&#25209;&#37327;&#26500;&#24314;&#26399;&#38388;&#39640;&#25928;&#31649;&#29702;&#23384;&#20648;&#26377;&#19981;&#21516;&#37197;&#32622;&#65288;&#20363;&#22914;&#25991;&#20214;&#25193;&#23637;&#21517;&#12289;&#25968;&#25454;&#31867;&#22411;&#65289;&#30340;&#25968;&#25454;&#65307;&#25552;&#20379;&#24191;&#27867;&#30340;&#26631;&#20934;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12289;&#25968;&#25454;&#22686;&#24378;&#21644;&#24212;&#29992;&#20110;&#33041;&#30005;&#22270;&#25968;&#25454;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#22522;&#20934;&#26041;&#27861;&#12290;SelfEEG&#30340;&#22823;&#22810;&#25968;&#21151;&#33021;&#26082;&#21487;&#20197;&#22312;GPU&#19978;&#25191;&#34892;&#65292;&#20063;&#21487;&#20197;&#22312;CPU&#19978;&#25191;&#34892;&#65292;&#25193;&#23637;&#20102;&#20854;&#21487;&#29992;&#24615;&#36229;&#20986;&#33258;&#30417;&#30563;&#23398;&#20064;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
SelfEEG is an open-source Python library developed to assist researchers in conducting Self-Supervised Learning (SSL) experiments on electroencephalography (EEG) data. Its primary objective is to offer a user-friendly but highly customizable environment, enabling users to efficiently design and execute self-supervised learning tasks on EEG data.  SelfEEG covers all the stages of a typical SSL pipeline, ranging from data import to model design and training. It includes modules specifically designed to: split data at various granularity levels (e.g., session-, subject-, or dataset-based splits); effectively manage data stored with different configurations (e.g., file extensions, data types) during mini-batch construction; provide a wide range of standard deep learning models, data augmentations and SSL baseline methods applied to EEG data.  Most of the functionalities offered by selfEEG can be executed both on GPUs and CPUs, expanding its usability beyond the self-supervised learning are
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21521;&#37327;&#22330;&#23450;&#21521;&#25193;&#25955;&#27169;&#22411;&#30340;&#26230;&#20307;&#26448;&#26009;&#29983;&#25104;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#20960;&#20309;&#31561;&#21464;GNN&#21516;&#26102;&#32771;&#34385;&#21407;&#23376;&#20301;&#32622;&#21644;&#26230;&#20307;&#26230;&#26684;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#33021;&#22815;&#26356;&#20840;&#38754;&#35780;&#20272;&#27169;&#22411;&#33021;&#21147;&#30340;&#26032;&#29983;&#25104;&#25351;&#26631;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#25193;&#25955;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.05402</link><description>&lt;p&gt;
&#22522;&#20110;&#21521;&#37327;&#22330;&#23450;&#21521;&#25193;&#25955;&#27169;&#22411;&#30340;&#26230;&#20307;&#26448;&#26009;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Vector Field Oriented Diffusion Model for Crystal Material Generation. (arXiv:2401.05402v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05402
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21521;&#37327;&#22330;&#23450;&#21521;&#25193;&#25955;&#27169;&#22411;&#30340;&#26230;&#20307;&#26448;&#26009;&#29983;&#25104;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#20960;&#20309;&#31561;&#21464;GNN&#21516;&#26102;&#32771;&#34385;&#21407;&#23376;&#20301;&#32622;&#21644;&#26230;&#20307;&#26230;&#26684;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#33021;&#22815;&#26356;&#20840;&#38754;&#35780;&#20272;&#27169;&#22411;&#33021;&#21147;&#30340;&#26032;&#29983;&#25104;&#25351;&#26631;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#25193;&#25955;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26448;&#26009;&#31185;&#23398;&#20013;&#65292;&#21457;&#29616;&#20855;&#26377;&#29305;&#23450;&#21270;&#23398;&#24615;&#36136;&#30340;&#26230;&#20307;&#32467;&#26500;&#24050;&#25104;&#20026;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#30740;&#31350;&#28966;&#28857;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#27169;&#22411;&#22312;&#29983;&#25104;&#26032;&#30340;&#26230;&#20307;&#26230;&#26684;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#21482;&#32771;&#34385;&#21407;&#23376;&#20301;&#32622;&#25110;&#21270;&#23398;&#32452;&#25104;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#29575;&#25193;&#25955;&#27169;&#22411;&#65292;&#21033;&#29992;&#20960;&#20309;&#31561;&#21464;GNN&#21516;&#26102;&#32771;&#34385;&#21407;&#23376;&#20301;&#32622;&#21644;&#26230;&#20307;&#26230;&#26684;&#12290;&#20026;&#35780;&#20272;&#25105;&#20204;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21463;Frechet Inception Distance&#21551;&#21457;&#30340;&#26032;&#30340;&#29983;&#25104;&#25351;&#26631;&#65292;&#20294;&#22522;&#20110;GNN&#33021;&#37327;&#39044;&#27979;&#65292;&#32780;&#19981;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#20351;&#29992;&#30340;InceptionV3&#12290;&#38500;&#20102;&#24120;&#29992;&#30340;&#35780;&#20272;&#25351;&#26631;&#65288;&#22914;&#26377;&#25928;&#24615;&#65292;&#35780;&#20272;&#32467;&#26500;&#30340;&#21512;&#29702;&#24615;&#65289;&#22806;&#65292;&#36825;&#31181;&#26032;&#25351;&#26631;&#25552;&#20379;&#20102;&#23545;&#25105;&#20204;&#27169;&#22411;&#33021;&#21147;&#26356;&#20840;&#38754;&#30340;&#35780;&#20272;&#12290;&#25105;&#20204;&#22312;&#29616;&#26377;&#22522;&#20934;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#25193;&#25955;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#23398;&#20064;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discovering crystal structures with specific chemical properties has become an increasingly important focus in material science. However, current models are limited in their ability to generate new crystal lattices, as they only consider atomic positions or chemical composition. To address this issue, we propose a probabilistic diffusion model that utilizes a geometrically equivariant GNN to consider atomic positions and crystal lattices jointly. To evaluate the effectiveness of our model, we introduce a new generation metric inspired by Frechet Inception Distance, but based on GNN energy prediction rather than InceptionV3 used in computer vision. In addition to commonly used metrics like validity, which assesses the plausibility of a structure, this new metric offers a more comprehensive evaluation of our model's capabilities. Our experiments on existing benchmarks show the significance of our diffusion model. We also show that our method can effectively learn meaningful representatio
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#39640;&#20809;&#35889;&#20809;&#21464;&#25512;&#26029;&#33322;&#22825;&#22120;&#23039;&#24577;&#21644;&#26059;&#36716;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#25968;&#20540;&#20248;&#21270;&#21644;&#26426;&#22120;&#23398;&#20064;&#20004;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#26368;&#23567;&#20449;&#24687;&#26465;&#20214;&#23436;&#25104;&#20934;&#30830;&#24230;&#27979;&#23450;&#20219;&#21153;&#65292;&#24182;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2401.05397</link><description>&lt;p&gt;
&#20934;&#30830;&#24230;&#27979;&#23450;&#30340;&#39640;&#20809;&#35889;&#20809;&#21464;&#21453;&#28436;
&lt;/p&gt;
&lt;p&gt;
Hyperspectral Lightcurve Inversion for Attitude Determination. (arXiv:2401.05397v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05397
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#39640;&#20809;&#35889;&#20809;&#21464;&#25512;&#26029;&#33322;&#22825;&#22120;&#23039;&#24577;&#21644;&#26059;&#36716;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#25968;&#20540;&#20248;&#21270;&#21644;&#26426;&#22120;&#23398;&#20064;&#20004;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#26368;&#23567;&#20449;&#24687;&#26465;&#20214;&#23436;&#25104;&#20934;&#30830;&#24230;&#27979;&#23450;&#20219;&#21153;&#65292;&#24182;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#30001;&#33322;&#22825;&#22120;&#30340;&#26102;&#38388;&#24207;&#21015;&#21333;&#20687;&#32032;&#20809;&#35889;&#27979;&#37327;&#32452;&#25104;&#30340;&#20809;&#35889;&#20809;&#21464;&#65292;&#25512;&#26029;&#33322;&#22825;&#22120;&#30340;&#23039;&#24577;&#21644;&#26059;&#36716;&#12290;&#37319;&#29992;&#20004;&#31181;&#26041;&#27861;&#65292;&#19968;&#31181;&#26159;&#22522;&#20110;&#27491;&#21017;&#21270;&#26368;&#23567;&#20108;&#20056;&#20195;&#20215;&#20989;&#25968;&#30340;&#25968;&#20540;&#20248;&#21270;&#65292;&#21478;&#19968;&#31181;&#26159;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#12290;&#30446;&#26631;&#26159;&#22312;&#26368;&#23567;&#30340;&#20449;&#24687;&#26465;&#20214;&#19979;&#36827;&#34892;&#24037;&#20316;&#65292;&#22240;&#27492;&#27809;&#26377;&#20851;&#20110;&#23039;&#24577;&#21644;&#24815;&#24615;&#24352;&#37327;&#30340;&#20808;&#39564;&#20449;&#24687;&#12290;&#30740;&#31350;&#20102;&#36825;&#20010;&#20219;&#21153;&#30340;&#29702;&#35770;&#21644;&#23454;&#38469;&#26041;&#38754;&#65292;&#24182;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#27979;&#35797;&#20102;&#26041;&#27861;&#12290;&#32467;&#26524;&#22522;&#20110;&#21512;&#25104;&#25968;&#25454;&#23637;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spectral lightcurves consisting of time series single-pixel spectral measurements of spacecraft are used to infer the spacecraft's attitude and rotation. Two methods are used. One based on numerical optimisation of a regularised least squares cost function, and another based on machine learning with a neural network model. The aim is to work with minimal information, thus no prior is available on the attitude nor on the inertia tensor. The theoretical and practical aspects of this task are investigated, and the methodology is tested on synthetic data. Results are shown based on synthetic data.
&lt;/p&gt;</description></item><item><title>SRNI-CAR&#26159;&#19968;&#20221;&#29992;&#20110;&#20998;&#26512;&#20013;&#22269;&#27773;&#36710;&#24066;&#22330;&#30340;&#32508;&#21512;&#25968;&#25454;&#38598;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#27773;&#36710;&#34892;&#19994;&#25968;&#25454;&#38598;&#35206;&#30422;&#33539;&#22260;&#26377;&#38480;&#30340;&#32570;&#21475;&#65292;&#23545;&#20110;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12289;&#25193;&#22823;&#21830;&#19994;&#24212;&#29992;&#33539;&#22260;&#12289;&#25351;&#23548;&#25919;&#31574;&#21046;&#23450;&#19982;&#30417;&#31649;&#20197;&#21450;&#25512;&#21160;&#27773;&#36710;&#34892;&#19994;&#30340;&#23398;&#26415;&#30740;&#31350;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.05395</link><description>&lt;p&gt;
SRNI-CAR: &#29992;&#20110;&#20998;&#26512;&#20013;&#22269;&#27773;&#36710;&#24066;&#22330;&#30340;&#32508;&#21512;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
SRNI-CAR: A comprehensive dataset for analyzing the Chinese automotive market. (arXiv:2401.05395v1 [econ.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05395
&lt;/p&gt;
&lt;p&gt;
SRNI-CAR&#26159;&#19968;&#20221;&#29992;&#20110;&#20998;&#26512;&#20013;&#22269;&#27773;&#36710;&#24066;&#22330;&#30340;&#32508;&#21512;&#25968;&#25454;&#38598;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#27773;&#36710;&#34892;&#19994;&#25968;&#25454;&#38598;&#35206;&#30422;&#33539;&#22260;&#26377;&#38480;&#30340;&#32570;&#21475;&#65292;&#23545;&#20110;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12289;&#25193;&#22823;&#21830;&#19994;&#24212;&#29992;&#33539;&#22260;&#12289;&#25351;&#23548;&#25919;&#31574;&#21046;&#23450;&#19982;&#30417;&#31649;&#20197;&#21450;&#25512;&#21160;&#27773;&#36710;&#34892;&#19994;&#30340;&#23398;&#26415;&#30740;&#31350;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27773;&#36710;&#34892;&#19994;&#22312;&#20840;&#29699;&#32463;&#27982;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#23588;&#20854;&#37325;&#35201;&#30340;&#26159;&#20013;&#22269;&#27773;&#36710;&#24066;&#22330;&#30340;&#19981;&#26029;&#25193;&#22823;&#65292;&#30001;&#20110;&#20854;&#24040;&#22823;&#30340;&#35268;&#27169;&#21644;&#24433;&#21709;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#27773;&#36710;&#34892;&#19994;&#25968;&#25454;&#38598;&#22312;&#35206;&#30422;&#33539;&#22260;&#19978;&#26377;&#38480;&#65292;&#26410;&#33021;&#20805;&#20998;&#32771;&#34385;&#21040;&#23545;&#26356;&#22810;&#21644;&#22810;&#26679;&#21270;&#21464;&#37327;&#30340;&#19981;&#26029;&#22686;&#38271;&#30340;&#38656;&#27714;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#20171;&#32461;&#19968;&#20221;&#20174;2016&#24180;&#21040;2022&#24180;&#30340;&#32508;&#21512;&#25968;&#25454;&#38598;&#26469;&#22635;&#34917;&#36825;&#19968;&#25968;&#25454;&#32570;&#21475;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#38144;&#21806;&#25968;&#25454;&#65292;&#22312;&#32447;&#35780;&#35770;&#20197;&#21450;&#19982;&#20013;&#22269;&#27773;&#36710;&#34892;&#19994;&#30456;&#20851;&#30340;&#22823;&#37327;&#20449;&#24687;&#12290;&#35813;&#25968;&#25454;&#38598;&#20316;&#20026;&#23453;&#36149;&#30340;&#36164;&#28304;&#65292;&#26497;&#22823;&#22320;&#25193;&#23637;&#20102;&#21487;&#29992;&#25968;&#25454;&#12290;&#23427;&#30340;&#24433;&#21709;&#21147;&#28085;&#30422;&#20102;&#22810;&#20010;&#26041;&#38754;&#65292;&#21253;&#25324;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#25193;&#22823;&#21830;&#19994;&#24212;&#29992;&#33539;&#22260;&#65292;&#25351;&#23548;&#25919;&#31574;&#21046;&#23450;&#19982;&#30417;&#31649;&#65292;&#25512;&#21160;&#27773;&#36710;&#34892;&#19994;&#30340;&#23398;&#26415;&#30740;&#31350;&#12290;&#20026;&#20102;&#23637;&#31034;&#35813;&#25968;&#25454;&#38598;&#22312;&#21830;&#19994;&#21644;&#23398;&#26415;&#32972;&#26223;&#19979;&#30340;&#28508;&#22312;&#24212;&#29992;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#24212;&#29992;&#26696;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
The automotive industry plays a critical role in the global economy, and particularly important is the expanding Chinese automobile market due to its immense scale and influence. However, existing automotive sector datasets are limited in their coverage, failing to adequately consider the growing demand for more and diverse variables. This paper aims to bridge this data gap by introducing a comprehensive dataset spanning the years from 2016 to 2022, encompassing sales data, online reviews, and a wealth of information related to the Chinese automotive industry. This dataset serves as a valuable resource, significantly expanding the available data. Its impact extends to various dimensions, including improving forecasting accuracy, expanding the scope of business applications, informing policy development and regulation, and advancing academic research within the automotive sector. To illustrate the dataset's potential applications in both business and academic contexts, we present two ap
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#36845;&#20195;&#27491;&#21017;&#21270;&#31639;&#27861;IRKSN&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;$k$&#25903;&#25745;&#33539;&#25968;&#27491;&#21017;&#21270;&#23454;&#29616;&#31232;&#30095;&#24674;&#22797;&#65292;&#24182;&#25552;&#20379;&#20102;&#26465;&#20214;&#12290;&#36825;&#26159;&#23545;&#22522;&#20110;$\ell_1$&#33539;&#25968;&#30340;&#36845;&#20195;&#26041;&#27861;&#30340;&#19968;&#31181;&#37325;&#35201;&#34917;&#20805;&#12290;</title><link>http://arxiv.org/abs/2401.05394</link><description>&lt;p&gt;
&#36845;&#20195;&#27491;&#21017;&#21270;&#19982;k&#25903;&#25745;&#33539;&#25968;&#65306;&#31232;&#30095;&#24674;&#22797;&#30340;&#37325;&#35201;&#34917;&#20805;
&lt;/p&gt;
&lt;p&gt;
Iterative Regularization with k-Support Norm: an Important Complement to Sparse Recovery. (arXiv:2401.05394v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05394
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#36845;&#20195;&#27491;&#21017;&#21270;&#31639;&#27861;IRKSN&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;$k$&#25903;&#25745;&#33539;&#25968;&#27491;&#21017;&#21270;&#23454;&#29616;&#31232;&#30095;&#24674;&#22797;&#65292;&#24182;&#25552;&#20379;&#20102;&#26465;&#20214;&#12290;&#36825;&#26159;&#23545;&#22522;&#20110;$\ell_1$&#33539;&#25968;&#30340;&#36845;&#20195;&#26041;&#27861;&#30340;&#19968;&#31181;&#37325;&#35201;&#34917;&#20805;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#24674;&#22797;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#20449;&#21495;&#22788;&#29702;&#20013;&#26080;&#22788;&#19981;&#22312;&#12290;&#30001;&#20110;&#31232;&#30095;&#24674;&#22797;&#30340;NP&#22256;&#38590;&#24615;&#36136;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#35201;&#20040;&#21463;&#38480;&#20110;&#36866;&#29992;&#26465;&#20214;&#65288;&#29978;&#33267;&#26410;&#30693;&#65289;&#65292;&#35201;&#20040;&#35745;&#31639;&#25104;&#26412;&#39640;&#12290;&#26368;&#36817;&#65292;&#36845;&#20195;&#27491;&#21017;&#21270;&#26041;&#27861;&#20316;&#20026;&#19968;&#31181;&#24555;&#36895;&#26041;&#27861;&#20986;&#29616;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#36890;&#36807;&#25552;&#21069;&#20572;&#27490;&#19968;&#27425;&#36890;&#36807;&#26469;&#23454;&#29616;&#31232;&#30095;&#24674;&#22797;&#65292;&#32780;&#19981;&#26159;&#20256;&#32479;&#26041;&#27861;&#20013;&#32321;&#29712;&#30340;&#32593;&#26684;&#25628;&#32034;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;&#36845;&#20195;&#26041;&#27861;&#37117;&#22522;&#20110;$\ell_1$&#33539;&#25968;&#65292;&#38656;&#35201;&#21463;&#38480;&#30340;&#36866;&#29992;&#26465;&#20214;&#65292;&#24182;&#19988;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#21487;&#33021;&#20250;&#22833;&#36133;&#12290;&#22240;&#27492;&#65292;&#36845;&#20195;&#27491;&#21017;&#21270;&#26041;&#27861;&#22312;&#26356;&#24191;&#27867;&#30340;&#26465;&#20214;&#19979;&#23454;&#29616;&#31232;&#30095;&#24674;&#22797;&#20173;&#38656;&#36827;&#19968;&#27493;&#25506;&#32034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36845;&#20195;&#27491;&#21017;&#21270;&#31639;&#27861;IRKSN&#65292;&#23427;&#22522;&#20110;$k$&#25903;&#25745;&#33539;&#25968;&#27491;&#21017;&#21270;&#32780;&#19981;&#26159;$\ell_1$&#33539;&#25968;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20351;&#29992;IRKSN&#36827;&#34892;&#31232;&#30095;&#24674;&#22797;&#30340;&#26465;&#20214;&#65292;&#24182;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sparse recovery is ubiquitous in machine learning and signal processing. Due to the NP-hard nature of sparse recovery, existing methods are known to suffer either from restrictive (or even unknown) applicability conditions, or high computational cost. Recently, iterative regularization methods have emerged as a promising fast approach because they can achieve sparse recovery in one pass through early stopping, rather than the tedious grid-search used in the traditional methods. However, most of those iterative methods are based on the $\ell_1$ norm which requires restrictive applicability conditions and could fail in many cases. Therefore, achieving sparse recovery with iterative regularization methods under a wider range of conditions has yet to be further explored. To address this issue, we propose a novel iterative regularization algorithm, IRKSN, based on the $k$-support norm regularizer rather than the $\ell_1$ norm. We provide conditions for sparse recovery with IRKSN, and compar
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20581;&#24247;&#24515;&#30005;&#22270;&#25968;&#25454;&#35757;&#32451;&#30340;&#21435;&#22122;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#65292;&#25104;&#21151;&#29983;&#25104;&#36924;&#30495;&#30340;&#24515;&#30005;&#22270;&#20449;&#21495;&#65292;&#24182;&#19988;&#24212;&#29992;&#20110;&#24515;&#33039;&#20581;&#24247;&#30417;&#27979;&#21644;&#35786;&#26029;&#39046;&#22495;&#65292;&#23454;&#29616;&#20102;&#26657;&#27491;QT&#38388;&#26399;&#12289;&#22122;&#22768;&#25233;&#21046;&#12289;&#24515;&#30005;&#22270;&#23548;&#32852;&#24674;&#22797;&#21644;&#24322;&#24120;&#35835;&#25968;&#35782;&#21035;&#31561;&#37325;&#35201;&#20020;&#24202;&#24037;&#20855;&#30340;&#24320;&#21457;&#12290;</title><link>http://arxiv.org/abs/2401.05388</link><description>&lt;p&gt;
&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#24515;&#30005;&#22270;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Bayesian ECG reconstruction using denoising diffusion generative models. (arXiv:2401.05388v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05388
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20581;&#24247;&#24515;&#30005;&#22270;&#25968;&#25454;&#35757;&#32451;&#30340;&#21435;&#22122;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#65292;&#25104;&#21151;&#29983;&#25104;&#36924;&#30495;&#30340;&#24515;&#30005;&#22270;&#20449;&#21495;&#65292;&#24182;&#19988;&#24212;&#29992;&#20110;&#24515;&#33039;&#20581;&#24247;&#30417;&#27979;&#21644;&#35786;&#26029;&#39046;&#22495;&#65292;&#23454;&#29616;&#20102;&#26657;&#27491;QT&#38388;&#26399;&#12289;&#22122;&#22768;&#25233;&#21046;&#12289;&#24515;&#30005;&#22270;&#23548;&#32852;&#24674;&#22797;&#21644;&#24322;&#24120;&#35835;&#25968;&#35782;&#21035;&#31561;&#37325;&#35201;&#20020;&#24202;&#24037;&#20855;&#30340;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20581;&#24247;&#24515;&#30005;&#22270;&#25968;&#25454;&#35757;&#32451;&#30340;&#21435;&#22122;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#65288;DDGM&#65289;&#65292;&#35813;&#27169;&#22411;&#20851;&#27880;&#24515;&#30005;&#22270;&#24418;&#24577;&#21644;&#23548;&#32852;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#21019;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#21487;&#20197;&#25104;&#21151;&#29983;&#25104;&#36924;&#30495;&#30340;&#24515;&#30005;&#22270;&#20449;&#21495;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#20351;&#29992;DDGM&#35299;&#20915;&#32447;&#24615;&#21453;&#38382;&#39064;&#30340;&#26368;&#26032;&#31361;&#30772;&#22312;&#24515;&#30005;&#22270;&#37325;&#24314;&#20013;&#30340;&#24212;&#29992;&#65292;&#36825;&#20026;&#21457;&#23637;&#20960;&#31181;&#37325;&#35201;&#30340;&#20020;&#24202;&#24037;&#20855;&#25552;&#20379;&#20102;&#21487;&#33021;&#12290;&#36825;&#20123;&#24037;&#20855;&#21253;&#25324;&#26657;&#27491;QT&#38388;&#26399;(QTc)&#30340;&#35745;&#31639;&#12289;&#24515;&#30005;&#22270;&#20449;&#21495;&#30340;&#26377;&#25928;&#22122;&#22768;&#25233;&#21046;&#12289;&#20002;&#22833;&#30340;&#24515;&#30005;&#22270;&#23548;&#32852;&#30340;&#24674;&#22797;&#20197;&#21450;&#24322;&#24120;&#35835;&#25968;&#30340;&#35782;&#21035;&#65292;&#20174;&#32780;&#22312;&#24515;&#33039;&#20581;&#24247;&#30417;&#27979;&#21644;&#35786;&#26029;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose a denoising diffusion generative model (DDGM) trained with healthy electrocardiogram (ECG) data that focuses on ECG morphology and inter-lead dependence. Our results show that this innovative generative model can successfully generate realistic ECG signals. Furthermore, we explore the application of recent breakthroughs in solving linear inverse Bayesian problems using DDGM. This approach enables the development of several important clinical tools. These include the calculation of corrected QT intervals (QTc), effective noise suppression of ECG signals, recovery of missing ECG leads, and identification of anomalous readings, enabling significant advances in cardiac health monitoring and diagnosis.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#32908;&#30005;&#22270;&#30340;&#25163;&#21183;&#35782;&#21035;&#31995;&#32479;&#20013;&#30340;&#36328;&#20010;&#20307;&#27867;&#21270;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#35782;&#21035;&#31283;&#20581;&#30340;&#20302;&#32500;&#23376;&#31354;&#38388;&#24182;&#23558;&#20854;&#19982;&#30446;&#26631;&#20010;&#20307;&#23545;&#40784;&#65292;&#25913;&#21892;&#20102;&#36328;&#20010;&#20307;&#20272;&#35745;&#12290;&#36890;&#36807;&#23545;&#23376;&#31354;&#38388;&#30340;&#21487;&#35270;&#21270;&#65292;&#20026;&#20351;&#29992;EMG&#20449;&#21495;&#25913;&#21892;&#36328;&#20010;&#20307;&#27867;&#21270;&#25552;&#20379;&#20102;&#26377;&#30410;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2401.05386</link><description>&lt;p&gt;
EMG&#23376;&#31354;&#38388;&#23545;&#40784;&#19982;&#21487;&#35270;&#21270;&#29992;&#20110;&#36328;&#20010;&#20307;&#25163;&#21183;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
EMG subspace alignment and visualization for cross-subject hand gesture classification. (arXiv:2401.05386v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05386
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#32908;&#30005;&#22270;&#30340;&#25163;&#21183;&#35782;&#21035;&#31995;&#32479;&#20013;&#30340;&#36328;&#20010;&#20307;&#27867;&#21270;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#35782;&#21035;&#31283;&#20581;&#30340;&#20302;&#32500;&#23376;&#31354;&#38388;&#24182;&#23558;&#20854;&#19982;&#30446;&#26631;&#20010;&#20307;&#23545;&#40784;&#65292;&#25913;&#21892;&#20102;&#36328;&#20010;&#20307;&#20272;&#35745;&#12290;&#36890;&#36807;&#23545;&#23376;&#31354;&#38388;&#30340;&#21487;&#35270;&#21270;&#65292;&#20026;&#20351;&#29992;EMG&#20449;&#21495;&#25913;&#21892;&#36328;&#20010;&#20307;&#27867;&#21270;&#25552;&#20379;&#20102;&#26377;&#30410;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#32908;&#30005;&#22270;&#65288;EMG&#65289;&#30340;&#25163;&#21183;&#35782;&#21035;&#31995;&#32479;&#26159;&#20154;&#26426;&#30028;&#38754;&#30340;&#26377;&#21069;&#36884;&#30340;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20027;&#35201;&#30340;&#23616;&#38480;&#24615;&#20043;&#19968;&#26159;&#36890;&#24120;&#38656;&#35201;&#38271;&#26102;&#38388;&#36827;&#34892;&#26657;&#20934;&#20197;&#36866;&#24212;&#26032;&#29992;&#25143;&#12290;&#26412;&#25991;&#36890;&#36807;&#19968;&#20010;&#21253;&#21547;&#20102;14&#21517;&#20154;&#31867;&#20027;&#20307;&#22312;&#25163;&#21183;&#26399;&#38388;&#30340;EMG&#20449;&#21495;&#30340;&#21407;&#22987;&#25968;&#25454;&#38598;&#65292;&#35752;&#35770;&#21644;&#20998;&#26512;&#20102;&#36328;&#20010;&#20307;&#27867;&#21270;&#30340;&#25361;&#25112;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;&#22522;&#20110;&#27719;&#38598;&#22810;&#20010;&#20010;&#20307;&#30340;&#31934;&#30830;&#27867;&#21270;&#20960;&#20046;&#26159;&#19981;&#21487;&#33021;&#30340;&#65292;&#20294;&#26159;&#36890;&#36807;&#35782;&#21035;&#22810;&#20010;&#20010;&#20307;&#30340;&#31283;&#20581;&#20302;&#32500;&#23376;&#31354;&#38388;&#65292;&#24182;&#23558;&#20854;&#23545;&#40784;&#21040;&#30446;&#26631;&#20010;&#20307;&#65292;&#21487;&#20197;&#25913;&#21892;&#36328;&#20010;&#20307;&#20272;&#35745;&#12290;&#36890;&#36807;&#23376;&#31354;&#38388;&#30340;&#21487;&#35270;&#21270;&#65292;&#25105;&#20204;&#33021;&#22815;&#25552;&#20379;&#26377;&#20851;&#20351;&#29992;EMG&#20449;&#21495;&#25913;&#21892;&#36328;&#20010;&#20307;&#27867;&#21270;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electromyograms (EMG)-based hand gesture recognition systems are a promising technology for human/machine interfaces. However, one of their main limitations is the long calibration time that is typically required to handle new users. The paper discusses and analyses the challenge of cross-subject generalization thanks to an original dataset containing the EMG signals of 14 human subjects during hand gestures. The experimental results show that, though an accurate generalization based on pooling multiple subjects is hardly achievable, it is possible to improve the cross-subject estimation by identifying a robust low-dimensional subspace for multiple subjects and aligning it to a target subject. A visualization of the subspace enables us to provide insights for the improvement of cross-subject generalization with EMG signals.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#27773;&#36710;&#38647;&#36798;&#20013;&#21033;&#29992;&#35282;&#24230;&#31561;&#21464;&#24615;&#30340;&#23436;&#20840;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#24178;&#25200;&#25233;&#21046;&#26041;&#38754;&#20248;&#20110;&#20808;&#21069;&#30340;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2401.05385</link><description>&lt;p&gt;
&#38754;&#21521;&#27773;&#36710;&#38647;&#36798;&#20013;&#30340;&#24178;&#25200;&#25233;&#21046;&#30340;&#35282;&#24230;&#31561;&#21464;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Angle-Equivariant Convolutional Neural Networks for Interference Mitigation in Automotive Radar. (arXiv:2401.05385v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05385
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#27773;&#36710;&#38647;&#36798;&#20013;&#21033;&#29992;&#35282;&#24230;&#31561;&#21464;&#24615;&#30340;&#23436;&#20840;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#24178;&#25200;&#25233;&#21046;&#26041;&#38754;&#20248;&#20110;&#20808;&#21069;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27773;&#36710;&#24212;&#29992;&#20013;&#65292;&#39057;&#29575;&#35843;&#21046;&#36830;&#32493;&#27874;&#65288;FMCW&#65289;&#38647;&#36798;&#26159;&#19968;&#31181;&#30830;&#23450;&#36710;&#36742;&#21608;&#22260;&#29289;&#20307;&#36317;&#31163;&#12289;&#36895;&#24230;&#21644;&#35282;&#24230;&#30340;&#25104;&#29087;&#25216;&#26415;&#12290;&#22914;&#26524;&#38647;&#36798;&#20256;&#24863;&#22120;&#20043;&#38388;&#21457;&#29983;&#20114;&#30456;&#24178;&#25200;&#65292;&#39044;&#27979;&#36136;&#37327;&#21487;&#33021;&#20250;&#20005;&#37325;&#21463;&#25439;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#65288;NNs&#65289;&#24182;&#34892;&#22788;&#29702;&#25972;&#20010;&#25509;&#25910;&#22120;&#38453;&#21015;&#30340;&#25968;&#25454;&#65292;&#20197;&#25552;&#39640;&#24178;&#25200;&#25233;&#21046;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26550;&#26500;&#22312;&#19981;&#21516;&#35282;&#24230;&#30340;&#24178;&#25200;&#21644;&#29289;&#20307;&#21040;&#36798;&#65288;AoAs&#65289;&#20043;&#38388;&#19981;&#20855;&#22791;&#24456;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#23436;&#20840;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#19982;&#19977;&#38454;&#21367;&#31215;&#65292;&#33021;&#22815;&#22312;&#19981;&#21516;&#30340;AoAs&#20043;&#38388;&#20256;&#36882;&#23398;&#20064;&#21040;&#30340;&#27169;&#24335;&#12290;&#25105;&#20204;&#30340;&#25552;&#35758;&#26550;&#26500;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#20808;&#21069;&#30340;&#24037;&#20316;&#65292;&#21516;&#26102;&#20855;&#26377;&#26356;&#39640;&#30340;&#40065;&#26834;&#24615;&#21644;&#26356;&#23569;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#25968;&#37327;&#12290;&#25105;&#20204;&#22312;&#22810;&#26679;&#30340;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#32593;&#32476;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#35282;&#24230;&#31561;&#21464;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In automotive applications, frequency modulated continuous wave (FMCW) radar is an established technology to determine the distance, velocity and angle of objects in the vicinity of the vehicle. The quality of predictions might be seriously impaired if mutual interference between radar sensors occurs. Previous work processes data from the entire receiver array in parallel to increase interference mitigation quality using neural networks (NNs). However, these architectures do not generalize well across different angles of arrival (AoAs) of interferences and objects. In this paper we introduce fully convolutional neural network (CNN) with rank-three convolutions which is able to transfer learned patterns between different AoAs. Our proposed architecture outperforms previous work while having higher robustness and a lower number of trainable parameters. We evaluate our network on a diverse data set and demonstrate its angle equivariance.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#36951;&#20256;&#32534;&#31243;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#39044;&#27979;&#21322;&#33258;&#30952;&#30952;&#26426;&#30340;&#20135;&#37327;&#12290;&#26032;&#30340;&#36951;&#20256;&#32534;&#31243;&#21464;&#20307;&#21487;&#20197;&#25552;&#21462;&#22810;&#20010;&#26041;&#31243;&#24335;&#65292;&#20174;&#32780;&#31934;&#30830;&#39044;&#27979;&#19981;&#21516;&#35757;&#32451;&#25968;&#25454;&#32676;&#38598;&#30340;&#20135;&#37327;&#12290;</title><link>http://arxiv.org/abs/2401.05382</link><description>&lt;p&gt;
&#29992;&#20110;&#39044;&#27979;&#21322;&#33258;&#30952;&#30952;&#26426;&#20135;&#37327;&#30340;&#25913;&#36827;&#36951;&#20256;&#32534;&#31243;
&lt;/p&gt;
&lt;p&gt;
An improved genetic programming for predicting semi autogenous grinding mill throughput. (arXiv:2401.05382v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05382
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#36951;&#20256;&#32534;&#31243;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#39044;&#27979;&#21322;&#33258;&#30952;&#30952;&#26426;&#30340;&#20135;&#37327;&#12290;&#26032;&#30340;&#36951;&#20256;&#32534;&#31243;&#21464;&#20307;&#21487;&#20197;&#25552;&#21462;&#22810;&#20010;&#26041;&#31243;&#24335;&#65292;&#20174;&#32780;&#31934;&#30830;&#39044;&#27979;&#19981;&#21516;&#35757;&#32451;&#25968;&#25454;&#32676;&#38598;&#30340;&#20135;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#33258;&#30952;&#30952;&#26426;&#22312;&#30719;&#29289;&#22788;&#29702;&#21378;&#30340;&#30952;&#30719;&#22238;&#36335;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#20934;&#30830;&#39044;&#27979;&#21322;&#33258;&#30952;&#30952;&#26426;&#30340;&#20135;&#37327;&#26159;&#19968;&#20010;&#20851;&#38190;&#24615;&#33021;&#25351;&#26631;&#12290;&#23613;&#31649;&#20197;&#21069;&#30340;&#30740;&#31350;&#20013;&#24050;&#32463;&#24320;&#21457;&#20986;&#20102;&#32463;&#39564;&#27169;&#22411;&#26469;&#39044;&#27979;&#21322;&#33258;&#30952;&#30952;&#26426;&#30340;&#20135;&#37327;&#65292;&#20294;&#26159;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#26469;&#20570;&#27492;&#39044;&#27979;&#30340;&#28508;&#21147;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#21457;&#25381;&#12290;&#19982;&#20381;&#36182;&#26114;&#36149;&#19988;&#32791;&#26102;&#30340;&#23454;&#39564;&#25968;&#25454;&#30340;&#32463;&#39564;&#24314;&#27169;&#19981;&#21516;&#65292;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#21487;&#20197;&#21033;&#29992;&#22312;&#27491;&#24120;&#36816;&#34892;&#26399;&#38388;&#25910;&#38598;&#30340;&#25968;&#25454;&#12290;&#36951;&#20256;&#32534;&#31243;&#65288;Genetic Programming&#65292;GP&#65289;&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#20854;&#20248;&#21183;&#22312;&#20110;&#33021;&#22815;&#25552;&#20379;&#19968;&#20010;&#36879;&#26126;&#30340;&#26041;&#31243;&#24335;&#26469;&#31934;&#30830;&#39044;&#27979;&#30952;&#26426;&#30340;&#20135;&#37327;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;GP&#24212;&#29992;&#20110;&#39044;&#27979;&#21322;&#33258;&#30952;&#30952;&#26426;&#20135;&#37327;&#65292;&#24182;&#24341;&#20837;&#20102;&#20116;&#31181;&#26032;&#30340;GP&#21464;&#31181;&#26469;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;&#36825;&#20123;&#21464;&#31181;&#25552;&#21462;&#20102;&#22810;&#20010;&#26041;&#31243;&#24335;&#65292;&#27599;&#20010;&#26041;&#31243;&#24335;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#29305;&#23450;&#35757;&#32451;&#25968;&#25454;&#32676;&#38598;&#30340;&#20135;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semi-autogenous grinding (SAG) mills play a pivotal role in the grinding circuit of mineral processing plants. Accurate prediction of SAG mill throughput as a crucial performance metric is of utmost importance. While empirical models have been developed in previous studies for SAG mill throughput prediction, the potential of applying machine learning (ML) techniques for this purpose remains underexplored. Unlike empirical modelling, which relies on expensive and time-consuming experimental data, ML techniques can utilize data collected during regular operations. Genetic programming (GP) is one of ML techniques that offers the advantage of providing a transparent equation for precise mill throughput prediction. This study explores the application of GP to predict SAG mill throughput and introduces five new GP variants to enhance prediction performance. These variants extract multiple equations, each accurately predicting mill throughput for specific clusters of training data. These equa
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;ADF&#26694;&#26550;&#21644;TransApp&#20998;&#31867;&#22120;&#65292;&#29992;&#20110;&#22522;&#20110;&#26234;&#33021;&#30005;&#34920;&#29992;&#30005;&#24207;&#21015;&#30340;&#30005;&#22120;&#26816;&#27979;&#12290;&#36825;&#20123;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#22823;&#37327;&#25968;&#25454;&#21644;&#21464;&#21270;&#38271;&#24230;&#30340;&#24207;&#21015;&#65292;&#20026;&#30005;&#21147;&#20379;&#24212;&#21830;&#25552;&#20379;&#24110;&#21161;&#23458;&#25143;&#23454;&#29616;&#33021;&#28304;&#36716;&#22411;&#30340;&#23453;&#36149;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2401.05381</link><description>&lt;p&gt;
ADF&#21644;TransApp&#65306;&#22522;&#20110;Transformer&#30340;&#26234;&#33021;&#30005;&#34920;&#29992;&#30005;&#24207;&#21015;&#20013;&#30340;&#30005;&#22120;&#26816;&#27979;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
ADF &amp; TransApp: A Transformer-Based Framework for Appliance Detection Using Smart Meter Consumption Series. (arXiv:2401.05381v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05381
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;ADF&#26694;&#26550;&#21644;TransApp&#20998;&#31867;&#22120;&#65292;&#29992;&#20110;&#22522;&#20110;&#26234;&#33021;&#30005;&#34920;&#29992;&#30005;&#24207;&#21015;&#30340;&#30005;&#22120;&#26816;&#27979;&#12290;&#36825;&#20123;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#22823;&#37327;&#25968;&#25454;&#21644;&#21464;&#21270;&#38271;&#24230;&#30340;&#24207;&#21015;&#65292;&#20026;&#30005;&#21147;&#20379;&#24212;&#21830;&#25552;&#20379;&#24110;&#21161;&#23458;&#25143;&#23454;&#29616;&#33021;&#28304;&#36716;&#22411;&#30340;&#23453;&#36149;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#20840;&#29699;&#30005;&#21147;&#20379;&#24212;&#21830;&#23433;&#35013;&#20102;&#25968;&#30334;&#19975;&#20010;&#26234;&#33021;&#30005;&#34920;&#65292;&#20351;&#20182;&#20204;&#33021;&#22815;&#25910;&#38598;&#22823;&#37327;&#30340;&#29992;&#30005;&#25968;&#25454;&#65292;&#23613;&#31649;&#37319;&#26679;&#39057;&#29575;&#36739;&#20302;&#65288;&#27599;30&#20998;&#38047;&#19968;&#20010;&#28857;&#65289;&#12290;&#36825;&#20123;&#20379;&#24212;&#21830;&#38754;&#20020;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#26159;&#22914;&#20309;&#21033;&#29992;&#36825;&#20123;&#25968;&#25454;&#26469;&#26816;&#27979;&#23458;&#25143;&#23478;&#24237;&#20013;&#19981;&#21516;&#30005;&#22120;&#30340;&#23384;&#22312;/&#19981;&#23384;&#22312;&#12290;&#36825;&#20123;&#23453;&#36149;&#30340;&#20449;&#24687;&#21487;&#20197;&#24110;&#21161;&#20182;&#20204;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#20248;&#24800;&#21644;&#24314;&#35758;&#65292;&#20197;&#24110;&#21161;&#23458;&#25143;&#23454;&#29616;&#33021;&#28304;&#36716;&#22411;&#12290;&#30005;&#22120;&#26816;&#27979;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#19968;&#20010;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22823;&#37327;&#30340;&#25968;&#25454;&#32467;&#21512;&#28040;&#36153;&#24207;&#21015;&#30340;&#38271;&#24230;&#38271;&#19988;&#21464;&#21270;&#19981;&#23450;&#65292;&#22312;&#35757;&#32451;&#20998;&#31867;&#22120;&#26102;&#20250;&#24102;&#26469;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;ADF&#65292;&#23427;&#20351;&#29992;&#23458;&#25143;&#29992;&#30005;&#24207;&#21015;&#30340;&#23376;&#24207;&#21015;&#26469;&#26816;&#27979;&#30005;&#22120;&#30340;&#23384;&#22312;/&#19981;&#23384;&#22312;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;TransApp&#65292;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#22120;&#65292;&#35813;&#20998;&#31867;&#22120;&#39318;&#20808;&#22312;&#39044;&#35757;&#32451;&#20013;&#36827;&#34892;&#20102;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the past decade, millions of smart meters have been installed by electricity suppliers worldwide, allowing them to collect a large amount of electricity consumption data, albeit sampled at a low frequency (one point every 30min). One of the important challenges these suppliers face is how to utilize these data to detect the presence/absence of different appliances in the customers' households. This valuable information can help them provide personalized offers and recommendations to help customers towards the energy transition. Appliance detection can be cast as a time series classification problem. However, the large amount of data combined with the long and variable length of the consumption series pose challenges when training a classifier. In this paper, we propose ADF, a framework that uses subsequences of a client consumption series to detect the presence/absence of appliances. We also introduce TransApp, a Transformer-based time series classifier that is first pretrained in
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#24930;&#24615;&#30149;&#39044;&#27979;&#20013;&#24212;&#29992;&#22522;&#22240;&#31639;&#27861;&#12289;&#31890;&#23376;&#32676;&#20248;&#21270;&#21644;&#40120;&#40060;&#20248;&#21270;&#31639;&#27861;&#31561;&#29983;&#29289;&#21551;&#21457;&#20248;&#21270;&#31639;&#27861;&#36827;&#34892;&#29305;&#24449;&#36873;&#25321;&#30340;&#26041;&#27861;&#65292;&#30446;&#26631;&#26159;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12289;&#20248;&#21270;&#25968;&#25454;&#32500;&#24230;&#65292;&#24182;&#20351;&#39044;&#27979;&#32467;&#26524;&#26356;&#26131;&#35299;&#37322;&#21644;&#34892;&#21160;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#29983;&#29289;&#21551;&#21457;&#20248;&#21270;&#31639;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#20943;&#23569;&#20934;&#30830;&#20998;&#31867;&#25152;&#38656;&#30340;&#29305;&#24449;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2401.05380</link><description>&lt;p&gt;
&#22522;&#20110;&#29983;&#29289;&#21551;&#21457;&#20248;&#21270;&#31639;&#27861;&#30340;&#24930;&#24615;&#30149;&#39044;&#27979;&#25968;&#25454;&#38598;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Dataset Optimization for Chronic Disease Prediction with Bio-Inspired Feature Selection. (arXiv:2401.05380v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05380
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#24930;&#24615;&#30149;&#39044;&#27979;&#20013;&#24212;&#29992;&#22522;&#22240;&#31639;&#27861;&#12289;&#31890;&#23376;&#32676;&#20248;&#21270;&#21644;&#40120;&#40060;&#20248;&#21270;&#31639;&#27861;&#31561;&#29983;&#29289;&#21551;&#21457;&#20248;&#21270;&#31639;&#27861;&#36827;&#34892;&#29305;&#24449;&#36873;&#25321;&#30340;&#26041;&#27861;&#65292;&#30446;&#26631;&#26159;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12289;&#20248;&#21270;&#25968;&#25454;&#32500;&#24230;&#65292;&#24182;&#20351;&#39044;&#27979;&#32467;&#26524;&#26356;&#26131;&#35299;&#37322;&#21644;&#34892;&#21160;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#29983;&#29289;&#21551;&#21457;&#20248;&#21270;&#31639;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#20943;&#23569;&#20934;&#30830;&#20998;&#31867;&#25152;&#38656;&#30340;&#29305;&#24449;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22522;&#22240;&#31639;&#27861;&#12289;&#31890;&#23376;&#32676;&#20248;&#21270;&#21644;&#40120;&#40060;&#20248;&#21270;&#31639;&#27861;&#31561;&#29983;&#29289;&#21551;&#21457;&#20248;&#21270;&#31639;&#27861;&#22312;&#24930;&#24615;&#30149;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#20027;&#35201;&#30446;&#26631;&#26159;&#25552;&#39640;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12289;&#20248;&#21270;&#25968;&#25454;&#32500;&#24230;&#65292;&#20351;&#24471;&#39044;&#27979;&#32467;&#26524;&#26356;&#26131;&#35299;&#37322;&#21644;&#34892;&#21160;&#21270;&#12290;&#30740;&#31350;&#28085;&#30422;&#20102;&#23545;&#31958;&#23615;&#30149;&#12289;&#30284;&#30151;&#12289;&#32958;&#33039;&#30142;&#30149;&#21644;&#24515;&#34880;&#31649;&#30142;&#30149;&#31561;&#19981;&#21516;&#24930;&#24615;&#30149;&#30340;&#19977;&#31181;&#29983;&#29289;&#21551;&#21457;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#20998;&#26512;&#12290;&#20351;&#29992;&#20934;&#30830;&#29575;&#12289;&#31934;&#30830;&#24230;&#12289;&#21484;&#22238;&#29575;&#21644;F1&#24471;&#20998;&#31561;&#24615;&#33021;&#25351;&#26631;&#35780;&#20272;&#31639;&#27861;&#22312;&#20934;&#30830;&#20998;&#31867;&#25152;&#38656;&#29305;&#24449;&#25968;&#37327;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;&#24635;&#20307;&#32467;&#26524;&#34920;&#26126;&#65292;&#29983;&#29289;&#21551;&#21457;&#20248;&#21270;&#31639;&#27861;&#22312;&#20943;&#23569;&#20934;&#30830;&#20998;&#31867;&#25152;&#38656;&#29305;&#24449;&#25968;&#37327;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#22312;&#19981;&#21516;&#24930;&#24615;&#30149;&#19978;&#20250;&#26377;&#19968;&#23450;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we investigated the application of bio-inspired optimization algorithms, including Genetic Algorithm, Particle Swarm Optimization, and Whale Optimization Algorithm, for feature selection in chronic disease prediction. The primary goal was to enhance the predictive accuracy of models streamline data dimensionality, and make predictions more interpretable and actionable.  The research encompassed a comparative analysis of the three bio-inspired feature selection approaches across diverse chronic diseases, including diabetes, cancer, kidney, and cardiovascular diseases. Performance metrics such as accuracy, precision, recall, and f1 score are used to assess the effectiveness of the algorithms in reducing the number of features needed for accurate classification.  The results in general demonstrate that the bio-inspired optimization algorithms are effective in reducing the number of features required for accurate classification. However, there have been variations in the per
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20174;&#21333;&#23548;&#32852;&#24515;&#30005;&#22270;&#20013;&#25512;&#26029;QT&#38388;&#26399;&#65292;&#24182;&#29992;&#20110;&#26816;&#27979;Dofetilide&#33647;&#29289;&#36733;&#33655;&#26399;&#38388;&#26377;&#20020;&#24202;&#24847;&#20041;&#30340;QT&#24310;&#38271;&#20107;&#20214;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#22312;&#22823;&#35268;&#27169;&#30340;&#24515;&#30005;&#22270;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#21644;&#39564;&#35777;&#65292;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2401.05378</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#20174;&#21333;&#23548;&#32852;&#24515;&#30005;&#22270;&#20013;&#26816;&#27979;QT&#24310;&#38271;
&lt;/p&gt;
&lt;p&gt;
Detecting QT prolongation From a Single-lead ECG With Deep Learning. (arXiv:2401.05378v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05378
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20174;&#21333;&#23548;&#32852;&#24515;&#30005;&#22270;&#20013;&#25512;&#26029;QT&#38388;&#26399;&#65292;&#24182;&#29992;&#20110;&#26816;&#27979;Dofetilide&#33647;&#29289;&#36733;&#33655;&#26399;&#38388;&#26377;&#20020;&#24202;&#24847;&#20041;&#30340;QT&#24310;&#38271;&#20107;&#20214;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#22312;&#22823;&#35268;&#27169;&#30340;&#24515;&#30005;&#22270;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#21644;&#39564;&#35777;&#65292;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#19968;&#20123;&#25239;&#24515;&#24459;&#22833;&#24120;&#33647;&#29289;&#65292;&#33647;&#29289;&#36733;&#33655;&#38656;&#35201;3&#22825;&#20303;&#38498;&#35266;&#23519;QT&#24310;&#38271;&#24773;&#20917;&#12290;&#36890;&#36807;&#20329;&#25140;&#24335;&#24515;&#30005;&#22270;&#30417;&#27979;&#20202;&#36827;&#34892;&#33258;&#21160;&#21270;QT&#30417;&#27979;&#23558;&#26377;&#21161;&#20110;&#38498;&#22806;&#25252;&#29702;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20174;&#23548;&#32852;I&#30340;&#24515;&#30005;&#22270;&#20013;&#25512;&#26029;QT&#38388;&#26399; - &#36825;&#26159;&#20174;&#27493;&#34892;&#24335;&#24515;&#30005;&#22270;&#30417;&#27979;&#20202;&#20013;&#26368;&#24120;&#33719;&#21462;&#30340;&#23548;&#32852;&#65292;&#24182;&#20351;&#29992;&#35813;&#27169;&#22411;&#22312;&#22810;&#36153;&#26367;&#21033;&#24471;&#65288;Dofetilide&#65289;&#33647;&#29289;&#36733;&#33655;&#26399;&#38388;&#26816;&#27979;&#26377;&#20020;&#24202;&#24847;&#20041;&#30340;QT&#24310;&#38271;&#20107;&#20214;&#12290;&#25105;&#20204;&#20351;&#29992;&#20174;&#39532;&#33832;&#35832;&#22622;&#24030;&#32508;&#21512;&#21307;&#38498;&#30340;903.6&#19975;&#21517;&#24739;&#32773;&#20013;&#33719;&#24471;&#30340;422&#19975;&#20221;12&#23548;&#32852;&#24515;&#30005;&#22270;&#35760;&#24405;&#26469;&#24320;&#21457;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;QTNet&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#20174;&#23548;&#32852;I&#25512;&#26029;QT&#38388;&#26399;&#12290;&#21033;&#29992;&#26469;&#33258;65.3&#19975;&#21517;&#24739;&#32773;&#30340;300&#22810;&#19975;&#20221;&#24515;&#30005;&#22270;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#65292;&#20351;&#29992;&#21253;&#21547;&#26469;&#33258;13.5&#19975;&#21517;&#24739;&#32773;&#30340;63.3&#19975;&#20221;&#24515;&#30005;&#22270;&#30340;&#20869;&#37096;&#27979;&#35797;&#38598;&#36827;&#34892;&#27979;&#35797;&#12290;QTNet&#36824;&#22312;&#21478;&#22806;&#19968;&#23478;&#26426;&#26500;&#30340;&#19968;&#22871;&#21253;&#21547;667&#19975;&#21517;&#24739;&#32773;&#30340;310&#19975;&#24515;&#30005;&#22270;&#30340;&#22806;&#37096;&#39564;&#35777;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;QTNet&#34987;&#29992;&#20110;&#26816;&#27979;Dofetilide&#24341;&#36215;&#30340;QT&#24310;&#38271;&#20107;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
For a number of antiarrhythmics, drug loading requires a 3 day hospitalization with monitoring for QT prolongation. Automated QT monitoring with wearable ECG monitors would facilitate out-of-hospital care. We develop a deep learning model that infers QT intervals from ECG lead-I - the lead most often acquired from ambulatory ECG monitors - and to use this model to detect clinically meaningful QT-prolongation episodes during Dofetilide drug loading. Using 4.22 million 12-lead ECG recordings from 903.6 thousand patients at the Massachusetts General Hospital, we develop a deep learning model, QTNet, that infers QT intervals from lead-I. Over 3 million ECGs from 653 thousand patients are used to train the model and an internal-test set containing 633 thousand ECGs from 135 thousand patients was used for testing. QTNet is further evaluated on an external-validation set containing 3.1 million ECGs from 667 thousand patients at another institution. QTNet was used to detect Dofetilide-induced 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;"&#21160;&#24577;&#23574;&#23792;&#22270;&#31070;&#32463;&#32593;&#32476;"&#65288;DSGNN&#65289;&#30340;&#26694;&#26550;&#65292;&#23427;&#23558;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#19982;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#35299;&#20915;&#21160;&#24577;&#22270;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#22797;&#26434;&#24615;&#21644;&#20869;&#23384;&#24320;&#38144;&#38382;&#39064;&#12290;DSGNN&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#23574;&#23792;&#31070;&#32463;&#20803;&#30340;&#29366;&#24577;&#21644;&#36830;&#25509;&#26435;&#37325;&#65292;&#22312;&#20256;&#25773;&#36807;&#31243;&#20013;&#20445;&#25345;&#22270;&#32467;&#26500;&#20449;&#24687;&#30340;&#23436;&#25972;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.05373</link><description>&lt;p&gt;
&#21160;&#24577;&#23574;&#23792;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Dynamic Spiking Graph Neural Networks. (arXiv:2401.05373v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05373
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;"&#21160;&#24577;&#23574;&#23792;&#22270;&#31070;&#32463;&#32593;&#32476;"&#65288;DSGNN&#65289;&#30340;&#26694;&#26550;&#65292;&#23427;&#23558;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#19982;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#35299;&#20915;&#21160;&#24577;&#22270;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#22797;&#26434;&#24615;&#21644;&#20869;&#23384;&#24320;&#38144;&#38382;&#39064;&#12290;DSGNN&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#23574;&#23792;&#31070;&#32463;&#20803;&#30340;&#29366;&#24577;&#21644;&#36830;&#25509;&#26435;&#37325;&#65292;&#22312;&#20256;&#25773;&#36807;&#31243;&#20013;&#20445;&#25345;&#22270;&#32467;&#26500;&#20449;&#24687;&#30340;&#23436;&#25972;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30456;&#32467;&#21512;&#28176;&#28176;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#65292;&#36825;&#26159;&#22240;&#20026;&#23427;&#22312;&#22788;&#29702;&#30001;&#22270;&#34920;&#31034;&#30340;&#38750;&#27431;&#20960;&#37324;&#24471;&#25968;&#25454;&#26102;&#20855;&#26377;&#20302;&#21151;&#32791;&#21644;&#39640;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#20316;&#20026;&#19968;&#20010;&#24120;&#35265;&#30340;&#38382;&#39064;&#65292;&#21160;&#24577;&#22270;&#34920;&#31034;&#23398;&#20064;&#38754;&#20020;&#30528;&#39640;&#22797;&#26434;&#24615;&#21644;&#22823;&#20869;&#23384;&#24320;&#38144;&#30340;&#25361;&#25112;&#12290;&#30446;&#21069;&#30340;&#24037;&#20316;&#36890;&#24120;&#36890;&#36807;&#20351;&#29992;&#20108;&#36827;&#21046;&#29305;&#24449;&#32780;&#19981;&#26159;&#36830;&#32493;&#29305;&#24449;&#30340;SNNs&#26469;&#26367;&#20195;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNNs&#65289;&#36827;&#34892;&#39640;&#25928;&#35757;&#32451;&#65292;&#36825;&#20250;&#24573;&#35270;&#22270;&#32467;&#26500;&#20449;&#24687;&#24182;&#22312;&#20256;&#25773;&#36807;&#31243;&#20013;&#23548;&#33268;&#32454;&#33410;&#30340;&#20002;&#22833;&#12290;&#27492;&#22806;&#65292;&#20248;&#21270;&#21160;&#24577;&#23574;&#23792;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#22312;&#26102;&#38388;&#27493;&#20043;&#38388;&#20256;&#25773;&#20449;&#24687;&#65292;&#36825;&#22686;&#21152;&#20102;&#20869;&#23384;&#38656;&#27714;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;"&#21160;&#24577;&#23574;&#23792;&#22270;&#31070;&#32463;&#32593;&#32476;"&#65288;\method{}&#65289;&#30340;&#26694;&#26550;&#12290;&#20026;&#20102;&#20943;&#36731;&#20449;&#24687;&#20002;&#22833;&#38382;&#39064;&#65292;\method{} &#22312;&#20256;&#25773;&#36807;&#31243;&#20013;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#21046;&#65292;&#23427;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#20013;&#21160;&#24577;&#22320;&#35843;&#25972;&#23574;&#23792;&#31070;&#32463;&#20803;&#30340;&#29366;&#24577;&#21644;&#36830;&#25509;&#26435;&#37325;&#65292;&#20197;&#20445;&#25345;&#22270;&#32467;&#26500;&#20449;&#24687;&#30340;&#23436;&#25972;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The integration of Spiking Neural Networks (SNNs) and Graph Neural Networks (GNNs) is gradually attracting attention due to the low power consumption and high efficiency in processing the non-Euclidean data represented by graphs. However, as a common problem, dynamic graph representation learning faces challenges such as high complexity and large memory overheads. Current work often uses SNNs instead of Recurrent Neural Networks (RNNs) by using binary features instead of continuous ones for efficient training, which would overlooks graph structure information and leads to the loss of details during propagation. Additionally, optimizing dynamic spiking models typically requires propagation of information across time steps, which increases memory requirements. To address these challenges, we present a framework named \underline{Dy}namic \underline{S}p\underline{i}king \underline{G}raph \underline{N}eural Networks (\method{}). To mitigate the information loss problem, \method{} propagates
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;AutoFragDiff&#65292;&#19968;&#31181;&#22522;&#20110;&#29255;&#27573;&#30340;&#33258;&#22238;&#24402;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#29983;&#25104;3D&#20998;&#23376;&#32467;&#26500;&#12290;&#36890;&#36807;&#39044;&#27979;&#20998;&#23376;&#29255;&#27573;&#30340;&#21407;&#23376;&#31867;&#22411;&#21644;&#31354;&#38388;&#22352;&#26631;&#65292;&#25913;&#21892;&#20102;&#29983;&#25104;&#20998;&#23376;&#30340;&#23616;&#37096;&#20960;&#20309;&#32467;&#26500;&#65292;&#24182;&#20445;&#25345;&#39640;&#30340;&#32467;&#21512;&#20146;&#21644;&#21147;&#12290;&#22914;&#26524;&#25552;&#20379;&#36215;&#22987;&#20998;&#23376;&#39592;&#26550;&#65292;&#27169;&#22411;&#36824;&#21487;&#20197;&#36827;&#34892;&#25193;&#23637;&#12290;</title><link>http://arxiv.org/abs/2401.05370</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#22238;&#24402;&#29255;&#27573;&#25193;&#25955;&#30340;&#32467;&#21512;&#20301;&#24863;&#30693;&#37197;&#20307;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Autoregressive fragment-based diffusion for pocket-aware ligand design. (arXiv:2401.05370v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05370
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;AutoFragDiff&#65292;&#19968;&#31181;&#22522;&#20110;&#29255;&#27573;&#30340;&#33258;&#22238;&#24402;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#29983;&#25104;3D&#20998;&#23376;&#32467;&#26500;&#12290;&#36890;&#36807;&#39044;&#27979;&#20998;&#23376;&#29255;&#27573;&#30340;&#21407;&#23376;&#31867;&#22411;&#21644;&#31354;&#38388;&#22352;&#26631;&#65292;&#25913;&#21892;&#20102;&#29983;&#25104;&#20998;&#23376;&#30340;&#23616;&#37096;&#20960;&#20309;&#32467;&#26500;&#65292;&#24182;&#20445;&#25345;&#39640;&#30340;&#32467;&#21512;&#20146;&#21644;&#21147;&#12290;&#22914;&#26524;&#25552;&#20379;&#36215;&#22987;&#20998;&#23376;&#39592;&#26550;&#65292;&#27169;&#22411;&#36824;&#21487;&#20197;&#36827;&#34892;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;AutoFragDiff&#65292;&#19968;&#31181;&#22522;&#20110;&#29255;&#27573;&#30340;&#33258;&#22238;&#24402;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#26681;&#25454;&#30446;&#26631;&#34507;&#30333;&#32467;&#26500;&#29983;&#25104;3D&#20998;&#23376;&#32467;&#26500;&#12290;&#25105;&#20204;&#20351;&#29992;&#20960;&#20309;&#21521;&#37327;&#24863;&#30693;&#22120;&#26469;&#39044;&#27979;&#20998;&#23376;&#39592;&#26550;&#21644;&#34507;&#30333;&#36127;&#34955;&#26465;&#20214;&#19979;&#26032;&#20998;&#23376;&#29255;&#27573;&#30340;&#21407;&#23376;&#31867;&#22411;&#21644;&#31354;&#38388;&#22352;&#26631;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25913;&#21892;&#20102;&#29983;&#25104;&#30340;3D&#20998;&#23376;&#30340;&#23616;&#37096;&#20960;&#20309;&#32467;&#26500;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#39044;&#27979;&#30340;&#32467;&#21512;&#20146;&#21644;&#21147;&#12290;&#35813;&#27169;&#22411;&#36824;&#21487;&#20197;&#26681;&#25454;&#29992;&#25143;&#25552;&#20379;&#30340;&#36215;&#22987;&#20998;&#23376;&#39592;&#26550;&#36827;&#34892;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we introduce AutoFragDiff, a fragment-based autoregressive diffusion model for generating 3D molecular structures conditioned on target protein structures. We employ geometric vector perceptrons to predict atom types and spatial coordinates of new molecular fragments conditioned on molecular scaffolds and protein pockets. Our approach improves the local geometry of the resulting 3D molecules while maintaining high predicted binding affinity to protein targets. The model can also perform scaffold extension from user-provided starting molecular scaffold.
&lt;/p&gt;</description></item><item><title>&#31526;&#21495;&#22238;&#24402;&#26159;&#19968;&#31181;&#36890;&#36807;&#22797;&#21046;&#32593;&#32476;&#30340;&#32467;&#26500;&#21644;&#36807;&#31243;&#26469;&#35299;&#37322;&#32593;&#32476;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#19981;&#20381;&#36182;&#31185;&#23398;&#23478;&#30340;&#30452;&#35273;&#25110;&#19987;&#19994;&#30693;&#35782;&#65292;&#25104;&#21151;&#22320;&#26816;&#32034;&#21040;&#20102;&#21512;&#25104;&#30340;&#22686;&#38271;&#36807;&#31243;&#21644;&#31616;&#21333;&#21487;&#35299;&#37322;&#30340;&#35268;&#21017;&#12290;</title><link>http://arxiv.org/abs/2401.05369</link><description>&lt;p&gt;
&#21160;&#24577;&#32593;&#32476;&#27169;&#22411;&#30340;&#31526;&#21495;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Symbolic Regression of Dynamic Network Models. (arXiv:2401.05369v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05369
&lt;/p&gt;
&lt;p&gt;
&#31526;&#21495;&#22238;&#24402;&#26159;&#19968;&#31181;&#36890;&#36807;&#22797;&#21046;&#32593;&#32476;&#30340;&#32467;&#26500;&#21644;&#36807;&#31243;&#26469;&#35299;&#37322;&#32593;&#32476;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#19981;&#20381;&#36182;&#31185;&#23398;&#23478;&#30340;&#30452;&#35273;&#25110;&#19987;&#19994;&#30693;&#35782;&#65292;&#25104;&#21151;&#22320;&#26816;&#32034;&#21040;&#20102;&#21512;&#25104;&#30340;&#22686;&#38271;&#36807;&#31243;&#21644;&#31616;&#21333;&#21487;&#35299;&#37322;&#30340;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#22797;&#26434;&#31995;&#32479;&#65288;&#20174;&#22823;&#33041;&#21040;&#31038;&#20250;&#21040;&#22478;&#24066;&#65289;&#20351;&#29992;&#32593;&#32476;&#36827;&#34892;&#24314;&#27169;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#65292;&#36825;&#23548;&#33268;&#20154;&#20204;&#23545;&#35299;&#37322;&#36825;&#20123;&#32593;&#32476;&#30340;&#29983;&#25104;&#36807;&#31243;&#30340;&#21162;&#21147;&#22686;&#21152;&#12290;&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#26032;&#25104;&#21151;&#20419;&#20351;&#20351;&#29992;&#36827;&#21270;&#35745;&#31639;&#65292;&#23588;&#20854;&#26159;&#36951;&#20256;&#32534;&#31243;&#26469;&#28436;&#21270;&#35745;&#31639;&#26426;&#31243;&#24207;&#65292;&#20197;&#26377;&#25928;&#22320;&#22312;&#22810;&#32500;&#25628;&#32034;&#31354;&#38388;&#20013;&#23547;&#25214;&#35299;&#37322;&#32593;&#32476;&#32467;&#26500;&#30340;&#26356;&#22909;&#35299;&#20915;&#26041;&#26696;&#12290;&#31526;&#21495;&#22238;&#24402;&#36890;&#36807;&#22797;&#21046;&#32593;&#32476;&#24418;&#24577;&#21644;&#36807;&#31243;&#26469;&#20026;&#36825;&#20123;&#26041;&#27861;&#20570;&#20986;&#36129;&#29486;&#65292;&#21516;&#26102;&#19981;&#20381;&#36182;&#31185;&#23398;&#23478;&#30340;&#30452;&#35273;&#25110;&#19987;&#19994;&#30693;&#35782;&#12290;&#23427;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#32593;&#32476;&#29983;&#25104;&#22120;&#30340;&#20844;&#24335;&#21644;&#26080;&#21442;&#25968;&#36866;&#24212;&#24230;&#20989;&#25968;&#26469;&#35780;&#20272;&#29983;&#25104;&#30340;&#32593;&#32476;&#65292;&#24182;&#34987;&#21457;&#29616;&#33021;&#22815;&#36830;&#32493;&#22320;&#26816;&#32034;&#21040;&#21512;&#25104;&#30340;&#22686;&#38271;&#36807;&#31243;&#65292;&#20197;&#21450;&#19968;&#31995;&#21015;&#32463;&#39564;&#32593;&#32476;&#30340;&#31616;&#21333;&#19988;&#21487;&#35299;&#37322;&#30340;&#35268;&#21017;&#12290;&#25105;&#20204;&#36890;&#36807;&#20462;&#25913;&#29983;&#25104;&#22120;&#26469;&#25193;&#23637;&#36825;&#31181;&#26041;&#27861;&#8230;
&lt;/p&gt;
&lt;p&gt;
Growing interest in modelling complex systems from brains to societies to cities using networks has led to increased efforts to describe generative processes that explain those networks. Recent successes in machine learning have prompted the usage of evolutionary computation, especially genetic programming to evolve computer programs that effectively forage a multidimensional search space to iteratively find better solutions that explain network structure. Symbolic regression contributes to these approaches by replicating network morphologies using both structure and processes, all while not relying on the scientists intuition or expertise. It distinguishes itself by introducing a novel formulation of a network generator and a parameter-free fitness function to evaluate the generated network and is found to consistently retrieve synthetically generated growth processes as well as simple, interpretable rules for a range of empirical networks. We extend this approach by modifying generat
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#26085;&#24120;&#29615;&#22659;&#20013;&#21033;&#29992;&#29983;&#29702;&#21644;&#29615;&#22659;&#25968;&#25454;&#26469;&#23458;&#35266;&#36319;&#36394;&#21387;&#21147;&#27700;&#24179;&#30340;&#30417;&#27979;&#31995;&#32479;&#65292;&#24182;&#25972;&#21512;&#20102;&#26234;&#33021;&#26631;&#35760;&#26041;&#27861;&#26469;&#20248;&#21270;&#25968;&#25454;&#25910;&#38598;&#12290;&#19977;&#23618;&#29289;&#32852;&#32593;&#31995;&#32479;&#26550;&#26500;&#34987;&#29992;&#26469;&#35299;&#20915;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.05367</link><description>&lt;p&gt;
&#22312;&#26085;&#24120;&#29615;&#22659;&#20013;&#21033;&#29992;&#21487;&#31359;&#25140;&#21644;&#31227;&#21160;&#25216;&#26415;&#36827;&#34892;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#21387;&#21147;&#30417;&#27979;
&lt;/p&gt;
&lt;p&gt;
Context-Aware Stress Monitoring using Wearable and Mobile Technologies in Everyday Settings. (arXiv:2401.05367v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05367
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#26085;&#24120;&#29615;&#22659;&#20013;&#21033;&#29992;&#29983;&#29702;&#21644;&#29615;&#22659;&#25968;&#25454;&#26469;&#23458;&#35266;&#36319;&#36394;&#21387;&#21147;&#27700;&#24179;&#30340;&#30417;&#27979;&#31995;&#32479;&#65292;&#24182;&#25972;&#21512;&#20102;&#26234;&#33021;&#26631;&#35760;&#26041;&#27861;&#26469;&#20248;&#21270;&#25968;&#25454;&#25910;&#38598;&#12290;&#19977;&#23618;&#29289;&#32852;&#32593;&#31995;&#32479;&#26550;&#26500;&#34987;&#29992;&#26469;&#35299;&#20915;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21387;&#21147;&#30340;&#26085;&#24120;&#30417;&#27979;&#26159;&#20445;&#25345;&#36523;&#24515;&#20581;&#24247;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#26368;&#36817;&#65292;&#29983;&#29702;&#20449;&#21495;&#21644;&#29615;&#22659;&#20449;&#24687;&#24050;&#32463;&#25104;&#20026;&#26816;&#27979;&#39640;&#21387;&#21147;&#24773;&#20917;&#30340;&#26377;&#24076;&#26395;&#30340;&#25351;&#26631;&#12290;&#28982;&#32780;&#65292;&#22312;&#26085;&#24120;&#29615;&#22659;&#20013;&#24320;&#21457;&#19968;&#20010;&#23454;&#26102;&#30417;&#27979;&#31995;&#32479;&#65292;&#21033;&#29992;&#29983;&#29702;&#21644;&#29615;&#22659;&#25968;&#25454;&#26469;&#39044;&#27979;&#21387;&#21147;&#27700;&#24179;&#65292;&#21516;&#26102;&#20174;&#21442;&#19982;&#32773;&#37027;&#37324;&#25910;&#38598;&#21387;&#21147;&#26631;&#31614;&#65292;&#36825;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#30417;&#27979;&#31995;&#32479;&#65292;&#22312;&#26085;&#24120;&#29983;&#27963;&#29615;&#22659;&#20013;&#21033;&#29992;&#29983;&#29702;&#21644;&#29615;&#22659;&#25968;&#25454;&#23458;&#35266;&#22320;&#36319;&#36394;&#27599;&#22825;&#30340;&#21387;&#21147;&#27700;&#24179;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25972;&#21512;&#20102;&#26234;&#33021;&#26631;&#35760;&#26041;&#27861;&#26469;&#20248;&#21270;&#29983;&#24577;&#26102;&#21051;&#35780;&#20272;&#65288;EMA&#65289;&#30340;&#25910;&#38598;&#65292;&#35813;&#25910;&#38598;&#23545;&#20110;&#24314;&#31435;&#21387;&#21147;&#26816;&#27979;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26159;&#24517;&#38656;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#23618;&#29289;&#32852;&#32593;&#31995;&#32479;&#26550;&#26500;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#25105;&#20204;&#21033;&#29992;&#20132;&#21449;&#39564;&#35777;&#25216;&#26415;&#26469;&#20934;&#30830;&#20272;&#35745;&#21387;&#21147;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Daily monitoring of stress is a critical component of maintaining optimal physical and mental health. Physiological signals and contextual information have recently emerged as promising indicators for detecting instances of heightened stress. Nonetheless, developing a real-time monitoring system that utilizes both physiological and contextual data to anticipate stress levels in everyday settings while also gathering stress labels from participants represents a significant challenge. We present a monitoring system that objectively tracks daily stress levels by utilizing both physiological and contextual data in a daily-life environment. Additionally, we have integrated a smart labeling approach to optimize the ecological momentary assessment (EMA) collection, which is required for building machine learning models for stress detection. We propose a three-tier Internet-of-Things-based system architecture to address the challenges. We utilized a cross-validation technique to accurately est
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31359;&#25140;&#35774;&#22791;&#30340;&#22312;&#32447;&#21160;&#20316;&#35782;&#21035;&#26694;&#26550;&#65292;&#34701;&#21512;&#20102;&#22312;&#32447;&#20154;&#20307;&#29366;&#24577;&#20272;&#35745;&#12289;&#21160;&#20316;&#35782;&#21035;&#21644;&#36816;&#21160;&#39044;&#27979;&#65292;&#23454;&#29616;&#20102;&#22312;&#20030;&#37325;&#20219;&#21153;&#20013;&#23545;&#24037;&#20154;&#29983;&#29289;&#21147;&#23398;&#39118;&#38505;&#30340;&#26089;&#26399;&#35780;&#20272;&#21644;&#39044;&#38450;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;NIOSH&#25351;&#25968;&#36827;&#34892;&#22312;&#32447;&#39118;&#38505;&#35780;&#20272;&#65292;&#24182;&#36890;&#36807;&#39044;&#27979;&#26410;&#26469;&#30340;&#36816;&#21160;&#21644;&#35302;&#35273;&#35686;&#31034;&#26469;&#39044;&#38450;&#28508;&#22312;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2401.05365</link><description>&lt;p&gt;
&#22522;&#20110;&#31359;&#25140;&#35774;&#22791;&#30340;&#22312;&#32447;&#20154;&#20307;&#39118;&#38505;&#39044;&#27979;&#21450;&#39044;&#26399;&#35302;&#35273;&#35686;&#31034;&#30340;&#21160;&#20316;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Online Action Recognition for Human Risk Prediction with Anticipated Haptic Alert via Wearables. (arXiv:2401.05365v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05365
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31359;&#25140;&#35774;&#22791;&#30340;&#22312;&#32447;&#21160;&#20316;&#35782;&#21035;&#26694;&#26550;&#65292;&#34701;&#21512;&#20102;&#22312;&#32447;&#20154;&#20307;&#29366;&#24577;&#20272;&#35745;&#12289;&#21160;&#20316;&#35782;&#21035;&#21644;&#36816;&#21160;&#39044;&#27979;&#65292;&#23454;&#29616;&#20102;&#22312;&#20030;&#37325;&#20219;&#21153;&#20013;&#23545;&#24037;&#20154;&#29983;&#29289;&#21147;&#23398;&#39118;&#38505;&#30340;&#26089;&#26399;&#35780;&#20272;&#21644;&#39044;&#38450;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;NIOSH&#25351;&#25968;&#36827;&#34892;&#22312;&#32447;&#39118;&#38505;&#35780;&#20272;&#65292;&#24182;&#36890;&#36807;&#39044;&#27979;&#26410;&#26469;&#30340;&#36816;&#21160;&#21644;&#35302;&#35273;&#35686;&#31034;&#26469;&#39044;&#38450;&#28508;&#22312;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#22312;&#32447;&#20154;&#20307;&#29366;&#24577;&#20272;&#35745;&#12289;&#21160;&#20316;&#35782;&#21035;&#21644;&#36816;&#21160;&#39044;&#27979;&#32467;&#21512;&#36215;&#26469;&#65292;&#23454;&#29616;&#22312;&#20030;&#37325;&#20219;&#21153;&#20013;&#25552;&#21069;&#35780;&#20272;&#21644;&#39044;&#38450;&#24037;&#20154;&#29983;&#29289;&#21147;&#23398;&#39118;&#38505;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;NIOSH&#25351;&#25968;&#36827;&#34892;&#22312;&#32447;&#39118;&#38505;&#35780;&#20272;&#65292;&#36866;&#29992;&#20110;&#23454;&#26102;&#24212;&#29992;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#36870;&#36816;&#21160;&#23398;/&#21160;&#21147;&#23398;&#31639;&#27861;&#20174;&#21487;&#31359;&#25140;&#20256;&#24863;&#22120;&#25968;&#25454;&#20013;&#26816;&#32034;&#20154;&#20307;&#29366;&#24577;&#12290;&#36890;&#36807;&#23454;&#29616;&#22522;&#20110;LSTM&#30340;&#24341;&#23548;&#19987;&#23478;&#32452;&#21512;&#26550;&#26500;&#65292;&#23454;&#29616;&#20154;&#20307;&#21160;&#20316;&#35782;&#21035;&#21644;&#36816;&#21160;&#39044;&#27979;&#12290;&#35782;&#21035;&#20986;&#30340;&#21160;&#20316;&#23558;&#19968;&#27425;&#20030;&#37325;&#27963;&#21160;&#20998;&#20026;&#19968;&#31995;&#21015;&#36830;&#32493;&#30340;&#21160;&#20316;&#65292;&#21487;&#24212;&#29992;&#20462;&#35746;&#21518;&#30340;NIOSH&#20030;&#37325;&#26041;&#31243;&#36827;&#34892;&#39118;&#38505;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#39044;&#27979;&#30340;&#36816;&#21160;&#21487;&#20197;&#39044;&#27979;&#26410;&#26469;&#30340;&#39118;&#38505;&#12290;&#21487;&#31359;&#25140;&#31995;&#32479;&#20013;&#23884;&#20837;&#30340;&#35302;&#35273;&#25191;&#34892;&#22120;&#33021;&#22815;&#35686;&#31034;&#21463;&#35797;&#32773;&#28508;&#22312;&#30340;&#39118;&#38505;&#65292;&#20316;&#20026;&#19968;&#31181;&#20027;&#21160;&#30340;&#39044;&#38450;&#35013;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a framework that combines online human state estimation, action recognition and motion prediction to enable early assessment and prevention of worker biomechanical risk during lifting tasks. The framework leverages the NIOSH index to perform online risk assessment, thus fitting real-time applications. In particular, the human state is retrieved via inverse kinematics/dynamics algorithms from wearable sensor data. Human action recognition and motion prediction are achieved by implementing an LSTM-based Guided Mixture of Experts architecture, which is trained offline and inferred online. With the recognized actions, a single lifting activity is divided into a series of continuous movements and the Revised NIOSH Lifting Equation can be applied for risk assessment. Moreover, the predicted motions enable anticipation of future risks. A haptic actuator, embedded in the wearable system, can alert the subject of potential risk, acting as an active prevention device. The per
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#30561;&#30496;&#20998;&#26399;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22495;&#27867;&#21270;&#27010;&#24565;&#65292;&#32467;&#21512;&#22810;&#32423;&#29305;&#24449;&#23545;&#40784;&#30340;&#24605;&#24819;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#23545;&#26410;&#35265;&#36807;&#25968;&#25454;&#38598;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.05363</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#32423;&#22495;&#23545;&#40784;&#23454;&#29616;&#36890;&#29992;&#30340;&#30561;&#30496;&#20998;&#26399;
&lt;/p&gt;
&lt;p&gt;
Generalizable Sleep Staging via Multi-level Domain Alignment. (arXiv:2401.05363v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05363
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#30561;&#30496;&#20998;&#26399;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22495;&#27867;&#21270;&#27010;&#24565;&#65292;&#32467;&#21512;&#22810;&#32423;&#29305;&#24449;&#23545;&#40784;&#30340;&#24605;&#24819;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#23545;&#26410;&#35265;&#36807;&#25968;&#25454;&#38598;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#30561;&#30496;&#20998;&#26399;&#23545;&#20110;&#30561;&#30496;&#35780;&#20272;&#21644;&#30142;&#30149;&#35786;&#26029;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#26041;&#27861;&#20381;&#36182;&#20110;&#29305;&#23450;&#25968;&#25454;&#38598;&#65292;&#24182;&#19988;&#20165;&#36866;&#29992;&#20110;&#30456;&#21516;&#25968;&#25454;&#38598;&#30340;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#22495;&#27867;&#21270;&#27010;&#24565;&#21040;&#33258;&#21160;&#30561;&#30496;&#20998;&#26399;&#20013;&#65292;&#24182;&#25552;&#20986;&#20102;&#36890;&#29992;&#30340;&#30561;&#30496;&#20998;&#26399;&#20219;&#21153;&#65292;&#26088;&#22312;&#25552;&#39640;&#27169;&#22411;&#23545;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#38598;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#21463;&#21040;&#29616;&#26377;&#30340;&#22495;&#27867;&#21270;&#26041;&#27861;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#37319;&#29992;&#29305;&#24449;&#23545;&#40784;&#24605;&#24819;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SleepDG&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;&#32771;&#34385;&#21040;&#23616;&#37096;&#26174;&#33879;&#29305;&#24449;&#21644;&#26102;&#24207;&#29305;&#24449;&#23545;&#20110;&#30561;&#30496;&#20998;&#26399;&#37117;&#24456;&#37325;&#35201;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#32423;&#29305;&#24449;&#23545;&#40784;&#65292;&#23558;&#26102;&#20195;&#32423;&#21644;&#24207;&#21015;&#32423;&#29305;&#24449;&#23545;&#40784;&#26469;&#23398;&#20064;&#22495;&#19981;&#21464;&#29305;&#24449;&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26102;&#20195;&#32423;&#29305;&#24449;&#23545;&#40784;&#26041;&#27861;&#65292;&#23545;&#19981;&#21516;&#30561;&#30496;&#26102;&#20195;&#30340;&#29305;&#24449;&#20998;&#24067;&#36827;&#34892;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic sleep staging is essential for sleep assessment and disorder diagnosis. Most existing methods depend on one specific dataset and are limited to be generalized to other unseen datasets, for which the training data and testing data are from the same dataset. In this paper, we introduce domain generalization into automatic sleep staging and propose the task of generalizable sleep staging which aims to improve the model generalization ability to unseen datasets. Inspired by existing domain generalization methods, we adopt the feature alignment idea and propose a framework called SleepDG to solve it. Considering both of local salient features and sequential features are important for sleep staging, we propose a Multi-level Feature Alignment combining epoch-level and sequence-level feature alignment to learn domain-invariant feature representations. Specifically, we design an Epoch-level Feature Alignment to align the feature distribution of each single sleep epoch among different 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;U-SWIM&#65292;&#19968;&#31181;&#29992;&#20110;&#35745;&#31639;&#20869;&#23384;&#31070;&#32463;&#21152;&#36895;&#22120;&#30340;&#36890;&#29992;&#36873;&#25321;&#24615;&#20889;&#20837;&#39564;&#35777;&#25216;&#26415;&#12290;&#36890;&#36807;&#21482;&#23545;&#23569;&#37327;&#26435;&#37325;&#36827;&#34892;&#20889;&#20837;&#39564;&#35777;&#22788;&#29702;&#65292;&#21487;&#20197;&#21152;&#24555;&#32534;&#31243;&#36895;&#24230;&#24182;&#20445;&#25345;DNN&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.05357</link><description>&lt;p&gt;
U-SWIM: &#36890;&#29992;&#36873;&#25321;&#24615;&#20889;&#20837;&#39564;&#35777;&#25216;&#26415;&#29992;&#20110;&#35745;&#31639;&#20869;&#23384;&#31070;&#32463;&#21152;&#36895;&#22120;
&lt;/p&gt;
&lt;p&gt;
U-SWIM: Universal Selective Write-Verify for Computing-in-Memory Neural Accelerators. (arXiv:2401.05357v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05357
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;U-SWIM&#65292;&#19968;&#31181;&#29992;&#20110;&#35745;&#31639;&#20869;&#23384;&#31070;&#32463;&#21152;&#36895;&#22120;&#30340;&#36890;&#29992;&#36873;&#25321;&#24615;&#20889;&#20837;&#39564;&#35777;&#25216;&#26415;&#12290;&#36890;&#36807;&#21482;&#23545;&#23569;&#37327;&#26435;&#37325;&#36827;&#34892;&#20889;&#20837;&#39564;&#35777;&#22788;&#29702;&#65292;&#21487;&#20197;&#21152;&#24555;&#32534;&#31243;&#36895;&#24230;&#24182;&#20445;&#25345;DNN&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#26032;&#20852;&#30340;&#38750;&#26131;&#22833;&#24615;&#20869;&#23384;(Non-Volatile Memory, NVM)&#35774;&#22791;&#26469;&#23454;&#29616;&#35745;&#31639;&#20869;&#23384;(Computing-in-Memory, CiM)&#30340;&#26550;&#26500;&#30001;&#20110;&#20854;&#24778;&#20154;&#30340;&#33021;&#25928;&#32780;&#25104;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#21152;&#36895;&#30340;&#26377;&#21147;&#31454;&#20105;&#32773;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#36825;&#20123;&#26032;&#20852;&#35774;&#22791;&#26102;&#20250;&#38754;&#20020;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65306;&#22312;&#26435;&#37325;&#26144;&#23556;&#36807;&#31243;&#20013;&#20250;&#20986;&#29616;&#22823;&#30340;&#21464;&#21270;&#12290;&#22914;&#26524;&#19981;&#21152;&#20197;&#32531;&#35299;&#65292;&#36825;&#20250;&#20005;&#37325;&#24433;&#21709;DNN&#30340;&#20934;&#30830;&#24615;&#12290;&#38024;&#23545;&#19981;&#23436;&#32654;&#30340;&#26435;&#37325;&#26144;&#23556;&#65292;&#19968;&#20010;&#34987;&#24191;&#27867;&#25509;&#21463;&#30340;&#35299;&#20915;&#26041;&#27861;&#26159;&#36845;&#20195;&#24335;&#20889;&#20837;&#39564;&#35777;&#26041;&#27861;&#65292;&#36825;&#28041;&#21450;&#39564;&#35777;&#23548;&#32435;&#20540;&#24182;&#22312;&#38656;&#35201;&#26102;&#35843;&#25972;&#35774;&#22791;&#12290;&#29616;&#26377;&#30340;&#25152;&#26377;&#21457;&#34920;&#20316;&#21697;&#37117;&#23558;&#27492;&#36807;&#31243;&#24212;&#29992;&#20110;&#27599;&#20010;&#21333;&#29420;&#30340;&#35774;&#22791;&#65292;&#23548;&#33268;&#26174;&#33879;&#30340;&#32534;&#31243;&#26102;&#38388;&#24320;&#38144;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#21482;&#26377;&#19968;&#20010;&#23567;&#37096;&#20998;&#26435;&#37325;&#38656;&#35201;&#36827;&#34892;&#36825;&#31181;&#20889;&#20837;&#39564;&#35777;&#22788;&#29702;&#65292;&#21487;&#20197;&#20445;&#25345;&#30456;&#24212;&#35774;&#22791;&#21644;DNN&#30340;&#20934;&#30830;&#24615;&#65292;&#20174;&#32780;&#21152;&#24555;&#32534;&#31243;&#36895;&#24230;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861; USWIM&#65292;
&lt;/p&gt;
&lt;p&gt;
Architectures that incorporate Computing-in-Memory (CiM) using emerging non-volatile memory (NVM) devices have become strong contenders for deep neural network (DNN) acceleration due to their impressive energy efficiency. Yet, a significant challenge arises when using these emerging devices: they can show substantial variations during the weight-mapping process. This can severely impact DNN accuracy if not mitigated. A widely accepted remedy for imperfect weight mapping is the iterative write-verify approach, which involves verifying conductance values and adjusting devices if needed. In all existing publications, this procedure is applied to every individual device, resulting in a significant programming time overhead. In our research, we illustrate that only a small fraction of weights need this write-verify treatment for the corresponding devices and the DNN accuracy can be preserved, yielding a notable programming acceleration. Building on this, we introduce USWIM, a novel method b
&lt;/p&gt;</description></item><item><title>&#24320;&#21457;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#34920;&#38754;&#32570;&#38519;&#26816;&#27979;&#30340;&#36164;&#28304;&#21463;&#38480;EdgeAI&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.05355</link><description>&lt;p&gt;
&#24320;&#21457;&#36866;&#29992;&#20110;&#34920;&#38754;&#32570;&#38519;&#26816;&#27979;&#30340;&#36164;&#28304;&#21463;&#38480;EdgeAI&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Developing a Resource-Constraint EdgeAI model for Surface Defect Detection. (arXiv:2401.05355v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05355
&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#34920;&#38754;&#32570;&#38519;&#26816;&#27979;&#30340;&#36164;&#28304;&#21463;&#38480;EdgeAI&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36164;&#28304;&#38480;&#21046;&#38480;&#21046;&#20102;&#20960;&#20010;EdgeAI&#24212;&#29992;&#31243;&#24207;&#21482;&#33021;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25512;&#29702;&#26041;&#27861;&#65292;&#20854;&#20013;&#27169;&#22411;&#22312;&#20113;&#31471;&#35757;&#32451;&#24182;&#37096;&#32626;&#21040;&#36793;&#32536;&#35774;&#22791;&#12290;&#36825;&#24102;&#26469;&#20102;&#35832;&#22914;&#24102;&#23485;&#12289;&#24310;&#36831;&#21644;&#38544;&#31169;&#31561;&#19982;&#23384;&#20648;&#25968;&#25454;&#22312;&#22806;&#37096;&#36827;&#34892;&#27169;&#22411;&#26500;&#24314;&#30340;&#25361;&#25112;&#12290;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#36890;&#36807;&#28040;&#38500;&#23558;&#25968;&#25454;&#20256;&#36755;&#21040;&#21478;&#19968;&#35774;&#22791;&#36827;&#34892;&#23384;&#20648;&#21644;&#27169;&#22411;&#24320;&#21457;&#30340;&#38656;&#27714;&#26469;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#12290;&#22312;&#35774;&#22791;&#19978;&#36827;&#34892;&#35757;&#32451;&#36824;&#21487;&#20197;&#25552;&#20379;&#23545;&#25968;&#25454;&#21464;&#21270;&#30340;&#31283;&#20581;&#24615;&#65292;&#22240;&#20026;&#21487;&#20197;&#20351;&#29992;&#26032;&#33719;&#21462;&#30340;&#25968;&#25454;&#23545;&#27169;&#22411;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;EdgeAI&#26550;&#26500;&#65292;&#25913;&#32534;&#33258;Xception&#65292;&#29992;&#20110;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#29615;&#22659;&#20013;&#36827;&#34892;&#35774;&#22791;&#19978;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#23545;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;PCB&#32570;&#38519;&#26816;&#27979;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#23558;&#20854;&#24615;&#33021;&#19982;&#29616;&#26377;&#30340;&#36731;&#37327;&#32423;&#27169;&#22411;-MobileNetV2&#12289;EfficientNetV2B0&#21644;MobileViT-XXS&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#23454;&#39564;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20855;&#26377;&#26174;&#30528;&#30340;&#24615;&#33021;&#65292;&#27979;&#35797;&#20934;&#30830;&#29575;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Resource constraints have restricted several EdgeAI applications to machine learning inference approaches, where models are trained on the cloud and deployed to the edge device. This poses challenges such as bandwidth, latency, and privacy associated with storing data off-site for model building. Training on the edge device can overcome these challenges by eliminating the need to transfer data to another device for storage and model development. On-device training also provides robustness to data variations as models can be retrained on newly acquired data to improve performance. We, therefore, propose a lightweight EdgeAI architecture modified from Xception, for on-device training in a resource-constraint edge environment. We evaluate our model on a PCB defect detection task and compare its performance against existing lightweight models - MobileNetV2, EfficientNetV2B0, and MobileViT-XXS. The results of our experiment show that our model has a remarkable performance with a test accura
&lt;/p&gt;</description></item><item><title>ImbaGCD&#26159;&#19968;&#20010;&#20197;&#26368;&#20248;&#20256;&#36755;&#20026;&#22522;&#30784;&#30340;&#26399;&#26395;&#26497;&#22823;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#19981;&#24179;&#34913;&#30340;&#24191;&#20041;&#31867;&#21035;&#21457;&#29616;&#38382;&#39064;&#65292;&#36890;&#36807;&#23545;&#40784;&#36793;&#32536;&#31867;&#21035;&#20808;&#39564;&#20998;&#24067;&#26469;&#25512;&#26029;&#24050;&#30693;&#21644;&#26410;&#30693;&#31867;&#21035;&#12290;</title><link>http://arxiv.org/abs/2401.05353</link><description>&lt;p&gt;
ImbaGCD: &#19981;&#24179;&#34913;&#30340;&#24191;&#20041;&#31867;&#21035;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
ImbaGCD: Imbalanced Generalized Category Discovery. (arXiv:2401.05353v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05353
&lt;/p&gt;
&lt;p&gt;
ImbaGCD&#26159;&#19968;&#20010;&#20197;&#26368;&#20248;&#20256;&#36755;&#20026;&#22522;&#30784;&#30340;&#26399;&#26395;&#26497;&#22823;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#19981;&#24179;&#34913;&#30340;&#24191;&#20041;&#31867;&#21035;&#21457;&#29616;&#38382;&#39064;&#65292;&#36890;&#36807;&#23545;&#40784;&#36793;&#32536;&#31867;&#21035;&#20808;&#39564;&#20998;&#24067;&#26469;&#25512;&#26029;&#24050;&#30693;&#21644;&#26410;&#30693;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24191;&#20041;&#31867;&#21035;&#21457;&#29616;&#65288;GCD&#65289;&#26088;&#22312;&#21033;&#29992;&#24050;&#26631;&#35760;&#30340;&#24050;&#30693;&#31867;&#21035;&#38598;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#25512;&#26029;&#20986;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#20013;&#30340;&#24050;&#30693;&#21644;&#26410;&#30693;&#31867;&#21035;&#12290;&#29616;&#26377;&#30740;&#31350;&#38544;&#21547;&#22320;&#25110;&#26126;&#30830;&#22320;&#20551;&#35774;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#27599;&#20010;&#31867;&#21035;&#65288;&#24050;&#30693;&#25110;&#26410;&#30693;&#65289;&#30340;&#20986;&#29616;&#39057;&#29575;&#22823;&#33268;&#30456;&#21516;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#26681;&#25454;&#35270;&#35273;&#31867;&#21035;&#30340;&#38271;&#23614;&#29305;&#24615;&#65292;&#25105;&#20204;&#26356;&#26377;&#21487;&#33021;&#36935;&#21040;&#24050;&#30693;/&#24120;&#35265;&#31867;&#21035;&#32780;&#19981;&#26159;&#26410;&#30693;/&#19981;&#24120;&#35265;&#31867;&#21035;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#19988;&#23454;&#29992;&#30340;&#38382;&#39064;&#65292;&#21363;&#19981;&#24179;&#34913;&#30340;&#24191;&#20041;&#31867;&#21035;&#21457;&#29616;&#65288;ImbaGCD&#65289;&#65292;&#20854;&#20013;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#20998;&#24067;&#19981;&#24179;&#34913;&#65292;&#24050;&#30693;&#31867;&#21035;&#27604;&#26410;&#30693;&#31867;&#21035;&#26356;&#39057;&#32321;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ImbaGCD&#65292;&#19968;&#31181;&#22522;&#20110;&#26368;&#20248;&#20256;&#36755;&#30340;&#26399;&#26395;&#26497;&#22823;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#40784;&#36793;&#32536;&#31867;&#21035;&#20808;&#39564;&#20998;&#24067;&#26469;&#23454;&#29616;&#24191;&#20041;&#31867;&#21035;&#21457;&#29616;&#12290;ImbaGCD&#36824;&#21253;&#25324;&#19968;&#20010;&#31995;&#32479;&#26426;&#21046;&#26469;&#20272;&#35745;&#26410;&#30693;&#31867;&#21035;&#30340;&#23545;&#40784;&#21518;&#39564;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generalized class discovery (GCD) aims to infer known and unknown categories in an unlabeled dataset leveraging prior knowledge of a labeled set comprising known classes. Existing research implicitly/explicitly assumes that the frequency of occurrence for each category, whether known or unknown, is approximately the same in the unlabeled data. However, in nature, we are more likely to encounter known/common classes than unknown/uncommon ones, according to the long-tailed property of visual classes. Therefore, we present a challenging and practical problem, Imbalanced Generalized Category Discovery (ImbaGCD), where the distribution of unlabeled data is imbalanced, with known classes being more frequent than unknown ones. To address these issues, we propose ImbaGCD, A novel optimal transport-based expectation maximization framework that accomplishes generalized category discovery by aligning the marginal class prior distribution. ImbaGCD also incorporates a systematic mechanism for estim
&lt;/p&gt;</description></item><item><title>&#38271;&#23614;&#35782;&#21035;&#20013;&#30340;&#36890;&#29992;&#31867;&#21035;&#21457;&#29616;&#26041;&#27861;(GCD)&#30340;&#37325;&#22823;&#38480;&#21046;&#26159;&#20551;&#35774;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#30340;&#31867;&#21035;&#20998;&#24067;&#26159;&#22343;&#34913;&#30340;&#65292;&#32780;&#20107;&#23454;&#19978;&#33258;&#28982;&#29615;&#22659;&#20013;&#30340;&#35270;&#35273;&#31867;&#21035;&#36890;&#24120;&#21576;&#29616;&#38271;&#23614;&#20998;&#24067;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38271;&#23614;&#36890;&#29992;&#31867;&#21035;&#21457;&#29616;&#65288;Long-tailed GCD&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#20010;&#31574;&#30053;&#24615;&#27491;&#21017;&#21270;&#23454;&#29616;&#20102;&#23545;&#36739;&#23569;&#20986;&#29616;&#30340;&#23614;&#37096;&#31867;&#21035;&#30340;&#37325;&#35201;&#24615;&#30340;&#22686;&#24378;&#12290;</title><link>http://arxiv.org/abs/2401.05352</link><description>&lt;p&gt;
&#38271;&#23614;&#35782;&#21035;&#20013;&#30340;&#36890;&#29992;&#31867;&#21035;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Generalized Categories Discovery for Long-tailed Recognition. (arXiv:2401.05352v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05352
&lt;/p&gt;
&lt;p&gt;
&#38271;&#23614;&#35782;&#21035;&#20013;&#30340;&#36890;&#29992;&#31867;&#21035;&#21457;&#29616;&#26041;&#27861;(GCD)&#30340;&#37325;&#22823;&#38480;&#21046;&#26159;&#20551;&#35774;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#30340;&#31867;&#21035;&#20998;&#24067;&#26159;&#22343;&#34913;&#30340;&#65292;&#32780;&#20107;&#23454;&#19978;&#33258;&#28982;&#29615;&#22659;&#20013;&#30340;&#35270;&#35273;&#31867;&#21035;&#36890;&#24120;&#21576;&#29616;&#38271;&#23614;&#20998;&#24067;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38271;&#23614;&#36890;&#29992;&#31867;&#21035;&#21457;&#29616;&#65288;Long-tailed GCD&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#20010;&#31574;&#30053;&#24615;&#27491;&#21017;&#21270;&#23454;&#29616;&#20102;&#23545;&#36739;&#23569;&#20986;&#29616;&#30340;&#23614;&#37096;&#31867;&#21035;&#30340;&#37325;&#35201;&#24615;&#30340;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#29992;&#31867;&#21035;&#21457;&#29616;&#65288;GCD&#65289;&#22312;&#20174;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#20013;&#35782;&#21035;&#24050;&#30693;&#21644;&#26410;&#30693;&#31867;&#21035;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#23427;&#21033;&#29992;&#20102;&#36890;&#36807;&#24050;&#26631;&#35760;&#31867;&#21035;&#38598;&#21512;&#33719;&#21462;&#30340;&#27934;&#23519;&#21147;&#12290;&#29616;&#26377;&#30340;GCD&#26041;&#27861;&#30340;&#19968;&#20010;&#26174;&#33879;&#38480;&#21046;&#26159;&#23427;&#20204;&#20551;&#35774;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#30340;&#31867;&#21035;&#20998;&#24067;&#26159;&#22343;&#34913;&#30340;&#12290;&#19982;&#36825;&#19968;&#20551;&#35774;&#30456;&#21453;&#65292;&#33258;&#28982;&#29615;&#22659;&#20013;&#30340;&#35270;&#35273;&#31867;&#21035;&#36890;&#24120;&#34920;&#29616;&#20986;&#38271;&#23614;&#20998;&#24067;&#65292;&#24050;&#30693;&#25110;&#26222;&#36941;&#30340;&#31867;&#21035;&#27604;&#32597;&#35265;&#30340;&#31867;&#21035;&#26356;&#39057;&#32321;&#22320;&#20986;&#29616;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#33268;&#21147;&#20110;&#24357;&#21512;&#36825;&#31181;&#24046;&#36317;&#65292;&#30528;&#37325;&#20110;&#38271;&#23614;&#36890;&#29992;&#31867;&#21035;&#21457;&#29616;&#65288;Long-tailed GCD&#65289;&#33539;&#24335;&#65292;&#35813;&#33539;&#24335;&#21453;&#26144;&#20102;&#29616;&#23454;&#19990;&#30028;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#30340;&#22266;&#26377;&#19981;&#24179;&#34913;&#24615;&#12290;&#38024;&#23545;&#38271;&#23614;GCD&#25152;&#24102;&#26469;&#30340;&#29420;&#29305;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20004;&#20010;&#31574;&#30053;&#24615;&#27491;&#21017;&#21270;&#30340;&#24378;&#22823;&#26041;&#27861;:&#65288;i&#65289;&#19968;&#31181;&#21152;&#26435;&#26426;&#21046;&#65292;&#22686;&#24378;&#20102;&#36739;&#23569;&#20986;&#29616;&#30340;&#23614;&#37096;&#31867;&#21035;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generalized Class Discovery (GCD) plays a pivotal role in discerning both known and unknown categories from unlabeled datasets by harnessing the insights derived from a labeled set comprising recognized classes. A significant limitation in prevailing GCD methods is their presumption of an equitably distributed category occurrence in unlabeled data. Contrary to this assumption, visual classes in natural environments typically exhibit a long-tailed distribution, with known or prevalent categories surfacing more frequently than their rarer counterparts. Our research endeavors to bridge this disconnect by focusing on the long-tailed Generalized Category Discovery (Long-tailed GCD) paradigm, which echoes the innate imbalances of real-world unlabeled datasets. In response to the unique challenges posed by Long-tailed GCD, we present a robust methodology anchored in two strategic regularizations: (i) a reweighting mechanism that bolsters the prominence of less-represented, tail-end categories
&lt;/p&gt;</description></item><item><title>&#20256;&#32479;&#30340;RNA&#20108;&#32423;&#32467;&#26500;&#39044;&#27979;&#35780;&#20272;&#25351;&#26631;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#26680;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#33021;&#22815;&#20844;&#24179;&#20934;&#30830;&#22320;&#35780;&#20272;RNA&#32467;&#26500;&#39044;&#27979;&#31639;&#27861;&#65292;&#24182;&#22312;RNA&#35774;&#35745;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#20449;&#24687;&#25351;&#23548;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.05351</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;RNA&#20108;&#32423;&#32467;&#26500;&#38382;&#39064;&#30340;&#24615;&#33021;&#24230;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Rethinking Performance Measures of RNA Secondary Structure Problems. (arXiv:2401.05351v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05351
&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;RNA&#20108;&#32423;&#32467;&#26500;&#39044;&#27979;&#35780;&#20272;&#25351;&#26631;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#26680;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#33021;&#22815;&#20844;&#24179;&#20934;&#30830;&#22320;&#35780;&#20272;RNA&#32467;&#26500;&#39044;&#27979;&#31639;&#27861;&#65292;&#24182;&#22312;RNA&#35774;&#35745;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#20449;&#24687;&#25351;&#23548;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;RNA&#20108;&#32423;&#32467;&#26500;&#39044;&#27979;&#23545;&#20110;&#29702;&#35299;&#32454;&#32990;&#35843;&#25511;&#21644;&#30142;&#30149;&#26426;&#21046;&#33267;&#20851;&#37325;&#35201;&#12290;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#26041;&#27861;&#36890;&#36807;&#39044;&#27979;&#20266;&#32467;&#21644;&#22810;&#31181;&#30456;&#20114;&#20316;&#29992;&#30340;&#30897;&#22522;&#23545;&#31561;&#22797;&#26434;&#29305;&#24449;&#65292;&#24050;&#32463;&#36229;&#36234;&#20102;&#20256;&#32479;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#36317;&#31163;&#24230;&#37327;&#22312;&#22788;&#29702;&#36825;&#31181;&#19977;&#32423;&#30456;&#20114;&#20316;&#29992;&#26102;&#24456;&#22256;&#38590;&#65292;&#30446;&#21069;&#20351;&#29992;&#30340;&#35780;&#20272;&#25351;&#26631;&#65288;F1&#20998;&#25968;&#65292;MCC&#65289;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Weisfeiler-Lehman&#22270;&#26680;&#65288;WL&#65289;&#20316;&#20026;&#19968;&#31181;&#26367;&#20195;&#24230;&#37327;&#26041;&#24335;&#12290;&#37319;&#29992;&#20687;WL&#36825;&#26679;&#30340;&#22522;&#20110;&#22270;&#30340;&#25351;&#26631;&#33021;&#22815;&#20844;&#24179;&#32780;&#20934;&#30830;&#22320;&#35780;&#20272;RNA&#32467;&#26500;&#39044;&#27979;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;WL&#25552;&#20379;&#20102;&#26377;&#20449;&#24687;&#37327;&#30340;&#25351;&#23548;&#65292;&#22914;&#22312;RNA&#35774;&#35745;&#23454;&#39564;&#20013;&#30340;&#23637;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate RNA secondary structure prediction is vital for understanding cellular regulation and disease mechanisms. Deep learning (DL) methods have surpassed traditional algorithms by predicting complex features like pseudoknots and multi-interacting base pairs. However, traditional distance measures can hardly deal with such tertiary interactions and the currently used evaluation measures (F1 score, MCC) have limitations. We propose the Weisfeiler-Lehman graph kernel (WL) as an alternative metric. Embracing graph-based metrics like WL enables fair and accurate evaluation of RNA structure prediction algorithms. Further, WL provides informative guidance, as demonstrated in an RNA design experiment.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#20013;&#30340;&#25506;&#32034;&#21644;&#24320;&#21457;&#24179;&#34913;&#38382;&#39064;&#65292;&#20197;&#21450;&#21033;&#29992;&#24191;&#20041;&#32463;&#39564;&#26469;&#21457;&#23637;&#36890;&#29992;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2401.05350</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#36816;&#31639;&#31526;&#36873;&#25321;&#21033;&#29992;&#24191;&#20041;&#32463;&#39564;
&lt;/p&gt;
&lt;p&gt;
Adaptive operator selection utilising generalised experience. (arXiv:2401.05350v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05350
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#20013;&#30340;&#25506;&#32034;&#21644;&#24320;&#21457;&#24179;&#34913;&#38382;&#39064;&#65292;&#20197;&#21450;&#21033;&#29992;&#24191;&#20041;&#32463;&#39564;&#26469;&#21457;&#23637;&#36890;&#29992;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20248;&#21270;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#30001;&#20110;&#20854;&#22797;&#26434;&#24615;&#21644;&#38590;&#24230;&#32780;&#38590;&#20197;&#35299;&#20915;&#12290;&#36827;&#21270;&#21644;&#32676;&#20307;&#26234;&#33021;&#31639;&#27861;&#24050;&#25104;&#21151;&#35299;&#20915;&#36825;&#31867;&#38382;&#39064;&#65292;&#23588;&#20854;&#26159;&#22312;&#20108;&#36827;&#21046;&#26684;&#24335;&#20013;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25506;&#32034;&#21644;&#24320;&#21457;&#27963;&#21160;&#20043;&#38388;&#30340;&#24179;&#34913;&#38382;&#39064;(EvE)&#65292;&#36825;&#31181;&#36817;&#20284;&#21487;&#33021;&#21463;&#21040;&#24433;&#21709;&#65292;&#36825;&#20173;&#28982;&#26159;&#36825;&#20010;&#39046;&#22495;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#23613;&#31649;&#20351;&#29992;&#22810;&#20010;&#36816;&#31639;&#31526;&#36827;&#34892;&#20114;&#34917;&#20351;&#29992;&#30340;&#33258;&#36866;&#24212;&#36816;&#31639;&#31526;&#36873;&#25321;&#26041;&#26696;&#22312;&#31649;&#29702;EvE&#26041;&#38754;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#20294;&#23450;&#21046;&#30340;&#33258;&#36866;&#24212;&#36873;&#25321;&#31995;&#32479;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#35758;&#39064;&#12290;&#26368;&#36817;&#65292;&#24378;&#21270;&#23398;&#20064;(RL)&#34987;&#25552;&#20986;&#20316;&#20026;&#19968;&#31181;&#23450;&#21046;&#21644;&#22609;&#36896;&#39640;&#24230;&#26377;&#25928;&#30340;&#33258;&#36866;&#24212;&#36873;&#25321;&#31995;&#32479;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20174;&#21487;&#20280;&#32553;&#24615;&#30340;&#35282;&#24230;&#26469;&#22788;&#29702;&#35813;&#38382;&#39064;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;RL&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#24110;&#21161;&#24320;&#21457;&#36890;&#29992;&#26694;&#26550;&#65292;&#20197;&#33719;&#24471;&#12289;&#22788;&#29702;&#21644;&#21033;&#29992;&#24191;&#20041;&#32463;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimisation problems, particularly combinatorial optimisation problems, are difficult to solve due to their complexity and hardness. Such problems have been successfully solved by evolutionary and swarm intelligence algorithms, especially in binary format. However, the approximation may suffer due to the the issues in balance between exploration and exploitation activities (EvE), which remain as the major challenge in this context. Although the complementary usage of multiple operators is becoming more popular for managing EvE with adaptive operator selection schemes, a bespoke adaptive selection system is still an important topic in research. Reinforcement Learning (RL) has recently been proposed as a way to customise and shape up a highly effective adaptive selection system. However, it is still challenging to handle the problem in terms of scalability. This paper proposes and assesses a RL-based novel approach to help develop a generalised framework for gaining, processing, and uti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26368;&#20855;&#21306;&#20998;&#24615;&#21050;&#28608;&#29289;&#30340;&#20248;&#21270;&#32858;&#31867;&#26041;&#27861;&#65292;&#25104;&#21151;&#22320;&#35782;&#21035;&#20102;&#23567;&#40736;&#35270;&#32593;&#33180;&#12289;&#24658;&#27827;&#29492;&#35270;&#32593;&#33180;&#21644;&#29461;&#29492;V4&#35270;&#35273;&#21306;&#30340;&#21151;&#33021;&#32454;&#32990;&#31867;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.05342</link><description>&lt;p&gt;
&#26368;&#20855;&#21306;&#20998;&#24615;&#30340;&#21050;&#28608;&#29289;&#29992;&#20110;&#21151;&#33021;&#32454;&#32990;&#31867;&#22411;&#30340;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Most discriminative stimuli for functional cell type identification. (arXiv:2401.05342v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05342
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26368;&#20855;&#21306;&#20998;&#24615;&#21050;&#28608;&#29289;&#30340;&#20248;&#21270;&#32858;&#31867;&#26041;&#27861;&#65292;&#25104;&#21151;&#22320;&#35782;&#21035;&#20102;&#23567;&#40736;&#35270;&#32593;&#33180;&#12289;&#24658;&#27827;&#29492;&#35270;&#32593;&#33180;&#21644;&#29461;&#29492;V4&#35270;&#35273;&#21306;&#30340;&#21151;&#33021;&#32454;&#32990;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35782;&#21035;&#32454;&#32990;&#31867;&#22411;&#24182;&#29702;&#35299;&#20854;&#21151;&#33021;&#29305;&#24615;&#23545;&#25581;&#31034;&#24863;&#30693;&#21644;&#35748;&#30693;&#26426;&#21046;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#35270;&#32593;&#33180;&#20013;&#65292;&#21487;&#20197;&#36890;&#36807;&#31934;&#24515;&#36873;&#25321;&#30340;&#21050;&#28608;&#29289;&#26469;&#35782;&#21035;&#21151;&#33021;&#31867;&#22411;&#65292;&#20294;&#36825;&#38656;&#35201;&#19987;&#19994;&#39046;&#22495;&#30693;&#35782;&#65292;&#24182;&#20250;&#23545;&#20197;&#21069;&#24050;&#30693;&#30340;&#32454;&#32990;&#31867;&#22411;&#20135;&#29983;&#20559;&#35265;&#12290;&#22312;&#35270;&#35273;&#30382;&#23618;&#20013;&#65292;&#20173;&#28982;&#19981;&#30693;&#36947;&#23384;&#22312;&#20160;&#20040;&#21151;&#33021;&#31867;&#22411;&#20197;&#21450;&#22914;&#20309;&#35782;&#21035;&#23427;&#20204;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#26032;&#30340;&#26041;&#27861;&#26469;&#23545;&#35270;&#32593;&#33180;&#21644;&#35270;&#35273;&#30382;&#23618;&#20013;&#30340;&#21151;&#33021;&#32454;&#32990;&#31867;&#22411;&#36827;&#34892;&#26080;&#20559;&#35265;&#30340;&#35782;&#21035;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20248;&#21270;&#30340;&#32858;&#31867;&#26041;&#27861;&#65292;&#20351;&#29992;&#26368;&#20855;&#21306;&#20998;&#24615;&#30340;&#21050;&#28608;&#29289;&#65288;MDS&#65289;&#26469;&#33719;&#24471;&#31070;&#32463;&#20803;&#30340;&#21151;&#33021;&#32858;&#31867;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#21050;&#28608;&#29289;&#30340;&#20248;&#21270;&#21644;&#32858;&#31867;&#37325;&#26032;&#20998;&#37197;&#20043;&#38388;&#30340;&#20132;&#26367;&#36827;&#34892;&#65292;&#31867;&#20284;&#20110;&#26399;&#26395;&#26368;&#22823;&#21270;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#25104;&#21151;&#24674;&#22797;&#20102;&#23567;&#40736;&#35270;&#32593;&#33180;&#12289;&#24658;&#27827;&#29492;&#35270;&#32593;&#33180;&#21644;&#29461;&#29492;V4&#35270;&#35273;&#21306;&#30340;&#21151;&#33021;&#32858;&#31867;&#12290;&#36825;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25104;&#21151;&#22320;&#36827;&#34892;&#21151;&#33021;&#32454;&#32990;&#31867;&#22411;&#30340;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identifying cell types and understanding their functional properties is crucial for unraveling the mechanisms underlying perception and cognition. In the retina, functional types can be identified by carefully selected stimuli, but this requires expert domain knowledge and biases the procedure towards previously known cell types. In the visual cortex, it is still unknown what functional types exist and how to identify them. Thus, for unbiased identification of the functional cell types in retina and visual cortex, new approaches are needed. Here we propose an optimization-based clustering approach using deep predictive models to obtain functional clusters of neurons using Most Discriminative Stimuli (MDS). Our approach alternates between stimulus optimization with cluster reassignment akin to an expectation-maximization algorithm. The algorithm recovers functional clusters in mouse retina, marmoset retina and macaque visual area V4. This demonstrates that our approach can successfully 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#25239;&#20307;&#35774;&#35745;&#39046;&#22495;&#29420;&#29305;&#25361;&#25112;&#30340;&#21019;&#26032;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#32447;&#25110;&#31163;&#32447;&#23398;&#20064;&#21487;&#20197;&#35774;&#35745;&#20986;&#38024;&#23545;&#22810;&#20010;&#38774;&#26631;&#30340;&#39640;&#20146;&#21644;&#21147;&#25239;&#20307;&#65292;&#19988;&#22312;Absolut!&#25968;&#25454;&#24211;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.05341</link><description>&lt;p&gt;
&#31283;&#23450;&#30340;&#22312;&#32447;&#21644;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#25239;&#20307;CDRH3&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Stable Online and Offline Reinforcement Learning for Antibody CDRH3 Design. (arXiv:2401.05341v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05341
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#25239;&#20307;&#35774;&#35745;&#39046;&#22495;&#29420;&#29305;&#25361;&#25112;&#30340;&#21019;&#26032;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#32447;&#25110;&#31163;&#32447;&#23398;&#20064;&#21487;&#20197;&#35774;&#35745;&#20986;&#38024;&#23545;&#22810;&#20010;&#38774;&#26631;&#30340;&#39640;&#20146;&#21644;&#21147;&#25239;&#20307;&#65292;&#19988;&#22312;Absolut!&#25968;&#25454;&#24211;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#25239;&#20307;&#31867;&#27835;&#30103;&#39046;&#22495;&#24471;&#21040;&#20102;&#26174;&#33879;&#30340;&#21457;&#23637;&#65292;&#26377;&#38024;&#23545;&#24615;&#30340;&#25239;&#20307;&#34987;&#35748;&#20026;&#26159;&#20010;&#24615;&#21270;&#27835;&#30103;&#30340;&#19968;&#31181;&#28508;&#22312;&#26377;&#25928;&#26041;&#27861;&#12290;&#23545;&#20110;&#22797;&#26434;&#12289;&#39640;&#24230;&#20010;&#20307;&#21270;&#30340;&#30142;&#30149;&#22914;&#30284;&#30151;&#65292;&#36825;&#26679;&#30340;&#27835;&#30103;&#21487;&#33021;&#29305;&#21035;&#26377;&#30410;&#12290;&#28982;&#32780;&#65292;&#25239;&#20307;&#35774;&#35745;&#30340;&#22522;&#30784;&#27688;&#22522;&#37240;&#24207;&#21015;&#30340;&#24191;&#27867;&#25628;&#32034;&#31354;&#38388;&#24120;&#24120;&#38480;&#21046;&#20102;&#35813;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#19987;&#38376;&#35299;&#20915;&#36825;&#19968;&#39046;&#22495;&#30340;&#29420;&#29305;&#25361;&#25112;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#35745;&#31639;&#26426;&#27169;&#25311;&#20013;&#23398;&#20064;&#35774;&#35745;&#38024;&#23545;&#22810;&#20010;&#38774;&#26631;&#30340;&#39640;&#20146;&#21644;&#21147;&#25239;&#20307;&#65292;&#26080;&#35770;&#26159;&#36890;&#36807;&#22312;&#32447;&#20132;&#20114;&#36824;&#26159;&#31163;&#32447;&#25968;&#25454;&#38598;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#39318;&#20010;&#22312;Absolut!&#25968;&#25454;&#24211;&#20013;&#23545;&#25152;&#26377;&#27979;&#35797;&#25239;&#21407;&#34920;&#29616;&#20248;&#36234;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of antibody-based therapeutics has grown significantly in recent years, with targeted antibodies emerging as a potentially effective approach to personalized therapies. Such therapies could be particularly beneficial for complex, highly individual diseases such as cancer. However, progress in this field is often constrained by the extensive search space of amino acid sequences that form the foundation of antibody design. In this study, we introduce a novel reinforcement learning method specifically tailored to address the unique challenges of this domain. We demonstrate that our method can learn the design of high-affinity antibodies against multiple targets in silico, utilizing either online interaction or offline datasets. To the best of our knowledge, our approach is the first of its kind and outperforms existing methods on all tested antigens in the Absolut! database.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;STR-Cert&#65292;&#26159;&#19968;&#31181;&#38024;&#23545;&#40065;&#26834;&#24615;&#35748;&#35777;&#30340;&#28145;&#24230;&#25991;&#26412;&#35782;&#21035;&#26041;&#27861;&#12290;&#36890;&#36807;&#25193;&#23637;DeepPoly&#22810;&#38754;&#20307;&#39564;&#35777;&#26694;&#26550;&#65292;&#25105;&#20204;&#38024;&#23545;&#22330;&#26223;&#25991;&#26412;&#35782;&#21035;&#27169;&#22411;&#30340;&#21508;&#20010;&#32452;&#20214;&#25552;&#20986;&#20102;&#26032;&#30340;&#22810;&#38754;&#20307;&#36793;&#30028;&#21644;&#31639;&#27861;&#12290;&#36890;&#36807;&#22312;&#20845;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35748;&#35777;&#21644;&#27604;&#36739;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.05338</link><description>&lt;p&gt;
STR-Cert: &#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#27969;&#27700;&#32447;&#21644;&#35270;&#35273;Transformer&#30340;&#28145;&#24230;&#25991;&#26412;&#35782;&#21035;&#30340;&#40065;&#26834;&#24615;&#35748;&#35777;
&lt;/p&gt;
&lt;p&gt;
STR-Cert: Robustness Certification for Deep Text Recognition on Deep Learning Pipelines and Vision Transformers. (arXiv:2401.05338v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05338
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;STR-Cert&#65292;&#26159;&#19968;&#31181;&#38024;&#23545;&#40065;&#26834;&#24615;&#35748;&#35777;&#30340;&#28145;&#24230;&#25991;&#26412;&#35782;&#21035;&#26041;&#27861;&#12290;&#36890;&#36807;&#25193;&#23637;DeepPoly&#22810;&#38754;&#20307;&#39564;&#35777;&#26694;&#26550;&#65292;&#25105;&#20204;&#38024;&#23545;&#22330;&#26223;&#25991;&#26412;&#35782;&#21035;&#27169;&#22411;&#30340;&#21508;&#20010;&#32452;&#20214;&#25552;&#20986;&#20102;&#26032;&#30340;&#22810;&#38754;&#20307;&#36793;&#30028;&#21644;&#31639;&#27861;&#12290;&#36890;&#36807;&#22312;&#20845;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35748;&#35777;&#21644;&#27604;&#36739;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40065;&#26834;&#24615;&#35748;&#35777;&#26088;&#22312;&#27491;&#24335;&#35748;&#35777;&#31070;&#32463;&#32593;&#32476;&#23545;&#25239;&#24615;&#36755;&#20837;&#30340;&#39044;&#27979;&#65292;&#24050;&#25104;&#20026;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#30340;&#37325;&#35201;&#24037;&#20855;&#20043;&#19968;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#36827;&#23637;&#65292;&#29616;&#26377;&#30340;&#35748;&#35777;&#26041;&#27861;&#20165;&#38480;&#20110;&#22522;&#26412;&#26550;&#26500;&#65288;&#22914;&#21367;&#31215;&#32593;&#32476;&#12289;&#24490;&#29615;&#32593;&#32476;&#21644;&#26368;&#36817;&#30340;Transformer&#65289;&#21644;&#22522;&#20934;&#25968;&#25454;&#38598;&#65288;&#22914;MNIST&#65289;&#12290;&#26412;&#25991;&#20851;&#27880;&#20110;&#22330;&#26223;&#25991;&#26412;&#35782;&#21035;&#65288;STR&#65289;&#30340;&#40065;&#26834;&#24615;&#35748;&#35777;&#65292;&#36825;&#26159;&#19968;&#31181;&#22797;&#26434;&#19988;&#24191;&#27867;&#37096;&#32626;&#30340;&#22522;&#20110;&#22270;&#20687;&#30340;&#24207;&#21015;&#39044;&#27979;&#38382;&#39064;&#12290;&#25105;&#20204;&#38024;&#23545;&#19977;&#31181;STR&#27169;&#22411;&#26550;&#26500;&#65292;&#21253;&#25324;&#26631;&#20934;STR&#27969;&#27700;&#32447;&#21644;&#35270;&#35273;Transformer&#12290;&#36890;&#36807;&#34893;&#29983;&#20986;&#26032;&#30340;&#22810;&#38754;&#20307;&#36793;&#30028;&#21644;&#20851;&#38190;STR&#27169;&#22411;&#32452;&#20214;&#30340;&#31639;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;STR-Cert&#65292;&#36825;&#26159;&#31532;&#19968;&#31181;&#29992;&#20110;STR&#27169;&#22411;&#30340;&#35748;&#35777;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#20845;&#20010;&#25968;&#25454;&#38598;&#19978;&#23545;STR&#27169;&#22411;&#36827;&#34892;&#35748;&#35777;&#21644;&#27604;&#36739;&#65292;&#23637;&#31034;&#20102;&#20854;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robustness certification, which aims to formally certify the predictions of neural networks against adversarial inputs, has become an integral part of important tool for safety-critical applications. Despite considerable progress, existing certification methods are limited to elementary architectures, such as convolutional networks, recurrent networks and recently Transformers, on benchmark datasets such as MNIST. In this paper, we focus on the robustness certification of scene text recognition (STR), which is a complex and extensively deployed image-based sequence prediction problem. We tackle three types of STR model architectures, including the standard STR pipelines and the Vision Transformer. We propose STR-Cert, the first certification method for STR models, by significantly extending the DeepPoly polyhedral verification framework via deriving novel polyhedral bounds and algorithms for key STR model components. Finally, we certify and compare STR models on six datasets, demonstra
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20248;&#21270;&#37327;&#21270;&#37329;&#34701;&#20013;&#30340;&#21033;&#28070;&#21644;&#25439;&#22833;&#12290;&#36890;&#36807;&#32447;&#24615;&#32452;&#21512;&#22806;&#29983;&#21464;&#37327;&#26500;&#24314;&#30340;&#20449;&#21495;&#65292;&#26368;&#22823;&#21270;&#22799;&#26222;&#27604;&#29575;&#12290;&#23454;&#35777;&#24212;&#29992;&#26174;&#31034;&#20102;&#35813;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#36827;&#19968;&#27493;&#21457;&#23637;&#30340;&#28508;&#22312;&#36884;&#24452;&#12290;</title><link>http://arxiv.org/abs/2401.05337</link><description>&lt;p&gt;
&#26368;&#20248;&#32447;&#24615;&#20449;&#21495;: &#19968;&#31181;&#29992;&#20110;&#20248;&#21270;&#32447;&#24615;&#20449;&#21495;PnL&#30340;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Optimal Linear Signal: An Unsupervised Machine Learning Framework to Optimize PnL with Linear Signals. (arXiv:2401.05337v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05337
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20248;&#21270;&#37327;&#21270;&#37329;&#34701;&#20013;&#30340;&#21033;&#28070;&#21644;&#25439;&#22833;&#12290;&#36890;&#36807;&#32447;&#24615;&#32452;&#21512;&#22806;&#29983;&#21464;&#37327;&#26500;&#24314;&#30340;&#20449;&#21495;&#65292;&#26368;&#22823;&#21270;&#22799;&#26222;&#27604;&#29575;&#12290;&#23454;&#35777;&#24212;&#29992;&#26174;&#31034;&#20102;&#35813;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#36827;&#19968;&#27493;&#21457;&#23637;&#30340;&#28508;&#22312;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20248;&#21270;&#37327;&#21270;&#37329;&#34701;&#20013;&#30340;&#21033;&#28070;&#21644;&#25439;&#22833;&#65288;PnL&#65289;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#31867;&#20284;&#20110;&#26080;&#30417;&#30563;&#32447;&#24615;&#22238;&#24402;&#65292;&#36890;&#36807;&#32447;&#24615;&#32452;&#21512;&#22806;&#29983;&#21464;&#37327;&#26500;&#24314;&#30340;&#20449;&#21495;&#65292;&#26368;&#22823;&#21270;PnL&#30340;&#22799;&#26222;&#27604;&#29575;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#22806;&#29983;&#21464;&#37327;&#19982;&#20132;&#26131;&#20449;&#21495;&#20043;&#38388;&#30340;&#32447;&#24615;&#20851;&#31995;&#65292;&#36890;&#36807;&#21442;&#25968;&#20248;&#21270;&#26469;&#26368;&#22823;&#21270;&#22799;&#26222;&#27604;&#29575;&#12290;&#22312;&#20195;&#34920;&#32654;&#22269;&#22269;&#20538;&#30340;ETF&#19978;&#30340;&#23454;&#35777;&#24212;&#29992;&#26174;&#31034;&#20102;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#36890;&#36807;&#27491;&#21017;&#21270;&#25216;&#26415;&#26469;&#20943;&#36731;&#36807;&#24230;&#25311;&#21512;&#12290;&#30740;&#31350;&#26368;&#21518;&#25552;&#20986;&#20102;&#36827;&#19968;&#27493;&#21457;&#23637;&#30340;&#28508;&#22312;&#36884;&#24452;&#65292;&#21253;&#25324;&#24191;&#20041;&#26102;&#38388;&#27493;&#38271;&#21644;&#22686;&#24378;&#30340;&#32416;&#27491;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study presents an unsupervised machine learning approach for optimizing Profit and Loss (PnL) in quantitative finance. Our algorithm, akin to an unsupervised variant of linear regression, maximizes the Sharpe Ratio of PnL generated from signals constructed linearly from exogenous variables. The methodology employs a linear relationship between exogenous variables and the trading signal, with the objective of maximizing the Sharpe Ratio through parameter optimization. Empirical application on an ETF representing U.S. Treasury bonds demonstrates the model's effectiveness, supported by regularization techniques to mitigate overfitting. The study concludes with potential avenues for further development, including generalized time steps and enhanced corrective terms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#28145;&#20837;&#20998;&#26512;&#24191;&#20041;&#31561;&#22686;&#36882;&#24402;&#20998;&#21106;&#31639;&#27861;&#65288;GIRP&#65289;&#65292;&#22312;&#21487;&#20998;&#31163;&#20984;&#25439;&#22833;&#21644;&#19981;&#21487;&#24494;&#25439;&#22833;&#30340;&#24773;&#20917;&#19979;&#65292;&#35299;&#20915;&#20102;&#31561;&#22686;&#22238;&#24402;&#38382;&#39064;&#30340;&#23384;&#22312;&#24615;&#21644;&#21807;&#19968;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#36882;&#24402;&#20108;&#20998;&#20998;&#21106;&#30340;&#26041;&#27861;&#26469;&#25214;&#21040;&#35299;&#12290;</title><link>http://arxiv.org/abs/2401.04847</link><description>&lt;p&gt;
&#20851;&#20110;&#24191;&#20041;&#31561;&#22686;&#36882;&#24402;&#20998;&#21106;&#31639;&#27861;&#30340;&#27491;&#30830;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Correctness of the Generalized Isotonic Recursive Partitioning Algorithm. (arXiv:2401.04847v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04847
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#28145;&#20837;&#20998;&#26512;&#24191;&#20041;&#31561;&#22686;&#36882;&#24402;&#20998;&#21106;&#31639;&#27861;&#65288;GIRP&#65289;&#65292;&#22312;&#21487;&#20998;&#31163;&#20984;&#25439;&#22833;&#21644;&#19981;&#21487;&#24494;&#25439;&#22833;&#30340;&#24773;&#20917;&#19979;&#65292;&#35299;&#20915;&#20102;&#31561;&#22686;&#22238;&#24402;&#38382;&#39064;&#30340;&#23384;&#22312;&#24615;&#21644;&#21807;&#19968;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#36882;&#24402;&#20108;&#20998;&#20998;&#21106;&#30340;&#26041;&#27861;&#26469;&#25214;&#21040;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28145;&#20837;&#20998;&#26512;&#20102;&#24191;&#20041;&#31561;&#22686;&#36882;&#24402;&#20998;&#21106;&#31639;&#27861;&#65288;GIRP&#65289;&#65292;&#35813;&#31639;&#27861;&#29992;&#20110;&#25311;&#21512;&#21487;&#20998;&#31163;&#20984;&#25439;&#22833;&#19979;&#30340;&#31561;&#22686;&#27169;&#22411;&#65292;&#35813;&#31639;&#27861;&#30001;Luss&#21644;Rosset&#25552;&#20986; [J. Comput. Graph. Statist., 23 (2014), pp. 192--201] &#24182;&#30001;Painsky&#21644;Rosset [IEEE Trans. Pattern Anal. Mach. Intell., 38 (2016), pp. 308-321] &#25193;&#23637;&#36866;&#29992;&#20110;&#19981;&#21487;&#24494;&#25439;&#22833;&#12290;GIRP&#31639;&#27861;&#20855;&#26377;&#21560;&#24341;&#20154;&#30340;&#29305;&#28857;&#65292;&#21363;&#22312;&#31639;&#27861;&#30340;&#27599;&#19968;&#27493;&#20013;&#65292;&#20013;&#38388;&#35299;&#28385;&#36275;&#31561;&#22686;&#32422;&#26463;&#12290;&#25991;&#31456;&#20197;&#19968;&#20010;&#20363;&#23376;&#24320;&#22987;&#65292;&#23637;&#31034;&#20102;&#25991;&#29486;&#20013;&#25551;&#36848;&#30340;GIRP&#31639;&#27861;&#21487;&#33021;&#26080;&#27861;&#20135;&#29983;&#31561;&#22686;&#27169;&#22411;&#30340;&#24773;&#20917;&#65292;&#34920;&#26126;&#24517;&#39035;&#20180;&#32454;&#35752;&#35770;&#31561;&#22686;&#22238;&#24402;&#38382;&#39064;&#30340;&#35299;&#30340;&#23384;&#22312;&#24615;&#21644;&#21807;&#19968;&#24615;&#12290;&#25991;&#31456;&#25509;&#30528;&#23637;&#31034;&#65292;&#21487;&#33021;&#23384;&#22312;&#35768;&#22810;&#35299;&#20043;&#19968;&#65292;&#21487;&#20197;&#36890;&#36807;&#23545;&#35266;&#23519;&#25968;&#25454;&#38598;&#36827;&#34892;&#36882;&#24402;&#20108;&#20998;&#20998;&#21106;&#26469;&#25214;&#21040;&#35299;&#12290;&#19968;&#20010;&#23567;&#30340;&#20462;&#25913;
&lt;/p&gt;
&lt;p&gt;
This paper presents an in-depth analysis of the generalized isotonic recursive partitioning (GIRP) algorithm for fitting isotonic models under separable convex losses, proposed by Luss and Rosset [J. Comput. Graph. Statist., 23 (2014), pp. 192--201] for differentiable losses and extended by Painsky and Rosset [IEEE Trans. Pattern Anal. Mach. Intell., 38 (2016), pp. 308-321] for nondifferentiable losses. The GIRP algorithm poseses an attractive feature that in each step of the algorithm, the intermediate solution satisfies the isotonicity constraint. The paper begins with an example showing that the GIRP algorithm as described in the literature may fail to produce an isotonic model, suggesting that the existence and uniqueness of the solution to the isotonic regression problem must be carefully addressed. It proceeds with showing that, among possibly many solutions, there indeed exists a solution that can be found by recursive binary partitioning of the set of observed data. A small mod
&lt;/p&gt;</description></item><item><title>RoSA&#26159;&#19968;&#31181;&#26032;&#30340;PEFT&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#26435;&#37325;&#19978;&#35757;&#32451;&#20302;&#31209;&#21644;&#39640;&#24230;&#31232;&#30095;&#30340;&#32452;&#20214;&#65292;&#20197;&#39640;&#25928;&#36817;&#20284;&#23436;&#20840;&#24494;&#35843;&#30340;&#24615;&#33021;&#65292;&#26469;&#23454;&#29616;&#20934;&#30830;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#12290;&#22312;&#22810;&#20010;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;RoSA&#34920;&#29616;&#20986;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.04679</link><description>&lt;p&gt;
RoSA: &#36890;&#36807;&#40065;&#26834;&#36866;&#24212;&#23454;&#29616;&#20934;&#30830;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation. (arXiv:2401.04679v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04679
&lt;/p&gt;
&lt;p&gt;
RoSA&#26159;&#19968;&#31181;&#26032;&#30340;PEFT&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#26435;&#37325;&#19978;&#35757;&#32451;&#20302;&#31209;&#21644;&#39640;&#24230;&#31232;&#30095;&#30340;&#32452;&#20214;&#65292;&#20197;&#39640;&#25928;&#36817;&#20284;&#23436;&#20840;&#24494;&#35843;&#30340;&#24615;&#33021;&#65292;&#26469;&#23454;&#29616;&#20934;&#30830;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#12290;&#22312;&#22810;&#20010;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;RoSA&#34920;&#29616;&#20986;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#22823;&#35821;&#35328;&#27169;&#22411; (LLMs) &#30340;&#32972;&#26223;&#19979;&#65292;&#33021;&#22815;&#22312;&#26377;&#38480;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#39044;&#31639;&#19979;&#25552;&#20379;&#33391;&#22909;&#20934;&#30830;&#24615;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843; (PEFT) &#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;PEFT&#26041;&#27861;&#65292;&#31216;&#20026;RoSA&#65292;&#21463;&#40065;&#26834;&#20027;&#25104;&#20998;&#20998;&#26512; (PCA) &#30340;&#21551;&#21457;&#65292;&#23427;&#22312;&#19968;&#32452;&#22266;&#23450;&#30340;&#39044;&#35757;&#32451;&#26435;&#37325;&#19978;&#20849;&#21516;&#35757;&#32451;$\textit{&#20302;&#31209;}$&#21644;$\textit{&#39640;&#24230;&#31232;&#30095;}$&#30340;&#32452;&#20214;&#65292;&#20197;&#39640;&#25928;&#36817;&#20284;&#23436;&#20840;&#24494;&#35843;&#65288;FFT&#65289;&#35299;&#20915;&#26041;&#26696;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;RoSA&#22312;&#19968;&#31995;&#21015;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#20363;&#22914;&#23567;&#23398;&#25968;&#23398;&#21644;SQL&#26597;&#35810;&#29983;&#25104;&#65292;&#36825;&#20123;&#20219;&#21153;&#38656;&#35201;&#36827;&#34892;&#24494;&#35843;&#20197;&#33719;&#24471;&#33391;&#22909;&#24615;&#33021;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#30456;&#21516;&#30340;&#21442;&#25968;&#39044;&#31639;&#19979;&#65292;RoSA&#20248;&#20110;LoRA&#21644;&#32431;&#31929;&#30340;&#31232;&#30095;&#24494;&#35843;&#12290;&#25105;&#20204;&#36890;&#36807;&#31232;&#30095;GPU&#20869;&#26680;&#20026;RoSA&#25552;&#20379;&#31995;&#32479;&#25903;&#25345;&#65292;&#20197;&#34917;&#20805;&#35757;&#32451;&#31639;&#27861;&#65292;&#20174;&#32780;&#23454;&#29616;&#20869;&#23384;&#21644;&#35745;&#31639;&#25928;&#29575;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#23558;&#22312;https://github.com/IST-DASLab&#19978;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate parameter-efficient fine-tuning (PEFT) methods that can provide good accuracy under limited computational and memory budgets in the context of large language models (LLMs). We present a new PEFT method called Robust Adaptation (RoSA) inspired by robust principal component analysis (PCA) that jointly trains $\textit{low-rank}$ and $\textit{highly-sparse}$ components on top of a set of fixed pretrained weights to efficiently approximate the performance of a full-fine-tuning (FFT) solution. Across a series of challenging generative tasks such as grade-school math and SQL query generation, which require fine-tuning for good performance, we show that RoSA outperforms both LoRA and pure sparse fine-tuning, at the same parameter budget. We provide system support for RoSA to complement the training algorithm, specifically in the form of sparse GPU kernels which enable memoryand computationally-efficient training. Our code will be made available at https://github.com/IST-DASLab
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#24494;&#23567;&#26102;&#38388;&#28151;&#21512;&#22120; (TTMs) &#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#38024;&#23545;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#30340;&#38646;/&#23569;&#26679;&#26412;&#39044;&#27979;&#36827;&#34892;&#20102;&#20248;&#21270;&#12290;&#19982;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30456;&#27604;&#65292;TTMs&#27169;&#22411;&#26356;&#23567;&#12289;&#26356;&#24555;&#65292;&#24182;&#32771;&#34385;&#20102;&#36328;&#36890;&#36947;&#30456;&#20851;&#24615;&#65292;&#33021;&#22815;&#22312;&#30701;&#26102;&#38388;&#20869;&#36827;&#34892;&#26377;&#25928;&#30340;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2401.03955</link><description>&lt;p&gt;
&#24494;&#23567;&#26102;&#38388;&#28151;&#21512;&#22120; (TTMs): &#38024;&#23545;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#30340;&#22686;&#24378;&#38646;/&#23569;&#26679;&#26412;&#39044;&#27979;&#30340;&#24555;&#36895;&#39044;&#35757;&#32451;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Tiny Time Mixers (TTMs): Fast Pretrained Models for Enhanced Zero/Few-Shot Forecasting of Multivariate Time Series. (arXiv:2401.03955v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03955
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#24494;&#23567;&#26102;&#38388;&#28151;&#21512;&#22120; (TTMs) &#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#38024;&#23545;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#30340;&#38646;/&#23569;&#26679;&#26412;&#39044;&#27979;&#36827;&#34892;&#20102;&#20248;&#21270;&#12290;&#19982;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30456;&#27604;&#65292;TTMs&#27169;&#22411;&#26356;&#23567;&#12289;&#26356;&#24555;&#65292;&#24182;&#32771;&#34385;&#20102;&#36328;&#36890;&#36947;&#30456;&#20851;&#24615;&#65292;&#33021;&#22815;&#22312;&#30701;&#26102;&#38388;&#20869;&#36827;&#34892;&#26377;&#25928;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;/&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#35821;&#35328;&#21644;&#35270;&#35273;&#39046;&#22495;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015; (TS) &#20013;&#38754;&#20020;&#30528;&#22810;&#26679;&#24615;&#21644;&#20844;&#24320;&#39044;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#30340;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#26368;&#36817;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#36827;&#34892;&#21508;&#31181;&#36866;&#24212;&#30340;&#36235;&#21183;&#36880;&#28176;&#22686;&#21152;&#12290;&#36825;&#20123;&#26041;&#27861;&#21033;&#29992;&#36328;&#39046;&#22495;&#36801;&#31227;&#23398;&#20064;&#65292;&#20986;&#22855;&#22320;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#38750;&#24120;&#32531;&#24930;&#19988;&#24222;&#22823;&#65288;&#22823;&#32422;&#21313;&#20159;&#20010;&#21442;&#25968;&#65289;&#65292;&#24182;&#19988;&#19981;&#32771;&#34385;&#36328;&#36890;&#36947;&#30456;&#20851;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#23618;&#24494;&#23567;&#26102;&#38388;&#28151;&#21512;&#22120; (TTM)&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#36731;&#37327;&#32423; TSMixer &#32467;&#26500;&#30340;&#26174;&#33879;&#23567;&#22411;&#27169;&#22411;&#12290;TTM &#26159;&#39318;&#20010;&#25104;&#21151;&#24320;&#21457;&#30340;&#24494;&#22411;&#36890;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;&#8804;100&#19975;&#20010;&#21442;&#25968;&#65289;&#65292;&#19987;&#38376;&#22312;&#20844;&#24320;TS&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24555;&#36895;&#35757;&#32451;&#65288;&#20165;&#38656;4-8&#23567;&#26102;&#65289;&#65292;&#20855;&#26377;&#26377;&#25928;&#30340;&#36801;&#31227;&#23398;&#20064;&#33021;&#21147;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Pretrained models for zero/few-shot learning excel in language and vision domains but encounter challenges in multivariate time series (TS) due to the diverse nature and scarcity of publicly available pretraining data. Consequently, there has been a recent surge in utilizing pretrained large language models (LLMs) with various adaptations for time series forecasting. These approaches employ cross-domain transfer learning and surprisingly yield impressive results. However, these models are typically very slow and large ($\sim$billion parameters) and do not consider cross-channel correlations. To address this, we present Multi-level Tiny Time Mixers (TTM), a significantly small model based on the lightweight TSMixer architecture. TTM marks the first success in developing tiny general-pretrained models ($\le$1 million parameters), exclusively trained on public TS datasets in a flash of just 4-8 hrs with effective transfer learning capabilities for forecasting. To tackle the complexi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38271;&#26399;&#23433;&#20840;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;LoBiSaRL&#65292;&#35813;&#31639;&#27861;&#38024;&#23545;&#20855;&#26377;&#20108;&#36827;&#21046;&#23433;&#20840;&#21453;&#39304;&#21644;&#26410;&#30693;&#38543;&#26426;&#29366;&#24577;&#36716;&#25442;&#20989;&#25968;&#30340;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;CMDPs&#65289;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#22870;&#21169;&#30340;&#26041;&#24335;&#20248;&#21270;&#31574;&#30053;&#65292;&#21516;&#26102;&#20197;&#39640;&#27010;&#29575;&#30830;&#20445;&#27599;&#20010;&#22238;&#21512;&#20013;&#20195;&#29702;&#21482;&#25191;&#34892;&#23433;&#20840;&#30340;&#29366;&#24577;-&#21160;&#20316;&#23545;&#12290;</title><link>http://arxiv.org/abs/2401.03786</link><description>&lt;p&gt;
&#38271;&#26399;&#23433;&#20840;&#30340;&#24378;&#21270;&#23398;&#20064;&#19982;&#20108;&#36827;&#21046;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
Long-term Safe Reinforcement Learning with Binary Feedback. (arXiv:2401.03786v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03786
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38271;&#26399;&#23433;&#20840;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;LoBiSaRL&#65292;&#35813;&#31639;&#27861;&#38024;&#23545;&#20855;&#26377;&#20108;&#36827;&#21046;&#23433;&#20840;&#21453;&#39304;&#21644;&#26410;&#30693;&#38543;&#26426;&#29366;&#24577;&#36716;&#25442;&#20989;&#25968;&#30340;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;CMDPs&#65289;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#22870;&#21169;&#30340;&#26041;&#24335;&#20248;&#21270;&#31574;&#30053;&#65292;&#21516;&#26102;&#20197;&#39640;&#27010;&#29575;&#30830;&#20445;&#27599;&#20010;&#22238;&#21512;&#20013;&#20195;&#29702;&#21482;&#25191;&#34892;&#23433;&#20840;&#30340;&#29366;&#24577;-&#21160;&#20316;&#23545;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#26159;&#23558;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#20110;&#23454;&#38469;&#38382;&#39064;&#30340;&#19981;&#21487;&#25110;&#32570;&#30340;&#35201;&#27714;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;&#25552;&#20986;&#20102;&#22823;&#37327;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#24037;&#20316;&#36890;&#24120;1&#65289;&#20381;&#36182;&#20110;&#25509;&#25910;&#25968;&#20540;&#23433;&#20840;&#21453;&#39304;&#65307;2&#65289;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#26080;&#27861;&#20445;&#35777;&#23433;&#20840;&#24615;&#65307;3&#65289;&#23558;&#38382;&#39064;&#38480;&#21046;&#22312;&#20808;&#39564;&#24050;&#30693;&#30340;&#30830;&#23450;&#24615;&#36716;&#25442;&#21160;&#24577;&#65307;&#20197;&#21450;/&#25110;&#32773;4&#65289;&#20551;&#35774;&#23384;&#22312;&#19968;&#20010;&#24050;&#30693;&#30340;&#23433;&#20840;&#31574;&#30053;&#20197;&#22788;&#29702;&#20219;&#20309;&#29366;&#24577;&#12290;&#38024;&#23545;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38271;&#26399;&#20108;&#36827;&#21046;&#21453;&#39304;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#65288;LoBiSaRL&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;&#20855;&#26377;&#20108;&#36827;&#21046;&#23433;&#20840;&#21453;&#39304;&#21644;&#26410;&#30693;&#38543;&#26426;&#29366;&#24577;&#36716;&#25442;&#20989;&#25968;&#30340;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;CMDPs&#65289;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;LoBiSaRL&#20248;&#21270;&#19968;&#20010;&#31574;&#30053;&#20197;&#20351;&#22870;&#21169;&#26368;&#22823;&#21270;&#65292;&#21516;&#26102;&#20445;&#35777;&#22312;&#27599;&#20010;&#22238;&#21512;&#20013;&#20195;&#29702;&#20165;&#20197;&#39640;&#27010;&#29575;&#25191;&#34892;&#23433;&#20840;&#30340;&#29366;&#24577;-&#21160;&#20316;&#23545;&#65292;&#20174;&#32780;&#30830;&#20445;&#38271;&#26399;&#23433;&#20840;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;LoBiSaRL&#36890;&#36807;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#65288;GLM&#65289;&#26469;&#24314;&#27169;&#20108;&#36827;&#21046;&#23433;&#20840;&#20989;&#25968;&#65292;&#24182;&#20445;&#35777;&#38271;&#26399;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Safety is an indispensable requirement for applying reinforcement learning (RL) to real problems. Although there has been a surge of safe RL algorithms proposed in recent years, most existing work typically 1) relies on receiving numeric safety feedback; 2) does not guarantee safety during the learning process; 3) limits the problem to a priori known, deterministic transition dynamics; and/or 4) assume the existence of a known safe policy for any states. Addressing the issues mentioned above, we thus propose Long-term Binaryfeedback Safe RL (LoBiSaRL), a safe RL algorithm for constrained Markov decision processes (CMDPs) with binary safety feedback and an unknown, stochastic state transition function. LoBiSaRL optimizes a policy to maximize rewards while guaranteeing a long-term safety that an agent executes only safe state-action pairs throughout each episode with high probability. Specifically, LoBiSaRL models the binary safety function via a generalized linear model (GLM) and conser
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#22238;&#39038;&#20102;&#22312;&#21019;&#20260;&#24615;&#33041;&#25439;&#20260;&#65288;TBI&#65289;&#20013;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#26032;&#25216;&#26415;&#65292;&#29305;&#21035;&#20851;&#27880;&#36731;&#24230;&#33041;&#25439;&#20260;&#65288;mTBI&#65289;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#24456;&#22810;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#29992;&#20110;&#35786;&#26029;&#65292;&#20294;&#23545;&#39044;&#27979;&#39044;&#21518;&#30340;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#12290;</title><link>http://arxiv.org/abs/2401.03621</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#21019;&#20260;&#24615;&#33041;&#25439;&#20260;&#20013;&#30340;&#24212;&#29992;&#65306;&#20851;&#27880;&#36731;&#24230;&#33041;&#25439;&#20260;
&lt;/p&gt;
&lt;p&gt;
Machine Learning Applications in Traumatic Brain Injury: A Spotlight on Mild TBI. (arXiv:2401.03621v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03621
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#22238;&#39038;&#20102;&#22312;&#21019;&#20260;&#24615;&#33041;&#25439;&#20260;&#65288;TBI&#65289;&#20013;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#26032;&#25216;&#26415;&#65292;&#29305;&#21035;&#20851;&#27880;&#36731;&#24230;&#33041;&#25439;&#20260;&#65288;mTBI&#65289;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#24456;&#22810;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#29992;&#20110;&#35786;&#26029;&#65292;&#20294;&#23545;&#39044;&#27979;&#39044;&#21518;&#30340;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21019;&#20260;&#24615;&#33041;&#25439;&#20260;&#65288;TBI&#65289;&#26159;&#20840;&#29699;&#20844;&#20849;&#21355;&#29983;&#39046;&#22495;&#38754;&#20020;&#30340;&#37325;&#22823;&#25361;&#25112;&#65292;&#23548;&#33268;&#39640;&#21457;&#30149;&#29575;&#21644;&#27515;&#20129;&#29575;&#65292;&#24182;&#32473;&#20840;&#29699;&#21307;&#30103;&#31995;&#32479;&#24102;&#26469;&#24040;&#22823;&#32463;&#27982;&#36127;&#25285;&#12290;TBI&#30340;&#35786;&#26029;&#20381;&#36182;&#20110;&#20020;&#24202;&#20449;&#24687;&#21644;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#12290;&#20026;&#20102;&#24212;&#23545;TBI&#25152;&#24102;&#26469;&#30340;&#22810;&#26041;&#38754;&#25361;&#25112;&#65292;&#20986;&#29616;&#20102;&#21019;&#26032;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#12290;&#23588;&#20854;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#36731;&#24230;&#33041;&#25439;&#20260;&#65288;mTBI&#65289;&#30340;&#26222;&#36941;&#23384;&#22312;&#65292;&#21344;&#22810;&#25968;TBI&#30149;&#20363;&#65292;&#20256;&#32479;&#26041;&#27861;&#24448;&#24448;&#38590;&#20197;&#32988;&#20219;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#37325;&#28857;&#22238;&#39038;&#20102;&#22312;TBI&#20013;&#24212;&#29992;&#20110;&#20020;&#24202;&#20449;&#24687;&#21644;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#30340;&#26368;&#26032;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#25216;&#26415;&#65292;&#29305;&#21035;&#20851;&#27880;mTBI&#12290;&#25105;&#20204;&#26681;&#25454;&#25968;&#25454;&#26469;&#28304;&#23545;ML&#24212;&#29992;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#21487;&#20197;&#30475;&#21040;&#36804;&#20170;&#20026;&#27490;&#20351;&#29992;&#20102;&#21508;&#31181;ML&#25216;&#26415;&#12290;&#36825;&#20123;&#25216;&#26415;&#22823;&#22810;&#20027;&#35201;&#20851;&#27880;&#35786;&#26029;&#65292;&#32780;&#23545;&#39044;&#27979;&#39044;&#21518;&#30340;&#21162;&#21147;&#30456;&#23545;&#36739;&#23569;&#12290;&#36825;&#20010;&#22238;&#39038;&#20250;&#23545;&#24403;&#21069;&#30340;&#30740;&#31350;&#36827;&#34892;&#27010;&#36848;&#65292;&#24182;&#25506;&#32034;&#26410;&#26469;&#21457;&#23637;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traumatic Brain Injury (TBI) poses a significant global public health challenge, contributing to high morbidity and mortality rates and placing a substantial economic burden on healthcare systems worldwide. The diagnosis of TBI relies on clinical information along with Computed Tomography (CT) scans. Addressing the multifaceted challenges posed by TBI has seen the development of innovative, data-driven approaches, for this complex condition. Particularly noteworthy is the prevalence of mild TBI (mTBI), which constitutes the majority of TBI cases where conventional methods often fall short. As such, we review the state-of-the-art Machine Learning (ML) techniques applied to clinical information and CT scans in TBI, with a particular focus on mTBI. We categorize ML applications based on their data sources, and there is a spectrum of ML techniques used to date. Most of these techniques have primarily focused on diagnosis, with relatively few attempts at predicting the prognosis. This revie
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#20917;&#19979;&#26816;&#27979;&#21644;&#20998;&#31867;&#33041;&#32959;&#30244;&#65292;&#24182;&#35299;&#20915;&#20102;&#22312;&#32597;&#35265;&#24773;&#20917;&#19979;&#30340;&#32959;&#30244;&#26816;&#27979;&#38382;&#39064;&#12290;&#30740;&#31350;&#20351;&#29992;&#20102;&#26469;&#33258;&#22269;&#23478;&#33041;&#26144;&#23556;&#23454;&#39564;&#23460;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#20462;&#25913;&#26679;&#26412;&#25968;&#37327;&#21644;&#24739;&#32773;&#20998;&#24067;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#24212;&#23545;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#30340;&#24322;&#24120;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2401.03302</link><description>&lt;p&gt;
&#34892;&#21160;&#20013;&#30340;&#29616;&#23454;&#20027;&#20041;&#65306;&#20351;&#29992;YOLOv8&#21644;DeiT&#20174;&#21307;&#23398;&#22270;&#20687;&#20013;&#35786;&#26029;&#33041;&#32959;&#30244;&#30340;&#24322;&#24120;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Realism in Action: Anomaly-Aware Diagnosis of Brain Tumors from Medical Images Using YOLOv8 and DeiT. (arXiv:2401.03302v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03302
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#20917;&#19979;&#26816;&#27979;&#21644;&#20998;&#31867;&#33041;&#32959;&#30244;&#65292;&#24182;&#35299;&#20915;&#20102;&#22312;&#32597;&#35265;&#24773;&#20917;&#19979;&#30340;&#32959;&#30244;&#26816;&#27979;&#38382;&#39064;&#12290;&#30740;&#31350;&#20351;&#29992;&#20102;&#26469;&#33258;&#22269;&#23478;&#33041;&#26144;&#23556;&#23454;&#39564;&#23460;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#20462;&#25913;&#26679;&#26412;&#25968;&#37327;&#21644;&#24739;&#32773;&#20998;&#24067;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#24212;&#23545;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#30340;&#24322;&#24120;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#23398;&#31185;&#23398;&#39046;&#22495;&#65292;&#30001;&#20110;&#33041;&#32959;&#30244;&#22312;&#24739;&#32773;&#20013;&#30340;&#32597;&#35265;&#31243;&#24230;&#65292;&#21487;&#38752;&#22320;&#26816;&#27979;&#21644;&#20998;&#31867;&#33041;&#32959;&#30244;&#20173;&#28982;&#26159;&#19968;&#20010;&#33392;&#24040;&#30340;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#22312;&#24322;&#24120;&#24773;&#20917;&#19979;&#26816;&#27979;&#32959;&#30244;&#30340;&#33021;&#21147;&#23545;&#20110;&#30830;&#20445;&#21450;&#26102;&#24178;&#39044;&#21644;&#25913;&#21892;&#24739;&#32773;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#20917;&#19979;&#26816;&#27979;&#21644;&#20998;&#31867;&#33041;&#32959;&#30244;&#12290;&#26469;&#33258;&#22269;&#23478;&#33041;&#26144;&#23556;&#23454;&#39564;&#23460;&#65288;NBML&#65289;&#30340;&#31934;&#36873;&#25968;&#25454;&#38598;&#21253;&#25324;81&#21517;&#24739;&#32773;&#65292;&#20854;&#20013;&#21253;&#25324;30&#20363;&#32959;&#30244;&#30149;&#20363;&#21644;51&#20363;&#27491;&#24120;&#30149;&#20363;&#12290;&#26816;&#27979;&#21644;&#20998;&#31867;&#27969;&#31243;&#34987;&#20998;&#20026;&#20004;&#20010;&#36830;&#32493;&#30340;&#20219;&#21153;&#12290;&#26816;&#27979;&#38454;&#27573;&#21253;&#25324;&#20840;&#38754;&#30340;&#25968;&#25454;&#20998;&#26512;&#21644;&#39044;&#22788;&#29702;&#65292;&#20197;&#20462;&#25913;&#22270;&#20687;&#26679;&#26412;&#21644;&#27599;&#20010;&#31867;&#21035;&#30340;&#24739;&#32773;&#25968;&#37327;&#65292;&#20197;&#31526;&#21512;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#30340;&#24322;&#24120;&#20998;&#24067;&#65288;9&#20010;&#27491;&#24120;&#26679;&#26412;&#23545;&#24212;1&#20010;&#32959;&#30244;&#26679;&#26412;&#65289;&#12290;&#27492;&#22806;&#65292;&#22312;&#27979;&#35797;&#20013;&#38500;&#20102;&#24120;&#35265;&#30340;&#35780;&#20272;&#25351;&#26631;&#22806;&#65292;&#25105;&#20204;&#36824;&#37319;&#29992;&#20102;... [&#25688;&#35201;&#38271;&#24230;&#24050;&#36798;&#21040;&#19978;&#38480;]
&lt;/p&gt;
&lt;p&gt;
In the field of medical sciences, reliable detection and classification of brain tumors from images remains a formidable challenge due to the rarity of tumors within the population of patients. Therefore, the ability to detect tumors in anomaly scenarios is paramount for ensuring timely interventions and improved patient outcomes. This study addresses the issue by leveraging deep learning (DL) techniques to detect and classify brain tumors in challenging situations. The curated data set from the National Brain Mapping Lab (NBML) comprises 81 patients, including 30 Tumor cases and 51 Normal cases. The detection and classification pipelines are separated into two consecutive tasks. The detection phase involved comprehensive data analysis and pre-processing to modify the number of image samples and the number of patients of each class to anomaly distribution (9 Normal per 1 Tumor) to comply with real world scenarios. Next, in addition to common evaluation metrics for the testing, we emplo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#27963;&#20307;&#32454;&#32990;&#26174;&#24494;&#38236;&#25429;&#25417;&#21040;&#30340;&#20116;&#32500;&#35270;&#39057;&#20013;&#23547;&#25214;&#32454;&#32990;&#20449;&#21495;&#21160;&#21147;&#23398;&#26102;&#31354;&#27169;&#24335;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#20219;&#20309;&#20808;&#39564;&#30340;&#39044;&#26399;&#27169;&#24335;&#21160;&#21147;&#23398;&#21644;&#35757;&#32451;&#25968;&#25454;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#32454;&#32990;&#20449;&#21495;&#32467;&#26500;&#20989;&#25968;&#65288;SSF&#65289;&#65292;&#36890;&#36807;&#27979;&#37327;&#32454;&#32990;&#20449;&#21495;&#29366;&#24577;&#21644;&#21608;&#22260;&#32454;&#32990;&#36136;&#20043;&#38388;&#30340;&#26680;&#31958;&#20307;&#24378;&#24230;&#65292;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26680;&#31958;&#20307;&#19982;&#32454;&#32990;&#26680;&#27604;&#20540;&#30456;&#27604;&#26377;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;&#36890;&#36807;&#24402;&#19968;&#21270;&#21387;&#32553;&#36317;&#31163;&#65288;NCD&#65289;&#26469;&#35782;&#21035;&#30456;&#20284;&#30340;&#27169;&#24335;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#23558;&#36755;&#20837;&#30340;SSF&#26500;&#22270;&#34920;&#31034;&#20026;&#20302;&#32500;&#23884;&#20837;&#20013;&#30340;&#28857;&#65292;&#26368;&#20248;&#22320;&#25429;&#25417;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2401.02501</link><description>&lt;p&gt;
&#32454;&#32990;&#20449;&#21495;&#20256;&#23548;&#32467;&#26500;&#21644;&#21151;&#33021;
&lt;/p&gt;
&lt;p&gt;
The cell signaling structure function. (arXiv:2401.02501v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02501
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#27963;&#20307;&#32454;&#32990;&#26174;&#24494;&#38236;&#25429;&#25417;&#21040;&#30340;&#20116;&#32500;&#35270;&#39057;&#20013;&#23547;&#25214;&#32454;&#32990;&#20449;&#21495;&#21160;&#21147;&#23398;&#26102;&#31354;&#27169;&#24335;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#20219;&#20309;&#20808;&#39564;&#30340;&#39044;&#26399;&#27169;&#24335;&#21160;&#21147;&#23398;&#21644;&#35757;&#32451;&#25968;&#25454;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#32454;&#32990;&#20449;&#21495;&#32467;&#26500;&#20989;&#25968;&#65288;SSF&#65289;&#65292;&#36890;&#36807;&#27979;&#37327;&#32454;&#32990;&#20449;&#21495;&#29366;&#24577;&#21644;&#21608;&#22260;&#32454;&#32990;&#36136;&#20043;&#38388;&#30340;&#26680;&#31958;&#20307;&#24378;&#24230;&#65292;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26680;&#31958;&#20307;&#19982;&#32454;&#32990;&#26680;&#27604;&#20540;&#30456;&#27604;&#26377;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;&#36890;&#36807;&#24402;&#19968;&#21270;&#21387;&#32553;&#36317;&#31163;&#65288;NCD&#65289;&#26469;&#35782;&#21035;&#30456;&#20284;&#30340;&#27169;&#24335;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#23558;&#36755;&#20837;&#30340;SSF&#26500;&#22270;&#34920;&#31034;&#20026;&#20302;&#32500;&#23884;&#20837;&#20013;&#30340;&#28857;&#65292;&#26368;&#20248;&#22320;&#25429;&#25417;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27963;&#20307;&#32454;&#32990;&#26174;&#24494;&#38236;&#25429;&#25417;&#21040;&#30340;&#20116;&#32500;$(x,y,z,channel,time)$&#35270;&#39057;&#26174;&#31034;&#20102;&#32454;&#32990;&#36816;&#21160;&#21644;&#20449;&#21495;&#21160;&#21147;&#23398;&#30340;&#27169;&#24335;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#25552;&#20986;&#19968;&#31181;&#22312;&#20116;&#32500;&#27963;&#20307;&#32454;&#32990;&#26174;&#24494;&#38236;&#35270;&#39057;&#20013;&#23547;&#25214;&#32454;&#32990;&#20449;&#21495;&#21160;&#21147;&#23398;&#26102;&#31354;&#27169;&#24335;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#29420;&#29305;&#20043;&#22788;&#22312;&#20110;&#19981;&#38656;&#35201;&#39044;&#20808;&#20102;&#35299;&#39044;&#26399;&#30340;&#27169;&#24335;&#21160;&#21147;&#23398;&#20197;&#21450;&#27809;&#26377;&#35757;&#32451;&#25968;&#25454;&#12290;&#25152;&#25552;&#20986;&#30340;&#32454;&#32990;&#20449;&#21495;&#32467;&#26500;&#20989;&#25968;&#65288;SSF&#65289;&#26159;&#19968;&#31181;Kolmogorov&#32467;&#26500;&#20989;&#25968;&#65292;&#21487;&#20197;&#36890;&#36807;&#26680;&#24515;&#21306;&#22495;&#30456;&#23545;&#20110;&#21608;&#22260;&#32454;&#32990;&#36136;&#30340;&#26680;&#31958;&#20307;&#24378;&#24230;&#26469;&#26368;&#20248;&#22320;&#27979;&#37327;&#32454;&#32990;&#20449;&#21495;&#29366;&#24577;&#65292;&#30456;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26680;&#31958;&#20307;&#19982;&#32454;&#32990;&#26680;&#27604;&#20540;&#26377;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#36890;&#36807;&#24230;&#37327;&#24402;&#19968;&#21270;&#21387;&#32553;&#36317;&#31163;&#65288;NCD&#65289;&#26469;&#35782;&#21035;&#30456;&#20284;&#30340;&#27169;&#24335;&#12290;NCD&#26159;&#19968;&#20010;&#29992;&#20110;&#34920;&#31034;&#36755;&#20837;&#30340;SSF&#26500;&#22270;&#22312;&#20302;&#32500;&#23884;&#20837;&#20013;&#20316;&#20026;&#28857;&#30340;Hilbert&#31354;&#38388;&#30340;&#20877;&#29983;&#26680;&#65292;&#21487;&#20197;&#26368;&#20248;&#22320;&#25429;&#25417;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Live cell microscopy captures 5-D $(x,y,z,channel,time)$ movies that display patterns of cellular motion and signaling dynamics. We present here an approach to finding spatiotemporal patterns of cell signaling dynamics in 5-D live cell microscopy movies unique in requiring no \emph{a priori} knowledge of expected pattern dynamics, and no training data. The proposed cell signaling structure function (SSF) is a Kolmogorov structure function that optimally measures cell signaling state as nuclear intensity w.r.t. surrounding cytoplasm, a significant improvement compared to the current state-of-the-art cytonuclear ratio. SSF kymographs store at each spatiotemporal cell centroid the SSF value, or a functional output such as velocity. Patterns of similarity are identified via the metric normalized compression distance (NCD). The NCD is a reproducing kernel for a Hilbert space that represents the input SSF kymographs as points in a low dimensional embedding that optimally captures the pattern
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#37319;&#29992;ReLU$^k$&#28608;&#27963;&#20989;&#25968;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#36817;&#20284;&#24615;&#36136;&#65292;&#36890;&#36807;&#20840;&#38754;&#12289;&#26500;&#36896;&#24615;&#30340;&#35777;&#26126;&#65292;&#35777;&#26126;&#20102;&#28145;&#23618;ReLU$^k$&#32593;&#32476;&#21487;&#20197;&#31934;&#30830;&#34920;&#31034;&#39640;&#38454;&#22810;&#39033;&#24335;&#65292;&#24182;&#24314;&#31435;&#20102;&#32593;&#32476;&#21442;&#25968;&#30340;&#19978;&#30028;&#12290;&#27492;&#22806;&#65292;&#21457;&#29616;&#28145;&#23618;ReLU$^k$&#32593;&#32476;&#21487;&#20197;&#36924;&#36817;&#22810;&#31181;&#21464;&#21270;&#31354;&#38388;&#20013;&#30340;&#20989;&#25968;&#65292;&#25193;&#23637;&#20102;&#20165;&#30001;ReLU$^k$&#28608;&#27963;&#20989;&#25968;&#29983;&#25104;&#30340;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2312.16483</link><description>&lt;p&gt;
ReLU$^k$&#28608;&#27963;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#36817;&#20284;&#24615;&#36136;
&lt;/p&gt;
&lt;p&gt;
Expressivity and Approximation Properties of Deep Neural Networks with ReLU$^k$ Activation. (arXiv:2312.16483v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.16483
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#37319;&#29992;ReLU$^k$&#28608;&#27963;&#20989;&#25968;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#36817;&#20284;&#24615;&#36136;&#65292;&#36890;&#36807;&#20840;&#38754;&#12289;&#26500;&#36896;&#24615;&#30340;&#35777;&#26126;&#65292;&#35777;&#26126;&#20102;&#28145;&#23618;ReLU$^k$&#32593;&#32476;&#21487;&#20197;&#31934;&#30830;&#34920;&#31034;&#39640;&#38454;&#22810;&#39033;&#24335;&#65292;&#24182;&#24314;&#31435;&#20102;&#32593;&#32476;&#21442;&#25968;&#30340;&#19978;&#30028;&#12290;&#27492;&#22806;&#65292;&#21457;&#29616;&#28145;&#23618;ReLU$^k$&#32593;&#32476;&#21487;&#20197;&#36924;&#36817;&#22810;&#31181;&#21464;&#21270;&#31354;&#38388;&#20013;&#30340;&#20989;&#25968;&#65292;&#25193;&#23637;&#20102;&#20165;&#30001;ReLU$^k$&#28608;&#27963;&#20989;&#25968;&#29983;&#25104;&#30340;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#37319;&#29992;ReLU$^k$&#28608;&#27963;&#20989;&#25968;&#65288;$k \geq 2$&#65289;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#36817;&#20284;&#24615;&#36136;&#12290;&#34429;&#28982;&#28145;&#23618;ReLU&#32593;&#32476;&#21487;&#20197;&#26377;&#25928;&#22320;&#36924;&#36817;&#22810;&#39033;&#24335;&#65292;&#20294;&#28145;&#23618;ReLU$^k$&#32593;&#32476;&#33021;&#22815;&#31934;&#30830;&#34920;&#31034;&#39640;&#38454;&#22810;&#39033;&#24335;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#36129;&#29486;&#26159;&#36890;&#36807;&#20840;&#38754;&#12289;&#26500;&#36896;&#24615;&#30340;&#35777;&#26126;&#65292;&#35777;&#26126;&#20102;&#28145;&#23618;ReLU$^k$&#32593;&#32476;&#21487;&#20197;&#34920;&#31034;&#22810;&#39033;&#24335;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#24314;&#31435;&#32593;&#32476;&#21442;&#25968;&#30340;&#22823;&#23567;&#21644;&#25968;&#37327;&#30340;&#19978;&#30028;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#33021;&#22815;&#35777;&#26126;Sobolev&#31354;&#38388;&#20013;&#30340;&#20989;&#25968;&#20197;&#21450;&#35299;&#26512;&#20989;&#25968;&#30340;&#27425;&#20248;&#36924;&#36817;&#29575;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#30740;&#31350;&#28145;&#23618;ReLU$^k$&#32593;&#32476;&#23545;&#27973;&#23618;&#32593;&#32476;&#30340;&#34920;&#31034;&#33021;&#21147;&#65292;&#25105;&#20204;&#21457;&#29616;&#28145;&#23618;ReLU$^k$&#32593;&#32476;&#33021;&#22815;&#36924;&#36817;&#19968;&#23450;&#33539;&#22260;&#20869;&#30340;&#21464;&#21270;&#31354;&#38388;&#20013;&#30340;&#20989;&#25968;&#65292;&#25193;&#23637;&#20102;&#20165;&#30001;ReLU$^k$&#28608;&#27963;&#20989;&#25968;&#29983;&#25104;&#30340;&#31354;&#38388;&#12290;&#36825;&#19968;&#21457;&#29616;&#35777;&#26126;&#20102;&#28145;&#23618;ReLU$^k$&#32593;&#32476;&#30340;&#24191;&#27867;&#34920;&#31034;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate the expressivity and approximation properties of deep neural networks employing the ReLU$^k$ activation function for $k \geq 2$. Although deep ReLU networks can approximate polynomials effectively, deep ReLU$^k$ networks have the capability to represent higher-degree polynomials precisely. Our initial contribution is a comprehensive, constructive proof for polynomial representation using deep ReLU$^k$ networks. This allows us to establish an upper bound on both the size and count of network parameters. Consequently, we are able to demonstrate a suboptimal approximation rate for functions from Sobolev spaces as well as for analytic functions. Additionally, through an exploration of the representation power of deep ReLU$^k$ networks for shallow networks, we reveal that deep ReLU$^k$ networks can approximate functions from a range of variation spaces, extending beyond those generated solely by the ReLU$^k$ activation function. This finding demonstrates the ad
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#40065;&#26834;&#38543;&#26426;&#22270;&#29983;&#25104;&#22120;&#65288;RSGG-CE&#65289;&#29992;&#20110;&#21453;&#20107;&#23454;&#35299;&#37322;&#65292;&#35813;&#26041;&#27861;&#22312;&#29983;&#25104;&#31867;&#20284;&#20110;&#21407;&#22987;&#22270;&#24418;&#30340;&#26032;&#22270;&#24418;&#30340;&#21516;&#26102;&#65292;&#22522;&#20110;&#28508;&#22312;&#39044;&#27979;&#27169;&#22411;&#20135;&#29983;&#19981;&#21516;&#30340;&#32467;&#26524;&#12290;&#27492;&#26041;&#27861;&#21033;&#29992;&#29983;&#25104;&#26426;&#21046;&#29983;&#25104;&#21453;&#20107;&#23454;&#23454;&#20363;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#35299;&#37322;&#33021;&#21147;&#65292;&#24182;&#22312;&#20854;&#20182;&#39046;&#22495;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2312.11747</link><description>&lt;p&gt;
&#24378;&#22823;&#30340;&#38543;&#26426;&#22270;&#29983;&#25104;&#22120;&#29992;&#20110;&#21453;&#20107;&#23454;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Robust Stochastic Graph Generator for Counterfactual Explanations. (arXiv:2312.11747v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.11747
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#40065;&#26834;&#38543;&#26426;&#22270;&#29983;&#25104;&#22120;&#65288;RSGG-CE&#65289;&#29992;&#20110;&#21453;&#20107;&#23454;&#35299;&#37322;&#65292;&#35813;&#26041;&#27861;&#22312;&#29983;&#25104;&#31867;&#20284;&#20110;&#21407;&#22987;&#22270;&#24418;&#30340;&#26032;&#22270;&#24418;&#30340;&#21516;&#26102;&#65292;&#22522;&#20110;&#28508;&#22312;&#39044;&#27979;&#27169;&#22411;&#20135;&#29983;&#19981;&#21516;&#30340;&#32467;&#26524;&#12290;&#27492;&#26041;&#27861;&#21033;&#29992;&#29983;&#25104;&#26426;&#21046;&#29983;&#25104;&#21453;&#20107;&#23454;&#23454;&#20363;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#35299;&#37322;&#33021;&#21147;&#65292;&#24182;&#22312;&#20854;&#20182;&#39046;&#22495;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#35299;&#37322;&#65288;CE&#65289;&#25216;&#26415;&#20316;&#20026;&#21521;&#19982;AI&#31995;&#32479;&#20114;&#21160;&#30340;&#29992;&#25143;&#25552;&#20379;&#27934;&#23519;&#21147;&#30340;&#25163;&#27573;&#24050;&#24341;&#36215;&#20851;&#27880;&#12290;&#34429;&#28982;&#22312;&#21307;&#23398;&#25104;&#20687;&#21644;&#33258;&#21160;&#39550;&#39542;&#31561;&#39046;&#22495;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#22270;&#24418;&#21453;&#20107;&#23454;&#35299;&#37322;&#65288;GCE&#65289;&#26041;&#27861;&#30456;&#23545;&#36739;&#23569;&#12290; GCE&#29983;&#25104;&#19968;&#20010;&#31867;&#20284;&#20110;&#21407;&#22987;&#22270;&#24418;&#30340;&#26032;&#22270;&#24418;&#65292;&#20854;&#32467;&#26524;&#22522;&#20110;&#28508;&#22312;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;&#22312;&#36825;&#20123;GCE&#25216;&#26415;&#20013;&#65292;&#23613;&#31649;&#22312;&#20854;&#20182;&#39046;&#22495;&#22914;&#33402;&#26415;&#39118;&#26684;&#21644;&#33258;&#28982;&#35821;&#35328;&#24314;&#27169;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25104;&#23601;&#65292;&#20294;&#22522;&#20110;&#29983;&#25104;&#26426;&#21046;&#30340;&#35299;&#37322;&#22120;&#20173;&#28982;&#21463;&#21040;&#20102;&#30456;&#23545;&#26377;&#38480;&#30340;&#30740;&#31350;&#12290;&#23545;&#29983;&#25104;&#35299;&#37322;&#22120;&#30340;&#20559;&#22909;&#28304;&#20110;&#23427;&#20204;&#22312;&#25512;&#26029;&#26399;&#38388;&#29983;&#25104;&#21453;&#20107;&#23454;&#23454;&#20363;&#30340;&#33021;&#21147;&#65292;&#21033;&#29992;&#33258;&#21160;&#33719;&#21462;&#30340;&#36755;&#20837;&#22270;&#24418;&#30340;&#25200;&#21160;&#12290;&#22312;&#19978;&#36848;&#29702;&#30001;&#30340;&#25512;&#21160;&#19979;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#20171;&#32461;&#20102;RSGG-CE&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#40065;&#26834;&#38543;&#26426;&#22270;&#29983;&#25104;&#22120;&#65292;&#29992;&#20110;&#21453;&#20107;&#23454;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Explanation (CE) techniques have garnered attention as a means to provide insights to the users engaging with AI systems. While extensively researched in domains such as medical imaging and autonomous vehicles, Graph Counterfactual Explanation (GCE) methods have been comparatively under-explored. GCEs generate a new graph similar to the original one, with a different outcome grounded on the underlying predictive model. Among these GCE techniques, those rooted in generative mechanisms have received relatively limited investigation despite demonstrating impressive accomplishments in other domains, such as artistic styles and natural language modelling. The preference for generative explainers stems from their capacity to generate counterfactual instances during inference, leveraging autonomously acquired perturbations of the input graph. Motivated by the rationales above, our study introduces RSGG-CE, a novel Robust Stochastic Graph Generator for Counterfactual Explanation
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#37325;&#26032;&#21442;&#25968;&#21270;&#20302;&#31209;&#25552;&#31034;&#65288;RLP&#65289;&#65292;&#29992;&#20110;&#22312;&#22823;&#22411;&#39044;&#35757;&#32451;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#36866;&#24212;&#36807;&#31243;&#20013;&#23454;&#29616;&#39640;&#25928;&#21644;&#26377;&#25928;&#30340;&#30693;&#35782;&#36716;&#31227;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#21487;&#35843;&#21442;&#25968;&#21644;&#23384;&#20648;&#24320;&#38144;&#12290;</title><link>http://arxiv.org/abs/2312.10813</link><description>&lt;p&gt;
&#37325;&#26032;&#21442;&#25968;&#21270;&#20302;&#31209;&#25552;&#31034;&#65306;&#22312;0.5K&#21442;&#25968;&#20869;&#25512;&#24191;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Re-parameterized Low-rank Prompt: Generalize a Vision-Language Model within 0.5K Parameters. (arXiv:2312.10813v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.10813
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#37325;&#26032;&#21442;&#25968;&#21270;&#20302;&#31209;&#25552;&#31034;&#65288;RLP&#65289;&#65292;&#29992;&#20110;&#22312;&#22823;&#22411;&#39044;&#35757;&#32451;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#36866;&#24212;&#36807;&#31243;&#20013;&#23454;&#29616;&#39640;&#25928;&#21644;&#26377;&#25928;&#30340;&#30693;&#35782;&#36716;&#31227;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#21487;&#35843;&#21442;&#25968;&#21644;&#23384;&#20648;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#39044;&#35757;&#32451;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#22914;&#20309;&#26377;&#25928;&#22320;&#23558;&#36825;&#20123;&#22522;&#30784;&#27169;&#22411;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#19979;&#28216;&#20219;&#21153;&#20013;&#25104;&#20026;&#19968;&#20010;&#28909;&#38376;&#35805;&#39064;&#65292;&#23588;&#20854;&#26159;&#22312;&#25968;&#25454;&#19981;&#36275;&#30340;&#24773;&#20917;&#19979;&#12290;&#26368;&#36817;&#65292;&#25552;&#31034;&#35843;&#20248;&#24050;&#25104;&#20026;&#19968;&#31181;&#27969;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#35843;&#25972;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#26102;&#65292;&#30740;&#31350;&#20154;&#21592;&#20923;&#32467;&#39592;&#24178;&#37096;&#20998;&#30340;&#21442;&#25968;&#65292;&#21482;&#35774;&#35745;&#21644;&#35843;&#25972;&#25552;&#31034;&#12290;&#19968;&#26041;&#38754;&#65292;&#25552;&#31034;&#35843;&#20248;&#30340;&#31934;&#24515;&#35774;&#35745;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22797;&#26434;&#30340;&#32467;&#26500;&#21644;&#26356;&#26032;&#35268;&#21017;&#22823;&#22823;&#22686;&#21152;&#20102;&#35745;&#31639;&#21644;&#23384;&#20648;&#25104;&#26412;&#12290;&#21463;&#21040;&#35266;&#23519;&#21040;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#27867;&#21270;&#33021;&#21147;&#30340;&#28436;&#21464;&#27169;&#24335;&#19982;&#36866;&#24212;&#36807;&#31243;&#20013;&#25552;&#31034;&#30697;&#38453;&#31209;&#21464;&#21270;&#36235;&#21183;&#30340;&#35843;&#21644;&#19968;&#33268;&#24615;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#22411;&#25552;&#31034;&#65292;&#37325;&#26032;&#21442;&#25968;&#21270;&#20302;&#31209;&#25552;&#31034;&#65288;RLP&#65289;&#65292;&#29992;&#20110;&#39640;&#25928;&#21644;&#26377;&#25928;&#30340;&#36866;&#24212;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22823;&#22823;&#20943;&#23569;&#21487;&#35843;&#21442;&#25968;&#21644;&#23384;&#20648;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the development of large pre-trained vision-language models, how to effectively transfer the knowledge of such foundational models to downstream tasks becomes a hot topic, especially in a data-deficient scenario. Recently, prompt tuning has become a popular solution. When adapting the vision-language models, researchers freeze the parameters in the backbone and only design and tune the prompts. On the one hand, the delicate design of prompt tuning exhibits strong performance. On the other hand, complicated structures and update rules largely increase the computation and storage cost. Motivated by the observation that the evolution pattern of the generalization capability in visual-language models aligns harmoniously with the trend of rank variations in the prompt matrix during adaptation, we design a new type of prompt, Re-parameterized Low-rank Prompt (RLP), for both efficient and effective adaptation. Our method could largely reduce the number of tunable parameters and storage s
&lt;/p&gt;</description></item><item><title>ConFormer&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#26088;&#22312;&#33258;&#21160;&#20272;&#35745;&#36229;&#22768;&#24515;&#21160;&#22270;&#20013;&#30340;&#23556;&#34880;&#20998;&#25968;&#65288;EF&#65289;&#21644;&#24038;&#24515;&#23460;&#22721;&#21402;&#24230;&#65292;&#20174;&#32780;&#23454;&#29616;&#25104;&#26412;&#25928;&#30410;&#39640;&#12289;&#26131;&#33719;&#21462;&#21644;&#20840;&#38754;&#30340;&#24515;&#33039;&#20581;&#24247;&#30417;&#27979;&#12290;</title><link>http://arxiv.org/abs/2312.08567</link><description>&lt;p&gt;
ConFormer: &#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38598;&#21512;&#65292;&#29992;&#20110;&#21327;&#21161;&#24515;&#33039;&#30149;&#23398;&#23478;&#35780;&#20272;&#24515;&#33039;&#21151;&#33021;
&lt;/p&gt;
&lt;p&gt;
ConFormer: A Novel Collection of Deep Learning Models to Assist Cardiologists in the Assessment of Cardiac Function. (arXiv:2312.08567v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.08567
&lt;/p&gt;
&lt;p&gt;
ConFormer&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#26088;&#22312;&#33258;&#21160;&#20272;&#35745;&#36229;&#22768;&#24515;&#21160;&#22270;&#20013;&#30340;&#23556;&#34880;&#20998;&#25968;&#65288;EF&#65289;&#21644;&#24038;&#24515;&#23460;&#22721;&#21402;&#24230;&#65292;&#20174;&#32780;&#23454;&#29616;&#25104;&#26412;&#25928;&#30410;&#39640;&#12289;&#26131;&#33719;&#21462;&#21644;&#20840;&#38754;&#30340;&#24515;&#33039;&#20581;&#24247;&#30417;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#34880;&#31649;&#30142;&#30149;&#65292;&#29305;&#21035;&#26159;&#24515;&#21147;&#34928;&#31469;&#65292;&#26159;&#20840;&#29699;&#27515;&#20129;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#24120;&#35268;&#36229;&#22768;&#24515;&#21160;&#22270;&#31579;&#26597;&#26089;&#26399;&#24515;&#21147;&#34928;&#31469;&#30340;&#26816;&#27979;&#24120;&#24120;&#21463;&#21040;&#26114;&#36149;&#21644;&#21171;&#21160;&#23494;&#38598;&#22411;&#30340;&#38480;&#21046;&#65292;&#36825;&#20010;&#38556;&#30861;&#21487;&#33021;&#24847;&#21619;&#30528;&#29983;&#19982;&#27515;&#30340;&#24046;&#21035;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;ConFormer&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#26088;&#22312;&#33258;&#21160;&#20272;&#35745;&#36229;&#22768;&#24515;&#21160;&#22270;&#20013;&#30340;&#23556;&#34880;&#20998;&#25968;&#65288;EF&#65289;&#21644;&#24038;&#24515;&#23460;&#22721;&#21402;&#24230;&#12290;ConFormer&#30340;&#23454;&#26045;&#26377;&#21487;&#33021;&#36890;&#36807;&#23454;&#29616;&#25104;&#26412;&#25928;&#30410;&#39640;&#12289;&#26131;&#33719;&#21462;&#21644;&#20840;&#38754;&#30340;&#24515;&#33039;&#20581;&#24247;&#30417;&#27979;&#65292;&#20174;&#32780;&#25327;&#25937;&#26080;&#25968;&#29983;&#21629;&#12290;&#28304;&#20195;&#30721;&#21487;&#22312;https://github.com/Aether111/ConFormer&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cardiovascular diseases, particularly heart failure, are a leading cause of death globally. The early detection of heart failure through routine echocardiogram screenings is often impeded by the high cost and labor-intensive nature of these procedures, a barrier that can mean the difference between life and death. This paper presents ConFormer, a novel deep learning model designed to automate the estimation of Ejection Fraction (EF) and Left Ventricular Wall Thickness from echocardiograms. The implementation of ConFormer has the potential to enhance preventative cardiology by enabling cost-effective, accessible, and comprehensive heart health monitoring, thereby saving countless lives. The source code is available at https://github.com/Aether111/ConFormer.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#27744;&#21270;&#23618;&#36755;&#20837;&#26469;&#23454;&#29616;&#23545;&#38544;&#31169;&#25915;&#20987;&#30340;&#25913;&#36827;&#12290;&#36890;&#36807;&#24674;&#22797;&#27744;&#21270;&#23618;&#36755;&#20837;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#22312;&#19981;&#21516;&#30340;&#25209;&#22788;&#29702;&#22823;&#23567;&#19979;&#25552;&#20379;&#26356;&#39640;&#30340;&#25991;&#26412;&#24674;&#22797;&#29575;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#32454;&#33268;&#21644;&#26377;&#25928;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2312.05720</link><description>&lt;p&gt;
&#36229;&#36234;&#26799;&#24230;&#21644;&#20808;&#39564;&#30693;&#35782;&#22312;&#38544;&#31169;&#25915;&#20987;&#20013;&#65306;&#21033;&#29992;&#32852;&#37030;&#23398;&#20064;&#20013;&#35821;&#35328;&#27169;&#22411;&#30340;&#27744;&#21270;&#23618;&#36755;&#20837;
&lt;/p&gt;
&lt;p&gt;
Beyond Gradient and Priors in Privacy Attacks: Leveraging Pooler Layer Inputs of Language Models in Federated Learning. (arXiv:2312.05720v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.05720
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#27744;&#21270;&#23618;&#36755;&#20837;&#26469;&#23454;&#29616;&#23545;&#38544;&#31169;&#25915;&#20987;&#30340;&#25913;&#36827;&#12290;&#36890;&#36807;&#24674;&#22797;&#27744;&#21270;&#23618;&#36755;&#20837;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#22312;&#19981;&#21516;&#30340;&#25209;&#22788;&#29702;&#22823;&#23567;&#19979;&#25552;&#20379;&#26356;&#39640;&#30340;&#25991;&#26412;&#24674;&#22797;&#29575;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#32454;&#33268;&#21644;&#26377;&#25928;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#24378;&#35843;&#20998;&#25955;&#24335;&#35757;&#32451;&#65292;&#36890;&#36807;&#26412;&#22320;&#23384;&#20648;&#25968;&#25454;&#24182;&#20165;&#21457;&#36865;&#27169;&#22411;&#26356;&#26032;&#65292;&#24378;&#35843;&#29992;&#25143;&#38544;&#31169;&#12290;&#26368;&#36817;&#65292;&#19968;&#31995;&#21015;&#26377;&#20851;&#38544;&#31169;&#25915;&#20987;&#30340;&#24037;&#20316;&#36890;&#36807;&#20174;&#32852;&#37030;&#23398;&#20064;&#19978;&#19979;&#25991;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#25935;&#24863;&#30340;&#35757;&#32451;&#25991;&#26412;&#26469;&#25439;&#23475;&#29992;&#25143;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25915;&#20987;&#25216;&#26415;&#38754;&#20020;&#30528;&#19981;&#21516;&#30340;&#38556;&#30861;&#65306;&#19968;&#20123;&#24037;&#20316;&#20027;&#35201;&#20351;&#29992;&#26377;&#38480;&#30340;&#25209;&#22788;&#29702;&#22823;&#23567;&#65288;&#20363;&#22914;&#65292;&#25209;&#22788;&#29702;&#22823;&#23567;&#20026;1&#65289;&#65292;&#32780;&#20854;&#20182;&#25216;&#26415;&#21017;&#23481;&#26131;&#34987;&#26816;&#27979;&#20986;&#26469;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#38590;&#20197;&#26816;&#27979;&#30340;&#29305;&#28857;&#65292;&#22312;&#19981;&#21516;&#30340;&#25209;&#22788;&#29702;&#22823;&#23567;&#35774;&#32622;&#19979;&#26174;&#33879;&#25552;&#39640;&#20102;&#25991;&#26412;&#24674;&#22797;&#29575;&#12290;&#22522;&#20110;&#22522;&#26412;&#30340;&#26799;&#24230;&#21305;&#37197;&#21644;&#39046;&#22495;&#20808;&#39564;&#30693;&#35782;&#65292;&#25105;&#20204;&#36890;&#36807;&#24674;&#22797;&#35821;&#35328;&#27169;&#22411;&#30340;&#27744;&#21270;&#23618;&#36755;&#20837;&#26469;&#22686;&#24378;&#25915;&#20987;&#33021;&#21147;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#29305;&#24449;&#32423;&#21035;&#25552;&#20379;&#39069;&#22806;&#30340;&#30417;&#30563;&#20449;&#21495;&#12290;&#19982;&#26799;&#24230;&#25968;&#25454;&#19981;&#21516;&#65292;&#36825;&#20123;&#20449;&#21495;&#19981;&#20250;&#22312;&#21477;&#23376;&#21644;&#26631;&#35760;&#20043;&#38388;&#36827;&#34892;&#24179;&#22343;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#32454;&#33268;&#21644;&#26377;&#25928;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) emphasizes decentralized training by storing data locally and sending only model updates, underlining user privacy. Recently, a line of works on privacy attacks impairs user privacy by extracting sensitive training text from language models in the context of FL. Yet, these attack techniques face distinct hurdles: some work chiefly with limited batch sizes (e.g., batch size of 1), and others are easily detectable. This paper introduces an innovative approach that is challenging to detect, significantly enhancing the recovery rate of text in various batch-size settings. Building on fundamental gradient matching and domain prior knowledge, we enhance the attack by recovering the input of the Pooler layer of language models, which enables us to provide additional supervised signals at the feature level. Unlike gradient data, these signals do not average across sentences and tokens, thereby offering more nuanced and effective insights. We benchmark our method using t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;StyleAligned&#30340;&#26032;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;T2I&#27169;&#22411;&#20013;&#20351;&#29992;&#8220;&#27880;&#24847;&#21147;&#20849;&#20139;&#8221;&#65292;&#23454;&#29616;&#20102;&#22312;&#19968;&#31995;&#21015;&#29983;&#25104;&#30340;&#22270;&#20687;&#20043;&#38388;&#24314;&#31435;&#39118;&#26684;&#23545;&#40784;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#36890;&#36807;&#31616;&#21333;&#30340;&#21453;&#36716;&#25805;&#20316;&#65292;&#20351;&#29992;&#21442;&#32771;&#39118;&#26684;&#21019;&#24314;&#20855;&#26377;&#19968;&#33268;&#39118;&#26684;&#30340;&#22270;&#20687;&#65292;&#24182;&#22312;&#21508;&#31181;&#36755;&#20837;&#20013;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#21644;&#20445;&#30495;&#24230;&#12290;</title><link>http://arxiv.org/abs/2312.02133</link><description>&lt;p&gt;
&#36890;&#36807;&#20849;&#20139;&#27880;&#24847;&#21147;&#23454;&#29616;&#39118;&#26684;&#23545;&#40784;&#30340;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Style Aligned Image Generation via Shared Attention. (arXiv:2312.02133v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.02133
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;StyleAligned&#30340;&#26032;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;T2I&#27169;&#22411;&#20013;&#20351;&#29992;&#8220;&#27880;&#24847;&#21147;&#20849;&#20139;&#8221;&#65292;&#23454;&#29616;&#20102;&#22312;&#19968;&#31995;&#21015;&#29983;&#25104;&#30340;&#22270;&#20687;&#20043;&#38388;&#24314;&#31435;&#39118;&#26684;&#23545;&#40784;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#36890;&#36807;&#31616;&#21333;&#30340;&#21453;&#36716;&#25805;&#20316;&#65292;&#20351;&#29992;&#21442;&#32771;&#39118;&#26684;&#21019;&#24314;&#20855;&#26377;&#19968;&#33268;&#39118;&#26684;&#30340;&#22270;&#20687;&#65292;&#24182;&#22312;&#21508;&#31181;&#36755;&#20837;&#20013;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#21644;&#20445;&#30495;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#27169;&#22411;&#22312;&#21019;&#24847;&#39046;&#22495;&#36805;&#36895;&#23853;&#38706;&#22836;&#35282;&#65292;&#20174;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;&#35270;&#35273;&#19978;&#24341;&#20154;&#27880;&#30446;&#30340;&#36755;&#20986;&#29289;&#12290;&#28982;&#32780;&#65292;&#25511;&#21046;&#36825;&#20123;&#27169;&#22411;&#20197;&#30830;&#20445;&#19968;&#33268;&#30340;&#39118;&#26684;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#29616;&#26377;&#26041;&#27861;&#38656;&#35201;&#36827;&#34892;&#31934;&#32454;&#35843;&#25972;&#21644;&#25163;&#21160;&#24178;&#39044;&#20197;&#35299;&#24320;&#20869;&#23481;&#21644;&#39118;&#26684;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;StyleAligned&#30340;&#26032;&#25216;&#26415;&#65292;&#26088;&#22312;&#22312;&#19968;&#31995;&#21015;&#29983;&#25104;&#30340;&#22270;&#20687;&#20043;&#38388;&#24314;&#31435;&#39118;&#26684;&#23545;&#40784;&#12290;&#36890;&#36807;&#22312;&#25193;&#25955;&#36807;&#31243;&#20013;&#20351;&#29992;&#26368;&#23567;&#30340;&#8220;&#27880;&#24847;&#21147;&#20849;&#20139;&#8221;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;T2I&#27169;&#22411;&#20013;&#20445;&#25345;&#22270;&#20687;&#20043;&#38388;&#30340;&#39118;&#26684;&#19968;&#33268;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#31616;&#21333;&#30340;&#21453;&#36716;&#25805;&#20316;&#65292;&#21487;&#20351;&#29992;&#21442;&#32771;&#39118;&#26684;&#26469;&#21019;&#24314;&#20855;&#26377;&#19968;&#33268;&#39118;&#26684;&#30340;&#22270;&#20687;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#39118;&#26684;&#21644;&#25991;&#26412;&#25552;&#31034;&#19978;&#23545;&#35813;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#20986;&#20102;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#21644;&#20445;&#30495;&#24230;&#65292;&#20984;&#26174;&#20102;&#20854;&#22312;&#23454;&#29616;&#21508;&#31181;&#36755;&#20837;&#30340;&#19968;&#33268;&#39118;&#26684;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale Text-to-Image (T2I) models have rapidly gained prominence across creative fields, generating visually compelling outputs from textual prompts. However, controlling these models to ensure consistent style remains challenging, with existing methods necessitating fine-tuning and manual intervention to disentangle content and style. In this paper, we introduce StyleAligned, a novel technique designed to establish style alignment among a series of generated images. By employing minimal `attention sharing' during the diffusion process, our method maintains style consistency across images within T2I models. This approach allows for the creation of style-consistent images using a reference style through a straightforward inversion operation. Our method's evaluation across diverse styles and text prompts demonstrates high-quality synthesis and fidelity, underscoring its efficacy in achieving consistent style across various inputs.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20449;&#24687;&#29942;&#39048;&#36974;&#34109;&#23376;&#32593;&#32476; (IBM) &#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28040;&#38500;&#23376;&#32593;&#32476;&#20869;&#37096;&#30340;&#20887;&#20313;&#26469;&#35299;&#20915;&#19981;&#26029;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#32047;&#31215;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#21040;&#37325;&#35201;&#30340;&#26435;&#37325;&#20013;&#65292;&#26500;&#24314;&#20102;&#26080;&#20887;&#20313;&#30340;&#23376;&#32593;&#32476;&#65292;&#26377;&#25928;&#22320;&#20943;&#36731;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#20419;&#36827;&#20102;&#26032;&#20219;&#21153;&#30340;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2312.00840</link><description>&lt;p&gt;
&#26397;&#30528;&#26080;&#20887;&#20313;&#23376;&#32593;&#32476;&#30340;&#19981;&#26029;&#23398;&#20064;&#20043;&#36335;
&lt;/p&gt;
&lt;p&gt;
Towards Redundancy-Free Sub-networks in Continual Learning. (arXiv:2312.00840v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.00840
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20449;&#24687;&#29942;&#39048;&#36974;&#34109;&#23376;&#32593;&#32476; (IBM) &#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28040;&#38500;&#23376;&#32593;&#32476;&#20869;&#37096;&#30340;&#20887;&#20313;&#26469;&#35299;&#20915;&#19981;&#26029;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#32047;&#31215;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#21040;&#37325;&#35201;&#30340;&#26435;&#37325;&#20013;&#65292;&#26500;&#24314;&#20102;&#26080;&#20887;&#20313;&#30340;&#23376;&#32593;&#32476;&#65292;&#26377;&#25928;&#22320;&#20943;&#36731;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#20419;&#36827;&#20102;&#26032;&#20219;&#21153;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#26029;&#23398;&#20064;&#20013;&#65292;&#28798;&#38590;&#24615;&#36951;&#24536;&#26159;&#19968;&#20010;&#31361;&#20986;&#30340;&#38382;&#39064;&#12290;&#21442;&#25968;&#38548;&#31163;&#36890;&#36807;&#36974;&#34109;&#27599;&#20010;&#20219;&#21153;&#30340;&#23376;&#32593;&#32476;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#20197;&#20943;&#36731;&#23545;&#26087;&#20219;&#21153;&#30340;&#24178;&#25200;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#23376;&#32593;&#32476;&#26159;&#22522;&#20110;&#26435;&#37325;&#22823;&#23567;&#26500;&#24314;&#30340;&#65292;&#32780;&#26435;&#37325;&#22823;&#23567;&#24182;&#19981;&#19968;&#23450;&#23545;&#24212;&#26435;&#37325;&#30340;&#37325;&#35201;&#24615;&#65292;&#23548;&#33268;&#20445;&#30041;&#19981;&#37325;&#35201;&#30340;&#26435;&#37325;&#24182;&#26500;&#24314;&#20887;&#20313;&#30340;&#23376;&#32593;&#32476;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#21463;&#21040;&#20449;&#24687;&#29942;&#39048;&#30340;&#21551;&#21457;&#65292;&#35813;&#26041;&#27861;&#28040;&#38500;&#20102;&#30456;&#37051;&#32593;&#32476;&#23618;&#20043;&#38388;&#30340;&#20887;&#20313;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20449;&#24687;&#29942;&#39048;&#36974;&#34109;&#23376;&#32593;&#32476; (IBM) &#26469;&#28040;&#38500;&#23376;&#32593;&#32476;&#20869;&#30340;&#20887;&#20313;&#12290;&#20855;&#20307;&#22320;&#65292;IBM&#23558;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#32047;&#31215;&#21040;&#37325;&#35201;&#30340;&#26435;&#37325;&#20013;&#65292;&#26500;&#24314;&#26080;&#20887;&#20313;&#30340;&#23376;&#32593;&#32476;&#65292;&#19981;&#20165;&#36890;&#36807;&#20923;&#32467;&#23376;&#32593;&#32476;&#26377;&#25928;&#22320;&#20943;&#36731;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#36824;&#36890;&#36807;&#26377;&#20215;&#20540;&#30340;&#30693;&#35782;&#20256;&#36882;&#20419;&#36827;&#20102;&#26032;&#20219;&#21153;&#30340;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;IBM&#20998;&#35299;&#20102;&#38544;&#34255;&#30340;r
&lt;/p&gt;
&lt;p&gt;
Catastrophic Forgetting (CF) is a prominent issue in continual learning. Parameter isolation addresses this challenge by masking a sub-network for each task to mitigate interference with old tasks. However, these sub-networks are constructed relying on weight magnitude, which does not necessarily correspond to the importance of weights, resulting in maintaining unimportant weights and constructing redundant sub-networks. To overcome this limitation, inspired by information bottleneck, which removes redundancy between adjacent network layers, we propose \textbf{\underline{I}nformation \underline{B}ottleneck \underline{M}asked sub-network (IBM)} to eliminate redundancy within sub-networks. Specifically, IBM accumulates valuable information into essential weights to construct redundancy-free sub-networks, not only effectively mitigating CF by freezing the sub-networks but also facilitating new tasks training through the transfer of valuable knowledge. Additionally, IBM decomposes hidden r
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#29983;&#25104;&#24615;&#20154;&#24037;&#26234;&#33021;&#20013;&#25968;&#25454;&#38544;&#31169;&#21644;&#29256;&#26435;&#20445;&#25252;&#30340;&#22810;&#26041;&#38754;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#23558;&#25216;&#26415;&#21019;&#26032;&#19982;&#20262;&#29702;&#21069;&#30651;&#30456;&#32467;&#21512;&#30340;&#32508;&#21512;&#26041;&#27861;&#65292;&#26088;&#22312;&#20840;&#38754;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2311.18252</link><description>&lt;p&gt;
&#36328;&#36234;&#29983;&#25104;&#24615;&#20154;&#24037;&#26234;&#33021;&#25968;&#25454;&#29983;&#21629;&#21608;&#26399;&#30340;&#38544;&#31169;&#21644;&#29256;&#26435;&#25361;&#25112;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Navigating Privacy and Copyright Challenges Across the Data Lifecycle of Generative AI. (arXiv:2311.18252v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.18252
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#29983;&#25104;&#24615;&#20154;&#24037;&#26234;&#33021;&#20013;&#25968;&#25454;&#38544;&#31169;&#21644;&#29256;&#26435;&#20445;&#25252;&#30340;&#22810;&#26041;&#38754;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#23558;&#25216;&#26415;&#21019;&#26032;&#19982;&#20262;&#29702;&#21069;&#30651;&#30456;&#32467;&#21512;&#30340;&#32508;&#21512;&#26041;&#27861;&#65292;&#26088;&#22312;&#20840;&#38754;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24615;&#20154;&#24037;&#26234;&#33021;&#30340;&#20986;&#29616;&#26631;&#24535;&#30528;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#37325;&#35201;&#37324;&#31243;&#30865;&#65292;&#23637;&#31034;&#20986;&#22312;&#29983;&#25104;&#30495;&#23454;&#22270;&#20687;&#12289;&#25991;&#26412;&#21644;&#25968;&#25454;&#27169;&#24335;&#26041;&#38754;&#30340;&#21331;&#36234;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#36827;&#23637;&#20063;&#24102;&#26469;&#20102;&#23545;&#25968;&#25454;&#38544;&#31169;&#21644;&#29256;&#26435;&#20405;&#29359;&#30340;&#26356;&#39640;&#20851;&#27880;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#27169;&#22411;&#35757;&#32451;&#23545;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#20381;&#36182;&#12290;&#20256;&#32479;&#26041;&#27861;&#22914;&#24046;&#20998;&#38544;&#31169;&#12289;&#26426;&#22120;&#36951;&#24536;&#21644;&#25968;&#25454;&#20013;&#27602;&#21482;&#25552;&#20379;&#20102;&#23545;&#36825;&#20123;&#22797;&#26434;&#38382;&#39064;&#30340;&#29255;&#38754;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#25968;&#25454;&#29983;&#21629;&#21608;&#26399;&#20869;&#38544;&#31169;&#21644;&#29256;&#26435;&#20445;&#25252;&#30340;&#22810;&#26041;&#38754;&#25361;&#25112;&#12290;&#25105;&#20204;&#20027;&#24352;&#37319;&#29992;&#23558;&#25216;&#26415;&#21019;&#26032;&#19982;&#20262;&#29702;&#21069;&#30651;&#30456;&#32467;&#21512;&#30340;&#32508;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#30740;&#31350;&#21644;&#21046;&#23450;&#22312;&#29983;&#21629;&#21608;&#26399;&#35270;&#35282;&#19979;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20840;&#38754;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25512;&#21160;&#26356;&#24191;&#27867;&#30340;&#35752;&#35770;&#65292;&#24182;&#28608;&#21169;&#23545;&#29983;&#25104;&#24615;&#20154;&#24037;&#26234;&#33021;&#20013;&#25968;&#25454;&#38544;&#31169;&#21644;&#29256;&#26435;&#23436;&#25972;&#24615;&#30340;&#21327;&#21516;&#21162;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of Generative AI has marked a significant milestone in artificial intelligence, demonstrating remarkable capabilities in generating realistic images, texts, and data patterns. However, these advancements come with heightened concerns over data privacy and copyright infringement, primarily due to the reliance on vast datasets for model training. Traditional approaches like differential privacy, machine unlearning, and data poisoning only offer fragmented solutions to these complex issues. Our paper delves into the multifaceted challenges of privacy and copyright protection within the data lifecycle. We advocate for integrated approaches that combines technical innovation with ethical foresight, holistically addressing these concerns by investigating and devising solutions that are informed by the lifecycle perspective. This work aims to catalyze a broader discussion and inspire concerted efforts towards data privacy and copyright integrity in Generative AI.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#22270;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#21487;&#20197;&#26377;&#25928;&#35782;&#21035;&#21435;&#20013;&#24515;&#21270;&#33258;&#27835;&#32452;&#32455;&#65288;DAO&#65289;&#20013;&#30340;&#34394;&#20551;&#36523;&#20221;&#65292;&#20026;&#20998;&#25955;&#27835;&#29702;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;</title><link>http://arxiv.org/abs/2311.17929</link><description>&lt;p&gt;
&#26032;&#30340;&#22312;&#32447;&#31038;&#21306;&#65306;&#22312;&#21311;&#21517;&#25237;&#31080;&#32593;&#32476;&#19978;&#36827;&#34892;&#22270;&#28145;&#24230;&#23398;&#20064;&#20197;&#35782;&#21035;&#22810;&#20013;&#24515;&#27835;&#29702;&#20013;&#30340;&#34394;&#20551;&#36523;&#20221;
&lt;/p&gt;
&lt;p&gt;
New Online Communities: Graph Deep Learning on Anonymous Voting Networks to Identify Sybils in Polycentric Governance. (arXiv:2311.17929v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.17929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#22270;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#21487;&#20197;&#26377;&#25928;&#35782;&#21035;&#21435;&#20013;&#24515;&#21270;&#33258;&#27835;&#32452;&#32455;&#65288;DAO&#65289;&#20013;&#30340;&#34394;&#20551;&#36523;&#20221;&#65292;&#20026;&#20998;&#25955;&#27835;&#29702;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#21435;&#20013;&#24515;&#21270;&#33258;&#27835;&#32452;&#32455;&#65288;DAO&#65289;&#20013;&#25968;&#23383;&#36164;&#20135;&#30340;&#22810;&#20013;&#24515;&#27835;&#29702;&#12290;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#24320;&#21457;&#19968;&#31181;&#26041;&#27861;&#26469;&#35782;&#21035;&#34394;&#20551;&#36523;&#20221;&#65288;sybils&#65289;&#26469;&#35299;&#20915;&#20998;&#25955;&#27835;&#29702;&#38754;&#20020;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#22270;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;DAO&#27835;&#29702;&#25968;&#25454;&#38598;&#65288;snapshot.org&#65289;&#20013;&#35782;&#21035;&#34394;&#20551;&#36523;&#20221;&#30340;&#27963;&#21160;&#12290;&#20855;&#20307;&#22320;&#65292;&#19968;&#20010;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;GCNN&#65289;&#23398;&#20064;&#20102;&#25237;&#31080;&#34892;&#20026;&#65292;&#24182;&#20351;&#29992;&#39640;&#32500;&#23884;&#20837;&#26469;&#35782;&#21035;&#22270;&#20013;&#30456;&#20284;&#33410;&#28857;&#30340;&#24555;&#36895;k&#22343;&#20540;&#21521;&#37327;&#32858;&#31867;&#31639;&#27861;&#65288;FAISS&#65289;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#28145;&#24230;&#23398;&#20064;&#33021;&#22815;&#26377;&#25928;&#35782;&#21035;&#34394;&#20551;&#36523;&#20221;&#65292;&#22312;&#25237;&#31080;&#22270;&#20013;&#20943;&#23569;2-5%&#12290;&#26412;&#30740;&#31350;&#24378;&#35843;&#20102;&#22312;DAO&#20013;&#30340;sybil&#25239;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#20026;&#20998;&#25955;&#27835;&#29702;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#65292;&#23545;&#26410;&#26469;&#30340;&#25919;&#31574;&#12289;&#30417;&#31649;&#21644;&#27835;&#29702;&#23454;&#36341;&#20855;&#26377;&#21551;&#31034;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research examines the polycentric governance of digital assets in blockchain-based Decentralized Autonomous Organizations (DAOs). It offers a theoretical framework and addresses a critical challenge facing decentralized governance by developing a method to identify sybils, or spurious identities. The method uses graph deep learning techniques to identify sybil activity in a DAO governance dataset (snapshot.org). Specifically, a Graph Convolutional Neural Network (GCNN) learned voting behaviours and a fast k-means vector clustering algorithm (FAISS) used the high dimensional embeddings to identify similar nodes in a graph. The results reveal that deep learning can effectively identify sybils, reducing the voting graph by 2-5%. This research underscores the importance of sybil resistance in DAOs and offers a novel perspective on decentralized governance, informing future policy, regulation, and governance practices.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20108;&#20540;&#31070;&#32463;&#32593;&#32476;&#65288;BNNs&#65289;&#30340;&#35268;&#27169;&#20002;&#24323;&#30340;&#26032;&#22411;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#24182;&#22522;&#20110;&#33945;&#29305;&#21345;&#27931;&#24314;&#31435;BayNN&#27169;&#22411;&#20197;&#39640;&#25928;&#22320;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.15816</link><description>&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#38543;&#26426;&#23610;&#24230;&#26469;&#20272;&#35745;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#19981;&#30830;&#23450;&#24615;&#30340;&#35268;&#27169;&#20002;&#24323;
&lt;/p&gt;
&lt;p&gt;
Scale-Dropout: Estimating Uncertainty in Deep Neural Networks Using Stochastic Scale. (arXiv:2311.15816v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.15816
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20108;&#20540;&#31070;&#32463;&#32593;&#32476;&#65288;BNNs&#65289;&#30340;&#35268;&#27169;&#20002;&#24323;&#30340;&#26032;&#22411;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#24182;&#22522;&#20110;&#33945;&#29305;&#21345;&#27931;&#24314;&#31435;BayNN&#27169;&#22411;&#20197;&#39640;&#25928;&#22320;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#23545;&#20110;&#25913;&#21892;&#22312;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#30340;&#21487;&#38752;&#24615;&#21644;&#39044;&#27979;&#30340;&#20449;&#24515;&#33267;&#20851;&#37325;&#35201;&#12290;&#20855;&#26377;Dropout&#20316;&#20026;&#36817;&#20284;&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;BayNNs&#65289;&#20026;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#25552;&#20379;&#20102;&#19968;&#31181;&#31995;&#32479;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#22312;&#21151;&#32791;&#12289;&#20869;&#23384;&#21644;&#35745;&#31639;&#26041;&#38754;&#20855;&#26377;&#39640;&#30828;&#20214;&#24320;&#38144;&#12290;&#22240;&#27492;&#65292;&#23558;BayNNs&#24212;&#29992;&#20110;&#36164;&#28304;&#26377;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#25110;&#39640;&#24615;&#33021;&#24212;&#29992;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#36890;&#36807;&#22312;&#20855;&#26377;&#33258;&#26059;&#30005;&#23376;&#23384;&#20648;&#22120;&#21644;&#21442;&#25968;&#20108;&#20540;&#21270;&#30340;&#35745;&#31639;&#20869;&#23384;&#65288;CIM&#65289;&#26550;&#26500;&#19978;&#21152;&#36895;&#23427;&#20204;&#21487;&#20197;&#20943;&#23569;BayNNs&#30340;&#19968;&#20123;&#22266;&#26377;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#23454;&#26045;&#24120;&#35268;&#22522;&#20110;dropout&#30340;BayNN&#38656;&#35201;&#22823;&#37327;&#30340;&#38543;&#26426;&#21333;&#20803;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20108;&#20540;&#31070;&#32463;&#32593;&#32476;&#65288;BNNs&#65289;&#30340;&#35268;&#27169;&#20002;&#24323;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#24182;&#22522;&#20110;&#33945;&#29305;&#21345;&#27931;-&#35268;&#27169;&#20002;&#24323;&#65288;MC-Scale Dropout&#65289;&#30340;BayNNs&#36827;&#34892;&#39640;&#25928;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Uncertainty estimation in Neural Networks (NNs) is vital in improving reliability and confidence in predictions, particularly in safety-critical applications. Bayesian Neural Networks (BayNNs) with Dropout as an approximation offer a systematic approach to quantifying uncertainty, but they inherently suffer from high hardware overhead in terms of power, memory, and computation. Thus, the applicability of BayNNs to edge devices with limited resources or to high-performance applications is challenging. Some of the inherent costs of BayNNs can be reduced by accelerating them in hardware on a Computation-In-Memory (CIM) architecture with spintronic memories and binarizing their parameters. However, numerous stochastic units are required to implement conventional dropout-based BayNN. In this paper, we propose the Scale Dropout, a novel regularization technique for Binary Neural Networks (BNNs), and Monte Carlo-Scale Dropout (MC-Scale Dropout)-based BayNNs for efficient uncertainty estimatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#22312;&#22797;&#26434;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#19978;&#25506;&#32034;&#35838;&#31243;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#35838;&#31243;&#23398;&#20064;&#26159;&#25913;&#21892;&#22797;&#26434;&#26102;&#38388;&#24207;&#21015;&#25511;&#21046;&#20219;&#21153;&#24615;&#33021;&#30340;&#26032;&#36884;&#24452;&#65292;&#32780;&#27169;&#20223;&#23398;&#20064;&#20063;&#24212;&#35813;&#34987;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2311.13326</link><description>&lt;p&gt;
&#12298;&#22522;&#20110;&#35838;&#31243;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#30340;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#26080;&#20851;&#25511;&#21046;&#12299;
&lt;/p&gt;
&lt;p&gt;
Curriculum Learning and Imitation Learning for Model-free Control on Financial Time-series. (arXiv:2311.13326v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.13326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22312;&#22797;&#26434;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#19978;&#25506;&#32034;&#35838;&#31243;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#35838;&#31243;&#23398;&#20064;&#26159;&#25913;&#21892;&#22797;&#26434;&#26102;&#38388;&#24207;&#21015;&#25511;&#21046;&#20219;&#21153;&#24615;&#33021;&#30340;&#26032;&#36884;&#24452;&#65292;&#32780;&#27169;&#20223;&#23398;&#20064;&#20063;&#24212;&#35813;&#34987;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35838;&#31243;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#22312;&#26426;&#22120;&#20154;&#39046;&#22495;&#24050;&#34987;&#24191;&#27867;&#36816;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#39640;&#24230;&#38543;&#26426;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#19978;&#21033;&#29992;&#36825;&#20123;&#24819;&#27861;&#36827;&#34892;&#25511;&#21046;&#20219;&#21153;&#30340;&#30740;&#31350;&#38750;&#24120;&#26377;&#38480;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20174;&#29702;&#35770;&#21644;&#23454;&#35777;&#20004;&#20010;&#26041;&#38754;&#25506;&#35752;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#22797;&#26434;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#19978;&#30340;&#20195;&#34920;&#24615;&#25511;&#21046;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#23454;&#29616;&#20102;&#35838;&#31243;&#23398;&#20064;&#30340;&#22522;&#26412;&#24605;&#24819;&#65292;&#32780;&#36890;&#36807;&#27169;&#20223;&#23398;&#20064;&#20174;&#19987;&#23478;&#20013;&#33976;&#39311;&#20986;&#31574;&#30053;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35838;&#31243;&#23398;&#20064;&#22312;&#25913;&#36827;&#22797;&#26434;&#26102;&#38388;&#24207;&#21015;&#25511;&#21046;&#30340;&#20219;&#21153;&#24615;&#33021;&#26041;&#38754;&#24212;&#34987;&#35270;&#20026;&#19968;&#31181;&#26032;&#30340;&#26041;&#21521;&#12290;&#25105;&#20204;&#30340;&#22823;&#37327;&#38543;&#26426;&#31181;&#23376;&#22806;&#26679;&#26412;&#23454;&#35777;&#21644;&#28040;&#34701;&#30740;&#31350;&#23545;&#20110;&#26102;&#38388;&#24207;&#21015;&#25511;&#21046;&#30340;&#35838;&#31243;&#23398;&#20064;&#38750;&#24120;&#40723;&#33310;&#20154;&#24515;&#12290;&#36825;&#20123;&#21457;&#29616;&#23588;&#20854;&#40723;&#33310;&#20154;&#24515;&#65292;&#22240;&#20026;&#25105;&#20204;&#22312;&#22522;&#32447;&#19978;&#35843;&#25972;&#20102;&#25152;&#26377;&#37325;&#21472;&#30340;&#36229;&#21442;&#25968;&#65292;&#32473;&#20986;&#20102;&#22522;&#32447;&#30340;&#20248;&#21183;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#21457;&#29616;&#27169;&#20223;&#23398;&#20064;&#24212;&#35813;&#34987;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Curriculum learning and imitation learning have been leveraged extensively in the robotics domain. However, minimal research has been done on leveraging these ideas on control tasks over highly stochastic time-series data. Here, we theoretically and empirically explore these approaches in a representative control task over complex time-series data. We implement the fundamental ideas of curriculum learning via data augmentation, while imitation learning is implemented via policy distillation from an oracle. Our findings reveal that curriculum learning should be considered a novel direction in improving control-task performance over complex time-series. Our ample random-seed out-sample empirics and ablation studies are highly encouraging for curriculum learning for time-series control. These findings are especially encouraging as we tune all overlapping hyperparameters on the baseline -- giving an advantage to the baseline. On the other hand, we find that imitation learning should be use
&lt;/p&gt;</description></item><item><title>&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#22411;&#32508;&#21512;&#20010;&#24615;&#21270;&#30340;&#30196;&#21574;&#39118;&#38505;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#25972;&#21512;&#26426;&#22120;&#23398;&#20064;&#21644;&#32593;&#32476;&#31995;&#32479;&#29983;&#29289;&#23398;&#26041;&#27861;&#65292;&#25104;&#21151;&#24314;&#27169;&#20102;&#21464;&#37327;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2311.09229</link><description>&lt;p&gt;
&#36890;&#36807;&#25972;&#21512;&#26426;&#22120;&#23398;&#20064;&#21644;&#32593;&#32476;&#31995;&#32479;&#29983;&#29289;&#23398;&#26041;&#27861;&#24320;&#21457;&#19968;&#31181;&#26032;&#22411;&#30340;&#12289;&#20010;&#24615;&#21270;&#30340;&#20840;&#38754;&#30196;&#21574;&#39118;&#38505;&#39044;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Developing a Novel Holistic, Personalized Dementia Risk Prediction Model via Integration of Machine Learning and Network Systems Biology Approaches. (arXiv:2311.09229v2 [q-bio.NC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.09229
&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#22411;&#32508;&#21512;&#20010;&#24615;&#21270;&#30340;&#30196;&#21574;&#39118;&#38505;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#25972;&#21512;&#26426;&#22120;&#23398;&#20064;&#21644;&#32593;&#32476;&#31995;&#32479;&#29983;&#29289;&#23398;&#26041;&#27861;&#65292;&#25104;&#21151;&#24314;&#27169;&#20102;&#21464;&#37327;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20840;&#29699;&#39044;&#26399;&#23551;&#21629;&#30340;&#25552;&#39640;&#21644;&#20154;&#21475;&#32769;&#40836;&#21270;&#65292;&#30196;&#21574;&#30340;&#24739;&#30149;&#29575;&#36880;&#28176;&#22686;&#21152;&#12290;&#20010;&#20307;&#21457;&#23637;&#30196;&#21574;&#30340;&#39118;&#38505;&#21463;&#21040;&#21508;&#31181;&#36951;&#20256;&#12289;&#29983;&#27963;&#26041;&#24335;&#21644;&#29615;&#22659;&#22240;&#32032;&#31561;&#30340;&#24433;&#21709;&#12290;&#39044;&#27979;&#30196;&#21574;&#39118;&#38505;&#21487;&#20197;&#20351;&#20010;&#20307;&#37319;&#21462;&#32531;&#35299;&#31574;&#30053;&#25110;&#25913;&#21464;&#29983;&#27963;&#26041;&#24335;&#26469;&#24310;&#32531;&#30196;&#21574;&#21457;&#20316;&#12290;&#24403;&#21069;&#23545;&#30196;&#21574;&#39044;&#27979;&#30340;&#35745;&#31639;&#26041;&#27861;&#21482;&#36820;&#22238;&#29421;&#31364;&#31867;&#21035;&#30340;&#21464;&#37327;&#39118;&#38505;&#65292;&#24182;&#27809;&#26377;&#32771;&#34385;&#19981;&#21516;&#39118;&#38505;&#21464;&#37327;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#37319;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#20840;&#38754;&#30196;&#21574;&#39118;&#38505;&#39044;&#27979;&#26041;&#27861;&#65292;&#24182;&#39318;&#27425;&#23558;&#21508;&#31181;&#34920;&#26684;&#29615;&#22659;&#27745;&#26579;&#21644;&#29983;&#27963;&#26041;&#24335;&#22240;&#32032;&#25968;&#25454;&#19982;&#22522;&#20110;&#32593;&#32476;&#31995;&#32479;&#29983;&#29289;&#23398;&#30340;&#36951;&#20256;&#25968;&#25454;&#30456;&#32467;&#21512;&#12290;&#37319;&#29992;LightGBM&#26799;&#24230;&#25552;&#21319;&#26041;&#27861;&#30830;&#20445;&#25152;&#21253;&#21547;&#22240;&#32032;&#30340;&#26377;&#25928;&#24615;&#12290;&#35813;&#26041;&#27861;&#25104;&#21151;&#22320;&#36890;&#36807;&#19968;&#31181;&#34987;&#31216;&#20026;Sysable&#30340;&#21407;&#22987;&#21152;&#26435;&#25972;&#21512;&#26041;&#27861;&#24314;&#27169;&#20102;&#21464;&#37327;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The prevalence of dementia has increased over time as global life expectancy improves and populations age. An individual's risk of developing dementia is influenced by various genetic, lifestyle, and environmental factors, among others. Predicting dementia risk may enable individuals to employ mitigation strategies or lifestyle changes to delay dementia onset. Current computational approaches to dementia prediction only return risk upon narrow categories of variables and do not account for interactions between different risk variables. The proposed framework utilizes a novel holistic approach to dementia risk prediction and is the first to incorporate various sources of tabular environmental pollution and lifestyle factor data with network systems biology-based genetic data. LightGBM gradient boosting was employed to ensure validity of included factors. This approach successfully models interactions between variables through an original weighted integration method coined Sysable. Multi
&lt;/p&gt;</description></item><item><title>CausalCite&#26159;&#19968;&#31181;&#20197;&#22240;&#26524;&#25512;&#26029;&#20026;&#22522;&#30784;&#30340;&#35770;&#25991;&#24341;&#29992;&#20844;&#24335;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25991;&#26412;&#36827;&#34892;&#23884;&#20837;&#21644;&#30456;&#20284;&#26679;&#26412;&#30340;&#25552;&#21462;&#26469;&#35780;&#20272;&#35770;&#25991;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#22312;&#21508;&#20010;&#26631;&#20934;&#19978;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.02790</link><description>&lt;p&gt;
CausalCite&#65306;&#19968;&#31181;&#35770;&#25991;&#24341;&#29992;&#30340;&#22240;&#26524;&#20844;&#24335;&#21270;
&lt;/p&gt;
&lt;p&gt;
CausalCite: A Causal Formulation of Paper Citations. (arXiv:2311.02790v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.02790
&lt;/p&gt;
&lt;p&gt;
CausalCite&#26159;&#19968;&#31181;&#20197;&#22240;&#26524;&#25512;&#26029;&#20026;&#22522;&#30784;&#30340;&#35770;&#25991;&#24341;&#29992;&#20844;&#24335;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25991;&#26412;&#36827;&#34892;&#23884;&#20837;&#21644;&#30456;&#20284;&#26679;&#26412;&#30340;&#25552;&#21462;&#26469;&#35780;&#20272;&#35770;&#25991;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#22312;&#21508;&#20010;&#26631;&#20934;&#19978;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#31185;&#23398;&#30028;&#26469;&#35828;&#65292;&#35780;&#20272;&#19968;&#31687;&#35770;&#25991;&#30340;&#37325;&#35201;&#24615;&#33267;&#20851;&#37325;&#35201;&#20294;&#20063;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#23613;&#31649;&#24341;&#29992;&#27425;&#25968;&#26159;&#26368;&#24120;&#29992;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#20294;&#23427;&#20204;&#34987;&#24191;&#27867;&#25209;&#35780;&#20026;&#26080;&#27861;&#20934;&#30830;&#21453;&#26144;&#19968;&#31687;&#35770;&#25991;&#30340;&#30495;&#27491;&#24433;&#21709;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#65292;&#31216;&#20026;TextMatch&#65292;&#23427;&#23558;&#20256;&#32479;&#30340;&#21305;&#37197;&#26694;&#26550;&#36866;&#24212;&#20110;&#39640;&#32500;&#25991;&#26412;&#23884;&#20837;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23545;&#27599;&#31687;&#35770;&#25991;&#36827;&#34892;&#25991;&#26412;&#23884;&#20837;&#65292;&#36890;&#36807;&#20313;&#24358;&#30456;&#20284;&#24615;&#25552;&#21462;&#30456;&#20284;&#26679;&#26412;&#65292;&#24182;&#26681;&#25454;&#30456;&#20284;&#24230;&#20540;&#30340;&#21152;&#26435;&#24179;&#22343;&#21512;&#25104;&#19968;&#20010;&#21453;&#20107;&#23454;&#26679;&#26412;&#12290;&#25105;&#20204;&#23558;&#24471;&#21040;&#30340;&#25351;&#26631;&#31216;&#20026;CausalCite&#65292;&#20316;&#20026;&#35770;&#25991;&#24341;&#29992;&#30340;&#22240;&#26524;&#20844;&#24335;&#21270;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#22312;&#21508;&#31181;&#26631;&#20934;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#22914;&#19982;&#31185;&#23398;&#19987;&#23478;&#23545;1K&#31687;&#35770;&#25991;&#30340;&#25253;&#21578;&#30340;&#35770;&#25991;&#24433;&#21709;&#21147;&#30340;&#39640;&#30456;&#20851;&#24615;&#65292;&#36807;&#21435;&#35770;&#25991;&#30340;&#65288;&#32463;&#36807;&#26102;&#38388;&#32771;&#39564;&#30340;&#65289;&#22870;&#39033;&#65292;&#20197;&#21450;&#22312;&#21508;&#20010;&#23376;&#39046;&#22495;&#30340;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating the significance of a paper is pivotal yet challenging for the scientific community. While the citation count is the most commonly used proxy for this purpose, they are widely criticized for failing to accurately reflect a paper's true impact. In this work, we propose a causal inference method, TextMatch, which adapts the traditional matching framework to high-dimensional text embeddings. Specifically, we encode each paper using the text embeddings by large language models (LLMs), extract similar samples by cosine similarity, and synthesize a counterfactual sample by the weighted average of similar papers according to their similarity values. We apply the resulting metric, called CausalCite, as a causal formulation of paper citations. We show its effectiveness on various criteria, such as high correlation with paper impact as reported by scientific experts on a previous dataset of 1K papers, (test-of-time) awards for past papers, and its stability across various sub-fields o
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#20811;&#32599;&#20869;&#20811;&#36817;&#20284;&#26354;&#29575;&#31639;&#27861;&#65292;&#21487;&#20197;&#21152;&#36895;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#21644;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#12290;&#20316;&#32773;&#21457;&#29616;&#20102;&#20004;&#31181;&#20855;&#26377;&#32447;&#24615;&#26435;&#37325;&#20849;&#20139;&#23618;&#19981;&#21516;&#35774;&#32622;&#65292;&#24182;&#35777;&#26126;&#20102;&#30456;&#24212;&#35774;&#32622;&#19979;&#30340;K-FAC&#31639;&#27861;&#30340;&#31934;&#30830;&#24615;&#12290;K-FAC-reduce&#36890;&#24120;&#27604;K-FAC-expand&#26356;&#24555;&#65292;&#21487;&#20197;&#29992;&#20110;&#21152;&#36895;&#33258;&#21160;&#36229;&#21442;&#25968;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2311.00636</link><description>&lt;p&gt;
Kronecker-Factored Approximate Curvature for Modern Neural Network Architectures&#65288;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#20811;&#32599;&#20869;&#20811;&#36817;&#20284;&#26354;&#29575;&#65289;
&lt;/p&gt;
&lt;p&gt;
Kronecker-Factored Approximate Curvature for Modern Neural Network Architectures. (arXiv:2311.00636v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00636
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#20811;&#32599;&#20869;&#20811;&#36817;&#20284;&#26354;&#29575;&#31639;&#27861;&#65292;&#21487;&#20197;&#21152;&#36895;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#21644;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#12290;&#20316;&#32773;&#21457;&#29616;&#20102;&#20004;&#31181;&#20855;&#26377;&#32447;&#24615;&#26435;&#37325;&#20849;&#20139;&#23618;&#19981;&#21516;&#35774;&#32622;&#65292;&#24182;&#35777;&#26126;&#20102;&#30456;&#24212;&#35774;&#32622;&#19979;&#30340;K-FAC&#31639;&#27861;&#30340;&#31934;&#30830;&#24615;&#12290;K-FAC-reduce&#36890;&#24120;&#27604;K-FAC-expand&#26356;&#24555;&#65292;&#21487;&#20197;&#29992;&#20110;&#21152;&#36895;&#33258;&#21160;&#36229;&#21442;&#25968;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#26680;&#24515;&#32452;&#20214;&#65292;&#22914;transformers&#12289;&#21367;&#31215;&#25110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#34920;&#36798;&#20026;&#20855;&#26377;&#8220;&#26435;&#37325;&#20849;&#20139;&#8221;&#30340;&#32447;&#24615;&#23618;&#12290;&#20811;&#32599;&#20869;&#20811;&#36817;&#20284;&#26354;&#29575;&#65288;K-FAC&#65289;&#26159;&#19968;&#31181;&#20108;&#38454;&#20248;&#21270;&#26041;&#27861;&#65292;&#24050;&#26174;&#31034;&#20986;&#21152;&#36895;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#24182;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#23558;&#20854;&#24212;&#29992;&#20110;&#36890;&#29992;&#30340;&#26550;&#26500;&#30340;&#26694;&#26550;&#65292;&#29305;&#21035;&#26159;&#20855;&#26377;&#32447;&#24615;&#26435;&#37325;&#20849;&#20139;&#23618;&#30340;&#26550;&#26500;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#20855;&#26377;&#32447;&#24615;&#26435;&#37325;&#20849;&#20139;&#23618;&#30340;&#20004;&#31181;&#19981;&#21516;&#35774;&#32622;&#65292;&#36825;&#20419;&#20351;&#20102;&#20004;&#31181;K-FAC&#30340;&#21464;&#20307;&#8212;&#8212;&#8220;&#25193;&#23637;&#8221;&#21644;&#8220;&#20943;&#23569;&#8221;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#20110;&#20855;&#26377;&#30456;&#24212;&#35774;&#32622;&#30340;&#28145;&#24230;&#32447;&#24615;&#32593;&#32476;&#65292;&#23427;&#20204;&#26159;&#31934;&#30830;&#30340;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;K-FAC-reduce&#36890;&#24120;&#27604;K-FAC-expand&#26356;&#24555;&#65292;&#25105;&#20204;&#21033;&#29992;&#23427;&#26469;&#21152;&#36895;&#36890;&#36807;&#20248;&#21270;Wide ResNet&#30340;&#36793;&#38469;&#20284;&#28982;&#26469;&#36873;&#25321;&#33258;&#21160;&#36229;&#21442;&#25968;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;
&lt;/p&gt;
&lt;p&gt;
The core components of many modern neural network architectures, such as transformers, convolutional, or graph neural networks, can be expressed as linear layers with $\textit{weight-sharing}$. Kronecker-Factored Approximate Curvature (K-FAC), a second-order optimisation method, has shown promise to speed up neural network training and thereby reduce computational costs. However, there is currently no framework to apply it to generic architectures, specifically ones with linear weight-sharing layers. In this work, we identify two different settings of linear weight-sharing layers which motivate two flavours of K-FAC -- $\textit{expand}$ and $\textit{reduce}$. We show that they are exact for deep linear networks with weight-sharing in their respective setting. Notably, K-FAC-reduce is generally faster than K-FAC-expand, which we leverage to speed up automatic hyperparameter selection via optimising the marginal likelihood for a Wide ResNet. Finally, we observe little difference between 
&lt;/p&gt;</description></item><item><title>Laplacian&#35268;&#33539;&#21270;&#26159;&#19968;&#31181;&#35299;&#20915;&#35889;&#23884;&#20837;&#20013;&#31526;&#21495;&#21644;&#22522;&#24213;&#19981;&#21464;&#24615;&#38382;&#39064;&#30340;&#36731;&#37327;&#32423;&#39044;&#22788;&#29702;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#22312;&#22270;&#25968;&#25454;&#19978;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.18716</link><description>&lt;p&gt;
Laplacian&#35268;&#33539;&#21270;&#65306;&#19968;&#31181;&#23545;&#31216;&#21644;&#22522;&#24213;&#19981;&#21464;&#35889;&#23884;&#20837;&#30340;&#26497;&#31616;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Laplacian Canonization: A Minimalist Approach to Sign and Basis Invariant Spectral Embedding. (arXiv:2310.18716v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18716
&lt;/p&gt;
&lt;p&gt;
Laplacian&#35268;&#33539;&#21270;&#26159;&#19968;&#31181;&#35299;&#20915;&#35889;&#23884;&#20837;&#20013;&#31526;&#21495;&#21644;&#22522;&#24213;&#19981;&#21464;&#24615;&#38382;&#39064;&#30340;&#36731;&#37327;&#32423;&#39044;&#22788;&#29702;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#22312;&#22270;&#25968;&#25454;&#19978;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35889;&#23884;&#20837;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#22270;&#23884;&#20837;&#25216;&#26415;&#65292;&#30001;&#20110;&#20854;&#23545;&#22270;&#36716;&#25442;&#22120;&#30340;&#26377;&#25928;&#24615;&#65292;&#26368;&#36817;&#21463;&#21040;&#20102;&#24456;&#22810;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#20174;&#29702;&#35770;&#35282;&#24230;&#26469;&#30475;&#65292;&#35889;&#23884;&#20837;&#30340;&#26222;&#36941;&#34920;&#36798;&#33021;&#21147;&#36896;&#25104;&#20102;&#22270;&#30340;&#20004;&#20010;&#37325;&#35201;&#19981;&#21464;&#24615;&#23646;&#24615;&#65288;&#31526;&#21495;&#21644;&#22522;&#24213;&#19981;&#21464;&#24615;&#65289;&#30340;&#20007;&#22833;&#65292;&#36825;&#20063;&#38480;&#21046;&#20102;&#20854;&#22312;&#22270;&#25968;&#25454;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20043;&#21069;&#30340;&#35768;&#22810;&#26041;&#27861;&#24320;&#21457;&#20102;&#26114;&#36149;&#30340;&#23398;&#20064;&#26032;&#19981;&#21464;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#35745;&#31639;&#22797;&#26434;&#24230;&#24456;&#39640;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#26368;&#23567;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#30452;&#25509;&#25214;&#21040;&#29305;&#24449;&#21521;&#37327;&#30340;&#35268;&#33539;&#26041;&#21521;&#26469;&#35299;&#20915;&#27169;&#31946;&#24615;&#38382;&#39064;&#65292;&#21629;&#21517;&#20026;Laplacian&#35268;&#33539;&#21270;&#65288;LC&#65289;&#12290;&#20316;&#20026;&#19968;&#31181;&#32431;&#39044;&#22788;&#29702;&#26041;&#27861;&#65292;LC&#20855;&#26377;&#36731;&#37327;&#32423;&#21644;&#21487;&#24212;&#29992;&#20110;&#20219;&#20309;&#29616;&#26377;GNN&#30340;&#29305;&#28857;&#12290;&#25105;&#20204;&#23545;&#36825;&#31181;&#26041;&#27861;&#36827;&#34892;&#20102;&#20174;&#29702;&#35770;&#21040;&#31639;&#27861;&#30340;&#24443;&#24213;&#30740;&#31350;&#65292;&#24182;&#21457;&#29616;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#21517;&#20026;&#26368;&#22823;&#36724;&#25237;&#24433;&#65288;MAP&#65289;&#65292;&#36866;&#29992;&#20110;&#31526;&#21495;&#21644;&#22522;&#24213;&#20004;&#31181;&#19981;&#21464;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spectral embedding is a powerful graph embedding technique that has received a lot of attention recently due to its effectiveness on Graph Transformers. However, from a theoretical perspective, the universal expressive power of spectral embedding comes at the price of losing two important invariance properties of graphs, sign and basis invariance, which also limits its effectiveness on graph data. To remedy this issue, many previous methods developed costly approaches to learn new invariants and suffer from high computation complexity. In this work, we explore a minimal approach that resolves the ambiguity issues by directly finding canonical directions for the eigenvectors, named Laplacian Canonization (LC). As a pure pre-processing method, LC is light-weighted and can be applied to any existing GNNs. We provide a thorough investigation, from theory to algorithm, on this approach, and discover an efficient algorithm named Maximal Axis Projection (MAP) that works for both sign and basi
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20998;&#26512;&#20102;&#19981;&#21516;&#30340;&#27169;&#22359;&#21270;&#26368;&#22823;&#21270;&#31639;&#27861;&#23545;&#20110;&#31038;&#21306;&#26816;&#27979;&#20013;&#23454;&#29616;&#26368;&#20248;&#21010;&#20998;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#26368;&#24120;&#29992;&#30340;&#27169;&#22359;&#21270;&#26041;&#27861;&#19982;&#26368;&#20248;&#21010;&#20998;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2310.10898</link><description>&lt;p&gt;
&#23545;&#20110;&#31038;&#21306;&#26816;&#27979;&#20013;&#27169;&#22359;&#21270;&#26368;&#22823;&#21270;&#22312;&#36817;&#20284;&#12289;&#21551;&#21457;&#24335;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#31639;&#27861;&#20013;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Analyzing Modularity Maximization in Approximation, Heuristic, and Graph Neural Network Algorithms for Community Detection. (arXiv:2310.10898v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10898
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20998;&#26512;&#20102;&#19981;&#21516;&#30340;&#27169;&#22359;&#21270;&#26368;&#22823;&#21270;&#31639;&#27861;&#23545;&#20110;&#31038;&#21306;&#26816;&#27979;&#20013;&#23454;&#29616;&#26368;&#20248;&#21010;&#20998;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#26368;&#24120;&#29992;&#30340;&#27169;&#22359;&#21270;&#26041;&#27861;&#19982;&#26368;&#20248;&#21010;&#20998;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#21306;&#26816;&#27979;&#26159;&#35745;&#31639;&#31185;&#23398;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#22312;&#21508;&#20010;&#39046;&#22495;&#37117;&#26377;&#24212;&#29992;&#12290;&#36890;&#24120;&#36890;&#36807;&#26368;&#22823;&#21270;&#19968;&#20010;&#30446;&#26631;&#20989;&#25968;&#65288;&#27169;&#22359;&#24615;&#65289;&#22312;&#32593;&#32476;&#33410;&#28857;&#30340;&#21010;&#20998;&#19978;&#26469;&#26816;&#27979;&#31038;&#21306;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#19981;&#21516;&#27169;&#22359;&#21270;&#26368;&#22823;&#21270;&#31639;&#27861;&#22312;&#23454;&#29616;&#26368;&#20248;&#21010;&#20998;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;104&#20010;&#32593;&#32476;&#65292;&#21253;&#25324;&#26469;&#33258;&#19981;&#21516;&#32972;&#26223;&#30340;&#30495;&#23454;&#19990;&#30028;&#23454;&#20363;&#21644;&#20855;&#26377;&#27169;&#22359;&#21270;&#32467;&#26500;&#30340;&#21512;&#25104;&#22270;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#21313;&#31181;&#36817;&#20284;&#27169;&#22359;&#21270;&#31639;&#27861;&#65292;&#23545;&#27604;&#20102;&#19968;&#31181;&#31934;&#30830;&#30340;&#22522;&#20934;&#32447;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26159;&#19968;&#31181;&#31934;&#30830;&#30340;&#25972;&#25968;&#35268;&#21010;&#26041;&#27861;&#65292;&#21487;&#20197;&#20840;&#23616;&#20248;&#21270;&#27169;&#22359;&#24615;&#12290;&#20998;&#26512;&#30340;&#21313;&#31181;&#31639;&#27861;&#21253;&#25324;&#20843;&#20010;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#20004;&#31181;&#22270;&#31070;&#32463;&#32593;&#32476;&#31639;&#27861;&#30340;&#21464;&#31181;&#65292;&#20197;&#21450;&#20960;&#31181;Bayan&#36817;&#20284;&#31639;&#27861;&#30340;&#21464;&#31181;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#26368;&#24120;&#29992;&#30340;&#27169;&#22359;&#21270;&#26041;&#27861;&#24471;&#21040;&#30340;&#21010;&#20998;&#19982;&#32593;&#32476;&#30340;&#26368;&#20248;&#21010;&#20998;&#20043;&#38388;&#30340;&#26174;&#33879;&#24046;&#24322;&#65292;&#22914;&#19979;&#25152;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Community detection, a fundamental problem in computational sciences, finds applications in various domains. Heuristics are often employed to detect communities through maximizing an objective function, modularity, over partitions of network nodes. Our research delves into the performance of different modularity maximization algorithms in achieving optimal partitions. We use 104 networks, comprising real-world instances from diverse contexts and synthetic graphs with modular structures. We analyze ten inexact modularity-based algorithms against an exact baseline which is an exact integer programming method that globally optimizes modularity. The ten algorithms analyzed include eight heuristics, two variations of a graph neural network algorithm, and several variations of the Bayan approximation algorithm. Our analysis uncovers substantial dissimilarities between the partitions obtained by most commonly used modularity-based methods and any optimal partition of the networks, as indicate
&lt;/p&gt;</description></item><item><title>CodeFuse-13B&#26159;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#20195;&#30721;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#19987;&#20026;&#20195;&#30721;&#30456;&#20851;&#20219;&#21153;&#35774;&#35745;&#65292;&#25903;&#25345;&#36229;&#36807;40&#31181;&#32534;&#31243;&#35821;&#35328;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#39640;&#36136;&#37327;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20197;&#21450;&#22823;&#37327;&#23454;&#39564;&#30340;&#39564;&#35777;&#65292;&#23637;&#29616;&#20102;&#20854;&#22312;&#22810;&#35821;&#35328;&#36755;&#20837;&#19979;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.06266</link><description>&lt;p&gt;
CodeFuse-13B: &#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#20195;&#30721;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CodeFuse-13B: A Pretrained Multi-lingual Code Large Language Model. (arXiv:2310.06266v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06266
&lt;/p&gt;
&lt;p&gt;
CodeFuse-13B&#26159;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#20195;&#30721;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#19987;&#20026;&#20195;&#30721;&#30456;&#20851;&#20219;&#21153;&#35774;&#35745;&#65292;&#25903;&#25345;&#36229;&#36807;40&#31181;&#32534;&#31243;&#35821;&#35328;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#39640;&#36136;&#37327;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20197;&#21450;&#22823;&#37327;&#23454;&#39564;&#30340;&#39564;&#35777;&#65292;&#23637;&#29616;&#20102;&#20854;&#22312;&#22810;&#35821;&#35328;&#36755;&#20837;&#19979;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#30721;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(Code LLMs)&#22240;&#20854;&#22312;&#36719;&#20214;&#24037;&#31243;&#20840;&#29983;&#21629;&#21608;&#26399;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#32780;&#21463;&#21040;&#24037;&#19994;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#27169;&#22411;&#22312;&#29702;&#35299;&#38750;&#33521;&#35821;&#36755;&#20837;&#30340;&#22810;&#35821;&#35328;&#20195;&#30721;&#30456;&#20851;&#20219;&#21153;&#26041;&#38754;&#30340;&#25928;&#26524;&#20173;&#28982;&#36828;&#26410;&#34987;&#20805;&#20998;&#30740;&#31350;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;CodeFuse-13B&#65292;&#19968;&#20010;&#24320;&#28304;&#30340;&#39044;&#35757;&#32451;&#20195;&#30721;LLM&#12290;&#23427;&#19987;&#20026;&#21253;&#21547;&#33521;&#25991;&#21644;&#20013;&#25991;&#25552;&#31034;&#30340;&#20195;&#30721;&#30456;&#20851;&#20219;&#21153;&#32780;&#35774;&#35745;&#65292;&#24182;&#25903;&#25345;&#36229;&#36807;40&#31181;&#32534;&#31243;&#35821;&#35328;&#12290;CodeFuse&#36890;&#36807;&#21033;&#29992;&#30001;&#31243;&#24207;&#20998;&#26512;&#22120;&#31934;&#24515;&#31579;&#36873;&#24182;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20248;&#21270;&#30340;&#39640;&#36136;&#37327;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#26469;&#23454;&#29616;&#20854;&#25928;&#26524;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#21253;&#25324;&#20351;&#29992;&#30495;&#23454;&#19990;&#30028;&#30340;&#20351;&#29992;&#22330;&#26223;&#12289;&#24037;&#19994;&#26631;&#20934;&#22522;&#20934;HumanEval-x&#65292;&#20197;&#21450;&#19987;&#20026;&#20013;&#25991;&#25552;&#31034;&#35774;&#35745;&#30340;CodeFuseEval&#12290;&#20026;&#20102;&#35780;&#20272;CodeFuse&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#31215;&#26497;&#25910;&#38598;&#20102;AntGroup&#36719;&#20214;&#24320;&#21457;&#22242;&#38431;&#30340;&#23453;&#36149;&#20154;&#24037;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Code Large Language Models (Code LLMs) have gained significant attention in the industry due to their wide applications in the full lifecycle of software engineering. However, the effectiveness of existing models in understanding non-English inputs for multi-lingual code-related tasks is still far from well studied. This paper introduces CodeFuse-13B, an open-sourced pre-trained code LLM. It is specifically designed for code-related tasks with both English and Chinese prompts and supports over 40 programming languages. CodeFuse achieves its effectiveness by utilizing a high quality pre-training dataset that is carefully filtered by program analyzers and optimized during the training process. Extensive experiments are conducted using real-world usage scenarios, the industry-standard benchmark HumanEval-x, and the specially designed CodeFuseEval for Chinese prompts. To assess the effectiveness of CodeFuse, we actively collected valuable human feedback from the AntGroup's software develop
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#34701;&#21512;&#30495;&#23454;&#25968;&#25454;&#21644;&#38544;&#24615;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#20135;&#29983;&#30340;&#22686;&#24191;&#25968;&#25454;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#25552;&#39640;&#23545;&#26089;&#26399;&#30315;&#30187;&#21457;&#20316;&#20449;&#21495;&#30340;&#39044;&#27979;&#20934;&#30830;&#29575;&#65292;&#24182;&#36890;&#36807;&#25552;&#21462;&#30340;&#20020;&#30028;&#21160;&#21147;&#23398;&#29305;&#24449;&#26469;&#26631;&#35760;&#22122;&#22768;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2310.06059</link><description>&lt;p&gt;
&#39044;&#35686;&#21644;&#38544;&#21464;&#21270;&#38543;&#26426;&#21160;&#21147;&#23398;&#31995;&#32479;&#21450;&#20803;&#26631;&#31614;&#32416;&#27491;&#30340;&#26089;&#26399;&#39044;&#35686;
&lt;/p&gt;
&lt;p&gt;
Early Warning via tipping-preserving latent stochastic dynamical system and meta label correcting. (arXiv:2310.06059v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06059
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#34701;&#21512;&#30495;&#23454;&#25968;&#25454;&#21644;&#38544;&#24615;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#20135;&#29983;&#30340;&#22686;&#24191;&#25968;&#25454;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#25552;&#39640;&#23545;&#26089;&#26399;&#30315;&#30187;&#21457;&#20316;&#20449;&#21495;&#30340;&#39044;&#27979;&#20934;&#30830;&#29575;&#65292;&#24182;&#36890;&#36807;&#25552;&#21462;&#30340;&#20020;&#30028;&#21160;&#21147;&#23398;&#29305;&#24449;&#26469;&#26631;&#35760;&#22122;&#22768;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#30315;&#30187;&#24739;&#32773;&#36827;&#34892;&#26089;&#26399;&#39044;&#35686;&#23545;&#20110;&#20182;&#20204;&#30340;&#23433;&#20840;&#21644;&#31119;&#31049;&#33267;&#20851;&#37325;&#35201;&#65292;&#21487;&#20197;&#39044;&#38450;&#25110;&#20943;&#23569;&#30315;&#30187;&#21457;&#20316;&#30340;&#20005;&#37325;&#31243;&#24230;&#12290;&#36890;&#36807;&#24739;&#32773;&#30340;&#33041;&#30005;&#22270;&#25968;&#25454;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#25913;&#36827;&#23545;&#26089;&#26399;&#30315;&#30187;&#21457;&#20316;&#20449;&#21495;&#30340;&#39044;&#27979;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#21033;&#29992;&#20803;&#26631;&#31614;&#32416;&#27491;&#26041;&#27861;&#65292;&#25105;&#20204;&#34701;&#21512;&#20102;&#30495;&#23454;&#25968;&#25454;&#21644;&#38544;&#24615;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDE&#65289;&#20135;&#29983;&#30340;&#22686;&#24191;&#25968;&#25454;&#30340;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#30495;&#23454;&#25968;&#25454;&#21644;&#38544;&#24615;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#36807;&#28193;&#26102;&#38388;&#20998;&#24067;&#26469;&#20248;&#36873;&#36873;&#25321;&#28508;&#22312;&#30340;&#21160;&#21147;&#31995;&#32479;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25552;&#21462;&#30340;&#20020;&#30028;&#21160;&#21147;&#23398;&#29305;&#24449;&#20063;&#34987;&#38598;&#25104;&#21040;&#20803;&#32593;&#32476;&#20013;&#65292;&#20197;&#26356;&#22909;&#22320;&#26631;&#35760;&#22122;&#22768;&#25968;&#25454;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#23558;LSTM&#23454;&#26045;&#20026;&#22522;&#20934;&#27169;&#22411;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#26469;&#39044;&#27979;&#21508;&#31181;&#38271;&#26399;&#31383;&#21475;&#65288;1-2&#31186;&#30340;&#36755;&#20837;&#25968;&#25454;&#65289;&#20869;&#30340;&#30315;&#30187;&#21457;&#20316;&#65292;&#24182;&#21457;&#29616;&#39044;&#27979;&#20934;&#30830;&#29575;&#20986;&#29616;&#20102;&#20196;&#20154;&#24778;&#35766;&#30340;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
Early warning for epilepsy patients is crucial for their safety and well-being, in terms of preventing or minimizing the severity of seizures. Through the patients' EEG data, we propose a meta learning framework for improving prediction on early ictal signals. To better utilize the meta label corrector method, we fuse the information from both the real data and the augmented data from the latent Stochastic differential equation(SDE). Besides, we also optimally select the latent dynamical system via distribution of transition time between real data and that from the latent SDE. In this way, the extracted tipping dynamical feature is also integrated into the meta network to better label the noisy data. To validate our method, LSTM is implemented as the baseline model. We conduct a series of experiments to predict seizure in various long-term window from 1-2 seconds input data and find surprisingly increment of prediction accuracy.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20284;&#28982;&#30340;&#20256;&#24863;&#22120;&#26657;&#20934;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#29289;&#32852;&#32593;&#31995;&#32479;&#20013;&#23454;&#29616;&#19987;&#23478;&#25903;&#25345;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#31639;&#27861;&#12290;&#36890;&#36807;&#23545;&#27169;&#25311;&#21644;&#23454;&#38469;&#27979;&#37327;&#25968;&#25454;&#30340;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#25913;&#36827;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.11526</link><description>&lt;p&gt;
&#22522;&#20110;&#20284;&#28982;&#30340;&#29289;&#32852;&#32593;&#31995;&#32479;&#20013;&#19987;&#23478;&#25903;&#25345;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#31639;&#27861;&#20013;&#20256;&#24863;&#22120;&#26657;&#20934;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Likelihood-based Sensor Calibration for Expert-Supported Distributed Learning Algorithms in IoT Systems. (arXiv:2309.11526v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11526
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20284;&#28982;&#30340;&#20256;&#24863;&#22120;&#26657;&#20934;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#29289;&#32852;&#32593;&#31995;&#32479;&#20013;&#23454;&#29616;&#19987;&#23478;&#25903;&#25345;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#31639;&#27861;&#12290;&#36890;&#36807;&#23545;&#27169;&#25311;&#21644;&#23454;&#38469;&#27979;&#37327;&#25968;&#25454;&#30340;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#25913;&#36827;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#24863;&#22120;&#25216;&#26415;&#39046;&#22495;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#20219;&#21153;&#26159;&#23558;&#19968;&#20010;&#20256;&#24863;&#22120;&#30340;&#27979;&#37327;&#32467;&#26524;&#39640;&#25928;&#22320;&#36866;&#24212;&#21040;&#21478;&#19968;&#20010;&#20855;&#26377;&#30456;&#21516;&#35774;&#35745;&#30340;&#20256;&#24863;&#22120;&#12290;&#19968;&#31181;&#24819;&#27861;&#26159;&#20351;&#29992;&#19981;&#21516;&#31995;&#32479;&#20043;&#38388;&#30340;&#20223;&#23556;&#21464;&#25442;&#20272;&#35745;&#65292;&#36825;&#21487;&#20197;&#36890;&#36807;&#19987;&#23478;&#30340;&#30693;&#35782;&#36827;&#34892;&#25913;&#36827;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Glacier Research&#22312;1973&#24180;&#21457;&#34920;&#30340;&#25913;&#36827;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#29992;&#20110;&#20256;&#24863;&#22120;&#30340;&#36719;&#20214;&#26657;&#20934;&#12289;&#22522;&#20110;&#19987;&#23478;&#30340;&#36866;&#24212;&#21644;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#27169;&#25311;&#21644;&#23454;&#38469;&#27979;&#37327;&#25968;&#25454;&#23545;&#25105;&#20204;&#30340;&#30740;&#31350;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#23454;&#39564;&#20013;&#20351;&#29992;&#20102;&#19968;&#20010;&#20855;&#26377;8&#20010;&#30456;&#21516;&#20256;&#24863;&#22120;&#30340;&#22810;&#20256;&#24863;&#22120;&#26495;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#26080;&#35770;&#26159;&#27169;&#25311;&#36824;&#26159;&#23454;&#39564;&#25968;&#25454;&#65292;&#37117;&#24471;&#21040;&#20102;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
An important task in the field of sensor technology is the efficient implementation of adaptation procedures of measurements from one sensor to another sensor of identical design. One idea is to use the estimation of an affine transformation between different systems, which can be improved by the knowledge of experts. This paper presents an improved solution from Glacier Research that was published back in 1973. It is shown that this solution can be adapted for software calibration of sensors, implementation of expert-based adaptation, and federated learning methods. We evaluate our research with simulations and also with real measured data of a multi-sensor board with 8 identical sensors. The results show an improvement for both the simulation and the experiments with real data.
&lt;/p&gt;</description></item><item><title>EarthPT&#26159;&#19968;&#20010;&#22320;&#29699;&#35266;&#27979;&#30340;&#39044;&#35757;&#32451;transformer&#27169;&#22411;&#65292;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#22320;&#34920;&#21453;&#23556;&#20540;&#65292;&#24182;&#25552;&#20379;&#26377;&#24847;&#20041;&#30340;&#23884;&#20837;&#20449;&#24687;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2309.07207</link><description>&lt;p&gt;
EarthPT&#65306;&#22320;&#29699;&#35266;&#27979;&#30340;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
EarthPT: a foundation model for Earth Observation. (arXiv:2309.07207v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07207
&lt;/p&gt;
&lt;p&gt;
EarthPT&#26159;&#19968;&#20010;&#22320;&#29699;&#35266;&#27979;&#30340;&#39044;&#35757;&#32451;transformer&#27169;&#22411;&#65292;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#22320;&#34920;&#21453;&#23556;&#20540;&#65292;&#24182;&#25552;&#20379;&#26377;&#24847;&#20041;&#30340;&#23884;&#20837;&#20449;&#24687;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;EarthPT - &#19968;&#31181;&#22320;&#29699;&#35266;&#27979;(EO)&#39044;&#35757;&#32451;&#30340;transformer&#27169;&#22411;&#12290;EarthPT&#26159;&#19968;&#20010;7&#20159;&#21442;&#25968;&#30340;&#35299;&#30721;transformer&#22522;&#30784;&#27169;&#22411;&#65292;&#20197;&#33258;&#30417;&#30563;&#30340;&#26041;&#24335;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#19987;&#38376;&#38024;&#23545;EO&#24212;&#29992;&#36827;&#34892;&#24320;&#21457;&#12290;&#25105;&#20204;&#35777;&#26126;EarthPT&#26159;&#19968;&#20010;&#26377;&#25928;&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#39044;&#27979;&#26410;&#26469;400-2300 nm&#33539;&#22260;&#20869;&#30340;&#20687;&#32032;&#32423;&#22320;&#34920;&#21453;&#23556;&#20540;&#12290;&#20363;&#22914;&#65292;&#22312;&#19968;&#20010;&#20026;&#26399;&#20116;&#20010;&#26376;&#30340;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#65292;&#22320;&#34920;&#26893;&#34987;&#25351;&#25968;&#65288;NDVI&#65289;&#30340;&#28436;&#21464;&#39044;&#27979;&#30340;&#20856;&#22411;&#35823;&#24046;&#32422;&#20026;0.05&#65288;&#22312;-1 -&gt; 1&#30340;&#33258;&#28982;&#33539;&#22260;&#20869;&#65289;&#65292;&#24615;&#33021;&#36229;&#36807;&#22522;&#20110;&#21382;&#21490;&#24179;&#22343;&#30340;&#31616;&#21333;&#30456;&#20301;&#25240;&#21472;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;EarthPT&#23398;&#21040;&#30340;&#23884;&#20837;&#20855;&#26377;&#35821;&#20041;&#19978;&#26377;&#24847;&#20041;&#30340;&#20449;&#24687;&#65292;&#24182;&#19988;&#21487;&#20197;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#65292;&#22914;&#39640;&#31934;&#24230;&#12289;&#21160;&#24577;&#30340;&#22303;&#22320;&#21033;&#29992;&#20998;&#31867;&#12290;&#20196;&#20154;&#20852;&#22859;&#30340;&#26159;&#65292;&#25105;&#20204;&#27880;&#24847;&#21040;EO&#25968;&#25454;&#30340;&#20016;&#23500;&#24615;&#25552;&#20379;&#20102;&#29702;&#35770;&#19978;&#30340;...
&lt;/p&gt;
&lt;p&gt;
We introduce EarthPT -- an Earth Observation (EO) pretrained transformer. EarthPT is a 700 million parameter decoding transformer foundation model trained in an autoregressive self-supervised manner and developed specifically with EO use-cases in mind. We demonstrate that EarthPT is an effective forecaster that can accurately predict future pixel-level surface reflectances across the 400-2300 nm range well into the future. For example, forecasts of the evolution of the Normalised Difference Vegetation Index (NDVI) have a typical error of approximately 0.05 (over a natural range of -1 -&gt; 1) at the pixel level over a five month test set horizon, out-performing simple phase-folded models based on historical averaging. We also demonstrate that embeddings learnt by EarthPT hold semantically meaningful information and could be exploited for downstream tasks such as highly granular, dynamic land use classification. Excitingly, we note that the abundance of EO data provides us with -- in theor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21333;&#36890;&#36947;&#33041;&#30005;&#22270;&#30340;&#30561;&#30496;&#38454;&#27573;&#20998;&#31867;&#26041;&#27861;&#65292;&#20351;&#29992;SE-Resnet-Bi-LSTM&#26550;&#26500;&#36827;&#34892;&#30561;&#30496;&#38454;&#27573;&#30340;&#20998;&#31867;&#65292;&#24182;&#36827;&#34892;&#20102;&#27169;&#22411;&#35299;&#37322;&#24615;&#20998;&#26512;&#12290;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#20934;&#30830;&#29575;&#21644;&#23439;F1&#24471;&#20998;&#12290;</title><link>http://arxiv.org/abs/2309.07156</link><description>&lt;p&gt;
&#28145;&#20837;&#30740;&#31350;&#30561;&#30496;&#65306;&#22522;&#20110;&#21333;&#36890;&#36947;&#33041;&#30005;&#22270;&#30340;&#30561;&#30496;&#38454;&#27573;&#20998;&#31867;&#19982;&#27169;&#22411;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
A Deep Dive into Sleep: Single-Channel EEG-Based Sleep Stage Classification with Model Interpretability. (arXiv:2309.07156v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07156
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21333;&#36890;&#36947;&#33041;&#30005;&#22270;&#30340;&#30561;&#30496;&#38454;&#27573;&#20998;&#31867;&#26041;&#27861;&#65292;&#20351;&#29992;SE-Resnet-Bi-LSTM&#26550;&#26500;&#36827;&#34892;&#30561;&#30496;&#38454;&#27573;&#30340;&#20998;&#31867;&#65292;&#24182;&#36827;&#34892;&#20102;&#27169;&#22411;&#35299;&#37322;&#24615;&#20998;&#26512;&#12290;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#20934;&#30830;&#29575;&#21644;&#23439;F1&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30561;&#30496;&#26159;&#19968;&#31181;&#22522;&#26412;&#30340;&#29983;&#29702;&#36807;&#31243;&#65292;&#22312;&#25105;&#20204;&#30340;&#29983;&#27963;&#20013;&#21344;&#25454;&#30528;&#37325;&#35201;&#30340;&#37096;&#20998;&#12290;&#20934;&#30830;&#20998;&#31867;&#30561;&#30496;&#38454;&#27573;&#26159;&#35780;&#20272;&#30561;&#30496;&#36136;&#37327;&#21644;&#35782;&#21035;&#21487;&#33021;&#30340;&#30561;&#30496;&#38556;&#30861;&#30340;&#20851;&#38190;&#24037;&#20855;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;SE-Resnet-Bi-LSTM&#26550;&#26500;&#23558;&#30561;&#30496;&#20998;&#31867;&#20026;&#20116;&#20010;&#19981;&#21516;&#30340;&#38454;&#27573;&#12290;&#20998;&#31867;&#36807;&#31243;&#22522;&#20110;&#23545;&#21333;&#36890;&#36947;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#30340;&#20998;&#26512;&#12290;&#24314;&#35758;&#30340;&#26694;&#26550;&#30001;&#20004;&#20010;&#22522;&#26412;&#20803;&#32032;&#32452;&#25104;&#65306;&#21033;&#29992;SE-ResNet&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#21033;&#29992;Bi-LSTM&#21333;&#20803;&#22534;&#26632;&#30340;&#26102;&#38388;&#19978;&#19979;&#25991;&#32534;&#30721;&#22120;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#36890;&#36807;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#20840;&#38754;&#35780;&#20272;&#24471;&#21040;&#35777;&#23454;&#65292;&#20998;&#21035;&#26159;SLeepEDF-20&#12289;SleepEDF-78&#21644;SHHS&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#30456;&#24212;&#30340;&#25968;&#25454;&#38598;&#19978;&#20998;&#21035;&#36798;&#21040;&#20102;&#26174;&#33879;&#30340;&#20934;&#30830;&#29575;&#65292;&#20998;&#21035;&#20026;87.5&#65285;&#12289;83.9&#65285;&#21644;87.8&#65285;&#65292;&#24182;&#19988;&#22312;&#23439;F1&#24471;&#20998;&#26041;&#38754;&#20998;&#21035;&#20026;82.5&#12289;78.9&#21644;81.9&#12290;
&lt;/p&gt;
&lt;p&gt;
Sleep, a fundamental physiological process, occupies a significant portion of our lives. Accurate classification of sleep stages serves as a crucial tool for evaluating sleep quality and identifying probable sleep disorders. This work introduces a novel methodology that utilises a SE-Resnet-Bi-LSTM architecture to classify sleep into five separate stages. The classification process is based on the analysis of single-channel electroencephalograms (EEGs). The framework that has been suggested consists of two fundamental elements: a feature extractor that utilises SE-ResNet, and a temporal context encoder that use stacks of Bi-LSTM units.The effectiveness of our approach is substantiated by thorough assessments conducted on three different datasets, namely SLeepEDF-20, SleepEDF-78, and SHHS. Significantly, our methodology attains notable levels of accuracy, specifically 87.5\%, 83.9\%, and 87.8\%, along with macro-F1 scores of 82.5, 78.9, and 81.9 for the corresponding datasets. Notably, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;$d$-DRFWL(2) GNNs&#30340;&#26032;&#22411;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#36890;&#36807;&#38480;&#21046;&#33410;&#28857;&#20043;&#38388;&#30340;&#36317;&#31163;&#26469;&#23454;&#29616;&#24490;&#29615;&#35745;&#25968;&#33021;&#21147;&#65292;&#20811;&#26381;&#20102;&#23376;&#22270;GNNs&#30340;&#39044;&#22788;&#29702;&#21644;&#35745;&#31639;&#25104;&#26412;&#39640;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2309.04941</link><description>&lt;p&gt;
&#38480;&#21046;&#36317;&#31163;&#30340;&#27665;&#38388;&#20256;&#35828;Weisfeiler-Leman&#22270;&#31070;&#32463;&#32593;&#32476;&#21450;&#21487;&#35777;&#26126;&#30340;&#24490;&#29615;&#35745;&#25968;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Distance-Restricted Folklore Weisfeiler-Leman GNNs with Provable Cycle Counting Power. (arXiv:2309.04941v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04941
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;$d$-DRFWL(2) GNNs&#30340;&#26032;&#22411;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#36890;&#36807;&#38480;&#21046;&#33410;&#28857;&#20043;&#38388;&#30340;&#36317;&#31163;&#26469;&#23454;&#29616;&#24490;&#29615;&#35745;&#25968;&#33021;&#21147;&#65292;&#20811;&#26381;&#20102;&#23376;&#22270;GNNs&#30340;&#39044;&#22788;&#29702;&#21644;&#35745;&#31639;&#25104;&#26412;&#39640;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#33021;&#21147;&#22312;&#24191;&#27867;&#30340;&#20219;&#21153;&#20013;&#25104;&#21151;&#35745;&#25968;&#29305;&#23450;&#30340;&#22270;&#23376;&#32467;&#26500;&#65292;&#23588;&#20854;&#26159;&#24490;&#29615;&#65292;&#23545;&#20110;GNNs&#30340;&#25104;&#21151;&#38750;&#24120;&#20851;&#38190;&#12290;&#26368;&#36817;&#65292;&#23427;&#24050;&#34987;&#29992;&#20316;&#35780;&#20272;GNNs&#34920;&#36798;&#33021;&#21147;&#30340;&#19968;&#31181;&#24120;&#29992;&#25351;&#26631;&#12290;&#35768;&#22810;&#20855;&#26377;&#21487;&#35777;&#26126;&#30340;&#24490;&#29615;&#35745;&#25968;&#33021;&#21147;&#30340;GNN&#27169;&#22411;&#37117;&#22522;&#20110;&#23376;&#22270;GNNs&#65292;&#21363;&#20174;&#36755;&#20837;&#22270;&#20013;&#25552;&#21462;&#19968;&#32452;&#23376;&#22270;&#65292;&#20026;&#27599;&#20010;&#23376;&#22270;&#29983;&#25104;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#23427;&#20204;&#26469;&#22686;&#24378;&#36755;&#20837;&#22270;&#30340;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#36827;&#34892;&#32321;&#37325;&#30340;&#39044;&#22788;&#29702;&#65292;&#24182;&#19988;&#26102;&#38388;&#21644;&#20869;&#23384;&#25104;&#26412;&#36739;&#39640;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;GNN&#31867;&#21035;-- $d$-Distance-Restricted FWL(2) GNNs&#65292;&#25110;&#32773; $d$-DRFWL(2) GNNs&#65292;&#20811;&#26381;&#20102;&#23376;&#22270;GNNs&#30340;&#19978;&#36848;&#38480;&#21046;&#12290;$d$-DRFWL(2) GNNs&#23558;&#20114;&#30456;&#20043;&#38388;&#36317;&#31163;&#19981;&#36229;&#36807;$d$&#30340;&#33410;&#28857;&#23545;&#20316;&#20026;&#20449;&#24687;&#20256;&#36882;&#30340;&#21333;&#20301;&#65292;&#20197;&#24179;&#34913;&#34920;&#36798;&#33021;&#21147;&#21644;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability of graph neural networks (GNNs) to count certain graph substructures, especially cycles, is important for the success of GNNs on a wide range of tasks. It has been recently used as a popular metric for evaluating the expressive power of GNNs. Many of the proposed GNN models with provable cycle counting power are based on subgraph GNNs, i.e., extracting a bag of subgraphs from the input graph, generating representations for each subgraph, and using them to augment the representation of the input graph. However, those methods require heavy preprocessing, and suffer from high time and memory costs. In this paper, we overcome the aforementioned limitations of subgraph GNNs by proposing a novel class of GNNs -- $d$-Distance-Restricted FWL(2) GNNs, or $d$-DRFWL(2) GNNs. $d$-DRFWL(2) GNNs use node pairs whose mutual distances are at most $d$ as the units for message passing to balance the expressive power and complexity. By performing message passing among distance-restricted node
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#20132;&#20114;&#24335;&#36229;&#21442;&#25968;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#24212;&#29992;&#20559;&#22909;&#23398;&#20064;&#26469;&#35299;&#20915;&#22810;&#30446;&#26631;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.03581</link><description>&lt;p&gt;
&#36890;&#36807;&#20559;&#22909;&#23398;&#20064;&#22312;&#22810;&#30446;&#26631;&#38382;&#39064;&#20013;&#36827;&#34892;&#20132;&#20114;&#24335;&#36229;&#21442;&#25968;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Interactive Hyperparameter Optimization in Multi-Objective Problems via Preference Learning. (arXiv:2309.03581v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03581
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#20132;&#20114;&#24335;&#36229;&#21442;&#25968;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#24212;&#29992;&#20559;&#22909;&#23398;&#20064;&#26469;&#35299;&#20915;&#22810;&#30446;&#26631;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#21442;&#25968;&#20248;&#21270;&#23545;&#20110;&#21457;&#25381;&#26426;&#22120;&#23398;&#20064;&#30340;&#28508;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#29992;&#25143;&#36890;&#24120;&#23545;&#22810;&#30446;&#26631;&#38382;&#39064;&#24863;&#20852;&#36259;&#65292;&#21363;&#20248;&#21270;&#21487;&#33021;&#23384;&#22312;&#20914;&#31361;&#30340;&#30446;&#26631;&#65292;&#27604;&#22914;&#20934;&#30830;&#24615;&#21644;&#33021;&#32791;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#32477;&#22823;&#22810;&#25968;&#22810;&#30446;&#26631;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#23558;&#19968;&#32452;&#38750;&#25903;&#37197;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24085;&#32047;&#25176;&#21069;&#27839;&#36820;&#22238;&#32473;&#29992;&#25143;&#12290;&#28982;&#32780;&#65292;&#20248;&#21270;&#36825;&#31181;&#31639;&#27861;&#30340;&#36229;&#21442;&#25968;&#24182;&#19981;&#23481;&#26131;&#65292;&#22240;&#20026;&#35780;&#20272;&#19968;&#20010;&#36229;&#21442;&#25968;&#37197;&#32622;&#28041;&#21450;&#35780;&#20272;&#24471;&#21040;&#30340;&#24085;&#32047;&#25176;&#21069;&#27839;&#30340;&#36136;&#37327;&#12290;&#22312;&#25991;&#29486;&#20013;&#65292;&#24050;&#26377;&#19968;&#20123;&#25351;&#26631;&#21487;&#20197;&#36890;&#36807;&#37327;&#21270;&#19981;&#21516;&#23646;&#24615;&#65288;&#22914;&#20307;&#31215;&#12289;&#19982;&#21442;&#32771;&#28857;&#30340;&#25509;&#36817;&#31243;&#24230;&#65289;&#26469;&#35780;&#20272;&#24085;&#32047;&#25176;&#21069;&#27839;&#30340;&#36136;&#37327;&#65288;&#20363;&#22914;&#36229;&#20307;&#31215;&#12289;R2&#65289;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#29992;&#25143;&#26469;&#35828;&#65292;&#36873;&#25321;&#23548;&#33268;&#26399;&#26395;&#30340;&#24085;&#32047;&#25176;&#21069;&#27839;&#30340;&#25351;&#26631;&#21487;&#33021;&#26159;&#19968;&#39033;&#22256;&#38590;&#30340;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#20132;&#20114;&#24335;&#36229;&#21442;&#25968;&#20248;&#21270;&#26041;&#27861;&#65292;&#38024;&#23545;&#22810;&#30446;&#26631;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20559;&#22909;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hyperparameter optimization (HPO) is important to leverage the full potential of machine learning (ML). In practice, users are often interested in multi-objective (MO) problems, i.e., optimizing potentially conflicting objectives, like accuracy and energy consumption. To tackle this, the vast majority of MO-ML algorithms return a Pareto front of non-dominated machine learning models to the user. Optimizing the hyperparameters of such algorithms is non-trivial as evaluating a hyperparameter configuration entails evaluating the quality of the resulting Pareto front. In literature, there are known indicators that assess the quality of a Pareto front (e.g., hypervolume, R2) by quantifying different properties (e.g., volume, proximity to a reference point). However, choosing the indicator that leads to the desired Pareto front might be a hard task for a user. In this paper, we propose a human-centered interactive HPO approach tailored towards multi-objective ML leveraging preference learnin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#19977;&#20540;&#20915;&#31574;&#26641;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;&#20915;&#31574;&#26641;&#22312;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#26102;&#30340;&#34920;&#29616;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#19981;&#21516;&#65292;&#35813;&#31639;&#27861;&#19981;&#20551;&#35774;&#32570;&#22833;&#20540;&#21253;&#21547;&#20219;&#20309;&#20851;&#20110;&#21709;&#24212;&#30340;&#20449;&#24687;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#29305;&#23450;&#32570;&#22833;&#25968;&#25454;&#22330;&#26223;&#19979;&#65292;&#19977;&#20540;&#20915;&#31574;&#26641;&#22312;MCAR&#35774;&#32622;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#22312;IM&#35774;&#32622;&#20013;&#30053;&#36874;&#19968;&#31609;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#23558;&#19977;&#20540;&#20915;&#31574;&#26641;&#19982;&#32570;&#22833;&#22312;&#23646;&#24615;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#33719;&#24471;&#26356;&#31283;&#20581;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#35757;&#32451;&#36895;&#24230;&#36739;&#24930;&#65292;&#20294;&#19977;&#20540;&#20915;&#31574;&#26641;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#19988;&#26356;&#20934;&#30830;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.03561</link><description>&lt;p&gt;
&#32570;&#22833;&#20540;&#22788;&#29702;&#30340;&#19977;&#20540;&#20915;&#31574;&#26641;
&lt;/p&gt;
&lt;p&gt;
Trinary Decision Trees for missing value handling. (arXiv:2309.03561v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03561
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#19977;&#20540;&#20915;&#31574;&#26641;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;&#20915;&#31574;&#26641;&#22312;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#26102;&#30340;&#34920;&#29616;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#19981;&#21516;&#65292;&#35813;&#31639;&#27861;&#19981;&#20551;&#35774;&#32570;&#22833;&#20540;&#21253;&#21547;&#20219;&#20309;&#20851;&#20110;&#21709;&#24212;&#30340;&#20449;&#24687;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#29305;&#23450;&#32570;&#22833;&#25968;&#25454;&#22330;&#26223;&#19979;&#65292;&#19977;&#20540;&#20915;&#31574;&#26641;&#22312;MCAR&#35774;&#32622;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#22312;IM&#35774;&#32622;&#20013;&#30053;&#36874;&#19968;&#31609;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#23558;&#19977;&#20540;&#20915;&#31574;&#26641;&#19982;&#32570;&#22833;&#22312;&#23646;&#24615;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#33719;&#24471;&#26356;&#31283;&#20581;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#35757;&#32451;&#36895;&#24230;&#36739;&#24930;&#65292;&#20294;&#19977;&#20540;&#20915;&#31574;&#26641;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#19988;&#26356;&#20934;&#30830;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19977;&#20540;&#20915;&#31574;&#26641;&#65292;&#36825;&#26159;&#19968;&#31181;&#26088;&#22312;&#25913;&#21892;&#20915;&#31574;&#26641;&#22238;&#24402;&#22120;&#21644;&#20998;&#31867;&#22120;&#20013;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#30340;&#31639;&#27861;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#19981;&#21516;&#65292;&#19977;&#20540;&#20915;&#31574;&#26641;&#19981;&#20551;&#35774;&#32570;&#22833;&#20540;&#21253;&#21547;&#26377;&#20851;&#21709;&#24212;&#30340;&#20219;&#20309;&#20449;&#24687;&#12290;&#26412;&#25991;&#36890;&#36807;&#29702;&#35770;&#35745;&#31639;&#21644;&#20351;&#29992;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#25968;&#20540;&#31034;&#20363;&#65292;&#27604;&#36739;&#20102;&#20854;&#22312;&#19981;&#21516;&#32570;&#22833;&#25968;&#25454;&#22330;&#26223;&#65288;&#23436;&#20840;&#38543;&#26426;&#32570;&#22833;&#65288;MCAR&#65289;&#21644;&#20449;&#24687;&#24615;&#32570;&#22833;&#65288;IM&#65289;&#65289;&#20013;&#19982;&#24050;&#24314;&#31435;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;MCAR&#35774;&#32622;&#20013;&#65292;&#19977;&#20540;&#26641;&#22312;&#21482;&#26377;&#26679;&#26412;&#22806;&#32570;&#22833;&#25968;&#25454;&#26102;&#34920;&#29616;&#20248;&#20110;&#20854;&#21516;&#34892;&#65292;&#32780;&#22312;IM&#35774;&#32622;&#20013;&#33853;&#21518;&#12290;&#19968;&#20010;&#28151;&#21512;&#27169;&#22411;&#65292;&#21363;&#19977;&#20540;&#32570;&#22833;&#22312;&#23646;&#24615;&#65288;MIA&#65289;&#26041;&#27861;&#21644;&#19977;&#20540;&#26641;&#30456;&#32467;&#21512;&#30340;TrinaryMIA&#26641;&#65292;&#22312;&#25152;&#26377;&#32570;&#22833;&#31867;&#22411;&#20013;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#35757;&#32451;&#36895;&#24230;&#36739;&#24930;&#21487;&#33021;&#26159;&#19968;&#20010;&#28508;&#22312;&#30340;&#32570;&#28857;&#65292;&#20294;&#19977;&#20540;&#20915;&#31574;&#26641;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#19988;&#26356;&#20934;&#30830;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces the Trinary decision tree, an algorithm designed to improve the handling of missing data in decision tree regressors and classifiers. Unlike other approaches, the Trinary decision tree does not assume that missing values contain any information about the response. Both theoretical calculations on estimator bias and numerical illustrations using real data sets are presented to compare its performance with established algorithms in different missing data scenarios (Missing Completely at Random (MCAR), and Informative Missingness (IM)). Notably, the Trinary tree outperforms its peers in MCAR settings, especially when data is only missing out-of-sample, while lacking behind in IM settings. A hybrid model, the TrinaryMIA tree, which combines the Trinary tree and the Missing In Attributes (MIA) approach, shows robust performance in all types of missingness. Despite the potential drawback of slower training speed, the Trinary tree offers a promising and more accurate met
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21644;&#22270;&#34920;&#31034;&#31070;&#32463;&#32593;&#32476;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36793;&#32536;&#29983;&#25104;&#35843;&#24230;&#26694;&#26550;(EGS)&#65292;&#29992;&#20110;&#35299;&#20915;&#23454;&#26102;DAG&#20219;&#21153;&#35843;&#24230;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#36845;&#20195;&#29983;&#25104;&#36793;&#32536;&#20197;&#26368;&#23567;&#21270;DAG&#23485;&#24230;&#65292;&#24182;&#30830;&#20445;&#28385;&#36275;&#25130;&#27490;&#26102;&#38388;&#32422;&#26463;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#23454;&#39564;&#20013;&#24471;&#21040;&#39564;&#35777;&#65292;&#24182;&#19982;&#20854;&#20182;DAG&#35843;&#24230;&#21551;&#21457;&#24335;&#31639;&#27861;&#21644;&#26368;&#20339;&#28151;&#21512;&#25972;&#25968;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2308.14647</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;DAG&#20219;&#21153;&#30340;&#36793;&#32536;&#29983;&#25104;&#35843;&#24230;
&lt;/p&gt;
&lt;p&gt;
Edge Generation Scheduling for DAG Tasks Using Deep Reinforcement Learning. (arXiv:2308.14647v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14647
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21644;&#22270;&#34920;&#31034;&#31070;&#32463;&#32593;&#32476;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36793;&#32536;&#29983;&#25104;&#35843;&#24230;&#26694;&#26550;(EGS)&#65292;&#29992;&#20110;&#35299;&#20915;&#23454;&#26102;DAG&#20219;&#21153;&#35843;&#24230;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#36845;&#20195;&#29983;&#25104;&#36793;&#32536;&#20197;&#26368;&#23567;&#21270;DAG&#23485;&#24230;&#65292;&#24182;&#30830;&#20445;&#28385;&#36275;&#25130;&#27490;&#26102;&#38388;&#32422;&#26463;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#23454;&#39564;&#20013;&#24471;&#21040;&#39564;&#35777;&#65292;&#24182;&#19982;&#20854;&#20182;DAG&#35843;&#24230;&#21551;&#21457;&#24335;&#31639;&#27861;&#21644;&#26368;&#20339;&#28151;&#21512;&#25972;&#25968;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#22312;&#23454;&#26102;&#39046;&#22495;&#20013;&#37319;&#29992;&#26377;&#21521;&#26080;&#29615;&#22270;(DAG)&#20219;&#21153;&#26469;&#27169;&#25311;&#27773;&#36710;&#12289;&#33322;&#31354;&#33322;&#22825;&#21644;&#24037;&#19994;&#39046;&#22495;&#20013;&#36890;&#36807;&#38142;&#24335;&#20114;&#36890;&#20219;&#21153;&#23454;&#29616;&#21151;&#33021;&#30340;&#22797;&#26434;&#24212;&#29992;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#24494;&#19981;&#36275;&#36947;&#35843;&#24230;&#27010;&#24565;&#30340;&#26032;&#22411;&#21487;&#35843;&#24230;&#24615;&#27979;&#35797;&#65292;&#30740;&#31350;&#20102;&#23454;&#26102;DAG&#20219;&#21153;&#35843;&#24230;&#38382;&#39064;&#12290;&#21033;&#29992;&#36825;&#20010;&#21487;&#35843;&#24230;&#24615;&#27979;&#35797;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;DAG&#35843;&#24230;&#26694;&#26550;(&#36793;&#32536;&#29983;&#25104;&#35843;&#24230;--EGS)&#65292;&#23427;&#35797;&#22270;&#36890;&#36807;&#36845;&#20195;&#29983;&#25104;&#36793;&#32536;&#26469;&#26368;&#23567;&#21270;DAG&#23485;&#24230;&#65292;&#24182;&#21516;&#26102;&#20445;&#35777;&#25130;&#27490;&#26102;&#38388;&#32422;&#26463;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21644;&#22270;&#34920;&#31034;&#31070;&#32463;&#32593;&#32476;&#26469;&#39640;&#25928;&#35299;&#20915;&#29983;&#25104;&#36793;&#32536;&#30340;&#38382;&#39064;&#65292;&#20197;&#23398;&#20064;EGS&#30340;&#39640;&#25928;&#36793;&#32536;&#29983;&#25104;&#31574;&#30053;&#12290;&#36890;&#36807;&#23558;&#20854;&#19982;&#26368;&#20808;&#36827;&#30340;DAG&#35843;&#24230;&#21551;&#21457;&#24335;&#31639;&#27861;&#21644;&#26368;&#20339;&#28151;&#21512;&#25972;&#25968;&#36827;&#34892;&#27604;&#36739;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#25152;&#25552;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Directed acyclic graph (DAG) tasks are currently adopted in the real-time domain to model complex applications from the automotive, avionics, and industrial domains that implement their functionalities through chains of intercommunicating tasks. This paper studies the problem of scheduling real-time DAG tasks by presenting a novel schedulability test based on the concept of trivial schedulability. Using this schedulability test, we propose a new DAG scheduling framework (edge generation scheduling -- EGS) that attempts to minimize the DAG width by iteratively generating edges while guaranteeing the deadline constraint. We study how to efficiently solve the problem of generating edges by developing a deep reinforcement learning algorithm combined with a graph representation neural network to learn an efficient edge generation policy for EGS. We evaluate the effectiveness of the proposed algorithm by comparing it with state-of-the-art DAG scheduling heuristics and an optimal mixed-intege
&lt;/p&gt;</description></item><item><title>ProAgent&#26159;&#19968;&#20010;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#30340;&#20027;&#21160;&#21512;&#20316;&#30340;AI&#26694;&#26550;&#65292;&#33021;&#22815;&#39044;&#27979;&#38431;&#21451;&#30340;&#20915;&#31574;&#24182;&#20026;&#33258;&#24049;&#21046;&#23450;&#22686;&#24378;&#35745;&#21010;&#65292;&#20855;&#26377;&#39640;&#24230;&#30340;&#27169;&#22359;&#21270;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.11339</link><description>&lt;p&gt;
ProAgent&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#20027;&#21160;&#21512;&#20316;&#30340;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
ProAgent: Building Proactive Cooperative AI with Large Language Models. (arXiv:2308.11339v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11339
&lt;/p&gt;
&lt;p&gt;
ProAgent&#26159;&#19968;&#20010;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#30340;&#20027;&#21160;&#21512;&#20316;&#30340;AI&#26694;&#26550;&#65292;&#33021;&#22815;&#39044;&#27979;&#38431;&#21451;&#30340;&#20915;&#31574;&#24182;&#20026;&#33258;&#24049;&#21046;&#23450;&#22686;&#24378;&#35745;&#21010;&#65292;&#20855;&#26377;&#39640;&#24230;&#30340;&#27169;&#22359;&#21270;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;AGI&#30740;&#31350;&#20013;&#65292;&#26500;&#24314;&#20855;&#26377;&#33258;&#36866;&#24212;&#34892;&#20026;&#30340;&#20154;&#24037;&#26234;&#33021;&#20197;&#36827;&#34892;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#30340;&#21512;&#20316;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#20851;&#27880;&#28857;&#12290;&#30446;&#21069;&#65292;&#24320;&#21457;&#21512;&#20316;&#20195;&#29702;&#20154;&#30340;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#25919;&#31574;&#27867;&#21270;&#20005;&#37325;&#20381;&#36182;&#20110;&#19982;&#29305;&#23450;&#38431;&#21451;&#30340;&#36807;&#21435;&#20114;&#21160;&#12290;&#36825;&#20123;&#26041;&#27861;&#38480;&#21046;&#20102;&#20195;&#29702;&#20154;&#22312;&#38754;&#23545;&#26032;&#30340;&#38431;&#21451;&#26102;&#37325;&#26032;&#26657;&#20934;&#31574;&#30053;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;ProAgent&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#21019;&#24314;&#19968;&#20010;&#20855;&#26377;&#39044;&#27979;&#38431;&#21451;&#26410;&#26469;&#20915;&#31574;&#33021;&#21147;&#21644;&#20026;&#33258;&#36523;&#21046;&#23450;&#22686;&#24378;&#35745;&#21010;&#33021;&#21147;&#30340;&#20027;&#21160;&#20195;&#29702;&#12290;ProAgent&#22312;&#21512;&#20316;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#33021;&#22815;&#21160;&#24577;&#35843;&#25972;&#34892;&#20026;&#20197;&#22686;&#24378;&#19982;&#38431;&#21451;&#30340;&#21327;&#20316;&#21162;&#21147;&#12290;&#27492;&#22806;&#65292;ProAgent&#26694;&#26550;&#20855;&#26377;&#39640;&#24230;&#30340;&#27169;&#22359;&#21270;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#20415;&#20110;&#26080;&#32541;&#38598;&#25104;&#65292;&#20197;&#24212;&#23545;&#21508;&#31181;&#21327;&#35843;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building AIs with adaptive behaviors in human-AI cooperation stands as a pivotal focus in AGI research. Current methods for developing cooperative agents predominantly rely on learning-based methods, where policy generalization heavily hinges on past interactions with specific teammates. These approaches constrain the agent's capacity to recalibrate its strategy when confronted with novel teammates. We propose \textbf{ProAgent}, a novel framework that harnesses large language models (LLMs) to fashion a \textit{pro}active \textit{agent} empowered with the ability to anticipate teammates' forthcoming decisions and formulate enhanced plans for itself. ProAgent excels at cooperative reasoning with the capacity to dynamically adapt its behavior to enhance collaborative efforts with teammates. Moreover, the ProAgent framework exhibits a high degree of modularity and interpretability, facilitating seamless integration to address a wide array of coordination scenarios. Experimental evaluations
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35843;&#26597;&#20102;&#22312;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#20013;&#24212;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#29616;&#29366;&#12289;&#26426;&#20250;&#21644;&#24320;&#25918;&#30740;&#31350;&#38382;&#39064;&#12290;&#20316;&#32773;&#20027;&#35201;&#35752;&#35770;&#20102;&#19977;&#31181;&#24212;&#29992;&#31867;&#22411;&#65306;&#26080;&#29305;&#23450;&#30446;&#26631;&#30340;&#29983;&#25104;&#26041;&#24335;&#12289;&#21516;&#26102;&#26368;&#22823;&#21270;&#30446;&#26631;&#20989;&#25968;&#30340;&#36755;&#20986;&#29983;&#25104;&#26041;&#24335;&#20197;&#21450;&#23558;&#26080;&#27861;&#36890;&#36807;&#30446;&#26631;&#20989;&#25968;&#25429;&#25417;&#30340;&#26399;&#26395;&#29305;&#24449;&#23884;&#20837;&#29983;&#25104;&#36807;&#31243;&#30340;&#26041;&#24335;&#12290;&#36825;&#20010;&#26032;&#20852;&#39046;&#22495;&#20013;&#23384;&#22312;&#30528;&#20016;&#23500;&#30340;&#26426;&#20250;&#21644;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.00031</link><description>&lt;p&gt;
&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#30340;&#24378;&#21270;&#23398;&#20064;&#65306;&#29616;&#29366;&#12289;&#26426;&#20250;&#21644;&#24320;&#25918;&#30740;&#31350;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning for Generative AI: State of the Art, Opportunities and Open Research Challenges. (arXiv:2308.00031v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00031
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35843;&#26597;&#20102;&#22312;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#20013;&#24212;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#29616;&#29366;&#12289;&#26426;&#20250;&#21644;&#24320;&#25918;&#30740;&#31350;&#38382;&#39064;&#12290;&#20316;&#32773;&#20027;&#35201;&#35752;&#35770;&#20102;&#19977;&#31181;&#24212;&#29992;&#31867;&#22411;&#65306;&#26080;&#29305;&#23450;&#30446;&#26631;&#30340;&#29983;&#25104;&#26041;&#24335;&#12289;&#21516;&#26102;&#26368;&#22823;&#21270;&#30446;&#26631;&#20989;&#25968;&#30340;&#36755;&#20986;&#29983;&#25104;&#26041;&#24335;&#20197;&#21450;&#23558;&#26080;&#27861;&#36890;&#36807;&#30446;&#26631;&#20989;&#25968;&#25429;&#25417;&#30340;&#26399;&#26395;&#29305;&#24449;&#23884;&#20837;&#29983;&#25104;&#36807;&#31243;&#30340;&#26041;&#24335;&#12290;&#36825;&#20010;&#26032;&#20852;&#39046;&#22495;&#20013;&#23384;&#22312;&#30528;&#20016;&#23500;&#30340;&#26426;&#20250;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26159;&#36817;&#21313;&#24180;&#26469;&#35745;&#31639;&#26426;&#31185;&#23398;&#39046;&#22495;&#26368;&#20196;&#20154;&#20852;&#22859;&#30340;&#21457;&#23637;&#20043;&#19968;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#24050;&#32463;&#25104;&#20026;&#38750;&#24120;&#25104;&#21151;&#30340;&#33539;&#24335;&#12290;&#22312;&#26412;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#23558;RL&#24212;&#29992;&#20110;&#29983;&#25104;AI&#20013;&#30340;&#29616;&#29366;&#12289;&#26426;&#20250;&#21644;&#24320;&#25918;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#35752;&#35770;&#19977;&#31181;&#24212;&#29992;&#31867;&#22411;&#65292;&#21363;&#20316;&#20026;&#19968;&#31181;&#26080;&#29305;&#23450;&#30446;&#26631;&#30340;&#29983;&#25104;&#26041;&#24335;&#65292;&#20316;&#20026;&#19968;&#31181;&#21516;&#26102;&#26368;&#22823;&#21270;&#30446;&#26631;&#20989;&#25968;&#30340;&#36755;&#20986;&#29983;&#25104;&#26041;&#24335;&#65292;&#20197;&#21450;&#20316;&#20026;&#19968;&#31181;&#23558;&#26080;&#27861;&#36890;&#36807;&#30446;&#26631;&#20989;&#25968;&#36731;&#26494;&#25429;&#25417;&#30340;&#26399;&#26395;&#29305;&#24449;&#23884;&#20837;&#29983;&#25104;&#36807;&#31243;&#30340;&#26041;&#24335;&#12290;&#25105;&#20204;&#22312;&#35843;&#26597;&#32467;&#26524;&#20013;&#23545;&#36825;&#20010;&#36855;&#20154;&#30340;&#26032;&#20852;&#39046;&#22495;&#20013;&#30340;&#26426;&#20250;&#21644;&#25361;&#25112;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Artificial Intelligence (AI) is one of the most exciting developments in Computer Science of the last decade. At the same time, Reinforcement Learning (RL) has emerged as a very successful paradigm for a variety of machine learning tasks. In this survey, we discuss the state of the art, opportunities and open research questions in applying RL to generative AI. In particular, we will discuss three types of applications, namely, RL as an alternative way for generation without specified objectives; as a way for generating outputs while concurrently maximizing an objective function; and, finally, as a way of embedding desired characteristics, which cannot be easily captured by means of an objective function, into the generative process. We conclude the survey with an in-depth discussion of the opportunities and challenges in this fascinating emerging area.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32553;&#25918;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#21270;&#24037;&#21378;&#27169;&#25311;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#24494;&#35843;&#27169;&#22411;&#35299;&#20915;&#20102;&#22312;&#24212;&#29992;&#20110;&#36739;&#22823;&#24037;&#21378;&#26102;&#20986;&#29616;&#30340;&#24490;&#29615;&#27714;&#35299;&#19981;&#31283;&#23450;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.13621</link><description>&lt;p&gt;
&#32553;&#25918;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#21270;&#24037;&#21378;&#27169;&#25311;&#65306;&#29992;&#20110;&#35843;&#25972;&#27169;&#22411;&#20197;&#35825;&#23548;&#31283;&#23450;&#22266;&#23450;&#28857;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Scaling machine learning-based chemical plant simulation: A method for fine-tuning a model to induce stable fixed points. (arXiv:2307.13621v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13621
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32553;&#25918;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#21270;&#24037;&#21378;&#27169;&#25311;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#24494;&#35843;&#27169;&#22411;&#35299;&#20915;&#20102;&#22312;&#24212;&#29992;&#20110;&#36739;&#22823;&#24037;&#21378;&#26102;&#20986;&#29616;&#30340;&#24490;&#29615;&#27714;&#35299;&#19981;&#31283;&#23450;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#24819;&#21270;&#30340;&#21270;&#24037;&#21378;&#22522;&#20110;&#31532;&#19968;&#21407;&#29702;&#30340;&#27169;&#22411;&#21487;&#33021;&#19981;&#20934;&#30830;&#12290;&#21478;&#19968;&#31181;&#26041;&#27861;&#26159;&#30452;&#25509;&#23558;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#25311;&#21512;&#21040;&#24037;&#21378;&#20256;&#24863;&#22120;&#25968;&#25454;&#19978;&#12290;&#25105;&#20204;&#37319;&#29992;&#19968;&#31181;&#32467;&#26500;&#21270;&#30340;&#26041;&#27861;&#65306;&#23558;&#24037;&#21378;&#20869;&#30340;&#27599;&#20010;&#21333;&#20803;&#34920;&#31034;&#20026;&#19968;&#20010;ML&#27169;&#22411;&#12290;&#22312;&#23558;&#27169;&#22411;&#25311;&#21512;&#21040;&#25968;&#25454;&#21518;&#65292;&#23558;&#27169;&#22411;&#36830;&#25509;&#25104;&#31867;&#20284;&#27969;&#31243;&#22270;&#30340;&#26377;&#21521;&#22270;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23545;&#20110;&#36739;&#23567;&#30340;&#24037;&#21378;&#65292;&#36825;&#31181;&#26041;&#27861;&#25928;&#26524;&#24456;&#22909;&#65292;&#20294;&#23545;&#20110;&#36739;&#22823;&#30340;&#24037;&#21378;&#65292;&#30001;&#20110;&#27969;&#31243;&#22270;&#20013;&#23384;&#22312;&#22823;&#22411;&#21644;&#23884;&#22871;&#24490;&#29615;&#25152;&#23548;&#33268;&#30340;&#22797;&#26434;&#21160;&#21147;&#23398;&#20250;&#23548;&#33268;&#24490;&#29615;&#27714;&#35299;&#22120;&#30340;&#19981;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#23545;&#36825;&#20010;&#38382;&#39064;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#24182;&#34920;&#26126;&#36825;&#19981;&#20165;&#20165;&#26159;&#19968;&#20010;&#29305;&#27530;&#20851;&#27880;&#28857;&#65292;&#32780;&#26159;&#19968;&#20010;&#26356;&#26222;&#36941;&#30340;&#25361;&#25112;&#65292;&#21487;&#33021;&#22312;&#24212;&#29992;&#20110;&#36739;&#22823;&#30340;&#24037;&#21378;&#26102;&#20250;&#21457;&#29983;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;ML&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20174;&#32780;&#20351;&#24471;&#24120;&#35268;&#26041;&#27861;&#19979;&#30340;&#24490;&#29615;&#27714;&#35299;&#21464;&#24471;&#31283;&#20581;&#12290;
&lt;/p&gt;
&lt;p&gt;
Idealized first-principles models of chemical plants can be inaccurate. An alternative is to fit a Machine Learning (ML) model directly to plant sensor data. We use a structured approach: Each unit within the plant gets represented by one ML model. After fitting the models to the data, the models are connected into a flowsheet-like directed graph. We find that for smaller plants, this approach works well, but for larger plants, the complex dynamics arising from large and nested cycles in the flowsheet lead to instabilities in the cycle solver. We analyze this problem in depth and show that it is not merely a specialized concern but rather a more pervasive challenge that will likely occur whenever ML is applied to larger plants. To address this problem, we present a way to fine-tune ML models such that solving cycles with the usual methods becomes robust again.
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#35843;&#26597;&#20102;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#22312;&#24037;&#19994;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#20351;&#29992;&#12290;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#36890;&#36807;&#21033;&#29992;&#30456;&#20851;&#20219;&#21153;&#30340;&#30693;&#35782;&#21644;&#32771;&#34385;&#25968;&#25454;&#20998;&#24067;&#30340;&#21464;&#21270;&#65292;&#35299;&#20915;&#20102;&#20165;&#26377;&#23569;&#37327;&#25110;&#27809;&#26377;&#38468;&#21152;&#26631;&#35760;&#25968;&#25454;&#24773;&#20917;&#19979;&#30340;&#26032;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2307.05638</link><description>&lt;p&gt;
&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#22312;&#24037;&#19994;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#32508;&#21512;&#35843;&#26597;&#65306;&#26041;&#27861;&#12289;&#24212;&#29992;&#21644;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey of Deep Transfer Learning for Anomaly Detection in Industrial Time Series: Methods, Applications, and Directions. (arXiv:2307.05638v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05638
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35843;&#26597;&#20102;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#22312;&#24037;&#19994;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#20351;&#29992;&#12290;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#36890;&#36807;&#21033;&#29992;&#30456;&#20851;&#20219;&#21153;&#30340;&#30693;&#35782;&#21644;&#32771;&#34385;&#25968;&#25454;&#20998;&#24067;&#30340;&#21464;&#21270;&#65292;&#35299;&#20915;&#20102;&#20165;&#26377;&#23569;&#37327;&#25110;&#27809;&#26377;&#38468;&#21152;&#26631;&#35760;&#25968;&#25454;&#24773;&#20917;&#19979;&#30340;&#26032;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#30417;&#27979;&#24037;&#19994;&#36807;&#31243;&#26377;&#28508;&#21147;&#36890;&#36807;&#21450;&#26102;&#26816;&#27979;&#24322;&#24120;&#20107;&#20214;&#24182;&#20419;&#36827;&#21450;&#26102;&#24178;&#39044;&#26469;&#25552;&#39640;&#25928;&#29575;&#21644;&#20248;&#21270;&#36136;&#37327;&#12290;&#28145;&#24230;&#23398;&#20064;&#36890;&#36807;&#35782;&#21035;&#22823;&#25968;&#25454;&#38598;&#20013;&#30340;&#38750;&#24179;&#20961;&#27169;&#24335;&#65292;&#22312;&#36825;&#19968;&#36807;&#31243;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#26631;&#20934;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36866;&#29992;&#20110;&#35299;&#20915;&#29305;&#23450;&#31867;&#22411;&#30340;&#25968;&#25454;&#32473;&#23450;&#29305;&#23450;&#20219;&#21153;&#30340;&#38382;&#39064;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#36825;&#20123;&#31639;&#27861;&#38656;&#35201;&#22823;&#37327;&#30340;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24037;&#33402;&#21644;&#29615;&#22659;&#30340;&#21160;&#24577;&#24615;&#65292;&#20026;&#27599;&#20010;&#31245;&#26377;&#19981;&#21516;&#30340;&#24773;&#20917;&#37325;&#26032;&#33719;&#24471;&#25152;&#38656;&#25968;&#25454;&#36827;&#34892;&#26631;&#20934;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#25552;&#20379;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#30456;&#20851;&#20219;&#21153;&#30340;&#30693;&#35782;&#21644;&#32771;&#34385;&#25968;&#25454;&#20998;&#24067;&#30340;&#21464;&#21270;&#65292;&#36825;&#20010;&#23398;&#20064;&#26694;&#26550;&#21487;&#20197;&#35299;&#20915;&#26032;&#20219;&#21153;&#65292;&#21363;&#20351;&#27809;&#26377;&#25110;&#21482;&#26377;&#24456;&#23569;&#30340;&#38468;&#21152;&#26631;&#35760;&#25968;&#25454;&#12290;&#36825;&#31181;&#26041;&#27861;&#36991;&#20813;&#20102;&#20174;&#22836;&#24320;&#22987;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#30340;&#38656;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automating the monitoring of industrial processes has the potential to enhance efficiency and optimize quality by promptly detecting abnormal events and thus facilitating timely interventions. Deep learning, with its capacity to discern non-trivial patterns within large datasets, plays a pivotal role in this process. Standard deep learning methods are suitable to solve a specific task given a specific type of data. During training, the algorithms demand large volumes of labeled training data. However, due to the dynamic nature of processes and the environment, it is impractical to acquire the needed data for standard deep learning training for every slightly different case anew. Deep transfer learning offers a solution to this problem. By leveraging knowledge from related tasks and accounting for variations in data distributions, this learning framework solves new tasks even with little or no additional labeled data. The approach bypasses the need to retrain a model from scratch for ev
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Crossway Diffusion&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#30446;&#26631;&#22686;&#24378;&#20102;&#22522;&#20110;&#25193;&#25955;&#30340;&#35270;&#35273;&#26426;&#22120;&#20154;&#31574;&#30053;&#23398;&#20064;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#21508;&#31181;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2307.01849</link><description>&lt;p&gt;
&#20132;&#21449;&#25193;&#25955;&#65306;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#25913;&#36827;&#22522;&#20110;&#25193;&#25955;&#30340;&#35270;&#35273;&#26426;&#22120;&#20154;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Crossway Diffusion: Improving Diffusion-based Visuomotor Policy via Self-supervised Learning. (arXiv:2307.01849v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01849
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Crossway Diffusion&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#30446;&#26631;&#22686;&#24378;&#20102;&#22522;&#20110;&#25193;&#25955;&#30340;&#35270;&#35273;&#26426;&#22120;&#20154;&#31574;&#30053;&#23398;&#20064;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#21508;&#31181;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24207;&#21015;&#24314;&#27169;&#26041;&#27861;&#22312;&#26426;&#22120;&#20154;&#27169;&#20223;&#23398;&#20064;&#20013;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#26368;&#36817;&#65292;&#25193;&#25955;&#27169;&#22411;&#24050;&#32463;&#34987;&#37319;&#29992;&#20110;&#34892;&#20026;&#20811;&#38534;&#20013;&#65292;&#24182;&#20174;&#20854;&#22312;&#24314;&#27169;&#22797;&#26434;&#25968;&#25454;&#20998;&#24067;&#26041;&#38754;&#30340;&#29305;&#24322;&#33021;&#21147;&#20013;&#33719;&#30410;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Crossway Diffusion&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#39069;&#22806;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#30446;&#26631;&#26469;&#22686;&#24378;&#22522;&#20110;&#25193;&#25955;&#30340;&#35270;&#35273;&#26426;&#22120;&#20154;&#31574;&#30053;&#23398;&#20064;&#12290;&#26631;&#20934;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#31574;&#30053;&#20174;&#38543;&#26426;&#22122;&#22768;&#20013;&#29983;&#25104;&#21160;&#20316;&#24207;&#21015;&#65292;&#26465;&#20214;&#26159;&#35270;&#35273;&#35266;&#27979;&#21644;&#20854;&#20182;&#20302;&#32500;&#29366;&#24577;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25193;&#23637;&#20102;&#36825;&#19968;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#35299;&#30721;&#22120;&#65292;&#20174;&#21453;&#21521;&#25193;&#25955;&#36807;&#31243;&#30340;&#20013;&#38388;&#34920;&#31034;&#20013;&#37325;&#26500;&#21407;&#22987;&#22270;&#20687;&#20687;&#32032;&#65288;&#21644;&#20854;&#20182;&#29366;&#24577;&#20449;&#24687;&#65289;&#65292;&#24182;&#20351;&#29992;SSL&#25439;&#22833;&#32852;&#21512;&#35757;&#32451;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;Crossway Diffusion&#22312;&#21508;&#31181;&#27169;&#25311;&#21644;&#30495;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#39564;&#35777;&#20102;&#20854;&#30456;&#23545;&#20110;&#26631;&#20934;&#22522;&#20110;&#25193;&#25955;&#30340;&#31574;&#30053;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequence modeling approaches have shown promising results in robot imitation learning. Recently, diffusion models have been adopted for behavioral cloning, benefiting from their exceptional capabilities in modeling complex data distribution. In this work, we propose Crossway Diffusion, a method to enhance diffusion-based visuomotor policy learning by using an extra self-supervised learning (SSL) objective. The standard diffusion-based policy generates action sequences from random noise conditioned on visual observations and other low-dimensional states. We further extend this by introducing a new decoder that reconstructs raw image pixels (and other state information) from the intermediate representations of the reverse diffusion process, and train the model jointly using the SSL loss. Our experiments demonstrate the effectiveness of Crossway Diffusion in various simulated and real-world robot tasks, confirming its advantages over the standard diffusion-based policy. We demonstrate tha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38236;&#38754;&#19979;&#38477;&#26041;&#27861;&#26469;&#32479;&#19968;&#25511;&#21046;&#22238;&#24402;&#21644;&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#30340;&#26041;&#27861;&#65292;&#22312;&#25152;&#26377;&#26631;&#20934;&#20960;&#20309;&#19979;&#37117;&#21487;&#20197;&#23454;&#29616;$\ell_p$&#65288;$p\in[1,\infty]$&#65289;&#33539;&#24335;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#65292;&#24182;&#19988;&#21487;&#20197;&#23454;&#29616;&#23398;&#20064;&#29702;&#35770;&#20013;&#35768;&#22810;&#29305;&#27530;&#31867;&#30340;&#25511;&#21046;&#30340;&#27867;&#21270;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2306.13853</link><description>&lt;p&gt;
&#19968;&#31181;&#36890;&#36807;&#38236;&#38754;&#19979;&#38477;&#25511;&#21046;&#38544;&#24335;&#27491;&#21017;&#21270;&#30340;&#32479;&#19968;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Unified Approach to Controlling Implicit Regularization via Mirror Descent. (arXiv:2306.13853v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13853
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38236;&#38754;&#19979;&#38477;&#26041;&#27861;&#26469;&#32479;&#19968;&#25511;&#21046;&#22238;&#24402;&#21644;&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#30340;&#26041;&#27861;&#65292;&#22312;&#25152;&#26377;&#26631;&#20934;&#20960;&#20309;&#19979;&#37117;&#21487;&#20197;&#23454;&#29616;$\ell_p$&#65288;$p\in[1,\infty]$&#65289;&#33539;&#24335;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#65292;&#24182;&#19988;&#21487;&#20197;&#23454;&#29616;&#23398;&#20064;&#29702;&#35770;&#20013;&#35768;&#22810;&#29305;&#27530;&#31867;&#30340;&#25511;&#21046;&#30340;&#27867;&#21270;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26174;&#33879;&#25104;&#21151;&#21551;&#21457;&#65292;&#20154;&#20204;&#23545;&#36229;&#21442;&#25968;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#20135;&#29983;&#20102;&#26497;&#22823;&#30340;&#20852;&#36259;&#12290;&#20154;&#20204;&#33457;&#36153;&#20102;&#22823;&#37327;&#31934;&#21147;&#26469;&#30830;&#23450;&#20248;&#21270;&#31639;&#27861;&#36890;&#36807;&#20854;&#8220;&#39318;&#36873;&#8221;&#35299;&#22914;&#20309;&#24433;&#21709;&#27867;&#21270;&#65292;&#36825;&#31181;&#29616;&#35937;&#36890;&#24120;&#34987;&#31216;&#20026;&#38544;&#24335;&#27491;&#21017;&#21270;&#12290;&#29305;&#21035;&#22320;&#65292;&#24050;&#32463;&#26377;&#20154;&#35770;&#35777;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#22312;&#22238;&#24402;&#21644;&#20998;&#31867;&#38382;&#39064;&#20013;&#20250;&#24341;&#36215;&#38544;&#24335;&#30340;$\ell_2$ -&#33539;&#25968;&#27491;&#21017;&#21270;&#12290;&#28982;&#32780;&#65292;&#19981;&#21516;&#31639;&#27861;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#21463;&#38480;&#20110;&#29305;&#23450;&#30340;&#20960;&#20309;&#25110;&#29305;&#23450;&#31867;&#30340;&#23398;&#20064;&#38382;&#39064;&#65292;&#34920;&#26126;&#38656;&#35201;&#19968;&#20010;&#36890;&#29992;&#30340;&#26041;&#27861;&#26469;&#25511;&#21046;&#38544;&#24335;&#27491;&#21017;&#21270;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#38236;&#38754;&#19979;&#38477;&#65288;MD&#65289;&#26469;&#25511;&#21046;&#22238;&#24402;&#21644;&#20998;&#31867;&#35774;&#32622;&#20013;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;MD&#19982;&#36890;&#29992;&#30340;&#19979;&#38477;&#26041;&#21521;&#19968;&#36215;&#20351;&#29992;&#26102;&#65292;&#22312;&#25152;&#26377;&#26631;&#20934;&#20960;&#20309;&#19979;&#37117;&#21487;&#20197;&#23454;&#29616;$\ell_p$&#65288;$p\in[1,\infty]$&#65289;&#33539;&#24335;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#65292;&#24182;&#19988;&#22312;&#23398;&#20064;&#29702;&#35770;&#20013;&#20351;&#29992;&#30340;&#35768;&#22810;&#29305;&#27530;&#31867;&#20013;&#20063;&#21487;&#20197;&#23454;&#29616;&#25511;&#21046;&#30340;&#27867;&#21270;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspired by the remarkable success of deep neural networks, there has been significant interest in understanding the generalization performance of overparameterized models. Substantial efforts have been invested in characterizing how optimization algorithms impact generalization through their "preferred" solutions, a phenomenon commonly referred to as implicit regularization. In particular, it has been argued that gradient descent (GD) induces an implicit $\ell_2$-norm regularization in regression and classification problems. However, the implicit regularization of different algorithms are confined to either a specific geometry or a particular class of learning problems, indicating a gap in a general approach for controlling the implicit regularization. To address this, we present a unified approach using mirror descent (MD), a notable generalization of GD, to control implicit regularization in both regression and classification settings. More specifically, we show that MD with the gen
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28155;&#21152;&#22122;&#22768;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;Gibbs&#37319;&#26679;&#22120;&#20174;&#21518;&#39564;&#20998;&#24067;&#20013;&#36827;&#34892;&#37319;&#26679;&#65292;&#35813;&#26041;&#27861;&#22312;&#30495;&#23454;&#25968;&#25454;&#21644;&#21512;&#25104;&#25968;&#25454;&#20013;&#33021;&#22815;&#36798;&#21040;&#31867;&#20284;&#20110;&#39532;&#23572;&#31185;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.02729</link><description>&lt;p&gt;
Gibbs&#37319;&#26679;&#31070;&#32463;&#32593;&#32476;&#30340;&#21518;&#39564;&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;
Gibbs Sampling the Posterior of Neural Networks. (arXiv:2306.02729v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02729
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28155;&#21152;&#22122;&#22768;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;Gibbs&#37319;&#26679;&#22120;&#20174;&#21518;&#39564;&#20998;&#24067;&#20013;&#36827;&#34892;&#37319;&#26679;&#65292;&#35813;&#26041;&#27861;&#22312;&#30495;&#23454;&#25968;&#25454;&#21644;&#21512;&#25104;&#25968;&#25454;&#20013;&#33021;&#22815;&#36798;&#21040;&#31867;&#20284;&#20110;&#39532;&#23572;&#31185;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#31070;&#32463;&#32593;&#32476;&#30340;&#21518;&#39564;&#20998;&#24067;&#20013;&#36827;&#34892;&#37319;&#26679;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27010;&#29575;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#32593;&#32476;&#30340;&#27599;&#20010;&#39044;&#28608;&#27963;&#21644;&#21518;&#28608;&#27963;&#20013;&#28155;&#21152;&#22122;&#22768;&#65292;&#24182;&#35748;&#20026;&#20351;&#29992;&#26377;&#25928;&#30340;Gibbs&#37319;&#26679;&#22120;&#21487;&#20197;&#37319;&#26679;&#24471;&#21040;&#25152;&#24471;&#21040;&#30340;&#21518;&#39564;&#20998;&#24067;&#12290;&#22312;&#30495;&#23454;&#25968;&#25454;&#21644;&#21512;&#25104;&#25968;&#25454;&#19978;&#65292;Gibbs&#37319;&#26679;&#22120;&#33021;&#22815;&#36798;&#21040;&#31867;&#20284;&#20110;&#29366;&#24577;-of-the-art&#30340;&#39532;&#23572;&#31185;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#65288;&#22914;&#21704;&#23494;&#39039;&#33945;&#29305;&#21345;&#27931;&#25110;Metropolis&#35843;&#25972;Langevin&#31639;&#27861;&#65289;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#22312;&#24072;&#29983;&#35774;&#32622;&#20013;&#36827;&#34892;&#20998;&#26512;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#28909;&#21270;&#20934;&#21017;&#65292;&#35813;&#20934;&#21017;&#20801;&#35768;&#25105;&#20204;&#26816;&#27979;&#31639;&#27861;&#22312;&#20351;&#29992;&#21512;&#25104;&#26631;&#31614;&#30340;&#25968;&#25454;&#19978;&#36816;&#34892;&#26102;&#26159;&#21542;&#26080;&#27861;&#20174;&#21518;&#39564;&#20998;&#24067;&#20013;&#37319;&#26679;&#12290;&#35813;&#20934;&#21017;&#22522;&#20110;&#24072;&#29983;&#35774;&#32622;&#20013;&#30340;&#20107;&#23454;&#65292;&#25105;&#20204;&#21487;&#20197;&#30452;&#25509;&#22312;&#24179;&#34913;&#28857;&#22788;&#21021;&#22987;&#21270;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study sampling from a posterior derived from a neural network. We propose a new probabilistic model consisting of adding noise at every pre- and post-activation in the network, arguing that the resulting posterior can be sampled using an efficient Gibbs sampler. The Gibbs sampler attains similar performances as the state-of-the-art Monte Carlo Markov chain methods, such as the Hamiltonian Monte Carlo or the Metropolis adjusted Langevin algorithm, both on real and synthetic data. By framing our analysis in the teacher-student setting, we introduce a thermalization criterion that allows us to detect when an algorithm, when run on data with synthetic labels, fails to sample from the posterior. The criterion is based on the fact that in the teacher-student setting we can initialize an algorithm directly at equilibrium.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;&#25239;&#24178;&#25200;&#32422;&#26463;&#23398;&#20064;&#8221;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22312;&#37096;&#32626;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#26102;&#38656;&#35201;&#28385;&#36275;&#38500;&#20102;&#20934;&#30830;&#24615;&#20197;&#22806;&#30340;&#22810;&#20010;&#35201;&#27714;&#65292;&#24182;&#20197;&#24179;&#34913;&#20174;&#25918;&#23485;&#20013;&#33719;&#24471;&#30340;&#24615;&#33021;&#22686;&#30410;&#19982;&#29992;&#25143;&#23450;&#20041;&#30340;&#25918;&#23485;&#25104;&#26412;&#20043;&#38388;&#30340;&#20851;&#31995;&#30340;&#26041;&#24335;&#25918;&#26494;&#23398;&#20064;&#32422;&#26463;&#12290;</title><link>http://arxiv.org/abs/2306.02426</link><description>&lt;p&gt;
&#25239;&#24178;&#25200;&#32422;&#26463;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Resilient Constrained Learning. (arXiv:2306.02426v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02426
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;&#25239;&#24178;&#25200;&#32422;&#26463;&#23398;&#20064;&#8221;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22312;&#37096;&#32626;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#26102;&#38656;&#35201;&#28385;&#36275;&#38500;&#20102;&#20934;&#30830;&#24615;&#20197;&#22806;&#30340;&#22810;&#20010;&#35201;&#27714;&#65292;&#24182;&#20197;&#24179;&#34913;&#20174;&#25918;&#23485;&#20013;&#33719;&#24471;&#30340;&#24615;&#33021;&#22686;&#30410;&#19982;&#29992;&#25143;&#23450;&#20041;&#30340;&#25918;&#23485;&#25104;&#26412;&#20043;&#38388;&#30340;&#20851;&#31995;&#30340;&#26041;&#24335;&#25918;&#26494;&#23398;&#20064;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37096;&#32626;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#26102;&#65292;&#38500;&#20102;&#20934;&#30830;&#24615;&#20043;&#22806;&#65292;&#23427;&#20204;&#24517;&#39035;&#28385;&#36275;&#22810;&#20010;&#35201;&#27714;&#65292;&#22914;&#20844;&#24179;&#24615;&#12289;&#40065;&#26834;&#24615;&#25110;&#23433;&#20840;&#24615;&#12290;&#36825;&#20123;&#35201;&#27714;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#24809;&#32602;&#26469;&#38544;&#24335;&#22320;&#26045;&#21152;&#65292;&#25110;&#32773;&#36890;&#36807;&#22522;&#20110;Lagrangian&#23545;&#20598;&#30340;&#32422;&#26463;&#20248;&#21270;&#26041;&#27861;&#26469;&#26174;&#24335;&#22320;&#26045;&#21152;&#12290;&#26080;&#35770;&#21738;&#31181;&#26041;&#24335;&#65292;&#25351;&#23450;&#35201;&#27714;&#37117;&#21463;&#21040;&#22949;&#21327;&#21644;&#26377;&#38480;&#30340;&#26377;&#20851;&#25968;&#25454;&#30340;&#20808;&#21069;&#30693;&#35782;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#36890;&#24120;&#21482;&#33021;&#36890;&#36807;&#23454;&#38469;&#35299;&#20915;&#23398;&#20064;&#38382;&#39064;&#26469;&#35780;&#20272;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32422;&#26463;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#21516;&#26102;&#35299;&#20915;&#23398;&#20064;&#20219;&#21153;&#30340;&#21516;&#26102;&#35843;&#25972;&#35201;&#27714;&#12290;&#20026;&#27492;&#65292;&#23427;&#20197;&#24179;&#34913;&#20174;&#25918;&#23485;&#20013;&#33719;&#24471;&#30340;&#24615;&#33021;&#22686;&#30410;&#19982;&#29992;&#25143;&#23450;&#20041;&#30340;&#25918;&#23485;&#25104;&#26412;&#20043;&#38388;&#30340;&#20851;&#31995;&#30340;&#26041;&#24335;&#25918;&#26494;&#20102;&#23398;&#20064;&#32422;&#26463;&#12290;&#25105;&#20204;&#23558;&#27492;&#26041;&#27861;&#31216;&#20026;&#20855;&#26377;&#24377;&#24615;&#30340;&#32422;&#26463;&#23398;&#20064;&#65292;&#36825;&#26159;&#23545;&#29992;&#20110;&#25551;&#36848;&#29983;&#24577;&#31995;&#32479;&#30340;&#26415;&#35821;&#30340;&#19968;&#31181;&#20511;&#37492;&#12290;
&lt;/p&gt;
&lt;p&gt;
When deploying machine learning solutions, they must satisfy multiple requirements beyond accuracy, such as fairness, robustness, or safety. These requirements are imposed during training either implicitly, using penalties, or explicitly, using constrained optimization methods based on Lagrangian duality. Either way, specifying requirements is hindered by the presence of compromises and limited prior knowledge about the data. Furthermore, their impact on performance can often only be evaluated by actually solving the learning problem. This paper presents a constrained learning approach that adapts the requirements while simultaneously solving the learning task. To do so, it relaxes the learning constraints in a way that contemplates how much they affect the task at hand by balancing the performance gains obtained from the relaxation against a user-defined cost of that relaxation. We call this approach resilient constrained learning after the term used to describe ecological systems tha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#20844;&#24320;&#25968;&#25454;&#38598;&#21644;&#39044;&#35757;&#32451;&#30340;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#65292;&#32467;&#21512;&#31169;&#26377;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#23454;&#29616;&#29983;&#25104;&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#24615;&#36136;&#30340;&#21512;&#25104;&#25991;&#26412;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#12290;&#29983;&#25104;&#30340;&#25991;&#26412;&#25968;&#25454;&#38598;&#21487;&#20197;&#37325;&#22797;&#20351;&#29992;&#20110;&#20854;&#20182;&#20219;&#21153;&#65292;&#21487;&#26080;&#38480;&#26399;&#20445;&#30041;&#65292;&#25110;&#19982;&#31532;&#19977;&#26041;&#20849;&#20139;&#32780;&#19981;&#25439;&#22833;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2306.01684</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#31169;&#26377;&#30340;&#21512;&#25104;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
Harnessing large-language models to generate private synthetic text. (arXiv:2306.01684v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01684
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#20844;&#24320;&#25968;&#25454;&#38598;&#21644;&#39044;&#35757;&#32451;&#30340;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#65292;&#32467;&#21512;&#31169;&#26377;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#23454;&#29616;&#29983;&#25104;&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#24615;&#36136;&#30340;&#21512;&#25104;&#25991;&#26412;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#12290;&#29983;&#25104;&#30340;&#25991;&#26412;&#25968;&#25454;&#38598;&#21487;&#20197;&#37325;&#22797;&#20351;&#29992;&#20110;&#20854;&#20182;&#20219;&#21153;&#65292;&#21487;&#26080;&#38480;&#26399;&#20445;&#30041;&#65292;&#25110;&#19982;&#31532;&#19977;&#26041;&#20849;&#20139;&#32780;&#19981;&#25439;&#22833;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#35757;&#32451;&#26041;&#27861;&#65292;&#22914;DP-SGD&#65292;&#21487;&#20197;&#36890;&#36807;&#30830;&#20445;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#19981;&#20250;&#36879;&#38706;&#31169;&#26377;&#20449;&#24687;&#26469;&#20445;&#25252;&#25935;&#24863;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65292;&#21033;&#29992;&#25935;&#24863;&#25968;&#25454;&#38598;&#29983;&#25104;&#26032;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#24182;&#30830;&#20445;&#30456;&#23545;&#20110;&#21407;&#22987;&#25968;&#25454;&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#12290;&#36825;&#26679;&#20570;&#26377;&#20960;&#20010;&#20248;&#28857;&#65306;&#21512;&#25104;&#25968;&#25454;&#21487;&#20197;&#37325;&#22797;&#20351;&#29992;&#20110;&#20854;&#20182;&#20219;&#21153;&#65288;&#21253;&#25324;&#36229;&#21442;&#25968;&#35843;&#25972;&#65289;&#65292;&#21487;&#20197;&#26080;&#38480;&#26399;&#20445;&#30041;&#65292;&#25110;&#19982;&#31532;&#19977;&#26041;&#20849;&#20139;&#32780;&#19981;&#25439;&#22833;&#38544;&#31169;&#12290;&#20294;&#26159;&#65292;&#33719;&#21462;&#24046;&#20998;&#38544;&#31169;&#25968;&#25454;&#27604;&#22312;&#35757;&#32451;&#26399;&#38388;&#24341;&#20837;&#24046;&#20998;&#38544;&#31169;&#26356;&#21152;&#22256;&#38590;&#12290;&#20026;&#20102;&#20351;&#20854;&#22312;&#25991;&#26412;&#20013;&#21487;&#34892;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#21033;&#29992;&#20844;&#20849;&#25968;&#25454;&#65292;&#20174;&#39044;&#35757;&#32451;&#30340;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#24320;&#22987;&#65292;&#24182;&#22312;&#25935;&#24863;&#25968;&#25454;&#19978;&#36827;&#34892;&#31169;&#20154;&#35843;&#25972;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#29992;&#20110;&#25277;&#26679;&#24046;&#20998;&#38544;&#31169;&#21512;&#25104;&#25968;&#25454;&#38598;&#12290;&#34429;&#28982;&#36825;&#20010;&#31574;&#30053;&#20284;&#20046;&#24456;&#31616;&#21333;&#65292;&#20294;&#25191;&#34892;&#23427;&#24050;&#34987;&#35777;&#26126;&#26159;&#26377;&#38382;&#39064;&#30340;&#12290;&#20197;&#21069;&#30340;&#26041;&#27861;&#35201;&#20040;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#25439;&#22833;&#65292;&#35201;&#20040;...
&lt;/p&gt;
&lt;p&gt;
Differentially private (DP) training methods like DP-SGD can protect sensitive training data by ensuring that ML models will not reveal private information. An alternative approach, which this paper studies, is to use a sensitive dataset to generate a new synthetic dataset which is differentially private with respect to the original data. Doing so has several advantages: synthetic data can be reused for other tasks (including for hyper parameter tuning), retained indefinitely, or shared with third parties without sacrificing privacy.  However, obtaining DP data is much harder than introducing DP during training. To make it feasible for text, recent work has utilized public data by starting with a pre-trained generative language model and privately finetuning it on sensitive data. This model can be used to sample a DP synthetic dataset. While this strategy seems straightforward, executing it has proven problematic. Previous approaches either show significant performance loss, or have, a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36830;&#32493;&#24615;&#32467;&#26524;&#30340;&#37096;&#20998;&#21453;&#20107;&#23454;&#35782;&#21035;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25935;&#24863;&#24615;&#27169;&#22411;&#8212;&#8212;&#26354;&#29575;&#25935;&#24863;&#27169;&#22411;&#65292;&#36890;&#36807;&#38480;&#21046;&#20989;&#25968;&#32423;&#38598;&#30340;&#26354;&#29575;&#26469;&#33719;&#24471;&#20449;&#24687;&#36793;&#30028;&#12290;</title><link>http://arxiv.org/abs/2306.01424</link><description>&lt;p&gt;
&#24102;&#26354;&#29575;&#25935;&#24863;&#27169;&#22411;&#30340;&#36830;&#32493;&#24615;&#32467;&#26524;&#30340;&#37096;&#20998;&#21453;&#20107;&#23454;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Partial Counterfactual Identification of Continuous Outcomes with a Curvature Sensitivity Model. (arXiv:2306.01424v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36830;&#32493;&#24615;&#32467;&#26524;&#30340;&#37096;&#20998;&#21453;&#20107;&#23454;&#35782;&#21035;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25935;&#24863;&#24615;&#27169;&#22411;&#8212;&#8212;&#26354;&#29575;&#25935;&#24863;&#27169;&#22411;&#65292;&#36890;&#36807;&#38480;&#21046;&#20989;&#25968;&#32423;&#38598;&#30340;&#26354;&#29575;&#26469;&#33719;&#24471;&#20449;&#24687;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#25512;&#26029;&#26088;&#22312;&#22238;&#31572;&#8220;&#22914;&#26524;&#8221;&#38382;&#39064;&#65292;&#22240;&#27492;&#23646;&#20110;Pearl&#22240;&#26524;&#20851;&#31995;&#38454;&#26799;&#20013;&#26368;&#31934;&#32454;&#30340;&#25512;&#29702;&#31867;&#22411;&#12290;&#29616;&#26377;&#30340;&#38024;&#23545;&#20855;&#26377;&#36830;&#32493;&#32467;&#26524;&#30340;&#21453;&#20107;&#23454;&#25512;&#26029;&#26041;&#27861;&#26088;&#22312;&#36827;&#34892;&#28857;&#35782;&#21035;&#65292;&#22240;&#27492;&#23545;&#22522;&#30784;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#36827;&#34892;&#20102;&#24378;&#26377;&#21147;&#19988;&#19981;&#33258;&#28982;&#30340;&#20551;&#35774;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25918;&#23485;&#20102;&#36825;&#20123;&#20551;&#35774;&#65292;&#26088;&#22312;&#36827;&#34892;&#36830;&#32493;&#32467;&#26524;&#30340;&#37096;&#20998;&#21453;&#20107;&#23454;&#35782;&#21035;&#65292;&#21363;&#24403;&#21453;&#20107;&#23454;&#26597;&#35810;&#23384;&#22312;&#20855;&#26377;&#20449;&#24687;&#36793;&#30028;&#30340;&#26080;&#30693;&#21306;&#38388;&#20013;&#26102;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#65292;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#65292;&#21363;&#20351;&#26159;&#36830;&#32493;&#21487;&#24494;&#30340;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#20989;&#25968;&#30340;&#32423;&#38598;&#30340;&#26354;&#29575;&#20063;&#26159;&#38750;&#20449;&#24687;&#30340;&#65292;&#21453;&#20107;&#23454;&#26597;&#35810;&#30340;&#26080;&#30693;&#21306;&#38388;&#20063;&#26159;&#38750;&#20449;&#24687;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25935;&#24863;&#24615;&#27169;&#22411;&#31216;&#20026;&#26354;&#29575;&#25935;&#24863;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#23427;&#20801;&#35768;&#25105;&#20204;&#36890;&#36807;&#38480;&#21046;&#20989;&#25968;&#32423;&#38598;&#30340;&#26354;&#29575;&#26469;&#33719;&#24471;&#20449;&#24687;&#36793;&#30028;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#29616;&#26377;&#30340;&#28857;&#21453;&#20107;&#23454;&#35782;&#21035;&#26041;&#27861;&#21487;&#20197;&#35270;&#20026;&#25105;&#20204;&#25552;&#20986;&#26694;&#26550;&#30340;&#29305;&#23450;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual inference aims to answer retrospective ''what if'' questions and thus belongs to the most fine-grained type of inference in Pearl's causality ladder. Existing methods for counterfactual inference with continuous outcomes aim at point identification and thus make strong and unnatural assumptions about the underlying structural causal model. In this paper, we relax these assumptions and aim at partial counterfactual identification of continuous outcomes, i.e., when the counterfactual query resides in an ignorance interval with informative bounds. We prove that, in general, the ignorance interval of the counterfactual queries has non-informative bounds, already when functions of structural causal models are continuously differentiable. As a remedy, we propose a novel sensitivity model called Curvature Sensitivity Model. This allows us to obtain informative bounds by bounding the curvature of level sets of the functions. We further show that existing point counterfactual ide
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#39046;&#22495;&#30693;&#35782;&#30340;&#33647;&#29289;&#25512;&#33616;&#26694;&#26550;DKINet&#65292;&#23558;&#39046;&#22495;&#30693;&#35782;&#19982;&#24739;&#32773;&#20020;&#24202;&#34920;&#29616;&#30456;&#32467;&#21512;&#65292;&#27492;&#20026;&#39318;&#27425;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2305.19604</link><description>&lt;p&gt;
&#36890;&#36807;&#39046;&#22495;&#30693;&#35782;&#21551;&#31034;&#30340;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#33647;&#29289;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Medication Recommendation via Domain Knowledge Informed Deep Learning. (arXiv:2305.19604v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19604
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#39046;&#22495;&#30693;&#35782;&#30340;&#33647;&#29289;&#25512;&#33616;&#26694;&#26550;DKINet&#65292;&#23558;&#39046;&#22495;&#30693;&#35782;&#19982;&#24739;&#32773;&#20020;&#24202;&#34920;&#29616;&#30456;&#32467;&#21512;&#65292;&#27492;&#20026;&#39318;&#27425;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;&#25512;&#33616;&#26159;&#21307;&#30103;&#20445;&#20581;&#30340;&#22522;&#26412;&#20294;&#33267;&#20851;&#37325;&#35201;&#30340;&#20998;&#25903;&#65292;&#25552;&#20379;&#26426;&#20250;&#20026;&#22797;&#26434;&#20581;&#24247;&#29366;&#20917;&#30340;&#24739;&#32773;&#25903;&#25345;&#20020;&#24202;&#21307;&#29983;&#26356;&#31934;&#30830;&#30340;&#33647;&#29289;&#22788;&#26041;&#12290;&#20174;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#20013;&#23398;&#20064;&#25512;&#33616;&#33647;&#29289;&#26159;&#20808;&#21069;&#30740;&#31350;&#20013;&#26368;&#24120;&#35265;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#24573;&#35270;&#20102;&#26681;&#25454;&#24739;&#32773;&#30340;EHR&#20013;&#30340;&#20020;&#24202;&#34920;&#29616;&#32435;&#20837;&#39046;&#22495;&#30693;&#35782;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#21160;&#24577;&#39046;&#22495;&#30693;&#35782;&#30340;&#33647;&#29289;&#25512;&#33616;&#26694;&#26550;&#65292;&#21363;&#39046;&#22495;&#30693;&#35782;&#21551;&#31034;&#32593;&#32476;&#65288;DKINet&#65289;&#65292;&#29992;&#20110;&#23558;&#39046;&#22495;&#30693;&#35782;&#19982;&#21487;&#35266;&#23519;&#30340;&#24739;&#32773;&#20020;&#24202;&#34920;&#29616;&#30456;&#32467;&#21512;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#39046;&#22495;&#30693;&#35782;&#30340;&#32534;&#30721;&#22120;&#26469;&#25429;&#25417;&#39046;&#22495;&#20449;&#24687;&#65292;&#28982;&#21518;&#24320;&#21457;&#20102;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#30340;&#32534;&#30721;&#22120;&#23558;&#39046;&#22495;&#30693;&#35782;&#25972;&#21512;&#21040;&#21487;&#35266;&#23519;&#30340;EHR&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medication recommendation is a fundamental yet crucial branch of healthcare, which provides opportunities to support clinical physicians with more accurate medication prescriptions for patients with complex health conditions. Learning from electronic health records (EHR) to recommend medications is the most common way in previous studies. However, most of them neglect incorporating domain knowledge according to the clinical manifestations in the EHR of the patient. To address these issues, we propose a novel \textbf{D}omain \textbf{K}nowledge \textbf{I}nformed \textbf{Net}work (DKINet) to integrate domain knowledge with observable clinical manifestations of the patient, which is the first dynamic domain knowledge informed framework toward medication recommendation. In particular, we first design a knowledge-driven encoder to capture the domain information and then develop a data-driven encoder to integrate domain knowledge into the observable EHR. To endow the model with the capability
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;WiFi&#20449;&#21495;&#30340;&#20154;&#20307;&#20132;&#20114;&#35782;&#21035;&#26041;&#27861;WiFi-TCN&#12290;&#20256;&#32479;&#22522;&#20110;WiFi&#30340;&#26041;&#27861;&#23384;&#22312;&#22330;&#26223;&#25110;&#23545;&#35937;&#21464;&#21270;&#26102;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#65292;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#38656;&#35201;&#20351;&#29992;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#25104;&#26412;&#20302;&#12289;&#37096;&#32626;&#31616;&#26131;&#31561;&#20248;&#21183;&#65292;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#22522;&#20110;WiFi&#30340;&#20154;&#20307;&#20132;&#20114;&#35782;&#21035;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.18211</link><description>&lt;p&gt;
WiFi-TCN&#65306;&#22522;&#20110;WiFi&#20449;&#21495;&#30340;&#20154;&#20307;&#20132;&#20114;&#35782;&#21035;&#30340;&#26102;&#38388;&#21367;&#31215;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
WiFi-TCN: Temporal Convolution for Human Interaction Recognition based on WiFi signal. (arXiv:2305.18211v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18211
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;WiFi&#20449;&#21495;&#30340;&#20154;&#20307;&#20132;&#20114;&#35782;&#21035;&#26041;&#27861;WiFi-TCN&#12290;&#20256;&#32479;&#22522;&#20110;WiFi&#30340;&#26041;&#27861;&#23384;&#22312;&#22330;&#26223;&#25110;&#23545;&#35937;&#21464;&#21270;&#26102;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#65292;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#38656;&#35201;&#20351;&#29992;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#25104;&#26412;&#20302;&#12289;&#37096;&#32626;&#31616;&#26131;&#31561;&#20248;&#21183;&#65292;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#22522;&#20110;WiFi&#30340;&#20154;&#20307;&#20132;&#20114;&#35782;&#21035;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;Wi-Fi&#30340;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#22312;&#35832;&#22810;&#39046;&#22495;&#22914;&#21307;&#30103;&#20445;&#20581;&#12289;&#23433;&#38450;&#21644;&#32769;&#24180;&#25252;&#29702;&#31561;&#26041;&#38754;&#24471;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#30456;&#27604;&#20110;&#20381;&#36182;&#25668;&#20687;&#22836;&#21644;&#20256;&#24863;&#22120;&#30340;&#20256;&#32479;&#26041;&#27861;&#65292;&#22522;&#20110;Wi-Fi&#30340;&#26041;&#27861;&#20855;&#26377;&#25104;&#26412;&#20302;&#12289;&#37096;&#32626;&#31616;&#26131;&#31561;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;Wi-Fi&#30340;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#38754;&#20020;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#65292;&#21363;&#24403;&#22330;&#26223;&#25110;&#23545;&#35937;&#21457;&#29983;&#21464;&#21270;&#26102;&#65292;&#24615;&#33021;&#26126;&#26174;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#37325;&#35201;&#30340;&#26159;&#20351;&#29992;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20013;&#65292;&#20351;&#29992;&#22522;&#20110;CNN&#25110;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#22914;LSTM&#12289;GRU&#25110;Transformer&#30340;&#26041;&#27861;&#21464;&#24471;&#27969;&#34892;&#12290;&#34429;&#28982;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#21487;&#20197;&#26356;&#31934;&#30830;&#65292;&#20294;&#23427;&#20204;&#30340;&#35745;&#31639;&#37327;&#26356;&#22823;&#65292;&#38656;&#35201;&#26356;&#22810;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The utilization of Wi-Fi based human activity recognition has gained considerable interest in recent times, primarily owing to its applications in various domains such as healthcare for monitoring breath and heart rate, security, elderly care. These Wi-Fi-based methods exhibit several advantages over conventional state-of-the-art techniques that rely on cameras and sensors, including lower costs and ease of deployment. However, a significant challenge associated with Wi-Fi-based HAR is the significant decline in performance when the scene or subject changes. To mitigate this issue, it is imperative to train the model using an extensive dataset. In recent studies, the utilization of CNN-based models or sequence-to-sequence models such as LSTM, GRU, or Transformer has become prevalent. While sequence-to-sequence models can be more precise, they are also more computationally intensive and require a larger amount of training data. To tackle these limitations, we propose a novel approach th
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23384;&#39640;&#25928;&#30340;&#38646;&#38454;&#20248;&#21270;&#22120;&#65292;&#21487;&#20197;&#20351;&#29992;&#19982;&#25512;&#29702;&#30456;&#21516;&#30340;&#23384;&#20648;&#31354;&#38388;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#65292;&#20854;&#21487;&#20197;&#22312;&#22823;&#35268;&#27169;&#27169;&#22411;&#19979;&#26356;&#24555;&#22320;&#20248;&#21270;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.17333</link><description>&lt;p&gt;
&#21482;&#20351;&#29992;&#21069;&#21521;&#20256;&#36882;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Fine-Tuning Language Models with Just Forward Passes. (arXiv:2305.17333v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17333
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23384;&#39640;&#25928;&#30340;&#38646;&#38454;&#20248;&#21270;&#22120;&#65292;&#21487;&#20197;&#20351;&#29992;&#19982;&#25512;&#29702;&#30456;&#21516;&#30340;&#23384;&#20648;&#31354;&#38388;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#65292;&#20854;&#21487;&#20197;&#22312;&#22823;&#35268;&#27169;&#27169;&#22411;&#19979;&#26356;&#24555;&#22320;&#20248;&#21270;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#30340;&#22686;&#22823;&#65292;&#21453;&#21521;&#20256;&#25773;&#38656;&#35201;&#30340;&#23384;&#20648;&#31354;&#38388;&#25968;&#37327;&#21464;&#24471;&#36807;&#39640;&#12290;&#38646;&#38454;&#65288;ZO&#65289;&#26041;&#27861;&#29702;&#35770;&#19978;&#20165;&#20351;&#29992;&#20004;&#27425;&#21069;&#21521;&#20256;&#36882;&#23601;&#21487;&#20197;&#20272;&#35745;&#26799;&#24230;&#65292;&#20294;&#36890;&#24120;&#24773;&#20917;&#19979;&#23545;&#22823;&#22411;&#27169;&#22411;&#36827;&#34892;&#20248;&#21270;&#30340;&#36895;&#24230;&#38750;&#24120;&#24930;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23384;&#39640;&#25928;&#30340;&#38646;&#38454;&#20248;&#21270;&#22120;&#65288;MeZO&#65289;&#65292;&#23558;&#32463;&#20856;&#30340;ZO-SGD&#26041;&#27861;&#36866;&#24212;&#20110;&#21407;&#22320;&#25805;&#20316;&#65292;&#20174;&#32780;&#20351;&#29992;&#19982;&#25512;&#29702;&#30456;&#21516;&#30340;&#23384;&#20648;&#31354;&#38388;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#12290;&#20363;&#22914;&#65292;&#21482;&#20351;&#29992;&#19968;&#24352;A100 80GB GPU&#65292;MeZO&#23601;&#21487;&#20197;&#35757;&#32451;&#19968;&#20010;300&#20159;&#21442;&#25968;&#30340;&#27169;&#22411;&#65292;&#32780;&#20351;&#29992;&#21453;&#21521;&#20256;&#25773;&#21487;&#20197;&#22312;&#30456;&#21516;&#30340;&#39044;&#31639;&#19979;&#20165;&#35757;&#32451;&#19968;&#20010;27&#20159;&#20010;&#21442;&#25968;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#27169;&#22411;&#31867;&#22411;&#65288;&#25513;&#30721;&#21644;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#65289;&#12289;&#27169;&#22411;&#35268;&#27169;&#65288;&#39640;&#36798;66B&#65289;&#21644;&#19979;&#28216;&#20219;&#21153;&#65288;&#20998;&#31867;&#12289;&#22810;&#39033;&#36873;&#25321;&#21644;&#29983;&#25104;&#65289;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#65288;1&#65289;MeZO&#26126;&#26174;&#20248;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#32447;&#24615;PR&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning language models (LMs) has yielded success on diverse downstream tasks, but as LMs grow in size, backpropagation requires a prohibitively large amount of memory. Zeroth-order (ZO) methods can in principle estimate gradients using only two forward passes but are theorized to be catastrophically slow for optimizing large models. In this work, we propose a memory-efficient zerothorder optimizer (MeZO), adapting the classical ZO-SGD method to operate in-place, thereby fine-tuning LMs with the same memory footprint as inference. For example, with a single A100 80GB GPU, MeZO can train a 30-billion parameter model, whereas fine-tuning with backpropagation can train only a 2.7B LM with the same budget. We conduct comprehensive experiments across model types (masked and autoregressive LMs), model scales (up to 66B), and downstream tasks (classification, multiple-choice, and generation). Our results demonstrate that (1) MeZO significantly outperforms in-context learning and linear pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#23545;&#40784;&#35780;&#20272;&#26041;&#27861;A2EHV&#65292;&#37319;&#29992;&#24322;&#36136;&#20215;&#20540;&#31995;&#32479;&#65292;&#24182;&#22522;&#20110;&#20215;&#20540;&#21512;&#29702;&#24615;&#21644;&#31038;&#20250;&#20215;&#20540;&#23450;&#21521;&#26694;&#26550;&#35780;&#20272;&#20195;&#29702;&#20154;&#34892;&#20026;&#30340;&#31038;&#20250;&#20559;&#22909;&#65292;&#32467;&#26524;&#34920;&#26126;&#27604;&#20256;&#32479;&#23545;&#40784;&#26041;&#27861;&#26356;&#21512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2305.17147</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24322;&#36136;&#20215;&#20540;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous Value Evaluation for Large Language Models. (arXiv:2305.17147v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17147
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#23545;&#40784;&#35780;&#20272;&#26041;&#27861;A2EHV&#65292;&#37319;&#29992;&#24322;&#36136;&#20215;&#20540;&#31995;&#32479;&#65292;&#24182;&#22522;&#20110;&#20215;&#20540;&#21512;&#29702;&#24615;&#21644;&#31038;&#20250;&#20215;&#20540;&#23450;&#21521;&#26694;&#26550;&#35780;&#20272;&#20195;&#29702;&#20154;&#34892;&#20026;&#30340;&#31038;&#20250;&#20559;&#22909;&#65292;&#32467;&#26524;&#34920;&#26126;&#27604;&#20256;&#32479;&#23545;&#40784;&#26041;&#27861;&#26356;&#21512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20986;&#29616;&#20351;&#24471;&#23558;&#23427;&#20204;&#30340;&#20215;&#20540;&#19982;&#20154;&#31867;&#20215;&#20540;&#23545;&#40784;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#23581;&#35797;&#23558;&#20854;&#19982;&#19968;&#31181;&#21516;&#36136;&#30340;&#20154;&#31867;&#20215;&#20540;&#23545;&#40784;&#65292;&#24182;&#38656;&#35201;&#20154;&#31867;&#39564;&#35777;&#65292;&#20294;&#32570;&#20047;&#23545;&#23545;&#40784;&#25152;&#38656;&#26041;&#38754;&#21644;&#28145;&#24230;&#30340;&#20849;&#35782;&#20197;&#21450;&#36896;&#25104;&#30340;&#20154;&#31867;&#20559;&#35265;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#23545;&#40784;&#35780;&#20272;&#26041;&#27861;A2EHV&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#24322;&#36136;&#20215;&#20540;&#31995;&#32479;&#65292;&#65288;1&#65289;&#26159;&#33258;&#21160;&#21270;&#30340;&#65292;&#20197;&#26368;&#23567;&#21270;&#21333;&#20010;&#20154;&#31867;&#20559;&#35265;&#65292;&#24182;&#19988;&#65288;2&#65289;&#20801;&#35768;&#35780;&#20272;&#38024;&#23545;&#21508;&#31181;&#30446;&#26631;&#20540;&#30340;&#24322;&#36136;&#20195;&#29702;&#20154;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#20215;&#20540;&#21512;&#29702;&#24615;&#30340;&#27010;&#24565;&#65292;&#23427;&#20195;&#34920;&#20102;&#20195;&#29702;&#20154;&#25191;&#34892;&#26368;&#33021;&#28385;&#36275;&#30446;&#26631;&#20215;&#20540;&#34892;&#20026;&#30340;&#33021;&#21147;&#12290;&#20215;&#20540;&#21512;&#29702;&#24615;&#30340;&#37327;&#21270;&#26159;&#36890;&#36807;&#31038;&#20250;&#24515;&#29702;&#23398;&#20013;&#30340;&#31038;&#20250;&#20215;&#20540;&#23450;&#21521;&#26694;&#26550;&#36827;&#34892;&#30340;&#65292;&#35813;&#26694;&#26550;&#23558;&#20215;&#20540;&#31354;&#38388;&#20998;&#20026;&#22235;&#20010;&#31867;&#21035;&#65292;&#20197;&#35780;&#20272;&#20195;&#29702;&#20154;&#34892;&#20026;&#30340;&#31038;&#20250;&#20559;&#22909;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#19977;&#20010;&#27169;&#22411;&#30340;&#20215;&#20540;&#21512;&#29702;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;A2EHV&#26041;&#27861;&#27604;&#20256;&#32479;&#23545;&#40784;&#26041;&#27861;&#26356;&#21512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergent capabilities of Large Language Models (LLMs) have made it crucial to align their values with those of humans. Current methodologies typically attempt alignment with a homogeneous human value and requires human verification, yet lack consensus on the desired aspect and depth of alignment and resulting human biases. In this paper, we propose A2EHV, an Automated Alignment Evaluation with a Heterogeneous Value system that (1) is automated to minimize individual human biases, and (2) allows assessments against various target values to foster heterogeneous agents. Our approach pivots on the concept of value rationality, which represents the ability for agents to execute behaviors that satisfy a target value the most. The quantification of value rationality is facilitated by the Social Value Orientation framework from social psychology, which partitions the value space into four categories to assess social preferences from agents' behaviors. We evaluate the value rationality of e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#24067;&#24335;&#20248;&#21270;&#20013;&#36890;&#20449;&#21387;&#32553;&#30340;&#25928;&#26524;&#65292;&#25506;&#35752;&#20102;&#26080;&#20559;&#21387;&#32553;&#38477;&#20302;&#24635;&#36890;&#20449;&#25104;&#26412;&#30340;&#26465;&#20214;&#21644;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.16297</link><description>&lt;p&gt;
&#26080;&#20559;&#21387;&#32553;&#22312;&#20998;&#24067;&#24335;&#20248;&#21270;&#20013;&#33410;&#30465;&#36890;&#20449;&#65306;&#20309;&#26102;&#20197;&#21450;&#22810;&#23569;&#65311;
&lt;/p&gt;
&lt;p&gt;
Unbiased Compression Saves Communication in Distributed Optimization: When and How Much?. (arXiv:2305.16297v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16297
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#24067;&#24335;&#20248;&#21270;&#20013;&#36890;&#20449;&#21387;&#32553;&#30340;&#25928;&#26524;&#65292;&#25506;&#35752;&#20102;&#26080;&#20559;&#21387;&#32553;&#38477;&#20302;&#24635;&#36890;&#20449;&#25104;&#26412;&#30340;&#26465;&#20214;&#21644;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#20449;&#21387;&#32553;&#26159;&#22312;&#20998;&#24067;&#24335;&#20248;&#21270;&#20013;&#24120;&#29992;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#36890;&#36807;&#20256;&#36755;&#21387;&#32553;&#26799;&#24230;&#21644;&#27169;&#22411;&#21442;&#25968;&#26469;&#20943;&#23569;&#36890;&#20449;&#24320;&#38144;&#12290;&#28982;&#32780;&#65292;&#21387;&#32553;&#20250;&#24341;&#20837;&#20449;&#24687;&#22833;&#30495;&#65292;&#23548;&#33268;&#25910;&#25947;&#20943;&#24930;&#24182;&#22686;&#21152;&#36890;&#20449;&#36718;&#27425;&#20197;&#36798;&#21040;&#26399;&#26395;&#35299;&#12290;&#37492;&#20110;&#27599;&#36718;&#36890;&#20449;&#25104;&#26412;&#30340;&#38477;&#20302;&#21644;&#39069;&#22806;&#30340;&#36890;&#20449;&#36718;&#27425;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#23578;&#19981;&#28165;&#26970;&#36890;&#20449;&#21387;&#32553;&#26159;&#21542;&#38477;&#20302;&#20102;&#24635;&#36890;&#20449;&#25104;&#26412;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#26080;&#20559;&#21387;&#32553;&#65292;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#21387;&#32553;&#24418;&#24335;&#65292;&#22312;&#20160;&#20040;&#26465;&#20214;&#19979;&#21487;&#20197;&#38477;&#20302;&#24635;&#36890;&#20449;&#25104;&#26412;&#65292;&#20197;&#21450;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#21487;&#20197;&#38477;&#20302;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#34920;&#24449;&#20855;&#26377;&#36890;&#20449;&#21387;&#32553;&#30340;&#20998;&#24067;&#24335;&#20248;&#21270;&#20013;&#30340;&#24635;&#36890;&#20449;&#25104;&#26412;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20165;&#20165;&#20351;&#29992;&#26080;&#20559;&#21387;&#32553;&#24182;&#19981;&#33021;&#19968;&#23450;&#33410;&#30465;&#24635;&#36890;&#20449;&#25104;&#26412;&#65292;&#20294;&#36825;&#31181;&#32467;&#26524;&#26377;
&lt;/p&gt;
&lt;p&gt;
Communication compression is a common technique in distributed optimization that can alleviate communication overhead by transmitting compressed gradients and model parameters. However, compression can introduce information distortion, which slows down convergence and incurs more communication rounds to achieve desired solutions. Given the trade-off between lower per-round communication costs and additional rounds of communication, it is unclear whether communication compression reduces the total communication cost.  This paper explores the conditions under which unbiased compression, a widely used form of compression, can reduce the total communication cost, as well as the extent to which it can do so. To this end, we present the first theoretical formulation for characterizing the total communication cost in distributed optimization with communication compression. We demonstrate that unbiased compression alone does not necessarily save the total communication cost, but this outcome c
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;&#65288;BBVI&#65289;&#30340;&#20998;&#26512;&#65292;&#21457;&#29616;&#19968;&#20123;&#24120;&#35265;&#30340;&#31639;&#27861;&#35774;&#35745;&#36873;&#25321;&#21487;&#33021;&#20250;&#23548;&#33268;&#27425;&#20248;&#25910;&#25947;&#36895;&#29575;&#65292;&#20294;&#20351;&#29992;&#24102;&#26377;&#36817;&#31471;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;BBVI&#21487;&#20197;&#23454;&#29616;&#26368;&#24378;&#25910;&#25947;&#29575;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.15349</link><description>&lt;p&gt;
&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;&#25910;&#25947;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Black-Box Variational Inference Converges. (arXiv:2305.15349v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15349
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;&#65288;BBVI&#65289;&#30340;&#20998;&#26512;&#65292;&#21457;&#29616;&#19968;&#20123;&#24120;&#35265;&#30340;&#31639;&#27861;&#35774;&#35745;&#36873;&#25321;&#21487;&#33021;&#20250;&#23548;&#33268;&#27425;&#20248;&#25910;&#25947;&#36895;&#29575;&#65292;&#20294;&#20351;&#29992;&#24102;&#26377;&#36817;&#31471;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;BBVI&#21487;&#20197;&#23454;&#29616;&#26368;&#24378;&#25910;&#25947;&#29575;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#23436;&#25972;&#30340;&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;&#65288;BBVI&#65289;&#30340;&#25910;&#25947;&#20445;&#35777;&#65292;&#20063;&#31216;&#20026;&#33945;&#29305;&#21345;&#32599;&#21464;&#20998;&#25512;&#26029;&#12290;&#23613;&#31649;&#26089;&#26399;&#30340;&#30740;&#31350;&#21482;&#38024;&#23545;&#31616;&#21270;&#29256;&#26412;&#30340;BBVI&#36827;&#34892;&#20102;&#30740;&#31350;&#65288;&#20363;&#22914;&#65292;&#26377;&#30028;&#22495;&#12289;&#26377;&#30028;&#25903;&#25345;&#12289;&#20165;&#38024;&#23545;&#23610;&#24230;&#36827;&#34892;&#20248;&#21270;&#31561;&#65289;&#65292;&#20294;&#25105;&#20204;&#30340;&#35774;&#32622;&#19981;&#38656;&#35201;&#20219;&#20309;&#36825;&#26679;&#30340;&#31639;&#27861;&#20462;&#25913;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36866;&#29992;&#20110;&#23545;&#25968;&#24179;&#28369;&#21518;&#39564;&#23494;&#24230;&#65292;&#26080;&#35770;&#26159;&#21542;&#24378;&#23545;&#25968;&#20985;&#24615;&#20197;&#21450;&#20301;&#32622;-&#23610;&#24230;&#21464;&#20998;&#26063;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20986;&#20102;&#19968;&#20123;&#24120;&#35265;&#30340;&#31639;&#27861;&#35774;&#35745;&#36873;&#25321;&#65292;&#29305;&#21035;&#26159;&#21464;&#20998;&#36817;&#20284;&#23610;&#24230;&#30340;&#38750;&#32447;&#24615;&#21442;&#25968;&#21270;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#27425;&#20248;&#25910;&#25947;&#36895;&#29575;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#36816;&#34892;&#24102;&#26377;&#36817;&#31471;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;BBVI&#21487;&#20197;&#32416;&#27491;&#36825;&#20123;&#38480;&#21046;&#65292;&#20174;&#32780;&#23454;&#29616;&#24050;&#30693;&#30340;&#26368;&#24378;&#25910;&#25947;&#29575;&#20445;&#35777;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#36817;&#31471;SGD&#19982;&#20854;&#20182;&#26631;&#20934;&#30340;BBVI&#23454;&#29616;&#36827;&#34892;&#27604;&#36739;&#65292;&#39564;&#35777;&#20102;&#36825;&#19968;&#29702;&#35770;&#32467;&#35770;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide the first convergence guarantee for full black-box variational inference (BBVI), also known as Monte Carlo variational inference. While preliminary investigations worked on simplified versions of BBVI (e.g., bounded domain, bounded support, only optimizing for the scale, and such), our setup does not need any such algorithmic modifications. Our results hold for log-smooth posterior densities with and without strong log-concavity and the location-scale variational family. Also, our analysis reveals that certain algorithm design choices commonly employed in practice, particularly, nonlinear parameterizations of the scale of the variational approximation, can result in suboptimal convergence rates. Fortunately, running BBVI with proximal stochastic gradient descent fixes these limitations, and thus achieves the strongest known convergence rate guarantees. We evaluate this theoretical insight by comparing proximal SGD against other standard implementations of BBVI on large-scale
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#20381;&#36182;&#20110;&#25391;&#24133;&#30340;&#20809; plethysmography (PPG) &#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21487;&#35265;&#24615;&#22270;&#21644;&#36801;&#31227;&#23398;&#20064;&#23454;&#29616;&#20102;&#23545;&#24515;&#29575;&#21644;&#34880;&#31649;&#32769;&#21270;&#31561;&#29983;&#29289;&#29305;&#24449;&#30340;&#31283;&#20581;&#20272;&#35745;&#21644;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2305.14062</link><description>&lt;p&gt;
&#19981;&#20381;&#36182;&#20110;&#25391;&#24133;&#30340;&#20809; plethysmography (PPG) &#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65306;&#36890;&#36807;&#21487;&#35265;&#24615;&#22270;&#21644;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Amplitude-Independent Machine Learning for PPG through Visibility Graphs and Transfer Learning. (arXiv:2305.14062v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14062
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#20381;&#36182;&#20110;&#25391;&#24133;&#30340;&#20809; plethysmography (PPG) &#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21487;&#35265;&#24615;&#22270;&#21644;&#36801;&#31227;&#23398;&#20064;&#23454;&#29616;&#20102;&#23545;&#24515;&#29575;&#21644;&#34880;&#31649;&#32769;&#21270;&#31561;&#29983;&#29289;&#29305;&#24449;&#30340;&#31283;&#20581;&#20272;&#35745;&#21644;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#20307;&#31215;&#25551;&#35760;&#27861; (PPG) &#26159;&#20351;&#29992;&#20809;&#27979;&#37327;&#34880;&#28082;&#20307;&#31215;&#30340;&#21464;&#21270;&#30340;&#19968;&#31181;&#26041;&#27861;&#65292;&#26159;&#22823;&#22810;&#25968;&#21487;&#31359;&#25140;&#35774;&#22791;&#30340;&#29305;&#24449;&#12290;PPG&#20449;&#21495;&#33021;&#22815;&#25552;&#20379;&#23545;&#20154;&#20307;&#24490;&#29615;&#31995;&#32479;&#30340;&#27934;&#23519;&#65292;&#24182;&#21487;&#29992;&#20110;&#25552;&#21462;&#21508;&#31181;&#29983;&#29289;&#29305;&#24449;&#65292;&#20363;&#22914;&#24515;&#29575;&#21644;&#34880;&#31649;&#32769;&#21270;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#31639;&#27861;&#65292;&#20294;&#35768;&#22810;&#31639;&#27861;&#23384;&#22312;&#38480;&#21046;&#65292;&#21253;&#25324;&#36807;&#22810;&#22320;&#20381;&#36182;&#20154;&#24037;&#26657;&#20934;&#12289;&#39640;&#20449;&#21495;&#36136;&#37327;&#35201;&#27714;&#21644;&#32570;&#20047;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#22270;&#35770;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#31639;&#27861;&#30340;PPG&#20449;&#21495;&#22788;&#29702;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23545;&#25391;&#24133;&#26080;&#20851;&#24182;&#19988;&#23545;&#20223;&#23556;&#21464;&#25442;&#19981;&#21464;&#12290;&#23427;&#36824;&#38656;&#35201;&#26368;&#23569;&#30340;&#39044;&#22788;&#29702;&#65292;&#36890;&#36807;RGB&#36890;&#36947;&#34701;&#21512;&#20449;&#24687;&#65292;&#24182;&#22312;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20102;&#31283;&#20581;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25152;&#25552;&#20986;&#30340;VGTL-net&#22312;&#34880;&#31649;&#32769;&#21270;&#39044;&#27979;&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#31283;&#20581;&#30340;&#20272;&#35745;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Photoplethysmography (PPG) refers to the measurement of variations in blood volume using light and is a feature of most wearable devices. The PPG signals provide insight into the body's circulatory system and can be employed to extract various bio-features, such as heart rate and vascular ageing. Although several algorithms have been proposed for this purpose, many exhibit limitations, including heavy reliance on human calibration, high signal quality requirements, and a lack of generalisation. In this paper, we introduce a PPG signal processing framework that integrates graph theory and computer vision algorithms, to provide an analysis framework which is amplitude-independent and invariant to affine transformations. It also requires minimal preprocessing, fuses information through RGB channels and exhibits robust generalisation across tasks and datasets. The proposed VGTL-net achieves state-of-the-art performance in the prediction of vascular ageing and demonstrates robust estimation
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#21160;&#24577;&#33258;&#32534;&#30721;&#22120;&#65288;MDVAE&#65289;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#38899;&#35270;&#39057;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#65292;&#35813;&#26041;&#27861;&#22312;&#20013;&#38388;&#34920;&#31034;&#19978;&#36827;&#34892;&#20102;&#38745;&#24577;&#19982;&#21160;&#24577;&#20449;&#24687;&#12289;&#27169;&#24577;&#29305;&#24322;&#19982;&#20849;&#21516;&#20449;&#24687;&#30340;&#20998;&#31163;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.03582</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#38899;&#35270;&#39057;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#30340;&#22810;&#27169;&#24577;&#21160;&#24577;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
A Multimodal Dynamical Variational Autoencoder for Audiovisual Speech Representation Learning. (arXiv:2305.03582v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03582
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#21160;&#24577;&#33258;&#32534;&#30721;&#22120;&#65288;MDVAE&#65289;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#38899;&#35270;&#39057;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#65292;&#35813;&#26041;&#27861;&#22312;&#20013;&#38388;&#34920;&#31034;&#19978;&#36827;&#34892;&#20102;&#38745;&#24577;&#19982;&#21160;&#24577;&#20449;&#24687;&#12289;&#27169;&#24577;&#29305;&#24322;&#19982;&#20849;&#21516;&#20449;&#24687;&#30340;&#20998;&#31163;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#21160;&#24577;&#33258;&#32534;&#30721;&#22120;&#65288;MDVAE&#65289;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#38899;&#35270;&#39057;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#12290;&#28508;&#22312;&#31354;&#38388;&#34987;&#26500;&#36896;&#20026;&#23558;&#22312;&#21508;&#20010;&#27169;&#24577;&#20043;&#38388;&#20849;&#20139;&#30340;&#28508;&#22312;&#21160;&#24577;&#22240;&#32032;&#19982;&#27599;&#20010;&#27169;&#24577;&#29305;&#23450;&#30340;&#22240;&#32032;&#21306;&#20998;&#24320;&#26469;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#19968;&#20010;&#38745;&#24577;&#28508;&#21464;&#37327;&#26469;&#32534;&#30721;&#38899;&#35270;&#39057;&#35821;&#38899;&#24207;&#21015;&#20013;&#38543;&#26102;&#38388;&#24658;&#23450;&#30340;&#20449;&#24687;&#12290;&#27169;&#22411;&#22312;&#19968;&#20010;&#38899;&#35270;&#39057;&#24773;&#24863;&#35821;&#38899;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#26080;&#30417;&#30563;&#35757;&#32451;&#20998;&#20004;&#20010;&#38454;&#27573;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#23545;&#20110;&#27599;&#20010;&#27169;&#24577;&#65292;&#39318;&#20808;&#29420;&#31435;&#23398;&#20064;&#19968;&#20010;&#21521;&#37327;&#37327;&#21270;&#33258;&#32534;&#30721;&#22120;&#65288;VQ-VAE&#65289;&#65292;&#32780;&#27809;&#26377;&#26102;&#38388;&#24314;&#27169;&#12290;&#31532;&#20108;&#38454;&#27573;&#21017;&#22312;&#21521;&#37327;&#37327;&#21270;&#33258;&#32534;&#30721;&#22120;&#65288;VQ-VAEs&#65289;&#30340;&#20013;&#38388;&#34920;&#31034;&#19978;&#23398;&#20064;MDVAE&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#26041;&#38754;&#65292;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a multimodal \textit{and} dynamical VAE (MDVAE) applied to unsupervised audio-visual speech representation learning. The latent space is structured to dissociate the latent dynamical factors that are shared between the modalities from those that are specific to each modality. A static latent variable is also introduced to encode the information that is constant over time within an audiovisual speech sequence. The model is trained in an unsupervised manner on an audiovisual emotional speech dataset, in two stages. In the first stage, a vector quantized VAE (VQ-VAE) is learned independently for each modality, without temporal modeling. The second stage consists in learning the MDVAE model on the intermediate representation of the VQ-VAEs before quantization. The disentanglement between static versus dynamical and modality-specific versus modality-common information occurs during this second training stage. Extensive experiments are conducted to investigate how a
&lt;/p&gt;</description></item><item><title>&#20445;&#25252;&#20010;&#20154;&#38544;&#31169;&#20449;&#24687;&#26159;&#24456;&#37325;&#35201;&#30340;&#65292;&#20294;&#35268;&#36991;&#25193;&#25955;&#27169;&#22411;&#20013;&#28155;&#21152;&#30340;&#22122;&#22768;&#23545;&#25968;&#25454;&#36827;&#34892;&#20445;&#25252;&#23384;&#22312;&#25361;&#25112;&#12290;AVATAR&#31639;&#27861;&#20511;&#21161;&#25193;&#25955;&#27169;&#22411;&#30340;&#23041;&#21147;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#31934;&#24515;&#35774;&#35745;&#30340;&#21435;&#22122;&#36807;&#31243;&#26469;&#28040;&#38500;&#25968;&#25454;&#20445;&#25252;&#25200;&#21160;&#30340;&#24433;&#21709;&#65292;&#24182;&#33719;&#24471;&#20102;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.08500</link><description>&lt;p&gt;
&#35268;&#36991;&#25193;&#25955;&#27169;&#22411;&#20013;&#28155;&#21152;&#30340;&#22122;&#22768;&#23545;&#25968;&#25454;&#36827;&#34892;&#20445;&#25252;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
The Devil's Advocate: Shattering the Illusion of Unexploitable Data using Diffusion Models. (arXiv:2303.08500v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08500
&lt;/p&gt;
&lt;p&gt;
&#20445;&#25252;&#20010;&#20154;&#38544;&#31169;&#20449;&#24687;&#26159;&#24456;&#37325;&#35201;&#30340;&#65292;&#20294;&#35268;&#36991;&#25193;&#25955;&#27169;&#22411;&#20013;&#28155;&#21152;&#30340;&#22122;&#22768;&#23545;&#25968;&#25454;&#36827;&#34892;&#20445;&#25252;&#23384;&#22312;&#25361;&#25112;&#12290;AVATAR&#31639;&#27861;&#20511;&#21161;&#25193;&#25955;&#27169;&#22411;&#30340;&#23041;&#21147;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#31934;&#24515;&#35774;&#35745;&#30340;&#21435;&#22122;&#36807;&#31243;&#26469;&#28040;&#38500;&#25968;&#25454;&#20445;&#25252;&#25200;&#21160;&#30340;&#24433;&#21709;&#65292;&#24182;&#33719;&#24471;&#20102;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20445;&#25252;&#20010;&#20154;&#25968;&#25454;&#20813;&#21463;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21033;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#65292;&#21487;&#29992;&#24615;&#25915;&#20987;&#23637;&#29616;&#20986;&#25552;&#20379;&#39069;&#22806;&#20445;&#25252;&#25514;&#26045;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#20197;&#38450;&#27490;&#26410;&#32463;&#25480;&#26435;&#22320;&#20351;&#29992;&#25968;&#25454;&#26469;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#12290;&#36825;&#20123;&#26041;&#27861;&#26088;&#22312;&#21521;&#24178;&#20928;&#25968;&#25454;&#28155;&#21152;&#38590;&#20197;&#23519;&#35273;&#30340;&#22122;&#22768;&#65292;&#20351;&#31070;&#32463;&#32593;&#32476;&#26080;&#27861;&#20174;&#21463;&#20445;&#25252;&#30340;&#25968;&#25454;&#20013;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#27169;&#24335;&#65292;&#22768;&#31216;&#21487;&#20197;&#20351;&#20010;&#20154;&#25968;&#25454;&#8220;&#26080;&#27861;&#21033;&#29992;&#8221;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#38024;&#23545;&#36825;&#31181;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#26377;&#21147;&#30340;&#23545;&#25239;&#25514;&#26045;&#65292;&#34920;&#26126;&#19981;&#21487;&#21033;&#29992;&#30340;&#25968;&#25454;&#21487;&#33021;&#21482;&#26159;&#19968;&#31181;&#24187;&#35273;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#23041;&#21147;&#65292;&#24182;&#23637;&#31034;&#31934;&#24515;&#35774;&#35745;&#30340;&#21435;&#22122;&#36807;&#31243;&#21487;&#20197;&#28040;&#38500;&#25968;&#25454;&#20445;&#25252;&#25200;&#21160;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#20005;&#35880;&#22320;&#20998;&#26512;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#25152;&#38656;&#21435;&#22122;&#30340;&#37327;&#30452;&#25509;&#19982;&#25968;&#25454;&#20445;&#25252;&#25200;&#21160;&#30340;&#25968;&#37327;&#25104;&#27491;&#27604;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21517;&#20026;AVATAR&#65292;&#22312;&#21253;&#25324;CelebA&#25968;&#25454;&#38598;&#22312;&#20869;&#30340;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#25552;&#20379;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20854;&#20013;&#23427;&#20197;&#24040;&#22823;&#30340;&#20248;&#21183;&#32988;&#20986;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Protecting personal data against the exploitation of machine learning models is of paramount importance. Recently, availability attacks have shown great promise to provide an extra layer of protection against the unauthorized use of data to train neural networks. These methods aim to add imperceptible noise to clean data so that the neural networks cannot extract meaningful patterns from the protected data, claiming that they can make personal data "unexploitable." In this paper, we provide a strong countermeasure against such approaches, showing that unexploitable data might only be an illusion. In particular, we leverage the power of diffusion models and show that a carefully designed denoising process can defuse the ramifications of the data-protecting perturbations. We rigorously analyze our algorithm, and theoretically prove that the amount of required denoising is directly related to the magnitude of the data-protecting perturbations. Our approach, called AVATAR, delivers state-o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22810;&#27169;&#24577;&#25968;&#25454;&#25972;&#21512;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#20197;&#25552;&#39640;&#30284;&#30151;&#35786;&#26029;&#21644;&#27835;&#30103;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.06471</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26102;&#20195;&#30340;&#32959;&#30244;&#22810;&#27169;&#24577;&#25968;&#25454;&#25972;&#21512;&#65306;&#19968;&#31687;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Multimodal Data Integration for Oncology in the Era of Deep Neural Networks: A Review. (arXiv:2303.06471v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06471
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22810;&#27169;&#24577;&#25968;&#25454;&#25972;&#21512;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#20197;&#25552;&#39640;&#30284;&#30151;&#35786;&#26029;&#21644;&#27835;&#30103;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This review article analyzes the application of deep neural networks in multimodal data integration to improve the accuracy and reliability of cancer diagnosis and treatment.
&lt;/p&gt;
&lt;p&gt;
&#30284;&#30151;&#22312;&#19981;&#21516;&#23610;&#24230;&#12289;&#27169;&#24577;&#21644;&#20998;&#36776;&#29575;&#30340;&#33719;&#21462;&#25968;&#25454;&#20013;&#20855;&#26377;&#20851;&#31995;&#20449;&#24687;&#65292;&#20363;&#22914;&#25918;&#23556;&#23398;&#12289;&#30149;&#29702;&#23398;&#12289;&#22522;&#22240;&#32452;&#23398;&#12289;&#34507;&#30333;&#36136;&#32452;&#23398;&#21644;&#20020;&#24202;&#35760;&#24405;&#12290;&#25972;&#21512;&#22810;&#31181;&#25968;&#25454;&#31867;&#22411;&#21487;&#20197;&#25552;&#39640;&#30284;&#30151;&#35786;&#26029;&#21644;&#27835;&#30103;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;&#21487;&#33021;&#23384;&#22312;&#20154;&#31867;&#25110;&#29616;&#26377;&#25216;&#26415;&#24037;&#20855;&#26080;&#27861;&#35270;&#35273;&#19978;&#21306;&#20998;&#30340;&#19982;&#30142;&#30149;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;&#20256;&#32479;&#26041;&#27861;&#36890;&#24120;&#20851;&#27880;&#21333;&#20010;&#23610;&#24230;&#30340;&#29983;&#29289;&#31995;&#32479;&#30340;&#37096;&#20998;&#25110;&#21333;&#19968;&#27169;&#24577;&#20449;&#24687;&#65292;&#24182;&#26410;&#28085;&#30422;&#25968;&#25454;&#24322;&#36136;&#24615;&#30340;&#23436;&#25972;&#20809;&#35889;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20419;&#36827;&#20102;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#34701;&#21512;&#26041;&#27861;&#30340;&#21457;&#23637;&#65292;&#21487;&#20197;&#20174;&#22810;&#20010;&#26469;&#28304;&#25552;&#21462;&#21644;&#25972;&#21512;&#30456;&#20851;&#20449;&#24687;&#12290;&#26368;&#36817;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#22914;&#22270;&#24418;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#21644;&#21464;&#21387;&#22120;&#65292;&#22312;&#22810;&#27169;&#24577;&#23398;&#20064;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25104;&#21151;&#12290;&#26412;&#32508;&#36848;&#25991;&#31456;&#25552;&#20379;&#20102;&#23545;&#24403;&#21069;&#22810;&#27169;&#24577;&#25968;&#25454;&#25972;&#21512;&#26041;&#27861;&#30340;&#28145;&#20837;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cancer has relational information residing at varying scales, modalities, and resolutions of the acquired data, such as radiology, pathology, genomics, proteomics, and clinical records. Integrating diverse data types can improve the accuracy and reliability of cancer diagnosis and treatment. There can be disease-related information that is too subtle for humans or existing technological tools to discern visually. Traditional methods typically focus on partial or unimodal information about biological systems at individual scales and fail to encapsulate the complete spectrum of the heterogeneous nature of data. Deep neural networks have facilitated the development of sophisticated multimodal data fusion approaches that can extract and integrate relevant information from multiple sources. Recent deep learning frameworks such as Graph Neural Networks (GNNs) and Transformers have shown remarkable success in multimodal learning. This review article provides an in-depth analysis of the state-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#32452;&#21512;&#32467;&#26500;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23884;&#20837;&#31354;&#38388;&#20013;&#36739;&#23567;&#38598;&#21512;&#30340;&#21521;&#37327;&#32452;&#21512;&#26469;&#36817;&#20284;&#34920;&#31034;&#26469;&#33258;&#32534;&#30721;&#22120;&#30340;&#34920;&#31034;&#24418;&#24335;&#30340;&#26041;&#27861;&#65292;&#23558;&#36825;&#20123;&#21521;&#37327;&#35270;&#20026;&#8220;&#29702;&#24819;&#21333;&#35789;&#8221;&#65292;&#24182;&#22312;CLIP&#30340;&#23884;&#20837;&#20013;&#20197;&#23454;&#39564;&#26041;&#24335;&#25506;&#32034;&#20102;&#36825;&#20123;&#32467;&#26500;&#30340;&#21487;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.14383</link><description>&lt;p&gt;
&#24847;&#20041;&#30340;&#32447;&#24615;&#31354;&#38388;&#65306;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#32452;&#21512;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Linear Spaces of Meanings: Compositional Structures in Vision-Language Models. (arXiv:2302.14383v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14383
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#32452;&#21512;&#32467;&#26500;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23884;&#20837;&#31354;&#38388;&#20013;&#36739;&#23567;&#38598;&#21512;&#30340;&#21521;&#37327;&#32452;&#21512;&#26469;&#36817;&#20284;&#34920;&#31034;&#26469;&#33258;&#32534;&#30721;&#22120;&#30340;&#34920;&#31034;&#24418;&#24335;&#30340;&#26041;&#27861;&#65292;&#23558;&#36825;&#20123;&#21521;&#37327;&#35270;&#20026;&#8220;&#29702;&#24819;&#21333;&#35789;&#8221;&#65292;&#24182;&#22312;CLIP&#30340;&#23884;&#20837;&#20013;&#20197;&#23454;&#39564;&#26041;&#24335;&#25506;&#32034;&#20102;&#36825;&#20123;&#32467;&#26500;&#30340;&#21487;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#20013;&#30340;&#25968;&#25454;&#23884;&#20837;&#30340;&#32452;&#21512;&#32467;&#26500;&#12290;&#20256;&#32479;&#19978;&#65292;&#32452;&#21512;&#24615;&#19982;&#39044;&#20808;&#23384;&#22312;&#30340;&#35789;&#27719;&#34920;&#20013;&#30340;&#21333;&#35789;&#23884;&#20837;&#30340;&#20195;&#25968;&#36816;&#31639;&#26377;&#20851;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#35797;&#22270;&#20351;&#29992;&#23884;&#20837;&#31354;&#38388;&#20013;&#36739;&#23567;&#38598;&#21512;&#30340;&#21521;&#37327;&#32452;&#21512;&#26469;&#36817;&#20284;&#34920;&#31034;&#26469;&#33258;&#32534;&#30721;&#22120;&#30340;&#34920;&#31034;&#24418;&#24335;&#12290;&#36825;&#20123;&#21521;&#37327;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#22312;&#27169;&#22411;&#30340;&#23884;&#20837;&#31354;&#38388;&#20013;&#30452;&#25509;&#29983;&#25104;&#27010;&#24565;&#30340;&#8220;&#29702;&#24819;&#21333;&#35789;&#8221;&#12290;&#25105;&#20204;&#39318;&#20808;&#20174;&#20960;&#20309;&#23398;&#30340;&#35282;&#24230;&#25552;&#20986;&#20102;&#29702;&#35299;&#32452;&#21512;&#32467;&#26500;&#30340;&#26694;&#26550;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35299;&#37322;&#20102;VLM&#23884;&#20837;&#22312;&#27010;&#29575;&#19978;&#30340;&#36825;&#20123;&#32452;&#21512;&#32467;&#26500;&#30340;&#21547;&#20041;&#65292;&#24182;&#25552;&#20379;&#20102;&#23427;&#20204;&#22312;&#23454;&#36341;&#20013;&#20135;&#29983;&#30340;&#30452;&#35273;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;CLIP&#30340;&#23884;&#20837;&#20013;&#20197;&#23454;&#39564;&#26041;&#24335;&#25506;&#32034;&#20102;&#36825;&#20123;&#32467;&#26500;&#65292;&#24182;&#35780;&#20272;&#20102;&#23427;&#20204;&#22312;&#35299;&#20915;&#20998;&#31867;&#12289;&#21435;&#20559;&#21644;&#26816;&#32034;&#31561;&#19981;&#21516;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#20013;&#30340;&#26377;&#29992;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23884;&#20837;&#31354;&#38388;&#20013;&#31616;&#21333;&#30340;&#32447;&#24615;&#20195;&#25968;&#36816;&#31639;&#21487;&#20197;&#23454;&#29616;&#19982;&#26356;&#22797;&#26434;&#30340;&#26041;&#27861;&#30456;&#23218;&#32654;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#24847;&#20041;&#30340;&#32447;&#24615;&#31354;&#38388;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate compositional structures in data embeddings from pre-trained vision-language models (VLMs). Traditionally, compositionality has been associated with algebraic operations on embeddings of words from a pre-existing vocabulary. In contrast, we seek to approximate representations from an encoder as combinations of a smaller set of vectors in the embedding space. These vectors can be seen as "ideal words" for generating concepts directly within the embedding space of the model. We first present a framework for understanding compositional structures from a geometric perspective. We then explain what these compositional structures entail probabilistically in the case of VLM embeddings, providing intuitions for why they arise in practice. Finally, we empirically explore these structures in CLIP's embeddings and we evaluate their usefulness for solving different vision-language tasks such as classification, debiasing, and retrieval. Our results show that simple linear algebraic o
&lt;/p&gt;</description></item><item><title>Classy Ensemble&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#38598;&#25104;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#27599;&#31867;&#20934;&#30830;&#29575;&#30340;&#21152;&#26435;&#32452;&#21512;&#26469;&#32858;&#21512;&#27169;&#22411;&#65292;&#22312;&#22823;&#37327;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;&#24182;&#25552;&#20986;Classy Cluster Ensemble&#21644;Classy Evolutionary Ensemble&#20004;&#31181;&#22686;&#24378;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.10580</link><description>&lt;p&gt;
Classy Ensemble: &#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#20998;&#31867;&#30340;&#38598;&#25104;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Classy Ensemble: A Novel Ensemble Algorithm for Classification. (arXiv:2302.10580v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10580
&lt;/p&gt;
&lt;p&gt;
Classy Ensemble&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#38598;&#25104;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#27599;&#31867;&#20934;&#30830;&#29575;&#30340;&#21152;&#26435;&#32452;&#21512;&#26469;&#32858;&#21512;&#27169;&#22411;&#65292;&#22312;&#22823;&#37327;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;&#24182;&#25552;&#20986;Classy Cluster Ensemble&#21644;Classy Evolutionary Ensemble&#20004;&#31181;&#22686;&#24378;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Classy Ensemble&#65292;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#20998;&#31867;&#20219;&#21153;&#30340;&#38598;&#25104;&#31639;&#27861;&#65292;&#23427;&#36890;&#36807;&#27599;&#31867;&#20934;&#30830;&#29575;&#30340;&#21152;&#26435;&#32452;&#21512;&#26469;&#32858;&#21512;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;153&#20010;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;Classy Ensemble&#20248;&#20110;&#20004;&#31181;&#20854;&#20182;&#33879;&#21517;&#30340;&#32858;&#21512;&#31639;&#27861;&#8212;&#8212;&#22522;&#20110;&#39034;&#24207;&#30340;&#20462;&#21098;&#21644;&#22522;&#20110;&#32858;&#31867;&#30340;&#20462;&#21098;&#8212;&#8212;&#20197;&#21450;&#26368;&#36817;&#24341;&#20837;&#30340;lexigarden&#38598;&#25104;&#29983;&#25104;&#22120;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#22686;&#24378;&#26041;&#27861;&#65306;1&#65289;Classy Cluster Ensemble&#65292;&#23558;Classy Ensemble&#21644;&#22522;&#20110;&#32858;&#31867;&#30340;&#20462;&#21098;&#30456;&#32467;&#21512;&#65307;2&#65289;&#28145;&#24230;&#23398;&#20064;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102;Classy Ensemble&#22312;&#22235;&#20010;&#22270;&#20687;&#25968;&#25454;&#38598;Fashion MNIST&#12289;CIFAR10&#12289;CIFAR100&#21644;ImageNet&#19978;&#30340;&#20248;&#36234;&#24615;&#65307;&#20197;&#21450;3&#65289;Classy Evolutionary Ensemble&#65292;&#20854;&#20013;&#20351;&#29992;&#36827;&#21270;&#31639;&#27861;&#26469;&#36873;&#25321;Classy Ensemble&#20174;&#20013;&#36873;&#25321;&#30340;&#27169;&#22411;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Classy Ensemble, a novel ensemble-generation algorithm for classification tasks, which aggregates models through a weighted combination of per-class accuracy. Tested over 153 machine learning datasets we demonstrate that Classy Ensemble outperforms two other well-known aggregation algorithms -order-based pruning and clustering-based pruning -- as well as the recently introduced lexigarden ensemble generator. We then present three enhancements: 1) Classy Cluster Ensemble, which combines Classy Ensemble and cluster-based pruning; 2) Deep Learning experiments, showing the merits of Classy Ensemble over four image datasets: Fashion MNIST, CIFAR10, CIFAR100, and ImageNet; and 3) Classy Evolutionary Ensemble, wherein an evolutionary algorithm is used to select the set of models which Classy Ensemble picks from.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#26041;&#27861;&#65292;&#36873;&#25321;&#24615;&#35843;&#25972;&#31070;&#32463;&#32593;&#32476;&#30340;&#21508;&#20010;&#23618;&#30340;&#31934;&#24230;&#65292;&#20197;&#23454;&#29616;&#22312;&#20219;&#21153;&#24615;&#33021;&#19979;&#38477;&#26368;&#23567;&#30340;&#24773;&#20917;&#19979;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#24555;&#36895;&#12289;&#33410;&#33021;&#25512;&#26029;&#12290;</title><link>http://arxiv.org/abs/2301.13330</link><description>&lt;p&gt;
&#28151;&#21512;&#31934;&#24230;&#31070;&#32463;&#32593;&#32476;&#37327;&#21270;&#30340;&#39640;&#25928;&#26377;&#25928;&#26041;&#27861;&#65292;&#29992;&#20110;&#26356;&#24555;&#12289;&#26356;&#33410;&#33021;&#30340;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficient and Effective Methods for Mixed Precision Neural Network Quantization for Faster, Energy-efficient Inference. (arXiv:2301.13330v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13330
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#26041;&#27861;&#65292;&#36873;&#25321;&#24615;&#35843;&#25972;&#31070;&#32463;&#32593;&#32476;&#30340;&#21508;&#20010;&#23618;&#30340;&#31934;&#24230;&#65292;&#20197;&#23454;&#29616;&#22312;&#20219;&#21153;&#24615;&#33021;&#19979;&#38477;&#26368;&#23567;&#30340;&#24773;&#20917;&#19979;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#24555;&#36895;&#12289;&#33410;&#33021;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#23454;&#29616;&#39640;&#25928;&#30340;&#31070;&#32463;&#32593;&#32476;&#25512;&#26029;&#65292;&#24076;&#26395;&#20197;&#26368;&#31616;&#21333;&#30340;&#32593;&#32476;&#12289;&#26368;&#23569;&#30340;&#35745;&#31639;&#12289;&#23384;&#20648;&#21644;&#21151;&#32791;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#12290;&#23558;&#32593;&#32476;&#37327;&#21270;&#20026;&#36739;&#20302;&#30340;&#31934;&#24230;&#26159;&#31616;&#21270;&#32593;&#32476;&#30340;&#24378;&#22823;&#25216;&#26415;&#12290;&#30001;&#20110;&#32593;&#32476;&#30340;&#27599;&#19968;&#23618;&#23545;&#37327;&#21270;&#30340;&#25935;&#24863;&#31243;&#24230;&#21487;&#33021;&#19981;&#21516;&#65292;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#26041;&#27861;&#36873;&#25321;&#24615;&#22320;&#35843;&#25972;&#21508;&#20010;&#23618;&#30340;&#31934;&#24230;&#65292;&#20197;&#23454;&#29616;&#20219;&#21153;&#24615;&#33021;&#65288;&#22914;&#20934;&#30830;&#24615;&#65289;&#30340;&#26368;&#23567;&#19979;&#38477;&#12290;&#20026;&#20102;&#20272;&#35745;&#23618;&#31934;&#24230;&#36873;&#25321;&#23545;&#20219;&#21153;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24341;&#20837;&#20102;&#20004;&#31181;&#26041;&#27861;&#65306;i) &#22522;&#20110;&#29109;&#36817;&#20284;&#30340;&#23618;&#36873;&#25321;&#65288;EAGL&#65289;&#24555;&#36895;&#22320;&#20351;&#29992;&#26435;&#37325;&#20998;&#24067;&#30340;&#29109;&#65292;ii) &#22522;&#20110;&#20934;&#30830;&#24615;&#24863;&#30693;&#30340;&#23618;&#31934;&#24230;&#36873;&#25321;&#65288;ALPS&#65289;&#31616;&#21333;&#22320;&#20381;&#36182;&#23618;&#31934;&#24230;&#38477;&#20302;&#21518;&#30340;&#21333;&#27425;&#36845;&#20195;&#24494;&#35843;&#12290;&#20351;&#29992;EAGL&#21644;ALPS&#36827;&#34892;&#23618;&#31934;&#24230;&#36873;&#25321;&#65292;&#22312;ResNet-50&#12289;ResNet-101&#21644;...
&lt;/p&gt;
&lt;p&gt;
For efficient neural network inference, it is desirable to achieve state-of-the-art accuracy with the simplest networks requiring the least computation, memory, and power. Quantizing networks to lower precision is a powerful technique for simplifying networks. As each layer of a network may have different sensitivity to quantization, mixed precision quantization methods selectively tune the precision of individual layers to achieve a minimum drop in task performance (e.g., accuracy). To estimate the impact of layer precision choice on task performance, two methods are introduced: i) Entropy Approximation Guided Layer selection (EAGL) is fast and uses the entropy of the weight distribution, and ii) Accuracy-aware Layer Precision Selection (ALPS) is straightforward and relies on single epoch fine-tuning after layer precision reduction. Using EAGL and ALPS for layer precision selection, full-precision accuracy is recovered with a mix of 4-bit and 2-bit layers for ResNet-50, ResNet-101 and
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#65288;cINN&#65289;&#30340;&#23637;&#24320;&#26041;&#27861;&#65288;IcINN&#65289;&#65292;&#36890;&#36807;&#36845;&#20195;&#35757;&#32451;&#35843;&#25972;&#35757;&#32451;&#26679;&#26412;&#19982;&#25968;&#25454;&#20043;&#38388;&#30340;&#20559;&#24046;&#65292;&#23454;&#29616;&#20102;&#23545;&#25968;&#25454;&#19982;&#29702;&#35770;&#39044;&#27979;&#30340;&#27604;&#36739;&#20013;&#25506;&#27979;&#22120;&#25928;&#24212;&#30340;&#23637;&#24320;&#12290;</title><link>http://arxiv.org/abs/2212.08674</link><description>&lt;p&gt;
&#22522;&#20110;&#26465;&#20214;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#65288;cINN&#65289;&#30340;&#23637;&#24320;&#26041;&#27861;&#21450;&#20854;&#36845;&#20195;&#35757;&#32451;&#65288;arXiv&#65306;2212.08674v3 [hep-ph] UPDATED&#65289;
&lt;/p&gt;
&lt;p&gt;
An unfolding method based on conditional Invertible Neural Networks (cINN) using iterative training. (arXiv:2212.08674v3 [hep-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08674
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#65288;cINN&#65289;&#30340;&#23637;&#24320;&#26041;&#27861;&#65288;IcINN&#65289;&#65292;&#36890;&#36807;&#36845;&#20195;&#35757;&#32451;&#35843;&#25972;&#35757;&#32451;&#26679;&#26412;&#19982;&#25968;&#25454;&#20043;&#38388;&#30340;&#20559;&#24046;&#65292;&#23454;&#29616;&#20102;&#23545;&#25968;&#25454;&#19982;&#29702;&#35770;&#39044;&#27979;&#30340;&#27604;&#36739;&#20013;&#25506;&#27979;&#22120;&#25928;&#24212;&#30340;&#23637;&#24320;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25506;&#27979;&#22120;&#25928;&#24212;&#23637;&#24320;&#23545;&#20110;&#25968;&#25454;&#19982;&#29702;&#35770;&#39044;&#27979;&#30340;&#27604;&#36739;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#26041;&#27861;&#21482;&#33021;&#23558;&#25968;&#25454;&#34920;&#31034;&#20026;&#20302;&#32500;&#24230;&#65292;&#32780;&#26426;&#22120;&#23398;&#20064;&#20351;&#24471;&#20445;&#30041;&#23436;&#25972;&#32500;&#24230;&#30340;&#26032;&#23637;&#24320;&#25216;&#26415;&#25104;&#20026;&#21487;&#33021;&#12290;&#29983;&#25104;&#32593;&#32476;&#22914;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#65288;INN&#65289;&#21487;&#20197;&#23454;&#29616;&#27010;&#29575;&#23637;&#24320;&#65292;&#23558;&#20010;&#20307;&#20107;&#20214;&#26144;&#23556;&#21040;&#30456;&#24212;&#30340;&#23637;&#24320;&#27010;&#29575;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#21463;&#21040;&#35757;&#32451;&#26679;&#26412;&#19982;&#23454;&#38469;&#23637;&#24320;&#25968;&#25454;&#30340;&#19968;&#33268;&#24615;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#36845;&#20195;&#26465;&#20214;INN&#65288;IcINN&#65289;&#23637;&#24320;&#26041;&#27861;&#65292;&#29992;&#20110;&#35843;&#25972;&#35757;&#32451;&#26679;&#26412;&#19982;&#25968;&#25454;&#20043;&#38388;&#30340;&#20559;&#24046;&#12290;&#39318;&#20808;&#22312;&#29609;&#20855;&#25968;&#25454;&#19978;&#39564;&#35777;&#20102;IcINN&#23637;&#24320;&#26041;&#27861;&#65292;&#28982;&#21518;&#23558;&#20854;&#24212;&#29992;&#20110;$pp \to Z \gamma \gamma$&#36807;&#31243;&#30340;&#20266;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
The unfolding of detector effects is crucial for the comparison of data to theory predictions. While traditional methods are limited to representing the data in a low number of dimensions, machine learning has enabled new unfolding techniques while retaining the full dimensionality. Generative networks like invertible neural networks~(INN) enable a probabilistic unfolding, which map individual events to their corresponding unfolded probability distribution. The accuracy of such methods is however limited by how well simulated training samples model the actual data that is unfolded. We introduce the iterative conditional INN~(IcINN) for unfolding that adjusts for deviations between simulated training samples and data. The IcINN unfolding is first validated on toy data and then applied to pseudo-data for the $pp \to Z \gamma \gamma$ process.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20998;&#24067;&#24335;&#29615;&#22659;&#30340;&#36890;&#20449;&#39640;&#25928;&#30340;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21487;&#25193;&#23637;&#30340;&#26080;&#32447;&#32858;&#21512;&#26041;&#26696;&#21644;&#24102;&#23485;&#26377;&#38480;&#30340;&#24191;&#25773;&#26041;&#26696;&#65292;&#35299;&#20915;&#20102;&#35774;&#22791;&#24178;&#25200;&#21644;&#36793;&#32536;&#26381;&#21153;&#22120;&#24178;&#25200;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.16162</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#20998;&#23618;&#26080;&#32447;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Scalable Hierarchical Over-the-Air Federated Learning. (arXiv:2211.16162v2 [cs.IT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.16162
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20998;&#24067;&#24335;&#29615;&#22659;&#30340;&#36890;&#20449;&#39640;&#25928;&#30340;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21487;&#25193;&#23637;&#30340;&#26080;&#32447;&#32858;&#21512;&#26041;&#26696;&#21644;&#24102;&#23485;&#26377;&#38480;&#30340;&#24191;&#25773;&#26041;&#26696;&#65292;&#35299;&#20915;&#20102;&#35774;&#22791;&#24178;&#25200;&#21644;&#36793;&#32536;&#26381;&#21153;&#22120;&#24178;&#25200;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#21253;&#21547;&#26680;&#24515;&#26381;&#21153;&#22120;&#21644;&#22810;&#20010;&#36793;&#32536;&#26381;&#21153;&#22120;&#21450;&#35774;&#22791;&#38598;&#32676;&#30340;&#20998;&#24067;&#24335;&#29615;&#22659;&#30340;&#36890;&#20449;&#39640;&#25928;&#30340;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#12290;&#20551;&#35774;&#19981;&#21516;&#30340;&#23398;&#20064;&#20219;&#21153;&#65292;&#20855;&#26377;&#30456;&#21516;&#20219;&#21153;&#30340;&#38598;&#32676;&#36827;&#34892;&#21327;&#20316;&#12290;&#20026;&#20102;&#22312;&#26080;&#32447;&#38142;&#36335;&#19978;&#23454;&#29616;&#31639;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#20998;&#31751;&#26080;&#32447;&#32858;&#21512;&#26041;&#26696;&#65292;&#29992;&#20110;&#19978;&#34892;&#38142;&#36335;&#65292;&#21516;&#26102;&#37319;&#29992;&#24102;&#23485;&#26377;&#38480;&#30340;&#24191;&#25773;&#26041;&#26696;&#29992;&#20110;&#19979;&#34892;&#38142;&#36335;&#65292;&#27599;&#20010;&#31639;&#27861;&#36845;&#20195;&#21482;&#38656;&#35201;&#19968;&#20010;&#36164;&#28304;&#22359;&#65292;&#19981;&#21463;&#36793;&#32536;&#26381;&#21153;&#22120;&#21644;&#35774;&#22791;&#25968;&#37327;&#30340;&#24433;&#21709;&#12290;&#36825;&#31181;&#35774;&#32622;&#38754;&#20020;&#30528;&#19978;&#34892;&#38142;&#36335;&#35774;&#22791;&#24178;&#25200;&#21644;&#19979;&#34892;&#38142;&#36335;&#36793;&#32536;&#26381;&#21153;&#22120;&#24178;&#25200;&#30340;&#38382;&#39064;&#65292;&#38656;&#35201;&#36827;&#34892;&#20005;&#26684;&#30340;&#24314;&#27169;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#23558;&#35774;&#22791;&#24314;&#27169;&#20026;&#19968;&#20010;&#27850;&#26494;&#38598;&#32676;&#36807;&#31243;&#65292;&#22312;&#35774;&#32622;&#20013;&#24314;&#31435;&#20102;&#19968;&#20010;&#31354;&#38388;&#27169;&#22411;&#65292;&#24182;&#23545;&#30001;&#24178;&#25200;&#24341;&#36215;&#30340;&#19978;&#34892;&#38142;&#36335;&#21644;&#19979;&#34892;&#38142;&#36335;&#30340;&#35823;&#24046;&#36827;&#34892;&#37327;&#21270;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#30340;&#25968;&#23398;&#26041;&#27861;&#26469;&#25512;&#23548;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose a communication-efficient hierarchical federated learning algorithm for distributed setups including core servers and multiple edge servers with clusters of devices. Assuming different learning tasks, clusters with a same task collaborate. To implement the algorithm over wireless links, we propose a scalable clustered over-the-air aggregation scheme for the uplink with a bandwidth-limited broadcast scheme for the downlink that requires only a single resource block for each algorithm iteration, independent of the number of edge servers and devices. This setup is faced with interference of devices in the uplink and interference of edge servers in the downlink that are to be modeled rigorously. We first develop a spatial model for the setup by modeling devices as a Poisson cluster process over the edge servers and quantify uplink and downlink error terms due to the interference. Accordingly, we present a comprehensive mathematical approach to derive the convergenc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;ARMA&#21333;&#20803;&#65292;&#19968;&#31181;&#26356;&#31616;&#21333;&#12289;&#27169;&#22359;&#21270;&#21644;&#26377;&#25928;&#30340;&#31070;&#32463;&#32593;&#32476;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#26041;&#27861;&#65292;&#33021;&#22815;&#33258;&#28982;&#22320;&#22788;&#29702;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#65292;&#24182;&#24341;&#20837;&#20102;ConvARMA&#21333;&#20803;&#20316;&#20026;&#19968;&#31181;&#35299;&#20915;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2208.14919</link><description>&lt;p&gt;
ARMA&#21333;&#20803;&#65306;&#31070;&#32463;&#33258;&#22238;&#24402;&#24314;&#27169;&#30340;&#27169;&#22359;&#21270;&#21644;&#26377;&#25928;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ARMA Cell: A Modular and Effective Approach for Neural Autoregressive Modeling. (arXiv:2208.14919v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.14919
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;ARMA&#21333;&#20803;&#65292;&#19968;&#31181;&#26356;&#31616;&#21333;&#12289;&#27169;&#22359;&#21270;&#21644;&#26377;&#25928;&#30340;&#31070;&#32463;&#32593;&#32476;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#26041;&#27861;&#65292;&#33021;&#22815;&#33258;&#28982;&#22320;&#22788;&#29702;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#65292;&#24182;&#24341;&#20837;&#20102;ConvARMA&#21333;&#20803;&#20316;&#20026;&#19968;&#31181;&#35299;&#20915;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#22238;&#24402;&#31227;&#21160;&#24179;&#22343;(ARMA)&#27169;&#22411;&#26159;&#19968;&#31181;&#32463;&#20856;&#30340;&#12289;&#34987;&#24191;&#27867;&#30740;&#31350;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#24314;&#27169;&#26041;&#27861;&#12290;&#23427;&#20855;&#26377;&#24341;&#20154;&#20837;&#32988;&#30340;&#29702;&#35770;&#24615;&#36136;&#65292;&#22312;&#23454;&#36341;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#12290;&#26368;&#36817;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26222;&#21450;&#20102;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;(RNN)&#65292;&#29305;&#21035;&#26159;&#38271;&#30701;&#26399;&#35760;&#24518;(LSTM)&#21333;&#20803;&#65292;&#22312;&#31070;&#32463;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#20013;&#25104;&#20026;&#34920;&#29616;&#26368;&#22909;&#21644;&#26368;&#24120;&#35265;&#30340;&#26500;&#24314;&#27169;&#22359;&#20043;&#19968;&#12290;&#34429;&#28982;&#22797;&#26434;&#30340;RNN&#21333;&#20803;&#23545;&#20110;&#20855;&#26377;&#38271;&#26399;&#24433;&#21709;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#25110;&#24207;&#21015;&#26377;&#20248;&#21183;&#65292;&#20294;&#24182;&#19981;&#24635;&#26159;&#24517;&#38656;&#30340;&#65292;&#26377;&#26102;&#29978;&#33267;&#19981;&#22914;&#26356;&#31616;&#21333;&#30340;&#24490;&#29615;&#26041;&#27861;&#22909;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;ARMA&#21333;&#20803;&#65292;&#36825;&#26159;&#19968;&#31181;&#26356;&#31616;&#21333;&#12289;&#27169;&#22359;&#21270;&#21644;&#26377;&#25928;&#30340;&#31070;&#32463;&#32593;&#32476;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#26041;&#27861;&#12290;&#36825;&#20010;&#21333;&#20803;&#21487;&#20197;&#22312;&#20219;&#20309;&#23384;&#22312;&#24490;&#29615;&#32467;&#26500;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20013;&#20351;&#29992;&#65292;&#24182;&#19988;&#21487;&#20197;&#33258;&#28982;&#22320;&#22788;&#29702;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#65292;&#20351;&#29992;&#21521;&#37327;&#33258;&#22238;&#24402;&#25216;&#26415;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;ConvARMA&#21333;&#20803;&#20316;&#20026;&#19968;&#31181;&#33258;&#28982;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The autoregressive moving average (ARMA) model is a classical, and arguably one of the most studied approaches to model time series data. It has compelling theoretical properties and is widely used among practitioners. More recent deep learning approaches popularize recurrent neural networks (RNNs) and, in particular, Long Short-Term Memory (LSTM) cells that have become one of the best performing and most common building blocks in neural time series modeling. While advantageous for time series data or sequences with long-term effects, complex RNN cells are not always a must and can sometimes even be inferior to simpler recurrent approaches. In this work, we introduce the ARMA cell, a simpler, modular, and effective approach for time series modeling in neural networks. This cell can be used in any neural network architecture where recurrent structures are present and naturally handles multivariate time series using vector autoregression. We also introduce the ConvARMA cell as a natural 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;CP-PINNs&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;PINNs&#19982;&#24635;&#21464;&#24046;&#24809;&#32602;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#21464;&#28857;&#26816;&#27979;&#21644;PDE&#30340;&#21457;&#29616;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#20803;&#23398;&#20064;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#25968;&#25454;&#30340;&#36830;&#32493;&#25209;&#27425;&#19978;&#21160;&#24577;&#25913;&#36827;&#20248;&#21270;&#30446;&#26631;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#23384;&#22312;&#21464;&#28857;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20934;&#30830;&#20272;&#35745;&#21442;&#25968;&#21644;&#27169;&#22411;&#23545;&#40784;&#65292;&#22312;&#27809;&#26377;&#21464;&#28857;&#30340;&#24773;&#20917;&#19979;&#33021;&#22815;&#25968;&#20540;&#19978;&#25910;&#25947;&#21040;&#21407;&#22987;PINNs&#27169;&#22411;&#30340;&#35299;&#12290;</title><link>http://arxiv.org/abs/2208.08626</link><description>&lt;p&gt;
CP-PINNs: &#20351;&#29992;&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;&#21644;&#24635;&#21464;&#24046;&#24809;&#32602;&#36827;&#34892;PDE&#20013;&#30340;&#21464;&#28857;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
CP-PINNs: Changepoints Detection in PDEs using Physics Informed Neural Networks with Total-Variation Penalty. (arXiv:2208.08626v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.08626
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;CP-PINNs&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;PINNs&#19982;&#24635;&#21464;&#24046;&#24809;&#32602;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#21464;&#28857;&#26816;&#27979;&#21644;PDE&#30340;&#21457;&#29616;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#20803;&#23398;&#20064;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#25968;&#25454;&#30340;&#36830;&#32493;&#25209;&#27425;&#19978;&#21160;&#24577;&#25913;&#36827;&#20248;&#21270;&#30446;&#26631;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#23384;&#22312;&#21464;&#28857;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20934;&#30830;&#20272;&#35745;&#21442;&#25968;&#21644;&#27169;&#22411;&#23545;&#40784;&#65292;&#22312;&#27809;&#26377;&#21464;&#28857;&#30340;&#24773;&#20917;&#19979;&#33021;&#22815;&#25968;&#20540;&#19978;&#25910;&#25947;&#21040;&#21407;&#22987;PINNs&#27169;&#22411;&#30340;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#22312;&#21442;&#25968;&#20013;&#23384;&#22312;&#26410;&#30693;&#21464;&#28857;&#30340;&#24773;&#20917;&#19979;&#65292;&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#21487;&#33021;&#26080;&#27861;&#27491;&#30830;&#20272;&#35745;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#30340;&#21160;&#24577;&#36807;&#31243;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;CP-PINNs&#27169;&#22411;&#65292;&#23558;PINNs&#19982;&#24635;&#21464;&#24046;&#24809;&#32602;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#20934;&#30830;&#30340;&#21464;&#28857;&#26816;&#27979;&#21644;PDE&#30340;&#21457;&#29616;&#12290;&#20026;&#20102;&#22312;&#27169;&#22411;&#25311;&#21512;&#12289;PDE&#21457;&#29616;&#21644;&#21464;&#28857;&#26816;&#27979;&#20219;&#21153;&#20043;&#38388;&#36827;&#34892;&#26368;&#20248;&#32452;&#21512;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#20803;&#23398;&#20064;&#31639;&#27861;&#65292;&#21033;&#29992;&#25209;&#37327;&#23398;&#20064;&#22312;&#25968;&#25454;&#30340;&#36830;&#32493;&#25209;&#27425;&#19978;&#21160;&#24577;&#25913;&#36827;&#20248;&#21270;&#30446;&#26631;&#12290;&#22312;&#23454;&#35777;&#26041;&#38754;&#65292;&#22312;&#21160;&#24577;&#36807;&#31243;&#20013;&#23384;&#22312;&#21464;&#28857;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#20934;&#30830;&#20272;&#35745;&#21442;&#25968;&#21644;&#27169;&#22411;&#23545;&#40784;&#65292;&#22312;&#25968;&#25454;&#20013;&#27809;&#26377;&#21464;&#28857;&#30340;&#24773;&#20917;&#19979;&#65292;&#25968;&#20540;&#19978;&#25910;&#25947;&#21040;&#21407;&#22987;PINNs&#27169;&#22411;&#30340;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper shows that Physics-Informed Neural Networks (PINNs) can fail to estimate the correct Partial Differential Equations (PDEs) dynamics in cases of unknown changepoints in the parameters. To address this, we propose a new CP-PINNs model which integrates PINNs with Total-Variation penalty for accurate changepoints detection and PDEs discovery. In order to optimally combine the tasks of model fitting, PDEs discovery, and changepoints detection, we develop a new meta-learning algorithm that exploits batch learning to dynamically refines the optimization objective when moving over the consecutive batches of the data. Empirically, in case of changepoints in the dynamics, our approach demonstrates accurate parameter estimation and model alignment, and in case of no changepoints in the data, it converges numerically to the solution from the original PINNs model.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#30740;&#31350;&#20102;&#21387;&#32553;&#24863;&#30693;MRI&#20013;&#30340;&#23616;&#37096;&#23545;&#25239;&#24615;&#20266;&#20687;&#65292;&#24182;&#21457;&#29616;&#19982;&#20256;&#32479;&#24635;&#21464;&#24046;&#65288;TV&#65289;&#26368;&#23567;&#21270;&#30456;&#27604;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#22312;&#25269;&#25239;&#35813;&#23545;&#25239;&#25200;&#21160;&#26041;&#38754;&#20855;&#26377;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2206.05289</link><description>&lt;p&gt;
&#21387;&#32553;&#24863;&#30693;MRI&#20013;&#30340;&#23616;&#37096;&#23545;&#25239;&#24615;&#20266;&#20687;
&lt;/p&gt;
&lt;p&gt;
Localized adversarial artifacts for compressed sensing MRI. (arXiv:2206.05289v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.05289
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#21387;&#32553;&#24863;&#30693;MRI&#20013;&#30340;&#23616;&#37096;&#23545;&#25239;&#24615;&#20266;&#20687;&#65292;&#24182;&#21457;&#29616;&#19982;&#20256;&#32479;&#24635;&#21464;&#24046;&#65288;TV&#65289;&#26368;&#23567;&#21270;&#30456;&#27604;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#22312;&#25269;&#25239;&#35813;&#23545;&#25239;&#25200;&#21160;&#26041;&#38754;&#20855;&#26377;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#22312;&#22270;&#20687;&#37325;&#24314;&#20219;&#21153;&#20013;&#30340;&#20852;&#36259;&#22686;&#21152;&#65292;&#23427;&#20204;&#30340;&#21487;&#38752;&#24615;&#21463;&#21040;&#20102;&#36136;&#30097;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#36866;&#24403;&#27491;&#21017;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#19982;&#24635;&#21464;&#24046;&#65288;TV&#65289;&#26368;&#23567;&#21270;&#30456;&#27604;&#65292;DNN&#22312;$\ell^2$&#37325;&#24314;&#35823;&#24046;&#26041;&#38754;&#26174;&#31034;&#20986;&#31867;&#20284;&#30340;&#23545;&#25239;&#22122;&#22768;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#31181;&#19981;&#21516;&#30340;&#40065;&#26834;&#24615;&#27010;&#24565;&#65292;&#20351;&#29992;$\ell^\infty$&#33539;&#25968;&#65292;&#35748;&#20026;&#23616;&#37096;&#37325;&#24314;&#20266;&#20687;&#27604;$\ell^2$&#35823;&#24046;&#26356;&#30456;&#20851;&#12290;&#25105;&#20204;&#23545;&#39057;&#22495;&#19979;&#37319;&#26679;&#30340;&#30913;&#20849;&#25391;&#25104;&#20687;&#27979;&#37327;&#21019;&#24314;&#20102;&#23545;&#25239;&#24615;&#25200;&#21160;&#65292;&#35813;&#25200;&#21160;&#22312;TV&#27491;&#21017;&#21270;&#37325;&#24314;&#20013;&#24341;&#36215;&#20005;&#37325;&#30340;&#23616;&#37096;&#20266;&#20687;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#21516;&#26679;&#30340;&#25915;&#20987;&#26041;&#27861;&#23545;&#22522;&#20110;DNN&#30340;&#37325;&#24314;&#25928;&#26524;&#19981;&#22914;&#27492;&#26174;&#33879;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#29616;&#35937;&#22312;&#21487;&#20197;&#20445;&#35777;&#31934;&#30830;&#24674;&#22797;&#30340;&#37325;&#24314;&#26041;&#27861;&#20013;&#26159;&#22266;&#26377;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
As interest in deep neural networks (DNNs) for image reconstruction tasks grows, their reliability has been called into question (Antun et al., 2020; Gottschling et al., 2020). However, recent work has shown that, compared to total variation (TV) minimization, when appropriately regularized, DNNs show similar robustness to adversarial noise in terms of $\ell^2$-reconstruction error (Genzel et al., 2022). We consider a different notion of robustness, using the $\ell^\infty$-norm, and argue that localized reconstruction artifacts are a more relevant defect than the $\ell^2$-error. We create adversarial perturbations to undersampled magnetic resonance imaging measurements (in the frequency domain) which induce severe localized artifacts in the TV-regularized reconstruction. Notably, the same attack method is not as effective against DNN based reconstruction. Finally, we show that this phenomenon is inherent to reconstruction methods for which exact recovery can be guaranteed, as with comp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#27599;&#36718;&#20165;&#35266;&#23519;&#37096;&#20998;&#26410;&#30693;&#21327;&#26041;&#24046;&#30340;&#39640;&#26031;&#21521;&#37327;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#22343;&#26041;&#35823;&#24046;&#20272;&#35745;&#30340;&#39034;&#24207;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#36830;&#32493;&#28040;&#38500;&#31639;&#27861;&#30340;&#19968;&#31181;&#21464;&#20307;&#12290;&#21516;&#26102;&#65292;&#23548;&#20986;&#20102;&#26679;&#26412;&#22797;&#26434;&#24615;&#30340;&#26497;&#23567;&#20540;&#19979;&#30028;&#12290;</title><link>http://arxiv.org/abs/2203.16810</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#20272;&#35745;&#24102;&#26377;&#36172;&#21338;&#21453;&#39304;&#30340;&#38543;&#26426;&#21521;&#37327;&#65306;&#20174;&#22343;&#26041;&#35823;&#24046;&#35270;&#35282;&#26469;&#30475;
&lt;/p&gt;
&lt;p&gt;
Adaptive Estimation of Random Vectors with Bandit Feedback: A mean-squared error viewpoint. (arXiv:2203.16810v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.16810
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#27599;&#36718;&#20165;&#35266;&#23519;&#37096;&#20998;&#26410;&#30693;&#21327;&#26041;&#24046;&#30340;&#39640;&#26031;&#21521;&#37327;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#22343;&#26041;&#35823;&#24046;&#20272;&#35745;&#30340;&#39034;&#24207;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#36830;&#32493;&#28040;&#38500;&#31639;&#27861;&#30340;&#19968;&#31181;&#21464;&#20307;&#12290;&#21516;&#26102;&#65292;&#23548;&#20986;&#20102;&#26679;&#26412;&#22797;&#26434;&#24615;&#30340;&#26497;&#23567;&#20540;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#22312;&#27599;&#36718;&#35266;&#23519;&#20165;&#26377;$ m &lt; K $&#20010;&#26410;&#30693;&#21327;&#26041;&#24046;&#30340;&#39640;&#26031;$ K $&#21521;&#37327;&#30340;&#38382;&#39064;&#19979;&#65292;&#36890;&#36807;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#20272;&#35745;&#39034;&#24207;&#23398;&#20064;&#12290;&#25105;&#20204;&#39318;&#20808;&#24314;&#31435;&#20102;MSE&#20272;&#35745;&#30340;&#38598;&#20013;&#30028;&#38480;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#36172;&#21338;&#21453;&#39304;&#30340;&#26041;&#27861;&#37325;&#26032;&#26500;&#24314;&#20272;&#35745;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;&#28040;&#38500;&#31639;&#27861;&#30340;&#21464;&#20307;&#12290;&#25105;&#20204;&#36824;&#23548;&#20986;&#20102;&#19968;&#20010;&#26497;&#23567;&#20540;&#19979;&#30028;&#65292;&#20197;&#20102;&#35299;&#35813;&#38382;&#39064;&#26679;&#26412;&#22797;&#26434;&#24615;&#30340;&#22522;&#26412;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of sequentially learning to estimate, in the mean squared error (MSE) sense, a Gaussian $K$-vector of unknown covariance by observing only $m &lt; K$ of its entries in each round. We first establish a concentration bound for MSE estimation. We then frame the estimation problem with bandit feedback, and propose a variant of the successive elimination algorithm. We also derive a minimax lower bound to understand the fundamental limit on the sample complexity of this problem.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#21472;&#21152;&#38598;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#38745;&#24577;&#26080;&#36335;&#24452;&#20272;&#35745;&#21040;&#36798;&#26102;&#38388;&#12290;&#35813;&#27169;&#22411;&#23558;&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#32452;&#21512;&#25104;&#19968;&#20010;&#26032;&#30340;&#38598;&#25104;&#32467;&#26500;&#65292;&#33021;&#22815;&#36229;&#36234;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2203.09438</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#38745;&#24577;&#26080;&#36335;&#24452;&#20272;&#35745;&#21040;&#36798;&#26102;&#38388;&#30340;&#21487;&#35299;&#37322;&#21472;&#21152;&#38598;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
An Explainable Stacked Ensemble Model for Static Route-Free Estimation of Time of Arrival. (arXiv:2203.09438v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.09438
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#21472;&#21152;&#38598;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#38745;&#24577;&#26080;&#36335;&#24452;&#20272;&#35745;&#21040;&#36798;&#26102;&#38388;&#12290;&#35813;&#27169;&#22411;&#23558;&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#32452;&#21512;&#25104;&#19968;&#20010;&#26032;&#30340;&#38598;&#25104;&#32467;&#26500;&#65292;&#33021;&#22815;&#36229;&#36234;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#27604;&#36739;&#22791;&#36873;&#30340;&#20986;&#31199;&#36710;&#34892;&#31243;&#24182;&#35745;&#31639;&#23427;&#20204;&#65292;&#20197;&#21450;&#20026;&#39550;&#39542;&#21592;&#21644;&#20056;&#23458;&#25552;&#20379;&#20851;&#20110;&#21363;&#23558;&#21040;&#26469;&#30340;&#20986;&#31199;&#36710;&#34892;&#31243;&#30340;&#35265;&#35299;&#65292;&#38656;&#35201;&#39044;&#27979;&#34892;&#31243;&#30340;&#25345;&#32493;&#26102;&#38388;&#25110;&#20854;&#39044;&#35745;&#21040;&#36798;&#26102;&#38388;&#65288;ETA&#65289;&#12290;&#20026;&#20102;&#36798;&#21040;&#36739;&#39640;&#30340;&#39044;&#27979;&#31934;&#24230;&#65292;ETA&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26159;&#30446;&#21069;&#30340;&#25216;&#26415;&#27700;&#24179;&#12290;&#36827;&#19968;&#27493;&#25552;&#39640;&#39044;&#27979;&#31934;&#24230;&#30340;&#19968;&#20010;&#23578;&#26410;&#24320;&#21457;&#30340;&#36873;&#39033;&#26159;&#23558;&#22810;&#20010;ETA&#27169;&#22411;&#32452;&#21512;&#25104;&#19968;&#20010;&#38598;&#25104;&#27169;&#22411;&#12290;&#23613;&#31649;&#39044;&#27979;&#31934;&#24230;&#21487;&#33021;&#20250;&#22686;&#21152;&#65292;&#20294;&#36825;&#31181;&#38598;&#25104;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#30001;&#20110;&#22797;&#26434;&#30340;&#38598;&#25104;&#32467;&#26500;&#32780;&#21464;&#24471;&#19981;&#22815;&#36879;&#26126;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#31181;&#26041;&#27861;&#26159;&#24212;&#29992;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#12290;&#26412;&#25991;&#30340;&#36129;&#29486;&#26377;&#19977;&#20010;&#26041;&#38754;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#20808;&#21069;&#24037;&#20316;&#20013;&#30340;&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#32452;&#21512;&#25104;&#19968;&#20010;&#20004;&#23618;&#30340;&#38598;&#25104;&#27169;&#22411;- &#19968;&#20010;&#21472;&#21152;&#38598;&#25104;&#27169;&#22411;- &#36825;&#26412;&#36523;&#23601;&#26159;&#19968;&#31181;&#21019;&#26032;&#65307;&#22240;&#27492;&#65292;&#25105;&#20204;&#21487;&#20197;&#36229;&#36234;&#20808;&#21069;&#30340;&#38745;&#24577;&#36335;&#24452;&#26080;&#20851;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
To compare alternative taxi schedules and to compute them, as well as to provide insights into an upcoming taxi trip to drivers and passengers, the duration of a trip or its Estimated Time of Arrival (ETA) is predicted. To reach a high prediction precision, machine learning models for ETA are state of the art. One yet unexploited option to further increase prediction precision is to combine multiple ETA models into an ensemble. While an increase of prediction precision is likely, the main drawback is that the predictions made by such an ensemble become less transparent due to the sophisticated ensemble architecture. One option to remedy this drawback is to apply eXplainable Artificial Intelligence (XAI). The contribution of this paper is three-fold. First, we combine multiple machine learning models from our previous work for ETA into a two-level ensemble model - a stacked ensemble model - which on its own is novel; therefore, we can outperform previous state-of-the-art static route-fr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#39069;&#22806;&#35745;&#31639;&#25104;&#26412;&#30340;DNN&#35270;&#35273;&#27880;&#24847;&#26426;&#21046;&#65292;&#21517;&#20026;ISNet&#65292;&#33021;&#22815;&#24573;&#30053;&#22270;&#20687;&#32972;&#26223;&#24182;&#22312;COVID-19&#21644;&#32467;&#26680;&#30149;&#26816;&#27979;&#20219;&#21153;&#20013;&#27604;&#22810;&#20010;&#26368;&#20808;&#36827;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#26356;&#22909;&#30340;&#26368;&#23567;&#21270;&#20998;&#31867;&#22120;&#20915;&#31574;&#20559;&#24046;&#24433;&#21709;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2202.00232</link><description>&lt;p&gt;
&#26080;&#38656;&#25104;&#26412;&#30340;DNN&#35270;&#35273;&#27880;&#24847;&#26426;&#21046;&#65306;&#24573;&#30053;&#32972;&#26223;&#65292;&#25552;&#39640;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Towards Ignoring Backgrounds and Improving Generalization: a Costless DNN Visual Attention Mechanism. (arXiv:2202.00232v6 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.00232
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#39069;&#22806;&#35745;&#31639;&#25104;&#26412;&#30340;DNN&#35270;&#35273;&#27880;&#24847;&#26426;&#21046;&#65292;&#21517;&#20026;ISNet&#65292;&#33021;&#22815;&#24573;&#30053;&#22270;&#20687;&#32972;&#26223;&#24182;&#22312;COVID-19&#21644;&#32467;&#26680;&#30149;&#26816;&#27979;&#20219;&#21153;&#20013;&#27604;&#22810;&#20010;&#26368;&#20808;&#36827;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#26356;&#22909;&#30340;&#26368;&#23567;&#21270;&#20998;&#31867;&#22120;&#20915;&#31574;&#20559;&#24046;&#24433;&#21709;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#27880;&#24847;&#26426;&#21046;&#65292;&#20197;&#21450;&#30456;&#24212;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#20307;&#31995;&#32467;&#26500;&#65292;&#31216;&#20026;ISNet&#12290;&#22312;&#35757;&#32451;&#26399;&#38388;&#65292;ISNet&#20351;&#29992;&#20998;&#21106;&#30446;&#26631;&#26469;&#23398;&#20064;&#22914;&#20309;&#25214;&#21040;&#22270;&#20687;&#30340;&#24863;&#20852;&#36259;&#21306;&#22495;&#65292;&#24182;&#23558;&#27880;&#24847;&#21147;&#38598;&#20013;&#22312;&#35813;&#21306;&#22495;&#19978;&#12290;&#35813;&#25552;&#35758;&#22522;&#20110;&#19968;&#31181;&#26032;&#39062;&#30340;&#27010;&#24565;&#65292;&#21363;&#22312;LRP&#35299;&#37322;&#28909;&#22270;&#20013;&#26368;&#23567;&#21270;&#32972;&#26223;&#30456;&#20851;&#24615;&#12290;&#23427;&#21487;&#20197;&#24212;&#29992;&#20110;&#20960;&#20046;&#20219;&#20309;&#20998;&#31867;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#32780;&#19981;&#20250;&#22312;&#36816;&#34892;&#26102;&#22686;&#21152;&#20219;&#20309;&#39069;&#22806;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#30001;&#20110;&#33021;&#22815;&#24573;&#30053;&#32972;&#26223;&#65292;&#22240;&#27492;&#32467;&#26524;&#21333;&#20010;DNN&#21487;&#20197;&#20195;&#26367;&#24120;&#35265;&#30340;&#20998;&#21106;&#22120;&#21518;&#36319;&#20998;&#31867;&#22120;&#30340;&#27969;&#27700;&#32447;&#65292;&#36895;&#24230;&#26356;&#24555;&#65292;&#26356;&#36731;&#12290;&#22312;&#27880;&#20837;&#22270;&#20687;&#32972;&#26223;&#30340;&#21512;&#25104;&#20559;&#24046;&#20043;&#21518;&#65288;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#65289;&#65292;&#25105;&#20204;&#23558;ISNet&#19982;&#22810;&#20010;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#23450;&#37327;&#35777;&#26126;&#20854;&#22312;&#26368;&#23567;&#21270;&#20998;&#31867;&#22120;&#20915;&#31574;&#20013;&#20559;&#24046;&#24433;&#21709;&#26041;&#38754;&#20855;&#26377;&#20248;&#36234;&#30340;&#33021;&#21147;&#12290; COVID-19&#21644;&#32467;&#26680;&#30149;&#26816;&#27979;&#20219;&#21153;&#20998;&#21035;&#20316;&#20026;&#20351;&#29992;&#26696;&#20363;&#26469;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work introduces an attention mechanism for image classifiers and the corresponding deep neural network (DNN) architecture, dubbed ISNet. During training, the ISNet uses segmentation targets to learn how to find the image's region of interest and concentrate its attention on it. The proposal is based on a novel concept, background relevance minimization in LRP explanation heatmaps. It can be applied to virtually any classification neural network architecture, without any extra computational cost at run-time. Capable of ignoring the background, the resulting single DNN can substitute the common pipeline of a segmenter followed by a classifier, being faster and lighter. After injecting synthetic bias in images' backgrounds (in diverse applications), we compare the ISNet to multiple state-of-the-art neural networks, and quantitatively demonstrate its superior capacity of minimizing the bias influence over the classifier decisions. The tasks of COVID-19 and tuberculosis detection in ch
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;GPEX&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#37322;&#28145;&#24230;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#25512;&#23548;&#20986;&#19968;&#20010;&#35777;&#25454;&#19979;&#30028;&#26469;&#21305;&#37197;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20986;&#65292;&#32780;&#19981;&#23545;&#31070;&#32463;&#32593;&#32476;&#20570;&#20986;&#20219;&#20309;&#29305;&#23450;&#35201;&#27714;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#19968;&#20123;&#29702;&#35770;&#20551;&#35774;&#19979;&#65292;&#21482;&#38656;&#35201;&#31616;&#21333;&#30340;&#32593;&#32476;&#32467;&#26500;&#21363;&#21487;&#36798;&#21040;&#33391;&#22909;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2112.09820</link><description>&lt;p&gt;
GPEX&#65292;&#29992;&#20110;&#35299;&#37322;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
GPEX, A Framework For Interpreting Artificial Neural Networks. (arXiv:2112.09820v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.09820
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;GPEX&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#37322;&#28145;&#24230;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#25512;&#23548;&#20986;&#19968;&#20010;&#35777;&#25454;&#19979;&#30028;&#26469;&#21305;&#37197;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20986;&#65292;&#32780;&#19981;&#23545;&#31070;&#32463;&#32593;&#32476;&#20570;&#20986;&#20219;&#20309;&#29305;&#23450;&#35201;&#27714;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#19968;&#20123;&#29702;&#35770;&#20551;&#35774;&#19979;&#65292;&#21482;&#38656;&#35201;&#31616;&#21333;&#30340;&#32593;&#32476;&#32467;&#26500;&#21363;&#21487;&#36798;&#21040;&#33391;&#22909;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#65288;GPs&#65289;&#19982;&#28145;&#24230;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANNs&#65289;&#20043;&#38388;&#30340;&#31867;&#27604;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#24182;&#26174;&#31034;&#20986;&#25581;&#31034;&#28145;&#24230;ANN&#30340;&#40657;&#31665;&#30340;&#28508;&#21147;&#12290;&#29616;&#26377;&#30340;&#29702;&#35770;&#24037;&#20316;&#23545;ANN&#25552;&#20986;&#20102;&#20005;&#26684;&#30340;&#20551;&#35774;&#65288;&#20363;&#22914;&#65292;&#35201;&#27714;&#25152;&#26377;&#20013;&#38388;&#23618;&#20026;&#23485;&#23618;&#65292;&#25110;&#20351;&#29992;&#29305;&#23450;&#30340;&#28608;&#27963;&#20989;&#25968;&#65289;&#12290;&#36866;&#24212;&#36825;&#20123;&#29702;&#35770;&#20551;&#35774;&#22312;&#26368;&#36817;&#30340;&#28145;&#23618;&#26550;&#26500;&#20013;&#24456;&#22256;&#38590;&#65292;&#24182;&#19988;&#38543;&#30528;&#26032;&#30340;&#28145;&#23618;&#26550;&#26500;&#30340;&#20986;&#29616;&#65292;&#36825;&#20123;&#29702;&#35770;&#26465;&#20214;&#38656;&#35201;&#36827;&#19968;&#27493;&#23436;&#21892;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#19968;&#20010;&#35777;&#25454;&#19979;&#30028;&#65292;&#40723;&#21169;GP&#30340;&#21518;&#39564;&#19982;ANN&#30340;&#36755;&#20986;&#21305;&#37197;&#65292;&#32780;&#19981;&#23545;ANN&#20570;&#20219;&#20309;&#35201;&#27714;&#12290;&#20351;&#29992;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;5&#20010;&#25968;&#25454;&#38598;&#19978;&#65292;&#21482;&#26377;&#19968;&#37096;&#20998;&#29702;&#35770;&#20551;&#35774;&#23601;&#36275;&#22815;&#20102;&#12290;&#23454;&#38469;&#19978;&#65292;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#26222;&#36890;&#30340;ResNet-18&#25110;&#21069;&#39304;&#39592;&#24178;&#32593;&#32476;&#65292;&#24182;&#22312;&#26411;&#31471;&#20351;&#29992;&#20102;&#19968;&#20010;&#23485;&#23618;&#12290;&#35757;&#32451;GPs&#30340;&#19968;&#20010;&#23616;&#38480;&#24615;&#26159;&#22312;&#20110;&#19982;&#35825;&#23548;&#28857;&#25968;&#37327;&#30340;&#21487;&#25193;&#23637;&#24615;&#30340;&#32570;&#20047;&#12290;
&lt;/p&gt;
&lt;p&gt;
The analogy between Gaussian processes (GPs) and deep artificial neural networks (ANNs) has received a lot of interest, and has shown promise to unbox the blackbox of deep ANNs. Existing theoretical works put strict assumptions on the ANN (e.g. requiring all intermediate layers to be wide, or using specific activation functions). Accommodating those theoretical assumptions is hard in recent deep architectures, and those theoretical conditions need refinement as new deep architectures emerge. In this paper we derive an evidence lower-bound that encourages the GP's posterior to match the ANN's output without any requirement on the ANN. Using our method we find out that on 5 datasets, only a subset of those theoretical assumptions are sufficient. Indeed, in our experiments we used a normal ResNet-18 or feed-forward backbone with a single wide layer in the end. One limitation of training GPs is the lack of scalability with respect to the number of inducing points. We use novel computationa
&lt;/p&gt;</description></item></channel></rss>