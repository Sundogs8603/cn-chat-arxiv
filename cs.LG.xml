<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26368;&#23567;&#21270;&#21407;&#22987;Wasserstein&#36317;&#31163;&#26469;&#21305;&#37197;&#19987;&#23478;&#21644;&#23398;&#20064;&#32773;&#29366;&#24577;&#21344;&#29992;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#31163;&#32447;&#23398;&#20064;&#20174;&#35266;&#23519;&#20013;&#27169;&#20223;&#20219;&#21153;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2311.01331</link><description>&lt;p&gt;
&#36890;&#36807;&#21407;&#22987;Wasserstein&#29366;&#24577;&#21344;&#29992;&#21305;&#37197;&#23454;&#29616;&#30340;&#31163;&#32447;&#35266;&#23519;&#27169;&#20223;
&lt;/p&gt;
&lt;p&gt;
Offline Imitation from Observation via Primal Wasserstein State Occupancy Matching. (arXiv:2311.01331v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01331
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26368;&#23567;&#21270;&#21407;&#22987;Wasserstein&#36317;&#31163;&#26469;&#21305;&#37197;&#19987;&#23478;&#21644;&#23398;&#20064;&#32773;&#29366;&#24577;&#21344;&#29992;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#31163;&#32447;&#23398;&#20064;&#20174;&#35266;&#23519;&#20013;&#27169;&#20223;&#20219;&#21153;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24773;&#22659;&#20013;&#65292;&#19982;&#29615;&#22659;&#30340;&#20219;&#24847;&#20132;&#20114;&#24448;&#24448;&#26159;&#26114;&#36149;&#30340;&#65292;&#24182;&#19988;&#19987;&#23478;&#31034;&#33539;&#30340;&#34892;&#20026;&#24182;&#19981;&#24635;&#26159;&#21487;&#29992;&#30340;&#12290;&#20026;&#20102;&#20943;&#23569;&#36825;&#20004;&#32773;&#30340;&#38656;&#27714;&#65292;&#31163;&#32447;&#23398;&#20064;&#20174;&#35266;&#23519;&#65288;LfO&#65289;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20854;&#20013;&#20195;&#29702;&#36890;&#36807;&#21482;&#26377;&#19987;&#23478;&#29366;&#24577;&#21644;&#19982;&#20219;&#21153;&#26080;&#20851;&#30340;&#38750;&#19987;&#23478;&#29366;&#24577;-&#21160;&#20316;&#23545;&#26469;&#23398;&#20064;&#35299;&#20915;&#20219;&#21153;&#12290;&#26368;&#20808;&#36827;&#30340;&#20998;&#24067;&#26657;&#27491;&#20272;&#35745;&#65288;DICE&#65289;&#26041;&#27861;&#26368;&#23567;&#21270;&#20102;&#23398;&#20064;&#32773;&#21644;&#19987;&#23478;&#31574;&#30053;&#20043;&#38388;&#30340;&#29366;&#24577;&#21344;&#29992;&#24046;&#24322;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20165;&#38480;&#20110;$f$-divergences&#65288;KL&#21644;$\chi^2$&#65289;&#25110;&#24102;&#26377;Rubinstein&#23545;&#20598;&#30340;Wasserstein&#36317;&#31163;&#65292;&#21518;&#32773;&#38480;&#21046;&#20102;&#23545;&#24615;&#33021;&#20851;&#38190;&#30340;&#22522;&#30784;&#36317;&#31163;&#24230;&#37327;&#30340;&#20351;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21407;&#22987;Wasserstein DICE&#65288;PW-DICE&#65289;&#65292;&#23427;&#36890;&#36807;&#24754;&#35266;&#27491;&#21017;&#21270;&#22120;&#26368;&#23567;&#21270;&#19987;&#23478;&#21644;&#23398;&#20064;&#32773;&#29366;&#24577;&#21344;&#29992;&#20043;&#38388;&#30340;&#21407;&#22987;Wasserstein&#36317;&#31163;&#65292;&#24182;&#21033;&#29992;&#20102;&#23545;&#27604;&#23398;&#20064;&#30340;dis
&lt;/p&gt;
&lt;p&gt;
In real-world scenarios, arbitrary interactions with the environment can often be costly, and actions of expert demonstrations are not always available. To reduce the need for both, Offline Learning from Observations (LfO) is extensively studied, where the agent learns to solve a task with only expert states and \textit{task-agnostic} non-expert state-action pairs. The state-of-the-art DIstribution Correction Estimation (DICE) methods minimize the state occupancy divergence between the learner and expert policies. However, they are limited to either $f$-divergences (KL and $\chi^2$) or Wasserstein distance with Rubinstein duality, the latter of which constrains the underlying distance metric crucial to the performance of Wasserstein-based solutions. To address this problem, we propose Primal Wasserstein DICE (PW-DICE), which minimizes the primal Wasserstein distance between the expert and learner state occupancies with a pessimistic regularizer and leverages a contrastively learned dis
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#28608;&#27963;&#35270;&#35282;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#31934;&#24515;&#36873;&#25321;&#36739;&#23569;&#30340;&#25968;&#25454;&#25552;&#39640;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#36825;&#19968;&#26041;&#27861;&#33021;&#22815;&#22312;&#22270;&#25968;&#25454;&#20013;&#36873;&#25321;&#26368;&#20855;&#20195;&#34920;&#24615;&#21644;&#25351;&#23548;&#24615;&#30340;&#25968;&#25454;&#28857;&#36827;&#34892;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2311.01038</link><description>&lt;p&gt;
&#26356;&#23569;&#26356;&#22909;&#65306;&#22522;&#20110;&#25968;&#25454;&#28608;&#27963;&#35270;&#35282;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Better with Less: A Data-Active Perspective on Pre-Training Graph Neural Networks. (arXiv:2311.01038v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01038
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#28608;&#27963;&#35270;&#35282;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#31934;&#24515;&#36873;&#25321;&#36739;&#23569;&#30340;&#25968;&#25454;&#25552;&#39640;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#36825;&#19968;&#26041;&#27861;&#33021;&#22815;&#22312;&#22270;&#25968;&#25454;&#20013;&#36873;&#25321;&#26368;&#20855;&#20195;&#34920;&#24615;&#21644;&#25351;&#23548;&#24615;&#30340;&#25968;&#25454;&#28857;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#39044;&#35757;&#32451;&#26088;&#22312;&#21033;&#29992;&#26080;&#26631;&#27880;&#25968;&#25454;&#23398;&#20064;&#21487;&#36801;&#31227;&#30340;&#30693;&#35782;&#20197;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#65292;&#24182;&#19988;&#26368;&#36817;&#24050;&#25104;&#20026;&#19968;&#20010;&#27963;&#36291;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#22270;&#39044;&#35757;&#32451;&#20013;&#23384;&#22312;&#30528;&#22823;&#25968;&#25454;&#30340;&#35781;&#21650;&#29616;&#35937;&#65306;&#26356;&#22810;&#30340;&#35757;&#32451;&#25968;&#25454;&#24182;&#19981;&#19968;&#23450;&#23548;&#33268;&#26356;&#22909;&#30340;&#19979;&#28216;&#24615;&#33021;&#12290;&#21463;&#36825;&#20010;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#23569;&#26356;&#22909;&#30340;&#22270;&#39044;&#35757;&#32451;&#26694;&#26550;&#65306;&#36873;&#25321;&#23569;&#37327;&#20294;&#31934;&#24515;&#36873;&#25321;&#30340;&#25968;&#25454;&#36755;&#20837;&#21040;GNN&#27169;&#22411;&#20013;&#20197;&#22686;&#24378;&#39044;&#35757;&#32451;&#12290;&#25152;&#25552;&#20986;&#30340;&#39044;&#35757;&#32451;&#27969;&#31243;&#34987;&#31216;&#20026;&#25968;&#25454;&#28608;&#27963;&#22270;&#39044;&#35757;&#32451;&#65288;APT&#65289;&#26694;&#26550;&#65292;&#30001;&#22270;&#36873;&#25321;&#22120;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#32452;&#25104;&#12290;&#22270;&#36873;&#25321;&#22120;&#26681;&#25454;&#22270;&#30340;&#22266;&#26377;&#23646;&#24615;&#21644;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#36873;&#25321;&#26368;&#20855;&#20195;&#34920;&#24615;&#21644;&#25351;&#23548;&#24615;&#30340;&#25968;&#25454;&#28857;&#12290;&#25152;&#25552;&#20986;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#20316;&#20026;&#26469;&#33258;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21453;&#39304;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-training on graph neural networks (GNNs) aims to learn transferable knowledge for downstream tasks with unlabeled data, and it has recently become an active research area. The success of graph pre-training models is often attributed to the massive amount of input data. In this paper, however, we identify the curse of big data phenomenon in graph pre-training: more training data do not necessarily lead to better downstream performance. Motivated by this observation, we propose a better-with-less framework for graph pre-training: fewer, but carefully chosen data are fed into a GNN model to enhance pre-training. The proposed pre-training pipeline is called the data-active graph pre-training (APT) framework, and is composed of a graph selector and a pre-training model. The graph selector chooses the most representative and instructive data points based on the inherent properties of graphs as well as predictive uncertainty. The proposed predictive uncertainty, as feedback from the pre-t
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SERA&#30340;&#22870;&#21169;&#22686;&#24378;&#26694;&#26550;&#65292;&#29992;&#20110;&#25913;&#21892;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#33021;&#21147;&#12290;&#23427;&#36890;&#36807;&#35774;&#35745;&#20869;&#22312;&#22870;&#21169;&#26469;&#40723;&#21169;agent&#36827;&#34892;&#25506;&#32034;&#65292;&#24182;&#23454;&#29616;&#26356;&#22909;&#30340;&#22312;&#32447;&#24494;&#35843;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.19805</link><description>&lt;p&gt;
SERA&#65306;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26679;&#26412;&#39640;&#25928;&#22870;&#21169;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
SERA:Sample Efficient Reward Augmentation in offline-to-online Reinforcement Learning. (arXiv:2310.19805v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19805
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SERA&#30340;&#22870;&#21169;&#22686;&#24378;&#26694;&#26550;&#65292;&#29992;&#20110;&#25913;&#21892;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#33021;&#21147;&#12290;&#23427;&#36890;&#36807;&#35774;&#35745;&#20869;&#22312;&#22870;&#21169;&#26469;&#40723;&#21169;agent&#36827;&#34892;&#25506;&#32034;&#65292;&#24182;&#23454;&#29616;&#26356;&#22909;&#30340;&#22312;&#32447;&#24494;&#35843;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#19968;&#20010;&#28508;&#22312;&#24212;&#29992;&#26159;&#20351;&#29992;&#29616;&#26377;&#30340;&#38745;&#24577;&#25968;&#25454;&#38598;&#26469;&#21021;&#22987;&#21270;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#28982;&#21518;&#36827;&#34892;&#21518;&#32493;&#22312;&#32447;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#23545;&#31163;&#32447;&#39044;&#35757;&#32451;&#31574;&#30053;&#36827;&#34892;&#24494;&#35843;&#24448;&#24448;&#20250;&#23548;&#33268;&#27425;&#20248;&#24615;&#33021;&#12290;&#20027;&#35201;&#21407;&#22240;&#26159;&#31163;&#32447;&#20445;&#23432;&#26041;&#27861;&#38477;&#20302;&#20102;agent&#30340;&#25506;&#32034;&#33021;&#21147;&#65292;&#20174;&#32780;&#24433;&#21709;&#20102;&#22312;&#32447;&#24494;&#35843;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#22686;&#24378;&#22312;&#32447;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#25506;&#32034;&#33021;&#21147;&#65292;&#20174;&#32780;&#25552;&#39640;&#25972;&#20307;&#30340;&#22312;&#32447;&#24494;&#35843;&#24615;&#33021;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#26679;&#26412;&#39640;&#25928;&#22870;&#21169;&#22686;&#24378;&#65288;SERA&#65289;&#30340;&#36890;&#29992;&#22870;&#21169;&#22686;&#24378;&#26694;&#26550;&#12290;SERA&#26088;&#22312;&#36890;&#36807;&#35774;&#35745;&#40723;&#21169;agent&#36827;&#34892;&#25506;&#32034;&#30340;&#20869;&#22312;&#22870;&#21169;&#26469;&#25913;&#21892;&#22312;&#32447;&#24494;&#35843;&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#23427;&#38544;&#24335;&#22320;&#23454;&#29616;&#20102;&#29366;&#24577;&#36793;&#32536;&#21305;&#37197;&#65288;SMM&#65289;&#24182;&#24809;&#32602;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#29366;&#24577;&#34892;&#21160;&#65292;&#20174;&#32780;&#40723;&#21169;agent&#35206;&#30422;&#30446;&#26631;&#29366;&#24577;&#23494;&#24230;&#65292;&#24182;&#23454;&#29616;&#26356;&#22909;&#30340;&#22312;&#32447;&#24494;&#35843;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
A prospective application of offline reinforcement learning (RL) involves initializing a pre-trained policy using existing static datasets for subsequent online fine-tuning. However, direct fine-tuning of the offline pre-trained policy often results in sub-optimal performance. A primary reason is that offline conservative methods diminish the agent's capability of exploration, thereby impacting online fine-tuning performance. To enhance exploration during online fine-tuning and thus enhance the overall online fine-tuning performance, we introduce a generalized reward augmentation framework called Sample Efficient Reward Augmentation (SERA). SERA aims to improve the performance of online fine-tuning by designing intrinsic rewards that encourage the agent to explore. Specifically, it implicitly implements State Marginal Matching (SMM) and penalizes out-of-distribution (OOD) state actions, thus encouraging agents to cover the target state density, and achieving better online fine-tuning r
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#20154;&#35774;&#26469;&#24314;&#27169;&#30495;&#23454;&#24615;&#30340;&#21487;&#33021;&#24615;&#12290;&#36890;&#36807;&#24314;&#27169;&#30495;&#23454;&#20154;&#35774;&#65292;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23558;&#30495;&#23454;&#24615;&#25512;&#24191;&#21040;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#65292;&#24182;&#36890;&#36807;&#30456;&#20851;&#29305;&#24449;&#21028;&#26029;&#20010;&#20307;&#20135;&#29983;&#25991;&#26412;&#30340;&#30495;&#23454;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.18168</link><description>&lt;p&gt;
&#20351;&#29992;&#20154;&#35774;&#26469;&#24314;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30495;&#23454;&#24615;
&lt;/p&gt;
&lt;p&gt;
Personas as a Way to Model Truthfulness in Language Models. (arXiv:2310.18168v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18168
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#20154;&#35774;&#26469;&#24314;&#27169;&#30495;&#23454;&#24615;&#30340;&#21487;&#33021;&#24615;&#12290;&#36890;&#36807;&#24314;&#27169;&#30495;&#23454;&#20154;&#35774;&#65292;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23558;&#30495;&#23454;&#24615;&#25512;&#24191;&#21040;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#65292;&#24182;&#36890;&#36807;&#30456;&#20851;&#29305;&#24449;&#21028;&#26029;&#20010;&#20307;&#20135;&#29983;&#25991;&#26412;&#30340;&#30495;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20351;&#29992;&#20114;&#32852;&#32593;&#19978;&#30340;&#22823;&#37327;&#25991;&#26412;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#20123;&#25991;&#26412;&#20013;&#26082;&#21253;&#21547;&#20102;&#20107;&#23454;&#65292;&#20063;&#21253;&#21547;&#20102;&#35823;&#23548;&#24615;&#30340;&#20449;&#24687;&#12290;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20174;&#36825;&#20123;&#30456;&#20114;&#30683;&#30462;&#30340;&#25968;&#25454;&#20013;&#36776;&#21035;&#30495;&#23454;&#19982;&#34394;&#20551;&#21527;&#65311;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#24314;&#27169;&#19981;&#21516;&#20135;&#29983;&#25991;&#26412;&#30340;&#20010;&#20307;&#36825;&#19968;&#35266;&#28857;&#65292;&#25105;&#20204;&#20551;&#35774;&#23427;&#20204;&#21487;&#20197;&#36890;&#36807;&#24314;&#27169;&#30495;&#23454;&#20154;&#35774;&#26469;&#32858;&#31867;&#30495;&#23454;&#25991;&#26412;&#65306;&#19968;&#32676;&#24456;&#21487;&#33021;&#20135;&#29983;&#30495;&#23454;&#25991;&#26412;&#24182;&#20855;&#26377;&#30456;&#20284;&#29305;&#24449;&#30340;&#20010;&#20307;&#12290;&#20363;&#22914;&#65292;&#21487;&#20449;&#28304;&#22914;&#32500;&#22522;&#30334;&#31185;&#21644;&#31185;&#23398;&#26399;&#21002;&#36890;&#24120;&#20351;&#29992;&#27491;&#24335;&#30340;&#20889;&#20316;&#39118;&#26684;&#24182;&#25552;&#20986;&#19968;&#33268;&#30340;&#20027;&#24352;&#12290;&#36890;&#36807;&#24314;&#27169;&#36825;&#19968;&#20154;&#35774;&#65292;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23558;&#30495;&#23454;&#24615;&#25512;&#24191;&#21040;&#27599;&#20010;&#20010;&#20307;&#29983;&#25104;&#35757;&#32451;&#25991;&#26412;&#30340;&#29305;&#23450;&#19978;&#19979;&#25991;&#20043;&#22806;&#12290;&#20363;&#22914;&#65292;&#27169;&#22411;&#21487;&#20197;&#25512;&#26029;&#20986;&#8220;&#32500;&#22522;&#30334;&#31185;&#8221;&#36825;&#20010;&#20010;&#20307;&#22312;&#8220;&#31185;&#23398;&#8221;&#29983;&#25104;&#30340;&#20027;&#39064;&#19978;&#20250;&#34920;&#29616;&#20986;&#30495;&#23454;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#20849;&#20139;&#19968;&#20010;&#20154;&#35774;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#20004;&#20010;&#35266;&#23519;&#32467;&#26524;&#20026;&#20154;&#35774;&#20551;&#35774;&#25552;&#20379;&#20102;&#35777;&#25454;&#65306;&#65288;1&#65289;&#25105;&#20204;&#21487;&#20197;&#25506;&#27979;&#27169;&#22411;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#21028;&#26029;&#30495;&#23454;&#24615;&#30340;&#33021;&#21147;&#65307;&#65288;2&#65289;&#27169;&#22411;&#21487;&#20197;&#20174;&#30456;&#20851;&#29305;&#24449;&#20013;&#25512;&#27979;&#20010;&#20307;&#20135;&#29983;&#25991;&#26412;&#30340;&#30495;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are trained on vast amounts of text from the internet, which contains both factual and misleading information about the world. Can language models discern truth from falsehood in this contradicting data? Expanding on the view that LLMs can model different agents producing the corpora, we hypothesize that they can cluster truthful text by modeling a truthful persona: a group of agents that are likely to produce truthful text and share similar features. For example, trustworthy sources like Wikipedia and Science usually use formal writing styles and make consistent claims. By modeling this persona, LLMs can generalize truthfulness beyond the specific contexts in which each agent generated the training text. For example, the model can infer that the agent "Wikipedia" will behave truthfully on topics that were only generated by "Science" because they share a persona. We first show evidence for the persona hypothesis via two observations: (1) we can probe whether a mod
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#32467;&#26500;&#65292;&#31216;&#20026;&#35268;&#21017;&#21069;&#32512;&#26641;&#65292;&#29992;&#20110;&#23384;&#20648;&#30001;&#20851;&#32852;&#35268;&#21017;&#25366;&#25496;&#29983;&#25104;&#30340;&#35268;&#21017;&#38598;&#12290;&#36825;&#20010;&#25968;&#25454;&#32467;&#26500;&#33021;&#22815;&#39640;&#25928;&#22320;&#34920;&#31034;&#21644;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#25366;&#25496;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2310.17355</link><description>&lt;p&gt;
&#25506;&#32034;&#35268;&#21017;&#21069;&#32512;&#26641;&#65306;&#19968;&#31181;&#29992;&#20110;&#20851;&#32852;&#35268;&#21017;&#34920;&#31034;&#30340;&#39640;&#25928;&#25968;&#25454;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Exploring the Trie of Rules: a fast data structure for the representation of association rules. (arXiv:2310.17355v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17355
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#32467;&#26500;&#65292;&#31216;&#20026;&#35268;&#21017;&#21069;&#32512;&#26641;&#65292;&#29992;&#20110;&#23384;&#20648;&#30001;&#20851;&#32852;&#35268;&#21017;&#25366;&#25496;&#29983;&#25104;&#30340;&#35268;&#21017;&#38598;&#12290;&#36825;&#20010;&#25968;&#25454;&#32467;&#26500;&#33021;&#22815;&#39640;&#25928;&#22320;&#34920;&#31034;&#21644;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#25366;&#25496;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#32852;&#35268;&#21017;&#25366;&#25496;&#25216;&#26415;&#22312;&#20107;&#21153;&#24615;&#25968;&#25454;&#24211;&#19978;&#30340;&#23454;&#29616;&#21487;&#20197;&#29983;&#25104;&#22823;&#37327;&#30340;&#39034;&#24207;&#25968;&#25454;&#12290;&#20174;&#22823;&#37327;&#20851;&#32852;&#35268;&#21017;&#20013;&#25552;&#21462;&#27934;&#23519;&#21147;&#34987;&#21457;&#29616;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#36807;&#31243;&#12290;&#24403;&#26816;&#26597;&#19968;&#32452;&#35268;&#21017;&#26102;&#65292;&#26368;&#22522;&#26412;&#30340;&#38382;&#39064;&#26159;&#22914;&#20309;&#39640;&#25928;&#22320;&#27719;&#24635;&#21644;&#34920;&#31034;&#26377;&#24847;&#20041;&#30340;&#25366;&#25496;&#30693;&#35782;&#12290;&#35768;&#22810;&#31639;&#27861;&#21644;&#31574;&#30053;&#24050;&#32463;&#34987;&#24320;&#21457;&#20986;&#26469;&#26469;&#35299;&#20915;&#30693;&#35782;&#25552;&#21462;&#30340;&#38382;&#39064;&#65307;&#28982;&#32780;&#65292;&#36825;&#20010;&#36807;&#31243;&#30340;&#26377;&#25928;&#24615;&#21487;&#33021;&#21463;&#21040;&#25968;&#25454;&#32467;&#26500;&#30340;&#38480;&#21046;&#12290;&#26356;&#22909;&#30340;&#25968;&#25454;&#32467;&#26500;&#21487;&#20197;&#26377;&#25928;&#22320;&#24433;&#21709;&#30693;&#35782;&#25552;&#21462;&#36807;&#31243;&#30340;&#36895;&#24230;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#32467;&#26500;&#65292;&#31216;&#20026;&#35268;&#21017;&#21069;&#32512;&#26641;&#65292;&#29992;&#20110;&#23384;&#20648;&#30001;&#20851;&#32852;&#35268;&#21017;&#25366;&#25496;&#29983;&#25104;&#30340;&#35268;&#21017;&#38598;&#12290;&#32467;&#26524;&#25968;&#25454;&#32467;&#26500;&#26159;&#19968;&#20010;&#30001;&#39044;&#20808;&#25366;&#25496;&#30340;&#35268;&#21017;&#32452;&#25104;&#30340;&#21069;&#32512;&#26641;&#22270;&#32467;&#26500;&#12290;&#36825;&#20010;&#22270;&#23558;&#35268;&#21017;&#20197;&#36335;&#24452;&#30340;&#26041;&#24335;&#23384;&#20648;&#22312;&#21069;&#32512;&#26641;&#20013;&#65292;&#31867;&#20284;&#30340;&#35268;&#21017;&#20250;&#20114;&#30456;&#35206;&#30422;&#12290;&#26641;&#20013;&#30340;&#27599;&#20010;&#33410;&#28857;&#34920;&#31034;&#19968;&#26465;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
Association rule mining techniques can generate a large volume of sequential data when implemented on transactional databases. Extracting insights from a large set of association rules has been found to be a challenging process. When examining a ruleset, the fundamental question is how to summarise and represent meaningful mined knowledge efficiently. Many algorithms and strategies have been developed to address issue of knowledge extraction; however, the effectiveness of this process can be limited by the data structures. A better data structure can sufficiently affect the speed of the knowledge extraction process. This paper proposes a novel data structure, called the Trie of rules, for storing a ruleset that is generated by association rule mining. The resulting data structure is a prefix-tree graph structure made of pre-mined rules. This graph stores the rules as paths within the prefix-tree in a way that similar rules overlay each other. Each node in the tree represents a rule whe
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#32477;&#23545;&#31574;&#30053;&#20248;&#21270;&#65288;APO&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#19968;&#20010;&#26032;&#39062;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#22312;&#20445;&#35777;&#24615;&#33021;&#19979;&#30028;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#21644;Atari&#28216;&#25103;&#20013;&#30340;&#20196;&#20154;&#30633;&#30446;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.13230</link><description>&lt;p&gt;
&#32477;&#23545;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Absolute Policy Optimization. (arXiv:2310.13230v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13230
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#32477;&#23545;&#31574;&#30053;&#20248;&#21270;&#65288;APO&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#19968;&#20010;&#26032;&#39062;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#22312;&#20445;&#35777;&#24615;&#33021;&#19979;&#30028;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#21644;Atari&#28216;&#25103;&#20013;&#30340;&#20196;&#20154;&#30633;&#30446;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#20449;&#20219;&#22495;&#30340;&#22312;&#32447;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#22312;&#35299;&#20915;&#22797;&#26434;&#25511;&#21046;&#20219;&#21153;&#21644;&#28216;&#25103;&#22330;&#26223;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#31867;&#21035;&#20013;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#20027;&#35201;&#24378;&#35843;&#23545;&#39044;&#26399;&#24615;&#33021;&#30340;&#25913;&#36827;&#65292;&#32570;&#20047;&#23545;&#26368;&#22351;&#24773;&#20917;&#19979;&#24615;&#33021;&#32467;&#26524;&#30340;&#25511;&#21046;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#30446;&#26631;&#20989;&#25968;&#65307;&#36890;&#36807;&#20248;&#21270;&#35813;&#20989;&#25968;&#65292;&#21487;&#20197;&#30830;&#20445;&#36817;&#20046;&#24635;&#20307;&#24615;&#33021;&#26679;&#26412;&#30340;&#19979;&#30028;&#65288;&#32477;&#23545;&#24615;&#33021;&#65289;&#21576;&#29616;&#21333;&#35843;&#25913;&#36827;&#12290;&#32771;&#34385;&#21040;&#36825;&#19968;&#20855;&#26377;&#31361;&#30772;&#24615;&#30340;&#29702;&#35770;&#36827;&#23637;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#31995;&#21015;&#30340;&#36817;&#20284;&#23545;&#36825;&#20010;&#29702;&#35770;&#22522;&#30784;&#31639;&#27861;&#36827;&#34892;&#20102;&#25913;&#36827;&#65292;&#24471;&#21040;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#31216;&#20026;&#32477;&#23545;&#31574;&#30053;&#20248;&#21270;&#65288;APO&#65289;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#36830;&#32493;&#25511;&#21046;&#22522;&#20934;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23558;&#20854;&#36866;&#29992;&#24615;&#25193;&#23637;&#21040;&#25484;&#25569;Atari&#28216;&#25103;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;APO&#22312;&#25552;&#39640;&#24615;&#33021;&#30340;&#21516;&#26102;&#20063;&#26174;&#33879;&#25913;&#21892;&#20102;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, trust region on-policy reinforcement learning has achieved impressive results in addressing complex control tasks and gaming scenarios. However, contemporary state-of-the-art algorithms within this category primarily emphasize improvement in expected performance, lacking the ability to control over the worst-case performance outcomes. To address this limitation, we introduce a novel objective function; by optimizing which, it will lead to guaranteed monotonic improvement in the lower bound of near-total performance samples (absolute performance). Considering this groundbreaking theoretical advancement, we then refine this theoretically grounded algorithm through a series of approximations, resulting in a practical solution called Absolute Policy Optimization (APO). Our experiments demonstrate the effectiveness of our approach across challenging continuous control benchmark tasks and extend its applicability to mastering Atari games. Our findings reveal that APO signifi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#29256;DeepFool&#31639;&#27861;&#65292;&#21517;&#20026;Targeted DeepFool&#65292;&#21487;&#20197;&#38024;&#23545;&#29305;&#23450;&#31867;&#21035;&#36827;&#34892;&#38169;&#35823;&#20998;&#31867;&#65292;&#24182;&#24341;&#20837;&#20102;&#26368;&#23567;&#32622;&#20449;&#24230;&#20998;&#25968;&#35201;&#27714;&#36229;&#21442;&#25968;&#26469;&#25552;&#39640;&#28789;&#27963;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.13019</link><description>&lt;p&gt;
&#36890;&#36807;DeepFool&#31639;&#27861;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#26377;&#38024;&#23545;&#24615;&#30340;&#31867;&#21035;&#25805;&#32437;&#30340;&#23545;&#25239;&#25915;&#20987;&#23450;&#21046;
&lt;/p&gt;
&lt;p&gt;
Tailoring Adversarial Attacks on Deep Neural Networks for Targeted Class Manipulation Using DeepFool Algorithm. (arXiv:2310.13019v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13019
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#29256;DeepFool&#31639;&#27861;&#65292;&#21517;&#20026;Targeted DeepFool&#65292;&#21487;&#20197;&#38024;&#23545;&#29305;&#23450;&#31867;&#21035;&#36827;&#34892;&#38169;&#35823;&#20998;&#31867;&#65292;&#24182;&#24341;&#20837;&#20102;&#26368;&#23567;&#32622;&#20449;&#24230;&#20998;&#25968;&#35201;&#27714;&#36229;&#21442;&#25968;&#26469;&#25552;&#39640;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#37117;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#23545;&#25239;&#25915;&#20987;&#30340;&#26131;&#21463;&#25915;&#20987;&#24615;&#24341;&#36215;&#20102;&#20005;&#37325;&#20851;&#27880;&#12290;&#20102;&#35299;&#36825;&#20123;&#26131;&#21463;&#25915;&#20987;&#24615;&#24182;&#24320;&#21457;&#26377;&#25928;&#30340;&#38450;&#24481;&#26426;&#21046;&#33267;&#20851;&#37325;&#35201;&#12290;DeepFool&#26159;Moosavi-Dezfooli&#31561;&#20154;&#65288;2016&#24180;&#65289;&#25552;&#20986;&#30340;&#19968;&#31181;&#31639;&#27861;&#65292;&#29992;&#20110;&#25214;&#21040;&#23558;&#36755;&#20837;&#22270;&#20687;&#38169;&#35823;&#20998;&#31867;&#30340;&#26368;&#23567;&#25200;&#21160;&#12290;&#28982;&#32780;&#65292;DeepFool&#32570;&#20047;&#26377;&#38024;&#23545;&#24615;&#30340;&#26041;&#27861;&#65292;&#20351;&#20854;&#22312;&#29305;&#23450;&#25915;&#20987;&#22330;&#26223;&#20013;&#30340;&#26377;&#25928;&#24615;&#36739;&#20302;&#12290;&#27492;&#22806;&#65292;&#22312;&#20808;&#21069;&#30340;&#30456;&#20851;&#24037;&#20316;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#20027;&#35201;&#20851;&#27880;&#30340;&#26159;&#25104;&#21151;&#29575;&#65292;&#32780;&#27809;&#26377;&#32771;&#34385;&#22270;&#20687;&#34987;&#25197;&#26354;&#30340;&#31243;&#24230;&#12289;&#22270;&#20687;&#36136;&#37327;&#30340;&#23436;&#25972;&#24615;&#20197;&#21450;&#38169;&#35823;&#20998;&#31867;&#30340;&#32622;&#20449;&#24230;&#27700;&#24179;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Targeted DeepFool&#65292;&#36825;&#26159;DeepFool&#30340;&#22686;&#24378;&#29256;&#65292;&#21487;&#20197;&#38024;&#23545;&#29305;&#23450;&#31867;&#21035;&#36827;&#34892;&#38169;&#35823;&#20998;&#31867;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#26368;&#23567;&#32622;&#20449;&#24230;&#20998;&#25968;&#35201;&#27714;&#36229;&#21442;&#25968;&#26469;&#22686;&#24378;&#28789;&#27963;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#26041;&#27861;&#22312;&#19981;&#21516;&#24773;&#20917;&#19979;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) have significantly advanced various domains, but their vulnerability to adversarial attacks poses serious concerns. Understanding these vulnerabilities and developing effective defense mechanisms is crucial. DeepFool, an algorithm proposed by Moosavi-Dezfooli et al. (2016), finds minimal perturbations to misclassify input images. However, DeepFool lacks a targeted approach, making it less effective in specific attack scenarios. Also, in previous related works, researchers primarily focus on success, not considering how much an image is getting distorted; the integrity of the image quality, and the confidence level to misclassifying. So, in this paper, we propose Targeted DeepFool, an augmented version of DeepFool that allows targeting specific classes for misclassification. We also introduce a minimum confidence score requirement hyperparameter to enhance flexibility. Our experiments demonstrate the effectiveness and efficiency of the proposed method across 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#39640;&#25928;Transformer&#30340;&#36817;&#20284;&#20004;&#23618;&#21069;&#39304;&#32593;&#32476;&#26041;&#27861;&#65292;&#36890;&#36807;&#31232;&#30095;&#30340;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#21644;&#20135;&#21697;-&#38190;&#23384;&#20648;&#23454;&#29616;&#36164;&#28304;&#39640;&#25928;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#24182;&#22312;&#21442;&#25968;&#30456;&#31561;&#30340;&#26465;&#20214;&#19979;&#23637;&#31034;&#20102;&#20854;&#22312;&#19981;&#21516;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2310.10837</link><description>&lt;p&gt;
&#29992;&#20110;&#39640;&#25928;Transformer&#30340;&#36817;&#20284;&#20004;&#23618;&#21069;&#39304;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Approximating Two-Layer Feedforward Networks for Efficient Transformers. (arXiv:2310.10837v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10837
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#39640;&#25928;Transformer&#30340;&#36817;&#20284;&#20004;&#23618;&#21069;&#39304;&#32593;&#32476;&#26041;&#27861;&#65292;&#36890;&#36807;&#31232;&#30095;&#30340;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#21644;&#20135;&#21697;-&#38190;&#23384;&#20648;&#23454;&#29616;&#36164;&#28304;&#39640;&#25928;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#24182;&#22312;&#21442;&#25968;&#30456;&#31561;&#30340;&#26465;&#20214;&#19979;&#23637;&#31034;&#20102;&#20854;&#22312;&#19981;&#21516;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20309;&#22312;&#19981;&#29306;&#29298;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#20943;&#23569;&#31070;&#32463;&#32593;&#32476;(NNs)&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#38656;&#27714;&#65311;&#35768;&#22810;&#26368;&#36817;&#30340;&#30740;&#31350;&#20351;&#29992;&#31232;&#30095;&#30340;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;(MoEs)&#26500;&#24314;&#36164;&#28304;&#39640;&#25928;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LMs)&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20851;&#20110;MoEs&#30340;&#20960;&#20010;&#26032;&#39062;&#35266;&#28857;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#23558;&#21508;&#31181;&#26041;&#27861;&#32479;&#19968;&#36215;&#26469;&#20197;&#36817;&#20284;&#20004;&#23618;NNs(&#20363;&#22914;Transformer&#30340;&#21069;&#39304;&#22359;)&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#21253;&#25324;&#20135;&#21697;-&#38190;&#23384;&#20648;(PKMs)&#12290;&#20511;&#21161;&#36825;&#20010;&#26694;&#26550;&#30340;&#35265;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25913;&#36827;MoEs&#21644;PKMs&#30340;&#26041;&#27861;&#12290;&#19982;&#20043;&#21069;&#22312;&#35745;&#31639;&#30456;&#31561;&#26465;&#20214;&#19979;&#27604;&#36739;MoEs&#19982;&#23494;&#38598;&#22522;&#20934;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#35780;&#20272;&#26465;&#20214;&#26159;&#21442;&#25968;&#30456;&#31561;&#65292;&#36825;&#23545;&#20110;&#27491;&#30830;&#35780;&#20272;LMs&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;MoEs&#22312;WikiText-103&#21644;enwiki8&#25968;&#25454;&#38598;&#30340;&#20004;&#20010;&#19981;&#21516;&#35268;&#27169;&#19978;&#19982;&#23494;&#38598;&#30340;Transformer-XL&#30456;&#31454;&#20105;&#65292;&#21516;&#26102;&#36164;&#28304;&#25928;&#29575;&#26356;&#39640;&#12290;&#36825;&#35777;&#26126;MoEs&#19981;&#20165;&#36866;&#29992;&#20110;&#36229;&#22823;&#22411;LMs&#65292;&#20063;&#36866;&#29992;&#20110;&#20219;&#20309;&#35268;&#27169;&#30340;&#36164;&#28304;-
&lt;/p&gt;
&lt;p&gt;
How to reduce compute and memory requirements of neural networks (NNs) without sacrificing performance? Many recent works use sparse Mixtures of Experts (MoEs) to build resource-efficient large language models (LMs). Here we introduce several novel perspectives on MoEs, presenting a general framework that unifies various methods to approximate two-layer NNs (e.g., feedforward blocks of Transformers), including product-key memories (PKMs). Leveraging insights from this framework, we propose methods to improve both MoEs and PKMs. Unlike prior work that compares MoEs with dense baselines under the compute-equal condition, our evaluation condition is parameter-equal, which is crucial to properly evaluate LMs. We show that our MoEs are competitive with the dense Transformer-XL on both the WikiText-103 and enwiki8 datasets at two different scales, while being much more resource efficient. This demonstrates that MoEs are relevant not only to extremely large LMs but also to any-scale resource-
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#23383;&#23402;&#29983;&#36741;&#21161;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#26469;&#35299;&#20915;&#32593;&#32476;&#20999;&#29255;&#20837;&#22330;&#25511;&#21046;&#20013;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#21021;&#22987;&#19981;&#31283;&#23450;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.09299</link><description>&lt;p&gt;
&#25968;&#23383;&#23402;&#29983;&#36741;&#21161;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#32593;&#32476;&#20999;&#29255;&#20837;&#22330;&#25511;&#21046;&#30340;&#22312;&#32447;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Digital Twin Assisted Deep Reinforcement Learning for Online Optimization of Network Slicing Admission Control. (arXiv:2310.09299v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09299
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#23383;&#23402;&#29983;&#36741;&#21161;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#26469;&#35299;&#20915;&#32593;&#32476;&#20999;&#29255;&#20837;&#22330;&#25511;&#21046;&#20013;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#21021;&#22987;&#19981;&#31283;&#23450;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
5G&#21450;&#20197;&#19978;&#32593;&#32476;&#20013;&#22810;&#26679;&#21270;&#30340;&#32593;&#32476;&#26381;&#21153;&#30340;&#26222;&#21450;&#23548;&#33268;&#20102;&#32593;&#32476;&#20999;&#29255;&#25216;&#26415;&#30340;&#20986;&#29616;&#12290;&#22312;&#20854;&#20013;&#65292;&#20837;&#22330;&#25511;&#21046;&#36890;&#36807;&#36873;&#25321;&#24615;&#25509;&#21463;&#26381;&#21153;&#35831;&#27714;&#26469;&#23454;&#29616;&#29305;&#23450;&#30340;&#20248;&#21270;&#30446;&#26631;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#23613;&#31649;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#22312;&#35768;&#22810;&#20837;&#22330;&#25511;&#21046;&#26041;&#27861;&#20013;&#36215;&#30528;&#22522;&#30784;&#21644;&#28789;&#27963;&#24615;&#30340;&#20316;&#29992;&#65292;&#20294;DRL&#27169;&#22411;&#30340;&#21021;&#22987;&#19981;&#31283;&#23450;&#24615;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#23454;&#38469;&#32593;&#32476;&#20013;&#30340;&#23454;&#38469;&#37096;&#32626;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#23383;&#23402;&#29983;(DT)&#36741;&#21161;&#30340;DRL&#35299;&#20915;&#26041;&#26696;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#20837;&#22330;&#20915;&#31574;&#36807;&#31243;&#24418;&#24335;&#21270;&#20026;&#21322;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#38543;&#21518;&#31616;&#21270;&#20026;&#31561;&#20215;&#30340;&#31163;&#25955;&#26102;&#38388;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#20197;&#20415;&#23454;&#26045;DRL&#26041;&#27861;&#12290;DT&#26159;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#24314;&#31435;&#30340;&#65292;&#24182;&#29992;&#20110;&#36741;&#21161;DRL&#27169;&#22411;&#30340;&#35757;&#32451;&#38454;&#27573;&#12290;&#24191;&#27867;&#30340;&#27169;&#25311;&#34920;&#26126;&#65292;DT&#20316;&#20026;&#19968;&#31181;&#36741;&#21161;&#25163;&#27573;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;DRL&#30340;&#24615;&#33021;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The proliferation of diverse network services in 5G and beyond networks has led to the emergence of network slicing technologies. Among these, admission control plays a crucial role in achieving specific optimization goals through the selective acceptance of service requests. Although Deep Reinforcement Learning (DRL) forms the foundation in many admission control approaches for its effectiveness and flexibility, the initial instability of DRL models hinders their practical deployment in real-world networks. In this work, we propose a digital twin (DT) assisted DRL solution to address this issue. Specifically, we first formulate the admission decision-making process as a semi-Markov decision process, which is subsequently simplified into an equivalent discrete-time Markov decision process to facilitate the implementation of DRL methods. The DT is established through supervised learning and employed to assist the training phase of the DRL model. Extensive simulations show that the DT-as
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#21367;&#31215;&#26680;&#25163;&#24037;&#29305;&#24449;&#34701;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#24378;&#36229;&#22768;&#24515;&#21160;&#22270;&#24038;&#23460;&#39640;&#34880;&#21387;&#30149;&#21464;&#30340;&#35782;&#21035;&#12290;&#36890;&#36807;&#23558;&#21367;&#31215;&#28388;&#27874;&#22120;&#24212;&#29992;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#39044;&#22788;&#29702;&#20013;&#65292;&#23558;&#22270;&#20687;&#36716;&#25442;&#20026;&#29305;&#24449;&#22270;&#65292;&#23454;&#29616;&#20102;&#22312;&#19981;&#21516;&#25104;&#20687;&#35774;&#22791;&#21644;&#21327;&#35758;&#19979;&#30340;&#19968;&#33268;&#29305;&#24449;&#25552;&#21462;&#12290;</title><link>http://arxiv.org/abs/2310.08897</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#21367;&#31215;&#26680;&#25163;&#24037;&#29305;&#24449;&#30340;&#29305;&#33394;&#34701;&#21512;&#65306;&#22686;&#24378;&#36229;&#22768;&#24515;&#21160;&#22270;&#24038;&#23460;&#39640;&#34880;&#21387;&#30149;&#21464;&#34920;&#22411;&#30340;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Self supervised convolutional kernel based handcrafted feature harmonization: Enhanced left ventricle hypertension disease phenotyping on echocardiography. (arXiv:2310.08897v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08897
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#21367;&#31215;&#26680;&#25163;&#24037;&#29305;&#24449;&#34701;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#24378;&#36229;&#22768;&#24515;&#21160;&#22270;&#24038;&#23460;&#39640;&#34880;&#21387;&#30149;&#21464;&#30340;&#35782;&#21035;&#12290;&#36890;&#36807;&#23558;&#21367;&#31215;&#28388;&#27874;&#22120;&#24212;&#29992;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#39044;&#22788;&#29702;&#20013;&#65292;&#23558;&#22270;&#20687;&#36716;&#25442;&#20026;&#29305;&#24449;&#22270;&#65292;&#23454;&#29616;&#20102;&#22312;&#19981;&#21516;&#25104;&#20687;&#35774;&#22791;&#21644;&#21327;&#35758;&#19979;&#30340;&#19968;&#33268;&#29305;&#24449;&#25552;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25918;&#23556;&#23398;&#29305;&#24449;&#23398;&#26159;&#19968;&#31181;&#36890;&#36807;&#22270;&#20687;&#25552;&#21462;&#23450;&#37327;&#25163;&#24037;&#29305;&#24449;&#26469;&#39044;&#27979;&#30142;&#30149;&#30340;&#21307;&#23398;&#25104;&#20687;&#25216;&#26415;&#12290;&#22312;&#36825;&#20123;&#29305;&#24449;&#20013;&#36827;&#34892;&#34701;&#21512;&#65292;&#21487;&#20197;&#30830;&#20445;&#22312;&#19981;&#21516;&#30340;&#25104;&#20687;&#35774;&#22791;&#21644;&#21327;&#35758;&#20013;&#36827;&#34892;&#19968;&#33268;&#30340;&#29305;&#24449;&#25552;&#21462;&#12290;&#34701;&#21512;&#30340;&#26041;&#27861;&#21253;&#25324;&#26631;&#20934;&#21270;&#25104;&#20687;&#21327;&#35758;&#12289;&#32479;&#35745;&#35843;&#25972;&#21644;&#35780;&#20272;&#29305;&#24449;&#30340;&#31283;&#20581;&#24615;&#12290;&#36890;&#36807;&#36229;&#22768;&#24515;&#21160;&#22270;&#21487;&#20197;&#35786;&#26029;&#24515;&#32908;&#30142;&#30149;&#65292;&#22914;&#24038;&#23460;&#32933;&#21402;(LVH)&#21644;&#39640;&#34880;&#21387;&#24515;&#33039;&#30149;(HHD)&#65292;&#20294;&#19981;&#21516;&#30340;&#25104;&#20687;&#35774;&#32622;&#20250;&#24102;&#26469;&#25361;&#25112;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#29305;&#24449;&#34701;&#21512;&#25216;&#26415;&#23545;&#20110;&#22312;&#30142;&#30149;&#35786;&#26029;&#20013;&#24212;&#29992;&#25163;&#24037;&#29305;&#24449;&#33267;&#20851;&#37325;&#35201;&#12290;&#33258;&#30417;&#30563;&#23398;&#20064;(SSl)&#36890;&#36807;&#38480;&#21046;&#30340;&#25968;&#25454;&#38598;&#22686;&#24378;&#25968;&#25454;&#29702;&#35299;&#65292;&#24182;&#36866;&#24212;&#22810;&#26679;&#30340;&#25968;&#25454;&#35774;&#32622;&#12290;ConvNeXt-V2&#23558;&#21367;&#31215;&#23618;&#38598;&#25104;&#21040;SSL&#20013;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#20391;&#37325;&#20110;SSL&#20013;&#30340;&#21367;&#31215;&#28388;&#27874;&#22120;&#65292;&#23558;&#23427;&#20204;&#29992;&#20316;&#39044;&#22788;&#29702;&#65292;&#23558;&#22270;&#20687;&#36716;&#25442;&#20026;&#29305;&#24449;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Radiomics, a medical imaging technique, extracts quantitative handcrafted features from images to predict diseases. Harmonization in those features ensures consistent feature extraction across various imaging devices and protocols. Methods for harmonization include standardized imaging protocols, statistical adjustments, and evaluating feature robustness. Myocardial diseases such as Left Ventricular Hypertrophy (LVH) and Hypertensive Heart Disease (HHD) are diagnosed via echocardiography, but variable imaging settings pose challenges. Harmonization techniques are crucial for applying handcrafted features in disease diagnosis in such scenario. Self-supervised learning (SSL) enhances data understanding within limited datasets and adapts to diverse data settings. ConvNeXt-V2 integrates convolutional layers into SSL, displaying superior performance in various tasks. This study focuses on convolutional filters within SSL, using them as preprocessing to convert images into feature maps for h
&lt;/p&gt;</description></item><item><title>LGL-BCI&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#20960;&#20309;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#22788;&#29702;EEG&#25968;&#25454;&#22312;&#38750;&#27431;&#20960;&#37324;&#24503;&#24230;&#37327;&#31354;&#38388;&#20013;&#25429;&#25417;&#36816;&#21160;&#24819;&#35937;&#20219;&#21153;&#30340;&#31354;&#38388;&#30456;&#20851;&#24615;&#65292;&#24182;&#36890;&#36807;&#29305;&#24449;&#20998;&#35299;&#31639;&#27861;&#36827;&#34892;EEG&#36890;&#36947;&#36873;&#25321;&#20197;&#25552;&#39640;&#25512;&#26029;&#36895;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;LGL-BCI&#30456;&#27604;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.08051</link><description>&lt;p&gt;
LGL-BCI&#65306;&#19968;&#31181;&#36731;&#37327;&#32423;&#20960;&#20309;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#22522;&#20110;&#36816;&#21160;&#24819;&#35937;&#30340;&#33041;&#26426;&#25509;&#21475;
&lt;/p&gt;
&lt;p&gt;
LGL-BCI: A Lightweight Geometric Learning Framework for Motor Imagery-Based Brain-Computer Interfaces. (arXiv:2310.08051v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08051
&lt;/p&gt;
&lt;p&gt;
LGL-BCI&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#20960;&#20309;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#22788;&#29702;EEG&#25968;&#25454;&#22312;&#38750;&#27431;&#20960;&#37324;&#24503;&#24230;&#37327;&#31354;&#38388;&#20013;&#25429;&#25417;&#36816;&#21160;&#24819;&#35937;&#20219;&#21153;&#30340;&#31354;&#38388;&#30456;&#20851;&#24615;&#65292;&#24182;&#36890;&#36807;&#29305;&#24449;&#20998;&#35299;&#31639;&#27861;&#36827;&#34892;EEG&#36890;&#36947;&#36873;&#25321;&#20197;&#25552;&#39640;&#25512;&#26029;&#36895;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;LGL-BCI&#30456;&#27604;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#26426;&#25509;&#21475;&#26159;&#19968;&#31181;&#20351;&#29992;&#33041;&#20449;&#21495;&#19982;&#22806;&#37096;&#35774;&#22791;&#36827;&#34892;&#20132;&#20114;&#30340;&#24320;&#21019;&#24615;&#25216;&#26415;&#12290;&#23613;&#31649;&#26377;&#25152;&#36827;&#23637;&#65292;&#22522;&#20110;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#30340;&#36816;&#21160;&#24819;&#35937;&#20219;&#21153;&#38754;&#20020;&#25361;&#25112;&#65292;&#22914;&#24133;&#24230;&#21644;&#30456;&#20301;&#21464;&#24322;&#65292;&#20197;&#21450;&#22797;&#26434;&#30340;&#31354;&#38388;&#30456;&#20851;&#24615;&#65292;&#38656;&#35201;&#26356;&#23567;&#30340;&#27169;&#22411;&#22823;&#23567;&#21644;&#26356;&#24555;&#30340;&#25512;&#26029;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;LGL-BCI&#26694;&#26550;&#65292;&#37319;&#29992;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#22788;&#29702;&#38750;&#27431;&#20960;&#37324;&#24503;&#24230;&#37327;&#31354;&#38388;&#20013;&#30340;EEG&#65292;&#29305;&#21035;&#26159;&#23545;&#31216;&#27491;&#23450;&#65288;SPD&#65289;&#27969;&#24418;&#31354;&#38388;&#12290;LGL-BCI&#25552;&#20379;&#20102;&#31283;&#20581;&#30340;EEG&#25968;&#25454;&#34920;&#31034;&#65292;&#24182;&#25429;&#25417;&#20102;&#31354;&#38388;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#29305;&#24449;&#20998;&#35299;&#31639;&#27861;&#36827;&#34892;EEG&#36890;&#36947;&#36873;&#25321;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#20943;&#23569;SPD&#30697;&#38453;&#30340;&#32500;&#24230;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#25512;&#26029;&#36895;&#24230;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#26174;&#31034;&#65292;&#19982;&#24403;&#21069;&#35299;&#20915;&#26041;&#26696;&#30456;&#27604;&#65292;LGL-BCI&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#65292;&#31361;&#20986;&#20102;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#22312;&#36816;&#21160;&#24819;&#35937;-&#33041;&#26426;&#25509;&#21475;&#24212;&#29992;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Brain-Computer Interfaces (BCIs) are a groundbreaking technology for interacting with external devices using brain signals. Despite advancements, electroencephalogram (EEG)-based Motor Imagery (MI) tasks face challenges like amplitude and phase variability, and complex spatial correlations, with a need for smaller model size and faster inference. This study introduces the LGL-BCI framework, employing a Geometric Deep Learning Framework for EEG processing in non-Euclidean metric spaces, particularly the Symmetric Positive Definite (SPD) Manifold space. LGL-BCI offers robust EEG data representation and captures spatial correlations. We propose an EEG channel selection solution via a feature decomposition algorithm to reduce SPD matrix dimensionality, with a lossless transformation boosting inference speed. Extensive experiments show LGL-BCI's superior accuracy and efficiency compared to current solutions, highlighting geometric deep learning's potential in MI-BCI applications. The effici
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;&#65292;&#21457;&#29616;&#25915;&#20987;&#32773;&#24448;&#24448;&#19981;&#33021;&#33410;&#32422;&#25968;&#25454;&#21644;&#26631;&#27880;&#25104;&#26412;&#65292;&#22240;&#20026;&#25915;&#20987;&#38544;&#21547;&#22320;&#20381;&#36182;&#20110;&#20174;&#21463;&#23475;&#27169;&#22411;&#30340;&#25968;&#25454;&#20998;&#24067;&#20013;&#37319;&#26679;&#30340;&#33021;&#21147;&#12290;&#25915;&#20987;&#32773;&#30340;&#20808;&#21069;&#30693;&#35782;&#23545;&#25915;&#20987;&#25104;&#21151;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2310.01959</link><description>&lt;p&gt;
&#36229;&#36234;&#26631;&#31614;&#31070;&#35861;&#65306;&#20160;&#20040;&#26159;&#27169;&#22411;&#31363;&#21462;&#30340;&#21547;&#20041;&#65311;
&lt;/p&gt;
&lt;p&gt;
Beyond Labeling Oracles: What does it mean to steal ML models?. (arXiv:2310.01959v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01959
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;&#65292;&#21457;&#29616;&#25915;&#20987;&#32773;&#24448;&#24448;&#19981;&#33021;&#33410;&#32422;&#25968;&#25454;&#21644;&#26631;&#27880;&#25104;&#26412;&#65292;&#22240;&#20026;&#25915;&#20987;&#38544;&#21547;&#22320;&#20381;&#36182;&#20110;&#20174;&#21463;&#23475;&#27169;&#22411;&#30340;&#25968;&#25454;&#20998;&#24067;&#20013;&#37319;&#26679;&#30340;&#33021;&#21147;&#12290;&#25915;&#20987;&#32773;&#30340;&#20808;&#21069;&#30693;&#35782;&#23545;&#25915;&#20987;&#25104;&#21151;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;&#26088;&#22312;&#36890;&#36807;&#21482;&#26377;&#26597;&#35810;&#35775;&#38382;&#26435;&#38480;&#26469;&#31363;&#21462;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#65292;&#36890;&#24120;&#36890;&#36807;ML-as-a-Service&#25552;&#20379;&#30340;API&#26469;&#23454;&#29616;&#12290;&#30001;&#20110;&#25968;&#25454;&#38590;&#20197;&#33719;&#21462;&#65292;&#35757;&#32451;ML&#27169;&#22411;&#30340;&#25104;&#26412;&#24456;&#39640;&#65292;&#22240;&#27492;&#27169;&#22411;&#25552;&#21462;&#30340;&#20027;&#35201;&#21160;&#26426;&#26159;&#22312;&#27604;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#26356;&#23569;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#33719;&#21462;&#27169;&#22411;&#12290;&#20851;&#20110;&#27169;&#22411;&#25552;&#21462;&#30340;&#25991;&#29486;&#26222;&#36941;&#22768;&#31216;&#25110;&#20551;&#35774;&#25915;&#20987;&#32773;&#33021;&#22815;&#33410;&#32422;&#25968;&#25454;&#33719;&#21462;&#21644;&#26631;&#27880;&#25104;&#26412;&#12290;&#28982;&#32780;&#25105;&#20204;&#21457;&#29616;&#25915;&#20987;&#32773;&#24448;&#24448;&#19981;&#33021;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#22240;&#20026;&#24403;&#21069;&#30340;&#25915;&#20987;&#38544;&#21547;&#22320;&#20381;&#36182;&#20110;&#25915;&#20987;&#32773;&#33021;&#22815;&#20174;&#21463;&#23475;&#27169;&#22411;&#30340;&#25968;&#25454;&#20998;&#24067;&#20013;&#37319;&#26679;&#12290;&#25105;&#20204;&#23545;&#24433;&#21709;&#27169;&#22411;&#25552;&#21462;&#25104;&#21151;&#30340;&#22240;&#32032;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21457;&#29616;&#25915;&#20987;&#32773;&#23545;&#21463;&#23475;&#32773;&#30340;&#20808;&#21069;&#30693;&#35782;&#65292;&#21363;&#23545;&#20998;&#24067;&#25968;&#25454;&#30340;&#35775;&#38382;&#65292;&#27604;&#25915;&#20987;&#31574;&#30053;&#65288;&#20915;&#23450;&#21521;&#21463;&#23475;&#32773;&#27169;&#22411;API&#21457;&#20986;&#21738;&#20123;&#26597;&#35810;&#65289;&#31561;&#20854;&#20182;&#22240;&#32032;&#26356;&#20026;&#37325;&#35201;&#12290;&#22240;&#27492;&#65292;&#19968;&#20010;&#24076;&#26395;&#24320;&#21457;&#21516;&#31561;&#27700;&#24179;&#30340;&#25915;&#20987;&#32773;&#26356;&#37325;&#35201;&#30340;&#26159;&#33719;&#21462;&#23545;&#20998;&#24067;&#25968;&#25454;&#30340;&#20808;&#21069;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model extraction attacks are designed to steal trained models with only query access, as is often provided through APIs that ML-as-a-Service providers offer. ML models are expensive to train, in part because data is hard to obtain, and a primary incentive for model extraction is to acquire a model while incurring less cost than training from scratch. Literature on model extraction commonly claims or presumes that the attacker is able to save on both data acquisition and labeling costs. We show that the attacker often does not. This is because current attacks implicitly rely on the adversary being able to sample from the victim model's data distribution. We thoroughly evaluate factors influencing the success of model extraction. We discover that prior knowledge of the attacker, i.e. access to in-distribution data, dominates other factors like the attack policy the adversary follows to choose which queries to make to the victim model API. Thus, an adversary looking to develop an equally 
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22522;&#20110;&#24352;&#37327;&#32593;&#32476;&#30340;&#30005;&#36335;&#32534;&#35793;&#26041;&#27861;AMLET&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#37327;&#23376;&#30005;&#36335;&#21152;&#36733;&#32463;&#20856;&#25968;&#25454;&#30340;&#8220;&#36755;&#20837;&#38382;&#39064;&#8221;&#12290;&#20316;&#32773;&#22312;&#37329;&#34701;&#12289;&#22270;&#20687;&#12289;&#27969;&#20307;&#21644;&#34507;&#30333;&#36136;&#39046;&#22495;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#25968;&#20540;&#23454;&#39564;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.13108</link><description>&lt;p&gt;
&#25968;&#25454;&#21152;&#36733;&#36890;&#24120;&#20855;&#26377;&#30701;&#28145;&#24230;&#65306;&#22522;&#20110;&#24352;&#37327;&#32593;&#32476;&#30340;&#37329;&#34701;&#12289;&#22270;&#20687;&#12289;&#27969;&#20307;&#21644;&#34507;&#30333;&#36136;&#37327;&#23376;&#30005;&#36335;
&lt;/p&gt;
&lt;p&gt;
Data is often loadable in short depth: Quantum circuits from tensor networks for finance, images, fluids, and proteins. (arXiv:2309.13108v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13108
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#24352;&#37327;&#32593;&#32476;&#30340;&#30005;&#36335;&#32534;&#35793;&#26041;&#27861;AMLET&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#37327;&#23376;&#30005;&#36335;&#21152;&#36733;&#32463;&#20856;&#25968;&#25454;&#30340;&#8220;&#36755;&#20837;&#38382;&#39064;&#8221;&#12290;&#20316;&#32773;&#22312;&#37329;&#34701;&#12289;&#22270;&#20687;&#12289;&#27969;&#20307;&#21644;&#34507;&#30333;&#36136;&#39046;&#22495;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#25968;&#20540;&#23454;&#39564;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#24320;&#21457;&#29992;&#20110;&#30740;&#31350;&#32463;&#20856;&#25968;&#25454;&#38598;&#30340;&#37327;&#23376;&#31639;&#27861;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#31616;&#21333;&#21152;&#36733;&#32463;&#20856;&#25968;&#25454;&#30340;&#25104;&#26412;&#26159;&#23454;&#29616;&#37327;&#23376;&#20248;&#21183;&#30340;&#38556;&#30861;&#12290;&#24403;&#20351;&#29992;&#25391;&#24133;&#32534;&#30721;&#26102;&#65292;&#21152;&#36733;&#20219;&#24847;&#32463;&#20856;&#21521;&#37327;&#38656;&#35201;&#19982;&#27604;&#29305;&#25968;&#25104;&#25351;&#25968;&#20851;&#31995;&#30340;&#30005;&#36335;&#28145;&#24230;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#36129;&#29486;&#26469;&#35299;&#20915;&#36825;&#20010;&#8220;&#36755;&#20837;&#38382;&#39064;&#8221;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#24352;&#37327;&#32593;&#32476;&#65288;TN&#65289;&#29702;&#35770;&#30340;&#30005;&#36335;&#32534;&#35793;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#8212;&#8212;AMLET&#65288;&#33258;&#21160;&#22810;&#23618;&#21152;&#36733;&#22120;&#21033;&#29992;TNs&#65289;&#8212;&#8212;&#36890;&#36807;&#31934;&#24515;&#26500;&#24314;&#29305;&#23450;&#30340;TN&#25299;&#25169;&#32467;&#26500;&#65292;&#24182;&#21487;&#20197;&#26681;&#25454;&#38656;&#35201;&#35843;&#25972;&#30005;&#36335;&#28145;&#24230;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23545;&#26469;&#33258;&#37329;&#34701;&#12289;&#22270;&#20687;&#12289;&#27969;&#20307;&#21147;&#23398;&#21644;&#34507;&#30333;&#36136;&#22235;&#20010;&#19981;&#21516;&#39046;&#22495;&#30340;&#30495;&#23454;&#32463;&#20856;&#25968;&#25454;&#36827;&#34892;&#20102;&#25968;&#20540;&#23454;&#39564;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#36804;&#20170;&#20026;&#27490;&#20851;&#20110;&#23558;&#32463;&#20856;&#25968;&#25454;&#21152;&#36733;&#21040;&#37327;&#23376;&#35745;&#31639;&#26426;&#20013;&#30340;&#26368;&#24191;&#27867;&#30340;&#25968;&#20540;&#20998;&#26512;&#12290;&#19982;&#36825;&#19968;&#39046;&#22495;&#26368;&#36817;&#30340;&#20854;&#20182;&#24037;&#20316;&#19968;&#33268;&#65292;&#25152;&#38656;&#30340;
&lt;/p&gt;
&lt;p&gt;
Though there has been substantial progress in developing quantum algorithms to study classical datasets, the cost of simply loading classical data is an obstacle to quantum advantage. When the amplitude encoding is used, loading an arbitrary classical vector requires up to exponential circuit depths with respect to the number of qubits. Here, we address this ``input problem'' with two contributions. First, we introduce a circuit compilation method based on tensor network (TN) theory. Our method -- AMLET (Automatic Multi-layer Loader Exploiting TNs) -- proceeds via careful construction of a specific TN topology and can be tailored to arbitrary circuit depths. Second, we perform numerical experiments on real-world classical data from four distinct areas: finance, images, fluid mechanics, and proteins. To the best of our knowledge, this is the broadest numerical analysis to date of loading classical data into a quantum computer. Consistent with other recent work in this area, the required
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31934;&#32454;&#30340;&#27169;&#24577;&#35780;&#20272;&#25351;&#26631;&#65292;&#29992;&#20110;&#35780;&#20272;&#27599;&#20010;&#27169;&#24577;&#22312;&#26679;&#26412;&#32423;&#21035;&#30340;&#36129;&#29486;&#65292;&#24182;&#21457;&#29616;&#22810;&#27169;&#24577;&#27169;&#22411;&#20542;&#21521;&#20110;&#20381;&#36182;&#19968;&#20010;&#29305;&#23450;&#30340;&#27169;&#24577;&#65292;&#23548;&#33268;&#20854;&#20182;&#27169;&#24577;&#30340;&#36129;&#29486;&#36739;&#20302;&#12290;</title><link>http://arxiv.org/abs/2309.06255</link><description>&lt;p&gt;
&#36890;&#36807;&#31934;&#32454;&#30340;&#27169;&#24577;&#35780;&#20272;&#22686;&#24378;&#22810;&#27169;&#24577;&#21327;&#20316;
&lt;/p&gt;
&lt;p&gt;
Enhancing Multi-modal Cooperation via Fine-grained Modality Valuation. (arXiv:2309.06255v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06255
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31934;&#32454;&#30340;&#27169;&#24577;&#35780;&#20272;&#25351;&#26631;&#65292;&#29992;&#20110;&#35780;&#20272;&#27599;&#20010;&#27169;&#24577;&#22312;&#26679;&#26412;&#32423;&#21035;&#30340;&#36129;&#29486;&#65292;&#24182;&#21457;&#29616;&#22810;&#27169;&#24577;&#27169;&#22411;&#20542;&#21521;&#20110;&#20381;&#36182;&#19968;&#20010;&#29305;&#23450;&#30340;&#27169;&#24577;&#65292;&#23548;&#33268;&#20854;&#20182;&#27169;&#24577;&#30340;&#36129;&#29486;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064;&#26159;&#22914;&#20309;&#23558;&#26469;&#33258;&#19981;&#21516;&#27169;&#24577;&#30340;&#24322;&#36136;&#20449;&#24687;&#20849;&#21516;&#32467;&#21512;&#36215;&#26469;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#27169;&#22411;&#22312;&#22810;&#27169;&#24577;&#21327;&#20316;&#26041;&#38754;&#24120;&#24120;&#23384;&#22312;&#19981;&#23613;&#20154;&#24847;&#30340;&#38382;&#39064;&#65292;&#19981;&#33021;&#24456;&#22909;&#22320;&#20849;&#21516;&#21033;&#29992;&#25152;&#26377;&#27169;&#24577;&#12290;&#19968;&#20123;&#26041;&#27861;&#34987;&#25552;&#20986;&#26469;&#35782;&#21035;&#21644;&#22686;&#24378;&#23398;&#20064;&#25928;&#26524;&#36739;&#24046;&#30340;&#27169;&#24577;&#65292;&#20294;&#24448;&#24448;&#38590;&#20197;&#22312;&#29702;&#35770;&#19978;&#25552;&#20379;&#23545;&#26679;&#26412;&#32423;&#21035;&#22810;&#27169;&#24577;&#21327;&#20316;&#30340;&#32454;&#31890;&#24230;&#35266;&#23519;&#21644;&#25903;&#25345;&#12290;&#22240;&#27492;&#65292;&#21512;&#29702;&#35266;&#23519;&#21644;&#25913;&#36827;&#27169;&#24577;&#20043;&#38388;&#32454;&#31890;&#24230;&#30340;&#21327;&#20316;&#23588;&#20026;&#37325;&#35201;&#65292;&#23588;&#20854;&#26159;&#22312;&#38754;&#23545;&#27169;&#24577;&#24046;&#24322;&#22312;&#19981;&#21516;&#26679;&#26412;&#20043;&#38388;&#21487;&#33021;&#21464;&#21270;&#30340;&#23454;&#38469;&#22330;&#26223;&#26102;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31934;&#32454;&#30340;&#27169;&#24577;&#35780;&#20272;&#25351;&#26631;&#65292;&#20197;&#35780;&#20272;&#27599;&#20010;&#27169;&#24577;&#22312;&#26679;&#26412;&#32423;&#21035;&#30340;&#36129;&#29486;&#12290;&#36890;&#36807;&#27169;&#24577;&#35780;&#20272;&#65292;&#25105;&#20204;&#36951;&#25022;&#22320;&#21457;&#29616;&#22810;&#27169;&#24577;&#27169;&#22411;&#20542;&#21521;&#20110;&#20381;&#36182;&#19968;&#20010;&#29305;&#23450;&#30340;&#27169;&#24577;&#65292;&#23548;&#33268;&#20854;&#20182;&#27169;&#24577;&#30340;&#36129;&#29486;&#36739;&#20302;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20998;&#26512;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
One primary topic of multi-modal learning is to jointly incorporate heterogeneous information from different modalities. However, most models often suffer from unsatisfactory multi-modal cooperation, which could not jointly utilize all modalities well. Some methods are proposed to identify and enhance the worse learnt modality, but are often hard to provide the fine-grained observation of multi-modal cooperation at sample-level with theoretical support. Hence, it is essential to reasonably observe and improve the fine-grained cooperation between modalities, especially when facing realistic scenarios where the modality discrepancy could vary across different samples. To this end, we introduce a fine-grained modality valuation metric to evaluate the contribution of each modality at sample-level. Via modality valuation, we regretfully observe that the multi-modal model tends to rely on one specific modality, resulting in other modalities being low-contributing. We further analyze this iss
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#65288;MAGE&#21644;Ms-IV&#65289;&#65292;&#29992;&#20110;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#20915;&#31574;&#65292;&#25552;&#39640;&#20840;&#23616;&#21487;&#35299;&#37322;&#24615;&#12290;MAGE&#21487;&#20197;&#21457;&#29616;&#24418;&#25104;&#35821;&#20041;&#21547;&#20041;&#30340;&#29305;&#24449;&#32452;&#21512;&#65292;&#23558;&#20854;&#31216;&#20026;&#27010;&#24565;&#65292;&#24182;&#36890;&#36807;&#32858;&#31867;&#20998;&#32452;&#20026;&#8220;&#27010;&#24565;&#8221;&#65292;&#28982;&#21518;&#36890;&#36807;Ms-IV&#36827;&#34892;&#21487;&#35270;&#21270;&#12290;&#36825;&#19968;&#26041;&#27861;&#21463;&#21040;&#38459;&#26029;&#21644;&#25935;&#24863;&#24615;&#20998;&#26512;&#30340;&#21551;&#21457;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#25351;&#26631;&#65288;CaOC&#65289;&#20840;&#23616;&#35780;&#20272;&#27169;&#22411;&#26368;&#37325;&#35201;&#30340;&#22270;&#20687;&#21306;&#22495;&#12290;</title><link>http://arxiv.org/abs/2309.00018</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#21457;&#29616;&#21487;&#35299;&#37322;&#30340;&#35270;&#35273;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
Unsupervised discovery of Interpretable Visual Concepts. (arXiv:2309.00018v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00018
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#65288;MAGE&#21644;Ms-IV&#65289;&#65292;&#29992;&#20110;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#20915;&#31574;&#65292;&#25552;&#39640;&#20840;&#23616;&#21487;&#35299;&#37322;&#24615;&#12290;MAGE&#21487;&#20197;&#21457;&#29616;&#24418;&#25104;&#35821;&#20041;&#21547;&#20041;&#30340;&#29305;&#24449;&#32452;&#21512;&#65292;&#23558;&#20854;&#31216;&#20026;&#27010;&#24565;&#65292;&#24182;&#36890;&#36807;&#32858;&#31867;&#20998;&#32452;&#20026;&#8220;&#27010;&#24565;&#8221;&#65292;&#28982;&#21518;&#36890;&#36807;Ms-IV&#36827;&#34892;&#21487;&#35270;&#21270;&#12290;&#36825;&#19968;&#26041;&#27861;&#21463;&#21040;&#38459;&#26029;&#21644;&#25935;&#24863;&#24615;&#20998;&#26512;&#30340;&#21551;&#21457;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#25351;&#26631;&#65288;CaOC&#65289;&#20840;&#23616;&#35780;&#20272;&#27169;&#22411;&#26368;&#37325;&#35201;&#30340;&#22270;&#20687;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#23545;&#20110;&#38750;&#19987;&#23478;&#29992;&#25143;&#38750;&#24120;&#37325;&#35201;&#65292;&#20294;&#26159;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#25552;&#20379;&#32473;&#29992;&#25143;&#30340;&#27169;&#22411;&#35299;&#37322;&#24615;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#35832;&#22914;&#38598;&#25104;&#26799;&#24230;&#31561;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#20135;&#29983;&#20102;&#21253;&#21547;&#22823;&#37327;&#20449;&#24687;&#20294;&#38590;&#20197;&#35299;&#37322;&#30340;&#24402;&#22240;&#26144;&#23556;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#65292;&#26368;&#22823;&#28608;&#27963;&#32452;&#25552;&#21462;&#65288;MAGE&#65289;&#21644;&#22810;&#23610;&#24230;&#21487;&#35299;&#37322;&#24615;&#21487;&#35270;&#21270;&#65288;Ms-IV&#65289;&#65292;&#29992;&#20110;&#35299;&#37322;&#27169;&#22411;&#30340;&#20915;&#31574;&#65292;&#25552;&#39640;&#20840;&#23616;&#21487;&#35299;&#37322;&#24615;&#12290;MAGE&#21487;&#20197;&#25214;&#21040;&#32473;&#23450;CNN&#20013;&#24418;&#25104;&#35821;&#20041;&#21547;&#20041;&#30340;&#29305;&#24449;&#32452;&#21512;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#27010;&#24565;&#65292;&#24182;&#36890;&#36807;&#32858;&#31867;&#23558;&#36825;&#20123;&#30456;&#20284;&#29305;&#24449;&#27169;&#24335;&#20998;&#32452;&#20026;&#8220;&#27010;&#24565;&#8221;&#65292;&#28982;&#21518;&#36890;&#36807;Ms-IV&#36827;&#34892;&#21487;&#35270;&#21270;&#12290;&#36825;&#19968;&#26041;&#27861;&#21463;&#21040;&#38459;&#26029;&#21644;&#25935;&#24863;&#24615;&#20998;&#26512;&#30340;&#21551;&#21457;&#65288;&#21253;&#25324;&#22240;&#26524;&#20851;&#31995;&#65289;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#25351;&#26631;&#65292;&#31216;&#20026;&#31867;&#21035;&#24863;&#30693;&#39034;&#24207;&#30456;&#20851;&#24615;&#65288;CaOC&#65289;&#65292;&#20840;&#23616;&#35780;&#20272;&#26681;&#25454;&#27169;&#22411;&#39044;&#27979;&#32467;&#26524;&#26368;&#37325;&#35201;&#30340;&#22270;&#20687;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Providing interpretability of deep-learning models to non-experts, while fundamental for a responsible real-world usage, is challenging. Attribution maps from xAI techniques, such as Integrated Gradients, are a typical example of a visualization technique containing a high level of information, but with difficult interpretation. In this paper, we propose two methods, Maximum Activation Groups Extraction (MAGE) and Multiscale Interpretable Visualization (Ms-IV), to explain the model's decision, enhancing global interpretability. MAGE finds, for a given CNN, combinations of features which, globally, form a semantic meaning, that we call concepts. We group these similar feature patterns by clustering in ``concepts'', that we visualize through Ms-IV. This last method is inspired by Occlusion and Sensitivity analysis (incorporating causality), and uses a novel metric, called Class-aware Order Correlation (CaOC), to globally evaluate the most important image regions according to the model's 
&lt;/p&gt;</description></item><item><title>survex&#26159;&#19968;&#20010;R&#36719;&#20214;&#21253;&#65292;&#36890;&#36807;&#24212;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#36830;&#36143;&#30340;&#26694;&#26550;&#26469;&#35299;&#37322;&#20219;&#20309;&#29983;&#23384;&#27169;&#22411;&#65292;&#21487;&#20197;&#25913;&#36827;&#27169;&#22411;&#65292;&#25552;&#39640;&#36879;&#26126;&#24230;&#21644;&#36131;&#20219;&#24863;&#12290;</title><link>http://arxiv.org/abs/2308.16113</link><description>&lt;p&gt;
survex&#65306;&#29992;&#20110;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#29983;&#23384;&#27169;&#22411;&#30340;R&#36719;&#20214;&#21253;
&lt;/p&gt;
&lt;p&gt;
survex: an R package for explaining machine learning survival models. (arXiv:2308.16113v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16113
&lt;/p&gt;
&lt;p&gt;
survex&#26159;&#19968;&#20010;R&#36719;&#20214;&#21253;&#65292;&#36890;&#36807;&#24212;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#36830;&#36143;&#30340;&#26694;&#26550;&#26469;&#35299;&#37322;&#20219;&#20309;&#29983;&#23384;&#27169;&#22411;&#65292;&#21487;&#20197;&#25913;&#36827;&#27169;&#22411;&#65292;&#25552;&#39640;&#36879;&#26126;&#24230;&#21644;&#36131;&#20219;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#28789;&#27963;&#24615;&#21644;&#20986;&#33394;&#24615;&#33021;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#32463;&#24120;&#29992;&#20110;&#34917;&#20805;&#21644;&#36229;&#36234;&#20256;&#32479;&#30340;&#32479;&#35745;&#29983;&#23384;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#24191;&#27867;&#24212;&#29992;&#21463;&#21040;&#32570;&#20047;&#29992;&#25143;&#21451;&#22909;&#30340;&#24037;&#20855;&#26469;&#35299;&#37322;&#20854;&#20869;&#37096;&#25805;&#20316;&#21644;&#39044;&#27979;&#21407;&#29702;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;survex R&#36719;&#20214;&#21253;&#65292;&#36890;&#36807;&#24212;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#36830;&#36143;&#30340;&#26694;&#26550;&#26469;&#35299;&#37322;&#20219;&#20309;&#29983;&#23384;&#27169;&#22411;&#12290;&#25152;&#25552;&#36719;&#20214;&#30340;&#21151;&#33021;&#21253;&#25324;&#29702;&#35299;&#21644;&#35786;&#26029;&#29983;&#23384;&#27169;&#22411;&#65292;&#20174;&#32780;&#21487;&#20197;&#25913;&#36827;&#23427;&#20204;&#12290;&#36890;&#36807;&#25581;&#31034;&#21464;&#37327;&#25928;&#24212;&#21644;&#37325;&#35201;&#24615;&#31561;&#20915;&#31574;&#36807;&#31243;&#30340;&#35265;&#35299;&#65292;survex&#33021;&#22815;&#35780;&#20272;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#24182;&#26816;&#27979;&#20559;&#24046;&#12290;&#22240;&#27492;&#65292;&#22312;&#29983;&#29289;&#21307;&#23398;&#30740;&#31350;&#21644;&#21307;&#30103;&#24212;&#29992;&#31561;&#25935;&#24863;&#39046;&#22495;&#21487;&#20197;&#20419;&#36827;&#36879;&#26126;&#24230;&#21644;&#36131;&#20219;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to their flexibility and superior performance, machine learning models frequently complement and outperform traditional statistical survival models. However, their widespread adoption is hindered by a lack of user-friendly tools to explain their internal operations and prediction rationales. To tackle this issue, we introduce the survex R package, which provides a cohesive framework for explaining any survival model by applying explainable artificial intelligence techniques. The capabilities of the proposed software encompass understanding and diagnosing survival models, which can lead to their improvement. By revealing insights into the decision-making process, such as variable effects and importances, survex enables the assessment of model reliability and the detection of biases. Thus, transparency and responsibility may be promoted in sensitive areas, such as biomedical research and healthcare applications.
&lt;/p&gt;</description></item><item><title>CartiMorph&#26159;&#19968;&#31181;&#33258;&#21160;&#21270;&#33181;&#20851;&#33410;&#36719;&#39592;&#24418;&#24577;&#23398;&#27979;&#37327;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#22270;&#20687;&#20998;&#26512;&#65292;&#36890;&#36807;&#23450;&#37327;&#25351;&#26631;&#35780;&#20272;&#20102;&#36719;&#39592;&#30340;&#25439;&#22833;&#21644;&#21402;&#24230;&#65292;&#24182;&#19982;&#25163;&#21160;&#20998;&#21106;&#30340;&#32467;&#26524;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#32467;&#26524;&#26174;&#31034;&#34920;&#38754;&#27861;&#32447;&#30340;&#21402;&#24230;&#26144;&#23556;&#26041;&#27861;&#20855;&#26377;&#36739;&#23567;&#30340;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2308.01981</link><description>&lt;p&gt;
CartiMorph:&#19968;&#31181;&#33258;&#21160;&#21270;&#33181;&#20851;&#33410;&#36719;&#39592;&#24418;&#24577;&#23398;&#27979;&#37327;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
CartiMorph: a framework for automated knee articular cartilage morphometrics. (arXiv:2308.01981v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01981
&lt;/p&gt;
&lt;p&gt;
CartiMorph&#26159;&#19968;&#31181;&#33258;&#21160;&#21270;&#33181;&#20851;&#33410;&#36719;&#39592;&#24418;&#24577;&#23398;&#27979;&#37327;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#22270;&#20687;&#20998;&#26512;&#65292;&#36890;&#36807;&#23450;&#37327;&#25351;&#26631;&#35780;&#20272;&#20102;&#36719;&#39592;&#30340;&#25439;&#22833;&#21644;&#21402;&#24230;&#65292;&#24182;&#19982;&#25163;&#21160;&#20998;&#21106;&#30340;&#32467;&#26524;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#32467;&#26524;&#26174;&#31034;&#34920;&#38754;&#27861;&#32447;&#30340;&#21402;&#24230;&#26144;&#23556;&#26041;&#27861;&#20855;&#26377;&#36739;&#23567;&#30340;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;CartiMorph&#65292;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#21270;&#33181;&#20851;&#33410;&#36719;&#39592;&#24418;&#24577;&#23398;&#27979;&#37327;&#30340;&#26694;&#26550;&#12290;&#23427;&#20197;&#22270;&#20687;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#29983;&#25104;&#36719;&#39592;&#20122;&#21306;&#22495;&#30340;&#23450;&#37327;&#25351;&#26631;&#65292;&#21253;&#25324;&#20840;&#21402;&#24230;&#36719;&#39592;&#20002;&#22833;&#65288;FCL&#65289;&#30340;&#30334;&#20998;&#27604;&#12289;&#24179;&#22343;&#21402;&#24230;&#12289;&#34920;&#38754;&#31215;&#21644;&#20307;&#31215;&#12290;CartiMorph&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20998;&#23618;&#22270;&#20687;&#29305;&#24449;&#34920;&#31034;&#12290;&#25105;&#20204;&#35757;&#32451;&#21644;&#39564;&#35777;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#32452;&#32455;&#20998;&#21106;&#12289;&#27169;&#26495;&#26500;&#24314;&#21644;&#27169;&#26495;&#21040;&#22270;&#20687;&#30340;&#27880;&#20876;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#22522;&#20110;&#34920;&#38754;&#27861;&#32447;&#30340;&#36719;&#39592;&#21402;&#24230;&#26144;&#23556;&#12289;FCL&#20272;&#35745;&#21644;&#22522;&#20110;&#35268;&#21017;&#30340;&#36719;&#39592;&#20998;&#21106;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#36719;&#39592;&#21402;&#24230;&#22270;&#22312;&#34180;&#21644;&#21608;&#36793;&#21306;&#22495;&#26174;&#31034;&#20986;&#36739;&#23567;&#30340;&#35823;&#24046;&#12290;&#25105;&#20204;&#36890;&#36807;&#27604;&#36739;&#36890;&#36807;&#27169;&#22411;&#20998;&#21106;&#21644;&#25163;&#21160;&#20998;&#21106;&#33719;&#24471;&#30340;&#23450;&#37327;&#25351;&#26631;&#65292;&#35780;&#20272;&#20102;&#25152;&#37319;&#29992;&#30340;&#20998;&#21106;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;FCL&#27979;&#37327;&#30340;&#22343;&#26041;&#26681;&#20559;&#24046;&#23567;&#20110;8%&#65292;&#24182;&#19988;&#19982;&#25163;&#21160;&#20998;&#21106;&#30340;&#25351;&#26631;&#23384;&#22312;&#24378;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce CartiMorph, a framework for automated knee articular cartilage morphometrics. It takes an image as input and generates quantitative metrics for cartilage subregions, including the percentage of full-thickness cartilage loss (FCL), mean thickness, surface area, and volume. CartiMorph leverages the power of deep learning models for hierarchical image feature representation. Deep learning models were trained and validated for tissue segmentation, template construction, and template-to-image registration. We established methods for surface-normal-based cartilage thickness mapping, FCL estimation, and rule-based cartilage parcellation. Our cartilage thickness map showed less error in thin and peripheral regions. We evaluated the effectiveness of the adopted segmentation model by comparing the quantitative metrics obtained from model segmentation and those from manual segmentation. The root-mean-squared deviation of the FCL measurements was less than 8%, and strong correlations 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;16&#20301;&#35745;&#31639;&#20013;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25968;&#20540;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Adam&#20248;&#21270;&#22120;&#30340;&#26032;&#26041;&#27861;&#26469;&#25552;&#39640;16&#20301;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#36807;&#31243;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.16189</link><description>&lt;p&gt;
&#29992;&#20110;16&#20301;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#25968;&#20540;&#19981;&#31283;&#23450;&#24615;&#30340;&#39640;&#25928;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Efficient Approach to Mitigate Numerical Instability in Backpropagation for 16-bit Neural Network Training. (arXiv:2307.16189v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16189
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;16&#20301;&#35745;&#31639;&#20013;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25968;&#20540;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Adam&#20248;&#21270;&#22120;&#30340;&#26032;&#26041;&#27861;&#26469;&#25552;&#39640;16&#20301;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#36807;&#31243;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#22312;16&#20301;&#35745;&#31639;&#20013;&#20351;&#29992;&#27969;&#34892;&#30340;&#20248;&#21270;&#31639;&#27861;&#65288;&#22914;RMSProp&#21644;Adam&#65289;&#26102;&#35266;&#23519;&#21040;&#30340;&#25968;&#20540;&#19981;&#31283;&#23450;&#24615;&#30340;&#22797;&#26434;&#24615;&#12290;&#36825;&#31181;&#19981;&#31283;&#23450;&#24615;&#36890;&#24120;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#38454;&#27573;&#20013;&#20986;&#29616;&#65292;&#23548;&#33268;&#23398;&#20064;&#36807;&#31243;&#21463;&#21040;&#24178;&#25200;&#65292;&#20174;&#32780;&#22952;&#30861;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#26377;&#25928;&#37096;&#32626;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#21333;&#19968;&#36229;&#21442;&#25968;epsilon&#26159;&#36825;&#31181;&#25968;&#20540;&#19981;&#31283;&#23450;&#24615;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#23545;16&#20301;&#35745;&#31639;&#20013;&#36825;&#20123;&#20248;&#21270;&#22120;&#20013;epsilon&#30340;&#20316;&#29992;&#36827;&#34892;&#20102;&#28145;&#20837;&#25506;&#32034;&#65292;&#21457;&#29616;&#24494;&#35843;&#20854;&#20540;&#21487;&#20197;&#24674;&#22797;RMSProp&#21644;Adam&#30340;&#21151;&#33021;&#65292;&#20174;&#32780;&#23454;&#29616;&#26377;&#25928;&#21033;&#29992;16&#20301;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#34987;&#21457;&#29616;&#30340;&#25968;&#20540;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;Adam&#20248;&#21270;&#22120;&#30340;&#26356;&#26032;&#65292;&#24182;&#26174;&#33879;&#25913;&#21892;&#20102;16&#20301;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#36807;&#31243;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this research, we delve into the intricacies of the numerical instability observed in 16-bit computations of machine learning models, particularly when employing popular optimization algorithms such as RMSProp and Adam. This instability is commonly experienced during the training phase of deep neural networks, leading to disrupted learning processes and hindering the effective deployment of such models. We identify the single hyperparameter, epsilon, as the main culprit behind this numerical instability. An in-depth exploration of the role of epsilon in these optimizers within 16-bit computations reveals that a minor adjustment of its value can restore the functionality of RMSProp and Adam, consequently enabling the effective utilization of 16-bit neural networks. We propose a novel method to mitigate the identified numerical instability issues. This method capitalizes on the updates from the Adam optimizer and significantly improves the robustness of the learning process in 16-bit 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#33258;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#65292;&#31216;&#20026;TSDiff&#12290;&#35813;&#27169;&#22411;&#19981;&#38656;&#35201;&#36741;&#21161;&#32593;&#32476;&#25110;&#35757;&#32451;&#36807;&#31243;&#30340;&#25913;&#21464;&#65292;&#22312;&#39044;&#27979;&#12289;&#25913;&#36827;&#21644;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#31561;&#26102;&#38388;&#24207;&#21015;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#20102;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.11494</link><description>&lt;p&gt;
&#39044;&#27979;&#12289;&#25913;&#36827;&#12289;&#21512;&#25104;&#65306;&#38754;&#21521;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#33258;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Predict, Refine, Synthesize: Self-Guiding Diffusion Models for Probabilistic Time Series Forecasting. (arXiv:2307.11494v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11494
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#33258;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#65292;&#31216;&#20026;TSDiff&#12290;&#35813;&#27169;&#22411;&#19981;&#38656;&#35201;&#36741;&#21161;&#32593;&#32476;&#25110;&#35757;&#32451;&#36807;&#31243;&#30340;&#25913;&#21464;&#65292;&#22312;&#39044;&#27979;&#12289;&#25913;&#36827;&#21644;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#31561;&#26102;&#38388;&#24207;&#21015;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#20102;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#29983;&#25104;&#24314;&#27169;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#20043;&#21069;&#20851;&#20110;&#26102;&#38388;&#24207;&#21015;&#25193;&#25955;&#27169;&#22411;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#24320;&#21457;&#38024;&#23545;&#29305;&#23450;&#39044;&#27979;&#25110;&#22635;&#34917;&#20219;&#21153;&#30340;&#26465;&#20214;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#38754;&#21521;&#22810;&#31181;&#26102;&#38388;&#24207;&#21015;&#24212;&#29992;&#30340;&#20219;&#21153;&#19981;&#21487;&#30693;&#26465;&#20214;&#19979;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;TSDiff&#65292;&#19968;&#31181;&#38754;&#21521;&#26102;&#38388;&#24207;&#21015;&#30340;&#26080;&#26465;&#20214;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#33258;&#24341;&#23548;&#26426;&#21046;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#20351;&#24471;TSDiff&#33021;&#22815;&#20026;&#19979;&#28216;&#20219;&#21153;&#36827;&#34892;&#26465;&#20214;&#35774;&#32622;&#65292;&#32780;&#26080;&#38656;&#36741;&#21161;&#32593;&#32476;&#25110;&#25913;&#21464;&#35757;&#32451;&#36807;&#31243;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#26102;&#38388;&#24207;&#21015;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65306;&#39044;&#27979;&#12289;&#25913;&#36827;&#21644;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#34920;&#26126;TSDiff&#19982;&#20960;&#31181;&#20219;&#21153;&#29305;&#23450;&#30340;&#26465;&#20214;&#39044;&#27979;&#26041;&#27861;&#30456;&#31454;&#20105;&#65288;&#39044;&#27979;&#65289;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#21033;&#29992;TSDiff&#23398;&#21040;&#30340;&#38544;&#24615;&#27010;&#29575;&#23494;&#24230;&#26469;&#36845;&#20195;&#22320;&#25913;&#36827;p
&lt;/p&gt;
&lt;p&gt;
Diffusion models have achieved state-of-the-art performance in generative modeling tasks across various domains. Prior works on time series diffusion models have primarily focused on developing conditional models tailored to specific forecasting or imputation tasks. In this work, we explore the potential of task-agnostic, unconditional diffusion models for several time series applications. We propose TSDiff, an unconditionally trained diffusion model for time series. Our proposed self-guidance mechanism enables conditioning TSDiff for downstream tasks during inference, without requiring auxiliary networks or altering the training procedure. We demonstrate the effectiveness of our method on three different time series tasks: forecasting, refinement, and synthetic data generation. First, we show that TSDiff is competitive with several task-specific conditional forecasting methods (predict). Second, we leverage the learned implicit probability density of TSDiff to iteratively refine the p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21457;&#29616;&#32593;&#32476;&#26435;&#37325;&#30340;&#26041;&#24046;&#21644;&#22823;&#26435;&#37325;&#30340;&#31354;&#38388;&#38598;&#20013;&#26159;&#24433;&#21709;&#31070;&#32463;&#25345;&#20037;&#24615;&#30340;&#20027;&#35201;&#22240;&#32032;&#65292;&#24182;&#25552;&#20986;&#20102;&#23558;&#31070;&#32463;&#25345;&#20037;&#24615;&#25193;&#23637;&#21040;&#25972;&#20010;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#22270;&#25345;&#20037;&#24615;&#27979;&#37327;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.10865</link><description>&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#22270;&#30340;&#25345;&#20037;&#24615;&#35299;&#20915;&#31070;&#32463;&#25345;&#20037;&#24615;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Addressing caveats of neural persistence with deep graph persistence. (arXiv:2307.10865v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21457;&#29616;&#32593;&#32476;&#26435;&#37325;&#30340;&#26041;&#24046;&#21644;&#22823;&#26435;&#37325;&#30340;&#31354;&#38388;&#38598;&#20013;&#26159;&#24433;&#21709;&#31070;&#32463;&#25345;&#20037;&#24615;&#30340;&#20027;&#35201;&#22240;&#32032;&#65292;&#24182;&#25552;&#20986;&#20102;&#23558;&#31070;&#32463;&#25345;&#20037;&#24615;&#25193;&#23637;&#21040;&#25972;&#20010;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#22270;&#25345;&#20037;&#24615;&#27979;&#37327;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#25345;&#20037;&#24615;&#26159;&#19968;&#31181;&#29992;&#20110;&#37327;&#21270;&#31070;&#32463;&#32593;&#32476;&#22797;&#26434;&#24615;&#30340;&#37325;&#35201;&#25351;&#26631;&#65292;&#25552;&#20986;&#20110;&#28145;&#24230;&#23398;&#20064;&#20013;&#26032;&#20852;&#30340;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#22312;&#29702;&#35770;&#21644;&#23454;&#35777;&#19978;&#25105;&#20204;&#21457;&#29616;&#65292;&#32593;&#32476;&#26435;&#37325;&#30340;&#26041;&#24046;&#21644;&#22823;&#26435;&#37325;&#30340;&#31354;&#38388;&#38598;&#20013;&#26159;&#24433;&#21709;&#31070;&#32463;&#25345;&#20037;&#24615;&#30340;&#20027;&#35201;&#22240;&#32032;&#12290;&#34429;&#28982;&#36825;&#23545;&#20110;&#32447;&#24615;&#20998;&#31867;&#22120;&#26377;&#29992;&#30340;&#20449;&#24687;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21518;&#20960;&#23618;&#20013;&#27809;&#26377;&#30456;&#20851;&#30340;&#31354;&#38388;&#32467;&#26500;&#65292;&#20351;&#24471;&#31070;&#32463;&#25345;&#20037;&#24615;&#22823;&#33268;&#31561;&#20110;&#26435;&#37325;&#30340;&#26041;&#24046;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#25152;&#25552;&#20986;&#30340;&#23618;&#38388;&#24179;&#22343;&#36807;&#31243;&#27809;&#26377;&#32771;&#34385;&#23618;&#38388;&#30340;&#20132;&#20114;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#31070;&#32463;&#25345;&#20037;&#24615;&#22522;&#30784;&#32467;&#26500;&#30340;&#25193;&#23637;&#65292;&#20174;&#21333;&#23618;&#25913;&#20026;&#25972;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#36825;&#30456;&#24403;&#20110;&#22312;&#19968;&#20010;&#29305;&#23450;&#30697;&#38453;&#19978;&#35745;&#31639;&#31070;&#32463;&#25345;&#20037;&#24615;&#12290;&#36825;&#24471;&#21040;&#20102;&#25105;&#20204;&#30340;&#28145;&#24230;&#22270;&#25345;&#20037;&#24615;&#27979;&#37327;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Persistence is a prominent measure for quantifying neural network complexity, proposed in the emerging field of topological data analysis in deep learning. In this work, however, we find both theoretically and empirically that the variance of network weights and spatial concentration of large weights are the main factors that impact neural persistence. Whilst this captures useful information for linear classifiers, we find that no relevant spatial structure is present in later layers of deep neural networks, making neural persistence roughly equivalent to the variance of weights. Additionally, the proposed averaging procedure across layers for deep neural networks does not consider interaction between layers. Based on our analysis, we propose an extension of the filtration underlying neural persistence to the whole neural network instead of single layers, which is equivalent to calculating neural persistence on one particular matrix. This yields our deep graph persistence measur
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26680;&#21270;&#29256;&#26412;&#30340;t-SNE&#31639;&#27861;&#65292;&#21487;&#20197;&#23558;&#39640;&#32500;&#25968;&#25454;&#26144;&#23556;&#21040;&#20302;&#32500;&#31354;&#38388;&#24182;&#20445;&#25345;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#38750;&#27431;&#20960;&#37324;&#24503;&#24230;&#37327;&#19979;&#30340;&#36317;&#31163;&#12290;&#35813;&#31639;&#27861;&#20855;&#26377;&#25913;&#21892;&#24615;&#33021;&#21644;&#20934;&#30830;&#24615;&#30340;&#28508;&#21147;&#65292;&#22312;&#28041;&#21450;&#26680;&#26041;&#27861;&#30340;&#20998;&#31867;&#38382;&#39064;&#20013;&#23637;&#29616;&#20986;&#26356;&#28165;&#26224;&#30340;&#32858;&#31867;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.07081</link><description>&lt;p&gt;
&#26680;t&#20998;&#24067;&#38543;&#26426;&#37051;&#22495;&#23884;&#20837;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Kernel t-distributed stochastic neighbor embedding. (arXiv:2307.07081v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07081
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26680;&#21270;&#29256;&#26412;&#30340;t-SNE&#31639;&#27861;&#65292;&#21487;&#20197;&#23558;&#39640;&#32500;&#25968;&#25454;&#26144;&#23556;&#21040;&#20302;&#32500;&#31354;&#38388;&#24182;&#20445;&#25345;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#38750;&#27431;&#20960;&#37324;&#24503;&#24230;&#37327;&#19979;&#30340;&#36317;&#31163;&#12290;&#35813;&#31639;&#27861;&#20855;&#26377;&#25913;&#21892;&#24615;&#33021;&#21644;&#20934;&#30830;&#24615;&#30340;&#28508;&#21147;&#65292;&#22312;&#28041;&#21450;&#26680;&#26041;&#27861;&#30340;&#20998;&#31867;&#38382;&#39064;&#20013;&#23637;&#29616;&#20986;&#26356;&#28165;&#26224;&#30340;&#32858;&#31867;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;t-SNE&#31639;&#27861;&#30340;&#26680;&#21270;&#29256;&#26412;&#65292;&#33021;&#22815;&#23558;&#39640;&#32500;&#25968;&#25454;&#26144;&#23556;&#21040;&#20302;&#32500;&#31354;&#38388;&#65292;&#21516;&#26102;&#20445;&#30041;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#38750;&#27431;&#20960;&#37324;&#24503;&#24230;&#37327;&#19979;&#30340;&#25104;&#23545;&#36317;&#31163;&#12290;&#36825;&#21487;&#20197;&#36890;&#36807;&#22312;&#39640;&#32500;&#31354;&#38388;&#25110;&#20004;&#20010;&#31354;&#38388;&#20013;&#20351;&#29992;&#26680;&#25216;&#24039;&#26469;&#23454;&#29616;&#65292;&#20174;&#32780;&#24471;&#21040;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#26680;&#21270;&#29256;&#26412;&#12290;t-SNE&#31639;&#27861;&#30340;&#26680;&#21270;&#29256;&#26412;&#21487;&#20197;&#25552;&#20379;&#20851;&#20110;&#25968;&#25454;&#28857;&#20043;&#38388;&#20851;&#31995;&#30340;&#26032;&#35270;&#35282;&#65292;&#21487;&#20197;&#22312;&#29305;&#23450;&#24212;&#29992;&#20013;&#25913;&#21892;&#24615;&#33021;&#21644;&#20934;&#30830;&#24615;&#65292;&#22914;&#28041;&#21450;&#26680;&#26041;&#27861;&#30340;&#20998;&#31867;&#38382;&#39064;&#12290;&#36890;&#36807;&#23545;&#22810;&#20010;&#25968;&#25454;&#38598;&#30340;&#27604;&#36739;&#65292;&#23637;&#31034;&#20102;t-SNE&#21644;&#20854;&#26680;&#21270;&#29256;&#26412;&#30340;&#24046;&#24322;&#65292;&#26174;&#31034;&#20986;&#19981;&#21516;&#31867;&#21035;&#30340;&#25968;&#25454;&#28857;&#20043;&#38388;&#26356;&#28165;&#26224;&#30340;&#32858;&#31867;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a kernelized version of the t-SNE algorithm, capable of mapping high-dimensional data to a low-dimensional space while preserving the pairwise distances between the data points in a non-Euclidean metric. This can be achieved using a kernel trick only in the high dimensional space or in both spaces, leading to an end-to-end kernelized version. The proposed kernelized version of the t-SNE algorithm can offer new views on the relationships between data points, which can improve performance and accuracy in particular applications, such as classification problems involving kernel methods. The differences between t-SNE and its kernelized version are illustrated for several datasets, showing a neater clustering of points belonging to different classes.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#32508;&#36848;&#24635;&#32467;&#20102;&#22240;&#26524;&#24378;&#21270;&#23398;&#20064;&#30340;&#30740;&#31350;&#25991;&#29486;&#65292;&#24378;&#35843;&#22240;&#26524;&#20851;&#31995;&#30340;&#37325;&#35201;&#20316;&#29992;&#65292;&#23427;&#33021;&#22815;&#24418;&#24335;&#21270;&#30693;&#35782;&#24182;&#23454;&#29616;&#26377;&#25928;&#30340;&#30693;&#35782;&#20256;&#36882;&#12290;</title><link>http://arxiv.org/abs/2307.01452</link><description>&lt;p&gt;
&#22240;&#26524;&#24378;&#21270;&#23398;&#20064;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Causal Reinforcement Learning: A Survey. (arXiv:2307.01452v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01452
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#32508;&#36848;&#24635;&#32467;&#20102;&#22240;&#26524;&#24378;&#21270;&#23398;&#20064;&#30340;&#30740;&#31350;&#25991;&#29486;&#65292;&#24378;&#35843;&#22240;&#26524;&#20851;&#31995;&#30340;&#37325;&#35201;&#20316;&#29992;&#65292;&#23427;&#33021;&#22815;&#24418;&#24335;&#21270;&#30693;&#35782;&#24182;&#23454;&#29616;&#26377;&#25928;&#30340;&#30693;&#35782;&#20256;&#36882;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#26159;&#22312;&#19981;&#30830;&#23450;&#24615;&#26465;&#20214;&#19979;&#35299;&#20915;&#24207;&#21015;&#20915;&#31574;&#38382;&#39064;&#30340;&#19968;&#31181;&#37325;&#35201;&#33539;&#24335;&#12290;&#23613;&#31649;&#36817;&#20960;&#21313;&#24180;&#26469;&#21462;&#24471;&#20102;&#35768;&#22810;&#26174;&#33879;&#30340;&#25104;&#23601;&#65292;&#20294;&#23558;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#24212;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#38556;&#30861;&#26159;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#32570;&#20047;&#23545;&#19990;&#30028;&#30340;&#22522;&#26412;&#29702;&#35299;&#65292;&#22240;&#27492;&#24517;&#39035;&#36890;&#36807;&#22823;&#37327;&#30340;&#35797;&#38169;&#20132;&#20114;&#23398;&#20064;&#12290;&#20182;&#20204;&#21487;&#33021;&#22312;&#35299;&#37322;&#33258;&#24049;&#30340;&#20915;&#31574;&#20197;&#21450;&#25512;&#24191;&#25152;&#33719;&#24471;&#30340;&#30693;&#35782;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#22240;&#26524;&#20851;&#31995;&#20855;&#26377;&#26126;&#26174;&#30340;&#20248;&#21183;&#65292;&#23427;&#21487;&#20197;&#20197;&#31995;&#32479;&#21270;&#30340;&#26041;&#24335;&#24418;&#24335;&#21270;&#30693;&#35782;&#65292;&#24182;&#21033;&#29992;&#19981;&#21464;&#24615;&#36827;&#34892;&#26377;&#25928;&#30340;&#30693;&#35782;&#20256;&#36882;&#12290;&#36825;&#23548;&#33268;&#20102;&#22240;&#26524;&#24378;&#21270;&#23398;&#20064;&#30340;&#20986;&#29616;&#65292;&#23427;&#26159;&#24378;&#21270;&#23398;&#20064;&#30340;&#19968;&#20010;&#23376;&#39046;&#22495;&#65292;&#26088;&#22312;&#36890;&#36807;&#23558;&#22240;&#26524;&#20851;&#31995;&#32435;&#20837;&#23398;&#20064;&#36807;&#31243;&#26469;&#22686;&#24378;&#29616;&#26377;&#31639;&#27861;&#12290;&#22312;&#36825;&#31687;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#22238;&#39038;&#20102;&#26377;&#20851;&#22240;&#26524;&#24378;&#21270;&#23398;&#20064;&#30340;&#25991;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning is an essential paradigm for solving sequential decision problems under uncertainty. Despite many remarkable achievements in recent decades, applying reinforcement learning methods in the real world remains challenging. One of the main obstacles is that reinforcement learning agents lack a fundamental understanding of the world and must therefore learn from scratch through numerous trial-and-error interactions. They may also face challenges in providing explanations for their decisions and generalizing the acquired knowledge. Causality, however, offers a notable advantage as it can formalize knowledge in a systematic manner and leverage invariance for effective knowledge transfer. This has led to the emergence of causal reinforcement learning, a subfield of reinforcement learning that seeks to enhance existing algorithms by incorporating causal relationships into the learning process. In this survey, we comprehensively review the literature on causal reinforcemen
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#31070;&#32463;&#32593;&#32476;&#22312;&#24050;&#30693;&#31639;&#27861;&#20219;&#21153;&#20013;&#26377;&#26102;&#20250;&#21457;&#29616;&#36136;&#24577;&#19981;&#21516;&#30340;&#31639;&#27861;&#12290;&#21363;&#20351;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#23567;&#35843;&#25972;&#65292;&#20063;&#21487;&#33021;&#20986;&#29616;&#24182;&#34892;&#23454;&#29616;&#22810;&#20010;&#31639;&#27861;&#30340;&#24773;&#20917;&#12290;&#36825;&#19968;&#32467;&#35770;&#34920;&#26126;&#65292;&#21363;&#20351;&#26159;&#31616;&#21333;&#30340;&#23398;&#20064;&#38382;&#39064;&#65292;&#20063;&#21487;&#20197;&#26377;&#20196;&#20154;&#24778;&#35766;&#30340;&#22810;&#26679;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2306.17844</link><description>&lt;p&gt;
&#26102;&#38047;&#19982;&#25259;&#33832;&#65306;&#31070;&#32463;&#32593;&#32476;&#30340;&#26426;&#26800;&#35299;&#37322;&#20013;&#30340;&#20004;&#20010;&#25925;&#20107;
&lt;/p&gt;
&lt;p&gt;
The Clock and the Pizza: Two Stories in Mechanistic Explanation of Neural Networks. (arXiv:2306.17844v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17844
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#31070;&#32463;&#32593;&#32476;&#22312;&#24050;&#30693;&#31639;&#27861;&#20219;&#21153;&#20013;&#26377;&#26102;&#20250;&#21457;&#29616;&#36136;&#24577;&#19981;&#21516;&#30340;&#31639;&#27861;&#12290;&#21363;&#20351;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#23567;&#35843;&#25972;&#65292;&#20063;&#21487;&#33021;&#20986;&#29616;&#24182;&#34892;&#23454;&#29616;&#22810;&#20010;&#31639;&#27861;&#30340;&#24773;&#20917;&#12290;&#36825;&#19968;&#32467;&#35770;&#34920;&#26126;&#65292;&#21363;&#20351;&#26159;&#31616;&#21333;&#30340;&#23398;&#20064;&#38382;&#39064;&#65292;&#20063;&#21487;&#20197;&#26377;&#20196;&#20154;&#24778;&#35766;&#30340;&#22810;&#26679;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#22312;&#24050;&#30693;&#30340;&#31639;&#27861;&#20219;&#21153;&#19978;&#35757;&#32451;&#65292;&#33021;&#21542;&#21487;&#38752;&#22320;&#37325;&#26032;&#21457;&#29616;&#35299;&#20915;&#36825;&#20123;&#20219;&#21153;&#30340;&#24050;&#30693;&#31639;&#27861;&#65311;&#26368;&#36817;&#30340;&#19968;&#20123;&#30740;&#31350;&#65292;&#28041;&#21450;&#21040;&#20174;&#32676;&#31639;&#26415;&#21040;&#19978;&#19979;&#25991;&#32447;&#24615;&#22238;&#24402;&#30340;&#20219;&#21153;&#65292;&#34920;&#26126;&#31572;&#26696;&#26159;&#32943;&#23450;&#30340;&#12290;&#25105;&#20204;&#20197;&#27169;&#22359;&#21152;&#27861;&#20026;&#20856;&#22411;&#38382;&#39064;&#65292;&#23637;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#31639;&#27861;&#21457;&#29616;&#26377;&#26102;&#26356;&#21152;&#22797;&#26434;&#12290;&#23545;&#27169;&#22411;&#36229;&#21442;&#25968;&#21644;&#21021;&#22987;&#21270;&#36827;&#34892;&#24494;&#23567;&#30340;&#26356;&#25913;&#65292;&#21487;&#20197;&#23548;&#33268;&#20174;&#22266;&#23450;&#35757;&#32451;&#38598;&#20013;&#21457;&#29616;&#23450;&#24615;&#19981;&#21516;&#30340;&#31639;&#27861;&#65292;&#29978;&#33267;&#26159;&#24182;&#34892;&#23454;&#29616;&#22810;&#20010;&#36825;&#26679;&#30340;&#31639;&#27861;&#12290;&#19968;&#20123;&#35757;&#32451;&#29992;&#20110;&#25191;&#34892;&#27169;&#22359;&#21152;&#27861;&#30340;&#32593;&#32476;&#23454;&#29616;&#20102;&#29087;&#24713;&#30340;&#26102;&#38047;&#31639;&#27861;&#65307;&#20854;&#20182;&#23454;&#29616;&#20102;&#20197;&#21069;&#26410;&#25551;&#36848;&#36807;&#30340;&#12289;&#19981;&#22826;&#30452;&#35266;&#20294;&#21487;&#29702;&#35299;&#30340;&#36807;&#31243;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#25259;&#33832;&#31639;&#27861;&#65292;&#25110;&#32773;&#20854;&#20182;&#26356;&#22797;&#26434;&#30340;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#31616;&#21333;&#30340;&#23398;&#20064;&#38382;&#39064;&#20063;&#21487;&#20197;&#26377;&#20196;&#20154;&#24778;&#35766;&#30340;&#22810;&#26679;&#35299;&#20915;&#26041;&#26696;&#65292;&#20419;&#36827;&#20102;&#26032;&#24037;&#20855;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Do neural networks, trained on well-understood algorithmic tasks, reliably rediscover known algorithms for solving those tasks? Several recent studies, on tasks ranging from group arithmetic to in-context linear regression, have suggested that the answer is yes. Using modular addition as a prototypical problem, we show that algorithm discovery in neural networks is sometimes more complex. Small changes to model hyperparameters and initializations can induce the discovery of qualitatively different algorithms from a fixed training set, and even parallel implementations of multiple such algorithms. Some networks trained to perform modular addition implement a familiar Clock algorithm; others implement a previously undescribed, less intuitive, but comprehensible procedure which we term the Pizza algorithm, or a variety of even more complex procedures. Our results show that even simple learning problems can admit a surprising diversity of solutions, motivating the development of new tools 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;M3Act&#65292;&#19968;&#20010;&#22810;&#35270;&#22270;&#22810;&#22242;&#38431;&#22810;&#20154;&#30340;&#20154;&#31867;&#21407;&#23376;&#21160;&#20316;&#21644;&#22242;&#38431;&#27963;&#21160;&#25968;&#25454;&#29983;&#25104;&#22120;&#65292;&#36890;&#36807;Unity&#24341;&#25806;&#39537;&#21160;&#23454;&#29616;&#12290;&#35813;&#29983;&#25104;&#22120;&#20855;&#26377;&#22823;&#35268;&#27169;&#25968;&#25454;&#29983;&#25104;&#12289;&#22810;&#27169;&#24577;&#21644;&#39640;&#36136;&#37327;&#27880;&#37322;&#31561;&#29305;&#28857;&#65292;&#33021;&#22815;&#29992;&#20110;&#30740;&#31350;&#22797;&#26434;&#30340;&#20154;&#31867;&#20114;&#21160;&#21644;&#22242;&#38431;&#27963;&#21160;&#12290;</title><link>http://arxiv.org/abs/2306.16772</link><description>&lt;p&gt;
&#20174;&#21512;&#25104;&#30340;&#20154;&#31867;&#22242;&#38431;&#27963;&#21160;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning from Synthetic Human Group Activities. (arXiv:2306.16772v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16772
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;M3Act&#65292;&#19968;&#20010;&#22810;&#35270;&#22270;&#22810;&#22242;&#38431;&#22810;&#20154;&#30340;&#20154;&#31867;&#21407;&#23376;&#21160;&#20316;&#21644;&#22242;&#38431;&#27963;&#21160;&#25968;&#25454;&#29983;&#25104;&#22120;&#65292;&#36890;&#36807;Unity&#24341;&#25806;&#39537;&#21160;&#23454;&#29616;&#12290;&#35813;&#29983;&#25104;&#22120;&#20855;&#26377;&#22823;&#35268;&#27169;&#25968;&#25454;&#29983;&#25104;&#12289;&#22810;&#27169;&#24577;&#21644;&#39640;&#36136;&#37327;&#27880;&#37322;&#31561;&#29305;&#28857;&#65292;&#33021;&#22815;&#29992;&#20110;&#30740;&#31350;&#22797;&#26434;&#30340;&#20154;&#31867;&#20114;&#21160;&#21644;&#22242;&#38431;&#27963;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#65292;&#23545;&#22797;&#26434;&#30340;&#20154;&#31867;&#20114;&#21160;&#21644;&#22242;&#38431;&#27963;&#21160;&#30340;&#29702;&#35299;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30456;&#20851;&#20219;&#21153;&#30340;&#36827;&#23637;&#21463;&#21040;&#20102;&#33719;&#21462;&#22823;&#35268;&#27169;&#26631;&#35760;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#22256;&#38590;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;M3Act&#65292;&#19968;&#20010;&#22810;&#35270;&#22270;&#22810;&#22242;&#38431;&#22810;&#20154;&#30340;&#20154;&#31867;&#21407;&#23376;&#21160;&#20316;&#21644;&#22242;&#38431;&#27963;&#21160;&#25968;&#25454;&#29983;&#25104;&#22120;&#12290;M3Act&#37319;&#29992;Unity&#24341;&#25806;&#39537;&#21160;&#65292;&#21253;&#21547;&#21487;&#20379;&#20223;&#30495;&#20351;&#29992;&#30340;&#19977;&#32500;&#22330;&#26223;&#21644;&#20154;&#29289;&#36164;&#28304;&#65292;&#21487;&#37197;&#32622;&#30340;&#29031;&#26126;&#21644;&#25668;&#20687;&#31995;&#32479;&#65292;&#39640;&#24230;&#21442;&#25968;&#21270;&#30340;&#27169;&#22359;&#21270;&#22242;&#38431;&#27963;&#21160;&#65292;&#20197;&#21450;&#22312;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20013;&#20855;&#26377;&#22823;&#37327;&#39046;&#22495;&#38543;&#26426;&#21270;&#30340;&#29305;&#28857;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#29983;&#25104;&#22120;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#22810;&#20010;&#35270;&#22270;&#12289;&#27169;&#24577;&#65288;RGB&#22270;&#20687;&#12289;2D&#23039;&#21183;&#12289;3D&#21160;&#20316;&#65289;&#21644;&#39640;&#36136;&#37327;&#27880;&#37322;&#30340;&#22823;&#35268;&#27169;&#20154;&#31867;&#27963;&#21160;&#25968;&#25454;&#38598;&#65288;2D&#36793;&#30028;&#26694;&#12289;&#23454;&#20363;&#20998;&#21106;&#25513;&#27169;&#12289;&#20010;&#20307;&#21160;&#20316;&#21644;&#22242;&#38431;&#27963;&#21160;&#31867;&#21035;&#65289;&#12290;&#21033;&#29992;M3Act&#65292;&#25105;&#20204;&#21487;&#20197;&#29983;&#25104;&#22823;&#35268;&#27169;&#30340;&#20154;&#31867;&#27963;&#21160;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#30740;&#31350;&#20154;&#31867;&#20114;&#21160;&#21644;&#22242;&#38431;&#27963;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
The understanding of complex human interactions and group activities has garnered attention in human-centric computer vision. However, the advancement of the related tasks is hindered due to the difficulty of obtaining large-scale labeled real-world datasets. To mitigate the issue, we propose M3Act, a multi-view multi-group multi-person human atomic action and group activity data generator. Powered by the Unity engine, M3Act contains simulation-ready 3D scenes and human assets, configurable lighting and camera systems, highly parameterized modular group activities, and a large degree of domain randomization during the data generation process. Our data generator is capable of generating large-scale datasets of human activities with multiple viewpoints, modalities (RGB images, 2D poses, 3D motions), and high-quality annotations for individual persons and multi-person groups (2D bounding boxes, instance segmentation masks, individual actions and group activity categories). Using M3Act, we
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;GNN&#22312;&#27979;&#35797;&#26102;&#33410;&#28857;&#30340;&#26412;&#22320;&#21516;&#36136;&#24615;&#27700;&#24179;&#19982;&#20854;&#22270;&#30340;&#20840;&#23616;&#21516;&#36136;&#24615;&#27700;&#24179;&#20559;&#31163;&#26102;&#30340;&#24615;&#33021;&#65292;&#24182;&#20171;&#32461;&#19968;&#31181;&#26032;&#21442;&#25968;&#29992;&#20110;&#25511;&#21046;&#21516;&#36136;&#24615;&#65292;&#22312;&#29983;&#25104;&#30340;&#22270;&#20013;&#31995;&#32479;&#22320;&#30740;&#31350;&#26412;&#22320;&#21516;&#36136;&#24615;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.05557</link><description>&lt;p&gt;
&#20851;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#26412;&#22320;&#21516;&#36136;&#24615;&#27700;&#24179;&#30340;&#24615;&#33021;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
On Performance Discrepancies Across Local Homophily Levels in Graph Neural Networks. (arXiv:2306.05557v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05557
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;GNN&#22312;&#27979;&#35797;&#26102;&#33410;&#28857;&#30340;&#26412;&#22320;&#21516;&#36136;&#24615;&#27700;&#24179;&#19982;&#20854;&#22270;&#30340;&#20840;&#23616;&#21516;&#36136;&#24615;&#27700;&#24179;&#20559;&#31163;&#26102;&#30340;&#24615;&#33021;&#65292;&#24182;&#20171;&#32461;&#19968;&#31181;&#26032;&#21442;&#25968;&#29992;&#20110;&#25511;&#21046;&#21516;&#36136;&#24615;&#65292;&#22312;&#29983;&#25104;&#30340;&#22270;&#20013;&#31995;&#32479;&#22320;&#30740;&#31350;&#26412;&#22320;&#21516;&#36136;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
GNN&#30340;&#30740;&#31350;&#24378;&#35843;&#39640;&#21516;&#36136;&#24615;&#65288;&#21363;&#30456;&#20284;&#31867;&#33410;&#28857;&#30456;&#20114;&#36830;&#25509;&#30340;&#20542;&#21521;&#65289;&#19982;&#33410;&#28857;&#20998;&#31867;&#30340;&#24378;&#39044;&#27979;&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#36825;&#31181;&#20851;&#31995;&#26356;&#21152;&#24494;&#22937;&#65292;&#35777;&#26126;&#21363;&#20351;&#31616;&#21333;&#30340;GNN&#20063;&#21487;&#20197;&#22312;&#26576;&#20123;&#24322;&#36136;&#24615;&#29615;&#22659;&#20013;&#23398;&#20064;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#20123;&#21457;&#29616;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#25105;&#20204;&#37325;&#26032;&#24605;&#32771;&#20102;&#20808;&#21069;&#20316;&#21697;&#20013;&#30340;&#20551;&#35774;&#65292;&#24182;&#30830;&#23450;&#25968;&#25454;&#38598;&#32463;&#24120;&#34987;&#35270;&#20026;&#22312;&#33410;&#28857;&#38388;&#20855;&#26377;&#24658;&#23450;&#30340;&#21516;&#36136;&#24615;&#27700;&#24179;&#12290;&#20026;&#20102;&#26356;&#25509;&#36817;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#29702;&#35770;&#19978;&#21644;&#23454;&#35777;&#22320;&#30740;&#31350;&#20102;GNN&#22312;&#27979;&#35797;&#26102;&#33410;&#28857;&#30340;&#26412;&#22320;&#21516;&#36136;&#24615;&#27700;&#24179;&#19982;&#20854;&#22270;&#30340;&#20840;&#23616;&#21516;&#36136;&#24615;&#27700;&#24179;&#20559;&#31163;&#26102;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#24110;&#21161;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#22312;&#21516;&#36136;&#24615;&#20998;&#26512;&#20013;&#24120;&#29992;&#30340;&#20248;&#20808;&#38468;&#21152;&#27169;&#22411;&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#21442;&#25968;&#65292;&#20197;&#25511;&#21046;&#29983;&#25104;&#30340;&#22270;&#20013;&#30340;&#26412;&#22320;&#21516;&#36136;&#24615;&#27700;&#24179;&#65292;&#20174;&#32780;&#23454;&#29616;&#31995;&#32479;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#25506;&#31350;&#26412;&#22320;&#21516;&#36136;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Research on GNNs has highlighted a relationship between high homophily (i.e., the tendency for nodes of a similar class to connect) and strong predictive performance in node classification. However, recent research has found the relationship to be more nuanced, demonstrating that even simple GNNs can learn in certain heterophilous settings. To bridge the gap between these findings, we revisit the assumptions made in previous works and identify that datasets are often treated as having a constant homophily level across nodes. To align closer to real-world datasets, we theoretically and empirically study the performance of GNNs when the local homophily level of a node deviates at test-time from the global homophily level of its graph. To aid our theoretical analysis, we introduce a new parameter to the preferential attachment model commonly used in homophily analysis to enable the control of local homophily levels in generated graphs, enabling a systematic empirical study on how local ho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#30740;&#31350;&#65292;&#25506;&#31350;&#20102;&#22312;&#19981;&#21516;&#22823;&#38470;&#12289;&#20113;&#20379;&#24212;&#21830;&#21644;&#25968;&#25454;&#20013;&#24515;&#33539;&#22260;&#20869;&#65292;&#20351;&#29992;&#20998;&#24067;&#24335;&#25968;&#25454;&#24182;&#34892;&#28857;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#26159;&#21542;&#26159;&#26356;&#20855;&#25104;&#26412;&#25928;&#30410;&#30340;&#36873;&#25321;&#65292;&#24182;&#27604;&#36739;&#20102;&#20854;&#19982;&#38598;&#20013;&#24335;&#35757;&#32451;&#30340;&#21487;&#25193;&#23637;&#24615;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.03163</link><description>&lt;p&gt;
&#22914;&#20309;&#36328;&#36234;&#20113;&#21644;&#22823;&#38470;&#22521;&#35757;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65311;&#19968;&#39033;&#23454;&#39564;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
How Can We Train Deep Learning Models Across Clouds and Continents? An Experimental Study. (arXiv:2306.03163v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03163
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#30740;&#31350;&#65292;&#25506;&#31350;&#20102;&#22312;&#19981;&#21516;&#22823;&#38470;&#12289;&#20113;&#20379;&#24212;&#21830;&#21644;&#25968;&#25454;&#20013;&#24515;&#33539;&#22260;&#20869;&#65292;&#20351;&#29992;&#20998;&#24067;&#24335;&#25968;&#25454;&#24182;&#34892;&#28857;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#26159;&#21542;&#26159;&#26356;&#20855;&#25104;&#26412;&#25928;&#30410;&#30340;&#36873;&#25321;&#65292;&#24182;&#27604;&#36739;&#20102;&#20854;&#19982;&#38598;&#20013;&#24335;&#35757;&#32451;&#30340;&#21487;&#25193;&#23637;&#24615;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20113;&#31471;&#25110;&#19987;&#29992;&#30828;&#20214;&#19978;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26159;&#26114;&#36149;&#30340;&#12290;&#19968;&#31181;&#26356;&#20855;&#25104;&#26412;&#25928;&#30410;&#30340;&#36873;&#25321;&#26159;&#25552;&#20379;&#28857;&#23454;&#20363;&#30340;&#39640;&#36229;&#35268;&#27169;&#20113;&#65292;&#36825;&#26159;&#19968;&#20010;&#20415;&#23452;&#20294;&#30701;&#26242;&#30340;&#36873;&#25321;&#65292;&#29992;&#20110;&#26367;&#20195;&#25353;&#38656;&#36164;&#28304;&#12290;&#30001;&#20110;&#28857;&#23454;&#20363;&#30340;&#21487;&#29992;&#24615;&#21487;&#33021;&#20250;&#22240;&#26085;&#26399;&#12289;&#22823;&#38470;&#21644;&#20113;&#20379;&#24212;&#21830;&#19981;&#21516;&#32780;&#21457;&#29983;&#21464;&#21270;&#65292;&#22240;&#27492;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#20998;&#37197;&#36164;&#28304;&#21487;&#33021;&#26356;&#20855;&#25104;&#26412;&#25928;&#30410;&#12290;&#20294;&#26159;&#65292;&#23578;&#26410;&#35843;&#26597;&#22320;&#29702;&#20998;&#24067;&#24335;&#25968;&#25454;&#24182;&#34892;&#28857;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#26159;&#21542;&#26159;&#38598;&#20013;&#24335;&#35757;&#32451;&#30340;&#26356;&#20855;&#25104;&#26412;&#25928;&#30410;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#26412;&#25991;&#26088;&#22312;&#22238;&#31572;&#19968;&#20010;&#38382;&#39064;&#65306;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#33021;&#21542;&#22312;&#35206;&#30422;&#19981;&#21516;&#25968;&#25454;&#20013;&#24515;&#21644;&#20113;&#25552;&#20379;&#21830;&#30340;&#28857; VM &#20840;&#29699;&#24066;&#22330;&#19978;&#20197;&#26356;&#20855;&#25104;&#26412;&#25928;&#30410;&#30340;&#26041;&#24335;&#36827;&#34892;&#35757;&#32451;&#65311;&#20026;&#20102;&#25552;&#20379;&#25351;&#23548;&#65292;&#25105;&#20204;&#24191;&#27867;&#35780;&#20272;&#20102;&#19981;&#21516;&#21306;&#22495;&#12289;&#22823;&#38470;&#21644;&#20113;&#23545;&#20195;&#34920;&#24615; CV &#21644; NLP &#27169;&#22411;&#30340;&#25104;&#26412;&#21644;&#21534;&#21520;&#37327;&#24433;&#21709;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25193;&#23637;&#24403;&#21069;&#30340;&#22521;&#35757;&#36873;&#25321;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#21487;&#25193;&#23637;&#24615;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training deep learning models in the cloud or on dedicated hardware is expensive. A more cost-efficient option are hyperscale clouds offering spot instances, a cheap but ephemeral alternative to on-demand resources. As spot instance availability can change depending on the time of day, continent, and cloud provider, it could be more cost-efficient to distribute resources over the world. Still, it has not been investigated whether geo-distributed, data-parallel spot deep learning training could be a more cost-efficient alternative to centralized training.  This paper aims to answer the question: Can deep learning models be cost-efficiently trained on a global market of spot VMs spanning different data centers and cloud providers? To provide guidance, we extensively evaluate the cost and throughput implications of training in different zones, continents, and clouds for representative CV and NLP models. To expand the current training options further, we compare the scalability potential f
&lt;/p&gt;</description></item><item><title>&#21453;&#20107;&#23454;&#25968;&#25454;&#22686;&#24378;&#26159;&#19968;&#31181;&#32531;&#35299;&#25968;&#25454;&#20013;&#28151;&#28102;&#20559;&#24046;&#30340;&#26041;&#27861;&#65292;&#26412;&#25991;&#20174;&#22240;&#26524;&#30340;&#35282;&#24230;&#20998;&#26512;&#20102;&#28151;&#28102;&#20559;&#24046;&#23545;&#20998;&#31867;&#22120;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#21435;&#38500;&#28151;&#28102;&#20559;&#24046;&#30340;&#25163;&#27573;&#65292;&#26377;&#21161;&#20110;&#22312;&#35266;&#23519;&#21040;&#30340;&#25968;&#25454;&#20998;&#24067;&#20043;&#22806;&#36827;&#34892;&#27867;&#21270;&#12290;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#31639;&#27861;&#29992;&#20110;&#29983;&#25104;&#21453;&#20107;&#23454;&#22270;&#20687;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.18183</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#28151;&#28102;&#19979;&#30340;&#21453;&#20107;&#23454;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Rethinking Counterfactual Data Augmentation Under Confounding. (arXiv:2305.18183v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18183
&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#25968;&#25454;&#22686;&#24378;&#26159;&#19968;&#31181;&#32531;&#35299;&#25968;&#25454;&#20013;&#28151;&#28102;&#20559;&#24046;&#30340;&#26041;&#27861;&#65292;&#26412;&#25991;&#20174;&#22240;&#26524;&#30340;&#35282;&#24230;&#20998;&#26512;&#20102;&#28151;&#28102;&#20559;&#24046;&#23545;&#20998;&#31867;&#22120;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#21435;&#38500;&#28151;&#28102;&#20559;&#24046;&#30340;&#25163;&#27573;&#65292;&#26377;&#21161;&#20110;&#22312;&#35266;&#23519;&#21040;&#30340;&#25968;&#25454;&#20998;&#24067;&#20043;&#22806;&#36827;&#34892;&#27867;&#21270;&#12290;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#31639;&#27861;&#29992;&#20110;&#29983;&#25104;&#21453;&#20107;&#23454;&#22270;&#20687;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#25968;&#25454;&#22686;&#24378;&#26368;&#36817;&#34987;&#25552;&#20986;&#26469;&#20316;&#20026;&#32531;&#35299;&#35757;&#32451;&#25968;&#25454;&#20013;&#28151;&#28102;&#20559;&#24046;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;&#36825;&#20123;&#20559;&#24046;&#65292;&#27604;&#22914;&#34394;&#20551;&#30340;&#20851;&#32852;&#65292;&#26159;&#30001;&#20110;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20013;&#21508;&#31181;&#35266;&#23519;&#21040;&#30340;&#21644;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#21464;&#37327;&#24341;&#36215;&#30340;&#12290;&#26412;&#25991;&#27491;&#24335;&#20998;&#26512;&#20102;&#28151;&#28102;&#20559;&#24046;&#22914;&#20309;&#24433;&#21709;&#19979;&#28216;&#20998;&#31867;&#22120;&#65292;&#24182;&#20174;&#22240;&#26524;&#30340;&#35282;&#24230;&#25506;&#35752;&#22522;&#20110;&#21453;&#20107;&#23454;&#25968;&#25454;&#22686;&#24378;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#25506;&#35752;&#22914;&#20309;&#21435;&#38500;&#28151;&#28102;&#20559;&#24046;&#20316;&#20026;&#23398;&#20064;&#19981;&#21464;&#29305;&#24449;&#30340;&#25163;&#27573;&#65292;&#26368;&#32456;&#26377;&#21161;&#20110;&#22312;&#35266;&#23519;&#21040;&#30340;&#25968;&#25454;&#20998;&#24067;&#20043;&#22806;&#36827;&#34892;&#27867;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#24378;&#22823;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#21453;&#20107;&#23454;&#22270;&#20687;&#65292;&#26377;&#25928;&#22320;&#32531;&#35299;&#28151;&#28102;&#25928;&#24212;&#23545;&#19979;&#28216;&#20998;&#31867;&#22120;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#22312;MNIST&#21464;&#20307;&#21644;CelebA&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual data augmentation has recently emerged as a method to mitigate confounding biases in the training data for a machine learning model. These biases, such as spurious correlations, arise due to various observed and unobserved confounding variables in the data generation process. In this paper, we formally analyze how confounding biases impact downstream classifiers and present a causal viewpoint to the solutions based on counterfactual data augmentation. We explore how removing confounding biases serves as a means to learn invariant features, ultimately aiding in generalization beyond the observed data distribution. Additionally, we present a straightforward yet powerful algorithm for generating counterfactual images, which effectively mitigates the influence of confounding effects on downstream classifiers. Through experiments on MNIST variants and the CelebA datasets, we demonstrate the effectiveness and practicality of our approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20999;&#32447;&#31354;&#38388;&#20013;&#36827;&#34892;&#20219;&#21153;&#31639;&#26415;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#26435;&#37325;&#20998;&#31163;&#26159;&#20854;&#26377;&#25928;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22522;&#20110;&#20999;&#32447;&#31354;&#38388;&#20013;&#30340;&#20219;&#21153;&#31639;&#26415;&#25216;&#26415;Tan&#65292;&#20854;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.12827</link><description>&lt;p&gt;
&#20999;&#32447;&#31354;&#38388;&#20013;&#30340;&#20219;&#21153;&#31639;&#26415;&#65306;&#39044;&#35757;&#32451;&#27169;&#22411;&#25913;&#36827;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Task Arithmetic in the Tangent Space: Improved Editing of Pre-Trained Models. (arXiv:2305.12827v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12827
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20999;&#32447;&#31354;&#38388;&#20013;&#36827;&#34892;&#20219;&#21153;&#31639;&#26415;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#26435;&#37325;&#20998;&#31163;&#26159;&#20854;&#26377;&#25928;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22522;&#20110;&#20999;&#32447;&#31354;&#38388;&#20013;&#30340;&#20219;&#21153;&#31639;&#26415;&#25216;&#26415;Tan&#65292;&#20854;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20219;&#21153;&#31639;&#26415;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#32463;&#27982;&#39640;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#30452;&#25509;&#22312;&#26435;&#37325;&#31354;&#38388;&#20013;&#32534;&#36753;&#39044;&#35757;&#32451;&#27169;&#22411;&#65306;&#36890;&#36807;&#28155;&#21152;&#19981;&#21516;&#20219;&#21153;&#30340;&#24494;&#35843;&#26435;&#37325;&#65292;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#22312;&#36825;&#20123;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#32780;&#25269;&#28040;&#23427;&#20204;&#21017;&#20250;&#23548;&#33268;&#20219;&#21153;&#36951;&#24536;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23545;&#20219;&#21153;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#20854;&#22522;&#26412;&#21407;&#29702;&#30340;&#29702;&#35299;&#20173;&#28982;&#26377;&#38480;&#12290;&#26412;&#25991;&#22312;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#23545;&#20219;&#21153;&#31639;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#65292;&#24182;&#34920;&#26126;&#26435;&#37325;&#20998;&#31163;&#26159;&#20351;&#20854;&#26377;&#25928;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#36825;&#31181;&#23646;&#24615;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#20986;&#29616;&#65292;&#24182;&#22312;&#26435;&#37325;&#31354;&#38388;&#20013;&#30340;&#19981;&#21516;&#26041;&#21521;&#19978;&#20135;&#29983;&#65292;&#22312;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#20989;&#25968;&#31354;&#38388;&#20013;&#27835;&#29702;&#29420;&#31435;&#30340;&#23616;&#37096;&#21306;&#22495;&#26102;&#20307;&#29616;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#36890;&#36807;&#23558;&#27169;&#22411;&#32447;&#24615;&#21270;&#20197;&#22312;&#20999;&#32447;&#31354;&#38388;&#20013;&#24494;&#35843;&#27169;&#22411;&#21487;&#20197;&#25918;&#22823;&#26435;&#37325;&#20998;&#31163;&#12290;&#36825;&#23548;&#33268;&#22312;&#22810;&#20010;&#20219;&#21153;&#31639;&#27861;&#22522;&#20934;&#21644;&#19981;&#21516;&#27169;&#22411;&#19978;&#23454;&#29616;&#20102;&#23454;&#36136;&#24615;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22522;&#20110;&#20999;&#32447;&#31354;&#38388;&#20013;&#30340;&#20219;&#21153;&#31639;&#26415;&#25216;&#26415;Tan&#65292;&#20854;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#19968;&#31181;&#23558;&#20219;&#21153;&#26435;&#37325;&#22686;&#37327;&#25237;&#24433;&#21040;&#20999;&#32447;&#31354;&#38388;&#19978;&#30340;&#26032;&#25237;&#24433;&#65292;&#30830;&#20445;&#32534;&#36753;&#30340;&#26435;&#37325;&#20445;&#25345;&#25509;&#36817;&#39044;&#35757;&#32451;&#27969;&#24418;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#20219;&#21153;&#31639;&#26415;&#30340;&#24037;&#20316;&#21407;&#29702;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#65292;&#24182;&#25351;&#20986;&#26435;&#37325;&#20998;&#31163;&#26159;&#20351;&#20854;&#25104;&#20026;&#21487;&#33021;&#30340;&#22522;&#26412;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Task arithmetic has recently emerged as a cost-effective and scalable approach to edit pre-trained models directly in weight space: By adding the fine-tuned weights of different tasks, the model's performance can be improved on these tasks, while negating them leads to task forgetting. Yet, our understanding of the effectiveness of task arithmetic and its underlying principles remains limited. We present a comprehensive study of task arithmetic in vision-language models and show that weight disentanglement is the crucial factor that makes it effective. This property arises during pre-training and manifests when distinct directions in weight space govern separate, localized regions in function space associated with the tasks. Notably, we show that fine-tuning models in their tangent space by linearizing them amplifies weight disentanglement. This leads to substantial performance improvements across multiple task arithmetic benchmarks and diverse models. Building on these findings, we pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#23558;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#36716;&#21270;&#20026;&#19968;&#32452;&#21333;&#30446;&#26631;&#38382;&#39064;&#36827;&#34892;&#35299;&#20915;&#65292;&#24182;&#20171;&#32461;&#20102;R2&#25928;&#29992;&#20989;&#25968;&#20316;&#20026;&#36866;&#24403;&#30340;&#30446;&#26631;&#20989;&#25968;&#12290;&#35813;&#25928;&#29992;&#20989;&#25968;&#21333;&#35843;&#19988;&#27425;&#27169;&#65292;&#21487;&#20197;&#20351;&#29992;&#36138;&#24515;&#20248;&#21270;&#31639;&#27861;&#35745;&#31639;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;</title><link>http://arxiv.org/abs/2305.11774</link><description>&lt;p&gt;
&#20351;&#29992;R2&#25928;&#29992;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Multi-Objective Optimization Using the R2 Utility. (arXiv:2305.11774v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11774
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#23558;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#36716;&#21270;&#20026;&#19968;&#32452;&#21333;&#30446;&#26631;&#38382;&#39064;&#36827;&#34892;&#35299;&#20915;&#65292;&#24182;&#20171;&#32461;&#20102;R2&#25928;&#29992;&#20989;&#25968;&#20316;&#20026;&#36866;&#24403;&#30340;&#30446;&#26631;&#20989;&#25968;&#12290;&#35813;&#25928;&#29992;&#20989;&#25968;&#21333;&#35843;&#19988;&#27425;&#27169;&#65292;&#21487;&#20197;&#20351;&#29992;&#36138;&#24515;&#20248;&#21270;&#31639;&#27861;&#35745;&#31639;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#30446;&#26631;&#20248;&#21270;&#30340;&#30446;&#26631;&#26159;&#30830;&#23450;&#25551;&#36848;&#22810;&#30446;&#26631;&#20043;&#38388;&#26368;&#20339;&#26435;&#34913;&#30340;&#28857;&#38598;&#21512;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#30690;&#37327;&#20540;&#20248;&#21270;&#38382;&#39064;&#65292;&#20174;&#19994;&#32773;&#24120;&#24120;&#20351;&#29992;&#26631;&#37327;&#21270;&#20989;&#25968;&#23558;&#22810;&#30446;&#26631;&#38382;&#39064;&#36716;&#21270;&#20026;&#19968;&#32452;&#21333;&#30446;&#26631;&#38382;&#39064;&#12290;&#36825;&#32452;&#26631;&#37327;&#21270;&#38382;&#39064;&#21487;&#20197;&#20351;&#29992;&#20256;&#32479;&#30340;&#21333;&#30446;&#26631;&#20248;&#21270;&#25216;&#26415;&#26469;&#35299;&#20915;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#32422;&#23450;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#36890;&#29992;&#30340;&#25968;&#23398;&#26694;&#26550;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#31574;&#30053;&#22914;&#20309;&#26377;&#25928;&#22320;&#23558;&#21407;&#22987;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#37325;&#26032;&#36716;&#21270;&#20026;&#23450;&#20041;&#22312;&#38598;&#21512;&#19978;&#30340;&#21333;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#12290;&#38024;&#23545;&#36825;&#20010;&#26032;&#38382;&#39064;&#30340;&#36866;&#24403;&#31867;&#21035;&#30340;&#30446;&#26631;&#20989;&#25968;&#26159;R2&#25928;&#29992;&#20989;&#25968;&#65292;&#23427;&#34987;&#23450;&#20041;&#20026;&#26631;&#37327;&#21270;&#20248;&#21270;&#38382;&#39064;&#30340;&#21152;&#26435;&#31215;&#20998;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20010;&#25928;&#29992;&#20989;&#25968;&#26159;&#21333;&#35843;&#30340;&#21644;&#27425;&#27169;&#30340;&#38598;&#21512;&#20989;&#25968;&#65292;&#21487;&#20197;&#36890;&#36807;&#36138;&#24515;&#20248;&#21270;&#31639;&#27861;&#26377;&#25928;&#22320;&#35745;&#31639;&#20986;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of multi-objective optimization is to identify a collection of points which describe the best possible trade-offs between the multiple objectives. In order to solve this vector-valued optimization problem, practitioners often appeal to the use of scalarization functions in order to transform the multi-objective problem into a collection of single-objective problems. This set of scalarized problems can then be solved using traditional single-objective optimization techniques. In this work, we formalise this convention into a general mathematical framework. We show how this strategy effectively recasts the original multi-objective optimization problem into a single-objective optimization problem defined over sets. An appropriate class of objective functions for this new problem is the R2 utility function, which is defined as a weighted integral over the scalarized optimization problems. We show that this utility function is a monotone and submodular set function, which can be op
&lt;/p&gt;</description></item><item><title>DoReMi&#26041;&#27861;&#20351;&#29992;&#20998;&#32452;&#20998;&#24067;&#24335;&#40065;&#26834;&#20248;&#21270;&#35757;&#32451;&#23567;&#22411;&#20195;&#29702;&#27169;&#22411;&#20197;&#20135;&#29983;&#22495;&#26435;&#37325;&#65292;&#20877;&#20351;&#29992;&#36825;&#20123;&#26435;&#37325;&#37325;&#26032;&#37319;&#26679;&#25968;&#25454;&#38598;&#35757;&#32451;&#22823;&#22411;&#27169;&#22411;&#65292;&#30456;&#27604;&#20351;&#29992;&#40664;&#35748;&#26435;&#37325;&#30340;&#22522;&#32447;&#27169;&#22411;&#65292;&#22312;The Pile&#21644;GLaM&#25968;&#25454;&#38598;&#19978;&#24179;&#22343;&#25552;&#39640;&#20102;6.5%&#21644;4.7%&#30340;few-shot&#19979;&#28216;&#20934;&#30830;&#24230;&#65292;&#20998;&#21035;&#20351;&#29992;2.6&#20493;&#21644;&#30456;&#21516;&#30340;&#35757;&#32451;&#27493;&#39588;&#36798;&#21040;&#22522;&#32447;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.10429</link><description>&lt;p&gt;
DoReMi: &#20248;&#21270;&#25968;&#25454;&#28151;&#21512;&#21152;&#36895;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining. (arXiv:2305.10429v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10429
&lt;/p&gt;
&lt;p&gt;
DoReMi&#26041;&#27861;&#20351;&#29992;&#20998;&#32452;&#20998;&#24067;&#24335;&#40065;&#26834;&#20248;&#21270;&#35757;&#32451;&#23567;&#22411;&#20195;&#29702;&#27169;&#22411;&#20197;&#20135;&#29983;&#22495;&#26435;&#37325;&#65292;&#20877;&#20351;&#29992;&#36825;&#20123;&#26435;&#37325;&#37325;&#26032;&#37319;&#26679;&#25968;&#25454;&#38598;&#35757;&#32451;&#22823;&#22411;&#27169;&#22411;&#65292;&#30456;&#27604;&#20351;&#29992;&#40664;&#35748;&#26435;&#37325;&#30340;&#22522;&#32447;&#27169;&#22411;&#65292;&#22312;The Pile&#21644;GLaM&#25968;&#25454;&#38598;&#19978;&#24179;&#22343;&#25552;&#39640;&#20102;6.5%&#21644;4.7%&#30340;few-shot&#19979;&#28216;&#20934;&#30830;&#24230;&#65292;&#20998;&#21035;&#20351;&#29992;2.6&#20493;&#21644;&#30456;&#21516;&#30340;&#35757;&#32451;&#27493;&#39588;&#36798;&#21040;&#22522;&#32447;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#25968;&#25454;&#22495;&#30340;&#28151;&#21512;&#27604;&#20363;&#65288;&#20363;&#22914;&#65292;&#32500;&#22522;&#30334;&#31185;&#12289;&#22270;&#20070;&#12289;&#32593;&#39029;&#25991;&#26412;&#65289;&#26497;&#22823;&#22320;&#24433;&#21709;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;DoReMi&#30340;Domain Reweighting with Minimax Optimization&#26041;&#27861;&#65292;&#23427;&#39318;&#20808;&#20351;&#29992;&#20998;&#32452;&#20998;&#24067;&#24335;&#40065;&#26834;&#20248;&#21270;&#65288;Group DRO&#65289;&#35757;&#32451;&#19968;&#20010;&#23567;&#20195;&#29702;&#27169;&#22411;&#65292;&#20197;&#20135;&#29983;&#22495;&#26435;&#37325;&#65288;&#28151;&#21512;&#27604;&#20363;&#65289;&#65292;&#32780;&#19981;&#38656;&#35201;&#30693;&#36947;&#19979;&#28216;&#20219;&#21153;&#30340;&#30693;&#35782;&#12290;&#28982;&#21518;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#22495;&#26435;&#37325;&#37325;&#26032;&#37319;&#26679;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#24182;&#35757;&#32451;&#19968;&#20010;&#26356;&#22823;&#30340;&#65292;&#20840;&#23610;&#23544;&#30340;&#27169;&#22411;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;DoReMi&#22312;&#19968;&#20010;280M&#21442;&#25968;&#30340;&#20195;&#29702;&#27169;&#22411;&#19978;&#65292;&#26356;&#26377;&#25928;&#22320;&#25214;&#21040;&#35757;&#32451;&#19968;&#20010;8B&#21442;&#25968;&#27169;&#22411;&#65288;30&#20493;&#22823;&#65289;&#30340;&#22495;&#26435;&#37325;&#12290;&#22312;The Pile&#19978;&#65292;&#21363;&#20351;&#22312;&#20943;&#23567;&#19968;&#20123;&#22495;&#30340;&#27604;&#37325;&#26102;&#65292;DoReMi&#20063;&#33021;&#25552;&#39640;&#25152;&#26377;&#22495;&#30340;perplexity&#12290;&#30456;&#27604;&#20351;&#29992;The Pile&#30340;&#40664;&#35748;&#22495;&#26435;&#37325;&#35757;&#32451;&#30340;&#22522;&#32447;&#27169;&#22411;&#65292;DoReMi&#23558;&#24179;&#22343;few-shot&#19979;&#28216;&#20934;&#30830;&#24230;&#25552;&#39640;&#20102;6.5%&#65292;&#24182;&#20351;&#29992;2.6&#20493;&#30340;&#35757;&#32451;&#27493;&#39588;&#36798;&#21040;&#22522;&#32447;&#20934;&#30830;&#24230;&#12290;&#22312;GLaM&#25968;&#25454;&#38598;&#19978;&#65292;DoReMi&#27809;&#26377;&#20219;&#20309;&#20851;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#30693;&#35782;&#65292;&#25552;&#39640;&#20102;4.7%&#65288;&#27425;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65289;&#30340;few-shot&#20934;&#30830;&#24230;&#65292;&#22312;&#30456;&#21516;&#30340;&#35757;&#32451;&#27493;&#39588;&#19979;&#25552;&#39640;&#20102;9.0%&#30340;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The mixture proportions of pretraining data domains (e.g., Wikipedia, books, web text) greatly affect language model (LM) performance. In this paper, we propose Domain Reweighting with Minimax Optimization (DoReMi), which first trains a small proxy model using group distributionally robust optimization (Group DRO) over domains to produce domain weights (mixture proportions) without knowledge of downstream tasks. We then resample a dataset with these domain weights and train a larger, full-sized model. In our experiments, we use DoReMi on a 280M-parameter proxy model to find domain weights for training an 8B-parameter model (30x larger) more efficiently. On The Pile, DoReMi improves perplexity across all domains, even when it downweights a domain. DoReMi improves average few-shot downstream accuracy by 6.5% over a baseline model trained using The Pile's default domain weights and reaches the baseline accuracy with 2.6x fewer training steps. On the GLaM dataset, DoReMi, which has no know
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#23545;&#27604;&#23398;&#20064;&#21644;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#24418;&#25104;&#30340;&#31751;&#65292;&#25581;&#31034;&#20102;&#23545;&#27604;&#23398;&#20064;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#23616;&#37096;&#23494;&#24230;&#20294;&#26080;&#20840;&#23616;&#23494;&#24230;&#30340;&#31751;&#65292;&#32780;&#30417;&#30563;&#23398;&#20064;&#21019;&#24314;&#20855;&#26377;&#23616;&#37096;&#21644;&#20840;&#23616;&#23494;&#24230;&#30340;&#31751;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#20351;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;&#20998;&#31867;&#22120;&#20316;&#20026;&#22788;&#29702;&#23616;&#37096;&#23494;&#38598;&#31751;&#30340;&#32447;&#24615;&#20998;&#31867;&#22120;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#24182;&#21033;&#29992;t-SNE&#21487;&#35270;&#21270;&#35777;&#26126;&#20102;&#23545;&#27604;&#21644;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#20135;&#29983;&#30340;&#29305;&#24449;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2305.10229</link><description>&lt;p&gt;
&#25506;&#32034;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#24402;&#32435;&#20559;&#24046;&#65306;&#20174;&#32858;&#31867;&#35282;&#24230;&#20986;&#21457;
&lt;/p&gt;
&lt;p&gt;
Exploring Inductive Biases in Contrastive Learning: A Clustering Perspective. (arXiv:2305.10229v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10229
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#23545;&#27604;&#23398;&#20064;&#21644;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#24418;&#25104;&#30340;&#31751;&#65292;&#25581;&#31034;&#20102;&#23545;&#27604;&#23398;&#20064;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#23616;&#37096;&#23494;&#24230;&#20294;&#26080;&#20840;&#23616;&#23494;&#24230;&#30340;&#31751;&#65292;&#32780;&#30417;&#30563;&#23398;&#20064;&#21019;&#24314;&#20855;&#26377;&#23616;&#37096;&#21644;&#20840;&#23616;&#23494;&#24230;&#30340;&#31751;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#20351;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;&#20998;&#31867;&#22120;&#20316;&#20026;&#22788;&#29702;&#23616;&#37096;&#23494;&#38598;&#31751;&#30340;&#32447;&#24615;&#20998;&#31867;&#22120;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#24182;&#21033;&#29992;t-SNE&#21487;&#35270;&#21270;&#35777;&#26126;&#20102;&#23545;&#27604;&#21644;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#20135;&#29983;&#30340;&#29305;&#24449;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#21644;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#20043;&#38388;&#25968;&#25454;&#32452;&#32455;&#30340;&#24046;&#24322;&#65292;&#37325;&#28857;&#20851;&#27880;&#23616;&#37096;&#23494;&#38598;&#31751;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#24341;&#20837;&#19968;&#20010;&#26032;&#30340;&#24230;&#37327;&#25351;&#26631;&#65292;&#30456;&#23545;&#23616;&#37096;&#23494;&#24230;&#65288;RLD&#65289;&#65292;&#29992;&#20110;&#23450;&#37327;&#27979;&#37327;&#31751;&#20869;&#30340;&#23616;&#37096;&#23494;&#24230;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#35270;&#35273;&#31034;&#20363;&#65292;&#20197;&#31361;&#20986;&#23616;&#37096;&#23494;&#38598;&#31751;&#21644;&#20840;&#23616;&#23494;&#38598;&#31751;&#20043;&#38388;&#30340;&#21306;&#21035;&#12290;&#36890;&#36807;&#23545;&#27604;&#23545;&#27604;&#23398;&#20064;&#21644;&#30417;&#30563;&#23398;&#20064;&#24418;&#25104;&#30340;&#31751;&#65292;&#25105;&#20204;&#21457;&#29616;&#23545;&#27604;&#23398;&#20064;&#29983;&#25104;&#20855;&#26377;&#23616;&#37096;&#23494;&#24230;&#32780;&#26080;&#20840;&#23616;&#23494;&#24230;&#30340;&#31751;&#65292;&#32780;&#30417;&#30563;&#23398;&#20064;&#21019;&#24314;&#20855;&#26377;&#23616;&#37096;&#21644;&#20840;&#23616;&#23494;&#24230;&#30340;&#31751;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#20351;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#20998;&#31867;&#22120;&#20316;&#20026;&#22788;&#29702;&#23616;&#37096;&#23494;&#38598;&#31751;&#30340;&#32447;&#24615;&#20998;&#31867;&#22120;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;t-SNE&#21487;&#35270;&#21270;&#26469;&#35777;&#26126;&#23545;&#27604;&#21644;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#20135;&#29983;&#30340;&#29305;&#24449;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#26469;&#32467;&#26463;&#26412;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the differences in data organization between contrastive and supervised learning methods, focusing on the concept of locally dense clusters. We introduce a novel metric, Relative Local Density (RLD), to quantitatively measure local density within clusters. Visual examples are provided to highlight the distinctions between locally dense clusters and globally dense ones. By comparing the clusters formed by contrastive and supervised learning, we reveal that contrastive learning generates locally dense clusters without global density, while supervised learning creates clusters with both local and global density. We further explore the use of a Graph Convolutional Network (GCN) classifier as an alternative to linear classifiers for handling locally dense clusters. Finally, we utilize t-SNE visualizations to substantiate the differences between the features generated by contrastive and supervised learning methods. We conclude by proposing future research directions, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#35745;&#31639;&#29615;&#36335;&#30340;&#26041;&#27861;&#26469;&#39044;&#27979;&#22810;&#32452;&#20998;&#26448;&#26009;&#30340;&#34920;&#38754;&#30456;&#22270;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#20132;&#20114;&#20316;&#29992;&#21183;&#21152;&#36895;&#20102;&#33021;&#37327;&#35780;&#20998;&#21644;&#32479;&#35745;&#37319;&#26679;&#26041;&#27861;&#65292;&#22312;NiAl (110)&#19978;&#39044;&#27979;&#20102;&#26032;&#39062;&#30340;&#34920;&#38754;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2305.07251</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21152;&#36895;&#30340;&#27169;&#25311;&#20351;&#24471;&#26080;&#32463;&#39564;&#34920;&#38754;&#37325;&#24314;&#25104;&#20026;&#21487;&#33021;
&lt;/p&gt;
&lt;p&gt;
Machine-learning-accelerated simulations enable heuristic-free surface reconstruction. (arXiv:2305.07251v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07251
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#35745;&#31639;&#29615;&#36335;&#30340;&#26041;&#27861;&#26469;&#39044;&#27979;&#22810;&#32452;&#20998;&#26448;&#26009;&#30340;&#34920;&#38754;&#30456;&#22270;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#20132;&#20114;&#20316;&#29992;&#21183;&#21152;&#36895;&#20102;&#33021;&#37327;&#35780;&#20998;&#21644;&#32479;&#35745;&#37319;&#26679;&#26041;&#27861;&#65292;&#22312;NiAl (110)&#19978;&#39044;&#27979;&#20102;&#26032;&#39062;&#30340;&#34920;&#38754;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20652;&#21270;&#25110;&#30005;&#23376;&#31561;&#39046;&#22495;&#65292;&#29702;&#35299;&#29289;&#36136;&#34920;&#38754;&#21644;&#30028;&#38754;&#33267;&#20851;&#37325;&#35201;&#12290;&#20174;&#30005;&#23376;&#32467;&#26500;&#20013;&#30340;&#33021;&#37327;&#21644;&#32479;&#35745;&#21147;&#23398;&#30456;&#32467;&#21512;&#30340;&#31532;&#19968;&#24615;&#21407;&#29702;&#27169;&#25311;&#21487;&#20197;&#22312;&#21407;&#21017;&#19978;&#39044;&#27979;&#26448;&#26009;&#34920;&#38754;&#30340;&#32467;&#26500;&#19982;&#28909;&#21147;&#23398;&#21464;&#37327;&#30340;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#24403;&#19982;&#24517;&#39035;&#36827;&#34892;&#32479;&#35745;&#37319;&#26679;&#30340;&#24191;&#38420;&#30456;&#31354;&#38388;&#32806;&#21512;&#26102;&#65292;&#31934;&#30830;&#30340;&#33021;&#37327;&#27169;&#25311;&#26159;&#31105;&#27490;&#30340;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#37325;&#35745;&#31639;&#29615;&#36335;&#26469;&#39044;&#27979;&#22810;&#32452;&#20998;&#26448;&#26009;&#30340;&#34920;&#38754;&#30456;&#22270;&#65292;&#21516;&#26102;&#21152;&#36895;&#33021;&#37327;&#35780;&#20998;&#21644;&#32479;&#35745;&#37319;&#26679;&#26041;&#27861;&#12290;&#36890;&#36807;&#38381;&#29615;&#20027;&#21160;&#23398;&#20064;&#65292;&#20351;&#29992;&#24555;&#36895;&#12289;&#21487;&#25193;&#23637;&#21644;&#25968;&#25454;&#39640;&#25928;&#30340;&#26426;&#22120;&#23398;&#20064;&#30456;&#20114;&#20316;&#29992;&#21183;&#22312;&#39640;&#36890;&#37327;&#23494;&#24230;&#27867;&#20989;&#29702;&#35770;&#35745;&#31639;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;&#20551;&#24819;&#34920;&#38754;&#20301;&#28857;&#19978;&#20351;&#29992;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#37319;&#26679;&#65292;&#22312;&#21322;&#27491;&#21017;&#31995;&#32508;&#20013;&#23454;&#29616;&#37319;&#26679;&#12290;&#22312;&#36825;&#31181;&#26041;&#27861;&#30340;&#24341;&#23548;&#19979;&#65292;&#39044;&#27979;&#30340;GaN&#65288;0001&#65289;&#21644;SrTiO3&#65288;001&#65289;&#34920;&#38754;&#19982;&#36807;&#21435;&#30340;&#24037;&#20316;&#19968;&#33268;&#65292;&#24182;&#19988;&#22312;NiAl(110)&#19978;&#39044;&#27979;&#20102;&#26032;&#39062;&#30340;&#34920;&#38754;&#32467;&#26500;&#65292;&#20854;&#20013;&#21457;&#29616;&#22312;&#31354;&#20301;&#30340;&#23384;&#22312;&#19979;&#65292;&#21453;&#20301;&#28857;&#26434;&#36136;&#26159;&#31283;&#23450;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding material surfaces and interfaces is vital in applications like catalysis or electronics. Ab initio simulations, combining energies from electronic structure with statistical mechanics, can, in principle, predict the structure of material surfaces as a function of thermodynamic variables. However, accurate energy simulations are prohibitive when coupled to the vast phase space that must be statistically sampled. Here, we present a bi-faceted computational loop to predict surface phase diagrams of multi-component materials that accelerates both the energy scoring and statistical sampling methods. Fast, scalable, and data-efficient machine learning interatomic potentials are trained on high-throughput density-functional theory calculations through closed-loop active learning. Markov-chain Monte Carlo sampling in the semi-grand canonical ensemble is enabled by using virtual surface sites. The predicted surfaces for GaN(0001) and SrTiO3(001) are in agreement with past work and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#20449;&#24687;&#35770;&#30340;&#35282;&#24230;&#22238;&#39038;&#20102;&#21508;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#27491;&#24335;&#30340;&#8220;&#33258;&#30417;&#30563;&#20449;&#24687;&#29702;&#35770;&#23398;&#20064;&#38382;&#39064;&#8221;&#32479;&#19968;&#26694;&#26550;&#12290;&#27492;&#22806;&#65292;&#35752;&#35770;&#20102;&#21387;&#32553;&#24615;&#21644;&#21387;&#32553;&#31639;&#27861;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#20984;&#26174;&#20102;&#28508;&#22312;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2304.09355</link><description>&lt;p&gt;
&#21387;&#32553;&#19982;&#21542;&#8212;&#8212;&#33258;&#30417;&#30563;&#23398;&#20064;&#19982;&#20449;&#24687;&#35770;:&#19968;&#31687;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
To Compress or Not to Compress -- Self-Supervised Learning and Information Theory: A Review. (arXiv:2304.09355v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09355
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#20449;&#24687;&#35770;&#30340;&#35282;&#24230;&#22238;&#39038;&#20102;&#21508;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#27491;&#24335;&#30340;&#8220;&#33258;&#30417;&#30563;&#20449;&#24687;&#29702;&#35770;&#23398;&#20064;&#38382;&#39064;&#8221;&#32479;&#19968;&#26694;&#26550;&#12290;&#27492;&#22806;&#65292;&#35752;&#35770;&#20102;&#21387;&#32553;&#24615;&#21644;&#21387;&#32553;&#31639;&#27861;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#20984;&#26174;&#20102;&#28508;&#22312;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#30340;&#26631;&#27880;&#25968;&#25454;&#12290;&#33258;&#30417;&#30563;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#20010;&#26367;&#20195;&#33539;&#20363;&#65292;&#20351;&#24471;&#27169;&#22411;&#21487;&#20197;&#22312;&#27809;&#26377;&#26126;&#30830;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#12290;&#20449;&#24687;&#35770;&#22312;&#29702;&#35299;&#21644;&#20248;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#29305;&#21035;&#22320;&#65292;&#20449;&#24687;&#29942;&#39048;&#21407;&#21017;&#34987;&#24212;&#29992;&#20110;&#22312;&#30417;&#30563;&#35774;&#32622;&#20013;&#20248;&#21270;&#21387;&#32553;&#21644;&#30456;&#20851;&#20449;&#24687;&#20445;&#23384;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#28982;&#32780;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#26368;&#20339;&#20449;&#24687;&#30446;&#26631;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#20449;&#24687;&#35770;&#30340;&#35282;&#24230;&#22238;&#39038;&#20102;&#21508;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#27491;&#24335;&#30340;&#8220;&#33258;&#30417;&#30563;&#20449;&#24687;&#29702;&#35770;&#23398;&#20064;&#38382;&#39064;&#8221;&#32479;&#19968;&#26694;&#26550;&#12290;&#25105;&#20204;&#23558;&#29616;&#26377;&#30740;&#31350;&#34701;&#21512;&#25104;&#19968;&#20010;&#19968;&#33268;&#30340;&#26694;&#26550;&#65292;&#30740;&#31350;&#20102;&#26368;&#36817;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#24182;&#30830;&#23450;&#20102;&#30740;&#31350;&#26426;&#20250;&#21644;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#21387;&#32553;&#24615;&#21644;&#21387;&#32553;&#31639;&#27861;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#20984;&#26174;&#20102;&#28508;&#22312;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks have demonstrated remarkable performance in supervised learning tasks but require large amounts of labeled data. Self-supervised learning offers an alternative paradigm, enabling the model to learn from data without explicit labels. Information theory has been instrumental in understanding and optimizing deep neural networks. Specifically, the information bottleneck principle has been applied to optimize the trade-off between compression and relevant information preservation in supervised settings. However, the optimal information objective in self-supervised learning remains unclear. In this paper, we review various approaches to self-supervised learning from an information-theoretic standpoint and present a unified framework that formalizes the \textit{self-supervised information-theoretic learning problem}. We integrate existing research into a coherent framework, examine recent self-supervised methods, and identify research opportunities and challenges. Moreove
&lt;/p&gt;</description></item><item><title>LASER&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#26041;&#27861;&#26469;&#23398;&#20064;&#35821;&#20041;&#35270;&#39057;&#34920;&#31034;&#65292;&#36890;&#36807;&#36923;&#36753;&#35268;&#33539;&#25429;&#25417;&#35270;&#39057;&#25968;&#25454;&#20013;&#30340;&#26102;&#31354;&#23646;&#24615;&#65292;&#33021;&#22815;&#23545;&#40784;&#21407;&#22987;&#35270;&#39057;&#21644;&#35268;&#33539;&#65292;&#26377;&#25928;&#22320;&#35757;&#32451;&#20302;&#32423;&#24863;&#30693;&#27169;&#22411;&#20197;&#25552;&#21462;&#31526;&#21512;&#25152;&#38656;&#39640;&#32423;&#35268;&#33539;&#30340;&#35270;&#39057;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2304.07647</link><description>&lt;p&gt;
LASER&#65306;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#35821;&#20041;&#35270;&#39057;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
LASER: Neuro-Symbolic Learning of Semantic Video Representations. (arXiv:2304.07647v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07647
&lt;/p&gt;
&lt;p&gt;
LASER&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#26041;&#27861;&#26469;&#23398;&#20064;&#35821;&#20041;&#35270;&#39057;&#34920;&#31034;&#65292;&#36890;&#36807;&#36923;&#36753;&#35268;&#33539;&#25429;&#25417;&#35270;&#39057;&#25968;&#25454;&#20013;&#30340;&#26102;&#31354;&#23646;&#24615;&#65292;&#33021;&#22815;&#23545;&#40784;&#21407;&#22987;&#35270;&#39057;&#21644;&#35268;&#33539;&#65292;&#26377;&#25928;&#22320;&#35757;&#32451;&#20302;&#32423;&#24863;&#30693;&#27169;&#22411;&#20197;&#25552;&#21462;&#31526;&#21512;&#25152;&#38656;&#39640;&#32423;&#35268;&#33539;&#30340;&#35270;&#39057;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#28041;&#21450;&#35270;&#39057;&#30340;AI&#24212;&#29992;&#65288;&#22914;&#35270;&#39057;-&#25991;&#26412;&#23545;&#40784;&#12289;&#35270;&#39057;&#25628;&#32034;&#21644;&#35270;&#39057;&#23383;&#24149;&#65289;&#21463;&#30410;&#20110;&#23545;&#35270;&#39057;&#35821;&#20041;&#30340;&#32454;&#33268;&#29702;&#35299;&#12290;&#29616;&#26377;&#30340;&#35270;&#39057;&#29702;&#35299;&#26041;&#27861;&#35201;&#20040;&#38656;&#35201;&#22823;&#37327;&#27880;&#37322;&#65292;&#35201;&#20040;&#22522;&#20110;&#19981;&#21487;&#35299;&#37322;&#30340;&#36890;&#29992;&#23884;&#20837;&#65292;&#21487;&#33021;&#20250;&#24573;&#30053;&#37325;&#35201;&#32454;&#33410;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;LASER&#65292;&#36825;&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#33021;&#22815;&#25429;&#25417;&#35270;&#39057;&#25968;&#25454;&#20013;&#20016;&#23500;&#30340;&#26102;&#31354;&#23646;&#24615;&#30340;&#36923;&#36753;&#35268;&#33539;&#26469;&#23398;&#20064;&#35821;&#20041;&#35270;&#39057;&#34920;&#31034;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#36890;&#36807;&#21407;&#22987;&#35270;&#39057;&#19982;&#35268;&#33539;&#20043;&#38388;&#30340;&#23545;&#40784;&#26469;&#20844;&#24335;&#21270;&#38382;&#39064;&#12290;&#23545;&#40784;&#36807;&#31243;&#26377;&#25928;&#22320;&#35757;&#32451;&#20102;&#20302;&#23618;&#24863;&#30693;&#27169;&#22411;&#65292;&#20197;&#25552;&#21462;&#31526;&#21512;&#25152;&#38656;&#39640;&#23618;&#35268;&#33539;&#30340;&#32454;&#31890;&#24230;&#35270;&#39057;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#27969;&#31243;&#21487;&#20197;&#31471;&#21040;&#31471;&#22320;&#35757;&#32451;&#65292;&#24182;&#21487;&#32435;&#20837;&#20174;&#35268;&#33539;&#23548;&#20986;&#30340;&#23545;&#27604;&#21644;&#35821;&#20041;&#25439;&#22833;&#20989;&#25968;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20855;&#26377;&#20016;&#23500;&#31354;&#38388;&#21644;&#26102;&#38388;&#20449;&#24687;&#30340;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern AI applications involving video, such as video-text alignment, video search, and video captioning, benefit from a fine-grained understanding of video semantics. Existing approaches for video understanding are either data-hungry and need low-level annotation, or are based on general embeddings that are uninterpretable and can miss important details. We propose LASER, a neuro-symbolic approach that learns semantic video representations by leveraging logic specifications that can capture rich spatial and temporal properties in video data. In particular, we formulate the problem in terms of alignment between raw videos and specifications. The alignment process efficiently trains low-level perception models to extract a fine-grained video representation that conforms to the desired high-level specification. Our pipeline can be trained end-to-end and can incorporate contrastive and semantic loss functions derived from specifications. We evaluate our method on two datasets with rich sp
&lt;/p&gt;</description></item><item><title>TinyML&#37096;&#32626;&#20102;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21040;&#20302;&#25104;&#26412;&#30340;&#24494;&#25511;&#21046;&#22120;&#31995;&#32479;&#19978;&#65292;&#21487;&#20197;&#35299;&#38145;&#26080;&#25968;&#22987;&#32456;&#22788;&#20110;&#24320;&#21551;&#29366;&#24577;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#65292;&#36825;&#39033;&#26032;&#20852;&#25216;&#26415;&#26377;&#21161;&#20110;&#35299;&#20915;&#21487;&#25345;&#32493;&#21457;&#23637;&#25361;&#25112;&#65292;&#20294;&#38656;&#35201;&#35780;&#20272;&#21644;&#32531;&#35299;&#20854;&#29615;&#22659;&#24433;&#21709;&#20197;&#30830;&#20445;&#21487;&#25345;&#32493;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.11899</link><description>&lt;p&gt;
TinyML&#30340;&#21487;&#25345;&#32493;&#24615;&#35780;&#20272;&#65306;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#23545;&#24494;&#25511;&#21046;&#22120;&#30340;&#29615;&#22659;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Is TinyML Sustainable? Assessing the Environmental Impacts of Machine Learning on Microcontrollers. (arXiv:2301.11899v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11899
&lt;/p&gt;
&lt;p&gt;
TinyML&#37096;&#32626;&#20102;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21040;&#20302;&#25104;&#26412;&#30340;&#24494;&#25511;&#21046;&#22120;&#31995;&#32479;&#19978;&#65292;&#21487;&#20197;&#35299;&#38145;&#26080;&#25968;&#22987;&#32456;&#22788;&#20110;&#24320;&#21551;&#29366;&#24577;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#65292;&#36825;&#39033;&#26032;&#20852;&#25216;&#26415;&#26377;&#21161;&#20110;&#35299;&#20915;&#21487;&#25345;&#32493;&#21457;&#23637;&#25361;&#25112;&#65292;&#20294;&#38656;&#35201;&#35780;&#20272;&#21644;&#32531;&#35299;&#20854;&#29615;&#22659;&#24433;&#21709;&#20197;&#30830;&#20445;&#21487;&#25345;&#32493;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#22686;&#38271;&#30340;&#30899;&#25490;&#25918;&#21644;&#20840;&#29699;&#22403;&#22334;&#38382;&#39064;&#24341;&#36215;&#20102;&#20154;&#20204;&#23545;&#29615;&#22659;&#26410;&#26469;&#30340;&#21487;&#25345;&#32493;&#24615;&#20851;&#27880;&#12290;&#24555;&#36895;&#22686;&#38271;&#30340;&#29289;&#32852;&#32593;&#21487;&#33021;&#20250;&#21152;&#21095;&#36825;&#19968;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;Tiny Machine Learning&#65288;TinyML&#65289;&#36825;&#19968;&#26032;&#20852;&#39046;&#22495;&#26377;&#26426;&#20250;&#36890;&#36807;&#21487;&#25345;&#32493;&#35745;&#31639;&#23454;&#36341;&#26469;&#24110;&#21161;&#35299;&#20915;&#36825;&#20123;&#29615;&#22659;&#25361;&#25112;&#12290;TinyML&#26159;&#23558;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#31639;&#27861;&#37096;&#32626;&#21040;&#20302;&#25104;&#26412;&#12289;&#20302;&#21151;&#32791;&#30340;&#24494;&#25511;&#21046;&#22120;&#31995;&#32479;&#19978;&#65292;&#23454;&#29616;&#20102;&#35774;&#22791;&#19978;&#30340;&#20256;&#24863;&#22120;&#20998;&#26512;&#65292;&#20174;&#32780;&#37322;&#25918;&#20986;&#26080;&#25968;&#30340;&#22987;&#32456;&#22788;&#20110;&#24320;&#21551;&#29366;&#24577;&#30340;ML&#24212;&#29992;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#36825;&#20123;TinyML&#24212;&#29992;&#24212;&#23545;&#20851;&#38190;&#21487;&#25345;&#32493;&#24615;&#25361;&#25112;&#30340;&#28508;&#21147;&#65292;&#20197;&#21450;&#36825;&#19968;&#26032;&#20852;&#25216;&#26415;&#30340;&#29615;&#22659;&#36275;&#36857;&#12290;&#36890;&#36807;&#23436;&#20840;&#30340;&#29983;&#21629;&#21608;&#26399;&#20998;&#26512;&#65288;LCA&#65289;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;TinyML&#31995;&#32479;&#36890;&#36807;&#21551;&#29992;&#20943;&#23569;&#20854;&#20182;&#34892;&#19994;&#25490;&#25918;&#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#20026;&#25269;&#28040;&#30899;&#25490;&#25918;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#19981;&#21487;&#25345;&#32493;&#22320;&#25193;&#22823;&#35268;&#27169;&#65292;TinyML&#30340;&#22686;&#38271;&#21487;&#33021;&#20250;&#32473;&#29615;&#22659;&#36896;&#25104;&#37325;&#22823;&#36127;&#25285;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65306;TinyML&#31038;&#21306;&#30340;&#21033;&#30410;&#30456;&#20851;&#32773;&#24517;&#39035;&#31215;&#26497;&#35780;&#20272;&#21644;&#32531;&#35299;&#36825;&#39033;&#25216;&#26415;&#30340;&#29615;&#22659;&#24433;&#21709;&#65292;&#20197;&#30830;&#20445;&#20854;&#21487;&#25345;&#32493;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The sustained growth of carbon emissions and global waste elicits significant sustainability concerns for our environment's future. The growing Internet of Things (IoT) has the potential to exacerbate this issue. However, an emerging area known as Tiny Machine Learning (TinyML) has the opportunity to help address these environmental challenges through sustainable computing practices. TinyML, the deployment of machine learning (ML) algorithms onto low-cost, low-power microcontroller systems, enables on-device sensor analytics that unlocks numerous always-on ML applications. This article discusses both the potential of these TinyML applications to address critical sustainability challenges, as well as the environmental footprint of this emerging technology. Through a complete life cycle analysis (LCA), we find that TinyML systems present opportunities to offset their carbon emissions by enabling applications that reduce the emissions of other sectors. Nevertheless, when globally scaled, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;(pFedMB)&#65292;&#36890;&#36807;&#22810;&#20998;&#25903;&#32467;&#26500;&#23454;&#29616;&#20010;&#24615;&#21270;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#32858;&#21512;&#26041;&#27861;&#20197;&#25552;&#39640;&#36890;&#20449;&#25928;&#29575;&#21644;&#38477;&#20302;&#35745;&#31639;&#24320;&#38144;&#12290;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#23454;&#39564;&#20013;&#22343;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#22312;&#23458;&#25143;&#31471;&#20855;&#26377;&#26469;&#33258;&#22797;&#26434;&#20998;&#24067;&#30340;&#25968;&#25454;&#19988;&#19981;&#33021;&#30830;&#23450;&#24444;&#27492;&#30340;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#20419;&#36827;&#25317;&#26377;&#30456;&#20284;&#25968;&#25454;&#30340;&#23458;&#25143;&#31471;&#36827;&#34892;&#26356;&#22810;&#21327;&#20316;&#12290;</title><link>http://arxiv.org/abs/2211.07931</link><description>&lt;p&gt;
&#22810;&#20998;&#25903;&#32467;&#26500;&#19979;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Personalized Federated Learning with Multi-branch Architecture. (arXiv:2211.07931v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.07931
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;(pFedMB)&#65292;&#36890;&#36807;&#22810;&#20998;&#25903;&#32467;&#26500;&#23454;&#29616;&#20010;&#24615;&#21270;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#32858;&#21512;&#26041;&#27861;&#20197;&#25552;&#39640;&#36890;&#20449;&#25928;&#29575;&#21644;&#38477;&#20302;&#35745;&#31639;&#24320;&#38144;&#12290;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#23454;&#39564;&#20013;&#22343;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#22312;&#23458;&#25143;&#31471;&#20855;&#26377;&#26469;&#33258;&#22797;&#26434;&#20998;&#24067;&#30340;&#25968;&#25454;&#19988;&#19981;&#33021;&#30830;&#23450;&#24444;&#27492;&#30340;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#20419;&#36827;&#25317;&#26377;&#30456;&#20284;&#25968;&#25454;&#30340;&#23458;&#25143;&#31471;&#36827;&#34892;&#26356;&#22810;&#21327;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#21435;&#20013;&#24515;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#20351;&#24471;&#22810;&#20010;&#23458;&#25143;&#31471;&#21487;&#20197;&#21327;&#20316;&#35757;&#32451;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#23458;&#25143;&#31471;&#30456;&#20114;&#25581;&#31034;&#20854;&#21407;&#22987;&#25968;&#25454;&#12290;&#34429;&#28982;&#20256;&#32479;&#30340;FL&#35757;&#32451;&#21333;&#19968;&#30340;&#20840;&#23616;&#27169;&#22411;&#65292;&#34920;&#29616;&#22312;&#23458;&#25143;&#31471;&#30340;&#24179;&#22343;&#24615;&#33021;&#65292;&#20294;&#26159;&#36328;&#23458;&#25143;&#31471;&#30340;&#32479;&#35745;&#25968;&#25454;&#24322;&#36136;&#24615;&#20419;&#20351;&#20102;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;PFL&#65289;&#30340;&#21457;&#23637;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#20010;&#24615;&#21270;&#30340;&#27169;&#22411;&#65292;&#24182;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;PFL&#38754;&#20020;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#22914;&#20309;&#22312;&#27599;&#20010;&#23458;&#25143;&#31471;&#20855;&#26377;&#26469;&#33258;&#22797;&#26434;&#20998;&#24067;&#30340;&#25968;&#25454;&#19988;&#19981;&#33021;&#30830;&#23450;&#24444;&#27492;&#30340;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#20419;&#36827;&#25317;&#26377;&#30456;&#20284;&#25968;&#25454;&#30340;&#23458;&#25143;&#31471;&#36827;&#34892;&#26356;&#22810;&#21327;&#20316;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#20998;&#25903;&#32467;&#26500;&#30340;&#26032;&#22411;PFL&#26041;&#27861;(pFedMB)&#65292;&#36890;&#36807;&#23558;&#31070;&#32463;&#32593;&#32476;&#30340;&#27599;&#20010;&#23618;&#20998;&#25104;&#22810;&#20010;&#20998;&#25903;&#24182;&#20026;&#27599;&#20010;&#20998;&#25903;&#20998;&#37197;&#23458;&#25143;&#31471;&#29305;&#23450;&#30340;&#26435;&#37325;&#26469;&#23454;&#29616;&#20010;&#24615;&#21270;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#31181;&#32858;&#21512;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;pFedMB&#30340;&#36890;&#20449;&#25928;&#29575;&#24182;&#38477;&#20302;&#35745;&#31639;&#24320;&#38144;&#12290;&#36890;&#36807;&#23545;&#21512;&#25104;&#25968;&#25454;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;pFedMB&#22312;&#20010;&#24615;&#21270;&#21644;&#36890;&#20449;&#25928;&#29575;&#26041;&#38754;&#37117;&#33021;&#22815;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a decentralized machine learning technique that enables multiple clients to collaboratively train models without requiring clients to reveal their raw data to each other. Although traditional FL trains a single global model with average performance among clients, statistical data heterogeneity across clients has resulted in the development of personalized FL (PFL), which trains personalized models with good performance on each client's data. A key challenge with PFL is how to facilitate clients with similar data to collaborate more in a situation where each client has data from complex distribution and cannot determine one another's distribution. In this paper, we propose a new PFL method (pFedMB) using multi-branch architecture, which achieves personalization by splitting each layer of a neural network into multiple branches and assigning client-specific weights to each branch. We also design an aggregation method to improve the communication efficiency and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#30456;&#23545;&#29109;&#27491;&#21017;&#21270;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#38382;&#39064;&#65292;&#20854;&#20013;&#21442;&#32771;&#24230;&#37327;&#20026;sigma&#26377;&#38480;&#27979;&#24230;&#65292;&#35299;&#20026;&#21807;&#19968;&#30340;&#27010;&#29575;&#27979;&#24230;&#24182;&#23637;&#29616;&#20102;&#20960;&#20046;&#27491;&#30830;&#30340;&#20445;&#35777;&#12290;ERM-RER&#38382;&#39064;&#30340;&#35299;&#34987;&#31216;&#20026;Gibbs&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2211.06617</link><description>&lt;p&gt;
&#30456;&#23545;&#29109;&#27491;&#21017;&#21270;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Empirical Risk Minimization with Relative Entropy Regularization. (arXiv:2211.06617v2 [math.ST] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.06617
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#30456;&#23545;&#29109;&#27491;&#21017;&#21270;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#38382;&#39064;&#65292;&#20854;&#20013;&#21442;&#32771;&#24230;&#37327;&#20026;sigma&#26377;&#38480;&#27979;&#24230;&#65292;&#35299;&#20026;&#21807;&#19968;&#30340;&#27010;&#29575;&#27979;&#24230;&#24182;&#23637;&#29616;&#20102;&#20960;&#20046;&#27491;&#30830;&#30340;&#20445;&#35777;&#12290;ERM-RER&#38382;&#39064;&#30340;&#35299;&#34987;&#31216;&#20026;Gibbs&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20551;&#23450;&#21442;&#32771;&#24230;&#37327;&#20026;sigma&#26377;&#38480;&#27979;&#24230;&#65288;measure&#65289;&#32780;&#38750;&#27010;&#29575;&#27979;&#24230;&#30340;&#24773;&#20917;&#19979;&#65292;&#30740;&#31350;&#20102;&#30456;&#23545;&#29109;&#27491;&#21017;&#21270;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;ERM-RER&#65289;&#38382;&#39064;&#12290;&#22312;&#36825;&#31181;&#20551;&#35774;&#19979;&#65292;&#23384;&#22312;&#19968;&#20010;ERM-RER&#38382;&#39064;&#30340;&#27867;&#21270;&#65292;&#20801;&#35768;&#26356;&#22823;&#31243;&#24230;&#22320;&#28789;&#27963;&#22320;&#24182;&#20837;&#20808;&#39564;&#30693;&#35782;&#12290;&#22312;&#36825;&#20123;&#24615;&#36136;&#20013;&#65292;&#22914;&#26524;&#23384;&#22312;ERM-RER&#38382;&#39064;&#30340;&#35299;&#65292;&#21017;&#35813;&#35299;&#26159;&#21807;&#19968;&#30340;&#27010;&#29575;&#27979;&#24230;&#65292;&#36890;&#24120;&#19982;&#21442;&#32771;&#27979;&#24230;&#30456;&#20114;&#32477;&#23545;&#36830;&#32493;&#12290;&#36825;&#26679;&#30340;&#35299;&#23545;&#20110;ERM&#38382;&#39064;&#23637;&#29616;&#20102;&#20960;&#20046;&#27491;&#30830;&#30340;&#20445;&#35777;&#65292;&#32780;&#19981;&#38656;&#20851;&#24515;ERM&#38382;&#39064;&#26159;&#21542;&#26377;&#35299;&#12290;&#24403;&#20174;ERM-RER&#38382;&#39064;&#30340;&#35299;&#25277;&#21462;&#27169;&#22411;&#26102;&#65292;&#22266;&#23450;&#25968;&#25454;&#38598;&#26102;&#65292;&#32463;&#39564;&#39118;&#38505;&#34987;&#35777;&#26126;&#26159;&#19968;&#20010;&#20122;&#39640;&#26031;&#38543;&#26426;&#21464;&#37327;&#12290;ERM-RER&#38382;&#39064;&#30340;&#35299;&#65288;Gibbs&#31639;&#27861;&#65289;&#30340;&#27867;&#21270;&#33021;&#21147;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
The empirical risk minimization (ERM) problem with relative entropy regularization (ERM-RER) is investigated under the assumption that the reference measure is a {\sigma}-finite measure, and not necessarily a probability measure. Under this assumption, which leads to a generalization of the ERM-RER problem allowing a larger degree of flexibility for incorporating prior knowledge, numerous relevant properties are stated. Among these properties, the solution to this problem, if it exists, is shown to be a unique probability measure, often mutually absolutely continuous with the reference measure. Such a solution exhibits a probably-approximately-correct guarantee for the ERM problem independently of whether the latter possesses a solution. For a fixed dataset, the empirical risk is shown to be a sub-Gaussian random variable when the models are sampled from the solution to the ERM-RER problem. The generalization capabilities of the solution to the ERM-RER problem (the Gibbs algorithm) are
&lt;/p&gt;</description></item><item><title>Laser &#26159;&#19968;&#31181;&#22522;&#20110;&#28508;&#22312;&#20195;&#29702;&#34920;&#31034;&#23398;&#20064;&#30340;&#20272;&#35745;&#38271;&#26399;&#22240;&#26524;&#25928;&#24212;&#30340;&#28789;&#27963;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#20195;&#29702;&#21644;&#20854;&#20195;&#29702;&#28151;&#21512;&#22312;&#19968;&#36215;&#30340;&#30495;&#23454;&#19990;&#30028;&#24773;&#26223;&#20013;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2208.04589</link><description>&lt;p&gt;
&#22522;&#20110;&#28508;&#22312;&#20195;&#29702;&#34920;&#31034;&#23398;&#20064;&#30340;&#38271;&#26399;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Long-term Causal Effects Estimation via Latent Surrogates Representation Learning. (arXiv:2208.04589v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.04589
&lt;/p&gt;
&lt;p&gt;
Laser &#26159;&#19968;&#31181;&#22522;&#20110;&#28508;&#22312;&#20195;&#29702;&#34920;&#31034;&#23398;&#20064;&#30340;&#20272;&#35745;&#38271;&#26399;&#22240;&#26524;&#25928;&#24212;&#30340;&#28789;&#27963;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#20195;&#29702;&#21644;&#20854;&#20195;&#29702;&#28151;&#21512;&#22312;&#19968;&#36215;&#30340;&#30495;&#23454;&#19990;&#30028;&#24773;&#26223;&#20013;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#65292;&#22914;&#33829;&#38144;&#21644;&#21307;&#23398;&#20013;&#65292;&#22522;&#20110;&#30701;&#26399;&#20195;&#29702;&#26469;&#20272;&#35745;&#38271;&#26399;&#22240;&#26524;&#25928;&#24212;&#26159;&#19968;&#20010;&#37325;&#35201;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#23613;&#31649;&#26576;&#20123;&#39046;&#22495;&#20013;&#24050;&#26377;&#25152;&#25104;&#21151;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20197;&#19968;&#31181;&#29702;&#24819;&#21270;&#21644;&#31616;&#21270;&#30340;&#26041;&#24335;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#65292;&#24573;&#30053;&#20102;&#30701;&#26399;&#32467;&#26524;&#20043;&#38388;&#30340;&#22240;&#26524;&#32467;&#26500;&#65292;&#24182;&#23558;&#23427;&#20204;&#20840;&#37096;&#35270;&#20026;&#20195;&#29702;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#26080;&#27861;&#24456;&#22909;&#22320;&#24212;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#30340;&#24773;&#26223;&#65292;&#20854;&#20013;&#23616;&#37096;&#35266;&#23519;&#21040;&#30340;&#20195;&#29702;&#19982;&#23427;&#20204;&#22312;&#30701;&#26399;&#32467;&#26524;&#20013;&#30340;&#20195;&#29702;&#28151;&#21512;&#22312;&#19968;&#36215;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#28789;&#27963;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;Laser&#65292;&#20197;&#22312;&#26356;&#29616;&#23454;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;&#38271;&#26399;&#22240;&#26524;&#25928;&#24212;&#65292;&#20854;&#20013;&#35266;&#23519;&#21040;&#20195;&#29702;&#25110;&#20855;&#26377;&#35266;&#23519;&#20195;&#29702;&#12290;&#37492;&#20110;&#20195;&#29702;&#21644;&#20195;&#29702;&#20043;&#38388;&#30340;&#19981;&#21487;&#21306;&#20998;&#24615;&#65292;&#25105;&#20204;&#21033;&#29992;&#21487;&#35782;&#21035;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;iVAE&#65289;&#22312;&#19981;&#38656;&#35201;&#21306;&#20998;&#35266;&#23519;&#21040;&#30340;&#20195;&#29702;&#25110;&#20808;&#39564;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#24674;&#22797;&#25152;&#26377;&#26377;&#25928;&#20195;&#29702;&#20505;&#36873;&#32773;&#19978;&#30340;&#25972;&#20010;&#26377;&#25928;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating long-term causal effects based on short-term surrogates is a significant but challenging problem in many real-world applications, e.g., marketing and medicine. Despite its success in certain domains, most existing methods estimate causal effects in an idealistic and simplistic way - ignoring the causal structure among short-term outcomes and treating all of them as surrogates. However, such methods cannot be well applied to real-world scenarios, in which the partially observed surrogates are mixed with their proxies among short-term outcomes. To this end, we develop our flexible method, Laser, to estimate long-term causal effects in the more realistic situation that the surrogates are observed or have observed proxies.Given the indistinguishability between the surrogates and proxies, we utilize identifiable variational auto-encoder (iVAE) to recover the whole valid surrogates on all the surrogates candidates without the need of distinguishing the observed surrogates or the p
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21387;&#32553;&#20613;&#37324;&#21494;&#33394;&#25955;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#23450;&#20041;&#22312;&#39640;&#32500;&#21608;&#26399;&#36793;&#30028;&#26465;&#20214;&#22495;&#19978;&#30340;&#25193;&#25955;&#26041;&#31243;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#21387;&#32553;&#24863;&#30693;&#21644;&#31232;&#30095;&#24674;&#22797;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#33945;&#29305;&#21345;&#32599;&#37319;&#26679;&#19978;&#36817;&#20284;&#35299;&#30340;&#20613;&#37324;&#21494;&#31995;&#25968;&#65292;&#26377;&#25928;&#22320;&#20811;&#26381;&#20102;&#32500;&#24230;&#35781;&#21650;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2206.01255</link><description>&lt;p&gt;
&#21387;&#32553;&#20613;&#37324;&#21494;&#33394;&#25955;&#26041;&#27861;&#29992;&#20110;&#20855;&#26377;&#21608;&#26399;&#36793;&#30028;&#26465;&#20214;&#30340;&#39640;&#32500;&#25193;&#25955;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Compressive Fourier collocation methods for high-dimensional diffusion equations with periodic boundary conditions. (arXiv:2206.01255v3 [math.NA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.01255
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21387;&#32553;&#20613;&#37324;&#21494;&#33394;&#25955;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#23450;&#20041;&#22312;&#39640;&#32500;&#21608;&#26399;&#36793;&#30028;&#26465;&#20214;&#22495;&#19978;&#30340;&#25193;&#25955;&#26041;&#31243;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#21387;&#32553;&#24863;&#30693;&#21644;&#31232;&#30095;&#24674;&#22797;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#33945;&#29305;&#21345;&#32599;&#37319;&#26679;&#19978;&#36817;&#20284;&#35299;&#30340;&#20613;&#37324;&#21494;&#31995;&#25968;&#65292;&#26377;&#25928;&#22320;&#20811;&#26381;&#20102;&#32500;&#24230;&#35781;&#21650;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#32500;&#20559;&#24494;&#20998;&#26041;&#31243;&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#25968;&#23398;&#24314;&#27169;&#24037;&#20855;&#65292;&#24212;&#29992;&#33539;&#22260;&#20174;&#37329;&#34701;&#21040;&#35745;&#31639;&#21270;&#23398;&#12290;&#28982;&#32780;&#65292;&#35299;&#20915;&#36825;&#20123;&#26041;&#31243;&#30340;&#26631;&#20934;&#25968;&#20540;&#25216;&#26415;&#36890;&#24120;&#21463;&#21040;&#32500;&#24230;&#35781;&#21650;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#36825;&#20010;&#25361;&#25112;&#65292;&#37325;&#28857;&#20851;&#27880;&#23450;&#20041;&#22312;&#39640;&#32500;&#22495;&#19978;&#20855;&#26377;&#21608;&#26399;&#36793;&#30028;&#26465;&#20214;&#30340;&#23450;&#24120;&#25193;&#25955;&#26041;&#31243;&#12290;&#21463;&#39640;&#32500;&#31232;&#30095;&#20989;&#25968;&#36924;&#36817;&#30340;&#26368;&#26032;&#36827;&#23637;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#21387;&#32553;&#20613;&#37324;&#21494;&#33394;&#25955;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#21387;&#32553;&#24863;&#30693;&#21644;&#35889;&#33394;&#25955;&#30340;&#24605;&#24819;&#65292;&#29992;&#33945;&#29305;&#21345;&#32599;&#25277;&#26679;&#20195;&#26367;&#20102;&#32467;&#26500;&#21270;&#33394;&#25955;&#32593;&#26684;&#30340;&#20351;&#29992;&#65292;&#24182;&#20351;&#29992;&#31232;&#30095;&#24674;&#22797;&#25216;&#26415;&#65288;&#22914;&#27491;&#20132;&#21305;&#37197;&#36861;&#36394;&#21644;&#8467;^1&#26368;&#23567;&#21270;&#65289;&#26469;&#36817;&#20284;PDE&#35299;&#30340;&#20613;&#37324;&#21494;&#31995;&#25968;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#36924;&#36817;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-dimensional Partial Differential Equations (PDEs) are a popular mathematical modelling tool, with applications ranging from finance to computational chemistry. However, standard numerical techniques for solving these PDEs are typically affected by the curse of dimensionality. In this work, we tackle this challenge while focusing on stationary diffusion equations defined over a high-dimensional domain with periodic boundary conditions. Inspired by recent progress in sparse function approximation in high dimensions, we propose a new method called compressive Fourier collocation. Combining ideas from compressive sensing and spectral collocation, our method replaces the use of structured collocation grids with Monte Carlo sampling and employs sparse recovery techniques, such as orthogonal matching pursuit and $\ell^1$ minimization, to approximate the Fourier coefficients of the PDE solution. We conduct a rigorous theoretical analysis showing that the approximation error of the propose
&lt;/p&gt;</description></item><item><title>Relphormer&#26159;&#19968;&#31181;&#26032;&#30340;Transformer&#21464;&#20307;&#65292;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#12290;&#23427;&#24341;&#20837;&#20102;Triple2Seq&#21644;&#22686;&#24378;&#24335;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#65292;&#20197;&#35299;&#20915;&#22522;&#26412;Transformer&#26550;&#26500;&#22312;&#25429;&#25417;&#30693;&#35782;&#22270;&#35889;&#32467;&#26500;&#21644;&#35821;&#20041;&#20449;&#24687;&#26041;&#38754;&#30340;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2205.10852</link><description>&lt;p&gt;
Relphormer&#65306;&#20851;&#31995;&#22270;&#36716;&#25442;&#22120;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Relphormer: Relational Graph Transformer for Knowledge Graph Representations. (arXiv:2205.10852v5 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.10852
&lt;/p&gt;
&lt;p&gt;
Relphormer&#26159;&#19968;&#31181;&#26032;&#30340;Transformer&#21464;&#20307;&#65292;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#12290;&#23427;&#24341;&#20837;&#20102;Triple2Seq&#21644;&#22686;&#24378;&#24335;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#65292;&#20197;&#35299;&#20915;&#22522;&#26412;Transformer&#26550;&#26500;&#22312;&#25429;&#25417;&#30693;&#35782;&#22270;&#35889;&#32467;&#26500;&#21644;&#35821;&#20041;&#20449;&#24687;&#26041;&#38754;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#24050;&#32463;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#22270;&#24418;&#25366;&#25496;&#31561;&#24191;&#27867;&#39046;&#22495;&#20013;&#21462;&#24471;&#20102;remarkable&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22522;&#26412;&#30340;Transformer&#26550;&#26500;&#22312;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#34920;&#31034;&#20013;&#24182;&#27809;&#26377;&#21462;&#24471;&#24456;&#22909;&#30340;&#25913;&#36827;&#65292;&#20854;&#20013;&#24179;&#31227;&#36317;&#31163;&#27169;&#22411;&#25903;&#37197;&#20102;&#36825;&#20010;&#39046;&#22495;&#12290;&#38656;&#27880;&#24847;&#30340;&#26159;&#65292;&#22522;&#26412;&#30340;Transformer&#26550;&#26500;&#38590;&#20197;&#25429;&#25417;&#21040;&#30693;&#35782;&#22270;&#35889;&#30340;&#20869;&#22312;&#24322;&#26500;&#32467;&#26500;&#21644;&#35821;&#20041;&#20449;&#24687;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#30340;Transformer&#21464;&#20307;&#65292;&#21517;&#20026;Relphormer&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Triple2Seq&#65292;&#21487;&#20197;&#21160;&#24577;&#22320;&#37319;&#26679;&#19978;&#19979;&#25991;&#21270;&#30340;&#23376;&#22270;&#24207;&#21015;&#20316;&#20026;&#36755;&#20837;&#65292;&#20197;&#32531;&#35299;&#24322;&#26500;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22686;&#24378;&#24335;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#65292;&#29992;&#20110;&#23545;&#20851;&#31995;&#20449;&#24687;&#36827;&#34892;&#32534;&#30721;&#65292;&#24182;&#20445;&#25345;&#23454;&#20307;&#21644;&#20851;&#31995;&#20869;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#25513;&#34109;&#24335;&#30693;&#35782;&#24314;&#27169;&#26469;&#23454;&#29616;&#36890;&#29992;&#30340;&#30693;&#35782;&#22270;&#24418;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers have achieved remarkable performance in widespread fields, including natural language processing, computer vision and graph mining. However, vanilla Transformer architectures have not yielded promising improvements in the Knowledge Graph (KG) representations, where the translational distance paradigm dominates this area. Note that vanilla Transformer architectures struggle to capture the intrinsically heterogeneous structural and semantic information of knowledge graphs. To this end, we propose a new variant of Transformer for knowledge graph representations dubbed Relphormer. Specifically, we introduce Triple2Seq which can dynamically sample contextualized sub-graph sequences as the input to alleviate the heterogeneity issue. We propose a novel structure-enhanced self-attention mechanism to encode the relational information and keep the semantic information within entities and relations. Moreover, we utilize masked knowledge modeling for general knowledge graph representa
&lt;/p&gt;</description></item></channel></rss>