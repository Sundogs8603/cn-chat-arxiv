<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>TeleMoMa &#26159;&#19968;&#31181;&#38754;&#21521;&#31227;&#21160;&#25805;&#20316;&#30340;&#27169;&#22359;&#21270;&#22810;&#21151;&#33021;&#36828;&#31243;&#25805;&#20316;&#31995;&#32479;&#65292;&#36890;&#36807;&#25972;&#21512;&#22810;&#31181;&#20154;&#26426;&#25509;&#21475;&#12289;&#38477;&#20302;&#38376;&#27099;&#19988;&#20855;&#26377;&#36890;&#29992;&#24615;&#65292;&#20026;&#31227;&#21160;&#25805;&#20316;&#22120;&#25552;&#20379;&#20102;&#20840;&#36523;&#36828;&#31243;&#25805;&#20316;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.07869</link><description>&lt;p&gt;
TeleMoMa&#65306;&#19968;&#31181;&#29992;&#20110;&#31227;&#21160;&#25805;&#20316;&#30340;&#27169;&#22359;&#21270;&#22810;&#21151;&#33021;&#36828;&#31243;&#25805;&#20316;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
TeleMoMa: A Modular and Versatile Teleoperation System for Mobile Manipulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07869
&lt;/p&gt;
&lt;p&gt;
TeleMoMa &#26159;&#19968;&#31181;&#38754;&#21521;&#31227;&#21160;&#25805;&#20316;&#30340;&#27169;&#22359;&#21270;&#22810;&#21151;&#33021;&#36828;&#31243;&#25805;&#20316;&#31995;&#32479;&#65292;&#36890;&#36807;&#25972;&#21512;&#22810;&#31181;&#20154;&#26426;&#25509;&#21475;&#12289;&#38477;&#20302;&#38376;&#27099;&#19988;&#20855;&#26377;&#36890;&#29992;&#24615;&#65292;&#20026;&#31227;&#21160;&#25805;&#20316;&#22120;&#25552;&#20379;&#20102;&#20840;&#36523;&#36828;&#31243;&#25805;&#20316;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#23398;&#20013;&#38480;&#21046;&#27169;&#20223;&#23398;&#20064;&#30340;&#20851;&#38190;&#29942;&#39048;&#26159;&#25968;&#25454;&#30340;&#21294;&#20047;&#12290;&#36825;&#20010;&#38382;&#39064;&#22312;&#31227;&#21160;&#25805;&#20316;&#20013;&#26356;&#20026;&#20005;&#37325;&#65292;&#22240;&#20026;&#19982;&#38745;&#27490;&#25805;&#20316;&#30456;&#27604;&#65292;&#30001;&#20110;&#32570;&#20047;&#21487;&#29992;&#19988;&#26131;&#20110;&#20351;&#29992;&#30340;&#36828;&#31243;&#25805;&#20316;&#30028;&#38754;&#65292;&#25910;&#38598;&#28436;&#31034;&#26356;&#21152;&#22256;&#38590;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;TeleMoMa&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#20840;&#36523;&#36828;&#31243;&#25805;&#20316;&#31227;&#21160;&#25805;&#20316;&#22120;&#30340;&#36890;&#29992;&#21644;&#27169;&#22359;&#21270;&#30028;&#38754;&#12290;TeleMoMa&#23558;&#21253;&#25324;RGB&#21644;&#28145;&#24230;&#25668;&#20687;&#22836;&#12289;&#34394;&#25311;&#29616;&#23454;&#25511;&#21046;&#22120;&#12289;&#38190;&#30424;&#12289;&#25805;&#32437;&#26438;&#31561;&#22810;&#20010;&#20154;&#26426;&#25509;&#21475;&#25972;&#21512;&#22312;&#19968;&#36215;&#65292;&#20197;&#21450;&#36825;&#20123;&#25509;&#21475;&#30340;&#20219;&#20309;&#32452;&#21512;&#12290;&#22312;&#20854;&#26356;&#26131;&#35775;&#38382;&#30340;&#29256;&#26412;&#20013;&#65292; TeleMoMa&#21487;&#20197;&#20165;&#20351;&#29992;&#35270;&#35273;&#65288;&#22914;RGB-D&#30456;&#26426;&#65289;&#21363;&#21487;&#24037;&#20316;&#65292;&#38477;&#20302;&#20102;&#20154;&#31867;&#25552;&#20379;&#31227;&#21160;&#25805;&#20316;&#28436;&#31034;&#30340;&#38376;&#27099;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#27169;&#25311;&#29615;&#22659;&#21644;&#29616;&#23454;&#19990;&#30028;&#20013;&#36828;&#31243;&#25805;&#20316;&#20960;&#20010;&#29616;&#26377;&#30340;&#31227;&#21160;&#25805;&#20316;&#22120;&#8212;&#8212;PAL Tiago++, Toyota HSR&#21644;Fetch&#26469;&#23637;&#29616;TeleMoMa&#30340;&#22810;&#21151;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07869v1 Announce Type: cross  Abstract: A critical bottleneck limiting imitation learning in robotics is the lack of data. This problem is more severe in mobile manipulation, where collecting demonstrations is harder than in stationary manipulation due to the lack of available and easy-to-use teleoperation interfaces. In this work, we demonstrate TeleMoMa, a general and modular interface for whole-body teleoperation of mobile manipulators. TeleMoMa unifies multiple human interfaces including RGB and depth cameras, virtual reality controllers, keyboard, joysticks, etc., and any combination thereof. In its more accessible version, TeleMoMa works using simply vision (e.g., an RGB-D camera), lowering the entry bar for humans to provide mobile manipulation demonstrations. We demonstrate the versatility of TeleMoMa by teleoperating several existing mobile manipulators - PAL Tiago++, Toyota HSR, and Fetch - in simulation and the real world. We demonstrate the quality of the demonst
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;CodeAttack&#26694;&#26550;&#29992;&#20110;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#27867;&#21270;&#65292;&#30740;&#31350;&#21457;&#29616;GPT-4&#12289;Claude-2&#21644;Llama-2&#31995;&#21015;&#31561;&#26368;&#26032;&#27169;&#22411;&#23384;&#22312;&#20195;&#30721;&#36755;&#20837;&#30340;&#23433;&#20840;&#28431;&#27934;&#12290;</title><link>https://arxiv.org/abs/2403.07865</link><description>&lt;p&gt;
&#36890;&#36807;&#20195;&#30721;&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#27867;&#21270;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Exploring Safety Generalization Challenges of Large Language Models via Code
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;CodeAttack&#26694;&#26550;&#29992;&#20110;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#27867;&#21270;&#65292;&#30740;&#31350;&#21457;&#29616;GPT-4&#12289;Claude-2&#21644;Llama-2&#31995;&#21015;&#31561;&#26368;&#26032;&#27169;&#22411;&#23384;&#22312;&#20195;&#30721;&#36755;&#20837;&#30340;&#23433;&#20840;&#28431;&#27934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#24102;&#26469;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#30340;&#26174;&#33879;&#33021;&#21147;&#65292;&#20294;&#20063;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#23427;&#20204;&#28508;&#22312;&#35823;&#29992;&#30340;&#25285;&#24551;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;CodeAttack&#65292;&#19968;&#20010;&#23558;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#36716;&#25442;&#20026;&#20195;&#30721;&#36755;&#20837;&#30340;&#26694;&#26550;&#65292;&#20026;&#27979;&#35797;LLMs&#30340;&#23433;&#20840;&#27867;&#21270;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#29615;&#22659;&#12290;&#25105;&#20204;&#23545;&#21253;&#25324;GPT-4&#12289;Claude-2&#21644;Llama-2&#31995;&#21015;&#22312;&#20869;&#30340;&#26368;&#26032;LLMs&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#20195;&#30721;&#36755;&#20837;&#23384;&#22312;&#20849;&#21516;&#30340;&#23433;&#20840;&#28431;&#27934;&#65306;CodeAttack&#22312;&#36229;&#36807;80%&#30340;&#26102;&#38388;&#20869;&#22987;&#32456;&#32469;&#36807;&#25152;&#26377;&#27169;&#22411;&#30340;&#23433;&#20840;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07865v1 Announce Type: cross  Abstract: The rapid advancement of Large Language Models (LLMs) has brought about remarkable capabilities in natural language processing but also raised concerns about their potential misuse. While strategies like supervised fine-tuning and reinforcement learning from human feedback have enhanced their safety, these methods primarily focus on natural languages, which may not generalize to other domains. This paper introduces CodeAttack, a framework that transforms natural language inputs into code inputs, presenting a novel environment for testing the safety generalization of LLMs. Our comprehensive studies on state-of-the-art LLMs including GPT-4, Claude-2, and Llama-2 series reveal a common safety vulnerability of these models against code input: CodeAttack consistently bypasses the safety guardrails of all models more than 80\% of the time. Furthermore, we find that a larger distribution gap between CodeAttack and natural language leads to we
&lt;/p&gt;</description></item><item><title>&#27169;&#22411;&#35825;&#23548;&#30340;&#20998;&#24067;&#36716;&#31227;&#21487;&#33021;&#23548;&#33268;&#24615;&#33021;&#12289;&#20844;&#24179;&#24615;&#21644;&#36793;&#32536;&#32676;&#20307;&#34920;&#29616;&#30340;&#25439;&#22833;&#65292;&#25552;&#20986;&#20102;&#31639;&#27861;&#20462;&#22797;(AR)&#26694;&#26550;&#20197;&#36890;&#36807;&#31215;&#26497;&#24178;&#39044;&#23454;&#29616;&#23545;&#21382;&#21490;&#27495;&#35270;&#30340;&#34917;&#25937;</title><link>https://arxiv.org/abs/2403.07857</link><description>&lt;p&gt;
&#20844;&#24179;&#21453;&#39304;&#24490;&#29615;&#65306;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#35757;&#32451;&#20250;&#25918;&#22823;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Fairness Feedback Loops: Training on Synthetic Data Amplifies Bias
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07857
&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#35825;&#23548;&#30340;&#20998;&#24067;&#36716;&#31227;&#21487;&#33021;&#23548;&#33268;&#24615;&#33021;&#12289;&#20844;&#24179;&#24615;&#21644;&#36793;&#32536;&#32676;&#20307;&#34920;&#29616;&#30340;&#25439;&#22833;&#65292;&#25552;&#20986;&#20102;&#31639;&#27861;&#20462;&#22797;(AR)&#26694;&#26550;&#20197;&#36890;&#36807;&#31215;&#26497;&#24178;&#39044;&#23454;&#29616;&#23545;&#21382;&#21490;&#27495;&#35270;&#30340;&#34917;&#25937;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#35825;&#23548;&#30340;&#20998;&#24067;&#36716;&#31227;(MIDS)&#20250;&#23548;&#33268;&#20808;&#21069;&#27169;&#22411;&#36755;&#20986;&#27745;&#26579;&#26032;&#27169;&#22411;&#35757;&#32451;&#38598;&#65292;&#38543;&#30528;&#27169;&#22411;&#30340;&#28436;&#21464;&#12290;&#23545;&#20110;&#29983;&#25104;&#27169;&#22411;&#26469;&#35828;&#65292;&#36825;&#34987;&#31216;&#20026;&#27169;&#22411;&#23849;&#28291;&#65292;&#23545;&#20110;&#30417;&#30563;&#27169;&#22411;&#26469;&#35828;&#21017;&#26159;&#34920;&#29616;&#39044;&#27979;&#25110;&#19981;&#20844;&#24179;&#21453;&#39304;&#24490;&#29615;&#12290;&#24403;&#19968;&#20010;&#27169;&#22411;&#35825;&#23548;&#20102;&#20998;&#24067;&#30340;&#36716;&#31227;&#65292;&#23427;&#20063;&#23558;&#20854;&#38169;&#35823;&#12289;&#20559;&#35265;&#21644;&#19981;&#20844;&#24179;&#24615;&#32534;&#30721;&#21040;&#25968;&#25454;&#29983;&#24577;&#31995;&#32479;&#30340;&#22522;&#26412;&#20107;&#23454;&#20013;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#36319;&#36394;&#22810;&#20010;MIDS&#38271;&#26102;&#38388;&#30340;&#28436;&#21464;&#65292;&#21457;&#29616;&#23427;&#20204;&#21487;&#33021;&#23548;&#33268;&#24615;&#33021;&#12289;&#20844;&#24179;&#24615;&#21644;&#36793;&#32536;&#32676;&#20307;&#34920;&#29616;&#30340;&#25439;&#22833;&#65292;&#21363;&#20351;&#22312;&#26368;&#21021;&#26159;&#26080;&#20559;&#30340;&#25968;&#25454;&#38598;&#20013;&#20063;&#26159;&#22914;&#27492;&#12290;&#23613;&#31649;&#23384;&#22312;&#36825;&#20123;&#36127;&#38754;&#21518;&#26524;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#27169;&#22411;&#22914;&#20309;&#21487;&#20197;&#29992;&#20110;&#31215;&#26497;&#12289;&#26377;&#24847;&#30340;&#24178;&#39044;&#20854;&#25968;&#25454;&#29983;&#24577;&#31995;&#32479;&#65292;&#36890;&#36807;&#19968;&#31181;&#21517;&#20026;&#31639;&#27861;&#20462;&#22797;(AR)&#30340;&#26694;&#26550;&#20026;&#21382;&#21490;&#19978;&#30340;&#27495;&#35270;&#25552;&#20379;&#34917;&#25937;&#12290;&#25105;&#20204;&#36890;&#36807;&#31574;&#21010;&#20195;&#34920;&#24615;&#22521;&#35757;&#26469;&#27169;&#25311;AR&#24178;&#39044;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07857v1 Announce Type: new  Abstract: Model-induced distribution shifts (MIDS) occur as previous model outputs pollute new model training sets over generations of models. This is known as model collapse in the case of generative models, and performative prediction or unfairness feedback loops for supervised models. When a model induces a distribution shift, it also encodes its mistakes, biases, and unfairnesses into the ground truth of its data ecosystem. We introduce a framework that allows us to track multiple MIDS over many generations, finding that they can lead to loss in performance, fairness, and minoritized group representation, even in initially unbiased datasets. Despite these negative consequences, we identify how models might be used for positive, intentional, interventions in their data ecosystems, providing redress for historical discrimination through a framework called algorithmic reparation (AR). We simulate AR interventions by curating representative traini
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#37327;&#23376;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;QSVM&#65289;&#22312;&#21069;&#21015;&#33146;&#30284;&#26816;&#27979;&#20013;&#23637;&#29616;&#20986;&#26174;&#33879;&#30340;&#35786;&#26029;&#24615;&#33021;&#25552;&#21319;&#65292;&#30456;&#36739;&#20110;&#32463;&#20856;SVM&#26041;&#27861;&#65292;&#24182;&#19988;&#25104;&#21151;&#21019;&#24314;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#23454;&#29616;&#20102;&#23545;&#25968;&#25454;&#20013;&#22797;&#26434;&#38750;&#32447;&#24615;&#27169;&#24335;&#30340;&#26816;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.07856</link><description>&lt;p&gt;
&#37327;&#23376;&#25903;&#25345;&#21521;&#37327;&#26426;&#29992;&#20110;&#21069;&#21015;&#33146;&#30284;&#26816;&#27979;&#65306;&#24615;&#33021;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Quantum Support Vector Machine for Prostate Cancer Detection: A Performance Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07856
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#37327;&#23376;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;QSVM&#65289;&#22312;&#21069;&#21015;&#33146;&#30284;&#26816;&#27979;&#20013;&#23637;&#29616;&#20986;&#26174;&#33879;&#30340;&#35786;&#26029;&#24615;&#33021;&#25552;&#21319;&#65292;&#30456;&#36739;&#20110;&#32463;&#20856;SVM&#26041;&#27861;&#65292;&#24182;&#19988;&#25104;&#21151;&#21019;&#24314;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#23454;&#29616;&#20102;&#23545;&#25968;&#25454;&#20013;&#22797;&#26434;&#38750;&#32447;&#24615;&#27169;&#24335;&#30340;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#20808;&#36827;&#25216;&#26415;&#35299;&#20915;&#20102;&#25913;&#36827;&#21069;&#21015;&#33146;&#30284;&#26816;&#27979;&#26041;&#27861;&#30340;&#36843;&#20999;&#38656;&#27714;&#12290;&#25105;&#20204;&#23558;&#37327;&#23376;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;QSVM&#65289;&#24212;&#29992;&#20110;&#36825;&#19968;&#37325;&#35201;&#30340;&#21307;&#30103;&#25361;&#25112;&#65292;&#23637;&#31034;&#20102;&#30456;&#27604;&#32463;&#20856;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#26041;&#27861;&#22312;&#35786;&#26029;&#24615;&#33021;&#19978;&#30340;&#25552;&#21319;&#12290;&#30740;&#31350;&#19981;&#20165;&#27010;&#36848;&#20102;QSVM&#30456;&#36739;&#20110;&#32463;&#20856;SVM&#25216;&#26415;&#22312;&#35786;&#26029;&#24615;&#33021;&#19978;&#30340;&#26174;&#33879;&#25913;&#36827;&#65292;&#32780;&#19988;&#28145;&#20837;&#25506;&#35752;&#20102;&#30001;&#37327;&#23376;&#29305;&#24449;&#26144;&#23556;&#26550;&#26500;&#24102;&#26469;&#30340;&#36827;&#27493;&#65292;&#35813;&#26550;&#26500;&#32463;&#36807;&#35748;&#30495;&#35782;&#21035;&#21644;&#35780;&#20272;&#65292;&#30830;&#20445;&#20854;&#19982;&#25105;&#20204;&#30340;&#21069;&#21015;&#33146;&#30284;&#25968;&#25454;&#38598;&#30340;&#29420;&#29305;&#29305;&#24449;&#23436;&#32654;&#22865;&#21512;&#12290;&#36825;&#31181;&#26550;&#26500;&#25104;&#21151;&#21019;&#24314;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#23454;&#29616;&#20102;&#23545;&#25968;&#25454;&#20013;&#22797;&#26434;&#38750;&#32447;&#24615;&#27169;&#24335;&#30340;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07856v1 Announce Type: new  Abstract: This study addresses the urgent need for improved prostate cancer detection methods by harnessing the power of advanced technological solutions. We introduce the application of Quantum Support Vector Machine (QSVM) to this critical healthcare challenge, showcasing an enhancement in diagnostic performance over the classical Support Vector Machine (SVM) approach. Our study not only outlines the remarkable improvements in diagnostic performance made by QSVM over the classic SVM technique, but it delves into the advancements brought about by the quantum feature map architecture, which has been carefully identified and evaluated, ensuring it aligns seamlessly with the unique characteristics of our prostate cancer dataset. This architecture succeded in creating a distinct feature space, enabling the detection of complex, non-linear patterns in the data. The findings reveal not only a comparable accuracy with classical SVM ($92\%$) but also a $
&lt;/p&gt;</description></item><item><title>&#22312;&#25968;&#25454;&#21098;&#26525;&#20013;&#24341;&#20837;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#39044;&#20808;&#35757;&#32451;&#30340;&#25945;&#24072;&#32593;&#32476;&#36719;&#39044;&#27979;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#12289;&#21098;&#26525;&#26041;&#27861;&#21644;&#25152;&#26377;&#21098;&#26525;&#20998;&#25968;&#19978;&#30340;&#26174;&#33879;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.07854</link><description>&lt;p&gt;
&#22312;&#25968;&#25454;&#21098;&#26525;&#20013;&#33976;&#39311;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Distilling the Knowledge in Data Pruning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07854
&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#25454;&#21098;&#26525;&#20013;&#24341;&#20837;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#39044;&#20808;&#35757;&#32451;&#30340;&#25945;&#24072;&#32593;&#32476;&#36719;&#39044;&#27979;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#12289;&#21098;&#26525;&#26041;&#27861;&#21644;&#25152;&#26377;&#21098;&#26525;&#20998;&#25968;&#19978;&#30340;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#35268;&#27169;&#19981;&#26029;&#22686;&#21152;&#65292;&#25968;&#25454;&#21098;&#26525;&#25104;&#20026;&#20102;&#19968;&#20010;&#26377;&#21560;&#24341;&#21147;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#24403;&#21069;&#30340;&#25968;&#25454;&#21098;&#26525;&#31639;&#27861;&#22312;&#20445;&#25345;&#20934;&#30830;&#24615;&#26041;&#38754;&#21463;&#21040;&#38480;&#21046;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#24230;&#21098;&#26525;&#30340;&#24773;&#20917;&#19979;&#19982;&#20351;&#29992;&#23436;&#25972;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#27604;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#35757;&#32451;&#22522;&#20110;&#21098;&#26525;&#23376;&#38598;&#30340;&#27169;&#22411;&#26102;&#65292;&#32467;&#21512;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#30340;&#24212;&#29992;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#25105;&#20204;&#19981;&#20165;&#20381;&#36182;&#20110;&#22320;&#38754;&#30495;&#23454;&#26631;&#31614;&#65292;&#36824;&#20351;&#29992;&#20102;&#24050;&#22312;&#23436;&#25972;&#25968;&#25454;&#19978;&#39044;&#20808;&#35757;&#32451;&#30340;&#32769;&#24072;&#32593;&#32476;&#30340;&#36719;&#39044;&#27979;&#12290;&#36890;&#36807;&#23558;&#30693;&#35782;&#33976;&#39311;&#25972;&#21512;&#21040;&#35757;&#32451;&#20013;&#65292;&#25105;&#20204;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#12289;&#21098;&#26525;&#26041;&#27861;&#21644;&#25152;&#26377;&#21098;&#26525;&#20998;&#25968;&#19978;&#37117;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#39318;&#20808;&#24314;&#31435;&#20102;&#37319;&#29992;&#33258;&#33976;&#39311;&#26469;&#25913;&#21892;&#22312;&#21098;&#26525;&#25968;&#25454;&#19978;&#30340;&#35757;&#32451;&#30340;&#29702;&#35770;&#21160;&#26426;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#23454;&#35777;&#19978;&#36827;&#34892;&#20102;&#24341;&#20154;&#27880;&#30446;&#19988;&#39640;&#24230;&#23454;&#29992;&#30340;&#35266;&#23519;&#65306;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#65292;&#31616;&#21333;&#30340;&#38543;&#26426;&#21098;&#26525;&#20063;&#20250;&#21462;&#24471;&#26174;&#30528;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07854v1 Announce Type: cross  Abstract: With the increasing size of datasets used for training neural networks, data pruning becomes an attractive field of research. However, most current data pruning algorithms are limited in their ability to preserve accuracy compared to models trained on the full data, especially in high pruning regimes. In this paper we explore the application of data pruning while incorporating knowledge distillation (KD) when training on a pruned subset. That is, rather than relying solely on ground-truth labels, we also use the soft predictions from a teacher network pre-trained on the complete data. By integrating KD into training, we demonstrate significant improvement across datasets, pruning methods, and on all pruning fractions. We first establish a theoretical motivation for employing self-distillation to improve training on pruned data. Then, we empirically make a compelling and highly practical observation: using KD, simple random pruning is c
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#36731;&#37327;&#32423;&#27169;&#22411;&#30340;&#22312;&#32447;&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#23398;&#20064;&#65288;O-FSCIL&#65289;&#65292;&#20351;&#29992;&#29305;&#24449;&#27491;&#20132;&#27491;&#21017;&#21270;&#21644;&#22810;&#36793;&#30028;&#25439;&#22833;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#20803;&#23398;&#20064;&#65292;&#22312;&#35774;&#22791;&#19978;&#23454;&#29616;&#27599;&#31867;12&#27627;&#28966;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.07851</link><description>&lt;p&gt;
&#35774;&#22791;&#19978;&#27599;&#31867;12&#27627;&#28966;&#30340;&#22312;&#32447;&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
12 mJ per Class On-Device Online Few-Shot Class-Incremental Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07851
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#36731;&#37327;&#32423;&#27169;&#22411;&#30340;&#22312;&#32447;&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#23398;&#20064;&#65288;O-FSCIL&#65289;&#65292;&#20351;&#29992;&#29305;&#24449;&#27491;&#20132;&#27491;&#21017;&#21270;&#21644;&#22810;&#36793;&#30028;&#25439;&#22833;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#20803;&#23398;&#20064;&#65292;&#22312;&#35774;&#22791;&#19978;&#23454;&#29616;&#27599;&#31867;12&#27627;&#28966;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#12298;&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#23398;&#20064;&#65288;FSCIL&#65289;&#12299;&#20351;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#20165;&#20351;&#29992;&#23569;&#37327;&#24050;&#26631;&#35760;&#30340;&#31034;&#20363;&#21363;&#21487;&#25193;&#23637;&#20854;&#23545;&#26032;&#31867;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#32780;&#19981;&#20250;&#24536;&#35760;&#20808;&#21069;&#23398;&#20064;&#36807;&#30340;&#31867;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#36731;&#37327;&#32423;&#27169;&#22411;&#30340;&#12298;&#22312;&#32447;&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#23398;&#20064;&#65288;O-FSCIL&#65289;&#12299;&#65292;&#35813;&#27169;&#22411;&#30001;&#39044;&#35757;&#32451;&#21644;&#20803;&#23398;&#20064;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#20197;&#21450;&#23384;&#20648;&#31867;&#21407;&#22411;&#30340;&#21487;&#25193;&#23637;&#26174;&#24335;&#20869;&#23384;&#32452;&#25104;&#12290;&#35813;&#26550;&#26500;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#29305;&#24449;&#27491;&#20132;&#27491;&#21017;&#21270;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;&#22810;&#36793;&#30028;&#25439;&#22833;&#36827;&#34892;&#20803;&#23398;&#20064;&#12290;&#23545;&#20110;&#23398;&#20064;&#26032;&#31867;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#26032;&#30340;&#31867;&#21407;&#22411;&#25193;&#23637;&#26174;&#24335;&#20869;&#23384;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#20313;&#26550;&#26500;&#20923;&#32467;&#12290;&#36825;&#26679;&#21487;&#20197;&#22522;&#20110;&#20165;&#20165;&#23569;&#37327;&#31034;&#20363;&#23398;&#20064;&#20197;&#21069;&#26410;&#35265;&#36807;&#30340;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07851v1 Announce Type: new  Abstract: Few-Shot Class-Incremental Learning (FSCIL) enables machine learning systems to expand their inference capabilities to new classes using only a few labeled examples, without forgetting the previously learned classes. Classical backpropagation-based learning and its variants are often unsuitable for battery-powered, memory-constrained systems at the extreme edge. In this work, we introduce Online Few-Shot Class-Incremental Learning (O-FSCIL), based on a lightweight model consisting of a pretrained and metalearned feature extractor and an expandable explicit memory storing the class prototypes. The architecture is pretrained with a novel feature orthogonality regularization and metalearned with a multi-margin loss. For learning a new class, our approach extends the explicit memory with novel class prototypes, while the remaining architecture is kept frozen. This allows learning previously unseen classes based on only a few examples with on
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35299;&#37322;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36845;&#20195;&#33258;&#25105;&#25913;&#36827;&#30340;&#31639;&#27861;EEGL&#65292;&#36890;&#36807;&#39057;&#32321;&#23376;&#22270;&#25366;&#25496;&#21644;&#36807;&#28388;&#26469;&#33719;&#21462;&#33410;&#28857;&#37051;&#22495;&#20013;&#29305;&#23450;&#23376;&#22270;&#30340;&#24212;&#29992;&#30456;&#20851;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2403.07849</link><description>&lt;p&gt;
&#36890;&#36807;&#39057;&#32321;&#23376;&#22270;&#25366;&#25496;&#22686;&#24378;&#30340;&#36845;&#20195;&#22270;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Iterative Graph Neural Network Enhancement via Frequent Subgraph Mining of Explanations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07849
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35299;&#37322;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36845;&#20195;&#33258;&#25105;&#25913;&#36827;&#30340;&#31639;&#27861;EEGL&#65292;&#36890;&#36807;&#39057;&#32321;&#23376;&#22270;&#25366;&#25496;&#21644;&#36807;&#28388;&#26469;&#33719;&#21462;&#33410;&#28857;&#37051;&#22495;&#20013;&#29305;&#23450;&#23376;&#22270;&#30340;&#24212;&#29992;&#30456;&#20851;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#30340;&#27169;&#22411;&#25913;&#36827;&#26041;&#27861;&#65292;&#29992;&#20110;&#33410;&#28857;&#20998;&#31867;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#65292;&#31216;&#20026;Explanation Enhanced Graph Learning (EEGL)&#12290;&#35813;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#35299;&#37322;&#26469;&#25552;&#39640;GNN&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;EEGL&#26159;&#19968;&#20010;&#36845;&#20195;&#33258;&#25105;&#25913;&#36827;&#30340;&#31639;&#27861;&#65292;&#20174;&#23398;&#20064;&#21040;&#30340;&#8220;&#21407;&#22987;&#8221;GNN&#24320;&#22987;&#65292;&#36890;&#36807;&#39057;&#32321;&#23376;&#22270;&#25366;&#25496;&#21453;&#22797;&#21457;&#29616;&#35299;&#37322;&#23376;&#22270;&#20013;&#30340;&#30456;&#20851;&#27169;&#24335;&#12290;&#36825;&#20123;&#27169;&#24335;&#36827;&#19968;&#27493;&#34987;&#36807;&#28388;&#20197;&#33719;&#21462;&#19982;&#33410;&#28857;&#37051;&#22495;&#20013;&#29305;&#23450;&#23376;&#22270;&#30340;&#23384;&#22312;&#23545;&#24212;&#30340;&#24212;&#29992;&#30456;&#20851;&#29305;&#24449;&#12290;&#20197;&#24212;&#29992;&#30456;&#20851;&#31639;&#27861;&#20026;&#20363;&#65292;&#23545;&#20110;Weisfeiler-Leman &#65288;1-WL&#65289;&#31639;&#27861;&#30340;&#23376;&#22270;&#25193;&#23637;&#27492;&#21069;&#34987;&#25552;&#20986;&#20026;&#19968;&#20010;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23454;&#39564;&#35777;&#25454;&#65292;&#20351;&#29992;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#65292;&#26174;&#31034;EEGL&#22312;&#39044;&#27979;&#24615;&#33021;&#19978;&#20248;&#20110;&#30456;&#20851;&#26041;&#27861;&#65292;&#24182;&#20855;&#26377;&#36229;&#36234;&#31616;&#21333;&#33410;&#28857;&#21306;&#20998;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07849v1 Announce Type: new  Abstract: We formulate an XAI-based model improvement approach for Graph Neural Networks (GNNs) for node classification, called Explanation Enhanced Graph Learning (EEGL). The goal is to improve predictive performance of GNN using explanations. EEGL is an iterative self-improving algorithm, which starts with a learned "vanilla" GNN, and repeatedly uses frequent subgraph mining to find relevant patterns in explanation subgraphs. These patterns are then filtered further to obtain application-dependent features corresponding to the presence of certain subgraphs in the node neighborhoods. Giving an application-dependent algorithm for such a subgraph-based extension of the Weisfeiler-Leman (1-WL) algorithm has previously been posed as an open problem. We present experimental evidence, with synthetic and real-world data, which show that EEGL outperforms related approaches in predictive performance and that it has a node-distinguishing power beyond that 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#37319;&#29992;&#20102;XGBoost&#21644;&#20462;&#25913;&#29256;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;B2B&#30005;&#23376;&#21830;&#21153;&#20013;&#20934;&#30830;&#39044;&#27979;&#20080;&#23478;&#35746;&#21333;&#19979;&#36798;&#34892;&#20026;&#30340;&#25361;&#25112;</title><link>https://arxiv.org/abs/2403.07843</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#32463;&#39564;&#36125;&#21494;&#26031;&#26041;&#27861;&#29992;&#20110;B2B&#30005;&#23376;&#21830;&#21153;&#20013;&#30340;&#39044;&#27979;&#36141;&#20080;
&lt;/p&gt;
&lt;p&gt;
A Machine learning and Empirical Bayesian Approach for Predictive Buying in B2B E-commerce
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07843
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#37319;&#29992;&#20102;XGBoost&#21644;&#20462;&#25913;&#29256;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;B2B&#30005;&#23376;&#21830;&#21153;&#20013;&#20934;&#30830;&#39044;&#27979;&#20080;&#23478;&#35746;&#21333;&#19979;&#36798;&#34892;&#20026;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20687;&#21360;&#24230;&#36825;&#26679;&#30340;&#21457;&#23637;&#20013;&#22269;&#23478;&#30340;&#32972;&#26223;&#19979;&#65292;&#20256;&#32479;&#30340;&#20225;&#19994;&#38388;&#65288;B2B&#65289;&#21830;&#19994;&#22312;&#20080;&#23478;&#21644;&#21334;&#23478;&#20043;&#38388;&#24314;&#31435;&#24378;&#22823;&#30340;&#20851;&#31995;&#12289;&#20449;&#20219;&#21644;&#20449;&#29992;&#23433;&#25490;&#30340;&#22522;&#30784;&#19978;&#36827;&#34892;&#65292;&#22240;&#27492;&#65292;&#30005;&#23376;&#21830;&#21153;&#20225;&#19994;&#32463;&#24120;&#12290;&#21019;&#31435;&#20110;2016&#24180;&#65292;Udaan&#30340;&#24895;&#26223;&#26159;&#36890;&#36807;&#25216;&#26415;&#25913;&#21464;&#21360;&#24230;&#30340;&#36152;&#26131;&#65292;&#26159;&#21360;&#24230;&#26368;&#22823;&#30340;&#20225;&#19994;&#38388;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#12290;Udaan&#22312;&#21253;&#25324;&#29983;&#27963;&#26041;&#24335;&#12289;&#30005;&#23376;&#20135;&#21697;&#12289;&#23478;&#23621;&#22312;&#20869;&#30340;&#21508;&#31181;&#20135;&#21697;&#31867;&#21035;&#20013;&#36816;&#33829;&#65292;&#24182;&#38599;&#29992;&#30005;&#35805;&#38144;&#21806;&#21592;&#26469;&#22521;&#20859;&#20080;&#23478;&#20851;&#31995;&#12289;&#31616;&#21270;&#35746;&#21333;&#19979;&#36798;&#31243;&#24207;&#24182;&#20419;&#36827;&#29305;&#21035;&#20419;&#38144;&#12290;&#20934;&#30830;&#39044;&#27979;&#20080;&#23478;&#35746;&#21333;&#19979;&#36798;&#34892;&#20026;&#25104;&#20026;&#23454;&#29616;&#21487;&#25345;&#32493;&#22686;&#38271;&#12289;&#22686;&#24378;&#31454;&#20105;&#21147;&#21644;&#20248;&#21270;&#36825;&#20123;&#30005;&#35805;&#38144;&#21806;&#21592;&#25928;&#29575;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;XGBoost&#21644;&#20462;&#25913;&#29256;&#30340;&#38598;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07843v1 Announce Type: new  Abstract: In the context of developing nations like India, traditional business to business (B2B) commerce heavily relies on the establishment of robust relationships, trust, and credit arrangements between buyers and sellers. Consequently, ecommerce enterprises frequently. Established in 2016 with a vision to revolutionize trade in India through technology, Udaan is the countrys largest business to business ecommerce platform. Udaan operates across diverse product categories, including lifestyle, electronics, home and employ telecallers to cultivate buyer relationships, streamline order placement procedures, and promote special promotions. The accurate anticipation of buyer order placement behavior emerges as a pivotal factor for attaining sustainable growth, heightening competitiveness, and optimizing the efficiency of these telecallers. To address this challenge, we have employed an ensemble approach comprising XGBoost and a modified version of
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#37327;&#21270;&#21644;&#20943;&#36731;&#34920;&#26684;&#29983;&#25104;&#27169;&#22411;&#30340;&#38544;&#31169;&#39118;&#38505;&#65292;&#36890;&#36807;&#23545;&#20116;&#31181;&#26368;&#20808;&#36827;&#30340;&#34920;&#26684;&#21512;&#25104;&#22120;&#36827;&#34892;&#23454;&#35777;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#24046;&#20998;&#38544;&#31169;&#34920;&#26684;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.07842</link><description>&lt;p&gt;
&#37327;&#21270;&#21644;&#20943;&#36731;&#34920;&#26684;&#29983;&#25104;&#27169;&#22411;&#30340;&#38544;&#31169;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Quantifying and Mitigating Privacy Risks for Tabular Generative Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07842
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#37327;&#21270;&#21644;&#20943;&#36731;&#34920;&#26684;&#29983;&#25104;&#27169;&#22411;&#30340;&#38544;&#31169;&#39118;&#38505;&#65292;&#36890;&#36807;&#23545;&#20116;&#31181;&#26368;&#20808;&#36827;&#30340;&#34920;&#26684;&#21512;&#25104;&#22120;&#36827;&#34892;&#23454;&#35777;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#24046;&#20998;&#38544;&#31169;&#34920;&#26684;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#27169;&#22411;&#20986;&#29616;&#20316;&#20026;&#20445;&#25252;&#38544;&#31169;&#30340;&#25968;&#25454;&#20849;&#20139;&#35299;&#20915;&#26041;&#26696;&#30340;&#24773;&#20917;&#65292;&#35813;&#21512;&#25104;&#25968;&#25454;&#38598;&#24212;&#35813;&#31867;&#20284;&#20110;&#21407;&#22987;&#25968;&#25454;&#65292;&#32780;&#19981;&#20250;&#36879;&#38706;&#21487;&#35782;&#21035;&#30340;&#31169;&#20154;&#20449;&#24687;&#12290;&#34920;&#26684;&#21512;&#25104;&#22120;&#30340;&#26680;&#24515;&#25216;&#26415;&#26681;&#26893;&#20110;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65292;&#33539;&#22260;&#20174;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#21040;&#26368;&#36817;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;&#26368;&#36817;&#30340;&#20808;&#21069;&#24037;&#20316;&#25581;&#31034;&#21644;&#37327;&#21270;&#20102;&#34920;&#26684;&#25968;&#25454;&#19978;&#30340;&#25928;&#29992;-&#38544;&#31169;&#26435;&#34913;&#65292;&#25581;&#31034;&#20102;&#21512;&#25104;&#25968;&#25454;&#30340;&#38544;&#31169;&#39118;&#38505;&#12290;&#25105;&#20204;&#39318;&#20808;&#36827;&#34892;&#20102;&#35814;&#23613;&#30340;&#23454;&#35777;&#20998;&#26512;&#65292;&#31361;&#20986;&#20102;&#20116;&#31181;&#26368;&#20808;&#36827;&#30340;&#34920;&#26684;&#21512;&#25104;&#22120;&#38024;&#23545;&#20843;&#31181;&#38544;&#31169;&#25915;&#20987;&#30340;&#25928;&#29992;-&#38544;&#31169;&#26435;&#34913;&#65292;&#29305;&#21035;&#20851;&#27880;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#12290;&#22312;&#35266;&#23519;&#21040;&#34920;&#26684;&#25193;&#25955;&#20013;&#39640;&#25968;&#25454;&#36136;&#37327;&#20294;&#20063;&#39640;&#38544;&#31169;&#39118;&#38505;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DP-TLDM&#65292;&#24046;&#20998;&#38544;&#31169;&#34920;&#26684;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#65292;&#30001;&#33258;&#21160;&#32534;&#30721;&#22120;&#32593;&#32476;&#32452;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07842v1 Announce Type: new  Abstract: Synthetic data from generative models emerges as the privacy-preserving data-sharing solution. Such a synthetic data set shall resemble the original data without revealing identifiable private information. The backbone technology of tabular synthesizers is rooted in image generative models, ranging from Generative Adversarial Networks (GANs) to recent diffusion models. Recent prior work sheds light on the utility-privacy tradeoff on tabular data, revealing and quantifying privacy risks on synthetic data. We first conduct an exhaustive empirical analysis, highlighting the utility-privacy tradeoff of five state-of-the-art tabular synthesizers, against eight privacy attacks, with a special focus on membership inference attacks. Motivated by the observation of high data quality but also high privacy risk in tabular diffusion, we propose DP-TLDM, Differentially Private Tabular Latent Diffusion Model, which is composed of an autoencoder networ
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36125;&#21494;&#26031;&#32479;&#35745;&#26694;&#26550;&#30340;&#21487;&#35782;&#21035;&#21644;&#21487;&#35299;&#37322;&#30340;&#33258;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#34701;&#21512;&#27668;&#20505;&#25968;&#25454;&#20135;&#21697;&#65292;&#20854;&#31354;&#38388;&#21464;&#21270;&#25429;&#25417;&#20102;&#26377;&#29992;&#30340;&#31354;&#38388;&#27169;&#24335;&#12290;</title><link>https://arxiv.org/abs/2403.07822</link><description>&lt;p&gt;
&#20351;&#29992;&#31354;&#38388;&#21464;&#21270;&#30340;&#33258;&#32534;&#30721;&#22120;&#34701;&#21512;&#27668;&#20505;&#25968;&#25454;&#20135;&#21697;
&lt;/p&gt;
&lt;p&gt;
Fusing Climate Data Products using a Spatially Varying Autoencoder
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07822
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36125;&#21494;&#26031;&#32479;&#35745;&#26694;&#26550;&#30340;&#21487;&#35782;&#21035;&#21644;&#21487;&#35299;&#37322;&#30340;&#33258;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#34701;&#21512;&#27668;&#20505;&#25968;&#25454;&#20135;&#21697;&#65292;&#20854;&#31354;&#38388;&#21464;&#21270;&#25429;&#25417;&#20102;&#26377;&#29992;&#30340;&#31354;&#38388;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#32534;&#30721;&#22120;&#26159;&#24378;&#22823;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#21387;&#32553;&#26469;&#33258;&#22810;&#20010;&#25968;&#25454;&#28304;&#30340;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#20687;&#25152;&#26377;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#19968;&#26679;&#65292;&#33258;&#32534;&#30721;&#22120;&#36890;&#24120;&#26159;&#19981;&#21487;&#35782;&#21035;&#19988;&#19981;&#21487;&#35299;&#37322;&#30340;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#21019;&#24314;&#19968;&#20010;&#21487;&#35782;&#21035;&#21644;&#21487;&#35299;&#37322;&#30340;&#33258;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#28151;&#21512;&#21644;&#21512;&#24182;&#27668;&#20505;&#25968;&#25454;&#20135;&#21697;&#12290;&#25152;&#25552;&#20986;&#30340;&#33258;&#32534;&#30721;&#22120;&#21033;&#29992;&#36125;&#21494;&#26031;&#32479;&#35745;&#26694;&#26550;&#65292;&#20801;&#35768;&#27010;&#29575;&#24615;&#35299;&#37322;&#65292;&#21516;&#26102;&#22312;&#31354;&#38388;&#19978;&#21464;&#21270;&#20197;&#25429;&#25417;&#21508;&#31181;&#25968;&#25454;&#20135;&#21697;&#20043;&#38388;&#30340;&#26377;&#29992;&#31354;&#38388;&#27169;&#24335;&#12290;&#22312;&#33258;&#32534;&#30721;&#22120;&#23398;&#20064;&#25968;&#25454;&#27169;&#24335;&#26102;&#23545;&#20854;&#26045;&#21152;&#32422;&#26463;&#65292;&#24418;&#25104;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#20849;&#35782;&#65292;&#21253;&#25324;&#27599;&#20010;&#36755;&#20837;&#20013;&#30340;&#37325;&#35201;&#29305;&#24449;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#26469;&#33258;&#22810;&#20010;&#38477;&#27700;&#20135;&#21697;&#30340;&#20449;&#24687;&#32467;&#21512;&#22312;&#39640;&#23665;&#20122;&#27954;&#23637;&#31034;&#20102;&#33258;&#32534;&#30721;&#22120;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07822v1 Announce Type: cross  Abstract: Autoencoders are powerful machine learning models used to compress information from multiple data sources. However, autoencoders, like all artificial neural networks, are often unidentifiable and uninterpretable. This research focuses on creating an identifiable and interpretable autoencoder that can be used to meld and combine climate data products. The proposed autoencoder utilizes a Bayesian statistical framework, allowing for probabilistic interpretations while also varying spatially to capture useful spatial patterns across the various data products. Constraints are placed on the autoencoder as it learns patterns in the data, creating an interpretable consensus that includes the important features from each input. We demonstrate the utility of the autoencoder by combining information from multiple precipitation products in High Mountain Asia.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#21033;&#29992;&#22810;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#28145;&#24230;&#23398;&#20064;&#36229;&#22768;&#24515;&#21160;&#22270;&#20998;&#21106;&#65292;&#22312;&#22788;&#29702;&#37096;&#20998;&#26631;&#35760;&#25968;&#25454;&#26102;&#37319;&#29992;&#25913;&#36827;&#30340;&#20132;&#21449;&#29109;&#25439;&#22833;&#20989;&#25968;&#12290;</title><link>https://arxiv.org/abs/2403.07818</link><description>&lt;p&gt;
&#26631;&#31614;&#20002;&#22833;&#29575;&#65306;&#21033;&#29992;&#20855;&#26377;&#22495;&#36716;&#31227;&#21644;&#37096;&#20998;&#26631;&#35760;&#30340;&#22810;&#20010;&#25968;&#25454;&#38598;&#25913;&#36827;&#28145;&#24230;&#23398;&#20064;&#36229;&#22768;&#24515;&#21160;&#22270;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Label Dropout: Improved Deep Learning Echocardiography Segmentation Using Multiple Datasets With Domain Shift and Partial Labelling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07818
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#21033;&#29992;&#22810;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#28145;&#24230;&#23398;&#20064;&#36229;&#22768;&#24515;&#21160;&#22270;&#20998;&#21106;&#65292;&#22312;&#22788;&#29702;&#37096;&#20998;&#26631;&#35760;&#25968;&#25454;&#26102;&#37319;&#29992;&#25913;&#36827;&#30340;&#20132;&#21449;&#29109;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#22768;&#24515;&#21160;&#22270;&#65288;&#36229;&#22768;&#65289;&#26159;&#35780;&#20272;&#24515;&#33039;&#21151;&#33021;&#26102;&#20351;&#29992;&#30340;&#31532;&#19968;&#31181;&#25104;&#20687;&#26041;&#24335;&#12290;&#20174;&#36229;&#22768;&#20013;&#27979;&#37327;&#21151;&#33021;&#29983;&#29289;&#26631;&#24535;&#29289;&#20381;&#36182;&#20110;&#23545;&#24515;&#33039;&#32467;&#26500;&#36827;&#34892;&#20998;&#21106;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#34987;&#25552;&#20986;&#26469;&#33258;&#21160;&#21270;&#36825;&#19968;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#23558;&#36825;&#20123;&#24037;&#20855;&#36716;&#21270;&#20026;&#24191;&#27867;&#30340;&#20020;&#24202;&#24212;&#29992;&#65292;&#37325;&#35201;&#30340;&#26159;&#20998;&#21106;&#27169;&#22411;&#23545;&#21508;&#31181;&#22270;&#20687;&#20855;&#26377;&#40065;&#26834;&#24615;&#65288;&#20363;&#22914;&#65292;&#30001;&#19981;&#21516;&#25195;&#25551;&#20202;&#33719;&#24471;&#65292;&#30001;&#19981;&#21516;&#32423;&#21035;&#30340;&#19987;&#23478;&#25805;&#20316;&#21592;&#33719;&#24471;&#31561;&#65289;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#31181;&#40065;&#26834;&#24615;&#27700;&#24179;&#65292;&#26377;&#24517;&#35201;&#20351;&#29992;&#22810;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#27169;&#22411;&#12290;&#22312;&#20351;&#29992;&#22810;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#26102;&#38754;&#20020;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#26159;&#26631;&#31614;&#23384;&#22312;&#30340;&#21464;&#21270;&#65292;&#21363;&#21512;&#24182;&#25968;&#25454;&#36890;&#24120;&#26159;&#37096;&#20998;&#26631;&#35760;&#30340;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#20132;&#21449;&#29109;&#25439;&#22833;&#20989;&#25968;&#30340;&#25913;&#36827;&#26469;&#22788;&#29702;&#37096;&#20998;&#26631;&#35760;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35757;&#32451;&#30340;naively
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07818v1 Announce Type: cross  Abstract: Echocardiography (echo) is the first imaging modality used when assessing cardiac function. The measurement of functional biomarkers from echo relies upon the segmentation of cardiac structures and deep learning models have been proposed to automate the segmentation process. However, in order to translate these tools to widespread clinical use it is important that the segmentation models are robust to a wide variety of images (e.g. acquired from different scanners, by operators with different levels of expertise etc.). To achieve this level of robustness it is necessary that the models are trained with multiple diverse datasets. A significant challenge faced when training with multiple diverse datasets is the variation in label presence, i.e. the combined data are often partially-labelled. Adaptations of the cross entropy loss function have been proposed to deal with partially labelled data. In this paper we show that training naively 
&lt;/p&gt;</description></item><item><title>Chronos&#26694;&#26550;&#36890;&#36807;&#22312;&#22266;&#23450;&#35789;&#27719;&#19978;&#35757;&#32451;&#39044;&#35757;&#32451;&#30340;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#65292;&#22312;&#22823;&#37327;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#22522;&#20934;&#27979;&#35797;&#65292;&#34920;&#29616;&#20986;&#22312;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#30340;&#25968;&#25454;&#38598;&#19978;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#26032;&#25968;&#25454;&#38598;&#19978;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#34920;&#29616;&#21487;&#27604;&#29978;&#33267;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.07815</link><description>&lt;p&gt;
Chronos: &#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#30340;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
Chronos: Learning the Language of Time Series
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07815
&lt;/p&gt;
&lt;p&gt;
Chronos&#26694;&#26550;&#36890;&#36807;&#22312;&#22266;&#23450;&#35789;&#27719;&#19978;&#35757;&#32451;&#39044;&#35757;&#32451;&#30340;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#65292;&#22312;&#22823;&#37327;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#22522;&#20934;&#27979;&#35797;&#65292;&#34920;&#29616;&#20986;&#22312;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#30340;&#25968;&#25454;&#38598;&#19978;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#26032;&#25968;&#25454;&#38598;&#19978;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#34920;&#29616;&#21487;&#27604;&#29978;&#33267;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Chronos&#65292;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#39044;&#35757;&#32451;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#26694;&#26550;&#12290;Chronos&#20351;&#29992;&#32553;&#25918;&#21644;&#37327;&#21270;&#23558;&#26102;&#38388;&#24207;&#21015;&#20540;&#26631;&#35760;&#21270;&#20026;&#22266;&#23450;&#35789;&#27719;&#65292;&#24182;&#36890;&#36807;&#20132;&#21449;&#29109;&#25439;&#22833;&#22312;&#36825;&#20123;&#26631;&#35760;&#21270;&#30340;&#26102;&#38388;&#24207;&#21015;&#19978;&#35757;&#32451;&#29616;&#26377;&#30340;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#12290;&#25105;&#20204;&#22312;&#22823;&#37327;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#38598;&#19978;&#22522;&#20110;T5&#31995;&#21015;&#65288;&#21442;&#25968;&#33539;&#22260;&#20174;20M&#21040;710M&#65289;&#23545;Chronos&#27169;&#22411;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#65292;&#21516;&#26102;&#36890;&#36807;&#39640;&#26031;&#36807;&#31243;&#29983;&#25104;&#20102;&#19968;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#20197;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#21253;&#21547;42&#20010;&#25968;&#25454;&#38598;&#30340;&#20840;&#38754;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#28085;&#30422;&#20102;&#20256;&#32479;&#30340;&#26412;&#22320;&#27169;&#22411;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Chronos&#27169;&#22411;&#65306;&#65288;a&#65289;&#22312;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#30340;&#25968;&#25454;&#38598;&#19978;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65307;&#65288;b&#65289;&#30456;&#23545;&#20110;&#19987;&#38376;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#22312;&#26032;&#25968;&#25454;&#38598;&#19978;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#21487;&#27604;&#29978;&#33267;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07815v1 Announce Type: cross  Abstract: We introduce Chronos, a simple yet effective framework for pretrained probabilistic time series models. Chronos tokenizes time series values using scaling and quantization into a fixed vocabulary and trains existing transformer-based language model architectures on these tokenized time series via the cross-entropy loss. We pretrained Chronos models based on the T5 family (ranging from 20M to 710M parameters) on a large collection of publicly available datasets, complemented by a synthetic dataset that we generated via Gaussian processes to improve generalization. In a comprehensive benchmark consisting of 42 datasets, and comprising both classical local models and deep learning methods, we show that Chronos models: (a) significantly outperform other methods on datasets that were part of the training corpus; and (b) have comparable and occasionally superior zero-shot performance on new datasets, relative to methods that were trained spe
&lt;/p&gt;</description></item><item><title>pyvene&#26159;&#19968;&#20010;&#24320;&#28304;Python&#24211;&#65292;&#25903;&#25345;&#22312;PyTorch&#27169;&#22411;&#19978;&#36827;&#34892;&#21487;&#23450;&#21046;&#30340;&#24178;&#39044;&#65292;&#25552;&#20379;&#32479;&#19968;&#21644;&#21487;&#25193;&#23637;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#37322;&#31070;&#32463;&#27169;&#22411;&#24182;&#19982;&#20182;&#20154;&#20998;&#20139;&#32463;&#36807;&#24178;&#39044;&#30340;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.07809</link><description>&lt;p&gt;
pyvene: &#36890;&#36807;&#24178;&#39044;&#26469;&#29702;&#35299;&#21644;&#25913;&#36827;PyTorch&#27169;&#22411;&#30340;&#24211;
&lt;/p&gt;
&lt;p&gt;
pyvene: A Library for Understanding and Improving PyTorch Models via Interventions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07809
&lt;/p&gt;
&lt;p&gt;
pyvene&#26159;&#19968;&#20010;&#24320;&#28304;Python&#24211;&#65292;&#25903;&#25345;&#22312;PyTorch&#27169;&#22411;&#19978;&#36827;&#34892;&#21487;&#23450;&#21046;&#30340;&#24178;&#39044;&#65292;&#25552;&#20379;&#32479;&#19968;&#21644;&#21487;&#25193;&#23637;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#37322;&#31070;&#32463;&#27169;&#22411;&#24182;&#19982;&#20182;&#20154;&#20998;&#20139;&#32463;&#36807;&#24178;&#39044;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#20869;&#37096;&#29366;&#24577;&#30340;&#24178;&#39044;&#26159;&#20154;&#24037;&#26234;&#33021;&#35768;&#22810;&#39046;&#22495;&#30340;&#22522;&#26412;&#25805;&#20316;&#65292;&#21253;&#25324;&#27169;&#22411;&#32534;&#36753;&#12289;&#25511;&#21046;&#12289;&#31283;&#20581;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#20026;&#20102;&#20419;&#36827;&#36825;&#26041;&#38754;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;pyvene&#65292;&#19968;&#20010;&#24320;&#28304;&#30340;Python&#24211;&#65292;&#25903;&#25345;&#22312;&#21508;&#31181;&#19981;&#21516;PyTorch&#27169;&#22359;&#19978;&#36827;&#34892;&#21487;&#23450;&#21046;&#30340;&#24178;&#39044;&#12290;Pyvene&#25903;&#25345;&#22797;&#26434;&#30340;&#24178;&#39044;&#26041;&#26696;&#65292;&#20855;&#26377;&#30452;&#35266;&#30340;&#37197;&#32622;&#26684;&#24335;&#65292;&#24182;&#19988;&#20854;&#24178;&#39044;&#21487;&#20197;&#26159;&#38745;&#24577;&#30340;&#25110;&#21253;&#21547;&#21487;&#35757;&#32451;&#21442;&#25968;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;pyvene&#22914;&#20309;&#25552;&#20379;&#19968;&#20010;&#32479;&#19968;&#21644;&#21487;&#25193;&#23637;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#31070;&#32463;&#27169;&#22411;&#36827;&#34892;&#24178;&#39044;&#24182;&#19982;&#20182;&#20154;&#20849;&#20139;&#32463;&#36807;&#24178;&#39044;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#22240;&#26524;&#25277;&#35937;&#21644;&#30693;&#35782;&#23450;&#20301;&#30340;&#21487;&#35299;&#37322;&#24615;&#20998;&#26512;&#23637;&#31034;&#20102;&#35813;&#24211;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;Python Package Index&#65288;PyPI&#65289;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#24211;&#65292;&#24182;&#22312;https://github.com/stanfordnlp/pyvene &#25552;&#20379;&#20102;&#20195;&#30721;&#12289;&#25991;&#26723;&#21644;&#25945;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07809v1 Announce Type: cross  Abstract: Interventions on model-internal states are fundamental operations in many areas of AI, including model editing, steering, robustness, and interpretability. To facilitate such research, we introduce $\textbf{pyvene}$, an open-source Python library that supports customizable interventions on a range of different PyTorch modules. $\textbf{pyvene}$ supports complex intervention schemes with an intuitive configuration format, and its interventions can be static or include trainable parameters. We show how $\textbf{pyvene}$ provides a unified and extensible framework for performing interventions on neural models and sharing the intervened upon models with others. We illustrate the power of the library via interpretability analyses using causal abstraction and knowledge localization. We publish our library through Python Package Index (PyPI) and provide code, documentation, and tutorials at https://github.com/stanfordnlp/pyvene.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#35774;&#22791;&#26412;&#22320;&#23398;&#20064;&#26550;&#26500;&#65292;&#36890;&#36807;&#23398;&#20064;&#29992;&#25143;&#35821;&#38899;&#29305;&#24449;&#26469;&#22686;&#24378;&#20851;&#38190;&#35789;&#35782;&#21035;&#31995;&#32479;&#65292;&#22312;35&#31867;&#38382;&#39064;&#30340;Google Speech Commands&#25968;&#25454;&#38598;&#20013;&#23454;&#29616;&#20102;&#39640;&#36798;19%&#30340;&#38169;&#35823;&#29575;&#38477;&#20302;&#12290;</title><link>https://arxiv.org/abs/2403.07802</link><description>&lt;p&gt;
&#36890;&#36807;&#35774;&#22791;&#26412;&#22320;&#21487;&#23398;&#20064;&#30340;&#29992;&#25143;&#35821;&#38899;&#29305;&#24449;&#22686;&#24378;&#20851;&#38190;&#35789;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Boosting keyword spotting through on-device learnable user speech characteristics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07802
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#35774;&#22791;&#26412;&#22320;&#23398;&#20064;&#26550;&#26500;&#65292;&#36890;&#36807;&#23398;&#20064;&#29992;&#25143;&#35821;&#38899;&#29305;&#24449;&#26469;&#22686;&#24378;&#20851;&#38190;&#35789;&#35782;&#21035;&#31995;&#32479;&#65292;&#22312;35&#31867;&#38382;&#39064;&#30340;Google Speech Commands&#25968;&#25454;&#38598;&#20013;&#23454;&#29616;&#20102;&#39640;&#36798;19%&#30340;&#38169;&#35823;&#29575;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
TinyML&#21463;&#38480;&#24212;&#29992;&#30340;&#20851;&#38190;&#35789;&#35782;&#21035;&#31995;&#32479;&#38656;&#35201;&#29616;&#22330;&#35843;&#25972;&#65292;&#20197;&#25552;&#39640;&#31163;&#32447;&#35757;&#32451;&#20998;&#31867;&#22120;&#22312;&#26410;&#30693;&#25512;&#29702;&#26465;&#20214;&#19979;&#37096;&#32626;&#26102;&#30340;&#20934;&#30830;&#24615;&#12290;&#36866;&#24212;&#30446;&#26631;&#29992;&#25143;&#30340;&#35821;&#38899;&#29305;&#28857;&#38656;&#35201;&#22823;&#37327;&#39046;&#22495;&#20869;&#26679;&#26412;&#65292;&#36890;&#24120;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#19981;&#21487;&#29992;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#30340;&#35774;&#22791;&#26412;&#22320;&#23398;&#20064;&#25216;&#26415;&#20381;&#36182;&#20110;&#35745;&#31639;&#23494;&#38598;&#22411;&#21644;&#20869;&#23384;&#28040;&#32791;&#26497;&#22823;&#30340;&#39592;&#24178;&#26356;&#26032;&#26041;&#26696;&#65292;&#19981;&#36866;&#29992;&#20110;&#22987;&#32456;&#24320;&#21551;&#12289;&#20351;&#29992;&#30005;&#27744;&#20379;&#30005;&#30340;&#35774;&#22791;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35774;&#22791;&#26412;&#22320;&#23398;&#20064;&#26550;&#26500;&#65292;&#30001;&#39044;&#35757;&#32451;&#39592;&#24178;&#21644;&#23398;&#20064;&#29992;&#25143;&#35821;&#38899;&#29305;&#24449;&#30340;&#29992;&#25143;&#24863;&#30693;&#23884;&#20837;&#32452;&#25104;&#12290;&#29983;&#25104;&#30340;&#29305;&#24449;&#34987;&#34701;&#21512;&#24182;&#29992;&#20110;&#23545;&#36755;&#20837;&#30340;&#35805;&#35821;&#36827;&#34892;&#20998;&#31867;&#12290;&#38024;&#23545;&#30001;&#26410;&#30693;&#35762;&#35805;&#32773;&#24341;&#36215;&#30340;&#39046;&#22495;&#36716;&#31227;&#65292;&#25105;&#20204;&#22312;Google Speech Commands&#25968;&#25454;&#38598;&#30340;35&#31867;&#38382;&#39064;&#19978;&#27979;&#24471;&#38169;&#35823;&#29575;&#38477;&#20302;&#39640;&#36798;19%&#65292;&#20174;30.1%&#38477;&#33267;24.3%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07802v1 Announce Type: cross  Abstract: Keyword spotting systems for always-on TinyML-constrained applications require on-site tuning to boost the accuracy of offline trained classifiers when deployed in unseen inference conditions. Adapting to the speech peculiarities of target users requires many in-domain samples, often unavailable in real-world scenarios. Furthermore, current on-device learning techniques rely on computationally intensive and memory-hungry backbone update schemes, unfit for always-on, battery-powered devices. In this work, we propose a novel on-device learning architecture, composed of a pretrained backbone and a user-aware embedding learning the user's speech characteristics. The so-generated features are fused and used to classify the input utterance. For domain shifts generated by unseen speakers, we measure error rate reductions of up to 19% from 30.1% to 24.3% based on the 35-class problem of the Google Speech Commands dataset, through the inexpensi
&lt;/p&gt;</description></item><item><title>jam-pgm&#26426;&#21046;&#22312;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#20013;&#33021;&#22815;&#32852;&#21512;&#36873;&#25321;&#20844;&#20849;&#25968;&#25454;&#21644;&#31169;&#23494;&#25968;&#25454;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#20844;&#20849;&#25968;&#25454;&#20998;&#24067;&#23384;&#22312;&#20559;&#24046;&#30340;&#24773;&#20917;&#19979;&#20248;&#20110;&#20854;&#20182;&#26426;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.07797</link><description>&lt;p&gt;
&#32852;&#21512;&#36873;&#25321;&#65306;&#36866;&#24212;&#24615;&#22320;&#23558;&#20844;&#20849;&#20449;&#24687;&#32435;&#20837;&#31169;&#23494;&#21512;&#25104;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Joint Selection: Adaptively Incorporating Public Information for Private Synthetic Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07797
&lt;/p&gt;
&lt;p&gt;
jam-pgm&#26426;&#21046;&#22312;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#20013;&#33021;&#22815;&#32852;&#21512;&#36873;&#25321;&#20844;&#20849;&#25968;&#25454;&#21644;&#31169;&#23494;&#25968;&#25454;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#20844;&#20849;&#25968;&#25454;&#20998;&#24067;&#23384;&#22312;&#20559;&#24046;&#30340;&#24773;&#20917;&#19979;&#20248;&#20110;&#20854;&#20182;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#36793;&#38469;&#21644;&#22270;&#27169;&#22411;&#30340;&#19981;&#21516;ially&#31169;&#23494;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#26426;&#21046;&#24050;&#22312;&#21508;&#31181;&#35774;&#32622;&#20013;&#21462;&#24471;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#19968;&#20010;&#23616;&#38480;&#24615;&#26159;&#23427;&#20204;&#26080;&#27861;&#25972;&#21512;&#20844;&#20849;&#25968;&#25454;&#12290;&#36890;&#36807;&#22312;&#20844;&#20849;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#26469;&#21021;&#22987;&#21270;&#25968;&#25454;&#29983;&#25104;&#27169;&#22411;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#25552;&#39640;&#21512;&#25104;&#25968;&#25454;&#30340;&#36136;&#37327;&#65292;&#20294;&#24403;&#27169;&#22411;&#32467;&#26500;&#26410;&#20107;&#20808;&#30830;&#23450;&#26102;&#65292;&#35813;&#25216;&#26415;&#26080;&#27861;&#24212;&#29992;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#26426;&#21046;jam-pgm&#65292;&#23558;&#33258;&#36866;&#24212;&#27979;&#37327;&#26694;&#26550;&#25193;&#23637;&#21040;&#32852;&#21512;&#36873;&#25321;&#27979;&#37327;&#20844;&#20849;&#25968;&#25454;&#21644;&#31169;&#23494;&#25968;&#25454;&#20043;&#38388;&#12290;&#36825;&#19968;&#25216;&#26415;&#20801;&#35768;&#23558;&#20844;&#20849;&#25968;&#25454;&#32435;&#20837;&#22522;&#20110;&#22270;&#27169;&#22411;&#30340;&#26426;&#21046;&#20013;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#65292;&#21363;&#20351;&#22312;&#20844;&#20849;&#25968;&#25454;&#20998;&#24067;&#23384;&#22312;&#20559;&#24046;&#30340;&#24773;&#20917;&#19979;&#65292;jam-pgm&#33021;&#22815;&#32988;&#36807;&#20844;&#20849;&#36741;&#21161;&#21644;&#38750;&#20844;&#20849;&#36741;&#21161;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07797v1 Announce Type: cross  Abstract: Mechanisms for generating differentially private synthetic data based on marginals and graphical models have been successful in a wide range of settings. However, one limitation of these methods is their inability to incorporate public data. Initializing a data generating model by pre-training on public data has shown to improve the quality of synthetic data, but this technique is not applicable when model structure is not determined a priori. We develop the mechanism jam-pgm, which expands the adaptive measurements framework to jointly select between measuring public data and private data. This technique allows for public data to be included in a graphical-model-based mechanism. We show that jam-pgm is able to outperform both publicly assisted and non publicly assisted synthetic data generation mechanisms even when the public data distribution is biased.
&lt;/p&gt;</description></item><item><title>DexCap&#26159;&#19968;&#20010;&#21487;&#31227;&#26893;&#30340;&#25163;&#37096;&#21160;&#20316;&#25429;&#25417;&#31995;&#32479;&#65292;&#32467;&#21512;DexIL&#31639;&#27861;&#20174;&#20154;&#31867;&#25163;&#37096;&#36816;&#21160;&#25968;&#25454;&#20013;&#35757;&#32451;&#26426;&#22120;&#20154;&#25216;&#33021;&#65292;&#20855;&#26377;&#31934;&#30830;&#36861;&#36394;&#21644;&#22797;&#21046;&#20154;&#31867;&#21160;&#20316;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.07788</link><description>&lt;p&gt;
DexCap&#65306;&#29992;&#20110;&#28789;&#24039;&#25805;&#20316;&#30340;&#21487;&#25193;&#23637;&#21644;&#21487;&#31227;&#26893;&#21160;&#20316;&#25429;&#25417;&#25968;&#25454;&#25910;&#38598;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
DexCap: Scalable and Portable Mocap Data Collection System for Dexterous Manipulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07788
&lt;/p&gt;
&lt;p&gt;
DexCap&#26159;&#19968;&#20010;&#21487;&#31227;&#26893;&#30340;&#25163;&#37096;&#21160;&#20316;&#25429;&#25417;&#31995;&#32479;&#65292;&#32467;&#21512;DexIL&#31639;&#27861;&#20174;&#20154;&#31867;&#25163;&#37096;&#36816;&#21160;&#25968;&#25454;&#20013;&#35757;&#32451;&#26426;&#22120;&#20154;&#25216;&#33021;&#65292;&#20855;&#26377;&#31934;&#30830;&#36861;&#36394;&#21644;&#22797;&#21046;&#20154;&#31867;&#21160;&#20316;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20154;&#31867;&#25163;&#37096;&#36816;&#21160;&#25968;&#25454;&#20013;&#23398;&#20064;&#26159;&#20026;&#26426;&#22120;&#20154;&#36171;&#20104;&#31867;&#20154;&#28789;&#24039;&#22312;&#29616;&#23454;&#25805;&#32437;&#20219;&#21153;&#20013;&#30340;&#28508;&#22312;&#36884;&#24452;&#65292;&#28982;&#32780;&#65292;&#29616;&#23384;&#25163;&#37096;&#21160;&#20316;&#25429;&#25417;&#31995;&#32479;&#30340;&#21487;&#31227;&#26893;&#24615;&#20197;&#21450;&#23558;&#21160;&#20316;&#25429;&#25417;&#25968;&#25454;&#36716;&#21270;&#20026;&#26377;&#25928;&#25511;&#21046;&#31574;&#30053;&#30340;&#22256;&#38590;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;DexCap&#65292;&#19968;&#20010;&#20415;&#25658;&#24335;&#25163;&#37096;&#21160;&#20316;&#25429;&#25417;&#31995;&#32479;&#65292;&#20197;&#21450;DexIL&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#20223;&#31639;&#27861;&#65292;&#21487;&#30452;&#25509;&#20174;&#20154;&#31867;&#25163;&#37096;&#21160;&#20316;&#25429;&#25417;&#25968;&#25454;&#35757;&#32451;&#28789;&#24039;&#26426;&#22120;&#20154;&#25216;&#33021;&#12290;DexCap&#22522;&#20110;SLAM&#21644;&#30005;&#30913;&#22330;&#20197;&#21450;&#29615;&#22659;&#30340;3D&#35266;&#23519;&#65292;&#25552;&#20379;&#20102;&#23545;&#25163;&#33109;&#21644;&#25163;&#25351;&#36816;&#21160;&#30340;&#31934;&#30830;&#12289;&#25239;&#36974;&#25377;&#30340;&#36319;&#36394;&#12290;&#21033;&#29992;&#36825;&#19968;&#20016;&#23500;&#30340;&#25968;&#25454;&#38598;&#65292;DexIL&#37319;&#29992;&#36870;&#36816;&#21160;&#23398;&#21644;&#22522;&#20110;&#28857;&#20113;&#30340;&#27169;&#20223;&#23398;&#20064;&#26469;&#22797;&#21046;&#20154;&#31867;&#21160;&#20316;&#19982;&#26426;&#22120;&#20154;&#25163;&#12290;&#38500;&#20102;&#20174;&#20154;&#31867;&#36816;&#21160;&#20013;&#23398;&#20064;&#22806;&#65292;DexCap&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;op
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07788v1 Announce Type: cross  Abstract: Imitation learning from human hand motion data presents a promising avenue for imbuing robots with human-like dexterity in real-world manipulation tasks. Despite this potential, substantial challenges persist, particularly with the portability of existing hand motion capture (mocap) systems and the difficulty of translating mocap data into effective control policies. To tackle these issues, we introduce DexCap, a portable hand motion capture system, alongside DexIL, a novel imitation algorithm for training dexterous robot skills directly from human hand mocap data. DexCap offers precise, occlusion-resistant tracking of wrist and finger motions based on SLAM and electromagnetic field together with 3D observations of the environment. Utilizing this rich dataset, DexIL employs inverse kinematics and point cloud-based imitation learning to replicate human actions with robot hands. Beyond learning from human motion, DexCap also offers an op
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FairRR&#30340;&#39044;&#22788;&#29702;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#38543;&#26426;&#21709;&#24212;&#26694;&#26550;&#20013;&#20462;&#25913;&#21709;&#24212;&#21464;&#37327;&#30340;&#26368;&#20248;&#35774;&#35745;&#30697;&#38453;&#65292;&#30452;&#25509;&#25511;&#21046;&#32676;&#20307;&#20844;&#24179;&#24615;&#30340;&#24230;&#37327;&#65292;&#20174;&#32780;&#20135;&#29983;&#20986;&#33394;&#30340;&#19979;&#28216;&#27169;&#22411;&#25928;&#29992;&#21644;&#20844;&#24179;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.07780</link><description>&lt;p&gt;
&#20844;&#24179;RR&#65306;&#36890;&#36807;&#38543;&#26426;&#21709;&#24212;&#23454;&#29616;&#32676;&#20307;&#20844;&#24179;&#30340;&#39044;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
FairRR: Pre-Processing for Group Fairness through Randomized Response
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FairRR&#30340;&#39044;&#22788;&#29702;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#38543;&#26426;&#21709;&#24212;&#26694;&#26550;&#20013;&#20462;&#25913;&#21709;&#24212;&#21464;&#37327;&#30340;&#26368;&#20248;&#35774;&#35745;&#30697;&#38453;&#65292;&#30452;&#25509;&#25511;&#21046;&#32676;&#20307;&#20844;&#24179;&#24615;&#30340;&#24230;&#37327;&#65292;&#20174;&#32780;&#20135;&#29983;&#20986;&#33394;&#30340;&#19979;&#28216;&#27169;&#22411;&#25928;&#29992;&#21644;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#37325;&#35201;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#20351;&#29992;&#26085;&#30410;&#22686;&#22810;&#65292;&#36825;&#20419;&#20351;&#20154;&#20204;&#24320;&#22987;&#30740;&#31350;&#36825;&#20123;&#31995;&#32479;&#30340;&#20844;&#24179;&#24615;&#12290;&#34429;&#28982;&#22312;&#22788;&#29702;&#36807;&#31243;&#21644;&#21518;&#22788;&#29702;&#35774;&#32622;&#20013;&#24050;&#32463;&#36827;&#34892;&#20102;&#22823;&#37327;&#24037;&#20316;&#26469;&#30740;&#31350;&#32676;&#20307;&#20844;&#24179;&#24615;&#65292;&#20294;&#24456;&#23569;&#26377;&#30740;&#31350;&#33021;&#22815;&#20174;&#29702;&#35770;&#19978;&#23558;&#36825;&#20123;&#32467;&#26524;&#19982;&#39044;&#22788;&#29702;&#39046;&#22495;&#36830;&#25509;&#36215;&#26469;&#12290;&#26412;&#25991;&#25552;&#20986;&#65292;&#23454;&#29616;&#19979;&#28216;&#27169;&#22411;&#20013;&#30340;&#32676;&#20307;&#20844;&#24179;&#24615;&#21487;&#20197;&#34987;&#24418;&#24335;&#21270;&#20026;&#22312;&#38543;&#26426;&#21709;&#24212;&#26694;&#26550;&#20013;&#20462;&#25913;&#21709;&#24212;&#21464;&#37327;&#30340;&#26368;&#20248;&#35774;&#35745;&#30697;&#38453;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#32676;&#20307;&#20844;&#24179;&#24615;&#30340;&#24230;&#37327;&#21487;&#20197;&#36890;&#36807;&#26368;&#20248;&#27169;&#22411;&#25928;&#29992;&#30452;&#25509;&#25511;&#21046;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;FairRR&#30340;&#39044;&#22788;&#29702;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20135;&#29983;&#20986;&#33394;&#30340;&#19979;&#28216;&#27169;&#22411;&#25928;&#29992;&#21644;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07780v1 Announce Type: cross  Abstract: The increasing usage of machine learning models in consequential decision-making processes has spurred research into the fairness of these systems. While significant work has been done to study group fairness in the in-processing and post-processing setting, there has been little that theoretically connects these results to the pre-processing domain. This paper proposes that achieving group fairness in downstream models can be formulated as finding the optimal design matrix in which to modify a response variable in a Randomized Response framework. We show that measures of group fairness can be directly controlled for with optimal model utility, proposing a pre-processing algorithm called FairRR that yields excellent downstream model utility and fairness.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25209;&#21028;&#24615;&#35780;&#20272;&#20102;&#35821;&#38899;&#35782;&#21035;&#25968;&#25454;&#38598;&#20013;&#30340;&#25991;&#26412;&#20381;&#36182;&#24615;&#65292;&#25581;&#31034;&#20102;&#19968;&#20123;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#33021;&#20250;&#36807;&#20110;&#20851;&#27880;&#35789;&#27719;&#29305;&#24449;&#32780;&#38750;&#39044;&#26399;&#30340;&#35821;&#38899;&#20132;&#38469;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2403.07767</link><description>&lt;p&gt;
&#36229;&#36234;&#26631;&#31614;&#65306;&#25581;&#31034;&#35821;&#38899;&#35782;&#21035;&#25968;&#25454;&#38598;&#20013;&#30340;&#25991;&#26412;&#20381;&#36182;&#24615;
&lt;/p&gt;
&lt;p&gt;
Beyond the Labels: Unveiling Text-Dependency in Paralinguistic Speech Recognition Datasets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07767
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25209;&#21028;&#24615;&#35780;&#20272;&#20102;&#35821;&#38899;&#35782;&#21035;&#25968;&#25454;&#38598;&#20013;&#30340;&#25991;&#26412;&#20381;&#36182;&#24615;&#65292;&#25581;&#31034;&#20102;&#19968;&#20123;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#33021;&#20250;&#36807;&#20110;&#20851;&#27880;&#35789;&#27719;&#29305;&#24449;&#32780;&#38750;&#39044;&#26399;&#30340;&#35821;&#38899;&#20132;&#38469;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#20284;&#35748;&#30693;&#36127;&#33655;&#21644;&#24773;&#32490;&#31561;&#35821;&#38899;&#20132;&#38469;&#29305;&#24449;&#36234;&#26469;&#36234;&#34987;&#35748;&#21487;&#20026;&#35821;&#38899;&#35782;&#21035;&#30740;&#31350;&#20013;&#30340;&#20851;&#38190;&#39046;&#22495;&#65292;&#36890;&#24120;&#36890;&#36807;&#19987;&#38376;&#30340;&#25968;&#25454;&#38598;&#65288;&#22914;CLSE&#21644;IEMOCAP&#65289;&#36827;&#34892;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#20154;&#23457;&#26597;&#36825;&#20123;&#25968;&#25454;&#38598;&#26159;&#21542;&#23384;&#22312;&#25991;&#26412;&#20381;&#36182;&#24615;&#12290;&#26412;&#25991;&#25209;&#21028;&#24615;&#22320;&#35780;&#20272;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#36825;&#20123;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#26102;&#30495;&#27491;&#23398;&#20250;&#35782;&#21035;&#35821;&#38899;&#20132;&#38469;&#29305;&#24449;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#25429;&#25417;&#35789;&#27719;&#29305;&#24449;&#30340;&#26222;&#36941;&#20551;&#35774;&#12290;&#36890;&#36807;&#26816;&#26597;&#36825;&#20123;&#25968;&#25454;&#38598;&#20013;&#30340;&#35789;&#27719;&#37325;&#21472;&#24182;&#27979;&#35797;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#29305;&#24449;&#26631;&#31614;&#20013;&#30340;&#26174;&#33879;&#25991;&#26412;&#20381;&#36182;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#19968;&#20123;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#20687;HuBERT&#36825;&#26679;&#30340;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#21487;&#33021;&#26080;&#24847;&#20013;&#19987;&#27880;&#20110;&#35789;&#27719;&#29305;&#24449;&#65292;&#32780;&#19981;&#26159;&#39044;&#26399;&#30340;&#35821;&#38899;&#20132;&#38469;&#29305;&#24449;&#12290;&#26412;&#30740;&#31350;&#21495;&#21484;&#30740;&#31350;&#30028;&#37325;&#26032;&#35780;&#20272;&#25968;&#25454;&#38598;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07767v1 Announce Type: cross  Abstract: Paralinguistic traits like cognitive load and emotion are increasingly recognized as pivotal areas in speech recognition research, often examined through specialized datasets like CLSE and IEMOCAP. However, the integrity of these datasets is seldom scrutinized for text-dependency. This paper critically evaluates the prevalent assumption that machine learning models trained on such datasets genuinely learn to identify paralinguistic traits, rather than merely capturing lexical features. By examining the lexical overlap in these datasets and testing the performance of machine learning models, we expose significant text-dependency in trait-labeling. Our results suggest that some machine learning models, especially large pre-trained models like HuBERT, might inadvertently focus on lexical characteristics rather than the intended paralinguistic features. The study serves as a call to action for the research community to reevaluate the relia
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Probabilistic Easy Variational Causal Effect (PEACE)&#30340;&#20989;&#25968;&#65292;&#21487;&#20197;&#27979;&#37327;X&#23545;Y&#30340;&#30452;&#25509;&#22240;&#26524;&#25928;&#24212;&#65292;&#36866;&#29992;&#20110;&#36830;&#32493;&#21644;&#31163;&#25955;&#24773;&#20917;&#65292;&#36890;&#36807;&#31649;&#29702;&#27010;&#29575;&#23494;&#24230;&#20540;&#24378;&#24230;$d\ge 0$&#26469;&#23454;&#29616;&#24178;&#39044;&#12290;</title><link>https://arxiv.org/abs/2403.07745</link><description>&lt;p&gt;
&#27010;&#29575;&#26131;&#21464;&#37327;&#22240;&#26524;&#25928;&#24212;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Easy Variational Causal Effect
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07745
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Probabilistic Easy Variational Causal Effect (PEACE)&#30340;&#20989;&#25968;&#65292;&#21487;&#20197;&#27979;&#37327;X&#23545;Y&#30340;&#30452;&#25509;&#22240;&#26524;&#25928;&#24212;&#65292;&#36866;&#29992;&#20110;&#36830;&#32493;&#21644;&#31163;&#25955;&#24773;&#20917;&#65292;&#36890;&#36807;&#31649;&#29702;&#27010;&#29575;&#23494;&#24230;&#20540;&#24378;&#24230;$d\ge 0$&#26469;&#23454;&#29616;&#24178;&#39044;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#30740;&#31350;&#38543;&#26426;&#21521;&#37327;$X$&#21644;$Z$&#65292;&#20197;&#21450;$Y=g(X,Z)$&#30340;&#24773;&#20917;&#12290;&#19968;&#26041;&#38754;&#65292;&#23545;&#20110;&#36830;&#32493;&#30340;$X$&#21644;$Z$&#65292;&#36890;&#36807;&#20351;&#29992;&#24635;&#21464;&#24046;&#21644;$g$&#30340;&#36890;&#37327;&#30340;&#24605;&#24819;&#65292;&#25105;&#20204;&#21457;&#23637;&#20102;&#19968;&#20010;&#22312;&#22240;&#26524;&#25512;&#26029;&#20013;&#33021;&#22815;&#22788;&#29702;&#24191;&#27867;&#22240;&#26524;&#38382;&#39064;&#39046;&#22495;&#30340;&#35270;&#35282;&#12290;&#25105;&#20204;&#20851;&#27880;&#19968;&#20010;&#31216;&#20026;Probabilistic Easy Variational Causal Effect (PEACE)&#30340;&#20989;&#25968;&#65292;&#23427;&#21487;&#20197;&#27979;&#37327;$X$&#23545;$Y$&#30340;&#30452;&#25509;&#22240;&#26524;&#25928;&#24212;&#65292;&#32780;&#21516;&#26102;&#25913;&#21464;$X$&#30340;&#20540;&#65292;&#20294;&#20445;&#25345;$Z$&#30340;&#20540;&#19981;&#21464;&#12290;PEACE&#26159;&#20851;&#20110;$d\ge 0$&#30340;&#19968;&#20010;&#20989;&#25968;&#65292;&#31649;&#29702;&#30528;&#27010;&#29575;&#23494;&#24230;&#20540;$f(x|z)$&#30340;&#24378;&#24230;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#23558;&#19978;&#36848;&#24605;&#24819;&#25512;&#24191;&#21040;&#31163;&#25955;&#24773;&#20917;&#65292;&#24182;&#23637;&#31034;&#20854;&#19982;&#36830;&#32493;&#24773;&#20917;&#30340;&#20860;&#23481;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#27979;&#24230;&#35770;&#27010;&#24565;&#30740;&#31350;&#20102;PEACE&#30340;&#19968;&#20123;&#24615;&#36136;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20123;&#21487;&#36776;&#35782;&#24615;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07745v1 Announce Type: cross  Abstract: Let $X$ and $Z$ be random vectors, and $Y=g(X,Z)$. In this paper, on the one hand, for the case that $X$ and $Z$ are continuous, by using the ideas from the total variation and the flux of $g$, we develop a point of view in causal inference capable of dealing with a broad domain of causal problems. Indeed, we focus on a function, called Probabilistic Easy Variational Causal Effect (PEACE), which can measure the direct causal effect of $X$ on $Y$ with respect to continuously and interventionally changing the values of $X$ while keeping the value of $Z$ constant. PEACE is a function of $d\ge 0$, which is a degree managing the strengths of probability density values $f(x|z)$. On the other hand, we generalize the above idea for the discrete case and show its compatibility with the continuous case. Further, we investigate some properties of PEACE using measure theoretical concepts. Furthermore, we provide some identifiability criteria and s
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#23478;&#28151;&#21512;&#26041;&#26696;&#65292;&#29992;&#20110;&#22312;&#35745;&#31639;&#30149;&#29702;&#23398;&#31995;&#32479;&#20013;&#26816;&#27979;&#21644;&#25490;&#38500;&#20116;&#31181;&#26174;&#33879;&#30340;&#24037;&#20214;&#65292;&#24182;&#24212;&#29992;&#27010;&#29575;&#38408;&#20540;&#22788;&#29702;&#12290;</title><link>https://arxiv.org/abs/2403.07743</link><description>&lt;p&gt;
&#20026;&#35745;&#31639;&#30149;&#29702;&#23398;&#31995;&#32479;&#37197;&#22791;&#24037;&#20214;&#22788;&#29702;&#27969;&#27700;&#32447;&#65306;&#35745;&#31639;&#19982;&#24615;&#33021;&#26435;&#34913;&#30340;&#23637;&#31034;
&lt;/p&gt;
&lt;p&gt;
Equipping Computational Pathology Systems with Artifact Processing Pipelines: A Showcase for Computation and Performance Trade-offs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07743
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#23478;&#28151;&#21512;&#26041;&#26696;&#65292;&#29992;&#20110;&#22312;&#35745;&#31639;&#30149;&#29702;&#23398;&#31995;&#32479;&#20013;&#26816;&#27979;&#21644;&#25490;&#38500;&#20116;&#31181;&#26174;&#33879;&#30340;&#24037;&#20214;&#65292;&#24182;&#24212;&#29992;&#27010;&#29575;&#38408;&#20540;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#32455;&#30149;&#29702;&#23398;&#26159;&#30284;&#30151;&#35786;&#26029;&#30340;&#40644;&#37329;&#26631;&#20934;&#65292;&#22312;&#26174;&#24494;&#38236;&#19979;&#36827;&#34892;&#26816;&#26597;&#12290;&#28982;&#32780;&#65292;&#32452;&#32455;&#30149;&#29702;&#23398;&#22788;&#29702;&#36807;&#31243;&#20250;&#20135;&#29983;&#19968;&#20123;&#24037;&#20214;&#65292;&#26368;&#32456;&#20250;&#36716;&#31227;&#21040;&#29627;&#29827;&#36733;&#29627;&#29255;&#30340;&#25968;&#23383;&#21270;&#29256;&#26412;&#65292;&#21363;&#20840;&#29627;&#24187;&#28783;&#29255;&#12290;&#24037;&#20214;&#26159;&#35786;&#26029;&#26080;&#20851;&#30340;&#21306;&#22495;&#65292;&#21487;&#33021;&#23548;&#33268;&#38169;&#35823;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#39044;&#27979;&#12290;&#22240;&#27492;&#65292;&#22312;&#35745;&#31639;&#30149;&#29702;&#23398;&#65288;CPATH&#65289;&#31995;&#32479;&#20013;&#26816;&#27979;&#21644;&#25490;&#38500;&#24037;&#20214;&#23545;&#20110;&#21487;&#38752;&#30340;&#33258;&#21160;&#35786;&#26029;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#23478;&#28151;&#21512;&#65288;MoE&#65289;&#26041;&#26696;&#65292;&#29992;&#20110;&#26816;&#27979;&#21253;&#25324;&#25439;&#22351;&#32452;&#32455;&#12289;&#27169;&#31946;&#12289;&#35126;&#30385;&#32452;&#32455;&#12289;&#27668;&#27873;&#21644;&#22312;WSIs&#20013;&#30340;&#32452;&#32455;&#23398;&#26080;&#20851;&#34880;&#28082;&#31561;&#20116;&#31181;&#26174;&#33879;&#24037;&#20214;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35757;&#32451;&#29420;&#31435;&#30340;&#20108;&#20803;DL&#27169;&#22411;&#20316;&#20026;&#19987;&#23478;&#26469;&#25429;&#25417;&#29305;&#23450;&#30340;&#24037;&#20214;&#24418;&#24577;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#34701;&#21512;&#26426;&#21046;&#26469;&#38598;&#25104;&#23427;&#20204;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#23545;&#26368;&#32456;&#30340;&#27010;&#29575;&#36827;&#34892;&#27010;&#29575;&#38408;&#20540;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07743v1 Announce Type: cross  Abstract: Histopathology is a gold standard for cancer diagnosis under a microscopic examination. However, histological tissue processing procedures result in artifacts, which are ultimately transferred to the digitized version of glass slides, known as whole slide images (WSIs). Artifacts are diagnostically irrelevant areas and may result in wrong deep learning (DL) algorithms predictions. Therefore, detecting and excluding artifacts in the computational pathology (CPATH) system is essential for reliable automated diagnosis. In this paper, we propose a mixture of experts (MoE) scheme for detecting five notable artifacts, including damaged tissue, blur, folded tissue, air bubbles, and histologically irrelevant blood from WSIs. First, we train independent binary DL models as experts to capture particular artifact morphology. Then, we ensemble their predictions using a fusion mechanism. We apply probabilistic thresholding over the final probabilit
&lt;/p&gt;</description></item><item><title>HSIC&#20272;&#35745;&#30340;&#26497;&#23567;&#21270;&#29575;&#23545;&#24179;&#31227;&#19981;&#21464;&#26680;&#30340;&#29420;&#31435;&#24615;&#24230;&#37327;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;</title><link>https://arxiv.org/abs/2403.07735</link><description>&lt;p&gt;
HSIC&#20272;&#35745;&#30340;&#26497;&#23567;&#21270;&#29575;&#23545;&#24179;&#31227;&#19981;&#21464;&#26680;
&lt;/p&gt;
&lt;p&gt;
The Minimax Rate of HSIC Estimation for Translation-Invariant Kernels
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07735
&lt;/p&gt;
&lt;p&gt;
HSIC&#20272;&#35745;&#30340;&#26497;&#23567;&#21270;&#29575;&#23545;&#24179;&#31227;&#19981;&#21464;&#26680;&#30340;&#29420;&#31435;&#24615;&#24230;&#37327;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Kernel&#25216;&#26415;&#26159;&#25968;&#25454;&#31185;&#23398;&#21644;&#32479;&#35745;&#23398;&#20013;&#26368;&#26377;&#24433;&#21709;&#21147;&#30340;&#26041;&#27861;&#20043;&#19968;&#12290;&#22312;&#28201;&#21644;&#26465;&#20214;&#19979;&#65292;&#19982;&#26680;&#30456;&#20851;&#30340;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#33021;&#22815;&#32534;&#30721;$M\ge 2$&#20010;&#38543;&#26426;&#21464;&#37327;&#30340;&#29420;&#31435;&#24615;&#12290;&#22312;&#26680;&#19978;&#20381;&#36182;&#30340;&#26368;&#26222;&#36941;&#30340;&#29420;&#31435;&#24615;&#24230;&#37327;&#21487;&#33021;&#26159;&#25152;&#35859;&#30340;Hilbert-Schmidt&#29420;&#31435;&#24615;&#20934;&#21017;(HSIC; &#22312;&#32479;&#35745;&#25991;&#29486;&#20013;&#20063;&#31216;&#20026;&#36317;&#31163;&#21327;&#26041;&#24046;)&#12290;&#23613;&#31649;&#33258;&#36817;&#20108;&#21313;&#24180;&#21069;&#24341;&#20837;&#20197;&#26469;&#24050;&#32463;&#26377;&#21508;&#31181;&#29616;&#26377;&#30340;&#35774;&#35745;&#30340;HSIC&#20272;&#35745;&#37327;&#65292;HSIC&#21487;&#20197;&#34987;&#20272;&#35745;&#30340;&#36895;&#24230;&#30340;&#22522;&#26412;&#38382;&#39064;&#20173;&#28982;&#26159;&#24320;&#25918;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#21253;&#21547;&#20855;&#26377;&#36830;&#32493;&#26377;&#30028;&#24179;&#31227;&#19981;&#21464;&#29305;&#24449;&#26680;&#30340;&#39640;&#26031;Borel&#27979;&#24230;&#22312;$\mathbb R^d$&#19978;&#30340;HSIC&#20272;&#35745;&#30340;&#26497;&#23567;&#21270;&#26368;&#20248;&#36895;&#29575;&#26159;$\mathcal O\!\left(n^{-1/2}\right)$&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#24847;&#21619;&#30528;&#35768;&#22810;&#26041;&#38754;&#22312;&#26497;&#23567;&#21270;&#24847;&#20041;&#19978;&#30340;&#26368;&#20248;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07735v1 Announce Type: cross  Abstract: Kernel techniques are among the most influential approaches in data science and statistics. Under mild conditions, the reproducing kernel Hilbert space associated to a kernel is capable of encoding the independence of $M\ge 2$ random variables. Probably the most widespread independence measure relying on kernels is the so-called Hilbert-Schmidt independence criterion (HSIC; also referred to as distance covariance in the statistics literature). Despite various existing HSIC estimators designed since its introduction close to two decades ago, the fundamental question of the rate at which HSIC can be estimated is still open. In this work, we prove that the minimax optimal rate of HSIC estimation on $\mathbb R^d$ for Borel measures containing the Gaussians with continuous bounded translation-invariant characteristic kernels is $\mathcal O\!\left(n^{-1/2}\right)$. Specifically, our result implies the optimality in the minimax sense of many 
&lt;/p&gt;</description></item><item><title>CAS&#26694;&#26550;&#20801;&#35768;&#22312;&#22312;&#32447;&#36873;&#25321;&#24615;&#39044;&#27979;&#20013;&#25511;&#21046;FCR&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#36873;&#25321;&#21644;&#26657;&#20934;&#38598;&#26500;&#36896;&#36755;&#20986;&#31526;&#21512;&#39044;&#27979;&#21306;&#38388;</title><link>https://arxiv.org/abs/2403.07728</link><description>&lt;p&gt;
CAS: &#19968;&#31181;&#20855;&#26377;FCR&#25511;&#21046;&#30340;&#22312;&#32447;&#36873;&#25321;&#24615;&#31526;&#21512;&#39044;&#27979;&#30340;&#36890;&#29992;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
CAS: A General Algorithm for Online Selective Conformal Prediction with FCR Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07728
&lt;/p&gt;
&lt;p&gt;
CAS&#26694;&#26550;&#20801;&#35768;&#22312;&#22312;&#32447;&#36873;&#25321;&#24615;&#39044;&#27979;&#20013;&#25511;&#21046;FCR&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#36873;&#25321;&#21644;&#26657;&#20934;&#38598;&#26500;&#36896;&#36755;&#20986;&#31526;&#21512;&#39044;&#27979;&#21306;&#38388;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32447;&#26041;&#24335;&#19979;&#21518;&#36873;&#25321;&#39044;&#27979;&#25512;&#26029;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#36991;&#20813;&#23558;&#36164;&#28304;&#32791;&#36153;&#22312;&#19981;&#37325;&#35201;&#30340;&#21333;&#20301;&#19978;&#65292;&#22312;&#25253;&#21578;&#20854;&#39044;&#27979;&#21306;&#38388;&#20043;&#21069;&#23545;&#24403;&#21069;&#20010;&#20307;&#36827;&#34892;&#21021;&#27493;&#36873;&#25321;&#22312;&#22312;&#32447;&#39044;&#27979;&#20219;&#21153;&#20013;&#26159;&#24120;&#35265;&#19988;&#26377;&#24847;&#20041;&#30340;&#12290;&#30001;&#20110;&#22312;&#32447;&#36873;&#25321;&#23548;&#33268;&#25152;&#36873;&#39044;&#27979;&#21306;&#38388;&#20013;&#23384;&#22312;&#26102;&#38388;&#22810;&#37325;&#24615;&#65292;&#22240;&#27492;&#25511;&#21046;&#23454;&#26102;&#35823;&#35206;&#30422;&#38472;&#36848;&#29575;&#65288;FCR&#65289;&#26469;&#27979;&#37327;&#24179;&#22343;&#35823;&#35206;&#30422;&#35823;&#24046;&#26159;&#37325;&#35201;&#30340;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;CAS&#65288;&#36866;&#24212;&#24615;&#36873;&#25321;&#21518;&#26657;&#20934;&#65289;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#21487;&#20197;&#21253;&#35065;&#20219;&#20309;&#39044;&#27979;&#27169;&#22411;&#21644;&#22312;&#32447;&#36873;&#25321;&#35268;&#21017;&#65292;&#20197;&#36755;&#20986;&#21518;&#36873;&#25321;&#30340;&#39044;&#27979;&#21306;&#38388;&#12290;&#22914;&#26524;&#36873;&#25321;&#20102;&#24403;&#21069;&#20010;&#20307;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;&#21382;&#21490;&#25968;&#25454;&#36827;&#34892;&#33258;&#36866;&#24212;&#36873;&#25321;&#26469;&#26500;&#24314;&#26657;&#20934;&#38598;&#65292;&#28982;&#21518;&#20026;&#26410;&#35266;&#23519;&#21040;&#30340;&#26631;&#31614;&#36755;&#20986;&#31526;&#21512;&#39044;&#27979;&#21306;&#38388;&#12290;&#25105;&#20204;&#20026;&#26657;&#20934;&#38598;&#25552;&#20379;&#20102;&#21487;&#34892;&#30340;&#26500;&#36896;&#26041;&#24335;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07728v1 Announce Type: cross  Abstract: We study the problem of post-selection predictive inference in an online fashion. To avoid devoting resources to unimportant units, a preliminary selection of the current individual before reporting its prediction interval is common and meaningful in online predictive tasks. Since the online selection causes a temporal multiplicity in the selected prediction intervals, it is important to control the real-time false coverage-statement rate (FCR) to measure the averaged miscoverage error. We develop a general framework named CAS (Calibration after Adaptive Selection) that can wrap around any prediction model and online selection rule to output post-selection prediction intervals. If the current individual is selected, we first perform an adaptive selection on historical data to construct a calibration set, then output a conformal prediction interval for the unobserved label. We provide tractable constructions for the calibration set for 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#30452;&#25509;&#20998;&#26512;&#26368;&#20248;&#36125;&#21494;&#26031;&#20998;&#31867;&#22120;&#22312;&#25968;&#25454;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#30340;&#34892;&#20026;&#65292;&#20197;&#24179;&#34913;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.07724</link><description>&lt;p&gt;
&#22312;&#25968;&#25454;&#38480;&#21046;&#30340;&#20108;&#20998;&#31867;&#20013;&#24179;&#34913;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Balancing Fairness and Accuracy in Data-Restricted Binary Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07724
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#30452;&#25509;&#20998;&#26512;&#26368;&#20248;&#36125;&#21494;&#26031;&#20998;&#31867;&#22120;&#22312;&#25968;&#25454;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#30340;&#34892;&#20026;&#65292;&#20197;&#24179;&#34913;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22788;&#29702;&#25935;&#24863;&#20449;&#24687;&#30340;&#24212;&#29992;&#21487;&#33021;&#23545;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#20998;&#31867;&#22120;&#21487;&#29992;&#25968;&#25454;&#35774;&#32622;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#20010;&#26694;&#26550;&#65292;&#27169;&#25311;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#22312;&#22235;&#31181;&#23454;&#38469;&#24773;&#26223;&#19979;&#25506;&#35752;&#21487;&#29992;&#20110;&#20998;&#26512;&#30340;&#25968;&#25454;&#31867;&#22411;&#12290;&#19982;&#20808;&#21069;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#32463;&#35757;&#32451;&#20197;&#38544;&#24335;&#23398;&#20064;&#25968;&#25454;&#38598;&#30340;&#29305;&#24449;&#21521;&#37327;&#12289;&#31867;&#21035;&#26631;&#31614;&#21644;&#25935;&#24863;&#23646;&#24615;&#30340;&#28508;&#22312;&#20998;&#24067;&#30340;&#35780;&#20998;&#20989;&#25968;&#30340;&#36755;&#20986;&#26469;&#32771;&#34385;&#36825;&#31181;&#26435;&#34913;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#30452;&#25509;&#36890;&#36807;&#20174;&#25968;&#25454;&#38598;&#26412;&#36523;&#26500;&#24314;&#30340;&#31163;&#25955;&#36817;&#20284;&#26469;&#20998;&#26512;&#26368;&#20248;&#36125;&#21494;&#26031;&#20998;&#31867;&#22120;&#22312;&#36825;&#20010;&#28508;&#22312;&#20998;&#24067;&#19978;&#30340;&#34892;&#20026;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#21046;&#23450;&#22810;&#20010;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#20197;&#26356;&#22909;&#22320;&#24179;&#34913;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07724v1 Announce Type: cross  Abstract: Applications that deal with sensitive information may have restrictions placed on the data available to a machine learning (ML) classifier. For example, in some applications, a classifier may not have direct access to sensitive attributes, affecting its ability to produce accurate and fair decisions. This paper proposes a framework that models the trade-off between accuracy and fairness under four practical scenarios that dictate the type of data available for analysis. Prior works examine this trade-off by analyzing the outputs of a scoring function that has been trained to implicitly learn the underlying distribution of the feature vector, class label, and sensitive attribute of a dataset. In contrast, our framework directly analyzes the behavior of the optimal Bayesian classifier on this underlying distribution by constructing a discrete approximation it from the dataset itself. This approach enables us to formulate multiple convex 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#38024;&#23545;&#30446;&#26631;&#20989;&#25968;&#30340;&#27927;&#29260;&#26799;&#24230;&#26041;&#27861;&#26368;&#21518;&#36845;&#20195;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#24357;&#21512;&#20102;&#22312;&#19981;&#21516;&#35774;&#32622;&#20013;&#26368;&#21518;&#36845;&#20195;&#30340;&#33391;&#22909;&#24615;&#33021;&#19982;&#29616;&#26377;&#29702;&#35770;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2403.07723</link><description>&lt;p&gt;
&#20851;&#20110;&#27927;&#29260;&#26799;&#24230;&#26041;&#27861;&#30340;&#26368;&#21518;&#36845;&#20195;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Last-Iterate Convergence of Shuffling Gradient Methods
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07723
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#38024;&#23545;&#30446;&#26631;&#20989;&#25968;&#30340;&#27927;&#29260;&#26799;&#24230;&#26041;&#27861;&#26368;&#21518;&#36845;&#20195;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#24357;&#21512;&#20102;&#22312;&#19981;&#21516;&#35774;&#32622;&#20013;&#26368;&#21518;&#36845;&#20195;&#30340;&#33391;&#22909;&#24615;&#33021;&#19982;&#29616;&#26377;&#29702;&#35770;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27927;&#29260;&#26799;&#24230;&#26041;&#27861;&#65292;&#20063;&#34987;&#31216;&#20026;&#26080;&#26367;&#25442;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#65292;&#22312;&#23454;&#36341;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#65292;&#29305;&#21035;&#21253;&#25324;&#19977;&#31181;&#27969;&#34892;&#31639;&#27861;&#65306;Random Reshuffle&#65288;RR&#65289;&#12289;Shuffle Once&#65288;SO&#65289;&#21644;Incremental Gradient&#65288;IG&#65289;&#12290;&#19982;&#32463;&#39564;&#25104;&#21151;&#30456;&#27604;&#65292;&#38271;&#26399;&#20197;&#26469;&#23545;&#20110;&#27927;&#29260;&#26799;&#24230;&#26041;&#27861;&#30340;&#29702;&#35770;&#20445;&#35777;&#24182;&#19981;&#20805;&#20998;&#20102;&#35299;&#12290;&#26368;&#36817;&#65292;&#21482;&#20026;&#20984;&#20989;&#25968;&#30340;&#24179;&#22343;&#36845;&#20195;&#21644;&#24378;&#20984;&#38382;&#39064;&#30340;&#26368;&#21518;&#36845;&#20195;&#65288;&#20197;&#24179;&#26041;&#36317;&#31163;&#20026;&#24230;&#37327;&#65289;&#24314;&#31435;&#20102;&#25910;&#25947;&#36895;&#29575;&#12290;&#28982;&#32780;&#65292;&#24403;&#23558;&#20989;&#25968;&#20540;&#24046;&#20316;&#20026;&#25910;&#25947;&#20934;&#21017;&#26102;&#65292;&#29616;&#26377;&#29702;&#35770;&#26080;&#27861;&#35299;&#37322;&#22312;&#19981;&#21516;&#35774;&#32622;&#20013;&#65288;&#20363;&#22914;&#21463;&#32422;&#26463;&#30340;&#20248;&#21270;&#65289;&#26368;&#21518;&#36845;&#20195;&#30340;&#33391;&#22909;&#24615;&#33021;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#31181;&#23454;&#36341;&#19982;&#29702;&#35770;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#25105;&#20204;&#38024;&#23545;&#30446;&#26631;&#20989;&#25968;&#35777;&#26126;&#20102;&#27927;&#29260;&#26799;&#24230;&#26041;&#27861;&#26368;&#21518;&#36845;&#20195;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07723v1 Announce Type: new  Abstract: Shuffling gradient methods, which are also known as stochastic gradient descent (SGD) without replacement, are widely implemented in practice, particularly including three popular algorithms: Random Reshuffle (RR), Shuffle Once (SO), and Incremental Gradient (IG). Compared to the empirical success, the theoretical guarantee of shuffling gradient methods was not well-understanding for a long time. Until recently, the convergence rates had just been established for the average iterate for convex functions and the last iterate for strongly convex problems (using squared distance as the metric). However, when using the function value gap as the convergence criterion, existing theories cannot interpret the good performance of the last iterate in different settings (e.g., constrained optimization). To bridge this gap between practice and theory, we prove last-iterate convergence rates for shuffling gradient methods with respect to the objectiv
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#31350;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#29702;&#22312;&#36890;&#36807;web&#27983;&#35272;&#22120;&#19982;&#36719;&#20214;&#20132;&#20114;&#26102;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;WorkArena&#21644;BrowserGym&#20004;&#20010;&#24037;&#20855;&#65292;&#22312;29&#20010;&#20219;&#21153;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#20294;&#20063;&#25581;&#31034;&#20102;&#23454;&#29616;&#23436;&#20840;&#20219;&#21153;&#33258;&#21160;&#21270;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.07718</link><description>&lt;p&gt;
WorkArena&#65306;Web&#20195;&#29702;&#22312;&#35299;&#20915;&#24120;&#35265;&#30693;&#35782;&#24037;&#20316;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#22914;&#20309;&#65311;
&lt;/p&gt;
&lt;p&gt;
WorkArena: How Capable Are Web Agents at Solving Common Knowledge Work Tasks?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07718
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#31350;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#29702;&#22312;&#36890;&#36807;web&#27983;&#35272;&#22120;&#19982;&#36719;&#20214;&#20132;&#20114;&#26102;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;WorkArena&#21644;BrowserGym&#20004;&#20010;&#24037;&#20855;&#65292;&#22312;29&#20010;&#20219;&#21153;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#20294;&#20063;&#25581;&#31034;&#20102;&#23454;&#29616;&#23436;&#20840;&#20219;&#21153;&#33258;&#21160;&#21270;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#29702;&#19982;&#36719;&#20214;&#36890;&#36807;web&#27983;&#35272;&#22120;&#20132;&#20114;&#30340;&#24212;&#29992;&#12290;&#19982;&#20808;&#21069;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#20851;&#27880;&#34913;&#37327;&#36825;&#20123;&#20195;&#29702;&#25191;&#34892;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#36825;&#20123;&#20219;&#21153;&#28085;&#30422;&#20102;&#21033;&#29992;&#20225;&#19994;&#36719;&#20214;&#31995;&#32479;&#30340;&#30693;&#35782;&#24037;&#20316;&#32773;&#30340;&#20856;&#22411;&#26085;&#24120;&#24037;&#20316;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;WorkArena&#65292;&#19968;&#20010;&#22522;&#20110;&#24191;&#27867;&#20351;&#29992;&#30340;ServiceNow&#24179;&#21488;&#30340;29&#20010;&#20219;&#21153;&#30340;&#36828;&#31243;&#20027;&#26426;&#22522;&#20934;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;BrowserGym&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35774;&#35745;&#21644;&#35780;&#20272;&#36825;&#20123;&#20195;&#29702;&#30340;&#29615;&#22659;&#65292;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#34892;&#20026;&#21644;&#22810;&#27169;&#24577;&#35266;&#23519;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35780;&#20272;&#26174;&#31034;&#65292;&#23613;&#31649;&#24403;&#21069;&#30340;&#20195;&#29702;&#22312;WorkArena&#19978;&#34920;&#29616;&#20986;&#20102;&#28508;&#21147;&#65292;&#20294;&#35201;&#23454;&#29616;&#23436;&#20840;&#20219;&#21153;&#33258;&#21160;&#21270;&#20173;&#23384;&#22312;&#30456;&#24403;&#22823;&#30340;&#24046;&#36317;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#24320;&#28304;&#21644;&#38381;&#28304;LLMs&#20043;&#38388;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#31361;&#20986;&#20102;&#26410;&#26469;&#25506;&#32034;&#21644;&#21457;&#23637;&#39046;&#22495;&#30340;&#19968;&#20010;&#37325;&#35201;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07718v1 Announce Type: cross  Abstract: We study the use of large language model-based agents for interacting with software via web browsers. Unlike prior work, we focus on measuring the agents' ability to perform tasks that span the typical daily work of knowledge workers utilizing enterprise software systems. To this end, we propose WorkArena, a remote-hosted benchmark of 29 tasks based on the widely-used ServiceNow platform. We also introduce BrowserGym, an environment for the design and evaluation of such agents, offering a rich set of actions as well as multimodal observations. Our empirical evaluation reveals that while current agents show promise on WorkArena, there remains a considerable gap towards achieving full task automation. Notably, our analysis uncovers a significant performance disparity between open and closed-source LLMs, highlighting a critical area for future exploration and development in the field.
&lt;/p&gt;</description></item><item><title>&#35813;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#30340;&#35299;&#37322;&#65288;FBI&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#35745;&#31639;&#27599;&#20010;&#28857;&#22312;&#29942;&#39048;&#23618;&#20043;&#21069;&#30340;&#29305;&#24449;&#33539;&#25968;&#65292;&#23454;&#29616;&#20102;&#19982;&#24403;&#21069;XAI&#26041;&#27861;&#33267;&#23569;&#19977;&#20010;&#25968;&#37327;&#32423;&#30340;&#36895;&#24230;&#25552;&#21319;&#65292;&#36866;&#29992;&#20110;&#22823;&#22411;&#28857;&#20113;&#25110;&#22823;&#35268;&#27169;&#26550;&#26500;&#12290;</title><link>https://arxiv.org/abs/2403.07706</link><description>&lt;p&gt;
&#28857;&#20113;&#32593;&#32476;&#30340;&#24555;&#36895;&#31616;&#21333;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Fast and Simple Explainability for Point Cloud Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07706
&lt;/p&gt;
&lt;p&gt;
&#35813;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#30340;&#35299;&#37322;&#65288;FBI&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#35745;&#31639;&#27599;&#20010;&#28857;&#22312;&#29942;&#39048;&#23618;&#20043;&#21069;&#30340;&#29305;&#24449;&#33539;&#25968;&#65292;&#23454;&#29616;&#20102;&#19982;&#24403;&#21069;XAI&#26041;&#27861;&#33267;&#23569;&#19977;&#20010;&#25968;&#37327;&#32423;&#30340;&#36895;&#24230;&#25552;&#21319;&#65292;&#36866;&#29992;&#20110;&#22823;&#22411;&#28857;&#20113;&#25110;&#22823;&#35268;&#27169;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#28857;&#20113;&#25968;&#25454;&#30340;&#24555;&#36895;&#31616;&#21333;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26041;&#27861;&#12290;&#23427;&#35745;&#31639;&#20102;&#30456;&#23545;&#20110;&#24050;&#32463;&#35757;&#32451;&#22909;&#30340;&#32593;&#32476;&#19979;&#28216;&#20219;&#21153;&#30340;&#27599;&#20010;&#28857;&#30340;&#37325;&#35201;&#24615;&#12290;&#36825;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#29702;&#35299;&#32593;&#32476;&#30340;&#29305;&#24615;&#65292;&#23545;&#20110;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#38500;&#20102;&#35843;&#35797;&#21644;&#21487;&#35270;&#21270;&#20043;&#22806;&#65292;&#25105;&#20204;&#30340;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#26377;&#21161;&#20110;&#22312;&#32447;&#21453;&#39304;&#21040;&#32593;&#32476;&#36827;&#34892;&#25512;&#26029;&#12290;&#36825;&#21487;&#20197;&#29992;&#20110;&#20943;&#23569;&#19981;&#30830;&#23450;&#24615;&#24182;&#25552;&#39640;&#40065;&#26834;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;&#22522;&#20110;&#29305;&#24449;&#30340;&#35299;&#37322;&#8221;&#65288;FBI&#65289;&#65292;&#22312;&#29942;&#39048;&#23618;&#20043;&#21069;&#35745;&#31639;&#27599;&#20010;&#28857;&#30340;&#29305;&#24449;&#33539;&#25968;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#26799;&#24230;&#30340;&#20351;&#29992;&#20197;&#21450;&#21518;&#29942;&#39048;&#21644;&#21069;&#29942;&#39048;&#31574;&#30053;&#65292;&#32467;&#26524;&#26174;&#31034;&#21069;&#29942;&#39048;&#26356;&#21463;&#38738;&#30544;&#65292;&#20174;&#24179;&#28369;&#24230;&#21644;&#25490;&#21517;&#35282;&#24230;&#26469;&#30475;&#12290;&#19982;&#24403;&#21069;&#30340;XAI&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#33267;&#23569;&#19977;&#20010;&#25968;&#37327;&#32423;&#30340;&#36895;&#24230;&#25552;&#21319;&#65292;&#22240;&#27492;&#36866;&#29992;&#20110;&#22823;&#22411;&#28857;&#20113;&#25110;&#22823;&#35268;&#27169;&#26550;&#26500;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;SOTA&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07706v1 Announce Type: cross  Abstract: We propose a fast and simple explainable AI (XAI) method for point cloud data. It computes pointwise importance with respect to a trained network downstream task. This allows better understanding of the network properties, which is imperative for safety-critical applications. In addition to debugging and visualization, our low computational complexity facilitates online feedback to the network at inference. This can be used to reduce uncertainty and to increase robustness. In this work, we introduce \emph{Feature Based Interpretability} (FBI), where we compute the features' norm, per point, before the bottleneck. We analyze the use of gradients and post- and pre-bottleneck strategies, showing pre-bottleneck is preferred, in terms of smoothness and ranking. We obtain at least three orders of magnitude speedup, compared to current XAI methods, thus, scalable for big point clouds or large-scale architectures. Our approach achieves SOTA re
&lt;/p&gt;</description></item><item><title>&#23545;&#31216; Q-learning&#26041;&#27861;&#36890;&#36807;&#28155;&#21152;&#21512;&#25104;&#22122;&#22768;&#26469;&#20943;&#23569;&#36125;&#23572;&#26364;&#35823;&#24046;&#30340;&#20559;&#26012;&#65292;&#22312;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#25552;&#39640;&#20102;&#26679;&#26412;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.07704</link><description>&lt;p&gt;
&#23545;&#31216; Q-learning&#65306;&#20943;&#23569;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#36125;&#23572;&#26364;&#35823;&#24046;&#30340;&#20559;&#26012;
&lt;/p&gt;
&lt;p&gt;
Symmetric Q-learning: Reducing Skewness of Bellman Error in Online Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07704
&lt;/p&gt;
&lt;p&gt;
&#23545;&#31216; Q-learning&#26041;&#27861;&#36890;&#36807;&#28155;&#21152;&#21512;&#25104;&#22122;&#22768;&#26469;&#20943;&#23569;&#36125;&#23572;&#26364;&#35823;&#24046;&#30340;&#20559;&#26012;&#65292;&#22312;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#25552;&#39640;&#20102;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#20272;&#35745;&#20540;&#20989;&#25968;&#20197;&#35780;&#20272;&#29366;&#24577;&#21644;&#21160;&#20316;&#30340;&#36136;&#37327;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#35813;&#20540;&#20989;&#25968;&#36890;&#24120;&#20351;&#29992;&#26368;&#23567;&#20108;&#20056;&#27861;&#36827;&#34892;&#35757;&#32451;&#65292;&#38544;&#21547;&#22320;&#20551;&#23450;&#19968;&#20010;&#39640;&#26031;&#35823;&#24046;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#30001;&#20110;&#36125;&#23572;&#26364;&#31639;&#23376;&#30340;&#29305;&#24615;&#65292;&#29992;&#20110;&#35757;&#32451;&#20540;&#20989;&#25968;&#30340;&#35823;&#24046;&#20998;&#24067;&#36890;&#24120;&#26159;&#20542;&#26012;&#30340;&#65292;&#36829;&#21453;&#20102;&#26368;&#23567;&#20108;&#20056;&#27861;&#20013;&#23545;&#27491;&#24577;&#35823;&#24046;&#20998;&#24067;&#30340;&#38544;&#21547;&#20551;&#35774;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#23545;&#31216; Q-learning &#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#20174;&#38646;&#22343;&#20540;&#20998;&#24067;&#29983;&#25104;&#30340;&#21512;&#25104;&#22122;&#22768;&#34987;&#28155;&#21152;&#21040;&#30446;&#26631;&#20540;&#20013;&#65292;&#20197;&#29983;&#25104;&#39640;&#26031;&#35823;&#24046;&#20998;&#24067;&#12290;&#25105;&#20204;&#22312;MuJoCo&#30340;&#36830;&#32493;&#25511;&#21046;&#22522;&#20934;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20943;&#23569;&#38169;&#35823;&#20998;&#24067;&#30340;&#20559;&#26012;&#65292;&#23427;&#25552;&#39640;&#20102;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07704v1 Announce Type: cross  Abstract: In deep reinforcement learning, estimating the value function to evaluate the quality of states and actions is essential. The value function is often trained using the least squares method, which implicitly assumes a Gaussian error distribution. However, a recent study suggested that the error distribution for training the value function is often skewed because of the properties of the Bellman operator, and violates the implicit assumption of normal error distribution in the least squares method. To address this, we proposed a method called Symmetric Q-learning, in which the synthetic noise generated from a zero-mean distribution is added to the target values to generate a Gaussian error distribution. We evaluated the proposed method on continuous control benchmark tasks in MuJoCo. It improved the sample efficiency of a state-of-the-art reinforcement learning method by reducing the skewness of the error distribution.
&lt;/p&gt;</description></item><item><title>&#37325;&#26032;&#35780;&#20272;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#27515;&#20129;&#31070;&#32463;&#20803;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;Demon Pruning&#65288;DemP&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#25511;&#21046;&#27515;&#20129;&#31070;&#32463;&#20803;&#30340;&#20135;&#29983;&#65292;&#21160;&#24577;&#23454;&#29616;&#32593;&#32476;&#31232;&#30095;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.07688</link><description>&lt;p&gt;
Maxwell&#30340;&#24694;&#39764;&#20043;&#24037;&#20316;&#65306;&#36890;&#36807;&#21033;&#29992;&#31070;&#32463;&#20803;&#39281;&#21644;&#23454;&#29616;&#26377;&#25928;&#20462;&#21098;
&lt;/p&gt;
&lt;p&gt;
Maxwell's Demon at Work: Efficient Pruning by Leveraging Saturation of Neurons
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07688
&lt;/p&gt;
&lt;p&gt;
&#37325;&#26032;&#35780;&#20272;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#27515;&#20129;&#31070;&#32463;&#20803;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;Demon Pruning&#65288;DemP&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#25511;&#21046;&#27515;&#20129;&#31070;&#32463;&#20803;&#30340;&#20135;&#29983;&#65292;&#21160;&#24577;&#23454;&#29616;&#32593;&#32476;&#31232;&#30095;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26102;&#65292;$\textit{&#27515;&#20129;&#31070;&#32463;&#20803;}$&#29616;&#35937;&#8212;&#8212;&#22312;&#35757;&#32451;&#26399;&#38388;&#21464;&#24471;&#19981;&#27963;&#36291;&#25110;&#39281;&#21644;&#65292;&#36755;&#20986;&#20026;&#38646;&#30340;&#21333;&#20803;&#8212;&#20256;&#32479;&#19978;&#34987;&#35270;&#20026;&#19981;&#21487;&#21462;&#30340;&#65292;&#19982;&#20248;&#21270;&#25361;&#25112;&#26377;&#20851;&#65292;&#24182;&#23548;&#33268;&#22312;&#19981;&#26029;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#20007;&#22833;&#21487;&#22609;&#24615;&#12290;&#26412;&#25991;&#37325;&#26032;&#35780;&#20272;&#20102;&#36825;&#19968;&#29616;&#35937;&#65292;&#19987;&#27880;&#20110;&#31232;&#30095;&#24615;&#21644;&#20462;&#21098;&#12290;&#36890;&#36807;&#31995;&#32479;&#22320;&#25506;&#32034;&#21508;&#31181;&#36229;&#21442;&#25968;&#37197;&#32622;&#23545;&#27515;&#20129;&#31070;&#32463;&#20803;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#23427;&#20204;&#26377;&#21161;&#20110;&#20419;&#36827;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#32467;&#26500;&#21270;&#20462;&#21098;&#31639;&#27861;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;$\textit{Demon Pruning}$&#65288;DemP&#65289;&#65292;&#19968;&#31181;&#25511;&#21046;&#27515;&#20129;&#31070;&#32463;&#20803;&#25193;&#24352;&#65292;&#21160;&#24577;&#23548;&#33268;&#32593;&#32476;&#31232;&#30095;&#24615;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#27963;&#36291;&#21333;&#20803;&#19978;&#27880;&#20837;&#22122;&#22768;&#21644;&#37319;&#29992;&#21333;&#21608;&#26399;&#35843;&#24230;&#27491;&#21017;&#21270;&#31574;&#30053;&#30340;&#32452;&#21512;&#65292;DemP&#22240;&#20854;&#31616;&#21333;&#24615;&#21644;&#24191;&#27867;&#36866;&#29992;&#24615;&#32780;&#33073;&#39062;&#32780;&#20986;&#12290;&#22312;CIFAR10&#19978;&#30340;&#23454;&#39564;&#20013;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07688v1 Announce Type: cross  Abstract: When training deep neural networks, the phenomenon of $\textit{dying neurons}$ $\unicode{x2013}$units that become inactive or saturated, output zero during training$\unicode{x2013}$ has traditionally been viewed as undesirable, linked with optimization challenges, and contributing to plasticity loss in continual learning scenarios. In this paper, we reassess this phenomenon, focusing on sparsity and pruning. By systematically exploring the impact of various hyperparameter configurations on dying neurons, we unveil their potential to facilitate simple yet effective structured pruning algorithms. We introduce $\textit{Demon Pruning}$ (DemP), a method that controls the proliferation of dead neurons, dynamically leading to network sparsity. Achieved through a combination of noise injection on active units and a one-cycled schedule regularization strategy, DemP stands out for its simplicity and broad applicability. Experiments on CIFAR10 an
&lt;/p&gt;</description></item><item><title>&#26426;&#22120;&#23398;&#20064;&#22312;&#36275;&#29699;&#27604;&#36187;&#32467;&#26524;&#39044;&#27979;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#30446;&#21069;&#26368;&#20339;&#34920;&#29616;&#30340;&#27169;&#22411;&#26159;&#26799;&#24230;&#25552;&#21319;&#26641;&#27169;&#22411;&#65288;&#22914;CatBoost&#65289;&#24212;&#29992;&#20110;&#36275;&#29699;&#29305;&#23450;&#35780;&#20998;&#65292;&#20294;&#20173;&#38656;&#36827;&#19968;&#27493;&#27604;&#36739;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.07669</link><description>&lt;p&gt;
&#29992;&#20110;&#36275;&#29699;&#27604;&#36187;&#32467;&#26524;&#39044;&#27979;&#30340;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Machine Learning for Soccer Match Result Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07669
&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#36275;&#29699;&#27604;&#36187;&#32467;&#26524;&#39044;&#27979;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#30446;&#21069;&#26368;&#20339;&#34920;&#29616;&#30340;&#27169;&#22411;&#26159;&#26799;&#24230;&#25552;&#21319;&#26641;&#27169;&#22411;&#65288;&#22914;CatBoost&#65289;&#24212;&#29992;&#20110;&#36275;&#29699;&#29305;&#23450;&#35780;&#20998;&#65292;&#20294;&#20173;&#38656;&#36827;&#19968;&#27493;&#27604;&#36739;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#39044;&#27979;&#36275;&#29699;&#27604;&#36187;&#32467;&#26524;&#30340;&#24120;&#29992;&#26041;&#27861;&#65292;&#36825;&#19968;&#39046;&#22495;&#30340;&#25991;&#29486;&#25968;&#37327;&#22312;&#36807;&#21435;&#21313;&#20116;&#24180;&#20013;&#22823;&#24133;&#22686;&#38271;&#12290;&#26412;&#31456;&#35752;&#35770;&#20102;&#21487;&#29992;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#31867;&#22411;&#21644;&#29305;&#24449;&#65292;&#20197;&#21450;&#35780;&#20272;&#35813;&#24212;&#29992;&#39046;&#22495;&#20013;&#27169;&#22411;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;&#26412;&#31456;&#26088;&#22312;&#24191;&#27867;&#27010;&#36848;&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#29992;&#20110;&#36275;&#29699;&#27604;&#36187;&#32467;&#26524;&#39044;&#27979;&#30340;&#29616;&#29366;&#21644;&#28508;&#22312;&#26410;&#26469;&#21457;&#23637;&#65292;&#20379;&#26377;&#20852;&#36259;&#22312;&#35813;&#39046;&#22495;&#24320;&#23637;&#26410;&#26469;&#30740;&#31350;&#30340;&#20154;&#21442;&#32771;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#21457;&#29616;&#26159;&#65292;&#34429;&#28982;&#26799;&#24230;&#25552;&#21319;&#26641;&#27169;&#22411;&#65288;&#22914;CatBoost&#65289;&#24212;&#29992;&#20110;&#35832;&#22914;pi-&#35780;&#20998;&#20043;&#31867;&#30340;&#36275;&#29699;&#29305;&#23450;&#35780;&#20998;&#65292;&#30446;&#21069;&#26159;&#22312;&#21482;&#21253;&#21547;&#36827;&#29699;&#20316;&#20026;&#27604;&#36187;&#29305;&#24449;&#30340;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#26368;&#20339;&#30340;&#27169;&#22411;&#65292;&#20294;&#38656;&#35201;&#23545;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#30340;&#24615;&#33021;&#36827;&#34892;&#26356;&#24443;&#24213;&#30340;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07669v1 Announce Type: new  Abstract: Machine learning has become a common approach to predicting the outcomes of soccer matches, and the body of literature in this domain has grown substantially in the past decade and a half. This chapter discusses available datasets, the types of models and features, and ways of evaluating model performance in this application domain. The aim of this chapter is to give a broad overview of the current state and potential future developments in machine learning for soccer match results prediction, as a resource for those interested in conducting future studies in the area. Our main findings are that while gradient-boosted tree models such as CatBoost, applied to soccer-specific ratings such as pi-ratings, are currently the best-performing models on datasets containing only goals as the match features, there needs to be a more thorough comparison of the performance of deep learning models and Random Forest on a range of datasets with differen
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#36125;&#21494;&#26031;&#31070;&#32463;&#22330;&#65288;BayesNF&#65289;&#65292;&#32467;&#21512;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#20998;&#23618;&#36125;&#21494;&#26031;&#25512;&#26029;&#65292;&#29992;&#20110;&#22788;&#29702;&#22823;&#35268;&#27169;&#26102;&#31354;&#39044;&#27979;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.07657</link><description>&lt;p&gt;
&#20351;&#29992;&#36125;&#21494;&#26031;&#31070;&#32463;&#22330;&#36827;&#34892;&#21487;&#25193;&#23637;&#30340;&#26102;&#31354;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Scalable Spatiotemporal Prediction with Bayesian Neural Fields
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07657
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#36125;&#21494;&#26031;&#31070;&#32463;&#22330;&#65288;BayesNF&#65289;&#65292;&#32467;&#21512;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#20998;&#23618;&#36125;&#21494;&#26031;&#25512;&#26029;&#65292;&#29992;&#20110;&#22788;&#29702;&#22823;&#35268;&#27169;&#26102;&#31354;&#39044;&#27979;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#31354;&#25968;&#25454;&#38598;&#30001;&#31354;&#38388;&#21442;&#32771;&#30340;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#65292;&#24191;&#27867;&#24212;&#29992;&#20110;&#35768;&#22810;&#31185;&#23398;&#21644;&#21830;&#19994;&#26234;&#33021;&#39046;&#22495;&#65292;&#20363;&#22914;&#31354;&#27668;&#27745;&#26579;&#30417;&#27979;&#65292;&#30142;&#30149;&#36319;&#36394;&#21644;&#20113;&#38656;&#27714;&#39044;&#27979;&#12290;&#38543;&#30528;&#29616;&#20195;&#25968;&#25454;&#38598;&#35268;&#27169;&#21644;&#22797;&#26434;&#24615;&#30340;&#19981;&#26029;&#22686;&#21152;&#65292;&#38656;&#35201;&#26032;&#30340;&#32479;&#35745;&#26041;&#27861;&#26469;&#25429;&#25417;&#22797;&#26434;&#30340;&#26102;&#31354;&#21160;&#24577;&#24182;&#22788;&#29702;&#22823;&#35268;&#27169;&#39044;&#27979;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;Bayesian Neural Field (BayesNF)&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#25512;&#26029;&#26102;&#31354;&#22495;&#19978;&#20016;&#23500;&#27010;&#29575;&#20998;&#24067;&#30340;&#36890;&#29992;&#39046;&#22495;&#32479;&#35745;&#27169;&#22411;&#65292;&#21487;&#29992;&#20110;&#21253;&#25324;&#39044;&#27979;&#12289;&#25554;&#20540;&#21644;&#21464;&#24322;&#20998;&#26512;&#22312;&#20869;&#30340;&#25968;&#25454;&#20998;&#26512;&#20219;&#21153;&#12290;BayesNF&#23558;&#29992;&#20110;&#39640;&#23481;&#37327;&#20989;&#25968;&#20272;&#35745;&#30340;&#26032;&#22411;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#19982;&#29992;&#20110;&#40065;&#26834;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#20998;&#23618;&#36125;&#21494;&#26031;&#25512;&#26029;&#30456;&#32467;&#21512;&#12290;&#36890;&#36807;&#22312;&#23450;&#20041;&#20808;&#39564;&#20998;&#24067;&#26041;&#38754;&#36827;&#34892;&#24207;&#21015;&#21270;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07657v1 Announce Type: cross  Abstract: Spatiotemporal datasets, which consist of spatially-referenced time series, are ubiquitous in many scientific and business-intelligence applications, such as air pollution monitoring, disease tracking, and cloud-demand forecasting. As modern datasets continue to increase in size and complexity, there is a growing need for new statistical methods that are flexible enough to capture complex spatiotemporal dynamics and scalable enough to handle large prediction problems. This work presents the Bayesian Neural Field (BayesNF), a domain-general statistical model for inferring rich probability distributions over a spatiotemporal domain, which can be used for data-analysis tasks including forecasting, interpolation, and variography. BayesNF integrates a novel deep neural network architecture for high-capacity function estimation with hierarchical Bayesian inference for robust uncertainty quantification. By defining the prior through a sequenc
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21160;&#24577;&#36873;&#25321;&#19987;&#23478;&#26469;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#21644;&#27169;&#22411;&#24615;&#33021;&#65292;&#38024;&#23545;&#19981;&#21516;&#38590;&#24230;&#30340;&#20219;&#21153;&#28608;&#27963;&#19981;&#21516;&#25968;&#37327;&#30340;&#19987;&#23478;&#65292;&#30456;&#27604;&#20256;&#32479;&#30340;Top-K&#36335;&#30001;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#21160;&#24577;&#36335;&#30001;&#26041;&#27861;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26126;&#26174;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2403.07652</link><description>&lt;p&gt;
&#36739;&#22256;&#38590;&#30340;&#20219;&#21153;&#38656;&#35201;&#26356;&#22810;&#19987;&#23478;&#65306;MoE&#27169;&#22411;&#20013;&#30340;&#21160;&#24577;&#36335;&#30001;
&lt;/p&gt;
&lt;p&gt;
Harder Tasks Need More Experts: Dynamic Routing in MoE Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07652
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21160;&#24577;&#36873;&#25321;&#19987;&#23478;&#26469;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#21644;&#27169;&#22411;&#24615;&#33021;&#65292;&#38024;&#23545;&#19981;&#21516;&#38590;&#24230;&#30340;&#20219;&#21153;&#28608;&#27963;&#19981;&#21516;&#25968;&#37327;&#30340;&#19987;&#23478;&#65292;&#30456;&#27604;&#20256;&#32479;&#30340;Top-K&#36335;&#30001;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#21160;&#24577;&#36335;&#30001;&#26041;&#27861;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26126;&#26174;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21160;&#24577;&#19987;&#23478;&#36873;&#25321;&#26694;&#26550;&#65292;&#29992;&#20110;Mixture of Experts&#65288;MoE&#65289;&#27169;&#22411;&#65292;&#26088;&#22312;&#36890;&#36807;&#26681;&#25454;&#36755;&#20837;&#38590;&#24230;&#35843;&#25972;&#28608;&#27963;&#30340;&#19987;&#23478;&#25968;&#37327;&#65292;&#22686;&#24378;&#35745;&#31639;&#25928;&#29575;&#21644;&#27169;&#22411;&#24615;&#33021;&#12290;&#19982;&#20381;&#36182;&#20110;&#22266;&#23450;Top-K&#36335;&#30001;&#30340;&#20256;&#32479;MoE&#26041;&#27861;&#19981;&#21516;&#65292;&#35813;&#26041;&#27861;&#26681;&#25454;&#23545;&#27599;&#20010;&#36755;&#20837;&#30340;&#19987;&#23478;&#36873;&#25321;&#30340;&#32622;&#20449;&#27700;&#24179;&#21160;&#24577;&#36873;&#25321;&#19987;&#23478;&#12290;&#36825;&#20801;&#35768;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#35745;&#31639;&#36164;&#28304;&#65292;&#23545;&#38656;&#35201;&#39640;&#32423;&#25512;&#29702;&#30340;&#22797;&#26434;&#20219;&#21153;&#28608;&#27963;&#26356;&#22810;&#30340;&#19987;&#23478;&#65292;&#23545;&#36739;&#31616;&#21333;&#30340;&#20219;&#21153;&#28608;&#27963;&#26356;&#23569;&#30340;&#19987;&#23478;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#30340;&#21160;&#24577;&#36335;&#30001;&#26041;&#27861;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#26126;&#26174;&#30340;&#25913;&#36827;&#65292;&#19982;&#24120;&#35268;Top-2&#36335;&#30001;&#30456;&#27604;&#65292;&#23454;&#29616;&#20102;&#24179;&#22343;&#25913;&#36827;0.7%&#30340;&#25928;&#26524;&#65292;&#19988;&#28608;&#27963;&#21442;&#25968;&#23569;&#20110;90%&#12290;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#26174;&#31034;&#25105;&#20204;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07652v1 Announce Type: cross  Abstract: In this paper, we introduce a novel dynamic expert selection framework for Mixture of Experts (MoE) models, aiming to enhance computational efficiency and model performance by adjusting the number of activated experts based on input difficulty. Unlike traditional MoE approaches that rely on fixed Top-K routing, which activates a predetermined number of experts regardless of the input's complexity, our method dynamically selects experts based on the confidence level in expert selection for each input. This allows for a more efficient utilization of computational resources, activating more experts for complex tasks requiring advanced reasoning and fewer for simpler tasks. Through extensive evaluations, our dynamic routing method demonstrates substantial improvements over conventional Top-2 routing across various benchmarks, achieving an average improvement of 0.7% with less than 90% activated parameters. Further analysis shows our model 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#21457;&#24037;&#20316;&#36127;&#36733;&#36827;&#34892;&#20102;&#28145;&#20837;&#29305;&#24449;&#21270;&#30740;&#31350;&#65292;&#21457;&#29616;&#20102;&#19982;&#20808;&#21069;&#20219;&#21153;&#29305;&#23450;&#28145;&#24230;&#23398;&#20064;&#24037;&#20316;&#36127;&#36733;&#30340;&#24046;&#24322;&#65292;&#25506;&#32034;&#20102;&#36164;&#28304;&#21033;&#29992;&#27169;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#20248;&#21270;&#31995;&#32479;&#20197;&#36866;&#24212;LLMs&#30340;&#28508;&#22312;&#26426;&#20250;&#12290;</title><link>https://arxiv.org/abs/2403.07648</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#25454;&#20013;&#24515;&#24320;&#21457;&#30340;&#29305;&#24449;&#21270;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Characterization of Large Language Model Development in the Datacenter
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07648
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#21457;&#24037;&#20316;&#36127;&#36733;&#36827;&#34892;&#20102;&#28145;&#20837;&#29305;&#24449;&#21270;&#30740;&#31350;&#65292;&#21457;&#29616;&#20102;&#19982;&#20808;&#21069;&#20219;&#21153;&#29305;&#23450;&#28145;&#24230;&#23398;&#20064;&#24037;&#20316;&#36127;&#36733;&#30340;&#24046;&#24322;&#65292;&#25506;&#32034;&#20102;&#36164;&#28304;&#21033;&#29992;&#27169;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#20248;&#21270;&#31995;&#32479;&#20197;&#36866;&#24212;LLMs&#30340;&#28508;&#22312;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22810;&#20010;&#38761;&#21629;&#24615;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#35201;&#26377;&#25928;&#21033;&#29992;&#22823;&#35268;&#27169;&#38598;&#32676;&#36164;&#28304;&#26469;&#24320;&#21457;LLMs&#24182;&#38750;&#26131;&#20107;&#65292;&#32463;&#24120;&#38754;&#20020;&#35832;&#22810;&#25361;&#25112;&#65292;&#22914;&#39057;&#32321;&#30340;&#30828;&#20214;&#25925;&#38556;&#12289;&#22797;&#26434;&#30340;&#24182;&#34892;&#21270;&#31574;&#30053;&#21644;&#36164;&#28304;&#21033;&#29992;&#19981;&#24179;&#34913;&#12290;&#26412;&#25991;&#23545;&#25105;&#20204;&#30340;GPU&#25968;&#25454;&#20013;&#24515;Acme&#20013;&#25910;&#38598;&#30340;&#20026;&#26399;&#20845;&#20010;&#26376;&#30340;LLM&#24320;&#21457;&#24037;&#20316;&#36127;&#36733;&#36319;&#36394;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#29305;&#24449;&#21270;&#30740;&#31350;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;LLMs&#19982;&#20808;&#21069;&#20219;&#21153;&#29305;&#23450;&#30340;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#24037;&#20316;&#36127;&#36733;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#25506;&#32034;&#20102;&#36164;&#28304;&#21033;&#29992;&#27169;&#24335;&#65292;&#24182;&#30830;&#23450;&#20102;&#21508;&#31181;&#20316;&#19994;&#25925;&#38556;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#24635;&#32467;&#20102;&#25105;&#20204;&#36935;&#21040;&#30340;&#38556;&#30861;&#65292;&#24182;&#21457;&#29616;&#20102;&#20248;&#21270;&#19987;&#20026;LLMs&#23450;&#21046;&#30340;&#31995;&#32479;&#30340;&#28508;&#22312;&#26426;&#20250;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#25105;&#20204;&#30340;&#31995;&#32479;&#21162;&#21147;&#65306;&#65288;1&#65289;&#23481;&#38169;&#39044;&#35757;&#32451;&#65292;&#36890;&#36807;LLM&#21442;&#19982;&#26469;&#22686;&#24378;&#23481;&#38169;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07648v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have presented impressive performance across several transformative tasks. However, it is non-trivial to efficiently utilize large-scale cluster resources to develop LLMs, often riddled with numerous challenges such as frequent hardware failures, intricate parallelization strategies, and imbalanced resource utilization. In this paper, we present an in-depth characterization study of a six-month LLM development workload trace collected from our GPU datacenter Acme. Specifically, we investigate discrepancies between LLMs and prior task-specific Deep Learning (DL) workloads, explore resource utilization patterns, and identify the impact of various job failures. Our analysis summarizes hurdles we encountered and uncovers potential opportunities to optimize systems tailored for LLMs. Furthermore, we introduce our system efforts: (1) fault-tolerant pretraining, which enhances fault tolerance through LLM-involved 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;CardioGenAI&#65292;&#19968;&#20010;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20943;&#23569;&#33647;&#29289;&#30340;hERG&#27963;&#24615;&#24182;&#20445;&#30041;&#33647;&#29702;&#27963;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.07632</link><description>&lt;p&gt;
CardioGenAI&#65306;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26694;&#26550;&#29992;&#20110;&#20943;&#23569;hERG&#27602;&#24615;&#30340;&#33647;&#29289;&#20877;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
CardioGenAI: A Machine Learning-Based Framework for Re-Engineering Drugs for Reduced hERG Liability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07632
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;CardioGenAI&#65292;&#19968;&#20010;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20943;&#23569;&#33647;&#29289;&#30340;hERG&#27963;&#24615;&#24182;&#20445;&#30041;&#33647;&#29702;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;&#35825;&#23548;&#30340;&#24515;&#33039;&#27602;&#24615;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#20581;&#24247;&#38382;&#39064;&#65292;&#21487;&#33021;&#23548;&#33268;&#20005;&#37325;&#19981;&#33391;&#21453;&#24212;&#65292;&#21253;&#25324;&#36890;&#36807;&#38459;&#28382;&#30005;&#21387;&#38376;&#25511;&#30340;hERG&#38078;&#31163;&#23376;&#36890;&#36947;&#23548;&#33268;&#29983;&#21629;&#23041;&#32961;&#30340;&#24515;&#24459;&#22833;&#24120;&#12290;&#22240;&#27492;&#65292;&#24320;&#21457;&#26089;&#26399;&#38454;&#27573;&#37492;&#23450;hERG&#27963;&#24615;&#21270;&#21512;&#29289;&#30340;&#20808;&#36827;&#26041;&#27861;&#65292;&#20197;&#21450;&#20248;&#21270;&#21830;&#19994;&#21270;&#33647;&#29289;&#20197;&#20943;&#23569;hERG&#27963;&#24615;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CardioGenAI&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20877;&#35774;&#35745;&#24320;&#21457;&#20013;&#21644;&#24050;&#19978;&#24066;&#33647;&#29289;&#65292;&#20197;&#20943;&#23569;hERG&#27963;&#24615;&#21516;&#26102;&#20445;&#30041;&#20854;&#33647;&#29702;&#27963;&#24615;&#12290;&#35813;&#26694;&#26550;&#32467;&#21512;&#20102;&#29992;&#20110;&#39044;&#27979;hERG&#36890;&#36947;&#27963;&#24615;&#30340;&#26368;&#26032;&#21028;&#21035;&#27169;&#22411;&#65292;&#20197;&#21450;&#38048;&#31163;&#23376;&#36890;&#36947;NaV1.5&#21644;&#38041;&#31163;&#23376;&#36890;&#36947;CaV1.2&#27963;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#35843;&#33410;&#30001;hERG&#36890;&#36947;&#38459;&#28382;&#24341;&#36215;&#30340;&#24515;&#24459;&#22833;&#24120;&#28508;&#22312;&#24433;&#21709;&#20013;&#20855;&#26377;&#28508;&#22312;&#24847;&#20041;&#12290;&#36825;&#20123;&#27169;&#22411;&#36824;&#21487;&#20197;se
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07632v1 Announce Type: new  Abstract: Drug-induced cardiotoxicity is a major health concern which can lead to serious adverse effects including life-threatening cardiac arrhythmias via the blockade of the voltage-gated hERG potassium ion channel. It is therefore of tremendous interest to develop advanced methods to identify hERG-active compounds in early stages of drug development, as well as to optimize commercially available drugs for reduced hERG activity. In this work, we present CardioGenAI, a machine learning-based framework for re-engineering both developmental and marketed drugs for reduced hERG activity while preserving their pharmacological activity. The framework incorporates novel state-of-the-art discriminative models for predicting hERG channel activity, as well as activity against the voltage-gated NaV1.5 and CaV1.2 channels due to their potential implications in modulating the arrhythmogenic potential induced by hERG channel blockade. These models can also se
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26641;-&#29615;&#36335;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#26463;&#25628;&#32034;&#26641;&#19982;&#21508;&#31181;&#23567;&#37096;&#20214;&#30456;&#32467;&#21512;&#65292;&#25552;&#20379;&#20102;&#21487;&#35270;&#21270;&#21644;&#20132;&#20114;&#21487;&#33021;&#24615;&#65292;&#20174;&#32780;&#20998;&#26512;&#12289;&#35299;&#37322;&#21644;&#35843;&#25972;&#29983;&#25104;&#30340;&#36755;&#20986;&#12290;</title><link>https://arxiv.org/abs/2403.07627</link><description>&lt;p&gt;
generAItor: &#26641;-&#29615;&#36335;&#25991;&#26412;&#29983;&#25104;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#36866;&#24212;&#24615;
&lt;/p&gt;
&lt;p&gt;
generAItor: Tree-in-the-Loop Text Generation for Language Model Explainability and Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07627
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26641;-&#29615;&#36335;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#26463;&#25628;&#32034;&#26641;&#19982;&#21508;&#31181;&#23567;&#37096;&#20214;&#30456;&#32467;&#21512;&#65292;&#25552;&#20379;&#20102;&#21487;&#35270;&#21270;&#21644;&#20132;&#20114;&#21487;&#33021;&#24615;&#65292;&#20174;&#32780;&#20998;&#26512;&#12289;&#35299;&#37322;&#21644;&#35843;&#25972;&#29983;&#25104;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24191;&#27867;&#37096;&#32626;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#65292;&#20363;&#22914;&#33258;&#21160;&#23436;&#25104;&#12289;&#36741;&#21161;&#20889;&#20316;&#25110;&#22522;&#20110;&#23545;&#35805;&#30340;&#25991;&#26412;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#25628;&#32034;&#31639;&#27861;&#30340;&#36755;&#20986;&#20505;&#36873;&#32467;&#26524;&#34987;&#36739;&#23569;&#25506;&#32034;&#21644;&#35299;&#37322;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26641;-&#29615;&#36335;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#19968;&#19981;&#36275;&#65292;&#20854;&#20013;&#26463;&#25628;&#32034;&#26641;&#30340;&#21487;&#35270;&#21270;&#34920;&#31034;&#26159;&#20998;&#26512;&#12289;&#35299;&#37322;&#21644;&#35843;&#25972;&#29983;&#25104;&#30340;&#36755;&#20986;&#30340;&#26680;&#24515;&#32452;&#20214;&#12290;&#20026;&#25903;&#25345;&#36825;&#20123;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;generAItor&#65292;&#19968;&#31181;&#35270;&#35273;&#20998;&#26512;&#25216;&#26415;&#65292;&#23558;&#20013;&#24515;&#26463;&#25628;&#32034;&#26641;&#19982;&#21508;&#31181;&#29305;&#23450;&#20110;&#20219;&#21153;&#30340;&#23567;&#37096;&#20214;&#30456;&#32467;&#21512;&#65292;&#25552;&#20379;&#26377;&#38024;&#23545;&#24615;&#30340;&#21487;&#35270;&#21270;&#21644;&#20132;&#20114;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#22312;&#22810;&#20010;&#23618;&#38754;&#36827;&#34892;&#20132;&#20114;&#65292;&#24182;&#25552;&#20379;&#19968;&#20010;&#36845;&#20195;&#27969;&#31243;&#65292;&#21253;&#25324;&#29983;&#25104;&#12289;&#25506;&#32034;&#21644;&#27604;&#36739;&#36755;&#20986;&#30340;&#20505;&#36873;&#32467;&#26524;&#65292;&#20197;&#21450;&#26681;&#25454;&#36866;&#24212;&#25968;&#25454;&#24494;&#35843;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26696;&#20363;&#30740;&#31350;&#34920;&#26126;&#25105;&#20204;&#30340;&#24037;&#20855; generat
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07627v1 Announce Type: cross  Abstract: Large language models (LLMs) are widely deployed in various downstream tasks, e.g., auto-completion, aided writing, or chat-based text generation. However, the considered output candidates of the underlying search algorithm are under-explored and under-explained. We tackle this shortcoming by proposing a tree-in-the-loop approach, where a visual representation of the beam search tree is the central component for analyzing, explaining, and adapting the generated outputs. To support these tasks, we present generAItor, a visual analytics technique, augmenting the central beam search tree with various task-specific widgets, providing targeted visualizations and interaction possibilities. Our approach allows interactions on multiple levels and offers an iterative pipeline that encompasses generating, exploring, and comparing output candidates, as well as fine-tuning the model based on adapted data. Our case study shows that our tool generat
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#22120;&#36951;&#24536;&#31639;&#27861;&#65292;&#20998;&#21035;&#37319;&#29992;&#37096;&#20998;&#22833;&#24518;&#24335;&#36951;&#24536;&#21644;&#36880;&#23618;&#37096;&#20998;&#26356;&#26032;&#30340;&#26041;&#27861;&#65292;&#20197;&#26356;&#39640;&#25928;&#22320;&#22312;&#35757;&#32451;&#27169;&#22411;&#20013;&#21024;&#38500;&#30693;&#35782;&#12290;</title><link>https://arxiv.org/abs/2403.07611</link><description>&lt;p&gt;
&#36890;&#36807;&#36880;&#23618;&#37096;&#20998;&#26426;&#22120;&#36951;&#24536;&#23454;&#29616;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#26377;&#25928;&#30693;&#35782;&#21024;&#38500;
&lt;/p&gt;
&lt;p&gt;
Efficient Knowledge Deletion from Trained Models through Layer-wise Partial Machine Unlearning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07611
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#22120;&#36951;&#24536;&#31639;&#27861;&#65292;&#20998;&#21035;&#37319;&#29992;&#37096;&#20998;&#22833;&#24518;&#24335;&#36951;&#24536;&#21644;&#36880;&#23618;&#37096;&#20998;&#26356;&#26032;&#30340;&#26041;&#27861;&#65292;&#20197;&#26356;&#39640;&#25928;&#22320;&#22312;&#35757;&#32451;&#27169;&#22411;&#20013;&#21024;&#38500;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07611v1 &#21457;&#34920;&#31867;&#22411;&#65306;cross  &#25688;&#35201;&#65306;&#26426;&#22120;&#36951;&#24536;&#22240;&#20854;&#33021;&#22815;&#26377;&#36873;&#25321;&#22320;&#25830;&#38500;&#24050;&#32463;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#20174;&#29305;&#23450;&#35757;&#32451;&#25968;&#25454;&#26679;&#26412;&#33719;&#24471;&#30340;&#30693;&#35782;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#36825;&#31181;&#33021;&#21147;&#20351;&#25968;&#25454;&#25345;&#26377;&#32773;&#33021;&#22815;&#20005;&#26684;&#36981;&#23432;&#25968;&#25454;&#20445;&#25252;&#27861;&#35268;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#36951;&#24536;&#25216;&#26415;&#38754;&#20020;&#23454;&#38469;&#32422;&#26463;&#65292;&#36890;&#24120;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#65292;&#38656;&#35201;&#36951;&#24536;&#21518;&#36827;&#34892;&#31616;&#30701;&#30340;&#24494;&#35843;&#65292;&#24182;&#38656;&#35201;&#22823;&#37327;&#23384;&#20648;&#31354;&#38388;&#12290;&#20316;&#20026;&#22238;&#24212;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#22120;&#36951;&#24536;&#31639;&#27861;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#26159;&#37096;&#20998;&#22833;&#24518;&#24335;&#36951;&#24536;&#65292;&#23558;&#36880;&#23618;&#20462;&#21098;&#19982;&#22833;&#24518;&#24335;&#36951;&#24536;&#30456;&#32467;&#21512;&#12290;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#65292;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#27169;&#22411;&#36827;&#34892;&#30340;&#26356;&#26032;&#34987;&#20462;&#21098;&#24182;&#23384;&#20648;&#65292;&#38543;&#21518;&#29992;&#20110;&#20174;&#35757;&#32451;&#27169;&#22411;&#20013;&#36951;&#24536;&#29305;&#23450;&#25968;&#25454;&#12290;&#31532;&#20108;&#31181;&#26041;&#27861;&#23558;&#36880;&#23618;&#37096;&#20998;&#26356;&#26032;&#38598;&#25104;&#21040;&#26631;&#31614;&#32763;&#36716;&#21644;&#22522;&#20110;&#20248;&#21270;&#30340;&#36951;&#24536;&#20013;&#65292;&#20197;&#20943;&#36731;&#30001;&#20110;&#36951;&#24536;&#32780;&#20135;&#29983;&#30340;&#19981;&#21033;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07611v1 Announce Type: cross  Abstract: Machine unlearning has garnered significant attention due to its ability to selectively erase knowledge obtained from specific training data samples in an already trained machine learning model. This capability enables data holders to adhere strictly to data protection regulations. However, existing unlearning techniques face practical constraints, often causing performance degradation, demanding brief fine-tuning post unlearning, and requiring significant storage. In response, this paper introduces a novel class of machine unlearning algorithms. First method is partial amnesiac unlearning, integration of layer-wise pruning with amnesiac unlearning. In this method, updates made to the model during training are pruned and stored, subsequently used to forget specific data from trained model. The second method assimilates layer-wise partial-updates into label-flipping and optimization-based unlearning to mitigate the adverse effects of da
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#24182;&#23454;&#29616;&#20102;Couler&#31995;&#32479;&#65292;&#29992;&#20110;&#20113;&#20013;&#32479;&#19968;&#26426;&#22120;&#23398;&#20064;&#24037;&#20316;&#27969;&#20248;&#21270;&#65292;&#20027;&#35201;&#35265;&#35299;&#22312;&#20110;&#33021;&#22815;&#20351;&#29992;&#33258;&#28982;&#29983;&#25104;ML&#24037;&#20316;&#27969;</title><link>https://arxiv.org/abs/2403.07608</link><description>&lt;p&gt;
Couler: &#20113;&#20013;&#32479;&#19968;&#26426;&#22120;&#23398;&#20064;&#24037;&#20316;&#27969;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Couler: Unified Machine Learning Workflow Optimization in Cloud
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07608
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#24182;&#23454;&#29616;&#20102;Couler&#31995;&#32479;&#65292;&#29992;&#20110;&#20113;&#20013;&#32479;&#19968;&#26426;&#22120;&#23398;&#20064;&#24037;&#20316;&#27969;&#20248;&#21270;&#65292;&#20027;&#35201;&#35265;&#35299;&#22312;&#20110;&#33021;&#22815;&#20351;&#29992;&#33258;&#28982;&#29983;&#25104;ML&#24037;&#20316;&#27969;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#24050;&#32463;&#21464;&#24471;&#26080;&#22788;&#19981;&#22312;&#65292;&#25512;&#21160;&#30528;&#21508;&#31181;&#32452;&#32455;&#20013;&#30340;&#25968;&#25454;&#39537;&#21160;&#24212;&#29992;&#12290;&#19982;&#20256;&#32479;&#35266;&#24565;&#20013;&#30740;&#31350;&#20013;&#30340;ML&#30456;&#21453;&#65292;ML&#24037;&#20316;&#27969;&#21487;&#33021;&#26159;&#22797;&#26434;&#30340;&#65292;&#36164;&#28304;&#23494;&#38598;&#30340;&#65292;&#24182;&#19988;&#32791;&#26102;&#30340;&#12290;&#25193;&#23637;ML&#24037;&#20316;&#27969;&#20197;&#21253;&#21547;&#26356;&#24191;&#27867;&#30340;&#25968;&#25454;&#22522;&#30784;&#35774;&#26045;&#21644;&#25968;&#25454;&#31867;&#22411;&#21487;&#33021;&#23548;&#33268;&#26356;&#22823;&#30340;&#24037;&#20316;&#37327;&#21644;&#22686;&#21152;&#30340;&#37096;&#32626;&#25104;&#26412;&#12290;&#30446;&#21069;&#65292;&#26377;&#35768;&#22810;&#24037;&#20316;&#27969;&#24341;&#25806;&#21487;&#29992;&#65288;&#20854;&#20013;&#36229;&#36807;&#21313;&#20010;&#24471;&#21040;&#24191;&#27867;&#35748;&#21487;&#65289;&#12290;&#36825;&#31181;&#22810;&#26679;&#24615;&#23545;&#20110;&#26368;&#32456;&#29992;&#25143;&#26469;&#35828;&#26500;&#25104;&#20102;&#25484;&#25569;&#19981;&#21516;&#24341;&#25806;API&#30340;&#25361;&#25112;&#12290;&#34429;&#28982;&#30446;&#21069;&#30340;&#21162;&#21147;&#20027;&#35201;&#38598;&#20013;&#22312;&#38024;&#23545;&#29305;&#23450;&#24037;&#20316;&#27969;&#24341;&#25806;&#20248;&#21270;ML&#25805;&#20316;&#65288;MLOps&#65289;&#65292;&#20294;&#24403;&#21069;&#26041;&#27861;&#22312;&#36328;&#19981;&#21516;&#24341;&#25806;&#36827;&#34892;&#24037;&#20316;&#27969;&#20248;&#21270;&#26041;&#38754;&#24456;&#22823;&#31243;&#24230;&#19978;&#34987;&#24573;&#35270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07608v1 Announce Type: cross  Abstract: Machine Learning (ML) has become ubiquitous, fueling data-driven applications across various organizations. Contrary to the traditional perception of ML in research, ML workflows can be complex, resource-intensive, and time-consuming. Expanding an ML workflow to encompass a wider range of data infrastructure and data types may lead to larger workloads and increased deployment costs. Currently, numerous workflow engines are available (with over ten being widely recognized). This variety poses a challenge for end-users in terms of mastering different engine APIs. While efforts have primarily focused on optimizing ML Operations (MLOps) for a specific workflow engine, current methods largely overlook workflow optimization across different engines.   In this work, we design and implement Couler, a system designed for unified ML workflow optimization in the cloud. Our main insight lies in the ability to generate an ML workflow using natural 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;NegOpt&#26041;&#27861;&#65292;&#36890;&#36807;&#30417;&#30563;&#24494;&#35843;&#21644;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#36127;&#38754;&#25552;&#31034;&#30340;&#29983;&#25104;&#65292;&#26174;&#33879;&#25552;&#39640;&#22270;&#20687;&#29983;&#25104;&#36136;&#37327;&#65292;&#36229;&#36234;&#20854;&#20182;&#26041;&#27861;&#24182;&#26500;&#24314;&#20102;&#36127;&#38754;&#25552;&#31034;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2403.07605</link><description>&lt;p&gt;
&#20248;&#21270;&#36127;&#38754;&#25552;&#31034;&#20197;&#22686;&#24378;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#30340;&#32654;&#23398;&#21644;&#20445;&#30495;&#24230;
&lt;/p&gt;
&lt;p&gt;
Optimizing Negative Prompts for Enhanced Aesthetics and Fidelity in Text-To-Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07605
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;NegOpt&#26041;&#27861;&#65292;&#36890;&#36807;&#30417;&#30563;&#24494;&#35843;&#21644;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#36127;&#38754;&#25552;&#31034;&#30340;&#29983;&#25104;&#65292;&#26174;&#33879;&#25552;&#39640;&#22270;&#20687;&#29983;&#25104;&#36136;&#37327;&#65292;&#36229;&#36234;&#20854;&#20182;&#26041;&#27861;&#24182;&#26500;&#24314;&#20102;&#36127;&#38754;&#25552;&#31034;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#65292;&#20351;&#29992;&#25551;&#36848;&#19981;&#33391;&#22270;&#20687;&#29305;&#24449;&#30340;&#36127;&#38754;&#25552;&#31034;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22270;&#20687;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#33391;&#22909;&#30340;&#36127;&#38754;&#25552;&#31034;&#26159;&#19968;&#39033;&#25163;&#24037;&#32780;&#32321;&#29712;&#30340;&#24037;&#20316;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;NegOpt&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#30417;&#30563;&#24494;&#35843;&#21644;&#24378;&#21270;&#23398;&#20064;&#26469;&#20248;&#21270;&#36127;&#38754;&#25552;&#31034;&#29983;&#25104;&#65292;&#20174;&#32780;&#22686;&#24378;&#22270;&#20687;&#29983;&#25104;&#12290;&#25105;&#20204;&#30340;&#32508;&#21512;&#26041;&#27861;&#30456;&#23545;&#20110;&#20854;&#20182;&#26041;&#27861;&#22823;&#24133;&#25552;&#39640;&#20102;25%&#30340;Inception Score&#65292;&#24182;&#36229;&#36234;&#20102;&#26469;&#33258;&#27979;&#35797;&#38598;&#30340;&#26631;&#20934;&#36127;&#38754;&#25552;&#31034;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;NegOpt&#65292;&#25105;&#20204;&#21487;&#20197;&#26377;&#36873;&#25321;&#22320;&#20248;&#21270;&#23545;&#25105;&#20204;&#26368;&#37325;&#35201;&#30340;&#25351;&#26631;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#36127;&#38754;&#25552;&#31034;&#25968;&#25454;&#38598;Negative Prompts DB&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07605v1 Announce Type: cross  Abstract: In text-to-image generation, using negative prompts, which describe undesirable image characteristics, can significantly boost image quality. However, producing good negative prompts is manual and tedious. To address this, we propose NegOpt, a novel method for optimizing negative prompt generation toward enhanced image generation, using supervised fine-tuning and reinforcement learning. Our combined approach results in a substantial increase of 25% in Inception Score compared to other approaches and surpasses ground-truth negative prompts from the test set. Furthermore, with NegOpt we can preferentially optimize the metrics most important to us. Finally, we construct Negative Prompts DB, a dataset of negative prompts.
&lt;/p&gt;</description></item><item><title>ProPML&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#27010;&#29575;&#26041;&#27861;&#65292;&#29992;&#20110;&#25193;&#23637;&#20108;&#20803;&#20132;&#21449;&#29109;&#21040;Partial Multi-label Learning&#65288;PML&#65289;&#35774;&#32622;&#20013;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#28145;&#24230;&#26550;&#26500;&#65292;&#24182;&#19988;&#22312;&#20154;&#24037;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#20505;&#36873;&#38598;&#20013;&#39640;&#22122;&#22768;&#30340;&#24773;&#20917;&#12290;</title><link>https://arxiv.org/abs/2403.07603</link><description>&lt;p&gt;
ProPML: &#27010;&#29575;&#37096;&#20998;&#22810;&#26631;&#31614;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
ProPML: Probability Partial Multi-label Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07603
&lt;/p&gt;
&lt;p&gt;
ProPML&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#27010;&#29575;&#26041;&#27861;&#65292;&#29992;&#20110;&#25193;&#23637;&#20108;&#20803;&#20132;&#21449;&#29109;&#21040;Partial Multi-label Learning&#65288;PML&#65289;&#35774;&#32622;&#20013;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#28145;&#24230;&#26550;&#26500;&#65292;&#24182;&#19988;&#22312;&#20154;&#24037;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#20505;&#36873;&#38598;&#20013;&#39640;&#22122;&#22768;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Partial Multi-label Learning (PML)&#26159;&#19968;&#31181;&#24369;&#30417;&#30563;&#23398;&#20064;&#31867;&#22411;&#65292;&#20854;&#20013;&#27599;&#20010;&#35757;&#32451;&#23454;&#20363;&#23545;&#24212;&#20110;&#19968;&#32452;&#20505;&#36873;&#26631;&#31614;&#65292;&#20854;&#20013;&#21482;&#26377;&#19968;&#20123;&#26159;&#30495;&#23454;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;ProPML&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#27010;&#29575;&#26041;&#27861;&#65292;&#29992;&#20110;&#25193;&#23637;&#20108;&#20803;&#20132;&#21449;&#29109;&#21040;PML&#35774;&#32622;&#20013;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#23427;&#19981;&#38656;&#35201;&#27425;&#20248;&#30340;&#28040;&#27495;&#65292;&#22240;&#27492;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#28145;&#24230;&#26550;&#26500;&#12290;&#27492;&#22806;&#65292;&#22312;&#20154;&#24037;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;ProPML&#22312;&#20505;&#36873;&#38598;&#20013;&#23384;&#22312;&#39640;&#22122;&#22768;&#26102;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07603v1 Announce Type: new  Abstract: Partial Multi-label Learning (PML) is a type of weakly supervised learning where each training instance corresponds to a set of candidate labels, among which only some are true. In this paper, we introduce \our{}, a novel probabilistic approach to this problem that extends the binary cross entropy to the PML setup. In contrast to existing methods, it does not require suboptimal disambiguation and, as such, can be applied to any deep architecture. Furthermore, experiments conducted on artificial and real-world datasets indicate that \our{} outperforms existing approaches, especially for high noise in a candidate set.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;RoBoT&#31639;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#26080;&#35757;&#32451;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#20013;&#25351;&#26631;&#20272;&#35745;&#19981;&#20934;&#30830;&#21644;&#24615;&#33021;&#38480;&#21046;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.07591</link><description>&lt;p&gt;
&#24378;&#21270;&#21644;&#22686;&#24378;&#26080;&#35757;&#32451;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Robustifying and Boosting Training-Free Neural Architecture Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07591
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;RoBoT&#31639;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#26080;&#35757;&#32451;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#20013;&#25351;&#26631;&#20272;&#35745;&#19981;&#20934;&#30830;&#21644;&#24615;&#33021;&#38480;&#21046;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#24050;&#25104;&#20026;AutoML&#30340;&#20851;&#38190;&#32452;&#20214;&#21644;&#33258;&#21160;&#35774;&#35745;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26631;&#20934;&#24037;&#20855;&#12290;&#26368;&#36817;&#65292;&#20316;&#20026;&#26032;&#20852;&#33539;&#24335;&#30340;&#26080;&#35757;&#32451;NAS&#36890;&#36807;&#20165;&#20351;&#29992;&#26080;&#35757;&#32451;&#25351;&#26631;&#20272;&#35745;&#30495;&#23454;&#26550;&#26500;&#24615;&#33021;&#65292;&#25104;&#21151;&#38477;&#20302;&#20102;&#26631;&#20934;&#22522;&#20110;&#35757;&#32451;NAS&#30340;&#25628;&#32034;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25351;&#26631;&#30340;&#20272;&#35745;&#33021;&#21147;&#36890;&#24120;&#22312;&#19981;&#21516;&#20219;&#21153;&#38388;&#21464;&#21270;&#65292;&#20351;&#24471;&#20165;&#20351;&#29992;&#21333;&#19968;&#26080;&#35757;&#32451;&#25351;&#26631;&#22312;&#22810;&#26679;&#20219;&#21153;&#19978;&#23454;&#29616;&#31283;&#20581;&#21644;&#19968;&#33268;&#33391;&#22909;&#30340;&#25628;&#32034;&#24615;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#26080;&#35757;&#32451;&#25351;&#26631;&#19982;&#30495;&#23454;&#26550;&#26500;&#24615;&#33021;&#20043;&#38388;&#30340;&#20272;&#35745;&#24046;&#36317;&#38480;&#21046;&#20102;&#26080;&#35757;&#32451;NAS&#23454;&#29616;&#21331;&#36234;&#24615;&#33021;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24378;&#21270;&#21644;&#22686;&#24378;&#26080;&#35757;&#32451;NAS&#65288;RoBoT&#65289;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#65288;a&#65289;&#21033;&#29992;&#20174;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#25506;&#32034;&#20986;&#30340;&#29616;&#26377;&#26080;&#35757;&#32451;&#25351;&#26631;&#30340;&#20248;&#21270;&#32452;&#21512;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07591v1 Announce Type: new  Abstract: Neural architecture search (NAS) has become a key component of AutoML and a standard tool to automate the design of deep neural networks. Recently, training-free NAS as an emerging paradigm has successfully reduced the search costs of standard training-based NAS by estimating the true architecture performance with only training-free metrics. Nevertheless, the estimation ability of these metrics typically varies across different tasks, making it challenging to achieve robust and consistently good search performance on diverse tasks with only a single training-free metric. Meanwhile, the estimation gap between training-free metrics and the true architecture performances limits training-free NAS to achieve superior performance. To address these challenges, we propose the robustifying and boosting training-free NAS (RoBoT) algorithm which (a) employs the optimized combination of existing training-free metrics explored from Bayesian optimizat
&lt;/p&gt;</description></item><item><title>&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#36890;&#36807;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#37325;&#24314;&#25915;&#20987;&#65292;&#20316;&#32773;&#21457;&#29616;&#22312;DP-SGD&#19979;&#65292;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#20808;&#39564;&#23545;&#20110;&#37325;&#24314;&#25104;&#21151;&#20855;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.07588</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#35270;&#35273;&#38544;&#31169;&#23457;&#35745;
&lt;/p&gt;
&lt;p&gt;
Visual Privacy Auditing with Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07588
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#36890;&#36807;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#37325;&#24314;&#25915;&#20987;&#65292;&#20316;&#32773;&#21457;&#29616;&#22312;DP-SGD&#19979;&#65292;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#20808;&#39564;&#23545;&#20110;&#37325;&#24314;&#25104;&#21151;&#20855;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07588v1 &#22768;&#26126;&#31867;&#22411;: &#26032;&#30340; &#25688;&#35201;: &#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#22270;&#20687;&#37325;&#24314;&#25915;&#20987;&#21487;&#33021;&#20250;&#23548;&#33268;&#27844;&#38706;&#25935;&#24863;&#20449;&#24687;&#65292;&#20174;&#32780;&#23545;&#38544;&#31169;&#26500;&#25104;&#37325;&#22823;&#39118;&#38505;&#12290;&#34429;&#28982;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;(DP)&#26469;&#25269;&#24481;&#27492;&#31867;&#25915;&#20987;&#24050;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#30830;&#23450;&#36866;&#24403;&#30340;DP&#21442;&#25968;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#24403;&#21069;&#23545;&#25968;&#25454;&#37325;&#24314;&#25104;&#21151;&#30340;&#24418;&#24335;&#21270;&#20445;&#35777;&#21463;&#21040;&#20102;&#20851;&#20110;&#23545;&#25163;&#23545;&#30446;&#26631;&#25968;&#25454;&#30340;&#20102;&#35299;&#30340;&#36807;&#20110;&#29702;&#35770;&#21270;&#30340;&#20551;&#35774;&#30340;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#22312;&#22270;&#20687;&#39046;&#22495;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#35843;&#26597;&#36825;&#19968;&#24046;&#24322;&#65292;&#24182;&#21457;&#29616;&#36825;&#20123;&#20551;&#35774;&#30340;&#23454;&#38469;&#24615;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#25968;&#25454;&#20808;&#39564;&#21644;&#37325;&#24314;&#30446;&#26631;&#20043;&#38388;&#30340;&#22495;&#36716;&#31227;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;(DMs)&#30340;&#37325;&#24314;&#25915;&#20987;&#65292;&#20551;&#35774;&#23545;&#25163;&#21487;&#20197;&#35775;&#38382;&#30495;&#23454;&#19990;&#30028;&#30340;&#22270;&#20687;&#20808;&#39564;&#65292;&#24182;&#35780;&#20272;&#20854;&#23545;&#22312;DP-SGD&#19979;&#30340;&#38544;&#31169;&#27844;&#38706;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;(1)&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#20808;&#39564;&#26174;&#33879;&#24433;&#21709;&#37325;&#24314;&#25104;&#21151;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07588v1 Announce Type: new  Abstract: Image reconstruction attacks on machine learning models pose a significant risk to privacy by potentially leaking sensitive information. Although defending against such attacks using differential privacy (DP) has proven effective, determining appropriate DP parameters remains challenging. Current formal guarantees on data reconstruction success suffer from overly theoretical assumptions regarding adversary knowledge about the target data, particularly in the image domain. In this work, we empirically investigate this discrepancy and find that the practicality of these assumptions strongly depends on the domain shift between the data prior and the reconstruction target. We propose a reconstruction attack based on diffusion models (DMs) that assumes adversary access to real-world image priors and assess its implications on privacy leakage under DP-SGD. We show that (1) real-world data priors significantly influence reconstruction success, 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;&#27169;&#25311;&#23478;&#24237;&#29615;&#22659;&#20013;&#36827;&#34892;&#32852;&#37030;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#26088;&#22312;&#35780;&#20272;&#26426;&#22120;&#20154;&#34892;&#20026;&#30340;&#31038;&#20250;&#36866;&#23452;&#24615;&#65292;&#24182;&#32467;&#21512;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#26426;&#22120;&#20154;&#21487;&#20197;&#20174;&#24444;&#27492;&#30340;&#32463;&#39564;&#20013;&#23398;&#20064;&#31038;&#20250;&#35268;&#33539;&#12290;</title><link>https://arxiv.org/abs/2403.07586</link><description>&lt;p&gt;
&#22312;&#27169;&#25311;&#23478;&#24237;&#29615;&#22659;&#20013;&#32852;&#37030;&#23398;&#20064;&#31038;&#20250;&#36866;&#23452;&#30340;&#26426;&#22120;&#20154;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Federated Learning of Socially Appropriate Agent Behaviours in Simulated Home Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07586
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;&#27169;&#25311;&#23478;&#24237;&#29615;&#22659;&#20013;&#36827;&#34892;&#32852;&#37030;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#26088;&#22312;&#35780;&#20272;&#26426;&#22120;&#20154;&#34892;&#20026;&#30340;&#31038;&#20250;&#36866;&#23452;&#24615;&#65292;&#24182;&#32467;&#21512;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#26426;&#22120;&#20154;&#21487;&#20197;&#20174;&#24444;&#27492;&#30340;&#32463;&#39564;&#20013;&#23398;&#20064;&#31038;&#20250;&#35268;&#33539;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31038;&#20132;&#26426;&#22120;&#20154;&#36234;&#26469;&#36234;&#22810;&#22320;&#34701;&#20837;&#26085;&#24120;&#29983;&#27963;&#65292;&#30830;&#20445;&#23427;&#20204;&#30340;&#34892;&#20026;&#31526;&#21512;&#31038;&#20250;&#35268;&#33539;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#24191;&#27867;&#24212;&#29992;&#20110;&#24320;&#25918;&#19990;&#30028;&#65292;&#25506;&#32034;&#20010;&#20307;&#26426;&#22120;&#20154;&#21487;&#20197;&#22312;&#23398;&#20064;&#20854;&#29420;&#29305;&#29615;&#22659;&#30340;&#21516;&#26102;&#20063;&#20174;&#24444;&#27492;&#30340;&#32463;&#39564;&#20013;&#23398;&#20064;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#35774;&#32622;&#38750;&#24120;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;FL&#22522;&#20934;&#65292;&#35780;&#20272;&#19981;&#21516;&#31574;&#30053;&#65292;&#20351;&#29992;&#22810;&#26631;&#31614;&#22238;&#24402;&#30446;&#26631;&#65292;&#20854;&#20013;&#27599;&#20010;&#23458;&#25143;&#31471;&#20998;&#21035;&#23398;&#20064;&#39044;&#27979;&#19981;&#21516;&#26426;&#22120;&#20154;&#34892;&#20026;&#30340;&#31038;&#20250;&#36866;&#23452;&#24615;&#65292;&#21516;&#26102;&#19982;&#20182;&#20154;&#20849;&#20139;&#20854;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#23558;&#35757;&#32451;&#25968;&#25454;&#25353;&#19981;&#21516;&#19978;&#19979;&#25991;&#25286;&#20998;&#65292;&#20351;&#27599;&#20010;&#23458;&#25143;&#31471;&#36880;&#28176;&#36328;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32852;&#37030;&#25345;&#32493;&#23398;&#20064;&#65288;FCL&#65289;&#22522;&#20934;&#65292;&#23558;FL&#26041;&#27861;&#35843;&#25972;&#20026;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#25345;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#26041;&#27861;&#65292;&#20197;&#25345;&#32493;&#23398;&#20064;&#31038;&#20250;&#36866;&#23452;&#30340;&#20195;&#29702;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07586v1 Announce Type: cross  Abstract: As social robots become increasingly integrated into daily life, ensuring their behaviours align with social norms is crucial. For their widespread open-world application, it is important to explore Federated Learning (FL) settings where individual robots can learn about their unique environments while also learning from each others' experiences. In this paper, we present a novel FL benchmark that evaluates different strategies, using multi-label regression objectives, where each client individually learns to predict the social appropriateness of different robot actions while also sharing their learning with others. Furthermore, splitting the training data by different contexts such that each client incrementally learns across contexts, we present a novel Federated Continual Learning (FCL) benchmark that adapts FL-based methods to use state-of-the-art Continual Learning (CL) methods to continually learn socially appropriate agent behav
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20998;&#24067;&#24335;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#36890;&#20449;&#20248;&#21270;&#26550;&#26500;&#65292;&#24182;&#23545;&#24182;&#34892;&#21270;&#31574;&#30053;&#12289;&#38598;&#20307;&#36890;&#20449;&#24211;&#21644;&#32593;&#32476;&#20851;&#31995;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#24635;&#32467;&#20102;&#24403;&#21069;&#30340;&#30740;&#31350;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.07585</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#35757;&#32451;&#30340;&#36890;&#20449;&#20248;&#21270;&#65306;&#26550;&#26500;&#12289;&#36827;&#23637;&#21644;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
Communication Optimization for Distributed Training: Architecture, Advances, and Opportunities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07585
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20998;&#24067;&#24335;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#36890;&#20449;&#20248;&#21270;&#26550;&#26500;&#65292;&#24182;&#23545;&#24182;&#34892;&#21270;&#31574;&#30053;&#12289;&#38598;&#20307;&#36890;&#20449;&#24211;&#21644;&#32593;&#32476;&#20851;&#31995;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#24635;&#32467;&#20102;&#24403;&#21069;&#30340;&#30740;&#31350;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#35268;&#27169;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#34028;&#21187;&#21457;&#23637;&#65292;&#21442;&#25968;&#37327;&#19981;&#26029;&#22686;&#38271;&#12290;&#35757;&#32451;&#36825;&#20123;&#22823;&#35268;&#27169;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#24222;&#22823;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#36164;&#28304;&#65292;&#36229;&#20986;&#20102;&#21333;&#20010;GPU&#30340;&#33539;&#22260;&#65292;&#38656;&#35201;&#36827;&#34892;&#20998;&#24067;&#24335;&#35757;&#32451;&#12290;&#30001;&#20110;&#36817;&#24180;&#26469;GPU&#24615;&#33021;&#36805;&#36895;&#21457;&#23637;&#65292;&#35745;&#31639;&#26102;&#38388;&#32553;&#30701;&#65292;&#22240;&#27492;&#36890;&#20449;&#22312;&#25972;&#20307;&#35757;&#32451;&#26102;&#38388;&#20013;&#30340;&#27604;&#20363;&#22686;&#21152;&#12290;&#22240;&#27492;&#65292;&#20248;&#21270;&#20998;&#24067;&#24335;&#35757;&#32451;&#30340;&#36890;&#20449;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#32039;&#36843;&#38382;&#39064;&#12290;&#26412;&#25991;&#31616;&#35201;&#20171;&#32461;&#20102;&#20998;&#24067;&#24335;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#24635;&#20307;&#26550;&#26500;&#65292;&#24182;&#20174;&#36890;&#20449;&#20248;&#21270;&#30340;&#35282;&#24230;&#20998;&#26512;&#20102;&#24182;&#34892;&#21270;&#31574;&#30053;&#12289;&#38598;&#20307;&#36890;&#20449;&#24211;&#21644;&#32593;&#32476;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24418;&#25104;&#20102;&#19968;&#20010;&#19977;&#23618;&#33539;&#24335;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#24403;&#21069;&#20855;&#26377;&#20195;&#34920;&#24615;&#30340;&#30740;&#31350;&#36827;&#23637;&#19982;&#36825;&#20010;&#19977;&#23618;&#33539;&#24335;&#12290;&#25105;&#20204;&#21457;&#29616;lay
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07585v1 Announce Type: cross  Abstract: The past few years have witnessed the flourishing of large-scale deep neural network models with ever-growing parameter numbers. Training such large-scale models typically requires massive memory and computing resources that exceed those of a single GPU, necessitating distributed training. As GPU performance has rapidly evolved in recent years, computation time has shrunk, thereby increasing the proportion of communication in the overall training time. Therefore, optimizing communication for distributed training has become an urgent issue. In this article, we briefly introduce the general architecture of distributed deep neural network training and analyze relationships among Parallelization Strategy, Collective Communication Library, and Network from the perspective of communication optimization, which forms a three-layer paradigm. We then review current representative research advances with this three-layer paradigm. We find that lay
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21487;&#36866;&#24212;&#24615;CNC&#65288;ACNC&#65289;&#30340;&#27010;&#24565;&#65292;&#20316;&#20026;&#19968;&#31181;&#33258;&#20027;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#36741;&#21161;&#26426;&#21046;&#65292;&#26088;&#22312;&#32852;&#21512;&#32534;&#25490;&#35745;&#31639;&#21644;&#32593;&#32476;&#36164;&#28304;&#65292;&#28385;&#36275;&#23545;&#21160;&#24577;&#21644;&#22823;&#37327;&#29992;&#25143;&#35831;&#27714;&#30340;&#20005;&#26684;&#35201;&#27714;&#12290;</title><link>https://arxiv.org/abs/2403.07573</link><description>&lt;p&gt;
&#36808;&#21521;&#20855;&#26377;&#21487;&#36866;&#24212;&#24615;&#35745;&#31639;&#21644;&#32593;&#32476;&#34701;&#21512;&#30340;&#21160;&#24577;&#26410;&#26469;&#65288;ACNC&#65289;
&lt;/p&gt;
&lt;p&gt;
Towards a Dynamic Future with Adaptable Computing and Network Convergence (ACNC)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07573
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21487;&#36866;&#24212;&#24615;CNC&#65288;ACNC&#65289;&#30340;&#27010;&#24565;&#65292;&#20316;&#20026;&#19968;&#31181;&#33258;&#20027;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#36741;&#21161;&#26426;&#21046;&#65292;&#26088;&#22312;&#32852;&#21512;&#32534;&#25490;&#35745;&#31639;&#21644;&#32593;&#32476;&#36164;&#28304;&#65292;&#28385;&#36275;&#23545;&#21160;&#24577;&#21644;&#22823;&#37327;&#29992;&#25143;&#35831;&#27714;&#30340;&#20005;&#26684;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25512;&#36827;6G&#30340;&#32972;&#26223;&#19979;&#65292;&#39044;&#35745;&#20250;&#20986;&#29616;&#23454;&#36136;&#24615;&#30340;&#33539;&#24335;&#36716;&#21464;&#65292;&#31361;&#20986;&#20102;&#30001;&#22823;&#37327;&#36830;&#25509;&#21644;&#20005;&#26684;&#36981;&#23432;&#26381;&#21153;&#36136;&#37327;/&#20307;&#39564;&#65288;QoS/E&#65289;&#20808;&#20915;&#26465;&#20214;&#25152;&#29305;&#24449;&#21270;&#30340;&#20840;&#38754;&#30340;&#19968;&#20999;&#23545;&#19968;&#20999;&#20132;&#20114;&#12290;&#21363;&#23558;&#38754;&#20020;&#30340;&#25361;&#25112;&#28304;&#20110;&#36164;&#28304;&#31232;&#32570;&#65292;&#20419;&#20351;&#26377;&#24847;&#35782;&#22320;&#21521;&#35745;&#31639;-&#32593;&#32476;&#34701;&#21512;&#65288;CNC&#65289;&#36807;&#28193;&#65292;&#20316;&#20026;&#32852;&#21512;&#36164;&#28304;&#32534;&#25490;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#34429;&#28982;&#22522;&#20110;CNC&#30340;&#26426;&#21046;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#65292;&#20294;&#23427;&#20204;&#22312;&#23454;&#29616;&#26410;&#26469;&#26381;&#21153;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#31867;&#20284;Metaverse&#30340;&#20351;&#29992;&#24773;&#26223;&#20013;&#65292;&#21487;&#33021;&#20250;&#30001;&#20110;&#29992;&#25143;&#12289;&#26381;&#21153;&#21644;&#36164;&#28304;&#19981;&#26029;&#21464;&#21270;&#30340;&#29305;&#24615;&#32780;&#21463;&#21040;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#21487;&#36866;&#24212;&#24615;CNC&#65288;ACNC&#65289;&#30340;&#27010;&#24565;&#65292;&#20316;&#20026;&#19968;&#31181;&#33258;&#20027;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#36741;&#21161;&#26426;&#21046;&#65292;&#26088;&#22312;&#32852;&#21512;&#32534;&#25490;&#35745;&#31639;&#21644;&#32593;&#32476;&#36164;&#28304;&#65292;&#28385;&#36275;&#23545;&#21160;&#24577;&#21644;&#22823;&#37327;&#29992;&#25143;&#35831;&#27714;&#30340;&#20005;&#26684;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07573v1 Announce Type: cross  Abstract: In the context of advancing 6G, a substantial paradigm shift is anticipated, highlighting comprehensive everything-to-everything interactions characterized by numerous connections and stringent adherence to Quality of Service/Experience (QoS/E) prerequisites. The imminent challenge stems from resource scarcity, prompting a deliberate transition to Computing-Network Convergence (CNC) as an auspicious approach for joint resource orchestration. While CNC-based mechanisms have garnered attention, their effectiveness in realizing future services, particularly in use cases like the Metaverse, may encounter limitations due to the continually changing nature of users, services, and resources. Hence, this paper presents the concept of Adaptable CNC (ACNC) as an autonomous Machine Learning (ML)-aided mechanism crafted for the joint orchestration of computing and network resources, catering to dynamic and voluminous user requests with stringent r
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20174;&#22330;&#22320;&#36816;&#21160;&#35760;&#24405;&#20013;&#23398;&#20064;&#30340;&#25928;&#26524;&#65292;&#24182;&#25506;&#35752;&#36741;&#21161;&#20449;&#24687;&#23545;&#27492;&#36807;&#31243;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.07569</link><description>&lt;p&gt;
&#25506;&#31350;&#21333;&#21488;&#22330;&#22320;&#36816;&#21160;&#35760;&#24405;&#30340;&#28145;&#24230;&#23398;&#20064;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Exploring Challenges in Deep Learning of Single-Station Ground Motion Records
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07569
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20174;&#22330;&#22320;&#36816;&#21160;&#35760;&#24405;&#20013;&#23398;&#20064;&#30340;&#25928;&#26524;&#65292;&#24182;&#25506;&#35752;&#36741;&#21161;&#20449;&#24687;&#23545;&#27492;&#36807;&#31243;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20195;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#22320;&#38663;&#23398;&#21644;&#22320;&#38663;&#24037;&#31243;&#30340;&#21508;&#31181;&#24212;&#29992;&#20013;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#32467;&#26524;&#12290;&#36825;&#20123;&#27169;&#22411;&#20027;&#35201;&#20381;&#36182;&#21033;&#29992;&#22330;&#22320;&#36816;&#21160;&#35760;&#24405;&#36827;&#34892;&#22320;&#38663;&#20107;&#20214;&#20998;&#31867;&#12289;&#23450;&#20301;&#12289;&#22320;&#38663;&#26089;&#26399;&#39044;&#35686;&#31995;&#32479;&#21644;&#32467;&#26500;&#20581;&#24247;&#30417;&#27979;&#31561;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#20174;&#36825;&#20123;&#22797;&#26434;&#30340;&#26102;&#38388;&#24207;&#21015;&#20449;&#21495;&#20013;&#26377;&#25928;&#23398;&#20064;&#30340;&#31243;&#24230;&#23578;&#26410;&#24471;&#21040;&#24443;&#24213;&#20998;&#26512;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#35780;&#20272;&#36741;&#21161;&#20449;&#24687;&#65288;&#22914;&#22320;&#38663;&#30456;&#21040;&#36798;&#26102;&#38388;&#25110;&#32593;&#32476;&#20869;&#22320;&#38663;&#21488;&#31449;&#20998;&#24067;&#65289;&#22312;&#20174;&#22330;&#22320;&#36816;&#21160;&#35760;&#24405;&#20013;&#36827;&#34892;&#28145;&#24230;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#20027;&#23548;&#31243;&#24230;&#65292;&#21487;&#33021;&#20250;&#24433;&#21709;&#20854;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#23545;&#20004;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#36229;&#21442;&#25968;&#25628;&#32034;&#65292;&#35780;&#20272;&#23427;&#20204;&#22312;&#20174;&#22330;&#22320;&#36816;&#21160;&#35760;&#24405;&#20013;&#36827;&#34892;&#28145;&#24230;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#65292;&#21516;&#26102;&#26816;&#26597;&#36741;&#21161;&#20449;&#24687;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07569v1 Announce Type: cross  Abstract: Contemporary deep learning models have demonstrated promising results across various applications within seismology and earthquake engineering. These models rely primarily on utilizing ground motion records for tasks such as earthquake event classification, localization, earthquake early warning systems, and structural health monitoring. However, the extent to which these models effectively learn from these complex time-series signals has not been thoroughly analyzed. In this study, our objective is to evaluate the degree to which auxiliary information, such as seismic phase arrival times or seismic station distribution within a network, dominates the process of deep learning from ground motion records, potentially hindering its effectiveness. We perform a hyperparameter search on two deep learning models to assess their effectiveness in deep learning from ground motion records while also examining the impact of auxiliary information o
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;GeFF&#65288;&#36890;&#29992;&#29305;&#24449;&#22330;&#65289;&#65292;&#20316;&#20026;&#23548;&#33322;&#21644;&#25805;&#20316;&#30340;&#32479;&#19968;&#34920;&#31034;&#65292;&#21487;&#20197;&#23454;&#26102;&#25191;&#34892;&#65292;&#36890;&#36807;&#23558;&#29983;&#25104;&#30340;&#20016;&#23500;&#22330;&#26223;&#20808;&#39564;&#19982;&#33258;&#28982;&#35821;&#35328;&#23545;&#40784;&#26469;&#25552;&#39640;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.07563</link><description>&lt;p&gt;
&#23398;&#20064;&#31227;&#21160;&#25805;&#20316;&#30340;&#36890;&#29992;&#29305;&#24449;&#22330;
&lt;/p&gt;
&lt;p&gt;
Learning Generalizable Feature Fields for Mobile Manipulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07563
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;GeFF&#65288;&#36890;&#29992;&#29305;&#24449;&#22330;&#65289;&#65292;&#20316;&#20026;&#23548;&#33322;&#21644;&#25805;&#20316;&#30340;&#32479;&#19968;&#34920;&#31034;&#65292;&#21487;&#20197;&#23454;&#26102;&#25191;&#34892;&#65292;&#36890;&#36807;&#23558;&#29983;&#25104;&#30340;&#20016;&#23500;&#22330;&#26223;&#20808;&#39564;&#19982;&#33258;&#28982;&#35821;&#35328;&#23545;&#40784;&#26469;&#25552;&#39640;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;&#25805;&#20316;&#20013;&#30340;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#26159;&#22914;&#20309;&#20197;&#32479;&#19968;&#30340;&#26041;&#24335;&#34920;&#31034;&#29289;&#20307;&#21644;&#22330;&#26223;&#65292;&#20351;&#24471;&#26426;&#22120;&#20154;&#21487;&#20197;&#21516;&#26102;&#29992;&#20110;&#22312;&#29615;&#22659;&#20013;&#23548;&#33322;&#21644;&#25805;&#20316;&#29289;&#20307;&#12290;&#26412;&#24037;&#20316;&#25552;&#20986;&#20102;GeFF&#65288;&#36890;&#29992;&#29305;&#24449;&#22330;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#22330;&#26223;&#32423;&#30340;&#36890;&#29992;&#31070;&#32463;&#29305;&#24449;&#22330;&#65292;&#20316;&#20026;&#23548;&#33322;&#21644;&#25805;&#20316;&#30340;&#32479;&#19968;&#34920;&#31034;&#65292;&#21487;&#20197;&#23454;&#26102;&#25191;&#34892;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;&#29983;&#25104;&#26032;&#35270;&#22270;&#21512;&#25104;&#35270;&#20026;&#19968;&#20010;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#28982;&#21518;&#36890;&#36807;CLIP&#29305;&#24449;&#25552;&#28860;&#23558;&#29983;&#25104;&#30340;&#20016;&#23500;&#22330;&#26223;&#20808;&#39564;&#19982;&#33258;&#28982;&#35821;&#35328;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07563v1 Announce Type: cross  Abstract: An open problem in mobile manipulation is how to represent objects and scenes in a unified manner, so that robots can use it both for navigating in the environment and manipulating objects. The latter requires capturing intricate geometry while understanding fine-grained semantics, whereas the former involves capturing the complexity inherit to an expansive physical scale. In this work, we present GeFF (Generalizable Feature Fields), a scene-level generalizable neural feature field that acts as a unified representation for both navigation and manipulation that performs in real-time. To do so, we treat generative novel view synthesis as a pre-training task, and then align the resulting rich scene priors with natural language via CLIP feature distillation. We demonstrate the effectiveness of this approach by deploying GeFF on a quadrupedal robot equipped with a manipulator. We evaluate GeFF's ability to generalize to open-set objects as 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;&#20998;&#31867;&#26041;&#27861;&#30340;&#26356;&#28789;&#27963;&#30340;&#21333;&#20803;&#20998;&#31867;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#22522;&#20110;&#35268;&#21017;&#21644;&#20915;&#31574;&#26641;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#35774;&#35745;&#21407;&#29702;&#65292;&#24182;&#35814;&#32454;&#25551;&#36848;&#20102;&#24320;&#21457;&#30340;&#20998;&#31867;&#22120;&#12290;</title><link>https://arxiv.org/abs/2403.07562</link><description>&lt;p&gt;
Jupyter&#31508;&#35760;&#26412;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#39033;&#30446;&#28789;&#27963;&#21333;&#20803;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
A Flexible Cell Classification for ML Projects in Jupyter Notebooks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07562
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;&#20998;&#31867;&#26041;&#27861;&#30340;&#26356;&#28789;&#27963;&#30340;&#21333;&#20803;&#20998;&#31867;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#22522;&#20110;&#35268;&#21017;&#21644;&#20915;&#31574;&#26641;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#35774;&#35745;&#21407;&#29702;&#65292;&#24182;&#35814;&#32454;&#25551;&#36848;&#20102;&#24320;&#21457;&#30340;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Jupyter Notebook&#26159;&#19968;&#20010;&#24120;&#29992;&#30340;&#20132;&#20114;&#24335;&#24320;&#21457;&#29615;&#22659;&#65292;&#29992;&#20110;&#24555;&#36895;&#23454;&#39564;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#35299;&#20915;&#26041;&#26696;&#12290;&#25551;&#36848;&#20195;&#30721;&#21333;&#20803;&#20013;&#25191;&#34892;&#30340;ML&#27963;&#21160;&#21487;&#25552;&#39640;&#31508;&#35760;&#26412;&#30340;&#21487;&#35835;&#24615;&#21644;&#29702;&#35299;&#24615;&#12290;&#25163;&#21160;&#27880;&#37322;&#20195;&#30721;&#21333;&#20803;&#32791;&#26102;&#19988;&#23481;&#26131;&#20986;&#38169;&#12290;&#22240;&#27492;&#65292;&#24050;&#24320;&#21457;&#20986;&#24037;&#20855;&#65292;&#26681;&#25454;&#22312;&#20854;&#20013;&#25191;&#34892;&#30340;ML&#27963;&#21160;&#23545;&#31508;&#35760;&#26412;&#30340;&#21333;&#20803;&#36827;&#34892;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#24037;&#20855;&#32570;&#20047;&#28789;&#27963;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#22522;&#20110;&#24050;&#21019;&#24314;&#30340;&#26597;&#25214;&#34920;&#24037;&#20316;&#65292;&#35813;&#26597;&#25214;&#34920;&#23558;&#24120;&#29992;ML&#24211;&#30340;&#20989;&#25968;&#35843;&#29992;&#26144;&#23556;&#21040;ML&#27963;&#21160;&#12290;&#36825;&#20123;&#34920;&#24517;&#39035;&#25163;&#21160;&#35843;&#25972;&#20197;&#32771;&#34385;&#26032;&#30340;&#25110;&#26356;&#25913;&#30340;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07562v1 Announce Type: cross  Abstract: Jupyter Notebook is an interactive development environment commonly used for rapid experimentation of machine learning (ML) solutions. Describing the ML activities performed along code cells improves the readability and understanding of Notebooks. Manual annotation of code cells is time-consuming and error-prone. Therefore, tools have been developed that classify the cells of a notebook concerning the ML activity performed in them. However, the current tools are not flexible, as they work based on look-up tables that have been created, which map function calls of commonly used ML libraries to ML activities. These tables must be manually adjusted to account for new or changed libraries.   This paper presents a more flexible approach to cell classification based on a hybrid classification approach that combines a rule-based and a decision tree classifier. We discuss the design rationales and describe the developed classifiers in detail. 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Ensembling Prioritized Hybrid Policies (EPH)&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#36890;&#20449;&#27169;&#22359;&#21644;&#19977;&#31181;&#39640;&#32423;&#25512;&#29702;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#22522;&#20110;&#36890;&#20449;&#30340;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#35299;&#20915;&#26041;&#26696;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.07559</link><description>&lt;p&gt;
&#20026;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#38598;&#25104;&#20248;&#20808;&#32423;&#28151;&#21512;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Ensembling Prioritized Hybrid Policies for Multi-agent Pathfinding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07559
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Ensembling Prioritized Hybrid Policies (EPH)&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#36890;&#20449;&#27169;&#22359;&#21644;&#19977;&#31181;&#39640;&#32423;&#25512;&#29702;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#22522;&#20110;&#36890;&#20449;&#30340;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#35299;&#20915;&#26041;&#26696;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#30340;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#65288;MAPF&#65289;&#36817;&#26469;&#22240;&#20854;&#39640;&#25928;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#32780;&#21463;&#21040;&#20851;&#27880;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;Ensembling Prioritized Hybrid Policies (EPH)&#65292;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#22522;&#20110;&#36890;&#20449;&#30340;MARL-MAPF&#27714;&#35299;&#22120;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#36873;&#25321;&#24615;&#36890;&#20449;&#27169;&#22359;&#65292;&#20197;&#22312;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#25910;&#38598;&#26356;&#20016;&#23500;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#26234;&#33021;&#20307;&#21327;&#35843;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;Q-learning&#30340;&#31639;&#27861;&#23545;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07559v1 Announce Type: cross  Abstract: Multi-Agent Reinforcement Learning (MARL) based Multi-Agent Path Finding (MAPF) has recently gained attention due to its efficiency and scalability. Several MARL-MAPF methods choose to use communication to enrich the information one agent can perceive. However, existing works still struggle in structured environments with high obstacle density and a high number of agents. To further improve the performance of the communication-based MARL-MAPF solvers, we propose a new method, Ensembling Prioritized Hybrid Policies (EPH). We first propose a selective communication block to gather richer information for better agent coordination within multi-agent environments and train the model with a Q-learning-based algorithm. We further introduce three advanced inference strategies aimed at bolstering performance during the execution phase. First, we hybridize the neural policy with single-agent expert guidance for navigating conflict-free zones. Se
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37325;&#26032;&#35780;&#20272;&#20102;&#20351;&#29992;LLM&#36827;&#34892;&#25688;&#35201;&#19981;&#19968;&#33268;&#24615;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;SIFiD&#65288;&#24102;&#26377;&#36807;&#28388;&#25991;&#26723;&#30340;&#25688;&#35201;&#19981;&#19968;&#33268;&#24615;&#26816;&#27979;&#65289;&#65292;&#26088;&#22312;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#25110;&#27979;&#37327;&#35821;&#20041;&#30456;&#20284;&#24615;&#26469;&#35782;&#21035;&#25991;&#26723;&#20013;&#30340;&#20851;&#38190;&#21477;&#23376;&#12290;</title><link>https://arxiv.org/abs/2403.07557</link><description>&lt;p&gt;
SIFiD&#65306;&#20351;&#29992;LLM&#37325;&#26032;&#35780;&#20272;&#25688;&#35201;&#30340;&#20107;&#23454;&#19981;&#19968;&#33268;&#24615;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
SIFiD: Reassess Summary Factual Inconsistency Detection with LLM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07557
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37325;&#26032;&#35780;&#20272;&#20102;&#20351;&#29992;LLM&#36827;&#34892;&#25688;&#35201;&#19981;&#19968;&#33268;&#24615;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;SIFiD&#65288;&#24102;&#26377;&#36807;&#28388;&#25991;&#26723;&#30340;&#25688;&#35201;&#19981;&#19968;&#33268;&#24615;&#26816;&#27979;&#65289;&#65292;&#26088;&#22312;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#25110;&#27979;&#37327;&#35821;&#20041;&#30456;&#20284;&#24615;&#26469;&#35782;&#21035;&#25991;&#26723;&#20013;&#30340;&#20851;&#38190;&#21477;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30830;&#20445;&#25688;&#35201;&#19982;&#21407;&#22987;&#25991;&#26723;&#20043;&#38388;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#22312;&#25688;&#35201;&#20219;&#21153;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#22240;&#27492;&#65292;&#20154;&#20204;&#33268;&#21147;&#20110;&#26816;&#27979;&#19981;&#19968;&#33268;&#24615;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#24320;&#22987;&#21033;&#29992;&#23427;&#20204;&#20808;&#36827;&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#36827;&#34892;&#19981;&#19968;&#33268;&#24615;&#26816;&#27979;&#12290;&#28982;&#32780;&#65292;&#26089;&#26399;&#23581;&#35797;&#34920;&#26126;&#65292;&#30001;&#20110;LLMs&#26377;&#38480;&#30340;&#36981;&#24490;&#25351;&#20196;&#33021;&#21147;&#21644;&#32570;&#20047;&#26377;&#25928;&#30340;&#26816;&#27979;&#26041;&#27861;&#35770;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#19981;&#21450;&#20256;&#32479;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;GPT-3.5&#21644;GPT-4&#27604;&#36739;LLMs&#30340;&#25688;&#35201;&#19981;&#19968;&#33268;&#24615;&#26816;&#27979;&#34920;&#29616;&#65292;&#20197;&#25512;&#21160;&#22522;&#20110;LLM&#30340;&#19981;&#19968;&#33268;&#24615;&#26816;&#27979;&#30740;&#31350;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SIFiD&#65288;&#24102;&#26377;&#36807;&#28388;&#25991;&#26723;&#30340;&#25688;&#35201;&#19981;&#19968;&#33268;&#24615;&#26816;&#27979;&#65289;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#25110;&#27979;&#37327;&#25688;&#35201;&#21644;&#25991;&#26723;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#26469;&#35782;&#21035;&#25991;&#26723;&#20013;&#30340;&#20851;&#38190;&#21477;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07557v1 Announce Type: new  Abstract: Ensuring factual consistency between the summary and the original document is paramount in summarization tasks. Consequently, considerable effort has been dedicated to detecting inconsistencies. With the advent of Large Language Models (LLMs), recent studies have begun to leverage their advanced language understanding capabilities for inconsistency detection. However, early attempts have shown that LLMs underperform traditional models due to their limited ability to follow instructions and the absence of an effective detection methodology. In this study, we reassess summary inconsistency detection with LLMs, comparing the performances of GPT-3.5 and GPT-4. To advance research in LLM-based inconsistency detection, we propose SIFiD (Summary Inconsistency Detection with Filtered Document) that identify key sentences within documents by either employing natural language inference or measuring semantic similarity between summaries and documen
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;&#20855;&#36523;&#20195;&#29702;&#30340;&#20004;&#31181;&#25345;&#32493;&#23398;&#20064;&#35774;&#32622;&#65306;&#23398;&#20064;&#26032;&#34892;&#20026;&#21644;&#26032;&#29615;&#22659;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36890;&#36807;&#33258;&#20449;&#24230;&#24471;&#20998;&#26469;&#26356;&#26032;&#23384;&#20648;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#36991;&#20813;&#38656;&#35201;&#20219;&#21153;&#36793;&#30028;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.07548</link><description>&lt;p&gt;
&#20114;&#21160;&#25351;&#20196;&#36319;&#38543;&#20195;&#29702;&#30340;&#22312;&#32447;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Online Continual Learning For Interactive Instruction Following Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07548
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;&#20855;&#36523;&#20195;&#29702;&#30340;&#20004;&#31181;&#25345;&#32493;&#23398;&#20064;&#35774;&#32622;&#65306;&#23398;&#20064;&#26032;&#34892;&#20026;&#21644;&#26032;&#29615;&#22659;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36890;&#36807;&#33258;&#20449;&#24230;&#24471;&#20998;&#26469;&#26356;&#26032;&#23384;&#20648;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#36991;&#20813;&#38656;&#35201;&#20219;&#21153;&#36793;&#30028;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36890;&#36807;&#35821;&#35328;&#25351;&#20196;&#25191;&#34892;&#26085;&#24120;&#20219;&#21153;&#30340;&#20855;&#36523;&#20195;&#29702;&#23398;&#20064;&#36807;&#31243;&#20013;&#65292;&#25991;&#29486;&#22823;&#37117;&#20551;&#23450;&#20195;&#29702;&#22312;&#24320;&#22987;&#26102;&#23601;&#23398;&#20064;&#25152;&#26377;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#26679;&#30340;&#23398;&#20064;&#22330;&#26223;&#36739;&#19981;&#29616;&#23454;&#65292;&#22240;&#20026;&#26426;&#22120;&#20154;&#20195;&#29702;&#24212;&#35813;&#22312;&#25506;&#32034;&#21644;&#24863;&#30693;&#19990;&#30028;&#30340;&#36807;&#31243;&#20013;&#19981;&#26029;&#22320;&#23398;&#20064;&#12290;&#20026;&#20102;&#26397;&#30528;&#26356;&#30495;&#23454;&#30340;&#20855;&#36523;&#20195;&#29702;&#23398;&#20064;&#22330;&#26223;&#36808;&#36827;&#19968;&#27493;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#25345;&#32493;&#23398;&#20064;&#35774;&#32622;&#20379;&#20855;&#36523;&#20195;&#29702;&#20351;&#29992;&#65307;&#23398;&#20064;&#26032;&#34892;&#20026;&#65288;&#34892;&#20026;&#22686;&#37327;&#23398;&#20064;&#65292;Behavior-IL&#65289;&#21644;&#26032;&#29615;&#22659;&#65288;&#29615;&#22659;&#22686;&#37327;&#23398;&#20064;&#65292;Environment-IL&#65289;&#12290;&#22312;&#20219;&#21153;&#20013;&#65292;&#20808;&#21069;&#22522;&#20110;&#8220;&#25968;&#25454;&#20808;&#39564;&#8221;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#32500;&#25252;&#36807;&#21435;&#20219;&#21153;&#30340;logits&#12290;&#28982;&#32780;&#65292;&#23384;&#20648;&#30340;&#20449;&#24687;&#24448;&#24448;&#26159;&#19981;&#20805;&#20998;&#23398;&#20064;&#30340;&#20449;&#24687;&#65292;&#38656;&#35201;&#20219;&#21153;&#36793;&#30028;&#20449;&#24687;&#65292;&#32780;&#36825;&#31181;&#20449;&#24687;&#24182;&#19981;&#24635;&#26159;&#21487;&#29992;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#35758;&#22522;&#20110;&#33258;&#20449;&#24230;&#24471;&#20998;&#32780;&#26080;&#38656;&#20219;&#21153;&#36793;&#30028;&#20449;&#24687;&#26469;&#26356;&#26032;&#23427;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07548v1 Announce Type: new  Abstract: In learning an embodied agent executing daily tasks via language directives, the literature largely assumes that the agent learns all training data at the beginning. We argue that such a learning scenario is less realistic since a robotic agent is supposed to learn the world continuously as it explores and perceives it. To take a step towards a more realistic embodied agent learning scenario, we propose two continual learning setups for embodied agents; learning new behaviors (Behavior Incremental Learning, Behavior-IL) and new environments (Environment Incremental Learning, Environment-IL) For the tasks, previous 'data prior' based continual learning methods maintain logits for the past tasks. However, the stored information is often insufficiently learned information and requires task boundary information, which might not always be available. Here, we propose to update them based on confidence scores without task boundary information d
&lt;/p&gt;</description></item><item><title>&#35270;&#35273;Transformer&#27169;&#22411;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#25104;&#21151;&#24212;&#29992;&#34920;&#26126;&#20854;&#22312;&#20840;&#23616;&#19978;&#19979;&#25991;&#25429;&#25417;&#26041;&#38754;&#30340;&#20248;&#21183;&#65292;&#23545;&#20110;&#23454;&#26102;&#12289;&#21160;&#24577;&#35270;&#35273;&#22330;&#26223;&#22788;&#29702;&#20855;&#26377;&#20851;&#38190;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2403.07542</link><description>&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#20013;&#35270;&#35273;Transformer&#30340;&#35843;&#30740;&#65306;&#24403;&#21069;&#36235;&#21183;&#19982;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
A Survey of Vision Transformers in Autonomous Driving: Current Trends and Future Directions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07542
&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;Transformer&#27169;&#22411;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#25104;&#21151;&#24212;&#29992;&#34920;&#26126;&#20854;&#22312;&#20840;&#23616;&#19978;&#19979;&#25991;&#25429;&#25417;&#26041;&#38754;&#30340;&#20248;&#21183;&#65292;&#23545;&#20110;&#23454;&#26102;&#12289;&#21160;&#24577;&#35270;&#35273;&#22330;&#26223;&#22788;&#29702;&#20855;&#26377;&#20851;&#38190;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#30740;&#25506;&#35752;&#20102;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#37319;&#29992;&#35270;&#35273;Transformer&#27169;&#22411;&#30340;&#36866;&#24212;&#24615;&#65292;&#36825;&#26159;&#21463;&#21040;&#23427;&#20204;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#21462;&#24471;&#25104;&#21151;&#30340;&#21551;&#21457;&#12290;Transformer&#27169;&#22411;&#22312;&#36830;&#32493;&#22270;&#20687;&#22788;&#29702;&#31561;&#20219;&#21153;&#20013;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#22312;&#20840;&#23616;&#19978;&#19979;&#25991;&#25429;&#25417;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#22914;&#22312;&#22797;&#26434;&#22330;&#26223;&#35782;&#21035;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#22240;&#27492;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#20013;&#24471;&#21040;&#20102;&#35748;&#21487;&#12290;&#36825;&#20123;&#33021;&#21147;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#23454;&#26102;&#12289;&#21160;&#24577;&#35270;&#35273;&#22330;&#26223;&#22788;&#29702;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#30340;&#35843;&#30740;&#20840;&#38754;&#27010;&#36848;&#20102;&#33258;&#21160;&#39550;&#39542;&#20013;&#35270;&#35273;Transformer&#30340;&#24212;&#29992;&#65292;&#37325;&#28857;&#20851;&#27880;&#33258;&#27880;&#24847;&#21147;&#12289;&#22810;&#22836;&#27880;&#24847;&#21147;&#21644;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#31561;&#22522;&#30784;&#27010;&#24565;&#12290;&#25105;&#20204;&#28085;&#30422;&#20102;&#30446;&#26631;&#26816;&#27979;&#12289;&#20998;&#21106;&#12289;&#34892;&#20154;&#26816;&#27979;&#12289;&#36710;&#36947;&#26816;&#27979;&#31561;&#21508;&#31181;&#24212;&#29992;&#65292;&#27604;&#36739;&#23427;&#20204;&#30340;&#26550;&#26500;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#12290;&#35843;&#30740;&#26368;&#21518;&#25506;&#35752;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07542v1 Announce Type: cross  Abstract: This survey explores the adaptation of visual transformer models in Autonomous Driving, a transition inspired by their success in Natural Language Processing. Surpassing traditional Recurrent Neural Networks in tasks like sequential image processing and outperforming Convolutional Neural Networks in global context capture, as evidenced in complex scene recognition, Transformers are gaining traction in computer vision. These capabilities are crucial in Autonomous Driving for real-time, dynamic visual scene processing. Our survey provides a comprehensive overview of Vision Transformer applications in Autonomous Driving, focusing on foundational concepts such as self-attention, multi-head attention, and encoder-decoder architecture. We cover applications in object detection, segmentation, pedestrian detection, lane detection, and more, comparing their architectural merits and limitations. The survey concludes with future research directio
&lt;/p&gt;</description></item><item><title>LaB-GATr &#26159;&#19968;&#31181;&#20960;&#20309;&#20195;&#25968;&#21464;&#25442;&#22120;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#24207;&#21015;&#21387;&#32553;&#21644;&#25554;&#20540;&#26377;&#25928;&#22320;&#23398;&#20064;&#22823;&#35268;&#27169;&#29983;&#29289;&#21307;&#23398;&#34920;&#38754;&#21644;&#20307;&#31215;&#32593;&#26684;&#65292;&#25193;&#23637;&#20102;&#20256;&#32479;&#30340; GATr &#26041;&#27861;&#24182;&#23562;&#37325;&#20102;&#27431;&#20960;&#37324;&#24471;&#23545;&#31216;&#24615;&#65292;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.07536</link><description>&lt;p&gt;
LaB-GATr&#65306;&#22823;&#35268;&#27169;&#29983;&#29289;&#21307;&#23398;&#34920;&#38754;&#21644;&#20307;&#31215;&#32593;&#26684;&#30340;&#20960;&#20309;&#20195;&#25968;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
LaB-GATr: geometric algebra transformers for large biomedical surface and volume meshes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07536
&lt;/p&gt;
&lt;p&gt;
LaB-GATr &#26159;&#19968;&#31181;&#20960;&#20309;&#20195;&#25968;&#21464;&#25442;&#22120;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#24207;&#21015;&#21387;&#32553;&#21644;&#25554;&#20540;&#26377;&#25928;&#22320;&#23398;&#20064;&#22823;&#35268;&#27169;&#29983;&#29289;&#21307;&#23398;&#34920;&#38754;&#21644;&#20307;&#31215;&#32593;&#26684;&#65292;&#25193;&#23637;&#20102;&#20256;&#32479;&#30340; GATr &#26041;&#27861;&#24182;&#23562;&#37325;&#20102;&#27431;&#20960;&#37324;&#24471;&#23545;&#31216;&#24615;&#65292;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#35299;&#21078;&#32467;&#26500;&#21487;&#20197;&#29992;&#34920;&#38754;&#25110;&#20307;&#31215;&#32593;&#26684;&#26469;&#25551;&#36848;&#12290;&#26426;&#22120;&#23398;&#20064;&#26159;&#20174;&#36825;&#20123;3D&#27169;&#22411;&#20013;&#25552;&#21462;&#20449;&#24687;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#39640;&#20445;&#30495;&#24230;&#30340;&#32593;&#26684;&#36890;&#24120;&#21253;&#21547;&#25104;&#21315;&#19978;&#19975;&#20010;&#39030;&#28857;&#65292;&#36825;&#22312;&#26500;&#24314;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26102;&#24102;&#26469;&#20102;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#24739;&#32773;&#29305;&#24322;&#24615;&#32593;&#26684;&#21487;&#33021;&#27809;&#26377;&#32463;&#20856;&#23545;&#40784;&#65292;&#36825;&#38480;&#21046;&#20102;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#27867;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;LaB-GATr&#65292;&#19968;&#31181;&#20855;&#26377;&#20960;&#20309;&#26631;&#35760;&#21270;&#30340;&#36716;&#25442;&#22120;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#24207;&#21015;&#21387;&#32553;&#21644;&#25554;&#20540;&#26377;&#25928;&#22320;&#23398;&#20064;&#22823;&#35268;&#27169;&#65288;&#29983;&#29289;&#65289;&#21307;&#23398;&#34920;&#38754;&#21644;&#20307;&#31215;&#32593;&#26684;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25193;&#23637;&#20102;&#26368;&#36817;&#25552;&#20986;&#30340;&#20960;&#20309;&#20195;&#25968;&#21464;&#25442;&#22120;&#65288;GATr&#65289;&#65292;&#22240;&#27492;&#23562;&#37325;&#25152;&#26377;&#27431;&#20960;&#37324;&#24471;&#23545;&#31216;&#24615;&#65292;&#21363;&#26059;&#36716;&#12289;&#24179;&#31227;&#21644;&#21453;&#23556;&#65292;&#26377;&#25928;&#22320;&#32531;&#35299;&#20102;&#24739;&#32773;&#20043;&#38388;&#32463;&#20856;&#23545;&#40784;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07536v1 Announce Type: cross  Abstract: Many anatomical structures can be described by surface or volume meshes. Machine learning is a promising tool to extract information from these 3D models. However, high-fidelity meshes often contain hundreds of thousands of vertices, which creates unique challenges in building deep neural network architectures. Furthermore, patient-specific meshes may not be canonically aligned which limits the generalisation of machine learning algorithms. We propose LaB-GATr, a transfomer neural network with geometric tokenisation that can effectively learn with large-scale (bio-)medical surface and volume meshes through sequence compression and interpolation. Our method extends the recently proposed geometric algebra transformer (GATr) and thus respects all Euclidean symmetries, i.e. rotation, translation and reflection, effectively mitigating the problem of canonical alignment between patients. LaB-GATr achieves state-of-the-art results on three ta
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20351;&#29992;&#29289;&#29702;&#36801;&#31227;&#26694;&#26550;&#26469;&#23398;&#20064;&#26230;&#20307;&#22609;&#24615;&#29289;&#29702;&#24182;&#20174;&#21407;&#23376;&#27169;&#25311;&#20013;&#39044;&#27979;Peierls&#24212;&#21147;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.07526</link><description>&lt;p&gt;
&#29289;&#29702;&#23398;&#36801;&#31227;&#23398;&#20064;&#29992;&#20110;&#26448;&#26009;&#24378;&#24230;&#31579;&#36873;
&lt;/p&gt;
&lt;p&gt;
Physics-Transfer Learning for Material Strength Screening
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07526
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20351;&#29992;&#29289;&#29702;&#36801;&#31227;&#26694;&#26550;&#26469;&#23398;&#20064;&#26230;&#20307;&#22609;&#24615;&#29289;&#29702;&#24182;&#20174;&#21407;&#23376;&#27169;&#25311;&#20013;&#39044;&#27979;Peierls&#24212;&#21147;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26448;&#26009;&#24378;&#24230;&#65292;&#20687;&#35768;&#22810;&#33258;&#28982;&#31185;&#23398;&#20013;&#30340;&#38382;&#39064;&#19968;&#26679;&#65292;&#28085;&#30422;&#22810;&#20010;&#38271;&#24230;&#21644;&#26102;&#38388;&#23610;&#24230;&#65292;&#24182;&#19988;&#35299;&#20915;&#26041;&#26696;&#24517;&#39035;&#22312;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;Peierls&#24212;&#21147;&#26159;&#26230;&#20307;&#22609;&#24615;&#20013;&#30340;&#19968;&#20010;&#20013;&#24515;&#27010;&#24565;&#65292;&#36890;&#36807;&#20301;&#38169;&#23545;&#22609;&#24615;&#27969;&#21160;&#30340;&#38459;&#21147;&#26469;&#34913;&#37327;&#24378;&#24230;&#12290;&#30830;&#23450;Peierls&#24212;&#21147;&#28041;&#21450;&#21040;&#24377;&#24615;&#26230;&#26684;&#21709;&#24212;&#21644;&#26230;&#20307;&#28369;&#31227;&#33021;&#37327;&#26223;&#35266;&#30340;&#22810;&#23610;&#24230;&#24615;&#36136;&#12290;&#36890;&#36807;&#31532;&#19968;&#24615;&#21407;&#29702;&#35745;&#31639;&#36890;&#36807;Peierls&#24212;&#21147;&#30340;&#26448;&#26009;&#24378;&#24230;&#31579;&#36873;&#23545;&#20110;&#20301;&#38169;&#30340;&#38750;&#23616;&#37096;&#29305;&#24615;&#32780;&#35328;&#22312;&#35745;&#31639;&#19978;&#24456;&#38590;&#65292;&#24182;&#19988;&#27809;&#26377;&#21253;&#21547;&#22312;&#26368;&#20808;&#36827;&#30340;&#35745;&#31639;&#26448;&#26009;&#25968;&#25454;&#24211;&#20013;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29289;&#29702;&#36801;&#31227;&#26694;&#26550;&#65292;&#20174;&#32463;&#39564;&#24615;&#21407;&#23376;&#27169;&#25311;&#20013;&#23398;&#20064;&#26230;&#20307;&#22609;&#24615;&#30340;&#29289;&#29702;&#35268;&#24459;&#65292;&#28982;&#21518;&#39044;&#27979;Peierls&#24212;&#21147;&#20174;&#21270;&#23398;&#20934;&#30830;&#30340;&#22522;&#20110;&#23494;&#24230;&#27867;&#20989;&#29702;&#35770;&#30340;&#35745;&#31639;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07526v1 Announce Type: cross  Abstract: The strength of materials, like many problems in the natural sciences, spans multiple length and time scales, and the solution has to balance accuracy and performance. Peierls stress is one of the central concepts in crystal plasticity that measures the strength through the resistance of a dislocation to plastic flow. The determination of Peierls stress involves a multiscale nature depending on both elastic lattice responses and the energy landscape of crystal slips. Material screening by strength via the Peierls stress from first-principles calculations is computationally intractable for the nonlocal characteristics of dislocations, and not included in the state-of-the-art computational material databases. In this work, we propose a physics-transfer framework to learn the physics of crystal plasticity from empirical atomistic simulations and then predict the Peierls stress from chemically accurate density functional theory-based calcu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#37325;&#24314;&#26408;&#26143;&#20869;&#37096;&#30913;&#22330;&#30340;&#26032;&#26041;&#27861;&#65292;&#30456;&#27604;&#20854;&#20182;&#26041;&#27861;&#65292;&#33021;&#22815;&#26356;&#28165;&#26224;&#22320;&#35299;&#26512;&#23616;&#37096;&#32467;&#26500;&#24182;&#36991;&#20813;&#28145;&#24230;&#22122;&#22768;&#24178;&#25200;&#12290;</title><link>https://arxiv.org/abs/2403.07507</link><description>&lt;p&gt;
&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#37325;&#24314;&#26408;&#26143;&#30340;&#30913;&#22330;
&lt;/p&gt;
&lt;p&gt;
Reconstructions of Jupiter's magnetic field using physics informed neural networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07507
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#37325;&#24314;&#26408;&#26143;&#20869;&#37096;&#30913;&#22330;&#30340;&#26032;&#26041;&#27861;&#65292;&#30456;&#27604;&#20854;&#20182;&#26041;&#27861;&#65292;&#33021;&#22815;&#26356;&#28165;&#26224;&#22320;&#35299;&#26512;&#23616;&#37096;&#32467;&#26500;&#24182;&#36991;&#20813;&#28145;&#24230;&#22122;&#22768;&#24178;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#30001;Juno&#20219;&#21153;&#25910;&#38598;&#30340;&#25968;&#25454;&#36827;&#34892;&#30913;&#27979;&#21487;&#20197;&#29992;&#26469;&#32422;&#26463;&#26408;&#26143;&#30340;&#20869;&#37096;&#12290;&#28982;&#32780;&#65292;&#20551;&#35774;&#30005;&#23548;&#29575;&#20026;&#38646;&#24182;&#20197;&#29699;&#35856;&#20989;&#25968;&#34920;&#31034;&#30340;&#37325;&#24314;&#21521;&#20869;&#32487;&#32493;&#26159;&#21463;&#21040;&#23567;&#23610;&#24230;&#22122;&#22768;&#22686;&#24378;&#30340;&#38480;&#21046;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#30340;&#26408;&#26143;&#20869;&#37096;&#30913;&#22330;&#30340;&#26032;&#37325;&#24314;&#26041;&#27861;&#65292;&#20351;&#29992;Juno&#36712;&#36947;&#30340;&#21069;33&#20010;&#65288;PINN33&#65289;&#25110;&#21069;50&#20010;&#65288;PINN50&#65289;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#35299;&#26512;&#20986;&#23616;&#37096;&#32467;&#26500;&#65292;&#24182;&#20801;&#35768;&#23384;&#22312;&#24369;&#29615;&#22659;&#30005;&#27969;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#23545;&#26408;&#26143;&#30913;&#22330;&#30340;&#34920;&#38754;&#21644;&#19978;&#26041;&#30340;&#37325;&#24314;&#37117;&#30456;&#20284;&#65292;&#24182;&#19988;&#19982;Juno&#25968;&#25454;&#25311;&#21512;&#30456;&#20284;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#19981;&#21463;&#28145;&#24230;&#22122;&#22768;&#24178;&#25200;&#65292;&#22240;&#27492;&#33021;&#22815;&#25552;&#20379;&#26356;&#28165;&#26224;&#30340;&#20869;&#37096;&#32467;&#26500;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07507v1 Announce Type: cross  Abstract: Magnetic sounding using data collected from the Juno mission can be used to provide constraints on Jupiter's interior. However, inwards continuation of reconstructions assuming zero electrical conductivity and a representation in spherical harmonics are limited by the enhancement of noise at small scales. In this paper we describe new reconstructions of Jupiter's internal magnetic field based on physics-informed neural networks and either the first 33 (PINN33) or the first 50 (PINN50) of Juno's orbits. The method can resolve local structures, and allows for weak ambient electrical currents. Compared with other methods, our reconstructions of Jupiter's magnetic field both on and above the surface are similar, and we achieve a similar fit to the Juno data. However, our models are not hampered by noise at depth, and so offer a much clearer picture of the interior structure. We estimate that the dynamo boundary is at a fractional radius of
&lt;/p&gt;</description></item><item><title>&#39318;&#27425;&#25552;&#20986;&#20102;&#20174;&#21463;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#30340;&#35270;&#35282;&#20840;&#29699;&#39318;&#27425;&#25552;&#20379;&#28151;&#21512;&#21160;&#21147;&#36710;&#36742;&#30340;&#21463;&#32422;&#26463;&#26368;&#20248;&#29123;&#26009;&#28040;&#32791;&#30340;&#25968;&#23398;&#34920;&#36798;&#65292;&#24182;&#39318;&#27425;&#21033;&#29992;&#20004;&#31181;&#20027;&#27969;&#30340;&#21463;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26469;&#33719;&#24471;&#36710;&#36742;&#22312;&#30005;&#27744;&#30005;&#27668;&#24179;&#34913;&#26465;&#20214;&#19979;&#30340;&#26368;&#23567;&#29123;&#26009;&#28040;&#32791;&#12290;</title><link>https://arxiv.org/abs/2403.07503</link><description>&lt;p&gt;
&#21463;&#32422;&#26463;&#30340;&#28151;&#21512;&#21160;&#21147;&#36710;&#36742;&#30340;&#26368;&#20248;&#29123;&#26009;&#28040;&#32791;&#65306;&#19968;&#31181;&#21463;&#32422;&#26463;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Constrained Optimal Fuel Consumption of HEV: A Constrained Reinforcement Learning Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07503
&lt;/p&gt;
&lt;p&gt;
&#39318;&#27425;&#25552;&#20986;&#20102;&#20174;&#21463;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#30340;&#35270;&#35282;&#20840;&#29699;&#39318;&#27425;&#25552;&#20379;&#28151;&#21512;&#21160;&#21147;&#36710;&#36742;&#30340;&#21463;&#32422;&#26463;&#26368;&#20248;&#29123;&#26009;&#28040;&#32791;&#30340;&#25968;&#23398;&#34920;&#36798;&#65292;&#24182;&#39318;&#27425;&#21033;&#29992;&#20004;&#31181;&#20027;&#27969;&#30340;&#21463;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26469;&#33719;&#24471;&#36710;&#36742;&#22312;&#30005;&#27744;&#30005;&#27668;&#24179;&#34913;&#26465;&#20214;&#19979;&#30340;&#26368;&#23567;&#29123;&#26009;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#21160;&#21147;&#36710;&#36742;&#65288;HEVs&#65289;&#22240;&#33021;&#26356;&#22909;&#22320;&#32467;&#21512;&#20869;&#29123;&#26426;&#21644;&#30005;&#21160;&#26426;&#30340;&#24037;&#20316;&#29305;&#24615;&#32780;&#26085;&#30410;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#22312;&#29305;&#23450;&#35013;&#37197;&#26465;&#20214;&#21644;&#29305;&#23450;&#36895;&#24230;&#26354;&#32447;&#19979;&#65292;HEV&#30340;&#26368;&#23567;&#29123;&#26009;&#28040;&#32791;&#23545;&#20110;&#30005;&#27744;&#30005;&#27668;&#24179;&#34913;&#24773;&#20917;&#20173;&#38656;&#22312;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#36827;&#19968;&#27493;&#38416;&#26126;&#12290;&#38024;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#24037;&#20316;&#39318;&#27425;&#20174;&#21463;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#65288;CRL&#65289;&#30340;&#35270;&#35282;&#25552;&#20379;&#20102;&#21463;&#32422;&#26463;&#30340;&#26368;&#20248;&#29123;&#26009;&#28040;&#32791;&#65288;COFC&#65289;&#30340;&#25968;&#23398;&#34920;&#36798;&#12290;&#21516;&#26102;&#65292;&#39318;&#27425;&#21033;&#29992;&#20102;&#20004;&#31181;&#20027;&#27969;&#30340;CRL&#26041;&#27861;&#65292;&#21363;&#21463;&#32422;&#26463;&#21464;&#20998;&#31574;&#30053;&#20248;&#21270;&#65288;CVPO&#65289;&#21644;&#22522;&#20110;&#25289;&#26684;&#26391;&#26085;&#30340;&#26041;&#27861;&#65292;&#20197;&#22312;&#30005;&#27744;&#30005;&#27668;&#24179;&#34913;&#26465;&#20214;&#19979;&#33719;&#24471;&#36710;&#36742;&#30340;&#26368;&#23567;&#29123;&#26009;&#28040;&#32791;&#12290;&#25105;&#20204;&#22312;&#30693;&#21517;&#30340;&#26222;&#38160;&#26031;&#20016;&#30000;&#28151;&#21512;&#31995;&#32479;&#65288;THS&#65289;&#19979;NEDC&#26465;&#20214;&#19979;&#36827;&#34892;&#26696;&#20363;&#30740;&#31350;&#65307;&#25105;&#20204;g
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07503v1 Announce Type: new  Abstract: Hybrid electric vehicles (HEVs) are becoming increasingly popular because they can better combine the working characteristics of internal combustion engines and electric motors. However, the minimum fuel consumption of an HEV for a battery electrical balance case under a specific assembly condition and a specific speed curve still needs to be clarified in academia and industry. Regarding this problem, this work provides the mathematical expression of constrained optimal fuel consumption (COFC) from the perspective of constrained reinforcement learning (CRL) for the first time globally. Also, two mainstream approaches of CRL, constrained variational policy optimization (CVPO) and Lagrangian-based approaches, are utilized for the first time to obtain the vehicle's minimum fuel consumption under the battery electrical balance condition. We conduct case studies on the well-known Prius TOYOTA hybrid system (THS) under the NEDC condition; we g
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#26631;&#31614;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26816;&#27979;&#23433;&#20840;&#30456;&#20851;&#26041;&#27861;&#30340;&#25554;&#20214;&#65292;&#21487;&#20197;&#33258;&#21160;&#29983;&#25104;&#38745;&#24577;&#20998;&#26512;&#24037;&#20855;&#37197;&#32622;&#65292;&#36816;&#34892;&#20998;&#26512;&#24182;&#26174;&#31034;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.07501</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#26631;&#31614;&#26426;&#22120;&#23398;&#20064;&#26816;&#27979;&#23433;&#20840;&#30456;&#20851;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Detecting Security-Relevant Methods using Multi-label Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07501
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#26631;&#31614;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26816;&#27979;&#23433;&#20840;&#30456;&#20851;&#26041;&#27861;&#30340;&#25554;&#20214;&#65292;&#21487;&#20197;&#33258;&#21160;&#29983;&#25104;&#38745;&#24577;&#20998;&#26512;&#24037;&#20855;&#37197;&#32622;&#65292;&#36816;&#34892;&#20998;&#26512;&#24182;&#26174;&#31034;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#26816;&#27979;&#23433;&#20840;&#28431;&#27934;&#65292;&#38745;&#24577;&#20998;&#26512;&#24037;&#20855;&#38656;&#35201;&#37197;&#32622;&#23433;&#20840;&#30456;&#20851;&#26041;&#27861;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#21487;&#20197;&#20351;&#29992;&#20108;&#20803;&#20851;&#32852;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#33258;&#21160;&#35782;&#21035;&#36825;&#20123;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24573;&#30053;&#20102;&#23433;&#20840;&#30456;&#20851;&#26041;&#27861;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#22312;&#23454;&#36341;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#27492;&#22806;&#65292;&#29992;&#25143;&#20173;&#28982;&#24517;&#39035;&#25163;&#21160;&#37197;&#32622;&#38745;&#24577;&#20998;&#26512;&#24037;&#20855;&#20351;&#29992;&#26816;&#27979;&#21040;&#30340;&#26041;&#27861;&#12290;&#22522;&#20110;&#29992;&#25143;&#30340;&#21453;&#39304;&#21644;&#25105;&#20204;&#30340;&#35266;&#23519;&#65292;&#36825;&#20123;&#32321;&#29712;&#30340;&#25163;&#21160;&#27493;&#39588;&#24448;&#24448;&#20196;&#20154;&#21388;&#28902;&#12289;&#23481;&#26131;&#20986;&#38169;&#19988;&#19981;&#30452;&#35266;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Dev-Assist&#65292;&#19968;&#20010;IntelliJ IDEA&#25554;&#20214;&#65292;&#23427;&#20351;&#29992;&#32771;&#34385;&#26631;&#31614;&#20043;&#38388;&#20381;&#36182;&#20851;&#31995;&#30340;&#22810;&#26631;&#31614;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#26816;&#27979;&#23433;&#20840;&#30456;&#20851;&#26041;&#27861;&#12290;&#35813;&#25554;&#20214;&#21487;&#20197;&#33258;&#21160;&#29983;&#25104;&#38745;&#24577;&#20998;&#26512;&#24037;&#20855;&#30340;&#37197;&#32622;&#65292;&#36816;&#34892;&#38745;&#24577;&#20998;&#26512;&#65292;&#24182;&#22312;IntelliJ IDEA&#20013;&#26174;&#31034;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;Dev-A
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07501v1 Announce Type: new  Abstract: To detect security vulnerabilities, static analysis tools need to be configured with security-relevant methods. Current approaches can automatically identify such methods using binary relevance machine learning approaches. However, they ignore dependencies among security-relevant methods, over-generalize and perform poorly in practice. Additionally, users have to nevertheless manually configure static analysis tools using the detected methods. Based on feedback from users and our observations, the excessive manual steps can often be tedious, error-prone and counter-intuitive.   In this paper, we present Dev-Assist, an IntelliJ IDEA plugin that detects security-relevant methods using a multi-label machine learning approach that considers dependencies among labels. The plugin can automatically generate configurations for static analysis tools, run the static analysis, and show the results in IntelliJ IDEA. Our experiments reveal that Dev-A
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#31526;&#21495;&#22270;&#30340;&#21487;&#36890;&#20449;&#24615;&#20960;&#20309;&#27010;&#24565;&#65292;&#35777;&#26126;&#20102;&#20854;&#24230;&#37327;&#26159;&#27431;&#20960;&#37324;&#24503;&#30340;&#21644;&#29699;&#24418;&#30340;&#65292;&#28982;&#21518;&#24212;&#29992;&#20110;&#35299;&#20915;&#31526;&#21495;&#22270;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#22810;&#20010;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.07493</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#36890;&#20449;&#24615;&#20960;&#20309;&#23398;&#22312;&#25968;&#25454;&#31185;&#23398;&#20013;&#30340;&#31526;&#21495;&#22270;
&lt;/p&gt;
&lt;p&gt;
Signed graphs in data sciences via communicability geometry
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07493
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#31526;&#21495;&#22270;&#30340;&#21487;&#36890;&#20449;&#24615;&#20960;&#20309;&#27010;&#24565;&#65292;&#35777;&#26126;&#20102;&#20854;&#24230;&#37327;&#26159;&#27431;&#20960;&#37324;&#24503;&#30340;&#21644;&#29699;&#24418;&#30340;&#65292;&#28982;&#21518;&#24212;&#29992;&#20110;&#35299;&#20915;&#31526;&#21495;&#22270;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#22810;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31526;&#21495;&#22270;&#26159;&#34920;&#31034;&#22810;&#31181;&#23384;&#22312;&#20914;&#31361;&#20132;&#20114;&#30340;&#25968;&#25454;&#30340;&#26032;&#20852;&#26041;&#24335;&#65292;&#21253;&#25324;&#26469;&#33258;&#29983;&#29289;&#23398;&#12289;&#29983;&#24577;&#23398;&#21644;&#31038;&#20250;&#31995;&#32479;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#25552;&#20986;&#20102;&#31526;&#21495;&#22270;&#30340;&#21487;&#36890;&#20449;&#24615;&#20960;&#20309;&#27010;&#24565;&#65292;&#35777;&#26126;&#20102;&#22312;&#36825;&#20010;&#31354;&#38388;&#20013;&#30340;&#24230;&#37327;&#65292;&#27604;&#22914;&#21487;&#36890;&#20449;&#24615;&#36317;&#31163;&#21644;&#35282;&#24230;&#65292;&#26159;&#27431;&#20960;&#37324;&#24503;&#30340;&#21644;&#29699;&#24418;&#30340;&#12290;&#28982;&#21518;&#25105;&#20204;&#23558;&#36825;&#20123;&#24230;&#37327;&#24212;&#29992;&#20110;&#20197;&#32479;&#19968;&#26041;&#24335;&#35299;&#20915;&#31526;&#21495;&#22270;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#20960;&#20010;&#38382;&#39064;&#65292;&#21253;&#25324;&#31526;&#21495;&#22270;&#30340;&#20998;&#21306;&#12289;&#32500;&#24230;&#32422;&#31616;&#12289;&#25214;&#21040;&#31526;&#21495;&#32593;&#32476;&#20013;&#30340;&#32852;&#30431;&#31561;&#32423;&#20197;&#21450;&#37327;&#21270;&#31995;&#32479;&#20013;&#29616;&#26377;&#27966;&#31995;&#20043;&#38388;&#26497;&#21270;&#31243;&#24230;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07493v1 Announce Type: cross  Abstract: Signed graphs are an emergent way of representing data in a variety of contexts were conflicting interactions exist. These include data from biological, ecological, and social systems. Here we propose the concept of communicability geometry for signed graphs, proving that metrics in this space, such as the communicability distance and angles, are Euclidean and spherical. We then apply these metrics to solve several problems in data analysis of signed graphs in a unified way. They include the partitioning of signed graphs, dimensionality reduction, finding hierarchies of alliances in signed networks as well as the quantification of the degree of polarization between the existing factions in systems represented by this type of graphs.
&lt;/p&gt;</description></item><item><title>XpertAI&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#39044;&#27979;&#31574;&#30053;&#35299;&#24320;&#20026;&#22810;&#20010;&#29305;&#23450;&#33539;&#22260;&#30340;&#23376;&#31574;&#30053;&#65292;&#24182;&#20801;&#35768;&#23558;&#27169;&#22411;&#30340;&#26597;&#35810;&#21046;&#23450;&#20026;&#36825;&#20123;&#23376;&#31574;&#30053;&#30340;&#32447;&#24615;&#32452;&#21512;&#12290;</title><link>https://arxiv.org/abs/2403.07486</link><description>&lt;p&gt;
XpertAI&#65306;&#25581;&#31034;&#23376;&#27969;&#24418;&#30340;&#27169;&#22411;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
XpertAI: uncovering model strategies for sub-manifolds
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07486
&lt;/p&gt;
&lt;p&gt;
XpertAI&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#39044;&#27979;&#31574;&#30053;&#35299;&#24320;&#20026;&#22810;&#20010;&#29305;&#23450;&#33539;&#22260;&#30340;&#23376;&#31574;&#30053;&#65292;&#24182;&#20801;&#35768;&#23558;&#27169;&#22411;&#30340;&#26597;&#35810;&#21046;&#23450;&#20026;&#36825;&#20123;&#23376;&#31574;&#30053;&#30340;&#32447;&#24615;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26041;&#27861;&#24050;&#32463;&#20419;&#36827;&#20102;&#28145;&#20837;&#39564;&#35777;&#21644;&#30693;&#35782;&#25552;&#21462;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#23613;&#31649;&#38024;&#23545;&#20998;&#31867;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#24456;&#23569;&#26377;XAI&#35299;&#20915;&#26041;&#26696;&#35299;&#20915;&#20102;&#29305;&#23450;&#20110;&#22238;&#24402;&#27169;&#22411;&#30340;&#25361;&#25112;&#12290;&#22312;&#22238;&#24402;&#20013;&#65292;&#35299;&#37322;&#38656;&#35201;&#31934;&#30830;&#21046;&#23450;&#20197;&#24212;&#23545;&#29305;&#23450;&#29992;&#25143;&#26597;&#35810;&#65288;&#20363;&#22914;&#21306;&#20998;&#8220;&#20026;&#20160;&#20040;&#36755;&#20986;&#22823;&#20110;0&#65311;&#8221;&#21644;&#8220;&#20026;&#20160;&#20040;&#36755;&#20986;&#22823;&#20110;50&#65311;&#8221;&#65289;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#24212;&#21453;&#26144;&#27169;&#22411;&#22312;&#30456;&#20851;&#25968;&#25454;&#23376;&#27969;&#24418;&#19978;&#30340;&#34892;&#20026;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;XpertAI&#65292;&#36825;&#26159;&#19968;&#20010;&#23558;&#39044;&#27979;&#31574;&#30053;&#35299;&#24320;&#20026;&#22810;&#20010;&#33539;&#22260;&#29305;&#23450;&#30340;&#23376;&#31574;&#30053;&#65292;&#24182;&#20801;&#35768;&#23558;&#23545;&#27169;&#22411;&#30340;&#31934;&#20934;&#26597;&#35810;&#65288;&#8220;&#34987;&#35299;&#37322;&#29289;&#8221;&#65289;&#30340;&#21046;&#23450;&#20026;&#36825;&#20123;&#23376;&#31574;&#30053;&#30340;&#32447;&#24615;&#32452;&#21512;&#30340;&#26694;&#26550;&#12290;XpertAI&#36890;&#24120;&#21046;&#23450;&#21487;&#20197;&#19982;&#22522;&#20110;&#36974;&#25377;&#12289;&#26799;&#24230;&#38598;&#25104;&#25110;&#21453;&#21521;&#20256;&#25773;&#30340;&#27969;&#34892;XAI&#24402;&#22240;&#25216;&#26415;&#19968;&#36215;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07486v1 Announce Type: new  Abstract: In recent years, Explainable AI (XAI) methods have facilitated profound validation and knowledge extraction from ML models. While extensively studied for classification, few XAI solutions have addressed the challenges specific to regression models. In regression, explanations need to be precisely formulated to address specific user queries (e.g.\ distinguishing between `Why is the output above 0?' and `Why is the output above 50?'). They should furthermore reflect the model's behavior on the relevant data sub-manifold. In this paper, we introduce XpertAI, a framework that disentangles the prediction strategy into multiple range-specific sub-strategies and allows the formulation of precise queries about the model (the `explanandum') as a linear combination of those sub-strategies. XpertAI is formulated generally to work alongside popular XAI attribution techniques, based on occlusion, gradient integration, or reverse propagation. Qualitat
&lt;/p&gt;</description></item><item><title>PMBO&#36890;&#36807;&#22810;&#39033;&#24335;&#27169;&#22411;&#36924;&#36817;&#21644;&#36125;&#21494;&#26031;&#20248;&#21270;&#32467;&#21512;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#36125;&#21494;&#26031;&#20248;&#21270;&#34920;&#29616;&#26356;&#22909;&#65292;&#23545;&#30456;&#20851;&#20989;&#25968;&#21644;&#36229;&#21442;&#25968;&#35774;&#32622;&#26356;&#21152;&#40065;&#26834;&#65292;&#19982;&#36827;&#21270;&#31639;&#27861;&#24615;&#33021;&#30456;&#24403;&#12290;</title><link>https://arxiv.org/abs/2403.07485</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#39033;&#24335;&#27169;&#22411;&#20195;&#29702;&#25552;&#21319;&#40657;&#30418;&#20248;&#21270;&#24615;&#33021;&#30340; PMBO
&lt;/p&gt;
&lt;p&gt;
PMBO: Enhancing Black-Box Optimization through Multivariate Polynomial Surrogates
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07485
&lt;/p&gt;
&lt;p&gt;
PMBO&#36890;&#36807;&#22810;&#39033;&#24335;&#27169;&#22411;&#36924;&#36817;&#21644;&#36125;&#21494;&#26031;&#20248;&#21270;&#32467;&#21512;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#36125;&#21494;&#26031;&#20248;&#21270;&#34920;&#29616;&#26356;&#22909;&#65292;&#23545;&#30456;&#20851;&#20989;&#25968;&#21644;&#36229;&#21442;&#25968;&#35774;&#32622;&#26356;&#21152;&#40065;&#26834;&#65292;&#19982;&#36827;&#21270;&#31639;&#27861;&#24615;&#33021;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#29702;&#30340;&#40657;&#30418;&#20248;&#21270;&#26041;&#27861;&#65292;&#31216;&#20026;&#22810;&#39033;&#24335;&#27169;&#22411;&#20248;&#21270;&#65288;PMBO&#65289;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#22810;&#39033;&#24335;&#36924;&#36817;&#21644;&#36125;&#21494;&#26031;&#20248;&#21270;&#27493;&#39588;&#20132;&#26367;&#36827;&#34892;&#65292;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#26469;&#27169;&#25311;&#30446;&#26631;&#19982;&#20854;&#22810;&#39033;&#24335;&#25311;&#21512;&#20043;&#38388;&#30340;&#35823;&#24046;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;PMBO&#30340;&#31639;&#27861;&#35774;&#35745;&#65292;&#24182;&#23558;PMBO&#30340;&#24615;&#33021;&#32467;&#26524;&#19982;&#20960;&#31181;&#20248;&#21270;&#26041;&#27861;&#22312;&#19968;&#32452;&#35299;&#26512;&#27979;&#35797;&#20989;&#25968;&#19978;&#30340;&#34920;&#29616;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;PMBO&#20248;&#20110;&#32463;&#20856;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#24182;&#19988;&#23545;&#20110;&#30456;&#20851;&#20989;&#25968;&#26063;&#21644;&#36229;&#21442;&#25968;&#35774;&#32622;&#30340;&#36873;&#25321;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#32780;&#36825;&#22312;&#32463;&#20856;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#38656;&#35201;&#20180;&#32454;&#35843;&#25972;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;PMBO&#30340;&#24615;&#33021;&#19982;&#21327;&#26041;&#24046;&#30697;&#38453;&#36866;&#24212; - &#36827;&#21270;&#31574;&#30053;&#65288;CMA-ES&#65289;&#31561;&#26368;&#20808;&#36827;&#30340;&#36827;&#21270;&#31639;&#27861;&#30456;&#24403;&#12290;&#36825;&#19968;&#21457;&#29616;&#34920;&#26126;&#65292;PMBO&#25104;&#20026;&#20195;&#29702;&#26041;&#27861;&#20013;&#30340;&#37325;&#35201;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07485v1 Announce Type: cross  Abstract: We introduce a surrogate-based black-box optimization method, termed Polynomial-model-based optimization (PMBO). The algorithm alternates polynomial approximation with Bayesian optimization steps, using Gaussian processes to model the error between the objective and its polynomial fit. We describe the algorithmic design of PMBO and compare the results of the performance of PMBO with several optimization methods for a set of analytic test functions.   The results show that PMBO outperforms the classic Bayesian optimization and is robust with respect to the choice of its correlation function family and its hyper-parameter setting, which, on the contrary, need to be carefully tuned in classic Bayesian optimization. Remarkably, PMBO performs comparably with state-of-the-art evolutionary algorithms such as the Covariance Matrix Adaptation -- Evolution Strategy (CMA-ES). This finding suggests that PMBO emerges as the pivotal choice among sur
&lt;/p&gt;</description></item><item><title>&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#25552;&#20986;&#19968;&#31181;&#26080;&#21019;&#31958;&#23615;&#30149;&#35786;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#31070;&#32463;&#32593;&#32476;&#21644;&#25968;&#25454;&#24179;&#34913;&#25216;&#26415;&#65292;&#22312;&#20934;&#30830;&#24615;&#12289;&#25935;&#24863;&#24615;&#21644;&#29305;&#24322;&#24615;&#26041;&#38754;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;</title><link>https://arxiv.org/abs/2403.07483</link><description>&lt;p&gt;
&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#31958;&#23615;&#30149;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
A Deep Learning Approach to Diabetes Diagnosis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07483
&lt;/p&gt;
&lt;p&gt;
&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#25552;&#20986;&#19968;&#31181;&#26080;&#21019;&#31958;&#23615;&#30149;&#35786;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#31070;&#32463;&#32593;&#32476;&#21644;&#25968;&#25454;&#24179;&#34913;&#25216;&#26415;&#65292;&#22312;&#20934;&#30830;&#24615;&#12289;&#25935;&#24863;&#24615;&#21644;&#29305;&#24322;&#24615;&#26041;&#38754;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31958;&#23615;&#30149;&#26159;&#30001;&#33008;&#23707;&#32032;&#20135;&#29983;&#25110;&#21033;&#29992;&#19981;&#36275;&#23548;&#33268;&#30340;&#65292;&#23545;&#36523;&#20307;&#36896;&#25104;&#20102;&#24191;&#27867;&#30340;&#21361;&#23475;&#12290;&#29616;&#26377;&#30340;&#35786;&#26029;&#26041;&#27861;&#36890;&#24120;&#26159;&#20405;&#20837;&#24615;&#30340;&#65292;&#24182;&#20276;&#26377;&#35832;&#22810;&#32570;&#28857;&#65292;&#27604;&#22914;&#25104;&#26412;&#38480;&#21046;&#12290;&#23613;&#31649;&#23384;&#22312;&#20687;&#31867;&#38388;k&#26368;&#36817;&#37051;(CkNN)&#21644;&#36890;&#29992;&#22238;&#24402;&#31070;&#32463;&#32593;&#32476;(GRNN)&#36825;&#26679;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20294;&#23427;&#20204;&#22312;&#22788;&#29702;&#19981;&#24179;&#34913;&#25968;&#25454;&#26102;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#12290;&#21033;&#29992;&#20256;&#24863;&#25216;&#26415;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#36827;&#23637;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24102;&#26377;&#25209;&#37327;&#26631;&#20934;&#21270;&#30340;&#21453;&#21521;&#20256;&#25773;&#31070;&#32463;&#32593;&#32476;(BPNN)&#36827;&#34892;&#26080;&#21019;&#31958;&#23615;&#30149;&#35786;&#26029;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#25968;&#25454;&#37325;&#37319;&#26679;&#21644;&#24402;&#19968;&#21270;&#20197;&#23454;&#29616;&#31867;&#24179;&#34913;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#35832;&#22810;&#25361;&#25112;&#65292;&#27604;&#22914;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#20851;&#30340;&#24615;&#33021;&#21463;&#38480;&#12290;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#22312;&#25972;&#20307;&#20934;&#30830;&#24615;&#12289;&#25935;&#24863;&#24615;&#21644;&#29305;&#24322;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#29575;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07483v1 Announce Type: cross  Abstract: Diabetes, resulting from inadequate insulin production or utilization, causes extensive harm to the body. Existing diagnostic methods are often invasive and come with drawbacks, such as cost constraints. Although there are machine learning models like Classwise k Nearest Neighbor (CkNN) and General Regression Neural Network (GRNN), they struggle with imbalanced data and result in under-performance. Leveraging advancements in sensor technology and machine learning, we propose a non-invasive diabetes diagnosis using a Back Propagation Neural Network (BPNN) with batch normalization, incorporating data re-sampling and normalization for class balancing. Our method addresses existing challenges such as limited performance associated with traditional machine learning. Experimental results on three datasets show significant improvements in overall accuracy, sensitivity, and specificity compared to traditional methods. Notably, we achieve accur
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#20010;&#24615;&#21270;&#30340;&#22522;&#20110;&#22270;&#30340;&#22522;&#30784;&#24314;&#27169;&#26041;&#27861;&#65292;&#20854;&#20013;&#30340;Heterogeneous GNN&#26088;&#22312;&#25429;&#25417;&#36328;&#22810;&#31181;&#21487;&#25512;&#33616;&#39033;&#30446;&#31867;&#22411;&#30340;&#22810;&#36339;&#20869;&#23481;&#21644;&#28040;&#36153;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2403.07478</link><description>&lt;p&gt;
&#38754;&#21521;&#20010;&#24615;&#21270;&#30340;&#22270;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Towards Graph Foundation Models for Personalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07478
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#20010;&#24615;&#21270;&#30340;&#22522;&#20110;&#22270;&#30340;&#22522;&#30784;&#24314;&#27169;&#26041;&#27861;&#65292;&#20854;&#20013;&#30340;Heterogeneous GNN&#26088;&#22312;&#25429;&#25417;&#36328;&#22810;&#31181;&#21487;&#25512;&#33616;&#39033;&#30446;&#31867;&#22411;&#30340;&#22810;&#36339;&#20869;&#23481;&#21644;&#28040;&#36153;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20010;&#24615;&#21270;&#39046;&#22495;&#65292;&#25972;&#21512;&#28040;&#36153;&#20449;&#21495;&#21644;&#22522;&#20110;&#20869;&#23481;&#30340;&#34920;&#31034;&#31561;&#22810;&#26679;&#20449;&#24687;&#28304;&#21464;&#24471;&#26085;&#30410;&#20851;&#38190;&#65292;&#20197;&#26500;&#24314;&#26368;&#20808;&#36827;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#22260;&#32469;Graph Neural Networks&#65288;GNNs&#65289;&#21644;Foundation Models&#65288;FMs&#65289;&#30340;&#30740;&#31350;&#23384;&#22312;&#20004;&#22823;&#36235;&#21183;&#12290;&#34429;&#28982;GNNs&#25104;&#20026;&#24037;&#19994;&#30028;&#22312;&#35268;&#27169;&#19978;&#23454;&#29616;&#20010;&#24615;&#21270;&#30340;&#28909;&#38376;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;FMs&#26368;&#36817;&#25165;&#22240;&#20854;&#22312;&#25490;&#21517;&#21644;&#26816;&#32034;&#31561;&#20010;&#24615;&#21270;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#32780;&#21463;&#21040;&#20851;&#27880;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20010;&#24615;&#21270;&#30340;&#22522;&#20110;&#22270;&#30340;&#22522;&#30784;&#24314;&#27169;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#19968;&#31181;&#35774;&#35745;&#29992;&#20110;&#25429;&#25417;&#36328;&#21508;&#31181;&#21487;&#25512;&#33616;&#39033;&#30446;&#31867;&#22411;&#30340;&#22810;&#36339;&#20869;&#23481;&#21644;&#28040;&#36153;&#20851;&#31995;&#30340;&#24322;&#36136;GNN&#65288;HGNN&#65289;&#12290;&#20026;&#30830;&#20445;&#22522;&#30784;&#27169;&#22411;&#25152;&#38656;&#30340;&#19968;&#33324;&#24615;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25991;&#26412;&#29305;&#24449;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07478v1 Announce Type: cross  Abstract: In the realm of personalization, integrating diverse information sources such as consumption signals and content-based representations is becoming increasingly critical to build state-of-the-art solutions. In this regard, two of the biggest trends in research around this subject are Graph Neural Networks (GNNs) and Foundation Models (FMs). While GNNs emerged as a popular solution in industry for powering personalization at scale, FMs have only recently caught attention for their promising performance in personalization tasks like ranking and retrieval. In this paper, we present a graph-based foundation modeling approach tailored to personalization. Central to this approach is a Heterogeneous GNN (HGNN) designed to capture multi-hop content and consumption relationships across a range of recommendable item types. To ensure the generality required from a Foundation Model, we employ a Large Language Model (LLM) text-based featurization of
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#24179;&#34913;&#23384;&#22312;-&#20165;&#25439;&#22833;&#20989;&#25968;&#22312;&#20844;&#27665;&#31185;&#23398;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#35299;&#20915;&#29289;&#31181;&#20998;&#24067;&#27169;&#22411;&#20013;&#23384;&#22312;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#21462;&#24471;&#20102;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.07472</link><description>&lt;p&gt;
&#32771;&#34385;&#19981;&#24179;&#34913;&#30340;&#23384;&#22312;-&#20165;&#25439;&#22833;&#20989;&#25968;&#29992;&#20110;&#29289;&#31181;&#20998;&#24067;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Imbalance-aware Presence-only Loss Function for Species Distribution Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07472
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#24179;&#34913;&#23384;&#22312;-&#20165;&#25439;&#22833;&#20989;&#25968;&#22312;&#20844;&#27665;&#31185;&#23398;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#35299;&#20915;&#29289;&#31181;&#20998;&#24067;&#27169;&#22411;&#20013;&#23384;&#22312;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#21462;&#24471;&#20102;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#29983;&#29289;&#22810;&#26679;&#24615;&#26174;&#33879;&#19979;&#38477;&#65292;&#29289;&#31181;&#20998;&#24067;&#27169;&#22411;&#65288;SDMs&#65289;&#23545;&#20110;&#36890;&#36807;&#23558;&#29615;&#22659;&#26465;&#20214;&#19982;&#29289;&#31181;&#20986;&#29616;&#22320;&#32852;&#31995;&#36215;&#26469;&#65292;&#29702;&#35299;&#27668;&#20505;&#21464;&#21270;&#23545;&#29289;&#31181;&#26646;&#24687;&#22320;&#30340;&#24433;&#21709;&#33267;&#20851;&#37325;&#35201;&#12290;&#19968;&#30452;&#20197;&#26469;&#65292;&#30001;&#20110;&#29289;&#31181;&#35266;&#23519;&#30340;&#31232;&#32570;&#24615;&#21463;&#21040;&#38480;&#21046;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#36807;&#25972;&#21512;&#20844;&#27665;&#31185;&#23398;&#20513;&#35758;&#25552;&#20379;&#30340;&#26356;&#22823;&#25968;&#25454;&#38598;&#32780;&#24471;&#21040;&#26174;&#33879;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20173;&#28982;&#21463;&#21040;&#25968;&#25454;&#38598;&#20013;&#29289;&#31181;&#20043;&#38388;&#24378;&#28872;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#30340;&#22256;&#25200;&#65292;&#36825;&#24448;&#24448;&#23548;&#33268;&#23545;&#31232;&#26377;&#29289;&#31181;&#65288;&#23545;&#20445;&#25252;&#24037;&#20316;&#33267;&#20851;&#37325;&#35201;&#30340;&#29289;&#31181;&#65289;&#30340;&#24809;&#32602;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#20351;&#29992;&#24179;&#34913;&#30340;&#23384;&#22312;-&#20165;&#25439;&#22833;&#20989;&#25968;&#22312;&#22823;&#22411;&#22522;&#20110;&#20844;&#27665;&#31185;&#23398;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#32771;&#34385;&#19981;&#24179;&#34913;&#30340;&#25439;&#22833;&#20989;&#25968;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#20013;&#22343;&#20248;&#20110;&#20256;&#32479;&#25439;&#22833;&#20989;&#25968;&#65292;&#29305;&#21035;&#26159;&#22312;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07472v1 Announce Type: new  Abstract: In the face of significant biodiversity decline, species distribution models (SDMs) are essential for understanding the impact of climate change on species habitats by connecting environmental conditions to species occurrences. Traditionally limited by a scarcity of species observations, these models have significantly improved in performance through the integration of larger datasets provided by citizen science initiatives. However, they still suffer from the strong class imbalance between species within these datasets, often resulting in the penalization of rare species--those most critical for conservation efforts. To tackle this issue, this study assesses the effectiveness of training deep learning models using a balanced presence-only loss function on large citizen science-based datasets. We demonstrate that this imbalance-aware loss function outperforms traditional loss functions across various datasets and tasks, particularly in a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#25512;&#36827;&#32422;&#26463;&#30340;&#38750;&#20984;&#24615;&#30340;&#29702;&#35770;&#35265;&#35299;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#23545;&#30456;&#20851;&#23398;&#20064;&#38382;&#39064;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.07471</link><description>&lt;p&gt;
&#26377;&#20851;&#26576;&#20123;&#25512;&#36827;&#32422;&#26463;&#30340;&#38750;&#20984;&#24615;&#21450;&#20854;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
On the nonconvexity of some push-forward constraints and its consequences in machine learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07471
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#25512;&#36827;&#32422;&#26463;&#30340;&#38750;&#20984;&#24615;&#30340;&#29702;&#35770;&#35265;&#35299;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#23545;&#30456;&#20851;&#23398;&#20064;&#38382;&#39064;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
push-forward&#25805;&#20316;&#20351;&#20154;&#33021;&#22815;&#36890;&#36807;&#30830;&#23450;&#24615;&#26144;&#23556;&#37325;&#26032;&#20998;&#37197;&#27010;&#29575;&#27979;&#24230;&#12290;&#23427;&#22312;&#32479;&#35745;&#21644;&#20248;&#21270;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65306;&#35768;&#22810;&#23398;&#20064;&#38382;&#39064;&#65288;&#29305;&#21035;&#26159;&#26469;&#33258;&#26368;&#20248;&#36755;&#36816;&#12289;&#29983;&#25104;&#24314;&#27169;&#21644;&#31639;&#27861;&#20844;&#24179;&#24615;&#30340;&#38382;&#39064;&#65289;&#21253;&#25324;&#20316;&#20026;&#27169;&#22411;&#19978;&#30340;&#25512;&#36827;&#26465;&#20214;&#25110;&#22788;&#32602;&#30340;&#32422;&#26463;&#12290;&#28982;&#32780;&#65292;&#25991;&#29486;&#32570;&#20047;&#20851;&#20110;&#36825;&#20123;&#32422;&#26463;&#30340;&#65288;&#38750;&#65289;&#20984;&#24615;&#21450;&#20854;&#23545;&#30456;&#20851;&#23398;&#20064;&#38382;&#39064;&#30340;&#24433;&#21709;&#30340;&#19968;&#33324;&#29702;&#35770;&#35265;&#35299;&#12290;&#26412;&#25991;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#22312;&#31532;&#19968;&#37096;&#20998;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20004;&#32452;&#20989;&#25968;&#65288;&#23558;&#19968;&#20010;&#27010;&#29575;&#27979;&#24230;&#20256;&#36755;&#21040;&#21478;&#19968;&#20010;&#30340;&#26144;&#23556;&#65307;&#35825;&#23548;&#19981;&#21516;&#27010;&#29575;&#27979;&#24230;&#20043;&#38388;&#30456;&#31561;&#36755;&#20986;&#20998;&#24067;&#30340;&#26144;&#23556;&#65289;&#30340;&#65288;&#38750;&#65289;&#20984;&#24615;&#30340;&#19968;&#31995;&#21015;&#20805;&#20998;&#24517;&#35201;&#26465;&#20214;&#12290;&#36825;&#31361;&#20986;&#20102;&#23545;&#20110;&#22823;&#22810;&#25968;&#27010;&#29575;&#27979;&#24230;&#32780;&#35328;&#65292;&#36825;&#20123;&#25512;&#36827;&#32422;&#26463;&#26159;&#38750;&#20984;&#30340;&#12290;&#22312;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#19968;&#32467;&#26524;&#22914;&#20309;&#26263;&#31034;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07471v1 Announce Type: cross  Abstract: The push-forward operation enables one to redistribute a probability measure through a deterministic map. It plays a key role in statistics and optimization: many learning problems (notably from optimal transport, generative modeling, and algorithmic fairness) include constraints or penalties framed as push-forward conditions on the model. However, the literature lacks general theoretical insights on the (non)convexity of such constraints and its consequences on the associated learning problems. This paper aims at filling this gap. In a first part, we provide a range of sufficient and necessary conditions for the (non)convexity of two sets of functions: the maps transporting one probability measure to another; the maps inducing equal output distributions across distinct probability measures. This highlights that for most probability measures, these push-forward constraints are not convex. In a second time, we show how this result impli
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#23884;&#20837;&#24335;&#35774;&#22791;&#30340;&#36731;&#37327;&#32423; CFA &#26041;&#27861; RAGE&#65292;&#21487;&#20197;&#26816;&#27979;&#20195;&#30721;&#37325;&#29992;&#25915;&#20987;&#65288;CRA&#65289;&#65292;&#21253;&#25324;&#25511;&#21046;&#21644;&#38750;&#25511;&#21046;&#25968;&#25454;&#25915;&#20987;&#65292;&#26377;&#25928;&#25552;&#21462;&#19968;&#20010;&#25191;&#34892;&#36319;&#36394;&#30340;&#29305;&#24449;&#65292;&#24182;&#21033;&#29992;&#26080;&#30417;&#30563;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#35782;&#21035;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2403.07465</link><description>&lt;p&gt;
&#19968;&#20010;&#37117;&#19981;&#33021;&#23569;&#65306;&#22522;&#20110;GNN&#30340;&#23884;&#20837;&#24335;&#35774;&#22791;&#25511;&#21046;&#27969;&#35748;&#35777;
&lt;/p&gt;
&lt;p&gt;
One for All and All for One: GNN-based Control-Flow Attestation for Embedded Devices
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07465
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#23884;&#20837;&#24335;&#35774;&#22791;&#30340;&#36731;&#37327;&#32423; CFA &#26041;&#27861; RAGE&#65292;&#21487;&#20197;&#26816;&#27979;&#20195;&#30721;&#37325;&#29992;&#25915;&#20987;&#65288;CRA&#65289;&#65292;&#21253;&#25324;&#25511;&#21046;&#21644;&#38750;&#25511;&#21046;&#25968;&#25454;&#25915;&#20987;&#65292;&#26377;&#25928;&#25552;&#21462;&#19968;&#20010;&#25191;&#34892;&#36319;&#36394;&#30340;&#29305;&#24449;&#65292;&#24182;&#21033;&#29992;&#26080;&#30417;&#30563;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#35782;&#21035;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25511;&#21046;&#27969; attestation (CFA) &#26159;&#19968;&#31181;&#23433;&#20840;&#26381;&#21153;&#65292;&#20801;&#35768;&#19968;&#20010;&#23454;&#20307;&#65288;&#39564;&#35777;&#32773;&#65289;&#39564;&#35777;&#36828;&#31243;&#35745;&#31639;&#26426;&#31995;&#32479;&#65288;&#35777;&#26126;&#32773;&#65289;&#19978;&#20195;&#30721;&#25191;&#34892;&#30340;&#23436;&#25972;&#24615;&#12290;&#29616;&#26377;&#30340; CFA &#26041;&#26696;&#23384;&#22312;&#19981;&#20999;&#23454;&#38469;&#30340;&#20551;&#35774;&#65292;&#20363;&#22914;&#38656;&#35201;&#35775;&#38382;&#35777;&#26126;&#32773;&#30340;&#20869;&#37096;&#29366;&#24577;&#65288;&#22914;&#20869;&#23384;&#25110;&#20195;&#30721;&#65289;&#12289;&#35777;&#26126;&#32773;&#36719;&#20214;&#30340;&#23436;&#25972;&#25511;&#21046;&#27969;&#22270;&#65288;CFG&#65289;&#12289;&#22823;&#37327;&#27979;&#37327;&#25968;&#25454;&#25110;&#23450;&#21046;&#30828;&#20214;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#30340; CFA &#26041;&#26696;&#22240;&#35745;&#31639;&#24320;&#38144;&#39640;&#21644;&#36164;&#28304;&#20351;&#29992;&#29575;&#39640;&#32780;&#19981;&#36866;&#21512;&#29992;&#20110;&#23545;&#23884;&#20837;&#24335;&#31995;&#32479;&#30340;&#35748;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07465v1 Announce Type: cross  Abstract: Control-Flow Attestation (CFA) is a security service that allows an entity (verifier) to verify the integrity of code execution on a remote computer system (prover). Existing CFA schemes suffer from impractical assumptions, such as requiring access to the prover's internal state (e.g., memory or code), the complete Control-Flow Graph (CFG) of the prover's software, large sets of measurements, or tailor-made hardware. Moreover, current CFA schemes are inadequate for attesting embedded systems due to their high computational overhead and resource usage.   In this paper, we overcome the limitations of existing CFA schemes for embedded devices by introducing RAGE, a novel, lightweight CFA approach with minimal requirements. RAGE can detect Code Reuse Attacks (CRA), including control- and non-control-data attacks. It efficiently extracts features from one execution trace and leverages Unsupervised Graph Neural Networks (GNNs) to identify de
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#38598;&#25104;&#26041;&#27861;&#21644;&#20107;&#20214;&#26102;&#38388;&#20998;&#26512;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#65292;&#35777;&#26126;&#20102;&#38598;&#25104;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#24182;&#22686;&#24378;&#39044;&#27979;&#24615;&#33021;&#30340;&#31283;&#20581;&#24615;</title><link>https://arxiv.org/abs/2403.07460</link><description>&lt;p&gt;
&#36890;&#36807;&#32508;&#21512;Brier&#24471;&#20998;&#21644;&#21327;&#35843;&#25351;&#25968;&#23545;&#38598;&#25104;&#26041;&#27861;&#21644;&#20107;&#20214;&#26102;&#38388;&#20998;&#26512;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Experimental Comparison of Ensemble Methods and Time-to-Event Analysis Models Through Integrated Brier Score and Concordance Index
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07460
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#38598;&#25104;&#26041;&#27861;&#21644;&#20107;&#20214;&#26102;&#38388;&#20998;&#26512;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#65292;&#35777;&#26126;&#20102;&#38598;&#25104;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#24182;&#22686;&#24378;&#39044;&#27979;&#24615;&#33021;&#30340;&#31283;&#20581;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#21040;&#20107;&#20214;&#20998;&#26512;&#26159;&#32479;&#35745;&#23398;&#30340;&#19968;&#20010;&#20998;&#25903;&#65292;&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#22240;&#20854;&#22312;&#39044;&#27979;&#24615;&#32500;&#25252;&#12289;&#23458;&#25143;&#27969;&#22833;&#39044;&#27979;&#21644;&#20154;&#21475;&#23551;&#21629;&#20272;&#35745;&#31561;&#35768;&#22810;&#24212;&#29992;&#39046;&#22495;&#20013;&#30340;&#22686;&#21152;&#32780;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#26412;&#25991;&#22238;&#39038;&#21644;&#27604;&#36739;&#20102;&#20960;&#31181;&#29992;&#20110;&#26102;&#38388;&#21040;&#20107;&#20214;&#20998;&#26512;&#30340;&#39044;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#27169;&#22411;&#21253;&#25324;&#21322;&#21442;&#25968;&#21644;&#21442;&#25968;&#32479;&#35745;&#27169;&#22411;&#65292;&#20197;&#21450;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#29992;&#20004;&#31181;&#19981;&#21516;&#30340;&#20998;&#25968;&#65288;&#32508;&#21512;Brier&#24471;&#20998;&#21644;&#21327;&#35843;&#25351;&#25968;&#65289;&#36827;&#34892;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#38598;&#25104;&#26041;&#27861;&#22914;&#20309;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#24182;&#22686;&#24378;&#39044;&#27979;&#24615;&#33021;&#30340;&#31283;&#20581;&#24615;&#65292;&#36825;&#22312;&#26102;&#38388;&#21040;&#20107;&#20214;&#20998;&#26512;&#20013;&#20196;&#20154;&#24778;&#35766;&#22320;&#23578;&#26410;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#12290;&#25105;&#20204;&#29992;&#19968;&#20010;&#27169;&#25311;&#23454;&#39564;&#24635;&#32467;&#20102;&#20998;&#26512;&#65292;&#35780;&#20272;&#20102;&#24433;&#21709;&#26041;&#27861;&#24615;&#33021;&#25490;&#21517;&#30340;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07460v1 Announce Type: new  Abstract: Time-to-event analysis is a branch of statistics that has increased in popularity during the last decades due to its many application fields, such as predictive maintenance, customer churn prediction and population lifetime estimation. In this paper, we review and compare the performance of several prediction models for time-to-event analysis. These consist of semi-parametric and parametric statistical models, in addition to machine learning approaches. Our study is carried out on three datasets and evaluated in two different scores (the integrated Brier score and concordance index). Moreover, we show how ensemble methods, which surprisingly have not yet been much studied in time-to-event analysis, can improve the prediction accuracy and enhance the robustness of the prediction performance. We conclude the analysis with a simulation experiment in which we evaluate the factors influencing the performance ranking of the methods using both 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#35270;&#22270;&#33258;&#32534;&#30721;&#22120;&#25968;&#23398;&#26694;&#26550;&#65292;&#25972;&#21512;&#20102;&#21508;&#31181;&#20844;&#24335;&#65292;&#24182;&#25299;&#23637;&#20102; \texttt{multi-view-AE} &#24211;&#30340;&#25991;&#26723;&#21644;&#21151;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.07456</link><description>&lt;p&gt;
&#20351;&#29992; multi-view-AE &#24211;&#30340;&#22810;&#35270;&#22270;&#33258;&#32534;&#30721;&#22120;&#25945;&#31243;
&lt;/p&gt;
&lt;p&gt;
A tutorial on multi-view autoencoders using the multi-view-AE library
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07456
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#35270;&#22270;&#33258;&#32534;&#30721;&#22120;&#25968;&#23398;&#26694;&#26550;&#65292;&#25972;&#21512;&#20102;&#21508;&#31181;&#20844;&#24335;&#65292;&#24182;&#25299;&#23637;&#20102; \texttt{multi-view-AE} &#24211;&#30340;&#25991;&#26723;&#21644;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23545;&#24314;&#27169;&#25968;&#25454;&#30340;&#22810;&#20010;&#27169;&#24577;&#65288;&#25110;&#35270;&#22270;&#65289;&#20197;&#20415;&#29702;&#35299;&#27169;&#24577;&#20043;&#38388;&#30340;&#20851;&#31995;&#25110;&#29983;&#25104;&#32570;&#22833;&#25968;&#25454;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#22810;&#35270;&#22270;&#33258;&#32534;&#30721;&#22120;&#22240;&#20854;&#33021;&#22815;&#36866;&#24212;&#21644;&#28789;&#27963;&#24314;&#27169;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#33021;&#21147;&#32780;&#22791;&#21463;&#20851;&#27880;&#65292;&#34920;&#26126;&#20854;&#20855;&#26377;&#26681;&#25454;&#25163;&#22836;&#25968;&#25454;&#29305;&#24449;&#35843;&#25972;&#26041;&#27861;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#22810;&#35270;&#22270;&#33258;&#32534;&#30721;&#22120;&#23384;&#22312;&#19968;&#33268;&#24615;&#31526;&#21495;&#26631;&#27880;&#19981;&#19968;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#36890;&#24120;&#20351;&#29992;&#19981;&#21516;&#30340;&#32534;&#30721;&#26694;&#26550;&#23454;&#29616;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#35270;&#22270;&#33258;&#32534;&#30721;&#22120;&#25968;&#23398;&#26694;&#26550;&#65292;&#25972;&#21512;&#20102;&#23427;&#20204;&#30340;&#20844;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#27599;&#20010;&#27169;&#22411;&#21160;&#26426;&#21644;&#29702;&#35770;&#20248;&#21183;&#30340;&#35265;&#35299;&#12290;&#20026;&#20102;&#26041;&#20415;&#35775;&#38382;&#21644;&#23454;&#38469;&#20351;&#29992;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#20808;&#21069;&#20171;&#32461;&#30340; multi-view-AE &#24211;&#30340;&#25991;&#26723;&#21644;&#21151;&#33021;&#12290;&#35813;&#24211;&#25552;&#20379;&#20102; Python &#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07456v1 Announce Type: new  Abstract: There has been a growing interest in recent years in modelling multiple modalities (or views) of data to for example, understand the relationship between modalities or to generate missing data. Multi-view autoencoders have gained significant traction for their adaptability and versatility in modelling multi-modal data, demonstrating an ability to tailor their approach to suit the characteristics of the data at hand. However, most multi-view autoencoders have inconsistent notation and are often implemented using different coding frameworks. To address this, we present a unified mathematical framework for multi-view autoencoders, consolidating their formulations. Moreover, we offer insights into the motivation and theoretical advantages of each model. To facilitate accessibility and practical use, we extend the documentation and functionality of the previously introduced \texttt{multi-view-AE} library. This library offers Python implementa
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#32467;&#26500;&#28151;&#21512;&#27010;&#29575;&#20998;&#24067;&#25552;&#20379;&#20102;&#20934;&#30830;&#30340;&#21518;&#39564;&#25512;&#26029;&#65292;&#21516;&#26102;&#20855;&#26377;&#26356;&#23567;&#30340;&#35745;&#31639;&#21344;&#29992;&#37327;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;SBI&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.07454</link><description>&lt;p&gt;
&#20351;&#29992;&#39640;&#26031;&#23616;&#37096;&#32447;&#24615;&#26144;&#23556;&#36827;&#34892;&#24555;&#36895;&#12289;&#20934;&#30830;&#21644;&#36731;&#37327;&#32423;&#30340;&#39034;&#24207;&#20223;&#30495;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Fast, accurate and lightweight sequential simulation-based inference using Gaussian locally linear mappings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07454
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#32467;&#26500;&#28151;&#21512;&#27010;&#29575;&#20998;&#24067;&#25552;&#20379;&#20102;&#20934;&#30830;&#30340;&#21518;&#39564;&#25512;&#26029;&#65292;&#21516;&#26102;&#20855;&#26377;&#26356;&#23567;&#30340;&#35745;&#31639;&#21344;&#29992;&#37327;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;SBI&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07454v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#38024;&#23545;&#20855;&#26377;&#38590;&#20197;&#22788;&#29702;&#30340;&#20284;&#28982;&#20989;&#25968;&#30340;&#22797;&#26434;&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#21487;&#20197;&#20351;&#29992;&#22810;&#27425;&#35843;&#29992;&#35745;&#31639;&#27169;&#25311;&#22120;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#12290; &#36825;&#20123;&#26041;&#27861;&#34987;&#32479;&#31216;&#20026;&#8220;&#22522;&#20110;&#20223;&#30495;&#30340;&#25512;&#26029;&#8221;&#65288;SBI&#65289;&#12290; &#26368;&#36817;&#30340;SBI&#26041;&#27861;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#25552;&#20379;&#36817;&#20284;&#20294;&#34920;&#36798;&#20016;&#23500;&#30340;&#26500;&#36896;&#65292;&#29992;&#20110;&#19981;&#21487;&#29992;&#30340;&#20284;&#28982;&#20989;&#25968;&#21644;&#21518;&#39564;&#20998;&#24067;&#12290; &#28982;&#32780;&#65292;&#23427;&#20204;&#36890;&#24120;&#26080;&#27861;&#23454;&#29616;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#38656;&#27714;&#20043;&#38388;&#30340;&#26368;&#20339;&#25240;&#34935;&#12290; &#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25552;&#20379;&#20284;&#28982;&#20989;&#25968;&#21644;&#21518;&#39564;&#20998;&#24067;&#36817;&#20284;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#20351;&#29992;&#32467;&#26500;&#21270;&#30340;&#27010;&#29575;&#20998;&#24067;&#28151;&#21512;&#29289;&#12290; &#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;NN&#30340;SBI&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20135;&#29983;&#20934;&#30830;&#30340;&#21518;&#39564;&#25512;&#26029;&#30340;&#21516;&#26102;&#65292;&#20855;&#26377;&#26356;&#23567;&#30340;&#35745;&#31639;&#21344;&#29992;&#37327;&#12290; &#25105;&#20204;&#22312;SBI&#25991;&#29486;&#20013;&#30340;&#20960;&#20010;&#22522;&#20934;&#27169;&#22411;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07454v1 Announce Type: cross  Abstract: Bayesian inference for complex models with an intractable likelihood can be tackled using algorithms performing many calls to computer simulators. These approaches are collectively known as "simulation-based inference" (SBI). Recent SBI methods have made use of neural networks (NN) to provide approximate, yet expressive constructs for the unavailable likelihood function and the posterior distribution. However, they do not generally achieve an optimal trade-off between accuracy and computational demand. In this work, we propose an alternative that provides both approximations to the likelihood and the posterior distribution, using structured mixtures of probability distributions. Our approach produces accurate posterior inference when compared to state-of-the-art NN-based SBI methods, while exhibiting a much smaller computational footprint. We illustrate our results on several benchmark models from the SBI literature.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#38024;&#23545;&#36153;&#31859;&#23376;&#26102;&#38388;&#30456;&#20851;&#27874;&#20989;&#25968;&#30340;&#21464;&#20998;&#26041;&#27861;&#65292;&#36890;&#36807;&#25429;&#25417;&#22810;&#20307;&#30456;&#20851;&#24615;&#36229;&#36234;&#24179;&#22343;&#22330;&#36817;&#20284;&#65292;&#21487;&#20197;&#35299;&#20915;&#23454;&#26102;&#28436;&#21270;&#38750;&#24179;&#34913;&#37327;&#23376;&#30005;&#23376;&#31995;&#32479;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.07447</link><description>&lt;p&gt;
&#38024;&#23545;&#26102;&#38388;&#30456;&#20851;&#22810;&#30005;&#23376;Schr\"odinger&#26041;&#31243;&#30340;&#20174;&#22836;&#21464;&#20998;&#27874;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Ab-initio variational wave functions for the time-dependent many-electron Schr\"odinger equation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07447
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#38024;&#23545;&#36153;&#31859;&#23376;&#26102;&#38388;&#30456;&#20851;&#27874;&#20989;&#25968;&#30340;&#21464;&#20998;&#26041;&#27861;&#65292;&#36890;&#36807;&#25429;&#25417;&#22810;&#20307;&#30456;&#20851;&#24615;&#36229;&#36234;&#24179;&#22343;&#22330;&#36817;&#20284;&#65292;&#21487;&#20197;&#35299;&#20915;&#23454;&#26102;&#28436;&#21270;&#38750;&#24179;&#34913;&#37327;&#23376;&#30005;&#23376;&#31995;&#32479;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07447v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449; &#25688;&#35201;&#65306;&#25551;&#36848;&#22810;&#30005;&#23376;&#37327;&#23376;&#31995;&#32479;&#21160;&#21147;&#23398;&#23545;&#20110;&#39044;&#27979;&#37327;&#23376;&#21270;&#23398;&#20013;&#30340;&#30005;&#23376;&#32467;&#26500;&#12289;&#20957;&#32858;&#24577;&#31995;&#32479;&#30340;&#24615;&#36136;&#21644;&#22797;&#26434;&#26448;&#26009;&#30340;&#34892;&#20026;&#31561;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#38750;&#24179;&#34913;&#37327;&#23376;&#30005;&#23376;&#31995;&#32479;&#30340;&#23454;&#26102;&#28436;&#21270;&#23545;&#20110;&#29702;&#35770;&#21644;&#35745;&#31639;&#26041;&#27861;&#26469;&#35828;&#26159;&#19968;&#20010;&#24040;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#31995;&#32479;&#25506;&#32034;&#20102;&#24191;&#38420;&#30340;&#26500;&#22411;&#31354;&#38388;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#38024;&#23545;&#36153;&#31859;&#23376;&#26102;&#38388;&#30456;&#20851;&#27874;&#20989;&#25968;&#30340;&#21464;&#20998;&#26041;&#27861;&#65292;&#36890;&#36807;&#25429;&#25417;&#22810;&#20307;&#30456;&#20851;&#24615;&#36229;&#36234;&#24179;&#22343;&#22330;&#36817;&#20284;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#28041;&#21450;&#21442;&#25968;&#21270;&#26102;&#38388;&#28436;&#21270;&#30340;&#37327;&#23376;&#24577;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#24577;&#28436;&#21270;&#30340;&#36817;&#20284;&#12290;&#20026;&#20102;&#32771;&#34385;&#30005;&#23376;&#30456;&#20851;&#24615;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#26102;&#38388;&#30456;&#20851;&#30340;Jastrow&#22240;&#23376;&#21644;&#22238;&#27969;&#21464;&#25442;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#21487;&#20197;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#21442;&#25968;&#21270;&#36825;&#20123;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07447v1 Announce Type: cross  Abstract: Describing the dynamics of many-electron quantum systems is crucial for applications such as predicting electronic structures in quantum chemistry, the properties of condensed matter systems, and the behaviors of complex materials. However, the real-time evolution of non-equilibrium quantum electronic systems poses a significant challenge for theoretical and computational approaches, due to the system's exploration of a vast configuration space. This work introduces a variational approach for fermionic time-dependent wave functions, surpassing mean-field approximations by capturing many-body correlations. The proposed methodology involves parameterizing the time-evolving quantum state, enabling the approximation of the state's evolution. To account for electron correlations, we employ time-dependent Jastrow factors and backflow transformations. We also show that we can incorporate neural networks to parameterize these functions. The ti
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#22495;&#33258;&#36866;&#24212;&#38382;&#39064;&#30340;&#20195;&#29702;&#26041;&#27861;&#65292;&#21033;&#29992;&#36817;&#31471;&#22240;&#26524;&#23398;&#20064;&#25216;&#26415;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#65292;&#22312;&#19981;&#24674;&#22797;&#25110;&#24314;&#27169;&#28508;&#22312;&#21464;&#37327;&#30340;&#24773;&#20917;&#19979;&#36890;&#36807;&#20195;&#29702;&#21464;&#37327;&#23454;&#29616;&#20102;&#23545;&#20998;&#24067;&#36716;&#31227;&#30340;&#36866;&#24212;&#12290;</title><link>https://arxiv.org/abs/2403.07442</link><description>&lt;p&gt;
&#38024;&#23545;&#22495;&#33258;&#36866;&#24212;&#30340;&#20195;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Proxy Methods for Domain Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07442
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#22495;&#33258;&#36866;&#24212;&#38382;&#39064;&#30340;&#20195;&#29702;&#26041;&#27861;&#65292;&#21033;&#29992;&#36817;&#31471;&#22240;&#26524;&#23398;&#20064;&#25216;&#26415;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#65292;&#22312;&#19981;&#24674;&#22797;&#25110;&#24314;&#27169;&#28508;&#22312;&#21464;&#37327;&#30340;&#24773;&#20917;&#19979;&#36890;&#36807;&#20195;&#29702;&#21464;&#37327;&#23454;&#29616;&#20102;&#23545;&#20998;&#24067;&#36716;&#31227;&#30340;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#20998;&#24067;&#36716;&#31227;&#19979;&#30340;&#22495;&#33258;&#36866;&#24212;&#38382;&#39064;&#65292;&#36825;&#31181;&#36716;&#31227;&#26159;&#30001;&#20110;&#26410;&#35266;&#23519;&#21040;&#30340;&#28508;&#22312;&#21464;&#37327;&#20998;&#24067;&#21457;&#29983;&#25913;&#21464;&#65292;&#23548;&#33268;&#21327;&#21464;&#37327;&#21644;&#26631;&#31614;&#37117;&#34987;&#28151;&#28102;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#26082;&#19981;&#36866;&#29992;&#21327;&#21464;&#37327;&#36716;&#31227;&#20063;&#19981;&#36866;&#29992;&#26631;&#31614;&#36716;&#31227;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#30340;&#33258;&#36866;&#24212;&#26041;&#27861;&#37319;&#29992;&#20102;&#36817;&#31471;&#22240;&#26524;&#23398;&#20064;&#65292;&#19968;&#31181;&#22312;&#21487;&#33719;&#24471;&#26410;&#35266;&#23519;&#28151;&#28102;&#21464;&#37327;&#20195;&#29702;&#30340;&#35774;&#32622;&#20013;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#30340;&#25216;&#26415;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20195;&#29702;&#21464;&#37327;&#20801;&#35768;&#36866;&#24212;&#20998;&#24067;&#36716;&#31227;&#65292;&#32780;&#26080;&#38656;&#26174;&#24335;&#24674;&#22797;&#25110;&#24314;&#27169;&#28508;&#22312;&#21464;&#37327;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#31181;&#24773;&#20917;&#65292;(i) &#27010;&#24565;&#29942;&#39048;&#65306;&#35266;&#23519;&#21040;&#19968;&#20010;&#39069;&#22806;&#30340;&#8220;&#27010;&#24565;&#8221;&#21464;&#37327;&#65292;&#23427;&#22312;&#21327;&#21464;&#37327;&#21644;&#26631;&#31614;&#20043;&#38388;&#36215;&#21040;&#20013;&#20171;&#20316;&#29992;&#65307;(ii) &#22810;&#39046;&#22495;&#65306;&#21487;&#29992;&#26469;&#33258;&#22810;&#20010;&#28304;&#39046;&#22495;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20854;&#20013;&#27599;&#20010;&#28304;&#39046;&#22495;&#23637;&#31034;&#20102;&#23545;&#28508;&#22312;&#28151;&#26434;&#21464;&#37327;&#30340;&#19981;&#21516;&#20998;&#24067;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340; k
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07442v1 Announce Type: new  Abstract: We study the problem of domain adaptation under distribution shift, where the shift is due to a change in the distribution of an unobserved, latent variable that confounds both the covariates and the labels. In this setting, neither the covariate shift nor the label shift assumptions apply. Our approach to adaptation employs proximal causal learning, a technique for estimating causal effects in settings where proxies of unobserved confounders are available. We demonstrate that proxy variables allow for adaptation to distribution shift without explicitly recovering or modeling latent variables. We consider two settings, (i) Concept Bottleneck: an additional ''concept'' variable is observed that mediates the relationship between the covariates and labels; (ii) Multi-domain: training data from multiple source domains is available, where each source domain exhibits a different distribution over the latent confounder. We develop a two-stage k
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#22810;&#20010;&#20027;&#25104;&#20998;&#20998;&#26512;&#30740;&#31350;&#30340;&#30693;&#35782;&#36716;&#31227;&#31639;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#22810;&#20010;&#30740;&#31350;&#20013;&#20849;&#20139;&#30340;&#23376;&#31354;&#38388;&#20449;&#24687;&#26469;&#22686;&#24378;&#30446;&#26631;PCA&#20219;&#21153;&#30340;&#20272;&#35745;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.07431</link><description>&lt;p&gt;
&#36328;&#22810;&#20010;&#20027;&#25104;&#20998;&#20998;&#26512;&#30740;&#31350;&#30340;&#30693;&#35782;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
Knowledge Transfer across Multiple Principal Component Analysis Studies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07431
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#22810;&#20010;&#20027;&#25104;&#20998;&#20998;&#26512;&#30740;&#31350;&#30340;&#30693;&#35782;&#36716;&#31227;&#31639;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#22810;&#20010;&#30740;&#31350;&#20013;&#20849;&#20139;&#30340;&#23376;&#31354;&#38388;&#20449;&#24687;&#26469;&#22686;&#24378;&#30446;&#26631;PCA&#20219;&#21153;&#30340;&#20272;&#35745;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transfer learning&#22312;&#32479;&#35745;&#39046;&#22495;&#24341;&#36215;&#20102;&#26497;&#22823;&#20852;&#36259;&#12290;&#26412;&#25991;&#20851;&#27880;&#20110;&#26080;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#30340;&#30693;&#35782;&#36716;&#31227;&#65292;&#19982;&#25991;&#29486;&#20013;&#30340;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#30456;&#23545;&#27604;&#12290;&#22312;&#32473;&#23450;&#21487;&#36716;&#31227;&#30340;&#28304;&#20154;&#21475;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#27493;&#30340;&#36716;&#31227;&#23398;&#20064;&#31639;&#27861;&#65292;&#20174;&#22810;&#20010;&#28304;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#30740;&#31350;&#20013;&#25552;&#21462;&#26377;&#29992;&#20449;&#24687;&#65292;&#20174;&#32780;&#22686;&#24378;&#30446;&#26631;PCA&#20219;&#21153;&#30340;&#20272;&#35745;&#20934;&#30830;&#24615;&#12290;&#31532;&#19968;&#27493;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#21517;&#20026;Grassmannian barycenter&#30340;&#26041;&#27861;&#65292;&#25972;&#21512;&#36328;&#22810;&#20010;&#30740;&#31350;&#20849;&#20139;&#30340;&#23376;&#31354;&#38388;&#20449;&#24687;&#65292;&#32780;&#19981;&#26159;&#30452;&#25509;&#22312;&#27719;&#24635;&#25968;&#25454;&#38598;&#19978;&#25191;&#34892;PCA&#12290;&#25552;&#20986;&#30340;Grassmannian barycenter&#26041;&#27861;&#22312;&#26356;&#19968;&#33324;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#40065;&#26834;&#24615;&#21644;&#35745;&#31639;&#20248;&#21183;&#12290;&#28982;&#21518;&#65292;&#31532;&#19968;&#27493;&#24471;&#21040;&#30340;&#20849;&#20139;&#23376;&#31354;&#38388;&#30340;&#20272;&#35745;&#22120;&#36827;&#19968;&#27493;&#29992;&#20110;&#20272;&#35745;&#31532;&#20108;&#27493;&#20013;&#30340;&#30446;&#26631;&#31169;&#26377;&#23376;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07431v1 Announce Type: cross  Abstract: Transfer learning has aroused great interest in the statistical community. In this article, we focus on knowledge transfer for unsupervised learning tasks in contrast to the supervised learning tasks in the literature. Given the transferable source populations, we propose a two-step transfer learning algorithm to extract useful information from multiple source principal component analysis (PCA) studies, thereby enhancing estimation accuracy for the target PCA task. In the first step, we integrate the shared subspace information across multiple studies by a proposed method named as Grassmannian barycenter, instead of directly performing PCA on the pooled dataset. The proposed Grassmannian barycenter method enjoys robustness and computational advantages in more general cases. Then the resulting estimator for the shared subspace from the first step is further utilized to estimate the target private subspace in the second step. Our theoret
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20027;&#35201;&#25506;&#35752;&#20102;&#22312;&#22312;&#32447;&#38382;&#39064;&#20013;&#37319;&#29992;&#20855;&#26377;&#26174;&#24335;&#39044;&#27979;&#22120;&#30340;&#23398;&#20064;&#22686;&#24378;&#31639;&#27861;&#65292;&#36890;&#36807;&#20801;&#35768;&#39044;&#27979;&#22120;&#38543;&#30528;&#36755;&#20837;&#30340;&#22686;&#21152;&#36827;&#34892;&#23398;&#20064;&#65292;&#26088;&#22312;&#35774;&#35745;&#20986;&#19987;&#38376;&#38024;&#23545;&#29305;&#23450;&#31639;&#27861;&#20219;&#21153;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.07413</link><description>&lt;p&gt;
&#20855;&#26377;&#26174;&#24335;&#39044;&#27979;&#22120;&#30340;&#23398;&#20064;&#22686;&#24378;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning-Augmented Algorithms with Explicit Predictors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07413
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20027;&#35201;&#25506;&#35752;&#20102;&#22312;&#22312;&#32447;&#38382;&#39064;&#20013;&#37319;&#29992;&#20855;&#26377;&#26174;&#24335;&#39044;&#27979;&#22120;&#30340;&#23398;&#20064;&#22686;&#24378;&#31639;&#27861;&#65292;&#36890;&#36807;&#20801;&#35768;&#39044;&#27979;&#22120;&#38543;&#30528;&#36755;&#20837;&#30340;&#22686;&#21152;&#36827;&#34892;&#23398;&#20064;&#65292;&#26088;&#22312;&#35774;&#35745;&#20986;&#19987;&#38376;&#38024;&#23545;&#29305;&#23450;&#31639;&#27861;&#20219;&#21153;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#31639;&#27861;&#35774;&#35745;&#26041;&#38754;&#30340;&#36827;&#23637;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#30001;&#36807;&#21435;&#21644;&#24403;&#21069;&#25968;&#25454;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24471;&#21040;&#30340;&#39044;&#27979;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#39044;&#27979;&#20934;&#30830;&#26102;&#34920;&#29616;&#20986;&#24615;&#33021;&#22686;&#24378;&#65292;&#21516;&#26102;&#22312;&#39044;&#27979;&#22833;&#36133;&#26102;&#25552;&#20379;&#26368;&#22351;&#24773;&#20917;&#20445;&#35777;&#20197;&#30830;&#20445;&#40065;&#26834;&#24615;&#12290;&#26412;&#25991;&#20851;&#27880;&#22312;&#32447;&#38382;&#39064;&#65307;&#20808;&#21069;&#30740;&#31350;&#38598;&#20013;&#22312;&#19968;&#20010;&#33539;&#24335;&#19978;&#65292;&#20854;&#20013;&#39044;&#27979;&#22120;&#22312;&#36807;&#21435;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#20316;&#20026;&#40657;&#21283;&#23376;&#20351;&#29992;&#65288;&#33719;&#24471;&#23427;&#34987;&#35757;&#32451;&#29992;&#20110;&#30340;&#39044;&#27979;&#65289;&#12290;&#30456;&#21453;&#65292;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25286;&#35299;&#39044;&#27979;&#22120;&#24182;&#23558;&#20854;&#24341;&#21457;&#30340;&#23398;&#20064;&#38382;&#39064;&#25972;&#21512;&#21040;&#31639;&#27861;&#25361;&#25112;&#20013;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20801;&#35768;&#39044;&#27979;&#22120;&#22312;&#25509;&#25910;&#26356;&#22823;&#36755;&#20837;&#37096;&#20998;&#26102;&#36827;&#34892;&#23398;&#20064;&#65292;&#26368;&#32456;&#35774;&#35745;&#20986;&#19987;&#38376;&#38024;&#23545;&#25163;&#22836;&#31639;&#27861;&#20219;&#21153;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#12290;&#37319;&#29992;&#36825;&#31181;&#35266;&#28857;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#20960;&#20010;&#22522;&#26412;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07413v1 Announce Type: new  Abstract: Recent advances in algorithmic design show how to utilize predictions obtained by machine learning models from past and present data. These approaches have demonstrated an enhancement in performance when the predictions are accurate, while also ensuring robustness by providing worst-case guarantees when predictions fail. In this paper we focus on online problems; prior research in this context was focused on a paradigm where the predictor is pre-trained on past data and then used as a black box (to get the predictions it was trained for). In contrast, in this work, we unpack the predictor and integrate the learning problem it gives rise for within the algorithmic challenge. In particular we allow the predictor to learn as it receives larger parts of the input, with the ultimate goal of designing online learning algorithms specifically tailored for the algorithmic task at hand. Adopting this perspective, we focus on a number of fundamenta
&lt;/p&gt;</description></item><item><title>&#26089;&#26399;&#36864;&#20986;&#32593;&#32476;&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#23637;&#29616;&#20986;&#38477;&#20302;&#36951;&#24536;&#21644;&#22312;&#36164;&#28304;&#21033;&#29992;&#19978;&#34920;&#29616;&#20248;&#24322;&#30340;&#29305;&#28857;</title><link>https://arxiv.org/abs/2403.07404</link><description>&lt;p&gt;
&#25552;&#39640;&#25512;&#29702;&#36895;&#24230;&#21644;&#20943;&#23569;&#36951;&#24536;&#65306;&#26089;&#26399;&#36864;&#20986;&#32593;&#32476;&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#30340;&#21452;&#37325;&#22909;&#22788;
&lt;/p&gt;
&lt;p&gt;
Accelerated Inference and Reduced Forgetting: The Dual Benefits of Early-Exit Networks in Continual Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07404
&lt;/p&gt;
&lt;p&gt;
&#26089;&#26399;&#36864;&#20986;&#32593;&#32476;&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#23637;&#29616;&#20986;&#38477;&#20302;&#36951;&#24536;&#21644;&#22312;&#36164;&#28304;&#21033;&#29992;&#19978;&#34920;&#29616;&#20248;&#24322;&#30340;&#29305;&#28857;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07404v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#30028; &#25688;&#35201;: &#21463;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#33021;&#28304;&#39640;&#25928;&#21033;&#29992;&#38656;&#27714;&#39537;&#21160;&#65292;&#26089;&#26399;&#36864;&#20986;&#26041;&#27861;&#22791;&#21463;&#20851;&#27880;&#12290;&#36825;&#20123;&#31574;&#30053;&#36890;&#36807;&#22312;&#32593;&#32476;&#26089;&#26399;&#20570;&#20986;&#20915;&#23450;&#65292;&#23454;&#29616;&#24555;&#36895;&#39044;&#27979;&#65292;&#20174;&#32780;&#33410;&#30465;&#35745;&#31639;&#26102;&#38388;&#21644;&#36164;&#28304;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#26089;&#26399;&#36864;&#20986;&#32593;&#32476;&#20165;&#38024;&#23545;&#38745;&#24577;&#25968;&#25454;&#20998;&#24067;&#36827;&#34892;&#20102;&#24320;&#21457;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#20855;&#26377;&#25345;&#32493;&#38750;&#38745;&#24577;&#25968;&#25454;&#30340;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#26089;&#26399;&#36864;&#20986;&#32593;&#32476;&#30340;&#25345;&#32493;&#23398;&#20064;&#12290;&#25105;&#20204;&#25913;&#32534;&#29616;&#26377;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#20197;&#36866;&#24212;&#26089;&#26399;&#36864;&#20986;&#26550;&#26500;&#65292;&#24182;&#30740;&#31350;&#23427;&#20204;&#22312;&#25345;&#32493;&#35774;&#32622;&#20013;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#27880;&#24847;&#21040;&#65292;&#26089;&#26399;&#32593;&#32476;&#23618;&#34920;&#29616;&#20986;&#20943;&#23569;&#36951;&#24536;&#65292;&#21363;&#20351;&#20351;&#29992;&#30340;&#36164;&#28304;&#26174;&#33879;&#26356;&#23569;&#65292;&#20063;&#33021;&#32988;&#36807;&#26631;&#20934;&#32593;&#32476;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20998;&#26512;&#20219;&#21153;&#26368;&#36817;&#24615;&#20559;&#24046;&#23545;&#26089;&#26399;&#36864;&#20986;&#25512;&#29702;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20219;&#21153;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07404v1 Announce Type: cross  Abstract: Driven by the demand for energy-efficient employment of deep neural networks, early-exit methods have experienced a notable increase in research attention. These strategies allow for swift predictions by making decisions early in the network, thereby conserving computation time and resources. However, so far the early-exit networks have only been developed for stationary data distributions, which restricts their application in real-world scenarios with continuous non-stationary data. This study aims to explore the continual learning of the early-exit networks. We adapt existing continual learning methods to fit with early-exit architectures and investigate their behavior in the continual setting. We notice that early network layers exhibit reduced forgetting and can outperform standard networks even when using significantly fewer resources. Furthermore, we analyze the impact of task-recency bias on early-exit inference and propose Task
&lt;/p&gt;</description></item><item><title>S2L&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24635;&#32467;&#23567;&#27169;&#22411;&#30340;&#35757;&#32451;&#36712;&#36857;&#65292;&#26469;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25968;&#25454;&#36873;&#25321;&#30340;&#26041;&#27861;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#20013;&#30417;&#30563;&#24494;&#35843;&#30340;&#25968;&#25454;&#25928;&#29575;&#65292;&#24182;&#22312;&#25968;&#25454;&#38598;&#24615;&#33021;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>https://arxiv.org/abs/2403.07384</link><description>&lt;p&gt;
SmallToLarge (S2L): &#36890;&#36807;&#24635;&#32467;&#23567;&#27169;&#22411;&#30340;&#35757;&#32451;&#36712;&#36857;&#65292;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#25552;&#20379;&#21487;&#20280;&#32553;&#30340;&#25968;&#25454;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
SmallToLarge (S2L): Scalable Data Selection for Fine-tuning Large Language Models by Summarizing Training Trajectories of Small Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07384
&lt;/p&gt;
&lt;p&gt;
S2L&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24635;&#32467;&#23567;&#27169;&#22411;&#30340;&#35757;&#32451;&#36712;&#36857;&#65292;&#26469;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25968;&#25454;&#36873;&#25321;&#30340;&#26041;&#27861;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#20013;&#30417;&#30563;&#24494;&#35843;&#30340;&#25968;&#25454;&#25928;&#29575;&#65292;&#24182;&#22312;&#25968;&#25454;&#38598;&#24615;&#33021;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25968;&#25454;&#36873;&#25321;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#39044;&#35757;&#32451;&#21644;&#25351;&#23548;&#24494;&#35843;&#38454;&#27573;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#22312;&#19987;&#19994;&#39046;&#22495;&#30340;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#20013;&#25913;&#21892;&#25968;&#25454;&#25928;&#29575;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#65292;&#21407;&#22240;&#26159;&#24494;&#35843;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#12290;&#20026;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26377;&#25928;&#19988;&#21487;&#20280;&#32553;&#30340;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;S2L&#65288;SmallToLarge&#65289;&#65292;&#23427;&#21033;&#29992;&#23567;&#27169;&#22411;&#30340;&#35757;&#32451;&#36712;&#36857;&#26469;&#25351;&#23548;&#26356;&#22823;&#27169;&#22411;&#30340;&#25968;&#25454;&#36873;&#25321;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;S2L&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#30340;SFT&#25968;&#25454;&#25928;&#29575;&#65292;&#23558;&#35757;&#32451;&#25968;&#25454;&#32553;&#20943;&#21040;&#21407;&#22987;MathInstruct&#25968;&#25454;&#38598;&#65288;Yue&#31561;&#20154;&#65292;2023&#65289;&#30340;&#20165;11%&#65292;&#20197;&#36798;&#21040;&#20840;&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;6&#20010;&#39046;&#22495;&#20869;&#22806;&#35780;&#20272;&#25968;&#25454;&#38598;&#20013;&#24179;&#22343;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#25968;&#25454;&#36873;&#25321;&#31639;&#27861;4.7%&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#20165;&#36873;&#25321;50K&#25968;&#25454;&#36827;&#34892;SFT&#65292;S2L&#23454;&#29616;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07384v1 Announce Type: cross  Abstract: Despite the effectiveness of data selection for large language models (LLMs) during pretraining and instruction fine-tuning phases, improving data efficiency in supervised fine-tuning (SFT) for specialized domains poses significant challenges due to the complexity of fine-tuning data. To bridge this gap, we introduce an effective and scalable data selection method for SFT, SmallToLarge (S2L), which leverages training trajectories from small models to guide the data selection for larger models. We demonstrate through extensive experiments that S2L significantly improves data efficiency in SFT for mathematical problem-solving, reducing the training data to just 11% of the original MathInstruct dataset (Yue et al., 2023) to match full dataset performance while outperforming state-of-the-art data selection algorithms by an average of 4.7% across 6 in- and out-domain evaluation datasets. Remarkably, selecting only 50K data for SFT, S2L achi
&lt;/p&gt;</description></item><item><title>&#20998;&#26512;&#31070;&#32463;&#32593;&#32476;&#21644;LLMs&#20013;&#20248;&#21270;&#36712;&#36857;&#30340;&#22797;&#26434;&#24615;&#65292;&#25581;&#31034;&#20102;&#20248;&#21270;&#36807;&#31243;&#20013;&#30340;&#20851;&#38190;&#29305;&#24449;&#65292;&#21253;&#25324;&#26041;&#21521;&#25506;&#32034;&#21644;&#26041;&#21521;&#27491;&#21017;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.07379</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#21644;LLMs&#20013;&#20248;&#21270;&#36712;&#36857;&#30340;&#29305;&#24449;&#65306;&#38271;&#24230;&#12289;&#25296;&#28857;&#21644;&#27515;&#32993;&#21516;
&lt;/p&gt;
&lt;p&gt;
Hallmarks of Optimization Trajectories in Neural Networks and LLMs: The Lengths, Bends, and Dead Ends
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07379
&lt;/p&gt;
&lt;p&gt;
&#20998;&#26512;&#31070;&#32463;&#32593;&#32476;&#21644;LLMs&#20013;&#20248;&#21270;&#36712;&#36857;&#30340;&#22797;&#26434;&#24615;&#65292;&#25581;&#31034;&#20102;&#20248;&#21270;&#36807;&#31243;&#20013;&#30340;&#20851;&#38190;&#29305;&#24449;&#65292;&#21253;&#25324;&#26041;&#21521;&#25506;&#32034;&#21644;&#26041;&#21521;&#27491;&#21017;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#26041;&#27861;&#26469;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#30340;&#26426;&#21046;&#65292;&#36890;&#36807;&#20998;&#26512;&#20854;&#20248;&#21270;&#36712;&#36857;&#20013;&#21253;&#21547;&#30340;&#20016;&#23500;&#21442;&#25968;&#32467;&#26500;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20123;&#20851;&#20110;&#20248;&#21270;&#36712;&#36857;&#22797;&#26434;&#24615;&#30340;&#33258;&#28982;&#27010;&#24565;&#65292;&#26082;&#23450;&#24615;&#21448;&#23450;&#37327;&#22320;&#25581;&#31034;&#20102;&#21508;&#31181;&#20248;&#21270;&#36873;&#25321;&#65288;&#22914;&#21160;&#37327;&#12289;&#26435;&#37325;&#34928;&#20943;&#21644;&#25209;&#22823;&#23567;&#65289;&#20043;&#38388;&#25152;&#28041;&#21450;&#30340;&#20869;&#22312;&#24494;&#22937;&#21644;&#30456;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#27010;&#24565;&#26469;&#25552;&#20379;&#20851;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#26412;&#36136;&#30340;&#20851;&#38190;&#29305;&#24449;&#65306;&#20309;&#26102;&#39034;&#21033;&#36827;&#34892;&#65292;&#20309;&#26102;&#38519;&#20837;&#27515;&#32993;&#21516;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#25105;&#20204;&#30340;&#36712;&#36857;&#35270;&#35282;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#21160;&#37327;&#21644;&#26435;&#37325;&#34928;&#20943;&#20043;&#38388;&#20419;&#36827;&#26041;&#21521;&#25506;&#32034;&#30340;&#20132;&#32455;&#34892;&#20026;&#65292;&#20197;&#21450;&#20854;&#20182;&#19968;&#20123;&#34892;&#20026;&#30340;&#26041;&#21521;&#27491;&#21017;&#21270;&#34892;&#20026;&#12290;&#25105;&#20204;&#22312;&#22823;&#35268;&#27169;&#35270;&#35273;&#21644;&#35821;&#35328;&#35774;&#32622;&#20013;&#36827;&#34892;&#23454;&#39564;&#65292;&#21253;&#25324;&#20855;&#26377;&#26368;&#22810;120&#20159;&#20010;&#21442;&#25968;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07379v1 Announce Type: cross  Abstract: We propose a fresh take on understanding the mechanisms of neural networks by analyzing the rich structure of parameters contained within their optimization trajectories. Towards this end, we introduce some natural notions of the complexity of optimization trajectories, both qualitative and quantitative, which reveal the inherent nuance and interplay involved between various optimization choices, such as momentum, weight decay, and batch size. We use them to provide key hallmarks about the nature of optimization in deep neural networks: when it goes right, and when it finds itself in a dead end. Further, thanks to our trajectory perspective, we uncover an intertwined behaviour of momentum and weight decay that promotes directional exploration, as well as a directional regularization behaviour of some others. We perform experiments over large-scale vision and language settings, including large language models (LLMs) with up to 12 billio
&lt;/p&gt;</description></item><item><title>SVD-LLM&#26159;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;SVD&#30340;LLM&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#25130;&#26029;&#24863;&#30693;&#25968;&#25454;&#30333;&#21270;&#31574;&#30053;&#21644;&#36880;&#23618;&#38381;&#24335;&#27169;&#22411;&#21442;&#25968;&#26356;&#26032;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#23454;&#29616;&#20102;&#30452;&#25509;&#26144;&#23556;&#22855;&#24322;&#20540;&#21644;&#21387;&#32553;&#25439;&#22833;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2403.07378</link><description>&lt;p&gt;
SVD-LLM: &#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21387;&#32553;&#30340;&#25130;&#26029;&#24863;&#30693;&#22855;&#24322;&#20540;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
SVD-LLM: Truncation-aware Singular Value Decomposition for Large Language Model Compression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07378
&lt;/p&gt;
&lt;p&gt;
SVD-LLM&#26159;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;SVD&#30340;LLM&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#25130;&#26029;&#24863;&#30693;&#25968;&#25454;&#30333;&#21270;&#31574;&#30053;&#21644;&#36880;&#23618;&#38381;&#24335;&#27169;&#22411;&#21442;&#25968;&#26356;&#26032;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#23454;&#29616;&#20102;&#30452;&#25509;&#26144;&#23556;&#22855;&#24322;&#20540;&#21644;&#21387;&#32553;&#25439;&#22833;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#21463;&#21040;&#20854;&#24222;&#22823;&#23610;&#23544;&#30340;&#38480;&#21046;&#65292;&#36825;&#38656;&#35201;LLM&#21387;&#32553;&#26041;&#27861;&#20197;&#23454;&#29616;&#23454;&#38469;&#37096;&#32626;&#12290;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;SVD&#65289;&#20026;LLM&#21387;&#32553;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;SVD&#30340;LLM&#21387;&#32553;&#26041;&#27861;&#23384;&#22312;&#20004;&#20010;&#20851;&#38190;&#38480;&#21046;&#65306;&#25130;&#26029;&#36739;&#23567;&#30340;&#22855;&#24322;&#20540;&#21487;&#33021;&#23548;&#33268;&#26356;&#39640;&#30340;&#21387;&#32553;&#25439;&#22833;&#65292;&#24182;&#19988;&#22312;SVD&#25130;&#26029;&#21518;&#21097;&#20313;&#27169;&#22411;&#21442;&#25968;&#30340;&#26356;&#26032;&#32570;&#22833;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SVD-LLM&#65292;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;SVD&#30340;LLM&#21387;&#32553;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#38480;&#21046;&#12290;SVD-LLM&#37319;&#29992;&#20102;&#19968;&#31181;&#25130;&#26029;&#24863;&#30693;&#30340;&#25968;&#25454;&#30333;&#21270;&#31574;&#30053;&#65292;&#20197;&#30830;&#20445;&#22855;&#24322;&#20540;&#21644;&#21387;&#32553;&#25439;&#22833;&#20043;&#38388;&#30340;&#30452;&#25509;&#26144;&#23556;&#12290;&#27492;&#22806;&#65292;SVD-LLM&#37319;&#29992;&#19968;&#31181;&#36880;&#23618;&#38381;&#24335;&#27169;&#22411;&#21442;&#25968;&#26356;&#26032;&#31574;&#30053;&#65292;&#20197;&#24357;&#34917;SVD&#25130;&#26029;&#24341;&#36215;&#30340;&#20934;&#30830;&#24615;&#38477;&#20302;&#12290;&#25105;&#20204;&#22312;&#24635;&#20849;11&#20010;&#25968;&#25454;&#38598;&#21644;&#19971;&#20010;m&#19978;&#35780;&#20272;&#20102;SVD-LLM&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07378v1 Announce Type: new  Abstract: The advancements in Large Language Models (LLMs) have been hindered by their substantial sizes, which necessitate LLM compression methods for practical deployment. Singular Value Decomposition (SVD) offers a promising solution for LLM compression. However, state-of-the-art SVD-based LLM compression methods have two key limitations: truncating smaller singular values may lead to higher compression loss, and the lack of update on the remaining model parameters after SVD truncation. In this work, we propose SVD-LLM, a new SVD-based LLM compression method that addresses the limitations of existing methods. SVD-LLM incorporates a truncation-aware data whitening strategy to ensure a direct mapping between singular values and compression loss. Moreover, SVD-LLM adopts a layer-wise closed-form model parameter update strategy to compensate for accuracy degradation caused by SVD truncation. We evaluate SVD-LLM on a total of 11 datasets and seven m
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#26174;&#31034;&#29109;&#20316;&#20026;&#27979;&#35797;&#26102;&#36866;&#24212;&#24615;&#30340;&#32622;&#20449;&#24230;&#24230;&#37327;&#22312;&#20559;&#20506;&#22330;&#26223;&#19979;&#19981;&#21487;&#38752;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27979;&#35797;&#26102;&#36866;&#24212;&#24615;&#26041;&#27861;DeYO&#65292;&#21033;&#29992;Pseudo-Label Probability Difference&#65288;PLPD&#65289;&#20316;&#20026;&#32622;&#20449;&#24230;&#24230;&#37327;</title><link>https://arxiv.org/abs/2403.07366</link><description>&lt;p&gt;
&#29109;&#23545;&#20110;&#27979;&#35797;&#26102;&#36866;&#24212;&#24615;&#26469;&#35828;&#36824;&#19981;&#22815;&#65306;&#20174;&#35299;&#32544;&#20998;&#22240;&#32032;&#30340;&#35282;&#24230;&#30475;
&lt;/p&gt;
&lt;p&gt;
Entropy is not Enough for Test-Time Adaptation: From the Perspective of Disentangled Factors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07366
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#26174;&#31034;&#29109;&#20316;&#20026;&#27979;&#35797;&#26102;&#36866;&#24212;&#24615;&#30340;&#32622;&#20449;&#24230;&#24230;&#37327;&#22312;&#20559;&#20506;&#22330;&#26223;&#19979;&#19981;&#21487;&#38752;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27979;&#35797;&#26102;&#36866;&#24212;&#24615;&#26041;&#27861;DeYO&#65292;&#21033;&#29992;Pseudo-Label Probability Difference&#65288;PLPD&#65289;&#20316;&#20026;&#32622;&#20449;&#24230;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27979;&#35797;&#26102;&#36866;&#24212;&#24615;&#65288;TTA&#65289;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20197;&#36866;&#24212;&#26410;&#30693;&#30340;&#27979;&#35797;&#25968;&#25454;&#12290;TTA&#30340;&#20027;&#35201;&#25361;&#25112;&#26159;&#22312;&#32447;&#26356;&#26032;&#36807;&#31243;&#20013;&#23545;&#25972;&#20010;&#27979;&#35797;&#25968;&#25454;&#38598;&#30340;&#26377;&#38480;&#35775;&#38382;&#65292;&#23548;&#33268;&#35823;&#24046;&#32047;&#31215;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#31181;&#24773;&#20917;&#65292;TTA&#26041;&#27861;&#21033;&#29992;&#27169;&#22411;&#36755;&#20986;&#30340;&#29109;&#20316;&#20026;&#32622;&#20449;&#24230;&#24230;&#37327;&#65292;&#26088;&#22312;&#30830;&#23450;&#21738;&#20123;&#26679;&#26412;&#26356;&#19981;&#21487;&#33021;&#23548;&#33268;&#38169;&#35823;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#23454;&#39564;&#30740;&#31350;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#20559;&#20506;&#22330;&#26223;&#19979;&#29109;&#20316;&#20026;TTA&#30340;&#32622;&#20449;&#24230;&#24230;&#37327;&#26159;&#19981;&#21487;&#38752;&#30340;&#65292;&#29702;&#35770;&#19978;&#25581;&#31034;&#20102;&#36825;&#31181;&#19981;&#21487;&#38752;&#24615;&#28304;&#20110;&#24573;&#35270;&#25968;&#25454;&#30340;&#28508;&#22312;&#35299;&#32544;&#20998;&#22240;&#32032;&#23545;&#39044;&#27979;&#30340;&#24433;&#21709;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Destroy Your Object&#65288;DeYO&#65289;&#30340;&#26032;&#22411;TTA&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#20102;&#19968;&#31181;&#26032;&#25552;&#20986;&#30340;&#32622;&#20449;&#24230;&#24230;&#37327;&#65292;&#21517;&#20026;&#20266;&#26631;&#31614;&#27010;&#29575;&#24046;&#65288;PLPD&#65289;&#12290;PLPD&#36890;&#36807;&#27979;&#37327;&#23545;&#35937;&#24418;&#29366;&#23545;&#39044;&#27979;&#30340;&#24433;&#21709;&#31243;&#24230;&#26469;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07366v1 Announce Type: cross  Abstract: Test-time adaptation (TTA) fine-tunes pre-trained deep neural networks for unseen test data. The primary challenge of TTA is limited access to the entire test dataset during online updates, causing error accumulation. To mitigate it, TTA methods have utilized the model output's entropy as a confidence metric that aims to determine which samples have a lower likelihood of causing error. Through experimental studies, however, we observed the unreliability of entropy as a confidence metric for TTA under biased scenarios and theoretically revealed that it stems from the neglect of the influence of latent disentangled factors of data on predictions. Building upon these findings, we introduce a novel TTA method named Destroy Your Object (DeYO), which leverages a newly proposed confidence metric named Pseudo-Label Probability Difference (PLPD). PLPD quantifies the influence of the shape of an object on prediction by measuring the difference b
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20174;&#23545;&#25239;&#30340;&#35282;&#24230;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#36951;&#24536;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#30830;&#23450;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#23376;&#38598;&#65292;&#21363;&#26368;&#22351;&#24773;&#20917;&#36951;&#24536;&#38598;&#65292;&#26469;&#22686;&#24378;&#23545;&#24433;&#21709;&#25830;&#38500;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.07362</link><description>&lt;p&gt;
&#25361;&#25112;&#36951;&#24536;&#65306;&#25581;&#31034;&#26426;&#22120;&#36951;&#24536;&#20013;&#26368;&#22351;&#24773;&#20917;&#36951;&#24536;&#38598;
&lt;/p&gt;
&lt;p&gt;
Challenging Forgets: Unveiling the Worst-Case Forget Sets in Machine Unlearning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07362
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20174;&#23545;&#25239;&#30340;&#35282;&#24230;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#36951;&#24536;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#30830;&#23450;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#23376;&#38598;&#65292;&#21363;&#26368;&#22351;&#24773;&#20917;&#36951;&#24536;&#38598;&#65292;&#26469;&#22686;&#24378;&#23545;&#24433;&#21709;&#25830;&#38500;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38752;&#35889;&#30340;&#26426;&#22120;&#23398;&#20064;(Machine Learning, ML)&#31038;&#21306;&#36234;&#26469;&#36234;&#35748;&#35782;&#21040;&#27169;&#22411;&#22312;&#35757;&#32451;&#21518;&#26377;&#36873;&#25321;&#24615;&#22320;&#8220;&#36951;&#24536;&#8221;&#25968;&#25454;&#28857;&#30340;&#37325;&#35201;&#24615;&#12290;&#36825;&#24341;&#20986;&#20102;&#26426;&#22120;&#36951;&#24536;(Machine Unlearning, MU)&#38382;&#39064;&#65292;&#26088;&#22312;&#28040;&#38500;&#36873;&#23450;&#25968;&#25454;&#28857;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#21516;&#26102;&#20173;&#20445;&#25345;&#27169;&#22411;&#22312;&#36951;&#24536;&#21518;&#30340;&#23454;&#29992;&#24615;&#12290;&#23613;&#31649;&#26377;&#21508;&#31181;MU&#26041;&#27861;&#26469;&#25830;&#38500;&#25968;&#25454;&#24433;&#21709;&#65292;&#35780;&#20272;&#20027;&#35201;&#38598;&#20013;&#22312;&#38543;&#26426;&#25968;&#25454;&#36951;&#24536;&#19978;&#65292;&#24573;&#35270;&#20102;&#23545;&#20110;&#30495;&#23454;&#34913;&#37327;&#36951;&#24536;&#24615;&#33021;&#30340;&#25968;&#25454;&#23376;&#38598;&#36873;&#25321;&#30340;&#37325;&#35201;&#25506;&#31350;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#20174;&#23545;&#25239;&#30340;&#35282;&#24230;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;MU&#35780;&#20272;&#35270;&#35282;&#12290;&#25105;&#20204;&#25552;&#20986;&#30830;&#23450;&#37027;&#20123;&#23545;&#24433;&#21709;&#25830;&#38500;&#26500;&#25104;&#26368;&#22823;&#25361;&#25112;&#30340;&#25968;&#25454;&#23376;&#38598;&#65292;&#21363;&#25214;&#20986;&#26368;&#22351;&#24773;&#20917;&#36951;&#24536;&#38598;&#12290;&#21033;&#29992;&#21452;&#23618;&#20248;&#21270;&#21407;&#21017;&#65292;&#25105;&#20204;&#22686;&#24378;&#20102;&#22312;&#19978;&#23618;&#20248;&#21270;&#20013;&#30340;&#36951;&#24536;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07362v1 Announce Type: cross  Abstract: The trustworthy machine learning (ML) community is increasingly recognizing the crucial need for models capable of selectively 'unlearning' data points after training. This leads to the problem of machine unlearning (MU), aiming to eliminate the influence of chosen data points on model performance, while still maintaining the model's utility post-unlearning. Despite various MU methods for data influence erasure, evaluations have largely focused on random data forgetting, ignoring the vital inquiry into which subset should be chosen to truly gauge the authenticity of unlearning performance. To tackle this issue, we introduce a new evaluative angle for MU from an adversarial viewpoint. We propose identifying the data subset that presents the most significant challenge for influence erasure, i.e., pinpointing the worst-case forget set. Utilizing a bi-level optimization principle, we amplify unlearning challenges at the upper optimization 
&lt;/p&gt;</description></item><item><title>&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65292;&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#21033;&#29992;&#39044;&#24863;&#26469;&#39044;&#27979;&#25968;&#25454;&#21464;&#21270;&#65292;&#20026;&#30417;&#30563;&#39044;&#35757;&#32451;&#25552;&#20379;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;</title><link>https://arxiv.org/abs/2403.07356</link><description>&lt;p&gt;
&#39044;&#24863;&#65306;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#39044;&#27979;&#25345;&#32493;&#23398;&#20064;&#20013;&#26410;&#26469;&#25968;&#25454;&#21464;&#21270;
&lt;/p&gt;
&lt;p&gt;
Premonition: Using Generative Models to Preempt Future Data Changes in Continual Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07356
&lt;/p&gt;
&lt;p&gt;
&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65292;&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#21033;&#29992;&#39044;&#24863;&#26469;&#39044;&#27979;&#25968;&#25454;&#21464;&#21270;&#65292;&#20026;&#30417;&#30563;&#39044;&#35757;&#32451;&#25552;&#20379;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#35201;&#27714;&#27169;&#22411;&#33021;&#22815;&#36866;&#24212;&#25968;&#25454;&#20998;&#24067;&#30340;&#25345;&#32493;&#21464;&#21270;&#65292;&#36890;&#24120;&#20063;&#35201;&#36866;&#24212;&#35201;&#25191;&#34892;&#30340;&#20219;&#21153;&#38598;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#21644;&#20219;&#21153;&#21464;&#21270;&#24456;&#23569;&#26159;&#23436;&#20840;&#19981;&#21487;&#39044;&#27979;&#30340;&#12290;&#37492;&#20110;&#19968;&#20010;&#27010;&#25324;&#24615;&#30446;&#26631;&#25110;&#25968;&#25454;&#20027;&#39064;&#30340;&#25551;&#36848;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#39046;&#22495;&#65292;&#20154;&#31867;&#36890;&#24120;&#21487;&#20197;&#29468;&#27979;&#19982;&#20043;&#30456;&#20851;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#23637;&#31034;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#32452;&#21512;&#21487;&#20197;&#31867;&#20284;&#22320;&#25552;&#20379;&#26377;&#29992;&#30340;&#39044;&#24863;&#65292;&#20197;&#39044;&#27979;&#25345;&#32493;&#23398;&#20064;&#25361;&#25112;&#38543;&#26102;&#38388;&#22914;&#20309;&#21457;&#23637;&#12290;&#25105;&#20204;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#26410;&#26469;&#21487;&#33021;&#20986;&#29616;&#22312;&#25968;&#25454;&#27969;&#20013;&#30340;&#35821;&#20041;&#30456;&#20851;&#31867;&#21035;&#30340;&#25991;&#26412;&#25551;&#36848;&#12290;&#28982;&#21518;&#21033;&#29992;&#31283;&#23450;&#25193;&#25955;&#26469;&#29983;&#25104;&#26032;&#30340;&#24102;&#26631;&#31614;&#22270;&#20687;&#26679;&#26412;&#12290;&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#29992;&#20110;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#20294;&#22312;&#24320;&#22987;&#25345;&#32493;&#23398;&#20064;&#20043;&#21069;&#20250;&#34987;&#20002;&#24323;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07356v1 Announce Type: cross  Abstract: Continual learning requires a model to adapt to ongoing changes in the data distribution, and often to the set of tasks to be performed. It is rare, however, that the data and task changes are completely unpredictable. Given a description of an overarching goal or data theme, which we call a realm, humans can often guess what concepts are associated with it. We show here that the combination of a large language model and an image generation model can similarly provide useful premonitions as to how a continual learning challenge might develop over time. We use the large language model to generate text descriptions of semantically related classes that might potentially appear in the data stream in future. These descriptions are then rendered using Stable Diffusion to generate new labelled image samples. The resulting synthetic dataset is employed for supervised pre-training, but is discarded prior to commencing continual learning, along 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#21435;&#38500;&#26694;&#26550;GraphRevoker&#65292;&#36890;&#36807;&#22270;&#23646;&#24615;&#24863;&#30693;&#21010;&#20998;&#21644;&#22270;&#23545;&#27604;&#23376;&#27169;&#22411;&#32858;&#21512;&#65292;&#26356;&#22909;&#22320;&#20445;&#25345;&#20102;&#19981;&#21487;&#35757;&#32451;GNNs&#30340;&#27169;&#22411;&#25928;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.07353</link><description>&lt;p&gt;
&#20855;&#26377;&#39640;&#25928;&#37096;&#20998;&#37325;&#26032;&#35757;&#32451;&#30340;&#22270;&#21435;&#38500;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Graph Unlearning with Efficient Partial Retraining
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07353
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#21435;&#38500;&#26694;&#26550;GraphRevoker&#65292;&#36890;&#36807;&#22270;&#23646;&#24615;&#24863;&#30693;&#21010;&#20998;&#21644;&#22270;&#23545;&#27604;&#23376;&#27169;&#22411;&#32858;&#21512;&#65292;&#26356;&#22909;&#22320;&#20445;&#25345;&#20102;&#19981;&#21487;&#35757;&#32451;GNNs&#30340;&#27169;&#22411;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#21508;&#31181;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;GNNs &#21487;&#33021;&#20250;&#22312;&#19981;&#33391;&#30340;&#22270;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#21487;&#33021;&#20250;&#38477;&#20302;&#23427;&#20204;&#30340;&#24615;&#33021;&#21644;&#21487;&#38752;&#24615;&#12290;&#20026;&#20102;&#35753;&#24050;&#32463;&#35757;&#32451;&#36807;&#30340;GNNs&#33021;&#22815;&#26377;&#25928;&#22320;&#21435;&#38500;&#19981;&#38656;&#35201;&#30340;&#25968;&#25454;&#65292;&#19968;&#31181;&#29702;&#24819;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#22522;&#20110;&#37325;&#26032;&#35757;&#32451;&#30340;&#22270;&#21435;&#38500;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#35757;&#32451;&#22270;&#20998;&#25104;&#23376;&#22270;&#65292;&#24182;&#22312;&#20854;&#19978;&#35757;&#32451;&#23376;&#27169;&#22411;&#65292;&#20174;&#32780;&#36890;&#36807;&#37096;&#20998;&#37325;&#26032;&#35757;&#32451;&#23454;&#29616;&#24555;&#36895;&#21435;&#38500;&#12290;&#28982;&#32780;&#65292;&#22270;&#20998;&#21306;&#36807;&#31243;&#20250;&#23548;&#33268;&#35757;&#32451;&#22270;&#20013;&#30340;&#20449;&#24687;&#20002;&#22833;&#65292;&#20174;&#32780;&#23548;&#33268;&#23376;GNN&#27169;&#22411;&#30340;&#27169;&#22411;&#25928;&#29992;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07353v1 Announce Type: new  Abstract: Graph Neural Networks (GNNs) have achieved remarkable success in various real-world applications. However, GNNs may be trained on undesirable graph data, which can degrade their performance and reliability. To enable trained GNNs to efficiently unlearn unwanted data, a desirable solution is retraining-based graph unlearning, which partitions the training graph into subgraphs and trains sub-models on them, allowing fast unlearning through partial retraining. However, the graph partition process causes information loss in the training graph, resulting in the low model utility of sub-GNN models. In this paper, we propose GraphRevoker, a novel graph unlearning framework that better maintains the model utility of unlearnable GNNs. Specifically, we preserve the graph property with graph property-aware sharding and effectively aggregate the sub-GNN models for prediction with graph contrastive sub-model aggregation. We conduct extensive experime
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#39564;&#35777;&#22312;&#31227;&#38500;&#20302;&#20301;&#23485;&#38480;&#21046;&#26102;&#65292;&#23545;&#20110;&#21508;&#31181;Transformer-based&#27169;&#22411;&#65292;&#25972;&#25968;&#26159;&#21542;&#36275;&#20197;&#28385;&#36275;&#25152;&#26377;GEMM&#38656;&#27714;&#65288;&#35757;&#32451;&#21644;&#25512;&#26029;&#38454;&#27573;&#65289;&#65292;&#24182;&#19988;&#21487;&#20197;&#19982;&#28014;&#28857;&#25968;&#30456;&#23218;&#32654;&#65292;&#32780;&#26080;&#38656;&#22797;&#26434;&#25216;&#24039;&#12290;</title><link>https://arxiv.org/abs/2403.07339</link><description>&lt;p&gt;
IM-Unpack: &#20351;&#29992;&#20219;&#24847;&#20302;&#31934;&#24230;&#25972;&#25968;&#36827;&#34892;&#35757;&#32451;&#21644;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
IM-Unpack: Training and Inference with Arbitrarily Low Precision Integers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07339
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#39564;&#35777;&#22312;&#31227;&#38500;&#20302;&#20301;&#23485;&#38480;&#21046;&#26102;&#65292;&#23545;&#20110;&#21508;&#31181;Transformer-based&#27169;&#22411;&#65292;&#25972;&#25968;&#26159;&#21542;&#36275;&#20197;&#28385;&#36275;&#25152;&#26377;GEMM&#38656;&#27714;&#65288;&#35757;&#32451;&#21644;&#25512;&#26029;&#38454;&#27573;&#65289;&#65292;&#24182;&#19988;&#21487;&#20197;&#19982;&#28014;&#28857;&#25968;&#30456;&#23218;&#32654;&#65292;&#32780;&#26080;&#38656;&#22797;&#26434;&#25216;&#24039;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
GEneral Matrix Multiply (GEMM)&#26159;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#25805;&#20316;&#65292;&#23545;&#24212;&#20110;&#35745;&#31639;&#21344;&#27604;&#26368;&#22823;&#30340;&#37096;&#20998;&#12290;&#22240;&#27492;&#65292;&#25552;&#39640;&#20854;&#25928;&#29575;&#26159;&#19968;&#20010;&#27491;&#22312;&#36827;&#34892;&#30740;&#31350;&#30340;&#28909;&#38376;&#20027;&#39064;&#12290;&#19968;&#31181;&#27969;&#34892;&#30340;&#31574;&#30053;&#26159;&#20351;&#29992;&#20302;&#20301;&#23485;&#25972;&#25968;&#26469;&#36817;&#20284;&#30697;&#38453;&#20013;&#30340;&#21407;&#22987;&#26465;&#30446;&#12290;&#36825;&#26679;&#21487;&#20197;&#25552;&#39640;&#25928;&#29575;&#65292;&#20294;&#24120;&#24120;&#38656;&#35201;&#22797;&#26434;&#30340;&#25216;&#26415;&#26469;&#25511;&#21046;&#20135;&#29983;&#30340;&#33293;&#20837;&#35823;&#24046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#39564;&#35777;&#24403;&#31227;&#38500;&#20302;&#20301;&#23485;&#38480;&#21046;&#26102;&#65292;&#23545;&#20110;&#21508;&#31181;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#25972;&#25968;&#26159;&#21542;&#36275;&#22815;&#28385;&#36275;&#25152;&#26377;GEMMs&#30340;&#38656;&#27714; - &#26080;&#35770;&#26159;&#35757;&#32451;&#38454;&#27573;&#36824;&#26159;&#25512;&#26029;&#38454;&#27573;&#65292;&#24182;&#19988;&#21487;&#20197;&#19982;&#28014;&#28857;&#25968;&#23545;&#24212;&#39033;&#36798;&#21040;&#19968;&#33268;&#12290;&#26080;&#38656;&#22797;&#26434;&#25216;&#26415;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#34429;&#28982;&#22312;&#36825;&#20123;&#27169;&#22411;&#20013;&#36935;&#21040;&#30340;&#22823;&#22810;&#25968;&#30697;&#38453;&#26465;&#30446;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#29992;&#20302;&#20301;&#23485;&#25972;&#25968;&#34920;&#31034;&#65292;&#20294;&#23384;&#22312;&#19968;&#20123;&#37325;&#35201;&#26465;&#30446;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07339v1 Announce Type: cross  Abstract: GEneral Matrix Multiply (GEMM) is a central operation in deep learning and corresponds to the largest chunk of the compute footprint. Therefore, improving its efficiency is an active topic of ongoing research. A popular strategy is the use of low bit-width integers to approximate the original entries in a matrix. This allows efficiency gains, but often requires sophisticated techniques to control the rounding error incurred. In this work, we first verify/check that when the low bit-width restriction is removed, for a variety of Transformer-based models, whether integers are sufficient for all GEMMs need -- for {\em both} training and inference stages, and can achieve parity with floating point counterparts. No sophisticated techniques are needed. We find that while a large majority of entries in matrices (encountered in such models) can be easily represented by {\em low} bit-width integers, the existence of a few heavy hitter entries m
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#26410;&#30693;&#39046;&#22495;&#19981;&#19968;&#33268;&#24615;&#26368;&#23567;&#21270;&#65288;UDIM&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38477;&#20302;&#28304;&#39046;&#22495;&#21644;&#26410;&#30693;&#39046;&#22495;&#20043;&#38388;&#30340;&#25439;&#22833;&#26223;&#35266;&#19981;&#19968;&#33268;&#24615;&#26469;&#25913;&#21892;&#39046;&#22495;&#27867;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.07329</link><description>&lt;p&gt;
&#26410;&#30693;&#39046;&#22495;&#19981;&#19968;&#33268;&#24615;&#26368;&#23567;&#21270;&#30340;&#22495;&#27867;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unknown Domain Inconsistency Minimization for Domain Generalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07329
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#26410;&#30693;&#39046;&#22495;&#19981;&#19968;&#33268;&#24615;&#26368;&#23567;&#21270;&#65288;UDIM&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38477;&#20302;&#28304;&#39046;&#22495;&#21644;&#26410;&#30693;&#39046;&#22495;&#20043;&#38388;&#30340;&#25439;&#22833;&#26223;&#35266;&#19981;&#19968;&#33268;&#24615;&#26469;&#25913;&#21892;&#39046;&#22495;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#27867;&#21270;&#65288;DG&#65289;&#30340;&#30446;&#26631;&#26159;&#22686;&#24378;&#20174;&#28304;&#39046;&#22495;&#23398;&#24471;&#30340;&#27169;&#22411;&#23545;&#26410;&#35266;&#27979;&#39046;&#22495;&#30340;&#21487;&#36801;&#31227;&#24615;&#12290;&#20026;&#20102;&#38450;&#27490;&#23545;&#29305;&#23450;&#39046;&#22495;&#30340;&#36807;&#25311;&#21512;&#65292;Sharpness-Aware Minimization&#65288;SAM&#65289;&#20943;&#23569;&#28304;&#39046;&#22495;&#30340;&#25439;&#22833;&#23574;&#38160;&#24230;&#12290;&#23613;&#31649;SAM&#30340;&#21464;&#31181;&#22312;DG&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#20294;&#25105;&#20204;&#25351;&#20986;&#65292;&#22312;&#25968;&#25454;&#31354;&#38388;&#20013;&#20173;&#26377;&#28508;&#21147;&#36890;&#36807;&#25506;&#32034;&#26469;&#25552;&#39640;&#23545;&#26410;&#30693;&#39046;&#22495;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26681;&#26893;&#20110;&#21442;&#25968;&#21644;&#25968;&#25454;&#25200;&#21160;&#21306;&#22495;&#30340;&#39046;&#22495;&#27867;&#21270;&#30446;&#26631;&#65292;&#21629;&#21517;&#20026;&#26410;&#30693;&#39046;&#22495;&#19981;&#19968;&#33268;&#24615;&#26368;&#23567;&#21270;&#65288;UDIM&#65289;&#12290;UDIM&#20943;&#23569;&#20102;&#28304;&#39046;&#22495;&#21644;&#26410;&#30693;&#39046;&#22495;&#20043;&#38388;&#30340;&#25439;&#22833;&#26223;&#35266;&#19981;&#19968;&#33268;&#24615;&#12290;&#30001;&#20110;&#26080;&#27861;&#35775;&#38382;&#26410;&#30693;&#39046;&#22495;&#65292;&#36825;&#20123;&#22495;&#26159;&#36890;&#36807;&#25200;&#21160;&#28304;&#39046;&#22495;&#25968;&#25454;&#38598;&#20013;&#30340;&#23454;&#20363;&#26469;&#32463;&#39564;&#24615;&#22320;&#26500;&#24314;&#30340;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#36890;&#36807;&#23558;&#22312;&#28304;&#39046;&#22495;&#20013;&#33719;&#24471;&#30340;&#25439;&#22833;&#26223;&#35266;&#19982;&#25439;&#22833;&#26223;&#35266;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07329v1 Announce Type: new  Abstract: The objective of domain generalization (DG) is to enhance the transferability of the model learned from a source domain to unobserved domains. To prevent overfitting to a specific domain, Sharpness-Aware Minimization (SAM) reduces source domain's loss sharpness. Although SAM variants have delivered significant improvements in DG, we highlight that there's still potential for improvement in generalizing to unknown domains through the exploration on data space. This paper introduces an objective rooted in both parameter and data perturbed regions for domain generalization, coined Unknown Domain Inconsistency Minimization (UDIM). UDIM reduces the loss landscape inconsistency between source domain and unknown domains. As unknown domains are inaccessible, these domains are empirically crafted by perturbing instances from the source domain dataset. In particular, by aligning the loss landscape acquired in the source domain to the loss landscap
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#19968;&#20010;&#38382;&#39064;&#20013;&#24515;&#30340;&#22810;&#19987;&#23478;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#25552;&#39640;&#28145;&#24230;&#24207;&#21015;&#30693;&#35782;&#36861;&#36394;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#35299;&#20915;&#20102;&#30693;&#35782;&#36861;&#36394;&#20013;&#20010;&#20307;&#38382;&#39064;&#20449;&#24687;&#24314;&#27169;&#21644;&#27169;&#22411;&#39044;&#27979;&#32467;&#26524;&#35299;&#37322;&#30340;&#37325;&#35201;&#25361;&#25112;</title><link>https://arxiv.org/abs/2403.07322</link><description>&lt;p&gt;
&#19968;&#20010;&#38382;&#39064;&#20013;&#24515;&#30340;&#22810;&#19987;&#23478;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#25552;&#39640;&#28145;&#24230;&#24207;&#21015;&#30693;&#35782;&#36861;&#36394;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
A Question-centric Multi-experts Contrastive Learning Framework for Improving the Accuracy and Interpretability of Deep Sequential Knowledge Tracing Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07322
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#19968;&#20010;&#38382;&#39064;&#20013;&#24515;&#30340;&#22810;&#19987;&#23478;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#25552;&#39640;&#28145;&#24230;&#24207;&#21015;&#30693;&#35782;&#36861;&#36394;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#35299;&#20915;&#20102;&#30693;&#35782;&#36861;&#36394;&#20013;&#20010;&#20307;&#38382;&#39064;&#20449;&#24687;&#24314;&#27169;&#21644;&#27169;&#22411;&#39044;&#27979;&#32467;&#26524;&#35299;&#37322;&#30340;&#37325;&#35201;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#36861;&#36394;&#22312;&#36890;&#36807;&#20998;&#26512;&#23398;&#29983;&#21382;&#21490;&#23398;&#20064;&#36807;&#31243;&#26469;&#39044;&#27979;&#20854;&#26410;&#26469;&#34920;&#29616;&#20013;&#21457;&#25381;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#35299;&#20915;&#30693;&#35782;&#36861;&#36394;&#38382;&#39064;&#26041;&#38754;&#23637;&#29616;&#20986;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#23558;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#24212;&#29992;&#20110;&#27169;&#25311;&#30693;&#35782;&#36861;&#36394;&#36807;&#31243;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#37325;&#35201;&#25361;&#25112;&#12290;&#31532;&#19968;&#20010;&#25361;&#25112;&#22312;&#20110;&#23558;&#38382;&#39064;&#30340;&#20010;&#20307;&#20449;&#24687;&#34701;&#20837;&#24314;&#27169;&#20013;&#12290;&#36825;&#24456;&#20851;&#38190;&#65292;&#22240;&#20026;&#23613;&#31649;&#38382;&#39064;&#20849;&#20139;&#30456;&#21516;&#30340;&#30693;&#35782;&#32452;&#20214;&#65288;KC&#65289;&#65292;&#20294;&#23398;&#29983;&#23545;&#21516;&#36136;&#38382;&#39064;&#30340;&#30693;&#35782;&#20064;&#24471;&#21487;&#20197;&#26377;&#26174;&#33879;&#24046;&#24322;&#12290;&#31532;&#20108;&#20010;&#25361;&#25112;&#22312;&#20110;&#35299;&#37322;&#29616;&#26377;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#30693;&#35782;&#36861;&#36394;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#22312;&#30495;&#23454;&#24212;&#29992;&#20013;&#65292;&#34429;&#28982;&#21487;&#33021;&#24182;&#19981;&#38656;&#35201;&#23436;&#20840;&#36879;&#26126;&#21644;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#21442;&#25968;&#65292;&#20294;&#20851;&#38190;&#26159;&#20197;&#32769;&#24072;&#33021;&#29702;&#35299;&#30340;&#26041;&#24335;&#21576;&#29616;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07322v1 Announce Type: cross  Abstract: Knowledge tracing (KT) plays a crucial role in predicting students' future performance by analyzing their historical learning processes. Deep neural networks (DNNs) have shown great potential in solving the KT problem. However, there still exist some important challenges when applying deep learning techniques to model the KT process. The first challenge lies in taking the individual information of the question into modeling. This is crucial because, despite questions sharing the same knowledge component (KC), students' knowledge acquisition on homogeneous questions can vary significantly. The second challenge lies in interpreting the prediction results from existing deep learning-based KT models. In real-world applications, while it may not be necessary to have complete transparency and interpretability of the model parameters, it is crucial to present the model's prediction results in a manner that teachers find interpretable. This ma
&lt;/p&gt;</description></item><item><title>&#26684;&#28857;&#21464;&#25442;&#32534;&#30721;&#65288;LTC&#65289;&#36890;&#36807;&#22312;&#28508;&#31354;&#38388;&#20013;&#37319;&#29992;&#26684;&#28857;&#37327;&#21270;&#65292;&#23454;&#29616;&#20102;&#31070;&#32463;&#21387;&#32553;&#20013;&#25509;&#36817;&#36895;&#29575;&#22833;&#30495;&#26497;&#38480;&#30340;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.07320</link><description>&lt;p&gt;
&#29992;&#26684;&#28857;&#21464;&#25442;&#32534;&#30721;&#25509;&#36817;&#31070;&#32463;&#21387;&#32553;&#20013;&#30340;&#36895;&#29575;&#22833;&#30495;&#26497;&#38480;
&lt;/p&gt;
&lt;p&gt;
Approaching Rate-Distortion Limits in Neural Compression with Lattice Transform Coding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07320
&lt;/p&gt;
&lt;p&gt;
&#26684;&#28857;&#21464;&#25442;&#32534;&#30721;&#65288;LTC&#65289;&#36890;&#36807;&#22312;&#28508;&#31354;&#38388;&#20013;&#37319;&#29992;&#26684;&#28857;&#37327;&#21270;&#65292;&#23454;&#29616;&#20102;&#31070;&#32463;&#21387;&#32553;&#20013;&#25509;&#36817;&#36895;&#29575;&#22833;&#30495;&#26497;&#38480;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#21387;&#32553;&#22312;&#35774;&#35745;&#20855;&#26377;&#33391;&#22909;&#36895;&#29575;&#22833;&#30495;&#65288;RD&#65289;&#24615;&#33021;&#20294;&#22797;&#26434;&#24230;&#20302;&#30340;&#26377;&#25439;&#21387;&#32553;&#22120;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#31070;&#32463;&#21387;&#32553;&#35774;&#35745;&#28041;&#21450;&#23558;&#28304;&#36716;&#25442;&#20026;&#28508;&#21464;&#37327;&#65292;&#28982;&#21518;&#33293;&#20837;&#20026;&#25972;&#25968;&#24182;&#36827;&#34892;&#29109;&#32534;&#30721;&#12290;&#23613;&#31649;&#36825;&#31181;&#26041;&#27861;&#24050;&#34987;&#35777;&#26126;&#22312;&#26576;&#20123;&#28304;&#19978;&#30340;&#19968;&#27425;&#24615;&#24773;&#20917;&#19979;&#26159;&#26368;&#20339;&#30340;&#65292;&#20294;&#25105;&#20204;&#34920;&#26126;&#22312;i.i.d.&#24207;&#21015;&#19978;&#23427;&#26159;&#39640;&#24230;&#27425;&#20248;&#30340;&#65292;&#20107;&#23454;&#19978;&#24635;&#26159;&#24674;&#22797;&#21407;&#22987;&#28304;&#24207;&#21015;&#30340;&#26631;&#37327;&#37327;&#21270;&#12290;&#25105;&#20204;&#23637;&#31034;&#20122;&#20248;&#36234;&#24615;&#26159;&#30001;&#20110;&#28508;&#31354;&#38388;&#20013;&#37327;&#21270;&#26041;&#26696;&#30340;&#36873;&#25321;&#65292;&#32780;&#38750;&#21464;&#25442;&#35774;&#35745;&#25152;&#33268;&#12290;&#36890;&#36807;&#22312;&#28508;&#31354;&#38388;&#20013;&#37319;&#29992;&#26684;&#28857;&#37327;&#21270;&#32780;&#38750;&#26631;&#37327;&#37327;&#21270;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26684;&#28857;&#21464;&#25442;&#32534;&#30721;&#65288;Lattice Transform Coding&#65292;LTC&#65289;&#33021;&#22815;&#22312;&#21508;&#20010;&#32500;&#24230;&#19978;&#24674;&#22797;&#26368;&#20339;&#30690;&#37327;&#37327;&#21270;&#65292;&#24182;&#22312;&#21512;&#29702;&#30340;&#22797;&#26434;&#24230;&#19979;&#25509;&#36817;&#28176;&#36817;&#21487;&#23454;&#29616;&#30340;&#36895;&#29575;&#22833;&#30495;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07320v1 Announce Type: cross  Abstract: Neural compression has brought tremendous progress in designing lossy compressors with good rate-distortion (RD) performance at low complexity. Thus far, neural compression design involves transforming the source to a latent vector, which is then rounded to integers and entropy coded. While this approach has been shown to be optimal in a one-shot sense on certain sources, we show that it is highly sub-optimal on i.i.d. sequences, and in fact always recovers scalar quantization of the original source sequence. We demonstrate that the sub-optimality is due to the choice of quantization scheme in the latent space, and not the transform design. By employing lattice quantization instead of scalar quantization in the latent space, we demonstrate that Lattice Transform Coding (LTC) is able to recover optimal vector quantization at various dimensions and approach the asymptotically-achievable rate-distortion function at reasonable complexity. 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23450;&#21046;&#21270;&#22836;&#20687;&#31995;&#32479;CADyFACE&#65292;&#36890;&#36807;FACS&#26631;&#35760;&#30340;&#21160;&#24577;&#38754;&#37096;&#34920;&#24773;&#65292;&#20197;&#21450;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;BeCoME-Net&#26469;&#37327;&#21270;&#29992;&#25143;&#23545;&#21050;&#28608;&#30340;&#38754;&#37096;&#21453;&#24212;&#12290;</title><link>https://arxiv.org/abs/2403.07314</link><description>&lt;p&gt;
&#21487;&#23450;&#21046;&#21270;&#22836;&#20687;&#30340;&#21160;&#24577;&#38754;&#37096;&#34920;&#24773;&#32534;&#30721;&#34920;&#36798;&#65288;CADyFACE&#65289;&#20197;&#25552;&#21319;&#29992;&#25143;&#21442;&#19982;&#24230;
&lt;/p&gt;
&lt;p&gt;
Customizable Avatars with Dynamic Facial Action Coded Expressions (CADyFACE) for Improved User Engagement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07314
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23450;&#21046;&#21270;&#22836;&#20687;&#31995;&#32479;CADyFACE&#65292;&#36890;&#36807;FACS&#26631;&#35760;&#30340;&#21160;&#24577;&#38754;&#37096;&#34920;&#24773;&#65292;&#20197;&#21450;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;BeCoME-Net&#26469;&#37327;&#21270;&#29992;&#25143;&#23545;&#21050;&#28608;&#30340;&#38754;&#37096;&#21453;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23450;&#21046;&#21270;&#30340;3D&#22836;&#20687;&#20026;&#22522;&#30784;&#30340;&#38754;&#37096;&#34920;&#24773;&#21050;&#28608;&#21487;&#33021;&#25552;&#39640;&#29992;&#25143;&#22312;&#34892;&#20026;&#29983;&#29289;&#26631;&#24535;&#29289;&#21457;&#29616;&#21644;&#33258;&#38381;&#30151;&#12289;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#12289;&#38754;&#30251;&#31561;&#30142;&#30149;&#27835;&#30103;&#24178;&#39044;&#20013;&#30340;&#21442;&#19982;&#24230;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#20855;&#26377;&#38754;&#37096;&#21160;&#20316;&#32534;&#30721;&#31995;&#32479;&#65288;FACS&#65289;&#21160;&#20316;&#21333;&#20803;&#65288;AU&#65289;&#26631;&#31614;&#30340;&#21487;&#23450;&#21046;&#21270;&#22836;&#20687;&#21050;&#28608;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#20391;&#37325;&#20110;&#65288;1&#65289;&#20855;&#26377;FACS&#26631;&#35760;&#30340;&#21487;&#23450;&#21046;&#21270;&#22836;&#20687;&#34920;&#36798;&#21050;&#28608;&#65292;&#20197;&#32500;&#25345;&#21463;&#35797;&#32773;&#30340;&#21442;&#19982;&#24230;&#65292;&#65288;2&#65289;&#22522;&#20110;&#23398;&#20064;&#30340;&#27979;&#37327;&#65292;&#37327;&#21270;&#21463;&#35797;&#32773;&#23545;&#27492;&#31867;&#21050;&#28608;&#30340;&#38754;&#37096;&#21453;&#24212;&#65292;&#20197;&#21450;&#65288;3&#65289;&#39564;&#35777;&#30001;&#21050;&#28608;-&#27979;&#37327;&#23545;&#34920;&#31034;&#30340;&#26500;&#36896;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#30001;&#33719;&#24471;FACS&#19987;&#23478;&#35748;&#35777;&#30340;AU&#26631;&#35760;&#30340;Customizable Avatars with Dynamic Facial Action Coded Expressions&#65288;CADyFACE&#65289;&#12290;&#20026;&#20102;&#27979;&#37327;&#21463;&#35797;&#32773;&#23545;CADyFACE&#30340;AU&#30340;&#21453;&#24212;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Beta&#24341;&#23548;&#30456;&#20851;&#21644;&#22810;&#20219;&#21153;&#34920;&#36798;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#65288;BeCoME-Net&#65289;&#29992;&#20110;&#22810;&#26631;&#31614;AU&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07314v1 Announce Type: cross  Abstract: Customizable 3D avatar-based facial expression stimuli may improve user engagement in behavioral biomarker discovery and therapeutic intervention for autism, Alzheimer's disease, facial palsy, and more. However, there is a lack of customizable avatar-based stimuli with Facial Action Coding System (FACS) action unit (AU) labels. Therefore, this study focuses on (1) FACS-labeled, customizable avatar-based expression stimuli for maintaining subjects' engagement, (2) learning-based measurements that quantify subjects' facial responses to such stimuli, and (3) validation of constructs represented by stimulus-measurement pairs. We propose Customizable Avatars with Dynamic Facial Action Coded Expressions (CADyFACE) labeled with AUs by a certified FACS expert. To measure subjects' AUs in response to CADyFACE, we propose a novel Beta-guided Correlation and Multi-task Expression learning neural network (BeCoME-Net) for multi-label AU detection. 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#30693;&#35782;&#22270;&#35889;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65288;KG-LLM&#65289;&#65292;&#21033;&#29992;&#24605;&#32500;&#38142;&#25552;&#31034;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#31561;NLP&#33539;&#20363;&#65292;&#20197;&#22686;&#24378;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#22810;&#36339;&#38142;&#25509;&#39044;&#27979;&#65292;&#24182;&#23637;&#31034;&#20102;&#26694;&#26550;&#22312;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#38646;&#27425;&#23581;&#35797;&#33021;&#21147;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.07311</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;KG-LLM&#65289;&#29992;&#20110;&#38142;&#25509;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph Large Language Model (KG-LLM) for Link Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07311
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#30693;&#35782;&#22270;&#35889;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65288;KG-LLM&#65289;&#65292;&#21033;&#29992;&#24605;&#32500;&#38142;&#25552;&#31034;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#31561;NLP&#33539;&#20363;&#65292;&#20197;&#22686;&#24378;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#22810;&#36339;&#38142;&#25509;&#39044;&#27979;&#65292;&#24182;&#23637;&#31034;&#20102;&#26694;&#26550;&#22312;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#38646;&#27425;&#23581;&#35797;&#33021;&#21147;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30693;&#35782;&#22270;&#35889;&#20998;&#26512;&#39046;&#22495;&#65292;&#39044;&#27979;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#20869;&#22810;&#20010;&#38142;&#25509;&#30340;&#20219;&#21153;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#30001;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#30693;&#35782;&#22270;&#23884;&#20837;&#25216;&#26415;&#30340;&#36827;&#27493;&#65292;&#36825;&#19968;&#25361;&#25112;&#21464;&#24471;&#36234;&#26469;&#36234;&#21487;&#35299;&#20915;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;&#30693;&#35782;&#22270;&#35889;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65288;KG-LLM&#65289;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#20851;&#38190;&#30340;NLP&#33539;&#20363;&#65292;&#21253;&#25324;&#24605;&#32500;&#38142;&#25552;&#31034;&#65288;CoT&#65289;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#65292;&#20197;&#22686;&#24378;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#22810;&#36339;&#38142;&#25509;&#39044;&#27979;&#12290;&#36890;&#36807;&#23558;KG&#36716;&#25442;&#20026;CoT&#25552;&#31034;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#26088;&#22312;&#35782;&#21035;&#24182;&#23398;&#20064;&#23454;&#20307;&#21450;&#20854;&#30456;&#20114;&#20851;&#31995;&#30340;&#28508;&#22312;&#34920;&#31034;&#12290;&#20026;&#20102;&#23637;&#31034;KG-LLM&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22312;&#35813;&#26694;&#26550;&#20869;&#24494;&#35843;&#20102;&#19977;&#31181;&#20027;&#35201;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#21516;&#26102;&#37319;&#29992;&#20102;&#38750;ICL&#21644;ICL&#20219;&#21153;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#35813;&#26694;&#26550;&#20026;LLMs&#25552;&#20379;&#38646;&#27425;&#23581;&#35797;&#33021;&#21147;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07311v1 Announce Type: new  Abstract: The task of predicting multiple links within knowledge graphs (KGs) stands as a challenge in the field of knowledge graph analysis, a challenge increasingly resolvable due to advancements in natural language processing (NLP) and KG embedding techniques. This paper introduces a novel methodology, the Knowledge Graph Large Language Model Framework (KG-LLM), which leverages pivotal NLP paradigms, including chain-of-thought (CoT) prompting and in-context learning (ICL), to enhance multi-hop link prediction in KGs. By converting the KG to a CoT prompt, our framework is designed to discern and learn the latent representations of entities and their interrelations. To show the efficacy of the KG-LLM Framework, we fine-tune three leading Large Language Models (LLMs) within this framework, employing both non-ICL and ICL tasks for a comprehensive evaluation. Further, we explore the framework's potential to provide LLMs with zero-shot capabilities f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#37327;&#21270;&#20102;&#32676;&#20307;&#19981;&#24179;&#34913;&#23545;&#26679;&#26412;&#22797;&#26434;&#24615;&#12289;&#25910;&#25947;&#36895;&#29575;&#21644;&#24179;&#22343;&#20197;&#21450;&#32676;&#20307;&#32423;&#27979;&#35797;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#39318;&#27425;&#25552;&#20379;&#20102;ERM&#22312;&#32676;&#20307;&#32423;&#27867;&#21270;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2403.07310</link><description>&lt;p&gt;
&#25512;&#21160;&#23569;&#25968;&#32676;&#20307;&#20221;&#39069;&#22914;&#20309;&#24433;&#21709;&#27867;&#21270;&#65311;&#20851;&#20110;&#19968;&#23618;&#38544;&#34255;&#23618;&#31070;&#32463;&#32593;&#32476;&#22312;&#32676;&#20307;&#19981;&#24179;&#34913;&#19978;&#30340;&#29702;&#35770;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
How does promoting the minority fraction affect generalization? A theoretical study of the one-hidden-layer neural network on group imbalance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07310
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#37327;&#21270;&#20102;&#32676;&#20307;&#19981;&#24179;&#34913;&#23545;&#26679;&#26412;&#22797;&#26434;&#24615;&#12289;&#25910;&#25947;&#36895;&#29575;&#21644;&#24179;&#22343;&#20197;&#21450;&#32676;&#20307;&#32423;&#27979;&#35797;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#39318;&#27425;&#25552;&#20379;&#20102;ERM&#22312;&#32676;&#20307;&#32423;&#27867;&#21270;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32676;&#20307;&#19981;&#24179;&#34913;&#19968;&#30452;&#26159;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;ERM&#65289;&#20013;&#24050;&#30693;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#21462;&#24471;&#30340;&#39640;&#24179;&#22343;&#20934;&#30830;&#29575;&#20276;&#38543;&#30528;&#23569;&#25968;&#32676;&#20307;&#30340;&#20302;&#20934;&#30830;&#29575;&#12290;&#23613;&#31649;&#26377;&#31639;&#27861;&#21162;&#21147;&#25913;&#21892;&#23569;&#25968;&#32676;&#20307;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#20851;&#20110;ERM&#22312;&#21508;&#20010;&#32676;&#20307;&#19978;&#30340;&#29702;&#35770;&#27867;&#21270;&#20998;&#26512;&#20173;&#28982;&#38590;&#20197;&#23454;&#29616;&#12290;&#36890;&#36807;&#29992;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#34920;&#36798;&#32676;&#20307;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#26412;&#25991;&#37327;&#21270;&#20102;&#21508;&#20010;&#32676;&#20307;&#23545;&#26679;&#26412;&#22797;&#26434;&#24615;&#12289;&#25910;&#25947;&#36895;&#29575;&#20197;&#21450;&#24179;&#22343;&#21644;&#32676;&#20307;&#32423;&#27979;&#35797;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#34429;&#28982;&#25105;&#20204;&#30340;&#29702;&#35770;&#26694;&#26550;&#38598;&#20013;&#22312;&#20351;&#29992;&#19968;&#23618;&#38544;&#34255;&#23618;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20108;&#20998;&#31867;&#65292;&#20294;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20379;&#20102;ERM&#22312;&#32676;&#20307;&#32423;&#27867;&#21270;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#38500;&#20102;&#36890;&#24120;&#30740;&#31350;&#30340;&#24179;&#22343;&#27867;&#21270;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#30340;&#19968;&#20123;&#35265;&#35299;&#21253;&#25324;&#24403;&#25152;&#26377;&#32676;&#20307;&#32423;&#21327;&#26041;&#24046;&#37117;&#22312;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07310v1 Announce Type: cross  Abstract: Group imbalance has been a known problem in empirical risk minimization (ERM), where the achieved high average accuracy is accompanied by low accuracy in a minority group. Despite algorithmic efforts to improve the minority group accuracy, a theoretical generalization analysis of ERM on individual groups remains elusive. By formulating the group imbalance problem with the Gaussian Mixture Model, this paper quantifies the impact of individual groups on the sample complexity, the convergence rate, and the average and group-level testing performance. Although our theoretical framework is centered on binary classification using a one-hidden-layer neural network, to the best of our knowledge, we provide the first theoretical analysis of the group-level generalization of ERM in addition to the commonly studied average generalization performance. Sample insights of our theoretical results include that when all group-level co-variance is in th
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;POSNEGDM&#26694;&#26550;&#65292;&#21033;&#29992;&#21464;&#21387;&#22120;&#27169;&#22411;&#21644;&#21453;&#39304;&#24378;&#21270;&#22120;&#65292;&#22312;&#36133;&#34880;&#30151;&#27835;&#30103;&#20013;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#65292;&#23558;&#24739;&#32773;&#29983;&#23384;&#29575;&#25552;&#39640;&#33267;97.39&#65285;&#65292;&#26126;&#26174;&#20248;&#20110;&#20256;&#32479;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.07309</link><description>&lt;p&gt;
&#38024;&#23545;&#36133;&#34880;&#30151;&#27835;&#30103;&#30340;&#24378;&#21270;&#24207;&#36143;&#20915;&#31574;&#65306;&#20855;&#26377;&#27515;&#20129;&#20998;&#31867;&#22120;&#21644;&#21464;&#21387;&#22120;&#30340;POSNEGDM&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Reinforced Sequential Decision-Making for Sepsis Treatment: The POSNEGDM Framework with Mortality Classifier and Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07309
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;POSNEGDM&#26694;&#26550;&#65292;&#21033;&#29992;&#21464;&#21387;&#22120;&#27169;&#22411;&#21644;&#21453;&#39304;&#24378;&#21270;&#22120;&#65292;&#22312;&#36133;&#34880;&#30151;&#27835;&#30103;&#20013;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#65292;&#23558;&#24739;&#32773;&#29983;&#23384;&#29575;&#25552;&#39640;&#33267;97.39&#65285;&#65292;&#26126;&#26174;&#20248;&#20110;&#20256;&#32479;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36133;&#34880;&#30151;&#26159;&#30001;&#26426;&#20307;&#23545;&#24863;&#26579;&#20135;&#29983;&#22840;&#24352;&#21453;&#24212;&#24341;&#21457;&#30340;&#19968;&#31181;&#21361;&#21450;&#29983;&#21629;&#30340;&#24773;&#20917;&#65292;&#35201;&#27714;&#32039;&#24613;&#24178;&#39044;&#20197;&#38450;&#27490;&#20005;&#37325;&#24182;&#21457;&#30151;&#12290;&#29616;&#26377;&#30340;&#29992;&#20110;&#22788;&#29702;&#36133;&#34880;&#30151;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#31163;&#32447;&#22330;&#26223;&#20013;&#25928;&#26524;&#19981;&#20339;&#65292;&#23384;&#27963;&#29575;&#20302;&#20110;50&#65285;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;POSNEGDM&#26694;&#26550;&#65292;&#21363;&#8220;&#29992;&#20110;&#24207;&#36143;&#20915;&#31574;&#30340;&#20855;&#26377;&#27491;&#36127;&#31034;&#33539;&#30340;&#24378;&#21270;&#23398;&#20064;&#8221;&#65292;&#21033;&#29992;&#21019;&#26032;&#30340;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#27169;&#22411;&#21644;&#21453;&#39304;&#24378;&#21270;&#22120;&#22797;&#21046;&#19987;&#23478;&#34892;&#20026;&#65292;&#21516;&#26102;&#32771;&#34385;&#20010;&#20307;&#24739;&#32773;&#29305;&#24449;&#12290;&#20855;&#26377;96.7&#65285;&#20934;&#30830;&#24230;&#30340;&#27515;&#20129;&#20998;&#31867;&#22120;&#25351;&#23548;&#27835;&#30103;&#20915;&#31574;&#21462;&#24471;&#31215;&#26497;&#32467;&#26524;&#12290;POSNEGDM&#26694;&#26550;&#26174;&#33879;&#25552;&#39640;&#20102;&#24739;&#32773;&#30340;&#29983;&#23384;&#29575;&#65292;&#25405;&#25937;&#20102;97.39&#65285;&#30340;&#24739;&#32773;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65288;&#20915;&#31574;&#21464;&#21387;&#22120;&#21644;&#34892;&#20026;&#20811;&#38534;&#65289;&#30340;&#23384;&#27963;&#29575;&#20998;&#21035;&#20026;33.4&#65285;&#21644;43.5&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07309v1 Announce Type: cross  Abstract: Sepsis, a life-threatening condition triggered by the body's exaggerated response to infection, demands urgent intervention to prevent severe complications. Existing machine learning methods for managing sepsis struggle in offline scenarios, exhibiting suboptimal performance with survival rates below 50%. This paper introduces the POSNEGDM -- ``Reinforcement Learning with Positive and Negative Demonstrations for Sequential Decision-Making" framework utilizing an innovative transformer-based model and a feedback reinforcer to replicate expert actions while considering individual patient characteristics. A mortality classifier with 96.7\% accuracy guides treatment decisions towards positive outcomes. The POSNEGDM framework significantly improves patient survival, saving 97.39% of patients, outperforming established machine learning algorithms (Decision Transformer and Behavioral Cloning) with survival rates of 33.4% and 43.5%, respective
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39564;&#35777;&#36741;&#21161;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#23631;&#38556;&#20989;&#25968;&#30340;&#32456;&#27490;&#20445;&#35777;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.07308</link><description>&lt;p&gt;
&#20351;&#29992;&#39564;&#35777;&#36741;&#21161;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#23631;&#38556;&#20989;&#25968;&#24182;&#20855;&#26377;&#32456;&#27490;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Verification-Aided Learning of Neural Network Barrier Functions with Termination Guarantees
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07308
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39564;&#35777;&#36741;&#21161;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#23631;&#38556;&#20989;&#25968;&#30340;&#32456;&#27490;&#20445;&#35777;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23631;&#38556;&#20989;&#25968;&#26159;&#20026;&#31995;&#32479;&#24314;&#31435;&#23433;&#20840;&#20445;&#35777;&#30340;&#19968;&#33324;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#26080;&#36890;&#29992;&#26041;&#27861;&#25214;&#21040;&#36825;&#20123;&#20989;&#25968;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#32570;&#28857;&#65292;&#26368;&#36817;&#30340;&#26041;&#27861;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#30001;&#39564;&#35777;&#31243;&#24207;&#21608;&#26399;&#29983;&#25104;&#30340;&#35757;&#32451;&#25968;&#25454;&#26469;&#23398;&#20064;&#36825;&#20123;&#20989;&#25968;&#65292;&#20174;&#32780;&#23548;&#33268;&#19968;&#20010;&#20855;&#26377;&#39564;&#35777;&#36741;&#21161;&#30340;&#23398;&#20064;&#26694;&#26550;&#12290;&#23613;&#31649;&#39564;&#35777;&#36741;&#21161;&#23398;&#20064;&#26694;&#26550;&#22312;&#33258;&#21160;&#21512;&#25104;&#23631;&#38556;&#20989;&#25968;&#26041;&#38754;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#35813;&#26694;&#26550;&#32570;&#20047;&#32456;&#27490;&#20445;&#35777;&#65292;&#24182;&#19988;&#22312;&#23454;&#36341;&#20013;&#21487;&#33021;&#22312;&#25214;&#21040;&#26377;&#25928;&#23631;&#38556;&#20989;&#25968;&#30340;&#25104;&#21151;&#29575;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#35299;&#20915;&#36825;&#20123;&#32570;&#28857;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#23631;&#38556;&#20989;&#25968;&#21512;&#25104;&#30340;&#20984;&#24418;&#24335;&#21270;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#23398;&#20064;&#19968;&#20010;&#32463;&#39564;&#33391;&#22909;&#30340;NN&#22522;&#20989;&#25968;&#65292;&#28982;&#21518;&#24212;&#29992;&#19968;&#31181;&#21033;&#29992;&#20984;&#24615;&#21644;&#39564;&#35777;&#20013;&#30340;&#21453;&#20363;&#30340;&#24494;&#35843;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07308v1 Announce Type: cross  Abstract: Barrier functions are a general framework for establishing a safety guarantee for a system. However, there is no general method for finding these functions. To address this shortcoming, recent approaches use self-supervised learning techniques to learn these functions using training data that are periodically generated by a verification procedure, leading to a verification-aided learning framework. Despite its immense potential in automating barrier function synthesis, the verification-aided learning framework does not have termination guarantees and may suffer from a low success rate of finding a valid barrier function in practice. In this paper, we propose a holistic approach to address these drawbacks. With a convex formulation of the barrier function synthesis, we propose to first learn an empirically well-behaved NN basis function and then apply a fine-tuning algorithm that exploits the convexity and counterexamples from the verif
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36328;&#27169;&#24577;&#30693;&#35782;&#33976;&#39311;&#21644;LLMs&#23545;&#40784;&#26694;&#26550;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#38745;&#24577;&#21644;&#21160;&#24577;&#30693;&#35782;&#65292;&#20805;&#20998;&#37322;&#25918;LLMs&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#28508;&#21147;</title><link>https://arxiv.org/abs/2403.07300</link><description>&lt;p&gt;
&#36890;&#36807;&#36328;&#27169;&#24577;&#30693;&#35782;&#33976;&#39311;&#25511;&#21046;&#39044;&#35757;&#32451;LLMs&#36827;&#34892;&#24191;&#20041;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Taming Pre-trained LLMs for Generalised Time Series Forecasting via Cross-modal Knowledge Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07300
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36328;&#27169;&#24577;&#30693;&#35782;&#33976;&#39311;&#21644;LLMs&#23545;&#40784;&#26694;&#26550;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#38745;&#24577;&#21644;&#21160;&#24577;&#30693;&#35782;&#65292;&#20805;&#20998;&#37322;&#25918;LLMs&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26368;&#36817;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24555;&#36895;&#22686;&#38271;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;&#26377;&#38480;&#30340;&#26102;&#38388;&#25968;&#25454;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#27169;&#22411;&#65292;&#38459;&#30861;&#20102;&#23427;&#20204;&#30340;&#27867;&#21270;&#12290;&#26368;&#36817;&#65292;&#38543;&#30528;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#28608;&#22686;&#65292;&#19968;&#20123;&#24037;&#20316;&#23581;&#35797;&#23558;LLMs&#24341;&#20837;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#30452;&#25509;&#23558;&#26102;&#38388;&#24207;&#21015;&#20316;&#20026;LLMs&#30340;&#36755;&#20837;&#65292;&#24573;&#30053;&#20102;&#26102;&#38388;&#21644;&#25991;&#26412;&#25968;&#25454;&#20043;&#38388;&#22266;&#26377;&#30340;&#27169;&#24577;&#24046;&#36317;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#26102;&#38388;&#24207;&#21015;&#23545;&#40784;&#26694;&#26550;&#65292;&#31216;&#20026;LLaTA&#65292;&#20197;&#20805;&#20998;&#21457;&#25381;LLMs&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#25361;&#25112;&#20013;&#30340;&#28508;&#21147;&#12290;&#22522;&#20110;&#36328;&#27169;&#24577;&#30693;&#35782;&#33976;&#39311;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#39044;&#35757;&#32451;LLMs&#20013;&#30340;&#36755;&#20837;&#26080;&#20851;&#38745;&#24577;&#30693;&#35782;&#21644;&#36755;&#20837;&#30456;&#20851;&#21160;&#24577;&#30693;&#35782;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#35813;&#26041;&#27861;&#20026;&#39044;&#27979;&#27169;&#22411;&#36171;&#33021;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07300v1 Announce Type: cross  Abstract: Multivariate time series forecasting has recently gained great success with the rapid growth of deep learning models. However, existing approaches usually train models from scratch using limited temporal data, preventing their generalization. Recently, with the surge of the Large Language Models (LLMs), several works have attempted to introduce LLMs into time series forecasting. Despite promising results, these methods directly take time series as the input to LLMs, ignoring the inherent modality gap between temporal and text data. In this work, we propose a novel Large Language Models and time series alignment framework, dubbed LLaTA, to fully unleash the potentials of LLMs in the time series forecasting challenge. Based on cross-modal knowledge distillation, the proposed method exploits both input-agnostic static knowledge and input-dependent dynamic knowledge in pre-trained LLMs. In this way, it empowers the forecasting model with f
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#33258;&#34920;&#36798;&#22270;&#32467;&#26500;&#37325;&#24314;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#22270;&#25968;&#25454;&#21387;&#32553;&#20013;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.07294</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#34920;&#36798;&#22270;&#32467;&#26500;&#37325;&#24314;&#23545;&#22270;&#25968;&#25454;&#36827;&#34892;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Graph Data Condensation via Self-expressive Graph Structure Reconstruction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07294
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#33258;&#34920;&#36798;&#22270;&#32467;&#26500;&#37325;&#24314;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#22270;&#25968;&#25454;&#21387;&#32553;&#20013;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35757;&#32451;&#22823;&#35268;&#27169;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#38656;&#27714;&#30340;&#22686;&#21152;&#65292;&#22270;&#25968;&#25454;&#21387;&#32553;&#24050;&#32463;&#25104;&#20026;&#22312;&#35757;&#32451;&#38454;&#27573;&#20943;&#36731;&#23384;&#20648;&#21644;&#26102;&#38388;&#25104;&#26412;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#23427;&#26088;&#22312;&#23558;&#21407;&#22987;&#22823;&#35268;&#27169;&#22270;&#21387;&#32553;&#20026;&#19968;&#20010;&#26356;&#23567;&#30340;&#21512;&#25104;&#22270;&#65292;&#21516;&#26102;&#20445;&#30041;&#35757;&#32451;&#19979;&#28216;GNN&#25152;&#38656;&#30340;&#22522;&#26412;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#35201;&#20040;&#38598;&#20013;&#20110;&#20165;&#20248;&#21270;&#33410;&#28857;&#29305;&#24449;&#65292;&#35201;&#20040;&#21162;&#21147;&#29420;&#31435;&#23398;&#20064;&#33410;&#28857;&#29305;&#24449;&#21644;&#22270;&#32467;&#26500;&#29983;&#25104;&#22120;&#12290;&#23427;&#20204;&#26080;&#27861;&#26126;&#30830;&#21033;&#29992;&#21407;&#22987;&#22270;&#32467;&#26500;&#30340;&#20449;&#24687;&#65292;&#24182;&#26410;&#33021;&#20026;&#21512;&#25104;&#25968;&#25454;&#38598;&#26500;&#24314;&#21487;&#35299;&#37322;&#30340;&#22270;&#32467;&#26500;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;\textbf{G}raph Data \textbf{C}ondensation via \textbf{S}elf-expressive Graph Structure \textbf{R}econstruction (\textbf{GCSR})&#30340;&#26032;&#22411;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#31361;&#20986;&#20043;&#22788;&#22312;&#20110;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07294v1 Announce Type: cross  Abstract: With the increasing demands of training graph neural networks (GNNs) on large-scale graphs, graph data condensation has emerged as a critical technique to relieve the storage and time costs during the training phase. It aims to condense the original large-scale graph to a much smaller synthetic graph while preserving the essential information necessary for efficiently training a downstream GNN. However, existing methods concentrate either on optimizing node features exclusively or endeavor to independently learn node features and the graph structure generator. They could not explicitly leverage the information of the original graph structure and failed to construct an interpretable graph structure for the synthetic dataset. To address these issues, we introduce a novel framework named \textbf{G}raph Data \textbf{C}ondensation via \textbf{S}elf-expressive Graph Structure \textbf{R}econstruction (\textbf{GCSR}). Our method stands out by 
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CypherTalk&#30340;&#25104;&#26412;&#25928;&#30410;&#21644;&#33258;&#36866;&#24212;&#30340;LLM&#25671;&#26179;&#35843;&#25972;&#21644;&#24674;&#22797;&#26426;&#21046;&#65292;&#36890;&#36807;&#20248;&#21270;&#25671;&#26179;&#25805;&#20316;&#31526;&#35774;&#32622;&#65292;&#23454;&#29616;&#20102;&#22312;&#25104;&#26412;&#12289;&#27169;&#22411;&#25928;&#29992;&#21644;&#38544;&#31169;&#20043;&#38388;&#26435;&#34913;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.07283</link><description>&lt;p&gt;
&#19968;&#31181;&#25104;&#26412;&#25928;&#30410;&#21644;&#33258;&#36866;&#24212;&#30340;LLM&#25671;&#26179;&#21644;&#24674;&#22797;&#26426;&#21046;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Framework for Cost-Effective and Self-Adaptive LLM Shaking and Recovery Mechanism
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07283
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CypherTalk&#30340;&#25104;&#26412;&#25928;&#30410;&#21644;&#33258;&#36866;&#24212;&#30340;LLM&#25671;&#26179;&#35843;&#25972;&#21644;&#24674;&#22797;&#26426;&#21046;&#65292;&#36890;&#36807;&#20248;&#21270;&#25671;&#26179;&#25805;&#20316;&#31526;&#35774;&#32622;&#65292;&#23454;&#29616;&#20102;&#22312;&#25104;&#26412;&#12289;&#27169;&#22411;&#25928;&#29992;&#21644;&#38544;&#31169;&#20043;&#38388;&#26435;&#34913;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#30495;&#23454;&#24212;&#29992;&#20013;&#21462;&#24471;&#24040;&#22823;&#25104;&#21151;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#29992;&#25143;&#24076;&#26395;&#36890;&#36807;&#20113;&#26381;&#21153;&#24320;&#21457;&#21644;&#37096;&#32626;&#20182;&#20204;&#23450;&#21046;&#30340;LLMs&#12290;&#28982;&#32780;&#65292;&#22312;&#19968;&#20123;&#29305;&#23450;&#39046;&#22495;&#65292;&#20154;&#20204;&#20173;&#28982;&#20851;&#27880;&#25104;&#26412;&#12289;&#38544;&#31169;&#38382;&#39064;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;CypherTalk&#30340;&#25104;&#26412;&#25928;&#30410;&#21644;&#33258;&#36866;&#24212;LLM&#25671;&#26179;&#35843;&#25972;&#21644;&#24674;&#22797;&#26426;&#21046;&#65292;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#27700;&#24179;&#21644;&#22402;&#30452;&#25671;&#26179;&#25805;&#20316;&#31526;&#65292;&#25105;&#20204;&#33021;&#22815;&#23454;&#29616;&#19982;&#22522;&#20110;&#23494;&#30721;&#23398;&#25110;&#24046;&#20998;&#38544;&#31169;&#26041;&#27861;&#30340;LLM&#38544;&#31169;&#20445;&#25252;&#26041;&#26696;&#30456;&#24403;&#30340;&#20934;&#30830;&#24615;&#32467;&#26524;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#20351;&#29992;CypherTalk&#26694;&#26550;&#65292;&#29992;&#25143;&#21487;&#20197;&#22312;&#20351;&#29992;&#20248;&#21270;&#25671;&#26179;&#25805;&#20316;&#31526;&#35774;&#32622;&#26102;&#23454;&#29616;&#21487;&#38752;&#30340;&#20934;&#30830;&#24615;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#39318;&#20010;&#32771;&#34385;&#22312;LLM&#22330;&#26223;&#20013;&#25104;&#26412;&#12289;&#27169;&#22411;&#25928;&#29992;&#21644;&#38544;&#31169;&#20043;&#38388;&#26435;&#34913;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07283v1 Announce Type: cross  Abstract: As Large Language Models (LLMs) gain great success in real-world applications, an increasing number of users are seeking to develop and deploy their customized LLMs through cloud services. Nonetheless, in some specific domains, there are still concerns regarding cost and trade-offs between privacy issues and accuracy. In this study, we introduce a cost-effective and self-adaptive LLM shaking tuning and recovery mechanism, named CypherTalk. With carefully designed horizontal and vertical shaking operators, we can achieve comparable accuracy results with SOTA privacy-preserving LLM schemes using Cryptography-based or Differential Privacy-based methods. Experiments also show that with the CypherTalk framework, users can achieve reliable accuracy when using optimized shaking operator settings. To our best knowledge, this is the first work that considers cost, and trade-off between model utility and privacy in LLM scenarios.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#28789;&#27963;&#30340;&#38750;&#21442;&#25968;&#21518;&#39564;&#25277;&#26679;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;&#38750;&#21442;&#25968;&#36801;&#31227;&#23398;&#20064;&#65288;NPTL&#65289;&#65292;&#29992;&#20197;&#35299;&#20915;&#22312;&#38750;&#21442;&#25968;&#23398;&#20064;&#32972;&#26223;&#19979;&#30340;&#20998;&#24067;&#36716;&#31227;&#38382;&#39064;&#65292;&#24182;&#25552;&#39640;&#20102;&#36801;&#31227;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.07282</link><description>&lt;p&gt;
&#36890;&#36807;&#28789;&#27963;&#30340;&#38750;&#21442;&#25968;&#21518;&#39564;&#25277;&#26679;&#22686;&#24378;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Enhancing Transfer Learning with Flexible Nonparametric Posterior Sampling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07282
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#28789;&#27963;&#30340;&#38750;&#21442;&#25968;&#21518;&#39564;&#25277;&#26679;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;&#38750;&#21442;&#25968;&#36801;&#31227;&#23398;&#20064;&#65288;NPTL&#65289;&#65292;&#29992;&#20197;&#35299;&#20915;&#22312;&#38750;&#21442;&#25968;&#23398;&#20064;&#32972;&#26223;&#19979;&#30340;&#20998;&#24067;&#36716;&#31227;&#38382;&#39064;&#65292;&#24182;&#25552;&#39640;&#20102;&#36801;&#31227;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#24037;&#20316;&#34920;&#26126;&#65292;&#28041;&#21450;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21508;&#31181;&#20219;&#21153;&#20013;&#65292;&#36801;&#31227;&#23398;&#20064;&#26174;&#31034;&#20986;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#20123;&#36801;&#31227;&#23398;&#20064;&#22330;&#26223;&#20013;&#65292;&#19979;&#28216;&#25968;&#25454;&#30340;&#20808;&#39564;&#20998;&#24067;&#22312;&#36125;&#21494;&#26031;&#27169;&#22411;&#24179;&#22343;&#65288;BMA&#65289;&#20013;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#20808;&#21069;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#20197;&#22260;&#32469;&#39044;&#35757;&#32451;&#35299;&#20915;&#26041;&#26696;&#20026;&#20013;&#24515;&#30340;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#30340;&#20808;&#39564;&#65292;&#20294;&#22312;&#22788;&#29702;&#19978;&#28216;&#25968;&#25454;&#21644;&#19979;&#28216;&#25968;&#25454;&#20043;&#38388;&#30340;&#20998;&#24067;&#36716;&#31227;&#26102;&#65292;&#36825;&#26679;&#30340;&#31574;&#30053;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#38750;&#21442;&#25968;&#36801;&#31227;&#23398;&#20064;&#65288;NPTL&#65289;&#65292;&#19968;&#31181;&#28789;&#27963;&#30340;&#21518;&#39564;&#25277;&#26679;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22312;&#38750;&#21442;&#25968;&#23398;&#20064;&#32972;&#26223;&#19979;&#30340;&#20998;&#24067;&#36716;&#31227;&#38382;&#39064;&#12290;&#38750;&#21442;&#25968;&#23398;&#20064;&#65288;NPL&#65289;&#26041;&#27861;&#26159;&#19968;&#31181;&#26368;&#36817;&#24341;&#20837;&#30340;&#26041;&#27861;&#65292;&#23427;&#37319;&#29992;&#20102;&#38750;&#21442;&#25968;&#20808;&#39564;&#36827;&#34892;&#21518;&#39564;&#25277;&#26679;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#27169;&#22411;&#38169;&#35823;&#35268;&#33539;&#24773;&#20917;&#65292;&#36866;&#29992;&#20110;&#28041;&#21450;&#19978;&#28216;&#21644;&#19979;&#28216;&#25968;&#25454;&#20043;&#38388;&#20998;&#24067;&#36716;&#31227;&#30340;&#36801;&#31227;&#23398;&#20064;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07282v1 Announce Type: new  Abstract: Transfer learning has recently shown significant performance across various tasks involving deep neural networks. In these transfer learning scenarios, the prior distribution for downstream data becomes crucial in Bayesian model averaging (BMA). While previous works proposed the prior over the neural network parameters centered around the pre-trained solution, such strategies have limitations when dealing with distribution shifts between upstream and downstream data. This paper introduces nonparametric transfer learning (NPTL), a flexible posterior sampling method to address the distribution shift issue within the context of nonparametric learning. The nonparametric learning (NPL) method is a recent approach that employs a nonparametric prior for posterior sampling, efficiently accounting for model misspecification scenarios, which is suitable for transfer learning scenarios that may involve the distribution shift between upstream and do
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;Anderson&#21152;&#36895;&#30340;IRL1&#31639;&#27861;&#65292;&#23558;&#20854;&#25910;&#25947;&#32467;&#26524;&#25193;&#23637;&#21040;&#38750;&#20809;&#28369;&#22330;&#26223;&#65292;&#19981;&#20381;&#36182;&#20110;Kurdyka-Lojasiewicz&#26465;&#20214;</title><link>https://arxiv.org/abs/2403.07271</link><description>&lt;p&gt;
Anderson&#21152;&#36895;&#29992;&#20110;&#36845;&#20195;&#37325;&#26032;&#21152;&#26435;&#30340;$\ell_1$&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Anderson acceleration for iteratively reweighted $\ell_1$ algorithm
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07271
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;Anderson&#21152;&#36895;&#30340;IRL1&#31639;&#27861;&#65292;&#23558;&#20854;&#25910;&#25947;&#32467;&#26524;&#25193;&#23637;&#21040;&#38750;&#20809;&#28369;&#22330;&#26223;&#65292;&#19981;&#20381;&#36182;&#20110;Kurdyka-Lojasiewicz&#26465;&#20214;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36845;&#20195;&#37325;&#26032;&#21152;&#26435;L1&#65288;IRL1&#65289;&#31639;&#27861;&#26159;&#19968;&#31181;&#24120;&#35265;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#38750;&#20984;&#21644;&#38750;&#20809;&#28369;&#27491;&#21017;&#21270;&#30340;&#31232;&#30095;&#20248;&#21270;&#38382;&#39064;&#12290;&#20854;&#21152;&#36895;&#31639;&#27861;&#30340;&#21457;&#23637;&#65292;&#36890;&#24120;&#37319;&#29992;Nesterov&#21152;&#36895;&#65292;&#24341;&#36215;&#20102;&#26497;&#22823;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#21152;&#36895;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#21644;&#22797;&#26434;&#24615;&#20998;&#26512;&#19968;&#30452;&#23384;&#22312;&#37325;&#22823;&#25361;&#25112;&#12290;&#26368;&#36817;&#65292;Anderson&#21152;&#36895;&#22240;&#20854;&#22312;&#21152;&#36895;&#22266;&#23450;&#28857;&#36845;&#20195;&#26041;&#38754;&#30340;&#20986;&#33394;&#24615;&#33021;&#32780;&#22791;&#21463;&#30633;&#30446;&#65292;&#35768;&#22810;&#26368;&#36817;&#30340;&#30740;&#31350;&#23558;&#20854;&#24212;&#29992;&#20110;&#22522;&#20110;&#26799;&#24230;&#30340;&#31639;&#27861;&#12290;&#21463;&#21040;Anderson&#21152;&#36895;&#24378;&#22823;&#24433;&#21709;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;Anderson&#21152;&#36895;&#30340;IRL1&#31639;&#27861;&#65292;&#24182;&#24314;&#31435;&#20102;&#20854;&#23616;&#37096;&#32447;&#24615;&#25910;&#25947;&#36895;&#24230;&#12290;&#25105;&#20204;&#23558;&#36825;&#19968;&#36890;&#24120;&#22312;&#24179;&#28369;&#35774;&#32622;&#20013;&#35266;&#23519;&#21040;&#30340;&#25910;&#25947;&#32467;&#26524;&#25193;&#23637;&#21040;&#38750;&#20809;&#28369;&#22330;&#26223;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#19981;&#20381;&#36182;&#20110;Kurdyka-Lojasiewicz&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07271v1 Announce Type: cross  Abstract: Iteratively reweighted L1 (IRL1) algorithm is a common algorithm for solving sparse optimization problems with nonconvex and nonsmooth regularization. The development of its acceleration algorithm, often employing Nesterov acceleration, has sparked significant interest. Nevertheless, the convergence and complexity analysis of these acceleration algorithms consistently poses substantial challenges. Recently, Anderson acceleration has gained prominence owing to its exceptional performance for speeding up fixed-point iteration, with numerous recent studies applying it to gradient-based algorithms. Motivated by the powerful impact of Anderson acceleration, we propose an Anderson-accelerated IRL1 algorithm and establish its local linear convergence rate. We extend this convergence result, typically observed in smooth settings, to a nonsmooth scenario. Importantly, our theoretical results do not depend on the Kurdyka-Lojasiewicz condition, a
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#20960;&#20046;&#25554;&#20540;&#32447;&#24615;&#22238;&#24402;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#35777;&#26126;&#20102;&#33539;&#25968;&#22686;&#38271;&#36805;&#36895;&#19988;&#25554;&#20540;&#19982;&#27867;&#21270;&#20043;&#38388;&#23384;&#22312;&#26126;&#30830;&#30340;&#26435;&#34913;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2403.07264</link><description>&lt;p&gt;
&#36817;&#25554;&#20540;&#22120;&#65306;&#24555;&#36895;&#33539;&#25968;&#22686;&#38271;&#19982;&#25554;&#20540;&#19982;&#27867;&#21270;&#20043;&#38388;&#30340;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Near-Interpolators: Rapid Norm Growth and the Trade-Off between Interpolation and Generalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07264
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#20960;&#20046;&#25554;&#20540;&#32447;&#24615;&#22238;&#24402;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#35777;&#26126;&#20102;&#33539;&#25968;&#22686;&#38271;&#36805;&#36895;&#19988;&#25554;&#20540;&#19982;&#27867;&#21270;&#20043;&#38388;&#23384;&#22312;&#26126;&#30830;&#30340;&#26435;&#34913;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20960;&#20046;&#25554;&#20540;&#32447;&#24615;&#22238;&#24402;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#65306;&#20854;&#35757;&#32451;&#35823;&#24046;&#964;&#20026;&#27491;&#20294;&#24456;&#23567;&#65292;&#21363;&#20302;&#20110;&#22122;&#22768;&#27700;&#24179;&#12290;&#22312;&#23545;&#25968;&#25454;&#20998;&#24067;&#36827;&#34892;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#20551;&#35774;&#21644;&#23545;&#25968;&#25454;&#21327;&#26041;&#24046;&#30697;&#38453;&#931;&#36827;&#34892;&#29305;&#24449;&#34928;&#20943;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20219;&#20309;&#36817;&#25554;&#20540;&#22120;&#37117;&#34920;&#29616;&#20986;&#24555;&#36895;&#30340;&#33539;&#25968;&#22686;&#38271;&#65306;&#23545;&#20110;&#22266;&#23450;&#30340;&#964;&#65292;&#946;&#30340;&#24179;&#26041;&#8741;&#946;&#8741;2 &#30340;&#22686;&#38271;&#29575;&#20026;&#937;(n^&#945;)&#65292;&#20854;&#20013;n&#20026;&#26679;&#26412;&#25968;&#37327;&#65292;&#945;&gt;1&#26159;&#29305;&#24449;&#34928;&#20943;&#30340;&#25351;&#25968;&#65292;&#21363;&#955;_i(&#931;)&#8764;i^(-&#945;)&#12290;&#36825;&#24847;&#21619;&#30528;&#29616;&#26377;&#30340;&#29420;&#31435;&#20110;&#25968;&#25454;&#30340;&#33539;&#25968;&#30028;&#38480;&#24517;&#23450;&#26494;&#24347;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22312;&#30456;&#21516;&#30340;&#21306;&#38388;&#20869;&#65292;&#25105;&#20204;&#31934;&#30830;&#22320;&#21051;&#30011;&#20102;&#25554;&#20540;&#21644;&#27867;&#21270;&#20043;&#38388;&#30340;&#28176;&#36817;&#26435;&#34913;&#12290;&#25105;&#20204;&#30340;&#34920;&#24449;&#25581;&#31034;&#20986;&#20102;&#22823;&#22810;&#25968;&#29616;&#26377;&#33539;&#25968;&#30028;&#38480;&#26159;&#23485;&#26494;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07264v1 Announce Type: cross  Abstract: We study the generalization capability of nearly-interpolating linear regressors: $\boldsymbol{\beta}$'s whose training error $\tau$ is positive but small, i.e., below the noise floor. Under a random matrix theoretic assumption on the data distribution and an eigendecay assumption on the data covariance matrix $\boldsymbol{\Sigma}$, we demonstrate that any near-interpolator exhibits rapid norm growth: for $\tau$ fixed, $\boldsymbol{\beta}$ has squared $\ell_2$-norm $\mathbb{E}[\|{\boldsymbol{\beta}}\|_{2}^{2}] = \Omega(n^{\alpha})$ where $n$ is the number of samples and $\alpha &gt;1$ is the exponent of the eigendecay, i.e., $\lambda_i(\boldsymbol{\Sigma}) \sim i^{-\alpha}$. This implies that existing data-independent norm-based bounds are necessarily loose. On the other hand, in the same regime we precisely characterize the asymptotic trade-off between interpolation and generalization. Our characterization reveals that larger norm scalin
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20004;&#27493;&#24418;&#24335;&#39044;&#27979;&#26041;&#27861;&#65292;&#26412;&#25991;&#23454;&#29616;&#20102;&#33258;&#36866;&#24212;&#36793;&#30028;&#26694;&#19981;&#30830;&#23450;&#24615;&#30340;&#37327;&#21270;&#65292;&#20445;&#35777;&#20102;&#23545;&#35937;&#36793;&#30028;&#26694;&#19981;&#30830;&#23450;&#24615;&#21306;&#38388;&#30340;&#35206;&#30422;&#29575;&#65292;&#21253;&#25324;&#20102;&#38169;&#35823;&#20998;&#31867;&#30340;&#23545;&#35937;&#65292;&#21516;&#26102;&#30830;&#20445;&#36793;&#30028;&#26694;&#21306;&#38388;&#33021;&#22815;&#36866;&#24212;&#29289;&#20307;&#22823;&#23567;&#65292;&#23454;&#29616;&#26356;&#24179;&#34913;&#30340;&#35206;&#30422;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.07263</link><description>&lt;p&gt;
&#36890;&#36807;&#20004;&#27493;&#24418;&#24335;&#39044;&#27979;&#23454;&#29616;&#33258;&#36866;&#24212;&#36793;&#30028;&#26694;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Adaptive Bounding Box Uncertainties via Two-Step Conformal Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07263
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20004;&#27493;&#24418;&#24335;&#39044;&#27979;&#26041;&#27861;&#65292;&#26412;&#25991;&#23454;&#29616;&#20102;&#33258;&#36866;&#24212;&#36793;&#30028;&#26694;&#19981;&#30830;&#23450;&#24615;&#30340;&#37327;&#21270;&#65292;&#20445;&#35777;&#20102;&#23545;&#35937;&#36793;&#30028;&#26694;&#19981;&#30830;&#23450;&#24615;&#21306;&#38388;&#30340;&#35206;&#30422;&#29575;&#65292;&#21253;&#25324;&#20102;&#38169;&#35823;&#20998;&#31867;&#30340;&#23545;&#35937;&#65292;&#21516;&#26102;&#30830;&#20445;&#36793;&#30028;&#26694;&#21306;&#38388;&#33021;&#22815;&#36866;&#24212;&#29289;&#20307;&#22823;&#23567;&#65292;&#23454;&#29616;&#26356;&#24179;&#34913;&#30340;&#35206;&#30422;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#27169;&#22411;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#23545;&#20110;&#20687;&#33258;&#21160;&#39550;&#39542;&#36825;&#26679;&#30340;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#32771;&#34385;&#20026;&#22810;&#29289;&#20307;&#26816;&#27979;&#37327;&#21270;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#24418;&#24335;&#39044;&#27979;&#26469;&#33719;&#24471;&#20855;&#26377;&#20445;&#35777;&#35206;&#30422;&#29575;&#30340;&#29289;&#20307;&#36793;&#30028;&#26694;&#19981;&#30830;&#23450;&#24615;&#21306;&#38388;&#12290;&#36825;&#26679;&#20570;&#30340;&#19968;&#20010;&#25361;&#25112;&#26159;&#36793;&#30028;&#26694;&#30340;&#39044;&#27979;&#21462;&#20915;&#20110;&#29289;&#20307;&#30340;&#31867;&#21035;&#26631;&#31614;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20004;&#27493;&#24418;&#24335;&#26041;&#27861;&#65292;&#23558;&#23545;&#39044;&#27979;&#31867;&#21035;&#26631;&#31614;&#30340;&#19981;&#30830;&#23450;&#24615;&#20256;&#25773;&#21040;&#36793;&#30028;&#26694;&#30340;&#19981;&#30830;&#23450;&#24615;&#21306;&#38388;&#20013;&#12290;&#36825;&#26679;&#65292;&#25105;&#20204;&#30340;&#24418;&#24335;&#35206;&#30422;&#20445;&#35777;&#30340;&#26377;&#25928;&#24615;&#26356;&#24191;&#27867;&#65292;&#21253;&#25324;&#20102;&#34987;&#38169;&#35823;&#20998;&#31867;&#30340;&#29289;&#20307;&#65292;&#30830;&#20445;&#23427;&#20204;&#22312;&#38656;&#35201;&#26368;&#22823;&#23433;&#20840;&#20445;&#35777;&#26102;&#30340;&#23454;&#29992;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26032;&#39062;&#30340;&#38598;&#25104;&#21644;&#20998;&#20301;&#25968;&#22238;&#24402;&#24418;&#24335;&#65292;&#20197;&#30830;&#20445;&#36793;&#30028;&#26694;&#21306;&#38388;&#33021;&#22815;&#36866;&#24212;&#29289;&#20307;&#22823;&#23567;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#24179;&#34913;&#30340;&#35206;&#30422;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07263v1 Announce Type: cross  Abstract: Quantifying a model's predictive uncertainty is essential for safety-critical applications such as autonomous driving. We consider quantifying such uncertainty for multi-object detection. In particular, we leverage conformal prediction to obtain uncertainty intervals with guaranteed coverage for object bounding boxes. One challenge in doing so is that bounding box predictions are conditioned on the object's class label. Thus, we develop a novel two-step conformal approach that propagates uncertainty in predicted class labels into the uncertainty intervals for the bounding boxes. This broadens the validity of our conformal coverage guarantees to include incorrectly classified objects, ensuring their usefulness when maximal safety assurances are required. Moreover, we investigate novel ensemble and quantile regression formulations to ensure the bounding box intervals are adaptive to object size, leading to a more balanced coverage across
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#36866;&#24212;&#20248;&#21183;&#30340;&#31574;&#30053;&#20248;&#21270;&#65288;A2PO&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#31163;&#32447;&#23398;&#20064;&#65292;&#33021;&#22815;&#35299;&#20915;&#22810;&#34892;&#20026;&#31574;&#30053;&#25910;&#38598;&#30340;&#32422;&#26463;&#20914;&#31361;&#38382;&#39064;&#65292;&#26377;&#25928;&#36991;&#20813;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.07262</link><description>&lt;p&gt;
&#36866;&#24212;&#20248;&#21183;&#30340;&#31574;&#30053;&#20248;&#21270;&#29992;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Advantage-Aware Policy Optimization for Offline Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07262
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#36866;&#24212;&#20248;&#21183;&#30340;&#31574;&#30053;&#20248;&#21270;&#65288;A2PO&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#31163;&#32447;&#23398;&#20064;&#65292;&#33021;&#22815;&#35299;&#20915;&#22810;&#34892;&#20026;&#31574;&#30053;&#25910;&#38598;&#30340;&#32422;&#26463;&#20914;&#31361;&#38382;&#39064;&#65292;&#26377;&#25928;&#36991;&#20813;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#33268;&#21147;&#20110;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#38598;&#26469;&#21046;&#23450;&#26377;&#25928;&#30340;&#26234;&#33021;&#20307;&#31574;&#30053;&#65292;&#32780;&#26080;&#38656;&#22312;&#32447;&#20132;&#20114;&#65292;&#36890;&#36807;&#22312;&#34892;&#20026;&#31574;&#30053;&#30340;&#25903;&#25345;&#19979;&#26045;&#21152;&#36866;&#24403;&#30340;&#20445;&#23432;&#32422;&#26463;&#26469;&#35299;&#20915;&#20998;&#24067;&#22806;&#38382;&#39064;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#36866;&#24212;&#20248;&#21183;&#30340;&#31574;&#30053;&#20248;&#21270;&#65288;A2PO&#65289;&#26041;&#27861;&#65292;&#20197;&#26126;&#30830;&#26500;&#24314;&#38024;&#23545;&#28151;&#21512;&#36136;&#37327;&#25968;&#25454;&#38598;&#30340;&#31163;&#32447;&#23398;&#20064;&#20248;&#21183;&#24863;&#30693;&#31574;&#30053;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07262v1 Announce Type: cross  Abstract: Offline Reinforcement Learning (RL) endeavors to leverage offline datasets to craft effective agent policy without online interaction, which imposes proper conservative constraints with the support of behavior policies to tackle the Out-Of-Distribution (OOD) problem. However, existing works often suffer from the constraint conflict issue when offline datasets are collected from multiple behavior policies, i.e., different behavior policies may exhibit inconsistent actions with distinct returns across the state space. To remedy this issue, recent Advantage-Weighted (AW) methods prioritize samples with high advantage values for agent training while inevitably leading to overfitting on these samples. In this paper, we introduce a novel Advantage-Aware Policy Optimization (A2PO) method to explicitly construct advantage-aware policy constraints for offline learning under mixed-quality datasets. Specifically, A2PO employs a Conditional Variat
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#25239;&#24615;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#35813;&#30740;&#31350;&#35299;&#20915;&#20102;&#20174;&#26377;&#38480;&#25968;&#37327;&#31574;&#30053;&#20013;&#23398;&#20064;&#20219;&#21153;&#34920;&#31034;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#23558;&#31574;&#30053;&#19982;&#31163;&#32447;&#20219;&#21153;&#34920;&#31034;&#23398;&#20064;&#20998;&#31163;&#12290;</title><link>https://arxiv.org/abs/2403.07261</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#25239;&#24615;&#25968;&#25454;&#22686;&#24378;&#26469;&#23558;&#31574;&#30053;&#20174;&#31163;&#32447;&#20219;&#21153;&#34920;&#31034;&#23398;&#20064;&#20013;&#35299;&#34261;
&lt;/p&gt;
&lt;p&gt;
Disentangling Policy from Offline Task Representation Learning via Adversarial Data Augmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07261
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#25239;&#24615;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#35813;&#30740;&#31350;&#35299;&#20915;&#20102;&#20174;&#26377;&#38480;&#25968;&#37327;&#31574;&#30053;&#20013;&#23398;&#20064;&#20219;&#21153;&#34920;&#31034;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#23558;&#31574;&#30053;&#19982;&#31163;&#32447;&#20219;&#21153;&#34920;&#31034;&#23398;&#20064;&#20998;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#20803;&#24378;&#21270;&#23398;&#20064;&#65288;OMRL&#65289;&#39640;&#25928;&#22320;&#20801;&#35768;&#19968;&#20010;&#20195;&#29702;&#22312;&#20165;&#20381;&#36182;&#20110;&#38745;&#24577;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#22788;&#29702;&#26032;&#39062;&#20219;&#21153;&#12290;&#20026;&#20102;&#31934;&#20934;&#39640;&#25928;&#22320;&#35782;&#21035;&#20219;&#21153;&#65292;&#29616;&#26377;&#30340;OMRL&#30740;&#31350;&#24314;&#35758;&#23398;&#20064;&#21333;&#29420;&#30340;&#20219;&#21153;&#34920;&#31034;&#65292;&#24182;&#23558;&#20854;&#19982;&#31574;&#30053;&#36755;&#20837;&#32467;&#21512;&#65292;&#20174;&#32780;&#24418;&#25104;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#20803;&#31574;&#30053;&#12290;&#35757;&#32451;&#20219;&#21153;&#34920;&#31034;&#30340;&#20027;&#35201;&#26041;&#27861;&#26159;&#37319;&#29992;&#20351;&#29992;&#22810;&#20219;&#21153;&#31163;&#32447;&#25968;&#25454;&#30340;&#23545;&#27604;&#23398;&#20064;&#12290;&#25968;&#25454;&#38598;&#36890;&#24120;&#28085;&#30422;&#26469;&#33258;&#21508;&#31181;&#31574;&#30053;&#65288;&#21363;&#34892;&#20026;&#31574;&#30053;&#65289;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#20851;&#20110;&#19981;&#21516;&#20219;&#21153;&#30340;&#22823;&#37327;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#35774;&#32622;&#20013;&#65292;&#25910;&#38598;&#26469;&#33258;&#22823;&#37327;&#31574;&#30053;&#30340;&#25968;&#25454;&#19981;&#20165;&#19981;&#20999;&#23454;&#38469;&#65292;&#32780;&#19988;&#36890;&#24120;&#38590;&#20197;&#23454;&#29616;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#36716;&#32780;&#37319;&#21462;&#26356;&#21463;&#38480;&#21046;&#20294;&#26356;&#23454;&#38469;&#30340;&#24773;&#24418;&#65292;&#21363;&#20351;&#29992;&#26377;&#38480;&#25968;&#37327;&#30340;&#31574;&#30053;&#36827;&#34892;&#22810;&#20219;&#21153;&#25968;&#25454;&#25910;&#38598;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#23398;&#20064;&#21040;&#30340;&#20219;&#21153;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07261v1 Announce Type: cross  Abstract: Offline meta-reinforcement learning (OMRL) proficiently allows an agent to tackle novel tasks while solely relying on a static dataset. For precise and efficient task identification, existing OMRL research suggests learning separate task representations that be incorporated with policy input, thus forming a context-based meta-policy. A major approach to train task representations is to adopt contrastive learning using multi-task offline data. The dataset typically encompasses interactions from various policies (i.e., the behavior policies), thus providing a plethora of contextual information regarding different tasks. Nonetheless, amassing data from a substantial number of policies is not only impractical but also often unattainable in realistic settings. Instead, we resort to a more constrained yet practical scenario, where multi-task data collection occurs with a limited number of policies. We observed that learned task representatio
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#36741;&#21161;&#30340;&#24182;&#34892;&#24178;&#25200;&#28040;&#38500;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#26080;&#25480;&#26435;NOMA&#31995;&#32479;&#20013;&#32852;&#21512;&#22788;&#29702;&#27963;&#21160;&#26816;&#27979;&#12289;&#20449;&#36947;&#20272;&#35745;&#21644;&#25968;&#25454;&#26816;&#27979;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.07255</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#36741;&#21161;&#30340;&#26426;&#22120;&#31867;&#22411;&#36890;&#20449;&#20013;&#26080;&#25480;&#26435;NOMA&#30340;&#24182;&#34892;&#24178;&#25200;&#28040;&#38500;
&lt;/p&gt;
&lt;p&gt;
Deep Learning-Assisted Parallel Interference Cancellation for Grant-Free NOMA in Machine-Type Communication
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07255
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#36741;&#21161;&#30340;&#24182;&#34892;&#24178;&#25200;&#28040;&#38500;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#26080;&#25480;&#26435;NOMA&#31995;&#32479;&#20013;&#32852;&#21512;&#22788;&#29702;&#27963;&#21160;&#26816;&#27979;&#12289;&#20449;&#36947;&#20272;&#35745;&#21644;&#25968;&#25454;&#26816;&#27979;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#19978;&#34892;&#26080;&#25480;&#26435;&#38750;&#27491;&#20132;&#22810;&#22336;&#25509;&#20837;&#65288;NOMA&#65289;&#31995;&#32479;&#20013;&#32852;&#21512;&#36827;&#34892;&#27963;&#21160;&#26816;&#27979;&#65288;AD&#65289;&#12289;&#20449;&#36947;&#20272;&#35745;&#65288;CE&#65289;&#21644;&#25968;&#25454;&#26816;&#27979;&#65288;DD&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#20102;&#19968;&#20010;&#21463;&#24182;&#34892;&#24178;&#25200;&#28040;&#38500;&#65288;PIC&#65289;&#21551;&#21457;&#30340;&#36845;&#20195;&#21644;&#24182;&#34892;&#24178;&#25200;&#31227;&#38500;&#31574;&#30053;&#65292;&#24182;&#32467;&#21512;&#20102;&#28145;&#24230;&#23398;&#20064;&#26469;&#20849;&#21516;&#35299;&#20915;AD&#12289;CE&#21644;DD&#38382;&#39064;&#12290;&#22522;&#20110;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19977;&#31181;PIC&#26694;&#26550;&#65292;&#27599;&#31181;&#26694;&#26550;&#37117;&#35774;&#35745;&#29992;&#20110;&#30456;&#24178;&#25110;&#38750;&#19968;&#33268;&#26041;&#26696;&#12290;&#31532;&#19968;&#20010;&#26694;&#26550;&#22312;&#30456;&#24178;&#26041;&#26696;&#20013;&#20351;&#29992;&#25509;&#25910;&#21040;&#30340;&#23548;&#39057;&#20449;&#21495;&#36827;&#34892;&#32852;&#21512;AD&#21644;CE&#12290;&#22312;&#27492;&#26694;&#26550;&#22522;&#30784;&#19978;&#65292;&#31532;&#20108;&#20010;&#26694;&#26550;&#21033;&#29992;&#25509;&#25910;&#21040;&#30340;&#23548;&#39057;&#21644;&#25968;&#25454;&#20449;&#21495;&#36827;&#34892;CE&#65292;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#30456;&#24178;&#26041;&#26696;&#20013;AD&#12289;CE&#21644;DD&#30340;&#24615;&#33021;&#12290;&#31532;&#19977;&#20010;&#26694;&#26550;&#35774;&#35745;&#29992;&#20110;&#36866;&#24212;&#21253;&#21547;&#23569;&#37327;&#25968;&#25454;&#20301;&#30340;&#38750;&#30456;&#24178;&#26041;&#26696;&#65292;&#21516;&#26102;&#23454;&#29616;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07255v1 Announce Type: cross  Abstract: In this paper, we present a novel approach for joint activity detection (AD), channel estimation (CE), and data detection (DD) in uplink grant-free non-orthogonal multiple access (NOMA) systems. Our approach employs an iterative and parallel interference removal strategy inspired by parallel interference cancellation (PIC), enhanced with deep learning to jointly tackle the AD, CE, and DD problems. Based on this approach, we develop three PIC frameworks, each of which is designed for either coherent or non-coherence schemes. The first framework performs joint AD and CE using received pilot signals in the coherent scheme. Building upon this framework, the second framework utilizes both the received pilot and data signals for CE, further enhancing the performances of AD, CE, and DD in the coherent scheme. The third framework is designed to accommodate the non-coherent scheme involving a small number of data bits, which simultaneously perf
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GuideGen&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#26681;&#25454;&#25991;&#26412;&#25552;&#31034;&#32852;&#21512;&#29983;&#25104;CT&#22270;&#20687;&#21644;&#33145;&#37096;&#22120;&#23448;&#20197;&#21450;&#32467;&#30452;&#32928;&#30284;&#32452;&#32455;&#25513;&#33180;&#65292;&#20026;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#39046;&#22495;&#25552;&#20379;&#20102;&#19968;&#31181;&#29983;&#25104;&#25968;&#25454;&#38598;&#30340;&#26032;&#36884;&#24452;&#12290;</title><link>https://arxiv.org/abs/2403.07247</link><description>&lt;p&gt;
GuideGen&#65306;&#19968;&#31181;&#29992;&#20110;&#32852;&#21512;CT&#20307;&#31215;&#21644;&#35299;&#21078;&#32467;&#26500;&#29983;&#25104;&#30340;&#25991;&#26412;&#24341;&#23548;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
GuideGen: A Text-guided Framework for Joint CT Volume and Anatomical structure Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07247
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GuideGen&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#26681;&#25454;&#25991;&#26412;&#25552;&#31034;&#32852;&#21512;&#29983;&#25104;CT&#22270;&#20687;&#21644;&#33145;&#37096;&#22120;&#23448;&#20197;&#21450;&#32467;&#30452;&#32928;&#30284;&#32452;&#32455;&#25513;&#33180;&#65292;&#20026;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#39046;&#22495;&#25552;&#20379;&#20102;&#19968;&#31181;&#29983;&#25104;&#25968;&#25454;&#38598;&#30340;&#26032;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07247v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449; &#25688;&#35201;&#65306;&#20026;&#20102;&#25910;&#38598;&#24102;&#26377;&#22270;&#20687;&#21644;&#30456;&#24212;&#26631;&#31614;&#30340;&#22823;&#22411;&#21307;&#23398;&#25968;&#25454;&#38598;&#32780;&#36827;&#34892;&#30340;&#27880;&#37322;&#36127;&#25285;&#21644;&#22823;&#37327;&#24037;&#20316;&#24456;&#23569;&#26159;&#21010;&#31639;&#19988;&#20196;&#20154;&#26395;&#32780;&#29983;&#30031;&#30340;&#12290;&#36825;&#23548;&#33268;&#20102;&#32570;&#20047;&#20016;&#23500;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#21066;&#24369;&#20102;&#19979;&#28216;&#20219;&#21153;&#65292;&#24182;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#21152;&#21095;&#20102;&#21307;&#23398;&#39046;&#22495;&#38754;&#20020;&#30340;&#22270;&#20687;&#20998;&#26512;&#25361;&#25112;&#12290;&#20316;&#20026;&#19968;&#31181;&#26435;&#23452;&#20043;&#35745;&#65292;&#37492;&#20110;&#29983;&#25104;&#24615;&#31070;&#32463;&#27169;&#22411;&#30340;&#26368;&#36817;&#25104;&#21151;&#65292;&#29616;&#22312;&#21487;&#20197;&#22312;&#22806;&#37096;&#32422;&#26463;&#30340;&#24341;&#23548;&#19979;&#20197;&#39640;&#20445;&#30495;&#24230;&#21512;&#25104;&#22270;&#20687;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#36825;&#31181;&#21487;&#33021;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;GuideGen&#65306;&#19968;&#31181;&#32852;&#21512;&#29983;&#25104;&#33145;&#37096;&#22120;&#23448;&#21644;&#32467;&#30452;&#32928;&#30284;CT&#22270;&#20687;&#21644;&#32452;&#32455;&#25513;&#33180;&#30340;&#31649;&#32447;&#65292;&#20854;&#21463;&#25991;&#26412;&#25552;&#31034;&#26465;&#20214;&#32422;&#26463;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20307;&#31215;&#25513;&#33180;&#37319;&#26679;&#22120;&#65292;&#20197;&#36866;&#24212;&#25513;&#33180;&#26631;&#31614;&#30340;&#31163;&#25955;&#20998;&#24067;&#24182;&#29983;&#25104;&#20302;&#20998;&#36776;&#29575;3D&#32452;&#32455;&#25513;&#33180;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#30340;&#26465;&#20214;&#22270;&#20687;&#29983;&#25104;&#22120;&#20250;&#22312;&#25910;&#21040;&#30456;&#24212;&#25991;&#26412;&#25552;&#31034;&#30340;&#24773;&#20917;&#19979;&#33258;&#22238;&#24402;&#29983;&#25104;CT&#20999;&#29255;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07247v1 Announce Type: cross  Abstract: The annotation burden and extensive labor for gathering a large medical dataset with images and corresponding labels are rarely cost-effective and highly intimidating. This results in a lack of abundant training data that undermines downstream tasks and partially contributes to the challenge image analysis faces in the medical field. As a workaround, given the recent success of generative neural models, it is now possible to synthesize image datasets at a high fidelity guided by external constraints. This paper explores this possibility and presents \textbf{GuideGen}: a pipeline that jointly generates CT images and tissue masks for abdominal organs and colorectal cancer conditioned on a text prompt. Firstly, we introduce Volumetric Mask Sampler to fit the discrete distribution of mask labels and generate low-resolution 3D tissue masks. Secondly, our Conditional Image Generator autoregressively generates CT slices conditioned on a corre
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Dataset Condensation&#8221;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#21452;&#22495;&#21305;&#37197;&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#20016;&#23500;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2403.07245</link><description>&lt;p&gt;
&#36890;&#36807;&#21452;&#22495;&#21305;&#37197;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#30340;&#25968;&#25454;&#38598;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Dataset Condensation for Time Series Classification via Dual Domain Matching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07245
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Dataset Condensation&#8221;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#21452;&#22495;&#21305;&#37197;&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#20016;&#23500;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#22312;&#21508;&#31181;&#30740;&#31350;&#39046;&#22495;&#20013;&#34987;&#35777;&#26126;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#31649;&#29702;&#22823;&#37327;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#22312;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#26041;&#38754;&#23384;&#22312;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;&#26368;&#36817;&#65292;&#19968;&#31181;&#21517;&#20026;&#8220;&#25968;&#25454;&#38598;&#21387;&#32553;&#8221;&#25216;&#26415;&#24050;&#32463;&#25104;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#35813;&#25216;&#26415;&#29983;&#25104;&#19968;&#20010;&#36739;&#23567;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#20854;&#22312;&#35832;&#22914;&#20998;&#31867;&#31561;&#19979;&#28216;&#20219;&#21153;&#20013;&#20855;&#26377;&#19982;&#23436;&#25972;&#30495;&#23454;&#25968;&#25454;&#38598;&#30456;&#36817;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#26041;&#27861;&#20027;&#35201;&#35774;&#35745;&#29992;&#20110;&#22270;&#20687;&#21644;&#22270;&#25968;&#25454;&#38598;&#65292;&#30452;&#25509;&#23558;&#23427;&#20204;&#36866;&#24212;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#20250;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#65292;&#22240;&#20026;&#23427;&#20204;&#26080;&#27861;&#26377;&#25928;&#21033;&#29992;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#22266;&#26377;&#30340;&#20016;&#23500;&#20449;&#24687;&#65292;&#23588;&#20854;&#26159;&#22312;&#39057;&#22495;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07245v1 Announce Type: new  Abstract: Time series data has been demonstrated to be crucial in various research fields. The management of large quantities of time series data presents challenges in terms of deep learning tasks, particularly for training a deep neural network. Recently, a technique named \textit{Dataset Condensation} has emerged as a solution to this problem. This technique generates a smaller synthetic dataset that has comparable performance to the full real dataset in downstream tasks such as classification. However, previous methods are primarily designed for image and graph datasets, and directly adapting them to the time series dataset leads to suboptimal performance due to their inability to effectively leverage the rich information inherent in time series data, particularly in the frequency domain. In this paper, we propose a novel framework named Dataset \textit{\textbf{Cond}}ensation for \textit{\textbf{T}}ime \textit{\textbf{S}}eries \textit{\textbf{
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#25506;&#32034;&#22914;&#20309;&#20943;&#23569;CLIP&#23545;&#20266;&#29305;&#24449;&#30340;&#20381;&#36182;&#65292;&#20174;&#32780;&#25552;&#39640;&#32676;&#20307;&#40065;&#26834;&#24615;&#65292;&#32780;&#26080;&#38656;&#20351;&#29992;&#27880;&#37322;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2403.07241</link><description>&lt;p&gt;
&#26657;&#20934;&#22810;&#27169;&#24577;&#34920;&#31034;&#65306;&#22312;&#19981;&#20351;&#29992;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#36861;&#27714;&#32676;&#20307;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Calibrating Multi-modal Representations: A Pursuit of Group Robustness without Annotations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07241
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#25506;&#32034;&#22914;&#20309;&#20943;&#23569;CLIP&#23545;&#20266;&#29305;&#24449;&#30340;&#20381;&#36182;&#65292;&#20174;&#32780;&#25552;&#39640;&#32676;&#20307;&#40065;&#26834;&#24615;&#65292;&#32780;&#26080;&#38656;&#20351;&#29992;&#27880;&#37322;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07241v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449; &#25688;&#35201;&#65306;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65292;&#22914;CLIP&#65292;&#22312;&#22810;&#26679;&#30340;&#19979;&#28216;&#20219;&#21153;&#19978;&#21462;&#24471;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#33539;&#24335;&#23384;&#22312;&#19968;&#20123;&#30171;&#28857;&#65306;(i) &#30452;&#25509;&#24494;&#35843;&#25972;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#26082;&#26102;&#38388;&#23494;&#38598;&#21448;&#35745;&#31639;&#25104;&#26412;&#39640;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#35843;&#25972;&#21518;&#30340;&#27169;&#22411;&#24448;&#24448;&#21464;&#24471;&#39640;&#24230;&#19987;&#19994;&#21270;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#23454;&#38469;&#37096;&#32626;&#20013;&#30340;&#23454;&#29992;&#24615;&#65307;(ii) &#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#20998;&#31867;&#22120;&#21487;&#33021;&#36807;&#24230;&#20381;&#36182;&#20110;&#20266;&#29305;&#24449;-&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#19982;&#30446;&#26631;&#30456;&#20851;&#30340;&#27169;&#24335;&#65292;&#20294;&#19982;&#30495;&#23454;&#26631;&#31614;&#20989;&#25968;&#26080;&#20851;&#65307;(iii) &#29616;&#26377;&#20851;&#20110;&#20943;&#23569;&#23545;&#20266;&#29305;&#24449;&#20381;&#36182;&#30340;&#30740;&#31350;&#65292;&#20027;&#35201;&#22522;&#20110;&#25105;&#20204;&#33021;&#22815;&#35782;&#21035;&#36825;&#20123;&#29305;&#24449;&#30340;&#20551;&#35774;&#65292;&#23545;&#20110;&#23454;&#38469;&#24212;&#29992;&#24182;&#27809;&#26377;&#25552;&#20379;&#30830;&#20999;&#30340;&#20445;&#35777;&#12290;&#20316;&#20026;&#19968;&#39033;&#35797;&#28857;&#30740;&#31350;&#65292;&#26412;&#24037;&#20316;&#20391;&#37325;&#20110;&#25506;&#32034;&#22312;&#19981;&#20351;&#29992;&#20219;&#20309;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#20943;&#23569;CLIP&#23545;&#20266;&#29305;&#24449;&#20381;&#36182;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07241v1 Announce Type: cross  Abstract: Fine-tuning pre-trained vision-language models, like CLIP, has yielded success on diverse downstream tasks. However, several pain points persist for this paradigm: (i) directly tuning entire pre-trained models becomes both time-intensive and computationally costly. Additionally, these tuned models tend to become highly specialized, limiting their practicality for real-world deployment; (ii) recent studies indicate that pre-trained vision-language classifiers may overly depend on spurious features -- patterns that correlate with the target in training data, but are not related to the true labeling function; and (iii) existing studies on mitigating the reliance on spurious features, largely based on the assumption that we can identify such features, does not provide definitive assurance for real-world applications. As a piloting study, this work focuses on exploring mitigating the reliance on spurious features for CLIP without using any 
&lt;/p&gt;</description></item><item><title>&#22312;&#22478;&#24066;&#39550;&#39542;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#22788;&#29702;&#30340;&#32852;&#21512;&#39044;&#27979;&#21644;&#35268;&#21010;&#26041;&#27861;&#65292;&#21033;&#29992;&#23398;&#20064;&#30340;&#38170;&#28857;&#23884;&#20837;&#26469;&#21442;&#25968;&#21270;&#39640;&#32423;&#39550;&#39542;&#34892;&#20026;&#30340;&#31163;&#25955;&#27169;&#24335;&#65292;&#24182;&#23454;&#29616;&#38024;&#23545;&#36825;&#20123;&#31163;&#25955;&#27169;&#24335;&#30340;&#38381;&#29615;&#35268;&#21010;&#12290;</title><link>https://arxiv.org/abs/2403.07232</link><description>&lt;p&gt;
&#21487;&#22788;&#29702;&#30340;&#22478;&#24066;&#39550;&#39542;&#20013;&#31163;&#25955;&#34892;&#20026;&#27169;&#24335;&#30340;&#32852;&#21512;&#39044;&#27979;&#21644;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Tractable Joint Prediction and Planning over Discrete Behavior Modes for Urban Driving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07232
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22478;&#24066;&#39550;&#39542;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#22788;&#29702;&#30340;&#32852;&#21512;&#39044;&#27979;&#21644;&#35268;&#21010;&#26041;&#27861;&#65292;&#21033;&#29992;&#23398;&#20064;&#30340;&#38170;&#28857;&#23884;&#20837;&#26469;&#21442;&#25968;&#21270;&#39640;&#32423;&#39550;&#39542;&#34892;&#20026;&#30340;&#31163;&#25955;&#27169;&#24335;&#65292;&#24182;&#23454;&#29616;&#38024;&#23545;&#36825;&#20123;&#31163;&#25955;&#27169;&#24335;&#30340;&#38381;&#29615;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#21160;&#39550;&#39542;&#26041;&#38754;&#65292;&#35757;&#32451;&#22810;&#27169;&#24577;&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#26377;&#25928;&#22320;&#23558;&#36825;&#20123;&#27169;&#22411;&#19982;&#19979;&#28216;&#35268;&#21010;&#22120;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#25511;&#21046;&#26041;&#27861;&#25972;&#21512;&#20173;&#28982;&#26159;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#65292;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#20256;&#32479;&#19978;&#34987;&#29992;&#20110;&#24320;&#29615;&#39044;&#27979;&#30340;&#35780;&#20272;&#65292;&#20294;&#23427;&#20204;&#21487;&#20197;&#34987;&#29992;&#26469;&#21442;&#25968;&#21270;&#19981;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#30340;&#33258;&#22238;&#24402;&#38381;&#29615;&#27169;&#22411;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#21033;&#29992;&#23398;&#20064;&#30340;&#38170;&#28857;&#23884;&#20837;&#26469;&#39044;&#27979;&#22810;&#26465;&#36712;&#36857;&#30340;&#26368;&#36817;&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;&#65292;&#21457;&#29616;&#36825;&#20123;&#38170;&#28857;&#23884;&#20837;&#21487;&#20197;&#21442;&#25968;&#21270;&#20195;&#34920;&#39640;&#32423;&#39550;&#39542;&#34892;&#20026;&#30340;&#31163;&#25955;&#21644;&#29420;&#29305;&#27169;&#24335;&#12290;&#25105;&#20204;&#24314;&#35758;&#22312;&#36825;&#20123;&#31163;&#25955;&#28508;&#22312;&#27169;&#24335;&#19978;&#25191;&#34892;&#20805;&#20998;&#21453;&#24212;&#24615;&#30340;&#38381;&#29615;&#35268;&#21010;&#65292;&#20174;&#32780;&#20351;&#25105;&#20204;&#33021;&#22815;&#21487;&#22788;&#29702;&#22320;&#27169;&#25311;&#27599;&#19968;&#27493;&#39588;&#20013;&#20195;&#29702;&#20043;&#38388;&#30340;&#22240;&#26524;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07232v1 Announce Type: cross  Abstract: Significant progress has been made in training multimodal trajectory forecasting models for autonomous driving. However, effectively integrating these models with downstream planners and model-based control approaches is still an open problem. Although these models have conventionally been evaluated for open-loop prediction, we show that they can be used to parameterize autoregressive closed-loop models without retraining. We consider recent trajectory prediction approaches which leverage learned anchor embeddings to predict multiple trajectories, finding that these anchor embeddings can parameterize discrete and distinct modes representing high-level driving behaviors. We propose to perform fully reactive closed-loop planning over these discrete latent modes, allowing us to tractably model the causal interactions between agents at each step. We validate our approach on a suite of more dynamic merging scenarios, finding that our approa
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Curry-DPO&#30340;&#26041;&#27861;&#65292;&#22312;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;(DPO)&#20013;&#21033;&#29992;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#22810;&#20010;&#20559;&#22909;&#23545;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#30456;&#27604;&#20110;&#26631;&#20934;&#21333;&#19968;&#23545;DPO&#35774;&#32622;&#26377;&#30528;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.07230</link><description>&lt;p&gt;
Curry-DPO&#65306;&#21033;&#29992;&#35838;&#31243;&#23398;&#20064;&#21644;&#25490;&#21517;&#20559;&#22909;&#22686;&#24378;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Curry-DPO: Enhancing Alignment using Curriculum Learning &amp; Ranked Preferences
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07230
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Curry-DPO&#30340;&#26041;&#27861;&#65292;&#22312;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;(DPO)&#20013;&#21033;&#29992;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#22810;&#20010;&#20559;&#22909;&#23545;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#30456;&#27604;&#20110;&#26631;&#20934;&#21333;&#19968;&#23545;DPO&#35774;&#32622;&#26377;&#30528;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;(DPO)&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#25216;&#26415;&#65292;&#21033;&#29992;&#25104;&#23545;&#20559;&#22909;&#25968;&#25454;(&#36890;&#24120;&#26159;&#27599;&#20010;&#29992;&#25143;&#25552;&#31034;&#36873;&#25321;&#21644;&#25298;&#32477;&#30340;&#21709;&#24212;&#23545;)&#23558;LLMs&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#23545;&#20110;&#32473;&#23450;&#25552;&#31034;&#21487;&#33021;&#20250;&#23384;&#22312;&#22810;&#20010;&#21709;&#24212;&#65292;&#36825;&#20123;&#21709;&#24212;&#30340;&#36136;&#37327;&#30456;&#23545;&#20110;&#24444;&#27492;&#32780;&#35328;&#26377;&#25152;&#19981;&#21516;&#12290;&#26377;&#20102;&#36825;&#20123;&#22810;&#20010;&#21709;&#24212;&#30340;&#36136;&#37327;&#35780;&#32423;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#36825;&#20123;&#21709;&#24212;&#20026;&#32473;&#23450;&#25552;&#31034;&#21019;&#24314;&#22810;&#20010;&#20559;&#22909;&#23545;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20391;&#37325;&#20110;&#36890;&#36807;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#31995;&#32479;&#22320;&#21033;&#29992;&#26500;&#24314;&#30340;&#22810;&#20010;&#20559;&#22909;&#23545;&#26469;&#36827;&#34892;DPO&#35757;&#32451;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#26681;&#25454;&#19981;&#21516;&#30340;&#26631;&#20934;&#23558;&#36825;&#20123;&#22810;&#20010;&#20559;&#22909;&#25968;&#25454;&#23545;&#20174;&#26131;&#21040;&#38590;(&#27169;&#25311;&#35838;&#31243;&#35757;&#32451;)&#25490;&#24207;&#12290;&#25105;&#20204;&#35814;&#32454;&#27604;&#36739;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#19982;&#26631;&#20934;&#21333;&#19968;&#23545;DPO&#35774;&#32622;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;Curry-DPO&#65292;&#22312;MTbench&#12289;Vicuna&#12289;Wiz&#19978;&#22987;&#32456;&#34920;&#29616;&#20986;&#22686;&#24378;&#30340;&#24615;&#33021;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07230v1 Announce Type: cross  Abstract: Direct Preference Optimization (DPO) is an effective technique that leverages pairwise preference data (usually one chosen and rejected response pair per user prompt) to align LLMs to human preferences. In practice, multiple responses can exist for a given prompt with varying quality relative to each other. With availability of such quality ratings for multiple responses, we propose utilizing these responses to create multiple preference pairs for a given prompt. Our work focuses on systematically using the constructed multiple preference pair in DPO training via curriculum learning methodology. In particular, we order these multiple pairs of preference data from easy to hard (emulating curriculum training) according to various criteria. We show detailed comparisons of our proposed approach to the standard single-pair DPO setting. Our method, which we call Curry-DPO consistently shows increased performance gains on MTbench, Vicuna, Wiz
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LookupFFN&#30340;&#26367;&#20195;&#27169;&#22359;&#65292;&#36890;&#36807;&#23558;&#20851;&#38190;&#25805;&#20316;&#37325;&#26032;&#26500;&#24314;&#20026;&#20869;&#23384;&#26597;&#25214;&#65292;&#20351;&#24471;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#21069;&#39304;&#32593;&#32476;&#65288;FFNs&#65289;&#22312;CPU&#25512;&#26029;&#20013;&#21464;&#24471;&#26356;&#36731;&#24039;</title><link>https://arxiv.org/abs/2403.07221</link><description>&lt;p&gt;
LookupFFN: &#35753;&#21464;&#21387;&#22120;&#22312;CPU&#25512;&#26029;&#20013;&#26356;&#36731;&#24039;
&lt;/p&gt;
&lt;p&gt;
LookupFFN: Making Transformers Compute-lite for CPU inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07221
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LookupFFN&#30340;&#26367;&#20195;&#27169;&#22359;&#65292;&#36890;&#36807;&#23558;&#20851;&#38190;&#25805;&#20316;&#37325;&#26032;&#26500;&#24314;&#20026;&#20869;&#23384;&#26597;&#25214;&#65292;&#20351;&#24471;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#21069;&#39304;&#32593;&#32476;&#65288;FFNs&#65289;&#22312;CPU&#25512;&#26029;&#20013;&#21464;&#24471;&#26356;&#36731;&#24039;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;GPU&#38598;&#32676;&#22914;&#20170;&#26159;&#35757;&#32451;&#22823;&#22411;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#27169;&#22411;&#30340;&#39318;&#36873;&#65292;&#20294;&#20986;&#20110;&#35832;&#22810;&#21407;&#22240;&#65292;&#21253;&#25324;&#24037;&#20316;&#27969;&#31243;&#30340;&#20415;&#21033;&#24615;&#12289;&#23433;&#20840;&#24615;&#21644;&#25104;&#26412;&#65292;&#19968;&#20123;&#21162;&#21147;&#27491;&#22312;&#25506;&#35752;CPU&#22312;&#34892;&#19994;&#30340;&#35768;&#22810;&#39046;&#22495;&#20013;&#26159;&#21542;&#21487;&#20197;&#25104;&#20026;&#25512;&#26029;&#30340;&#21487;&#34892;&#36873;&#25321;&#12290;&#21463;&#36825;&#20123;&#32771;&#34385;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#29616;&#20195;DNN&#26550;&#26500;&#20013;&#30340;&#19968;&#20010;&#24037;&#20316;&#27169;&#22359;&#65292;&#22522;&#20110;GEMM&#30340;&#21069;&#39304;&#32593;&#32476;&#65288;FFNs&#65289;&#65292;&#24182;&#35780;&#20272;&#23427;&#21487;&#20197;&#34987;&#21046;&#20316;&#20026;&#35745;&#31639;&#36731;&#37327;&#65288;&#25110;FLOP&#36731;&#37327;&#65289;&#30340;&#31243;&#24230;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#20844;&#24335;&#65288;&#25105;&#20204;&#31216;&#20043;&#20026;LookupFFN&#65289;&#65292;&#29992;&#20110;&#21462;&#20195;&#21463;&#26368;&#36817;&#20851;&#20110;&#20351;&#29992;&#23616;&#37096;&#25935;&#24863;&#21704;&#24076;&#65288;LSH&#65289;&#36817;&#20284;FFNs&#30340;&#30740;&#31350;&#21551;&#21457;&#30340;GEMM&#22522;&#30784;&#30340;FFNs&#12290;&#25105;&#20204;&#30340;&#20844;&#24335;&#23558;&#22823;&#37096;&#20998;&#22522;&#26412;&#25805;&#20316;&#37325;&#26032;&#26500;&#24314;&#20026;&#20869;&#23384;&#26597;&#25214;&#65292;&#21033;&#29992;&#20102;&#20219;&#20309;&#24179;&#21488;&#19978;&#20004;&#31181;&#36164;&#28304;&#20043;&#38388;&#30340;&#26435;&#34913;&#65306;&#35745;&#31639;&#21644;&#20869;&#23384;&#65288;&#22240;&#20026;CPU&#25552;&#20379;&#20102;&#36825;&#31181;&#26435;&#34913;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07221v1 Announce Type: new  Abstract: While GPU clusters are the de facto choice for training large deep neural network (DNN) models today, several reasons including ease of workflow, security and cost have led to efforts investigating whether CPUs may be viable for inference in routine use in many sectors of the industry. But the imbalance between the compute capabilities of GPUs and CPUs is huge. Motivated by these considerations, we study a module which is a workhorse within modern DNN architectures, GEMM based Feed Forward Networks (FFNs), and assess the extent to which it can be made compute- (or FLOP-) lite. Specifically, we propose an alternative formulation (we call it LookupFFN) to GEMM based FFNs inspired by the recent studies of using Locality Sensitive Hashing (LSH) to approximate FFNs. Our formulation recasts most essential operations as a memory look-up, leveraging the trade-off between the two resources on any platform: compute and memory (since CPUs offer it 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26088;&#22312;&#35774;&#35745;&#20445;&#25252;&#38544;&#31169;&#30340;&#36712;&#36857;&#21457;&#24067;&#26041;&#27861;&#30340;&#26694;&#26550;&#65292;&#29305;&#21035;&#24378;&#35843;&#20102;&#36873;&#25321;&#36866;&#24403;&#38544;&#31169;&#21333;&#20301;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.07218</link><description>&lt;p&gt;
SoK&#65306;&#36712;&#36857;&#29983;&#25104;&#26159;&#21542;&#33021;&#22815;&#20860;&#39038;&#38544;&#31169;&#21644;&#23454;&#29992;&#24615;&#65311;
&lt;/p&gt;
&lt;p&gt;
SoK: Can Trajectory Generation Combine Privacy and Utility?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07218
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26088;&#22312;&#35774;&#35745;&#20445;&#25252;&#38544;&#31169;&#30340;&#36712;&#36857;&#21457;&#24067;&#26041;&#27861;&#30340;&#26694;&#26550;&#65292;&#29305;&#21035;&#24378;&#35843;&#20102;&#36873;&#25321;&#36866;&#24403;&#38544;&#31169;&#21333;&#20301;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#20301;&#32622;&#36712;&#36857;&#20195;&#34920;&#30528;&#20379;&#21508;&#31181;&#20998;&#26512;&#21644;&#22522;&#20110;&#20301;&#32622;&#30340;&#26381;&#21153;&#30340;&#23453;&#36149;&#25968;&#25454;&#26469;&#28304;&#65292;&#20294;&#23427;&#20204;&#21487;&#33021;&#27844;&#28431;&#25935;&#24863;&#20449;&#24687;&#65292;&#22914;&#25919;&#27835;&#21644;&#23447;&#25945;&#20559;&#22909;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#19981;&#21516;ially private&#21457;&#24067;&#26426;&#21046;&#65292;&#20801;&#35768;&#22312;&#20005;&#26684;&#30340;&#38544;&#31169;&#20445;&#35777;&#19979;&#36827;&#34892;&#20998;&#26512;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#20445;&#25252;&#26041;&#26696;&#23384;&#22312;&#38544;&#31169;&#21644;&#23454;&#29992;&#24615;&#30340;&#26435;&#34913;&#38480;&#21046;&#65292;&#24182;&#23481;&#26131;&#21463;&#21040;&#30456;&#20851;&#24615;&#21644;&#37325;&#26500;&#25915;&#20987;&#30340;&#23041;&#32961;&#12290;&#21512;&#25104;&#36712;&#36857;&#25968;&#25454;&#29983;&#25104;&#21644;&#21457;&#24067;&#20195;&#34920;&#20102;&#20445;&#25252;&#31639;&#27861;&#30340;&#19968;&#20010;&#20855;&#26377;&#21069;&#26223;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#34429;&#28982;&#26368;&#21021;&#30340;&#25552;&#35758;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#23454;&#29992;&#24615;&#65292;&#20294;&#26410;&#33021;&#25552;&#20379;&#20005;&#26684;&#30340;&#38544;&#31169;&#20445;&#35777;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#23450;&#20041;&#20116;&#20010;&#35774;&#35745;&#30446;&#26631;&#65292;&#29305;&#21035;&#24378;&#35843;&#36873;&#25321;&#36866;&#24403;&#30340;&#38544;&#31169;&#21333;&#20301;&#30340;&#37325;&#35201;&#24615;&#65292;&#26469;&#35774;&#35745;&#19968;&#20010;&#20445;&#25252;&#38544;&#31169;&#30340;&#36712;&#36857;&#21457;&#24067;&#26041;&#27861;&#12290;&#22522;&#20110;&#36825;&#19968;&#26694;&#26550;&#65292;&#25105;&#20204;&#31616;&#35201;&#35752;&#35770;&#20102;&#29616;&#26377;&#30340;&#36712;&#36857;&#21457;&#24067;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07218v1 Announce Type: cross  Abstract: While location trajectories represent a valuable data source for analyses and location-based services, they can reveal sensitive information, such as political and religious preferences. Differentially private publication mechanisms have been proposed to allow for analyses under rigorous privacy guarantees. However, the traditional protection schemes suffer from a limiting privacy-utility trade-off and are vulnerable to correlation and reconstruction attacks. Synthetic trajectory data generation and release represent a promising alternative to protection algorithms. While initial proposals achieve remarkable utility, they fail to provide rigorous privacy guarantees. This paper proposes a framework for designing a privacy-preserving trajectory publication approach by defining five design goals, particularly stressing the importance of choosing an appropriate Unit of Privacy. Based on this framework, we briefly discuss the existing traje
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#31574;&#30053;&#36880;&#27493;&#35843;&#25972;&#22235;&#36724;&#39134;&#34892;&#22120;&#25511;&#21046;&#22120;&#30340;&#22686;&#30410;, &#26174;&#33879;&#20943;&#23567;&#36319;&#36394;&#35823;&#24046;&#12290;</title><link>https://arxiv.org/abs/2403.07216</link><description>&lt;p&gt;
&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#26080;&#20154;&#26426;&#25511;&#21046;&#30340;&#33258;&#36866;&#24212;&#22686;&#30410;&#35843;&#24230;
&lt;/p&gt;
&lt;p&gt;
Adaptive Gain Scheduling using Reinforcement Learning for Quadcopter Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07216
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#31574;&#30053;&#36880;&#27493;&#35843;&#25972;&#22235;&#36724;&#39134;&#34892;&#22120;&#25511;&#21046;&#22120;&#30340;&#22686;&#30410;, &#26174;&#33879;&#20943;&#23567;&#36319;&#36394;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26469;&#35843;&#25972;&#22235;&#36724;&#39134;&#34892;&#22120;&#25511;&#21046;&#22120;&#22686;&#30410;&#30340;&#25216;&#26415;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;&#36817;&#31471;&#25919;&#31574;&#20248;&#21270;&#65288;PPO&#65289;&#26469;&#35757;&#32451;&#19968;&#20010;&#22312;&#39134;&#34892;&#20013;&#35843;&#25972;&#32423;&#32852;&#21453;&#39304;&#25511;&#21046;&#22120;&#22686;&#30410;&#30340;&#31574;&#30053;&#12290;&#35813;&#25511;&#21046;&#22120;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#22312;&#27839;&#30528;&#25351;&#23450;&#36712;&#36857;&#39134;&#34892;&#26102;&#26368;&#23567;&#21270;&#36319;&#36394;&#35823;&#24046;&#12290;&#35770;&#25991;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#20998;&#26512;&#33258;&#36866;&#24212;&#22686;&#30410;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23558;&#20854;&#19982;&#38745;&#24577;&#22686;&#30410;&#25511;&#21046;&#31639;&#27861;&#30340;&#24615;&#33021;&#36827;&#34892;&#27604;&#36739;&#65292;&#20854;&#20013;&#31215;&#20998;&#24179;&#26041;&#35823;&#24046;&#21644;&#31215;&#20998;&#26102;&#38388;&#24179;&#26041;&#35823;&#24046;&#34987;&#29992;&#20316;&#24230;&#37327;&#26631;&#20934;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#38745;&#24577;&#22686;&#30410;&#25511;&#21046;&#22120;&#30456;&#27604;&#65292;&#33258;&#36866;&#24212;&#22686;&#30410;&#26041;&#26696;&#30340;&#36319;&#36394;&#35823;&#24046;&#20943;&#23569;&#20102;&#36229;&#36807;40&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07216v1 Announce Type: cross  Abstract: The paper presents a technique using reinforcement learning (RL) to adapt the control gains of a quadcopter controller. Specifically, we employed Proximal Policy Optimization (PPO) to train a policy which adapts the gains of a cascaded feedback controller in-flight. The primary goal of this controller is to minimize tracking error while following a specified trajectory. The paper's key objective is to analyze the effectiveness of the adaptive gain policy and compare it to the performance of a static gain control algorithm, where the Integral Squared Error and Integral Time Squared Error are used as metrics. The results show that the adaptive gain scheme achieves over 40$\%$ decrease in tracking error as compared to the static gain controller.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#25910;&#25947;&#24847;&#35782;&#30340;&#22686;&#37327;&#26102;&#38388;&#33218;&#22312;&#32447;&#27169;&#22411;&#36873;&#25321;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#36873;&#25321;&#26368;&#20339;&#27169;&#22411;&#26102;&#24179;&#34913;&#20219;&#21153;&#22870;&#21169;&#21644;&#25506;&#32034;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2403.07213</link><description>&lt;p&gt;
&#36873;&#25321;&#21738;&#20010;LLM&#65311;&#20855;&#26377;&#25910;&#25947;&#24847;&#35782;&#30340;&#22686;&#37327;&#26102;&#38388;&#33218;&#30340;&#22312;&#32447;&#27169;&#22411;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Which LLM to Play? Convergence-Aware Online Model Selection with Time-Increasing Bandits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07213
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#25910;&#25947;&#24847;&#35782;&#30340;&#22686;&#37327;&#26102;&#38388;&#33218;&#22312;&#32447;&#27169;&#22411;&#36873;&#25321;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#36873;&#25321;&#26368;&#20339;&#27169;&#22411;&#26102;&#24179;&#34913;&#20219;&#21153;&#22870;&#21169;&#21644;&#25506;&#32034;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Web-based&#24212;&#29992;&#65292;&#22914;&#32842;&#22825;&#26426;&#22120;&#20154;&#12289;&#25628;&#32034;&#24341;&#25806;&#21644;&#26032;&#38395;&#25512;&#33616;&#65292;&#38543;&#30528;LLMs&#30340;&#26085;&#30410;&#26222;&#21450;&#65292;&#22312;&#35268;&#27169;&#21644;&#22797;&#26434;&#24615;&#19978;&#32487;&#32493;&#22686;&#38271;&#12290;&#22312;&#32447;&#27169;&#22411;&#36873;&#25321;&#22240;&#38656;&#35201;&#22312;&#24179;&#34913;&#20219;&#21153;&#22870;&#21169;&#21644;&#25506;&#32034;&#25104;&#26412;&#30340;&#21516;&#26102;&#36873;&#25321;&#26368;&#20339;&#27169;&#22411;&#32780;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#20256;&#32479;&#30340;&#36873;&#25321;&#26041;&#27861;&#36890;&#24120;&#22312;&#36873;&#25321;&#19968;&#20010;&#27169;&#22411;&#20043;&#21069;&#35780;&#20272;&#27599;&#20010;&#20505;&#36873;&#27169;&#22411;&#65292;&#38543;&#30528;&#35757;&#32451;&#21644;&#24494;&#35843;LLMs&#25104;&#26412;&#30340;&#19978;&#21319;&#65292;&#36825;&#20123;&#26041;&#27861;&#21464;&#24471;&#19981;&#20999;&#23454;&#38469;&#12290;&#27492;&#22806;&#65292;&#20998;&#37197;&#36807;&#22810;&#36164;&#28304;&#21435;&#25506;&#32034;&#34920;&#29616;&#19981;&#20339;&#30340;&#27169;&#22411;&#26159;&#19981;&#21487;&#21462;&#30340;&#12290;&#23613;&#31649;&#19968;&#20123;&#26368;&#26032;&#30340;&#24037;&#20316;&#21033;&#29992;&#22312;&#32447;&#33218;&#31639;&#27861;&#26469;&#31649;&#29702;&#27169;&#22411;&#36873;&#25321;&#20013;&#30340;&#36825;&#31181;&#25506;&#32034;-&#24320;&#21457;&#26435;&#34913;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#24573;&#35270;&#20102;&#22686;&#38271;&#28982;&#21518;&#25910;&#25947;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07213v1 Announce Type: new  Abstract: Web-based applications such as chatbots, search engines and news recommendations continue to grow in scale and complexity with the recent surge in the adoption of LLMs. Online model selection has thus garnered increasing attention due to the need to choose the best model among a diverse set while balancing task reward and exploration cost. Organizations faces decisions like whether to employ a costly API-based LLM or a locally finetuned small LLM, weighing cost against performance. Traditional selection methods often evaluate every candidate model before choosing one, which are becoming impractical given the rising costs of training and finetuning LLMs. Moreover, it is undesirable to allocate excessive resources towards exploring poor-performing models. While some recent works leverage online bandit algorithm to manage such exploration-exploitation trade-off in model selection, they tend to overlook the increasing-then-converging trend i
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#29702;&#35770;&#19978;&#23545;&#28369;&#21160;&#31383;&#21475;&#39640;&#26031;&#26680;&#23494;&#24230;&#20272;&#35745;&#22120;&#36827;&#34892;&#30740;&#31350;&#65292;&#25552;&#20379;&#20102;&#36873;&#25321;&#26368;&#20248;&#26435;&#37325;&#24207;&#21015;&#30340;&#21407;&#21017;&#25351;&#21335;&#65292;&#36890;&#36807;&#23454;&#35777;&#35777;&#25454;&#34920;&#26126;&#35813;&#21152;&#26435;&#26041;&#26696;&#30456;&#27604;&#21551;&#21457;&#24335;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#36319;&#36394;&#24615;&#33021;</title><link>https://arxiv.org/abs/2403.07207</link><description>&lt;p&gt;
&#20351;&#29992;&#29702;&#35770;&#19978;&#26368;&#20248;&#30340;&#28369;&#21160;&#31383;&#21475;&#26041;&#27861;&#36319;&#36394;&#21160;&#24577;&#39640;&#26031;&#23494;&#24230;
&lt;/p&gt;
&lt;p&gt;
Tracking Dynamic Gaussian Density with a Theoretically Optimal Sliding Window Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07207
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#29702;&#35770;&#19978;&#23545;&#28369;&#21160;&#31383;&#21475;&#39640;&#26031;&#26680;&#23494;&#24230;&#20272;&#35745;&#22120;&#36827;&#34892;&#30740;&#31350;&#65292;&#25552;&#20379;&#20102;&#36873;&#25321;&#26368;&#20248;&#26435;&#37325;&#24207;&#21015;&#30340;&#21407;&#21017;&#25351;&#21335;&#65292;&#36890;&#36807;&#23454;&#35777;&#35777;&#25454;&#34920;&#26126;&#35813;&#21152;&#26435;&#26041;&#26696;&#30456;&#27604;&#21551;&#21457;&#24335;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#36319;&#36394;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#23494;&#24230;&#20272;&#35745;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#26159;&#26222;&#36941;&#23384;&#22312;&#30340;&#65292;&#21253;&#25324;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#20449;&#21495;&#22788;&#29702;&#12290;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#19968;&#31181;&#27969;&#34892;&#26041;&#27861;&#26159;&#8220;&#28369;&#21160;&#31383;&#21475;&#8221;&#26680;&#23494;&#24230;&#20272;&#35745;&#22120;&#12290;&#35813;&#26041;&#27861;&#23384;&#22312;&#22810;&#31181;&#19981;&#21516;&#30340;&#23454;&#29616;&#65292;&#36825;&#20123;&#23454;&#29616;&#20351;&#29992;&#21551;&#21457;&#24335;&#23450;&#20041;&#30340;&#21152;&#26435;&#24207;&#21015;&#29992;&#20110;&#35266;&#27979;&#25968;&#25454;&#12290;&#20294;&#26159;&#65292;&#26435;&#37325;&#24207;&#21015;&#26159;&#24433;&#21709;&#20272;&#35745;&#22120;&#36319;&#36394;&#24615;&#33021;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#38024;&#23545;&#19981;&#26029;&#28436;&#21464;&#30340;&#39640;&#26031;&#23494;&#24230;&#30340;&#8220;&#28369;&#21160;&#31383;&#21475;&#8221;&#39640;&#26031;&#26680;&#23494;&#24230;&#20272;&#35745;&#22120;&#30340;&#31934;&#30830;&#22343;&#26041;&#38598;&#25104;&#35823;&#24046;&#65288;MISE&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#29702;&#35770;&#19978;&#34920;&#24449;&#20934;&#30830;MISE&#30340;&#26041;&#24335;&#25552;&#20379;&#20102;&#36873;&#25321;&#26368;&#20248;&#26435;&#37325;&#24207;&#21015;&#30340;&#21407;&#21017;&#25351;&#21335;&#65292;&#23427;&#21487;&#20197;&#34987;&#26500;&#24314;&#20026;&#21463;&#38480;&#20108;&#27425;&#35268;&#21010;&#12290;&#25105;&#20204;&#36890;&#36807;&#21512;&#25104;&#25968;&#25454;&#38598;&#25552;&#20379;&#23454;&#35777;&#35777;&#25454;&#65292;&#34920;&#26126;&#25105;&#20204;&#30340;&#21152;&#26435;&#26041;&#26696;&#30830;&#23454;&#25552;&#39640;&#20102;&#36319;&#36394;&#24615;&#33021;&#65292;&#19982;&#21551;&#21457;&#24335;&#26041;&#27861;&#30456;&#27604;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07207v1 Announce Type: cross  Abstract: Dynamic density estimation is ubiquitous in many applications, including computer vision and signal processing. One popular method to tackle this problem is the "sliding window" kernel density estimator. There exist various implementations of this method that use heuristically defined weight sequences for the observed data. The weight sequence, however, is a key aspect of the estimator affecting the tracking performance significantly. In this work, we study the exact mean integrated squared error (MISE) of "sliding window" Gaussian Kernel Density Estimators for evolving Gaussian densities. We provide a principled guide for choosing the optimal weight sequence by theoretically characterizing the exact MISE, which can be formulated as constrained quadratic programming. We present empirical evidence with synthetic datasets to show that our weighting scheme indeed improves the tracking performance compared to heuristic approaches.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#24320;&#21457;&#20102;&#29992;&#20110;ICU&#30149;&#20154;&#24613;&#24615;&#33041;&#21151;&#33021;&#38556;&#30861;&#39044;&#27979;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#21160;&#24577;&#39044;&#27979;&#35893;&#22916;&#12289;&#26127;&#36855;&#21644;&#27515;&#20129;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#25991;&#29486;&#20013;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2403.07201</link><description>&lt;p&gt;
&#20351;&#29992;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#39044;&#27979;&#24613;&#24615;&#33041;&#21151;&#33021;&#38556;&#30861;&#29366;&#24577;&#30340;&#22810;&#38431;&#21015;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A multi-cohort study on prediction of acute brain dysfunction states using selective state space models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07201
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#24320;&#21457;&#20102;&#29992;&#20110;ICU&#30149;&#20154;&#24613;&#24615;&#33041;&#21151;&#33021;&#38556;&#30861;&#39044;&#27979;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#21160;&#24577;&#39044;&#27979;&#35893;&#22916;&#12289;&#26127;&#36855;&#21644;&#27515;&#20129;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#25991;&#29486;&#20013;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#24613;&#24615;&#33041;&#21151;&#33021;&#38556;&#30861;&#65288;&#21253;&#25324;&#37325;&#30151;&#30417;&#25252;&#23460;&#65288;ICU&#65289;&#20013;&#30340;&#35893;&#22916;&#21644;&#26127;&#36855;&#65289;&#26159;&#19968;&#39033;&#37325;&#35201;&#25361;&#25112;&#65292;&#30446;&#21069;&#30340;&#35786;&#26029;&#26041;&#27861;&#20381;&#36182;&#20110;&#19981;&#32463;&#24120;&#30340;&#20020;&#24202;&#35266;&#23519;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#30005;&#23376;&#20581;&#24247;&#35760;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07201v1 Announce Type: cross  Abstract: Assessing acute brain dysfunction (ABD), including delirium and coma in the intensive care unit (ICU), is a critical challenge due to its prevalence and severe implications for patient outcomes. Current diagnostic methods rely on infrequent clinical observations, which can only determine a patient's ABD status after onset. Our research attempts to solve these problems by harnessing Electronic Health Records (EHR) data to develop automated methods for ABD prediction for patients in the ICU. Existing models solely predict a single state (e.g., either delirium or coma), require at least 24 hours of observation data to make predictions, do not dynamically predict fluctuating ABD conditions during ICU stay (typically a one-time prediction), and use small sample size, proprietary single-hospital datasets. Our research fills these gaps in the existing literature by dynamically predicting delirium, coma, and mortality for 12-hour intervals thr
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#23646;&#24615;&#36873;&#25321;&#21644;&#38598;&#25104;&#26041;&#27861;&#65292;&#32467;&#21512;&#19981;&#21516;&#22810;&#27169;&#24577;&#25968;&#25454;&#28304;&#65292;&#21487;&#20197;&#25913;&#36827;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#20013;&#23545;&#23398;&#29983;&#34920;&#29616;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.07194</link><description>&lt;p&gt;
&#20351;&#29992;&#23646;&#24615;&#36873;&#25321;&#21644;&#19981;&#21516;&#22810;&#27169;&#24577;&#25968;&#25454;&#28304;&#30340;&#38598;&#25104;&#25913;&#36827;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#20013;&#39044;&#27979;&#23398;&#29983;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Improving prediction of students' performance in intelligent tutoring systems using attribute selection and ensembles of different multimodal data sources
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07194
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#23646;&#24615;&#36873;&#25321;&#21644;&#38598;&#25104;&#26041;&#27861;&#65292;&#32467;&#21512;&#19981;&#21516;&#22810;&#27169;&#24577;&#25968;&#25454;&#28304;&#65292;&#21487;&#20197;&#25913;&#36827;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#20013;&#23545;&#23398;&#29983;&#34920;&#29616;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#26469;&#39044;&#27979;&#22823;&#23398;&#29983;&#30340;&#23398;&#20064;&#34920;&#29616;&#65292;&#20351;&#29992;&#20102;&#19981;&#21516;&#26469;&#28304;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#20174;40&#21517;&#23398;&#29983;&#37319;&#38598;&#21644;&#39044;&#22788;&#29702;&#20102;&#26469;&#33258;&#19981;&#21516;&#22810;&#27169;&#24577;&#26469;&#28304;&#30340;&#25968;&#25454;&#65306;&#31995;&#32479;&#26085;&#24535;&#20013;&#30340;&#23398;&#20064;&#31574;&#30053;&#65292;&#38754;&#37096;&#24405;&#20687;&#20013;&#30340;&#24773;&#32490;&#65292;&#30524;&#21160;&#36861;&#36394;&#20013;&#30340;&#20132;&#20114;&#21306;&#22495;&#65292;&#20197;&#21450;&#26368;&#32456;&#30693;&#35782;&#35780;&#20272;&#30340;&#27979;&#35797;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#20351;&#29992;&#23646;&#24615;&#36873;&#25321;&#21644;&#20998;&#31867;&#38598;&#25104;&#26469;&#27979;&#35797;&#26159;&#21542;&#21487;&#20197;&#25913;&#36827;&#39044;&#27979;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#20845;&#31181;&#20998;&#31867;&#31639;&#27861;&#24212;&#29992;&#20110;&#25968;&#20540;&#21270;&#21644;&#31163;&#25955;&#21270;&#39044;&#22788;&#29702;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#36827;&#34892;&#20102;&#19977;&#20010;&#23454;&#39564;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#38598;&#25104;&#21644;&#36873;&#25321;&#26368;&#20339;&#23646;&#24615;&#30340;&#26041;&#27861;&#32467;&#21512;&#25968;&#20540;&#25968;&#25454;&#26102;&#21487;&#20197;&#24471;&#21040;&#26368;&#20339;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07194v1 Announce Type: cross  Abstract: The aim of this study was to predict university students' learning performance using different sources of data from an Intelligent Tutoring System. We collected and preprocessed data from 40 students from different multimodal sources: learning strategies from system logs, emotions from face recording videos, interaction zones from eye tracking, and test performance from final knowledge evaluation. Our objective was to test whether the prediction could be improved by using attribute selection and classification ensembles. We carried out three experiments by applying six classification algorithms to numerical and discretized preprocessed multimodal data. The results show that the best predictions were produced using ensembles and selecting the best attributes approach with numerical data.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;$(N,K)$-Puzzle&#27979;&#35797;&#24179;&#21488;&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.07191</link><description>&lt;p&gt;
$\mathbf{(N,K)}$-Puzzle&#65306;&#19968;&#31181;&#29992;&#20110;&#22522;&#20934;&#27979;&#35797;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#20013;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#25104;&#26412;&#25928;&#30410;&#27979;&#35797;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
$\mathbf{(N,K)}$-Puzzle: A Cost-Efficient Testbed for Benchmarking Reinforcement Learning Algorithms in Generative Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07191
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;$(N,K)$-Puzzle&#27979;&#35797;&#24179;&#21488;&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#30340;&#26368;&#26032;&#36827;&#23637;&#26088;&#22312;&#25552;&#39640;&#35268;&#27169;&#21270;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290; &#28982;&#32780;&#65292;&#32570;&#20047;&#19968;&#31181;&#25104;&#26412;&#25928;&#30410;&#19988;&#26631;&#20934;&#21270;&#30340;&#27979;&#35797;&#24179;&#21488;&#65292;&#19987;&#38376;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#36825;&#20123;&#31639;&#27861;&#12290; &#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;24-Puzzle&#30340;&#19968;&#33324;&#21270;&#29256;&#26412;&#65306;$(N, K)$-Puzzle&#65292;&#25361;&#25112;&#35821;&#35328;&#27169;&#22411;&#20197;&#20351;&#29992;$N$&#20010;&#25972;&#25968;&#36798;&#21040;&#30446;&#26631;&#20540;$K$&#12290; &#25105;&#20204;&#35780;&#20272;&#20102;&#24050;&#24314;&#31435;&#30340;RL&#31639;&#27861;&#65288;&#22914;Proximal Policy Optimization&#65288;PPO&#65289;&#65289;&#65292;&#20197;&#21450;&#26032;&#39062;&#26041;&#27861;&#65288;&#22914;Identity Policy Optimization&#65288;IPO&#65289;&#21644;Direct Policy Optimization&#65288;DPO&#65289;&#65289;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07191v1 Announce Type: cross  Abstract: Recent advances in reinforcement learning (RL) algorithms aim to enhance the performance of language models at scale. Yet, there is a noticeable absence of a cost-effective and standardized testbed tailored to evaluating and comparing these algorithms. To bridge this gap, we present a generalized version of the 24-Puzzle: the $(N,K)$-Puzzle, which challenges language models to reach a target value $K$ with $N$ integers. We evaluate the effectiveness of established RL algorithms such as Proximal Policy Optimization (PPO), alongside novel approaches like Identity Policy Optimization (IPO) and Direct Policy Optimization (DPO).
&lt;/p&gt;</description></item><item><title>UPS&#36890;&#36807;&#36328;&#27169;&#24577;&#36866;&#24212;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#65292;&#23558;&#19981;&#21516;PDE&#32479;&#19968;&#21040;&#19968;&#33268;&#30340;&#34920;&#31034;&#31354;&#38388;&#65292;&#24182;&#22312;&#23569;&#26679;&#26412;&#19979;&#36798;&#21040;&#20102;&#24378;&#26377;&#21147;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#65292;&#23454;&#29616;&#20102;1D&#21644;2D&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.07187</link><description>&lt;p&gt;
UPS: &#36890;&#36807;&#36328;&#27169;&#24577;&#36866;&#24212;&#23454;&#29616;&#20559;&#24494;&#20998;&#26041;&#31243;&#27714;&#35299;&#30340;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
UPS: Towards Foundation Models for PDE Solving via Cross-Modal Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07187
&lt;/p&gt;
&lt;p&gt;
UPS&#36890;&#36807;&#36328;&#27169;&#24577;&#36866;&#24212;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#65292;&#23558;&#19981;&#21516;PDE&#32479;&#19968;&#21040;&#19968;&#33268;&#30340;&#34920;&#31034;&#31354;&#38388;&#65292;&#24182;&#22312;&#23569;&#26679;&#26412;&#19979;&#36798;&#21040;&#20102;&#24378;&#26377;&#21147;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#65292;&#23454;&#29616;&#20102;1D&#21644;2D&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;UPS&#65288;&#32479;&#19968;PDE&#27714;&#35299;&#22120;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#25968;&#25454;&#39640;&#25928;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#19981;&#21516;&#22495;&#12289;&#32500;&#24230;&#21644;&#20998;&#36776;&#29575;&#19978;&#23450;&#20041;&#30340;&#21508;&#31181;&#26102;&#31354;PDE&#12290;UPS&#23558;&#19981;&#21516;&#30340;PDE&#32479;&#19968;&#21040;&#19968;&#33268;&#30340;&#34920;&#31034;&#31354;&#38388;&#20013;&#65292;&#24182;&#20351;&#29992;&#23558;LLMs&#19982;&#29305;&#23450;&#22495;&#31070;&#32463;&#31639;&#23376;&#30456;&#32467;&#21512;&#30340;&#32479;&#19968;&#32593;&#32476;&#26550;&#26500;&#22788;&#29702;&#21508;&#31181;PDE&#25968;&#25454;&#38598;&#21512;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#38454;&#27573;&#30340;&#36328;&#27169;&#24577;&#36866;&#24212;&#36807;&#31243;&#35757;&#32451;&#32593;&#32476;&#65292;&#21033;&#29992;&#27169;&#24577;&#23545;&#40784;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#24605;&#24819;&#12290;&#36890;&#36807;&#20174;&#39044;&#35757;&#32451;&#30340;LLMs&#36827;&#34892;&#35843;&#25972;&#24182;&#21033;&#29992;&#25991;&#26412;&#24418;&#24335;&#30340;&#20803;&#20449;&#24687;&#65292;&#25105;&#20204;&#33021;&#22815;&#20351;&#29992;&#27604;&#20197;&#21069;&#30340;&#26041;&#27861;&#23569;&#24471;&#22810;&#30340;&#35757;&#32451;&#26679;&#26412;&#65292;&#24182;&#33719;&#24471;&#24378;&#26377;&#21147;&#30340;&#23454;&#35777;&#32467;&#26524;&#12290;UPS&#22312;PDEBench&#30340;&#24191;&#27867;1D&#21644;2D&#25968;&#25454;&#38598;&#19978;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#65292;&#23545;&#32771;&#34385;&#30340;10&#20010;&#20219;&#21153;&#20013;&#30340;8&#20010;&#20219;&#21153;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#23427;&#33021;&#22815;&#23569;&#26679;&#26412;&#24555;&#36895;&#36716;&#31227;&#33267;&#19981;&#21516;&#30340;PDE&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07187v1 Announce Type: new  Abstract: We introduce UPS (Unified PDE Solver), an effective and data-efficient approach to solve diverse spatiotemporal PDEs defined over various domains, dimensions, and resolutions. UPS unifies different PDEs into a consistent representation space and processes diverse collections of PDE data using a unified network architecture that combines LLMs with domain-specific neural operators. We train the network via a two-stage cross-modal adaptation process, leveraging ideas of modality alignment and multi-task learning. By adapting from pretrained LLMs and exploiting text-form meta information, we are able to use considerably fewer training samples than previous methods while obtaining strong empirical results. UPS outperforms existing baselines, often by a large margin, on a wide range of 1D and 2D datasets in PDEBench, achieving state-of-the-art results on 8 of 10 tasks considered. Meanwhile, it is capable of few-shot transfer to different PDE f
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#26597;&#26088;&#22312;&#20840;&#38754;&#27010;&#36848;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#20851;&#20110;&#22914;&#20309;&#35782;&#21035;&#12289;&#37327;&#21270;&#21644;&#21033;&#29992;&#19981;&#30830;&#23450;&#24615;&#26469;&#22686;&#24378;&#27169;&#22411;&#24615;&#33021;&#21644; GNN &#39044;&#27979;&#21487;&#38752;&#24615;&#30340;&#35266;&#28857;&#12290;</title><link>https://arxiv.org/abs/2403.07185</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Uncertainty in Graph Neural Networks: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07185
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#26088;&#22312;&#20840;&#38754;&#27010;&#36848;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#20851;&#20110;&#22914;&#20309;&#35782;&#21035;&#12289;&#37327;&#21270;&#21644;&#21033;&#29992;&#19981;&#30830;&#23450;&#24615;&#26469;&#22686;&#24378;&#27169;&#22411;&#24615;&#33021;&#21644; GNN &#39044;&#27979;&#21487;&#38752;&#24615;&#30340;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#12290;&#28982;&#32780;&#65292;GNNs&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#28304;&#33258;&#25968;&#25454;&#20013;&#30340;&#22266;&#26377;&#38543;&#26426;&#24615;&#21644;&#27169;&#22411;&#35757;&#32451;&#35823;&#24046;&#31561;&#22810;&#31181;&#22240;&#32032;&#65292;&#21487;&#33021;&#23548;&#33268;&#19981;&#31283;&#23450;&#21644;&#38169;&#35823;&#30340;&#39044;&#27979;&#12290;&#22240;&#27492;&#65292;&#35782;&#21035;&#12289;&#37327;&#21270;&#21644;&#21033;&#29992;&#19981;&#30830;&#23450;&#24615;&#23545;&#20110;&#22686;&#24378;&#27169;&#22411;&#24615;&#33021;&#21644;GNN&#39044;&#27979;&#30340;&#21487;&#38752;&#24615;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#26412;&#35843;&#26597;&#26088;&#22312;&#20174;&#19981;&#30830;&#23450;&#24615;&#30340;&#35282;&#24230;&#20840;&#38754;&#27010;&#36848;GNNs&#65292;&#24182;&#24378;&#35843;&#20854;&#22312;&#22270;&#23398;&#20064;&#20013;&#30340;&#25972;&#21512;&#12290;&#25105;&#20204;&#27604;&#36739;&#21644;&#24635;&#32467;&#20102;&#29616;&#26377;&#30340;&#22270;&#19981;&#30830;&#23450;&#24615;&#29702;&#35770;&#21644;&#26041;&#27861;&#65292;&#20197;&#21450;&#30456;&#24212;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#24357;&#21512;&#20102;&#29702;&#35770;&#19982;&#23454;&#36341;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#21516;&#26102;&#36830;&#25509;&#19981;&#21516;&#30340;GNN&#31038;&#21306;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;&#36825;&#19968;&#39046;&#22495;&#30340;&#26410;&#26469;&#26041;&#21521;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07185v1 Announce Type: new  Abstract: Graph Neural Networks (GNNs) have been extensively used in various real-world applications. However, the predictive uncertainty of GNNs stemming from diverse sources such as inherent randomness in data and model training errors can lead to unstable and erroneous predictions. Therefore, identifying, quantifying, and utilizing uncertainty are essential to enhance the performance of the model for the downstream tasks as well as the reliability of the GNN predictions. This survey aims to provide a comprehensive overview of the GNNs from the perspective of uncertainty with an emphasis on its integration in graph learning. We compare and summarize existing graph uncertainty theory and methods, alongside the corresponding downstream tasks. Thereby, we bridge the gap between theory and practice, meanwhile connecting different GNN communities. Moreover, our work provides valuable insights into promising directions in this field.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20272;&#35745;&#22823;&#35821;&#26009;&#24211;&#20013;&#34987;&#22823;&#35821;&#35328;&#27169;&#22411;&#22823;&#24133;&#20462;&#25913;&#30340;&#25991;&#26412;&#27604;&#20363;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;AI&#20250;&#35758;&#30340;&#21516;&#34892;&#35780;&#23457;&#20013;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#65292;&#21457;&#29616;6.5%&#33267;16.9%&#30340;&#25991;&#26412;&#21487;&#33021;&#34987;LLMs&#22823;&#24133;&#20462;&#25913;&#65292;&#25581;&#31034;&#20102;&#29992;&#25143;&#34892;&#20026;&#30340;&#19968;&#20123;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.07183</link><description>&lt;p&gt;
&#22312;&#35268;&#27169;&#19978;&#30417;&#27979;AI&#20462;&#25913;&#30340;&#20869;&#23481;&#65306;AI&#20250;&#35758;&#21516;&#34892;&#35780;&#23457;&#20013;ChatGPT&#24433;&#21709;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Monitoring AI-Modified Content at Scale: A Case Study on the Impact of ChatGPT on AI Conference Peer Reviews
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07183
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20272;&#35745;&#22823;&#35821;&#26009;&#24211;&#20013;&#34987;&#22823;&#35821;&#35328;&#27169;&#22411;&#22823;&#24133;&#20462;&#25913;&#30340;&#25991;&#26412;&#27604;&#20363;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;AI&#20250;&#35758;&#30340;&#21516;&#34892;&#35780;&#23457;&#20013;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#65292;&#21457;&#29616;6.5%&#33267;16.9%&#30340;&#25991;&#26412;&#21487;&#33021;&#34987;LLMs&#22823;&#24133;&#20462;&#25913;&#65292;&#25581;&#31034;&#20102;&#29992;&#25143;&#34892;&#20026;&#30340;&#19968;&#20123;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20272;&#35745;&#22823;&#35821;&#26009;&#24211;&#20013;&#25991;&#26412;&#21487;&#33021;&#34987;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22823;&#24133;&#20462;&#25913;&#25110;&#29983;&#25104;&#30340;&#37096;&#20998;&#27604;&#20363;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26368;&#22823;&#20284;&#28982;&#27169;&#22411;&#21033;&#29992;&#19987;&#23478;&#25776;&#20889;&#21644;AI&#29983;&#25104;&#30340;&#21442;&#32771;&#25991;&#26412;&#65292;&#20934;&#30830;&#39640;&#25928;&#22320;&#26816;&#26597;&#35821;&#26009;&#24211;&#32423;&#21035;&#19978;&#30495;&#23454;&#19990;&#30028;LLM&#20351;&#29992;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;AI&#20250;&#35758;&#19978;&#31185;&#23398;&#21516;&#34892;&#35780;&#23457;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#35813;&#30740;&#31350;&#21457;&#29983;&#22312;ChatGPT&#21457;&#24067;&#20043;&#21518;&#65292;&#21253;&#25324;ICLR 2024&#12289;NeurIPS 2023&#12289;CoRL 2023&#21644;EMNLP 2023&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#36825;&#20123;&#20250;&#35758;&#25552;&#20132;&#30340;&#21516;&#34892;&#35780;&#23457;&#20013;&#65292;6.5%&#33267;16.9%&#30340;&#25991;&#26412;&#21487;&#33021;&#26159;&#30001;LLMs&#22823;&#24133;&#20462;&#25913;&#30340;&#65292;&#21363;&#36229;&#20986;&#25340;&#20889;&#26816;&#26597;&#25110;&#23567;&#24133;&#26356;&#26032;&#30340;&#33539;&#22260;&#12290;&#29983;&#25104;&#25991;&#26412;&#20986;&#29616;&#30340;&#24773;&#20917;&#20026;&#29992;&#25143;&#34892;&#20026;&#25552;&#20379;&#20102;&#35265;&#35299;&#65306;&#22312;&#25253;&#21578;&#20449;&#24515;&#36739;&#20302;&#12289;&#22312;&#25130;&#27490;&#26085;&#26399;&#21069;&#25552;&#20132;&#30340;&#35780;&#35770;&#20197;&#21450;&#20174;&#35780;&#35770;&#20844;&#21496;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07183v1 Announce Type: cross  Abstract: We present an approach for estimating the fraction of text in a large corpus which is likely to be substantially modified or produced by a large language model (LLM). Our maximum likelihood model leverages expert-written and AI-generated reference texts to accurately and efficiently examine real-world LLM-use at the corpus level. We apply this approach to a case study of scientific peer review in AI conferences that took place after the release of ChatGPT: ICLR 2024, NeurIPS 2023, CoRL 2023 and EMNLP 2023. Our results suggest that between 6.5% and 16.9% of text submitted as peer reviews to these conferences could have been substantially modified by LLMs, i.e. beyond spell-checking or minor writing updates. The circumstances in which generated text occurs offer insight into user behavior: the estimated fraction of LLM-generated text is higher in reviews which report lower confidence, were submitted close to the deadline, and from review
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;3M-Diffusion&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#20998;&#23376;&#22270;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#25152;&#38656;&#23646;&#24615;&#30340;&#22810;&#26679;&#21270;&#12289;&#29702;&#24819;&#24773;&#20917;&#19979;&#26159;&#26032;&#39062;&#30340;&#20998;&#23376;&#12290;</title><link>https://arxiv.org/abs/2403.07179</link><description>&lt;p&gt;
3M-Diffusion&#65306;&#29992;&#20110;&#25991;&#26412;&#24341;&#23548;&#29983;&#25104;&#20998;&#23376;&#22270;&#30340;&#28508;&#22312;&#22810;&#27169;&#24577;&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
3M-Diffusion: Latent Multi-Modal Diffusion for Text-Guided Generation of Molecular Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07179
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;3M-Diffusion&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#20998;&#23376;&#22270;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#25152;&#38656;&#23646;&#24615;&#30340;&#22810;&#26679;&#21270;&#12289;&#29702;&#24819;&#24773;&#20917;&#19979;&#26159;&#26032;&#39062;&#30340;&#20998;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#20855;&#26377;&#25152;&#38656;&#23646;&#24615;&#30340;&#20998;&#23376;&#26159;&#19968;&#39033;&#20851;&#38190;&#20219;&#21153;&#65292;&#22312;&#33647;&#29289;&#21457;&#29616;&#21644;&#26448;&#26009;&#35774;&#35745;&#20013;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#12290;&#21463;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#23545;&#20351;&#29992;&#20998;&#23376;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#26469;&#29983;&#25104;&#20855;&#26377;&#25152;&#38656;&#23646;&#24615;&#30340;&#20998;&#23376;&#20135;&#29983;&#20102;&#20852;&#36259;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20391;&#37325;&#20110;&#29983;&#25104;&#19982;&#25991;&#26412;&#25551;&#36848;&#31934;&#30830;&#21305;&#37197;&#30340;&#20998;&#23376;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#24212;&#29992;&#38656;&#35201;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#25152;&#38656;&#23646;&#24615;&#30340;&#22810;&#26679;&#21270;&#65292;&#29702;&#24819;&#24773;&#20917;&#19979;&#26159;&#26032;&#39062;&#30340;&#20998;&#23376;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#20998;&#23376;&#22270;&#29983;&#25104;&#26041;&#27861;3M-Diffusion&#65292;&#20197;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07179v1 Announce Type: cross  Abstract: Generating molecules with desired properties is a critical task with broad applications in drug discovery and materials design. Inspired by recent advances in large language models, there is a growing interest in using natural language descriptions of molecules to generate molecules with the desired properties. Most existing methods focus on generating molecules that precisely match the text description. However, practical applications call for methods that generate diverse, and ideally novel, molecules with the desired properties. We propose 3M-Diffusion, a novel multi-modal molecular graph generation method, to address this challenge. 3M-Diffusion first encodes molecular graphs into a graph latent space aligned with text descriptions. It then reconstructs the molecular structure and atomic attributes based on the given text descriptions using the molecule decoder. It then learns a probabilistic mapping from the text space to the late
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21382;&#21490;&#24863;&#30693;&#30340;&#21338;&#24328;&#29702;&#35770;&#26694;&#26550;FLContrib&#65292;&#29992;&#26469;&#35780;&#20272;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#23458;&#25143;&#36129;&#29486;&#12290;</title><link>https://arxiv.org/abs/2403.07151</link><description>&lt;p&gt;
&#19981;&#35201;&#24536;&#35760;&#25105;&#20570;&#30340;&#20107;&#65306;&#35780;&#20272;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#23458;&#25143;&#36129;&#29486;
&lt;/p&gt;
&lt;p&gt;
Don't Forget What I did?: Assessing Client Contributions in Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07151
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21382;&#21490;&#24863;&#30693;&#30340;&#21338;&#24328;&#29702;&#35770;&#26694;&#26550;FLContrib&#65292;&#29992;&#26469;&#35780;&#20272;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#23458;&#25143;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#21327;&#20316;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#65292;&#22810;&#20010;&#23458;&#25143;&#21442;&#19982;&#35757;&#32451;ML&#27169;&#22411;&#65292;&#32780;&#19981;&#26292;&#38706;&#31169;&#20154;&#25968;&#25454;&#12290;&#20844;&#24179;&#20934;&#30830;&#35780;&#20272;&#23458;&#25143;&#36129;&#29486;&#22312;FL&#20013;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65292;&#20197;&#20419;&#36827;&#28608;&#21169;&#20998;&#37197;&#24182;&#40723;&#21169;&#22810;&#26679;&#21270;&#23458;&#25143;&#21442;&#19982;&#32479;&#19968;&#27169;&#22411;&#35757;&#32451;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21382;&#21490;&#24863;&#30693;&#30340;&#21338;&#24328;&#29702;&#35770;&#26694;&#26550;FLContrib&#65292;&#29992;&#20110;&#35780;&#20272;&#22312;&#27599;&#20010;FL&#35757;&#32451;&#26102;&#26399;&#20013;&#30340;&#65288;&#28508;&#22312;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#65289;&#23458;&#25143;&#21442;&#19982;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07151v1 Announce Type: cross  Abstract: Federated Learning (FL) is a collaborative machine learning (ML) approach, where multiple clients participate in training an ML model without exposing the private data. Fair and accurate assessment of client contributions is an important problem in FL to facilitate incentive allocation and encouraging diverse clients to participate in a unified model training. Existing methods for assessing client contribution adopts co-operative game-theoretic concepts, such as Shapley values, but under simplified assumptions. In this paper, we propose a history-aware game-theoretic framework, called FLContrib, to assess client contributions when a subset of (potentially non-i.i.d.) clients participate in each epoch of FL training. By exploiting the FL training process and linearity of Shapley value, we develop FLContrib that yields a historical timeline of client contributions as FL training progresses over epochs. Additionally, to assess client cont
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#38024;&#23545;&#19977;&#31867;&#21464;&#20998;&#19981;&#31561;&#24335;&#38382;&#39064;&#25552;&#20986;&#20102;&#20855;&#26377;&#38543;&#26426;&#37325;&#25490;&#30340;&#38543;&#26426;&#22806;&#25512;&#27861;&#65288;SEG-RR&#65289;&#65292;&#24182;&#35777;&#26126;&#20854;&#22312;&#21333;&#35843;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#27604;&#22343;&#21248;&#26367;&#25442;&#37319;&#26679;SEG&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.07148</link><description>&lt;p&gt;
&#20855;&#26377;&#38543;&#26426;&#37325;&#25490;&#30340;&#38543;&#26426;&#22806;&#25512;&#27861;&#65306;&#25913;&#36827;&#21464;&#20998;&#19981;&#31561;&#24335;&#30340;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
Stochastic Extragradient with Random Reshuffling: Improved Convergence for Variational Inequalities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07148
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#38024;&#23545;&#19977;&#31867;&#21464;&#20998;&#19981;&#31561;&#24335;&#38382;&#39064;&#25552;&#20986;&#20102;&#20855;&#26377;&#38543;&#26426;&#37325;&#25490;&#30340;&#38543;&#26426;&#22806;&#25512;&#27861;&#65288;SEG-RR&#65289;&#65292;&#24182;&#35777;&#26126;&#20854;&#22312;&#21333;&#35843;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#27604;&#22343;&#21248;&#26367;&#25442;&#37319;&#26679;SEG&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#22806;&#25512;&#27861;&#65288;SEG&#65289;&#26041;&#27861;&#26159;&#35299;&#20915;&#20986;&#29616;&#22312;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#26377;&#38480;&#27714;&#21644;&#26497;&#23567;-&#26497;&#22823;&#20248;&#21270;&#21644;&#21464;&#20998;&#19981;&#31561;&#24335;&#38382;&#39064;&#65288;VIPs&#65289;&#30340;&#26368;&#27969;&#34892;&#31639;&#27861;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;SEG&#25910;&#25947;&#20998;&#26512;&#19987;&#27880;&#20110;&#20854;&#24102;&#26367;&#25442;&#21464;&#20307;&#65292;&#32780;&#26041;&#27861;&#30340;&#23454;&#38469;&#23454;&#29616;&#20250;&#38543;&#26426;&#37325;&#26032;&#25490;&#21015;&#20998;&#37327;&#24182;&#25353;&#39034;&#24207;&#20351;&#29992;&#23427;&#20204;&#12290;&#19982;&#24191;&#20026;&#30740;&#31350;&#30340;&#24102;&#26367;&#25442;&#21464;&#20307;&#19981;&#21516;&#65292;&#20855;&#26377;&#38543;&#26426;&#37325;&#25490;&#30340;SEG&#65288;SEG-RR&#65289;&#32570;&#20047;&#24050;&#24314;&#31435;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#38024;&#23545;&#19977;&#31867;VIPs&#65288;i&#65289;&#24378;&#21333;&#35843;&#65292;&#65288;ii&#65289;&#20223;&#23556;&#21644;&#65288;iii&#65289;&#21333;&#35843;&#25552;&#20379;&#20102;SEG-RR&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;SEG-RR&#23454;&#29616;&#27604;&#22343;&#21248;&#24102;&#26367;&#25442;&#37319;&#26679;SEG&#20855;&#26377;&#26356;&#24555;&#25910;&#25947;&#36895;&#24230;&#30340;&#26465;&#20214;&#12290;&#22312;&#21333;&#35843;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#30340;SEG-RR&#20998;&#26512;&#20445;&#35777;&#20102;&#25910;&#25947;&#21040;&#20219;&#24847;&#31934;&#24230;&#32780;&#26080;&#38656;&#22823;&#25209;&#37327;&#22823;&#23567;&#65292;&#36825;&#26159;&#23545;&#22823;&#25209;&#37327;&#22823;&#23567;&#32780;&#35328;&#30340;&#24378;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07148v1 Announce Type: cross  Abstract: The Stochastic Extragradient (SEG) method is one of the most popular algorithms for solving finite-sum min-max optimization and variational inequality problems (VIPs) appearing in various machine learning tasks. However, existing convergence analyses of SEG focus on its with-replacement variants, while practical implementations of the method randomly reshuffle components and sequentially use them. Unlike the well-studied with-replacement variants, SEG with Random Reshuffling (SEG-RR) lacks established theoretical guarantees. In this work, we provide a convergence analysis of SEG-RR for three classes of VIPs: (i) strongly monotone, (ii) affine, and (iii) monotone. We derive conditions under which SEG-RR achieves a faster convergence rate than the uniform with-replacement sampling SEG. In the monotone setting, our analysis of SEG-RR guarantees convergence to an arbitrary accuracy without large batch sizes, a strong requirement needed in 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20174;&#22312;&#32447;&#23398;&#20064;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;&#37325;&#22797;&#30340;&#22996;&#25176;-&#20195;&#29702;&#38382;&#39064;&#65292;&#38024;&#23545;&#19981;&#21516;&#24773;&#24418;&#25552;&#20986;&#20102;&#35774;&#35745;&#23398;&#20064;&#31639;&#27861;&#30340;&#19981;&#21516;&#26041;&#27861;&#21644;&#25216;&#26415;&#65292;&#21253;&#25324;&#24322;&#36136;&#20195;&#29702;&#12289;&#21516;&#36136;&#20195;&#29702;&#21644;&#38750;&#21333;&#32431;&#35270;&#35282;&#20195;&#29702;&#12290;</title><link>https://arxiv.org/abs/2403.07143</link><description>&lt;p&gt;
&#22312;&#32447;&#21512;&#21516;&#35774;&#35745;&#30340;&#26032;&#35270;&#35282;&#65306;&#24322;&#36136;&#12289;&#21516;&#36136;&#12289;&#38750;&#21333;&#32431;&#35270;&#35282;&#20195;&#29702;&#21644;&#22242;&#38431;&#29983;&#20135;
&lt;/p&gt;
&lt;p&gt;
New Perspectives in Online Contract Design: Heterogeneous, Homogeneous, Non-myopic Agents and Team Production
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07143
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#22312;&#32447;&#23398;&#20064;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;&#37325;&#22797;&#30340;&#22996;&#25176;-&#20195;&#29702;&#38382;&#39064;&#65292;&#38024;&#23545;&#19981;&#21516;&#24773;&#24418;&#25552;&#20986;&#20102;&#35774;&#35745;&#23398;&#20064;&#31639;&#27861;&#30340;&#19981;&#21516;&#26041;&#27861;&#21644;&#25216;&#26415;&#65292;&#21253;&#25324;&#24322;&#36136;&#20195;&#29702;&#12289;&#21516;&#36136;&#20195;&#29702;&#21644;&#38750;&#21333;&#32431;&#35270;&#35282;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20174;&#22312;&#32447;&#23398;&#20064;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;&#37325;&#22797;&#30340;&#22996;&#25176;-&#20195;&#29702;&#38382;&#39064;&#12290; &#22996;&#25176;&#26041;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#37325;&#22797;&#20114;&#21160;&#23398;&#20064;&#26368;&#22823;&#21270;&#20854;&#25928;&#29992;&#30340;&#26368;&#20339;&#21512;&#21516;&#65292;&#32780;&#27809;&#26377;&#20851;&#20110;&#20195;&#29702;&#26041;&#31867;&#22411;&#65288;&#21363;&#20195;&#29702;&#26041;&#30340;&#25104;&#26412;&#21644;&#29983;&#20135;&#20989;&#25968;&#65289;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290; &#25105;&#30740;&#31350;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#24773;&#22659;&#65292;&#22996;&#25176;&#26041;&#22312;&#27599;&#19968;&#36718;&#19982;$\textit{&#21333;&#20010;}$&#20195;&#29702;&#26041;&#31614;&#35746;&#21512;&#21516;&#26102;&#65306;1. &#20195;&#29702;&#26041;&#26159;&#24322;&#36136;&#30340;&#65307;2. &#20195;&#29702;&#26041;&#26159;&#21516;&#36136;&#30340;&#65307;3. &#22996;&#25176;&#26041;&#19982;&#30456;&#21516;&#30340;&#20195;&#29702;&#26041;&#20114;&#21160;&#19988;&#35813;&#20195;&#29702;&#26041;&#26159;&#38750;&#21333;&#32431;&#30340;&#12290; &#25105;&#25552;&#20986;&#19981;&#21516;&#30340;&#26041;&#27861;&#21644;&#25216;&#26415;&#26469;&#35774;&#35745;&#27599;&#31181;&#24773;&#20917;&#19979;&#30340;&#23398;&#20064;&#31639;&#27861;&#12290; &#23545;&#20110;&#24322;&#36136;&#20195;&#29702;&#31867;&#22411;&#65292;&#25105;&#30830;&#23450;&#20102;&#19968;&#20010;&#26465;&#20214;&#65292;&#20801;&#35768;&#23558;&#38382;&#39064;&#30452;&#25509;&#31616;&#21270;&#20026;Lipschitz&#32769;&#34382;&#26426;&#38382;&#39064;&#12290; &#23545;&#20110;&#30456;&#21516;&#20195;&#29702;&#26041;&#65292;&#25105;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#36870;&#21338;&#24328;&#35770;&#30340;&#22810;&#39033;&#24335;&#26679;&#26412;&#22797;&#26434;&#24230;&#26041;&#26696;&#26469;&#23398;&#20064;&#26368;&#20339;&#21512;&#21516;&#12290; &#23545;&#20110;&#25112;&#30053;&#24615;&#38750;&#21333;&#32431;&#20195;&#29702;&#65292;&#25105;&#35774;&#35745;&#20102;&#19968;&#20010;&#20302;&#25112;&#30053;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07143v1 Announce Type: cross  Abstract: This work studies the repeated principal-agent problem from an online learning perspective. The principal's goal is to learn the optimal contract that maximizes her utility through repeated interactions, without prior knowledge of the agent's type (i.e., the agent's cost and production functions).   I study three different settings when the principal contracts with a $\textit{single}$ agent each round: 1. The agents are heterogeneous; 2. the agents are homogenous; 3. the principal interacts with the same agent and the agent is non-myopic. I present different approaches and techniques for designing learning algorithms in each setting. For heterogeneous agent types, I identify a condition that allows the problem to be reduced to Lipschitz bandits directly. For identical agents, I give a polynomial sample complexity scheme to learn the optimal contract based on inverse game theory. For strategic non-myopic agents, I design a low strategic
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#25968;&#25454;&#38598;&#31934;&#28860;&#30340;&#26032;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#25968;&#25454;&#38598;&#31934;&#28860;&#22312;&#22788;&#29702;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#21644;&#22797;&#26434;&#26550;&#26500;&#26102;&#30340;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.07142</link><description>&lt;p&gt;
&#19968;&#20010;&#31867;&#21035;&#19968;&#20010;&#25552;&#31034;&#65306;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#25968;&#25454;&#38598;&#31934;&#28860;
&lt;/p&gt;
&lt;p&gt;
One Category One Prompt: Dataset Distillation using Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07142
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#25968;&#25454;&#38598;&#31934;&#28860;&#30340;&#26032;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#25968;&#25454;&#38598;&#31934;&#28860;&#22312;&#22788;&#29702;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#21644;&#22797;&#26434;&#26550;&#26500;&#26102;&#30340;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#25152;&#38656;&#30340;&#22823;&#37327;&#25968;&#25454;&#23545;&#23384;&#20648;&#21644;&#20256;&#36755;&#26041;&#38754;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#25968;&#25454;&#38598;&#31934;&#28860;&#24050;&#32463;&#25104;&#20026;&#23558;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#20449;&#24687;&#21387;&#32553;&#25104;&#19968;&#32452;&#20195;&#34920;&#24615;&#21512;&#25104;&#26679;&#26412;&#30340;&#26377;&#21069;&#36884;&#30340;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#25968;&#25454;&#38598;&#31934;&#28860;&#26041;&#27861;&#36890;&#24120;&#22312;&#22788;&#29702;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#21644;&#26356;&#22797;&#26434;&#26550;&#26500;&#26102;&#24456;&#38590;&#26377;&#25928;&#25193;&#23637;&#65292;&#36825;&#26159;&#30001;&#20110;&#21452;&#23618;&#20248;&#21270;&#30340;&#38480;&#21046;&#12290;&#26368;&#36817;&#65292;&#19968;&#20123;&#24037;&#20316;&#25552;&#20986;&#21033;&#29992;&#20998;&#31163;&#30340;&#20248;&#21270;&#26041;&#26696;&#23558;&#30693;&#35782;&#31934;&#28860;&#21644;&#25968;&#25454;&#38598;&#31934;&#28860;&#30456;&#32467;&#21512;&#65292;&#20197;&#25193;&#22823;&#25968;&#25454;&#38598;&#31934;&#28860;&#35268;&#27169;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#20381;&#36182;&#20110;&#24191;&#27867;&#30340;&#22270;&#20687;&#22686;&#24378;&#65292;&#38656;&#35201;&#23384;&#20648;&#22686;&#24378;&#22270;&#20687;&#30340;&#36719;&#26631;&#31614;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#25968;&#25454;&#38598;&#31934;&#28860;&#65288;D3M&#65289;&#20316;&#20026;&#25968;&#25454;&#38598;&#31934;&#28860;&#30340;&#26032;&#33539;&#24335;&#65292;&#21033;&#29992;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07142v1 Announce Type: cross  Abstract: The extensive amounts of data required for training deep neural networks pose significant challenges on storage and transmission fronts. Dataset distillation has emerged as a promising technique to condense the information of massive datasets into a much smaller yet representative set of synthetic samples. However, traditional dataset distillation approaches often struggle to scale effectively with high-resolution images and more complex architectures due to the limitations in bi-level optimization. Recently, several works have proposed exploiting knowledge distillation with decoupled optimization schemes to scale up dataset distillation. Although these methods effectively address the scalability issue, they rely on extensive image augmentations requiring the storage of soft labels for augmented images. In this paper, we introduce Dataset Distillation using Diffusion Models (D3M) as a novel paradigm for dataset distillation, leveraging
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23612;&#32599;&#23572;&#29275;&#35270;&#35273;&#35780;&#20998;&#19982;&#27979;&#37327;&#20540;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#25552;&#20986;&#20351;&#29992;k&#22343;&#20540;&#31639;&#27861;&#23545;&#29275;&#30340;&#19968;&#25209;&#36827;&#34892;&#32858;&#31867;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.07137</link><description>&lt;p&gt;
&#25506;&#35752;&#23612;&#32599;&#23572;&#29275;&#35270;&#35273;&#35780;&#20998;&#24402;&#22240;&#20013;&#30340;&#32858;&#31867;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Exploring Cluster Analysis in Nelore Cattle Visual Score Attribution
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07137
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23612;&#32599;&#23572;&#29275;&#35270;&#35273;&#35780;&#20998;&#19982;&#27979;&#37327;&#20540;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#25552;&#20986;&#20351;&#29992;k&#22343;&#20540;&#31639;&#27861;&#23545;&#29275;&#30340;&#19968;&#25209;&#36827;&#34892;&#32858;&#31867;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20154;&#31867;&#35270;&#35273;&#26816;&#26597;&#35780;&#20272;&#29275;&#30340;&#29983;&#29289;&#31867;&#22411;&#26159;&#31934;&#20934;&#32946;&#31181;&#20013;&#38750;&#24120;&#24120;&#35265;&#21644;&#37325;&#35201;&#30340;&#23454;&#36341;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#23545;&#23612;&#32599;&#23572;&#29275;&#30340;&#35780;&#20998;&#19982;&#20174;&#22270;&#20687;&#25110;&#20854;&#20182;&#20202;&#22120;&#20013;&#21487;&#20197;&#24471;&#20986;&#30340;&#21508;&#31181;&#27979;&#37327;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#20998;&#26512;&#32467;&#26524;&#12290;&#23427;&#36824;&#23637;&#31034;&#20102;&#20351;&#29992;k&#22343;&#20540;&#31639;&#27861;&#36827;&#34892;&#30740;&#31350;&#65292;&#20197;&#29983;&#25104;&#21033;&#29992;&#19982;&#21160;&#29289;&#20307;&#37325;&#21644;&#35270;&#35273;&#35780;&#20998;&#26368;&#30456;&#20851;&#30340;&#27979;&#37327;&#23545;&#19968;&#25209;&#29275;&#36827;&#34892;&#32858;&#31867;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07137v1 Announce Type: cross  Abstract: Assessing the biotype of cattle through human visual inspection is a very common and important practice in precision cattle breeding. This paper presents the results of a correlation analysis between scores produced by humans for Nelore cattle and a variety of measurements that can be derived from images or other instruments. It also presents a study using the k-means algorithm to generate new ways of clustering a batch of cattle using the measurements that most correlate with the animal's body weight and visual scores.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#22522;&#20110;&#20540;&#20989;&#25968;&#30340;&#34920;&#24449;&#33021;&#21147;&#26377;&#38480;&#65292;&#23548;&#33268;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#22522;&#20110;&#20540;&#20989;&#25968;&#30340;&#26041;&#27861;&#22312;&#32479;&#35745;&#19978;&#20302;&#25928;&#29575;&#65292;&#36825;&#25581;&#31034;&#20102;&#20215;&#20540;&#20989;&#25968;&#21644;&#32479;&#35745;&#25928;&#29575;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;</title><link>https://arxiv.org/abs/2403.07136</link><description>&lt;p&gt;
&#20215;&#20540;&#20989;&#25968;&#30340;&#26377;&#38480;&#34920;&#24449;&#33021;&#21147;&#21450;&#20854;&#19982;&#32479;&#35745;(&#19981;)&#25928;&#29575;&#30340;&#20851;&#32852;
&lt;/p&gt;
&lt;p&gt;
On the Limited Representational Power of Value Functions and its Links to Statistical (In)Efficiency
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07136
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#22522;&#20110;&#20540;&#20989;&#25968;&#30340;&#34920;&#24449;&#33021;&#21147;&#26377;&#38480;&#65292;&#23548;&#33268;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#22522;&#20110;&#20540;&#20989;&#25968;&#30340;&#26041;&#27861;&#22312;&#32479;&#35745;&#19978;&#20302;&#25928;&#29575;&#65292;&#36825;&#25581;&#31034;&#20102;&#20215;&#20540;&#20989;&#25968;&#21644;&#32479;&#35745;&#25928;&#29575;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#35782;&#21035;&#22522;&#20110;&#27169;&#22411;&#21644;&#26080;&#27169;&#22411;&#26041;&#27861;&#20043;&#38388;&#30340;&#26435;&#34913;&#26159;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#12290; &#22522;&#20110;&#20540;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#35745;&#31639;&#20248;&#21183;&#65292;&#24182;&#19988;&#26377;&#26102;&#22312;&#32479;&#35745;&#19978;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#19968;&#26679;&#26377;&#25928;&#12290; &#28982;&#32780;&#65292;&#24403;&#20851;&#27880;&#31574;&#30053;&#35780;&#20272;&#30340;&#26680;&#24515;&#38382;&#39064;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#20851;&#20110;&#36716;&#31227;&#21160;&#24577;&#30340;&#20449;&#24687;&#21487;&#33021;&#26080;&#27861;&#22312;&#20215;&#20540;&#20989;&#25968;&#31354;&#38388;&#20013;&#34920;&#31034;&#12290; &#25105;&#20204;&#36890;&#36807;&#19968;&#31995;&#21015;&#30528;&#37325;&#20110;&#35768;&#22810;&#37325;&#35201;&#38382;&#39064;&#20013;&#20986;&#29616;&#30340;&#32467;&#26500;&#30340;&#26696;&#20363;&#30740;&#31350;&#26469;&#25506;&#31350;&#36825;&#19968;&#28857;&#12290; &#22312;&#20854;&#20013;&#20960;&#31181;&#24773;&#20917;&#20013;&#65292;&#27809;&#26377;&#20449;&#24687;&#20002;&#22833;&#65292;&#22522;&#20110;&#20540;&#30340;&#26041;&#27861;&#19982;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#22312;&#32479;&#35745;&#25928;&#29575;&#19978;&#30456;&#24403;&#12290; &#22312;&#20854;&#20182;&#30456;&#20851;&#31034;&#20363;&#20013;&#65292;&#20449;&#24687;&#20002;&#22833;&#20005;&#37325;&#65292;&#22522;&#20110;&#20540;&#30340;&#26041;&#27861;&#24615;&#33021;&#20005;&#37325;&#19981;&#21450;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290; &#26356;&#28145;&#20837;&#30340;&#30740;&#31350;&#25351;&#20986;&#20102;&#34920;&#24449;&#33021;&#21147;&#30340;&#38480;&#21046;&#20316;&#20026;&#20302;&#25928;&#24615;&#30340;&#39537;&#21160;&#22240;&#32032;&#65292;&#32780;&#38750;&#31639;&#27861;&#35774;&#35745;&#19978;&#30340;&#22833;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07136v1 Announce Type: cross  Abstract: Identifying the trade-offs between model-based and model-free methods is a central question in reinforcement learning. Value-based methods offer substantial computational advantages and are sometimes just as statistically efficient as model-based methods. However, focusing on the core problem of policy evaluation, we show information about the transition dynamics may be impossible to represent in the space of value functions. We explore this through a series of case studies focused on structures that arises in many important problems. In several, there is no information loss and value-based methods are as statistically efficient as model based ones. In other closely-related examples, information loss is severe and value-based methods are severely outperformed. A deeper investigation points to the limitations of the representational power as the driver of the inefficiency, as opposed to failure in algorithm design.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;COMQ&#30340;&#21019;&#26032;&#21518;&#35757;&#32451;&#37327;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#36880;&#23618;&#20943;&#23567;&#37325;&#26500;&#35823;&#24046;&#26469;&#26377;&#25928;&#38477;&#20302;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#23384;&#20648;&#35201;&#27714;&#65292;&#21516;&#26102;&#20445;&#25345;&#21407;&#22987;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.07134</link><description>&lt;p&gt;
COMQ: &#19968;&#31181;&#26080;&#38656;&#21453;&#21521;&#20256;&#25773;&#30340;&#21518;&#35757;&#32451;&#37327;&#21270;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
COMQ: A Backpropagation-Free Algorithm for Post-Training Quantization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07134
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;COMQ&#30340;&#21019;&#26032;&#21518;&#35757;&#32451;&#37327;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#36880;&#23618;&#20943;&#23567;&#37325;&#26500;&#35823;&#24046;&#26469;&#26377;&#25928;&#38477;&#20302;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#23384;&#20648;&#35201;&#27714;&#65292;&#21516;&#26102;&#20445;&#25345;&#21407;&#22987;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#35757;&#32451;&#37327;&#21270;&#65288;PTQ&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#23558;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#30340;&#23454;&#29992;&#26041;&#27861;&#65292;&#20351;&#20854;&#22312;&#37096;&#32626;&#26102;&#39640;&#24230;&#39640;&#25928;&#12290;&#28982;&#32780;&#65292;&#26377;&#25928;&#22320;&#23558;&#36825;&#20123;&#27169;&#22411;&#38477;&#33267;&#20302;&#27604;&#29305;&#34920;&#31034;&#32780;&#19981;&#25439;&#23475;&#21407;&#22987;&#20934;&#30830;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;PTQ&#31639;&#27861;&#31216;&#20026;COMQ&#65292;&#23427;&#36890;&#36807;&#20381;&#27425;&#20943;&#23567;&#36880;&#23618;&#37325;&#26500;&#35823;&#24046;&#26469;&#36827;&#34892;&#22352;&#26631;&#26041;&#21521;&#19978;&#30340;&#26368;&#23567;&#21270;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#24191;&#27867;&#20351;&#29992;&#30340;&#25972;&#25968;&#37327;&#21270;&#65292;&#20854;&#20013;&#27599;&#20010;&#37327;&#21270;&#26435;&#37325;&#21487;&#20197;&#20998;&#35299;&#20026;&#19968;&#20010;&#20849;&#20139;&#30340;&#28014;&#28857;&#26631;&#37327;&#21644;&#19968;&#20010;&#25972;&#25968;&#20301;&#32534;&#30721;&#12290;&#22312;&#22266;&#23450;&#23618;&#20869;&#65292;COMQ&#23558;&#25152;&#26377;&#32553;&#25918;&#22240;&#23376;&#21644;&#20301;&#32534;&#30721;&#35270;&#20026;&#37325;&#26500;&#35823;&#24046;&#30340;&#21464;&#37327;&#12290;&#27599;&#27425;&#36845;&#20195;&#37117;&#20250;&#27839;&#30528;&#19968;&#20010;&#22352;&#26631;&#36724;&#25913;&#36827;&#36825;&#20010;&#38169;&#35823;&#65292;&#21516;&#26102;&#20445;&#25345;&#25152;&#26377;&#20854;&#20182;&#21464;&#37327;&#24658;&#23450;&#12290;COMQ&#26131;&#20110;&#20351;&#29992;&#65292;&#26080;&#38656;&#35843;&#25972;&#36229;&#21442;&#25968;&#12290;&#23427;&#21482;&#28041;&#21450;&#28857;&#20056;&#21644;&#22235;&#33293;&#20116;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07134v1 Announce Type: new  Abstract: Post-training quantization (PTQ) has emerged as a practical approach to compress large neural networks, making them highly efficient for deployment. However, effectively reducing these models to their low-bit counterparts without compromising the original accuracy remains a key challenge. In this paper, we propose an innovative PTQ algorithm termed COMQ, which sequentially conducts coordinate-wise minimization of the layer-wise reconstruction errors. We consider the widely used integer quantization, where every quantized weight can be decomposed into a shared floating-point scalar and an integer bit-code. Within a fixed layer, COMQ treats all the scaling factor(s) and bit-codes as the variables of the reconstruction error. Every iteration improves this error along a single coordinate while keeping all other variables constant. COMQ is easy to use and requires no hyper-parameter tuning. It instead involves only dot products and rounding o
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29422;&#23376;&#29399;&#40763;&#23380;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#29421;&#31364;&#31243;&#24230;&#20998;&#31867;&#65292;&#21516;&#26102;&#25506;&#32034;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#33258;&#21160;&#25512;&#26029;&#29421;&#31364;&#31243;&#24230;&#65292;&#22312;&#22810;&#31181;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#19979;&#36827;&#34892;&#20102;&#35780;&#20272;</title><link>https://arxiv.org/abs/2403.07132</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#29422;&#23376;&#29399;&#40763;&#23380;&#22270;&#20687;&#30340;&#29421;&#31364;&#31243;&#24230;&#20998;&#31867;&#30340;&#26032;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
A New Machine Learning Dataset of Bulldog Nostril Images for Stenosis Degree Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07132
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29422;&#23376;&#29399;&#40763;&#23380;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#29421;&#31364;&#31243;&#24230;&#20998;&#31867;&#65292;&#21516;&#26102;&#25506;&#32034;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#33258;&#21160;&#25512;&#26029;&#29421;&#31364;&#31243;&#24230;&#65292;&#22312;&#22810;&#31181;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#19979;&#36827;&#34892;&#20102;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30701;&#22836;&#29356;&#21697;&#31181;&#20013;&#30340;&#19968;&#31181;&#22806;&#24418;&#29305;&#24449;&#8212;&#8212;&#30701;&#39045;&#65292;&#24341;&#36215;&#20102;BOAS&#65292;&#19968;&#31181;&#21628;&#21560;&#38556;&#30861;&#30142;&#30149;&#65292;&#24433;&#21709;&#20102;&#29399;&#30340;&#20581;&#24247;&#21644;&#31119;&#31049;&#65292;&#34920;&#29616;&#20026;&#21508;&#31181;&#30151;&#29366;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#30001;190&#24352;&#29422;&#23376;&#29399;&#40763;&#23380;&#22270;&#20687;&#32452;&#25104;&#30340;&#26032;&#30340;&#27880;&#37322;&#25968;&#25454;&#38598;&#12290;&#25968;&#25454;&#38598;&#20013;&#22823;&#32422;&#22343;&#21248;&#20195;&#34920;&#20102;&#19977;&#31181;&#29421;&#31364;&#31243;&#24230;&#65306;&#36731;&#24230;&#12289;&#20013;&#24230;&#21644;&#20005;&#37325;&#29421;&#31364;&#12290;&#25968;&#25454;&#38598;&#36824;&#21253;&#25324;&#23569;&#37327;&#38750;&#29421;&#31364;&#40763;&#23380;&#22270;&#20687;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#39318;&#20010;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#36824;&#25506;&#35752;&#20102;&#20351;&#29992;&#40763;&#23380;&#22270;&#20687;&#33258;&#21160;&#25512;&#26029;&#29421;&#31364;&#31243;&#24230;&#30340;&#28145;&#24230;&#23398;&#20064;&#20316;&#20026;&#26367;&#20195;&#26041;&#27861;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#27979;&#35797;&#20102;&#20960;&#31181;&#31070;&#32463;&#32593;&#32476;&#65306;ResNet50&#12289;MobileNetV3&#12289;DenseNet201&#12289;SwinV2&#21644;MaxViT&#12290;&#22312;&#36825;&#20010;&#35780;&#20272;&#20013;&#65292;&#38382;&#39064;&#34987;&#24314;&#27169;&#20026;&#20004;&#31181;&#19981;&#21516;&#30340;&#26041;&#24335;&#65306;&#39318;&#20808;&#65292;&#20316;&#20026;&#19968;&#20010;&#19977;&#31867;&#20998;&#31867;&#38382;&#39064;&#65288;&#36731;&#24230;&#25110;&#24320;&#25918;&#65292;&#20013;&#24230;&#65292;&#21644;&#20005;&#37325;&#65289;&#65307;&#20854;&#27425;&#65292;&#20316;&#20026;&#19968;&#20010;&#20108;&#20998;&#31867;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07132v1 Announce Type: cross  Abstract: Brachycephaly, a conformation trait in some dog breeds, causes BOAS, a respiratory disorder that affects the health and welfare of the dogs with various symptoms. In this paper, a new annotated dataset composed of 190 images of bulldogs' nostrils is presented. Three degrees of stenosis are approximately equally represented in the dataset: mild, moderate and severe stenosis. The dataset also comprises a small quantity of non stenotic nostril images. To the best of our knowledge, this is the first image dataset addressing this problem. Furthermore, deep learning is investigated as an alternative to automatically infer stenosis degree using nostril images. In this work, several neural networks were tested: ResNet50, MobileNetV3, DenseNet201, SwinV2 and MaxViT. For this evaluation, the problem was modeled in two different ways: first, as a three-class classification problem (mild or open, moderate, and severe); second, as a binary classifi
&lt;/p&gt;</description></item><item><title>FAX&#26159;&#19968;&#20010;&#22312;JAX&#20013;&#23884;&#20837;&#32852;&#37030;&#35745;&#31639;&#21407;&#35821;&#30340;&#24211;&#65292;&#25903;&#25345;&#22823;&#35268;&#27169;&#20998;&#24067;&#24335;&#35745;&#31639;&#65292;&#25552;&#20379;&#20102;&#32852;&#37030;&#33258;&#21160;&#24494;&#20998;&#30340;&#23454;&#29616;&#65292;&#24182;&#21487;&#35299;&#37322;&#33267;&#29616;&#26377;&#30340;&#29983;&#20135;&#36328;&#35774;&#22791;&#32852;&#37030;&#35745;&#31639;&#31995;&#32479;&#12290;</title><link>https://arxiv.org/abs/2403.07128</link><description>&lt;p&gt;
FAX: JAX&#20013;&#21487;&#25193;&#23637;&#19988;&#21487;&#24494;&#20998;&#30340;&#32852;&#37030;&#21407;&#35821;
&lt;/p&gt;
&lt;p&gt;
FAX: Scalable and Differentiable Federated Primitives in JAX
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07128
&lt;/p&gt;
&lt;p&gt;
FAX&#26159;&#19968;&#20010;&#22312;JAX&#20013;&#23884;&#20837;&#32852;&#37030;&#35745;&#31639;&#21407;&#35821;&#30340;&#24211;&#65292;&#25903;&#25345;&#22823;&#35268;&#27169;&#20998;&#24067;&#24335;&#35745;&#31639;&#65292;&#25552;&#20379;&#20102;&#32852;&#37030;&#33258;&#21160;&#24494;&#20998;&#30340;&#23454;&#29616;&#65292;&#24182;&#21487;&#35299;&#37322;&#33267;&#29616;&#26377;&#30340;&#29983;&#20135;&#36328;&#35774;&#22791;&#32852;&#37030;&#35745;&#31639;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;FAX&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;JAX&#35774;&#35745;&#30340;&#24211;&#65292;&#26088;&#22312;&#25903;&#25345;&#25968;&#25454;&#20013;&#24515;&#21644;&#36328;&#35774;&#22791;&#24212;&#29992;&#20013;&#30340;&#22823;&#35268;&#27169;&#20998;&#24067;&#24335;&#21644;&#32852;&#37030;&#35745;&#31639;&#12290;FAX&#21033;&#29992;JAX&#30340;&#20998;&#29255;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#21407;&#29983;&#38024;&#23545;TPU&#21644;&#26368;&#20808;&#36827;&#30340;JAX&#36816;&#34892;&#26102;&#65288;&#21253;&#25324;Pathways&#65289;&#30340;&#23450;&#20301;&#12290;FAX&#23558;&#32852;&#37030;&#35745;&#31639;&#30340;&#22522;&#26412;&#26500;&#20214;&#23884;&#20837;JAX&#20013;&#65292;&#24102;&#26469;&#20102;&#19977;&#20010;&#20851;&#38190;&#22909;&#22788;&#12290;&#39318;&#20808;&#65292;FAX&#30340;&#35745;&#31639;&#21487;&#20197;&#36716;&#25442;&#20026;XLA HLO&#12290;&#20854;&#27425;&#65292;FAX&#25552;&#20379;&#20102;&#32852;&#37030;&#33258;&#21160;&#24494;&#20998;&#30340;&#23436;&#25972;&#23454;&#29616;&#65292;&#26497;&#22823;&#22320;&#31616;&#21270;&#20102;&#32852;&#37030;&#35745;&#31639;&#30340;&#34920;&#36798;&#12290;&#26368;&#21518;&#65292;FAX&#30340;&#35745;&#31639;&#21487;&#20197;&#35299;&#37322;&#25104;&#29616;&#26377;&#30340;&#29983;&#20135;&#36328;&#35774;&#22791;&#32852;&#37030;&#35745;&#31639;&#31995;&#32479;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;FAX&#20026;&#25968;&#25454;&#20013;&#24515;&#20013;&#30340;&#32852;&#37030;&#35745;&#31639;&#25552;&#20379;&#20102;&#26131;&#32534;&#31243;&#12289;&#39640;&#24615;&#33021;&#21644;&#21487;&#25193;&#23637;&#30340;&#26694;&#26550;&#12290;FAX&#21487;&#22312;https://github.com/google-research/google-research/tree/master/fax &#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07128v1 Announce Type: cross  Abstract: We present FAX, a JAX-based library designed to support large-scale distributed and federated computations in both data center and cross-device applications. FAX leverages JAX's sharding mechanisms to enable native targeting of TPUs and state-of-the-art JAX runtimes, including Pathways. FAX embeds building blocks for federated computations as primitives in JAX. This enables three key benefits. First, FAX computations can be translated to XLA HLO. Second, FAX provides a full implementation of federated automatic differentiation, greatly simplifying the expression of federated computations. Last, FAX computations can be interpreted out to existing production cross-device federated compute systems. We show that FAX provides an easily programmable, performant, and scalable framework for federated computations in the data center. FAX is available at https://github.com/google-research/google-research/tree/master/fax .
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#23545;&#35937;&#26816;&#27979;&#20013;&#21069;&#26223;-&#21069;&#26223;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#22522;&#20110;YOLOv5&#26816;&#27979;&#22120;&#30340;&#22522;&#20934;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;COCO-ZIPF&#25968;&#25454;&#38598;&#21644;&#27604;&#36739;&#20998;&#26512;&#37319;&#26679;&#12289;&#25439;&#22833;&#21152;&#26435;&#31561;&#26041;&#27861;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#21333;&#38454;&#27573;&#26816;&#27979;&#22120;&#20013;&#30340;&#25928;&#26524;&#24182;&#19981;&#22914;&#22312;&#21452;&#38454;&#27573;&#26816;&#27979;&#22120;&#20013;&#26174;&#33879;&#12290;</title><link>https://arxiv.org/abs/2403.07113</link><description>&lt;p&gt;
&#23545;&#35937;&#26816;&#27979;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#65306;&#23454;&#39564;&#35786;&#26029;&#21644;&#32531;&#35299;&#31574;&#30053;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Class Imbalance in Object Detection: An Experimental Diagnosis and Study of Mitigation Strategies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07113
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#23545;&#35937;&#26816;&#27979;&#20013;&#21069;&#26223;-&#21069;&#26223;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#22522;&#20110;YOLOv5&#26816;&#27979;&#22120;&#30340;&#22522;&#20934;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;COCO-ZIPF&#25968;&#25454;&#38598;&#21644;&#27604;&#36739;&#20998;&#26512;&#37319;&#26679;&#12289;&#25439;&#22833;&#21152;&#26435;&#31561;&#26041;&#27861;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#21333;&#38454;&#27573;&#26816;&#27979;&#22120;&#20013;&#30340;&#25928;&#26524;&#24182;&#19981;&#22914;&#22312;&#21452;&#38454;&#27573;&#26816;&#27979;&#22120;&#20013;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35937;&#26816;&#27979;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#65292;&#32463;&#24120;&#21463;&#21040;&#25968;&#25454;&#38598;&#19981;&#24179;&#34913;&#30340;&#24433;&#21709;&#65292;&#23588;&#20854;&#26159;&#21069;&#26223;-&#21069;&#26223;&#31867;&#19981;&#24179;&#34913;&#36825;&#19968;&#26410;&#34987;&#20805;&#20998;&#25506;&#35752;&#30340;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;YOLOv5&#21333;&#38454;&#27573;&#26816;&#27979;&#22120;&#30340;&#22522;&#20934;&#26694;&#26550;&#65292;&#20197;&#24212;&#23545;&#21069;&#26223;-&#21069;&#26223;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#25105;&#20204;&#20174;COCO&#25968;&#25454;&#38598;&#20013;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;10&#31867;&#38271;&#23614;&#25968;&#25454;&#38598;&#65292;&#21517;&#20026;COCO-ZIPF&#65292;&#26088;&#22312;&#21453;&#26144;&#20855;&#26377;&#26377;&#38480;&#29289;&#20307;&#31867;&#21035;&#25968;&#37327;&#30340;&#24120;&#35265;&#23454;&#38469;&#26816;&#27979;&#22330;&#26223;&#12290;&#22312;&#27492;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#23545;&#19977;&#31181;&#24050;&#24314;&#31435;&#30340;&#25216;&#26415;&#36827;&#34892;&#20102;&#35814;&#32454;&#30740;&#31350;&#65306;&#37319;&#26679;&#12289;&#25439;&#22833;&#21152;&#26435;&#21644;&#25968;&#25454;&#22686;&#24378;&#12290;&#25105;&#20204;&#30340;&#27604;&#36739;&#20998;&#26512;&#34920;&#26126;&#65292;&#37319;&#26679;&#21644;&#25439;&#22833;&#21152;&#26435;&#26041;&#27861;&#65292;&#22312;&#21452;&#38454;&#27573;&#26816;&#27979;&#22120;&#35774;&#32622;&#20013;&#34987;&#35777;&#26126;&#26377;&#30410;&#30340;&#65292;&#20294;&#22312;&#21333;&#38454;&#27573;&#26816;&#27979;&#22120;&#20013;&#24182;&#27809;&#26377;&#21516;&#26679;&#26377;&#25928;&#36716;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07113v1 Announce Type: cross  Abstract: Object detection, a pivotal task in computer vision, is frequently hindered by dataset imbalances, particularly the under-explored issue of foreground-foreground class imbalance. This lack of attention to foreground-foreground class imbalance becomes even more pronounced in the context of single-stage detectors. This study introduces a benchmarking framework utilizing the YOLOv5 single-stage detector to address the problem of foreground-foreground class imbalance. We crafted a novel 10-class long-tailed dataset from the COCO dataset, termed COCO-ZIPF, tailored to reflect common real-world detection scenarios with a limited number of object classes. Against this backdrop, we scrutinized three established techniques: sampling, loss weighing, and data augmentation. Our comparative analysis reveals that sampling and loss reweighing methods, while shown to be beneficial in two-stage detector settings, do not translate as effectively in impr
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35757;&#32451;ResNet-18&#32593;&#32476;&#23545;&#28107;&#24052;&#30244;PET/CT&#22270;&#20687;&#30340;&#20999;&#29255;&#36827;&#34892;&#20998;&#31867;&#65292;&#23454;&#29616;&#23545;&#32959;&#30244;&#20999;&#29255;&#30340;&#33258;&#21160;&#21270;&#35782;&#21035;&#12290;</title><link>https://arxiv.org/abs/2403.07105</link><description>&lt;p&gt;
&#29992;&#20110;&#26469;&#33258;&#22810;&#20013;&#24515;&#28107;&#24052;&#30244;&#25968;&#25454;&#38598;&#30340;&#33258;&#21160;&#20998;&#31867;&#30340;&#20999;&#29255;&#20998;&#31867;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
A slice classification neural network for automated classification of axial PET/CT slices from a multi-centric lymphoma dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07105
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35757;&#32451;ResNet-18&#32593;&#32476;&#23545;&#28107;&#24052;&#30244;PET/CT&#22270;&#20687;&#30340;&#20999;&#29255;&#36827;&#34892;&#20998;&#31867;&#65292;&#23454;&#29616;&#23545;&#32959;&#30244;&#20999;&#29255;&#30340;&#33258;&#21160;&#21270;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#20999;&#29255;&#20998;&#31867;&#22312;&#20020;&#24202;&#19978;&#20855;&#26377;&#30456;&#20851;&#24615;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#34987;&#32435;&#20837;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#24037;&#20316;&#27969;&#31243;&#20013;&#20316;&#20026;&#19968;&#20010;&#39044;&#22788;&#29702;&#27493;&#39588;&#65292;&#21487;&#20197;&#26631;&#35760;&#20986;&#21487;&#33021;&#21253;&#21547;&#32959;&#30244;&#36739;&#39640;&#27010;&#29575;&#30340;&#20999;&#29255;&#65292;&#20174;&#32780;&#24341;&#23548;&#21307;&#29983;&#20851;&#27880;&#37325;&#35201;&#30340;&#20999;&#29255;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;ResNet-18&#32593;&#32476;&#65292;&#26681;&#25454;&#36724;&#24615;&#28107;&#24052;&#30244;PET/CT&#22270;&#20687;&#30340;&#20999;&#29255;&#26159;&#21542;&#19982;3D&#22270;&#20687;&#20013;&#30340;&#32959;&#30244;&#30456;&#20132;&#65288;&#38451;&#24615;&#20999;&#29255;&#65289;&#25110;&#19981;&#30456;&#20132;&#65288;&#38452;&#24615;&#20999;&#29255;&#65289;&#26469;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07105v1 Announce Type: cross  Abstract: Automated slice classification is clinically relevant since it can be incorporated into medical image segmentation workflows as a preprocessing step that would flag slices with a higher probability of containing tumors, thereby directing physicians attention to the important slices. In this work, we train a ResNet-18 network to classify axial slices of lymphoma PET/CT images (collected from two institutions) depending on whether the slice intercepted a tumor (positive slice) in the 3D image or if the slice did not (negative slice). Various instances of the network were trained on 2D axial datasets created in different ways: (i) slice-level split and (ii) patient-level split; inputs of different types were used: (i) only PET slices and (ii) concatenated PET and CT slices; and different training strategies were employed: (i) center-aware (CAW) and (ii) center-agnostic (CAG). Model performances were compared using the area under the recei
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#39640;&#26031;&#25439;&#22833;&#24179;&#28369;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;PGPE&#31639;&#27861;&#21644;&#19981;&#21516;&#20984;&#25918;&#23485;&#30340;&#35748;&#35777;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#26102;&#32531;&#35299;&#32039;&#20945;&#20984;&#26494;&#24347;&#24102;&#26469;&#30340;&#38382;&#39064;&#65292;&#24182;&#33719;&#24471;&#26356;&#22909;&#24615;&#33021;&#30340;&#32593;&#32476;&#12290;</title><link>https://arxiv.org/abs/2403.07095</link><description>&lt;p&gt;
&#29992;&#39640;&#26031;&#24179;&#28369;&#20811;&#26381;&#35748;&#35777;&#22521;&#35757;&#30340;&#24726;&#35770;
&lt;/p&gt;
&lt;p&gt;
Overcoming the Paradox of Certified Training with Gaussian Smoothing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07095
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#39640;&#26031;&#25439;&#22833;&#24179;&#28369;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;PGPE&#31639;&#27861;&#21644;&#19981;&#21516;&#20984;&#25918;&#23485;&#30340;&#35748;&#35777;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#26102;&#32531;&#35299;&#32039;&#20945;&#20984;&#26494;&#24347;&#24102;&#26469;&#30340;&#38382;&#39064;&#65292;&#24182;&#33719;&#24471;&#26356;&#22909;&#24615;&#33021;&#30340;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20184;&#20986;&#20102;&#22823;&#37327;&#21162;&#21147;&#65292;&#20294;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#20197;&#39640;&#35748;&#35777;&#20934;&#30830;&#24230;&#23545;&#25239;&#23545;&#25239;&#24615;&#31034;&#20363;&#20173;&#28982;&#26159;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#35757;&#32451;&#20013;&#65292;&#23613;&#31649;&#35748;&#35777;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#21033;&#29992;&#32039;&#20945;&#30340;&#20984;&#26494;&#24347;&#36827;&#34892;&#30028;&#35745;&#31639;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#34920;&#29616;&#19981;&#22914;&#36739;&#26494;&#30340;&#26494;&#24347;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#20551;&#35774;&#36825;&#26159;&#30001;&#36825;&#20123;&#26356;&#32039;&#30340;&#26494;&#24347;&#23548;&#33268;&#30340;&#25439;&#22833;&#34920;&#38754;&#30340;&#19981;&#36830;&#32493;&#24615;&#21644;&#25200;&#21160;&#25935;&#24863;&#24615;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#29702;&#35770;&#19978;&#23637;&#31034;&#20102;&#39640;&#26031;&#25439;&#22833;&#24179;&#28369;&#21487;&#20197;&#32531;&#35299;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#32467;&#21512;PGPE&#30340;&#35748;&#35777;&#35757;&#32451;&#26041;&#27861;&#65292;&#35813;&#31639;&#27861;&#35745;&#31639;&#24179;&#28369;&#25439;&#22833;&#30340;&#26799;&#24230;&#65292;&#24182;&#20351;&#29992;&#19981;&#21516;&#30340;&#20984;&#25918;&#23485;&#26469;&#30830;&#35748;&#36825;&#19968;&#28857;&#12290;&#22312;&#20351;&#29992;&#36825;&#31181;&#35757;&#32451;&#26041;&#27861;&#26102;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#26356;&#32039;&#23494;&#30340;&#30028;&#38480;&#30830;&#23454;&#23548;&#33268;&#26356;&#22909;&#30340;&#32593;&#32476;&#65292;&#21487;&#20197;&#22312;&#30456;&#21516;&#32593;&#32476;&#19978;&#32988;&#36807;&#21516;&#31867;&#25216;&#26415;&#12290;&#23613;&#31649;&#25193;&#23637;&#22522;&#20110;PGPE&#30340;&#35757;&#32451;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07095v1 Announce Type: new  Abstract: Training neural networks with high certified accuracy against adversarial examples remains an open problem despite significant efforts. While certification methods can effectively leverage tight convex relaxations for bound computation, in training, these methods perform worse than looser relaxations. Prior work hypothesized that this is caused by the discontinuity and perturbation sensitivity of the loss surface induced by these tighter relaxations. In this work, we show theoretically that Gaussian Loss Smoothing can alleviate both of these issues. We confirm this empirically by proposing a certified training method combining PGPE, an algorithm computing gradients of a smoothed loss, with different convex relaxations. When using this training method, we observe that tighter bounds indeed lead to strictly better networks that can outperform state-of-the-art methods on the same network. While scaling PGPE-based training remains challengin
&lt;/p&gt;</description></item><item><title>FALCON&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#31070;&#32463;&#32593;&#32476;&#21098;&#26525;&#30340;&#26032;&#22411;&#20248;&#21270;&#26694;&#26550;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#27169;&#22411;&#20934;&#30830;&#24615;&#12289;FLOPs&#21644;&#31232;&#30095;&#24615;&#32422;&#26463;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19968;&#38454;&#26041;&#27861;&#26469;&#20248;&#21270;&#32593;&#32476;&#21098;&#26525;&#12290;</title><link>https://arxiv.org/abs/2403.07094</link><description>&lt;p&gt;
FALCON&#65306;&#38754;&#21521;&#31070;&#32463;&#32593;&#32476;&#21098;&#26525;&#30340;FLOP&#24863;&#30693;&#32452;&#21512;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
FALCON: FLOP-Aware Combinatorial Optimization for Neural Network Pruning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07094
&lt;/p&gt;
&lt;p&gt;
FALCON&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#31070;&#32463;&#32593;&#32476;&#21098;&#26525;&#30340;&#26032;&#22411;&#20248;&#21270;&#26694;&#26550;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#27169;&#22411;&#20934;&#30830;&#24615;&#12289;FLOPs&#21644;&#31232;&#30095;&#24615;&#32422;&#26463;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19968;&#38454;&#26041;&#27861;&#26469;&#20248;&#21270;&#32593;&#32476;&#21098;&#26525;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#22686;&#21152;&#30340;&#35745;&#31639;&#38656;&#27714;&#32473;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#30340;&#37096;&#32626;&#24102;&#26469;&#25361;&#25112;&#12290;&#32593;&#32476;&#21098;&#26525;&#25552;&#20379;&#20102;&#19968;&#31181;&#20943;&#23567;&#27169;&#22411;&#22823;&#23567;&#21644;&#35745;&#31639;&#25104;&#26412;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21516;&#26102;&#20445;&#25345;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#24403;&#21069;&#30340;&#21098;&#26525;&#26041;&#27861;&#20027;&#35201;&#19987;&#27880;&#20110;&#36890;&#36807;&#20943;&#23569;&#38750;&#38646;&#21442;&#25968;&#30340;&#25968;&#37327;&#26469;&#25552;&#39640;&#31232;&#30095;&#24615;&#65292;&#36890;&#24120;&#24573;&#30053;&#20102;&#19982;&#28014;&#28857;&#36816;&#31639;&#25968;&#37327;&#65288;FLOPs&#65289;&#23494;&#20999;&#30456;&#20851;&#30340;&#20854;&#20182;&#37096;&#32626;&#25104;&#26412;&#65292;&#22914;&#25512;&#26029;&#26102;&#38388;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FALCON&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110;&#32452;&#21512;&#20248;&#21270;&#30340;&#32593;&#32476;&#21098;&#26525;&#26694;&#26550;&#65292;&#23427;&#21516;&#26102;&#32771;&#34385;&#20102;&#27169;&#22411;&#20934;&#30830;&#24615;&#65288;&#24544;&#23454;&#24230;&#65289;&#12289;FLOPs&#21644;&#31232;&#30095;&#24615;&#32422;&#26463;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#19968;&#20010;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#26159;&#19968;&#20010;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#65288;ILP&#65289;&#65292;&#23427;&#21516;&#26102;&#22788;&#29702;FLOP&#21644;&#31232;&#30095;&#24615;&#32422;&#26463;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#31639;&#27861;&#26469;&#36817;&#20284;&#35299;&#20915;ILP&#12290;&#25105;&#20204;&#20026;&#25105;&#20204;&#30340;&#20248;&#21270;&#26694;&#26550;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19968;&#38454;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07094v1 Announce Type: new  Abstract: The increasing computational demands of modern neural networks present deployment challenges on resource-constrained devices. Network pruning offers a solution to reduce model size and computational cost while maintaining performance. However, most current pruning methods focus primarily on improving sparsity by reducing the number of nonzero parameters, often neglecting other deployment costs such as inference time, which are closely related to the number of floating-point operations (FLOPs). In this paper, we propose FALCON, a novel combinatorial-optimization-based framework for network pruning that jointly takes into account model accuracy (fidelity), FLOPs, and sparsity constraints. A main building block of our approach is an integer linear program (ILP) that simultaneously handles FLOP and sparsity constraints. We present a novel algorithm to approximately solve the ILP. We propose a novel first-order method for our optimization fra
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#39640;&#25928;&#30340;&#19977;&#27493;&#32423;&#32852;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#33258;&#21160;&#26816;&#27979;&#21644;&#20998;&#21106;DLBCL PET&#22270;&#20687;&#20013;&#30340;&#32959;&#30244;&#65292;&#30456;&#27604;&#20110;&#21333;&#19968;&#31471;&#21040;&#31471;&#32593;&#32476;&#65292;&#20854;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#20998;&#21106;&#20934;&#30830;&#24615;&#65288;&#23558;3D Dice&#24471;&#20998;&#20174;58.9%&#25552;&#39640;&#21040;78.1%&#65289;&#12290;</title><link>https://arxiv.org/abs/2403.07092</link><description>&lt;p&gt;
&#29992;&#20110;&#20020;&#24202;PET&#25104;&#20687;&#20013;&#24357;&#28459;&#22823;B&#32454;&#32990;&#28107;&#24052;&#30244;&#33258;&#21160;&#32959;&#30244;&#26816;&#27979;&#21644;&#20998;&#21106;&#30340;&#32423;&#32852;&#28145;&#24230;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
A cascaded deep network for automated tumor detection and segmentation in clinical PET imaging of diffuse large B-cell lymphoma
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07092
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#39640;&#25928;&#30340;&#19977;&#27493;&#32423;&#32852;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#33258;&#21160;&#26816;&#27979;&#21644;&#20998;&#21106;DLBCL PET&#22270;&#20687;&#20013;&#30340;&#32959;&#30244;&#65292;&#30456;&#27604;&#20110;&#21333;&#19968;&#31471;&#21040;&#31471;&#32593;&#32476;&#65292;&#20854;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#20998;&#21106;&#20934;&#30830;&#24615;&#65288;&#23558;3D Dice&#24471;&#20998;&#20174;58.9%&#25552;&#39640;&#21040;78.1%&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#26816;&#27979;&#21644;&#20998;&#21106;&#24357;&#28459;&#24615;&#22823;B&#32454;&#32990;&#28107;&#24052;&#30244;(DLBCL) PET&#22270;&#20687;&#22312;&#24635;&#20195;&#35874;&#24615;&#32959;&#30244;&#20307;&#31215;&#20272;&#35745;&#65292;&#25918;&#23556;&#32452;&#32455;&#23398;&#20998;&#26512;&#65292;&#25163;&#26415;&#24178;&#39044;&#21644;&#25918;&#23556;&#27835;&#30103;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#25163;&#21160;&#20998;&#21106;&#20840;&#36523;PET&#22270;&#20687;&#20013;&#30340;&#32959;&#30244;&#26159;&#32791;&#26102;&#12289;&#21171;&#21160;&#23494;&#38598;&#21644;&#20381;&#36182;&#25805;&#20316;&#21592;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#24182;&#39564;&#35777;&#20102;&#19968;&#20010;&#24555;&#36895;&#26377;&#25928;&#30340;&#19977;&#27493;&#32423;&#32852;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#20174;PET&#22270;&#20687;&#33258;&#21160;&#26816;&#27979;&#21644;&#20998;&#21106;DLBCL&#32959;&#30244;&#12290;&#19982;&#29992;&#20110;&#20840;&#36523;PET&#22270;&#20687;&#20013;&#32959;&#30244;&#20998;&#21106;&#30340;&#21333;&#19968;&#31471;&#21040;&#31471;&#32593;&#32476;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#19977;&#27493;&#27169;&#22411;&#26356;&#20026;&#26377;&#25928;&#65288;&#23558;3D Dice&#24471;&#20998;&#20174;58.9%&#25552;&#39640;&#21040;78.1%&#65289;&#65292;&#22240;&#20026;&#23427;&#30340;&#21508;&#20010;&#19987;&#38376;&#27169;&#22359;&#65292;&#21363;&#20999;&#29255;&#20998;&#31867;&#22120;&#65292;&#32959;&#30244;&#26816;&#27979;&#22120;&#21644;&#32959;&#30244;&#20998;&#27573;&#22120;&#65292;&#21487;&#20197;&#29420;&#31435;&#35757;&#32451;&#21040;&#39640;&#25216;&#33021;&#27700;&#24179;&#20197;&#25191;&#34892;&#29305;&#23450;&#20219;&#21153;&#65292;&#32780;&#19981;&#26159;&#19968;&#20010;&#20855;&#26377;&#27425;&#20248;&#24615;&#33021;&#30340;&#21333;&#19968;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07092v1 Announce Type: cross  Abstract: Accurate detection and segmentation of diffuse large B-cell lymphoma (DLBCL) from PET images has important implications for estimation of total metabolic tumor volume, radiomics analysis, surgical intervention and radiotherapy. Manual segmentation of tumors in whole-body PET images is time-consuming, labor-intensive and operator-dependent. In this work, we develop and validate a fast and efficient three-step cascaded deep learning model for automated detection and segmentation of DLBCL tumors from PET images. As compared to a single end-to-end network for segmentation of tumors in whole-body PET images, our three-step model is more effective (improves 3D Dice score from 58.9% to 78.1%) since each of its specialized modules, namely the slice classifier, the tumor detector and the tumor segmentor, can be trained independently to a high degree of skill to carry out a specific task, rather than a single network with suboptimal performance 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#22914;&#20309;&#20511;&#21161;&#20808;&#39564;&#30693;&#35782;&#21644;&#35748;&#30693;&#27169;&#22411;&#26469;&#25913;&#21892;&#28145;&#24230;&#23398;&#20064;&#65292;&#20197;&#25552;&#21319;&#23545;&#25239;&#38450;&#24481;&#12289;&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#21644;&#38646;&#26679;&#26412;&#23398;&#20064;&#65292;&#24357;&#34917;&#29616;&#26377;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#39046;&#22495;&#30693;&#35782;&#21033;&#29992;&#12289;&#23545;&#25239;&#24615;&#25915;&#20987;&#38450;&#24481;&#12289;&#35299;&#37322;&#24615;&#20197;&#21450;&#22312;&#24320;&#25918;&#29615;&#22659;&#25512;&#29702;&#20013;&#30340;&#24615;&#33021;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.07078</link><description>&lt;p&gt;
&#20511;&#21161;&#20808;&#39564;&#30693;&#35782;&#21644;&#35748;&#30693;&#27169;&#22411;&#25913;&#21892;&#28145;&#24230;&#23398;&#20064;&#65306;&#22686;&#24378;&#21487;&#35299;&#37322;&#24615;&#12289;&#23545;&#25239;&#40065;&#26834;&#24615;&#21644;&#38646;&#26679;&#26412;&#23398;&#20064;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Improving deep learning with prior knowledge and cognitive models: A survey on enhancing explainability, adversarial robustness and zero-shot learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07078
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#22914;&#20309;&#20511;&#21161;&#20808;&#39564;&#30693;&#35782;&#21644;&#35748;&#30693;&#27169;&#22411;&#26469;&#25913;&#21892;&#28145;&#24230;&#23398;&#20064;&#65292;&#20197;&#25552;&#21319;&#23545;&#25239;&#38450;&#24481;&#12289;&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#21644;&#38646;&#26679;&#26412;&#23398;&#20064;&#65292;&#24357;&#34917;&#29616;&#26377;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#39046;&#22495;&#30693;&#35782;&#21033;&#29992;&#12289;&#23545;&#25239;&#24615;&#25915;&#20987;&#38450;&#24481;&#12289;&#35299;&#37322;&#24615;&#20197;&#21450;&#22312;&#24320;&#25918;&#29615;&#22659;&#25512;&#29702;&#20013;&#30340;&#24615;&#33021;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23457;&#26597;&#20102;&#24403;&#21069;&#21644;&#26032;&#20852;&#30340;&#30693;&#35782;&#39537;&#21160;&#21644;&#33041;&#21551;&#21457;&#30340;&#35748;&#30693;&#31995;&#32479;&#65292;&#29992;&#20110;&#23454;&#29616;&#23545;&#25239;&#24615;&#38450;&#24481;&#12289;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#20197;&#21450;&#38646;&#26679;&#26412;&#25110;&#23569;&#26679;&#26412;&#23398;&#20064;&#12290;&#25968;&#25454;&#39537;&#21160;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#36229;&#36234;&#20154;&#31867;&#19987;&#23478;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30001;&#20110;&#26080;&#27861;&#21033;&#29992;&#39046;&#22495;&#30693;&#35782;&#32780;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#23384;&#22312;&#20005;&#37325;&#24615;&#33021;&#38480;&#21046;&#12290;&#29305;&#21035;&#26159;&#65292;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#23427;&#20204;&#20570;&#20986;&#26126;&#26174;&#38169;&#35823;&#30340;&#20915;&#23450;&#12290;&#27492;&#22806;&#65292;&#22797;&#26434;&#30340;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#36890;&#24120;&#32570;&#20047;&#35299;&#37322;&#24615;&#65292;&#21363;&#23427;&#20204;&#30340;&#20915;&#31574;&#26080;&#27861;&#34987;&#20154;&#31867;&#20027;&#20307;&#29702;&#35299;&#12290;&#27492;&#22806;&#65292;&#27169;&#22411;&#36890;&#24120;&#26159;&#22312;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#65292;&#20855;&#26377;&#23553;&#38381;&#19990;&#30028;&#30340;&#20551;&#35774;&#12290;&#22240;&#27492;&#65292;&#22312;&#23454;&#38469;&#30340;&#24320;&#25918;&#29615;&#22659;&#20013;&#36827;&#34892;&#25512;&#29702;&#26102;&#65292;&#23427;&#20204;&#24456;&#38590;&#25512;&#24191;&#21040;&#26410;&#35265;&#26696;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07078v1 Announce Type: cross  Abstract: We review current and emerging knowledge-informed and brain-inspired cognitive systems for realizing adversarial defenses, eXplainable Artificial Intelligence (XAI), and zero-shot or few-short learning. Data-driven deep learning models have achieved remarkable performance and demonstrated capabilities surpassing human experts in many applications. Yet, their inability to exploit domain knowledge leads to serious performance limitations in practical applications. In particular, deep learning systems are exposed to adversarial attacks, which can trick them into making glaringly incorrect decisions. Moreover, complex data-driven models typically lack interpretability or explainability, i.e., their decisions cannot be understood by human subjects. Furthermore, models are usually trained on standard datasets with a closed-world assumption. Hence, they struggle to generalize to unseen cases during inference in practical open-world environmen
&lt;/p&gt;</description></item><item><title>&#26412;&#24037;&#20316;&#25506;&#35752;&#20102;&#22312;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#20013;&#30340;&#29305;&#24449;&#24402;&#22240;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;&#23450;&#20041;&#26041;&#27861;&#65292;&#24471;&#21040;&#20102;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;&#24402;&#22240;&#30340;&#38381;&#24335;&#34920;&#36798;&#24335;&#65292;&#21516;&#26102;&#23637;&#31034;&#20102;GPR&#27169;&#22411;&#24402;&#22240;&#20063;&#36981;&#24490;&#39640;&#26031;&#36807;&#31243;&#20998;&#24067;&#12290;</title><link>https://arxiv.org/abs/2403.07072</link><description>&lt;p&gt;
&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#36827;&#34892;&#21487;&#35299;&#37322;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Explainable Learning with Gaussian Processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07072
&lt;/p&gt;
&lt;p&gt;
&#26412;&#24037;&#20316;&#25506;&#35752;&#20102;&#22312;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#20013;&#30340;&#29305;&#24449;&#24402;&#22240;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;&#23450;&#20041;&#26041;&#27861;&#65292;&#24471;&#21040;&#20102;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;&#24402;&#22240;&#30340;&#38381;&#24335;&#34920;&#36798;&#24335;&#65292;&#21516;&#26102;&#23637;&#31034;&#20102;GPR&#27169;&#22411;&#24402;&#22240;&#20063;&#36981;&#24490;&#39640;&#26031;&#36807;&#31243;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#33021;&#22815;&#25581;&#31034;&#22797;&#26434;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#39044;&#27979;&#30340;&#26041;&#27861;&#12290;&#24456;&#22810;&#35299;&#37322;&#26041;&#27861;&#37117;&#38598;&#20013;&#22312;&#29305;&#24449;&#24402;&#22240;&#30340;&#27010;&#24565;&#19978;&#65292;&#21363;&#23558;&#27169;&#22411;&#30340;&#39044;&#27979;&#20998;&#35299;&#20026;&#23545;&#24212;&#20110;&#27599;&#20010;&#36755;&#20837;&#29305;&#24449;&#30340;&#20010;&#20307;&#36129;&#29486;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#65288;GPR&#65289;&#32972;&#26223;&#19979;&#30340;&#29305;&#24449;&#24402;&#22240;&#38382;&#39064;&#12290;&#25105;&#20204;&#37319;&#21462;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;&#21407;&#21017;&#24615;&#26041;&#27861;&#26469;&#23450;&#20041;&#29305;&#24449;&#24402;&#22240;&#65292;&#20174;&#32780;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;&#25991;&#29486;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#65292;&#23613;&#31649;GPR&#26159;&#19968;&#31181;&#39640;&#24230;&#28789;&#27963;&#21644;&#38750;&#21442;&#25968;&#21270;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#21487;&#20197;&#25512;&#23548;&#20986;&#23545;&#29305;&#24449;&#24402;&#22240;&#30340;&#21487;&#35299;&#37322;&#30340;&#38381;&#24335;&#34920;&#36798;&#24335;&#12290;&#24403;&#20351;&#29992;&#38598;&#25104;&#26799;&#24230;&#20316;&#20026;&#24402;&#22240;&#26041;&#27861;&#26102;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;GPR&#27169;&#22411;&#30340;&#24402;&#22240;&#20063;&#36981;&#24490;&#19968;&#20010;&#39640;&#26031;&#36807;&#31243;&#20998;&#24067;&#65292;&#29992;&#20197;&#37327;&#21270;&#24402;&#22240;&#20013;&#20986;&#29616;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07072v1 Announce Type: new  Abstract: The field of explainable artificial intelligence (XAI) attempts to develop methods that provide insight into how complicated machine learning methods make predictions. Many methods of explanation have focused on the concept of feature attribution, a decomposition of the model's prediction into individual contributions corresponding to each input feature. In this work, we explore the problem of feature attribution in the context of Gaussian process regression (GPR). We take a principled approach to defining attributions under model uncertainty, extending the existing literature. We show that although GPR is a highly flexible and non-parametric approach, we can derive interpretable, closed-form expressions for the feature attributions. When using integrated gradients as an attribution method, we show that the attributions of a GPR model also follow a Gaussian process distribution, which quantifies the uncertainty in attribution arising fro
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#37325;&#26032;&#27169;&#25311;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#31574;&#30053;RS3L&#65292;&#36890;&#36807;&#20171;&#20837;&#27169;&#25311;&#36807;&#31243;&#24182;&#37325;&#26032;&#27169;&#25311;&#20107;&#20214;&#23454;&#29616;&#65292;&#29983;&#25104;&#19968;&#32452;&#28085;&#30422;&#25152;&#26377;&#29289;&#29702;&#39537;&#21160;&#21464;&#21270;&#30340;&#25968;&#25454;&#22686;&#24378;&#65292;&#20174;&#32780;&#20419;&#36827;&#22522;&#30784;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#24182;&#23637;&#31034;&#20102;&#39044;&#35757;&#32451;R3SL&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#24378;&#22823;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.07066</link><description>&lt;p&gt;
&#22522;&#20110;&#37325;&#26032;&#27169;&#25311;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#29992;&#20110;&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Re-Simulation-based Self-Supervised Learning for Pre-Training Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07066
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#37325;&#26032;&#27169;&#25311;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#31574;&#30053;RS3L&#65292;&#36890;&#36807;&#20171;&#20837;&#27169;&#25311;&#36807;&#31243;&#24182;&#37325;&#26032;&#27169;&#25311;&#20107;&#20214;&#23454;&#29616;&#65292;&#29983;&#25104;&#19968;&#32452;&#28085;&#30422;&#25152;&#26377;&#29289;&#29702;&#39537;&#21160;&#21464;&#21270;&#30340;&#25968;&#25454;&#22686;&#24378;&#65292;&#20174;&#32780;&#20419;&#36827;&#22522;&#30784;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#24182;&#23637;&#31034;&#20102;&#39044;&#35757;&#32451;R3SL&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#24378;&#22823;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26159;&#35757;&#32451;&#29616;&#20195;&#22823;&#22411;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26680;&#24515;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#23398;&#20064;&#24378;&#22823;&#34920;&#31034;&#30340;&#26041;&#26696;&#65292;&#21487;&#29992;&#20110;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;SSL&#31574;&#30053;&#24517;&#39035;&#36866;&#24212;&#25152;&#38656;&#30340;&#35757;&#32451;&#25968;&#25454;&#31867;&#22411;&#21644;&#19979;&#28216;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;RS3L&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#27169;&#25311;&#30340;SSL&#31574;&#30053;&#65292;&#37319;&#29992;&#37325;&#26032;&#27169;&#25311;&#30340;&#26041;&#27861;&#26469;&#39537;&#21160;&#23545;&#27604;&#23398;&#20064;&#30340;&#25968;&#25454;&#22686;&#24378;&#12290;&#36890;&#36807;&#20171;&#20837;&#27169;&#25311;&#36807;&#31243;&#30340;&#20013;&#38388;&#24182;&#37325;&#26032;&#36816;&#34892;&#20171;&#20837;&#20043;&#21518;&#30340;&#27169;&#25311;&#32452;&#20214;&#65292;&#25105;&#20204;&#29983;&#25104;&#19968;&#20010;&#20107;&#20214;&#30340;&#22810;&#20010;&#23454;&#29616;&#65292;&#20174;&#32780;&#20135;&#29983;&#19968;&#32452;&#28085;&#30422;&#27169;&#25311;&#22120;&#20013;&#25152;&#26377;&#29289;&#29702;&#39537;&#21160;&#21464;&#21270;&#30340;&#22686;&#24378;&#12290;&#36890;&#36807;&#20351;&#29992;&#39640;&#33021;&#29289;&#29702;&#23454;&#39564;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#36825;&#31181;&#31574;&#30053;&#22914;&#20309;&#20419;&#36827;&#22522;&#30784;&#27169;&#22411;&#30340;&#21457;&#23637;&#65307;&#25105;&#20204;&#23637;&#31034;&#20102;R3SL&#39044;&#35757;&#32451;&#22914;&#20309;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#23454;&#29616;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#20363;&#22914;&#21306;&#20998;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07066v1 Announce Type: cross  Abstract: Self-Supervised Learning (SSL) is at the core of training modern large machine learning models, providing a scheme for learning powerful representations that can be used in a variety of downstream tasks. However, SSL strategies must be adapted to the type of training data and downstream tasks required. We propose RS3L, a novel simulation-based SSL strategy that employs a method of re-simulation to drive data augmentation for contrastive learning. By intervening in the middle of the simulation process and re-running simulation components downstream of the intervention, we generate multiple realizations of an event, thus producing a set of augmentations covering all physics-driven variations available in the simulator. Using experiments from high-energy physics, we explore how this strategy may enable the development of a foundation model; we show how R3SL pre-training enables powerful performance in downstream tasks such as discriminati
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24320;&#21457;&#24320;&#28304;&#36719;&#20214;&#21253;&#36827;&#34892;&#22823;&#35268;&#27169;&#30740;&#31350;&#65292;&#30740;&#31350;&#21457;&#29616;&#24635;&#20307;&#32780;&#35328;&#65292;&#24320;&#31665;&#21363;&#29992;&#30340;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#32988;&#36807;&#37327;&#23376;&#20998;&#31867;&#22120;&#65292;&#36824;&#21457;&#29616;&#23558;&#37327;&#23376;&#27169;&#22411;&#20013;&#30340;&#32416;&#32544;&#21435;&#38500;&#36890;&#24120;&#20250;&#23548;&#33268;&#21516;&#26679;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.07059</link><description>&lt;p&gt;
&#20248;&#20110;&#32463;&#20856;&#65311;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22522;&#20934;&#27979;&#35797;&#30340;&#24494;&#22937;&#33402;&#26415;
&lt;/p&gt;
&lt;p&gt;
Better than classical? The subtle art of benchmarking quantum machine learning models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07059
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24320;&#21457;&#24320;&#28304;&#36719;&#20214;&#21253;&#36827;&#34892;&#22823;&#35268;&#27169;&#30740;&#31350;&#65292;&#30740;&#31350;&#21457;&#29616;&#24635;&#20307;&#32780;&#35328;&#65292;&#24320;&#31665;&#21363;&#29992;&#30340;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#32988;&#36807;&#37327;&#23376;&#20998;&#31867;&#22120;&#65292;&#36824;&#21457;&#29616;&#23558;&#37327;&#23376;&#27169;&#22411;&#20013;&#30340;&#32416;&#32544;&#21435;&#38500;&#36890;&#24120;&#20250;&#23548;&#33268;&#21516;&#26679;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32463;&#20856;&#27169;&#25311;&#26469;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#26159;&#22312;&#27809;&#26377;&#26080;&#22122;&#22768;&#30828;&#20214;&#21487;&#29992;&#20043;&#21069;&#35780;&#20272;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#24605;&#24819;&#30340;&#20027;&#35201;&#26041;&#24335;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#23454;&#39564;&#35774;&#35745;&#23545;&#32467;&#26524;&#30340;&#24040;&#22823;&#24433;&#21709;&#65292;&#24403;&#21069;&#21487;&#36798;&#21040;&#30340;&#23567;&#35268;&#27169;&#65292;&#20197;&#21450;&#21463;&#37327;&#23376;&#25216;&#26415;&#21830;&#19994;&#21270;&#24433;&#21709;&#30340;&#21465;&#36848;&#20351;&#24471;&#38590;&#20197;&#33719;&#24471;&#31283;&#20581;&#30340;&#35265;&#35299;&#12290;&#20026;&#20102;&#20419;&#36827;&#26356;&#22909;&#30340;&#20915;&#31574;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;PennyLane&#36719;&#20214;&#26694;&#26550;&#30340;&#24320;&#28304;&#36719;&#20214;&#21253;&#65292;&#24182;&#20351;&#29992;&#23427;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#30740;&#31350;&#65292;&#31995;&#32479;&#22320;&#27979;&#35797;&#20102;12&#31181;&#27969;&#34892;&#30340;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#29992;&#20110;&#21019;&#24314;160&#20010;&#21333;&#29420;&#25968;&#25454;&#38598;&#30340;6&#20010;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#19978;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24635;&#20307;&#32780;&#35328;&#65292;&#24320;&#31665;&#21363;&#29992;&#30340;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#32988;&#36807;&#37327;&#23376;&#20998;&#31867;&#22120;&#12290;&#27492;&#22806;&#65292;&#20174;&#37327;&#23376;&#27169;&#22411;&#20013;&#31227;&#38500;&#32416;&#32544;&#24448;&#24448;&#20250;&#23548;&#33268;&#21516;&#26679;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#36825;&#34920;&#26126;&#8220;&#37327;&#23376;&#29305;&#24615;&#8221;&#21487;&#33021;&#24182;&#38750;&#20851;&#38190;&#25104;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07059v1 Announce Type: cross  Abstract: Benchmarking models via classical simulations is one of the main ways to judge ideas in quantum machine learning before noise-free hardware is available. However, the huge impact of the experimental design on the results, the small scales within reach today, as well as narratives influenced by the commercialisation of quantum technologies make it difficult to gain robust insights. To facilitate better decision-making we develop an open-source package based on the PennyLane software framework and use it to conduct a large-scale study that systematically tests 12 popular quantum machine learning models on 6 binary classification tasks used to create 160 individual datasets. We find that overall, out-of-the-box classical machine learning models outperform the quantum classifiers. Moreover, removing entanglement from a quantum model often results in as good or better performance, suggesting that "quantumness" may not be the crucial ingredi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#29983;&#25104;&#27969;&#34433;&#32676;&#37319;&#26679;&#22120;&#65288;GFACS&#65289;&#65292;&#19968;&#31181;&#32467;&#21512;&#29983;&#25104;&#27969;&#32593;&#32476;&#19982;&#34433;&#32676;&#20248;&#21270;&#26041;&#27861;&#30340;&#31070;&#32463;&#24341;&#23548;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#22312;&#32452;&#21512;&#20248;&#21270;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#22522;&#32447;ACO&#31639;&#27861;&#24182;&#19982;&#29305;&#23450;&#38382;&#39064;&#21551;&#21457;&#24335;&#26041;&#27861;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.07041</link><description>&lt;p&gt;
&#20351;&#29992;GFlowNets&#30340;&#34433;&#32676;&#37319;&#26679;&#29992;&#20110;&#32452;&#21512;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Ant Colony Sampling with GFlowNets for Combinatorial Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07041
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#29983;&#25104;&#27969;&#34433;&#32676;&#37319;&#26679;&#22120;&#65288;GFACS&#65289;&#65292;&#19968;&#31181;&#32467;&#21512;&#29983;&#25104;&#27969;&#32593;&#32476;&#19982;&#34433;&#32676;&#20248;&#21270;&#26041;&#27861;&#30340;&#31070;&#32463;&#24341;&#23548;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#22312;&#32452;&#21512;&#20248;&#21270;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#22522;&#32447;ACO&#31639;&#27861;&#24182;&#19982;&#29305;&#23450;&#38382;&#39064;&#21551;&#21457;&#24335;&#26041;&#27861;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#29983;&#25104;&#27969;&#34433;&#32676;&#37319;&#26679;&#22120;&#65288;GFACS&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#32452;&#21512;&#20248;&#21270;&#30340;&#31070;&#32463;&#24341;&#23548;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#12290;GFACS &#23558;&#29983;&#25104;&#27969;&#32593;&#32476;&#65288;GFlowNets&#65289;&#19982;&#34433;&#32676;&#20248;&#21270;&#65288;ACO&#65289;&#26041;&#27861;&#30456;&#32467;&#21512;&#12290;GFlowNets &#26159;&#19968;&#31181;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#22312;&#32452;&#21512;&#31354;&#38388;&#20013;&#23398;&#20064;&#26500;&#36896;&#24615;&#31574;&#30053;&#65292;&#36890;&#36807;&#22312;&#36755;&#20837;&#22270;&#23454;&#20363;&#19978;&#25552;&#20379;&#20915;&#31574;&#21464;&#37327;&#30340;&#30693;&#24773;&#20808;&#39564;&#20998;&#24067;&#26469;&#22686;&#24378; ACO&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35757;&#32451;&#25216;&#24039;&#32452;&#21512;&#65292;&#21253;&#25324;&#25628;&#32034;&#24341;&#23548;&#30340;&#23616;&#37096;&#25506;&#32034;&#12289;&#33021;&#37327;&#24402;&#19968;&#21270;&#21644;&#33021;&#37327;&#22609;&#24418;&#65292;&#20197;&#25552;&#39640; GFACS &#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;GFACS &#22312;&#19971;&#20010;&#32452;&#21512;&#20248;&#21270;&#20219;&#21153;&#20013;&#20248;&#20110;&#22522;&#32447; ACO &#31639;&#27861;&#65292;&#24182;&#19988;&#22312;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#30340;&#38382;&#39064;&#29305;&#23450;&#21551;&#21457;&#24335;&#26041;&#27861;&#20013;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#28304;&#20195;&#30721;&#21487;&#22312; \url{https://github.com/ai4co/gfacs} &#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07041v1 Announce Type: new  Abstract: This paper introduces the Generative Flow Ant Colony Sampler (GFACS), a novel neural-guided meta-heuristic algorithm for combinatorial optimization. GFACS integrates generative flow networks (GFlowNets) with the ant colony optimization (ACO) methodology. GFlowNets, a generative model that learns a constructive policy in combinatorial spaces, enhance ACO by providing an informed prior distribution of decision variables conditioned on input graph instances. Furthermore, we introduce a novel combination of training tricks, including search-guided local exploration, energy normalization, and energy shaping to improve GFACS. Our experimental results demonstrate that GFACS outperforms baseline ACO algorithms in seven CO tasks and is competitive with problem-specific heuristics for vehicle routing problems. The source code is available at \url{https://github.com/ai4co/gfacs}.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#20219;&#21153;&#25552;&#31034;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#39044;&#35757;&#32451;&#22270;&#27169;&#22411;&#19982;&#19981;&#21516;&#20219;&#21153;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#21551;&#21457;&#33258;NLP&#20013;&#25552;&#31034;&#23398;&#20064;&#30340;&#25104;&#21151;&#12290;</title><link>https://arxiv.org/abs/2403.07040</link><description>&lt;p&gt;
&#19968;&#31449;&#24335;&#65306;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#20219;&#21153;&#25552;&#31034;&#65288;&#25193;&#23637;&#25688;&#35201;&#65289;
&lt;/p&gt;
&lt;p&gt;
All in One: Multi-Task Prompting for Graph Neural Networks (Extended Abstract)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07040
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#20219;&#21153;&#25552;&#31034;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#39044;&#35757;&#32451;&#22270;&#27169;&#22411;&#19982;&#19981;&#21516;&#20219;&#21153;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#21551;&#21457;&#33258;NLP&#20013;&#25552;&#31034;&#23398;&#20064;&#30340;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26159;&#25105;&#20204;&#22312;KDD23&#20013;&#33719;&#24471;&#26368;&#20339;&#30740;&#31350;&#35770;&#25991;&#22870;&#30340;&#21407;&#22987;&#24037;&#20316;&#30340;&#25193;&#23637;&#25688;&#35201;&#65292;&#20854;&#20013;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#24357;&#21512;&#39044;&#35757;&#32451;&#22270;&#27169;&#22411;&#21644;&#23427;&#20204;&#24212;&#29992;&#20110;&#30340;&#19981;&#21516;&#20219;&#21153;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#28789;&#24863;&#26469;&#28304;&#20110;NLP&#20013;&#25552;&#31034;&#23398;&#20064;&#30340;&#25104;&#21151;&#12290;&#25105;&#20204;&#24847;&#35782;&#21040;&#20102;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#19982;&#21508;&#31181;&#22270;&#20219;&#21153;&#65288;&#33410;&#28857;&#32423;&#12289;&#36793;&#32423;&#21644;&#22270;&#32423;&#65289;&#23545;&#40784;&#30340;&#25361;&#25112;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#36127;&#36801;&#31227;&#21644;&#24615;&#33021;&#19979;&#38477;&#65292;&#22240;&#27492;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22270;&#30340;&#22810;&#20219;&#21153;&#25552;&#31034;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#28041;&#21450;&#32479;&#19968;&#22270;&#21644;&#35821;&#35328;&#25552;&#31034;&#26684;&#24335;&#65292;&#20351;NLP&#30340;&#25552;&#31034;&#31574;&#30053;&#33021;&#22815;&#36866;&#29992;&#20110;&#22270;&#20219;&#21153;&#12290;&#36890;&#36807;&#20998;&#26512;&#22270;&#24212;&#29992;&#30340;&#20219;&#21153;&#31354;&#38388;&#65292;&#25105;&#20204;&#37325;&#26032;&#21046;&#23450;&#38382;&#39064;&#20197;&#36866;&#24212;&#22270;&#32423;&#20219;&#21153;&#65292;&#24182;&#24212;&#29992;&#20803;&#23398;&#20064;&#26469;&#25913;&#36827;&#25552;&#31034;&#21021;&#22987;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07040v1 Announce Type: cross  Abstract: This paper is an extended abstract of our original work published in KDD23, where we won the best research paper award (Xiangguo Sun, Hong Cheng, Jia Li, Bo Liu, and Jihong Guan. All in one: Multi-task prompting for graph neural networks. KDD 23) The paper introduces a novel approach to bridging the gap between pre-trained graph models and the diverse tasks they're applied to, inspired by the success of prompt learning in NLP. Recognizing the challenge of aligning pre-trained models with varied graph tasks (node level, edge level, and graph level), which can lead to negative transfer and poor performance, we propose a multi-task prompting method for graphs. This method involves unifying graph and language prompt formats, enabling NLP's prompting strategies to be adapted for graph tasks. By analyzing the task space of graph applications, we reformulate problems to fit graph-level tasks and apply meta-learning to improve prompt initializ
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#20026;&#24613;&#35786;&#31185;&#30340;&#24739;&#32773;&#20998;&#35786;&#36807;&#31243;&#21152;&#20837;&#20102;&#20154;&#24037;&#26234;&#33021;&#27169;&#22359;&#65292;&#29992;&#20110;&#31649;&#29702;&#24739;&#32773;&#30340;&#24613;&#30151;&#32534;&#30721;&#20998;&#37197;&#65292;&#21487;&#26368;&#22823;&#21270;&#20449;&#24687;&#25910;&#38598;&#24182;&#26368;&#23567;&#21270;&#38169;&#35823;&#12290;</title><link>https://arxiv.org/abs/2403.07038</link><description>&lt;p&gt;
&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#25903;&#25345;&#24739;&#32773;&#33258;&#21160;&#20998;&#35786;
&lt;/p&gt;
&lt;p&gt;
Leveraging graph neural networks for supporting Automatic Triage of Patients
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07038
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#20026;&#24613;&#35786;&#31185;&#30340;&#24739;&#32773;&#20998;&#35786;&#36807;&#31243;&#21152;&#20837;&#20102;&#20154;&#24037;&#26234;&#33021;&#27169;&#22359;&#65292;&#29992;&#20110;&#31649;&#29702;&#24739;&#32773;&#30340;&#24613;&#30151;&#32534;&#30721;&#20998;&#37197;&#65292;&#21487;&#26368;&#22823;&#21270;&#20449;&#24687;&#25910;&#38598;&#24182;&#26368;&#23567;&#21270;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24739;&#32773;&#20998;&#35786;&#22312;&#24613;&#35786;&#31185;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#30830;&#20445;&#26681;&#25454;&#27491;&#30830;&#35780;&#20272;&#24739;&#32773;&#30149;&#24773;&#30340;&#24613;&#30151;&#31561;&#32423;&#21450;&#26102;&#21644;&#36866;&#24403;&#22320;&#25552;&#20379;&#25252;&#29702;&#12290;&#20256;&#32479;&#30340;&#20998;&#35786;&#26041;&#27861;&#20027;&#35201;&#30001;&#20154;&#21592;&#22522;&#20110;&#33258;&#24049;&#30340;&#32463;&#39564;&#21644;&#20174;&#24739;&#32773;&#31649;&#29702;&#36807;&#31243;&#20013;&#25910;&#38598;&#30340;&#20449;&#24687;&#25191;&#34892;&#12290;&#22240;&#27492;&#65292;&#36825;&#26159;&#19968;&#20010;&#21487;&#33021;&#20250;&#20135;&#29983;&#24613;&#30151;&#32423;&#21035;&#20851;&#32852;&#38169;&#35823;&#30340;&#36807;&#31243;&#12290;&#26368;&#36817;&#65292;&#20256;&#32479;&#30340;&#20998;&#35786;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#20110;&#20154;&#31867;&#20915;&#31574;&#65292;&#36825;&#21487;&#33021;&#26159;&#20027;&#35266;&#30340;&#24182;&#19988;&#23481;&#26131;&#20986;&#29616;&#38169;&#35823;&#12290;&#26368;&#36817;&#65292;&#20154;&#20204;&#24320;&#22987;&#20851;&#27880;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#24320;&#21457;&#33021;&#22815;&#26368;&#22823;&#21270;&#20449;&#24687;&#25910;&#38598;&#24182;&#26368;&#23567;&#21270;&#24739;&#32773;&#20998;&#35786;&#22788;&#29702;&#20013;&#38169;&#35823;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#23450;&#20041;&#24182;&#23454;&#26045;&#20102;&#19968;&#20010;&#22522;&#20110;AI&#30340;&#27169;&#22359;&#26469;&#31649;&#29702;&#24613;&#35786;&#31185;&#20013;&#24739;&#32773;&#24613;&#30151;&#32534;&#30721;&#20998;&#37197;&#12290;&#23427;&#20351;&#29992;&#24613;&#35786;&#31185;&#21382;&#21490;&#25968;&#25454;&#26469;&#35757;&#32451;&#21307;&#30103;&#20915;&#31574;&#36807;&#31243;&#12290;&#25968;&#25454;&#21253;&#21547;&#26377;&#20851;&#30456;&#20851;&#24615;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07038v1 Announce Type: new  Abstract: Patient triage plays a crucial role in emergency departments, ensuring timely and appropriate care based on correctly evaluating the emergency grade of patient conditions.   Triage methods are generally performed by human operator based on her own experience and information that are gathered from the patient management process.   Thus, it is a process that can generate errors in emergency level associations. Recently, Traditional triage methods heavily rely on human decisions, which can be subjective and prone to errors.   Recently, a growing interest has been focused on leveraging artificial intelligence (AI) to develop algorithms able to maximize information gathering and minimize errors in patient triage processing.   We define and implement an AI based module to manage patients emergency code assignments in emergency departments. It uses emergency department historical data to train the medical decision process. Data containing relev
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#36793;&#32536;&#35774;&#22791;&#30340;&#20302;&#24310;&#36831;&#21644;&#39640;&#33021;&#25928;DNN&#25512;&#26029;&#26694;&#26550;&#65292;&#21033;&#29992;&#8220;&#36716;&#25442;&#8221;&#33258;&#32534;&#30721;&#22120;&#21644;&#36731;&#37327;&#32423;DNN&#65292;&#25913;&#36827;&#20102;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.07036</link><description>&lt;p&gt;
&#19968;&#31181;&#38754;&#21521;&#36793;&#32536;&#20302;&#24310;&#36831;&#21644;&#39640;&#33021;&#25928;DNN&#25512;&#26029;&#30340;&#36716;&#25442;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
A Converting Autoencoder Toward Low-latency and Energy-efficient DNN Inference at the Edge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07036
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#36793;&#32536;&#35774;&#22791;&#30340;&#20302;&#24310;&#36831;&#21644;&#39640;&#33021;&#25928;DNN&#25512;&#26029;&#26694;&#26550;&#65292;&#21033;&#29992;&#8220;&#36716;&#25442;&#8221;&#33258;&#32534;&#30721;&#22120;&#21644;&#36731;&#37327;&#32423;DNN&#65292;&#25913;&#36827;&#20102;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38477;&#20302;&#25512;&#26029;&#26102;&#38388;&#21644;&#33021;&#37327;&#20351;&#29992;&#65292;&#21516;&#26102;&#20445;&#25345;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#24050;&#25104;&#20026;&#36164;&#28304;&#21463;&#38480;&#36793;&#32536;&#35774;&#22791;&#19978;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#25512;&#26029;&#30340;&#19968;&#20010;&#37325;&#35201;&#20851;&#27880;&#28857;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#8220;&#36716;&#25442;&#8221;&#33258;&#32534;&#30721;&#22120;&#21644;&#36731;&#37327;&#32423;DNN&#30340;&#26032;&#26041;&#27861;&#12290;&#36825;&#19968;&#26041;&#27861;&#25913;&#36827;&#20102;&#26368;&#36817;&#30340;&#24037;&#20316;&#65292;&#22914;&#25552;&#21069;&#36864;&#20986;&#26694;&#26550;&#21644;DNN&#20998;&#21306;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07036v1 Announce Type: new  Abstract: Reducing inference time and energy usage while maintaining prediction accuracy has become a significant concern for deep neural networks (DNN) inference on resource-constrained edge devices. To address this problem, we propose a novel approach based on "converting" autoencoder and lightweight DNNs. This improves upon recent work such as early-exiting framework and DNN partitioning. Early-exiting frameworks spend different amounts of computation power for different input data depending upon their complexity. However, they can be inefficient in real-world scenarios that deal with many hard image samples. On the other hand, DNN partitioning algorithms that utilize the computation power of both the cloud and edge devices can be affected by network delays and intermittent connections between the cloud and the edge. We present CBNet, a low-latency and energy-efficient DNN inference framework tailored for edge devices. It utilizes a "converting
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22810;&#31181;&#32676;&#20132;&#26367;&#36827;&#21270;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;MPAE&#65289;&#33539;&#24335;&#65292;&#33021;&#22815;&#22312;&#26356;&#23567;&#30340;&#25628;&#32034;&#25104;&#26412;&#19979;&#23454;&#29616;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#30340;&#27169;&#22359;&#22810;&#26679;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.07035</link><description>&lt;p&gt;
&#22810;&#31181;&#32676;&#20132;&#26367;&#36827;&#21270;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Multiple Population Alternate Evolution Neural Architecture Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07035
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22810;&#31181;&#32676;&#20132;&#26367;&#36827;&#21270;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;MPAE&#65289;&#33539;&#24335;&#65292;&#33021;&#22815;&#22312;&#26356;&#23567;&#30340;&#25628;&#32034;&#25104;&#26412;&#19979;&#23454;&#29616;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#30340;&#27169;&#22359;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07035v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#36827;&#21270;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;(ENAS)&#30340;&#26377;&#25928;&#24615;&#21463;&#21040;&#25628;&#32034;&#31354;&#38388;&#35774;&#35745;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#21253;&#25324;&#20840;&#23616;&#25628;&#32034;&#31354;&#38388;&#12289;&#21487;&#25193;&#23637;&#25628;&#32034;&#31354;&#38388;&#21644;&#20998;&#23618;&#25628;&#32034;&#31354;&#38388;&#22312;&#20869;&#30340;&#24120;&#35265;&#26041;&#27861;&#37117;&#23384;&#22312;&#19968;&#23450;&#23616;&#38480;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#20840;&#23616;&#25628;&#32034;&#31354;&#38388;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#21644;&#26102;&#38388;&#65292;&#21487;&#25193;&#23637;&#25628;&#32034;&#31354;&#38388;&#29306;&#29298;&#20102;&#32593;&#32476;&#32467;&#26500;&#30340;&#22810;&#26679;&#24615;&#65292;&#32780;&#20998;&#23618;&#25628;&#32034;&#31354;&#38388;&#21017;&#22686;&#21152;&#20102;&#25628;&#32034;&#25104;&#26412;&#20197;&#25442;&#21462;&#32593;&#32476;&#22810;&#26679;&#24615;&#12290;&#20026;&#35299;&#20915;&#19978;&#36848;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#33539;&#24335;&#65292;&#35774;&#35745;&#20102;&#22810;&#31181;&#32676;&#20132;&#26367;&#36827;&#21270;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;MPAE&#65289;&#65292;&#21487;&#20197;&#22312;&#26356;&#23567;&#30340;&#25628;&#32034;&#25104;&#26412;&#19979;&#23454;&#29616;&#27169;&#22359;&#22810;&#26679;&#24615;&#12290;MPAE&#23558;&#25628;&#32034;&#31354;&#38388;&#36716;&#25442;&#20026;L&#20010;&#20114;&#36830;&#21333;&#20803;&#65292;&#24182;&#20381;&#27425;&#25628;&#32034;&#36825;&#20123;&#21333;&#20803;&#65292;&#28982;&#21518;&#37325;&#22797;&#20960;&#27425;&#25628;&#32034;&#25972;&#20010;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07035v1 Announce Type: cross  Abstract: The effectiveness of Evolutionary Neural Architecture Search (ENAS) is influenced by the design of the search space. Nevertheless, common methods including the global search space, scalable search space and hierarchical search space have certain limitations. Specifically, the global search space requires a significant amount of computational resources and time, the scalable search space sacrifices the diversity of network structures and the hierarchical search space increases the search cost in exchange for network diversity. To address above limitation, we propose a novel paradigm of searching neural network architectures and design the Multiple Population Alternate Evolution Neural Architecture Search (MPAE), which can achieve module diversity with a smaller search cost. MPAE converts the search space into L interconnected units and sequentially searches the units, then the above search of the entire network be cycled several times t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#21407;&#22411;&#21305;&#37197;&#32593;&#32476;&#65288;PMN&#65289;&#65292;&#32467;&#21512;&#20154;&#31867;&#22266;&#26377;&#21407;&#22411;&#21305;&#37197;&#21644;&#33258;&#32534;&#30721;&#22120;&#65288;AE&#65289;&#65292;&#36890;&#36807;&#21305;&#37197;&#29305;&#24449;&#19982;&#21407;&#22411;&#24182;&#36873;&#25321;&#26368;&#30456;&#20284;&#30340;&#21407;&#22411;&#26469;&#35299;&#37322;&#20856;&#22411;&#25925;&#38556;&#20449;&#21495;&#30340;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2403.07033</link><description>&lt;p&gt;
&#36890;&#36807;&#21407;&#22411;&#21305;&#37197;&#35299;&#37322;&#20856;&#22411;&#25925;&#38556;&#20449;&#21495;&#30340;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Interpreting What Typical Fault Signals Look Like via Prototype-matching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07033
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#21407;&#22411;&#21305;&#37197;&#32593;&#32476;&#65288;PMN&#65289;&#65292;&#32467;&#21512;&#20154;&#31867;&#22266;&#26377;&#21407;&#22411;&#21305;&#37197;&#21644;&#33258;&#32534;&#30721;&#22120;&#65288;AE&#65289;&#65292;&#36890;&#36807;&#21305;&#37197;&#29305;&#24449;&#19982;&#21407;&#22411;&#24182;&#36873;&#25321;&#26368;&#30456;&#20284;&#30340;&#21407;&#22411;&#26469;&#35299;&#37322;&#20856;&#22411;&#25925;&#38556;&#20449;&#21495;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#24378;&#22823;&#30340;&#38750;&#32447;&#24615;&#26144;&#23556;&#21644;&#20998;&#31867;&#33021;&#21147;&#65292;&#22312;&#26426;&#26800;&#25925;&#38556;&#35786;&#26029;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#20197;&#30830;&#20445;&#23433;&#20840;&#12290;&#28982;&#32780;&#65292;&#20316;&#20026;&#20856;&#22411;&#30340;&#40657;&#30418;&#27169;&#22411;&#65292;&#23427;&#20204;&#22312;&#39640;&#21487;&#38752;&#24615;&#35201;&#27714;&#30340;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#21463;&#21040;&#38480;&#21046;&#12290;&#20026;&#20102;&#29702;&#35299;&#20998;&#31867;&#36923;&#36753;&#24182;&#35299;&#37322;&#20856;&#22411;&#25925;&#38556;&#20449;&#21495;&#30340;&#29305;&#24449;&#65292;&#25552;&#20986;&#20102;&#32467;&#21512;&#20154;&#31867;&#22266;&#26377;&#21407;&#22411;&#21305;&#37197;&#21644;&#33258;&#32534;&#30721;&#22120;&#65288;AE&#65289;&#30340;&#21407;&#22411;&#21305;&#37197;&#32593;&#32476;&#65288;PMN&#65289;&#12290;PMN&#23558;AE&#25552;&#21462;&#30340;&#29305;&#24449;&#19982;&#27599;&#20010;&#21407;&#22411;&#36827;&#34892;&#21305;&#37197;&#65292;&#24182;&#36873;&#25321;&#26368;&#30456;&#20284;&#30340;&#21407;&#22411;&#20316;&#20026;&#39044;&#27979;&#32467;&#26524;&#12290;&#23427;&#22312;&#20998;&#31867;&#36923;&#36753;&#12289;&#25925;&#38556;&#21407;&#22411;&#21644;&#21305;&#37197;&#36129;&#29486;&#26041;&#38754;&#26377;&#19977;&#26465;&#35299;&#37322;&#36335;&#24452;&#12290;&#20256;&#32479;&#35786;&#26029;&#21644;&#39046;&#22495;&#27867;&#21270;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#20855;&#26377;&#31454;&#20105;&#24615;&#30340;&#35786;&#26029;&#24615;&#33021;&#20197;&#21450;&#22312;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#26174;&#30528;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;&#23398;&#20064;&#21040;&#30340;&#20856;&#22411;&#25925;&#38556;&#20449;&#21495;&#65288;&#21363;&#26679;&#26412;&#32423;&#21407;&#22411;&#65289;&#23637;&#31034;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07033v1 Announce Type: cross  Abstract: Neural networks, with powerful nonlinear mapping and classification capabilities, are widely applied in mechanical fault diagnosis to ensure safety. However, being typical black-box models, their application is limited in high-reliability-required scenarios. To understand the classification logic and explain what typical fault signals look like, the prototype matching network (PMN) is proposed by combining the human-inherent prototype-matching with autoencoder (AE). The PMN matches AE-extracted feature with each prototype and selects the most similar prototype as the prediction result. It has three interpreting paths on classification logic, fault prototypes, and matching contributions. Conventional diagnosis and domain generalization experiments demonstrate its competitive diagnostic performance and distinguished advantages in representation learning. Besides, the learned typical fault signals (i.e., sample-level prototypes) showcase 
&lt;/p&gt;</description></item><item><title>Cram&#26041;&#27861;&#26159;&#19968;&#31181;&#21516;&#26102;&#23398;&#20064;&#21644;&#35780;&#20272;&#30340;&#39640;&#25928;&#26041;&#27861;&#65292;&#21033;&#29992;&#25972;&#20010;&#26679;&#26412;&#36827;&#34892;&#35757;&#32451;&#21644;&#27979;&#35797;&#65292;&#27604;&#20256;&#32479;&#30340;&#26679;&#26412;&#20998;&#21106;&#31574;&#30053;&#26356;&#39640;&#25928;&#12290;</title><link>https://arxiv.org/abs/2403.07031</link><description>&lt;p&gt;
&#29992;&#20110;&#39640;&#25928;&#21516;&#26102;&#23398;&#20064;&#21644;&#35780;&#20272;&#30340;Cram&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
The Cram Method for Efficient Simultaneous Learning and Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07031
&lt;/p&gt;
&lt;p&gt;
Cram&#26041;&#27861;&#26159;&#19968;&#31181;&#21516;&#26102;&#23398;&#20064;&#21644;&#35780;&#20272;&#30340;&#39640;&#25928;&#26041;&#27861;&#65292;&#21033;&#29992;&#25972;&#20010;&#26679;&#26412;&#36827;&#34892;&#35757;&#32451;&#21644;&#27979;&#35797;&#65292;&#27604;&#20256;&#32479;&#30340;&#26679;&#26412;&#20998;&#21106;&#31574;&#30053;&#26356;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#8220;Cram&#8221;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#29992;&#19988;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#36890;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#31639;&#27861;&#36827;&#34892;&#21516;&#26102;&#23398;&#20064;&#21644;&#35780;&#20272;&#12290;&#22312;&#25209;&#22788;&#29702;&#25968;&#25454;&#30340;&#21333;&#27425;&#20256;&#36882;&#20013;&#65292;&#35813;&#26041;&#27861;&#21453;&#22797;&#35757;&#32451;ML&#31639;&#27861;&#24182;&#27979;&#35797;&#20854;&#32463;&#39564;&#24615;&#33021;&#12290;&#30001;&#20110;&#23427;&#21516;&#26102;&#21033;&#29992;&#20102;&#25972;&#20010;&#26679;&#26412;&#36827;&#34892;&#23398;&#20064;&#21644;&#35780;&#20272;&#65292;&#25152;&#20197;Cram&#26041;&#27861;&#27604;&#26679;&#26412;&#20998;&#21106;&#35201;&#39640;&#25928;&#24471;&#22810;&#12290;Cram&#26041;&#27861;&#36824;&#33258;&#28982;&#22320;&#36866;&#29992;&#20110;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#20351;&#20854;&#23454;&#26045;&#20855;&#26377;&#35745;&#31639;&#25928;&#29575;&#12290;&#20026;&#20102;&#23637;&#31034;Cram&#26041;&#27861;&#30340;&#24378;&#22823;&#20043;&#22788;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#26631;&#20934;&#31574;&#30053;&#23398;&#20064;&#35774;&#32622;&#65292;&#20854;&#20013;&#23558;Cram&#24212;&#29992;&#20110;&#30456;&#21516;&#25968;&#25454;&#20197;&#24320;&#21457;&#20010;&#24615;&#21270;&#27835;&#30103;&#35268;&#21017;&#65288;ITR&#65289;&#24182;&#20272;&#35745;&#22914;&#26524;&#23398;&#20064;&#30340;ITR&#34987;&#37096;&#32626;&#23558;&#20250;&#20135;&#29983;&#30340;&#24179;&#22343;&#32467;&#26524;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#26368;&#23567;&#19968;&#32452;&#20551;&#35774;&#19979;&#65292;&#30001;&#27492;&#20135;&#29983;&#30340;Cram&#35780;&#20272;&#20272;&#35745;&#22120;&#26159;&#19968;&#33268;&#19988;&#28176;&#36817;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07031v1 Announce Type: new  Abstract: We introduce the "cram" method, a general and efficient approach to simultaneous learning and evaluation using a generic machine learning (ML) algorithm. In a single pass of batched data, the proposed method repeatedly trains an ML algorithm and tests its empirical performance. Because it utilizes the entire sample for both learning and evaluation, cramming is significantly more data-efficient than sample-splitting. The cram method also naturally accommodates online learning algorithms, making its implementation computationally efficient. To demonstrate the power of the cram method, we consider the standard policy learning setting where cramming is applied to the same data to both develop an individualized treatment rule (ITR) and estimate the average outcome that would result if the learned ITR were to be deployed. We show that under a minimal set of assumptions, the resulting crammed evaluation estimator is consistent and asymptoticall
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AuG-KD&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22522;&#20110;&#38170;&#28857;&#30340;&#28151;&#21512;&#29983;&#25104;&#65292;&#35299;&#20915;&#20102;&#26080;&#25968;&#25454;&#30693;&#35782;&#33976;&#39311;&#20013;&#30340;&#30693;&#35782;&#36716;&#31227;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.07030</link><description>&lt;p&gt;
AuG-KD: &#22522;&#20110;&#38170;&#28857;&#30340;&#28151;&#21512;&#29983;&#25104;&#29992;&#20110;&#39046;&#22495;&#20043;&#22806;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
AuG-KD: Anchor-Based Mixup Generation for Out-of-Domain Knowledge Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07030
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AuG-KD&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22522;&#20110;&#38170;&#28857;&#30340;&#28151;&#21512;&#29983;&#25104;&#65292;&#35299;&#20915;&#20102;&#26080;&#25968;&#25454;&#30693;&#35782;&#33976;&#39311;&#20013;&#30340;&#30693;&#35782;&#36716;&#31227;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#38544;&#31169;&#25110;&#19987;&#21033;&#38382;&#39064;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#22823;&#22411;&#27169;&#22411;&#21457;&#24067;&#26102;&#19981;&#25552;&#20379;&#20854;&#35757;&#32451;&#25968;&#25454;&#30340;&#35775;&#38382;&#26435;&#38480;&#65292;&#36825;&#20351;&#24471;&#23558;&#23427;&#20204;&#30340;&#30693;&#35782;&#36716;&#31227;&#21464;&#24471;&#20302;&#25928;&#19988;&#38382;&#39064;&#22797;&#26434;&#12290;&#38024;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#20986;&#29616;&#20102;&#26080;&#25968;&#25454;&#30693;&#35782;&#33976;&#39311;&#65288;DFKD&#65289;&#26041;&#27861;&#20316;&#20026;&#30452;&#25509;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#31616;&#21333;&#22320;&#37319;&#29992;&#20174;DFKD&#27966;&#29983;&#30340;&#27169;&#22411;&#29992;&#20110;&#23454;&#38469;&#24212;&#29992;&#20250;&#23548;&#33268;&#26174;&#33879;&#30340;&#24615;&#33021;&#19979;&#38477;&#65292;&#36825;&#26159;&#22240;&#20026;&#25945;&#24072;&#35757;&#32451;&#25968;&#25454;&#19982;&#23454;&#38469;&#22330;&#26223;&#65288;&#23398;&#29983;&#39046;&#22495;&#65289;&#20043;&#38388;&#23384;&#22312;&#24040;&#22823;&#24046;&#24322;&#12290;&#36825;&#31181;&#24615;&#33021;&#19979;&#38477;&#28304;&#20110;&#25945;&#24072;&#30693;&#35782;&#20013;&#19981;&#36866;&#29992;&#20110;&#23398;&#29983;&#39046;&#22495;&#30340;&#37096;&#20998;&#65292;&#36825;&#20123;&#30693;&#35782;&#26159;&#29305;&#23450;&#20110;&#25945;&#24072;&#39046;&#22495;&#30340;&#65292;&#20250;&#21066;&#24369;&#23398;&#29983;&#30340;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#22312;DFKD&#20013;&#65292;&#26377;&#36873;&#25321;&#22320;&#36716;&#31227;&#36866;&#29992;&#20110;&#23398;&#29983;&#39046;&#22495;&#30340;&#25945;&#24072;&#30693;&#35782;&#25104;&#20026;&#20027;&#35201;&#25361;&#25112;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;AuG-KD&#12290;&#23427;&#21033;&#29992;&#19968;&#31181;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#21644;&#26679;&#26412;&#29305;&#23450;&#30340;&#38170;&#28857;&#26469;&#23545;&#40784;&#23398;&#29983;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07030v1 Announce Type: new  Abstract: Due to privacy or patent concerns, a growing number of large models are released without granting access to their training data, making transferring their knowledge inefficient and problematic. In response, Data-Free Knowledge Distillation (DFKD) methods have emerged as direct solutions. However, simply adopting models derived from DFKD for real-world applications suffers significant performance degradation, due to the discrepancy between teachers' training data and real-world scenarios (student domain). The degradation stems from the portions of teachers' knowledge that are not applicable to the student domain. They are specific to the teacher domain and would undermine students' performance. Hence, selectively transferring teachers' appropriate knowledge becomes the primary challenge in DFKD. In this work, we propose a simple but effective method AuG-KD. It utilizes an uncertainty-guided and sample-specific anchor to align student-doma
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#35299;&#20915;&#22120;&#65292;&#36890;&#36807;&#24341;&#20837;&#26041;&#21521;&#24863;&#30693;&#25216;&#26415;&#21644;&#30417;&#30563;&#24378;&#21270;&#23398;&#20064;&#65292;&#26174;&#33879;&#32553;&#23567;&#20102;&#19982;&#20808;&#36827;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;&#24046;&#36317;&#65292;&#21516;&#26102;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.07028</link><description>&lt;p&gt;
&#19968;&#31181;&#19982;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#21487;&#27604;&#30340;&#39640;&#25928;&#23398;&#20064;&#22411;&#35299;&#20915;&#22120;&#65292;&#29992;&#20110;&#23481;&#37327;&#24359;&#36335;&#30001;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
An Efficient Learning-based Solver Comparable to Metaheuristics for the Capacitated Arc Routing Problem
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07028
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#35299;&#20915;&#22120;&#65292;&#36890;&#36807;&#24341;&#20837;&#26041;&#21521;&#24863;&#30693;&#25216;&#26415;&#21644;&#30417;&#30563;&#24378;&#21270;&#23398;&#20064;&#65292;&#26174;&#33879;&#32553;&#23567;&#20102;&#19982;&#20808;&#36827;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;&#24046;&#36317;&#65292;&#21516;&#26102;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#22312;&#32452;&#21512;&#20248;&#21270;&#26041;&#38754;&#21462;&#24471;&#20102;&#38271;&#36275;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#35299;&#20915;&#23481;&#37327;&#24359;&#36335;&#30001;&#38382;&#39064;&#65288;CARP&#65289;&#26102;&#38754;&#20020;&#25361;&#25112;&#65292;CARP&#26159;&#25351;&#22312;&#22270;&#19978;&#25214;&#21040;&#35206;&#30422;&#25152;&#26377;&#24517;&#38656;&#36793;&#30340;&#26368;&#23567;&#25104;&#26412;&#36335;&#24452;&#65292;&#21516;&#26102;&#22312;&#23481;&#37327;&#32422;&#26463;&#20869;&#12290;&#22312;&#35299;&#20915;CARP&#26041;&#38754;&#65292;&#22522;&#20110;NN&#30340;&#26041;&#27861;&#24448;&#24448;&#33853;&#21518;&#20110;&#20808;&#36827;&#30340;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#22240;&#20026;&#23427;&#20204;&#32570;&#20047;&#38024;&#23545;&#22797;&#26434;CARP&#23450;&#21046;&#30340;&#23450;&#21521;&#24359;&#24314;&#27169;&#21644;&#39640;&#25928;&#23398;&#20064;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;NN&#30340;&#35299;&#20915;&#22120;&#65292;&#20197;&#22823;&#22823;&#32553;&#23567;&#19982;&#20808;&#36827;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#21516;&#26102;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#25928;&#29575;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26041;&#21521;&#24863;&#30693;&#27880;&#24847;&#27169;&#22411;&#65288;DaAM&#65289;&#65292;&#23558;&#26041;&#21521;&#24615;&#24341;&#20837;&#23884;&#20837;&#36807;&#31243;&#65292;&#20419;&#36827;&#26356;&#26377;&#25928;&#30340;&#19968;&#38454;&#27573;&#20915;&#31574;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#30417;&#30563;&#24378;&#21270;&#23398;&#20064;&#26041;&#26696;&#65292;&#28041;&#21450;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#20026;&#38543;&#21518;&#30340;&#24378;&#21270;&#23398;&#20064;&#24314;&#31435;&#19968;&#20010;&#24378;&#22823;&#30340;&#21021;&#22987;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07028v1 Announce Type: cross  Abstract: Recently, neural networks (NN) have made great strides in combinatorial optimization. However, they face challenges when solving the capacitated arc routing problem (CARP) which is to find the minimum-cost tour covering all required edges on a graph, while within capacity constraints. In tackling CARP, NN-based approaches tend to lag behind advanced metaheuristics, since they lack directed arc modeling and efficient learning methods tailored for complex CARP. In this paper, we introduce an NN-based solver to significantly narrow the gap with advanced metaheuristics while exhibiting superior efficiency. First, we propose the direction-aware attention model (DaAM) to incorporate directionality into the embedding process, facilitating more effective one-stage decision-making. Second, we design a supervised reinforcement learning scheme that involves supervised pre-training to establish a robust initial policy for subsequent reinforcement 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;FWin&#36716;&#25442;&#22120;&#23545;&#26032;&#21152;&#22369;2000&#24180;&#33267;2019&#24180;&#27668;&#20505;&#25968;&#25454;&#36827;&#34892;&#38271;&#26399;&#30331;&#38761;&#30149;&#20363;&#39044;&#27979;&#65292;&#21457;&#29616;FWin&#34920;&#29616;&#26368;&#20339;&#12290;</title><link>https://arxiv.org/abs/2403.07027</link><description>&lt;p&gt;
&#22522;&#20110;FWin&#36716;&#25442;&#22120;&#30340;&#30331;&#38761;&#28909;&#39044;&#27979;&#22312;&#27668;&#20505;&#21644;&#28023;&#27915;&#24433;&#21709;&#19979;
&lt;/p&gt;
&lt;p&gt;
FWin transformer for dengue prediction under climate and ocean influence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07027
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;FWin&#36716;&#25442;&#22120;&#23545;&#26032;&#21152;&#22369;2000&#24180;&#33267;2019&#24180;&#27668;&#20505;&#25968;&#25454;&#36827;&#34892;&#38271;&#26399;&#30331;&#38761;&#30149;&#20363;&#39044;&#27979;&#65292;&#21457;&#29616;FWin&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39592;&#30149;&#28909;&#26159;&#26368;&#33268;&#21629;&#30340;&#34442;&#23186;&#20256;&#26579;&#30149;&#20043;&#19968;&#12290;&#35814;&#32454;&#30340;&#38271;&#26399;&#39044;&#27979;&#27169;&#22411;&#23545;&#20110;&#25511;&#21046;&#30142;&#30149;&#20256;&#25773;&#21644;&#36827;&#34892;&#32531;&#35299;&#24037;&#20316;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#32771;&#23519;&#20102;&#29992;&#20110;&#38271;&#26399;&#39044;&#27979;&#39592;&#30149;&#30149;&#20363;&#30340;&#26041;&#27861;&#12290;&#25968;&#25454;&#38598;&#21253;&#25324;2000&#24180;&#33267;2019&#24180;&#26032;&#21152;&#22369;&#30340;&#26412;&#22320;&#27668;&#20505;/&#22825;&#27668;&#20197;&#21450;&#20840;&#29699;&#27668;&#20505;&#25351;&#26631;&#12290;&#25105;&#20204;&#21033;&#29992;&#26032;&#24320;&#21457;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26469;&#23398;&#20064;&#29305;&#24449;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;&#26412;&#30740;&#31350;&#20013;&#30340;&#22522;&#20934;&#27169;&#22411;&#23646;&#20110;&#26368;&#36817;&#29992;&#20110;&#38271;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#30340;&#21464;&#21387;&#22120;&#31867;&#21035;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22522;&#20110;&#20613;&#37324;&#21494;&#28151;&#21512;&#31383;&#21475;&#27880;&#24847;&#21147;&#65288;FWin&#65289;&#30340;&#21464;&#21387;&#22120;&#22312;&#38271;&#36798;60&#21608;&#30340;&#30331;&#38761;&#28909;&#39044;&#27979;&#20013;&#65292;&#22312;&#22343;&#26041;&#35823;&#24046;&#21644;&#26368;&#22823;&#32477;&#23545;&#35823;&#24046;&#19978;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07027v1 Announce Type: new  Abstract: Dengue fever is one of the most deadly mosquito-born tropical infectious diseases. Detailed long range forecast model is vital in controlling the spread of disease and making mitigation efforts. In this study, we examine methods used to forecast dengue cases for long range predictions. The dataset consists of local climate/weather in addition to global climate indicators of Singapore from 2000 to 2019. We utilize newly developed deep neural networks to learn the intricate relationship between the features. The baseline models in this study are in the class of recent transformers for long sequence forecasting tasks. We found that a Fourier mixed window attention (FWin) based transformer performed the best in terms of both the mean square error and the maximum absolute error on the long range dengue forecast up to 60 weeks.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30333;&#24230;&#30340;&#25104;&#20687;&#21453;&#38382;&#39064;&#20013;&#23398;&#20064;&#27491;&#21017;&#21270;&#21442;&#25968;&#30340;&#21452;&#23618;&#20248;&#21270;&#31574;&#30053;&#65292;&#26080;&#38656;&#22320;&#38754;&#23454;&#20917;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2403.07026</link><description>&lt;p&gt;
&#22522;&#20110;&#30333;&#24230;&#30340;&#25104;&#20687;&#27491;&#21017;&#21442;&#25968;&#21452;&#23618;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Whiteness-based bilevel learning of regularization parameters in imaging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07026
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30333;&#24230;&#30340;&#25104;&#20687;&#21453;&#38382;&#39064;&#20013;&#23398;&#20064;&#27491;&#21017;&#21270;&#21442;&#25968;&#30340;&#21452;&#23618;&#20248;&#21270;&#31574;&#30053;&#65292;&#26080;&#38656;&#22320;&#38754;&#23454;&#20917;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#21452;&#23618;&#20248;&#21270;&#31574;&#30053;&#65292;&#29992;&#20110;&#22312;&#23384;&#22312;&#21152;&#24615;&#30333;&#39640;&#26031;&#22122;&#22768;&#30340;&#25104;&#20687;&#21453;&#38382;&#39064;&#32972;&#26223;&#19979;&#23398;&#20064;&#27491;&#21017;&#21270;&#21442;&#25968;&#12290;&#19982;&#20381;&#36182;&#20110;&#21442;&#32771;&#25968;&#25454;&#30340;&#20808;&#39564;&#30693;&#35782;&#21644;/&#25110;&#22122;&#22768;&#32479;&#35745;&#30340;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#24230;&#37327;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#21270;&#20102;&#35266;&#27979;&#25968;&#25454;&#21644;&#35266;&#27979;&#27169;&#22411;&#20043;&#38388;&#27531;&#24046;&#30340;&#30333;&#24230;&#65292;&#26080;&#38656;&#22320;&#38754;&#23454;&#20917;&#25968;&#25454;&#12290;&#25105;&#20204;&#22312;&#26631;&#20934;&#24635;&#21464;&#24046;&#27491;&#21017;&#21270;&#22270;&#20687;&#21453;&#21367;&#31215;&#38382;&#39064;&#19978;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#65292;&#32467;&#26524;&#34920;&#26126;&#25152;&#25552;&#20986;&#30340;&#36136;&#37327;&#24230;&#37327;&#25552;&#20379;&#20102;&#25509;&#36817;&#22343;&#26041;&#35823;&#24046;&#24052;&#25289;&#24471;&#21407;&#21017;&#30340;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07026v1 Announce Type: cross  Abstract: We consider an unsupervised bilevel optimization strategy for learning regularization parameters in the context of imaging inverse problems in the presence of additive white Gaussian noise. Compared to supervised and semi-supervised metrics relying either on the prior knowledge of reference data and/or on some (partial) knowledge on the noise statistics, the proposed approach optimizes the whiteness of the residual between the observed data and the observation model with no need of ground-truth data.We validate the approach on standard Total Variation-regularized image deconvolution problems which show that the proposed quality metric provides estimates close to the mean-square error oracle and to discrepancy-based principles.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#38646;&#22122;&#22768;&#22806;&#25512;&#65292;&#20197;&#25913;&#21892;&#37327;&#23376;&#21464;&#20998;&#31639;&#27861;&#22312;&#22122;&#22768;&#29615;&#22659;&#20013;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;</title><link>https://arxiv.org/abs/2403.07025</link><description>&lt;p&gt;
&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#38646;&#22122;&#22768;&#22806;&#25512;&#22686;&#24378;&#37327;&#23376;&#21464;&#20998;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Enhancing Quantum Variational Algorithms with Zero Noise Extrapolation via Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07025
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#38646;&#22122;&#22768;&#22806;&#25512;&#65292;&#20197;&#25913;&#21892;&#37327;&#23376;&#21464;&#20998;&#31639;&#27861;&#22312;&#22122;&#22768;&#29615;&#22659;&#20013;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26032;&#20852;&#30340;&#37327;&#23376;&#35745;&#31639;&#39046;&#22495;&#20013;&#65292;&#21464;&#20998;&#37327;&#23376;&#26412;&#24449;&#27714;&#35299;&#22120;&#65288;VQE&#65289;&#20316;&#20026;&#19968;&#31181;&#26377;&#24076;&#26395;&#35299;&#20915;&#22797;&#26434;&#37327;&#23376;&#38382;&#39064;&#30340;&#31639;&#27861;&#22312;&#22024;&#26434;&#30340;&#20013;&#31561;&#35268;&#27169;&#37327;&#23376;&#65288;NISQ&#65289;&#26102;&#20195;&#23588;&#20026;&#31361;&#20986;&#12290;&#28982;&#32780;&#65292;&#37327;&#23376;&#35774;&#22791;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#22122;&#22768;&#32463;&#24120;&#38480;&#21046;&#20102;VQE&#32467;&#26524;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#22312;VQE&#35745;&#31639;&#20013;&#36827;&#34892;&#38646;&#22122;&#22768;&#22806;&#25512;&#65288;ZNE&#65289;&#26469;&#25913;&#21892;&#36825;&#19968;&#25361;&#25112;&#12290;&#36890;&#36807;&#20351;&#29992;Qiskit&#26694;&#26550;&#65292;&#25105;&#20204;&#20351;&#29992;RY-RZ&#21442;&#25968;&#21270;&#37327;&#23376;&#30005;&#36335;&#65292;&#24182;&#26816;&#26597;&#23427;&#20204;&#22312;&#19981;&#21516;&#32423;&#21035;&#30340;&#21435;&#26497;&#21270;&#22122;&#22768;&#19979;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#28085;&#30422;&#20102;&#22312;&#19981;&#21516;&#22122;&#22768;&#24378;&#24230;&#19979;&#30830;&#23450;Hamiltonian&#65288;&#23450;&#20041;&#20026;Z&#31639;&#23376;&#30340;&#24352;&#37327;&#31215;&#65289;&#30340;&#26399;&#26395;&#20540;&#65292;&#20197;&#25552;&#21462;&#22522;&#24577;&#33021;&#37327;&#12290;&#20026;&#20102;&#23558;&#35266;&#23519;&#21040;&#30340;&#22122;&#22768;&#19979;&#30340;&#32467;&#26524;&#19982;&#29702;&#24819;&#30340;&#26080;&#22122;&#22768;&#24773;&#20917;&#36830;&#25509;&#36215;&#26469;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07025v1 Announce Type: cross  Abstract: In the emergent realm of quantum computing, the Variational Quantum Eigensolver (VQE) stands out as a promising algorithm for solving complex quantum problems, especially in the noisy intermediate-scale quantum (NISQ) era. However, the ubiquitous presence of noise in quantum devices often limits the accuracy and reliability of VQE outcomes. This research introduces a novel approach to ameliorate this challenge by utilizing neural networks for zero noise extrapolation (ZNE) in VQE computations. By employing the Qiskit framework, we crafted parameterized quantum circuits using the RY-RZ ansatz and examined their behavior under varying levels of depolarizing noise. Our investigations spanned from determining the expectation values of a Hamiltonian, defined as a tensor product of Z operators, under different noise intensities to extracting the ground state energy. To bridge the observed outcomes under noise with the ideal noise-free scenar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;One4All-ST&#26694;&#26550;&#65292;&#21487;&#20197;&#20165;&#20351;&#29992;&#19968;&#20010;&#27169;&#22411;&#20026;&#20219;&#24847;&#21487;&#20462;&#25913;&#21306;&#22495;&#21333;&#20301;&#36827;&#34892;ST&#39044;&#27979;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#22810;&#23610;&#24230;&#39044;&#27979;&#30340;&#25104;&#26412;&#38382;&#39064;&#21644;&#39044;&#27979;&#19981;&#19968;&#33268;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.07022</link><description>&lt;p&gt;
&#19968;&#31181;&#32479;&#19968;&#30340;&#27169;&#22411;&#29992;&#20110;&#20855;&#26377;&#20219;&#24847;&#21487;&#20462;&#25913;&#21306;&#22495;&#21333;&#20301;&#30340;&#26102;&#31354;&#39044;&#27979;&#26597;&#35810;
&lt;/p&gt;
&lt;p&gt;
A Unified Model for Spatio-Temporal Prediction Queries with Arbitrary Modifiable Areal Units
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07022
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;One4All-ST&#26694;&#26550;&#65292;&#21487;&#20197;&#20165;&#20351;&#29992;&#19968;&#20010;&#27169;&#22411;&#20026;&#20219;&#24847;&#21487;&#20462;&#25913;&#21306;&#22495;&#21333;&#20301;&#36827;&#34892;ST&#39044;&#27979;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#22810;&#23610;&#24230;&#39044;&#27979;&#30340;&#25104;&#26412;&#38382;&#39064;&#21644;&#39044;&#27979;&#19981;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#31354;&#65288;ST&#65289;&#39044;&#27979;&#23545;&#20110;&#22312;&#22478;&#24066;&#22522;&#20110;&#20301;&#32622;&#30340;&#24212;&#29992;&#20013;&#20570;&#20986;&#26126;&#26234;&#20915;&#31574;&#65288;&#22914;&#39034;&#39118;&#36710;&#65289;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;ST&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#21306;&#22495;&#21010;&#20998;&#20316;&#20026;&#20808;&#20915;&#26465;&#20214;&#65292;&#23548;&#33268;&#20004;&#20010;&#20027;&#35201;&#32570;&#28857;&#12290;&#39318;&#20808;&#65292;&#22522;&#20110;&#20301;&#32622;&#30340;&#26381;&#21153;&#38656;&#35201;&#20026;&#19981;&#21516;&#30446;&#30340;&#32780;&#23450;&#20041;&#20020;&#26102;&#21306;&#22495;&#65292;&#38656;&#35201;&#25903;&#25345;&#25104;&#26412;&#39640;&#26114;&#30340;&#22810;&#20010;&#20855;&#26377;&#19981;&#21516;&#35268;&#27169;&#21644;&#21306;&#22495;&#30340;ST&#27169;&#22411;&#12290;&#20854;&#27425;&#65292;&#19981;&#21516;&#30340;ST&#27169;&#22411;&#21487;&#33021;&#20135;&#29983;&#20914;&#31361;&#30340;&#36755;&#20986;&#65292;&#23548;&#33268;&#28151;&#20081;&#30340;&#39044;&#27979;&#12290; &#26412;&#25991;&#25552;&#20986;&#20102;One4All-ST&#26694;&#26550;&#65292;&#21487;&#20197;&#20165;&#20351;&#29992;&#19968;&#20010;&#27169;&#22411;&#26469;&#20026;&#20219;&#24847;&#21487;&#20462;&#25913;&#30340;&#21306;&#22495;&#21333;&#20803;&#36827;&#34892;ST&#39044;&#27979;&#12290;&#20026;&#20102;&#20943;&#23569;&#33719;&#21462;&#22810;&#23610;&#24230;&#39044;&#27979;&#30340;&#25104;&#26412;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20855;&#26377;&#20998;&#23618;&#31354;&#38388;&#24314;&#27169;&#21644;&#35268;&#27169;&#24402;&#19968;&#21270;&#27169;&#22359;&#30340;ST&#32593;&#32476;&#65292;&#20197;&#26377;&#25928;&#19988;&#24179;&#31561;&#22320;&#23398;&#20064;&#22810;&#23610;&#24230;&#34920;&#31034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36328;&#23610;&#24230;&#30340;&#39044;&#27979;&#19981;&#19968;&#33268;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#35268;&#21010;sch
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07022v1 Announce Type: cross  Abstract: Spatio-Temporal (ST) prediction is crucial for making informed decisions in urban location-based applications like ride-sharing. However, existing ST models often require region partition as a prerequisite, resulting in two main pitfalls. Firstly, location-based services necessitate ad-hoc regions for various purposes, requiring multiple ST models with varying scales and zones, which can be costly to support. Secondly, different ST models may produce conflicting outputs, resulting in confusing predictions. In this paper, we propose One4All-ST, a framework that can conduct ST prediction for arbitrary modifiable areal units using only one model. To reduce the cost of getting multi-scale predictions, we design an ST network with hierarchical spatial modeling and scale normalization modules to efficiently and equally learn multi-scale representations. To address prediction inconsistencies across scales, we propose a dynamic programming sch
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#25506;&#35752;&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#30340;&#36229;&#21442;&#25968;&#36873;&#25321;&#20316;&#29992;&#21644;&#26681;&#25454;&#20219;&#21153;&#22797;&#26434;&#24615;&#25345;&#32493;&#33258;&#21160;&#35843;&#25972;&#23427;&#20204;&#30340;&#24517;&#35201;&#24615;&#65292;&#36890;&#36807;&#21033;&#29992;&#24207;&#21015;&#20219;&#21153;&#23398;&#20064;&#29305;&#24615;&#26469;&#25552;&#39640;&#36229;&#21442;&#25968;&#20248;&#21270;&#25928;&#29575;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20351;&#36229;&#21442;&#25968;&#20248;&#21270;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#25345;&#32493;&#21152;&#36895;&#12290;</title><link>https://arxiv.org/abs/2403.07015</link><description>&lt;p&gt;
&#38024;&#23545;&#25345;&#32493;&#23398;&#20064;&#22330;&#26223;&#30340;&#33258;&#36866;&#24212;&#36229;&#21442;&#25968;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Adaptive Hyperparameter Optimization for Continual Learning Scenarios
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07015
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#25506;&#35752;&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#30340;&#36229;&#21442;&#25968;&#36873;&#25321;&#20316;&#29992;&#21644;&#26681;&#25454;&#20219;&#21153;&#22797;&#26434;&#24615;&#25345;&#32493;&#33258;&#21160;&#35843;&#25972;&#23427;&#20204;&#30340;&#24517;&#35201;&#24615;&#65292;&#36890;&#36807;&#21033;&#29992;&#24207;&#21015;&#20219;&#21153;&#23398;&#20064;&#29305;&#24615;&#26469;&#25552;&#39640;&#36229;&#21442;&#25968;&#20248;&#21270;&#25928;&#29575;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20351;&#36229;&#21442;&#25968;&#20248;&#21270;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#25345;&#32493;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25345;&#32493;&#23398;&#20064;&#22330;&#26223;&#20013;&#65292;&#36229;&#21442;&#25968;&#36873;&#25321;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#23578;&#26410;&#20805;&#20998;&#25506;&#32034;&#30340;&#26041;&#38754;&#65292;&#29305;&#21035;&#26159;&#22312;&#23454;&#38469;&#38750;&#24179;&#31283;&#29615;&#22659;&#20013;&#12290;&#26412;&#25991;&#26088;&#22312;&#25506;&#35752;&#36229;&#21442;&#25968;&#36873;&#25321;&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#65292;&#20197;&#21450;&#26681;&#25454;&#25163;&#22836;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#25345;&#32493;&#33258;&#21160;&#35843;&#25972;&#23427;&#20204;&#30340;&#24517;&#35201;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#24207;&#21015;&#20219;&#21153;&#23398;&#20064;&#30340;&#29305;&#24615;&#26469;&#25552;&#39640;&#36229;&#21442;&#25968;&#20248;&#21270;&#25928;&#29575;&#12290;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#26041;&#24046;&#30340;&#21151;&#33021;&#20998;&#26512;&#25216;&#26415;&#65292;&#25105;&#20204;&#35782;&#21035;&#20986;&#23545;&#24615;&#33021;&#20135;&#29983;&#24433;&#21709;&#30340;&#26368;&#20851;&#38190;&#30340;&#36229;&#21442;&#25968;&#12290;&#25105;&#20204;&#36890;&#36807;&#32463;&#39564;&#24615;&#22320;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#26080;&#35270;&#25345;&#32493;&#22330;&#26223;&#21644;&#31574;&#30053;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#25345;&#32493;&#21152;&#24555;&#36229;&#21442;&#25968;&#22312;&#19981;&#21516;&#20219;&#21153;&#38388;&#30340;&#20248;&#21270;&#65292;&#24182;&#23637;&#31034;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07015v1 Announce Type: new  Abstract: Hyperparameter selection in continual learning scenarios is a challenging and underexplored aspect, especially in practical non-stationary environments. Traditional approaches, such as grid searches with held-out validation data from all tasks, are unrealistic for building accurate lifelong learning systems. This paper aims to explore the role of hyperparameter selection in continual learning and the necessity of continually and automatically tuning them according to the complexity of the task at hand. Hence, we propose leveraging the nature of sequence task learning to improve Hyperparameter Optimization efficiency. By using the functional analysis of variance-based techniques, we identify the most crucial hyperparameters that have an impact on performance. We demonstrate empirically that this approach, agnostic to continual scenarios and strategies, allows us to speed up hyperparameters optimization continually across tasks and exhibit
&lt;/p&gt;</description></item><item><title>AdaNovo &#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35745;&#31639;&#20809;&#35889;&#21644;&#27599;&#20010;&#27688;&#22522;&#37240;/&#32957;&#27573;&#20043;&#38388;&#30340;&#26465;&#20214;&#20114;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#33258;&#36866;&#24212;&#27169;&#22411;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2403.07013</link><description>&lt;p&gt;
AdaNovo&#65306;&#20855;&#26377;&#26465;&#20214;&#20114;&#20449;&#24687;&#30340;&#33258;&#36866;&#24212;\emph{De Novo}&#32957;&#29255;&#27573;&#27979;&#24207;
&lt;/p&gt;
&lt;p&gt;
AdaNovo: Adaptive \emph{De Novo} Peptide Sequencing with Conditional Mutual Information
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07013
&lt;/p&gt;
&lt;p&gt;
AdaNovo &#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35745;&#31639;&#20809;&#35889;&#21644;&#27599;&#20010;&#27688;&#22522;&#37240;/&#32957;&#27573;&#20043;&#38388;&#30340;&#26465;&#20214;&#20114;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#33258;&#36866;&#24212;&#27169;&#22411;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36136;&#35889;&#32852;&#29992;&#24050;&#22312;&#20419;&#36827;&#34507;&#30333;&#36136;&#32452;&#23398;&#26041;&#38754;&#21457;&#25381;&#20851;&#38190;&#20316;&#29992;&#65292;&#20351;&#24471;&#21487;&#20197;&#20998;&#26512;&#29983;&#29289;&#26679;&#26412;&#20013;&#30340;&#34507;&#30333;&#36136;&#32452;&#25104;&#12290;&#23613;&#31649;&#24050;&#24320;&#21457;&#20102;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#35782;&#21035;&#23548;&#33268;&#35266;&#23519;&#20809;&#35889;&#30340;&#27688;&#22522;&#37240;&#24207;&#21015;&#65288;&#32957;&#27573;&#65289;&#65292;&#20294;\emph{de novo}&#32957;&#27573;&#27979;&#24207;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AdaNovo&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#23427;&#35745;&#31639;&#20102;&#20809;&#35889;&#21644;&#27599;&#20010;&#27688;&#22522;&#37240;/&#32957;&#27573;&#20043;&#38388;&#30340;&#26465;&#20214;&#20114;&#20449;&#24687;&#65288;CMI&#65289;&#65292;&#24182;&#21033;&#29992;CMI&#36827;&#34892;&#33258;&#36866;&#24212;&#27169;&#22411;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07013v1 Announce Type: cross  Abstract: Tandem mass spectrometry has played a pivotal role in advancing proteomics, enabling the analysis of protein composition in biological samples. Despite the development of various deep learning methods for identifying amino acid sequences (peptides) responsible for observed spectra, challenges persist in \emph{de novo} peptide sequencing. Firstly, prior methods struggle to identify amino acids with post-translational modifications (PTMs) due to their lower frequency in training data compared to canonical amino acids, further resulting in decreased peptide-level identification precision. Secondly, diverse types of noise and missing peaks in mass spectra reduce the reliability of training data (peptide-spectrum matches, PSMs). To address these challenges, we propose AdaNovo, a novel framework that calculates conditional mutual information (CMI) between the spectrum and each amino acid/peptide, using CMI for adaptive model training. Extens
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#24352;&#37327;&#20998;&#35299;&#30340;&#38750;&#20405;&#20837;&#24335;&#36127;&#36733;&#30417;&#27979;&#30340;&#32570;&#22833;&#25968;&#25454;&#25554;&#34917;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;PID&#25511;&#21046;&#22120;&#21644;&#38750;&#36127;&#26356;&#26032;&#35268;&#21017;&#65292;&#35299;&#20915;&#20102;NILM&#25968;&#25454;&#20002;&#22833;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.07012</link><description>&lt;p&gt;
&#22522;&#20110;&#24352;&#37327;&#20998;&#35299;&#30340;&#32570;&#22833;&#25968;&#25454;&#25554;&#34917;&#38750;&#20405;&#20837;&#24335;&#36127;&#36733;&#30417;&#27979;
&lt;/p&gt;
&lt;p&gt;
Non-Intrusive Load Monitoring with Missing Data Imputation Based on Tensor Decomposition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07012
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#24352;&#37327;&#20998;&#35299;&#30340;&#38750;&#20405;&#20837;&#24335;&#36127;&#36733;&#30417;&#27979;&#30340;&#32570;&#22833;&#25968;&#25454;&#25554;&#34917;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;PID&#25511;&#21046;&#22120;&#21644;&#38750;&#36127;&#26356;&#26032;&#35268;&#21017;&#65292;&#35299;&#20915;&#20102;NILM&#25968;&#25454;&#20002;&#22833;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#38750;&#20405;&#20837;&#24335;&#36127;&#36733;&#30417;&#27979;&#65288;NILM&#65289;&#22312;&#24314;&#31569;&#33021;&#28304;&#31649;&#29702;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#30830;&#20445;NILM&#25968;&#25454;&#30340;&#39640;&#36136;&#37327;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;NILM&#30340;&#23454;&#38469;&#24212;&#29992;&#38754;&#20020;&#25968;&#25454;&#20002;&#22833;&#30340;&#25361;&#25112;&#65292;&#20005;&#37325;&#24433;&#21709;&#33021;&#28304;&#31649;&#29702;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#21019;&#26032;&#30340;&#24352;&#37327;&#23436;&#25104;&#65288;TC&#65289;&#27169;&#22411;-&#22522;&#20110;&#31215;&#20998;&#27604;-&#23548;&#25968;&#65288;PID&#65289;&#30340;&#24352;&#37327;&#30340;&#38750;&#36127;&#28508;&#22240;&#23376;&#20998;&#35299;&#65288;PNLFT&#65289;&#26469;&#35299;&#20915;NILM&#25968;&#25454;&#20002;&#22833;&#38382;&#39064;&#65292;&#20854;&#20013;&#21253;&#21547;&#20004;&#20010;&#24605;&#24819;&#65306;1&#65289;&#20026;&#35299;&#20915;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#20013;&#28508;&#22312;&#24352;&#37327;&#20998;&#35299;&#65288;LFT&#65289;&#30340;&#25910;&#25947;&#32531;&#24930;&#38382;&#39064;&#65292;&#23398;&#20064;&#36807;&#31243;&#20013;&#24341;&#20837;&#27604;&#20363;-&#31215;&#20998;-&#23548;&#25968;&#25511;&#21046;&#22120;&#12290;PID&#25511;&#21046;&#22120;&#21033;&#29992;&#21382;&#21490;&#20449;&#24687;&#21644;&#24403;&#21069;&#20449;&#24687;&#25511;&#21046;&#23398;&#20064;&#27531;&#24046;&#12290;2&#65289;&#32771;&#34385;&#21040;NILM&#25968;&#25454;&#30340;&#29305;&#24615;&#65292;&#25552;&#20986;&#20102;&#38750;&#36127;&#26356;&#26032;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07012v1 Announce Type: new  Abstract: With the widespread adoption of Non-Intrusive Load Monitoring (NILM) in building energy management, ensuring the high quality of NILM data has become imperative. However, practical applications of NILM face challenges associated with data loss, significantly impacting accuracy and reliability in energy management. This paper addresses the issue of NILM data loss by introducing an innovative tensor completion(TC) model- Proportional-Integral-Derivative (PID)-incorporated Non-negative Latent Factorization of Tensors (PNLFT) with twofold ideas: 1) To tackle the issue of slow convergence in Latent Factorization of Tensors (LFT) using Stochastic Gradient Descent (SGD), a Proportional-Integral-Derivative controller is introduced during the learning process. The PID controller utilizes historical and current information to control learning residuals. 2) Considering the characteristics of NILM data, non-negative update rules are proposed in the 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39640;&#25928;&#21644;&#32479;&#35745;&#19978;&#21512;&#29702;&#30340;&#31639;&#27861;&#65292;&#22312;GPT-4&#23454;&#39564;&#20013;&#26377;&#25928;&#30340;&#20154;&#24037;&#26631;&#35760;&#26679;&#26412;&#22823;&#23567;&#22686;&#21152;&#20102;50%&#12290;</title><link>https://arxiv.org/abs/2403.07008</link><description>&lt;p&gt;
&#33258;&#21160;&#35780;&#20215;&#27491;&#30830;: &#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
AutoEval Done Right: Using Synthetic Data for Model Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07008
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39640;&#25928;&#21644;&#32479;&#35745;&#19978;&#21512;&#29702;&#30340;&#31639;&#27861;&#65292;&#22312;GPT-4&#23454;&#39564;&#20013;&#26377;&#25928;&#30340;&#20154;&#24037;&#26631;&#35760;&#26679;&#26412;&#22823;&#23567;&#22686;&#21152;&#20102;50%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35780;&#20272;&#20351;&#29992;&#20154;&#24037;&#26631;&#35760;&#30340;&#39564;&#35777;&#25968;&#25454;&#21487;&#33021;&#26082;&#26114;&#36149;&#21448;&#32791;&#26102;&#12290;&#21487;&#20197;&#20351;&#29992;AI&#26631;&#35760;&#30340;&#21512;&#25104;&#25968;&#25454;&#26469;&#20943;&#23569;&#27492;&#31867;&#30446;&#30340;&#20154;&#24037;&#27880;&#37322;&#25968;&#37327;&#65292;&#36825;&#19968;&#36807;&#31243;&#31216;&#20026;&#33258;&#21160;&#35780;&#20272;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#27492;&#30446;&#30340;&#30340;&#39640;&#25928;&#21644;&#32479;&#35745;&#19978;&#21512;&#29702;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#65292;&#21516;&#26102;&#20445;&#25345;&#19981;&#20559;&#12290;&#36825;&#20123;&#31639;&#27861;&#22312;&#19982;GPT-4&#36827;&#34892;&#30340;&#23454;&#39564;&#20013;&#23558;&#26377;&#25928;&#30340;&#20154;&#24037;&#26631;&#35760;&#26679;&#26412;&#22823;&#23567;&#22686;&#21152;&#20102;&#39640;&#36798;50%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07008v1 Announce Type: cross  Abstract: The evaluation of machine learning models using human-labeled validation data can be expensive and time-consuming. AI-labeled synthetic data can be used to decrease the number of human annotations required for this purpose in a process called autoevaluation. We suggest efficient and statistically principled algorithms for this purpose that improve sample efficiency while remaining unbiased. These algorithms increase the effective human-labeled sample size by up to 50% on experiments with GPT-4.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20855;&#26377;&#23618;&#27425;&#32467;&#26500;&#22870;&#21169;&#26426;&#22120;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MAHRM&#65289;&#65292;&#21487;&#20197;&#22788;&#29702;&#26356;&#22797;&#26434;&#30340;&#24773;&#20917;&#65292;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#31616;&#21333;&#23376;&#20219;&#21153;&#30340;&#23618;&#27425;&#32467;&#26500;&#65292;&#20197;&#20943;&#23569;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.07005</link><description>&lt;p&gt;
&#20855;&#26377;&#22870;&#21169;&#26426;&#22120;&#23618;&#27425;&#32467;&#26500;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multi-Agent Reinforcement Learning with a Hierarchy of Reward Machines
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07005
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20855;&#26377;&#23618;&#27425;&#32467;&#26500;&#22870;&#21169;&#26426;&#22120;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MAHRM&#65289;&#65292;&#21487;&#20197;&#22788;&#29702;&#26356;&#22797;&#26434;&#30340;&#24773;&#20917;&#65292;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#31616;&#21333;&#23376;&#20219;&#21153;&#30340;&#23618;&#27425;&#32467;&#26500;&#65292;&#20197;&#20943;&#23569;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21033;&#29992;&#22870;&#21169;&#26426;&#22120;&#65288;RMs&#65289;&#26469;&#25351;&#23450;&#22870;&#21169;&#20989;&#25968;&#65292;&#24182;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#21033;&#29992;&#20219;&#21153;&#20013;&#39640;&#32423;&#20107;&#20214;&#30340;&#20808;&#39564;&#30693;&#35782;&#26469;&#20419;&#36827;&#23398;&#20064;&#25928;&#29575;&#30340;&#21512;&#20316;&#24335;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20855;&#26377;&#23618;&#27425;&#32467;&#26500;&#22870;&#21169;&#26426;&#22120;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MAHRM&#65289;&#65292;&#33021;&#22815;&#22788;&#29702;&#26356;&#22797;&#26434;&#30340;&#22330;&#26223;&#65292;&#20854;&#20013;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#20107;&#20214;&#21487;&#20197;&#21516;&#26102;&#21457;&#29983;&#19988;&#20114;&#30456;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07005v1 Announce Type: new  Abstract: In this paper, we study the cooperative Multi-Agent Reinforcement Learning (MARL) problems using Reward Machines (RMs) to specify the reward functions such that the prior knowledge of high-level events in a task can be leveraged to facilitate the learning efficiency. Unlike the existing work that RMs have been incorporated into MARL for task decomposition and policy learning in relatively simple domains or with an assumption of independencies among the agents, we present Multi-Agent Reinforcement Learning with a Hierarchy of RMs (MAHRM) that is capable of dealing with more complex scenarios when the events among agents can occur concurrently and the agents are highly interdependent.   MAHRM exploits the relationship of high-level events to decompose a task into a hierarchy of simpler subtasks that are assigned to a small group of agents, so as to reduce the overall computational complexity.   Experimental results in three cooperative MAR
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35777;&#26126;&#20102;&#19968;&#20123;&#20984;&#28040;&#24687;&#20256;&#36882;&#31639;&#27861;&#20250;&#25910;&#25947;&#21040;&#22266;&#23450;&#28857;&#65292;&#24182;&#22312;&#19968;&#23450;&#36845;&#20195;&#27425;&#25968;&#20869;&#36798;&#21040;&#29305;&#23450;&#31934;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.07004</link><description>&lt;p&gt;
&#19968;&#20123;&#20984;&#28040;&#24687;&#20256;&#36882;&#31639;&#27861;&#25910;&#25947;&#21040;&#22266;&#23450;&#28857;
&lt;/p&gt;
&lt;p&gt;
Convergence of Some Convex Message Passing Algorithms to a Fixed Point
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07004
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35777;&#26126;&#20102;&#19968;&#20123;&#20984;&#28040;&#24687;&#20256;&#36882;&#31639;&#27861;&#20250;&#25910;&#25947;&#21040;&#22266;&#23450;&#28857;&#65292;&#24182;&#22312;&#19968;&#23450;&#36845;&#20195;&#27425;&#25968;&#20869;&#36798;&#21040;&#29305;&#23450;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22270;&#27169;&#22411;&#20013;&#35299;&#20915;MAP&#25512;&#26029;&#38382;&#39064;&#30340;&#19968;&#31181;&#27969;&#34892;&#26041;&#27861;&#26159;&#36890;&#36807;&#65288;&#22359;&#29366;&#65289;&#22352;&#26631;&#19979;&#38477;&#26368;&#23567;&#21270;&#20174;&#23545;&#20598;&#32447;&#24615;&#35268;&#21010;&#25110;Lagrange&#26494;&#24347;&#20013;&#33719;&#24471;&#30340;&#19968;&#20010;&#19978;&#30028;&#12290;&#36825;&#26679;&#30340;&#31639;&#27861;&#21253;&#25324;&#26368;&#22823;&#21644;&#25193;&#25955;&#20197;&#21450;&#39034;&#24207;&#26641;&#37325;&#26032;&#21152;&#26435;&#28040;&#24687;&#20256;&#36882;&#12290;&#36825;&#20123;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#36136;&#30446;&#21069;&#23578;&#26410;&#23436;&#20840;&#29702;&#35299;&#12290;&#23427;&#20204;&#24050;&#34987;&#35777;&#26126;&#20250;&#25910;&#25947;&#21040;&#30001;&#27963;&#36291;&#32422;&#26463;&#30340;&#23616;&#37096;&#19968;&#33268;&#24615;&#25152;&#34920;&#24449;&#30340;&#38598;&#21512;&#65292;&#20294;&#25910;&#25947;&#36895;&#24230;&#26410;&#30693;&#65307;&#28982;&#32780;&#65292;&#23578;&#19981;&#28165;&#26970;&#36845;&#20195;&#26159;&#21542;&#20250;&#25910;&#25947;&#65288;&#21040;&#20219;&#20309;&#19968;&#20010;&#21333;&#19968;&#28857;&#65289;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;&#26356;&#24378;&#30340;&#32467;&#26524;&#65288;&#20043;&#21069;&#26377;&#29468;&#24819;&#20294;&#20174;&#26410;&#35777;&#26126;&#36807;&#65289;&#65306;&#36845;&#20195;&#20250;&#25910;&#25947;&#21040;&#31639;&#27861;&#30340;&#19968;&#20010;&#22266;&#23450;&#28857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#23427;&#20204;&#22312;$\mathcal{O}(1/\varepsilon)$&#27425;&#36845;&#20195;&#20013;&#36798;&#21040;&#20102;&#31934;&#24230;$\varepsilon&gt;0$&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07004v1 Announce Type: new  Abstract: A popular approach to the MAP inference problem in graphical models is to minimize an upper bound obtained from a dual linear programming or Lagrangian relaxation by (block-)coordinate descent. Examples of such algorithms are max-sum diffusion and sequential tree-reweighted message passing. Convergence properties of these methods are currently not fully understood. They have been proved to converge to the set characterized by local consistency of active constraints, with unknown convergence rate; however, it was not clear if the iterates converge at all (to any single point). We prove a stronger result (which was conjectured before but never proved): the iterates converge to a fixed point of the algorithm. Moreover, we show that they achieve precision $\varepsilon&gt;0$ in $\mathcal{O}(1/\varepsilon)$ iterations.   We first prove this for a version of coordinate descent applied to a general piecewise-affine convex objective, using a novel p
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#26234;&#33021;&#22478;&#24066;&#20840;&#35206;&#30422;&#26234;&#33021;&#24212;&#24613;&#20114;&#21160;&#21709;&#24212;&#31995;&#32479;&#30340;&#30095;&#25955;&#31649;&#29702;&#26694;&#26550;&#65292;&#32467;&#21512;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#26088;&#22312;&#25913;&#36827;&#29616;&#26377;&#30340;&#24212;&#24613;&#21709;&#24212;&#31995;&#32479;&#65292;&#20197;&#25552;&#39640;&#23621;&#27665;&#30340;&#20844;&#20849;&#26381;&#21153;&#21644;&#29983;&#27963;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.07003</link><description>&lt;p&gt;
&#38754;&#21521;&#26234;&#33021;&#22478;&#24066;&#20840;&#35206;&#30422;&#26234;&#33021;&#24212;&#24613;&#20114;&#21160;&#21709;&#24212;&#31995;&#32479;&#30340;&#30095;&#25955;&#31649;&#29702;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Evacuation Management Framework towards Smart City-wide Intelligent Emergency Interactive Response System
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07003
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#26234;&#33021;&#22478;&#24066;&#20840;&#35206;&#30422;&#26234;&#33021;&#24212;&#24613;&#20114;&#21160;&#21709;&#24212;&#31995;&#32479;&#30340;&#30095;&#25955;&#31649;&#29702;&#26694;&#26550;&#65292;&#32467;&#21512;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#26088;&#22312;&#25913;&#36827;&#29616;&#26377;&#30340;&#24212;&#24613;&#21709;&#24212;&#31995;&#32479;&#65292;&#20197;&#25552;&#39640;&#23621;&#27665;&#30340;&#20844;&#20849;&#26381;&#21153;&#21644;&#29983;&#27963;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#38754;&#21521;&#26410;&#26469;6G&#32593;&#32476;&#37096;&#32626;&#30340;&#26234;&#33021;&#22478;&#24066;&#35299;&#20915;&#26041;&#26696;&#65292;&#20801;&#35768;&#20013;&#23567;&#20225;&#19994;&#12289;&#34892;&#19994;&#21644;&#25919;&#24220;&#26426;&#26500;&#19982;&#22522;&#30784;&#35774;&#26045;&#36830;&#25509;&#65292;&#24182;&#22312;&#25552;&#39640;&#24212;&#24613;&#20934;&#22791;&#33021;&#21147;&#26041;&#38754;&#21457;&#25381;&#20851;&#38190;&#20316;&#29992;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20986;&#19968;&#22871;&#21327;&#35843;&#30340;&#25216;&#26415;&#35299;&#20915;&#26041;&#26696;&#65292;&#23558;&#29616;&#26377;&#30340;&#24212;&#24613;&#21709;&#24212;&#31995;&#32479;&#36716;&#21464;&#20026;&#26234;&#33021;&#20114;&#21160;&#31995;&#32479;&#65292;&#20174;&#32780;&#25913;&#21892;&#23621;&#27665;&#22312;&#23478;&#20013;&#12289;&#22312;&#36947;&#36335;&#19978;&#12289;&#22312;&#21307;&#38498;&#12289;&#20132;&#36890;&#26530;&#32445;&#31561;&#22320;&#30340;&#20844;&#20849;&#26381;&#21153;&#21644;&#29983;&#27963;&#36136;&#37327;&#12290;&#22312;&#36825;&#19968;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#20174;&#19982;&#20154;&#20204;&#26085;&#24120;&#29983;&#27963;&#23494;&#20999;&#30456;&#20851;&#30340;&#19977;&#20010;&#19981;&#21516;&#24212;&#29992;&#22330;&#26223;&#30528;&#25163;&#32771;&#34385;&#22478;&#24066;&#20840;&#26223;&#35270;&#35282;&#65292;&#20197;&#20248;&#21270;&#30456;&#20851;&#37096;&#38376;&#37319;&#21462;&#30340;&#34892;&#21160;&#12290;&#22240;&#27492;&#65292;&#37319;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#25216;&#26415;&#26469;&#23454;&#29616;&#19979;&#19968;&#20195;&#20114;&#32852;&#36710;&#36742;&#20307;&#39564;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#21457;&#29983;&#22312;&#24037;&#21378;&#30340;&#20107;&#25925;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07003v1 Announce Type: new  Abstract: A smart city solution toward future 6G network deployment allows small and medium sized enterprises (SMEs), industry, and government entities to connect with the infrastructures and play a crucial role in enhancing emergency preparedness with advanced sensors. The objective of this work is to propose a set of coordinated technological solutions to transform an existing emergency response system into an intelligent interactive system, thereby improving the public services and the quality of life for residents at home, on road, in hospitals, transport hubs, etc. In this context, we consider a city wide view from three different application scenes that are closely related to peoples daily life, to optimize the actions taken at relevant departments. Therefore, using artificial intelligence (AI) and machine learning (ML) techniques to enable the next generation connected vehicle experiences, we specifically focus on accidents happening in ind
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36827;&#34892;&#20102;&#20351;&#29992;&#22810;&#31181;&#29983;&#23384;&#20998;&#26512;&#26041;&#27861;&#30340;&#27604;&#36739;&#30740;&#31350;&#65292;&#21253;&#25324;&#20256;&#32479;&#32479;&#35745;&#27169;&#22411;&#21644;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#26088;&#22312;&#39044;&#27979;&#20837;&#38498;&#21518;&#30340;&#27515;&#20129;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.06999</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#32479;&#35745;&#26041;&#27861;&#30340;&#29983;&#23384;&#24314;&#27169;&#65306;&#39044;&#27979;&#20837;&#38498;&#21518;&#27515;&#20129;&#29575;&#30340;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Survival modeling using deep learning, machine learning and statistical methods: A comparative analysis for predicting mortality after hospital admission
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06999
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36827;&#34892;&#20102;&#20351;&#29992;&#22810;&#31181;&#29983;&#23384;&#20998;&#26512;&#26041;&#27861;&#30340;&#27604;&#36739;&#30740;&#31350;&#65292;&#21253;&#25324;&#20256;&#32479;&#32479;&#35745;&#27169;&#22411;&#21644;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#26088;&#22312;&#39044;&#27979;&#20837;&#38498;&#21518;&#30340;&#27515;&#20129;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#23384;&#20998;&#26512;&#23545;&#20110;&#30740;&#31350;&#20107;&#20214;&#21457;&#29983;&#30340;&#26102;&#38388;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#19988;&#33021;&#22815;&#21160;&#24577;&#22320;&#29702;&#35299;&#20107;&#20214;&#38543;&#26102;&#38388;&#21457;&#29983;&#30340;&#27010;&#29575;&#12290;&#21508;&#31181;&#29983;&#23384;&#20998;&#26512;&#25216;&#26415;&#65292;&#20174;&#20256;&#32479;&#30340;&#32479;&#35745;&#27169;&#22411;&#21040;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#25903;&#25345;&#21307;&#30103;&#24178;&#39044;&#21644;&#25919;&#31574;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#27604;&#36739;&#24615;&#33021;&#20173;&#22312;&#25345;&#32493;&#35752;&#35770;&#20013;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20960;&#31181;&#29983;&#23384;&#20998;&#26512;&#26041;&#27861;&#30340;&#27604;&#36739;&#30740;&#31350;&#65292;&#21253;&#25324;Cox&#27604;&#20363;&#39118;&#38505;&#27169;&#22411;&#65288;CoxPH&#65289;&#12289;&#36880;&#27493;CoxPH&#12289;&#24377;&#24615;&#32593;&#24809;&#32602;Cox&#27169;&#22411;&#12289;&#38543;&#26426;&#29983;&#23384;&#26862;&#26519;&#65288;RSF&#65289;&#12289;&#26799;&#24230;&#25552;&#21319;&#26426;&#22120;&#65288;GBM&#65289;&#23398;&#20064;&#12289;AutoScore-Survival&#12289;DeepSurv&#12289;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26102;&#38388;&#30456;&#20851;Cox&#27169;&#22411;&#65288;CoxTime&#65289;&#20197;&#21450;DeepHit&#29983;&#23384;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#24212;&#29992;&#20102;&#19968;&#33268;&#24615;&#25351;&#25968;&#65288;C&#25351;&#25968;&#65289;&#36827;&#34892;&#27169;&#22411;&#25311;&#21512;&#24230;&#35780;&#20272;&#65292;&#29992;&#31215;&#20998;Brier&#20998;&#25968;&#65288;IBS&#65289;&#36827;&#34892;&#26657;&#20934;&#65292;&#24182;&#32771;&#34385;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06999v1 Announce Type: cross  Abstract: Survival analysis is essential for studying time-to-event outcomes and providing a dynamic understanding of the probability of an event occurring over time. Various survival analysis techniques, from traditional statistical models to state-of-the-art machine learning algorithms, support healthcare intervention and policy decisions. However, there remains ongoing discussion about their comparative performance. We conducted a comparative study of several survival analysis methods, including Cox proportional hazards (CoxPH), stepwise CoxPH, elastic net penalized Cox model, Random Survival Forests (RSF), Gradient Boosting machine (GBM) learning, AutoScore-Survival, DeepSurv, time-dependent Cox model based on neural network (CoxTime), and DeepHit survival neural network. We applied the concordance index (C-index) for model goodness-of-fit, and integral Brier scores (IBS) for calibration, and considered the model interpretability. As a case 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#29289;&#29702;&#20256;&#24863;&#22120;&#30340;&#28145;&#24230;&#23398;&#20064;&#36300;&#20498;&#26816;&#27979;&#31995;&#32479;TSFallDetect&#65292;&#21033;&#29992;&#39034;&#24207;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#36300;&#20498;&#21160;&#20316;&#39044;&#27979;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.06994</link><description>&lt;p&gt;
&#22522;&#20110;&#29289;&#29702;&#20256;&#24863;&#22120;&#30340;&#28145;&#24230;&#23398;&#20064;&#36300;&#20498;&#26816;&#27979;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Physics Sensor Based Deep Learning Fall Detection System
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06994
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#29289;&#29702;&#20256;&#24863;&#22120;&#30340;&#28145;&#24230;&#23398;&#20064;&#36300;&#20498;&#26816;&#27979;&#31995;&#32479;TSFallDetect&#65292;&#21033;&#29992;&#39034;&#24207;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#36300;&#20498;&#21160;&#20316;&#39044;&#27979;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#23884;&#20837;&#24335;&#20256;&#24863;&#22120;&#30340;&#36300;&#20498;&#26816;&#27979;&#26159;&#36817;&#24180;&#26469;&#30340;&#19968;&#20010;&#23454;&#29992;&#19988;&#27969;&#34892;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;TSFallDetect&#30340;&#23436;&#25972;&#31995;&#32479;&#65292;&#21253;&#25324;&#22522;&#20110;&#23884;&#20837;&#24335;&#20256;&#24863;&#22120;&#30340;&#25968;&#25454;&#25509;&#25910;&#35774;&#22791;&#12289;&#31227;&#21160;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#37096;&#32626;&#24179;&#21488;&#20197;&#21450;&#19968;&#20010;&#31616;&#21333;&#30340;&#26381;&#21153;&#22120;&#65292;&#29992;&#20110;&#25910;&#38598;&#27169;&#22411;&#21644;&#25968;&#25454;&#20197;&#36827;&#34892;&#26410;&#26469;&#25193;&#23637;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#39034;&#24207;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#22522;&#20110;&#24815;&#24615;&#21644;&#34180;&#33180;&#21387;&#21147;&#20256;&#24863;&#22120;&#25910;&#38598;&#30340;&#25968;&#25454;&#30340;&#36300;&#20498;&#21160;&#20316;&#39044;&#27979;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06994v1 Announce Type: cross  Abstract: Fall detection based on embedded sensor is a practical and popular research direction in recent years. In terms of a specific application: fall detection methods based upon physics sensors such as [gyroscope and accelerator] have been exploited using traditional hand crafted features and feed them in machine learning models like Markov chain or just threshold based classification methods. In this paper, we build a complete system named TSFallDetect including data receiving device based on embedded sensor, mobile deep-learning model deploying platform, and a simple server, which will be used to gather models and data for future expansion. On the other hand, we exploit the sequential deep-learning methods to address this falling motion prediction problem based on data collected by inertial and film pressure sensors. We make a empirical study based on existing datasets and our datasets collected from our system separately, which shows tha
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#23433;&#20840;&#25935;&#24863;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LSTM&#30340;&#33258;&#21160;&#39550;&#39542;&#36710;&#36947;&#21464;&#26356;&#23433;&#20840;&#39044;&#27979;&#27169;&#22411;&#65292;&#26088;&#22312;&#25552;&#39640;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#39550;&#39542;&#23433;&#20840;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.06993</link><description>&lt;p&gt;
&#22522;&#20110;LSTM&#30340;&#33258;&#21160;&#39550;&#39542;&#36710;&#36947;&#21464;&#26356;&#23433;&#20840;&#39044;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Automatic driving lane change safety prediction model based on LSTM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06993
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#23433;&#20840;&#25935;&#24863;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LSTM&#30340;&#33258;&#21160;&#39550;&#39542;&#36710;&#36947;&#21464;&#26356;&#23433;&#20840;&#39044;&#27979;&#27169;&#22411;&#65292;&#26088;&#22312;&#25552;&#39640;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#39550;&#39542;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#25216;&#26415;&#21487;&#20197;&#25552;&#39640;&#20132;&#36890;&#23433;&#20840;&#65292;&#20943;&#23569;&#20132;&#36890;&#20107;&#25925;&#12290;&#27492;&#22806;&#65292;&#23427;&#25552;&#39640;&#20102;&#20132;&#36890;&#27969;&#37327;&#65292;&#20943;&#23569;&#25317;&#22581;&#65292;&#33410;&#32422;&#33021;&#28304;&#24182;&#25552;&#39640;&#20986;&#34892;&#25928;&#29575;&#12290;&#22312;&#30456;&#23545;&#25104;&#29087;&#30340;&#33258;&#21160;&#39550;&#39542;&#25216;&#26415;&#20013;&#65292;&#33258;&#21160;&#39550;&#39542;&#21151;&#33021;&#20998;&#20026;&#20960;&#20010;&#27169;&#22359;&#65306;&#24863;&#30693;&#12289;&#20915;&#31574;&#12289;&#35268;&#21010;&#21644;&#25511;&#21046;&#65292;&#21512;&#29702;&#30340;&#20998;&#24037;&#21487;&#20197;&#25552;&#39640;&#31995;&#32479;&#30340;&#31283;&#23450;&#24615;&#12290;&#22240;&#27492;&#65292;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#38656;&#35201;&#20855;&#22791;&#39044;&#27979;&#21608;&#22260;&#36710;&#36742;&#36712;&#36857;&#30340;&#33021;&#21147;&#65292;&#20197;&#20570;&#20986;&#21512;&#29702;&#30340;&#20915;&#31574;&#35268;&#21010;&#21644;&#23433;&#20840;&#25514;&#26045;&#65292;&#25552;&#39640;&#39550;&#39542;&#23433;&#20840;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#12289;&#20197;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#32593;&#32476;&#20026;&#22522;&#30784;&#30340;&#23433;&#20840;&#25935;&#24863;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#32531;&#35299;&#24403;&#21069;&#33258;&#21160;&#39550;&#39542;&#36712;&#36857;&#35268;&#21010;&#30340;&#32570;&#28857;&#65292;&#36755;&#20986;&#36712;&#36857;&#19981;&#20165;&#20445;&#35777;&#20102;&#39640;&#20934;&#30830;&#24615;&#65292;&#36824;&#25552;&#21319;&#20102;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06993v1 Announce Type: cross  Abstract: Autonomous driving technology can improve traffic safety and reduce traffic accidents. In addition, it improves traffic flow, reduces congestion, saves energy and increases travel efficiency. In the relatively mature automatic driving technology, the automatic driving function is divided into several modules: perception, decision-making, planning and control, and a reasonable division of labor can improve the stability of the system. Therefore, autonomous vehicles need to have the ability to predict the trajectory of surrounding vehicles in order to make reasonable decision planning and safety measures to improve driving safety. By using deep learning method, a safety-sensitive deep learning model based on short term memory (LSTM) network is proposed. This model can alleviate the shortcomings of current automatic driving trajectory planning, and the output trajectory not only ensures high accuracy but also improves safety. The cell sta
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#26497;&#38480;&#29615;&#25391;&#33633;&#22120;&#30340;&#30456;&#20301;&#33258;&#32534;&#30721;&#22120;&#65292;&#21487;&#20197;&#36890;&#36807;&#35757;&#32451;&#20272;&#35745;&#25391;&#33633;&#22120;&#30340;&#28176;&#36817;&#30456;&#20301;&#21644;&#30456;&#20301;&#25935;&#24863;&#24230;&#20989;&#25968;&#65292;&#20197;&#21450;&#22312;&#21407;&#22987;&#31354;&#38388;&#20013;&#37325;&#24314;&#25391;&#33633;&#22120;&#29366;&#24577;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#26041;&#27861;&#26469;&#23454;&#29616;&#20004;&#20010;&#25391;&#33633;&#22120;&#30340;&#20840;&#23616;&#21516;&#27493;&#12290;</title><link>https://arxiv.org/abs/2403.06992</link><description>&lt;p&gt;
&#38024;&#23545;&#26497;&#38480;&#29615;&#25391;&#33633;&#22120;&#30340;&#30456;&#20301;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Phase autoencoder for limit-cycle oscillators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06992
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#26497;&#38480;&#29615;&#25391;&#33633;&#22120;&#30340;&#30456;&#20301;&#33258;&#32534;&#30721;&#22120;&#65292;&#21487;&#20197;&#36890;&#36807;&#35757;&#32451;&#20272;&#35745;&#25391;&#33633;&#22120;&#30340;&#28176;&#36817;&#30456;&#20301;&#21644;&#30456;&#20301;&#25935;&#24863;&#24230;&#20989;&#25968;&#65292;&#20197;&#21450;&#22312;&#21407;&#22987;&#31354;&#38388;&#20013;&#37325;&#24314;&#25391;&#33633;&#22120;&#29366;&#24577;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#26041;&#27861;&#26469;&#23454;&#29616;&#20004;&#20010;&#25391;&#33633;&#22120;&#30340;&#20840;&#23616;&#21516;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#30456;&#20301;&#33258;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#32534;&#30721;&#26497;&#38480;&#29615;&#25391;&#33633;&#22120;&#30340;&#28176;&#36817;&#30456;&#20301;&#65292;&#36825;&#26159;&#34920;&#24449;&#20854;&#21516;&#27493;&#21160;&#21147;&#23398;&#30340;&#22522;&#26412;&#37327;&#12290;&#35813;&#33258;&#32534;&#30721;&#22120;&#32463;&#36807;&#35757;&#32451;&#65292;&#20351;&#20854;&#28508;&#22312;&#21464;&#37327;&#30452;&#25509;&#34920;&#31034;&#25391;&#33633;&#22120;&#30340;&#28176;&#36817;&#30456;&#20301;&#12290;&#32463;&#36807;&#35757;&#32451;&#30340;&#33258;&#32534;&#30721;&#22120;&#21487;&#20197;&#25191;&#34892;&#20004;&#20010;&#21151;&#33021;&#65292;&#26080;&#38656;&#20381;&#36182;&#25391;&#33633;&#22120;&#30340;&#25968;&#23398;&#27169;&#22411;&#65306;&#39318;&#20808;&#65292;&#23427;&#21487;&#20197;&#35780;&#20272;&#25391;&#33633;&#22120;&#30340;&#28176;&#36817;&#30456;&#20301;&#21644;&#30456;&#20301;&#25935;&#24863;&#24230;&#20989;&#25968;&#65307;&#20854;&#27425;&#65292;&#23427;&#21487;&#20197;&#20174;&#30456;&#20301;&#20540;&#20316;&#20026;&#36755;&#20837;&#65292;&#22312;&#21407;&#22987;&#31354;&#38388;&#20013;&#37325;&#24314;&#26497;&#38480;&#29615;&#19978;&#30340;&#25391;&#33633;&#22120;&#29366;&#24577;&#12290;&#36890;&#36807;&#20960;&#20010;&#26497;&#38480;&#29615;&#25391;&#33633;&#22120;&#30340;&#31034;&#20363;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#35757;&#32451;&#30340;&#33258;&#32534;&#30721;&#22120;&#21482;&#33021;&#20174;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#20272;&#35745;&#20986;&#28176;&#36817;&#30456;&#20301;&#21644;&#30456;&#20301;&#25935;&#24863;&#24230;&#20989;&#25968;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#20316;&#20026;&#35757;&#32451;&#33258;&#32534;&#30721;&#22120;&#30340;&#24212;&#29992;&#65292;&#29992;&#20110;&#20840;&#23616;&#21516;&#27493;&#20004;&#20010;&#25391;&#33633;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06992v1 Announce Type: cross  Abstract: We present a phase autoencoder that encodes the asymptotic phase of a limit-cycle oscillator, a fundamental quantity characterizing its synchronization dynamics. This autoencoder is trained in such a way that its latent variables directly represent the asymptotic phase of the oscillator. The trained autoencoder can perform two functions without relying on the mathematical model of the oscillator: first, it can evaluate the asymptotic phase and phase sensitivity function of the oscillator; second, it can reconstruct the oscillator state on the limit cycle in the original space from the phase value as an input. Using several examples of limit-cycle oscillators, we demonstrate that the asymptotic phase and phase sensitivity function can be estimated only from time-series data by the trained autoencoder. We also present a simple method for globally synchronizing two oscillators as an application of the trained autoencoder.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#30721;&#31639;&#27861;DOMINO&#65292;&#22312;&#29983;&#25104;&#25991;&#26412;&#36807;&#31243;&#20013;&#20197;&#23436;&#20840;&#22522;&#20110;&#23376;&#35789;&#23545;&#40784;&#30340;&#26041;&#24335;&#24378;&#21046;&#25191;&#34892;&#32422;&#26463;&#65292;&#20960;&#20046;&#27809;&#26377;&#24615;&#33021;&#24320;&#38144;&#24182;&#26377;&#26102;&#29978;&#33267;&#23454;&#29616;&#36817;2&#20493;&#36895;&#24230;&#25552;&#21319;&#65292;&#36828;&#36828;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.06988</link><description>&lt;p&gt;
&#24341;&#23548;LLM&#36208;&#21521;&#27491;&#30830;&#20043;&#36335;&#65306;&#24555;&#36895;&#12289;&#38750;&#20405;&#20837;&#24335;&#21463;&#38480;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Guiding LLMs The Right Way: Fast, Non-Invasive Constrained Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06988
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#30721;&#31639;&#27861;DOMINO&#65292;&#22312;&#29983;&#25104;&#25991;&#26412;&#36807;&#31243;&#20013;&#20197;&#23436;&#20840;&#22522;&#20110;&#23376;&#35789;&#23545;&#40784;&#30340;&#26041;&#24335;&#24378;&#21046;&#25191;&#34892;&#32422;&#26463;&#65292;&#20960;&#20046;&#27809;&#26377;&#24615;&#33021;&#24320;&#38144;&#24182;&#26377;&#26102;&#29978;&#33267;&#23454;&#29616;&#36817;2&#20493;&#36895;&#24230;&#25552;&#21319;&#65292;&#36828;&#36828;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#30830;&#20445;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#30340;&#25991;&#26412;&#31526;&#21512;&#39044;&#26399;&#26684;&#24335;&#65292;&#21463;&#38480;&#35299;&#30721;&#25552;&#20986;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#24378;&#21046;&#25191;&#34892;&#20005;&#26684;&#30340;&#24418;&#24335;&#35821;&#35328;&#32422;&#26463;&#12290;&#28982;&#32780;&#65292;&#27491;&#22914;&#25105;&#20204;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#25152;&#23637;&#31034;&#30340;&#65292;&#36825;&#31867;&#26041;&#27861;&#19981;&#20165;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#20135;&#29983;&#24615;&#33021;&#24320;&#38144;&#65292;&#32780;&#19988;&#35768;&#22810;&#26041;&#27861;&#22914;&#26524;&#27809;&#26377;&#27491;&#30830;&#22320;&#23558;LLM&#23376;&#35789;&#35789;&#27719;&#19982;&#22806;&#37096;&#32422;&#26463;&#23545;&#40784;&#65292;&#21017;&#36824;&#20250;&#26174;&#33879;&#25439;&#23475;&#20219;&#21153;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#30721;&#31639;&#27861;DOMINO&#65292;&#21487;&#20197;&#20197;&#23436;&#20840;&#22522;&#20110;&#23376;&#35789;&#23545;&#40784;&#30340;&#26041;&#24335;&#24378;&#21046;&#25191;&#34892;&#32422;&#26463;&#65292;&#21516;&#26102;&#21033;&#29992;&#39044;&#35745;&#31639;&#21644;&#25512;&#27979;&#35299;&#30721;&#26469;&#23454;&#29616;&#20960;&#20046;&#38646;&#24320;&#38144;&#65292;&#26377;&#26102;&#29978;&#33267;&#27604;&#19981;&#21463;&#38480;&#21046;&#30340;&#35299;&#30721;&#24555;&#36817;2&#20493;&#65292;&#20174;&#32780;&#36828;&#36828;&#36229;&#36807;&#29616;&#26377;&#26041;&#27861;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06988v1 Announce Type: cross  Abstract: To ensure that text generated by large language models (LLMs) is in an expected format, constrained decoding proposes to enforce strict formal language constraints during generation. However, as we show in this work, not only do such methods incur performance overhead during generation, but many of them also significantly impair task accuracy, if they do not correctly align the underlying LLM sub-word vocabularies with external constraints. To address this, we present a novel decoding algorithm, DOMINO, that can enforce constraints in a fully subword-aligned fashion, while leveraging pre-computation and speculative decoding to achieve virtually no overhead and in some cases even almost 2$\times$ speedup over unconstrained decoding -- thereby outperforming existing approaches by a wide margin.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#30340;&#37325;&#32534;&#31243;&#29305;&#24615;&#65292;&#30740;&#31350;&#32773;&#25104;&#21151;&#23558;&#21407;&#26412;&#35774;&#35745;&#29992;&#20110;&#20581;&#20840;&#20154;&#30340;&#27169;&#22411;&#25913;&#32534;&#20026;&#36866;&#29992;&#20110;&#25130;&#32930;&#32773;&#39044;&#27979;&#20851;&#33410;&#36816;&#21160;&#12290;</title><link>https://arxiv.org/abs/2403.06569</link><description>&lt;p&gt;
&#36890;&#36807;&#27169;&#22411;&#37325;&#32534;&#31243;&#22686;&#24378;&#20551;&#32930;&#20351;&#29992;&#32773;&#30340;&#20851;&#33410;&#36816;&#21160;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Enhancing Joint Motion Prediction for Individuals with Limb Loss Through Model Reprogramming
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06569
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#30340;&#37325;&#32534;&#31243;&#29305;&#24615;&#65292;&#30740;&#31350;&#32773;&#25104;&#21151;&#23558;&#21407;&#26412;&#35774;&#35745;&#29992;&#20110;&#20581;&#20840;&#20154;&#30340;&#27169;&#22411;&#25913;&#32534;&#20026;&#36866;&#29992;&#20110;&#25130;&#32930;&#32773;&#39044;&#27979;&#20851;&#33410;&#36816;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32930;&#20307;&#20007;&#22833;&#23548;&#33268;&#30340;&#34892;&#21160;&#38556;&#30861;&#26159;&#20840;&#29699;&#25968;&#30334;&#19975;&#20154;&#38754;&#20020;&#30340;&#37325;&#22823;&#25361;&#25112;&#12290;&#20808;&#36827;&#30340;&#36741;&#21161;&#25216;&#26415;&#65288;&#22914;&#20551;&#32930;&#35774;&#22791;&#65289;&#30340;&#24320;&#21457;&#26377;&#21487;&#33021;&#22823;&#22823;&#25552;&#39640;&#25130;&#32930;&#24739;&#32773;&#30340;&#29983;&#27963;&#36136;&#37327;&#12290;&#35774;&#35745;&#36825;&#31867;&#25216;&#26415;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#26159;&#20934;&#30830;&#39044;&#27979;&#32570;&#22833;&#32930;&#20307;&#30340;&#21442;&#32771;&#20851;&#33410;&#36816;&#21160;&#12290;&#28982;&#32780;&#65292;&#19982;&#22823;&#37327;&#26469;&#33258;&#20581;&#20840;&#21463;&#35797;&#32773;&#30340;&#25968;&#25454;&#24418;&#25104;&#23545;&#27604;&#65292;&#25130;&#32930;&#24739;&#32773;&#20851;&#33410;&#36816;&#21160;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#38459;&#30861;&#20102;&#36825;&#19968;&#20219;&#21153;&#30340;&#23436;&#25104;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38556;&#30861;&#65292;&#25105;&#20204;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#37325;&#32534;&#31243;&#29305;&#24615;&#65292;&#26080;&#38656;&#25913;&#21464;&#27169;&#22411;&#21442;&#25968;&#21363;&#21487;&#37325;&#26032;&#21033;&#29992;&#35757;&#32451;&#26377;&#32032;&#30340;&#27169;&#22411;&#23454;&#29616;&#26032;&#30446;&#26631;&#12290;&#36890;&#36807;&#20165;&#22312;&#25968;&#25454;&#32423;&#21035;&#36827;&#34892;&#25805;&#20316;&#65292;&#25105;&#20204;&#23558;&#21407;&#26412;&#35774;&#35745;&#29992;&#20110;&#20581;&#20840;&#20154;&#30340;&#27169;&#22411;&#25913;&#32534;&#20026;&#36866;&#29992;&#20110;&#25130;&#32930;&#32773;&#39044;&#27979;&#20851;&#33410;&#36816;&#21160;&#12290;&#26412;&#30740;&#31350;&#32467;&#26524;&#23545;&#20110;&#25512;&#36827;&#36741;&#21161;&#25216;&#26415;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06569v1 Announce Type: new  Abstract: Mobility impairment caused by limb loss is a significant challenge faced by millions of individuals worldwide. The development of advanced assistive technologies, such as prosthetic devices, has the potential to greatly improve the quality of life for amputee patients. A critical component in the design of such technologies is the accurate prediction of reference joint motion for the missing limb. However, this task is hindered by the scarcity of joint motion data available for amputee patients, in contrast to the substantial quantity of data from able-bodied subjects. To overcome this, we leverage deep learning's reprogramming property to repurpose well-trained models for a new goal without altering the model parameters. With only data-level manipulation, we adapt models originally designed for able-bodied people to forecast joint motion in amputees. The findings in this study have significant implications for advancing assistive tech a
&lt;/p&gt;</description></item><item><title>DeepSafeMPC&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#26041;&#27861;&#65292;&#26088;&#22312;&#26377;&#25928;&#39044;&#27979;&#22810;&#26234;&#20307;&#29615;&#22659;&#30340;&#22797;&#26434;&#21160;&#24577;&#65292;&#24182;&#24212;&#29992;MARL&#21407;&#21017;&#23547;&#25214;&#26368;&#20248;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.06397</link><description>&lt;p&gt;
DeepSafeMPC: &#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#23433;&#20840;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
DeepSafeMPC: Deep Learning-Based Model Predictive Control for Safe Multi-Agent Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06397
&lt;/p&gt;
&lt;p&gt;
DeepSafeMPC&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#26041;&#27861;&#65292;&#26088;&#22312;&#26377;&#25928;&#39044;&#27979;&#22810;&#26234;&#20307;&#29615;&#22659;&#30340;&#22797;&#26434;&#21160;&#24577;&#65292;&#24182;&#24212;&#29992;MARL&#21407;&#21017;&#23547;&#25214;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;safe MARL&#65289;&#22312;&#26368;&#36817;&#20960;&#24180;&#36880;&#28176;&#21463;&#21040;&#20851;&#27880;&#65292;&#24378;&#35843;&#20102;&#26234;&#20307;&#19981;&#20165;&#38656;&#35201;&#20248;&#21270;&#20840;&#23616;&#22238;&#25253;&#65292;&#36824;&#38656;&#35201;&#36890;&#36807;&#34892;&#20026;&#32422;&#26463;&#36981;&#23432;&#23433;&#20840;&#35201;&#27714;&#30340;&#24517;&#35201;&#24615;&#12290;&#36817;&#26399;&#19968;&#20123;&#24037;&#20316;&#23558;&#25511;&#21046;&#29702;&#35770;&#19982;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#20197;&#35299;&#20915;&#30830;&#20445;&#23433;&#20840;&#24615;&#30340;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#19968;&#39046;&#22495;&#20013;&#24212;&#29992;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;MPC&#65289;&#26041;&#27861;&#30340;&#24212;&#29992;&#38750;&#24120;&#26377;&#38480;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#22810;&#26234;&#20307;&#29615;&#22659;&#20013;&#22797;&#26434;&#19988;&#38544;&#24335;&#21160;&#24577;&#30340;&#29305;&#24615;&#12290;&#20026;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#23433;&#20840;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;DeepSafeMPC&#65289;&#30340;&#26032;&#26041;&#27861;&#12290;DeepSafeMPC &#30340;&#20851;&#38190;&#35265;&#35299;&#26159;&#21033;&#29992;&#38598;&#20013;&#24335;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24456;&#22909;&#22320;&#39044;&#27979;&#29615;&#22659;&#21160;&#24577;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;MARL&#21407;&#21017;&#26469;&#23547;&#25214;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06397v1 Announce Type: cross  Abstract: Safe Multi-agent reinforcement learning (safe MARL) has increasingly gained attention in recent years, emphasizing the need for agents to not only optimize the global return but also adhere to safety requirements through behavioral constraints. Some recent work has integrated control theory with multi-agent reinforcement learning to address the challenge of ensuring safety. However, there have been only very limited applications of Model Predictive Control (MPC) methods in this domain, primarily due to the complex and implicit dynamics characteristic of multi-agent environments. To bridge this gap, we propose a novel method called Deep Learning-Based Model Predictive Control for Safe Multi-Agent Reinforcement Learning (DeepSafeMPC). The key insight of DeepSafeMPC is leveraging a entralized deep learning model to well predict environmental dynamics. Our method applies MARL principles to search for optimal solutions. Through the employme
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#33268;&#21147;&#20110;&#30740;&#31350;&#36830;&#32493;&#26102;&#38388;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#29109;&#27491;&#21017;&#21270;&#24494;&#35843;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#20998;&#26512;&#22914;&#20309;&#25193;&#23637;&#21040;&#28041;&#21450;&#19968;&#33324;$f$-&#25955;&#24230;&#27491;&#21017;&#21270;&#30340;&#24494;&#35843;&#20013;&#12290;</title><link>https://arxiv.org/abs/2403.06279</link><description>&lt;p&gt;
&#36890;&#36807;&#38543;&#26426;&#25511;&#21046;&#36827;&#34892;&#25193;&#25955;&#27169;&#22411;&#30340;&#24494;&#35843;&#65306;&#29109;&#27491;&#21017;&#21270;&#21450;&#26356;&#22810;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning of diffusion models via stochastic control: entropy regularization and beyond
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06279
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#33268;&#21147;&#20110;&#30740;&#31350;&#36830;&#32493;&#26102;&#38388;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#29109;&#27491;&#21017;&#21270;&#24494;&#35843;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#20998;&#26512;&#22914;&#20309;&#25193;&#23637;&#21040;&#28041;&#21450;&#19968;&#33324;$f$-&#25955;&#24230;&#27491;&#21017;&#21270;&#30340;&#24494;&#35843;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#21457;&#23637;&#24182;&#23545;&#36830;&#32493;&#26102;&#38388;&#25193;&#25955;&#27169;&#22411;&#20013;&#29109;&#27491;&#21017;&#21270;&#24494;&#35843;&#38382;&#39064;&#36827;&#34892;&#20005;&#26684;&#22788;&#29702;&#65292;&#35813;&#38382;&#39064;&#26368;&#36817;&#30001;&#19978;&#21407;&#31561;&#20154;&#25552;&#20986;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#20998;&#26512;&#25193;&#23637;&#21040;&#28041;&#21450;&#19968;&#33324;$f$-&#25955;&#24230;&#27491;&#21017;&#21270;&#30340;&#24494;&#35843;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06279v1 Announce Type: cross  Abstract: This paper aims to develop and provide a rigorous treatment to the problem of entropy regularized fine-tuning in the context of continuous-time diffusion models, which was recently proposed by Uehara et al. ( arXiv:2402.15194, 2024). We also show how the analysis can be extended to fine-tuning involving a general $f$-divergence regularizer.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#31163;&#21453;&#21521;&#36807;&#31243;&#21644;&#25968;&#25454;&#19968;&#33268;&#24615;&#27493;&#39588;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#22270;&#20687;&#24674;&#22797;&#27714;&#35299;&#22120;&#12290;</title><link>https://arxiv.org/abs/2403.06054</link><description>&lt;p&gt;
&#20855;&#26377;&#25193;&#25955;&#20928;&#21270;&#30340;&#20998;&#31163;&#25968;&#25454;&#19968;&#33268;&#24615;&#30340;&#22270;&#20687;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
Decoupled Data Consistency with Diffusion Purification for Image Restoration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06054
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#31163;&#21453;&#21521;&#36807;&#31243;&#21644;&#25968;&#25454;&#19968;&#33268;&#24615;&#27493;&#39588;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#22270;&#20687;&#24674;&#22797;&#27714;&#35299;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#28145;&#24230;&#29983;&#25104;&#20808;&#39564;&#31867;&#21035;&#24050;&#32463;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#65292;&#30001;&#20110;&#20854;&#20986;&#33394;&#22320;&#24314;&#27169;&#25968;&#25454;&#20998;&#24067;&#30340;&#33021;&#21147;&#65292;&#22312;&#21508;&#31181;&#22270;&#20687;&#24674;&#22797;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#20026;&#20102;&#35299;&#20915;&#22270;&#20687;&#24674;&#22797;&#38382;&#39064;&#65292;&#35768;&#22810;&#29616;&#26377;&#25216;&#26415;&#36890;&#36807;&#23558;&#39069;&#22806;&#30340;&#20284;&#28982;&#26799;&#24230;&#27493;&#39588;&#32435;&#20837;&#21040;&#25193;&#25955;&#27169;&#22411;&#30340;&#21453;&#21521;&#37319;&#26679;&#36807;&#31243;&#20013;&#26469;&#23454;&#29616;&#25968;&#25454;&#19968;&#33268;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#39069;&#22806;&#30340;&#26799;&#24230;&#27493;&#39588;&#23545;&#20110;&#23454;&#38469;&#24212;&#29992;&#20013;&#23384;&#22312;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#36896;&#25104;&#20102;&#24040;&#22823;&#30340;&#35745;&#31639;&#24320;&#38144;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#25512;&#29702;&#26102;&#38388;&#12290;&#24403;&#20351;&#29992;&#21152;&#36895;&#30340;&#25193;&#25955;&#27169;&#22411;&#37319;&#26679;&#22120;&#26102;&#65292;&#36825;&#20123;&#39069;&#22806;&#30340;&#27493;&#39588;&#36824;&#20250;&#23548;&#33268;&#39069;&#22806;&#30340;&#22256;&#38590;&#65292;&#22240;&#20026;&#25968;&#25454;&#19968;&#33268;&#24615;&#27493;&#39588;&#30340;&#25968;&#37327;&#21463;&#38480;&#20110;&#21453;&#21521;&#37319;&#26679;&#27493;&#39588;&#30340;&#25968;&#37327;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#22270;&#20687;&#24674;&#22797;&#27714;&#35299;&#22120;&#65292;&#36890;&#36807;&#23558;&#21453;&#21521;&#36807;&#31243;&#19982;&#25968;&#25454;&#19968;&#33268;&#24615;&#27493;&#39588;&#20998;&#31163;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06054v1 Announce Type: cross  Abstract: Diffusion models have recently gained traction as a powerful class of deep generative priors, excelling in a wide range of image restoration tasks due to their exceptional ability to model data distributions. To solve image restoration problems, many existing techniques achieve data consistency by incorporating additional likelihood gradient steps into the reverse sampling process of diffusion models. However, the additional gradient steps pose a challenge for real-world practical applications as they incur a large computational overhead, thereby increasing inference time. They also present additional difficulties when using accelerated diffusion model samplers, as the number of data consistency steps is limited by the number of reverse sampling steps. In this work, we propose a novel diffusion-based image restoration solver that addresses these issues by decoupling the reverse process from the data consistency steps. Our method involv
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24320;&#28304;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;&#36890;&#29992;&#22806;&#31185;&#35270;&#39057;&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;&#29992;&#20110;&#22806;&#31185;&#24212;&#29992;&#30340;&#35270;&#39057;&#39044;&#35757;&#32451;&#36890;&#29992;&#22806;&#31185;&#35270;&#35273;&#21464;&#25442;&#22120;&#65288;GSViT&#65289;&#25216;&#26415;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;Cholec80&#38454;&#27573;&#27880;&#37322;&#20219;&#21153;&#19978;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.05949</link><description>&lt;p&gt;
&#36890;&#29992;&#22806;&#31185;&#35270;&#35273;&#21464;&#25442;&#22120;&#65306;&#29992;&#20110;&#36890;&#29992;&#22806;&#31185;&#30340;&#35270;&#39057;&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
General surgery vision transformer: A video pre-trained foundation model for general surgery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05949
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24320;&#28304;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;&#36890;&#29992;&#22806;&#31185;&#35270;&#39057;&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;&#29992;&#20110;&#22806;&#31185;&#24212;&#29992;&#30340;&#35270;&#39057;&#39044;&#35757;&#32451;&#36890;&#29992;&#22806;&#31185;&#35270;&#35273;&#21464;&#25442;&#22120;&#65288;GSViT&#65289;&#25216;&#26415;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;Cholec80&#38454;&#27573;&#27880;&#37322;&#20219;&#21153;&#19978;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32570;&#20047;&#24320;&#25918;&#33719;&#21462;&#30340;&#25968;&#25454;&#21644;&#19987;&#38376;&#30340;&#22522;&#30784;&#27169;&#22411;&#26159;&#22806;&#31185;&#35745;&#31639;&#30740;&#31350;&#30340;&#20027;&#35201;&#38556;&#30861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#28304;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;&#36890;&#29992;&#22806;&#31185;&#35270;&#39057;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#26469;&#33258;28&#31181;&#25163;&#26415;&#25216;&#26415;&#30340;680&#23567;&#26102;&#25163;&#26415;&#35270;&#39057;&#25968;&#25454;&#65307;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21069;&#21521;&#35270;&#39057;&#39044;&#27979;&#30340;&#36890;&#29992;&#22806;&#31185;&#35270;&#35273;&#21464;&#25442;&#22120;&#65288;GSViT&#65289;&#35270;&#39057;&#39044;&#35757;&#32451;&#25216;&#26415;&#65292;&#21487;&#23454;&#26102;&#36816;&#34892;&#29992;&#20110;&#22806;&#31185;&#24212;&#29992;&#65292;&#25105;&#20204;&#36824;&#24320;&#28304;&#20102;GSViT&#30340;&#20195;&#30721;&#21644;&#26435;&#37325;&#65307;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#38024;&#23545;10&#31181;&#25163;&#26415;&#31243;&#24207;&#30340;&#29305;&#23450;&#31243;&#24207;&#24494;&#35843;&#29256;&#26412;&#30340;GSViT&#30340;&#20195;&#30721;&#21644;&#26435;&#37325;&#65307;&#25105;&#20204;&#23637;&#31034;&#20102;GSViT&#22312;Cholec80&#38454;&#27573;&#27880;&#37322;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#26174;&#31034;&#20986;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#21333;&#24103;&#39044;&#27979;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05949v1 Announce Type: cross  Abstract: The absence of openly accessible data and specialized foundation models is a major barrier for computational research in surgery. Toward this, (i) we open-source the largest dataset of general surgery videos to-date, consisting of 680 hours of surgical videos, including data from robotic and laparoscopic techniques across 28 procedures; (ii) we propose a technique for video pre-training a general surgery vision transformer (GSViT) on surgical videos based on forward video prediction that can run in real-time for surgical applications, toward which we open-source the code and weights of GSViT; (iii) we also release code and weights for procedure-specific fine-tuned versions of GSViT across 10 procedures; (iv) we demonstrate the performance of GSViT on the Cholec80 phase annotation task, displaying improved performance over state-of-the-art single frame predictors.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#27531;&#24046;&#32593;&#32476;&#30340;&#25193;&#25955;&#24314;&#27169;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#19981;&#24179;&#34913;&#25968;&#25454;&#65292;&#20811;&#26381;&#20102;&#32463;&#20856;&#36807;&#37319;&#26679;&#26041;&#27861;&#21644;&#22522;&#20110;&#29983;&#25104;&#32593;&#32476;&#30340;&#27169;&#24335;&#22604;&#38519;&#19982;&#35757;&#32451;&#19981;&#31283;&#23450;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.05918</link><description>&lt;p&gt;
&#22522;&#20110;&#27531;&#24046;&#32593;&#32476;&#30340;&#25193;&#25955;&#24314;&#27169;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#19978;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
SEMRes-DDPM: Residual Network Based Diffusion Modelling Applied to Imbalanced Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05918
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27531;&#24046;&#32593;&#32476;&#30340;&#25193;&#25955;&#24314;&#27169;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#19981;&#24179;&#34913;&#25968;&#25454;&#65292;&#20811;&#26381;&#20102;&#32463;&#20856;&#36807;&#37319;&#26679;&#26041;&#27861;&#21644;&#22522;&#20110;&#29983;&#25104;&#32593;&#32476;&#30340;&#27169;&#24335;&#22604;&#38519;&#19982;&#35757;&#32451;&#19981;&#31283;&#23450;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#25454;&#25366;&#25496;&#21644;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#36890;&#24120;&#20351;&#29992;&#30340;&#20998;&#31867;&#27169;&#22411;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#20013;&#26080;&#27861;&#26377;&#25928;&#23398;&#20064;&#12290;&#20026;&#20102;&#24179;&#34913;&#27169;&#22411;&#35757;&#32451;&#21069;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#36890;&#24120;&#20351;&#29992;&#36807;&#37319;&#26679;&#26041;&#27861;&#20026;&#23569;&#25968;&#31867;&#29983;&#25104;&#25968;&#25454;&#65292;&#20197;&#35299;&#20915;&#20998;&#31867;&#19981;&#24179;&#34913;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#22823;&#22810;&#25968;&#32463;&#20856;&#30340;&#36807;&#37319;&#26679;&#26041;&#27861;&#22522;&#20110;SMOTE&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#20165;&#20851;&#27880;&#25968;&#25454;&#30340;&#23616;&#37096;&#20449;&#24687;&#65292;&#22240;&#27492;&#29983;&#25104;&#30340;&#25968;&#25454;&#21487;&#33021;&#23384;&#22312;&#19981;&#22815;&#36924;&#30495;&#30340;&#38382;&#39064;&#12290;&#22312;&#22522;&#20110;&#29983;&#25104;&#32593;&#32476;&#30340;&#24403;&#21069;&#36807;&#37319;&#26679;&#26041;&#27861;&#20013;&#65292;&#22522;&#20110;GAN&#30340;&#26041;&#27861;&#21487;&#20197;&#25429;&#33719;&#25968;&#25454;&#30340;&#30495;&#23454;&#20998;&#24067;&#65292;&#20294;&#35757;&#32451;&#20013;&#23384;&#22312;&#27169;&#24335;&#23849;&#28291;&#21644;&#19981;&#31283;&#23450;&#24615;&#30340;&#38382;&#39064;&#65307;&#22522;&#20110;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30340;&#36807;&#37319;&#26679;&#26041;&#27861;&#20013;&#65292;&#20351;&#29992;U-Net&#30340;&#36870;&#25193;&#25955;&#36807;&#31243;&#31070;&#32463;&#32593;&#32476;&#19981;&#36866;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05918v1 Announce Type: cross  Abstract: In the field of data mining and machine learning, commonly used classification models cannot effectively learn in unbalanced data. In order to balance the data distribution before model training,oversamplingmethods are often used to generate data for a small number of classes to solve the problem of classifying unbalanced data. Most of the classical oversampling methods are based on theSMOTE technique, which only focuses on the local information of the data, and therefore the generated data may have the problem of not being realistic enough. In the current oversampling methods based on generative networks, the methods based on GANs can capture the true distribution of data, but there is the problem of pattern collapse and training instability in training; in the oversampling methods based on denoising diffusion probability models, the neural network of the inverse diffusion process using the U-Net is not applicable to tabular data, and
&lt;/p&gt;</description></item><item><title>PR-NET&#27169;&#22411;&#36890;&#36807;&#21387;&#32553;&#21644;&#20248;&#21270;P-NET&#30340;&#32593;&#32476;&#32467;&#26500;&#65292;&#38477;&#20302;&#20102;&#27169;&#22411;&#22797;&#26434;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#22312;&#21069;&#21015;&#33146;&#30284;&#24739;&#32773;&#29366;&#20917;&#39044;&#27979;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.05818</link><description>&lt;p&gt;
PR-NET&#65306;&#21033;&#29992;&#31934;&#32454;&#21270;&#36890;&#36335;&#32593;&#32476;&#32467;&#26500;&#36827;&#34892;&#21069;&#21015;&#33146;&#30284;&#24739;&#32773;&#29366;&#20917;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
PR-NET: Leveraging Pathway Refined Network Structures for Prostate Cancer Patient Condition Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05818
&lt;/p&gt;
&lt;p&gt;
PR-NET&#27169;&#22411;&#36890;&#36807;&#21387;&#32553;&#21644;&#20248;&#21270;P-NET&#30340;&#32593;&#32476;&#32467;&#26500;&#65292;&#38477;&#20302;&#20102;&#27169;&#22411;&#22797;&#26434;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#22312;&#21069;&#21015;&#33146;&#30284;&#24739;&#32773;&#29366;&#20917;&#39044;&#27979;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#26426;&#65306;&#35786;&#26029;&#21644;&#30417;&#27979;&#21435;&#21183;&#25269;&#25239;&#24615;&#21069;&#21015;&#33146;&#30284;&#65288;CRPC&#65289;&#23545;&#30284;&#30151;&#24739;&#32773;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#30446;&#21069;&#30340;&#27169;&#22411;&#65288;&#22914;P-NET&#65289;&#22312;&#21442;&#25968;&#25968;&#37327;&#12289;&#27867;&#21270;&#33021;&#21147;&#21644;&#25104;&#26412;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#32467;&#26524;&#65306;&#20026;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26356;&#20934;&#30830;&#39640;&#25928;&#30340;&#21069;&#21015;&#33146;&#30284;&#24739;&#32773;&#29366;&#20917;&#39044;&#27979;&#27169;&#22411;&#65292;&#21517;&#20026;PR-NET&#12290;&#36890;&#36807;&#21387;&#32553;&#21644;&#20248;&#21270;P-NET&#30340;&#32593;&#32476;&#32467;&#26500;&#65292;&#38477;&#20302;&#20102;&#27169;&#22411;&#22797;&#26434;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#39640;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;PR-NET&#22312;&#39044;&#27979;&#21069;&#21015;&#33146;&#30284;&#24739;&#32773;&#32467;&#26524;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#36229;&#36807;&#20102;P-NET&#21644;&#20854;&#20182;&#20845;&#31181;&#20256;&#32479;&#27169;&#22411;&#26174;&#33879;&#12290;&#22312;&#25105;&#20204;&#30340;&#20005;&#26684;&#35780;&#20272;&#20013;&#65292;PR-NET&#19981;&#20165;&#22312;&#24050;&#30693;&#25968;&#25454;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24179;&#22343;AUC&#21644;&#21484;&#22238;&#29575;&#20998;&#25968;&#65288;&#20998;&#21035;&#20026;0.94&#21644;0.83&#65289;&#65292;&#32780;&#19988;&#22312;&#20116;&#20010;&#26410;&#30693;&#25968;&#25454;&#38598;&#19978;&#20445;&#25345;&#20102;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24179;&#22343;AUC&#20026;0.73&#65292;&#21484;&#22238;&#29575;&#20026;0
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05818v1 Announce Type: new  Abstract: Motivation: The diagnosis and monitoring of Castrate Resistant Prostate Cancer (CRPC) are crucial for cancer patients, but the current models (such as P-NET) have limitations in terms of parameter count, generalization, and cost. Results: To address the above issues, we develop a more accurate and efficient Prostate Cancer patient condition prediction model, named PR-NET. By compressing and optimizing the network structure of P-NET, the model complexity is reduced while maintaining high accuracy and interpretability. The PR-NET demonstrated superior performance in predicting prostate cancer patient outcomes, outshining P-NET and six other traditional models with a significant margin. In our rigorous evaluation, PR-NET not only achieved impressive average AUC and Recall scores of 0.94 and 0.83, respectively, on known data but also maintained robust generalizability on five unknown datasets with a higher average AUC of 0.73 and Recall of 0
&lt;/p&gt;</description></item><item><title>GEAR&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;KV&#32531;&#23384;&#21387;&#32553;&#26694;&#26550;&#65292;&#23454;&#29616;&#20960;&#20046;&#26080;&#25439;&#30340;&#39640;&#27604;&#29575;&#21387;&#32553;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#20013;&#22240;&#32531;&#23384;&#38656;&#27714;&#22686;&#38271;&#32780;&#23548;&#33268;&#30340;&#35760;&#24518;&#32465;&#23450;&#38382;&#39064;&#21644;&#24615;&#33021;&#19979;&#38477;&#12290;</title><link>https://arxiv.org/abs/2403.05527</link><description>&lt;p&gt;
GEAR: &#19968;&#31181;&#29992;&#20110;&#20960;&#20046;&#26080;&#25439;&#29983;&#25104;&#25512;&#26029;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;KV&#32531;&#23384;&#21387;&#32553;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
GEAR: An Efficient KV Cache Compression Recipefor Near-Lossless Generative Inference of LLM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05527
&lt;/p&gt;
&lt;p&gt;
GEAR&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;KV&#32531;&#23384;&#21387;&#32553;&#26694;&#26550;&#65292;&#23454;&#29616;&#20960;&#20046;&#26080;&#25439;&#30340;&#39640;&#27604;&#29575;&#21387;&#32553;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#20013;&#22240;&#32531;&#23384;&#38656;&#27714;&#22686;&#38271;&#32780;&#23548;&#33268;&#30340;&#35760;&#24518;&#32465;&#23450;&#38382;&#39064;&#21644;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#38190;-&#20540;&#65288;KV&#65289;&#32531;&#23384;&#24050;&#25104;&#20026;&#21152;&#24555;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25512;&#26029;&#29983;&#25104;&#36895;&#24230;&#30340;&#20107;&#23454;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#24207;&#21015;&#38271;&#24230;&#22686;&#21152;&#32780;&#22686;&#38271;&#30340;&#32531;&#23384;&#38656;&#27714;&#24050;&#23558;LLM&#25512;&#26029;&#36716;&#21464;&#20026;&#19968;&#20010;&#35760;&#24518;&#32465;&#23450;&#38382;&#39064;&#65292;&#26174;&#33879;&#22320;&#38480;&#21046;&#20102;&#31995;&#32479;&#21534;&#21520;&#37327;&#12290;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#20110;&#20002;&#24323;&#19981;&#37325;&#35201;&#30340;&#26631;&#35760;&#25110;&#22343;&#21248;&#37327;&#21270;&#25152;&#26377;&#26465;&#30446;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#24448;&#24448;&#20250;&#20135;&#29983;&#36739;&#39640;&#30340;&#36817;&#20284;&#35823;&#24046;&#26469;&#34920;&#31034;&#21387;&#32553;&#21518;&#30340;&#30697;&#38453;&#12290;&#33258;&#22238;&#24402;&#35299;&#30721;&#36807;&#31243;&#36827;&#19968;&#27493;&#22686;&#21152;&#20102;&#27599;&#20010;&#27493;&#39588;&#30340;&#35823;&#24046;&#65292;&#23548;&#33268;&#27169;&#22411;&#29983;&#25104;&#20013;&#30340;&#37325;&#22823;&#20559;&#24046;&#21644;&#24615;&#33021;&#24694;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GEAR&#65292;&#19968;&#31181;&#39640;&#25928;&#30340;KV&#32531;&#23384;&#21387;&#32553;&#26694;&#26550;&#65292;&#23454;&#29616;&#20960;&#20046;&#26080;&#25439;&#30340;&#39640;&#21387;&#32553;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05527v1 Announce Type: cross  Abstract: Key-value (KV) caching has become the de-facto to accelerate generation speed for large language models (LLMs) inference. However, the growing cache demand with increasing sequence length has transformed LLM inference to be a memory bound problem, significantly constraining the system throughput. Existing methods rely on dropping unimportant tokens or quantizing all entries uniformly. Such methods, however, often incur high approximation errors to represent the compressed matrices. The autoregressive decoding process further compounds the error of each step, resulting in critical deviation in model generation and deterioration of performance. To tackle this challenge, we propose GEAR, an efficient KV cache compression framework that achieves near-lossless high-ratio compression. GEAR first applies quantization to majority of entries of similar magnitudes to ultra-low precision. It then employs a low rank matrix to approximate the quant
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#23545;&#25968;&#25439;&#22833;&#20989;&#25968;&#26469;&#35757;&#32451;&#36866;&#21512;&#30340;Q&#36845;&#20195;&#30340;&#25209;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#23454;&#29616;&#30446;&#26631;&#26102;&#19981;&#20135;&#29983;&#25104;&#26412;&#30340;&#38382;&#39064;&#20013;&#65292;&#20854;&#26679;&#26412;&#25968;&#37327;&#38656;&#27714;&#19982;&#26368;&#20248;&#31574;&#30053;&#30340;&#32047;&#31215;&#25104;&#26412;&#25104;&#27604;&#20363;&#65292;&#33021;&#22815;&#25552;&#20379;&#19982;&#26368;&#20248;&#21487;&#36798;&#25104;&#26412;&#25104;&#27604;&#20363;&#30340;&#8220;&#23567;&#25104;&#26412;&#8221;&#30028;&#38480;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#39564;&#35777;&#22312;&#37027;&#20123;&#26368;&#20248;&#31574;&#30053;&#21487;&#38752;&#23454;&#29616;&#30446;&#26631;&#30340;&#38382;&#39064;&#20013;&#65292;FQI-LOG&#27604;&#20351;&#29992;&#24179;&#26041;&#25439;&#22833;&#35757;&#32451;&#30340;FQI&#20351;&#29992;&#26356;&#23569;&#30340;&#26679;&#26412;&#12290;</title><link>https://arxiv.org/abs/2403.05385</link><description>&lt;p&gt;
&#22312;&#25209;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#36890;&#36807;&#20999;&#25442;&#25439;&#22833;&#20989;&#25968;&#26469;&#38477;&#20302;&#25104;&#26412;
&lt;/p&gt;
&lt;p&gt;
Switching the Loss Reduces the Cost in Batch Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05385
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#23545;&#25968;&#25439;&#22833;&#20989;&#25968;&#26469;&#35757;&#32451;&#36866;&#21512;&#30340;Q&#36845;&#20195;&#30340;&#25209;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#23454;&#29616;&#30446;&#26631;&#26102;&#19981;&#20135;&#29983;&#25104;&#26412;&#30340;&#38382;&#39064;&#20013;&#65292;&#20854;&#26679;&#26412;&#25968;&#37327;&#38656;&#27714;&#19982;&#26368;&#20248;&#31574;&#30053;&#30340;&#32047;&#31215;&#25104;&#26412;&#25104;&#27604;&#20363;&#65292;&#33021;&#22815;&#25552;&#20379;&#19982;&#26368;&#20248;&#21487;&#36798;&#25104;&#26412;&#25104;&#27604;&#20363;&#30340;&#8220;&#23567;&#25104;&#26412;&#8221;&#30028;&#38480;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#39564;&#35777;&#22312;&#37027;&#20123;&#26368;&#20248;&#31574;&#30053;&#21487;&#38752;&#23454;&#29616;&#30446;&#26631;&#30340;&#38382;&#39064;&#20013;&#65292;FQI-LOG&#27604;&#20351;&#29992;&#24179;&#26041;&#25439;&#22833;&#35757;&#32451;&#30340;FQI&#20351;&#29992;&#26356;&#23569;&#30340;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23545;&#25968;&#25439;&#22833;&#65288;FQI-LOG&#65289;&#26469;&#35757;&#32451;&#36866;&#21512;&#30340;Q&#36845;&#20195;&#30340;&#25209;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;FQI-LOG&#23398;&#20064;&#25509;&#36817;&#26368;&#20248;&#31574;&#30053;&#25152;&#38656;&#30340;&#26679;&#26412;&#25968;&#37327;&#19982;&#26368;&#20248;&#31574;&#30053;&#30340;&#32047;&#31215;&#25104;&#26412;&#25104;&#27604;&#20363;&#65292;&#23545;&#20110;&#37027;&#20123;&#36890;&#36807;&#26368;&#20248;&#34892;&#20026;&#23454;&#29616;&#30446;&#26631;&#19988;&#19981;&#20135;&#29983;&#25104;&#26412;&#30340;&#38382;&#39064;&#65292;&#26368;&#20248;&#31574;&#30053;&#30340;&#32047;&#31215;&#25104;&#26412;&#20026;&#38646;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#22312;&#25209;RL&#20013;&#35777;&#26126;&#20855;&#26377;&#19982;&#26368;&#20248;&#21487;&#36798;&#25104;&#26412;&#25104;&#27604;&#20363;&#30340;&#8220;&#23567;&#25104;&#26412;&#8221;&#30028;&#38480;&#30340;&#19968;&#33324;&#26694;&#26550;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20174;&#32463;&#39564;&#19978;&#39564;&#35777;&#65292;FQI-LOG&#22312;&#37027;&#20123;&#26368;&#20248;&#31574;&#30053;&#21487;&#38752;&#22320;&#23454;&#29616;&#30446;&#26631;&#30340;&#38382;&#39064;&#19978;&#20351;&#29992;&#30340;&#26679;&#26412;&#27604;&#20351;&#29992;&#24179;&#26041;&#25439;&#22833;&#35757;&#32451;&#30340;FQI&#35201;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05385v1 Announce Type: new  Abstract: We propose training fitted Q-iteration with log-loss (FQI-LOG) for batch reinforcement learning (RL). We show that the number of samples needed to learn a near-optimal policy with FQI-LOG scales with the accumulated cost of the optimal policy, which is zero in problems where acting optimally achieves the goal and incurs no cost. In doing so, we provide a general framework for proving $\textit{small-cost}$ bounds, i.e. bounds that scale with the optimal achievable cost, in batch RL. Moreover, we empirically verify that FQI-LOG uses fewer samples than FQI trained with squared loss on problems where the optimal policy reliably achieves the goal.
&lt;/p&gt;</description></item><item><title>QCGNN&#36890;&#36807;&#37327;&#23376;&#24182;&#34892;&#24615;&#23454;&#29616;&#20102;&#23545;&#21943;&#27880;&#21028;&#21035;&#30340;&#22810;&#39033;&#24335;&#21152;&#36895;&#65292;&#20026;&#21943;&#27880;&#21028;&#21035;&#38382;&#39064;&#24102;&#26469;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;</title><link>https://arxiv.org/abs/2403.04990</link><description>&lt;p&gt;
&#20351;&#29992;&#37327;&#23376;&#23436;&#20840;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21943;&#27880;&#21028;&#21035;
&lt;/p&gt;
&lt;p&gt;
Jet Discrimination with Quantum Complete Graph Neural Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04990
&lt;/p&gt;
&lt;p&gt;
QCGNN&#36890;&#36807;&#37327;&#23376;&#24182;&#34892;&#24615;&#23454;&#29616;&#20102;&#23545;&#21943;&#27880;&#21028;&#21035;&#30340;&#22810;&#39033;&#24335;&#21152;&#36895;&#65292;&#20026;&#21943;&#27880;&#21028;&#21035;&#38382;&#39064;&#24102;&#26469;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65292;&#29305;&#21035;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#39640;&#33021;&#29289;&#29702;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#65292;&#24182;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#23637;&#29616;&#20986;&#26174;&#33879;&#30340;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#26426;&#22120;&#23398;&#20064;&#30340;&#27010;&#24565;&#24050;&#25193;&#23637;&#21040;&#37327;&#23376;&#35745;&#31639;&#26426;&#65292;&#24418;&#25104;&#20102;&#19968;&#20010;&#34987;&#31216;&#20026;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#30340;&#26032;&#30740;&#31350;&#39046;&#22495;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#21464;&#20998;&#37327;&#23376;&#30005;&#36335;&#27169;&#22411;&#65292;&#21363;&#37327;&#23376;&#23436;&#20840;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;QCGNN&#65289;&#65292;&#26088;&#22312;&#23398;&#20064;&#23436;&#20840;&#22270;&#12290;&#25105;&#20204;&#35748;&#20026;QCGNN&#30001;&#20110;&#37327;&#23376;&#24182;&#34892;&#24615;&#30340;&#29305;&#24615;&#65292;&#22312;&#36895;&#24230;&#19978;&#23545;&#20854;&#32463;&#20856;&#23545;&#24212;&#29289;&#20855;&#26377;&#22810;&#39033;&#24335;&#21152;&#36895;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;QCGNN&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21943;&#27880;&#21028;&#21035;&#20013;&#30340;&#24212;&#29992;&#65292;&#20854;&#20013;&#21943;&#27880;&#29992;&#23436;&#20840;&#22270;&#34920;&#31034;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#19982;&#32463;&#20856;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#65292;&#20197;&#24314;&#31435;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04990v1 Announce Type: cross  Abstract: Machine learning, particularly deep neural networks, has been widely utilized in high energy physics and has shown remarkable results in various applications. Moreover, the concept of machine learning has been extended to quantum computers, giving rise to a new research area known as quantum machine learning. In this paper, we propose a novel variational quantum circuit model, Quantum Complete Graph Neural Network (QCGNN), designed for learning complete graphs. We argue that QCGNN has a polynomial speedup against its classical counterpart, due to the property of quantum parallelism. In this paper, we study the application of QCGNN through the challenging jet discrimination, where the jets are represented with complete graphs. Subsequently, we conduct a comparative analysis with classical graph neural networks to establish a benchmark.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38544;&#31169;&#28431;&#26007;&#26041;&#27861;&#30340;&#39640;&#25928;&#27714;&#35299;&#22120;&#65292;&#33021;&#22312;&#24050;&#30693;&#21644;&#26410;&#30693;&#20998;&#24067;&#26465;&#20214;&#19979;&#22343;&#26377;&#25928;&#22320;&#36827;&#34892;&#27714;&#35299;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20986;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.04778</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#38544;&#31169;&#28431;&#26007;&#30340;&#39640;&#25928;&#20984;&#24046;&#20998;&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
An Efficient Difference-of-Convex Solver for Privacy Funnel
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04778
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38544;&#31169;&#28431;&#26007;&#26041;&#27861;&#30340;&#39640;&#25928;&#27714;&#35299;&#22120;&#65292;&#33021;&#22312;&#24050;&#30693;&#21644;&#26410;&#30693;&#20998;&#24067;&#26465;&#20214;&#19979;&#22343;&#26377;&#25928;&#22320;&#36827;&#34892;&#27714;&#35299;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20986;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38544;&#31169;&#28431;&#26007;&#65288;PF&#65289;&#26041;&#27861;&#30340;&#39640;&#25928;&#27714;&#35299;&#22120;&#65292;&#21033;&#29992;&#20854;&#20984;&#24046;&#20998;&#65288;DC&#65289;&#32467;&#26500;&#12290;&#25152;&#25552;&#20986;&#30340;DC&#20998;&#31163;&#23548;&#33268;&#20102;&#38381;&#24335;&#26356;&#26032;&#26041;&#31243;&#65292;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#24050;&#30693;&#21644;&#26410;&#30693;&#20998;&#24067;&#35774;&#32622;&#12290;&#23545;&#20110;&#24050;&#30693;&#20998;&#24067;&#24773;&#20917;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#38750;&#36138;&#23146;&#27714;&#35299;&#22120;&#30340;&#25910;&#25947;&#24615;&#65288;&#23616;&#37096;&#31283;&#23450;&#28857;&#65289;&#65292;&#24182;&#22312;&#32463;&#39564;&#19978;&#23637;&#31034;&#23427;&#22312;&#34920;&#24449;&#38544;&#31169;-&#25928;&#29992;&#26435;&#34913;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;DC&#26041;&#27861;&#27934;&#23519;&#21147;&#36866;&#29992;&#20110;&#20855;&#26377;&#26631;&#35760;&#32463;&#39564;&#26679;&#26412;&#30340;&#26410;&#30693;&#20998;&#24067;&#35774;&#32622;&#12290;&#21033;&#29992;&#36825;&#20123;&#27934;&#23519;&#21147;&#65292;&#25105;&#20204;&#30340;&#20132;&#26367;&#26368;&#23567;&#21270;&#27714;&#35299;&#22120;&#28385;&#36275;&#20102;PF&#30340;&#22522;&#26412;Markov&#20851;&#31995;&#65292;&#19982;&#20197;&#24448;&#22522;&#20110;&#21464;&#20998;&#25512;&#29702;&#30340;&#27714;&#35299;&#22120;&#30456;&#27604;&#12290;&#22312;&#32463;&#39564;&#19978;&#65292;&#25105;&#20204;&#20351;&#29992;MNIST&#21644;Fashion-MNIST&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#27714;&#35299;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04778v1 Announce Type: new  Abstract: We propose an efficient solver for the privacy funnel (PF) method, leveraging its difference-of-convex (DC) structure. The proposed DC separation results in a closed-form update equation, which allows straightforward application to both known and unknown distribution settings. For known distribution case, we prove the convergence (local stationary points) of the proposed non-greedy solver, and empirically show that it outperforms the state-of-the-art approaches in characterizing the privacy-utility trade-off. The insights of our DC approach apply to unknown distribution settings where labeled empirical samples are available instead. Leveraging the insights, our alternating minimization solver satisfies the fundamental Markov relation of PF in contrast to previous variational inference-based solvers. Empirically, we evaluate the proposed solver with MNIST and Fashion-MNIST datasets. Our results show that under a comparable reconstruction 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#35745;&#31639;&#30149;&#29702;&#23398;&#20013;&#20943;&#23569;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#22797;&#26434;&#24615;&#23545;&#20998;&#31867;&#24615;&#33021;&#30340;&#25913;&#21892;&#65292;&#36890;&#36807;&#21033;&#29992;&#28040;&#36153;&#32423;&#30828;&#20214;&#12290;</title><link>https://arxiv.org/abs/2403.04558</link><description>&lt;p&gt;
&#20943;&#23569;&#33258;&#30417;&#30563;&#23398;&#20064;&#22797;&#26434;&#24615;&#25913;&#21892;&#35745;&#31639;&#30149;&#29702;&#23398;&#20013;&#30340;&#24369;&#30417;&#30563;&#20998;&#31867;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Reducing self-supervised learning complexity improves weakly-supervised classification performance in computational pathology
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04558
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#35745;&#31639;&#30149;&#29702;&#23398;&#20013;&#20943;&#23569;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#22797;&#26434;&#24615;&#23545;&#20998;&#31867;&#24615;&#33021;&#30340;&#25913;&#21892;&#65292;&#36890;&#36807;&#21033;&#29992;&#28040;&#36153;&#32423;&#30828;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#20174;&#24120;&#35268;&#21487;&#29992;&#30340;&#32452;&#32455;&#23398;&#25968;&#25454;&#20013;&#25552;&#21462;&#20020;&#24202;&#21487;&#25805;&#20316;&#35265;&#35299;&#12290;&#36890;&#24120;&#65292;&#36825;&#20123;&#27169;&#22411;&#38656;&#35201;&#20020;&#24202;&#21307;&#29983;&#36827;&#34892;&#30340;&#26631;&#27880;&#65292;&#36825;&#31181;&#26631;&#27880;&#31232;&#32570;&#19988;&#26114;&#36149;&#12290;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26041;&#27861;&#30340;&#20986;&#29616;&#28040;&#38500;&#20102;&#36825;&#19968;&#38556;&#30861;&#65292;&#20801;&#35768;&#23545;&#38750;&#26631;&#27880;&#25968;&#25454;&#36827;&#34892;&#22823;&#35268;&#27169;&#20998;&#26512;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;SSL&#26041;&#27861;&#37319;&#29992;&#26085;&#30410;&#24222;&#22823;&#30340;&#27169;&#22411;&#26550;&#26500;&#21644;&#26356;&#22823;&#30340;&#25968;&#25454;&#38598;&#65292;&#23548;&#33268;&#25968;&#25454;&#37327;&#36805;&#36895;&#22686;&#21152;&#65292;&#30828;&#20214;&#35201;&#27714;&#21644;&#25972;&#20307;&#25104;&#26412;&#22686;&#21152;&#65292;&#20351;&#24471;&#24456;&#23569;&#26426;&#26500;&#33021;&#22815;&#33719;&#24471;&#36825;&#20123;&#36164;&#28304;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#35745;&#31639;&#30149;&#29702;&#23398;&#20013;&#30340;&#22797;&#26434;&#24615;&#19982;&#20998;&#31867;&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21033;&#29992;&#28040;&#36153;&#32423;&#30828;&#20214;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#25968;&#25454;&#37327;&#12289;&#26550;&#26500;&#21644;&#31639;&#27861;&#30340;&#35843;&#25972;&#23545;&#19979;&#28216;&#20998;&#31867;&#20219;&#21153;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04558v1 Announce Type: cross  Abstract: Deep Learning models have been successfully utilized to extract clinically actionable insights from routinely available histology data. Generally, these models require annotations performed by clinicians, which are scarce and costly to generate. The emergence of self-supervised learning (SSL) methods remove this barrier, allowing for large-scale analyses on non-annotated data. However, recent SSL approaches apply increasingly expansive model architectures and larger datasets, causing the rapid escalation of data volumes, hardware prerequisites, and overall expenses, limiting access to these resources to few institutions. Therefore, we investigated the complexity of contrastive SSL in computational pathology in relation to classification performance with the utilization of consumer-grade hardware. Specifically, we analyzed the effects of adaptations in data volume, architecture, and algorithms on downstream clas- sification tasks, empha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20998;&#25955;&#30340;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#21644;&#20998;&#25955;&#20844;&#24179;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#20855;&#26377;&#36845;&#20195;&#22797;&#26434;&#24230;&#20026;O(1/{\epsilon})&#30340;&#21333;&#24490;&#29615;&#20998;&#25955;&#31639;&#27861;&#65292;&#20197;&#21305;&#37197;&#29616;&#26377;&#30340;&#38598;&#20013;&#24335;&#19968;&#38454;&#26041;&#27861;&#65292;&#24182;&#25913;&#36827;&#20102;&#29616;&#26377;&#38598;&#20013;&#24335;&#31639;&#27861;&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.04259</link><description>&lt;p&gt;
&#20998;&#25955;&#19988;&#20844;&#24179;&#30340;&#26368;&#20248;&#36755;&#36816;
&lt;/p&gt;
&lt;p&gt;
Decentralized and Equitable Optimal Transport
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04259
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20998;&#25955;&#30340;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#21644;&#20998;&#25955;&#20844;&#24179;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#20855;&#26377;&#36845;&#20195;&#22797;&#26434;&#24230;&#20026;O(1/{\epsilon})&#30340;&#21333;&#24490;&#29615;&#20998;&#25955;&#31639;&#27861;&#65292;&#20197;&#21305;&#37197;&#29616;&#26377;&#30340;&#38598;&#20013;&#24335;&#19968;&#38454;&#26041;&#27861;&#65292;&#24182;&#25913;&#36827;&#20102;&#29616;&#26377;&#38598;&#20013;&#24335;&#31639;&#27861;&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#20998;&#25955;&#65288;&#31163;&#25955;&#65289;&#26368;&#20248;&#36755;&#36816;&#65288;D-OT&#65289;&#38382;&#39064;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#19968;&#32452;&#20195;&#29702;&#20154;&#20849;&#21516;&#35774;&#35745;&#36816;&#36755;&#26041;&#26696;&#65292;&#20854;&#20013;&#25104;&#26412;&#20989;&#25968;&#26159;&#27599;&#20010;&#20195;&#29702;&#20154;&#25345;&#26377;&#30340;&#25104;&#26412;&#20043;&#21644;&#12290;&#25105;&#20204;&#23558;D-OT&#38382;&#39064;&#37325;&#26032;&#34920;&#36848;&#20026;&#32422;&#26463;&#32806;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#36845;&#20195;&#22797;&#26434;&#24230;&#20026;O(1/{\epsilon})&#30340;&#21333;&#24490;&#29615;&#20998;&#25955;&#31639;&#27861;&#65292;&#19982;&#29616;&#26377;&#30340;&#38598;&#20013;&#24335;&#19968;&#38454;&#26041;&#27861;&#30456;&#21305;&#37197;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20998;&#25955;&#20844;&#24179;&#26368;&#20248;&#36755;&#36816;&#65288;DE-OT&#65289;&#38382;&#39064;&#12290;&#22312;DE-OT&#20013;&#65292;&#20195;&#29702;&#19981;&#20165;&#21327;&#20316;&#35774;&#35745;&#26368;&#23567;&#21270;&#36816;&#36755;&#25104;&#26412;&#30340;&#36816;&#36755;&#35745;&#21010;&#65292;&#36824;&#21162;&#21147;&#30830;&#20445;&#21508;&#33258;&#25104;&#26412;&#30340;&#20844;&#24179;&#24615;&#12290;&#35299;&#20915;DE-OT&#30340;&#26041;&#27861;&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#20063;&#26159;O(1/{\epsilon})&#65292;&#36825;&#19968;&#36895;&#29575;&#25552;&#39640;&#20102;&#29616;&#26377;&#30340;&#38598;&#20013;&#24335;&#31639;&#27861;&#65292;&#20854;&#20013;&#33719;&#24471;&#30340;&#26368;&#20339;&#36845;&#20195;&#22797;&#26434;&#24230;&#20026;O(1/{\epsilon}^2)&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04259v1 Announce Type: cross  Abstract: This paper considers the decentralized (discrete) optimal transport (D-OT) problem. In this setting, a network of agents seeks to design a transportation plan jointly, where the cost function is the sum of privately held costs for each agent. We reformulate the D-OT problem as a constraint-coupled optimization problem and propose a single-loop decentralized algorithm with an iteration complexity of O(1/{\epsilon}) that matches existing centralized first-order approaches. Moreover, we propose the decentralized equitable optimal transport (DE-OT) problem. In DE-OT, in addition to cooperatively designing a transportation plan that minimizes transportation costs, agents seek to ensure equity in their individual costs. The iteration complexity of the proposed method to solve DE-OT is also O(1/{\epsilon}). This rate improves existing centralized algorithms, where the best iteration complexity obtained is O(1/{\epsilon}^2).
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#30417;&#30563;&#26641;&#26680;&#24378;&#21270;&#38598;&#25104;&#27169;&#22411;&#65292;&#39044;&#27979;&#21491;&#24515;&#23460;&#23481;&#31215;&#30340;&#20108;&#32500;&#36229;&#22768;&#24515;&#21160;&#22270;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#24471;&#20998;&#22686;&#24378;&#20102;&#39044;&#27979;&#34920;&#29616;&#65292;&#35813;&#26041;&#27861;&#22312;&#23567;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#27010;&#29575;&#21644;&#28857;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.03229</link><description>&lt;p&gt;
&#25317;&#25265;&#19981;&#30830;&#23450;&#24615;&#28789;&#27963;&#24615;&#65306;&#21033;&#29992;&#30417;&#30563;&#26641;&#26680;&#24378;&#21270;&#38598;&#25104;&#27169;&#22411;&#65292;&#39044;&#27979;&#21491;&#24515;&#23460;&#23481;&#31215;&#30340;&#20108;&#32500;&#36229;&#22768;&#24515;&#21160;&#22270;
&lt;/p&gt;
&lt;p&gt;
Embracing Uncertainty Flexibility: Harnessing a Supervised Tree Kernel to Empower Ensemble Modelling for 2D Echocardiography-Based Prediction of Right Ventricular Volume
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03229
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#30417;&#30563;&#26641;&#26680;&#24378;&#21270;&#38598;&#25104;&#27169;&#22411;&#65292;&#39044;&#27979;&#21491;&#24515;&#23460;&#23481;&#31215;&#30340;&#20108;&#32500;&#36229;&#22768;&#24515;&#21160;&#22270;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#24471;&#20998;&#22686;&#24378;&#20102;&#39044;&#27979;&#34920;&#29616;&#65292;&#35813;&#26041;&#27861;&#22312;&#23567;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#27010;&#29575;&#21644;&#28857;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21491;&#24515;&#23460;&#65288;RV&#65289;&#21151;&#33021;&#24694;&#21270;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#37117;&#33021;&#24378;&#21147;&#39044;&#27979;&#20020;&#24202;&#32467;&#26524;&#12290;&#20026;&#20102;&#22686;&#24378;&#20351;&#29992;&#24191;&#27867;&#21487;&#29992;&#30340;&#20108;&#32500;&#36229;&#22768;&#24515;&#21160;&#22270;&#65288;2DE&#65289;&#30340;&#34920;&#26684;&#25968;&#25454;&#37327;&#21270;RV&#23481;&#31215;&#30340;&#38598;&#25104;&#22238;&#24402;&#26041;&#27861;&#30340;&#20020;&#24202;&#37096;&#32626;&#65292;&#25105;&#20204;&#24314;&#35758;&#23558;&#20307;&#31215;&#39044;&#27979;&#19982;&#19981;&#30830;&#23450;&#24615;&#24471;&#20998;&#30456;&#32467;&#21512;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#37319;&#29992;&#19968;&#31181;&#22522;&#20110;&#23454;&#20363;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#23398;&#20064;&#30340;&#26641;&#32467;&#26500;&#26469;&#35782;&#21035;&#30446;&#26631;&#23454;&#20363;&#21608;&#22260;&#30340;&#26368;&#36817;&#35757;&#32451;&#26679;&#26412;&#65292;&#28982;&#21518;&#20351;&#29992;&#22810;&#31181;&#20998;&#24067;&#31867;&#22411;&#26469;&#26356;&#28789;&#27963;&#22320;&#24314;&#27169;&#36755;&#20986;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#30340;&#27010;&#29575;&#21644;&#28857;&#39044;&#27979;&#24615;&#33021;&#22312;&#19968;&#20010;&#30456;&#23545;&#23567;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#21253;&#25324;100&#20010;&#33298;&#24352;&#26411;&#21644;&#25910;&#32553;&#26411;RV&#23481;&#31215;&#12290;&#28857;&#24615;&#33021;&#30340;&#21442;&#32771;&#20540;&#26469;&#33258;MRI&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#28789;&#27963;&#26041;&#27861;&#22312;&#27010;&#29575;&#21644;&#28857;&#39044;&#27979;&#24615;&#33021;&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03229v1 Announce Type: cross  Abstract: The right ventricular (RV) function deterioration strongly predicts clinical outcomes in numerous circumstances. To boost the clinical deployment of ensemble regression methods that quantify RV volumes using tabular data from the widely available two-dimensional echocardiography (2DE), we propose to complement the volume predictions with uncertainty scores. To this end, we employ an instance-based method which uses the learned tree structure to identify the nearest training samples to a target instance and then uses a number of distribution types to more flexibly model the output. The probabilistic and point-prediction performances of the proposed framework are evaluated on a relatively small-scale dataset, comprising 100 end-diastolic and end-systolic RV volumes. The reference values for point performance were obtained from MRI. The results demonstrate that our flexible approach yields improved probabilistic and point performances ove
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#24085;&#32047;&#25176;&#26368;&#20248;&#20272;&#35745;&#21644;&#31574;&#30053;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#30830;&#23450;&#22914;&#20309;&#22312;&#30701;&#26399;&#21644;&#38271;&#26399;&#27835;&#30103;&#25928;&#26524;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#20174;&#32780;&#23454;&#29616;&#26368;&#20339;&#27835;&#30103;&#12290;</title><link>https://arxiv.org/abs/2403.02624</link><description>&lt;p&gt;
Pareto-Optimal Estimation and Policy Learning on Short-term and Long-term Treatment Effects
&lt;/p&gt;
&lt;p&gt;
Pareto-Optimal Estimation and Policy Learning on Short-term and Long-term Treatment Effects
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02624
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#24085;&#32047;&#25176;&#26368;&#20248;&#20272;&#35745;&#21644;&#31574;&#30053;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#30830;&#23450;&#22914;&#20309;&#22312;&#30701;&#26399;&#21644;&#38271;&#26399;&#27835;&#30103;&#25928;&#26524;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#20174;&#32780;&#23454;&#29616;&#26368;&#20339;&#27835;&#30103;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#19987;&#27880;&#20110;&#21457;&#23637;&#24085;&#32047;&#25176;&#26368;&#20248;&#20272;&#35745;&#21644;&#31574;&#30053;&#23398;&#20064;&#65292;&#20197;&#30830;&#23450;&#26368;&#26377;&#25928;&#30340;&#27835;&#30103;&#26041;&#27861;&#65292;&#20174;&#32780;&#26368;&#22823;&#21270;&#26469;&#33258;&#30701;&#26399;&#21644;&#38271;&#26399;&#25928;&#26524;&#30340;&#24635;&#22870;&#21169;&#65292;&#36825;&#21487;&#33021;&#20250;&#30456;&#20114;&#20914;&#31361;&#12290; &#20363;&#22914;&#65292;&#33647;&#29289;&#21058;&#37327;&#30340;&#22686;&#21152;&#21487;&#33021;&#20250;&#25552;&#39640;&#24739;&#32773;&#24247;&#22797;&#36895;&#24230;&#65288;&#30701;&#26399;&#65289;&#65292;&#20294;&#20063;&#21487;&#33021;&#23548;&#33268;&#20005;&#37325;&#30340;&#38271;&#26399;&#21103;&#20316;&#29992;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#25506;&#35752;&#20102;&#26377;&#20851;&#30701;&#26399;&#25110;&#38271;&#26399;&#25928;&#24212;&#25110;&#20004;&#32773;&#30340;&#38382;&#39064;&#65292;&#20294;&#22914;&#20309;&#22312;&#23427;&#20204;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#20197;&#23454;&#29616;&#26368;&#20339;&#27835;&#30103;&#20173;&#28982;&#26159;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#24403;&#20351;&#29992;&#20256;&#32479;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#30452;&#25509;&#20272;&#35745;&#22810;&#20010;&#30446;&#26631;&#26102;&#65292;&#21508;&#31181;&#20219;&#21153;&#20043;&#38388;&#30340;&#20248;&#21270;&#26041;&#21521;&#20063;&#21487;&#33021;&#21457;&#29983;&#20914;&#31361;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#24085;&#32047;&#25176;&#26377;&#25928;&#31639;&#27861;&#65292;&#21253;&#25324;&#24085;&#32047;&#25176;&#26368;&#20248;&#20272;&#35745;&#65288;POE&#65289;&#21644;&#24085;&#32047;&#25176;&#26368;&#20248;&#31574;&#30053;&#23398;&#20064;&#65288;POPL&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02624v1 Announce Type: cross  Abstract: This paper focuses on developing Pareto-optimal estimation and policy learning to identify the most effective treatment that maximizes the total reward from both short-term and long-term effects, which might conflict with each other. For example, a higher dosage of medication might increase the speed of a patient's recovery (short-term) but could also result in severe long-term side effects. Although recent works have investigated the problems about short-term or long-term effects or the both, how to trade-off between them to achieve optimal treatment remains an open challenge. Moreover, when multiple objectives are directly estimated using conventional causal representation learning, the optimization directions among various tasks can conflict as well. In this paper, we systematically investigate these issues and introduce a Pareto-Efficient algorithm, comprising Pareto-Optimal Estimation (POE) and Pareto-Optimal Policy Learning (POPL
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#38376;&#20026;&#34920;&#26684;&#25968;&#25454;&#39044;&#27979;&#32780;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;TP-BERTa&#65292;&#36890;&#36807;&#26032;&#39062;&#30340;&#30456;&#23545;&#22823;&#23567;&#26631;&#35760;&#21270;&#26041;&#27861;&#21644;&#20869;&#37096;&#29305;&#24449;&#20851;&#27880;&#26041;&#27861;&#35299;&#20915;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#20540;&#29305;&#24449;&#20540;&#19978;&#30340;&#19981;&#20860;&#23481;&#24615;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.01841</link><description>&lt;p&gt;
&#22312;&#34920;&#26684;&#39044;&#27979;&#19978;&#20248;&#21270;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Making Pre-trained Language Models Great on Tabular Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01841
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#38376;&#20026;&#34920;&#26684;&#25968;&#25454;&#39044;&#27979;&#32780;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;TP-BERTa&#65292;&#36890;&#36807;&#26032;&#39062;&#30340;&#30456;&#23545;&#22823;&#23567;&#26631;&#35760;&#21270;&#26041;&#27861;&#21644;&#20869;&#37096;&#29305;&#24449;&#20851;&#27880;&#26041;&#27861;&#35299;&#20915;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#20540;&#29305;&#24449;&#20540;&#19978;&#30340;&#19981;&#20860;&#23481;&#24615;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#21487;&#36801;&#31227;&#24615;&#22312;&#22270;&#20687;&#21644;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#34920;&#26684;&#20043;&#38388;&#30340;&#24322;&#36136;&#24615;&#65292;&#36825;&#31181;DNN&#30340;&#20248;&#21183;&#22312;&#34920;&#26684;&#25968;&#25454;&#39044;&#27979;&#65288;&#20363;&#22914;&#22238;&#24402;&#25110;&#20998;&#31867;&#20219;&#21153;&#65289;&#19978;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#21033;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;TP-BERTa&#65292;&#36825;&#26159;&#19968;&#31181;&#19987;&#38376;&#20026;&#34920;&#26684;&#25968;&#25454;&#39044;&#27979;&#32780;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#30456;&#23545;&#22823;&#23567;&#26631;&#35760;&#21270;&#26041;&#27861;&#23558;&#26631;&#37327;&#25968;&#20540;&#29305;&#24449;&#20540;&#36716;&#25442;&#20026;&#31163;&#25955;&#24230;&#39640;&#12289;&#39640;&#32500;&#24230;&#30340;&#26631;&#35760;&#65292;&#24182;&#19988;&#19968;&#31181;&#20869;&#37096;&#29305;&#24449;&#20851;&#27880;&#26041;&#27861;&#25972;&#21512;&#20102;&#29305;&#24449;&#21517;&#31216;&#21644;&#25968;&#20540;&#29305;&#24449;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01841v1 Announce Type: new  Abstract: The transferability of deep neural networks (DNNs) has made significant progress in image and language processing. However, due to the heterogeneity among tables, such DNN bonus is still far from being well exploited on tabular data prediction (e.g., regression or classification tasks). Condensing knowledge from diverse domains, language models (LMs) possess the capability to comprehend feature names from various tables, potentially serving as versatile learners in transferring knowledge across distinct tables and diverse prediction tasks, but their discrete text representation space is inherently incompatible with numerical feature values in tables. In this paper, we present TP-BERTa, a specifically pre-trained LM model for tabular data prediction. Concretely, a novel relative magnitude tokenization converts scalar numerical feature values to finely discrete, high-dimensional tokens, and an intra-feature attention approach integrates fe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20174;&#20869;&#37096;&#34920;&#24449;&#35282;&#24230;&#28145;&#20837;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24187;&#35273;&#30340;&#26426;&#21046;&#65292;&#21457;&#29616;&#20102;&#24187;&#35273;&#30340;&#19968;&#20010;&#26174;&#33879;&#27169;&#24335;&#65292;&#21363;&#22312;&#19978;&#19979;&#25991;&#26631;&#35760;&#30340;&#38544;&#34255;&#29366;&#24577;&#20013;&#65292;&#27491;&#30830;&#29983;&#25104;&#20855;&#26377;&#26356;&#28165;&#26224;&#30340;&#19978;&#19979;&#25991;&#28608;&#27963;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29109;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#23558;&#8220;&#38160;&#24230;&#8221;&#32435;&#20837;&#35299;&#30721;&#36807;&#31243;&#20013;&#65292;&#21046;&#23450;&#20102;&#19968;&#31181;&#21463;&#38480;&#35299;&#30721;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#30693;&#35782;&#23547;&#27714;&#21644;&#24187;&#35273;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.01548</link><description>&lt;p&gt;
&#22522;&#20110;&#20869;&#37096;&#34920;&#24449;&#30340;&#19978;&#19979;&#25991;&#38160;&#24230;&#20316;&#20026;&#35686;&#25253;&#65306;&#20943;&#23569;&#24187;&#35273;&#30340;&#19968;&#20010;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
In-Context Sharpness as Alerts: An Inner Representation Perspective for Hallucination Mitigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#20869;&#37096;&#34920;&#24449;&#35282;&#24230;&#28145;&#20837;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24187;&#35273;&#30340;&#26426;&#21046;&#65292;&#21457;&#29616;&#20102;&#24187;&#35273;&#30340;&#19968;&#20010;&#26174;&#33879;&#27169;&#24335;&#65292;&#21363;&#22312;&#19978;&#19979;&#25991;&#26631;&#35760;&#30340;&#38544;&#34255;&#29366;&#24577;&#20013;&#65292;&#27491;&#30830;&#29983;&#25104;&#20855;&#26377;&#26356;&#28165;&#26224;&#30340;&#19978;&#19979;&#25991;&#28608;&#27963;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29109;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#23558;&#8220;&#38160;&#24230;&#8221;&#32435;&#20837;&#35299;&#30721;&#36807;&#31243;&#20013;&#65292;&#21046;&#23450;&#20102;&#19968;&#31181;&#21463;&#38480;&#35299;&#30721;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#30693;&#35782;&#23547;&#27714;&#21644;&#24187;&#35273;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#32463;&#24120;&#20250;&#20135;&#29983;&#24187;&#35273;&#24182;&#20135;&#29983;&#20107;&#23454;&#38169;&#35823;&#65292;&#28982;&#32780;&#25105;&#20204;&#23545;&#23427;&#20204;&#20026;&#20160;&#20040;&#20250;&#29359;&#36825;&#20123;&#38169;&#35823;&#30340;&#29702;&#35299;&#20173;&#28982;&#26377;&#38480;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20174;&#20869;&#37096;&#34920;&#24449;&#30340;&#35282;&#24230;&#28145;&#20837;&#25506;&#35752;LLM&#24187;&#35273;&#30340;&#28508;&#22312;&#26426;&#21046;&#65292;&#24182;&#21457;&#29616;&#19982;&#24187;&#35273;&#30456;&#20851;&#30340;&#19968;&#20010;&#31361;&#20986;&#27169;&#24335;&#65306;&#27491;&#30830;&#30340;&#29983;&#25104;&#22312;&#19978;&#19979;&#25991;&#26631;&#35760;&#30340;&#38544;&#34255;&#29366;&#24577;&#20013;&#20855;&#26377;&#26356;&#28165;&#26224;&#30340;&#19978;&#19979;&#25991;&#28608;&#27963;&#65292;&#32780;&#19981;&#27491;&#30830;&#30340;&#29983;&#25104;&#21017;&#27809;&#26377;&#12290;&#21033;&#29992;&#36825;&#19968;&#35265;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29109;&#30340;&#24230;&#37327;&#26469;&#37327;&#21270;&#19978;&#19979;&#25991;&#38544;&#34255;&#29366;&#24577;&#20043;&#38388;&#30340;&#8220;&#38160;&#24230;&#8221;&#65292;&#24182;&#23558;&#20854;&#32435;&#20837;&#35299;&#30721;&#36807;&#31243;&#20013;&#20197;&#21046;&#23450;&#19968;&#31181;&#21463;&#38480;&#35299;&#30721;&#26041;&#27861;&#12290;&#22312;&#21508;&#31181;&#30693;&#35782;&#23547;&#27714;&#21644;&#24187;&#35273;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#19968;&#33268;&#26377;&#25928;&#24615;&#65292;&#20363;&#22914;&#65292;&#22312;TruthfulQA&#19978;&#23454;&#29616;&#20102;&#39640;&#36798;8.6&#28857;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#30456;&#20449;&#36825;&#39033;&#30740;&#31350;&#21487;&#20197;&#25552;&#39640;&#25105;&#20204;&#23545;&#24187;&#35273;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01548v1 Announce Type: cross  Abstract: Large language models (LLMs) frequently hallucinate and produce factual errors, yet our understanding of why they make these errors remains limited. In this study, we delve into the underlying mechanisms of LLM hallucinations from the perspective of inner representations, and discover a salient pattern associated with hallucinations: correct generations tend to have sharper context activations in the hidden states of the in-context tokens, compared to the incorrect ones. Leveraging this insight, we propose an entropy-based metric to quantify the ``sharpness'' among the in-context hidden states and incorporate it into the decoding process to formulate a constrained decoding approach. Experiments on various knowledge-seeking and hallucination benchmarks demonstrate our approach's consistent effectiveness, for example, achieving up to an 8.6 point improvement on TruthfulQA. We believe this study can improve our understanding of hallucinat
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#38754;&#21521;&#26234;&#33021;&#30005;&#32593;&#36127;&#33655;&#39044;&#27979;&#30340;&#38544;&#31169;&#20445;&#25252;&#21327;&#21516;&#20998;&#35010;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20998;&#35010;&#20026;Grid Station&#65288;GS&#65289;&#21644;&#26381;&#21153;&#25552;&#20379;&#21830;&#65288;SP&#65289;&#37096;&#20998;&#65292;&#23454;&#29616;&#26234;&#33021;&#30005;&#34920;&#25968;&#25454;&#30340;&#38544;&#31169;&#20445;&#25252;&#21644;&#36127;&#36733;&#39044;&#27979;&#30340;&#20010;&#24615;&#21270;&#27169;&#22411;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2403.01438</link><description>&lt;p&gt;
&#38754;&#21521;&#26234;&#33021;&#30005;&#32593;&#36127;&#33655;&#39044;&#27979;&#30340;&#38544;&#31169;&#20445;&#25252;&#21327;&#21516;&#20998;&#35010;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Privacy-Preserving Collaborative Split Learning Framework for Smart Grid Load Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01438
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#38754;&#21521;&#26234;&#33021;&#30005;&#32593;&#36127;&#33655;&#39044;&#27979;&#30340;&#38544;&#31169;&#20445;&#25252;&#21327;&#21516;&#20998;&#35010;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20998;&#35010;&#20026;Grid Station&#65288;GS&#65289;&#21644;&#26381;&#21153;&#25552;&#20379;&#21830;&#65288;SP&#65289;&#37096;&#20998;&#65292;&#23454;&#29616;&#26234;&#33021;&#30005;&#34920;&#25968;&#25454;&#30340;&#38544;&#31169;&#20445;&#25252;&#21644;&#36127;&#36733;&#39044;&#27979;&#30340;&#20010;&#24615;&#21270;&#27169;&#22411;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#36127;&#33655;&#39044;&#27979;&#23545;&#33021;&#28304;&#31649;&#29702;&#12289;&#22522;&#30784;&#35774;&#26045;&#35268;&#21010;&#21644;&#20379;&#38656;&#24179;&#34913;&#33267;&#20851;&#37325;&#35201;&#12290;&#26234;&#33021;&#30005;&#34920;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#23548;&#33268;&#20102;&#20256;&#24863;&#22120;&#25968;&#25454;&#39537;&#21160;&#30340;&#36127;&#33655;&#39044;&#27979;&#38656;&#27714;&#12290;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#20801;&#35768;&#20351;&#29992;&#26469;&#33258;&#22810;&#20010;&#26234;&#33021;&#30005;&#34920;&#30340;&#25968;&#25454;&#35757;&#32451;&#21333;&#20010;&#20840;&#23616;&#27169;&#22411;&#65292;&#36825;&#38656;&#35201;&#23558;&#25968;&#25454;&#20256;&#36755;&#21040;&#20013;&#22830;&#26381;&#21153;&#22120;&#65292;&#24341;&#21457;&#20102;&#23545;&#32593;&#32476;&#35201;&#27714;&#12289;&#38544;&#31169;&#21644;&#23433;&#20840;&#24615;&#30340;&#25285;&#24551;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#35010;&#23398;&#20064;&#30340;&#36127;&#33655;&#39044;&#27979;&#26694;&#26550;&#65292;&#20197;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20998;&#20026;&#20004;&#20010;&#37096;&#20998;&#65292;&#19968;&#20010;&#29992;&#20110;&#27599;&#20010;Grid Station&#65288;GS&#65289;&#65292;&#36127;&#36131;&#19968;&#20010;&#25972;&#20010;&#31038;&#21306;&#30340;&#26234;&#33021;&#30005;&#34920;&#65307;&#21478;&#19968;&#20010;&#29992;&#20110;&#26381;&#21153;&#25552;&#20379;&#21830;&#65288;SP&#65289;&#12290;&#23458;&#25143;&#26234;&#33021;&#30005;&#34920;&#19981;&#20849;&#20139;&#20854;&#25968;&#25454;&#65292;&#32780;&#26159;&#20351;&#29992;&#21508;&#33258;&#30340;GS&#27169;&#22411;&#25286;&#20998;&#36827;&#34892;&#21069;&#21521;&#20256;&#36882;&#65292;&#21482;&#23558;&#20854;&#28608;&#27963;&#19982;GS&#20849;&#20139;&#12290;&#22312;&#36825;&#19968;&#26694;&#26550;&#19979;&#65292;&#27599;&#20010;GS&#36127;&#36131;&#20026;&#20854;&#21508;&#33258;&#30340;&#31038;&#21306;&#35757;&#32451;&#20010;&#24615;&#21270;&#27169;&#22411;&#20998;&#35010;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01438v1 Announce Type: new  Abstract: Accurate load forecasting is crucial for energy management, infrastructure planning, and demand-supply balancing. Smart meter data availability has led to the demand for sensor-based load forecasting. Conventional ML allows training a single global model using data from multiple smart meters requiring data transfer to a central server, raising concerns for network requirements, privacy, and security. We propose a split learning-based framework for load forecasting to alleviate this issue. We split a deep neural network model into two parts, one for each Grid Station (GS) responsible for an entire neighbourhood's smart meters and the other for the Service Provider (SP). Instead of sharing their data, client smart meters use their respective GSs' model split for forward pass and only share their activations with the GS. Under this framework, each GS is responsible for training a personalized model split for their respective neighbourhoods,
&lt;/p&gt;</description></item><item><title>PowerFlowMultiNet&#26159;&#19968;&#31181;&#19987;&#38376;&#20026;&#19981;&#24179;&#34913;&#19977;&#30456;&#21151;&#29575;&#32593;&#26684;&#35774;&#35745;&#30340;&#26032;&#39062;&#22810;&#22270;GNN&#26694;&#26550;&#65292;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#19981;&#24179;&#34913;&#32593;&#26684;&#20013;&#30340;&#19981;&#23545;&#31216;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#22270;&#23884;&#20837;&#26426;&#21046;&#26469;&#25429;&#33719;&#30005;&#21147;&#31995;&#32479;&#32593;&#32476;&#20869;&#37096;&#30340;&#31354;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2403.00892</link><description>&lt;p&gt;
PowerFlowMultiNet&#65306;&#29992;&#20110;&#19981;&#24179;&#34913;&#19977;&#30456;&#37197;&#30005;&#31995;&#32479;&#30340;&#22810;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
PowerFlowMultiNet: Multigraph Neural Networks for Unbalanced Three-Phase Distribution Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00892
&lt;/p&gt;
&lt;p&gt;
PowerFlowMultiNet&#26159;&#19968;&#31181;&#19987;&#38376;&#20026;&#19981;&#24179;&#34913;&#19977;&#30456;&#21151;&#29575;&#32593;&#26684;&#35774;&#35745;&#30340;&#26032;&#39062;&#22810;&#22270;GNN&#26694;&#26550;&#65292;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#19981;&#24179;&#34913;&#32593;&#26684;&#20013;&#30340;&#19981;&#23545;&#31216;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#22270;&#23884;&#20837;&#26426;&#21046;&#26469;&#25429;&#33719;&#30005;&#21147;&#31995;&#32479;&#32593;&#32476;&#20869;&#37096;&#30340;&#31354;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#35299;&#20915;&#37197;&#30005;&#32593;&#20013;&#19981;&#24179;&#34913;&#30340;&#19977;&#30456;&#21151;&#29575;&#27969;&#38382;&#39064;&#23545;&#20110;&#32593;&#26684;&#20998;&#26512;&#21644;&#20223;&#30495;&#33267;&#20851;&#37325;&#35201;&#12290;&#30446;&#21069;&#24613;&#38656;&#21487;&#22788;&#29702;&#22823;&#35268;&#27169;&#19981;&#24179;&#34913;&#21151;&#29575;&#32593;&#26684;&#24182;&#33021;&#25552;&#20379;&#20934;&#30830;&#24555;&#36895;&#35299;&#20915;&#26041;&#26696;&#30340;&#21487;&#25193;&#23637;&#31639;&#27861;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#23588;&#20854;&#26159;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#24212;&#36816;&#32780;&#29983;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#25991;&#29486;&#20027;&#35201;&#38598;&#20013;&#22312;&#24179;&#34913;&#32593;&#32476;&#19978;&#65292;&#32570;&#20047;&#25903;&#25345;&#19981;&#24179;&#34913;&#19977;&#30456;&#21151;&#29575;&#32593;&#32476;&#30340;&#20851;&#38190;&#20869;&#23481;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;PowerFlowMultiNet&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#19981;&#24179;&#34913;&#19977;&#30456;&#21151;&#29575;&#32593;&#26684;&#35774;&#35745;&#30340;&#26032;&#39062;&#22810;&#22270;GNN&#26694;&#26550;&#12290;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22810;&#22270;&#34920;&#31034;&#20013;&#20998;&#21035;&#23545;&#27599;&#20010;&#30456;&#36827;&#34892;&#24314;&#27169;&#65292;&#26377;&#25928;&#25429;&#25417;&#19981;&#24179;&#34913;&#32593;&#26684;&#20013;&#22266;&#26377;&#30340;&#19981;&#23545;&#31216;&#24615;&#12290;&#24341;&#20837;&#20102;&#21033;&#29992;&#28040;&#24687;&#20256;&#36882;&#25429;&#33719;&#30005;&#21147;&#31995;&#32479;&#32593;&#32476;&#20869;&#37096;&#31354;&#38388;&#20381;&#36182;&#20851;&#31995;&#30340;&#22270;&#23884;&#20837;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00892v1 Announce Type: cross  Abstract: Efficiently solving unbalanced three-phase power flow in distribution grids is pivotal for grid analysis and simulation. There is a pressing need for scalable algorithms capable of handling large-scale unbalanced power grids that can provide accurate and fast solutions. To address this, deep learning techniques, especially Graph Neural Networks (GNNs), have emerged. However, existing literature primarily focuses on balanced networks, leaving a critical gap in supporting unbalanced three-phase power grids. This letter introduces PowerFlowMultiNet, a novel multigraph GNN framework explicitly designed for unbalanced three-phase power grids. The proposed approach models each phase separately in a multigraph representation, effectively capturing the inherent asymmetry in unbalanced grids. A graph embedding mechanism utilizing message passing is introduced to capture spatial dependencies within the power system network. PowerFlowMultiNet out
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#24555;&#29031;&#24378;&#21270;&#23398;&#20064;&#65288;SnapshotRL&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#31616;&#21333;&#25913;&#21464;&#29615;&#22659;&#26469;&#22686;&#24378;&#26679;&#26412;&#25928;&#29575;&#65292;&#32780;&#26080;&#38656;&#23545;&#31639;&#27861;&#21644;&#27169;&#22411;&#36827;&#34892;&#20219;&#20309;&#20462;&#25913;</title><link>https://arxiv.org/abs/2403.00673</link><description>&lt;p&gt;
&#24555;&#29031;&#24378;&#21270;&#23398;&#20064;&#65306;&#21033;&#29992;&#20808;&#21069;&#36712;&#36857;&#25552;&#39640;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Snapshot Reinforcement Learning: Leveraging Prior Trajectories for Efficiency
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00673
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#24555;&#29031;&#24378;&#21270;&#23398;&#20064;&#65288;SnapshotRL&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#31616;&#21333;&#25913;&#21464;&#29615;&#22659;&#26469;&#22686;&#24378;&#26679;&#26412;&#25928;&#29575;&#65292;&#32780;&#26080;&#38656;&#23545;&#31639;&#27861;&#21644;&#27169;&#22411;&#36827;&#34892;&#20219;&#20309;&#20462;&#25913;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#31639;&#27861;&#38656;&#35201;&#22823;&#37327;&#26679;&#26412;&#21644;&#35745;&#31639;&#36164;&#28304;&#25165;&#33021;&#23454;&#29616;&#26356;&#39640;&#30340;&#24615;&#33021;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#23454;&#38469;&#24212;&#29992;&#24182;&#23545;&#36827;&#19968;&#27493;&#21457;&#23637;&#26500;&#25104;&#25361;&#25112;&#12290;&#37492;&#20110;&#36164;&#28304;&#26377;&#38480;&#30340;&#32422;&#26463;&#65292;&#21033;&#29992;&#29616;&#26377;&#30340;&#35745;&#31639;&#24037;&#20316;&#65288;&#20363;&#22914;&#23398;&#20064;&#31574;&#30053;&#12289;&#26679;&#26412;&#65289;&#26469;&#22686;&#24378;&#26679;&#26412;&#25928;&#29575;&#21644;&#20943;&#23569;DRL&#31639;&#27861;&#30340;&#35745;&#31639;&#36164;&#28304;&#28040;&#32791;&#33267;&#20851;&#37325;&#35201;&#12290;&#20197;&#21069;&#21033;&#29992;&#29616;&#26377;&#35745;&#31639;&#24037;&#20316;&#30340;&#30740;&#31350;&#38656;&#35201;&#23545;&#29616;&#26377;&#31639;&#27861;&#21644;&#27169;&#22411;&#36827;&#34892;&#24178;&#25200;&#24615;&#20462;&#25913;&#65292;&#19987;&#38376;&#20026;&#29305;&#23450;&#31639;&#27861;&#35774;&#35745;&#65292;&#32570;&#20047;&#28789;&#27963;&#24615;&#21644;&#36890;&#29992;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#24555;&#29031;&#24378;&#21270;&#23398;&#20064;&#65288;SnapshotRL&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#31616;&#21333;&#25913;&#21464;&#29615;&#22659;&#26469;&#22686;&#24378;&#26679;&#26412;&#25928;&#29575;&#65292;&#32780;&#26080;&#38656;&#23545;&#31639;&#27861;&#21644;&#27169;&#22411;&#36827;&#34892;&#20219;&#20309;&#20462;&#25913;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00673v1 Announce Type: new  Abstract: Deep reinforcement learning (DRL) algorithms require substantial samples and computational resources to achieve higher performance, which restricts their practical application and poses challenges for further development. Given the constraint of limited resources, it is essential to leverage existing computational work (e.g., learned policies, samples) to enhance sample efficiency and reduce the computational resource consumption of DRL algorithms. Previous works to leverage existing computational work require intrusive modifications to existing algorithms and models, designed specifically for specific algorithms, lacking flexibility and universality. In this paper, we present the Snapshot Reinforcement Learning (SnapshotRL) framework, which enhances sample efficiency by simply altering environments, without making any modifications to algorithms and models. By allowing student agents to choose states in teacher trajectories as the initi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22522;&#20110;&#39044;&#25490;&#24207;GNN&#30340;&#23450;&#26102;&#39044;&#27979;&#26041;&#27861;&#65292;&#21253;&#25324;&#20840;&#23616;&#30005;&#36335;&#39044;&#35757;&#32451;&#12289;&#23616;&#37096;&#26102;&#24310;&#23398;&#20064;&#21644;&#27880;&#24847;&#21147;&#21333;&#20803;&#24314;&#27169;&#65292;&#20197;&#35299;&#20915;&#22823;&#35268;&#27169;&#24037;&#19994;&#30005;&#36335;&#20013;&#30340;&#20449;&#21495;&#34928;&#20943;&#21644;&#35823;&#24046;&#32047;&#31215;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.00012</link><description>&lt;p&gt;
&#22522;&#20110;&#39044;&#25490;&#24207;GNN&#30340;&#23450;&#26102;&#39044;&#27979;&#65306;&#20840;&#23616;&#30005;&#36335;&#39044;&#35757;&#32451;&#65292;&#23616;&#37096;&#26102;&#24310;&#23398;&#20064;&#21644;&#27880;&#24847;&#21147;&#21333;&#20803;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
PreRoutGNN for Timing Prediction with Order Preserving Partition: Global Circuit Pre-training, Local Delay Learning and Attentional Cell Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00012
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22522;&#20110;&#39044;&#25490;&#24207;GNN&#30340;&#23450;&#26102;&#39044;&#27979;&#26041;&#27861;&#65292;&#21253;&#25324;&#20840;&#23616;&#30005;&#36335;&#39044;&#35757;&#32451;&#12289;&#23616;&#37096;&#26102;&#24310;&#23398;&#20064;&#21644;&#27880;&#24847;&#21147;&#21333;&#20803;&#24314;&#27169;&#65292;&#20197;&#35299;&#20915;&#22823;&#35268;&#27169;&#24037;&#19994;&#30005;&#36335;&#20013;&#30340;&#20449;&#21495;&#34928;&#20943;&#21644;&#35823;&#24046;&#32047;&#31215;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#36335;&#30001;&#23450;&#26102;&#39044;&#27979;&#26368;&#36817;&#34987;&#30740;&#31350;&#29992;&#20110;&#35780;&#20272;&#33455;&#29255;&#35774;&#35745;&#20013;&#20505;&#36873;&#21333;&#20803;&#24067;&#23616;&#30340;&#36136;&#37327;&#12290;&#23427;&#30452;&#25509;&#20272;&#35745;&#24341;&#33050;&#32423;&#65288;&#20313;&#37327;&#12289;&#26012;&#29575;&#65289;&#21644;&#36793;&#32423;&#65288;&#32593;&#24310;&#36831;&#12289;&#21333;&#20803;&#24310;&#36831;&#65289;&#30340;&#23450;&#26102;&#25351;&#26631;&#65292;&#32780;&#26080;&#38656;&#32791;&#26102;&#36335;&#30001;&#12290;&#28982;&#32780;&#65292;&#22312;&#22823;&#35268;&#27169;&#24037;&#19994;&#30005;&#36335;&#20013;&#65292;&#30001;&#20110;&#38271;&#26102;&#24310;&#36335;&#24452;&#65292;&#23427;&#32463;&#24120;&#36973;&#21463;&#20449;&#21495;&#34928;&#20943;&#21644;&#35823;&#24046;&#32047;&#31215;&#30340;&#22256;&#25200;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20840;&#23616;&#30005;&#36335;&#35757;&#32451;&#26469;&#39044;&#35757;&#32451;&#19968;&#20010;&#22270;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#20174;&#30005;&#36335;&#32593;&#34920;&#20013;&#23398;&#20064;&#20840;&#23616;&#22270;&#23884;&#20837;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#33410;&#28857;&#26356;&#26032;&#26041;&#26696;&#36827;&#34892;GCN&#19978;&#30340;&#28040;&#24687;&#20256;&#36882;&#65292;&#36981;&#24490;&#23398;&#20064;&#21040;&#30340;&#22270;&#23884;&#20837;&#21644;&#30005;&#36335;&#22270;&#30340;&#25299;&#25169;&#25490;&#24207;&#24207;&#21015;&#12290;&#36825;&#20010;&#26041;&#26696;&#22312;&#26356;&#26032;&#24207;&#21015;&#20013;&#27531;&#30041;&#22320;&#24314;&#27169;&#20102;&#30456;&#37051;&#24341;&#33050;&#20043;&#38388;&#30340;&#23616;&#37096;&#26102;&#38388;&#24310;&#36831;&#65292;&#24182;&#36890;&#36807;&#19968;&#20010;&#27880;&#24847;&#21147;&#21333;&#20803;&#25552;&#21462;&#20102;&#27599;&#20010;&#21333;&#20803;&#20869;&#37096;&#30340;&#26597;&#25214;&#34920;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00012v1 Announce Type: new  Abstract: Pre-routing timing prediction has been recently studied for evaluating the quality of a candidate cell placement in chip design. It involves directly estimating the timing metrics for both pin-level (slack, slew) and edge-level (net delay, cell delay), without time-consuming routing. However, it often suffers from signal decay and error accumulation due to the long timing paths in large-scale industrial circuits. To address these challenges, we propose a two-stage approach. First, we propose global circuit training to pre-train a graph auto-encoder that learns the global graph embedding from circuit netlist. Second, we use a novel node updating scheme for message passing on GCN, following the topological sorting sequence of the learned graph embedding and circuit graph. This scheme residually models the local time delay between two adjacent pins in the updating sequence, and extracts the lookup table information inside each cell via a ne
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#25968;&#25454;&#35299;&#37322;&#22120;&#65292;&#37319;&#29992;&#21160;&#24577;&#35268;&#21010;&#12289;&#24037;&#20855;&#38598;&#25104;&#21644;&#36923;&#36753;&#38169;&#35823;&#35782;&#21035;&#31561;&#20851;&#38190;&#25216;&#26415;&#65292;&#26088;&#22312;&#22686;&#24378;&#25968;&#25454;&#31185;&#23398;&#20013;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.18679</link><description>&lt;p&gt;
&#25968;&#25454;&#35299;&#37322;&#22120;&#65306;&#29992;&#20110;&#25968;&#25454;&#31185;&#23398;&#30340;LLM&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Data Interpreter: An LLM Agent For Data Science
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18679
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#25968;&#25454;&#35299;&#37322;&#22120;&#65292;&#37319;&#29992;&#21160;&#24577;&#35268;&#21010;&#12289;&#24037;&#20855;&#38598;&#25104;&#21644;&#36923;&#36753;&#38169;&#35823;&#35782;&#21035;&#31561;&#20851;&#38190;&#25216;&#26415;&#65292;&#26088;&#22312;&#22686;&#24378;&#25968;&#25454;&#31185;&#23398;&#20013;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20195;&#29702;&#24050;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#38656;&#35201;&#23454;&#26102;&#25968;&#25454;&#35843;&#25972;&#12289;&#20248;&#21270;&#19987;&#19994;&#30693;&#35782;&#20197;&#24212;&#23545;&#21508;&#31181;&#20219;&#21153;&#38388;&#22797;&#26434;&#20381;&#36182;&#24615;&#20197;&#21450;&#31934;&#30830;&#25512;&#29702;&#30340;&#36923;&#36753;&#38169;&#35823;&#35782;&#21035;&#30340;&#25968;&#25454;&#31185;&#23398;&#22330;&#26223;&#20013;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#21487;&#33021;&#20250;&#21463;&#21040;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#25968;&#25454;&#35299;&#37322;&#22120;&#65292;&#36825;&#26159;&#19968;&#20010;&#35774;&#35745;&#29992;&#20110;&#35299;&#20915;&#24378;&#35843;&#19977;&#31181;&#20851;&#38190;&#25216;&#26415;&#20197;&#22686;&#24378;&#25968;&#25454;&#31185;&#23398;&#20013;&#38382;&#39064;&#35299;&#20915;&#30340;&#26041;&#26696;&#30340;&#20195;&#30721;&#65306;1&#65289;&#20855;&#26377;&#20998;&#23618;&#22270;&#32467;&#26500;&#30340;&#21160;&#24577;&#35268;&#21010;&#65292;&#29992;&#20110;&#23454;&#26102;&#25968;&#25454;&#36866;&#24212;&#24615;&#65307;2&#65289;&#24037;&#20855;&#38598;&#25104;&#21160;&#24577;&#21270;&#65292;&#20197;&#22686;&#24378;&#20195;&#30721;&#25191;&#34892;&#36807;&#31243;&#20013;&#30340;&#29087;&#32451;&#24230;&#65292;&#20016;&#23500;&#24517;&#35201;&#30340;&#19987;&#19994;&#30693;&#35782;&#65307;3&#65289;&#22312;&#21453;&#39304;&#20013;&#35782;&#21035;&#36923;&#36753;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#36890;&#36807;&#32463;&#39564;&#35760;&#24405;&#26469;&#25552;&#39640;&#25928;&#29575;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25968;&#25454;&#35299;&#37322;&#22120;&#22312;&#21508;&#31181;&#25968;&#25454;&#31185;&#23398;&#21644;&#29616;&#23454;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;&#19982;&#24320;&#28304;&#22522;&#32447;&#30456;&#27604;&#65292;&#23427;&#23637;&#29616;&#20102;s
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18679v1 Announce Type: new  Abstract: Large Language Model (LLM)-based agents have demonstrated remarkable effectiveness. However, their performance can be compromised in data science scenarios that require real-time data adjustment, expertise in optimization due to complex dependencies among various tasks, and the ability to identify logical errors for precise reasoning. In this study, we introduce the Data Interpreter, a solution designed to solve with code that emphasizes three pivotal techniques to augment problem-solving in data science: 1) dynamic planning with hierarchical graph structures for real-time data adaptability;2) tool integration dynamically to enhance code proficiency during execution, enriching the requisite expertise;3) logical inconsistency identification in feedback, and efficiency enhancement through experience recording. We evaluate the Data Interpreter on various data science and real-world tasks. Compared to open-source baselines, it demonstrated s
&lt;/p&gt;</description></item><item><title>&#31526;&#21495;&#22238;&#24402;&#34987;&#35270;&#20026;&#19968;&#20010;&#22810;&#27169;&#24577;&#20219;&#21153;&#65292;&#30740;&#31350;&#20154;&#21592;&#23558;&#25968;&#25454;&#21040;&#34920;&#36798;&#24335;&#30340;&#26144;&#23556;&#35270;&#20026;&#32763;&#35793;&#38382;&#39064;&#65292;&#24341;&#20837;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.18603</link><description>&lt;p&gt;
MMSR&#65306;&#31526;&#21495;&#22238;&#24402;&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
MMSR: Symbolic Regression is a Multimodal Task
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18603
&lt;/p&gt;
&lt;p&gt;
&#31526;&#21495;&#22238;&#24402;&#34987;&#35270;&#20026;&#19968;&#20010;&#22810;&#27169;&#24577;&#20219;&#21153;&#65292;&#30740;&#31350;&#20154;&#21592;&#23558;&#25968;&#25454;&#21040;&#34920;&#36798;&#24335;&#30340;&#26144;&#23556;&#35270;&#20026;&#32763;&#35793;&#38382;&#39064;&#65292;&#24341;&#20837;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23398;&#20844;&#24335;&#26159;&#25506;&#32034;&#33258;&#28982;&#35268;&#24459;&#20960;&#21315;&#24180;&#26469;&#20154;&#31867;&#26234;&#24935;&#30340;&#32467;&#26230;&#12290;&#29992;&#31616;&#27905;&#30340;&#25968;&#23398;&#20844;&#24335;&#25551;&#36848;&#22797;&#26434;&#30340;&#33258;&#28982;&#35268;&#24459;&#26159;&#31185;&#23398;&#23478;&#19981;&#26029;&#36861;&#27714;&#30340;&#30446;&#26631;&#65292;&#20063;&#26159;&#20154;&#24037;&#26234;&#33021;&#38754;&#20020;&#30340;&#37325;&#22823;&#25361;&#25112;&#12290;&#36825;&#19968;&#39046;&#22495;&#34987;&#31216;&#20026;&#31526;&#21495;&#22238;&#24402;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#23558;&#20174;&#25968;&#25454;&#21040;&#34920;&#36798;&#24335;&#30340;&#26144;&#23556;&#35270;&#20026;&#32763;&#35793;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#30456;&#24212;&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18603v1 Announce Type: cross  Abstract: Mathematical formulas are the crystallization of human wisdom in exploring the laws of nature for thousands of years. Describing the complex laws of nature with a concise mathematical formula is a constant pursuit of scientists and a great challenge for artificial intelligence. This field is called symbolic regression. Symbolic regression was originally formulated as a combinatorial optimization problem, and GP and reinforcement learning algorithms were used to solve it. However, GP is sensitive to hyperparameters, and these two types of algorithms are inefficient. To solve this problem, researchers treat the mapping from data to expressions as a translation problem. And the corresponding large-scale pre-trained model is introduced. However, the data and expression skeletons do not have very clear word correspondences as the two languages do. Instead, they are more like two modalities (e.g., image and text). Therefore, in this paper, w
&lt;/p&gt;</description></item><item><title>RIME&#26159;&#19968;&#31181;&#38024;&#23545;&#22024;&#26434;&#20559;&#22909;&#30340;&#20581;&#22766;PbRL&#31639;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#36807;&#28388;&#21435;&#22122;&#20559;&#22909;&#21644;&#28909;&#21551;&#21160;&#22870;&#21169;&#27169;&#22411;&#65292;&#26497;&#22823;&#22686;&#24378;&#20102;&#29616;&#26377;&#26368;&#20808;&#36827;PbRL&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.17257</link><description>&lt;p&gt;
RIME: &#20855;&#26377;&#22024;&#26434;&#20559;&#22909;&#30340;&#20581;&#22766;&#20559;&#22909;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
RIME: Robust Preference-based Reinforcement Learning with Noisy Preferences
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17257
&lt;/p&gt;
&lt;p&gt;
RIME&#26159;&#19968;&#31181;&#38024;&#23545;&#22024;&#26434;&#20559;&#22909;&#30340;&#20581;&#22766;PbRL&#31639;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#36807;&#28388;&#21435;&#22122;&#20559;&#22909;&#21644;&#28909;&#21551;&#21160;&#22870;&#21169;&#27169;&#22411;&#65292;&#26497;&#22823;&#22686;&#24378;&#20102;&#29616;&#26377;&#26368;&#20808;&#36827;PbRL&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20559;&#22909;&#24378;&#21270;&#23398;&#20064;&#65288;PbRL&#65289;&#36890;&#36807;&#21033;&#29992;&#20154;&#31867;&#20559;&#22909;&#20316;&#20026;&#22870;&#21169;&#20449;&#21495;&#65292;&#36991;&#20813;&#20102;&#23545;&#22870;&#21169;&#35774;&#35745;&#30340;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;PbRL&#31639;&#27861;&#36807;&#24230;&#20381;&#36182;&#26469;&#33258;&#39046;&#22495;&#19987;&#23478;&#30340;&#39640;&#36136;&#37327;&#21453;&#39304;&#65292;&#23548;&#33268;&#32570;&#20047;&#40065;&#26834;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RIME&#65292;&#19968;&#31181;&#38024;&#23545;&#22024;&#26434;&#20559;&#22909;&#30340;&#20581;&#22766;PbRL&#31639;&#27861;&#65292;&#29992;&#20110;&#26377;&#25928;&#22320;&#20174;&#22024;&#26434;&#20559;&#22909;&#20013;&#23398;&#20064;&#22870;&#21169;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#22522;&#20110;&#26679;&#26412;&#36873;&#25321;&#30340;&#37492;&#21035;&#22120;&#65292;&#21160;&#24577;&#36807;&#28388;&#21435;&#22122;&#20559;&#22909;&#20197;&#36827;&#34892;&#20581;&#22766;&#35757;&#32451;&#12290;&#20026;&#20102;&#20943;&#36731;&#36873;&#25321;&#19981;&#27491;&#30830;&#36896;&#25104;&#30340;&#32047;&#31215;&#35823;&#24046;&#65292;&#25105;&#20204;&#25552;&#20986;&#28909;&#21551;&#21160;&#22870;&#21169;&#27169;&#22411;&#65292;&#27492;&#22806;&#36824;&#33021;&#22635;&#34917;PbRL&#20013;&#20174;&#39044;&#35757;&#32451;&#21040;&#22312;&#32447;&#35757;&#32451;&#36807;&#28193;&#26102;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#25105;&#20204;&#22312;&#26426;&#22120;&#20154;&#25805;&#32437;&#21644;&#36816;&#21160;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;RIME&#26174;&#33879;&#25552;&#21319;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;PbRL&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;&#28040;&#34701;&#30740;&#31350;&#36827;&#19968;&#27493;&#34920;&#26126;&#65292;&#28909;&#21551;&#21160;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17257v1 Announce Type: cross  Abstract: Preference-based Reinforcement Learning (PbRL) avoids the need for reward engineering by harnessing human preferences as the reward signal. However, current PbRL algorithms over-reliance on high-quality feedback from domain experts, which results in a lack of robustness. In this paper, we present RIME, a robust PbRL algorithm for effective reward learning from noisy preferences. Our method incorporates a sample selection-based discriminator to dynamically filter denoised preferences for robust training. To mitigate the accumulated error caused by incorrect selection, we propose to warm start the reward model, which additionally bridges the performance gap during transition from pre-training to online training in PbRL. Our experiments on robotic manipulation and locomotion tasks demonstrate that RIME significantly enhances the robustness of the current state-of-the-art PbRL method. Ablation studies further demonstrate that the warm star
&lt;/p&gt;</description></item><item><title>ITL&#26159;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#30340;&#36716;&#23548;&#24335;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#29616;&#23454;&#19990;&#30028;&#35774;&#32622;&#20013;&#33258;&#36866;&#24212;&#37319;&#26679;&#65292;&#20197;&#26368;&#22823;&#21270;&#20851;&#20110;&#25351;&#23450;&#39044;&#27979;&#30446;&#26631;&#30340;&#20449;&#24687;&#33719;&#21462;&#65292;&#24182;&#22312;&#23569;&#26679;&#26412;&#24494;&#35843;&#21644;&#23433;&#20840;&#36125;&#21494;&#26031;&#20248;&#21270;&#24212;&#29992;&#20013;&#26174;&#33879;&#20248;&#20110;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2402.15898</link><description>&lt;p&gt;
&#22522;&#20110;&#20449;&#24687;&#30340;&#36716;&#23548;&#24335;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Information-based Transductive Active Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15898
&lt;/p&gt;
&lt;p&gt;
ITL&#26159;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#30340;&#36716;&#23548;&#24335;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#29616;&#23454;&#19990;&#30028;&#35774;&#32622;&#20013;&#33258;&#36866;&#24212;&#37319;&#26679;&#65292;&#20197;&#26368;&#22823;&#21270;&#20851;&#20110;&#25351;&#23450;&#39044;&#27979;&#30446;&#26631;&#30340;&#20449;&#24687;&#33719;&#21462;&#65292;&#24182;&#22312;&#23569;&#26679;&#26412;&#24494;&#35843;&#21644;&#23433;&#20840;&#36125;&#21494;&#26031;&#20248;&#21270;&#24212;&#29992;&#20013;&#26174;&#33879;&#20248;&#20110;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#20027;&#21160;&#23398;&#20064;&#25512;&#24191;&#21040;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#20013;&#37319;&#26679;&#21463;&#38480;&#20110;&#21487;&#35775;&#38382;&#22495;&#30340;&#24773;&#20917;&#65292;&#32780;&#39044;&#27979;&#30446;&#26631;&#21487;&#33021;&#20301;&#20110;&#36825;&#20010;&#22495;&#20043;&#22806;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ITL&#65292;&#21363;&#22522;&#20110;&#20449;&#24687;&#30340;&#36716;&#23548;&#24335;&#23398;&#20064;&#65292;&#19968;&#31181;&#33258;&#36866;&#24212;&#37319;&#26679;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#26368;&#22823;&#21270;&#20851;&#20110;&#25351;&#23450;&#39044;&#27979;&#30446;&#26631;&#30340;&#20449;&#24687;&#33719;&#21462;&#12290;&#22312;&#19968;&#33324;&#27491;&#21017;&#24615;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;ITL&#25910;&#25947;&#21040;&#21487;&#20174;&#21487;&#35775;&#38382;&#25968;&#25454;&#20013;&#33719;&#24471;&#30340;&#26368;&#23567;&#21487;&#33021;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20851;&#38190;&#24212;&#29992;&#20013;&#23637;&#31034;&#20102;ITL&#65306;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#23569;&#26679;&#26412;&#24494;&#35843;&#21644;&#23433;&#20840;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#22312;&#20004;&#31181;&#24773;&#20917;&#19979;&#65292;ITL&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15898v1 Announce Type: cross  Abstract: We generalize active learning to address real-world settings where sampling is restricted to an accessible region of the domain, while prediction targets may lie outside this region. To this end, we propose ITL, short for information-based transductive learning, an approach which samples adaptively to maximize the information gained about specified prediction targets. We show, under general regularity assumptions, that ITL converges uniformly to the smallest possible uncertainty obtainable from the accessible data. We demonstrate ITL in two key applications: Few-shot fine-tuning of large neural networks and safe Bayesian optimization, and in both cases, ITL significantly outperforms the state-of-the-art.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;ITL&#26041;&#27861;&#26469;&#23454;&#29616;&#20027;&#21160;&#23569;&#26679;&#26412;&#24494;&#35843;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#20449;&#24687;&#33719;&#21462;&#65292;&#20174;&#32780;&#22312;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#24494;&#35843;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.15441</link><description>&lt;p&gt;
&#20027;&#21160;&#23569;&#26679;&#26412;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Active Few-Shot Fine-Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15441
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;ITL&#26041;&#27861;&#26469;&#23454;&#29616;&#20027;&#21160;&#23569;&#26679;&#26412;&#24494;&#35843;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#20449;&#24687;&#33719;&#21462;&#65292;&#20174;&#32780;&#22312;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#24494;&#35843;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#23545;&#19979;&#28216;&#20219;&#21153;&#36827;&#34892;&#20027;&#21160;&#23569;&#26679;&#26412;&#24494;&#35843;&#12290;&#25105;&#20204;&#34920;&#26126;&#23569;&#26679;&#26412;&#24494;&#35843;&#26159;&#20256;&#32479;&#20027;&#21160;&#23398;&#20064;&#21644;&#36716;&#23548;&#20027;&#21160;&#23398;&#20064;&#30340;&#27867;&#21270;&#23454;&#20363;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20449;&#24687;&#22522;&#20110;&#36716;&#23548;&#23398;&#20064;&#65288;ITL&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33258;&#36866;&#24212;&#22320;&#36827;&#34892;&#37319;&#26679;&#20197;&#26368;&#22823;&#21270;&#33719;&#24471;&#23545;&#25351;&#23450;&#19979;&#28216;&#20219;&#21153;&#30340;&#20449;&#24687;&#12290;&#22312;&#19968;&#33324;&#27491;&#21017;&#24615;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;ITL&#22343;&#21248;&#25910;&#25947;&#21040;&#21487;&#20174;&#21487;&#35775;&#38382;&#25968;&#25454;&#33719;&#21462;&#30340;&#26368;&#23567;&#21487;&#33021;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#26159;&#39318;&#25209;&#25512;&#23548;&#20986;&#36825;&#31181;&#27867;&#21270;&#30028;&#38480;&#30340;&#20154;&#65292;&#36825;&#23545;&#20110;&#20027;&#21160;&#23398;&#20064;&#21487;&#33021;&#26159;&#20855;&#26377;&#29420;&#31435;&#24847;&#20041;&#30340;&#12290;&#25105;&#20204;&#23558;ITL&#24212;&#29992;&#20110;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#23569;&#26679;&#26412;&#24494;&#35843;&#20013;&#65292;&#32467;&#26524;&#26174;&#31034;ITL&#26126;&#26174;&#25913;&#36827;&#20102;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15441v1 Announce Type: cross  Abstract: We study the active few-shot fine-tuning of large neural networks to downstream tasks. We show that few-shot fine-tuning is an instance of a generalization of classical active learning, transductive active learning, and we propose ITL, short for information-based transductive learning, an approach which samples adaptively to maximize the information gained about specified downstream tasks. Under general regularity assumptions, we prove that ITL converges uniformly to the smallest possible uncertainty obtainable from the accessible data. To the best of our knowledge, we are the first to derive generalization bounds of this kind, and they may be of independent interest for active learning. We apply ITL to the few-shot fine-tuning of large neural networks and show that ITL substantially improves upon the state-of-the-art.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#21322;&#30417;&#30563;&#26694;&#26550;SigNova&#65292;&#29992;&#20110;&#26816;&#27979;&#23556;&#30005;&#22825;&#25991;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#20540;&#65292;&#37319;&#29992;&#29305;&#24449;&#36716;&#25442;&#25552;&#21462;&#25688;&#35201;&#32479;&#35745;&#20449;&#24687;&#24182;&#35745;&#31639;&#26032;&#39062;&#24615;&#35780;&#20998;&#65292;&#20197;&#35782;&#21035;&#20559;&#31163;&#39044;&#26399;&#34892;&#20026;&#30340;&#35266;&#23519;&#33539;&#22260;&#12290;</title><link>https://arxiv.org/abs/2402.14892</link><description>&lt;p&gt;
&#21033;&#29992;&#29305;&#24449;&#23545;&#23556;&#30005;&#22825;&#25991;&#25968;&#25454;&#36827;&#34892;&#26032;&#39062;&#24615;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Novelty Detection on Radio Astronomy Data using Signatures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14892
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#21322;&#30417;&#30563;&#26694;&#26550;SigNova&#65292;&#29992;&#20110;&#26816;&#27979;&#23556;&#30005;&#22825;&#25991;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#20540;&#65292;&#37319;&#29992;&#29305;&#24449;&#36716;&#25442;&#25552;&#21462;&#25688;&#35201;&#32479;&#35745;&#20449;&#24687;&#24182;&#35745;&#31639;&#26032;&#39062;&#24615;&#35780;&#20998;&#65292;&#20197;&#35782;&#21035;&#20559;&#31163;&#39044;&#26399;&#34892;&#20026;&#30340;&#35266;&#23519;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;SigNova&#65292;&#19968;&#20010;&#26032;&#30340;&#21322;&#30417;&#30563;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#27979;&#27969;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#20540;&#12290;&#23613;&#31649;&#25105;&#20204;&#26368;&#21021;&#30340;&#20363;&#23376;&#20391;&#37325;&#20110;&#22312;&#23556;&#30005;&#22825;&#25991;&#23398;&#39046;&#22495;&#20869;&#26816;&#27979;&#25968;&#23383;&#20449;&#21495;&#20013;&#30340;&#23556;&#39057;&#24178;&#25200;&#65288;RFI&#65289;&#65292;&#20294;&#37325;&#35201;&#30340;&#26159;&#35201;&#27880;&#24847;&#65292;SigNova&#30340;&#36866;&#29992;&#33539;&#22260;&#25193;&#23637;&#21040;&#20219;&#20309;&#31867;&#22411;&#30340;&#27969;&#25968;&#25454;&#12290;&#35813;&#26694;&#26550;&#30001;&#19977;&#20010;&#20027;&#35201;&#32452;&#20214;&#32452;&#25104;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#29305;&#24449;&#36716;&#25442;&#20174;&#35266;&#27979;&#24207;&#21015;&#20013;&#25552;&#21462;&#19968;&#32452;&#35268;&#33539;&#30340;&#25688;&#35201;&#32479;&#35745;&#20449;&#24687;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#21487;&#21464;&#38271;&#24230;&#30340;&#21487;&#35265;&#24615;&#26679;&#26412;&#34920;&#31034;&#20026;&#26377;&#38480;&#32500;&#29305;&#24449;&#21521;&#37327;&#12290;&#20854;&#27425;&#65292;&#27599;&#20010;&#29305;&#24449;&#21521;&#37327;&#34987;&#20998;&#37197;&#19968;&#20010;&#26032;&#39062;&#24615;&#35780;&#20998;&#65292;&#35745;&#31639;&#20026;&#21040;&#26080;RFI&#35757;&#32451;&#38598;&#20013;&#26368;&#36817;&#37051;&#30340;&#39532;&#27663;&#36317;&#31163;&#12290;&#36890;&#36807;&#35774;&#23450;&#36825;&#20123;&#20998;&#25968;&#30340;&#38408;&#20540;&#65292;&#25105;&#20204;&#35782;&#21035;&#20986;&#20559;&#31163;&#26080;RFI&#21487;&#35265;&#24615;&#26679;&#26412;&#39044;&#26399;&#34892;&#20026;&#30340;&#35266;&#23519;&#33539;&#22260;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#20005;&#26684;&#30340;&#20998;&#24067;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14892v1 Announce Type: cross  Abstract: We introduce SigNova, a new semi-supervised framework for detecting anomalies in streamed data. While our initial examples focus on detecting radio-frequency interference (RFI) in digitized signals within the field of radio astronomy, it is important to note that SigNova's applicability extends to any type of streamed data. The framework comprises three primary components. Firstly, we use the signature transform to extract a canonical collection of summary statistics from observational sequences. This allows us to represent variable-length visibility samples as finite-dimensional feature vectors. Secondly, each feature vector is assigned a novelty score, calculated as the Mahalanobis distance to its nearest neighbor in an RFI-free training set. By thresholding these scores we identify observation ranges that deviate from the expected behavior of RFI-free visibility samples without relying on stringent distributional assumptions. Thirdl
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#21152;&#36895;&#20855;&#26377;RoPE&#30340;transformer&#25512;&#26029;&#30340;&#25216;&#24039;&#65292;&#36890;&#36807;&#39044;&#35745;&#31639;&#31532;&#19968;&#23618;&#26469;&#38477;&#20302;&#24310;&#36831;&#21644;&#25104;&#26412;&#65292;&#26368;&#22823;&#33410;&#30465;&#21462;&#20915;&#20110;&#24635;&#23618;&#25968;&#12290;</title><link>https://arxiv.org/abs/2402.13388</link><description>&lt;p&gt;
Transformer &#25216;&#24039;&#65306;&#39044;&#35745;&#31639;&#31532;&#19968;&#23618;
&lt;/p&gt;
&lt;p&gt;
Transformer tricks: Precomputing the first layer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13388
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#21152;&#36895;&#20855;&#26377;RoPE&#30340;transformer&#25512;&#26029;&#30340;&#25216;&#24039;&#65292;&#36890;&#36807;&#39044;&#35745;&#31639;&#31532;&#19968;&#23618;&#26469;&#38477;&#20302;&#24310;&#36831;&#21644;&#25104;&#26412;&#65292;&#26368;&#22823;&#33410;&#30465;&#21462;&#20915;&#20110;&#24635;&#23618;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#31616;&#30701;&#30340;&#35770;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#21152;&#36895;&#20855;&#26377; RoPE&#65288;&#22914; LLaMA&#12289;Mistral &#21644; PaLM&#65289;&#30340; transformer &#25512;&#26029;&#30340;&#25216;&#24039;&#12290;&#23545;&#20110;&#36825;&#20123;&#27169;&#22411;&#65292;&#31532;&#19968;&#20010; transformer &#23618;&#30340;&#22823;&#37096;&#20998;&#20869;&#23481;&#21487;&#20197;&#39044;&#20808;&#35745;&#31639;&#65292;&#20174;&#32780;&#23548;&#33268;&#31245;&#20302;&#30340;&#24310;&#36831;&#21644;&#26356;&#20302;&#30340;&#27599;&#20196;&#29260;&#25104;&#26412;&#12290;&#22240;&#20026;&#36825;&#31181;&#25216;&#24039;&#20165;&#20248;&#21270;&#20102;&#19968;&#23618;&#65292;&#30456;&#23545;&#33410;&#30465;&#21462;&#20915;&#20110;&#24635;&#23618;&#25968;&#12290;&#20363;&#22914;&#65292;&#23545;&#20110;&#21482;&#26377; 4 &#23618;&#30340;&#27169;&#22411;&#65288;&#22914; Whisper tiny&#65289;&#65292;&#26368;&#22823;&#33410;&#30465;&#20165;&#38480;&#20110; 25%&#65292;&#32780;&#23545;&#20110; 32 &#23618;&#27169;&#22411;&#65288;&#22914; Mistral-7B&#65289;&#65292;&#33410;&#30465;&#21017;&#26159; 3%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13388v1 Announce Type: new  Abstract: This short paper describes a trick to speed up inference of transformers with RoPE (such as LLaMA, Mistral, and PaLM). For these models, a large portion of the first transformer layer can be precomputed, which results in slightly lower latency and lower cost-per-token. Because this trick optimizes only one layer, the relative savings depend on the total number of layers. For example, the maximum savings for a model with only 4 layers (such as Whisper tiny) is limited to 25%, while a 32-layer model (such as Mistral-7B) is limited to 3% savings.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;CounterCurate&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#27604;&#20363;&#23376;&#21644;&#29983;&#25104;&#24335;&#24494;&#35843;&#65292;&#20840;&#38754;&#25552;&#21319;&#35270;&#35273;-&#35821;&#35328;&#32452;&#21512;&#25512;&#29702;&#33021;&#21147;&#65292;&#35299;&#20915;&#20102;&#29289;&#29702;&#25512;&#29702;&#21644;&#35821;&#20041;&#23545;&#29031;&#24494;&#35843;&#26041;&#38754;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.13254</link><description>&lt;p&gt;
CounterCurate: &#36890;&#36807;&#23545;&#29031;&#20363;&#23376;&#22686;&#24378;&#29289;&#29702;&#21644;&#35821;&#20041;&#35270;&#35273;-&#35821;&#35328;&#32452;&#21512;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
CounterCurate: Enhancing Physical and Semantic Visio-Linguistic Compositional Reasoning via Counterfactual Examples
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13254
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;CounterCurate&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#27604;&#20363;&#23376;&#21644;&#29983;&#25104;&#24335;&#24494;&#35843;&#65292;&#20840;&#38754;&#25552;&#21319;&#35270;&#35273;-&#35821;&#35328;&#32452;&#21512;&#25512;&#29702;&#33021;&#21147;&#65292;&#35299;&#20915;&#20102;&#29289;&#29702;&#25512;&#29702;&#21644;&#35821;&#20041;&#23545;&#29031;&#24494;&#35843;&#26041;&#38754;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;CounterCurate&#65292;&#19968;&#20010;&#26694;&#26550;&#65292;&#20840;&#38754;&#25552;&#21319;&#23545;&#27604;&#21644;&#29983;&#25104;&#24335;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#35270;&#35273;-&#35821;&#35328;&#32452;&#21512;&#25512;&#29702;&#33021;&#21147;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#20004;&#20010;&#23578;&#26410;&#20805;&#20998;&#25506;&#35752;&#30340;&#20851;&#38190;&#38382;&#39064;&#65306;&#24573;&#35270;&#20102;&#22522;&#20110;&#29289;&#29702;&#30340;&#25512;&#29702;&#65288;&#35745;&#25968;&#21644;&#20301;&#32622;&#29702;&#35299;&#65289;&#65292;&#20197;&#21450;&#21033;&#29992;&#39640;&#24615;&#33021;&#25991;&#26412;&#21644;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#35821;&#20041;&#21453;&#20107;&#23454;&#24494;&#35843;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24320;&#21019;&#20102;&#19968;&#20010;&#35299;&#20915;&#36825;&#20123;&#31354;&#30333;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#31361;&#20986;&#20102;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;&#22914;CLIP&#21644;LLaVA&#65289;&#22312;&#22522;&#20110;&#29289;&#29702;&#30340;&#32452;&#21512;&#25512;&#29702;&#20013;&#20960;&#20046;&#26080;&#27861;&#32988;&#20219;&#30340;&#34920;&#29616;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24212;&#29992;&#31616;&#21333;&#30340;&#25968;&#25454;&#22686;&#24378;&#65292;&#20351;&#29992;&#22522;&#20110;&#22270;&#20687;&#30340;&#29983;&#25104;&#27169;&#22411;GLIGEN&#29983;&#25104;&#24494;&#35843;&#25968;&#25454;&#65292;&#20351;&#24471;&#24615;&#33021;&#26174;&#33879;&#25552;&#39640;&#65306;&#22312;&#25105;&#20204;&#26032;&#30340;&#31574;&#21010;&#30340;Flickr30k-Positions&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;CLIP&#21644;LLaVA&#30340;&#24615;&#33021;&#20998;&#21035;&#25552;&#39640;&#20102;+33%&#21644;+37%&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#39640;&#24615;&#33021;&#25991;&#26412;&#21644;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13254v1 Announce Type: cross  Abstract: We propose CounterCurate, a framework to comprehensively improve the visio-linguistic compositional reasoning capability for both contrastive and generative multimodal models. In particular, we identify two under-explored critical problems: the neglect of the physically grounded reasoning (counting and position understanding) and the potential of using highly capable text and image generation models for semantic counterfactual fine-tuning. Our work pioneers an approach that addresses these gaps. We first spotlight the near-chance performance of multimodal models like CLIP and LLaVA in physically grounded compositional reasoning. We then apply simple data augmentation using a grounded image generation model, GLIGEN, to generate finetuning data, resulting in significant performance improvements: +33% and +37% for CLIP and LLaVA, respectively, on our newly curated Flickr30k-Positions benchmark. Moreover, we exploit the capabilities of hig
&lt;/p&gt;</description></item><item><title>Mafin&#36890;&#36807;&#24341;&#20837;&#27169;&#22411;&#22686;&#24378;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#21482;&#26377;&#40657;&#30418;&#23884;&#20837;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.12177</link><description>&lt;p&gt;
Mafin: &#29992;&#27169;&#22411;&#22686;&#24378;&#24494;&#35843;&#26469;&#22686;&#24378;&#40657;&#30418;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Mafin: Enhancing Black-Box Embeddings with Model Augmented Fine-tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12177
&lt;/p&gt;
&lt;p&gt;
Mafin&#36890;&#36807;&#24341;&#20837;&#27169;&#22411;&#22686;&#24378;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#21482;&#26377;&#40657;&#30418;&#23884;&#20837;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#24050;&#32463;&#25104;&#20026;&#32531;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#24187;&#35273;&#30340;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;RAG&#20013;&#30340;&#26816;&#32034;&#38454;&#27573;&#36890;&#24120;&#28041;&#21450;&#39044;&#35757;&#32451;&#30340;&#23884;&#20837;&#27169;&#22411;&#65292;&#23558;&#26597;&#35810;&#21644;&#27573;&#33853;&#36716;&#25442;&#20026;&#21521;&#37327;&#20197;&#25429;&#33719;&#23427;&#20204;&#30340;&#35821;&#20041;&#12290;&#28982;&#32780;&#65292;&#24403;&#24212;&#29992;&#20110;&#29305;&#23450;&#39046;&#22495;&#30693;&#35782;&#26102;&#65292;&#26631;&#20934;&#30340;&#39044;&#35757;&#32451;&#23884;&#20837;&#27169;&#22411;&#21487;&#33021;&#34920;&#29616;&#20986;&#27425;&#20248;&#24615;&#33021;&#65292;&#38656;&#35201;&#36827;&#34892;&#24494;&#35843;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#20165;&#33021;&#20174;&#40657;&#30418;&#27169;&#22411;&#33719;&#21462;&#23884;&#20837;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#27169;&#22411;&#22686;&#24378;&#24494;&#35843;&#65288;Mafin&#65289;--&#19968;&#31181;&#36890;&#36807;&#29992;&#21487;&#35757;&#32451;&#30340;&#23884;&#20837;&#27169;&#22411;&#22686;&#24378;&#40657;&#30418;&#23884;&#20837;&#27169;&#22411;&#26469;&#36827;&#34892;&#24494;&#35843;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;Mafin&#20165;&#38656;&#35201;&#35757;&#32451;&#19968;&#20010;&#23567;&#30340;&#22686;&#24378;&#27169;&#22411;&#23601;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#40657;&#30418;&#23884;&#20837;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26377;&#26631;&#31614;&#21644;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12177v1 Announce Type: cross  Abstract: Retrieval Augmented Generation (RAG) has emerged as an effective solution for mitigating hallucinations in Large Language Models (LLMs). The retrieval stage in RAG typically involves a pre-trained embedding model, which converts queries and passages into vectors to capture their semantics. However, a standard pre-trained embedding model may exhibit sub-optimal performance when applied to specific domain knowledge, necessitating fine-tuning. This paper addresses scenarios where the embeddings are only available from a black-box model. We introduce Model augmented fine-tuning (Mafin) -- a novel approach for fine-tuning a black-box embedding model by augmenting it with a trainable embedding model. Our results demonstrate that Mafin significantly enhances the performance of the black-box embeddings by only requiring the training of a small augmented model. We validate the effectiveness of our method on both labeled and unlabeled datasets, 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31574;&#30053;&#25193;&#25955;&#21644;3D&#22330;&#26223;&#34920;&#31034;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;3D Diffuser Actor&#65292;&#19968;&#20010;&#31070;&#32463;&#31574;&#30053;&#26550;&#26500;&#65292;&#21487;&#20197;&#26681;&#25454;&#35821;&#35328;&#25351;&#20196;&#26500;&#24314;3D&#35270;&#35273;&#22330;&#26223;&#34920;&#31034;&#65292;&#24182;&#23545;&#26426;&#22120;&#20154;&#26411;&#31471;&#25191;&#34892;&#22120;&#30340;3D&#26059;&#36716;&#21644;&#24179;&#31227;&#36827;&#34892;&#36845;&#20195;&#21435;&#22122;&#12290;</title><link>https://arxiv.org/abs/2402.10885</link><description>&lt;p&gt;
&#22522;&#20110;3D&#22330;&#26223;&#34920;&#31034;&#30340;3D&#25193;&#25955;&#22120;Actor&#65306;&#36890;&#36807;&#31574;&#30053;&#25193;&#25955;&#36827;&#34892;&#26426;&#22120;&#20154;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
3D Diffuser Actor: Policy Diffusion with 3D Scene Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10885
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31574;&#30053;&#25193;&#25955;&#21644;3D&#22330;&#26223;&#34920;&#31034;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;3D Diffuser Actor&#65292;&#19968;&#20010;&#31070;&#32463;&#31574;&#30053;&#26550;&#26500;&#65292;&#21487;&#20197;&#26681;&#25454;&#35821;&#35328;&#25351;&#20196;&#26500;&#24314;3D&#35270;&#35273;&#22330;&#26223;&#34920;&#31034;&#65292;&#24182;&#23545;&#26426;&#22120;&#20154;&#26411;&#31471;&#25191;&#34892;&#22120;&#30340;3D&#26059;&#36716;&#21644;&#24179;&#31227;&#36827;&#34892;&#36845;&#20195;&#21435;&#22122;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#25193;&#25955;&#31574;&#30053;&#21644;3D&#22330;&#26223;&#34920;&#31034;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#26426;&#22120;&#20154;&#25805;&#20316;&#12290;&#25193;&#25955;&#31574;&#30053;&#36890;&#36807;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#22522;&#20110;&#26426;&#22120;&#20154;&#21644;&#29615;&#22659;&#29366;&#24577;&#30340;&#21160;&#20316;&#20998;&#24067;&#12290;&#26368;&#36817;&#65292;&#23427;&#20204;&#24050;&#32463;&#34920;&#29616;&#20986;&#20248;&#20110;&#30830;&#23450;&#24615;&#21644;&#20854;&#20182;&#22522;&#20110;&#29366;&#24577;&#30340;&#21160;&#20316;&#20998;&#24067;&#23398;&#20064;&#26041;&#27861;&#12290;3D&#26426;&#22120;&#20154;&#31574;&#30053;&#20351;&#29992;&#20174;&#21333;&#20010;&#25110;&#22810;&#20010;&#25668;&#20687;&#22836;&#35270;&#35282;&#33719;&#21462;&#30340;&#24863;&#24212;&#28145;&#24230;&#32858;&#21512;&#30340;3D&#22330;&#26223;&#29305;&#24449;&#34920;&#31034;&#12290;&#23427;&#20204;&#24050;&#32463;&#35777;&#26126;&#27604;&#20854;2D&#23545;&#24212;&#29289;&#22312;&#25668;&#20687;&#26426;&#35270;&#35282;&#19978;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#32479;&#19968;&#20102;&#36825;&#20004;&#26465;&#32447;&#36335;&#30340;&#24037;&#20316;&#65292;&#24182;&#25552;&#20986;&#20102;3D&#25193;&#25955;&#22120;Actor&#65292;&#36825;&#26159;&#19968;&#20010;&#31070;&#32463;&#31574;&#30053;&#26550;&#26500;&#65292;&#23427;&#22312;&#32473;&#23450;&#35821;&#35328;&#25351;&#20196;&#30340;&#24773;&#20917;&#19979;&#65292;&#26500;&#24314;&#35270;&#35273;&#22330;&#26223;&#30340;3D&#34920;&#31034;&#65292;&#24182;&#22312;&#20854;&#19978;&#36827;&#34892;&#26465;&#20214;&#36845;&#20195;&#21435;&#22122;&#26426;&#22120;&#20154;&#26411;&#31471;&#25191;&#34892;&#22120;&#30340;3D&#26059;&#36716;&#21644;&#24179;&#31227;&#12290;&#22312;&#27599;&#20010;&#21435;&#22122;&#36845;&#20195;&#20013;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23558;&#26411;&#31471;&#25191;&#34892;&#22120;&#23039;&#24577;&#20272;&#35745;&#34920;&#31034;&#20026;3D&#22330;&#26223;&#20196;&#29260;&#65292;&#24182;&#39044;&#27979;t
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10885v1 Announce Type: cross  Abstract: We marry diffusion policies and 3D scene representations for robot manipulation. Diffusion policies learn the action distribution conditioned on the robot and environment state using conditional diffusion models. They have recently shown to outperform both deterministic and alternative state-conditioned action distribution learning methods. 3D robot policies use 3D scene feature representations aggregated from a single or multiple camera views using sensed depth. They have shown to generalize better than their 2D counterparts across camera viewpoints. We unify these two lines of work and present 3D Diffuser Actor, a neural policy architecture that, given a language instruction, builds a 3D representation of the visual scene and conditions on it to iteratively denoise 3D rotations and translations for the robot's end-effector. At each denoising iteration, our model represents end-effector pose estimates as 3D scene tokens and predicts t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#39044;&#35757;&#32451;&#29983;&#25104;&#37319;&#26679;&#22120;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#32467;&#21512;MD&#27169;&#25311;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#34507;&#30333;&#36136;&#26500;&#35937;&#37319;&#26679;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.10433</link><description>&lt;p&gt;
&#23558;&#31070;&#32463;&#19982;&#29289;&#29702;&#34701;&#21512;&#65306;&#29992;&#21487;&#22788;&#29702;&#30340;&#27169;&#25311;&#22686;&#24378;&#34507;&#30333;&#36136;&#26500;&#35937;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Fusing Neural and Physical: Augment Protein Conformation Sampling with Tractable Simulations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10433
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#39044;&#35757;&#32451;&#29983;&#25104;&#37319;&#26679;&#22120;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#32467;&#21512;MD&#27169;&#25311;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#34507;&#30333;&#36136;&#26500;&#35937;&#37319;&#26679;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34507;&#30333;&#36136;&#21160;&#21147;&#23398;&#23545;&#20110;&#23427;&#20204;&#30340;&#29983;&#29289;&#21151;&#33021;&#21644;&#24615;&#36136;&#38750;&#24120;&#26222;&#36941;&#19988;&#37325;&#35201;&#65292;&#30740;&#31350;&#36890;&#24120;&#28041;&#21450;&#32791;&#26102;&#30340;&#20998;&#23376;&#21160;&#21147;&#23398;(MD)&#27169;&#25311;&#12290;&#26368;&#36817;&#65292;&#29983;&#25104;&#27169;&#22411;&#34987;&#21033;&#29992;&#20316;&#20026;&#19968;&#20010;&#26367;&#20195;&#37319;&#26679;&#22120;&#65292;&#20197;&#27604;&#20256;&#32479;&#26041;&#27861;&#24555;&#20960;&#20010;&#25968;&#37327;&#32423;&#22320;&#33719;&#24471;&#26500;&#35937;&#38598;&#21512;&#65292;&#32780;&#19988;&#19981;&#38656;&#35201;&#20219;&#20309;&#27169;&#25311;&#25968;&#25454;&#65288;&#8220;&#38646;&#27425;&#25512;&#26029;&#8221;&#65289;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#19981;&#32771;&#34385;&#24213;&#23618;&#33021;&#37327;&#26223;&#35266;&#65292;&#36825;&#31181;&#29983;&#25104;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#20173;&#28982;&#21487;&#33021;&#21463;&#21040;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#36825;&#31181;&#39044;&#35757;&#32451;&#29983;&#25104;&#37319;&#26679;&#22120;&#30340;&#23569;&#26679;&#26412;&#35774;&#32622;&#65292;&#23427;&#20197;&#19968;&#31181;&#21487;&#22788;&#29702;&#30340;&#26041;&#24335;&#32467;&#21512;&#20102;MD&#27169;&#25311;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23545;&#20110;&#19968;&#20010;&#30446;&#26631;&#34507;&#30333;&#36136;&#65292;&#25105;&#20204;&#39318;&#20808;&#20174;&#39044;&#35757;&#32451;&#37319;&#26679;&#22120;&#20013;&#33719;&#21462;&#19968;&#20123;&#31181;&#23376;&#26500;&#35937;&#65292;&#28982;&#21518;&#20174;&#36825;&#20123;&#31181;&#23376;&#26679;&#26412;&#24320;&#22987;&#36827;&#34892;&#19968;&#31995;&#21015;&#29289;&#29702;&#27169;&#25311;&#12290;&#28982;&#21518;&#25105;&#20204;&#36890;&#36807;&#27169;&#25311;&#36712;&#36857;&#23545;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10433v1 Announce Type: cross  Abstract: The protein dynamics are common and important for their biological functions and properties, the study of which usually involves time-consuming molecular dynamics (MD) simulations in silico. Recently, generative models has been leveraged as a surrogate sampler to obtain conformation ensembles with orders of magnitude faster and without requiring any simulation data (a "zero-shot" inference). However, being agnostic of the underlying energy landscape, the accuracy of such generative model may still be limited. In this work, we explore the few-shot setting of such pre-trained generative sampler which incorporates MD simulations in a tractable manner. Specifically, given a target protein of interest, we first acquire some seeding conformations from the pre-trained sampler followed by a number of physical simulations in parallel starting from these seeding samples. Then we fine-tuned the generative model using the simulation trajectories a
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#20102;StyleGAN3&#27169;&#22411;&#20013;&#21028;&#21035;&#22120;&#30340;&#30149;&#24577;&#20559;&#35265;&#65292;&#23427;&#22312;&#22270;&#20687;&#21644;&#38754;&#37096;&#36136;&#37327;&#19978;&#30340;&#24471;&#20998;&#20998;&#23618;&#24433;&#21709;&#20102;&#19981;&#21516;&#24615;&#21035;&#12289;&#31181;&#26063;&#21644;&#20854;&#20182;&#31867;&#21035;&#30340;&#22270;&#20687;&#12290;</title><link>https://arxiv.org/abs/2402.09786</link><description>&lt;p&gt;
&#26816;&#26597;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21028;&#21035;&#22120;&#20013;&#30340;&#30149;&#24577;&#20559;&#35265;&#65306;&#20197;StyleGAN3&#27169;&#22411;&#20026;&#20363;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Examining Pathological Bias in a Generative Adversarial Network Discriminator: A Case Study on a StyleGAN3 Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09786
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#20102;StyleGAN3&#27169;&#22411;&#20013;&#21028;&#21035;&#22120;&#30340;&#30149;&#24577;&#20559;&#35265;&#65292;&#23427;&#22312;&#22270;&#20687;&#21644;&#38754;&#37096;&#36136;&#37327;&#19978;&#30340;&#24471;&#20998;&#20998;&#23618;&#24433;&#21709;&#20102;&#19981;&#21516;&#24615;&#21035;&#12289;&#31181;&#26063;&#21644;&#20854;&#20182;&#31867;&#21035;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21487;&#20197;&#29983;&#25104;&#36924;&#30495;&#30340;&#20154;&#33080;&#65292;&#24448;&#24448;&#38590;&#20197;&#34987;&#20154;&#31867;&#21306;&#20998;&#20986;&#26469;&#12290;&#25105;&#20204;&#21457;&#29616;&#39044;&#35757;&#32451;&#30340;StyleGAN3&#27169;&#22411;&#20013;&#30340;&#21028;&#21035;&#22120;&#22312;&#22270;&#20687;&#21644;&#38754;&#37096;&#36136;&#37327;&#19978;&#31995;&#32479;&#22320;&#23545;&#24471;&#20998;&#36827;&#34892;&#20998;&#23618;&#65292;&#24182;&#19988;&#36825;&#19981;&#25104;&#27604;&#20363;&#22320;&#24433;&#21709;&#20102;&#19981;&#21516;&#24615;&#21035;&#12289;&#31181;&#26063;&#21644;&#20854;&#20182;&#31867;&#21035;&#30340;&#22270;&#20687;&#12290;&#25105;&#20204;&#26816;&#26597;&#20102;&#21028;&#21035;&#22120;&#22312;&#33394;&#24425;&#21644;&#20142;&#24230;&#26041;&#38754;&#23545;&#24863;&#30693;&#30340;&#31181;&#26063;&#21644;&#24615;&#21035;&#30340;&#20559;&#35265;&#65292;&#28982;&#21518;&#26816;&#26597;&#20102;&#31038;&#20250;&#24515;&#29702;&#23398;&#20013;&#20851;&#20110;&#21051;&#26495;&#21360;&#35937;&#30740;&#31350;&#20013;&#24120;&#35265;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09786v1 Announce Type: cross  Abstract: Generative adversarial networks generate photorealistic faces that are often indistinguishable by humans from real faces. We find that the discriminator in the pre-trained StyleGAN3 model, a popular GAN network, systematically stratifies scores by both image- and face-level qualities and that this disproportionately affects images across gender, race, and other categories. We examine the discriminator's bias for color and luminance across axes perceived race and gender; we then examine axes common in research on stereotyping in social psychology.
&lt;/p&gt;</description></item><item><title>NetInfoF&#26694;&#26550;&#25552;&#20379;&#20102;&#19968;&#31181;&#24555;&#36895;&#24230;&#37327;&#21644;&#21033;&#29992;&#33410;&#28857;&#23646;&#24615;&#22270;&#20013;&#30340;&#21487;&#21033;&#29992;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#33021;&#21516;&#26102;&#22788;&#29702;&#38142;&#36335;&#39044;&#27979;&#21644;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#20855;&#22791;&#29702;&#35770;&#20445;&#35777;&#21644;&#38381;&#24335;&#35299;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.07999</link><description>&lt;p&gt;
NetInfoF &#26694;&#26550;&#65306;&#24230;&#37327;&#21644;&#21033;&#29992;&#32593;&#32476;&#21487;&#21033;&#29992;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
NetInfoF Framework: Measuring and Exploiting Network Usable Information
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07999
&lt;/p&gt;
&lt;p&gt;
NetInfoF&#26694;&#26550;&#25552;&#20379;&#20102;&#19968;&#31181;&#24555;&#36895;&#24230;&#37327;&#21644;&#21033;&#29992;&#33410;&#28857;&#23646;&#24615;&#22270;&#20013;&#30340;&#21487;&#21033;&#29992;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#33021;&#21516;&#26102;&#22788;&#29702;&#38142;&#36335;&#39044;&#27979;&#21644;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#20855;&#22791;&#29702;&#35770;&#20445;&#35777;&#21644;&#38381;&#24335;&#35299;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#19968;&#20010;&#33410;&#28857;&#23646;&#24615;&#22270;&#21644;&#19968;&#20010;&#22270;&#20219;&#21153;&#65288;&#38142;&#36335;&#39044;&#27979;&#25110;&#33410;&#28857;&#20998;&#31867;&#65289;&#65292;&#25105;&#20204;&#33021;&#21542;&#21028;&#26029;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26159;&#21542;&#33021;&#24456;&#22909;&#22320;&#23436;&#25104;&#20219;&#21153;&#65311;&#20855;&#20307;&#32780;&#35328;&#65292;&#22270;&#32467;&#26500;&#21644;&#33410;&#28857;&#29305;&#24449;&#26159;&#21542;&#21253;&#21547;&#36275;&#22815;&#21487;&#21033;&#29992;&#30340;&#20449;&#24687;&#26469;&#23436;&#25104;&#20219;&#21153;&#65311;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#65288;1&#65289;&#24320;&#21457;&#19968;&#20010;&#24555;&#36895;&#24037;&#20855;&#26469;&#24230;&#37327;&#22270;&#32467;&#26500;&#21644;&#33410;&#28857;&#29305;&#24449;&#20013;&#21253;&#21547;&#30340;&#20449;&#24687;&#37327;&#65292;&#20197;&#21450;&#65288;2&#65289;&#26681;&#25454;&#20449;&#24687;&#37327;&#26469;&#35299;&#20915;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;NetInfoF&#26694;&#26550;&#65292;&#21253;&#25324;NetInfoF_Probe&#21644;NetInfoF_Act&#20004;&#20010;&#37096;&#20998;&#65292;&#20998;&#21035;&#29992;&#20110;&#24230;&#37327;&#21644;&#21033;&#29992;&#32593;&#32476;&#21487;&#21033;&#29992;&#20449;&#24687;&#65288;NUI&#65289;&#12290;&#32473;&#23450;&#19968;&#20010;&#22270;&#25968;&#25454;&#65292;NetInfoF_Probe&#22312;&#19981;&#36827;&#34892;&#20219;&#20309;&#27169;&#22411;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#24230;&#37327;NUI&#65292;&#32780;NetInfoF_Act&#21017;&#29992;&#20110;&#35299;&#20915;&#38142;&#36335;&#39044;&#27979;&#21644;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#65292;&#20004;&#20010;&#27169;&#22359;&#20849;&#20139;&#30456;&#21516;&#30340;&#39592;&#24178;&#32593;&#32476;&#12290;&#24635;&#20043;&#65292;NetInfoF&#20855;&#26377;&#20197;&#19979;&#26174;&#33879;&#20248;&#28857;&#65306;&#65288;a&#65289;&#36890;&#29992;&#24615;&#65292;&#33021;&#22788;&#29702;&#38142;&#36335;&#39044;&#27979;&#21644;&#33410;&#28857;&#20998;&#31867;&#20004;&#31181;&#20219;&#21153;&#65307;&#65288;b&#65289;&#21407;&#21017;&#24615;&#65292;&#20855;&#22791;&#29702;&#35770;&#20445;&#35777;&#21644;&#38381;&#24335;&#35299;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given a node-attributed graph, and a graph task (link prediction or node classification), can we tell if a graph neural network (GNN) will perform well? More specifically, do the graph structure and the node features carry enough usable information for the task? Our goals are (1) to develop a fast tool to measure how much information is in the graph structure and in the node features, and (2) to exploit the information to solve the task, if there is enough. We propose NetInfoF, a framework including NetInfoF_Probe and NetInfoF_Act, for the measurement and the exploitation of network usable information (NUI), respectively. Given a graph data, NetInfoF_Probe measures NUI without any model training, and NetInfoF_Act solves link prediction and node classification, while two modules share the same backbone. In summary, NetInfoF has following notable advantages: (a) General, handling both link prediction and node classification; (b) Principled, with theoretical guarantee and closed-form solu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#20405;&#20837;&#24615;&#31070;&#32463;&#36136;&#37327;&#35780;&#20272;&#27169;&#22411;QASE-net&#65292;&#21487;&#20197;&#26377;&#25928;&#39044;&#27979;&#34920;&#38754;&#32908;&#30005;&#20449;&#21495;&#30340;&#20449;&#22122;&#27604;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#30456;&#27604;&#20043;&#21069;&#30340;&#27169;&#22411;&#20855;&#26377;&#26356;&#20302;&#30340;&#39044;&#27979;&#35823;&#24046;&#21644;&#26356;&#39640;&#30340;&#32447;&#24615;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.05482</link><description>&lt;p&gt;
&#19968;&#31181;&#38750;&#20405;&#20837;&#24615;&#31070;&#32463;&#36136;&#37327;&#35780;&#20272;&#27169;&#22411;&#24212;&#29992;&#20110;&#34920;&#38754;&#32908;&#30005;&#20449;&#21495;
&lt;/p&gt;
&lt;p&gt;
A Non-Intrusive Neural Quality Assessment Model for Surface Electromyography Signals
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05482
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#20405;&#20837;&#24615;&#31070;&#32463;&#36136;&#37327;&#35780;&#20272;&#27169;&#22411;QASE-net&#65292;&#21487;&#20197;&#26377;&#25928;&#39044;&#27979;&#34920;&#38754;&#32908;&#30005;&#20449;&#21495;&#30340;&#20449;&#22122;&#27604;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#30456;&#27604;&#20043;&#21069;&#30340;&#27169;&#22411;&#20855;&#26377;&#26356;&#20302;&#30340;&#39044;&#27979;&#35823;&#24046;&#21644;&#26356;&#39640;&#30340;&#32447;&#24615;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28041;&#21450;&#27979;&#37327;&#32908;&#32905;&#34920;&#38754;&#32908;&#30005;&#65288;sEMG&#65289;&#30340;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#23588;&#20854;&#26159;&#38752;&#36817;&#24515;&#33039;&#30340;&#21306;&#22495;&#65292;&#20027;&#35201;&#30340;&#27745;&#26579;&#28304;&#20043;&#19968;&#26159;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#20449;&#21495;&#30340;&#23384;&#22312;&#12290;&#20026;&#20102;&#26356;&#26377;&#25928;&#22320;&#35780;&#20272;&#23454;&#38469;&#19990;&#30028;&#20013;&#30340;sEMG&#25968;&#25454;&#36136;&#37327;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;QASE-net&#65292;&#19968;&#31181;&#26032;&#30340;&#38750;&#20405;&#20837;&#24615;&#27169;&#22411;&#65292;&#21487;&#20197;&#39044;&#27979;sEMG&#20449;&#21495;&#30340;&#20449;&#22122;&#27604;&#12290;QASE-net&#23558;CNN-BLSTM&#19982;&#27880;&#24847;&#21147;&#26426;&#21046;&#30456;&#32467;&#21512;&#65292;&#24182;&#37319;&#29992;&#31471;&#21040;&#31471;&#30340;&#35757;&#32451;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#26694;&#26550;&#21033;&#29992;&#20102;&#20004;&#20010;&#24320;&#25918;&#35775;&#38382;&#25968;&#25454;&#24211;&#30340;&#23454;&#38469;&#19990;&#30028;sEMG&#21644;ECG&#25968;&#25454;&#65292;&#20998;&#21035;&#26159;&#38750;&#20405;&#20837;&#24615;&#36866;&#24212;&#24615;&#20551;&#32930;&#25968;&#25454;&#24211;&#21644;MIT-BIH&#27491;&#24120;&#31398;&#24615;&#24515;&#24459;&#25968;&#25454;&#24211;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;QASE-net&#20248;&#20110;&#20808;&#21069;&#30340;&#35780;&#20272;&#27169;&#22411;&#65292;&#20855;&#26377;&#26174;&#33879;&#38477;&#20302;&#30340;&#39044;&#27979;&#35823;&#24046;&#21644;&#26126;&#26174;&#26356;&#39640;&#30340;&#32447;&#24615;&#30456;&#20851;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#26174;&#31034;&#20102;QASE-net&#22312;&#25552;&#39640;&#21487;&#38752;&#24615;&#21644;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
In practical scenarios involving the measurement of surface electromyography (sEMG) in muscles, particularly those areas near the heart, one of the primary sources of contamination is the presence of electrocardiogram (ECG) signals. To assess the quality of real-world sEMG data more effectively, this study proposes QASE-net, a new non-intrusive model that predicts the SNR of sEMG signals. QASE-net combines CNN-BLSTM with attention mechanisms and follows an end-to-end training strategy. Our experimental framework utilizes real-world sEMG and ECG data from two open-access databases, the Non-Invasive Adaptive Prosthetics Database and the MIT-BIH Normal Sinus Rhythm Database, respectively. The experimental results demonstrate the superiority of QASE-net over the previous assessment model, exhibiting significantly reduced prediction errors and notably higher linear correlations with the ground truth. These findings show the potential of QASE-net to substantially enhance the reliability and 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#20998;&#27835;&#31243;&#24207;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#28041;&#21450;&#37325;&#22797;&#23376;&#20219;&#21153;&#21644;/&#25110;&#20855;&#26377;&#27450;&#39575;&#24615;&#20869;&#23481;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;LLM&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.05359</link><description>&lt;p&gt;
&#21033;&#29992;&#20998;&#27835;&#31243;&#24207;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#38382;&#39064;&#27714;&#35299;&#36827;&#34892;&#24341;&#23548;
&lt;/p&gt;
&lt;p&gt;
Guiding Large Language Models with Divide-and-Conquer Program for Discerning Problem Solving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05359
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#20998;&#27835;&#31243;&#24207;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#28041;&#21450;&#37325;&#22797;&#23376;&#20219;&#21153;&#21644;/&#25110;&#20855;&#26377;&#27450;&#39575;&#24615;&#20869;&#23481;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;LLM&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#65292;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22240;&#20854;&#24191;&#27867;&#30340;&#24212;&#29992;&#32780;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36866;&#24403;&#30340;&#25552;&#31034;&#35774;&#35745;&#65292;&#22914;&#24605;&#32500;&#38142;&#65292;&#21487;&#20197;&#37322;&#25918;LLM&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#24378;&#22823;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22788;&#29702;&#28041;&#21450;&#37325;&#22797;&#23376;&#20219;&#21153;&#21644;/&#25110;&#20855;&#26377;&#27450;&#39575;&#24615;&#20869;&#23481;&#30340;&#20219;&#21153;&#65288;&#22914;&#31639;&#26415;&#35745;&#31639;&#21644;&#25991;&#31456;&#32423;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;&#65289;&#65292;&#29616;&#26377;&#30340;&#25552;&#31034;&#31574;&#30053;&#35201;&#20040;&#34920;&#29616;&#20986;&#34920;&#36798;&#33021;&#21147;&#19981;&#36275;&#65292;&#35201;&#20040;&#30001;&#24187;&#35273;&#24341;&#21457;&#20013;&#38388;&#38169;&#35823;&#12290;&#20026;&#20102;&#20351;LLM&#23545;&#36825;&#20123;&#20013;&#38388;&#38169;&#35823;&#26356;&#20855;&#36776;&#21035;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#20998;&#27835;&#31243;&#24207;&#24341;&#23548;LLM&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#30830;&#20445;&#20248;&#36234;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#20219;&#21153;&#20998;&#35299;&#12289;&#23376;&#20219;&#21153;&#35299;&#20915;&#21644;&#35299;&#20915;&#32452;&#35013;&#36807;&#31243;&#30340;&#20998;&#31163;&#12290;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31574;&#30053;&#21487;&#20197;&#24341;&#23548;LLM&#25193;&#23637;&#22266;&#23450;&#28145;&#24230;Transformer&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
Foundation models, such as Large language Models (LLMs), have attracted significant amount of interest due to their large number of applications. Existing works show that appropriate prompt design, such as Chain-of-Thoughts, can unlock LLM's powerful capacity in diverse areas. However, when handling tasks involving repetitive sub-tasks and/or deceptive contents, such as arithmetic calculation and article-level fake news detection, existing prompting strategies either suffers from insufficient expressive power or intermediate errors triggered by hallucination. To make LLM more discerning to such intermediate errors, we propose to guide LLM with a Divide-and-Conquer program that simultaneously ensures superior expressive power and disentangles task decomposition, sub-task resolution, and resolution assembly process. Theoretic analysis reveals that our strategy can guide LLM to extend the expressive power of fixed-depth Transformer. Experiments indicate that our proposed method can achiev
&lt;/p&gt;</description></item><item><title>IGUANe&#26159;&#19968;&#31181;&#19977;&#32500;&#36890;&#29992;CycleGAN&#27169;&#22411;&#65292;&#36890;&#36807;&#38598;&#25104;&#22810;&#20010;&#22495;&#30340;&#35757;&#32451;&#23454;&#29616;&#20102;&#33041;MR&#22270;&#20687;&#30340;&#22810;&#20013;&#24515;&#21327;&#35843;&#65292;&#20351;&#20854;&#25104;&#20026;&#36890;&#29992;&#29983;&#25104;&#22120;&#12290;</title><link>https://arxiv.org/abs/2402.03227</link><description>&lt;p&gt;
IGUANe: &#19968;&#31181;&#36866;&#29992;&#20110;&#33041;MR&#22270;&#20687;&#22810;&#20013;&#24515;&#21327;&#35843;&#30340;&#19977;&#32500;&#36890;&#29992;CycleGAN&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
IGUANe: a 3D generalizable CycleGAN for multicenter harmonization of brain MR images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03227
&lt;/p&gt;
&lt;p&gt;
IGUANe&#26159;&#19968;&#31181;&#19977;&#32500;&#36890;&#29992;CycleGAN&#27169;&#22411;&#65292;&#36890;&#36807;&#38598;&#25104;&#22810;&#20010;&#22495;&#30340;&#35757;&#32451;&#23454;&#29616;&#20102;&#33041;MR&#22270;&#20687;&#30340;&#22810;&#20013;&#24515;&#21327;&#35843;&#65292;&#20351;&#20854;&#25104;&#20026;&#36890;&#29992;&#29983;&#25104;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;MRI&#30740;&#31350;&#20013;&#65292;&#26469;&#33258;&#22810;&#20010;&#37319;&#38598;&#28857;&#30340;&#22270;&#20687;&#25968;&#25454;&#30340;&#32858;&#21512;&#21487;&#20197;&#22686;&#21152;&#26679;&#26412;&#22823;&#23567;&#65292;&#20294;&#21487;&#33021;&#24341;&#20837;&#38459;&#30861;&#21518;&#32493;&#20998;&#26512;&#19968;&#33268;&#24615;&#30340;&#19982;&#37319;&#38598;&#28857;&#30456;&#20851;&#30340;&#21464;&#24322;&#12290;&#22270;&#20687;&#32763;&#35793;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#24050;&#32463;&#25104;&#20026;&#21327;&#35843;MR&#22270;&#20687;&#36328;&#31449;&#28857;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;IGUANe&#65288;&#20855;&#26377;&#32479;&#19968;&#23545;&#25239;&#32593;&#32476;&#30340;&#22270;&#20687;&#29983;&#25104;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21407;&#22987;&#30340;&#19977;&#32500;&#27169;&#22411;&#65292;&#23427;&#32467;&#21512;&#20102;&#22495;&#36716;&#25442;&#30340;&#20248;&#21183;&#21644;&#30452;&#25509;&#24212;&#29992;&#26679;&#24335;&#36716;&#31227;&#26041;&#27861;&#26469;&#23454;&#29616;&#22810;&#20013;&#24515;&#33041;MR&#22270;&#20687;&#21327;&#35843;&#12290;IGUANe&#36890;&#36807;&#22810;&#23545;&#19968;&#31574;&#30053;&#65292;&#38598;&#25104;&#20102;&#20219;&#24847;&#25968;&#37327;&#30340;&#22495;&#36827;&#34892;&#35757;&#32451;&#65292;&#25193;&#23637;&#20102;CycleGAN&#26550;&#26500;&#12290;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#22270;&#20687;&#65292;&#29978;&#33267;&#26469;&#33258;&#26410;&#30693;&#37319;&#38598;&#28857;&#65292;&#20351;&#20854;&#25104;&#20026;&#21327;&#35843;&#30340;&#36890;&#29992;&#29983;&#25104;&#22120;&#12290;&#22312;&#30001;11&#21488;&#19981;&#21516;&#25195;&#25551;&#20202;&#30340;T1&#21152;&#26435;&#22270;&#20687;&#32452;&#25104;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;IGUANe&#22312;&#26410;&#35265;&#31449;&#28857;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
In MRI studies, the aggregation of imaging data from multiple acquisition sites enhances sample size but may introduce site-related variabilities that hinder consistency in subsequent analyses. Deep learning methods for image translation have emerged as a solution for harmonizing MR images across sites. In this study, we introduce IGUANe (Image Generation with Unified Adversarial Networks), an original 3D model that leverages the strengths of domain translation and straightforward application of style transfer methods for multicenter brain MR image harmonization. IGUANe extends CycleGAN architecture by integrating an arbitrary number of domains for training through a many-to-one strategy. During inference, the model can be applied to any image, even from an unknown acquisition site, making it a universal generator for harmonization. Trained on a dataset comprising T1-weighted images from 11 different scanners, IGUANe was evaluated on data from unseen sites. The assessments included the
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#19968;&#32500;&#32467;&#26500;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25830;&#38500;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#27010;&#24565;&#28040;&#38500;&#26041;&#27861;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#23454;&#29616;&#38750;&#20405;&#20837;&#24615;&#12289;&#31934;&#30830;&#24615;&#12289;&#21487;&#23450;&#21046;&#24615;&#21644;&#21487;&#36716;&#31227;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.16145</link><description>&lt;p&gt;
&#19968;&#32500;&#36866;&#37197;&#22120;&#26469;&#32479;&#27835;&#23427;&#20204;&#25152;&#26377;&#65306;&#27010;&#24565;&#12289;&#25193;&#25955;&#27169;&#22411;&#21644;&#28040;&#38500;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
One-Dimensional Adapter to Rule Them All: Concepts, Diffusion Models and Erasing Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.16145
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#19968;&#32500;&#32467;&#26500;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25830;&#38500;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#27010;&#24565;&#28040;&#38500;&#26041;&#27861;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#23454;&#29616;&#38750;&#20405;&#20837;&#24615;&#12289;&#31934;&#30830;&#24615;&#12289;&#21487;&#23450;&#21046;&#24615;&#21644;&#21487;&#36716;&#31227;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#21830;&#19994;&#21644;&#24320;&#28304;&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#20026;&#38450;&#27490;&#19981;&#33391;&#34892;&#20026;&#65292;&#29616;&#26377;&#30340;&#27010;&#24565;&#25830;&#38500;&#26041;&#27861;&#22522;&#20110;&#23436;&#20840;&#21442;&#25968;&#25110;&#22522;&#20110;&#35268;&#33539;&#30340;&#31934;&#32454;&#35843;&#25972;&#65292;&#35266;&#23519;&#21040;&#20197;&#19979;&#38382;&#39064;&#65306;1&#65289;&#19981;&#26029;&#20405;&#34432;&#26397;&#21521;&#30340;&#29983;&#25104;&#65306;&#30446;&#26631;&#28040;&#38500;&#36807;&#31243;&#20013;&#30340;&#21442;&#25968;&#28418;&#31227;&#23548;&#33268;&#25152;&#26377;&#29983;&#25104;&#30340;&#21464;&#21270;&#21644;&#28508;&#22312;&#21464;&#24418;&#65292;&#29978;&#33267;&#22312;&#22810;&#27010;&#24565;&#28040;&#38500;&#26102;&#20405;&#34432;&#20854;&#20182;&#27010;&#24565;&#65292;&#22312;&#19981;&#21516;&#31243;&#24230;&#19978;&#26356;&#21152;&#26126;&#26174;&#65307;2&#65289;&#36716;&#31227;&#33021;&#21147;&#21644;&#37096;&#32626;&#25928;&#29575;&#65306;&#20043;&#21069;&#22522;&#20110;&#27169;&#22411;&#30340;&#25830;&#38500;&#38459;&#30861;&#20102;&#27010;&#24565;&#30340;&#28789;&#27963;&#32452;&#21512;&#21644;&#35757;&#32451;&#20813;&#36153;&#36716;&#31227;&#21040;&#20854;&#20182;&#27169;&#22411;&#65292;&#23548;&#33268;&#37096;&#32626;&#22330;&#26223;&#22686;&#21152;&#26102;&#25104;&#26412;&#32447;&#24615;&#22686;&#38271;&#12290;&#20026;&#20102;&#23454;&#29616;&#26080;&#20405;&#20837;&#12289;&#31934;&#30830;&#12289;&#21487;&#23450;&#21046;&#21644;&#21487;&#36716;&#31227;&#30340;&#28040;&#38500;&#65292;&#25105;&#20204;&#23558;&#25830;&#38500;&#26694;&#26550;&#24314;&#31435;&#22312;&#19968;&#32500;&#22522;&#30784;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.16145v2 Announce Type: replace-cross  Abstract: The prevalent use of commercial and open-source diffusion models (DMs) for text-to-image generation prompts risk mitigation to prevent undesired behaviors. Existing concept erasing methods in academia are all based on full parameter or specification-based fine-tuning, from which we observe the following issues: 1) Generation alternation towards erosion: Parameter drift during target elimination causes alternations and potential deformations across all generations, even eroding other concepts at varying degrees, which is more evident with multi-concept erased; 2) Transfer inability &amp; deployment inefficiency: Previous model-specific erasure impedes the flexible combination of concepts and the training-free transfer towards other models, resulting in linear cost growth as the deployment scenarios increase. To achieve non-invasive, precise, customizable, and transferable elimination, we ground our erasing framework on one-dimension
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;ACPO&#26694;&#26550;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#32473;LLVM&#31616;&#21333;&#20840;&#38754;&#30340;&#24037;&#20855;&#65292;&#20197;&#23454;&#29616;&#32534;&#35793;&#22120;&#39537;&#21160;&#30340;&#31243;&#24207;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2312.09982</link><description>&lt;p&gt;
ACPO: AI-Enabled Compiler-Driven Program Optimization
&lt;/p&gt;
&lt;p&gt;
ACPO: AI-Enabled Compiler-Driven Program Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.09982
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;ACPO&#26694;&#26550;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#32473;LLVM&#31616;&#21333;&#20840;&#38754;&#30340;&#24037;&#20855;&#65292;&#20197;&#23454;&#29616;&#32534;&#35793;&#22120;&#39537;&#21160;&#30340;&#31243;&#24207;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;ACPO&#65306;AI-Enabled Compiler-driven Program Optimization&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#20026;LLVM&#25552;&#20379;&#31616;&#21333;&#20840;&#38754;&#30340;&#24037;&#20855;&#65292;&#20197;&#20174;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#36827;&#34892;&#19981;&#21516;&#30340;&#20248;&#21270;&#36890;&#36335;&#20013;&#33719;&#30410;&#12290;&#39318;&#20808;&#23637;&#31034;&#20102;ACPO&#30340;&#39640;&#23618;&#35270;&#22270;&#12289;&#31867;&#23618;&#27425;&#32467;&#26500;&#21644;&#21151;&#33021;&#65292;&#28982;&#21518;&#36890;&#36807;&#23558;&#24490;&#29615;&#23637;&#24320;&#21644;&#20989;&#25968;&#20869;&#32852;&#20256;&#36882;&#30340;ML&#20351;&#33021;&#21270;&#65292;&#23637;&#31034;&#20102;ACPO&#30340;&#19968;&#20123;&#29992;&#20363;&#65292;&#25551;&#36848;&#20102;ACPO&#22914;&#20309;&#21457;&#25381;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.09982v2 Announce Type: replace-cross  Abstract: The key to performance optimization of a program is to decide correctly when a certain transformation should be applied by a compiler. This is an ideal opportunity to apply machine-learning models to speed up the tuning process; while this realization has been around since the late 90s, only recent advancements in ML enabled a practical application of ML to compilers as an end-to-end framework.   This paper presents ACPO: \textbf{\underline{A}}I-Enabled \textbf{\underline{C}}ompiler-driven \textbf{\underline{P}}rogram \textbf{\underline{O}}ptimization; a novel framework to provide LLVM with simple and comprehensive tools to benefit from employing ML models for different optimization passes. We first showcase the high-level view, class hierarchy, and functionalities of ACPO and subsequently, demonstrate a couple of use cases of ACPO by ML-enabling the Loop Unroll and Function Inlining passes and describe how ACPO can be leverage
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#38543;&#26426;&#26799;&#24230;&#26041;&#27861;&#30340;&#26368;&#32456;&#36845;&#20195;&#25910;&#25947;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19981;&#38656;&#35201;&#38480;&#21046;&#24615;&#20551;&#35774;&#30340;&#26368;&#20248;&#25910;&#25947;&#36895;&#29575;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2312.08531</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#38543;&#26426;&#26799;&#24230;&#26041;&#27861;&#30340;&#26368;&#32456;&#36845;&#20195;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
Revisiting the Last-Iterate Convergence of Stochastic Gradient Methods
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.08531
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#38543;&#26426;&#26799;&#24230;&#26041;&#27861;&#30340;&#26368;&#32456;&#36845;&#20195;&#25910;&#25947;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19981;&#38656;&#35201;&#38480;&#21046;&#24615;&#20551;&#35774;&#30340;&#26368;&#20248;&#25910;&#25947;&#36895;&#29575;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#20960;&#24180;&#37324;&#65292;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#31639;&#27861;&#30340;&#26368;&#32456;&#36845;&#20195;&#25910;&#25947;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20852;&#36259;&#65292;&#22240;&#20026;&#23427;&#22312;&#23454;&#36341;&#20013;&#34920;&#29616;&#33391;&#22909;&#20294;&#32570;&#20047;&#29702;&#35770;&#29702;&#35299;&#12290;&#23545;&#20110;Lipschitz&#20984;&#20989;&#25968;&#65292;&#19981;&#21516;&#30340;&#30740;&#31350;&#24314;&#31435;&#20102;&#26368;&#20339;&#30340;$O(\log(1/\delta)\log T/\sqrt{T})$&#25110;$O(\sqrt{\log(1/\delta)/T})$&#26368;&#32456;&#36845;&#20195;&#30340;&#39640;&#27010;&#29575;&#25910;&#25947;&#36895;&#29575;&#65292;&#20854;&#20013;$T$&#26159;&#26102;&#38388;&#36328;&#24230;&#65292;$\delta$&#26159;&#22833;&#36133;&#27010;&#29575;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#35777;&#26126;&#36825;&#20123;&#30028;&#38480;&#65292;&#25152;&#26377;&#29616;&#26377;&#30340;&#24037;&#20316;&#35201;&#20040;&#23616;&#38480;&#20110;&#32039;&#33268;&#22495;&#65292;&#35201;&#20040;&#38656;&#35201;&#20960;&#20046;&#32943;&#23450;&#26377;&#30028;&#30340;&#22122;&#22768;&#12290;&#24456;&#33258;&#28982;&#22320;&#20250;&#38382;&#65292;&#19981;&#38656;&#35201;&#36825;&#20004;&#20010;&#38480;&#21046;&#24615;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;SGD&#30340;&#26368;&#32456;&#36845;&#20195;&#26159;&#21542;&#20173;&#28982;&#21487;&#20197;&#20445;&#35777;&#26368;&#20339;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;&#38500;&#20102;&#36825;&#20010;&#37325;&#35201;&#38382;&#39064;&#22806;&#65292;&#36824;&#26377;&#24456;&#22810;&#29702;&#35770;&#38382;&#39064;&#20173;&#28982;&#27809;&#26377;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.08531v2 Announce Type: replace  Abstract: In the past several years, the last-iterate convergence of the Stochastic Gradient Descent (SGD) algorithm has triggered people's interest due to its good performance in practice but lack of theoretical understanding. For Lipschitz convex functions, different works have established the optimal $O(\log(1/\delta)\log T/\sqrt{T})$ or $O(\sqrt{\log(1/\delta)/T})$ high-probability convergence rates for the final iterate, where $T$ is the time horizon and $\delta$ is the failure probability. However, to prove these bounds, all the existing works are either limited to compact domains or require almost surely bounded noises. It is natural to ask whether the last iterate of SGD can still guarantee the optimal convergence rate but without these two restrictive assumptions. Besides this important question, there are still lots of theoretical problems lacking an answer. For example, compared with the last-iterate convergence of SGD for non-smoot
&lt;/p&gt;</description></item><item><title>SparQ Attention&#36890;&#36807;&#20943;&#23569;&#27880;&#24847;&#21147;&#22359;&#20869;&#23384;&#24102;&#23485;&#38656;&#27714;&#30340;&#25216;&#26415;&#65292;&#20174;&#32780;&#22686;&#21152;LLMs&#25512;&#29702;&#30340;&#21534;&#21520;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.04985</link><description>&lt;p&gt;
SparQ&#27880;&#24847;&#21147;&#65306;&#39640;&#25928;&#24102;&#23485;&#30340;LLM&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
SparQ Attention: Bandwidth-Efficient LLM Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.04985
&lt;/p&gt;
&lt;p&gt;
SparQ Attention&#36890;&#36807;&#20943;&#23569;&#27880;&#24847;&#21147;&#22359;&#20869;&#23384;&#24102;&#23485;&#38656;&#27714;&#30340;&#25216;&#26415;&#65292;&#20174;&#32780;&#22686;&#21152;LLMs&#25512;&#29702;&#30340;&#21534;&#21520;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24320;&#21019;&#20102;&#35768;&#22810;&#26032;&#21487;&#33021;&#24615;&#65292;&#20294;&#30001;&#20110;&#20854;&#24040;&#22823;&#30340;&#35745;&#31639;&#38656;&#27714;&#65292;&#23427;&#20204;&#30340;&#26222;&#36941;&#20351;&#29992;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;SparQ&#27880;&#24847;&#21147;&#65292;&#19968;&#31181;&#36890;&#36807;&#36873;&#25321;&#24615;&#33719;&#21462;&#32531;&#23384;&#21382;&#21490;&#26469;&#20943;&#23569;&#27880;&#24847;&#21147;&#22359;&#20869;&#23384;&#24102;&#23485;&#38656;&#27714;&#30340;&#25216;&#26415;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;LLMs&#30340;&#25512;&#29702;&#21534;&#21520;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.04985v2 Announce Type: replace  Abstract: Generative large language models (LLMs) have opened up numerous novel possibilities, but due to their significant computational requirements their ubiquitous use remains challenging. Some of the most useful applications require processing large numbers of samples at a time and using long contexts, both significantly increasing the memory communication load of the models. We introduce SparQ Attention, a technique for increasing the inference throughput of LLMs by reducing the memory bandwidth requirements within the attention blocks through selective fetching of the cached history. Our proposed technique can be applied directly to off-the-shelf LLMs during inference, without requiring any modification to the pre-training setup or additional fine-tuning. We show how SparQ Attention can decrease the attention memory bandwidth requirements up to eight times without any loss in accuracy by evaluating Llama 2 and Pythia models on a wide ra
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#37327;&#23376;&#21551;&#33945;&#20998;&#25968;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#37327;&#23376;&#29983;&#25104;&#27169;&#22411;&#36136;&#37327;&#30340;&#26032;&#25351;&#26631;&#65292;&#35777;&#26126;&#37327;&#23376;&#29983;&#25104;&#27169;&#22411;&#22312;&#36136;&#37327;&#19978;&#20248;&#20110;&#32463;&#20856;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#37327;&#23376;&#27874;&#21160;&#23450;&#29702;&#25581;&#31034;&#20102;&#20854;&#29289;&#29702;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2311.12163</link><description>&lt;p&gt;
&#37327;&#23376;&#21551;&#33945;&#20998;&#25968;
&lt;/p&gt;
&lt;p&gt;
Quantum Inception Score
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.12163
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#37327;&#23376;&#21551;&#33945;&#20998;&#25968;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#37327;&#23376;&#29983;&#25104;&#27169;&#22411;&#36136;&#37327;&#30340;&#26032;&#25351;&#26631;&#65292;&#35777;&#26126;&#37327;&#23376;&#29983;&#25104;&#27169;&#22411;&#22312;&#36136;&#37327;&#19978;&#20248;&#20110;&#32463;&#20856;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#37327;&#23376;&#27874;&#21160;&#23450;&#29702;&#25581;&#31034;&#20102;&#20854;&#29289;&#29702;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#32463;&#20856;&#29983;&#25104;&#27169;&#22411;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#21462;&#24471;&#24040;&#22823;&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#36817;&#26399;&#24320;&#22987;&#20102;&#23545;&#23427;&#20204;&#37327;&#23376;&#29256;&#26412;&#30340;&#28909;&#20999;&#25506;&#32034;&#12290;&#20026;&#20102;&#24320;&#22987;&#36825;&#19968;&#25506;&#32034;&#20043;&#26053;&#65292;&#24320;&#21457;&#19968;&#20010;&#30456;&#20851;&#30340;&#24230;&#37327;&#26631;&#20934;&#26469;&#35780;&#20272;&#37327;&#23376;&#29983;&#25104;&#27169;&#22411;&#30340;&#36136;&#37327;&#26159;&#24456;&#37325;&#35201;&#30340;&#65307;&#22312;&#32463;&#20856;&#24773;&#20917;&#19979;&#65292;&#19968;&#20010;&#36825;&#26679;&#30340;&#20363;&#23376;&#20415;&#26159;&#21551;&#33945;&#20998;&#25968;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#37327;&#23376;&#21551;&#33945;&#20998;&#25968;&#65292;&#23427;&#23558;&#36136;&#37327;&#19982;&#29992;&#20110;&#23545;&#32473;&#23450;&#25968;&#25454;&#38598;&#36827;&#34892;&#20998;&#31867;&#30340;&#37327;&#23376;&#36890;&#36947;&#30340;Holevo&#20449;&#24687;&#32852;&#31995;&#36215;&#26469;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#36825;&#20010;&#25552;&#20986;&#30340;&#24230;&#37327;&#26631;&#20934;&#19979;&#65292;&#37327;&#23376;&#29983;&#25104;&#27169;&#22411;&#25552;&#20379;&#27604;&#23427;&#20204;&#30340;&#32463;&#20856;&#23545;&#24212;&#29289;&#26356;&#22909;&#30340;&#36136;&#37327;&#65292;&#22240;&#20026;&#23384;&#22312;&#30528;&#30001;&#19981;&#23545;&#31216;&#24615;&#30340;&#36164;&#28304;&#29702;&#35770;&#21644;&#32416;&#32544;&#25152;&#34920;&#24449;&#30340;&#37327;&#23376;&#30456;&#24178;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#37327;&#23376;&#27874;&#21160;&#23450;&#29702;&#26469;&#34920;&#24449;&#38480;&#21046;&#37327;&#23376;&#29983;&#25104;&#27169;&#22411;&#36136;&#37327;&#30340;&#29289;&#29702;&#38480;&#21046;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24212;&#29992;&#37327;&#23376;&#21551;&#33945;&#20998;&#25968;&#26469;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.12163v2 Announce Type: replace-cross  Abstract: Motivated by the great success of classical generative models in machine learning, enthusiastic exploration of their quantum version has recently started. To depart on this journey, it is important to develop a relevant metric to evaluate the quality of quantum generative models; in the classical case, one such example is the inception score. In this paper, we propose the quantum inception score, which relates the quality to the Holevo information of the quantum channel that classifies a given dataset. We prove that, under this proposed measure, the quantum generative models provide better quality than their classical counterparts because of the presence of quantum coherence, characterized by the resource theory of asymmetry, and entanglement. Furthermore, we harness the quantum fluctuation theorem to characterize the physical limitation of the quality of quantum generative models. Finally, we apply the quantum inception score 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedRA&#30340;&#32852;&#37030;&#35843;&#20248;&#31639;&#27861;&#65292;&#21487;&#20197;&#38543;&#26426;&#29983;&#25104;&#20998;&#37197;&#30697;&#38453;&#26469;&#24212;&#23545;&#25317;&#26377;&#19981;&#21516;&#35745;&#31639;&#21644;&#36890;&#20449;&#36164;&#28304;&#30340;&#24322;&#26500;&#23458;&#25143;&#31471;&#65292;&#22312;&#19981;&#38656;&#35201;&#20462;&#25913;&#21407;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#24494;&#35843;&#12290;</title><link>https://arxiv.org/abs/2311.11227</link><description>&lt;p&gt;
FedRA:&#19968;&#31181;&#29992;&#20110;&#37322;&#25918;&#24322;&#26500;&#23458;&#25143;&#31471;&#24378;&#22823;&#28508;&#21147;&#30340;&#38543;&#26426;&#20998;&#37197;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
FedRA: A Random Allocation Strategy for Federated Tuning to Unleash the Power of Heterogeneous Clients
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.11227
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedRA&#30340;&#32852;&#37030;&#35843;&#20248;&#31639;&#27861;&#65292;&#21487;&#20197;&#38543;&#26426;&#29983;&#25104;&#20998;&#37197;&#30697;&#38453;&#26469;&#24212;&#23545;&#25317;&#26377;&#19981;&#21516;&#35745;&#31639;&#21644;&#36890;&#20449;&#36164;&#28304;&#30340;&#24322;&#26500;&#23458;&#25143;&#31471;&#65292;&#22312;&#19981;&#38656;&#35201;&#20462;&#25913;&#21407;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22522;&#30784;&#27169;&#22411;&#30340;&#26085;&#30410;&#21487;&#29992;&#65292;&#32852;&#37030;&#35843;&#20248;&#22312;&#32852;&#37030;&#23398;&#20064;&#39046;&#22495;&#24341;&#36215;&#20102;&#20851;&#27880;&#65292;&#21033;&#29992;&#22810;&#20010;&#23458;&#25143;&#31471;&#30340;&#25968;&#25454;&#21644;&#35745;&#31639;&#36164;&#28304;&#20849;&#21516;&#23545;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#32852;&#37030;&#22330;&#26223;&#20013;&#65292;&#36890;&#24120;&#23384;&#22312;&#22823;&#37327;&#20855;&#26377;&#19981;&#21516;&#35745;&#31639;&#21644;&#36890;&#20449;&#36164;&#28304;&#30340;&#24322;&#26500;&#23458;&#25143;&#31471;&#65292;&#23548;&#33268;&#23427;&#20204;&#26080;&#27861;&#25903;&#25345;&#25972;&#20010;&#27169;&#22411;&#30340;&#24494;&#35843;&#36807;&#31243;&#12290;&#38024;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32852;&#37030;&#35843;&#20248;&#31639;&#27861;FedRA&#12290;FedRA&#30340;&#23454;&#26045;&#31616;&#21333;&#65292;&#21487;&#20197;&#26080;&#32541;&#38598;&#25104;&#21040;&#20219;&#20309;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#20013;&#65292;&#26080;&#38656;&#23545;&#21407;&#27169;&#22411;&#36827;&#34892;&#36827;&#19968;&#27493;&#20462;&#25913;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#27599;&#19968;&#36718;&#36890;&#20449;&#20013;&#65292;FedRA&#20250;&#38543;&#26426;&#29983;&#25104;&#19968;&#20010;&#20998;&#37197;&#30697;&#38453;&#12290;&#23545;&#20110;&#36164;&#28304;&#21463;&#38480;&#30340;&#23458;&#25143;&#31471;&#65292;&#23427;&#20250;&#26681;&#25454;&#20998;&#37197;&#24773;&#20917;&#37325;&#26032;&#32452;&#32455;&#21407;&#27169;&#22411;&#20013;&#30340;&#23569;&#37327;&#23618;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.11227v2 Announce Type: replace-cross  Abstract: With the increasing availability of Foundation Models, federated tuning has garnered attention in the field of federated learning, utilizing data and computation resources from multiple clients to collaboratively fine-tune foundation models. However, in real-world federated scenarios, there often exist a multitude of heterogeneous clients with varying computation and communication resources, rendering them incapable of supporting the entire model fine-tuning process. In response to this challenge, we propose a novel federated tuning algorithm, FedRA. The implementation of FedRA is straightforward and can be seamlessly integrated into any transformer-based model without the need for further modification to the original model. Specifically, in each communication round, FedRA randomly generates an allocation matrix. For resource-constrained clients, it reorganizes a small number of layers from the original model based on the alloc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#20687;&#22788;&#29702;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#19977;&#32500;&#22521;&#20859;&#20013;&#37327;&#21270;&#32454;&#32990;&#30340;&#27963;&#21147;&#65292;&#26080;&#38656;&#22522;&#20110;&#35797;&#21058;&#30340;&#25351;&#31034;&#29289;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#20854;&#19982;&#20154;&#31867;&#19987;&#23478;&#30340;&#34920;&#29616;&#31867;&#20284;&#12290;</title><link>https://arxiv.org/abs/2311.09354</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#22270;&#20687;&#20998;&#21106;&#36827;&#34892;&#19977;&#32500;&#32452;&#32455;&#22521;&#20859;&#30340;&#38750;&#30772;&#22351;&#24615;&#23450;&#37327;&#27963;&#21147;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Nondestructive, quantitative viability analysis of 3D tissue cultures using machine learning image segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09354
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#20687;&#22788;&#29702;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#19977;&#32500;&#22521;&#20859;&#20013;&#37327;&#21270;&#32454;&#32990;&#30340;&#27963;&#21147;&#65292;&#26080;&#38656;&#22522;&#20110;&#35797;&#21058;&#30340;&#25351;&#31034;&#29289;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#20854;&#19982;&#20154;&#31867;&#19987;&#23478;&#30340;&#34920;&#29616;&#31867;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30830;&#23450;&#19981;&#21516;&#32454;&#32990;&#22521;&#20859;&#26465;&#20214;&#19979;&#32454;&#32990;&#30340;&#38598;&#20307;&#27963;&#21147;&#36890;&#24120;&#20381;&#36182;&#20110;&#24179;&#22343;&#33394;&#24230;&#25351;&#26631;&#65292;&#24182;&#19988;&#36890;&#24120;&#20197;&#31616;&#21333;&#30340;&#20108;&#36827;&#21046;&#35835;&#25968;&#25253;&#21578;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#23558;&#27963;&#21147;&#35780;&#20272;&#25216;&#26415;&#19982;&#22522;&#20110;&#22270;&#20687;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#20197;&#33258;&#21160;&#21270;&#29305;&#24449;&#21270;&#32454;&#32990;&#23646;&#24615;&#12290;&#28982;&#32780;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#21457;&#23637;&#27963;&#21147;&#27979;&#37327;&#25216;&#26415;&#65292;&#20197;&#35780;&#20272;&#21487;&#33021;&#30340;&#32454;&#32990;&#29366;&#24577;&#36830;&#32493;&#24615;&#21644;&#23545;&#22521;&#20859;&#26465;&#20214;&#19979;&#30340;&#24178;&#25200;&#21709;&#24212;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#22270;&#20687;&#22788;&#29702;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#19977;&#32500;&#22521;&#20859;&#20013;&#37327;&#21270;&#32454;&#32990;&#30340;&#27963;&#21147;&#65292;&#26080;&#38656;&#22522;&#20110;&#35797;&#21058;&#30340;&#25351;&#31034;&#29289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#19968;&#31995;&#21015;&#22825;&#25968;&#21644;&#22521;&#20859;&#22522;&#32452;&#25104;&#19979;&#22312;&#20840;&#23380;&#22270;&#20687;&#20013;&#30340;&#34920;&#29616;&#19982;&#19968;&#23545;&#20154;&#31867;&#19987;&#23478;&#31867;&#20284;&#12290;&#20026;&#20102;&#23637;&#31034;&#28508;&#22312;&#30340;&#23454;&#29992;&#24615;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#32437;&#21521;&#30740;&#31350;&#65292;&#35843;&#26597;&#24050;&#30693;&#30103;&#27861;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09354v2 Announce Type: replace-cross  Abstract: Ascertaining the collective viability of cells in different cell culture conditions has typically relied on averaging colorimetric indicators and is often reported out in simple binary readouts. Recent research has combined viability assessment techniques with image-based deep-learning models to automate the characterization of cellular properties. However, further development of viability measurements to assess the continuity of possible cellular states and responses to perturbation across cell culture conditions is needed. In this work, we demonstrate an image processing algorithm for quantifying cellular viability in 3D cultures without the need for assay-based indicators. We show that our algorithm performs similarly to a pair of human experts in whole-well images over a range of days and culture matrix compositions. To demonstrate potential utility, we perform a longitudinal study investigating the impact of a known therap
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20379;&#20248;&#21270;&#36712;&#36857;&#20316;&#20026;&#39069;&#22806;&#35757;&#32451;&#25968;&#25454;&#65292;&#21487;&#20197;&#25913;&#21892;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#23376;&#26500;&#35937;&#33021;&#37327;&#26368;&#23567;&#21270;&#36136;&#37327;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#39069;&#22806;&#30340;&#26500;&#35937;&#25968;&#25454;&#25165;&#33021;&#36798;&#21040;&#29289;&#29702;&#27169;&#25311;&#22120;&#30340;&#20248;&#21270;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2311.06295</link><description>&lt;p&gt;
&#36880;&#27493;&#20248;&#21270;&#23398;&#20064;&#29992;&#20110;&#26500;&#35937;&#33021;&#37327;&#26368;&#23567;&#21270;
&lt;/p&gt;
&lt;p&gt;
Gradual Optimization Learning for Conformational Energy Minimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.06295
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20379;&#20248;&#21270;&#36712;&#36857;&#20316;&#20026;&#39069;&#22806;&#35757;&#32451;&#25968;&#25454;&#65292;&#21487;&#20197;&#25913;&#21892;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#23376;&#26500;&#35937;&#33021;&#37327;&#26368;&#23567;&#21270;&#36136;&#37327;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#39069;&#22806;&#30340;&#26500;&#35937;&#25968;&#25454;&#25165;&#33021;&#36798;&#21040;&#29289;&#29702;&#27169;&#25311;&#22120;&#30340;&#20248;&#21270;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#26500;&#35937;&#20248;&#21270;&#23545;&#20110;&#35745;&#31639;&#36741;&#21161;&#33647;&#29289;&#21457;&#29616;&#21644;&#26448;&#26009;&#35774;&#35745;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#30340;&#33021;&#37327;&#26368;&#23567;&#21270;&#25216;&#26415;&#20381;&#36182;&#20110;&#20351;&#29992;&#29289;&#29702;&#27169;&#25311;&#22120;&#65288;oracle&#65289;&#35745;&#31639;&#30340;&#20998;&#23376;&#21147;&#20316;&#20026;&#21453;&#26799;&#24230;&#30340;&#36845;&#20195;&#20248;&#21270;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#26159;&#19968;&#31181;&#35745;&#31639;&#26114;&#36149;&#30340;&#26041;&#27861;&#65292;&#38656;&#35201;&#19982;&#29289;&#29702;&#27169;&#25311;&#22120;&#36827;&#34892;&#35768;&#22810;&#20132;&#20114;&#12290;&#21152;&#36895;&#36825;&#20010;&#36807;&#31243;&#30340;&#19968;&#31181;&#26041;&#27861;&#26159;&#29992;&#31070;&#32463;&#32593;&#32476;&#26367;&#25442;&#29289;&#29702;&#27169;&#25311;&#22120;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;&#31070;&#32463;&#32593;&#32476;&#22312;&#20998;&#23376;&#26500;&#35937;&#33021;&#37327;&#39044;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#36825;&#31181;&#27169;&#22411;&#23481;&#26131;&#21457;&#29983;&#20998;&#24067;&#36716;&#31227;&#65292;&#23548;&#33268;&#33021;&#37327;&#26368;&#23567;&#21270;&#19981;&#20934;&#30830;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36890;&#36807;&#25552;&#20379;&#20248;&#21270;&#36712;&#36857;&#20316;&#20026;&#39069;&#22806;&#35757;&#32451;&#25968;&#25454;&#65292;&#21487;&#20197;&#25913;&#21892;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#33021;&#37327;&#26368;&#23567;&#21270;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#38656;&#35201;&#39069;&#22806;&#32422;$5 \times 10^5$&#20010;&#26500;&#35937;&#25165;&#33021;&#21305;&#37197;&#29289;&#29702;&#27169;&#25311;&#22120;&#30340;&#20248;&#21270;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.06295v2 Announce Type: replace-cross  Abstract: Molecular conformation optimization is crucial to computer-aided drug discovery and materials design. Traditional energy minimization techniques rely on iterative optimization methods that use molecular forces calculated by a physical simulator (oracle) as anti-gradients. However, this is a computationally expensive approach that requires many interactions with a physical simulator. One way to accelerate this procedure is to replace the physical simulator with a neural network. Despite recent progress in neural networks for molecular conformation energy prediction, such models are prone to distribution shift, leading to inaccurate energy minimization. We find that the quality of energy minimization with neural networks can be improved by providing optimization trajectories as additional training data. Still, it takes around $5 \times 10^5$ additional conformations to match the physical simulator's optimization quality. In this 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#28145;&#24230;&#27169;&#22411;&#30340;&#23398;&#20064;&#21160;&#24577;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21387;&#32553;&#36229;&#21442;&#25968;&#21270;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20302;&#32500;&#19981;&#21464;&#23376;&#31354;&#38388;&#20869;&#26356;&#26032;&#26435;&#37325;&#30697;&#38453;&#26469;&#21387;&#32553;&#28145;&#24230;&#32447;&#24615;&#32593;&#32476;&#65292;&#24182;&#22312;&#30697;&#38453;&#24674;&#22797;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#26377;&#25928;&#24615;&#35780;&#20272;</title><link>https://arxiv.org/abs/2311.05061</link><description>&lt;p&gt;
&#36890;&#36807;&#20302;&#32500;&#23398;&#20064;&#21160;&#24577;&#23454;&#29616;&#36229;&#21442;&#25968;&#21270;&#28145;&#24230;&#27169;&#22411;&#30340;&#39640;&#25928;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Efficient Compression of Overparameterized Deep Models through Low-Dimensional Learning Dynamics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.05061
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#28145;&#24230;&#27169;&#22411;&#30340;&#23398;&#20064;&#21160;&#24577;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21387;&#32553;&#36229;&#21442;&#25968;&#21270;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20302;&#32500;&#19981;&#21464;&#23376;&#31354;&#38388;&#20869;&#26356;&#26032;&#26435;&#37325;&#30697;&#38453;&#26469;&#21387;&#32553;&#28145;&#24230;&#32447;&#24615;&#32593;&#32476;&#65292;&#24182;&#22312;&#30697;&#38453;&#24674;&#22797;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#26377;&#25928;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#21442;&#25968;&#21270;&#27169;&#22411;&#24050;&#34987;&#35777;&#26126;&#26159;&#35299;&#20915;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#36807;&#24230;&#21442;&#25968;&#21270;&#24448;&#24448;&#23548;&#33268;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#22823;&#24133;&#22686;&#21152;&#65292;&#36827;&#32780;&#38656;&#35201;&#22823;&#37327;&#36164;&#28304;&#26469;&#35757;&#32451;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#21387;&#32553;&#36229;&#21442;&#25968;&#21270;&#27169;&#22411;&#65292;&#36890;&#36807;&#30740;&#31350;&#23427;&#20204;&#30340;&#23398;&#20064;&#21160;&#24577;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#23545;&#20110;&#35768;&#22810;&#28145;&#24230;&#27169;&#22411;&#65292;&#26435;&#37325;&#30697;&#38453;&#30340;&#26356;&#26032;&#21457;&#29983;&#22312;&#20302;&#32500;&#19981;&#21464;&#23376;&#31354;&#38388;&#20869;&#12290;&#23545;&#20110;&#28145;&#24230;&#32447;&#24615;&#27169;&#22411;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#20204;&#30340;&#20027;&#35201;&#25104;&#20998;&#22312;&#19968;&#20010;&#23567;&#23376;&#31354;&#38388;&#20869;&#36880;&#28176;&#36866;&#37197;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#35265;&#35299;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#28145;&#24230;&#32447;&#24615;&#32593;&#32476;&#30340;&#21387;&#32553;&#31639;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;&#20943;&#23567;&#20854;&#20013;&#38388;&#23618;&#30340;&#23485;&#24230;&#12290;&#25105;&#20204;&#20174;&#23454;&#39564;&#35282;&#24230;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#21387;&#32553;&#25216;&#26415;&#22312;&#30697;&#38453;&#24674;&#22797;&#38382;&#39064;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.05061v2 Announce Type: replace  Abstract: Overparameterized models have proven to be powerful tools for solving various machine learning tasks. However, overparameterization often leads to a substantial increase in computational and memory costs, which in turn requires extensive resources to train. In this work, we present a novel approach for compressing overparameterized models, developed through studying their learning dynamics. We observe that for many deep models, updates to the weight matrices occur within a low-dimensional invariant subspace. For deep linear models, we demonstrate that their principal components are fitted incrementally within a small subspace, and use these insights to propose a compression algorithm for deep linear networks that involve decreasing the width of their intermediate layers. We empirically evaluate the effectiveness of our compression technique on matrix recovery problems. Remarkably, by using an initialization that exploits the structur
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#23545;&#36825;&#20123;&#21830;&#29992;AI/ML&#21152;&#36895;&#22120;&#30340;&#21021;&#27493;&#35780;&#20272;&#21644;&#27604;&#36739;&#65292;&#28145;&#20837;&#25506;&#35752;&#23427;&#20204;&#30340;&#30828;&#20214;&#21644;&#36719;&#20214;&#35774;&#35745;&#29305;&#28857;&#65292;&#20197;&#36776;&#21035;&#23427;&#20204;&#30340;&#21019;&#26032;&#25968;&#25454;&#27969;&#26550;&#26500;&#21644;&#20854;&#20182;&#35774;&#35745;&#20248;&#21270;&#65292;&#25215;&#35834;&#20026;AI/ML&#20219;&#21153;&#25552;&#20379;&#21331;&#36234;&#24615;&#33021;&#21644;&#33021;&#37327;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2311.04417</link><description>&lt;p&gt;
&#35780;&#20272;&#26032;&#20852;&#30340;AI/ML&#21152;&#36895;&#22120;&#65306;IPU&#12289;RDU&#21644;NVIDIA/AMD GPU
&lt;/p&gt;
&lt;p&gt;
Evaluating Emerging AI/ML Accelerators: IPU, RDU, and NVIDIA/AMD GPUs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.04417
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#23545;&#36825;&#20123;&#21830;&#29992;AI/ML&#21152;&#36895;&#22120;&#30340;&#21021;&#27493;&#35780;&#20272;&#21644;&#27604;&#36739;&#65292;&#28145;&#20837;&#25506;&#35752;&#23427;&#20204;&#30340;&#30828;&#20214;&#21644;&#36719;&#20214;&#35774;&#35745;&#29305;&#28857;&#65292;&#20197;&#36776;&#21035;&#23427;&#20204;&#30340;&#21019;&#26032;&#25968;&#25454;&#27969;&#26550;&#26500;&#21644;&#20854;&#20182;&#35774;&#35745;&#20248;&#21270;&#65292;&#25215;&#35834;&#20026;AI/ML&#20219;&#21153;&#25552;&#20379;&#21331;&#36234;&#24615;&#33021;&#21644;&#33021;&#37327;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#24212;&#29992;&#30340;&#19981;&#26029;&#21457;&#23637;&#38656;&#35201;&#24320;&#21457;&#33021;&#22815;&#22788;&#29702;&#26085;&#30410;&#22797;&#26434;&#21644;&#35745;&#31639;&#38656;&#27714;&#30340;&#19987;&#29992;&#30828;&#20214;&#21152;&#36895;&#22120;&#12290;&#20256;&#32479;&#35745;&#31639;&#26550;&#26500;&#22522;&#20110;&#20911;&#183;&#35834;&#20234;&#26364;&#27169;&#22411;&#65292;&#24050;&#32463;&#34987;&#24403;&#20195;AI/ML&#31639;&#27861;&#30340;&#35201;&#27714;&#36229;&#36234;&#65292;&#23548;&#33268;&#20687;Graphcore Intelligence Processing Unit (IPU)&#12289;Sambanova Reconfigurable Datafl&#183;&#183;&#183;&#183;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.04417v2 Announce Type: replace-cross  Abstract: The relentless advancement of artificial intelligence (AI) and machine learning (ML) applications necessitates the development of specialized hardware accelerators capable of handling the increasing complexity and computational demands. Traditional computing architectures, based on the von Neumann model, are being outstripped by the requirements of contemporary AI/ML algorithms, leading to a surge in the creation of accelerators like the Graphcore Intelligence Processing Unit (IPU), Sambanova Reconfigurable Dataflow Unit (RDU), and enhanced GPU platforms. These hardware accelerators are characterized by their innovative data-flow architectures and other design optimizations that promise to deliver superior performance and energy efficiency for AI/ML tasks.   This research provides a preliminary evaluation and comparison of these commercial AI/ML accelerators, delving into their hardware and software design features to discern t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#22238;&#24402;&#24066;&#22330;&#26426;&#21046;&#65292;&#20026;&#25968;&#25454;&#20849;&#20139;&#25552;&#20379;&#20102;&#32463;&#27982;&#28608;&#21169;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#32531;&#35299;&#24066;&#22330;&#20195;&#29702;&#21830;&#38754;&#20020;&#30340;&#36130;&#21153;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2310.14992</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#22238;&#24402;&#24066;&#22330;
&lt;/p&gt;
&lt;p&gt;
Bayesian Regression Markets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.14992
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#22238;&#24402;&#24066;&#22330;&#26426;&#21046;&#65292;&#20026;&#25968;&#25454;&#20849;&#20139;&#25552;&#20379;&#20102;&#32463;&#27982;&#28608;&#21169;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#32531;&#35299;&#24066;&#22330;&#20195;&#29702;&#21830;&#38754;&#20020;&#30340;&#36130;&#21153;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#23545;&#25152;&#20351;&#29992;&#30340;&#25968;&#25454;&#36136;&#37327;&#24456;&#25935;&#24863;&#12290;&#28982;&#32780;&#65292;&#20844;&#21496;&#24448;&#24448;&#24456;&#38590;&#33719;&#24471;&#36275;&#22815;&#30340;&#25968;&#25454;&#38598;&#65292;&#22240;&#20026;&#36825;&#20123;&#25968;&#25454;&#33258;&#28982;&#20998;&#24067;&#22312;&#21508;&#20010;&#25152;&#26377;&#32773;&#20043;&#38388;&#65292;&#32780;&#36825;&#20123;&#25152;&#26377;&#32773;&#22312;&#23454;&#36341;&#20013;&#21487;&#33021;&#26159;&#31454;&#20105;&#23545;&#25163;&#65292;&#19981;&#24895;&#24847;&#20849;&#20139;&#20449;&#24687;&#12290;&#25105;&#20204;&#38024;&#23545;&#22238;&#24402;&#20219;&#21153;&#30340;&#30417;&#30563;&#23398;&#20064;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#22238;&#24402;&#24066;&#22330;&#65292;&#20197;&#25552;&#20379;&#25968;&#25454;&#20849;&#20139;&#30340;&#32463;&#27982;&#28608;&#21169;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26426;&#21046;&#37319;&#29992;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#32771;&#34385;&#26356;&#19968;&#33324;&#30340;&#22238;&#24402;&#20219;&#21153;&#31867;&#21035;&#12290;&#25105;&#20204;&#23545;&#24066;&#22330;&#23646;&#24615;&#36827;&#34892;&#20102;&#24443;&#24213;&#25506;&#35752;&#65292;&#24182;&#23637;&#31034;&#20102;&#30446;&#21069;&#25991;&#29486;&#20013;&#31867;&#20284;&#25552;&#35758;&#26292;&#38706;&#24066;&#22330;&#20195;&#29702;&#21830;&#38754;&#20020;&#21487;&#35266;&#30340;&#36130;&#21153;&#39118;&#38505;&#65292;&#32780;&#36825;&#20123;&#39118;&#38505;&#22312;&#25105;&#20204;&#30340;&#35774;&#32622;&#20013;&#21487;&#20197;&#24471;&#21040;&#32531;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.14992v2 Announce Type: replace  Abstract: Machine learning tasks are vulnerable to the quality of data used as input. Yet, it is often challenging for firms to obtain adequate datasets, with them being naturally distributed amongst owners, that in practice, may be competitors in a downstream market and reluctant to share information. Focusing on supervised learning for regression tasks, we develop a regression market to provide a monetary incentive for data sharing. Our proposed mechanism adopts a Bayesian framework, allowing us to consider a more general class of regression tasks. We present a thorough exploration of the market properties, and show that similar proposals in current literature expose the market agents to sizeable financial risks, which can be mitigated in our setup.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Aranyani&#65292;&#19968;&#31181;&#26012;&#35009;&#38598;&#25104;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#22312;&#32447;&#29615;&#22659;&#20013;&#20248;&#21270;&#32676;&#20307;&#20844;&#24179;&#24615;&#30446;&#26631;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;</title><link>https://arxiv.org/abs/2310.11401</link><description>&lt;p&gt;
&#21033;&#29992;&#26012;&#35009;&#20915;&#31574;&#26862;&#26519;&#22686;&#24378;&#22312;&#32447;&#29615;&#22659;&#20013;&#30340;&#32676;&#20307;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Enhancing Group Fairness in Online Settings Using Oblique Decision Forests
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.11401
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Aranyani&#65292;&#19968;&#31181;&#26012;&#35009;&#38598;&#25104;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#22312;&#32447;&#29615;&#22659;&#20013;&#20248;&#21270;&#32676;&#20307;&#20844;&#24179;&#24615;&#30446;&#26631;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#24179;&#24615;&#65292;&#29305;&#21035;&#26159;&#32676;&#20307;&#20844;&#24179;&#24615;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#20013;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#32771;&#34385;&#22240;&#32032;&#12290;&#30446;&#21069;&#26368;&#24120;&#35265;&#30340;&#32676;&#20307;&#20844;&#24179;&#24615;&#22686;&#24378;&#25216;&#26415;&#26159;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20381;&#36182;&#20844;&#24179;&#30446;&#26631;&#65288;&#20363;&#22914;&#20154;&#21475;&#32479;&#35745;&#24179;&#31561;&#65289;&#21644;&#20219;&#21153;&#29305;&#23450;&#30446;&#26631;&#65288;&#20363;&#22914;&#20132;&#21449;&#29109;&#65289;&#30340;&#28151;&#21512;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22312;&#25968;&#25454;&#20197;&#22312;&#32447;&#26041;&#24335;&#19968;&#27425;&#19968;&#20010;&#23454;&#20363;&#21040;&#36798;&#26102;&#65292;&#20248;&#21270;&#36825;&#26679;&#30340;&#20844;&#24179;&#24615;&#30446;&#26631;&#38754;&#20020;&#30528;&#20960;&#20010;&#25361;&#25112;&#12290;&#29305;&#21035;&#26159;&#65292;&#32676;&#20307;&#20844;&#24179;&#24615;&#30446;&#26631;&#26159;&#36890;&#36807;&#19981;&#21516;&#20154;&#21475;&#32479;&#35745;&#32676;&#20307;&#30340;&#39044;&#27979;&#26399;&#26395;&#26469;&#23450;&#20041;&#30340;&#12290;&#22312;&#22312;&#32447;&#29615;&#22659;&#20013;&#65292;&#31639;&#27861;&#27599;&#27425;&#21482;&#33021;&#35775;&#38382;&#19968;&#20010;&#23454;&#20363;&#65292;&#20272;&#35745;&#32676;&#20307;&#20844;&#24179;&#24615;&#30446;&#26631;&#38656;&#35201;&#39069;&#22806;&#30340;&#23384;&#20648;&#21644;&#27604;&#20219;&#21153;&#29305;&#23450;&#30446;&#26631;&#26356;&#22810;&#30340;&#35745;&#31639;&#65288;&#20363;&#22914;&#21069;&#21521;/&#21518;&#21521;&#20256;&#36882;&#65289;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.11401v2 Announce Type: replace  Abstract: Fairness, especially group fairness, is an important consideration in the context of machine learning systems. The most commonly adopted group fairness-enhancing techniques are in-processing methods that rely on a mixture of a fairness objective (e.g., demographic parity) and a task-specific objective (e.g., cross-entropy) during the training process. However, when data arrives in an online fashion -- one instance at a time -- optimizing such fairness objectives poses several challenges. In particular, group fairness objectives are defined using expectations of predictions across different demographic groups. In the online setting, where the algorithm has access to a single instance at a time, estimating the group fairness objective requires additional storage and significantly more computation (e.g., forward/backward passes) than the task-specific objective at every time step. In this paper, we propose Aranyani, an ensemble of obliq
&lt;/p&gt;</description></item><item><title>Bongard-OpenWorld&#22522;&#20934;&#26088;&#22312;&#35780;&#20272;&#26426;&#22120;&#35270;&#35273;&#20013;&#23545;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#33258;&#30001;&#24418;&#24335;&#35270;&#35273;&#27010;&#24565;&#36827;&#34892;&#23569;&#26679;&#26412;&#25512;&#29702;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#24320;&#25918;&#19990;&#30028;&#33258;&#30001;&#24418;&#24335;&#27010;&#24565;&#21644;&#30495;&#23454;&#19990;&#30028;&#22270;&#20687;&#20004;&#39033;&#26032;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2310.10207</link><description>&lt;p&gt;
Bongard-OpenWorld: &#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#36827;&#34892;&#33258;&#30001;&#24418;&#24335;&#35270;&#35273;&#27010;&#24565;&#30340;&#23569;&#26679;&#26412;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Bongard-OpenWorld: Few-Shot Reasoning for Free-form Visual Concepts in the Real World
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.10207
&lt;/p&gt;
&lt;p&gt;
Bongard-OpenWorld&#22522;&#20934;&#26088;&#22312;&#35780;&#20272;&#26426;&#22120;&#35270;&#35273;&#20013;&#23545;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#33258;&#30001;&#24418;&#24335;&#35270;&#35273;&#27010;&#24565;&#36827;&#34892;&#23569;&#26679;&#26412;&#25512;&#29702;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#24320;&#25918;&#19990;&#30028;&#33258;&#30001;&#24418;&#24335;&#27010;&#24565;&#21644;&#30495;&#23454;&#19990;&#30028;&#22270;&#20687;&#20004;&#39033;&#26032;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Bongard-OpenWorld&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#26426;&#22120;&#35270;&#35273;&#20013;&#30495;&#23454;&#19990;&#30028;&#23569;&#26679;&#26412;&#25512;&#29702;&#30340;&#26032;&#22522;&#20934;&#12290; &#23427;&#28304;&#33258;&#32463;&#20856;&#30340;Bongard&#38382;&#39064;&#65288;BPs&#65289;&#65306;&#32473;&#23450;&#20004;&#32452;&#22270;&#20687;&#65288;&#27491;&#21644;&#36127;&#65289;&#65292;&#27169;&#22411;&#38656;&#35201;&#36890;&#36807;&#35825;&#23548;&#35270;&#35273;&#27010;&#24565;&#26469;&#30830;&#23450;&#26597;&#35810;&#22270;&#20687;&#25152;&#23646;&#30340;&#22270;&#20687;&#38598;&#65292;&#36825;&#20123;&#27010;&#24565;&#20165;&#30001;&#27491;&#38598;&#20013;&#30340;&#22270;&#20687;&#25152;&#25551;&#36848;&#12290; &#25105;&#20204;&#30340;&#22522;&#20934;&#32487;&#25215;&#20102;&#21407;&#22987;BPs&#30340;&#23569;&#26679;&#26412;&#27010;&#24565;&#24402;&#32435;&#65292;&#21516;&#26102;&#22686;&#21152;&#20102;&#20004;&#23618;&#26032;&#25361;&#25112;&#65306;1&#65289;&#24320;&#25918;&#19990;&#30028;&#30340;&#33258;&#30001;&#24418;&#24335;&#27010;&#24565;&#65292;&#22240;&#20026;Bongard-OpenWorld&#20013;&#30340;&#35270;&#35273;&#27010;&#24565;&#26159;&#20174;&#24320;&#25918;&#35789;&#27719;&#34920;&#20013;&#29420;&#29305;&#32452;&#21512;&#30340;&#26415;&#35821;&#65292;&#33539;&#22260;&#20174;&#23545;&#35937;&#31867;&#21035;&#21040;&#25277;&#35937;&#35270;&#35273;&#23646;&#24615;&#21644;&#24120;&#35782;&#20107;&#23454;&#30693;&#35782;&#65307; 2&#65289;&#30495;&#23454;&#19990;&#30028;&#30340;&#22270;&#20687;&#65292;&#32780;&#19981;&#26159;&#35768;&#22810;&#31867;&#20284;&#29289;&#20351;&#29992;&#30340;&#21512;&#25104;&#22270;&#34920;&#12290;&#22312;&#25105;&#20204;&#30340;&#25506;&#32034;&#20013;&#65292;Bongard-OpenWorld&#24050;&#32463;&#23545;&#24403;&#21069;&#30340;&#23569;&#26679;&#26412;&#25512;&#29702;&#31639;&#27861;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#25105;&#20204;&#36824;&#36828;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.10207v2 Announce Type: replace  Abstract: We introduce Bongard-OpenWorld, a new benchmark for evaluating real-world few-shot reasoning for machine vision. It originates from the classical Bongard Problems (BPs): Given two sets of images (positive and negative), the model needs to identify the set that query images belong to by inducing the visual concepts, which is exclusively depicted by images from the positive set. Our benchmark inherits the few-shot concept induction of the original BPs while adding the two novel layers of challenge: 1) open-world free-form concepts, as the visual concepts in Bongard-OpenWorld are unique compositions of terms from an open vocabulary, ranging from object categories to abstract visual attributes and commonsense factual knowledge; 2) real-world images, as opposed to the synthetic diagrams used by many counterparts. In our exploration, Bongard-OpenWorld already imposes a significant challenge to current few-shot reasoning algorithms. We furt
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#32534;&#36753;&#30340;&#28508;&#22312;&#38519;&#38449;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#21457;&#29616;&#30693;&#35782;&#20914;&#31361;&#21644;&#30693;&#35782;&#25197;&#26354;&#26159;&#20004;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2310.02129</link><description>&lt;p&gt;
&#25581;&#31034;&#22823;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#32534;&#36753;&#30340;&#38519;&#38449;
&lt;/p&gt;
&lt;p&gt;
Unveiling the Pitfalls of Knowledge Editing for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.02129
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#32534;&#36753;&#30340;&#28508;&#22312;&#38519;&#38449;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#21457;&#29616;&#30693;&#35782;&#20914;&#31361;&#21644;&#30693;&#35782;&#25197;&#26354;&#26159;&#20004;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25104;&#26412;&#19981;&#26029;&#19978;&#21319;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#24037;&#20316;&#24050;&#32463;&#36716;&#21521;&#24320;&#21457;&#32534;&#36753;LLMs&#20869;&#22312;&#30693;&#35782;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20173;&#26377;&#19968;&#20010;&#38452;&#20113;&#24748;&#22312;&#22836;&#39030;&#19978; - &#30693;&#35782;&#32534;&#36753;&#26159;&#21542;&#20250;&#35302;&#21457;&#34676;&#34678;&#25928;&#24212;&#65311;&#22240;&#20026;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#30693;&#35782;&#32534;&#36753;&#26159;&#21542;&#20250;&#24341;&#20837;&#21487;&#33021;&#24102;&#26469;&#28508;&#22312;&#39118;&#38505;&#30340;&#21103;&#20316;&#29992;&#12290;&#26412;&#25991;&#39318;&#27425;&#25506;&#35752;&#20102;&#19982;LLMs&#30693;&#35782;&#32534;&#36753;&#30456;&#20851;&#30340;&#28508;&#22312;&#38519;&#38449;&#12290;&#20026;&#23454;&#29616;&#27492;&#30446;&#30340;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#24182;&#25552;&#20986;&#20102;&#21019;&#26032;&#24615;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#24378;&#35843;&#20102;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#65288;1&#65289;&#30693;&#35782;&#20914;&#31361;&#65306;&#32534;&#36753;&#36923;&#36753;&#20914;&#31361;&#30340;&#20107;&#23454;&#32452;&#21487;&#33021;&#20250;&#25918;&#22823;LLMs&#22266;&#26377;&#30340;&#19981;&#19968;&#33268;&#24615; - &#36825;&#26159;&#20197;&#21069;&#26041;&#27861;&#24573;&#30053;&#30340;&#19968;&#20010;&#26041;&#38754;&#12290;&#65288;2&#65289;&#30693;&#35782;&#25197;&#26354;&#65306;&#20026;&#20102;&#32534;&#36753;&#20107;&#23454;&#30693;&#35782;&#32780;&#26356;&#25913;&#21442;&#25968;&#21487;&#33021;&#20250;&#19981;&#21487;&#36870;&#22320;&#25197;&#26354;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.02129v3 Announce Type: replace-cross  Abstract: As the cost associated with fine-tuning Large Language Models (LLMs) continues to rise, recent research efforts have pivoted towards developing methodologies to edit implicit knowledge embedded within LLMs. Yet, there's still a dark cloud lingering overhead -- will knowledge editing trigger butterfly effect? since it is still unclear whether knowledge editing might introduce side effects that pose potential risks or not. This paper pioneers the investigation into the potential pitfalls associated with knowledge editing for LLMs. To achieve this, we introduce new benchmark datasets and propose innovative evaluation metrics. Our results underline two pivotal concerns: (1) Knowledge Conflict: Editing groups of facts that logically clash can magnify the inherent inconsistencies in LLMs-a facet neglected by previous methods. (2) Knowledge Distortion: Altering parameters with the aim of editing factual knowledge can irrevocably warp 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#20613;&#31435;&#21494;&#21644;&#22810;&#39033;&#24335;&#29305;&#24449;&#30340;&#26041;&#27861;&#65292;&#24182;&#22522;&#20110;&#27492;&#29305;&#24449;&#37327;&#21270;&#25552;&#20986;&#23558;&#30456;&#20851;&#27169;&#22411;&#26435;&#37325;&#20063;&#36827;&#34892;&#37327;&#21270;&#65292;&#24471;&#21040;&#20102;&#26356;&#20855;&#34920;&#36798;&#21147;&#30340;&#24352;&#37327;&#32593;&#32476;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2309.05436</link><description>&lt;p&gt;
&#29992;&#20110;&#26356;&#20855;&#34920;&#36798;&#21147;&#30340;&#24352;&#37327;&#32593;&#32476;&#27169;&#22411;&#30340;&#37327;&#21270;&#20613;&#31435;&#21494;&#21644;&#22810;&#39033;&#24335;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Quantized Fourier and Polynomial Features for more Expressive Tensor Network Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.05436
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#20613;&#31435;&#21494;&#21644;&#22810;&#39033;&#24335;&#29305;&#24449;&#30340;&#26041;&#27861;&#65292;&#24182;&#22522;&#20110;&#27492;&#29305;&#24449;&#37327;&#21270;&#25552;&#20986;&#23558;&#30456;&#20851;&#27169;&#22411;&#26435;&#37325;&#20063;&#36827;&#34892;&#37327;&#21270;&#65292;&#24471;&#21040;&#20102;&#26356;&#20855;&#34920;&#36798;&#21147;&#30340;&#24352;&#37327;&#32593;&#32476;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26680;&#26426;&#22120;&#30340;&#32972;&#26223;&#19979;&#65292;&#22810;&#39033;&#24335;&#21644;&#20613;&#31435;&#21494;&#29305;&#24449;&#36890;&#24120;&#29992;&#20110;&#36890;&#36807;&#23558;&#25968;&#25454;&#26144;&#23556;&#21040;&#26356;&#39640;&#32500;&#31354;&#38388;&#26469;&#20026;&#32447;&#24615;&#27169;&#22411;&#25552;&#20379;&#38750;&#32447;&#24615;&#25193;&#23637;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37327;&#21270;&#20102;&#22810;&#39033;&#24335;&#21644;&#20613;&#31435;&#21494;&#29305;&#24449;&#65292;&#25552;&#20986;&#20102;&#23558;&#30456;&#20851;&#27169;&#22411;&#26435;&#37325;&#37327;&#21270;&#30340;&#26041;&#27861;&#65292;&#24471;&#21040;&#20102;&#37327;&#21270;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.05436v2 Announce Type: replace  Abstract: In the context of kernel machines, polynomial and Fourier features are commonly used to provide a nonlinear extension to linear models by mapping the data to a higher-dimensional space. Unless one considers the dual formulation of the learning problem, which renders exact large-scale learning unfeasible, the exponential increase of model parameters in the dimensionality of the data caused by their tensor-product structure prohibits to tackle high-dimensional problems. One of the possible approaches to circumvent this exponential scaling is to exploit the tensor structure present in the features by constraining the model weights to be an underparametrized tensor network. In this paper we quantize, i.e. further tensorize, polynomial and Fourier features. Based on this feature quantization we propose to quantize the associated model weights, yielding quantized models. We show that, for the same number of model parameters, the resulting 
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#30452;&#26041;&#22270;&#21305;&#37197;&#26469;&#36716;&#25442;&#39069;&#22806;&#25968;&#25454;&#22312;&#22788;&#29702;&#22495;&#20559;&#31227;&#26102;&#27604;&#31616;&#21333;&#24402;&#19968;&#21270;&#21462;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;</title><link>https://arxiv.org/abs/2309.02001</link><description>&lt;p&gt;
&#20998;&#26512;&#22312;MICCAI KiTS23&#25361;&#25112;&#20013;&#20351;&#29992;&#39069;&#22806;&#25968;&#25454;&#26102;&#30340;&#22495;&#20559;&#31227;
&lt;/p&gt;
&lt;p&gt;
Analyzing domain shift when using additional data for the MICCAI KiTS23 Challenge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.02001
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#30452;&#26041;&#22270;&#21305;&#37197;&#26469;&#36716;&#25442;&#39069;&#22806;&#25968;&#25454;&#22312;&#22788;&#29702;&#22495;&#20559;&#31227;&#26102;&#27604;&#31616;&#21333;&#24402;&#19968;&#21270;&#21462;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#39069;&#22806;&#30340;&#35757;&#32451;&#25968;&#25454;&#24050;&#30693;&#21487;&#20197;&#25913;&#21892;&#32467;&#26524;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#21307;&#23398;&#22270;&#20687;3D&#20998;&#21106;&#65292;&#22312;&#37027;&#37324;&#32570;&#20047;&#35757;&#32451;&#36164;&#26009;&#65292;&#27169;&#22411;&#38656;&#35201;&#22312;&#23569;&#37327;&#25968;&#25454;&#19978;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#26032;&#25968;&#25454;&#21487;&#33021;&#26159;&#20351;&#29992;&#20854;&#20182;&#20202;&#22120;&#33719;&#21462;&#24182;&#32463;&#36807;&#39044;&#22788;&#29702;&#65292;&#20351;&#24471;&#20854;&#20998;&#24067;&#19982;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#26174;&#33879;&#19981;&#21516;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#25913;&#21892;&#22495;&#20559;&#31227;&#30340;&#25216;&#26415;&#65292;&#20351;&#24471;&#39069;&#22806;&#25968;&#25454;&#21487;&#20197;&#26356;&#22909;&#22320;&#29992;&#20110;&#39044;&#22788;&#29702;&#21644;&#19982;&#21407;&#22987;&#25968;&#25454;&#19968;&#36215;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#30452;&#26041;&#22270;&#21305;&#37197;&#36716;&#25442;&#39069;&#22806;&#25968;&#25454;&#30340;&#25928;&#26524;&#20248;&#20110;&#31616;&#21333;&#30340;&#24402;&#19968;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.02001v2 Announce Type: replace-cross  Abstract: Using additional training data is known to improve the results, especially for medical image 3D segmentation where there is a lack of training material and the model needs to generalize well from few available data. However, the new data could have been acquired using other instruments and preprocessed such its distribution is significantly different from the original training data. Therefore, we study techniques which ameliorate domain shift during training so that the additional data becomes better usable for preprocessing and training together with the original data. Our results show that transforming the additional data using histogram matching has better results than using simple normalization.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39046;&#22495;&#25552;&#20986;&#20102;&#20559;&#35265;&#35780;&#20272;&#21644;&#32531;&#35299;&#25216;&#26415;&#30340;&#32508;&#21512;&#35843;&#26597;&#65292;&#23450;&#20041;&#20102;&#20844;&#24179;&#24615;&#30340;&#19981;&#21516;&#26041;&#38754;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#20010;&#20998;&#31867;&#20307;&#31995;&#65292;&#20197;&#21327;&#21161;&#30740;&#31350;&#20154;&#21592;&#23545;LLMs&#36827;&#34892;&#20844;&#24179;&#24615;&#20998;&#26512;&#21644;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2309.00770</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#19982;&#20844;&#24179;&#24615;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Bias and Fairness in Large Language Models: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.00770
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39046;&#22495;&#25552;&#20986;&#20102;&#20559;&#35265;&#35780;&#20272;&#21644;&#32531;&#35299;&#25216;&#26415;&#30340;&#32508;&#21512;&#35843;&#26597;&#65292;&#23450;&#20041;&#20102;&#20844;&#24179;&#24615;&#30340;&#19981;&#21516;&#26041;&#38754;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#20010;&#20998;&#31867;&#20307;&#31995;&#65292;&#20197;&#21327;&#21161;&#30740;&#31350;&#20154;&#21592;&#23545;LLMs&#36827;&#34892;&#20844;&#24179;&#24615;&#20998;&#26512;&#21644;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#20351;&#24471;&#20154;&#20204;&#33021;&#22815;&#22788;&#29702;&#12289;&#29702;&#35299;&#21644;&#29983;&#25104;&#31867;&#20284;&#20154;&#31867;&#25991;&#26412;&#65292;&#36880;&#28176;&#34701;&#20837;&#35302;&#21450;&#25105;&#20204;&#31038;&#20132;&#39046;&#22495;&#30340;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#21462;&#24471;&#25104;&#21151;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#23398;&#20064;&#12289;&#24310;&#32493;&#21644;&#25918;&#22823;&#26377;&#23475;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;&#26412;&#25991;&#23545;LLMs&#30340;&#20559;&#35265;&#35780;&#20272;&#21644;&#32531;&#35299;&#25216;&#26415;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#12290;&#25105;&#20204;&#39318;&#20808;&#25972;&#21512;&#12289;&#24418;&#24335;&#21270;&#21644;&#25193;&#23637;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#31038;&#20250;&#20559;&#35265;&#21644;&#20844;&#24179;&#24615;&#30340;&#27010;&#24565;&#65292;&#23450;&#20041;&#20102;&#20260;&#23475;&#30340;&#19981;&#21516;&#26041;&#38754;&#65292;&#24182;&#24341;&#20837;&#20102;&#20960;&#20010;&#23454;&#29616;LLMs&#20844;&#24179;&#24615;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19977;&#20010;&#30452;&#35266;&#30340;&#20998;&#31867;&#20307;&#31995;&#32479;&#19968;&#20102;&#25991;&#29486;&#65292;&#20854;&#20013;&#21253;&#25324;&#20004;&#20010;&#29992;&#20110;&#20559;&#35265;&#35780;&#20272;&#30340;&#20998;&#31867;&#20307;&#31995;&#65292;&#21363;&#25351;&#26631;&#21644;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#19968;&#20010;&#29992;&#20110;&#32531;&#35299;&#30340;&#20998;&#31867;&#20307;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.00770v2 Announce Type: replace-cross  Abstract: Rapid advancements of large language models (LLMs) have enabled the processing, understanding, and generation of human-like text, with increasing integration into systems that touch our social sphere. Despite this success, these models can learn, perpetuate, and amplify harmful social biases. In this paper, we present a comprehensive survey of bias evaluation and mitigation techniques for LLMs. We first consolidate, formalize, and expand notions of social bias and fairness in natural language processing, defining distinct facets of harm and introducing several desiderata to operationalize fairness for LLMs. We then unify the literature by proposing three intuitive taxonomies, two for bias evaluation, namely metrics and datasets, and one for mitigation. Our first taxonomy of metrics for bias evaluation disambiguates the relationship between metrics and evaluation datasets, and organizes metrics by the different levels at which t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;Stiefel&#27969;&#24418;&#19978;&#36827;&#34892;&#20998;&#24067;&#24335;&#20248;&#21270;&#30340;&#40654;&#26364;&#20849;&#36717;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#20811;&#26381;&#20102;&#20840;&#23616;&#20989;&#25968;&#38750;&#20984;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2308.10547</link><description>&lt;p&gt;
&#22522;&#20110;Stiefel&#27969;&#24418;&#30340;&#20998;&#24067;&#24335;&#40654;&#26364;&#20849;&#36717;&#26799;&#24230;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Decentralized Riemannian Conjugate Gradient Method on the Stiefel Manifold
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.10547
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;Stiefel&#27969;&#24418;&#19978;&#36827;&#34892;&#20998;&#24067;&#24335;&#20248;&#21270;&#30340;&#40654;&#26364;&#20849;&#36717;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#20811;&#26381;&#20102;&#20840;&#23616;&#20989;&#25968;&#38750;&#20984;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20849;&#36717;&#26799;&#24230;&#27861;&#26159;&#19968;&#31181;&#33267;&#20851;&#37325;&#35201;&#30340;&#19968;&#38454;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#24120;&#27604;&#26368;&#36895;&#19979;&#38477;&#27861;&#25910;&#25947;&#26356;&#24555;&#65292;&#35745;&#31639;&#25104;&#26412;&#20063;&#36828;&#20302;&#20110;&#20108;&#38454;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#22312;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#21644;&#40654;&#26364;&#27969;&#24418;&#19978;&#24050;&#30740;&#31350;&#20102;&#21508;&#31181;&#31867;&#22411;&#30340;&#20849;&#36717;&#26799;&#24230;&#26041;&#27861;&#65292;&#20294;&#22312;&#20998;&#24067;&#24335;&#22330;&#26223;&#19979;&#30340;&#30740;&#31350;&#21364;&#24456;&#23569;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26088;&#22312;&#22312;Stiefel&#27969;&#24418;&#19978;&#26368;&#23567;&#21270;&#20840;&#23616;&#20989;&#25968;&#30340;&#20998;&#24067;&#24335;&#40654;&#26364;&#20849;&#36717;&#26799;&#24230;&#19979;&#38477;&#65288;DRCGD&#65289;&#26041;&#27861;&#12290;&#20248;&#21270;&#38382;&#39064;&#22312;&#19968;&#32452;&#20195;&#29702;&#32593;&#32476;&#20013;&#20998;&#24067;&#65292;&#27599;&#20010;&#20195;&#29702;&#19982;&#19968;&#20010;&#23616;&#37096;&#20989;&#25968;&#30456;&#20851;&#32852;&#65292;&#24182;&#19988;&#20195;&#29702;&#20043;&#38388;&#30340;&#36890;&#20449;&#22312;&#19968;&#20010;&#26080;&#21521;&#36830;&#36890;&#22270;&#19978;&#36827;&#34892;&#12290;&#30001;&#20110;Stiefel&#27969;&#24418;&#26159;&#19968;&#20010;&#38750;&#20984;&#38598;&#65292;&#20840;&#23616;&#20989;&#25968;&#34987;&#34920;&#31034;&#20026;&#21487;&#33021;&#38750;&#20984;&#65288;&#20294;&#24179;&#28369;&#65289;&#23616;&#37096;&#20989;&#25968;&#30340;&#26377;&#38480;&#21644;&#12290;&#35813;&#26041;&#27861;&#19981;&#21463;&#26368;&#20248;&#38750;&#20984;&#24120;&#25968;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2308.10547v2 Announce Type: replace-cross  Abstract: The conjugate gradient method is a crucial first-order optimization method that generally converges faster than the steepest descent method, and its computational cost is much lower than the second-order methods. However, while various types of conjugate gradient methods have been studied in Euclidean spaces and on Riemannian manifolds, there is little study for those in distributed scenarios. This paper proposes a decentralized Riemannian conjugate gradient descent (DRCGD) method that aims at minimizing a global function over the Stiefel manifold. The optimization problem is distributed among a network of agents, where each agent is associated with a local function, and the communication between agents occurs over an undirected connected graph. Since the Stiefel manifold is a non-convex set, a global function is represented as a finite sum of possibly non-convex (but smooth) local functions. The proposed method is free from ex
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21306;&#22359;&#38142;&#21644;&#20998;&#24067;&#24335;&#20998;&#31867;&#36134;&#25216;&#26415;&#30340;&#23433;&#20840;&#21644;&#21487;&#38752;&#30340;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#65292;&#21253;&#25324;&#28857;&#23545;&#28857;&#25237;&#31080;&#26426;&#21046;&#21644;&#22870;&#21169;&#21644;&#24809;&#32602;&#26426;&#21046;&#65292;&#20197;&#26816;&#27979;&#21644;&#38459;&#27490;&#24694;&#24847;&#34892;&#20026;&#65292;&#35777;&#26126;&#20102;&#35813;&#26694;&#26550;&#23545;&#25239;&#24694;&#24847;&#23458;&#25143;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2307.00543</link><description>&lt;p&gt;
&#20351;&#29992;&#21306;&#22359;&#38142;&#38450;&#24481;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#24694;&#24847;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Defending Against Malicious Behaviors in Federated Learning with Blockchain
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2307.00543
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21306;&#22359;&#38142;&#21644;&#20998;&#24067;&#24335;&#20998;&#31867;&#36134;&#25216;&#26415;&#30340;&#23433;&#20840;&#21644;&#21487;&#38752;&#30340;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#65292;&#21253;&#25324;&#28857;&#23545;&#28857;&#25237;&#31080;&#26426;&#21046;&#21644;&#22870;&#21169;&#21644;&#24809;&#32602;&#26426;&#21046;&#65292;&#20197;&#26816;&#27979;&#21644;&#38459;&#27490;&#24694;&#24847;&#34892;&#20026;&#65292;&#35777;&#26126;&#20102;&#35813;&#26694;&#26550;&#23545;&#25239;&#24694;&#24847;&#23458;&#25143;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#26102;&#20195;&#65292;&#32852;&#37030;&#23398;&#20064;(FL)&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#20801;&#35768;&#22810;&#23478;&#26426;&#26500;&#25968;&#25454;&#25152;&#26377;&#32773;&#25110;&#23458;&#25143;&#20849;&#21516;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#32780;&#19981;&#20250;&#25439;&#23475;&#25968;&#25454;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;FL&#26041;&#27861;&#20381;&#36182;&#20110;&#29992;&#20110;&#20840;&#23616;&#27169;&#22411;&#32858;&#21512;&#30340;&#38598;&#20013;&#24335;&#26381;&#21153;&#22120;&#65292;&#23548;&#33268;&#21333;&#28857;&#25925;&#38556;&#12290;&#36825;&#20351;&#31995;&#32479;&#22312;&#22788;&#29702;&#19981;&#35802;&#23454;&#30340;&#23458;&#25143;&#26102;&#23481;&#26131;&#21463;&#21040;&#24694;&#24847;&#25915;&#20987;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#22522;&#20110;&#21306;&#22359;&#38142;&#21644;&#20998;&#24067;&#24335;&#20998;&#31867;&#36134;&#25216;&#26415;&#30340;&#23433;&#20840;&#21487;&#38752;FL&#31995;&#32479;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#32467;&#21512;&#20102;&#28857;&#23545;&#28857;&#25237;&#31080;&#26426;&#21046;&#21644;&#22870;&#21169;&#21644;&#24809;&#32602;&#26426;&#21046;&#65292;&#30001;&#38142;&#19978;&#26234;&#33021;&#21512;&#32422;&#25552;&#20379;&#21160;&#21147;&#65292;&#20197;&#26816;&#27979;&#21644;&#38459;&#27490;&#24694;&#24847;&#34892;&#20026;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#29702;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#65292;&#20197;&#23637;&#31034;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#34920;&#26126;&#25105;&#20204;&#30340;&#26694;&#26550;&#23545;&#24694;&#24847;&#23458;&#25143;&#26159;&#24378;&#22823;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2307.00543v2 Announce Type: replace-cross  Abstract: In the era of deep learning, federated learning (FL) presents a promising approach that allows multi-institutional data owners, or clients, to collaboratively train machine learning models without compromising data privacy. However, most existing FL approaches rely on a centralized server for global model aggregation, leading to a single point of failure. This makes the system vulnerable to malicious attacks when dealing with dishonest clients. In this work, we address this problem by proposing a secure and reliable FL system based on blockchain and distributed ledger technology. Our system incorporates a peer-to-peer voting mechanism and a reward-and-slash mechanism, which are powered by on-chain smart contracts, to detect and deter malicious behaviors. Both theoretical and empirical analyses are presented to demonstrate the effectiveness of the proposed approach, showing that our framework is robust against malicious client-s
&lt;/p&gt;</description></item><item><title>ProMIL&#26159;&#19968;&#31181;&#22522;&#20110;&#23454;&#20363;&#30340;&#26041;&#27861;&#65292;&#22312;&#21307;&#23398;&#24212;&#29992;&#20013;&#33021;&#22815;&#33258;&#21160;&#26816;&#27979;&#26368;&#20339;&#20915;&#31574;&#30334;&#20998;&#27604;&#27700;&#24179;&#65292;&#24182;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#34920;&#29616;&#20248;&#24322;.</title><link>https://arxiv.org/abs/2306.10535</link><description>&lt;p&gt;
ProMIL&#65306;&#38754;&#21521;&#21307;&#23398;&#25104;&#20687;&#30340;&#27010;&#29575;&#22810;&#23454;&#20363;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
ProMIL: Probabilistic Multiple Instance Learning for Medical Imaging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.10535
&lt;/p&gt;
&lt;p&gt;
ProMIL&#26159;&#19968;&#31181;&#22522;&#20110;&#23454;&#20363;&#30340;&#26041;&#27861;&#65292;&#22312;&#21307;&#23398;&#24212;&#29992;&#20013;&#33021;&#22815;&#33258;&#21160;&#26816;&#27979;&#26368;&#20339;&#20915;&#31574;&#30334;&#20998;&#27604;&#27700;&#24179;&#65292;&#24182;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#34920;&#29616;&#20248;&#24322;.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#23454;&#20363;&#23398;&#20064;&#65288;MIL&#65289;&#26159;&#19968;&#20010;&#24369;&#30417;&#30563;&#38382;&#39064;&#65292;&#20854;&#20013;&#19968;&#20010;&#26631;&#31614;&#34987;&#20998;&#37197;&#32473;&#25972;&#20010;&#23454;&#20363;&#21253;&#12290;&#20854;&#20013;&#19968;&#31181;&#37325;&#35201;&#30340;MIL&#27169;&#22411;&#26159;&#22522;&#20110;&#23454;&#20363;&#30340;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;&#23454;&#20363;&#36827;&#34892;&#20998;&#31867;&#65292;&#28982;&#21518;&#32858;&#21512;&#36825;&#20123;&#39044;&#27979;&#32467;&#26524;&#20197;&#33719;&#24471;&#21253;&#26631;&#31614;&#12290;&#26368;&#24120;&#35265;&#30340;MIL&#27169;&#22411;&#26159;&#23558;&#21253;&#35270;&#20026;&#27491;&#31867;&#65292;&#22914;&#26524;&#20854;&#23454;&#20363;&#20013;&#33267;&#23569;&#26377;&#19968;&#20010;&#23454;&#20363;&#20855;&#26377;&#27491;&#26631;&#31614;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#36825;&#31181;&#25512;&#29702;&#24182;&#19981;&#25104;&#31435;&#65292;&#20854;&#20013;&#27491;&#21253;&#26631;&#31614;&#36890;&#24120;&#26159;&#37096;&#20998;&#27491;&#23454;&#20363;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#19987;&#38376;&#30340;&#22522;&#20110;&#23454;&#20363;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;ProMIL&#65292;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;Bernstein&#22810;&#39033;&#24335;&#20272;&#35745;&#12290;ProMIL&#30340;&#19968;&#20010;&#37325;&#35201;&#20248;&#21183;&#26159;&#23427;&#21487;&#20197;&#33258;&#21160;&#26816;&#27979;&#29992;&#20110;&#20915;&#31574;&#30340;&#26368;&#20339;&#30334;&#20998;&#27604;&#27700;&#24179;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;ProMIL&#22312;&#30495;&#23454;&#19990;&#30028;&#21307;&#23398;&#24212;&#29992;&#20013;&#20248;&#20110;&#26631;&#20934;&#22522;&#20110;&#23454;&#20363;&#30340;MIL&#12290;&#25105;&#20204;&#25552;&#20379;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2306.10535v2 Announce Type: replace-cross  Abstract: Multiple Instance Learning (MIL) is a weakly-supervised problem in which one label is assigned to the whole bag of instances. An important class of MIL models is instance-based, where we first classify instances and then aggregate those predictions to obtain a bag label. The most common MIL model is when we consider a bag as positive if at least one of its instances has a positive label. However, this reasoning does not hold in many real-life scenarios, where the positive bag label is often a consequence of a certain percentage of positive instances. To address this issue, we introduce a dedicated instance-based method called ProMIL, based on deep neural networks and Bernstein polynomial estimation. An important advantage of ProMIL is that it can automatically detect the optimal percentage level for decision-making. We show that ProMIL outperforms standard instance-based MIL in real-world medical applications. We make the code 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;The Attention Patch&#65288;TAP&#65289;&#31070;&#32463;&#32593;&#32476;&#38468;&#21152;&#32452;&#20214;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20801;&#35768;&#20174;&#26410;&#26631;&#35760;&#30340;&#27425;&#35201;&#27169;&#24577;&#23454;&#29616;&#36328;&#27169;&#24577;&#30340;&#25968;&#25454;&#32423;&#30693;&#35782;&#20256;&#36882;&#12290;</title><link>https://arxiv.org/abs/2302.02224</link><description>&lt;p&gt;
TAP: &#36328;&#27169;&#24577;&#30693;&#35782;&#20256;&#36882;&#20013;&#30340;&#27880;&#24847;&#21147;&#34917;&#19969;
&lt;/p&gt;
&lt;p&gt;
TAP: The Attention Patch for Cross-Modal Knowledge Transfer from Unlabeled Modality
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.02224
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;The Attention Patch&#65288;TAP&#65289;&#31070;&#32463;&#32593;&#32476;&#38468;&#21152;&#32452;&#20214;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20801;&#35768;&#20174;&#26410;&#26631;&#35760;&#30340;&#27425;&#35201;&#27169;&#24577;&#23454;&#29616;&#36328;&#27169;&#24577;&#30340;&#25968;&#25454;&#32423;&#30693;&#35782;&#20256;&#36882;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#36328;&#27169;&#24577;&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#30446;&#26631;&#26159;&#36890;&#36807;&#26410;&#26631;&#35760;&#12289;&#19981;&#37197;&#23545;&#30340;&#27425;&#35201;&#27169;&#24577;&#65292;&#22686;&#24378;&#20027;&#35201;&#27169;&#24577;&#20013;&#30417;&#30563;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;&#37319;&#29992;&#27010;&#29575;&#26041;&#27861;&#36827;&#34892;&#32570;&#22833;&#20449;&#24687;&#20272;&#35745;&#65292;&#25105;&#20204;&#34920;&#26126;&#27425;&#35201;&#27169;&#24577;&#20013;&#21253;&#21547;&#30340;&#39069;&#22806;&#20449;&#24687;&#21487;&#20197;&#36890;&#36807;Nadaraya-Watson&#65288;NW&#65289;&#26680;&#22238;&#24402;&#36827;&#34892;&#20272;&#35745;&#65292;&#20854;&#21487;&#20197;&#36827;&#19968;&#27493;&#34920;&#31034;&#20026;&#32463;&#36807;&#32447;&#24615;&#21464;&#25442;&#30340;&#26680;&#20132;&#21449;&#27880;&#24847;&#21147;&#27169;&#22359;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20026;&#24341;&#20837;The Attention Patch&#65288;TAP&#65289;&#22880;&#23450;&#20102;&#22522;&#30784;&#65292;&#36825;&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#31070;&#32463;&#32593;&#32476;&#38468;&#21152;&#32452;&#20214;&#65292;&#20801;&#35768;&#20174;&#26410;&#26631;&#35760;&#30340;&#27169;&#24577;&#36827;&#34892;&#25968;&#25454;&#32423;&#30693;&#35782;&#20256;&#36882;&#12290;&#25105;&#20204;&#20351;&#29992;&#22235;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#22823;&#37327;&#25968;&#20540;&#27169;&#25311;&#65292;&#32467;&#26524;&#34920;&#26126;TAP&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#36328;&#19981;&#21516;&#39046;&#22495;&#21644;&#19981;&#21516;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21033;&#29992;&#30475;&#20284;&#26080;&#29992;&#30340;&#26410;&#26631;&#35760;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.02224v2 Announce Type: replace  Abstract: This paper addresses a cross-modal learning framework, where the objective is to enhance the performance of supervised learning in the primary modality using an unlabeled, unpaired secondary modality. Taking a probabilistic approach for missing information estimation, we show that the extra information contained in the secondary modality can be estimated via Nadaraya-Watson (NW) kernel regression, which can further be expressed as a kernelized cross-attention module (under linear transformation). Our results lay the foundations for introducing The Attention Patch (TAP), a simple neural network add-on that allows data-level knowledge transfer from the unlabeled modality. We provide extensive numerical simulations using four real-world datasets to show that TAP can provide statistically significant improvement in generalization across different domains and different neural network architectures, making use of seemingly unusable unlabel
&lt;/p&gt;</description></item><item><title>QLABGrad&#26159;&#19968;&#31181;&#26080;&#38656;&#36229;&#21442;&#25968;&#30340;&#23398;&#20064;&#29575;&#33258;&#36866;&#24212;&#26041;&#26696;&#65292;&#36890;&#36807;&#20248;&#21270;QLAB&#20989;&#25968;&#33258;&#21160;&#30830;&#23450;&#23398;&#20064;&#29575;&#65292;&#24182;&#22312;&#24179;&#28369;&#30340;Lipschitz&#26465;&#20214;&#19979;&#35777;&#26126;&#20102;&#20854;&#25910;&#25947;&#24615;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#22810;&#31181;&#26550;&#26500;&#21644;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#36234;&#12290;</title><link>https://arxiv.org/abs/2302.00252</link><description>&lt;p&gt;
QLABGrad: &#19968;&#31181;&#26080;&#38656;&#36229;&#21442;&#25968;&#19988;&#25910;&#25947;&#20445;&#35777;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
QLABGrad: a Hyperparameter-Free and Convergence-Guaranteed Scheme for Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.00252
&lt;/p&gt;
&lt;p&gt;
QLABGrad&#26159;&#19968;&#31181;&#26080;&#38656;&#36229;&#21442;&#25968;&#30340;&#23398;&#20064;&#29575;&#33258;&#36866;&#24212;&#26041;&#26696;&#65292;&#36890;&#36807;&#20248;&#21270;QLAB&#20989;&#25968;&#33258;&#21160;&#30830;&#23450;&#23398;&#20064;&#29575;&#65292;&#24182;&#22312;&#24179;&#28369;&#30340;Lipschitz&#26465;&#20214;&#19979;&#35777;&#26126;&#20102;&#20854;&#25910;&#25947;&#24615;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#22810;&#31181;&#26550;&#26500;&#21644;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#29575;&#26159;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#20013;&#20851;&#38190;&#30340;&#36229;&#21442;&#25968;&#65292;&#22240;&#20026;&#23427;&#20915;&#23450;&#20102;&#27169;&#22411;&#21442;&#25968;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#26356;&#26032;&#31243;&#24230;&#12290;&#28982;&#32780;&#65292;&#23398;&#20064;&#29575;&#30340;&#36873;&#25321;&#36890;&#24120;&#20381;&#36182;&#20110;&#32463;&#39564;&#21028;&#26029;&#65292;&#22312;&#27809;&#26377;&#36827;&#34892;&#22823;&#37327;&#23581;&#35797;&#21644;&#38169;&#35823;&#23454;&#39564;&#30340;&#24773;&#20917;&#19979;&#21487;&#33021;&#26080;&#27861;&#24471;&#21040;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;QLABGrad&#30340;&#26032;&#22411;&#23398;&#20064;&#29575;&#33258;&#36866;&#24212;&#26041;&#26696;&#12290;QLABGrad&#26080;&#38656;&#20219;&#20309;&#29992;&#25143;&#25351;&#23450;&#30340;&#36229;&#21442;&#25968;&#65292;&#36890;&#36807;&#20248;&#21270;&#22522;&#20110;&#20108;&#27425;&#25439;&#22833;&#36817;&#20284;(QLAB)&#20989;&#25968;&#26469;&#33258;&#21160;&#30830;&#23450;&#23398;&#20064;&#29575;&#65292;&#20165;&#38656;&#35201;&#36827;&#34892;&#19968;&#27425;&#39069;&#22806;&#30340;&#21069;&#21521;&#20256;&#25773;&#12290;&#25105;&#20204;&#22312;&#25439;&#22833;&#20989;&#25968;&#19978;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;QLABGrad&#30340;&#25910;&#25947;&#24615;&#20855;&#26377;&#24179;&#28369;&#30340;Lipschitz&#26465;&#20214;&#12290;&#22312;&#22810;&#31181;&#26550;&#26500;&#65288;&#21253;&#25324;MLP&#12289;CNN&#21644;ResNet&#65289;&#20197;&#21450;MNIST&#12289;CIFAR10&#21644;ImageNet&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;QLABGrad&#34920;&#29616;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.00252v2 Announce Type: replace  Abstract: The learning rate is a critical hyperparameter for deep learning tasks since it determines the extent to which the model parameters are updated during the learning course. However, the choice of learning rates typically depends on empirical judgment, which may not result in satisfactory outcomes without intensive try-and-error experiments. In this study, we propose a novel learning rate adaptation scheme called QLABGrad. Without any user-specified hyperparameter, QLABGrad automatically determines the learning rate by optimizing the Quadratic Loss Approximation-Based (QLAB) function for a given gradient descent direction, where only one extra forward propagation is required. We theoretically prove the convergence of QLABGrad with a smooth Lipschitz condition on the loss function. Experiment results on multiple architectures, including MLP, CNN, and ResNet, on MNIST, CIFAR10, and ImageNet datasets, demonstrate that QLABGrad outperforms
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#38543;&#26426;&#20877;&#29983;&#27169;&#22411;&#65292;&#29992;&#20110;&#35821;&#38899;&#22686;&#24378;&#21644;&#21435;&#28151;&#21709;&#65292;&#22635;&#34917;&#20102;&#39044;&#27979;&#24615;&#21644;&#29983;&#25104;&#24615;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#30340;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2212.11851</link><description>&lt;p&gt;
StoRM&#65306;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#38543;&#26426;&#20877;&#29983;&#27169;&#22411;&#29992;&#20110;&#35821;&#38899;&#22686;&#24378;&#21644;&#21435;&#28151;&#21709;
&lt;/p&gt;
&lt;p&gt;
StoRM: A Diffusion-based Stochastic Regeneration Model for Speech Enhancement and Dereverberation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2212.11851
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#38543;&#26426;&#20877;&#29983;&#27169;&#22411;&#65292;&#29992;&#20110;&#35821;&#38899;&#22686;&#24378;&#21644;&#21435;&#28151;&#21709;&#65292;&#22635;&#34917;&#20102;&#39044;&#27979;&#24615;&#21644;&#29983;&#25104;&#24615;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#22635;&#34917;&#35821;&#38899;&#22686;&#24378;&#20013;&#39044;&#27979;&#24615;&#21644;&#29983;&#25104;&#24615;&#26041;&#27861;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#24040;&#22823;&#33021;&#21147;&#12290;&#25105;&#20204;&#24050;&#32463;&#35777;&#26126;&#65292;&#23427;&#20204;&#29978;&#33267;&#22312;&#38750;&#21152;&#24615;&#27745;&#26579;&#31867;&#22411;&#25110;&#22312;&#35780;&#20272;&#19981;&#21305;&#37197;&#30340;&#26465;&#20214;&#19979;&#21487;&#33021;&#20248;&#20110;&#23427;&#20204;&#30340;&#39044;&#27979;&#24615;&#23545;&#24212;&#29289;&#12290;&#28982;&#32780;&#65292;&#25193;&#25955;&#27169;&#22411;&#23384;&#22312;&#30528;&#36739;&#39640;&#30340;&#35745;&#31639;&#36127;&#25285;&#65292;&#20027;&#35201;&#26159;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#20026;&#27599;&#20010;&#21453;&#21521;&#20256;&#25773;&#27493;&#39588;&#36816;&#34892;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#32780;&#39044;&#27979;&#24615;&#26041;&#27861;&#21482;&#38656;&#35201;&#19968;&#27425;&#36941;&#21382;&#12290;&#30001;&#20110;&#25193;&#25955;&#27169;&#22411;&#26159;&#29983;&#25104;&#26041;&#27861;&#65292;&#23427;&#20204;&#22312;&#19981;&#21033;&#26465;&#20214;&#19979;&#21487;&#33021;&#36824;&#20250;&#20135;&#29983;&#21457;&#22768;&#21644;&#21628;&#21560;&#30340;&#20154;&#24037;&#30165;&#36857;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22312;&#36825;&#31181;&#22256;&#38590;&#24773;&#20917;&#19979;&#65292;&#39044;&#27979;&#27169;&#22411;&#36890;&#24120;&#19981;&#20250;&#20135;&#29983;&#36825;&#26679;&#30340;&#20154;&#24037;&#30165;&#36857;&#65292;&#20294;&#20542;&#21521;&#20110;&#25197;&#26354;&#30446;&#26631;&#35821;&#38899;&#65292;&#20174;&#32780;&#38477;&#20302;&#35821;&#38899;&#36136;&#37327;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38543;&#26426;&#20877;&#29983;&#26041;&#27861;&#65292;&#20854;&#20013;&#30001;&#39044;&#27979;&#27169;&#22411;&#32473;&#20986;&#30340;&#20272;&#35745;&#34987;&#25552;&#20379;&#20026;
&lt;/p&gt;
&lt;p&gt;
arXiv:2212.11851v2 Announce Type: replace-cross  Abstract: Diffusion models have shown a great ability at bridging the performance gap between predictive and generative approaches for speech enhancement. We have shown that they may even outperform their predictive counterparts for non-additive corruption types or when they are evaluated on mismatched conditions. However, diffusion models suffer from a high computational burden, mainly as they require to run a neural network for each reverse diffusion step, whereas predictive approaches only require one pass. As diffusion models are generative approaches they may also produce vocalizing and breathing artifacts in adverse conditions. In comparison, in such difficult scenarios, predictive models typically do not produce such artifacts but tend to distort the target speech instead, thereby degrading the speech quality. In this work, we present a stochastic regeneration approach where an estimate given by a predictive model is provided as a
&lt;/p&gt;</description></item><item><title>APOLLO&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#25968;&#23383;&#24863;&#30693;&#30340;&#36127;&#37319;&#26679;&#31574;&#30053;&#21644;&#22522;&#20110;&#19968;&#33268;&#24615;&#30340;&#24378;&#21270;&#23398;&#20064;&#65292;&#25552;&#39640;&#20102;&#38271;&#31687;&#25968;&#23383;&#25512;&#29702;&#26694;&#26550;&#30340;&#20934;&#30830;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;</title><link>https://arxiv.org/abs/2212.07249</link><description>&lt;p&gt;
APOLLO: &#19968;&#31181;&#29992;&#20110;&#38271;&#31687;&#25968;&#23383;&#25512;&#29702;&#30340;&#20248;&#21270;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
APOLLO: An Optimized Training Approach for Long-form Numerical Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2212.07249
&lt;/p&gt;
&lt;p&gt;
APOLLO&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#25968;&#23383;&#24863;&#30693;&#30340;&#36127;&#37319;&#26679;&#31574;&#30053;&#21644;&#22522;&#20110;&#19968;&#33268;&#24615;&#30340;&#24378;&#21270;&#23398;&#20064;&#65292;&#25552;&#39640;&#20102;&#38271;&#31687;&#25968;&#23383;&#25512;&#29702;&#26694;&#26550;&#30340;&#20934;&#30830;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#36130;&#21153;&#20998;&#26512;&#20013;&#38271;&#31687;&#25968;&#23383;&#25512;&#29702;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#35757;&#32451;&#26041;&#27861;&#65292;&#26088;&#22312;&#29983;&#25104;&#19968;&#20010;&#25512;&#29702;&#31243;&#24207;&#20197;&#35745;&#31639;&#32473;&#23450;&#38382;&#39064;&#30340;&#27491;&#30830;&#31572;&#26696;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;APOLLO&#26469;&#25913;&#21892;&#38271;&#31687;&#25968;&#23383;&#25512;&#29702;&#26694;&#26550;&#65292;&#38024;&#23545;&#30456;&#20851;&#24615;&#36873;&#25321;&#22120;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#25968;&#23383;&#24863;&#30693;&#30340;&#36127;&#37319;&#26679;&#31574;&#30053;&#65292;&#20351;&#20854;&#33021;&#22815;&#26356;&#21152;&#36776;&#21035;&#20851;&#38190;&#30340;&#25968;&#23383;&#20107;&#23454;&#12290;&#32780;&#23545;&#20110;&#29983;&#25104;&#22120;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#22522;&#20110;&#19968;&#33268;&#24615;&#30340;&#24378;&#21270;&#23398;&#20064;&#21644;&#30446;&#26631;&#31243;&#24207;&#22686;&#24378;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2212.07249v3 Announce Type: replace  Abstract: Long-form numerical reasoning in financial analysis aims to generate a reasoning program to calculate the correct answer for a given question. Previous work followed a retriever-generator framework, where the retriever selects key facts from a long-form document, and the generator generates a reasoning program based on retrieved facts. However, they treated all facts equally without considering the different contributions of facts with and without numbers. Meanwhile, the program consistency were ignored under supervised training, resulting in lower training accuracy and diversity. To solve these problems, we proposed APOLLO to improve the long-form numerical reasoning framework. For the retriever, we adopt a number-aware negative sampling strategy to enable the retriever to be more discriminative on key numerical facts. For the generator, we design consistency-based reinforcement learning and target program augmentation strategy base
&lt;/p&gt;</description></item><item><title>&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#65288;DCAI&#65289;&#24378;&#35843;&#25968;&#25454;&#30340;&#36136;&#37327;&#21644;&#21160;&#24577;&#24615;&#65292;&#25552;&#20986;&#20845;&#39033;&#25351;&#23548;&#21407;&#21017;&#65292;&#24182;&#20026;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#25351;&#26126;&#20102;&#36947;&#36335;&#12290;</title><link>https://arxiv.org/abs/2211.14611</link><description>&lt;p&gt;
&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#65288;DCAI&#65289;&#30340;&#21407;&#21017;
&lt;/p&gt;
&lt;p&gt;
The Principles of Data-Centric AI (DCAI)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2211.14611
&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#65288;DCAI&#65289;&#24378;&#35843;&#25968;&#25454;&#30340;&#36136;&#37327;&#21644;&#21160;&#24577;&#24615;&#65292;&#25552;&#20986;&#20845;&#39033;&#25351;&#23548;&#21407;&#21017;&#65292;&#24182;&#20026;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#25351;&#26126;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#23545;&#20110;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#31995;&#32479;&#30340;&#23398;&#20064;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#36825;&#20123;&#31995;&#32479;&#20027;&#35201;&#20197;&#27169;&#22411;&#20026;&#20013;&#24515;&#65292;&#20197;&#25968;&#25454;&#36136;&#37327;&#20026;&#20195;&#20215;&#12290;&#25968;&#25454;&#36136;&#37327;&#38382;&#39064;&#24433;&#21709;&#20102;AI&#31995;&#32479;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#22312;&#19979;&#28216;&#37096;&#32626;&#21644;&#23454;&#38469;&#24212;&#29992;&#20013;&#12290;&#20316;&#20026;&#19968;&#20010;&#26032;&#20852;&#27010;&#24565;&#65292;&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#65288;DCAI&#65289;&#36890;&#36807;&#36845;&#20195;&#21644;&#31995;&#32479;&#21270;&#30340;&#26041;&#27861;&#23558;&#25968;&#25454;&#12289;&#20854;&#36136;&#37327;&#21644;&#21160;&#24577;&#24615;&#32622;&#20110;AI&#31995;&#32479;&#32771;&#34385;&#30340;&#21069;&#27839;&#12290;&#20316;&#20026;&#39318;&#27425;&#27010;&#36848;&#20043;&#19968;&#65292;&#26412;&#25991;&#27719;&#38598;&#20102;&#25968;&#25454;&#20013;&#24515;&#30340;&#35270;&#35282;&#21644;&#27010;&#24565;&#65292;&#21246;&#21202;&#20102;DCAI&#30340;&#22522;&#30784;&#12290;&#23427;&#29305;&#21035;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#21046;&#23450;&#20102;&#20845;&#39033;&#25351;&#23548;&#21407;&#21017;&#65292;&#24182;&#20026;DCAI&#30340;&#26410;&#26469;&#21457;&#23637;&#25552;&#20379;&#20102;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2211.14611v2 Announce Type: replace-cross  Abstract: Data is a crucial infrastructure to how artificial intelligence (AI) systems learn. However, these systems to date have been largely model-centric, putting a premium on the model at the expense of the data quality. Data quality issues beset the performance of AI systems, particularly in downstream deployments and in real-world applications. Data-centric AI (DCAI) as an emerging concept brings data, its quality and its dynamism to the forefront in considerations of AI systems through an iterative and systematic approach. As one of the first overviews, this article brings together data-centric perspectives and concepts to outline the foundations of DCAI. It specifically formulates six guiding principles for researchers and practitioners and gives direction for future advancement of DCAI.
&lt;/p&gt;</description></item><item><title>SATformer&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#65292;&#37319;&#29992;&#23545;&#19981;&#21487;&#28385;&#36275;&#24615;&#36827;&#34892;&#24314;&#27169;&#30340;&#26041;&#24335;&#65292;&#20197;&#35782;&#21035;&#19981;&#21487;&#28385;&#36275;&#30340;&#23376;&#38382;&#39064;&#65292;&#24471;&#21040;&#20102;&#20248;&#20110;NeuroSAT&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2209.00953</link><description>&lt;p&gt;
SATformer: &#22522;&#20110;Transformer&#30340;UNSAT&#26680;&#24515;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SATformer: Transformer-Based UNSAT Core Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2209.00953
&lt;/p&gt;
&lt;p&gt;
SATformer&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#65292;&#37319;&#29992;&#23545;&#19981;&#21487;&#28385;&#36275;&#24615;&#36827;&#34892;&#24314;&#27169;&#30340;&#26041;&#24335;&#65292;&#20197;&#35782;&#21035;&#19981;&#21487;&#28385;&#36275;&#30340;&#23376;&#38382;&#39064;&#65292;&#24471;&#21040;&#20102;&#20248;&#20110;NeuroSAT&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;SATformer&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#24067;&#23572;&#21487;&#28385;&#36275;&#24615;&#65288;SAT&#65289;&#38382;&#39064;&#30340;&#26032;&#22411;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#12290; SATformer&#24182;&#38750;&#30452;&#25509;&#35299;&#20915;&#38382;&#39064;&#65292;&#32780;&#26159;&#20174;&#30456;&#21453;&#30340;&#26041;&#21521;&#20837;&#25163;&#65292;&#30528;&#37325;&#20110;&#19981;&#21487;&#28385;&#36275;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#23427;&#36890;&#36807;&#27169;&#25311;&#23376;&#21477;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#26469;&#35782;&#21035;&#20219;&#20309;&#19981;&#21487;&#28385;&#36275;&#30340;&#23376;&#38382;&#39064;&#12290;&#25105;&#20204;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#23558;&#23376;&#21477;&#36716;&#25442;&#20026;&#23376;&#21477;&#23884;&#20837;&#65292;&#24182;&#37319;&#29992;&#20998;&#23618;Transformer&#27169;&#22411;&#26469;&#29702;&#35299;&#23376;&#21477;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290; SATformer&#36890;&#36807;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#65292;&#20351;&#29992;&#21333;&#27604;&#29305;&#21487;&#28385;&#36275;&#24615;&#32467;&#26524;&#20197;&#21450;&#26368;&#23567;&#19981;&#21487;&#28385;&#36275;&#26680;&#24515;&#65288;MUC&#65289;&#20316;&#20026;&#23376;&#21477;&#30417;&#30563;&#26469;&#22788;&#29702;UNSAT&#38382;&#39064;&#12290;&#20316;&#20026;&#31471;&#21040;&#31471;&#23398;&#20064;&#30340;&#21487;&#28385;&#36275;&#24615;&#20998;&#31867;&#22120;&#65292;SATformer&#30340;&#24615;&#33021;&#26174;&#33879;&#36229;&#36234;&#20102;NeuroSAT&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;SATformer&#20570;&#20986;&#30340;&#23376;&#21477;&#39044;&#27979;&#38598;&#25104;&#21040;&#29616;&#20195;&#21551;&#21457;&#24335;SAT&#27714;&#35299;&#22120;&#20013;&#65292;&#24182;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2209.00953v2 Announce Type: replace  Abstract: This paper introduces SATformer, a novel Transformer-based approach for the Boolean Satisfiability (SAT) problem. Rather than solving the problem directly, SATformer approaches the problem from the opposite direction by focusing on unsatisfiability. Specifically, it models clause interactions to identify any unsatisfiable sub-problems. Using a graph neural network, we convert clauses into clause embeddings and employ a hierarchical Transformer-based model to understand clause correlation. SATformer is trained through a multi-task learning approach, using the single-bit satisfiability result and the minimal unsatisfiable core (MUC) for UNSAT problems as clause supervision. As an end-to-end learning-based satisfiability classifier, the performance of SATformer surpasses that of NeuroSAT significantly. Furthermore, we integrate the clause predictions made by SATformer into modern heuristic-based SAT solvers and validate our approach wit
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21333;&#35843;&#21464;&#20998;&#19981;&#31561;&#24335;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#24555;&#36895;&#25910;&#25947;&#24182;&#22312;&#29305;&#23450;&#24773;&#20917;&#19979;&#25552;&#20379;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2202.08876</link><description>&lt;p&gt;
&#20351;&#29992;&#21333;&#35843;&#21464;&#20998;&#19981;&#31561;&#24335;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#21478;&#19968;&#31181;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An alternative approach to train neural networks using monotone variational inequality
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2202.08876
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21333;&#35843;&#21464;&#20998;&#19981;&#31561;&#24335;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#24555;&#36895;&#25910;&#25947;&#24182;&#22312;&#29305;&#23450;&#24773;&#20917;&#19979;&#25552;&#20379;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21333;&#35843;&#30690;&#37327;&#22330;&#30340;&#26367;&#20195;&#26041;&#27861;&#26469;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#65292;&#36825;&#20010;&#24819;&#27861;&#21463;&#21040;Juditsky&#21644;Nemirovski&#30340;&#24320;&#21019;&#24615;&#24037;&#20316;&#30340;&#21551;&#21457;&#65292;&#26368;&#21021;&#26159;&#20026;&#20102;&#35299;&#20915;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#65288;GLM&#65289;&#30340;&#21442;&#25968;&#20272;&#35745;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#21407;&#22987;&#38750;&#20984;&#38382;&#39064;&#31616;&#21270;&#20026;&#35299;&#20915;&#21333;&#35843;&#21464;&#20998;&#19981;&#31561;&#24335;&#65288;VI&#65289;&#30340;&#20984;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23548;&#33268;&#20102;&#35745;&#31639;&#25928;&#29575;&#39640;&#19988;&#22312;&#26576;&#20123;&#29305;&#27530;&#24773;&#20917;&#19979;&#25910;&#25947;&#24555;&#36895;&#24182;&#25552;&#20379;&#20102;&#20445;&#35777;&#65292;&#20363;&#22914;&#35757;&#32451;&#21333;&#23618;&#31070;&#32463;&#32593;&#32476;&#25110;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26368;&#21518;&#19968;&#23618;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#26356;&#39640;&#25928;&#22320;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21516;&#26102;&#20923;&#32467;&#24213;&#23618;&#65292;&#36825;&#26159;&#37096;&#32626;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65288;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLM&#65289;&#30340;&#37325;&#35201;&#27493;&#39588;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23427;&#22312;&#35757;&#32451;&#20840;&#36830;&#25509;&#65288;FC&#65289;&#31070;&#32463;&#32593;&#32476;&#12289;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#38754;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2202.08876v4 Announce Type: replace-cross  Abstract: We propose an alternative approach to neural network training using the monotone vector field, an idea inspired by the seminal work of Juditsky and Nemirovski [Juditsky &amp; Nemirovsky, 2019] developed originally to solve parameter estimation problems for generalized linear models (GLM) by reducing the original non-convex problem to a convex problem of solving a monotone variational inequality (VI). Our approach leads to computationally efficient procedures that converge fast and offer guarantee in some special cases, such as training a single-layer neural network or fine-tuning the last layer of the pre-trained model. Our approach can be used for more efficient fine-tuning of a pre-trained model while freezing the bottom layers, an essential step for deploying many machine learning models such as large language models (LLM). We demonstrate its applicability in training fully-connected (FC) neural networks, graph neural networks (
&lt;/p&gt;</description></item><item><title>&#37325;&#26032;&#23457;&#35270;&#20102;&#20351;&#29992;&#20999;&#27604;&#38634;&#22827;&#22810;&#39033;&#24335;&#36924;&#36817;&#35889;&#22270;&#21367;&#31215;&#30340;&#38382;&#39064;&#65292;&#21457;&#29616;ChebNet&#24615;&#33021;&#36739;&#24046;&#20027;&#35201;&#26159;&#30001;&#20110;&#20854;&#23398;&#20064;&#21040;&#30340;&#38750;&#27861;&#31995;&#25968;&#36817;&#20284;&#35299;&#26512;&#28388;&#27874;&#22120;&#20989;&#25968;</title><link>https://arxiv.org/abs/2202.03580</link><description>&lt;p&gt;
&#20351;&#29992;&#20999;&#27604;&#38634;&#22827;&#36924;&#36817;&#30340;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#37325;&#26032;&#23457;&#35270;
&lt;/p&gt;
&lt;p&gt;
Convolutional Neural Networks on Graphs with Chebyshev Approximation, Revisited
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2202.03580
&lt;/p&gt;
&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#20102;&#20351;&#29992;&#20999;&#27604;&#38634;&#22827;&#22810;&#39033;&#24335;&#36924;&#36817;&#35889;&#22270;&#21367;&#31215;&#30340;&#38382;&#39064;&#65292;&#21457;&#29616;ChebNet&#24615;&#33021;&#36739;&#24046;&#20027;&#35201;&#26159;&#30001;&#20110;&#20854;&#23398;&#20064;&#21040;&#30340;&#38750;&#27861;&#31995;&#25968;&#36817;&#20284;&#35299;&#26512;&#28388;&#27874;&#22120;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22270;&#23398;&#20064;&#20013;&#65292;&#35774;&#35745;&#35889;&#21367;&#31215;&#32593;&#32476;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;ChebNet&#26159;&#26089;&#26399;&#23581;&#35797;&#20043;&#19968;&#65292;&#23427;&#20351;&#29992;&#20999;&#27604;&#38634;&#22827;&#22810;&#39033;&#24335;&#36817;&#20284;&#35889;&#22270;&#21367;&#31215;&#12290;GCN&#31616;&#21270;&#20102;ChebNet&#65292;&#20165;&#21033;&#29992;&#21069;&#20004;&#20010;&#20999;&#27604;&#38634;&#22827;&#22810;&#39033;&#24335;&#65292;&#21516;&#26102;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#24615;&#33021;&#20248;&#20110;&#20854;&#12290;GPR-GNN&#21644;BernNet&#34920;&#26126;&#65292;&#21333;&#39033;&#24335;&#21644;&#20271;&#24681;&#26031;&#22374;&#22522;&#20063;&#22312;&#23398;&#20064;&#35889;&#22270;&#21367;&#31215;&#26041;&#38754;&#20248;&#20110;&#20999;&#27604;&#38634;&#22827;&#22522;&#12290;&#36825;&#26679;&#30340;&#32467;&#35770;&#22312;&#36924;&#36817;&#29702;&#35770;&#39046;&#22495;&#26159;&#21453;&#30452;&#35273;&#30340;&#65292;&#36924;&#36817;&#20989;&#25968;&#26102;&#20999;&#27604;&#38634;&#22827;&#22810;&#39033;&#24335;&#23454;&#29616;&#20102;&#26368;&#20339;&#25910;&#25947;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2202.03580v5 Announce Type: replace-cross  Abstract: Designing spectral convolutional networks is a challenging problem in graph learning. ChebNet, one of the early attempts, approximates the spectral graph convolutions using Chebyshev polynomials. GCN simplifies ChebNet by utilizing only the first two Chebyshev polynomials while still outperforming it on real-world datasets. GPR-GNN and BernNet demonstrate that the Monomial and Bernstein bases also outperform the Chebyshev basis in terms of learning the spectral graph convolutions. Such conclusions are counter-intuitive in the field of approximation theory, where it is established that the Chebyshev polynomial achieves the optimum convergent rate for approximating a function.   In this paper, we revisit the problem of approximating the spectral graph convolutions with Chebyshev polynomials. We show that ChebNet's inferior performance is primarily due to illegal coefficients learnt by ChebNet approximating analytic filter functio
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#29992;&#20110;&#19981;&#30456;&#20132;&#27880;&#35299;&#30340;&#23545;&#27604;&#22238;&#24402;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#22810;&#20010;&#26631;&#27880;&#32773;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#21644;&#20559;&#35265;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2112.15411</link><description>&lt;p&gt;
&#22810;&#28304;&#27880;&#35299;&#30340;&#19981;&#30456;&#20132;&#23545;&#27604;&#22238;&#24402;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Disjoint Contrastive Regression Learning for Multi-Sourced Annotations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2112.15411
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#29992;&#20110;&#19981;&#30456;&#20132;&#27880;&#35299;&#30340;&#23545;&#27604;&#22238;&#24402;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#22810;&#20010;&#26631;&#27880;&#32773;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#21644;&#20559;&#35265;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21457;&#23637;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#20123;&#25968;&#25454;&#38598;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#27880;&#35299;&#24037;&#20316;&#65292;&#36825;&#26159;&#38750;&#24120;&#32791;&#26102;&#19988;&#26114;&#36149;&#30340;&#12290;&#20026;&#20102;&#21152;&#36895;&#27880;&#35299;&#36807;&#31243;&#65292;&#21487;&#20197;&#38599;&#29992;&#22810;&#20010;&#26631;&#27880;&#32773;&#20026;&#25968;&#25454;&#30340;&#19981;&#21516;&#23376;&#38598;&#36827;&#34892;&#26631;&#35760;&#12290;&#28982;&#32780;&#65292;&#19981;&#21516;&#26631;&#27880;&#32773;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#21644;&#20559;&#35265;&#23545;&#27169;&#22411;&#35757;&#32451;&#26377;&#23475;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#23450;&#24615;&#21644;&#20027;&#35266;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#22238;&#24402;&#26694;&#26550;&#26469;&#35299;&#20915;&#19981;&#30456;&#20132;&#27880;&#35299;&#38382;&#39064;&#65292;&#20854;&#20013;&#27599;&#20010;&#26679;&#26412;&#20165;&#30001;&#19968;&#20010;&#26631;&#27880;&#32773;&#26631;&#35760;&#65292;&#22810;&#20010;&#26631;&#27880;&#32773;&#22312;&#25968;&#25454;&#30340;&#19981;&#30456;&#20132;&#23376;&#38598;&#19978;&#24037;&#20316;&#12290;&#20026;&#20102;&#32771;&#34385;&#26631;&#27880;&#32773;&#20869;&#19968;&#33268;&#24615;&#21644;&#26631;&#27880;&#32773;&#38388;&#19981;&#19968;&#33268;&#24615;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#20004;&#31181;&#31574;&#30053;&#12290;&#39318;&#20808;&#65292;&#24212;&#29992;&#22522;&#20110;&#23545;&#27604;&#30340;&#25439;&#22833;&#26469;&#23398;&#20064;&#21516;&#19968;&#26631;&#27880;&#32773;&#30340;&#19981;&#21516;&#26679;&#26412;&#20043;&#38388;&#30340;&#30456;&#23545;&#25490;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2112.15411v2 Announce Type: replace  Abstract: Large-scale datasets are important for the development of deep learning models. Such datasets usually require a heavy workload of annotations, which are extremely time-consuming and expensive. To accelerate the annotation procedure, multiple annotators may be employed to label different subsets of the data. However, the inconsistency and bias among different annotators are harmful to the model training, especially for qualitative and subjective tasks.To address this challenge, in this paper, we propose a novel contrastive regression framework to address the disjoint annotations problem, where each sample is labeled by only one annotator and multiple annotators work on disjoint subsets of the data. To take account of both the intra-annotator consistency and inter-annotator inconsistency, two strategies are employed.Firstly, a contrastive-based loss is applied to learn the relative ranking among different samples of the same annotator,
&lt;/p&gt;</description></item><item><title>DACZSL&#25552;&#20986;&#20102;&#38754;&#21521;&#22495;&#24863;&#30693;&#30340;&#25345;&#32493;&#38646;&#26679;&#26412;&#23398;&#20064;&#20219;&#21153;&#65292;&#24341;&#20837;&#20102;&#22495;&#19981;&#21464;&#32593;&#32476;(DIN)&#65292;&#19981;&#26029;&#23398;&#20064;&#20840;&#23616;&#20849;&#20139;&#32593;&#32476;&#29992;&#20110;&#39046;&#22495;&#19981;&#21464;&#21644;&#20219;&#21153;&#19981;&#21464;&#29305;&#24449;&#65292;&#20197;&#21450;&#20026;&#27599;&#20010;&#20219;&#21153;&#25552;&#20379;&#19987;&#38376;&#30340;&#31169;&#26377;&#32593;&#32476;&#29992;&#20110;&#29305;&#23450;&#20219;&#21153;&#30340;&#29305;&#24449;</title><link>https://arxiv.org/abs/2112.12989</link><description>&lt;p&gt;
&#38754;&#21521;&#22495;&#24863;&#30693;&#30340;&#25345;&#32493;&#38646;&#26679;&#26412;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Domain-Aware Continual Zero-Shot Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2112.12989
&lt;/p&gt;
&lt;p&gt;
DACZSL&#25552;&#20986;&#20102;&#38754;&#21521;&#22495;&#24863;&#30693;&#30340;&#25345;&#32493;&#38646;&#26679;&#26412;&#23398;&#20064;&#20219;&#21153;&#65292;&#24341;&#20837;&#20102;&#22495;&#19981;&#21464;&#32593;&#32476;(DIN)&#65292;&#19981;&#26029;&#23398;&#20064;&#20840;&#23616;&#20849;&#20139;&#32593;&#32476;&#29992;&#20110;&#39046;&#22495;&#19981;&#21464;&#21644;&#20219;&#21153;&#19981;&#21464;&#29305;&#24449;&#65292;&#20197;&#21450;&#20026;&#27599;&#20010;&#20219;&#21153;&#25552;&#20379;&#19987;&#38376;&#30340;&#31169;&#26377;&#32593;&#32476;&#29992;&#20110;&#29305;&#23450;&#20219;&#21153;&#30340;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#35270;&#35273;&#31995;&#32479;&#22312;&#33258;&#28982;&#31185;&#23398;&#30740;&#31350;&#20013;&#20855;&#26377;&#24191;&#27867;&#30340;&#28508;&#22312;&#24212;&#29992;&#65292;&#22914;&#24110;&#21161;&#21457;&#29616;&#29289;&#31181;&#65292;&#30417;&#27979;&#37326;&#29983;&#21160;&#29289;&#31561;&#12290;&#28982;&#32780;&#65292;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#35270;&#35273;&#20219;&#21153;&#21487;&#33021;&#20250;&#32463;&#21382;&#29615;&#22659;&#26465;&#20214;&#30340;&#21464;&#21270;&#65292;&#23548;&#33268;&#25429;&#25417;&#22270;&#20687;&#21576;&#29616;&#26041;&#24335;&#30340;&#21464;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#38754;&#21521;&#22495;&#24863;&#30693;&#30340;&#25345;&#32493;&#38646;&#26679;&#26412;&#23398;&#20064;&#65288;DACZSL&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#25345;&#32493;&#35782;&#21035;&#19981;&#26029;&#21464;&#21270;&#22495;&#20013;&#26410;&#35265;&#31867;&#21035;&#22270;&#20687;&#30340;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22495;&#19981;&#21464;&#32593;&#32476;&#65288;DIN&#65289;&#65292;&#29992;&#20110;&#23398;&#20064;&#36866;&#24212;&#21464;&#21270;&#22495;&#30340;&#20998;&#35299;&#29305;&#24449;&#21644;&#25913;&#36827;&#30340;&#25991;&#26412;&#34920;&#31034;&#26410;&#35265;&#31867;&#21035;&#12290;DIN&#19981;&#26029;&#23398;&#20064;&#20840;&#23616;&#20849;&#20139;&#32593;&#32476;&#20197;&#33719;&#21462;&#39046;&#22495;&#19981;&#21464;&#21644;&#20219;&#21153;&#19981;&#21464;&#29305;&#24449;&#65292;&#24182;&#20026;&#27599;&#20010;&#20219;&#21153;&#25552;&#20379;&#19987;&#38376;&#30340;&#31169;&#26377;&#32593;&#32476;&#29992;&#20110;&#29305;&#23450;&#20219;&#21153;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2112.12989v3 Announce Type: replace-cross  Abstract: Modern visual systems have a wide range of potential applications in vision tasks for natural science research, such as aiding in species discovery, monitoring animals in the wild, and so on. However, real-world vision tasks may experience changes in environmental conditions, leading to shifts in how captured images are presented. To address this issue, we introduce Domain-Aware Continual Zero-Shot Learning (DACZSL), a task to recognize images of unseen categories in continuously changing domains. Accordingly, we propose a Domain-Invariant Network (DIN) to learn factorized features for shifting domains and improved textual representation for unseen classes. DIN continually learns a global shared network for domain-invariant and task-invariant features, and per-task private networks for task-specific features. Furthermore, we enhance the dual network with class-wise learnable prompts to improve class-level text representation, t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Outcome Adaptive Elastic Net&#65288;OAENet&#65289;&#30340;&#20004;&#38454;&#27573;&#29305;&#24449;&#36873;&#25321;&#25216;&#26415;&#65292;&#29992;&#20110;&#22312;&#39640;&#32500;&#35266;&#23519;&#25968;&#25454;&#20013;&#36827;&#34892;&#31283;&#20581;&#30340;&#22240;&#26524;&#25512;&#26029;&#20915;&#31574;&#12290;</title><link>https://arxiv.org/abs/2111.13800</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#22312;&#39640;&#32500;&#35266;&#23519;&#25968;&#25454;&#20013;&#31283;&#20581;&#35780;&#20272;&#27835;&#30103;&#25928;&#26524;&#30340;&#20004;&#38454;&#27573;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Two-Stage Feature Selection Approach for Robust Evaluation of Treatment Effects in High-Dimensional Observational Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2111.13800
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Outcome Adaptive Elastic Net&#65288;OAENet&#65289;&#30340;&#20004;&#38454;&#27573;&#29305;&#24449;&#36873;&#25321;&#25216;&#26415;&#65292;&#29992;&#20110;&#22312;&#39640;&#32500;&#35266;&#23519;&#25968;&#25454;&#20013;&#36827;&#34892;&#31283;&#20581;&#30340;&#22240;&#26524;&#25512;&#26029;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#23545;&#29031;&#35797;&#39564;&#65288;RCT&#65289;&#34987;&#35748;&#20026;&#26159;&#35780;&#20272;&#20219;&#20309;&#24178;&#39044;&#25110;&#27835;&#30103;&#25928;&#26524;&#30340;&#40644;&#37329;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20262;&#29702;&#12289;&#32463;&#27982;&#21644;&#27861;&#24459;&#32771;&#34385;&#65292;&#20854;&#21487;&#34892;&#24615;&#32463;&#24120;&#21463;&#38459;&#65292;&#20351;&#24471;&#35266;&#23519;&#25968;&#25454;&#25104;&#20026;&#32472;&#21046;&#22240;&#26524;&#32467;&#35770;&#30340;&#23453;&#36149;&#26367;&#20195;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21307;&#30103;&#35266;&#23519;&#25968;&#25454;&#20855;&#26377;&#39640;&#32500;&#24615;&#65292;&#36825;&#24102;&#26469;&#20102;&#22256;&#38590;&#25361;&#25112;&#65292;&#38656;&#35201;&#20180;&#32454;&#32771;&#34385;&#20197;&#30830;&#20445;&#26080;&#20559;&#12289;&#21487;&#38752;&#21644;&#31283;&#20581;&#30340;&#22240;&#26524;&#25512;&#26029;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Outcome Adaptive Elastic Net&#65288;OAENet&#65289;&#30340;&#26032;&#39062;&#20004;&#38454;&#27573;&#29305;&#24449;&#36873;&#25321;&#25216;&#26415;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#20351;&#29992;&#21305;&#37197;&#25216;&#26415;&#20570;&#20986;&#31283;&#20581;&#30340;&#22240;&#26524;&#25512;&#26029;&#20915;&#31574;&#12290;OAENet&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#20855;&#26377;&#20960;&#20010;&#20851;&#38190;&#20248;&#21183;&#65306;&#22312;&#30456;&#20851;&#21644;&#39640;&#32500;&#25968;&#25454;&#19978;&#34920;&#29616;&#20248;&#36234;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#20855;&#26377;&#36873;&#25321;&#29305;&#23450;&#21464;&#37327;&#38598;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2111.13800v2 Announce Type: replace-cross  Abstract: A Randomized Control Trial (RCT) is considered as the gold standard for evaluating the effect of any intervention or treatment. However, its feasibility is often hindered by ethical, economical, and legal considerations, making observational data a valuable alternative for drawing causal conclusions. Nevertheless, healthcare observational data presents a difficult challenge due to its high dimensionality, requiring careful consideration to ensure unbiased, reliable, and robust causal inferences. To overcome this challenge, in this study, we propose a novel two-stage feature selection technique called, Outcome Adaptive Elastic Net (OAENet), explicitly designed for making robust causal inference decisions using matching techniques. OAENet offers several key advantages over existing methods: superior performance on correlated and high-dimensional data compared to the existing methods and the ability to select specific sets of vari
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#29616;&#20195;GPU&#19978;&#30340;&#39640;&#25928;&#22788;&#29702;&#25805;&#20316;&#65292;&#23558;&#38543;&#26426;&#21270;&#20998;&#35299;&#38382;&#39064;&#37325;&#26032;&#21046;&#23450;&#65292;&#32467;&#21512;&#24555;&#36895;&#30697;&#38453;&#20056;&#27861;&#25805;&#20316;&#21644;&#38543;&#26426;&#25968;&#21457;&#29983;&#22120;&#65292;&#20805;&#20998;&#21457;&#25381;GPU&#24182;&#34892;&#22788;&#29702;&#30340;&#28508;&#21147;&#65292;&#20943;&#23569;&#35745;&#31639;&#30697;&#38453;&#20998;&#35299;&#30340;&#35745;&#31639;&#36127;&#25285;&#12290;</title><link>https://arxiv.org/abs/2110.03423</link><description>&lt;p&gt;
&#39640;&#25928;GPU&#23454;&#29616;&#38543;&#26426;&#21270;SVD&#21450;&#20854;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Efficient GPU implementation of randomized SVD and its applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2110.03423
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#29616;&#20195;GPU&#19978;&#30340;&#39640;&#25928;&#22788;&#29702;&#25805;&#20316;&#65292;&#23558;&#38543;&#26426;&#21270;&#20998;&#35299;&#38382;&#39064;&#37325;&#26032;&#21046;&#23450;&#65292;&#32467;&#21512;&#24555;&#36895;&#30697;&#38453;&#20056;&#27861;&#25805;&#20316;&#21644;&#38543;&#26426;&#25968;&#21457;&#29983;&#22120;&#65292;&#20805;&#20998;&#21457;&#25381;GPU&#24182;&#34892;&#22788;&#29702;&#30340;&#28508;&#21147;&#65292;&#20943;&#23569;&#35745;&#31639;&#30697;&#38453;&#20998;&#35299;&#30340;&#35745;&#31639;&#36127;&#25285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30697;&#38453;&#20998;&#35299;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#24212;&#29992;&#24191;&#27867;&#65292;&#21253;&#25324;&#22312;&#38477;&#32500;&#12289;&#25968;&#25454;&#21387;&#32553;&#21644;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#20013;&#30340;&#24212;&#29992;&#12290;&#20256;&#32479;&#30340;&#30697;&#38453;&#20998;&#35299;&#26041;&#27861;&#20855;&#26377;&#22810;&#39033;&#24335;&#22797;&#26434;&#24230;&#65292;&#36825;&#26174;&#33879;&#22686;&#21152;&#20102;&#23427;&#20204;&#30340;&#35745;&#31639;&#25104;&#26412;&#21644;&#26102;&#38388;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#29616;&#20195;&#22270;&#24418;&#22788;&#29702;&#21333;&#20803;&#65288;GPU&#65289;&#19978;&#21487;&#24182;&#34892;&#36816;&#34892;&#30340;&#39640;&#25928;&#22788;&#29702;&#25805;&#20316;&#65292;&#36825;&#26159;&#28145;&#24230;&#23398;&#20064;&#31561;&#39046;&#22495;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#35745;&#31639;&#26550;&#26500;&#65292;&#20197;&#20943;&#23569;&#35745;&#31639;&#30697;&#38453;&#20998;&#35299;&#30340;&#35745;&#31639;&#36127;&#25285;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37325;&#26032;&#21046;&#23450;&#20102;&#38543;&#26426;&#21270;&#20998;&#35299;&#38382;&#39064;&#65292;&#20197;&#21253;&#21547;&#24555;&#36895;&#30697;&#38453;&#20056;&#27861;&#25805;&#20316;&#65288;BLAS-3&#65289;&#20316;&#20026;&#22522;&#26412;&#26500;&#24314;&#27169;&#22359;&#12290;&#25105;&#20204;&#23637;&#31034;&#35813;&#34920;&#36848;&#19982;&#24555;&#36895;&#38543;&#26426;&#25968;&#21457;&#29983;&#22120;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#20805;&#20998;&#21033;&#29992;GPU&#20013;&#23454;&#29616;&#30340;&#24182;&#34892;&#22788;&#29702;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#24191;&#27867;&#35780;&#20272;&#30830;&#35748;&#20102;&#36825;&#31181;&#26041;&#27861;&#20248;&#20110;co
&lt;/p&gt;
&lt;p&gt;
arXiv:2110.03423v2 Announce Type: replace  Abstract: Matrix decompositions are ubiquitous in machine learning, including applications in dimensionality reduction, data compression and deep learning algorithms. Typical solutions for matrix decompositions have polynomial complexity which significantly increases their computational cost and time. In this work, we leverage efficient processing operations that can be run in parallel on modern Graphical Processing Units (GPUs), predominant computing architecture used e.g. in deep learning, to reduce the computational burden of computing matrix decompositions. More specifically, we reformulate the randomized decomposition problem to incorporate fast matrix multiplication operations (BLAS-3) as building blocks. We show that this formulation, combined with fast random number generators, allows to fully exploit the potential of parallel processing implemented in GPUs. Our extensive evaluation confirms the superiority of this approach over the co
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39318;&#20808;&#23450;&#20041;&#19968;&#20010;&#34913;&#37327;&#22522;&#20110;&#25968;&#25454;&#20844;&#24335;&#36136;&#37327;&#30340;&#26631;&#23610;&#65292;&#28982;&#21518;&#23547;&#25214;&#26368;&#20248;&#20844;&#24335;&#65292;&#20351;&#20854;&#22312;&#20445;&#35777;&#26679;&#26412;&#22806;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#26356;&#21152;&#25509;&#36817;&#30495;&#23454;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2109.06911</link><description>&lt;p&gt;
&#20351;&#29992;&#25968;&#25454;&#36827;&#34892;&#23398;&#20064;&#21644;&#20915;&#31574;&#65306;&#26368;&#20248;&#20844;&#24335;&#19982;&#30456;&#21464;
&lt;/p&gt;
&lt;p&gt;
Learning and Decision-Making with Data: Optimal Formulations and Phase Transitions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2109.06911
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39318;&#20808;&#23450;&#20041;&#19968;&#20010;&#34913;&#37327;&#22522;&#20110;&#25968;&#25454;&#20844;&#24335;&#36136;&#37327;&#30340;&#26631;&#23610;&#65292;&#28982;&#21518;&#23547;&#25214;&#26368;&#20248;&#20844;&#24335;&#65292;&#20351;&#20854;&#22312;&#20445;&#35777;&#26679;&#26412;&#22806;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#26356;&#21152;&#25509;&#36817;&#30495;&#23454;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#24403;&#21482;&#26377;&#21382;&#21490;&#25968;&#25454;&#21487;&#29992;&#26102;&#35774;&#35745;&#26368;&#20248;&#23398;&#20064;&#21644;&#20915;&#31574;&#20844;&#24335;&#30340;&#38382;&#39064;&#12290;&#20043;&#21069;&#30340;&#24037;&#20316;&#36890;&#24120;&#20250;&#33268;&#21147;&#20110;&#26576;&#19968;&#31867;&#22522;&#20110;&#25968;&#25454;&#30340;&#20844;&#24335;&#65292;&#28982;&#21518;&#35797;&#22270;&#24314;&#31435;&#26679;&#26412;&#22806;&#24615;&#33021;&#20445;&#35777;&#12290;&#25105;&#20204;&#36825;&#37324;&#37319;&#21462;&#20102;&#30456;&#21453;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#23450;&#20041;&#19968;&#20010;&#21512;&#29702;&#30340;&#26631;&#23610;&#26469;&#34913;&#37327;&#20219;&#20309;&#22522;&#20110;&#25968;&#25454;&#30340;&#20844;&#24335;&#30340;&#36136;&#37327;&#65292;&#28982;&#21518;&#23547;&#27714;&#25214;&#21040;&#19968;&#20010;&#26368;&#20248;&#30340;&#36825;&#26679;&#30340;&#20844;&#24335;&#12290;&#19981;&#27491;&#24335;&#22320;&#35828;&#65292;&#20219;&#20309;&#22522;&#20110;&#25968;&#25454;&#30340;&#20844;&#24335;&#21487;&#20197;&#34987;&#35270;&#20026;&#22312;&#20445;&#35777;&#26679;&#26412;&#22806;&#24615;&#33021;&#27700;&#24179;&#30340;&#21516;&#26102;&#24179;&#34913;&#20272;&#35745;&#25104;&#26412;&#19982;&#23454;&#38469;&#25104;&#26412;&#30340;&#25509;&#36817;&#24230;&#30340;&#24230;&#37327;&#12290;&#22312;&#32473;&#23450;&#19968;&#20010;&#21487;&#25509;&#21463;&#30340;&#26679;&#26412;&#22806;&#24615;&#33021;&#27700;&#24179;&#21518;&#65292;&#25105;&#20204;&#26174;&#24335;&#26500;&#36896;&#20102;&#19968;&#20010;&#22522;&#20110;&#25968;&#25454;&#30340;&#20844;&#24335;&#65292;&#35813;&#20844;&#24335;&#22312;&#19982;&#21516;&#26679;&#20855;&#26377;&#26679;&#26412;&#22806;&#24615;&#33021;&#30340;&#20854;&#20182;&#20844;&#24335;&#30456;&#27604;&#26159;&#32479;&#19968;&#26356;&#25509;&#36817;&#30495;&#23454;&#25104;&#26412;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#26679;&#26412;&#22806;&#25191;&#34892;
&lt;/p&gt;
&lt;p&gt;
arXiv:2109.06911v3 Announce Type: replace-cross  Abstract: We study the problem of designing optimal learning and decision-making formulations when only historical data is available. Prior work typically commits to a particular class of data-driven formulation and subsequently tries to establish out-of-sample performance guarantees. We take here the opposite approach. We define first a sensible yard stick with which to measure the quality of any data-driven formulation and subsequently seek to find an optimal such formulation. Informally, any data-driven formulation can be seen to balance a measure of proximity of the estimated cost to the actual cost while guaranteeing a level of out-of-sample performance. Given an acceptable level of out-of-sample performance, we construct explicitly a data-driven formulation that is uniformly closer to the true cost than any other formulation enjoying the same out-of-sample performance. We show the existence of three distinct out-of-sample performan
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#24120;&#29992;&#30340;&#21367;&#31215;&#25805;&#20316;&#37325;&#26032;&#26500;&#36896;&#20026;&#22810;&#23618;&#21367;&#31215;&#31232;&#30095;&#32534;&#30721;&#22359;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#33021;&#29992;&#20110;&#26174;&#33879;&#25552;&#39640;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#24615;&#33021;&#30340;&#26032;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2108.00408</link><description>&lt;p&gt;
CSC-Unet&#65306;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#31232;&#30095;&#32534;&#30721;&#31574;&#30053;&#30340;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
CSC-Unet: A Novel Convolutional Sparse Coding Strategy Based Neural Network for Semantic Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2108.00408
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#24120;&#29992;&#30340;&#21367;&#31215;&#25805;&#20316;&#37325;&#26032;&#26500;&#36896;&#20026;&#22810;&#23618;&#21367;&#31215;&#31232;&#30095;&#32534;&#30721;&#22359;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#33021;&#29992;&#20110;&#26174;&#33879;&#25552;&#39640;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#24615;&#33021;&#30340;&#26032;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#25191;&#34892;&#35821;&#20041;&#20998;&#21106;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#36825;&#26159;&#30001;&#20110;&#30495;&#23454;&#22270;&#20687;&#22330;&#26223;&#30340;&#22797;&#26434;&#24615;&#12290;&#35768;&#22810;&#22522;&#20110;&#20256;&#32479;&#28145;&#24230;&#23398;&#20064;&#30340;&#35821;&#20041;&#20998;&#21106;&#26041;&#27861;&#26410;&#33021;&#20805;&#20998;&#25429;&#25417;&#22270;&#20687;&#30340;&#35821;&#20041;&#21644;&#22806;&#35266;&#20449;&#24687;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#23545;&#21508;&#31181;&#24212;&#29992;&#22330;&#26223;&#30340;&#26222;&#36866;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31574;&#30053;&#65292;&#23558;&#24120;&#29992;&#30340;&#21367;&#31215;&#25805;&#20316;&#37325;&#26032;&#26500;&#36896;&#20026;&#22810;&#23618;&#21367;&#31215;&#31232;&#30095;&#32534;&#30721;&#22359;&#65292;&#20197;&#32531;&#35299;&#21069;&#36848;&#32570;&#38519;&#12290;&#35813;&#31574;&#30053;&#21487;&#33021;&#29992;&#20110;&#26174;&#33879;&#25552;&#39640;&#20219;&#20309;&#28041;&#21450;&#21367;&#31215;&#25805;&#20316;&#30340;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#30340;&#20998;&#21106;&#24615;&#33021;&#12290;&#20026;&#20102;&#35777;&#26126;&#25105;&#20204;&#24819;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;&#24191;&#27867;&#20351;&#29992;&#30340;U-Net&#27169;&#22411;&#36827;&#34892;&#28436;&#31034;&#65292;&#24182;&#22522;&#20110;U-Net&#35774;&#35745;&#20102;CSC-Unet&#27169;&#22411;&#31995;&#21015;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#20998;&#26512;&#21644;&#23454;&#39564;&#35777;&#25454;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#21487;&#20449;&#30340;&#35777;&#25454;&#26174;&#31034;
&lt;/p&gt;
&lt;p&gt;
arXiv:2108.00408v2 Announce Type: replace-cross  Abstract: It is a challenging task to accurately perform semantic segmentation due to the complexity of real picture scenes. Many semantic segmentation methods based on traditional deep learning insufficiently captured the semantic and appearance information of images, which put limit on their generality and robustness for various application scenes. In this paper, we proposed a novel strategy that reformulated the popularly-used convolution operation to multi-layer convolutional sparse coding block to ease the aforementioned deficiency. This strategy can be possibly used to significantly improve the segmentation performance of any semantic segmentation model that involves convolutional operations. To prove the effectiveness of our idea, we chose the widely-used U-Net model for the demonstration purpose, and we designed CSC-Unet model series based on U-Net. Through extensive analysis and experiments, we provided credible evidence showing
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#36335;&#24452;&#37325;&#21152;&#26435;&#30340;&#23450;&#21046;&#22270;&#31070;&#32463;&#32593;&#32476;CustomGNN&#21487;&#20197;&#33258;&#21160;&#23398;&#20064;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#30340;&#39640;&#23618;&#35821;&#20041;&#65292;&#31361;&#20986;&#19982;&#35821;&#20041;&#30456;&#20851;&#30340;&#36335;&#24452;&#65292;&#24182;&#36807;&#28388;&#25481;&#20219;&#21153;&#26080;&#20851;&#30340;&#22122;&#38899;&#65292;&#36991;&#20813;&#20256;&#32479;GNN&#20013;&#30340;&#36807;&#24230;&#24179;&#28369;&#12289;&#40065;&#26834;&#24615;&#24046;&#21644;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2106.10866</link><description>&lt;p&gt;
&#20351;&#29992;&#36335;&#24452;&#37325;&#21152;&#26435;&#23450;&#21046;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Customizing Graph Neural Networks using Path Reweighting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2106.10866
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#36335;&#24452;&#37325;&#21152;&#26435;&#30340;&#23450;&#21046;&#22270;&#31070;&#32463;&#32593;&#32476;CustomGNN&#21487;&#20197;&#33258;&#21160;&#23398;&#20064;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#30340;&#39640;&#23618;&#35821;&#20041;&#65292;&#31361;&#20986;&#19982;&#35821;&#20041;&#30456;&#20851;&#30340;&#36335;&#24452;&#65292;&#24182;&#36807;&#28388;&#25481;&#20219;&#21153;&#26080;&#20851;&#30340;&#22122;&#38899;&#65292;&#36991;&#20813;&#20256;&#32479;GNN&#20013;&#30340;&#36807;&#24230;&#24179;&#28369;&#12289;&#40065;&#26834;&#24615;&#24046;&#21644;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#24050;&#24191;&#27867;&#29992;&#20110;&#25366;&#25496;&#22270;&#32467;&#26500;&#25968;&#25454;&#65292;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;GNNs&#24182;&#26410;&#21306;&#20998;&#19981;&#21516;&#19979;&#28216;&#20219;&#21153;&#65292;&#22240;&#27492;&#23427;&#20204;&#23884;&#20837;&#30340;&#23884;&#20837;&#21521;&#37327;&#24182;&#38750;&#24635;&#26159;&#26377;&#25928;&#12290;&#26412;&#25991;&#20197;&#22270;&#20013;&#30340;&#36335;&#24452;&#20026;&#28789;&#24863;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;GNN&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#24102;&#26377;&#36335;&#24452;&#37325;&#21152;&#26435;&#30340;&#23450;&#21046;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;&#31616;&#31216;CustomGNN&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25152;&#25552;&#20986;&#30340;CustomGNN&#21487;&#20197;&#33258;&#21160;&#23398;&#20064;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#30340;&#39640;&#23618;&#35821;&#20041;&#65292;&#31361;&#20986;&#19982;&#35821;&#20041;&#30456;&#20851;&#30340;&#36335;&#24452;&#65292;&#24182;&#36807;&#28388;&#25481;&#22270;&#20013;&#19982;&#20219;&#21153;&#26080;&#20851;&#30340;&#22122;&#38899;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;CustomGNN&#23398;&#20064;&#30340;&#35821;&#20041;&#20197;&#21450;&#20854;&#36991;&#20813;&#20256;&#32479;GNN&#20013;&#23384;&#22312;&#30340;&#36807;&#24230;&#24179;&#28369;&#12289;&#40065;&#26834;&#24615;&#24046;&#21644;&#36807;&#25311;&#21512;&#31561;&#19977;&#20010;&#22266;&#26377;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2106.10866v3 Announce Type: replace  Abstract: Graph Neural Networks (GNNs) have been extensively used for mining graph-structured data with impressive performance. However, because these traditional GNNs do not distinguish among various downstream tasks, embeddings embedded by them are not always effective. Intuitively, paths in a graph imply different semantics for different downstream tasks. Inspired by this, we design a novel GNN solution, namely Customized Graph Neural Network with Path Reweighting (CustomGNN for short). Specifically, the proposed CustomGNN can automatically learn the high-level semantics for specific downstream tasks to highlight semantically relevant paths as well to filter out task-irrelevant noises in a graph. Furthermore, we empirically analyze the semantics learned by CustomGNN and demonstrate its ability to avoid the three inherent problems in traditional GNNs, i.e., over-smoothing, poor robustness, and overfitting. In experiments with the node classi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#31890;&#24230;SvF&#23398;&#20064;&#31574;&#30053;&#65292;&#36890;&#36807;&#35774;&#35745;&#39057;&#29575;&#24863;&#30693;&#27491;&#21017;&#21270;&#21644;&#29305;&#24449;&#31354;&#38388;&#32452;&#21512;&#25805;&#20316;&#26469;&#24179;&#34913;&#20445;&#30041;&#26087;&#30693;&#35782;&#19982;&#36866;&#24212;&#26032;&#30693;&#35782;&#65292;&#20197;&#35299;&#20915;&#23569;&#26679;&#26412;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;&#20013;&#30340;"&#24930;vs.&#24555;"&#22256;&#22659;</title><link>https://arxiv.org/abs/2006.15524</link><description>&lt;p&gt;
MgSvF&#65306;&#22810;&#31890;&#24230;&#24930;vs.&#24555;&#26694;&#26550;&#29992;&#20110;&#23569;&#26679;&#26412;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
MgSvF: Multi-Grained Slow vs. Fast Framework for Few-Shot Class-Incremental Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2006.15524
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#31890;&#24230;SvF&#23398;&#20064;&#31574;&#30053;&#65292;&#36890;&#36807;&#35774;&#35745;&#39057;&#29575;&#24863;&#30693;&#27491;&#21017;&#21270;&#21644;&#29305;&#24449;&#31354;&#38388;&#32452;&#21512;&#25805;&#20316;&#26469;&#24179;&#34913;&#20445;&#30041;&#26087;&#30693;&#35782;&#19982;&#36866;&#24212;&#26032;&#30693;&#35782;&#65292;&#20197;&#35299;&#20915;&#23569;&#26679;&#26412;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;&#20013;&#30340;"&#24930;vs.&#24555;"&#22256;&#22659;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#23569;&#26679;&#26412;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;(FSCIL)&#19981;&#26029;&#23398;&#20064;&#19968;&#31995;&#21015;&#20219;&#21153;&#65292;&#38754;&#20020;&#30528;&#22312;&#24536;&#35760;&#26087;&#30693;&#35782;&#21644;&#24555;&#36895;&#36866;&#24212;&#26032;&#30693;&#35782;&#20043;&#38388;&#30340;&#22256;&#22659;&#12290;&#26412;&#25991;&#20851;&#27880;&#36825;&#31181;&#8220;&#24930;vs.&#24555;&#8221;(SvF)&#22256;&#22659;&#65292;&#30830;&#23450;&#21738;&#20123;&#30693;&#35782;&#32452;&#20214;&#24212;&#20197;&#24930;&#36895;&#25110;&#24555;&#36895;&#26041;&#24335;&#26356;&#26032;&#65292;&#20174;&#32780;&#24179;&#34913;&#20445;&#30041;&#26087;&#30693;&#35782;&#21644;&#36866;&#24212;&#26032;&#30693;&#35782;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#31890;&#24230;SvF&#23398;&#20064;&#31574;&#30053;&#65292;&#20197;&#24212;&#23545;&#26469;&#33258;&#20004;&#20010;&#19981;&#21516;&#31890;&#24230;&#30340;SvF&#22256;&#22659;&#65306;&#20869;&#37096;&#31354;&#38388;(&#22312;&#30456;&#21516;&#29305;&#24449;&#31354;&#38388;&#20869;)&#21644;&#22806;&#37096;&#31354;&#38388;(&#22312;&#20004;&#20010;&#19981;&#21516;&#29305;&#24449;&#31354;&#38388;&#20043;&#38388;)&#12290;&#25152;&#25552;&#20986;&#30340;&#31574;&#30053;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39057;&#29575;&#24863;&#30693;&#27491;&#21017;&#21270;&#65292;&#20197;&#25552;&#21319;&#20869;&#37096;&#31354;&#38388;SvF&#33021;&#21147;&#65292;&#24182;&#21516;&#26102;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#29305;&#24449;&#31354;&#38388;&#32452;&#21512;&#25805;&#20316;&#65292;&#20197;&#22686;&#24378;&#22806;&#37096;&#31354;&#38388;SvF&#23398;&#20064;&#24615;&#33021;&#12290;&#36890;&#36807;&#22810;&#31890;&#24230;SvF&#23398;&#20064;&#31574;&#30053;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33719;&#24471;&#20102;&#21738;
&lt;/p&gt;
&lt;p&gt;
arXiv:2006.15524v4 Announce Type: replace-cross  Abstract: As a challenging problem, few-shot class-incremental learning (FSCIL) continually learns a sequence of tasks, confronting the dilemma between slow forgetting of old knowledge and fast adaptation to new knowledge. In this paper, we concentrate on this "slow vs. fast" (SvF) dilemma to determine which knowledge components to be updated in a slow fashion or a fast fashion, and thereby balance old-knowledge preservation and new-knowledge adaptation. We propose a multi-grained SvF learning strategy to cope with the SvF dilemma from two different grains: intra-space (within the same feature space) and inter-space (between two different feature spaces). The proposed strategy designs a novel frequency-aware regularization to boost the intra-space SvF capability, and meanwhile develops a new feature space composition operation to enhance the inter-space SvF learning performance. With the multi-grained SvF learning strategy, our method ou
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GPGL&#30340;epoch-evolving Gaussian Process Guided Learning&#23398;&#20064;&#26041;&#26696;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#26631;&#31614;&#21644;&#22320;&#38754;&#30495;&#23454;&#26631;&#31614;&#25351;&#23548;&#27169;&#22411;&#21442;&#25968;&#26356;&#26032;&#65292;&#36827;&#19968;&#27493;&#25512;&#24191;&#24182;&#24212;&#29992;&#20110;&#24403;&#21069;&#28145;&#24230;&#27169;&#22411;&#65292;&#22312;&#20027;&#27969;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#22522;&#20110;&#25209;&#27425;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;</title><link>https://arxiv.org/abs/2006.14347</link><description>&lt;p&gt;
&#19981;&#26029;&#28436;&#21464;&#30340;&#39640;&#26031;&#36807;&#31243;&#24341;&#23548;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Epoch-evolving Gaussian Process Guided Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2006.14347
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GPGL&#30340;epoch-evolving Gaussian Process Guided Learning&#23398;&#20064;&#26041;&#26696;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#26631;&#31614;&#21644;&#22320;&#38754;&#30495;&#23454;&#26631;&#31614;&#25351;&#23548;&#27169;&#22411;&#21442;&#25968;&#26356;&#26032;&#65292;&#36827;&#19968;&#27493;&#25512;&#24191;&#24182;&#24212;&#29992;&#20110;&#24403;&#21069;&#28145;&#24230;&#27169;&#22411;&#65292;&#22312;&#20027;&#27969;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#22522;&#20110;&#25209;&#27425;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#19981;&#26029;&#28436;&#21464;&#30340;&#39640;&#26031;&#36807;&#31243;&#24341;&#23548;&#23398;&#20064;&#65288;GPGL&#65289;&#30340;&#26032;&#39062;&#23398;&#20064;&#26041;&#26696;&#65292;&#26088;&#22312;&#25551;&#36848;&#25209;&#32423;&#20998;&#24067;&#19982;&#20840;&#23616;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#30340;&#30456;&#20851;&#20449;&#24687;&#12290;&#36825;&#31181;&#30456;&#20851;&#20449;&#24687;&#34987;&#32534;&#30721;&#20026;&#19978;&#19979;&#25991;&#26631;&#31614;&#65292;&#38656;&#35201;&#27599;&#20010;&#26102;&#26399;&#36827;&#34892;&#26356;&#26032;&#12290;&#22312;&#19978;&#19979;&#25991;&#26631;&#31614;&#21644;&#22320;&#38754;&#30495;&#23454;&#26631;&#31614;&#30340;&#25351;&#23548;&#19979;&#65292;GPGL&#26041;&#26696;&#36890;&#36807;&#20351;&#29992;&#19977;&#35282;&#19968;&#33268;&#24615;&#25439;&#22833;&#26469;&#26356;&#26032;&#27169;&#22411;&#21442;&#25968;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#39640;&#25928;&#30340;&#20248;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;GPGL&#26041;&#26696;&#21487;&#20197;&#36827;&#19968;&#27493;&#25512;&#24191;&#24182;&#33258;&#28982;&#24212;&#29992;&#20110;&#24403;&#21069;&#30340;&#28145;&#24230;&#27169;&#22411;&#65292;&#22312;&#20027;&#27969;&#25968;&#25454;&#38598;&#65288;CIFAR-10&#12289;CIFAR-100&#21644;Tiny-ImageNet&#65289;&#19978;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#22522;&#20110;&#25209;&#27425;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2006.14347v2 Announce Type: replace  Abstract: In this paper, we propose a novel learning scheme called epoch-evolving Gaussian Process Guided Learning (GPGL), which aims at characterizing the correlation information between the batch-level distribution and the global data distribution. Such correlation information is encoded as context labels and needs renewal every epoch. With the guidance of the context label and ground truth label, GPGL scheme provides a more efficient optimization through updating the model parameters with a triangle consistency loss. Furthermore, our GPGL scheme can be further generalized and naturally applied to the current deep models, outperforming the existing batch-based state-of-the-art models on mainstream datasets (CIFAR-10, CIFAR-100, and Tiny-ImageNet) remarkably.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;lil'HDoC&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#23567;&#38408;&#20540;&#38388;&#38553;&#19979;&#30340;&#22909;&#33218;&#35782;&#21035;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#31639;&#27861;&#22312;&#26679;&#26412;&#25928;&#29575;&#19978;&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.15879</link><description>&lt;p&gt;
&#23567;&#38408;&#20540;&#38388;&#38553;&#19979;&#30340;&#22909;&#33218;&#35782;&#21035;&#31639;&#27861;: lil'HDoC
&lt;/p&gt;
&lt;p&gt;
lil'HDoC: An Algorithm for Good Arm Identification under Small Threshold Gap. (arXiv:2401.15879v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15879
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;lil'HDoC&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#23567;&#38408;&#20540;&#38388;&#38553;&#19979;&#30340;&#22909;&#33218;&#35782;&#21035;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#31639;&#27861;&#22312;&#26679;&#26412;&#25928;&#29575;&#19978;&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22909;&#33218;&#35782;&#21035;&#65288;GAI&#65289;&#26159;&#19968;&#20010;&#32431;&#25506;&#32034;&#24615;&#30340;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#65292;&#19968;&#20010;&#21333;&#29420;&#30340;&#23398;&#20064;&#22120;&#20250;&#22312;&#30830;&#23450;&#19968;&#20010;&#33218;&#26159;&#22909;&#33218;&#26102;&#31435;&#21363;&#36755;&#20986;&#35813;&#33218;&#12290;&#22909;&#33218;&#34987;&#23450;&#20041;&#20026;&#26399;&#26395;&#22238;&#25253;&#22823;&#20110;&#31561;&#20110;&#32473;&#23450;&#38408;&#20540;&#30340;&#33218;&#12290;&#26412;&#25991;&#32858;&#28966;&#20110;&#23567;&#38408;&#20540;&#38388;&#38553;&#19979;&#30340;GAI&#38382;&#39064;&#65292;&#35813;&#38388;&#38553;&#25351;&#30340;&#26159;&#33218;&#30340;&#26399;&#26395;&#22238;&#25253;&#19982;&#32473;&#23450;&#38408;&#20540;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;lil'HDoC&#30340;&#26032;&#31639;&#27861;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;HDoC&#31639;&#27861;&#30340;&#24635;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#23567;&#38408;&#20540;&#38388;&#38553;&#19979;&#65292;lil'HDoC&#31639;&#27861;&#36755;&#20986;&#30340;&#31532;&#19968;&#20010;&#955;&#33218;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#19982;&#21407;&#22987;HDoC&#31639;&#27861;&#30456;&#27604;&#20165;&#26377;&#24494;&#23567;&#30340;&#24046;&#24322;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Good arm identification (GAI) is a pure-exploration bandit problem in which a single learner outputs an arm as soon as it is identified as a good arm. A good arm is defined as an arm with an expected reward greater than or equal to a given threshold. This paper focuses on the GAI problem under a small threshold gap, which refers to the distance between the expected rewards of arms and the given threshold. We propose a new algorithm called lil'HDoC to significantly improve the total sample complexity of the HDoC algorithm. We demonstrate that the sample complexity of the first $\lambda$ output arm in lil'HDoC is bounded by the original HDoC algorithm, except for one negligible term, when the distance between the expected reward and threshold is small. Extensive experiments confirm that our algorithm outperforms the state-of-the-art algorithms in both synthetic and real-world datasets.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23558;&#19981;&#21516;&#32463;&#39564;&#29255;&#27573;&#32452;&#21512;&#36215;&#26469;&#35299;&#20915;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#30340;&#23646;&#24615;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#21457;&#29616;&#36825;&#31181;&#32452;&#21512;&#23646;&#24615;&#19982;&#32452;&#21512;&#27867;&#21270;&#26377;&#20851;&#12290;</title><link>http://arxiv.org/abs/2401.11237</link><description>&lt;p&gt;
&#32553;&#23567;TD&#23398;&#20064;&#21644;&#30417;&#30563;&#23398;&#20064;&#20043;&#38388;&#30340;&#24046;&#36317;--&#20174;&#19968;&#33324;&#21270;&#30340;&#35282;&#24230;&#26469;&#30475;
&lt;/p&gt;
&lt;p&gt;
Closing the Gap between TD Learning and Supervised Learning -- A Generalisation Point of View. (arXiv:2401.11237v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11237
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23558;&#19981;&#21516;&#32463;&#39564;&#29255;&#27573;&#32452;&#21512;&#36215;&#26469;&#35299;&#20915;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#30340;&#23646;&#24615;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#21457;&#29616;&#36825;&#31181;&#32452;&#21512;&#23646;&#24615;&#19982;&#32452;&#21512;&#27867;&#21270;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20123;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#23558;&#32463;&#39564;&#29255;&#27573;&#32452;&#21512;&#36215;&#26469;&#35299;&#20915;&#35757;&#32451;&#36807;&#31243;&#20013;&#20174;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#12290;&#36825;&#31181;&#32463;&#24120;&#34987;&#36861;&#27714;&#30340;&#29305;&#24615;&#26159;&#22522;&#20110;&#21160;&#24577;&#35268;&#21010;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#19982;&#22522;&#20110;&#30417;&#30563;&#23398;&#20064;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#20043;&#38388;&#30340;&#20960;&#31181;&#21306;&#21035;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#26576;&#20123;&#22522;&#20110;&#29616;&#25104;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#27809;&#26377;&#26126;&#30830;&#30340;&#32452;&#21512;&#26426;&#21046;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#33021;&#21462;&#24471;&#20986;&#33394;&#30340;&#32467;&#26524;&#65307;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#36825;&#20123;&#26041;&#27861;&#26159;&#21542;&#25918;&#24323;&#20102;&#36825;&#31181;&#37325;&#35201;&#30340;&#32452;&#21512;&#29305;&#24615;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#21363;&#23558;&#36798;&#21040;&#30446;&#26631;&#29366;&#24577;&#21644;&#36798;&#21040;&#30446;&#26631;&#22238;&#25253;&#20540;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#26159;&#23637;&#31034;&#20102;&#32452;&#21512;&#29305;&#24615;&#23545;&#24212;&#20102;&#19968;&#31181;&#32452;&#21512;&#27867;&#21270;&#65306;&#22312;(state, goal)&#23545;&#30340;&#20998;&#24067;&#19978;&#36827;&#34892;&#35757;&#32451;&#21518;&#65292;&#24076;&#26395;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#27809;&#26377;&#21516;&#26102;&#20986;&#29616;&#30340;(state, goal)&#23545;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#36825;&#31181;&#32452;&#21512;&#27867;&#21270;&#19982;i.i.d.&#27867;&#21270;&#26159;&#19981;&#21516;&#30340;&#12290;&#36825;&#31181;&#36830;&#25509;&#23558;&#32452;&#21512;&#29305;&#24615;&#19982;&#27867;&#21270;&#32852;&#31995;&#22312;&#19968;&#36215;&#12290;
&lt;/p&gt;
&lt;p&gt;
Some reinforcement learning (RL) algorithms can stitch pieces of experience to solve a task never seen before during training. This oft-sought property is one of the few ways in which RL methods based on dynamic-programming differ from RL methods based on supervised-learning (SL). Yet, certain RL methods based on off-the-shelf SL algorithms achieve excellent results without an explicit mechanism for stitching; it remains unclear whether those methods forgo this important stitching property. This paper studies this question for the problems of achieving a target goal state and achieving a target return value. Our main result is to show that the stitching property corresponds to a form of combinatorial generalization: after training on a distribution of (state, goal) pairs, one would like to evaluate on (state, goal) pairs not seen together in the training data. Our analysis shows that this sort of generalization is different from i.i.d. generalization. This connection between stitching 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#20041;&#30340;&#32423;&#32852;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#32771;&#34385;&#20102;&#29992;&#25143;&#29366;&#24577;&#21644;&#29366;&#24577;&#36716;&#25442;&#23545;&#20915;&#31574;&#30340;&#24433;&#21709;&#65292;&#22312;&#32423;&#32852;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#25105;&#20204;&#38656;&#35201;&#36873;&#25321;&#19981;&#20165;&#20855;&#26377;&#36739;&#22823;&#21560;&#24341;&#27010;&#29575;&#30340;&#39033;&#30446;&#65292;&#36824;&#35201;&#36873;&#25321;&#33021;&#22815;&#23548;&#33268;&#33391;&#22909;&#21518;&#32487;&#29366;&#24577;&#30340;&#39033;&#30446;&#12290;</title><link>http://arxiv.org/abs/2401.08961</link><description>&lt;p&gt;
&#32423;&#32852;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Cascading Reinforcement Learning. (arXiv:2401.08961v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08961
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#20041;&#30340;&#32423;&#32852;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#32771;&#34385;&#20102;&#29992;&#25143;&#29366;&#24577;&#21644;&#29366;&#24577;&#36716;&#25442;&#23545;&#20915;&#31574;&#30340;&#24433;&#21709;&#65292;&#22312;&#32423;&#32852;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#25105;&#20204;&#38656;&#35201;&#36873;&#25321;&#19981;&#20165;&#20855;&#26377;&#36739;&#22823;&#21560;&#24341;&#27010;&#29575;&#30340;&#39033;&#30446;&#65292;&#36824;&#35201;&#36873;&#25321;&#33021;&#22815;&#23548;&#33268;&#33391;&#22909;&#21518;&#32487;&#29366;&#24577;&#30340;&#39033;&#30446;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#32423;&#32852;&#36172;&#21338;&#26426;&#22312;&#25512;&#33616;&#31995;&#32479;&#21644;&#22312;&#32447;&#24191;&#21578;&#20013;&#24212;&#29992;&#24191;&#27867;&#12290;&#22312;&#32423;&#32852;&#36172;&#21338;&#26426;&#27169;&#22411;&#20013;&#65292;&#27599;&#20010;&#26102;&#21051;&#65292;&#19968;&#20010;&#20195;&#29702;&#20154;&#20174;&#19968;&#32452;&#20855;&#26377;&#26410;&#30693;&#21560;&#24341;&#27010;&#29575;&#30340;&#39033;&#30446;&#20013;&#25512;&#33616;&#19968;&#20010;&#26377;&#24207;&#30340;&#39033;&#30446;&#23376;&#38598;&#65288;&#31216;&#20026;&#39033;&#30446;&#21015;&#34920;&#65289;&#12290;&#28982;&#21518;&#65292;&#29992;&#25143;&#26816;&#26597;&#21015;&#34920;&#65292;&#24182;&#28857;&#20987;&#31532;&#19968;&#20010;&#26377;&#21560;&#24341;&#21147;&#30340;&#39033;&#30446;&#65288;&#22914;&#26524;&#26377;&#30340;&#35805;&#65289;&#65292;&#20043;&#21518;&#65292;&#20195;&#29702;&#25910;&#21040;&#19968;&#20010;&#22870;&#21169;&#12290;&#20195;&#29702;&#30340;&#30446;&#26631;&#26159;&#26368;&#22823;&#21270;&#39044;&#26399;&#30340;&#32047;&#31215;&#22870;&#21169;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#32423;&#32852;&#36172;&#21338;&#26426;&#25991;&#29486;&#24573;&#30053;&#20102;&#29992;&#25143;&#29366;&#24577;&#65288;&#20363;&#22914;&#21382;&#21490;&#34892;&#20026;&#65289;&#23545;&#25512;&#33616;&#30340;&#24433;&#21709;&#20197;&#21450;&#20250;&#35805;&#36827;&#34892;&#36807;&#31243;&#20013;&#29366;&#24577;&#30340;&#21464;&#21270;&#12290;&#21463;&#27492;&#20107;&#23454;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#20041;&#30340;&#32423;&#32852;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#32771;&#34385;&#20102;&#29992;&#25143;&#29366;&#24577;&#21644;&#29366;&#24577;&#36716;&#25442;&#23545;&#20915;&#31574;&#30340;&#24433;&#21709;&#12290;&#22312;&#32423;&#32852;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#25105;&#20204;&#38656;&#35201;&#36873;&#25321;&#19981;&#20165;&#20855;&#26377;&#36739;&#22823;&#21560;&#24341;&#27010;&#29575;&#30340;&#39033;&#30446;&#65292;&#36824;&#35201;&#36873;&#25321;&#33021;&#22815;&#23548;&#33268;&#33391;&#22909;&#21518;&#32487;&#29366;&#24577;&#30340;&#39033;&#30446;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cascading bandits have gained popularity in recent years due to their applicability to recommendation systems and online advertising. In the cascading bandit model, at each timestep, an agent recommends an ordered subset of items (called an item list) from a pool of items, each associated with an unknown attraction probability. Then, the user examines the list, and clicks the first attractive item (if any), and after that, the agent receives a reward. The goal of the agent is to maximize the expected cumulative reward. However, the prior literature on cascading bandits ignores the influences of user states (e.g., historical behaviors) on recommendations and the change of states as the session proceeds. Motivated by this fact, we propose a generalized cascading RL framework, which considers the impact of user states and state transition into decisions. In cascading RL, we need to select items not only with large attraction probabilities but also leading to good successor states. This im
&lt;/p&gt;</description></item><item><title>Scissorhands &#26159;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#21462;&#28040;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#36830;&#25509;&#25935;&#24863;&#24615;&#35782;&#21035;&#19982;&#36951;&#24536;&#25968;&#25454;&#30456;&#20851;&#30340;&#26368;&#30456;&#20851;&#21442;&#25968;&#65292;&#24182;&#36890;&#36807;&#37325;&#26032;&#35757;&#32451;&#20462;&#21098;&#30340;&#27169;&#22411;&#26469;&#25830;&#38500;&#25968;&#25454;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.06187</link><description>&lt;p&gt;
Scissorhands: &#36890;&#36807;&#32593;&#32476;&#36830;&#25509;&#25935;&#24863;&#24615;&#22312;&#25968;&#25454;&#24433;&#21709;&#20013;&#36827;&#34892;&#25968;&#25454;&#25830;&#38500;
&lt;/p&gt;
&lt;p&gt;
Scissorhands: Scrub Data Influence via Connection Sensitivity in Networks. (arXiv:2401.06187v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06187
&lt;/p&gt;
&lt;p&gt;
Scissorhands &#26159;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#21462;&#28040;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#36830;&#25509;&#25935;&#24863;&#24615;&#35782;&#21035;&#19982;&#36951;&#24536;&#25968;&#25454;&#30456;&#20851;&#30340;&#26368;&#30456;&#20851;&#21442;&#25968;&#65292;&#24182;&#36890;&#36807;&#37325;&#26032;&#35757;&#32451;&#20462;&#21098;&#30340;&#27169;&#22411;&#26469;&#25830;&#38500;&#25968;&#25454;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#21462;&#28040;&#23398;&#20064;&#24050;&#25104;&#20026;&#19968;&#39033;&#37325;&#35201;&#20219;&#21153;&#65292;&#26088;&#22312;&#25830;&#38500;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#25968;&#25454;&#24433;&#21709;&#12290;&#23427;&#31526;&#21512;&#26368;&#26032;&#30340;&#25968;&#25454;&#30417;&#31649;&#26631;&#20934;&#65292;&#22686;&#24378;&#20102;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#24615;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26426;&#22120;&#21462;&#28040;&#23398;&#20064;&#26041;&#27861;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#36890;&#24120;&#38656;&#35201;&#35775;&#38382;&#20854;&#20313;&#25968;&#25454;&#30340;&#20840;&#37096;&#20869;&#23481;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21487;&#33021;&#19981;&#21487;&#34892;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#21462;&#28040;&#23398;&#20064;&#26041;&#27861;&#8220;Scissorhands&#8221;&#65292;&#23427;&#21482;&#20351;&#29992;&#35757;&#32451;&#25968;&#25454;&#30340;&#23376;&#38598;&#26469;&#26377;&#25928;&#36816;&#34892;&#12290;&#21021;&#22987;&#38454;&#27573;&#65292;Scissorhands&#36890;&#36807;&#36830;&#25509;&#25935;&#24863;&#24615;&#22312;&#32473;&#23450;&#27169;&#22411;&#20013;&#35782;&#21035;&#19982;&#36951;&#24536;&#25968;&#25454;&#30456;&#20851;&#30340;&#26368;&#30456;&#20851;&#21442;&#25968;&#12290;&#35813;&#36807;&#31243;&#36890;&#36807;&#37325;&#26032;&#21021;&#22987;&#21270;&#36825;&#20123;&#21442;&#25968;&#20013;&#20855;&#26377;&#26368;&#22823;&#24433;&#21709;&#21147;&#30340;&#21069;k%&#30340;&#26368;&#30456;&#20851;&#21442;&#25968;&#65292;&#20174;&#32780;&#20135;&#29983;&#19968;&#20010;&#29992;&#20110;&#25830;&#38500;&#36951;&#24536;&#25968;&#25454;&#24433;&#21709;&#30340;&#20462;&#21098;&#27169;&#22411;&#12290;&#38543;&#21518;&#65292;Scissorhands&#36890;&#36807;&#26368;&#23567;-&#26368;&#22823;&#20248;&#21270;&#36807;&#31243;&#23545;&#20462;&#21098;&#30340;&#27169;&#22411;&#36827;&#34892;&#20877;&#35757;&#32451;&#65292;&#23547;&#25214;&#20445;&#30041;&#20449;&#24687;&#30340;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine unlearning has become a pivotal task to erase the influence of data from a trained model. It adheres to recent data regulation standards and enhances the privacy and security of machine learning applications. Most existing machine unlearning methods perform well, however, they typically necessitate access to the entirety of the remaining data, which might not be feasible in certain scenarios. In this work, we present a new machine unlearning approach Scissorhands, which operates effectively with only a subset of the training data. Initially, Scissorhands identifies the most pertinent parameters in the given model relative to the forgetting data via connection sensitivity. This process involves reinitializing the most influential top-$k$ percent of these parameters, resulting in a trimmed model for erasing the influence of the forgetting data. Subsequently, Scissorhands retrains the trimmed model through a min-max optimization process, seeking parameters that preserve informatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30001;&#30149;&#29702;&#23398;&#23478;&#20026;&#30149;&#29702;&#23398;&#23478;&#26500;&#24314;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#36890;&#36807;&#25968;&#25454;&#25972;&#29702;&#21644;&#34701;&#20837;&#30149;&#29702;&#23398;&#39046;&#22495;&#30693;&#35782;&#65292;&#25193;&#23637;&#20102;&#25968;&#23383;&#30149;&#29702;&#23398;&#20840;&#29627;&#29255;&#22270;&#20687;&#22522;&#30784;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#22788;&#29702;&#32597;&#35265;&#30142;&#30149;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.04079</link><description>&lt;p&gt;
RudolfV&#65306;&#19968;&#31181;&#30001;&#30149;&#29702;&#23398;&#23478;&#20026;&#30149;&#29702;&#23398;&#23478;&#26500;&#24314;&#30340;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
RudolfV: A Foundation Model by Pathologists for Pathologists. (arXiv:2401.04079v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04079
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30001;&#30149;&#29702;&#23398;&#23478;&#20026;&#30149;&#29702;&#23398;&#23478;&#26500;&#24314;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#36890;&#36807;&#25968;&#25454;&#25972;&#29702;&#21644;&#34701;&#20837;&#30149;&#29702;&#23398;&#39046;&#22495;&#30693;&#35782;&#65292;&#25193;&#23637;&#20102;&#25968;&#23383;&#30149;&#29702;&#23398;&#20840;&#29627;&#29255;&#22270;&#20687;&#22522;&#30784;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#22788;&#29702;&#32597;&#35265;&#30142;&#30149;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#32455;&#30149;&#29702;&#23398;&#22312;&#20020;&#24202;&#21307;&#23398;&#21644;&#29983;&#29289;&#21307;&#23398;&#30740;&#31350;&#20013;&#36215;&#30528;&#26680;&#24515;&#20316;&#29992;&#12290;&#34429;&#28982;&#20154;&#24037;&#26234;&#33021;&#22312;&#35768;&#22810;&#30149;&#29702;&#23398;&#20219;&#21153;&#19978;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#22312;&#27867;&#21270;&#21644;&#22788;&#29702;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#30340;&#32597;&#35265;&#30142;&#30149;&#26041;&#38754;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#22312;&#23398;&#20064;&#26469;&#33258;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#20043;&#21069;&#65292;&#20174;&#26080;&#26631;&#35760;&#25968;&#25454;&#20013;&#25552;&#21462;&#30693;&#35782;&#21040;&#22522;&#30784;&#27169;&#22411;&#21487;&#20197;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#21322;&#33258;&#21160;&#25968;&#25454;&#25972;&#29702;&#21644;&#34701;&#20837;&#30149;&#29702;&#23398;&#39046;&#22495;&#30693;&#35782;&#65292;&#25193;&#23637;&#20102;&#25968;&#23383;&#30149;&#29702;&#23398;&#20840;&#29627;&#29255;&#22270;&#20687;&#22522;&#30784;&#27169;&#22411;&#30340;&#26368;&#26032;&#25216;&#26415;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32467;&#21512;&#35745;&#31639;&#21644;&#30149;&#29702;&#23398;&#39046;&#22495;&#30693;&#35782;(1)&#25972;&#29702;&#20102;&#19968;&#20010;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;10.3&#19975;&#20010;&#29627;&#29255;&#22270;&#20687;&#23545;&#24212;&#30340;7.5&#20159;&#20010;&#22270;&#20687;&#22359;&#65292;&#28085;&#30422;&#20102;&#26469;&#33258;&#27431;&#32654;&#19981;&#21516;&#20462;&#22797;&#12289;&#26579;&#33394;&#21644;&#25195;&#25551;&#21327;&#35758;&#20197;&#21450;&#19981;&#21516;&#25351;&#31034;&#21644;&#23454;&#39564;&#23460;&#30340;&#25968;&#25454;&#65292;(2)&#29992;&#20110;&#23545;&#35821;&#20041;&#19978;&#30456;&#20284;&#30340;&#29627;&#29255;&#21644;&#32452;&#32455;&#22359;&#36827;&#34892;&#20998;&#32452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Histopathology plays a central role in clinical medicine and biomedical research. While artificial intelligence shows promising results on many pathological tasks, generalization and dealing with rare diseases, where training data is scarce, remains a challenge. Distilling knowledge from unlabeled data into a foundation model before learning from, potentially limited, labeled data provides a viable path to address these challenges. In this work, we extend the state of the art of foundation models for digital pathology whole slide images by semi-automated data curation and incorporating pathologist domain knowledge. Specifically, we combine computational and pathologist domain knowledge (1) to curate a diverse dataset of 103k slides corresponding to 750 million image patches covering data from different fixation, staining, and scanning protocols as well as data from different indications and labs across the EU and US, (2) for grouping semantically similar slides and tissue patches, and 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22235;&#20010;&#26032;&#30340;&#20960;&#20309;&#24230;&#37327;&#65292;&#21487;&#20197;&#27604;&#36739;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#22312;&#32771;&#34385;&#26597;&#35810;&#28857;&#21644;&#20840;&#23616;&#26368;&#20248;&#35299;&#30340;&#20960;&#20309;&#29305;&#24615;&#26102;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.01981</link><description>&lt;p&gt;
&#36229;&#36234;&#36951;&#25022;&#65306;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#20960;&#20309;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
Beyond Regrets: Geometric Metrics for Bayesian Optimization. (arXiv:2401.01981v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01981
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22235;&#20010;&#26032;&#30340;&#20960;&#20309;&#24230;&#37327;&#65292;&#21487;&#20197;&#27604;&#36739;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#22312;&#32771;&#34385;&#26597;&#35810;&#28857;&#21644;&#20840;&#23616;&#26368;&#20248;&#35299;&#30340;&#20960;&#20309;&#29305;&#24615;&#26102;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#26159;&#19968;&#31181;&#38024;&#23545;&#40657;&#30418;&#23376;&#30446;&#26631;&#20989;&#25968;&#30340;&#21407;&#21017;&#24615;&#20248;&#21270;&#31574;&#30053;&#12290;&#23427;&#22312;&#31185;&#23398;&#21457;&#29616;&#21644;&#23454;&#39564;&#35774;&#35745;&#31561;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#25928;&#26524;&#24471;&#21040;&#20102;&#35777;&#26126;&#12290;&#36890;&#24120;&#65292;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#24615;&#33021;&#26159;&#36890;&#36807;&#22522;&#20110;&#36951;&#25022;&#30340;&#24230;&#37327;&#26469;&#35780;&#20272;&#30340;&#65292;&#22914;&#30636;&#26102;&#36951;&#25022;&#12289;&#31616;&#21333;&#36951;&#25022;&#21644;&#32047;&#31215;&#36951;&#25022;&#12290;&#36825;&#20123;&#24230;&#37327;&#20165;&#20381;&#36182;&#20110;&#20989;&#25968;&#35780;&#20272;&#65292;&#22240;&#27492;&#23427;&#20204;&#19981;&#32771;&#34385;&#26597;&#35810;&#28857;&#21644;&#20840;&#23616;&#35299;&#20043;&#38388;&#30340;&#20960;&#20309;&#20851;&#31995;&#65292;&#20063;&#19981;&#32771;&#34385;&#26597;&#35810;&#28857;&#26412;&#36523;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#23427;&#20204;&#19981;&#33021;&#21306;&#20998;&#26159;&#21542;&#25104;&#21151;&#25214;&#21040;&#20102;&#22810;&#20010;&#20840;&#23616;&#35299;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#20063;&#19981;&#33021;&#35780;&#20272;&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#32473;&#23450;&#25628;&#32034;&#31354;&#38388;&#20013;&#21033;&#29992;&#21644;&#25506;&#32034;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22235;&#20010;&#26032;&#30340;&#20960;&#20309;&#24230;&#37327;&#65292;&#21363;&#31934;&#30830;&#24230;&#12289;&#21484;&#22238;&#29575;&#12289;&#24179;&#22343;&#24230;&#21644;&#24179;&#22343;&#36317;&#31163;&#12290;&#36825;&#20123;&#24230;&#37327;&#20351;&#25105;&#20204;&#33021;&#22815;&#27604;&#36739;&#32771;&#34385;&#26597;&#35810;&#28857;&#21644;&#20840;&#23616;&#26368;&#20248;&#35299;&#30340;&#20960;&#20309;&#29305;&#24615;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian optimization is a principled optimization strategy for a black-box objective function. It shows its effectiveness in a wide variety of real-world applications such as scientific discovery and experimental design. In general, the performance of Bayesian optimization is assessed by regret-based metrics such as instantaneous, simple, and cumulative regrets. These metrics only rely on function evaluations, so that they do not consider geometric relationships between query points and global solutions, or query points themselves. Notably, they cannot discriminate if multiple global solutions are successfully found. Moreover, they do not evaluate Bayesian optimization's abilities to exploit and explore a search space given. To tackle these issues, we propose four new geometric metrics, i.e., precision, recall, average degree, and average distance. These metrics allow us to compare Bayesian optimization algorithms considering the geometry of both query points and global optima, or que
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#25968;&#25454;&#39537;&#21160;&#30340;&#12289;&#20445;&#25345;&#36523;&#20221;&#30340;&#12289;&#30643;&#23380;&#23610;&#23544;&#21464;&#21270;&#30340;&#34425;&#33180;&#22270;&#20687;&#21512;&#25104;&#26041;&#27861;&#65292;&#33021;&#22815;&#21512;&#25104;&#19981;&#21516;&#30643;&#23380;&#23610;&#23544;&#30340;&#34425;&#33180;&#22270;&#20687;&#65292;&#20195;&#34920;&#19981;&#23384;&#22312;&#30340;&#36523;&#20221;&#65292;&#24182;&#33021;&#22815;&#22312;&#20445;&#25345;&#36523;&#20221;&#30340;&#21516;&#26102;&#36827;&#34892;&#38750;&#32447;&#24615;&#32441;&#29702;&#21464;&#24418;&#12290;</title><link>http://arxiv.org/abs/2312.12028</link><description>&lt;p&gt;
EyePreserve: &#20445;&#25345;&#36523;&#20221;&#30340;&#34425;&#33180;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
EyePreserve: Identity-Preserving Iris Synthesis. (arXiv:2312.12028v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.12028
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#25968;&#25454;&#39537;&#21160;&#30340;&#12289;&#20445;&#25345;&#36523;&#20221;&#30340;&#12289;&#30643;&#23380;&#23610;&#23544;&#21464;&#21270;&#30340;&#34425;&#33180;&#22270;&#20687;&#21512;&#25104;&#26041;&#27861;&#65292;&#33021;&#22815;&#21512;&#25104;&#19981;&#21516;&#30643;&#23380;&#23610;&#23544;&#30340;&#34425;&#33180;&#22270;&#20687;&#65292;&#20195;&#34920;&#19981;&#23384;&#22312;&#30340;&#36523;&#20221;&#65292;&#24182;&#33021;&#22815;&#22312;&#20445;&#25345;&#36523;&#20221;&#30340;&#21516;&#26102;&#36827;&#34892;&#38750;&#32447;&#24615;&#32441;&#29702;&#21464;&#24418;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24191;&#27867;&#30340;&#30643;&#23380;&#23610;&#23544;&#33539;&#22260;&#20869;&#20445;&#25345;&#36523;&#20221;&#30340;&#21516;&#36523;&#20221;&#29983;&#29289;&#29305;&#24449;&#34425;&#33180;&#22270;&#20687;&#30340;&#21512;&#25104;&#26159;&#22797;&#26434;&#30340;&#65292;&#22240;&#20026;&#23427;&#28041;&#21450;&#21040;&#34425;&#33180;&#32908;&#32905;&#25910;&#32553;&#26426;&#21046;&#65292;&#38656;&#35201;&#23558;&#34425;&#33180;&#38750;&#32447;&#24615;&#32441;&#29702;&#21464;&#24418;&#27169;&#22411;&#23884;&#20837;&#21040;&#21512;&#25104;&#27969;&#31243;&#20013;&#12290;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#25968;&#25454;&#39537;&#21160;&#30340;&#12289;&#20445;&#25345;&#36523;&#20221;&#30340;&#12289;&#30643;&#23380;&#23610;&#23544;&#21464;&#21270;&#30340;&#34425;&#33180;&#22270;&#20687;&#21512;&#25104;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#21512;&#25104;&#20855;&#26377;&#19981;&#21516;&#30643;&#23380;&#23610;&#23544;&#30340;&#34425;&#33180;&#22270;&#20687;&#65292;&#20195;&#34920;&#19981;&#23384;&#22312;&#30340;&#36523;&#20221;&#65292;&#24182;&#33021;&#22815;&#22312;&#32473;&#23450;&#30446;&#26631;&#34425;&#33180;&#22270;&#20687;&#30340;&#20998;&#21106;&#25513;&#33180;&#19979;&#38750;&#32447;&#24615;&#22320;&#21464;&#24418;&#29616;&#26377;&#20027;&#20307;&#30340;&#34425;&#33180;&#22270;&#20687;&#32441;&#29702;&#12290;&#34425;&#33180;&#35782;&#21035;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#21464;&#24418;&#27169;&#22411;&#19981;&#20165;&#22312;&#25913;&#21464;&#30643;&#23380;&#23610;&#23544;&#26102;&#20445;&#25345;&#36523;&#20221;&#65292;&#32780;&#19988;&#22312;&#30643;&#23380;&#23610;&#23544;&#26377;&#26174;&#33879;&#24046;&#24322;&#30340;&#21516;&#36523;&#20221;&#34425;&#33180;&#26679;&#26412;&#20043;&#38388;&#25552;&#20379;&#26356;&#22909;&#30340;&#30456;&#20284;&#24230;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#32447;&#24615;&#26041;&#27861;&#30456;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
Synthesis of same-identity biometric iris images, both for existing and non-existing identities while preserving the identity across a wide range of pupil sizes, is complex due to intricate iris muscle constriction mechanism, requiring a precise model of iris non-linear texture deformations to be embedded into the synthesis pipeline. This paper presents the first method of fully data-driven, identity-preserving, pupil size-varying s ynthesis of iris images. This approach is capable of synthesizing images of irises with different pupil sizes representing non-existing identities as well as non-linearly deforming the texture of iris images of existing subjects given the segmentation mask of the target iris image. Iris recognition experiments suggest that the proposed deformation model not only preserves the identity when changing the pupil size but offers better similarity between same-identity iris samples with significant differences in pupil size, compared to state-of-the-art linear an
&lt;/p&gt;</description></item><item><title>LOTUS&#26159;&#19968;&#31181;&#25345;&#32493;&#27169;&#20223;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#65292;&#20351;&#24471;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#20854;&#25972;&#20010;&#23551;&#21629;&#20013;&#25345;&#32493;&#23398;&#20064;&#35299;&#20915;&#26032;&#30340;&#25805;&#20316;&#20219;&#21153;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#26500;&#24314;&#25216;&#33021;&#24211;&#65292;&#24182;&#20351;&#29992;&#20803;&#25511;&#21046;&#22120;&#28789;&#27963;&#32452;&#21512;&#25216;&#33021;&#26469;&#25552;&#39640;&#25104;&#21151;&#29575;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#30693;&#35782;&#20256;&#36882;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2311.02058</link><description>&lt;p&gt;
LOTUS&#65306;&#36890;&#36807;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#30340;&#25345;&#32493;&#27169;&#20223;&#23398;&#20064;&#65292;&#29992;&#20110;&#26426;&#22120;&#20154;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
LOTUS: Continual Imitation Learning for Robot Manipulation Through Unsupervised Skill Discovery. (arXiv:2311.02058v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.02058
&lt;/p&gt;
&lt;p&gt;
LOTUS&#26159;&#19968;&#31181;&#25345;&#32493;&#27169;&#20223;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#65292;&#20351;&#24471;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#20854;&#25972;&#20010;&#23551;&#21629;&#20013;&#25345;&#32493;&#23398;&#20064;&#35299;&#20915;&#26032;&#30340;&#25805;&#20316;&#20219;&#21153;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#26500;&#24314;&#25216;&#33021;&#24211;&#65292;&#24182;&#20351;&#29992;&#20803;&#25511;&#21046;&#22120;&#28789;&#27963;&#32452;&#21512;&#25216;&#33021;&#26469;&#25552;&#39640;&#25104;&#21151;&#29575;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#30693;&#35782;&#20256;&#36882;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;LOTUS&#30340;&#25345;&#32493;&#27169;&#20223;&#23398;&#20064;&#31639;&#27861;&#65292;&#23427;&#20351;&#24471;&#29289;&#29702;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#20854;&#25972;&#20010;&#23551;&#21629;&#20013;&#25345;&#32493;&#32780;&#39640;&#25928;&#22320;&#23398;&#20064;&#35299;&#20915;&#26032;&#30340;&#25805;&#20316;&#20219;&#21153;&#12290;LOTUS&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#36890;&#36807;&#19968;&#31995;&#21015;&#26032;&#20219;&#21153;&#30340;&#23569;&#37327;&#20154;&#31867;&#28436;&#31034;&#26500;&#24314;&#19968;&#20010;&#19981;&#26029;&#22686;&#38271;&#30340;&#25216;&#33021;&#24211;&#12290;LOTUS&#39318;&#20808;&#20351;&#29992;&#24320;&#25918;&#35789;&#27719;&#35270;&#35273;&#27169;&#22411;&#36827;&#34892;&#25345;&#32493;&#25216;&#33021;&#21457;&#29616;&#36807;&#31243;&#65292;&#35813;&#27169;&#22411;&#20174;&#26410;&#20998;&#27573;&#30340;&#28436;&#31034;&#20013;&#25552;&#21462;&#37325;&#22797;&#20986;&#29616;&#30340;&#25216;&#33021;&#27169;&#24335;&#12290;&#25345;&#32493;&#25216;&#33021;&#21457;&#29616;&#26356;&#26032;&#29616;&#26377;&#25216;&#33021;&#20197;&#36991;&#20813;&#23545;&#20197;&#21069;&#20219;&#21153;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#28155;&#21152;&#26032;&#25216;&#33021;&#20197;&#35299;&#20915;&#26032;&#20219;&#21153;&#12290;LOTUS&#35757;&#32451;&#19968;&#20010;&#20803;&#25511;&#21046;&#22120;&#65292;&#22312;&#32456;&#36523;&#23398;&#20064;&#36807;&#31243;&#20013;&#28789;&#27963;&#22320;&#32452;&#21512;&#21508;&#31181;&#25216;&#33021;&#26469;&#35299;&#20915;&#22522;&#20110;&#35270;&#35273;&#30340;&#25805;&#20316;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#32508;&#21512;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#20808;&#21069;&#26041;&#27861;&#30456;&#27604;&#65292;LOTUS&#22312;&#25104;&#21151;&#29575;&#19978;&#36229;&#36807;&#20102;&#29616;&#26377;&#25216;&#26415;&#22522;&#32447;&#26041;&#27861;11&#65285;&#20197;&#19978;&#65292;&#26174;&#31034;&#20102;&#20854;&#20248;&#36234;&#30340;&#30693;&#35782;&#20256;&#36882;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce LOTUS, a continual imitation learning algorithm that empowers a physical robot to continuously and efficiently learn to solve new manipulation tasks throughout its lifespan. The core idea behind LOTUS is constructing an ever-growing skill library from a sequence of new tasks with a small number of human demonstrations. LOTUS starts with a continual skill discovery process using an open-vocabulary vision model, which extracts skills as recurring patterns presented in unsegmented demonstrations. Continual skill discovery updates existing skills to avoid catastrophic forgetting of previous tasks and adds new skills to solve novel tasks. LOTUS trains a meta-controller that flexibly composes various skills to tackle vision-based manipulation tasks in the lifelong learning process. Our comprehensive experiments show that LOTUS outperforms state-of-the-art baselines by over 11% in success rate, showing its superior knowledge transfer ability compared to prior methods. More result
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#25554;&#20837;/&#21024;&#38500;&#25351;&#26631;&#24863;&#30693;&#30340;&#22522;&#20110;&#35299;&#37322;&#30340;&#20248;&#21270;(ID-ExpO)&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#21487;&#21306;&#20998;&#30340;&#39044;&#27979;&#22120;&#26469;&#25552;&#39640;&#35299;&#37322;&#30340;&#25554;&#20837;&#21644;&#21024;&#38500;&#24471;&#20998;&#65292;&#24182;&#20445;&#25345;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ID-ExpO&#33021;&#22815;&#20351;&#27969;&#34892;&#30340;&#20107;&#21518;&#35299;&#37322;&#22120;&#20135;&#29983;&#26356;&#24544;&#23454;&#30340;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2310.12553</link><description>&lt;p&gt;
&#21033;&#29992;&#21487;&#21306;&#20998;&#25554;&#20837;/&#21024;&#38500;&#25351;&#26631;&#24863;&#30693;&#27491;&#21017;&#21270;&#36827;&#34892;&#35299;&#37322;&#24615;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Explanation-Based Training with Differentiable Insertion/Deletion Metric-Aware Regularizers. (arXiv:2310.12553v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12553
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#25554;&#20837;/&#21024;&#38500;&#25351;&#26631;&#24863;&#30693;&#30340;&#22522;&#20110;&#35299;&#37322;&#30340;&#20248;&#21270;(ID-ExpO)&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#21487;&#21306;&#20998;&#30340;&#39044;&#27979;&#22120;&#26469;&#25552;&#39640;&#35299;&#37322;&#30340;&#25554;&#20837;&#21644;&#21024;&#38500;&#24471;&#20998;&#65292;&#24182;&#20445;&#25345;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ID-ExpO&#33021;&#22815;&#20351;&#27969;&#34892;&#30340;&#20107;&#21518;&#35299;&#37322;&#22120;&#20135;&#29983;&#26356;&#24544;&#23454;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#22120;&#30340;&#35299;&#37322;&#36136;&#37327;&#36890;&#24120;&#20351;&#29992;&#25554;&#20837;&#21644;&#21024;&#38500;&#25351;&#26631;&#36827;&#34892;&#34913;&#37327;&#65292;&#36825;&#20123;&#25351;&#26631;&#35780;&#20272;&#35299;&#37322;&#30340;&#24544;&#23454;&#24230;&#65292;&#21363;&#35299;&#37322;&#27491;&#30830;&#22320;&#21453;&#26144;&#20102;&#39044;&#27979;&#22120;&#30340;&#34892;&#20026;&#31243;&#24230;&#12290;&#20026;&#20102;&#25552;&#39640;&#24544;&#23454;&#24230;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25554;&#20837;/&#21024;&#38500;&#25351;&#26631;&#24863;&#30693;&#30340;&#22522;&#20110;&#35299;&#37322;&#30340;&#20248;&#21270;&#65288;ID-ExpO&#65289;&#65292;&#35813;&#20248;&#21270;&#33021;&#22815;&#25913;&#21892;&#35299;&#37322;&#30340;&#25554;&#20837;&#21644;&#21024;&#38500;&#24471;&#20998;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#30001;&#20110;&#21407;&#22987;&#30340;&#25554;&#20837;&#21644;&#21024;&#38500;&#25351;&#26631;&#23545;&#20110;&#35299;&#37322;&#26469;&#35828;&#26159;&#19981;&#21487;&#21306;&#20998;&#30340;&#65292;&#24182;&#19988;&#26080;&#27861;&#30452;&#25509;&#36827;&#34892;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#36825;&#20123;&#25351;&#26631;&#20197;&#20351;&#20854;&#21487;&#21306;&#20998;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#24418;&#24335;&#21270;&#25554;&#20837;&#21644;&#21024;&#38500;&#25351;&#26631;&#30340;&#27491;&#21017;&#21270;&#12290;&#22312;&#22270;&#20687;&#21644;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;ID-ExpO&#36827;&#34892;&#24494;&#35843;&#30340;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#22120;&#33021;&#22815;&#20351;&#27969;&#34892;&#30340;&#20107;&#21518;&#35299;&#37322;&#22120;&#20135;&#29983;&#26356;&#24544;&#23454;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
The quality of explanations for the predictions of complex machine learning predictors is often measured using insertion and deletion metrics, which assess the faithfulness of the explanations, i.e., how correctly the explanations reflect the predictor's behavior. To improve the faithfulness, we propose insertion/deletion metric-aware explanation-based optimization (ID-ExpO), which optimizes differentiable predictors to improve both insertion and deletion scores of the explanations while keeping their predictive accuracy. Since the original insertion and deletion metrics are indifferentiable with respect to the explanations and directly unavailable for gradient-based optimization, we extend the metrics to be differentiable and use them to formalize insertion and deletion metric-based regularizers. The experimental results on image and tabular datasets show that the deep neural networks-based predictors fine-tuned using ID-ExpO enable popular post-hoc explainers to produce more faithful
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22810;&#35270;&#22270;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#26469;&#22635;&#20805;&#38750;&#30446;&#26631;&#20195;&#35874;&#32452;&#23398;&#20013;&#30340;&#32570;&#22833;&#20540;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#20840;&#22522;&#22240;&#32452;&#27979;&#24207;&#25968;&#25454;&#21644;&#21442;&#32771;&#20195;&#35874;&#29289;&#30340;&#20449;&#24687;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#26681;&#25454;&#22522;&#22240;&#32452;&#20449;&#24687;&#22635;&#20805;&#32570;&#22833;&#30340;&#20195;&#35874;&#32452;&#23398;&#20540;&#12290;</title><link>http://arxiv.org/abs/2310.07990</link><description>&lt;p&gt;
&#22810;&#35270;&#22270;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#22312;&#38750;&#30446;&#26631;&#20195;&#35874;&#32452;&#23398;&#20013;&#32570;&#22833;&#20540;&#22635;&#20805;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Multi-View Variational Autoencoder for Missing Value Imputation in Untargeted Metabolomics. (arXiv:2310.07990v1 [q-bio.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07990
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22810;&#35270;&#22270;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#26469;&#22635;&#20805;&#38750;&#30446;&#26631;&#20195;&#35874;&#32452;&#23398;&#20013;&#30340;&#32570;&#22833;&#20540;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#20840;&#22522;&#22240;&#32452;&#27979;&#24207;&#25968;&#25454;&#21644;&#21442;&#32771;&#20195;&#35874;&#29289;&#30340;&#20449;&#24687;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#26681;&#25454;&#22522;&#22240;&#32452;&#20449;&#24687;&#22635;&#20805;&#32570;&#22833;&#30340;&#20195;&#35874;&#32452;&#23398;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#22312;&#22522;&#20110;&#36136;&#35889;&#30340;&#20195;&#35874;&#32452;&#23398;&#20013;&#65292;&#32570;&#22833;&#25968;&#25454;&#26159;&#19968;&#20010;&#24120;&#35265;&#30340;&#25361;&#25112;&#65292;&#21487;&#33021;&#23548;&#33268;&#20559;&#20506;&#21644;&#19981;&#23436;&#25972;&#30340;&#20998;&#26512;&#12290;&#23558;&#20840;&#22522;&#22240;&#32452;&#27979;&#24207;&#65288;WGS&#65289;&#25968;&#25454;&#19982;&#20195;&#35874;&#32452;&#23398;&#25968;&#25454;&#25972;&#21512;&#36215;&#26469;&#65292;&#24050;&#32463;&#25104;&#20026;&#22686;&#24378;&#20195;&#35874;&#32452;&#23398;&#30740;&#31350;&#20013;&#25968;&#25454;&#22635;&#20805;&#20934;&#30830;&#24615;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#12290;&#26041;&#27861;&#65306;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#26469;&#33258;WGS&#25968;&#25454;&#21644;&#21442;&#32771;&#20195;&#35874;&#29289;&#30340;&#20449;&#24687;&#26469;&#22635;&#20805;&#26410;&#30693;&#20195;&#35874;&#29289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#22810;&#35270;&#22270;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#20849;&#21516;&#23545;&#36127;&#25285;&#35780;&#20998;&#12289;&#22810;&#22522;&#22240;&#39118;&#38505;&#35780;&#20998;&#65288;PGS&#65289;&#21644;&#36830;&#38145;&#19981;&#24179;&#34913;&#65288;LD&#65289;&#21024;&#20943;&#30340;&#21333;&#26680;&#33527;&#37240;&#22810;&#24577;&#24615;&#65288;SNPs&#65289;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#21644;&#32570;&#22833;&#20195;&#35874;&#32452;&#23398;&#25968;&#25454;&#30340;&#22635;&#20805;&#12290;&#36890;&#36807;&#23398;&#20064;&#20004;&#31181;&#32452;&#23398;&#25968;&#25454;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26681;&#25454;&#22522;&#22240;&#32452;&#20449;&#24687;&#26377;&#25928;&#22320;&#22635;&#20805;&#32570;&#22833;&#30340;&#20195;&#35874;&#32452;&#23398;&#20540;&#12290;&#32467;&#26524;&#65306;&#25105;&#20204;&#22312;&#20855;&#26377;&#32570;&#22833;&#20540;&#21644;&#19981;&#23436;&#25972;&#25968;&#25454;&#30340;&#23454;&#39564;&#20195;&#35874;&#32452;&#23398;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Background: Missing data is a common challenge in mass spectrometry-based metabolomics, which can lead to biased and incomplete analyses. The integration of whole-genome sequencing (WGS) data with metabolomics data has emerged as a promising approach to enhance the accuracy of data imputation in metabolomics studies. Method: In this study, we propose a novel method that leverages the information from WGS data and reference metabolites to impute unknown metabolites. Our approach utilizes a multi-view variational autoencoder to jointly model the burden score, polygenetic risk score (PGS), and linkage disequilibrium (LD) pruned single nucleotide polymorphisms (SNPs) for feature extraction and missing metabolomics data imputation. By learning the latent representations of both omics data, our method can effectively impute missing metabolomics values based on genomic information. Results: We evaluate the performance of our method on empirical metabolomics datasets with missing values and de
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#65288;BDFL&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#21306;&#22359;&#38142;&#36827;&#34892;&#20998;&#25955;&#24335;&#27169;&#22411;&#39564;&#35777;&#21644;&#23457;&#35745;&#65292;&#35299;&#20915;&#20102;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#20013;&#34394;&#20551;&#27169;&#22411;&#21644;&#25968;&#25454;&#25915;&#20987;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.07079</link><description>&lt;p&gt;
&#20351;&#29992;&#21306;&#22359;&#38142;&#23454;&#29616;&#23433;&#20840;&#30340;&#20998;&#25955;&#24335;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Secure Decentralized Learning with Blockchain. (arXiv:2310.07079v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07079
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#65288;BDFL&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#21306;&#22359;&#38142;&#36827;&#34892;&#20998;&#25955;&#24335;&#27169;&#22411;&#39564;&#35777;&#21644;&#23457;&#35745;&#65292;&#35299;&#20915;&#20102;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#20013;&#34394;&#20551;&#27169;&#22411;&#21644;&#25968;&#25454;&#25915;&#20987;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#22312;&#31227;&#21160;&#35774;&#22791;&#21644;&#29289;&#32852;&#32593;&#35774;&#22791;&#19978;&#30340;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#23427;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#24182;&#20248;&#21270;&#36890;&#20449;&#25928;&#29575;&#12290;&#20026;&#20102;&#36991;&#20813;&#32852;&#37030;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#21333;&#28857;&#25925;&#38556;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#65288;DFL&#65289;&#65292;&#21033;&#29992;&#28857;&#23545;&#28857;&#36890;&#20449;&#36827;&#34892;&#27169;&#22411;&#32858;&#21512;&#65292;&#36825;&#34987;&#35748;&#20026;&#26159;&#20998;&#24067;&#24335;&#20010;&#20154;&#35774;&#22791;&#19978;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#30340;&#26377;&#21560;&#24341;&#21147;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#36807;&#31243;&#23481;&#26131;&#21463;&#21040;&#20849;&#20139;&#34394;&#20551;&#27169;&#22411;&#21644;&#25968;&#25454;&#30340;&#25915;&#20987;&#32773;&#30340;&#25915;&#20987;&#12290;&#22914;&#26524;&#23384;&#22312;&#19968;&#32452;&#24694;&#24847;&#23458;&#25143;&#31471;&#65292;&#20182;&#20204;&#21487;&#33021;&#36890;&#36807;&#36827;&#34892;&#25237;&#27602;&#25915;&#20987;&#26469;&#25439;&#23475;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#22312;DFL&#20013;&#65292;&#23458;&#25143;&#31471;&#36890;&#24120;&#32570;&#20047;&#28608;&#21169;&#26469;&#36129;&#29486;&#20182;&#20204;&#30340;&#35745;&#31639;&#33021;&#21147;&#26469;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#65288;BDFL&#65289;&#65292;&#23427;&#21033;&#29992;&#21306;&#22359;&#38142;&#36827;&#34892;&#20998;&#25955;&#24335;&#27169;&#22411;&#39564;&#35777;&#21644;&#23457;&#35745;&#12290;BDFL&#21253;&#25324;&#19968;&#20010;&#23457;&#26680;&#22996;&#21592;&#20250;&#29992;&#20110;&#27169;&#22411;&#39564;&#35777;&#65292;&#19968;&#20010;...
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a well-known paradigm of distributed machine learning on mobile and IoT devices, which preserves data privacy and optimizes communication efficiency. To avoid the single point of failure problem in FL, decentralized federated learning (DFL) has been proposed to use peer-to-peer communication for model aggregation, which has been considered an attractive solution for machine learning tasks on distributed personal devices. However, this process is vulnerable to attackers who share false models and data. If there exists a group of malicious clients, they might harm the performance of the model by carrying out a poisoning attack. In addition, in DFL, clients often lack the incentives to contribute their computing powers to do model training. In this paper, we proposed Blockchain-based Decentralized Federated Learning (BDFL), which leverages a blockchain for decentralized model verification and auditing. BDFL includes an auditor committee for model verification, a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#25552;&#31034;&#25216;&#26415;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#25277;&#35937;&#33719;&#24471;&#39640;&#23618;&#27010;&#24565;&#21644;&#22522;&#26412;&#21407;&#29702;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#25512;&#29702;&#36335;&#24452;&#20013;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#21319;&#27169;&#22411;&#22312;&#21508;&#31181;&#25512;&#29702;&#23494;&#38598;&#22411;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.06117</link><description>&lt;p&gt;
&#36864;&#21518;&#19968;&#27493;&#65306;&#36890;&#36807;&#25277;&#35937;&#21796;&#36215;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models. (arXiv:2310.06117v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06117
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#25552;&#31034;&#25216;&#26415;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#25277;&#35937;&#33719;&#24471;&#39640;&#23618;&#27010;&#24565;&#21644;&#22522;&#26412;&#21407;&#29702;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#25512;&#29702;&#36335;&#24452;&#20013;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#21319;&#27169;&#22411;&#22312;&#21508;&#31181;&#25512;&#29702;&#23494;&#38598;&#22411;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#36864;&#21518;&#25552;&#31034;&#8221;&#30340;&#31616;&#21333;&#25552;&#31034;&#25216;&#26415;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#20174;&#21253;&#21547;&#20855;&#20307;&#32454;&#33410;&#30340;&#23454;&#20363;&#20013;&#36827;&#34892;&#25277;&#35937;&#65292;&#24471;&#20986;&#39640;&#23618;&#27010;&#24565;&#21644;&#22522;&#26412;&#21407;&#29702;&#12290;&#21033;&#29992;&#36825;&#20123;&#27010;&#24565;&#21644;&#21407;&#29702;&#26469;&#25351;&#23548;&#25512;&#29702;&#27493;&#39588;&#65292;&#35821;&#35328;&#27169;&#22411;&#22312;&#27491;&#30830;&#25512;&#29702;&#36335;&#24452;&#19978;&#26174;&#33879;&#25552;&#21319;&#20102;&#33021;&#21147;&#12290;&#25105;&#20204;&#20351;&#29992;PaLM-2L&#27169;&#22411;&#36827;&#34892;&#20102;&#36864;&#21518;&#25552;&#31034;&#23454;&#39564;&#65292;&#22312;&#21253;&#25324;STEM&#12289;&#30693;&#35782;&#38382;&#31572;&#21644;&#22810;&#36339;&#25512;&#29702;&#22312;&#20869;&#30340;&#21508;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25512;&#29702;&#23494;&#38598;&#22411;&#20219;&#21153;&#19978;&#35266;&#23519;&#21040;&#20102;&#26126;&#26174;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#20363;&#22914;&#65292;&#22312;MMLU&#29289;&#29702;&#21644;&#21270;&#23398;&#20219;&#21153;&#19978;&#65292;&#36864;&#21518;&#25552;&#31034;&#21487;&#20197;&#23558;PaLM-2L&#30340;&#24615;&#33021;&#25552;&#21319;7%&#21644;11%&#65292;&#22312;TimeQA&#20219;&#21153;&#19978;&#25552;&#21319;27%&#65292;&#22312;MuSiQue&#20219;&#21153;&#19978;&#25552;&#21319;7%&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Step-Back Prompting, a simple prompting technique that enables LLMs to do abstractions to derive high-level concepts and first principles from instances containing specific details. Using the concepts and principles to guide the reasoning steps, LLMs significantly improve their abilities in following a correct reasoning path towards the solution. We conduct experiments of Step-Back Prompting with PaLM-2L models and observe substantial performance gains on a wide range of challenging reasoning-intensive tasks including STEM, Knowledge QA, and Multi-Hop Reasoning. For instance, Step-Back Prompting improves PaLM-2L performance on MMLU Physics and Chemistry by 7% and 11%, TimeQA by 27%, and MuSiQue by 7%.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#37319;&#29992;&#36136;&#37327;-&#22810;&#26679;&#24615;&#26041;&#27861;&#30340;&#27969;&#27700;&#32447;&#65292;&#29992;&#20110;&#23558;&#29983;&#25104;&#30340;&#25235;&#21462;&#36712;&#36857;&#36866;&#24212;&#21040;&#26032;&#29289;&#20307;&#23039;&#24577;&#12290;&#20351;&#29992;RGB-D&#25968;&#25454;&#27969;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#33258;&#21160;&#26816;&#27979;&#30446;&#26631;&#29289;&#20307;&#12289;&#39044;&#27979;&#20854;&#23039;&#24577;&#65292;&#24182;&#29983;&#25104;&#21487;&#36798;&#21040;&#30340;&#25235;&#21462;&#36712;&#36857;&#12290;</title><link>http://arxiv.org/abs/2310.04349</link><description>&lt;p&gt;
&#20174;&#26576;&#22788;&#21040;&#20219;&#20309;&#22320;&#26041;&#30340;&#23398;&#20064;&#65306;&#25235;&#21462;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Learning to Grasp: from Somewhere to Anywhere. (arXiv:2310.04349v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04349
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#37319;&#29992;&#36136;&#37327;-&#22810;&#26679;&#24615;&#26041;&#27861;&#30340;&#27969;&#27700;&#32447;&#65292;&#29992;&#20110;&#23558;&#29983;&#25104;&#30340;&#25235;&#21462;&#36712;&#36857;&#36866;&#24212;&#21040;&#26032;&#29289;&#20307;&#23039;&#24577;&#12290;&#20351;&#29992;RGB-D&#25968;&#25454;&#27969;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#33258;&#21160;&#26816;&#27979;&#30446;&#26631;&#29289;&#20307;&#12289;&#39044;&#27979;&#20854;&#23039;&#24577;&#65292;&#24182;&#29983;&#25104;&#21487;&#36798;&#21040;&#30340;&#25235;&#21462;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#25235;&#21462;&#20173;&#28982;&#26159;&#19968;&#20010;&#37096;&#20998;&#35299;&#20915;&#30340;&#12289;&#22810;&#23398;&#31185;&#30340;&#38382;&#39064;&#65292;&#22312;&#20854;&#20013;&#25968;&#25454;&#39537;&#21160;&#30340;&#25216;&#26415;&#21457;&#25381;&#30528;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#22870;&#21169;&#30340;&#31232;&#30095;&#24615;&#20351;&#24471;&#33258;&#21160;&#29983;&#25104;&#25235;&#21462;&#25968;&#25454;&#38598;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#38750;&#20256;&#32479;&#24418;&#24577;&#25110;&#39640;&#24230;&#39537;&#21160;&#30340;&#26411;&#31471;&#25191;&#34892;&#22120;&#12290;&#33719;&#24471;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#22823;&#22810;&#25968;&#26041;&#27861;&#20381;&#36182;&#20110;&#20247;&#22810;&#20154;&#24037;&#25552;&#20379;&#30340;&#28436;&#31034;&#25110;&#20005;&#37325;&#24037;&#31243;&#21270;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36825;&#20123;&#26041;&#27861;&#24456;&#38590;&#25193;&#23637;&#12290;&#26368;&#26032;&#30340;&#36136;&#37327;-&#22810;&#26679;&#24615;&#65288;QD&#65289;&#26041;&#27861;&#30340;&#36827;&#23637;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;&#19981;&#21516;&#26426;&#22120;&#20154;&#24418;&#24577;&#23398;&#20064;&#29305;&#23450;&#23039;&#21183;&#19979;&#30340;&#29289;&#20307;&#25235;&#21462;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#23558;QD&#29983;&#25104;&#30340;&#36712;&#36857;&#36866;&#24212;&#21040;&#26032;&#29289;&#20307;&#23039;&#24577;&#30340;&#27969;&#27700;&#32447;&#12290;&#20351;&#29992;RGB-D&#25968;&#25454;&#27969;&#65292;&#35270;&#35273;&#27969;&#27700;&#32447;&#39318;&#20808;&#26816;&#27979;&#30446;&#26631;&#29289;&#20307;&#65292;&#39044;&#27979;&#20854;6&#33258;&#30001;&#24230;&#23039;&#24577;&#65292;&#26368;&#21518;&#36319;&#36394;&#23427;&#12290;&#28982;&#21518;&#36890;&#36807;&#23558;&#36712;&#36857;&#30456;&#23545;&#20110;&#29289;&#20307;&#26694;&#26550;&#36827;&#34892;&#25237;&#24433;&#26469;&#33258;&#21160;&#29983;&#25104;&#21487;&#36798;&#21040;&#30340;&#25235;&#21462;&#36712;&#36857;&#12290;&#25968;&#30334;&#20010;&#36712;&#36857;&#24050;&#32463;&#22312;&#23454;&#39564;&#20013;&#37096;&#32626;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robotic grasping is still a partially solved, multidisciplinary problem where data-driven techniques play an increasing role. The sparse nature of rewards make the automatic generation of grasping datasets challenging, especially for unconventional morphologies or highly actuated end-effectors. Most approaches for obtaining large-scale datasets rely on numerous human-provided demonstrations or heavily engineered solutions that do not scale well. Recent advances in Quality-Diversity (QD) methods have investigated how to learn object grasping at a specific pose with different robot morphologies. The present work introduces a pipeline for adapting QD-generated trajectories to new object poses. Using an RGB-D data stream, the vision pipeline first detects the targeted object, predicts its 6-DOF pose, and finally tracks it. An automatically generated reach-and-grasp trajectory can then be adapted by projecting it relatively to the object frame. Hundreds of trajectories have been deployed in
&lt;/p&gt;</description></item><item><title>TRAM&#26159;&#19968;&#31181;&#26725;&#25509;&#20449;&#20219;&#21306;&#22495;&#21644;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#20943;&#23569;&#25439;&#22833;&#26354;&#38754;&#30340;&#26354;&#29575;&#26469;&#25552;&#20379;&#40065;&#26834;&#24615;&#25913;&#36827;&#12290;&#23427;&#36890;&#36807;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#20248;&#21270;&#21487;&#36716;&#31227;&#30340;&#34920;&#31034;&#26469;&#23454;&#29616;&#39046;&#22495;&#22806;&#27867;&#21270;&#65292;&#24182;&#19988;&#36890;&#36807;&#32467;&#21512;&#20449;&#20219;&#21306;&#22495;&#26041;&#27861;&#21644;SAM&#39118;&#26684;&#30340;&#27491;&#21017;&#21270;&#22120;&#26469;&#32479;&#19968;&#21442;&#25968;&#21644;&#34920;&#31034;&#31354;&#38388;&#24179;&#28369;&#26041;&#27861;&#12290;TRAM&#22312;&#20445;&#25345;&#39044;&#35757;&#32451;&#32467;&#26500;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#24179;&#22374;&#30340;&#26497;&#23567;&#20540;&#21644;&#24179;&#28369;&#12289;&#26377;&#20449;&#24687;&#37327;&#30340;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2310.03646</link><description>&lt;p&gt;
TRAM: &#26725;&#25509;&#20449;&#20219;&#21306;&#22495;&#21644;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;
&lt;/p&gt;
&lt;p&gt;
TRAM: Bridging Trust Regions and Sharpness Aware Minimization. (arXiv:2310.03646v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03646
&lt;/p&gt;
&lt;p&gt;
TRAM&#26159;&#19968;&#31181;&#26725;&#25509;&#20449;&#20219;&#21306;&#22495;&#21644;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#20943;&#23569;&#25439;&#22833;&#26354;&#38754;&#30340;&#26354;&#29575;&#26469;&#25552;&#20379;&#40065;&#26834;&#24615;&#25913;&#36827;&#12290;&#23427;&#36890;&#36807;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#20248;&#21270;&#21487;&#36716;&#31227;&#30340;&#34920;&#31034;&#26469;&#23454;&#29616;&#39046;&#22495;&#22806;&#27867;&#21270;&#65292;&#24182;&#19988;&#36890;&#36807;&#32467;&#21512;&#20449;&#20219;&#21306;&#22495;&#26041;&#27861;&#21644;SAM&#39118;&#26684;&#30340;&#27491;&#21017;&#21270;&#22120;&#26469;&#32479;&#19968;&#21442;&#25968;&#21644;&#34920;&#31034;&#31354;&#38388;&#24179;&#28369;&#26041;&#27861;&#12290;TRAM&#22312;&#20445;&#25345;&#39044;&#35757;&#32451;&#32467;&#26500;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#24179;&#22374;&#30340;&#26497;&#23567;&#20540;&#21644;&#24179;&#28369;&#12289;&#26377;&#20449;&#24687;&#37327;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20943;&#23569;&#21442;&#25968;&#31354;&#38388;&#20013;&#25439;&#22833;&#26354;&#38754;&#30340;&#26354;&#29575;&#65292;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#65288;SAM&#65289;&#22312;&#39046;&#22495;&#36716;&#31227;&#19979;&#25552;&#20379;&#20102;&#24191;&#27867;&#30340;&#40065;&#26834;&#24615;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#26412;&#25991;&#19981;&#26159;&#20851;&#27880;&#21442;&#25968;&#65292;&#32780;&#26159;&#32771;&#34385;&#21040;&#34920;&#31034;&#30340;&#21487;&#36716;&#31227;&#24615;&#20316;&#20026;&#20248;&#21270;&#30446;&#26631;&#65292;&#22312;&#24494;&#35843;&#35774;&#32622;&#20013;&#23454;&#29616;&#39046;&#22495;&#22806;&#27867;&#21270;&#12290;&#20026;&#20102;&#40723;&#21169;&#20445;&#30041;&#21487;&#36716;&#31227;&#30340;&#34920;&#31034;&#65292;&#25105;&#20204;&#32771;&#34385;&#21040;&#22522;&#20110;&#20449;&#20219;&#21306;&#22495;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#21033;&#29992;&#20219;&#21153;&#29305;&#23450;&#30340;&#25216;&#33021;&#65292;&#32780;&#19981;&#20250;&#24536;&#35760;&#39044;&#35757;&#32451;&#30340;&#20219;&#21153;&#26080;&#20851;&#34920;&#31034;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#20449;&#20219;&#21306;&#22495;&#36793;&#30028;&#22312;&#36825;&#20004;&#31181;&#20248;&#21270;&#34920;&#38754;&#19978;&#36890;&#30693;SAM&#39118;&#26684;&#30340;&#27491;&#21017;&#21270;&#22120;&#65292;&#32479;&#19968;&#20102;&#21442;&#25968;&#21644;&#34920;&#31034;&#31354;&#38388;&#24179;&#28369;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Trust Region Aware Minimization (TRAM)&#65292;&#19968;&#31181;&#20248;&#21270;&#24179;&#22374;&#30340;&#26497;&#23567;&#20540;&#21644;&#24179;&#28369;&#12289;&#26377;&#20449;&#24687;&#37327;&#30340;&#34920;&#31034;&#30340;&#24494;&#35843;&#31639;&#27861;&#65292;&#32780;&#19981;&#20250;&#24536;&#35760;&#39044;&#20808;&#35757;&#32451;&#30340;&#32467;&#26500;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;TRAM&#20248;&#20110;&#38160;&#24230;&#24863;&#30693;&#21644;&#22522;&#20110;&#20449;&#20219;&#21306;&#22495;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
By reducing the curvature of the loss surface in the parameter space, Sharpness-aware minimization (SAM) yields widespread robustness improvement under domain transfer. Instead of focusing on parameters, however, this work considers the transferability of representations as the optimization target for out-of-domain generalization in a fine-tuning setup. To encourage the retention of transferable representations, we consider trust region-based fine-tuning methods, which exploit task-specific skills without forgetting task-agnostic representations from pre-training. We unify parameter- and representation-space smoothing approaches by using trust region bounds to inform SAM-style regularizers on both of these optimization surfaces. We propose Trust Region Aware Minimization (TRAM), a fine-tuning algorithm that optimizes for flat minima and smooth, informative representations without forgetting pre-trained structure. We find that TRAM outperforms both sharpness-aware and trust region-based
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#20013;&#38388;&#23618;&#21464;&#25442;&#30340;&#24179;&#28369;&#24615;&#26469;&#26816;&#27979;&#24102;&#22806;&#25968;&#25454;&#30340;&#26041;&#27861;(BLOOD),&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#27809;&#26377;&#35757;&#32451;&#25968;&#25454;&#35775;&#38382;&#26435;&#38480;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#22312;Transformer&#32593;&#32476;&#19978;&#30340;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.02832</link><description>&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#23618;&#38388;&#21464;&#25442;&#30340;&#24179;&#28369;&#24615;&#36827;&#34892;&#24102;&#22806;&#20998;&#24067;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Out-of-Distribution Detection by Leveraging Between-Layer Transformation Smoothness. (arXiv:2310.02832v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02832
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#20013;&#38388;&#23618;&#21464;&#25442;&#30340;&#24179;&#28369;&#24615;&#26469;&#26816;&#27979;&#24102;&#22806;&#25968;&#25454;&#30340;&#26041;&#27861;(BLOOD),&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#27809;&#26377;&#35757;&#32451;&#25968;&#25454;&#35775;&#38382;&#26435;&#38480;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#22312;Transformer&#32593;&#32476;&#19978;&#30340;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30340;&#24102;&#22806;&#20998;&#24067;&#26816;&#27979;&#23545;&#20110;&#21487;&#38752;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#65292;&#28982;&#32780;&#22823;&#22810;&#25968;&#24403;&#21069;&#26041;&#27861;&#30001;&#20110;&#38656;&#35201;&#35775;&#38382;&#35757;&#32451;&#25968;&#25454;&#25110;&#32773;&#24178;&#39044;&#35757;&#32451;&#32780;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21463;&#21040;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32593;&#32476;&#20013;&#38388;&#23618;&#30340;&#21464;&#25442;&#24179;&#28369;&#24615;&#26469;&#26816;&#27979;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24102;&#22806;&#25968;&#25454;&#65288;BLOOD&#65289;&#65292;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#27809;&#26377;&#35757;&#32451;&#25968;&#25454;&#35775;&#38382;&#26435;&#38480;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;BLOOD&#21033;&#29992;&#20869;&#20998;&#24067;&#65288;ID&#65289;&#25968;&#25454;&#30340;&#23618;&#38388;&#34920;&#31034;&#21464;&#25442;&#30456;&#36739;&#20110;&#24102;&#22806;&#25968;&#25454;&#30340;&#21464;&#25442;&#26356;&#24179;&#28369;&#30340;&#20542;&#21521;&#65292;&#36825;&#20063;&#26159;&#25105;&#20204;&#22312;Transformer&#32593;&#32476;&#20013;&#32463;&#39564;&#35777;&#26126;&#30340;&#19968;&#20010;&#29305;&#24615;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;BLOOD&#19982;Transformer&#32593;&#32476;&#65292;&#24182;&#35777;&#26126;&#20854;&#22312;&#36164;&#28304;&#38656;&#27714;&#30456;&#24403;&#30340;&#26041;&#27861;&#19978;&#24615;&#33021;&#26356;&#22909;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#36824;&#34920;&#26126;&#65292;&#24403;&#23398;&#20064;&#26356;&#31616;&#21333;&#30340;&#20219;&#21153;&#26102;&#65292;&#24102;&#22806;&#25968;&#25454;&#30340;&#21464;&#25442;&#20250;&#20445;&#25345;&#20854;&#21407;&#22987;&#30340;&#38160;&#24230;&#65292;&#32780;&#38160;&#24230;&#20250;&#38543;&#30528;&#20219;&#21153;&#30340;&#22686;&#21152;&#32780;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
Effective OOD detection is crucial for reliable machine learning models, yet most current methods are limited in practical use due to requirements like access to training data or intervention in training. We present a novel method for detecting OOD data in deep neural networks based on transformation smoothness between intermediate layers of a network (BLOOD), which is applicable to pre-trained models without access to training data. BLOOD utilizes the tendency of between-layer representation transformations of in-distribution (ID) data to be smoother than the corresponding transformations of OOD data, a property that we also demonstrate empirically for Transformer networks. We evaluate BLOOD on several text classification tasks with Transformer networks and demonstrate that it outperforms methods with comparable resource requirements. Our analysis also suggests that when learning simpler tasks, OOD data transformations maintain their original sharpness, whereas sharpness increases wit
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GRANDE&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#20915;&#31574;&#26641;&#38598;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#31471;&#21040;&#31471;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#22362;&#30828;&#12289;&#36724;&#23545;&#40784;&#30340;&#20915;&#31574;&#26641;&#38598;&#25104;&#65292;&#24182;&#32467;&#21512;&#20102;&#36724;&#23545;&#40784;&#20998;&#21106;&#21644;&#26799;&#24230;&#20248;&#21270;&#30340;&#28789;&#27963;&#24615;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#36880;&#20010;&#23454;&#20363;&#21152;&#26435;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19968;&#20010;&#27169;&#22411;&#20013;&#20415;&#20110;&#23398;&#20064;&#31616;&#21333;&#21644;&#22797;&#26434;&#20851;&#31995;&#30340;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2309.17130</link><description>&lt;p&gt;
GRANDE: &#22522;&#20110;&#26799;&#24230;&#30340;&#20915;&#31574;&#26641;&#38598;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GRANDE: Gradient-Based Decision Tree Ensembles. (arXiv:2309.17130v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17130
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GRANDE&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#20915;&#31574;&#26641;&#38598;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#31471;&#21040;&#31471;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#22362;&#30828;&#12289;&#36724;&#23545;&#40784;&#30340;&#20915;&#31574;&#26641;&#38598;&#25104;&#65292;&#24182;&#32467;&#21512;&#20102;&#36724;&#23545;&#40784;&#20998;&#21106;&#21644;&#26799;&#24230;&#20248;&#21270;&#30340;&#28789;&#27963;&#24615;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#36880;&#20010;&#23454;&#20363;&#21152;&#26435;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19968;&#20010;&#27169;&#22411;&#20013;&#20415;&#20110;&#23398;&#20064;&#31616;&#21333;&#21644;&#22797;&#26434;&#20851;&#31995;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#22312;&#25991;&#26412;&#21644;&#22270;&#20687;&#25968;&#25454;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#22522;&#20110;&#26641;&#30340;&#38598;&#25104;&#27169;&#22411;&#20173;&#28982;&#26159;&#22788;&#29702;&#24322;&#26500;&#34920;&#26684;&#25968;&#25454;&#30340;&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#39640;&#28789;&#27963;&#24615;&#65292;&#23545;&#20110;&#34920;&#26684;&#25968;&#25454;&#26469;&#35828;&#65292;&#23384;&#22312;&#23545;&#29305;&#23450;&#20110;&#34920;&#26684;&#30340;&#26799;&#24230;&#26041;&#27861;&#30340;&#26174;&#33879;&#38656;&#27714;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GRANDE&#30340;&#26032;&#26041;&#27861;&#65292;&#21363;&#22522;&#20110;&#26799;&#24230;&#30340;&#20915;&#31574;&#26641;&#38598;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#31471;&#21040;&#31471;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#22362;&#30828;&#12289;&#36724;&#23545;&#40784;&#30340;&#20915;&#31574;&#26641;&#38598;&#25104;&#12290;GRANDE&#22522;&#20110;&#20915;&#31574;&#26641;&#38598;&#25104;&#30340;&#31264;&#23494;&#34920;&#31034;&#65292;&#21487;&#20197;&#20351;&#29992;&#30452;&#36890;&#25805;&#20316;&#31526;&#21644;&#21453;&#21521;&#20256;&#25773;&#19968;&#36215;&#20248;&#21270;&#25152;&#26377;&#27169;&#22411;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#36724;&#23545;&#40784;&#20998;&#21106;&#65288;&#36825;&#26159;&#34920;&#26684;&#25968;&#25454;&#30340;&#19968;&#20010;&#26377;&#29992;&#30340;&#24402;&#32435;&#20559;&#32622;&#65289;&#21644;&#26799;&#24230;&#20248;&#21270;&#30340;&#28789;&#27963;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#36880;&#20010;&#23454;&#20363;&#21152;&#26435;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19968;&#20010;&#27169;&#22411;&#20013;&#20415;&#20110;&#23398;&#20064;&#31616;&#21333;&#21644;&#22797;&#26434;&#20851;&#31995;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#22312;&#24191;&#27867;&#30340;&#23454;&#39564;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;GRANDE&#30340;&#24615;&#33021;&#65292;&#24182;&#19982;&#20854;&#20182;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the success of deep learning for text and image data, tree-based ensemble models are still state-of-the-art for machine learning with heterogeneous tabular data. However, there is a significant need for tabular-specific gradient-based methods due to their high flexibility. In this paper, we propose $\text{GRANDE}$, $\text{GRA}$die$\text{N}$t-Based $\text{D}$ecision Tree $\text{E}$nsembles, a novel approach for learning hard, axis-aligned decision tree ensembles using end-to-end gradient descent. GRANDE is based on a dense representation of tree ensembles, which affords to use backpropagation with a straight-through operator to jointly optimize all model parameters. Our method combines axis-aligned splits, which is a useful inductive bias for tabular data, with the flexibility of gradient-based optimization. Furthermore, we introduce an advanced instance-wise weighting that facilitates learning representations for both, simple and complex relations, within a single model. We con
&lt;/p&gt;</description></item><item><title>MINT&#26159;&#19968;&#20010;&#35780;&#20272;LLMs&#22312;&#22810;&#36718;&#20132;&#20114;&#20013;&#35299;&#20915;&#20219;&#21153;&#33021;&#21147;&#30340;&#22522;&#20934;&#65292;&#36890;&#36807;&#20351;&#29992;&#24037;&#20855;&#21644;&#21033;&#29992;&#29992;&#25143;&#30340;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#12290;&#23427;&#35299;&#20915;&#20102;&#24403;&#21069;&#35780;&#20272;&#21327;&#35758;&#24573;&#30053;&#32454;&#33268;&#20114;&#21160;&#21644;&#20302;&#20272;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#30340;&#38382;&#39064;&#65292;&#20419;&#36827;&#20102;&#30740;&#31350;&#22522;&#20934;&#35780;&#20272;&#21644;&#23454;&#38469;&#24212;&#29992;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.10691</link><description>&lt;p&gt;
MINT: &#35780;&#20272;&#22312;&#19982;&#24037;&#20855;&#21644;&#35821;&#35328;&#21453;&#39304;&#36827;&#34892;&#22810;&#36718;&#20132;&#20114;&#20013;&#30340;LLMs&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
MINT: Evaluating LLMs in Multi-turn Interaction with Tools and Language Feedback. (arXiv:2309.10691v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10691
&lt;/p&gt;
&lt;p&gt;
MINT&#26159;&#19968;&#20010;&#35780;&#20272;LLMs&#22312;&#22810;&#36718;&#20132;&#20114;&#20013;&#35299;&#20915;&#20219;&#21153;&#33021;&#21147;&#30340;&#22522;&#20934;&#65292;&#36890;&#36807;&#20351;&#29992;&#24037;&#20855;&#21644;&#21033;&#29992;&#29992;&#25143;&#30340;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#12290;&#23427;&#35299;&#20915;&#20102;&#24403;&#21069;&#35780;&#20272;&#21327;&#35758;&#24573;&#30053;&#32454;&#33268;&#20114;&#21160;&#21644;&#20302;&#20272;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#30340;&#38382;&#39064;&#65292;&#20419;&#36827;&#20102;&#30740;&#31350;&#22522;&#20934;&#35780;&#20272;&#21644;&#23454;&#38469;&#24212;&#29992;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#65292;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#24120;&#38656;&#35201;&#19982;&#29992;&#25143;&#36827;&#34892;&#22810;&#36718;&#20132;&#20114;&#65292;&#26377;&#26102;&#20505;&#36741;&#20197;&#22806;&#37096;&#24037;&#20855;&#30340;&#24110;&#21161;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#35780;&#20272;&#21327;&#35758;&#24120;&#24120;&#24378;&#35843;&#29992;&#21333;&#36718;&#20132;&#27969;&#30340;&#22522;&#20934;&#24615;&#33021;&#65292;&#24573;&#30053;&#20102;&#29992;&#25143;&#12289;LLMs&#21644;&#22806;&#37096;&#24037;&#20855;&#20043;&#38388;&#30340;&#32454;&#33268;&#20114;&#21160;&#65292;&#24182;&#20302;&#20272;&#20102;&#29992;&#25143;&#30340;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#30340;&#37325;&#35201;&#24615;&#12290;&#36825;&#20123;&#30095;&#24573;&#23548;&#33268;&#20102;&#30740;&#31350;&#22522;&#20934;&#35780;&#20272;&#32467;&#26524;&#19982;&#23454;&#38469;&#24212;&#29992;&#24773;&#20917;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;MINT&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#36807;&#20351;&#29992;&#24037;&#20855;&#21644;&#21033;&#29992;&#29992;&#25143;&#30340;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#26469;&#35780;&#20272;LLMs&#35299;&#20915;&#22810;&#36718;&#20132;&#20114;&#20219;&#21153;&#33021;&#21147;&#30340;&#22522;&#20934;&#12290;&#20026;&#20102;&#20445;&#35777;&#21487;&#37325;&#22797;&#24615;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#35780;&#20272;&#26694;&#26550;&#65292;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;LLMs&#21487;&#20197;&#36890;&#36807;&#25191;&#34892;Python&#20195;&#30721;&#26469;&#35775;&#38382;&#24037;&#20855;&#65292;&#24182;&#25509;&#25910;&#30001;GPT-4&#27169;&#25311;&#30340;&#29992;&#25143;&#30340;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#12290;&#25105;&#20204;&#37325;&#26032;&#21033;&#29992;&#20102;&#19968;&#31995;&#21015;&#22810;&#26679;&#30340;&#24050;&#24314;&#31435;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#37325;&#28857;&#20851;&#27880;&#25512;&#29702;&#12289;&#32534;&#30721;&#21644;&#20915;&#31574;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
To solve complex tasks, large language models (LLMs) often require multiple rounds of interactions with the user, sometimes assisted by external tools. However, current evaluation protocols often emphasize benchmark performance with single-turn exchanges, neglecting the nuanced interactions among the user, LLMs, and external tools, while also underestimating the importance of natural language feedback from users. These oversights contribute to discrepancies between research benchmark evaluations and real-world use cases. We introduce MINT, a benchmark that evaluates LLMs' ability to solve tasks with multi-turn interactions by (1) using tools and (2) leveraging natural language feedback. To ensure reproducibility, we provide an evaluation framework where LLMs can access tools by executing Python code and receive users' natural language feedback simulated by GPT-4. We repurpose a diverse set of established evaluation datasets focusing on reasoning, coding, and decision-making and careful
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#23433;&#20840;&#32447;&#24615;&#36172;&#21338;&#26426;&#38382;&#39064;&#30340;&#20960;&#20309;&#29305;&#24449;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;&#36951;&#25022;&#20445;&#35777;&#31639;&#27861;&#65292;&#24182;&#23558;&#20854;&#25512;&#24191;&#21040;&#20855;&#26377;&#20984;&#32422;&#26463;&#30340;&#24773;&#20917;&#12290;&#27169;&#25311;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#21508;&#31181;&#38543;&#26426;&#37319;&#26679;&#30340;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.15006</link><description>&lt;p&gt;
&#21033;&#29992;&#38382;&#39064;&#20960;&#20309;&#29305;&#24449;&#30340;&#23433;&#20840;&#32447;&#24615;&#36172;&#21338;&#26426;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Exploiting Problem Geometry in Safe Linear Bandits. (arXiv:2308.15006v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15006
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#23433;&#20840;&#32447;&#24615;&#36172;&#21338;&#26426;&#38382;&#39064;&#30340;&#20960;&#20309;&#29305;&#24449;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;&#36951;&#25022;&#20445;&#35777;&#31639;&#27861;&#65292;&#24182;&#23558;&#20854;&#25512;&#24191;&#21040;&#20855;&#26377;&#20984;&#32422;&#26463;&#30340;&#24773;&#20917;&#12290;&#27169;&#25311;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#21508;&#31181;&#38543;&#26426;&#37319;&#26679;&#30340;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#32447;&#24615;&#36172;&#21338;&#26426;&#38382;&#39064;&#26159;&#32463;&#20856;&#32447;&#24615;&#36172;&#21338;&#26426;&#38382;&#39064;&#30340;&#19968;&#20010;&#29256;&#26412;&#65292;&#20854;&#20013;&#23398;&#20064;&#22120;&#30340;&#34892;&#21160;&#24517;&#39035;&#22312;&#25152;&#26377;&#22238;&#21512;&#28385;&#36275;&#19968;&#20010;&#19981;&#30830;&#23450;&#30340;&#32447;&#24615;&#32422;&#26463;&#12290;&#30001;&#20110;&#20854;&#22312;&#35768;&#22810;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#36817;&#24180;&#26469;&#36825;&#20010;&#38382;&#39064;&#21463;&#21040;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#12290;&#25105;&#20204;&#21457;&#29616;&#36890;&#36807;&#21033;&#29992;&#29305;&#23450;&#38382;&#39064;&#35774;&#32622;&#30340;&#20960;&#20309;&#29305;&#24449;&#65292;&#21487;&#20197;&#20026;&#30456;&#20114;&#20998;&#31163;&#30340;&#38382;&#39064;&#23454;&#20363;&#21644;&#26377;&#38480;&#26143;&#20984;&#38598;&#30340;&#34892;&#21160;&#38598;&#25552;&#20379;&#25913;&#36827;&#30340;&#36951;&#25022;&#20445;&#35777;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#36873;&#25321;&#38382;&#39064;&#21442;&#25968;&#65292;&#24182;&#20855;&#26377;&#33267;&#23569;&#19982;&#29616;&#26377;&#31639;&#27861;&#30456;&#24403;&#30340;&#36951;&#25022;&#20445;&#35777;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#23433;&#20840;&#32447;&#24615;&#36172;&#21338;&#26426;&#35774;&#32622;&#30340;&#25512;&#24191;&#65292;&#20854;&#20013;&#32422;&#26463;&#26159;&#20984;&#30340;&#65292;&#24182;&#21033;&#29992;&#20102;&#19968;&#31181;&#22522;&#20110;&#20984;&#20998;&#26512;&#30340;&#26032;&#26041;&#27861;&#26469;&#35843;&#25972;&#25105;&#20204;&#30340;&#31639;&#27861;&#21644;&#20998;&#26512;&#12290;&#36890;&#36807;&#27169;&#25311;&#32467;&#26524;&#26174;&#31034;&#65292;&#30456;&#23545;&#20110;&#29616;&#26377;&#31639;&#27861;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#21508;&#31181;&#38543;&#26426;&#37319;&#26679;&#30340;&#35774;&#32622;&#20013;&#34920;&#29616;&#20986;&#25913;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The safe linear bandit problem is a version of the classic linear bandit problem where the learner's actions must satisfy an uncertain linear constraint at all rounds. Due its applicability to many real-world settings, this problem has received considerable attention in recent years. We find that by exploiting the geometry of the specific problem setting, we can achieve improved regret guarantees for both well-separated problem instances and action sets that are finite star convex sets. Additionally, we propose a novel algorithm for this setting that chooses problem parameters adaptively and enjoys at least as good regret guarantees as existing algorithms. Lastly, we introduce a generalization of the safe linear bandit setting where the constraints are convex and adapt our algorithms and analyses to this setting by leveraging a novel convex-analysis based approach. Simulation results show improved performance over existing algorithms for a variety of randomly sampled settings.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#31070;&#32463;&#22270;ODE&#31639;&#27861;&#65288;PINGO&#65289;&#65292;&#29992;&#20110;&#27169;&#25311;&#21644;&#24314;&#27169;&#22810;&#23545;&#35937;&#29289;&#29702;&#31995;&#32479;&#30340;&#38271;&#26399;&#21160;&#24577;&#65292;&#24182;&#35299;&#20915;&#20102;&#24403;&#21069;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#24046;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.13212</link><description>&lt;p&gt;
&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#31070;&#32463;&#22270;ODE&#29992;&#20110;&#38271;&#26399;&#21160;&#21147;&#23398;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Physics-Inspired Neural Graph ODE for Long-term Dynamical Simulation. (arXiv:2308.13212v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13212
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#31070;&#32463;&#22270;ODE&#31639;&#27861;&#65288;PINGO&#65289;&#65292;&#29992;&#20110;&#27169;&#25311;&#21644;&#24314;&#27169;&#22810;&#23545;&#35937;&#29289;&#29702;&#31995;&#32479;&#30340;&#38271;&#26399;&#21160;&#24577;&#65292;&#24182;&#35299;&#20915;&#20102;&#24403;&#21069;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#24046;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#25311;&#21644;&#24314;&#27169;&#22810;&#23545;&#35937;&#29289;&#29702;&#31995;&#32479;&#30340;&#38271;&#26399;&#21160;&#24577;&#26159;&#19968;&#39033;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#30446;&#21069;&#30340;&#30740;&#31350;&#21033;&#29992;&#20855;&#26377;&#31561;&#21464;&#24615;&#36136;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#23545;&#29289;&#29702;&#31995;&#32479;&#36827;&#34892;&#24314;&#27169;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20182;&#20204;&#23558;&#21160;&#21147;&#23398;&#24314;&#27169;&#20026;&#19968;&#31995;&#21015;&#20855;&#26377;&#22266;&#23450;&#26102;&#38388;&#38388;&#38548;&#30340;&#31163;&#25955;&#29366;&#24577;&#65292;&#24182;&#23398;&#20064;&#25152;&#26377;&#30456;&#37051;&#29366;&#24577;&#20043;&#38388;&#30340;&#30452;&#25509;&#26144;&#23556;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#30452;&#25509;&#26144;&#23556;&#24573;&#30053;&#20102;&#20004;&#20010;&#29366;&#24577;&#20043;&#38388;&#30340;&#36830;&#32493;&#24615;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#25105;&#20204;&#24050;&#32463;&#39564;&#35777;&#20102;&#22312;&#24403;&#21069;&#22522;&#20110;GNN&#30340;&#30452;&#25509;&#26144;&#23556;&#27169;&#22411;&#20013;&#65292;&#20004;&#20010;&#31163;&#25955;&#21160;&#24577;&#29366;&#24577;&#20043;&#38388;&#23384;&#22312;&#26080;&#25968;&#21487;&#33021;&#30340;&#36712;&#36857;&#12290;&#36825;&#20010;&#38382;&#39064;&#26497;&#22823;&#22320;&#38459;&#30861;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#23548;&#33268;&#38271;&#26399;&#27169;&#25311;&#30340;&#24615;&#33021;&#36739;&#24046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#20026;&#20102;&#26356;&#22909;&#22320;&#36890;&#36807;&#31163;&#25955;&#30417;&#30563;&#20449;&#21495;&#24314;&#27169;&#28508;&#22312;&#36712;&#36857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#31070;&#32463;&#22270;ODE(PINGO)&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simulating and modeling the long-term dynamics of multi-object physical systems is an essential and challenging task. Current studies model the physical systems utilizing Graph Neural Networks (GNNs) with equivariant properties. Specifically, they model the dynamics as a sequence of discrete states with a fixed time interval and learn a direct mapping for all the two adjacent states. However, this direct mapping overlooks the continuous nature between the two states. Namely, we have verified that there are countless possible trajectories between two discrete dynamic states in current GNN-based direct mapping models. This issue greatly hinders the model generalization ability, leading to poor performance of the long-term simulation. In this paper, to better model the latent trajectory through discrete supervision signals, we propose a Physics-Inspired Neural Graph ODE (PINGO) algorithm. In PINGO, to ensure the uniqueness of the trajectory, we construct a Physics-Inspired Neural ODE fram
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#22320;&#19979;&#25968;&#25454;&#38598;&#30340;&#31283;&#23450;&#38477;&#32500;&#26041;&#27861;&#65292;&#36890;&#36807;&#21018;&#24615;&#21464;&#25442;&#23454;&#29616;&#20102;&#27431;&#20960;&#37324;&#24503;&#19981;&#21464;&#34920;&#31034;&#65292;&#33021;&#22815;&#37327;&#21270;&#22320;&#19979;&#25968;&#25454;&#30340;&#19981;&#30830;&#23450;&#24615;&#24182;&#19988;&#36866;&#24212;&#22806;&#26679;&#26412;&#28857;&#30340;&#25193;&#23637;&#12290;</title><link>http://arxiv.org/abs/2308.08079</link><description>&lt;p&gt;
&#29992;&#20110;&#25903;&#25345;&#22320;&#19979;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#35299;&#37322;&#30340;&#31283;&#23450;&#20302;&#32500;&#31354;&#38388;&#21018;&#24615;&#21464;&#25442;
&lt;/p&gt;
&lt;p&gt;
Rigid Transformations for Stabilized Lower Dimensional Space to Support Subsurface Uncertainty Quantification and Interpretation. (arXiv:2308.08079v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08079
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#22320;&#19979;&#25968;&#25454;&#38598;&#30340;&#31283;&#23450;&#38477;&#32500;&#26041;&#27861;&#65292;&#36890;&#36807;&#21018;&#24615;&#21464;&#25442;&#23454;&#29616;&#20102;&#27431;&#20960;&#37324;&#24503;&#19981;&#21464;&#34920;&#31034;&#65292;&#33021;&#22815;&#37327;&#21270;&#22320;&#19979;&#25968;&#25454;&#30340;&#19981;&#30830;&#23450;&#24615;&#24182;&#19988;&#36866;&#24212;&#22806;&#26679;&#26412;&#28857;&#30340;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22320;&#19979;&#25968;&#25454;&#38598;&#22825;&#28982;&#22320;&#20855;&#26377;&#22823;&#25968;&#25454;&#29305;&#24449;&#65292;&#22914;&#24222;&#22823;&#30340;&#20307;&#31215;&#12289;&#22810;&#26679;&#30340;&#29305;&#24449;&#21644;&#39640;&#36895;&#37319;&#26679;&#36895;&#24230;&#65292;&#21463;&#21040;&#21508;&#31181;&#29289;&#29702;&#12289;&#24037;&#31243;&#21644;&#22320;&#36136;&#36755;&#20837;&#24341;&#36215;&#30340;&#32500;&#25968;&#35781;&#21650;&#30340;&#24433;&#21709;&#12290;&#22312;&#29616;&#26377;&#30340;&#38477;&#32500;&#26041;&#27861;&#20013;&#65292;&#38750;&#32447;&#24615;&#38477;&#32500;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#24230;&#37327;&#22810;&#32500;&#32553;&#25918;&#65288;MDS&#65289;&#65292;&#26159;&#22320;&#19979;&#25968;&#25454;&#38598;&#20013;&#39318;&#36873;&#30340;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#22266;&#26377;&#30340;&#22797;&#26434;&#24615;&#12290;&#34429;&#28982;MDS&#20445;&#30041;&#20102;&#20869;&#22312;&#30340;&#25968;&#25454;&#32467;&#26500;&#21644;&#19981;&#30830;&#23450;&#24615;&#30340;&#37327;&#21270;&#65292;&#20294;&#20854;&#23616;&#38480;&#24615;&#21253;&#25324;&#19981;&#31283;&#23450;&#30340;&#21807;&#19968;&#35299;&#65292;&#19981;&#21464;&#20110;&#27431;&#20960;&#37324;&#24503;&#21464;&#25442;&#65292;&#24182;&#19988;&#27809;&#26377;&#22806;&#26679;&#26412;&#28857;&#65288;OOSP&#65289;&#25193;&#23637;&#12290;&#20026;&#20102;&#22686;&#24378;&#22320;&#19979;&#25512;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#24037;&#20316;&#27969;&#31243;&#65292;&#24517;&#39035;&#23558;&#25968;&#25454;&#38598;&#36716;&#21270;&#20026;&#31283;&#23450;&#30340;&#12289;&#38477;&#32500;&#30340;&#34920;&#31034;&#65292;&#20197;&#23481;&#32435;OOSP&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#21033;&#29992;&#21018;&#24615;&#21464;&#25442;&#23454;&#29616;&#20102;LDS&#30340;&#31283;&#23450;&#27431;&#20960;&#37324;&#24503;&#19981;&#21464;&#34920;&#31034;&#12290;&#36890;&#36807;&#35745;&#31639;MDS&#36755;&#20837;&#30340;&#19981;&#30456;&#20284;&#24230;&#65292;
&lt;/p&gt;
&lt;p&gt;
Subsurface datasets inherently possess big data characteristics such as vast volume, diverse features, and high sampling speeds, further compounded by the curse of dimensionality from various physical, engineering, and geological inputs. Among the existing dimensionality reduction (DR) methods, nonlinear dimensionality reduction (NDR) methods, especially Metric-multidimensional scaling (MDS), are preferred for subsurface datasets due to their inherent complexity. While MDS retains intrinsic data structure and quantifies uncertainty, its limitations include unstabilized unique solutions invariant to Euclidean transformations and an absence of out-of-sample points (OOSP) extension. To enhance subsurface inferential and machine learning workflows, datasets must be transformed into stable, reduced-dimension representations that accommodate OOSP.  Our solution employs rigid transformations for a stabilized Euclidean invariant representation for LDS. By computing an MDS input dissimilarity m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19977;&#32500;&#28857;&#20113;&#30340;&#20020;&#30028;&#28857;&#19982;&#38750;&#20998;&#24067;&#26679;&#26412;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#23558;&#20020;&#30028;&#28857;&#30340;&#27010;&#24565;&#25512;&#24191;&#20026;&#37325;&#35201;&#24615;&#24230;&#37327;&#26041;&#27861;&#12290;&#36890;&#36807;&#20165;&#22522;&#20110;&#38750;&#37325;&#35201;&#28857;&#36827;&#34892;&#20998;&#31867;&#32593;&#32476;&#35757;&#32451;&#65292;&#21487;&#20197;&#25552;&#39640;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#22312;&#24178;&#20928;&#25968;&#25454;&#38598;&#19978;&#20250;&#26377;&#20123;&#24615;&#33021;&#25439;&#22833;&#12290;&#24314;&#35758;&#20351;&#29992;&#26631;&#20934;&#21270;&#29109;&#36873;&#25321;&#38750;&#20020;&#30028;&#28857;&#38598;&#21512;&#30340;&#33258;&#36866;&#24212;&#38408;&#20540;&#12290;&#36825;&#31181;&#37325;&#35201;&#24615;&#24230;&#37327;&#26041;&#27861;&#35745;&#31639;&#36895;&#24230;&#26497;&#24555;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#21487;&#35299;&#37322;AI&#12289;&#31163;&#32676;&#20540;&#21435;&#38500;&#12289;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12289;&#40065;&#26834;&#20998;&#31867;&#21644;&#23545;&#25239;&#24615;&#38450;&#24481;&#31561;&#22810;&#31181;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.05525</link><description>&lt;p&gt;
&#20020;&#30028;&#28857;++&#65306;&#19968;&#31181;&#29992;&#20110;&#40065;&#26834;&#20998;&#31867;&#12289;&#23545;&#25239;&#24615;&#38450;&#24481;&#21644;&#21487;&#35299;&#37322;AI&#30340;&#25935;&#25463;&#28857;&#20113;&#37325;&#35201;&#24615;&#24230;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Critical Points ++: An Agile Point Cloud Importance Measure for Robust Classification, Adversarial Defense and Explainable AI. (arXiv:2308.05525v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05525
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19977;&#32500;&#28857;&#20113;&#30340;&#20020;&#30028;&#28857;&#19982;&#38750;&#20998;&#24067;&#26679;&#26412;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#23558;&#20020;&#30028;&#28857;&#30340;&#27010;&#24565;&#25512;&#24191;&#20026;&#37325;&#35201;&#24615;&#24230;&#37327;&#26041;&#27861;&#12290;&#36890;&#36807;&#20165;&#22522;&#20110;&#38750;&#37325;&#35201;&#28857;&#36827;&#34892;&#20998;&#31867;&#32593;&#32476;&#35757;&#32451;&#65292;&#21487;&#20197;&#25552;&#39640;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#22312;&#24178;&#20928;&#25968;&#25454;&#38598;&#19978;&#20250;&#26377;&#20123;&#24615;&#33021;&#25439;&#22833;&#12290;&#24314;&#35758;&#20351;&#29992;&#26631;&#20934;&#21270;&#29109;&#36873;&#25321;&#38750;&#20020;&#30028;&#28857;&#38598;&#21512;&#30340;&#33258;&#36866;&#24212;&#38408;&#20540;&#12290;&#36825;&#31181;&#37325;&#35201;&#24615;&#24230;&#37327;&#26041;&#27861;&#35745;&#31639;&#36895;&#24230;&#26497;&#24555;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#21487;&#35299;&#37322;AI&#12289;&#31163;&#32676;&#20540;&#21435;&#38500;&#12289;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12289;&#40065;&#26834;&#20998;&#31867;&#21644;&#23545;&#25239;&#24615;&#38450;&#24481;&#31561;&#22810;&#31181;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#23433;&#20840;&#38656;&#27714;&#24212;&#29992;&#20013;&#65292;&#20934;&#30830;&#19988;&#24555;&#36895;&#22320;&#22788;&#29702;&#38750;&#20998;&#24067;&#26679;&#26412;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#26412;&#25991;&#39318;&#20808;&#30740;&#31350;&#20102;&#19977;&#32500;&#28857;&#20113;&#30340;&#20020;&#30028;&#28857;&#19982;&#38750;&#20998;&#24067;&#26679;&#26412;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#24120;&#35265;&#30340;&#25968;&#25454;&#25439;&#22351;&#21644;&#31163;&#32676;&#28857;&#24448;&#24448;&#20250;&#34987;&#35299;&#37322;&#20026;&#20020;&#30028;&#28857;&#12290;&#25105;&#20204;&#23558;&#20020;&#30028;&#28857;&#30340;&#27010;&#24565;&#25512;&#24191;&#20026;&#37325;&#35201;&#24615;&#24230;&#37327;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#20165;&#22522;&#20110;&#38750;&#37325;&#35201;&#28857;&#30340;&#20998;&#31867;&#32593;&#32476;&#35757;&#32451;&#65292;&#21487;&#20197;&#22823;&#22823;&#25552;&#39640;&#40065;&#26834;&#24615;&#65292;&#32780;&#22312;&#24178;&#20928;&#25968;&#25454;&#38598;&#19978;&#20250;&#31245;&#24494;&#25439;&#22833;&#19968;&#20123;&#24615;&#33021;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#26631;&#20934;&#21270;&#29109;&#23545;&#20110;&#25968;&#25454;&#25439;&#22351;&#20998;&#26512;&#38750;&#24120;&#26377;&#20449;&#24687;&#37327;&#12290;&#24314;&#35758;&#22522;&#20110;&#26631;&#20934;&#21270;&#29109;&#36873;&#25321;&#38750;&#20020;&#30028;&#28857;&#38598;&#21512;&#30340;&#33258;&#36866;&#24212;&#38408;&#20540;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#37325;&#35201;&#24615;&#24230;&#37327;&#35745;&#31639;&#26497;&#20854;&#24555;&#36895;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#21487;&#20197;&#29992;&#20110;&#22810;&#31181;&#24212;&#29992;&#65292;&#20363;&#22914;&#21487;&#35299;&#37322;AI(XAI)&#65292;&#31163;&#32676;&#20540;&#21435;&#38500;&#65292;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#40065;&#26834;&#20998;&#31867;&#21644;&#23545;&#25239;&#24615;&#38450;&#24481;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to cope accurately and fast with Out-Of-Distribution (OOD) samples is crucial in real-world safety demanding applications. In this work we first study the interplay between critical points of 3D point clouds and OOD samples. Our findings are that common corruptions and outliers are often interpreted as critical points. We generalize the notion of critical points into importance measures. We show that training a classification network based only on less important points dramatically improves robustness, at a cost of minor performance loss on the clean set. We observe that normalized entropy is highly informative for corruption analysis. An adaptive threshold based on normalized entropy is suggested for selecting the set of uncritical points. Our proposed importance measure is extremely fast to compute. We show it can be used for a variety of applications, such as Explainable AI (XAI), Outlier Removal, Uncertainty Estimation, Robust Classification and Adversarial Defense. We 
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#35770;&#25991;&#35814;&#32454;&#32508;&#36848;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38544;&#20889;&#20998;&#26512;&#25216;&#26415;&#22312;&#25968;&#23383;&#23186;&#20307;&#20013;&#26816;&#27979;&#38544;&#34255;&#20449;&#24687;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2308.04522</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#19981;&#21516;&#25968;&#25454;&#31867;&#22411;&#38544;&#20889;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Deep Learning for Diverse Data Types Steganalysis: A Review. (arXiv:2308.04522v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04522
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35770;&#25991;&#35814;&#32454;&#32508;&#36848;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38544;&#20889;&#20998;&#26512;&#25216;&#26415;&#22312;&#25968;&#23383;&#23186;&#20307;&#20013;&#26816;&#27979;&#38544;&#34255;&#20449;&#24687;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#20889;&#26415;&#21644;&#38544;&#20889;&#20998;&#26512;&#26159;&#20449;&#24687;&#23433;&#20840;&#39046;&#22495;&#30340;&#20004;&#20010;&#30456;&#20851;&#26041;&#38754;&#12290;&#38544;&#20889;&#26415;&#26088;&#22312;&#38544;&#34255;&#36890;&#20449;&#65292;&#32780;&#38544;&#20889;&#20998;&#26512;&#21017;&#26088;&#22312;&#25214;&#21040;&#36825;&#20123;&#38544;&#34255;&#20449;&#24687;&#65292;&#29978;&#33267;&#23581;&#35797;&#24674;&#22797;&#20854;&#25152;&#21253;&#21547;&#30340;&#25968;&#25454;&#12290;&#38544;&#20889;&#26415;&#21644;&#38544;&#20889;&#20998;&#26512;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#65292;&#29305;&#21035;&#21463;&#21040;&#25191;&#27861;&#37096;&#38376;&#30340;&#20851;&#27880;&#12290;&#38544;&#20889;&#26415;&#24120;&#34987;&#32593;&#32476;&#29359;&#32618;&#20998;&#23376;&#29978;&#33267;&#24656;&#24598;&#20998;&#23376;&#29992;&#26469;&#36991;&#20813;&#22312;&#25317;&#26377;&#35777;&#25454;&#26102;&#34987;&#25429;&#65292;&#21363;&#20351;&#21152;&#23494;&#20063;&#19968;&#26679;&#65292;&#22240;&#20026;&#22312;&#35768;&#22810;&#22269;&#23478;&#31105;&#27490;&#25110;&#38480;&#21046;&#20351;&#29992;&#23494;&#30721;&#23398;&#12290;&#22240;&#27492;&#65292;&#20102;&#35299;&#25581;&#31034;&#38544;&#34255;&#20449;&#24687;&#30340;&#23574;&#31471;&#25216;&#26415;&#23545;&#25581;&#38706;&#38750;&#27861;&#34892;&#20026;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#25991;&#29486;&#20013;&#24341;&#20837;&#20102;&#35768;&#22810;&#24378;&#22823;&#21487;&#38752;&#30340;&#38544;&#20889;&#26415;&#21644;&#38544;&#20889;&#20998;&#26512;&#25216;&#26415;&#12290;&#26412;&#32508;&#36848;&#35770;&#25991;&#25552;&#20379;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38544;&#20889;&#20998;&#26512;&#25216;&#26415;&#22312;&#25968;&#23383;&#23186;&#20307;&#20013;&#26816;&#27979;&#38544;&#34255;&#20449;&#24687;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Steganography and steganalysis are two interrelated aspects of the field of information security. Steganography seeks to conceal communications, whereas steganalysis is aimed to either find them or even, if possible, recover the data they contain. Steganography and steganalysis have attracted a great deal of interest, particularly from law enforcement. Steganography is often used by cybercriminals and even terrorists to avoid being captured while in possession of incriminating evidence, even encrypted, since cryptography is prohibited or restricted in many countries. Therefore, knowledge of cutting-edge techniques to uncover concealed information is crucial in exposing illegal acts. Over the last few years, a number of strong and reliable steganography and steganalysis techniques have been introduced in the literature. This review paper provides a comprehensive overview of deep learning-based steganalysis techniques used to detect hidden information within digital media. The paper cove
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#34701;&#21512;&#22810;&#20010;&#31354;&#38388;&#32500;&#24230;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#33041;&#30005;&#20449;&#21495;&#23545;&#24515;&#29702;&#36127;&#33655;&#36827;&#34892;&#20998;&#31867;&#21644;&#20272;&#35745;&#36830;&#32493;&#32423;&#21035;&#12290;&#22312;&#26102;&#38388;&#22495;&#20013;&#20351;&#29992;&#20102;&#26102;&#24577;&#21367;&#31215;&#32593;&#32476;&#65292;&#32780;&#22312;&#39057;&#29575;&#22495;&#20013;&#24341;&#20837;&#20102;&#26032;&#30340;&#26550;&#26500;&#8212;&#8212;&#22810;&#32500;&#27531;&#24046;&#22359;&#12290;</title><link>http://arxiv.org/abs/2308.02409</link><description>&lt;p&gt;
&#36890;&#36807;&#34701;&#21512;&#22810;&#31354;&#38388;&#28145;&#24230;&#27169;&#22411;&#30340;&#33041;&#30005;&#20449;&#21495;&#26469;&#20272;&#35745;&#24515;&#29702;&#36127;&#33655;
&lt;/p&gt;
&lt;p&gt;
Mental Workload Estimation with Electroencephalogram Signals by Combining Multi-Space Deep Models. (arXiv:2308.02409v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02409
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#34701;&#21512;&#22810;&#20010;&#31354;&#38388;&#32500;&#24230;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#33041;&#30005;&#20449;&#21495;&#23545;&#24515;&#29702;&#36127;&#33655;&#36827;&#34892;&#20998;&#31867;&#21644;&#20272;&#35745;&#36830;&#32493;&#32423;&#21035;&#12290;&#22312;&#26102;&#38388;&#22495;&#20013;&#20351;&#29992;&#20102;&#26102;&#24577;&#21367;&#31215;&#32593;&#32476;&#65292;&#32780;&#22312;&#39057;&#29575;&#22495;&#20013;&#24341;&#20837;&#20102;&#26032;&#30340;&#26550;&#26500;&#8212;&#8212;&#22810;&#32500;&#27531;&#24046;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#33041;&#22312;&#24037;&#20316;&#21644;&#20241;&#24687;&#26102;&#37117;&#22788;&#20110;&#25345;&#32493;&#27963;&#21160;&#30340;&#29366;&#24577;&#12290;&#24515;&#29702;&#27963;&#21160;&#26159;&#26085;&#24120;&#36807;&#31243;&#20013;&#30340;&#19968;&#37096;&#20998;&#65292;&#24403;&#22823;&#33041;&#36807;&#24230;&#21171;&#32047;&#26102;&#65292;&#20250;&#23545;&#20154;&#20307;&#20581;&#24247;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#23545;&#20110;&#26089;&#26399;&#26816;&#27979;&#24515;&#29702;&#20581;&#24247;&#38382;&#39064;&#30340;&#37325;&#35270;&#36880;&#28176;&#22686;&#21152;&#65292;&#22240;&#20026;&#36825;&#21487;&#20197;&#24110;&#21161;&#39044;&#38450;&#20005;&#37325;&#30340;&#20581;&#24247;&#38382;&#39064;&#65292;&#24182;&#25913;&#21892;&#29983;&#27963;&#36136;&#37327;&#12290;&#22810;&#31181;&#20449;&#21495;&#34987;&#29992;&#20110;&#35780;&#20272;&#24515;&#29702;&#29366;&#24577;&#65292;&#20294;&#30001;&#20110;&#22823;&#37327;&#25552;&#20379;&#20851;&#20110;&#22823;&#33041;&#20449;&#24687;&#30340;&#29305;&#28857;&#65292;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#34987;&#30740;&#31350;&#20154;&#21592;&#24191;&#27867;&#20351;&#29992;&#12290;&#26412;&#25991;&#26088;&#22312;&#23558;&#24515;&#29702;&#36127;&#33655;&#20998;&#20026;&#19977;&#31181;&#29366;&#24577;&#24182;&#20272;&#35745;&#36830;&#32493;&#32423;&#21035;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#34701;&#21512;&#22810;&#20010;&#31354;&#38388;&#32500;&#24230;&#26469;&#23454;&#29616;&#26368;&#20339;&#30340;&#24515;&#29702;&#20272;&#35745;&#32467;&#26524;&#12290;&#22312;&#26102;&#38388;&#22495;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#26102;&#24577;&#21367;&#31215;&#32593;&#32476;&#65292;&#32780;&#22312;&#39057;&#29575;&#22495;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#32500;&#27531;&#24046;&#22359;&#30340;&#26032;&#26550;&#26500;&#65292;&#23427;&#32467;&#21512;&#20102;&#27531;&#24046;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
The human brain is in a continuous state of activity during both work and rest. Mental activity is a daily process, and when the brain is overworked, it can have negative effects on human health. In recent years, great attention has been paid to early detection of mental health problems because it can help prevent serious health problems and improve quality of life. Several signals are used to assess mental state, but the electroencephalogram (EEG) is widely used by researchers because of the large amount of information it provides about the brain. This paper aims to classify mental workload into three states and estimate continuum levels. Our method combines multiple dimensions of space to achieve the best results for mental estimation. In the time domain approach, we use Temporal Convolutional Networks, and in the frequency domain, we propose a new architecture called the Multi-Dimensional Residual Block, which combines residual blocks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#31216;&#22343;&#34913;&#23398;&#20064;&#26041;&#27861;&#65292;&#20801;&#35768;&#22312;&#21482;&#33021;&#36890;&#36807;&#37319;&#26679;&#33719;&#24471;&#25968;&#25454;&#21644;&#28508;&#22312;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;VAEs&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#19982;&#20256;&#32479;ELBO&#23398;&#20064;&#26041;&#27861;&#33719;&#24471;&#30340;&#27169;&#22411;&#30456;&#24403;&#65292;&#24182;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.09883</link><description>&lt;p&gt;
VAE&#30340;&#23545;&#31216;&#22343;&#34913;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Symmetric Equilibrium Learning of VAEs. (arXiv:2307.09883v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09883
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#31216;&#22343;&#34913;&#23398;&#20064;&#26041;&#27861;&#65292;&#20801;&#35768;&#22312;&#21482;&#33021;&#36890;&#36807;&#37319;&#26679;&#33719;&#24471;&#25968;&#25454;&#21644;&#28508;&#22312;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;VAEs&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#19982;&#20256;&#32479;ELBO&#23398;&#20064;&#26041;&#27861;&#33719;&#24471;&#30340;&#27169;&#22411;&#30456;&#24403;&#65292;&#24182;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#35270;&#20026;&#35299;&#30721;&#22120;-&#32534;&#30721;&#22120;&#23545;&#65292;&#23558;&#25968;&#25454;&#31354;&#38388;&#20013;&#30340;&#20998;&#24067;&#26144;&#23556;&#21040;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#20998;&#24067;&#65292;&#21453;&#20043;&#20134;&#28982;&#12290;VAEs&#30340;&#26631;&#20934;&#23398;&#20064;&#26041;&#27861;&#65292;&#21363;&#26368;&#22823;&#21270;&#35777;&#25454;&#19979;&#30028;&#65288;ELBO&#65289;&#65292;&#23384;&#22312;&#26126;&#26174;&#30340;&#19981;&#23545;&#31216;&#24615;&#12290;&#27492;&#22806;&#65292;&#23427;&#38656;&#35201;&#19968;&#20010;&#38381;&#21512;&#24418;&#24335;&#30340;&#20808;&#39564;&#28508;&#22312;&#20998;&#24067;&#12290;&#36825;&#38480;&#21046;&#20102;VAEs&#22312;&#26356;&#22797;&#26434;&#30340;&#24773;&#20917;&#19979;&#30340;&#36866;&#29992;&#24615;&#65292;&#22914;&#19968;&#33324;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#20351;&#29992;&#22797;&#26434;&#30340;&#29983;&#25104;&#27169;&#22411;&#20316;&#20026;&#20808;&#39564;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32435;&#20160;&#22343;&#34913;&#23398;&#20064;&#26041;&#27861;&#65292;&#25918;&#23485;&#20102;&#36825;&#20123;&#38480;&#21046;&#65292;&#22312;&#21482;&#33021;&#36890;&#36807;&#37319;&#26679;&#33719;&#24471;&#25968;&#25454;&#21644;&#28508;&#22312;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;VAEs&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#28789;&#27963;&#24615;&#21644;&#31616;&#21333;&#24615;&#20351;&#20854;&#36866;&#29992;&#20110;&#24191;&#27867;&#30340;&#23398;&#20064;&#22330;&#26223;&#21644;&#19979;&#28216;&#20219;&#21153;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#23398;&#20064;&#30340;&#27169;&#22411;&#19982;ELBO&#23398;&#20064;&#33719;&#24471;&#30340;&#27169;&#22411;&#30456;&#24403;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#23454;&#36341;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We view variational autoencoders (VAE) as decoder-encoder pairs, which map distributions in the data space to distributions in the latent space and vice versa. The standard learning approach for VAEs, i.e. maximisation of the evidence lower bound (ELBO), has an obvious asymmetry in that respect. Moreover, it requires a closed form a-priori latent distribution. This limits the applicability of VAEs in more complex scenarios, such as general semi-supervised learning and employing complex generative models as priors. We propose a Nash equilibrium learning approach that relaxes these restrictions and allows learning VAEs in situations where both the data and the latent distributions are accessible only by sampling. The flexibility and simplicity of this approach allows its application to a wide range of learning scenarios and downstream tasks. We show experimentally that the models learned by this method are comparable to those obtained by ELBO learning and demonstrate its applicability fo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#34394;&#20551;&#28436;&#31034;&#26102;&#20986;&#29616;&#30340;&#36807;&#24230;&#24605;&#32771;&#21644;&#38169;&#35823;&#24402;&#32435;&#22836;&#29616;&#35937;&#12290;&#36890;&#36807;&#30740;&#31350;&#27169;&#22411;&#30340;&#20869;&#37096;&#34920;&#31034;&#65292;&#21457;&#29616;&#27169;&#22411;&#22312;&#20013;&#38388;&#23618;&#20043;&#21518;&#23545;&#38169;&#35823;&#28436;&#31034;&#30340;&#22788;&#29702;&#20934;&#30830;&#24615;&#36880;&#28176;&#38477;&#20302;&#65292;&#24182;&#25351;&#20986;&#20102;&#38169;&#35823;&#24402;&#32435;&#22836;&#26426;&#21046;&#21487;&#33021;&#23548;&#33268;&#36807;&#24230;&#24605;&#32771;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2307.09476</link><description>&lt;p&gt;
&#36807;&#24230;&#24605;&#32771;&#30495;&#30456;&#65306;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#22788;&#29702;&#34394;&#20551;&#28436;&#31034;
&lt;/p&gt;
&lt;p&gt;
Overthinking the Truth: Understanding how Language Models Process False Demonstrations. (arXiv:2307.09476v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09476
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#34394;&#20551;&#28436;&#31034;&#26102;&#20986;&#29616;&#30340;&#36807;&#24230;&#24605;&#32771;&#21644;&#38169;&#35823;&#24402;&#32435;&#22836;&#29616;&#35937;&#12290;&#36890;&#36807;&#30740;&#31350;&#27169;&#22411;&#30340;&#20869;&#37096;&#34920;&#31034;&#65292;&#21457;&#29616;&#27169;&#22411;&#22312;&#20013;&#38388;&#23618;&#20043;&#21518;&#23545;&#38169;&#35823;&#28436;&#31034;&#30340;&#22788;&#29702;&#20934;&#30830;&#24615;&#36880;&#28176;&#38477;&#20302;&#65292;&#24182;&#25351;&#20986;&#20102;&#38169;&#35823;&#24402;&#32435;&#22836;&#26426;&#21046;&#21487;&#33021;&#23548;&#33268;&#36807;&#24230;&#24605;&#32771;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#23569;&#37327;&#31034;&#33539;&#36827;&#34892;&#22797;&#26434;&#27169;&#24335;&#30340;&#27169;&#20223;&#23398;&#20064;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#27809;&#26377;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#23436;&#25104;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#27169;&#20223;&#20063;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#20013;&#37325;&#29616;&#19981;&#20934;&#30830;&#25110;&#26377;&#23475;&#30340;&#20869;&#23481;&#12290;&#25105;&#20204;&#36890;&#36807;&#27169;&#22411;&#30340;&#20869;&#37096;&#34920;&#31034;&#26469;&#30740;&#31350;&#26377;&#23475;&#30340;&#27169;&#20223;&#65292;&#24182;&#30830;&#23450;&#20102;&#20004;&#20010;&#30456;&#20851;&#29616;&#35937;&#65306;&#36807;&#24230;&#24605;&#32771;&#21644;&#38169;&#35823;&#24402;&#32435;&#22836;&#12290;&#31532;&#19968;&#20010;&#29616;&#35937;&#65292;&#36807;&#24230;&#24605;&#32771;&#65292;&#22312;&#32473;&#20986;&#27491;&#30830;&#19982;&#38169;&#35823;&#30340;&#23569;&#37327;&#31034;&#33539;&#26102;&#65292;&#25105;&#20204;&#20174;&#20013;&#38388;&#23618;&#35299;&#30721;&#39044;&#27979;&#12290;&#22312;&#26089;&#26399;&#23618;&#20013;&#65292;&#20004;&#31181;&#31034;&#33539;&#24341;&#36215;&#20102;&#30456;&#20284;&#30340;&#27169;&#22411;&#34892;&#20026;&#65292;&#20294;&#22312;&#26576;&#20010;&#8220;&#20851;&#38190;&#23618;&#8221;&#20043;&#21518;&#65292;&#32473;&#20986;&#38169;&#35823;&#31034;&#33539;&#30340;&#20934;&#30830;&#24615;&#36880;&#28176;&#38477;&#20302;&#12290;&#31532;&#20108;&#20010;&#29616;&#35937;&#65292;&#38169;&#35823;&#24402;&#32435;&#22836;&#65292;&#21487;&#33021;&#26159;&#36807;&#24230;&#24605;&#32771;&#30340;&#19968;&#31181;&#26426;&#21046;&#24615;&#21407;&#22240;&#65306;&#36825;&#20123;&#26159;&#20301;&#20110;&#36739;&#26202;&#23618;&#30340;&#22836;&#37096;&#65292;&#23427;&#20204;&#20851;&#27880;&#24182;&#22797;&#21046;&#20808;&#21069;&#31034;&#33539;&#20013;&#30340;&#38169;&#35823;&#20449;&#24687;&#65292;&#20854;&#21066;&#24369;&#20250;&#20943;&#23569;&#36807;&#24230;&#24605;&#32771;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern language models can imitate complex patterns through few-shot learning, enabling them to complete challenging tasks without fine-tuning. However, imitation can also lead models to reproduce inaccuracies or harmful content if present in the context. We study harmful imitation through the lens of a model's internal representations, and identify two related phenomena: overthinking and false induction heads. The first phenomenon, overthinking, appears when we decode predictions from intermediate layers, given correct vs. incorrect few-shot demonstrations. At early layers, both demonstrations induce similar model behavior, but the behavior diverges sharply at some "critical layer", after which the accuracy given incorrect demonstrations progressively decreases. The second phenomenon, false induction heads, are a possible mechanistic cause of overthinking: these are heads in late layers that attend to and copy false information from previous demonstrations, and whose ablation reduces 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;time-dependent Cox&#27169;&#22411;&#21644;&#31070;&#32463;&#32593;&#32476;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#39044;&#27979;&#27169;&#22411;&#26469;&#39044;&#27979;&#36827;&#34892;&#24615;&#30524;&#37096;&#30142;&#30149;&#24180;&#40836;&#30456;&#20851;&#24615;&#40644;&#26001;&#21464;&#24615;&#65288;AMD&#65289;&#30340;&#36827;&#23637;&#12290;&#36890;&#36807;&#20351;&#29992;&#32437;&#21521;&#30524;&#24213;&#22270;&#20687;&#20316;&#20026;&#36755;&#20837;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#24314;&#31435;&#19968;&#20010;&#20010;&#20307;&#21270;&#30340;&#39118;&#38505;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;&#23454;&#35777;&#30740;&#31350;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.05881</link><description>&lt;p&gt;
&#21160;&#24577;&#39044;&#27979;&#20351;&#29992;&#26102;&#21464;Cox&#29983;&#23384;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Dynamic Prediction using Time-Dependent Cox Survival Neural Network. (arXiv:2307.05881v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05881
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;time-dependent Cox&#27169;&#22411;&#21644;&#31070;&#32463;&#32593;&#32476;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#39044;&#27979;&#27169;&#22411;&#26469;&#39044;&#27979;&#36827;&#34892;&#24615;&#30524;&#37096;&#30142;&#30149;&#24180;&#40836;&#30456;&#20851;&#24615;&#40644;&#26001;&#21464;&#24615;&#65288;AMD&#65289;&#30340;&#36827;&#23637;&#12290;&#36890;&#36807;&#20351;&#29992;&#32437;&#21521;&#30524;&#24213;&#22270;&#20687;&#20316;&#20026;&#36755;&#20837;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#24314;&#31435;&#19968;&#20010;&#20010;&#20307;&#21270;&#30340;&#39118;&#38505;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;&#23454;&#35777;&#30740;&#31350;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#39044;&#27979;&#30340;&#30446;&#26631;&#26159;&#22312;&#19981;&#26029;&#26356;&#26032;&#30340;&#26032;&#25968;&#25454;&#21487;&#29992;&#26102;&#25552;&#20379;&#20010;&#20307;&#21270;&#30340;&#39118;&#38505;&#39044;&#27979;&#12290;&#21463;&#21040;&#24314;&#31435;&#19968;&#20010;&#38024;&#23545;&#36827;&#34892;&#24615;&#30524;&#37096;&#30142;&#30149;&#65292;&#24180;&#40836;&#30456;&#20851;&#24615;&#40644;&#26001;&#21464;&#24615;&#65288;AMD&#65289;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#21464;Cox&#27169;&#22411;&#30340;&#29983;&#23384;&#31070;&#32463;&#32593;&#32476;&#65288;tdCoxSNN&#65289;&#26469;&#39044;&#27979;&#20854;&#22312;&#25345;&#32493;&#26102;&#38388;&#23610;&#24230;&#19978;&#30340;&#36827;&#23637;&#65292;&#20351;&#29992;&#32437;&#21521;&#30524;&#24213;&#22270;&#20687;&#12290;tdCoxSNN&#36890;&#36807;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#27169;&#25311;&#26102;&#21464;&#21327;&#21464;&#37327;&#23545;&#29983;&#23384;&#32467;&#26524;&#30340;&#38750;&#32447;&#24615;&#24433;&#21709;&#25193;&#23637;&#20102;&#26102;&#21464;Cox&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#32467;&#21512;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#65292;tdCoxSNN&#21487;&#20197;&#20197;&#32437;&#21521;&#21407;&#22987;&#22270;&#20687;&#20316;&#20026;&#36755;&#20837;&#12290;&#25105;&#20204;&#36890;&#36807;&#20840;&#38754;&#30340;&#27169;&#25311;&#65292;&#20351;&#29992;&#20004;&#20010;&#26102;&#21464;&#31934;&#24230;&#24230;&#37327;&#26631;&#20934;&#65292;Brier&#20998;&#25968;&#21644;&#21160;&#24577;AUC&#27604;&#36739;&#21644;&#35780;&#20272;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#19982;&#32852;&#21512;&#24314;&#27169;&#21644;&#37324;&#31243;&#30865;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#20004;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#12290;&#19968;&#20010;&#26159;&#19968;&#20010;&#22823;&#22411;AMD&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
The target of dynamic prediction is to provide individualized risk predictions over time which can be updated as new data become available. Motivated by establishing a dynamic prediction model for the progressive eye disease, age-related macular degeneration (AMD), we proposed a time-dependent Cox model-based survival neural network (tdCoxSNN) to predict its progression on a continuous time scale using longitudinal fundus images. tdCoxSNN extends the time-dependent Cox model by utilizing a neural network to model the non-linear effect of the time-dependent covariates on the survival outcome. Additionally, by incorporating the convolutional neural network (CNN), tdCoxSNN can take the longitudinal raw images as input. We evaluate and compare our proposed method with joint modeling and landmarking approaches through comprehensive simulations using two time-dependent accuracy metrics, the Brier Score and dynamic AUC. We applied the proposed approach to two real datasets. One is a large AMD
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25299;&#25169;&#31070;&#32463;&#27850;&#26494;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#32771;&#34385;&#20808;&#39564;&#25299;&#25169;&#32593;&#32476;&#21644;&#28508;&#22312;&#30340;Granger&#22240;&#26524;&#32467;&#26500;&#26469;&#23398;&#20064;&#20107;&#20214;&#24207;&#21015;&#20013;&#30340;Granger&#22240;&#26524;&#20851;&#31995;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#25512;&#26029;&#22240;&#26524;&#20851;&#31995;&#26469;&#35299;&#20915;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#38750; i.i.d. &#38382;&#39064;</title><link>http://arxiv.org/abs/2306.14114</link><description>&lt;p&gt;
TNPAR: &#22522;&#20110;&#25299;&#25169;&#31070;&#32463;&#27850;&#26494;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#20107;&#20214;&#24207;&#21015;Granger&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
TNPAR: Topological Neural Poisson Auto-Regressive Model for Learning Granger Causal Structure from Event Sequences. (arXiv:2306.14114v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14114
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25299;&#25169;&#31070;&#32463;&#27850;&#26494;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#32771;&#34385;&#20808;&#39564;&#25299;&#25169;&#32593;&#32476;&#21644;&#28508;&#22312;&#30340;Granger&#22240;&#26524;&#32467;&#26500;&#26469;&#23398;&#20064;&#20107;&#20214;&#24207;&#21015;&#20013;&#30340;Granger&#22240;&#26524;&#20851;&#31995;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#25512;&#26029;&#22240;&#26524;&#20851;&#31995;&#26469;&#35299;&#20915;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#38750; i.i.d. &#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20107;&#20214;&#24207;&#21015;&#20013;&#23398;&#20064;Granger&#22240;&#26524;&#20851;&#31995;&#26159;&#21508;&#31181;&#24212;&#29992;&#20013;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#21448;&#33267;&#20851;&#37325;&#35201;&#30340;&#20219;&#21153;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#37117;&#20381;&#36182;&#20110;&#20107;&#20214;&#24207;&#21015;&#29420;&#31435;&#21516;&#20998;&#24067; (i.i.d.) &#30340;&#20551;&#35774;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20107;&#20214;&#24207;&#21015;&#20043;&#38388;&#30340;&#22266;&#26377;&#20381;&#36182;&#20851;&#31995;&#65292;&#36825;&#19968; i.i.d. &#20551;&#35774;&#32463;&#24120;&#34987;&#36829;&#21453;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#20381;&#36182;&#20851;&#31995;&#21487;&#20197;&#34987;&#24314;&#27169;&#25104;&#19968;&#20010;&#25299;&#25169;&#32593;&#32476;&#65292;&#22240;&#27492;&#21487;&#20197;&#36890;&#36807;&#23558;&#20808;&#39564;&#25299;&#25169;&#32593;&#32476;&#24341;&#20837;Granger&#22240;&#26524;&#21457;&#29616;&#26469;&#35299;&#20915;&#38750; i.i.d. &#38382;&#39064;&#12290;&#36825;&#19968;&#21457;&#29616;&#20419;&#20351;&#25105;&#20204;&#35299;&#20915;&#20004;&#20010;&#38382;&#39064;&#65306;1) &#22914;&#20309;&#22312;&#27169;&#22411;&#20107;&#20214;&#24207;&#21015;&#26102;&#21516;&#26102;&#32771;&#34385;&#20808;&#39564;&#25299;&#25169;&#32593;&#32476;&#21644;&#28508;&#22312;&#30340;Granger&#22240;&#26524;&#32467;&#26500;&#65307;2) &#22914;&#20309;&#23398;&#20064;Granger&#22240;&#26524;&#32467;&#26500;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#32479;&#19968;&#25299;&#25169;&#31070;&#32463;&#27850;&#26494;&#33258;&#22238;&#24402;&#27169;&#22411;&#12290;&#22312;&#29983;&#25104;&#38454;&#27573;&#65292;&#25105;&#20204;&#37319;&#29992;&#31070;&#32463;&#27850;&#26494;&#36807;&#31243;&#30340;&#19968;&#31181;&#21464;&#20307;&#26469;&#24314;&#27169;&#20107;&#20214;&#21457;&#29983;&#30340;&#26102;&#21051;&#65292;&#24182;&#36890;&#36807;&#25299;&#25169;&#20851;&#31995;&#21644;&#29616;&#26377;&#20107;&#20214;&#24207;&#21015;&#25512;&#26029;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning Granger causality from event sequences is a challenging but essential task across various applications. Most existing methods rely on the assumption that event sequences are independent and identically distributed (i.i.d.). However, this i.i.d. assumption is often violated due to the inherent dependencies among the event sequences. Fortunately, in practice, we find these dependencies can be modeled by a topological network, suggesting a potential solution to the non-i.i.d. problem by introducing the prior topological network into Granger causal discovery. This observation prompts us to tackle two ensuing challenges: 1) how to model the event sequences while incorporating both the prior topological network and the latent Granger causal structure, and 2) how to learn the Granger causal structure. To this end, we devise a two-stage unified topological neural Poisson auto-regressive model. During the generation stage, we employ a variant of the neural Poisson process to model the 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#37096;&#20998;&#22312;&#32447;&#29366;&#24577;&#20449;&#24687;&#30340;POMDP&#38382;&#39064;&#30340;&#29702;&#35770;&#22256;&#38590;&#24615;&#21644;&#21487;&#35745;&#31639;&#24615;&#12290;&#20316;&#32773;&#36890;&#36807;&#24314;&#31435;&#19979;&#30028;&#24471;&#20986;&#19968;&#20010;&#24778;&#20154;&#30340;&#38590;&#24230;&#32467;&#26524;&#65306;&#38500;&#38750;&#20855;&#26377;&#23436;&#25972;&#30340;&#22312;&#32447;&#29366;&#24577;&#20449;&#24687;&#65292;&#21542;&#21017;&#38656;&#35201;&#25351;&#25968;&#32423;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#25165;&#33021;&#24471;&#21040;POMDP&#30340;&#26368;&#20248;&#31574;&#30053;&#35299;&#12290;&#28982;&#32780;&#65292;&#20316;&#32773;&#36824;&#21457;&#29616;&#20102;&#20855;&#26377;&#37096;&#20998;&#22312;&#32447;&#29366;&#24577;&#20449;&#24687;&#19979;&#30340;&#21487;&#35745;&#31639;POMDP&#31867;&#21035;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#31639;&#27861;&#26469;&#35777;&#26126;&#20854;&#25509;&#36817;&#26368;&#20248;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.08762</link><description>&lt;p&gt;
&#22312;&#20855;&#26377;&#37096;&#20998;&#22312;&#32447;&#29366;&#24577;&#20449;&#24687;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;POMDP&#30340;&#29702;&#35770;&#38590;&#24230;&#21644;&#21487;&#35745;&#31639;&#24615;
&lt;/p&gt;
&lt;p&gt;
Theoretical Hardness and Tractability of POMDPs in RL with Partial Online State Information. (arXiv:2306.08762v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08762
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#37096;&#20998;&#22312;&#32447;&#29366;&#24577;&#20449;&#24687;&#30340;POMDP&#38382;&#39064;&#30340;&#29702;&#35770;&#22256;&#38590;&#24615;&#21644;&#21487;&#35745;&#31639;&#24615;&#12290;&#20316;&#32773;&#36890;&#36807;&#24314;&#31435;&#19979;&#30028;&#24471;&#20986;&#19968;&#20010;&#24778;&#20154;&#30340;&#38590;&#24230;&#32467;&#26524;&#65306;&#38500;&#38750;&#20855;&#26377;&#23436;&#25972;&#30340;&#22312;&#32447;&#29366;&#24577;&#20449;&#24687;&#65292;&#21542;&#21017;&#38656;&#35201;&#25351;&#25968;&#32423;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#25165;&#33021;&#24471;&#21040;POMDP&#30340;&#26368;&#20248;&#31574;&#30053;&#35299;&#12290;&#28982;&#32780;&#65292;&#20316;&#32773;&#36824;&#21457;&#29616;&#20102;&#20855;&#26377;&#37096;&#20998;&#22312;&#32447;&#29366;&#24577;&#20449;&#24687;&#19979;&#30340;&#21487;&#35745;&#31639;POMDP&#31867;&#21035;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#31639;&#27861;&#26469;&#35777;&#26126;&#20854;&#25509;&#36817;&#26368;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#25429;&#25417;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#29702;&#35770;&#32467;&#26524;&#24050;&#32463;&#34920;&#26126;&#65292;&#22312;&#19968;&#33324;&#30340;POMDP&#20013;&#23398;&#20064;&#21487;&#33021;&#26159;&#19981;&#21487;&#35745;&#31639;&#30340;&#65292;&#20027;&#35201;&#25361;&#25112;&#22312;&#20110;&#32570;&#20047;&#28508;&#22312;&#30340;&#29366;&#24577;&#20449;&#24687;&#12290;&#19968;&#20010;&#20851;&#38190;&#30340;&#22522;&#26412;&#38382;&#39064;&#26159;&#26377;&#22810;&#23569;&#22312;&#32447;&#29366;&#24577;&#20449;&#24687;&#65288;OSI&#65289;&#36275;&#20197;&#23454;&#29616;&#21487;&#35745;&#31639;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#19979;&#30028;&#65292;&#25581;&#31034;&#20102;&#19968;&#20010;&#24778;&#20154;&#30340;&#38590;&#24230;&#32467;&#26524;&#65306;&#38500;&#38750;&#25105;&#20204;&#20855;&#26377;&#23436;&#25972;&#30340;OSI&#65292;&#21542;&#21017;&#25105;&#20204;&#38656;&#35201;&#25351;&#25968;&#32423;&#30340;&#37319;&#26679;&#22797;&#26434;&#24230;&#25165;&#33021;&#33719;&#24471;POMDP&#30340;$\epsilon$-&#26368;&#20248;&#31574;&#30053;&#35299;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#21463;&#21040;&#25105;&#20204;&#19979;&#30028;&#35774;&#35745;&#30340;&#20851;&#38190;&#35265;&#35299;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#21457;&#29616;&#21363;&#20351;&#21482;&#26377;&#37096;&#20998;OSI&#65292;&#20063;&#23384;&#22312;&#37325;&#35201;&#30340;&#21487;&#35745;&#31639;&#30340;POMDP&#31867;&#21035;&#12290;&#29305;&#21035;&#22320;&#65292;&#23545;&#20110;&#20855;&#26377;&#37096;&#20998;OSI&#30340;&#20004;&#20010;&#26032;&#39062;&#30340;POMDP&#31867;&#21035;&#65292;&#25105;&#20204;&#36890;&#36807;&#24314;&#31435;&#26032;&#30340;&#36951;&#25022;&#19978;&#19979;&#30028;&#35777;&#26126;&#20102;&#26032;&#30340;&#31639;&#27861;&#26159;&#25509;&#36817;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Partially observable Markov decision processes (POMDPs) have been widely applied to capture many real-world applications. However, existing theoretical results have shown that learning in general POMDPs could be intractable, where the main challenge lies in the lack of latent state information. A key fundamental question here is how much online state information (OSI) is sufficient to achieve tractability. In this paper, we establish a lower bound that reveals a surprising hardness result: unless we have full OSI, we need an exponentially scaling sample complexity to obtain an $\epsilon$-optimal policy solution for POMDPs. Nonetheless, inspired by the key insights in our lower bound design, we find that there exist important tractable classes of POMDPs even with only partial OSI. In particular, for two novel classes of POMDPs with partial OSI, we provide new algorithms that are proved to be near-optimal by establishing new regret upper and lower bounds.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;SGAT4PASS&#65292;&#19968;&#31181;&#38754;&#21521;&#29699;&#38754;&#20960;&#20309;&#24847;&#35782;&#30340;&#20840;&#26223;&#35821;&#20041;&#20998;&#21106;Transformer&#65292;&#36890;&#36807;&#21152;&#20837;&#29699;&#38754;&#20960;&#20309;&#24863;&#30693;&#30340;&#32422;&#26463;&#65292;&#33021;&#26356;&#22909;&#22320;&#25429;&#25417;&#20840;&#26223;&#22270;&#20687;&#30340;3D&#23646;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#20998;&#21106;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.03403</link><description>&lt;p&gt;
SGAT4PASS&#65306;&#38754;&#21521;&#29699;&#38754;&#20960;&#20309;&#24847;&#35782;&#30340;&#20840;&#26223;&#35821;&#20041;&#20998;&#21106;Transformer
&lt;/p&gt;
&lt;p&gt;
SGAT4PASS: Spherical Geometry-Aware Transformer for PAnoramic Semantic Segmentation. (arXiv:2306.03403v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;SGAT4PASS&#65292;&#19968;&#31181;&#38754;&#21521;&#29699;&#38754;&#20960;&#20309;&#24847;&#35782;&#30340;&#20840;&#26223;&#35821;&#20041;&#20998;&#21106;Transformer&#65292;&#36890;&#36807;&#21152;&#20837;&#29699;&#38754;&#20960;&#20309;&#24863;&#30693;&#30340;&#32422;&#26463;&#65292;&#33021;&#26356;&#22909;&#22320;&#25429;&#25417;&#20840;&#26223;&#22270;&#20687;&#30340;3D&#23646;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#20998;&#21106;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#19968;&#20010;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#20840;&#26223;&#35821;&#20041;&#20998;&#21106;&#21487;&#20197;&#26681;&#25454;&#36229;&#24191;&#35282;&#35266;&#23519;&#21040;&#30340;&#23436;&#25972;&#22330;&#26223;&#26469;&#36827;&#34892;&#24863;&#30693;&#12290;&#20256;&#32479;&#30340;&#38024;&#23545;2D&#20840;&#26223;&#22270;&#20687;&#30340;PASS&#26041;&#27861;&#20391;&#37325;&#20110;&#35299;&#20915;&#22270;&#20687;&#30072;&#21464;&#38382;&#39064;&#65292;&#20294;&#32570;&#20047;&#23545;&#21407;&#22987;360&#176;&#25968;&#25454;&#30340;3D&#23646;&#24615;&#30340;&#32771;&#34385;&#12290;&#22240;&#27492;&#65292;&#24403;&#36755;&#20837;&#20855;&#26377;3D&#25200;&#21160;&#30340;&#20840;&#26223;&#22270;&#20687;&#26102;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#20250;&#22823;&#24133;&#19979;&#38477;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#24212;&#23545;3D&#25200;&#21160;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#29699;&#38754;&#20960;&#20309;&#24847;&#35782;&#30340;&#20840;&#26223;&#35821;&#20041;&#20998;&#21106;Transformer&#65292;&#21363;SGAT4PASS&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29699;&#38754;&#20960;&#20309;&#24847;&#35782;&#30340;&#20998;&#21106;&#26694;&#26550;&#65292;&#23427;&#21253;&#25324;&#19977;&#20010;&#27169;&#22359;&#65292;&#21363;&#29699;&#38754;&#20960;&#20309;&#24863;&#30693;&#22270;&#20687;&#25237;&#24433;&#65292;&#29699;&#38754;&#21487;&#24418;&#21464;&#34917;&#19969;&#23884;&#20837;&#21644;&#20840;&#26223;&#24863;&#30693;&#25439;&#22833;&#65292;&#23427;&#23545;&#20855;&#26377;3D&#25200;&#21160;&#30340;&#36755;&#20837;&#22270;&#20687;&#36827;&#34892;&#22788;&#29702;&#65292;&#24182;&#23545;&#24050;&#26377;&#30340;&#21487;&#24418;&#21464;&#34917;&#19969;&#23884;&#20837;&#21152;&#20837;&#20102;&#29699;&#38754;&#20960;&#20309;&#24863;&#30693;&#30340;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
As an important and challenging problem in computer vision, PAnoramic Semantic Segmentation (PASS) gives complete scene perception based on an ultra-wide angle of view. Usually, prevalent PASS methods with 2D panoramic image input focus on solving image distortions but lack consideration of the 3D properties of original $360^{\circ}$ data. Therefore, their performance will drop a lot when inputting panoramic images with the 3D disturbance. To be more robust to 3D disturbance, we propose our Spherical Geometry-Aware Transformer for PAnoramic Semantic Segmentation (SGAT4PASS), considering 3D spherical geometry knowledge. Specifically, a spherical geometry-aware framework is proposed for PASS. It includes three modules, i.e., spherical geometry-aware image projection, spherical deformable patch embedding, and a panorama-aware loss, which takes input images with 3D disturbance into account, adds a spherical geometry-aware constraint on the existing deformable patch embedding, and indicates
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#8212;&#8212; Sharpened Lazy Incremental Quasi-Newton (SLIQN) &#26041;&#27861;&#65292;&#20854;&#20855;&#26377;&#26174;&#24335;&#30340;&#36229;&#32447;&#24615;&#25910;&#25947;&#36895;&#29575;&#21644;$O(d^2)$&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.17283</link><description>&lt;p&gt;
&#20248;&#21270;&#36864;&#28779;&#31639;&#27861;&#30340;&#35823;&#24046;&#30028;&#21644;&#23616;&#37096;&#25628;&#32034;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Sharpened Lazy Incremental Quasi-Newton Method. (arXiv:2305.17283v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17283
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#8212;&#8212; Sharpened Lazy Incremental Quasi-Newton (SLIQN) &#26041;&#27861;&#65292;&#20854;&#20855;&#26377;&#26174;&#24335;&#30340;&#36229;&#32447;&#24615;&#25910;&#25947;&#36895;&#29575;&#21644;$O(d^2)$&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#20855;&#26377;$Lipschitz$&#36830;&#32493;Hessian&#30697;&#38453;&#22312;$d$&#32500;&#31354;&#38388;&#20013;&#65292;$n$&#20010;&#24378;&#20984;&#20809;&#28369;&#20989;&#25968;&#30340;&#26377;&#38480;&#21644;&#26368;&#23567;&#21270;&#38382;&#39064;&#12290;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;$n$&#30340;&#25968;&#37327;&#24456;&#22823;&#65292;&#22240;&#27492;&#24517;&#39035;&#20351;&#29992;&#27599;&#36845;&#20195;&#19968;&#27425;&#19982;$n$&#26080;&#20851;&#30340;&#22686;&#37327;&#24335;&#25110;&#38543;&#26426;&#31639;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#8212;&#8212; Sharpened Lazy Incremental Quasi-Newton (SLIQN) &#26041;&#27861;&#65292;&#20854;&#20855;&#26377;&#26174;&#24335;&#30340;&#36229;&#32447;&#24615;&#25910;&#25947;&#36895;&#29575;&#21644;$O(d^2)$&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the finite sum minimization of $n$ strongly convex and smooth functions with Lipschitz continuous Hessians in $d$ dimensions. In many applications where such problems arise, including maximum likelihood estimation, empirical risk minimization, and unsupervised learning, the number of observations $n$ is large, and it becomes necessary to use incremental or stochastic algorithms whose per-iteration complexity is independent of $n$. Of these, the incremental/stochastic variants of the Newton method exhibit superlinear convergence, but incur a per-iteration complexity of $O(d^3)$, which may be prohibitive in large-scale settings. On the other hand, the incremental Quasi-Newton method incurs a per-iteration complexity of $O(d^2)$ but its superlinear convergence rate has only been characterized asymptotically. This work puts forth the Sharpened Lazy Incremental Quasi-Newton (SLIQN) method that achieves the best of both worlds: an explicit superlinear convergence rate with a per-
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24314;&#31435;&#19968;&#20010;&#35268;&#33539;&#21270;&#30340;&#20266;&#31070;&#32463;&#20999;&#32447;&#26680;&#65292;&#35777;&#26126;&#20102;&#23427;&#33021;&#22815;&#26356;&#22909;&#22320;&#19982;&#31070;&#32463;&#32593;&#32476;&#20915;&#31574;&#20989;&#25968;&#30456;&#20851;&#65292;&#27604;&#22522;&#20110;&#23884;&#20837;&#21644;&#24433;&#21709;&#30340;&#26367;&#20195;&#21697;&#26356;&#26377;&#25928;&#65292;&#24182;&#19988;&#20174;&#23427;&#21019;&#24314;&#30340;&#24402;&#22240;&#20250;&#26356;&#20934;&#30830;&#22320;&#36873;&#25321;&#34987;&#25200;&#21160;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20174;&#32780;&#35777;&#26126;&#20102;&#26680;&#32447;&#24615;&#27169;&#22411;&#26159;&#36328;&#22810;&#20010;&#25968;&#25454;&#39046;&#22495;&#24182;&#26377;&#25928;&#30340;&#26367;&#20195;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.14585</link><description>&lt;p&gt;
&#36890;&#36807;&#20266;&#31070;&#32463;&#20999;&#32447;&#26680;&#20195;&#29702;&#27169;&#22411;&#25552;&#20379;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Robust Explanations for Deep Neural Networks via Pseudo Neural Tangent Kernel Surrogate Models. (arXiv:2305.14585v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14585
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24314;&#31435;&#19968;&#20010;&#35268;&#33539;&#21270;&#30340;&#20266;&#31070;&#32463;&#20999;&#32447;&#26680;&#65292;&#35777;&#26126;&#20102;&#23427;&#33021;&#22815;&#26356;&#22909;&#22320;&#19982;&#31070;&#32463;&#32593;&#32476;&#20915;&#31574;&#20989;&#25968;&#30456;&#20851;&#65292;&#27604;&#22522;&#20110;&#23884;&#20837;&#21644;&#24433;&#21709;&#30340;&#26367;&#20195;&#21697;&#26356;&#26377;&#25928;&#65292;&#24182;&#19988;&#20174;&#23427;&#21019;&#24314;&#30340;&#24402;&#22240;&#20250;&#26356;&#20934;&#30830;&#22320;&#36873;&#25321;&#34987;&#25200;&#21160;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20174;&#32780;&#35777;&#26126;&#20102;&#26680;&#32447;&#24615;&#27169;&#22411;&#26159;&#36328;&#22810;&#20010;&#25968;&#25454;&#39046;&#22495;&#24182;&#26377;&#25928;&#30340;&#26367;&#20195;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#36890;&#36807;&#25968;&#25454;&#24402;&#23646;&#20219;&#21153;&#65292;&#35299;&#37322;&#22411;AI&#30340;&#36827;&#27493;&#20043;&#19968;&#26159;&#36890;&#36807;&#35299;&#37322;&#31034;&#20363;&#31574;&#30053;&#23454;&#29616;&#30340;&#12290;&#28982;&#32780;&#65292;&#29992;&#20110;&#23558;&#20915;&#31574;&#24402;&#22240;&#20110;&#35757;&#32451;&#25968;&#25454;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#23578;&#26410;&#30456;&#20114;&#27604;&#36739;&#65292;&#20197;&#30830;&#23450;&#23427;&#20204;&#26159;&#21542;&#24418;&#25104;&#31070;&#32463;&#32593;&#32476;(NN)&#30340;&#30495;&#27491;&#20195;&#29702;&#27169;&#22411;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36890;&#36807;&#20004;&#31181;&#26041;&#24335;&#35777;&#26126;&#20102;&#32447;&#24615;&#29305;&#24449;&#31354;&#38388;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#26377;&#25928;&#24615;&#65306;(1)&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#35268;&#33539;&#21270;&#30340;&#20266;&#31070;&#32463;&#20999;&#32447;&#26680;(pNTK)&#65292;&#23427;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#22823;&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#20013;&#19982;&#31070;&#32463;&#32593;&#32476;&#20915;&#31574;&#20989;&#25968;&#26356;&#30456;&#20851;&#65292;&#27604;&#22522;&#20110;&#23884;&#20837;&#21644;&#24433;&#21709;&#30340;&#26367;&#20195;&#21697;&#26356;&#20026;&#26377;&#25928;&#65307;(2)&#25105;&#20204;&#23637;&#31034;&#20102;&#20174;&#35268;&#33539;&#21270;pNTK&#21019;&#24314;&#30340;&#24402;&#22240;&#27604;&#36825;&#20123;&#26367;&#20195;&#21697;&#26356;&#20934;&#30830;&#22320;&#36873;&#25321;&#34987;&#25200;&#21160;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#22522;&#20110;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#65292;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#26680;&#32447;&#24615;&#27169;&#22411;&#26159;&#36328;&#22810;&#20010;&#25968;&#25454;&#39046;&#22495;&#24182;&#26377;&#25928;&#30340;&#26367;&#20195;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the ways recent progress has been made on explainable AI has been via explain-by-example strategies, specifically, through data attribution tasks. The feature spaces used to attribute decisions to training data, however, have not been compared against one another as to whether they form a truly representative surrogate model of the neural network (NN). Here, we demonstrate the efficacy of surrogate linear feature spaces to neural networks through two means: (1) we establish that a normalized psuedo neural tangent kernel (pNTK) is more correlated to the neural network decision functions than embedding based and influence based alternatives in both computer vision and large language model architectures; (2) we show that the attributions created from the normalized pNTK more accurately select perturbed training data in a data poisoning attribution task than these alternatives. Based on these observations, we conclude that kernel linear models are effective surrogate models across m
&lt;/p&gt;</description></item><item><title>&#24207;&#21015;&#24314;&#27169;&#26159;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#27604;Q-Learning&#21644;Imitation Learning&#26356;&#36866;&#21512;&#22312;&#31232;&#30095;&#22870;&#21169;&#21644;&#20302;&#36136;&#37327;&#25968;&#25454;&#35774;&#32622;&#19979;&#30340;&#36873;&#25321;&#65292;&#22312;&#20219;&#21153;&#33539;&#22260;&#22686;&#21152;&#26102;&#65292;&#24207;&#21015;&#24314;&#27169;&#21644;&#27169;&#20223;&#23398;&#20064;&#26356;&#21487;&#21462;&#12290;</title><link>http://arxiv.org/abs/2305.14550</link><description>&lt;p&gt;
&#24207;&#21015;&#24314;&#27169;&#26159;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#19968;&#20010;&#24378;&#26377;&#21147;&#30340;&#31454;&#20105;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequence Modeling is a Robust Contender for Offline Reinforcement Learning. (arXiv:2305.14550v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14550
&lt;/p&gt;
&lt;p&gt;
&#24207;&#21015;&#24314;&#27169;&#26159;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#27604;Q-Learning&#21644;Imitation Learning&#26356;&#36866;&#21512;&#22312;&#31232;&#30095;&#22870;&#21169;&#21644;&#20302;&#36136;&#37327;&#25968;&#25454;&#35774;&#32622;&#19979;&#30340;&#36873;&#25321;&#65292;&#22312;&#20219;&#21153;&#33539;&#22260;&#22686;&#21152;&#26102;&#65292;&#24207;&#21015;&#24314;&#27169;&#21644;&#27169;&#20223;&#23398;&#20064;&#26356;&#21487;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20351;&#20195;&#29702;&#33021;&#22815;&#20174;&#38745;&#24577;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#26377;&#25928;&#30340;&#26368;&#22823;&#21270;&#25910;&#30410;&#31574;&#30053;&#12290;&#31163;&#32447;RL&#30340;&#19977;&#22823;&#33539;&#24335;&#26159;Q-Learning&#12289;Imitation Learning&#21644;Sequence Modeling&#12290;&#19968;&#20010;&#20851;&#38190;&#30340;&#38382;&#39064;&#26159;&#65306;&#22312;&#20160;&#20040;&#26465;&#20214;&#19979;&#65292;&#21738;&#31181;&#33539;&#24335;&#34987;&#20248;&#20808;&#36873;&#25321;&#65311;&#25105;&#20204;&#36890;&#36807;&#25506;&#32034;&#20195;&#34920;&#24615;&#31639;&#27861;&#8212;&#8212;&#20445;&#23432;Q-Learning(CQL)&#12289;&#34892;&#20026;&#20811;&#38534; (BC)&#21644;&#20915;&#31574;Transformer (DT)&#8212;&#8212;&#22312;&#24120;&#29992;&#30340;D4RL&#21644;Robomimic&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#34920;&#29616;&#26469;&#23545;&#36825;&#20010;&#38382;&#39064;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#26377;&#38024;&#23545;&#24615;&#30340;&#23454;&#39564;&#26469;&#29702;&#35299;&#23427;&#20204;&#22312;&#25968;&#25454;&#23376;&#20248;&#24615;&#21644;&#20219;&#21153;&#22797;&#26434;&#24615;&#26041;&#38754;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#21457;&#29616;&#26159;&#65306;(1)&#24207;&#21015;&#24314;&#27169;&#38656;&#35201;&#27604;Q-Learning&#26356;&#22810;&#30340;&#25968;&#25454;&#26469;&#23398;&#20064;&#31454;&#20105;&#24615;&#31574;&#30053;&#65292;&#20294;&#26356;&#21152;&#31283;&#20581;&#65307;(2)&#24207;&#21015;&#24314;&#27169;&#22312;&#31232;&#30095;&#22870;&#21169;&#21644;&#20302;&#36136;&#37327;&#25968;&#25454;&#35774;&#32622;&#20013;&#27604;Q-Learning&#21644;Imitation Learning&#37117;&#35201;&#22909;&#24471;&#22810;&#65307;(3)&#38543;&#30528;&#20219;&#21153;&#33539;&#22260;&#30340;&#22686;&#21152;&#65292;&#24207;&#21015;&#24314;&#27169;&#21644;&#27169;&#20223;&#23398;&#20064;&#26356;&#21487;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) allows agents to learn effective, return-maximizing policies from a static dataset. Three major paradigms for offline RL are Q-Learning, Imitation Learning, and Sequence Modeling. A key open question is: which paradigm is preferred under what conditions? We study this question empirically by exploring the performance of representative algorithms -- Conservative Q-Learning (CQL), Behavior Cloning (BC), and Decision Transformer (DT) -- across the commonly used D4RL and Robomimic benchmarks. We design targeted experiments to understand their behavior concerning data suboptimality and task complexity. Our key findings are: (1) Sequence Modeling requires more data than Q-Learning to learn competitive policies but is more robust; (2) Sequence Modeling is a substantially better choice than both Q-Learning and Imitation Learning in sparse-reward and low-quality data settings; and (3) Sequence Modeling and Imitation Learning are preferable as task horizon inc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#31574;&#30053;&#65292;&#36890;&#36807;&#23545;&#25239;&#24615;CAM&#29983;&#25104;&#32593;&#32476;&#29983;&#25104;&#31283;&#20581;&#30340;&#35821;&#20041;&#20998;&#21106;&#25552;&#35758;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#20998;&#21106;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.12522</link><description>&lt;p&gt;
P-NOC:&#23545;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#36827;&#34892;&#23545;&#25239;&#24615;CAM&#29983;&#25104;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
P-NOC: Adversarial CAM Generation for Weakly Supervised Semantic Segmentation. (arXiv:2305.12522v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12522
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#31574;&#30053;&#65292;&#36890;&#36807;&#23545;&#25239;&#24615;CAM&#29983;&#25104;&#32593;&#32476;&#29983;&#25104;&#31283;&#20581;&#30340;&#35821;&#20041;&#20998;&#21106;&#25552;&#35758;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#20998;&#21106;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20943;&#23569;&#23545;&#22823;&#37327;&#26377;&#30417;&#30563;&#20998;&#21106;&#27880;&#37322;&#38598;&#30340;&#20381;&#36182;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#22810;&#31181;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#65288;WSSS&#65289;&#31574;&#30053;&#12290;&#36825;&#20123;&#31574;&#30053;&#36890;&#24120;&#20381;&#36182;&#20110;&#20808;&#36827;&#30340;&#25968;&#25454;&#21644;&#27169;&#22411;&#27491;&#21017;&#21270;&#31574;&#30053;&#65292;&#20197;&#28608;&#21457;&#20998;&#21106;&#20808;&#39564;&#20013;&#26377;&#29992;&#23646;&#24615;&#65288;&#20363;&#22914;&#65292;&#39044;&#27979;&#23436;&#25972;&#24615;&#21644;&#23545;&#35821;&#20041;&#36793;&#30028;&#30340;&#24544;&#23454;&#24230;&#65289;&#30340;&#21457;&#23637;&#65292;&#32780;&#19981;&#32771;&#34385;&#27880;&#37322;&#20449;&#24687;&#30340;&#32570;&#20047;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#20998;&#26512;&#20114;&#34917;&#30340;WSSS&#25216;&#26415;&#21644;&#27491;&#21017;&#21270;&#31574;&#30053;&#65292;&#32771;&#34385;&#23427;&#20204;&#30340;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#22522;&#20934;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31867;&#21035;&#29305;&#23450;&#30340;&#23545;&#25239;&#24615;&#25830;&#38500;&#31574;&#30053;&#65292;&#21253;&#25324;&#36880;&#28176;&#25913;&#36827;&#30340;&#20004;&#20010;&#23545;&#25239;&#24615;CAM&#29983;&#25104;&#32593;&#32476;&#65292;&#20197;&#20135;&#29983;&#31283;&#20581;&#30340;&#35821;&#20041;&#20998;&#21106;&#25552;&#35758;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#22522;&#20934;&#30340;&#25928;&#26524;&#65292;&#22312;Pascal VOC 2012&#21644;MS COCO 2014&#25968;&#25454;&#38598;&#19978;&#37117;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
To mitigate the necessity for large amounts of supervised segmentation annotation sets, multiple Weakly Supervised Semantic Segmentation (WSSS) strategies have been devised. These will often rely on advanced data and model regularization strategies to instigate the development of useful properties (e.g., prediction completeness and fidelity to semantic boundaries) in segmentation priors, notwithstanding the lack of annotated information. In this work, we first create a strong baseline by analyzing complementary WSSS techniques and regularizing strategies, considering their strengths and limitations. We then propose a new Class-specific Adversarial Erasing strategy, comprising two adversarial CAM generating networks being gradually refined to produce robust semantic segmentation proposals. Empirical results suggest that our approach induces substantial improvement in the effectiveness of the baseline, resulting in a noticeable improvement over both Pascal VOC 2012 and MS COCO 2014 datas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#32479;&#35745;&#23398;&#35282;&#24230;&#24314;&#31435;&#25928;&#29992;&#29702;&#35770;&#65292;&#26088;&#22312;&#22522;&#20110;&#19968;&#33324;&#24615;&#25351;&#26631;&#23450;&#37327;&#35780;&#20272;&#21512;&#25104;&#31639;&#27861;&#30340;&#25928;&#29992;&#65292;&#25928;&#29992;&#25351;&#26631;&#30340;&#20998;&#26512;&#30028;&#38480;&#25581;&#31034;&#20102;&#25351;&#26631;&#25910;&#25947;&#30340;&#20851;&#38190;&#26465;&#20214;&#65292;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#21482;&#35201;&#19979;&#28216;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#27169;&#22411;&#35268;&#33539;&#26159;&#27491;&#30830;&#30340;&#65292;&#21512;&#25104;&#29305;&#24449;&#20998;&#24067;&#19981;&#19968;&#23450;&#19982;&#21407;&#22987;&#29305;&#24449;&#20998;&#24067;&#30456;&#21516;&#65292;&#25928;&#29992;&#25351;&#26631;&#20250;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2305.10015</link><description>&lt;p&gt;
&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#25928;&#29992;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Utility Theory of Synthetic Data Generation. (arXiv:2305.10015v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10015
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#32479;&#35745;&#23398;&#35282;&#24230;&#24314;&#31435;&#25928;&#29992;&#29702;&#35770;&#65292;&#26088;&#22312;&#22522;&#20110;&#19968;&#33324;&#24615;&#25351;&#26631;&#23450;&#37327;&#35780;&#20272;&#21512;&#25104;&#31639;&#27861;&#30340;&#25928;&#29992;&#65292;&#25928;&#29992;&#25351;&#26631;&#30340;&#20998;&#26512;&#30028;&#38480;&#25581;&#31034;&#20102;&#25351;&#26631;&#25910;&#25947;&#30340;&#20851;&#38190;&#26465;&#20214;&#65292;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#21482;&#35201;&#19979;&#28216;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#27169;&#22411;&#35268;&#33539;&#26159;&#27491;&#30830;&#30340;&#65292;&#21512;&#25104;&#29305;&#24449;&#20998;&#24067;&#19981;&#19968;&#23450;&#19982;&#21407;&#22987;&#29305;&#24449;&#20998;&#24067;&#30456;&#21516;&#65292;&#25928;&#29992;&#25351;&#26631;&#20250;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#21512;&#25104;&#25968;&#25454;&#30340;&#25928;&#29992;&#23545;&#20110;&#34913;&#37327;&#21512;&#25104;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#32467;&#26524;&#20391;&#37325;&#20110;&#23545;&#21512;&#25104;&#25968;&#25454;&#25928;&#29992;&#30340;&#32463;&#39564;&#35780;&#20272;&#65292;&#32780;&#38024;&#23545;&#21512;&#25104;&#25968;&#25454;&#31639;&#27861;&#22914;&#20309;&#24433;&#21709;&#25928;&#29992;&#30340;&#29702;&#35770;&#29702;&#35299;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#25991;&#20174;&#32479;&#35745;&#23398;&#35282;&#24230;&#24314;&#31435;&#25928;&#29992;&#29702;&#35770;&#65292;&#26088;&#22312;&#22522;&#20110;&#19968;&#33324;&#24615;&#25351;&#26631;&#23450;&#37327;&#35780;&#20272;&#21512;&#25104;&#31639;&#27861;&#30340;&#25928;&#29992;&#12290;&#35813;&#25351;&#26631;&#23450;&#20041;&#20026;&#22312;&#21512;&#25104;&#21644;&#21407;&#22987;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#20043;&#38388;&#27867;&#21270;&#30340;&#32477;&#23545;&#24046;&#24322;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#35813;&#25928;&#29992;&#25351;&#26631;&#30340;&#20998;&#26512;&#30028;&#38480;&#26469;&#30740;&#31350;&#25351;&#26631;&#25910;&#25947;&#30340;&#20851;&#38190;&#26465;&#20214;&#12290;&#19968;&#20010;&#26377;&#36259;&#30340;&#32467;&#26524;&#26159;&#65292;&#21482;&#35201;&#19979;&#28216;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#27169;&#22411;&#35268;&#33539;&#26159;&#27491;&#30830;&#30340;&#65292;&#21512;&#25104;&#29305;&#24449;&#20998;&#24067;&#19981;&#19968;&#23450;&#19982;&#21407;&#22987;&#29305;&#24449;&#20998;&#24067;&#30456;&#21516;&#65292;&#21017;&#35813;&#25928;&#29992;&#25351;&#26631;&#20250;&#25910;&#25947;&#12290;&#21478;&#19968;&#20010;&#37325;&#35201;&#30340;&#25928;&#29992;&#25351;&#26631;&#22522;&#20110;&#21512;&#25104;&#21644;&#21407;&#22987;&#25968;&#25454;&#20043;&#38388;&#28508;&#22312;&#30340;&#22240;&#26524;&#26426;&#21046;&#19968;&#33268;&#24615;&#12290;&#35813;&#29702;&#35770;&#20351;&#29992;&#20960;&#31181;&#21512;&#25104;&#31639;&#27861;&#36827;&#34892;&#35828;&#26126;&#65292;&#24182;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#25928;&#29992;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating the utility of synthetic data is critical for measuring the effectiveness and efficiency of synthetic algorithms. Existing results focus on empirical evaluations of the utility of synthetic data, whereas the theoretical understanding of how utility is affected by synthetic data algorithms remains largely unexplored. This paper establishes utility theory from a statistical perspective, aiming to quantitatively assess the utility of synthetic algorithms based on a general metric. The metric is defined as the absolute difference in generalization between models trained on synthetic and original datasets. We establish analytical bounds for this utility metric to investigate critical conditions for the metric to converge. An intriguing result is that the synthetic feature distribution is not necessarily identical to the original one for the convergence of the utility metric as long as the model specification in downstream learning tasks is correct. Another important utility metri
&lt;/p&gt;</description></item><item><title>OpenBox&#26159;&#19968;&#20010;&#36890;&#29992;&#40657;&#30418;&#20248;&#21270;&#30340;Python&#24037;&#20855;&#21253;&#65292;&#25552;&#20379;&#20102;&#29992;&#25143;&#21451;&#22909;&#30340;&#25509;&#21475;&#21644;&#21487;&#35270;&#21270;&#21151;&#33021;&#65292;&#27169;&#22359;&#21270;&#35774;&#35745;&#33021;&#22815;&#22312;&#29616;&#26377;&#31995;&#32479;&#20013;&#28789;&#27963;&#37096;&#32626;&#65292;&#24182;&#19988;&#23454;&#39564;&#35777;&#26126;&#20854;&#27604;&#29616;&#26377;&#31995;&#32479;&#26356;&#26377;&#25928;&#21644;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2304.13339</link><description>&lt;p&gt;
OpenBox&#65306;&#36890;&#29992;&#40657;&#30418;&#20248;&#21270;&#30340; Python &#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
OpenBox: A Python Toolkit for Generalized Black-box Optimization. (arXiv:2304.13339v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13339
&lt;/p&gt;
&lt;p&gt;
OpenBox&#26159;&#19968;&#20010;&#36890;&#29992;&#40657;&#30418;&#20248;&#21270;&#30340;Python&#24037;&#20855;&#21253;&#65292;&#25552;&#20379;&#20102;&#29992;&#25143;&#21451;&#22909;&#30340;&#25509;&#21475;&#21644;&#21487;&#35270;&#21270;&#21151;&#33021;&#65292;&#27169;&#22359;&#21270;&#35774;&#35745;&#33021;&#22815;&#22312;&#29616;&#26377;&#31995;&#32479;&#20013;&#28789;&#27963;&#37096;&#32626;&#65292;&#24182;&#19988;&#23454;&#39564;&#35777;&#26126;&#20854;&#27604;&#29616;&#26377;&#31995;&#32479;&#26356;&#26377;&#25928;&#21644;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40657;&#30418;&#20248;&#21270;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#12289;&#23454;&#39564;&#35774;&#35745;&#21644;&#25968;&#25454;&#24211;&#21442;&#25968;&#35843;&#25972;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#29616;&#26377;&#36719;&#20214;&#21253;&#26102;&#65292;&#29992;&#25143;&#22312;&#36866;&#29992;&#24615;&#12289;&#24615;&#33021;&#21644;&#25928;&#29575;&#26041;&#38754;&#20173;&#38754;&#20020;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102; OpenBox&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#40657;&#30418;&#20248;&#21270;&#24037;&#20855;&#21253;&#65292;&#25552;&#39640;&#20102;&#20854;&#21487;&#29992;&#24615;&#12290;&#23427;&#23454;&#29616;&#20102;&#29992;&#25143;&#21451;&#22909;&#30340;&#25509;&#21475;&#21644;&#21487;&#35270;&#21270;&#21151;&#33021;&#65292;&#35753;&#29992;&#25143;&#33021;&#22815;&#23450;&#20041;&#21644;&#31649;&#29702;&#20219;&#21153;&#12290;OpenBox &#30340;&#27169;&#22359;&#21270;&#35774;&#35745;&#26377;&#21161;&#20110;&#22312;&#29616;&#26377;&#31995;&#32479;&#20013;&#28789;&#27963;&#37096;&#32626;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;OpenBox&#27604;&#29616;&#26377;&#31995;&#32479;&#26356;&#26377;&#25928;&#21644;&#39640;&#25928;&#12290;OpenBox &#30340;&#28304;&#20195;&#30721;&#21487;&#22312; https://github.com/PKU-DAIR/open-box &#20013;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Black-box optimization (BBO) has a broad range of applications, including automatic machine learning, experimental design, and database knob tuning. However, users still face challenges when applying BBO methods to their problems at hand with existing software packages in terms of applicability, performance, and efficiency. This paper presents OpenBox, an open-source BBO toolkit with improved usability. It implements user-friendly inferfaces and visualization for users to define and manage their tasks. The modular design behind OpenBox facilitates its flexible deployment in existing systems. Experimental results demonstrate the effectiveness and efficiency of OpenBox over existing systems. The source code of OpenBox is available at https://github.com/PKU-DAIR/open-box.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#30340;&#38750;&#27604;&#20363;&#36180;&#29575;&#27169;&#22411; (N$^3$POM) &#29992;&#20110;&#26377;&#24207;&#22238;&#24402;&#65292;&#21487;&#20197;&#23545;&#36830;&#32493;&#21464;&#37327;&#36827;&#34892;&#39044;&#27979;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#20855;&#26377;&#28789;&#27963;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.17823</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#21487;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#30340;&#36830;&#32493;&#22238;&#24212;&#26377;&#24207;&#22238;&#24402;&#38750;&#27604;&#20363;&#36180;&#29575;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
An interpretable neural network-based non-proportional odds model for ordinal regression with continuous response. (arXiv:2303.17823v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17823
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#30340;&#38750;&#27604;&#20363;&#36180;&#29575;&#27169;&#22411; (N$^3$POM) &#29992;&#20110;&#26377;&#24207;&#22238;&#24402;&#65292;&#21487;&#20197;&#23545;&#36830;&#32493;&#21464;&#37327;&#36827;&#34892;&#39044;&#27979;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#20855;&#26377;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#30340;&#38750;&#27604;&#20363;&#36180;&#29575;&#27169;&#22411;&#65288;N$^3$POM) &#29992;&#20110;&#26377;&#24207;&#22238;&#24402;&#65292;&#20854;&#20013;&#21453;&#24212;&#21464;&#37327;&#19981;&#20165;&#21487;&#20197;&#21462;&#31163;&#25955;&#20540;&#65292;&#20063;&#21487;&#20197;&#21462;&#36830;&#32493;&#20540;&#65292;&#32780;&#22238;&#24402;&#31995;&#25968;&#26681;&#25454;&#39044;&#27979;&#39034;&#24207;&#21453;&#24212;&#20063;&#19981;&#21516;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#30452;&#25509;&#20174;&#31163;&#25955;&#21453;&#24212;&#20272;&#35745;&#32447;&#24615;&#31995;&#25968;&#19981;&#21516;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#38750;&#32447;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#20197;&#21453;&#24212;&#20026;&#36755;&#20837;&#20135;&#29983;&#32447;&#24615;&#31995;&#25968;&#12290;&#30001;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21183;&#65292;N$^3$POM&#21487;&#20197;&#22312;&#20445;&#30041;&#20256;&#32479;&#26377;&#24207;&#22238;&#24402;&#30340;&#21487;&#35299;&#37322;&#24615;&#30340;&#21516;&#26102;&#20855;&#26377;&#28789;&#27963;&#24615;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#20805;&#20998;&#30340;&#26465;&#20214;&#65292;&#20351;&#24471;&#22312;&#25351;&#23450;&#30340;&#29992;&#25143;&#21306;&#22495;&#20869;&#65292;&#39044;&#27979;&#30340;&#26465;&#20214;&#32047;&#31215;&#27010;&#29575;&#65288;CCP&#65289;&#28385;&#36275;&#23616;&#37096;&#21333;&#35843;&#24615;&#32422;&#26463;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#20445;&#25345;&#21333;&#35843;&#24615;&#30340;&#38543;&#26426;&#65288;MPS&#65289;&#31639;&#27861;&#26469;&#20805;&#20998;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an interpretable neural network-based non-proportional odds model (N$^3$POM) for ordinal regression, where the response variable can take not only discrete but also continuous values, and the regression coefficients vary depending on the predicting ordinal response. In contrast to conventional approaches estimating the linear coefficients of regression directly from the discrete response, we train a non-linear neural network that outputs the linear coefficients by taking the response as its input. By virtue of the neural network, N$^3$POM may have flexibility while preserving the interpretability of the conventional ordinal regression. We show a sufficient condition so that the predicted conditional cumulative probability~(CCP) satisfies the monotonicity constraint locally over a user-specified region in the covariate space; we also provide a monotonicity-preserving stochastic (MPS) algorithm for training the neural network adequately.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#36741;&#21161;&#20195;&#25968;&#36712;&#36857;&#20449;&#24687;&#26126;&#30830;&#28155;&#21152;&#21040;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#31283;&#23450;&#21270;&#21644;&#25237;&#24433;&#26041;&#27861;&#21512;&#24182;&#20449;&#24687;&#65292;&#23545;&#22810;&#20307;&#25670;&#21644;&#20998;&#23376;&#21160;&#21147;&#23398;&#24773;&#26223;&#36827;&#34892;&#20102;&#27169;&#25311;&#23454;&#39564;&#12290;&#35813;&#26041;&#27861;&#26131;&#20110;&#23454;&#29616;&#65292;&#23545;&#35757;&#32451;&#24615;&#33021;&#24433;&#21709;&#26377;&#38480;&#65292;&#22312;&#25512;&#29702;&#26041;&#38754;&#32473;&#20986;&#20102;&#26174;&#33879;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2211.14302</link><description>&lt;p&gt;
&#32422;&#26463;&#31070;&#32463;&#32593;&#32476;&#65306;&#31070;&#32463;DAEs
&lt;/p&gt;
&lt;p&gt;
Neural DAEs: Constrained neural networks. (arXiv:2211.14302v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14302
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#36741;&#21161;&#20195;&#25968;&#36712;&#36857;&#20449;&#24687;&#26126;&#30830;&#28155;&#21152;&#21040;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#31283;&#23450;&#21270;&#21644;&#25237;&#24433;&#26041;&#27861;&#21512;&#24182;&#20449;&#24687;&#65292;&#23545;&#22810;&#20307;&#25670;&#21644;&#20998;&#23376;&#21160;&#21147;&#23398;&#24773;&#26223;&#36827;&#34892;&#20102;&#27169;&#25311;&#23454;&#39564;&#12290;&#35813;&#26041;&#27861;&#26131;&#20110;&#23454;&#29616;&#65292;&#23545;&#35757;&#32451;&#24615;&#33021;&#24433;&#21709;&#26377;&#38480;&#65292;&#22312;&#25512;&#29702;&#26041;&#38754;&#32473;&#20986;&#20102;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#36741;&#21161;&#20195;&#25968;&#36712;&#36857;&#20449;&#24687;&#26126;&#30830;&#28155;&#21152;&#21040;&#21160;&#24577;&#31995;&#32479;&#30340;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#20174;&#24494;&#20998;&#20195;&#25968;&#26041;&#31243;&#21644;&#27969;&#24418;&#19978;&#30340;&#24494;&#20998;&#26041;&#31243;&#39046;&#22495;&#27762;&#21462;&#28789;&#24863;&#65292;&#24182;&#22312;&#27531;&#24046;&#31070;&#32463;&#32593;&#32476;&#20013;&#23454;&#29616;&#30456;&#20851;&#26041;&#27861;&#65292;&#23613;&#31649;&#23384;&#22312;&#19968;&#20123;&#22522;&#26412;&#24773;&#22659;&#19978;&#30340;&#24046;&#24322;&#12290;&#36890;&#36807;&#31283;&#23450;&#21270;&#21644;&#25237;&#24433;&#26041;&#27861;&#65292;&#23558;&#32422;&#26463;&#25110;&#36741;&#21161;&#20449;&#24687;&#25928;&#26524;&#21512;&#24182;&#65292;&#24182;&#36890;&#36807;&#23545;&#22810;&#20307;&#25670;&#21644;&#20998;&#23376;&#21160;&#21147;&#23398;&#24773;&#26223;&#30340;&#27169;&#25311;&#23454;&#39564;&#23637;&#31034;&#20102;&#20309;&#26102;&#20351;&#29992;&#21738;&#31181;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#19968;&#20123;&#26041;&#27861;&#26131;&#20110;&#22312;&#29616;&#26377;&#20195;&#30721;&#20013;&#23454;&#29616;&#65292;&#24182;&#23545;&#35757;&#32451;&#24615;&#33021;&#24433;&#21709;&#26377;&#38480;&#65292;&#21516;&#26102;&#22312;&#25512;&#29702;&#26041;&#38754;&#32473;&#20986;&#20102;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article investigates the effect of explicitly adding auxiliary algebraic trajectory information to neural networks for dynamical systems. We draw inspiration from the field of differential-algebraic equations and differential equations on manifolds and implement related methods in residual neural networks, despite some fundamental scenario differences. Constraint or auxiliary information effects are incorporated through stabilization as well as projection methods, and we show when to use which method based on experiments involving simulations of multi-body pendulums and molecular dynamics scenarios. Several of our methods are easy to implement in existing code and have limited impact on training performance while giving significant boosts in terms of inference.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#22810;&#39046;&#22495;&#21327;&#20316;&#23398;&#20064;&#30340;&#32852;&#37030;&#33258;&#36866;&#24212;&#25552;&#31034;&#35843;&#20248;&#31639;&#27861; FedAPT&#65292;&#21033;&#29992;&#24378;&#22823;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#23454;&#29616;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;&#20854;&#26680;&#24515;&#24605;&#24819;&#26159;&#20026;&#27599;&#20010;&#27979;&#35797;&#26679;&#26412;&#25552;&#20379;&#20010;&#24615;&#21270;&#25552;&#31034;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#22320;&#37322;&#25918;&#29305;&#23450;&#39046;&#22495;&#30340;&#30693;&#35782;&#26469;&#23454;&#29616;&#12290;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#33258;&#36866;&#24212;&#25552;&#31034;&#35843;&#20248;&#27169;&#22359;&#65292;&#26381;&#21153;&#22120;&#29983;&#25104;&#20851;&#38190;&#20449;&#24687;&#24182;&#20998;&#37197;&#32473;&#23458;&#25143;&#31471;&#65292;&#20174;&#32780;&#23454;&#29616;&#21327;&#21516;&#35757;&#32451;&#20840;&#23616;&#30340;&#33258;&#36866;&#24212;&#32593;&#32476;&#21644;&#20803;&#25552;&#31034;&#12290;</title><link>http://arxiv.org/abs/2211.07864</link><description>&lt;p&gt;
&#38754;&#21521;&#22810;&#39046;&#22495;&#21327;&#20316;&#23398;&#20064;&#30340;&#32852;&#37030;&#33258;&#36866;&#24212;&#25552;&#31034;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
Federated Adaptive Prompt Tuning for Multi-domain Collaborative Learning. (arXiv:2211.07864v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.07864
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#22810;&#39046;&#22495;&#21327;&#20316;&#23398;&#20064;&#30340;&#32852;&#37030;&#33258;&#36866;&#24212;&#25552;&#31034;&#35843;&#20248;&#31639;&#27861; FedAPT&#65292;&#21033;&#29992;&#24378;&#22823;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#23454;&#29616;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;&#20854;&#26680;&#24515;&#24605;&#24819;&#26159;&#20026;&#27599;&#20010;&#27979;&#35797;&#26679;&#26412;&#25552;&#20379;&#20010;&#24615;&#21270;&#25552;&#31034;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#22320;&#37322;&#25918;&#29305;&#23450;&#39046;&#22495;&#30340;&#30693;&#35782;&#26469;&#23454;&#29616;&#12290;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#33258;&#36866;&#24212;&#25552;&#31034;&#35843;&#20248;&#27169;&#22359;&#65292;&#26381;&#21153;&#22120;&#29983;&#25104;&#20851;&#38190;&#20449;&#24687;&#24182;&#20998;&#37197;&#32473;&#23458;&#25143;&#31471;&#65292;&#20174;&#32780;&#23454;&#29616;&#21327;&#21516;&#35757;&#32451;&#20840;&#23616;&#30340;&#33258;&#36866;&#24212;&#32593;&#32476;&#21644;&#20803;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20351;&#24471;&#22810;&#20010;&#23458;&#25143;&#31471;&#33021;&#22815;&#22312;&#19981;&#27844;&#38706;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21327;&#21516;&#35757;&#32451;&#20840;&#23616;&#27169;&#22411;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#36890;&#24120;&#38656;&#35201;&#35757;&#32451;&#23436;&#25972;&#30340;&#27169;&#22411;&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;&#24378;&#22823;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20986;&#29616;&#20351;&#24471;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20351;&#29992;&#26356;&#23569;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#33021;&#22815;&#23454;&#29616;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#22810;&#39046;&#22495;&#21327;&#20316;&#22270;&#20687;&#20998;&#31867;&#30340;&#32852;&#37030;&#33258;&#36866;&#24212;&#25552;&#31034;&#35843;&#20248;&#31639;&#27861; FedAPT&#65292;&#21033;&#29992;&#31867;&#20284; CLIP &#30340;&#24378;&#22823;&#22522;&#30784;&#27169;&#22411;&#12290;&#19982;&#30452;&#25509;&#32852;&#37030;&#25552;&#31034;&#35843;&#20248;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#38024;&#23545;&#27599;&#20010;&#27979;&#35797;&#26679;&#26412;&#33258;&#36866;&#24212;&#22320;&#37322;&#25918;&#29305;&#23450;&#39046;&#22495;&#30340;&#30693;&#35782;&#65292;&#20026;&#20854;&#25552;&#20379;&#20010;&#24615;&#21270;&#25552;&#31034;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#24819;&#27861;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#25552;&#31034;&#35843;&#20248;&#27169;&#22359;&#65292;&#23427;&#21253;&#25324;&#20803;&#25552;&#31034;&#65292;&#33258;&#36866;&#24212;&#32593;&#32476;&#21644;&#19968;&#20123;&#20851;&#38190;&#20449;&#24687;&#12290;&#26381;&#21153;&#22120;&#38543;&#26426;&#29983;&#25104;&#19968;&#32452;&#20851;&#38190;&#20449;&#24687;&#65292;&#24182;&#23558;&#27599;&#20010;&#23458;&#25143;&#31471;&#20998;&#37197;&#19968;&#20010;&#21807;&#19968;&#30340;&#20851;&#38190;&#20449;&#24687;&#12290;&#28982;&#21518;&#65292;&#25152;&#26377;&#23458;&#25143;&#31471;&#21327;&#21516;&#35757;&#32451;&#20840;&#23616;&#33258;&#36866;&#24212;&#32593;&#32476;&#21644;&#20803;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) enables multiple clients to collaboratively train a global model without disclosing their data. Previous researches often require training the complete model parameters. However, the emergence of powerful pre-trained models makes it possible to achieve higher performance with fewer learnable parameters in FL. In this paper, we propose a federated adaptive prompt tuning algorithm, FedAPT, for multi-domain collaborative image classification with powerful foundation models, like CLIP. Compared with direct federated prompt tuning, our core idea is to adaptively unlock specific domain knowledge for each test sample in order to provide them with personalized prompts. To implement this idea, we design an adaptive prompt tuning module, which consists of a meta prompt, an adaptive network, and some keys. The server randomly generates a set of keys and assigns a unique key to each client. Then all clients cooperatively train the global adaptive network and meta prompt wit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102; $\beta$-&#24046;&#24322;&#30340;&#31232;&#30095;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#30340;&#20027;&#23548;&#26368;&#23567;&#21270;&#31639;&#27861;&#65292;&#20854;&#33021;&#22815;&#36866;&#29992;&#20110;&#20219;&#20309; $\beta$-&#24046;&#24322;&#21644;&#20854;&#20182;&#31232;&#30095;&#32422;&#26463;&#12290;</title><link>http://arxiv.org/abs/2207.06316</link><description>&lt;p&gt;
&#24102; $\beta$-&#24046;&#24322;&#30340;&#31232;&#30095;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#30340;&#20027;&#23548;&#26368;&#23567;&#21270;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Majorization-minimization for Sparse Nonnegative Matrix Factorization with the $\beta$-divergence. (arXiv:2207.06316v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.06316
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102; $\beta$-&#24046;&#24322;&#30340;&#31232;&#30095;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#30340;&#20027;&#23548;&#26368;&#23567;&#21270;&#31639;&#27861;&#65292;&#20854;&#33021;&#22815;&#36866;&#29992;&#20110;&#20219;&#20309; $\beta$-&#24046;&#24322;&#21644;&#20854;&#20182;&#31232;&#30095;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#20803;&#20056;&#27861;&#26356;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#20855;&#26377; $\beta$-&#24046;&#24322;&#21644;&#20004;&#20010;&#22240;&#23376;&#20013;&#30340;&#19968;&#20010;&#65288;&#27604;&#22914;&#35828;&#65292;&#28608;&#27963;&#30697;&#38453;&#65289;&#31232;&#30095;&#27491;&#21017;&#21270;&#30340;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#12290;&#26631;&#20934;&#30340;&#20570;&#27861;&#26159;&#38480;&#21046;&#23383;&#20856;&#30340;&#21015;&#20855;&#26377;&#21333;&#20301;&#33539;&#25968;&#65292;&#20174;&#32780;&#25511;&#21046;&#21478;&#19968;&#20010;&#22240;&#23376;&#65288;&#23383;&#20856;&#30697;&#38453;&#65289;&#30340;&#33539;&#25968;&#65292;&#20197;&#36991;&#20813;&#30149;&#24577;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#21407;&#38382;&#39064;&#37325;&#26032;&#21442;&#25968;&#21270;&#20026;&#31561;&#20215;&#30340;&#26631;&#24230;&#19981;&#21464;&#30340;&#30446;&#26631;&#20989;&#25968;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23548;&#20986;&#22359;&#19979;&#38477;&#20027;&#23548;&#26368;&#23567;&#21270;&#31639;&#27861;&#65292;&#36825;&#20123;&#31639;&#27861;&#23545;&#20110; $\ell_{1}$-&#27491;&#21017;&#21270;&#25110;&#26356; "&#28608;&#36827;" &#30340;&#23545;&#25968;&#27491;&#21017;&#21270;&#37117;&#21487;&#20197;&#20135;&#29983;&#31616;&#21333;&#30340;&#22810;&#20803;&#20056;&#27861;&#26356;&#26032;&#12290;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#20219;&#20309; $\beta$-&#24046;&#24322;&#65288;&#21363;&#20219;&#20309; $\beta$ &#30340;&#20540;&#65289;&#21644;&#20854;&#20182;&#31232;&#30095;&#32422;&#26463;&#19978;&#20063;&#20855;&#26377;&#36890;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article introduces new multiplicative updates for nonnegative matrix factorization with the $\beta$-divergence and sparse regularization of one of the two factors (say, the activation matrix). It is well known that the norm of the other factor (the dictionary matrix) needs to be controlled in order to avoid an ill-posed formulation. Standard practice consists in constraining the columns of the dictionary to have unit norm, which leads to a nontrivial optimization problem. Our approach leverages a reparametrization of the original problem into the optimization of an equivalent scale-invariant objective function. From there, we derive block-descent majorization-minimization algorithms that result in simple multiplicative updates for either $\ell_{1}$-regularization or the more "aggressive" log-regularization. In contrast with other state-of-the-art methods, our algorithms are universal in the sense that they can be applied to any $\beta$-divergence (i.e., any value of $\beta$) and t
&lt;/p&gt;</description></item></channel></rss>